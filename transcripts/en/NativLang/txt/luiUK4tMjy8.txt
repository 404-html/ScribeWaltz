Speaker 1:          00:01          MMM.

Speaker 2:          00:06          Hi. This video is for anyone interested in syntax, especially anyone who's still interested in syntax. After going through my introduction to the grammar of sentences series, I'd like to present a short overview of computational grammars and generative syntax. I'll spend some time exploring what simple sentences tell us about human language grammar. I'll take a look at the now classic parse tree or x Bar model that attempts to capture what humans are doing when we speak a language and put sentences together and I'll conclude by mentioning a few other models of human grammar. The terms language and grammar get thrown around a lot and it would take me well beyond my time here to work out a sturdy definition with you. I'll merely point you in a single direction narrowing language down to natural human language, which is an umbrella term for the specific first language you learned as a child, your first language and whatever system or process you had available to you as a child that allowed you to learn that language.

Speaker 2:          01:01          At this point, the notion of grammar becomes relevant under one approach. The grammar is something you have in your mind that gets configured in a certain way. Allowing you to use a language under another approach of grammar is something you puzzle out of a language as you learn it. In either case, notice that grammar is not as set of literary stylistic or social preferences like don't say eight eight, eight a word. Instead, the linguistic understanding of grammar seeks out the fundamental mechanism for sentence production. The common way to conceive of this grammar of natural languages too grounded in the speech of real speakers and to explain it as a series of rules. Let's clarify rule here. These rules or statements or imperatives that tell the system how to handle chunks of language, the grammar using our linguistic definition of that term, take some input processes, processes that input using a series of rule statements and produces or generates some output.

Speaker 2:          01:57          This set up means that grammar allows certain sentences and disallows others. Its ability to generate language earns this broad stroke model of grammar. The title, Generative Grammar. One tool for modeling linguistic grammar is x bar theory. This model parses the grammar of sentences. By looking for the relationships between word classes, determining how to group these words into phrases, working out the keyword or head word of the phrase and building back until we can view the grammar of a whole sentence as a single expanded tree. Here's a simple tree for the sentence. The boss ate soup. The major nodes on this tree are phrases. Each phrase has a head, for example, the head of a verb, phrases, a verb. The other elements in a phrase are either specifiers are complements. Specifiers are sisters of the sub phrases within a phrase, meaning that they branch off from the main phrase here and sit parallel with any sub phrase below the main phrase.

Speaker 2:          02:54          Comments. Compliments are sisters of the head, so they branch out next to the head word like happy and from Rome. If we added happy boss from Rome, the specifiers still stays up here on the branch. The first branch of the Noun phrase specifiers and complements our relative and found throughout the tree. Just like heads the subject of the sentence. This noun phrase here is the specifier of this sentence. Node together, the head and the specifiers and complements make up all the material in a phrase. The tree still relies on terms for word classes, which traditional grammarians called parts of speech like Noun for preposition or determiner. What if I want to add a prepositional phrase like at home to the sentence while we have to expand the nodes and our verb phrase instead of just the no down here for the verb and the noun phrase over here, we need a prepositional phrase, but this structure, verb, Noun, phrase preposition was phrase starts to flatten out our tree.

Speaker 2:          03:53          When we start asserting the syntactic structure of a language of, of a chunk of language like this, it makes sense to figure out if the arrangements were choosing have linguistic support. Do we have any reason to maintain this flat structure or on the contrary, we have any rationale for adding some depth to this word phrase considered two pieces of natural language data as evidence. The boss ate soup at home, the boss ate at home soup. The second sentence doesn't work because the propositional phrase material falls too close to the verb. It seems like that the verb expects the core arguments. Soup to follow. Core arguments seem to fall adjacent to the verb. For more about core arguments, see my video on the Bourbon. It's arguments. I'll leave a link below before we solve this flat, this problem in our our tree. Let me just take a quick sidetrack into the issue of linguistic evidence.

Speaker 2:          04:47          Ideally, any grammar we're proposing, especially any grammar that we claim models how humans produce language or at least how computers can produce human language must account for the kinds of sentences it models. Evidence includes sentences that impact the claim that our grammar works and this evidence can be positive because it's a well formed natural language sentence or nay negative because of speaker of a natural language would judge it as unfit, negative evidence like the sentence the boss ate soup at home is marked with an asterisk by convention back to our tree. Let's account for the ordering of Verb Noun phrase, prepositional phrase by prioritizing their distance from the head. Verb eight we had some intermediate nodes below the v, the verb, some v Bar nodes that allow us to expand adjacent phrases. Now we can see that the noun phrase sits closer to the verb and the propositional phrase appears at a distance from the verb.

Speaker 2:          05:46          We've modeled a complete sentence. Now let's pull back and make some general observations. Then works through some useful terminology for moving around this tree. First, notice that we have kinds of phrases. Noun, phrase for a prepositional phrase for phrase, but they all behave in the same way containing some head noun, verb or preposition and expanding out either to the left or the right. We can abstract this concept and say that all of our phrases take the form x phrase where x stands for some head word class. Since the x rays expands to some node x Bar, the whole model takes name x box theory. What about our top node s while there's a mismatch between all of our expanding x phrase nodes here which have the form of some x and P and this top branching node which isn't named some x phrase, think of of more appropriate name like inflectional phrase.

Speaker 2:          06:41          Of course it follows that the head of this inflectional phrases some eye, the inflection may be pulled out as tense in agreement information because it's a finite sentence. We can even expand beyond this individual sentence here to a CP compliment as a phrase with somewhere that that connects it to even larger chunks of language. At this point we we've made a parse tree that we can walk starting at the top node. This node dominates all the nodes below it. The nodes below each dominate the nodes below them. In all cases, notice that each note in our tree dominates at most two other nodes. We've followed the binary branching hypothesis which maintains this kind of structure to give our sentences depth, flexibility and according to proponents grammar, grammatical accuracy nodes falling to the left of other nodes on the same level, proceed nodes to the right.

Speaker 2:          07:36          This precedence differs from dominance nodes immediately dominated by a node. See Command. Every node dominated by that immediate dominating node. So here V-bar, see commands prepositional phrase. We continue walking all of these branches of this tree to all of these branching nodes until we get down to the bottom nodes, the terminal symbols which contain the actual words and morphemes of the sentence. There's a shorthand way to capture this whole grammar in a collection of statements called rewrite rules. We've seen that a noun phrase dominates and end bar and an end that a verb praise expands to the bar and V or another phrase like a noun phrase or prepositional phrase. So we can take all these together and say that an ex phrase where x is your head word can be rewritten as some why phrase some other phrase or an x bar. Then an x bar can be rewritten as an x or a wife prays and that in any x, once you get to an ax, you're headed, you've arrived at a terminal symbol.

Speaker 2:          08:44          This takes us from the top to the bottom of a tree. Parse trees may help computers and people resolve the kinds of ambiguities that thrive in natural language. You're are already aware that sentences can be ambiguous. Somebody has certainly told you something that could be interpreted in different ways. Ambiguity has two general scopes. A sentence can have global ambiguity or local ambiguity. Global ambiguity impacts the whole sentence. As in the canonical example, time flies like an arrow. Local ambiguity is limited to one or more pieces of a sentence. Besides scope. There are also different types of ambiguity. Structural ambiguity occurs when more than one parse tree can be used to represent the sentence, so different grammatical structures are yielding different interpretations of the sentence. Consider again the sentence, time flies like an arrow. If you haven't evaluated that phrase before, think about it. Now you can understand the sentence in multiple ways, including with time as the verb flies as the verb or like is the verb.

Speaker 2:          09:48          When one of the leaves of a parse tree, the words can be understood in multiple ways. You have an example of word sense ambiguity. The meaning of the word cards in she has cards in her pocket is ambiguous. We know what's a noun, but does it mean credit cards or playing cards when it's unclear what a pronoun refers back to? That's a case of referential ambiguity in John told Jake that he has to come to the party. We know that he is a pronoun, but does it refer to John or to Jake? All right. It's time to conclude this rough survey of parse trees. The topic of Grammars is deep, contentious and central to questions about the structure of individual languages, the human brain's ability to acquire language, computer processing of language and artificial intelligence. There are many linguistic models of grammar, including other rule based approaches, statistical approaches based on large amounts of natural language data and approaches that see language as the outcome of competing ranked viable constraints instead of inviolable row rules. But those are all topics for another day. It's been fun to work through a simplified model with you and thanks for learning with me.