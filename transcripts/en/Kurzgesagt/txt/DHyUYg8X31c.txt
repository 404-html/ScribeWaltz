Speaker 1:          00:02          Imagine a future where your toaster anticipates what kind of toast you want during the day. It scans the Internet for new and exciting types of toast. Maybe it asks you about your day and wants to chat about new achievements in toast technology. At what level would it become a person? At which point will you ask yourself if your toaster has feelings, if it did, would unplug it, be murdered and would you still own it? Will we someday be forced to give our machines rights? Ai is already all around you. It makes sure discounters are stocked with enough snacks. It serves you up just the right Internet ad and you may have even read a new story written entirely by a machine. Right now we look at chat bots like Siri and laugh at their primitive simulation emotions, but it's likely that we will have to deal with beings that make it hard to draw the line between real and simulated humanity.

Speaker 1:          01:08          Are there any machines in existence that deserve rights? Most likely not yet, but if they come we are not prepared for it. Most of the philosophy of rights is ill equipped to deal with the case of artificial intelligence. Most claims for right with a human or animal are centered around the question of consciousness. Unfortunately, nobody knows what consciousness is something that it's immaterial. Others say it's a state of matter like gas or liquid. Regardless of the precise definition, we have an intuitive knowledge of consciousness because we experience it. We are aware of ourselves and our surroundings and know what unconsciousness feels like. Some neuroscientists believe that any sufficiently advanced system can generate consciousness, so if your toast is hardware was powerful enough, it may become self aware. If it does what it deserve. Rights, well, not so fast would what we define as rights make sense to as consciousness entitles beings to have rights because it gives a being the ability to suffer.

Speaker 1:          02:17          It means the ability to not only feel pain but to be aware of it. Robots don't suffer and they probably won't unless we programmed them to without pain or pleasure. There's no preference and rights are meaningless. Our human rights are deeply tied to our own programming. For example, we dislike pain because our brains evolved to keep us alive, to stop us from touching a hot fire. Automakers runaway from predators, so we came up with a rights that protect us from infringements that causes pain. Even more abstract rights like freedom are rooted in the way our brains are wired to detect what is fair and unfair. What a toaster that is unable to move mind being locked in a cage. Would it mind being dismantled if it had no fear of death? Would it mind being insulted if it had no need for self esteem? But what if we programmed a robot to feel pain and emotions to prefer justice over injustice, pleasure over pain, and be aware of it?

Speaker 1:          03:22          Would that make them sufficiently human? Many technologies, it's belief that an explosion in technology will occur when artificial intelligence can learn and create their own. Artificial intelligence is even smarter than themselves at this point. The question of how robots are programmed will be largely out of our control water. If an artificial intelligence found it necessary to program the ability to feel pain just as evolutionary biology found it necessary. In most loving creatures. Do. Robots deserve those rights, but maybe we should be less worried about the risk that's superintelligent robots post to us and more worried about the danger we posed to them. Our whole human identity is based on the idea of human exceptionalism, that we are special, unique snowflakes in titled to dominate the natural world. Humans have a history of denying that other beings are capable of suffering as they do. In the midst of the scientific revolution, many to cart argued that animals were mayor.

Speaker 1:          04:22          Automata robots, if you will, as such, injuring a rabbit was about as morally repugnant as punching a stuffed animal and many of the greatest crimes against humanity. We're justified by their perpetrators on the grounds that the victims were more animal than civilized human. Even more problematic is that we have an economic interest in denying robot rights. If we can coerce a sentient AI possibly through programs torture into doing as we please the economic potential is unlimited. We've done it before or after all violence has been used to force our fellow humans into working and we've never had trouble coming up with an ideological justifications. Slave owners argued that slavery benefited the slaves. They put a roof over their head and taught them. Christianity. Men who were against women voting argued that it was in women's own interest to leave the hard decisions to men. Farmers argued that looking after animals and feeding them justifies that early death for our dietary preferences. If robots become sentient, there will be no shortage of arguments for those who say that they should remain without rights, especially from those who stand to profit from it. Artificial intelligence raises serious questions about philosophical boundaries. While we may ask if sent in, robots aren't conscious or deserving of rights, it forces us to pose basic questions like what makes us human, what makes us deserving of rights, regardless of what we think the question need to be resolved in the near future. What are we going to do with robots? Start demanding bear own rights.

Speaker 1:          06:07          What can robots to mining rights teach us about ourselves? Our friends at wisecrack made a video exploring this very question, using the philosophy of Westworld wisecrack dissects pop culture and a unique and philosophical way. Click here to check out their video and subscribe to their channel.