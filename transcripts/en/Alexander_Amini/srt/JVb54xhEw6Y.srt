1
00:00:03,030 --> 00:00:03,863
Hey,
thanks very much for the invitation to 

2
00:00:05,790 --> 00:00:07,770
speak to you.
Um,

3
00:00:08,010 --> 00:00:08,843
yeah,
so I'm going to be talking about deep 

4
00:00:09,720 --> 00:00:11,550
generative models.
Um,

5
00:00:11,580 --> 00:00:13,980
so when we talk about deep generative 
models,

6
00:00:13,981 --> 00:00:14,814
what we're really talking about here 
from my point of view is to essentially 

7
00:00:17,941 --> 00:00:20,140
train neural nets,
uh,

8
00:00:20,190 --> 00:00:24,120
from training examples in order to 
represent,

9
00:00:24,150 --> 00:00:26,460
uh,
the distribution from which these came.

10
00:00:26,820 --> 00:00:27,653
So we can think about this as either 
explicitly doing density estimation 

11
00:00:30,571 --> 00:00:31,404
where we have some samples here and we 
try to model those samples with some 

12
00:00:34,111 --> 00:00:38,700
density estimation.
Or we can think of it more like a this,

13
00:00:38,701 --> 00:00:41,850
which is what actually I'll be doing a 
lot more of this kind of thing,

14
00:00:41,851 --> 00:00:44,880
which is to essentially we're worried 
about sample generation here.

15
00:00:44,881 --> 00:00:45,714
So we have some training examples like 
this where we're sort of just natural 

16
00:00:49,801 --> 00:00:53,430
images from the world and we're asked 
the asking a model to train,

17
00:00:53,431 --> 00:00:56,370
to learn to output images like this.

18
00:00:56,370 --> 00:00:57,270
Now,
uh,

19
00:00:57,570 --> 00:00:58,403
these are actually not true samples.
These are actually just other images 

20
00:01:02,431 --> 00:01:06,090
from the same trainings.
I believe this is from image net.

21
00:01:06,330 --> 00:01:07,163
Uh,
a few years ago or even a few months 

22
00:01:09,180 --> 00:01:10,013
ago,
this would have seemed obvious that 

23
00:01:11,011 --> 00:01:13,260
this,
you couldn't generate samples like this,

24
00:01:13,261 --> 00:01:14,094
but in fact nowadays,
this is actually not so obvious that we 

25
00:01:16,471 --> 00:01:17,304
couldn't generate these.
So it's been a very exciting time in 

26
00:01:19,441 --> 00:01:20,274
this area,
in the amount of work we've done in the 

27
00:01:22,110 --> 00:01:22,943
amount of progress we've made in the 
last few years has been pretty 

28
00:01:24,991 --> 00:01:27,630
remarkable.
And so I think part of what I want to do

29
00:01:27,631 --> 00:01:29,760
here is tell you a little bit about that
progress.

30
00:01:29,761 --> 00:01:30,594
Give you some sense of where we were in 
say 2014 when this started really to 

31
00:01:35,610 --> 00:01:37,830
accelerate and,
and where we are now.

32
00:01:38,670 --> 00:01:39,690
So,
yeah.

33
00:01:39,691 --> 00:01:41,310
So,
so why generative models?

34
00:01:41,311 --> 00:01:43,140
Why do we care about gender to modeling?
Well,

35
00:01:43,141 --> 00:01:44,370
there's a,
there's a bunch of reasons.

36
00:01:44,490 --> 00:01:46,770
Some of us are just really interested in
making pretty pictures.

37
00:01:46,771 --> 00:01:49,950
And I confess that for the most part,
that's what I'll be showing you today is

38
00:01:49,951 --> 00:01:52,410
just as an evaluation metric,
uh,

39
00:01:52,650 --> 00:01:53,760
we'll,
we'll just be looking at,

40
00:01:53,761 --> 00:01:54,594
at pictures,
I'm just natural images and how well 

41
00:01:56,581 --> 00:01:57,414
we're doing in natural images.
But there's actually real tasks that we 

42
00:01:59,071 --> 00:02:00,660
care about when we talk about gender to 
modeling.

43
00:02:00,840 --> 00:02:01,673
One of them is just,
let's say you want to do some 

44
00:02:03,181 --> 00:02:05,430
conditional generation,
like for example,

45
00:02:05,790 --> 00:02:06,960
machine translation,
right?

46
00:02:06,961 --> 00:02:10,740
So we're conditioning on some source 
sentence and we want to output some,

47
00:02:10,760 --> 00:02:11,970
uh,
target sentence.

48
00:02:12,060 --> 00:02:16,140
Will the structure within that target 
sentence that the target language,

49
00:02:16,141 --> 00:02:18,000
let's say that the,
the rules,

50
00:02:18,030 --> 00:02:18,863
the grammatical rules,
you can model that structure using a 

51
00:02:21,271 --> 00:02:24,000
generative model.
So this is an instance of where we would

52
00:02:24,001 --> 00:02:27,480
do conditional generative modeling.
Another example,

53
00:02:27,481 --> 00:02:28,261
uh,
where,

54
00:02:28,261 --> 00:02:29,094
uh,
this is something that we're actually 

55
00:02:29,431 --> 00:02:30,264
looking a little bit towards is,
is can we use generative models is 

56
00:02:33,171 --> 00:02:34,010
outlier detection.

57
00:02:34,080 --> 00:02:34,913
And actually recently these types of 
models have been integrated into rl 

58
00:02:38,280 --> 00:02:41,010
algorithms to help them do exploration 
more effectively.

59
00:02:41,280 --> 00:02:43,020
This was duct work done,
um,

60
00:02:43,080 --> 00:02:43,950
at deep mind,
I believe.

61
00:02:44,640 --> 00:02:45,510
Uh,
so here we're,

62
00:02:45,680 --> 00:02:46,920
we're,
we're looking at,

63
00:02:46,921 --> 00:02:47,820
you know,
a case where,

64
00:02:48,500 --> 00:02:50,070
you know,
if you can think about kind of a,

65
00:02:50,071 --> 00:02:53,370
this is a toy toyish version of the 
autonomous vehicle,

66
00:02:53,490 --> 00:02:54,323
the task,
and you want to be able to distinguish 

67
00:02:55,710 --> 00:02:56,543
cars and wheelchairs and then you're 
going to have something like this and 

68
00:02:58,951 --> 00:03:01,690
you don't want your classifier to just 
blindly say,

69
00:03:01,691 --> 00:03:02,524
oh,
I think it's either a car or a 

70
00:03:03,641 --> 00:03:04,474
wheelchair.
You want your classifier to understand 

71
00:03:06,521 --> 00:03:08,350
that this is an outlier,
right?

72
00:03:08,351 --> 00:03:09,184
And you can use generative modeling to 
be able to do that by noticing that 

73
00:03:12,521 --> 00:03:16,570
there aren't very many things like this 
example from the training set.

74
00:03:16,840 --> 00:03:18,430
So you can proceed with caution.

75
00:03:18,790 --> 00:03:20,330
And this is kind of a big deal,
right?

76
00:03:20,331 --> 00:03:21,880
Because we don't want,
our classifiers are,

77
00:03:22,090 --> 00:03:24,520
our neural net classifiers are very,
very capable of,

78
00:03:24,521 --> 00:03:26,650
of doing excellent performance 
classification.

79
00:03:26,651 --> 00:03:27,484
But,
but any classifiers just trained to 

80
00:03:28,631 --> 00:03:31,450
output one of the classes that it's been
given.

81
00:03:31,780 --> 00:03:32,830
And so,
uh,

82
00:03:32,831 --> 00:03:37,270
in cases where we actually are faced 
with something really new and that's not

83
00:03:37,271 --> 00:03:38,104
seen before or perhaps a illumination 
conditions that it's never been trained 

84
00:03:41,171 --> 00:03:43,300
to,
to cope with.

85
00:03:43,720 --> 00:03:46,570
We want models that are conservative in 
those cases.

86
00:03:47,260 --> 00:03:49,300
So we hope to be able to use generative 
models for that.

87
00:03:49,570 --> 00:03:50,403
Another case where we're looking at 
generative models being useful is in 

88
00:03:53,531 --> 00:03:56,070
going from simulation to real,
uh,

89
00:03:56,080 --> 00:03:58,150
examples of in robotics.
So,

90
00:03:58,360 --> 00:03:59,193
so in robotics training these robots 
with neural nets is actually quite 

91
00:04:02,200 --> 00:04:03,033
laborious.
If you're really trying to do this on 

92
00:04:04,991 --> 00:04:06,160
the real robot,
it would take many,

93
00:04:06,161 --> 00:04:09,670
many trials and it's,
it's not really practical in simulation.

94
00:04:09,670 --> 00:04:10,690
This works much,
much better.

95
00:04:10,691 --> 00:04:11,524
But the problem is,
is that if you train a policy and 

96
00:04:13,031 --> 00:04:15,820
simulation and transfer it to the real 
robot,

97
00:04:16,030 --> 00:04:18,820
well that's not going to work well.
That hasn't worked very well because the

98
00:04:18,821 --> 00:04:19,654
environment is just too different.
But what if we could use a generative 

99
00:04:21,551 --> 00:04:25,600
model to make our simulations so 
realistic that that transfer is viable?

100
00:04:26,170 --> 00:04:28,420
So this is the one,
another area that a number of groups are

101
00:04:28,421 --> 00:04:31,870
looking at this kind of x pushing gender
modeling in this direction.

102
00:04:32,020 --> 00:04:32,853
So there's lots of really practical ways
to use generate models beyond just 

103
00:04:35,831 --> 00:04:37,900
looking at pretty pictures.
Um,

104
00:04:38,530 --> 00:04:39,130
right.
So,

105
00:04:39,130 --> 00:04:39,963
so I,
I break down the kinds of generative 

106
00:04:41,561 --> 00:04:45,340
models there are in the world and to two
rough categories here.

107
00:04:45,341 --> 00:04:47,170
And maybe we can take issue with this.
Oh,

108
00:04:47,171 --> 00:04:49,060
by the way,
if you guys have questions,

109
00:04:49,420 --> 00:04:51,670
go ahead and ahead and ask me a while.

110
00:04:51,670 --> 00:04:52,670
You have them.
I think I,

111
00:04:52,671 --> 00:04:55,510
I like interaction as possible or you 
can just save them to the end.

112
00:04:55,810 --> 00:04:57,550
Either way is fine.
Um,

113
00:04:58,000 --> 00:05:00,550
and sorry for my voice,
I've got a cold.

114
00:05:00,910 --> 00:05:01,481
Uh,
so yeah,

115
00:05:01,481 --> 00:05:05,170
we have auto regressive models and we 
have latent variable models.

116
00:05:05,890 --> 00:05:06,723
So auto regressive models are models 
where you basically define an ordering 

117
00:05:11,201 --> 00:05:12,910
over your input,
right?

118
00:05:12,911 --> 00:05:17,860
So for things like speech recognition or
strather speech synthesis and the gender

119
00:05:17,861 --> 00:05:19,720
and modeling case,
this is natural,

120
00:05:19,721 --> 00:05:20,321
right?
It's just,

121
00:05:20,321 --> 00:05:21,154
there's a natural ordering to that data.
It's just the temporal sequence for 

122
00:05:24,701 --> 00:05:26,980
things like images.
It's a little less obvious how you would

123
00:05:26,981 --> 00:05:30,820
define an ordering over pixels.
But there are nonetheless models such as

124
00:05:30,821 --> 00:05:33,820
a pixel rnn and Pixel cnn that do just 
this.

125
00:05:33,970 --> 00:05:35,440
In fact,
pixel CNN is a,

126
00:05:35,680 --> 00:05:38,230
is a really interesting model from this 
point of view.

127
00:05:38,231 --> 00:05:41,990
They basically define a convolutional 
neural net with a mask.

128
00:05:42,010 --> 00:05:44,380
So if you remember our previous lecture 
we did,

129
00:05:44,381 --> 00:05:48,490
we saw these convolutional neural nets,
but what they do is they stick a mask on

130
00:05:48,491 --> 00:05:50,890
it so that you've got sort of a causal 
direction.

131
00:05:50,891 --> 00:05:51,724
So you're only looking at previous 
pixels in the ordering that you've 

132
00:05:55,031 --> 00:05:56,470
defined the,
the,

133
00:05:56,471 --> 00:05:57,304
the,
um,

134
00:05:57,620 --> 00:05:59,180
the,
well,

135
00:05:59,210 --> 00:06:02,150
the ordering that defined by the,
the auto regressive model.

136
00:06:02,390 --> 00:06:04,610
So,
so you kind of maintain this,

137
00:06:04,820 --> 00:06:08,900
this ordering as you go through the 
confident and what that allows you to do

138
00:06:08,901 --> 00:06:11,980
is come up with a generative model 
that's supported by this continent,

139
00:06:12,280 --> 00:06:12,730
uh,
that,

140
00:06:12,730 --> 00:06:13,563
you know,
it's just a full generative model and 

141
00:06:14,811 --> 00:06:15,644
it's a,
it's a pretty interesting model in its 

142
00:06:16,491 --> 00:06:17,324
own right.
But because of I have rather limited 

143
00:06:19,251 --> 00:06:20,084
time,
I'm actually not going to go into that 

144
00:06:21,771 --> 00:06:22,604
model in particular.
Another thing I just want to point out 

145
00:06:24,021 --> 00:06:24,854
here is that wave net is probably the 
state of the art model for speech 

146
00:06:28,731 --> 00:06:29,870
synthesis right now.

147
00:06:29,870 --> 00:06:32,990
And it forms the basis of a very 
interesting speech synthesis systems.

148
00:06:33,140 --> 00:06:33,973
It's another area where gender models 
have made remarkable contributions in 

149
00:06:36,471 --> 00:06:38,720
the last few years,
even the last few months.

150
00:06:38,960 --> 00:06:39,793
So now we're seeing models that you 
would be fairly hard pressed to 

151
00:06:42,291 --> 00:06:43,124
distinguish between natural speech and 
and these kinds of models for the most 

152
00:06:47,241 --> 00:06:49,760
part.
So what I'm going to concentrate on is a

153
00:06:49,761 --> 00:06:50,594
latent variable models.
So latent variable models are models 

154
00:06:54,140 --> 00:06:58,910
that essentially posit that you have 
some latent variables that are hope that

155
00:06:58,911 --> 00:07:03,560
you hope will represent some latent 
factors of variation in the data,

156
00:07:04,010 --> 00:07:04,850
right?
So these are,

157
00:07:05,030 --> 00:07:06,800
these are things that as you wiggle 
them,

158
00:07:06,830 --> 00:07:10,610
they're going to move the data in,
in what you hope will be natural ways.

159
00:07:10,640 --> 00:07:12,500
Right?
So you can imagine a latent variable for

160
00:07:12,501 --> 00:07:16,670
images corresponding to illumination 
conditions,

161
00:07:16,671 --> 00:07:18,980
right?
Or if their faces a common,

162
00:07:19,010 --> 00:07:19,843
it's a common thing.
We find is latent variable corresponded 

163
00:07:21,021 --> 00:07:21,950
to a smart,
right?

164
00:07:21,950 --> 00:07:26,390
So if we move this latent variable,
the image that we see that we generate,

165
00:07:26,420 --> 00:07:29,330
you know,
a smile appears and disappears and these

166
00:07:29,331 --> 00:07:33,550
are the kind of latent variables that we
want and we want to discover these.

167
00:07:33,551 --> 00:07:36,380
So this is really a challenging task in 
general.

168
00:07:36,381 --> 00:07:37,214
We want to take natural data,
just unlabeled data and discover these 

169
00:07:40,221 --> 00:07:43,010
latent factors that give rise to the 
variation.

170
00:07:43,011 --> 00:07:47,180
We see there's two kinds of models in 
this family that I'm going to be talking

171
00:07:47,181 --> 00:07:48,014
about.
A adversarial auto encoders and 

172
00:07:49,671 --> 00:07:51,920
generative adversarial nets are gans 
here.

173
00:07:52,920 --> 00:07:53,753
Uh,
I work personally with both of these 

174
00:07:54,831 --> 00:07:58,370
kinds of models.
They serve different purposes for me.

175
00:07:58,371 --> 00:07:59,204
And,
uh,

176
00:07:59,450 --> 00:08:00,430
yeah,
well let's,

177
00:08:00,540 --> 00:08:03,620
let's dive in a,
so first I'll talk about,

178
00:08:03,700 --> 00:08:05,910
uh,
the variation all encoders.

179
00:08:05,990 --> 00:08:09,680
This actually was a model was developed 
simultaneously by two different groups.

180
00:08:10,100 --> 00:08:15,100
One at a deep mind is the bottom one 
here and then a king man dwelling at the

181
00:08:15,620 --> 00:08:16,453
University of Amsterdam.

182
00:08:16,670 --> 00:08:18,620
So again,
the idea behind the,

183
00:08:18,621 --> 00:08:19,550
uh,
the,

184
00:08:19,580 --> 00:08:21,320
uh,
the,

185
00:08:22,040 --> 00:08:22,873
the well latent variable models in 
general is kind of represented in this 

186
00:08:26,271 --> 00:08:26,901
picture,
right?

187
00:08:26,901 --> 00:08:27,734
So here's the space of our latent 
variables and we consist this kind of 

188
00:08:30,081 --> 00:08:30,914
represented as being fairly simple.
And we have our two coordinate zed one 

189
00:08:34,071 --> 00:08:37,400
and zed too.
And they're independent in this case and

190
00:08:37,401 --> 00:08:38,234
they're sort of fairly regular and they 
sort of form a chart for what is our 

191
00:08:41,901 --> 00:08:44,270
complicated distribution here in,
in,

192
00:08:44,271 --> 00:08:45,560
in x space,
right?

193
00:08:45,561 --> 00:08:46,394
So this is,
you can think of this as third of the 

194
00:08:47,121 --> 00:08:47,954
data manifold.
So you can think of this as image space 

195
00:08:49,581 --> 00:08:50,331
for example,
right?

196
00:08:50,331 --> 00:08:53,180
So image space embedded in pixel space,
natural images,

197
00:08:53,200 --> 00:08:55,680
embedded pixel space form,
this kind of manifold.

198
00:08:55,890 --> 00:08:58,650
And what we want is coordinates that 
allow you to,

199
00:08:58,651 --> 00:09:01,050
as you move sort of smoothly in this 
space,

200
00:09:01,170 --> 00:09:03,920
move along this what can be a very 
complicated manifold.

201
00:09:04,360 --> 00:09:05,990
And so that's the kind of hope.
What that,

202
00:09:06,000 --> 00:09:07,950
what,
what we're looking for when we do latent

203
00:09:07,951 --> 00:09:11,240
variable modeling.
So here's just an example of,

204
00:09:11,260 --> 00:09:12,093
of what I mean by exactly that.
This is an early example using these 

205
00:09:14,671 --> 00:09:17,810
various channel auto encoders.
So here's a,

206
00:09:17,820 --> 00:09:18,653
the fray face data set,
just a whole bunch of images of Brendan 

207
00:09:21,121 --> 00:09:22,320
phrase face.
Uh,

208
00:09:22,321 --> 00:09:26,640
that's in the Dataset.
What we're showing here is the model,

209
00:09:26,670 --> 00:09:28,860
the,
the model output,

210
00:09:29,560 --> 00:09:30,393
uh,
of this variational auto encoder for 

211
00:09:32,401 --> 00:09:33,234
different values of these latent 
variable zed one in zed to now we've 

212
00:09:36,811 --> 00:09:37,644
kind of post hoc added these labels,
pose and expression on them because you 

213
00:09:41,431 --> 00:09:42,264
see if as we move the zed to here,
you can see the expression kind of 

214
00:09:44,761 --> 00:09:45,594
smoothly changes from what looks like a 
frown to eventually smile and through 

215
00:09:49,651 --> 00:09:53,580
what looks over here like a stinging as 
well.

216
00:09:53,650 --> 00:09:55,140
Sticking his tongue out,
I guess.

217
00:09:55,470 --> 00:09:56,303
Uh,
and in this direction there's a slight 

218
00:09:58,200 --> 00:10:00,300
head turn.
It's pretty subtle,

219
00:10:00,301 --> 00:10:01,920
but it's there.
So,

220
00:10:02,310 --> 00:10:03,960
so we did,
like I said,

221
00:10:03,961 --> 00:10:04,794
these were sort of post hoc,
added the model just discovered that 

222
00:10:06,931 --> 00:10:09,330
these were two sources of variation in 
the data,

223
00:10:09,331 --> 00:10:10,164
that we're relatively independent.
And the model just pulled those out for 

224
00:10:14,311 --> 00:10:15,870
something like amnesty.
This is the,

225
00:10:15,871 --> 00:10:18,720
these are samples drawn on a model 
trained by ethnicity.

226
00:10:18,810 --> 00:10:20,100
It's a little less natural,
right?

227
00:10:20,101 --> 00:10:22,140
Because in this case,
you could,

228
00:10:22,500 --> 00:10:23,333
you could argue the data.
It's really best model as something not 

229
00:10:25,171 --> 00:10:27,570
with Leighton factors like continuously 
and factors,

230
00:10:27,571 --> 00:10:28,404
but more like in clusters.
So you get this kind of somewhat 

231
00:10:30,871 --> 00:10:31,704
interesting,
somewhat bizarre relationship where you 

232
00:10:33,841 --> 00:10:34,674
know,
you've got some of this relationship 

233
00:10:35,551 --> 00:10:38,220
with tilt happens here,
but then the ones heard of morphs into a

234
00:10:38,221 --> 00:10:39,990
seven,
which morphs into a nine.

235
00:10:40,220 --> 00:10:41,053
And you know,
because these different regions in this 

236
00:10:43,261 --> 00:10:46,620
continuous space that represent 
different examples here.

237
00:10:49,560 --> 00:10:50,393
So a little bit more detail into how we 
do these kinds of a latent variable 

238
00:10:53,521 --> 00:10:54,354
models,
at least in the context of the 

239
00:10:55,081 --> 00:10:57,960
variational auto encoder or va model.
Um,

240
00:10:58,170 --> 00:10:59,003
so what we're trying to learn here is,
is p of x some distribution over the 

241
00:11:02,521 --> 00:11:03,354
data.
We're trying to maximize the likelihood 

242
00:11:04,471 --> 00:11:05,940
of the data.
That's it.

243
00:11:06,150 --> 00:11:11,150
But the way we're going to parameterize 
our model is with a p of x given zed zed

244
00:11:11,411 --> 00:11:13,350
or is our latent variables,
I'm sorry to Z,

245
00:11:13,470 --> 00:11:15,120
I guess for you guys.
Uh,

246
00:11:15,150 --> 00:11:17,940
p of x given Z and then P of Z,
right?

247
00:11:17,941 --> 00:11:20,070
Some,
some prior distribution.

248
00:11:20,100 --> 00:11:23,430
Right?
So this P of z here is,

249
00:11:23,580 --> 00:11:25,800
is typically something simple,
right?

250
00:11:25,801 --> 00:11:26,634
It's some prior distribution.
We actually generally want it to be 

251
00:11:29,071 --> 00:11:29,904
independent.
There's some modeling compromises to be 

252
00:11:31,501 --> 00:11:32,334
made there.
But the reason why you'd want it 

253
00:11:33,061 --> 00:11:33,894
independent is because that helps get 
you the kind of orthogonal 

254
00:11:36,991 --> 00:11:37,890
representation here.

255
00:11:37,890 --> 00:11:40,470
So these guys,
this dimension and this dimension,

256
00:11:40,471 --> 00:11:41,304
we want sort of not very much 
interaction in order to make them more 

257
00:11:43,291 --> 00:11:46,080
interpretable.
Um,

258
00:11:46,140 --> 00:11:46,651
yeah.
And so,

259
00:11:46,651 --> 00:11:50,460
and the other thing we want to do is we 
want to think about how are we going to,

260
00:11:50,700 --> 00:11:52,470
so,
so going from something simple,

261
00:11:52,471 --> 00:11:53,304
like you can think about this as like in
a Gaussian distribution or a uniform 

262
00:11:55,541 --> 00:11:56,374
distribution,
but now we want a model g here that 

263
00:11:58,951 --> 00:12:02,950
transforms zed from this space into this
complicated space.

264
00:12:03,250 --> 00:12:05,230
And the way we're going to do that is 
with a neural net.

265
00:12:05,440 --> 00:12:07,390
Actually,
in all of the examples that I'm going to

266
00:12:07,391 --> 00:12:08,920
show you today,
the way we're going to do that is with a

267
00:12:08,921 --> 00:12:11,680
convolutional neural net.
And the way to think about that,

268
00:12:11,860 --> 00:12:12,693
it's a bit of an interesting thing to 
think about going from some fully 

269
00:12:14,771 --> 00:12:15,604
connected things that into some two 
dimensional input with a typology here 

270
00:12:21,860 --> 00:12:24,160
in the natural image space x.

271
00:12:24,370 --> 00:12:27,070
And so it's just the way going from a,
what we talked about,

272
00:12:27,071 --> 00:12:27,904
it's kind of like the opposite path of 
where you would take to do a confidante 

273
00:12:30,731 --> 00:12:32,310
classification.
Um,

274
00:12:32,590 --> 00:12:34,510
there's a few different ways you could 
think about doing that.

275
00:12:34,660 --> 00:12:36,940
One of which is called a transposed 
convolution.

276
00:12:36,941 --> 00:12:39,280
This turns out to not to be such a good 
idea.

277
00:12:39,490 --> 00:12:40,323
Uh,
this is a case where you essentially 

278
00:12:41,230 --> 00:12:43,220
fill in a bunch of Zeros.
Uh,

279
00:12:43,270 --> 00:12:46,660
it seems like the most acceptable way to
do that right now is to just,

280
00:12:46,870 --> 00:12:49,480
once you get some small level typology 
here,

281
00:12:49,481 --> 00:12:51,640
you just,
you do interpolation.

282
00:12:51,850 --> 00:12:53,790
So you just,
you know,

283
00:12:53,890 --> 00:12:55,630
super sample from the image.
Uh,

284
00:12:55,690 --> 00:13:00,280
you can do by linear interpolation and 
then do a conflict preserves that size.

285
00:13:00,281 --> 00:13:02,320
And then up sample again,
comp up,

286
00:13:02,321 --> 00:13:04,990
sample comp,
that tends to give you the best results.

287
00:13:06,230 --> 00:13:06,581
So,
right?

288
00:13:06,581 --> 00:13:09,850
So when you see this kind of thing,
this kind of thing for our purposes,

289
00:13:09,851 --> 00:13:11,860
think convolutional neural net.

290
00:13:13,070 --> 00:13:15,750
All right?
So it's important to point out that that

291
00:13:15,760 --> 00:13:17,770
if we had,
uh,

292
00:13:17,980 --> 00:13:20,430
if we had over here,
uh,

293
00:13:20,470 --> 00:13:21,303
ceds that go went,
where's this [inaudible] that went with 

294
00:13:22,991 --> 00:13:24,130
our ex,
uh,

295
00:13:24,170 --> 00:13:25,003
we'd be done right?
Because this is just a supervised 

296
00:13:26,111 --> 00:13:28,210
learning problem at this point.
And we'd be fine.

297
00:13:28,480 --> 00:13:29,880
The trick is these are latent,
right?

298
00:13:30,000 --> 00:13:31,840
Need their hidden.
We don't know these sets.

299
00:13:31,841 --> 00:13:33,400
We don't have,
we haven't discovered them yet.

300
00:13:33,880 --> 00:13:37,000
So how do we learn with this model?
Well,

301
00:13:37,030 --> 00:13:37,863
the way we're going to do it is we're 
going to use a trick that's actually 

302
00:13:39,851 --> 00:13:41,570
been around for quite some time.
Uh,

303
00:13:41,571 --> 00:13:42,910
it says,
this isn't particularly new.

304
00:13:42,911 --> 00:13:46,480
We're going to use a variation lower 
bound on the data likelihood,

305
00:13:47,080 --> 00:13:47,913
right?
So it turns out that we can actually 

306
00:13:48,551 --> 00:13:49,980
express,
uh,

307
00:13:50,000 --> 00:13:51,700
the,
the data likelihood heard.

308
00:13:51,730 --> 00:13:52,563
Again,
this is the thing we're trying to 

309
00:13:53,441 --> 00:13:54,274
maximize.
We can express a lower bound for it 

310
00:13:56,800 --> 00:13:57,940
given by something like that.

311
00:13:57,940 --> 00:13:58,773
So we posit that we have some cue 
distribution of zed that estimates is 

312
00:14:02,380 --> 00:14:07,380
the posterior of zed for a given X.
And we're trying to then um,

313
00:14:08,350 --> 00:14:13,120
I guess maximize this joint probability 
over x and zed minus the log cues that,

314
00:14:13,121 --> 00:14:15,820
so this is,
this is this variation of lower bound.

315
00:14:16,380 --> 00:14:17,213
Um,
one of the ways we can express this is 

316
00:14:20,111 --> 00:14:23,080
you're trying to find the cute from 
[inaudible] point of view is if you were

317
00:14:23,081 --> 00:14:23,914
to find a cue that actually recovered 
the exact posterior distribution over 

318
00:14:29,261 --> 00:14:32,830
zed given x,
this would actually be a tight,

319
00:14:32,831 --> 00:14:33,940
lower bound,
right?

320
00:14:34,390 --> 00:14:35,223
So then we would for sure be optimizing 
if we were at all now optimize this 

321
00:14:38,351 --> 00:14:41,230
lower boundary would be for sure,
optimizing likelihood in practice.

322
00:14:41,231 --> 00:14:43,090
That's what we're going to do anyway.
We're going to have this lower bound,

323
00:14:43,280 --> 00:14:44,113
we're going to try to optimize it.
We're trying to raise that up in hopes 

324
00:14:46,271 --> 00:14:47,830
of raising up our likelihood.

325
00:14:48,130 --> 00:14:50,070
Um,
but the problem is this posterior,

326
00:14:50,090 --> 00:14:53,360
the actual posterior of,
of say of this g model here,

327
00:14:53,361 --> 00:14:54,194
this neural net,
right?

328
00:14:54,290 --> 00:14:56,450
This is just some forward model neural 
net.

329
00:14:56,451 --> 00:15:00,350
So computing the posterior of zed given 
acts and tractable.

330
00:15:00,680 --> 00:15:01,513
We have no good way of doing this.
It's going to be some complicated thing 

331
00:15:03,411 --> 00:15:05,330
and we have no sensible way of doing 
this,

332
00:15:05,450 --> 00:15:09,470
so we're going to approximate it with 
this queue in this way.

333
00:15:09,471 --> 00:15:11,150
So we're going to,
now we can actually,

334
00:15:11,151 --> 00:15:12,950
and what's interesting about this 
formulation,

335
00:15:12,951 --> 00:15:14,540
and this is this is new to the 
variation.

336
00:15:14,541 --> 00:15:15,374
A lot of queries they've started or just
reformulated this a little bit 

337
00:15:16,941 --> 00:15:20,390
differently and what they've got is they
come up with this,

338
00:15:20,450 --> 00:15:21,283
this different expression here,
which actually can be thought of in two 

339
00:15:24,321 --> 00:15:26,720
terms here.
One is the reconstruction term here.

340
00:15:26,721 --> 00:15:28,670
If you look at what this is,
this is just,

341
00:15:28,671 --> 00:15:29,504
you get some from some queue,
you get a zed and you're just trying to 

342
00:15:33,141 --> 00:15:36,290
reconstruct x from that said,
so you start with x,

343
00:15:36,320 --> 00:15:37,153
you get a zed,
and then you're trying to do a 

344
00:15:38,001 --> 00:15:39,620
reconstruction of x.

345
00:15:39,620 --> 00:15:40,453
From that said,
this is where the name variational auto 

346
00:15:42,201 --> 00:15:46,280
encoder comes from is because this 
really looks like an encoder on the side

347
00:15:46,281 --> 00:15:47,114
of queue here and a decoder here,
and you're just trying to minimize that 

348
00:15:50,271 --> 00:15:52,700
reconstruction error.
But in addition to this,

349
00:15:52,910 --> 00:15:56,750
they add this regularization term,
and this is interesting,

350
00:15:56,751 --> 00:15:57,584
right?
So this is what they're doing here is 

351
00:15:58,281 --> 00:15:59,210
they're basically saying,
well,

352
00:15:59,270 --> 00:16:03,020
we want to regularize this posterior and
there's actually new auto encoders don't

353
00:16:03,021 --> 00:16:03,771
have this,
right?

354
00:16:03,771 --> 00:16:04,604
So I'm just trying to regularize this 
posterior to try to be a little bit 

355
00:16:08,661 --> 00:16:09,494
closer to the prior here.
And it's a common mistake when people 

356
00:16:11,781 --> 00:16:12,614
learn about this to sort of think that,
oh well the goal is for these things to 

357
00:16:14,541 --> 00:16:16,250
actually match.
That would be terrible,

358
00:16:16,251 --> 00:16:18,710
right?
That means that you lose all information

359
00:16:18,711 --> 00:16:19,544
about x.
You definitely don't want these things 

360
00:16:20,331 --> 00:16:21,164
to match.

361
00:16:21,170 --> 00:16:22,003
But it does act as a regular riser,
sort of as a counterpoint to this 

362
00:16:24,981 --> 00:16:25,814
reconstruction term.
And so now we've talked a little bit 

363
00:16:29,061 --> 00:16:30,740
about this,
but what is this queue?

364
00:16:30,760 --> 00:16:32,800
Well,
for the variation of wind coated,

365
00:16:32,810 --> 00:16:34,360
the queues going to be another neural 
net.

366
00:16:34,790 --> 00:16:35,623
And in this case we can think of it.
This is just a straight confident for 

367
00:16:37,131 --> 00:16:40,310
the case of natural images.
So again,

368
00:16:40,311 --> 00:16:41,144
we've got our lower bound,
their objective that we're trying to 

369
00:16:42,801 --> 00:16:47,801
minimize and we're going to parameterize
q as this neural net confident that goes

370
00:16:48,081 --> 00:16:48,914
from x to zed and we've got now are 
generative model here are decoder that 

371
00:16:53,211 --> 00:16:57,510
goes from zed to x.
Um,

372
00:16:57,660 --> 00:16:58,493
I'm going to add a few more details to 
make this thing actually Prac in 

373
00:17:01,101 --> 00:17:01,970
practice.
I've till now,

374
00:17:01,971 --> 00:17:04,430
this is not to uh,
new,

375
00:17:04,431 --> 00:17:05,264
there's been instances of this kind of 
formalism of an encoder network and a 

376
00:17:08,140 --> 00:17:08,973
decoder network,
but what they do next is actually kind 

377
00:17:10,821 --> 00:17:11,210
of interesting.

378
00:17:11,210 --> 00:17:15,110
They notice that that if they 
parameterize this a certain way,

379
00:17:15,111 --> 00:17:15,944
if they say q is equal to it,
actually you can use any continuous 

380
00:17:18,741 --> 00:17:20,270
distribution here,
but they pick a,

381
00:17:20,450 --> 00:17:21,283
a normal distribution here.
So q of x is some normal distribution 

382
00:17:24,561 --> 00:17:28,880
where the parameters mew in sigma from 
the defined this normal distribution are

383
00:17:28,881 --> 00:17:33,230
defined by this encoder network.
Then they can actually encode it.

384
00:17:33,231 --> 00:17:35,420
Like this is called the reprioritization
trick,

385
00:17:35,600 --> 00:17:36,433
where they take said our random variable
here is equal to some function of the 

386
00:17:39,801 --> 00:17:40,634
input.
Mew Plus Sigma are scaling factor over 

387
00:17:42,831 --> 00:17:43,664
some noise.
And what this allows us to do now when 

388
00:17:46,821 --> 00:17:48,920
they formulate it this way,
is the one in training.

389
00:17:48,921 --> 00:17:49,754
This model,
they can actually back prop through the 

390
00:17:51,751 --> 00:17:55,710
decoder and into the encoder to train 
both models simultaneously.

391
00:17:57,480 --> 00:18:01,080
It looks a little bit like this.
So they can do for propagation,

392
00:18:01,081 --> 00:18:06,000
start with an ex forward propagate to 
zed add noise here.

393
00:18:06,060 --> 00:18:09,960
That was that epsilon.
And then for propagate here to this x Ha

394
00:18:10,050 --> 00:18:11,610
hat,
which is our reconstruction,

395
00:18:12,210 --> 00:18:15,030
compute the air between x and x hat and 
back,

396
00:18:15,031 --> 00:18:17,160
propagate that error all the way 
through.

397
00:18:19,620 --> 00:18:23,610
And that allows them to actually train 
this model very effectively in ways that

398
00:18:23,611 --> 00:18:25,140
we've never been able to train before 
this.

399
00:18:25,141 --> 00:18:28,200
This card trick came up and when you do 
that,

400
00:18:28,201 --> 00:18:31,230
this is the kind of thing that came out.
So this came out in 2014.

401
00:18:31,350 --> 00:18:32,183
These are actually really,
I promise these were really impressive 

402
00:18:33,481 --> 00:18:35,310
results in,
in 2014.

403
00:18:35,790 --> 00:18:37,500
Um,
the first time we were seeing,

404
00:18:37,501 --> 00:18:38,334
so this is not a,
this is from the FDA labeled facing the 

405
00:18:39,961 --> 00:18:42,390
wild these days we use celebrate,
um,

406
00:18:42,660 --> 00:18:44,100
and uh,
this is imaging it.

407
00:18:44,101 --> 00:18:45,600
So,
so not a whole lot there.

408
00:18:45,601 --> 00:18:47,670
Actually,
this is a small version of image net.

409
00:18:48,090 --> 00:18:48,923
Um,
but you can do things with this model 

410
00:18:51,481 --> 00:18:52,590
actually.
So for example,

411
00:18:52,591 --> 00:18:55,050
one of the things that we've done with 
this model is we actually just,

412
00:18:55,220 --> 00:18:57,180
we talked to,
I mentioned briefly this pixel,

413
00:18:57,181 --> 00:18:58,980
CNN,
we,

414
00:18:59,010 --> 00:19:03,570
we actually include this pixel cnn into 
the decoder side.

415
00:19:03,750 --> 00:19:05,400
So one of the problems,
if I just go back,

416
00:19:05,401 --> 00:19:06,234
one of the problems why we get these 
kinds of images is this model makes a 

417
00:19:08,611 --> 00:19:10,380
lot of independence assumptions,
right?

418
00:19:10,620 --> 00:19:11,453
And part of it is,
is because we want those independents 

419
00:19:12,781 --> 00:19:14,550
assumptions to make hers eds more 
interpretable,

420
00:19:14,610 --> 00:19:15,443
but they have consequences to them.
And one of the consequences is you end 

421
00:19:18,121 --> 00:19:19,540
up with kind of blurry images.
That's,

422
00:19:19,590 --> 00:19:20,423
that's part of why you end up with 
biliary images because we're making 

423
00:19:22,111 --> 00:19:25,130
these approximations in the variational,
um,

424
00:19:25,830 --> 00:19:26,663
lower bound.
And so by adding the pixel CNN that 

425
00:19:30,001 --> 00:19:33,270
allows us to encode more complexity in 
here.

426
00:19:33,271 --> 00:19:34,104
And by the way,
this is now a hierarchical version of 

427
00:19:35,491 --> 00:19:36,324
the vie using pixel CNN that Lowe's,
the law allows us to encode sort of 

428
00:19:40,291 --> 00:19:42,600
complicated distributions in zed one 
here,

429
00:19:42,800 --> 00:19:43,660
uh,
given the,

430
00:19:43,810 --> 00:19:46,650
the upper levels eds and with this kind 
of thing,

431
00:19:46,870 --> 00:19:47,703
uh,
this is the kind of images that we can 

432
00:19:48,451 --> 00:19:50,710
just synthesize using this variational.

433
00:19:50,730 --> 00:19:51,880
Uh,
we'll call this the,

434
00:19:51,881 --> 00:19:55,620
the pixel vie model.
So a bedroom,

435
00:19:55,650 --> 00:19:57,500
these are bedrooms scenes,
uh,

436
00:19:57,690 --> 00:20:01,530
so you can sort of see it's reasonably 
good clear bedroom scenes.

437
00:20:01,531 --> 00:20:03,180
And then image net,
which you're,

438
00:20:03,510 --> 00:20:04,343
that,
you know,

439
00:20:04,360 --> 00:20:07,260
you can see that it gets roughly the,
the texture's,

440
00:20:07,261 --> 00:20:09,120
right?
It's not really getting objects yet.

441
00:20:09,121 --> 00:20:09,954
Actually objects that are really tough 
to get when you model things in an 

442
00:20:12,001 --> 00:20:14,610
unconditional way.
What I mean by that is the model doesn't

443
00:20:14,611 --> 00:20:16,320
know that it's supposed to generate a 
dog,

444
00:20:16,321 --> 00:20:17,820
let's say if it was going to generate 
something.

445
00:20:17,821 --> 00:20:20,700
So it's just generating from p of x in 
general.

446
00:20:20,701 --> 00:20:23,330
And it's actually pretty challenging 
when we talk about image net.

447
00:20:24,690 --> 00:20:25,620
All right.
So,

448
00:20:25,621 --> 00:20:29,640
so that's one way we can improve the uh,
the va model.

449
00:20:29,880 --> 00:20:33,330
Another way we can improve the Vau 
models work on the encoder side.

450
00:20:33,600 --> 00:20:35,130
And that was done,
uh,

451
00:20:35,190 --> 00:20:36,670
by uh,
uh,

452
00:20:36,680 --> 00:20:40,680
a few people but culminating I think in 
the inverse auto regressive flow model.

453
00:20:40,920 --> 00:20:43,520
So this is actually a very effective way
to deal with it.

454
00:20:43,650 --> 00:20:44,483
The same kind of independence problems 
we saw that we're addressing on the 

455
00:20:46,561 --> 00:20:48,520
decoder side,
but they're addressing it on the encoder

456
00:20:48,521 --> 00:20:51,610
side.
So you can kind of see just briefly what

457
00:20:51,611 --> 00:20:53,440
this is doing.
So this is your prior distribution.

458
00:20:53,441 --> 00:20:55,960
Ideally you would like sort of the 
marginal posterior,

459
00:20:55,961 --> 00:20:56,794
which is sort of like combining all 
these things together to be as close to 

460
00:20:59,021 --> 00:21:04,021
this as possible because any sort of 
disagreement between those two is really

461
00:21:04,511 --> 00:21:07,090
a modeling error.
So it's an approximation error.

462
00:21:07,210 --> 00:21:08,043
So standard vie model is going to get 
going to learn to do something like 

463
00:21:10,961 --> 00:21:11,561
this,
which is,

464
00:21:11,561 --> 00:21:12,394
it's kind of as close as it can get to 
this while still maintaining 

465
00:21:14,351 --> 00:21:18,460
independence in the distributions.
Using this I f method,

466
00:21:18,550 --> 00:21:20,770
it's a bit of a complicated method that 
involves many,

467
00:21:20,771 --> 00:21:24,580
many iterations of transformations that 
you can actually,

468
00:21:24,720 --> 00:21:25,553
um,
compute that are actually invertible 

469
00:21:27,461 --> 00:21:27,820
that.

470
00:21:27,820 --> 00:21:29,770
And you need this to be able to do the 
computation.

471
00:21:29,950 --> 00:21:31,630
But with that you can get this kind of 
thing,

472
00:21:31,631 --> 00:21:34,030
which is pretty much exactly what you'd 
want in this setting.

473
00:21:34,180 --> 00:21:35,980
So we've played around with this model 
and in fact,

474
00:21:36,250 --> 00:21:38,860
you know it,
we find it works really well in practice

475
00:21:38,861 --> 00:21:40,210
actually.
Um,

476
00:21:40,600 --> 00:21:41,070
yeah,
so,

477
00:21:41,070 --> 00:21:41,681
but it's,
again,

478
00:21:41,681 --> 00:21:42,514
it's on the encoder side.
What we were doing with the Pixel va's 

479
00:21:44,801 --> 00:21:47,290
on the is working on the decoder side.
Um,

480
00:21:47,500 --> 00:21:50,320
but now,
so this is actually fairly complicated.

481
00:21:50,321 --> 00:21:51,154
Both these models actually fairly 
complicated to use and a fairly 

482
00:21:53,531 --> 00:21:55,690
involved.
So one question is,

483
00:21:56,470 --> 00:21:59,560
is there another way to train this model
but isn't quite so complicated?

484
00:21:59,830 --> 00:22:00,460
And,
uh,

485
00:22:00,460 --> 00:22:01,420
so,
uh,

486
00:22:01,600 --> 00:22:03,830
at the time a student of Yoshua Bengio 
and I,

487
00:22:03,860 --> 00:22:04,693
uh,
Ian goodfellow was toying around with 

488
00:22:07,721 --> 00:22:11,410
this idea and he came up with a 
generative adversarial nets.

489
00:22:11,950 --> 00:22:12,783
And the way generative adversarial nets 
work is it posits the learning of a 

490
00:22:15,821 --> 00:22:18,590
generative model g,
um,

491
00:22:18,640 --> 00:22:19,473
in the form of a game.
So it's a game between this generative 

492
00:22:22,991 --> 00:22:23,824
model g here and a discriminator d.
So the discriminators job is to try to 

493
00:22:33,191 --> 00:22:38,191
tell the difference between true data 
and data generated from the generator,

494
00:22:39,340 --> 00:22:40,173
right?
So it's trying to tell the difference 

495
00:22:40,811 --> 00:22:41,644
between fake data that's generated by 
the generator and true data from the 

496
00:22:45,460 --> 00:22:49,030
trending distribution.
And it's just trained to do so.

497
00:22:49,031 --> 00:22:51,100
This guys trained to try to output one 
if,

498
00:22:51,120 --> 00:22:51,953
if it's true data and output zero.
If it's fake data and the generator is 

499
00:22:55,691 --> 00:22:56,524
being trained to fool the discriminator 
by using its own gradience against it 

500
00:23:00,131 --> 00:23:02,550
essentially.
So we back propagate the,

501
00:23:02,551 --> 00:23:03,384
the discriminate,
the discriminator air all the way 

502
00:23:05,711 --> 00:23:06,544
through through x.
We usually have to use a continuous x 

503
00:23:10,541 --> 00:23:12,490
for this and into the discriminator.

504
00:23:12,490 --> 00:23:14,180
Now we're going to change the parameters
of the,

505
00:23:14,240 --> 00:23:19,240
of the generator here in order to try to
maximally fool the discriminator.

506
00:23:20,950 --> 00:23:22,540
So in us,
sort of a more uh,

507
00:23:23,190 --> 00:23:25,180
um,
uh,

508
00:23:25,630 --> 00:23:28,090
I guess abstract way to represent this 
looks like this.

509
00:23:28,091 --> 00:23:28,924
So we have the data on this side.
We have the discriminator here with its 

510
00:23:31,481 --> 00:23:32,410
own parameters.
And this,

511
00:23:32,411 --> 00:23:33,710
again,
for our purposes,

512
00:23:33,711 --> 00:23:35,650
those almost always a convolutional 
neural net.

513
00:23:35,890 --> 00:23:36,723
And then we have the generator,
which is again one of these kind of 

514
00:23:38,380 --> 00:23:41,130
flipped convolutional models cause it 
takes noises,

515
00:23:41,200 --> 00:23:42,033
input,
it needs noise because it needs 

516
00:23:42,890 --> 00:23:47,570
variability and then it converts that 
noise into something.

517
00:23:47,571 --> 00:23:48,404
An image space that's trained with these
parameters are trained to fool the 

518
00:23:51,051 --> 00:23:51,884
discriminator.

519
00:23:55,340 --> 00:23:56,340
All right,
so you know,

520
00:23:56,341 --> 00:23:57,930
we can be a little bit more formal about
this.

521
00:23:57,931 --> 00:24:00,150
This is actually your objective function
we're training on.

522
00:24:01,200 --> 00:24:02,730
So let's just break this down for a 
second.

523
00:24:02,731 --> 00:24:04,860
So from the discriminators point of 
view,

524
00:24:04,890 --> 00:24:06,030
what is this?
This is just,

525
00:24:06,060 --> 00:24:08,760
it's called the cross entropy loss.
It's literally just what you would apply

526
00:24:08,761 --> 00:24:10,920
if you're doing a classification with 
this discriminator.

527
00:24:11,220 --> 00:24:14,190
That's all this is from the generators 
point of view,

528
00:24:14,191 --> 00:24:16,290
the generator comes in just right here,
right?

529
00:24:16,530 --> 00:24:17,363
It's the thing you draw these samples 
from and it's trying to minimize while 

530
00:24:21,870 --> 00:24:25,530
the discriminators trying to maximize 
this quantity,

531
00:24:25,710 --> 00:24:27,770
this is essentially likelihood,
uh,

532
00:24:28,020 --> 00:24:28,950
and,
and,

533
00:24:28,970 --> 00:24:31,950
and the discriminant the generators 
moving in the opposite direction,

534
00:24:33,120 --> 00:24:33,953
right?

535
00:24:35,120 --> 00:24:38,470
So we can analyze this,
this game to see,

536
00:24:38,471 --> 00:24:39,261
you know,
this is a question,

537
00:24:39,261 --> 00:24:39,930
right?
Does,

538
00:24:39,930 --> 00:24:40,580
well,
you know,

539
00:24:40,580 --> 00:24:42,640
actually the way this happened was,
um,

540
00:24:42,890 --> 00:24:47,890
at first he just tried it and it worked.
It was kind of a overnight kind of thing

541
00:24:48,021 --> 00:24:48,854
and we got some very promising results.
And then we set about trying to think 

542
00:24:52,491 --> 00:24:53,324
about,
well how do we actually explain what 

543
00:24:54,381 --> 00:24:55,790
it's doing?
Why does this work?

544
00:24:55,820 --> 00:24:56,660
And so,
uh,

545
00:24:56,661 --> 00:24:59,800
we did a little bit of theory,
which is useful to,

546
00:24:59,860 --> 00:25:00,693
to discuss.
And I can tell you there's been a lot 

547
00:25:02,181 --> 00:25:03,014
more theory on this topic that's been 
done that I will not be telling you 

548
00:25:05,121 --> 00:25:05,954
about.
But it's actually been a very 

549
00:25:06,501 --> 00:25:09,440
interesting development in the last few 
years.

550
00:25:10,520 --> 00:25:11,353
But this,
this is the theory that appeared in the 

551
00:25:12,591 --> 00:25:14,600
original paper.
So that the way we approached this was,

552
00:25:14,601 --> 00:25:17,080
let's imagine we have an optimal 
discriminator.

553
00:25:17,100 --> 00:25:18,800
This turns out you can eat pretty 
easily.

554
00:25:18,801 --> 00:25:20,930
Show this is the optimal discriminator 
up here.

555
00:25:21,110 --> 00:25:22,790
Now you couldn't,
this is not a practical thing,

556
00:25:22,791 --> 00:25:24,230
right?
Because we don't know pr,

557
00:25:24,231 --> 00:25:26,660
which is probability of the real 
distribution.

558
00:25:26,661 --> 00:25:28,310
We don't,
this is not available to us.

559
00:25:28,311 --> 00:25:31,820
This is only defined over training set.
So only by training examples.

560
00:25:31,821 --> 00:25:34,040
So we actually don't,
we can't instantiate this,

561
00:25:34,041 --> 00:25:37,880
but in theory,
if we had this optimal discriminator,

562
00:25:38,450 --> 00:25:39,283
then the generator would be trained to 
minimize the Jensen Shannon divergence 

563
00:25:43,671 --> 00:25:44,504
between the true distribution that gave 
rise to the training data and are 

564
00:25:47,511 --> 00:25:50,240
generated distribution.
So this is good,

565
00:25:50,241 --> 00:25:51,074
right?
This is telling us that we're actually 

566
00:25:51,471 --> 00:25:52,304
doing something sensible in this kind of
nonparametric ideal setting that we 

567
00:25:55,401 --> 00:25:57,500
don't,
we're not really using but,

568
00:25:57,501 --> 00:25:58,340
but it's,
um,

569
00:25:59,090 --> 00:26:01,250
but it's actually,
it's interesting nonetheless.

570
00:26:02,780 --> 00:26:05,450
Um,
so one thing I can say though,

571
00:26:05,480 --> 00:26:06,313
that in practice,
we actually don't use exactly the 

572
00:26:08,001 --> 00:26:09,830
objective function that I was just 
describing.

573
00:26:10,100 --> 00:26:12,110
What we use instead is a modified 
objective function.

574
00:26:12,111 --> 00:26:14,690
And the reason is,
is because if we were to minimize,

575
00:26:14,720 --> 00:26:15,553
Gee,
what we had before was this term 

576
00:26:17,121 --> 00:26:19,010
minimizing g,
what happens is,

577
00:26:19,011 --> 00:26:21,290
is that as the discriminator to gets 
better and better,

578
00:26:22,010 --> 00:26:25,340
that the gradient on g actually saturate
it goes to zero.

579
00:26:26,480 --> 00:26:28,380
So that's not very,
uh,

580
00:26:28,700 --> 00:26:30,230
useful.
If we want to train this small,

581
00:26:30,231 --> 00:26:33,110
and this is actually one of these,
one of the practical issues that you see

582
00:26:33,111 --> 00:26:33,944
when you actually train these models is 
that you're constantly fighting this 

583
00:26:36,980 --> 00:26:38,080
game between the,
you're,

584
00:26:38,081 --> 00:26:43,081
you're sort of on this edge of,
of the discriminator doing too well or,

585
00:26:44,220 --> 00:26:45,270
or the generator,
uh,

586
00:26:45,271 --> 00:26:45,681
you know,
it's a,

587
00:26:45,681 --> 00:26:46,501
it's,
it's essentially,

588
00:26:46,501 --> 00:26:47,334
you're basically almost always fighting 
the discriminator because it's always 

589
00:26:49,860 --> 00:26:52,170
going to,
as soon as this discriminators starts to

590
00:26:52,171 --> 00:26:53,004
win this,
this a competition between the gender 

591
00:26:54,931 --> 00:26:57,090
and the discriminator,
you end up with unstable training.

592
00:26:57,091 --> 00:26:59,640
And in this case,
you end up with basically the,

593
00:27:00,090 --> 00:27:03,120
the generators stops training and the 
discriminator runs away with it.

594
00:27:03,390 --> 00:27:04,890
Well,
that's actually in the original case.

595
00:27:04,950 --> 00:27:07,410
So what we'll do instead is we optimize 
this,

596
00:27:07,500 --> 00:27:11,310
which is a slight modification,
but it's still monotonic and it actually

597
00:27:11,311 --> 00:27:13,620
corresponds to the same,
the same fixed point.

598
00:27:13,621 --> 00:27:14,454
But,
but what we're doing is we're just 

599
00:27:15,151 --> 00:27:19,140
actually with respect to g,
again coming in through the samples here

600
00:27:19,230 --> 00:27:22,140
or maximizing this quantity rather than 
minimizing this one.

601
00:27:24,360 --> 00:27:25,193
Okay.
So that's just a practical kind of 

602
00:27:26,251 --> 00:27:27,084
heuristic thing,
but it actually makes a big difference 

603
00:27:28,051 --> 00:27:28,884
in practice.

604
00:27:30,460 --> 00:27:32,210
All right.
So when we did this,

605
00:27:32,270 --> 00:27:33,860
when we published first published this 
paper,

606
00:27:33,861 --> 00:27:34,694
this is the kind of results we would see
and what you're looking at now is kind 

607
00:27:36,891 --> 00:27:41,570
of movies formed by moving smoothly and 
zed space,

608
00:27:41,660 --> 00:27:42,380
right?
So this is,

609
00:27:42,380 --> 00:27:43,213
you're kind of looking at 
transformations on the image manifold 

610
00:27:46,641 --> 00:27:47,474
coming from smooth motions in zed space.
So we were pretty impressed with these 

611
00:27:51,081 --> 00:27:53,420
results again,
and maybe they felt good at the time.

612
00:27:53,710 --> 00:27:55,310
Um,
but there's been,

613
00:27:55,670 --> 00:27:56,503
you know,
a few papers that have come out 

614
00:27:57,891 --> 00:27:59,210
recently,
well,

615
00:27:59,720 --> 00:28:02,690
not so recently.
Actually at this point in 2016 there was

616
00:28:02,691 --> 00:28:05,750
a,
this was came out in 2014 and 2016 there

617
00:28:05,751 --> 00:28:08,330
was a big jump in the quality.
Um,

618
00:28:08,350 --> 00:28:09,183
and,
and this was sort of one of those 

619
00:28:10,041 --> 00:28:11,460
stages.
This is the least squares Gan.

620
00:28:11,490 --> 00:28:14,420
This is just one example of many I could
have pointed out.

621
00:28:14,780 --> 00:28:15,613
Um,
but this is the kind of results we're 

622
00:28:16,551 --> 00:28:17,180
seeing.
So,

623
00:28:17,180 --> 00:28:20,450
so part of one of the,
one of the secrets here is that it's one

624
00:28:20,451 --> 00:28:21,284
28 by one 28.
So bigger images actually give you a 

625
00:28:23,960 --> 00:28:26,360
much better perception of quality in 
terms of the images.

626
00:28:26,361 --> 00:28:30,650
But so these are not necessarily,
or generally not real bedrooms,

627
00:28:30,651 --> 00:28:31,484
these are,
these are actually generated from the 

628
00:28:33,771 --> 00:28:34,940
model,
right?

629
00:28:34,970 --> 00:28:37,160
So trained on,
you know,

630
00:28:37,190 --> 00:28:38,023
roughly I think 100,000
or at least a hundred thousand bedroom 

631
00:28:40,011 --> 00:28:42,950
scenes asked to generate from these 
random zed bits.

632
00:28:42,951 --> 00:28:43,970
This is what it gives you.

633
00:28:46,300 --> 00:28:47,133
So one thing you could think of,
one thing that's certainly kind of 

634
00:28:48,761 --> 00:28:49,594
occurred to me when I first saw these 
kinds of results is that well it's just 

635
00:28:52,931 --> 00:28:53,764
over fit on some small set of examples 
and it's just learning these Delta 

636
00:28:55,931 --> 00:28:57,400
functions,
right?

637
00:28:57,820 --> 00:28:59,830
So,
so that's not that interesting.

638
00:28:59,831 --> 00:29:00,664
In some sense it's kind of memorize some
small set of data and it's enough that 

639
00:29:04,040 --> 00:29:09,040
it looks good and it's impressive,
but it doesn't seem like that's actually

640
00:29:09,131 --> 00:29:09,964
the case in one of the evidence.
One of the parts of evidence it was 

641
00:29:11,381 --> 00:29:13,330
pointed to,
and this is in the DC Gann paper,

642
00:29:13,470 --> 00:29:14,500
um,
was this,

643
00:29:14,501 --> 00:29:15,334
so that same trick that I showed you 
with the movies in damnest where where 

644
00:29:18,701 --> 00:29:20,920
we were sort of moving smoothly and zed 
space,

645
00:29:21,160 --> 00:29:23,530
they applied basically that same idea 
here.

646
00:29:23,680 --> 00:29:28,600
So this is basically one long trajectory
through zed space.

647
00:29:28,990 --> 00:29:32,170
And what you can see is starting up here
on the ending up all the way down here,

648
00:29:32,320 --> 00:29:34,810
when you can see it's a smooth 
transition,

649
00:29:35,500 --> 00:29:38,020
right?
And at every point it seems like,

650
00:29:38,170 --> 00:29:40,630
you know,
a reasonable bedroom scene.

651
00:29:41,600 --> 00:29:42,000
Okay.

652
00:29:42,000 --> 00:29:42,833
So it really does seem like,
like that picture that I showed you 

653
00:29:45,271 --> 00:29:46,104
where we had the zed space,
it was sort of smooth and then we had 

654
00:29:48,481 --> 00:29:50,580
this x space,
they've had this manifold on it.

655
00:29:50,790 --> 00:29:52,530
It really does feel like that's what's 
happening here,

656
00:29:52,531 --> 00:29:53,364
right,
where we're moving smoothly and zed 

657
00:29:54,331 --> 00:29:57,600
space and we're moving along the image 
manifold in x space.

658
00:30:04,010 --> 00:30:05,240
So for example,
this,

659
00:30:05,480 --> 00:30:07,520
I guess,
I don't know if this is a picture or tv,

660
00:30:07,521 --> 00:30:12,080
but it slowly morphs into a window I 
guess,

661
00:30:12,081 --> 00:30:12,914
and then kind of becomes clearly a 
window and then it turns into just into 

662
00:30:15,020 --> 00:30:17,750
these edge sort of an edge of the room.

663
00:30:20,410 --> 00:30:22,690
So one of the things actually if you 
want to nitpick about these,

664
00:30:22,720 --> 00:30:26,410
the models actually don't seem to 
understand three d geometry very well.

665
00:30:26,411 --> 00:30:27,244
It often gets the perspective just a 
little wrong sort of something might be 

666
00:30:31,181 --> 00:30:33,580
interesting for future work.
Um,

667
00:30:33,581 --> 00:30:34,780
so yeah.
So one question,

668
00:30:34,820 --> 00:30:35,890
uh,
you might be,

669
00:30:35,891 --> 00:30:38,410
it's a why,
why do these things work well?

670
00:30:38,411 --> 00:30:40,470
And they keep in mind that,
you know,

671
00:30:40,480 --> 00:30:41,313
when we talked about the v model,
we actually had to do a quite a bit of 

672
00:30:44,021 --> 00:30:46,270
work to get comparable results,
right?

673
00:30:46,271 --> 00:30:48,370
We had to embed these,
pick these,

674
00:30:48,400 --> 00:30:52,900
a pixel CNNS and the decoder,
or we had to do some quite a bit of work

675
00:30:52,901 --> 00:30:55,330
to get the encoder to work right in 
these models.

676
00:30:55,331 --> 00:30:58,720
We literally just took a confident stuck
in some noise at the beginning,

677
00:30:58,750 --> 00:31:01,120
pushed it through and we got these 
fantastic samples.

678
00:31:01,121 --> 00:31:02,440
It really is kind of that simple.

679
00:31:02,770 --> 00:31:03,603
So what's going on?
Like why is it working as well as its 

680
00:31:06,150 --> 00:31:06,940
wills?
It,

681
00:31:06,940 --> 00:31:08,050
it is.
Um,

682
00:31:08,051 --> 00:31:10,230
and so I have a kind of an intuition for
you.

683
00:31:10,250 --> 00:31:12,280
I kind of a cartoon view.
Um,

684
00:31:12,281 --> 00:31:14,920
so imagine that this is the image 
manifold.

685
00:31:14,921 --> 00:31:15,754
So,
so this is kind of a cartoon view of an 

686
00:31:16,901 --> 00:31:17,451
image,
a metaphor,

687
00:31:17,451 --> 00:31:19,990
but like this is in,
in two pixel dimensions here.

688
00:31:20,990 --> 00:31:21,823
And,
and we're imagining here that these are 

689
00:31:23,201 --> 00:31:25,600
just parts of image manifold and they 
sort of,

690
00:31:25,870 --> 00:31:28,480
you know,
share some features close by.

691
00:31:28,481 --> 00:31:29,314
But,
but what this is basically representing 

692
00:31:30,401 --> 00:31:34,780
is the fact that that most of this space
isn't on the image manifold,

693
00:31:34,800 --> 00:31:35,633
right?
Image manifold,

694
00:31:35,650 --> 00:31:40,360
some complicated nonlinearity and if you
were to randomly sample in Pixel space,

695
00:31:40,361 --> 00:31:42,010
you would not land on this image 
manifold,

696
00:31:42,310 --> 00:31:43,061
which makes sense,
right?

697
00:31:43,061 --> 00:31:43,894
Randomly sample in Pixel space,
you're not getting a natural image out 

698
00:31:45,791 --> 00:31:46,624
of it.
This is sort of a cartoon view or my 

699
00:31:49,181 --> 00:31:52,540
perspective on the difference between 
what you see with

700
00:31:53,790 --> 00:31:58,560
maximum likelihood models of which the 
va is one and something like a Gan.

701
00:31:58,680 --> 00:32:02,910
So the maximum likelihood,
the way it's trained is it has to give a

702
00:32:02,911 --> 00:32:06,660
certain amount of likelihood density for
each real sample.

703
00:32:07,320 --> 00:32:10,170
If it doesn't,
it's punished very severely for that.

704
00:32:10,230 --> 00:32:14,800
So it's willing to spread out it's 
probability mass over regions of the,

705
00:32:14,900 --> 00:32:17,790
of the input space or the or of the 
Pixel space,

706
00:32:17,880 --> 00:32:20,370
which actually doesn't correspond to a 
natural image.

707
00:32:20,880 --> 00:32:21,713
And so when we sample from this model,
that's most of boy our samples come 

708
00:32:23,791 --> 00:32:24,241
from,
right?

709
00:32:24,241 --> 00:32:26,130
This is,
these are these blurry images that we're

710
00:32:26,131 --> 00:32:28,740
looking at again,
models things differently,

711
00:32:28,741 --> 00:32:29,574
right?
Because it's the only playing this game 

712
00:32:30,511 --> 00:32:32,130
between the encoder and,
well,

713
00:32:32,160 --> 00:32:32,993
sorry,
between the discriminator and the 

714
00:32:33,391 --> 00:32:37,860
generator,
all it has to do is sort of stick on,

715
00:32:37,920 --> 00:32:38,753
you know,
some subset of the examples or maybe 

716
00:32:41,031 --> 00:32:41,864
some subset of the manifolds that are 
present and not have enough diversity 

717
00:32:46,071 --> 00:32:50,240
that the discriminator can't notice that
it's modeling a subset.

718
00:32:50,360 --> 00:32:53,690
So there's pressure on the model too to 
maintain a certain amount of diversity,

719
00:32:53,930 --> 00:32:54,763
but at the same time it doesn't actually
face any pressure to model all aspects 

720
00:32:58,791 --> 00:32:59,624
of the training distribution.
It can just ignore certain aspects of 

721
00:33:02,271 --> 00:33:03,104
the training examples or certain aspects
of the training distribution without 

722
00:33:06,680 --> 00:33:11,420
significant punishment from the,
from the training algorithm.

723
00:33:11,900 --> 00:33:12,560
So anyway,
that's,

724
00:33:12,560 --> 00:33:13,393
that's I think a good idea to have in 
your mind about the difference between 

725
00:33:15,291 --> 00:33:18,650
how these methods work.
Um,

726
00:33:18,680 --> 00:33:19,071
yeah.
So,

727
00:33:19,071 --> 00:33:19,904
so just I'd like to sort of conclude 
with a few steps that have happened 

728
00:33:23,241 --> 00:33:24,610
since the,
uh,

729
00:33:24,650 --> 00:33:25,760
the gains.
One of the things,

730
00:33:25,761 --> 00:33:28,160
this is something that we've done.
We've actually,

731
00:33:28,460 --> 00:33:29,750
you know,
you might ask,

732
00:33:29,751 --> 00:33:30,584
well gans are great,
but in a way it's kind of unsatisfying 

733
00:33:32,761 --> 00:33:35,150
because we start with [inaudible] and 
then we can generate images.

734
00:33:35,151 --> 00:33:37,280
So yes,
we generate really nice looking images.

735
00:33:37,280 --> 00:33:38,690
But,
but we had this,

736
00:33:38,720 --> 00:33:39,553
you know,
a hope when we started talking about 

737
00:33:40,461 --> 00:33:41,294
these latent variable mount models that 
we could actually maybe infer the zed 

738
00:33:44,751 --> 00:33:45,710
from an image,
right?

739
00:33:45,711 --> 00:33:49,100
So we can actually extract some 
semantics out of the,

740
00:33:49,160 --> 00:33:49,993
of the image using these latent 
variables that you discover and in 

741
00:33:53,511 --> 00:33:54,350
again,
we don't have,

742
00:33:54,351 --> 00:33:55,184
then the question is can we actually 
within this generative adversarial 

743
00:33:57,891 --> 00:33:58,724
framework,
can we re incorporate inference 

744
00:34:00,741 --> 00:34:01,574
mechanism?
So that's exactly what we're doing here 

745
00:34:03,111 --> 00:34:03,944
with this work.
And this is actually a model we call 

746
00:34:05,361 --> 00:34:06,021
alley,
but they,

747
00:34:06,021 --> 00:34:09,260
the identical work essentially came out 
at the same time.

748
00:34:09,290 --> 00:34:10,760
Uh,
I'll known as by again.

749
00:34:10,910 --> 00:34:15,200
And the basic idea here is just to 
incorporate in encoder into the model.

750
00:34:15,410 --> 00:34:18,410
So rather than just giving the data,
distribute the data set,

751
00:34:18,411 --> 00:34:20,510
or here on,
on the left or earlier,

752
00:34:20,511 --> 00:34:23,090
the Gan was defined,
we had the decoder or generative model.

753
00:34:23,210 --> 00:34:26,480
But over here,
we only gave it x training examples.

754
00:34:26,570 --> 00:34:28,580
And here we only compare it against x,
uh,

755
00:34:28,581 --> 00:34:32,390
generated from the,
from the j the generator.

756
00:34:32,840 --> 00:34:33,673
But in this case,
what we're doing is we take x and then 

757
00:34:35,511 --> 00:34:40,511
we actually use an encoder here to,
to generate a zed given x.

758
00:34:41,510 --> 00:34:44,570
And on the decoder we have again,
our traditional Ganz style.

759
00:34:44,571 --> 00:34:48,500
We take us said sample from some simple 
distribution and we generate x.

760
00:34:48,710 --> 00:34:49,543
So again,
this is the data distribution over here 

761
00:34:50,990 --> 00:34:51,823
and code it to zed.
And then we take our decode the sample 

762
00:34:54,351 --> 00:34:55,184
from zed and we decode to x.
And our discriminator now crucially is 

763
00:34:57,771 --> 00:34:59,960
not just given x,
but it's given x and zed.

764
00:35:00,350 --> 00:35:01,183
And it's asked,
can you tell the difference in this 

765
00:35:02,271 --> 00:35:05,420
joint distribution between the encoder 
side and the decoder side?

766
00:35:06,650 --> 00:35:09,980
That's the game.
And what we find is,

767
00:35:10,220 --> 00:35:12,650
well first of all,
it actually generates very good samples,

768
00:35:12,651 --> 00:35:13,700
which is interesting.

769
00:35:13,730 --> 00:35:14,810
Uh,
it's actually,

770
00:35:15,920 --> 00:35:20,000
it seems to generate sort of better 
samples and we see with comparable gans,

771
00:35:20,001 --> 00:35:21,800
which there might be some regularization
effect.

772
00:35:21,801 --> 00:35:22,634
I'm not entirely sure why that would be,
but actually it gets fairly compelling 

773
00:35:24,861 --> 00:35:27,230
samples.
This is just with a seller bay,

774
00:35:27,231 --> 00:35:30,770
a large data set of a celebrity images.
Um,

775
00:35:30,980 --> 00:35:32,690
but this is the more interesting plot.
So,

776
00:35:32,691 --> 00:35:35,900
so this is actually corresponds to a 
hierarchical version of this model.

777
00:35:35,901 --> 00:35:39,450
So this is why we have multiple sets.
So this is ed one and said to,

778
00:35:39,451 --> 00:35:42,600
this is a two layer model inversion of 
this model.

779
00:35:42,810 --> 00:35:45,480
So if we just reconstruct from the 
higher levels Ed,

780
00:35:45,570 --> 00:35:47,090
which is this,
you know,

781
00:35:47,190 --> 00:35:49,860
containing fairly little information 
because is this you?

782
00:35:49,920 --> 00:35:54,630
It's a single vector and then it has to 
synthesize into this a large image.

783
00:35:55,050 --> 00:35:56,970
What we're looking at here,
a reconstruction.

784
00:35:56,971 --> 00:35:57,804
So we take this image,
we encoded all the way to zed two and 

785
00:36:01,021 --> 00:36:01,710
then we decode it.

786
00:36:01,710 --> 00:36:03,660
And what we end up with is,
is this,

787
00:36:03,870 --> 00:36:04,703
which is,
you know,

788
00:36:04,920 --> 00:36:07,210
reasonably close but not that great.
Right?

789
00:36:07,530 --> 00:36:09,630
And so as the same thing,
you're going to kind of all,

790
00:36:09,640 --> 00:36:12,090
they sort of hold some piece of the 
information,

791
00:36:12,240 --> 00:36:13,073
which in some sense this is kind of,
it's remarkable that does as well as it 

792
00:36:16,201 --> 00:36:19,050
does because it's actually not trained 
to do reconstruction.

793
00:36:19,710 --> 00:36:20,543
Unlike something like the vie which 
actually explicitly trained to do 

794
00:36:22,321 --> 00:36:25,590
reconstruction and this is just trained 
to match these two distributions.

795
00:36:25,591 --> 00:36:27,450
And this is kind of a probe to see how 
well it's doing.

796
00:36:27,451 --> 00:36:32,330
Cause we take x from one map a to zed 
and we take that set over and when we re

797
00:36:32,331 --> 00:36:35,040
synthesize the acts and we see we're 
seeing now an x space,

798
00:36:35,100 --> 00:36:37,440
how close did it come?
And you know it does.

799
00:36:37,441 --> 00:36:40,880
Okay.
But over here when we give it x one ends

800
00:36:40,900 --> 00:36:41,740
ed too.

801
00:36:42,690 --> 00:36:43,523
So in this case we're really just giving
it all of the latent variable 

802
00:36:45,751 --> 00:36:47,400
information.
We actually get much,

803
00:36:47,401 --> 00:36:48,234
much closer.
Which is interesting because this is 

804
00:36:49,261 --> 00:36:51,600
telling us that this pure joint 
modeling,

805
00:36:51,601 --> 00:36:54,070
in this case it'd be a joint modeling 
between x,

806
00:36:54,071 --> 00:36:54,950
Y,
zed one ends,

807
00:36:54,951 --> 00:36:55,784
zed to that.
This is enough to do fairly good 

808
00:36:58,801 --> 00:37:01,830
reconstruction without ever actually 
explicitly building that in.

809
00:37:02,130 --> 00:37:05,340
So it's giving us an interesting probe 
into how close are we coming to learning

810
00:37:05,341 --> 00:37:06,174
this joint distribution and it seems 
like we're getting actually surprising 

811
00:37:08,951 --> 00:37:09,784
the clothes.
So it's like it's a testament to how 

812
00:37:12,091 --> 00:37:12,924
effective I think this generative 
adversarial training exam algorithm 

813
00:37:16,531 --> 00:37:17,364
actually is.
So I want to just end with a few other 

814
00:37:20,971 --> 00:37:23,160
things that have nothing to do with our 
work but I think are very,

815
00:37:23,161 --> 00:37:25,680
very interesting and well worth you guys
learning about.

816
00:37:25,950 --> 00:37:30,950
So first one is cycle Gan cycle again is
this really cool idea starting with,

817
00:37:31,770 --> 00:37:35,610
let's imagine you have two sets of 
datasets that somehow correspond but you

818
00:37:35,611 --> 00:37:37,860
don't know the correspondence you don't 
have,

819
00:37:37,861 --> 00:37:40,530
like there's an alignment that exists 
between these two data sets,

820
00:37:40,531 --> 00:37:43,020
but you might not know what,
you might not have paired data.

821
00:37:43,230 --> 00:37:45,540
This actually happens a lot.
Say for example,

822
00:37:46,080 --> 00:37:46,913
this is not an image space,
but a great example of this happens in 

823
00:37:49,530 --> 00:37:50,760
machine translation,
right?

824
00:37:51,150 --> 00:37:54,390
You almost always have lots of 
unilingual a data.

825
00:37:54,570 --> 00:37:57,210
So just text data in in a given 
language,

826
00:37:57,390 --> 00:37:59,490
but it's very expensive to get aligned 
data.

827
00:37:59,550 --> 00:38:03,630
But to data paired as a source and 
target distribution.

828
00:38:03,990 --> 00:38:07,320
If the question is what can you do if 
you just have unilingual data,

829
00:38:07,560 --> 00:38:10,830
how successful kicking you'd be at 
learning a mapping between the two.

830
00:38:11,160 --> 00:38:13,260
And they essentially use gans to do 
this.

831
00:38:13,380 --> 00:38:14,213
So this is the setup.
They have some domain x here and her 

832
00:38:16,711 --> 00:38:18,000
domain.
Why here?

833
00:38:18,510 --> 00:38:20,340
And what they do is they start with an 
ex,

834
00:38:20,460 --> 00:38:22,950
they transform it through some 
convolutional neural net,

835
00:38:22,980 --> 00:38:25,800
usually a resonant based model into some
y.

836
00:38:25,920 --> 00:38:28,290
And on this why they're going to 
evaluate it as a,

837
00:38:28,560 --> 00:38:31,320
as a,
as a Gan style discriminator here.

838
00:38:31,590 --> 00:38:36,590
So Ken acts a through g make a 
convincing why that's being,

839
00:38:37,220 --> 00:38:38,053
what's being measured here.
So you can think of x is taking the 

840
00:38:40,571 --> 00:38:42,670
place,
which is some other image.

841
00:38:42,671 --> 00:38:44,590
Let's say this is some image to another 
image.

842
00:38:44,860 --> 00:38:47,530
This image x is taking the place of our,
of our zed,

843
00:38:47,590 --> 00:38:51,370
of our random bits.
It's getting it's randomness from x.

844
00:38:52,270 --> 00:38:56,050
And then we do the same thing.
We can kind of transform it through f.

845
00:38:56,051 --> 00:38:58,570
So we've got x here,
transform it through g,

846
00:38:58,610 --> 00:38:59,443
t to D and we evaluate on,
on the our discriminator here on Gann 

847
00:39:03,110 --> 00:39:05,770
style training.
And then we re encode this annex.

848
00:39:05,920 --> 00:39:08,800
Now once we get here,
that's over here.

849
00:39:08,801 --> 00:39:11,740
So we've taken ecs transformed it into 
why transform it back.

850
00:39:12,100 --> 00:39:14,260
They actually do what's called a cycle 
consistency loss,

851
00:39:14,261 --> 00:39:16,000
which is,
this is actually a reconstruction.

852
00:39:16,210 --> 00:39:17,043
This is an l one reconstruction error 
and they back prop through f and g and 

853
00:39:21,161 --> 00:39:22,630
then they,
it's a symmetric relationship.

854
00:39:22,630 --> 00:39:24,550
So they do the exact same thing on the 
other side.

855
00:39:24,670 --> 00:39:27,490
They start with why they see if they can
transform it into acs,

856
00:39:27,491 --> 00:39:28,324
the compare the,
they compare that that that generated x 

857
00:39:30,851 --> 00:39:31,684
with true x is via a discriminator.
And then again transform that to y and 

858
00:39:35,321 --> 00:39:39,970
do the cycle consistency loss.
So without any pair of data,

859
00:39:40,120 --> 00:39:41,890
this is the kind of thing that they can 
get.

860
00:39:43,370 --> 00:39:46,960
So a particular note is a horses and 
zebras,

861
00:39:46,961 --> 00:39:47,381
right?
So,

862
00:39:47,381 --> 00:39:49,210
and this is a case where,
you know,

863
00:39:49,240 --> 00:39:50,260
it's,
it's impossible.

864
00:39:50,261 --> 00:39:52,450
Do they get this kind of pair data?
Say you wanted a,

865
00:39:52,451 --> 00:39:56,290
a transformation that transformed horses
to zebras and vice versa.

866
00:39:57,040 --> 00:40:02,040
You will never find pictures of horses 
and zebras in the exact same pose,

867
00:40:02,351 --> 00:40:03,184
right?
That's just not a kind of Datas that 

868
00:40:03,791 --> 00:40:04,624
you're ever going to be able to collect.
And yet they do a fairly convincing job 

869
00:40:07,630 --> 00:40:08,290
of doing this.

870
00:40:08,290 --> 00:40:11,260
And you can see that they even turn like
there's a little bit,

871
00:40:11,350 --> 00:40:12,610
this one actually doesn't do it very 
well,

872
00:40:12,611 --> 00:40:13,444
but oftentimes what you see as the turn 
like green grass a little bit more 

873
00:40:16,301 --> 00:40:17,134
savannah like you took that kind of 
dulls it out because zebras are found 

874
00:40:20,621 --> 00:40:22,570
generally in Savannah,
let conditions,

875
00:40:22,950 --> 00:40:23,783
uh,
they can do a winter to summer kind of 

876
00:40:25,781 --> 00:40:28,710
transitions.
I've seen examples data night,

877
00:40:29,330 --> 00:40:30,163
um,
these are pretty interested in the 

878
00:40:31,061 --> 00:40:33,940
various other things.
Now I think there's,

879
00:40:33,970 --> 00:40:36,420
there's a lot of interesting things you 
can do with this dataset.

880
00:40:36,450 --> 00:40:37,750
And,
and going back to,

881
00:40:37,751 --> 00:40:41,120
if you think about that simulation 
example with ro robotics that I gave the

882
00:40:41,121 --> 00:40:41,954
motivating example at the beginning.
This is a prime application area for 

883
00:40:44,921 --> 00:40:45,754
this kind of technology,
but it will say that one of the things 

884
00:40:47,471 --> 00:40:48,304
that they've done here is they assume a 
deterministic transformation between 

885
00:40:51,071 --> 00:40:52,000
these two domains.

886
00:40:52,300 --> 00:40:53,133
So I think there's a lot of interest 
looking at how do you actually break 

887
00:40:55,721 --> 00:40:56,554
that kind of restriction and imagine 
something more like in many to many 

888
00:40:59,141 --> 00:40:59,974
mapping between these two domains.
So the last thing I want to show you is 

889
00:41:03,011 --> 00:41:03,844
kind of the most recent stuff,
which is just kind of mind blowing in 

890
00:41:05,561 --> 00:41:07,610
terms of just the quality of generation 
that they,

891
00:41:07,960 --> 00:41:08,793
they show.
So these are images from invidia 

892
00:41:11,171 --> 00:41:12,190
actually.
So I don't know if I'm,

893
00:41:12,340 --> 00:41:15,340
I'm uh,
I'm sort of a undercutting,

894
00:41:15,520 --> 00:41:16,840
I don't know if he was going to show 
these or not,

895
00:41:16,841 --> 00:41:17,890
but uh,
yeah,

896
00:41:17,891 --> 00:41:21,610
so these trained on,
um,

897
00:41:22,210 --> 00:41:24,930
on the original,
um,

898
00:41:25,070 --> 00:41:27,220
celebrate data set,
the same one we had before,

899
00:41:27,221 --> 00:41:29,410
but now much,
much larger images.

900
00:41:29,440 --> 00:41:30,820
Right?
So 1,024

901
00:41:30,821 --> 00:41:34,450
by 2024 and they're able get these kinds
of uh,

902
00:41:35,240 --> 00:41:38,690
generated models.
So I would argue that many of these,

903
00:41:38,691 --> 00:41:42,790
maybe all of the ones shown here 
essentially pass a kind of a,

904
00:41:43,080 --> 00:41:44,270
a turing test,
right?

905
00:41:44,270 --> 00:41:45,103
An image turing test.
You cannot tell that these are not real 

906
00:41:46,341 --> 00:41:50,060
people right now.
I shouldn't say not all images actually,

907
00:41:50,080 --> 00:41:51,140
uh,
look this good.

908
00:41:51,141 --> 00:41:51,974
Some of them are actually really spooky,
but you can go online and look at the 

909
00:41:55,131 --> 00:41:57,080
video and pick some out there.
Yeah.

910
00:41:57,830 --> 00:41:58,663
Um,
how they do this is with a really 

911
00:42:01,401 --> 00:42:03,680
interesting technique and this is 
actually a so new,

912
00:42:03,681 --> 00:42:04,514
we,
I have students that are starting to 

913
00:42:05,541 --> 00:42:06,374
look at this,
but we haven't really probed this very 

914
00:42:08,421 --> 00:42:09,680
far.
So I actually don't know,

915
00:42:10,020 --> 00:42:12,170
uh,
how effective this is in general.

916
00:42:12,380 --> 00:42:13,910
But it's actually seems very 
interesting.

917
00:42:13,911 --> 00:42:14,744
So they just start with a tiny four by 
four image and once you train up that 

918
00:42:19,671 --> 00:42:20,504
parameter,
those parameters for both the 

919
00:42:21,471 --> 00:42:24,080
discriminator here and the generator.
So again,

920
00:42:24,081 --> 00:42:26,000
these are convolutions but we're 
starting with,

921
00:42:26,150 --> 00:42:30,110
with a relatively small input,
we increased the size of the input,

922
00:42:30,111 --> 00:42:32,060
we add a layer and,
and,

923
00:42:32,070 --> 00:42:32,903
and the,
some of these parameters here actually 

924
00:42:34,491 --> 00:42:36,560
formed by the parameters that gave you 
this image.

925
00:42:36,560 --> 00:42:37,393
So you sort of just stick this up here 
and now you add some parameters and 

926
00:42:40,041 --> 00:42:40,874
you'd now train the model to learn 
something bigger and you keep going and 

927
00:42:43,671 --> 00:42:45,740
you keep going.
And then you get something like this.

928
00:42:46,550 --> 00:42:47,780
As far as I'm concerned,
this is this sort of,

929
00:42:47,781 --> 00:42:51,560
this amounts to kind of a curriculum of 
training does two things for you.

930
00:42:51,740 --> 00:42:52,573
One is it helps build a bit of global 
structure to the image because you're 

931
00:42:55,851 --> 00:42:58,160
starting with such low dimensional,
um,

932
00:42:58,820 --> 00:42:59,311
inputs.
It,

933
00:42:59,311 --> 00:43:01,670
it helps reinforce the kind of global 
structure,

934
00:43:01,970 --> 00:43:03,680
but it also does something else,
which is pretty important.

935
00:43:03,910 --> 00:43:05,950
It allows the model to sort of,
um,

936
00:43:06,140 --> 00:43:08,660
not have to spend a lot of time training
a very,

937
00:43:08,661 --> 00:43:10,280
very large model like this.
Right?

938
00:43:10,281 --> 00:43:11,114
You sweet.
I would imagine they spend relatively 

939
00:43:12,351 --> 00:43:14,480
little time training here.
Although this is Nvidia,

940
00:43:14,481 --> 00:43:16,850
so they might spend a lot of time 
training this,

941
00:43:17,180 --> 00:43:18,013
but um,
but it allows you to spend a lot of the 

942
00:43:19,611 --> 00:43:21,470
time sort of in much,
much smaller models,

943
00:43:21,860 --> 00:43:25,010
so much more computationally efficient 
to train this model.

944
00:43:25,610 --> 00:43:26,870
All right.
So that's it for me.

945
00:43:26,871 --> 00:43:28,280
Thanks a lot.
Oh wait.

946
00:43:28,310 --> 00:43:29,143
Oh yeah.
Sorry.

947
00:43:29,150 --> 00:43:30,530
One more thing I forgot.
Uh,

948
00:43:30,531 --> 00:43:33,290
this is just what they get.
Unconditional image.

949
00:43:33,330 --> 00:43:35,880
A generation now with amnesty.
So this is again,

950
00:43:35,881 --> 00:43:39,260
so you give it horse and then it's able 
to generate this kind of thing.

951
00:43:39,560 --> 00:43:41,330
So far it's able to generate this,
right?

952
00:43:41,360 --> 00:43:43,550
So bicycles,
it's able to generate these,

953
00:43:44,030 --> 00:43:45,670
which is pretty amazing.
Um,

954
00:43:46,250 --> 00:43:47,310
quality.
Uh,

955
00:43:47,360 --> 00:43:48,740
if you zoom in here,
you can actually,

956
00:43:48,741 --> 00:43:51,740
it's kind of fun because the,
it kind of gets the idea of,

957
00:43:51,770 --> 00:43:52,431
of,
uh,

958
00:43:52,431 --> 00:43:53,840
these,
these spokes,

959
00:43:53,841 --> 00:43:56,930
but not exactly like some of them just 
sort of end midway.

960
00:43:57,400 --> 00:43:58,310
Uh,
but yeah,

961
00:43:58,880 --> 00:44:00,800
but still a pretty,
pretty remarkable.

962
00:44:01,460 --> 00:44:02,271
All right.
So,

963
00:44:02,271 --> 00:44:03,020
uh,
thanks.

964
00:44:03,020 --> 00:44:04,780
If they have questions,
I'll take them.

