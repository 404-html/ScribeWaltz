1
00:00:02,610 --> 00:00:06,570
I want to bring this part of the class 
to an end,

2
00:00:06,571 --> 00:00:10,620
so this is our last lecture,
but for our series of guest lectures and

3
00:00:10,621 --> 00:00:11,454
in this talk I hope to address some of 
the state of deep learning today and 

4
00:00:14,701 --> 00:00:15,534
kind of bring up some of the limitations
of the algorithms that you've been 

5
00:00:18,091 --> 00:00:20,580
seeing in this class so far.
So we've got a really good taste of some

6
00:00:20,581 --> 00:00:21,414
of the limitations specifically in 
reinforcement learning algorithms that 

7
00:00:25,111 --> 00:00:29,170
lex gave in the last lecture and that's 
really going to build on a.

8
00:00:29,480 --> 00:00:30,313
or I'm going to use that to build on top
of during this lecture and just to end 

9
00:00:33,901 --> 00:00:35,490
on.
I'm gonna bring you.

10
00:00:35,550 --> 00:00:36,383
I'm going to introduce you to some new 
frontiers in deep learning that are 

11
00:00:40,171 --> 00:00:41,004
really,
really inspiring and that the cutting 

12
00:00:42,391 --> 00:00:47,010
edge of research today.
Before we do that,

13
00:00:47,850 --> 00:00:48,683
I'd like to just make some 
administrative announcements so tee 

14
00:00:51,971 --> 00:00:54,900
shirts have arrived and will be 
distributing them today and we'd like to

15
00:00:54,901 --> 00:00:58,110
distribute first to the registered for 
credit students.

16
00:00:59,400 --> 00:01:00,233
After that,
we will be happy to distribute to 

17
00:01:01,771 --> 00:01:03,480
registered listeners and then after 
that,

18
00:01:03,481 --> 00:01:06,330
if there's any remaining goal,
give out to listeners if they'd want,

19
00:01:06,360 --> 00:01:07,280
if they're interested.

20
00:01:10,080 --> 00:01:12,180
So for those of you who are taking this 
class for credit,

21
00:01:12,181 --> 00:01:17,181
I need to reiterate what kind of options
you have to actually fulfill your credit

22
00:01:17,731 --> 00:01:18,564
requirement.
So the first option is a group project 

23
00:01:20,761 --> 00:01:23,610
proposal presentation.
So for this option,

24
00:01:23,611 --> 00:01:27,420
you'll be given the opportunity to pitch
a novel deep learning idea to a panel of

25
00:01:27,421 --> 00:01:28,254
judges on Friday.
You'll have exactly one minute to make 

26
00:01:31,471 --> 00:01:35,250
your pitch as error,
as clear and as concisely as possible.

27
00:01:35,850 --> 00:01:36,683
So this is really difficult to do in one
minute and this kind of one of the 

28
00:01:39,121 --> 00:01:39,954
challenges that we're,
we're putting on you in addition to 

29
00:01:41,911 --> 00:01:44,970
actually coming up with the deep 
learning idea itself.

30
00:01:46,500 --> 00:01:49,170
If you want to go down this route for 
your final project,

31
00:01:49,920 --> 00:01:50,753
then you'll need to submit your teams,
which have to be of size three or four 

32
00:01:54,061 --> 00:01:56,070
by the end of today.
So at 9:00

33
00:01:56,070 --> 00:01:57,090
PM today,
we'd like those in,

34
00:01:57,890 --> 00:01:58,723
um,
you'll have to do teams of three and 

35
00:02:00,571 --> 00:02:00,900
four.

36
00:02:00,900 --> 00:02:03,510
So if you want a group working in groups
of one or two,

37
00:02:03,570 --> 00:02:05,120
then you'll have to,
you're,

38
00:02:05,150 --> 00:02:07,770
you're welcome to do that,
but you won't be able to actually submit

39
00:02:07,990 --> 00:02:12,180
or final project as part of a 
presentation on Friday.

40
00:02:12,210 --> 00:02:13,043
You can submit it to us and we'll,
we'll give you the grade for the class 

41
00:02:15,871 --> 00:02:17,760
like that.
Um,

42
00:02:17,940 --> 00:02:18,773
so groups are do 9:00
PM today and you have to submit your 

43
00:02:21,871 --> 00:02:23,280
slides by 9:00
PM tomorrow.

44
00:02:23,490 --> 00:02:27,090
Presentations or our class on Friday in 
this room.

45
00:02:27,840 --> 00:02:30,120
If you don't want to do a project or a 
presentation,

46
00:02:30,150 --> 00:02:30,983
you have a second option,
which is to write a one page paper 

47
00:02:33,080 --> 00:02:35,520
review of a deep learning idea.
Uh,

48
00:02:35,521 --> 00:02:39,670
so any idea or any paper that you find 
interesting is,

49
00:02:39,760 --> 00:02:40,593
is welcome here.
So we really accept anything and we're 

50
00:02:42,061 --> 00:02:43,890
really free in this,
in this,

51
00:02:43,891 --> 00:02:45,420
um,
option as well.

52
00:02:46,740 --> 00:02:47,573
I want to highlight some of the exciting
new talks that we have coming up after 

53
00:02:50,701 --> 00:02:51,534
today.

54
00:02:51,600 --> 00:02:53,790
So tomorrow we'll have two sets of guest
lectures.

55
00:02:53,791 --> 00:02:57,930
First we'll hear from Ernest Mueller,
who is the chief architect of and videos

56
00:02:58,200 --> 00:02:59,033
self driving car team.
So erzen and his team were actually 

57
00:03:01,931 --> 00:03:02,764
known for some really exciting work that
Aldo is showing yesterday during her 

58
00:03:06,221 --> 00:03:07,054
lecture and they're known for this 
development of an end to end platform 

59
00:03:09,641 --> 00:03:10,474
for autonomous driving that takes 
directly image data and produces a 

60
00:03:14,320 --> 00:03:17,290
steering control command at the car for 
the car at the output.

61
00:03:18,150 --> 00:03:18,983
Then we'll hear about,
we'll hear from two google brain 

62
00:03:21,011 --> 00:03:25,300
researchers on recent advancements on 
image classification at Google.

63
00:03:26,020 --> 00:03:30,280
And also we'll hear about some super 
recent advancements and additions to the

64
00:03:30,281 --> 00:03:34,360
tensorflow pipeline that we're actually 
just released a couple of days ago.

65
00:03:34,660 --> 00:03:36,130
So this is really,
really new stuff.

66
00:03:37,070 --> 00:03:37,903
Uh,
tomorrow afternoon we'll get together 

67
00:03:39,131 --> 00:03:41,830
for one of the most exciting parts of 
this class.

68
00:03:42,510 --> 00:03:43,343
Um,
so what will happen is we'll have each 

69
00:03:45,221 --> 00:03:47,770
of the sponsors actually come up to the 
front of the class here.

70
00:03:48,190 --> 00:03:49,023
We have four sponsors that will present 
on each of these four boards and you'll 

71
00:03:52,211 --> 00:03:55,360
be given the opportunity to basically 
connect with each of them through the,

72
00:03:56,200 --> 00:03:59,020
uh,
through the ways of a recruitment booth.

73
00:03:59,050 --> 00:03:59,883
And basically they're going to be 
looking at students that might be 

74
00:04:02,171 --> 00:04:05,800
interested in deep learning internships 
or employment opportunities.

75
00:04:05,801 --> 00:04:09,370
So this is really an incredible 
opportunity for you guys to connect with

76
00:04:09,371 --> 00:04:12,190
these companies in a very,
very,

77
00:04:12,191 --> 00:04:13,024
very direct manner.
So we highly recommend that you take 

78
00:04:15,611 --> 00:04:16,444
advantage of that.
There'll be info sessions with pizza 

79
00:04:19,211 --> 00:04:20,044
provided on Thursday with one of these 
guests lecture with one of these 

80
00:04:23,470 --> 00:04:24,303
industry companies and we'll be sending 
out more details with that today as 

81
00:04:27,281 --> 00:04:28,114
well.

82
00:04:29,770 --> 00:04:30,603
So on Friday we'll continue with the 
guest lectures and hear from Lisa and 

83
00:04:33,041 --> 00:04:36,220
meany who is the head of IBM research 
and Cambridge.

84
00:04:37,090 --> 00:04:39,950
She's actually also the,
uh,

85
00:04:40,120 --> 00:04:40,953
director of the MIT IBM lab.
And this is a lab that was just founded 

86
00:04:46,001 --> 00:04:48,970
a couple or actually about a month ago 
or two months ago.

87
00:04:50,210 --> 00:04:51,043
We'll be hearing about how IBM is 
creating ai systems that are capable of 

88
00:04:54,731 --> 00:04:57,250
not only deep learning,
we're going to step past deep learning,

89
00:04:57,880 --> 00:04:58,713
they're capable of or trying to be 
capable of learning and recently on a 

90
00:05:01,541 --> 00:05:02,374
higher order sense.
And then finally we'll hear from a 

91
00:05:05,681 --> 00:05:06,514
principal researcher at ten cent ai lab 
about combining computer vision and 

92
00:05:10,361 --> 00:05:11,194
social networks.
It's a very interesting topic that we 

93
00:05:14,021 --> 00:05:15,820
haven't really touched upon in this 
class.

94
00:05:16,500 --> 00:05:17,333
I'm just topic of social networks and 
using massive big data collected from 

95
00:05:20,660 --> 00:05:25,420
from humans themselves.
And then as I mentioned before,

96
00:05:25,421 --> 00:05:26,254
in the afternoon,
we'll go through and hear about the 

97
00:05:27,131 --> 00:05:28,480
final project presentations.

98
00:05:28,480 --> 00:05:32,320
We'll celebrate with some pizza and the 
awards that will be given out to the top

99
00:05:32,800 --> 00:05:35,800
projects during those during that 
session as well.

100
00:05:37,570 --> 00:05:40,060
So now let's start with the technical 
content for this class,

101
00:05:41,350 --> 00:05:42,183
I'd like to start by just kind of 
overviewing the type of architectures 

102
00:05:46,001 --> 00:05:48,790
that we've talked about so far.
For the most part,

103
00:05:48,791 --> 00:05:52,360
these architectures can be thought of 
almost pattern recognition architecture.

104
00:05:52,361 --> 00:05:57,361
So they take us input data and the whole
point of their pipeline,

105
00:05:57,891 --> 00:05:58,724
their internals are performing feature 
extraction and and what they're really 

106
00:06:03,111 --> 00:06:03,944
doing is taking all of the century data,
trying to figure out what are the 

107
00:06:05,751 --> 00:06:06,584
important pieces,
what are the patterns to be learned 

108
00:06:08,331 --> 00:06:11,840
within the data such that they can 
produce a decision at the output.

109
00:06:12,470 --> 00:06:15,980
We've seen this take many forms,
so the decision could be a prediction,

110
00:06:16,380 --> 00:06:19,850
could be a detection or even an action 
like in reinforcement learning setting.

111
00:06:20,780 --> 00:06:25,250
We've even learned how these models can 
be viewed in a generative sense to go in

112
00:06:25,251 --> 00:06:28,550
the opposite direction.
It actually generate new synthetic data,

113
00:06:30,950 --> 00:06:31,783
but in general we've been dealing with 
algorithms that are really optimized to 

114
00:06:34,401 --> 00:06:35,234
do well and only a single task,
but they really failed to think like 

115
00:06:38,961 --> 00:06:39,794
humans do,
especially when we consider a higher 

116
00:06:41,931 --> 00:06:44,990
order level of intelligence like I 
defined on the first day.

117
00:06:48,150 --> 00:06:48,983
To understand this in a lot more detail,
we have to go back to this very famous 

118
00:06:51,371 --> 00:06:52,204
theorem,
those dating back almost 30 years from 

119
00:06:54,611 --> 00:06:56,860
today,
the steering,

120
00:06:56,861 --> 00:06:58,900
which is known as the universal 
approximation.

121
00:06:58,930 --> 00:06:59,763
Durham was one of the most impactful 
theorems in neural networks when it 

122
00:07:03,551 --> 00:07:06,340
first came out because it had such a 
profound.

123
00:07:07,000 --> 00:07:07,833
It such a profound claim.
What it states is that a neural network 

124
00:07:11,141 --> 00:07:11,974
with a single hidden layer is sufficient
to approximate any function to any 

125
00:07:16,331 --> 00:07:18,910
arbitrary level of accuracy.
Now,

126
00:07:18,911 --> 00:07:21,790
in this class,
we deal with networks that are deep.

127
00:07:21,791 --> 00:07:23,050
They're not single.
Layered,

128
00:07:23,080 --> 00:07:24,760
so they're actually more than a single 
layer,

129
00:07:24,970 --> 00:07:28,570
so actually they continue to add more 
complexity than the network down,

130
00:07:28,571 --> 00:07:33,571
referring to here,
but this theorem proves that we actually

131
00:07:33,611 --> 00:07:38,611
only need one layer to accomplish or to 
approximate any function in the world,

132
00:07:39,130 --> 00:07:39,963
and if you believe that any problem can 
actually be reduced to a sets of inputs 

133
00:07:43,991 --> 00:07:44,824
and outputs and it's a form of a 
function than this theory on shows you 

134
00:07:48,720 --> 00:07:49,553
that a neural network with just a single
layer is able to solve any problem in 

135
00:07:53,291 --> 00:07:54,124
the world.

136
00:07:54,730 --> 00:07:56,800
Now,
this is an incredibly powerful result,

137
00:07:56,801 --> 00:07:59,950
but if you look closely,
there are a few very important caveats.

138
00:08:00,170 --> 00:08:01,003
I'm not actually telling you how large 
that hidden layer has to be to 

139
00:08:03,911 --> 00:08:06,100
accomplish this task.
Now,

140
00:08:06,101 --> 00:08:09,820
with the size of your problem,
the hidden layer and the number of units

141
00:08:09,821 --> 00:08:12,190
in that hidden layer,
maybe exponentially large.

142
00:08:12,191 --> 00:08:15,520
I know grow exponentially with the 
difficulty of your problem.

143
00:08:16,060 --> 00:08:18,310
This makes training that network very 
difficult,

144
00:08:18,311 --> 00:08:21,700
so I never actually told you anything 
about how to obtain that network.

145
00:08:21,701 --> 00:08:26,470
I just told you that it existed and 
there is a possible network in the realm

146
00:08:26,471 --> 00:08:28,360
of all neural networks that can solve 
that problem,

147
00:08:28,361 --> 00:08:29,194
but as we know in practice,
actually training neural networks 

148
00:08:32,530 --> 00:08:35,620
because of their noncombat structure is 
extremely difficult.

149
00:08:40,550 --> 00:08:45,550
So this term is really a perfect example
of the possible effects of overhyping in

150
00:08:45,711 --> 00:08:46,544
Ai.
So over the history of ai we've had to 

151
00:08:49,611 --> 00:08:50,444
ai winters and this theorem was one of 
the resurgence after the first day I 

152
00:08:57,571 --> 00:08:58,404
went there,
but it also caused a huge false hype in 

153
00:09:01,021 --> 00:09:01,854
the power of these neural networks which
ultimately lead to yet another ai 

154
00:09:04,501 --> 00:09:05,334
winter.
And I feel like as a class it's very 

155
00:09:07,711 --> 00:09:08,544
important to bring this up because right
now we're very much in the state of a 

156
00:09:14,431 --> 00:09:19,431
huge amount of overhyping and deep 
learning algorithms.

157
00:09:20,070 --> 00:09:20,903
So these algorithms are,
especially in the media being portrayed 

158
00:09:23,850 --> 00:09:24,683
that they can accomplish human level 
intelligence and human level reasoning 

159
00:09:28,950 --> 00:09:29,783
and simply this is not true.
So I think such overhype is extremely 

160
00:09:32,641 --> 00:09:33,474
dangerous and resulted well.
We know it resulted in both of the two 

161
00:09:37,231 --> 00:09:41,370
past ai winters and I think as a class 
that's very important for us to focus on

162
00:09:41,371 --> 00:09:42,204
some of the limitations of these 
algorithms so that we don't overhype 

163
00:09:45,270 --> 00:09:47,400
them,
but we provide realistic guarantees,

164
00:09:47,401 --> 00:09:52,401
are realistic expectations rather on 
what these algorithms can accomplish and

165
00:09:54,450 --> 00:09:57,240
finally going past these limitations.
The last part of this talk will actually

166
00:09:57,241 --> 00:10:00,690
focus on some of the exciting research,
like I mentioned before,

167
00:10:00,900 --> 00:10:05,900
that tries to take a couple of these 
limitations and really focus on possible

168
00:10:06,541 --> 00:10:08,970
solutions and possible ways that we can 
move past them.

169
00:10:11,100 --> 00:10:11,933
Okay,
so let's start and I think one of the 

170
00:10:13,380 --> 00:10:14,213
best examples of a potential danger of 
neural networks comes from this paper 

171
00:10:18,601 --> 00:10:19,434
from Google deep mind,
a named understanding deep neural 

172
00:10:22,411 --> 00:10:23,244
networks requires rethinking 
generalization and generalizations was 

173
00:10:27,991 --> 00:10:30,510
this topic that we discussed in the 
first lecture.

174
00:10:30,511 --> 00:10:31,344
So this is the notion of a gap or a 
difference between your training 

175
00:10:35,431 --> 00:10:40,431
accuracy and your test accuracy.
If you're able to achieve equal training

176
00:10:40,981 --> 00:10:41,814
and test accuracy,
that means you have essentially no 

177
00:10:43,111 --> 00:10:43,944
generalization gap.
You're able to generalize perfectly to 

178
00:10:47,460 --> 00:10:48,293
to your test data set,
but if there's a huge disparity between 

179
00:10:50,851 --> 00:10:51,684
these two datasets in your model is 
performing much better on your training 

180
00:10:54,691 --> 00:10:55,524
data set than your test data.
So this means that you're not able to 

181
00:10:57,571 --> 00:11:02,571
actually generalize to brand new images.
You're only just memorizing the training

182
00:11:03,061 --> 00:11:03,894
examples.
And what this paper did was they 

183
00:11:06,990 --> 00:11:11,130
performed the following experiment,
so they took images from image net so he

184
00:11:11,131 --> 00:11:13,410
can see four examples of these images 
here.

185
00:11:13,590 --> 00:11:14,423
And what they did was they rolled a case
I did die where k is the number of all 

186
00:11:19,321 --> 00:11:20,154
possible labels in that data set and 
this allowed them to randomly assigned 

187
00:11:25,621 --> 00:11:27,690
brand new labels to each of these 
images.

188
00:11:28,110 --> 00:11:28,943
So what used to be a dog,
they call it now a banana and what they 

189
00:11:32,760 --> 00:11:33,593
were used to be,
that banana is now called the dog and 

190
00:11:35,341 --> 00:11:37,860
what it used to be called that second 
dog is now a tree.

191
00:11:38,250 --> 00:11:39,083
So note that the two dogs have actually 
been transformed into two separate 

192
00:11:42,001 --> 00:11:42,834
things.
So things that used to be in the same 

193
00:11:43,561 --> 00:11:44,394
class are now in completely disjoined 
classes and things that were in 

194
00:11:46,981 --> 00:11:49,290
destroying classes may be now in the 
same class.

195
00:11:49,680 --> 00:11:53,350
So basically we're completely 
randomizing our labels entirely.

196
00:11:54,040 --> 00:11:54,873
And what they did was they tried to see 
if a neural network could still learn 

197
00:11:57,910 --> 00:12:01,690
random labels and here's what they 
found.

198
00:12:03,430 --> 00:12:07,510
So as you'd expect when they tested this
neural network with random labels,

199
00:12:07,870 --> 00:12:10,420
as they increase the randomness on the x
axis,

200
00:12:11,590 --> 00:12:12,423
so going from left to right,
this is the original labels before 

201
00:12:14,500 --> 00:12:15,333
randomizing anything,
and then they started randomizing their 

202
00:12:17,680 --> 00:12:18,513
test accuracy gradually decreased.
And this is as expected because we're 

203
00:12:22,030 --> 00:12:25,480
trying to learn something that has 
absolutely no pattern in it.

204
00:12:28,000 --> 00:12:28,833
But then what's really interesting is 
that then they looked at the training 

205
00:12:30,461 --> 00:12:35,410
accuracy and what they found was that 
the neural network was able to,

206
00:12:35,740 --> 00:12:39,670
with 100 percent accuracy,
get the training said correct.

207
00:12:40,360 --> 00:12:41,193
Every single time,
no matter how many random labels they 

208
00:12:44,441 --> 00:12:45,274
introduced,
the training set would always be 

209
00:12:47,321 --> 00:12:48,154
shattered or another words.
Every single example in the training 

210
00:12:50,831 --> 00:12:51,664
side could be perfectly classified.
So this means that modern deep neural 

211
00:12:57,461 --> 00:13:02,461
networks actually have the capacity to 
brute force memorize massive data sets,

212
00:13:03,190 --> 00:13:07,240
even on the size of image net with 
completely random labels.

213
00:13:07,241 --> 00:13:11,110
They're able to memorize every single 
example in that data set.

214
00:13:11,890 --> 00:13:12,723
And this is a very powerful result is it
drives home this point that neural 

215
00:13:16,331 --> 00:13:19,930
networks are really,
really excellent function approximators.

216
00:13:20,650 --> 00:13:21,483
So this also connects back to the 
universal approximation theorem that I 

217
00:13:24,310 --> 00:13:25,143
talked about before,
but they're really good approximators 

218
00:13:27,941 --> 00:13:29,710
for just the single function like I 
said,

219
00:13:30,610 --> 00:13:31,443
which means that we can always create 
this maximum likelihood estimate of our 

220
00:13:33,611 --> 00:13:34,444
data using a neural network such that if
we were given a new data points like 

221
00:13:38,651 --> 00:13:39,484
this purple one on the bottom,
it's easy for us to compute is estimate 

222
00:13:43,070 --> 00:13:43,903
probability estimate output just by 
intercepting it with that maximum 

223
00:13:47,591 --> 00:13:48,424
likelihood estimate,
but that's only if I'm looking at a 

224
00:13:51,431 --> 00:13:54,370
place that we have sufficient training 
data already.

225
00:13:54,400 --> 00:13:55,233
What if I extend this x axes and look at
what the neural network predicts beyond 

226
00:13:58,721 --> 00:14:02,200
that in these locations.
These are actually the locations that we

227
00:14:02,201 --> 00:14:03,730
care about most,
right?

228
00:14:03,731 --> 00:14:04,564
These are the edge cases and driving.
These are the cases that we don't have 

229
00:14:07,901 --> 00:14:11,620
mit,
many or a lot of data that was collected

230
00:14:12,100 --> 00:14:15,040
and these are usually the cases were 
safety.

231
00:14:15,041 --> 00:14:20,041
Critical applications are like are most 
important,

232
00:14:20,590 --> 00:14:21,423
right?
So we need to be able to make sure when 

233
00:14:22,601 --> 00:14:25,030
we sampled in neural network from these 
locations,

234
00:14:26,050 --> 00:14:28,120
are we able to know that the neural 
network,

235
00:14:28,180 --> 00:14:29,013
are we able to get feedback from the 
neural network that it actually doesn't 

236
00:14:30,851 --> 00:14:31,870
know what it's talking about.

237
00:14:34,600 --> 00:14:35,433
So this notion leads nicely into the 
idea of what is known as advertar 

238
00:14:38,590 --> 00:14:43,590
adversarial attacks where I can give you
can give and neural network to images on

239
00:14:44,471 --> 00:14:45,304
the left like this one,
and on the right and adversarial image 

240
00:14:48,760 --> 00:14:52,260
that to a human look exactly the same,
but to the network,

241
00:14:52,261 --> 00:14:56,000
they're incorrectly classified 100 
percent of the time.

242
00:14:56,540 --> 00:14:59,910
So the image on the right shows an 
example of a temple which when I feed to

243
00:14:59,930 --> 00:15:02,510
a neural network and gives me back label
of a temple,

244
00:15:03,830 --> 00:15:04,663
but when I apply some adversarial noise,
it classifies this image incorrectly as 

245
00:15:09,561 --> 00:15:14,010
an ostrich.
So for this,

246
00:15:14,011 --> 00:15:16,590
I'd like to focus on this piece 
specifically,

247
00:15:16,591 --> 00:15:18,590
so to understand the limitations of 
neural networks,

248
00:15:18,610 --> 00:15:19,443
the first thing we have to do is 
actually understand how we can break 

249
00:15:21,541 --> 00:15:22,374
them,
and this perturbed noise is actually 

250
00:15:24,961 --> 00:15:25,794
very

251
00:15:26,660 --> 00:15:29,540
intelligently designed,
so this is not just random noise,

252
00:15:29,900 --> 00:15:30,733
but we're actually modifying pixels in 
specific locations to maximally change 

253
00:15:35,240 --> 00:15:36,073
or mess up or output prediction.
So we want to modify the pixels in such 

254
00:15:39,411 --> 00:15:43,310
a way that we're decreasing our accuracy
as much as possible.

255
00:15:43,790 --> 00:15:46,400
And if you remember back to how we 
actually train our neural networks,

256
00:15:46,401 --> 00:15:50,210
this might sound very similar.
So if you're a member training,

257
00:15:50,211 --> 00:15:53,720
a neural network is simply optimizing 
over our weights feta.

258
00:15:54,560 --> 00:15:55,393
So to do this,
we simply compute the gradients of Feta 

259
00:15:57,111 --> 00:15:57,944
with sorry,
the great into our last function with 

260
00:15:59,600 --> 00:16:03,380
respect to theater,
and we simply perturb our weights in the

261
00:16:03,381 --> 00:16:05,660
direction that will minimize our loss.

262
00:16:07,600 --> 00:16:10,870
Now also remember that when we do this,
we're perturbing theater,

263
00:16:10,920 --> 00:16:14,080
but we're fixing or x in our y.
This is our training label.

264
00:16:14,120 --> 00:16:16,570
Our training data and returning labels.
Now,

265
00:16:16,571 --> 00:16:20,860
for adversarial examples were just 
shuffling the variables a little bit,

266
00:16:20,861 --> 00:16:23,710
so now we want to optimize over the 
image itself,

267
00:16:23,740 --> 00:16:24,573
not the wage,
so we fixed the weights and the target 

268
00:16:27,611 --> 00:16:31,480
label itself and we optimize over the 
image x.

269
00:16:31,690 --> 00:16:32,523
We want to make small changes to that 
image x such that we increased our loss 

270
00:16:36,040 --> 00:16:39,010
as much as possible and we want to go in
the opposite direction of training now,

271
00:16:40,680 --> 00:16:41,513
and these are just some of the 
limitations of neural networks and for 

272
00:16:44,891 --> 00:16:47,900
the remainder of this class,
I want to focus on some of the really,

273
00:16:47,910 --> 00:16:51,580
really exciting new frontiers of deep 
learning that focus on just two of these

274
00:16:52,120 --> 00:16:52,953
specifically.
I want to focus on the notion of 

275
00:16:54,491 --> 00:16:55,324
understanding uncertainty and deep 
neural networks and understanding when 

276
00:16:58,781 --> 00:17:01,780
our model doesn't know what it was 
trained to know.

277
00:17:01,990 --> 00:17:02,823
Maybe because it wasn't.
It didn't receive enough training data 

278
00:17:05,110 --> 00:17:09,120
to support that hypothesis.
And furthermore,

279
00:17:09,121 --> 00:17:13,230
I wanted to focus on this notion of 
learning how to learn models.

280
00:17:13,380 --> 00:17:17,760
Because optimization of neural networks 
is extremely difficult.

281
00:17:17,850 --> 00:17:21,570
It's extremely limited and its current 
nature because they're optimized just to

282
00:17:21,571 --> 00:17:22,404
do a single test.
So what we really want to do is create 

283
00:17:24,001 --> 00:17:28,290
neural networks that are capable of 
performing one task,

284
00:17:28,470 --> 00:17:33,390
but a set of sequences of tasks that are
maybe dependent in some fashion.

285
00:17:35,400 --> 00:17:38,640
So let's start with this notion of 
uncertainty in deep neural networks.

286
00:17:38,641 --> 00:17:39,474
And to do that,
I'd like to introduce this field called 

287
00:17:43,591 --> 00:17:44,520
Bayesian deep learning.

288
00:17:46,840 --> 00:17:48,760
Nope.
To understand beige and deep learning.

289
00:17:48,761 --> 00:17:52,440
Let's understand why we even care about 
uncertainty.

290
00:17:52,500 --> 00:17:57,240
So this should be pretty obvious.
Let's suppose we're given a network that

291
00:17:57,241 --> 00:18:01,590
was trained to distinguish between cats 
and dogs at input.

292
00:18:01,591 --> 00:18:05,400
We're given a lot of testing or training
images of cats and dogs,

293
00:18:05,670 --> 00:18:09,090
and it's simply at the output we're 
producing an output probability of being

294
00:18:09,091 --> 00:18:10,260
a cat or a dog.

295
00:18:12,030 --> 00:18:12,863
Now,
this model is trained on either on only 

296
00:18:14,281 --> 00:18:16,500
cats or dogs.
So if I showed another cat,

297
00:18:16,770 --> 00:18:19,200
it should be very confident in its 
output,

298
00:18:19,800 --> 00:18:20,633
but let's suppose I give it a horse and 
I forced that network because it's the 

299
00:18:24,721 --> 00:18:25,554
same network to produce an output being 
a probability of a cat or a probability 

300
00:18:28,741 --> 00:18:29,574
of a dog.
Now we know that these probabilities 

301
00:18:32,881 --> 00:18:33,714
have to add up to one because that's 
actually the definition that we 

302
00:18:35,341 --> 00:18:39,930
constrain our network to follow.
So that means by definition one of these

303
00:18:39,931 --> 00:18:40,764
categories,
so the network has to produce one of 

304
00:18:42,391 --> 00:18:43,224
these categories,
so the notion of probability and the 

305
00:18:46,081 --> 00:18:48,210
notion of uncertainty are actually very 
different,

306
00:18:48,690 --> 00:18:52,230
but a lot of deep learning practitioners
optin mixed these two ideas,

307
00:18:52,470 --> 00:18:57,060
so uncertainty is not probability.
Neural networks or detect are trained to

308
00:18:57,150 --> 00:19:01,290
detect or produce probabilities at three
output at their output,

309
00:19:01,560 --> 00:19:04,770
but they're not trained to produce 
uncertainty values.

310
00:19:06,050 --> 00:19:08,600
So if we put this horse into the same 
network,

311
00:19:08,870 --> 00:19:12,380
we'll get a set of uncertain of 
probability values that add up to one.

312
00:19:13,580 --> 00:19:14,413
But what we really want to see is we 
want to see a very low uncertainty in 

313
00:19:17,301 --> 00:19:20,330
that very low certainty in that 
prediction.

314
00:19:22,290 --> 00:19:23,123
And one possible way to accomplish this 
in deep learning is through the eyes of 

315
00:19:25,760 --> 00:19:28,200
Bayesian deep learning.
And to understand this,

316
00:19:28,201 --> 00:19:30,780
let's briefly start by formulating our 
problem again.

317
00:19:31,920 --> 00:19:36,060
So first let's go through like the 
variables,

318
00:19:36,061 --> 00:19:36,894
right?
So we want to approximate this variable 

319
00:19:39,721 --> 00:19:40,554
y or output y given some raw data x.
and really what we mean by training is 

320
00:19:44,850 --> 00:19:49,850
we want to find this functional mapping 
f parameterized by our weights data such

321
00:19:50,070 --> 00:19:54,660
that we minimize the loss between our 
predict examples and our true outputs.

322
00:19:54,690 --> 00:19:55,523
Why?
So based on neural networks take a 

323
00:19:58,831 --> 00:20:01,950
different approach to solve this 
problem,

324
00:20:02,340 --> 00:20:05,970
they aim to learn it postier over our 
weights,

325
00:20:06,060 --> 00:20:08,670
given the data.
So they attempt to say,

326
00:20:08,880 --> 00:20:09,713
what is the probability that I see this 
model with these weights given the data 

327
00:20:13,681 --> 00:20:14,514
in my training set.
Now it's called Bayesian deep learning 

328
00:20:16,831 --> 00:20:21,270
because we can simply have a rewrite 
this posterior using bayes rule.

329
00:20:25,090 --> 00:20:25,923
However,
in practice it's rarely possible to 

330
00:20:27,581 --> 00:20:32,080
actually compute this compute,
this bayes rule,

331
00:20:32,140 --> 00:20:36,280
updates,
and it just turns out to be intractable.

332
00:20:36,340 --> 00:20:37,173
So instead we have to find out ways to 
actually approximate it through 

333
00:20:40,511 --> 00:20:41,344
sampling.
So one way that we'll talk about today 

334
00:20:43,510 --> 00:20:47,140
is a very simple notion that we've 
actually already seen in the first.

335
00:20:47,950 --> 00:20:50,230
And it goes back to this idea of using 
dropout.

336
00:20:50,860 --> 00:20:51,693
So if you're a member,
what dropout was dropped out is this 

337
00:20:53,741 --> 00:20:54,574
notion of randomly killing off a certain
percentage of neurons in each of the 

338
00:21:00,220 --> 00:21:03,310
hidden layers.
Now I'm going to tell you not how to use

339
00:21:03,311 --> 00:21:04,144
it as a regular advisor,
but how to use dropbox as a way to 

340
00:21:06,731 --> 00:21:10,060
produce reliable uncertainty measures 
for your neural network.

341
00:21:11,290 --> 00:21:15,940
So to do this,
we have to think of capital t stochastic

342
00:21:15,941 --> 00:21:16,774
passes through our network where each 
stochastic pass performance one 

343
00:21:19,121 --> 00:21:20,290
iteration of dropouts.

344
00:21:20,980 --> 00:21:22,230
Each time you iterate,
dropout,

345
00:21:22,240 --> 00:21:23,073
you're basically just applying a 
Bernoulli mask of ones and Zeros over 

346
00:21:26,441 --> 00:21:29,110
each of your weights.
So going from the left to the right,

347
00:21:29,111 --> 00:21:32,320
you can see our weights,
which is like this matrix here.

348
00:21:32,380 --> 00:21:33,213
Different colors represent the intensity
of that weight and we element wise 

349
00:21:36,851 --> 00:21:39,310
multiply those weights by our Bernoulli 
mask with,

350
00:21:39,610 --> 00:21:42,340
which is just either a one or a zero in 
every location.

351
00:21:43,810 --> 00:21:44,643
The output is a new set of weights with 
certain of those dropped out with 

352
00:21:48,461 --> 00:21:49,294
certain aspects of those dropped out.
Now all we have to do is compute this t 

353
00:21:56,411 --> 00:21:57,244
times capital t times.
We'll get a feta t weights and we use 

354
00:22:01,191 --> 00:22:06,191
those stated t different models to 
actually produce an empirical average of

355
00:22:07,271 --> 00:22:12,190
our output class given the data.
So that's this guy.

356
00:22:13,240 --> 00:22:14,073
Well,
we're actually really interested in why 

357
00:22:14,981 --> 00:22:17,740
I brought this topic up was the notion 
of uncertainty though,

358
00:22:18,280 --> 00:22:23,110
and that's the variance of our 
predictions right there.

359
00:22:24,220 --> 00:22:25,053
So this is a very powerful idea.
All it means is that we can attain 

360
00:22:29,710 --> 00:22:34,050
reliable model uncertainty estimates 
simply by trading our network during run

361
00:22:34,051 --> 00:22:34,884
time with dropout.
And then instead of estimating or 

362
00:22:37,750 --> 00:22:41,200
classifying just a single pass through 
this network at test time,

363
00:22:41,440 --> 00:22:42,273
we'd classify capital t two iterations 
of this network and then use that to 

364
00:22:46,661 --> 00:22:50,410
compute a variance over these outputs.
And that variance gives us an estimation

365
00:22:50,440 --> 00:22:51,280
of our uncertainty.

366
00:22:53,500 --> 00:22:55,930
Not to give you an example of how this 
looks in practice.

367
00:22:56,080 --> 00:22:57,280
Let's look at this,
uh,

368
00:22:57,880 --> 00:22:58,713
this network that was trained to take as
input images of the real world and 

369
00:23:04,090 --> 00:23:07,540
outputs predicted depth maps.
Oh,

370
00:23:07,570 --> 00:23:11,290
it looks like my text was a little off.
That's okay.

371
00:23:12,910 --> 00:23:15,760
So at the output we have a predicted 
death,

372
00:23:15,770 --> 00:23:16,603
not for each pixel to network is 
predicting the depth in the real world 

373
00:23:21,070 --> 00:23:23,740
of that Pixel.
Now,

374
00:23:23,741 --> 00:23:28,270
when we run Bayesian model uncertainty 
using the exact same dropout method that

375
00:23:28,271 --> 00:23:32,850
I just described,
we can see that the end,

376
00:23:32,890 --> 00:23:36,190
the model is most uncertain in some very
interesting locations.

377
00:23:36,550 --> 00:23:37,383
So first of all,
pay attention to that location right 

378
00:23:38,651 --> 00:23:41,360
there.
And if you look where,

379
00:23:41,410 --> 00:23:42,243
where is that location exactly?
It's just the window sill of this car 

380
00:23:46,270 --> 00:23:47,103
and in computer vision windows and 
specular objects are very difficult to 

381
00:23:53,530 --> 00:23:57,710
to basically model because we can't 
actually tell their surface reliably,

382
00:23:57,711 --> 00:23:57,950
right?

383
00:23:57,950 --> 00:24:00,470
So we're seeing the light from actually 
the sky.

384
00:24:00,620 --> 00:24:03,380
We're not actually seeing the surface of
the window in that location,

385
00:24:03,620 --> 00:24:07,730
so it can be very difficult for us to 
model the depth in that place.

386
00:24:08,840 --> 00:24:09,673
Additionally,
we see that the model is very uncertain 

387
00:24:11,600 --> 00:24:12,433
on the edges of the cars because these 
are places where the depth is changing 

388
00:24:17,390 --> 00:24:18,223
very rapidly,
so the prediction may be least accurate 

389
00:24:20,181 --> 00:24:24,470
and these locations.
So having reliable uncertainty estimates

390
00:24:24,710 --> 00:24:25,543
can be an extremely powerful way to 
actually interpret deep learning models 

391
00:24:29,181 --> 00:24:34,181
and also provide human practitioners,
especially in the realm of safe ai,

392
00:24:34,970 --> 00:24:35,803
is the way to interpret the results and 
also trust our results with a certain 

393
00:24:42,441 --> 00:24:43,274
amount or a certain gram of salt.
So for the next and final part of this 

394
00:24:49,021 --> 00:24:49,854
talk,
I'd like to address this notion of 

395
00:24:51,750 --> 00:24:54,900
learning to learn.
So this is a really cool sounding topic.

396
00:24:56,810 --> 00:25:01,810
It aims to basically learn not just a 
single model that's optimized to perform

397
00:25:02,071 --> 00:25:02,904
a single task like we've learned 
basically in all of our lectures 

398
00:25:06,360 --> 00:25:07,193
previous to this one,
but it learns how to learn which model 

399
00:25:10,530 --> 00:25:12,330
to use to train that task.

400
00:25:14,380 --> 00:25:16,600
So first,
let's understand why we might want to do

401
00:25:16,601 --> 00:25:19,690
something like that.
I hope this is pretty obvious to by now,

402
00:25:20,050 --> 00:25:20,883
but humans are not built in a way where 
we're learning where we're executing 

403
00:25:25,811 --> 00:25:28,510
just a single task at a time were 
executing many,

404
00:25:28,511 --> 00:25:29,344
many,
many different tasks and all of these 

405
00:25:32,171 --> 00:25:33,004
tasks are constantly interacting with 
each other in ways that learning one 

406
00:25:37,181 --> 00:25:40,250
task can actually aid speed up or deter 
the learning.

407
00:25:40,251 --> 00:25:45,100
You have another task at any given time,
modern deep neural network architectures

408
00:25:45,101 --> 00:25:45,934
and not like this.
They're optimized for a single task and 

409
00:25:47,381 --> 00:25:48,214
this goes back to the very beginning of 
this talk where we talked about the 

410
00:25:49,961 --> 00:25:54,961
universal approximator and as these 
models become more and more complex,

411
00:25:56,150 --> 00:25:56,983
what ends up happening is that you have 
to have more and more expert knowledge 

412
00:26:00,740 --> 00:26:01,573
to actually build and deploy these 
models in practice and that's exactly 

413
00:26:03,981 --> 00:26:04,814
why all of you are here.
You're here to basically get that 

414
00:26:06,501 --> 00:26:10,730
experience such that you yourselves can 
build these deep learning models.

415
00:26:13,350 --> 00:26:14,183
So what we want is actually an automated
machine learning framework where we can 

416
00:26:18,571 --> 00:26:21,750
actually learn to learn and this 
basically means we want to build a model

417
00:26:21,751 --> 00:26:26,751
that learns which model to use given a 
problem definition.

418
00:26:28,760 --> 00:26:31,850
One example I'd like to just use as an 
illustration of this idea,

419
00:26:31,880 --> 00:26:32,713
so there are many ways that auto ml can 
be accomplished and this is just one 

420
00:26:36,351 --> 00:26:37,184
example of those ways.
So I'd like to focus on this 

421
00:26:39,560 --> 00:26:41,630
illustration here and I'd like to walk 
through it.

422
00:26:41,660 --> 00:26:45,090
It's just a way that we can learn to 
learn.

423
00:26:46,650 --> 00:26:49,650
So this,
this system focuses on two parts.

424
00:26:49,800 --> 00:26:52,830
The first part is the controller rnn in 
red on the left,

425
00:26:53,640 --> 00:26:58,080
and this controller rnn is basically 
just sampling different architectures of

426
00:26:58,081 --> 00:27:01,500
neural networks.
So if you remember in your first lab you

427
00:27:01,501 --> 00:27:04,980
created an rnn that could sample 
different music notes.

428
00:27:05,970 --> 00:27:09,120
This is no different except now we're 
not sampling music notes.

429
00:27:09,330 --> 00:27:12,570
We're sampling an entire neural network 
itself.

430
00:27:12,780 --> 00:27:15,870
So we're sampling parameters that define
that neural network.

431
00:27:16,530 --> 00:27:19,110
So let's call that the architecture or 
the child network.

432
00:27:19,170 --> 00:27:22,830
So that's the network that will actually
be used to solve our task in the end.

433
00:27:24,390 --> 00:27:27,120
So that network has passed on to the 
second bop.

434
00:27:28,680 --> 00:27:30,810
So that network has passed on to the 
second one.

435
00:27:32,280 --> 00:27:37,280
And in that piece we actually used that 
network that was generated by the rnn to

436
00:27:37,681 --> 00:27:41,370
train a model depending on how well that
model did,

437
00:27:42,950 --> 00:27:46,730
we can provide feedback to the rnn such 
that it can produce an even better model

438
00:27:46,790 --> 00:27:47,810
on the next time step.

439
00:27:49,510 --> 00:27:50,343
So let's go into this piece by piece.
Let's look at just the rnn part in more 

440
00:27:53,621 --> 00:27:54,454
detail.
So this is the rnn or the architecture 

441
00:27:56,741 --> 00:27:58,480
generator.
So like I said,

442
00:27:58,481 --> 00:28:02,200
this is very similar to the way that you
are generating songs and your first lab,

443
00:28:02,440 --> 00:28:03,273
except now we're not generating songs,
the timestamps are going from layers on 

444
00:28:07,421 --> 00:28:08,254
the x axis and we're just generating 
parameters or hyper parameters rather 

445
00:28:15,700 --> 00:28:16,533
for each of those layers.
So this is a generator for a 

446
00:28:18,461 --> 00:28:19,294
convolutional neural network because 
we're producing parameters like the 

447
00:28:22,121 --> 00:28:24,140
filter height,
the filter with the stride height,

448
00:28:24,170 --> 00:28:28,000
etc.
So what we can do is we can at each time

449
00:28:28,001 --> 00:28:28,834
step produce a probability distribution 
of over each of these parameters and we 

450
00:28:32,411 --> 00:28:35,770
can essentially just sample and 
architecture or sample of child network.

451
00:28:36,430 --> 00:28:39,850
Once we have that child network,
which I'm Showing right here in blue,

452
00:28:40,570 --> 00:28:45,460
we can train it using our data set that 
we ultimately want to solve.

453
00:28:46,150 --> 00:28:49,390
So we put our training data in and we 
get our predicted labels out.

454
00:28:49,420 --> 00:28:50,253
This is the,
this is the realm that we've been 

455
00:28:51,581 --> 00:28:54,010
dealing with so far in this class,
right?

456
00:28:54,011 --> 00:28:54,844
So we have our,
this is basically what we've seen so 

457
00:28:56,741 --> 00:28:57,574
far.
So this is just a single network and we 

458
00:28:59,051 --> 00:29:01,090
have our training data that we're using 
to train it.

459
00:29:02,680 --> 00:29:07,000
We see how well this does,
depending on the accuracy of this model,

460
00:29:07,510 --> 00:29:08,343
that accuracy is used to provide 
feedback back to the rnn and update how 

461
00:29:13,391 --> 00:29:15,850
it produces or how it generates these 
models.

462
00:29:18,340 --> 00:29:20,860
So let's look at this one more time.
To summarize,

463
00:29:21,490 --> 00:29:23,770
this is an extremely powerful idea.
It's really,

464
00:29:23,771 --> 00:29:27,100
really,
really exciting because it shows that an

465
00:29:27,130 --> 00:29:27,963
rnn can be actually combined in a 
reinforcement learning paradigm where 

466
00:29:31,691 --> 00:29:35,500
the rnn itself is almost like the agent 
in reinforcement learning.

467
00:29:35,530 --> 00:29:40,530
It's learning to make changes to the 
child network architecture.

468
00:29:41,080 --> 00:29:44,740
Depending on how that child network 
performs on a training set,

469
00:29:46,750 --> 00:29:47,583
this means that we're able to create an 
ai system capable of generating brand 

470
00:29:50,531 --> 00:29:55,390
new neural networks specialized to solve
specific tasks rather than just creating

471
00:29:55,391 --> 00:29:59,320
a single neural network that we create 
just to solve that tasks that we want to

472
00:29:59,321 --> 00:30:02,440
create that we want to solve.
Thus,

473
00:30:02,441 --> 00:30:03,274
this has significantly reduced.
The difficulty in optimizing these 

474
00:30:05,861 --> 00:30:06,694
neural networks for our architecture is 
for different tasks and it's also 

475
00:30:10,481 --> 00:30:14,470
reduces the need for expert engineers to
design these architectures,

476
00:30:15,930 --> 00:30:19,080
so this really gets at the heart of 
artificial intelligence.

477
00:30:19,500 --> 00:30:23,520
So when I began this course,
we spoke about what it actually means to

478
00:30:23,521 --> 00:30:24,354
be intelligent and lucy.
I defined this as the ability to take 

479
00:30:27,451 --> 00:30:29,520
information,
process that information,

480
00:30:29,521 --> 00:30:33,510
and use it to inform future decisions.

481
00:30:36,100 --> 00:30:40,270
So the human learning pipeline is not 
restricted to solving just one task at a

482
00:30:40,271 --> 00:30:41,560
time.
Like I mentioned before,

483
00:30:41,561 --> 00:30:42,394
how we learn one task can greatly impact
speed up or even slowed down our 

484
00:30:46,841 --> 00:30:47,674
learning of other tasks and the 
artificial models that we've created 

485
00:30:50,771 --> 00:30:52,930
today.
Simply do not capture this phenomenon.

486
00:30:53,770 --> 00:30:55,900
To reach artificial general 
intelligence.

487
00:30:55,901 --> 00:30:59,560
We need to actually build ai that can 
not only learn a single task,

488
00:31:00,640 --> 00:31:01,473
but also be able to improve its own 
learning and reasoning such that it can 

489
00:31:04,721 --> 00:31:08,260
generalize to sets of related and 
dependent tasks.

490
00:31:09,310 --> 00:31:10,143
I'll leave this with you as a thought 
provoking points that encourage you to 

491
00:31:13,420 --> 00:31:16,500
to all talk to each other on on some 
ways that we can reach this,

492
00:31:16,680 --> 00:31:20,980
this higher order level of intelligence 
that's not just pattern recognition,

493
00:31:21,460 --> 00:31:22,293
but rather a higher order form of 
reasoning and actually thinking about 

494
00:31:26,580 --> 00:31:28,570
about the problems that we're trying to 
solve.

495
00:31:30,150 --> 00:31:35,150
Thank you.
Thank you.

