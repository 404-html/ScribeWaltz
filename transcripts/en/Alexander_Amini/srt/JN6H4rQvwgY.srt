1
00:00:02,490 --> 00:00:04,620
Good morning everyone,
thank you for.

2
00:00:05,100 --> 00:00:10,100
Thank you all for joining us.
This is mit success one nine one,

3
00:00:10,470 --> 00:00:11,303
and we'd like to welcome to welcome you 
to this course on introduction to deep 

4
00:00:13,981 --> 00:00:14,814
learning.
So in this course you'll learn how to 

5
00:00:18,330 --> 00:00:21,690
build remarkable algorithms,
intelligent algorithms,

6
00:00:22,290 --> 00:00:24,840
capable of solving very complex 
problems.

7
00:00:24,841 --> 00:00:25,674
They just a decade ago,
we're not even a feasible to solve and 

8
00:00:30,420 --> 00:00:33,600
let's just start with this notion of 
intelligence.

9
00:00:34,560 --> 00:00:35,393
So at a very high level,
intelligence is the ability to process 

10
00:00:39,330 --> 00:00:44,330
information so that it can be used to 
inform future predictions and decisions.

11
00:00:45,990 --> 00:00:49,500
Now when this intelligence is not 
engineered,

12
00:00:49,830 --> 00:00:54,830
but rather a biological inspiration such
as in humans,

13
00:00:55,580 --> 00:00:57,420
uh,
it's called human intelligence,

14
00:00:57,450 --> 00:00:58,283
but when it's engineered,
we referred to it as artificial 

15
00:00:59,821 --> 00:01:00,654
intelligence.
So this course is a course on deep 

16
00:01:02,701 --> 00:01:03,534
learning,
which is just a subset of artificial 

17
00:01:05,101 --> 00:01:08,430
intelligence.
And really it's just a subset of machine

18
00:01:08,431 --> 00:01:09,264
learning which involves more traditional
methods where we tried to learn a 

19
00:01:14,640 --> 00:01:16,680
representations directly from data.

20
00:01:16,770 --> 00:01:19,350
And we'll talk about this more in detail
later today.

21
00:01:20,370 --> 00:01:24,420
But let me first just start by talking 
about some of the amazing successes that

22
00:01:24,421 --> 00:01:29,280
deep learning has had in the past.
So in 2012,

23
00:01:29,730 --> 00:01:30,563
this competition called image net came 
out which tasks ai researchers to build 

24
00:01:34,501 --> 00:01:38,340
an AI system capable of recognizing 
images,

25
00:01:38,370 --> 00:01:39,203
objects in images,
and there is millions of examples in 

26
00:01:42,391 --> 00:01:43,224
this data set and the winter in 2012 for
the first time ever was a deep learning 

27
00:01:47,641 --> 00:01:48,474
based system and when it came out,
it absolutely shattered all other 

28
00:01:52,260 --> 00:01:55,500
competitors and crushed the competition.
I'm crushed,

29
00:01:55,980 --> 00:01:56,813
crushed the challenge,
and today these deep learning based 

30
00:02:00,121 --> 00:02:00,954
systems have actually surpassed human 
level accuracy on the image net 

31
00:02:04,231 --> 00:02:08,370
challenge and can actually recognize 
images even better than humans can.

32
00:02:10,650 --> 00:02:11,481
Now,
in this class,

33
00:02:11,481 --> 00:02:14,580
you'll actually learn how to build 
complex vision systems,

34
00:02:14,581 --> 00:02:16,620
building a computer that knows how to 
see,

35
00:02:17,070 --> 00:02:17,903
and just tomorrow you'll learn how to 
build an algorithm that will take as 

36
00:02:21,871 --> 00:02:22,704
input x ray images and as output,
it will detect if that person has a 

37
00:02:27,271 --> 00:02:31,650
pneumothorax.
Just from that single input image.

38
00:02:33,150 --> 00:02:33,983
You'll even make the network explained 
to you why it decided to diagnose the 

39
00:02:38,071 --> 00:02:42,480
way a diagnosed by looking inside the 
network and understanding exactly why it

40
00:02:42,481 --> 00:02:47,481
made that decision.
Deep neural networks can also be used to

41
00:02:47,611 --> 00:02:50,760
model sequences where your data points 
are not just single images,

42
00:02:50,761 --> 00:02:54,780
but rather temporarily dependent.
So for this you can think of things like

43
00:02:55,230 --> 00:03:00,230
predicting the stock price,
translating sentences from English to or

44
00:03:00,521 --> 00:03:01,354
even generating new music.
So actually today you'll learn how to 

45
00:03:03,641 --> 00:03:04,474
create and actually you'll create 
yourselves and algorithm that learns 

46
00:03:07,870 --> 00:03:08,703
that first listened to hours of music,
learns the underlying representation of 

47
00:03:13,391 --> 00:03:18,070
the notes that are being played in those
songs and then alerts to build brand new

48
00:03:18,071 --> 00:03:19,780
songs that have never been heard before.

49
00:03:22,390 --> 00:03:23,223
And there are really so many other 
incredible success stories of deep 

50
00:03:25,301 --> 00:03:29,170
learning that I could talk for many 
hours about it and we'll try to cover as

51
00:03:29,170 --> 00:03:31,060
many of these as possible as part of 
this course.

52
00:03:31,510 --> 00:03:32,343
But I just wanted to give you an 
overview of some of the amazing ones 

53
00:03:34,211 --> 00:03:37,090
that we'll be covering as part of the 
labs that you'll be implementing.

54
00:03:38,590 --> 00:03:42,250
And that's really a goal of what we want
you to accomplish as part of this class.

55
00:03:42,310 --> 00:03:43,143
Firstly,
we want to provide you with the 

56
00:03:44,411 --> 00:03:45,244
foundation to do deep learning to 
understand what these algorithms are 

57
00:03:48,941 --> 00:03:50,950
doing underneath the hood,
how they work,

58
00:03:50,951 --> 00:03:51,784
and why they work.
We will provide you with some of the 

59
00:03:54,881 --> 00:03:55,714
practical skills to implement these 
algorithms and deploy them on your own 

60
00:03:58,631 --> 00:03:59,464
machines and we'll talk to you about 
some of the state state of art and 

61
00:04:03,821 --> 00:04:07,840
cutting edge research that's happening 
in a deep learning industries,

62
00:04:07,900 --> 00:04:12,220
deep learning academia institutions.
Finally,

63
00:04:12,280 --> 00:04:13,113
the main purpose of this course is we 
want to build a community here at mit 

64
00:04:17,620 --> 00:04:21,100
that is devoted to advancing the state 
of artificial intelligence.

65
00:04:21,160 --> 00:04:21,993
Advancing a state of deep learning.
As part of this course will cover some 

66
00:04:25,241 --> 00:04:28,570
of the limitations of these algorithms.
There are many.

67
00:04:29,020 --> 00:04:30,940
We need to be mindful of these 
limitations.

68
00:04:30,941 --> 00:04:31,774
So I said we as a community can move 
forward and create more intelligent 

69
00:04:34,691 --> 00:04:35,524
systems.

70
00:04:37,730 --> 00:04:38,563
But before we do that,
let's just start with some 

71
00:04:40,070 --> 00:04:45,070
administrative details in this course.
So this course is a one week course.

72
00:04:45,800 --> 00:04:49,160
Today is the first lecture we meet 
everyday this week,

73
00:04:49,340 --> 00:04:51,320
10:30
AM to 1:30

74
00:04:51,320 --> 00:04:52,153
PM,
and this during this three hour time 

75
00:04:54,381 --> 00:04:55,214
slot,
we're broken down into a one and a half 

76
00:04:57,860 --> 00:05:00,530
hour time slots,
around 50 percent of the course.

77
00:05:00,560 --> 00:05:01,393
Each and each of those half a half 
sections of this course will consist of 

78
00:05:08,570 --> 00:05:10,550
lectures,
which is what you're in right now,

79
00:05:10,850 --> 00:05:11,683
and the second part is the labs where 
you will actually get practice 

80
00:05:13,611 --> 00:05:15,770
implementing what you learn in lectures.

81
00:05:18,500 --> 00:05:20,660
We have an amazing set of lectures lined
up for you,

82
00:05:20,750 --> 00:05:21,583
so today we're going to be talking about
some of the introduction to neural 

83
00:05:24,951 --> 00:05:25,784
networks,
which is really the backbone of deep 

84
00:05:27,111 --> 00:05:27,944
learning.
We're also talking about modeling 

85
00:05:30,111 --> 00:05:30,944
sequence data,
so this is what I was mentioning about 

86
00:05:32,631 --> 00:05:33,464
the temporarily dependent data.
Tomorrow we'll talk about computer 

87
00:05:37,281 --> 00:05:38,114
vision and deep generative models.
We have one of the inventors of 

88
00:05:41,481 --> 00:05:44,720
generative adversarial networks coming 
to give that lecture for us,

89
00:05:44,721 --> 00:05:45,554
so that's going to be a great lecture 
and the day after that we'll touch on 

90
00:05:49,071 --> 00:05:53,570
deep reinforcement learning and some of 
the open challenges in ai and how we can

91
00:05:53,571 --> 00:05:54,404
move forward past this course.
We'll spend final two days of this 

92
00:05:58,910 --> 00:05:59,743
course talking or hearing from some of 
the leading industry representatives 

93
00:06:04,461 --> 00:06:05,294
doing deep learning in their respective 
companies and these are bound to be 

94
00:06:09,471 --> 00:06:11,870
extremely interesting,
extremely exciting,

95
00:06:11,871 --> 00:06:13,610
so I highly recommend attending of these
as well.

96
00:06:16,520 --> 00:06:18,710
For those of you who are taking this 
course for credit,

97
00:06:19,100 --> 00:06:22,550
you have two options to fulfill your 
greatest assignment.

98
00:06:23,330 --> 00:06:24,163
The first option is a project proposal.
It's a one minute project pitch that 

99
00:06:28,311 --> 00:06:33,311
will take place during Friday and for 
this you have to work in groups of three

100
00:06:33,651 --> 00:06:34,484
or four and what you'll be tasked to do 
is just come up with a interesting deep 

101
00:06:37,791 --> 00:06:42,791
learning idea and tried to show some 
sort of results if possible.

102
00:06:42,860 --> 00:06:43,693
We understand that one week is extremely
short to create any type of results or 

103
00:06:47,091 --> 00:06:49,910
even come up with an interesting idea 
for that matter,

104
00:06:50,390 --> 00:06:51,223
but um,
we're going to be giving out some 

105
00:06:52,401 --> 00:06:55,910
amazing prizes,
so including some Nvidia,

106
00:06:55,911 --> 00:07:00,170
Gpu and Google homes,
uh,

107
00:07:00,230 --> 00:07:01,910
on Friday you'll,
like I said,

108
00:07:01,911 --> 00:07:02,744
give a one minute pitch.
There is somewhat of an arts to a 

109
00:07:06,560 --> 00:07:09,620
pitching your idea and just one minute,
even though it's extremely short,

110
00:07:09,950 --> 00:07:12,650
so we will be holding you to a strict 
deadline up that one minute.

111
00:07:14,750 --> 00:07:16,520
The second option is a little more 
boring,

112
00:07:16,790 --> 00:07:20,390
but you'll be able to write a one page 
paper about any deep learning paper that

113
00:07:20,391 --> 00:07:21,224
you find interesting.
And really that's if you can't do the 

114
00:07:23,691 --> 00:07:25,760
project proposal,
you can do that.

115
00:07:29,230 --> 00:07:33,160
This class has a lot of online resources
you can find support on Piazza.

116
00:07:33,161 --> 00:07:37,060
Please post if you have any questions 
about the lectures to labs,

117
00:07:37,600 --> 00:07:39,340
installing any of the software,
et Cetera.

118
00:07:40,780 --> 00:07:41,613
Also,
try to keep up to date with the course 

119
00:07:42,551 --> 00:07:46,030
website where we'll be posting all of 
the lectures,

120
00:07:46,090 --> 00:07:49,480
labs and video recordings online as 
well.

121
00:07:51,920 --> 00:07:52,753
We have an amazing team that you can 
reach out to at any time in case you 

122
00:07:56,121 --> 00:07:56,954
have any problems with anything,
feel free to reach out to any of us and 

123
00:07:59,211 --> 00:08:03,110
we want to give a huge thanks to all of 
our sponsors who without this,

124
00:08:03,200 --> 00:08:06,470
without their support,
this class would simply not happened the

125
00:08:06,471 --> 00:08:07,830
way,
uh,

126
00:08:07,831 --> 00:08:09,020
the way it's happening this year.

127
00:08:11,110 --> 00:08:11,943
So now let's start with the fun stuff 
and let's start by actually asking 

128
00:08:15,461 --> 00:08:19,900
ourselves the question,
why do we even care about deep learning?

129
00:08:20,590 --> 00:08:25,590
So why now and why do we,
why do we even sit in this class today?

130
00:08:27,160 --> 00:08:27,993
So traditional machine learning 
algorithms typically defined sets of 

131
00:08:32,560 --> 00:08:33,393
preprogrammed features and the data and 
they work to extract these features as 

132
00:08:38,051 --> 00:08:40,120
part of their pipeline.
Now,

133
00:08:40,121 --> 00:08:40,954
the key differentiating point of deep 
learning is that it recognizes that in 

134
00:08:43,901 --> 00:08:47,980
many practical situations,
these features can be extremely brittle.

135
00:08:48,970 --> 00:08:49,803
So at deep learning tries to do is learn
these features directly from data as 

136
00:08:54,251 --> 00:08:57,090
opposed to being hand engineered by the 
human.

137
00:08:58,960 --> 00:08:59,793
That is,
can we learn if we want to learn to 

138
00:09:01,451 --> 00:09:02,284
detect faces,
can we first learn automatically from 

139
00:09:04,001 --> 00:09:04,834
data that to detect faces,
we first need to detect edges and the 

140
00:09:07,961 --> 00:09:12,400
image compose these edges together to 
detect eyes and ears,

141
00:09:12,910 --> 00:09:13,743
then compose these eyes and ears 
together to form higher level facial 

142
00:09:16,421 --> 00:09:17,254
structure and this way,
deep learning represents a form of a 

143
00:09:20,140 --> 00:09:20,973
hierarchical model capable of 
representing different levels of 

144
00:09:24,821 --> 00:09:25,654
abstraction in the data.
So actually the fundamental building 

145
00:09:29,591 --> 00:09:30,424
blocks of deep learning,
which are neural networks have actually 

146
00:09:33,281 --> 00:09:36,670
been existing,
have actually existed for decades.

147
00:09:37,060 --> 00:09:39,340
So why are we studying this now?
Well,

148
00:09:39,341 --> 00:09:44,341
there's three key points here.
The first is that data has become

149
00:09:46,320 --> 00:09:49,770
much more pervasive.
We're living in a big data environment.

150
00:09:49,800 --> 00:09:50,633
These algorithms are hungry for more and
more data and accessing that data has 

151
00:09:55,891 --> 00:09:59,280
become easier than ever before.
Second,

152
00:09:59,340 --> 00:10:01,400
these algorithms are massively parallel,
likable,

153
00:10:01,440 --> 00:10:02,273
and can benefit tremendously from modern
gpu architectures that simply just did 

154
00:10:07,531 --> 00:10:10,710
not exist just less than more than a 
decade ago.

155
00:10:11,520 --> 00:10:12,353
And finally,
due to open source tool boxes like 

156
00:10:14,040 --> 00:10:14,873
tensorflow,
building and deploying these algorithms 

157
00:10:18,150 --> 00:10:18,983
has become so streamlined,
so simple that we can teach it in a one 

158
00:10:21,511 --> 00:10:25,710
week course like this,
and it's become extremely deployable for

159
00:10:25,711 --> 00:10:26,544
the massive public.
So let's start with now looking at the 

160
00:10:30,781 --> 00:10:33,000
fundamental building block of deep 
learning.

161
00:10:33,001 --> 00:10:36,900
And that's the perceptron.
This is really just a single neuron in a

162
00:10:36,901 --> 00:10:41,030
neural network.
So yeah,

163
00:10:41,040 --> 00:10:45,640
I do have a perceptron or a single 
neuron is extremely simple.

164
00:10:46,090 --> 00:10:49,630
Let's start by talking about the forward
propagation of information.

165
00:10:49,660 --> 00:10:50,493
Through this data unit,
we defined a set of inputs x one x m on 

166
00:10:54,881 --> 00:10:55,714
the left,
and all we do is we multiply each of 

167
00:10:59,571 --> 00:11:02,780
these inputs by their corresponding wait
theta one through,

168
00:11:02,810 --> 00:11:04,670
say the end,
which are those arrows.

169
00:11:05,840 --> 00:11:06,673
We take this weighted,
we will take this weighted combination 

170
00:11:10,310 --> 00:11:11,143
of all of our inputs,
sum them up and pass them through a 

171
00:11:13,620 --> 00:11:15,470
nonlinear activation function.

172
00:11:17,260 --> 00:11:18,670
And that produces our output.
Why?

173
00:11:18,730 --> 00:11:20,980
It's that simple.
So we have m inputs,

174
00:11:21,100 --> 00:11:21,933
one output number,
and you can see a summarized on the 

175
00:11:25,960 --> 00:11:29,020
righthand side as a mathematics single 
mathematical equation.

176
00:11:30,490 --> 00:11:34,990
But actually I left out one important 
detail that makes the previous slide not

177
00:11:34,991 --> 00:11:35,824
exactly correct,
so I left that this notion of a bias 

178
00:11:39,400 --> 00:11:40,460
biases,
uh,

179
00:11:40,910 --> 00:11:41,743
that green term you see on the left,
and this just represents some way that 

180
00:11:44,651 --> 00:11:45,484
we can allow our model to learn or we 
can allow our activation function to 

181
00:11:49,391 --> 00:11:52,000
shift to the left or right.
So it allows,

182
00:11:52,330 --> 00:11:54,310
if we provide,
it allows us to,

183
00:11:54,311 --> 00:11:55,144
um,
when we have no input features to still 

184
00:11:57,311 --> 00:12:02,311
provide a positive output.
So on this equation,

185
00:12:02,941 --> 00:12:03,774
on the right,
we can actually rewrite this using 

186
00:12:06,151 --> 00:12:10,470
linear Algebra and dod products to make 
this a lot cleaner.

187
00:12:11,310 --> 00:12:12,143
So let's do that.
Let's say x capital x is a vector 

188
00:12:16,741 --> 00:12:18,460
containing all of our inputs.
X One,

189
00:12:18,470 --> 00:12:19,303
three,
Xn capitol theater is now just a vector 

190
00:12:23,401 --> 00:12:25,860
containing all of our status.
They don't want to say to him,

191
00:12:27,120 --> 00:12:27,953
we can rewrite that equation that we had
before is just applying a dot product 

192
00:12:30,391 --> 00:12:33,960
between x and data.
Adding our bias fate is zero.

193
00:12:34,860 --> 00:12:36,570
And applying our nonlinear u,
G,

194
00:12:36,720 --> 00:12:37,553
g.

195
00:12:40,180 --> 00:12:43,200
Now you might be wondering,
since I've mentioned this a couple times

196
00:12:43,201 --> 00:12:46,380
now,
what is this nonlinear function g?

197
00:12:47,100 --> 00:12:49,080
Well,
I said it's the activation function,

198
00:12:49,170 --> 00:12:49,770
but,
uh,

199
00:12:49,770 --> 00:12:53,580
let's see an example of what in practice
ge actually could be.

200
00:12:54,800 --> 00:12:58,050
So one very popular activation function 
is the sigmoid function.

201
00:12:58,410 --> 00:13:00,990
You can see a plot of it here on the 
bottom right,

202
00:13:02,250 --> 00:13:04,230
and this is a function that takes its 
input.

203
00:13:05,070 --> 00:13:05,903
Any real number on the x axis and 
transforms it into an output between 

204
00:13:09,031 --> 00:13:11,620
zero and one,
because I'll,

205
00:13:11,621 --> 00:13:13,410
I'll put to this function are between 
zero and one.

206
00:13:13,411 --> 00:13:14,244
It makes it a very popular choice in 
deep learning to represent 

207
00:13:16,621 --> 00:13:17,520
probabilities.

208
00:13:20,410 --> 00:13:21,243
In fact,
there are many types of nonlinear 

209
00:13:22,930 --> 00:13:25,060
activation functions in deep neural 
networks.

210
00:13:25,360 --> 00:13:28,510
And here's some of the common ones 
throughout this presentation.

211
00:13:28,511 --> 00:13:29,344
You'll also see tensor flow code 
snippets like the ones you see on the 

212
00:13:31,961 --> 00:13:32,794
bottom here,
since we'll be using tensorflow for our 

213
00:13:33,971 --> 00:13:34,804
labs.
And well this is some way that I can 

214
00:13:36,761 --> 00:13:37,594
provide to you to kind of link the 
material in our lectures with what 

215
00:13:40,181 --> 00:13:44,710
you'll be implementing in labs.
So the sigmoid activation function,

216
00:13:44,711 --> 00:13:46,210
which I talked about in the previous 
slide,

217
00:13:46,390 --> 00:13:49,730
now on the left,
is a.

218
00:13:49,750 --> 00:13:50,950
it's just a function,
like I said,

219
00:13:50,951 --> 00:13:54,370
it's commonly used to produce 
probability outputs.

220
00:13:54,700 --> 00:13:58,420
Each of these activation functions has 
their own advantages and disadvantages.

221
00:13:58,740 --> 00:13:59,573
Underwrite a very common activation 
function is rectified linear unit or 

222
00:14:02,570 --> 00:14:03,403
value.
This function is very popular because 

223
00:14:05,981 --> 00:14:08,950
it's extremely simple to compute.
It's piecewise linear.

224
00:14:08,951 --> 00:14:11,890
It's zero before with inputs less than 
zero,

225
00:14:12,120 --> 00:14:17,120
it's x with any input greater than zero,
and the gradients are zero or one with a

226
00:14:18,281 --> 00:14:20,020
single nonlinearity at the origin.

227
00:14:22,260 --> 00:14:25,890
And you might be wondering why we even 
need activation functions.

228
00:14:25,891 --> 00:14:29,310
Why can't we just take our dod product 
at our bias and that's our output.

229
00:14:29,670 --> 00:14:30,503
Why do we need the activation function?
Activation functions introduce 

230
00:14:33,570 --> 00:14:34,403
nonlinearities into the network.
That's the whole point of why 

231
00:14:37,110 --> 00:14:37,943
activations themselves are nonlinear.
We want to model nonlinear data in the 

232
00:14:44,101 --> 00:14:46,820
world because the world is extremely 
nonlinear.

233
00:14:48,030 --> 00:14:51,320
Let's suppose I gave you this,
this plot green and red points,

234
00:14:51,321 --> 00:14:54,830
and I asked you to draw a single line,
not a curve.

235
00:14:54,831 --> 00:14:58,340
Just align between the green and red 
points to separate them perfectly.

236
00:14:58,880 --> 00:14:59,713
You would find this really difficult and
probably you could get as best as 

237
00:15:02,121 --> 00:15:03,860
something like this.
Now,

238
00:15:03,861 --> 00:15:08,360
if your activation function and your 
deep neural network was linear century,

239
00:15:08,390 --> 00:15:10,730
just composing linear functions with 
linear functions,

240
00:15:10,731 --> 00:15:11,564
your output will always be linear,
so the most complicated deep neural 

241
00:15:14,121 --> 00:15:15,800
network,
no matter how big or how deep,

242
00:15:16,150 --> 00:15:20,240
if the activation function is linear,
your output can only look like this,

243
00:15:20,780 --> 00:15:25,550
but once we introduced nonlinearities,
our network is extremely more.

244
00:15:25,730 --> 00:15:28,790
As the capacity of our network has 
extremely increased,

245
00:15:28,970 --> 00:15:32,000
we're now able to model much more 
complex functions.

246
00:15:32,270 --> 00:15:33,103
We're able to draw decision boundaries 
that were not possible with only linear 

247
00:15:36,171 --> 00:15:40,250
activation functions.
Let's understand this with a very simple

248
00:15:40,251 --> 00:15:41,084
example.
Imagine I gave you a train network like 

249
00:15:43,221 --> 00:15:44,450
the one we saw before.
Sorry,

250
00:15:44,451 --> 00:15:45,284
I trained perceptron not in network yet.
Just a single node and the weights are 

251
00:15:49,731 --> 00:15:50,451
on the top,
right,

252
00:15:50,451 --> 00:15:51,284
so status zero is one and the state of 
vector is three and negative to the 

253
00:15:56,451 --> 00:15:59,330
network has two inputs,
x one and x two,

254
00:15:59,420 --> 00:16:00,253
and if we want to get the output,
all we have to do is apply the same 

255
00:16:02,811 --> 00:16:06,590
story as before.
So we apply the product of x and Theta,

256
00:16:07,010 --> 00:16:09,830
we add the bias and apply or 
nonlinearity,

257
00:16:10,520 --> 00:16:12,200
but let's take a look at what's actually
inside.

258
00:16:12,201 --> 00:16:14,880
Before we apply that nonlinearity,
this little,

259
00:16:14,910 --> 00:16:19,910
it looks a lot like just a two d line 
because we have two inputs and it is.

260
00:16:20,570 --> 00:16:24,110
We can actually plot this line way 
equals zero in feature space.

261
00:16:24,111 --> 00:16:26,660
So this is space where I'm plotting x 
one,

262
00:16:26,750 --> 00:16:31,750
one of our features on the x axis and x 
to the other feature.

263
00:16:32,151 --> 00:16:34,190
On the y axis,
we plot that line.

264
00:16:34,220 --> 00:16:38,930
It's just a decision boundary separating
our entire space into two sub spaces.

265
00:16:40,220 --> 00:16:41,570
Now,
if I give you a new point,

266
00:16:41,990 --> 00:16:43,040
negative one,
two,

267
00:16:43,550 --> 00:16:45,860
and plotted on the sub and this feature 
space,

268
00:16:46,160 --> 00:16:48,140
depending on which side of the line of 
false on,

269
00:16:48,380 --> 00:16:49,213
I can automatically determine whether 
our output is less than zero or greater 

270
00:16:52,161 --> 00:16:52,994
than zero.
Since our line represents a decision 

271
00:16:54,591 --> 00:16:59,180
boundary equal to zero,
now we can follow the mass on the bottom

272
00:16:59,181 --> 00:17:02,690
and see that computing the inside of 
this activation function.

273
00:17:02,810 --> 00:17:07,810
We get one minus three minus two,
sorry minus four,

274
00:17:10,610 --> 00:17:11,443
and we get minus six at the output 
before we applied the activation 

275
00:17:14,601 --> 00:17:14,960
function.

276
00:17:14,960 --> 00:17:18,080
Once we apply the activation function,
we get zero point zero,

277
00:17:18,081 --> 00:17:18,914
$0 to so it negative what was applied to
the activation functionalists negative 

278
00:17:23,841 --> 00:17:27,890
because we fell on the negative piece of
this sub space.

279
00:17:29,870 --> 00:17:31,520
Well,
if we remember with the sigmoid function

280
00:17:31,521 --> 00:17:32,354
actually defines our space into two 
parts greater than one point five and 

281
00:17:35,361 --> 00:17:36,194
less than point five.
Since we're modeling probabilities and 

282
00:17:37,641 --> 00:17:38,474
everything is between zero and one.
So actually our decision boundary where 

283
00:17:42,380 --> 00:17:45,710
the input to our network equals zero.
Sorry,

284
00:17:45,711 --> 00:17:46,340
the.
Sorry.

285
00:17:46,340 --> 00:17:48,530
The input to our activation function 
equals zero,

286
00:17:49,200 --> 00:17:50,033
corresponds to the output of our 
activation function being greater than 

287
00:17:53,371 --> 00:17:54,204
or less than point five.
So now that we have an idea of what a 

288
00:17:59,011 --> 00:17:59,844
perceptron is,
let's just start out by understanding 

289
00:18:02,431 --> 00:18:03,264
how we can compose these perceptrons 
together to actually build neural 

290
00:18:07,531 --> 00:18:08,364
networks.
And let's see how this all comes 

291
00:18:10,731 --> 00:18:12,740
together.
So let's revisit our previous diagram up

292
00:18:12,741 --> 00:18:17,300
the perceptron know if there's a few 
things that you learned from this class,

293
00:18:17,330 --> 00:18:20,450
let this be one of them and we'll keep 
repeating it over and over.

294
00:18:21,410 --> 00:18:24,080
In deep learning,
you do a dot product,

295
00:18:24,230 --> 00:18:26,840
you apply a bias and you add your non 
linearity.

296
00:18:27,290 --> 00:18:30,740
You keep repeating that many,
many times for each node,

297
00:18:30,770 --> 00:18:34,400
each neuron in your neural network,
and that's a neural network.

298
00:18:36,090 --> 00:18:40,200
So let's simplify this diagram a little.
I removed the bias since we are going to

299
00:18:40,201 --> 00:18:42,930
always have that and we just take her 
for granted from Nolan,

300
00:18:43,290 --> 00:18:44,123
I'll remove all of the weight labels for
simplicity and note that z is just the 

301
00:18:50,131 --> 00:18:50,964
input to our activation function.
So that's just the dot product plus our 

302
00:18:54,721 --> 00:18:57,360
bias.
If we want the output of the network,

303
00:18:57,361 --> 00:18:58,194
why we simply take z and we apply or 
nonlinearity like before we wanted to 

304
00:19:03,081 --> 00:19:06,070
find a multi output perceptron.
It's very simple.

305
00:19:06,071 --> 00:19:08,950
We just added another perceptron.
Now we have two outputs.

306
00:19:08,951 --> 00:19:13,360
Y One and y two.
Each one has weight matrices,

307
00:19:13,450 --> 00:19:18,220
has weight factor theta corresponding to
the weight of each of the inputs.

308
00:19:22,820 --> 00:19:25,880
Now let's suppose we want to go the next
step deeper.

309
00:19:26,900 --> 00:19:29,750
We want to create now a single layer 
neural network.

310
00:19:31,400 --> 00:19:34,310
Single layered neural networks are 
actually not deep networks yet.

311
00:19:34,311 --> 00:19:36,920
They're only.
They're still shallow networks.

312
00:19:36,921 --> 00:19:37,754
There are only one layer deep,
but let's look at the single layer 

313
00:19:40,720 --> 00:19:45,200
neural network where now all we do is we
have one hidden layer between our inputs

314
00:19:45,201 --> 00:19:46,034
and outputs.
We call this hidden layer because it's 

315
00:19:49,280 --> 00:19:54,280
states are not directly observable.
They're not directly forced by by the AI

316
00:19:55,161 --> 00:19:58,610
designer.
We only enforce the inputs and outputs.

317
00:19:58,640 --> 00:19:59,473
Typically the states in the middle are 
hidden and since we now have a 

318
00:20:03,951 --> 00:20:04,784
transformation to go from our intimate 
space to our hidden hidden layer space 

319
00:20:08,210 --> 00:20:11,030
and from our hidden layer space to our 
output layer space,

320
00:20:11,720 --> 00:20:14,260
we actually need to weight matrices,
data,

321
00:20:14,280 --> 00:20:18,560
one end data to corresponding to the 
weight matrices of each layer.

322
00:20:21,200 --> 00:20:23,300
Now,
if we look at just a single unit in that

323
00:20:23,301 --> 00:20:25,790
hidden layer,
it's the exact same story as before.

324
00:20:25,791 --> 00:20:26,624
It's one perceptron.
We take it stopped product with all of 

325
00:20:28,911 --> 00:20:33,380
the exits that came before it.
And we apply A.

326
00:20:33,380 --> 00:20:34,780
Sorry,
we take the dot product of the exes that

327
00:20:34,781 --> 00:20:36,780
came before with the weight matrices.
Fate,

328
00:20:36,781 --> 00:20:38,540
is that a one?
In this case,

329
00:20:39,370 --> 00:20:40,203
we apply a bias to get [inaudible].
And if we were to look at a different 

330
00:20:43,790 --> 00:20:45,140
hidden unit,
let's say z three,

331
00:20:45,141 --> 00:20:48,940
instead we would just take different 
weight matrices,

332
00:20:48,970 --> 00:20:50,070
different,
uh,

333
00:20:50,440 --> 00:20:53,110
our dod product would change,
our bias would change,

334
00:20:53,380 --> 00:20:55,420
but it does,
I see change,

335
00:20:55,480 --> 00:20:57,520
which means it's activation would also 
be different.

336
00:20:58,780 --> 00:20:59,613
So from now on,
I'm going to use this symbol to denote 

337
00:21:02,350 --> 00:21:04,180
what is called as a fully connected 
layer.

338
00:21:04,181 --> 00:21:05,650
And that's what we've been talking about
so far,

339
00:21:05,651 --> 00:21:06,484
so that's every note in one layer is 
connected to every node and another 

340
00:21:09,491 --> 00:21:11,500
layer by these weight matrices.

341
00:21:12,100 --> 00:21:12,933
And this is really just for simplicity,
so I don't have to keep redrawing those 

342
00:21:15,041 --> 00:21:17,320
lines.
Now,

343
00:21:17,321 --> 00:21:19,210
if we want to create a deep neural 
network,

344
00:21:20,050 --> 00:21:20,883
all we do is keep stalking these layers 
and fully connected weights between the 

345
00:21:24,521 --> 00:21:25,930
layers.
It's that simple.

346
00:21:26,200 --> 00:21:27,033
But the underlying building block is 
that single perceptron said single dod 

347
00:21:30,461 --> 00:21:33,220
product nonlinearity and bias.
That's it.

348
00:21:34,810 --> 00:21:38,770
So this is really incredible because 
something so simple at the foundation is

349
00:21:39,010 --> 00:21:41,890
able to create such incredible 
algorithms.

350
00:21:42,460 --> 00:21:43,293
And now let's see an example of how we 
can actually apply neural networks to a 

351
00:21:46,660 --> 00:21:51,370
very important question that I know you 
are all extremely worried about.

352
00:21:51,400 --> 00:21:55,000
You care a lot about.
Here's the question you want to build an

353
00:21:55,001 --> 00:21:58,090
ai system that answers the following 
question.

354
00:21:58,120 --> 00:22:00,400
Will I pass this class?
Yes or no?

355
00:22:00,580 --> 00:22:01,413
One or zero is the output to do this.
Let's start by defining a simple to 

356
00:22:06,581 --> 00:22:07,450
feature model.

357
00:22:07,600 --> 00:22:10,180
One feature is the number of lectures 
that you attend.

358
00:22:10,710 --> 00:22:13,810
Second feature is the number of hours 
that you spend on your final project.

359
00:22:15,460 --> 00:22:18,010
Let's plot this data.
In our feature space.

360
00:22:18,440 --> 00:22:20,740
We apply greenpoint's of people who 
pass.

361
00:22:20,950 --> 00:22:23,800
Red Points are people that fail.
We want to know,

362
00:22:23,801 --> 00:22:25,210
given a new person,
this guy,

363
00:22:26,800 --> 00:22:27,633
he spent a day,
spent five hours on their final project 

364
00:22:32,290 --> 00:22:33,123
and went to four lectures.
We want to know did that person pass or 

365
00:22:36,671 --> 00:22:39,700
fail the class and we want to build a 
neural network that will determine this.

366
00:22:40,600 --> 00:22:43,870
So let's do it.
We have two inputs.

367
00:22:43,930 --> 00:22:45,160
One is for the others.
Five,

368
00:22:45,670 --> 00:22:46,503
we have one hidden layer with three 
units and we want to see the final 

369
00:22:49,241 --> 00:22:51,190
output probability of passing this 
class,

370
00:22:51,191 --> 00:22:54,220
and we computed as zero point one or 10 
percent.

371
00:22:55,270 --> 00:22:58,600
That's really bad news because actually 
this person did pass the class.

372
00:22:58,960 --> 00:23:02,510
They passed it with probability one.
Nope.

373
00:23:02,540 --> 00:23:05,390
Can anyone tell me why the neural 
network got this?

374
00:23:05,680 --> 00:23:07,730
Such so wrong?
Why did it do this?

375
00:23:08,510 --> 00:23:09,343
Yeah,
it is.

376
00:23:11,570 --> 00:23:12,403
Exactly.

377
00:23:12,780 --> 00:23:16,140
So this network has never been trained.
It's never seen any data.

378
00:23:16,800 --> 00:23:19,500
It's basically a baby.
It's never learned anything.

379
00:23:19,890 --> 00:23:22,620
So we can't expect it to solve a problem
and knows nothing about.

380
00:23:23,400 --> 00:23:24,233
So to do this,
to tackle this problem with training a 

381
00:23:26,611 --> 00:23:28,920
neural network,
we have to first define a couple things.

382
00:23:28,921 --> 00:23:29,754
So first we'll talk about the loss.
The loss of a network basically tells 

383
00:23:36,150 --> 00:23:41,150
our algorithm or our model how wrong our
predictions are from the ground truth.

384
00:23:43,830 --> 00:23:44,663
So you can think of this as a between 
our predicted output and our actual 

385
00:23:48,501 --> 00:23:50,540
output.
If the two are very close,

386
00:23:50,541 --> 00:23:53,270
if we predict something that is very 
close to the true output,

387
00:23:53,510 --> 00:23:57,860
our loss is very low.
If we predict something that is very far

388
00:23:58,520 --> 00:24:01,010
in a high level sense,
far like in distance,

389
00:24:02,120 --> 00:24:02,953
then our loss is very high and we want 
to minimize this from happening as much 

390
00:24:05,901 --> 00:24:06,734
as possible.

391
00:24:07,760 --> 00:24:08,593
Now,
let's assume we were not given just one 

392
00:24:09,201 --> 00:24:10,034
data point one student,
but we're given a whole class of 

393
00:24:11,631 --> 00:24:13,550
students.
So as previous data,

394
00:24:13,551 --> 00:24:17,690
I use this entire class from last year 
and if we want to quantify what's called

395
00:24:17,691 --> 00:24:18,524
the empirical loss,
now we care about how the model did on 

396
00:24:21,531 --> 00:24:24,530
average over the entire Dataset.
Not for just a single student,

397
00:24:24,560 --> 00:24:26,960
but across the entire data set and how 
we do that is very simple.

398
00:24:26,961 --> 00:24:29,510
We just take the average of the loss of 
each data point.

399
00:24:30,040 --> 00:24:33,560
If we have end students,
it's the average over end data points.

400
00:24:34,790 --> 00:24:36,680
This has other names besides empirical 
last.

401
00:24:36,681 --> 00:24:38,660
Sometimes people call it the objective 
function,

402
00:24:38,900 --> 00:24:40,480
the cost function,
et cetera.

403
00:24:42,110 --> 00:24:44,030
All of these terms are completely the 
same thing.

404
00:24:45,770 --> 00:24:46,603
Now,
if we look at the problem with binary 

405
00:24:47,241 --> 00:24:48,074
classification,
predicting if you pass or fail this 

406
00:24:49,551 --> 00:24:50,720
class,
yes or no,

407
00:24:50,750 --> 00:24:51,583
one or zero,
we can actually use something that's 

408
00:24:54,591 --> 00:24:56,990
called the softmax cross entropy loss.

409
00:24:58,340 --> 00:24:59,173
Now,
for those of you who aren't familiar 

410
00:24:59,811 --> 00:25:00,644
with cross entropy or entropy,
this is an extremely powerful notion 

411
00:25:04,491 --> 00:25:05,324
that was actually developed,
were first introduced here at mit over 

412
00:25:08,541 --> 00:25:12,880
50 years ago by Claude Shannon.
Uh,

413
00:25:12,890 --> 00:25:14,990
in his master's thesis.
Like I said,

414
00:25:14,991 --> 00:25:15,824
this was 50 years ago.
It's huge in the field of signal 

415
00:25:18,471 --> 00:25:19,304
processing thermodynamics.
Really all of our computer science had 

416
00:25:21,821 --> 00:25:24,980
seen in information theory.
Now,

417
00:25:25,040 --> 00:25:27,950
instead of predicting a single one or 
zero output,

418
00:25:27,951 --> 00:25:30,290
yes or no,
let's suppose we want to predict a,

419
00:25:30,320 --> 00:25:32,630
um,
continuous valued function,

420
00:25:32,930 --> 00:25:33,763
not will I pass this class,
but what's the grade that I will get it 

421
00:25:37,251 --> 00:25:38,084
as a percentage,
let's say zero to 100 now we're no 

422
00:25:39,501 --> 00:25:40,334
longer limited to zero to one,
but can actually output any real number 

423
00:25:43,311 --> 00:25:47,030
on the number line.
Now instead of using cross entropy,

424
00:25:47,040 --> 00:25:49,370
we might want to use a different loss.
And for this,

425
00:25:49,371 --> 00:25:50,204
let's think of something like a mean 
squared error loss where as your 

426
00:25:52,401 --> 00:25:55,220
predicted and you're true output 
diverged from each other,

427
00:25:55,700 --> 00:25:58,280
the loss increases as a quadratic 
function.

428
00:26:01,140 --> 00:26:01,973
Okay,
great.

429
00:26:02,010 --> 00:26:02,843
So now let's put this new loss 
information to the test and actually 

430
00:26:06,061 --> 00:26:09,810
learn how we can train a neural network 
like quantifying it's lost.

431
00:26:12,640 --> 00:26:16,840
And really if we go back to what the 
loss is at the very high level,

432
00:26:16,870 --> 00:26:20,020
the last tells us how the network is 
performing,

433
00:26:20,021 --> 00:26:20,854
right?
Did last tells us the accuracy of the 

434
00:26:22,871 --> 00:26:23,704
network on a set of examples.
And what we want to do is basically 

435
00:26:26,291 --> 00:26:29,740
minimize the loss over our entire 
training set.

436
00:26:31,540 --> 00:26:32,373
Really,
we want to find the set of parameters 

437
00:26:34,571 --> 00:26:39,571
data such that that loss jff data,
that's our empirical loss is minimum.

438
00:26:40,900 --> 00:26:45,720
So remember jff data takes us data and 
data's just our weights.

439
00:26:45,780 --> 00:26:48,150
So these are the things that actually 
define our network.

440
00:26:53,660 --> 00:26:57,710
Remember that the last is just a 
function of these weights.

441
00:26:58,160 --> 00:27:01,520
If we want to think about the process of
training,

442
00:27:02,210 --> 00:27:05,000
we can imagine this landscape.
So if we only have two weights,

443
00:27:05,001 --> 00:27:05,834
we can plot this nice diagram like this,
Beta zero and Beta one or two weights 

444
00:27:10,460 --> 00:27:12,320
there on the floor.
They're on the planer.

445
00:27:12,321 --> 00:27:14,920
Access on the bottom,
JFK to zero,

446
00:27:14,930 --> 00:27:18,650
and three to one are plotted on the the 
Z axis.

447
00:27:20,050 --> 00:27:24,170
What we want to do is basically find the
minimum of this loss of this landscape.

448
00:27:24,320 --> 00:27:28,010
If we can find the minimum,
then this tells us where our loss is,

449
00:27:28,011 --> 00:27:30,750
the smallest,
and this tells us where theater,

450
00:27:31,120 --> 00:27:31,953
where or what values of Beta zero and 
Beta one we can use to attain that 

451
00:27:36,291 --> 00:27:40,720
minimum loss.
So how do we do this?

452
00:27:41,650 --> 00:27:45,100
Well,
we start with a random guests.

453
00:27:45,230 --> 00:27:47,410
We'd pick a point say to zero state of 
one,

454
00:27:47,411 --> 00:27:48,244
and we start there,
we compute the gradients of this point 

455
00:27:51,520 --> 00:27:54,340
on the last landscape.
That's Dj de Seto.

456
00:27:54,830 --> 00:27:58,570
It's how the loss is changing with 
respect to each of the weights.

457
00:28:00,260 --> 00:28:04,280
Now this gradient tells us the direction
of highest ascent,

458
00:28:04,490 --> 00:28:05,323
not decent.
So this is telling us the direction 

459
00:28:06,830 --> 00:28:08,660
going towards the top of the mountain.

460
00:28:10,120 --> 00:28:12,430
So let's take a small step in the 
opposite direction.

461
00:28:12,910 --> 00:28:16,270
So we negate our gradient and we adjust 
our weight.

462
00:28:16,271 --> 00:28:17,104
So should we step into the opposite 
direction of that gradient such that we 

463
00:28:19,781 --> 00:28:24,781
move continuously towards the lowest 
point in this landscape until we finally

464
00:28:25,811 --> 00:28:28,240
converged at a local minima.
And then we just stop.

465
00:28:29,820 --> 00:28:31,620
So let's summarize this with some pseudo
code.

466
00:28:31,621 --> 00:28:35,850
So we randomly initialized our weights.
We loop until convergence the following.

467
00:28:36,450 --> 00:28:37,283
We compute the gradient at that point,
and when simply we apply this update 

468
00:28:40,471 --> 00:28:45,471
rule with the update takes as input the 
negative gradient.

469
00:28:48,800 --> 00:28:51,350
Now let's look at this term here.
This is the gradient.

470
00:28:51,410 --> 00:28:52,243
Like I said,
it explains how the loss changes with 

471
00:28:54,561 --> 00:28:55,394
respect to each weight and the network,
but I never actually told you how to 

472
00:28:58,701 --> 00:29:02,570
compute this and this is actually a big,
a big issue in neural networks.

473
00:29:02,660 --> 00:29:03,493
I've just kind of took it for granted.
So now let's talk about this process of 

474
00:29:06,261 --> 00:29:07,094
actually computing this gradient because
without that gradient you're kind of 

475
00:29:09,351 --> 00:29:12,440
helpless.
You have no idea which way down is,

476
00:29:12,470 --> 00:29:14,390
you don't know where to go and your 
landscape.

477
00:29:16,140 --> 00:29:18,330
So let's consider a very simple neural 
network,

478
00:29:18,630 --> 00:29:20,850
probably the simplest neural network in 
the world.

479
00:29:21,450 --> 00:29:25,380
It contains one hidden units,
one hidden layer and one output unit.

480
00:29:27,760 --> 00:29:32,760
And we want to compute the gradient of 
our loss j of data with respect to theta

481
00:29:33,251 --> 00:29:34,084
to just stay at a two for now.
So this tells us how a small change in 

482
00:29:38,051 --> 00:29:40,600
theater to will impact our final loss at
the output.

483
00:29:41,080 --> 00:29:41,913
So let's write this out as a derivative.
We can start by just applying a chain 

484
00:29:46,721 --> 00:29:51,721
rule because jff data is dependent on 
why,

485
00:29:52,000 --> 00:29:52,833
right?
So first we want to back propagate 

486
00:29:54,430 --> 00:29:58,140
through why our output all the way back 
to theater.

487
00:29:58,150 --> 00:30:02,410
To we can do this because why are 
output,

488
00:30:02,470 --> 00:30:07,470
why is only dependent on the input and 
data too.

489
00:30:07,930 --> 00:30:08,763
That's it,
so we're able to just from that 

490
00:30:10,391 --> 00:30:12,580
perceptron equation that we wrote on the
previous slide,

491
00:30:12,760 --> 00:30:13,593
computer closed form,
great end or close from derivative of 

492
00:30:16,391 --> 00:30:17,224
that function.
Now let's suppose I change state of two 

493
00:30:20,541 --> 00:30:23,860
to three to one and I want to compute 
the same thing,

494
00:30:23,861 --> 00:30:26,980
but now for the previous layer and the 
previous wait,

495
00:30:28,830 --> 00:30:31,780
all we need to do is is a supply chain 
role.

496
00:30:31,781 --> 00:30:32,614
One more time back propagate those 
gradients that we previously computed 

497
00:30:36,010 --> 00:30:39,130
one layer further.
It's the same story.

498
00:30:39,131 --> 00:30:42,670
Again,
we can do this for the same reason.

499
00:30:42,790 --> 00:30:43,623
This is because Z,
one are hidden state is only dependent 

500
00:30:49,570 --> 00:30:54,570
on our previous and put x and that 
single weight Fado one.

501
00:30:57,020 --> 00:30:57,853
Now the process of backpropagation is 
basically you repeat this process over 

502
00:31:03,681 --> 00:31:04,514
and over again for every weight in your 
network until you compute that gradient 

503
00:31:07,520 --> 00:31:08,353
Dj d theater,
and you can use that as part of your 

504
00:31:10,491 --> 00:31:13,700
optimization process to find your local 
minimum.

505
00:31:16,040 --> 00:31:16,910
Now,
in theory,

506
00:31:16,970 --> 00:31:19,040
that sounds pretty simple.
I hope.

507
00:31:19,240 --> 00:31:20,073
I mean,
we just talked about some basic chain 

508
00:31:21,140 --> 00:31:21,973
rules,
but let's actually touch on some 

509
00:31:23,810 --> 00:31:28,250
insights on training these networks and 
computing backpropagation and practice.

510
00:31:28,820 --> 00:31:30,650
Now,
the picture I showed you before,

511
00:31:30,651 --> 00:31:33,500
it's not really accurate for modern deep
neural network architectures.

512
00:31:33,800 --> 00:31:37,760
Modern deep neural network architectures
are extremely non convex.

513
00:31:38,450 --> 00:31:42,380
This is an illustration or a 
visualization of the landscape like I've

514
00:31:42,381 --> 00:31:43,214
planted before,
but have a real deep neural network of 

515
00:31:45,621 --> 00:31:46,454
resonate 50 to be precise.
This was actually taken from a paper 

516
00:31:50,181 --> 00:31:51,014
published about a month ago,
was the author's attempt to visualize 

517
00:31:54,710 --> 00:31:55,543
the last landscape,
to show how difficult gradient descent 

518
00:31:58,131 --> 00:32:00,740
can actually be.
So if there's a possibility that you can

519
00:32:00,741 --> 00:32:02,780
get lost in any one of these local 
Minima,

520
00:32:02,781 --> 00:32:05,270
there's no guarantee that you will 
actually find a true global minimum.

521
00:32:07,030 --> 00:32:07,863
So let's recall that update equation 
that will we defined during gradient 

522
00:32:09,951 --> 00:32:10,784
descent.

523
00:32:11,390 --> 00:32:14,810
So take a look at this term here.
This is the learning rate.

524
00:32:14,870 --> 00:32:15,703
I didn't talk too much about it,
but this basically determines how large 

525
00:32:19,970 --> 00:32:20,803
of a step we take in the direction of 
our gradient and in practice setting 

526
00:32:24,801 --> 00:32:26,270
this learning great.
It's just a number,

527
00:32:26,300 --> 00:32:27,133
but setting it can be very difficult if 
we set the learning rate too low than 

528
00:32:31,641 --> 00:32:32,474
the model get may get stuck in a local 
minima and may never actually find its 

529
00:32:35,121 --> 00:32:38,460
way out of that local Minima ps at the 
bottom of local man.

530
00:32:38,461 --> 00:32:41,270
Well obviously gradient is zero,
so it was just going to stop moving.

531
00:32:42,500 --> 00:32:43,333
If I said the learning rate too large,
it could overshoot and actually 

532
00:32:46,251 --> 00:32:48,050
diverged.
Our model could blow up.

533
00:32:50,120 --> 00:32:50,953
Okay.
Ideally we want to use learning rates 

534
00:32:53,780 --> 00:32:57,350
that are large enough to avoid local 
minima,

535
00:32:57,680 --> 00:32:58,513
but also still converged to our global 
minimum so they can overshoot just 

536
00:33:01,311 --> 00:33:05,990
enough to avoid some local minima but 
then converge to our global minimum.

537
00:33:07,340 --> 00:33:08,173
Now,
how can we actually set the learning 

538
00:33:09,921 --> 00:33:11,090
rate?
Well,

539
00:33:11,091 --> 00:33:11,924
one idea is let's just try a lot of 
different things and see what works 

540
00:33:14,241 --> 00:33:16,790
best,
but I don't really like the solution.

541
00:33:16,880 --> 00:33:19,220
Let's try and see if we can be a little 
smarter than that.

542
00:33:20,360 --> 00:33:25,360
How about we tried to build an adaptive 
algorithm that changes its learning rate

543
00:33:25,610 --> 00:33:29,810
as training happens.
So this is a learning rate that actually

544
00:33:29,811 --> 00:33:30,644
adapts to the landscape that it's in.
So the learning grade is no longer a 

545
00:33:34,131 --> 00:33:35,360
fixed number.
It can change,

546
00:33:35,361 --> 00:33:36,194
it can go up and down,
and this will change depending on the 

547
00:33:38,781 --> 00:33:39,614
location that the,
that the update is currently yet the 

548
00:33:43,491 --> 00:33:44,324
great end in that location,
maybe how fast we're learning and many 

549
00:33:47,181 --> 00:33:51,560
other many other possible situations.
In fact,

550
00:33:52,370 --> 00:33:57,370
this process of optimization in deep 
neural networks and non convex situation

551
00:33:57,861 --> 00:34:00,710
has been extremely explored.
There's many,

552
00:34:00,740 --> 00:34:01,573
many,
many algorithms for computing adaptive 

553
00:34:04,221 --> 00:34:05,054
learning rates and here are some 
examples that we encourage you to try 

554
00:34:08,811 --> 00:34:13,160
out during your labs to see what works 
best and for your problems,

555
00:34:13,400 --> 00:34:14,233
especially with real world problems.
Things can change a lot depending on 

556
00:34:17,571 --> 00:34:22,571
what you learn in lecture and what 
really works in lab and we encourage you

557
00:34:22,641 --> 00:34:23,474
to just experiment,
get some intuition about each of these 

558
00:34:25,131 --> 00:34:28,310
learning rates and really understand 
them at a higher level.

559
00:34:30,740 --> 00:34:31,573
So I want to continue this talk and 
really talk about more the practice of 

560
00:34:34,401 --> 00:34:35,234
deep neural networks.
This incredibly powerful notion of many 

561
00:34:39,141 --> 00:34:44,141
batching and I'll focus for now if we go
back to this gradient descent algorithm,

562
00:34:46,490 --> 00:34:47,323
this is the same one that we saw before 
and let's look at this term again so we 

563
00:34:50,720 --> 00:34:53,900
found out how to compute this term using
backpropagation,

564
00:34:54,670 --> 00:34:55,503
but actually what I didn't tell you is 
that the computation here is extremely 

565
00:34:58,821 --> 00:35:00,470
calm,
is extremely expensive.

566
00:35:01,010 --> 00:35:01,843
We have a lot of data points potentially
in our data set and this takes us input 

567
00:35:06,060 --> 00:35:08,150
a summation over all of those data 
points.

568
00:35:08,151 --> 00:35:11,390
So if our Dataset is millions of 
examples large,

569
00:35:11,391 --> 00:35:15,170
which is not that large in the realm of 
today's deep neural networks,

570
00:35:16,180 --> 00:35:18,710
but this can be extremely expensive just
for one iteration,

571
00:35:18,711 --> 00:35:20,240
so we can compute this on every 
iteration.

572
00:35:20,450 --> 00:35:21,283
Instead,
lets create a variant of this algorithm 

573
00:35:23,900 --> 00:35:24,733
called stochastic gradient descent where
we compute the gradient just using a 

574
00:35:27,831 --> 00:35:29,390
single training example.

575
00:35:30,470 --> 00:35:31,303
Now this is nice because it's really 
easy to compute the gradient for a 

576
00:35:33,711 --> 00:35:34,544
single training example.
It's not nearly as intense as over the 

577
00:35:36,681 --> 00:35:40,020
entire training set,
but as the name might suggest,

578
00:35:40,021 --> 00:35:43,530
this is a more stochastic estimate,
as much more noisy.

579
00:35:44,280 --> 00:35:47,910
It can make us jump around the landscape
in ways that we didn't anticipate.

580
00:35:47,911 --> 00:35:48,744
It doesn't actually represent the true 
gradient of our data set because it's 

581
00:35:51,241 --> 00:35:54,180
only a single point,
so what's the middle ground?

582
00:35:54,690 --> 00:35:59,400
How about we define a mini batch of be 
data points,

583
00:35:59,850 --> 00:36:02,970
compute the average gradient across 
those data points,

584
00:36:03,540 --> 00:36:06,600
and actually use that as an estimate of 
our true gradient.

585
00:36:07,410 --> 00:36:08,243
Now this is much faster than computing 
the estimate over the entire batch 

586
00:36:10,531 --> 00:36:11,364
because b is usually something like 10 
to 100 and it's much more accurate than 

587
00:36:15,511 --> 00:36:18,260
sgd because we're not taking a single 
example,

588
00:36:18,261 --> 00:36:21,000
but we're learning over a smaller batch.
A larger batch.

589
00:36:21,001 --> 00:36:23,420
Sorry.
Nope.

590
00:36:23,450 --> 00:36:25,970
The more accurate our gradient 
estimation is,

591
00:36:26,450 --> 00:36:27,283
that means the more or the easier it 
will be for us to converge to the 

592
00:36:31,431 --> 00:36:32,264
solution,
faster means will converge smoother 

593
00:36:34,730 --> 00:36:38,090
because we'll actually follow the true 
landscape that exists.

594
00:36:39,110 --> 00:36:39,943
It also means that we can increase our 
learning great to trust each update 

595
00:36:41,961 --> 00:36:42,794
more.

596
00:36:44,840 --> 00:36:48,170
This also allows for massively parallel 
likable computation.

597
00:36:48,590 --> 00:36:49,423
If we split up batches on different 
workers on different gps or different 

598
00:36:53,391 --> 00:36:54,224
threads,
we can achieve even higher speed ups 

599
00:36:56,991 --> 00:36:59,120
because each thread can handle its own 
batch.

600
00:36:59,121 --> 00:36:59,954
Then they can come back together and 
aggregate together to basically create 

601
00:37:03,411 --> 00:37:07,640
that single learning grade or complete 
complete that single training iteration.

602
00:37:10,430 --> 00:37:11,263
Now finally,
the last topic I want to talk about is 

603
00:37:12,890 --> 00:37:13,723
that of overfitting and regularization.
Really this is a problem of 

604
00:37:18,560 --> 00:37:19,393
generalization,
which is one of the most fundamental 

605
00:37:23,841 --> 00:37:26,210
problems and all of artificial 
intelligence,

606
00:37:26,570 --> 00:37:29,750
not just deep learning,
but all of artificial intelligence,

607
00:37:31,070 --> 00:37:34,820
and for those who aren't familiar,
let me just go over on a high level what

608
00:37:34,910 --> 00:37:37,670
overfitting is,
what it means to generalize.

609
00:37:38,630 --> 00:37:39,463
Ideally in machine learning,
we want the model that accurately 

610
00:37:41,331 --> 00:37:44,600
describes our test data,
not our training data,

611
00:37:44,690 --> 00:37:45,590
but our test data

612
00:37:47,500 --> 00:37:48,333
said differently.
We want to build models that can learn 

613
00:37:50,081 --> 00:37:50,914
representations from our training data 
and still generalized well on unseen 

614
00:37:55,600 --> 00:37:56,433
test data.
Assuming you want to build a line to 

615
00:37:58,751 --> 00:38:02,590
describe these points under fitting 
describes the process on the left,

616
00:38:02,900 --> 00:38:03,733
where at the complexity of our model is 
simply not high enough to capture the 

617
00:38:06,851 --> 00:38:10,600
nuances of our data.
If we go over 50 on the right,

618
00:38:10,990 --> 00:38:11,823
we're actually having too complex of a 
model and actually just memorizing our 

619
00:38:15,521 --> 00:38:16,354
training data,
which means that if we introduced a new 

620
00:38:17,951 --> 00:38:20,590
test data point,
it's not going to generalize well.

621
00:38:20,890 --> 00:38:22,720
Ideally what we want to,
something in the middle,

622
00:38:23,230 --> 00:38:26,020
which is not too complex to memorize all
of the training data,

623
00:38:27,340 --> 00:38:28,173
but still a contains the capacity to 
learn some of these nuances in the test 

624
00:38:33,911 --> 00:38:38,050
set.
So just to address this problem,

625
00:38:38,051 --> 00:38:40,990
let's talk about this technique called 
regularization.

626
00:38:41,820 --> 00:38:42,653
Now regularization is just this way that
you can discourage your models from 

627
00:38:46,271 --> 00:38:50,380
becoming too complex and after we've 
seen,

628
00:38:50,480 --> 00:38:51,313
as we've seen before,
this is extremely critical because we 

629
00:38:53,961 --> 00:38:54,794
don't want our data.
We don't want our models to just 

630
00:38:57,801 --> 00:39:00,920
memorize data and only do well in our 
training set.

631
00:39:04,620 --> 00:39:05,453
One of the most popular techniques for 
regularization in neural networks is 

632
00:39:09,180 --> 00:39:11,760
dropout.
This is extremely simple idea.

633
00:39:12,210 --> 00:39:13,043
Let's revisit this picture of a deep 
neural network and then drop out all we 

634
00:39:15,391 --> 00:39:16,224
do during a training on every iteration,
we randomly drop some proportion of the 

635
00:39:22,500 --> 00:39:26,910
hidden neurons with some probability p,
so let's suppose p equals point five.

636
00:39:26,911 --> 00:39:29,280
That means we dropped 50 percent of 
those neurons like that.

637
00:39:29,760 --> 00:39:30,593
Those activations becomes zero and 
effectively they're no longer part of 

638
00:39:33,751 --> 00:39:34,584
our network.
This forces the network to not rely on 

639
00:39:38,841 --> 00:39:39,674
any single node,
but actually find alternative paths 

640
00:39:42,561 --> 00:39:43,394
through the network and not put too much
weight on any single example with any 

641
00:39:46,851 --> 00:39:49,400
syngrid single nodes,
so it discourages memorization.

642
00:39:49,401 --> 00:39:52,950
Essentially,
John,

643
00:39:52,980 --> 00:39:53,813
every iteration,
we randomly drop another 50 percent of 

644
00:39:56,191 --> 00:39:58,290
the nodes,
so in this iteration I may drop these on

645
00:39:58,291 --> 00:39:59,124
the next iteration.
I may drop those and since it's 

646
00:40:00,751 --> 00:40:01,584
different on every iteration,
you're encouraging the network to find 

647
00:40:04,051 --> 00:40:05,940
these different paths to his answer.

648
00:40:08,240 --> 00:40:09,073
The second technique for regularization 
that we'll talk about is this notion of 

649
00:40:11,810 --> 00:40:12,643
early stopping.
Now we know that the definition of 

650
00:40:15,471 --> 00:40:16,304
overfitting actually is just when our 
model starts to perform worse and worse 

651
00:40:20,241 --> 00:40:23,840
on our test data set.
So let's use that to our advantage.

652
00:40:23,841 --> 00:40:24,674
To create this early stopping algorithm,
if we set aside some of our training 

653
00:40:27,981 --> 00:40:30,980
data and use it only as test data,
we don't train with that data.

654
00:40:31,700 --> 00:40:32,533
We can use it to basically monitor the 
progress of our model on unseen data so 

655
00:40:36,921 --> 00:40:39,830
we can plot this curve.
We're on the x axis,

656
00:40:39,831 --> 00:40:41,720
we have the training iterations.
On the y axis,

657
00:40:41,721 --> 00:40:44,780
we have to loss.
Now they start off going down together.

658
00:40:44,870 --> 00:40:47,780
This is great because it means that 
we're learning or training,

659
00:40:47,781 --> 00:40:49,100
right?
That's great.

660
00:40:50,390 --> 00:40:53,720
There comes a point though where the 
testing data,

661
00:40:54,110 --> 00:40:59,110
where the testing data set and the loss 
for that Dataset starts to plateau.

662
00:41:00,980 --> 00:41:02,300
Now,
if we look a little further,

663
00:41:02,390 --> 00:41:06,800
the training data set loss will always 
continue to go down as long as our model

664
00:41:06,801 --> 00:41:10,130
has the capacity to learn and memorize 
some of that data,

665
00:41:10,430 --> 00:41:11,263
but that doesn't mean that it's actually
generalizing well because we can see 

666
00:41:13,041 --> 00:41:15,950
that the testing dataset has actually 
started to increase.

667
00:41:17,550 --> 00:41:19,530
This pattern continues for the rest of 
training,

668
00:41:19,650 --> 00:41:24,480
but I want to focus on this point here.
This is the point where you need to stop

669
00:41:24,510 --> 00:41:28,740
training because after this point,
you are overfitting and your model is no

670
00:41:28,741 --> 00:41:32,460
longer performing well on unseen data.
If you stopped before that point,

671
00:41:32,880 --> 00:41:33,713
you're under fitting and you're not 
utilizing the full potential to full 

672
00:41:36,441 --> 00:41:37,274
capacity of your network.
So I'll conclude this lecture by 

673
00:41:42,690 --> 00:41:46,200
summarizing three key points that we've 
covered so far.

674
00:41:47,410 --> 00:41:48,243
First,
we've learned about the fundamental 

675
00:41:49,201 --> 00:41:52,260
building blocks of neural networks 
called the perceptron.

676
00:41:53,220 --> 00:41:56,700
We've learned about stacking these 
units,

677
00:41:56,770 --> 00:42:01,770
these perceptrons together to compose 
very complex hierarchical models,

678
00:42:03,570 --> 00:42:04,403
and we've learned how to mathematically 
optimize these models using a process 

679
00:42:08,370 --> 00:42:11,790
called backpropagation and gradient 
descent.

680
00:42:13,290 --> 00:42:14,123
Finally,
we addressed some of the practical 

681
00:42:15,001 --> 00:42:15,834
challenges of training these models in 
real life that you'll find useful for 

682
00:42:19,471 --> 00:42:22,320
the labs today,
such as using adaptive learning rates,

683
00:42:22,470 --> 00:42:25,410
batching and regularization to combat 
overfitting.

684
00:42:27,920 --> 00:42:29,060
Thank you.
And,

685
00:42:29,061 --> 00:42:29,894
uh,
I'd be happy to answer any questions 

686
00:42:32,061 --> 00:42:33,260
now.
Otherwise,

687
00:42:33,290 --> 00:42:38,240
we'll have rainy talk to us about some 
of the deep sequence models for modeling

688
00:42:38,241 --> 00:42:39,080
temporal data.

