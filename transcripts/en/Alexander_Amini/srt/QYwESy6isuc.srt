1
00:00:02,580 --> 00:00:03,600
Thanks for having me here.
Yeah.

2
00:00:03,601 --> 00:00:04,470
So I'm,
uh,

3
00:00:04,490 --> 00:00:08,940
I'm based in the Cambridge office,
which is like a 100 meters that way.

4
00:00:09,030 --> 00:00:09,863
Um,
and we do a lot of stuff with a deep 

5
00:00:12,211 --> 00:00:13,044
learning.
We've got a large group in Google brain 

6
00:00:14,721 --> 00:00:15,554
and other related fields.
So hopefully that's interesting to some 

7
00:00:18,991 --> 00:00:19,824
of you at some point.
So I'm going to talk for about 20 

8
00:00:23,101 --> 00:00:24,190
minutes or so.
Um,

9
00:00:24,390 --> 00:00:27,090
this sort of issues and image 
classification theme.

10
00:00:27,610 --> 00:00:28,443
Let me hand it over to my excellent 
colleagues hunching king who's going to 

11
00:00:31,800 --> 00:00:36,270
go through an entirely different subject
in a using tensorflow debugger and eager

12
00:00:36,271 --> 00:00:41,271
mode to make work intensive flow easier.
Maybe that will be good.

13
00:00:42,300 --> 00:00:43,133
Okay.
Um,

14
00:00:43,710 --> 00:00:45,210
so let's,
let's take a step back.

15
00:00:45,720 --> 00:00:47,610
So have you guys seen happy graphs like 
this before?

16
00:00:47,611 --> 00:00:49,830
Go ahead and smile and nod if you've 
seen stuff like this.

17
00:00:49,890 --> 00:00:50,610
Yeah.
Okay.

18
00:00:50,610 --> 00:00:55,290
So this is a happy graph on image,
net based image classification.

19
00:00:55,320 --> 00:01:00,320
So image net is a data set of a million 
some odd images for this challenge.

20
00:01:01,291 --> 00:01:04,340
There were a thousand classes,
um,

21
00:01:05,040 --> 00:01:05,873
and uh,
in 2011 back in the dark ages when 

22
00:01:08,221 --> 00:01:09,054
nobody knew how to do anything,
the state of the art was something like 

23
00:01:12,091 --> 00:01:16,800
25 percent error rate on this stuff.
And in the last call it six,

24
00:01:16,801 --> 00:01:17,634
seven years,
the reduction in error rate has been 

25
00:01:20,670 --> 00:01:23,430
kind of astounding to the point where 
it's now been talked about so much.

26
00:01:23,431 --> 00:01:25,410
It's like no longer even surprising and 
everyone's like,

27
00:01:25,420 --> 00:01:25,850
yeah,
yeah,

28
00:01:25,850 --> 00:01:27,070
we've see this,
um,

29
00:01:28,300 --> 00:01:31,610
human error rate is somewhere between 
five and 10 percent on,

30
00:01:31,630 --> 00:01:32,770
on,
on this task.

31
00:01:33,010 --> 00:01:37,100
So the contemporary results of,
you know,

32
00:01:37,360 --> 00:01:38,193
two point two or whatever it is,
percent error rate or really kind of 

33
00:01:40,841 --> 00:01:42,920
astonishing.
Um,

34
00:01:43,120 --> 00:01:46,690
and you can look at a graph like this 
and make reasonable claims that well,

35
00:01:48,420 --> 00:01:49,253
machines we're using deep learning are 
better than humans that impinged 

36
00:01:51,950 --> 00:01:52,783
classification on this task.
That's kind of weird and kind of 

37
00:01:55,501 --> 00:01:56,334
amazing.
And maybe we can declare victory and 

38
00:01:57,841 --> 00:02:01,500
fill audiences full of people clamoring 
to learn about deep learning.

39
00:02:02,130 --> 00:02:03,090
That's cool.
Okay.

40
00:02:03,120 --> 00:02:04,440
So,
um,

41
00:02:06,530 --> 00:02:08,630
I'm going to talk not about image data 
itself,

42
00:02:08,631 --> 00:02:12,050
but about a slightly different image 
data set.

43
00:02:12,200 --> 00:02:13,550
Basically people were like,
okay,

44
00:02:13,790 --> 00:02:16,100
obviously image net is too easy.
Let's make a larger,

45
00:02:16,101 --> 00:02:16,934
more interesting data set.
So open images was released I think a 

46
00:02:19,581 --> 00:02:22,340
year or two ago.
It's got about 9 million as opposed to 1

47
00:02:22,340 --> 00:02:27,340
million images.
The base data set has 6,000

48
00:02:27,801 --> 00:02:31,070
labels as opposed to 1000 labels.
This ultra multi label.

49
00:02:31,071 --> 00:02:32,210
So you get,
you know,

50
00:02:32,240 --> 00:02:34,070
if there's a person holding a rugby 
ball,

51
00:02:34,071 --> 00:02:36,300
you get both person and rugby ball and 
the Dataset.

52
00:02:37,370 --> 00:02:39,530
It's got all kinds of classes including 
stairs here,

53
00:02:39,531 --> 00:02:41,810
which are lovely,
lovely illustrated.

54
00:02:41,930 --> 00:02:42,763
And you can find this on get hub.
It's a nice data set to play around 

55
00:02:45,051 --> 00:02:47,520
with.
So uh,

56
00:02:47,750 --> 00:02:50,120
some colleagues and I did some work of 
saying,

57
00:02:50,121 --> 00:02:52,550
okay,
what happens if we apply just a straight

58
00:02:52,551 --> 00:02:55,000
up inception based model,
um,

59
00:02:55,520 --> 00:02:56,780
to this data we traded up.

60
00:02:56,780 --> 00:02:57,613
And then we'd look at some how would 
classify as some that we found on the 

61
00:03:00,011 --> 00:03:03,250
web.
So here's one such image,

62
00:03:04,210 --> 00:03:05,043
a image that we found on the web.
All the images here are creative 

63
00:03:07,241 --> 00:03:09,010
comments and stuff like that.
So it's,

64
00:03:09,250 --> 00:03:14,250
it's okay for us to look at these and 
when we apply an image based,

65
00:03:15,290 --> 00:03:16,670
a,
a image,

66
00:03:16,671 --> 00:03:17,504
nobody kind of model to this 
classifications we get back or kind of 

67
00:03:20,171 --> 00:03:21,004
what I personally would expect.
I'm seeing things like bride dress 

68
00:03:23,801 --> 00:03:25,030
ceremony,
woman wedding,

69
00:03:25,360 --> 00:03:26,193
all things that as an American in this 
country at this time I'm thinking or 

70
00:03:30,460 --> 00:03:33,790
make make sense for this image.
Cool.

71
00:03:33,940 --> 00:03:35,650
Maybe we did solve it.
Image classification.

72
00:03:36,610 --> 00:03:37,840
So,
um,

73
00:03:37,930 --> 00:03:42,930
then we had played it to another image.
Also have a bride and a,

74
00:03:45,040 --> 00:03:45,873
the model that we had trained up on this
open source image data set returned the 

75
00:03:51,191 --> 00:03:54,700
following classifications,
clothing event,

76
00:03:55,540 --> 00:03:59,170
costume red and performance art.

77
00:03:59,910 --> 00:04:04,910
I'm no mention of bride,
also,

78
00:04:05,110 --> 00:04:09,760
no mention of person Nis regardless of 
gender.

79
00:04:10,270 --> 00:04:11,620
Um,
so in a sense this,

80
00:04:11,650 --> 00:04:15,400
this model is sort of like missed the 
fact that there's a human in the picture

81
00:04:15,610 --> 00:04:16,443
which is maybe not awesome and not 
really what I would think of as great 

82
00:04:21,461 --> 00:04:24,850
success if we're claiming that image 
classification is solved.

83
00:04:25,330 --> 00:04:27,870
Um,
okay.

84
00:04:28,000 --> 00:04:30,880
So what's going on here?
Um,

85
00:04:31,750 --> 00:04:34,910
I'm going to argue a little bit that 
what's going on is,

86
00:04:34,940 --> 00:04:35,773
is based in,
to some degree on the idea of 

87
00:04:38,201 --> 00:04:42,490
stereotypes and uh,
if you're,

88
00:04:42,520 --> 00:04:43,353
if you have your laptop open,
I'd like you to close your laptop for a 

89
00:04:44,621 --> 00:04:45,454
second.
This is the interactive portion where 

90
00:04:46,481 --> 00:04:49,300
you can interact by closing your laptop 
and,

91
00:04:50,160 --> 00:04:50,993
um,
I'd like you to find somebody sitting 

92
00:04:52,961 --> 00:04:57,961
next to you and exercise your human 
conversation skills for about one minute

93
00:04:58,960 --> 00:05:03,070
to come up with a definition between the
two of you have,

94
00:05:03,100 --> 00:05:03,933
what is a stereotype keeping in mind 
that we're in sort of a statistical 

95
00:05:07,061 --> 00:05:08,140
setting.
Okay?

96
00:05:08,141 --> 00:05:11,530
So have a quick one minute conversation 
with the person sitting next to you.

97
00:05:11,770 --> 00:05:13,360
If there's no one sitting next to you,
you may move.

98
00:05:13,630 --> 00:05:14,350
Ready,
set,

99
00:05:14,350 --> 00:05:15,183
go.

100
00:05:18,520 --> 00:05:20,350
Three,
two,

101
00:05:21,040 --> 00:05:21,873
one.
And thank you for having that 

102
00:05:25,600 --> 00:05:28,630
interesting conversation that easily 
could have lasted for much more than one

103
00:05:28,631 --> 00:05:30,100
minute,
but such is life.

104
00:05:30,760 --> 00:05:34,690
Let's hear from one or two folks who had
something that they came up with.

105
00:05:34,691 --> 00:05:36,460
It was interesting.
Yeah.

106
00:05:36,461 --> 00:05:38,480
Go ahead.
Your name is at that?

107
00:05:38,490 --> 00:05:38,950
Yeah.
Okay.

108
00:05:38,950 --> 00:05:39,783
What did you

109
00:05:42,800 --> 00:05:43,633
based?

110
00:05:46,260 --> 00:05:47,093
Okay,
so you saying that a stereotype is a 

111
00:05:48,751 --> 00:05:49,584
generalization that you find from a 
large group of people and you apply it 

112
00:05:52,171 --> 00:05:53,340
to more people.
Okay,

113
00:05:53,970 --> 00:05:54,803
interesting.
I'm certainly agree with large parts of 

114
00:05:56,730 --> 00:05:57,260
that.
Yeah,

115
00:05:57,260 --> 00:05:58,093
go ahead.

116
00:05:59,390 --> 00:06:04,390
That's based on probability and 
experience with the human goal.

117
00:06:04,750 --> 00:06:09,490
For you,
would it be free?

118
00:06:10,950 --> 00:06:11,471
Okay.
So,

119
00:06:11,471 --> 00:06:12,304
so here the claimant said it's a label 
that's based on experience from within 

120
00:06:15,841 --> 00:06:17,120
your training set.
Yep.

121
00:06:17,250 --> 00:06:18,083
Super interesting.
And I'm the probability of label based 

122
00:06:22,141 --> 00:06:23,130
on what's,
what's your training say?

123
00:06:23,131 --> 00:06:24,180
Cool.
Maybe one more.

124
00:06:26,700 --> 00:06:27,533
Oh yeah,
got it.

125
00:06:28,800 --> 00:06:33,800
You're making a prediction based upon to
be coordinated and integrated.

126
00:06:35,690 --> 00:06:36,980
Okay.
So the claim here,

127
00:06:36,981 --> 00:06:37,814
that stereotype has something to do with
unrelated features that happened to be 

128
00:06:39,801 --> 00:06:41,390
correlated.
I think that's interesting.

129
00:06:41,400 --> 00:06:43,670
Let me see if I can.
This was not a plant.

130
00:06:43,750 --> 00:06:44,600
I'm sorry,
your name was

131
00:06:46,120 --> 00:06:46,690
awesome.

132
00:06:46,690 --> 00:06:48,220
Constantine.
Constantine is not a plant.

133
00:06:48,350 --> 00:06:49,183
Um,
but I do want to look at this a little 

134
00:06:52,241 --> 00:06:54,160
bit more in detail.
So here's a,

135
00:06:54,190 --> 00:06:56,960
here's a Dataset that I'm going to claim
is,

136
00:06:57,310 --> 00:06:59,660
is based on running data.
Um,

137
00:06:59,950 --> 00:07:00,890
so,
uh,

138
00:07:01,120 --> 00:07:04,360
in the early mornings I pretend that I'm
an athlete and go for a run.

139
00:07:04,570 --> 00:07:06,800
Um,
and uh,

140
00:07:06,970 --> 00:07:07,803
this is a dataset that sort of based on 
risk that someone might not finish a 

141
00:07:12,591 --> 00:07:15,370
race that they enter in a.
So we've got high risk.

142
00:07:15,371 --> 00:07:16,204
People are,
you know,

143
00:07:16,420 --> 00:07:19,360
are in yellow and lower risk.
People are in red.

144
00:07:19,680 --> 00:07:21,400
I'm going to walk,
get this data.

145
00:07:21,401 --> 00:07:24,790
It's got a couple of dimensions.
I might fit a linear classifier.

146
00:07:25,240 --> 00:07:29,380
It's not quite perfect if I look a 
little more closely,

147
00:07:29,381 --> 00:07:30,214
if I've actually got some more 
information here I've done just to have 

148
00:07:32,951 --> 00:07:35,800
x and y also had this sort of color have
outline.

149
00:07:36,310 --> 00:07:41,310
So I might have a rule that if this data
point has a blue outline,

150
00:07:41,771 --> 00:07:42,604
I'm going to predict low risk.
Otherwise I'm going to predict high 

151
00:07:44,591 --> 00:07:47,230
risk.
Fair enough.

152
00:07:48,120 --> 00:07:50,110
Um,
now the big reveal,

153
00:07:50,230 --> 00:07:52,220
you'll never guess what,
um,

154
00:07:52,270 --> 00:07:53,110
the,
uh,

155
00:07:53,140 --> 00:07:57,360
the outline feature is based on shoe 
type.

156
00:07:58,210 --> 00:07:59,043
The other x and y are based on how long 
the races and sort of what a person's 

157
00:08:01,601 --> 00:08:05,920
weekly training volume is.
But whether you're foolish enough to buy

158
00:08:05,921 --> 00:08:06,754
expensive running shoes because you 
think they're going to make you faster 

159
00:08:08,321 --> 00:08:09,690
or whatever,
um,

160
00:08:10,040 --> 00:08:11,140
this is what's in the data.

161
00:08:13,370 --> 00:08:14,203
And in

162
00:08:15,670 --> 00:08:18,670
traditional machine learning,
supervised machine learning,

163
00:08:19,300 --> 00:08:20,500
we might say,
well,

164
00:08:20,501 --> 00:08:21,334
wait a minute,
I'm not sure that shoe type is going to 

165
00:08:23,801 --> 00:08:27,120
be actually predictive.
On the other hand,

166
00:08:27,150 --> 00:08:27,983
it's in her training data and it does 
seem to be awfully predictive on this 

167
00:08:31,111 --> 00:08:31,944
data set.
We have a really simple model is highly 

168
00:08:33,301 --> 00:08:34,950
regularized.
It still gives you know,

169
00:08:35,310 --> 00:08:38,370
perfect or near perfect accuracy.
Maybe it's fine.

170
00:08:39,420 --> 00:08:40,253
And

171
00:08:41,400 --> 00:08:43,350
the only way we can find out if it's 
not,

172
00:08:43,920 --> 00:08:44,753
I would argue is by gathering some more 
data and I'll point out that this data 

173
00:08:49,921 --> 00:08:50,754
set has been diabolically constructed so
that there are some points in the space 

174
00:08:56,100 --> 00:08:57,720
that are not particularly well 
represented.

175
00:08:58,650 --> 00:08:59,483
And you can maybe tell yourself a story 
about maybe this data was collected 

176
00:09:01,891 --> 00:09:06,390
after some corporate five k or something
like that.

177
00:09:06,420 --> 00:09:07,253
Um,
so if we go out and collect some more 

178
00:09:08,041 --> 00:09:08,874
data,
maybe we find that actually there's 

179
00:09:12,420 --> 00:09:15,980
people wearing all kinds of shoes on 
both sides of,

180
00:09:16,080 --> 00:09:16,913
of our imaginary classifier.
But that this shoe type feature is 

181
00:09:21,301 --> 00:09:22,134
really not predictive at all and this 
gets back to Constantine's point that 

182
00:09:25,261 --> 00:09:26,610
perhaps,
uh,

183
00:09:26,611 --> 00:09:31,611
relying on features that are strongly 
correlated but not necessarily causal,

184
00:09:32,220 --> 00:09:35,610
may be a point at which we're thinking 
about a stereotype in some way.

185
00:09:36,230 --> 00:09:39,180
Um,
so obviously given this data and what we

186
00:09:39,181 --> 00:09:40,014
know now,
I would probably go back and suggest to 

187
00:09:42,301 --> 00:09:43,134
linear classifier based on these,
these features of length of race and 

188
00:09:46,100 --> 00:09:51,100
weekly training volumes as potentially a
better model to how does this happen?

189
00:09:51,510 --> 00:09:52,343
What's,
what's,

190
00:09:54,120 --> 00:09:54,953
what's the issue here that said play.
I'm one of the issues that's at play is 

191
00:09:59,281 --> 00:10:00,114
that in supervised machine learning,
we often make the assumption that our 

192
00:10:05,671 --> 00:10:08,790
training distribution and our test 
distribution are identical.

193
00:10:09,360 --> 00:10:10,193
Right?
And we make this assumption for really 

194
00:10:12,031 --> 00:10:12,864
good reason,
which is that if we make that 

195
00:10:14,941 --> 00:10:15,774
assumption,
then we can pretend that there's no 

196
00:10:17,701 --> 00:10:19,590
difference between correlation and 
causation.

197
00:10:19,770 --> 00:10:20,603
And we can use all of our features,
whether they're what Constantine would 

198
00:10:23,371 --> 00:10:26,600
call meaningful or causal or not.
Um,

199
00:10:26,610 --> 00:10:27,443
we can throw them in there and so long 
as they're testing training 

200
00:10:28,981 --> 00:10:29,814
distributions are the same.
We're probably okay to within some some 

201
00:10:32,701 --> 00:10:33,534
degree,
but in the real world we don't just 

202
00:10:37,321 --> 00:10:38,154
apply models to a training or test set.
We also use them to make predictions 

203
00:10:43,230 --> 00:10:44,820
that may influence the world in some 
way.

204
00:10:46,290 --> 00:10:51,290
And there I think that the right sort of
phrase to use isn't so much tests set.

205
00:10:51,990 --> 00:10:55,770
It's more inference time performance.
Okay.

206
00:10:55,771 --> 00:10:57,030
Because at,
at inference time,

207
00:10:57,031 --> 00:10:59,760
when we're going and applying our model 
to some new instance in the world,

208
00:10:59,820 --> 00:11:00,653
we may not actually know what they let 
the true label is ever or things like 

209
00:11:02,791 --> 00:11:04,650
that,
but we still care very much about having

210
00:11:04,651 --> 00:11:07,560
good performance and making sure that 
our,

211
00:11:07,680 --> 00:11:08,513
uh,
test that our training set matches our 

212
00:11:12,271 --> 00:11:14,320
inference distribution to some degree 
is,

213
00:11:14,321 --> 00:11:16,920
is like super critical.
Um,

214
00:11:17,140 --> 00:11:19,200
so let's go back to open images and what
was happening there.

215
00:11:19,650 --> 00:11:20,483
Um,
you'll recall that it did quite badly 

216
00:11:23,270 --> 00:11:24,103
on,
at least anecdotally on that image of a 

217
00:11:27,031 --> 00:11:29,850
bride who appeared to be from India.
Um,

218
00:11:30,480 --> 00:11:32,430
if we look at the Geo diversity of open 
images,

219
00:11:32,431 --> 00:11:33,264
this is something where we did our best 
to sort of track down the geolocation 

220
00:11:35,870 --> 00:11:36,703
of,
of each of the images in the open image 

221
00:11:38,191 --> 00:11:39,024
data set.

222
00:11:39,080 --> 00:11:39,913
Uh,
what we found was that an overwhelming 

223
00:11:42,541 --> 00:11:46,090
proportion of the data in open images,
uh,

224
00:11:46,110 --> 00:11:49,140
was from North America and six countries
in Europe,

225
00:11:49,920 --> 00:11:50,753
um,
vanishingly small amounts of that data 

226
00:11:53,080 --> 00:11:53,913
were from countries such as India or 
China or other places where I've heard 

227
00:11:57,071 --> 00:11:59,170
there's actually a large number of 
people.

228
00:12:00,550 --> 00:12:01,383
So this is clearly not representative in
a meaningful way of sort of the global 

229
00:12:07,421 --> 00:12:10,210
diversity of the world.
How does this happen?

230
00:12:10,690 --> 00:12:13,530
It's not like the researchers who put 
the open images dataset.

231
00:12:13,560 --> 00:12:14,393
We're in any way on all intention.
They were working really hard to put 

232
00:12:17,111 --> 00:12:17,944
together what they believe was a more 
representative data set than the image 

233
00:12:21,221 --> 00:12:22,510
net.
Um,

234
00:12:22,750 --> 00:12:23,583
at the very least,
they don't have 100 categories of dogs 

235
00:12:24,731 --> 00:12:26,170
in this one.
Um,

236
00:12:27,970 --> 00:12:29,050
so what happens?
Well,

237
00:12:29,110 --> 00:12:29,943
you could make an argument that there's 
some strong correlation with the 

238
00:12:31,811 --> 00:12:36,811
distribution of open images with the 
distribution of countries with high,

239
00:12:38,030 --> 00:12:39,730
low,
high bandwidth,

240
00:12:39,731 --> 00:12:41,050
low cost,
Internet access.

241
00:12:41,870 --> 00:12:43,780
Um,
it's not a perfect correlation,

242
00:12:43,781 --> 00:12:45,910
but it's,
it's pretty close.

243
00:12:46,360 --> 00:12:49,710
Um,
and that if we're doing a,

244
00:12:49,780 --> 00:12:50,613
if one might do things like base an 
image classifier on data drawn from a 

245
00:12:55,691 --> 00:12:59,350
distribution of areas that have high 
bandwidth,

246
00:12:59,351 --> 00:13:01,270
low cost Internet access,
um,

247
00:13:01,750 --> 00:13:02,583
that may induce differences between the 
trading distribution and the inference 

248
00:13:06,761 --> 00:13:07,594
time distribution.
So none of this is like something you 

249
00:13:10,961 --> 00:13:15,670
wouldn't figure out without it if you 
sat down for five minutes.

250
00:13:15,671 --> 00:13:17,920
Right?
This is all like super basic statistics.

251
00:13:18,070 --> 00:13:18,903
It is in fact stuff that's this just six
people have been sort of railing at the 

252
00:13:22,391 --> 00:13:24,550
machine learning community at for the 
last several decades.

253
00:13:24,740 --> 00:13:25,573
Um,
but as machine learning models become 

254
00:13:28,691 --> 00:13:32,050
sort of more ubiquitous in everyday 
life,

255
00:13:32,300 --> 00:13:33,133
I think that paying attention to these 
kinds of issues becomes ever more 

256
00:13:35,291 --> 00:13:37,050
important.
Uh,

257
00:13:37,110 --> 00:13:37,943
so let's go back to,
to what is a stereotype and I think I 

258
00:13:39,791 --> 00:13:41,430
agree with Constantine's,
um,

259
00:13:41,610 --> 00:13:42,443
idea.
And I'm going to add one more tweak to 

260
00:13:43,781 --> 00:13:46,450
it.
So I'm going to say that a stereotype is

261
00:13:46,451 --> 00:13:47,284
a statistical confounder.
I think Susan Constantine's language 

262
00:13:49,361 --> 00:13:53,110
almost exactly a that has a societal 
basis.

263
00:13:55,510 --> 00:13:59,200
So when I think about issues of 
fairness,

264
00:13:59,560 --> 00:14:01,990
if it's the case that,
um,

265
00:14:02,560 --> 00:14:03,393
you know,
rainy weather is correlated with people 

266
00:14:05,140 --> 00:14:06,310
using umbrellas,
like,

267
00:14:06,311 --> 00:14:07,300
yes,
that's a confounder.

268
00:14:07,310 --> 00:14:10,380
The umbrellas did not cause the rain.
Um,

269
00:14:10,900 --> 00:14:11,733
but I'm not as worried as a individual 
human about the societal impact of 

270
00:14:16,121 --> 00:14:20,820
models that are based on that module.
I'm sure you could imagine some crazies,

271
00:14:20,920 --> 00:14:23,050
scary scenario where that was the case,
but in general,

272
00:14:24,150 --> 00:14:24,983
I don't think that that's an issue,
but when we think of things like 

273
00:14:26,711 --> 00:14:30,370
internet connectivity or other societal 
factors,

274
00:14:30,840 --> 00:14:31,673
I think that paying attention to 
questions of do we have confounders in 

275
00:14:34,541 --> 00:14:37,060
our data,
are they being picked up by our models?

276
00:14:37,130 --> 00:14:38,110
Um,
is,

277
00:14:38,230 --> 00:14:41,140
is incredibly important.
So,

278
00:14:41,250 --> 00:14:42,083
um,
if you take away nothing else from this 

279
00:14:44,080 --> 00:14:44,913
short talk,
I hope that you take away a caution to 

280
00:14:48,041 --> 00:14:48,874
be aware of differences between your 
training and inference asked the 

281
00:14:52,371 --> 00:14:53,720
question,
uh,

282
00:14:54,110 --> 00:14:54,943
because statistically this is not a 
particularly difficult thing to uncover 

283
00:14:58,220 --> 00:15:03,220
if you take the time to look in a world 
of kaggle competitions and people trying

284
00:15:04,641 --> 00:15:07,700
to get high marks on deep learning 
classes and things like that.

285
00:15:07,820 --> 00:15:08,653
I think it's all too easy for us to just
take datasets as given not thinking 

286
00:15:12,981 --> 00:15:13,814
about them too much and just try and get
our accuracy from 99 point one to 99 

287
00:15:16,581 --> 00:15:17,414
point two.
And as someone who's interested in 

288
00:15:21,171 --> 00:15:24,680
people coming out of programs like this 
being ready to do work in the real world

289
00:15:24,681 --> 00:15:27,500
at a,
I would caution that,

290
00:15:27,850 --> 00:15:28,683
um,
we can't only be training ourselves to 

291
00:15:31,071 --> 00:15:34,070
do that.
So with that,

292
00:15:34,610 --> 00:15:35,443
I'm going to leave you with a set of 
additional resources around machine 

293
00:15:38,151 --> 00:15:38,984
learning fairness.
These are super hot off the presses in 

294
00:15:41,031 --> 00:15:45,170
the sense that this particular little 
website was launched at I think 8:30

295
00:15:45,171 --> 00:15:46,640
this morning,
something like that.

296
00:15:46,641 --> 00:15:49,060
So you've got it first.
Um,

297
00:15:49,400 --> 00:15:54,080
mit leading the way,
I'm in on this page.

298
00:15:54,150 --> 00:15:55,340
Uh,
there are in,

299
00:15:55,390 --> 00:15:56,670
yeah,
you can open your laptops again now.

300
00:15:57,310 --> 00:15:58,930
Um,
uh,

301
00:15:58,940 --> 00:15:59,773
there are a number of papers that go 
through this sort of like a greatest 

302
00:16:03,141 --> 00:16:03,974
hits of the machine learning fairness 
literature from the last couple of 

303
00:16:05,780 --> 00:16:06,050
years.

304
00:16:06,050 --> 00:16:07,970
Um,
really interesting papers.

305
00:16:08,120 --> 00:16:08,953
I don't think any of them are like the 
one final solution to machine learning 

306
00:16:11,631 --> 00:16:14,690
fairness issues,
but they're super interesting reads that

307
00:16:14,691 --> 00:16:18,890
I think help sort of paint the space and
the landscape really usefully.

308
00:16:19,360 --> 00:16:21,740
There are also a couple of interesting 
exercises there,

309
00:16:22,010 --> 00:16:23,240
um,
that,

310
00:16:23,290 --> 00:16:25,040
uh,
you can access by a colab.

311
00:16:25,100 --> 00:16:27,500
Um,
and uh,

312
00:16:27,560 --> 00:16:30,650
if you're interested in this space,
there things that you can play with a,

313
00:16:30,670 --> 00:16:34,720
I think they include one on adversarial 
d biasing where I'm,

314
00:16:34,940 --> 00:16:37,160
because you guys all love deep learning.
Um,

315
00:16:37,190 --> 00:16:42,190
you can use a network to try and become 
unbiased by,

316
00:16:42,370 --> 00:16:43,203
uh,
making sure that by having an extra 

317
00:16:44,691 --> 00:16:45,524
output head that predicts a 
characteristic that you wish to be 

318
00:16:48,681 --> 00:16:53,480
unbiased on and then panelizing that 
model if it's good at predicting that,

319
00:16:53,530 --> 00:16:55,250
that characteristic.
Uh,

320
00:16:55,251 --> 00:16:56,084
and so this is trying to adversarially 
make sure that our internal 

321
00:16:58,221 --> 00:16:59,054
representation in a deep network is not 
picking up a unwanted correlations 

322
00:17:03,200 --> 00:17:05,930
around one biases.
So I hope that that's interesting.

323
00:17:06,160 --> 00:17:07,690
Um,
and,

324
00:17:07,691 --> 00:17:08,524
uh,
I'll be around afterwards to take 

325
00:17:09,921 --> 00:17:10,754
questions,
but at this point I'd like to make sure 

326
00:17:12,651 --> 00:17:15,110
that sunshine has plenty of time.
So thank you very much.

