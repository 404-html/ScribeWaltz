1
00:00:03,990 --> 00:00:04,823
Hi everybody.
My name is serine and I'm going to be 

2
00:00:06,271 --> 00:00:09,150
talking about how to use neural networks
to model sequences.

3
00:00:09,990 --> 00:00:10,823
In the previous lecture you saw how you 
could use a neural network to model a 

4
00:00:13,171 --> 00:00:14,004
Dataset of many examples.
The difference with sequences is that 

5
00:00:16,860 --> 00:00:19,770
each example consists of multiple data 
points.

6
00:00:20,370 --> 00:00:23,670
There can be a variable number of these 
data points per example,

7
00:00:24,060 --> 00:00:26,970
and the data points can depend on each 
other in complicated ways.

8
00:00:30,190 --> 00:00:32,770
So a sequence could be something like a 
sentence,

9
00:00:33,010 --> 00:00:35,080
like this morning I took the dog for a 
walk.

10
00:00:35,410 --> 00:00:36,243
This is one example,
but it consists of multiple words and 

11
00:00:38,981 --> 00:00:39,814
the words depend on each other.
Another example would be something like 

12
00:00:42,491 --> 00:00:46,000
a medical record one medical record 
would be one example,

13
00:00:46,330 --> 00:00:47,163
but it consists of many measurements.
Another example would be something like 

14
00:00:50,651 --> 00:00:53,380
a speech wave form where this one wave 
form as an example,

15
00:00:53,470 --> 00:00:55,660
but again it consists of many,
many measurements.

16
00:00:57,510 --> 00:01:00,960
You've probably encountered sequence 
modeling tasks in your everyday life,

17
00:01:00,990 --> 00:01:01,823
especially if you've used things like 
google translate Alexa or Siri tasks 

18
00:01:04,741 --> 00:01:05,574
like machine translation and question 
answering are all sequenced modeling 

19
00:01:08,401 --> 00:01:09,234
tasks.
And the state of the art in these tasks 

20
00:01:10,861 --> 00:01:12,360
is mostly deep learning based.

21
00:01:16,850 --> 00:01:17,683
Another interesting example I saw 
recently was this self parking car by 

22
00:01:21,171 --> 00:01:22,610
Audi.
When you think about it,

23
00:01:22,611 --> 00:01:23,444
parking is also a sequence modeling task
because parking is just a sequence of 

24
00:01:26,991 --> 00:01:30,650
movements and the next movement depends 
on all the previous movements.

25
00:01:31,370 --> 00:01:33,830
Um,
you can watch the rest of this video,

26
00:01:34,360 --> 00:01:35,450
uh,
online.

27
00:01:36,260 --> 00:01:39,110
Okay,
so a sequence modeling problem.

28
00:01:39,180 --> 00:01:40,013
Now I'm just going to walk through a 
sequence modeling problem to kind of 

29
00:01:41,961 --> 00:01:42,794
motivate why we need a different 
framework for specifically for modeling 

30
00:01:46,191 --> 00:01:49,550
sequences and what we should be looking 
for in that framework.

31
00:01:49,640 --> 00:01:50,473
Okay,

32
00:01:51,540 --> 00:01:53,610
so the problem is predicting the next 
word.

33
00:01:54,120 --> 00:01:57,510
Given these words,
we want to predict what comes next.

34
00:01:58,350 --> 00:01:59,183
The first problem we run into is that 
machine learning models that are not 

35
00:02:02,341 --> 00:02:03,174
explicitly designed to deal with 
sequences take as input a fixed length 

36
00:02:07,441 --> 00:02:08,274
factor.
Think back to the feed forward neural 

37
00:02:10,351 --> 00:02:11,184
network.
From the first lecture that Alexander 

38
00:02:12,451 --> 00:02:16,650
introduced,
we have to specify the size of the input

39
00:02:16,680 --> 00:02:17,513
right at the outset.
We can't sometimes feed in a vector of 

40
00:02:19,441 --> 00:02:20,274
length.
10 other times feed into elector a 

41
00:02:21,811 --> 00:02:24,330
vector of length 20 it has to be fixed 
line.

42
00:02:24,450 --> 00:02:25,283
So this is kind of an issue with 
sequences because sometimes we might 

43
00:02:28,531 --> 00:02:30,930
have seen 10 words and we want to 
predict the next word.

44
00:02:30,990 --> 00:02:33,720
Sometimes we might've seen forwards and 
we want to predict the next word,

45
00:02:34,080 --> 00:02:38,460
so we have to get that variable length 
input into a fixed length vector.

46
00:02:39,330 --> 00:02:43,330
One simple way to do this would be to 
just cut off the vectors.

47
00:02:43,331 --> 00:02:43,921
So say,
okay,

48
00:02:43,921 --> 00:02:47,820
we're going to just take a fixed window,
forced the specter to be fixed length by

49
00:02:47,821 --> 00:02:48,654
only considering the previous two words,
no matter where we're making the 

50
00:02:51,841 --> 00:02:52,674
prediction.
We'll just take the previous two words 

51
00:02:54,000 --> 00:02:54,833
and then try to predict the next word.
Now we can represent these two words as 

52
00:02:59,591 --> 00:03:00,424
a fixed length vector by creating a 
larger vector and then allocating space 

53
00:03:05,441 --> 00:03:07,330
in it for the first word.
And for the second word,

54
00:03:08,800 --> 00:03:10,000
we have a fixed length vector.
Now,

55
00:03:10,001 --> 00:03:10,834
no matter what,
two words we're using and we can feed 

56
00:03:12,671 --> 00:03:13,504
this into a machine learning model like 
a feed forward neural network or a 

57
00:03:16,001 --> 00:03:19,720
logistic regression or any other model 
and try to make a prediction.

58
00:03:21,010 --> 00:03:25,270
One thing you might be noticing here is 
that by using this fixed window,

59
00:03:25,271 --> 00:03:27,790
we're giving ourselves a very limited 
history.

60
00:03:28,030 --> 00:03:32,200
We're trying to predict the word walk.
Having only seen the words for and a,

61
00:03:32,470 --> 00:03:35,740
this is almost impossible,
put differently.

62
00:03:35,770 --> 00:03:39,670
It's really hard to model longterm 
dependencies to see this clearly.

63
00:03:39,671 --> 00:03:41,580
Consider the word in,
sorry,

64
00:03:41,600 --> 00:03:44,830
consider the sentence in France.
I had a great time and I learned some of

65
00:03:44,831 --> 00:03:45,664
the blank language.
We're trying to predict the word in the 

66
00:03:47,111 --> 00:03:48,910
blank.
I knew it was French,

67
00:03:48,911 --> 00:03:51,640
but that's because I looked very far 
back at the word France that appeared in

68
00:03:51,641 --> 00:03:52,474
the beginning of the sentence.
If we were only looking at the past two 

69
00:03:54,641 --> 00:03:57,190
words or the past three words or even 
the past five words,

70
00:03:57,191 --> 00:03:59,290
it will be really hard to guess the word
in that blank.

71
00:04:00,700 --> 00:04:04,210
So we don't want to limit ourselves so 
much.

72
00:04:04,300 --> 00:04:05,133
We want to ideally use all of the 
information that we have and the 

73
00:04:07,721 --> 00:04:10,630
sequence,
but we also need a fixed length vector.

74
00:04:11,350 --> 00:04:14,740
So one way we could do this is by using 
the entire sequence,

75
00:04:14,770 --> 00:04:17,980
but representing it as a set of counts 
in language.

76
00:04:17,981 --> 00:04:20,560
This representation is also known as a 
bag of words.

77
00:04:20,950 --> 00:04:25,950
All this is is a vector in which each 
slot represents a word and the number in

78
00:04:25,961 --> 00:04:29,890
that slot represents the number of times
that that word occurs in the sentence.

79
00:04:30,250 --> 00:04:32,530
So here,
the second slot represents the word this

80
00:04:32,650 --> 00:04:35,380
and there's a one because this appears 
once in the sentence.

81
00:04:36,010 --> 00:04:38,980
Now we have fixed length factor.
No matter how many words we have,

82
00:04:39,250 --> 00:04:42,370
the vector will always be the same size 
as the council just be different.

83
00:04:43,150 --> 00:04:46,450
We can feed this into a machine learning
model and try to make a prediction.

84
00:04:47,140 --> 00:04:47,973
The problem you may be noticing here is 
that we're losing all of the sequential 

85
00:04:50,951 --> 00:04:51,784
information.
These counts don't preserve any order 

86
00:04:54,371 --> 00:04:57,730
that we had in the sequence to see why 
this is really bad.

87
00:04:57,730 --> 00:05:00,250
Consider these two sentences.
The food was good,

88
00:05:00,280 --> 00:05:04,330
not bad at all versus the food was bad.
Not good at all.

89
00:05:04,810 --> 00:05:05,643
These are completely opposite sentences,
but their bag of words representation 

90
00:05:08,770 --> 00:05:12,880
would be exactly the same because they 
contain the same set of words.

91
00:05:13,870 --> 00:05:16,690
So by representing our sentence as 
counts,

92
00:05:16,720 --> 00:05:18,550
we're losing all of the sequential 
information,

93
00:05:18,551 --> 00:05:21,220
which is really important because we're 
trying to model sequences.

94
00:05:22,600 --> 00:05:23,411
Okay,
so what do we know?

95
00:05:23,411 --> 00:05:26,590
Now we want to preserve order in the 
sequence,

96
00:05:26,830 --> 00:05:29,620
but we also don't want to cut it off to 
um,

97
00:05:29,650 --> 00:05:32,200
to a very short length.
You might be saying,

98
00:05:32,201 --> 00:05:34,480
well,
why don't we just use a really big fixed

99
00:05:34,481 --> 00:05:35,314
window?
Before we were having issues because we 

100
00:05:36,491 --> 00:05:37,324
were just using a fixed window of size 
to what if we extended that to be a 

101
00:05:40,331 --> 00:05:41,164
fixed window of size seven.
And we think that by looking at seven 

102
00:05:43,661 --> 00:05:44,494
words,
we can get most of the context that we 

103
00:05:45,491 --> 00:05:46,870
need.
Well,

104
00:05:46,871 --> 00:05:47,261
yeah,
okay,

105
00:05:47,261 --> 00:05:48,094
we can do that.
Now we have another fixed length vector 

106
00:05:50,141 --> 00:05:51,340
just like before.
It's bigger,

107
00:05:51,341 --> 00:05:52,174
but it's still fixed length.
We have allocated space for each of the 

108
00:05:53,831 --> 00:05:56,410
seven words.
We can feed this into a model and try to

109
00:05:56,411 --> 00:05:57,250
make a prediction.

110
00:05:58,710 --> 00:06:02,940
The problem here is that I consider this
in the scenario where we're feeding this

111
00:06:02,941 --> 00:06:05,520
input factor into a feed forward neural 
network.

112
00:06:06,150 --> 00:06:06,983
Each of those inputs,
each of those ones and zeroes has a 

113
00:06:09,181 --> 00:06:11,460
separate weight connecting it to the 
network.

114
00:06:12,270 --> 00:06:15,540
If we see the words this morning at the 
beginning of the sentence,

115
00:06:15,600 --> 00:06:17,880
very,
very commonly the network,

116
00:06:17,881 --> 00:06:20,910
we'll learn that this morning represents
a time where a setting,

117
00:06:21,870 --> 00:06:24,690
if this morning then appears at the end 
of the sentence,

118
00:06:24,960 --> 00:06:25,793
we'll have a lot of trouble recognizing 
that because the weights at the end of 

119
00:06:28,831 --> 00:06:29,664
the vector never saw that phrase before 
and the weights from the beginning of 

120
00:06:33,811 --> 00:06:35,910
the vector and not being shared with the
end.

121
00:06:37,870 --> 00:06:40,180
In other words,
things we learn about the sequence won't

122
00:06:40,181 --> 00:06:43,660
transfer if they appear at different 
points in the sequence because we're not

123
00:06:43,661 --> 00:06:44,950
sharing any parameters.

124
00:06:47,790 --> 00:06:50,730
All right,
so you kind of see all the problems that

125
00:06:50,731 --> 00:06:53,280
arise with sequences now and why we need
a different framework.

126
00:06:53,281 --> 00:06:55,890
Specifically,
we want to be able to deal with variable

127
00:06:55,891 --> 00:06:56,724
length sequences.
We want to maintain sequence orders so 

128
00:06:59,611 --> 00:07:01,800
we can keep all of that sequential 
control information.

129
00:07:02,310 --> 00:07:03,143
We want to keep track of longer term 
dependencies rather than cutting it off 

130
00:07:06,151 --> 00:07:06,984
too short and we want to be able to 
share parameters across the sequence so 

131
00:07:10,980 --> 00:07:11,813
we don't have to relearn things across 
the sequence because this is a class 

132
00:07:14,731 --> 00:07:15,564
about deep learning.
I'm going to talk about how to address 

133
00:07:17,041 --> 00:07:19,770
these problems with neural networks,
but no,

134
00:07:19,771 --> 00:07:20,604
that time series modeling and sequential
modeling is a very active field in 

135
00:07:24,391 --> 00:07:25,224
machine learning and,
and it has been and there are lots of 

136
00:07:26,731 --> 00:07:27,564
other machine learning methods that have
been developed to deal with these 

137
00:07:29,131 --> 00:07:29,964
problems.
But for now all talk about recurrent 

138
00:07:32,341 --> 00:07:33,174
neural networks.

139
00:07:35,860 --> 00:07:36,180
Okay.

140
00:07:36,180 --> 00:07:37,013
Okay.
So a recurrent neural network is 

141
00:07:38,790 --> 00:07:42,150
architected in the same way as a normal 
neural network.

142
00:07:42,151 --> 00:07:42,984
We have some inputs,
we have some hidden layers and we have 

143
00:07:45,780 --> 00:07:48,990
some outputs.
The only difference is that each hitting

144
00:07:48,991 --> 00:07:51,740
unit is doing a slightly different 
functions.

145
00:07:51,741 --> 00:07:55,350
So let's take a look at this one hidden 
unit to see exactly what it's doing.

146
00:07:58,380 --> 00:08:03,380
Ever current hidden unit computes a 
function of an input and it's own.

147
00:08:03,511 --> 00:08:08,511
Previous output it's own previous output
is also known as the cell state.

148
00:08:09,030 --> 00:08:13,400
And in the diagram it's denoted by s the
subscript is the time timestamp.

149
00:08:14,070 --> 00:08:16,650
So at the very first time stuff t equals
zero.

150
00:08:17,490 --> 00:08:18,323
The recurrent current unit computes a 
function of the input at t equals zero 

151
00:08:22,410 --> 00:08:27,410
and of its initial state.
Similarly at the next time stop,

152
00:08:27,960 --> 00:08:32,070
it computes a function of the new input 
and it's previous cell state.

153
00:08:33,270 --> 00:08:34,980
If you look at the function at the 
bottom,

154
00:08:34,981 --> 00:08:35,814
the function to compute as to you'll see
it's really similar to the function for 

155
00:08:39,180 --> 00:08:41,220
that a hidden unit in a feed forward 
network,

156
00:08:41,550 --> 00:08:42,720
um,
computes.

157
00:08:43,410 --> 00:08:46,980
The only difference is that we're adding
in an additional term to incorporate its

158
00:08:46,981 --> 00:08:47,814
own previous state.
Common way of viewing recurrent neural 

159
00:08:52,831 --> 00:08:55,770
networks is by unfolding them across 
time.

160
00:08:56,190 --> 00:08:59,130
So this is the same hidden unit at 
different points in time.

161
00:08:59,790 --> 00:09:04,380
Here you can see that at every point in 
time it takes us input it's own previous

162
00:09:04,381 --> 00:09:07,380
state and the new input at that time 
step.

163
00:09:09,400 --> 00:09:10,233
One thing to notice here is that 
throughout the sequence we're using the 

164
00:09:12,941 --> 00:09:15,150
same weight matrices.
W and u.

165
00:09:16,180 --> 00:09:18,190
This solves our problem,
a parameter sharing.

166
00:09:18,640 --> 00:09:21,100
We don't have new parameters for every 
point in the sequence.

167
00:09:21,101 --> 00:09:21,934
Once we learned something,
it'll can apply at any point in the 

168
00:09:23,531 --> 00:09:24,364
sequence.
This also helps us deal with variable 

169
00:09:26,531 --> 00:09:30,100
lengths sequences because we're not pre 
specifying the length of the sequence.

170
00:09:30,610 --> 00:09:33,220
We don't have separate parameters for 
every point in the sequence.

171
00:09:33,221 --> 00:09:34,054
So in some cases we can unroll this rnn 
to four time steps and other cases we 

172
00:09:38,381 --> 00:09:40,090
can unroll it to 10 times steps.

173
00:09:41,870 --> 00:09:42,460
Yeah.

174
00:09:42,460 --> 00:09:44,430
Final thing to notice is that Essa,
Ben,

175
00:09:44,460 --> 00:09:45,293
the cell state at time n can contain 
information from all of the past time 

176
00:09:49,051 --> 00:09:49,884
steps.
Notice that each cell state is a 

177
00:09:52,351 --> 00:09:53,184
function of the previous cell state,
which is the function which is a 

178
00:09:55,111 --> 00:09:57,030
function of the previous cell state and 
so on.

179
00:09:57,780 --> 00:09:58,613
So this kind of solves our issue of 
longterm dependencies because at a time 

180
00:10:02,461 --> 00:10:03,294
step,
very far in the future that sal state 

181
00:10:06,931 --> 00:10:10,020
encompasses information about all of the
previous cell states.

182
00:10:13,460 --> 00:10:14,293
All right,
so now that you kind of understand what 

183
00:10:16,780 --> 00:10:19,420
a recurrent neural network is,
and just to clarify,

184
00:10:19,421 --> 00:10:21,670
actually shown you one hitting unit in 
the previous slide,

185
00:10:21,671 --> 00:10:23,860
but in a full network you would have 
many,

186
00:10:23,861 --> 00:10:27,490
many of those hidden units and even many
layers of many hidden units.

187
00:10:28,540 --> 00:10:31,960
So now we can talk about how you would 
train a recurrent neural network.

188
00:10:33,160 --> 00:10:35,980
It's really similar to how you train a 
normal neuro network.

189
00:10:35,981 --> 00:10:36,814
It's backpropagation.
There's just an additional time 

190
00:10:38,651 --> 00:10:43,180
dementia.
As a reminder in backpropagation,

191
00:10:43,420 --> 00:10:48,010
we want to find the parameters that 
minimize some loss function.

192
00:10:48,370 --> 00:10:49,203
The way that we do this is by first 
taking the derivative of the loss with 

193
00:10:52,511 --> 00:10:53,344
respect to each of the parameters and 
then shifting the parameters in the 

194
00:10:56,651 --> 00:11:00,250
opposite direction in order to try and 
minimize the loss.

195
00:11:01,000 --> 00:11:01,833
This process is called gradient descent.
So one difference with rns is that we 

196
00:11:08,221 --> 00:11:09,054
have many time steps so we can produce 
an output at every time step because we 

197
00:11:13,381 --> 00:11:14,214
have an output at every time step.
We can have a loss at every time step 

198
00:11:17,760 --> 00:11:20,130
rather than just one single loss of fee 
end.

199
00:11:21,520 --> 00:11:22,353
Because,
and the way that we deal with this is 

200
00:11:23,251 --> 00:11:24,084
pretty simple.
The total loss is just the sum of the 

201
00:11:26,131 --> 00:11:29,190
losses that every time stuff.
Similarly,

202
00:11:29,191 --> 00:11:33,480
the total gradient is just the sum of 
the gradients at every time step.

203
00:11:36,530 --> 00:11:37,363
So we can try this out by walking 
through this gradient computation for a 

204
00:11:41,841 --> 00:11:42,674
single parameter.
W W is the weight matrix that we're 

205
00:11:45,201 --> 00:11:49,220
multiplying by our inputs.
We know that the total loss,

206
00:11:49,550 --> 00:11:50,383
the the total gradient,
so the derivative of loss with respect 

207
00:11:52,911 --> 00:11:56,710
to will be the sum of the gradients at 
every time step.

208
00:11:57,220 --> 00:12:01,210
So for now we can focus on a single time
step knowing that at the end we would do

209
00:12:01,211 --> 00:12:04,720
this for each of the time steps and then
sum them up to get the total gradient.

210
00:12:06,130 --> 00:12:07,450
So let's take time.
Step two,

211
00:12:08,320 --> 00:12:10,790
we can solve this gradient using the 
chain rule.

212
00:12:11,110 --> 00:12:11,943
So the derivative of the loss with 
respect to w is the derivative of the 

213
00:12:15,431 --> 00:12:16,264
loss with respect to the output,
the derivative of the output with 

214
00:12:19,301 --> 00:12:20,134
respect to the south state at time two 
and then the derivative of the cell 

215
00:12:23,321 --> 00:12:24,730
state.
With respect to w.

216
00:12:25,720 --> 00:12:26,553
So this seems fine,
but let's take a closer look at this 

217
00:12:29,651 --> 00:12:34,651
last term.
You'll notice that as to also depends on

218
00:12:35,111 --> 00:12:40,111
s one and s one also depends on w.
So we can't just leave that last term as

219
00:12:40,841 --> 00:12:41,674
a constant.
We actually have to expand it out 

220
00:12:43,481 --> 00:12:45,250
farther.
Okay.

221
00:12:45,251 --> 00:12:46,084
So how do we expand this out farther?
What we really want to know is how 

222
00:12:49,271 --> 00:12:53,380
exactly does the cell state at time step
to depend on w?

223
00:12:55,250 --> 00:12:58,010
Well it depends directly on w because it
feeds right in.

224
00:12:58,460 --> 00:12:59,293
We also saw that as two depends on s one
which depends on w and you can also see 

225
00:13:04,191 --> 00:13:09,020
that s two depends on ss zero,
which also depends on w in other words.

226
00:13:10,100 --> 00:13:13,220
And here I'm just writing it as a 
summation and those uh,

227
00:13:13,230 --> 00:13:14,063
the,
the sun that you saw on the previous 

228
00:13:16,610 --> 00:13:17,443
slide as a summation form and you can 
see that the last two terms are 

229
00:13:20,571 --> 00:13:21,404
basically summing the contributions of w
in previous time stops to the error at 

230
00:13:26,451 --> 00:13:27,320
time.
Step t,

231
00:13:28,400 --> 00:13:31,910
this is key to how we model longer term 
dependencies.

232
00:13:32,420 --> 00:13:35,120
This gradient is how we shift our 
parameters and our parameters.

233
00:13:35,121 --> 00:13:35,954
Define our network by shifting our 
parameters such that they include 

234
00:13:40,640 --> 00:13:45,640
contributions to the error from past 
time stops there,

235
00:13:45,861 --> 00:13:48,530
shifted to model longer term 
dependencies.

236
00:13:51,590 --> 00:13:53,780
And here I'm just writing it as a 
general,

237
00:13:53,781 --> 00:13:55,370
some not just for time stuff too.

238
00:13:58,090 --> 00:13:58,380
Okay.

239
00:13:58,380 --> 00:13:59,280
Um,
okay.

240
00:13:59,281 --> 00:14:02,850
So this is basically the process of 
backpropagation to tap through time.

241
00:14:03,150 --> 00:14:03,983
You would do this for every parameter in
your network and then use that in the 

242
00:14:07,171 --> 00:14:08,280
process of gradient descent.

243
00:14:09,910 --> 00:14:10,540
Yeah.

244
00:14:10,540 --> 00:14:13,300
In practice,
rns are a bit difficult to train,

245
00:14:13,870 --> 00:14:18,300
so I kind of want to go through why that
is and um,

246
00:14:18,940 --> 00:14:19,773
what some ways,
some ways that we can address these 

247
00:14:21,461 --> 00:14:25,840
issues.
So let's go back to the summation.

248
00:14:25,990 --> 00:14:26,823
As a reminder,
this is the derivative of the loss with 

249
00:14:29,441 --> 00:14:33,430
respect to w and this is what we would 
use to shift our parameters.

250
00:14:33,460 --> 00:14:34,293
W The last two terms are considering the
air of wwe at all of the previous time 

251
00:14:42,991 --> 00:14:45,880
steps.
Let's take a look at this one term.

252
00:14:45,910 --> 00:14:49,150
This is how we,
this is the derivative of the cell state

253
00:14:49,151 --> 00:14:49,901
at time.
Step two,

254
00:14:49,901 --> 00:14:51,860
with respect to each of the cell states.

255
00:14:53,000 --> 00:14:53,400
Okay.

256
00:14:53,400 --> 00:14:57,150
You might notice that this itself is 
also a chain rule because as two depends

257
00:14:57,151 --> 00:15:00,600
on s one and s one depends on a zero.
We can expand this out farther.

258
00:15:01,650 --> 00:15:02,483
This is just for the derivative of s two
with respect to a zero but what if we 

259
00:15:05,761 --> 00:15:06,594
were looking at a time step very far in 
the future like time step n that term 

260
00:15:11,881 --> 00:15:16,881
would expand into a product of an terms 
and okay,

261
00:15:17,551 --> 00:15:18,384
you might be thinking so what?
Well as notice that as the gap between 

262
00:15:22,411 --> 00:15:27,000
time stops gets bigger and bigger,
this product in the gradient gets longer

263
00:15:27,001 --> 00:15:30,570
and longer and if we look at each of 
these terms,

264
00:15:30,770 --> 00:15:32,040
what,
what are each of these terms?

265
00:15:32,041 --> 00:15:35,850
They all kind of take the same form.
It's the derivative of a cell state with

266
00:15:35,851 --> 00:15:36,684
respect to the previous cell state.
That term can be written like this and 

267
00:15:43,581 --> 00:15:47,150
the actual form that actual formula 
isn't that important.

268
00:15:47,151 --> 00:15:50,600
Just notice that it's a product of two 
terms,

269
00:15:50,800 --> 00:15:55,800
ws and F primes.
W's are our weight matrices.

270
00:15:57,520 --> 00:16:00,610
These are sampled mostly from a standard
normal distribution,

271
00:16:00,970 --> 00:16:01,803
so most of the terms will be less than 
one f prime is the derivative of our 

272
00:16:05,741 --> 00:16:09,580
activation function.
If we use an activation function such as

273
00:16:09,581 --> 00:16:13,840
the hyperbolic tangent or a sigmoid f,
prime will always be less than one.

274
00:16:16,010 --> 00:16:16,650
Okay.

275
00:16:16,650 --> 00:16:19,140
In other words,
we're multiplying a lot of small numbers

276
00:16:19,141 --> 00:16:23,220
together in this product.
Okay,

277
00:16:23,310 --> 00:16:27,870
so what does this mean?
Basically it recall that this product is

278
00:16:27,871 --> 00:16:32,871
how we're adding the gradient from 
future time steps to the gradient.

279
00:16:33,630 --> 00:16:34,463
Sorry.
How we're adding the gradient from 

280
00:16:34,981 --> 00:16:38,040
pastime steps to the gradient at a 
future time stuff.

281
00:16:38,720 --> 00:16:39,553
Okay.

282
00:16:40,100 --> 00:16:40,933
What's happening then is that errors due
to further and further back time stops 

283
00:16:44,360 --> 00:16:45,193
have increasingly smaller gradients 
because that product for further back 

284
00:16:48,681 --> 00:16:52,640
time stops will be longer and since the 
numbers are all decimals,

285
00:16:52,670 --> 00:16:53,780
there'll be,
it will be,

286
00:16:53,781 --> 00:16:55,670
it will.
It will become increasingly smaller.

287
00:16:58,880 --> 00:16:59,713
What does,
what this ends up meaning at a high 

288
00:17:02,361 --> 00:17:03,194
level is that our parameters will become
biased to capture a shorter term 

289
00:17:05,931 --> 00:17:06,764
dependencies.
The errors that arise from further and 

290
00:17:09,201 --> 00:17:10,034
further back time stops will be harder 
and harder to propagate into the 

291
00:17:12,951 --> 00:17:17,951
gradient at future time stops.
Recall that recall,

292
00:17:19,081 --> 00:17:20,910
this example that I showed at the 
beginning,

293
00:17:22,110 --> 00:17:22,943
the whole point of using recurrent 
neural networks is because we wanted to 

294
00:17:25,561 --> 00:17:26,394
model longterm dependencies,
but if our parameters are biased to 

295
00:17:28,981 --> 00:17:32,220
capture short term dependencies,
even if they see the whole sequence,

296
00:17:32,221 --> 00:17:35,520
there'll be by the parameters will 
become biased to to predict things based

297
00:17:35,521 --> 00:17:40,320
mostly on the past couple of words.
Um,

298
00:17:40,980 --> 00:17:44,580
okay,
so now I'm going to go through some,

299
00:17:44,610 --> 00:17:49,610
a couple methods that are used to 
address this issue in practice that work

300
00:17:49,721 --> 00:17:50,554
pretty well.

301
00:17:57,270 --> 00:17:59,310
The first one is the choice of 
activation function.

302
00:18:00,060 --> 00:18:03,660
So you saw that one of the terms that 
was making that product really small was

303
00:18:03,661 --> 00:18:07,530
the f prime term.
I've crime is the derivative of whatever

304
00:18:07,531 --> 00:18:08,364
activation function we choose to use.
Here I've plotted the derivatives of 

305
00:18:12,121 --> 00:18:12,954
some common activation functions.
You could see that the derivative of 

306
00:18:15,421 --> 00:18:18,270
hyperbolic tangent and sigmoid is always
less than one.

307
00:18:18,271 --> 00:18:20,670
In fact for sigmoid is always less than 
0.25

308
00:18:21,380 --> 00:18:24,540
and instead we choose to use an 
activation function like rarely.

309
00:18:24,570 --> 00:18:25,403
It's always one above zero so that will 
at least prevent the f prime terms from 

310
00:18:29,851 --> 00:18:30,684
shrinking the gradient.

311
00:18:32,120 --> 00:18:32,420
Okay.

312
00:18:32,420 --> 00:18:34,970
Another solution would be how we 
initialize our weights.

313
00:18:35,660 --> 00:18:37,910
If we initialize the weights from a 
normal distribution,

314
00:18:37,911 --> 00:18:38,744
there'll be mostly less than one and 
they'll immediately strength the 

315
00:18:40,501 --> 00:18:41,334
gradients.
If instead we initialize the weights to 

316
00:18:44,511 --> 00:18:45,344
something like the identity matrix,
it will at least prevent that w term 

317
00:18:49,160 --> 00:18:52,220
from shrinking the product at least at 
the beginning.

318
00:18:54,460 --> 00:18:56,650
The next deletion is very different.
Um,

319
00:18:56,800 --> 00:18:57,633
it involves actually adding a lot more 
complexity to the network using a more 

320
00:19:01,901 --> 00:19:02,734
complex type of cell called a gated sell
rather than here rather than each node 

321
00:19:07,271 --> 00:19:10,510
just being that simple rnn unit that I 
showed at the beginning,

322
00:19:10,990 --> 00:19:13,350
we'll replace it with a much more 
complicated cell.

323
00:19:13,980 --> 00:19:14,813
A very common gated cell is something 
called an Lstm or long short term 

324
00:19:18,191 --> 00:19:19,024
memory.
So like its name implies lstm cells are 

325
00:19:21,791 --> 00:19:22,624
able to keep memory within the cell 
state unchanged for many times stuffs 

326
00:19:27,130 --> 00:19:30,580
this allows them to effectively model 
longer term dependencies.

327
00:19:31,740 --> 00:19:35,970
So I'm going to go through a very high 
level overview of how lsts work,

328
00:19:36,060 --> 00:19:36,893
but if you're interested,
feel free to email me or ask me 

329
00:19:38,671 --> 00:19:42,660
afterwards and I can direct you to some 
more resources to read about.

330
00:19:42,661 --> 00:19:44,010
LSTM is a lot more detail.

331
00:19:45,100 --> 00:19:45,933
Okay.

332
00:19:45,980 --> 00:19:46,813
All right.
So lstm basically have a three step 

333
00:19:49,101 --> 00:19:49,934
process.
The first step is to forget your 

334
00:19:52,671 --> 00:19:55,370
relevant parts of the cell state.
For example,

335
00:19:55,371 --> 00:19:58,400
if we're modeling a sentence and we see 
a new subject,

336
00:19:58,790 --> 00:19:59,623
we might want to forget things about the
old subject because we know that future 

337
00:20:02,361 --> 00:20:05,360
words will be conjugated according to 
the new subject.

338
00:20:06,770 --> 00:20:07,370
Okay.

339
00:20:07,370 --> 00:20:09,770
The next day,
the next step is an update stuff.

340
00:20:10,400 --> 00:20:13,670
Here's where we actually update the cell
state to reflect the new,

341
00:20:13,671 --> 00:20:16,730
the information from the new input.
In this example,

342
00:20:16,760 --> 00:20:18,710
like I said,
if we've just seen a new subject,

343
00:20:19,040 --> 00:20:19,873
we might want to,
this is where we actually update the 

344
00:20:21,501 --> 00:20:25,040
cell state with the gender or whether 
the new subject is plural or singular.

345
00:20:26,760 --> 00:20:27,120
Yeah.

346
00:20:27,120 --> 00:20:30,480
Finally we want to output certain parts 
of the cell state.

347
00:20:31,710 --> 00:20:35,190
So if we've just seen a subject,
we have an idea that the next word might

348
00:20:35,191 --> 00:20:36,830
be a verb.
So we'll,

349
00:20:36,831 --> 00:20:41,100
I'll put information relevant to 
predicting a verb like the 10th.

350
00:20:44,010 --> 00:20:48,580
Each of these three steps is implemented
using a set of logic gates and the logic

351
00:20:48,581 --> 00:20:50,650
gates are implemented using sigma 
functions.

352
00:20:52,790 --> 00:20:53,623
To give you some intuition on why Lstm 
is help with the vanishing gradient 

353
00:20:56,271 --> 00:20:57,830
problem,
uh,

354
00:20:57,860 --> 00:20:58,693
is that first the forget gate.
The first step can equivalently be 

355
00:21:02,541 --> 00:21:03,374
called the remember gate because there 
you're choosing what to forget and what 

356
00:21:06,711 --> 00:21:08,390
to keep in the cell state.

357
00:21:09,820 --> 00:21:14,290
The forget gate can choose to keep 
information in the cell state for many,

358
00:21:14,291 --> 00:21:15,124
many times steps.
There's no activation function or 

359
00:21:18,071 --> 00:21:20,410
anything else shrinking that 
information.

360
00:21:21,850 --> 00:21:22,683
The second step,
the second thing is that the cell state 

361
00:21:24,400 --> 00:21:27,210
is separate from what's outputted.
We're mate.

362
00:21:27,550 --> 00:21:28,383
This is not true of normal recurrent 
units like I showed you before in a 

363
00:21:33,221 --> 00:21:35,980
simple recurrent unit.
The cell state is the same thing as what

364
00:21:35,981 --> 00:21:40,420
that sell outputs with an Lstm,
it has a separate cell state and it only

365
00:21:40,421 --> 00:21:44,660
needs to output information relevant to 
the prediction at that time stuff.

366
00:21:45,280 --> 00:21:46,113
Because of this,
it can keep information in the cell 

367
00:21:47,651 --> 00:21:48,484
state,
which might not be relevant at this 

368
00:21:49,811 --> 00:21:50,644
time.
Stuff that might be relevant at a much 

369
00:21:51,791 --> 00:21:52,624
later time step.
So we can keep that information without 

370
00:21:55,031 --> 00:21:57,610
being penalized for that.
Finally,

371
00:21:57,611 --> 00:21:59,680
I didn't indicate this explicitly in the
diagram,

372
00:21:59,710 --> 00:22:00,543
but the way that the update step happens
is through an additive function not 

373
00:22:03,041 --> 00:22:05,920
through a multiplicative function.
So when we take the derivative,

374
00:22:05,921 --> 00:22:06,754
there's not a huge expansion.
So now I just want to move on to going 

375
00:22:12,641 --> 00:22:13,780
over,
um,

376
00:22:14,440 --> 00:22:19,440
some possible tasks.
So the first task is a classification.

377
00:22:19,900 --> 00:22:23,110
So here we want to classify tweets as 
positive,

378
00:22:23,111 --> 00:22:24,340
negative or neutral.

379
00:22:25,890 --> 00:22:28,500
And this task is also knowing that 
sentiment analysis.

380
00:22:29,880 --> 00:22:30,290
Okay?

381
00:22:30,290 --> 00:22:31,123
The way that we would design a recurrent
neural network to do this is actually 

382
00:22:33,711 --> 00:22:35,420
not by having an output at every time 
step.

383
00:22:35,450 --> 00:22:38,390
We only want one output for the entire 
sequence.

384
00:22:40,340 --> 00:22:42,830
And so we'll take him the entire 
sequence,

385
00:22:42,950 --> 00:22:45,020
the entire tweet,
one word at a time.

386
00:22:45,410 --> 00:22:48,860
And at the very end we'll produce an 
output which will,

387
00:22:48,890 --> 00:22:53,660
which will actually be a probability 
distribution over possible classes where

388
00:22:53,661 --> 00:22:55,370
our classes in this case would be 
positive,

389
00:22:55,371 --> 00:22:56,390
negative,
or neutral.

390
00:22:57,080 --> 00:22:57,913
Note that the only information that is 
producing the output at the end is the 

391
00:23:02,931 --> 00:23:03,920
final cell state.

392
00:23:05,370 --> 00:23:05,790
Okay.

393
00:23:05,790 --> 00:23:10,170
And so that final cell state kind of has
to summarize all of the information from

394
00:23:10,171 --> 00:23:13,440
the big,
from the entire sequence into that final

395
00:23:13,441 --> 00:23:14,274
cell state.
So we can imagine if we have very 

396
00:23:16,351 --> 00:23:18,300
complicated tweets or,
well,

397
00:23:18,301 --> 00:23:19,134
I don't know if that's possible,
but very complicated paragraphs or 

398
00:23:22,020 --> 00:23:25,380
sentences.
We might want to create a bigger network

399
00:23:25,381 --> 00:23:30,120
with more hidden states to allow that 
last state to be more expressive.

400
00:23:33,330 --> 00:23:35,850
The next task would be something like 
music generation,

401
00:23:36,000 --> 00:23:37,560
and I'll see if this will play.

402
00:23:38,780 --> 00:23:39,613
Yeah,

403
00:23:41,010 --> 00:23:46,010
you can kind of hear it.
Okay.

404
00:23:51,170 --> 00:23:53,840
So that was music generated by an Rnn,
which is pretty cool.

405
00:23:53,841 --> 00:23:57,590
And something you're actually also going
to do in the lab today.

406
00:23:58,160 --> 00:23:59,220
But,
uh,

407
00:23:59,240 --> 00:24:00,073
music generally you can,
an rnn can produce music because music 

408
00:24:02,631 --> 00:24:05,600
is just a sequence and the way that you 
would,

409
00:24:08,490 --> 00:24:12,450
the way that you would construct a neuro
on a recurrent neural network to do this

410
00:24:13,080 --> 00:24:18,080
would be at every time point taking in a
note and producing the most likely.

411
00:24:18,211 --> 00:24:20,850
Next note given the notes that you've 
seen so far.

412
00:24:20,851 --> 00:24:23,050
So here you would produce an output at 
every time step.

413
00:24:27,360 --> 00:24:28,193
The final task is machine translation.
Machine translation is interesting 

414
00:24:32,041 --> 00:24:34,920
because it's actually two recurrent 
neural networks,

415
00:24:35,310 --> 00:24:36,810
um,
side by side.

416
00:24:37,080 --> 00:24:41,970
The first is an encoder.
The encoder takes as input a sentence in

417
00:24:42,540 --> 00:24:45,930
a source language like English,
it.

418
00:24:45,931 --> 00:24:46,764
Then there's an a decoder which produces
the same sentence in a target language 

419
00:24:50,460 --> 00:24:51,293
like French notice in this architecture 
that the only information passed from 

420
00:24:56,911 --> 00:24:59,970
the encoder to the decoder is the final 
cell state.

421
00:25:01,410 --> 00:25:05,100
And the idea is that that final cell 
state should be kind of a summary of the

422
00:25:05,101 --> 00:25:08,550
entire encoder sentence.
And given that summary,

423
00:25:08,551 --> 00:25:09,384
the decoder should be able to figure out
what the encoder sentence was about and 

424
00:25:12,391 --> 00:25:14,640
then produce the same sentence in a 
different language.

425
00:25:15,030 --> 00:25:16,560
You can imagine though that,
okay,

426
00:25:16,561 --> 00:25:17,394
maybe this is possible for a sentence,
a really simple sentence like the dog 

427
00:25:20,610 --> 00:25:21,443
eats.
Maybe we can encode that in the final 

428
00:25:22,711 --> 00:25:23,544
cell state,
but if we had a much more complicated 

429
00:25:25,201 --> 00:25:26,034
sentence or a much longer sentence,
that would be very difficult to try and 

430
00:25:29,881 --> 00:25:31,710
summarize the whole thing and that one 
cell state.

431
00:25:32,580 --> 00:25:33,413
So what's typically done in practice for
machine translation is something called 

432
00:25:36,631 --> 00:25:37,464
a tension with attention rather than 
just taking in the final cell state to 

433
00:25:42,390 --> 00:25:43,223
the decoder at each time step,
we take in a weighted sum of all of the 

434
00:25:46,501 --> 00:25:47,334
previous cell states.
So in this case we're trying to produce 

435
00:25:50,221 --> 00:25:51,054
the first word.
We'll take in a weighted sum of all of 

436
00:25:54,901 --> 00:25:55,734
the encoder states.
Most of the weight will probably be on 

437
00:25:57,961 --> 00:25:58,794
the first state because that's what 
would be most relevant to producing the 

438
00:26:01,081 --> 00:26:04,500
first word.
Then when we produced the second word,

439
00:26:04,770 --> 00:26:07,440
most of the weight will probably be on 
the second cell state,

440
00:26:07,441 --> 00:26:11,250
but we might have som on the first and 
the third to try and get an idea for,

441
00:26:11,460 --> 00:26:13,590
um,
the tents or the gender of this now.

442
00:26:15,890 --> 00:26:17,930
And the same thing for all of the cell 
states.

443
00:26:18,320 --> 00:26:19,153
The way that you implement this is just 
by including those weight parameters in 

444
00:26:22,341 --> 00:26:23,174
the weighted sum as additional 
parameters that you chain using 

445
00:26:26,721 --> 00:26:28,490
backpropagation just like everything 
else.

446
00:26:31,680 --> 00:26:32,513
Okay.
So I hope that you have an idea now why 

447
00:26:34,951 --> 00:26:35,784
we need a different framework to model 
sequences and how recurrent neural 

448
00:26:38,341 --> 00:26:41,010
networks can solve some of the issues 
that we saw at the beginning,

449
00:26:41,310 --> 00:26:42,143
um,
as well as an idea of how to them and 

450
00:26:43,711 --> 00:26:46,140
saw some of the vanishing gradient 
problems.

451
00:26:47,040 --> 00:26:47,410
Okay.

452
00:26:47,410 --> 00:26:48,243
I've talked a lot about language,
but you can imagine using these exact 

453
00:26:52,571 --> 00:26:53,404
same neural network,
recurrent neural networks for modeling 

454
00:26:55,541 --> 00:26:56,374
time series or wave forms or doing other
interesting sequence prediction tasks 

455
00:26:59,981 --> 00:27:03,610
like predicting stock market trends or 
summarizing books or articles.

456
00:27:03,940 --> 00:27:04,773
Um,
and maybe you'll consider some sequence 

457
00:27:05,990 --> 00:27:08,710
modeling tasks for your final project.
So thank you.

458
00:27:09,530 --> 00:27:10,400
[inaudible].

