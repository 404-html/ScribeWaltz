1
00:00:02,540 --> 00:00:04,490
All right.
Good morning everyone.

2
00:00:06,730 --> 00:00:10,000
My name is lise Friedman.
I'm a research scientist here at Mit.

3
00:00:10,001 --> 00:00:13,600
I build the autonomous vehicles and 
perception systems for these vehicles.

4
00:00:13,900 --> 00:00:16,060
And today I,
I'd like to talk to him.

5
00:00:16,120 --> 00:00:16,953
First of all was great to be a part of 
this amazing course and intro to deep 

6
00:00:20,081 --> 00:00:21,630
learning.
Um,

7
00:00:21,690 --> 00:00:22,523
it's,
it's a wonderful course covering a 

8
00:00:24,490 --> 00:00:27,040
pretty quickly but deeply,
uh,

9
00:00:27,070 --> 00:00:29,050
some of the fundamental aspects of deep 
learning.

10
00:00:29,051 --> 00:00:29,884
This is awesome.
So the topic that I'm perhaps most 

11
00:00:32,501 --> 00:00:33,334
passionate about from the perspective of
just being a researcher in artificial 

12
00:00:37,091 --> 00:00:39,910
intelligence is deep reinforcement 
learning,

13
00:00:40,300 --> 00:00:45,300
which is a set of ideas and methods that
teach agents to act in the real world.

14
00:00:50,690 --> 00:00:51,340
Okay.

15
00:00:51,340 --> 00:00:55,090
If you think about what machine learning
is,

16
00:00:55,510 --> 00:01:00,510
it allows us to make sense of the world,
but to truly have impact in the physical

17
00:01:01,031 --> 00:01:04,570
world,
we need to also act in that world.

18
00:01:04,630 --> 00:01:05,463
So you bring the understanding of that 
you extract from the world through the 

19
00:01:07,841 --> 00:01:11,170
perception systems and actually decide 
to make actions.

20
00:01:11,560 --> 00:01:12,393
So you can think of intelligence systems
as this kind of stack from the top to 

21
00:01:15,221 --> 00:01:16,054
the bottom and the environment.
At the top of the world that the Asian 

22
00:01:18,491 --> 00:01:19,324
operates in.
And at the bottom is the factors that 

23
00:01:21,250 --> 00:01:22,810
actually make,
uh,

24
00:01:22,840 --> 00:01:26,110
changes to the world by moving the 
robot,

25
00:01:26,140 --> 00:01:28,870
moving the agent in a way that changes 
the world,

26
00:01:28,930 --> 00:01:29,763
that acts in the world.
And so from the environment it goes to 

27
00:01:32,111 --> 00:01:36,010
the sensors sensing the raw sensory 
data,

28
00:01:36,070 --> 00:01:39,100
extracting the features,
making sense of that features,

29
00:01:39,190 --> 00:01:40,023
understanding for me representations 
higher and higher order representations 

30
00:01:42,970 --> 00:01:45,970
from those representations,
we gain knowledge that's useful,

31
00:01:45,971 --> 00:01:46,804
actionable.

32
00:01:46,990 --> 00:01:50,620
Finally,
we reason the thing that we hold so dear

33
00:01:50,680 --> 00:01:55,060
as human beings,
the reasoning that builds and aggregates

34
00:01:55,061 --> 00:01:55,894
knowledge bases and using that reasoning
form short term and long term plans to 

35
00:02:00,041 --> 00:02:02,950
finally turn into action and act in that
world,

36
00:02:02,980 --> 00:02:03,813
changing it.
So that's kind of the stack of 

37
00:02:04,961 --> 00:02:09,280
artificial intelligence.
And the question is how much in the same

38
00:02:09,281 --> 00:02:10,114
way as human beings,
we learn most of the stack when we're 

39
00:02:12,301 --> 00:02:13,134
born,
we know very little and we'd take in 

40
00:02:14,501 --> 00:02:16,240
five sensory,
uh,

41
00:02:16,241 --> 00:02:19,390
sources of sensory data and make sense 
of the world,

42
00:02:19,391 --> 00:02:21,700
learn over time to act successful in 
that world.

43
00:02:22,000 --> 00:02:25,000
How much?
The question is can we use deep learning

44
00:02:25,001 --> 00:02:25,834
methods to learn parts of the stack,
the modules of the stack or the entire 

45
00:02:29,201 --> 00:02:32,110
stack end to end?
Let's go over them.

46
00:02:32,140 --> 00:02:33,930
Okay,
so for uh,

47
00:02:34,000 --> 00:02:36,520
robots that act in the world,
autonomous vehicles,

48
00:02:36,521 --> 00:02:37,660
humanoid,
robots,

49
00:02:37,750 --> 00:02:39,640
drones,
there are sensors,

50
00:02:39,641 --> 00:02:40,474
whether it's slide our camera and 
microphone coming from the audio 

51
00:02:42,970 --> 00:02:43,803
networking for the communications,
I am you getting the kinematics of the 

52
00:02:46,481 --> 00:02:47,410
different vehicles.

53
00:02:47,500 --> 00:02:50,890
That's the raw data coming in.
That's the eyes,

54
00:02:50,891 --> 00:02:53,500
ears,
nose for robots.

55
00:02:53,890 --> 00:02:54,723
Then the ones who have the sensory data,
the task is to form representations on 

56
00:02:59,140 --> 00:03:00,010
data.
Yeah.

57
00:03:00,011 --> 00:03:02,960
You make sense of this raw pixels,
a raw piece,

58
00:03:02,961 --> 00:03:03,794
pieces,
samples for whatever the sensor is and 

59
00:03:05,531 --> 00:03:06,364
you start to try to piece it together 
into something that can be used to gain 

60
00:03:11,291 --> 00:03:14,680
understanding.
It's just numbers and those numbers need

61
00:03:14,681 --> 00:03:17,380
to be converted into something that can 
be reasoned with.

62
00:03:17,620 --> 00:03:21,550
That's forming representations and 
that's where machine learning,

63
00:03:21,551 --> 00:03:24,760
deep learning steps in and takes this 
raw sensory data.

64
00:03:24,970 --> 00:03:27,110
The was some preprocessing,
some profit,

65
00:03:27,111 --> 00:03:27,944
some initial processing,
and for forming higher and higher order 

66
00:03:30,431 --> 00:03:31,264
representation in that data that can be 
reasoned with about in the computer 

67
00:03:34,661 --> 00:03:39,310
vision from edges two faces,
two entire entities.

68
00:03:39,430 --> 00:03:40,263
And finally the interpretation,
the semantic interpretation of the 

69
00:03:43,631 --> 00:03:45,850
scene.
That's machine learning,

70
00:03:46,840 --> 00:03:50,410
playing with the representations and the
reasoning part.

71
00:03:50,410 --> 00:03:51,243
One of the exciting fundamental open 
challenges of machine learning is how 

72
00:03:57,160 --> 00:03:57,993
does this greater and greater 
representation that can be formed 

73
00:04:00,401 --> 00:04:04,240
through deep neural networks can then 
lead to reasoning,

74
00:04:04,241 --> 00:04:06,460
to building knowledge,
not just,

75
00:04:06,600 --> 00:04:08,740
uh,
a memorization task,

76
00:04:08,741 --> 00:04:09,574
which is taking a supervised learning,
memorizing patterns in the input data 

77
00:04:12,211 --> 00:04:16,480
based on human annotations,
but also an extracting those patterns,

78
00:04:16,481 --> 00:04:20,440
but also then taking that knowledge and 
building over time as we humans do,

79
00:04:20,560 --> 00:04:21,393
building into something that can be 
called common sense and to knowledge 

80
00:04:23,501 --> 00:04:26,620
basis in a very trivial task.
This means,

81
00:04:26,890 --> 00:04:30,310
uh,
aggregating fusing multiple types of,

82
00:04:30,510 --> 00:04:31,990
uh,
multiple,

83
00:04:31,991 --> 00:04:33,910
uh,
types of extraction of knowledge.

84
00:04:34,030 --> 00:04:34,863
So from image recognition,
you could think if it looks like a duck 

85
00:04:36,821 --> 00:04:39,490
in an image,
it sounds like a duck when the audio and

86
00:04:39,491 --> 00:04:42,100
then you could do act activity 
recognition with the video.

87
00:04:42,400 --> 00:04:43,233
Uh,
it a swims like a duck and it must be a 

88
00:04:44,501 --> 00:04:44,740
duck.

89
00:04:44,740 --> 00:04:47,260
Just aggregating this trivial,
uh,

90
00:04:47,530 --> 00:04:50,530
different sources of information.
That's reasoning.

91
00:04:50,950 --> 00:04:54,970
Now I think a form the human perspective
from the very biased human perspective,

92
00:04:55,000 --> 00:04:56,330
one of the,
uh,

93
00:04:56,710 --> 00:04:57,543
illustrative aspects of reasoning is 
theory improving is the moment of 

94
00:05:02,891 --> 00:05:03,724
invention,
of creative genius of this breakthrough 

95
00:05:06,041 --> 00:05:09,880
ideas as we humans come up with and mean
really these aren't new ideas.

96
00:05:09,970 --> 00:05:11,860
Whenever we come up with an interesting 
idea,

97
00:05:11,890 --> 00:05:12,723
they're not new.
We're just collecting pieces of high 

98
00:05:15,700 --> 00:05:16,533
word representations of knowledge that 
we've gained over time and then piecing 

99
00:05:19,961 --> 00:05:20,794
them together to form something,
some simple beautiful distillation that 

100
00:05:25,991 --> 00:05:27,630
is useful for,
uh,

101
00:05:27,640 --> 00:05:28,720
for the rest of the world.
And,

102
00:05:28,960 --> 00:05:30,280
uh,
one of my favorite,

103
00:05:30,490 --> 00:05:34,930
uh,
sort of human stories and discoveries in

104
00:05:34,931 --> 00:05:37,720
pure theorem improving is from Oslo.
Last theorem.

105
00:05:37,840 --> 00:05:42,040
It stood for 358 years.
This is a trivial thing to explain.

106
00:05:42,160 --> 00:05:44,120
Most a six,
well,

107
00:05:44,260 --> 00:05:47,410
eight year olds can understand the 
definition of the,

108
00:05:47,680 --> 00:05:49,500
uh,
the conjecture that x,

109
00:05:49,501 --> 00:05:50,334
the n plus y to the n equals z to the n 
has no solution for and greater than a 

110
00:05:54,760 --> 00:05:56,260
three greater than you going to the 
three.

111
00:05:56,620 --> 00:05:57,291
Okay.
This,

112
00:05:57,291 --> 00:05:59,060
this has been unsolved,
has been,

113
00:05:59,120 --> 00:05:59,953
uh,
hundreds of thousands of people try to 

114
00:06:01,371 --> 00:06:02,630
solve it.
And,

115
00:06:02,631 --> 00:06:03,464
uh,
finally Andrew wiles from Oxford and 

116
00:06:05,541 --> 00:06:09,560
Princeton had this final breakthrough 
moment in,

117
00:06:09,561 --> 00:06:12,020
uh,
1994.

118
00:06:12,200 --> 00:06:15,830
So he first approved in 1993 and then it
was shown that he failed.

119
00:06:16,220 --> 00:06:16,921
And then,
so that's,

120
00:06:16,921 --> 00:06:18,200
that's the,
the human drama.

121
00:06:18,380 --> 00:06:19,760
And then night.
So he had a,

122
00:06:19,810 --> 00:06:20,643
uh,
he spent a year trying to find the 

123
00:06:22,311 --> 00:06:23,750
solution.
And there's this moment,

124
00:06:23,960 --> 00:06:24,793
this final breakthrough 358 years after 
this first form that buy from us as a 

125
00:06:29,181 --> 00:06:30,380
conjecture.
He,

126
00:06:30,381 --> 00:06:31,011
he,
uh,

127
00:06:31,011 --> 00:06:34,430
he said it was so incredibly beautiful.
It was so simple,

128
00:06:34,431 --> 00:06:35,264
so elegant.
I couldn't understand how I missed it 

129
00:06:37,310 --> 00:06:39,920
and I just stared at it and this belief 
for 20 minutes,

130
00:06:40,280 --> 00:06:44,300
this is him finally closing the loop,
figuring out the final proof.

131
00:06:45,170 --> 00:06:48,140
I just stared at it and,
but disbelief for 20 minutes.

132
00:06:48,170 --> 00:06:49,003
Then during the day I walked around the 
department and I keep coming back to my 

133
00:06:51,801 --> 00:06:53,690
desk looking to see if it was still 
there.

134
00:06:53,840 --> 00:06:56,090
It was still there.
I couldn't contain myself.

135
00:06:56,091 --> 00:06:56,924
I was so excited.
It was the most important moment of my 

136
00:06:59,211 --> 00:07:01,850
working life.
Nothing I ever done,

137
00:07:02,030 --> 00:07:04,430
nothing I ever do again,
we'll mean as much.

138
00:07:04,580 --> 00:07:09,020
So this moment of breakthrough,
how do we teach neural networks?

139
00:07:09,021 --> 00:07:12,170
How do we learn from data to achieve 
this level of breakthrough?

140
00:07:12,290 --> 00:07:17,270
That's the open question that I want you
to sort of walk away from this part a of

141
00:07:17,271 --> 00:07:22,250
the lecture thinking about what is the 
future of agents that think,

142
00:07:22,370 --> 00:07:23,203
and Alexandra,
we'll talk about the new future 

143
00:07:24,471 --> 00:07:25,304
challenges next,
but what can we use deep reinforcement 

144
00:07:28,911 --> 00:07:29,744
learning to extend past the memorization
pass pack pattern recognition into 

145
00:07:33,080 --> 00:07:35,870
something like reasoning and achieving 
this breakthrough moment.

146
00:07:36,260 --> 00:07:37,093
And at the very least something a 
brilliantly in 1995 after Andrew Weil's 

147
00:07:42,590 --> 00:07:43,423
homer Simpson and,
and those are fans of the simpsons 

148
00:07:45,980 --> 00:07:48,740
actually proved them wrong.
This is very interesting.

149
00:07:48,950 --> 00:07:49,783
Uh,
so he found an example where it does 

150
00:07:50,751 --> 00:07:52,880
hold true the Pharmas Theorem,
uh,

151
00:07:52,881 --> 00:07:55,220
to a certain number of digits after the 
period.

152
00:07:56,540 --> 00:07:58,370
Okay.
And then finally,

153
00:07:58,400 --> 00:07:59,233
aggregating this knowledge into action.
That's what deeper enforcement learning 

154
00:08:01,611 --> 00:08:02,444
is about.
Extracting patterns from raw data and 

155
00:08:05,600 --> 00:08:06,433
then finally being able to estimate the 
state of the world around the agent in 

156
00:08:12,201 --> 00:08:15,290
order to make successful action that 
completes a certain goal.

157
00:08:17,900 --> 00:08:18,531
Uh,
so,

158
00:08:18,531 --> 00:08:19,364
and I will talk about the difference 
between agents that are learning from 

159
00:08:22,971 --> 00:08:23,804
data and agents that are currently 
successfully being able to operate in 

160
00:08:26,271 --> 00:08:27,470
this world.
Example,

161
00:08:27,480 --> 00:08:28,313
the agents here from Boston dynamics I 
ones that don't use any deeper 

162
00:08:31,071 --> 00:08:33,020
enforcement learning,
they don't use any,

163
00:08:33,080 --> 00:08:35,510
they don't learn from data.
This is the open gap,

164
00:08:35,511 --> 00:08:36,344
the challenge that we have to solve.
How do we use reinforcement learning 

165
00:08:38,991 --> 00:08:40,880
methods,
build robots,

166
00:08:40,881 --> 00:08:44,000
agents that act in the real world,
that learn from that world.

167
00:08:44,440 --> 00:08:46,190
Uh,
except for the perception task.

168
00:08:46,460 --> 00:08:47,293
So in this stack,
you can think from the environment to 

169
00:08:49,311 --> 00:08:54,311
the effectors that promise the beautiful
power of deep learning is taking the raw

170
00:08:54,870 --> 00:08:57,750
sensory data and being able to,
in an automated way.

171
00:08:57,751 --> 00:09:02,730
Do feature learning to extract arbitrary
high order representations on that data.

172
00:09:02,820 --> 00:09:06,990
So it makes sense of the patterns in 
order to be able to learn in supervise,

173
00:09:07,770 --> 00:09:10,350
learning to learn the mapping of those 
patterns.

174
00:09:10,440 --> 00:09:14,760
Arbitrarily high order representations 
on those patterns to uh,

175
00:09:15,000 --> 00:09:16,020
to,
uh,

176
00:09:16,080 --> 00:09:16,913
to,
to extract actionable useful knowledge 

177
00:09:19,320 --> 00:09:20,153
that's in the red box.
So the promise of deep reinforcement 

178
00:09:22,441 --> 00:09:24,510
learning why it's so exciting for

179
00:09:25,270 --> 00:09:25,750
okay.

180
00:09:25,750 --> 00:09:27,500
Uh,
artificial intelligence community,

181
00:09:27,710 --> 00:09:28,543
why it captivates our imagination and 
about the possibility towards achieving 

182
00:09:32,421 --> 00:09:33,254
human level.
General intelligence is that you can 

183
00:09:35,121 --> 00:09:40,121
take not just the end to end extraction 
of a knowledge from raw sensory data.

184
00:09:40,880 --> 00:09:45,880
You can also do end to end from raw 
sensory data,

185
00:09:46,520 --> 00:09:51,520
be able to produce actions to brute 
force,

186
00:09:51,711 --> 00:09:55,520
learn from the raw data,
the uh,

187
00:09:55,580 --> 00:09:57,370
the semantic,
uh,

188
00:09:57,770 --> 00:09:58,603
context,
the meaning of the world around you in 

189
00:10:00,291 --> 00:10:04,220
order to successfully act in that world 
and to end just like we humans do.

190
00:10:04,221 --> 00:10:05,210
That's the promise.

191
00:10:06,690 --> 00:10:07,060
Okay.

192
00:10:07,060 --> 00:10:08,070
But,
uh,

193
00:10:08,140 --> 00:10:10,870
we're in the very early stages of,
um,

194
00:10:11,410 --> 00:10:12,760
of achieving that promise.

195
00:10:13,750 --> 00:10:14,340
Yeah.

196
00:10:14,340 --> 00:10:17,090
So,
uh,

197
00:10:17,100 --> 00:10:19,350
and successful presentation must include
cats.

198
00:10:19,351 --> 00:10:24,240
So supervised learning and unsupervised 
learning and reinforcement learning sits

199
00:10:24,270 --> 00:10:26,670
in the middle,
was a supervised learning.

200
00:10:26,760 --> 00:10:28,910
Most of the,
uh,

201
00:10:29,040 --> 00:10:29,873
the data has to come from the human.
They're the insights about what is 

202
00:10:33,991 --> 00:10:36,990
inside the data has to come from human 
annotations.

203
00:10:37,110 --> 00:10:37,943
And it's the task of the machine to 
learn how to generalize based on those 

204
00:10:40,501 --> 00:10:44,280
human annotations over future examples 
it hasn't seen before.

205
00:10:44,490 --> 00:10:47,100
And unsupervised learning,
you have no human annotation.

206
00:10:47,280 --> 00:10:50,820
S reinforcement learning is somewhere in
between,

207
00:10:50,821 --> 00:10:54,270
closer on unsupervised learning or the 
annotations from humans.

208
00:10:54,271 --> 00:10:55,980
The information and knowledge from 
humans,

209
00:10:55,981 --> 00:10:58,050
it's very sparse,
extremely sparse.

210
00:10:58,290 --> 00:11:01,020
And so you have to use the temporal 
dynamics,

211
00:11:01,021 --> 00:11:04,140
the fact that in time these,
the,

212
00:11:04,141 --> 00:11:04,974
uh,
the continuity of our world through 

213
00:11:08,431 --> 00:11:09,264
time,
you have to use the sparse little 

214
00:11:11,371 --> 00:11:15,240
rewards you have along the way to extend
that over the entirety of,

215
00:11:15,270 --> 00:11:16,041
uh,
the,

216
00:11:16,041 --> 00:11:18,630
the temporal domain to make some sense 
of the world,

217
00:11:18,631 --> 00:11:20,610
even though the rewards are really 
sparse.

218
00:11:20,820 --> 00:11:22,830
Those are the two cats learning,
uh,

219
00:11:23,280 --> 00:11:25,350
uh,
the Pavlov's cats,

220
00:11:25,351 --> 00:11:27,930
if you will.
Learning that I had to ring the door,

221
00:11:28,500 --> 00:11:31,680
ring the bell in order to get some food.
That's the basic for them,

222
00:11:31,710 --> 00:11:35,400
the reinforcement learning problem.
So from supervised learning,

223
00:11:35,460 --> 00:11:39,510
you can think of those networks as 
memorizers reinforcement learning is you

224
00:11:39,511 --> 00:11:43,440
can think of them of crudely.
So as sort of brute force reasoning,

225
00:11:43,590 --> 00:11:48,590
trying to propagate rewards into 
extending how to make sense of the world

226
00:11:49,141 --> 00:11:52,650
in order to act in it.
The pieces are simple.

227
00:11:52,870 --> 00:11:55,210
There's an environment,
there's an agent,

228
00:11:55,390 --> 00:11:56,223
it takes actions in that agent.
It senses the environment so there's 

229
00:11:58,601 --> 00:12:02,860
always a state of the agent senses.
And then I always,

230
00:12:03,190 --> 00:12:04,023
when taking an action received some kind
of reward or punishment from that world 

231
00:12:08,230 --> 00:12:10,270
so we can model any kind of world in 
this way.

232
00:12:10,730 --> 00:12:14,140
They can model an arcade game breakout 
here,

233
00:12:14,170 --> 00:12:16,690
Atari breakout,
the engine has the capacity to act,

234
00:12:16,691 --> 00:12:17,530
move the paddle.

235
00:12:17,780 --> 00:12:20,890
It has a,
it can influence the future state of the

236
00:12:20,891 --> 00:12:23,830
system by taking those actions and then 
it's uses a award.

237
00:12:23,831 --> 00:12:27,310
There's a goal to this game.
The goal is to maximize teacher award.

238
00:12:27,880 --> 00:12:31,900
They could model the card call balancing
problem where the,

239
00:12:31,901 --> 00:12:34,030
you can control the polling angle,
angle or speed.

240
00:12:34,060 --> 00:12:36,730
You can control the cart position,
the horizontal velocity,

241
00:12:37,030 --> 00:12:40,990
the actions of that is the pushing the 
cart,

242
00:12:41,120 --> 00:12:44,470
applying the force of the cart.
And the task is to balance the poll.

243
00:12:44,980 --> 00:12:45,813
Okay.
And their award is one at each time 

244
00:12:47,051 --> 00:12:48,300
step,
the pull still upright.

245
00:12:48,310 --> 00:12:49,143
That's the goal.
So that's a simple formulation of state 

246
00:12:52,030 --> 00:12:52,863
action reward.
You can play a game of doom with the 

247
00:12:55,421 --> 00:12:56,254
state being the raw pixels of the,
of the game and the actions that are 

248
00:12:59,651 --> 00:13:00,484
moving the uh,
the player around shooting and the 

249
00:13:03,651 --> 00:13:04,484
reward,
a positive one at eliminating an 

250
00:13:07,271 --> 00:13:09,910
opponent and negative when the agent has
eliminated,

251
00:13:10,210 --> 00:13:11,043
it's just a game,
industrial robotics and you kind of 

252
00:13:13,961 --> 00:13:17,170
humanoid robotics when you have to 
control multiple degrees of freedom,

253
00:13:17,171 --> 00:13:19,240
control the robotic arm control of the 
robot.

254
00:13:19,600 --> 00:13:20,433
The state is the raw pixels of the real 
world coming into the sensors of the 

255
00:13:23,591 --> 00:13:24,424
robot.
The actions of the possible actions the 

256
00:13:26,021 --> 00:13:28,630
robot can take.
So manipulating each of its sensors,

257
00:13:28,631 --> 00:13:29,890
actuators,
sorry,

258
00:13:29,891 --> 00:13:30,724
actuators.
And then their reward is positive and 

259
00:13:32,681 --> 00:13:35,080
placing a device successfully negative 
otherwise.

260
00:13:35,081 --> 00:13:37,210
So the task is to pick something up,
put it back down.

261
00:13:38,440 --> 00:13:39,900
Okay.
So and I,

262
00:13:39,910 --> 00:13:40,720
I'd like to,
uh,

263
00:13:40,720 --> 00:13:42,490
sort of,
uh,

264
00:13:43,040 --> 00:13:43,873
continuing this trajectory further and 
further complex systems to think the 

265
00:13:46,841 --> 00:13:51,580
biggest challenge for reinforcement 
learning is a formulating the world that

266
00:13:51,581 --> 00:13:54,870
we need to solve the set of goals in 
such a way that can be,

267
00:13:54,910 --> 00:13:57,970
we can apply these deep reinforcement or
reinforcement learning methods.

268
00:13:58,180 --> 00:14:00,880
So give you an intuition about us 
humans.

269
00:14:01,120 --> 00:14:01,953
Uh,
it's exceptionally difficult to 

270
00:14:03,041 --> 00:14:06,190
formulate the goals of life.
Was this a survival,

271
00:14:06,340 --> 00:14:07,720
homeostasis,
happiness?

272
00:14:07,750 --> 00:14:09,380
Who knows?
Uh,

273
00:14:09,840 --> 00:14:11,890
uh,
citations depending on who you are.

274
00:14:12,190 --> 00:14:13,400
Uh,
so the state,

275
00:14:13,470 --> 00:14:15,130
the state sight hearing,
taste,

276
00:14:15,131 --> 00:14:15,641
smell,
touch,

277
00:14:15,641 --> 00:14:19,030
that's the raw sensory data actions or 
think move.

278
00:14:19,120 --> 00:14:20,110
What else?
I don't know.

279
00:14:20,320 --> 00:14:22,750
And then the reward is just a,
it's open.

280
00:14:22,751 --> 00:14:23,584
All of these questions are open.
So if we want to actually start to 

281
00:14:26,051 --> 00:14:27,730
create more and more intelligent 
systems,

282
00:14:27,850 --> 00:14:31,030
it's hard to formulate what the goals 
are with the state spaces,

283
00:14:31,090 --> 00:14:33,910
with the action spaces.
That's the,

284
00:14:34,210 --> 00:14:35,043
that's if this the,
if you take away anything sort of in a 

285
00:14:37,181 --> 00:14:38,014
practical sense from today,
a form from the deeper enforcement 

286
00:14:40,661 --> 00:14:41,494
learning part here,
a few slides is that there's a fun part 

287
00:14:45,591 --> 00:14:49,180
and a hard part too to all of this work.
So the fun part,

288
00:14:49,181 --> 00:14:50,014
the what this course is about,
I hope it's inspire people about the 

289
00:14:53,211 --> 00:14:54,044
amazing,
interesting fundamental algorithms of 

290
00:14:55,701 --> 00:14:56,534
deep learning as the fun part.
The hard part is the collecting and 

291
00:15:01,851 --> 00:15:05,970
annotating huge amount of representative
data in deep learning and forming higher

292
00:15:06,000 --> 00:15:06,920
representations.

293
00:15:06,980 --> 00:15:11,210
Data does the hard work.
So data is everything.

294
00:15:11,270 --> 00:15:14,210
Once you have good algorithms,
data is everything in deep reinforcement

295
00:15:14,211 --> 00:15:18,510
learning the heart.
The fun part again is these algorithms,

296
00:15:18,530 --> 00:15:21,320
we'll talk about them today,
a little overview them,

297
00:15:21,680 --> 00:15:26,540
but the hard part is defining the world,
the action space and the reward space.

298
00:15:26,790 --> 00:15:27,623
It's just the defining,
formalizing the problem is actually 

299
00:15:30,981 --> 00:15:34,490
exceptionally difficult when you start 
to try to create,

300
00:15:34,670 --> 00:15:37,430
uh,
an agent that operates in the real world

301
00:15:37,431 --> 00:15:38,264
and actually operate with other human 
beings and are actually significantly 

302
00:15:40,911 --> 00:15:41,744
helps in the world.
So this isn't playing an arcade game 

303
00:15:44,450 --> 00:15:46,640
where everything is clean or playing 
chess go.

304
00:15:46,910 --> 00:15:48,830
It's when you're operating in the real 
world,

305
00:15:48,831 --> 00:15:50,900
everything is messy.
How do you formalize that?

306
00:15:50,901 --> 00:15:54,640
That's the hard part.
And then the hardest part is getting out

307
00:15:54,641 --> 00:15:57,590
a lot of meaningful data that 
represents,

308
00:15:57,740 --> 00:16:00,110
that fits into that formalization that 
you formed.

309
00:16:01,970 --> 00:16:03,020
Okay?
In the,

310
00:16:03,021 --> 00:16:03,854
uh,
the mark off decision process that's 

311
00:16:05,181 --> 00:16:07,160
underlying the thinking of reinforcement
learning.

312
00:16:07,161 --> 00:16:08,750
There's a state,
you take an action,

313
00:16:08,751 --> 00:16:11,420
receive her award and then observe the 
next state.

314
00:16:11,480 --> 00:16:12,313
So it's always state action award state.
That's the sample of data you get as an 

315
00:16:15,711 --> 00:16:18,950
agent acting in the world.
State action reward state.

316
00:16:20,030 --> 00:16:23,690
There's a policy where an agent tries to
form a,

317
00:16:23,691 --> 00:16:27,200
uh,
a policy how to act in the world so that

318
00:16:27,201 --> 00:16:28,970
in whatever state it's in,
it,

319
00:16:29,030 --> 00:16:31,200
it has a,
uh,

320
00:16:31,220 --> 00:16:35,660
a preference to a act in a certain way 
in order to optimize their award.

321
00:16:35,661 --> 00:16:39,860
There's a value function that can 
estimate how good a certain action isn't

322
00:16:39,861 --> 00:16:40,694
a certain state.
And there is sometimes a model that the 

323
00:16:43,161 --> 00:16:47,060
agent forms or about the world.
A quick example,

324
00:16:47,090 --> 00:16:47,923
you can have a robot in a room at the 
bottom left starting moving about this 

325
00:16:50,871 --> 00:16:51,170
room.

326
00:16:51,170 --> 00:16:54,860
It's a three by four grid.
It tries to get to the top right because

327
00:16:54,861 --> 00:16:57,800
it's a plus one,
but because this is the casting system,

328
00:16:57,801 --> 00:17:00,020
when it goes,
chooses to go up,

329
00:17:00,080 --> 00:17:02,600
it's sometimes goes left and right 10% 
of the time.

330
00:17:02,900 --> 00:17:05,160
So in this world it has to try to,
uh,

331
00:17:05,240 --> 00:17:07,490
come up with a policy.
So what's a policy?

332
00:17:07,491 --> 00:17:10,670
Is this a solution?
So as starting at the bottom,

333
00:17:10,790 --> 00:17:14,420
arrows show the actions whenever you in 
that state that you would like to take.

334
00:17:14,630 --> 00:17:16,850
So this is,
this is a pretty good solution to get to

335
00:17:16,851 --> 00:17:19,940
the plus one in a deterministic world.
And it's the cast,

336
00:17:19,941 --> 00:17:20,990
the quarreled.
Uh,

337
00:17:21,020 --> 00:17:22,700
when you don't,
when you go up,

338
00:17:22,730 --> 00:17:24,330
you don't always go up.
Uh,

339
00:17:24,350 --> 00:17:25,183
it's not a,
a optimal policy because you have to 

340
00:17:27,591 --> 00:17:28,424
have an optimum policy,
has to have an answer for every single 

341
00:17:31,011 --> 00:17:32,030
state you might be in.

342
00:17:32,080 --> 00:17:32,913
So this,
the outcome policy would look something 

343
00:17:34,191 --> 00:17:37,640
like this.
Now that's for when the,

344
00:17:37,700 --> 00:17:38,880
uh,
the costs,

345
00:17:38,900 --> 00:17:43,900
the reward is negative 0.01
for taking a step now feet.

346
00:17:44,840 --> 00:17:47,270
Every time we take a step,
it's really painful.

347
00:17:47,540 --> 00:17:49,740
It's a,
you get a negative to reward.

348
00:17:49,980 --> 00:17:51,570
And so there's a,
the,

349
00:17:51,630 --> 00:17:54,510
the optimal policy changes there is no 
matter what,

350
00:17:54,511 --> 00:17:56,670
no matter this tech cast,
the city of the system,

351
00:17:56,910 --> 00:17:57,743
the randomness,
you want to get to the end as fast as 

352
00:18:00,181 --> 00:18:02,220
possible.
Even if you go to the negative states,

353
00:18:02,490 --> 00:18:05,460
you just want to get to the plus one as 
quickly as possible.

354
00:18:05,820 --> 00:18:06,653
And then,
so the reward structure changes the 

355
00:18:08,731 --> 00:18:11,310
optimum policy.
If you make the reward negative 0.1,

356
00:18:11,490 --> 00:18:13,980
then there's some more incentive to 
explore.

357
00:18:14,100 --> 00:18:18,660
And as we increase the reward or 
decrease the punishment of taking a step

358
00:18:19,080 --> 00:18:23,370
more and more exploration is encouraged 
until we get to the kind of,

359
00:18:23,371 --> 00:18:24,540
uh,
uh,

360
00:18:24,600 --> 00:18:26,490
what I think of as college,
which is a,

361
00:18:26,491 --> 00:18:30,030
you encourage exploration by having a 
positive word to moving around.

362
00:18:30,170 --> 00:18:33,660
And so you never want to get to the end,
you just kinda walk around the world,

363
00:18:33,930 --> 00:18:35,700
uh,
without ever reaching the end.

364
00:18:36,570 --> 00:18:37,890
Okay.
So there's a,

365
00:18:38,040 --> 00:18:41,340
the main goal is to really optimize 
reward in this world.

366
00:18:41,400 --> 00:18:42,233
And uh,
because the reward is collected over 

367
00:18:43,561 --> 00:18:46,410
time,
you want to have some estimate of future

368
00:18:46,411 --> 00:18:50,220
award and because you don't have a 
perfect estimate of the future,

369
00:18:50,370 --> 00:18:52,500
you have to discount that reward over 
time.

370
00:18:53,430 --> 00:18:57,180
So the goal is to maximize the 
discounted reward over time.

371
00:18:57,840 --> 00:18:59,880
And in cue learning is,
uh,

372
00:18:59,940 --> 00:19:02,100
an approach that,
um,

373
00:19:02,660 --> 00:19:03,391
uh,
that,

374
00:19:03,391 --> 00:19:04,470
uh,
I'd like to talk to,

375
00:19:04,510 --> 00:19:06,720
to focus in on today.
The,

376
00:19:06,721 --> 00:19:10,170
there's a state action value cue.
It's a cue function.

377
00:19:10,171 --> 00:19:12,960
It takes an estate in action and it 
tells you the value of that state.

378
00:19:13,140 --> 00:19:15,520
It's off policy because,
uh,

379
00:19:15,570 --> 00:19:19,080
we can learn this function without 
forming a,

380
00:19:19,081 --> 00:19:19,914
an optimal,
without keeping an optimal policy and 

381
00:19:22,501 --> 00:19:24,150
estimate of an optimal policy with us.

382
00:19:24,540 --> 00:19:28,470
And what it turns out with the equation 
at the bottom with a bell mini equation,

383
00:19:28,560 --> 00:19:30,560
you can estimate,
uh,

384
00:19:30,630 --> 00:19:31,463
the,
you can update your estimate of the q 

385
00:19:33,511 --> 00:19:38,100
function in such a way that over time it
converges to an optimal policy.

386
00:19:38,490 --> 00:19:40,770
And the update is simple.
You have an estimate,

387
00:19:40,920 --> 00:19:43,270
you start knowing nothing.
The A,

388
00:19:43,280 --> 00:19:46,230
there's an old s,
this estimate of an old state q,

389
00:19:46,240 --> 00:19:48,420
s,
a t and then you take an action,

390
00:19:48,540 --> 00:19:49,373
you collect her award and you update 
your estimate based on the award he 

391
00:19:52,021 --> 00:19:52,854
received and the difference between what
you expected and what you actually 

392
00:19:55,351 --> 00:19:57,180
received.
And that's the update you.

393
00:19:57,181 --> 00:19:58,014
So you walk around this world exploring 
until he formed better and better 

394
00:20:00,631 --> 00:20:03,900
understanding of what is a good action 
to take in each individual state.

395
00:20:04,230 --> 00:20:05,063
And there's always as in life and in 
reinforcement learning in any agents 

396
00:20:08,131 --> 00:20:09,050
that act,
you've,

397
00:20:09,120 --> 00:20:11,610
there's a learning stage where you have 
to explore,

398
00:20:11,611 --> 00:20:14,460
exploring pays off when you know very 
little.

399
00:20:14,610 --> 00:20:15,443
The more and more you learn,
the less and less valuable it is to 

400
00:20:18,271 --> 00:20:19,710
explore.
And you weren't to exploit.

401
00:20:19,860 --> 00:20:20,693
You want to take greedy actions and 
that's always the balance you start 

402
00:20:23,311 --> 00:20:24,144
exploring at first.
But eventually you want to make some 

403
00:20:25,981 --> 00:20:26,814
money.
You whatever the,

404
00:20:27,160 --> 00:20:27,881
the,
the,

405
00:20:27,881 --> 00:20:30,360
the metric of successes and then you 
wanted to focus on,

406
00:20:30,370 --> 00:20:33,570
on the policy that you've converge 
towards that is pretty good,

407
00:20:33,810 --> 00:20:36,600
uh,
near optimal policy in order to act in a

408
00:20:36,601 --> 00:20:37,434
greedy way.
As you move around with this a bellman 

409
00:20:41,511 --> 00:20:42,660
equation,
move around the world,

410
00:20:42,661 --> 00:20:44,910
taking different states,
different actions.

411
00:20:45,030 --> 00:20:47,380
You can update a,
you could think of it as a cute table,

412
00:20:47,540 --> 00:20:50,320
uh,
and you can update the quality of a,

413
00:20:50,321 --> 00:20:52,630
taking a certain action in a certain 
state.

414
00:20:52,690 --> 00:20:54,790
So that's a,
that's a,

415
00:20:55,600 --> 00:21:00,400
that's a picture of a table there in the
world with four states and four actions.

416
00:21:00,490 --> 00:21:02,590
And you can move around using the 
bellmead equation,

417
00:21:02,620 --> 00:21:05,110
updating the value of being in that 
state.

418
00:21:05,320 --> 00:21:06,153
The problem is when this cute table 
grows exponentially in order to 

419
00:21:09,221 --> 00:21:12,280
represent raw sensory data like we 
humans have,

420
00:21:12,281 --> 00:21:16,510
when taking an envision or if you'd 
taken the raw pixels of an arcade game,

421
00:21:16,840 --> 00:21:19,600
that's the number of pixels that are 
there.

422
00:21:19,660 --> 00:21:24,660
Get is larger than is larger than it can
be stored in memory is largely,

423
00:21:25,691 --> 00:21:27,490
that can be explored.
The simulation,

424
00:21:27,610 --> 00:21:28,443
it's exceptionally large.
And if we know anything about 

425
00:21:30,910 --> 00:21:32,780
exceptionally large,
uh,

426
00:21:33,250 --> 00:21:36,370
high dimensional spaces and learning 
anything about them,

427
00:21:36,371 --> 00:21:38,650
that's what deep neural networks are 
good at.

428
00:21:38,770 --> 00:21:39,603
Forming the approximators,
forming some kind of representation and 

429
00:21:42,400 --> 00:21:44,820
exceptionally high dimensional complex,
uh,

430
00:21:44,950 --> 00:21:45,783
space.
So that's the hope for deeper 

431
00:21:48,041 --> 00:21:50,830
enforcement learning.
As you take these reinforcement learning

432
00:21:50,831 --> 00:21:54,340
ideas where an agent acts in the world 
and learn something about that world.

433
00:21:54,460 --> 00:21:58,810
And we use a neural network as the 
approximator as the thing that the agent

434
00:21:58,811 --> 00:22:00,940
uses in order to approximate the 
quality,

435
00:22:01,180 --> 00:22:02,013
uh,
either approximate the policy or 

436
00:22:03,221 --> 00:22:05,080
approximately the quality of taking 
certain actions,

437
00:22:05,081 --> 00:22:05,914
certain state and,
and therefore making sense of this raw 

438
00:22:08,321 --> 00:22:10,510
information,
forming,

439
00:22:10,630 --> 00:22:11,463
hiring higher order representations of 
the raw sensory information in order to 

440
00:22:15,311 --> 00:22:17,770
then as an output,
take an action.

441
00:22:19,210 --> 00:22:23,920
So the neural network is injected as a 
function approximator into the queue.

442
00:22:24,000 --> 00:22:26,160
It's,
it's the q function is approximated with

443
00:22:26,161 --> 00:22:29,740
a neural network that's dqn that's deep 
cue learning.

444
00:22:29,950 --> 00:22:33,520
So injecting into the cue learning 
framework and neural network.

445
00:22:33,910 --> 00:22:37,390
That's what's been the success with deep
mind with the playing the Atar Games.

446
00:22:37,630 --> 00:22:38,463
Having this neural network takes in the 
raw pixels of the Atari game and 

447
00:22:40,961 --> 00:22:44,350
produces actions of values of each 
individual actions.

448
00:22:44,351 --> 00:22:45,184
And then in a greedy way,
picking the best action and the 

449
00:22:48,641 --> 00:22:49,474
learning,
the loss function for these networks is 

450
00:22:51,900 --> 00:22:52,930
uh,
uh,

451
00:22:53,000 --> 00:22:53,530
two folds.

452
00:22:53,530 --> 00:22:55,360
So you have a,
you have a queue function,

453
00:22:55,370 --> 00:22:57,940
an estimate of taking a certain action.
Certain state,

454
00:22:58,060 --> 00:22:58,893
you'd take that action and then you 
observe how it's the actual reward 

455
00:23:02,441 --> 00:23:04,510
received is different.
So you have a target,

456
00:23:04,570 --> 00:23:09,070
you have a prediction and the loss is 
the squared error between those two.

457
00:23:09,250 --> 00:23:14,250
And dqn has used as the same network 
that traditional dqn the first,

458
00:23:14,270 --> 00:23:16,000
uh,
uh,

459
00:23:16,120 --> 00:23:19,420
yeah.
Dqi uses one network testimate both cues

460
00:23:19,421 --> 00:23:21,100
in that lost function,
uh,

461
00:23:21,270 --> 00:23:22,103
a double Dq,
not dq and uses a separate network for 

462
00:23:26,201 --> 00:23:31,201
each one of those a few tricks.
They're key experience replay.

463
00:23:33,010 --> 00:23:34,420
So,
uh,

464
00:23:35,080 --> 00:23:39,730
tricks in reinforcement learning because
the fact that it works is incredible.

465
00:23:40,030 --> 00:23:40,863
And uh,
as as a fundamental sort of 

466
00:23:42,941 --> 00:23:45,890
philosophical idea,
knowing so little and being able to make

467
00:23:45,891 --> 00:23:46,724
sense with a,
from such a high dimensional space is 

468
00:23:49,401 --> 00:23:50,234
amazing.
But actually these ideas have been 

469
00:23:51,921 --> 00:23:56,921
around for quite a long time and a few 
key trex is what made them really work.

470
00:23:57,470 --> 00:23:58,303
So in the first,
I think the two things for dqn is 

471
00:24:00,621 --> 00:24:04,520
experienced replay is instead of letting
the agent,

472
00:24:04,730 --> 00:24:06,290
uh,
learn as it,

473
00:24:06,410 --> 00:24:07,243
as it acts in the world,
agent is acting in the world and 

474
00:24:10,251 --> 00:24:11,910
collecting,
uh,

475
00:24:12,500 --> 00:24:15,350
experiences that can be replayed through
the learning.

476
00:24:15,351 --> 00:24:17,990
So the learning process jumps around 
through memory,

477
00:24:18,230 --> 00:24:21,470
through the experiences and instead,
so it doesn't,

478
00:24:21,560 --> 00:24:22,393
um,
it doesn't learn on the local evolution 

479
00:24:25,611 --> 00:24:26,444
of a particular stimulation.
Instead learn of the entirety of its 

480
00:24:28,791 --> 00:24:31,760
experiences.
Then the fix target network,

481
00:24:31,761 --> 00:24:35,600
as I mentioned with a double Gq n the 
fact that the loss function,

482
00:24:36,610 --> 00:24:40,180
uh,
a includes if you notice sort of to,

483
00:24:40,410 --> 00:24:41,570
uh,
uh,

484
00:24:41,630 --> 00:24:43,730
to forward passes through the neural 
network.

485
00:24:43,910 --> 00:24:47,090
And so because when you know,
very little in the beginning,

486
00:24:47,240 --> 00:24:49,850
it's a very unstable system and bias 
can,

487
00:24:49,920 --> 00:24:50,753
Eh,
I can have a significant negative 

488
00:24:51,441 --> 00:24:52,274
effect.
So there's some benefit in for the 

489
00:24:54,771 --> 00:24:55,604
target.

490
00:24:55,910 --> 00:24:59,630
The forward pass the neural network 
takes for the target function to,

491
00:24:59,631 --> 00:25:02,690
uh,
to be fixed and only be updated.

492
00:25:02,691 --> 00:25:03,524
Then neural network there to be only 
updated every a thousand a hundred 

493
00:25:06,830 --> 00:25:10,490
steps.
And there's a few other tricks,

494
00:25:10,491 --> 00:25:12,190
the slides that are available online.
I will,

495
00:25:12,350 --> 00:25:14,600
there's a few interesting bits,
uh,

496
00:25:14,810 --> 00:25:17,300
throughout these slides.
Please check them out.

497
00:25:17,301 --> 00:25:20,990
There's a lot of interesting results 
here on the slide.

498
00:25:21,750 --> 00:25:22,583
Uh,
showing the benefit that you get from 

499
00:25:24,861 --> 00:25:26,840
these tricks.
So replay experience,

500
00:25:26,841 --> 00:25:29,720
replay and fixed target network are the 
biggest.

501
00:25:29,721 --> 00:25:30,554
That's the magic.
That's the thing that made it work for 

502
00:25:32,121 --> 00:25:33,110
the Atar Games.

503
00:25:35,980 --> 00:25:36,780
Okay.

504
00:25:36,780 --> 00:25:40,790
And the result was achieving deep mine,
achieving super,

505
00:25:40,850 --> 00:25:41,683
uh,
above human level performance on these 

506
00:25:43,441 --> 00:25:44,274
Atari Games.
Now what's been very successful he use 

507
00:25:48,241 --> 00:25:52,560
now with Alphago and um,
the other more complex systems is policy

508
00:25:52,561 --> 00:25:55,740
gradients,
which is a slight variation on this idea

509
00:25:55,741 --> 00:25:58,560
of applying neural networks in this 
deeper enforcement learning space.

510
00:25:58,800 --> 00:26:03,510
So dqn is q learning neural network in 
the cue learning framework.

511
00:26:03,660 --> 00:26:04,493
It's off policy.
So it's approximating q and infer the 

512
00:26:07,831 --> 00:26:10,290
optimum policy,
a policy gradients.

513
00:26:10,291 --> 00:26:14,570
PG is on policy is directly optimizing 
the policy space.

514
00:26:14,610 --> 00:26:15,443
Then you'll network as estimating the 
probability of taking a certain action 

515
00:26:19,380 --> 00:26:20,730
and the learning.
And there's a great,

516
00:26:20,740 --> 00:26:21,573
if you want the details of this from 
Andrea Copath is a great post 

517
00:26:23,821 --> 00:26:24,860
explaining,
uh,

518
00:26:24,970 --> 00:26:25,803
illustrate it in an illustrative way.
Deeper enforcement learning by looking 

519
00:26:28,831 --> 00:26:29,664
at Pong Ping Pong.
So the training process there is you 

520
00:26:33,451 --> 00:26:34,284
look at the evolution of the different 
games and then reinforce also knows 

521
00:26:37,260 --> 00:26:38,250
actor critic.

522
00:26:38,280 --> 00:26:42,900
You take the policy gradient that 
increases the probability of good action

523
00:26:42,901 --> 00:26:46,920
and the probability of bad action.
So the policy network is the actor.

524
00:26:46,921 --> 00:26:50,010
So the neural network is the thing that 
takes him the raw pixels,

525
00:26:50,160 --> 00:26:52,890
usually a sequence of frames and 
outputs,

526
00:26:52,891 --> 00:26:54,720
a probability of taking a certain 
action.

527
00:26:55,140 --> 00:26:55,973
So you want to reward actions that have 
eventually led to a winning a higher 

528
00:27:00,751 --> 00:27:04,470
award and you want to punish the,
you earned two degrees.

529
00:27:04,820 --> 00:27:07,080
You don't have negative gradient for 
actions that,

530
00:27:07,110 --> 00:27:10,350
um,
that led to a negative reward.

531
00:27:11,190 --> 00:27:14,760
So the reward there is the critic.
The policy network is the actor,

532
00:27:16,060 --> 00:27:16,893
uh,
the pros and cons of Dqn of Policy 

533
00:27:20,510 --> 00:27:22,620
Grazers,
dqn most folks now,

534
00:27:22,621 --> 00:27:25,530
the success comes from policy gradients,
active critic methods,

535
00:27:25,560 --> 00:27:28,080
different variations of it.
The pros are,

536
00:27:28,081 --> 00:27:32,250
it's able to deal with more complex q 
function is faster convergence and most,

537
00:27:32,320 --> 00:27:33,480
uh,
in most cases,

538
00:27:33,810 --> 00:27:34,860
uh,
given,

539
00:27:35,190 --> 00:27:36,023
given you have enough data,
that's the big con is it's needs a lot 

540
00:27:39,931 --> 00:27:40,470
of data.

541
00:27:40,470 --> 00:27:42,420
It needs a lot of ability to simulate 
huge,

542
00:27:42,570 --> 00:27:44,460
huge amounts of evolutions of the 
system.

543
00:27:44,970 --> 00:27:46,260
And,
uh,

544
00:27:46,290 --> 00:27:47,123
because the model probabilities,
the posse grainy smile at the 

545
00:27:49,501 --> 00:27:51,630
probabilities of action,
they're able to,

546
00:27:51,870 --> 00:27:55,080
uh,
learn stochastic policies and Dq cannot.

547
00:27:56,580 --> 00:27:57,413
And that's where the game of go has 
received a lot of success with the 

548
00:28:01,321 --> 00:28:06,180
application of the policy gradients.
Where at first alpha go in 2016,

549
00:28:06,181 --> 00:28:08,190
beat the top humans in the world,
uh,

550
00:28:08,250 --> 00:28:10,680
at the game of go by training on an 
expert,

551
00:28:10,890 --> 00:28:13,830
a games.
So in a supervised way,

552
00:28:13,950 --> 00:28:14,783
starting from training on those human 
expert positions and Alphago zero and 

553
00:28:19,431 --> 00:28:24,390
2017 achieving a monumental feat.
And in artificial intelligence,

554
00:28:24,391 --> 00:28:25,730
one of the greatest,
in my opinion,

555
00:28:25,740 --> 00:28:30,740
the last decade of training on no human 
expert play playing against a itself,

556
00:28:32,520 --> 00:28:33,353
being able to beat under the initial 
Alphago and beat the best human players 

557
00:28:36,511 --> 00:28:37,170
in the world.

558
00:28:37,170 --> 00:28:38,003
This is an incredible achievement for 
reinforcement learning that captivated 

559
00:28:40,621 --> 00:28:43,350
our imagination of what's possible with 
these approaches.

560
00:28:43,920 --> 00:28:47,520
But the actual approach,
and you can look through the slides as a

561
00:28:47,521 --> 00:28:48,354
few,
uh,

562
00:28:48,360 --> 00:28:50,220
interesting tidbits in there,
but,

563
00:28:50,250 --> 00:28:51,180
uh,
uh,

564
00:28:51,181 --> 00:28:52,014
the,
it's using the same kind of methodology 

565
00:28:53,161 --> 00:28:55,600
that a lot of game engines have been 
using and uh,

566
00:28:55,620 --> 00:28:57,320
certainly goplayers,
uh,

567
00:28:57,330 --> 00:28:58,163
for the Monte Carlo tree search.
So you have this incredibly huge search 

568
00:29:01,131 --> 00:29:01,964
space and you have to figure out like 
which parts of it do I search in order 

569
00:29:06,241 --> 00:29:08,250
to find the good position as the good 
actions.

570
00:29:08,550 --> 00:29:09,383
And so there you'll networks I used to 
do the estimation of what are the good 

571
00:29:12,541 --> 00:29:14,340
actions,
what are the good positions.

572
00:29:15,630 --> 00:29:17,400
Again,
the slides have the fun,

573
00:29:17,640 --> 00:29:19,360
the fun details.
Um,

574
00:29:19,950 --> 00:29:22,500
for those of you who are gambling 
addicts,

575
00:29:22,540 --> 00:29:23,910
uh,
importantly,

576
00:29:23,911 --> 00:29:24,691
so,
uh,

577
00:29:24,691 --> 00:29:25,290
the,
uh,

578
00:29:25,290 --> 00:29:28,680
the stochastic element of poker,
at least heads up poker.

579
00:29:28,680 --> 00:29:29,513
So one on one has been for the first 
time ever this same exact approach had 

580
00:29:34,701 --> 00:29:35,534
been used in deep stack and other agents
to beat the top professional poker 

581
00:29:40,021 --> 00:29:44,800
players in the world in 2017.
The open challenge for the community,

582
00:29:44,830 --> 00:29:46,750
for maybe people in this room,
uh,

583
00:29:46,751 --> 00:29:47,584
in 2018 is to apply these methods to win
in a much more complex environment of 

584
00:29:51,700 --> 00:29:53,620
Terman plate when there's multiple 
players.

585
00:29:53,800 --> 00:29:56,140
So heads up poker is a much easier 
problem.

586
00:29:56,530 --> 00:29:57,363
Uh,
the human element is much more 

587
00:29:58,810 --> 00:30:02,290
formalized bubble and clear there when 
there's multiple players,

588
00:30:02,291 --> 00:30:04,330
it's exceptionally difficult and 
fascinating.

589
00:30:04,680 --> 00:30:05,513
A fascinating problem that's perhaps 
more representative of agents that have 

590
00:30:09,521 --> 00:30:12,190
to act in the real world.
So now the Downer part,

591
00:30:13,150 --> 00:30:13,983
a lot of the successful agents that we 
work with here at Mit and bill did the 

592
00:30:17,651 --> 00:30:18,940
robots that act in the,
in,

593
00:30:18,950 --> 00:30:23,440
in the real world are using almost no 
deep reinforcement learning.

594
00:30:23,680 --> 00:30:24,513
So deeper enforcement learning is 
successfully applied in context of 

595
00:30:28,211 --> 00:30:29,044
simulation in context of game playing.
But in a successfully controlling 

596
00:30:33,641 --> 00:30:34,474
humanoid robotics or human robots,
a humanoid robots or autonomous 

597
00:30:38,861 --> 00:30:39,940
vehicles,
for example,

598
00:30:40,500 --> 00:30:43,810
the deep learning methods they use 
primarily for the perception task.

599
00:30:43,930 --> 00:30:47,410
They're exceptionally good at making 
sense of the environment and uh,

600
00:30:47,530 --> 00:30:50,800
extracting useful knowledge from it.
But in terms of forming actions,

601
00:30:50,980 --> 00:30:53,470
that's usually done through optimization
based methods.

602
00:30:55,120 --> 00:30:59,890
Finally a quick comment on the 
unexpected local pockets.

603
00:30:59,950 --> 00:31:00,783
That's,
that's at the core of why these methods 

604
00:31:02,750 --> 00:31:04,930
are not used in the real world.
Uh,

605
00:31:04,931 --> 00:31:05,764
here is a game of coast runners where a 
boat is tasked with receiving a lot of 

606
00:31:09,341 --> 00:31:10,174
points.
Traditionally the game is played by 

607
00:31:12,101 --> 00:31:15,160
racing other boats and trying to get to 
the finish as quickly as possible.

608
00:31:15,580 --> 00:31:16,413
And this boat figures out that it 
doesn't need to do that in a brilliant 

609
00:31:19,541 --> 00:31:20,590
breakthrough idea.

610
00:31:20,890 --> 00:31:23,800
Uh,
it can just collect a regenerating green

611
00:31:23,860 --> 00:31:27,010
squares.
That's an unintended consequence,

612
00:31:27,250 --> 00:31:28,083
uh,
that you can extend to other systems 

613
00:31:30,230 --> 00:31:32,560
perhaps including,
you can imagine what,

614
00:31:32,860 --> 00:31:33,860
uh,
how the,

615
00:31:34,120 --> 00:31:34,953
the cat system over time at the bottom 
right can evolve into something 

616
00:31:38,270 --> 00:31:39,103
undesirable and further on in these 
reinforcement learning agents when they 

617
00:31:43,841 --> 00:31:48,700
act in the real world,
the human life is often the human factor

618
00:31:48,910 --> 00:31:49,743
are often injected into the system.
And so oftentimes in their reward 

619
00:31:52,931 --> 00:31:54,700
function,
the objective loss function,

620
00:31:54,790 --> 00:31:58,510
you start injecting concepts of risk and
even human life.

621
00:31:58,690 --> 00:31:59,523
So what does it look like in terms of ai
safety when the agent has to make 

622
00:32:02,321 --> 00:32:03,154
decision based on a loss function that 
includes an estimate or risk of killing 

623
00:32:07,841 --> 00:32:08,674
another human being?
This is a very important thing to think 

624
00:32:10,691 --> 00:32:11,550
about,
uh,

625
00:32:11,551 --> 00:32:16,551
about machines that learn from data.
And finally a to play around.

626
00:32:16,721 --> 00:32:17,290
There's,
there's,

627
00:32:17,290 --> 00:32:18,123
uh,
there's a lot of ways to explore and 

628
00:32:19,991 --> 00:32:24,991
learn about deeper enforcement learning.
And we have at the url below there,

629
00:32:25,790 --> 00:32:27,670
uh,
a deep traffic simulation game.

630
00:32:27,671 --> 00:32:29,800
It's a competition where you get to,
uh,

631
00:32:30,040 --> 00:32:32,050
uh,
build a car that speeds,

632
00:32:32,130 --> 00:32:34,210
uh,
that tries to achieve the as close to 80

633
00:32:34,211 --> 00:32:37,480
miles per hour as possible.
And I encourage you to participate,

634
00:32:38,760 --> 00:32:41,380
uh,
participate and try to win.

635
00:32:41,410 --> 00:32:42,243
Get it a leaderboard,
not enough mit folks there at the top 

636
00:32:44,051 --> 00:32:45,730
10.
So with that,

637
00:32:45,760 --> 00:32:47,230
thank you very much.
Thank you for having me.

