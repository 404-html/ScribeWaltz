1
00:00:00,390 --> 00:00:03,360
Humans are really good at pattern 
recognition.

2
00:00:04,020 --> 00:00:06,480
We use it in nearly every aspect of our 
lives.

3
00:00:06,510 --> 00:00:10,120
Recognizing people,
knowing a song by the first few bars and

4
00:00:10,121 --> 00:00:10,954
the list is endless and in some cases 
pattern recognition involves taking 

5
00:00:15,011 --> 00:00:15,844
existing empirical evidence to find 
trends and project future think the 

6
00:00:19,811 --> 00:00:20,644
weatherman.
Now the problem for humans is we often 

7
00:00:23,051 --> 00:00:25,900
get it wrong.
We overemphasize some facts,

8
00:00:26,110 --> 00:00:28,690
make big judgments based on too small of
data sets,

9
00:00:29,470 --> 00:00:30,303
but technology is changing.
All of this just as technology changed 

10
00:00:33,011 --> 00:00:33,844
your ability to make an informed 
decision on which type of barriers best 

11
00:00:36,710 --> 00:00:37,543
black bear.
It's also going to augment and improve 

12
00:00:39,731 --> 00:00:42,970
our abilities to make statistically 
sound decisions based on the patterns of

13
00:00:42,971 --> 00:00:46,180
millions of other humans.
Some of these augmentations have already

14
00:00:46,181 --> 00:00:49,810
happened and can be examined.
This is how Netflix can create a list of

15
00:00:49,811 --> 00:00:53,500
what you want to watch next based on 
your viewing history and millions of the

16
00:00:53,501 --> 00:00:54,450
other people's history.

17
00:00:55,030 --> 00:00:59,200
These augmentations also help Amazon 
know what you want to buy next and these

18
00:00:59,201 --> 00:01:02,260
are all good and pretty benign forms of 
statistical profiling,

19
00:01:03,100 --> 00:01:06,070
but there are some implications to this 
usage of big data.

20
00:01:06,130 --> 00:01:06,963
They get uncomfortable.
And if you're prone to obsessing over 

21
00:01:10,421 --> 00:01:12,850
moral dilemmas,
you might want to skip this one.

22
00:01:13,180 --> 00:01:15,460
So let's dive into this.
Here's a hypothetical.

23
00:01:15,640 --> 00:01:16,473
Let's say in the US,
the number one priority is to stop the 

24
00:01:18,971 --> 00:01:20,880
listening and proliferation of 
nickelback.

25
00:01:21,730 --> 00:01:24,310
And after years of nickelback fans 
slipping through the cracks,

26
00:01:24,311 --> 00:01:27,370
they notice something.
It seems that 90% of all known offenders

27
00:01:27,371 --> 00:01:30,790
had green hair.
Now us humans doing as we do,

28
00:01:30,791 --> 00:01:31,630
we say,
hey,

29
00:01:31,900 --> 00:01:32,733
we only have so many policing resources.
Let's focus it all on these nickelback 

30
00:01:35,200 --> 00:01:38,200
loving green haired people.
Now this might be more effective,

31
00:01:38,201 --> 00:01:39,034
but the question is,
should justice break its commitment to 

32
00:01:40,511 --> 00:01:42,400
impartiality to be more effective?

33
00:01:42,550 --> 00:01:44,560
Because out of a million green haired 
people search,

34
00:01:44,590 --> 00:01:45,423
maybe only one has nickelback CD.
So even though the likelihood of a 

35
00:01:48,131 --> 00:01:48,964
nickelback fan having green hair is 90% 
the likelihood of someone green hair 

36
00:01:52,040 --> 00:01:52,873
like nickelback is 0.01%
so is it right to accuse someone of 

37
00:01:57,191 --> 00:02:00,070
atrocious music tastes just based on the
trait of green hair?

38
00:02:00,580 --> 00:02:01,420
Well,
most people,

39
00:02:01,450 --> 00:02:02,830
including myself,
would say no,

40
00:02:02,831 --> 00:02:05,550
of course not.
Whether someone likes nickelback on only

41
00:02:05,551 --> 00:02:08,070
one trait creates an astronomical error 
margin.

42
00:02:08,370 --> 00:02:09,203
So it's easy to focus on individual 
rights and justice being blind and all 

43
00:02:11,401 --> 00:02:13,860
that.
But this is the problem.

44
00:02:14,160 --> 00:02:16,770
This error margin is going to get much 
smaller soon.

45
00:02:17,130 --> 00:02:17,963
The digital image of each individual is 
getting clearer and more nuance and 

46
00:02:21,121 --> 00:02:21,954
we're starting to use ai to sift through
big data to find patterns of human 

47
00:02:24,331 --> 00:02:26,550
behavior too large for humans to see on 
their own.

48
00:02:27,390 --> 00:02:31,770
So what happens when the s in Iaa is 
99.9%

49
00:02:31,771 --> 00:02:35,730
sure that you like nickelback based on a
hundred factors including such things as

50
00:02:35,731 --> 00:02:38,400
your spotify history of listening to 
nineties rock bands,

51
00:02:38,710 --> 00:02:39,543
frequent attendance of post grunge 
festivals and that Mega douchebag 

52
00:02:41,920 --> 00:02:42,753
haircut.

53
00:02:42,970 --> 00:02:46,750
So what do we do then?
Suddenly a lot of questions open up,

54
00:02:46,870 --> 00:02:49,840
but it is nonetheless profiling based on
circumstantial evidence.

55
00:02:49,990 --> 00:02:52,240
Are we allowed to get a search warrant 
for his house based on this?

56
00:02:52,570 --> 00:02:53,403
To what extent are we willing to let law
enforcement use modern data to catch 

57
00:02:56,021 --> 00:02:56,854
criminals?
This is a problem that the justice 

58
00:02:58,211 --> 00:03:00,040
system is going to,
to grapple with soon.

59
00:03:00,160 --> 00:03:00,993
And I'm not sure we're prepared to deal 
with the change in thinking that it 

60
00:03:02,681 --> 00:03:03,514
might require,
nor am I sure that there is a correct 

61
00:03:05,381 --> 00:03:08,080
answer.
So let's leave our nickelback scenario.

62
00:03:08,110 --> 00:03:08,943
Sorry,
nickelback fans and back into the real 

63
00:03:10,181 --> 00:03:11,014
world for a second.
What happens when the justice system is 

64
00:03:12,881 --> 00:03:13,714
99.9%
sure that this guy is going to shoot up 

65
00:03:15,221 --> 00:03:16,054
a school?
Next step,

66
00:03:16,630 --> 00:03:17,463
do we lock him up before he's committed 
the crime and how do we weigh the 

67
00:03:19,721 --> 00:03:20,554
individual's right to innocence until 
proven guilty against the collective 

68
00:03:23,351 --> 00:03:24,190
desire for safety?

69
00:03:24,910 --> 00:03:27,910
And if you for one second think this 
isn't a modern problem for us,

70
00:03:28,090 --> 00:03:28,923
considered this,
both the Orlando shooter and London 

71
00:03:31,211 --> 00:03:34,390
terrorists were both profile to be known
risks before they attack.

72
00:03:35,500 --> 00:03:36,333
As our risk assessments become more 
accurate is their moral culpability if 

73
00:03:39,071 --> 00:03:39,904
we don't act on this information,
this uncomfortable dilemma is starting 

74
00:03:42,341 --> 00:03:44,740
to pop up more and more and it's not 
going to go away.

75
00:03:45,580 --> 00:03:46,413
No,
I'm like most of my videos where I make 

76
00:03:47,531 --> 00:03:48,364
some final opinion on the issue.
I really don't know the answer to this 

77
00:03:51,011 --> 00:03:51,844
question,
but what I do know for sure it is 

78
00:03:53,740 --> 00:03:57,700
technology has augmented our ability to 
recognize patterns and digitally profile

79
00:03:57,701 --> 00:04:01,060
people more effectively than ever.
And this might change the justices

80
00:04:01,170 --> 00:04:04,110
some forever.
You know what they say?

81
00:04:04,290 --> 00:04:06,480
If you could do good things for other 
people,

82
00:04:06,481 --> 00:04:09,000
you had a moral obligation to do those 
things.

83
00:04:09,120 --> 00:04:11,460
That's what's at stake here,
not choice.

84
00:04:12,270 --> 00:04:13,210
We sponsored that.

85
00:04:14,780 --> 00:04:16,640
Thank you guys so much for watching that
video.

86
00:04:17,210 --> 00:04:18,043
It was a blast to make and I want to 
hear it from you in the comments below 

87
00:04:20,030 --> 00:04:20,863
what you think about it.
Do you think technology will change the 

88
00:04:22,701 --> 00:04:25,700
justice system and just let me know your
takeaway from the video.

89
00:04:26,120 --> 00:04:29,150
So if you liked that video,
you can subscribe or watch another video

90
00:04:29,151 --> 00:04:31,370
here and I make new videos every 
Wednesday,

91
00:04:31,371 --> 00:04:32,390
so I hope to see you then.

