1
00:00:02,580 --> 00:00:05,370
Let's get started.
So good morning everyone.

2
00:00:05,550 --> 00:00:06,383
My name is Aviva and today we're going 
to learn about how deep learning can be 

3
00:00:10,321 --> 00:00:11,154
used to build systems capable of 
perceiving images and making decisions 

4
00:00:15,960 --> 00:00:16,793
based on visual information.
Neural networks have really made a 

5
00:00:20,671 --> 00:00:21,504
tremendous impact in this area over the 
past 20 years and I think to really 

6
00:00:25,051 --> 00:00:28,410
appreciate how and why this has been the
case.

7
00:00:28,710 --> 00:00:33,710
It helps to take a step back way back to
540 million years ago.

8
00:00:34,530 --> 00:00:35,363
And the Cambrian explosion where 
biologists trace the evolutionary 

9
00:00:38,881 --> 00:00:39,714
origins of vision.
The reason vision seems so easy for us 

10
00:00:43,261 --> 00:00:44,094
as humans is because we have had 540 
million years of data for evolution to 

11
00:00:49,861 --> 00:00:52,800
train on.
Compare that to bipedal movement,

12
00:00:53,040 --> 00:00:55,800
human language,
and the difference is significant.

13
00:00:56,880 --> 00:01:00,690
Starting in around the 19 sixties,
there was a surge in interest,

14
00:01:01,080 --> 00:01:01,913
um,
in,

15
00:01:02,190 --> 00:01:03,023
in both the neuro basis of vision and in
developing methods to systematically 

16
00:01:07,111 --> 00:01:09,180
characterize visual processing.

17
00:01:09,630 --> 00:01:10,463
And this eventually led to computer 
scientists wondering about how these 

18
00:01:14,251 --> 00:01:18,360
findings from neuroscience could be 
applied to artificial intelligence.

19
00:01:18,930 --> 00:01:22,860
And one of the biggest breakthroughs in 
our understanding of the neural basis of

20
00:01:22,861 --> 00:01:27,330
vision came from to scientists as a 
Harvard Hubel and wiesel,

21
00:01:28,500 --> 00:01:32,550
and they had a pretty simple 
experimental setup where they were,

22
00:01:32,670 --> 00:01:33,503
where they were able to measure visual,
a neural activity in the visual Cortex 

23
00:01:38,790 --> 00:01:39,623
of cats by recording directly from the 
electrical signals from neurons in this 

24
00:01:45,211 --> 00:01:46,044
region of the brain.
They displayed a simple stimulus on a 

25
00:01:48,841 --> 00:01:49,674
screen and then probed the visual Cortex
to see which neurons fired in response 

26
00:01:54,661 --> 00:01:59,010
to that stimulus.
There were a few key takeaways from this

27
00:01:59,011 --> 00:02:02,790
experiment that I'd like you to keep in 
mind as we go through today's lecture.

28
00:02:03,660 --> 00:02:08,280
The first was that they found that there
was an mechanism for spatial invariance.

29
00:02:08,850 --> 00:02:12,330
They could record neural responses to 
particular patterns,

30
00:02:12,960 --> 00:02:13,793
and this was constant regardless of the 
location of those patterns on the 

31
00:02:16,891 --> 00:02:17,724
screen.

32
00:02:18,570 --> 00:02:19,403
The second was that the neurons they 
recorded from had what they called a 

33
00:02:22,411 --> 00:02:23,244
receptive field.
They certain neurons only responded to 

34
00:02:26,611 --> 00:02:30,450
certain regions of the input while 
others responded to other regions.

35
00:02:31,260 --> 00:02:32,093
Finally,
they were able to tease out that there 

36
00:02:33,271 --> 00:02:34,104
was a mapping,
a hierarchy to neuro neuro organization 

37
00:02:36,961 --> 00:02:37,794
in the visual cortex.
There are cells that are responded to 

38
00:02:40,231 --> 00:02:42,960
more simple images such as rods and 
rectangles,

39
00:02:43,290 --> 00:02:44,123
and then downstream layers of neurons.
They use the activations from these 

40
00:02:47,251 --> 00:02:48,084
upstream neurons in their computation,
so cognitive scientists and 

41
00:02:52,711 --> 00:02:53,544
neuroscientists have since built off 
this early work to indeed confirm that 

42
00:02:57,271 --> 00:02:58,104
the visual Cortex is organized into 
layers and this hierarchy of layers 

43
00:03:02,680 --> 00:03:06,610
allows for the recognition of 
increasingly complex features that,

44
00:03:06,611 --> 00:03:07,444
for example,
allow us to immediately recognize the 

45
00:03:09,971 --> 00:03:10,804
face of a friend.
So now that we've gotten a sense at a 

46
00:03:14,501 --> 00:03:18,520
very high level of how our brains 
process visual information,

47
00:03:18,880 --> 00:03:22,240
we can turn our attention to what 
computers see.

48
00:03:22,990 --> 00:03:27,760
How does it computer processing and 
image well to a computer.

49
00:03:27,820 --> 00:03:28,653
Images are just numbers.
So suppose we have a picture of Abraham 

50
00:03:32,650 --> 00:03:35,680
Lincoln.
It's made up of pixels and since this is

51
00:03:35,681 --> 00:03:36,514
a gray scale image,
each of these pixels can be represented 

52
00:03:39,010 --> 00:03:44,010
by just a single number.
So we can represent our image as a two d

53
00:03:44,171 --> 00:03:47,050
array of numbers.
One for each pixel in the image,

54
00:03:47,650 --> 00:03:50,380
and if we were to have an rgb color 
image,

55
00:03:50,410 --> 00:03:51,243
not gray scale,
we can represent that with a three d 

56
00:03:53,501 --> 00:03:57,820
array where we have two d matrices for 
each of our G and b.

57
00:03:59,230 --> 00:04:02,320
Now that we have a way to represent 
images to computers,

58
00:04:02,620 --> 00:04:07,620
we can next think about what types of 
computer vision tasks we can perform and

59
00:04:08,020 --> 00:04:08,853
in machine learning more broadly,
we can think of tasks of regression and 

60
00:04:12,971 --> 00:04:13,804
those of classification in regression.
Our output takes a continuous value and 

61
00:04:18,641 --> 00:04:20,890
in classification,
a single class label.

62
00:04:21,370 --> 00:04:24,010
So let's consider the task of image 
classification.

63
00:04:24,580 --> 00:04:28,210
We want to predict a single label for a 
given input image.

64
00:04:28,720 --> 00:04:29,553
For example,
let's say we have a bunch of images of 

65
00:04:31,001 --> 00:04:35,950
us presidents and we want to build a 
classification pipeline to tell us which

66
00:04:35,951 --> 00:04:36,784
president is an image.
I'll putting the probability that the 

67
00:04:39,671 --> 00:04:44,671
image is of a particular president.
So in order to be able to classify these

68
00:04:45,131 --> 00:04:45,964
images are pipeline needs to be able to 
tell what is unique about a picture of 

69
00:04:49,691 --> 00:04:53,620
Lincoln versus a picture of Washington 
versus a picture of Obama.

70
00:04:54,910 --> 00:04:57,940
And another way to think about this 
problem at a,

71
00:04:58,000 --> 00:04:58,833
at a high level is in terms of the 
features that are characteristic of a 

72
00:05:01,481 --> 00:05:02,314
particular class of images,
classification would then be done by 

73
00:05:05,801 --> 00:05:08,860
detecting the presence of these features
in a given image.

74
00:05:09,730 --> 00:05:10,563
If the,
if the feature is for a particular 

75
00:05:12,431 --> 00:05:14,620
image,
are all present in an image,

76
00:05:15,010 --> 00:05:18,820
we can then predict that class with a 
high probability.

77
00:05:19,840 --> 00:05:22,660
So for our image classification 
pipeline,

78
00:05:22,900 --> 00:05:27,900
our model needs to know first what the 
features are and secondly it needs to be

79
00:05:28,271 --> 00:05:30,610
able to detect those features in an 
image.

80
00:05:31,840 --> 00:05:32,673
So one way we can solve this is to 
leverage our knowledge about a 

81
00:05:35,261 --> 00:05:36,094
particular field and use this to 
manually define the features ourselves 

82
00:05:40,630 --> 00:05:41,463
and classification pipeline would then 
try to detect these mentally defined 

83
00:05:45,191 --> 00:05:46,024
features in images and use the results 
of some detection algorithm for 

84
00:05:49,571 --> 00:05:50,404
classification.
But there's a big problem with this 

85
00:05:53,351 --> 00:05:54,184
approach.
Remember that images are just 3d arrays 

86
00:05:57,561 --> 00:05:58,394
of brightness,
values and image data has a lot of 

87
00:06:01,551 --> 00:06:05,120
variation.
Occlusion variations in illumination,

88
00:06:05,121 --> 00:06:08,090
viewpoint variation,
interclass variation,

89
00:06:08,690 --> 00:06:09,523
and our classification pipeline has to 
be invariant to all these variations 

90
00:06:13,040 --> 00:06:17,240
while still being sensitive to the 
variability between different classes,

91
00:06:18,950 --> 00:06:19,783
even though our pipeline could use 
features that we the human define where 

92
00:06:23,721 --> 00:06:27,920
this manual extraction would break down 
is in the detection task itself,

93
00:06:28,640 --> 00:06:32,030
and this is due to the incredible 
variability that I just mentioned.

94
00:06:32,750 --> 00:06:37,220
The detection of these features would 
actually be really difficult in practice

95
00:06:37,460 --> 00:06:38,293
because your detection algorithm would 
need to withstand each and every one of 

96
00:06:42,381 --> 00:06:46,010
these different variations,
so how can we do better?

97
00:06:46,940 --> 00:06:47,773
We want a way to both extract features 
and detect their presence in images 

98
00:06:51,560 --> 00:06:52,393
automatically in a hierarchical fashion,
and we can use neuro network based 

99
00:06:57,351 --> 00:06:58,184
approaches to do exactly this,
to learn visual features directly from 

100
00:07:01,791 --> 00:07:02,624
image data and to learn a hierarchy of 
these features to construct an internal 

101
00:07:06,561 --> 00:07:09,770
representation of the image.
For example,

102
00:07:09,800 --> 00:07:12,890
if we wanted to be able to classify 
images of faces,

103
00:07:13,370 --> 00:07:17,270
maybe we could just first learn and 
detect low level features like edges and

104
00:07:17,271 --> 00:07:19,700
dark spots,
level features,

105
00:07:19,790 --> 00:07:21,260
eyes,
ears and noses,

106
00:07:21,560 --> 00:07:22,393
and then high level features that 
actually resemble facial structure and 

107
00:07:27,051 --> 00:07:27,884
neural networks will allow us to 
directly learn visual features in this 

108
00:07:31,671 --> 00:07:33,830
manner if we construct them cleverly.

109
00:07:35,270 --> 00:07:36,103
So in yesterday's first lecture,
we learned about fully connected neural 

110
00:07:38,751 --> 00:07:43,400
networks where you can have multiple 
hidden layers and where each neuron in a

111
00:07:43,401 --> 00:07:46,760
given layer is connected to every neuron
in the subsequent layer.

112
00:07:47,420 --> 00:07:48,253
Let's say we wanted to use a fully 
connected neural network like the one 

113
00:07:51,051 --> 00:07:54,980
you see here for image classification.
In this case,

114
00:07:54,981 --> 00:07:58,850
our input image would be transformed 
into a vector of pixel values,

115
00:07:59,150 --> 00:07:59,983
fed into the network,
and each neuron in the hidden layer 

116
00:08:02,331 --> 00:08:05,570
would be connected to all neurons in the
input layer.

117
00:08:06,470 --> 00:08:10,760
I hope you can appreciate that all 
spatial spatial information from our two

118
00:08:10,761 --> 00:08:14,390
d array would be completely lost 
additional weight.

119
00:08:14,480 --> 00:08:16,100
Additionally,
we'd have many,

120
00:08:16,101 --> 00:08:19,310
many parameters because this is a fully 
connected network,

121
00:08:19,610 --> 00:08:24,610
so it's not going to be really feasible 
to implement such network in practice,

122
00:08:26,240 --> 00:08:27,073
so how can we use the spatial structure 
that's inherent in the input to inform 

123
00:08:31,161 --> 00:08:33,290
the architecture of our network?

124
00:08:35,150 --> 00:08:35,983
The key insight in how we can do this is
to connect patches of the input 

125
00:08:41,270 --> 00:08:44,930
represented as a two d array to neurons 
in hidden layers.

126
00:08:45,590 --> 00:08:46,423
This is to say that each neuron in a 
hidden layer only sees a particular 

127
00:08:49,761 --> 00:08:52,280
region of what the input to that layer 
is.

128
00:08:53,060 --> 00:08:55,770
This will not only reduce the number of 
weights in our network,

129
00:08:56,040 --> 00:08:56,873
but also allow us to leverage the fact 
that in an image pixels that are near 

130
00:09:01,111 --> 00:09:01,944
each other are probably somehow related 
to define connections across the whole 

131
00:09:07,021 --> 00:09:07,854
input.
We can apply this same principle by 

132
00:09:10,411 --> 00:09:14,880
sliding the patch window across the 
entirety of the input image.

133
00:09:15,240 --> 00:09:16,073
In this case,
by two units in this way will take into 

134
00:09:19,081 --> 00:09:21,240
account the spatial structure that's 
present,

135
00:09:23,010 --> 00:09:23,843
but remember,
our ultimate task is to learn visual 

136
00:09:25,981 --> 00:09:26,814
features and we can do this by smartly 
waiting the connections between a patch 

137
00:09:32,490 --> 00:09:37,080
of our input to the neuron it's 
connected to in the next hidden layer,

138
00:09:37,290 --> 00:09:38,123
so as to detect particular features 
present in that input and essentially 

139
00:09:43,860 --> 00:09:46,620
what this amounts to is applying the 
filter,

140
00:09:46,670 --> 00:09:49,230
a set of weights to extract local 
features,

141
00:09:51,620 --> 00:09:52,453
and it would be useful for us in our 
classification pipeline to have many 

142
00:09:55,921 --> 00:10:00,480
different features to work with and we 
can do this by using multiple filters,

143
00:10:00,630 --> 00:10:01,463
multiple sets of weights,
and finally we want to be able to share 

144
00:10:05,791 --> 00:10:06,624
the parameters of each filter across all
the connections between the input layer 

145
00:10:10,560 --> 00:10:11,393
and the next layer because the features 
that matter in one part of the input 

146
00:10:15,630 --> 00:10:16,800
should matter elsewhere.

147
00:10:16,830 --> 00:10:17,663
This is the same concept of a spatial 
invariants that I alluded to earlier in 

148
00:10:24,541 --> 00:10:28,500
practice.
This amounts to a patchy operation known

149
00:10:28,501 --> 00:10:29,334
as convolution.
Let's first think about this at a high 

150
00:10:32,071 --> 00:10:34,950
level.
Suppose we have a four by four filter,

151
00:10:35,640 --> 00:10:40,140
which means we have 16 different weights
and we're going to apply the same filter

152
00:10:40,410 --> 00:10:41,243
to four by four patches in the input and
use the result of that operation to 

153
00:10:45,541 --> 00:10:46,374
somehow influence the state of the 
neuron and the next layer that this 

154
00:10:49,531 --> 00:10:50,364
patch is connected to.
Then we're going to shift our filter by 

155
00:10:53,761 --> 00:10:58,761
two pixels as for example,
and grabbed the next patch of the input,

156
00:11:00,450 --> 00:11:05,100
so in this way we can start thinking 
about convolution at a very high level,

157
00:11:06,090 --> 00:11:09,060
but you're probably wondering how does 
this actually work?

158
00:11:09,330 --> 00:11:10,163
What do we mean by features and how does
this convolution operation allow us to 

159
00:11:13,891 --> 00:11:14,724
extract them?

160
00:11:15,750 --> 00:11:19,410
Hopefully we can make this concrete by 
walking through a couple of examples.

161
00:11:20,460 --> 00:11:25,460
Suppose we want to classify x's from a 
set of black and white images of letters

162
00:11:26,010 --> 00:11:30,840
where black is equal to negative one and
why is represented by a value of one.

163
00:11:31,830 --> 00:11:32,663
For classification,
it would clearly not be possible to 

164
00:11:34,951 --> 00:11:38,460
simply compare the two matrices to see 
if they're equal.

165
00:11:39,360 --> 00:11:43,050
We want to be able to classify an x as 
an ex,

166
00:11:43,080 --> 00:11:45,840
even if it's shifted,
shrunk,

167
00:11:45,900 --> 00:11:47,110
rotated,
deform,

168
00:11:47,111 --> 00:11:47,944
transformed in some way.
We want our model to compare the images 

169
00:11:52,591 --> 00:11:53,424
of an x piece by piece and look for 
these important pieces that define an x 

170
00:11:58,961 --> 00:11:59,794
as an x.
those are the features and if our model 

171
00:12:02,531 --> 00:12:03,364
can find rough feature matches in 
roughly the same positions into 

172
00:12:06,371 --> 00:12:07,204
different images,
it can get a lot better at seeing the 

173
00:12:10,270 --> 00:12:13,240
similarity between different examples of
xs.

174
00:12:14,950 --> 00:12:17,620
You can think of each feature as many 
images,

175
00:12:17,621 --> 00:12:18,454
small two dimensional array of values,
and we're going to use filters to pick 

176
00:12:22,061 --> 00:12:22,894
up on the features common to xs in the 
case of an ex filters that can pick up 

177
00:12:28,090 --> 00:12:28,923
on diagonal lines and crossing will 
probably capture all the important 

178
00:12:33,130 --> 00:12:33,963
characteristics of most x's.
So no here that these smaller matrices 

179
00:12:38,530 --> 00:12:39,363
are the filters of weights that we'll 
use in our convolution operation to 

180
00:12:43,211 --> 00:12:46,540
detect the corresponding features in an 
input image.

181
00:12:47,350 --> 00:12:48,183
So now all that's left is to define this
operation that will pick up on when 

182
00:12:52,511 --> 00:12:53,344
these features pop up in our image,
and that operation is convolution 

183
00:12:58,180 --> 00:12:59,013
convolution preserves the spatial 
relationship between pixels by learning 

184
00:13:04,031 --> 00:13:06,820
image features in small squares of the 
input.

185
00:13:08,290 --> 00:13:09,123
To do this,
we perform an element wise 

186
00:13:10,391 --> 00:13:15,391
multiplication between the filter matrix
and a patch of the input image that's of

187
00:13:15,851 --> 00:13:16,684
the same dimensions as the filter.
This results in a three by three matrix 

188
00:13:21,820 --> 00:13:25,360
and the example you see here,
all entries in this matrix are one,

189
00:13:25,780 --> 00:13:26,613
and that's because there's a perfect 
correspondence between our filter and 

190
00:13:30,161 --> 00:13:32,680
the region of the input that we're 
involving it with.

191
00:13:33,550 --> 00:13:34,383
Finally,
we saw them all the entries in this 

192
00:13:35,981 --> 00:13:40,720
Matrix get nine and that's the result of
our convolution operation.

193
00:13:43,430 --> 00:13:44,263
Let's consider one more example.
Suppose now we want to compute the 

194
00:13:47,691 --> 00:13:51,680
convolution of a five by five image and 
a three by three filter.

195
00:13:52,250 --> 00:13:53,083
To do this,
we need to cover the entirety of the 

196
00:13:55,311 --> 00:13:59,420
input image by sliding the filter over 
the image,

197
00:13:59,660 --> 00:14:03,290
performing the same element wise,
multiplication and addition.

198
00:14:04,100 --> 00:14:04,933
Let's see what this looks like.
We'll first start off in the upper left 

199
00:14:08,031 --> 00:14:10,130
corner,
element wise,

200
00:14:10,190 --> 00:14:14,150
multiplied this three by three patch 
with the entries of the filter,

201
00:14:14,240 --> 00:14:15,073
and then add this results in the first 
entry in our output matrix called the 

202
00:14:19,761 --> 00:14:20,594
feature map.

203
00:14:22,630 --> 00:14:23,463
We will next slide the three by three 
filter over by one to grab the next 

204
00:14:27,821 --> 00:14:31,990
patch and repeat the same operation.
I'm in wise multiplication.

205
00:14:32,050 --> 00:14:32,883
Addition.
This results in the second entry and we 

206
00:14:35,681 --> 00:14:40,681
continue in this way until we have 
covered the entirety of our five by five

207
00:14:42,221 --> 00:14:43,054
image and that's it.
The feature map reflects where in the 

208
00:14:47,141 --> 00:14:50,470
input was activated by this filter that 
we applied.

209
00:14:51,980 --> 00:14:55,610
Now that we've gone through the 
mechanism of the convolution operation,

210
00:14:56,930 --> 00:15:01,310
let's see how different filters can be 
used to produce different feature maps.

211
00:15:02,000 --> 00:15:02,833
So on the left you'll see a picture of a
woman's face and next to it the output 

212
00:15:07,400 --> 00:15:08,233
of applying three different 
convolutional filters to that same 

213
00:15:11,211 --> 00:15:12,044
image.
And you can appreciate that by simply 

214
00:15:14,870 --> 00:15:15,703
changing the weights of the filters.
We can detect different features from 

215
00:15:19,071 --> 00:15:19,904
the image.

216
00:15:21,830 --> 00:15:22,663
So I hope you can now see how 
convolution allows us to capitalize on 

217
00:15:26,931 --> 00:15:27,764
the spatial structure inherent to image 
data and use sets of weights to extract 

218
00:15:32,661 --> 00:15:33,494
local features to end to very easily be 
able to detect different types of 

219
00:15:37,131 --> 00:15:40,100
features by simply using different 
filters.

220
00:15:41,180 --> 00:15:42,013
These concepts of spatial structure and 
local feature extraction using 

221
00:15:45,681 --> 00:15:50,660
convolution are at the core of the 
neural networks used for computer vision

222
00:15:50,661 --> 00:15:51,494
tasks,
which are very appropriately named 

223
00:15:54,081 --> 00:15:54,914
convolutional neural networks or CNNS.
So first we'll take a look at a CNN 

224
00:16:00,231 --> 00:16:03,410
architecture that's designed for image 
classification.

225
00:16:04,310 --> 00:16:09,230
Now there are three main operations to a
CNN first convolutions,

226
00:16:09,500 --> 00:16:12,950
which as we saw can be used to generate 
feature maps,

227
00:16:13,760 --> 00:16:14,593
second nonlinearity,
which we learned in the first lecture 

228
00:16:18,410 --> 00:16:21,680
yesterday because image data is highly 
nonlinear,

229
00:16:22,250 --> 00:16:26,180
and finally pooling which is a 
downsampling operation.

230
00:16:27,320 --> 00:16:29,120
In training,
we train our model,

231
00:16:29,180 --> 00:16:31,280
our CNN model,
on a set of images,

232
00:16:31,730 --> 00:16:32,563
and in training we learned the weights 
of the convolutional filters that 

233
00:16:36,621 --> 00:16:40,010
correspond to feature maps in 
convolutional layers.

234
00:16:41,090 --> 00:16:41,923
And in the case of classification,
we can feed the output of these 

235
00:16:45,171 --> 00:16:46,004
convolutional layers into a fully 
connected layer to perform 

236
00:16:48,651 --> 00:16:49,484
classification.
Now we'll go through each of these 

237
00:16:52,041 --> 00:16:56,180
operations to break down a the basic 
architecture of a CNN.

238
00:16:57,350 --> 00:16:58,183
First,
let's consider the convolution 

239
00:16:59,571 --> 00:17:03,020
operation.
As we saw yesterday,

240
00:17:04,580 --> 00:17:08,450
each neuron and the hidden layer will 
compute a weighted sum of its inputs,

241
00:17:08,660 --> 00:17:09,493
apply bias,
and eventually activate with a 

242
00:17:12,000 --> 00:17:12,833
nonlinearity.
What's special in CNN is this idea of 

243
00:17:16,011 --> 00:17:20,360
local connectivity.
Each neuron and hidden layer only sees a

244
00:17:20,361 --> 00:17:25,361
patch of its inputs.
We can now define the actual computation

245
00:17:26,240 --> 00:17:27,073
for a neuron in the hidden layer.
Its inputs are those neurons in the 

246
00:17:30,411 --> 00:17:32,990
patch of the input layer that it's 
connected to.

247
00:17:33,620 --> 00:17:38,180
We apply a matrix of weights are 
convolutional filter four by four.

248
00:17:38,181 --> 00:17:41,480
In this example,
do an element wise multiplication,

249
00:17:41,870 --> 00:17:42,703
add the outputs and apply bias,
so this defines how neurons in 

250
00:17:48,501 --> 00:17:53,501
convolutional are connected,
but within a single convolutional layer,

251
00:17:54,060 --> 00:17:54,893
we can have multiple different filters 
that we are learning to be able to 

252
00:17:58,711 --> 00:18:00,210
extract different features.

253
00:18:00,570 --> 00:18:01,403
And what this means that is that the 
output of a convolutional layer has a 

254
00:18:05,911 --> 00:18:10,080
volume where the height and the width 
are spatial dimensions,

255
00:18:10,170 --> 00:18:15,170
and these spatial dimensions are 
dependent on the dimensions of the input

256
00:18:15,301 --> 00:18:16,134
layer.
The dimensions of our filter and how 

257
00:18:19,411 --> 00:18:22,440
we're sliding our filter over the input,
the stride,

258
00:18:23,640 --> 00:18:24,473
the depth of a this output volume is 
then given by the number of different 

259
00:18:28,051 --> 00:18:28,884
filters we apply in that layer.
We can also think of how neurons in 

260
00:18:33,991 --> 00:18:38,010
convolutional layers are connected in 
terms of the receptive field,

261
00:18:38,580 --> 00:18:43,320
the locations in the original input 
image that a node is connected to.

262
00:18:44,160 --> 00:18:44,993
These parameters defined the spatial 
arrangement of the output of a 

263
00:18:47,791 --> 00:18:52,791
convolutional layer.
The next step is to apply a nonlinearity

264
00:18:53,070 --> 00:18:57,690
to this output volume as was introduced 
in yesterday's lecture.

265
00:18:57,900 --> 00:18:58,733
We do this because data is highly 
nonlinear and in Cnns it's common 

266
00:19:02,521 --> 00:19:03,354
practice to apply nonlinearity after 
every convolution operation and the 

267
00:19:07,621 --> 00:19:08,454
common activation function that's used 
is the value which is a pixel by pixel 

268
00:19:12,691 --> 00:19:17,691
operation that will replace all negative
values falling convolution with a zero.

269
00:19:20,590 --> 00:19:21,423
And the last key operation to a CNN is 
pulling and pulling is used to reduce 

270
00:19:26,081 --> 00:19:27,800
dimension dimension,
yet,

271
00:19:27,960 --> 00:19:29,710
excuse me,
dimensionality,

272
00:19:29,950 --> 00:19:30,783
and to preserve spatial invariance,
a common text technique which you'll 

273
00:19:34,930 --> 00:19:38,800
very often see is max pooling as shown 
in this example,

274
00:19:39,010 --> 00:19:39,843
and it's exactly what it sounds like.
We simply take the maximum value in a 

275
00:19:43,271 --> 00:19:46,090
patch of the input,
in this case,

276
00:19:46,091 --> 00:19:50,740
a two by two patch and that determines 
the output of the pooling operation.

277
00:19:51,400 --> 00:19:52,233
And I encourage you to kind of meditate 
on other ways in which we could perform 

278
00:19:57,131 --> 00:20:02,131
this sort of downsampling operation.
So these are the key operations of a CNN

279
00:20:06,060 --> 00:20:09,960
and we're now ready to put them together
to actually construct our network.

280
00:20:10,530 --> 00:20:11,363
We can layer these operations to learn a
hierarchy of features present in the 

281
00:20:14,911 --> 00:20:15,744
image data.

282
00:20:18,130 --> 00:20:23,130
A CNN bills for image classification can
roughly be broken down into two parts.

283
00:20:23,470 --> 00:20:24,303
The first is the feature learning 
pipeline where we learn features in 

284
00:20:27,281 --> 00:20:31,900
input images through convolution,
through the introduction of nonlinearity

285
00:20:32,080 --> 00:20:33,000
and,
uh,

286
00:20:33,110 --> 00:20:33,943
the pooling operation.
And the second part is how we're 

287
00:20:38,621 --> 00:20:39,454
actually performing the classification,
the convolutional and pooling layers 

288
00:20:43,451 --> 00:20:46,060
output,
high level features of the input data,

289
00:20:46,420 --> 00:20:47,253
and we can these into fully connected 
layers to perform the actual 

290
00:20:50,681 --> 00:20:51,514
classification.
And the output of the fully connected 

291
00:20:54,791 --> 00:20:55,624
layers in practice is a probability 
distribution for an input images 

292
00:21:00,460 --> 00:21:01,293
membership over set of possible classes.
And a common way to do this is using a 

293
00:21:06,641 --> 00:21:07,474
function called soft Max,
where the output represents this 

294
00:21:10,541 --> 00:21:12,760
categorical probability distribution.

295
00:21:15,310 --> 00:21:18,100
Now that we've gone through all the 
components of a CNN,

296
00:21:18,340 --> 00:21:20,680
all that's left is to discuss how to 
train him.

297
00:21:21,400 --> 00:21:22,233
Again,
we'll go back to a CNN for image 

298
00:21:24,041 --> 00:21:24,874
classification.
During training we learned the weights 

299
00:21:27,551 --> 00:21:31,720
of the convolutional filters.
What features the network is detecting,

300
00:21:32,200 --> 00:21:35,620
and in this case will also learn the 
weights of the fully connected layers.

301
00:21:36,310 --> 00:21:40,420
Since the output for classification is a
probability distribution,

302
00:21:40,870 --> 00:21:44,980
we can use cross entropy loss for 
optimization with back prop.

303
00:21:47,350 --> 00:21:51,160
Okay,
so I'd like to take a closer look at CNN

304
00:21:51,190 --> 00:21:56,190
for classification and discuss what is 
arguably the most famous example of of a

305
00:21:57,071 --> 00:21:57,904
CNN,
the the ones trained and tested on the 

306
00:22:03,011 --> 00:22:03,844
image net dataset.
Image net is a massive data set with 

307
00:22:07,270 --> 00:22:12,100
over 14 million images spanning 20,000
different categories.

308
00:22:12,730 --> 00:22:15,850
For example,
there are 1,409

309
00:22:15,880 --> 00:22:19,060
different pictures of bananas in this 
data set alone.

310
00:22:19,870 --> 00:22:20,703
Even better,
bananas are succinctly described as an 

311
00:22:23,890 --> 00:22:28,060
elongated,
crushing shaped yellow fruit with soft,

312
00:22:28,090 --> 00:22:28,923
sweet flesh,
which both gives a pretty good 

313
00:22:31,061 --> 00:22:36,061
descriptive value of what a banana is 
and speaks to its deliciousness.

314
00:22:38,080 --> 00:22:38,913
So the creators of the image net dataset
also have created a set of visual 

315
00:22:42,941 --> 00:22:43,774
recognition challenges on this Dataset.
And what's most famous is the image net 

316
00:22:48,671 --> 00:22:49,504
classification task where users are 
challengers are simply tasked with 

317
00:22:54,970 --> 00:22:59,710
producing a list of the object 
categories present in a given image.

318
00:22:59,820 --> 00:23:02,200
Oh,
across 1000 different categories.

319
00:23:03,730 --> 00:23:07,180
And the results of,
of that neural networks have had on this

320
00:23:07,181 --> 00:23:09,610
classification task are pretty 
remarkable.

321
00:23:10,180 --> 00:23:13,360
Two thousand 12 was the first time a CNN
one.

322
00:23:13,361 --> 00:23:15,990
This challenge with the famous,
uh,

323
00:23:16,010 --> 00:23:16,843
Alex Net cnn,
and since then neural networks have 

324
00:23:19,660 --> 00:23:23,950
dominated the competition and the error 
has kept decreasing,

325
00:23:24,160 --> 00:23:29,160
surpassing human error in 2015 with the 
resonant architecture.

326
00:23:30,640 --> 00:23:31,473
But with improved accuracy,
the number of layers in these networks 

327
00:23:35,531 --> 00:23:40,531
has been increasing quite dramatically.
So there's a trade off here,

328
00:23:40,931 --> 00:23:42,940
right?
Build your network deeper.

329
00:23:43,900 --> 00:23:45,100
How deep can you go?

330
00:23:47,450 --> 00:23:48,283
So,
so far we've talked only about using 

331
00:23:49,761 --> 00:23:53,420
CNNS for image classification,
but in reality,

332
00:23:53,421 --> 00:23:54,254
this idea of using convolutional layers 
to extract features can extend to a 

333
00:23:58,851 --> 00:23:59,684
number of different applications.
When we considered a CNN for 

334
00:24:03,771 --> 00:24:04,604
classification,
we saw that we could think of it in two 

335
00:24:06,321 --> 00:24:09,290
parts,
feature learning and the classification.

336
00:24:11,990 --> 00:24:12,823
What is at the core of the of 
convolutional neural networks is is the 

337
00:24:16,881 --> 00:24:19,400
feature learning pipeline.
After that,

338
00:24:19,401 --> 00:24:24,401
we can really change what follows to 
suit the application that we desire.

339
00:24:25,580 --> 00:24:26,413
For example,
the portion following the convolutional 

340
00:24:28,911 --> 00:24:32,540
layers may look different for different 
image classification domains.

341
00:24:32,990 --> 00:24:33,823
We can also introduce new architectures 
beyond fully connected layers for tasks 

342
00:24:38,121 --> 00:24:40,820
such as segmentation and image 
captioning.

343
00:24:42,710 --> 00:24:43,543
So today I'd like to go through three 
different applications of CNNS beyond 

344
00:24:47,241 --> 00:24:50,720
image classification,
semantic segmentation,

345
00:24:50,750 --> 00:24:51,583
where the task is to assign each pixel 
in an image and object class to produce 

346
00:24:57,051 --> 00:25:00,470
a segmentation of that image object 
detection,

347
00:25:00,560 --> 00:25:01,393
where we are tasked with detecting 
instances of semantic objects in an 

348
00:25:05,571 --> 00:25:06,404
image and image captioning where the 
task is to generate a short description 

349
00:25:10,851 --> 00:25:13,580
of the image that captures it's semantic
content.

350
00:25:15,110 --> 00:25:15,943
So first,
let's talk about semantic segmentation 

351
00:25:17,990 --> 00:25:21,260
with fully convolutional networks or 
fcns.

352
00:25:21,920 --> 00:25:26,120
Here the network again,
takes a image input of arbitrary size,

353
00:25:26,600 --> 00:25:27,433
but instead it has to produce a 
correspondingly sized output where each 

354
00:25:31,521 --> 00:25:32,354
pixel has been assigned a class label,
which we can then visualize as a 

355
00:25:36,501 --> 00:25:41,501
segmentation as we saw before with CNNS 
for image classification,

356
00:25:42,530 --> 00:25:43,363
we first have a series of convolutional 
layers that are downsampling operations 

357
00:25:46,731 --> 00:25:47,564
for feature extraction,
and this results in a hierarchy of 

358
00:25:50,331 --> 00:25:51,164
learned features.
Now the difference is that we have a 

359
00:25:54,831 --> 00:25:55,664
series of upsampling operations to 
increase the resolution of our output 

360
00:26:00,650 --> 00:26:03,350
from the fully from the convolutional 
layers,

361
00:26:05,320 --> 00:26:06,153
and to then combine this output of the 
upsampling operations with those from 

362
00:26:10,791 --> 00:26:13,940
our downsampling path to produce a 
segmentation.

363
00:26:15,410 --> 00:26:16,243
One application of this sort of 
architecture is to the real time 

364
00:26:19,160 --> 00:26:24,020
segmentation of the driving scene here.
The network has this encoder,

365
00:26:24,021 --> 00:26:24,854
decoder architecture for encoding the 
architecture is very similar to what we 

366
00:26:30,081 --> 00:26:31,700
discussed earlier,
earlier,

367
00:26:32,030 --> 00:26:35,390
convolutional layers to learn a 
hierarchy of feature maps,

368
00:26:35,870 --> 00:26:36,703
and then the decoder portion of the 
network actually uses the indices from 

369
00:26:40,670 --> 00:26:44,970
pooling operations to up sample from 
these feature maps.

370
00:26:45,030 --> 00:26:45,863
And output a segmentation,
another way CNNS have been extended and 

371
00:26:51,541 --> 00:26:52,374
applied is for object detection where 
we're trying to learn features that 

372
00:26:56,341 --> 00:27:01,341
characterize particular regions of the 
input and then classify those regions.

373
00:27:01,830 --> 00:27:06,030
The original pipeline for doing this 
called our CNN is pretty straightforward

374
00:27:06,720 --> 00:27:10,710
given an input image.
The algorithm extracts region proposals,

375
00:27:10,711 --> 00:27:11,544
bottom up,
computes features for each of these 

376
00:27:13,861 --> 00:27:18,861
proposals using convolutional layers and
then classifies each region proposal,

377
00:27:20,820 --> 00:27:24,270
but there's a huge downside to this.
So in their.

378
00:27:24,300 --> 00:27:25,133
In their original pipeline,
this group extracted about 2000 

379
00:27:29,191 --> 00:27:30,024
different region proposals,
which meant that they had to run 2000 

380
00:27:33,151 --> 00:27:36,780
CNNS for feature extraction.
So since then there.

381
00:27:37,050 --> 00:27:39,270
And that takes a really,
really long time.

382
00:27:40,370 --> 00:27:43,560
So since then there have been extensions
of this basic idea,

383
00:27:44,040 --> 00:27:44,873
one being to first run the CNN on the 
input image to first extract features 

384
00:27:49,590 --> 00:27:52,980
and then get region proposals from the 
feature maps.

385
00:27:54,720 --> 00:27:57,000
Finally,
let's consider image captioning.

386
00:27:57,210 --> 00:28:02,210
So suppose we're given an image of a cat
riding the skateboard in classification,

387
00:28:03,121 --> 00:28:06,810
our task is to output a class label for 
this image cap,

388
00:28:07,470 --> 00:28:10,740
and as we've probably hammered home by 
now,

389
00:28:11,040 --> 00:28:15,180
this is done by feeding our input image 
through a set of convolutional layers,

390
00:28:15,510 --> 00:28:16,343
extracting features,
and then passing these features onto 

391
00:28:18,901 --> 00:28:21,990
fully connected layers.
To predict a label.

392
00:28:23,510 --> 00:28:24,343
Image captioning,
we want to generate a sentence that 

393
00:28:26,761 --> 00:28:29,850
describes the semantic content of the 
same image.

394
00:28:30,420 --> 00:28:31,253
So let's take the same network from 
before and remove the fully connected 

395
00:28:34,711 --> 00:28:38,820
layers at the end.
Now we only have convolutional layers to

396
00:28:38,821 --> 00:28:39,654
extract features and we'll replace the 
fully connected layers with a recurrent 

397
00:28:44,221 --> 00:28:45,054
neural network.
The output of the convolutional layers 

398
00:28:47,791 --> 00:28:52,791
gives us a fixed length encoding of the 
features present in our input image,

399
00:28:53,370 --> 00:28:54,203
which we can use to initialize and rnn 
that we can then train to predict the 

400
00:28:58,801 --> 00:29:03,801
words that describe this image I'm using
using the rnn.

401
00:29:06,570 --> 00:29:09,900
So now that we've talked about 
convolutional neural networks,

402
00:29:09,960 --> 00:29:10,793
they're applications.
We can introduce some tools that have 

403
00:29:13,921 --> 00:29:18,921
recently been been designed to probe and
visualize the inner workings of a CNN to

404
00:29:19,831 --> 00:29:23,100
get at this question of what is the 
network actually seeing.

405
00:29:24,450 --> 00:29:26,130
So first off,
a few years ago,

406
00:29:26,131 --> 00:29:26,964
there was a paper that published an 
interactive visualization tool of a 

407
00:29:31,651 --> 00:29:36,450
convolutional neural network trained on 
a Dataset of handwritten digits,

408
00:29:36,451 --> 00:29:37,284
a very famous dataset called feminist,
and you can play around with this tool 

409
00:29:40,591 --> 00:29:41,424
to the behavior of the network given a 
number that you yourself drawn in and 

410
00:29:46,541 --> 00:29:47,374
what you're seeing here is the feature 
maps for this seven that someone has 

411
00:29:51,041 --> 00:29:54,790
drawn in and as we can see in the first 
layer,

412
00:29:54,820 --> 00:29:55,653
the first six filters are showing 
primarily edge detection and deeper 

413
00:30:00,281 --> 00:30:03,040
layers will start to pick up on corners,
crosses,

414
00:30:03,041 --> 00:30:05,380
curves,
more complex features,

415
00:30:06,460 --> 00:30:07,293
the exact hierarchy that that we 
introduced in the beginning of this 

416
00:30:10,781 --> 00:30:11,614
lecture.

417
00:30:12,750 --> 00:30:13,583
A second method which you'll you 
yourself will have the chance to play 

418
00:30:16,511 --> 00:30:19,780
around with in the second lap is called 
class activation.

419
00:30:19,781 --> 00:30:20,614
Maps or cans,
cans generate a heat map that indicates 

420
00:30:24,940 --> 00:30:29,940
the regions have an image to which are 
CNN for classification attends to in its

421
00:30:31,421 --> 00:30:32,254
final layers,
and the way that this is computed is is 

422
00:30:36,250 --> 00:30:37,083
the following.
We choose an output class that we want 

423
00:30:39,551 --> 00:30:40,384
to visualize.
We can then obtain the weights from the 

424
00:30:42,971 --> 00:30:43,804
last fully connected layer because these
represent the importance of each of the 

425
00:30:47,801 --> 00:30:50,920
final feature maps.
In outputting that class,

426
00:30:51,850 --> 00:30:52,683
we can compute our heat map as simply a 
weighted combination of each of the 

427
00:30:56,681 --> 00:30:57,514
final convolutional feature maps.
Using the weights from the last fully 

428
00:31:01,151 --> 00:31:01,984
connected layer,
we can apply cams to visualize both the 

429
00:31:06,701 --> 00:31:07,534
activation maps for the most likely 
predictions of an object class for one 

430
00:31:12,581 --> 00:31:13,414
image as you see on the left,
and also to visualize the image regions 

431
00:31:18,221 --> 00:31:23,221
used by the CNN to identify in different
instances of one object class.

432
00:31:24,160 --> 00:31:25,930
As you see on the right.

433
00:31:27,970 --> 00:31:28,803
So to conclude this talk,
I'd like to take a brief consideration 

434
00:31:33,281 --> 00:31:34,114
of how deep learning for computer vision
has really made an impact over the past 

435
00:31:40,510 --> 00:31:41,343
several years and the advances that have
been made in deep learning for computer 

436
00:31:44,321 --> 00:31:48,070
vision would really not be possible 
without the availability,

437
00:31:48,280 --> 00:31:52,480
availability of large and well annotated
image data sets.

438
00:31:53,320 --> 00:31:57,250
And this has really facilitated the 
progress that's been made.

439
00:31:57,580 --> 00:31:58,413
Some datasets include image net,
which we discussed amness a data data 

440
00:32:03,510 --> 00:32:04,343
Dataset of handwritten digits,
which was used in some of the first big 

441
00:32:08,080 --> 00:32:08,913
CNN papers,
places a database from here at mit of 

442
00:32:12,760 --> 00:32:13,593
scenes and landscapes and cipher 10,
which contains images from 10 different 

443
00:32:18,221 --> 00:32:23,221
categories listed here.
The impact has been broad and deep,

444
00:32:24,220 --> 00:32:27,140
um,
and spanning a variety of,

445
00:32:27,141 --> 00:32:32,141
of different fields.
Everything from medicine to self driving

446
00:32:32,561 --> 00:32:33,394
cars to security.
One area in which convolutional neural 

447
00:32:38,111 --> 00:32:42,770
networks really made a big impact early 
on was in facial recognition software.

448
00:32:43,010 --> 00:32:43,843
And if you think about it nowadays,
this software is pretty much ubiquitous 

449
00:32:46,880 --> 00:32:51,880
from social media to security systems.
Another area that's generate,

450
00:32:52,760 --> 00:32:57,200
generated a lot of excitement as of late
is in autonomous vehicles.

451
00:32:57,201 --> 00:33:02,000
And self driving cars,
so Nvidia has a research team working on

452
00:33:02,001 --> 00:33:05,990
a CNN based system for end to end 
control of self driving cars,

453
00:33:06,740 --> 00:33:11,720
their pipeline pizza,
single image from a camera on the car to

454
00:33:11,721 --> 00:33:15,320
a CNN that directly outputs a single 
number,

455
00:33:15,530 --> 00:33:20,210
which is the steering is the predicted 
steering wheel angle and the man you see

456
00:33:20,211 --> 00:33:21,044
in this video is actually one of our 
guests lecturers and on Thursday we'll 

457
00:33:26,421 --> 00:33:30,020
hear about how his team is,
is developing this platform.

458
00:33:31,820 --> 00:33:32,653
Finally,
we've also seen a significant impact in 

459
00:33:34,731 --> 00:33:38,870
the medical field where deep learning 
models have been applied to the analysis

460
00:33:38,871 --> 00:33:42,050
of a whole host of different types of 
medical images.

461
00:33:42,620 --> 00:33:43,453
Just this past year,
a team from Stanford developed a CNN 

462
00:33:46,461 --> 00:33:50,480
that could achieve dermatologists level 
classification of skin lesions.

463
00:33:51,140 --> 00:33:54,110
So you could imagine,
and this is what they actually did,

464
00:33:54,410 --> 00:33:57,320
having an app on your phone where you 
take a picture,

465
00:33:57,890 --> 00:34:02,150
upload that picture to the APP.
It's fed into a CNN that jen,

466
00:34:02,240 --> 00:34:06,950
that then generates a prediction of 
whether or not that lesion is reason for

467
00:34:06,951 --> 00:34:07,784
concern.
So to summarize what we've covered in 

468
00:34:11,961 --> 00:34:12,794
today's lecture,
we first considered the origins of the 

469
00:34:15,561 --> 00:34:19,910
computer vision problem,
how we can represent images as arrays of

470
00:34:19,911 --> 00:34:23,480
pixel values and what convolutions are,
how they work.

471
00:34:23,810 --> 00:34:24,643
We then discussed the basic architecture
of CNNS and finally we extended this to 

472
00:34:29,961 --> 00:34:32,720
consider some different applications,
um,

473
00:34:32,870 --> 00:34:35,060
of,
of convolutional neural networks,

474
00:34:35,120 --> 00:34:38,930
and also talked a bit about how we can 
visualize their behavior.

475
00:34:40,310 --> 00:34:41,060
So,
with that,

476
00:34:41,060 --> 00:34:42,680
I'd like to conclude,
um,

477
00:34:43,010 --> 00:34:47,330
I'm happy to take questions after the 
lecture portion is over.

478
00:34:48,140 --> 00:34:51,500
It's now my pleasure to introduce our 
next speaker,

479
00:34:51,560 --> 00:34:52,393
a special guest professor Aaron 
[inaudible] from the University of 

480
00:34:56,030 --> 00:34:56,863
Montreal.
Professor Kerrville is one of the 

481
00:34:58,430 --> 00:35:01,370
creators of generative adversarial 
networks,

482
00:35:01,400 --> 00:35:02,233
and he'll be talking to us about deep 
generative models and their 

483
00:35:05,031 --> 00:35:07,940
applications.
So please join me in welcoming him.

