1
00:00:04,270 --> 00:00:07,200
So Hi everyone.
My name is Alexander [inaudible].

2
00:00:07,260 --> 00:00:11,610
I'm a phd student and the computer 
science and ai lab at Mit.

3
00:00:11,880 --> 00:00:12,713
And today I'm going to be telling you 
about some of our work in building a 

4
00:00:15,721 --> 00:00:17,640
computer,
excuse me,

5
00:00:17,670 --> 00:00:20,400
building an autonomous vehicle control 
system,

6
00:00:20,870 --> 00:00:21,703
uh,
for parallel autonomy using and to end 

7
00:00:23,640 --> 00:00:26,250
learning.
I should also mention that this is joint

8
00:00:26,260 --> 00:00:30,780
work as part of the MIT tri or Toyota 
research institute partnership.

9
00:00:32,730 --> 00:00:33,563
So first of all,
when I say autonomous vehicle control 

10
00:00:35,011 --> 00:00:37,590
system,
what is it that I really mean?

11
00:00:37,980 --> 00:00:41,010
What does this require?
And by asking these questions,

12
00:00:41,011 --> 00:00:41,844
we can kind of start to see why we might
even want to care about an end to end 

13
00:00:45,720 --> 00:00:48,030
solution at all.
So for starters,

14
00:00:48,031 --> 00:00:52,800
we know that autonomous vehicle systems 
need to be able to handle many different

15
00:00:52,920 --> 00:00:53,753
types of scenarios.
So this includes things like changes in 

16
00:00:55,981 --> 00:00:59,160
luminosity,
the presence of lane markers,

17
00:00:59,220 --> 00:01:00,870
and even harsh weather conditions.

18
00:01:01,030 --> 00:01:01,863
This is something I,
a lot of autonomous vehicle systems 

19
00:01:03,661 --> 00:01:04,494
today are simply not able to handle.
Model based systems are often very 

20
00:01:09,031 --> 00:01:09,864
brittle and practice.
They require huge number of hand 

21
00:01:11,701 --> 00:01:16,701
programmed rules and engineered features
to be extracted.

22
00:01:16,771 --> 00:01:21,540
And instead in this project we aim to 
utilize large datasets to actually learn

23
00:01:21,541 --> 00:01:22,374
and underlying representation of how the
human actually drove so that we can 

24
00:01:25,141 --> 00:01:27,570
build a computer to replicate that in 
practice.

25
00:01:29,220 --> 00:01:30,053
So let's start by actually taking a step
back and looking at the standard comms 

26
00:01:32,581 --> 00:01:33,414
vehicle pipeline.
I know this might be very familiar to 

27
00:01:35,761 --> 00:01:37,260
some of you,
but I still wanted to go through it just

28
00:01:37,261 --> 00:01:39,810
so we can see the difference with an end
to end pipeline.

29
00:01:40,620 --> 00:01:44,040
And the key idea here is that the 
problem is broken up into many different

30
00:01:44,041 --> 00:01:47,970
submodules where each sub module is 
tackled independently.

31
00:01:48,000 --> 00:01:48,833
And you can see on the bottom I've put 
some references to actual works and 

32
00:01:51,991 --> 00:01:52,824
publications from both academia and 
industry focus on actually tackling an 

33
00:01:57,061 --> 00:01:59,430
individual sub problem of this pipeline.

34
00:01:59,610 --> 00:02:00,443
And it starts with on the left you can 
see that we're actually just collecting 

35
00:02:02,341 --> 00:02:03,174
data about our environment,
we're trying to learn what is happening 

36
00:02:05,671 --> 00:02:06,504
around us.
So this can include things like camera 

37
00:02:07,831 --> 00:02:08,790
data,
Lidar,

38
00:02:08,820 --> 00:02:10,500
radar,
initial data,

39
00:02:10,501 --> 00:02:11,334
et cetera.
And then we feed this into some sort of 

40
00:02:13,291 --> 00:02:14,124
detection pipeline cause it's the first 
thing we want to do is actually 

41
00:02:15,811 --> 00:02:18,090
understand where the obstacles around us
are.

42
00:02:18,300 --> 00:02:21,990
We need to identify these obstacles as 
both static or dynamic.

43
00:02:22,500 --> 00:02:23,333
We might want to pay even more attention
to dynamic obstacles and perhaps plan 

44
00:02:26,131 --> 00:02:26,964
their emotion and the environment.
And once we have an idea of what's 

45
00:02:29,611 --> 00:02:30,444
happening and the objects around us,
we want to actually localize ourselves 

46
00:02:33,480 --> 00:02:36,060
relative to those objects.
And this is called localization.

47
00:02:37,440 --> 00:02:40,740
Once we have an idea of where we are,
where the objects are under Sar,

48
00:02:40,770 --> 00:02:42,990
we can finally start to talk about 
planning,

49
00:02:42,991 --> 00:02:46,880
which is really one of the most 
fundamental parts of,

50
00:02:47,170 --> 00:02:48,090
of autonomous driving.

51
00:02:48,090 --> 00:02:53,090
So the goal of autonomous driving is to 
travel from point a to b safely,

52
00:02:53,340 --> 00:02:54,173
right?
And that's exactly what planning allows 

53
00:02:55,231 --> 00:02:57,660
us to accomplish.
So planning takes into account where the

54
00:02:57,661 --> 00:02:58,494
obstacles are and then allows us to 
actually plan around to reach our goal 

55
00:03:02,891 --> 00:03:05,770
destination.
Once we have this plan,

56
00:03:05,800 --> 00:03:06,633
we can now send actuation of signals to 
the car motor and actually execute that 

57
00:03:10,781 --> 00:03:11,614
plan.
So in this project we actually replaced 

58
00:03:14,611 --> 00:03:18,490
that entire intermediate pipeline with a
learned model.

59
00:03:18,550 --> 00:03:23,020
And in this project we're actually 
focused on that learn model being a deep

60
00:03:23,021 --> 00:03:23,854
neural network.
And instead of actually learning the 

61
00:03:26,681 --> 00:03:27,514
control directly from raw sensory data,
we're going to just focus on pixel 

62
00:03:31,091 --> 00:03:34,240
value.
So using a single front facing camera as

63
00:03:34,241 --> 00:03:39,040
our sensory input and instead of 
predicting all possible control,

64
00:03:39,070 --> 00:03:39,903
we're just going to focus on predicting 
the steering wheel angle at that 

65
00:03:42,101 --> 00:03:42,934
instant.

66
00:03:44,710 --> 00:03:47,560
And while this may seem like a very 
simple problem at first,

67
00:03:47,561 --> 00:03:49,750
because we are removing all of these 
intermediate pipeline,

68
00:03:49,900 --> 00:03:50,733
all of these intermediate problems,
this is actually an extremely hard 

69
00:03:54,011 --> 00:03:57,340
problem in practice to actually get 
working in real life.

70
00:03:58,000 --> 00:04:03,000
And there are a couple reasons why this 
is so difficult to work in real life.

71
00:04:03,101 --> 00:04:03,934
So firstly,
in the real world we face huge amounts 

72
00:04:07,121 --> 00:04:07,954
of uncertainty,
huge amounts of more important than 

73
00:04:09,311 --> 00:04:11,800
uncertainty.
We faced huge amounts of ambiguity.

74
00:04:12,370 --> 00:04:14,920
So this is a picture of a roundabout in 
Paris.

75
00:04:14,921 --> 00:04:19,921
You can see that we have to deal with 
huge amounts of complexity in real world

76
00:04:20,531 --> 00:04:23,320
driving.
We have to be able to handle uncertainty

77
00:04:23,350 --> 00:04:26,650
in the environment.
We have to be able to handle uncertainty

78
00:04:26,651 --> 00:04:27,484
and the cars around us and even 
uncertainty in their intentions and 

79
00:04:31,271 --> 00:04:34,070
their actions.
So we don't know what the vehicles,

80
00:04:34,071 --> 00:04:37,330
the pedestrians,
the cyclist are doing around us.

81
00:04:37,420 --> 00:04:40,180
And we have to account for that as part 
of our planning algorithms.

82
00:04:41,710 --> 00:04:44,860
And since we're focusing on vision data,
there are also many challenges that come

83
00:04:44,861 --> 00:04:47,770
with this as well.
So camera sensors are subject,

84
00:04:47,800 --> 00:04:51,280
so sun glare.
So he can see on the bottom here there's

85
00:04:51,281 --> 00:04:54,790
a image of the sun actually facing 
directly into the camera.

86
00:04:54,791 --> 00:04:57,430
And this kind of renders the entire 
driving seen underneath that,

87
00:04:57,900 --> 00:04:59,110
um,
almost black.

88
00:04:59,111 --> 00:05:03,940
So it makes the driving or the imagery 
very difficult to actually interpret.

89
00:05:05,020 --> 00:05:09,300
And additionally we have to be able to 
account for uncertainty in the,

90
00:05:09,320 --> 00:05:12,340
the perception itself.
So dealing with harsh weather conditions

91
00:05:12,341 --> 00:05:14,300
brings about,
uh,

92
00:05:14,350 --> 00:05:18,340
the inability to accurately image the 
environment and things like rain,

93
00:05:18,341 --> 00:05:21,910
snow and et Cetera.
And then finally,

94
00:05:22,030 --> 00:05:22,863
one of the,
perhaps most key challenge of all 

95
00:05:26,101 --> 00:05:27,910
autonomous driving,
not just end to end,

96
00:05:27,911 --> 00:05:29,290
is dealing with the edge cases.

97
00:05:29,860 --> 00:05:34,210
So what happens if we're driving and we 
see a plane in the road in front of us,

98
00:05:35,020 --> 00:05:38,190
like we can see on this top image here,
uh,

99
00:05:39,010 --> 00:05:39,843
or on the other hand,
what if we face assist a scenario like 

100
00:05:42,791 --> 00:05:43,624
this bottom spot them scenario here 
where there is actually a sign painted 

101
00:05:47,651 --> 00:05:48,484
on the back of a car of a pedestrian and
a cyclist and our object detector at 

102
00:05:51,900 --> 00:05:52,733
detection pipeline has actually 
recognized that sign as containing a 

103
00:05:55,691 --> 00:05:57,410
real life,
human or pedestrian.

104
00:05:58,760 --> 00:06:01,690
So these are real problems that they 
face all the time.

105
00:06:01,691 --> 00:06:03,860
His vehicles including end to end 
systems.

106
00:06:05,260 --> 00:06:05,580
Yeah,

107
00:06:05,580 --> 00:06:06,413
it's,
I'd like to talk about some of the 

108
00:06:06,821 --> 00:06:09,070
topics I'd like to go for as part of 
this talk.

109
00:06:10,270 --> 00:06:11,103
Firstly,
I'd like to begin this talk by 

110
00:06:12,910 --> 00:06:16,540
explaining this phrase that I introduced
in my title slide,

111
00:06:16,541 --> 00:06:17,374
which is parallel autonomy.
I have if I haven't actually mentioned 

112
00:06:20,291 --> 00:06:21,124
what parallel autonomy is yet.
So I'm going to talk about this in much 

113
00:06:24,191 --> 00:06:25,750
more detail in the first part of the 
talk.

114
00:06:26,820 --> 00:06:27,250
Yeah.

115
00:06:27,250 --> 00:06:31,180
And really I'll walk you through firstly
what the definition of parallel autonomy

116
00:06:31,181 --> 00:06:32,014
is and the autonomy system that we've 
built at mit to actually create a test 

117
00:06:38,251 --> 00:06:39,084
bed for a parallel tonomy testing.
Next I'll present some of our work and 

118
00:06:43,651 --> 00:06:47,460
not only learning the steering wheel 
control of an autonomous vehicle,

119
00:06:47,461 --> 00:06:48,294
but also the corresponding steering 
bounds for that vehicle and an end to 

120
00:06:51,391 --> 00:06:52,224
end manner.
I'll show you how this work actually 

121
00:06:54,751 --> 00:06:55,584
tackles some of the issues that I 
presented earlier that end to end 

122
00:06:59,341 --> 00:07:01,350
systems today currently face.

123
00:07:02,190 --> 00:07:02,840
Okay.

124
00:07:02,840 --> 00:07:03,673
And finally we'll talk about how we can 
actually predict when the model is 

125
00:07:05,691 --> 00:07:09,170
likely to fail,
when we can determine how uncertain does

126
00:07:09,171 --> 00:07:11,750
model is.
When can we trust our end to end system?

127
00:07:11,751 --> 00:07:13,520
So this is another huge challenge in 
intent,

128
00:07:14,030 --> 00:07:15,350
uh,
algorithms.

129
00:07:15,410 --> 00:07:16,243
We have to be able to account for the 
situations where the model doesn't know 

130
00:07:18,981 --> 00:07:22,670
what it's talking about and we have to 
be able to accurately predict these.

131
00:07:24,450 --> 00:07:24,860
Yeah.

132
00:07:24,860 --> 00:07:26,180
So let's start with the first part of 
the talk,

133
00:07:26,181 --> 00:07:30,140
which is all about parallel autonomy.
All this really means is that we have an

134
00:07:30,141 --> 00:07:33,620
autonomy controller that it's running 
two parallel threads in the background.

135
00:07:33,650 --> 00:07:37,760
One is the human and one is the robot or
the standard autonomy system.

136
00:07:39,380 --> 00:07:40,213
And the fusion of these two controls 
results in what we call parallel 

137
00:07:42,981 --> 00:07:46,760
autonomy or this,
this paradigm of like shared robot human

138
00:07:46,820 --> 00:07:47,653
interaction.
And intuitively the best way that I can 

139
00:07:50,931 --> 00:07:55,340
actually think of describing this as 
some sort of guardian angel in your car,

140
00:07:55,520 --> 00:07:57,560
preventing you from actually making an 
accident.

141
00:07:58,100 --> 00:07:58,933
So let me just play this commercial,
which I think actually sums this up 

142
00:08:00,861 --> 00:08:01,910
really,
really nicely.

143
00:08:01,911 --> 00:08:04,040
And this has really captured well,
uh,

144
00:08:04,070 --> 00:08:05,510
through this,
through this clip.

145
00:08:21,430 --> 00:08:26,430
Three,
remember when only dead could say today,

146
00:08:30,310 --> 00:08:32,620
sorry,
that's what auto emergency braking,

147
00:08:34,020 --> 00:08:35,830
right?
So,

148
00:08:36,400 --> 00:08:37,233
okay,

149
00:08:37,440 --> 00:08:39,320
so essentially,
no,

150
00:08:39,321 --> 00:08:40,154
I hope you have some sense of idea what 
I'm talking about when I say guardian 

151
00:08:42,631 --> 00:08:45,510
angels.
So parallel autonomy is this human robot

152
00:08:45,570 --> 00:08:46,403
shared control paradigm whereby the 
human is always in control of the 

153
00:08:49,321 --> 00:08:50,154
vehicle,
but the autonomy system is always 

154
00:08:51,901 --> 00:08:52,734
running in the background preventing and
is responsible preventing that vehicle 

155
00:08:56,551 --> 00:08:57,384
from ever causing an accident.
So you can imagine our software 

156
00:08:59,881 --> 00:09:02,370
architecture software pipeline starting 
with the country,

157
00:09:02,580 --> 00:09:04,770
the two control inputs.
One is the human,

158
00:09:04,771 --> 00:09:07,140
one is our standard series autonomy 
controller.

159
00:09:07,950 --> 00:09:10,650
And then these being fed into a shared 
control system,

160
00:09:10,651 --> 00:09:13,140
which is then fed into a low level 
tracking control,

161
00:09:13,560 --> 00:09:18,560
which we can feed into a drive by wire 
interface as part of our actual uh,

162
00:09:19,610 --> 00:09:20,443
autonomy hardware,
which is just a full scale autonomous 

163
00:09:22,591 --> 00:09:23,424
vehicle.
So at Mit we've created a test bed for 

164
00:09:28,191 --> 00:09:31,220
autonomous vehicle development.
Specifically we have a fee,

165
00:09:31,290 --> 00:09:35,750
a fleet of autonomous systems consisting
of two cars,

166
00:09:35,780 --> 00:09:36,613
two Toyota Prius's and two wheelchairs,
which we've completely retrofitted with 

167
00:09:40,940 --> 00:09:43,280
autonomous drive by wire capabilities.

168
00:09:44,210 --> 00:09:45,043
The wheelchair serve as basically this 
development platform for debugging and 

169
00:09:49,491 --> 00:09:50,324
developing our algorithms before we 
actually moved them onto the full scale 

170
00:09:52,561 --> 00:09:53,394
car.
So this is more of like a safety safety 

171
00:09:55,791 --> 00:09:58,670
step on our end that we take of 
debugging all of our,

172
00:09:58,880 --> 00:10:01,370
all of our algorithms,
all of our code on the wheelchair,

173
00:10:01,371 --> 00:10:05,570
which says the exact same sensor suite 
as the full scale cars.

174
00:10:05,571 --> 00:10:07,670
So we'd has all of the cameras,
the Lidars,

175
00:10:07,790 --> 00:10:08,870
the radars,
et cetera.

176
00:10:09,080 --> 00:10:11,480
So it serves as a great test bed before 
we moved to the car.

177
00:10:12,170 --> 00:10:14,780
But let's take a look at one of our cars
just to jump straight to it.

178
00:10:15,740 --> 00:10:16,573
So in our car we've installed five lidar
laser scanners for three d imaging of 

179
00:10:20,541 --> 00:10:21,374
the environment.

180
00:10:22,660 --> 00:10:23,040
Okay.

181
00:10:23,040 --> 00:10:26,370
Three GMS sell cameras on the front of 
the car.

182
00:10:26,371 --> 00:10:28,860
And actually now this is actually a 
little bit outdated.

183
00:10:28,861 --> 00:10:32,010
So we've installed two more cameras on 
the sides of the car for side imaging as

184
00:10:32,011 --> 00:10:32,844
well.
There's an ost x gps installed for 

185
00:10:35,820 --> 00:10:36,653
coarse grain localization,
an imu for collecting data like 

186
00:10:39,901 --> 00:10:42,870
acceleration,
rotation and orientation data.

187
00:10:43,740 --> 00:10:48,300
And finally we collect information about
the speed of the car through odometry,

188
00:10:48,370 --> 00:10:49,470
through,
sorry,

189
00:10:49,500 --> 00:10:52,500
through to will and coatings on the back
two wheels.

190
00:10:53,760 --> 00:10:54,320
Okay.

191
00:10:54,320 --> 00:10:55,153
All of the sensors are that fed into our
nvidia drive px to which serves as our 

192
00:10:59,691 --> 00:11:04,070
gpu enabled computing platform,
which takes all of this information in,

193
00:11:04,310 --> 00:11:06,440
combines it with our end to end 
controllers,

194
00:11:07,100 --> 00:11:07,933
and then sends these two are drive by 
wire interface and our custom Ecu Board 

195
00:11:11,151 --> 00:11:12,810
to actually,
uh,

196
00:11:13,040 --> 00:11:14,930
command the car and actually control the
car.

197
00:11:16,220 --> 00:11:21,060
The great thing about this [inaudible] 
is actually that it has to uh,

198
00:11:21,380 --> 00:11:22,213
onboard gps,
which an eye which allows us to do 

199
00:11:24,051 --> 00:11:25,590
really fast,
realtime,

200
00:11:25,640 --> 00:11:27,170
deep neural network in France as well.

201
00:11:29,240 --> 00:11:31,910
Now before going any further,
I want to actually draw this distinction

202
00:11:31,911 --> 00:11:35,480
between shared control and binary 
control.

203
00:11:35,930 --> 00:11:40,760
So typically when people think of a 
shared control in the autonomous vehicle

204
00:11:40,761 --> 00:11:41,594
setting,
they're thinking of something like 

205
00:11:42,380 --> 00:11:45,650
binary control.
And especially in the last talk,

206
00:11:45,651 --> 00:11:50,150
we got this idea of this handoff problem
where the human drivers and control,

207
00:11:50,151 --> 00:11:54,040
but we were trying to figure out when to
hand off control to the autonomous or if

208
00:11:54,041 --> 00:11:54,874
the autonomous system is in control.
We want to figure out when did tonto 

209
00:11:57,091 --> 00:11:57,924
system doesn't know what's going on and 
I can hand off control back to the 

210
00:11:59,561 --> 00:12:00,394
human.
This is actually a much simpler problem 

211
00:12:02,891 --> 00:12:05,680
than what we're doing with in our lab,
which is shared control,

212
00:12:05,681 --> 00:12:06,514
which incentive?
This binary switch we're dealing with 

213
00:12:08,351 --> 00:12:10,840
actually a fusion of the two control 
systems.

214
00:12:12,950 --> 00:12:16,820
So not only do we have to decide when,
which system to trust more,

215
00:12:17,000 --> 00:12:17,833
but we actually have to turn in the best
way to fuse them together in a very 

216
00:12:21,141 --> 00:12:25,070
natural manner such that the human who 
is always driving the car,

217
00:12:25,071 --> 00:12:27,830
it doesn't feel like they're losing 
control of what they're doing.

218
00:12:29,060 --> 00:12:29,630
Okay.

219
00:12:29,630 --> 00:12:30,463
This actually results in us not being 
able to rely on many of the classical 

220
00:12:33,131 --> 00:12:36,160
techniques for controlling our car,
uh,

221
00:12:36,220 --> 00:12:40,780
which a lot of other industries and uh,
academic institutions have,

222
00:12:41,390 --> 00:12:42,700
um,
have have,

223
00:12:42,710 --> 00:12:43,570
um,
gone through.

224
00:12:44,860 --> 00:12:45,693
So I'd actually like to take a second 
just through this slide and actually 

225
00:12:48,041 --> 00:12:50,440
talk about how we achieve control of our
cars.

226
00:12:50,950 --> 00:12:51,783
So one option is that we can,
we can do this by actually connecting a 

227
00:12:55,451 --> 00:12:56,284
motor straight to the steering wheel or 
to the throttle and actually commanding 

228
00:13:00,671 --> 00:13:05,080
the internal engine of this motor to,
to turn the steering wheel itself.

229
00:13:05,110 --> 00:13:07,570
And this suffers a lot from,
we actually tried this,

230
00:13:07,571 --> 00:13:09,930
it's very difficult to get this kind of 
system working.

231
00:13:09,950 --> 00:13:12,010
It's very fragile and practice very 
brittle,

232
00:13:12,710 --> 00:13:13,543
suffers from responsiveness,
also durability over time as you can 

233
00:13:16,031 --> 00:13:17,830
imagine.
The second way,

234
00:13:17,831 --> 00:13:18,664
which is really the classical way in 
many industries that mini intercities 

235
00:13:21,360 --> 00:13:25,210
I've taking is to actually send canned 
messages to the board,

236
00:13:25,270 --> 00:13:26,103
to the car.

237
00:13:26,290 --> 00:13:27,123
Ken is like this,
a language that cars can take to 

238
00:13:29,710 --> 00:13:31,510
actually communicate with different 
parts of the car.

239
00:13:31,511 --> 00:13:32,344
So one way that you can imagine,
it's us just sending the canned codes 

240
00:13:35,111 --> 00:13:38,770
for steering wheel angle to the car and 
then controlling it like that.

241
00:13:39,130 --> 00:13:41,890
This actually is not possible in a 
shared control paradigm.

242
00:13:41,891 --> 00:13:44,470
So this is possible if we're talking 
about binary control,

243
00:13:44,650 --> 00:13:45,483
but in shared control,
we actually want to read the effort of 

244
00:13:48,701 --> 00:13:51,880
the human on the wheel.
We don't care about that per se.

245
00:13:51,881 --> 00:13:55,420
We don't care about the angle that the 
wheel is turned.

246
00:13:55,450 --> 00:13:58,360
We really care about how much effort is 
being exerted on the wheel,

247
00:13:59,310 --> 00:14:01,910
right?
So something more like we care about the

248
00:14:01,911 --> 00:14:05,510
torque rather than the actual angle if 
the wheel at that instant.

249
00:14:06,230 --> 00:14:08,750
So instead we have to turn to this third
approach,

250
00:14:08,751 --> 00:14:09,584
which is actually what we did a which is
actually spoofing the input systems of 

251
00:14:13,221 --> 00:14:14,054
the car.

252
00:14:14,560 --> 00:14:14,810
Okay.

253
00:14:14,810 --> 00:14:15,643
This essentially means that we've 
created drive by wire capability by 

254
00:14:18,021 --> 00:14:21,290
sending the car like fake or synthetic 
data.

255
00:14:21,740 --> 00:14:25,340
This makes it believe that the human is 
still controlling it,

256
00:14:25,700 --> 00:14:26,533
but it is not able to accurately observe
the observation at the human is doing 

257
00:14:28,910 --> 00:14:29,743
because we're spoofing the input signals
and combining them with our autonomy 

258
00:14:32,391 --> 00:14:33,224
controller.

259
00:14:33,340 --> 00:14:34,173
Okay.

260
00:14:34,420 --> 00:14:36,520
And this is what we've created in our 
lab.

261
00:14:37,360 --> 00:14:41,170
This approach really leads to a very 
unique a control system.

262
00:14:41,730 --> 00:14:43,270
Um,
and to summarize,

263
00:14:43,271 --> 00:14:47,560
this really creates three distinct 
autonomous modes.

264
00:14:47,830 --> 00:14:48,663
And the first is just manual control.
This just means that the human is in 

265
00:14:51,381 --> 00:14:54,590
complete control of the car.
This is what we use when we're trying to

266
00:14:54,591 --> 00:14:55,424
focus on things like data collection.
And on the other extreme you can see 

267
00:14:59,390 --> 00:15:01,130
the,
when the computer controls the car.

268
00:15:01,131 --> 00:15:01,964
So this is when you have no human input.
And this is what people usually talk 

269
00:15:05,601 --> 00:15:07,730
about when they talk about autonomous 
vehicles.

270
00:15:07,731 --> 00:15:10,340
So this is just the computer controlling
of the car.

271
00:15:11,060 --> 00:15:14,810
But also what's unique here that I'd 
like to really hammer home this point is

272
00:15:14,811 --> 00:15:15,644
that we've created this third mode,
which is a combination of both manual 

273
00:15:18,471 --> 00:15:20,630
and computer,
which we call parallel autonomy,

274
00:15:21,370 --> 00:15:22,203
um,
where the robot is simultaneously 

275
00:15:23,901 --> 00:15:24,734
observing the human but also actuating 
with the human in the shirt controlled 

276
00:15:28,560 --> 00:15:32,780
powerline paradigm.
All three of these modes are part of the

277
00:15:32,781 --> 00:15:33,614
mit tri autonomous driving pipeline and 
this allows us to really test and debug 

278
00:15:37,130 --> 00:15:41,360
many of these algorithms I'm going to be
talking about in this talk.

279
00:15:41,390 --> 00:15:42,223
I'm just going to talk about one of the 
algorithms that we've created in my lab 

280
00:15:46,550 --> 00:15:47,383
to actually learn the steering control 
of a car but not only learned to 

281
00:15:50,481 --> 00:15:52,280
control,
but actually learned the balance,

282
00:15:52,400 --> 00:15:55,610
the safe and valid bounds for 
controlling in that environment.

283
00:15:57,720 --> 00:15:58,553
So there's actually been a lot of work 
and using machine learning to control a 

284
00:16:02,071 --> 00:16:02,904
car using raw perception data.
Most notably work here from Nvidia has 

285
00:16:07,741 --> 00:16:08,574
shown that it is possible to build a 
neural network that will take in as 

286
00:16:12,121 --> 00:16:15,330
input a single frame of an image of a 
camera,

287
00:16:15,331 --> 00:16:16,164
sorry,
and directly control the steering wheel 

288
00:16:18,541 --> 00:16:21,210
angle if that car.
So since then there have been many,

289
00:16:21,211 --> 00:16:25,530
many extensions of this work attempting 
very similar solutions.

290
00:16:25,680 --> 00:16:28,800
Some using LSTM is to capture a temporal
dependencies.

291
00:16:29,310 --> 00:16:32,310
Others utilize utilizing techniques from
imitation learning.

292
00:16:32,340 --> 00:16:34,000
But all of these,
uh,

293
00:16:34,180 --> 00:16:36,540
works really suffer from the same 
problem.

294
00:16:36,541 --> 00:16:37,374
So the key problem here that they all 
face is that they lack this notion of 

295
00:16:43,230 --> 00:16:46,770
the ability to feed into higher level 
navigational controllers.

296
00:16:47,310 --> 00:16:49,350
So taking videos and to a network,
for example,

297
00:16:49,351 --> 00:16:53,730
this network outputs a single real 
valued number at the output.

298
00:16:54,990 --> 00:16:58,140
What this means is that it is impossible
for that single real value number,

299
00:16:58,141 --> 00:17:01,890
which just represents a steering wheel 
angle to capture any form of uncertainty

300
00:17:01,891 --> 00:17:02,724
measurement.
It also means it's impossible for that 

301
00:17:04,830 --> 00:17:08,480
number to capture different possible 
ambiguous situations.

302
00:17:08,481 --> 00:17:09,314
So imagine if you're at an intersection 
and there are three different ways that 

303
00:17:11,821 --> 00:17:12,654
you can go.
If you only have one output of your 

304
00:17:14,281 --> 00:17:15,114
neural network,
it's impossible for that network to 

305
00:17:17,101 --> 00:17:20,220
handle anything other than an ambiguous 
situations.

306
00:17:20,880 --> 00:17:22,830
Thus,
this really makes all of these solutions

307
00:17:22,831 --> 00:17:25,620
ill suited to be used in conjunction 
with higher levels,

308
00:17:25,630 --> 00:17:29,820
navigational control,
which must be able to handle uncertainty

309
00:17:29,821 --> 00:17:33,210
in the environment and also ambiguity in
your decision making.

310
00:17:35,180 --> 00:17:36,290
So instead,
in this work,

311
00:17:36,291 --> 00:17:40,250
what we attempt to do is actually learn 
a full probability distribution over the

312
00:17:40,251 --> 00:17:41,084
control actions.
And we do this by first district ising 

313
00:17:44,000 --> 00:17:44,833
our entire action space of all steering 
wheel angles to handle ambiguity and 

314
00:17:49,501 --> 00:17:52,320
then learn a neural network to output 
this discrete distribution.

315
00:17:53,490 --> 00:17:54,323
Then we transformed this discrete 
distribution into a continuous 

316
00:17:57,151 --> 00:18:00,310
probability density function or pdf,
which could,

317
00:18:00,390 --> 00:18:05,260
which we can then use to analytically 
extract control bounds,

318
00:18:05,570 --> 00:18:07,770
uh,
that we can use in our parallel autonomy

319
00:18:07,771 --> 00:18:08,604
framework.
So let's break this pipeline down a 

320
00:18:11,311 --> 00:18:13,770
little bit more to go into it a little 
more detail.

321
00:18:14,580 --> 00:18:15,413
So first,
let's focus on this part where we just 

322
00:18:16,591 --> 00:18:17,424
take a single input image frame and we 
want to predict a one dimensional 

323
00:18:22,770 --> 00:18:27,770
discreet probability density function.
So Xii is the image from the data set of

324
00:18:27,811 --> 00:18:30,480
m images.
We want to learn some sort of functional

325
00:18:30,481 --> 00:18:35,481
mapping f with parameters data to 
predict this probability distribution on

326
00:18:35,491 --> 00:18:38,070
the right and F for us,
like I said,

327
00:18:38,071 --> 00:18:40,590
is a deep neural network and data or 
just the weights.

328
00:18:40,980 --> 00:18:41,813
So sinceF is just a deep neural network,
we can optimize this end to end using 

329
00:18:46,201 --> 00:18:49,440
backpropagation,
which entails effectively minimizing the

330
00:18:49,441 --> 00:18:52,380
cross entropy loss for all of our m 
examples.

331
00:18:53,040 --> 00:18:53,873
So note here that the true distribution,
which I did notice Pii is just the 

332
00:18:58,981 --> 00:19:01,650
distribution that the human took at that
time.

333
00:19:02,760 --> 00:19:03,870
I,
or sorry,

334
00:19:03,950 --> 00:19:05,700
the,
the um,

335
00:19:05,880 --> 00:19:09,930
estimate distribution is what we tried 
to get as close as possible to the,

336
00:19:10,140 --> 00:19:10,973
to the true distribution.
But keep in mind here that the tree 

337
00:19:13,601 --> 00:19:16,200
distribution that the human actually 
took at that time,

338
00:19:16,201 --> 00:19:18,210
it's actually a very simple 
distribution.

339
00:19:18,211 --> 00:19:20,010
It's just a Delta function,
right?

340
00:19:20,011 --> 00:19:20,844
So it has the value of one in the 
direction that this human drove and a 

341
00:19:23,611 --> 00:19:24,444
value of zero everywhere else.
So this is really remarkable because 

342
00:19:27,961 --> 00:19:32,961
what I'm telling you is that given data 
of only union modal actions,

343
00:19:34,080 --> 00:19:36,870
we're attempting to learn in multimodal 
distribution,

344
00:19:38,010 --> 00:19:38,490
right?

345
00:19:38,490 --> 00:19:42,210
So imagine a scene like an intersection 
like this so he can see that there's two

346
00:19:42,211 --> 00:19:43,044
possible actions a person can take.
Either go left or go right and we want 

347
00:19:46,711 --> 00:19:49,860
to train a,
let's suppose a very simple three output

348
00:19:49,861 --> 00:19:54,861
neural network that can handle these 
left right or straight actions.

349
00:19:55,950 --> 00:19:57,870
So when we started out,
we haven't seen any training data,

350
00:19:57,871 --> 00:19:59,970
so we may see something random that it's
outputting.

351
00:20:00,870 --> 00:20:03,840
But now if the human takes an action and
tries to turn left,

352
00:20:04,270 --> 00:20:05,103
the neural network learns from this,
it starts to promote the left action a 

353
00:20:09,211 --> 00:20:10,044
little bit more.
You can actually see that on the top 

354
00:20:11,971 --> 00:20:12,804
left here.
This is an example of what the true 

355
00:20:14,101 --> 00:20:17,250
distribution for what the human actually
took would look like.

356
00:20:17,251 --> 00:20:18,084
So it has a value of one when it went on
the bin that corresponds to left turns 

357
00:20:22,170 --> 00:20:23,003
and Zeros on the other two bins.
If on the next intersection in our 

358
00:20:28,321 --> 00:20:30,690
training dead or we see a similar 
intersection like this,

359
00:20:30,691 --> 00:20:33,150
it doesn't necessarily have to be the 
exact same intersection,

360
00:20:34,050 --> 00:20:34,883
but the human turned left again.
So now the network will now start to 

361
00:20:37,441 --> 00:20:38,274
really confidently promote this left 
turn action even more and discouraged 

362
00:20:42,720 --> 00:20:45,210
straight and right,
like I mentioned before.

363
00:20:47,030 --> 00:20:48,200
Now finally,
let's see something

364
00:20:48,200 --> 00:20:50,210
interesting happening.
So now finally,

365
00:20:50,240 --> 00:20:51,740
the human decisive turn,
right?

366
00:20:52,250 --> 00:20:55,280
The network is starting to learn both 
actions by promoting left,

367
00:20:55,340 --> 00:20:59,190
by promoting the right action.
And actually it's starting to discourage

368
00:20:59,191 --> 00:21:01,790
left action even though it,
it learned that before.

369
00:21:01,791 --> 00:21:02,624
So it actually starts to forget some of 
that information that I saw before in 

370
00:21:05,981 --> 00:21:06,814
bye bye.
In the process of promoting the current 

371
00:21:08,781 --> 00:21:12,200
action,
if we keep taking right turns,

372
00:21:12,201 --> 00:21:13,034
we keep promoting the right action and 
we start to lose some of the power on 

373
00:21:16,191 --> 00:21:20,270
the left.
And if we keep seeing straight runs,

374
00:21:20,271 --> 00:21:22,910
eventually the model even learned that,
right?

375
00:21:22,970 --> 00:21:24,530
Sorry.
If we keep seeing,

376
00:21:25,350 --> 00:21:26,890
uh,
right turns the model,

377
00:21:26,891 --> 00:21:27,724
we'll learn that a right turn is perhaps
even more likely than a left turn in 

378
00:21:31,151 --> 00:21:36,151
this particular scenario.
So over time we eventually expect to see

379
00:21:36,561 --> 00:21:37,394
some sort of convergence where the human
in our dre training Dataset has 

380
00:21:41,811 --> 00:21:45,560
collected enough data supporting both 
left and right action.

381
00:21:45,561 --> 00:21:48,830
So we expect to converge to some 
probability distribution,

382
00:21:48,831 --> 00:21:51,080
which is like a 0.5
on left and 0.5

383
00:21:51,081 --> 00:21:52,160
on right and zero

384
00:21:54,180 --> 00:21:57,360
everyone else.
Okay,

385
00:21:58,020 --> 00:21:58,853
sorry.

386
00:22:01,390 --> 00:22:02,223
So this is really awesome because it 
shows us that we can actually learn 

387
00:22:04,661 --> 00:22:08,290
multimodal distributions using only uni 
modal ground truth data.

388
00:22:09,440 --> 00:22:10,273
So this eliminates the need to actually 
manually label each possible control 

389
00:22:13,851 --> 00:22:15,440
action,
right?

390
00:22:15,980 --> 00:22:17,480
That the car could have taken at that 
frame.

391
00:22:17,481 --> 00:22:19,700
So we're using the raw data collected 
from the,

392
00:22:19,730 --> 00:22:22,760
from the human in that environment,
and instead we can actually use,

393
00:22:23,600 --> 00:22:24,433
we don't even need to keep driving over 
the exact same intersection over and 

394
00:22:26,811 --> 00:22:29,240
over.
What we're learning here is some sort of

395
00:22:29,241 --> 00:22:32,540
representation of where the,
um,

396
00:22:32,750 --> 00:22:35,240
where is the drivable space in this 
region?

397
00:22:36,090 --> 00:22:36,923
Yeah.

398
00:22:37,630 --> 00:22:38,463
Uh,
this is especially powerful because it 

399
00:22:39,761 --> 00:22:44,290
means that we are not constrained to 
predefined types of intersections like t

400
00:22:44,291 --> 00:22:46,840
way t intersections for way 
intersections.

401
00:22:46,990 --> 00:22:50,650
We are simply learning about drivable 
space in the environment.

402
00:22:50,651 --> 00:22:54,460
And this is a very powerful notion.
So to test this,

403
00:22:54,461 --> 00:22:59,461
we collected seven hours of driving data
and the Greater Boston area.

404
00:23:00,490 --> 00:23:03,790
This data consisted mostly of highway 
and city roads.

405
00:23:04,120 --> 00:23:08,830
And since our test environment where we 
actually test our cars is very different

406
00:23:08,831 --> 00:23:12,050
than this training environment.
We decided to do a bit of fine tuning on

407
00:23:12,051 --> 00:23:12,884
our models on roads without lane markers
to teach the network to be able to 

408
00:23:16,961 --> 00:23:20,860
handle some of some of these unmarked 
roads as well.

409
00:23:21,890 --> 00:23:22,723
So we feed all this data into our Nvidia
dgx one to actually train our models 

410
00:23:26,781 --> 00:23:30,170
using data parallelism on three deep on 
three gps.

411
00:23:30,200 --> 00:23:32,810
And this takes around one hour in 
practice.

412
00:23:34,880 --> 00:23:35,200
Okay,

413
00:23:35,200 --> 00:23:36,033
so I'm going to go over the next step of
the pipeline before sharing some 

414
00:23:38,500 --> 00:23:39,333
exciting results of the,
of the system actually working in 

415
00:23:41,381 --> 00:23:42,214
practice.
So now since we've learned a meaningful 

416
00:23:44,561 --> 00:23:48,650
discrete representation of where the 
drivable spaces,

417
00:23:48,680 --> 00:23:49,513
so what our control commands that the 
car can actually execute at that 

418
00:23:52,821 --> 00:23:53,654
instant,
we want to now transform this 

419
00:23:55,401 --> 00:23:59,930
distribution into a continuous pdf 
defined by a mixture of Gaussians.

420
00:24:00,460 --> 00:24:01,293
And specifically you can think of each 
of these coushins as representing I 

421
00:24:03,831 --> 00:24:04,664
possible action that the human could 
take or that the car could take at that 

422
00:24:08,151 --> 00:24:09,650
instant,
right?

423
00:24:09,651 --> 00:24:13,340
So if there's a multimodal distribution 
like you can see here,

424
00:24:14,000 --> 00:24:14,833
so this has two modes.
This is corresponding to a situation 

425
00:24:18,231 --> 00:24:19,064
where the car can either turn left or 
right and we'd expect to coushins to 

426
00:24:22,490 --> 00:24:23,323
roughly appear from this distribution.
So the mean simply represents the 

427
00:24:27,561 --> 00:24:31,010
steering wheel angle that is most likely
or most confidence.

428
00:24:31,011 --> 00:24:34,490
So this is the angle that the car should
actually be traveling in for that mode.

429
00:24:35,150 --> 00:24:39,800
The variant simply reflects some sort of
uncertainty about that mean.

430
00:24:39,920 --> 00:24:40,753
So the larger the variance,
the more uncertain you are about 

431
00:24:42,681 --> 00:24:45,470
following that exact a steering wheel 
angle.

432
00:24:45,770 --> 00:24:46,603
And finally,
the power is simply just a scale and 

433
00:24:47,941 --> 00:24:51,230
multiple that we multiply it by each 
mode of our distribution.

434
00:24:51,500 --> 00:24:53,630
To simply scale up,
scale it up and down.

435
00:24:55,640 --> 00:24:56,473
And we do this transformation from 
discreet to continuous by using this 

436
00:24:59,031 --> 00:24:59,864
technique,
which is known as very rich variational 

437
00:25:01,341 --> 00:25:03,170
basion.
Next you're modeling.

438
00:25:03,230 --> 00:25:05,600
And if you've ever heard of expectation 
maximization,

439
00:25:06,080 --> 00:25:11,080
this is a technique that's very similar.
So an e m or expectation maximization,

440
00:25:11,420 --> 00:25:12,253
you have to actually predefine the 
number of modes that you expect your 

441
00:25:15,261 --> 00:25:16,094
distribution to have.
And this is really the difference with 

442
00:25:19,040 --> 00:25:19,873
variational Bayes,
is that you don't have to predefine the 

443
00:25:21,681 --> 00:25:24,710
number of modes.
You simply have to define an upper bound

444
00:25:24,890 --> 00:25:25,723
to the maximum number of modes that you 
would ever expect to encounter while 

445
00:25:28,281 --> 00:25:31,130
you're driving.
So for us,

446
00:25:31,131 --> 00:25:31,964
we use something like 15 modes.
We never expect to see more than 15 

447
00:25:34,881 --> 00:25:37,010
possible different actions to take on 
the road.

448
00:25:37,310 --> 00:25:38,143
So this is really an upper bound.
And then what happens is we feed in our 

449
00:25:40,341 --> 00:25:41,174
discrete distribution,
then iteratively we start to kill off 

450
00:25:45,050 --> 00:25:49,850
modes that lose that,
that lose power,

451
00:25:50,120 --> 00:25:50,953
right?
So I won't go into too much detail on 

452
00:25:52,641 --> 00:25:55,130
how this actually happens,
but this is really the,

453
00:25:55,570 --> 00:25:56,403
the amazing thing about variational 
bayes compared to something like 

454
00:25:59,150 --> 00:26:01,490
expectation maximization.
Um,

455
00:26:01,550 --> 00:26:03,530
and I encourage you all to like look up 
online.

456
00:26:03,531 --> 00:26:04,364
This is a very well known technique if 
you're interested in learning some of 

457
00:26:06,471 --> 00:26:07,670
the math behind the algorithm.

458
00:26:08,420 --> 00:26:09,253
Okay.

459
00:26:09,460 --> 00:26:14,280
Eventually we converge to the minimum 
number of mixtures that this,

460
00:26:14,800 --> 00:26:17,740
that this discrete distribution can 
actually be modeled by.

461
00:26:21,480 --> 00:26:22,313
So now we can combine the learning of 
our discrete distributions with 

462
00:26:24,811 --> 00:26:25,644
variational Bayes to directly take us 
and put a single image from our camera 

463
00:26:30,030 --> 00:26:30,863
and then predict a continuous pdf of 
where the drive will a control or the 

464
00:26:37,480 --> 00:26:40,140
where the control that we can execute to
the car.

465
00:26:41,330 --> 00:26:41,990
Yeah.

466
00:26:41,990 --> 00:26:44,150
So here,
here's an of um,

467
00:26:44,940 --> 00:26:45,250
okay.

468
00:26:45,250 --> 00:26:46,083
A scene where the cars turning left and 
you can see the extracted discrete 

469
00:26:49,751 --> 00:26:54,700
distribution is actually heavily focused
on the left part of the control.

470
00:26:55,330 --> 00:26:59,590
You can also see the continuous pdf 
overlaid on top of this graph as well as

471
00:26:59,591 --> 00:27:00,424
bounds.
I haven't talked to you about how these 

472
00:27:01,301 --> 00:27:03,670
bounds are computed yet,
but I'll get back to that very soon.

473
00:27:04,610 --> 00:27:05,443
Yeah.

474
00:27:05,700 --> 00:27:06,533
A very similar situation here is if,
if I again show you a union modal 

475
00:27:10,091 --> 00:27:10,924
action.
So this is still a situation with 

476
00:27:13,950 --> 00:27:15,630
unambiguous,
um,

477
00:27:15,850 --> 00:27:16,683
control setting,
but here the road has been slightly 

478
00:27:19,781 --> 00:27:20,614
widened.
So the variants has actually been 

479
00:27:21,791 --> 00:27:22,624
increased.

480
00:27:23,410 --> 00:27:23,680
Okay,

481
00:27:23,680 --> 00:27:25,420
right?
But this is actually not showing off the

482
00:27:25,421 --> 00:27:26,254
full capabilities of our approach 
because the whole reason that I 

483
00:27:28,930 --> 00:27:31,420
motivated this approach was to deal with
ambiguity,

484
00:27:31,450 --> 00:27:33,610
deal with uncertainty in the 
environment.

485
00:27:33,611 --> 00:27:35,530
So let's actually see something much 
more interesting.

486
00:27:35,620 --> 00:27:39,490
Let's go to an intersection.
So now at an intersection we can finally

487
00:27:39,491 --> 00:27:43,480
see multimodal distributions and 
multimodal peaks emerging.

488
00:27:44,290 --> 00:27:46,960
So at this,
at this intersection,

489
00:27:46,961 --> 00:27:47,794
this is a t intersection.
It's possible to either turn left or 

490
00:27:49,871 --> 00:27:54,190
right and you can see three modes 
emerging from this distribution.

491
00:27:55,420 --> 00:27:57,550
The three modes are as you'd expect left
and right.

492
00:27:57,551 --> 00:27:59,860
But also we see the straight mode also 
emerging.

493
00:28:00,250 --> 00:28:03,040
And if you look at what the human 
actually did into scenario,

494
00:28:03,460 --> 00:28:07,270
it was actually to go straight as well.
So at this instant,

495
00:28:07,300 --> 00:28:08,133
the control command was to go straight.
But then if you go forward two or three 

496
00:28:11,051 --> 00:28:11,884
friends,
you'll see that that straight actually 

497
00:28:12,701 --> 00:28:15,010
becomes a right.
So they were trying to just basically go

498
00:28:15,011 --> 00:28:15,844
around something and you can see that 
this is actually incredible because the 

499
00:28:18,101 --> 00:28:19,760
model has captured,
uh,

500
00:28:19,780 --> 00:28:20,613
picked up on this and actually captured 
that to actually turn right or it goes 

501
00:28:23,770 --> 00:28:26,080
left.
You actually have to go straight first.

502
00:28:28,900 --> 00:28:29,733
So now let's take an example of a,
let's take a look at two example clips 

503
00:28:33,101 --> 00:28:33,934
of the system in real life.
So first on the left is an example of 

504
00:28:40,541 --> 00:28:44,770
navigating intersections reliably 
without spuriously replanning so you can

505
00:28:44,771 --> 00:28:45,604
see many,
four way intersections and were able to 

506
00:28:48,071 --> 00:28:48,904
successfully navigate through those 
without any jittering or anything like 

507
00:28:52,631 --> 00:28:54,350
that.
And on the right you can see the vehicle

508
00:28:54,351 --> 00:28:55,184
successfully handling roads with huge 
cracks and grass and vegetation growing 

509
00:29:00,071 --> 00:29:00,904
on them.
So keep in mind that this network has 

510
00:29:02,381 --> 00:29:03,214
never seen these roads before.
It has only been trained on highway and 

511
00:29:06,941 --> 00:29:07,774
residential streets and we only find 
tuned at using one minute of data 

512
00:29:11,920 --> 00:29:13,150
without lane markers.

513
00:29:15,710 --> 00:29:19,130
So this is really exciting because it 
shows that we can control the car,

514
00:29:19,430 --> 00:29:20,263
even with minimal training data in the 
actual environment that we're trying to 

515
00:29:23,661 --> 00:29:24,494
test on.
We just have to do a little bit of fine 

516
00:29:25,701 --> 00:29:26,534
tuning.
And by actually predicting the 

517
00:29:28,971 --> 00:29:32,180
uncertainty with this,
we're able to get some robustness in our

518
00:29:32,181 --> 00:29:33,440
control signal as well.

519
00:29:34,210 --> 00:29:34,990
Okay.

520
00:29:34,990 --> 00:29:35,823
But in the first part of this talk,
I talked to you all about parallel 

521
00:29:37,750 --> 00:29:38,583
autonomy.
So what's the connection here with this 

522
00:29:40,641 --> 00:29:42,790
approach to care?
Parallel autonomy.

523
00:29:43,270 --> 00:29:45,130
So in addition to predicting the 
control,

524
00:29:45,160 --> 00:29:45,993
we also want to predict control bounds 
that we can really use and a parallel 

525
00:29:50,621 --> 00:29:51,454
autonomous setting.
So given the setup and the framework 

526
00:29:54,401 --> 00:29:58,030
that we've talked about so far,
this is actually extremely simple to do.

527
00:29:58,180 --> 00:30:00,790
So this is the beauty of setting it up,
setting the problem up.

528
00:30:00,791 --> 00:30:01,624
The way we have moving into the parallel
autonomy framework is now extremely 

529
00:30:05,411 --> 00:30:09,780
simple.
So what we can do is for each mixture of

530
00:30:09,781 --> 00:30:10,614
our continuous Gow Shin,
we simply have to take plus or minus 

531
00:30:13,861 --> 00:30:17,430
some scaled variance from the ideal 
control command or from the mean of that

532
00:30:17,431 --> 00:30:18,264
mode.
We add those bounds into some set of 

533
00:30:20,761 --> 00:30:21,594
valid control signals.
And once we have these set of valid 

534
00:30:25,021 --> 00:30:25,854
controls,
all we have to do is simply set up a 

535
00:30:28,950 --> 00:30:31,740
shared controller or shared parallel 
autonomy controller,

536
00:30:32,070 --> 00:30:34,980
which is just the human control.
If it falls within that bound.

537
00:30:35,940 --> 00:30:39,150
And if the human is trying to do 
something outside of the bounds,

538
00:30:40,580 --> 00:30:41,413
the system provides some restorative 
feedback to bring them back within the 

539
00:30:44,481 --> 00:30:45,314
bounce.

540
00:30:47,870 --> 00:30:52,040
So now in the next section of this talk,
I'd like to discuss this notion of model

541
00:30:52,041 --> 00:30:55,100
uncertainty.
When can we trust our models?

542
00:30:55,370 --> 00:30:56,203
Uh,
and this is really a huge question and 

543
00:30:58,160 --> 00:31:00,800
actually all of deep learning,
not just autonomous vehicles,

544
00:31:01,550 --> 00:31:02,383
um,
but we want to essentially accurately 

545
00:31:03,831 --> 00:31:07,700
determine or accurately verify that our 
model is,

546
00:31:08,030 --> 00:31:08,863
is confident in the action that it's 
telling us to take at that given 

547
00:31:11,271 --> 00:31:12,104
instance of time.
So I hope this notion of uncertainty is 

548
00:31:17,101 --> 00:31:20,100
pretty clear,
like the motivation of why we care about

549
00:31:20,101 --> 00:31:21,630
uncertainty.
It's pretty clear to all of you,

550
00:31:21,870 --> 00:31:25,530
especially and a talk about autonomous 
vehicles.

551
00:31:26,130 --> 00:31:27,000
So

552
00:31:28,170 --> 00:31:29,003
anonymous vehicles are one of the most 
safety critical robotic applications 

553
00:31:32,341 --> 00:31:37,341
where it is extremely important for us 
to prioritize safety in the systems that

554
00:31:39,421 --> 00:31:40,254
we're building.
So what we've learned so far is that 

555
00:31:42,661 --> 00:31:43,494
modeling probabilities for fixed classes
like we do for a discreet control 

556
00:31:46,711 --> 00:31:47,544
signals is actually different than 
looking at the uncertainty of the model 

557
00:31:51,271 --> 00:31:53,250
as a whole.
So take for example,

558
00:31:53,251 --> 00:31:54,084
if I want to train this model of cats 
and dogs just to recognize a picture of 

559
00:31:57,691 --> 00:31:59,190
a cat and a dog,
and I at the output,

560
00:31:59,191 --> 00:32:00,024
I'm predicting a softmax layer that 
represents probably being a cat 

561
00:32:02,400 --> 00:32:04,440
probability of being a dog.
Now,

562
00:32:04,441 --> 00:32:08,280
if I showed this model,
I picture of a cat,

563
00:32:08,310 --> 00:32:10,650
assuming that the model has been 
sufficiently trained,

564
00:32:10,920 --> 00:32:13,920
I'd expect that this model,
it gives me an answer of yes,

565
00:32:13,921 --> 00:32:14,754
this is definitely a cat.
And I expect that model to be very 

566
00:32:17,341 --> 00:32:19,350
confident in its answer,
right?

567
00:32:19,351 --> 00:32:21,030
So in this scenario,
they're the same thing,

568
00:32:21,031 --> 00:32:22,950
but now let's think about it's different
scenario.

569
00:32:23,400 --> 00:32:25,980
What if I feed that same model?
A picture of a horse,

570
00:32:27,180 --> 00:32:28,013
right?
So these two output probabilities are 

571
00:32:29,551 --> 00:32:31,410
still fixed.
So the model,

572
00:32:31,411 --> 00:32:35,070
we'll still output the probability of 
being a cat or the probability of dog.

573
00:32:35,880 --> 00:32:38,940
And by definition those output 
probabilities have to add up to,

574
00:32:39,290 --> 00:32:42,200
so by definition one of them has to be 
greater than 0.5.

575
00:32:43,200 --> 00:32:43,410
Yeah.

576
00:32:43,410 --> 00:32:44,243
Right.
So even though one of the probabilities 

577
00:32:45,241 --> 00:32:47,130
are very high,
it may seem,

578
00:32:48,120 --> 00:32:48,953
it actually does not mean that the model
is confident and predicting that this 

579
00:32:51,481 --> 00:32:53,580
horse is a dog.
And in fact,

580
00:32:53,581 --> 00:32:54,414
what we want to see is that the 
uncertainty of this model in this 

581
00:32:56,161 --> 00:33:00,010
scenario is very low.
And that's something that that motivates

582
00:33:00,011 --> 00:33:01,200
this,
this whole notion of,

583
00:33:01,890 --> 00:33:04,440
of uh,
uncertainty or confidence.

584
00:33:04,830 --> 00:33:05,663
And the key points that I want to hammer
home here is that neural network 

585
00:33:07,981 --> 00:33:08,814
probability is created through softmax 
layer is do not correspond to 

586
00:33:12,031 --> 00:33:12,864
confidence.
They don't measure uncertainty even 

587
00:33:14,731 --> 00:33:19,060
though many people actually sent,
makes the two terms entertained.

588
00:33:19,450 --> 00:33:20,910
Um,
interchangeable.

589
00:33:22,870 --> 00:33:23,200
Yeah.

590
00:33:23,200 --> 00:33:26,560
So the spring sent the question how we 
can actually determine model uncertainty

591
00:33:27,130 --> 00:33:29,950
and for this return to this really 
hockfield and deep learning right now,

592
00:33:29,951 --> 00:33:34,630
which is called Beige and deep learning.
And just listen to just as an example of

593
00:33:34,631 --> 00:33:37,990
the power of beige and deep learning.
Let's take,

594
00:33:37,991 --> 00:33:39,880
um,
let's take a look at this figure here.

595
00:33:39,881 --> 00:33:43,270
So this is a system that was built to 
take us input,

596
00:33:43,271 --> 00:33:44,104
a single image and output the predicted 
depth of every single pixel in that 

597
00:33:49,301 --> 00:33:50,134
image.

598
00:33:51,020 --> 00:33:51,710
Okay.

599
00:33:51,710 --> 00:33:52,543
So when we use beige and deep learning,
well we actually want to do is compute 

600
00:33:55,760 --> 00:33:58,850
this very,
very rich uncertainty estimates for this

601
00:33:58,851 --> 00:33:59,684
neural network.
And we want to say for every single 

602
00:34:00,861 --> 00:34:01,694
pixel,
what is the confidence in that 

603
00:34:03,591 --> 00:34:04,424
prediction and beige and deep learning 
really allows us to do this in a 

604
00:34:07,851 --> 00:34:08,684
systematic manner.
And it's really remarkable because we 

605
00:34:12,561 --> 00:34:15,710
can see some really interesting things 
that start to arise.

606
00:34:15,980 --> 00:34:16,813
So you can see that the model is most 
uncertain around like the edges of the 

607
00:34:19,041 --> 00:34:19,874
car is also the shadows underneath the 
cars and even the reflections and the 

608
00:34:23,931 --> 00:34:25,340
windows.
Because then the reflections,

609
00:34:25,341 --> 00:34:27,380
you actually see trees that are far 
away.

610
00:34:27,381 --> 00:34:31,250
So the model is able to capture some 
very natural sense of uncertainty.

611
00:34:32,940 --> 00:34:33,220
Yeah.

612
00:34:33,220 --> 00:34:37,180
So now the question becomes how can we 
apply these techniques to the end?

613
00:34:37,181 --> 00:34:39,520
To end driving scenario for self driving
cars.

614
00:34:40,030 --> 00:34:42,730
So let's revisit the original end to end
pipeline.

615
00:34:43,390 --> 00:34:46,270
So this is actually a different pipeline
that I talked about in the first talk.

616
00:34:46,271 --> 00:34:47,104
This is an even simpler ones.
So this is basically the pipeline that 

617
00:34:49,361 --> 00:34:51,280
Nvidia has created for their autonomous 
vehicles.

618
00:34:51,281 --> 00:34:52,114
So let's just taking it in a single 
image and predicting just a single 

619
00:34:54,221 --> 00:34:55,054
control value.
There is no notion of probability 

620
00:34:56,411 --> 00:34:58,090
distributions like we talked about 
before.

621
00:34:58,300 --> 00:34:59,920
I'm just predicting a single real 
number.

622
00:35:00,700 --> 00:35:02,800
We've already discussed the issues with 
this approach.

623
00:35:02,830 --> 00:35:06,910
We lack uncertainty,
we lack ambiguous scenario handling.

624
00:35:09,430 --> 00:35:10,263
So instead,
what if we can maintain this 

625
00:35:11,951 --> 00:35:15,280
architecture,
but we still want to create some sort of

626
00:35:15,281 --> 00:35:17,530
uncertainty.
So we can recently,

627
00:35:17,620 --> 00:35:18,453
we can actually reason about the 
confidence of our model with that 

628
00:35:21,611 --> 00:35:24,550
control.
And the way we can do that is actually,

629
00:35:25,220 --> 00:35:26,053
uh,
by stochastically sampling over our 

630
00:35:27,941 --> 00:35:31,510
output to deterministically,
um,

631
00:35:32,020 --> 00:35:33,700
sorry to deterministic,
sorry.

632
00:35:33,880 --> 00:35:34,713
We want to sample over are deterministic
control signal to actually obtain an 

633
00:35:38,581 --> 00:35:42,150
expectation and variants of that 
instance.

634
00:35:42,151 --> 00:35:42,984
So this will give us the expectation is 
essentially the control that we want to 

635
00:35:45,331 --> 00:35:46,164
take.
And the variance is essentially the 

636
00:35:48,120 --> 00:35:49,740
uncertainty of the model at that 
instant.

637
00:35:50,770 --> 00:35:51,420
Okay.

638
00:35:51,420 --> 00:35:52,253
So more specifically,
we can do this using patient deep 

639
00:35:53,701 --> 00:35:54,660
learning,
like I said,

640
00:35:55,230 --> 00:35:56,063
and

641
00:35:56,600 --> 00:35:56,830
okay,

642
00:35:56,830 --> 00:35:59,080
to understand this,
let's actually reformulate our problem a

643
00:35:59,081 --> 00:36:02,200
little bit using a Bayesian state of 
mind.

644
00:36:02,410 --> 00:36:05,200
So this is a little different than the 
way we train our models,

645
00:36:05,930 --> 00:36:07,300
uh,
in the previous setting.

646
00:36:07,301 --> 00:36:09,940
So now in the Batian deep learning 
framework,

647
00:36:09,941 --> 00:36:10,930
we want to actually,

648
00:36:12,340 --> 00:36:12,760
okay,

649
00:36:12,760 --> 00:36:14,740
like I said,
have that patient's state of mind.

650
00:36:15,580 --> 00:36:16,413
So the goal here,
like before we're trying to create a 

651
00:36:18,161 --> 00:36:21,100
network to learn steering control 
directly from raw data.

652
00:36:21,790 --> 00:36:22,623
We're going to find that functional 
napping and we want to do it by 

653
00:36:24,101 --> 00:36:24,934
minimizing some empirical loss.
The different series that invasion 

654
00:36:28,630 --> 00:36:29,463
neural networks were actually attempting
to learn this posterior Oh far awaits 

655
00:36:33,160 --> 00:36:35,290
given our data.
So another,

656
00:36:35,440 --> 00:36:36,273
another way to say this is that we want 
to understand the likelihood of 

657
00:36:38,831 --> 00:36:39,664
observing some model.
Given the data that we see now based on 

658
00:36:44,471 --> 00:36:46,780
neural networks are actually called 
Beige and neural networks.

659
00:36:46,781 --> 00:36:50,530
Because we can rewrite this posterior,
this one,

660
00:36:51,300 --> 00:36:53,110
uh,
using Bayes rule,

661
00:36:53,620 --> 00:36:54,453
right?
And actually we can theoretically solve 

662
00:36:56,861 --> 00:36:57,694
it using this equation.
The problem is that in practice this is 

663
00:37:00,461 --> 00:37:02,770
rarely possible to compute and 
politically,

664
00:37:03,400 --> 00:37:04,233
uh,
so instead we have to resort to some 

665
00:37:05,081 --> 00:37:08,170
sort of sampling techniques to 
approximate it instead.

666
00:37:09,170 --> 00:37:10,000
Okay,

667
00:37:10,000 --> 00:37:13,390
so let's look at what this might look 
like for our network.

668
00:37:13,391 --> 00:37:14,224
With convolutional layers,
we start by performing t stochastic 

669
00:37:17,621 --> 00:37:18,454
samples through our network forward 
passes through our network and at each 

670
00:37:21,161 --> 00:37:21,994
time we're essentially sampling each of 
the weights of Feta according to a 

671
00:37:25,751 --> 00:37:26,584
Bernoulli distribution.
An element wise multiplied that 

672
00:37:29,291 --> 00:37:32,740
Bernoulli mask with the unregular eyes 
feature maps.

673
00:37:33,660 --> 00:37:34,350
Yeah,

674
00:37:34,350 --> 00:37:35,183
so on the bottom left you can see in the
illustration of three on regularized 

675
00:37:38,640 --> 00:37:41,040
convolutional kernels,
right?

676
00:37:42,630 --> 00:37:43,463
You can also see next to them the 
element that they are element wise 

677
00:37:46,171 --> 00:37:47,004
multiplied by masks of ones and Zeros,
which are just drawn from a Bernoulli 

678
00:37:51,241 --> 00:37:56,241
random variable Bernoulli distribution.
And this process is effectively repeated

679
00:37:56,371 --> 00:37:58,920
capital t times.
You repeat this tee times,

680
00:37:58,921 --> 00:38:01,910
you get an estimate of your expectation 
of your output and you get an x,

681
00:38:01,980 --> 00:38:02,813
you get a variance estimation as well.
Now the variance estimation is exactly 

682
00:38:06,241 --> 00:38:08,520
what we're looking for as part of our 
uncertainty estimates.

683
00:38:08,970 --> 00:38:12,270
But there's actually a problem here in 
terms of convolutional layers.

684
00:38:12,810 --> 00:38:13,643
So the original theory for Beige and 
deep learning didn't take into account 

685
00:38:16,890 --> 00:38:17,723
this huge spatial correlation between 
adjacent pixels in early convolutional 

686
00:38:22,051 --> 00:38:22,884
layers.
So especially in like I said in earlier 

687
00:38:26,611 --> 00:38:27,444
convolutional layers,
you're dealing with things like edges 

688
00:38:29,281 --> 00:38:32,280
and corners and things like that.

689
00:38:32,310 --> 00:38:35,220
And there are massive amounts of spatial
correlation,

690
00:38:35,530 --> 00:38:38,110
the pixels.
So when we apply our Bernoulli mask,

691
00:38:38,800 --> 00:38:39,633
we may actually lose a lot of valuable 
information and this may cause our 

692
00:38:42,821 --> 00:38:46,060
output to change undesirably.
So how do we,

693
00:38:46,180 --> 00:38:49,150
how do we handle this?
So let's make a small change and instead

694
00:38:49,151 --> 00:38:51,700
of sampling Bernoulli random variables 
over each,

695
00:38:51,701 --> 00:38:52,534
wait,
let's sample Bernoulli random variables 

696
00:38:54,580 --> 00:38:58,000
over each kernel itself.
So over the entire kernel,

697
00:38:58,001 --> 00:39:01,150
we'll call this colonel either a one or 
a zero and one element wise,

698
00:39:01,151 --> 00:39:05,020
multiply our unregular as feature maps 
by this new,

699
00:39:05,680 --> 00:39:09,100
um,
Bernoulli drop out mask.

700
00:39:09,940 --> 00:39:14,940
So effectively by dropping gout,
entire kernels in early layers,

701
00:39:15,191 --> 00:39:16,024
you,
you'd probably know that kernels 

702
00:39:16,751 --> 00:39:20,320
represent things like line detection,
oriented line detections.

703
00:39:20,380 --> 00:39:21,213
And things like that.
So effectively by dropping out some of 

704
00:39:22,991 --> 00:39:27,550
these kernels and keeping others were 
getting like a noisy estimate of,

705
00:39:28,180 --> 00:39:29,013
of for example,
lane detection algorithm if we're only 

706
00:39:30,971 --> 00:39:33,370
dropping out like oriented line 
detectors.

707
00:39:33,670 --> 00:39:35,980
And this is exactly what we implemented 
in our algorithm.

708
00:39:37,480 --> 00:39:38,313
So we found that stochastically dropping
out kernels as opposed to elements 

709
00:39:42,850 --> 00:39:43,683
actually resulted in both accelerated 
training as well as better convergence 

710
00:39:46,961 --> 00:39:47,794
of our end to end algorithms.
We performed a precision analysis to 

711
00:39:51,761 --> 00:39:55,190
actually analyze how precise our models 
come,

712
00:39:55,420 --> 00:39:57,460
uh,
work compared to the ground truth,

713
00:39:57,461 --> 00:39:58,294
human actions.
And finally we studied how the 

714
00:40:00,941 --> 00:40:01,774
uncertainty of our model changed as a 
function of the steering wheel angle or 

715
00:40:04,841 --> 00:40:06,930
the steering control,
uh,

716
00:40:06,970 --> 00:40:11,970
as well as the quantity of training data
for that specific type of control.

717
00:40:14,410 --> 00:40:18,910
So finally we found that the uncertainty
increased as we,

718
00:40:19,440 --> 00:40:22,150
um,
tried to make more dynamic turns,

719
00:40:22,151 --> 00:40:22,984
which you might expect because when 
you're trying to make a more dynamic 

720
00:40:24,581 --> 00:40:25,414
turn,
you actually have less of that scene in 

721
00:40:27,761 --> 00:40:30,400
your field of view.
So if you're making a very dynamic right

722
00:40:30,401 --> 00:40:33,400
turn and your cameras pointing straight 
in front of you,

723
00:40:33,580 --> 00:40:34,720
you actually can't see the road.

724
00:40:34,720 --> 00:40:36,700
You can only see maybe 10 meters in 
front of you.

725
00:40:36,940 --> 00:40:38,950
Whereas if you're going straight on a 
straight highway,

726
00:40:39,100 --> 00:40:41,830
you can see maybe even a hundred meters 
in front of you.

727
00:40:42,520 --> 00:40:44,800
So this really leads to this nice 
property,

728
00:40:44,801 --> 00:40:49,300
which we found was we have a more 
uncertain model as we tried to make more

729
00:40:49,301 --> 00:40:50,134
extreme turns is also aligns with the 
fact that the vast amount of training 

730
00:40:55,391 --> 00:40:56,224
data comes on straight roads.
So you'd expect that straight road 

731
00:40:58,781 --> 00:41:02,650
driving is the most confident that the 
model can,

732
00:41:02,730 --> 00:41:07,730
can get can be.
So now I'd like to take a step back,

733
00:41:07,871 --> 00:41:08,704
summarize some of the findings of this 
talk and tie things back with parallel 

734
00:41:11,921 --> 00:41:12,850
autonomy.
Once again.

735
00:41:14,350 --> 00:41:15,183
So we started this talk by exploring one
of the control algorithms that we've 

736
00:41:18,101 --> 00:41:18,934
created to actually predict the control 
of an autonomous vehicle and not only 

737
00:41:25,631 --> 00:41:26,464
predict the control,
but also predict the control bounce and 

738
00:41:28,271 --> 00:41:29,740
entirely end to end manner.

739
00:41:30,670 --> 00:41:33,040
We showed how we can officially handle 
ambiguity,

740
00:41:33,260 --> 00:41:34,093
this approach,
and we explored ways that we can use 

741
00:41:36,171 --> 00:41:37,004
techniques from beige and deep learning 
to actually get a sense of when our 

742
00:41:38,811 --> 00:41:42,590
model is and is not confident about its 
prediction.

743
00:41:43,760 --> 00:41:44,593
Finally,
I'd like to show how all this ties in 

744
00:41:45,741 --> 00:41:46,574
with a cool demonstration of parallel 
autonomy on a full scale autonomous 

745
00:41:49,671 --> 00:41:50,504
vehicle,
specifically demonstrating guardian 

746
00:41:51,981 --> 00:41:56,540
angel capabilities.
So here the human is controlling the car

747
00:41:56,840 --> 00:41:59,810
and as long as he adheres to the balance
of the learned model,

748
00:42:00,200 --> 00:42:01,670
it appears as though nothing is 
happening.

749
00:42:01,760 --> 00:42:02,593
But if he tries to drive off the road,
you'll see in a second the system will 

750
00:42:05,691 --> 00:42:09,530
see that he's exited the balance and 
tried to provide restorative feedback.

751
00:42:09,531 --> 00:42:10,364
Like what just happened there.
You'll see another example where the 

752
00:42:12,231 --> 00:42:13,064
human tries to even more aggressively 
tried to drive off the left of the road 

753
00:42:15,950 --> 00:42:16,783
where the model will respond even more 
aggressively to bring the car back onto 

754
00:42:21,741 --> 00:42:22,574
the road.

755
00:42:25,770 --> 00:42:29,250
So we hope that this kind of parallel 
autonomy way of thinking,

756
00:42:30,060 --> 00:42:31,770
uh,
brings about like this,

757
00:42:31,800 --> 00:42:36,800
this new realm of autonomous driving 
specifically of like this Guardian Angel

758
00:42:37,740 --> 00:42:38,940
Capability.
So the goal,

759
00:42:38,970 --> 00:42:39,803
the ultimate goal of our work is to 
create a vehicle that even if you want 

760
00:42:43,681 --> 00:42:46,260
to crash,
it is uncrackable,

761
00:42:46,380 --> 00:42:47,213
right?
So it will prohibit you from making any 

762
00:42:48,511 --> 00:42:53,511
dangerous actions that exit the balance 
of the set of valid controls,

763
00:42:53,731 --> 00:42:54,990
right?
So this is,

764
00:42:54,991 --> 00:42:57,870
this is really the holy grail of the 
work that we're doing.

765
00:42:58,410 --> 00:43:01,590
And I'd like to conclude this talk by 
actually acknowledging all of my,

766
00:43:02,270 --> 00:43:04,050
uh,
incredible lab at Mit,

767
00:43:04,350 --> 00:43:05,183
all of my collaborators.
If you're interested in reading up on 

768
00:43:07,051 --> 00:43:09,960
any of the references that I've 
presented as part of this talk,

769
00:43:09,961 --> 00:43:13,050
there's a link right here where you can 
find all of the references.

770
00:43:13,760 --> 00:43:14,760
Um,
and that's it.

771
00:43:14,761 --> 00:43:15,594
Thank you.

