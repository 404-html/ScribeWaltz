WEBVTT

1
00:00:03.110 --> 00:00:04.940
<v Speaker 1>Thanks d dot.</v>
<v Speaker 1>So I'm,</v>

2
00:00:05.480 --> 00:00:07.000
<v Speaker 1>I'm punching and uh,</v>
<v Speaker 1>um,</v>

3
00:00:07.050 --> 00:00:07.581
<v Speaker 1>like the,</v>
<v Speaker 1>um,</v>

4
00:00:07.581 --> 00:00:10.020
<v Speaker 1>also adds a Google Cambridge office and </v>
<v Speaker 1>uh,</v>

5
00:00:10.100 --> 00:00:10.933
<v Speaker 1>I'm one of the authors of tensorflow.</v>
<v Speaker 1>So we're going to change topic and talk </v>

6
00:00:13.101 --> 00:00:14.630
<v Speaker 1>about tensorflow.</v>
<v Speaker 1>So,</v>

7
00:00:14.631 --> 00:00:16.530
<v Speaker 1>um,</v>
<v Speaker 1>I read your course material a little bit</v>

8
00:00:16.550 --> 00:00:19.820
<v Speaker 1>and I gathered as you all use tensorflow</v>
<v Speaker 1>in previous parts of the lecture.</v>

9
00:00:19.821 --> 00:00:21.230
<v Speaker 1>Is that right?</v>
<v Speaker 1>Great.</v>

10
00:00:22.340 --> 00:00:23.173
<v Speaker 1>Okay.</v>
<v Speaker 1>So my focus will be how to more </v>

11
00:00:25.941 --> 00:00:26.840
<v Speaker 1>efficiently,</v>
<v Speaker 1>right,</v>

12
00:00:26.870 --> 00:00:29.030
<v Speaker 1>and debulk it,</v>
<v Speaker 1>machine learning models in tensorflow.</v>

13
00:00:29.900 --> 00:00:33.560
<v Speaker 1>So the question is whether you need to </v>
<v Speaker 1>debug a machine learning model.</v>

14
00:00:34.220 --> 00:00:35.053
<v Speaker 1>I think the answer is yes,</v>
<v Speaker 1>of course machine learning models are </v>

15
00:00:36.831 --> 00:00:38.600
<v Speaker 1>very different from traditional </v>
<v Speaker 1>programs,</v>

16
00:00:38.990 --> 00:00:43.990
<v Speaker 1>but their software and their code and </v>
<v Speaker 1>the software and they will have bugs and</v>

17
00:00:44.110 --> 00:00:44.943
<v Speaker 1>the,</v>
<v Speaker 1>you need to debug your models from time </v>

18
00:00:45.771 --> 00:00:48.350
<v Speaker 1>to time.</v>
<v Speaker 1>So hopefully after this lecture you will</v>

19
00:00:48.351 --> 00:00:51.650
<v Speaker 1>know a little bit more about how to more</v>
<v Speaker 1>efficiently devalue our machine learning</v>

20
00:00:51.920 --> 00:00:52.910
<v Speaker 1>models in tensorflow.</v>

21
00:00:54.230 --> 00:00:55.063
<v Speaker 1>So before we dive into debugging,</v>
<v Speaker 1>I want to talk about how machine </v>

22
00:00:58.131 --> 00:00:58.964
<v Speaker 1>learning models are represented in,</v>
<v Speaker 1>in the computer because that turns out </v>

23
00:01:02.961 --> 00:01:05.810
<v Speaker 1>to be important for how you write and </v>
<v Speaker 1>develop programs.</v>

24
00:01:06.530 --> 00:01:10.100
<v Speaker 1>So there are two ways in which a machine</v>
<v Speaker 1>learning model can be represented.</v>

25
00:01:10.580 --> 00:01:12.560
<v Speaker 1>So it's either a data structure or </v>
<v Speaker 1>program.</v>

26
00:01:13.340 --> 00:01:16.700
<v Speaker 1>So if it's a data structure,</v>
<v Speaker 1>then when you write code to,</v>

27
00:01:16.730 --> 00:01:19.210
<v Speaker 1>for example,</v>
<v Speaker 1>define a layer of neural network,</v>

28
00:01:19.760 --> 00:01:22.640
<v Speaker 1>you're actually building our graph and </v>
<v Speaker 1>those lines of code,</v>

29
00:01:22.670 --> 00:01:25.070
<v Speaker 1>when they're executed,</v>
<v Speaker 1>they don't actually do the competition,</v>

30
00:01:25.071 --> 00:01:25.904
<v Speaker 1>they're just building the graph and the </v>
<v Speaker 1>graph needs to be fed into some kind of </v>

31
00:01:29.031 --> 00:01:32.600
<v Speaker 1>machinery because I'm kind of execution </v>
<v Speaker 1>engine to actually run the model.</v>

32
00:01:33.140 --> 00:01:33.973
<v Speaker 1>And the second way in which you can </v>
<v Speaker 1>define a machine and the motto is to </v>

33
00:01:36.230 --> 00:01:38.300
<v Speaker 1>write it as a program.</v>
<v Speaker 1>So that's more straightforward.</v>

34
00:01:38.570 --> 00:01:39.403
<v Speaker 1>So those kinds of lines of code will </v>
<v Speaker 1>actually do the competition on either </v>

35
00:01:42.471 --> 00:01:45.710
<v Speaker 1>the CPU or Gpu depending on whether you </v>
<v Speaker 1>have a gpu or not.</v>

36
00:01:46.680 --> 00:01:47.513
<v Speaker 1>Um,</v>
<v Speaker 1>so the first paradigm is also called </v>

37
00:01:49.490 --> 00:01:51.650
<v Speaker 1>symbolic execution or different </v>
<v Speaker 1>execution.</v>

38
00:01:52.100 --> 00:01:54.020
<v Speaker 1>And the second one is also called I'm </v>
<v Speaker 1>eager,</v>

39
00:01:54.021 --> 00:01:54.854
<v Speaker 1>execution or imperative execution.</v>
<v Speaker 1>So I'm now the question for you is </v>

40
00:01:58.311 --> 00:02:01.070
<v Speaker 1>whether tensorflow is the first paradigm</v>
<v Speaker 1>or the Second Paradigm?</v>

41
00:02:03.550 --> 00:02:04.820
<v Speaker 1>So I heard someone said first,</v>

42
00:02:06.960 --> 00:02:07.793
<v Speaker 2>second,</v>

43
00:02:08.870 --> 00:02:09.351
<v Speaker 1>both.</v>
<v Speaker 1>Yeah.</v>

44
00:02:09.351 --> 00:02:10.960
<v Speaker 1>So I think it's a trick question,</v>
<v Speaker 1>right?</v>

45
00:02:10.961 --> 00:02:11.794
<v Speaker 1>So the answer is both.</v>
<v Speaker 1>So if you ask the question like half a </v>

46
00:02:14.900 --> 00:02:16.370
<v Speaker 1>year ago,</v>
<v Speaker 1>then the answer will be only the first.</v>

47
00:02:16.730 --> 00:02:20.090
<v Speaker 1>But in the latest version of tensorflow,</v>
<v Speaker 1>we support both modes.</v>

48
00:02:20.140 --> 00:02:20.973
<v Speaker 1>Um,</v>
<v Speaker 1>and I'm going to give you some examples </v>

49
00:02:21.681 --> 00:02:26.270
<v Speaker 1>in the following slides.</v>
<v Speaker 1>So by default tends to flow is the first</v>

50
00:02:26.271 --> 00:02:27.930
<v Speaker 1>node,</v>
<v Speaker 1>so that's the classical,</v>

51
00:02:27.950 --> 00:02:31.610
<v Speaker 1>um,</v>
<v Speaker 1>traditional tensorflow style.</v>

52
00:02:31.940 --> 00:02:32.773
<v Speaker 1>So,</v>
<v Speaker 1>um,</v>

53
00:02:32.780 --> 00:02:36.590
<v Speaker 1>just to give you a refresher of how to </v>
<v Speaker 1>use tensorflow to define a simple model,</v>

54
00:02:36.620 --> 00:02:40.820
<v Speaker 1>you import the tensorflow stf and then </v>
<v Speaker 1>you defined some constants or maybe some</v>

55
00:02:40.821 --> 00:02:43.940
<v Speaker 1>variables as inputs.</v>
<v Speaker 1>And then you write a line to say like,</v>

56
00:02:43.941 --> 00:02:44.774
<v Speaker 1>you want to multiply x and w and then </v>
<v Speaker 1>you want to add the results of the </v>

57
00:02:48.111 --> 00:02:49.590
<v Speaker 1>multiple patient to,</v>
<v Speaker 1>um,</v>

58
00:02:49.640 --> 00:02:50.473
<v Speaker 1>to another thing be right.</v>
<v Speaker 1>So you can think of this as a very </v>

59
00:02:52.341 --> 00:02:54.260
<v Speaker 1>simple linear regression model,</v>
<v Speaker 1>if you will.</v>

60
00:02:55.310 --> 00:02:56.143
<v Speaker 1>Now,</v>
<v Speaker 1>the important thing here is when this </v>

61
00:02:57.231 --> 00:03:01.990
<v Speaker 1>line is executed,</v>
<v Speaker 1>it's actually not doing the computation.</v>

62
00:03:02.020 --> 00:03:04.600
<v Speaker 1>So the multiplication will not happen at</v>
<v Speaker 1>this point.</v>

63
00:03:04.630 --> 00:03:06.970
<v Speaker 1>If we print the results of this line,</v>
<v Speaker 1>why?</v>

64
00:03:07.290 --> 00:03:10.360
<v Speaker 1>Then you will see it's not 40,</v>
<v Speaker 1>it's not 10 times four equals 40.</v>

65
00:03:10.630 --> 00:03:11.650
<v Speaker 1>Instead it's like,</v>
<v Speaker 1>um,</v>

66
00:03:11.651 --> 00:03:12.484
<v Speaker 1>it's like a abstract symbolic thing.</v>
<v Speaker 1>So it's called a tenser and it knows </v>

67
00:03:16.481 --> 00:03:17.314
<v Speaker 1>what kind of operation it needs to do </v>
<v Speaker 1>when it's actually executed in the </v>

68
00:03:19.151 --> 00:03:21.910
<v Speaker 1>future.</v>
<v Speaker 1>So mall is that operation.</v>

69
00:03:22.270 --> 00:03:23.103
<v Speaker 1>It also needs an information about it </v>
<v Speaker 1>also knows information about like what </v>

70
00:03:26.140 --> 00:03:29.230
<v Speaker 1>dependencies are,</v>
<v Speaker 1>which are x and w in this case,</v>

71
00:03:30.070 --> 00:03:32.260
<v Speaker 1>but it's not shown in the printed </v>
<v Speaker 1>message here.</v>

72
00:03:32.290 --> 00:03:34.880
<v Speaker 1>And likewise,</v>
<v Speaker 1>when you do Tfa ad,</v>

73
00:03:34.930 --> 00:03:37.840
<v Speaker 1>when that line of code is executed,</v>
<v Speaker 1>the addition will not happen.</v>

74
00:03:37.841 --> 00:03:39.940
<v Speaker 1>It's going to happen later.</v>
<v Speaker 1>So by later,</v>

75
00:03:39.941 --> 00:03:41.630
<v Speaker 1>I mean a case,</v>
<v Speaker 1>uh,</v>

76
00:03:41.860 --> 00:03:45.490
<v Speaker 1>the points at which you create a session</v>
<v Speaker 1>by calling tf that session.</v>

77
00:03:45.850 --> 00:03:47.500
<v Speaker 1>And when you have that session is </v>
<v Speaker 1>created,</v>

78
00:03:47.740 --> 00:03:50.040
<v Speaker 1>it will basically automatically pull in </v>
<v Speaker 1>the graph.</v>

79
00:03:50.050 --> 00:03:54.640
<v Speaker 1>You have not already built in the </v>
<v Speaker 1>previous lines of code and then you tell</v>

80
00:03:54.641 --> 00:03:55.474
<v Speaker 1>the session like which tensor,</v>
<v Speaker 1>which abstract symbol in the graph you </v>

81
00:03:57.971 --> 00:04:00.940
<v Speaker 1>want to execute,</v>
<v Speaker 1>and then it's going to basically analyze</v>

82
00:04:00.941 --> 00:04:04.440
<v Speaker 1>the structure of the graph,</v>
<v Speaker 1>sort out all the dependencies and topple</v>

83
00:04:04.760 --> 00:04:08.230
<v Speaker 1>logically execute other nodes in the </v>
<v Speaker 1>graph to do the multiplication first and</v>

84
00:04:08.231 --> 00:04:11.920
<v Speaker 1>then do the audition next and it's going</v>
<v Speaker 1>to give you the final results,</v>

85
00:04:11.921 --> 00:04:15.880
<v Speaker 1>which is 42 so it can think of tfs </v>
<v Speaker 1>session as a Ng.</v>

86
00:04:16.120 --> 00:04:19.090
<v Speaker 1>So it's going to run the model on CPU.</v>
<v Speaker 1>If you only have a CPU,</v>

87
00:04:19.120 --> 00:04:20.830
<v Speaker 1>it's going to run the model on Gpu if </v>
<v Speaker 1>you have</v>

88
00:04:21.430 --> 00:04:22.263
<v Speaker 2>GPU.</v>

89
00:04:25.000 --> 00:04:25.833
<v Speaker 1>Now obviously I'm this paradigm of </v>
<v Speaker 1>defining our model is not the most </v>

90
00:04:30.060 --> 00:04:33.010
<v Speaker 1>straightforward because those lines of </v>
<v Speaker 1>codes that looked like doing competition</v>

91
00:04:33.011 --> 00:04:33.844
<v Speaker 1>is not doing any actual competition.</v>
<v Speaker 1>And you need to learn a new api called </v>

92
00:04:37.650 --> 00:04:41.020
<v Speaker 1>TF session.</v>
<v Speaker 1>So why does tensorflow do it this way?</v>

93
00:04:42.630 --> 00:04:46.260
<v Speaker 1>So obviously it's because there are some</v>
<v Speaker 1>advantages you can get.</v>

94
00:04:46.560 --> 00:04:49.230
<v Speaker 1>So the first advantage is because the </v>
<v Speaker 1>model is a data structure,</v>

95
00:04:49.500 --> 00:04:51.890
<v Speaker 1>it's relatively easy to serialize this.</v>
<v Speaker 1>Um,</v>

96
00:04:52.050 --> 00:04:52.883
<v Speaker 1>and then these airlines,</v>
<v Speaker 1>there's somewhere else you can train a </v>

97
00:04:54.841 --> 00:04:55.674
<v Speaker 1>model and then you can load your model </v>
<v Speaker 1>onto some kind of other devices like </v>

98
00:04:58.711 --> 00:05:03.711
<v Speaker 1>mobile devices or embedded devices like </v>
<v Speaker 1>raspberry pie or a car or robots and you</v>

99
00:05:04.741 --> 00:05:05.574
<v Speaker 1>can also,</v>
<v Speaker 1>um,</v>

100
00:05:05.580 --> 00:05:06.413
<v Speaker 1>sterilize the motto and then load the </v>
<v Speaker 1>model on like a faster hardware like </v>

101
00:05:09.720 --> 00:05:11.320
<v Speaker 1>Google's tpu.</v>
<v Speaker 1>Um,</v>

102
00:05:11.370 --> 00:05:12.203
<v Speaker 1>so these things are hard,</v>
<v Speaker 1>hard to do if your model is a python </v>

103
00:05:14.341 --> 00:05:15.174
<v Speaker 1>right,</v>
<v Speaker 1>if your model is a python program </v>

104
00:05:15.721 --> 00:05:18.210
<v Speaker 1>because those devices may not have </v>
<v Speaker 1>python and running them.</v>

105
00:05:18.660 --> 00:05:21.280
<v Speaker 1>And even if those devices have python </v>
<v Speaker 1>running on,</v>

106
00:05:21.340 --> 00:05:23.760
<v Speaker 1>on them,</v>
<v Speaker 1>that's probably not what you want to use</v>

107
00:05:23.970 --> 00:05:28.970
<v Speaker 1>because python is slow sometimes.</v>
<v Speaker 1>So I have those links here in the slide.</v>

108
00:05:29.700 --> 00:05:30.533
<v Speaker 1>So I'm going to send those slides to the</v>
<v Speaker 1>course organizers and you can click on </v>

109
00:05:34.141 --> 00:05:34.974
<v Speaker 1>those links if you're interested in any </v>
<v Speaker 1>of those topics like deployments on </v>

110
00:05:37.160 --> 00:05:38.190
<v Speaker 1>mobile devices and so on.</v>

111
00:05:41.380 --> 00:05:45.210
<v Speaker 1>So the next advantage is because your </v>
<v Speaker 1>model is a data structure,</v>

112
00:05:45.220 --> 00:05:48.070
<v Speaker 1>you are not tied down to the language in</v>
<v Speaker 1>which the model is defined.</v>

113
00:05:48.400 --> 00:05:50.350
<v Speaker 1>So nowadays,</v>
<v Speaker 1>most machine learning models are written</v>

114
00:05:50.351 --> 00:05:51.184
<v Speaker 1>in python,</v>
<v Speaker 1>but maybe your application server or </v>

115
00:05:53.281 --> 00:05:54.114
<v Speaker 1>maybe a web server is running Java or c </v>
<v Speaker 1>plus plus and you don't want to rewrite </v>

116
00:05:57.180 --> 00:05:58.013
<v Speaker 1>the whole stack in python just to be </v>
<v Speaker 1>able to add some machine learning to </v>

117
00:06:01.471 --> 00:06:02.251
<v Speaker 1>your stack,</v>
<v Speaker 1>right?</v>

118
00:06:02.251 --> 00:06:03.030
<v Speaker 1>So,</v>
<v Speaker 1>um,</v>

119
00:06:03.030 --> 00:06:03.863
<v Speaker 1>if a model is a data structure that you </v>
<v Speaker 1>can save the model after training and </v>

120
00:06:06.211 --> 00:06:07.044
<v Speaker 1>they can load it into Java or c plus </v>
<v Speaker 1>plus or c sharp or any of the supported </v>

121
00:06:10.980 --> 00:06:11.813
<v Speaker 1>languages and that you will be ready to </v>
<v Speaker 1>serve the trained model from your web </v>

122
00:06:14.761 --> 00:06:15.900
<v Speaker 1>server or application server.</v>

123
00:06:17.910 --> 00:06:21.570
<v Speaker 1>And the other nice thing about </v>
<v Speaker 1>representing data as the model,</v>

124
00:06:21.571 --> 00:06:22.404
<v Speaker 1>as a data structure,</v>
<v Speaker 1>as you can distribute the model very </v>

125
00:06:24.841 --> 00:06:27.150
<v Speaker 1>easily onto a number of machines called </v>
<v Speaker 1>workers.</v>

126
00:06:27.540 --> 00:06:30.180
<v Speaker 1>And those workers will basically use the</v>
<v Speaker 1>same graph.</v>

127
00:06:30.240 --> 00:06:34.110
<v Speaker 1>They're going to do the exact same </v>
<v Speaker 1>competition,</v>

128
00:06:34.140 --> 00:06:36.540
<v Speaker 1>but they are going to do it on different</v>
<v Speaker 1>slices of the training later.</v>

129
00:06:36.541 --> 00:06:40.080
<v Speaker 1>So this kind of training in a </v>
<v Speaker 1>distributed way,</v>

130
00:06:40.081 --> 00:06:43.370
<v Speaker 1>it's very important for cases in which </v>
<v Speaker 1>you need to treat them out.</v>

131
00:06:43.430 --> 00:06:45.570
<v Speaker 1>I'm a very large amounts of data </v>
<v Speaker 1>quickly.</v>

132
00:06:45.600 --> 00:06:48.570
<v Speaker 1>I'm the kind of problem that's Google,</v>
<v Speaker 1>sometimes it has to deal with.</v>

133
00:06:50.100 --> 00:06:50.933
<v Speaker 1>So um,</v>
<v Speaker 1>of course you have to start to modify </v>

134
00:06:52.501 --> 00:06:56.400
<v Speaker 1>your model graph so that the shared </v>
<v Speaker 1>things like the weights variables in the</v>

135
00:06:56.401 --> 00:07:00.180
<v Speaker 1>model are shared on a server called </v>
<v Speaker 1>primary receiver here,</v>

136
00:07:00.810 --> 00:07:04.500
<v Speaker 1>but that's basically distributed </v>
<v Speaker 1>training intensive flow in a nutshell.</v>

137
00:07:04.770 --> 00:07:05.760
<v Speaker 1>So again,</v>
<v Speaker 1>if you're interested,</v>

138
00:07:05.761 --> 00:07:06.594
<v Speaker 1>you can look at the slide and can click </v>
<v Speaker 1>that link to learn more about </v>

139
00:07:09.360 --> 00:07:12.340
<v Speaker 1>distributor training.</v>
<v Speaker 1>Any questions so far?</v>

140
00:07:15.700 --> 00:07:18.090
<v Speaker 1>Okay.</v>
<v Speaker 1>Okay.</v>

141
00:07:18.091 --> 00:07:18.811
<v Speaker 1>So,</v>
<v Speaker 1>um,</v>

142
00:07:18.811 --> 00:07:21.840
<v Speaker 1>also because you're representing your </v>
<v Speaker 1>model as a data structure,</v>

143
00:07:22.320 --> 00:07:26.160
<v Speaker 1>you're not limited by the speed on </v>
<v Speaker 1>concurrency of the language in which the</v>

144
00:07:26.161 --> 00:07:28.890
<v Speaker 1>model is defined.</v>
<v Speaker 1>We know that on python is slow sometimes</v>

145
00:07:29.220 --> 00:07:30.053
<v Speaker 1>and even if you try to make python or </v>
<v Speaker 1>parallel parallel as the by writing </v>

146
00:07:34.141 --> 00:07:36.510
<v Speaker 1>multithreading,</v>
<v Speaker 1>you will run into the show called python</v>

147
00:07:36.511 --> 00:07:39.480
<v Speaker 1>global interpreter lock and that will </v>
<v Speaker 1>slow you down,</v>

148
00:07:39.510 --> 00:07:40.910
<v Speaker 1>especially for the kind of,</v>
<v Speaker 1>um,</v>

149
00:07:41.340 --> 00:07:43.410
<v Speaker 1>competition.</v>
<v Speaker 1>Got a deep learning model needs to do.</v>

150
00:07:43.940 --> 00:07:44.773
<v Speaker 1>Um,</v>
<v Speaker 1>so the way in which we solve this </v>

151
00:07:45.481 --> 00:07:46.314
<v Speaker 1>problem in symbolic execution is by </v>
<v Speaker 1>sending the model as a data structure </v>

152
00:07:50.551 --> 00:07:53.620
<v Speaker 1>from the layer of python c plus plus.</v>
<v Speaker 1>So there,</v>

153
00:07:53.660 --> 00:07:54.493
<v Speaker 1>um,</v>
<v Speaker 1>at the level of c plus plus you can use </v>

154
00:07:56.310 --> 00:07:57.143
<v Speaker 1>true concurrency,</v>
<v Speaker 1>you can fully parallel buys things and </v>

155
00:07:59.161 --> 00:08:00.870
<v Speaker 1>that can benefit the speed of the model.</v>

156
00:08:03.200 --> 00:08:04.090
<v Speaker 1>So obviously,</v>
<v Speaker 1>um,</v>

157
00:08:04.130 --> 00:08:06.260
<v Speaker 1>there are all those advantages,</v>
<v Speaker 1>but there are also,</v>

158
00:08:06.280 --> 00:08:07.113
<v Speaker 1>there was like,</v>
<v Speaker 1>um,</v>

159
00:08:07.490 --> 00:08:09.730
<v Speaker 1>shortcomings of symbolic execution,</v>
<v Speaker 1>for example.</v>

160
00:08:09.740 --> 00:08:11.810
<v Speaker 1>It's less intuitive,</v>
<v Speaker 1>it's harder to learn.</v>

161
00:08:11.950 --> 00:08:12.521
<v Speaker 1>Um,</v>
<v Speaker 1>so you,</v>

162
00:08:12.521 --> 00:08:15.600
<v Speaker 1>you need to spend some time getting used</v>
<v Speaker 1>to the idea of beauty.</v>

163
00:08:16.080 --> 00:08:19.880
<v Speaker 1>You're defining a model first and then </v>
<v Speaker 1>run the model later with tf dot session.</v>

164
00:08:20.230 --> 00:08:21.063
<v Speaker 1>Um,</v>
<v Speaker 1>and it's harder to debug a way a model </v>

165
00:08:22.520 --> 00:08:24.620
<v Speaker 1>goes around.</v>
<v Speaker 1>That's because everything,</v>

166
00:08:24.860 --> 00:08:27.650
<v Speaker 1>every actual competition happens inside </v>
<v Speaker 1>the TF dot session.</v>

167
00:08:27.651 --> 00:08:30.720
<v Speaker 1>And that's a single line of Python code </v>
<v Speaker 1>calling a c plus plus.</v>

168
00:08:30.800 --> 00:08:34.300
<v Speaker 1>So you can use the usual kinds of python</v>
<v Speaker 1>debugger to do bumped heads.</v>

169
00:08:34.820 --> 00:08:37.370
<v Speaker 1>But I'm going to show you that they were</v>
<v Speaker 1>actually very good tools in tensive flow</v>

170
00:08:37.371 --> 00:08:39.320
<v Speaker 1>that you can use to debug things that </v>
<v Speaker 1>had been in.</v>

171
00:08:39.650 --> 00:08:42.130
<v Speaker 1>Do you have that session?</v>
<v Speaker 1>And uh,</v>

172
00:08:42.131 --> 00:08:42.964
<v Speaker 1>another shortcoming of a symbolic </v>
<v Speaker 1>exclusion is that it's harder to write </v>

173
00:08:46.701 --> 00:08:47.660
<v Speaker 1>control flow structures.</v>

174
00:08:47.690 --> 00:08:48.523
<v Speaker 1>So by that I mean I'm structure as like </v>
<v Speaker 1>I'm looping over a number of things or </v>

175
00:08:52.221 --> 00:08:53.054
<v Speaker 1>if else branches,</v>
<v Speaker 1>so that kind of thing that we encounter </v>

176
00:08:55.320 --> 00:08:56.153
<v Speaker 1>in programming languages,</v>
<v Speaker 1>but some machine learning models also </v>

177
00:08:57.901 --> 00:08:58.740
<v Speaker 1>need to do that,</v>
<v Speaker 1>right?</v>

178
00:08:58.741 --> 00:08:59.574
<v Speaker 1>So likely recurrent neural networks need</v>
<v Speaker 1>to loop over things and some kind of </v>

179
00:09:02.430 --> 00:09:06.630
<v Speaker 1>fancy I'm a dynamic models need to do if</v>
<v Speaker 1>ellis branches and so on.</v>

180
00:09:06.660 --> 00:09:09.840
<v Speaker 1>I'm also going to show some slides to </v>
<v Speaker 1>show those concrete examples.</v>

181
00:09:10.860 --> 00:09:11.693
<v Speaker 1>So it's very hard to sometimes very hard</v>
<v Speaker 1>to write that kind of control flow </v>

182
00:09:14.551 --> 00:09:18.660
<v Speaker 1>structures in symbolic execution,</v>
<v Speaker 1>but it's much easier in eager execution.</v>

183
00:09:20.220 --> 00:09:21.053
<v Speaker 1>So I'm with Eager Execution Europe,</v>
<v Speaker 1>your program can be more authentic and </v>

184
00:09:24.511 --> 00:09:27.390
<v Speaker 1>it's easier to learn and easier to read.</v>
<v Speaker 1>So here's an example.</v>

185
00:09:27.420 --> 00:09:30.090
<v Speaker 1>So on the left you're seeing the same </v>
<v Speaker 1>code as before.</v>

186
00:09:30.120 --> 00:09:33.600
<v Speaker 1>So we are using the default symbolic </v>
<v Speaker 1>execution of tensorflow.</v>

187
00:09:34.040 --> 00:09:34.873
<v Speaker 1>No.</v>
<v Speaker 1>Now how do we switch to the new eager </v>

188
00:09:36.790 --> 00:09:38.850
<v Speaker 1>execution?</v>
<v Speaker 1>So it just add two lines of code.</v>

189
00:09:39.330 --> 00:09:40.163
<v Speaker 1>You import the eager module and then you</v>
<v Speaker 1>call a method called enabled eager </v>

190
00:09:42.960 --> 00:09:43.793
<v Speaker 1>execution,</v>
<v Speaker 1>and you don't have to make any other </v>

191
00:09:45.001 --> 00:09:46.990
<v Speaker 1>changes to your program in this case,</v>
<v Speaker 1>um,</v>

192
00:09:47.100 --> 00:09:47.933
<v Speaker 1>but you will because of these few lines,</v>
<v Speaker 1>you changed the semantics of these two </v>

193
00:09:51.271 --> 00:09:52.560
<v Speaker 1>lines,</v>
<v Speaker 1>multiply and add.</v>

194
00:09:52.890 --> 00:09:53.723
<v Speaker 1>So now instead of building a graph,</v>
<v Speaker 1>this line is actually doing the </v>

195
00:09:56.131 --> 00:09:58.440
<v Speaker 1>multiplication of ten four.</v>
<v Speaker 1>And if you print while,</v>

196
00:09:58.441 --> 00:10:01.440
<v Speaker 1>you will see the value and if you print </v>
<v Speaker 1>the value of z,</v>

197
00:10:01.441 --> 00:10:03.360
<v Speaker 1>you will also see the value.</v>
<v Speaker 1>So everything is,</v>

198
00:10:03.400 --> 00:10:05.640
<v Speaker 1>I'm like a flutter and easier to </v>
<v Speaker 1>understand.</v>

199
00:10:08.430 --> 00:10:10.140
<v Speaker 1>Now,</v>
<v Speaker 1>as I mentioned before,</v>

200
00:10:10.310 --> 00:10:12.900
<v Speaker 1>eager mode also makes it easier to </v>
<v Speaker 1>control flow,</v>

201
00:10:13.110 --> 00:10:18.110
<v Speaker 1>dependency and the dynamic models.</v>
<v Speaker 1>So here's an example.</v>

202
00:10:18.571 --> 00:10:21.030
<v Speaker 1>So suppose you want to write a recurrent</v>
<v Speaker 1>neural network,</v>

203
00:10:21.031 --> 00:10:23.280
<v Speaker 1>which I think you have seen in previous </v>
<v Speaker 1>parts of the lecture before,</v>

204
00:10:23.670 --> 00:10:24.420
<v Speaker 1>in,</v>
<v Speaker 1>um,</v>

205
00:10:24.420 --> 00:10:27.780
<v Speaker 1>in the default mode of tensorflow.</v>
<v Speaker 1>Here's about the amount of code you need</v>

206
00:10:27.781 --> 00:10:29.070
<v Speaker 1>to write.</v>
<v Speaker 1>So,</v>

207
00:10:29.071 --> 00:10:29.904
<v Speaker 1>um,</v>
<v Speaker 1>you cannot use the default native </v>

208
00:10:31.890 --> 00:10:35.770
<v Speaker 1>foreloop or while loop in python.</v>
<v Speaker 1>You have to use the tensorflow's special</v>

209
00:10:35.771 --> 00:10:38.100
<v Speaker 1>wild loop.</v>
<v Speaker 1>And in order to use it,</v>

210
00:10:38.101 --> 00:10:40.920
<v Speaker 1>you have to define two functions,</v>
<v Speaker 1>one for the termination,</v>

211
00:10:40.921 --> 00:10:43.950
<v Speaker 1>conditioner of the loop and one for the </v>
<v Speaker 1>body of the loop.</v>

212
00:10:44.310 --> 00:10:45.143
<v Speaker 1>And then you need to feed those two </v>
<v Speaker 1>functions into the while loop and get </v>

213
00:10:48.160 --> 00:10:49.440
<v Speaker 1>answers back.</v>
<v Speaker 1>And remember,</v>

214
00:10:49.441 --> 00:10:50.274
<v Speaker 1>those sensors are not actual values.</v>
<v Speaker 1>You have to send those 10 stairs into </v>

215
00:10:53.440 --> 00:10:54.900
<v Speaker 1>session that runs,</v>
<v Speaker 1>get together actual value.</v>

216
00:10:54.901 --> 00:10:55.734
<v Speaker 1>So there are a few hoops to jump through</v>
<v Speaker 1>if you want to write a rn and from </v>

217
00:10:58.801 --> 00:11:00.720
<v Speaker 1>scratch in the default mode of </v>
<v Speaker 1>tensorflow,</v>

218
00:11:01.260 --> 00:11:03.570
<v Speaker 1>but with eager execution that things </v>
<v Speaker 1>becomes much simpler.</v>

219
00:11:04.290 --> 00:11:05.123
<v Speaker 1>So you can use the native for loop in </v>
<v Speaker 1>Python to loop over time steps and the </v>

220
00:11:08.731 --> 00:11:09.564
<v Speaker 1>input and you don't have to worry about </v>
<v Speaker 1>those symbolic tensors or sessions are </v>

221
00:11:12.901 --> 00:11:14.050
<v Speaker 1>run the,</v>
<v Speaker 1>the,</v>

222
00:11:14.080 --> 00:11:14.880
<v Speaker 1>the,</v>
<v Speaker 1>um,</v>

223
00:11:14.880 --> 00:11:18.600
<v Speaker 1>the variables you get from this followup</v>
<v Speaker 1>is the result of the competition.</v>

224
00:11:21.390 --> 00:11:25.800
<v Speaker 1>So I'm eager mode makes it much easier </v>
<v Speaker 1>to write the so called dynamic models.</v>

225
00:11:25.830 --> 00:11:29.040
<v Speaker 1>So what do we mean by static models in </v>
<v Speaker 1>the dynamic models?</v>

226
00:11:29.370 --> 00:11:30.203
<v Speaker 1>So by static models we mean models who </v>
<v Speaker 1>was structured don't change with the </v>

227
00:11:33.631 --> 00:11:34.464
<v Speaker 1>input data.</v>
<v Speaker 1>And I think you have seen examples like </v>

228
00:11:36.701 --> 00:11:38.910
<v Speaker 1>that in the image model sections of this</v>
<v Speaker 1>lecture.</v>

229
00:11:38.911 --> 00:11:43.140
<v Speaker 1>So the diagram here shows the inception </v>
<v Speaker 1>model in tensorflow,</v>

230
00:11:43.530 --> 00:11:44.363
<v Speaker 1>so the model can be very complicated,</v>
<v Speaker 1>can have hundreds of layers and the can </v>

231
00:11:47.671 --> 00:11:48.504
<v Speaker 1>do,</v>
<v Speaker 1>can do like a complicated competition </v>

232
00:11:49.771 --> 00:11:53.470
<v Speaker 1>like convolution pooling and a dense </v>
<v Speaker 1>manipulation and so on.</v>

233
00:11:53.830 --> 00:11:54.663
<v Speaker 1>But the structure of the model is always</v>
<v Speaker 1>the same no matter how the image </v>

234
00:11:57.311 --> 00:11:58.144
<v Speaker 1>changes,</v>
<v Speaker 1>the image always has the same size and </v>

235
00:11:59.891 --> 00:12:01.830
<v Speaker 1>the same cutter depth.</v>
<v Speaker 1>Um,</v>

236
00:12:01.960 --> 00:12:03.970
<v Speaker 1>but the model will always compute the </v>
<v Speaker 1>same.</v>

237
00:12:04.170 --> 00:12:05.003
<v Speaker 1>I mean it will always do the same </v>
<v Speaker 1>competition regardless how the image </v>

238
00:12:07.390 --> 00:12:08.200
<v Speaker 1>changes.</v>

239
00:12:08.200 --> 00:12:11.990
<v Speaker 1>So that's what we mean by static model.</v>
<v Speaker 1>But there are also models was structured</v>

240
00:12:11.991 --> 00:12:15.160
<v Speaker 1>change with inputs and data.</v>
<v Speaker 1>So the recurrence,</v>

241
00:12:15.161 --> 00:12:15.994
<v Speaker 1>do you want network?</v>
<v Speaker 1>We have seen is actually an example of </v>

242
00:12:17.021 --> 00:12:17.854
<v Speaker 1>that.</v>
<v Speaker 1>And the reason why it's changes is </v>

243
00:12:18.911 --> 00:12:19.744
<v Speaker 1>because it needs to loop over things.</v>
<v Speaker 1>So in the simplest kind of recurrent </v>

244
00:12:22.790 --> 00:12:25.690
<v Speaker 1>neural network,</v>
<v Speaker 1>it will loop over items in the sequence,</v>

245
00:12:25.720 --> 00:12:26.553
<v Speaker 1>like a words in a sentence.</v>
<v Speaker 1>So you can say that the length of the </v>

246
00:12:29.951 --> 00:12:32.380
<v Speaker 1>model is proportional to the length of </v>
<v Speaker 1>the input sentence,</v>

247
00:12:32.800 --> 00:12:33.633
<v Speaker 1>but there are also more complicated </v>
<v Speaker 1>changes of the model structure with </v>

248
00:12:36.341 --> 00:12:37.174
<v Speaker 1>input data.</v>
<v Speaker 1>So some of the state of the art models </v>

249
00:12:40.121 --> 00:12:40.954
<v Speaker 1>that deal with natural language,</v>
<v Speaker 1>we actually take a parse tree of a </v>

250
00:12:43.750 --> 00:12:48.520
<v Speaker 1>sentence as the inputs and the structure</v>
<v Speaker 1>of the model will reflect that tree.</v>

251
00:12:49.720 --> 00:12:50.553
<v Speaker 1>So,</v>
<v Speaker 1>um,</v>

252
00:12:50.620 --> 00:12:52.800
<v Speaker 1>as we have seen before,</v>
<v Speaker 1>it's complicated to your rights are wild</v>

253
00:12:52.801 --> 00:12:54.740
<v Speaker 1>loops or control flow structures in,</v>
<v Speaker 1>um,</v>

254
00:12:54.840 --> 00:12:58.000
<v Speaker 1>in those defaults symbolic mode.</v>
<v Speaker 1>Now if you want one to ride that kind of</v>

255
00:12:58.001 --> 00:12:58.834
<v Speaker 1>model,</v>
<v Speaker 1>it gets even more complicated because </v>

256
00:13:00.791 --> 00:13:04.570
<v Speaker 1>there you will need to nest on </v>
<v Speaker 1>conditional branches and a wild loops,</v>

257
00:13:05.020 --> 00:13:05.853
<v Speaker 1>but it's much easier to revise in that </v>
<v Speaker 1>moment because you can just use the </v>

258
00:13:08.921 --> 00:13:11.980
<v Speaker 1>native for loops and while loops and if </v>
<v Speaker 1>statements in python.</v>

259
00:13:12.430 --> 00:13:17.320
<v Speaker 1>So we actually have an example to show </v>
<v Speaker 1>you how to ride that kind of models.</v>

260
00:13:17.340 --> 00:13:20.350
<v Speaker 1>Dads take parse trees as input and the </v>
<v Speaker 1>process natural language.</v>

261
00:13:20.680 --> 00:13:23.140
<v Speaker 1>So please check that out if you want.</v>
<v Speaker 1>You have a question,</v>

262
00:13:25.710 --> 00:13:26.543
<v Speaker 2>right?</v>

263
00:13:27.060 --> 00:13:27.893
<v Speaker 1>The tree is static.</v>

264
00:13:32.010 --> 00:13:33.050
<v Speaker 2>Okay.</v>

265
00:13:33.400 --> 00:13:34.233
<v Speaker 1>Yeah.</v>
<v Speaker 1>The tree is static in this particular </v>

266
00:13:35.530 --> 00:13:37.360
<v Speaker 1>input,</v>
<v Speaker 1>but you can have a longer sentence,</v>

267
00:13:37.390 --> 00:13:38.040
<v Speaker 1>right?</v>
<v Speaker 1>There's the,</v>

268
00:13:38.040 --> 00:13:40.480
<v Speaker 1>the,</v>
<v Speaker 1>the grammar of the sentence.</v>

269
00:13:40.620 --> 00:13:44.050
<v Speaker 1>The sentence can change from one </v>
<v Speaker 1>sentence to another,</v>

270
00:13:44.080 --> 00:13:44.913
<v Speaker 1>right?</v>
<v Speaker 1>So that will make the model structure </v>

271
00:13:46.030 --> 00:13:46.863
<v Speaker 1>change as well.</v>
<v Speaker 1>So basically you can hard code the </v>

272
00:13:49.241 --> 00:13:50.074
<v Speaker 1>structure of the model.</v>
<v Speaker 1>You have to like look how to treat and </v>

273
00:13:52.061 --> 00:13:52.910
<v Speaker 1>then like,</v>
<v Speaker 1>um,</v>

274
00:13:53.210 --> 00:13:54.043
<v Speaker 1>uh,</v>
<v Speaker 1>do some kind of like an if else </v>

275
00:13:55.051 --> 00:13:59.080
<v Speaker 1>statements and while loops in order to </v>
<v Speaker 1>like turn the theory into the model.</v>

276
00:14:00.260 --> 00:14:01.093
<v Speaker 2>Yep.</v>

277
00:14:03.200 --> 00:14:03.921
<v Speaker 1>Okay.</v>
<v Speaker 1>So,</v>

278
00:14:03.921 --> 00:14:04.754
<v Speaker 1>um,</v>
<v Speaker 1>we have seen that the eco mode is very </v>

279
00:14:05.601 --> 00:14:06.434
<v Speaker 1>good for um,</v>
<v Speaker 1>learning and debugging and for writing </v>

280
00:14:09.050 --> 00:14:11.950
<v Speaker 1>control flow structures.</v>
<v Speaker 1>But sometimes you may,</v>

281
00:14:11.970 --> 00:14:15.560
<v Speaker 1>you may still want to debug a tensorflow</v>
<v Speaker 1>programs running in the default symbolic</v>

282
00:14:15.650 --> 00:14:17.330
<v Speaker 1>modes.</v>
<v Speaker 1>And there are a few reasons for that.</v>

283
00:14:17.360 --> 00:14:18.193
<v Speaker 1>First,</v>
<v Speaker 1>you may be using some kind of old code </v>

284
00:14:20.210 --> 00:14:22.910
<v Speaker 1>of tensorflow that hasn't been ported to</v>
<v Speaker 1>eager mode yet.</v>

285
00:14:23.330 --> 00:14:25.160
<v Speaker 1>And some high level API as you might be </v>
<v Speaker 1>using,</v>

286
00:14:25.161 --> 00:14:29.120
<v Speaker 1>like tf learned or terrorists or tf slim</v>
<v Speaker 1>have not been important to.</v>

287
00:14:29.810 --> 00:14:32.330
<v Speaker 1>Yes.</v>
<v Speaker 1>And you may want to stick to the default</v>

288
00:14:32.331 --> 00:14:34.370
<v Speaker 1>symbolic moments.</v>
<v Speaker 1>It because you care about speed on,</v>

289
00:14:34.420 --> 00:14:37.820
<v Speaker 1>because eco mode is sometimes slower </v>
<v Speaker 1>than the default mode.</v>

290
00:14:38.240 --> 00:14:38.781
<v Speaker 1>So,</v>
<v Speaker 1>um,</v>

291
00:14:38.781 --> 00:14:39.614
<v Speaker 1>the good news is that we have a tool </v>
<v Speaker 1>intensive flow that can help you like </v>

292
00:14:42.110 --> 00:14:45.060
<v Speaker 1>debug attentive flow model running in </v>
<v Speaker 1>the tf,</v>

293
00:14:45.061 --> 00:14:49.820
<v Speaker 1>the TF dot session in the mode,</v>
<v Speaker 1>and that's what was called tf dbg,</v>

294
00:14:49.850 --> 00:14:51.650
<v Speaker 1>or tensorflow debugger.</v>

295
00:14:52.640 --> 00:14:53.473
<v Speaker 1>So the way in mentioned you use it is </v>
<v Speaker 1>kind of similar to the way in which he </v>

296
00:14:55.921 --> 00:14:57.320
<v Speaker 1>was eager,</v>
<v Speaker 1>execution,</v>

297
00:14:57.670 --> 00:14:58.503
<v Speaker 1>important and additional module.</v>
<v Speaker 1>And after you have created the session </v>

298
00:15:01.491 --> 00:15:02.151
<v Speaker 1>object,</v>
<v Speaker 1>you,</v>

299
00:15:02.151 --> 00:15:02.984
<v Speaker 1>we'll wrap the session object where the </v>
<v Speaker 1>rapper in this case is called local </v>

300
00:15:05.511 --> 00:15:08.030
<v Speaker 1>command line interface direct GRANDPA.</v>
<v Speaker 1>So,</v>

301
00:15:08.031 --> 00:15:09.890
<v Speaker 1>um,</v>
<v Speaker 1>you don't need to make any other changes</v>

302
00:15:09.891 --> 00:15:10.724
<v Speaker 1>to your code because this rapid object </v>
<v Speaker 1>has the same interface as the unwrapped </v>

303
00:15:14.270 --> 00:15:16.070
<v Speaker 1>object.</v>
<v Speaker 1>But basically you can,</v>

304
00:15:16.090 --> 00:15:16.923
<v Speaker 1>um,</v>
<v Speaker 1>you can think of this as like an </v>

305
00:15:17.631 --> 00:15:21.500
<v Speaker 1>oscilloscope on some kind of instrument </v>
<v Speaker 1>on your tf dot session,</v>

306
00:15:21.501 --> 00:15:24.260
<v Speaker 1>which is otherwise opaque.</v>
<v Speaker 1>So now,</v>

307
00:15:24.290 --> 00:15:28.700
<v Speaker 1>once you have wrapped that session,</v>
<v Speaker 1>when sessions are run is called,</v>

308
00:15:28.820 --> 00:15:30.710
<v Speaker 1>you're going to drop into the command </v>
<v Speaker 1>line interface.</v>

309
00:15:30.740 --> 00:15:33.050
<v Speaker 1>You're going to see basically a </v>
<v Speaker 1>presentation of,</v>

310
00:15:33.420 --> 00:15:36.350
<v Speaker 1>um,</v>
<v Speaker 1>one intermediate tensors are executed in</v>

311
00:15:36.351 --> 00:15:39.230
<v Speaker 1>the sessions are run and the structure </v>
<v Speaker 1>and the graph and so on.</v>

312
00:15:39.231 --> 00:15:43.910
<v Speaker 1>So I encourage you to encourage you to </v>
<v Speaker 1>play with that after the lecture.</v>

313
00:15:44.960 --> 00:15:45.793
<v Speaker 1>So</v>

314
00:15:47.540 --> 00:15:48.373
<v Speaker 1>the TTF debugger is also very useful for</v>
<v Speaker 1>debugging or kind of bugs in your </v>

315
00:15:51.531 --> 00:15:53.480
<v Speaker 1>machine learning models which will </v>
<v Speaker 1>probably occur.</v>

316
00:15:53.810 --> 00:15:55.730
<v Speaker 1>Those are called numerical instability </v>
<v Speaker 1>issues.</v>

317
00:15:55.760 --> 00:15:56.593
<v Speaker 1>So by that I mean sometimes value is in </v>
<v Speaker 1>the network who will become Nan's or </v>

318
00:15:59.601 --> 00:16:01.100
<v Speaker 1>infinities.</v>
<v Speaker 1>So Nan stands,</v>

319
00:16:01.370 --> 00:16:02.203
<v Speaker 1>stands for not a number.</v>
<v Speaker 1>Naza infinities are like bad float </v>

320
00:16:05.180 --> 00:16:07.430
<v Speaker 1>values that will sometimes happen.</v>
<v Speaker 1>Now,</v>

321
00:16:07.910 --> 00:16:09.770
<v Speaker 1>if you don't have a specialized tool in </v>
<v Speaker 1>tensorflow,</v>

322
00:16:09.830 --> 00:16:14.440
<v Speaker 1>it can be difficult to pinpoint the </v>
<v Speaker 1>exact notes which generates the nuns and</v>

323
00:16:14.460 --> 00:16:15.293
<v Speaker 1>infinities,</v>
<v Speaker 1>but the debugger has a special commands </v>

324
00:16:17.330 --> 00:16:21.560
<v Speaker 1>with which you can run the model until </v>
<v Speaker 1>any nodes in the graph contains Nan's or</v>

325
00:16:21.561 --> 00:16:24.260
<v Speaker 1>infinities.</v>
<v Speaker 1>So in our experience,</v>

326
00:16:24.530 --> 00:16:25.363
<v Speaker 1>that happens quite often and the most </v>
<v Speaker 1>common causes of Nancy and infinities </v>

327
00:16:28.940 --> 00:16:31.430
<v Speaker 1>are underflow and overflow.</v>
<v Speaker 1>So for example,</v>

328
00:16:31.431 --> 00:16:33.950
<v Speaker 1>if there's an underflow,</v>
<v Speaker 1>then a number will become zero.</v>

329
00:16:34.250 --> 00:16:36.460
<v Speaker 1>And when you use that number in division</v>
<v Speaker 1>or log,</v>

330
00:16:36.500 --> 00:16:37.333
<v Speaker 1>you will get infinities and overflow can</v>
<v Speaker 1>be caused by learning rates being too </v>

331
00:16:40.131 --> 00:16:40.964
<v Speaker 1>high or by some kind of bad training </v>
<v Speaker 1>example that you haven't sanitized or a </v>

332
00:16:45.370 --> 00:16:46.890
<v Speaker 1>pre processed.</v>
<v Speaker 1>But um,</v>

333
00:16:46.960 --> 00:16:47.793
<v Speaker 1>the debugger should help you find the </v>
<v Speaker 1>root cause of that kind of issue is </v>

334
00:16:49.631 --> 00:16:50.464
<v Speaker 1>more,</v>
<v Speaker 1>more quickly.</v>

335
00:16:53.290 --> 00:16:56.110
<v Speaker 1>So the TF debugger is a command line </v>
<v Speaker 1>tool.</v>

336
00:16:56.350 --> 00:16:57.880
<v Speaker 1>It's nice,</v>
<v Speaker 1>it's a low footprint.</v>

337
00:16:58.240 --> 00:17:01.690
<v Speaker 1>You can use it if you have access to a </v>
<v Speaker 1>computer only via a terminal,</v>

338
00:17:01.960 --> 00:17:02.793
<v Speaker 1>but obviously it's going to be even </v>
<v Speaker 1>nicer if we can debulk the tensorflow </v>

339
00:17:05.921 --> 00:17:06.754
<v Speaker 1>models in a graphical user interface.</v>
<v Speaker 1>So I'm excited to tell you about a </v>

340
00:17:11.260 --> 00:17:12.093
<v Speaker 1>feature of tensorflow that's upcoming.</v>
<v Speaker 1>So it's called tensor board director </v>

341
00:17:14.591 --> 00:17:17.940
<v Speaker 1>plugin or visual graphical debugger for </v>
<v Speaker 1>tensorflow.</v>

342
00:17:17.950 --> 00:17:19.980
<v Speaker 1>So it's not included in the latest </v>
<v Speaker 1>release of</v>

343
00:17:21.620 --> 00:17:22.880
<v Speaker 1>has low,</v>
<v Speaker 1>which is one point five,</v>

344
00:17:22.881 --> 00:17:25.160
<v Speaker 1>but it's coming in the next release one </v>
<v Speaker 1>point six.</v>

345
00:17:25.700 --> 00:17:27.950
<v Speaker 1>It's available for preview in Natalie's.</v>
<v Speaker 1>So,</v>

346
00:17:27.951 --> 00:17:28.784
<v Speaker 1>um,</v>
<v Speaker 1>you can copy and paste the code from </v>

347
00:17:30.141 --> 00:17:30.974
<v Speaker 1>here,</v>
<v Speaker 1>install those nightly builds of </v>

348
00:17:32.050 --> 00:17:35.300
<v Speaker 1>tensorflow and the cancer board.</v>
<v Speaker 1>And you can use the feature.</v>

349
00:17:35.690 --> 00:17:38.390
<v Speaker 1>So after you have installed these </v>
<v Speaker 1>packages,</v>

350
00:17:38.391 --> 00:17:39.224
<v Speaker 1>you can run a command.</v>
<v Speaker 1>So I'm all the code in my slides are </v>

351
00:17:41.571 --> 00:17:43.700
<v Speaker 1>copy paste to execute that Calabasas.</v>
<v Speaker 1>Yeah.</v>

352
00:17:43.701 --> 00:17:46.580
<v Speaker 1>So these are about the main features of </v>
<v Speaker 1>upcoming tool.</v>

353
00:17:46.820 --> 00:17:47.670
<v Speaker 1>So,</v>
<v Speaker 1>um,</v>

354
00:17:47.870 --> 00:17:48.703
<v Speaker 1>if you're interested,</v>
<v Speaker 1>please copy and paste these code and to </v>

355
00:17:50.200 --> 00:17:51.033
<v Speaker 1>try it out.</v>
<v Speaker 1>This slide here is just a reminder of </v>

356
00:17:54.121 --> 00:17:55.450
<v Speaker 1>the main features in here.</v>
<v Speaker 1>Um,</v>

357
00:17:56.370 --> 00:17:56.971
<v Speaker 1>okay.</v>
<v Speaker 1>So as,</v>

358
00:17:56.971 --> 00:17:58.470
<v Speaker 1>as um,</v>
<v Speaker 1>summary,</v>

359
00:17:58.500 --> 00:17:59.333
<v Speaker 1>we see that there are two ways to </v>
<v Speaker 1>represent the machinery models in </v>

360
00:18:01.171 --> 00:18:02.020
<v Speaker 1>tensorflow,</v>
<v Speaker 1>um,</v>

361
00:18:02.280 --> 00:18:06.720
<v Speaker 1>or in any deep learning framework either</v>
<v Speaker 1>as a data structure or as a program.</v>

362
00:18:07.020 --> 00:18:07.853
<v Speaker 1>If it's a data structure then you will </v>
<v Speaker 1>get symbolic execution and symbolic </v>

363
00:18:10.831 --> 00:18:14.580
<v Speaker 1>execution is good for deployments,</v>
<v Speaker 1>distribution and optimization.</v>

364
00:18:15.270 --> 00:18:17.730
<v Speaker 1>And if it's a program that you will get </v>
<v Speaker 1>eager execution,</v>

365
00:18:17.731 --> 00:18:19.500
<v Speaker 1>it's good for prototyping,</v>
<v Speaker 1>debugging,</v>

366
00:18:19.501 --> 00:18:24.210
<v Speaker 1>and a writing control flow structures </v>
<v Speaker 1>and it's also easier to learn.</v>

367
00:18:24.930 --> 00:18:25.763
<v Speaker 1>And the country in terms of law supports</v>
<v Speaker 1>both modes so you can pick and choose </v>

368
00:18:27.901 --> 00:18:29.760
<v Speaker 1>the best mode for you depending on your </v>
<v Speaker 1>need.</v>

369
00:18:30.360 --> 00:18:31.021
<v Speaker 1>And,</v>
<v Speaker 1>uh,</v>

370
00:18:31.021 --> 00:18:34.080
<v Speaker 1>um,</v>
<v Speaker 1>we also went over tens of load debugger,</v>

371
00:18:34.110 --> 00:18:36.660
<v Speaker 1>both the command line interface and the </v>
<v Speaker 1>browser version,</v>

372
00:18:36.960 --> 00:18:39.600
<v Speaker 1>which will help you debug your model </v>
<v Speaker 1>more efficiently.</v>

373
00:18:40.500 --> 00:18:41.333
<v Speaker 1>So with that,</v>
<v Speaker 1>I'm going to thank my colleagues on the </v>

374
00:18:43.831 --> 00:18:46.860
<v Speaker 1>Google brain team both in the amount of </v>
<v Speaker 1>new headquarters and here in Cambridge.</v>

375
00:18:47.090 --> 00:18:49.560
<v Speaker 1>Um,</v>
<v Speaker 1>she and Mahima are the two collaborators</v>

376
00:18:49.561 --> 00:18:52.020
<v Speaker 1>with me on the cancer board,</v>
<v Speaker 1>debugger plugging project.</v>

377
00:18:52.560 --> 00:18:54.640
<v Speaker 1>And a tensorflow is an open source </v>
<v Speaker 1>project.</v>

378
00:18:54.750 --> 00:18:55.583
<v Speaker 1>There have been over 1000 contributors </v>
<v Speaker 1>like you who have fixed bugs and </v>

379
00:18:58.831 --> 00:18:59.664
<v Speaker 1>contributing new features.</v>
<v Speaker 1>So if you see any interesting things </v>

380
00:19:01.530 --> 00:19:03.810
<v Speaker 1>that you can do,</v>
<v Speaker 1>feel free to contribute to tensorflow on</v>

381
00:19:03.811 --> 00:19:05.790
<v Speaker 1>get hub.</v>
<v Speaker 1>If you have questions,</v>

382
00:19:06.120 --> 00:19:06.953
<v Speaker 1>please email me.</v>
<v Speaker 1>And if you see any bugs or feature </v>

383
00:19:09.691 --> 00:19:14.610
<v Speaker 1>requests about tensorflow or 10 boards,</v>
<v Speaker 1>you can fall on the bugs on these links.</v>

384
00:19:15.810 --> 00:19:17.300
<v Speaker 1>Thank you very much for your attention.</v>
<v Speaker 1>The question.</v>

