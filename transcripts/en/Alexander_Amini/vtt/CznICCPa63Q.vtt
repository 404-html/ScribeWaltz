WEBVTT

1
00:00:03.990 --> 00:00:04.823
<v Speaker 1>Hi everybody.</v>
<v Speaker 1>My name is serine and I'm going to be </v>

2
00:00:06.271 --> 00:00:09.150
<v Speaker 1>talking about how to use neural networks</v>
<v Speaker 1>to model sequences.</v>

3
00:00:09.990 --> 00:00:10.823
<v Speaker 1>In the previous lecture you saw how you </v>
<v Speaker 1>could use a neural network to model a </v>

4
00:00:13.171 --> 00:00:14.004
<v Speaker 1>Dataset of many examples.</v>
<v Speaker 1>The difference with sequences is that </v>

5
00:00:16.860 --> 00:00:19.770
<v Speaker 1>each example consists of multiple data </v>
<v Speaker 1>points.</v>

6
00:00:20.370 --> 00:00:23.670
<v Speaker 1>There can be a variable number of these </v>
<v Speaker 1>data points per example,</v>

7
00:00:24.060 --> 00:00:26.970
<v Speaker 1>and the data points can depend on each </v>
<v Speaker 1>other in complicated ways.</v>

8
00:00:30.190 --> 00:00:32.770
<v Speaker 1>So a sequence could be something like a </v>
<v Speaker 1>sentence,</v>

9
00:00:33.010 --> 00:00:35.080
<v Speaker 1>like this morning I took the dog for a </v>
<v Speaker 1>walk.</v>

10
00:00:35.410 --> 00:00:36.243
<v Speaker 1>This is one example,</v>
<v Speaker 1>but it consists of multiple words and </v>

11
00:00:38.981 --> 00:00:39.814
<v Speaker 1>the words depend on each other.</v>
<v Speaker 1>Another example would be something like </v>

12
00:00:42.491 --> 00:00:46.000
<v Speaker 1>a medical record one medical record </v>
<v Speaker 1>would be one example,</v>

13
00:00:46.330 --> 00:00:47.163
<v Speaker 1>but it consists of many measurements.</v>
<v Speaker 1>Another example would be something like </v>

14
00:00:50.651 --> 00:00:53.380
<v Speaker 1>a speech wave form where this one wave </v>
<v Speaker 1>form as an example,</v>

15
00:00:53.470 --> 00:00:55.660
<v Speaker 1>but again it consists of many,</v>
<v Speaker 1>many measurements.</v>

16
00:00:57.510 --> 00:01:00.960
<v Speaker 1>You've probably encountered sequence </v>
<v Speaker 1>modeling tasks in your everyday life,</v>

17
00:01:00.990 --> 00:01:01.823
<v Speaker 1>especially if you've used things like </v>
<v Speaker 1>google translate Alexa or Siri tasks </v>

18
00:01:04.741 --> 00:01:05.574
<v Speaker 1>like machine translation and question </v>
<v Speaker 1>answering are all sequenced modeling </v>

19
00:01:08.401 --> 00:01:09.234
<v Speaker 1>tasks.</v>
<v Speaker 1>And the state of the art in these tasks </v>

20
00:01:10.861 --> 00:01:12.360
<v Speaker 1>is mostly deep learning based.</v>

21
00:01:16.850 --> 00:01:17.683
<v Speaker 1>Another interesting example I saw </v>
<v Speaker 1>recently was this self parking car by </v>

22
00:01:21.171 --> 00:01:22.610
<v Speaker 1>Audi.</v>
<v Speaker 1>When you think about it,</v>

23
00:01:22.611 --> 00:01:23.444
<v Speaker 1>parking is also a sequence modeling task</v>
<v Speaker 1>because parking is just a sequence of </v>

24
00:01:26.991 --> 00:01:30.650
<v Speaker 1>movements and the next movement depends </v>
<v Speaker 1>on all the previous movements.</v>

25
00:01:31.370 --> 00:01:33.830
<v Speaker 1>Um,</v>
<v Speaker 1>you can watch the rest of this video,</v>

26
00:01:34.360 --> 00:01:35.450
<v Speaker 1>uh,</v>
<v Speaker 1>online.</v>

27
00:01:36.260 --> 00:01:39.110
<v Speaker 1>Okay,</v>
<v Speaker 1>so a sequence modeling problem.</v>

28
00:01:39.180 --> 00:01:40.013
<v Speaker 1>Now I'm just going to walk through a </v>
<v Speaker 1>sequence modeling problem to kind of </v>

29
00:01:41.961 --> 00:01:42.794
<v Speaker 1>motivate why we need a different </v>
<v Speaker 1>framework for specifically for modeling </v>

30
00:01:46.191 --> 00:01:49.550
<v Speaker 1>sequences and what we should be looking </v>
<v Speaker 1>for in that framework.</v>

31
00:01:49.640 --> 00:01:50.473
<v Speaker 2>Okay,</v>

32
00:01:51.540 --> 00:01:53.610
<v Speaker 1>so the problem is predicting the next </v>
<v Speaker 1>word.</v>

33
00:01:54.120 --> 00:01:57.510
<v Speaker 1>Given these words,</v>
<v Speaker 1>we want to predict what comes next.</v>

34
00:01:58.350 --> 00:01:59.183
<v Speaker 1>The first problem we run into is that </v>
<v Speaker 1>machine learning models that are not </v>

35
00:02:02.341 --> 00:02:03.174
<v Speaker 1>explicitly designed to deal with </v>
<v Speaker 1>sequences take as input a fixed length </v>

36
00:02:07.441 --> 00:02:08.274
<v Speaker 1>factor.</v>
<v Speaker 1>Think back to the feed forward neural </v>

37
00:02:10.351 --> 00:02:11.184
<v Speaker 1>network.</v>
<v Speaker 1>From the first lecture that Alexander </v>

38
00:02:12.451 --> 00:02:16.650
<v Speaker 1>introduced,</v>
<v Speaker 1>we have to specify the size of the input</v>

39
00:02:16.680 --> 00:02:17.513
<v Speaker 1>right at the outset.</v>
<v Speaker 1>We can't sometimes feed in a vector of </v>

40
00:02:19.441 --> 00:02:20.274
<v Speaker 1>length.</v>
<v Speaker 1>10 other times feed into elector a </v>

41
00:02:21.811 --> 00:02:24.330
<v Speaker 1>vector of length 20 it has to be fixed </v>
<v Speaker 1>line.</v>

42
00:02:24.450 --> 00:02:25.283
<v Speaker 1>So this is kind of an issue with </v>
<v Speaker 1>sequences because sometimes we might </v>

43
00:02:28.531 --> 00:02:30.930
<v Speaker 1>have seen 10 words and we want to </v>
<v Speaker 1>predict the next word.</v>

44
00:02:30.990 --> 00:02:33.720
<v Speaker 1>Sometimes we might've seen forwards and </v>
<v Speaker 1>we want to predict the next word,</v>

45
00:02:34.080 --> 00:02:38.460
<v Speaker 1>so we have to get that variable length </v>
<v Speaker 1>input into a fixed length vector.</v>

46
00:02:39.330 --> 00:02:43.330
<v Speaker 1>One simple way to do this would be to </v>
<v Speaker 1>just cut off the vectors.</v>

47
00:02:43.331 --> 00:02:43.921
<v Speaker 1>So say,</v>
<v Speaker 1>okay,</v>

48
00:02:43.921 --> 00:02:47.820
<v Speaker 1>we're going to just take a fixed window,</v>
<v Speaker 1>forced the specter to be fixed length by</v>

49
00:02:47.821 --> 00:02:48.654
<v Speaker 1>only considering the previous two words,</v>
<v Speaker 1>no matter where we're making the </v>

50
00:02:51.841 --> 00:02:52.674
<v Speaker 1>prediction.</v>
<v Speaker 1>We'll just take the previous two words </v>

51
00:02:54.000 --> 00:02:54.833
<v Speaker 1>and then try to predict the next word.</v>
<v Speaker 1>Now we can represent these two words as </v>

52
00:02:59.591 --> 00:03:00.424
<v Speaker 1>a fixed length vector by creating a </v>
<v Speaker 1>larger vector and then allocating space </v>

53
00:03:05.441 --> 00:03:07.330
<v Speaker 1>in it for the first word.</v>
<v Speaker 1>And for the second word,</v>

54
00:03:08.800 --> 00:03:10.000
<v Speaker 1>we have a fixed length vector.</v>
<v Speaker 1>Now,</v>

55
00:03:10.001 --> 00:03:10.834
<v Speaker 1>no matter what,</v>
<v Speaker 1>two words we're using and we can feed </v>

56
00:03:12.671 --> 00:03:13.504
<v Speaker 1>this into a machine learning model like </v>
<v Speaker 1>a feed forward neural network or a </v>

57
00:03:16.001 --> 00:03:19.720
<v Speaker 1>logistic regression or any other model </v>
<v Speaker 1>and try to make a prediction.</v>

58
00:03:21.010 --> 00:03:25.270
<v Speaker 1>One thing you might be noticing here is </v>
<v Speaker 1>that by using this fixed window,</v>

59
00:03:25.271 --> 00:03:27.790
<v Speaker 1>we're giving ourselves a very limited </v>
<v Speaker 1>history.</v>

60
00:03:28.030 --> 00:03:32.200
<v Speaker 1>We're trying to predict the word walk.</v>
<v Speaker 1>Having only seen the words for and a,</v>

61
00:03:32.470 --> 00:03:35.740
<v Speaker 1>this is almost impossible,</v>
<v Speaker 1>put differently.</v>

62
00:03:35.770 --> 00:03:39.670
<v Speaker 1>It's really hard to model longterm </v>
<v Speaker 1>dependencies to see this clearly.</v>

63
00:03:39.671 --> 00:03:41.580
<v Speaker 1>Consider the word in,</v>
<v Speaker 1>sorry,</v>

64
00:03:41.600 --> 00:03:44.830
<v Speaker 1>consider the sentence in France.</v>
<v Speaker 1>I had a great time and I learned some of</v>

65
00:03:44.831 --> 00:03:45.664
<v Speaker 1>the blank language.</v>
<v Speaker 1>We're trying to predict the word in the </v>

66
00:03:47.111 --> 00:03:48.910
<v Speaker 1>blank.</v>
<v Speaker 1>I knew it was French,</v>

67
00:03:48.911 --> 00:03:51.640
<v Speaker 1>but that's because I looked very far </v>
<v Speaker 1>back at the word France that appeared in</v>

68
00:03:51.641 --> 00:03:52.474
<v Speaker 1>the beginning of the sentence.</v>
<v Speaker 1>If we were only looking at the past two </v>

69
00:03:54.641 --> 00:03:57.190
<v Speaker 1>words or the past three words or even </v>
<v Speaker 1>the past five words,</v>

70
00:03:57.191 --> 00:03:59.290
<v Speaker 1>it will be really hard to guess the word</v>
<v Speaker 1>in that blank.</v>

71
00:04:00.700 --> 00:04:04.210
<v Speaker 1>So we don't want to limit ourselves so </v>
<v Speaker 1>much.</v>

72
00:04:04.300 --> 00:04:05.133
<v Speaker 1>We want to ideally use all of the </v>
<v Speaker 1>information that we have and the </v>

73
00:04:07.721 --> 00:04:10.630
<v Speaker 1>sequence,</v>
<v Speaker 1>but we also need a fixed length vector.</v>

74
00:04:11.350 --> 00:04:14.740
<v Speaker 1>So one way we could do this is by using </v>
<v Speaker 1>the entire sequence,</v>

75
00:04:14.770 --> 00:04:17.980
<v Speaker 1>but representing it as a set of counts </v>
<v Speaker 1>in language.</v>

76
00:04:17.981 --> 00:04:20.560
<v Speaker 1>This representation is also known as a </v>
<v Speaker 1>bag of words.</v>

77
00:04:20.950 --> 00:04:25.950
<v Speaker 1>All this is is a vector in which each </v>
<v Speaker 1>slot represents a word and the number in</v>

78
00:04:25.961 --> 00:04:29.890
<v Speaker 1>that slot represents the number of times</v>
<v Speaker 1>that that word occurs in the sentence.</v>

79
00:04:30.250 --> 00:04:32.530
<v Speaker 1>So here,</v>
<v Speaker 1>the second slot represents the word this</v>

80
00:04:32.650 --> 00:04:35.380
<v Speaker 1>and there's a one because this appears </v>
<v Speaker 1>once in the sentence.</v>

81
00:04:36.010 --> 00:04:38.980
<v Speaker 1>Now we have fixed length factor.</v>
<v Speaker 1>No matter how many words we have,</v>

82
00:04:39.250 --> 00:04:42.370
<v Speaker 1>the vector will always be the same size </v>
<v Speaker 1>as the council just be different.</v>

83
00:04:43.150 --> 00:04:46.450
<v Speaker 1>We can feed this into a machine learning</v>
<v Speaker 1>model and try to make a prediction.</v>

84
00:04:47.140 --> 00:04:47.973
<v Speaker 1>The problem you may be noticing here is </v>
<v Speaker 1>that we're losing all of the sequential </v>

85
00:04:50.951 --> 00:04:51.784
<v Speaker 1>information.</v>
<v Speaker 1>These counts don't preserve any order </v>

86
00:04:54.371 --> 00:04:57.730
<v Speaker 1>that we had in the sequence to see why </v>
<v Speaker 1>this is really bad.</v>

87
00:04:57.730 --> 00:05:00.250
<v Speaker 1>Consider these two sentences.</v>
<v Speaker 1>The food was good,</v>

88
00:05:00.280 --> 00:05:04.330
<v Speaker 1>not bad at all versus the food was bad.</v>
<v Speaker 1>Not good at all.</v>

89
00:05:04.810 --> 00:05:05.643
<v Speaker 1>These are completely opposite sentences,</v>
<v Speaker 1>but their bag of words representation </v>

90
00:05:08.770 --> 00:05:12.880
<v Speaker 1>would be exactly the same because they </v>
<v Speaker 1>contain the same set of words.</v>

91
00:05:13.870 --> 00:05:16.690
<v Speaker 1>So by representing our sentence as </v>
<v Speaker 1>counts,</v>

92
00:05:16.720 --> 00:05:18.550
<v Speaker 1>we're losing all of the sequential </v>
<v Speaker 1>information,</v>

93
00:05:18.551 --> 00:05:21.220
<v Speaker 1>which is really important because we're </v>
<v Speaker 1>trying to model sequences.</v>

94
00:05:22.600 --> 00:05:23.411
<v Speaker 1>Okay,</v>
<v Speaker 1>so what do we know?</v>

95
00:05:23.411 --> 00:05:26.590
<v Speaker 1>Now we want to preserve order in the </v>
<v Speaker 1>sequence,</v>

96
00:05:26.830 --> 00:05:29.620
<v Speaker 1>but we also don't want to cut it off to </v>
<v Speaker 1>um,</v>

97
00:05:29.650 --> 00:05:32.200
<v Speaker 1>to a very short length.</v>
<v Speaker 1>You might be saying,</v>

98
00:05:32.201 --> 00:05:34.480
<v Speaker 1>well,</v>
<v Speaker 1>why don't we just use a really big fixed</v>

99
00:05:34.481 --> 00:05:35.314
<v Speaker 1>window?</v>
<v Speaker 1>Before we were having issues because we </v>

100
00:05:36.491 --> 00:05:37.324
<v Speaker 1>were just using a fixed window of size </v>
<v Speaker 1>to what if we extended that to be a </v>

101
00:05:40.331 --> 00:05:41.164
<v Speaker 1>fixed window of size seven.</v>
<v Speaker 1>And we think that by looking at seven </v>

102
00:05:43.661 --> 00:05:44.494
<v Speaker 1>words,</v>
<v Speaker 1>we can get most of the context that we </v>

103
00:05:45.491 --> 00:05:46.870
<v Speaker 1>need.</v>
<v Speaker 1>Well,</v>

104
00:05:46.871 --> 00:05:47.261
<v Speaker 1>yeah,</v>
<v Speaker 1>okay,</v>

105
00:05:47.261 --> 00:05:48.094
<v Speaker 1>we can do that.</v>
<v Speaker 1>Now we have another fixed length vector </v>

106
00:05:50.141 --> 00:05:51.340
<v Speaker 1>just like before.</v>
<v Speaker 1>It's bigger,</v>

107
00:05:51.341 --> 00:05:52.174
<v Speaker 1>but it's still fixed length.</v>
<v Speaker 1>We have allocated space for each of the </v>

108
00:05:53.831 --> 00:05:56.410
<v Speaker 1>seven words.</v>
<v Speaker 1>We can feed this into a model and try to</v>

109
00:05:56.411 --> 00:05:57.250
<v Speaker 1>make a prediction.</v>

110
00:05:58.710 --> 00:06:02.940
<v Speaker 1>The problem here is that I consider this</v>
<v Speaker 1>in the scenario where we're feeding this</v>

111
00:06:02.941 --> 00:06:05.520
<v Speaker 1>input factor into a feed forward neural </v>
<v Speaker 1>network.</v>

112
00:06:06.150 --> 00:06:06.983
<v Speaker 1>Each of those inputs,</v>
<v Speaker 1>each of those ones and zeroes has a </v>

113
00:06:09.181 --> 00:06:11.460
<v Speaker 1>separate weight connecting it to the </v>
<v Speaker 1>network.</v>

114
00:06:12.270 --> 00:06:15.540
<v Speaker 1>If we see the words this morning at the </v>
<v Speaker 1>beginning of the sentence,</v>

115
00:06:15.600 --> 00:06:17.880
<v Speaker 1>very,</v>
<v Speaker 1>very commonly the network,</v>

116
00:06:17.881 --> 00:06:20.910
<v Speaker 1>we'll learn that this morning represents</v>
<v Speaker 1>a time where a setting,</v>

117
00:06:21.870 --> 00:06:24.690
<v Speaker 1>if this morning then appears at the end </v>
<v Speaker 1>of the sentence,</v>

118
00:06:24.960 --> 00:06:25.793
<v Speaker 1>we'll have a lot of trouble recognizing </v>
<v Speaker 1>that because the weights at the end of </v>

119
00:06:28.831 --> 00:06:29.664
<v Speaker 1>the vector never saw that phrase before </v>
<v Speaker 1>and the weights from the beginning of </v>

120
00:06:33.811 --> 00:06:35.910
<v Speaker 1>the vector and not being shared with the</v>
<v Speaker 1>end.</v>

121
00:06:37.870 --> 00:06:40.180
<v Speaker 1>In other words,</v>
<v Speaker 1>things we learn about the sequence won't</v>

122
00:06:40.181 --> 00:06:43.660
<v Speaker 1>transfer if they appear at different </v>
<v Speaker 1>points in the sequence because we're not</v>

123
00:06:43.661 --> 00:06:44.950
<v Speaker 1>sharing any parameters.</v>

124
00:06:47.790 --> 00:06:50.730
<v Speaker 1>All right,</v>
<v Speaker 1>so you kind of see all the problems that</v>

125
00:06:50.731 --> 00:06:53.280
<v Speaker 1>arise with sequences now and why we need</v>
<v Speaker 1>a different framework.</v>

126
00:06:53.281 --> 00:06:55.890
<v Speaker 1>Specifically,</v>
<v Speaker 1>we want to be able to deal with variable</v>

127
00:06:55.891 --> 00:06:56.724
<v Speaker 1>length sequences.</v>
<v Speaker 1>We want to maintain sequence orders so </v>

128
00:06:59.611 --> 00:07:01.800
<v Speaker 1>we can keep all of that sequential </v>
<v Speaker 1>control information.</v>

129
00:07:02.310 --> 00:07:03.143
<v Speaker 1>We want to keep track of longer term </v>
<v Speaker 1>dependencies rather than cutting it off </v>

130
00:07:06.151 --> 00:07:06.984
<v Speaker 1>too short and we want to be able to </v>
<v Speaker 1>share parameters across the sequence so </v>

131
00:07:10.980 --> 00:07:11.813
<v Speaker 1>we don't have to relearn things across </v>
<v Speaker 1>the sequence because this is a class </v>

132
00:07:14.731 --> 00:07:15.564
<v Speaker 1>about deep learning.</v>
<v Speaker 1>I'm going to talk about how to address </v>

133
00:07:17.041 --> 00:07:19.770
<v Speaker 1>these problems with neural networks,</v>
<v Speaker 1>but no,</v>

134
00:07:19.771 --> 00:07:20.604
<v Speaker 1>that time series modeling and sequential</v>
<v Speaker 1>modeling is a very active field in </v>

135
00:07:24.391 --> 00:07:25.224
<v Speaker 1>machine learning and,</v>
<v Speaker 1>and it has been and there are lots of </v>

136
00:07:26.731 --> 00:07:27.564
<v Speaker 1>other machine learning methods that have</v>
<v Speaker 1>been developed to deal with these </v>

137
00:07:29.131 --> 00:07:29.964
<v Speaker 1>problems.</v>
<v Speaker 1>But for now all talk about recurrent </v>

138
00:07:32.341 --> 00:07:33.174
<v Speaker 1>neural networks.</v>

139
00:07:35.860 --> 00:07:36.180
<v Speaker 2>Okay.</v>

140
00:07:36.180 --> 00:07:37.013
<v Speaker 1>Okay.</v>
<v Speaker 1>So a recurrent neural network is </v>

141
00:07:38.790 --> 00:07:42.150
<v Speaker 1>architected in the same way as a normal </v>
<v Speaker 1>neural network.</v>

142
00:07:42.151 --> 00:07:42.984
<v Speaker 1>We have some inputs,</v>
<v Speaker 1>we have some hidden layers and we have </v>

143
00:07:45.780 --> 00:07:48.990
<v Speaker 1>some outputs.</v>
<v Speaker 1>The only difference is that each hitting</v>

144
00:07:48.991 --> 00:07:51.740
<v Speaker 1>unit is doing a slightly different </v>
<v Speaker 1>functions.</v>

145
00:07:51.741 --> 00:07:55.350
<v Speaker 1>So let's take a look at this one hidden </v>
<v Speaker 1>unit to see exactly what it's doing.</v>

146
00:07:58.380 --> 00:08:03.380
<v Speaker 1>Ever current hidden unit computes a </v>
<v Speaker 1>function of an input and it's own.</v>

147
00:08:03.511 --> 00:08:08.511
<v Speaker 1>Previous output it's own previous output</v>
<v Speaker 1>is also known as the cell state.</v>

148
00:08:09.030 --> 00:08:13.400
<v Speaker 1>And in the diagram it's denoted by s the</v>
<v Speaker 1>subscript is the time timestamp.</v>

149
00:08:14.070 --> 00:08:16.650
<v Speaker 1>So at the very first time stuff t equals</v>
<v Speaker 1>zero.</v>

150
00:08:17.490 --> 00:08:18.323
<v Speaker 1>The recurrent current unit computes a </v>
<v Speaker 1>function of the input at t equals zero </v>

151
00:08:22.410 --> 00:08:27.410
<v Speaker 1>and of its initial state.</v>
<v Speaker 1>Similarly at the next time stop,</v>

152
00:08:27.960 --> 00:08:32.070
<v Speaker 1>it computes a function of the new input </v>
<v Speaker 1>and it's previous cell state.</v>

153
00:08:33.270 --> 00:08:34.980
<v Speaker 1>If you look at the function at the </v>
<v Speaker 1>bottom,</v>

154
00:08:34.981 --> 00:08:35.814
<v Speaker 1>the function to compute as to you'll see</v>
<v Speaker 1>it's really similar to the function for </v>

155
00:08:39.180 --> 00:08:41.220
<v Speaker 1>that a hidden unit in a feed forward </v>
<v Speaker 1>network,</v>

156
00:08:41.550 --> 00:08:42.720
<v Speaker 1>um,</v>
<v Speaker 1>computes.</v>

157
00:08:43.410 --> 00:08:46.980
<v Speaker 1>The only difference is that we're adding</v>
<v Speaker 1>in an additional term to incorporate its</v>

158
00:08:46.981 --> 00:08:47.814
<v Speaker 1>own previous state.</v>
<v Speaker 1>Common way of viewing recurrent neural </v>

159
00:08:52.831 --> 00:08:55.770
<v Speaker 1>networks is by unfolding them across </v>
<v Speaker 1>time.</v>

160
00:08:56.190 --> 00:08:59.130
<v Speaker 1>So this is the same hidden unit at </v>
<v Speaker 1>different points in time.</v>

161
00:08:59.790 --> 00:09:04.380
<v Speaker 1>Here you can see that at every point in </v>
<v Speaker 1>time it takes us input it's own previous</v>

162
00:09:04.381 --> 00:09:07.380
<v Speaker 1>state and the new input at that time </v>
<v Speaker 1>step.</v>

163
00:09:09.400 --> 00:09:10.233
<v Speaker 1>One thing to notice here is that </v>
<v Speaker 1>throughout the sequence we're using the </v>

164
00:09:12.941 --> 00:09:15.150
<v Speaker 1>same weight matrices.</v>
<v Speaker 1>W and u.</v>

165
00:09:16.180 --> 00:09:18.190
<v Speaker 1>This solves our problem,</v>
<v Speaker 1>a parameter sharing.</v>

166
00:09:18.640 --> 00:09:21.100
<v Speaker 1>We don't have new parameters for every </v>
<v Speaker 1>point in the sequence.</v>

167
00:09:21.101 --> 00:09:21.934
<v Speaker 1>Once we learned something,</v>
<v Speaker 1>it'll can apply at any point in the </v>

168
00:09:23.531 --> 00:09:24.364
<v Speaker 1>sequence.</v>
<v Speaker 1>This also helps us deal with variable </v>

169
00:09:26.531 --> 00:09:30.100
<v Speaker 1>lengths sequences because we're not pre </v>
<v Speaker 1>specifying the length of the sequence.</v>

170
00:09:30.610 --> 00:09:33.220
<v Speaker 1>We don't have separate parameters for </v>
<v Speaker 1>every point in the sequence.</v>

171
00:09:33.221 --> 00:09:34.054
<v Speaker 1>So in some cases we can unroll this rnn </v>
<v Speaker 1>to four time steps and other cases we </v>

172
00:09:38.381 --> 00:09:40.090
<v Speaker 1>can unroll it to 10 times steps.</v>

173
00:09:41.870 --> 00:09:42.460
<v Speaker 2>Yeah.</v>

174
00:09:42.460 --> 00:09:44.430
<v Speaker 1>Final thing to notice is that Essa,</v>
<v Speaker 1>Ben,</v>

175
00:09:44.460 --> 00:09:45.293
<v Speaker 1>the cell state at time n can contain </v>
<v Speaker 1>information from all of the past time </v>

176
00:09:49.051 --> 00:09:49.884
<v Speaker 1>steps.</v>
<v Speaker 1>Notice that each cell state is a </v>

177
00:09:52.351 --> 00:09:53.184
<v Speaker 1>function of the previous cell state,</v>
<v Speaker 1>which is the function which is a </v>

178
00:09:55.111 --> 00:09:57.030
<v Speaker 1>function of the previous cell state and </v>
<v Speaker 1>so on.</v>

179
00:09:57.780 --> 00:09:58.613
<v Speaker 1>So this kind of solves our issue of </v>
<v Speaker 1>longterm dependencies because at a time </v>

180
00:10:02.461 --> 00:10:03.294
<v Speaker 1>step,</v>
<v Speaker 1>very far in the future that sal state </v>

181
00:10:06.931 --> 00:10:10.020
<v Speaker 1>encompasses information about all of the</v>
<v Speaker 1>previous cell states.</v>

182
00:10:13.460 --> 00:10:14.293
<v Speaker 1>All right,</v>
<v Speaker 1>so now that you kind of understand what </v>

183
00:10:16.780 --> 00:10:19.420
<v Speaker 1>a recurrent neural network is,</v>
<v Speaker 1>and just to clarify,</v>

184
00:10:19.421 --> 00:10:21.670
<v Speaker 1>actually shown you one hitting unit in </v>
<v Speaker 1>the previous slide,</v>

185
00:10:21.671 --> 00:10:23.860
<v Speaker 1>but in a full network you would have </v>
<v Speaker 1>many,</v>

186
00:10:23.861 --> 00:10:27.490
<v Speaker 1>many of those hidden units and even many</v>
<v Speaker 1>layers of many hidden units.</v>

187
00:10:28.540 --> 00:10:31.960
<v Speaker 1>So now we can talk about how you would </v>
<v Speaker 1>train a recurrent neural network.</v>

188
00:10:33.160 --> 00:10:35.980
<v Speaker 1>It's really similar to how you train a </v>
<v Speaker 1>normal neuro network.</v>

189
00:10:35.981 --> 00:10:36.814
<v Speaker 1>It's backpropagation.</v>
<v Speaker 1>There's just an additional time </v>

190
00:10:38.651 --> 00:10:43.180
<v Speaker 1>dementia.</v>
<v Speaker 1>As a reminder in backpropagation,</v>

191
00:10:43.420 --> 00:10:48.010
<v Speaker 1>we want to find the parameters that </v>
<v Speaker 1>minimize some loss function.</v>

192
00:10:48.370 --> 00:10:49.203
<v Speaker 1>The way that we do this is by first </v>
<v Speaker 1>taking the derivative of the loss with </v>

193
00:10:52.511 --> 00:10:53.344
<v Speaker 1>respect to each of the parameters and </v>
<v Speaker 1>then shifting the parameters in the </v>

194
00:10:56.651 --> 00:11:00.250
<v Speaker 1>opposite direction in order to try and </v>
<v Speaker 1>minimize the loss.</v>

195
00:11:01.000 --> 00:11:01.833
<v Speaker 1>This process is called gradient descent.</v>
<v Speaker 1>So one difference with rns is that we </v>

196
00:11:08.221 --> 00:11:09.054
<v Speaker 1>have many time steps so we can produce </v>
<v Speaker 1>an output at every time step because we </v>

197
00:11:13.381 --> 00:11:14.214
<v Speaker 1>have an output at every time step.</v>
<v Speaker 1>We can have a loss at every time step </v>

198
00:11:17.760 --> 00:11:20.130
<v Speaker 1>rather than just one single loss of fee </v>
<v Speaker 1>end.</v>

199
00:11:21.520 --> 00:11:22.353
<v Speaker 1>Because,</v>
<v Speaker 1>and the way that we deal with this is </v>

200
00:11:23.251 --> 00:11:24.084
<v Speaker 1>pretty simple.</v>
<v Speaker 1>The total loss is just the sum of the </v>

201
00:11:26.131 --> 00:11:29.190
<v Speaker 1>losses that every time stuff.</v>
<v Speaker 1>Similarly,</v>

202
00:11:29.191 --> 00:11:33.480
<v Speaker 1>the total gradient is just the sum of </v>
<v Speaker 1>the gradients at every time step.</v>

203
00:11:36.530 --> 00:11:37.363
<v Speaker 1>So we can try this out by walking </v>
<v Speaker 1>through this gradient computation for a </v>

204
00:11:41.841 --> 00:11:42.674
<v Speaker 1>single parameter.</v>
<v Speaker 1>W W is the weight matrix that we're </v>

205
00:11:45.201 --> 00:11:49.220
<v Speaker 1>multiplying by our inputs.</v>
<v Speaker 1>We know that the total loss,</v>

206
00:11:49.550 --> 00:11:50.383
<v Speaker 1>the the total gradient,</v>
<v Speaker 1>so the derivative of loss with respect </v>

207
00:11:52.911 --> 00:11:56.710
<v Speaker 1>to will be the sum of the gradients at </v>
<v Speaker 1>every time step.</v>

208
00:11:57.220 --> 00:12:01.210
<v Speaker 1>So for now we can focus on a single time</v>
<v Speaker 1>step knowing that at the end we would do</v>

209
00:12:01.211 --> 00:12:04.720
<v Speaker 1>this for each of the time steps and then</v>
<v Speaker 1>sum them up to get the total gradient.</v>

210
00:12:06.130 --> 00:12:07.450
<v Speaker 1>So let's take time.</v>
<v Speaker 1>Step two,</v>

211
00:12:08.320 --> 00:12:10.790
<v Speaker 1>we can solve this gradient using the </v>
<v Speaker 1>chain rule.</v>

212
00:12:11.110 --> 00:12:11.943
<v Speaker 1>So the derivative of the loss with </v>
<v Speaker 1>respect to w is the derivative of the </v>

213
00:12:15.431 --> 00:12:16.264
<v Speaker 1>loss with respect to the output,</v>
<v Speaker 1>the derivative of the output with </v>

214
00:12:19.301 --> 00:12:20.134
<v Speaker 1>respect to the south state at time two </v>
<v Speaker 1>and then the derivative of the cell </v>

215
00:12:23.321 --> 00:12:24.730
<v Speaker 1>state.</v>
<v Speaker 1>With respect to w.</v>

216
00:12:25.720 --> 00:12:26.553
<v Speaker 1>So this seems fine,</v>
<v Speaker 1>but let's take a closer look at this </v>

217
00:12:29.651 --> 00:12:34.651
<v Speaker 1>last term.</v>
<v Speaker 1>You'll notice that as to also depends on</v>

218
00:12:35.111 --> 00:12:40.111
<v Speaker 1>s one and s one also depends on w.</v>
<v Speaker 1>So we can't just leave that last term as</v>

219
00:12:40.841 --> 00:12:41.674
<v Speaker 1>a constant.</v>
<v Speaker 1>We actually have to expand it out </v>

220
00:12:43.481 --> 00:12:45.250
<v Speaker 1>farther.</v>
<v Speaker 1>Okay.</v>

221
00:12:45.251 --> 00:12:46.084
<v Speaker 1>So how do we expand this out farther?</v>
<v Speaker 1>What we really want to know is how </v>

222
00:12:49.271 --> 00:12:53.380
<v Speaker 1>exactly does the cell state at time step</v>
<v Speaker 1>to depend on w?</v>

223
00:12:55.250 --> 00:12:58.010
<v Speaker 1>Well it depends directly on w because it</v>
<v Speaker 1>feeds right in.</v>

224
00:12:58.460 --> 00:12:59.293
<v Speaker 1>We also saw that as two depends on s one</v>
<v Speaker 1>which depends on w and you can also see </v>

225
00:13:04.191 --> 00:13:09.020
<v Speaker 1>that s two depends on ss zero,</v>
<v Speaker 1>which also depends on w in other words.</v>

226
00:13:10.100 --> 00:13:13.220
<v Speaker 1>And here I'm just writing it as a </v>
<v Speaker 1>summation and those uh,</v>

227
00:13:13.230 --> 00:13:14.063
<v Speaker 1>the,</v>
<v Speaker 1>the sun that you saw on the previous </v>

228
00:13:16.610 --> 00:13:17.443
<v Speaker 1>slide as a summation form and you can </v>
<v Speaker 1>see that the last two terms are </v>

229
00:13:20.571 --> 00:13:21.404
<v Speaker 1>basically summing the contributions of w</v>
<v Speaker 1>in previous time stops to the error at </v>

230
00:13:26.451 --> 00:13:27.320
<v Speaker 1>time.</v>
<v Speaker 1>Step t,</v>

231
00:13:28.400 --> 00:13:31.910
<v Speaker 1>this is key to how we model longer term </v>
<v Speaker 1>dependencies.</v>

232
00:13:32.420 --> 00:13:35.120
<v Speaker 1>This gradient is how we shift our </v>
<v Speaker 1>parameters and our parameters.</v>

233
00:13:35.121 --> 00:13:35.954
<v Speaker 1>Define our network by shifting our </v>
<v Speaker 1>parameters such that they include </v>

234
00:13:40.640 --> 00:13:45.640
<v Speaker 1>contributions to the error from past </v>
<v Speaker 1>time stops there,</v>

235
00:13:45.861 --> 00:13:48.530
<v Speaker 1>shifted to model longer term </v>
<v Speaker 1>dependencies.</v>

236
00:13:51.590 --> 00:13:53.780
<v Speaker 1>And here I'm just writing it as a </v>
<v Speaker 1>general,</v>

237
00:13:53.781 --> 00:13:55.370
<v Speaker 1>some not just for time stuff too.</v>

238
00:13:58.090 --> 00:13:58.380
<v Speaker 2>Okay.</v>

239
00:13:58.380 --> 00:13:59.280
<v Speaker 1>Um,</v>
<v Speaker 1>okay.</v>

240
00:13:59.281 --> 00:14:02.850
<v Speaker 1>So this is basically the process of </v>
<v Speaker 1>backpropagation to tap through time.</v>

241
00:14:03.150 --> 00:14:03.983
<v Speaker 1>You would do this for every parameter in</v>
<v Speaker 1>your network and then use that in the </v>

242
00:14:07.171 --> 00:14:08.280
<v Speaker 1>process of gradient descent.</v>

243
00:14:09.910 --> 00:14:10.540
<v Speaker 2>Yeah.</v>

244
00:14:10.540 --> 00:14:13.300
<v Speaker 1>In practice,</v>
<v Speaker 1>rns are a bit difficult to train,</v>

245
00:14:13.870 --> 00:14:18.300
<v Speaker 1>so I kind of want to go through why that</v>
<v Speaker 1>is and um,</v>

246
00:14:18.940 --> 00:14:19.773
<v Speaker 1>what some ways,</v>
<v Speaker 1>some ways that we can address these </v>

247
00:14:21.461 --> 00:14:25.840
<v Speaker 1>issues.</v>
<v Speaker 1>So let's go back to the summation.</v>

248
00:14:25.990 --> 00:14:26.823
<v Speaker 1>As a reminder,</v>
<v Speaker 1>this is the derivative of the loss with </v>

249
00:14:29.441 --> 00:14:33.430
<v Speaker 1>respect to w and this is what we would </v>
<v Speaker 1>use to shift our parameters.</v>

250
00:14:33.460 --> 00:14:34.293
<v Speaker 1>W The last two terms are considering the</v>
<v Speaker 1>air of wwe at all of the previous time </v>

251
00:14:42.991 --> 00:14:45.880
<v Speaker 1>steps.</v>
<v Speaker 1>Let's take a look at this one term.</v>

252
00:14:45.910 --> 00:14:49.150
<v Speaker 1>This is how we,</v>
<v Speaker 1>this is the derivative of the cell state</v>

253
00:14:49.151 --> 00:14:49.901
<v Speaker 1>at time.</v>
<v Speaker 1>Step two,</v>

254
00:14:49.901 --> 00:14:51.860
<v Speaker 1>with respect to each of the cell states.</v>

255
00:14:53.000 --> 00:14:53.400
<v Speaker 2>Okay.</v>

256
00:14:53.400 --> 00:14:57.150
<v Speaker 1>You might notice that this itself is </v>
<v Speaker 1>also a chain rule because as two depends</v>

257
00:14:57.151 --> 00:15:00.600
<v Speaker 1>on s one and s one depends on a zero.</v>
<v Speaker 1>We can expand this out farther.</v>

258
00:15:01.650 --> 00:15:02.483
<v Speaker 1>This is just for the derivative of s two</v>
<v Speaker 1>with respect to a zero but what if we </v>

259
00:15:05.761 --> 00:15:06.594
<v Speaker 1>were looking at a time step very far in </v>
<v Speaker 1>the future like time step n that term </v>

260
00:15:11.881 --> 00:15:16.881
<v Speaker 1>would expand into a product of an terms </v>
<v Speaker 1>and okay,</v>

261
00:15:17.551 --> 00:15:18.384
<v Speaker 1>you might be thinking so what?</v>
<v Speaker 1>Well as notice that as the gap between </v>

262
00:15:22.411 --> 00:15:27.000
<v Speaker 1>time stops gets bigger and bigger,</v>
<v Speaker 1>this product in the gradient gets longer</v>

263
00:15:27.001 --> 00:15:30.570
<v Speaker 1>and longer and if we look at each of </v>
<v Speaker 1>these terms,</v>

264
00:15:30.770 --> 00:15:32.040
<v Speaker 1>what,</v>
<v Speaker 1>what are each of these terms?</v>

265
00:15:32.041 --> 00:15:35.850
<v Speaker 1>They all kind of take the same form.</v>
<v Speaker 1>It's the derivative of a cell state with</v>

266
00:15:35.851 --> 00:15:36.684
<v Speaker 1>respect to the previous cell state.</v>
<v Speaker 1>That term can be written like this and </v>

267
00:15:43.581 --> 00:15:47.150
<v Speaker 1>the actual form that actual formula </v>
<v Speaker 1>isn't that important.</v>

268
00:15:47.151 --> 00:15:50.600
<v Speaker 1>Just notice that it's a product of two </v>
<v Speaker 1>terms,</v>

269
00:15:50.800 --> 00:15:55.800
<v Speaker 1>ws and F primes.</v>
<v Speaker 1>W's are our weight matrices.</v>

270
00:15:57.520 --> 00:16:00.610
<v Speaker 1>These are sampled mostly from a standard</v>
<v Speaker 1>normal distribution,</v>

271
00:16:00.970 --> 00:16:01.803
<v Speaker 1>so most of the terms will be less than </v>
<v Speaker 1>one f prime is the derivative of our </v>

272
00:16:05.741 --> 00:16:09.580
<v Speaker 1>activation function.</v>
<v Speaker 1>If we use an activation function such as</v>

273
00:16:09.581 --> 00:16:13.840
<v Speaker 1>the hyperbolic tangent or a sigmoid f,</v>
<v Speaker 1>prime will always be less than one.</v>

274
00:16:16.010 --> 00:16:16.650
<v Speaker 2>Okay.</v>

275
00:16:16.650 --> 00:16:19.140
<v Speaker 1>In other words,</v>
<v Speaker 1>we're multiplying a lot of small numbers</v>

276
00:16:19.141 --> 00:16:23.220
<v Speaker 1>together in this product.</v>
<v Speaker 1>Okay,</v>

277
00:16:23.310 --> 00:16:27.870
<v Speaker 1>so what does this mean?</v>
<v Speaker 1>Basically it recall that this product is</v>

278
00:16:27.871 --> 00:16:32.871
<v Speaker 1>how we're adding the gradient from </v>
<v Speaker 1>future time steps to the gradient.</v>

279
00:16:33.630 --> 00:16:34.463
<v Speaker 1>Sorry.</v>
<v Speaker 1>How we're adding the gradient from </v>

280
00:16:34.981 --> 00:16:38.040
<v Speaker 1>pastime steps to the gradient at a </v>
<v Speaker 1>future time stuff.</v>

281
00:16:38.720 --> 00:16:39.553
<v Speaker 2>Okay.</v>

282
00:16:40.100 --> 00:16:40.933
<v Speaker 1>What's happening then is that errors due</v>
<v Speaker 1>to further and further back time stops </v>

283
00:16:44.360 --> 00:16:45.193
<v Speaker 1>have increasingly smaller gradients </v>
<v Speaker 1>because that product for further back </v>

284
00:16:48.681 --> 00:16:52.640
<v Speaker 1>time stops will be longer and since the </v>
<v Speaker 1>numbers are all decimals,</v>

285
00:16:52.670 --> 00:16:53.780
<v Speaker 1>there'll be,</v>
<v Speaker 1>it will be,</v>

286
00:16:53.781 --> 00:16:55.670
<v Speaker 1>it will.</v>
<v Speaker 1>It will become increasingly smaller.</v>

287
00:16:58.880 --> 00:16:59.713
<v Speaker 1>What does,</v>
<v Speaker 1>what this ends up meaning at a high </v>

288
00:17:02.361 --> 00:17:03.194
<v Speaker 1>level is that our parameters will become</v>
<v Speaker 1>biased to capture a shorter term </v>

289
00:17:05.931 --> 00:17:06.764
<v Speaker 1>dependencies.</v>
<v Speaker 1>The errors that arise from further and </v>

290
00:17:09.201 --> 00:17:10.034
<v Speaker 1>further back time stops will be harder </v>
<v Speaker 1>and harder to propagate into the </v>

291
00:17:12.951 --> 00:17:17.951
<v Speaker 1>gradient at future time stops.</v>
<v Speaker 1>Recall that recall,</v>

292
00:17:19.081 --> 00:17:20.910
<v Speaker 1>this example that I showed at the </v>
<v Speaker 1>beginning,</v>

293
00:17:22.110 --> 00:17:22.943
<v Speaker 1>the whole point of using recurrent </v>
<v Speaker 1>neural networks is because we wanted to </v>

294
00:17:25.561 --> 00:17:26.394
<v Speaker 1>model longterm dependencies,</v>
<v Speaker 1>but if our parameters are biased to </v>

295
00:17:28.981 --> 00:17:32.220
<v Speaker 1>capture short term dependencies,</v>
<v Speaker 1>even if they see the whole sequence,</v>

296
00:17:32.221 --> 00:17:35.520
<v Speaker 1>there'll be by the parameters will </v>
<v Speaker 1>become biased to to predict things based</v>

297
00:17:35.521 --> 00:17:40.320
<v Speaker 1>mostly on the past couple of words.</v>
<v Speaker 1>Um,</v>

298
00:17:40.980 --> 00:17:44.580
<v Speaker 1>okay,</v>
<v Speaker 1>so now I'm going to go through some,</v>

299
00:17:44.610 --> 00:17:49.610
<v Speaker 1>a couple methods that are used to </v>
<v Speaker 1>address this issue in practice that work</v>

300
00:17:49.721 --> 00:17:50.554
<v Speaker 1>pretty well.</v>

301
00:17:57.270 --> 00:17:59.310
<v Speaker 1>The first one is the choice of </v>
<v Speaker 1>activation function.</v>

302
00:18:00.060 --> 00:18:03.660
<v Speaker 1>So you saw that one of the terms that </v>
<v Speaker 1>was making that product really small was</v>

303
00:18:03.661 --> 00:18:07.530
<v Speaker 1>the f prime term.</v>
<v Speaker 1>I've crime is the derivative of whatever</v>

304
00:18:07.531 --> 00:18:08.364
<v Speaker 1>activation function we choose to use.</v>
<v Speaker 1>Here I've plotted the derivatives of </v>

305
00:18:12.121 --> 00:18:12.954
<v Speaker 1>some common activation functions.</v>
<v Speaker 1>You could see that the derivative of </v>

306
00:18:15.421 --> 00:18:18.270
<v Speaker 1>hyperbolic tangent and sigmoid is always</v>
<v Speaker 1>less than one.</v>

307
00:18:18.271 --> 00:18:20.670
<v Speaker 1>In fact for sigmoid is always less than </v>
<v Speaker 1>0.25</v>

308
00:18:21.380 --> 00:18:24.540
<v Speaker 1>and instead we choose to use an </v>
<v Speaker 1>activation function like rarely.</v>

309
00:18:24.570 --> 00:18:25.403
<v Speaker 1>It's always one above zero so that will </v>
<v Speaker 1>at least prevent the f prime terms from </v>

310
00:18:29.851 --> 00:18:30.684
<v Speaker 1>shrinking the gradient.</v>

311
00:18:32.120 --> 00:18:32.420
<v Speaker 2>Okay.</v>

312
00:18:32.420 --> 00:18:34.970
<v Speaker 1>Another solution would be how we </v>
<v Speaker 1>initialize our weights.</v>

313
00:18:35.660 --> 00:18:37.910
<v Speaker 1>If we initialize the weights from a </v>
<v Speaker 1>normal distribution,</v>

314
00:18:37.911 --> 00:18:38.744
<v Speaker 1>there'll be mostly less than one and </v>
<v Speaker 1>they'll immediately strength the </v>

315
00:18:40.501 --> 00:18:41.334
<v Speaker 1>gradients.</v>
<v Speaker 1>If instead we initialize the weights to </v>

316
00:18:44.511 --> 00:18:45.344
<v Speaker 1>something like the identity matrix,</v>
<v Speaker 1>it will at least prevent that w term </v>

317
00:18:49.160 --> 00:18:52.220
<v Speaker 1>from shrinking the product at least at </v>
<v Speaker 1>the beginning.</v>

318
00:18:54.460 --> 00:18:56.650
<v Speaker 1>The next deletion is very different.</v>
<v Speaker 1>Um,</v>

319
00:18:56.800 --> 00:18:57.633
<v Speaker 1>it involves actually adding a lot more </v>
<v Speaker 1>complexity to the network using a more </v>

320
00:19:01.901 --> 00:19:02.734
<v Speaker 1>complex type of cell called a gated sell</v>
<v Speaker 1>rather than here rather than each node </v>

321
00:19:07.271 --> 00:19:10.510
<v Speaker 1>just being that simple rnn unit that I </v>
<v Speaker 1>showed at the beginning,</v>

322
00:19:10.990 --> 00:19:13.350
<v Speaker 1>we'll replace it with a much more </v>
<v Speaker 1>complicated cell.</v>

323
00:19:13.980 --> 00:19:14.813
<v Speaker 1>A very common gated cell is something </v>
<v Speaker 1>called an Lstm or long short term </v>

324
00:19:18.191 --> 00:19:19.024
<v Speaker 1>memory.</v>
<v Speaker 1>So like its name implies lstm cells are </v>

325
00:19:21.791 --> 00:19:22.624
<v Speaker 1>able to keep memory within the cell </v>
<v Speaker 1>state unchanged for many times stuffs </v>

326
00:19:27.130 --> 00:19:30.580
<v Speaker 1>this allows them to effectively model </v>
<v Speaker 1>longer term dependencies.</v>

327
00:19:31.740 --> 00:19:35.970
<v Speaker 1>So I'm going to go through a very high </v>
<v Speaker 1>level overview of how lsts work,</v>

328
00:19:36.060 --> 00:19:36.893
<v Speaker 1>but if you're interested,</v>
<v Speaker 1>feel free to email me or ask me </v>

329
00:19:38.671 --> 00:19:42.660
<v Speaker 1>afterwards and I can direct you to some </v>
<v Speaker 1>more resources to read about.</v>

330
00:19:42.661 --> 00:19:44.010
<v Speaker 1>LSTM is a lot more detail.</v>

331
00:19:45.100 --> 00:19:45.933
<v Speaker 2>Okay.</v>

332
00:19:45.980 --> 00:19:46.813
<v Speaker 1>All right.</v>
<v Speaker 1>So lstm basically have a three step </v>

333
00:19:49.101 --> 00:19:49.934
<v Speaker 1>process.</v>
<v Speaker 1>The first step is to forget your </v>

334
00:19:52.671 --> 00:19:55.370
<v Speaker 1>relevant parts of the cell state.</v>
<v Speaker 1>For example,</v>

335
00:19:55.371 --> 00:19:58.400
<v Speaker 1>if we're modeling a sentence and we see </v>
<v Speaker 1>a new subject,</v>

336
00:19:58.790 --> 00:19:59.623
<v Speaker 1>we might want to forget things about the</v>
<v Speaker 1>old subject because we know that future </v>

337
00:20:02.361 --> 00:20:05.360
<v Speaker 1>words will be conjugated according to </v>
<v Speaker 1>the new subject.</v>

338
00:20:06.770 --> 00:20:07.370
<v Speaker 2>Okay.</v>

339
00:20:07.370 --> 00:20:09.770
<v Speaker 1>The next day,</v>
<v Speaker 1>the next step is an update stuff.</v>

340
00:20:10.400 --> 00:20:13.670
<v Speaker 1>Here's where we actually update the cell</v>
<v Speaker 1>state to reflect the new,</v>

341
00:20:13.671 --> 00:20:16.730
<v Speaker 1>the information from the new input.</v>
<v Speaker 1>In this example,</v>

342
00:20:16.760 --> 00:20:18.710
<v Speaker 1>like I said,</v>
<v Speaker 1>if we've just seen a new subject,</v>

343
00:20:19.040 --> 00:20:19.873
<v Speaker 1>we might want to,</v>
<v Speaker 1>this is where we actually update the </v>

344
00:20:21.501 --> 00:20:25.040
<v Speaker 1>cell state with the gender or whether </v>
<v Speaker 1>the new subject is plural or singular.</v>

345
00:20:26.760 --> 00:20:27.120
<v Speaker 2>Yeah.</v>

346
00:20:27.120 --> 00:20:30.480
<v Speaker 1>Finally we want to output certain parts </v>
<v Speaker 1>of the cell state.</v>

347
00:20:31.710 --> 00:20:35.190
<v Speaker 1>So if we've just seen a subject,</v>
<v Speaker 1>we have an idea that the next word might</v>

348
00:20:35.191 --> 00:20:36.830
<v Speaker 1>be a verb.</v>
<v Speaker 1>So we'll,</v>

349
00:20:36.831 --> 00:20:41.100
<v Speaker 1>I'll put information relevant to </v>
<v Speaker 1>predicting a verb like the 10th.</v>

350
00:20:44.010 --> 00:20:48.580
<v Speaker 1>Each of these three steps is implemented</v>
<v Speaker 1>using a set of logic gates and the logic</v>

351
00:20:48.581 --> 00:20:50.650
<v Speaker 1>gates are implemented using sigma </v>
<v Speaker 1>functions.</v>

352
00:20:52.790 --> 00:20:53.623
<v Speaker 1>To give you some intuition on why Lstm </v>
<v Speaker 1>is help with the vanishing gradient </v>

353
00:20:56.271 --> 00:20:57.830
<v Speaker 1>problem,</v>
<v Speaker 1>uh,</v>

354
00:20:57.860 --> 00:20:58.693
<v Speaker 1>is that first the forget gate.</v>
<v Speaker 1>The first step can equivalently be </v>

355
00:21:02.541 --> 00:21:03.374
<v Speaker 1>called the remember gate because there </v>
<v Speaker 1>you're choosing what to forget and what </v>

356
00:21:06.711 --> 00:21:08.390
<v Speaker 1>to keep in the cell state.</v>

357
00:21:09.820 --> 00:21:14.290
<v Speaker 1>The forget gate can choose to keep </v>
<v Speaker 1>information in the cell state for many,</v>

358
00:21:14.291 --> 00:21:15.124
<v Speaker 1>many times steps.</v>
<v Speaker 1>There's no activation function or </v>

359
00:21:18.071 --> 00:21:20.410
<v Speaker 1>anything else shrinking that </v>
<v Speaker 1>information.</v>

360
00:21:21.850 --> 00:21:22.683
<v Speaker 1>The second step,</v>
<v Speaker 1>the second thing is that the cell state </v>

361
00:21:24.400 --> 00:21:27.210
<v Speaker 1>is separate from what's outputted.</v>
<v Speaker 1>We're mate.</v>

362
00:21:27.550 --> 00:21:28.383
<v Speaker 1>This is not true of normal recurrent </v>
<v Speaker 1>units like I showed you before in a </v>

363
00:21:33.221 --> 00:21:35.980
<v Speaker 1>simple recurrent unit.</v>
<v Speaker 1>The cell state is the same thing as what</v>

364
00:21:35.981 --> 00:21:40.420
<v Speaker 1>that sell outputs with an Lstm,</v>
<v Speaker 1>it has a separate cell state and it only</v>

365
00:21:40.421 --> 00:21:44.660
<v Speaker 1>needs to output information relevant to </v>
<v Speaker 1>the prediction at that time stuff.</v>

366
00:21:45.280 --> 00:21:46.113
<v Speaker 1>Because of this,</v>
<v Speaker 1>it can keep information in the cell </v>

367
00:21:47.651 --> 00:21:48.484
<v Speaker 1>state,</v>
<v Speaker 1>which might not be relevant at this </v>

368
00:21:49.811 --> 00:21:50.644
<v Speaker 1>time.</v>
<v Speaker 1>Stuff that might be relevant at a much </v>

369
00:21:51.791 --> 00:21:52.624
<v Speaker 1>later time step.</v>
<v Speaker 1>So we can keep that information without </v>

370
00:21:55.031 --> 00:21:57.610
<v Speaker 1>being penalized for that.</v>
<v Speaker 1>Finally,</v>

371
00:21:57.611 --> 00:21:59.680
<v Speaker 1>I didn't indicate this explicitly in the</v>
<v Speaker 1>diagram,</v>

372
00:21:59.710 --> 00:22:00.543
<v Speaker 1>but the way that the update step happens</v>
<v Speaker 1>is through an additive function not </v>

373
00:22:03.041 --> 00:22:05.920
<v Speaker 1>through a multiplicative function.</v>
<v Speaker 1>So when we take the derivative,</v>

374
00:22:05.921 --> 00:22:06.754
<v Speaker 1>there's not a huge expansion.</v>
<v Speaker 1>So now I just want to move on to going </v>

375
00:22:12.641 --> 00:22:13.780
<v Speaker 1>over,</v>
<v Speaker 1>um,</v>

376
00:22:14.440 --> 00:22:19.440
<v Speaker 1>some possible tasks.</v>
<v Speaker 1>So the first task is a classification.</v>

377
00:22:19.900 --> 00:22:23.110
<v Speaker 1>So here we want to classify tweets as </v>
<v Speaker 1>positive,</v>

378
00:22:23.111 --> 00:22:24.340
<v Speaker 1>negative or neutral.</v>

379
00:22:25.890 --> 00:22:28.500
<v Speaker 1>And this task is also knowing that </v>
<v Speaker 1>sentiment analysis.</v>

380
00:22:29.880 --> 00:22:30.290
<v Speaker 2>Okay?</v>

381
00:22:30.290 --> 00:22:31.123
<v Speaker 1>The way that we would design a recurrent</v>
<v Speaker 1>neural network to do this is actually </v>

382
00:22:33.711 --> 00:22:35.420
<v Speaker 1>not by having an output at every time </v>
<v Speaker 1>step.</v>

383
00:22:35.450 --> 00:22:38.390
<v Speaker 1>We only want one output for the entire </v>
<v Speaker 1>sequence.</v>

384
00:22:40.340 --> 00:22:42.830
<v Speaker 1>And so we'll take him the entire </v>
<v Speaker 1>sequence,</v>

385
00:22:42.950 --> 00:22:45.020
<v Speaker 1>the entire tweet,</v>
<v Speaker 1>one word at a time.</v>

386
00:22:45.410 --> 00:22:48.860
<v Speaker 1>And at the very end we'll produce an </v>
<v Speaker 1>output which will,</v>

387
00:22:48.890 --> 00:22:53.660
<v Speaker 1>which will actually be a probability </v>
<v Speaker 1>distribution over possible classes where</v>

388
00:22:53.661 --> 00:22:55.370
<v Speaker 1>our classes in this case would be </v>
<v Speaker 1>positive,</v>

389
00:22:55.371 --> 00:22:56.390
<v Speaker 1>negative,</v>
<v Speaker 1>or neutral.</v>

390
00:22:57.080 --> 00:22:57.913
<v Speaker 1>Note that the only information that is </v>
<v Speaker 1>producing the output at the end is the </v>

391
00:23:02.931 --> 00:23:03.920
<v Speaker 1>final cell state.</v>

392
00:23:05.370 --> 00:23:05.790
<v Speaker 2>Okay.</v>

393
00:23:05.790 --> 00:23:10.170
<v Speaker 1>And so that final cell state kind of has</v>
<v Speaker 1>to summarize all of the information from</v>

394
00:23:10.171 --> 00:23:13.440
<v Speaker 1>the big,</v>
<v Speaker 1>from the entire sequence into that final</v>

395
00:23:13.441 --> 00:23:14.274
<v Speaker 1>cell state.</v>
<v Speaker 1>So we can imagine if we have very </v>

396
00:23:16.351 --> 00:23:18.300
<v Speaker 1>complicated tweets or,</v>
<v Speaker 1>well,</v>

397
00:23:18.301 --> 00:23:19.134
<v Speaker 1>I don't know if that's possible,</v>
<v Speaker 1>but very complicated paragraphs or </v>

398
00:23:22.020 --> 00:23:25.380
<v Speaker 1>sentences.</v>
<v Speaker 1>We might want to create a bigger network</v>

399
00:23:25.381 --> 00:23:30.120
<v Speaker 1>with more hidden states to allow that </v>
<v Speaker 1>last state to be more expressive.</v>

400
00:23:33.330 --> 00:23:35.850
<v Speaker 1>The next task would be something like </v>
<v Speaker 1>music generation,</v>

401
00:23:36.000 --> 00:23:37.560
<v Speaker 1>and I'll see if this will play.</v>

402
00:23:38.780 --> 00:23:39.613
<v Speaker 2>Yeah,</v>

403
00:23:41.010 --> 00:23:46.010
<v Speaker 1>you can kind of hear it.</v>
<v Speaker 1>Okay.</v>

404
00:23:51.170 --> 00:23:53.840
<v Speaker 1>So that was music generated by an Rnn,</v>
<v Speaker 1>which is pretty cool.</v>

405
00:23:53.841 --> 00:23:57.590
<v Speaker 1>And something you're actually also going</v>
<v Speaker 1>to do in the lab today.</v>

406
00:23:58.160 --> 00:23:59.220
<v Speaker 1>But,</v>
<v Speaker 1>uh,</v>

407
00:23:59.240 --> 00:24:00.073
<v Speaker 1>music generally you can,</v>
<v Speaker 1>an rnn can produce music because music </v>

408
00:24:02.631 --> 00:24:05.600
<v Speaker 1>is just a sequence and the way that you </v>
<v Speaker 1>would,</v>

409
00:24:08.490 --> 00:24:12.450
<v Speaker 1>the way that you would construct a neuro</v>
<v Speaker 1>on a recurrent neural network to do this</v>

410
00:24:13.080 --> 00:24:18.080
<v Speaker 1>would be at every time point taking in a</v>
<v Speaker 1>note and producing the most likely.</v>

411
00:24:18.211 --> 00:24:20.850
<v Speaker 1>Next note given the notes that you've </v>
<v Speaker 1>seen so far.</v>

412
00:24:20.851 --> 00:24:23.050
<v Speaker 1>So here you would produce an output at </v>
<v Speaker 1>every time step.</v>

413
00:24:27.360 --> 00:24:28.193
<v Speaker 1>The final task is machine translation.</v>
<v Speaker 1>Machine translation is interesting </v>

414
00:24:32.041 --> 00:24:34.920
<v Speaker 1>because it's actually two recurrent </v>
<v Speaker 1>neural networks,</v>

415
00:24:35.310 --> 00:24:36.810
<v Speaker 1>um,</v>
<v Speaker 1>side by side.</v>

416
00:24:37.080 --> 00:24:41.970
<v Speaker 1>The first is an encoder.</v>
<v Speaker 1>The encoder takes as input a sentence in</v>

417
00:24:42.540 --> 00:24:45.930
<v Speaker 1>a source language like English,</v>
<v Speaker 1>it.</v>

418
00:24:45.931 --> 00:24:46.764
<v Speaker 1>Then there's an a decoder which produces</v>
<v Speaker 1>the same sentence in a target language </v>

419
00:24:50.460 --> 00:24:51.293
<v Speaker 1>like French notice in this architecture </v>
<v Speaker 1>that the only information passed from </v>

420
00:24:56.911 --> 00:24:59.970
<v Speaker 1>the encoder to the decoder is the final </v>
<v Speaker 1>cell state.</v>

421
00:25:01.410 --> 00:25:05.100
<v Speaker 1>And the idea is that that final cell </v>
<v Speaker 1>state should be kind of a summary of the</v>

422
00:25:05.101 --> 00:25:08.550
<v Speaker 1>entire encoder sentence.</v>
<v Speaker 1>And given that summary,</v>

423
00:25:08.551 --> 00:25:09.384
<v Speaker 1>the decoder should be able to figure out</v>
<v Speaker 1>what the encoder sentence was about and </v>

424
00:25:12.391 --> 00:25:14.640
<v Speaker 1>then produce the same sentence in a </v>
<v Speaker 1>different language.</v>

425
00:25:15.030 --> 00:25:16.560
<v Speaker 1>You can imagine though that,</v>
<v Speaker 1>okay,</v>

426
00:25:16.561 --> 00:25:17.394
<v Speaker 1>maybe this is possible for a sentence,</v>
<v Speaker 1>a really simple sentence like the dog </v>

427
00:25:20.610 --> 00:25:21.443
<v Speaker 1>eats.</v>
<v Speaker 1>Maybe we can encode that in the final </v>

428
00:25:22.711 --> 00:25:23.544
<v Speaker 1>cell state,</v>
<v Speaker 1>but if we had a much more complicated </v>

429
00:25:25.201 --> 00:25:26.034
<v Speaker 1>sentence or a much longer sentence,</v>
<v Speaker 1>that would be very difficult to try and </v>

430
00:25:29.881 --> 00:25:31.710
<v Speaker 1>summarize the whole thing and that one </v>
<v Speaker 1>cell state.</v>

431
00:25:32.580 --> 00:25:33.413
<v Speaker 1>So what's typically done in practice for</v>
<v Speaker 1>machine translation is something called </v>

432
00:25:36.631 --> 00:25:37.464
<v Speaker 1>a tension with attention rather than </v>
<v Speaker 1>just taking in the final cell state to </v>

433
00:25:42.390 --> 00:25:43.223
<v Speaker 1>the decoder at each time step,</v>
<v Speaker 1>we take in a weighted sum of all of the </v>

434
00:25:46.501 --> 00:25:47.334
<v Speaker 1>previous cell states.</v>
<v Speaker 1>So in this case we're trying to produce </v>

435
00:25:50.221 --> 00:25:51.054
<v Speaker 1>the first word.</v>
<v Speaker 1>We'll take in a weighted sum of all of </v>

436
00:25:54.901 --> 00:25:55.734
<v Speaker 1>the encoder states.</v>
<v Speaker 1>Most of the weight will probably be on </v>

437
00:25:57.961 --> 00:25:58.794
<v Speaker 1>the first state because that's what </v>
<v Speaker 1>would be most relevant to producing the </v>

438
00:26:01.081 --> 00:26:04.500
<v Speaker 1>first word.</v>
<v Speaker 1>Then when we produced the second word,</v>

439
00:26:04.770 --> 00:26:07.440
<v Speaker 1>most of the weight will probably be on </v>
<v Speaker 1>the second cell state,</v>

440
00:26:07.441 --> 00:26:11.250
<v Speaker 1>but we might have som on the first and </v>
<v Speaker 1>the third to try and get an idea for,</v>

441
00:26:11.460 --> 00:26:13.590
<v Speaker 1>um,</v>
<v Speaker 1>the tents or the gender of this now.</v>

442
00:26:15.890 --> 00:26:17.930
<v Speaker 1>And the same thing for all of the cell </v>
<v Speaker 1>states.</v>

443
00:26:18.320 --> 00:26:19.153
<v Speaker 1>The way that you implement this is just </v>
<v Speaker 1>by including those weight parameters in </v>

444
00:26:22.341 --> 00:26:23.174
<v Speaker 1>the weighted sum as additional </v>
<v Speaker 1>parameters that you chain using </v>

445
00:26:26.721 --> 00:26:28.490
<v Speaker 1>backpropagation just like everything </v>
<v Speaker 1>else.</v>

446
00:26:31.680 --> 00:26:32.513
<v Speaker 1>Okay.</v>
<v Speaker 1>So I hope that you have an idea now why </v>

447
00:26:34.951 --> 00:26:35.784
<v Speaker 1>we need a different framework to model </v>
<v Speaker 1>sequences and how recurrent neural </v>

448
00:26:38.341 --> 00:26:41.010
<v Speaker 1>networks can solve some of the issues </v>
<v Speaker 1>that we saw at the beginning,</v>

449
00:26:41.310 --> 00:26:42.143
<v Speaker 1>um,</v>
<v Speaker 1>as well as an idea of how to them and </v>

450
00:26:43.711 --> 00:26:46.140
<v Speaker 1>saw some of the vanishing gradient </v>
<v Speaker 1>problems.</v>

451
00:26:47.040 --> 00:26:47.410
<v Speaker 2>Okay.</v>

452
00:26:47.410 --> 00:26:48.243
<v Speaker 1>I've talked a lot about language,</v>
<v Speaker 1>but you can imagine using these exact </v>

453
00:26:52.571 --> 00:26:53.404
<v Speaker 1>same neural network,</v>
<v Speaker 1>recurrent neural networks for modeling </v>

454
00:26:55.541 --> 00:26:56.374
<v Speaker 1>time series or wave forms or doing other</v>
<v Speaker 1>interesting sequence prediction tasks </v>

455
00:26:59.981 --> 00:27:03.610
<v Speaker 1>like predicting stock market trends or </v>
<v Speaker 1>summarizing books or articles.</v>

456
00:27:03.940 --> 00:27:04.773
<v Speaker 1>Um,</v>
<v Speaker 1>and maybe you'll consider some sequence </v>

457
00:27:05.990 --> 00:27:08.710
<v Speaker 1>modeling tasks for your final project.</v>
<v Speaker 1>So thank you.</v>

458
00:27:09.530 --> 00:27:10.400
<v Speaker 2>[inaudible].</v>

