WEBVTT

1
00:00:02.580 --> 00:00:05.370
<v Speaker 1>Let's get started.</v>
<v Speaker 1>So good morning everyone.</v>

2
00:00:05.550 --> 00:00:06.383
<v Speaker 1>My name is Aviva and today we're going </v>
<v Speaker 1>to learn about how deep learning can be </v>

3
00:00:10.321 --> 00:00:11.154
<v Speaker 1>used to build systems capable of </v>
<v Speaker 1>perceiving images and making decisions </v>

4
00:00:15.960 --> 00:00:16.793
<v Speaker 1>based on visual information.</v>
<v Speaker 1>Neural networks have really made a </v>

5
00:00:20.671 --> 00:00:21.504
<v Speaker 1>tremendous impact in this area over the </v>
<v Speaker 1>past 20 years and I think to really </v>

6
00:00:25.051 --> 00:00:28.410
<v Speaker 1>appreciate how and why this has been the</v>
<v Speaker 1>case.</v>

7
00:00:28.710 --> 00:00:33.710
<v Speaker 1>It helps to take a step back way back to</v>
<v Speaker 1>540 million years ago.</v>

8
00:00:34.530 --> 00:00:35.363
<v Speaker 1>And the Cambrian explosion where </v>
<v Speaker 1>biologists trace the evolutionary </v>

9
00:00:38.881 --> 00:00:39.714
<v Speaker 1>origins of vision.</v>
<v Speaker 1>The reason vision seems so easy for us </v>

10
00:00:43.261 --> 00:00:44.094
<v Speaker 1>as humans is because we have had 540 </v>
<v Speaker 1>million years of data for evolution to </v>

11
00:00:49.861 --> 00:00:52.800
<v Speaker 1>train on.</v>
<v Speaker 1>Compare that to bipedal movement,</v>

12
00:00:53.040 --> 00:00:55.800
<v Speaker 1>human language,</v>
<v Speaker 1>and the difference is significant.</v>

13
00:00:56.880 --> 00:01:00.690
<v Speaker 1>Starting in around the 19 sixties,</v>
<v Speaker 1>there was a surge in interest,</v>

14
00:01:01.080 --> 00:01:01.913
<v Speaker 1>um,</v>
<v Speaker 1>in,</v>

15
00:01:02.190 --> 00:01:03.023
<v Speaker 1>in both the neuro basis of vision and in</v>
<v Speaker 1>developing methods to systematically </v>

16
00:01:07.111 --> 00:01:09.180
<v Speaker 1>characterize visual processing.</v>

17
00:01:09.630 --> 00:01:10.463
<v Speaker 1>And this eventually led to computer </v>
<v Speaker 1>scientists wondering about how these </v>

18
00:01:14.251 --> 00:01:18.360
<v Speaker 1>findings from neuroscience could be </v>
<v Speaker 1>applied to artificial intelligence.</v>

19
00:01:18.930 --> 00:01:22.860
<v Speaker 1>And one of the biggest breakthroughs in </v>
<v Speaker 1>our understanding of the neural basis of</v>

20
00:01:22.861 --> 00:01:27.330
<v Speaker 1>vision came from to scientists as a </v>
<v Speaker 1>Harvard Hubel and wiesel,</v>

21
00:01:28.500 --> 00:01:32.550
<v Speaker 1>and they had a pretty simple </v>
<v Speaker 1>experimental setup where they were,</v>

22
00:01:32.670 --> 00:01:33.503
<v Speaker 1>where they were able to measure visual,</v>
<v Speaker 1>a neural activity in the visual Cortex </v>

23
00:01:38.790 --> 00:01:39.623
<v Speaker 1>of cats by recording directly from the </v>
<v Speaker 1>electrical signals from neurons in this </v>

24
00:01:45.211 --> 00:01:46.044
<v Speaker 1>region of the brain.</v>
<v Speaker 1>They displayed a simple stimulus on a </v>

25
00:01:48.841 --> 00:01:49.674
<v Speaker 1>screen and then probed the visual Cortex</v>
<v Speaker 1>to see which neurons fired in response </v>

26
00:01:54.661 --> 00:01:59.010
<v Speaker 1>to that stimulus.</v>
<v Speaker 1>There were a few key takeaways from this</v>

27
00:01:59.011 --> 00:02:02.790
<v Speaker 1>experiment that I'd like you to keep in </v>
<v Speaker 1>mind as we go through today's lecture.</v>

28
00:02:03.660 --> 00:02:08.280
<v Speaker 1>The first was that they found that there</v>
<v Speaker 1>was an mechanism for spatial invariance.</v>

29
00:02:08.850 --> 00:02:12.330
<v Speaker 1>They could record neural responses to </v>
<v Speaker 1>particular patterns,</v>

30
00:02:12.960 --> 00:02:13.793
<v Speaker 1>and this was constant regardless of the </v>
<v Speaker 1>location of those patterns on the </v>

31
00:02:16.891 --> 00:02:17.724
<v Speaker 1>screen.</v>

32
00:02:18.570 --> 00:02:19.403
<v Speaker 1>The second was that the neurons they </v>
<v Speaker 1>recorded from had what they called a </v>

33
00:02:22.411 --> 00:02:23.244
<v Speaker 1>receptive field.</v>
<v Speaker 1>They certain neurons only responded to </v>

34
00:02:26.611 --> 00:02:30.450
<v Speaker 1>certain regions of the input while </v>
<v Speaker 1>others responded to other regions.</v>

35
00:02:31.260 --> 00:02:32.093
<v Speaker 1>Finally,</v>
<v Speaker 1>they were able to tease out that there </v>

36
00:02:33.271 --> 00:02:34.104
<v Speaker 1>was a mapping,</v>
<v Speaker 1>a hierarchy to neuro neuro organization </v>

37
00:02:36.961 --> 00:02:37.794
<v Speaker 1>in the visual cortex.</v>
<v Speaker 1>There are cells that are responded to </v>

38
00:02:40.231 --> 00:02:42.960
<v Speaker 1>more simple images such as rods and </v>
<v Speaker 1>rectangles,</v>

39
00:02:43.290 --> 00:02:44.123
<v Speaker 1>and then downstream layers of neurons.</v>
<v Speaker 1>They use the activations from these </v>

40
00:02:47.251 --> 00:02:48.084
<v Speaker 1>upstream neurons in their computation,</v>
<v Speaker 1>so cognitive scientists and </v>

41
00:02:52.711 --> 00:02:53.544
<v Speaker 1>neuroscientists have since built off </v>
<v Speaker 1>this early work to indeed confirm that </v>

42
00:02:57.271 --> 00:02:58.104
<v Speaker 1>the visual Cortex is organized into </v>
<v Speaker 1>layers and this hierarchy of layers </v>

43
00:03:02.680 --> 00:03:06.610
<v Speaker 1>allows for the recognition of </v>
<v Speaker 1>increasingly complex features that,</v>

44
00:03:06.611 --> 00:03:07.444
<v Speaker 1>for example,</v>
<v Speaker 1>allow us to immediately recognize the </v>

45
00:03:09.971 --> 00:03:10.804
<v Speaker 1>face of a friend.</v>
<v Speaker 1>So now that we've gotten a sense at a </v>

46
00:03:14.501 --> 00:03:18.520
<v Speaker 1>very high level of how our brains </v>
<v Speaker 1>process visual information,</v>

47
00:03:18.880 --> 00:03:22.240
<v Speaker 1>we can turn our attention to what </v>
<v Speaker 1>computers see.</v>

48
00:03:22.990 --> 00:03:27.760
<v Speaker 1>How does it computer processing and </v>
<v Speaker 1>image well to a computer.</v>

49
00:03:27.820 --> 00:03:28.653
<v Speaker 1>Images are just numbers.</v>
<v Speaker 1>So suppose we have a picture of Abraham </v>

50
00:03:32.650 --> 00:03:35.680
<v Speaker 1>Lincoln.</v>
<v Speaker 1>It's made up of pixels and since this is</v>

51
00:03:35.681 --> 00:03:36.514
<v Speaker 1>a gray scale image,</v>
<v Speaker 1>each of these pixels can be represented </v>

52
00:03:39.010 --> 00:03:44.010
<v Speaker 1>by just a single number.</v>
<v Speaker 1>So we can represent our image as a two d</v>

53
00:03:44.171 --> 00:03:47.050
<v Speaker 1>array of numbers.</v>
<v Speaker 1>One for each pixel in the image,</v>

54
00:03:47.650 --> 00:03:50.380
<v Speaker 1>and if we were to have an rgb color </v>
<v Speaker 1>image,</v>

55
00:03:50.410 --> 00:03:51.243
<v Speaker 1>not gray scale,</v>
<v Speaker 1>we can represent that with a three d </v>

56
00:03:53.501 --> 00:03:57.820
<v Speaker 1>array where we have two d matrices for </v>
<v Speaker 1>each of our G and b.</v>

57
00:03:59.230 --> 00:04:02.320
<v Speaker 1>Now that we have a way to represent </v>
<v Speaker 1>images to computers,</v>

58
00:04:02.620 --> 00:04:07.620
<v Speaker 1>we can next think about what types of </v>
<v Speaker 1>computer vision tasks we can perform and</v>

59
00:04:08.020 --> 00:04:08.853
<v Speaker 1>in machine learning more broadly,</v>
<v Speaker 1>we can think of tasks of regression and </v>

60
00:04:12.971 --> 00:04:13.804
<v Speaker 1>those of classification in regression.</v>
<v Speaker 1>Our output takes a continuous value and </v>

61
00:04:18.641 --> 00:04:20.890
<v Speaker 1>in classification,</v>
<v Speaker 1>a single class label.</v>

62
00:04:21.370 --> 00:04:24.010
<v Speaker 1>So let's consider the task of image </v>
<v Speaker 1>classification.</v>

63
00:04:24.580 --> 00:04:28.210
<v Speaker 1>We want to predict a single label for a </v>
<v Speaker 1>given input image.</v>

64
00:04:28.720 --> 00:04:29.553
<v Speaker 1>For example,</v>
<v Speaker 1>let's say we have a bunch of images of </v>

65
00:04:31.001 --> 00:04:35.950
<v Speaker 1>us presidents and we want to build a </v>
<v Speaker 1>classification pipeline to tell us which</v>

66
00:04:35.951 --> 00:04:36.784
<v Speaker 1>president is an image.</v>
<v Speaker 1>I'll putting the probability that the </v>

67
00:04:39.671 --> 00:04:44.671
<v Speaker 1>image is of a particular president.</v>
<v Speaker 1>So in order to be able to classify these</v>

68
00:04:45.131 --> 00:04:45.964
<v Speaker 1>images are pipeline needs to be able to </v>
<v Speaker 1>tell what is unique about a picture of </v>

69
00:04:49.691 --> 00:04:53.620
<v Speaker 1>Lincoln versus a picture of Washington </v>
<v Speaker 1>versus a picture of Obama.</v>

70
00:04:54.910 --> 00:04:57.940
<v Speaker 1>And another way to think about this </v>
<v Speaker 1>problem at a,</v>

71
00:04:58.000 --> 00:04:58.833
<v Speaker 1>at a high level is in terms of the </v>
<v Speaker 1>features that are characteristic of a </v>

72
00:05:01.481 --> 00:05:02.314
<v Speaker 1>particular class of images,</v>
<v Speaker 1>classification would then be done by </v>

73
00:05:05.801 --> 00:05:08.860
<v Speaker 1>detecting the presence of these features</v>
<v Speaker 1>in a given image.</v>

74
00:05:09.730 --> 00:05:10.563
<v Speaker 1>If the,</v>
<v Speaker 1>if the feature is for a particular </v>

75
00:05:12.431 --> 00:05:14.620
<v Speaker 1>image,</v>
<v Speaker 1>are all present in an image,</v>

76
00:05:15.010 --> 00:05:18.820
<v Speaker 1>we can then predict that class with a </v>
<v Speaker 1>high probability.</v>

77
00:05:19.840 --> 00:05:22.660
<v Speaker 1>So for our image classification </v>
<v Speaker 1>pipeline,</v>

78
00:05:22.900 --> 00:05:27.900
<v Speaker 1>our model needs to know first what the </v>
<v Speaker 1>features are and secondly it needs to be</v>

79
00:05:28.271 --> 00:05:30.610
<v Speaker 1>able to detect those features in an </v>
<v Speaker 1>image.</v>

80
00:05:31.840 --> 00:05:32.673
<v Speaker 1>So one way we can solve this is to </v>
<v Speaker 1>leverage our knowledge about a </v>

81
00:05:35.261 --> 00:05:36.094
<v Speaker 1>particular field and use this to </v>
<v Speaker 1>manually define the features ourselves </v>

82
00:05:40.630 --> 00:05:41.463
<v Speaker 1>and classification pipeline would then </v>
<v Speaker 1>try to detect these mentally defined </v>

83
00:05:45.191 --> 00:05:46.024
<v Speaker 1>features in images and use the results </v>
<v Speaker 1>of some detection algorithm for </v>

84
00:05:49.571 --> 00:05:50.404
<v Speaker 1>classification.</v>
<v Speaker 1>But there's a big problem with this </v>

85
00:05:53.351 --> 00:05:54.184
<v Speaker 1>approach.</v>
<v Speaker 1>Remember that images are just 3d arrays </v>

86
00:05:57.561 --> 00:05:58.394
<v Speaker 1>of brightness,</v>
<v Speaker 1>values and image data has a lot of </v>

87
00:06:01.551 --> 00:06:05.120
<v Speaker 1>variation.</v>
<v Speaker 1>Occlusion variations in illumination,</v>

88
00:06:05.121 --> 00:06:08.090
<v Speaker 1>viewpoint variation,</v>
<v Speaker 1>interclass variation,</v>

89
00:06:08.690 --> 00:06:09.523
<v Speaker 1>and our classification pipeline has to </v>
<v Speaker 1>be invariant to all these variations </v>

90
00:06:13.040 --> 00:06:17.240
<v Speaker 1>while still being sensitive to the </v>
<v Speaker 1>variability between different classes,</v>

91
00:06:18.950 --> 00:06:19.783
<v Speaker 1>even though our pipeline could use </v>
<v Speaker 1>features that we the human define where </v>

92
00:06:23.721 --> 00:06:27.920
<v Speaker 1>this manual extraction would break down </v>
<v Speaker 1>is in the detection task itself,</v>

93
00:06:28.640 --> 00:06:32.030
<v Speaker 1>and this is due to the incredible </v>
<v Speaker 1>variability that I just mentioned.</v>

94
00:06:32.750 --> 00:06:37.220
<v Speaker 1>The detection of these features would </v>
<v Speaker 1>actually be really difficult in practice</v>

95
00:06:37.460 --> 00:06:38.293
<v Speaker 1>because your detection algorithm would </v>
<v Speaker 1>need to withstand each and every one of </v>

96
00:06:42.381 --> 00:06:46.010
<v Speaker 1>these different variations,</v>
<v Speaker 1>so how can we do better?</v>

97
00:06:46.940 --> 00:06:47.773
<v Speaker 1>We want a way to both extract features </v>
<v Speaker 1>and detect their presence in images </v>

98
00:06:51.560 --> 00:06:52.393
<v Speaker 1>automatically in a hierarchical fashion,</v>
<v Speaker 1>and we can use neuro network based </v>

99
00:06:57.351 --> 00:06:58.184
<v Speaker 1>approaches to do exactly this,</v>
<v Speaker 1>to learn visual features directly from </v>

100
00:07:01.791 --> 00:07:02.624
<v Speaker 1>image data and to learn a hierarchy of </v>
<v Speaker 1>these features to construct an internal </v>

101
00:07:06.561 --> 00:07:09.770
<v Speaker 1>representation of the image.</v>
<v Speaker 1>For example,</v>

102
00:07:09.800 --> 00:07:12.890
<v Speaker 1>if we wanted to be able to classify </v>
<v Speaker 1>images of faces,</v>

103
00:07:13.370 --> 00:07:17.270
<v Speaker 1>maybe we could just first learn and </v>
<v Speaker 1>detect low level features like edges and</v>

104
00:07:17.271 --> 00:07:19.700
<v Speaker 1>dark spots,</v>
<v Speaker 1>level features,</v>

105
00:07:19.790 --> 00:07:21.260
<v Speaker 1>eyes,</v>
<v Speaker 1>ears and noses,</v>

106
00:07:21.560 --> 00:07:22.393
<v Speaker 1>and then high level features that </v>
<v Speaker 1>actually resemble facial structure and </v>

107
00:07:27.051 --> 00:07:27.884
<v Speaker 1>neural networks will allow us to </v>
<v Speaker 1>directly learn visual features in this </v>

108
00:07:31.671 --> 00:07:33.830
<v Speaker 1>manner if we construct them cleverly.</v>

109
00:07:35.270 --> 00:07:36.103
<v Speaker 1>So in yesterday's first lecture,</v>
<v Speaker 1>we learned about fully connected neural </v>

110
00:07:38.751 --> 00:07:43.400
<v Speaker 1>networks where you can have multiple </v>
<v Speaker 1>hidden layers and where each neuron in a</v>

111
00:07:43.401 --> 00:07:46.760
<v Speaker 1>given layer is connected to every neuron</v>
<v Speaker 1>in the subsequent layer.</v>

112
00:07:47.420 --> 00:07:48.253
<v Speaker 1>Let's say we wanted to use a fully </v>
<v Speaker 1>connected neural network like the one </v>

113
00:07:51.051 --> 00:07:54.980
<v Speaker 1>you see here for image classification.</v>
<v Speaker 1>In this case,</v>

114
00:07:54.981 --> 00:07:58.850
<v Speaker 1>our input image would be transformed </v>
<v Speaker 1>into a vector of pixel values,</v>

115
00:07:59.150 --> 00:07:59.983
<v Speaker 1>fed into the network,</v>
<v Speaker 1>and each neuron in the hidden layer </v>

116
00:08:02.331 --> 00:08:05.570
<v Speaker 1>would be connected to all neurons in the</v>
<v Speaker 1>input layer.</v>

117
00:08:06.470 --> 00:08:10.760
<v Speaker 1>I hope you can appreciate that all </v>
<v Speaker 1>spatial spatial information from our two</v>

118
00:08:10.761 --> 00:08:14.390
<v Speaker 1>d array would be completely lost </v>
<v Speaker 1>additional weight.</v>

119
00:08:14.480 --> 00:08:16.100
<v Speaker 1>Additionally,</v>
<v Speaker 1>we'd have many,</v>

120
00:08:16.101 --> 00:08:19.310
<v Speaker 1>many parameters because this is a fully </v>
<v Speaker 1>connected network,</v>

121
00:08:19.610 --> 00:08:24.610
<v Speaker 1>so it's not going to be really feasible </v>
<v Speaker 1>to implement such network in practice,</v>

122
00:08:26.240 --> 00:08:27.073
<v Speaker 1>so how can we use the spatial structure </v>
<v Speaker 1>that's inherent in the input to inform </v>

123
00:08:31.161 --> 00:08:33.290
<v Speaker 1>the architecture of our network?</v>

124
00:08:35.150 --> 00:08:35.983
<v Speaker 1>The key insight in how we can do this is</v>
<v Speaker 1>to connect patches of the input </v>

125
00:08:41.270 --> 00:08:44.930
<v Speaker 1>represented as a two d array to neurons </v>
<v Speaker 1>in hidden layers.</v>

126
00:08:45.590 --> 00:08:46.423
<v Speaker 1>This is to say that each neuron in a </v>
<v Speaker 1>hidden layer only sees a particular </v>

127
00:08:49.761 --> 00:08:52.280
<v Speaker 1>region of what the input to that layer </v>
<v Speaker 1>is.</v>

128
00:08:53.060 --> 00:08:55.770
<v Speaker 1>This will not only reduce the number of </v>
<v Speaker 1>weights in our network,</v>

129
00:08:56.040 --> 00:08:56.873
<v Speaker 1>but also allow us to leverage the fact </v>
<v Speaker 1>that in an image pixels that are near </v>

130
00:09:01.111 --> 00:09:01.944
<v Speaker 1>each other are probably somehow related </v>
<v Speaker 1>to define connections across the whole </v>

131
00:09:07.021 --> 00:09:07.854
<v Speaker 1>input.</v>
<v Speaker 1>We can apply this same principle by </v>

132
00:09:10.411 --> 00:09:14.880
<v Speaker 1>sliding the patch window across the </v>
<v Speaker 1>entirety of the input image.</v>

133
00:09:15.240 --> 00:09:16.073
<v Speaker 1>In this case,</v>
<v Speaker 1>by two units in this way will take into </v>

134
00:09:19.081 --> 00:09:21.240
<v Speaker 1>account the spatial structure that's </v>
<v Speaker 1>present,</v>

135
00:09:23.010 --> 00:09:23.843
<v Speaker 1>but remember,</v>
<v Speaker 1>our ultimate task is to learn visual </v>

136
00:09:25.981 --> 00:09:26.814
<v Speaker 1>features and we can do this by smartly </v>
<v Speaker 1>waiting the connections between a patch </v>

137
00:09:32.490 --> 00:09:37.080
<v Speaker 1>of our input to the neuron it's </v>
<v Speaker 1>connected to in the next hidden layer,</v>

138
00:09:37.290 --> 00:09:38.123
<v Speaker 1>so as to detect particular features </v>
<v Speaker 1>present in that input and essentially </v>

139
00:09:43.860 --> 00:09:46.620
<v Speaker 1>what this amounts to is applying the </v>
<v Speaker 1>filter,</v>

140
00:09:46.670 --> 00:09:49.230
<v Speaker 1>a set of weights to extract local </v>
<v Speaker 1>features,</v>

141
00:09:51.620 --> 00:09:52.453
<v Speaker 1>and it would be useful for us in our </v>
<v Speaker 1>classification pipeline to have many </v>

142
00:09:55.921 --> 00:10:00.480
<v Speaker 1>different features to work with and we </v>
<v Speaker 1>can do this by using multiple filters,</v>

143
00:10:00.630 --> 00:10:01.463
<v Speaker 1>multiple sets of weights,</v>
<v Speaker 1>and finally we want to be able to share </v>

144
00:10:05.791 --> 00:10:06.624
<v Speaker 1>the parameters of each filter across all</v>
<v Speaker 1>the connections between the input layer </v>

145
00:10:10.560 --> 00:10:11.393
<v Speaker 1>and the next layer because the features </v>
<v Speaker 1>that matter in one part of the input </v>

146
00:10:15.630 --> 00:10:16.800
<v Speaker 1>should matter elsewhere.</v>

147
00:10:16.830 --> 00:10:17.663
<v Speaker 1>This is the same concept of a spatial </v>
<v Speaker 1>invariants that I alluded to earlier in </v>

148
00:10:24.541 --> 00:10:28.500
<v Speaker 1>practice.</v>
<v Speaker 1>This amounts to a patchy operation known</v>

149
00:10:28.501 --> 00:10:29.334
<v Speaker 1>as convolution.</v>
<v Speaker 1>Let's first think about this at a high </v>

150
00:10:32.071 --> 00:10:34.950
<v Speaker 1>level.</v>
<v Speaker 1>Suppose we have a four by four filter,</v>

151
00:10:35.640 --> 00:10:40.140
<v Speaker 1>which means we have 16 different weights</v>
<v Speaker 1>and we're going to apply the same filter</v>

152
00:10:40.410 --> 00:10:41.243
<v Speaker 1>to four by four patches in the input and</v>
<v Speaker 1>use the result of that operation to </v>

153
00:10:45.541 --> 00:10:46.374
<v Speaker 1>somehow influence the state of the </v>
<v Speaker 1>neuron and the next layer that this </v>

154
00:10:49.531 --> 00:10:50.364
<v Speaker 1>patch is connected to.</v>
<v Speaker 1>Then we're going to shift our filter by </v>

155
00:10:53.761 --> 00:10:58.761
<v Speaker 1>two pixels as for example,</v>
<v Speaker 1>and grabbed the next patch of the input,</v>

156
00:11:00.450 --> 00:11:05.100
<v Speaker 1>so in this way we can start thinking </v>
<v Speaker 1>about convolution at a very high level,</v>

157
00:11:06.090 --> 00:11:09.060
<v Speaker 1>but you're probably wondering how does </v>
<v Speaker 1>this actually work?</v>

158
00:11:09.330 --> 00:11:10.163
<v Speaker 1>What do we mean by features and how does</v>
<v Speaker 1>this convolution operation allow us to </v>

159
00:11:13.891 --> 00:11:14.724
<v Speaker 1>extract them?</v>

160
00:11:15.750 --> 00:11:19.410
<v Speaker 1>Hopefully we can make this concrete by </v>
<v Speaker 1>walking through a couple of examples.</v>

161
00:11:20.460 --> 00:11:25.460
<v Speaker 1>Suppose we want to classify x's from a </v>
<v Speaker 1>set of black and white images of letters</v>

162
00:11:26.010 --> 00:11:30.840
<v Speaker 1>where black is equal to negative one and</v>
<v Speaker 1>why is represented by a value of one.</v>

163
00:11:31.830 --> 00:11:32.663
<v Speaker 1>For classification,</v>
<v Speaker 1>it would clearly not be possible to </v>

164
00:11:34.951 --> 00:11:38.460
<v Speaker 1>simply compare the two matrices to see </v>
<v Speaker 1>if they're equal.</v>

165
00:11:39.360 --> 00:11:43.050
<v Speaker 1>We want to be able to classify an x as </v>
<v Speaker 1>an ex,</v>

166
00:11:43.080 --> 00:11:45.840
<v Speaker 1>even if it's shifted,</v>
<v Speaker 1>shrunk,</v>

167
00:11:45.900 --> 00:11:47.110
<v Speaker 1>rotated,</v>
<v Speaker 1>deform,</v>

168
00:11:47.111 --> 00:11:47.944
<v Speaker 1>transformed in some way.</v>
<v Speaker 1>We want our model to compare the images </v>

169
00:11:52.591 --> 00:11:53.424
<v Speaker 1>of an x piece by piece and look for </v>
<v Speaker 1>these important pieces that define an x </v>

170
00:11:58.961 --> 00:11:59.794
<v Speaker 1>as an x.</v>
<v Speaker 1>those are the features and if our model </v>

171
00:12:02.531 --> 00:12:03.364
<v Speaker 1>can find rough feature matches in </v>
<v Speaker 1>roughly the same positions into </v>

172
00:12:06.371 --> 00:12:07.204
<v Speaker 1>different images,</v>
<v Speaker 1>it can get a lot better at seeing the </v>

173
00:12:10.270 --> 00:12:13.240
<v Speaker 1>similarity between different examples of</v>
<v Speaker 1>xs.</v>

174
00:12:14.950 --> 00:12:17.620
<v Speaker 1>You can think of each feature as many </v>
<v Speaker 1>images,</v>

175
00:12:17.621 --> 00:12:18.454
<v Speaker 1>small two dimensional array of values,</v>
<v Speaker 1>and we're going to use filters to pick </v>

176
00:12:22.061 --> 00:12:22.894
<v Speaker 1>up on the features common to xs in the </v>
<v Speaker 1>case of an ex filters that can pick up </v>

177
00:12:28.090 --> 00:12:28.923
<v Speaker 1>on diagonal lines and crossing will </v>
<v Speaker 1>probably capture all the important </v>

178
00:12:33.130 --> 00:12:33.963
<v Speaker 1>characteristics of most x's.</v>
<v Speaker 1>So no here that these smaller matrices </v>

179
00:12:38.530 --> 00:12:39.363
<v Speaker 1>are the filters of weights that we'll </v>
<v Speaker 1>use in our convolution operation to </v>

180
00:12:43.211 --> 00:12:46.540
<v Speaker 1>detect the corresponding features in an </v>
<v Speaker 1>input image.</v>

181
00:12:47.350 --> 00:12:48.183
<v Speaker 1>So now all that's left is to define this</v>
<v Speaker 1>operation that will pick up on when </v>

182
00:12:52.511 --> 00:12:53.344
<v Speaker 1>these features pop up in our image,</v>
<v Speaker 1>and that operation is convolution </v>

183
00:12:58.180 --> 00:12:59.013
<v Speaker 1>convolution preserves the spatial </v>
<v Speaker 1>relationship between pixels by learning </v>

184
00:13:04.031 --> 00:13:06.820
<v Speaker 1>image features in small squares of the </v>
<v Speaker 1>input.</v>

185
00:13:08.290 --> 00:13:09.123
<v Speaker 1>To do this,</v>
<v Speaker 1>we perform an element wise </v>

186
00:13:10.391 --> 00:13:15.391
<v Speaker 1>multiplication between the filter matrix</v>
<v Speaker 1>and a patch of the input image that's of</v>

187
00:13:15.851 --> 00:13:16.684
<v Speaker 1>the same dimensions as the filter.</v>
<v Speaker 1>This results in a three by three matrix </v>

188
00:13:21.820 --> 00:13:25.360
<v Speaker 1>and the example you see here,</v>
<v Speaker 1>all entries in this matrix are one,</v>

189
00:13:25.780 --> 00:13:26.613
<v Speaker 1>and that's because there's a perfect </v>
<v Speaker 1>correspondence between our filter and </v>

190
00:13:30.161 --> 00:13:32.680
<v Speaker 1>the region of the input that we're </v>
<v Speaker 1>involving it with.</v>

191
00:13:33.550 --> 00:13:34.383
<v Speaker 1>Finally,</v>
<v Speaker 1>we saw them all the entries in this </v>

192
00:13:35.981 --> 00:13:40.720
<v Speaker 1>Matrix get nine and that's the result of</v>
<v Speaker 1>our convolution operation.</v>

193
00:13:43.430 --> 00:13:44.263
<v Speaker 1>Let's consider one more example.</v>
<v Speaker 1>Suppose now we want to compute the </v>

194
00:13:47.691 --> 00:13:51.680
<v Speaker 1>convolution of a five by five image and </v>
<v Speaker 1>a three by three filter.</v>

195
00:13:52.250 --> 00:13:53.083
<v Speaker 1>To do this,</v>
<v Speaker 1>we need to cover the entirety of the </v>

196
00:13:55.311 --> 00:13:59.420
<v Speaker 1>input image by sliding the filter over </v>
<v Speaker 1>the image,</v>

197
00:13:59.660 --> 00:14:03.290
<v Speaker 1>performing the same element wise,</v>
<v Speaker 1>multiplication and addition.</v>

198
00:14:04.100 --> 00:14:04.933
<v Speaker 1>Let's see what this looks like.</v>
<v Speaker 1>We'll first start off in the upper left </v>

199
00:14:08.031 --> 00:14:10.130
<v Speaker 1>corner,</v>
<v Speaker 1>element wise,</v>

200
00:14:10.190 --> 00:14:14.150
<v Speaker 1>multiplied this three by three patch </v>
<v Speaker 1>with the entries of the filter,</v>

201
00:14:14.240 --> 00:14:15.073
<v Speaker 1>and then add this results in the first </v>
<v Speaker 1>entry in our output matrix called the </v>

202
00:14:19.761 --> 00:14:20.594
<v Speaker 1>feature map.</v>

203
00:14:22.630 --> 00:14:23.463
<v Speaker 1>We will next slide the three by three </v>
<v Speaker 1>filter over by one to grab the next </v>

204
00:14:27.821 --> 00:14:31.990
<v Speaker 1>patch and repeat the same operation.</v>
<v Speaker 1>I'm in wise multiplication.</v>

205
00:14:32.050 --> 00:14:32.883
<v Speaker 1>Addition.</v>
<v Speaker 1>This results in the second entry and we </v>

206
00:14:35.681 --> 00:14:40.681
<v Speaker 1>continue in this way until we have </v>
<v Speaker 1>covered the entirety of our five by five</v>

207
00:14:42.221 --> 00:14:43.054
<v Speaker 1>image and that's it.</v>
<v Speaker 1>The feature map reflects where in the </v>

208
00:14:47.141 --> 00:14:50.470
<v Speaker 1>input was activated by this filter that </v>
<v Speaker 1>we applied.</v>

209
00:14:51.980 --> 00:14:55.610
<v Speaker 1>Now that we've gone through the </v>
<v Speaker 1>mechanism of the convolution operation,</v>

210
00:14:56.930 --> 00:15:01.310
<v Speaker 1>let's see how different filters can be </v>
<v Speaker 1>used to produce different feature maps.</v>

211
00:15:02.000 --> 00:15:02.833
<v Speaker 1>So on the left you'll see a picture of a</v>
<v Speaker 1>woman's face and next to it the output </v>

212
00:15:07.400 --> 00:15:08.233
<v Speaker 1>of applying three different </v>
<v Speaker 1>convolutional filters to that same </v>

213
00:15:11.211 --> 00:15:12.044
<v Speaker 1>image.</v>
<v Speaker 1>And you can appreciate that by simply </v>

214
00:15:14.870 --> 00:15:15.703
<v Speaker 1>changing the weights of the filters.</v>
<v Speaker 1>We can detect different features from </v>

215
00:15:19.071 --> 00:15:19.904
<v Speaker 1>the image.</v>

216
00:15:21.830 --> 00:15:22.663
<v Speaker 1>So I hope you can now see how </v>
<v Speaker 1>convolution allows us to capitalize on </v>

217
00:15:26.931 --> 00:15:27.764
<v Speaker 1>the spatial structure inherent to image </v>
<v Speaker 1>data and use sets of weights to extract </v>

218
00:15:32.661 --> 00:15:33.494
<v Speaker 1>local features to end to very easily be </v>
<v Speaker 1>able to detect different types of </v>

219
00:15:37.131 --> 00:15:40.100
<v Speaker 1>features by simply using different </v>
<v Speaker 1>filters.</v>

220
00:15:41.180 --> 00:15:42.013
<v Speaker 1>These concepts of spatial structure and </v>
<v Speaker 1>local feature extraction using </v>

221
00:15:45.681 --> 00:15:50.660
<v Speaker 1>convolution are at the core of the </v>
<v Speaker 1>neural networks used for computer vision</v>

222
00:15:50.661 --> 00:15:51.494
<v Speaker 1>tasks,</v>
<v Speaker 1>which are very appropriately named </v>

223
00:15:54.081 --> 00:15:54.914
<v Speaker 1>convolutional neural networks or CNNS.</v>
<v Speaker 1>So first we'll take a look at a CNN </v>

224
00:16:00.231 --> 00:16:03.410
<v Speaker 1>architecture that's designed for image </v>
<v Speaker 1>classification.</v>

225
00:16:04.310 --> 00:16:09.230
<v Speaker 1>Now there are three main operations to a</v>
<v Speaker 1>CNN first convolutions,</v>

226
00:16:09.500 --> 00:16:12.950
<v Speaker 1>which as we saw can be used to generate </v>
<v Speaker 1>feature maps,</v>

227
00:16:13.760 --> 00:16:14.593
<v Speaker 1>second nonlinearity,</v>
<v Speaker 1>which we learned in the first lecture </v>

228
00:16:18.410 --> 00:16:21.680
<v Speaker 1>yesterday because image data is highly </v>
<v Speaker 1>nonlinear,</v>

229
00:16:22.250 --> 00:16:26.180
<v Speaker 1>and finally pooling which is a </v>
<v Speaker 1>downsampling operation.</v>

230
00:16:27.320 --> 00:16:29.120
<v Speaker 1>In training,</v>
<v Speaker 1>we train our model,</v>

231
00:16:29.180 --> 00:16:31.280
<v Speaker 1>our CNN model,</v>
<v Speaker 1>on a set of images,</v>

232
00:16:31.730 --> 00:16:32.563
<v Speaker 1>and in training we learned the weights </v>
<v Speaker 1>of the convolutional filters that </v>

233
00:16:36.621 --> 00:16:40.010
<v Speaker 1>correspond to feature maps in </v>
<v Speaker 1>convolutional layers.</v>

234
00:16:41.090 --> 00:16:41.923
<v Speaker 1>And in the case of classification,</v>
<v Speaker 1>we can feed the output of these </v>

235
00:16:45.171 --> 00:16:46.004
<v Speaker 1>convolutional layers into a fully </v>
<v Speaker 1>connected layer to perform </v>

236
00:16:48.651 --> 00:16:49.484
<v Speaker 1>classification.</v>
<v Speaker 1>Now we'll go through each of these </v>

237
00:16:52.041 --> 00:16:56.180
<v Speaker 1>operations to break down a the basic </v>
<v Speaker 1>architecture of a CNN.</v>

238
00:16:57.350 --> 00:16:58.183
<v Speaker 1>First,</v>
<v Speaker 1>let's consider the convolution </v>

239
00:16:59.571 --> 00:17:03.020
<v Speaker 1>operation.</v>
<v Speaker 1>As we saw yesterday,</v>

240
00:17:04.580 --> 00:17:08.450
<v Speaker 1>each neuron and the hidden layer will </v>
<v Speaker 1>compute a weighted sum of its inputs,</v>

241
00:17:08.660 --> 00:17:09.493
<v Speaker 1>apply bias,</v>
<v Speaker 1>and eventually activate with a </v>

242
00:17:12.000 --> 00:17:12.833
<v Speaker 1>nonlinearity.</v>
<v Speaker 1>What's special in CNN is this idea of </v>

243
00:17:16.011 --> 00:17:20.360
<v Speaker 1>local connectivity.</v>
<v Speaker 1>Each neuron and hidden layer only sees a</v>

244
00:17:20.361 --> 00:17:25.361
<v Speaker 1>patch of its inputs.</v>
<v Speaker 1>We can now define the actual computation</v>

245
00:17:26.240 --> 00:17:27.073
<v Speaker 1>for a neuron in the hidden layer.</v>
<v Speaker 1>Its inputs are those neurons in the </v>

246
00:17:30.411 --> 00:17:32.990
<v Speaker 1>patch of the input layer that it's </v>
<v Speaker 1>connected to.</v>

247
00:17:33.620 --> 00:17:38.180
<v Speaker 1>We apply a matrix of weights are </v>
<v Speaker 1>convolutional filter four by four.</v>

248
00:17:38.181 --> 00:17:41.480
<v Speaker 1>In this example,</v>
<v Speaker 1>do an element wise multiplication,</v>

249
00:17:41.870 --> 00:17:42.703
<v Speaker 1>add the outputs and apply bias,</v>
<v Speaker 1>so this defines how neurons in </v>

250
00:17:48.501 --> 00:17:53.501
<v Speaker 1>convolutional are connected,</v>
<v Speaker 1>but within a single convolutional layer,</v>

251
00:17:54.060 --> 00:17:54.893
<v Speaker 1>we can have multiple different filters </v>
<v Speaker 1>that we are learning to be able to </v>

252
00:17:58.711 --> 00:18:00.210
<v Speaker 1>extract different features.</v>

253
00:18:00.570 --> 00:18:01.403
<v Speaker 1>And what this means that is that the </v>
<v Speaker 1>output of a convolutional layer has a </v>

254
00:18:05.911 --> 00:18:10.080
<v Speaker 1>volume where the height and the width </v>
<v Speaker 1>are spatial dimensions,</v>

255
00:18:10.170 --> 00:18:15.170
<v Speaker 1>and these spatial dimensions are </v>
<v Speaker 1>dependent on the dimensions of the input</v>

256
00:18:15.301 --> 00:18:16.134
<v Speaker 1>layer.</v>
<v Speaker 1>The dimensions of our filter and how </v>

257
00:18:19.411 --> 00:18:22.440
<v Speaker 1>we're sliding our filter over the input,</v>
<v Speaker 1>the stride,</v>

258
00:18:23.640 --> 00:18:24.473
<v Speaker 1>the depth of a this output volume is </v>
<v Speaker 1>then given by the number of different </v>

259
00:18:28.051 --> 00:18:28.884
<v Speaker 1>filters we apply in that layer.</v>
<v Speaker 1>We can also think of how neurons in </v>

260
00:18:33.991 --> 00:18:38.010
<v Speaker 1>convolutional layers are connected in </v>
<v Speaker 1>terms of the receptive field,</v>

261
00:18:38.580 --> 00:18:43.320
<v Speaker 1>the locations in the original input </v>
<v Speaker 1>image that a node is connected to.</v>

262
00:18:44.160 --> 00:18:44.993
<v Speaker 1>These parameters defined the spatial </v>
<v Speaker 1>arrangement of the output of a </v>

263
00:18:47.791 --> 00:18:52.791
<v Speaker 1>convolutional layer.</v>
<v Speaker 1>The next step is to apply a nonlinearity</v>

264
00:18:53.070 --> 00:18:57.690
<v Speaker 1>to this output volume as was introduced </v>
<v Speaker 1>in yesterday's lecture.</v>

265
00:18:57.900 --> 00:18:58.733
<v Speaker 1>We do this because data is highly </v>
<v Speaker 1>nonlinear and in Cnns it's common </v>

266
00:19:02.521 --> 00:19:03.354
<v Speaker 1>practice to apply nonlinearity after </v>
<v Speaker 1>every convolution operation and the </v>

267
00:19:07.621 --> 00:19:08.454
<v Speaker 1>common activation function that's used </v>
<v Speaker 1>is the value which is a pixel by pixel </v>

268
00:19:12.691 --> 00:19:17.691
<v Speaker 1>operation that will replace all negative</v>
<v Speaker 1>values falling convolution with a zero.</v>

269
00:19:20.590 --> 00:19:21.423
<v Speaker 1>And the last key operation to a CNN is </v>
<v Speaker 1>pulling and pulling is used to reduce </v>

270
00:19:26.081 --> 00:19:27.800
<v Speaker 1>dimension dimension,</v>
<v Speaker 1>yet,</v>

271
00:19:27.960 --> 00:19:29.710
<v Speaker 1>excuse me,</v>
<v Speaker 1>dimensionality,</v>

272
00:19:29.950 --> 00:19:30.783
<v Speaker 1>and to preserve spatial invariance,</v>
<v Speaker 1>a common text technique which you'll </v>

273
00:19:34.930 --> 00:19:38.800
<v Speaker 1>very often see is max pooling as shown </v>
<v Speaker 1>in this example,</v>

274
00:19:39.010 --> 00:19:39.843
<v Speaker 1>and it's exactly what it sounds like.</v>
<v Speaker 1>We simply take the maximum value in a </v>

275
00:19:43.271 --> 00:19:46.090
<v Speaker 1>patch of the input,</v>
<v Speaker 1>in this case,</v>

276
00:19:46.091 --> 00:19:50.740
<v Speaker 1>a two by two patch and that determines </v>
<v Speaker 1>the output of the pooling operation.</v>

277
00:19:51.400 --> 00:19:52.233
<v Speaker 1>And I encourage you to kind of meditate </v>
<v Speaker 1>on other ways in which we could perform </v>

278
00:19:57.131 --> 00:20:02.131
<v Speaker 1>this sort of downsampling operation.</v>
<v Speaker 1>So these are the key operations of a CNN</v>

279
00:20:06.060 --> 00:20:09.960
<v Speaker 1>and we're now ready to put them together</v>
<v Speaker 1>to actually construct our network.</v>

280
00:20:10.530 --> 00:20:11.363
<v Speaker 1>We can layer these operations to learn a</v>
<v Speaker 1>hierarchy of features present in the </v>

281
00:20:14.911 --> 00:20:15.744
<v Speaker 1>image data.</v>

282
00:20:18.130 --> 00:20:23.130
<v Speaker 1>A CNN bills for image classification can</v>
<v Speaker 1>roughly be broken down into two parts.</v>

283
00:20:23.470 --> 00:20:24.303
<v Speaker 1>The first is the feature learning </v>
<v Speaker 1>pipeline where we learn features in </v>

284
00:20:27.281 --> 00:20:31.900
<v Speaker 1>input images through convolution,</v>
<v Speaker 1>through the introduction of nonlinearity</v>

285
00:20:32.080 --> 00:20:33.000
<v Speaker 1>and,</v>
<v Speaker 1>uh,</v>

286
00:20:33.110 --> 00:20:33.943
<v Speaker 1>the pooling operation.</v>
<v Speaker 1>And the second part is how we're </v>

287
00:20:38.621 --> 00:20:39.454
<v Speaker 1>actually performing the classification,</v>
<v Speaker 1>the convolutional and pooling layers </v>

288
00:20:43.451 --> 00:20:46.060
<v Speaker 1>output,</v>
<v Speaker 1>high level features of the input data,</v>

289
00:20:46.420 --> 00:20:47.253
<v Speaker 1>and we can these into fully connected </v>
<v Speaker 1>layers to perform the actual </v>

290
00:20:50.681 --> 00:20:51.514
<v Speaker 1>classification.</v>
<v Speaker 1>And the output of the fully connected </v>

291
00:20:54.791 --> 00:20:55.624
<v Speaker 1>layers in practice is a probability </v>
<v Speaker 1>distribution for an input images </v>

292
00:21:00.460 --> 00:21:01.293
<v Speaker 1>membership over set of possible classes.</v>
<v Speaker 1>And a common way to do this is using a </v>

293
00:21:06.641 --> 00:21:07.474
<v Speaker 1>function called soft Max,</v>
<v Speaker 1>where the output represents this </v>

294
00:21:10.541 --> 00:21:12.760
<v Speaker 1>categorical probability distribution.</v>

295
00:21:15.310 --> 00:21:18.100
<v Speaker 1>Now that we've gone through all the </v>
<v Speaker 1>components of a CNN,</v>

296
00:21:18.340 --> 00:21:20.680
<v Speaker 1>all that's left is to discuss how to </v>
<v Speaker 1>train him.</v>

297
00:21:21.400 --> 00:21:22.233
<v Speaker 1>Again,</v>
<v Speaker 1>we'll go back to a CNN for image </v>

298
00:21:24.041 --> 00:21:24.874
<v Speaker 1>classification.</v>
<v Speaker 1>During training we learned the weights </v>

299
00:21:27.551 --> 00:21:31.720
<v Speaker 1>of the convolutional filters.</v>
<v Speaker 1>What features the network is detecting,</v>

300
00:21:32.200 --> 00:21:35.620
<v Speaker 1>and in this case will also learn the </v>
<v Speaker 1>weights of the fully connected layers.</v>

301
00:21:36.310 --> 00:21:40.420
<v Speaker 1>Since the output for classification is a</v>
<v Speaker 1>probability distribution,</v>

302
00:21:40.870 --> 00:21:44.980
<v Speaker 1>we can use cross entropy loss for </v>
<v Speaker 1>optimization with back prop.</v>

303
00:21:47.350 --> 00:21:51.160
<v Speaker 1>Okay,</v>
<v Speaker 1>so I'd like to take a closer look at CNN</v>

304
00:21:51.190 --> 00:21:56.190
<v Speaker 1>for classification and discuss what is </v>
<v Speaker 1>arguably the most famous example of of a</v>

305
00:21:57.071 --> 00:21:57.904
<v Speaker 1>CNN,</v>
<v Speaker 1>the the ones trained and tested on the </v>

306
00:22:03.011 --> 00:22:03.844
<v Speaker 1>image net dataset.</v>
<v Speaker 1>Image net is a massive data set with </v>

307
00:22:07.270 --> 00:22:12.100
<v Speaker 1>over 14 million images spanning 20,000</v>
<v Speaker 1>different categories.</v>

308
00:22:12.730 --> 00:22:15.850
<v Speaker 1>For example,</v>
<v Speaker 1>there are 1,409</v>

309
00:22:15.880 --> 00:22:19.060
<v Speaker 1>different pictures of bananas in this </v>
<v Speaker 1>data set alone.</v>

310
00:22:19.870 --> 00:22:20.703
<v Speaker 1>Even better,</v>
<v Speaker 1>bananas are succinctly described as an </v>

311
00:22:23.890 --> 00:22:28.060
<v Speaker 1>elongated,</v>
<v Speaker 1>crushing shaped yellow fruit with soft,</v>

312
00:22:28.090 --> 00:22:28.923
<v Speaker 1>sweet flesh,</v>
<v Speaker 1>which both gives a pretty good </v>

313
00:22:31.061 --> 00:22:36.061
<v Speaker 1>descriptive value of what a banana is </v>
<v Speaker 1>and speaks to its deliciousness.</v>

314
00:22:38.080 --> 00:22:38.913
<v Speaker 1>So the creators of the image net dataset</v>
<v Speaker 1>also have created a set of visual </v>

315
00:22:42.941 --> 00:22:43.774
<v Speaker 1>recognition challenges on this Dataset.</v>
<v Speaker 1>And what's most famous is the image net </v>

316
00:22:48.671 --> 00:22:49.504
<v Speaker 1>classification task where users are </v>
<v Speaker 1>challengers are simply tasked with </v>

317
00:22:54.970 --> 00:22:59.710
<v Speaker 1>producing a list of the object </v>
<v Speaker 1>categories present in a given image.</v>

318
00:22:59.820 --> 00:23:02.200
<v Speaker 1>Oh,</v>
<v Speaker 1>across 1000 different categories.</v>

319
00:23:03.730 --> 00:23:07.180
<v Speaker 1>And the results of,</v>
<v Speaker 1>of that neural networks have had on this</v>

320
00:23:07.181 --> 00:23:09.610
<v Speaker 1>classification task are pretty </v>
<v Speaker 1>remarkable.</v>

321
00:23:10.180 --> 00:23:13.360
<v Speaker 1>Two thousand 12 was the first time a CNN</v>
<v Speaker 1>one.</v>

322
00:23:13.361 --> 00:23:15.990
<v Speaker 1>This challenge with the famous,</v>
<v Speaker 1>uh,</v>

323
00:23:16.010 --> 00:23:16.843
<v Speaker 1>Alex Net cnn,</v>
<v Speaker 1>and since then neural networks have </v>

324
00:23:19.660 --> 00:23:23.950
<v Speaker 1>dominated the competition and the error </v>
<v Speaker 1>has kept decreasing,</v>

325
00:23:24.160 --> 00:23:29.160
<v Speaker 1>surpassing human error in 2015 with the </v>
<v Speaker 1>resonant architecture.</v>

326
00:23:30.640 --> 00:23:31.473
<v Speaker 1>But with improved accuracy,</v>
<v Speaker 1>the number of layers in these networks </v>

327
00:23:35.531 --> 00:23:40.531
<v Speaker 1>has been increasing quite dramatically.</v>
<v Speaker 1>So there's a trade off here,</v>

328
00:23:40.931 --> 00:23:42.940
<v Speaker 1>right?</v>
<v Speaker 1>Build your network deeper.</v>

329
00:23:43.900 --> 00:23:45.100
<v Speaker 1>How deep can you go?</v>

330
00:23:47.450 --> 00:23:48.283
<v Speaker 1>So,</v>
<v Speaker 1>so far we've talked only about using </v>

331
00:23:49.761 --> 00:23:53.420
<v Speaker 1>CNNS for image classification,</v>
<v Speaker 1>but in reality,</v>

332
00:23:53.421 --> 00:23:54.254
<v Speaker 1>this idea of using convolutional layers </v>
<v Speaker 1>to extract features can extend to a </v>

333
00:23:58.851 --> 00:23:59.684
<v Speaker 1>number of different applications.</v>
<v Speaker 1>When we considered a CNN for </v>

334
00:24:03.771 --> 00:24:04.604
<v Speaker 1>classification,</v>
<v Speaker 1>we saw that we could think of it in two </v>

335
00:24:06.321 --> 00:24:09.290
<v Speaker 1>parts,</v>
<v Speaker 1>feature learning and the classification.</v>

336
00:24:11.990 --> 00:24:12.823
<v Speaker 1>What is at the core of the of </v>
<v Speaker 1>convolutional neural networks is is the </v>

337
00:24:16.881 --> 00:24:19.400
<v Speaker 1>feature learning pipeline.</v>
<v Speaker 1>After that,</v>

338
00:24:19.401 --> 00:24:24.401
<v Speaker 1>we can really change what follows to </v>
<v Speaker 1>suit the application that we desire.</v>

339
00:24:25.580 --> 00:24:26.413
<v Speaker 1>For example,</v>
<v Speaker 1>the portion following the convolutional </v>

340
00:24:28.911 --> 00:24:32.540
<v Speaker 1>layers may look different for different </v>
<v Speaker 1>image classification domains.</v>

341
00:24:32.990 --> 00:24:33.823
<v Speaker 1>We can also introduce new architectures </v>
<v Speaker 1>beyond fully connected layers for tasks </v>

342
00:24:38.121 --> 00:24:40.820
<v Speaker 1>such as segmentation and image </v>
<v Speaker 1>captioning.</v>

343
00:24:42.710 --> 00:24:43.543
<v Speaker 1>So today I'd like to go through three </v>
<v Speaker 1>different applications of CNNS beyond </v>

344
00:24:47.241 --> 00:24:50.720
<v Speaker 1>image classification,</v>
<v Speaker 1>semantic segmentation,</v>

345
00:24:50.750 --> 00:24:51.583
<v Speaker 1>where the task is to assign each pixel </v>
<v Speaker 1>in an image and object class to produce </v>

346
00:24:57.051 --> 00:25:00.470
<v Speaker 1>a segmentation of that image object </v>
<v Speaker 1>detection,</v>

347
00:25:00.560 --> 00:25:01.393
<v Speaker 1>where we are tasked with detecting </v>
<v Speaker 1>instances of semantic objects in an </v>

348
00:25:05.571 --> 00:25:06.404
<v Speaker 1>image and image captioning where the </v>
<v Speaker 1>task is to generate a short description </v>

349
00:25:10.851 --> 00:25:13.580
<v Speaker 1>of the image that captures it's semantic</v>
<v Speaker 1>content.</v>

350
00:25:15.110 --> 00:25:15.943
<v Speaker 1>So first,</v>
<v Speaker 1>let's talk about semantic segmentation </v>

351
00:25:17.990 --> 00:25:21.260
<v Speaker 1>with fully convolutional networks or </v>
<v Speaker 1>fcns.</v>

352
00:25:21.920 --> 00:25:26.120
<v Speaker 1>Here the network again,</v>
<v Speaker 1>takes a image input of arbitrary size,</v>

353
00:25:26.600 --> 00:25:27.433
<v Speaker 1>but instead it has to produce a </v>
<v Speaker 1>correspondingly sized output where each </v>

354
00:25:31.521 --> 00:25:32.354
<v Speaker 1>pixel has been assigned a class label,</v>
<v Speaker 1>which we can then visualize as a </v>

355
00:25:36.501 --> 00:25:41.501
<v Speaker 1>segmentation as we saw before with CNNS </v>
<v Speaker 1>for image classification,</v>

356
00:25:42.530 --> 00:25:43.363
<v Speaker 1>we first have a series of convolutional </v>
<v Speaker 1>layers that are downsampling operations </v>

357
00:25:46.731 --> 00:25:47.564
<v Speaker 1>for feature extraction,</v>
<v Speaker 1>and this results in a hierarchy of </v>

358
00:25:50.331 --> 00:25:51.164
<v Speaker 1>learned features.</v>
<v Speaker 1>Now the difference is that we have a </v>

359
00:25:54.831 --> 00:25:55.664
<v Speaker 1>series of upsampling operations to </v>
<v Speaker 1>increase the resolution of our output </v>

360
00:26:00.650 --> 00:26:03.350
<v Speaker 1>from the fully from the convolutional </v>
<v Speaker 1>layers,</v>

361
00:26:05.320 --> 00:26:06.153
<v Speaker 1>and to then combine this output of the </v>
<v Speaker 1>upsampling operations with those from </v>

362
00:26:10.791 --> 00:26:13.940
<v Speaker 1>our downsampling path to produce a </v>
<v Speaker 1>segmentation.</v>

363
00:26:15.410 --> 00:26:16.243
<v Speaker 1>One application of this sort of </v>
<v Speaker 1>architecture is to the real time </v>

364
00:26:19.160 --> 00:26:24.020
<v Speaker 1>segmentation of the driving scene here.</v>
<v Speaker 1>The network has this encoder,</v>

365
00:26:24.021 --> 00:26:24.854
<v Speaker 1>decoder architecture for encoding the </v>
<v Speaker 1>architecture is very similar to what we </v>

366
00:26:30.081 --> 00:26:31.700
<v Speaker 1>discussed earlier,</v>
<v Speaker 1>earlier,</v>

367
00:26:32.030 --> 00:26:35.390
<v Speaker 1>convolutional layers to learn a </v>
<v Speaker 1>hierarchy of feature maps,</v>

368
00:26:35.870 --> 00:26:36.703
<v Speaker 1>and then the decoder portion of the </v>
<v Speaker 1>network actually uses the indices from </v>

369
00:26:40.670 --> 00:26:44.970
<v Speaker 1>pooling operations to up sample from </v>
<v Speaker 1>these feature maps.</v>

370
00:26:45.030 --> 00:26:45.863
<v Speaker 1>And output a segmentation,</v>
<v Speaker 1>another way CNNS have been extended and </v>

371
00:26:51.541 --> 00:26:52.374
<v Speaker 1>applied is for object detection where </v>
<v Speaker 1>we're trying to learn features that </v>

372
00:26:56.341 --> 00:27:01.341
<v Speaker 1>characterize particular regions of the </v>
<v Speaker 1>input and then classify those regions.</v>

373
00:27:01.830 --> 00:27:06.030
<v Speaker 1>The original pipeline for doing this </v>
<v Speaker 1>called our CNN is pretty straightforward</v>

374
00:27:06.720 --> 00:27:10.710
<v Speaker 1>given an input image.</v>
<v Speaker 1>The algorithm extracts region proposals,</v>

375
00:27:10.711 --> 00:27:11.544
<v Speaker 1>bottom up,</v>
<v Speaker 1>computes features for each of these </v>

376
00:27:13.861 --> 00:27:18.861
<v Speaker 1>proposals using convolutional layers and</v>
<v Speaker 1>then classifies each region proposal,</v>

377
00:27:20.820 --> 00:27:24.270
<v Speaker 1>but there's a huge downside to this.</v>
<v Speaker 1>So in their.</v>

378
00:27:24.300 --> 00:27:25.133
<v Speaker 1>In their original pipeline,</v>
<v Speaker 1>this group extracted about 2000 </v>

379
00:27:29.191 --> 00:27:30.024
<v Speaker 1>different region proposals,</v>
<v Speaker 1>which meant that they had to run 2000 </v>

380
00:27:33.151 --> 00:27:36.780
<v Speaker 1>CNNS for feature extraction.</v>
<v Speaker 1>So since then there.</v>

381
00:27:37.050 --> 00:27:39.270
<v Speaker 1>And that takes a really,</v>
<v Speaker 1>really long time.</v>

382
00:27:40.370 --> 00:27:43.560
<v Speaker 1>So since then there have been extensions</v>
<v Speaker 1>of this basic idea,</v>

383
00:27:44.040 --> 00:27:44.873
<v Speaker 1>one being to first run the CNN on the </v>
<v Speaker 1>input image to first extract features </v>

384
00:27:49.590 --> 00:27:52.980
<v Speaker 1>and then get region proposals from the </v>
<v Speaker 1>feature maps.</v>

385
00:27:54.720 --> 00:27:57.000
<v Speaker 1>Finally,</v>
<v Speaker 1>let's consider image captioning.</v>

386
00:27:57.210 --> 00:28:02.210
<v Speaker 1>So suppose we're given an image of a cat</v>
<v Speaker 1>riding the skateboard in classification,</v>

387
00:28:03.121 --> 00:28:06.810
<v Speaker 1>our task is to output a class label for </v>
<v Speaker 1>this image cap,</v>

388
00:28:07.470 --> 00:28:10.740
<v Speaker 1>and as we've probably hammered home by </v>
<v Speaker 1>now,</v>

389
00:28:11.040 --> 00:28:15.180
<v Speaker 1>this is done by feeding our input image </v>
<v Speaker 1>through a set of convolutional layers,</v>

390
00:28:15.510 --> 00:28:16.343
<v Speaker 1>extracting features,</v>
<v Speaker 1>and then passing these features onto </v>

391
00:28:18.901 --> 00:28:21.990
<v Speaker 1>fully connected layers.</v>
<v Speaker 1>To predict a label.</v>

392
00:28:23.510 --> 00:28:24.343
<v Speaker 1>Image captioning,</v>
<v Speaker 1>we want to generate a sentence that </v>

393
00:28:26.761 --> 00:28:29.850
<v Speaker 1>describes the semantic content of the </v>
<v Speaker 1>same image.</v>

394
00:28:30.420 --> 00:28:31.253
<v Speaker 1>So let's take the same network from </v>
<v Speaker 1>before and remove the fully connected </v>

395
00:28:34.711 --> 00:28:38.820
<v Speaker 1>layers at the end.</v>
<v Speaker 1>Now we only have convolutional layers to</v>

396
00:28:38.821 --> 00:28:39.654
<v Speaker 1>extract features and we'll replace the </v>
<v Speaker 1>fully connected layers with a recurrent </v>

397
00:28:44.221 --> 00:28:45.054
<v Speaker 1>neural network.</v>
<v Speaker 1>The output of the convolutional layers </v>

398
00:28:47.791 --> 00:28:52.791
<v Speaker 1>gives us a fixed length encoding of the </v>
<v Speaker 1>features present in our input image,</v>

399
00:28:53.370 --> 00:28:54.203
<v Speaker 1>which we can use to initialize and rnn </v>
<v Speaker 1>that we can then train to predict the </v>

400
00:28:58.801 --> 00:29:03.801
<v Speaker 1>words that describe this image I'm using</v>
<v Speaker 1>using the rnn.</v>

401
00:29:06.570 --> 00:29:09.900
<v Speaker 1>So now that we've talked about </v>
<v Speaker 1>convolutional neural networks,</v>

402
00:29:09.960 --> 00:29:10.793
<v Speaker 1>they're applications.</v>
<v Speaker 1>We can introduce some tools that have </v>

403
00:29:13.921 --> 00:29:18.921
<v Speaker 1>recently been been designed to probe and</v>
<v Speaker 1>visualize the inner workings of a CNN to</v>

404
00:29:19.831 --> 00:29:23.100
<v Speaker 1>get at this question of what is the </v>
<v Speaker 1>network actually seeing.</v>

405
00:29:24.450 --> 00:29:26.130
<v Speaker 1>So first off,</v>
<v Speaker 1>a few years ago,</v>

406
00:29:26.131 --> 00:29:26.964
<v Speaker 1>there was a paper that published an </v>
<v Speaker 1>interactive visualization tool of a </v>

407
00:29:31.651 --> 00:29:36.450
<v Speaker 1>convolutional neural network trained on </v>
<v Speaker 1>a Dataset of handwritten digits,</v>

408
00:29:36.451 --> 00:29:37.284
<v Speaker 1>a very famous dataset called feminist,</v>
<v Speaker 1>and you can play around with this tool </v>

409
00:29:40.591 --> 00:29:41.424
<v Speaker 1>to the behavior of the network given a </v>
<v Speaker 1>number that you yourself drawn in and </v>

410
00:29:46.541 --> 00:29:47.374
<v Speaker 1>what you're seeing here is the feature </v>
<v Speaker 1>maps for this seven that someone has </v>

411
00:29:51.041 --> 00:29:54.790
<v Speaker 1>drawn in and as we can see in the first </v>
<v Speaker 1>layer,</v>

412
00:29:54.820 --> 00:29:55.653
<v Speaker 1>the first six filters are showing </v>
<v Speaker 1>primarily edge detection and deeper </v>

413
00:30:00.281 --> 00:30:03.040
<v Speaker 1>layers will start to pick up on corners,</v>
<v Speaker 1>crosses,</v>

414
00:30:03.041 --> 00:30:05.380
<v Speaker 1>curves,</v>
<v Speaker 1>more complex features,</v>

415
00:30:06.460 --> 00:30:07.293
<v Speaker 1>the exact hierarchy that that we </v>
<v Speaker 1>introduced in the beginning of this </v>

416
00:30:10.781 --> 00:30:11.614
<v Speaker 1>lecture.</v>

417
00:30:12.750 --> 00:30:13.583
<v Speaker 1>A second method which you'll you </v>
<v Speaker 1>yourself will have the chance to play </v>

418
00:30:16.511 --> 00:30:19.780
<v Speaker 1>around with in the second lap is called </v>
<v Speaker 1>class activation.</v>

419
00:30:19.781 --> 00:30:20.614
<v Speaker 1>Maps or cans,</v>
<v Speaker 1>cans generate a heat map that indicates </v>

420
00:30:24.940 --> 00:30:29.940
<v Speaker 1>the regions have an image to which are </v>
<v Speaker 1>CNN for classification attends to in its</v>

421
00:30:31.421 --> 00:30:32.254
<v Speaker 1>final layers,</v>
<v Speaker 1>and the way that this is computed is is </v>

422
00:30:36.250 --> 00:30:37.083
<v Speaker 1>the following.</v>
<v Speaker 1>We choose an output class that we want </v>

423
00:30:39.551 --> 00:30:40.384
<v Speaker 1>to visualize.</v>
<v Speaker 1>We can then obtain the weights from the </v>

424
00:30:42.971 --> 00:30:43.804
<v Speaker 1>last fully connected layer because these</v>
<v Speaker 1>represent the importance of each of the </v>

425
00:30:47.801 --> 00:30:50.920
<v Speaker 1>final feature maps.</v>
<v Speaker 1>In outputting that class,</v>

426
00:30:51.850 --> 00:30:52.683
<v Speaker 1>we can compute our heat map as simply a </v>
<v Speaker 1>weighted combination of each of the </v>

427
00:30:56.681 --> 00:30:57.514
<v Speaker 1>final convolutional feature maps.</v>
<v Speaker 1>Using the weights from the last fully </v>

428
00:31:01.151 --> 00:31:01.984
<v Speaker 1>connected layer,</v>
<v Speaker 1>we can apply cams to visualize both the </v>

429
00:31:06.701 --> 00:31:07.534
<v Speaker 1>activation maps for the most likely </v>
<v Speaker 1>predictions of an object class for one </v>

430
00:31:12.581 --> 00:31:13.414
<v Speaker 1>image as you see on the left,</v>
<v Speaker 1>and also to visualize the image regions </v>

431
00:31:18.221 --> 00:31:23.221
<v Speaker 1>used by the CNN to identify in different</v>
<v Speaker 1>instances of one object class.</v>

432
00:31:24.160 --> 00:31:25.930
<v Speaker 1>As you see on the right.</v>

433
00:31:27.970 --> 00:31:28.803
<v Speaker 1>So to conclude this talk,</v>
<v Speaker 1>I'd like to take a brief consideration </v>

434
00:31:33.281 --> 00:31:34.114
<v Speaker 1>of how deep learning for computer vision</v>
<v Speaker 1>has really made an impact over the past </v>

435
00:31:40.510 --> 00:31:41.343
<v Speaker 1>several years and the advances that have</v>
<v Speaker 1>been made in deep learning for computer </v>

436
00:31:44.321 --> 00:31:48.070
<v Speaker 1>vision would really not be possible </v>
<v Speaker 1>without the availability,</v>

437
00:31:48.280 --> 00:31:52.480
<v Speaker 1>availability of large and well annotated</v>
<v Speaker 1>image data sets.</v>

438
00:31:53.320 --> 00:31:57.250
<v Speaker 1>And this has really facilitated the </v>
<v Speaker 1>progress that's been made.</v>

439
00:31:57.580 --> 00:31:58.413
<v Speaker 1>Some datasets include image net,</v>
<v Speaker 1>which we discussed amness a data data </v>

440
00:32:03.510 --> 00:32:04.343
<v Speaker 1>Dataset of handwritten digits,</v>
<v Speaker 1>which was used in some of the first big </v>

441
00:32:08.080 --> 00:32:08.913
<v Speaker 1>CNN papers,</v>
<v Speaker 1>places a database from here at mit of </v>

442
00:32:12.760 --> 00:32:13.593
<v Speaker 1>scenes and landscapes and cipher 10,</v>
<v Speaker 1>which contains images from 10 different </v>

443
00:32:18.221 --> 00:32:23.221
<v Speaker 1>categories listed here.</v>
<v Speaker 1>The impact has been broad and deep,</v>

444
00:32:24.220 --> 00:32:27.140
<v Speaker 1>um,</v>
<v Speaker 1>and spanning a variety of,</v>

445
00:32:27.141 --> 00:32:32.141
<v Speaker 1>of different fields.</v>
<v Speaker 1>Everything from medicine to self driving</v>

446
00:32:32.561 --> 00:32:33.394
<v Speaker 1>cars to security.</v>
<v Speaker 1>One area in which convolutional neural </v>

447
00:32:38.111 --> 00:32:42.770
<v Speaker 1>networks really made a big impact early </v>
<v Speaker 1>on was in facial recognition software.</v>

448
00:32:43.010 --> 00:32:43.843
<v Speaker 1>And if you think about it nowadays,</v>
<v Speaker 1>this software is pretty much ubiquitous </v>

449
00:32:46.880 --> 00:32:51.880
<v Speaker 1>from social media to security systems.</v>
<v Speaker 1>Another area that's generate,</v>

450
00:32:52.760 --> 00:32:57.200
<v Speaker 1>generated a lot of excitement as of late</v>
<v Speaker 1>is in autonomous vehicles.</v>

451
00:32:57.201 --> 00:33:02.000
<v Speaker 1>And self driving cars,</v>
<v Speaker 1>so Nvidia has a research team working on</v>

452
00:33:02.001 --> 00:33:05.990
<v Speaker 1>a CNN based system for end to end </v>
<v Speaker 1>control of self driving cars,</v>

453
00:33:06.740 --> 00:33:11.720
<v Speaker 1>their pipeline pizza,</v>
<v Speaker 1>single image from a camera on the car to</v>

454
00:33:11.721 --> 00:33:15.320
<v Speaker 1>a CNN that directly outputs a single </v>
<v Speaker 1>number,</v>

455
00:33:15.530 --> 00:33:20.210
<v Speaker 1>which is the steering is the predicted </v>
<v Speaker 1>steering wheel angle and the man you see</v>

456
00:33:20.211 --> 00:33:21.044
<v Speaker 1>in this video is actually one of our </v>
<v Speaker 1>guests lecturers and on Thursday we'll </v>

457
00:33:26.421 --> 00:33:30.020
<v Speaker 1>hear about how his team is,</v>
<v Speaker 1>is developing this platform.</v>

458
00:33:31.820 --> 00:33:32.653
<v Speaker 1>Finally,</v>
<v Speaker 1>we've also seen a significant impact in </v>

459
00:33:34.731 --> 00:33:38.870
<v Speaker 1>the medical field where deep learning </v>
<v Speaker 1>models have been applied to the analysis</v>

460
00:33:38.871 --> 00:33:42.050
<v Speaker 1>of a whole host of different types of </v>
<v Speaker 1>medical images.</v>

461
00:33:42.620 --> 00:33:43.453
<v Speaker 1>Just this past year,</v>
<v Speaker 1>a team from Stanford developed a CNN </v>

462
00:33:46.461 --> 00:33:50.480
<v Speaker 1>that could achieve dermatologists level </v>
<v Speaker 1>classification of skin lesions.</v>

463
00:33:51.140 --> 00:33:54.110
<v Speaker 1>So you could imagine,</v>
<v Speaker 1>and this is what they actually did,</v>

464
00:33:54.410 --> 00:33:57.320
<v Speaker 1>having an app on your phone where you </v>
<v Speaker 1>take a picture,</v>

465
00:33:57.890 --> 00:34:02.150
<v Speaker 1>upload that picture to the APP.</v>
<v Speaker 1>It's fed into a CNN that jen,</v>

466
00:34:02.240 --> 00:34:06.950
<v Speaker 1>that then generates a prediction of </v>
<v Speaker 1>whether or not that lesion is reason for</v>

467
00:34:06.951 --> 00:34:07.784
<v Speaker 1>concern.</v>
<v Speaker 1>So to summarize what we've covered in </v>

468
00:34:11.961 --> 00:34:12.794
<v Speaker 1>today's lecture,</v>
<v Speaker 1>we first considered the origins of the </v>

469
00:34:15.561 --> 00:34:19.910
<v Speaker 1>computer vision problem,</v>
<v Speaker 1>how we can represent images as arrays of</v>

470
00:34:19.911 --> 00:34:23.480
<v Speaker 1>pixel values and what convolutions are,</v>
<v Speaker 1>how they work.</v>

471
00:34:23.810 --> 00:34:24.643
<v Speaker 1>We then discussed the basic architecture</v>
<v Speaker 1>of CNNS and finally we extended this to </v>

472
00:34:29.961 --> 00:34:32.720
<v Speaker 1>consider some different applications,</v>
<v Speaker 1>um,</v>

473
00:34:32.870 --> 00:34:35.060
<v Speaker 1>of,</v>
<v Speaker 1>of convolutional neural networks,</v>

474
00:34:35.120 --> 00:34:38.930
<v Speaker 1>and also talked a bit about how we can </v>
<v Speaker 1>visualize their behavior.</v>

475
00:34:40.310 --> 00:34:41.060
<v Speaker 1>So,</v>
<v Speaker 1>with that,</v>

476
00:34:41.060 --> 00:34:42.680
<v Speaker 1>I'd like to conclude,</v>
<v Speaker 1>um,</v>

477
00:34:43.010 --> 00:34:47.330
<v Speaker 1>I'm happy to take questions after the </v>
<v Speaker 1>lecture portion is over.</v>

478
00:34:48.140 --> 00:34:51.500
<v Speaker 1>It's now my pleasure to introduce our </v>
<v Speaker 1>next speaker,</v>

479
00:34:51.560 --> 00:34:52.393
<v Speaker 1>a special guest professor Aaron </v>
<v Speaker 1>[inaudible] from the University of </v>

480
00:34:56.030 --> 00:34:56.863
<v Speaker 1>Montreal.</v>
<v Speaker 1>Professor Kerrville is one of the </v>

481
00:34:58.430 --> 00:35:01.370
<v Speaker 1>creators of generative adversarial </v>
<v Speaker 1>networks,</v>

482
00:35:01.400 --> 00:35:02.233
<v Speaker 1>and he'll be talking to us about deep </v>
<v Speaker 1>generative models and their </v>

483
00:35:05.031 --> 00:35:07.940
<v Speaker 1>applications.</v>
<v Speaker 1>So please join me in welcoming him.</v>

