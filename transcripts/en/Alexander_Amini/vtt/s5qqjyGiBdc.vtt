WEBVTT

1
00:00:02.540 --> 00:00:04.490
<v Speaker 1>All right.</v>
<v Speaker 1>Good morning everyone.</v>

2
00:00:06.730 --> 00:00:10.000
<v Speaker 1>My name is lise Friedman.</v>
<v Speaker 1>I'm a research scientist here at Mit.</v>

3
00:00:10.001 --> 00:00:13.600
<v Speaker 1>I build the autonomous vehicles and </v>
<v Speaker 1>perception systems for these vehicles.</v>

4
00:00:13.900 --> 00:00:16.060
<v Speaker 1>And today I,</v>
<v Speaker 1>I'd like to talk to him.</v>

5
00:00:16.120 --> 00:00:16.953
<v Speaker 1>First of all was great to be a part of </v>
<v Speaker 1>this amazing course and intro to deep </v>

6
00:00:20.081 --> 00:00:21.630
<v Speaker 1>learning.</v>
<v Speaker 1>Um,</v>

7
00:00:21.690 --> 00:00:22.523
<v Speaker 1>it's,</v>
<v Speaker 1>it's a wonderful course covering a </v>

8
00:00:24.490 --> 00:00:27.040
<v Speaker 1>pretty quickly but deeply,</v>
<v Speaker 1>uh,</v>

9
00:00:27.070 --> 00:00:29.050
<v Speaker 1>some of the fundamental aspects of deep </v>
<v Speaker 1>learning.</v>

10
00:00:29.051 --> 00:00:29.884
<v Speaker 1>This is awesome.</v>
<v Speaker 1>So the topic that I'm perhaps most </v>

11
00:00:32.501 --> 00:00:33.334
<v Speaker 1>passionate about from the perspective of</v>
<v Speaker 1>just being a researcher in artificial </v>

12
00:00:37.091 --> 00:00:39.910
<v Speaker 1>intelligence is deep reinforcement </v>
<v Speaker 1>learning,</v>

13
00:00:40.300 --> 00:00:45.300
<v Speaker 1>which is a set of ideas and methods that</v>
<v Speaker 1>teach agents to act in the real world.</v>

14
00:00:50.690 --> 00:00:51.340
<v Speaker 2>Okay.</v>

15
00:00:51.340 --> 00:00:55.090
<v Speaker 1>If you think about what machine learning</v>
<v Speaker 1>is,</v>

16
00:00:55.510 --> 00:01:00.510
<v Speaker 1>it allows us to make sense of the world,</v>
<v Speaker 1>but to truly have impact in the physical</v>

17
00:01:01.031 --> 00:01:04.570
<v Speaker 1>world,</v>
<v Speaker 1>we need to also act in that world.</v>

18
00:01:04.630 --> 00:01:05.463
<v Speaker 1>So you bring the understanding of that </v>
<v Speaker 1>you extract from the world through the </v>

19
00:01:07.841 --> 00:01:11.170
<v Speaker 1>perception systems and actually decide </v>
<v Speaker 1>to make actions.</v>

20
00:01:11.560 --> 00:01:12.393
<v Speaker 1>So you can think of intelligence systems</v>
<v Speaker 1>as this kind of stack from the top to </v>

21
00:01:15.221 --> 00:01:16.054
<v Speaker 1>the bottom and the environment.</v>
<v Speaker 1>At the top of the world that the Asian </v>

22
00:01:18.491 --> 00:01:19.324
<v Speaker 1>operates in.</v>
<v Speaker 1>And at the bottom is the factors that </v>

23
00:01:21.250 --> 00:01:22.810
<v Speaker 1>actually make,</v>
<v Speaker 1>uh,</v>

24
00:01:22.840 --> 00:01:26.110
<v Speaker 1>changes to the world by moving the </v>
<v Speaker 1>robot,</v>

25
00:01:26.140 --> 00:01:28.870
<v Speaker 1>moving the agent in a way that changes </v>
<v Speaker 1>the world,</v>

26
00:01:28.930 --> 00:01:29.763
<v Speaker 1>that acts in the world.</v>
<v Speaker 1>And so from the environment it goes to </v>

27
00:01:32.111 --> 00:01:36.010
<v Speaker 1>the sensors sensing the raw sensory </v>
<v Speaker 1>data,</v>

28
00:01:36.070 --> 00:01:39.100
<v Speaker 1>extracting the features,</v>
<v Speaker 1>making sense of that features,</v>

29
00:01:39.190 --> 00:01:40.023
<v Speaker 1>understanding for me representations </v>
<v Speaker 1>higher and higher order representations </v>

30
00:01:42.970 --> 00:01:45.970
<v Speaker 1>from those representations,</v>
<v Speaker 1>we gain knowledge that's useful,</v>

31
00:01:45.971 --> 00:01:46.804
<v Speaker 1>actionable.</v>

32
00:01:46.990 --> 00:01:50.620
<v Speaker 1>Finally,</v>
<v Speaker 1>we reason the thing that we hold so dear</v>

33
00:01:50.680 --> 00:01:55.060
<v Speaker 1>as human beings,</v>
<v Speaker 1>the reasoning that builds and aggregates</v>

34
00:01:55.061 --> 00:01:55.894
<v Speaker 1>knowledge bases and using that reasoning</v>
<v Speaker 1>form short term and long term plans to </v>

35
00:02:00.041 --> 00:02:02.950
<v Speaker 1>finally turn into action and act in that</v>
<v Speaker 1>world,</v>

36
00:02:02.980 --> 00:02:03.813
<v Speaker 1>changing it.</v>
<v Speaker 1>So that's kind of the stack of </v>

37
00:02:04.961 --> 00:02:09.280
<v Speaker 1>artificial intelligence.</v>
<v Speaker 1>And the question is how much in the same</v>

38
00:02:09.281 --> 00:02:10.114
<v Speaker 1>way as human beings,</v>
<v Speaker 1>we learn most of the stack when we're </v>

39
00:02:12.301 --> 00:02:13.134
<v Speaker 1>born,</v>
<v Speaker 1>we know very little and we'd take in </v>

40
00:02:14.501 --> 00:02:16.240
<v Speaker 1>five sensory,</v>
<v Speaker 1>uh,</v>

41
00:02:16.241 --> 00:02:19.390
<v Speaker 1>sources of sensory data and make sense </v>
<v Speaker 1>of the world,</v>

42
00:02:19.391 --> 00:02:21.700
<v Speaker 1>learn over time to act successful in </v>
<v Speaker 1>that world.</v>

43
00:02:22.000 --> 00:02:25.000
<v Speaker 1>How much?</v>
<v Speaker 1>The question is can we use deep learning</v>

44
00:02:25.001 --> 00:02:25.834
<v Speaker 1>methods to learn parts of the stack,</v>
<v Speaker 1>the modules of the stack or the entire </v>

45
00:02:29.201 --> 00:02:32.110
<v Speaker 1>stack end to end?</v>
<v Speaker 1>Let's go over them.</v>

46
00:02:32.140 --> 00:02:33.930
<v Speaker 1>Okay,</v>
<v Speaker 1>so for uh,</v>

47
00:02:34.000 --> 00:02:36.520
<v Speaker 1>robots that act in the world,</v>
<v Speaker 1>autonomous vehicles,</v>

48
00:02:36.521 --> 00:02:37.660
<v Speaker 1>humanoid,</v>
<v Speaker 1>robots,</v>

49
00:02:37.750 --> 00:02:39.640
<v Speaker 1>drones,</v>
<v Speaker 1>there are sensors,</v>

50
00:02:39.641 --> 00:02:40.474
<v Speaker 1>whether it's slide our camera and </v>
<v Speaker 1>microphone coming from the audio </v>

51
00:02:42.970 --> 00:02:43.803
<v Speaker 1>networking for the communications,</v>
<v Speaker 1>I am you getting the kinematics of the </v>

52
00:02:46.481 --> 00:02:47.410
<v Speaker 1>different vehicles.</v>

53
00:02:47.500 --> 00:02:50.890
<v Speaker 1>That's the raw data coming in.</v>
<v Speaker 1>That's the eyes,</v>

54
00:02:50.891 --> 00:02:53.500
<v Speaker 1>ears,</v>
<v Speaker 1>nose for robots.</v>

55
00:02:53.890 --> 00:02:54.723
<v Speaker 1>Then the ones who have the sensory data,</v>
<v Speaker 1>the task is to form representations on </v>

56
00:02:59.140 --> 00:03:00.010
<v Speaker 1>data.</v>
<v Speaker 1>Yeah.</v>

57
00:03:00.011 --> 00:03:02.960
<v Speaker 1>You make sense of this raw pixels,</v>
<v Speaker 1>a raw piece,</v>

58
00:03:02.961 --> 00:03:03.794
<v Speaker 1>pieces,</v>
<v Speaker 1>samples for whatever the sensor is and </v>

59
00:03:05.531 --> 00:03:06.364
<v Speaker 1>you start to try to piece it together </v>
<v Speaker 1>into something that can be used to gain </v>

60
00:03:11.291 --> 00:03:14.680
<v Speaker 1>understanding.</v>
<v Speaker 1>It's just numbers and those numbers need</v>

61
00:03:14.681 --> 00:03:17.380
<v Speaker 1>to be converted into something that can </v>
<v Speaker 1>be reasoned with.</v>

62
00:03:17.620 --> 00:03:21.550
<v Speaker 1>That's forming representations and </v>
<v Speaker 1>that's where machine learning,</v>

63
00:03:21.551 --> 00:03:24.760
<v Speaker 1>deep learning steps in and takes this </v>
<v Speaker 1>raw sensory data.</v>

64
00:03:24.970 --> 00:03:27.110
<v Speaker 1>The was some preprocessing,</v>
<v Speaker 1>some profit,</v>

65
00:03:27.111 --> 00:03:27.944
<v Speaker 1>some initial processing,</v>
<v Speaker 1>and for forming higher and higher order </v>

66
00:03:30.431 --> 00:03:31.264
<v Speaker 1>representation in that data that can be </v>
<v Speaker 1>reasoned with about in the computer </v>

67
00:03:34.661 --> 00:03:39.310
<v Speaker 1>vision from edges two faces,</v>
<v Speaker 1>two entire entities.</v>

68
00:03:39.430 --> 00:03:40.263
<v Speaker 1>And finally the interpretation,</v>
<v Speaker 1>the semantic interpretation of the </v>

69
00:03:43.631 --> 00:03:45.850
<v Speaker 1>scene.</v>
<v Speaker 1>That's machine learning,</v>

70
00:03:46.840 --> 00:03:50.410
<v Speaker 1>playing with the representations and the</v>
<v Speaker 1>reasoning part.</v>

71
00:03:50.410 --> 00:03:51.243
<v Speaker 1>One of the exciting fundamental open </v>
<v Speaker 1>challenges of machine learning is how </v>

72
00:03:57.160 --> 00:03:57.993
<v Speaker 1>does this greater and greater </v>
<v Speaker 1>representation that can be formed </v>

73
00:04:00.401 --> 00:04:04.240
<v Speaker 1>through deep neural networks can then </v>
<v Speaker 1>lead to reasoning,</v>

74
00:04:04.241 --> 00:04:06.460
<v Speaker 1>to building knowledge,</v>
<v Speaker 1>not just,</v>

75
00:04:06.600 --> 00:04:08.740
<v Speaker 1>uh,</v>
<v Speaker 1>a memorization task,</v>

76
00:04:08.741 --> 00:04:09.574
<v Speaker 1>which is taking a supervised learning,</v>
<v Speaker 1>memorizing patterns in the input data </v>

77
00:04:12.211 --> 00:04:16.480
<v Speaker 1>based on human annotations,</v>
<v Speaker 1>but also an extracting those patterns,</v>

78
00:04:16.481 --> 00:04:20.440
<v Speaker 1>but also then taking that knowledge and </v>
<v Speaker 1>building over time as we humans do,</v>

79
00:04:20.560 --> 00:04:21.393
<v Speaker 1>building into something that can be </v>
<v Speaker 1>called common sense and to knowledge </v>

80
00:04:23.501 --> 00:04:26.620
<v Speaker 1>basis in a very trivial task.</v>
<v Speaker 1>This means,</v>

81
00:04:26.890 --> 00:04:30.310
<v Speaker 1>uh,</v>
<v Speaker 1>aggregating fusing multiple types of,</v>

82
00:04:30.510 --> 00:04:31.990
<v Speaker 1>uh,</v>
<v Speaker 1>multiple,</v>

83
00:04:31.991 --> 00:04:33.910
<v Speaker 1>uh,</v>
<v Speaker 1>types of extraction of knowledge.</v>

84
00:04:34.030 --> 00:04:34.863
<v Speaker 1>So from image recognition,</v>
<v Speaker 1>you could think if it looks like a duck </v>

85
00:04:36.821 --> 00:04:39.490
<v Speaker 1>in an image,</v>
<v Speaker 1>it sounds like a duck when the audio and</v>

86
00:04:39.491 --> 00:04:42.100
<v Speaker 1>then you could do act activity </v>
<v Speaker 1>recognition with the video.</v>

87
00:04:42.400 --> 00:04:43.233
<v Speaker 1>Uh,</v>
<v Speaker 1>it a swims like a duck and it must be a </v>

88
00:04:44.501 --> 00:04:44.740
<v Speaker 1>duck.</v>

89
00:04:44.740 --> 00:04:47.260
<v Speaker 1>Just aggregating this trivial,</v>
<v Speaker 1>uh,</v>

90
00:04:47.530 --> 00:04:50.530
<v Speaker 1>different sources of information.</v>
<v Speaker 1>That's reasoning.</v>

91
00:04:50.950 --> 00:04:54.970
<v Speaker 1>Now I think a form the human perspective</v>
<v Speaker 1>from the very biased human perspective,</v>

92
00:04:55.000 --> 00:04:56.330
<v Speaker 1>one of the,</v>
<v Speaker 1>uh,</v>

93
00:04:56.710 --> 00:04:57.543
<v Speaker 1>illustrative aspects of reasoning is </v>
<v Speaker 1>theory improving is the moment of </v>

94
00:05:02.891 --> 00:05:03.724
<v Speaker 1>invention,</v>
<v Speaker 1>of creative genius of this breakthrough </v>

95
00:05:06.041 --> 00:05:09.880
<v Speaker 1>ideas as we humans come up with and mean</v>
<v Speaker 1>really these aren't new ideas.</v>

96
00:05:09.970 --> 00:05:11.860
<v Speaker 1>Whenever we come up with an interesting </v>
<v Speaker 1>idea,</v>

97
00:05:11.890 --> 00:05:12.723
<v Speaker 1>they're not new.</v>
<v Speaker 1>We're just collecting pieces of high </v>

98
00:05:15.700 --> 00:05:16.533
<v Speaker 1>word representations of knowledge that </v>
<v Speaker 1>we've gained over time and then piecing </v>

99
00:05:19.961 --> 00:05:20.794
<v Speaker 1>them together to form something,</v>
<v Speaker 1>some simple beautiful distillation that </v>

100
00:05:25.991 --> 00:05:27.630
<v Speaker 1>is useful for,</v>
<v Speaker 1>uh,</v>

101
00:05:27.640 --> 00:05:28.720
<v Speaker 1>for the rest of the world.</v>
<v Speaker 1>And,</v>

102
00:05:28.960 --> 00:05:30.280
<v Speaker 1>uh,</v>
<v Speaker 1>one of my favorite,</v>

103
00:05:30.490 --> 00:05:34.930
<v Speaker 1>uh,</v>
<v Speaker 1>sort of human stories and discoveries in</v>

104
00:05:34.931 --> 00:05:37.720
<v Speaker 1>pure theorem improving is from Oslo.</v>
<v Speaker 1>Last theorem.</v>

105
00:05:37.840 --> 00:05:42.040
<v Speaker 1>It stood for 358 years.</v>
<v Speaker 1>This is a trivial thing to explain.</v>

106
00:05:42.160 --> 00:05:44.120
<v Speaker 1>Most a six,</v>
<v Speaker 1>well,</v>

107
00:05:44.260 --> 00:05:47.410
<v Speaker 1>eight year olds can understand the </v>
<v Speaker 1>definition of the,</v>

108
00:05:47.680 --> 00:05:49.500
<v Speaker 1>uh,</v>
<v Speaker 1>the conjecture that x,</v>

109
00:05:49.501 --> 00:05:50.334
<v Speaker 1>the n plus y to the n equals z to the n </v>
<v Speaker 1>has no solution for and greater than a </v>

110
00:05:54.760 --> 00:05:56.260
<v Speaker 1>three greater than you going to the </v>
<v Speaker 1>three.</v>

111
00:05:56.620 --> 00:05:57.291
<v Speaker 1>Okay.</v>
<v Speaker 1>This,</v>

112
00:05:57.291 --> 00:05:59.060
<v Speaker 1>this has been unsolved,</v>
<v Speaker 1>has been,</v>

113
00:05:59.120 --> 00:05:59.953
<v Speaker 1>uh,</v>
<v Speaker 1>hundreds of thousands of people try to </v>

114
00:06:01.371 --> 00:06:02.630
<v Speaker 1>solve it.</v>
<v Speaker 1>And,</v>

115
00:06:02.631 --> 00:06:03.464
<v Speaker 1>uh,</v>
<v Speaker 1>finally Andrew wiles from Oxford and </v>

116
00:06:05.541 --> 00:06:09.560
<v Speaker 1>Princeton had this final breakthrough </v>
<v Speaker 1>moment in,</v>

117
00:06:09.561 --> 00:06:12.020
<v Speaker 1>uh,</v>
<v Speaker 1>1994.</v>

118
00:06:12.200 --> 00:06:15.830
<v Speaker 1>So he first approved in 1993 and then it</v>
<v Speaker 1>was shown that he failed.</v>

119
00:06:16.220 --> 00:06:16.921
<v Speaker 1>And then,</v>
<v Speaker 1>so that's,</v>

120
00:06:16.921 --> 00:06:18.200
<v Speaker 1>that's the,</v>
<v Speaker 1>the human drama.</v>

121
00:06:18.380 --> 00:06:19.760
<v Speaker 1>And then night.</v>
<v Speaker 1>So he had a,</v>

122
00:06:19.810 --> 00:06:20.643
<v Speaker 1>uh,</v>
<v Speaker 1>he spent a year trying to find the </v>

123
00:06:22.311 --> 00:06:23.750
<v Speaker 1>solution.</v>
<v Speaker 1>And there's this moment,</v>

124
00:06:23.960 --> 00:06:24.793
<v Speaker 1>this final breakthrough 358 years after </v>
<v Speaker 1>this first form that buy from us as a </v>

125
00:06:29.181 --> 00:06:30.380
<v Speaker 1>conjecture.</v>
<v Speaker 1>He,</v>

126
00:06:30.381 --> 00:06:31.011
<v Speaker 1>he,</v>
<v Speaker 1>uh,</v>

127
00:06:31.011 --> 00:06:34.430
<v Speaker 1>he said it was so incredibly beautiful.</v>
<v Speaker 1>It was so simple,</v>

128
00:06:34.431 --> 00:06:35.264
<v Speaker 1>so elegant.</v>
<v Speaker 1>I couldn't understand how I missed it </v>

129
00:06:37.310 --> 00:06:39.920
<v Speaker 1>and I just stared at it and this belief </v>
<v Speaker 1>for 20 minutes,</v>

130
00:06:40.280 --> 00:06:44.300
<v Speaker 1>this is him finally closing the loop,</v>
<v Speaker 1>figuring out the final proof.</v>

131
00:06:45.170 --> 00:06:48.140
<v Speaker 1>I just stared at it and,</v>
<v Speaker 1>but disbelief for 20 minutes.</v>

132
00:06:48.170 --> 00:06:49.003
<v Speaker 1>Then during the day I walked around the </v>
<v Speaker 1>department and I keep coming back to my </v>

133
00:06:51.801 --> 00:06:53.690
<v Speaker 1>desk looking to see if it was still </v>
<v Speaker 1>there.</v>

134
00:06:53.840 --> 00:06:56.090
<v Speaker 1>It was still there.</v>
<v Speaker 1>I couldn't contain myself.</v>

135
00:06:56.091 --> 00:06:56.924
<v Speaker 1>I was so excited.</v>
<v Speaker 1>It was the most important moment of my </v>

136
00:06:59.211 --> 00:07:01.850
<v Speaker 1>working life.</v>
<v Speaker 1>Nothing I ever done,</v>

137
00:07:02.030 --> 00:07:04.430
<v Speaker 1>nothing I ever do again,</v>
<v Speaker 1>we'll mean as much.</v>

138
00:07:04.580 --> 00:07:09.020
<v Speaker 1>So this moment of breakthrough,</v>
<v Speaker 1>how do we teach neural networks?</v>

139
00:07:09.021 --> 00:07:12.170
<v Speaker 1>How do we learn from data to achieve </v>
<v Speaker 1>this level of breakthrough?</v>

140
00:07:12.290 --> 00:07:17.270
<v Speaker 1>That's the open question that I want you</v>
<v Speaker 1>to sort of walk away from this part a of</v>

141
00:07:17.271 --> 00:07:22.250
<v Speaker 1>the lecture thinking about what is the </v>
<v Speaker 1>future of agents that think,</v>

142
00:07:22.370 --> 00:07:23.203
<v Speaker 1>and Alexandra,</v>
<v Speaker 1>we'll talk about the new future </v>

143
00:07:24.471 --> 00:07:25.304
<v Speaker 1>challenges next,</v>
<v Speaker 1>but what can we use deep reinforcement </v>

144
00:07:28.911 --> 00:07:29.744
<v Speaker 1>learning to extend past the memorization</v>
<v Speaker 1>pass pack pattern recognition into </v>

145
00:07:33.080 --> 00:07:35.870
<v Speaker 1>something like reasoning and achieving </v>
<v Speaker 1>this breakthrough moment.</v>

146
00:07:36.260 --> 00:07:37.093
<v Speaker 1>And at the very least something a </v>
<v Speaker 1>brilliantly in 1995 after Andrew Weil's </v>

147
00:07:42.590 --> 00:07:43.423
<v Speaker 1>homer Simpson and,</v>
<v Speaker 1>and those are fans of the simpsons </v>

148
00:07:45.980 --> 00:07:48.740
<v Speaker 1>actually proved them wrong.</v>
<v Speaker 1>This is very interesting.</v>

149
00:07:48.950 --> 00:07:49.783
<v Speaker 1>Uh,</v>
<v Speaker 1>so he found an example where it does </v>

150
00:07:50.751 --> 00:07:52.880
<v Speaker 1>hold true the Pharmas Theorem,</v>
<v Speaker 1>uh,</v>

151
00:07:52.881 --> 00:07:55.220
<v Speaker 1>to a certain number of digits after the </v>
<v Speaker 1>period.</v>

152
00:07:56.540 --> 00:07:58.370
<v Speaker 1>Okay.</v>
<v Speaker 1>And then finally,</v>

153
00:07:58.400 --> 00:07:59.233
<v Speaker 1>aggregating this knowledge into action.</v>
<v Speaker 1>That's what deeper enforcement learning </v>

154
00:08:01.611 --> 00:08:02.444
<v Speaker 1>is about.</v>
<v Speaker 1>Extracting patterns from raw data and </v>

155
00:08:05.600 --> 00:08:06.433
<v Speaker 1>then finally being able to estimate the </v>
<v Speaker 1>state of the world around the agent in </v>

156
00:08:12.201 --> 00:08:15.290
<v Speaker 1>order to make successful action that </v>
<v Speaker 1>completes a certain goal.</v>

157
00:08:17.900 --> 00:08:18.531
<v Speaker 1>Uh,</v>
<v Speaker 1>so,</v>

158
00:08:18.531 --> 00:08:19.364
<v Speaker 1>and I will talk about the difference </v>
<v Speaker 1>between agents that are learning from </v>

159
00:08:22.971 --> 00:08:23.804
<v Speaker 1>data and agents that are currently </v>
<v Speaker 1>successfully being able to operate in </v>

160
00:08:26.271 --> 00:08:27.470
<v Speaker 1>this world.</v>
<v Speaker 1>Example,</v>

161
00:08:27.480 --> 00:08:28.313
<v Speaker 1>the agents here from Boston dynamics I </v>
<v Speaker 1>ones that don't use any deeper </v>

162
00:08:31.071 --> 00:08:33.020
<v Speaker 1>enforcement learning,</v>
<v Speaker 1>they don't use any,</v>

163
00:08:33.080 --> 00:08:35.510
<v Speaker 1>they don't learn from data.</v>
<v Speaker 1>This is the open gap,</v>

164
00:08:35.511 --> 00:08:36.344
<v Speaker 1>the challenge that we have to solve.</v>
<v Speaker 1>How do we use reinforcement learning </v>

165
00:08:38.991 --> 00:08:40.880
<v Speaker 1>methods,</v>
<v Speaker 1>build robots,</v>

166
00:08:40.881 --> 00:08:44.000
<v Speaker 1>agents that act in the real world,</v>
<v Speaker 1>that learn from that world.</v>

167
00:08:44.440 --> 00:08:46.190
<v Speaker 1>Uh,</v>
<v Speaker 1>except for the perception task.</v>

168
00:08:46.460 --> 00:08:47.293
<v Speaker 1>So in this stack,</v>
<v Speaker 1>you can think from the environment to </v>

169
00:08:49.311 --> 00:08:54.311
<v Speaker 1>the effectors that promise the beautiful</v>
<v Speaker 1>power of deep learning is taking the raw</v>

170
00:08:54.870 --> 00:08:57.750
<v Speaker 1>sensory data and being able to,</v>
<v Speaker 1>in an automated way.</v>

171
00:08:57.751 --> 00:09:02.730
<v Speaker 1>Do feature learning to extract arbitrary</v>
<v Speaker 1>high order representations on that data.</v>

172
00:09:02.820 --> 00:09:06.990
<v Speaker 1>So it makes sense of the patterns in </v>
<v Speaker 1>order to be able to learn in supervise,</v>

173
00:09:07.770 --> 00:09:10.350
<v Speaker 1>learning to learn the mapping of those </v>
<v Speaker 1>patterns.</v>

174
00:09:10.440 --> 00:09:14.760
<v Speaker 1>Arbitrarily high order representations </v>
<v Speaker 1>on those patterns to uh,</v>

175
00:09:15.000 --> 00:09:16.020
<v Speaker 1>to,</v>
<v Speaker 1>uh,</v>

176
00:09:16.080 --> 00:09:16.913
<v Speaker 1>to,</v>
<v Speaker 1>to extract actionable useful knowledge </v>

177
00:09:19.320 --> 00:09:20.153
<v Speaker 1>that's in the red box.</v>
<v Speaker 1>So the promise of deep reinforcement </v>

178
00:09:22.441 --> 00:09:24.510
<v Speaker 1>learning why it's so exciting for</v>

179
00:09:25.270 --> 00:09:25.750
<v Speaker 3>okay.</v>

180
00:09:25.750 --> 00:09:27.500
<v Speaker 1>Uh,</v>
<v Speaker 1>artificial intelligence community,</v>

181
00:09:27.710 --> 00:09:28.543
<v Speaker 1>why it captivates our imagination and </v>
<v Speaker 1>about the possibility towards achieving </v>

182
00:09:32.421 --> 00:09:33.254
<v Speaker 1>human level.</v>
<v Speaker 1>General intelligence is that you can </v>

183
00:09:35.121 --> 00:09:40.121
<v Speaker 1>take not just the end to end extraction </v>
<v Speaker 1>of a knowledge from raw sensory data.</v>

184
00:09:40.880 --> 00:09:45.880
<v Speaker 1>You can also do end to end from raw </v>
<v Speaker 1>sensory data,</v>

185
00:09:46.520 --> 00:09:51.520
<v Speaker 1>be able to produce actions to brute </v>
<v Speaker 1>force,</v>

186
00:09:51.711 --> 00:09:55.520
<v Speaker 1>learn from the raw data,</v>
<v Speaker 1>the uh,</v>

187
00:09:55.580 --> 00:09:57.370
<v Speaker 1>the semantic,</v>
<v Speaker 1>uh,</v>

188
00:09:57.770 --> 00:09:58.603
<v Speaker 1>context,</v>
<v Speaker 1>the meaning of the world around you in </v>

189
00:10:00.291 --> 00:10:04.220
<v Speaker 1>order to successfully act in that world </v>
<v Speaker 1>and to end just like we humans do.</v>

190
00:10:04.221 --> 00:10:05.210
<v Speaker 1>That's the promise.</v>

191
00:10:06.690 --> 00:10:07.060
<v Speaker 3>Okay.</v>

192
00:10:07.060 --> 00:10:08.070
<v Speaker 1>But,</v>
<v Speaker 1>uh,</v>

193
00:10:08.140 --> 00:10:10.870
<v Speaker 1>we're in the very early stages of,</v>
<v Speaker 1>um,</v>

194
00:10:11.410 --> 00:10:12.760
<v Speaker 1>of achieving that promise.</v>

195
00:10:13.750 --> 00:10:14.340
<v Speaker 3>Yeah.</v>

196
00:10:14.340 --> 00:10:17.090
<v Speaker 1>So,</v>
<v Speaker 1>uh,</v>

197
00:10:17.100 --> 00:10:19.350
<v Speaker 1>and successful presentation must include</v>
<v Speaker 1>cats.</v>

198
00:10:19.351 --> 00:10:24.240
<v Speaker 1>So supervised learning and unsupervised </v>
<v Speaker 1>learning and reinforcement learning sits</v>

199
00:10:24.270 --> 00:10:26.670
<v Speaker 1>in the middle,</v>
<v Speaker 1>was a supervised learning.</v>

200
00:10:26.760 --> 00:10:28.910
<v Speaker 1>Most of the,</v>
<v Speaker 1>uh,</v>

201
00:10:29.040 --> 00:10:29.873
<v Speaker 1>the data has to come from the human.</v>
<v Speaker 1>They're the insights about what is </v>

202
00:10:33.991 --> 00:10:36.990
<v Speaker 1>inside the data has to come from human </v>
<v Speaker 1>annotations.</v>

203
00:10:37.110 --> 00:10:37.943
<v Speaker 1>And it's the task of the machine to </v>
<v Speaker 1>learn how to generalize based on those </v>

204
00:10:40.501 --> 00:10:44.280
<v Speaker 1>human annotations over future examples </v>
<v Speaker 1>it hasn't seen before.</v>

205
00:10:44.490 --> 00:10:47.100
<v Speaker 1>And unsupervised learning,</v>
<v Speaker 1>you have no human annotation.</v>

206
00:10:47.280 --> 00:10:50.820
<v Speaker 1>S reinforcement learning is somewhere in</v>
<v Speaker 1>between,</v>

207
00:10:50.821 --> 00:10:54.270
<v Speaker 1>closer on unsupervised learning or the </v>
<v Speaker 1>annotations from humans.</v>

208
00:10:54.271 --> 00:10:55.980
<v Speaker 1>The information and knowledge from </v>
<v Speaker 1>humans,</v>

209
00:10:55.981 --> 00:10:58.050
<v Speaker 1>it's very sparse,</v>
<v Speaker 1>extremely sparse.</v>

210
00:10:58.290 --> 00:11:01.020
<v Speaker 1>And so you have to use the temporal </v>
<v Speaker 1>dynamics,</v>

211
00:11:01.021 --> 00:11:04.140
<v Speaker 1>the fact that in time these,</v>
<v Speaker 1>the,</v>

212
00:11:04.141 --> 00:11:04.974
<v Speaker 1>uh,</v>
<v Speaker 1>the continuity of our world through </v>

213
00:11:08.431 --> 00:11:09.264
<v Speaker 1>time,</v>
<v Speaker 1>you have to use the sparse little </v>

214
00:11:11.371 --> 00:11:15.240
<v Speaker 1>rewards you have along the way to extend</v>
<v Speaker 1>that over the entirety of,</v>

215
00:11:15.270 --> 00:11:16.041
<v Speaker 1>uh,</v>
<v Speaker 1>the,</v>

216
00:11:16.041 --> 00:11:18.630
<v Speaker 1>the temporal domain to make some sense </v>
<v Speaker 1>of the world,</v>

217
00:11:18.631 --> 00:11:20.610
<v Speaker 1>even though the rewards are really </v>
<v Speaker 1>sparse.</v>

218
00:11:20.820 --> 00:11:22.830
<v Speaker 1>Those are the two cats learning,</v>
<v Speaker 1>uh,</v>

219
00:11:23.280 --> 00:11:25.350
<v Speaker 1>uh,</v>
<v Speaker 1>the Pavlov's cats,</v>

220
00:11:25.351 --> 00:11:27.930
<v Speaker 1>if you will.</v>
<v Speaker 1>Learning that I had to ring the door,</v>

221
00:11:28.500 --> 00:11:31.680
<v Speaker 1>ring the bell in order to get some food.</v>
<v Speaker 1>That's the basic for them,</v>

222
00:11:31.710 --> 00:11:35.400
<v Speaker 1>the reinforcement learning problem.</v>
<v Speaker 1>So from supervised learning,</v>

223
00:11:35.460 --> 00:11:39.510
<v Speaker 1>you can think of those networks as </v>
<v Speaker 1>memorizers reinforcement learning is you</v>

224
00:11:39.511 --> 00:11:43.440
<v Speaker 1>can think of them of crudely.</v>
<v Speaker 1>So as sort of brute force reasoning,</v>

225
00:11:43.590 --> 00:11:48.590
<v Speaker 1>trying to propagate rewards into </v>
<v Speaker 1>extending how to make sense of the world</v>

226
00:11:49.141 --> 00:11:52.650
<v Speaker 1>in order to act in it.</v>
<v Speaker 1>The pieces are simple.</v>

227
00:11:52.870 --> 00:11:55.210
<v Speaker 1>There's an environment,</v>
<v Speaker 1>there's an agent,</v>

228
00:11:55.390 --> 00:11:56.223
<v Speaker 1>it takes actions in that agent.</v>
<v Speaker 1>It senses the environment so there's </v>

229
00:11:58.601 --> 00:12:02.860
<v Speaker 1>always a state of the agent senses.</v>
<v Speaker 1>And then I always,</v>

230
00:12:03.190 --> 00:12:04.023
<v Speaker 1>when taking an action received some kind</v>
<v Speaker 1>of reward or punishment from that world </v>

231
00:12:08.230 --> 00:12:10.270
<v Speaker 1>so we can model any kind of world in </v>
<v Speaker 1>this way.</v>

232
00:12:10.730 --> 00:12:14.140
<v Speaker 1>They can model an arcade game breakout </v>
<v Speaker 1>here,</v>

233
00:12:14.170 --> 00:12:16.690
<v Speaker 1>Atari breakout,</v>
<v Speaker 1>the engine has the capacity to act,</v>

234
00:12:16.691 --> 00:12:17.530
<v Speaker 1>move the paddle.</v>

235
00:12:17.780 --> 00:12:20.890
<v Speaker 1>It has a,</v>
<v Speaker 1>it can influence the future state of the</v>

236
00:12:20.891 --> 00:12:23.830
<v Speaker 1>system by taking those actions and then </v>
<v Speaker 1>it's uses a award.</v>

237
00:12:23.831 --> 00:12:27.310
<v Speaker 1>There's a goal to this game.</v>
<v Speaker 1>The goal is to maximize teacher award.</v>

238
00:12:27.880 --> 00:12:31.900
<v Speaker 1>They could model the card call balancing</v>
<v Speaker 1>problem where the,</v>

239
00:12:31.901 --> 00:12:34.030
<v Speaker 1>you can control the polling angle,</v>
<v Speaker 1>angle or speed.</v>

240
00:12:34.060 --> 00:12:36.730
<v Speaker 1>You can control the cart position,</v>
<v Speaker 1>the horizontal velocity,</v>

241
00:12:37.030 --> 00:12:40.990
<v Speaker 1>the actions of that is the pushing the </v>
<v Speaker 1>cart,</v>

242
00:12:41.120 --> 00:12:44.470
<v Speaker 1>applying the force of the cart.</v>
<v Speaker 1>And the task is to balance the poll.</v>

243
00:12:44.980 --> 00:12:45.813
<v Speaker 1>Okay.</v>
<v Speaker 1>And their award is one at each time </v>

244
00:12:47.051 --> 00:12:48.300
<v Speaker 1>step,</v>
<v Speaker 1>the pull still upright.</v>

245
00:12:48.310 --> 00:12:49.143
<v Speaker 1>That's the goal.</v>
<v Speaker 1>So that's a simple formulation of state </v>

246
00:12:52.030 --> 00:12:52.863
<v Speaker 1>action reward.</v>
<v Speaker 1>You can play a game of doom with the </v>

247
00:12:55.421 --> 00:12:56.254
<v Speaker 1>state being the raw pixels of the,</v>
<v Speaker 1>of the game and the actions that are </v>

248
00:12:59.651 --> 00:13:00.484
<v Speaker 1>moving the uh,</v>
<v Speaker 1>the player around shooting and the </v>

249
00:13:03.651 --> 00:13:04.484
<v Speaker 1>reward,</v>
<v Speaker 1>a positive one at eliminating an </v>

250
00:13:07.271 --> 00:13:09.910
<v Speaker 1>opponent and negative when the agent has</v>
<v Speaker 1>eliminated,</v>

251
00:13:10.210 --> 00:13:11.043
<v Speaker 1>it's just a game,</v>
<v Speaker 1>industrial robotics and you kind of </v>

252
00:13:13.961 --> 00:13:17.170
<v Speaker 1>humanoid robotics when you have to </v>
<v Speaker 1>control multiple degrees of freedom,</v>

253
00:13:17.171 --> 00:13:19.240
<v Speaker 1>control the robotic arm control of the </v>
<v Speaker 1>robot.</v>

254
00:13:19.600 --> 00:13:20.433
<v Speaker 1>The state is the raw pixels of the real </v>
<v Speaker 1>world coming into the sensors of the </v>

255
00:13:23.591 --> 00:13:24.424
<v Speaker 1>robot.</v>
<v Speaker 1>The actions of the possible actions the </v>

256
00:13:26.021 --> 00:13:28.630
<v Speaker 1>robot can take.</v>
<v Speaker 1>So manipulating each of its sensors,</v>

257
00:13:28.631 --> 00:13:29.890
<v Speaker 1>actuators,</v>
<v Speaker 1>sorry,</v>

258
00:13:29.891 --> 00:13:30.724
<v Speaker 1>actuators.</v>
<v Speaker 1>And then their reward is positive and </v>

259
00:13:32.681 --> 00:13:35.080
<v Speaker 1>placing a device successfully negative </v>
<v Speaker 1>otherwise.</v>

260
00:13:35.081 --> 00:13:37.210
<v Speaker 1>So the task is to pick something up,</v>
<v Speaker 1>put it back down.</v>

261
00:13:38.440 --> 00:13:39.900
<v Speaker 1>Okay.</v>
<v Speaker 1>So and I,</v>

262
00:13:39.910 --> 00:13:40.720
<v Speaker 1>I'd like to,</v>
<v Speaker 1>uh,</v>

263
00:13:40.720 --> 00:13:42.490
<v Speaker 1>sort of,</v>
<v Speaker 1>uh,</v>

264
00:13:43.040 --> 00:13:43.873
<v Speaker 1>continuing this trajectory further and </v>
<v Speaker 1>further complex systems to think the </v>

265
00:13:46.841 --> 00:13:51.580
<v Speaker 1>biggest challenge for reinforcement </v>
<v Speaker 1>learning is a formulating the world that</v>

266
00:13:51.581 --> 00:13:54.870
<v Speaker 1>we need to solve the set of goals in </v>
<v Speaker 1>such a way that can be,</v>

267
00:13:54.910 --> 00:13:57.970
<v Speaker 1>we can apply these deep reinforcement or</v>
<v Speaker 1>reinforcement learning methods.</v>

268
00:13:58.180 --> 00:14:00.880
<v Speaker 1>So give you an intuition about us </v>
<v Speaker 1>humans.</v>

269
00:14:01.120 --> 00:14:01.953
<v Speaker 1>Uh,</v>
<v Speaker 1>it's exceptionally difficult to </v>

270
00:14:03.041 --> 00:14:06.190
<v Speaker 1>formulate the goals of life.</v>
<v Speaker 1>Was this a survival,</v>

271
00:14:06.340 --> 00:14:07.720
<v Speaker 1>homeostasis,</v>
<v Speaker 1>happiness?</v>

272
00:14:07.750 --> 00:14:09.380
<v Speaker 1>Who knows?</v>
<v Speaker 1>Uh,</v>

273
00:14:09.840 --> 00:14:11.890
<v Speaker 1>uh,</v>
<v Speaker 1>citations depending on who you are.</v>

274
00:14:12.190 --> 00:14:13.400
<v Speaker 1>Uh,</v>
<v Speaker 1>so the state,</v>

275
00:14:13.470 --> 00:14:15.130
<v Speaker 1>the state sight hearing,</v>
<v Speaker 1>taste,</v>

276
00:14:15.131 --> 00:14:15.641
<v Speaker 1>smell,</v>
<v Speaker 1>touch,</v>

277
00:14:15.641 --> 00:14:19.030
<v Speaker 1>that's the raw sensory data actions or </v>
<v Speaker 1>think move.</v>

278
00:14:19.120 --> 00:14:20.110
<v Speaker 1>What else?</v>
<v Speaker 1>I don't know.</v>

279
00:14:20.320 --> 00:14:22.750
<v Speaker 1>And then the reward is just a,</v>
<v Speaker 1>it's open.</v>

280
00:14:22.751 --> 00:14:23.584
<v Speaker 1>All of these questions are open.</v>
<v Speaker 1>So if we want to actually start to </v>

281
00:14:26.051 --> 00:14:27.730
<v Speaker 1>create more and more intelligent </v>
<v Speaker 1>systems,</v>

282
00:14:27.850 --> 00:14:31.030
<v Speaker 1>it's hard to formulate what the goals </v>
<v Speaker 1>are with the state spaces,</v>

283
00:14:31.090 --> 00:14:33.910
<v Speaker 1>with the action spaces.</v>
<v Speaker 1>That's the,</v>

284
00:14:34.210 --> 00:14:35.043
<v Speaker 1>that's if this the,</v>
<v Speaker 1>if you take away anything sort of in a </v>

285
00:14:37.181 --> 00:14:38.014
<v Speaker 1>practical sense from today,</v>
<v Speaker 1>a form from the deeper enforcement </v>

286
00:14:40.661 --> 00:14:41.494
<v Speaker 1>learning part here,</v>
<v Speaker 1>a few slides is that there's a fun part </v>

287
00:14:45.591 --> 00:14:49.180
<v Speaker 1>and a hard part too to all of this work.</v>
<v Speaker 1>So the fun part,</v>

288
00:14:49.181 --> 00:14:50.014
<v Speaker 1>the what this course is about,</v>
<v Speaker 1>I hope it's inspire people about the </v>

289
00:14:53.211 --> 00:14:54.044
<v Speaker 1>amazing,</v>
<v Speaker 1>interesting fundamental algorithms of </v>

290
00:14:55.701 --> 00:14:56.534
<v Speaker 1>deep learning as the fun part.</v>
<v Speaker 1>The hard part is the collecting and </v>

291
00:15:01.851 --> 00:15:05.970
<v Speaker 1>annotating huge amount of representative</v>
<v Speaker 1>data in deep learning and forming higher</v>

292
00:15:06.000 --> 00:15:06.920
<v Speaker 1>representations.</v>

293
00:15:06.980 --> 00:15:11.210
<v Speaker 1>Data does the hard work.</v>
<v Speaker 1>So data is everything.</v>

294
00:15:11.270 --> 00:15:14.210
<v Speaker 1>Once you have good algorithms,</v>
<v Speaker 1>data is everything in deep reinforcement</v>

295
00:15:14.211 --> 00:15:18.510
<v Speaker 1>learning the heart.</v>
<v Speaker 1>The fun part again is these algorithms,</v>

296
00:15:18.530 --> 00:15:21.320
<v Speaker 1>we'll talk about them today,</v>
<v Speaker 1>a little overview them,</v>

297
00:15:21.680 --> 00:15:26.540
<v Speaker 1>but the hard part is defining the world,</v>
<v Speaker 1>the action space and the reward space.</v>

298
00:15:26.790 --> 00:15:27.623
<v Speaker 1>It's just the defining,</v>
<v Speaker 1>formalizing the problem is actually </v>

299
00:15:30.981 --> 00:15:34.490
<v Speaker 1>exceptionally difficult when you start </v>
<v Speaker 1>to try to create,</v>

300
00:15:34.670 --> 00:15:37.430
<v Speaker 1>uh,</v>
<v Speaker 1>an agent that operates in the real world</v>

301
00:15:37.431 --> 00:15:38.264
<v Speaker 1>and actually operate with other human </v>
<v Speaker 1>beings and are actually significantly </v>

302
00:15:40.911 --> 00:15:41.744
<v Speaker 1>helps in the world.</v>
<v Speaker 1>So this isn't playing an arcade game </v>

303
00:15:44.450 --> 00:15:46.640
<v Speaker 1>where everything is clean or playing </v>
<v Speaker 1>chess go.</v>

304
00:15:46.910 --> 00:15:48.830
<v Speaker 1>It's when you're operating in the real </v>
<v Speaker 1>world,</v>

305
00:15:48.831 --> 00:15:50.900
<v Speaker 1>everything is messy.</v>
<v Speaker 1>How do you formalize that?</v>

306
00:15:50.901 --> 00:15:54.640
<v Speaker 1>That's the hard part.</v>
<v Speaker 1>And then the hardest part is getting out</v>

307
00:15:54.641 --> 00:15:57.590
<v Speaker 1>a lot of meaningful data that </v>
<v Speaker 1>represents,</v>

308
00:15:57.740 --> 00:16:00.110
<v Speaker 1>that fits into that formalization that </v>
<v Speaker 1>you formed.</v>

309
00:16:01.970 --> 00:16:03.020
<v Speaker 1>Okay?</v>
<v Speaker 1>In the,</v>

310
00:16:03.021 --> 00:16:03.854
<v Speaker 1>uh,</v>
<v Speaker 1>the mark off decision process that's </v>

311
00:16:05.181 --> 00:16:07.160
<v Speaker 1>underlying the thinking of reinforcement</v>
<v Speaker 1>learning.</v>

312
00:16:07.161 --> 00:16:08.750
<v Speaker 1>There's a state,</v>
<v Speaker 1>you take an action,</v>

313
00:16:08.751 --> 00:16:11.420
<v Speaker 1>receive her award and then observe the </v>
<v Speaker 1>next state.</v>

314
00:16:11.480 --> 00:16:12.313
<v Speaker 1>So it's always state action award state.</v>
<v Speaker 1>That's the sample of data you get as an </v>

315
00:16:15.711 --> 00:16:18.950
<v Speaker 1>agent acting in the world.</v>
<v Speaker 1>State action reward state.</v>

316
00:16:20.030 --> 00:16:23.690
<v Speaker 1>There's a policy where an agent tries to</v>
<v Speaker 1>form a,</v>

317
00:16:23.691 --> 00:16:27.200
<v Speaker 1>uh,</v>
<v Speaker 1>a policy how to act in the world so that</v>

318
00:16:27.201 --> 00:16:28.970
<v Speaker 1>in whatever state it's in,</v>
<v Speaker 1>it,</v>

319
00:16:29.030 --> 00:16:31.200
<v Speaker 1>it has a,</v>
<v Speaker 1>uh,</v>

320
00:16:31.220 --> 00:16:35.660
<v Speaker 1>a preference to a act in a certain way </v>
<v Speaker 1>in order to optimize their award.</v>

321
00:16:35.661 --> 00:16:39.860
<v Speaker 1>There's a value function that can </v>
<v Speaker 1>estimate how good a certain action isn't</v>

322
00:16:39.861 --> 00:16:40.694
<v Speaker 1>a certain state.</v>
<v Speaker 1>And there is sometimes a model that the </v>

323
00:16:43.161 --> 00:16:47.060
<v Speaker 1>agent forms or about the world.</v>
<v Speaker 1>A quick example,</v>

324
00:16:47.090 --> 00:16:47.923
<v Speaker 1>you can have a robot in a room at the </v>
<v Speaker 1>bottom left starting moving about this </v>

325
00:16:50.871 --> 00:16:51.170
<v Speaker 1>room.</v>

326
00:16:51.170 --> 00:16:54.860
<v Speaker 1>It's a three by four grid.</v>
<v Speaker 1>It tries to get to the top right because</v>

327
00:16:54.861 --> 00:16:57.800
<v Speaker 1>it's a plus one,</v>
<v Speaker 1>but because this is the casting system,</v>

328
00:16:57.801 --> 00:17:00.020
<v Speaker 1>when it goes,</v>
<v Speaker 1>chooses to go up,</v>

329
00:17:00.080 --> 00:17:02.600
<v Speaker 1>it's sometimes goes left and right 10% </v>
<v Speaker 1>of the time.</v>

330
00:17:02.900 --> 00:17:05.160
<v Speaker 1>So in this world it has to try to,</v>
<v Speaker 1>uh,</v>

331
00:17:05.240 --> 00:17:07.490
<v Speaker 1>come up with a policy.</v>
<v Speaker 1>So what's a policy?</v>

332
00:17:07.491 --> 00:17:10.670
<v Speaker 1>Is this a solution?</v>
<v Speaker 1>So as starting at the bottom,</v>

333
00:17:10.790 --> 00:17:14.420
<v Speaker 1>arrows show the actions whenever you in </v>
<v Speaker 1>that state that you would like to take.</v>

334
00:17:14.630 --> 00:17:16.850
<v Speaker 1>So this is,</v>
<v Speaker 1>this is a pretty good solution to get to</v>

335
00:17:16.851 --> 00:17:19.940
<v Speaker 1>the plus one in a deterministic world.</v>
<v Speaker 1>And it's the cast,</v>

336
00:17:19.941 --> 00:17:20.990
<v Speaker 1>the quarreled.</v>
<v Speaker 1>Uh,</v>

337
00:17:21.020 --> 00:17:22.700
<v Speaker 1>when you don't,</v>
<v Speaker 1>when you go up,</v>

338
00:17:22.730 --> 00:17:24.330
<v Speaker 1>you don't always go up.</v>
<v Speaker 1>Uh,</v>

339
00:17:24.350 --> 00:17:25.183
<v Speaker 1>it's not a,</v>
<v Speaker 1>a optimal policy because you have to </v>

340
00:17:27.591 --> 00:17:28.424
<v Speaker 1>have an optimum policy,</v>
<v Speaker 1>has to have an answer for every single </v>

341
00:17:31.011 --> 00:17:32.030
<v Speaker 1>state you might be in.</v>

342
00:17:32.080 --> 00:17:32.913
<v Speaker 1>So this,</v>
<v Speaker 1>the outcome policy would look something </v>

343
00:17:34.191 --> 00:17:37.640
<v Speaker 1>like this.</v>
<v Speaker 1>Now that's for when the,</v>

344
00:17:37.700 --> 00:17:38.880
<v Speaker 1>uh,</v>
<v Speaker 1>the costs,</v>

345
00:17:38.900 --> 00:17:43.900
<v Speaker 1>the reward is negative 0.01</v>
<v Speaker 1>for taking a step now feet.</v>

346
00:17:44.840 --> 00:17:47.270
<v Speaker 1>Every time we take a step,</v>
<v Speaker 1>it's really painful.</v>

347
00:17:47.540 --> 00:17:49.740
<v Speaker 1>It's a,</v>
<v Speaker 1>you get a negative to reward.</v>

348
00:17:49.980 --> 00:17:51.570
<v Speaker 1>And so there's a,</v>
<v Speaker 1>the,</v>

349
00:17:51.630 --> 00:17:54.510
<v Speaker 1>the optimal policy changes there is no </v>
<v Speaker 1>matter what,</v>

350
00:17:54.511 --> 00:17:56.670
<v Speaker 1>no matter this tech cast,</v>
<v Speaker 1>the city of the system,</v>

351
00:17:56.910 --> 00:17:57.743
<v Speaker 1>the randomness,</v>
<v Speaker 1>you want to get to the end as fast as </v>

352
00:18:00.181 --> 00:18:02.220
<v Speaker 1>possible.</v>
<v Speaker 1>Even if you go to the negative states,</v>

353
00:18:02.490 --> 00:18:05.460
<v Speaker 1>you just want to get to the plus one as </v>
<v Speaker 1>quickly as possible.</v>

354
00:18:05.820 --> 00:18:06.653
<v Speaker 1>And then,</v>
<v Speaker 1>so the reward structure changes the </v>

355
00:18:08.731 --> 00:18:11.310
<v Speaker 1>optimum policy.</v>
<v Speaker 1>If you make the reward negative 0.1,</v>

356
00:18:11.490 --> 00:18:13.980
<v Speaker 1>then there's some more incentive to </v>
<v Speaker 1>explore.</v>

357
00:18:14.100 --> 00:18:18.660
<v Speaker 1>And as we increase the reward or </v>
<v Speaker 1>decrease the punishment of taking a step</v>

358
00:18:19.080 --> 00:18:23.370
<v Speaker 1>more and more exploration is encouraged </v>
<v Speaker 1>until we get to the kind of,</v>

359
00:18:23.371 --> 00:18:24.540
<v Speaker 1>uh,</v>
<v Speaker 1>uh,</v>

360
00:18:24.600 --> 00:18:26.490
<v Speaker 1>what I think of as college,</v>
<v Speaker 1>which is a,</v>

361
00:18:26.491 --> 00:18:30.030
<v Speaker 1>you encourage exploration by having a </v>
<v Speaker 1>positive word to moving around.</v>

362
00:18:30.170 --> 00:18:33.660
<v Speaker 1>And so you never want to get to the end,</v>
<v Speaker 1>you just kinda walk around the world,</v>

363
00:18:33.930 --> 00:18:35.700
<v Speaker 1>uh,</v>
<v Speaker 1>without ever reaching the end.</v>

364
00:18:36.570 --> 00:18:37.890
<v Speaker 1>Okay.</v>
<v Speaker 1>So there's a,</v>

365
00:18:38.040 --> 00:18:41.340
<v Speaker 1>the main goal is to really optimize </v>
<v Speaker 1>reward in this world.</v>

366
00:18:41.400 --> 00:18:42.233
<v Speaker 1>And uh,</v>
<v Speaker 1>because the reward is collected over </v>

367
00:18:43.561 --> 00:18:46.410
<v Speaker 1>time,</v>
<v Speaker 1>you want to have some estimate of future</v>

368
00:18:46.411 --> 00:18:50.220
<v Speaker 1>award and because you don't have a </v>
<v Speaker 1>perfect estimate of the future,</v>

369
00:18:50.370 --> 00:18:52.500
<v Speaker 1>you have to discount that reward over </v>
<v Speaker 1>time.</v>

370
00:18:53.430 --> 00:18:57.180
<v Speaker 1>So the goal is to maximize the </v>
<v Speaker 1>discounted reward over time.</v>

371
00:18:57.840 --> 00:18:59.880
<v Speaker 1>And in cue learning is,</v>
<v Speaker 1>uh,</v>

372
00:18:59.940 --> 00:19:02.100
<v Speaker 1>an approach that,</v>
<v Speaker 1>um,</v>

373
00:19:02.660 --> 00:19:03.391
<v Speaker 1>uh,</v>
<v Speaker 1>that,</v>

374
00:19:03.391 --> 00:19:04.470
<v Speaker 1>uh,</v>
<v Speaker 1>I'd like to talk to,</v>

375
00:19:04.510 --> 00:19:06.720
<v Speaker 1>to focus in on today.</v>
<v Speaker 1>The,</v>

376
00:19:06.721 --> 00:19:10.170
<v Speaker 1>there's a state action value cue.</v>
<v Speaker 1>It's a cue function.</v>

377
00:19:10.171 --> 00:19:12.960
<v Speaker 1>It takes an estate in action and it </v>
<v Speaker 1>tells you the value of that state.</v>

378
00:19:13.140 --> 00:19:15.520
<v Speaker 1>It's off policy because,</v>
<v Speaker 1>uh,</v>

379
00:19:15.570 --> 00:19:19.080
<v Speaker 1>we can learn this function without </v>
<v Speaker 1>forming a,</v>

380
00:19:19.081 --> 00:19:19.914
<v Speaker 1>an optimal,</v>
<v Speaker 1>without keeping an optimal policy and </v>

381
00:19:22.501 --> 00:19:24.150
<v Speaker 1>estimate of an optimal policy with us.</v>

382
00:19:24.540 --> 00:19:28.470
<v Speaker 1>And what it turns out with the equation </v>
<v Speaker 1>at the bottom with a bell mini equation,</v>

383
00:19:28.560 --> 00:19:30.560
<v Speaker 1>you can estimate,</v>
<v Speaker 1>uh,</v>

384
00:19:30.630 --> 00:19:31.463
<v Speaker 1>the,</v>
<v Speaker 1>you can update your estimate of the q </v>

385
00:19:33.511 --> 00:19:38.100
<v Speaker 1>function in such a way that over time it</v>
<v Speaker 1>converges to an optimal policy.</v>

386
00:19:38.490 --> 00:19:40.770
<v Speaker 1>And the update is simple.</v>
<v Speaker 1>You have an estimate,</v>

387
00:19:40.920 --> 00:19:43.270
<v Speaker 1>you start knowing nothing.</v>
<v Speaker 1>The A,</v>

388
00:19:43.280 --> 00:19:46.230
<v Speaker 1>there's an old s,</v>
<v Speaker 1>this estimate of an old state q,</v>

389
00:19:46.240 --> 00:19:48.420
<v Speaker 1>s,</v>
<v Speaker 1>a t and then you take an action,</v>

390
00:19:48.540 --> 00:19:49.373
<v Speaker 1>you collect her award and you update </v>
<v Speaker 1>your estimate based on the award he </v>

391
00:19:52.021 --> 00:19:52.854
<v Speaker 1>received and the difference between what</v>
<v Speaker 1>you expected and what you actually </v>

392
00:19:55.351 --> 00:19:57.180
<v Speaker 1>received.</v>
<v Speaker 1>And that's the update you.</v>

393
00:19:57.181 --> 00:19:58.014
<v Speaker 1>So you walk around this world exploring </v>
<v Speaker 1>until he formed better and better </v>

394
00:20:00.631 --> 00:20:03.900
<v Speaker 1>understanding of what is a good action </v>
<v Speaker 1>to take in each individual state.</v>

395
00:20:04.230 --> 00:20:05.063
<v Speaker 1>And there's always as in life and in </v>
<v Speaker 1>reinforcement learning in any agents </v>

396
00:20:08.131 --> 00:20:09.050
<v Speaker 1>that act,</v>
<v Speaker 1>you've,</v>

397
00:20:09.120 --> 00:20:11.610
<v Speaker 1>there's a learning stage where you have </v>
<v Speaker 1>to explore,</v>

398
00:20:11.611 --> 00:20:14.460
<v Speaker 1>exploring pays off when you know very </v>
<v Speaker 1>little.</v>

399
00:20:14.610 --> 00:20:15.443
<v Speaker 1>The more and more you learn,</v>
<v Speaker 1>the less and less valuable it is to </v>

400
00:20:18.271 --> 00:20:19.710
<v Speaker 1>explore.</v>
<v Speaker 1>And you weren't to exploit.</v>

401
00:20:19.860 --> 00:20:20.693
<v Speaker 1>You want to take greedy actions and </v>
<v Speaker 1>that's always the balance you start </v>

402
00:20:23.311 --> 00:20:24.144
<v Speaker 1>exploring at first.</v>
<v Speaker 1>But eventually you want to make some </v>

403
00:20:25.981 --> 00:20:26.814
<v Speaker 1>money.</v>
<v Speaker 1>You whatever the,</v>

404
00:20:27.160 --> 00:20:27.881
<v Speaker 1>the,</v>
<v Speaker 1>the,</v>

405
00:20:27.881 --> 00:20:30.360
<v Speaker 1>the metric of successes and then you </v>
<v Speaker 1>wanted to focus on,</v>

406
00:20:30.370 --> 00:20:33.570
<v Speaker 1>on the policy that you've converge </v>
<v Speaker 1>towards that is pretty good,</v>

407
00:20:33.810 --> 00:20:36.600
<v Speaker 1>uh,</v>
<v Speaker 1>near optimal policy in order to act in a</v>

408
00:20:36.601 --> 00:20:37.434
<v Speaker 1>greedy way.</v>
<v Speaker 1>As you move around with this a bellman </v>

409
00:20:41.511 --> 00:20:42.660
<v Speaker 1>equation,</v>
<v Speaker 1>move around the world,</v>

410
00:20:42.661 --> 00:20:44.910
<v Speaker 1>taking different states,</v>
<v Speaker 1>different actions.</v>

411
00:20:45.030 --> 00:20:47.380
<v Speaker 1>You can update a,</v>
<v Speaker 1>you could think of it as a cute table,</v>

412
00:20:47.540 --> 00:20:50.320
<v Speaker 1>uh,</v>
<v Speaker 1>and you can update the quality of a,</v>

413
00:20:50.321 --> 00:20:52.630
<v Speaker 1>taking a certain action in a certain </v>
<v Speaker 1>state.</v>

414
00:20:52.690 --> 00:20:54.790
<v Speaker 1>So that's a,</v>
<v Speaker 1>that's a,</v>

415
00:20:55.600 --> 00:21:00.400
<v Speaker 1>that's a picture of a table there in the</v>
<v Speaker 1>world with four states and four actions.</v>

416
00:21:00.490 --> 00:21:02.590
<v Speaker 1>And you can move around using the </v>
<v Speaker 1>bellmead equation,</v>

417
00:21:02.620 --> 00:21:05.110
<v Speaker 1>updating the value of being in that </v>
<v Speaker 1>state.</v>

418
00:21:05.320 --> 00:21:06.153
<v Speaker 1>The problem is when this cute table </v>
<v Speaker 1>grows exponentially in order to </v>

419
00:21:09.221 --> 00:21:12.280
<v Speaker 1>represent raw sensory data like we </v>
<v Speaker 1>humans have,</v>

420
00:21:12.281 --> 00:21:16.510
<v Speaker 1>when taking an envision or if you'd </v>
<v Speaker 1>taken the raw pixels of an arcade game,</v>

421
00:21:16.840 --> 00:21:19.600
<v Speaker 1>that's the number of pixels that are </v>
<v Speaker 1>there.</v>

422
00:21:19.660 --> 00:21:24.660
<v Speaker 1>Get is larger than is larger than it can</v>
<v Speaker 1>be stored in memory is largely,</v>

423
00:21:25.691 --> 00:21:27.490
<v Speaker 1>that can be explored.</v>
<v Speaker 1>The simulation,</v>

424
00:21:27.610 --> 00:21:28.443
<v Speaker 1>it's exceptionally large.</v>
<v Speaker 1>And if we know anything about </v>

425
00:21:30.910 --> 00:21:32.780
<v Speaker 1>exceptionally large,</v>
<v Speaker 1>uh,</v>

426
00:21:33.250 --> 00:21:36.370
<v Speaker 1>high dimensional spaces and learning </v>
<v Speaker 1>anything about them,</v>

427
00:21:36.371 --> 00:21:38.650
<v Speaker 1>that's what deep neural networks are </v>
<v Speaker 1>good at.</v>

428
00:21:38.770 --> 00:21:39.603
<v Speaker 1>Forming the approximators,</v>
<v Speaker 1>forming some kind of representation and </v>

429
00:21:42.400 --> 00:21:44.820
<v Speaker 1>exceptionally high dimensional complex,</v>
<v Speaker 1>uh,</v>

430
00:21:44.950 --> 00:21:45.783
<v Speaker 1>space.</v>
<v Speaker 1>So that's the hope for deeper </v>

431
00:21:48.041 --> 00:21:50.830
<v Speaker 1>enforcement learning.</v>
<v Speaker 1>As you take these reinforcement learning</v>

432
00:21:50.831 --> 00:21:54.340
<v Speaker 1>ideas where an agent acts in the world </v>
<v Speaker 1>and learn something about that world.</v>

433
00:21:54.460 --> 00:21:58.810
<v Speaker 1>And we use a neural network as the </v>
<v Speaker 1>approximator as the thing that the agent</v>

434
00:21:58.811 --> 00:22:00.940
<v Speaker 1>uses in order to approximate the </v>
<v Speaker 1>quality,</v>

435
00:22:01.180 --> 00:22:02.013
<v Speaker 1>uh,</v>
<v Speaker 1>either approximate the policy or </v>

436
00:22:03.221 --> 00:22:05.080
<v Speaker 1>approximately the quality of taking </v>
<v Speaker 1>certain actions,</v>

437
00:22:05.081 --> 00:22:05.914
<v Speaker 1>certain state and,</v>
<v Speaker 1>and therefore making sense of this raw </v>

438
00:22:08.321 --> 00:22:10.510
<v Speaker 1>information,</v>
<v Speaker 1>forming,</v>

439
00:22:10.630 --> 00:22:11.463
<v Speaker 1>hiring higher order representations of </v>
<v Speaker 1>the raw sensory information in order to </v>

440
00:22:15.311 --> 00:22:17.770
<v Speaker 1>then as an output,</v>
<v Speaker 1>take an action.</v>

441
00:22:19.210 --> 00:22:23.920
<v Speaker 1>So the neural network is injected as a </v>
<v Speaker 1>function approximator into the queue.</v>

442
00:22:24.000 --> 00:22:26.160
<v Speaker 1>It's,</v>
<v Speaker 1>it's the q function is approximated with</v>

443
00:22:26.161 --> 00:22:29.740
<v Speaker 1>a neural network that's dqn that's deep </v>
<v Speaker 1>cue learning.</v>

444
00:22:29.950 --> 00:22:33.520
<v Speaker 1>So injecting into the cue learning </v>
<v Speaker 1>framework and neural network.</v>

445
00:22:33.910 --> 00:22:37.390
<v Speaker 1>That's what's been the success with deep</v>
<v Speaker 1>mind with the playing the Atar Games.</v>

446
00:22:37.630 --> 00:22:38.463
<v Speaker 1>Having this neural network takes in the </v>
<v Speaker 1>raw pixels of the Atari game and </v>

447
00:22:40.961 --> 00:22:44.350
<v Speaker 1>produces actions of values of each </v>
<v Speaker 1>individual actions.</v>

448
00:22:44.351 --> 00:22:45.184
<v Speaker 1>And then in a greedy way,</v>
<v Speaker 1>picking the best action and the </v>

449
00:22:48.641 --> 00:22:49.474
<v Speaker 1>learning,</v>
<v Speaker 1>the loss function for these networks is </v>

450
00:22:51.900 --> 00:22:52.930
<v Speaker 1>uh,</v>
<v Speaker 1>uh,</v>

451
00:22:53.000 --> 00:22:53.530
<v Speaker 1>two folds.</v>

452
00:22:53.530 --> 00:22:55.360
<v Speaker 1>So you have a,</v>
<v Speaker 1>you have a queue function,</v>

453
00:22:55.370 --> 00:22:57.940
<v Speaker 1>an estimate of taking a certain action.</v>
<v Speaker 1>Certain state,</v>

454
00:22:58.060 --> 00:22:58.893
<v Speaker 1>you'd take that action and then you </v>
<v Speaker 1>observe how it's the actual reward </v>

455
00:23:02.441 --> 00:23:04.510
<v Speaker 1>received is different.</v>
<v Speaker 1>So you have a target,</v>

456
00:23:04.570 --> 00:23:09.070
<v Speaker 1>you have a prediction and the loss is </v>
<v Speaker 1>the squared error between those two.</v>

457
00:23:09.250 --> 00:23:14.250
<v Speaker 1>And dqn has used as the same network </v>
<v Speaker 1>that traditional dqn the first,</v>

458
00:23:14.270 --> 00:23:16.000
<v Speaker 1>uh,</v>
<v Speaker 1>uh,</v>

459
00:23:16.120 --> 00:23:19.420
<v Speaker 1>yeah.</v>
<v Speaker 1>Dqi uses one network testimate both cues</v>

460
00:23:19.421 --> 00:23:21.100
<v Speaker 1>in that lost function,</v>
<v Speaker 1>uh,</v>

461
00:23:21.270 --> 00:23:22.103
<v Speaker 1>a double Dq,</v>
<v Speaker 1>not dq and uses a separate network for </v>

462
00:23:26.201 --> 00:23:31.201
<v Speaker 1>each one of those a few tricks.</v>
<v Speaker 1>They're key experience replay.</v>

463
00:23:33.010 --> 00:23:34.420
<v Speaker 1>So,</v>
<v Speaker 1>uh,</v>

464
00:23:35.080 --> 00:23:39.730
<v Speaker 1>tricks in reinforcement learning because</v>
<v Speaker 1>the fact that it works is incredible.</v>

465
00:23:40.030 --> 00:23:40.863
<v Speaker 1>And uh,</v>
<v Speaker 1>as as a fundamental sort of </v>

466
00:23:42.941 --> 00:23:45.890
<v Speaker 1>philosophical idea,</v>
<v Speaker 1>knowing so little and being able to make</v>

467
00:23:45.891 --> 00:23:46.724
<v Speaker 1>sense with a,</v>
<v Speaker 1>from such a high dimensional space is </v>

468
00:23:49.401 --> 00:23:50.234
<v Speaker 1>amazing.</v>
<v Speaker 1>But actually these ideas have been </v>

469
00:23:51.921 --> 00:23:56.921
<v Speaker 1>around for quite a long time and a few </v>
<v Speaker 1>key trex is what made them really work.</v>

470
00:23:57.470 --> 00:23:58.303
<v Speaker 1>So in the first,</v>
<v Speaker 1>I think the two things for dqn is </v>

471
00:24:00.621 --> 00:24:04.520
<v Speaker 1>experienced replay is instead of letting</v>
<v Speaker 1>the agent,</v>

472
00:24:04.730 --> 00:24:06.290
<v Speaker 1>uh,</v>
<v Speaker 1>learn as it,</v>

473
00:24:06.410 --> 00:24:07.243
<v Speaker 1>as it acts in the world,</v>
<v Speaker 1>agent is acting in the world and </v>

474
00:24:10.251 --> 00:24:11.910
<v Speaker 1>collecting,</v>
<v Speaker 1>uh,</v>

475
00:24:12.500 --> 00:24:15.350
<v Speaker 1>experiences that can be replayed through</v>
<v Speaker 1>the learning.</v>

476
00:24:15.351 --> 00:24:17.990
<v Speaker 1>So the learning process jumps around </v>
<v Speaker 1>through memory,</v>

477
00:24:18.230 --> 00:24:21.470
<v Speaker 1>through the experiences and instead,</v>
<v Speaker 1>so it doesn't,</v>

478
00:24:21.560 --> 00:24:22.393
<v Speaker 1>um,</v>
<v Speaker 1>it doesn't learn on the local evolution </v>

479
00:24:25.611 --> 00:24:26.444
<v Speaker 1>of a particular stimulation.</v>
<v Speaker 1>Instead learn of the entirety of its </v>

480
00:24:28.791 --> 00:24:31.760
<v Speaker 1>experiences.</v>
<v Speaker 1>Then the fix target network,</v>

481
00:24:31.761 --> 00:24:35.600
<v Speaker 1>as I mentioned with a double Gq n the </v>
<v Speaker 1>fact that the loss function,</v>

482
00:24:36.610 --> 00:24:40.180
<v Speaker 1>uh,</v>
<v Speaker 1>a includes if you notice sort of to,</v>

483
00:24:40.410 --> 00:24:41.570
<v Speaker 1>uh,</v>
<v Speaker 1>uh,</v>

484
00:24:41.630 --> 00:24:43.730
<v Speaker 1>to forward passes through the neural </v>
<v Speaker 1>network.</v>

485
00:24:43.910 --> 00:24:47.090
<v Speaker 1>And so because when you know,</v>
<v Speaker 1>very little in the beginning,</v>

486
00:24:47.240 --> 00:24:49.850
<v Speaker 1>it's a very unstable system and bias </v>
<v Speaker 1>can,</v>

487
00:24:49.920 --> 00:24:50.753
<v Speaker 1>Eh,</v>
<v Speaker 1>I can have a significant negative </v>

488
00:24:51.441 --> 00:24:52.274
<v Speaker 1>effect.</v>
<v Speaker 1>So there's some benefit in for the </v>

489
00:24:54.771 --> 00:24:55.604
<v Speaker 1>target.</v>

490
00:24:55.910 --> 00:24:59.630
<v Speaker 1>The forward pass the neural network </v>
<v Speaker 1>takes for the target function to,</v>

491
00:24:59.631 --> 00:25:02.690
<v Speaker 1>uh,</v>
<v Speaker 1>to be fixed and only be updated.</v>

492
00:25:02.691 --> 00:25:03.524
<v Speaker 1>Then neural network there to be only </v>
<v Speaker 1>updated every a thousand a hundred </v>

493
00:25:06.830 --> 00:25:10.490
<v Speaker 1>steps.</v>
<v Speaker 1>And there's a few other tricks,</v>

494
00:25:10.491 --> 00:25:12.190
<v Speaker 1>the slides that are available online.</v>
<v Speaker 1>I will,</v>

495
00:25:12.350 --> 00:25:14.600
<v Speaker 1>there's a few interesting bits,</v>
<v Speaker 1>uh,</v>

496
00:25:14.810 --> 00:25:17.300
<v Speaker 1>throughout these slides.</v>
<v Speaker 1>Please check them out.</v>

497
00:25:17.301 --> 00:25:20.990
<v Speaker 1>There's a lot of interesting results </v>
<v Speaker 1>here on the slide.</v>

498
00:25:21.750 --> 00:25:22.583
<v Speaker 1>Uh,</v>
<v Speaker 1>showing the benefit that you get from </v>

499
00:25:24.861 --> 00:25:26.840
<v Speaker 1>these tricks.</v>
<v Speaker 1>So replay experience,</v>

500
00:25:26.841 --> 00:25:29.720
<v Speaker 1>replay and fixed target network are the </v>
<v Speaker 1>biggest.</v>

501
00:25:29.721 --> 00:25:30.554
<v Speaker 1>That's the magic.</v>
<v Speaker 1>That's the thing that made it work for </v>

502
00:25:32.121 --> 00:25:33.110
<v Speaker 1>the Atar Games.</v>

503
00:25:35.980 --> 00:25:36.780
<v Speaker 3>Okay.</v>

504
00:25:36.780 --> 00:25:40.790
<v Speaker 1>And the result was achieving deep mine,</v>
<v Speaker 1>achieving super,</v>

505
00:25:40.850 --> 00:25:41.683
<v Speaker 1>uh,</v>
<v Speaker 1>above human level performance on these </v>

506
00:25:43.441 --> 00:25:44.274
<v Speaker 1>Atari Games.</v>
<v Speaker 1>Now what's been very successful he use </v>

507
00:25:48.241 --> 00:25:52.560
<v Speaker 1>now with Alphago and um,</v>
<v Speaker 1>the other more complex systems is policy</v>

508
00:25:52.561 --> 00:25:55.740
<v Speaker 1>gradients,</v>
<v Speaker 1>which is a slight variation on this idea</v>

509
00:25:55.741 --> 00:25:58.560
<v Speaker 1>of applying neural networks in this </v>
<v Speaker 1>deeper enforcement learning space.</v>

510
00:25:58.800 --> 00:26:03.510
<v Speaker 1>So dqn is q learning neural network in </v>
<v Speaker 1>the cue learning framework.</v>

511
00:26:03.660 --> 00:26:04.493
<v Speaker 1>It's off policy.</v>
<v Speaker 1>So it's approximating q and infer the </v>

512
00:26:07.831 --> 00:26:10.290
<v Speaker 1>optimum policy,</v>
<v Speaker 1>a policy gradients.</v>

513
00:26:10.291 --> 00:26:14.570
<v Speaker 1>PG is on policy is directly optimizing </v>
<v Speaker 1>the policy space.</v>

514
00:26:14.610 --> 00:26:15.443
<v Speaker 1>Then you'll network as estimating the </v>
<v Speaker 1>probability of taking a certain action </v>

515
00:26:19.380 --> 00:26:20.730
<v Speaker 1>and the learning.</v>
<v Speaker 1>And there's a great,</v>

516
00:26:20.740 --> 00:26:21.573
<v Speaker 1>if you want the details of this from </v>
<v Speaker 1>Andrea Copath is a great post </v>

517
00:26:23.821 --> 00:26:24.860
<v Speaker 1>explaining,</v>
<v Speaker 1>uh,</v>

518
00:26:24.970 --> 00:26:25.803
<v Speaker 1>illustrate it in an illustrative way.</v>
<v Speaker 1>Deeper enforcement learning by looking </v>

519
00:26:28.831 --> 00:26:29.664
<v Speaker 1>at Pong Ping Pong.</v>
<v Speaker 1>So the training process there is you </v>

520
00:26:33.451 --> 00:26:34.284
<v Speaker 1>look at the evolution of the different </v>
<v Speaker 1>games and then reinforce also knows </v>

521
00:26:37.260 --> 00:26:38.250
<v Speaker 1>actor critic.</v>

522
00:26:38.280 --> 00:26:42.900
<v Speaker 1>You take the policy gradient that </v>
<v Speaker 1>increases the probability of good action</v>

523
00:26:42.901 --> 00:26:46.920
<v Speaker 1>and the probability of bad action.</v>
<v Speaker 1>So the policy network is the actor.</v>

524
00:26:46.921 --> 00:26:50.010
<v Speaker 1>So the neural network is the thing that </v>
<v Speaker 1>takes him the raw pixels,</v>

525
00:26:50.160 --> 00:26:52.890
<v Speaker 1>usually a sequence of frames and </v>
<v Speaker 1>outputs,</v>

526
00:26:52.891 --> 00:26:54.720
<v Speaker 1>a probability of taking a certain </v>
<v Speaker 1>action.</v>

527
00:26:55.140 --> 00:26:55.973
<v Speaker 1>So you want to reward actions that have </v>
<v Speaker 1>eventually led to a winning a higher </v>

528
00:27:00.751 --> 00:27:04.470
<v Speaker 1>award and you want to punish the,</v>
<v Speaker 1>you earned two degrees.</v>

529
00:27:04.820 --> 00:27:07.080
<v Speaker 1>You don't have negative gradient for </v>
<v Speaker 1>actions that,</v>

530
00:27:07.110 --> 00:27:10.350
<v Speaker 1>um,</v>
<v Speaker 1>that led to a negative reward.</v>

531
00:27:11.190 --> 00:27:14.760
<v Speaker 1>So the reward there is the critic.</v>
<v Speaker 1>The policy network is the actor,</v>

532
00:27:16.060 --> 00:27:16.893
<v Speaker 1>uh,</v>
<v Speaker 1>the pros and cons of Dqn of Policy </v>

533
00:27:20.510 --> 00:27:22.620
<v Speaker 1>Grazers,</v>
<v Speaker 1>dqn most folks now,</v>

534
00:27:22.621 --> 00:27:25.530
<v Speaker 1>the success comes from policy gradients,</v>
<v Speaker 1>active critic methods,</v>

535
00:27:25.560 --> 00:27:28.080
<v Speaker 1>different variations of it.</v>
<v Speaker 1>The pros are,</v>

536
00:27:28.081 --> 00:27:32.250
<v Speaker 1>it's able to deal with more complex q </v>
<v Speaker 1>function is faster convergence and most,</v>

537
00:27:32.320 --> 00:27:33.480
<v Speaker 1>uh,</v>
<v Speaker 1>in most cases,</v>

538
00:27:33.810 --> 00:27:34.860
<v Speaker 1>uh,</v>
<v Speaker 1>given,</v>

539
00:27:35.190 --> 00:27:36.023
<v Speaker 1>given you have enough data,</v>
<v Speaker 1>that's the big con is it's needs a lot </v>

540
00:27:39.931 --> 00:27:40.470
<v Speaker 1>of data.</v>

541
00:27:40.470 --> 00:27:42.420
<v Speaker 1>It needs a lot of ability to simulate </v>
<v Speaker 1>huge,</v>

542
00:27:42.570 --> 00:27:44.460
<v Speaker 1>huge amounts of evolutions of the </v>
<v Speaker 1>system.</v>

543
00:27:44.970 --> 00:27:46.260
<v Speaker 1>And,</v>
<v Speaker 1>uh,</v>

544
00:27:46.290 --> 00:27:47.123
<v Speaker 1>because the model probabilities,</v>
<v Speaker 1>the posse grainy smile at the </v>

545
00:27:49.501 --> 00:27:51.630
<v Speaker 1>probabilities of action,</v>
<v Speaker 1>they're able to,</v>

546
00:27:51.870 --> 00:27:55.080
<v Speaker 1>uh,</v>
<v Speaker 1>learn stochastic policies and Dq cannot.</v>

547
00:27:56.580 --> 00:27:57.413
<v Speaker 1>And that's where the game of go has </v>
<v Speaker 1>received a lot of success with the </v>

548
00:28:01.321 --> 00:28:06.180
<v Speaker 1>application of the policy gradients.</v>
<v Speaker 1>Where at first alpha go in 2016,</v>

549
00:28:06.181 --> 00:28:08.190
<v Speaker 1>beat the top humans in the world,</v>
<v Speaker 1>uh,</v>

550
00:28:08.250 --> 00:28:10.680
<v Speaker 1>at the game of go by training on an </v>
<v Speaker 1>expert,</v>

551
00:28:10.890 --> 00:28:13.830
<v Speaker 1>a games.</v>
<v Speaker 1>So in a supervised way,</v>

552
00:28:13.950 --> 00:28:14.783
<v Speaker 1>starting from training on those human </v>
<v Speaker 1>expert positions and Alphago zero and </v>

553
00:28:19.431 --> 00:28:24.390
<v Speaker 1>2017 achieving a monumental feat.</v>
<v Speaker 1>And in artificial intelligence,</v>

554
00:28:24.391 --> 00:28:25.730
<v Speaker 1>one of the greatest,</v>
<v Speaker 1>in my opinion,</v>

555
00:28:25.740 --> 00:28:30.740
<v Speaker 1>the last decade of training on no human </v>
<v Speaker 1>expert play playing against a itself,</v>

556
00:28:32.520 --> 00:28:33.353
<v Speaker 1>being able to beat under the initial </v>
<v Speaker 1>Alphago and beat the best human players </v>

557
00:28:36.511 --> 00:28:37.170
<v Speaker 1>in the world.</v>

558
00:28:37.170 --> 00:28:38.003
<v Speaker 1>This is an incredible achievement for </v>
<v Speaker 1>reinforcement learning that captivated </v>

559
00:28:40.621 --> 00:28:43.350
<v Speaker 1>our imagination of what's possible with </v>
<v Speaker 1>these approaches.</v>

560
00:28:43.920 --> 00:28:47.520
<v Speaker 1>But the actual approach,</v>
<v Speaker 1>and you can look through the slides as a</v>

561
00:28:47.521 --> 00:28:48.354
<v Speaker 1>few,</v>
<v Speaker 1>uh,</v>

562
00:28:48.360 --> 00:28:50.220
<v Speaker 1>interesting tidbits in there,</v>
<v Speaker 1>but,</v>

563
00:28:50.250 --> 00:28:51.180
<v Speaker 1>uh,</v>
<v Speaker 1>uh,</v>

564
00:28:51.181 --> 00:28:52.014
<v Speaker 1>the,</v>
<v Speaker 1>it's using the same kind of methodology </v>

565
00:28:53.161 --> 00:28:55.600
<v Speaker 1>that a lot of game engines have been </v>
<v Speaker 1>using and uh,</v>

566
00:28:55.620 --> 00:28:57.320
<v Speaker 1>certainly goplayers,</v>
<v Speaker 1>uh,</v>

567
00:28:57.330 --> 00:28:58.163
<v Speaker 1>for the Monte Carlo tree search.</v>
<v Speaker 1>So you have this incredibly huge search </v>

568
00:29:01.131 --> 00:29:01.964
<v Speaker 1>space and you have to figure out like </v>
<v Speaker 1>which parts of it do I search in order </v>

569
00:29:06.241 --> 00:29:08.250
<v Speaker 1>to find the good position as the good </v>
<v Speaker 1>actions.</v>

570
00:29:08.550 --> 00:29:09.383
<v Speaker 1>And so there you'll networks I used to </v>
<v Speaker 1>do the estimation of what are the good </v>

571
00:29:12.541 --> 00:29:14.340
<v Speaker 1>actions,</v>
<v Speaker 1>what are the good positions.</v>

572
00:29:15.630 --> 00:29:17.400
<v Speaker 1>Again,</v>
<v Speaker 1>the slides have the fun,</v>

573
00:29:17.640 --> 00:29:19.360
<v Speaker 1>the fun details.</v>
<v Speaker 1>Um,</v>

574
00:29:19.950 --> 00:29:22.500
<v Speaker 1>for those of you who are gambling </v>
<v Speaker 1>addicts,</v>

575
00:29:22.540 --> 00:29:23.910
<v Speaker 1>uh,</v>
<v Speaker 1>importantly,</v>

576
00:29:23.911 --> 00:29:24.691
<v Speaker 1>so,</v>
<v Speaker 1>uh,</v>

577
00:29:24.691 --> 00:29:25.290
<v Speaker 1>the,</v>
<v Speaker 1>uh,</v>

578
00:29:25.290 --> 00:29:28.680
<v Speaker 1>the stochastic element of poker,</v>
<v Speaker 1>at least heads up poker.</v>

579
00:29:28.680 --> 00:29:29.513
<v Speaker 1>So one on one has been for the first </v>
<v Speaker 1>time ever this same exact approach had </v>

580
00:29:34.701 --> 00:29:35.534
<v Speaker 1>been used in deep stack and other agents</v>
<v Speaker 1>to beat the top professional poker </v>

581
00:29:40.021 --> 00:29:44.800
<v Speaker 1>players in the world in 2017.</v>
<v Speaker 1>The open challenge for the community,</v>

582
00:29:44.830 --> 00:29:46.750
<v Speaker 1>for maybe people in this room,</v>
<v Speaker 1>uh,</v>

583
00:29:46.751 --> 00:29:47.584
<v Speaker 1>in 2018 is to apply these methods to win</v>
<v Speaker 1>in a much more complex environment of </v>

584
00:29:51.700 --> 00:29:53.620
<v Speaker 1>Terman plate when there's multiple </v>
<v Speaker 1>players.</v>

585
00:29:53.800 --> 00:29:56.140
<v Speaker 1>So heads up poker is a much easier </v>
<v Speaker 1>problem.</v>

586
00:29:56.530 --> 00:29:57.363
<v Speaker 1>Uh,</v>
<v Speaker 1>the human element is much more </v>

587
00:29:58.810 --> 00:30:02.290
<v Speaker 1>formalized bubble and clear there when </v>
<v Speaker 1>there's multiple players,</v>

588
00:30:02.291 --> 00:30:04.330
<v Speaker 1>it's exceptionally difficult and </v>
<v Speaker 1>fascinating.</v>

589
00:30:04.680 --> 00:30:05.513
<v Speaker 1>A fascinating problem that's perhaps </v>
<v Speaker 1>more representative of agents that have </v>

590
00:30:09.521 --> 00:30:12.190
<v Speaker 1>to act in the real world.</v>
<v Speaker 1>So now the Downer part,</v>

591
00:30:13.150 --> 00:30:13.983
<v Speaker 1>a lot of the successful agents that we </v>
<v Speaker 1>work with here at Mit and bill did the </v>

592
00:30:17.651 --> 00:30:18.940
<v Speaker 1>robots that act in the,</v>
<v Speaker 1>in,</v>

593
00:30:18.950 --> 00:30:23.440
<v Speaker 1>in the real world are using almost no </v>
<v Speaker 1>deep reinforcement learning.</v>

594
00:30:23.680 --> 00:30:24.513
<v Speaker 1>So deeper enforcement learning is </v>
<v Speaker 1>successfully applied in context of </v>

595
00:30:28.211 --> 00:30:29.044
<v Speaker 1>simulation in context of game playing.</v>
<v Speaker 1>But in a successfully controlling </v>

596
00:30:33.641 --> 00:30:34.474
<v Speaker 1>humanoid robotics or human robots,</v>
<v Speaker 1>a humanoid robots or autonomous </v>

597
00:30:38.861 --> 00:30:39.940
<v Speaker 1>vehicles,</v>
<v Speaker 1>for example,</v>

598
00:30:40.500 --> 00:30:43.810
<v Speaker 1>the deep learning methods they use </v>
<v Speaker 1>primarily for the perception task.</v>

599
00:30:43.930 --> 00:30:47.410
<v Speaker 1>They're exceptionally good at making </v>
<v Speaker 1>sense of the environment and uh,</v>

600
00:30:47.530 --> 00:30:50.800
<v Speaker 1>extracting useful knowledge from it.</v>
<v Speaker 1>But in terms of forming actions,</v>

601
00:30:50.980 --> 00:30:53.470
<v Speaker 1>that's usually done through optimization</v>
<v Speaker 1>based methods.</v>

602
00:30:55.120 --> 00:30:59.890
<v Speaker 1>Finally a quick comment on the </v>
<v Speaker 1>unexpected local pockets.</v>

603
00:30:59.950 --> 00:31:00.783
<v Speaker 1>That's,</v>
<v Speaker 1>that's at the core of why these methods </v>

604
00:31:02.750 --> 00:31:04.930
<v Speaker 1>are not used in the real world.</v>
<v Speaker 1>Uh,</v>

605
00:31:04.931 --> 00:31:05.764
<v Speaker 1>here is a game of coast runners where a </v>
<v Speaker 1>boat is tasked with receiving a lot of </v>

606
00:31:09.341 --> 00:31:10.174
<v Speaker 1>points.</v>
<v Speaker 1>Traditionally the game is played by </v>

607
00:31:12.101 --> 00:31:15.160
<v Speaker 1>racing other boats and trying to get to </v>
<v Speaker 1>the finish as quickly as possible.</v>

608
00:31:15.580 --> 00:31:16.413
<v Speaker 1>And this boat figures out that it </v>
<v Speaker 1>doesn't need to do that in a brilliant </v>

609
00:31:19.541 --> 00:31:20.590
<v Speaker 1>breakthrough idea.</v>

610
00:31:20.890 --> 00:31:23.800
<v Speaker 1>Uh,</v>
<v Speaker 1>it can just collect a regenerating green</v>

611
00:31:23.860 --> 00:31:27.010
<v Speaker 1>squares.</v>
<v Speaker 1>That's an unintended consequence,</v>

612
00:31:27.250 --> 00:31:28.083
<v Speaker 1>uh,</v>
<v Speaker 1>that you can extend to other systems </v>

613
00:31:30.230 --> 00:31:32.560
<v Speaker 1>perhaps including,</v>
<v Speaker 1>you can imagine what,</v>

614
00:31:32.860 --> 00:31:33.860
<v Speaker 1>uh,</v>
<v Speaker 1>how the,</v>

615
00:31:34.120 --> 00:31:34.953
<v Speaker 1>the cat system over time at the bottom </v>
<v Speaker 1>right can evolve into something </v>

616
00:31:38.270 --> 00:31:39.103
<v Speaker 1>undesirable and further on in these </v>
<v Speaker 1>reinforcement learning agents when they </v>

617
00:31:43.841 --> 00:31:48.700
<v Speaker 1>act in the real world,</v>
<v Speaker 1>the human life is often the human factor</v>

618
00:31:48.910 --> 00:31:49.743
<v Speaker 1>are often injected into the system.</v>
<v Speaker 1>And so oftentimes in their reward </v>

619
00:31:52.931 --> 00:31:54.700
<v Speaker 1>function,</v>
<v Speaker 1>the objective loss function,</v>

620
00:31:54.790 --> 00:31:58.510
<v Speaker 1>you start injecting concepts of risk and</v>
<v Speaker 1>even human life.</v>

621
00:31:58.690 --> 00:31:59.523
<v Speaker 1>So what does it look like in terms of ai</v>
<v Speaker 1>safety when the agent has to make </v>

622
00:32:02.321 --> 00:32:03.154
<v Speaker 1>decision based on a loss function that </v>
<v Speaker 1>includes an estimate or risk of killing </v>

623
00:32:07.841 --> 00:32:08.674
<v Speaker 1>another human being?</v>
<v Speaker 1>This is a very important thing to think </v>

624
00:32:10.691 --> 00:32:11.550
<v Speaker 1>about,</v>
<v Speaker 1>uh,</v>

625
00:32:11.551 --> 00:32:16.551
<v Speaker 1>about machines that learn from data.</v>
<v Speaker 1>And finally a to play around.</v>

626
00:32:16.721 --> 00:32:17.290
<v Speaker 1>There's,</v>
<v Speaker 1>there's,</v>

627
00:32:17.290 --> 00:32:18.123
<v Speaker 1>uh,</v>
<v Speaker 1>there's a lot of ways to explore and </v>

628
00:32:19.991 --> 00:32:24.991
<v Speaker 1>learn about deeper enforcement learning.</v>
<v Speaker 1>And we have at the url below there,</v>

629
00:32:25.790 --> 00:32:27.670
<v Speaker 1>uh,</v>
<v Speaker 1>a deep traffic simulation game.</v>

630
00:32:27.671 --> 00:32:29.800
<v Speaker 1>It's a competition where you get to,</v>
<v Speaker 1>uh,</v>

631
00:32:30.040 --> 00:32:32.050
<v Speaker 1>uh,</v>
<v Speaker 1>build a car that speeds,</v>

632
00:32:32.130 --> 00:32:34.210
<v Speaker 1>uh,</v>
<v Speaker 1>that tries to achieve the as close to 80</v>

633
00:32:34.211 --> 00:32:37.480
<v Speaker 1>miles per hour as possible.</v>
<v Speaker 1>And I encourage you to participate,</v>

634
00:32:38.760 --> 00:32:41.380
<v Speaker 1>uh,</v>
<v Speaker 1>participate and try to win.</v>

635
00:32:41.410 --> 00:32:42.243
<v Speaker 1>Get it a leaderboard,</v>
<v Speaker 1>not enough mit folks there at the top </v>

636
00:32:44.051 --> 00:32:45.730
<v Speaker 1>10.</v>
<v Speaker 1>So with that,</v>

637
00:32:45.760 --> 00:32:47.230
<v Speaker 1>thank you very much.</v>
<v Speaker 1>Thank you for having me.</v>

