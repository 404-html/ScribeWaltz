WEBVTT

1
00:00:02.580 --> 00:00:03.600
<v Speaker 1>Thanks for having me here.</v>
<v Speaker 1>Yeah.</v>

2
00:00:03.601 --> 00:00:04.470
<v Speaker 1>So I'm,</v>
<v Speaker 1>uh,</v>

3
00:00:04.490 --> 00:00:08.940
<v Speaker 1>I'm based in the Cambridge office,</v>
<v Speaker 1>which is like a 100 meters that way.</v>

4
00:00:09.030 --> 00:00:09.863
<v Speaker 1>Um,</v>
<v Speaker 1>and we do a lot of stuff with a deep </v>

5
00:00:12.211 --> 00:00:13.044
<v Speaker 1>learning.</v>
<v Speaker 1>We've got a large group in Google brain </v>

6
00:00:14.721 --> 00:00:15.554
<v Speaker 1>and other related fields.</v>
<v Speaker 1>So hopefully that's interesting to some </v>

7
00:00:18.991 --> 00:00:19.824
<v Speaker 1>of you at some point.</v>
<v Speaker 1>So I'm going to talk for about 20 </v>

8
00:00:23.101 --> 00:00:24.190
<v Speaker 1>minutes or so.</v>
<v Speaker 1>Um,</v>

9
00:00:24.390 --> 00:00:27.090
<v Speaker 1>this sort of issues and image </v>
<v Speaker 1>classification theme.</v>

10
00:00:27.610 --> 00:00:28.443
<v Speaker 1>Let me hand it over to my excellent </v>
<v Speaker 1>colleagues hunching king who's going to </v>

11
00:00:31.800 --> 00:00:36.270
<v Speaker 1>go through an entirely different subject</v>
<v Speaker 1>in a using tensorflow debugger and eager</v>

12
00:00:36.271 --> 00:00:41.271
<v Speaker 1>mode to make work intensive flow easier.</v>
<v Speaker 1>Maybe that will be good.</v>

13
00:00:42.300 --> 00:00:43.133
<v Speaker 1>Okay.</v>
<v Speaker 1>Um,</v>

14
00:00:43.710 --> 00:00:45.210
<v Speaker 1>so let's,</v>
<v Speaker 1>let's take a step back.</v>

15
00:00:45.720 --> 00:00:47.610
<v Speaker 1>So have you guys seen happy graphs like </v>
<v Speaker 1>this before?</v>

16
00:00:47.611 --> 00:00:49.830
<v Speaker 1>Go ahead and smile and nod if you've </v>
<v Speaker 1>seen stuff like this.</v>

17
00:00:49.890 --> 00:00:50.610
<v Speaker 1>Yeah.</v>
<v Speaker 1>Okay.</v>

18
00:00:50.610 --> 00:00:55.290
<v Speaker 1>So this is a happy graph on image,</v>
<v Speaker 1>net based image classification.</v>

19
00:00:55.320 --> 00:01:00.320
<v Speaker 1>So image net is a data set of a million </v>
<v Speaker 1>some odd images for this challenge.</v>

20
00:01:01.291 --> 00:01:04.340
<v Speaker 1>There were a thousand classes,</v>
<v Speaker 1>um,</v>

21
00:01:05.040 --> 00:01:05.873
<v Speaker 1>and uh,</v>
<v Speaker 1>in 2011 back in the dark ages when </v>

22
00:01:08.221 --> 00:01:09.054
<v Speaker 1>nobody knew how to do anything,</v>
<v Speaker 1>the state of the art was something like </v>

23
00:01:12.091 --> 00:01:16.800
<v Speaker 1>25 percent error rate on this stuff.</v>
<v Speaker 1>And in the last call it six,</v>

24
00:01:16.801 --> 00:01:17.634
<v Speaker 1>seven years,</v>
<v Speaker 1>the reduction in error rate has been </v>

25
00:01:20.670 --> 00:01:23.430
<v Speaker 1>kind of astounding to the point where </v>
<v Speaker 1>it's now been talked about so much.</v>

26
00:01:23.431 --> 00:01:25.410
<v Speaker 1>It's like no longer even surprising and </v>
<v Speaker 1>everyone's like,</v>

27
00:01:25.420 --> 00:01:25.850
<v Speaker 1>yeah,</v>
<v Speaker 1>yeah,</v>

28
00:01:25.850 --> 00:01:27.070
<v Speaker 1>we've see this,</v>
<v Speaker 1>um,</v>

29
00:01:28.300 --> 00:01:31.610
<v Speaker 1>human error rate is somewhere between </v>
<v Speaker 1>five and 10 percent on,</v>

30
00:01:31.630 --> 00:01:32.770
<v Speaker 1>on,</v>
<v Speaker 1>on this task.</v>

31
00:01:33.010 --> 00:01:37.100
<v Speaker 1>So the contemporary results of,</v>
<v Speaker 1>you know,</v>

32
00:01:37.360 --> 00:01:38.193
<v Speaker 1>two point two or whatever it is,</v>
<v Speaker 1>percent error rate or really kind of </v>

33
00:01:40.841 --> 00:01:42.920
<v Speaker 1>astonishing.</v>
<v Speaker 1>Um,</v>

34
00:01:43.120 --> 00:01:46.690
<v Speaker 1>and you can look at a graph like this </v>
<v Speaker 1>and make reasonable claims that well,</v>

35
00:01:48.420 --> 00:01:49.253
<v Speaker 1>machines we're using deep learning are </v>
<v Speaker 1>better than humans that impinged </v>

36
00:01:51.950 --> 00:01:52.783
<v Speaker 1>classification on this task.</v>
<v Speaker 1>That's kind of weird and kind of </v>

37
00:01:55.501 --> 00:01:56.334
<v Speaker 1>amazing.</v>
<v Speaker 1>And maybe we can declare victory and </v>

38
00:01:57.841 --> 00:02:01.500
<v Speaker 1>fill audiences full of people clamoring </v>
<v Speaker 1>to learn about deep learning.</v>

39
00:02:02.130 --> 00:02:03.090
<v Speaker 1>That's cool.</v>
<v Speaker 1>Okay.</v>

40
00:02:03.120 --> 00:02:04.440
<v Speaker 1>So,</v>
<v Speaker 1>um,</v>

41
00:02:06.530 --> 00:02:08.630
<v Speaker 1>I'm going to talk not about image data </v>
<v Speaker 1>itself,</v>

42
00:02:08.631 --> 00:02:12.050
<v Speaker 1>but about a slightly different image </v>
<v Speaker 1>data set.</v>

43
00:02:12.200 --> 00:02:13.550
<v Speaker 1>Basically people were like,</v>
<v Speaker 1>okay,</v>

44
00:02:13.790 --> 00:02:16.100
<v Speaker 1>obviously image net is too easy.</v>
<v Speaker 1>Let's make a larger,</v>

45
00:02:16.101 --> 00:02:16.934
<v Speaker 1>more interesting data set.</v>
<v Speaker 1>So open images was released I think a </v>

46
00:02:19.581 --> 00:02:22.340
<v Speaker 1>year or two ago.</v>
<v Speaker 1>It's got about 9 million as opposed to 1</v>

47
00:02:22.340 --> 00:02:27.340
<v Speaker 1>million images.</v>
<v Speaker 1>The base data set has 6,000</v>

48
00:02:27.801 --> 00:02:31.070
<v Speaker 1>labels as opposed to 1000 labels.</v>
<v Speaker 1>This ultra multi label.</v>

49
00:02:31.071 --> 00:02:32.210
<v Speaker 1>So you get,</v>
<v Speaker 1>you know,</v>

50
00:02:32.240 --> 00:02:34.070
<v Speaker 1>if there's a person holding a rugby </v>
<v Speaker 1>ball,</v>

51
00:02:34.071 --> 00:02:36.300
<v Speaker 1>you get both person and rugby ball and </v>
<v Speaker 1>the Dataset.</v>

52
00:02:37.370 --> 00:02:39.530
<v Speaker 1>It's got all kinds of classes including </v>
<v Speaker 1>stairs here,</v>

53
00:02:39.531 --> 00:02:41.810
<v Speaker 1>which are lovely,</v>
<v Speaker 1>lovely illustrated.</v>

54
00:02:41.930 --> 00:02:42.763
<v Speaker 1>And you can find this on get hub.</v>
<v Speaker 1>It's a nice data set to play around </v>

55
00:02:45.051 --> 00:02:47.520
<v Speaker 1>with.</v>
<v Speaker 1>So uh,</v>

56
00:02:47.750 --> 00:02:50.120
<v Speaker 1>some colleagues and I did some work of </v>
<v Speaker 1>saying,</v>

57
00:02:50.121 --> 00:02:52.550
<v Speaker 1>okay,</v>
<v Speaker 1>what happens if we apply just a straight</v>

58
00:02:52.551 --> 00:02:55.000
<v Speaker 1>up inception based model,</v>
<v Speaker 1>um,</v>

59
00:02:55.520 --> 00:02:56.780
<v Speaker 1>to this data we traded up.</v>

60
00:02:56.780 --> 00:02:57.613
<v Speaker 1>And then we'd look at some how would </v>
<v Speaker 1>classify as some that we found on the </v>

61
00:03:00.011 --> 00:03:03.250
<v Speaker 1>web.</v>
<v Speaker 1>So here's one such image,</v>

62
00:03:04.210 --> 00:03:05.043
<v Speaker 1>a image that we found on the web.</v>
<v Speaker 1>All the images here are creative </v>

63
00:03:07.241 --> 00:03:09.010
<v Speaker 1>comments and stuff like that.</v>
<v Speaker 1>So it's,</v>

64
00:03:09.250 --> 00:03:14.250
<v Speaker 1>it's okay for us to look at these and </v>
<v Speaker 1>when we apply an image based,</v>

65
00:03:15.290 --> 00:03:16.670
<v Speaker 1>a,</v>
<v Speaker 1>a image,</v>

66
00:03:16.671 --> 00:03:17.504
<v Speaker 1>nobody kind of model to this </v>
<v Speaker 1>classifications we get back or kind of </v>

67
00:03:20.171 --> 00:03:21.004
<v Speaker 1>what I personally would expect.</v>
<v Speaker 1>I'm seeing things like bride dress </v>

68
00:03:23.801 --> 00:03:25.030
<v Speaker 1>ceremony,</v>
<v Speaker 1>woman wedding,</v>

69
00:03:25.360 --> 00:03:26.193
<v Speaker 1>all things that as an American in this </v>
<v Speaker 1>country at this time I'm thinking or </v>

70
00:03:30.460 --> 00:03:33.790
<v Speaker 1>make make sense for this image.</v>
<v Speaker 1>Cool.</v>

71
00:03:33.940 --> 00:03:35.650
<v Speaker 1>Maybe we did solve it.</v>
<v Speaker 1>Image classification.</v>

72
00:03:36.610 --> 00:03:37.840
<v Speaker 1>So,</v>
<v Speaker 1>um,</v>

73
00:03:37.930 --> 00:03:42.930
<v Speaker 1>then we had played it to another image.</v>
<v Speaker 1>Also have a bride and a,</v>

74
00:03:45.040 --> 00:03:45.873
<v Speaker 1>the model that we had trained up on this</v>
<v Speaker 1>open source image data set returned the </v>

75
00:03:51.191 --> 00:03:54.700
<v Speaker 1>following classifications,</v>
<v Speaker 1>clothing event,</v>

76
00:03:55.540 --> 00:03:59.170
<v Speaker 1>costume red and performance art.</v>

77
00:03:59.910 --> 00:04:04.910
<v Speaker 1>I'm no mention of bride,</v>
<v Speaker 1>also,</v>

78
00:04:05.110 --> 00:04:09.760
<v Speaker 1>no mention of person Nis regardless of </v>
<v Speaker 1>gender.</v>

79
00:04:10.270 --> 00:04:11.620
<v Speaker 1>Um,</v>
<v Speaker 1>so in a sense this,</v>

80
00:04:11.650 --> 00:04:15.400
<v Speaker 1>this model is sort of like missed the </v>
<v Speaker 1>fact that there's a human in the picture</v>

81
00:04:15.610 --> 00:04:16.443
<v Speaker 1>which is maybe not awesome and not </v>
<v Speaker 1>really what I would think of as great </v>

82
00:04:21.461 --> 00:04:24.850
<v Speaker 1>success if we're claiming that image </v>
<v Speaker 1>classification is solved.</v>

83
00:04:25.330 --> 00:04:27.870
<v Speaker 1>Um,</v>
<v Speaker 1>okay.</v>

84
00:04:28.000 --> 00:04:30.880
<v Speaker 1>So what's going on here?</v>
<v Speaker 1>Um,</v>

85
00:04:31.750 --> 00:04:34.910
<v Speaker 1>I'm going to argue a little bit that </v>
<v Speaker 1>what's going on is,</v>

86
00:04:34.940 --> 00:04:35.773
<v Speaker 1>is based in,</v>
<v Speaker 1>to some degree on the idea of </v>

87
00:04:38.201 --> 00:04:42.490
<v Speaker 1>stereotypes and uh,</v>
<v Speaker 1>if you're,</v>

88
00:04:42.520 --> 00:04:43.353
<v Speaker 1>if you have your laptop open,</v>
<v Speaker 1>I'd like you to close your laptop for a </v>

89
00:04:44.621 --> 00:04:45.454
<v Speaker 1>second.</v>
<v Speaker 1>This is the interactive portion where </v>

90
00:04:46.481 --> 00:04:49.300
<v Speaker 1>you can interact by closing your laptop </v>
<v Speaker 1>and,</v>

91
00:04:50.160 --> 00:04:50.993
<v Speaker 1>um,</v>
<v Speaker 1>I'd like you to find somebody sitting </v>

92
00:04:52.961 --> 00:04:57.961
<v Speaker 1>next to you and exercise your human </v>
<v Speaker 1>conversation skills for about one minute</v>

93
00:04:58.960 --> 00:05:03.070
<v Speaker 1>to come up with a definition between the</v>
<v Speaker 1>two of you have,</v>

94
00:05:03.100 --> 00:05:03.933
<v Speaker 1>what is a stereotype keeping in mind </v>
<v Speaker 1>that we're in sort of a statistical </v>

95
00:05:07.061 --> 00:05:08.140
<v Speaker 1>setting.</v>
<v Speaker 1>Okay?</v>

96
00:05:08.141 --> 00:05:11.530
<v Speaker 1>So have a quick one minute conversation </v>
<v Speaker 1>with the person sitting next to you.</v>

97
00:05:11.770 --> 00:05:13.360
<v Speaker 1>If there's no one sitting next to you,</v>
<v Speaker 1>you may move.</v>

98
00:05:13.630 --> 00:05:14.350
<v Speaker 1>Ready,</v>
<v Speaker 1>set,</v>

99
00:05:14.350 --> 00:05:15.183
<v Speaker 1>go.</v>

100
00:05:18.520 --> 00:05:20.350
<v Speaker 1>Three,</v>
<v Speaker 1>two,</v>

101
00:05:21.040 --> 00:05:21.873
<v Speaker 1>one.</v>
<v Speaker 1>And thank you for having that </v>

102
00:05:25.600 --> 00:05:28.630
<v Speaker 1>interesting conversation that easily </v>
<v Speaker 1>could have lasted for much more than one</v>

103
00:05:28.631 --> 00:05:30.100
<v Speaker 1>minute,</v>
<v Speaker 1>but such is life.</v>

104
00:05:30.760 --> 00:05:34.690
<v Speaker 1>Let's hear from one or two folks who had</v>
<v Speaker 1>something that they came up with.</v>

105
00:05:34.691 --> 00:05:36.460
<v Speaker 1>It was interesting.</v>
<v Speaker 1>Yeah.</v>

106
00:05:36.461 --> 00:05:38.480
<v Speaker 1>Go ahead.</v>
<v Speaker 1>Your name is at that?</v>

107
00:05:38.490 --> 00:05:38.950
<v Speaker 1>Yeah.</v>
<v Speaker 1>Okay.</v>

108
00:05:38.950 --> 00:05:39.783
<v Speaker 1>What did you</v>

109
00:05:42.800 --> 00:05:43.633
<v Speaker 2>based?</v>

110
00:05:46.260 --> 00:05:47.093
<v Speaker 1>Okay,</v>
<v Speaker 1>so you saying that a stereotype is a </v>

111
00:05:48.751 --> 00:05:49.584
<v Speaker 1>generalization that you find from a </v>
<v Speaker 1>large group of people and you apply it </v>

112
00:05:52.171 --> 00:05:53.340
<v Speaker 1>to more people.</v>
<v Speaker 1>Okay,</v>

113
00:05:53.970 --> 00:05:54.803
<v Speaker 1>interesting.</v>
<v Speaker 1>I'm certainly agree with large parts of </v>

114
00:05:56.730 --> 00:05:57.260
<v Speaker 1>that.</v>
<v Speaker 1>Yeah,</v>

115
00:05:57.260 --> 00:05:58.093
<v Speaker 1>go ahead.</v>

116
00:05:59.390 --> 00:06:04.390
<v Speaker 3>That's based on probability and </v>
<v Speaker 3>experience with the human goal.</v>

117
00:06:04.750 --> 00:06:09.490
<v Speaker 3>For you,</v>
<v Speaker 3>would it be free?</v>

118
00:06:10.950 --> 00:06:11.471
<v Speaker 1>Okay.</v>
<v Speaker 1>So,</v>

119
00:06:11.471 --> 00:06:12.304
<v Speaker 1>so here the claimant said it's a label </v>
<v Speaker 1>that's based on experience from within </v>

120
00:06:15.841 --> 00:06:17.120
<v Speaker 1>your training set.</v>
<v Speaker 1>Yep.</v>

121
00:06:17.250 --> 00:06:18.083
<v Speaker 1>Super interesting.</v>
<v Speaker 1>And I'm the probability of label based </v>

122
00:06:22.141 --> 00:06:23.130
<v Speaker 1>on what's,</v>
<v Speaker 1>what's your training say?</v>

123
00:06:23.131 --> 00:06:24.180
<v Speaker 1>Cool.</v>
<v Speaker 1>Maybe one more.</v>

124
00:06:26.700 --> 00:06:27.533
<v Speaker 3>Oh yeah,</v>
<v Speaker 3>got it.</v>

125
00:06:28.800 --> 00:06:33.800
<v Speaker 3>You're making a prediction based upon to</v>
<v Speaker 3>be coordinated and integrated.</v>

126
00:06:35.690 --> 00:06:36.980
<v Speaker 1>Okay.</v>
<v Speaker 1>So the claim here,</v>

127
00:06:36.981 --> 00:06:37.814
<v Speaker 1>that stereotype has something to do with</v>
<v Speaker 1>unrelated features that happened to be </v>

128
00:06:39.801 --> 00:06:41.390
<v Speaker 1>correlated.</v>
<v Speaker 1>I think that's interesting.</v>

129
00:06:41.400 --> 00:06:43.670
<v Speaker 1>Let me see if I can.</v>
<v Speaker 1>This was not a plant.</v>

130
00:06:43.750 --> 00:06:44.600
<v Speaker 1>I'm sorry,</v>
<v Speaker 1>your name was</v>

131
00:06:46.120 --> 00:06:46.690
<v Speaker 3>awesome.</v>

132
00:06:46.690 --> 00:06:48.220
<v Speaker 1>Constantine.</v>
<v Speaker 1>Constantine is not a plant.</v>

133
00:06:48.350 --> 00:06:49.183
<v Speaker 1>Um,</v>
<v Speaker 1>but I do want to look at this a little </v>

134
00:06:52.241 --> 00:06:54.160
<v Speaker 1>bit more in detail.</v>
<v Speaker 1>So here's a,</v>

135
00:06:54.190 --> 00:06:56.960
<v Speaker 1>here's a Dataset that I'm going to claim</v>
<v Speaker 1>is,</v>

136
00:06:57.310 --> 00:06:59.660
<v Speaker 1>is based on running data.</v>
<v Speaker 1>Um,</v>

137
00:06:59.950 --> 00:07:00.890
<v Speaker 1>so,</v>
<v Speaker 1>uh,</v>

138
00:07:01.120 --> 00:07:04.360
<v Speaker 1>in the early mornings I pretend that I'm</v>
<v Speaker 1>an athlete and go for a run.</v>

139
00:07:04.570 --> 00:07:06.800
<v Speaker 1>Um,</v>
<v Speaker 1>and uh,</v>

140
00:07:06.970 --> 00:07:07.803
<v Speaker 1>this is a dataset that sort of based on </v>
<v Speaker 1>risk that someone might not finish a </v>

141
00:07:12.591 --> 00:07:15.370
<v Speaker 1>race that they enter in a.</v>
<v Speaker 1>So we've got high risk.</v>

142
00:07:15.371 --> 00:07:16.204
<v Speaker 1>People are,</v>
<v Speaker 1>you know,</v>

143
00:07:16.420 --> 00:07:19.360
<v Speaker 1>are in yellow and lower risk.</v>
<v Speaker 1>People are in red.</v>

144
00:07:19.680 --> 00:07:21.400
<v Speaker 1>I'm going to walk,</v>
<v Speaker 1>get this data.</v>

145
00:07:21.401 --> 00:07:24.790
<v Speaker 1>It's got a couple of dimensions.</v>
<v Speaker 1>I might fit a linear classifier.</v>

146
00:07:25.240 --> 00:07:29.380
<v Speaker 1>It's not quite perfect if I look a </v>
<v Speaker 1>little more closely,</v>

147
00:07:29.381 --> 00:07:30.214
<v Speaker 1>if I've actually got some more </v>
<v Speaker 1>information here I've done just to have </v>

148
00:07:32.951 --> 00:07:35.800
<v Speaker 1>x and y also had this sort of color have</v>
<v Speaker 1>outline.</v>

149
00:07:36.310 --> 00:07:41.310
<v Speaker 1>So I might have a rule that if this data</v>
<v Speaker 1>point has a blue outline,</v>

150
00:07:41.771 --> 00:07:42.604
<v Speaker 1>I'm going to predict low risk.</v>
<v Speaker 1>Otherwise I'm going to predict high </v>

151
00:07:44.591 --> 00:07:47.230
<v Speaker 1>risk.</v>
<v Speaker 1>Fair enough.</v>

152
00:07:48.120 --> 00:07:50.110
<v Speaker 1>Um,</v>
<v Speaker 1>now the big reveal,</v>

153
00:07:50.230 --> 00:07:52.220
<v Speaker 1>you'll never guess what,</v>
<v Speaker 1>um,</v>

154
00:07:52.270 --> 00:07:53.110
<v Speaker 1>the,</v>
<v Speaker 1>uh,</v>

155
00:07:53.140 --> 00:07:57.360
<v Speaker 1>the outline feature is based on shoe </v>
<v Speaker 1>type.</v>

156
00:07:58.210 --> 00:07:59.043
<v Speaker 1>The other x and y are based on how long </v>
<v Speaker 1>the races and sort of what a person's </v>

157
00:08:01.601 --> 00:08:05.920
<v Speaker 1>weekly training volume is.</v>
<v Speaker 1>But whether you're foolish enough to buy</v>

158
00:08:05.921 --> 00:08:06.754
<v Speaker 1>expensive running shoes because you </v>
<v Speaker 1>think they're going to make you faster </v>

159
00:08:08.321 --> 00:08:09.690
<v Speaker 1>or whatever,</v>
<v Speaker 1>um,</v>

160
00:08:10.040 --> 00:08:11.140
<v Speaker 1>this is what's in the data.</v>

161
00:08:13.370 --> 00:08:14.203
<v Speaker 3>And in</v>

162
00:08:15.670 --> 00:08:18.670
<v Speaker 1>traditional machine learning,</v>
<v Speaker 1>supervised machine learning,</v>

163
00:08:19.300 --> 00:08:20.500
<v Speaker 1>we might say,</v>
<v Speaker 1>well,</v>

164
00:08:20.501 --> 00:08:21.334
<v Speaker 1>wait a minute,</v>
<v Speaker 1>I'm not sure that shoe type is going to </v>

165
00:08:23.801 --> 00:08:27.120
<v Speaker 1>be actually predictive.</v>
<v Speaker 1>On the other hand,</v>

166
00:08:27.150 --> 00:08:27.983
<v Speaker 1>it's in her training data and it does </v>
<v Speaker 1>seem to be awfully predictive on this </v>

167
00:08:31.111 --> 00:08:31.944
<v Speaker 1>data set.</v>
<v Speaker 1>We have a really simple model is highly </v>

168
00:08:33.301 --> 00:08:34.950
<v Speaker 1>regularized.</v>
<v Speaker 1>It still gives you know,</v>

169
00:08:35.310 --> 00:08:38.370
<v Speaker 1>perfect or near perfect accuracy.</v>
<v Speaker 1>Maybe it's fine.</v>

170
00:08:39.420 --> 00:08:40.253
<v Speaker 3>And</v>

171
00:08:41.400 --> 00:08:43.350
<v Speaker 1>the only way we can find out if it's </v>
<v Speaker 1>not,</v>

172
00:08:43.920 --> 00:08:44.753
<v Speaker 1>I would argue is by gathering some more </v>
<v Speaker 1>data and I'll point out that this data </v>

173
00:08:49.921 --> 00:08:50.754
<v Speaker 1>set has been diabolically constructed so</v>
<v Speaker 1>that there are some points in the space </v>

174
00:08:56.100 --> 00:08:57.720
<v Speaker 1>that are not particularly well </v>
<v Speaker 1>represented.</v>

175
00:08:58.650 --> 00:08:59.483
<v Speaker 1>And you can maybe tell yourself a story </v>
<v Speaker 1>about maybe this data was collected </v>

176
00:09:01.891 --> 00:09:06.390
<v Speaker 1>after some corporate five k or something</v>
<v Speaker 1>like that.</v>

177
00:09:06.420 --> 00:09:07.253
<v Speaker 1>Um,</v>
<v Speaker 1>so if we go out and collect some more </v>

178
00:09:08.041 --> 00:09:08.874
<v Speaker 1>data,</v>
<v Speaker 1>maybe we find that actually there's </v>

179
00:09:12.420 --> 00:09:15.980
<v Speaker 1>people wearing all kinds of shoes on </v>
<v Speaker 1>both sides of,</v>

180
00:09:16.080 --> 00:09:16.913
<v Speaker 1>of our imaginary classifier.</v>
<v Speaker 1>But that this shoe type feature is </v>

181
00:09:21.301 --> 00:09:22.134
<v Speaker 1>really not predictive at all and this </v>
<v Speaker 1>gets back to Constantine's point that </v>

182
00:09:25.261 --> 00:09:26.610
<v Speaker 1>perhaps,</v>
<v Speaker 1>uh,</v>

183
00:09:26.611 --> 00:09:31.611
<v Speaker 1>relying on features that are strongly </v>
<v Speaker 1>correlated but not necessarily causal,</v>

184
00:09:32.220 --> 00:09:35.610
<v Speaker 1>may be a point at which we're thinking </v>
<v Speaker 1>about a stereotype in some way.</v>

185
00:09:36.230 --> 00:09:39.180
<v Speaker 1>Um,</v>
<v Speaker 1>so obviously given this data and what we</v>

186
00:09:39.181 --> 00:09:40.014
<v Speaker 1>know now,</v>
<v Speaker 1>I would probably go back and suggest to </v>

187
00:09:42.301 --> 00:09:43.134
<v Speaker 1>linear classifier based on these,</v>
<v Speaker 1>these features of length of race and </v>

188
00:09:46.100 --> 00:09:51.100
<v Speaker 1>weekly training volumes as potentially a</v>
<v Speaker 1>better model to how does this happen?</v>

189
00:09:51.510 --> 00:09:52.343
<v Speaker 1>What's,</v>
<v Speaker 1>what's,</v>

190
00:09:54.120 --> 00:09:54.953
<v Speaker 1>what's the issue here that said play.</v>
<v Speaker 1>I'm one of the issues that's at play is </v>

191
00:09:59.281 --> 00:10:00.114
<v Speaker 1>that in supervised machine learning,</v>
<v Speaker 1>we often make the assumption that our </v>

192
00:10:05.671 --> 00:10:08.790
<v Speaker 1>training distribution and our test </v>
<v Speaker 1>distribution are identical.</v>

193
00:10:09.360 --> 00:10:10.193
<v Speaker 1>Right?</v>
<v Speaker 1>And we make this assumption for really </v>

194
00:10:12.031 --> 00:10:12.864
<v Speaker 1>good reason,</v>
<v Speaker 1>which is that if we make that </v>

195
00:10:14.941 --> 00:10:15.774
<v Speaker 1>assumption,</v>
<v Speaker 1>then we can pretend that there's no </v>

196
00:10:17.701 --> 00:10:19.590
<v Speaker 1>difference between correlation and </v>
<v Speaker 1>causation.</v>

197
00:10:19.770 --> 00:10:20.603
<v Speaker 1>And we can use all of our features,</v>
<v Speaker 1>whether they're what Constantine would </v>

198
00:10:23.371 --> 00:10:26.600
<v Speaker 1>call meaningful or causal or not.</v>
<v Speaker 1>Um,</v>

199
00:10:26.610 --> 00:10:27.443
<v Speaker 1>we can throw them in there and so long </v>
<v Speaker 1>as they're testing training </v>

200
00:10:28.981 --> 00:10:29.814
<v Speaker 1>distributions are the same.</v>
<v Speaker 1>We're probably okay to within some some </v>

201
00:10:32.701 --> 00:10:33.534
<v Speaker 1>degree,</v>
<v Speaker 1>but in the real world we don't just </v>

202
00:10:37.321 --> 00:10:38.154
<v Speaker 1>apply models to a training or test set.</v>
<v Speaker 1>We also use them to make predictions </v>

203
00:10:43.230 --> 00:10:44.820
<v Speaker 1>that may influence the world in some </v>
<v Speaker 1>way.</v>

204
00:10:46.290 --> 00:10:51.290
<v Speaker 1>And there I think that the right sort of</v>
<v Speaker 1>phrase to use isn't so much tests set.</v>

205
00:10:51.990 --> 00:10:55.770
<v Speaker 1>It's more inference time performance.</v>
<v Speaker 1>Okay.</v>

206
00:10:55.771 --> 00:10:57.030
<v Speaker 1>Because at,</v>
<v Speaker 1>at inference time,</v>

207
00:10:57.031 --> 00:10:59.760
<v Speaker 1>when we're going and applying our model </v>
<v Speaker 1>to some new instance in the world,</v>

208
00:10:59.820 --> 00:11:00.653
<v Speaker 1>we may not actually know what they let </v>
<v Speaker 1>the true label is ever or things like </v>

209
00:11:02.791 --> 00:11:04.650
<v Speaker 1>that,</v>
<v Speaker 1>but we still care very much about having</v>

210
00:11:04.651 --> 00:11:07.560
<v Speaker 1>good performance and making sure that </v>
<v Speaker 1>our,</v>

211
00:11:07.680 --> 00:11:08.513
<v Speaker 1>uh,</v>
<v Speaker 1>test that our training set matches our </v>

212
00:11:12.271 --> 00:11:14.320
<v Speaker 1>inference distribution to some degree </v>
<v Speaker 1>is,</v>

213
00:11:14.321 --> 00:11:16.920
<v Speaker 1>is like super critical.</v>
<v Speaker 1>Um,</v>

214
00:11:17.140 --> 00:11:19.200
<v Speaker 1>so let's go back to open images and what</v>
<v Speaker 1>was happening there.</v>

215
00:11:19.650 --> 00:11:20.483
<v Speaker 1>Um,</v>
<v Speaker 1>you'll recall that it did quite badly </v>

216
00:11:23.270 --> 00:11:24.103
<v Speaker 1>on,</v>
<v Speaker 1>at least anecdotally on that image of a </v>

217
00:11:27.031 --> 00:11:29.850
<v Speaker 1>bride who appeared to be from India.</v>
<v Speaker 1>Um,</v>

218
00:11:30.480 --> 00:11:32.430
<v Speaker 1>if we look at the Geo diversity of open </v>
<v Speaker 1>images,</v>

219
00:11:32.431 --> 00:11:33.264
<v Speaker 1>this is something where we did our best </v>
<v Speaker 1>to sort of track down the geolocation </v>

220
00:11:35.870 --> 00:11:36.703
<v Speaker 1>of,</v>
<v Speaker 1>of each of the images in the open image </v>

221
00:11:38.191 --> 00:11:39.024
<v Speaker 1>data set.</v>

222
00:11:39.080 --> 00:11:39.913
<v Speaker 1>Uh,</v>
<v Speaker 1>what we found was that an overwhelming </v>

223
00:11:42.541 --> 00:11:46.090
<v Speaker 1>proportion of the data in open images,</v>
<v Speaker 1>uh,</v>

224
00:11:46.110 --> 00:11:49.140
<v Speaker 1>was from North America and six countries</v>
<v Speaker 1>in Europe,</v>

225
00:11:49.920 --> 00:11:50.753
<v Speaker 1>um,</v>
<v Speaker 1>vanishingly small amounts of that data </v>

226
00:11:53.080 --> 00:11:53.913
<v Speaker 1>were from countries such as India or </v>
<v Speaker 1>China or other places where I've heard </v>

227
00:11:57.071 --> 00:11:59.170
<v Speaker 1>there's actually a large number of </v>
<v Speaker 1>people.</v>

228
00:12:00.550 --> 00:12:01.383
<v Speaker 1>So this is clearly not representative in</v>
<v Speaker 1>a meaningful way of sort of the global </v>

229
00:12:07.421 --> 00:12:10.210
<v Speaker 1>diversity of the world.</v>
<v Speaker 1>How does this happen?</v>

230
00:12:10.690 --> 00:12:13.530
<v Speaker 1>It's not like the researchers who put </v>
<v Speaker 1>the open images dataset.</v>

231
00:12:13.560 --> 00:12:14.393
<v Speaker 1>We're in any way on all intention.</v>
<v Speaker 1>They were working really hard to put </v>

232
00:12:17.111 --> 00:12:17.944
<v Speaker 1>together what they believe was a more </v>
<v Speaker 1>representative data set than the image </v>

233
00:12:21.221 --> 00:12:22.510
<v Speaker 1>net.</v>
<v Speaker 1>Um,</v>

234
00:12:22.750 --> 00:12:23.583
<v Speaker 1>at the very least,</v>
<v Speaker 1>they don't have 100 categories of dogs </v>

235
00:12:24.731 --> 00:12:26.170
<v Speaker 1>in this one.</v>
<v Speaker 1>Um,</v>

236
00:12:27.970 --> 00:12:29.050
<v Speaker 1>so what happens?</v>
<v Speaker 1>Well,</v>

237
00:12:29.110 --> 00:12:29.943
<v Speaker 1>you could make an argument that there's </v>
<v Speaker 1>some strong correlation with the </v>

238
00:12:31.811 --> 00:12:36.811
<v Speaker 1>distribution of open images with the </v>
<v Speaker 1>distribution of countries with high,</v>

239
00:12:38.030 --> 00:12:39.730
<v Speaker 1>low,</v>
<v Speaker 1>high bandwidth,</v>

240
00:12:39.731 --> 00:12:41.050
<v Speaker 1>low cost,</v>
<v Speaker 1>Internet access.</v>

241
00:12:41.870 --> 00:12:43.780
<v Speaker 1>Um,</v>
<v Speaker 1>it's not a perfect correlation,</v>

242
00:12:43.781 --> 00:12:45.910
<v Speaker 1>but it's,</v>
<v Speaker 1>it's pretty close.</v>

243
00:12:46.360 --> 00:12:49.710
<v Speaker 1>Um,</v>
<v Speaker 1>and that if we're doing a,</v>

244
00:12:49.780 --> 00:12:50.613
<v Speaker 1>if one might do things like base an </v>
<v Speaker 1>image classifier on data drawn from a </v>

245
00:12:55.691 --> 00:12:59.350
<v Speaker 1>distribution of areas that have high </v>
<v Speaker 1>bandwidth,</v>

246
00:12:59.351 --> 00:13:01.270
<v Speaker 1>low cost Internet access,</v>
<v Speaker 1>um,</v>

247
00:13:01.750 --> 00:13:02.583
<v Speaker 1>that may induce differences between the </v>
<v Speaker 1>trading distribution and the inference </v>

248
00:13:06.761 --> 00:13:07.594
<v Speaker 1>time distribution.</v>
<v Speaker 1>So none of this is like something you </v>

249
00:13:10.961 --> 00:13:15.670
<v Speaker 1>wouldn't figure out without it if you </v>
<v Speaker 1>sat down for five minutes.</v>

250
00:13:15.671 --> 00:13:17.920
<v Speaker 1>Right?</v>
<v Speaker 1>This is all like super basic statistics.</v>

251
00:13:18.070 --> 00:13:18.903
<v Speaker 1>It is in fact stuff that's this just six</v>
<v Speaker 1>people have been sort of railing at the </v>

252
00:13:22.391 --> 00:13:24.550
<v Speaker 1>machine learning community at for the </v>
<v Speaker 1>last several decades.</v>

253
00:13:24.740 --> 00:13:25.573
<v Speaker 1>Um,</v>
<v Speaker 1>but as machine learning models become </v>

254
00:13:28.691 --> 00:13:32.050
<v Speaker 1>sort of more ubiquitous in everyday </v>
<v Speaker 1>life,</v>

255
00:13:32.300 --> 00:13:33.133
<v Speaker 1>I think that paying attention to these </v>
<v Speaker 1>kinds of issues becomes ever more </v>

256
00:13:35.291 --> 00:13:37.050
<v Speaker 1>important.</v>
<v Speaker 1>Uh,</v>

257
00:13:37.110 --> 00:13:37.943
<v Speaker 1>so let's go back to,</v>
<v Speaker 1>to what is a stereotype and I think I </v>

258
00:13:39.791 --> 00:13:41.430
<v Speaker 1>agree with Constantine's,</v>
<v Speaker 1>um,</v>

259
00:13:41.610 --> 00:13:42.443
<v Speaker 1>idea.</v>
<v Speaker 1>And I'm going to add one more tweak to </v>

260
00:13:43.781 --> 00:13:46.450
<v Speaker 1>it.</v>
<v Speaker 1>So I'm going to say that a stereotype is</v>

261
00:13:46.451 --> 00:13:47.284
<v Speaker 1>a statistical confounder.</v>
<v Speaker 1>I think Susan Constantine's language </v>

262
00:13:49.361 --> 00:13:53.110
<v Speaker 1>almost exactly a that has a societal </v>
<v Speaker 1>basis.</v>

263
00:13:55.510 --> 00:13:59.200
<v Speaker 1>So when I think about issues of </v>
<v Speaker 1>fairness,</v>

264
00:13:59.560 --> 00:14:01.990
<v Speaker 1>if it's the case that,</v>
<v Speaker 1>um,</v>

265
00:14:02.560 --> 00:14:03.393
<v Speaker 1>you know,</v>
<v Speaker 1>rainy weather is correlated with people </v>

266
00:14:05.140 --> 00:14:06.310
<v Speaker 1>using umbrellas,</v>
<v Speaker 1>like,</v>

267
00:14:06.311 --> 00:14:07.300
<v Speaker 1>yes,</v>
<v Speaker 1>that's a confounder.</v>

268
00:14:07.310 --> 00:14:10.380
<v Speaker 1>The umbrellas did not cause the rain.</v>
<v Speaker 1>Um,</v>

269
00:14:10.900 --> 00:14:11.733
<v Speaker 1>but I'm not as worried as a individual </v>
<v Speaker 1>human about the societal impact of </v>

270
00:14:16.121 --> 00:14:20.820
<v Speaker 1>models that are based on that module.</v>
<v Speaker 1>I'm sure you could imagine some crazies,</v>

271
00:14:20.920 --> 00:14:23.050
<v Speaker 1>scary scenario where that was the case,</v>
<v Speaker 1>but in general,</v>

272
00:14:24.150 --> 00:14:24.983
<v Speaker 1>I don't think that that's an issue,</v>
<v Speaker 1>but when we think of things like </v>

273
00:14:26.711 --> 00:14:30.370
<v Speaker 1>internet connectivity or other societal </v>
<v Speaker 1>factors,</v>

274
00:14:30.840 --> 00:14:31.673
<v Speaker 1>I think that paying attention to </v>
<v Speaker 1>questions of do we have confounders in </v>

275
00:14:34.541 --> 00:14:37.060
<v Speaker 1>our data,</v>
<v Speaker 1>are they being picked up by our models?</v>

276
00:14:37.130 --> 00:14:38.110
<v Speaker 1>Um,</v>
<v Speaker 1>is,</v>

277
00:14:38.230 --> 00:14:41.140
<v Speaker 1>is incredibly important.</v>
<v Speaker 1>So,</v>

278
00:14:41.250 --> 00:14:42.083
<v Speaker 1>um,</v>
<v Speaker 1>if you take away nothing else from this </v>

279
00:14:44.080 --> 00:14:44.913
<v Speaker 1>short talk,</v>
<v Speaker 1>I hope that you take away a caution to </v>

280
00:14:48.041 --> 00:14:48.874
<v Speaker 1>be aware of differences between your </v>
<v Speaker 1>training and inference asked the </v>

281
00:14:52.371 --> 00:14:53.720
<v Speaker 1>question,</v>
<v Speaker 1>uh,</v>

282
00:14:54.110 --> 00:14:54.943
<v Speaker 1>because statistically this is not a </v>
<v Speaker 1>particularly difficult thing to uncover </v>

283
00:14:58.220 --> 00:15:03.220
<v Speaker 1>if you take the time to look in a world </v>
<v Speaker 1>of kaggle competitions and people trying</v>

284
00:15:04.641 --> 00:15:07.700
<v Speaker 1>to get high marks on deep learning </v>
<v Speaker 1>classes and things like that.</v>

285
00:15:07.820 --> 00:15:08.653
<v Speaker 1>I think it's all too easy for us to just</v>
<v Speaker 1>take datasets as given not thinking </v>

286
00:15:12.981 --> 00:15:13.814
<v Speaker 1>about them too much and just try and get</v>
<v Speaker 1>our accuracy from 99 point one to 99 </v>

287
00:15:16.581 --> 00:15:17.414
<v Speaker 1>point two.</v>
<v Speaker 1>And as someone who's interested in </v>

288
00:15:21.171 --> 00:15:24.680
<v Speaker 1>people coming out of programs like this </v>
<v Speaker 1>being ready to do work in the real world</v>

289
00:15:24.681 --> 00:15:27.500
<v Speaker 1>at a,</v>
<v Speaker 1>I would caution that,</v>

290
00:15:27.850 --> 00:15:28.683
<v Speaker 1>um,</v>
<v Speaker 1>we can't only be training ourselves to </v>

291
00:15:31.071 --> 00:15:34.070
<v Speaker 1>do that.</v>
<v Speaker 1>So with that,</v>

292
00:15:34.610 --> 00:15:35.443
<v Speaker 1>I'm going to leave you with a set of </v>
<v Speaker 1>additional resources around machine </v>

293
00:15:38.151 --> 00:15:38.984
<v Speaker 1>learning fairness.</v>
<v Speaker 1>These are super hot off the presses in </v>

294
00:15:41.031 --> 00:15:45.170
<v Speaker 1>the sense that this particular little </v>
<v Speaker 1>website was launched at I think 8:30</v>

295
00:15:45.171 --> 00:15:46.640
<v Speaker 1>this morning,</v>
<v Speaker 1>something like that.</v>

296
00:15:46.641 --> 00:15:49.060
<v Speaker 1>So you've got it first.</v>
<v Speaker 1>Um,</v>

297
00:15:49.400 --> 00:15:54.080
<v Speaker 1>mit leading the way,</v>
<v Speaker 1>I'm in on this page.</v>

298
00:15:54.150 --> 00:15:55.340
<v Speaker 1>Uh,</v>
<v Speaker 1>there are in,</v>

299
00:15:55.390 --> 00:15:56.670
<v Speaker 1>yeah,</v>
<v Speaker 1>you can open your laptops again now.</v>

300
00:15:57.310 --> 00:15:58.930
<v Speaker 1>Um,</v>
<v Speaker 1>uh,</v>

301
00:15:58.940 --> 00:15:59.773
<v Speaker 1>there are a number of papers that go </v>
<v Speaker 1>through this sort of like a greatest </v>

302
00:16:03.141 --> 00:16:03.974
<v Speaker 1>hits of the machine learning fairness </v>
<v Speaker 1>literature from the last couple of </v>

303
00:16:05.780 --> 00:16:06.050
<v Speaker 1>years.</v>

304
00:16:06.050 --> 00:16:07.970
<v Speaker 1>Um,</v>
<v Speaker 1>really interesting papers.</v>

305
00:16:08.120 --> 00:16:08.953
<v Speaker 1>I don't think any of them are like the </v>
<v Speaker 1>one final solution to machine learning </v>

306
00:16:11.631 --> 00:16:14.690
<v Speaker 1>fairness issues,</v>
<v Speaker 1>but they're super interesting reads that</v>

307
00:16:14.691 --> 00:16:18.890
<v Speaker 1>I think help sort of paint the space and</v>
<v Speaker 1>the landscape really usefully.</v>

308
00:16:19.360 --> 00:16:21.740
<v Speaker 1>There are also a couple of interesting </v>
<v Speaker 1>exercises there,</v>

309
00:16:22.010 --> 00:16:23.240
<v Speaker 1>um,</v>
<v Speaker 1>that,</v>

310
00:16:23.290 --> 00:16:25.040
<v Speaker 1>uh,</v>
<v Speaker 1>you can access by a colab.</v>

311
00:16:25.100 --> 00:16:27.500
<v Speaker 1>Um,</v>
<v Speaker 1>and uh,</v>

312
00:16:27.560 --> 00:16:30.650
<v Speaker 1>if you're interested in this space,</v>
<v Speaker 1>there things that you can play with a,</v>

313
00:16:30.670 --> 00:16:34.720
<v Speaker 1>I think they include one on adversarial </v>
<v Speaker 1>d biasing where I'm,</v>

314
00:16:34.940 --> 00:16:37.160
<v Speaker 1>because you guys all love deep learning.</v>
<v Speaker 1>Um,</v>

315
00:16:37.190 --> 00:16:42.190
<v Speaker 1>you can use a network to try and become </v>
<v Speaker 1>unbiased by,</v>

316
00:16:42.370 --> 00:16:43.203
<v Speaker 1>uh,</v>
<v Speaker 1>making sure that by having an extra </v>

317
00:16:44.691 --> 00:16:45.524
<v Speaker 1>output head that predicts a </v>
<v Speaker 1>characteristic that you wish to be </v>

318
00:16:48.681 --> 00:16:53.480
<v Speaker 1>unbiased on and then panelizing that </v>
<v Speaker 1>model if it's good at predicting that,</v>

319
00:16:53.530 --> 00:16:55.250
<v Speaker 1>that characteristic.</v>
<v Speaker 1>Uh,</v>

320
00:16:55.251 --> 00:16:56.084
<v Speaker 1>and so this is trying to adversarially </v>
<v Speaker 1>make sure that our internal </v>

321
00:16:58.221 --> 00:16:59.054
<v Speaker 1>representation in a deep network is not </v>
<v Speaker 1>picking up a unwanted correlations </v>

322
00:17:03.200 --> 00:17:05.930
<v Speaker 1>around one biases.</v>
<v Speaker 1>So I hope that that's interesting.</v>

323
00:17:06.160 --> 00:17:07.690
<v Speaker 1>Um,</v>
<v Speaker 1>and,</v>

324
00:17:07.691 --> 00:17:08.524
<v Speaker 1>uh,</v>
<v Speaker 1>I'll be around afterwards to take </v>

325
00:17:09.921 --> 00:17:10.754
<v Speaker 1>questions,</v>
<v Speaker 1>but at this point I'd like to make sure </v>

326
00:17:12.651 --> 00:17:15.110
<v Speaker 1>that sunshine has plenty of time.</v>
<v Speaker 1>So thank you very much.</v>

