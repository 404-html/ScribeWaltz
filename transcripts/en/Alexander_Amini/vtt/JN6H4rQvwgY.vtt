WEBVTT

1
00:00:02.490 --> 00:00:04.620
<v Speaker 1>Good morning everyone,</v>
<v Speaker 1>thank you for.</v>

2
00:00:05.100 --> 00:00:10.100
<v Speaker 1>Thank you all for joining us.</v>
<v Speaker 1>This is mit success one nine one,</v>

3
00:00:10.470 --> 00:00:11.303
<v Speaker 1>and we'd like to welcome to welcome you </v>
<v Speaker 1>to this course on introduction to deep </v>

4
00:00:13.981 --> 00:00:14.814
<v Speaker 1>learning.</v>
<v Speaker 1>So in this course you'll learn how to </v>

5
00:00:18.330 --> 00:00:21.690
<v Speaker 1>build remarkable algorithms,</v>
<v Speaker 1>intelligent algorithms,</v>

6
00:00:22.290 --> 00:00:24.840
<v Speaker 1>capable of solving very complex </v>
<v Speaker 1>problems.</v>

7
00:00:24.841 --> 00:00:25.674
<v Speaker 1>They just a decade ago,</v>
<v Speaker 1>we're not even a feasible to solve and </v>

8
00:00:30.420 --> 00:00:33.600
<v Speaker 1>let's just start with this notion of </v>
<v Speaker 1>intelligence.</v>

9
00:00:34.560 --> 00:00:35.393
<v Speaker 1>So at a very high level,</v>
<v Speaker 1>intelligence is the ability to process </v>

10
00:00:39.330 --> 00:00:44.330
<v Speaker 1>information so that it can be used to </v>
<v Speaker 1>inform future predictions and decisions.</v>

11
00:00:45.990 --> 00:00:49.500
<v Speaker 1>Now when this intelligence is not </v>
<v Speaker 1>engineered,</v>

12
00:00:49.830 --> 00:00:54.830
<v Speaker 1>but rather a biological inspiration such</v>
<v Speaker 1>as in humans,</v>

13
00:00:55.580 --> 00:00:57.420
<v Speaker 1>uh,</v>
<v Speaker 1>it's called human intelligence,</v>

14
00:00:57.450 --> 00:00:58.283
<v Speaker 1>but when it's engineered,</v>
<v Speaker 1>we referred to it as artificial </v>

15
00:00:59.821 --> 00:01:00.654
<v Speaker 1>intelligence.</v>
<v Speaker 1>So this course is a course on deep </v>

16
00:01:02.701 --> 00:01:03.534
<v Speaker 1>learning,</v>
<v Speaker 1>which is just a subset of artificial </v>

17
00:01:05.101 --> 00:01:08.430
<v Speaker 1>intelligence.</v>
<v Speaker 1>And really it's just a subset of machine</v>

18
00:01:08.431 --> 00:01:09.264
<v Speaker 1>learning which involves more traditional</v>
<v Speaker 1>methods where we tried to learn a </v>

19
00:01:14.640 --> 00:01:16.680
<v Speaker 1>representations directly from data.</v>

20
00:01:16.770 --> 00:01:19.350
<v Speaker 1>And we'll talk about this more in detail</v>
<v Speaker 1>later today.</v>

21
00:01:20.370 --> 00:01:24.420
<v Speaker 1>But let me first just start by talking </v>
<v Speaker 1>about some of the amazing successes that</v>

22
00:01:24.421 --> 00:01:29.280
<v Speaker 1>deep learning has had in the past.</v>
<v Speaker 1>So in 2012,</v>

23
00:01:29.730 --> 00:01:30.563
<v Speaker 1>this competition called image net came </v>
<v Speaker 1>out which tasks ai researchers to build </v>

24
00:01:34.501 --> 00:01:38.340
<v Speaker 1>an AI system capable of recognizing </v>
<v Speaker 1>images,</v>

25
00:01:38.370 --> 00:01:39.203
<v Speaker 1>objects in images,</v>
<v Speaker 1>and there is millions of examples in </v>

26
00:01:42.391 --> 00:01:43.224
<v Speaker 1>this data set and the winter in 2012 for</v>
<v Speaker 1>the first time ever was a deep learning </v>

27
00:01:47.641 --> 00:01:48.474
<v Speaker 1>based system and when it came out,</v>
<v Speaker 1>it absolutely shattered all other </v>

28
00:01:52.260 --> 00:01:55.500
<v Speaker 1>competitors and crushed the competition.</v>
<v Speaker 1>I'm crushed,</v>

29
00:01:55.980 --> 00:01:56.813
<v Speaker 1>crushed the challenge,</v>
<v Speaker 1>and today these deep learning based </v>

30
00:02:00.121 --> 00:02:00.954
<v Speaker 1>systems have actually surpassed human </v>
<v Speaker 1>level accuracy on the image net </v>

31
00:02:04.231 --> 00:02:08.370
<v Speaker 1>challenge and can actually recognize </v>
<v Speaker 1>images even better than humans can.</v>

32
00:02:10.650 --> 00:02:11.481
<v Speaker 1>Now,</v>
<v Speaker 1>in this class,</v>

33
00:02:11.481 --> 00:02:14.580
<v Speaker 1>you'll actually learn how to build </v>
<v Speaker 1>complex vision systems,</v>

34
00:02:14.581 --> 00:02:16.620
<v Speaker 1>building a computer that knows how to </v>
<v Speaker 1>see,</v>

35
00:02:17.070 --> 00:02:17.903
<v Speaker 1>and just tomorrow you'll learn how to </v>
<v Speaker 1>build an algorithm that will take as </v>

36
00:02:21.871 --> 00:02:22.704
<v Speaker 1>input x ray images and as output,</v>
<v Speaker 1>it will detect if that person has a </v>

37
00:02:27.271 --> 00:02:31.650
<v Speaker 1>pneumothorax.</v>
<v Speaker 1>Just from that single input image.</v>

38
00:02:33.150 --> 00:02:33.983
<v Speaker 1>You'll even make the network explained </v>
<v Speaker 1>to you why it decided to diagnose the </v>

39
00:02:38.071 --> 00:02:42.480
<v Speaker 1>way a diagnosed by looking inside the </v>
<v Speaker 1>network and understanding exactly why it</v>

40
00:02:42.481 --> 00:02:47.481
<v Speaker 1>made that decision.</v>
<v Speaker 1>Deep neural networks can also be used to</v>

41
00:02:47.611 --> 00:02:50.760
<v Speaker 1>model sequences where your data points </v>
<v Speaker 1>are not just single images,</v>

42
00:02:50.761 --> 00:02:54.780
<v Speaker 1>but rather temporarily dependent.</v>
<v Speaker 1>So for this you can think of things like</v>

43
00:02:55.230 --> 00:03:00.230
<v Speaker 1>predicting the stock price,</v>
<v Speaker 1>translating sentences from English to or</v>

44
00:03:00.521 --> 00:03:01.354
<v Speaker 1>even generating new music.</v>
<v Speaker 1>So actually today you'll learn how to </v>

45
00:03:03.641 --> 00:03:04.474
<v Speaker 1>create and actually you'll create </v>
<v Speaker 1>yourselves and algorithm that learns </v>

46
00:03:07.870 --> 00:03:08.703
<v Speaker 1>that first listened to hours of music,</v>
<v Speaker 1>learns the underlying representation of </v>

47
00:03:13.391 --> 00:03:18.070
<v Speaker 1>the notes that are being played in those</v>
<v Speaker 1>songs and then alerts to build brand new</v>

48
00:03:18.071 --> 00:03:19.780
<v Speaker 1>songs that have never been heard before.</v>

49
00:03:22.390 --> 00:03:23.223
<v Speaker 1>And there are really so many other </v>
<v Speaker 1>incredible success stories of deep </v>

50
00:03:25.301 --> 00:03:29.170
<v Speaker 1>learning that I could talk for many </v>
<v Speaker 1>hours about it and we'll try to cover as</v>

51
00:03:29.170 --> 00:03:31.060
<v Speaker 1>many of these as possible as part of </v>
<v Speaker 1>this course.</v>

52
00:03:31.510 --> 00:03:32.343
<v Speaker 1>But I just wanted to give you an </v>
<v Speaker 1>overview of some of the amazing ones </v>

53
00:03:34.211 --> 00:03:37.090
<v Speaker 1>that we'll be covering as part of the </v>
<v Speaker 1>labs that you'll be implementing.</v>

54
00:03:38.590 --> 00:03:42.250
<v Speaker 1>And that's really a goal of what we want</v>
<v Speaker 1>you to accomplish as part of this class.</v>

55
00:03:42.310 --> 00:03:43.143
<v Speaker 1>Firstly,</v>
<v Speaker 1>we want to provide you with the </v>

56
00:03:44.411 --> 00:03:45.244
<v Speaker 1>foundation to do deep learning to </v>
<v Speaker 1>understand what these algorithms are </v>

57
00:03:48.941 --> 00:03:50.950
<v Speaker 1>doing underneath the hood,</v>
<v Speaker 1>how they work,</v>

58
00:03:50.951 --> 00:03:51.784
<v Speaker 1>and why they work.</v>
<v Speaker 1>We will provide you with some of the </v>

59
00:03:54.881 --> 00:03:55.714
<v Speaker 1>practical skills to implement these </v>
<v Speaker 1>algorithms and deploy them on your own </v>

60
00:03:58.631 --> 00:03:59.464
<v Speaker 1>machines and we'll talk to you about </v>
<v Speaker 1>some of the state state of art and </v>

61
00:04:03.821 --> 00:04:07.840
<v Speaker 1>cutting edge research that's happening </v>
<v Speaker 1>in a deep learning industries,</v>

62
00:04:07.900 --> 00:04:12.220
<v Speaker 1>deep learning academia institutions.</v>
<v Speaker 1>Finally,</v>

63
00:04:12.280 --> 00:04:13.113
<v Speaker 1>the main purpose of this course is we </v>
<v Speaker 1>want to build a community here at mit </v>

64
00:04:17.620 --> 00:04:21.100
<v Speaker 1>that is devoted to advancing the state </v>
<v Speaker 1>of artificial intelligence.</v>

65
00:04:21.160 --> 00:04:21.993
<v Speaker 1>Advancing a state of deep learning.</v>
<v Speaker 1>As part of this course will cover some </v>

66
00:04:25.241 --> 00:04:28.570
<v Speaker 1>of the limitations of these algorithms.</v>
<v Speaker 1>There are many.</v>

67
00:04:29.020 --> 00:04:30.940
<v Speaker 1>We need to be mindful of these </v>
<v Speaker 1>limitations.</v>

68
00:04:30.941 --> 00:04:31.774
<v Speaker 1>So I said we as a community can move </v>
<v Speaker 1>forward and create more intelligent </v>

69
00:04:34.691 --> 00:04:35.524
<v Speaker 1>systems.</v>

70
00:04:37.730 --> 00:04:38.563
<v Speaker 1>But before we do that,</v>
<v Speaker 1>let's just start with some </v>

71
00:04:40.070 --> 00:04:45.070
<v Speaker 1>administrative details in this course.</v>
<v Speaker 1>So this course is a one week course.</v>

72
00:04:45.800 --> 00:04:49.160
<v Speaker 1>Today is the first lecture we meet </v>
<v Speaker 1>everyday this week,</v>

73
00:04:49.340 --> 00:04:51.320
<v Speaker 1>10:30</v>
<v Speaker 1>AM to 1:30</v>

74
00:04:51.320 --> 00:04:52.153
<v Speaker 1>PM,</v>
<v Speaker 1>and this during this three hour time </v>

75
00:04:54.381 --> 00:04:55.214
<v Speaker 1>slot,</v>
<v Speaker 1>we're broken down into a one and a half </v>

76
00:04:57.860 --> 00:05:00.530
<v Speaker 1>hour time slots,</v>
<v Speaker 1>around 50 percent of the course.</v>

77
00:05:00.560 --> 00:05:01.393
<v Speaker 1>Each and each of those half a half </v>
<v Speaker 1>sections of this course will consist of </v>

78
00:05:08.570 --> 00:05:10.550
<v Speaker 1>lectures,</v>
<v Speaker 1>which is what you're in right now,</v>

79
00:05:10.850 --> 00:05:11.683
<v Speaker 1>and the second part is the labs where </v>
<v Speaker 1>you will actually get practice </v>

80
00:05:13.611 --> 00:05:15.770
<v Speaker 1>implementing what you learn in lectures.</v>

81
00:05:18.500 --> 00:05:20.660
<v Speaker 1>We have an amazing set of lectures lined</v>
<v Speaker 1>up for you,</v>

82
00:05:20.750 --> 00:05:21.583
<v Speaker 1>so today we're going to be talking about</v>
<v Speaker 1>some of the introduction to neural </v>

83
00:05:24.951 --> 00:05:25.784
<v Speaker 1>networks,</v>
<v Speaker 1>which is really the backbone of deep </v>

84
00:05:27.111 --> 00:05:27.944
<v Speaker 1>learning.</v>
<v Speaker 1>We're also talking about modeling </v>

85
00:05:30.111 --> 00:05:30.944
<v Speaker 1>sequence data,</v>
<v Speaker 1>so this is what I was mentioning about </v>

86
00:05:32.631 --> 00:05:33.464
<v Speaker 1>the temporarily dependent data.</v>
<v Speaker 1>Tomorrow we'll talk about computer </v>

87
00:05:37.281 --> 00:05:38.114
<v Speaker 1>vision and deep generative models.</v>
<v Speaker 1>We have one of the inventors of </v>

88
00:05:41.481 --> 00:05:44.720
<v Speaker 1>generative adversarial networks coming </v>
<v Speaker 1>to give that lecture for us,</v>

89
00:05:44.721 --> 00:05:45.554
<v Speaker 1>so that's going to be a great lecture </v>
<v Speaker 1>and the day after that we'll touch on </v>

90
00:05:49.071 --> 00:05:53.570
<v Speaker 1>deep reinforcement learning and some of </v>
<v Speaker 1>the open challenges in ai and how we can</v>

91
00:05:53.571 --> 00:05:54.404
<v Speaker 1>move forward past this course.</v>
<v Speaker 1>We'll spend final two days of this </v>

92
00:05:58.910 --> 00:05:59.743
<v Speaker 1>course talking or hearing from some of </v>
<v Speaker 1>the leading industry representatives </v>

93
00:06:04.461 --> 00:06:05.294
<v Speaker 1>doing deep learning in their respective </v>
<v Speaker 1>companies and these are bound to be </v>

94
00:06:09.471 --> 00:06:11.870
<v Speaker 1>extremely interesting,</v>
<v Speaker 1>extremely exciting,</v>

95
00:06:11.871 --> 00:06:13.610
<v Speaker 1>so I highly recommend attending of these</v>
<v Speaker 1>as well.</v>

96
00:06:16.520 --> 00:06:18.710
<v Speaker 1>For those of you who are taking this </v>
<v Speaker 1>course for credit,</v>

97
00:06:19.100 --> 00:06:22.550
<v Speaker 1>you have two options to fulfill your </v>
<v Speaker 1>greatest assignment.</v>

98
00:06:23.330 --> 00:06:24.163
<v Speaker 1>The first option is a project proposal.</v>
<v Speaker 1>It's a one minute project pitch that </v>

99
00:06:28.311 --> 00:06:33.311
<v Speaker 1>will take place during Friday and for </v>
<v Speaker 1>this you have to work in groups of three</v>

100
00:06:33.651 --> 00:06:34.484
<v Speaker 1>or four and what you'll be tasked to do </v>
<v Speaker 1>is just come up with a interesting deep </v>

101
00:06:37.791 --> 00:06:42.791
<v Speaker 1>learning idea and tried to show some </v>
<v Speaker 1>sort of results if possible.</v>

102
00:06:42.860 --> 00:06:43.693
<v Speaker 1>We understand that one week is extremely</v>
<v Speaker 1>short to create any type of results or </v>

103
00:06:47.091 --> 00:06:49.910
<v Speaker 1>even come up with an interesting idea </v>
<v Speaker 1>for that matter,</v>

104
00:06:50.390 --> 00:06:51.223
<v Speaker 1>but um,</v>
<v Speaker 1>we're going to be giving out some </v>

105
00:06:52.401 --> 00:06:55.910
<v Speaker 1>amazing prizes,</v>
<v Speaker 1>so including some Nvidia,</v>

106
00:06:55.911 --> 00:07:00.170
<v Speaker 1>Gpu and Google homes,</v>
<v Speaker 1>uh,</v>

107
00:07:00.230 --> 00:07:01.910
<v Speaker 1>on Friday you'll,</v>
<v Speaker 1>like I said,</v>

108
00:07:01.911 --> 00:07:02.744
<v Speaker 1>give a one minute pitch.</v>
<v Speaker 1>There is somewhat of an arts to a </v>

109
00:07:06.560 --> 00:07:09.620
<v Speaker 1>pitching your idea and just one minute,</v>
<v Speaker 1>even though it's extremely short,</v>

110
00:07:09.950 --> 00:07:12.650
<v Speaker 1>so we will be holding you to a strict </v>
<v Speaker 1>deadline up that one minute.</v>

111
00:07:14.750 --> 00:07:16.520
<v Speaker 1>The second option is a little more </v>
<v Speaker 1>boring,</v>

112
00:07:16.790 --> 00:07:20.390
<v Speaker 1>but you'll be able to write a one page </v>
<v Speaker 1>paper about any deep learning paper that</v>

113
00:07:20.391 --> 00:07:21.224
<v Speaker 1>you find interesting.</v>
<v Speaker 1>And really that's if you can't do the </v>

114
00:07:23.691 --> 00:07:25.760
<v Speaker 1>project proposal,</v>
<v Speaker 1>you can do that.</v>

115
00:07:29.230 --> 00:07:33.160
<v Speaker 1>This class has a lot of online resources</v>
<v Speaker 1>you can find support on Piazza.</v>

116
00:07:33.161 --> 00:07:37.060
<v Speaker 1>Please post if you have any questions </v>
<v Speaker 1>about the lectures to labs,</v>

117
00:07:37.600 --> 00:07:39.340
<v Speaker 1>installing any of the software,</v>
<v Speaker 1>et Cetera.</v>

118
00:07:40.780 --> 00:07:41.613
<v Speaker 1>Also,</v>
<v Speaker 1>try to keep up to date with the course </v>

119
00:07:42.551 --> 00:07:46.030
<v Speaker 1>website where we'll be posting all of </v>
<v Speaker 1>the lectures,</v>

120
00:07:46.090 --> 00:07:49.480
<v Speaker 1>labs and video recordings online as </v>
<v Speaker 1>well.</v>

121
00:07:51.920 --> 00:07:52.753
<v Speaker 1>We have an amazing team that you can </v>
<v Speaker 1>reach out to at any time in case you </v>

122
00:07:56.121 --> 00:07:56.954
<v Speaker 1>have any problems with anything,</v>
<v Speaker 1>feel free to reach out to any of us and </v>

123
00:07:59.211 --> 00:08:03.110
<v Speaker 1>we want to give a huge thanks to all of </v>
<v Speaker 1>our sponsors who without this,</v>

124
00:08:03.200 --> 00:08:06.470
<v Speaker 1>without their support,</v>
<v Speaker 1>this class would simply not happened the</v>

125
00:08:06.471 --> 00:08:07.830
<v Speaker 1>way,</v>
<v Speaker 1>uh,</v>

126
00:08:07.831 --> 00:08:09.020
<v Speaker 1>the way it's happening this year.</v>

127
00:08:11.110 --> 00:08:11.943
<v Speaker 1>So now let's start with the fun stuff </v>
<v Speaker 1>and let's start by actually asking </v>

128
00:08:15.461 --> 00:08:19.900
<v Speaker 1>ourselves the question,</v>
<v Speaker 1>why do we even care about deep learning?</v>

129
00:08:20.590 --> 00:08:25.590
<v Speaker 1>So why now and why do we,</v>
<v Speaker 1>why do we even sit in this class today?</v>

130
00:08:27.160 --> 00:08:27.993
<v Speaker 1>So traditional machine learning </v>
<v Speaker 1>algorithms typically defined sets of </v>

131
00:08:32.560 --> 00:08:33.393
<v Speaker 1>preprogrammed features and the data and </v>
<v Speaker 1>they work to extract these features as </v>

132
00:08:38.051 --> 00:08:40.120
<v Speaker 1>part of their pipeline.</v>
<v Speaker 1>Now,</v>

133
00:08:40.121 --> 00:08:40.954
<v Speaker 1>the key differentiating point of deep </v>
<v Speaker 1>learning is that it recognizes that in </v>

134
00:08:43.901 --> 00:08:47.980
<v Speaker 1>many practical situations,</v>
<v Speaker 1>these features can be extremely brittle.</v>

135
00:08:48.970 --> 00:08:49.803
<v Speaker 1>So at deep learning tries to do is learn</v>
<v Speaker 1>these features directly from data as </v>

136
00:08:54.251 --> 00:08:57.090
<v Speaker 1>opposed to being hand engineered by the </v>
<v Speaker 1>human.</v>

137
00:08:58.960 --> 00:08:59.793
<v Speaker 1>That is,</v>
<v Speaker 1>can we learn if we want to learn to </v>

138
00:09:01.451 --> 00:09:02.284
<v Speaker 1>detect faces,</v>
<v Speaker 1>can we first learn automatically from </v>

139
00:09:04.001 --> 00:09:04.834
<v Speaker 1>data that to detect faces,</v>
<v Speaker 1>we first need to detect edges and the </v>

140
00:09:07.961 --> 00:09:12.400
<v Speaker 1>image compose these edges together to </v>
<v Speaker 1>detect eyes and ears,</v>

141
00:09:12.910 --> 00:09:13.743
<v Speaker 1>then compose these eyes and ears </v>
<v Speaker 1>together to form higher level facial </v>

142
00:09:16.421 --> 00:09:17.254
<v Speaker 1>structure and this way,</v>
<v Speaker 1>deep learning represents a form of a </v>

143
00:09:20.140 --> 00:09:20.973
<v Speaker 1>hierarchical model capable of </v>
<v Speaker 1>representing different levels of </v>

144
00:09:24.821 --> 00:09:25.654
<v Speaker 1>abstraction in the data.</v>
<v Speaker 1>So actually the fundamental building </v>

145
00:09:29.591 --> 00:09:30.424
<v Speaker 1>blocks of deep learning,</v>
<v Speaker 1>which are neural networks have actually </v>

146
00:09:33.281 --> 00:09:36.670
<v Speaker 1>been existing,</v>
<v Speaker 1>have actually existed for decades.</v>

147
00:09:37.060 --> 00:09:39.340
<v Speaker 1>So why are we studying this now?</v>
<v Speaker 1>Well,</v>

148
00:09:39.341 --> 00:09:44.341
<v Speaker 1>there's three key points here.</v>
<v Speaker 1>The first is that data has become</v>

149
00:09:46.320 --> 00:09:49.770
<v Speaker 1>much more pervasive.</v>
<v Speaker 1>We're living in a big data environment.</v>

150
00:09:49.800 --> 00:09:50.633
<v Speaker 1>These algorithms are hungry for more and</v>
<v Speaker 1>more data and accessing that data has </v>

151
00:09:55.891 --> 00:09:59.280
<v Speaker 1>become easier than ever before.</v>
<v Speaker 1>Second,</v>

152
00:09:59.340 --> 00:10:01.400
<v Speaker 1>these algorithms are massively parallel,</v>
<v Speaker 1>likable,</v>

153
00:10:01.440 --> 00:10:02.273
<v Speaker 1>and can benefit tremendously from modern</v>
<v Speaker 1>gpu architectures that simply just did </v>

154
00:10:07.531 --> 00:10:10.710
<v Speaker 1>not exist just less than more than a </v>
<v Speaker 1>decade ago.</v>

155
00:10:11.520 --> 00:10:12.353
<v Speaker 1>And finally,</v>
<v Speaker 1>due to open source tool boxes like </v>

156
00:10:14.040 --> 00:10:14.873
<v Speaker 1>tensorflow,</v>
<v Speaker 1>building and deploying these algorithms </v>

157
00:10:18.150 --> 00:10:18.983
<v Speaker 1>has become so streamlined,</v>
<v Speaker 1>so simple that we can teach it in a one </v>

158
00:10:21.511 --> 00:10:25.710
<v Speaker 1>week course like this,</v>
<v Speaker 1>and it's become extremely deployable for</v>

159
00:10:25.711 --> 00:10:26.544
<v Speaker 1>the massive public.</v>
<v Speaker 1>So let's start with now looking at the </v>

160
00:10:30.781 --> 00:10:33.000
<v Speaker 1>fundamental building block of deep </v>
<v Speaker 1>learning.</v>

161
00:10:33.001 --> 00:10:36.900
<v Speaker 1>And that's the perceptron.</v>
<v Speaker 1>This is really just a single neuron in a</v>

162
00:10:36.901 --> 00:10:41.030
<v Speaker 1>neural network.</v>
<v Speaker 1>So yeah,</v>

163
00:10:41.040 --> 00:10:45.640
<v Speaker 1>I do have a perceptron or a single </v>
<v Speaker 1>neuron is extremely simple.</v>

164
00:10:46.090 --> 00:10:49.630
<v Speaker 1>Let's start by talking about the forward</v>
<v Speaker 1>propagation of information.</v>

165
00:10:49.660 --> 00:10:50.493
<v Speaker 1>Through this data unit,</v>
<v Speaker 1>we defined a set of inputs x one x m on </v>

166
00:10:54.881 --> 00:10:55.714
<v Speaker 1>the left,</v>
<v Speaker 1>and all we do is we multiply each of </v>

167
00:10:59.571 --> 00:11:02.780
<v Speaker 1>these inputs by their corresponding wait</v>
<v Speaker 1>theta one through,</v>

168
00:11:02.810 --> 00:11:04.670
<v Speaker 1>say the end,</v>
<v Speaker 1>which are those arrows.</v>

169
00:11:05.840 --> 00:11:06.673
<v Speaker 1>We take this weighted,</v>
<v Speaker 1>we will take this weighted combination </v>

170
00:11:10.310 --> 00:11:11.143
<v Speaker 1>of all of our inputs,</v>
<v Speaker 1>sum them up and pass them through a </v>

171
00:11:13.620 --> 00:11:15.470
<v Speaker 1>nonlinear activation function.</v>

172
00:11:17.260 --> 00:11:18.670
<v Speaker 1>And that produces our output.</v>
<v Speaker 1>Why?</v>

173
00:11:18.730 --> 00:11:20.980
<v Speaker 1>It's that simple.</v>
<v Speaker 1>So we have m inputs,</v>

174
00:11:21.100 --> 00:11:21.933
<v Speaker 1>one output number,</v>
<v Speaker 1>and you can see a summarized on the </v>

175
00:11:25.960 --> 00:11:29.020
<v Speaker 1>righthand side as a mathematics single </v>
<v Speaker 1>mathematical equation.</v>

176
00:11:30.490 --> 00:11:34.990
<v Speaker 1>But actually I left out one important </v>
<v Speaker 1>detail that makes the previous slide not</v>

177
00:11:34.991 --> 00:11:35.824
<v Speaker 1>exactly correct,</v>
<v Speaker 1>so I left that this notion of a bias </v>

178
00:11:39.400 --> 00:11:40.460
<v Speaker 1>biases,</v>
<v Speaker 1>uh,</v>

179
00:11:40.910 --> 00:11:41.743
<v Speaker 1>that green term you see on the left,</v>
<v Speaker 1>and this just represents some way that </v>

180
00:11:44.651 --> 00:11:45.484
<v Speaker 1>we can allow our model to learn or we </v>
<v Speaker 1>can allow our activation function to </v>

181
00:11:49.391 --> 00:11:52.000
<v Speaker 1>shift to the left or right.</v>
<v Speaker 1>So it allows,</v>

182
00:11:52.330 --> 00:11:54.310
<v Speaker 1>if we provide,</v>
<v Speaker 1>it allows us to,</v>

183
00:11:54.311 --> 00:11:55.144
<v Speaker 1>um,</v>
<v Speaker 1>when we have no input features to still </v>

184
00:11:57.311 --> 00:12:02.311
<v Speaker 1>provide a positive output.</v>
<v Speaker 1>So on this equation,</v>

185
00:12:02.941 --> 00:12:03.774
<v Speaker 1>on the right,</v>
<v Speaker 1>we can actually rewrite this using </v>

186
00:12:06.151 --> 00:12:10.470
<v Speaker 1>linear Algebra and dod products to make </v>
<v Speaker 1>this a lot cleaner.</v>

187
00:12:11.310 --> 00:12:12.143
<v Speaker 1>So let's do that.</v>
<v Speaker 1>Let's say x capital x is a vector </v>

188
00:12:16.741 --> 00:12:18.460
<v Speaker 1>containing all of our inputs.</v>
<v Speaker 1>X One,</v>

189
00:12:18.470 --> 00:12:19.303
<v Speaker 1>three,</v>
<v Speaker 1>Xn capitol theater is now just a vector </v>

190
00:12:23.401 --> 00:12:25.860
<v Speaker 1>containing all of our status.</v>
<v Speaker 1>They don't want to say to him,</v>

191
00:12:27.120 --> 00:12:27.953
<v Speaker 1>we can rewrite that equation that we had</v>
<v Speaker 1>before is just applying a dot product </v>

192
00:12:30.391 --> 00:12:33.960
<v Speaker 1>between x and data.</v>
<v Speaker 1>Adding our bias fate is zero.</v>

193
00:12:34.860 --> 00:12:36.570
<v Speaker 1>And applying our nonlinear u,</v>
<v Speaker 1>G,</v>

194
00:12:36.720 --> 00:12:37.553
<v Speaker 1>g.</v>

195
00:12:40.180 --> 00:12:43.200
<v Speaker 1>Now you might be wondering,</v>
<v Speaker 1>since I've mentioned this a couple times</v>

196
00:12:43.201 --> 00:12:46.380
<v Speaker 1>now,</v>
<v Speaker 1>what is this nonlinear function g?</v>

197
00:12:47.100 --> 00:12:49.080
<v Speaker 1>Well,</v>
<v Speaker 1>I said it's the activation function,</v>

198
00:12:49.170 --> 00:12:49.770
<v Speaker 1>but,</v>
<v Speaker 1>uh,</v>

199
00:12:49.770 --> 00:12:53.580
<v Speaker 1>let's see an example of what in practice</v>
<v Speaker 1>ge actually could be.</v>

200
00:12:54.800 --> 00:12:58.050
<v Speaker 1>So one very popular activation function </v>
<v Speaker 1>is the sigmoid function.</v>

201
00:12:58.410 --> 00:13:00.990
<v Speaker 1>You can see a plot of it here on the </v>
<v Speaker 1>bottom right,</v>

202
00:13:02.250 --> 00:13:04.230
<v Speaker 1>and this is a function that takes its </v>
<v Speaker 1>input.</v>

203
00:13:05.070 --> 00:13:05.903
<v Speaker 1>Any real number on the x axis and </v>
<v Speaker 1>transforms it into an output between </v>

204
00:13:09.031 --> 00:13:11.620
<v Speaker 1>zero and one,</v>
<v Speaker 1>because I'll,</v>

205
00:13:11.621 --> 00:13:13.410
<v Speaker 1>I'll put to this function are between </v>
<v Speaker 1>zero and one.</v>

206
00:13:13.411 --> 00:13:14.244
<v Speaker 1>It makes it a very popular choice in </v>
<v Speaker 1>deep learning to represent </v>

207
00:13:16.621 --> 00:13:17.520
<v Speaker 1>probabilities.</v>

208
00:13:20.410 --> 00:13:21.243
<v Speaker 1>In fact,</v>
<v Speaker 1>there are many types of nonlinear </v>

209
00:13:22.930 --> 00:13:25.060
<v Speaker 1>activation functions in deep neural </v>
<v Speaker 1>networks.</v>

210
00:13:25.360 --> 00:13:28.510
<v Speaker 1>And here's some of the common ones </v>
<v Speaker 1>throughout this presentation.</v>

211
00:13:28.511 --> 00:13:29.344
<v Speaker 1>You'll also see tensor flow code </v>
<v Speaker 1>snippets like the ones you see on the </v>

212
00:13:31.961 --> 00:13:32.794
<v Speaker 1>bottom here,</v>
<v Speaker 1>since we'll be using tensorflow for our </v>

213
00:13:33.971 --> 00:13:34.804
<v Speaker 1>labs.</v>
<v Speaker 1>And well this is some way that I can </v>

214
00:13:36.761 --> 00:13:37.594
<v Speaker 1>provide to you to kind of link the </v>
<v Speaker 1>material in our lectures with what </v>

215
00:13:40.181 --> 00:13:44.710
<v Speaker 1>you'll be implementing in labs.</v>
<v Speaker 1>So the sigmoid activation function,</v>

216
00:13:44.711 --> 00:13:46.210
<v Speaker 1>which I talked about in the previous </v>
<v Speaker 1>slide,</v>

217
00:13:46.390 --> 00:13:49.730
<v Speaker 1>now on the left,</v>
<v Speaker 1>is a.</v>

218
00:13:49.750 --> 00:13:50.950
<v Speaker 1>it's just a function,</v>
<v Speaker 1>like I said,</v>

219
00:13:50.951 --> 00:13:54.370
<v Speaker 1>it's commonly used to produce </v>
<v Speaker 1>probability outputs.</v>

220
00:13:54.700 --> 00:13:58.420
<v Speaker 1>Each of these activation functions has </v>
<v Speaker 1>their own advantages and disadvantages.</v>

221
00:13:58.740 --> 00:13:59.573
<v Speaker 1>Underwrite a very common activation </v>
<v Speaker 1>function is rectified linear unit or </v>

222
00:14:02.570 --> 00:14:03.403
<v Speaker 1>value.</v>
<v Speaker 1>This function is very popular because </v>

223
00:14:05.981 --> 00:14:08.950
<v Speaker 1>it's extremely simple to compute.</v>
<v Speaker 1>It's piecewise linear.</v>

224
00:14:08.951 --> 00:14:11.890
<v Speaker 1>It's zero before with inputs less than </v>
<v Speaker 1>zero,</v>

225
00:14:12.120 --> 00:14:17.120
<v Speaker 1>it's x with any input greater than zero,</v>
<v Speaker 1>and the gradients are zero or one with a</v>

226
00:14:18.281 --> 00:14:20.020
<v Speaker 1>single nonlinearity at the origin.</v>

227
00:14:22.260 --> 00:14:25.890
<v Speaker 1>And you might be wondering why we even </v>
<v Speaker 1>need activation functions.</v>

228
00:14:25.891 --> 00:14:29.310
<v Speaker 1>Why can't we just take our dod product </v>
<v Speaker 1>at our bias and that's our output.</v>

229
00:14:29.670 --> 00:14:30.503
<v Speaker 1>Why do we need the activation function?</v>
<v Speaker 1>Activation functions introduce </v>

230
00:14:33.570 --> 00:14:34.403
<v Speaker 1>nonlinearities into the network.</v>
<v Speaker 1>That's the whole point of why </v>

231
00:14:37.110 --> 00:14:37.943
<v Speaker 1>activations themselves are nonlinear.</v>
<v Speaker 1>We want to model nonlinear data in the </v>

232
00:14:44.101 --> 00:14:46.820
<v Speaker 1>world because the world is extremely </v>
<v Speaker 1>nonlinear.</v>

233
00:14:48.030 --> 00:14:51.320
<v Speaker 1>Let's suppose I gave you this,</v>
<v Speaker 1>this plot green and red points,</v>

234
00:14:51.321 --> 00:14:54.830
<v Speaker 1>and I asked you to draw a single line,</v>
<v Speaker 1>not a curve.</v>

235
00:14:54.831 --> 00:14:58.340
<v Speaker 1>Just align between the green and red </v>
<v Speaker 1>points to separate them perfectly.</v>

236
00:14:58.880 --> 00:14:59.713
<v Speaker 1>You would find this really difficult and</v>
<v Speaker 1>probably you could get as best as </v>

237
00:15:02.121 --> 00:15:03.860
<v Speaker 1>something like this.</v>
<v Speaker 1>Now,</v>

238
00:15:03.861 --> 00:15:08.360
<v Speaker 1>if your activation function and your </v>
<v Speaker 1>deep neural network was linear century,</v>

239
00:15:08.390 --> 00:15:10.730
<v Speaker 1>just composing linear functions with </v>
<v Speaker 1>linear functions,</v>

240
00:15:10.731 --> 00:15:11.564
<v Speaker 1>your output will always be linear,</v>
<v Speaker 1>so the most complicated deep neural </v>

241
00:15:14.121 --> 00:15:15.800
<v Speaker 1>network,</v>
<v Speaker 1>no matter how big or how deep,</v>

242
00:15:16.150 --> 00:15:20.240
<v Speaker 1>if the activation function is linear,</v>
<v Speaker 1>your output can only look like this,</v>

243
00:15:20.780 --> 00:15:25.550
<v Speaker 1>but once we introduced nonlinearities,</v>
<v Speaker 1>our network is extremely more.</v>

244
00:15:25.730 --> 00:15:28.790
<v Speaker 1>As the capacity of our network has </v>
<v Speaker 1>extremely increased,</v>

245
00:15:28.970 --> 00:15:32.000
<v Speaker 1>we're now able to model much more </v>
<v Speaker 1>complex functions.</v>

246
00:15:32.270 --> 00:15:33.103
<v Speaker 1>We're able to draw decision boundaries </v>
<v Speaker 1>that were not possible with only linear </v>

247
00:15:36.171 --> 00:15:40.250
<v Speaker 1>activation functions.</v>
<v Speaker 1>Let's understand this with a very simple</v>

248
00:15:40.251 --> 00:15:41.084
<v Speaker 1>example.</v>
<v Speaker 1>Imagine I gave you a train network like </v>

249
00:15:43.221 --> 00:15:44.450
<v Speaker 1>the one we saw before.</v>
<v Speaker 1>Sorry,</v>

250
00:15:44.451 --> 00:15:45.284
<v Speaker 1>I trained perceptron not in network yet.</v>
<v Speaker 1>Just a single node and the weights are </v>

251
00:15:49.731 --> 00:15:50.451
<v Speaker 1>on the top,</v>
<v Speaker 1>right,</v>

252
00:15:50.451 --> 00:15:51.284
<v Speaker 1>so status zero is one and the state of </v>
<v Speaker 1>vector is three and negative to the </v>

253
00:15:56.451 --> 00:15:59.330
<v Speaker 1>network has two inputs,</v>
<v Speaker 1>x one and x two,</v>

254
00:15:59.420 --> 00:16:00.253
<v Speaker 1>and if we want to get the output,</v>
<v Speaker 1>all we have to do is apply the same </v>

255
00:16:02.811 --> 00:16:06.590
<v Speaker 1>story as before.</v>
<v Speaker 1>So we apply the product of x and Theta,</v>

256
00:16:07.010 --> 00:16:09.830
<v Speaker 1>we add the bias and apply or </v>
<v Speaker 1>nonlinearity,</v>

257
00:16:10.520 --> 00:16:12.200
<v Speaker 1>but let's take a look at what's actually</v>
<v Speaker 1>inside.</v>

258
00:16:12.201 --> 00:16:14.880
<v Speaker 1>Before we apply that nonlinearity,</v>
<v Speaker 1>this little,</v>

259
00:16:14.910 --> 00:16:19.910
<v Speaker 1>it looks a lot like just a two d line </v>
<v Speaker 1>because we have two inputs and it is.</v>

260
00:16:20.570 --> 00:16:24.110
<v Speaker 1>We can actually plot this line way </v>
<v Speaker 1>equals zero in feature space.</v>

261
00:16:24.111 --> 00:16:26.660
<v Speaker 1>So this is space where I'm plotting x </v>
<v Speaker 1>one,</v>

262
00:16:26.750 --> 00:16:31.750
<v Speaker 1>one of our features on the x axis and x </v>
<v Speaker 1>to the other feature.</v>

263
00:16:32.151 --> 00:16:34.190
<v Speaker 1>On the y axis,</v>
<v Speaker 1>we plot that line.</v>

264
00:16:34.220 --> 00:16:38.930
<v Speaker 1>It's just a decision boundary separating</v>
<v Speaker 1>our entire space into two sub spaces.</v>

265
00:16:40.220 --> 00:16:41.570
<v Speaker 1>Now,</v>
<v Speaker 1>if I give you a new point,</v>

266
00:16:41.990 --> 00:16:43.040
<v Speaker 1>negative one,</v>
<v Speaker 1>two,</v>

267
00:16:43.550 --> 00:16:45.860
<v Speaker 1>and plotted on the sub and this feature </v>
<v Speaker 1>space,</v>

268
00:16:46.160 --> 00:16:48.140
<v Speaker 1>depending on which side of the line of </v>
<v Speaker 1>false on,</v>

269
00:16:48.380 --> 00:16:49.213
<v Speaker 1>I can automatically determine whether </v>
<v Speaker 1>our output is less than zero or greater </v>

270
00:16:52.161 --> 00:16:52.994
<v Speaker 1>than zero.</v>
<v Speaker 1>Since our line represents a decision </v>

271
00:16:54.591 --> 00:16:59.180
<v Speaker 1>boundary equal to zero,</v>
<v Speaker 1>now we can follow the mass on the bottom</v>

272
00:16:59.181 --> 00:17:02.690
<v Speaker 1>and see that computing the inside of </v>
<v Speaker 1>this activation function.</v>

273
00:17:02.810 --> 00:17:07.810
<v Speaker 1>We get one minus three minus two,</v>
<v Speaker 1>sorry minus four,</v>

274
00:17:10.610 --> 00:17:11.443
<v Speaker 1>and we get minus six at the output </v>
<v Speaker 1>before we applied the activation </v>

275
00:17:14.601 --> 00:17:14.960
<v Speaker 1>function.</v>

276
00:17:14.960 --> 00:17:18.080
<v Speaker 1>Once we apply the activation function,</v>
<v Speaker 1>we get zero point zero,</v>

277
00:17:18.081 --> 00:17:18.914
<v Speaker 1>$0 to so it negative what was applied to</v>
<v Speaker 1>the activation functionalists negative </v>

278
00:17:23.841 --> 00:17:27.890
<v Speaker 1>because we fell on the negative piece of</v>
<v Speaker 1>this sub space.</v>

279
00:17:29.870 --> 00:17:31.520
<v Speaker 1>Well,</v>
<v Speaker 1>if we remember with the sigmoid function</v>

280
00:17:31.521 --> 00:17:32.354
<v Speaker 1>actually defines our space into two </v>
<v Speaker 1>parts greater than one point five and </v>

281
00:17:35.361 --> 00:17:36.194
<v Speaker 1>less than point five.</v>
<v Speaker 1>Since we're modeling probabilities and </v>

282
00:17:37.641 --> 00:17:38.474
<v Speaker 1>everything is between zero and one.</v>
<v Speaker 1>So actually our decision boundary where </v>

283
00:17:42.380 --> 00:17:45.710
<v Speaker 1>the input to our network equals zero.</v>
<v Speaker 1>Sorry,</v>

284
00:17:45.711 --> 00:17:46.340
<v Speaker 1>the.</v>
<v Speaker 1>Sorry.</v>

285
00:17:46.340 --> 00:17:48.530
<v Speaker 1>The input to our activation function </v>
<v Speaker 1>equals zero,</v>

286
00:17:49.200 --> 00:17:50.033
<v Speaker 1>corresponds to the output of our </v>
<v Speaker 1>activation function being greater than </v>

287
00:17:53.371 --> 00:17:54.204
<v Speaker 1>or less than point five.</v>
<v Speaker 1>So now that we have an idea of what a </v>

288
00:17:59.011 --> 00:17:59.844
<v Speaker 1>perceptron is,</v>
<v Speaker 1>let's just start out by understanding </v>

289
00:18:02.431 --> 00:18:03.264
<v Speaker 1>how we can compose these perceptrons </v>
<v Speaker 1>together to actually build neural </v>

290
00:18:07.531 --> 00:18:08.364
<v Speaker 1>networks.</v>
<v Speaker 1>And let's see how this all comes </v>

291
00:18:10.731 --> 00:18:12.740
<v Speaker 1>together.</v>
<v Speaker 1>So let's revisit our previous diagram up</v>

292
00:18:12.741 --> 00:18:17.300
<v Speaker 1>the perceptron know if there's a few </v>
<v Speaker 1>things that you learned from this class,</v>

293
00:18:17.330 --> 00:18:20.450
<v Speaker 1>let this be one of them and we'll keep </v>
<v Speaker 1>repeating it over and over.</v>

294
00:18:21.410 --> 00:18:24.080
<v Speaker 1>In deep learning,</v>
<v Speaker 1>you do a dot product,</v>

295
00:18:24.230 --> 00:18:26.840
<v Speaker 1>you apply a bias and you add your non </v>
<v Speaker 1>linearity.</v>

296
00:18:27.290 --> 00:18:30.740
<v Speaker 1>You keep repeating that many,</v>
<v Speaker 1>many times for each node,</v>

297
00:18:30.770 --> 00:18:34.400
<v Speaker 1>each neuron in your neural network,</v>
<v Speaker 1>and that's a neural network.</v>

298
00:18:36.090 --> 00:18:40.200
<v Speaker 1>So let's simplify this diagram a little.</v>
<v Speaker 1>I removed the bias since we are going to</v>

299
00:18:40.201 --> 00:18:42.930
<v Speaker 1>always have that and we just take her </v>
<v Speaker 1>for granted from Nolan,</v>

300
00:18:43.290 --> 00:18:44.123
<v Speaker 1>I'll remove all of the weight labels for</v>
<v Speaker 1>simplicity and note that z is just the </v>

301
00:18:50.131 --> 00:18:50.964
<v Speaker 1>input to our activation function.</v>
<v Speaker 1>So that's just the dot product plus our </v>

302
00:18:54.721 --> 00:18:57.360
<v Speaker 1>bias.</v>
<v Speaker 1>If we want the output of the network,</v>

303
00:18:57.361 --> 00:18:58.194
<v Speaker 1>why we simply take z and we apply or </v>
<v Speaker 1>nonlinearity like before we wanted to </v>

304
00:19:03.081 --> 00:19:06.070
<v Speaker 1>find a multi output perceptron.</v>
<v Speaker 1>It's very simple.</v>

305
00:19:06.071 --> 00:19:08.950
<v Speaker 1>We just added another perceptron.</v>
<v Speaker 1>Now we have two outputs.</v>

306
00:19:08.951 --> 00:19:13.360
<v Speaker 1>Y One and y two.</v>
<v Speaker 1>Each one has weight matrices,</v>

307
00:19:13.450 --> 00:19:18.220
<v Speaker 1>has weight factor theta corresponding to</v>
<v Speaker 1>the weight of each of the inputs.</v>

308
00:19:22.820 --> 00:19:25.880
<v Speaker 1>Now let's suppose we want to go the next</v>
<v Speaker 1>step deeper.</v>

309
00:19:26.900 --> 00:19:29.750
<v Speaker 1>We want to create now a single layer </v>
<v Speaker 1>neural network.</v>

310
00:19:31.400 --> 00:19:34.310
<v Speaker 1>Single layered neural networks are </v>
<v Speaker 1>actually not deep networks yet.</v>

311
00:19:34.311 --> 00:19:36.920
<v Speaker 1>They're only.</v>
<v Speaker 1>They're still shallow networks.</v>

312
00:19:36.921 --> 00:19:37.754
<v Speaker 1>There are only one layer deep,</v>
<v Speaker 1>but let's look at the single layer </v>

313
00:19:40.720 --> 00:19:45.200
<v Speaker 1>neural network where now all we do is we</v>
<v Speaker 1>have one hidden layer between our inputs</v>

314
00:19:45.201 --> 00:19:46.034
<v Speaker 1>and outputs.</v>
<v Speaker 1>We call this hidden layer because it's </v>

315
00:19:49.280 --> 00:19:54.280
<v Speaker 1>states are not directly observable.</v>
<v Speaker 1>They're not directly forced by by the AI</v>

316
00:19:55.161 --> 00:19:58.610
<v Speaker 1>designer.</v>
<v Speaker 1>We only enforce the inputs and outputs.</v>

317
00:19:58.640 --> 00:19:59.473
<v Speaker 1>Typically the states in the middle are </v>
<v Speaker 1>hidden and since we now have a </v>

318
00:20:03.951 --> 00:20:04.784
<v Speaker 1>transformation to go from our intimate </v>
<v Speaker 1>space to our hidden hidden layer space </v>

319
00:20:08.210 --> 00:20:11.030
<v Speaker 1>and from our hidden layer space to our </v>
<v Speaker 1>output layer space,</v>

320
00:20:11.720 --> 00:20:14.260
<v Speaker 1>we actually need to weight matrices,</v>
<v Speaker 1>data,</v>

321
00:20:14.280 --> 00:20:18.560
<v Speaker 1>one end data to corresponding to the </v>
<v Speaker 1>weight matrices of each layer.</v>

322
00:20:21.200 --> 00:20:23.300
<v Speaker 1>Now,</v>
<v Speaker 1>if we look at just a single unit in that</v>

323
00:20:23.301 --> 00:20:25.790
<v Speaker 1>hidden layer,</v>
<v Speaker 1>it's the exact same story as before.</v>

324
00:20:25.791 --> 00:20:26.624
<v Speaker 1>It's one perceptron.</v>
<v Speaker 1>We take it stopped product with all of </v>

325
00:20:28.911 --> 00:20:33.380
<v Speaker 1>the exits that came before it.</v>
<v Speaker 1>And we apply A.</v>

326
00:20:33.380 --> 00:20:34.780
<v Speaker 1>Sorry,</v>
<v Speaker 1>we take the dot product of the exes that</v>

327
00:20:34.781 --> 00:20:36.780
<v Speaker 1>came before with the weight matrices.</v>
<v Speaker 1>Fate,</v>

328
00:20:36.781 --> 00:20:38.540
<v Speaker 1>is that a one?</v>
<v Speaker 1>In this case,</v>

329
00:20:39.370 --> 00:20:40.203
<v Speaker 1>we apply a bias to get [inaudible].</v>
<v Speaker 1>And if we were to look at a different </v>

330
00:20:43.790 --> 00:20:45.140
<v Speaker 1>hidden unit,</v>
<v Speaker 1>let's say z three,</v>

331
00:20:45.141 --> 00:20:48.940
<v Speaker 1>instead we would just take different </v>
<v Speaker 1>weight matrices,</v>

332
00:20:48.970 --> 00:20:50.070
<v Speaker 1>different,</v>
<v Speaker 1>uh,</v>

333
00:20:50.440 --> 00:20:53.110
<v Speaker 1>our dod product would change,</v>
<v Speaker 1>our bias would change,</v>

334
00:20:53.380 --> 00:20:55.420
<v Speaker 1>but it does,</v>
<v Speaker 1>I see change,</v>

335
00:20:55.480 --> 00:20:57.520
<v Speaker 1>which means it's activation would also </v>
<v Speaker 1>be different.</v>

336
00:20:58.780 --> 00:20:59.613
<v Speaker 1>So from now on,</v>
<v Speaker 1>I'm going to use this symbol to denote </v>

337
00:21:02.350 --> 00:21:04.180
<v Speaker 1>what is called as a fully connected </v>
<v Speaker 1>layer.</v>

338
00:21:04.181 --> 00:21:05.650
<v Speaker 1>And that's what we've been talking about</v>
<v Speaker 1>so far,</v>

339
00:21:05.651 --> 00:21:06.484
<v Speaker 1>so that's every note in one layer is </v>
<v Speaker 1>connected to every node and another </v>

340
00:21:09.491 --> 00:21:11.500
<v Speaker 1>layer by these weight matrices.</v>

341
00:21:12.100 --> 00:21:12.933
<v Speaker 1>And this is really just for simplicity,</v>
<v Speaker 1>so I don't have to keep redrawing those </v>

342
00:21:15.041 --> 00:21:17.320
<v Speaker 1>lines.</v>
<v Speaker 1>Now,</v>

343
00:21:17.321 --> 00:21:19.210
<v Speaker 1>if we want to create a deep neural </v>
<v Speaker 1>network,</v>

344
00:21:20.050 --> 00:21:20.883
<v Speaker 1>all we do is keep stalking these layers </v>
<v Speaker 1>and fully connected weights between the </v>

345
00:21:24.521 --> 00:21:25.930
<v Speaker 1>layers.</v>
<v Speaker 1>It's that simple.</v>

346
00:21:26.200 --> 00:21:27.033
<v Speaker 1>But the underlying building block is </v>
<v Speaker 1>that single perceptron said single dod </v>

347
00:21:30.461 --> 00:21:33.220
<v Speaker 1>product nonlinearity and bias.</v>
<v Speaker 1>That's it.</v>

348
00:21:34.810 --> 00:21:38.770
<v Speaker 1>So this is really incredible because </v>
<v Speaker 1>something so simple at the foundation is</v>

349
00:21:39.010 --> 00:21:41.890
<v Speaker 1>able to create such incredible </v>
<v Speaker 1>algorithms.</v>

350
00:21:42.460 --> 00:21:43.293
<v Speaker 1>And now let's see an example of how we </v>
<v Speaker 1>can actually apply neural networks to a </v>

351
00:21:46.660 --> 00:21:51.370
<v Speaker 1>very important question that I know you </v>
<v Speaker 1>are all extremely worried about.</v>

352
00:21:51.400 --> 00:21:55.000
<v Speaker 1>You care a lot about.</v>
<v Speaker 1>Here's the question you want to build an</v>

353
00:21:55.001 --> 00:21:58.090
<v Speaker 1>ai system that answers the following </v>
<v Speaker 1>question.</v>

354
00:21:58.120 --> 00:22:00.400
<v Speaker 1>Will I pass this class?</v>
<v Speaker 1>Yes or no?</v>

355
00:22:00.580 --> 00:22:01.413
<v Speaker 1>One or zero is the output to do this.</v>
<v Speaker 1>Let's start by defining a simple to </v>

356
00:22:06.581 --> 00:22:07.450
<v Speaker 1>feature model.</v>

357
00:22:07.600 --> 00:22:10.180
<v Speaker 1>One feature is the number of lectures </v>
<v Speaker 1>that you attend.</v>

358
00:22:10.710 --> 00:22:13.810
<v Speaker 1>Second feature is the number of hours </v>
<v Speaker 1>that you spend on your final project.</v>

359
00:22:15.460 --> 00:22:18.010
<v Speaker 1>Let's plot this data.</v>
<v Speaker 1>In our feature space.</v>

360
00:22:18.440 --> 00:22:20.740
<v Speaker 1>We apply greenpoint's of people who </v>
<v Speaker 1>pass.</v>

361
00:22:20.950 --> 00:22:23.800
<v Speaker 1>Red Points are people that fail.</v>
<v Speaker 1>We want to know,</v>

362
00:22:23.801 --> 00:22:25.210
<v Speaker 1>given a new person,</v>
<v Speaker 1>this guy,</v>

363
00:22:26.800 --> 00:22:27.633
<v Speaker 1>he spent a day,</v>
<v Speaker 1>spent five hours on their final project </v>

364
00:22:32.290 --> 00:22:33.123
<v Speaker 1>and went to four lectures.</v>
<v Speaker 1>We want to know did that person pass or </v>

365
00:22:36.671 --> 00:22:39.700
<v Speaker 1>fail the class and we want to build a </v>
<v Speaker 1>neural network that will determine this.</v>

366
00:22:40.600 --> 00:22:43.870
<v Speaker 1>So let's do it.</v>
<v Speaker 1>We have two inputs.</v>

367
00:22:43.930 --> 00:22:45.160
<v Speaker 1>One is for the others.</v>
<v Speaker 1>Five,</v>

368
00:22:45.670 --> 00:22:46.503
<v Speaker 1>we have one hidden layer with three </v>
<v Speaker 1>units and we want to see the final </v>

369
00:22:49.241 --> 00:22:51.190
<v Speaker 1>output probability of passing this </v>
<v Speaker 1>class,</v>

370
00:22:51.191 --> 00:22:54.220
<v Speaker 1>and we computed as zero point one or 10 </v>
<v Speaker 1>percent.</v>

371
00:22:55.270 --> 00:22:58.600
<v Speaker 1>That's really bad news because actually </v>
<v Speaker 1>this person did pass the class.</v>

372
00:22:58.960 --> 00:23:02.510
<v Speaker 1>They passed it with probability one.</v>
<v Speaker 1>Nope.</v>

373
00:23:02.540 --> 00:23:05.390
<v Speaker 1>Can anyone tell me why the neural </v>
<v Speaker 1>network got this?</v>

374
00:23:05.680 --> 00:23:07.730
<v Speaker 1>Such so wrong?</v>
<v Speaker 1>Why did it do this?</v>

375
00:23:08.510 --> 00:23:09.343
<v Speaker 1>Yeah,</v>
<v Speaker 1>it is.</v>

376
00:23:11.570 --> 00:23:12.403
<v Speaker 2>Exactly.</v>

377
00:23:12.780 --> 00:23:16.140
<v Speaker 1>So this network has never been trained.</v>
<v Speaker 1>It's never seen any data.</v>

378
00:23:16.800 --> 00:23:19.500
<v Speaker 1>It's basically a baby.</v>
<v Speaker 1>It's never learned anything.</v>

379
00:23:19.890 --> 00:23:22.620
<v Speaker 1>So we can't expect it to solve a problem</v>
<v Speaker 1>and knows nothing about.</v>

380
00:23:23.400 --> 00:23:24.233
<v Speaker 1>So to do this,</v>
<v Speaker 1>to tackle this problem with training a </v>

381
00:23:26.611 --> 00:23:28.920
<v Speaker 1>neural network,</v>
<v Speaker 1>we have to first define a couple things.</v>

382
00:23:28.921 --> 00:23:29.754
<v Speaker 1>So first we'll talk about the loss.</v>
<v Speaker 1>The loss of a network basically tells </v>

383
00:23:36.150 --> 00:23:41.150
<v Speaker 1>our algorithm or our model how wrong our</v>
<v Speaker 1>predictions are from the ground truth.</v>

384
00:23:43.830 --> 00:23:44.663
<v Speaker 1>So you can think of this as a between </v>
<v Speaker 1>our predicted output and our actual </v>

385
00:23:48.501 --> 00:23:50.540
<v Speaker 1>output.</v>
<v Speaker 1>If the two are very close,</v>

386
00:23:50.541 --> 00:23:53.270
<v Speaker 1>if we predict something that is very </v>
<v Speaker 1>close to the true output,</v>

387
00:23:53.510 --> 00:23:57.860
<v Speaker 1>our loss is very low.</v>
<v Speaker 1>If we predict something that is very far</v>

388
00:23:58.520 --> 00:24:01.010
<v Speaker 1>in a high level sense,</v>
<v Speaker 1>far like in distance,</v>

389
00:24:02.120 --> 00:24:02.953
<v Speaker 1>then our loss is very high and we want </v>
<v Speaker 1>to minimize this from happening as much </v>

390
00:24:05.901 --> 00:24:06.734
<v Speaker 1>as possible.</v>

391
00:24:07.760 --> 00:24:08.593
<v Speaker 1>Now,</v>
<v Speaker 1>let's assume we were not given just one </v>

392
00:24:09.201 --> 00:24:10.034
<v Speaker 1>data point one student,</v>
<v Speaker 1>but we're given a whole class of </v>

393
00:24:11.631 --> 00:24:13.550
<v Speaker 1>students.</v>
<v Speaker 1>So as previous data,</v>

394
00:24:13.551 --> 00:24:17.690
<v Speaker 1>I use this entire class from last year </v>
<v Speaker 1>and if we want to quantify what's called</v>

395
00:24:17.691 --> 00:24:18.524
<v Speaker 1>the empirical loss,</v>
<v Speaker 1>now we care about how the model did on </v>

396
00:24:21.531 --> 00:24:24.530
<v Speaker 1>average over the entire Dataset.</v>
<v Speaker 1>Not for just a single student,</v>

397
00:24:24.560 --> 00:24:26.960
<v Speaker 1>but across the entire data set and how </v>
<v Speaker 1>we do that is very simple.</v>

398
00:24:26.961 --> 00:24:29.510
<v Speaker 1>We just take the average of the loss of </v>
<v Speaker 1>each data point.</v>

399
00:24:30.040 --> 00:24:33.560
<v Speaker 1>If we have end students,</v>
<v Speaker 1>it's the average over end data points.</v>

400
00:24:34.790 --> 00:24:36.680
<v Speaker 1>This has other names besides empirical </v>
<v Speaker 1>last.</v>

401
00:24:36.681 --> 00:24:38.660
<v Speaker 1>Sometimes people call it the objective </v>
<v Speaker 1>function,</v>

402
00:24:38.900 --> 00:24:40.480
<v Speaker 1>the cost function,</v>
<v Speaker 1>et cetera.</v>

403
00:24:42.110 --> 00:24:44.030
<v Speaker 1>All of these terms are completely the </v>
<v Speaker 1>same thing.</v>

404
00:24:45.770 --> 00:24:46.603
<v Speaker 1>Now,</v>
<v Speaker 1>if we look at the problem with binary </v>

405
00:24:47.241 --> 00:24:48.074
<v Speaker 1>classification,</v>
<v Speaker 1>predicting if you pass or fail this </v>

406
00:24:49.551 --> 00:24:50.720
<v Speaker 1>class,</v>
<v Speaker 1>yes or no,</v>

407
00:24:50.750 --> 00:24:51.583
<v Speaker 1>one or zero,</v>
<v Speaker 1>we can actually use something that's </v>

408
00:24:54.591 --> 00:24:56.990
<v Speaker 1>called the softmax cross entropy loss.</v>

409
00:24:58.340 --> 00:24:59.173
<v Speaker 1>Now,</v>
<v Speaker 1>for those of you who aren't familiar </v>

410
00:24:59.811 --> 00:25:00.644
<v Speaker 1>with cross entropy or entropy,</v>
<v Speaker 1>this is an extremely powerful notion </v>

411
00:25:04.491 --> 00:25:05.324
<v Speaker 1>that was actually developed,</v>
<v Speaker 1>were first introduced here at mit over </v>

412
00:25:08.541 --> 00:25:12.880
<v Speaker 1>50 years ago by Claude Shannon.</v>
<v Speaker 1>Uh,</v>

413
00:25:12.890 --> 00:25:14.990
<v Speaker 1>in his master's thesis.</v>
<v Speaker 1>Like I said,</v>

414
00:25:14.991 --> 00:25:15.824
<v Speaker 1>this was 50 years ago.</v>
<v Speaker 1>It's huge in the field of signal </v>

415
00:25:18.471 --> 00:25:19.304
<v Speaker 1>processing thermodynamics.</v>
<v Speaker 1>Really all of our computer science had </v>

416
00:25:21.821 --> 00:25:24.980
<v Speaker 1>seen in information theory.</v>
<v Speaker 1>Now,</v>

417
00:25:25.040 --> 00:25:27.950
<v Speaker 1>instead of predicting a single one or </v>
<v Speaker 1>zero output,</v>

418
00:25:27.951 --> 00:25:30.290
<v Speaker 1>yes or no,</v>
<v Speaker 1>let's suppose we want to predict a,</v>

419
00:25:30.320 --> 00:25:32.630
<v Speaker 1>um,</v>
<v Speaker 1>continuous valued function,</v>

420
00:25:32.930 --> 00:25:33.763
<v Speaker 1>not will I pass this class,</v>
<v Speaker 1>but what's the grade that I will get it </v>

421
00:25:37.251 --> 00:25:38.084
<v Speaker 1>as a percentage,</v>
<v Speaker 1>let's say zero to 100 now we're no </v>

422
00:25:39.501 --> 00:25:40.334
<v Speaker 1>longer limited to zero to one,</v>
<v Speaker 1>but can actually output any real number </v>

423
00:25:43.311 --> 00:25:47.030
<v Speaker 1>on the number line.</v>
<v Speaker 1>Now instead of using cross entropy,</v>

424
00:25:47.040 --> 00:25:49.370
<v Speaker 1>we might want to use a different loss.</v>
<v Speaker 1>And for this,</v>

425
00:25:49.371 --> 00:25:50.204
<v Speaker 1>let's think of something like a mean </v>
<v Speaker 1>squared error loss where as your </v>

426
00:25:52.401 --> 00:25:55.220
<v Speaker 1>predicted and you're true output </v>
<v Speaker 1>diverged from each other,</v>

427
00:25:55.700 --> 00:25:58.280
<v Speaker 1>the loss increases as a quadratic </v>
<v Speaker 1>function.</v>

428
00:26:01.140 --> 00:26:01.973
<v Speaker 1>Okay,</v>
<v Speaker 1>great.</v>

429
00:26:02.010 --> 00:26:02.843
<v Speaker 1>So now let's put this new loss </v>
<v Speaker 1>information to the test and actually </v>

430
00:26:06.061 --> 00:26:09.810
<v Speaker 1>learn how we can train a neural network </v>
<v Speaker 1>like quantifying it's lost.</v>

431
00:26:12.640 --> 00:26:16.840
<v Speaker 1>And really if we go back to what the </v>
<v Speaker 1>loss is at the very high level,</v>

432
00:26:16.870 --> 00:26:20.020
<v Speaker 1>the last tells us how the network is </v>
<v Speaker 1>performing,</v>

433
00:26:20.021 --> 00:26:20.854
<v Speaker 1>right?</v>
<v Speaker 1>Did last tells us the accuracy of the </v>

434
00:26:22.871 --> 00:26:23.704
<v Speaker 1>network on a set of examples.</v>
<v Speaker 1>And what we want to do is basically </v>

435
00:26:26.291 --> 00:26:29.740
<v Speaker 1>minimize the loss over our entire </v>
<v Speaker 1>training set.</v>

436
00:26:31.540 --> 00:26:32.373
<v Speaker 1>Really,</v>
<v Speaker 1>we want to find the set of parameters </v>

437
00:26:34.571 --> 00:26:39.571
<v Speaker 1>data such that that loss jff data,</v>
<v Speaker 1>that's our empirical loss is minimum.</v>

438
00:26:40.900 --> 00:26:45.720
<v Speaker 1>So remember jff data takes us data and </v>
<v Speaker 1>data's just our weights.</v>

439
00:26:45.780 --> 00:26:48.150
<v Speaker 1>So these are the things that actually </v>
<v Speaker 1>define our network.</v>

440
00:26:53.660 --> 00:26:57.710
<v Speaker 1>Remember that the last is just a </v>
<v Speaker 1>function of these weights.</v>

441
00:26:58.160 --> 00:27:01.520
<v Speaker 1>If we want to think about the process of</v>
<v Speaker 1>training,</v>

442
00:27:02.210 --> 00:27:05.000
<v Speaker 1>we can imagine this landscape.</v>
<v Speaker 1>So if we only have two weights,</v>

443
00:27:05.001 --> 00:27:05.834
<v Speaker 1>we can plot this nice diagram like this,</v>
<v Speaker 1>Beta zero and Beta one or two weights </v>

444
00:27:10.460 --> 00:27:12.320
<v Speaker 1>there on the floor.</v>
<v Speaker 1>They're on the planer.</v>

445
00:27:12.321 --> 00:27:14.920
<v Speaker 1>Access on the bottom,</v>
<v Speaker 1>JFK to zero,</v>

446
00:27:14.930 --> 00:27:18.650
<v Speaker 1>and three to one are plotted on the the </v>
<v Speaker 1>Z axis.</v>

447
00:27:20.050 --> 00:27:24.170
<v Speaker 1>What we want to do is basically find the</v>
<v Speaker 1>minimum of this loss of this landscape.</v>

448
00:27:24.320 --> 00:27:28.010
<v Speaker 1>If we can find the minimum,</v>
<v Speaker 1>then this tells us where our loss is,</v>

449
00:27:28.011 --> 00:27:30.750
<v Speaker 1>the smallest,</v>
<v Speaker 1>and this tells us where theater,</v>

450
00:27:31.120 --> 00:27:31.953
<v Speaker 1>where or what values of Beta zero and </v>
<v Speaker 1>Beta one we can use to attain that </v>

451
00:27:36.291 --> 00:27:40.720
<v Speaker 1>minimum loss.</v>
<v Speaker 1>So how do we do this?</v>

452
00:27:41.650 --> 00:27:45.100
<v Speaker 1>Well,</v>
<v Speaker 1>we start with a random guests.</v>

453
00:27:45.230 --> 00:27:47.410
<v Speaker 1>We'd pick a point say to zero state of </v>
<v Speaker 1>one,</v>

454
00:27:47.411 --> 00:27:48.244
<v Speaker 1>and we start there,</v>
<v Speaker 1>we compute the gradients of this point </v>

455
00:27:51.520 --> 00:27:54.340
<v Speaker 1>on the last landscape.</v>
<v Speaker 1>That's Dj de Seto.</v>

456
00:27:54.830 --> 00:27:58.570
<v Speaker 1>It's how the loss is changing with </v>
<v Speaker 1>respect to each of the weights.</v>

457
00:28:00.260 --> 00:28:04.280
<v Speaker 1>Now this gradient tells us the direction</v>
<v Speaker 1>of highest ascent,</v>

458
00:28:04.490 --> 00:28:05.323
<v Speaker 1>not decent.</v>
<v Speaker 1>So this is telling us the direction </v>

459
00:28:06.830 --> 00:28:08.660
<v Speaker 1>going towards the top of the mountain.</v>

460
00:28:10.120 --> 00:28:12.430
<v Speaker 1>So let's take a small step in the </v>
<v Speaker 1>opposite direction.</v>

461
00:28:12.910 --> 00:28:16.270
<v Speaker 1>So we negate our gradient and we adjust </v>
<v Speaker 1>our weight.</v>

462
00:28:16.271 --> 00:28:17.104
<v Speaker 1>So should we step into the opposite </v>
<v Speaker 1>direction of that gradient such that we </v>

463
00:28:19.781 --> 00:28:24.781
<v Speaker 1>move continuously towards the lowest </v>
<v Speaker 1>point in this landscape until we finally</v>

464
00:28:25.811 --> 00:28:28.240
<v Speaker 1>converged at a local minima.</v>
<v Speaker 1>And then we just stop.</v>

465
00:28:29.820 --> 00:28:31.620
<v Speaker 1>So let's summarize this with some pseudo</v>
<v Speaker 1>code.</v>

466
00:28:31.621 --> 00:28:35.850
<v Speaker 1>So we randomly initialized our weights.</v>
<v Speaker 1>We loop until convergence the following.</v>

467
00:28:36.450 --> 00:28:37.283
<v Speaker 1>We compute the gradient at that point,</v>
<v Speaker 1>and when simply we apply this update </v>

468
00:28:40.471 --> 00:28:45.471
<v Speaker 1>rule with the update takes as input the </v>
<v Speaker 1>negative gradient.</v>

469
00:28:48.800 --> 00:28:51.350
<v Speaker 1>Now let's look at this term here.</v>
<v Speaker 1>This is the gradient.</v>

470
00:28:51.410 --> 00:28:52.243
<v Speaker 1>Like I said,</v>
<v Speaker 1>it explains how the loss changes with </v>

471
00:28:54.561 --> 00:28:55.394
<v Speaker 1>respect to each weight and the network,</v>
<v Speaker 1>but I never actually told you how to </v>

472
00:28:58.701 --> 00:29:02.570
<v Speaker 1>compute this and this is actually a big,</v>
<v Speaker 1>a big issue in neural networks.</v>

473
00:29:02.660 --> 00:29:03.493
<v Speaker 1>I've just kind of took it for granted.</v>
<v Speaker 1>So now let's talk about this process of </v>

474
00:29:06.261 --> 00:29:07.094
<v Speaker 1>actually computing this gradient because</v>
<v Speaker 1>without that gradient you're kind of </v>

475
00:29:09.351 --> 00:29:12.440
<v Speaker 1>helpless.</v>
<v Speaker 1>You have no idea which way down is,</v>

476
00:29:12.470 --> 00:29:14.390
<v Speaker 1>you don't know where to go and your </v>
<v Speaker 1>landscape.</v>

477
00:29:16.140 --> 00:29:18.330
<v Speaker 1>So let's consider a very simple neural </v>
<v Speaker 1>network,</v>

478
00:29:18.630 --> 00:29:20.850
<v Speaker 1>probably the simplest neural network in </v>
<v Speaker 1>the world.</v>

479
00:29:21.450 --> 00:29:25.380
<v Speaker 1>It contains one hidden units,</v>
<v Speaker 1>one hidden layer and one output unit.</v>

480
00:29:27.760 --> 00:29:32.760
<v Speaker 1>And we want to compute the gradient of </v>
<v Speaker 1>our loss j of data with respect to theta</v>

481
00:29:33.251 --> 00:29:34.084
<v Speaker 1>to just stay at a two for now.</v>
<v Speaker 1>So this tells us how a small change in </v>

482
00:29:38.051 --> 00:29:40.600
<v Speaker 1>theater to will impact our final loss at</v>
<v Speaker 1>the output.</v>

483
00:29:41.080 --> 00:29:41.913
<v Speaker 1>So let's write this out as a derivative.</v>
<v Speaker 1>We can start by just applying a chain </v>

484
00:29:46.721 --> 00:29:51.721
<v Speaker 1>rule because jff data is dependent on </v>
<v Speaker 1>why,</v>

485
00:29:52.000 --> 00:29:52.833
<v Speaker 1>right?</v>
<v Speaker 1>So first we want to back propagate </v>

486
00:29:54.430 --> 00:29:58.140
<v Speaker 1>through why our output all the way back </v>
<v Speaker 1>to theater.</v>

487
00:29:58.150 --> 00:30:02.410
<v Speaker 1>To we can do this because why are </v>
<v Speaker 1>output,</v>

488
00:30:02.470 --> 00:30:07.470
<v Speaker 1>why is only dependent on the input and </v>
<v Speaker 1>data too.</v>

489
00:30:07.930 --> 00:30:08.763
<v Speaker 1>That's it,</v>
<v Speaker 1>so we're able to just from that </v>

490
00:30:10.391 --> 00:30:12.580
<v Speaker 1>perceptron equation that we wrote on the</v>
<v Speaker 1>previous slide,</v>

491
00:30:12.760 --> 00:30:13.593
<v Speaker 1>computer closed form,</v>
<v Speaker 1>great end or close from derivative of </v>

492
00:30:16.391 --> 00:30:17.224
<v Speaker 1>that function.</v>
<v Speaker 1>Now let's suppose I change state of two </v>

493
00:30:20.541 --> 00:30:23.860
<v Speaker 1>to three to one and I want to compute </v>
<v Speaker 1>the same thing,</v>

494
00:30:23.861 --> 00:30:26.980
<v Speaker 1>but now for the previous layer and the </v>
<v Speaker 1>previous wait,</v>

495
00:30:28.830 --> 00:30:31.780
<v Speaker 1>all we need to do is is a supply chain </v>
<v Speaker 1>role.</v>

496
00:30:31.781 --> 00:30:32.614
<v Speaker 1>One more time back propagate those </v>
<v Speaker 1>gradients that we previously computed </v>

497
00:30:36.010 --> 00:30:39.130
<v Speaker 1>one layer further.</v>
<v Speaker 1>It's the same story.</v>

498
00:30:39.131 --> 00:30:42.670
<v Speaker 1>Again,</v>
<v Speaker 1>we can do this for the same reason.</v>

499
00:30:42.790 --> 00:30:43.623
<v Speaker 1>This is because Z,</v>
<v Speaker 1>one are hidden state is only dependent </v>

500
00:30:49.570 --> 00:30:54.570
<v Speaker 1>on our previous and put x and that </v>
<v Speaker 1>single weight Fado one.</v>

501
00:30:57.020 --> 00:30:57.853
<v Speaker 1>Now the process of backpropagation is </v>
<v Speaker 1>basically you repeat this process over </v>

502
00:31:03.681 --> 00:31:04.514
<v Speaker 1>and over again for every weight in your </v>
<v Speaker 1>network until you compute that gradient </v>

503
00:31:07.520 --> 00:31:08.353
<v Speaker 1>Dj d theater,</v>
<v Speaker 1>and you can use that as part of your </v>

504
00:31:10.491 --> 00:31:13.700
<v Speaker 1>optimization process to find your local </v>
<v Speaker 1>minimum.</v>

505
00:31:16.040 --> 00:31:16.910
<v Speaker 1>Now,</v>
<v Speaker 1>in theory,</v>

506
00:31:16.970 --> 00:31:19.040
<v Speaker 1>that sounds pretty simple.</v>
<v Speaker 1>I hope.</v>

507
00:31:19.240 --> 00:31:20.073
<v Speaker 1>I mean,</v>
<v Speaker 1>we just talked about some basic chain </v>

508
00:31:21.140 --> 00:31:21.973
<v Speaker 1>rules,</v>
<v Speaker 1>but let's actually touch on some </v>

509
00:31:23.810 --> 00:31:28.250
<v Speaker 1>insights on training these networks and </v>
<v Speaker 1>computing backpropagation and practice.</v>

510
00:31:28.820 --> 00:31:30.650
<v Speaker 1>Now,</v>
<v Speaker 1>the picture I showed you before,</v>

511
00:31:30.651 --> 00:31:33.500
<v Speaker 1>it's not really accurate for modern deep</v>
<v Speaker 1>neural network architectures.</v>

512
00:31:33.800 --> 00:31:37.760
<v Speaker 1>Modern deep neural network architectures</v>
<v Speaker 1>are extremely non convex.</v>

513
00:31:38.450 --> 00:31:42.380
<v Speaker 1>This is an illustration or a </v>
<v Speaker 1>visualization of the landscape like I've</v>

514
00:31:42.381 --> 00:31:43.214
<v Speaker 1>planted before,</v>
<v Speaker 1>but have a real deep neural network of </v>

515
00:31:45.621 --> 00:31:46.454
<v Speaker 1>resonate 50 to be precise.</v>
<v Speaker 1>This was actually taken from a paper </v>

516
00:31:50.181 --> 00:31:51.014
<v Speaker 1>published about a month ago,</v>
<v Speaker 1>was the author's attempt to visualize </v>

517
00:31:54.710 --> 00:31:55.543
<v Speaker 1>the last landscape,</v>
<v Speaker 1>to show how difficult gradient descent </v>

518
00:31:58.131 --> 00:32:00.740
<v Speaker 1>can actually be.</v>
<v Speaker 1>So if there's a possibility that you can</v>

519
00:32:00.741 --> 00:32:02.780
<v Speaker 1>get lost in any one of these local </v>
<v Speaker 1>Minima,</v>

520
00:32:02.781 --> 00:32:05.270
<v Speaker 1>there's no guarantee that you will </v>
<v Speaker 1>actually find a true global minimum.</v>

521
00:32:07.030 --> 00:32:07.863
<v Speaker 1>So let's recall that update equation </v>
<v Speaker 1>that will we defined during gradient </v>

522
00:32:09.951 --> 00:32:10.784
<v Speaker 1>descent.</v>

523
00:32:11.390 --> 00:32:14.810
<v Speaker 1>So take a look at this term here.</v>
<v Speaker 1>This is the learning rate.</v>

524
00:32:14.870 --> 00:32:15.703
<v Speaker 1>I didn't talk too much about it,</v>
<v Speaker 1>but this basically determines how large </v>

525
00:32:19.970 --> 00:32:20.803
<v Speaker 1>of a step we take in the direction of </v>
<v Speaker 1>our gradient and in practice setting </v>

526
00:32:24.801 --> 00:32:26.270
<v Speaker 1>this learning great.</v>
<v Speaker 1>It's just a number,</v>

527
00:32:26.300 --> 00:32:27.133
<v Speaker 1>but setting it can be very difficult if </v>
<v Speaker 1>we set the learning rate too low than </v>

528
00:32:31.641 --> 00:32:32.474
<v Speaker 1>the model get may get stuck in a local </v>
<v Speaker 1>minima and may never actually find its </v>

529
00:32:35.121 --> 00:32:38.460
<v Speaker 1>way out of that local Minima ps at the </v>
<v Speaker 1>bottom of local man.</v>

530
00:32:38.461 --> 00:32:41.270
<v Speaker 1>Well obviously gradient is zero,</v>
<v Speaker 1>so it was just going to stop moving.</v>

531
00:32:42.500 --> 00:32:43.333
<v Speaker 1>If I said the learning rate too large,</v>
<v Speaker 1>it could overshoot and actually </v>

532
00:32:46.251 --> 00:32:48.050
<v Speaker 1>diverged.</v>
<v Speaker 1>Our model could blow up.</v>

533
00:32:50.120 --> 00:32:50.953
<v Speaker 1>Okay.</v>
<v Speaker 1>Ideally we want to use learning rates </v>

534
00:32:53.780 --> 00:32:57.350
<v Speaker 1>that are large enough to avoid local </v>
<v Speaker 1>minima,</v>

535
00:32:57.680 --> 00:32:58.513
<v Speaker 1>but also still converged to our global </v>
<v Speaker 1>minimum so they can overshoot just </v>

536
00:33:01.311 --> 00:33:05.990
<v Speaker 1>enough to avoid some local minima but </v>
<v Speaker 1>then converge to our global minimum.</v>

537
00:33:07.340 --> 00:33:08.173
<v Speaker 1>Now,</v>
<v Speaker 1>how can we actually set the learning </v>

538
00:33:09.921 --> 00:33:11.090
<v Speaker 1>rate?</v>
<v Speaker 1>Well,</v>

539
00:33:11.091 --> 00:33:11.924
<v Speaker 1>one idea is let's just try a lot of </v>
<v Speaker 1>different things and see what works </v>

540
00:33:14.241 --> 00:33:16.790
<v Speaker 1>best,</v>
<v Speaker 1>but I don't really like the solution.</v>

541
00:33:16.880 --> 00:33:19.220
<v Speaker 1>Let's try and see if we can be a little </v>
<v Speaker 1>smarter than that.</v>

542
00:33:20.360 --> 00:33:25.360
<v Speaker 1>How about we tried to build an adaptive </v>
<v Speaker 1>algorithm that changes its learning rate</v>

543
00:33:25.610 --> 00:33:29.810
<v Speaker 1>as training happens.</v>
<v Speaker 1>So this is a learning rate that actually</v>

544
00:33:29.811 --> 00:33:30.644
<v Speaker 1>adapts to the landscape that it's in.</v>
<v Speaker 1>So the learning grade is no longer a </v>

545
00:33:34.131 --> 00:33:35.360
<v Speaker 1>fixed number.</v>
<v Speaker 1>It can change,</v>

546
00:33:35.361 --> 00:33:36.194
<v Speaker 1>it can go up and down,</v>
<v Speaker 1>and this will change depending on the </v>

547
00:33:38.781 --> 00:33:39.614
<v Speaker 1>location that the,</v>
<v Speaker 1>that the update is currently yet the </v>

548
00:33:43.491 --> 00:33:44.324
<v Speaker 1>great end in that location,</v>
<v Speaker 1>maybe how fast we're learning and many </v>

549
00:33:47.181 --> 00:33:51.560
<v Speaker 1>other many other possible situations.</v>
<v Speaker 1>In fact,</v>

550
00:33:52.370 --> 00:33:57.370
<v Speaker 1>this process of optimization in deep </v>
<v Speaker 1>neural networks and non convex situation</v>

551
00:33:57.861 --> 00:34:00.710
<v Speaker 1>has been extremely explored.</v>
<v Speaker 1>There's many,</v>

552
00:34:00.740 --> 00:34:01.573
<v Speaker 1>many,</v>
<v Speaker 1>many algorithms for computing adaptive </v>

553
00:34:04.221 --> 00:34:05.054
<v Speaker 1>learning rates and here are some </v>
<v Speaker 1>examples that we encourage you to try </v>

554
00:34:08.811 --> 00:34:13.160
<v Speaker 1>out during your labs to see what works </v>
<v Speaker 1>best and for your problems,</v>

555
00:34:13.400 --> 00:34:14.233
<v Speaker 1>especially with real world problems.</v>
<v Speaker 1>Things can change a lot depending on </v>

556
00:34:17.571 --> 00:34:22.571
<v Speaker 1>what you learn in lecture and what </v>
<v Speaker 1>really works in lab and we encourage you</v>

557
00:34:22.641 --> 00:34:23.474
<v Speaker 1>to just experiment,</v>
<v Speaker 1>get some intuition about each of these </v>

558
00:34:25.131 --> 00:34:28.310
<v Speaker 1>learning rates and really understand </v>
<v Speaker 1>them at a higher level.</v>

559
00:34:30.740 --> 00:34:31.573
<v Speaker 1>So I want to continue this talk and </v>
<v Speaker 1>really talk about more the practice of </v>

560
00:34:34.401 --> 00:34:35.234
<v Speaker 1>deep neural networks.</v>
<v Speaker 1>This incredibly powerful notion of many </v>

561
00:34:39.141 --> 00:34:44.141
<v Speaker 1>batching and I'll focus for now if we go</v>
<v Speaker 1>back to this gradient descent algorithm,</v>

562
00:34:46.490 --> 00:34:47.323
<v Speaker 1>this is the same one that we saw before </v>
<v Speaker 1>and let's look at this term again so we </v>

563
00:34:50.720 --> 00:34:53.900
<v Speaker 1>found out how to compute this term using</v>
<v Speaker 1>backpropagation,</v>

564
00:34:54.670 --> 00:34:55.503
<v Speaker 1>but actually what I didn't tell you is </v>
<v Speaker 1>that the computation here is extremely </v>

565
00:34:58.821 --> 00:35:00.470
<v Speaker 1>calm,</v>
<v Speaker 1>is extremely expensive.</v>

566
00:35:01.010 --> 00:35:01.843
<v Speaker 1>We have a lot of data points potentially</v>
<v Speaker 1>in our data set and this takes us input </v>

567
00:35:06.060 --> 00:35:08.150
<v Speaker 1>a summation over all of those data </v>
<v Speaker 1>points.</v>

568
00:35:08.151 --> 00:35:11.390
<v Speaker 1>So if our Dataset is millions of </v>
<v Speaker 1>examples large,</v>

569
00:35:11.391 --> 00:35:15.170
<v Speaker 1>which is not that large in the realm of </v>
<v Speaker 1>today's deep neural networks,</v>

570
00:35:16.180 --> 00:35:18.710
<v Speaker 1>but this can be extremely expensive just</v>
<v Speaker 1>for one iteration,</v>

571
00:35:18.711 --> 00:35:20.240
<v Speaker 1>so we can compute this on every </v>
<v Speaker 1>iteration.</v>

572
00:35:20.450 --> 00:35:21.283
<v Speaker 1>Instead,</v>
<v Speaker 1>lets create a variant of this algorithm </v>

573
00:35:23.900 --> 00:35:24.733
<v Speaker 1>called stochastic gradient descent where</v>
<v Speaker 1>we compute the gradient just using a </v>

574
00:35:27.831 --> 00:35:29.390
<v Speaker 1>single training example.</v>

575
00:35:30.470 --> 00:35:31.303
<v Speaker 1>Now this is nice because it's really </v>
<v Speaker 1>easy to compute the gradient for a </v>

576
00:35:33.711 --> 00:35:34.544
<v Speaker 1>single training example.</v>
<v Speaker 1>It's not nearly as intense as over the </v>

577
00:35:36.681 --> 00:35:40.020
<v Speaker 1>entire training set,</v>
<v Speaker 1>but as the name might suggest,</v>

578
00:35:40.021 --> 00:35:43.530
<v Speaker 1>this is a more stochastic estimate,</v>
<v Speaker 1>as much more noisy.</v>

579
00:35:44.280 --> 00:35:47.910
<v Speaker 1>It can make us jump around the landscape</v>
<v Speaker 1>in ways that we didn't anticipate.</v>

580
00:35:47.911 --> 00:35:48.744
<v Speaker 1>It doesn't actually represent the true </v>
<v Speaker 1>gradient of our data set because it's </v>

581
00:35:51.241 --> 00:35:54.180
<v Speaker 1>only a single point,</v>
<v Speaker 1>so what's the middle ground?</v>

582
00:35:54.690 --> 00:35:59.400
<v Speaker 1>How about we define a mini batch of be </v>
<v Speaker 1>data points,</v>

583
00:35:59.850 --> 00:36:02.970
<v Speaker 1>compute the average gradient across </v>
<v Speaker 1>those data points,</v>

584
00:36:03.540 --> 00:36:06.600
<v Speaker 1>and actually use that as an estimate of </v>
<v Speaker 1>our true gradient.</v>

585
00:36:07.410 --> 00:36:08.243
<v Speaker 1>Now this is much faster than computing </v>
<v Speaker 1>the estimate over the entire batch </v>

586
00:36:10.531 --> 00:36:11.364
<v Speaker 1>because b is usually something like 10 </v>
<v Speaker 1>to 100 and it's much more accurate than </v>

587
00:36:15.511 --> 00:36:18.260
<v Speaker 1>sgd because we're not taking a single </v>
<v Speaker 1>example,</v>

588
00:36:18.261 --> 00:36:21.000
<v Speaker 1>but we're learning over a smaller batch.</v>
<v Speaker 1>A larger batch.</v>

589
00:36:21.001 --> 00:36:23.420
<v Speaker 1>Sorry.</v>
<v Speaker 1>Nope.</v>

590
00:36:23.450 --> 00:36:25.970
<v Speaker 1>The more accurate our gradient </v>
<v Speaker 1>estimation is,</v>

591
00:36:26.450 --> 00:36:27.283
<v Speaker 1>that means the more or the easier it </v>
<v Speaker 1>will be for us to converge to the </v>

592
00:36:31.431 --> 00:36:32.264
<v Speaker 1>solution,</v>
<v Speaker 1>faster means will converge smoother </v>

593
00:36:34.730 --> 00:36:38.090
<v Speaker 1>because we'll actually follow the true </v>
<v Speaker 1>landscape that exists.</v>

594
00:36:39.110 --> 00:36:39.943
<v Speaker 1>It also means that we can increase our </v>
<v Speaker 1>learning great to trust each update </v>

595
00:36:41.961 --> 00:36:42.794
<v Speaker 1>more.</v>

596
00:36:44.840 --> 00:36:48.170
<v Speaker 1>This also allows for massively parallel </v>
<v Speaker 1>likable computation.</v>

597
00:36:48.590 --> 00:36:49.423
<v Speaker 1>If we split up batches on different </v>
<v Speaker 1>workers on different gps or different </v>

598
00:36:53.391 --> 00:36:54.224
<v Speaker 1>threads,</v>
<v Speaker 1>we can achieve even higher speed ups </v>

599
00:36:56.991 --> 00:36:59.120
<v Speaker 1>because each thread can handle its own </v>
<v Speaker 1>batch.</v>

600
00:36:59.121 --> 00:36:59.954
<v Speaker 1>Then they can come back together and </v>
<v Speaker 1>aggregate together to basically create </v>

601
00:37:03.411 --> 00:37:07.640
<v Speaker 1>that single learning grade or complete </v>
<v Speaker 1>complete that single training iteration.</v>

602
00:37:10.430 --> 00:37:11.263
<v Speaker 1>Now finally,</v>
<v Speaker 1>the last topic I want to talk about is </v>

603
00:37:12.890 --> 00:37:13.723
<v Speaker 1>that of overfitting and regularization.</v>
<v Speaker 1>Really this is a problem of </v>

604
00:37:18.560 --> 00:37:19.393
<v Speaker 1>generalization,</v>
<v Speaker 1>which is one of the most fundamental </v>

605
00:37:23.841 --> 00:37:26.210
<v Speaker 1>problems and all of artificial </v>
<v Speaker 1>intelligence,</v>

606
00:37:26.570 --> 00:37:29.750
<v Speaker 1>not just deep learning,</v>
<v Speaker 1>but all of artificial intelligence,</v>

607
00:37:31.070 --> 00:37:34.820
<v Speaker 1>and for those who aren't familiar,</v>
<v Speaker 1>let me just go over on a high level what</v>

608
00:37:34.910 --> 00:37:37.670
<v Speaker 1>overfitting is,</v>
<v Speaker 1>what it means to generalize.</v>

609
00:37:38.630 --> 00:37:39.463
<v Speaker 1>Ideally in machine learning,</v>
<v Speaker 1>we want the model that accurately </v>

610
00:37:41.331 --> 00:37:44.600
<v Speaker 1>describes our test data,</v>
<v Speaker 1>not our training data,</v>

611
00:37:44.690 --> 00:37:45.590
<v Speaker 1>but our test data</v>

612
00:37:47.500 --> 00:37:48.333
<v Speaker 1>said differently.</v>
<v Speaker 1>We want to build models that can learn </v>

613
00:37:50.081 --> 00:37:50.914
<v Speaker 1>representations from our training data </v>
<v Speaker 1>and still generalized well on unseen </v>

614
00:37:55.600 --> 00:37:56.433
<v Speaker 1>test data.</v>
<v Speaker 1>Assuming you want to build a line to </v>

615
00:37:58.751 --> 00:38:02.590
<v Speaker 1>describe these points under fitting </v>
<v Speaker 1>describes the process on the left,</v>

616
00:38:02.900 --> 00:38:03.733
<v Speaker 1>where at the complexity of our model is </v>
<v Speaker 1>simply not high enough to capture the </v>

617
00:38:06.851 --> 00:38:10.600
<v Speaker 1>nuances of our data.</v>
<v Speaker 1>If we go over 50 on the right,</v>

618
00:38:10.990 --> 00:38:11.823
<v Speaker 1>we're actually having too complex of a </v>
<v Speaker 1>model and actually just memorizing our </v>

619
00:38:15.521 --> 00:38:16.354
<v Speaker 1>training data,</v>
<v Speaker 1>which means that if we introduced a new </v>

620
00:38:17.951 --> 00:38:20.590
<v Speaker 1>test data point,</v>
<v Speaker 1>it's not going to generalize well.</v>

621
00:38:20.890 --> 00:38:22.720
<v Speaker 1>Ideally what we want to,</v>
<v Speaker 1>something in the middle,</v>

622
00:38:23.230 --> 00:38:26.020
<v Speaker 1>which is not too complex to memorize all</v>
<v Speaker 1>of the training data,</v>

623
00:38:27.340 --> 00:38:28.173
<v Speaker 1>but still a contains the capacity to </v>
<v Speaker 1>learn some of these nuances in the test </v>

624
00:38:33.911 --> 00:38:38.050
<v Speaker 1>set.</v>
<v Speaker 1>So just to address this problem,</v>

625
00:38:38.051 --> 00:38:40.990
<v Speaker 1>let's talk about this technique called </v>
<v Speaker 1>regularization.</v>

626
00:38:41.820 --> 00:38:42.653
<v Speaker 1>Now regularization is just this way that</v>
<v Speaker 1>you can discourage your models from </v>

627
00:38:46.271 --> 00:38:50.380
<v Speaker 1>becoming too complex and after we've </v>
<v Speaker 1>seen,</v>

628
00:38:50.480 --> 00:38:51.313
<v Speaker 1>as we've seen before,</v>
<v Speaker 1>this is extremely critical because we </v>

629
00:38:53.961 --> 00:38:54.794
<v Speaker 1>don't want our data.</v>
<v Speaker 1>We don't want our models to just </v>

630
00:38:57.801 --> 00:39:00.920
<v Speaker 1>memorize data and only do well in our </v>
<v Speaker 1>training set.</v>

631
00:39:04.620 --> 00:39:05.453
<v Speaker 1>One of the most popular techniques for </v>
<v Speaker 1>regularization in neural networks is </v>

632
00:39:09.180 --> 00:39:11.760
<v Speaker 1>dropout.</v>
<v Speaker 1>This is extremely simple idea.</v>

633
00:39:12.210 --> 00:39:13.043
<v Speaker 1>Let's revisit this picture of a deep </v>
<v Speaker 1>neural network and then drop out all we </v>

634
00:39:15.391 --> 00:39:16.224
<v Speaker 1>do during a training on every iteration,</v>
<v Speaker 1>we randomly drop some proportion of the </v>

635
00:39:22.500 --> 00:39:26.910
<v Speaker 1>hidden neurons with some probability p,</v>
<v Speaker 1>so let's suppose p equals point five.</v>

636
00:39:26.911 --> 00:39:29.280
<v Speaker 1>That means we dropped 50 percent of </v>
<v Speaker 1>those neurons like that.</v>

637
00:39:29.760 --> 00:39:30.593
<v Speaker 1>Those activations becomes zero and </v>
<v Speaker 1>effectively they're no longer part of </v>

638
00:39:33.751 --> 00:39:34.584
<v Speaker 1>our network.</v>
<v Speaker 1>This forces the network to not rely on </v>

639
00:39:38.841 --> 00:39:39.674
<v Speaker 1>any single node,</v>
<v Speaker 1>but actually find alternative paths </v>

640
00:39:42.561 --> 00:39:43.394
<v Speaker 1>through the network and not put too much</v>
<v Speaker 1>weight on any single example with any </v>

641
00:39:46.851 --> 00:39:49.400
<v Speaker 1>syngrid single nodes,</v>
<v Speaker 1>so it discourages memorization.</v>

642
00:39:49.401 --> 00:39:52.950
<v Speaker 1>Essentially,</v>
<v Speaker 1>John,</v>

643
00:39:52.980 --> 00:39:53.813
<v Speaker 1>every iteration,</v>
<v Speaker 1>we randomly drop another 50 percent of </v>

644
00:39:56.191 --> 00:39:58.290
<v Speaker 1>the nodes,</v>
<v Speaker 1>so in this iteration I may drop these on</v>

645
00:39:58.291 --> 00:39:59.124
<v Speaker 1>the next iteration.</v>
<v Speaker 1>I may drop those and since it's </v>

646
00:40:00.751 --> 00:40:01.584
<v Speaker 1>different on every iteration,</v>
<v Speaker 1>you're encouraging the network to find </v>

647
00:40:04.051 --> 00:40:05.940
<v Speaker 1>these different paths to his answer.</v>

648
00:40:08.240 --> 00:40:09.073
<v Speaker 1>The second technique for regularization </v>
<v Speaker 1>that we'll talk about is this notion of </v>

649
00:40:11.810 --> 00:40:12.643
<v Speaker 1>early stopping.</v>
<v Speaker 1>Now we know that the definition of </v>

650
00:40:15.471 --> 00:40:16.304
<v Speaker 1>overfitting actually is just when our </v>
<v Speaker 1>model starts to perform worse and worse </v>

651
00:40:20.241 --> 00:40:23.840
<v Speaker 1>on our test data set.</v>
<v Speaker 1>So let's use that to our advantage.</v>

652
00:40:23.841 --> 00:40:24.674
<v Speaker 1>To create this early stopping algorithm,</v>
<v Speaker 1>if we set aside some of our training </v>

653
00:40:27.981 --> 00:40:30.980
<v Speaker 1>data and use it only as test data,</v>
<v Speaker 1>we don't train with that data.</v>

654
00:40:31.700 --> 00:40:32.533
<v Speaker 1>We can use it to basically monitor the </v>
<v Speaker 1>progress of our model on unseen data so </v>

655
00:40:36.921 --> 00:40:39.830
<v Speaker 1>we can plot this curve.</v>
<v Speaker 1>We're on the x axis,</v>

656
00:40:39.831 --> 00:40:41.720
<v Speaker 1>we have the training iterations.</v>
<v Speaker 1>On the y axis,</v>

657
00:40:41.721 --> 00:40:44.780
<v Speaker 1>we have to loss.</v>
<v Speaker 1>Now they start off going down together.</v>

658
00:40:44.870 --> 00:40:47.780
<v Speaker 1>This is great because it means that </v>
<v Speaker 1>we're learning or training,</v>

659
00:40:47.781 --> 00:40:49.100
<v Speaker 1>right?</v>
<v Speaker 1>That's great.</v>

660
00:40:50.390 --> 00:40:53.720
<v Speaker 1>There comes a point though where the </v>
<v Speaker 1>testing data,</v>

661
00:40:54.110 --> 00:40:59.110
<v Speaker 1>where the testing data set and the loss </v>
<v Speaker 1>for that Dataset starts to plateau.</v>

662
00:41:00.980 --> 00:41:02.300
<v Speaker 1>Now,</v>
<v Speaker 1>if we look a little further,</v>

663
00:41:02.390 --> 00:41:06.800
<v Speaker 1>the training data set loss will always </v>
<v Speaker 1>continue to go down as long as our model</v>

664
00:41:06.801 --> 00:41:10.130
<v Speaker 1>has the capacity to learn and memorize </v>
<v Speaker 1>some of that data,</v>

665
00:41:10.430 --> 00:41:11.263
<v Speaker 1>but that doesn't mean that it's actually</v>
<v Speaker 1>generalizing well because we can see </v>

666
00:41:13.041 --> 00:41:15.950
<v Speaker 1>that the testing dataset has actually </v>
<v Speaker 1>started to increase.</v>

667
00:41:17.550 --> 00:41:19.530
<v Speaker 1>This pattern continues for the rest of </v>
<v Speaker 1>training,</v>

668
00:41:19.650 --> 00:41:24.480
<v Speaker 1>but I want to focus on this point here.</v>
<v Speaker 1>This is the point where you need to stop</v>

669
00:41:24.510 --> 00:41:28.740
<v Speaker 1>training because after this point,</v>
<v Speaker 1>you are overfitting and your model is no</v>

670
00:41:28.741 --> 00:41:32.460
<v Speaker 1>longer performing well on unseen data.</v>
<v Speaker 1>If you stopped before that point,</v>

671
00:41:32.880 --> 00:41:33.713
<v Speaker 1>you're under fitting and you're not </v>
<v Speaker 1>utilizing the full potential to full </v>

672
00:41:36.441 --> 00:41:37.274
<v Speaker 1>capacity of your network.</v>
<v Speaker 1>So I'll conclude this lecture by </v>

673
00:41:42.690 --> 00:41:46.200
<v Speaker 1>summarizing three key points that we've </v>
<v Speaker 1>covered so far.</v>

674
00:41:47.410 --> 00:41:48.243
<v Speaker 1>First,</v>
<v Speaker 1>we've learned about the fundamental </v>

675
00:41:49.201 --> 00:41:52.260
<v Speaker 1>building blocks of neural networks </v>
<v Speaker 1>called the perceptron.</v>

676
00:41:53.220 --> 00:41:56.700
<v Speaker 1>We've learned about stacking these </v>
<v Speaker 1>units,</v>

677
00:41:56.770 --> 00:42:01.770
<v Speaker 1>these perceptrons together to compose </v>
<v Speaker 1>very complex hierarchical models,</v>

678
00:42:03.570 --> 00:42:04.403
<v Speaker 1>and we've learned how to mathematically </v>
<v Speaker 1>optimize these models using a process </v>

679
00:42:08.370 --> 00:42:11.790
<v Speaker 1>called backpropagation and gradient </v>
<v Speaker 1>descent.</v>

680
00:42:13.290 --> 00:42:14.123
<v Speaker 1>Finally,</v>
<v Speaker 1>we addressed some of the practical </v>

681
00:42:15.001 --> 00:42:15.834
<v Speaker 1>challenges of training these models in </v>
<v Speaker 1>real life that you'll find useful for </v>

682
00:42:19.471 --> 00:42:22.320
<v Speaker 1>the labs today,</v>
<v Speaker 1>such as using adaptive learning rates,</v>

683
00:42:22.470 --> 00:42:25.410
<v Speaker 1>batching and regularization to combat </v>
<v Speaker 1>overfitting.</v>

684
00:42:27.920 --> 00:42:29.060
<v Speaker 1>Thank you.</v>
<v Speaker 1>And,</v>

685
00:42:29.061 --> 00:42:29.894
<v Speaker 1>uh,</v>
<v Speaker 1>I'd be happy to answer any questions </v>

686
00:42:32.061 --> 00:42:33.260
<v Speaker 1>now.</v>
<v Speaker 1>Otherwise,</v>

687
00:42:33.290 --> 00:42:38.240
<v Speaker 1>we'll have rainy talk to us about some </v>
<v Speaker 1>of the deep sequence models for modeling</v>

688
00:42:38.241 --> 00:42:39.080
<v Speaker 1>temporal data.</v>

