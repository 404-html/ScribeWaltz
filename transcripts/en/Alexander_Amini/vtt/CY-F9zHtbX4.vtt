WEBVTT

1
00:00:04.270 --> 00:00:07.200
<v Speaker 1>So Hi everyone.</v>
<v Speaker 1>My name is Alexander [inaudible].</v>

2
00:00:07.260 --> 00:00:11.610
<v Speaker 1>I'm a phd student and the computer </v>
<v Speaker 1>science and ai lab at Mit.</v>

3
00:00:11.880 --> 00:00:12.713
<v Speaker 1>And today I'm going to be telling you </v>
<v Speaker 1>about some of our work in building a </v>

4
00:00:15.721 --> 00:00:17.640
<v Speaker 1>computer,</v>
<v Speaker 1>excuse me,</v>

5
00:00:17.670 --> 00:00:20.400
<v Speaker 1>building an autonomous vehicle control </v>
<v Speaker 1>system,</v>

6
00:00:20.870 --> 00:00:21.703
<v Speaker 1>uh,</v>
<v Speaker 1>for parallel autonomy using and to end </v>

7
00:00:23.640 --> 00:00:26.250
<v Speaker 1>learning.</v>
<v Speaker 1>I should also mention that this is joint</v>

8
00:00:26.260 --> 00:00:30.780
<v Speaker 1>work as part of the MIT tri or Toyota </v>
<v Speaker 1>research institute partnership.</v>

9
00:00:32.730 --> 00:00:33.563
<v Speaker 1>So first of all,</v>
<v Speaker 1>when I say autonomous vehicle control </v>

10
00:00:35.011 --> 00:00:37.590
<v Speaker 1>system,</v>
<v Speaker 1>what is it that I really mean?</v>

11
00:00:37.980 --> 00:00:41.010
<v Speaker 1>What does this require?</v>
<v Speaker 1>And by asking these questions,</v>

12
00:00:41.011 --> 00:00:41.844
<v Speaker 1>we can kind of start to see why we might</v>
<v Speaker 1>even want to care about an end to end </v>

13
00:00:45.720 --> 00:00:48.030
<v Speaker 1>solution at all.</v>
<v Speaker 1>So for starters,</v>

14
00:00:48.031 --> 00:00:52.800
<v Speaker 1>we know that autonomous vehicle systems </v>
<v Speaker 1>need to be able to handle many different</v>

15
00:00:52.920 --> 00:00:53.753
<v Speaker 1>types of scenarios.</v>
<v Speaker 1>So this includes things like changes in </v>

16
00:00:55.981 --> 00:00:59.160
<v Speaker 1>luminosity,</v>
<v Speaker 1>the presence of lane markers,</v>

17
00:00:59.220 --> 00:01:00.870
<v Speaker 1>and even harsh weather conditions.</v>

18
00:01:01.030 --> 00:01:01.863
<v Speaker 1>This is something I,</v>
<v Speaker 1>a lot of autonomous vehicle systems </v>

19
00:01:03.661 --> 00:01:04.494
<v Speaker 1>today are simply not able to handle.</v>
<v Speaker 1>Model based systems are often very </v>

20
00:01:09.031 --> 00:01:09.864
<v Speaker 1>brittle and practice.</v>
<v Speaker 1>They require huge number of hand </v>

21
00:01:11.701 --> 00:01:16.701
<v Speaker 1>programmed rules and engineered features</v>
<v Speaker 1>to be extracted.</v>

22
00:01:16.771 --> 00:01:21.540
<v Speaker 1>And instead in this project we aim to </v>
<v Speaker 1>utilize large datasets to actually learn</v>

23
00:01:21.541 --> 00:01:22.374
<v Speaker 1>and underlying representation of how the</v>
<v Speaker 1>human actually drove so that we can </v>

24
00:01:25.141 --> 00:01:27.570
<v Speaker 1>build a computer to replicate that in </v>
<v Speaker 1>practice.</v>

25
00:01:29.220 --> 00:01:30.053
<v Speaker 1>So let's start by actually taking a step</v>
<v Speaker 1>back and looking at the standard comms </v>

26
00:01:32.581 --> 00:01:33.414
<v Speaker 1>vehicle pipeline.</v>
<v Speaker 1>I know this might be very familiar to </v>

27
00:01:35.761 --> 00:01:37.260
<v Speaker 1>some of you,</v>
<v Speaker 1>but I still wanted to go through it just</v>

28
00:01:37.261 --> 00:01:39.810
<v Speaker 1>so we can see the difference with an end</v>
<v Speaker 1>to end pipeline.</v>

29
00:01:40.620 --> 00:01:44.040
<v Speaker 1>And the key idea here is that the </v>
<v Speaker 1>problem is broken up into many different</v>

30
00:01:44.041 --> 00:01:47.970
<v Speaker 1>submodules where each sub module is </v>
<v Speaker 1>tackled independently.</v>

31
00:01:48.000 --> 00:01:48.833
<v Speaker 1>And you can see on the bottom I've put </v>
<v Speaker 1>some references to actual works and </v>

32
00:01:51.991 --> 00:01:52.824
<v Speaker 1>publications from both academia and </v>
<v Speaker 1>industry focus on actually tackling an </v>

33
00:01:57.061 --> 00:01:59.430
<v Speaker 1>individual sub problem of this pipeline.</v>

34
00:01:59.610 --> 00:02:00.443
<v Speaker 1>And it starts with on the left you can </v>
<v Speaker 1>see that we're actually just collecting </v>

35
00:02:02.341 --> 00:02:03.174
<v Speaker 1>data about our environment,</v>
<v Speaker 1>we're trying to learn what is happening </v>

36
00:02:05.671 --> 00:02:06.504
<v Speaker 1>around us.</v>
<v Speaker 1>So this can include things like camera </v>

37
00:02:07.831 --> 00:02:08.790
<v Speaker 1>data,</v>
<v Speaker 1>Lidar,</v>

38
00:02:08.820 --> 00:02:10.500
<v Speaker 1>radar,</v>
<v Speaker 1>initial data,</v>

39
00:02:10.501 --> 00:02:11.334
<v Speaker 1>et cetera.</v>
<v Speaker 1>And then we feed this into some sort of </v>

40
00:02:13.291 --> 00:02:14.124
<v Speaker 1>detection pipeline cause it's the first </v>
<v Speaker 1>thing we want to do is actually </v>

41
00:02:15.811 --> 00:02:18.090
<v Speaker 1>understand where the obstacles around us</v>
<v Speaker 1>are.</v>

42
00:02:18.300 --> 00:02:21.990
<v Speaker 1>We need to identify these obstacles as </v>
<v Speaker 1>both static or dynamic.</v>

43
00:02:22.500 --> 00:02:23.333
<v Speaker 1>We might want to pay even more attention</v>
<v Speaker 1>to dynamic obstacles and perhaps plan </v>

44
00:02:26.131 --> 00:02:26.964
<v Speaker 1>their emotion and the environment.</v>
<v Speaker 1>And once we have an idea of what's </v>

45
00:02:29.611 --> 00:02:30.444
<v Speaker 1>happening and the objects around us,</v>
<v Speaker 1>we want to actually localize ourselves </v>

46
00:02:33.480 --> 00:02:36.060
<v Speaker 1>relative to those objects.</v>
<v Speaker 1>And this is called localization.</v>

47
00:02:37.440 --> 00:02:40.740
<v Speaker 1>Once we have an idea of where we are,</v>
<v Speaker 1>where the objects are under Sar,</v>

48
00:02:40.770 --> 00:02:42.990
<v Speaker 1>we can finally start to talk about </v>
<v Speaker 1>planning,</v>

49
00:02:42.991 --> 00:02:46.880
<v Speaker 1>which is really one of the most </v>
<v Speaker 1>fundamental parts of,</v>

50
00:02:47.170 --> 00:02:48.090
<v Speaker 1>of autonomous driving.</v>

51
00:02:48.090 --> 00:02:53.090
<v Speaker 1>So the goal of autonomous driving is to </v>
<v Speaker 1>travel from point a to b safely,</v>

52
00:02:53.340 --> 00:02:54.173
<v Speaker 1>right?</v>
<v Speaker 1>And that's exactly what planning allows </v>

53
00:02:55.231 --> 00:02:57.660
<v Speaker 1>us to accomplish.</v>
<v Speaker 1>So planning takes into account where the</v>

54
00:02:57.661 --> 00:02:58.494
<v Speaker 1>obstacles are and then allows us to </v>
<v Speaker 1>actually plan around to reach our goal </v>

55
00:03:02.891 --> 00:03:05.770
<v Speaker 1>destination.</v>
<v Speaker 1>Once we have this plan,</v>

56
00:03:05.800 --> 00:03:06.633
<v Speaker 1>we can now send actuation of signals to </v>
<v Speaker 1>the car motor and actually execute that </v>

57
00:03:10.781 --> 00:03:11.614
<v Speaker 1>plan.</v>
<v Speaker 1>So in this project we actually replaced </v>

58
00:03:14.611 --> 00:03:18.490
<v Speaker 1>that entire intermediate pipeline with a</v>
<v Speaker 1>learned model.</v>

59
00:03:18.550 --> 00:03:23.020
<v Speaker 1>And in this project we're actually </v>
<v Speaker 1>focused on that learn model being a deep</v>

60
00:03:23.021 --> 00:03:23.854
<v Speaker 1>neural network.</v>
<v Speaker 1>And instead of actually learning the </v>

61
00:03:26.681 --> 00:03:27.514
<v Speaker 1>control directly from raw sensory data,</v>
<v Speaker 1>we're going to just focus on pixel </v>

62
00:03:31.091 --> 00:03:34.240
<v Speaker 1>value.</v>
<v Speaker 1>So using a single front facing camera as</v>

63
00:03:34.241 --> 00:03:39.040
<v Speaker 1>our sensory input and instead of </v>
<v Speaker 1>predicting all possible control,</v>

64
00:03:39.070 --> 00:03:39.903
<v Speaker 1>we're just going to focus on predicting </v>
<v Speaker 1>the steering wheel angle at that </v>

65
00:03:42.101 --> 00:03:42.934
<v Speaker 1>instant.</v>

66
00:03:44.710 --> 00:03:47.560
<v Speaker 1>And while this may seem like a very </v>
<v Speaker 1>simple problem at first,</v>

67
00:03:47.561 --> 00:03:49.750
<v Speaker 1>because we are removing all of these </v>
<v Speaker 1>intermediate pipeline,</v>

68
00:03:49.900 --> 00:03:50.733
<v Speaker 1>all of these intermediate problems,</v>
<v Speaker 1>this is actually an extremely hard </v>

69
00:03:54.011 --> 00:03:57.340
<v Speaker 1>problem in practice to actually get </v>
<v Speaker 1>working in real life.</v>

70
00:03:58.000 --> 00:04:03.000
<v Speaker 1>And there are a couple reasons why this </v>
<v Speaker 1>is so difficult to work in real life.</v>

71
00:04:03.101 --> 00:04:03.934
<v Speaker 1>So firstly,</v>
<v Speaker 1>in the real world we face huge amounts </v>

72
00:04:07.121 --> 00:04:07.954
<v Speaker 1>of uncertainty,</v>
<v Speaker 1>huge amounts of more important than </v>

73
00:04:09.311 --> 00:04:11.800
<v Speaker 1>uncertainty.</v>
<v Speaker 1>We faced huge amounts of ambiguity.</v>

74
00:04:12.370 --> 00:04:14.920
<v Speaker 1>So this is a picture of a roundabout in </v>
<v Speaker 1>Paris.</v>

75
00:04:14.921 --> 00:04:19.921
<v Speaker 1>You can see that we have to deal with </v>
<v Speaker 1>huge amounts of complexity in real world</v>

76
00:04:20.531 --> 00:04:23.320
<v Speaker 1>driving.</v>
<v Speaker 1>We have to be able to handle uncertainty</v>

77
00:04:23.350 --> 00:04:26.650
<v Speaker 1>in the environment.</v>
<v Speaker 1>We have to be able to handle uncertainty</v>

78
00:04:26.651 --> 00:04:27.484
<v Speaker 1>and the cars around us and even </v>
<v Speaker 1>uncertainty in their intentions and </v>

79
00:04:31.271 --> 00:04:34.070
<v Speaker 1>their actions.</v>
<v Speaker 1>So we don't know what the vehicles,</v>

80
00:04:34.071 --> 00:04:37.330
<v Speaker 1>the pedestrians,</v>
<v Speaker 1>the cyclist are doing around us.</v>

81
00:04:37.420 --> 00:04:40.180
<v Speaker 1>And we have to account for that as part </v>
<v Speaker 1>of our planning algorithms.</v>

82
00:04:41.710 --> 00:04:44.860
<v Speaker 1>And since we're focusing on vision data,</v>
<v Speaker 1>there are also many challenges that come</v>

83
00:04:44.861 --> 00:04:47.770
<v Speaker 1>with this as well.</v>
<v Speaker 1>So camera sensors are subject,</v>

84
00:04:47.800 --> 00:04:51.280
<v Speaker 1>so sun glare.</v>
<v Speaker 1>So he can see on the bottom here there's</v>

85
00:04:51.281 --> 00:04:54.790
<v Speaker 1>a image of the sun actually facing </v>
<v Speaker 1>directly into the camera.</v>

86
00:04:54.791 --> 00:04:57.430
<v Speaker 1>And this kind of renders the entire </v>
<v Speaker 1>driving seen underneath that,</v>

87
00:04:57.900 --> 00:04:59.110
<v Speaker 1>um,</v>
<v Speaker 1>almost black.</v>

88
00:04:59.111 --> 00:05:03.940
<v Speaker 1>So it makes the driving or the imagery </v>
<v Speaker 1>very difficult to actually interpret.</v>

89
00:05:05.020 --> 00:05:09.300
<v Speaker 1>And additionally we have to be able to </v>
<v Speaker 1>account for uncertainty in the,</v>

90
00:05:09.320 --> 00:05:12.340
<v Speaker 1>the perception itself.</v>
<v Speaker 1>So dealing with harsh weather conditions</v>

91
00:05:12.341 --> 00:05:14.300
<v Speaker 1>brings about,</v>
<v Speaker 1>uh,</v>

92
00:05:14.350 --> 00:05:18.340
<v Speaker 1>the inability to accurately image the </v>
<v Speaker 1>environment and things like rain,</v>

93
00:05:18.341 --> 00:05:21.910
<v Speaker 1>snow and et Cetera.</v>
<v Speaker 1>And then finally,</v>

94
00:05:22.030 --> 00:05:22.863
<v Speaker 1>one of the,</v>
<v Speaker 1>perhaps most key challenge of all </v>

95
00:05:26.101 --> 00:05:27.910
<v Speaker 1>autonomous driving,</v>
<v Speaker 1>not just end to end,</v>

96
00:05:27.911 --> 00:05:29.290
<v Speaker 1>is dealing with the edge cases.</v>

97
00:05:29.860 --> 00:05:34.210
<v Speaker 1>So what happens if we're driving and we </v>
<v Speaker 1>see a plane in the road in front of us,</v>

98
00:05:35.020 --> 00:05:38.190
<v Speaker 1>like we can see on this top image here,</v>
<v Speaker 1>uh,</v>

99
00:05:39.010 --> 00:05:39.843
<v Speaker 1>or on the other hand,</v>
<v Speaker 1>what if we face assist a scenario like </v>

100
00:05:42.791 --> 00:05:43.624
<v Speaker 1>this bottom spot them scenario here </v>
<v Speaker 1>where there is actually a sign painted </v>

101
00:05:47.651 --> 00:05:48.484
<v Speaker 1>on the back of a car of a pedestrian and</v>
<v Speaker 1>a cyclist and our object detector at </v>

102
00:05:51.900 --> 00:05:52.733
<v Speaker 1>detection pipeline has actually </v>
<v Speaker 1>recognized that sign as containing a </v>

103
00:05:55.691 --> 00:05:57.410
<v Speaker 1>real life,</v>
<v Speaker 1>human or pedestrian.</v>

104
00:05:58.760 --> 00:06:01.690
<v Speaker 1>So these are real problems that they </v>
<v Speaker 1>face all the time.</v>

105
00:06:01.691 --> 00:06:03.860
<v Speaker 1>His vehicles including end to end </v>
<v Speaker 1>systems.</v>

106
00:06:05.260 --> 00:06:05.580
<v Speaker 2>Yeah,</v>

107
00:06:05.580 --> 00:06:06.413
<v Speaker 1>it's,</v>
<v Speaker 1>I'd like to talk about some of the </v>

108
00:06:06.821 --> 00:06:09.070
<v Speaker 1>topics I'd like to go for as part of </v>
<v Speaker 1>this talk.</v>

109
00:06:10.270 --> 00:06:11.103
<v Speaker 1>Firstly,</v>
<v Speaker 1>I'd like to begin this talk by </v>

110
00:06:12.910 --> 00:06:16.540
<v Speaker 1>explaining this phrase that I introduced</v>
<v Speaker 1>in my title slide,</v>

111
00:06:16.541 --> 00:06:17.374
<v Speaker 1>which is parallel autonomy.</v>
<v Speaker 1>I have if I haven't actually mentioned </v>

112
00:06:20.291 --> 00:06:21.124
<v Speaker 1>what parallel autonomy is yet.</v>
<v Speaker 1>So I'm going to talk about this in much </v>

113
00:06:24.191 --> 00:06:25.750
<v Speaker 1>more detail in the first part of the </v>
<v Speaker 1>talk.</v>

114
00:06:26.820 --> 00:06:27.250
<v Speaker 2>Yeah.</v>

115
00:06:27.250 --> 00:06:31.180
<v Speaker 1>And really I'll walk you through firstly</v>
<v Speaker 1>what the definition of parallel autonomy</v>

116
00:06:31.181 --> 00:06:32.014
<v Speaker 1>is and the autonomy system that we've </v>
<v Speaker 1>built at mit to actually create a test </v>

117
00:06:38.251 --> 00:06:39.084
<v Speaker 1>bed for a parallel tonomy testing.</v>
<v Speaker 1>Next I'll present some of our work and </v>

118
00:06:43.651 --> 00:06:47.460
<v Speaker 1>not only learning the steering wheel </v>
<v Speaker 1>control of an autonomous vehicle,</v>

119
00:06:47.461 --> 00:06:48.294
<v Speaker 1>but also the corresponding steering </v>
<v Speaker 1>bounds for that vehicle and an end to </v>

120
00:06:51.391 --> 00:06:52.224
<v Speaker 1>end manner.</v>
<v Speaker 1>I'll show you how this work actually </v>

121
00:06:54.751 --> 00:06:55.584
<v Speaker 1>tackles some of the issues that I </v>
<v Speaker 1>presented earlier that end to end </v>

122
00:06:59.341 --> 00:07:01.350
<v Speaker 1>systems today currently face.</v>

123
00:07:02.190 --> 00:07:02.840
<v Speaker 2>Okay.</v>

124
00:07:02.840 --> 00:07:03.673
<v Speaker 1>And finally we'll talk about how we can </v>
<v Speaker 1>actually predict when the model is </v>

125
00:07:05.691 --> 00:07:09.170
<v Speaker 1>likely to fail,</v>
<v Speaker 1>when we can determine how uncertain does</v>

126
00:07:09.171 --> 00:07:11.750
<v Speaker 1>model is.</v>
<v Speaker 1>When can we trust our end to end system?</v>

127
00:07:11.751 --> 00:07:13.520
<v Speaker 1>So this is another huge challenge in </v>
<v Speaker 1>intent,</v>

128
00:07:14.030 --> 00:07:15.350
<v Speaker 1>uh,</v>
<v Speaker 1>algorithms.</v>

129
00:07:15.410 --> 00:07:16.243
<v Speaker 1>We have to be able to account for the </v>
<v Speaker 1>situations where the model doesn't know </v>

130
00:07:18.981 --> 00:07:22.670
<v Speaker 1>what it's talking about and we have to </v>
<v Speaker 1>be able to accurately predict these.</v>

131
00:07:24.450 --> 00:07:24.860
<v Speaker 2>Yeah.</v>

132
00:07:24.860 --> 00:07:26.180
<v Speaker 1>So let's start with the first part of </v>
<v Speaker 1>the talk,</v>

133
00:07:26.181 --> 00:07:30.140
<v Speaker 1>which is all about parallel autonomy.</v>
<v Speaker 1>All this really means is that we have an</v>

134
00:07:30.141 --> 00:07:33.620
<v Speaker 1>autonomy controller that it's running </v>
<v Speaker 1>two parallel threads in the background.</v>

135
00:07:33.650 --> 00:07:37.760
<v Speaker 1>One is the human and one is the robot or</v>
<v Speaker 1>the standard autonomy system.</v>

136
00:07:39.380 --> 00:07:40.213
<v Speaker 1>And the fusion of these two controls </v>
<v Speaker 1>results in what we call parallel </v>

137
00:07:42.981 --> 00:07:46.760
<v Speaker 1>autonomy or this,</v>
<v Speaker 1>this paradigm of like shared robot human</v>

138
00:07:46.820 --> 00:07:47.653
<v Speaker 1>interaction.</v>
<v Speaker 1>And intuitively the best way that I can </v>

139
00:07:50.931 --> 00:07:55.340
<v Speaker 1>actually think of describing this as </v>
<v Speaker 1>some sort of guardian angel in your car,</v>

140
00:07:55.520 --> 00:07:57.560
<v Speaker 1>preventing you from actually making an </v>
<v Speaker 1>accident.</v>

141
00:07:58.100 --> 00:07:58.933
<v Speaker 1>So let me just play this commercial,</v>
<v Speaker 1>which I think actually sums this up </v>

142
00:08:00.861 --> 00:08:01.910
<v Speaker 1>really,</v>
<v Speaker 1>really nicely.</v>

143
00:08:01.911 --> 00:08:04.040
<v Speaker 1>And this has really captured well,</v>
<v Speaker 1>uh,</v>

144
00:08:04.070 --> 00:08:05.510
<v Speaker 1>through this,</v>
<v Speaker 1>through this clip.</v>

145
00:08:21.430 --> 00:08:26.430
<v Speaker 3>Three,</v>
<v Speaker 3>remember when only dead could say today,</v>

146
00:08:30.310 --> 00:08:32.620
<v Speaker 3>sorry,</v>
<v Speaker 3>that's what auto emergency braking,</v>

147
00:08:34.020 --> 00:08:35.830
<v Speaker 2>right?</v>
<v Speaker 2>So,</v>

148
00:08:36.400 --> 00:08:37.233
<v Speaker 2>okay,</v>

149
00:08:37.440 --> 00:08:39.320
<v Speaker 1>so essentially,</v>
<v Speaker 1>no,</v>

150
00:08:39.321 --> 00:08:40.154
<v Speaker 1>I hope you have some sense of idea what </v>
<v Speaker 1>I'm talking about when I say guardian </v>

151
00:08:42.631 --> 00:08:45.510
<v Speaker 1>angels.</v>
<v Speaker 1>So parallel autonomy is this human robot</v>

152
00:08:45.570 --> 00:08:46.403
<v Speaker 1>shared control paradigm whereby the </v>
<v Speaker 1>human is always in control of the </v>

153
00:08:49.321 --> 00:08:50.154
<v Speaker 1>vehicle,</v>
<v Speaker 1>but the autonomy system is always </v>

154
00:08:51.901 --> 00:08:52.734
<v Speaker 1>running in the background preventing and</v>
<v Speaker 1>is responsible preventing that vehicle </v>

155
00:08:56.551 --> 00:08:57.384
<v Speaker 1>from ever causing an accident.</v>
<v Speaker 1>So you can imagine our software </v>

156
00:08:59.881 --> 00:09:02.370
<v Speaker 1>architecture software pipeline starting </v>
<v Speaker 1>with the country,</v>

157
00:09:02.580 --> 00:09:04.770
<v Speaker 1>the two control inputs.</v>
<v Speaker 1>One is the human,</v>

158
00:09:04.771 --> 00:09:07.140
<v Speaker 1>one is our standard series autonomy </v>
<v Speaker 1>controller.</v>

159
00:09:07.950 --> 00:09:10.650
<v Speaker 1>And then these being fed into a shared </v>
<v Speaker 1>control system,</v>

160
00:09:10.651 --> 00:09:13.140
<v Speaker 1>which is then fed into a low level </v>
<v Speaker 1>tracking control,</v>

161
00:09:13.560 --> 00:09:18.560
<v Speaker 1>which we can feed into a drive by wire </v>
<v Speaker 1>interface as part of our actual uh,</v>

162
00:09:19.610 --> 00:09:20.443
<v Speaker 1>autonomy hardware,</v>
<v Speaker 1>which is just a full scale autonomous </v>

163
00:09:22.591 --> 00:09:23.424
<v Speaker 1>vehicle.</v>
<v Speaker 1>So at Mit we've created a test bed for </v>

164
00:09:28.191 --> 00:09:31.220
<v Speaker 1>autonomous vehicle development.</v>
<v Speaker 1>Specifically we have a fee,</v>

165
00:09:31.290 --> 00:09:35.750
<v Speaker 1>a fleet of autonomous systems consisting</v>
<v Speaker 1>of two cars,</v>

166
00:09:35.780 --> 00:09:36.613
<v Speaker 1>two Toyota Prius's and two wheelchairs,</v>
<v Speaker 1>which we've completely retrofitted with </v>

167
00:09:40.940 --> 00:09:43.280
<v Speaker 1>autonomous drive by wire capabilities.</v>

168
00:09:44.210 --> 00:09:45.043
<v Speaker 1>The wheelchair serve as basically this </v>
<v Speaker 1>development platform for debugging and </v>

169
00:09:49.491 --> 00:09:50.324
<v Speaker 1>developing our algorithms before we </v>
<v Speaker 1>actually moved them onto the full scale </v>

170
00:09:52.561 --> 00:09:53.394
<v Speaker 1>car.</v>
<v Speaker 1>So this is more of like a safety safety </v>

171
00:09:55.791 --> 00:09:58.670
<v Speaker 1>step on our end that we take of </v>
<v Speaker 1>debugging all of our,</v>

172
00:09:58.880 --> 00:10:01.370
<v Speaker 1>all of our algorithms,</v>
<v Speaker 1>all of our code on the wheelchair,</v>

173
00:10:01.371 --> 00:10:05.570
<v Speaker 1>which says the exact same sensor suite </v>
<v Speaker 1>as the full scale cars.</v>

174
00:10:05.571 --> 00:10:07.670
<v Speaker 1>So we'd has all of the cameras,</v>
<v Speaker 1>the Lidars,</v>

175
00:10:07.790 --> 00:10:08.870
<v Speaker 1>the radars,</v>
<v Speaker 1>et cetera.</v>

176
00:10:09.080 --> 00:10:11.480
<v Speaker 1>So it serves as a great test bed before </v>
<v Speaker 1>we moved to the car.</v>

177
00:10:12.170 --> 00:10:14.780
<v Speaker 1>But let's take a look at one of our cars</v>
<v Speaker 1>just to jump straight to it.</v>

178
00:10:15.740 --> 00:10:16.573
<v Speaker 1>So in our car we've installed five lidar</v>
<v Speaker 1>laser scanners for three d imaging of </v>

179
00:10:20.541 --> 00:10:21.374
<v Speaker 1>the environment.</v>

180
00:10:22.660 --> 00:10:23.040
<v Speaker 2>Okay.</v>

181
00:10:23.040 --> 00:10:26.370
<v Speaker 1>Three GMS sell cameras on the front of </v>
<v Speaker 1>the car.</v>

182
00:10:26.371 --> 00:10:28.860
<v Speaker 1>And actually now this is actually a </v>
<v Speaker 1>little bit outdated.</v>

183
00:10:28.861 --> 00:10:32.010
<v Speaker 1>So we've installed two more cameras on </v>
<v Speaker 1>the sides of the car for side imaging as</v>

184
00:10:32.011 --> 00:10:32.844
<v Speaker 1>well.</v>
<v Speaker 1>There's an ost x gps installed for </v>

185
00:10:35.820 --> 00:10:36.653
<v Speaker 1>coarse grain localization,</v>
<v Speaker 1>an imu for collecting data like </v>

186
00:10:39.901 --> 00:10:42.870
<v Speaker 1>acceleration,</v>
<v Speaker 1>rotation and orientation data.</v>

187
00:10:43.740 --> 00:10:48.300
<v Speaker 1>And finally we collect information about</v>
<v Speaker 1>the speed of the car through odometry,</v>

188
00:10:48.370 --> 00:10:49.470
<v Speaker 1>through,</v>
<v Speaker 1>sorry,</v>

189
00:10:49.500 --> 00:10:52.500
<v Speaker 1>through to will and coatings on the back</v>
<v Speaker 1>two wheels.</v>

190
00:10:53.760 --> 00:10:54.320
<v Speaker 2>Okay.</v>

191
00:10:54.320 --> 00:10:55.153
<v Speaker 1>All of the sensors are that fed into our</v>
<v Speaker 1>nvidia drive px to which serves as our </v>

192
00:10:59.691 --> 00:11:04.070
<v Speaker 1>gpu enabled computing platform,</v>
<v Speaker 1>which takes all of this information in,</v>

193
00:11:04.310 --> 00:11:06.440
<v Speaker 1>combines it with our end to end </v>
<v Speaker 1>controllers,</v>

194
00:11:07.100 --> 00:11:07.933
<v Speaker 1>and then sends these two are drive by </v>
<v Speaker 1>wire interface and our custom Ecu Board </v>

195
00:11:11.151 --> 00:11:12.810
<v Speaker 1>to actually,</v>
<v Speaker 1>uh,</v>

196
00:11:13.040 --> 00:11:14.930
<v Speaker 1>command the car and actually control the</v>
<v Speaker 1>car.</v>

197
00:11:16.220 --> 00:11:21.060
<v Speaker 1>The great thing about this [inaudible] </v>
<v Speaker 1>is actually that it has to uh,</v>

198
00:11:21.380 --> 00:11:22.213
<v Speaker 1>onboard gps,</v>
<v Speaker 1>which an eye which allows us to do </v>

199
00:11:24.051 --> 00:11:25.590
<v Speaker 1>really fast,</v>
<v Speaker 1>realtime,</v>

200
00:11:25.640 --> 00:11:27.170
<v Speaker 1>deep neural network in France as well.</v>

201
00:11:29.240 --> 00:11:31.910
<v Speaker 1>Now before going any further,</v>
<v Speaker 1>I want to actually draw this distinction</v>

202
00:11:31.911 --> 00:11:35.480
<v Speaker 1>between shared control and binary </v>
<v Speaker 1>control.</v>

203
00:11:35.930 --> 00:11:40.760
<v Speaker 1>So typically when people think of a </v>
<v Speaker 1>shared control in the autonomous vehicle</v>

204
00:11:40.761 --> 00:11:41.594
<v Speaker 1>setting,</v>
<v Speaker 1>they're thinking of something like </v>

205
00:11:42.380 --> 00:11:45.650
<v Speaker 1>binary control.</v>
<v Speaker 1>And especially in the last talk,</v>

206
00:11:45.651 --> 00:11:50.150
<v Speaker 1>we got this idea of this handoff problem</v>
<v Speaker 1>where the human drivers and control,</v>

207
00:11:50.151 --> 00:11:54.040
<v Speaker 1>but we were trying to figure out when to</v>
<v Speaker 1>hand off control to the autonomous or if</v>

208
00:11:54.041 --> 00:11:54.874
<v Speaker 1>the autonomous system is in control.</v>
<v Speaker 1>We want to figure out when did tonto </v>

209
00:11:57.091 --> 00:11:57.924
<v Speaker 1>system doesn't know what's going on and </v>
<v Speaker 1>I can hand off control back to the </v>

210
00:11:59.561 --> 00:12:00.394
<v Speaker 1>human.</v>
<v Speaker 1>This is actually a much simpler problem </v>

211
00:12:02.891 --> 00:12:05.680
<v Speaker 1>than what we're doing with in our lab,</v>
<v Speaker 1>which is shared control,</v>

212
00:12:05.681 --> 00:12:06.514
<v Speaker 1>which incentive?</v>
<v Speaker 1>This binary switch we're dealing with </v>

213
00:12:08.351 --> 00:12:10.840
<v Speaker 1>actually a fusion of the two control </v>
<v Speaker 1>systems.</v>

214
00:12:12.950 --> 00:12:16.820
<v Speaker 1>So not only do we have to decide when,</v>
<v Speaker 1>which system to trust more,</v>

215
00:12:17.000 --> 00:12:17.833
<v Speaker 1>but we actually have to turn in the best</v>
<v Speaker 1>way to fuse them together in a very </v>

216
00:12:21.141 --> 00:12:25.070
<v Speaker 1>natural manner such that the human who </v>
<v Speaker 1>is always driving the car,</v>

217
00:12:25.071 --> 00:12:27.830
<v Speaker 1>it doesn't feel like they're losing </v>
<v Speaker 1>control of what they're doing.</v>

218
00:12:29.060 --> 00:12:29.630
<v Speaker 2>Okay.</v>

219
00:12:29.630 --> 00:12:30.463
<v Speaker 1>This actually results in us not being </v>
<v Speaker 1>able to rely on many of the classical </v>

220
00:12:33.131 --> 00:12:36.160
<v Speaker 1>techniques for controlling our car,</v>
<v Speaker 1>uh,</v>

221
00:12:36.220 --> 00:12:40.780
<v Speaker 1>which a lot of other industries and uh,</v>
<v Speaker 1>academic institutions have,</v>

222
00:12:41.390 --> 00:12:42.700
<v Speaker 1>um,</v>
<v Speaker 1>have have,</v>

223
00:12:42.710 --> 00:12:43.570
<v Speaker 1>um,</v>
<v Speaker 1>gone through.</v>

224
00:12:44.860 --> 00:12:45.693
<v Speaker 1>So I'd actually like to take a second </v>
<v Speaker 1>just through this slide and actually </v>

225
00:12:48.041 --> 00:12:50.440
<v Speaker 1>talk about how we achieve control of our</v>
<v Speaker 1>cars.</v>

226
00:12:50.950 --> 00:12:51.783
<v Speaker 1>So one option is that we can,</v>
<v Speaker 1>we can do this by actually connecting a </v>

227
00:12:55.451 --> 00:12:56.284
<v Speaker 1>motor straight to the steering wheel or </v>
<v Speaker 1>to the throttle and actually commanding </v>

228
00:13:00.671 --> 00:13:05.080
<v Speaker 1>the internal engine of this motor to,</v>
<v Speaker 1>to turn the steering wheel itself.</v>

229
00:13:05.110 --> 00:13:07.570
<v Speaker 1>And this suffers a lot from,</v>
<v Speaker 1>we actually tried this,</v>

230
00:13:07.571 --> 00:13:09.930
<v Speaker 1>it's very difficult to get this kind of </v>
<v Speaker 1>system working.</v>

231
00:13:09.950 --> 00:13:12.010
<v Speaker 1>It's very fragile and practice very </v>
<v Speaker 1>brittle,</v>

232
00:13:12.710 --> 00:13:13.543
<v Speaker 1>suffers from responsiveness,</v>
<v Speaker 1>also durability over time as you can </v>

233
00:13:16.031 --> 00:13:17.830
<v Speaker 1>imagine.</v>
<v Speaker 1>The second way,</v>

234
00:13:17.831 --> 00:13:18.664
<v Speaker 1>which is really the classical way in </v>
<v Speaker 1>many industries that mini intercities </v>

235
00:13:21.360 --> 00:13:25.210
<v Speaker 1>I've taking is to actually send canned </v>
<v Speaker 1>messages to the board,</v>

236
00:13:25.270 --> 00:13:26.103
<v Speaker 1>to the car.</v>

237
00:13:26.290 --> 00:13:27.123
<v Speaker 1>Ken is like this,</v>
<v Speaker 1>a language that cars can take to </v>

238
00:13:29.710 --> 00:13:31.510
<v Speaker 1>actually communicate with different </v>
<v Speaker 1>parts of the car.</v>

239
00:13:31.511 --> 00:13:32.344
<v Speaker 1>So one way that you can imagine,</v>
<v Speaker 1>it's us just sending the canned codes </v>

240
00:13:35.111 --> 00:13:38.770
<v Speaker 1>for steering wheel angle to the car and </v>
<v Speaker 1>then controlling it like that.</v>

241
00:13:39.130 --> 00:13:41.890
<v Speaker 1>This actually is not possible in a </v>
<v Speaker 1>shared control paradigm.</v>

242
00:13:41.891 --> 00:13:44.470
<v Speaker 1>So this is possible if we're talking </v>
<v Speaker 1>about binary control,</v>

243
00:13:44.650 --> 00:13:45.483
<v Speaker 1>but in shared control,</v>
<v Speaker 1>we actually want to read the effort of </v>

244
00:13:48.701 --> 00:13:51.880
<v Speaker 1>the human on the wheel.</v>
<v Speaker 1>We don't care about that per se.</v>

245
00:13:51.881 --> 00:13:55.420
<v Speaker 1>We don't care about the angle that the </v>
<v Speaker 1>wheel is turned.</v>

246
00:13:55.450 --> 00:13:58.360
<v Speaker 1>We really care about how much effort is </v>
<v Speaker 1>being exerted on the wheel,</v>

247
00:13:59.310 --> 00:14:01.910
<v Speaker 1>right?</v>
<v Speaker 1>So something more like we care about the</v>

248
00:14:01.911 --> 00:14:05.510
<v Speaker 1>torque rather than the actual angle if </v>
<v Speaker 1>the wheel at that instant.</v>

249
00:14:06.230 --> 00:14:08.750
<v Speaker 1>So instead we have to turn to this third</v>
<v Speaker 1>approach,</v>

250
00:14:08.751 --> 00:14:09.584
<v Speaker 1>which is actually what we did a which is</v>
<v Speaker 1>actually spoofing the input systems of </v>

251
00:14:13.221 --> 00:14:14.054
<v Speaker 1>the car.</v>

252
00:14:14.560 --> 00:14:14.810
<v Speaker 2>Okay.</v>

253
00:14:14.810 --> 00:14:15.643
<v Speaker 1>This essentially means that we've </v>
<v Speaker 1>created drive by wire capability by </v>

254
00:14:18.021 --> 00:14:21.290
<v Speaker 1>sending the car like fake or synthetic </v>
<v Speaker 1>data.</v>

255
00:14:21.740 --> 00:14:25.340
<v Speaker 1>This makes it believe that the human is </v>
<v Speaker 1>still controlling it,</v>

256
00:14:25.700 --> 00:14:26.533
<v Speaker 1>but it is not able to accurately observe</v>
<v Speaker 1>the observation at the human is doing </v>

257
00:14:28.910 --> 00:14:29.743
<v Speaker 1>because we're spoofing the input signals</v>
<v Speaker 1>and combining them with our autonomy </v>

258
00:14:32.391 --> 00:14:33.224
<v Speaker 1>controller.</v>

259
00:14:33.340 --> 00:14:34.173
<v Speaker 2>Okay.</v>

260
00:14:34.420 --> 00:14:36.520
<v Speaker 1>And this is what we've created in our </v>
<v Speaker 1>lab.</v>

261
00:14:37.360 --> 00:14:41.170
<v Speaker 1>This approach really leads to a very </v>
<v Speaker 1>unique a control system.</v>

262
00:14:41.730 --> 00:14:43.270
<v Speaker 1>Um,</v>
<v Speaker 1>and to summarize,</v>

263
00:14:43.271 --> 00:14:47.560
<v Speaker 1>this really creates three distinct </v>
<v Speaker 1>autonomous modes.</v>

264
00:14:47.830 --> 00:14:48.663
<v Speaker 1>And the first is just manual control.</v>
<v Speaker 1>This just means that the human is in </v>

265
00:14:51.381 --> 00:14:54.590
<v Speaker 1>complete control of the car.</v>
<v Speaker 1>This is what we use when we're trying to</v>

266
00:14:54.591 --> 00:14:55.424
<v Speaker 1>focus on things like data collection.</v>
<v Speaker 1>And on the other extreme you can see </v>

267
00:14:59.390 --> 00:15:01.130
<v Speaker 1>the,</v>
<v Speaker 1>when the computer controls the car.</v>

268
00:15:01.131 --> 00:15:01.964
<v Speaker 1>So this is when you have no human input.</v>
<v Speaker 1>And this is what people usually talk </v>

269
00:15:05.601 --> 00:15:07.730
<v Speaker 1>about when they talk about autonomous </v>
<v Speaker 1>vehicles.</v>

270
00:15:07.731 --> 00:15:10.340
<v Speaker 1>So this is just the computer controlling</v>
<v Speaker 1>of the car.</v>

271
00:15:11.060 --> 00:15:14.810
<v Speaker 1>But also what's unique here that I'd </v>
<v Speaker 1>like to really hammer home this point is</v>

272
00:15:14.811 --> 00:15:15.644
<v Speaker 1>that we've created this third mode,</v>
<v Speaker 1>which is a combination of both manual </v>

273
00:15:18.471 --> 00:15:20.630
<v Speaker 1>and computer,</v>
<v Speaker 1>which we call parallel autonomy,</v>

274
00:15:21.370 --> 00:15:22.203
<v Speaker 1>um,</v>
<v Speaker 1>where the robot is simultaneously </v>

275
00:15:23.901 --> 00:15:24.734
<v Speaker 1>observing the human but also actuating </v>
<v Speaker 1>with the human in the shirt controlled </v>

276
00:15:28.560 --> 00:15:32.780
<v Speaker 1>powerline paradigm.</v>
<v Speaker 1>All three of these modes are part of the</v>

277
00:15:32.781 --> 00:15:33.614
<v Speaker 1>mit tri autonomous driving pipeline and </v>
<v Speaker 1>this allows us to really test and debug </v>

278
00:15:37.130 --> 00:15:41.360
<v Speaker 1>many of these algorithms I'm going to be</v>
<v Speaker 1>talking about in this talk.</v>

279
00:15:41.390 --> 00:15:42.223
<v Speaker 1>I'm just going to talk about one of the </v>
<v Speaker 1>algorithms that we've created in my lab </v>

280
00:15:46.550 --> 00:15:47.383
<v Speaker 1>to actually learn the steering control </v>
<v Speaker 1>of a car but not only learned to </v>

281
00:15:50.481 --> 00:15:52.280
<v Speaker 1>control,</v>
<v Speaker 1>but actually learned the balance,</v>

282
00:15:52.400 --> 00:15:55.610
<v Speaker 1>the safe and valid bounds for </v>
<v Speaker 1>controlling in that environment.</v>

283
00:15:57.720 --> 00:15:58.553
<v Speaker 1>So there's actually been a lot of work </v>
<v Speaker 1>and using machine learning to control a </v>

284
00:16:02.071 --> 00:16:02.904
<v Speaker 1>car using raw perception data.</v>
<v Speaker 1>Most notably work here from Nvidia has </v>

285
00:16:07.741 --> 00:16:08.574
<v Speaker 1>shown that it is possible to build a </v>
<v Speaker 1>neural network that will take in as </v>

286
00:16:12.121 --> 00:16:15.330
<v Speaker 1>input a single frame of an image of a </v>
<v Speaker 1>camera,</v>

287
00:16:15.331 --> 00:16:16.164
<v Speaker 1>sorry,</v>
<v Speaker 1>and directly control the steering wheel </v>

288
00:16:18.541 --> 00:16:21.210
<v Speaker 1>angle if that car.</v>
<v Speaker 1>So since then there have been many,</v>

289
00:16:21.211 --> 00:16:25.530
<v Speaker 1>many extensions of this work attempting </v>
<v Speaker 1>very similar solutions.</v>

290
00:16:25.680 --> 00:16:28.800
<v Speaker 1>Some using LSTM is to capture a temporal</v>
<v Speaker 1>dependencies.</v>

291
00:16:29.310 --> 00:16:32.310
<v Speaker 1>Others utilize utilizing techniques from</v>
<v Speaker 1>imitation learning.</v>

292
00:16:32.340 --> 00:16:34.000
<v Speaker 1>But all of these,</v>
<v Speaker 1>uh,</v>

293
00:16:34.180 --> 00:16:36.540
<v Speaker 1>works really suffer from the same </v>
<v Speaker 1>problem.</v>

294
00:16:36.541 --> 00:16:37.374
<v Speaker 1>So the key problem here that they all </v>
<v Speaker 1>face is that they lack this notion of </v>

295
00:16:43.230 --> 00:16:46.770
<v Speaker 1>the ability to feed into higher level </v>
<v Speaker 1>navigational controllers.</v>

296
00:16:47.310 --> 00:16:49.350
<v Speaker 1>So taking videos and to a network,</v>
<v Speaker 1>for example,</v>

297
00:16:49.351 --> 00:16:53.730
<v Speaker 1>this network outputs a single real </v>
<v Speaker 1>valued number at the output.</v>

298
00:16:54.990 --> 00:16:58.140
<v Speaker 1>What this means is that it is impossible</v>
<v Speaker 1>for that single real value number,</v>

299
00:16:58.141 --> 00:17:01.890
<v Speaker 1>which just represents a steering wheel </v>
<v Speaker 1>angle to capture any form of uncertainty</v>

300
00:17:01.891 --> 00:17:02.724
<v Speaker 1>measurement.</v>
<v Speaker 1>It also means it's impossible for that </v>

301
00:17:04.830 --> 00:17:08.480
<v Speaker 1>number to capture different possible </v>
<v Speaker 1>ambiguous situations.</v>

302
00:17:08.481 --> 00:17:09.314
<v Speaker 1>So imagine if you're at an intersection </v>
<v Speaker 1>and there are three different ways that </v>

303
00:17:11.821 --> 00:17:12.654
<v Speaker 1>you can go.</v>
<v Speaker 1>If you only have one output of your </v>

304
00:17:14.281 --> 00:17:15.114
<v Speaker 1>neural network,</v>
<v Speaker 1>it's impossible for that network to </v>

305
00:17:17.101 --> 00:17:20.220
<v Speaker 1>handle anything other than an ambiguous </v>
<v Speaker 1>situations.</v>

306
00:17:20.880 --> 00:17:22.830
<v Speaker 1>Thus,</v>
<v Speaker 1>this really makes all of these solutions</v>

307
00:17:22.831 --> 00:17:25.620
<v Speaker 1>ill suited to be used in conjunction </v>
<v Speaker 1>with higher levels,</v>

308
00:17:25.630 --> 00:17:29.820
<v Speaker 1>navigational control,</v>
<v Speaker 1>which must be able to handle uncertainty</v>

309
00:17:29.821 --> 00:17:33.210
<v Speaker 1>in the environment and also ambiguity in</v>
<v Speaker 1>your decision making.</v>

310
00:17:35.180 --> 00:17:36.290
<v Speaker 1>So instead,</v>
<v Speaker 1>in this work,</v>

311
00:17:36.291 --> 00:17:40.250
<v Speaker 1>what we attempt to do is actually learn </v>
<v Speaker 1>a full probability distribution over the</v>

312
00:17:40.251 --> 00:17:41.084
<v Speaker 1>control actions.</v>
<v Speaker 1>And we do this by first district ising </v>

313
00:17:44.000 --> 00:17:44.833
<v Speaker 1>our entire action space of all steering </v>
<v Speaker 1>wheel angles to handle ambiguity and </v>

314
00:17:49.501 --> 00:17:52.320
<v Speaker 1>then learn a neural network to output </v>
<v Speaker 1>this discrete distribution.</v>

315
00:17:53.490 --> 00:17:54.323
<v Speaker 1>Then we transformed this discrete </v>
<v Speaker 1>distribution into a continuous </v>

316
00:17:57.151 --> 00:18:00.310
<v Speaker 1>probability density function or pdf,</v>
<v Speaker 1>which could,</v>

317
00:18:00.390 --> 00:18:05.260
<v Speaker 1>which we can then use to analytically </v>
<v Speaker 1>extract control bounds,</v>

318
00:18:05.570 --> 00:18:07.770
<v Speaker 1>uh,</v>
<v Speaker 1>that we can use in our parallel autonomy</v>

319
00:18:07.771 --> 00:18:08.604
<v Speaker 1>framework.</v>
<v Speaker 1>So let's break this pipeline down a </v>

320
00:18:11.311 --> 00:18:13.770
<v Speaker 1>little bit more to go into it a little </v>
<v Speaker 1>more detail.</v>

321
00:18:14.580 --> 00:18:15.413
<v Speaker 1>So first,</v>
<v Speaker 1>let's focus on this part where we just </v>

322
00:18:16.591 --> 00:18:17.424
<v Speaker 1>take a single input image frame and we </v>
<v Speaker 1>want to predict a one dimensional </v>

323
00:18:22.770 --> 00:18:27.770
<v Speaker 1>discreet probability density function.</v>
<v Speaker 1>So Xii is the image from the data set of</v>

324
00:18:27.811 --> 00:18:30.480
<v Speaker 1>m images.</v>
<v Speaker 1>We want to learn some sort of functional</v>

325
00:18:30.481 --> 00:18:35.481
<v Speaker 1>mapping f with parameters data to </v>
<v Speaker 1>predict this probability distribution on</v>

326
00:18:35.491 --> 00:18:38.070
<v Speaker 1>the right and F for us,</v>
<v Speaker 1>like I said,</v>

327
00:18:38.071 --> 00:18:40.590
<v Speaker 1>is a deep neural network and data or </v>
<v Speaker 1>just the weights.</v>

328
00:18:40.980 --> 00:18:41.813
<v Speaker 1>So sinceF is just a deep neural network,</v>
<v Speaker 1>we can optimize this end to end using </v>

329
00:18:46.201 --> 00:18:49.440
<v Speaker 1>backpropagation,</v>
<v Speaker 1>which entails effectively minimizing the</v>

330
00:18:49.441 --> 00:18:52.380
<v Speaker 1>cross entropy loss for all of our m </v>
<v Speaker 1>examples.</v>

331
00:18:53.040 --> 00:18:53.873
<v Speaker 1>So note here that the true distribution,</v>
<v Speaker 1>which I did notice Pii is just the </v>

332
00:18:58.981 --> 00:19:01.650
<v Speaker 1>distribution that the human took at that</v>
<v Speaker 1>time.</v>

333
00:19:02.760 --> 00:19:03.870
<v Speaker 1>I,</v>
<v Speaker 1>or sorry,</v>

334
00:19:03.950 --> 00:19:05.700
<v Speaker 1>the,</v>
<v Speaker 1>the um,</v>

335
00:19:05.880 --> 00:19:09.930
<v Speaker 1>estimate distribution is what we tried </v>
<v Speaker 1>to get as close as possible to the,</v>

336
00:19:10.140 --> 00:19:10.973
<v Speaker 1>to the true distribution.</v>
<v Speaker 1>But keep in mind here that the tree </v>

337
00:19:13.601 --> 00:19:16.200
<v Speaker 1>distribution that the human actually </v>
<v Speaker 1>took at that time,</v>

338
00:19:16.201 --> 00:19:18.210
<v Speaker 1>it's actually a very simple </v>
<v Speaker 1>distribution.</v>

339
00:19:18.211 --> 00:19:20.010
<v Speaker 1>It's just a Delta function,</v>
<v Speaker 1>right?</v>

340
00:19:20.011 --> 00:19:20.844
<v Speaker 1>So it has the value of one in the </v>
<v Speaker 1>direction that this human drove and a </v>

341
00:19:23.611 --> 00:19:24.444
<v Speaker 1>value of zero everywhere else.</v>
<v Speaker 1>So this is really remarkable because </v>

342
00:19:27.961 --> 00:19:32.961
<v Speaker 1>what I'm telling you is that given data </v>
<v Speaker 1>of only union modal actions,</v>

343
00:19:34.080 --> 00:19:36.870
<v Speaker 1>we're attempting to learn in multimodal </v>
<v Speaker 1>distribution,</v>

344
00:19:38.010 --> 00:19:38.490
<v Speaker 1>right?</v>

345
00:19:38.490 --> 00:19:42.210
<v Speaker 1>So imagine a scene like an intersection </v>
<v Speaker 1>like this so he can see that there's two</v>

346
00:19:42.211 --> 00:19:43.044
<v Speaker 1>possible actions a person can take.</v>
<v Speaker 1>Either go left or go right and we want </v>

347
00:19:46.711 --> 00:19:49.860
<v Speaker 1>to train a,</v>
<v Speaker 1>let's suppose a very simple three output</v>

348
00:19:49.861 --> 00:19:54.861
<v Speaker 1>neural network that can handle these </v>
<v Speaker 1>left right or straight actions.</v>

349
00:19:55.950 --> 00:19:57.870
<v Speaker 1>So when we started out,</v>
<v Speaker 1>we haven't seen any training data,</v>

350
00:19:57.871 --> 00:19:59.970
<v Speaker 1>so we may see something random that it's</v>
<v Speaker 1>outputting.</v>

351
00:20:00.870 --> 00:20:03.840
<v Speaker 1>But now if the human takes an action and</v>
<v Speaker 1>tries to turn left,</v>

352
00:20:04.270 --> 00:20:05.103
<v Speaker 1>the neural network learns from this,</v>
<v Speaker 1>it starts to promote the left action a </v>

353
00:20:09.211 --> 00:20:10.044
<v Speaker 1>little bit more.</v>
<v Speaker 1>You can actually see that on the top </v>

354
00:20:11.971 --> 00:20:12.804
<v Speaker 1>left here.</v>
<v Speaker 1>This is an example of what the true </v>

355
00:20:14.101 --> 00:20:17.250
<v Speaker 1>distribution for what the human actually</v>
<v Speaker 1>took would look like.</v>

356
00:20:17.251 --> 00:20:18.084
<v Speaker 1>So it has a value of one when it went on</v>
<v Speaker 1>the bin that corresponds to left turns </v>

357
00:20:22.170 --> 00:20:23.003
<v Speaker 1>and Zeros on the other two bins.</v>
<v Speaker 1>If on the next intersection in our </v>

358
00:20:28.321 --> 00:20:30.690
<v Speaker 1>training dead or we see a similar </v>
<v Speaker 1>intersection like this,</v>

359
00:20:30.691 --> 00:20:33.150
<v Speaker 1>it doesn't necessarily have to be the </v>
<v Speaker 1>exact same intersection,</v>

360
00:20:34.050 --> 00:20:34.883
<v Speaker 1>but the human turned left again.</v>
<v Speaker 1>So now the network will now start to </v>

361
00:20:37.441 --> 00:20:38.274
<v Speaker 1>really confidently promote this left </v>
<v Speaker 1>turn action even more and discouraged </v>

362
00:20:42.720 --> 00:20:45.210
<v Speaker 1>straight and right,</v>
<v Speaker 1>like I mentioned before.</v>

363
00:20:47.030 --> 00:20:48.200
<v Speaker 1>Now finally,</v>
<v Speaker 1>let's see something</v>

364
00:20:48.200 --> 00:20:50.210
<v Speaker 1>interesting happening.</v>
<v Speaker 1>So now finally,</v>

365
00:20:50.240 --> 00:20:51.740
<v Speaker 1>the human decisive turn,</v>
<v Speaker 1>right?</v>

366
00:20:52.250 --> 00:20:55.280
<v Speaker 1>The network is starting to learn both </v>
<v Speaker 1>actions by promoting left,</v>

367
00:20:55.340 --> 00:20:59.190
<v Speaker 1>by promoting the right action.</v>
<v Speaker 1>And actually it's starting to discourage</v>

368
00:20:59.191 --> 00:21:01.790
<v Speaker 1>left action even though it,</v>
<v Speaker 1>it learned that before.</v>

369
00:21:01.791 --> 00:21:02.624
<v Speaker 1>So it actually starts to forget some of </v>
<v Speaker 1>that information that I saw before in </v>

370
00:21:05.981 --> 00:21:06.814
<v Speaker 1>bye bye.</v>
<v Speaker 1>In the process of promoting the current </v>

371
00:21:08.781 --> 00:21:12.200
<v Speaker 1>action,</v>
<v Speaker 1>if we keep taking right turns,</v>

372
00:21:12.201 --> 00:21:13.034
<v Speaker 1>we keep promoting the right action and </v>
<v Speaker 1>we start to lose some of the power on </v>

373
00:21:16.191 --> 00:21:20.270
<v Speaker 1>the left.</v>
<v Speaker 1>And if we keep seeing straight runs,</v>

374
00:21:20.271 --> 00:21:22.910
<v Speaker 1>eventually the model even learned that,</v>
<v Speaker 1>right?</v>

375
00:21:22.970 --> 00:21:24.530
<v Speaker 1>Sorry.</v>
<v Speaker 1>If we keep seeing,</v>

376
00:21:25.350 --> 00:21:26.890
<v Speaker 1>uh,</v>
<v Speaker 1>right turns the model,</v>

377
00:21:26.891 --> 00:21:27.724
<v Speaker 1>we'll learn that a right turn is perhaps</v>
<v Speaker 1>even more likely than a left turn in </v>

378
00:21:31.151 --> 00:21:36.151
<v Speaker 1>this particular scenario.</v>
<v Speaker 1>So over time we eventually expect to see</v>

379
00:21:36.561 --> 00:21:37.394
<v Speaker 1>some sort of convergence where the human</v>
<v Speaker 1>in our dre training Dataset has </v>

380
00:21:41.811 --> 00:21:45.560
<v Speaker 1>collected enough data supporting both </v>
<v Speaker 1>left and right action.</v>

381
00:21:45.561 --> 00:21:48.830
<v Speaker 1>So we expect to converge to some </v>
<v Speaker 1>probability distribution,</v>

382
00:21:48.831 --> 00:21:51.080
<v Speaker 1>which is like a 0.5</v>
<v Speaker 1>on left and 0.5</v>

383
00:21:51.081 --> 00:21:52.160
<v Speaker 1>on right and zero</v>

384
00:21:54.180 --> 00:21:57.360
<v Speaker 2>everyone else.</v>
<v Speaker 2>Okay,</v>

385
00:21:58.020 --> 00:21:58.853
<v Speaker 2>sorry.</v>

386
00:22:01.390 --> 00:22:02.223
<v Speaker 1>So this is really awesome because it </v>
<v Speaker 1>shows us that we can actually learn </v>

387
00:22:04.661 --> 00:22:08.290
<v Speaker 1>multimodal distributions using only uni </v>
<v Speaker 1>modal ground truth data.</v>

388
00:22:09.440 --> 00:22:10.273
<v Speaker 1>So this eliminates the need to actually </v>
<v Speaker 1>manually label each possible control </v>

389
00:22:13.851 --> 00:22:15.440
<v Speaker 1>action,</v>
<v Speaker 1>right?</v>

390
00:22:15.980 --> 00:22:17.480
<v Speaker 1>That the car could have taken at that </v>
<v Speaker 1>frame.</v>

391
00:22:17.481 --> 00:22:19.700
<v Speaker 1>So we're using the raw data collected </v>
<v Speaker 1>from the,</v>

392
00:22:19.730 --> 00:22:22.760
<v Speaker 1>from the human in that environment,</v>
<v Speaker 1>and instead we can actually use,</v>

393
00:22:23.600 --> 00:22:24.433
<v Speaker 1>we don't even need to keep driving over </v>
<v Speaker 1>the exact same intersection over and </v>

394
00:22:26.811 --> 00:22:29.240
<v Speaker 1>over.</v>
<v Speaker 1>What we're learning here is some sort of</v>

395
00:22:29.241 --> 00:22:32.540
<v Speaker 1>representation of where the,</v>
<v Speaker 1>um,</v>

396
00:22:32.750 --> 00:22:35.240
<v Speaker 1>where is the drivable space in this </v>
<v Speaker 1>region?</v>

397
00:22:36.090 --> 00:22:36.923
<v Speaker 2>Yeah.</v>

398
00:22:37.630 --> 00:22:38.463
<v Speaker 1>Uh,</v>
<v Speaker 1>this is especially powerful because it </v>

399
00:22:39.761 --> 00:22:44.290
<v Speaker 1>means that we are not constrained to </v>
<v Speaker 1>predefined types of intersections like t</v>

400
00:22:44.291 --> 00:22:46.840
<v Speaker 1>way t intersections for way </v>
<v Speaker 1>intersections.</v>

401
00:22:46.990 --> 00:22:50.650
<v Speaker 1>We are simply learning about drivable </v>
<v Speaker 1>space in the environment.</v>

402
00:22:50.651 --> 00:22:54.460
<v Speaker 1>And this is a very powerful notion.</v>
<v Speaker 1>So to test this,</v>

403
00:22:54.461 --> 00:22:59.461
<v Speaker 1>we collected seven hours of driving data</v>
<v Speaker 1>and the Greater Boston area.</v>

404
00:23:00.490 --> 00:23:03.790
<v Speaker 1>This data consisted mostly of highway </v>
<v Speaker 1>and city roads.</v>

405
00:23:04.120 --> 00:23:08.830
<v Speaker 1>And since our test environment where we </v>
<v Speaker 1>actually test our cars is very different</v>

406
00:23:08.831 --> 00:23:12.050
<v Speaker 1>than this training environment.</v>
<v Speaker 1>We decided to do a bit of fine tuning on</v>

407
00:23:12.051 --> 00:23:12.884
<v Speaker 1>our models on roads without lane markers</v>
<v Speaker 1>to teach the network to be able to </v>

408
00:23:16.961 --> 00:23:20.860
<v Speaker 1>handle some of some of these unmarked </v>
<v Speaker 1>roads as well.</v>

409
00:23:21.890 --> 00:23:22.723
<v Speaker 1>So we feed all this data into our Nvidia</v>
<v Speaker 1>dgx one to actually train our models </v>

410
00:23:26.781 --> 00:23:30.170
<v Speaker 1>using data parallelism on three deep on </v>
<v Speaker 1>three gps.</v>

411
00:23:30.200 --> 00:23:32.810
<v Speaker 1>And this takes around one hour in </v>
<v Speaker 1>practice.</v>

412
00:23:34.880 --> 00:23:35.200
<v Speaker 2>Okay,</v>

413
00:23:35.200 --> 00:23:36.033
<v Speaker 1>so I'm going to go over the next step of</v>
<v Speaker 1>the pipeline before sharing some </v>

414
00:23:38.500 --> 00:23:39.333
<v Speaker 1>exciting results of the,</v>
<v Speaker 1>of the system actually working in </v>

415
00:23:41.381 --> 00:23:42.214
<v Speaker 1>practice.</v>
<v Speaker 1>So now since we've learned a meaningful </v>

416
00:23:44.561 --> 00:23:48.650
<v Speaker 1>discrete representation of where the </v>
<v Speaker 1>drivable spaces,</v>

417
00:23:48.680 --> 00:23:49.513
<v Speaker 1>so what our control commands that the </v>
<v Speaker 1>car can actually execute at that </v>

418
00:23:52.821 --> 00:23:53.654
<v Speaker 1>instant,</v>
<v Speaker 1>we want to now transform this </v>

419
00:23:55.401 --> 00:23:59.930
<v Speaker 1>distribution into a continuous pdf </v>
<v Speaker 1>defined by a mixture of Gaussians.</v>

420
00:24:00.460 --> 00:24:01.293
<v Speaker 1>And specifically you can think of each </v>
<v Speaker 1>of these coushins as representing I </v>

421
00:24:03.831 --> 00:24:04.664
<v Speaker 1>possible action that the human could </v>
<v Speaker 1>take or that the car could take at that </v>

422
00:24:08.151 --> 00:24:09.650
<v Speaker 1>instant,</v>
<v Speaker 1>right?</v>

423
00:24:09.651 --> 00:24:13.340
<v Speaker 1>So if there's a multimodal distribution </v>
<v Speaker 1>like you can see here,</v>

424
00:24:14.000 --> 00:24:14.833
<v Speaker 1>so this has two modes.</v>
<v Speaker 1>This is corresponding to a situation </v>

425
00:24:18.231 --> 00:24:19.064
<v Speaker 1>where the car can either turn left or </v>
<v Speaker 1>right and we'd expect to coushins to </v>

426
00:24:22.490 --> 00:24:23.323
<v Speaker 1>roughly appear from this distribution.</v>
<v Speaker 1>So the mean simply represents the </v>

427
00:24:27.561 --> 00:24:31.010
<v Speaker 1>steering wheel angle that is most likely</v>
<v Speaker 1>or most confidence.</v>

428
00:24:31.011 --> 00:24:34.490
<v Speaker 1>So this is the angle that the car should</v>
<v Speaker 1>actually be traveling in for that mode.</v>

429
00:24:35.150 --> 00:24:39.800
<v Speaker 1>The variant simply reflects some sort of</v>
<v Speaker 1>uncertainty about that mean.</v>

430
00:24:39.920 --> 00:24:40.753
<v Speaker 1>So the larger the variance,</v>
<v Speaker 1>the more uncertain you are about </v>

431
00:24:42.681 --> 00:24:45.470
<v Speaker 1>following that exact a steering wheel </v>
<v Speaker 1>angle.</v>

432
00:24:45.770 --> 00:24:46.603
<v Speaker 1>And finally,</v>
<v Speaker 1>the power is simply just a scale and </v>

433
00:24:47.941 --> 00:24:51.230
<v Speaker 1>multiple that we multiply it by each </v>
<v Speaker 1>mode of our distribution.</v>

434
00:24:51.500 --> 00:24:53.630
<v Speaker 1>To simply scale up,</v>
<v Speaker 1>scale it up and down.</v>

435
00:24:55.640 --> 00:24:56.473
<v Speaker 1>And we do this transformation from </v>
<v Speaker 1>discreet to continuous by using this </v>

436
00:24:59.031 --> 00:24:59.864
<v Speaker 1>technique,</v>
<v Speaker 1>which is known as very rich variational </v>

437
00:25:01.341 --> 00:25:03.170
<v Speaker 1>basion.</v>
<v Speaker 1>Next you're modeling.</v>

438
00:25:03.230 --> 00:25:05.600
<v Speaker 1>And if you've ever heard of expectation </v>
<v Speaker 1>maximization,</v>

439
00:25:06.080 --> 00:25:11.080
<v Speaker 1>this is a technique that's very similar.</v>
<v Speaker 1>So an e m or expectation maximization,</v>

440
00:25:11.420 --> 00:25:12.253
<v Speaker 1>you have to actually predefine the </v>
<v Speaker 1>number of modes that you expect your </v>

441
00:25:15.261 --> 00:25:16.094
<v Speaker 1>distribution to have.</v>
<v Speaker 1>And this is really the difference with </v>

442
00:25:19.040 --> 00:25:19.873
<v Speaker 1>variational Bayes,</v>
<v Speaker 1>is that you don't have to predefine the </v>

443
00:25:21.681 --> 00:25:24.710
<v Speaker 1>number of modes.</v>
<v Speaker 1>You simply have to define an upper bound</v>

444
00:25:24.890 --> 00:25:25.723
<v Speaker 1>to the maximum number of modes that you </v>
<v Speaker 1>would ever expect to encounter while </v>

445
00:25:28.281 --> 00:25:31.130
<v Speaker 1>you're driving.</v>
<v Speaker 1>So for us,</v>

446
00:25:31.131 --> 00:25:31.964
<v Speaker 1>we use something like 15 modes.</v>
<v Speaker 1>We never expect to see more than 15 </v>

447
00:25:34.881 --> 00:25:37.010
<v Speaker 1>possible different actions to take on </v>
<v Speaker 1>the road.</v>

448
00:25:37.310 --> 00:25:38.143
<v Speaker 1>So this is really an upper bound.</v>
<v Speaker 1>And then what happens is we feed in our </v>

449
00:25:40.341 --> 00:25:41.174
<v Speaker 1>discrete distribution,</v>
<v Speaker 1>then iteratively we start to kill off </v>

450
00:25:45.050 --> 00:25:49.850
<v Speaker 1>modes that lose that,</v>
<v Speaker 1>that lose power,</v>

451
00:25:50.120 --> 00:25:50.953
<v Speaker 1>right?</v>
<v Speaker 1>So I won't go into too much detail on </v>

452
00:25:52.641 --> 00:25:55.130
<v Speaker 1>how this actually happens,</v>
<v Speaker 1>but this is really the,</v>

453
00:25:55.570 --> 00:25:56.403
<v Speaker 1>the amazing thing about variational </v>
<v Speaker 1>bayes compared to something like </v>

454
00:25:59.150 --> 00:26:01.490
<v Speaker 1>expectation maximization.</v>
<v Speaker 1>Um,</v>

455
00:26:01.550 --> 00:26:03.530
<v Speaker 1>and I encourage you all to like look up </v>
<v Speaker 1>online.</v>

456
00:26:03.531 --> 00:26:04.364
<v Speaker 1>This is a very well known technique if </v>
<v Speaker 1>you're interested in learning some of </v>

457
00:26:06.471 --> 00:26:07.670
<v Speaker 1>the math behind the algorithm.</v>

458
00:26:08.420 --> 00:26:09.253
<v Speaker 2>Okay.</v>

459
00:26:09.460 --> 00:26:14.280
<v Speaker 1>Eventually we converge to the minimum </v>
<v Speaker 1>number of mixtures that this,</v>

460
00:26:14.800 --> 00:26:17.740
<v Speaker 1>that this discrete distribution can </v>
<v Speaker 1>actually be modeled by.</v>

461
00:26:21.480 --> 00:26:22.313
<v Speaker 1>So now we can combine the learning of </v>
<v Speaker 1>our discrete distributions with </v>

462
00:26:24.811 --> 00:26:25.644
<v Speaker 1>variational Bayes to directly take us </v>
<v Speaker 1>and put a single image from our camera </v>

463
00:26:30.030 --> 00:26:30.863
<v Speaker 1>and then predict a continuous pdf of </v>
<v Speaker 1>where the drive will a control or the </v>

464
00:26:37.480 --> 00:26:40.140
<v Speaker 1>where the control that we can execute to</v>
<v Speaker 1>the car.</v>

465
00:26:41.330 --> 00:26:41.990
<v Speaker 2>Yeah.</v>

466
00:26:41.990 --> 00:26:44.150
<v Speaker 1>So here,</v>
<v Speaker 1>here's an of um,</v>

467
00:26:44.940 --> 00:26:45.250
<v Speaker 2>okay.</v>

468
00:26:45.250 --> 00:26:46.083
<v Speaker 1>A scene where the cars turning left and </v>
<v Speaker 1>you can see the extracted discrete </v>

469
00:26:49.751 --> 00:26:54.700
<v Speaker 1>distribution is actually heavily focused</v>
<v Speaker 1>on the left part of the control.</v>

470
00:26:55.330 --> 00:26:59.590
<v Speaker 1>You can also see the continuous pdf </v>
<v Speaker 1>overlaid on top of this graph as well as</v>

471
00:26:59.591 --> 00:27:00.424
<v Speaker 1>bounds.</v>
<v Speaker 1>I haven't talked to you about how these </v>

472
00:27:01.301 --> 00:27:03.670
<v Speaker 1>bounds are computed yet,</v>
<v Speaker 1>but I'll get back to that very soon.</v>

473
00:27:04.610 --> 00:27:05.443
<v Speaker 2>Yeah.</v>

474
00:27:05.700 --> 00:27:06.533
<v Speaker 1>A very similar situation here is if,</v>
<v Speaker 1>if I again show you a union modal </v>

475
00:27:10.091 --> 00:27:10.924
<v Speaker 1>action.</v>
<v Speaker 1>So this is still a situation with </v>

476
00:27:13.950 --> 00:27:15.630
<v Speaker 1>unambiguous,</v>
<v Speaker 1>um,</v>

477
00:27:15.850 --> 00:27:16.683
<v Speaker 1>control setting,</v>
<v Speaker 1>but here the road has been slightly </v>

478
00:27:19.781 --> 00:27:20.614
<v Speaker 1>widened.</v>
<v Speaker 1>So the variants has actually been </v>

479
00:27:21.791 --> 00:27:22.624
<v Speaker 1>increased.</v>

480
00:27:23.410 --> 00:27:23.680
<v Speaker 2>Okay,</v>

481
00:27:23.680 --> 00:27:25.420
<v Speaker 1>right?</v>
<v Speaker 1>But this is actually not showing off the</v>

482
00:27:25.421 --> 00:27:26.254
<v Speaker 1>full capabilities of our approach </v>
<v Speaker 1>because the whole reason that I </v>

483
00:27:28.930 --> 00:27:31.420
<v Speaker 1>motivated this approach was to deal with</v>
<v Speaker 1>ambiguity,</v>

484
00:27:31.450 --> 00:27:33.610
<v Speaker 1>deal with uncertainty in the </v>
<v Speaker 1>environment.</v>

485
00:27:33.611 --> 00:27:35.530
<v Speaker 1>So let's actually see something much </v>
<v Speaker 1>more interesting.</v>

486
00:27:35.620 --> 00:27:39.490
<v Speaker 1>Let's go to an intersection.</v>
<v Speaker 1>So now at an intersection we can finally</v>

487
00:27:39.491 --> 00:27:43.480
<v Speaker 1>see multimodal distributions and </v>
<v Speaker 1>multimodal peaks emerging.</v>

488
00:27:44.290 --> 00:27:46.960
<v Speaker 1>So at this,</v>
<v Speaker 1>at this intersection,</v>

489
00:27:46.961 --> 00:27:47.794
<v Speaker 1>this is a t intersection.</v>
<v Speaker 1>It's possible to either turn left or </v>

490
00:27:49.871 --> 00:27:54.190
<v Speaker 1>right and you can see three modes </v>
<v Speaker 1>emerging from this distribution.</v>

491
00:27:55.420 --> 00:27:57.550
<v Speaker 1>The three modes are as you'd expect left</v>
<v Speaker 1>and right.</v>

492
00:27:57.551 --> 00:27:59.860
<v Speaker 1>But also we see the straight mode also </v>
<v Speaker 1>emerging.</v>

493
00:28:00.250 --> 00:28:03.040
<v Speaker 1>And if you look at what the human </v>
<v Speaker 1>actually did into scenario,</v>

494
00:28:03.460 --> 00:28:07.270
<v Speaker 1>it was actually to go straight as well.</v>
<v Speaker 1>So at this instant,</v>

495
00:28:07.300 --> 00:28:08.133
<v Speaker 1>the control command was to go straight.</v>
<v Speaker 1>But then if you go forward two or three </v>

496
00:28:11.051 --> 00:28:11.884
<v Speaker 1>friends,</v>
<v Speaker 1>you'll see that that straight actually </v>

497
00:28:12.701 --> 00:28:15.010
<v Speaker 1>becomes a right.</v>
<v Speaker 1>So they were trying to just basically go</v>

498
00:28:15.011 --> 00:28:15.844
<v Speaker 1>around something and you can see that </v>
<v Speaker 1>this is actually incredible because the </v>

499
00:28:18.101 --> 00:28:19.760
<v Speaker 1>model has captured,</v>
<v Speaker 1>uh,</v>

500
00:28:19.780 --> 00:28:20.613
<v Speaker 1>picked up on this and actually captured </v>
<v Speaker 1>that to actually turn right or it goes </v>

501
00:28:23.770 --> 00:28:26.080
<v Speaker 1>left.</v>
<v Speaker 1>You actually have to go straight first.</v>

502
00:28:28.900 --> 00:28:29.733
<v Speaker 1>So now let's take an example of a,</v>
<v Speaker 1>let's take a look at two example clips </v>

503
00:28:33.101 --> 00:28:33.934
<v Speaker 1>of the system in real life.</v>
<v Speaker 1>So first on the left is an example of </v>

504
00:28:40.541 --> 00:28:44.770
<v Speaker 1>navigating intersections reliably </v>
<v Speaker 1>without spuriously replanning so you can</v>

505
00:28:44.771 --> 00:28:45.604
<v Speaker 1>see many,</v>
<v Speaker 1>four way intersections and were able to </v>

506
00:28:48.071 --> 00:28:48.904
<v Speaker 1>successfully navigate through those </v>
<v Speaker 1>without any jittering or anything like </v>

507
00:28:52.631 --> 00:28:54.350
<v Speaker 1>that.</v>
<v Speaker 1>And on the right you can see the vehicle</v>

508
00:28:54.351 --> 00:28:55.184
<v Speaker 1>successfully handling roads with huge </v>
<v Speaker 1>cracks and grass and vegetation growing </v>

509
00:29:00.071 --> 00:29:00.904
<v Speaker 1>on them.</v>
<v Speaker 1>So keep in mind that this network has </v>

510
00:29:02.381 --> 00:29:03.214
<v Speaker 1>never seen these roads before.</v>
<v Speaker 1>It has only been trained on highway and </v>

511
00:29:06.941 --> 00:29:07.774
<v Speaker 1>residential streets and we only find </v>
<v Speaker 1>tuned at using one minute of data </v>

512
00:29:11.920 --> 00:29:13.150
<v Speaker 1>without lane markers.</v>

513
00:29:15.710 --> 00:29:19.130
<v Speaker 1>So this is really exciting because it </v>
<v Speaker 1>shows that we can control the car,</v>

514
00:29:19.430 --> 00:29:20.263
<v Speaker 1>even with minimal training data in the </v>
<v Speaker 1>actual environment that we're trying to </v>

515
00:29:23.661 --> 00:29:24.494
<v Speaker 1>test on.</v>
<v Speaker 1>We just have to do a little bit of fine </v>

516
00:29:25.701 --> 00:29:26.534
<v Speaker 1>tuning.</v>
<v Speaker 1>And by actually predicting the </v>

517
00:29:28.971 --> 00:29:32.180
<v Speaker 1>uncertainty with this,</v>
<v Speaker 1>we're able to get some robustness in our</v>

518
00:29:32.181 --> 00:29:33.440
<v Speaker 1>control signal as well.</v>

519
00:29:34.210 --> 00:29:34.990
<v Speaker 2>Okay.</v>

520
00:29:34.990 --> 00:29:35.823
<v Speaker 1>But in the first part of this talk,</v>
<v Speaker 1>I talked to you all about parallel </v>

521
00:29:37.750 --> 00:29:38.583
<v Speaker 1>autonomy.</v>
<v Speaker 1>So what's the connection here with this </v>

522
00:29:40.641 --> 00:29:42.790
<v Speaker 1>approach to care?</v>
<v Speaker 1>Parallel autonomy.</v>

523
00:29:43.270 --> 00:29:45.130
<v Speaker 1>So in addition to predicting the </v>
<v Speaker 1>control,</v>

524
00:29:45.160 --> 00:29:45.993
<v Speaker 1>we also want to predict control bounds </v>
<v Speaker 1>that we can really use and a parallel </v>

525
00:29:50.621 --> 00:29:51.454
<v Speaker 1>autonomous setting.</v>
<v Speaker 1>So given the setup and the framework </v>

526
00:29:54.401 --> 00:29:58.030
<v Speaker 1>that we've talked about so far,</v>
<v Speaker 1>this is actually extremely simple to do.</v>

527
00:29:58.180 --> 00:30:00.790
<v Speaker 1>So this is the beauty of setting it up,</v>
<v Speaker 1>setting the problem up.</v>

528
00:30:00.791 --> 00:30:01.624
<v Speaker 1>The way we have moving into the parallel</v>
<v Speaker 1>autonomy framework is now extremely </v>

529
00:30:05.411 --> 00:30:09.780
<v Speaker 1>simple.</v>
<v Speaker 1>So what we can do is for each mixture of</v>

530
00:30:09.781 --> 00:30:10.614
<v Speaker 1>our continuous Gow Shin,</v>
<v Speaker 1>we simply have to take plus or minus </v>

531
00:30:13.861 --> 00:30:17.430
<v Speaker 1>some scaled variance from the ideal </v>
<v Speaker 1>control command or from the mean of that</v>

532
00:30:17.431 --> 00:30:18.264
<v Speaker 1>mode.</v>
<v Speaker 1>We add those bounds into some set of </v>

533
00:30:20.761 --> 00:30:21.594
<v Speaker 1>valid control signals.</v>
<v Speaker 1>And once we have these set of valid </v>

534
00:30:25.021 --> 00:30:25.854
<v Speaker 1>controls,</v>
<v Speaker 1>all we have to do is simply set up a </v>

535
00:30:28.950 --> 00:30:31.740
<v Speaker 1>shared controller or shared parallel </v>
<v Speaker 1>autonomy controller,</v>

536
00:30:32.070 --> 00:30:34.980
<v Speaker 1>which is just the human control.</v>
<v Speaker 1>If it falls within that bound.</v>

537
00:30:35.940 --> 00:30:39.150
<v Speaker 1>And if the human is trying to do </v>
<v Speaker 1>something outside of the bounds,</v>

538
00:30:40.580 --> 00:30:41.413
<v Speaker 1>the system provides some restorative </v>
<v Speaker 1>feedback to bring them back within the </v>

539
00:30:44.481 --> 00:30:45.314
<v Speaker 1>bounce.</v>

540
00:30:47.870 --> 00:30:52.040
<v Speaker 1>So now in the next section of this talk,</v>
<v Speaker 1>I'd like to discuss this notion of model</v>

541
00:30:52.041 --> 00:30:55.100
<v Speaker 1>uncertainty.</v>
<v Speaker 1>When can we trust our models?</v>

542
00:30:55.370 --> 00:30:56.203
<v Speaker 1>Uh,</v>
<v Speaker 1>and this is really a huge question and </v>

543
00:30:58.160 --> 00:31:00.800
<v Speaker 1>actually all of deep learning,</v>
<v Speaker 1>not just autonomous vehicles,</v>

544
00:31:01.550 --> 00:31:02.383
<v Speaker 1>um,</v>
<v Speaker 1>but we want to essentially accurately </v>

545
00:31:03.831 --> 00:31:07.700
<v Speaker 1>determine or accurately verify that our </v>
<v Speaker 1>model is,</v>

546
00:31:08.030 --> 00:31:08.863
<v Speaker 1>is confident in the action that it's </v>
<v Speaker 1>telling us to take at that given </v>

547
00:31:11.271 --> 00:31:12.104
<v Speaker 1>instance of time.</v>
<v Speaker 1>So I hope this notion of uncertainty is </v>

548
00:31:17.101 --> 00:31:20.100
<v Speaker 1>pretty clear,</v>
<v Speaker 1>like the motivation of why we care about</v>

549
00:31:20.101 --> 00:31:21.630
<v Speaker 1>uncertainty.</v>
<v Speaker 1>It's pretty clear to all of you,</v>

550
00:31:21.870 --> 00:31:25.530
<v Speaker 1>especially and a talk about autonomous </v>
<v Speaker 1>vehicles.</v>

551
00:31:26.130 --> 00:31:27.000
<v Speaker 1>So</v>

552
00:31:28.170 --> 00:31:29.003
<v Speaker 1>anonymous vehicles are one of the most </v>
<v Speaker 1>safety critical robotic applications </v>

553
00:31:32.341 --> 00:31:37.341
<v Speaker 1>where it is extremely important for us </v>
<v Speaker 1>to prioritize safety in the systems that</v>

554
00:31:39.421 --> 00:31:40.254
<v Speaker 1>we're building.</v>
<v Speaker 1>So what we've learned so far is that </v>

555
00:31:42.661 --> 00:31:43.494
<v Speaker 1>modeling probabilities for fixed classes</v>
<v Speaker 1>like we do for a discreet control </v>

556
00:31:46.711 --> 00:31:47.544
<v Speaker 1>signals is actually different than </v>
<v Speaker 1>looking at the uncertainty of the model </v>

557
00:31:51.271 --> 00:31:53.250
<v Speaker 1>as a whole.</v>
<v Speaker 1>So take for example,</v>

558
00:31:53.251 --> 00:31:54.084
<v Speaker 1>if I want to train this model of cats </v>
<v Speaker 1>and dogs just to recognize a picture of </v>

559
00:31:57.691 --> 00:31:59.190
<v Speaker 1>a cat and a dog,</v>
<v Speaker 1>and I at the output,</v>

560
00:31:59.191 --> 00:32:00.024
<v Speaker 1>I'm predicting a softmax layer that </v>
<v Speaker 1>represents probably being a cat </v>

561
00:32:02.400 --> 00:32:04.440
<v Speaker 1>probability of being a dog.</v>
<v Speaker 1>Now,</v>

562
00:32:04.441 --> 00:32:08.280
<v Speaker 1>if I showed this model,</v>
<v Speaker 1>I picture of a cat,</v>

563
00:32:08.310 --> 00:32:10.650
<v Speaker 1>assuming that the model has been </v>
<v Speaker 1>sufficiently trained,</v>

564
00:32:10.920 --> 00:32:13.920
<v Speaker 1>I'd expect that this model,</v>
<v Speaker 1>it gives me an answer of yes,</v>

565
00:32:13.921 --> 00:32:14.754
<v Speaker 1>this is definitely a cat.</v>
<v Speaker 1>And I expect that model to be very </v>

566
00:32:17.341 --> 00:32:19.350
<v Speaker 1>confident in its answer,</v>
<v Speaker 1>right?</v>

567
00:32:19.351 --> 00:32:21.030
<v Speaker 1>So in this scenario,</v>
<v Speaker 1>they're the same thing,</v>

568
00:32:21.031 --> 00:32:22.950
<v Speaker 1>but now let's think about it's different</v>
<v Speaker 1>scenario.</v>

569
00:32:23.400 --> 00:32:25.980
<v Speaker 1>What if I feed that same model?</v>
<v Speaker 1>A picture of a horse,</v>

570
00:32:27.180 --> 00:32:28.013
<v Speaker 1>right?</v>
<v Speaker 1>So these two output probabilities are </v>

571
00:32:29.551 --> 00:32:31.410
<v Speaker 1>still fixed.</v>
<v Speaker 1>So the model,</v>

572
00:32:31.411 --> 00:32:35.070
<v Speaker 1>we'll still output the probability of </v>
<v Speaker 1>being a cat or the probability of dog.</v>

573
00:32:35.880 --> 00:32:38.940
<v Speaker 1>And by definition those output </v>
<v Speaker 1>probabilities have to add up to,</v>

574
00:32:39.290 --> 00:32:42.200
<v Speaker 1>so by definition one of them has to be </v>
<v Speaker 1>greater than 0.5.</v>

575
00:32:43.200 --> 00:32:43.410
<v Speaker 2>Yeah.</v>

576
00:32:43.410 --> 00:32:44.243
<v Speaker 1>Right.</v>
<v Speaker 1>So even though one of the probabilities </v>

577
00:32:45.241 --> 00:32:47.130
<v Speaker 1>are very high,</v>
<v Speaker 1>it may seem,</v>

578
00:32:48.120 --> 00:32:48.953
<v Speaker 1>it actually does not mean that the model</v>
<v Speaker 1>is confident and predicting that this </v>

579
00:32:51.481 --> 00:32:53.580
<v Speaker 1>horse is a dog.</v>
<v Speaker 1>And in fact,</v>

580
00:32:53.581 --> 00:32:54.414
<v Speaker 1>what we want to see is that the </v>
<v Speaker 1>uncertainty of this model in this </v>

581
00:32:56.161 --> 00:33:00.010
<v Speaker 1>scenario is very low.</v>
<v Speaker 1>And that's something that that motivates</v>

582
00:33:00.011 --> 00:33:01.200
<v Speaker 1>this,</v>
<v Speaker 1>this whole notion of,</v>

583
00:33:01.890 --> 00:33:04.440
<v Speaker 1>of uh,</v>
<v Speaker 1>uncertainty or confidence.</v>

584
00:33:04.830 --> 00:33:05.663
<v Speaker 1>And the key points that I want to hammer</v>
<v Speaker 1>home here is that neural network </v>

585
00:33:07.981 --> 00:33:08.814
<v Speaker 1>probability is created through softmax </v>
<v Speaker 1>layer is do not correspond to </v>

586
00:33:12.031 --> 00:33:12.864
<v Speaker 1>confidence.</v>
<v Speaker 1>They don't measure uncertainty even </v>

587
00:33:14.731 --> 00:33:19.060
<v Speaker 1>though many people actually sent,</v>
<v Speaker 1>makes the two terms entertained.</v>

588
00:33:19.450 --> 00:33:20.910
<v Speaker 1>Um,</v>
<v Speaker 1>interchangeable.</v>

589
00:33:22.870 --> 00:33:23.200
<v Speaker 2>Yeah.</v>

590
00:33:23.200 --> 00:33:26.560
<v Speaker 1>So the spring sent the question how we </v>
<v Speaker 1>can actually determine model uncertainty</v>

591
00:33:27.130 --> 00:33:29.950
<v Speaker 1>and for this return to this really </v>
<v Speaker 1>hockfield and deep learning right now,</v>

592
00:33:29.951 --> 00:33:34.630
<v Speaker 1>which is called Beige and deep learning.</v>
<v Speaker 1>And just listen to just as an example of</v>

593
00:33:34.631 --> 00:33:37.990
<v Speaker 1>the power of beige and deep learning.</v>
<v Speaker 1>Let's take,</v>

594
00:33:37.991 --> 00:33:39.880
<v Speaker 1>um,</v>
<v Speaker 1>let's take a look at this figure here.</v>

595
00:33:39.881 --> 00:33:43.270
<v Speaker 1>So this is a system that was built to </v>
<v Speaker 1>take us input,</v>

596
00:33:43.271 --> 00:33:44.104
<v Speaker 1>a single image and output the predicted </v>
<v Speaker 1>depth of every single pixel in that </v>

597
00:33:49.301 --> 00:33:50.134
<v Speaker 1>image.</v>

598
00:33:51.020 --> 00:33:51.710
<v Speaker 2>Okay.</v>

599
00:33:51.710 --> 00:33:52.543
<v Speaker 1>So when we use beige and deep learning,</v>
<v Speaker 1>well we actually want to do is compute </v>

600
00:33:55.760 --> 00:33:58.850
<v Speaker 1>this very,</v>
<v Speaker 1>very rich uncertainty estimates for this</v>

601
00:33:58.851 --> 00:33:59.684
<v Speaker 1>neural network.</v>
<v Speaker 1>And we want to say for every single </v>

602
00:34:00.861 --> 00:34:01.694
<v Speaker 1>pixel,</v>
<v Speaker 1>what is the confidence in that </v>

603
00:34:03.591 --> 00:34:04.424
<v Speaker 1>prediction and beige and deep learning </v>
<v Speaker 1>really allows us to do this in a </v>

604
00:34:07.851 --> 00:34:08.684
<v Speaker 1>systematic manner.</v>
<v Speaker 1>And it's really remarkable because we </v>

605
00:34:12.561 --> 00:34:15.710
<v Speaker 1>can see some really interesting things </v>
<v Speaker 1>that start to arise.</v>

606
00:34:15.980 --> 00:34:16.813
<v Speaker 1>So you can see that the model is most </v>
<v Speaker 1>uncertain around like the edges of the </v>

607
00:34:19.041 --> 00:34:19.874
<v Speaker 1>car is also the shadows underneath the </v>
<v Speaker 1>cars and even the reflections and the </v>

608
00:34:23.931 --> 00:34:25.340
<v Speaker 1>windows.</v>
<v Speaker 1>Because then the reflections,</v>

609
00:34:25.341 --> 00:34:27.380
<v Speaker 1>you actually see trees that are far </v>
<v Speaker 1>away.</v>

610
00:34:27.381 --> 00:34:31.250
<v Speaker 1>So the model is able to capture some </v>
<v Speaker 1>very natural sense of uncertainty.</v>

611
00:34:32.940 --> 00:34:33.220
<v Speaker 2>Yeah.</v>

612
00:34:33.220 --> 00:34:37.180
<v Speaker 1>So now the question becomes how can we </v>
<v Speaker 1>apply these techniques to the end?</v>

613
00:34:37.181 --> 00:34:39.520
<v Speaker 1>To end driving scenario for self driving</v>
<v Speaker 1>cars.</v>

614
00:34:40.030 --> 00:34:42.730
<v Speaker 1>So let's revisit the original end to end</v>
<v Speaker 1>pipeline.</v>

615
00:34:43.390 --> 00:34:46.270
<v Speaker 1>So this is actually a different pipeline</v>
<v Speaker 1>that I talked about in the first talk.</v>

616
00:34:46.271 --> 00:34:47.104
<v Speaker 1>This is an even simpler ones.</v>
<v Speaker 1>So this is basically the pipeline that </v>

617
00:34:49.361 --> 00:34:51.280
<v Speaker 1>Nvidia has created for their autonomous </v>
<v Speaker 1>vehicles.</v>

618
00:34:51.281 --> 00:34:52.114
<v Speaker 1>So let's just taking it in a single </v>
<v Speaker 1>image and predicting just a single </v>

619
00:34:54.221 --> 00:34:55.054
<v Speaker 1>control value.</v>
<v Speaker 1>There is no notion of probability </v>

620
00:34:56.411 --> 00:34:58.090
<v Speaker 1>distributions like we talked about </v>
<v Speaker 1>before.</v>

621
00:34:58.300 --> 00:34:59.920
<v Speaker 1>I'm just predicting a single real </v>
<v Speaker 1>number.</v>

622
00:35:00.700 --> 00:35:02.800
<v Speaker 1>We've already discussed the issues with </v>
<v Speaker 1>this approach.</v>

623
00:35:02.830 --> 00:35:06.910
<v Speaker 1>We lack uncertainty,</v>
<v Speaker 1>we lack ambiguous scenario handling.</v>

624
00:35:09.430 --> 00:35:10.263
<v Speaker 1>So instead,</v>
<v Speaker 1>what if we can maintain this </v>

625
00:35:11.951 --> 00:35:15.280
<v Speaker 1>architecture,</v>
<v Speaker 1>but we still want to create some sort of</v>

626
00:35:15.281 --> 00:35:17.530
<v Speaker 1>uncertainty.</v>
<v Speaker 1>So we can recently,</v>

627
00:35:17.620 --> 00:35:18.453
<v Speaker 1>we can actually reason about the </v>
<v Speaker 1>confidence of our model with that </v>

628
00:35:21.611 --> 00:35:24.550
<v Speaker 1>control.</v>
<v Speaker 1>And the way we can do that is actually,</v>

629
00:35:25.220 --> 00:35:26.053
<v Speaker 1>uh,</v>
<v Speaker 1>by stochastically sampling over our </v>

630
00:35:27.941 --> 00:35:31.510
<v Speaker 1>output to deterministically,</v>
<v Speaker 1>um,</v>

631
00:35:32.020 --> 00:35:33.700
<v Speaker 1>sorry to deterministic,</v>
<v Speaker 1>sorry.</v>

632
00:35:33.880 --> 00:35:34.713
<v Speaker 1>We want to sample over are deterministic</v>
<v Speaker 1>control signal to actually obtain an </v>

633
00:35:38.581 --> 00:35:42.150
<v Speaker 1>expectation and variants of that </v>
<v Speaker 1>instance.</v>

634
00:35:42.151 --> 00:35:42.984
<v Speaker 1>So this will give us the expectation is </v>
<v Speaker 1>essentially the control that we want to </v>

635
00:35:45.331 --> 00:35:46.164
<v Speaker 1>take.</v>
<v Speaker 1>And the variance is essentially the </v>

636
00:35:48.120 --> 00:35:49.740
<v Speaker 1>uncertainty of the model at that </v>
<v Speaker 1>instant.</v>

637
00:35:50.770 --> 00:35:51.420
<v Speaker 2>Okay.</v>

638
00:35:51.420 --> 00:35:52.253
<v Speaker 1>So more specifically,</v>
<v Speaker 1>we can do this using patient deep </v>

639
00:35:53.701 --> 00:35:54.660
<v Speaker 1>learning,</v>
<v Speaker 1>like I said,</v>

640
00:35:55.230 --> 00:35:56.063
<v Speaker 1>and</v>

641
00:35:56.600 --> 00:35:56.830
<v Speaker 2>okay,</v>

642
00:35:56.830 --> 00:35:59.080
<v Speaker 1>to understand this,</v>
<v Speaker 1>let's actually reformulate our problem a</v>

643
00:35:59.081 --> 00:36:02.200
<v Speaker 1>little bit using a Bayesian state of </v>
<v Speaker 1>mind.</v>

644
00:36:02.410 --> 00:36:05.200
<v Speaker 1>So this is a little different than the </v>
<v Speaker 1>way we train our models,</v>

645
00:36:05.930 --> 00:36:07.300
<v Speaker 1>uh,</v>
<v Speaker 1>in the previous setting.</v>

646
00:36:07.301 --> 00:36:09.940
<v Speaker 1>So now in the Batian deep learning </v>
<v Speaker 1>framework,</v>

647
00:36:09.941 --> 00:36:10.930
<v Speaker 1>we want to actually,</v>

648
00:36:12.340 --> 00:36:12.760
<v Speaker 2>okay,</v>

649
00:36:12.760 --> 00:36:14.740
<v Speaker 1>like I said,</v>
<v Speaker 1>have that patient's state of mind.</v>

650
00:36:15.580 --> 00:36:16.413
<v Speaker 1>So the goal here,</v>
<v Speaker 1>like before we're trying to create a </v>

651
00:36:18.161 --> 00:36:21.100
<v Speaker 1>network to learn steering control </v>
<v Speaker 1>directly from raw data.</v>

652
00:36:21.790 --> 00:36:22.623
<v Speaker 1>We're going to find that functional </v>
<v Speaker 1>napping and we want to do it by </v>

653
00:36:24.101 --> 00:36:24.934
<v Speaker 1>minimizing some empirical loss.</v>
<v Speaker 1>The different series that invasion </v>

654
00:36:28.630 --> 00:36:29.463
<v Speaker 1>neural networks were actually attempting</v>
<v Speaker 1>to learn this posterior Oh far awaits </v>

655
00:36:33.160 --> 00:36:35.290
<v Speaker 1>given our data.</v>
<v Speaker 1>So another,</v>

656
00:36:35.440 --> 00:36:36.273
<v Speaker 1>another way to say this is that we want </v>
<v Speaker 1>to understand the likelihood of </v>

657
00:36:38.831 --> 00:36:39.664
<v Speaker 1>observing some model.</v>
<v Speaker 1>Given the data that we see now based on </v>

658
00:36:44.471 --> 00:36:46.780
<v Speaker 1>neural networks are actually called </v>
<v Speaker 1>Beige and neural networks.</v>

659
00:36:46.781 --> 00:36:50.530
<v Speaker 1>Because we can rewrite this posterior,</v>
<v Speaker 1>this one,</v>

660
00:36:51.300 --> 00:36:53.110
<v Speaker 1>uh,</v>
<v Speaker 1>using Bayes rule,</v>

661
00:36:53.620 --> 00:36:54.453
<v Speaker 1>right?</v>
<v Speaker 1>And actually we can theoretically solve </v>

662
00:36:56.861 --> 00:36:57.694
<v Speaker 1>it using this equation.</v>
<v Speaker 1>The problem is that in practice this is </v>

663
00:37:00.461 --> 00:37:02.770
<v Speaker 1>rarely possible to compute and </v>
<v Speaker 1>politically,</v>

664
00:37:03.400 --> 00:37:04.233
<v Speaker 1>uh,</v>
<v Speaker 1>so instead we have to resort to some </v>

665
00:37:05.081 --> 00:37:08.170
<v Speaker 1>sort of sampling techniques to </v>
<v Speaker 1>approximate it instead.</v>

666
00:37:09.170 --> 00:37:10.000
<v Speaker 2>Okay,</v>

667
00:37:10.000 --> 00:37:13.390
<v Speaker 1>so let's look at what this might look </v>
<v Speaker 1>like for our network.</v>

668
00:37:13.391 --> 00:37:14.224
<v Speaker 1>With convolutional layers,</v>
<v Speaker 1>we start by performing t stochastic </v>

669
00:37:17.621 --> 00:37:18.454
<v Speaker 1>samples through our network forward </v>
<v Speaker 1>passes through our network and at each </v>

670
00:37:21.161 --> 00:37:21.994
<v Speaker 1>time we're essentially sampling each of </v>
<v Speaker 1>the weights of Feta according to a </v>

671
00:37:25.751 --> 00:37:26.584
<v Speaker 1>Bernoulli distribution.</v>
<v Speaker 1>An element wise multiplied that </v>

672
00:37:29.291 --> 00:37:32.740
<v Speaker 1>Bernoulli mask with the unregular eyes </v>
<v Speaker 1>feature maps.</v>

673
00:37:33.660 --> 00:37:34.350
<v Speaker 2>Yeah,</v>

674
00:37:34.350 --> 00:37:35.183
<v Speaker 1>so on the bottom left you can see in the</v>
<v Speaker 1>illustration of three on regularized </v>

675
00:37:38.640 --> 00:37:41.040
<v Speaker 1>convolutional kernels,</v>
<v Speaker 1>right?</v>

676
00:37:42.630 --> 00:37:43.463
<v Speaker 1>You can also see next to them the </v>
<v Speaker 1>element that they are element wise </v>

677
00:37:46.171 --> 00:37:47.004
<v Speaker 1>multiplied by masks of ones and Zeros,</v>
<v Speaker 1>which are just drawn from a Bernoulli </v>

678
00:37:51.241 --> 00:37:56.241
<v Speaker 1>random variable Bernoulli distribution.</v>
<v Speaker 1>And this process is effectively repeated</v>

679
00:37:56.371 --> 00:37:58.920
<v Speaker 1>capital t times.</v>
<v Speaker 1>You repeat this tee times,</v>

680
00:37:58.921 --> 00:38:01.910
<v Speaker 1>you get an estimate of your expectation </v>
<v Speaker 1>of your output and you get an x,</v>

681
00:38:01.980 --> 00:38:02.813
<v Speaker 1>you get a variance estimation as well.</v>
<v Speaker 1>Now the variance estimation is exactly </v>

682
00:38:06.241 --> 00:38:08.520
<v Speaker 1>what we're looking for as part of our </v>
<v Speaker 1>uncertainty estimates.</v>

683
00:38:08.970 --> 00:38:12.270
<v Speaker 1>But there's actually a problem here in </v>
<v Speaker 1>terms of convolutional layers.</v>

684
00:38:12.810 --> 00:38:13.643
<v Speaker 1>So the original theory for Beige and </v>
<v Speaker 1>deep learning didn't take into account </v>

685
00:38:16.890 --> 00:38:17.723
<v Speaker 1>this huge spatial correlation between </v>
<v Speaker 1>adjacent pixels in early convolutional </v>

686
00:38:22.051 --> 00:38:22.884
<v Speaker 1>layers.</v>
<v Speaker 1>So especially in like I said in earlier </v>

687
00:38:26.611 --> 00:38:27.444
<v Speaker 1>convolutional layers,</v>
<v Speaker 1>you're dealing with things like edges </v>

688
00:38:29.281 --> 00:38:32.280
<v Speaker 1>and corners and things like that.</v>

689
00:38:32.310 --> 00:38:35.220
<v Speaker 1>And there are massive amounts of spatial</v>
<v Speaker 1>correlation,</v>

690
00:38:35.530 --> 00:38:38.110
<v Speaker 1>the pixels.</v>
<v Speaker 1>So when we apply our Bernoulli mask,</v>

691
00:38:38.800 --> 00:38:39.633
<v Speaker 1>we may actually lose a lot of valuable </v>
<v Speaker 1>information and this may cause our </v>

692
00:38:42.821 --> 00:38:46.060
<v Speaker 1>output to change undesirably.</v>
<v Speaker 1>So how do we,</v>

693
00:38:46.180 --> 00:38:49.150
<v Speaker 1>how do we handle this?</v>
<v Speaker 1>So let's make a small change and instead</v>

694
00:38:49.151 --> 00:38:51.700
<v Speaker 1>of sampling Bernoulli random variables </v>
<v Speaker 1>over each,</v>

695
00:38:51.701 --> 00:38:52.534
<v Speaker 1>wait,</v>
<v Speaker 1>let's sample Bernoulli random variables </v>

696
00:38:54.580 --> 00:38:58.000
<v Speaker 1>over each kernel itself.</v>
<v Speaker 1>So over the entire kernel,</v>

697
00:38:58.001 --> 00:39:01.150
<v Speaker 1>we'll call this colonel either a one or </v>
<v Speaker 1>a zero and one element wise,</v>

698
00:39:01.151 --> 00:39:05.020
<v Speaker 1>multiply our unregular as feature maps </v>
<v Speaker 1>by this new,</v>

699
00:39:05.680 --> 00:39:09.100
<v Speaker 1>um,</v>
<v Speaker 1>Bernoulli drop out mask.</v>

700
00:39:09.940 --> 00:39:14.940
<v Speaker 1>So effectively by dropping gout,</v>
<v Speaker 1>entire kernels in early layers,</v>

701
00:39:15.191 --> 00:39:16.024
<v Speaker 1>you,</v>
<v Speaker 1>you'd probably know that kernels </v>

702
00:39:16.751 --> 00:39:20.320
<v Speaker 1>represent things like line detection,</v>
<v Speaker 1>oriented line detections.</v>

703
00:39:20.380 --> 00:39:21.213
<v Speaker 1>And things like that.</v>
<v Speaker 1>So effectively by dropping out some of </v>

704
00:39:22.991 --> 00:39:27.550
<v Speaker 1>these kernels and keeping others were </v>
<v Speaker 1>getting like a noisy estimate of,</v>

705
00:39:28.180 --> 00:39:29.013
<v Speaker 1>of for example,</v>
<v Speaker 1>lane detection algorithm if we're only </v>

706
00:39:30.971 --> 00:39:33.370
<v Speaker 1>dropping out like oriented line </v>
<v Speaker 1>detectors.</v>

707
00:39:33.670 --> 00:39:35.980
<v Speaker 1>And this is exactly what we implemented </v>
<v Speaker 1>in our algorithm.</v>

708
00:39:37.480 --> 00:39:38.313
<v Speaker 1>So we found that stochastically dropping</v>
<v Speaker 1>out kernels as opposed to elements </v>

709
00:39:42.850 --> 00:39:43.683
<v Speaker 1>actually resulted in both accelerated </v>
<v Speaker 1>training as well as better convergence </v>

710
00:39:46.961 --> 00:39:47.794
<v Speaker 1>of our end to end algorithms.</v>
<v Speaker 1>We performed a precision analysis to </v>

711
00:39:51.761 --> 00:39:55.190
<v Speaker 1>actually analyze how precise our models </v>
<v Speaker 1>come,</v>

712
00:39:55.420 --> 00:39:57.460
<v Speaker 1>uh,</v>
<v Speaker 1>work compared to the ground truth,</v>

713
00:39:57.461 --> 00:39:58.294
<v Speaker 1>human actions.</v>
<v Speaker 1>And finally we studied how the </v>

714
00:40:00.941 --> 00:40:01.774
<v Speaker 1>uncertainty of our model changed as a </v>
<v Speaker 1>function of the steering wheel angle or </v>

715
00:40:04.841 --> 00:40:06.930
<v Speaker 1>the steering control,</v>
<v Speaker 1>uh,</v>

716
00:40:06.970 --> 00:40:11.970
<v Speaker 1>as well as the quantity of training data</v>
<v Speaker 1>for that specific type of control.</v>

717
00:40:14.410 --> 00:40:18.910
<v Speaker 1>So finally we found that the uncertainty</v>
<v Speaker 1>increased as we,</v>

718
00:40:19.440 --> 00:40:22.150
<v Speaker 1>um,</v>
<v Speaker 1>tried to make more dynamic turns,</v>

719
00:40:22.151 --> 00:40:22.984
<v Speaker 1>which you might expect because when </v>
<v Speaker 1>you're trying to make a more dynamic </v>

720
00:40:24.581 --> 00:40:25.414
<v Speaker 1>turn,</v>
<v Speaker 1>you actually have less of that scene in </v>

721
00:40:27.761 --> 00:40:30.400
<v Speaker 1>your field of view.</v>
<v Speaker 1>So if you're making a very dynamic right</v>

722
00:40:30.401 --> 00:40:33.400
<v Speaker 1>turn and your cameras pointing straight </v>
<v Speaker 1>in front of you,</v>

723
00:40:33.580 --> 00:40:34.720
<v Speaker 1>you actually can't see the road.</v>

724
00:40:34.720 --> 00:40:36.700
<v Speaker 1>You can only see maybe 10 meters in </v>
<v Speaker 1>front of you.</v>

725
00:40:36.940 --> 00:40:38.950
<v Speaker 1>Whereas if you're going straight on a </v>
<v Speaker 1>straight highway,</v>

726
00:40:39.100 --> 00:40:41.830
<v Speaker 1>you can see maybe even a hundred meters </v>
<v Speaker 1>in front of you.</v>

727
00:40:42.520 --> 00:40:44.800
<v Speaker 1>So this really leads to this nice </v>
<v Speaker 1>property,</v>

728
00:40:44.801 --> 00:40:49.300
<v Speaker 1>which we found was we have a more </v>
<v Speaker 1>uncertain model as we tried to make more</v>

729
00:40:49.301 --> 00:40:50.134
<v Speaker 1>extreme turns is also aligns with the </v>
<v Speaker 1>fact that the vast amount of training </v>

730
00:40:55.391 --> 00:40:56.224
<v Speaker 1>data comes on straight roads.</v>
<v Speaker 1>So you'd expect that straight road </v>

731
00:40:58.781 --> 00:41:02.650
<v Speaker 1>driving is the most confident that the </v>
<v Speaker 1>model can,</v>

732
00:41:02.730 --> 00:41:07.730
<v Speaker 1>can get can be.</v>
<v Speaker 1>So now I'd like to take a step back,</v>

733
00:41:07.871 --> 00:41:08.704
<v Speaker 1>summarize some of the findings of this </v>
<v Speaker 1>talk and tie things back with parallel </v>

734
00:41:11.921 --> 00:41:12.850
<v Speaker 1>autonomy.</v>
<v Speaker 1>Once again.</v>

735
00:41:14.350 --> 00:41:15.183
<v Speaker 1>So we started this talk by exploring one</v>
<v Speaker 1>of the control algorithms that we've </v>

736
00:41:18.101 --> 00:41:18.934
<v Speaker 1>created to actually predict the control </v>
<v Speaker 1>of an autonomous vehicle and not only </v>

737
00:41:25.631 --> 00:41:26.464
<v Speaker 1>predict the control,</v>
<v Speaker 1>but also predict the control bounce and </v>

738
00:41:28.271 --> 00:41:29.740
<v Speaker 1>entirely end to end manner.</v>

739
00:41:30.670 --> 00:41:33.040
<v Speaker 1>We showed how we can officially handle </v>
<v Speaker 1>ambiguity,</v>

740
00:41:33.260 --> 00:41:34.093
<v Speaker 1>this approach,</v>
<v Speaker 1>and we explored ways that we can use </v>

741
00:41:36.171 --> 00:41:37.004
<v Speaker 1>techniques from beige and deep learning </v>
<v Speaker 1>to actually get a sense of when our </v>

742
00:41:38.811 --> 00:41:42.590
<v Speaker 1>model is and is not confident about its </v>
<v Speaker 1>prediction.</v>

743
00:41:43.760 --> 00:41:44.593
<v Speaker 1>Finally,</v>
<v Speaker 1>I'd like to show how all this ties in </v>

744
00:41:45.741 --> 00:41:46.574
<v Speaker 1>with a cool demonstration of parallel </v>
<v Speaker 1>autonomy on a full scale autonomous </v>

745
00:41:49.671 --> 00:41:50.504
<v Speaker 1>vehicle,</v>
<v Speaker 1>specifically demonstrating guardian </v>

746
00:41:51.981 --> 00:41:56.540
<v Speaker 1>angel capabilities.</v>
<v Speaker 1>So here the human is controlling the car</v>

747
00:41:56.840 --> 00:41:59.810
<v Speaker 1>and as long as he adheres to the balance</v>
<v Speaker 1>of the learned model,</v>

748
00:42:00.200 --> 00:42:01.670
<v Speaker 1>it appears as though nothing is </v>
<v Speaker 1>happening.</v>

749
00:42:01.760 --> 00:42:02.593
<v Speaker 1>But if he tries to drive off the road,</v>
<v Speaker 1>you'll see in a second the system will </v>

750
00:42:05.691 --> 00:42:09.530
<v Speaker 1>see that he's exited the balance and </v>
<v Speaker 1>tried to provide restorative feedback.</v>

751
00:42:09.531 --> 00:42:10.364
<v Speaker 1>Like what just happened there.</v>
<v Speaker 1>You'll see another example where the </v>

752
00:42:12.231 --> 00:42:13.064
<v Speaker 1>human tries to even more aggressively </v>
<v Speaker 1>tried to drive off the left of the road </v>

753
00:42:15.950 --> 00:42:16.783
<v Speaker 1>where the model will respond even more </v>
<v Speaker 1>aggressively to bring the car back onto </v>

754
00:42:21.741 --> 00:42:22.574
<v Speaker 1>the road.</v>

755
00:42:25.770 --> 00:42:29.250
<v Speaker 1>So we hope that this kind of parallel </v>
<v Speaker 1>autonomy way of thinking,</v>

756
00:42:30.060 --> 00:42:31.770
<v Speaker 1>uh,</v>
<v Speaker 1>brings about like this,</v>

757
00:42:31.800 --> 00:42:36.800
<v Speaker 1>this new realm of autonomous driving </v>
<v Speaker 1>specifically of like this Guardian Angel</v>

758
00:42:37.740 --> 00:42:38.940
<v Speaker 1>Capability.</v>
<v Speaker 1>So the goal,</v>

759
00:42:38.970 --> 00:42:39.803
<v Speaker 1>the ultimate goal of our work is to </v>
<v Speaker 1>create a vehicle that even if you want </v>

760
00:42:43.681 --> 00:42:46.260
<v Speaker 1>to crash,</v>
<v Speaker 1>it is uncrackable,</v>

761
00:42:46.380 --> 00:42:47.213
<v Speaker 1>right?</v>
<v Speaker 1>So it will prohibit you from making any </v>

762
00:42:48.511 --> 00:42:53.511
<v Speaker 1>dangerous actions that exit the balance </v>
<v Speaker 1>of the set of valid controls,</v>

763
00:42:53.731 --> 00:42:54.990
<v Speaker 1>right?</v>
<v Speaker 1>So this is,</v>

764
00:42:54.991 --> 00:42:57.870
<v Speaker 1>this is really the holy grail of the </v>
<v Speaker 1>work that we're doing.</v>

765
00:42:58.410 --> 00:43:01.590
<v Speaker 1>And I'd like to conclude this talk by </v>
<v Speaker 1>actually acknowledging all of my,</v>

766
00:43:02.270 --> 00:43:04.050
<v Speaker 1>uh,</v>
<v Speaker 1>incredible lab at Mit,</v>

767
00:43:04.350 --> 00:43:05.183
<v Speaker 1>all of my collaborators.</v>
<v Speaker 1>If you're interested in reading up on </v>

768
00:43:07.051 --> 00:43:09.960
<v Speaker 1>any of the references that I've </v>
<v Speaker 1>presented as part of this talk,</v>

769
00:43:09.961 --> 00:43:13.050
<v Speaker 1>there's a link right here where you can </v>
<v Speaker 1>find all of the references.</v>

770
00:43:13.760 --> 00:43:14.760
<v Speaker 1>Um,</v>
<v Speaker 1>and that's it.</v>

771
00:43:14.761 --> 00:43:15.594
<v Speaker 1>Thank you.</v>

