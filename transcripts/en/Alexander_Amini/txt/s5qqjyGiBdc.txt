Speaker 1:          00:02          All right. Good morning everyone.

Speaker 1:          00:06          My name is lise Friedman. I'm a research scientist here at Mit. I build the autonomous vehicles and perception systems for these vehicles. And today I, I'd like to talk to him. First of all was great to be a part of this amazing course and intro to deep learning. Um, it's, it's a wonderful course covering a pretty quickly but deeply, uh, some of the fundamental aspects of deep learning. This is awesome. So the topic that I'm perhaps most passionate about from the perspective of just being a researcher in artificial intelligence is deep reinforcement learning, which is a set of ideas and methods that teach agents to act in the real world.

Speaker 2:          00:50          Okay.

Speaker 1:          00:51          If you think about what machine learning is, it allows us to make sense of the world, but to truly have impact in the physical world, we need to also act in that world. So you bring the understanding of that you extract from the world through the perception systems and actually decide to make actions. So you can think of intelligence systems as this kind of stack from the top to the bottom and the environment. At the top of the world that the Asian operates in. And at the bottom is the factors that actually make, uh, changes to the world by moving the robot, moving the agent in a way that changes the world, that acts in the world. And so from the environment it goes to the sensors sensing the raw sensory data, extracting the features, making sense of that features, understanding for me representations higher and higher order representations from those representations, we gain knowledge that's useful, actionable.

Speaker 1:          01:46          Finally, we reason the thing that we hold so dear as human beings, the reasoning that builds and aggregates knowledge bases and using that reasoning form short term and long term plans to finally turn into action and act in that world, changing it. So that's kind of the stack of artificial intelligence. And the question is how much in the same way as human beings, we learn most of the stack when we're born, we know very little and we'd take in five sensory, uh, sources of sensory data and make sense of the world, learn over time to act successful in that world. How much? The question is can we use deep learning methods to learn parts of the stack, the modules of the stack or the entire stack end to end? Let's go over them. Okay, so for uh, robots that act in the world, autonomous vehicles, humanoid, robots, drones, there are sensors, whether it's slide our camera and microphone coming from the audio networking for the communications, I am you getting the kinematics of the different vehicles.

Speaker 1:          02:47          That's the raw data coming in. That's the eyes, ears, nose for robots. Then the ones who have the sensory data, the task is to form representations on data. Yeah. You make sense of this raw pixels, a raw piece, pieces, samples for whatever the sensor is and you start to try to piece it together into something that can be used to gain understanding. It's just numbers and those numbers need to be converted into something that can be reasoned with. That's forming representations and that's where machine learning, deep learning steps in and takes this raw sensory data. The was some preprocessing, some profit, some initial processing, and for forming higher and higher order representation in that data that can be reasoned with about in the computer vision from edges two faces, two entire entities. And finally the interpretation, the semantic interpretation of the scene. That's machine learning, playing with the representations and the reasoning part.

Speaker 1:          03:50          One of the exciting fundamental open challenges of machine learning is how does this greater and greater representation that can be formed through deep neural networks can then lead to reasoning, to building knowledge, not just, uh, a memorization task, which is taking a supervised learning, memorizing patterns in the input data based on human annotations, but also an extracting those patterns, but also then taking that knowledge and building over time as we humans do, building into something that can be called common sense and to knowledge basis in a very trivial task. This means, uh, aggregating fusing multiple types of, uh, multiple, uh, types of extraction of knowledge. So from image recognition, you could think if it looks like a duck in an image, it sounds like a duck when the audio and then you could do act activity recognition with the video. Uh, it a swims like a duck and it must be a duck.

Speaker 1:          04:44          Just aggregating this trivial, uh, different sources of information. That's reasoning. Now I think a form the human perspective from the very biased human perspective, one of the, uh, illustrative aspects of reasoning is theory improving is the moment of invention, of creative genius of this breakthrough ideas as we humans come up with and mean really these aren't new ideas. Whenever we come up with an interesting idea, they're not new. We're just collecting pieces of high word representations of knowledge that we've gained over time and then piecing them together to form something, some simple beautiful distillation that is useful for, uh, for the rest of the world. And, uh, one of my favorite, uh, sort of human stories and discoveries in pure theorem improving is from Oslo. Last theorem. It stood for 358 years. This is a trivial thing to explain. Most a six, well, eight year olds can understand the definition of the, uh, the conjecture that x, the n plus y to the n equals z to the n has no solution for and greater than a three greater than you going to the three.

Speaker 1:          05:56          Okay. This, this has been unsolved, has been, uh, hundreds of thousands of people try to solve it. And, uh, finally Andrew wiles from Oxford and Princeton had this final breakthrough moment in, uh, 1994. So he first approved in 1993 and then it was shown that he failed. And then, so that's, that's the, the human drama. And then night. So he had a, uh, he spent a year trying to find the solution. And there's this moment, this final breakthrough 358 years after this first form that buy from us as a conjecture. He, he, uh, he said it was so incredibly beautiful. It was so simple, so elegant. I couldn't understand how I missed it and I just stared at it and this belief for 20 minutes, this is him finally closing the loop, figuring out the final proof. I just stared at it and, but disbelief for 20 minutes.

Speaker 1:          06:48          Then during the day I walked around the department and I keep coming back to my desk looking to see if it was still there. It was still there. I couldn't contain myself. I was so excited. It was the most important moment of my working life. Nothing I ever done, nothing I ever do again, we'll mean as much. So this moment of breakthrough, how do we teach neural networks? How do we learn from data to achieve this level of breakthrough? That's the open question that I want you to sort of walk away from this part a of the lecture thinking about what is the future of agents that think, and Alexandra, we'll talk about the new future challenges next, but what can we use deep reinforcement learning to extend past the memorization pass pack pattern recognition into something like reasoning and achieving this breakthrough moment. And at the very least something a brilliantly in 1995 after Andrew Weil's homer Simpson and, and those are fans of the simpsons actually proved them wrong. This is very interesting. Uh, so he found an example where it does hold true the Pharmas Theorem, uh, to a certain number of digits after the period. Okay. And then finally, aggregating this knowledge into action. That's what deeper enforcement learning is about. Extracting patterns from raw data and then finally being able to estimate the state of the world around the agent in order to make successful action that completes a certain goal.

Speaker 1:          08:17          Uh, so, and I will talk about the difference between agents that are learning from data and agents that are currently successfully being able to operate in this world. Example, the agents here from Boston dynamics I ones that don't use any deeper enforcement learning, they don't use any, they don't learn from data. This is the open gap, the challenge that we have to solve. How do we use reinforcement learning methods, build robots, agents that act in the real world, that learn from that world. Uh, except for the perception task. So in this stack, you can think from the environment to the effectors that promise the beautiful power of deep learning is taking the raw sensory data and being able to, in an automated way. Do feature learning to extract arbitrary high order representations on that data. So it makes sense of the patterns in order to be able to learn in supervise, learning to learn the mapping of those patterns. Arbitrarily high order representations on those patterns to uh, to, uh, to, to extract actionable useful knowledge that's in the red box. So the promise of deep reinforcement learning why it's so exciting for

Speaker 3:          09:25          okay.

Speaker 1:          09:25          Uh, artificial intelligence community, why it captivates our imagination and about the possibility towards achieving human level. General intelligence is that you can take not just the end to end extraction of a knowledge from raw sensory data. You can also do end to end from raw sensory data, be able to produce actions to brute force, learn from the raw data, the uh, the semantic, uh, context, the meaning of the world around you in order to successfully act in that world and to end just like we humans do. That's the promise.

Speaker 3:          10:06          Okay.

Speaker 1:          10:07          But, uh, we're in the very early stages of, um, of achieving that promise.

Speaker 3:          10:13          Yeah.

Speaker 1:          10:14          So, uh, and successful presentation must include cats. So supervised learning and unsupervised learning and reinforcement learning sits in the middle, was a supervised learning. Most of the, uh, the data has to come from the human. They're the insights about what is inside the data has to come from human annotations. And it's the task of the machine to learn how to generalize based on those human annotations over future examples it hasn't seen before. And unsupervised learning, you have no human annotation. S reinforcement learning is somewhere in between, closer on unsupervised learning or the annotations from humans. The information and knowledge from humans, it's very sparse, extremely sparse. And so you have to use the temporal dynamics, the fact that in time these, the, uh, the continuity of our world through time, you have to use the sparse little rewards you have along the way to extend that over the entirety of, uh, the, the temporal domain to make some sense of the world, even though the rewards are really sparse.

Speaker 1:          11:20          Those are the two cats learning, uh, uh, the Pavlov's cats, if you will. Learning that I had to ring the door, ring the bell in order to get some food. That's the basic for them, the reinforcement learning problem. So from supervised learning, you can think of those networks as memorizers reinforcement learning is you can think of them of crudely. So as sort of brute force reasoning, trying to propagate rewards into extending how to make sense of the world in order to act in it. The pieces are simple. There's an environment, there's an agent, it takes actions in that agent. It senses the environment so there's always a state of the agent senses. And then I always, when taking an action received some kind of reward or punishment from that world so we can model any kind of world in this way. They can model an arcade game breakout here, Atari breakout, the engine has the capacity to act, move the paddle.

Speaker 1:          12:17          It has a, it can influence the future state of the system by taking those actions and then it's uses a award. There's a goal to this game. The goal is to maximize teacher award. They could model the card call balancing problem where the, you can control the polling angle, angle or speed. You can control the cart position, the horizontal velocity, the actions of that is the pushing the cart, applying the force of the cart. And the task is to balance the poll. Okay. And their award is one at each time step, the pull still upright. That's the goal. So that's a simple formulation of state action reward. You can play a game of doom with the state being the raw pixels of the, of the game and the actions that are moving the uh, the player around shooting and the reward, a positive one at eliminating an opponent and negative when the agent has eliminated, it's just a game, industrial robotics and you kind of humanoid robotics when you have to control multiple degrees of freedom, control the robotic arm control of the robot.

Speaker 1:          13:19          The state is the raw pixels of the real world coming into the sensors of the robot. The actions of the possible actions the robot can take. So manipulating each of its sensors, actuators, sorry, actuators. And then their reward is positive and placing a device successfully negative otherwise. So the task is to pick something up, put it back down. Okay. So and I, I'd like to, uh, sort of, uh, continuing this trajectory further and further complex systems to think the biggest challenge for reinforcement learning is a formulating the world that we need to solve the set of goals in such a way that can be, we can apply these deep reinforcement or reinforcement learning methods. So give you an intuition about us humans. Uh, it's exceptionally difficult to formulate the goals of life. Was this a survival, homeostasis, happiness? Who knows? Uh, uh, citations depending on who you are.

Speaker 1:          14:12          Uh, so the state, the state sight hearing, taste, smell, touch, that's the raw sensory data actions or think move. What else? I don't know. And then the reward is just a, it's open. All of these questions are open. So if we want to actually start to create more and more intelligent systems, it's hard to formulate what the goals are with the state spaces, with the action spaces. That's the, that's if this the, if you take away anything sort of in a practical sense from today, a form from the deeper enforcement learning part here, a few slides is that there's a fun part and a hard part too to all of this work. So the fun part, the what this course is about, I hope it's inspire people about the amazing, interesting fundamental algorithms of deep learning as the fun part. The hard part is the collecting and annotating huge amount of representative data in deep learning and forming higher representations.

Speaker 1:          15:06          Data does the hard work. So data is everything. Once you have good algorithms, data is everything in deep reinforcement learning the heart. The fun part again is these algorithms, we'll talk about them today, a little overview them, but the hard part is defining the world, the action space and the reward space. It's just the defining, formalizing the problem is actually exceptionally difficult when you start to try to create, uh, an agent that operates in the real world and actually operate with other human beings and are actually significantly helps in the world. So this isn't playing an arcade game where everything is clean or playing chess go. It's when you're operating in the real world, everything is messy. How do you formalize that? That's the hard part. And then the hardest part is getting out a lot of meaningful data that represents, that fits into that formalization that you formed.

Speaker 1:          16:01          Okay? In the, uh, the mark off decision process that's underlying the thinking of reinforcement learning. There's a state, you take an action, receive her award and then observe the next state. So it's always state action award state. That's the sample of data you get as an agent acting in the world. State action reward state. There's a policy where an agent tries to form a, uh, a policy how to act in the world so that in whatever state it's in, it, it has a, uh, a preference to a act in a certain way in order to optimize their award. There's a value function that can estimate how good a certain action isn't a certain state. And there is sometimes a model that the agent forms or about the world. A quick example, you can have a robot in a room at the bottom left starting moving about this room.

Speaker 1:          16:51          It's a three by four grid. It tries to get to the top right because it's a plus one, but because this is the casting system, when it goes, chooses to go up, it's sometimes goes left and right 10% of the time. So in this world it has to try to, uh, come up with a policy. So what's a policy? Is this a solution? So as starting at the bottom, arrows show the actions whenever you in that state that you would like to take. So this is, this is a pretty good solution to get to the plus one in a deterministic world. And it's the cast, the quarreled. Uh, when you don't, when you go up, you don't always go up. Uh, it's not a, a optimal policy because you have to have an optimum policy, has to have an answer for every single state you might be in.

Speaker 1:          17:32          So this, the outcome policy would look something like this. Now that's for when the, uh, the costs, the reward is negative 0.01 for taking a step now feet. Every time we take a step, it's really painful. It's a, you get a negative to reward. And so there's a, the, the optimal policy changes there is no matter what, no matter this tech cast, the city of the system, the randomness, you want to get to the end as fast as possible. Even if you go to the negative states, you just want to get to the plus one as quickly as possible. And then, so the reward structure changes the optimum policy. If you make the reward negative 0.1, then there's some more incentive to explore. And as we increase the reward or decrease the punishment of taking a step more and more exploration is encouraged until we get to the kind of, uh, uh, what I think of as college, which is a, you encourage exploration by having a positive word to moving around.

Speaker 1:          18:30          And so you never want to get to the end, you just kinda walk around the world, uh, without ever reaching the end. Okay. So there's a, the main goal is to really optimize reward in this world. And uh, because the reward is collected over time, you want to have some estimate of future award and because you don't have a perfect estimate of the future, you have to discount that reward over time. So the goal is to maximize the discounted reward over time. And in cue learning is, uh, an approach that, um, uh, that, uh, I'd like to talk to, to focus in on today. The, there's a state action value cue. It's a cue function. It takes an estate in action and it tells you the value of that state. It's off policy because, uh, we can learn this function without forming a, an optimal, without keeping an optimal policy and estimate of an optimal policy with us.

Speaker 1:          19:24          And what it turns out with the equation at the bottom with a bell mini equation, you can estimate, uh, the, you can update your estimate of the q function in such a way that over time it converges to an optimal policy. And the update is simple. You have an estimate, you start knowing nothing. The A, there's an old s, this estimate of an old state q, s, a t and then you take an action, you collect her award and you update your estimate based on the award he received and the difference between what you expected and what you actually received. And that's the update you. So you walk around this world exploring until he formed better and better understanding of what is a good action to take in each individual state. And there's always as in life and in reinforcement learning in any agents that act, you've, there's a learning stage where you have to explore, exploring pays off when you know very little.

Speaker 1:          20:14          The more and more you learn, the less and less valuable it is to explore. And you weren't to exploit. You want to take greedy actions and that's always the balance you start exploring at first. But eventually you want to make some money. You whatever the, the, the, the metric of successes and then you wanted to focus on, on the policy that you've converge towards that is pretty good, uh, near optimal policy in order to act in a greedy way. As you move around with this a bellman equation, move around the world, taking different states, different actions. You can update a, you could think of it as a cute table, uh, and you can update the quality of a, taking a certain action in a certain state. So that's a, that's a, that's a picture of a table there in the world with four states and four actions.

Speaker 1:          21:00          And you can move around using the bellmead equation, updating the value of being in that state. The problem is when this cute table grows exponentially in order to represent raw sensory data like we humans have, when taking an envision or if you'd taken the raw pixels of an arcade game, that's the number of pixels that are there. Get is larger than is larger than it can be stored in memory is largely, that can be explored. The simulation, it's exceptionally large. And if we know anything about exceptionally large, uh, high dimensional spaces and learning anything about them, that's what deep neural networks are good at. Forming the approximators, forming some kind of representation and exceptionally high dimensional complex, uh, space. So that's the hope for deeper enforcement learning. As you take these reinforcement learning ideas where an agent acts in the world and learn something about that world.

Speaker 1:          21:54          And we use a neural network as the approximator as the thing that the agent uses in order to approximate the quality, uh, either approximate the policy or approximately the quality of taking certain actions, certain state and, and therefore making sense of this raw information, forming, hiring higher order representations of the raw sensory information in order to then as an output, take an action. So the neural network is injected as a function approximator into the queue. It's, it's the q function is approximated with a neural network that's dqn that's deep cue learning. So injecting into the cue learning framework and neural network. That's what's been the success with deep mind with the playing the Atar Games. Having this neural network takes in the raw pixels of the Atari game and produces actions of values of each individual actions. And then in a greedy way, picking the best action and the learning, the loss function for these networks is uh, uh, two folds.

Speaker 1:          22:53          So you have a, you have a queue function, an estimate of taking a certain action. Certain state, you'd take that action and then you observe how it's the actual reward received is different. So you have a target, you have a prediction and the loss is the squared error between those two. And dqn has used as the same network that traditional dqn the first, uh, uh, yeah. Dqi uses one network testimate both cues in that lost function, uh, a double Dq, not dq and uses a separate network for each one of those a few tricks. They're key experience replay. So, uh, tricks in reinforcement learning because the fact that it works is incredible. And uh, as as a fundamental sort of philosophical idea, knowing so little and being able to make sense with a, from such a high dimensional space is amazing. But actually these ideas have been around for quite a long time and a few key trex is what made them really work.

Speaker 1:          23:57          So in the first, I think the two things for dqn is experienced replay is instead of letting the agent, uh, learn as it, as it acts in the world, agent is acting in the world and collecting, uh, experiences that can be replayed through the learning. So the learning process jumps around through memory, through the experiences and instead, so it doesn't, um, it doesn't learn on the local evolution of a particular stimulation. Instead learn of the entirety of its experiences. Then the fix target network, as I mentioned with a double Gq n the fact that the loss function, uh, a includes if you notice sort of to, uh, uh, to forward passes through the neural network. And so because when you know, very little in the beginning, it's a very unstable system and bias can, Eh, I can have a significant negative effect. So there's some benefit in for the target.

Speaker 1:          24:55          The forward pass the neural network takes for the target function to, uh, to be fixed and only be updated. Then neural network there to be only updated every a thousand a hundred steps. And there's a few other tricks, the slides that are available online. I will, there's a few interesting bits, uh, throughout these slides. Please check them out. There's a lot of interesting results here on the slide. Uh, showing the benefit that you get from these tricks. So replay experience, replay and fixed target network are the biggest. That's the magic. That's the thing that made it work for the Atar Games.

Speaker 3:          25:35          Okay.

Speaker 1:          25:36          And the result was achieving deep mine, achieving super, uh, above human level performance on these Atari Games. Now what's been very successful he use now with Alphago and um, the other more complex systems is policy gradients, which is a slight variation on this idea of applying neural networks in this deeper enforcement learning space. So dqn is q learning neural network in the cue learning framework. It's off policy. So it's approximating q and infer the optimum policy, a policy gradients. PG is on policy is directly optimizing the policy space. Then you'll network as estimating the probability of taking a certain action and the learning. And there's a great, if you want the details of this from Andrea Copath is a great post explaining, uh, illustrate it in an illustrative way. Deeper enforcement learning by looking at Pong Ping Pong. So the training process there is you look at the evolution of the different games and then reinforce also knows actor critic.

Speaker 1:          26:38          You take the policy gradient that increases the probability of good action and the probability of bad action. So the policy network is the actor. So the neural network is the thing that takes him the raw pixels, usually a sequence of frames and outputs, a probability of taking a certain action. So you want to reward actions that have eventually led to a winning a higher award and you want to punish the, you earned two degrees. You don't have negative gradient for actions that, um, that led to a negative reward. So the reward there is the critic. The policy network is the actor, uh, the pros and cons of Dqn of Policy Grazers, dqn most folks now, the success comes from policy gradients, active critic methods, different variations of it. The pros are, it's able to deal with more complex q function is faster convergence and most, uh, in most cases, uh, given, given you have enough data, that's the big con is it's needs a lot of data.

Speaker 1:          27:40          It needs a lot of ability to simulate huge, huge amounts of evolutions of the system. And, uh, because the model probabilities, the posse grainy smile at the probabilities of action, they're able to, uh, learn stochastic policies and Dq cannot. And that's where the game of go has received a lot of success with the application of the policy gradients. Where at first alpha go in 2016, beat the top humans in the world, uh, at the game of go by training on an expert, a games. So in a supervised way, starting from training on those human expert positions and Alphago zero and 2017 achieving a monumental feat. And in artificial intelligence, one of the greatest, in my opinion, the last decade of training on no human expert play playing against a itself, being able to beat under the initial Alphago and beat the best human players in the world.

Speaker 1:          28:37          This is an incredible achievement for reinforcement learning that captivated our imagination of what's possible with these approaches. But the actual approach, and you can look through the slides as a few, uh, interesting tidbits in there, but, uh, uh, the, it's using the same kind of methodology that a lot of game engines have been using and uh, certainly goplayers, uh, for the Monte Carlo tree search. So you have this incredibly huge search space and you have to figure out like which parts of it do I search in order to find the good position as the good actions. And so there you'll networks I used to do the estimation of what are the good actions, what are the good positions. Again, the slides have the fun, the fun details. Um, for those of you who are gambling addicts, uh, importantly, so, uh, the, uh, the stochastic element of poker, at least heads up poker.

Speaker 1:          29:28          So one on one has been for the first time ever this same exact approach had been used in deep stack and other agents to beat the top professional poker players in the world in 2017. The open challenge for the community, for maybe people in this room, uh, in 2018 is to apply these methods to win in a much more complex environment of Terman plate when there's multiple players. So heads up poker is a much easier problem. Uh, the human element is much more formalized bubble and clear there when there's multiple players, it's exceptionally difficult and fascinating. A fascinating problem that's perhaps more representative of agents that have to act in the real world. So now the Downer part, a lot of the successful agents that we work with here at Mit and bill did the robots that act in the, in, in the real world are using almost no deep reinforcement learning.

Speaker 1:          30:23          So deeper enforcement learning is successfully applied in context of simulation in context of game playing. But in a successfully controlling humanoid robotics or human robots, a humanoid robots or autonomous vehicles, for example, the deep learning methods they use primarily for the perception task. They're exceptionally good at making sense of the environment and uh, extracting useful knowledge from it. But in terms of forming actions, that's usually done through optimization based methods. Finally a quick comment on the unexpected local pockets. That's, that's at the core of why these methods are not used in the real world. Uh, here is a game of coast runners where a boat is tasked with receiving a lot of points. Traditionally the game is played by racing other boats and trying to get to the finish as quickly as possible. And this boat figures out that it doesn't need to do that in a brilliant breakthrough idea.

Speaker 1:          31:20          Uh, it can just collect a regenerating green squares. That's an unintended consequence, uh, that you can extend to other systems perhaps including, you can imagine what, uh, how the, the cat system over time at the bottom right can evolve into something undesirable and further on in these reinforcement learning agents when they act in the real world, the human life is often the human factor are often injected into the system. And so oftentimes in their reward function, the objective loss function, you start injecting concepts of risk and even human life. So what does it look like in terms of ai safety when the agent has to make decision based on a loss function that includes an estimate or risk of killing another human being? This is a very important thing to think about, uh, about machines that learn from data. And finally a to play around. There's, there's, uh, there's a lot of ways to explore and learn about deeper enforcement learning. And we have at the url below there, uh, a deep traffic simulation game. It's a competition where you get to, uh, uh, build a car that speeds, uh, that tries to achieve the as close to 80 miles per hour as possible. And I encourage you to participate, uh, participate and try to win. Get it a leaderboard, not enough mit folks there at the top 10. So with that, thank you very much. Thank you for having me.