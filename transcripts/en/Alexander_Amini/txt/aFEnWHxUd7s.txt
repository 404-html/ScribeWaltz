Speaker 1:          00:02          Thank you for the invitation and I'm glad to give up through then presentation or Basil Confederation meets a title as come to a vision meets social media social network. Actually a worry in this slice, I were introduced to the works that we have down in tens and Alf and as this techniques is one or some research topics in our labs and as a, this topics can be shifted with the, with the product of our company. Okay. First I will give our introduction of brief introduction of our company. So I'm never sure whether have you heard tensen before? Actually 10 center is, um, one of the largest internet company. I think I can say that not only man of China, either on all over the world. So this is the one of the largest, if not the company and the market capitalized is 500 billion US dollars. So I think that even comparable with facebook or even larger than facebook and in sometimes, and also we have several products and several, several trends for the most important.

Speaker 1:          01:02          One is a social, social, not social media, social network. So we have wechat and uh, they say which are also, we call the wishing in Chinese. So we chat and Oscar Qq. So this is a two types of instant messenger or something like that. Facebook and Instagram, I mean us and also we have games and the tencent games is the largest, I think that's the number one game company with have the ratio of you in the USA with the over 10 billion US dollars in 2016. And we have several, you know, maybe someone is familiar with summer a sale or should we have already acquired 80% of the officers shares. So this is a, we think that we are the largest. We, we are the owners are the eight percentage share. The owners are so sale. And uh, also we have some content working on incontinence, something like that.

Speaker 1:          01:56          Tencent video, this is something like the Netflix and also the tencent music. There's something like spotify and we'll have the largest a copywriter in a amount of China for the mill. That corns and also we have tens and news. So it about content. And also because the Tanzania, one of the largest internet companies. So we have the other, you know, the businesses. That's how as a finance or insurance or something like that, this is all based on internet. Okay. So these are the main structure of tensen and in the next hour give birth instruction. But Tessa allies that I were, I'm now in, so it tends to an Arab have actually is funded in 2016 and we have two locations. Why The or China in Shenzhen? I'm China and a one office is Seattle. USA. Okay. Actually we have four research topics under the center is the machinery.

Speaker 1:          02:49          It is, we've focused on the basic machine learning, uh, the machinery series and also large, large scale machine learning platform. And as well as the, and based on the machine learning part, we have three application and research areas come to vision, language and speech. And uh, we would like do something actually from, for this that we focus on the areas of this application and we'd do some or reinforced learning for the decision and also general agent for the creation. And also we would like to maybe make some combinations for the [inaudible] of the world of the humans of the world. Okay. So, uh, in the following, in a falling out this lecture I will give an introduction about competitor varies the works that we have done in computer vision. And uh, we from the low level image and image video processing to the middle of the image we do analysis and the human that depot was the major and the video understanding.

Speaker 1:          03:46          Okay. And we have done several works here, subtract or start transfer the husband and said Lucia. Under the hazing I would like us to like some topics that had already this technology. We have already got pop paper publish and also shifted to the product. And the second is the image analysis. We focusing will, I will try to introduce some process automation and as a video interest attractiveness and the video, uh, classifications. And also in the last four, the even harder deeper learning. In a deeper understanding of the videos, I will give some influx images. I will give some instruction about captioning and also the natural language we do localization. Okay. First they see the video stock transfer. Let's say they'll give a brief, uh, image about the video start transfer and we'll have the videos and we have a style imagery and we perform the stock transfer.

Speaker 1:          04:37          We would like the style of videos to preserve, to preserve the content of the video sequences. And also it will borrow the style information from the stout stout image. And the most important is, and this is very popular in 2016 as a prisma, but nowadays for the video, we have the, we are the first work working on the videos for the video. This is the most important thing to maintain the temperature consistence between adjacent offerings. So from the, you know, from the sucrose here, the temporal we have compared with the traditional image style transfer and though, oh, compared with our proposal mastered, the image is traditional image start transfer cannot maintain the consistencies. So the circles and the box stays there. You can see that as a style changing the changes a significantly. But for our video part, it can maintain a good consistency. So this will prevent, it will, uh, it will eliminate the chattering jet, the flickering artifacts in the videos.

Speaker 1:          05:42          Okay. So this is the Memphis and was a man network of our video stuff transfer. And the first party is a style stylized and network. This is a very traditional and a, we have a a encoder decoder encoder, decoder style style and we'll have the encoders. We escaped, we connected with five restaurants, rest blocks and as the decoders. And after that we realize the last networks to computers are stylized network and a content loss network and also the uh, the temperature loss. So with this loss there's no well designer we can, we can try the networks to makes an network had, can burn, can preserve the content and the ad well as introduced the style and as most important and can preserve the consistency between our jason the frames. Okay. So for this or special training, we have the two frame coordinator training. So it means that we input the address into firms, into the network and the then utilize that design as obsessive as it design the loss function to train the network.

Speaker 1:          06:43          And also, uh, another, another advantage is that we do not need to computer optical flow information on that, on the fly. So it means that we can't, well we can't, well Mexico we can well Max though inference very fast because of which do not need to make a computer that optic flow. Normally the optical or is our most complicated part in video parts. Okay. And I see that it, because we are uh, you know, [inaudible] instant message APP is so the person who will share their food, their pictures in with the, with the, with their friends. So the picture that may be taking and the different conditions for this case we want to handle the, yeah, makes the image quality house the house, but for the low light images. So we have, if you take an image in the low light, it can now you cannot define it cannot find the edge of the content and also the color.

Speaker 1:          07:41          You cannot find the, uh, you cannot take the right colors. So we proposed a method to enhance the low light images and we have the image here and with Medicaid displayed to the original video you made part and also the age apart. So because the algae the most selling to the human perception. So for the low light in low light to input our regional part we do could we use a [inaudible] style to encode the image and the color image, a catalog or no results. And also for the age part we utilize also something like the encoder decoder and the afterwards we have the skip the layer and the ways the island style to catch the edge information. Then paste this to the continent formation and eh as an also the edge information. We'll combine them together then to have enhanced the quality of the images and then we got some perceptual losses and also other adverse or loss is to enhance the quality of the image.

Speaker 1:          08:38          Okay. This is some of the results and we have the first row is a low light images provided and we use the our network that we encounter. We can, well rick, we recover the content of the image and also the colors that can be well preserved. Okay. So this is, this is a one pot of, we can share the image, improve the image quality and uh, the next is the next work is a mobile pose estimation actually for the post has measured helping well to study the [inaudible] had been well studied but for the mobile part if we need to perform the network compression and also Makayta to detect the scattering of other joints very accurately. So this means this part when mostly focusing on the network compression and the networks are proning. Okay. So they, we shifted the network from the pcs to the [inaudible] wise and we can perform the detection of the person that Marty persons and also for some front end background and something like, and the also instances to the uh, to the colors of their closest.

Speaker 1:          09:43          And as they see the demos here and then the first, the latte. I think that this is a maybe for the projection party. So this is not for real time. And they see the 22 key points for the joints of the persons under the under the left. Why is the way considered to detect the hand? Because the most of the hand, the hand is detection will we'll facilitate our, you know, the purchase, the interaction with the, with the phones, all the, all the TVS. Okay. And also for our mobile post estimation, we have some, you can see that for this one we are Robinson to the full or half body, half body, half body. That inclusion is on the half bodies. And also we are, we are able to perform a single or multiple persons. And also we are, you know, we are insensitive to the dressing colors or the other.

Speaker 1:          10:36          They are the dressings and also we are robust. The to the front and back, even from the front and back we had to tack to the joints under the uh, under the, under the, under the uh, information of the scallops. And uh, also we have already deployed this techniques into mobile Qq. And for this one, this is uh, the, the left, the right one is that detector of matter, person of the scallions and the laughter. Why is that we have the skeletons and we can have a dancing machine so we can follow the, you know, the guidance that underperformed the dances and the zen, they say they're very interesting to interact with the phones or interacted with the TVS. Yeah. So you can, with this one we can find that the, you know, with the disciplines detection we can track that can match your positions with a designed other specified the actions.

Speaker 1:          11:25          Okay. So this is, I work on the post that information. Okay. And another topic is very interesting topic either for the video attractiveness attractiveness. Now that, because for a, we have the tencent video, so we have a lot of, a lot of video dramas have a lot of times TV series uploaded on the website. So we want to find whether the users is interested in to some part of the videos. So we would like to, whether we can predict the video attractive from the only the visual information or the outer inflammations. So for this one we have the video input and we, sorry, we have the video input and also I also is associated with the auto import and that we would like to predict it's attracted us under the crunchiness is a orange line ended up predict wise is a predict ones. Either you the Blue Line actually for this attractive, it is very hard to get the ground truth data.

Speaker 1:          12:23          So we utilize the views because this is a tencent video have a lot of registrators. So there are many, many of you, many, many viewers at this time. So for each of these is this, each of these values in each points, it means that at this specific time harmon person have already have already watched this video sequence at this specific time. So the, uh, the statistically the larger of the number, it means that their amendment, there are a lot of men, a larger number of person watching this video at this moment. So this prominent, should it be the machine, uh, more attractive, much more attractive, attractive than they other with similar values. So we we utilize this video as a checklist. Crunch, choose value. Okay. So, uh, for this task we built a large large large scale data set. We have a one style about west of the pepsis and the in total about seven to eight hours.

Speaker 1:          13:24          And uh, there are, we consist of 2,270 episodes of American dramas and also search. So the two Korean dramas and also seven episodes of China's drummers. So we have this one, we can see that diverse of a types and we have also the v the view information as we can, you know, start a Mackie to the robusta, to the video or check themselves studying. And the full, this is a statistical information of our data set and the, this is a number of the episodes and this is a number of the views, attracting the views and the, the law once in a lot. Okay. And also besides of the views actually for it, because it's the father tencent video, you are for the v the video platform, you will have some of the other better data or some other user engagement tree formation. They are the nine catchment information would crowd is a start of a, is a stat of the forward and the end of the forward, the start of the backward and the start of the uh, end of the backward and also have some pallet screens and doubling the number of the billing squeeze and as the likes of the polling screens.

Speaker 1:          14:37          And also we have the, the, the forward skips and a, the first, the word faster. We were real wind skips. So with this ones we can get another more information about the user engagements with a video with as this user in catchments of videos, you can can have more info, more detailed information about, they do our checklist and we can pass on these engagements to analysis more. Uh, Alana's more about the human behaviors. Okay. Uh, as we mentioned before, we would like to make a prediction about the attracted from the audio and the video, all the information and the visual information. So for this, two different modalities because we have a audio part and the other visual part, so we four, we proposed three different theorists. Strategy. Why is in the low level and a y in the middle of error and the wine in the high level.

Speaker 1:          15:29          And uh, they see that three star. This is the structure of the film part. And uh, we have also each part we propose, we utilize, we perform our contact skating here. Okay. So based on this, this architecture that we perform, that prediction and we, we have the, we also propose us three, the three criteria to evaluate whether we can actually the pro, we can actually perform the Eh, uh, accurate prediction. So we have, we have the four metrics and a then we'll compare to with different visual representation from vg to inception to rest, rest, not. And therefore the other part would compare with MFCC and an and also the [inaudible] part. So we can say that you consider all the auto feature, that innovative feature together we can get better results and better results. But if we can sample the middle level, high level and a love of fueling strategy together, we can get to the best of performance.

Speaker 1:          16:31          So understanding that run and nothing and nothing to note that is, I will have the, if we have the engagements, the is that is the behavior of the users. So we can, if we can get an engagement so we can make a very accurate, very accurate prediction on the video of checkmates because if they like it, it will watch it and otherwise they will leave this, leave this videos so that the human engagements you the most important part in the indicator, the indicator of the video checklist. Okay. So these are for the video attractiveness starting. And also we started, we make some study around the video classification and as a mentioned there before them and mentioned them by Lisa before the moment at the moment, same time, the three seconds of the video that also about Vedo complications, right? And then a nother before that, there's a, uh, there's a benchmark tests that aren't the ucf one zero one.

Speaker 1:          17:29          So we perform for this part, we perform the two streams, we utilize that to stream [inaudible] on the, on the video classification and the one stream is, uh, describe the content. And once from it is crap. The optical information optical flow meet means that an emotion information between adjusting the frames and we proposed a principal back propagation propagation that we forward all the snippers so the forwarding Romanian can capture all the information of the videos and but for the backward we will perform a selectively to backward only a selected number of snippers netplus. So one info wine, a wine, one benefit is that from this efficient backpropagation we can't get no, we can perform a efficient and training. And uh, another benefit is that we can also perform that it's named Paris in different scales to characterize the emotion that different emotion formation in the videos.

Speaker 1:          18:25          Okay. This is our, uh, results and for this, that sets the pippin that here and how even now to achieve the type one error rate in about only 8.6 the percentage. So this is a slow, this is smaller than the other competitive models. Okay. Also for the beetle classifications, we we po, Po, we're participating in the youtube, Youtube eight main and challenge the here and now for the Youtube 8 million challenges here. They only provided the weight of the features for the visual features and it'll auto features. So we work on the audio and the video feature to perform the video classification. And a we for this is that we perform a tooth two level consultation. Why in the end the video liable. So the Beta level where average is uh, the all the frames together to get a little bit of of the representation and uh, uh, we propose the amount of fuel in layer here as a deputy here and as empathy with, for the customer fires, we tried to mix mixture of experts and luck.

Speaker 1:          19:27          Global accuracy. Precision is Arbutus only 0.82. Okay. And also we performed the friend level classification. Now from a level of transparency [inaudible] it very intuitively to have the LSTM and all that Gio to model the sequential data. So for the video, the video feature, this is the can be viewed as a sequential data. Say We, so we utilize the about directional lstm and [inaudible] and the GRU and also we consider different, uh, different emotions information. So we perform our motto scale wise. So for each scale we perform, we perform a bad direction, but direction of Gru, uh, our lstm and the JP here, the global, the global average precision is thereupon 0.83 and a if we consider the video level part and as a frame labral part together, we can get as a, jp is the 0.84. And uh, we achieved about to the force positions number four, the first productions over the 606 400 submissions.

Speaker 1:          20:34          Okay. So this is about the video and image and the data analysis. Okay. Afterwards I will ensure that I will, uh, afterwards I will give a instruction about some workers who would have done on the learn on the understanding. One part is the image or understanding here and the image I understanding this is a very difficult part because we a an image to understand the image caption and the image is a very difficult part because we're not only to understand the image content, but also we need to learn the language of the properties or the behavior of the language of compositions. And also we need to perform a Amati Motel interactions between the image and the sentences. Okay. So for this one here, we also utilize our traditional, something like the traditional architecture for the image. We utilize that image you see in to encode the information.

Speaker 1:          21:29          And the week Max we get the image representation in the global wise and in the local wise for the, if we have a saying here, if half a million, if you are familiar with sailing for the last, uh, at the uh, fully connected layer, this is a global representation and the for the intermediate convolution layers, this is a global, they see the local representations. So for each image, a image, a thing, we can get the, we can get to the image global and the local representations. And with this [inaudible] if we have different uh, uh, the such as the inception and the rest net or the egg, we have different things that we will have multiple, multiple representation of the image for each year. For each set we have a global representation and a local representation. And with this different representations we can perform a marty stage attention. And the Marty come, um, Maddie's stage attention can first to summarize the information from definitely encoders such as the rest, not in separate ways.

Speaker 1:          22:33          You separately before and also the uh, the vgg. So we can, we can sample this, the present them together and also Max this representative and to interact with each other and for the attention part so they can broadcast to their, their information to the other encoder to, to let the other encoder know whether you, what information has been already considered the for the generation part. Okay. If we had afterwards with a will perform the multi stage attention, then we'll utilize that. Another decoder, you did another uh, another decoder to decode these representations. Two sentences. The decoder. Normally we can utilize our R and R and, and uh, the Alstom and Gio is the most effective arlene's. So, uh, we have in here we utilize that Lstm so we have definitely, we have the image, you can call the catheter representation and then we have the Dakota to Dakota representation into a sentence.

Speaker 1:          23:37          Okay. So this is first, this, this pay first. This is, this task is you need to consider the information of the continent and then you made in the image and also integrate with the sentences to make the two folks on the, each object to is a generation of each words such as for the persons. You need to pay attention to the object of the, uh, the person needs to pay attention to the local region, other the persons, and then generate the words and the father, the hills. You need to pay attention to the local region of the image and then generates the words here. Okay. So for these things, we propel based on our multi stage attention. We have, uh, perform, uh, you know, we have some Meta hour that's under Mac, Microsoft, the cocoa challenge here they say the most, I think that is the most, uh, most influence board influence that fat on the image captioning.

Speaker 1:          24:36          For each image it will, uh, each image is a company with five sentences under there about uh, uh, about 100,000 images and therefore h majors as it is accompanied with five sentences. So I think the appeared the pair Datas for image. Understand the pay data. It is about 600 southern pairs. Okay. Based on this data, you can perform the, you can perform, you can perform, you can promote a training of the the of your model and that's the image that the results of the test data on the website and a for this website we have, we have several criteria to evaluate your results and for the reason I say that for this, see the blue and the blue blue one blue two plus three blue for and also we have the meter Raj and cider say cid or this is for the crd or this is the most suspicious specifically designed for evaluating the quality of the image captioning.

Speaker 1:          25:37          Okay. So far this fall for this we can say that for the CFI and the c 40 we can achieve all four. All the metrics we achieve the top one, top wide results and the compared to the other. Are there other models here? Okay. Also we also, we have transformed, we have, we have a collector, another Chinese dataset. We have an image here and also we have the image and that is a company that a Chinese sentences and a we're trying to models and also with deployed the, we have released the results and uh, we, we, we, we build a late later program in we chat so you can experience it if you have the, which had here. Okay. This is the Chinese for intelligent image recognition. Okay. So for this image it had for the Chinese give a brief brief instruction. Uh, very good disclosure about the content of the image.

Speaker 1:          26:40          Okay. And the, for this, if we have the image of captioning the results, we'll give a image and I gave her a sentence or two describe the content of danger. We can perform first. The most Australian boy, eataly is a performer he made discursion and other is that we have this sentences that were going to perform the image retrieval and the also as well as the magic recommendations and also for the visual dialogues. And uh, the most important. I say if we have the image under the, the discussions that we can help the visually impairment. Imparity he made a person to read or to see the images. Okay. So this is the work I imagine captioning and uh, afterwards, uh, the last work that I'm going to introduce is a natural language of video localization. They see the also are very challenging challenge a task one to you utilize a language we give.

Speaker 1:          27:33          Uh, will you try, are we giving our team the video and the uh, natural language or discretion? The girl, we would like to localize a stigma too in the video which a correspondent to the given natural languages Christian, it means that if you will, if we, sorry if we have a very long, they do say process and a, we have a sentence that described such as a woman rails, keep a kite back, lean forward herself. We would like to localize this sentence in the videos. So this is a virtue as same as the image captioning. It is a very challenge because first we need to understand the language and the next that we need to understand the video. Same question, but for the video sequence, the understanding is the human challenges that you majors. Because the temperature, we need to not only consider the image con the, the special content of the image, but also we need to consider the temporal relationship between the, uh, adjusting the frames.

Speaker 1:          28:32          So we perform, we propose to us single story, a single stream and natural language. They don't look as a Australian network for this part. We have the we, sorry. Uh, we have the videos and I will have a sentence in the world perform frame by word interactions or mentions here all mentioned Syria matching pro matching performance matching between the sentence under the video. Then we have, uh, uh, with, with this matching behaviors, we perform a temporal proposals and have different anchors. Then we'll have definitely scores with this, the single stream processes we can efficient and I localize the sentences. He's a video sequences. Okay. And as this is some results of our, this is someone who does of our proposed a masters and they see the videos and as they say, have a sentence, a man in a red shirt caps his hand. And, uh, the, the, the, the gray ones is our ground shoes and a, the green greenland is, we utilize a vgg 16 as the image as an encoder.

Speaker 1:          29:39          And a for the blue ones is we utilize the inception before as an encoder. And, uh, the yellow was when you guys assist really as the encoder. So you can see that if we utilize a vgg 16 and, uh, uh, inception before, we can get a reasonable accurate or is out at your, for this, for this, specify the task. We're up from all the competitor models on this, especially the task. And therefore this, that sat the, for this, the, the camera was zoom out to show where the, what four is coming front so they can, you can see that, uh, the for the, for the, uh, for the furion with during in the inception before and optic flow, we can get to the accurate prediction on this, on this example. Okay. Wait, uh, another, uh, besides we examine, we examine the frame and the frame by word attentions here and the, for this sentence, what for, for this sentence, for this center and worked for in forest for they say you can see that for this all video sequences in the forest, the first day is appeared over all frames.

Speaker 1:          30:50          So the attach matters will be attended, will be triggered at equally. But for this, for this workforce only appearing in the forest first of frame so it won't have this here. Okay. And uh, uh, the same observation that can be, uh, can be observed from this to give me examples. Okay. So this is a lots of works that I introduced about the computer vision meets with no social network. Actually, besides that in our lab with not only are we not only do the worker uncovered variant, but also we have some, some of the other works on the AI projects. The most important APP or one of the most important, I projected the APP plus house and we would like to deploys a or utilize the AI technology to Max a health care. And also the first, the healthcare, the first day is that we would like to ut that ai technology or deep learning and our to analysis a medical images and um, to, to help the, to help the doctors to Mac the uh, to, uh, to have the detection to make the diagnosis.

Speaker 1:          32:04          And the, for this, for this specific, for this specific cancers such as a lung cancer screens about for this recognition rate is about, is over 98 percentage. And also in the future we would like to, uh, would you lie to the, to perform a computer added that diagnosis. And we have some artists scripts about the distribution of the passions and also we have some medical images about the passions and they would like to have a doctor to attack max attack gnosis and also even in the future we have to have the doctor to Oh, this design the treatment about some of the disease. Okay. Another important topic is the Ai Gang because the gang, because tencent, the game is the largest in the number one game. Company is a of the word. So would like to utilize the AI technology to evolve to be more deeply involved in the game.

Speaker 1:          33:04          Maybe that designed the plane or something like that to make the game to be more interesting and a more, uh, more interesting to the you, to the players. Okay. So I will give a brief summary here. And the first I will give an introduction about our talk computer vision meets social networks. And we introduced some works that we have down in Tenson al up from that research, the product and something related to the image of video processing, understanding, analysis, something like that. And also we have, I have introduced the, about some one hour, uh, several, uh, our important projects. A applies to ax, the ax, maybe house. Are you in the future? Some robotics or something like that. Okay. So this is my lecture. Okay. Thank you.