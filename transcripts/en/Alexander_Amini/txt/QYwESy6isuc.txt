Speaker 1:          00:02          Thanks for having me here. Yeah. So I'm, uh, I'm based in the Cambridge office, which is like a 100 meters that way. Um, and we do a lot of stuff with a deep learning. We've got a large group in Google brain and other related fields. So hopefully that's interesting to some of you at some point. So I'm going to talk for about 20 minutes or so. Um, this sort of issues and image classification theme. Let me hand it over to my excellent colleagues hunching king who's going to go through an entirely different subject in a using tensorflow debugger and eager mode to make work intensive flow easier. Maybe that will be good. Okay. Um, so let's, let's take a step back. So have you guys seen happy graphs like this before? Go ahead and smile and nod if you've seen stuff like this. Yeah. Okay.

Speaker 1:          00:50          So this is a happy graph on image, net based image classification. So image net is a data set of a million some odd images for this challenge. There were a thousand classes, um, and uh, in 2011 back in the dark ages when nobody knew how to do anything, the state of the art was something like 25 percent error rate on this stuff. And in the last call it six, seven years, the reduction in error rate has been kind of astounding to the point where it's now been talked about so much. It's like no longer even surprising and everyone's like, yeah, yeah, we've see this, um, human error rate is somewhere between five and 10 percent on, on, on this task. So the contemporary results of, you know, two point two or whatever it is, percent error rate or really kind of astonishing. Um, and you can look at a graph like this and make reasonable claims that well, machines we're using deep learning are better than humans that impinged classification on this task. That's kind of weird and kind of amazing. And maybe we can declare victory and fill audiences full of people clamoring to learn about deep learning. That's cool. Okay. So, um,

Speaker 1:          02:06          I'm going to talk not about image data itself, but about a slightly different image data set. Basically people were like, okay, obviously image net is too easy. Let's make a larger, more interesting data set. So open images was released I think a year or two ago. It's got about 9 million as opposed to 1 million images. The base data set has 6,000 labels as opposed to 1000 labels. This ultra multi label. So you get, you know, if there's a person holding a rugby ball, you get both person and rugby ball and the Dataset. It's got all kinds of classes including stairs here, which are lovely, lovely illustrated. And you can find this on get hub. It's a nice data set to play around with. So uh, some colleagues and I did some work of saying, okay, what happens if we apply just a straight up inception based model, um, to this data we traded up.

Speaker 1:          02:56          And then we'd look at some how would classify as some that we found on the web. So here's one such image, a image that we found on the web. All the images here are creative comments and stuff like that. So it's, it's okay for us to look at these and when we apply an image based, a, a image, nobody kind of model to this classifications we get back or kind of what I personally would expect. I'm seeing things like bride dress ceremony, woman wedding, all things that as an American in this country at this time I'm thinking or make make sense for this image. Cool. Maybe we did solve it. Image classification. So, um, then we had played it to another image. Also have a bride and a, the model that we had trained up on this open source image data set returned the following classifications, clothing event, costume red and performance art.

Speaker 1:          03:59          I'm no mention of bride, also, no mention of person Nis regardless of gender. Um, so in a sense this, this model is sort of like missed the fact that there's a human in the picture which is maybe not awesome and not really what I would think of as great success if we're claiming that image classification is solved. Um, okay. So what's going on here? Um, I'm going to argue a little bit that what's going on is, is based in, to some degree on the idea of stereotypes and uh, if you're, if you have your laptop open, I'd like you to close your laptop for a second. This is the interactive portion where you can interact by closing your laptop and, um, I'd like you to find somebody sitting next to you and exercise your human conversation skills for about one minute to come up with a definition between the two of you have, what is a stereotype keeping in mind that we're in sort of a statistical setting. Okay? So have a quick one minute conversation with the person sitting next to you. If there's no one sitting next to you, you may move. Ready, set, go.

Speaker 1:          05:18          Three, two, one. And thank you for having that interesting conversation that easily could have lasted for much more than one minute, but such is life. Let's hear from one or two folks who had something that they came up with. It was interesting. Yeah. Go ahead. Your name is at that? Yeah. Okay. What did you

Speaker 2:          05:42          based?

Speaker 1:          05:46          Okay, so you saying that a stereotype is a generalization that you find from a large group of people and you apply it to more people. Okay, interesting. I'm certainly agree with large parts of that. Yeah, go ahead.

Speaker 3:          05:59          That's based on probability and experience with the human goal. For you, would it be free?

Speaker 1:          06:10          Okay. So, so here the claimant said it's a label that's based on experience from within your training set. Yep. Super interesting. And I'm the probability of label based on what's, what's your training say? Cool. Maybe one more.

Speaker 3:          06:26          Oh yeah, got it. You're making a prediction based upon to be coordinated and integrated.

Speaker 1:          06:35          Okay. So the claim here, that stereotype has something to do with unrelated features that happened to be correlated. I think that's interesting. Let me see if I can. This was not a plant. I'm sorry, your name was

Speaker 3:          06:46          awesome.

Speaker 1:          06:46          Constantine. Constantine is not a plant. Um, but I do want to look at this a little bit more in detail. So here's a, here's a Dataset that I'm going to claim is, is based on running data. Um, so, uh, in the early mornings I pretend that I'm an athlete and go for a run. Um, and uh, this is a dataset that sort of based on risk that someone might not finish a race that they enter in a. So we've got high risk. People are, you know, are in yellow and lower risk. People are in red. I'm going to walk, get this data. It's got a couple of dimensions. I might fit a linear classifier. It's not quite perfect if I look a little more closely, if I've actually got some more information here I've done just to have x and y also had this sort of color have outline.

Speaker 1:          07:36          So I might have a rule that if this data point has a blue outline, I'm going to predict low risk. Otherwise I'm going to predict high risk. Fair enough. Um, now the big reveal, you'll never guess what, um, the, uh, the outline feature is based on shoe type. The other x and y are based on how long the races and sort of what a person's weekly training volume is. But whether you're foolish enough to buy expensive running shoes because you think they're going to make you faster or whatever, um, this is what's in the data.

Speaker 3:          08:13          And in

Speaker 1:          08:15          traditional machine learning, supervised machine learning, we might say, well, wait a minute, I'm not sure that shoe type is going to be actually predictive. On the other hand, it's in her training data and it does seem to be awfully predictive on this data set. We have a really simple model is highly regularized. It still gives you know, perfect or near perfect accuracy. Maybe it's fine.

Speaker 3:          08:39          And

Speaker 1:          08:41          the only way we can find out if it's not, I would argue is by gathering some more data and I'll point out that this data set has been diabolically constructed so that there are some points in the space that are not particularly well represented. And you can maybe tell yourself a story about maybe this data was collected after some corporate five k or something like that. Um, so if we go out and collect some more data, maybe we find that actually there's people wearing all kinds of shoes on both sides of, of our imaginary classifier. But that this shoe type feature is really not predictive at all and this gets back to Constantine's point that perhaps, uh, relying on features that are strongly correlated but not necessarily causal, may be a point at which we're thinking about a stereotype in some way. Um, so obviously given this data and what we know now, I would probably go back and suggest to linear classifier based on these, these features of length of race and weekly training volumes as potentially a better model to how does this happen?

Speaker 1:          09:51          What's, what's, what's the issue here that said play. I'm one of the issues that's at play is that in supervised machine learning, we often make the assumption that our training distribution and our test distribution are identical. Right? And we make this assumption for really good reason, which is that if we make that assumption, then we can pretend that there's no difference between correlation and causation. And we can use all of our features, whether they're what Constantine would call meaningful or causal or not. Um, we can throw them in there and so long as they're testing training distributions are the same. We're probably okay to within some some degree, but in the real world we don't just apply models to a training or test set. We also use them to make predictions that may influence the world in some way. And there I think that the right sort of phrase to use isn't so much tests set.

Speaker 1:          10:51          It's more inference time performance. Okay. Because at, at inference time, when we're going and applying our model to some new instance in the world, we may not actually know what they let the true label is ever or things like that, but we still care very much about having good performance and making sure that our, uh, test that our training set matches our inference distribution to some degree is, is like super critical. Um, so let's go back to open images and what was happening there. Um, you'll recall that it did quite badly on, at least anecdotally on that image of a bride who appeared to be from India. Um, if we look at the Geo diversity of open images, this is something where we did our best to sort of track down the geolocation of, of each of the images in the open image data set.

Speaker 1:          11:39          Uh, what we found was that an overwhelming proportion of the data in open images, uh, was from North America and six countries in Europe, um, vanishingly small amounts of that data were from countries such as India or China or other places where I've heard there's actually a large number of people. So this is clearly not representative in a meaningful way of sort of the global diversity of the world. How does this happen? It's not like the researchers who put the open images dataset. We're in any way on all intention. They were working really hard to put together what they believe was a more representative data set than the image net. Um, at the very least, they don't have 100 categories of dogs in this one. Um, so what happens? Well, you could make an argument that there's some strong correlation with the distribution of open images with the distribution of countries with high, low, high bandwidth, low cost, Internet access.

Speaker 1:          12:41          Um, it's not a perfect correlation, but it's, it's pretty close. Um, and that if we're doing a, if one might do things like base an image classifier on data drawn from a distribution of areas that have high bandwidth, low cost Internet access, um, that may induce differences between the trading distribution and the inference time distribution. So none of this is like something you wouldn't figure out without it if you sat down for five minutes. Right? This is all like super basic statistics. It is in fact stuff that's this just six people have been sort of railing at the machine learning community at for the last several decades. Um, but as machine learning models become sort of more ubiquitous in everyday life, I think that paying attention to these kinds of issues becomes ever more important. Uh, so let's go back to, to what is a stereotype and I think I agree with Constantine's, um, idea. And I'm going to add one more tweak to it. So I'm going to say that a stereotype is a statistical confounder. I think Susan Constantine's language almost exactly a that has a societal basis.

Speaker 1:          13:55          So when I think about issues of fairness, if it's the case that, um, you know, rainy weather is correlated with people using umbrellas, like, yes, that's a confounder. The umbrellas did not cause the rain. Um, but I'm not as worried as a individual human about the societal impact of models that are based on that module. I'm sure you could imagine some crazies, scary scenario where that was the case, but in general, I don't think that that's an issue, but when we think of things like internet connectivity or other societal factors, I think that paying attention to questions of do we have confounders in our data, are they being picked up by our models? Um, is, is incredibly important. So, um, if you take away nothing else from this short talk, I hope that you take away a caution to be aware of differences between your training and inference asked the question, uh, because statistically this is not a particularly difficult thing to uncover if you take the time to look in a world of kaggle competitions and people trying to get high marks on deep learning classes and things like that.

Speaker 1:          15:07          I think it's all too easy for us to just take datasets as given not thinking about them too much and just try and get our accuracy from 99 point one to 99 point two. And as someone who's interested in people coming out of programs like this being ready to do work in the real world at a, I would caution that, um, we can't only be training ourselves to do that. So with that, I'm going to leave you with a set of additional resources around machine learning fairness. These are super hot off the presses in the sense that this particular little website was launched at I think 8:30 this morning, something like that. So you've got it first. Um, mit leading the way, I'm in on this page. Uh, there are in, yeah, you can open your laptops again now. Um, uh, there are a number of papers that go through this sort of like a greatest hits of the machine learning fairness literature from the last couple of years.

Speaker 1:          16:06          Um, really interesting papers. I don't think any of them are like the one final solution to machine learning fairness issues, but they're super interesting reads that I think help sort of paint the space and the landscape really usefully. There are also a couple of interesting exercises there, um, that, uh, you can access by a colab. Um, and uh, if you're interested in this space, there things that you can play with a, I think they include one on adversarial d biasing where I'm, because you guys all love deep learning. Um, you can use a network to try and become unbiased by, uh, making sure that by having an extra output head that predicts a characteristic that you wish to be unbiased on and then panelizing that model if it's good at predicting that, that characteristic. Uh, and so this is trying to adversarially make sure that our internal representation in a deep network is not picking up a unwanted correlations around one biases. So I hope that that's interesting. Um, and, uh, I'll be around afterwards to take questions, but at this point I'd like to make sure that sunshine has plenty of time. So thank you very much.