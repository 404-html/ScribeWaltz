Speaker 1:          00:02          Let's get started. So good morning everyone. My name is Aviva and today we're going to learn about how deep learning can be used to build systems capable of perceiving images and making decisions based on visual information. Neural networks have really made a tremendous impact in this area over the past 20 years and I think to really appreciate how and why this has been the case. It helps to take a step back way back to 540 million years ago. And the Cambrian explosion where biologists trace the evolutionary origins of vision. The reason vision seems so easy for us as humans is because we have had 540 million years of data for evolution to train on. Compare that to bipedal movement, human language, and the difference is significant. Starting in around the 19 sixties, there was a surge in interest, um, in, in both the neuro basis of vision and in developing methods to systematically characterize visual processing.

Speaker 1:          01:09          And this eventually led to computer scientists wondering about how these findings from neuroscience could be applied to artificial intelligence. And one of the biggest breakthroughs in our understanding of the neural basis of vision came from to scientists as a Harvard Hubel and wiesel, and they had a pretty simple experimental setup where they were, where they were able to measure visual, a neural activity in the visual Cortex of cats by recording directly from the electrical signals from neurons in this region of the brain. They displayed a simple stimulus on a screen and then probed the visual Cortex to see which neurons fired in response to that stimulus. There were a few key takeaways from this experiment that I'd like you to keep in mind as we go through today's lecture. The first was that they found that there was an mechanism for spatial invariance. They could record neural responses to particular patterns, and this was constant regardless of the location of those patterns on the screen.

Speaker 1:          02:18          The second was that the neurons they recorded from had what they called a receptive field. They certain neurons only responded to certain regions of the input while others responded to other regions. Finally, they were able to tease out that there was a mapping, a hierarchy to neuro neuro organization in the visual cortex. There are cells that are responded to more simple images such as rods and rectangles, and then downstream layers of neurons. They use the activations from these upstream neurons in their computation, so cognitive scientists and neuroscientists have since built off this early work to indeed confirm that the visual Cortex is organized into layers and this hierarchy of layers allows for the recognition of increasingly complex features that, for example, allow us to immediately recognize the face of a friend. So now that we've gotten a sense at a very high level of how our brains process visual information, we can turn our attention to what computers see.

Speaker 1:          03:22          How does it computer processing and image well to a computer. Images are just numbers. So suppose we have a picture of Abraham Lincoln. It's made up of pixels and since this is a gray scale image, each of these pixels can be represented by just a single number. So we can represent our image as a two d array of numbers. One for each pixel in the image, and if we were to have an rgb color image, not gray scale, we can represent that with a three d array where we have two d matrices for each of our G and b. Now that we have a way to represent images to computers, we can next think about what types of computer vision tasks we can perform and in machine learning more broadly, we can think of tasks of regression and those of classification in regression. Our output takes a continuous value and in classification, a single class label.

Speaker 1:          04:21          So let's consider the task of image classification. We want to predict a single label for a given input image. For example, let's say we have a bunch of images of us presidents and we want to build a classification pipeline to tell us which president is an image. I'll putting the probability that the image is of a particular president. So in order to be able to classify these images are pipeline needs to be able to tell what is unique about a picture of Lincoln versus a picture of Washington versus a picture of Obama. And another way to think about this problem at a, at a high level is in terms of the features that are characteristic of a particular class of images, classification would then be done by detecting the presence of these features in a given image. If the, if the feature is for a particular image, are all present in an image, we can then predict that class with a high probability.

Speaker 1:          05:19          So for our image classification pipeline, our model needs to know first what the features are and secondly it needs to be able to detect those features in an image. So one way we can solve this is to leverage our knowledge about a particular field and use this to manually define the features ourselves and classification pipeline would then try to detect these mentally defined features in images and use the results of some detection algorithm for classification. But there's a big problem with this approach. Remember that images are just 3d arrays of brightness, values and image data has a lot of variation. Occlusion variations in illumination, viewpoint variation, interclass variation, and our classification pipeline has to be invariant to all these variations while still being sensitive to the variability between different classes, even though our pipeline could use features that we the human define where this manual extraction would break down is in the detection task itself, and this is due to the incredible variability that I just mentioned.

Speaker 1:          06:32          The detection of these features would actually be really difficult in practice because your detection algorithm would need to withstand each and every one of these different variations, so how can we do better? We want a way to both extract features and detect their presence in images automatically in a hierarchical fashion, and we can use neuro network based approaches to do exactly this, to learn visual features directly from image data and to learn a hierarchy of these features to construct an internal representation of the image. For example, if we wanted to be able to classify images of faces, maybe we could just first learn and detect low level features like edges and dark spots, level features, eyes, ears and noses, and then high level features that actually resemble facial structure and neural networks will allow us to directly learn visual features in this manner if we construct them cleverly.

Speaker 1:          07:35          So in yesterday's first lecture, we learned about fully connected neural networks where you can have multiple hidden layers and where each neuron in a given layer is connected to every neuron in the subsequent layer. Let's say we wanted to use a fully connected neural network like the one you see here for image classification. In this case, our input image would be transformed into a vector of pixel values, fed into the network, and each neuron in the hidden layer would be connected to all neurons in the input layer. I hope you can appreciate that all spatial spatial information from our two d array would be completely lost additional weight. Additionally, we'd have many, many parameters because this is a fully connected network, so it's not going to be really feasible to implement such network in practice, so how can we use the spatial structure that's inherent in the input to inform the architecture of our network?

Speaker 1:          08:35          The key insight in how we can do this is to connect patches of the input represented as a two d array to neurons in hidden layers. This is to say that each neuron in a hidden layer only sees a particular region of what the input to that layer is. This will not only reduce the number of weights in our network, but also allow us to leverage the fact that in an image pixels that are near each other are probably somehow related to define connections across the whole input. We can apply this same principle by sliding the patch window across the entirety of the input image. In this case, by two units in this way will take into account the spatial structure that's present, but remember, our ultimate task is to learn visual features and we can do this by smartly waiting the connections between a patch of our input to the neuron it's connected to in the next hidden layer, so as to detect particular features present in that input and essentially what this amounts to is applying the filter, a set of weights to extract local features, and it would be useful for us in our classification pipeline to have many different features to work with and we can do this by using multiple filters, multiple sets of weights, and finally we want to be able to share the parameters of each filter across all the connections between the input layer and the next layer because the features that matter in one part of the input should matter elsewhere.

Speaker 1:          10:16          This is the same concept of a spatial invariants that I alluded to earlier in practice. This amounts to a patchy operation known as convolution. Let's first think about this at a high level. Suppose we have a four by four filter, which means we have 16 different weights and we're going to apply the same filter to four by four patches in the input and use the result of that operation to somehow influence the state of the neuron and the next layer that this patch is connected to. Then we're going to shift our filter by two pixels as for example, and grabbed the next patch of the input, so in this way we can start thinking about convolution at a very high level, but you're probably wondering how does this actually work? What do we mean by features and how does this convolution operation allow us to extract them?

Speaker 1:          11:15          Hopefully we can make this concrete by walking through a couple of examples. Suppose we want to classify x's from a set of black and white images of letters where black is equal to negative one and why is represented by a value of one. For classification, it would clearly not be possible to simply compare the two matrices to see if they're equal. We want to be able to classify an x as an ex, even if it's shifted, shrunk, rotated, deform, transformed in some way. We want our model to compare the images of an x piece by piece and look for these important pieces that define an x as an x. those are the features and if our model can find rough feature matches in roughly the same positions into different images, it can get a lot better at seeing the similarity between different examples of xs.

Speaker 1:          12:14          You can think of each feature as many images, small two dimensional array of values, and we're going to use filters to pick up on the features common to xs in the case of an ex filters that can pick up on diagonal lines and crossing will probably capture all the important characteristics of most x's. So no here that these smaller matrices are the filters of weights that we'll use in our convolution operation to detect the corresponding features in an input image. So now all that's left is to define this operation that will pick up on when these features pop up in our image, and that operation is convolution convolution preserves the spatial relationship between pixels by learning image features in small squares of the input. To do this, we perform an element wise multiplication between the filter matrix and a patch of the input image that's of the same dimensions as the filter. This results in a three by three matrix and the example you see here, all entries in this matrix are one, and that's because there's a perfect correspondence between our filter and the region of the input that we're involving it with. Finally, we saw them all the entries in this Matrix get nine and that's the result of our convolution operation.

Speaker 1:          13:43          Let's consider one more example. Suppose now we want to compute the convolution of a five by five image and a three by three filter. To do this, we need to cover the entirety of the input image by sliding the filter over the image, performing the same element wise, multiplication and addition. Let's see what this looks like. We'll first start off in the upper left corner, element wise, multiplied this three by three patch with the entries of the filter, and then add this results in the first entry in our output matrix called the feature map.

Speaker 1:          14:22          We will next slide the three by three filter over by one to grab the next patch and repeat the same operation. I'm in wise multiplication. Addition. This results in the second entry and we continue in this way until we have covered the entirety of our five by five image and that's it. The feature map reflects where in the input was activated by this filter that we applied. Now that we've gone through the mechanism of the convolution operation, let's see how different filters can be used to produce different feature maps. So on the left you'll see a picture of a woman's face and next to it the output of applying three different convolutional filters to that same image. And you can appreciate that by simply changing the weights of the filters. We can detect different features from the image.

Speaker 1:          15:21          So I hope you can now see how convolution allows us to capitalize on the spatial structure inherent to image data and use sets of weights to extract local features to end to very easily be able to detect different types of features by simply using different filters. These concepts of spatial structure and local feature extraction using convolution are at the core of the neural networks used for computer vision tasks, which are very appropriately named convolutional neural networks or CNNS. So first we'll take a look at a CNN architecture that's designed for image classification. Now there are three main operations to a CNN first convolutions, which as we saw can be used to generate feature maps, second nonlinearity, which we learned in the first lecture yesterday because image data is highly nonlinear, and finally pooling which is a downsampling operation. In training, we train our model, our CNN model, on a set of images, and in training we learned the weights of the convolutional filters that correspond to feature maps in convolutional layers.

Speaker 1:          16:41          And in the case of classification, we can feed the output of these convolutional layers into a fully connected layer to perform classification. Now we'll go through each of these operations to break down a the basic architecture of a CNN. First, let's consider the convolution operation. As we saw yesterday, each neuron and the hidden layer will compute a weighted sum of its inputs, apply bias, and eventually activate with a nonlinearity. What's special in CNN is this idea of local connectivity. Each neuron and hidden layer only sees a patch of its inputs. We can now define the actual computation for a neuron in the hidden layer. Its inputs are those neurons in the patch of the input layer that it's connected to. We apply a matrix of weights are convolutional filter four by four. In this example, do an element wise multiplication, add the outputs and apply bias, so this defines how neurons in convolutional are connected, but within a single convolutional layer, we can have multiple different filters that we are learning to be able to extract different features.

Speaker 1:          18:00          And what this means that is that the output of a convolutional layer has a volume where the height and the width are spatial dimensions, and these spatial dimensions are dependent on the dimensions of the input layer. The dimensions of our filter and how we're sliding our filter over the input, the stride, the depth of a this output volume is then given by the number of different filters we apply in that layer. We can also think of how neurons in convolutional layers are connected in terms of the receptive field, the locations in the original input image that a node is connected to. These parameters defined the spatial arrangement of the output of a convolutional layer. The next step is to apply a nonlinearity to this output volume as was introduced in yesterday's lecture. We do this because data is highly nonlinear and in Cnns it's common practice to apply nonlinearity after every convolution operation and the common activation function that's used is the value which is a pixel by pixel operation that will replace all negative values falling convolution with a zero.

Speaker 1:          19:20          And the last key operation to a CNN is pulling and pulling is used to reduce dimension dimension, yet, excuse me, dimensionality, and to preserve spatial invariance, a common text technique which you'll very often see is max pooling as shown in this example, and it's exactly what it sounds like. We simply take the maximum value in a patch of the input, in this case, a two by two patch and that determines the output of the pooling operation. And I encourage you to kind of meditate on other ways in which we could perform this sort of downsampling operation. So these are the key operations of a CNN and we're now ready to put them together to actually construct our network. We can layer these operations to learn a hierarchy of features present in the image data.

Speaker 1:          20:18          A CNN bills for image classification can roughly be broken down into two parts. The first is the feature learning pipeline where we learn features in input images through convolution, through the introduction of nonlinearity and, uh, the pooling operation. And the second part is how we're actually performing the classification, the convolutional and pooling layers output, high level features of the input data, and we can these into fully connected layers to perform the actual classification. And the output of the fully connected layers in practice is a probability distribution for an input images membership over set of possible classes. And a common way to do this is using a function called soft Max, where the output represents this categorical probability distribution.

Speaker 1:          21:15          Now that we've gone through all the components of a CNN, all that's left is to discuss how to train him. Again, we'll go back to a CNN for image classification. During training we learned the weights of the convolutional filters. What features the network is detecting, and in this case will also learn the weights of the fully connected layers. Since the output for classification is a probability distribution, we can use cross entropy loss for optimization with back prop. Okay, so I'd like to take a closer look at CNN for classification and discuss what is arguably the most famous example of of a CNN, the the ones trained and tested on the image net dataset. Image net is a massive data set with over 14 million images spanning 20,000 different categories. For example, there are 1,409 different pictures of bananas in this data set alone. Even better, bananas are succinctly described as an elongated, crushing shaped yellow fruit with soft, sweet flesh, which both gives a pretty good descriptive value of what a banana is and speaks to its deliciousness.

Speaker 1:          22:38          So the creators of the image net dataset also have created a set of visual recognition challenges on this Dataset. And what's most famous is the image net classification task where users are challengers are simply tasked with producing a list of the object categories present in a given image. Oh, across 1000 different categories. And the results of, of that neural networks have had on this classification task are pretty remarkable. Two thousand 12 was the first time a CNN one. This challenge with the famous, uh, Alex Net cnn, and since then neural networks have dominated the competition and the error has kept decreasing, surpassing human error in 2015 with the resonant architecture. But with improved accuracy, the number of layers in these networks has been increasing quite dramatically. So there's a trade off here, right? Build your network deeper. How deep can you go?

Speaker 1:          23:47          So, so far we've talked only about using CNNS for image classification, but in reality, this idea of using convolutional layers to extract features can extend to a number of different applications. When we considered a CNN for classification, we saw that we could think of it in two parts, feature learning and the classification. What is at the core of the of convolutional neural networks is is the feature learning pipeline. After that, we can really change what follows to suit the application that we desire. For example, the portion following the convolutional layers may look different for different image classification domains. We can also introduce new architectures beyond fully connected layers for tasks such as segmentation and image captioning. So today I'd like to go through three different applications of CNNS beyond image classification, semantic segmentation, where the task is to assign each pixel in an image and object class to produce a segmentation of that image object detection, where we are tasked with detecting instances of semantic objects in an image and image captioning where the task is to generate a short description of the image that captures it's semantic content.

Speaker 1:          25:15          So first, let's talk about semantic segmentation with fully convolutional networks or fcns. Here the network again, takes a image input of arbitrary size, but instead it has to produce a correspondingly sized output where each pixel has been assigned a class label, which we can then visualize as a segmentation as we saw before with CNNS for image classification, we first have a series of convolutional layers that are downsampling operations for feature extraction, and this results in a hierarchy of learned features. Now the difference is that we have a series of upsampling operations to increase the resolution of our output from the fully from the convolutional layers, and to then combine this output of the upsampling operations with those from our downsampling path to produce a segmentation. One application of this sort of architecture is to the real time segmentation of the driving scene here. The network has this encoder, decoder architecture for encoding the architecture is very similar to what we discussed earlier, earlier, convolutional layers to learn a hierarchy of feature maps, and then the decoder portion of the network actually uses the indices from pooling operations to up sample from these feature maps.

Speaker 1:          26:45          And output a segmentation, another way CNNS have been extended and applied is for object detection where we're trying to learn features that characterize particular regions of the input and then classify those regions. The original pipeline for doing this called our CNN is pretty straightforward given an input image. The algorithm extracts region proposals, bottom up, computes features for each of these proposals using convolutional layers and then classifies each region proposal, but there's a huge downside to this. So in their. In their original pipeline, this group extracted about 2000 different region proposals, which meant that they had to run 2000 CNNS for feature extraction. So since then there. And that takes a really, really long time. So since then there have been extensions of this basic idea, one being to first run the CNN on the input image to first extract features and then get region proposals from the feature maps.

Speaker 1:          27:54          Finally, let's consider image captioning. So suppose we're given an image of a cat riding the skateboard in classification, our task is to output a class label for this image cap, and as we've probably hammered home by now, this is done by feeding our input image through a set of convolutional layers, extracting features, and then passing these features onto fully connected layers. To predict a label. Image captioning, we want to generate a sentence that describes the semantic content of the same image. So let's take the same network from before and remove the fully connected layers at the end. Now we only have convolutional layers to extract features and we'll replace the fully connected layers with a recurrent neural network. The output of the convolutional layers gives us a fixed length encoding of the features present in our input image, which we can use to initialize and rnn that we can then train to predict the words that describe this image I'm using using the rnn.

Speaker 1:          29:06          So now that we've talked about convolutional neural networks, they're applications. We can introduce some tools that have recently been been designed to probe and visualize the inner workings of a CNN to get at this question of what is the network actually seeing. So first off, a few years ago, there was a paper that published an interactive visualization tool of a convolutional neural network trained on a Dataset of handwritten digits, a very famous dataset called feminist, and you can play around with this tool to the behavior of the network given a number that you yourself drawn in and what you're seeing here is the feature maps for this seven that someone has drawn in and as we can see in the first layer, the first six filters are showing primarily edge detection and deeper layers will start to pick up on corners, crosses, curves, more complex features, the exact hierarchy that that we introduced in the beginning of this lecture.

Speaker 1:          30:12          A second method which you'll you yourself will have the chance to play around with in the second lap is called class activation. Maps or cans, cans generate a heat map that indicates the regions have an image to which are CNN for classification attends to in its final layers, and the way that this is computed is is the following. We choose an output class that we want to visualize. We can then obtain the weights from the last fully connected layer because these represent the importance of each of the final feature maps. In outputting that class, we can compute our heat map as simply a weighted combination of each of the final convolutional feature maps. Using the weights from the last fully connected layer, we can apply cams to visualize both the activation maps for the most likely predictions of an object class for one image as you see on the left, and also to visualize the image regions used by the CNN to identify in different instances of one object class. As you see on the right.

Speaker 1:          31:27          So to conclude this talk, I'd like to take a brief consideration of how deep learning for computer vision has really made an impact over the past several years and the advances that have been made in deep learning for computer vision would really not be possible without the availability, availability of large and well annotated image data sets. And this has really facilitated the progress that's been made. Some datasets include image net, which we discussed amness a data data Dataset of handwritten digits, which was used in some of the first big CNN papers, places a database from here at mit of scenes and landscapes and cipher 10, which contains images from 10 different categories listed here. The impact has been broad and deep, um, and spanning a variety of, of different fields. Everything from medicine to self driving cars to security. One area in which convolutional neural networks really made a big impact early on was in facial recognition software.

Speaker 1:          32:43          And if you think about it nowadays, this software is pretty much ubiquitous from social media to security systems. Another area that's generate, generated a lot of excitement as of late is in autonomous vehicles. And self driving cars, so Nvidia has a research team working on a CNN based system for end to end control of self driving cars, their pipeline pizza, single image from a camera on the car to a CNN that directly outputs a single number, which is the steering is the predicted steering wheel angle and the man you see in this video is actually one of our guests lecturers and on Thursday we'll hear about how his team is, is developing this platform. Finally, we've also seen a significant impact in the medical field where deep learning models have been applied to the analysis of a whole host of different types of medical images.

Speaker 1:          33:42          Just this past year, a team from Stanford developed a CNN that could achieve dermatologists level classification of skin lesions. So you could imagine, and this is what they actually did, having an app on your phone where you take a picture, upload that picture to the APP. It's fed into a CNN that jen, that then generates a prediction of whether or not that lesion is reason for concern. So to summarize what we've covered in today's lecture, we first considered the origins of the computer vision problem, how we can represent images as arrays of pixel values and what convolutions are, how they work. We then discussed the basic architecture of CNNS and finally we extended this to consider some different applications, um, of, of convolutional neural networks, and also talked a bit about how we can visualize their behavior. So, with that, I'd like to conclude, um, I'm happy to take questions after the lecture portion is over. It's now my pleasure to introduce our next speaker, a special guest professor Aaron [inaudible] from the University of Montreal. Professor Kerrville is one of the creators of generative adversarial networks, and he'll be talking to us about deep generative models and their applications. So please join me in welcoming him.