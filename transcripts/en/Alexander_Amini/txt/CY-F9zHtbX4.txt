Speaker 1:          00:04          So Hi everyone. My name is Alexander [inaudible]. I'm a phd student and the computer science and ai lab at Mit. And today I'm going to be telling you about some of our work in building a computer, excuse me, building an autonomous vehicle control system, uh, for parallel autonomy using and to end learning. I should also mention that this is joint work as part of the MIT tri or Toyota research institute partnership. So first of all, when I say autonomous vehicle control system, what is it that I really mean? What does this require? And by asking these questions, we can kind of start to see why we might even want to care about an end to end solution at all. So for starters, we know that autonomous vehicle systems need to be able to handle many different types of scenarios. So this includes things like changes in luminosity, the presence of lane markers, and even harsh weather conditions.

Speaker 1:          01:01          This is something I, a lot of autonomous vehicle systems today are simply not able to handle. Model based systems are often very brittle and practice. They require huge number of hand programmed rules and engineered features to be extracted. And instead in this project we aim to utilize large datasets to actually learn and underlying representation of how the human actually drove so that we can build a computer to replicate that in practice. So let's start by actually taking a step back and looking at the standard comms vehicle pipeline. I know this might be very familiar to some of you, but I still wanted to go through it just so we can see the difference with an end to end pipeline. And the key idea here is that the problem is broken up into many different submodules where each sub module is tackled independently. And you can see on the bottom I've put some references to actual works and publications from both academia and industry focus on actually tackling an individual sub problem of this pipeline.

Speaker 1:          01:59          And it starts with on the left you can see that we're actually just collecting data about our environment, we're trying to learn what is happening around us. So this can include things like camera data, Lidar, radar, initial data, et cetera. And then we feed this into some sort of detection pipeline cause it's the first thing we want to do is actually understand where the obstacles around us are. We need to identify these obstacles as both static or dynamic. We might want to pay even more attention to dynamic obstacles and perhaps plan their emotion and the environment. And once we have an idea of what's happening and the objects around us, we want to actually localize ourselves relative to those objects. And this is called localization. Once we have an idea of where we are, where the objects are under Sar, we can finally start to talk about planning, which is really one of the most fundamental parts of, of autonomous driving.

Speaker 1:          02:48          So the goal of autonomous driving is to travel from point a to b safely, right? And that's exactly what planning allows us to accomplish. So planning takes into account where the obstacles are and then allows us to actually plan around to reach our goal destination. Once we have this plan, we can now send actuation of signals to the car motor and actually execute that plan. So in this project we actually replaced that entire intermediate pipeline with a learned model. And in this project we're actually focused on that learn model being a deep neural network. And instead of actually learning the control directly from raw sensory data, we're going to just focus on pixel value. So using a single front facing camera as our sensory input and instead of predicting all possible control, we're just going to focus on predicting the steering wheel angle at that instant.

Speaker 1:          03:44          And while this may seem like a very simple problem at first, because we are removing all of these intermediate pipeline, all of these intermediate problems, this is actually an extremely hard problem in practice to actually get working in real life. And there are a couple reasons why this is so difficult to work in real life. So firstly, in the real world we face huge amounts of uncertainty, huge amounts of more important than uncertainty. We faced huge amounts of ambiguity. So this is a picture of a roundabout in Paris. You can see that we have to deal with huge amounts of complexity in real world driving. We have to be able to handle uncertainty in the environment. We have to be able to handle uncertainty and the cars around us and even uncertainty in their intentions and their actions. So we don't know what the vehicles, the pedestrians, the cyclist are doing around us.

Speaker 1:          04:37          And we have to account for that as part of our planning algorithms. And since we're focusing on vision data, there are also many challenges that come with this as well. So camera sensors are subject, so sun glare. So he can see on the bottom here there's a image of the sun actually facing directly into the camera. And this kind of renders the entire driving seen underneath that, um, almost black. So it makes the driving or the imagery very difficult to actually interpret. And additionally we have to be able to account for uncertainty in the, the perception itself. So dealing with harsh weather conditions brings about, uh, the inability to accurately image the environment and things like rain, snow and et Cetera. And then finally, one of the, perhaps most key challenge of all autonomous driving, not just end to end, is dealing with the edge cases.

Speaker 1:          05:29          So what happens if we're driving and we see a plane in the road in front of us, like we can see on this top image here, uh, or on the other hand, what if we face assist a scenario like this bottom spot them scenario here where there is actually a sign painted on the back of a car of a pedestrian and a cyclist and our object detector at detection pipeline has actually recognized that sign as containing a real life, human or pedestrian. So these are real problems that they face all the time. His vehicles including end to end systems.

Speaker 2:          06:05          Yeah,

Speaker 1:          06:05          it's, I'd like to talk about some of the topics I'd like to go for as part of this talk. Firstly, I'd like to begin this talk by explaining this phrase that I introduced in my title slide, which is parallel autonomy. I have if I haven't actually mentioned what parallel autonomy is yet. So I'm going to talk about this in much more detail in the first part of the talk.

Speaker 2:          06:26          Yeah.

Speaker 1:          06:27          And really I'll walk you through firstly what the definition of parallel autonomy is and the autonomy system that we've built at mit to actually create a test bed for a parallel tonomy testing. Next I'll present some of our work and not only learning the steering wheel control of an autonomous vehicle, but also the corresponding steering bounds for that vehicle and an end to end manner. I'll show you how this work actually tackles some of the issues that I presented earlier that end to end systems today currently face.

Speaker 2:          07:02          Okay.

Speaker 1:          07:02          And finally we'll talk about how we can actually predict when the model is likely to fail, when we can determine how uncertain does model is. When can we trust our end to end system? So this is another huge challenge in intent, uh, algorithms. We have to be able to account for the situations where the model doesn't know what it's talking about and we have to be able to accurately predict these.

Speaker 2:          07:24          Yeah.

Speaker 1:          07:24          So let's start with the first part of the talk, which is all about parallel autonomy. All this really means is that we have an autonomy controller that it's running two parallel threads in the background. One is the human and one is the robot or the standard autonomy system. And the fusion of these two controls results in what we call parallel autonomy or this, this paradigm of like shared robot human interaction. And intuitively the best way that I can actually think of describing this as some sort of guardian angel in your car, preventing you from actually making an accident. So let me just play this commercial, which I think actually sums this up really, really nicely. And this has really captured well, uh, through this, through this clip.

Speaker 3:          08:21          Three, remember when only dead could say today, sorry, that's what auto emergency braking,

Speaker 2:          08:34          right? So, okay,

Speaker 1:          08:37          so essentially, no, I hope you have some sense of idea what I'm talking about when I say guardian angels. So parallel autonomy is this human robot shared control paradigm whereby the human is always in control of the vehicle, but the autonomy system is always running in the background preventing and is responsible preventing that vehicle from ever causing an accident. So you can imagine our software architecture software pipeline starting with the country, the two control inputs. One is the human, one is our standard series autonomy controller. And then these being fed into a shared control system, which is then fed into a low level tracking control, which we can feed into a drive by wire interface as part of our actual uh, autonomy hardware, which is just a full scale autonomous vehicle. So at Mit we've created a test bed for autonomous vehicle development. Specifically we have a fee, a fleet of autonomous systems consisting of two cars, two Toyota Prius's and two wheelchairs, which we've completely retrofitted with autonomous drive by wire capabilities.

Speaker 1:          09:44          The wheelchair serve as basically this development platform for debugging and developing our algorithms before we actually moved them onto the full scale car. So this is more of like a safety safety step on our end that we take of debugging all of our, all of our algorithms, all of our code on the wheelchair, which says the exact same sensor suite as the full scale cars. So we'd has all of the cameras, the Lidars, the radars, et cetera. So it serves as a great test bed before we moved to the car. But let's take a look at one of our cars just to jump straight to it. So in our car we've installed five lidar laser scanners for three d imaging of the environment.

Speaker 2:          10:22          Okay.

Speaker 1:          10:23          Three GMS sell cameras on the front of the car. And actually now this is actually a little bit outdated. So we've installed two more cameras on the sides of the car for side imaging as well. There's an ost x gps installed for coarse grain localization, an imu for collecting data like acceleration, rotation and orientation data. And finally we collect information about the speed of the car through odometry, through, sorry, through to will and coatings on the back two wheels.

Speaker 2:          10:53          Okay.

Speaker 1:          10:54          All of the sensors are that fed into our nvidia drive px to which serves as our gpu enabled computing platform, which takes all of this information in, combines it with our end to end controllers, and then sends these two are drive by wire interface and our custom Ecu Board to actually, uh, command the car and actually control the car. The great thing about this [inaudible] is actually that it has to uh, onboard gps, which an eye which allows us to do really fast, realtime, deep neural network in France as well.

Speaker 1:          11:29          Now before going any further, I want to actually draw this distinction between shared control and binary control. So typically when people think of a shared control in the autonomous vehicle setting, they're thinking of something like binary control. And especially in the last talk, we got this idea of this handoff problem where the human drivers and control, but we were trying to figure out when to hand off control to the autonomous or if the autonomous system is in control. We want to figure out when did tonto system doesn't know what's going on and I can hand off control back to the human. This is actually a much simpler problem than what we're doing with in our lab, which is shared control, which incentive? This binary switch we're dealing with actually a fusion of the two control systems. So not only do we have to decide when, which system to trust more, but we actually have to turn in the best way to fuse them together in a very natural manner such that the human who is always driving the car, it doesn't feel like they're losing control of what they're doing.

Speaker 2:          12:29          Okay.

Speaker 1:          12:29          This actually results in us not being able to rely on many of the classical techniques for controlling our car, uh, which a lot of other industries and uh, academic institutions have, um, have have, um, gone through. So I'd actually like to take a second just through this slide and actually talk about how we achieve control of our cars. So one option is that we can, we can do this by actually connecting a motor straight to the steering wheel or to the throttle and actually commanding the internal engine of this motor to, to turn the steering wheel itself. And this suffers a lot from, we actually tried this, it's very difficult to get this kind of system working. It's very fragile and practice very brittle, suffers from responsiveness, also durability over time as you can imagine. The second way, which is really the classical way in many industries that mini intercities I've taking is to actually send canned messages to the board, to the car.

Speaker 1:          13:26          Ken is like this, a language that cars can take to actually communicate with different parts of the car. So one way that you can imagine, it's us just sending the canned codes for steering wheel angle to the car and then controlling it like that. This actually is not possible in a shared control paradigm. So this is possible if we're talking about binary control, but in shared control, we actually want to read the effort of the human on the wheel. We don't care about that per se. We don't care about the angle that the wheel is turned. We really care about how much effort is being exerted on the wheel, right? So something more like we care about the torque rather than the actual angle if the wheel at that instant. So instead we have to turn to this third approach, which is actually what we did a which is actually spoofing the input systems of the car.

Speaker 2:          14:14          Okay.

Speaker 1:          14:14          This essentially means that we've created drive by wire capability by sending the car like fake or synthetic data. This makes it believe that the human is still controlling it, but it is not able to accurately observe the observation at the human is doing because we're spoofing the input signals and combining them with our autonomy controller.

Speaker 2:          14:33          Okay.

Speaker 1:          14:34          And this is what we've created in our lab. This approach really leads to a very unique a control system. Um, and to summarize, this really creates three distinct autonomous modes. And the first is just manual control. This just means that the human is in complete control of the car. This is what we use when we're trying to focus on things like data collection. And on the other extreme you can see the, when the computer controls the car. So this is when you have no human input. And this is what people usually talk about when they talk about autonomous vehicles. So this is just the computer controlling of the car. But also what's unique here that I'd like to really hammer home this point is that we've created this third mode, which is a combination of both manual and computer, which we call parallel autonomy, um, where the robot is simultaneously observing the human but also actuating with the human in the shirt controlled powerline paradigm. All three of these modes are part of the mit tri autonomous driving pipeline and this allows us to really test and debug many of these algorithms I'm going to be talking about in this talk. I'm just going to talk about one of the algorithms that we've created in my lab to actually learn the steering control of a car but not only learned to control, but actually learned the balance, the safe and valid bounds for controlling in that environment.

Speaker 1:          15:57          So there's actually been a lot of work and using machine learning to control a car using raw perception data. Most notably work here from Nvidia has shown that it is possible to build a neural network that will take in as input a single frame of an image of a camera, sorry, and directly control the steering wheel angle if that car. So since then there have been many, many extensions of this work attempting very similar solutions. Some using LSTM is to capture a temporal dependencies. Others utilize utilizing techniques from imitation learning. But all of these, uh, works really suffer from the same problem. So the key problem here that they all face is that they lack this notion of the ability to feed into higher level navigational controllers. So taking videos and to a network, for example, this network outputs a single real valued number at the output.

Speaker 1:          16:54          What this means is that it is impossible for that single real value number, which just represents a steering wheel angle to capture any form of uncertainty measurement. It also means it's impossible for that number to capture different possible ambiguous situations. So imagine if you're at an intersection and there are three different ways that you can go. If you only have one output of your neural network, it's impossible for that network to handle anything other than an ambiguous situations. Thus, this really makes all of these solutions ill suited to be used in conjunction with higher levels, navigational control, which must be able to handle uncertainty in the environment and also ambiguity in your decision making.

Speaker 1:          17:35          So instead, in this work, what we attempt to do is actually learn a full probability distribution over the control actions. And we do this by first district ising our entire action space of all steering wheel angles to handle ambiguity and then learn a neural network to output this discrete distribution. Then we transformed this discrete distribution into a continuous probability density function or pdf, which could, which we can then use to analytically extract control bounds, uh, that we can use in our parallel autonomy framework. So let's break this pipeline down a little bit more to go into it a little more detail. So first, let's focus on this part where we just take a single input image frame and we want to predict a one dimensional discreet probability density function. So Xii is the image from the data set of m images. We want to learn some sort of functional mapping f with parameters data to predict this probability distribution on the right and F for us, like I said, is a deep neural network and data or just the weights.

Speaker 1:          18:40          So sinceF is just a deep neural network, we can optimize this end to end using backpropagation, which entails effectively minimizing the cross entropy loss for all of our m examples. So note here that the true distribution, which I did notice Pii is just the distribution that the human took at that time. I, or sorry, the, the um, estimate distribution is what we tried to get as close as possible to the, to the true distribution. But keep in mind here that the tree distribution that the human actually took at that time, it's actually a very simple distribution. It's just a Delta function, right? So it has the value of one in the direction that this human drove and a value of zero everywhere else. So this is really remarkable because what I'm telling you is that given data of only union modal actions, we're attempting to learn in multimodal distribution, right?

Speaker 1:          19:38          So imagine a scene like an intersection like this so he can see that there's two possible actions a person can take. Either go left or go right and we want to train a, let's suppose a very simple three output neural network that can handle these left right or straight actions. So when we started out, we haven't seen any training data, so we may see something random that it's outputting. But now if the human takes an action and tries to turn left, the neural network learns from this, it starts to promote the left action a little bit more. You can actually see that on the top left here. This is an example of what the true distribution for what the human actually took would look like. So it has a value of one when it went on the bin that corresponds to left turns and Zeros on the other two bins. If on the next intersection in our training dead or we see a similar intersection like this, it doesn't necessarily have to be the exact same intersection, but the human turned left again. So now the network will now start to really confidently promote this left turn action even more and discouraged straight and right, like I mentioned before. Now finally, let's see something

Speaker 1:          20:48          interesting happening. So now finally, the human decisive turn, right? The network is starting to learn both actions by promoting left, by promoting the right action. And actually it's starting to discourage left action even though it, it learned that before. So it actually starts to forget some of that information that I saw before in bye bye. In the process of promoting the current action, if we keep taking right turns, we keep promoting the right action and we start to lose some of the power on the left. And if we keep seeing straight runs, eventually the model even learned that, right? Sorry. If we keep seeing, uh, right turns the model, we'll learn that a right turn is perhaps even more likely than a left turn in this particular scenario. So over time we eventually expect to see some sort of convergence where the human in our dre training Dataset has collected enough data supporting both left and right action. So we expect to converge to some probability distribution, which is like a 0.5 on left and 0.5 on right and zero

Speaker 2:          21:54          everyone else. Okay, sorry.

Speaker 1:          22:01          So this is really awesome because it shows us that we can actually learn multimodal distributions using only uni modal ground truth data. So this eliminates the need to actually manually label each possible control action, right? That the car could have taken at that frame. So we're using the raw data collected from the, from the human in that environment, and instead we can actually use, we don't even need to keep driving over the exact same intersection over and over. What we're learning here is some sort of representation of where the, um, where is the drivable space in this region?

Speaker 2:          22:36          Yeah.

Speaker 1:          22:37          Uh, this is especially powerful because it means that we are not constrained to predefined types of intersections like t way t intersections for way intersections. We are simply learning about drivable space in the environment. And this is a very powerful notion. So to test this, we collected seven hours of driving data and the Greater Boston area. This data consisted mostly of highway and city roads. And since our test environment where we actually test our cars is very different than this training environment. We decided to do a bit of fine tuning on our models on roads without lane markers to teach the network to be able to handle some of some of these unmarked roads as well. So we feed all this data into our Nvidia dgx one to actually train our models using data parallelism on three deep on three gps. And this takes around one hour in practice.

Speaker 2:          23:34          Okay,

Speaker 1:          23:35          so I'm going to go over the next step of the pipeline before sharing some exciting results of the, of the system actually working in practice. So now since we've learned a meaningful discrete representation of where the drivable spaces, so what our control commands that the car can actually execute at that instant, we want to now transform this distribution into a continuous pdf defined by a mixture of Gaussians. And specifically you can think of each of these coushins as representing I possible action that the human could take or that the car could take at that instant, right? So if there's a multimodal distribution like you can see here, so this has two modes. This is corresponding to a situation where the car can either turn left or right and we'd expect to coushins to roughly appear from this distribution. So the mean simply represents the steering wheel angle that is most likely or most confidence. So this is the angle that the car should actually be traveling in for that mode. The variant simply reflects some sort of uncertainty about that mean. So the larger the variance, the more uncertain you are about following that exact a steering wheel angle. And finally, the power is simply just a scale and multiple that we multiply it by each mode of our distribution. To simply scale up, scale it up and down.

Speaker 1:          24:55          And we do this transformation from discreet to continuous by using this technique, which is known as very rich variational basion. Next you're modeling. And if you've ever heard of expectation maximization, this is a technique that's very similar. So an e m or expectation maximization, you have to actually predefine the number of modes that you expect your distribution to have. And this is really the difference with variational Bayes, is that you don't have to predefine the number of modes. You simply have to define an upper bound to the maximum number of modes that you would ever expect to encounter while you're driving. So for us, we use something like 15 modes. We never expect to see more than 15 possible different actions to take on the road. So this is really an upper bound. And then what happens is we feed in our discrete distribution, then iteratively we start to kill off modes that lose that, that lose power, right? So I won't go into too much detail on how this actually happens, but this is really the, the amazing thing about variational bayes compared to something like expectation maximization. Um, and I encourage you all to like look up online. This is a very well known technique if you're interested in learning some of the math behind the algorithm.

Speaker 2:          26:08          Okay.

Speaker 1:          26:09          Eventually we converge to the minimum number of mixtures that this, that this discrete distribution can actually be modeled by. So now we can combine the learning of our discrete distributions with variational Bayes to directly take us and put a single image from our camera and then predict a continuous pdf of where the drive will a control or the where the control that we can execute to the car.

Speaker 2:          26:41          Yeah.

Speaker 1:          26:41          So here, here's an of um,

Speaker 2:          26:44          okay.

Speaker 1:          26:45          A scene where the cars turning left and you can see the extracted discrete distribution is actually heavily focused on the left part of the control. You can also see the continuous pdf overlaid on top of this graph as well as bounds. I haven't talked to you about how these bounds are computed yet, but I'll get back to that very soon.

Speaker 2:          27:04          Yeah.

Speaker 1:          27:05          A very similar situation here is if, if I again show you a union modal action. So this is still a situation with unambiguous, um, control setting, but here the road has been slightly widened. So the variants has actually been increased.

Speaker 2:          27:23          Okay,

Speaker 1:          27:23          right? But this is actually not showing off the full capabilities of our approach because the whole reason that I motivated this approach was to deal with ambiguity, deal with uncertainty in the environment. So let's actually see something much more interesting. Let's go to an intersection. So now at an intersection we can finally see multimodal distributions and multimodal peaks emerging. So at this, at this intersection, this is a t intersection. It's possible to either turn left or right and you can see three modes emerging from this distribution.

Speaker 1:          27:55          The three modes are as you'd expect left and right. But also we see the straight mode also emerging. And if you look at what the human actually did into scenario, it was actually to go straight as well. So at this instant, the control command was to go straight. But then if you go forward two or three friends, you'll see that that straight actually becomes a right. So they were trying to just basically go around something and you can see that this is actually incredible because the model has captured, uh, picked up on this and actually captured that to actually turn right or it goes left. You actually have to go straight first. So now let's take an example of a, let's take a look at two example clips of the system in real life. So first on the left is an example of navigating intersections reliably without spuriously replanning so you can see many, four way intersections and were able to successfully navigate through those without any jittering or anything like that. And on the right you can see the vehicle successfully handling roads with huge cracks and grass and vegetation growing on them. So keep in mind that this network has never seen these roads before. It has only been trained on highway and residential streets and we only find tuned at using one minute of data without lane markers.

Speaker 1:          29:15          So this is really exciting because it shows that we can control the car, even with minimal training data in the actual environment that we're trying to test on. We just have to do a little bit of fine tuning. And by actually predicting the uncertainty with this, we're able to get some robustness in our control signal as well.

Speaker 2:          29:34          Okay.

Speaker 1:          29:34          But in the first part of this talk, I talked to you all about parallel autonomy. So what's the connection here with this approach to care? Parallel autonomy. So in addition to predicting the control, we also want to predict control bounds that we can really use and a parallel autonomous setting. So given the setup and the framework that we've talked about so far, this is actually extremely simple to do. So this is the beauty of setting it up, setting the problem up. The way we have moving into the parallel autonomy framework is now extremely simple. So what we can do is for each mixture of our continuous Gow Shin, we simply have to take plus or minus some scaled variance from the ideal control command or from the mean of that mode. We add those bounds into some set of valid control signals. And once we have these set of valid controls, all we have to do is simply set up a shared controller or shared parallel autonomy controller, which is just the human control. If it falls within that bound. And if the human is trying to do something outside of the bounds, the system provides some restorative feedback to bring them back within the bounce.

Speaker 1:          30:47          So now in the next section of this talk, I'd like to discuss this notion of model uncertainty. When can we trust our models? Uh, and this is really a huge question and actually all of deep learning, not just autonomous vehicles, um, but we want to essentially accurately determine or accurately verify that our model is, is confident in the action that it's telling us to take at that given instance of time. So I hope this notion of uncertainty is pretty clear, like the motivation of why we care about uncertainty. It's pretty clear to all of you, especially and a talk about autonomous vehicles. So

Speaker 1:          31:28          anonymous vehicles are one of the most safety critical robotic applications where it is extremely important for us to prioritize safety in the systems that we're building. So what we've learned so far is that modeling probabilities for fixed classes like we do for a discreet control signals is actually different than looking at the uncertainty of the model as a whole. So take for example, if I want to train this model of cats and dogs just to recognize a picture of a cat and a dog, and I at the output, I'm predicting a softmax layer that represents probably being a cat probability of being a dog. Now, if I showed this model, I picture of a cat, assuming that the model has been sufficiently trained, I'd expect that this model, it gives me an answer of yes, this is definitely a cat. And I expect that model to be very confident in its answer, right? So in this scenario, they're the same thing, but now let's think about it's different scenario. What if I feed that same model? A picture of a horse, right? So these two output probabilities are still fixed. So the model, we'll still output the probability of being a cat or the probability of dog. And by definition those output probabilities have to add up to, so by definition one of them has to be greater than 0.5.

Speaker 2:          32:43          Yeah.

Speaker 1:          32:43          Right. So even though one of the probabilities are very high, it may seem, it actually does not mean that the model is confident and predicting that this horse is a dog. And in fact, what we want to see is that the uncertainty of this model in this scenario is very low. And that's something that that motivates this, this whole notion of, of uh, uncertainty or confidence. And the key points that I want to hammer home here is that neural network probability is created through softmax layer is do not correspond to confidence. They don't measure uncertainty even though many people actually sent, makes the two terms entertained. Um, interchangeable.

Speaker 2:          33:22          Yeah.

Speaker 1:          33:23          So the spring sent the question how we can actually determine model uncertainty and for this return to this really hockfield and deep learning right now, which is called Beige and deep learning. And just listen to just as an example of the power of beige and deep learning. Let's take, um, let's take a look at this figure here. So this is a system that was built to take us input, a single image and output the predicted depth of every single pixel in that image.

Speaker 2:          33:51          Okay.

Speaker 1:          33:51          So when we use beige and deep learning, well we actually want to do is compute this very, very rich uncertainty estimates for this neural network. And we want to say for every single pixel, what is the confidence in that prediction and beige and deep learning really allows us to do this in a systematic manner. And it's really remarkable because we can see some really interesting things that start to arise. So you can see that the model is most uncertain around like the edges of the car is also the shadows underneath the cars and even the reflections and the windows. Because then the reflections, you actually see trees that are far away. So the model is able to capture some very natural sense of uncertainty.

Speaker 2:          34:32          Yeah.

Speaker 1:          34:33          So now the question becomes how can we apply these techniques to the end? To end driving scenario for self driving cars. So let's revisit the original end to end pipeline. So this is actually a different pipeline that I talked about in the first talk. This is an even simpler ones. So this is basically the pipeline that Nvidia has created for their autonomous vehicles. So let's just taking it in a single image and predicting just a single control value. There is no notion of probability distributions like we talked about before. I'm just predicting a single real number. We've already discussed the issues with this approach. We lack uncertainty, we lack ambiguous scenario handling.

Speaker 1:          35:09          So instead, what if we can maintain this architecture, but we still want to create some sort of uncertainty. So we can recently, we can actually reason about the confidence of our model with that control. And the way we can do that is actually, uh, by stochastically sampling over our output to deterministically, um, sorry to deterministic, sorry. We want to sample over are deterministic control signal to actually obtain an expectation and variants of that instance. So this will give us the expectation is essentially the control that we want to take. And the variance is essentially the uncertainty of the model at that instant.

Speaker 2:          35:50          Okay.

Speaker 1:          35:51          So more specifically, we can do this using patient deep learning, like I said, and

Speaker 2:          35:56          okay,

Speaker 1:          35:56          to understand this, let's actually reformulate our problem a little bit using a Bayesian state of mind. So this is a little different than the way we train our models, uh, in the previous setting. So now in the Batian deep learning framework, we want to actually,

Speaker 2:          36:12          okay,

Speaker 1:          36:12          like I said, have that patient's state of mind. So the goal here, like before we're trying to create a network to learn steering control directly from raw data. We're going to find that functional napping and we want to do it by minimizing some empirical loss. The different series that invasion neural networks were actually attempting to learn this posterior Oh far awaits given our data. So another, another way to say this is that we want to understand the likelihood of observing some model. Given the data that we see now based on neural networks are actually called Beige and neural networks. Because we can rewrite this posterior, this one, uh, using Bayes rule, right? And actually we can theoretically solve it using this equation. The problem is that in practice this is rarely possible to compute and politically, uh, so instead we have to resort to some sort of sampling techniques to approximate it instead.

Speaker 2:          37:09          Okay,

Speaker 1:          37:10          so let's look at what this might look like for our network. With convolutional layers, we start by performing t stochastic samples through our network forward passes through our network and at each time we're essentially sampling each of the weights of Feta according to a Bernoulli distribution. An element wise multiplied that Bernoulli mask with the unregular eyes feature maps.

Speaker 2:          37:33          Yeah,

Speaker 1:          37:34          so on the bottom left you can see in the illustration of three on regularized convolutional kernels, right? You can also see next to them the element that they are element wise multiplied by masks of ones and Zeros, which are just drawn from a Bernoulli random variable Bernoulli distribution. And this process is effectively repeated capital t times. You repeat this tee times, you get an estimate of your expectation of your output and you get an x, you get a variance estimation as well. Now the variance estimation is exactly what we're looking for as part of our uncertainty estimates. But there's actually a problem here in terms of convolutional layers. So the original theory for Beige and deep learning didn't take into account this huge spatial correlation between adjacent pixels in early convolutional layers. So especially in like I said in earlier convolutional layers, you're dealing with things like edges and corners and things like that.

Speaker 1:          38:32          And there are massive amounts of spatial correlation, the pixels. So when we apply our Bernoulli mask, we may actually lose a lot of valuable information and this may cause our output to change undesirably. So how do we, how do we handle this? So let's make a small change and instead of sampling Bernoulli random variables over each, wait, let's sample Bernoulli random variables over each kernel itself. So over the entire kernel, we'll call this colonel either a one or a zero and one element wise, multiply our unregular as feature maps by this new, um, Bernoulli drop out mask. So effectively by dropping gout, entire kernels in early layers, you, you'd probably know that kernels represent things like line detection, oriented line detections. And things like that. So effectively by dropping out some of these kernels and keeping others were getting like a noisy estimate of, of for example, lane detection algorithm if we're only dropping out like oriented line detectors.

Speaker 1:          39:33          And this is exactly what we implemented in our algorithm. So we found that stochastically dropping out kernels as opposed to elements actually resulted in both accelerated training as well as better convergence of our end to end algorithms. We performed a precision analysis to actually analyze how precise our models come, uh, work compared to the ground truth, human actions. And finally we studied how the uncertainty of our model changed as a function of the steering wheel angle or the steering control, uh, as well as the quantity of training data for that specific type of control. So finally we found that the uncertainty increased as we, um, tried to make more dynamic turns, which you might expect because when you're trying to make a more dynamic turn, you actually have less of that scene in your field of view. So if you're making a very dynamic right turn and your cameras pointing straight in front of you, you actually can't see the road.

Speaker 1:          40:34          You can only see maybe 10 meters in front of you. Whereas if you're going straight on a straight highway, you can see maybe even a hundred meters in front of you. So this really leads to this nice property, which we found was we have a more uncertain model as we tried to make more extreme turns is also aligns with the fact that the vast amount of training data comes on straight roads. So you'd expect that straight road driving is the most confident that the model can, can get can be. So now I'd like to take a step back, summarize some of the findings of this talk and tie things back with parallel autonomy. Once again. So we started this talk by exploring one of the control algorithms that we've created to actually predict the control of an autonomous vehicle and not only predict the control, but also predict the control bounce and entirely end to end manner.

Speaker 1:          41:30          We showed how we can officially handle ambiguity, this approach, and we explored ways that we can use techniques from beige and deep learning to actually get a sense of when our model is and is not confident about its prediction. Finally, I'd like to show how all this ties in with a cool demonstration of parallel autonomy on a full scale autonomous vehicle, specifically demonstrating guardian angel capabilities. So here the human is controlling the car and as long as he adheres to the balance of the learned model, it appears as though nothing is happening. But if he tries to drive off the road, you'll see in a second the system will see that he's exited the balance and tried to provide restorative feedback. Like what just happened there. You'll see another example where the human tries to even more aggressively tried to drive off the left of the road where the model will respond even more aggressively to bring the car back onto the road.

Speaker 1:          42:25          So we hope that this kind of parallel autonomy way of thinking, uh, brings about like this, this new realm of autonomous driving specifically of like this Guardian Angel Capability. So the goal, the ultimate goal of our work is to create a vehicle that even if you want to crash, it is uncrackable, right? So it will prohibit you from making any dangerous actions that exit the balance of the set of valid controls, right? So this is, this is really the holy grail of the work that we're doing. And I'd like to conclude this talk by actually acknowledging all of my, uh, incredible lab at Mit, all of my collaborators. If you're interested in reading up on any of the references that I've presented as part of this talk, there's a link right here where you can find all of the references. Um, and that's it. Thank you.