Speaker 1:          00:03          Thanks d dot. So I'm, I'm punching and uh, um, like the, um, also adds a Google Cambridge office and uh, I'm one of the authors of tensorflow. So we're going to change topic and talk about tensorflow. So, um, I read your course material a little bit and I gathered as you all use tensorflow in previous parts of the lecture. Is that right? Great. Okay. So my focus will be how to more efficiently, right, and debulk it, machine learning models in tensorflow. So the question is whether you need to debug a machine learning model. I think the answer is yes, of course machine learning models are very different from traditional programs, but their software and their code and the software and they will have bugs and the, you need to debug your models from time to time. So hopefully after this lecture you will know a little bit more about how to more efficiently devalue our machine learning models in tensorflow.

Speaker 1:          00:54          So before we dive into debugging, I want to talk about how machine learning models are represented in, in the computer because that turns out to be important for how you write and develop programs. So there are two ways in which a machine learning model can be represented. So it's either a data structure or program. So if it's a data structure, then when you write code to, for example, define a layer of neural network, you're actually building our graph and those lines of code, when they're executed, they don't actually do the competition, they're just building the graph and the graph needs to be fed into some kind of machinery because I'm kind of execution engine to actually run the model. And the second way in which you can define a machine and the motto is to write it as a program. So that's more straightforward. So those kinds of lines of code will actually do the competition on either the CPU or Gpu depending on whether you have a gpu or not. Um, so the first paradigm is also called symbolic execution or different execution. And the second one is also called I'm eager, execution or imperative execution. So I'm now the question for you is whether tensorflow is the first paradigm or the Second Paradigm?

Speaker 1:          02:03          So I heard someone said first,

Speaker 2:          02:06          second,

Speaker 1:          02:08          both. Yeah. So I think it's a trick question, right? So the answer is both. So if you ask the question like half a year ago, then the answer will be only the first. But in the latest version of tensorflow, we support both modes. Um, and I'm going to give you some examples in the following slides. So by default tends to flow is the first node, so that's the classical, um, traditional tensorflow style. So, um, just to give you a refresher of how to use tensorflow to define a simple model, you import the tensorflow stf and then you defined some constants or maybe some variables as inputs. And then you write a line to say like, you want to multiply x and w and then you want to add the results of the multiple patient to, um, to another thing be right. So you can think of this as a very simple linear regression model, if you will.

Speaker 1:          02:55          Now, the important thing here is when this line is executed, it's actually not doing the computation. So the multiplication will not happen at this point. If we print the results of this line, why? Then you will see it's not 40, it's not 10 times four equals 40. Instead it's like, um, it's like a abstract symbolic thing. So it's called a tenser and it knows what kind of operation it needs to do when it's actually executed in the future. So mall is that operation. It also needs an information about it also knows information about like what dependencies are, which are x and w in this case, but it's not shown in the printed message here. And likewise, when you do Tfa ad, when that line of code is executed, the addition will not happen. It's going to happen later. So by later, I mean a case, uh, the points at which you create a session by calling tf that session.

Speaker 1:          03:45          And when you have that session is created, it will basically automatically pull in the graph. You have not already built in the previous lines of code and then you tell the session like which tensor, which abstract symbol in the graph you want to execute, and then it's going to basically analyze the structure of the graph, sort out all the dependencies and topple logically execute other nodes in the graph to do the multiplication first and then do the audition next and it's going to give you the final results, which is 42 so it can think of tfs session as a Ng. So it's going to run the model on CPU. If you only have a CPU, it's going to run the model on Gpu if you have

Speaker 2:          04:21          GPU.

Speaker 1:          04:25          Now obviously I'm this paradigm of defining our model is not the most straightforward because those lines of codes that looked like doing competition is not doing any actual competition. And you need to learn a new api called TF session. So why does tensorflow do it this way?

Speaker 1:          04:42          So obviously it's because there are some advantages you can get. So the first advantage is because the model is a data structure, it's relatively easy to serialize this. Um, and then these airlines, there's somewhere else you can train a model and then you can load your model onto some kind of other devices like mobile devices or embedded devices like raspberry pie or a car or robots and you can also, um, sterilize the motto and then load the model on like a faster hardware like Google's tpu. Um, so these things are hard, hard to do if your model is a python right, if your model is a python program because those devices may not have python and running them. And even if those devices have python running on, on them, that's probably not what you want to use because python is slow sometimes. So I have those links here in the slide. So I'm going to send those slides to the course organizers and you can click on those links if you're interested in any of those topics like deployments on mobile devices and so on.

Speaker 1:          05:41          So the next advantage is because your model is a data structure, you are not tied down to the language in which the model is defined. So nowadays, most machine learning models are written in python, but maybe your application server or maybe a web server is running Java or c plus plus and you don't want to rewrite the whole stack in python just to be able to add some machine learning to your stack, right? So, um, if a model is a data structure that you can save the model after training and they can load it into Java or c plus plus or c sharp or any of the supported languages and that you will be ready to serve the trained model from your web server or application server.

Speaker 1:          06:17          And the other nice thing about representing data as the model, as a data structure, as you can distribute the model very easily onto a number of machines called workers. And those workers will basically use the same graph. They're going to do the exact same competition, but they are going to do it on different slices of the training later. So this kind of training in a distributed way, it's very important for cases in which you need to treat them out. I'm a very large amounts of data quickly. I'm the kind of problem that's Google, sometimes it has to deal with. So um, of course you have to start to modify your model graph so that the shared things like the weights variables in the model are shared on a server called primary receiver here, but that's basically distributed training intensive flow in a nutshell. So again, if you're interested, you can look at the slide and can click that link to learn more about distributor training. Any questions so far?

Speaker 1:          07:15          Okay. Okay. So, um, also because you're representing your model as a data structure, you're not limited by the speed on concurrency of the language in which the model is defined. We know that on python is slow sometimes and even if you try to make python or parallel parallel as the by writing multithreading, you will run into the show called python global interpreter lock and that will slow you down, especially for the kind of, um, competition. Got a deep learning model needs to do. Um, so the way in which we solve this problem in symbolic execution is by sending the model as a data structure from the layer of python c plus plus. So there, um, at the level of c plus plus you can use true concurrency, you can fully parallel buys things and that can benefit the speed of the model.

Speaker 1:          08:03          So obviously, um, there are all those advantages, but there are also, there was like, um, shortcomings of symbolic execution, for example. It's less intuitive, it's harder to learn. Um, so you, you need to spend some time getting used to the idea of beauty. You're defining a model first and then run the model later with tf dot session. Um, and it's harder to debug a way a model goes around. That's because everything, every actual competition happens inside the TF dot session. And that's a single line of Python code calling a c plus plus. So you can use the usual kinds of python debugger to do bumped heads. But I'm going to show you that they were actually very good tools in tensive flow that you can use to debug things that had been in. Do you have that session? And uh, another shortcoming of a symbolic exclusion is that it's harder to write control flow structures.

Speaker 1:          08:47          So by that I mean I'm structure as like I'm looping over a number of things or if else branches, so that kind of thing that we encounter in programming languages, but some machine learning models also need to do that, right? So likely recurrent neural networks need to loop over things and some kind of fancy I'm a dynamic models need to do if ellis branches and so on. I'm also going to show some slides to show those concrete examples. So it's very hard to sometimes very hard to write that kind of control flow structures in symbolic execution, but it's much easier in eager execution. So I'm with Eager Execution Europe, your program can be more authentic and it's easier to learn and easier to read. So here's an example. So on the left you're seeing the same code as before. So we are using the default symbolic execution of tensorflow.

Speaker 1:          09:34          No. Now how do we switch to the new eager execution? So it just add two lines of code. You import the eager module and then you call a method called enabled eager execution, and you don't have to make any other changes to your program in this case, um, but you will because of these few lines, you changed the semantics of these two lines, multiply and add. So now instead of building a graph, this line is actually doing the multiplication of ten four. And if you print while, you will see the value and if you print the value of z, you will also see the value. So everything is, I'm like a flutter and easier to understand.

Speaker 1:          10:08          Now, as I mentioned before, eager mode also makes it easier to control flow, dependency and the dynamic models. So here's an example. So suppose you want to write a recurrent neural network, which I think you have seen in previous parts of the lecture before, in, um, in the default mode of tensorflow. Here's about the amount of code you need to write. So, um, you cannot use the default native foreloop or while loop in python. You have to use the tensorflow's special wild loop. And in order to use it, you have to define two functions, one for the termination, conditioner of the loop and one for the body of the loop. And then you need to feed those two functions into the while loop and get answers back. And remember, those sensors are not actual values. You have to send those 10 stairs into session that runs, get together actual value. So there are a few hoops to jump through if you want to write a rn and from scratch in the default mode of tensorflow, but with eager execution that things becomes much simpler. So you can use the native for loop in Python to loop over time steps and the input and you don't have to worry about those symbolic tensors or sessions are run the, the, the, um, the variables you get from this followup is the result of the competition.

Speaker 1:          11:21          So I'm eager mode makes it much easier to write the so called dynamic models. So what do we mean by static models in the dynamic models? So by static models we mean models who was structured don't change with the input data. And I think you have seen examples like that in the image model sections of this lecture. So the diagram here shows the inception model in tensorflow, so the model can be very complicated, can have hundreds of layers and the can do, can do like a complicated competition like convolution pooling and a dense manipulation and so on. But the structure of the model is always the same no matter how the image changes, the image always has the same size and the same cutter depth. Um, but the model will always compute the same. I mean it will always do the same competition regardless how the image changes.

Speaker 1:          12:08          So that's what we mean by static model. But there are also models was structured change with inputs and data. So the recurrence, do you want network? We have seen is actually an example of that. And the reason why it's changes is because it needs to loop over things. So in the simplest kind of recurrent neural network, it will loop over items in the sequence, like a words in a sentence. So you can say that the length of the model is proportional to the length of the input sentence, but there are also more complicated changes of the model structure with input data. So some of the state of the art models that deal with natural language, we actually take a parse tree of a sentence as the inputs and the structure of the model will reflect that tree. So, um, as we have seen before, it's complicated to your rights are wild loops or control flow structures in, um, in those defaults symbolic mode. Now if you want one to ride that kind of model, it gets even more complicated because there you will need to nest on conditional branches and a wild loops, but it's much easier to revise in that moment because you can just use the native for loops and while loops and if statements in python. So we actually have an example to show you how to ride that kind of models. Dads take parse trees as input and the process natural language. So please check that out if you want. You have a question,

Speaker 2:          13:25          right?

Speaker 1:          13:27          The tree is static.

Speaker 2:          13:32          Okay.

Speaker 1:          13:33          Yeah. The tree is static in this particular input, but you can have a longer sentence, right? There's the, the, the grammar of the sentence. The sentence can change from one sentence to another, right? So that will make the model structure change as well. So basically you can hard code the structure of the model. You have to like look how to treat and then like, um, uh, do some kind of like an if else statements and while loops in order to like turn the theory into the model.

Speaker 2:          14:00          Yep.

Speaker 1:          14:03          Okay. So, um, we have seen that the eco mode is very good for um, learning and debugging and for writing control flow structures. But sometimes you may, you may still want to debug a tensorflow programs running in the default symbolic modes. And there are a few reasons for that. First, you may be using some kind of old code of tensorflow that hasn't been ported to eager mode yet. And some high level API as you might be using, like tf learned or terrorists or tf slim have not been important to. Yes. And you may want to stick to the default symbolic moments. It because you care about speed on, because eco mode is sometimes slower than the default mode. So, um, the good news is that we have a tool intensive flow that can help you like debug attentive flow model running in the tf, the TF dot session in the mode, and that's what was called tf dbg, or tensorflow debugger.

Speaker 1:          14:52          So the way in mentioned you use it is kind of similar to the way in which he was eager, execution, important and additional module. And after you have created the session object, you, we'll wrap the session object where the rapper in this case is called local command line interface direct GRANDPA. So, um, you don't need to make any other changes to your code because this rapid object has the same interface as the unwrapped object. But basically you can, um, you can think of this as like an oscilloscope on some kind of instrument on your tf dot session, which is otherwise opaque. So now, once you have wrapped that session, when sessions are run is called, you're going to drop into the command line interface. You're going to see basically a presentation of, um, one intermediate tensors are executed in the sessions are run and the structure and the graph and so on. So I encourage you to encourage you to play with that after the lecture. So

Speaker 1:          15:47          the TTF debugger is also very useful for debugging or kind of bugs in your machine learning models which will probably occur. Those are called numerical instability issues. So by that I mean sometimes value is in the network who will become Nan's or infinities. So Nan stands, stands for not a number. Naza infinities are like bad float values that will sometimes happen. Now, if you don't have a specialized tool in tensorflow, it can be difficult to pinpoint the exact notes which generates the nuns and infinities, but the debugger has a special commands with which you can run the model until any nodes in the graph contains Nan's or infinities. So in our experience, that happens quite often and the most common causes of Nancy and infinities are underflow and overflow. So for example, if there's an underflow, then a number will become zero. And when you use that number in division or log, you will get infinities and overflow can be caused by learning rates being too high or by some kind of bad training example that you haven't sanitized or a pre processed. But um, the debugger should help you find the root cause of that kind of issue is more, more quickly.

Speaker 1:          16:53          So the TF debugger is a command line tool. It's nice, it's a low footprint. You can use it if you have access to a computer only via a terminal, but obviously it's going to be even nicer if we can debulk the tensorflow models in a graphical user interface. So I'm excited to tell you about a feature of tensorflow that's upcoming. So it's called tensor board director plugin or visual graphical debugger for tensorflow. So it's not included in the latest release of

Speaker 1:          17:21          has low, which is one point five, but it's coming in the next release one point six. It's available for preview in Natalie's. So, um, you can copy and paste the code from here, install those nightly builds of tensorflow and the cancer board. And you can use the feature. So after you have installed these packages, you can run a command. So I'm all the code in my slides are copy paste to execute that Calabasas. Yeah. So these are about the main features of upcoming tool. So, um, if you're interested, please copy and paste these code and to try it out. This slide here is just a reminder of the main features in here. Um, okay. So as, as um, summary, we see that there are two ways to represent the machinery models in tensorflow, um, or in any deep learning framework either as a data structure or as a program.

Speaker 1:          18:07          If it's a data structure then you will get symbolic execution and symbolic execution is good for deployments, distribution and optimization. And if it's a program that you will get eager execution, it's good for prototyping, debugging, and a writing control flow structures and it's also easier to learn. And the country in terms of law supports both modes so you can pick and choose the best mode for you depending on your need. And, uh, um, we also went over tens of load debugger, both the command line interface and the browser version, which will help you debug your model more efficiently. So with that, I'm going to thank my colleagues on the Google brain team both in the amount of new headquarters and here in Cambridge. Um, she and Mahima are the two collaborators with me on the cancer board, debugger plugging project. And a tensorflow is an open source project. There have been over 1000 contributors like you who have fixed bugs and contributing new features. So if you see any interesting things that you can do, feel free to contribute to tensorflow on get hub. If you have questions, please email me. And if you see any bugs or feature requests about tensorflow or 10 boards, you can fall on the bugs on these links. Thank you very much for your attention. The question.