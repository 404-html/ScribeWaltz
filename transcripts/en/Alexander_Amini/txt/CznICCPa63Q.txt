Speaker 1:          00:03          Hi everybody. My name is serine and I'm going to be talking about how to use neural networks to model sequences. In the previous lecture you saw how you could use a neural network to model a Dataset of many examples. The difference with sequences is that each example consists of multiple data points. There can be a variable number of these data points per example, and the data points can depend on each other in complicated ways. So a sequence could be something like a sentence, like this morning I took the dog for a walk. This is one example, but it consists of multiple words and the words depend on each other. Another example would be something like a medical record one medical record would be one example, but it consists of many measurements. Another example would be something like a speech wave form where this one wave form as an example, but again it consists of many, many measurements. You've probably encountered sequence modeling tasks in your everyday life, especially if you've used things like google translate Alexa or Siri tasks like machine translation and question answering are all sequenced modeling tasks. And the state of the art in these tasks is mostly deep learning based.

Speaker 1:          01:16          Another interesting example I saw recently was this self parking car by Audi. When you think about it, parking is also a sequence modeling task because parking is just a sequence of movements and the next movement depends on all the previous movements. Um, you can watch the rest of this video, uh, online. Okay, so a sequence modeling problem. Now I'm just going to walk through a sequence modeling problem to kind of motivate why we need a different framework for specifically for modeling sequences and what we should be looking for in that framework.

Speaker 2:          01:49          Okay,

Speaker 1:          01:51          so the problem is predicting the next word. Given these words, we want to predict what comes next. The first problem we run into is that machine learning models that are not explicitly designed to deal with sequences take as input a fixed length factor. Think back to the feed forward neural network. From the first lecture that Alexander introduced, we have to specify the size of the input right at the outset. We can't sometimes feed in a vector of length. 10 other times feed into elector a vector of length 20 it has to be fixed line. So this is kind of an issue with sequences because sometimes we might have seen 10 words and we want to predict the next word. Sometimes we might've seen forwards and we want to predict the next word, so we have to get that variable length input into a fixed length vector.

Speaker 1:          02:39          One simple way to do this would be to just cut off the vectors. So say, okay, we're going to just take a fixed window, forced the specter to be fixed length by only considering the previous two words, no matter where we're making the prediction. We'll just take the previous two words and then try to predict the next word. Now we can represent these two words as a fixed length vector by creating a larger vector and then allocating space in it for the first word. And for the second word, we have a fixed length vector. Now, no matter what, two words we're using and we can feed this into a machine learning model like a feed forward neural network or a logistic regression or any other model and try to make a prediction. One thing you might be noticing here is that by using this fixed window, we're giving ourselves a very limited history.

Speaker 1:          03:28          We're trying to predict the word walk. Having only seen the words for and a, this is almost impossible, put differently. It's really hard to model longterm dependencies to see this clearly. Consider the word in, sorry, consider the sentence in France. I had a great time and I learned some of the blank language. We're trying to predict the word in the blank. I knew it was French, but that's because I looked very far back at the word France that appeared in the beginning of the sentence. If we were only looking at the past two words or the past three words or even the past five words, it will be really hard to guess the word in that blank. So we don't want to limit ourselves so much. We want to ideally use all of the information that we have and the sequence, but we also need a fixed length vector.

Speaker 1:          04:11          So one way we could do this is by using the entire sequence, but representing it as a set of counts in language. This representation is also known as a bag of words. All this is is a vector in which each slot represents a word and the number in that slot represents the number of times that that word occurs in the sentence. So here, the second slot represents the word this and there's a one because this appears once in the sentence. Now we have fixed length factor. No matter how many words we have, the vector will always be the same size as the council just be different. We can feed this into a machine learning model and try to make a prediction. The problem you may be noticing here is that we're losing all of the sequential information. These counts don't preserve any order that we had in the sequence to see why this is really bad.

Speaker 1:          04:57          Consider these two sentences. The food was good, not bad at all versus the food was bad. Not good at all. These are completely opposite sentences, but their bag of words representation would be exactly the same because they contain the same set of words. So by representing our sentence as counts, we're losing all of the sequential information, which is really important because we're trying to model sequences. Okay, so what do we know? Now we want to preserve order in the sequence, but we also don't want to cut it off to um, to a very short length. You might be saying, well, why don't we just use a really big fixed window? Before we were having issues because we were just using a fixed window of size to what if we extended that to be a fixed window of size seven. And we think that by looking at seven words, we can get most of the context that we need. Well, yeah, okay, we can do that. Now we have another fixed length vector just like before. It's bigger, but it's still fixed length. We have allocated space for each of the seven words. We can feed this into a model and try to make a prediction.

Speaker 1:          05:58          The problem here is that I consider this in the scenario where we're feeding this input factor into a feed forward neural network. Each of those inputs, each of those ones and zeroes has a separate weight connecting it to the network. If we see the words this morning at the beginning of the sentence, very, very commonly the network, we'll learn that this morning represents a time where a setting, if this morning then appears at the end of the sentence, we'll have a lot of trouble recognizing that because the weights at the end of the vector never saw that phrase before and the weights from the beginning of the vector and not being shared with the end. In other words, things we learn about the sequence won't transfer if they appear at different points in the sequence because we're not sharing any parameters.

Speaker 1:          06:47          All right, so you kind of see all the problems that arise with sequences now and why we need a different framework. Specifically, we want to be able to deal with variable length sequences. We want to maintain sequence orders so we can keep all of that sequential control information. We want to keep track of longer term dependencies rather than cutting it off too short and we want to be able to share parameters across the sequence so we don't have to relearn things across the sequence because this is a class about deep learning. I'm going to talk about how to address these problems with neural networks, but no, that time series modeling and sequential modeling is a very active field in machine learning and, and it has been and there are lots of other machine learning methods that have been developed to deal with these problems. But for now all talk about recurrent neural networks.

Speaker 2:          07:35          Okay.

Speaker 1:          07:36          Okay. So a recurrent neural network is architected in the same way as a normal neural network. We have some inputs, we have some hidden layers and we have some outputs. The only difference is that each hitting unit is doing a slightly different functions. So let's take a look at this one hidden unit to see exactly what it's doing.

Speaker 1:          07:58          Ever current hidden unit computes a function of an input and it's own. Previous output it's own previous output is also known as the cell state. And in the diagram it's denoted by s the subscript is the time timestamp. So at the very first time stuff t equals zero. The recurrent current unit computes a function of the input at t equals zero and of its initial state. Similarly at the next time stop, it computes a function of the new input and it's previous cell state. If you look at the function at the bottom, the function to compute as to you'll see it's really similar to the function for that a hidden unit in a feed forward network, um, computes. The only difference is that we're adding in an additional term to incorporate its own previous state. Common way of viewing recurrent neural networks is by unfolding them across time. So this is the same hidden unit at different points in time. Here you can see that at every point in time it takes us input it's own previous state and the new input at that time step.

Speaker 1:          09:09          One thing to notice here is that throughout the sequence we're using the same weight matrices. W and u. This solves our problem, a parameter sharing. We don't have new parameters for every point in the sequence. Once we learned something, it'll can apply at any point in the sequence. This also helps us deal with variable lengths sequences because we're not pre specifying the length of the sequence. We don't have separate parameters for every point in the sequence. So in some cases we can unroll this rnn to four time steps and other cases we can unroll it to 10 times steps.

Speaker 2:          09:41          Yeah.

Speaker 1:          09:42          Final thing to notice is that Essa, Ben, the cell state at time n can contain information from all of the past time steps. Notice that each cell state is a function of the previous cell state, which is the function which is a function of the previous cell state and so on. So this kind of solves our issue of longterm dependencies because at a time step, very far in the future that sal state encompasses information about all of the previous cell states.

Speaker 1:          10:13          All right, so now that you kind of understand what a recurrent neural network is, and just to clarify, actually shown you one hitting unit in the previous slide, but in a full network you would have many, many of those hidden units and even many layers of many hidden units. So now we can talk about how you would train a recurrent neural network. It's really similar to how you train a normal neuro network. It's backpropagation. There's just an additional time dementia. As a reminder in backpropagation, we want to find the parameters that minimize some loss function. The way that we do this is by first taking the derivative of the loss with respect to each of the parameters and then shifting the parameters in the opposite direction in order to try and minimize the loss. This process is called gradient descent. So one difference with rns is that we have many time steps so we can produce an output at every time step because we have an output at every time step. We can have a loss at every time step rather than just one single loss of fee end. Because, and the way that we deal with this is pretty simple. The total loss is just the sum of the losses that every time stuff. Similarly, the total gradient is just the sum of the gradients at every time step.

Speaker 1:          11:36          So we can try this out by walking through this gradient computation for a single parameter. W W is the weight matrix that we're multiplying by our inputs. We know that the total loss, the the total gradient, so the derivative of loss with respect to will be the sum of the gradients at every time step. So for now we can focus on a single time step knowing that at the end we would do this for each of the time steps and then sum them up to get the total gradient. So let's take time. Step two, we can solve this gradient using the chain rule. So the derivative of the loss with respect to w is the derivative of the loss with respect to the output, the derivative of the output with respect to the south state at time two and then the derivative of the cell state. With respect to w. So this seems fine, but let's take a closer look at this last term. You'll notice that as to also depends on s one and s one also depends on w. So we can't just leave that last term as a constant. We actually have to expand it out farther. Okay. So how do we expand this out farther? What we really want to know is how exactly does the cell state at time step to depend on w?

Speaker 1:          12:55          Well it depends directly on w because it feeds right in. We also saw that as two depends on s one which depends on w and you can also see that s two depends on ss zero, which also depends on w in other words. And here I'm just writing it as a summation and those uh, the, the sun that you saw on the previous slide as a summation form and you can see that the last two terms are basically summing the contributions of w in previous time stops to the error at time. Step t, this is key to how we model longer term dependencies. This gradient is how we shift our parameters and our parameters. Define our network by shifting our parameters such that they include contributions to the error from past time stops there, shifted to model longer term dependencies. And here I'm just writing it as a general, some not just for time stuff too.

Speaker 2:          13:58          Okay.

Speaker 1:          13:58          Um, okay. So this is basically the process of backpropagation to tap through time. You would do this for every parameter in your network and then use that in the process of gradient descent.

Speaker 2:          14:09          Yeah.

Speaker 1:          14:10          In practice, rns are a bit difficult to train, so I kind of want to go through why that is and um, what some ways, some ways that we can address these issues. So let's go back to the summation. As a reminder, this is the derivative of the loss with respect to w and this is what we would use to shift our parameters. W The last two terms are considering the air of wwe at all of the previous time steps. Let's take a look at this one term. This is how we, this is the derivative of the cell state at time. Step two, with respect to each of the cell states.

Speaker 2:          14:53          Okay.

Speaker 1:          14:53          You might notice that this itself is also a chain rule because as two depends on s one and s one depends on a zero. We can expand this out farther. This is just for the derivative of s two with respect to a zero but what if we were looking at a time step very far in the future like time step n that term would expand into a product of an terms and okay, you might be thinking so what? Well as notice that as the gap between time stops gets bigger and bigger, this product in the gradient gets longer and longer and if we look at each of these terms, what, what are each of these terms? They all kind of take the same form. It's the derivative of a cell state with respect to the previous cell state. That term can be written like this and the actual form that actual formula isn't that important. Just notice that it's a product of two terms, ws and F primes. W's are our weight matrices. These are sampled mostly from a standard normal distribution, so most of the terms will be less than one f prime is the derivative of our activation function. If we use an activation function such as the hyperbolic tangent or a sigmoid f, prime will always be less than one.

Speaker 2:          16:16          Okay.

Speaker 1:          16:16          In other words, we're multiplying a lot of small numbers together in this product. Okay, so what does this mean? Basically it recall that this product is how we're adding the gradient from future time steps to the gradient. Sorry. How we're adding the gradient from pastime steps to the gradient at a future time stuff.

Speaker 2:          16:38          Okay.

Speaker 1:          16:40          What's happening then is that errors due to further and further back time stops have increasingly smaller gradients because that product for further back time stops will be longer and since the numbers are all decimals, there'll be, it will be, it will. It will become increasingly smaller. What does, what this ends up meaning at a high level is that our parameters will become biased to capture a shorter term dependencies. The errors that arise from further and further back time stops will be harder and harder to propagate into the gradient at future time stops. Recall that recall, this example that I showed at the beginning, the whole point of using recurrent neural networks is because we wanted to model longterm dependencies, but if our parameters are biased to capture short term dependencies, even if they see the whole sequence, there'll be by the parameters will become biased to to predict things based mostly on the past couple of words. Um, okay, so now I'm going to go through some, a couple methods that are used to address this issue in practice that work pretty well.

Speaker 1:          17:57          The first one is the choice of activation function. So you saw that one of the terms that was making that product really small was the f prime term. I've crime is the derivative of whatever activation function we choose to use. Here I've plotted the derivatives of some common activation functions. You could see that the derivative of hyperbolic tangent and sigmoid is always less than one. In fact for sigmoid is always less than 0.25 and instead we choose to use an activation function like rarely. It's always one above zero so that will at least prevent the f prime terms from shrinking the gradient.

Speaker 2:          18:32          Okay.

Speaker 1:          18:32          Another solution would be how we initialize our weights. If we initialize the weights from a normal distribution, there'll be mostly less than one and they'll immediately strength the gradients. If instead we initialize the weights to something like the identity matrix, it will at least prevent that w term from shrinking the product at least at the beginning.

Speaker 1:          18:54          The next deletion is very different. Um, it involves actually adding a lot more complexity to the network using a more complex type of cell called a gated sell rather than here rather than each node just being that simple rnn unit that I showed at the beginning, we'll replace it with a much more complicated cell. A very common gated cell is something called an Lstm or long short term memory. So like its name implies lstm cells are able to keep memory within the cell state unchanged for many times stuffs this allows them to effectively model longer term dependencies. So I'm going to go through a very high level overview of how lsts work, but if you're interested, feel free to email me or ask me afterwards and I can direct you to some more resources to read about. LSTM is a lot more detail.

Speaker 2:          19:45          Okay.

Speaker 1:          19:45          All right. So lstm basically have a three step process. The first step is to forget your relevant parts of the cell state. For example, if we're modeling a sentence and we see a new subject, we might want to forget things about the old subject because we know that future words will be conjugated according to the new subject.

Speaker 2:          20:06          Okay.

Speaker 1:          20:07          The next day, the next step is an update stuff. Here's where we actually update the cell state to reflect the new, the information from the new input. In this example, like I said, if we've just seen a new subject, we might want to, this is where we actually update the cell state with the gender or whether the new subject is plural or singular.

Speaker 2:          20:26          Yeah.

Speaker 1:          20:27          Finally we want to output certain parts of the cell state. So if we've just seen a subject, we have an idea that the next word might be a verb. So we'll, I'll put information relevant to predicting a verb like the 10th. Each of these three steps is implemented using a set of logic gates and the logic gates are implemented using sigma functions. To give you some intuition on why Lstm is help with the vanishing gradient problem, uh, is that first the forget gate. The first step can equivalently be called the remember gate because there you're choosing what to forget and what to keep in the cell state.

Speaker 1:          21:09          The forget gate can choose to keep information in the cell state for many, many times steps. There's no activation function or anything else shrinking that information. The second step, the second thing is that the cell state is separate from what's outputted. We're mate. This is not true of normal recurrent units like I showed you before in a simple recurrent unit. The cell state is the same thing as what that sell outputs with an Lstm, it has a separate cell state and it only needs to output information relevant to the prediction at that time stuff. Because of this, it can keep information in the cell state, which might not be relevant at this time. Stuff that might be relevant at a much later time step. So we can keep that information without being penalized for that. Finally, I didn't indicate this explicitly in the diagram, but the way that the update step happens is through an additive function not through a multiplicative function. So when we take the derivative, there's not a huge expansion. So now I just want to move on to going over, um, some possible tasks. So the first task is a classification. So here we want to classify tweets as positive, negative or neutral.

Speaker 1:          22:25          And this task is also knowing that sentiment analysis.

Speaker 2:          22:29          Okay?

Speaker 1:          22:30          The way that we would design a recurrent neural network to do this is actually not by having an output at every time step. We only want one output for the entire sequence. And so we'll take him the entire sequence, the entire tweet, one word at a time. And at the very end we'll produce an output which will, which will actually be a probability distribution over possible classes where our classes in this case would be positive, negative, or neutral. Note that the only information that is producing the output at the end is the final cell state.

Speaker 2:          23:05          Okay.

Speaker 1:          23:05          And so that final cell state kind of has to summarize all of the information from the big, from the entire sequence into that final cell state. So we can imagine if we have very complicated tweets or, well, I don't know if that's possible, but very complicated paragraphs or sentences. We might want to create a bigger network with more hidden states to allow that last state to be more expressive. The next task would be something like music generation, and I'll see if this will play.

Speaker 2:          23:38          Yeah,

Speaker 1:          23:41          you can kind of hear it. Okay. So that was music generated by an Rnn, which is pretty cool. And something you're actually also going to do in the lab today. But, uh, music generally you can, an rnn can produce music because music is just a sequence and the way that you would, the way that you would construct a neuro on a recurrent neural network to do this would be at every time point taking in a note and producing the most likely. Next note given the notes that you've seen so far. So here you would produce an output at every time step.

Speaker 1:          24:27          The final task is machine translation. Machine translation is interesting because it's actually two recurrent neural networks, um, side by side. The first is an encoder. The encoder takes as input a sentence in a source language like English, it. Then there's an a decoder which produces the same sentence in a target language like French notice in this architecture that the only information passed from the encoder to the decoder is the final cell state. And the idea is that that final cell state should be kind of a summary of the entire encoder sentence. And given that summary, the decoder should be able to figure out what the encoder sentence was about and then produce the same sentence in a different language. You can imagine though that, okay, maybe this is possible for a sentence, a really simple sentence like the dog eats. Maybe we can encode that in the final cell state, but if we had a much more complicated sentence or a much longer sentence, that would be very difficult to try and summarize the whole thing and that one cell state.

Speaker 1:          25:32          So what's typically done in practice for machine translation is something called a tension with attention rather than just taking in the final cell state to the decoder at each time step, we take in a weighted sum of all of the previous cell states. So in this case we're trying to produce the first word. We'll take in a weighted sum of all of the encoder states. Most of the weight will probably be on the first state because that's what would be most relevant to producing the first word. Then when we produced the second word, most of the weight will probably be on the second cell state, but we might have som on the first and the third to try and get an idea for, um, the tents or the gender of this now. And the same thing for all of the cell states. The way that you implement this is just by including those weight parameters in the weighted sum as additional parameters that you chain using backpropagation just like everything else. Okay. So I hope that you have an idea now why we need a different framework to model sequences and how recurrent neural networks can solve some of the issues that we saw at the beginning, um, as well as an idea of how to them and saw some of the vanishing gradient problems.

Speaker 2:          26:47          Okay.

Speaker 1:          26:47          I've talked a lot about language, but you can imagine using these exact same neural network, recurrent neural networks for modeling time series or wave forms or doing other interesting sequence prediction tasks like predicting stock market trends or summarizing books or articles. Um, and maybe you'll consider some sequence modeling tasks for your final project. So thank you.

Speaker 2:          27:09          [inaudible].