Speaker 1:          00:00          No to the second part, it's quite different. So it's told mapping. This is concerned in our, so before we had a optimization for the light simulation, now we'll will look at the very end of our rendering pipeline. So the issue of showing the image output on a display because the problem is that alight simulation outputs radiants so it's the how much light travels along one direction when it comes from a small surface page. So you collect radiance with your camera, record it, but somehow your display expects RGB values. So the radiance can carry some curling formation. So you could, for example, um, trays the are the g the B value, uh, general independently. You can do something with more fidelity like spectral ray tracing. So you trace rays that um, were not only radiance is carried along the rape, but spectro radiance. So gradients of a certain wavelength of the light.

Speaker 1:          01:25          This, for example, you can see this effect in prisons where you split white light into rainbow colors. So if you do not perform spectral ray tracing there and just assume we have white radiant, then you cannot simulate this effect because the, then the reflection, so the rage geometry changes with the wavelength that's associated to it. So your refraction angles get different and this causes the split into the rainbow colors. So, but in some way you have radiant as output. So to show them on this place or to bring them, you need to convert your images. Some are from Radians to RGP and there is an inherent problem with it because radiance of light simulation have a huge range because they tried to simulate real world physics. And in real world you have a huge difference between dark and bright. So for example, if you take, um, the ratio between the surf between a surface at say, ground level in the earth atmosphere that is either illuminated by the sun or the moon, the differences effect of $800. And then he mentioned that the patch of the ground is either white or black. This courses and other different in the reflected radiants quantities of approximately a factor 100. So if you want to do, say, a Chandler light simulation, and you can expect that elimination by Sun and moon should be in there, white and black surfaces should also be in there. You have to somehow cold with a ratio between the darkest and the brightest values of 80 million.

Speaker 2:          03:46          MMM.

Speaker 1:          03:49          So is it relevant to do that? So can people even see the difference between that? And yes, they can. So since these are highly relevant, um, features of our world to differentiate dark from bright especially and the very bright and dark conditions, so imagine a cave men in the woods at night, it will be very good to see small contrast differences that could indicate predators. So there was a evolutionary, um, forcing to develop, to develop a visual system that, um, also can take advantage of this huge dynamic range of radiants values. So for example, how this is built up. So photoreceptors in our eyes, they have a chemical bleaching when they are hit by light particles. So this can be regulated biochemical to enable adaptation in a range of two orders of magnitude. So just by regulating the CPO chemical properties of photoreceptors, you can adapt the I to a rain. The difference in dark and bright of 100 the pupil size gives you another order of magnitude. So a factor of 10 and the neural adaptation is more or less the signal processing. So what to do actually with the changing signal because your receptors get bleached. So the dynamic range of human vision is approximately 100 million. So that means that we can perceive in fact the dynamic range of realistic conditions on in our atmosphere.

Speaker 2:          05:51          But

Speaker 1:          05:53          then you have, so the output of the light, if it takes this into account can be, it can have a difference of 80 million fixed of 80 million between dark and bright and 10 you want to show this on the display. So the technology for a standard display gives you a dark too bright ratio of approximately 1000 and if you use eight feet in Kolding of your values, then you only have 265 values for it beat chambers. That means that somehow our technology is immensely inadequate to show realistic scenarios.

Speaker 1:          06:44          It's all great. This is also good because you get not blinded by your display, but we have to somehow account for that. So just taking the radiance output and converting it trivially, two images will carry some problems with it. Yeah, usually big bitmaps files or [inaudible]. Um, but um, usually you'll have, um, you split them into the different colors. So you have eight weeks for our eight weeks for g h big floppy. And you can somehow imagined that this gives you how much red light is at this pixel. And Yeah, you call 206. I mean, of course for the whole color gamut you have more venues. But if you just look at the dark brighter ratio of a single college, this is approximately what you have. So now Tom mapping is the methods that was developed to combat this problem. So the output of a light simulation as already said, high dynamic range because of the of real world dynamic range of brightness, values and display devices usually have a low dynamic range. So what do we need to do is to compress the range of alt book somehow. And this is referred to as added tone mapping, a tome reproduction. So these are the names that you find in the literature.

Speaker 2:          08:24          MMM,

Speaker 1:          08:27          there are two issues here. One is range compression. So how to convert high dynamic range luminant's to load and they make range luminance. This is the content of this lecture and then you still have um, Luminance's, but usually there are standardized color spaces in which images are stored. So you don't store your own, um, hold rooms format, which takes certain wavelength and thank yous, the luminance values for those, but they are standardized ways to do that. And this is covered in a different lecture about color. You'll see the, um, lecture number here. So I refer you to this lecture if you want to know more about color in here. Now I only, um, explain range comparation to some extent. So

Speaker 2:          09:28          okay,

Speaker 1:          09:29          now, uh, um, graphic example. So here we have a bright as single bright light source with no ambient light in the scene. That means that all, um, contrast comes only from this one light source via global illumination.

Speaker 2:          09:51          Okay.

Speaker 1:          09:52          So if you take, if you would photograph the scene and take a short exposure, this is what your kit. So all the um, dimly illuminated parts completely disappear in blech.

Speaker 2:          10:07          Okay?

Speaker 1:          10:08          And, but you see, um, details in previously to bright scenes. So that led to over exposure of your camera. So for example, here you now see the um, the outline. So the senior editor of the bulb, for example,

Speaker 2:          10:29          okay,

Speaker 1:          10:29          a very long exposure leads to uh, our overexposure of most of the image. So you do not see any detail anymore close to the lamp or ad directly illuminated surfaces. But what you see is that previously dimly illuminated objects in the background now are whale perceivable.

Speaker 2:          10:53          Okay?

Speaker 1:          10:54          So this is what you get if you take only a small part of it dynamic range and map those to an image.

Speaker 1:          11:05          And what could you do to combine all this information into one image so that the most trivial thing you can imagine is I just divide by the maximum of the scene. This gives me a nice, um, Newman ends values from zero to one. And this I can make to whatever range I want. The problem with this is that usually the maximum is not indicative of the whole scene. The maximum is usually extremely bright. So when you're divided by the maximum, the only thing you see is as a single reflection in the close to the light bulb.

Speaker 2:          11:44          Okay.

Speaker 1:          11:45          If your Clampett, so let's say I have arrange of enters that I'm interested in. Everything below is, is clamped by a Ciro. Everything above my maximum that I decide to have is completely white. This is exactly the problem that that becomes obvious with high dynamic range that parts are underexposed, other parts are way exposed. So clamping also doesn't get you something. You have to take the whole range. You cannot just ignore certain parts.

Speaker 1:          12:22          So one approach that gives you nice resources, explanation mapping, they are us zoom at exponential distribution in your, um, luminance values. And then rescale is exponential function into a linear one. That means that, um, very bright values, it gets cave down low. Alias gets scaled up, but you, you account for the very bright spots. So these are the things that gets caged the most. So the pride values at the bow get some reasonable white, but they do not dominate the whole scene. Um, and more sophisticated approaches. The Reinhart told mapper developed by Reinhardt, the guy. Um, and this is um, this is the one example that they will present in this lecture. Um, as you can imagine, there are many NATO members. So everyone tried to um, do some approach that works very well for a certain subset of scenes. Some, um, methods were optimize to four parallel hardware or for certain um, hardware architectures to give more speed there. Sam presented rough approximation that work in real time. So there is already a quite wide bunch of tone may pose to choose from. Um, that I end how tone may is one of the most common. So if you're look into some at our open source or commercial rendering software, this is usually one of the things to this implement.

Speaker 1:          14:17          So, um, entity, there is also some additional information to put this into context. Um, there is some, um, some approach in digital photography to take, um, multiple exposure images. So to combine, um, exposures of the same scene

Speaker 1:          14:49          that are taken with different exposure times into one image. So this is called high dynamic range photography and usually to use similar methodologies. So the iron hot tone mapa can also be used to combine or for HDR photography. But if you put a HDR photography into Google, you usually end up with images like that. And this is not for Tom may pick is about tone mapping is a perceptually and physically validated approach that gives you realistic impressions of the scene and it's not an artistic effect. So it's not that you take tone mapping, look at the image and say, ah, I would like to have it to have more contrast in there and then you change your tone member. I mean, this is not, it didn't stand at use. And the community of HDR photography to a large extent over, uh, uses this capability of the system.

Speaker 1:          16:01          For example, this photograph here is a completely botched implementation or use of Hdr, uh, range compression for example, things that you see that are not correct. Um, the halos around the balloons. So sadly, why the sky is brighter around the silhouettes of the balloons. This is, um, this is, uh, an effect that is present in the visual system to some extent, but not to this extent. So this is, some people think that this is nice, nice HDR photography, but this is just wrong. Simply a state. If you like that your images look like that, then this is an artist's decision not told, met big. This is just, yeah, contrast enhancement, you can call it. Also the colors in this image, uh, get screwed up because they are not oversaturated. So this is just a warning. The tone mapping the during counter for, um, lights in relations should not look like that because then you have arr in your application. Yeah. If I mean this, this is a combination of the forensics cultures.

Speaker 1:          17:39          Now it's, I mean it, it is a combination of different exposures and it's taken with a tone mapping approach and you have to do the combination somehow. It said before you can add a device with a maximum and combined them clamping. But, um, usually the toll paper give you a more realistic result. For example, why is it obvious that here already multiple exposures were combined? Because in the very left of the left, you see that you still see some kind of detailing the boot in behind, but at the same time use, you can call us direct look into the sun without having blood off the screen. So that means that already a huge dynamic range was compressed here, but it should not look like that. It should look like that if it's done correctly.

Speaker 2:          18:40          Yeah.

Speaker 1:          18:42          And this would be the correct application for this photograph. So it looks a whole realistic. It doesn't oversaturate the colors. It only has a very, uh, light, uh, halo around the cigarettes, which is also an effect that's present in the visual system.

Speaker 2:          19:04          Okay.

Speaker 1:          19:05          So this is what you should aim for. So, um, now about tone mapping itself. Um, there are two large different classes. So one or the global total Mack was and the other local, so the Global Tome map was used a mapping function that converts radiums at a certain Pixel to say RTP value. If you select this color space and this mapping functions uniform, what I mean here is not that it's uniform, it produces the uniform output value. So you don't get a image with a single color value, but the function it's safe just takes as input the radio into certain pixel and the old codes are Gbv and these functions than used four pixels image.

Speaker 2:          19:58          Okay.

Speaker 1:          19:59          Um, more complex methodologies are local. Told me papers, which not only take the a single pixel into account, but also it's in via its neighbors.

Speaker 2:          20:19          Okay.

Speaker 1:          20:22          Um, this is, this is perceptually motivated because um, Seo have seen before, that's called trust or brightness adaptation of your eyes in the, in the photo receptors for example, this is done locally, so it single fault, the Resectr attempts to different brightnesses. So that means that you told mapping in the human, I use a local behavior. Um, but they are, are, um, there are reasons to employ both of them. So the Global Tome mappers, the are fast because they have a single mapping function. So you take, you can execute this function parallel on each pixel. So this makes it perfectly usable for GPU approaches, for example. But you're encouraged some loss of detail because you cannot locally look as if the, is this already a dark patch of my scene so I can enhance the country's more there? Or is it at a dark, bright boundary where I do not do it that much? And the local, uh, don't mappers. And nowadays, so they allow a local contrast enhancement, but they are slope because you're not only have to say, look at the big four, but also its neighborhood. The neighborhood grows a quarter ethically if you enlarge it. So, um, the, you incur a different complexity in your problem.