Speaker 1:          00:01          Let's talk about just briefly about the PBR t or he touched her. Pbr t is not exactly the renderer that we are going to use. We're going to use luxe render but like render was built upon PBR t and therefore the basic structure you mean completely intact and this is a really good architecture that you would see that many of the rendering engines out there, global illumination rendering engines out there use, most of them use the very same architecture. So we have a main sampler, a random test that asks the sampler to provide random samples. So the Sampler, you can imagine as a random number generator, we need a lot of different random members because the pixel that we are sampling some techniques choose it deterministically going from Pixel to pixel. Some techniques take big those randomly,

Speaker 2:          00:57          I mean

Speaker 1:          00:58          which Pixel we choose to be sampled is usually deterministic, but the displacement because we would be sampling the pixels.

Speaker 2:          01:07          Okay,

Speaker 1:          01:07          not in, not only in the mid point, like recursive for each facing, but you, you would take completely random sample from nearby and you filtering to sum them up in a meaningful way. Now this requires random numbers. They come from the sampler. You would also send outgoing race in the hemisphere, have different objects. You also need random numbers for this. So in this sample, these random numbers arrive and this sample you would send to the camera and the camera would give back to you array. So you tell the camera, please give me a rate that points to this pixel and this camera would give you back array, which starts from the camera starting point in points. Exactly there. Now all you need to do is gift this rate to the integrator and the integrator will tell you how much radiant is coming along district. And what you can do after that is right into a film. And

Speaker 2:          02:07          okay,

Speaker 1:          02:08          this is not necessarily trivial because for instance, you could just simply write it to a ppm or a PNG file and be done with it.

Speaker 2:          02:19          Okay.

Speaker 1:          02:19          In contrast, what [inaudible] does is it has a film class and what you can do is that you can save different, for instance, different contributions in different buffers. So what you could do is for instance, separate direct and indirect illumination into different fields, different images. And then you can Indian sometime up. But maybe you could say that I don't need costings on this image and then you would just pop that image so you can do tricky things if you have a correctly implemented from class.

Speaker 2:          02:50          Okay.

Speaker 1:          02:51          Okay. So Lux wonder, just what I have been talking about, it's built upon Pvr t and uses the very same architecture. This is how it looks. So it has a graphical user interface and you can also manipulate different tone, lacking algorithms in there, different denoising image denoising algorithms in there and you can even manipulate light groups. This is another tricky thing with the film class. Basically what this means is that

Speaker 2:          03:21          okay,

Speaker 1:          03:22          you saved the contributions of different light sources into different films by themes. You can imagine

Speaker 1:          03:30          image files. So every single light source has a different PNG file if you will. And they are saved into their, and the final image would come up as a, some, uh, some of these individual films. But you could say that one of the light sources is a bit too bright. I would like to tone it down, but if you want to tone it down then you would have to render your image because you changed the physical properties of what's going on. Now you can do this if you have this light groups option because they are stored into individual buffers, so you can just dim one of these images and just add them up together and then you would have the effect of that light source a bit dimmer. You can, for instance, completely turn off sunlight or television that you don't, that you don't want to use in the scene. It, it sounded like a good idea, but it wasn't. You can just turn it off without we rendering the scene and you can operate all of these things into through the Luxor render gooey. Now before we go into algorithms, let's talk about algorithm classes. What kinds of algorithms are we interested in? First, what we are interested in is consistent algorithms. Consistent means that if I use an infinite number of Monte Carlo samples, then I would convert exactly to the right answer. I would get back the exact integral of the function.

Speaker 1:          04:59          Intuitively it says, if I ran this algorithm, sooner or later, it will converge.

Speaker 1:          05:06          It also is important to note that no one said anything about when the sooner or later happens. So if an algorithm is consistent, it doesn't mean that it is fast. It doesn't mean that it's slow. It can be anything, absolutely anything. It may be that there is an algorithm that's theoretically consistent, so after an infinite amount of samples, you would get the right answer, but it really feels like infinity. So it, it may be that after two weeks you still don't get the correct image. Our algorithms like that, and theoretically that's consistent. That's fine because you can prove that it's going to convert sooner or later.

Speaker 1:          05:47          The more difficult class that many people seem to mess up is unbiased algorithms. Now, what does it mean? If you just read the formula, then you can see that the expected error of the estimation is zero and we have to note that this is completely independent of n. N is the number of samples that we have taken. Now, the expected error of the estimation is zero. It doesn't mean that the error is zero because it's independent of the number of samples. It doesn't mean that after one sample per pixel, I get the right result. It says that the expected error is zero. I will give you many intuitions of this for this because it is very easy to misunderstand and misinterpret because in statistics there's a difference between expected value and variance,

Speaker 1:          06:40          and this doesn't say anything about the variance. This only tells you about the expected values. So for instance, if you're a mathematician and and think a bit about this, then you could say that if I have an unbiased algorithm and I have two noisy images, you're under something on your machine. I run something that my machine that's too noisy images, I could merge them together. I could ever judge them because they are unbiased samples. It doesn't matter where they come from. I would add the samples together, average them and I would get a better solution and we will see an example for that. My favorite intuition is that the algorithm has the very same chance of over and underestimating the integrated. So it means that if I would try to estimate the outcome of a dice roll, the expected value you can, you can roll from one to six with equal probabilities. The expected value is 3.5 so this means that I would have the very same probability of saying for as I would have the probability for saying three. So it's the very same chance to under and overestimating. And I'll give you my other favorite intuition. This is what journalists tend to like. The best means that there's no systematic error in the algorithm. The algorithm doesn't cut corners. And if there are errors in the image than this can be all the noise. And this noise comes because you don't have enough samples. And if you add more, you're guaranteed to get better.

Speaker 2:          08:14          Okay.

Speaker 1:          08:14          Now let's take another look at this really good intuition so I can combine together too noisy images. So this means that I should be able to do network rendering without actually using the network.

Speaker 2:          08:31          Okay.

Speaker 1:          08:32          Which sounds a bit mind boggling. I really liked the parallel to this, which is a famous saying of Einstein from long ago where they talked about sending electromagnetic waves out and they talked about the telephone and people could not grasp the idea of a telephone. And he said that we would have a super, super long cat one. The tail of the cat would be in Manhattan and if you would just pull the tail of the cat in Manhattan, then the front of the cat would be in New York. And if you pull the tail in Manhattan, then she would say meow in New York. And he asked the people, is this understandable? Yes, this is understandable. Okay, perfect. We're almost there. Now imagine that there's no cat and this is the exact same thing. So this is natural trend ring without an actual network. Well, okay. Mathematical theories.

Speaker 1:          09:30          Okay, but let's actually, let's give it a trial. So what I did here is I render this interior scene and this is how it looks like after two minutes, it's really noisy, right? Not what I did is I ran 10 of these rendering processes and save the images 10 times. So I didn't run one rendering process for long. I ran many completely independent rendering processes for to two minutes. And what I did is I merge the images together. What it means is that I averaged images, I added them together and average them. Now basically this means that you could do this on completely independent computers that have never heard of each other. And now let's take a look. This is the noisy image that we had and now let's merge 10 of these together. This is what we will get. Look closely, look at that. Now one more time. It's just a noisy after two minutes and this is merging some of these noisy images together. So this is unbelievable that this actually works. So if you have unbiased algorithms, you can expect this kind of behavior and you don't need to write sophisticated networking to use your path racer for instance. In a network because you don't need the network at all and this is really awesome.

Speaker 1:          11:02          No, because if you don't add any kind of seed to your computations, then you're computing completely independent samples and it doesn't matter if the sample is computed on the same machine or in a different machine. If you have some kind of determinism, then it may be possible that the same paths are computed by multiple machine and that's indeed wasted time. But otherwise it, it works just fine. Now let's practice a bit instead. There's a question. Yes, I was whoa. Biggest difference between one big feature renders 20 minutes and 10 pictures rendered two minutes each and then combined. Nothing in terms of samples, nothing that, the only difference is that you actually need to fire up that scene on multiple machines. So if there is like 10 gigabytes of textures, then it takes longer to load it up on multiple machines and, and maybe transfer the data together. But if you, if you think only in terms of sample, it doesn't matter where it comes from. Okay, let's practice a bit. We have different techniques and this is how the error is evolving in time. Now the intuition of consistent means that the error comes to zero over time, so if I render for long enough than the error is going to be zero.

Speaker 1:          12:26          Is this black one a consistent algorithm?

Speaker 2:          12:33          Hmm.

Speaker 1:          12:38          Nope, because it converges here to the dash line and not to zero. Now what about the other two guys? Are they consistent or not?

Speaker 2:          12:51          Yes.

Speaker 1:          12:53          Okay. Why

Speaker 2:          12:57          Baseline?

Speaker 1:          12:59          Okay, so the IRS seems to converge to zero. Okay. Now what about these techniques? Are they biased or unbiased? Which is which?

Speaker 1:          13:10          What about this one? This is the darker gray. Is this biased or unbiased? Now if we have this intuition that if you're under four more, the image is guaranteed to get better or at least not worse than this dark gray. It's definitely not unbiased because it is possible that I'm rendering for 10 minutes. That's this point, for instance. And I say, okay, I almost have a good enough image and I render for another five in each and expected to be better. And then I get this maybe a completely garbled up image full of artifacts and theirs. And that is entirely possible with biased algorithms. No one said that it's likely, but it is possible. So you cannot really predict how the error would evolve in time. And if you take a look at the other two lines, you can see that they are unbiased algorithms. So as you render for longer, you are guaranteed to get a better image.

Speaker 2:          14:11          Okay.