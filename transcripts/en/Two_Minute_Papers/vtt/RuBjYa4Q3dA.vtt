WEBVTT

1
00:00:01.380 --> 00:00:05.430
Now before we start the algorithms,
one more time,
a disclaimer.

2
00:00:07.020 --> 00:00:11.250
These results are coming from scientific papers and if you come up with a new

3
00:00:11.251 --> 00:00:12.084
method,

4
00:00:12.750 --> 00:00:17.750
you want to show that this method outperforms existing methods in the scenes or

5
00:00:18.451 --> 00:00:20.190
in the setups that you have tried.

6
00:00:21.090 --> 00:00:25.770
And some people are very open about the limitations of the techniques.

7
00:00:25.771 --> 00:00:30.120
Because if I have a technique that's better than the best technique out there on

8
00:00:30.121 --> 00:00:31.920
this scene,
that's great,

9
00:00:32.460 --> 00:00:36.210
but it doesn't mean that it will be better on all possible scenes.

10
00:00:37.530 --> 00:00:42.530
And some people are very candid about the limitations of the algorithms and some

11
00:00:43.441 --> 00:00:47.700
of them are not so candidate about this,
but with time,

12
00:00:47.701 --> 00:00:49.470
as people start to use the algorithm,

13
00:00:49.860 --> 00:00:54.860
these possible coronary cases or just simply difficult cases come up.

14
00:00:57.780 --> 00:01:02.100
So what do I mean by this?
But I mean is that if you see great results,

15
00:01:02.101 --> 00:01:06.450
that there is an algorithm,
wonderful results,
it's the best thing ever.
Okay.

16
00:01:06.480 --> 00:01:09.810
But always have a slide out.

17
00:01:10.320 --> 00:01:14.280
Whether this algorithm would be robust enough,
would it always work?

18
00:01:14.610 --> 00:01:18.900
When would it not work?
Because don't just extrapolate from one case.

19
00:01:20.250 --> 00:01:20.990
<v 0>Yeah,</v>

20
00:01:20.990 --> 00:01:25.080
<v 1>that may be drawbacks that are maybe not so clear,</v>

21
00:01:25.130 --> 00:01:28.700
not so clear when you first see the algorithm.
Now,

22
00:01:28.850 --> 00:01:33.850
mathematical details again will be omitted mostly and but what we are interested

23
00:01:33.861 --> 00:01:37.820
in,
the motivation for each algorithm,
what is the key idea,

24
00:01:37.850 --> 00:01:42.110
what are the advantages,
disadvantages,
how do the results look like?

25
00:01:42.440 --> 00:01:45.440
Where can you access implementations?
Where can you try these?

26
00:01:45.710 --> 00:01:49.610
And for most of them,
some additional literature.
If you think that,
wow,

27
00:01:49.611 --> 00:01:51.170
this is a really great algorithm.

28
00:01:51.230 --> 00:01:53.840
I would like to know more than there will be links.

29
00:01:54.020 --> 00:01:58.200
You click them and then you can read either the paper or some writing.
So far,

30
00:01:59.240 --> 00:02:00.290
so let's get started.

31
00:02:00.320 --> 00:02:04.340
Part facing from 1986 super old stuff,

32
00:02:04.580 --> 00:02:09.580
but this is the very first and the easiest way to wrap your head around global

33
00:02:09.891 --> 00:02:10.724
illumination.

34
00:02:12.080 --> 00:02:15.620
You'll start your race from the eye or the camera.

35
00:02:15.680 --> 00:02:17.270
You bounced them around the scene.

36
00:02:17.330 --> 00:02:20.840
If you would like to earn some style points done after every balance,

37
00:02:20.841 --> 00:02:24.740
you would also trace shadow race towards the light source.

38
00:02:24.741 --> 00:02:26.300
This is next event destination.

39
00:02:26.510 --> 00:02:30.440
This usually lower Sioux or variance and then you end up somewhere.

40
00:02:30.470 --> 00:02:33.980
You compute all these life paths and jolly good.

41
00:02:34.010 --> 00:02:36.970
You don't do any simplifications to the integral.

42
00:02:37.000 --> 00:02:40.220
You exhaustively sample all possible light paths.

43
00:02:40.490 --> 00:02:43.880
There's no interpolation,
no tricks,
no magic,

44
00:02:44.030 --> 00:02:47.420
so this should be an unbiased and consistent algorithm

45
00:02:49.270 --> 00:02:53.560
and bias.
The error is predictable.
I know that if I add more samples,

46
00:02:53.590 --> 00:02:57.370
there's going to be less error and I know that sooner or later the image is

47
00:02:57.371 --> 00:03:01.420
going to converge because I am all possible light paths.
There are,

48
00:03:01.720 --> 00:03:04.720
it is impossible that I would miss something.
Now,

49
00:03:05.020 --> 00:03:10.020
there may be corner cases but they are really difficult but fortunately well

50
00:03:10.451 --> 00:03:14.530
understood.
Corner cases where there are contributions that you may miss.

51
00:03:14.740 --> 00:03:17.800
I will discuss this during the next lecture.

52
00:03:20.690 --> 00:03:24.620
What are the advantages?
It's simple.
It's also very easy to implement.

53
00:03:24.950 --> 00:03:28.700
I didn't write it therapeutic also parallelized as well.
Why?

54
00:03:29.090 --> 00:03:32.900
Because it's a dump algorithm.
It doesn't do anything tricky.

55
00:03:32.901 --> 00:03:37.901
It doesn't feel super difficult and super complicated data structures.

56
00:03:38.420 --> 00:03:43.420
You just put it on the Gpu and you just cram as many and you just dish out as

57
00:03:44.751 --> 00:03:47.180
many light paths per second as possible.

58
00:03:48.380 --> 00:03:51.950
What is a common problem that people encounter with this but for instance

59
00:03:51.951 --> 00:03:56.840
caustics converge very slowly because cost takes are usually light paths that

60
00:03:56.841 --> 00:04:01.841
are extremely improbable to be sampled and you would need to compute many,

61
00:04:02.151 --> 00:04:06.650
many samples in order to hit these costs many times in order to clean them up.

62
00:04:06.980 --> 00:04:07.813
Clean them up.

63
00:04:08.990 --> 00:04:09.690
<v 0>Yeah.</v>

64
00:04:09.690 --> 00:04:12.870
<v 1>Onwards 1993 by directional path racing.</v>

65
00:04:13.110 --> 00:04:15.680
What is the motivation behind this guy?
Well,

66
00:04:15.750 --> 00:04:20.670
imagine a scene that this is your camera on the left and you have a light source

67
00:04:20.671 --> 00:04:25.671
for instance enclosed in this object which is for now for the sake of

68
00:04:26.011 --> 00:04:28.680
experiment,
the black body.
So if you hit it from anywhere,

69
00:04:28.681 --> 00:04:32.700
it's not a glass light bulb or anything like that.
It's a black party.

70
00:04:32.970 --> 00:04:37.710
So whichever part of the container you hit,
you won't continue your path,

71
00:04:38.430 --> 00:04:42.120
not you would start a part racer.
What do you do?

72
00:04:42.150 --> 00:04:47.150
You start tracing the race from the camera and it is not too likely to hit the

73
00:04:48.091 --> 00:04:51.870
light source is it?
So it's not a point light source.
It's an area light source.

74
00:04:51.871 --> 00:04:56.871
It is possible to hit it but it's not my like now after the previous lecture you

75
00:04:58.351 --> 00:05:02.610
would say no problem.
Next event estimation,
what do I do?

76
00:05:02.970 --> 00:05:04.980
I don't wait until I hit the light source.

77
00:05:05.040 --> 00:05:09.540
I would send out shadow raise after every bounce and I would get some of the

78
00:05:09.541 --> 00:05:14.130
energy of the light source,
the direct contribution of the light source.
Great.

79
00:05:14.190 --> 00:05:18.030
But the problem is that this also doesn't work because most of the connections

80
00:05:18.031 --> 00:05:21.720
would be obstructed because if I hit this very first bounce,

81
00:05:21.870 --> 00:05:25.170
I cannot hit the light source because there's the black body that contains it.

82
00:05:25.470 --> 00:05:29.670
After the second balance,
I also cannot connect to the light side.
It's again,

83
00:05:29.760 --> 00:05:33.600
even with next event estimation,
most of my samples are wasted.

84
00:05:37.930 --> 00:05:39.370
Yeah,
tracing random race,

85
00:05:39.460 --> 00:05:43.030
it is very unlikely to hit the light source and even if I connect to the light

86
00:05:43.031 --> 00:05:47.020
source,
it is very unlikely that I will see an obstructed connection.

87
00:05:48.040 --> 00:05:51.070
What is the solution by direction and path racing?

88
00:05:51.160 --> 00:05:56.160
What happens here is that I'm not starting only one light path from the I,

89
00:05:56.650 --> 00:06:01.650
I start to light paths one from the eye as with the regular path tracing and I

90
00:06:02.001 --> 00:06:06.050
also start light paths starting out from the light sources.

91
00:06:06.260 --> 00:06:11.260
This is called light Tracy and I tried to combine these two techniques into one

92
00:06:12.141 --> 00:06:12.974
framework.

93
00:06:13.220 --> 00:06:18.220
So what it means is that I start one or a given number of bounces from the I,

94
00:06:19.910 --> 00:06:24.350
I start given number of bounces from the light source and then I connect these

95
00:06:24.351 --> 00:06:29.351
light paths together and I pretend that I just built this life passage that and

96
00:06:31.891 --> 00:06:36.891
now with this I have a much better chance to sample these light sources because

97
00:06:37.350 --> 00:06:42.350
I would have the opportunity to get out of that small zone that is otherwise

98
00:06:43.110 --> 00:06:44.910
difficult to hit from the eye.

99
00:06:46.580 --> 00:06:47.150
<v 0>Yeah.</v>

100
00:06:47.150 --> 00:06:50.030
<v 1>Now let's see the difference between the two techniques.</v>

101
00:06:50.450 --> 00:06:55.280
These are taken after 10 seconds for the very same scene and you could say that

102
00:06:55.790 --> 00:06:59.180
there's a huge difference for this indoor scene between the two.

103
00:06:59.181 --> 00:07:01.580
So it's definitely worth looking into.

104
00:07:06.230 --> 00:07:11.230
Now what is actually difficult about bi-directional path tracing is that

105
00:07:12.800 --> 00:07:15.740
theoretically it's very simple.
There is not one light path.

106
00:07:15.741 --> 00:07:19.610
There are two and I connect them in all possible different ways.

107
00:07:21.340 --> 00:07:22.020
<v 0>Okay.</v>

108
00:07:22.020 --> 00:07:22.853
<v 1>No.</v>

109
00:07:23.500 --> 00:07:27.520
What you should take into consideration is that this is actually to Monte Carlo

110
00:07:27.521 --> 00:07:28.390
processes.

111
00:07:28.810 --> 00:07:33.810
One Monte Carlo process is when you start out from the eye and you hit a diffuse

112
00:07:34.291 --> 00:07:36.100
or a glossy object,

113
00:07:36.280 --> 00:07:41.020
then you would start to important sample it important sample the BRDF.

114
00:07:41.021 --> 00:07:45.160
This means that I would take the likely paths more often.

115
00:07:46.090 --> 00:07:49.930
Now if you start a light path from the light source,

116
00:07:50.500 --> 00:07:54.490
then what you will be sampling is actually the distribution of the light source

117
00:07:54.491 --> 00:07:55.990
itself because

118
00:07:57.110 --> 00:07:57.590
<v 0>hmm.</v>

119
00:07:57.590 --> 00:08:02.240
<v 1>Regions that are visible from the light source would be sampled extensively with</v>

120
00:08:02.241 --> 00:08:04.640
light tracing because you're always hitting them.

121
00:08:04.641 --> 00:08:08.630
They are in front of you and that's a completely different sampling
distribution.

122
00:08:08.900 --> 00:08:13.900
So you can imagine as if you had two different Monte Carlo processes that sample

123
00:08:14.451 --> 00:08:19.451
the very same integrate and one Monte Carlo process would have some variance and

124
00:08:20.601 --> 00:08:24.620
the other will have some other variants.
So different regions of the,

125
00:08:25.010 --> 00:08:29.360
of the path space different and also different regions of the image would

126
00:08:29.361 --> 00:08:34.100
converge quicker with light precinct and different images would would converge

127
00:08:34.101 --> 00:08:36.170
quicker with standard path tracing.

128
00:08:36.590 --> 00:08:41.330
And I would like to combine these two techniques together and this is entirely

129
00:08:41.331 --> 00:08:43.460
not trivial variance.

130
00:08:43.580 --> 00:08:46.610
I've written not noise in there to be more intuitive,

131
00:08:46.611 --> 00:08:50.870
but we are talking about it.
Variance noise comes from variance variances on a d,

132
00:08:50.900 --> 00:08:52.610
an additive quantity.

133
00:08:53.360 --> 00:08:57.240
So this means that if I have to Monte Carlo estimators have given variance and

134
00:08:57.241 --> 00:09:00.720
if I would just add them together and average d samples,

135
00:09:02.890 --> 00:09:05.380
then I would also average the error of the two.

136
00:09:05.920 --> 00:09:08.990
And that that doesn't give me that,

137
00:09:09.050 --> 00:09:12.910
that doesn't give me a great result because there are some regions that are

138
00:09:12.911 --> 00:09:17.470
sampled by light tracing well and there are regions that are sampled by part

139
00:09:17.471 --> 00:09:18.280
racing well.

140
00:09:18.280 --> 00:09:23.280
And I cannot just cut out the good parts from each sampling technique because

141
00:09:23.291 --> 00:09:28.270
the error would be averaged.
And this can be solved in a meaningful way,

142
00:09:30.640 --> 00:09:34.210
in a way that is actually proven to be optimal in some sense.

143
00:09:34.211 --> 00:09:38.050
And this technique is called multiple important sampling.
Now,

144
00:09:38.110 --> 00:09:40.330
multiple important sampling was

145
00:09:41.910 --> 00:09:46.910
brought to us by a person called Eric reach in his landmark thesis of beautiful,

146
00:09:49.681 --> 00:09:53.970
beautiful works by direction of path facing is one of them and he,

147
00:09:53.971 --> 00:09:56.640
if I remember correctly last year,

148
00:09:56.670 --> 00:09:59.490
he got an academy award for his work.

149
00:10:00.000 --> 00:10:02.190
This is basically gone.

150
00:10:02.610 --> 00:10:06.660
This is basically the Technical Oscar award,
if you will,

151
00:10:06.990 --> 00:10:11.200
and in his acceptance speech,
it was really funny because he,

152
00:10:11.210 --> 00:10:15.930
he has a daughter and his daughter had taken a look at his thesis which is

153
00:10:15.931 --> 00:10:17.380
hundreds of pages of,

154
00:10:17.420 --> 00:10:21.420
of heavy integral calculus and and she asked that,

155
00:10:21.440 --> 00:10:22.500
that that daddy do,

156
00:10:22.530 --> 00:10:27.530
do people actually read this huge tome of knowledge and he finally can say yes,

157
00:10:29.220 --> 00:10:32.700
people actually do read that.
We read it like the Holy Bible,

158
00:10:32.880 --> 00:10:37.880
multiple important sampling is among one of his discoveries and it is,

159
00:10:38.970 --> 00:10:42.390
maybe
it's a bit subjective,

160
00:10:42.391 --> 00:10:46.050
maybe the most powerful technique in there in all rendering.

161
00:10:46.051 --> 00:10:49.140
And I will show you plenty of examples to convince you that this is so,

162
00:10:51.000 --> 00:10:55.230
so on the left,
let's,
let's forget about the middle example for now.

163
00:10:55.380 --> 00:10:58.560
Let's just compare the left and the right.

164
00:10:58.830 --> 00:11:03.690
You can see that there are many artifacts and many of these fireflies that can

165
00:11:03.691 --> 00:11:05.580
be suppressed by this technique.

166
00:11:05.850 --> 00:11:10.850
So I can't unify multiple sampling techniques in a way that wherever they do

167
00:11:12.810 --> 00:11:13.680
really bad,

168
00:11:13.710 --> 00:11:18.330
I can just forget that and I would take only the best samples for each region.

169
00:11:20.110 --> 00:11:24.640
Let's take another look,
which is maybe even better.
This is called at least,

170
00:11:24.641 --> 00:11:26.530
this is what we call a Veech pyramid.

171
00:11:27.010 --> 00:11:31.930
This is created with bi directional path tracing and the code below each image

172
00:11:32.170 --> 00:11:37.170
means that we have taken a different number of steps from the light source and

173
00:11:37.301 --> 00:11:38.134
from the eye.

174
00:11:38.590 --> 00:11:43.510
So in every image you'll see one give a number of bounces.

175
00:11:43.511 --> 00:11:45.250
So if you would have path tracing,

176
00:11:45.430 --> 00:11:48.700
you would get like 10 or something images and not in a pyramid.

177
00:11:49.180 --> 00:11:52.810
One image would be the first bounce,
second image would be the second,

178
00:11:53.290 --> 00:11:56.310
third in will be the third bus for by directional pathway.

179
00:11:56.420 --> 00:12:01.090
Since you have a pyramid like that because you sub divide them to the first

180
00:12:01.091 --> 00:12:06.091
bounce from the I n d some bounce from the light source.

181
00:12:07.150 --> 00:12:09.520
So this is now the two dimensional thing

182
00:12:11.170 --> 00:12:16.170
and you can see that some of the effects are captured really well in some of

183
00:12:16.451 --> 00:12:21.310
these images.
And there are some other images which are absolutely,

184
00:12:21.970 --> 00:12:25.090
absolutely terrible and really noisy.
So for instance,

185
00:12:25.091 --> 00:12:29.620
if you take a look at the two sites,
these two sites mean that

186
00:12:31.450 --> 00:12:36.010
I am hitting either the camera or the light source by accident.

187
00:12:37.330 --> 00:12:37.650
<v 0>Okay?</v>

188
00:12:37.650 --> 00:12:41.790
<v 1>And if you have a small light source,
which we actually do,
look here,</v>

189
00:12:42.600 --> 00:12:46.200
this is a relatively low probability event.
And if there,

190
00:12:46.440 --> 00:12:50.220
and if this is a low probability event and most of my samples are going to be

191
00:12:50.221 --> 00:12:54.500
wasted and I'm going to be have a noisy image not to well converged image.

192
00:12:54.810 --> 00:12:59.810
So on the sides I have really low probability events and these are samples that

193
00:13:01.591 --> 00:13:02.940
are really don't want to use,

194
00:13:02.970 --> 00:13:06.630
imagined that I would add all of these images together.
Average time,

195
00:13:06.900 --> 00:13:09.840
I would have plenty of noise from the noisy ones.

196
00:13:10.350 --> 00:13:15.350
Now what if I could say that if you take a look at s equals one t equals five,

197
00:13:16.890 --> 00:13:19.050
you can see that we have caustics in there.

198
00:13:20.390 --> 00:13:24.710
And the costings is almost almost immediately converged in there.

199
00:13:24.770 --> 00:13:29.770
It is definitely good in a sense that I would for caustics I definitely would

200
00:13:30.291 --> 00:13:35.291
want to use these samples and not the ones for instance e l s equals zero equals

201
00:13:36.170 --> 00:13:38.780
six because there's also cost x but it's really noisy.

202
00:13:39.260 --> 00:13:42.260
It does not systematically looking for cost x.

203
00:13:42.290 --> 00:13:46.850
It just happened to hit it but it it's not good at sampling it and I don't want

204
00:13:46.851 --> 00:13:49.130
to leverage these guys together.
What I want to,

205
00:13:49.190 --> 00:13:54.020
what I would want to do is I would want to give a large weight to Assick was one

206
00:13:54.080 --> 00:13:59.080
team was five on caustics and I would just grab it in there in my image and I

207
00:13:59.271 --> 00:14:01.730
would just forget about the other contributions.

208
00:14:02.270 --> 00:14:07.270
And this is mathematically doing this in a mathematically sound way is not easy,

209
00:14:07.970 --> 00:14:12.970
but Eric has proven really good and super simple technique on how to do that and

210
00:14:14.961 --> 00:14:19.280
now look closer to the image.
This is without naive,
by direction,

211
00:14:19.281 --> 00:14:22.040
apart Resi,
without multiple important sampling.

212
00:14:22.130 --> 00:14:26.220
And now what you will see is if we add multiple important sampling.

213
00:14:26.240 --> 00:14:31.030
So look closely,
see the difference.

214
00:14:31.360 --> 00:14:36.360
There are many noisy images that were completely shutdown because they were not

215
00:14:37.181 --> 00:14:41.860
really good at sampling different parts of the space of light paths.

216
00:14:42.220 --> 00:14:45.220
Some images are not good at anything at all.

217
00:14:45.970 --> 00:14:50.950
Take a look at the two sites and there are images where I can take caustics

218
00:14:50.960 --> 00:14:55.070
from.
For instance,
like the s equals five t equals one.

219
00:14:55.220 --> 00:15:00.220
It seems to have been even better at sampling costs x because this ESIC was one

220
00:15:00.681 --> 00:15:02.540
teak was fine,
was also pretty good,

221
00:15:02.930 --> 00:15:06.020
but it was shut down by the other technique that was even better.

222
00:15:06.740 --> 00:15:11.740
So this is an amazingly powerful technique in order to create even even more

223
00:15:12.771 --> 00:15:17.120
converged images.
If you have multiple sampling strategies.

224
00:15:18.290 --> 00:15:21.980
Now you can also play with it.
It is implemented on shader toy,

225
00:15:22.180 --> 00:15:27.180
the nice classical Veech scene where there is lights or sampling and BSDF BRDF

226
00:15:28.671 --> 00:15:29.504
sampling.

227
00:15:29.630 --> 00:15:34.040
And it doesn't matter if you say yes the FRB RDF in this case by the way,

228
00:15:34.340 --> 00:15:39.340
but your reminder so you can play with it and I encourage you to do so.

229
00:15:40.040 --> 00:15:45.040
It is lots of fun and you will see what kind of light transport situations are

230
00:15:46.760 --> 00:15:50.750
captured well with which sampling technique and how to unify them in a way that

231
00:15:50.751 --> 00:15:52.880
everything looks converged almost immediately.

232
00:15:54.200 --> 00:15:56.960
And also what does a good engineer do?
Well,

233
00:15:58.050 --> 00:16:00.980
a good engineer obviously is interested in the problem.

234
00:16:00.981 --> 00:16:05.981
So I just sat down and also implemented the same thing in a simple example in

235
00:16:07.521 --> 00:16:12.521
one day to make sure that everyone really understands what is going on.

236
00:16:12.860 --> 00:16:17.860
So this is a simple Monte Carlo sampling problem in one day I have a function

237
00:16:19.371 --> 00:16:21.800
that I would want to integrate.
If I remember correctly,

238
00:16:21.870 --> 00:16:26.660
I'm integrating the Garcia and I would like to sample it with two different

239
00:16:26.661 --> 00:16:27.494
techniques.

240
00:16:32.050 --> 00:16:36.550
So this is two different Monte Carlo sampling processes and I would want to take

241
00:16:36.551 --> 00:16:40.450
only the best samples in order to get an approximation,

242
00:16:40.451 --> 00:16:41.770
which has the least variance.

243
00:16:42.070 --> 00:16:45.340
And there are multiple ways of combining them together.

244
00:16:45.341 --> 00:16:49.180
And there's also naive averaging,
which just average is the error.

245
00:16:49.360 --> 00:16:52.870
So it would give you back all of these images from the side

246
00:16:54.580 --> 00:16:59.080
and I write out what are the exact Monte Carlo estimators,
four different,

247
00:16:59.350 --> 00:17:03.180
multiple important sampling estimators as well.
So take a look,

248
00:17:03.200 --> 00:17:07.840
it is now part of small paint and you can run it super simple and hopefully

249
00:17:07.841 --> 00:17:11.410
super understandable.
I think it is less than a hundred lines of code.

250
00:17:13.530 --> 00:17:14.040
<v 0>Okay.</v>

251
00:17:14.040 --> 00:17:17.130
<v 1>Okay.
So what do we now know by directional path racing?</v>

252
00:17:17.490 --> 00:17:20.280
Definitely better convergence,
convergence,
speed,

253
00:17:20.310 --> 00:17:24.330
especially in scenes where you are not that likely to heat light sources.

254
00:17:24.870 --> 00:17:29.790
So especially in indoors scenes and you will also get quicker convergence for

255
00:17:29.791 --> 00:17:33.600
costs x because you will have sampling strategies that are very efficient in

256
00:17:33.601 --> 00:17:34.050
that.

257
00:17:34.050 --> 00:17:39.050
So plastics are usually visible from light sources and you will sample them very

258
00:17:39.391 --> 00:17:43.320
often.
So there's going to be at least what estimator that captures it well.

259
00:17:43.410 --> 00:17:46.470
So if you use M I s multiple important sampling,

260
00:17:46.650 --> 00:17:49.050
you're going to have caustics covered very

261
00:17:51.420 --> 00:17:56.420
now it is definitely not easy to grasp and it is definitely not easy to

262
00:17:56.760 --> 00:17:59.850
implement.
So it requires quite a bit of an effort,

263
00:18:00.540 --> 00:18:04.770
even if it sounds very intuitive.
It is,
but it is not easy.

264
00:18:07.880 --> 00:18:12.590
This is also a brute force method.
This also samples all possible light sources,

265
00:18:12.770 --> 00:18:15.470
and therefore this is also unbiased and consistent.

266
00:18:16.900 --> 00:18:21.370
Some more literature on by direction of path tracing and even better.

267
00:18:21.820 --> 00:18:26.680
There is a nice comparison coded up also on shader toy.
So when you at home,

268
00:18:26.681 --> 00:18:31.390
just fire it up and you will see the difference evolving in real time on your

269
00:18:31.391 --> 00:18:33.460
GPU,
on an indoor scene.

