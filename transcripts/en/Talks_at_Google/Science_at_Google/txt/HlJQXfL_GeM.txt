Speaker 1:          00:00:06       Yeah.

Speaker 2:          00:00:10       So I'll thank you for that lovely in production and uh, uh, and thank you for um, sharing lunch, sharing your lunch hour with me. Um, so I think what I'm going to do is kind of jump into the talk and if there are procedural questions you have or questions about methods, I'm happy to take those questions now. Otherwise we'll just wait till the end.

Speaker 2:          00:00:37       So lots of tech companies are trying to figure out how to detect emotion by reading facial expressions. It's a really exciting time because the technology is a developing really quickly advancing really fast. Um, and in fact the pace is even seems to me anyways, kind of speeding up and there was a growing economy of um, emotion reading gadgets and apps and algorithms. But the question I want to start today with is, can we really expect a machine to read emotion in a face? There are plenty of companies who are claiming to have already done it and their claims are based on some fundamental assumptions that we're going to systematically examined today.

Speaker 2:          00:01:27       And I'll just warn you, I'm going to maybe suggest some things that some people might find a little provocative and might challenge your deeply held beliefs. Um, because the message I'm going to suggest today is that machines, it's not the case that machines can't perceive emotion, but that company's currently seem to be going about this question in the wrong way because they fundamentally misunderstand the nature of emotion. And as a consequence, they're missing what, uh, I would think of as a game changing opportunity to really transform the science and its application to everyday problems. So emotion reading technology usually starts with the assumption that people are supposed to smile when they're happy, frown when they're sad, scowl, when they're angry and so on. And then everyone around the world should be able to recognize smiles and frowns and scowls as, uh, expressions of emotion. And it's this assumption that leads companies to claim that detecting a smile with computer vision algorithms is equivalent to detecting an emotion like joy.

Speaker 2:          00:02:38       But I want you to consider this evidence. This, um, here on the x axis are the presumed, uh, expressions for, um, the various emotions. For anger, disgust, fear, happiness, sadness, and surprise. And now we're going to look at some evidence from Meta analyses, statistical summaries of experiments to answer the question of how often people actually make these faces, um, during emotion. And the answer is not so much the y axis represents the proportion of times that people actually make these facial expressions during actual emotional events. So in real life, for example, um, people only make a wide eyed gasping face during an episode of fear 9% of the time across 16 different studies. And in fact, that face, if you were in pop when a Guinea in the Troy brand islands would be considered an anger face. It's a threat face. It's a face that you make to threaten someone.

Speaker 2:          00:03:50       So in real life, people are moving their faces in a variety of ways to express a given emotion. They might scowl and anger, um, about, uh, 30% of the time. Um, but they might cry in anger. They might have a stone face stare and anger. They might even smile in anger. And conversely, people often make these faces when they're not emotion at all. For example, people often scowl a, a full on facial scowl when they're just concentrating really hard. Nevertheless, there are hundreds of studies where test subjects are shown posed expressions like these. So post faces and then they're asked to identify the emotion being portrayed. And again, the proportions are on the y axis. And so you can see there's quite a difference, right? Even though people only make a wide eyed gasping pay face about 9% of the time, 68% of the time. Um, test subjects identify that as a fear expression and so on and so forth.

Speaker 2:          00:04:56       So which data are the companies using to, um, to, uh, as the basis of their development? They're using the blue bars. So when software detects someone as scowling, they infer that the person is angry. And in fact, you'll hear, um, companies refer to a scowl as an anger expression, as if there's a one to one correspondence and frowning for sadness and so on. And so the question is, well, if people sometimes make these faces to express emotion, the d the presumed emotion, but often not. Why our test subjects as perceivers identifying emotions in these faces. So, um, so frequently. So why are the blue bar so much higher than the white bars? Um, and now I'm going to show you the answer. Here's the kind of experiment that is almost always used in the sorts of studies that generate the data for those blue bars. Test subjects are shown opposed

Speaker 2:          00:05:58       face like this. And then there I'm shown a small set of words and then they're asked to pick the word that matches the face. So which word matches this face? Good job. When test subjects choose the expected word from the list, it's called accuracy. Even though this person is not angry, in fact, she's just posing a face. And in most of the faces that are used in these experiments, subjects are just posing faces right there. So it's not really accuracy, it's more like how much did you agree with the experimenters expectations, but it's called accuracy. So that's what we're going to call it today too. So hundreds of studies show pretty high accuracy using this method. This is on average. So this is a Meta analytic average across hundreds of studies and, um, emotion perception, you know, feels as easy as reading words on a page because in fact, that's actually what's happening in these experiments.

Speaker 2:          00:06:58       Um, when you remove the words and you merely ask test subjects to freely label the faces, accuracy drops precipitously. And for some emotions like contempt and shame and embarrassment, uh, the rates actually dropped to chance levels, which is about 17% in most of these studies. And here's what happens when we add a little bit of diversity into the picture. So things get a little more interesting. So we tested a group of hunter gatherers in Tanzania called the Hadza. The Hud's, uh, um, have been hunting and gathering continuously as a culture since the pleistocene. They don't live in, um, you know, the same exact same circumstances as ancient humans, but they are living, they are, you know, hunting and gathering, uh, on the African Savanna. So they are living a lifestyle that is similar to the conditions that some psychologists, evolutionary psychologists believe, gave rise to these universal expressions.

Speaker 2:          00:08:05       So there a great population to, um, to test. And there actually aren't that many of them left. It's actually really hard to get access to these, um, to this group of individuals. You have to have special research permits and so on. Um, so we were able to, with the help of an anthropologist who we collaborated with get access to the Hadza and who were very generous with their time, um, and um, you know, labeled some faces for us. So we showed them a set of faces and we ask them to do exactly what we ask other test subjects to do. Uh, and um, accuracy actually dropped even further. And this number is actually a little high because what the huddle we're very good at doing was distinguishing a smile from all the other faces, which were depicting negative emotions. So when you just look at the, um, accuracy for, for labeling scalpels and pouts and things like that, just the negative depictions of negative emotions, the rate dropped even further, pretty much to chance levels.

Speaker 2:          00:09:14       And so this is what happens when you remove the secret ingredient from these experiments. Um, the evidence for universal emotional expressions vanishes. No, I'm not saying that that means that faces carry no information or that we can't look at a face and, and uh, make a reasonable guests about how someone feels. But what I am telling you is that human brains are doing more than just looking at a face when they make such judgments. That is right now when you're looking at me or when I'm looking at you, some of you were smiling and nodding. Thank you very much. Others are, you know, maybe looking a little more skeptical or at least that's the guests that my brain is making. My brain isn't just using your face. There's a whole context around us. Um, but in these experiments were just looking, the experimenters were looking only for the signal value in the face alone, stripped away of all context except the context that they, unbeknownst to them actually had provided to the subjects, which are the words.

Speaker 2:          00:10:14       So to just to confirm that, um, that, you know, the experimental context was actually, uh, generating evidence making, making that it could make any emotion look universal. We decided to test this by going back to the original experimental method. And we identified six emotions from different cultures that have never been identified as universal, that can't be translated into English with a single word, which is important because all of the presumed universal emotions happen to be English categories and, um, they also don't exist. And the language that is spoken by the Hadza, uh, which is, um, heads on a, and then when we did is we invented expressions for these emotions. We just made them up. Uh, and in this case we were using vocalizations, although we have a version with faces, but we were using vocalizations because it's a complicated story, but we were basically replicating, uh, we're, we were replicating another experiment, um, and kind of criticizing it. Um, so we use vocalizations. So for example, the category Giggle is, uh, the overwhelming urge to squeeze or a pinch, something that's very cute. You know, when you see him cute and you just want to, you just want to squeeze the cheeks of a baby. Right? Um, that's the, that's the emotion. Um, and so we made up a vocalization to go with that, which sounds something like this.

Speaker 2:          00:11:54       Okay. We made that sound and then we asked our test subjects again from, uh, the Hadza test subjects to match each sound with a little story that we told about emotion because in remote samples, um, that are, um, you know, small scale, um, societies that are very remote from western cultures. Um, typically the way these experiments are done is you don't give them a list of words. You tell them a little story about the emotion that contains the emotion word. Uh, and then you give them two phases or two vocalizations and you ask them to basically pick the expression that matches. So that's what we did. Um, and then the average accuracy actually, um, was pretty high. And if you look at the individual emotions, five of the six of them look universal. Um, and in fact, these accuracy rates are pretty similar to what you see in many studies of Ang for fit, for anger, sadness, fear, and so on.

Speaker 2:          00:12:54       So this is where the blue bars come from. Scientists have been using really since the 1960s and experimental method that doesn't discover evidence for universal expressions of emotion, but it manufacturers that evidence this method of providing test subjects with linguistic cues is responsible for the scientific belief that a scowl expresses anger and only anger that a smile expresses happiness and only happiness and so on. And so if you're a company who wants to build AI to perceive emotions in humans by measuring their facial movements, then um, it's probably important to realize that these famous configurations don't actually consistently, um, display disgust, anger, and fear and so on. And that it's a mistake to infer that someone who has scowling is angry. And in fact, it's an, it's a mistake to call a scowl and anger expression because only sometimes, um, it doesn't out isn't a scowl indicative of anger.

Speaker 2:          00:14:07       Instead, what we see when we look at the data is that variation is the norm. And to show here, I'll just show you what I mean. So if you were looking at this person's face, um, how, how, how does she look to you? What, what emotion does she seem to be expressing? Sadness. She's sneezing, not even an emotion at all, smelling something good. So usually this isn't yet. Usually people see her as tired or, um, or as I'm grieving or as about to cry. I'm sad. Um, actually this is, uh, my daughter Sophia experiencing only what I can only describe to you is a profound and deep sense of pleasure, um, at the chocolate museum in Cologne, Germany. Um,

Speaker 1:          00:15:02       okay.

Speaker 2:          00:15:02       And this little sweetheart is also experiencing, uh, a profound sense of pleasure. And the lesson here is that people move their phases in many different ways during the same emotion. Now, if we were to only look at this little guy's eyebrows up to his, you know, eyes and nose, um, this, these facial actions actually are, um, very reminiscent of the presumed expression for anger. So for example, um,

Speaker 2:          00:15:43       this face is often seen as angry. Does anybody actually know who this is? Jim Webb, this is actually Jim Webb when he won the senatorial race in Virginia, which returned the Senate to democratic control, um, this victory return the Senate to democratic control. Sorry, I was just having a moment there. Uh, and so without context, we see his face as communicating anger because actually, um, this face symbolizes anger in our culture. So people don't just move their faces in different ways during the same emotion. They also moved, yeah, their faces, um, in the same way during different emotions.

Speaker 2:          00:16:27       So when in real life, a face doesn't speak for itself, when it comes to emotion, right? People usually see this guy a the fittest face as smug or pride or confidence. Um, actually it's at the supposedly universal expression for discussed. And what's really interesting is that when you stick a presume, you know, the presumed expression for discussed on a body or in any kind of context that I'm suggests a different emotion perceivers actually track the face differently. They're scanning of the face has a completely different pattern suggesting they're making different meaning of that face on the road by virtue of the, um, the context it's in. And I'll just tell you as an aside in every, well maybe that's an exaggeration in most studies, um, where you pit a face against a context, the face always loses faces are inherently ambiguous without contacts to make them meaningful.

Speaker 2:          00:17:29       So what's up with these expressions? Where did they come from? Well, it turns out that they were not discovered by actually observing people as they moved their faces, expressing emotions in real life. In fact, these are stipulated expressions. So a handful of scientists just anointed these as the expressions, emotion as universal truths, and then people built a whole science around. So basically their stereotypes. And um, what we have is a science of stereotypes or you know, emojis, which by themselves I should tell you also are highly ambiguous. It turns out, um, without context. Uh, so obviously we don't want to build a science, uh, of, uh, artificial intelligence on stereotypes. We want to build them on emotional episodes as they occur in, in real life. And in real life, an emotion is not an entity, right? It's a category that's filled with variety. When you're angry, your face does many things and your body does many things and it turns out your brain also does different things depending on the context that you're in.

Speaker 2:          00:18:42       Now, for those of you who build classification systems, you know about categories, right? So for example, if you were building a category, uh, if you're building a, a recognition system for cats, a cat recognition system, you would develop a classifier that could learn the features that cats have in common that distinguished them from other animals like dogs and birds and fish and so on. And this cat Aghori get it. My daughter made me say that. Okay. This category, thank you for laughing. Now I can tell her that you thought it was funny. Um, is a collection of instances that has similar features, but you know, there's actually plenty of variation in these instances of this category too, right? Some cats are big, some cats are small. Oh. Some cats have, um, you know, cats have different eye colors. Some cats have long first, some have short for some have no fur.

Speaker 3:          00:19:42       MMM.

Speaker 2:          00:19:44       But the human brain tends to ignore this variation in favor of what cats have in common. And the interesting thing is that, um, humans also have the capacity to make other kinds of categories, categories where, um, there are no physical similarities where the category is not based on physical similarities of the instances. Um, and this is something we do all the time. For example, here's a category. This is a category that every, I'm sure everyone in this room knows.

Speaker 3:          00:20:18       Yeah.

Speaker 2:          00:20:19       You want to take a guess what it is,

Speaker 2:          00:20:22       human made objects. I suppose if you treat the elephant like a picture of an elephant, then that would, that would be true. Yeah. Okay. Well these are all objects that you can't bring through airport security. Actually, the last time I did this one clever person actually said they're all, um, instances of things that you can squirt water of. And I thought, well, actually yeah, if you think of the gun as a water pistol then that that could work. Right. Um, this is a category that's not made of instances that chair physical features. Um, instead they share a common function. In this case, squirting water through them or not being able to take some through airport security. Um, uh, this category though exist inside our heads and in the head of every adult who has ever flown on an airplane. Um, it's a category of social reality. So for objects to belong to this category, um, they, they belong not because they all share the same physical features, but because we impose a similar function on them by collective agreement, we've all agreed that it is not okay to take water, uh, through the water bottle through security or a gun or a, we're an elephant.

Speaker 2:          00:21:47       And in fact, it turns out that most of the categories that we deal with in civilization are categories of social reality, who's instances don't necessarily share physical features, but we've imposed the same function on those features by collective agreement. Can you, can you think of any that might come to mind? Things that we treat as similar, but um, but are actually, their physical features actually vary quite a bit.

Speaker 2:          00:22:19       Money. Exactly. Money is the money is a great example. So throughout the course of human history and actually even right now, um, there's nothing about what humans have used as currency that defines those instances as currency. It's just a group of people decide that something can be traded for material goods and so they can, um, and you know, little pieces of paper, pieces of plastic shells, salt, big rocks in the ocean, which are immovable, um, mortgages. And when we remove our collective agreement, those things lose their value, right? So one way of thinking about the mortgage bubble is that, uh, mortgages, the value of mortgages is based on collective agreement and, uh, some people removed their agreement.

Speaker 4:          00:23:10       Anything else? Yeah,

Speaker 2:          00:23:15       that's true. You have to work really hard to, uh, accept the collective agreement, uh, of driving on the wrong side of the road.

Speaker 4:          00:23:25       Oh, come on. Beauty. How about

Speaker 2:          00:23:29       citizenship of a country? How about a country?

Speaker 2:          00:23:36       Right? If you go for example, into, um, if you look at a map from a 1940s or before the 1940s, the map of the world looks very different. The map of the earth is pretty much the same. The physical features of the earth are more or less the same. Um, but, uh, but the countries are, that are drawn, are, are different. Um, so we could go on and on like this. We can talk about, um, social roles, like being married. Marriage actually turns out is also a category of social reality. Um, the presidency of any country is, uh, you know, people don't have power because they're endowed by nature, with power. They have power because we all agree that certain positions give you power. And if we revoked our agreement, then they wouldn't have power anymore. Um, that's called the revolution. So, um, the emotion categories are our categories like this, anger and sadness and fear and so on. Um, our, um,

Speaker 2:          00:24:45       categories that exist because, uh, by of collective agreement, just in the same way that we had to impose, um, a function on the elephant that, uh, that wasn't there before. Um, in order for it to be belong to this category, um, uh, we also, um, impose meaning on a downturn, mouth, a scowl. We impose meaning on that scowl as anger, right? So a scowl isn't inherently a meaningful as anger. In this culture. We've learned, um, to impose that meaning based on our shared knowledge of anger and in um, the Trobriand islands, they would impose a different meaning. Um, they post impose the meeting of um, sorry, I've, they impose a meeting on a different face for anger, for the stereotype of anger. It's a wide eyed, um, face, a wide eyed gasping face. And this is also what allows us to see other expressive movements as anger, right? So what we're doing is imposing meaning on a smile or on a stone face stare or on a cry as anger in a particular situation. It transforms, mirror physical movements into something much more meaningful, which allows us to predict what's going to happen next. So if you want a machine to perceive emotions in a human,

Speaker 2:          00:26:14       then it has to learn to construct categories on the fly. Perceiving emotions is not a clustering problem. It's a category construction problem and it's a category construction problem. Whether you're measuring facial movements or bodily movements or the acoustics of someone's voice, or whether you're measuring the changes in their autonomic nervous system or even in the neural activity of the brain or even all of those, right? All of these things are physical changes that aren't inherently meaningful as um, emotions. Someone or something has to impose meaning on them, um, to make them meaningful, right? So an increase in heart rate is not inherently fear, but it can become fear when it is, um, it's pressed into service to serve a particular function in a particular situation. So emotions are not built into your brain from birth. They are just built as as you need them. And this is really hard to understand intuitively since you know, your brain categorizes very automatically and very effortlessly without your awareness. And so we need special examples to kind of reveal to us what our brains are doing, um, kind of categorizing very continuously, um, and effortlessly. And so, um, what I'd like you to do right now is we're going to go through one of these examples so I can, um, I can explain it to you. So

Speaker 2:          00:27:52       here's a bunch of black and white blobs. Um, tell me what you see. Sorry, person, person kicking a soccer ball.

Speaker 4:          00:28:04       Yeah,

Speaker 2:          00:28:05       an octopus. One I'd octopus.

Speaker 2:          00:28:14       Okay. So, um, right now what's happening in each of your brains is that billions of your neurons are working together to try to make sense of this so that you see something other than black and white blobs. And what your brain is actually doing is it's searching through a lifetime of past experience, issuing thousands of guesses at the same time, weighing the probabilities, trying to answer the question, what is this like? Not what is this, but what is this like, how similar is this to um, past experiences? And this is all happening in the blink of an eye.

Speaker 4:          00:28:56       Okay?

Speaker 2:          00:28:57       Now, if you are seeing merely black and white blobs, then your brain hasn't found a good match and you're in a state that scientists call experiential blindness. So now I'm going to cure you of your experiential blindness. This is always my favorite part of any talk. Should I do that again?

Speaker 4:          00:29:24       Yeah.

Speaker 2:          00:29:25       Now many of you see a B and the reason why is that now as your brain is searching through past experiences, there's new knowledge there from the coat color photograph that you just saw. And the really cool thing is that what you just saw a moment two ago, um, that knowledge is actually changing how you experience these blobs right now. So your brain is now categorizing this visual input as a member of the category B. And as a result, your brain is filling in lines where there are no lines. It's actually changing the firing of its own neurons so that you see a B where there is actually no be present. This kind of category induced hallucination is pretty much business as usual for your brain. This is just how your brain works and your brain also constructs emotions in exactly this way. And here's why.

Speaker 2:          00:30:22       Here's why it happens. Because your brain is actually in tuned in a dark silent box called your skull. And it has to learn what is going on around in the world by scraps of information that it gets through the sensory channels of the body. Now the brain is trying to figure out the causes of these sensations so that it understands what they mean and it knows what to do about them to keep you alive and well. And the problem is that the sensory information from the world is noisy, ambiguous. It's often incomplete. Like we saw with the blobby B example. And any given sensory input like a flash of light can have many different causes. So your brain is, has this dilemma and it doesn't just have this dilemma based on sensory inputs from the world. It also has this dilemma to solve, um, uh, regarding the sensory inputs from your body.

Speaker 2:          00:31:25       Um, so, uh, there are sensations that come from your body, like your lungs expanding and contracting and your heart beating. And, um, there are sensations from moving your muscles and from metabolizing glucose and so on and so forth. And, um, the, um, the same, uh, kind of problem that we faced with having to make sense of information from the world. We also face from having to make sense of our own bodies, which are largely a mystery to the brain or more or less. Um, so an ache in your gut for example, um, could be experienced as hunger if you were sitting at a dinner table. But if you were in a doctor's office waiting for test results, you, that gut, that ache in your gut would be experienced as anxiety. And if you were a judge in a courtroom, that ache would be experienced as a gut feeling that the defendant, um, can't be trusted. So your brain is basically constantly trying to solve a reverse inference problem because it has to determine the causes of sensations when all it actually has access to or the effects. And so how does it do this? How does the brain resolve this, this reverse inference problem? And the answer is by remembering past experiences that are similar in some way.

Speaker 2:          00:32:53       So it's remembering past experiences where physical changes in the world and in the body are functionally similar to the present conditions.

Speaker 4:          00:33:06       Okay.

Speaker 2:          00:33:07       Similar, it's creating basically categories.

Speaker 2:          00:33:15       So your brain is using past experience to create ad hoc categories, to make sense of sensory inputs so that it knows what they are and what to do about them. And these categories represent the causal relationships between the events in the world and in the body and the consequences, which is what the brain actually detects. And this is actually how your brain is wired to work. It's wired to work this way. It's metabolically efficient to work this way. Um, and this is how your brain constructs all of your experiences and guides all of your actions. Your brain begins with the initial conditions in the body and in the world. And then it predicts forward in time predicting what's about to happen next by creating categories that are candidates to make sense of incoming sensory inputs to make them meaningful so that your brain knows what to do next.

Speaker 2:          00:34:15       And the information from the world and from the body either confirms those categories or it, um, it, uh, prompts the brain to um, to, to learn something and try again at updates. Um, and then the brain makes another attempt at categorization. So emotions are not, you know, reactions to the world. They are actually your constructions of the world. It's not like something happens in the world and then you react to it with an emotion. In fact, what's happening is that your brain is constructing an experience and an an episode or an event, um, where it's, what it's trying to do is make sense of or categorize what is going on inside your own body, like an ache in relation to what's happening in the world. Like being in a doctor's office. So emotions are basically, um, brain guesses that are forged by billions of neurons working together. And so the emotions that seem to happen to you are actually made by you.

Speaker 2:          00:35:27       And categorization is also how your brain allows you to see emotions in other people. So your brain remembers past experiences from similar situations to make meaning of the present. You know, to make meaning of the rays of an eyebrow or, um, the movement of the mouth and so on. So to perceive emotion in somebody else, what your brain is actually doing is it's categorizing that person's facial movements and their body movements and the acoustics of their voice and the surrounding context and actually stuff that's happening inside their own bodies, all conditioned on past experience. So even though when we're talking to each other, we're mainly looking at each other's faces and we're aware of each the movements of each other's faces and we might be maybe aware of the tone of voice. We're now, our attention is not given to the rest of the sensory array that the brain has available, including what's going on inside your own body. You know, your body inside your own body is a context that you carry around with you. Everywhere you go, that is involved in every single action and experience that your brain, um, creates. Largely, you are lord and you are largely unaware of it actually. And this is how, um, a scowl can become anger or confusion were, uh, indigestion or even amusement. So that emotions that you seem to detect and other people are partly made inside your own head.

Speaker 2:          00:37:05       So when one human perceives emotion and another person, she is not detecting emotion, her brain is guessing by creating categories for emotion in the moment. And this is how a single physical feature can take on different emotional meanings in different contexts. So for a machine to perceive emotion, it has to be trained on more than stereotypes. It actually has to capture the full high dimensional, the high dimensional detail of the context, um, not just measuring a face or a face and a body which is inherently ambiguous without the context. So perceiving emotion means learning to construct categories, um, using the features from biology, um, like faces and bodies and brains. But in a particular context. And I, the thing I want to point out here is that I'm using the context of the word context pretty liberally here because context also often includes the, the actions of other people, right?

Speaker 2:          00:38:06       So we are social animals and other humans make up important parts of our context, which suggests that when you want to measure a, when you want to, I'm sorry, detect emotion in a person you want to build AI and you measuring the context, you might consider also measuring the physical changes in the people who are around that person because that can give you a clue, um, about uh, about what the physical changes in the target person really need. So measuring the features of other people, um, that is there physical changes and actions that are contingent on the biological changes in the person of interest is uh, an extension of the con, the idea of, of um, context which is really important. And the last few minutes, what I'm going to do is switch gears here and I'm from perceiving emotion to ask whether it's possible to build machines who can actually experience emotion the way that humans do.

Speaker 2:          00:39:07       And this is a question I think that interest AI people who work in AI often because they are interested in, in questions about empathy. And so if emotions are made by categorizing sensations from the body, um, and from the surrounding context, using past experience than machines would need all three of these ingredients or something like them. And so we're gonna just take this really quickly, one at a time. So the first this past experience, can machines actually recall past experience? Well, machines are really great at storage and retrieval. Unfortunately, brains don't work like a file system. Memories aren't retrieved like files. They are. Memories are dynamically constructed in the moment. And brains have this amazing capacity to kind of combine bits and pieces of the past in novel ways. Their brains are generative. They are, um, information, uh, gaining structures. They can create new content, not just merely reinstate old content, which is necessary for constructing categories on the fly.

Speaker 2:          00:40:14       To my knowledge, and maybe you know, which might be out of day, but to my knowledge, there are no computing systems that are powered by dynamic categorization that can create abstract categories by grouping things together that are physically dissimilar. But because they, um, they are all in that particular situation serving a similar function. So an important challenge for computers, um, to experience emotion is to, to be able to develop a computing systems that have that capability. The second ingredient is context. So computers are getting better and better at sensing the world. Um, though there are advances in computer vision and um, in speech recognition and so on, um, but uh, system doesn't just have to detect information in the world. It also has to decide which information is relevant and which information is not. Right. This is the signal versus noise problem. Um, and uh, this is what scientists call value.

Speaker 2:          00:41:18       So value is not something that's detectable in the world. Value is not a property of, um, uh, sights and sounds and so on or the information that creates sights and sounds and so on from the world. Um, value is a function of that information in relation to the state of the organism or the system that's doing the sensing. So if there's a blurry shape in the distance, does it have value for you as food or can you ignore it? Well, partly that depends on what the shape is, but it also depends on when you last ate. And even more importantly, uh, the value also depends on whether or not that shape wants to eat you. And so to solve this problem, it turns out that, you know, um, the brain didn't start off Britain. If you look at brain evolution, it didn't start off with systems that allow creatures to compute value.

Speaker 2:          00:42:14       Those evolved in concert with sensory systems in concert with the ability to see and hear and so on. Um, for exactly this reason. Um, and so the evolution basically gave us brain circuitry that allows us to compute value, which also gives us our mood or what scientists call an effect, which are simple feelings of feeling pleasant, feeling unpleasant, feeling worked up, feeling calm, um, affect or mood is not emotion. It's just a quick summary of the state of what's going on inside your own body. Like a barometer and aspect is a signal that something is relevant to your body or not. Whether that thing has value to you or not. And so for a machine to experience emotion, it also needs something that allows it to estimate the value of things in the world in relation to a body, which brings us to the third ingredient.

Speaker 2:          00:43:12       Um, brains evolved for the purposes of controlling and balancing the systems of a body. Brains didn't evolve so that we could see really well or here really well or feel anything. They evolved to control the body, um, to, to keep, uh, the systems of the body in balance. And the bigger the body gets with the more systems, the bigger the brain gets. Um, so a disembodied brain has no bodily systems to balance. It has no bodily sensations to make sense of. It has no effect to a signal value. So a disembodied brain would not experience emotion, which means that for a machine to experience emotion like a human does, it needs a body or something like a body, a collection of systems that it has to keep in balance with sensations that it has to explain. And to me, I think this is the most surprising insight, um, about AI and emotion.

Speaker 2:          00:44:13       I'm not saying that a machine has to have an actual flesh and blood body to experience emotions, but I am suggesting that it needs something like a body. And, um, I have a deep belief that there are clever engineers who can come up with something that is enough like a body, um, to provide this necessary ingredient for emotion. Now these ideas, uh, and others, uh, the science behind them, um, and, and related ideas can be found in my book. How emotions are made, the secret life of the brain. Um, and there's also additional information on my website. And, uh, even though this is not strictly speaking, I'm not throwing tons of data at you. I do. I'm always at the end of talks like to thank my lab. Um, really are, they're the ones who actually do all the really hard work. Um, uh, you know, scientists like me just get to stand up here and talk about it. Um, so, um, uh, I just want to thank them as well and thank you for your attention and I'll take it

Speaker 1:          00:45:14       questions.

Speaker 5:          00:45:20       I am wondering how someone who has say blind from birth will perceive emotion because they don't, um, they cannot depend on visual cues, whether it's facial expression or body language. So I'm guessing they usually go off of, um, um, vocal tones or lack there off. Um, have you looked into their accuracy of predicting emotions and is that better or worse than people who rely on visual cues?

Speaker 2:          00:45:48       So, um, people who are born congenitally blind have no difficulty experiencing emotion and they have no difficulty perceiving emotion through the sensory channels that they have access to because their brains work largely in the same way that a sighted person's brain works at birth. The brain is collecting patterns, statistical patterns, um, and so, uh, it's just vision isn't part of that pattern. And what's really interesting actually is that, um, so for someone who is, is congenitally blind, um, they, um, they're learning patterns that include, you know, uh, changes in sound pressure that become hearing changes, um, in, um, the pressure on the skin, which becomes touch. Um, they have taste, they have sensations from the body which become an effect. So they can do multimodal learning just like the rest of us. And they can learn to experience and express emotion and perceive it through the channels they have access to. What's really interesting is that when adults have, um, let's say who are congenitally blind because they have cataracts, um, have those cataracts removed for the first time, they can see,

Speaker 1:          00:47:03       yeah.

Speaker 2:          00:47:04       Or they should be able to see. But actually it takes them a while to learn to see. And when they finally learned to see they, their experience, if you talk to these people, what they say is that they're, they feel like they're always guessing what faces mean and what body postures mean. They find faces in particular hard. Um, for example, even as I'm one, there's one person, um, Michael May, who's been studied really extensively over a number of years and even a couple of years, years after his cataracts were replaced, um, is corneal, uh, sorry, he had corneal abrasions that a, so as cornea were replaced, um, he was still guessing, consciously guessing at whether a face was male or female before someone spoke. He just couldn't, it was really hard for him to do. And he experienced his vision as separate from everything else, like us, like a second language that he was learning to, to speak, um, which had no effect to it.

Speaker 2:          00:48:07       Right. So, um, so, but the answer to your question is, um, so you critique it, ask a bunch of questions, like, so do blind people who are congenitally blind, do they actually make facial expressions, you know, the way that a sighted person does and the answer is, um, they, uh, their facial movement, they don't make the stereotypic expressions when they're angry or sad or whatever, but they do learn to make sur deliberate movements in a particular way. Um, for example, um, when they are, there are these studies showing that when, um, congenitally blind athletes win an award and they, uh, they know they're being filmed, um, they will make a body movements that indicate being really thrilled. Um, uh, what they don't, but they're doing it because they've learned it in the, in the same way that if you test a congenitally blind person on the meaning of color words, they're mapping of color words largely as the same as a sighted person because they've stuttered.

Speaker 2:          00:49:09       They've learned from the statistical regularities in language which words are more similar to each other and which ones aren't. So their abilities, that emotion perception and emotion expression largely look the same as a sighted person's in what without the, without the visual component was really interesting is that people who are congenitally deaf, who don't, who, who tend to learn mental state language, they develop concepts for mental states later also are delayed in their ability to perceive emotion in other people. So, um, that literature suggests a coupling between, um, emotion words and the ability to learn to form emotion categories in childhood.

Speaker 6:          00:49:55       So, so you said that an essential component in recognizing an emotion is the context. I would never say recognizing, but yeah, yeah. Um, um, if we didn't have the context and we could monitor whatever's happening inside a person's body and the brain really well, uh, would we be able to recognize emotions and what specifically would it take? What would we want to monitor?

Speaker 2:          00:50:24       Yeah, so it's interesting that you, I mean, when I was originally thinking about giving this talk, I thought I might start with, um, machine learning, uh, attempts to, um, identify emotion with neural activity, patterns of neural activity. And it turns out that you can, in a given study, uh, if you, um, if you show people films, say, uh, and you try to evoke emotions by showing them films, you can actually build a pattern classifier that can distinguish, um, uh, anger from sadness, from fear, um, meaning you can distinguish when people are watching films for that. Will that presumably evoke anger versus sadness versus fear? The problem with those studies is that, uh, that classify or can't be used in another study. Like it doesn't generalize, right? So you're what you're building is your building, um, uh, a set of classifiers at work in a specific context.

Speaker 2:          00:51:24       Um, but when you generalize, try to generalize to another sample of subjects maybe. Um, so let me say it this way. If you have the same subjects in the same study, watch movies, and so you've voke anger by watching a movie and you've voke anger by having them, let's say, remember a prior anger episode, um, you get, um, you can classify the emotions and distinguished from one another and you can, you get a little bit of carry over from one modality of evoking to another. But if you go to a completely separate study, the patterns look completely different. And this is true across hundreds of studies. So reasonable. We developed a, we have a, I published a pattern classification paper where we use 400 studies, um, uh, and we developed these classifiers based on this metta analytic database that, that those classifiers are not successful at classifying any new set of instances.

Speaker 2:          00:52:22       I mean, they show really good. Um, you know, I mean, we used a leave one out method. We used, uh, um, you know, multivariate Basie and a me, you know, there, there are no problems with the statistics. The issue is that, um, when scientists do this, they believe that what they're discovering, um, in these patterns is actually a literal brain state for the motion, the literal brain state for anger. And then they think it should generalize to sell to every brains to every instance of anger. And they don't generalize usually outside of their own studies. Um, this is also true for physiology. Um, where it, so we just published a Meta analysis where we examined the physiological changes in people's bodies, like their heart rate changes or breathing, um, their skin conductance and so on. And, um, you see that these physical measures can distinguish one emotion, sometimes one emotion category from another in a study, but they don't generalize across studies.

Speaker 2:          00:53:19       And in fact, the patterns themselves really changed from study to study. Um, and so there's, uh, when you look at it in a Meta analytic sense, it looks like all in all for all emotions, heart rates would go up or go down or stay the same depending on what the situation is. So there's so far no one has done a really high dimensional. Nobody's made a really high dimensional attempt at this. Meaning they haven't tried to measure the brain and measure the body and measure the face and measure aspects of the context. Um, that's actually what I think needs to be done. So I think this is a solvable problem. I just think we have not been going about it in the right way. Um, and I think that this is a real opportunity, uh, for, um,

Speaker 3:          00:54:04       okay

Speaker 2:          00:54:05       for any company that is serious about doing this.

Speaker 7:          00:54:08       I love the way you mentioned in the book that, and then the talk as well, how we perceive emotions based on context. So we look at the context and then we for emotion. And one of the examples that you have in the book and, and you have here as well was a Serena Williams winning a grand slam and you have her here, you have the versions.

Speaker 2:          00:54:29       Why I switched it out because people were starting to say, oh, that's Serena Williams. I'm like, okay, well that's right. Yeah.

Speaker 7:          00:54:35       So, but there's something that is troubling to me at least in that, in that example. Um,

Speaker 2:          00:54:41       well I think that's, that's certainly possible. And what I would say, what I would say to that though is, um, that um, there are studies particularly by a Hillel Fvs are, who's actually done work with you? No, he didn't. I published the picture of Serena Williams and I'm 2007. I published that example and um, hello, came out with a great set of experiments in 2008 and then again in 2012 and he proceeded to continue where he knows he has the reports of people, people's subjective experience and he has their facial movements. Um, and in fact there are Meta analyses which, uh, have the subjective reports of people and their facial movements and in sometimes also the reports of people interacting with the people whose faces, but, and there is um, no evidence

Speaker 3:          00:55:33       that, um, that the, that, that the

Speaker 2:          00:55:40       variability is due to a series of quick emotions being evoked over time. So what, you know, but I want to back up one step and say this when, when you asked the question, um, well maybe Serena Williams is really experiencing, maybe she really is in a state of anger in that moment or in that case it's actually looks like fear more or terror more.

Speaker 3:          00:56:03       Um, uh,

Speaker 2:          00:56:06       when you say really that implies that there's some objective criteria and that you could use to measure the state that Serena Williams has really in. And there is no objective criteria for any emotion that's ever been studied ever.

Speaker 3:          00:56:20       Okay.

Speaker 2:          00:56:20       So what scientists use is agreement. They use collective agreement essentially. So you can ask, does the face, does it face match her report? Does the face match somebody else's report due to people agree on what they see. So you're using all kinds of basically a perceiver based agreement, which is basically consensus, um, because no one has found the, there is no ground truth when it comes to emotion that anyone has ever found that replicates from study to study. Um, and so, so there's a part of me that wants to say, I can't even answer your question because I think it's not even a scientific question that's answerable, but, but we can answer it in other ways by see, by looking at various forms of consensus. And while I can't say anything about Serena Williams and what she experienced, I can say that in other studies it's very clear that people are scowling.

Speaker 2:          00:57:13       Absolutely. When they are not angry. My husband, this is my husband Dan Barrett who works for Google. Sorry honey, I'm going to out you. Um, you know, key Scott, he gives a full on facial scowl, uh, when he's concentrating really hard. And it was only after I learned that, that I, and I was telling my students actually like, can you believe? And they're like, can we believe it? We experience it every time we give a presentation in front of you. Right? So I'm sitting there are, you know, paying a lot of attention to every single thing they say and they think, oh my God, she hates it. And the whole, uh, you know, emotional climate and my lab changed the moment that I realized that. So, so that, you know, that's an anecdote, but it's an anecdote that reflects what is in the literature, which is that people are making a variety of fish.

Speaker 2:          00:58:03       I'm not saying it's random, I'm saying there's a pattern. There are patterns there that we haven't really yet detected. And I think it's in part because we are measuring individual signals or we think we're doing really well if we measure the face and the body or we measure the face and acoustics or measure the face and something about, you know, uh, maybe heart rate that we pick, we pick up two channels instead of doing something really high dimensional. I'm not saying there's no meaning there. If there was, if that were true, we, you know, you and I couldn't have a conversation right now I'm saying that it's probably something high dimensional and it might be quite idiographic meaning there could be different, different brains may be have the capacity to do um, a different number of categories to make different number of categories. And um, that's also something I discuss in my book actually.

Speaker 2:          00:58:55       So when you listed all the, uh, the sort of pre qualifications for maybe emotion forming, um, I was thinking, um, you know, a lot of vegetarians say, oh, you know, all animals have feeling, have this ability to, to emote and to feel emotion. And a lot of mediators are like, no, no, no, no, no, no, that's impossible. They don't. Um, do you have any opinion? Yeah, here's my opinion. I think that, um, I think everybody has to stop calling affect like many, many, many problems disappear. They just completely dissolve. When we understand that every waking moment of our lives, there's stuff going on inside our bodies and we can't, we don't have access to the small, every little small change in our bodies that gives that sense sensory information to the brain. If we did, we would never pay attention to anything outside our own skins ever again.

Speaker 2:          00:59:51       So instead, evolution has given us an effect. So we sense what's going on inside our own bodies by feeling pleasant or unpleasant feeling worked up or feeling kind of calm, feeling comfortable or uncomfortable. That's not emotion, that's an affect or mood that's with us always every waking moment of your life you have some effect, there are some aspect of features to your experience and it's very likely also true of of nonhuman animals. Um, I, you know, I can say this circuitry is, is, is very similar, similar enough that I think you could go down all the way, uh, to, um, certainly all vertebrates and I would even guess that there are some invertebrates, um, actually maybe all invertebrates, I don't know, even insects potentially could actually have an effect, although I think that's drawing. I mean I might draw the line at like flies or something, but, but recently there was a study that came out that suggested maybe they do have ethics.

Speaker 2:          01:00:49       So in my feeling about this is I guess two fold. One is I think we have to stop conflating an affect and emotion aspect is with you always, even when you, you experience yourself as being rational, even when you experience yourself as just thinking or just remembering, it's just that one aspect is super strong. Our brains explain it as emotion. Once we make that distinction and we understand that distinction, that emotions and an effect maybe effect, you could think of it as a feature of emotion. It's actually a feature of consciousness. Then I think we can say without hesitation, we don't know for sure whether nonhuman animals, uh, feel an effect. Um, but they probably do and we should probably treat them as if they do. Um, that that solves a lot of problems. It actually doesn't matter really. Um, from a moral standpoint, whether an animal feels emotion, it matters whether they can feel pleasure and pain.

Speaker 2:          01:01:49       That's enough. Actually, it's an interesting scientific question whether or not they can, they can see their brains can create emotion. That's a whole different conversation. But I think the answer to your question isn't really about emotion. It's about an effect. And I think there, it's really obvious that, um, if you're going to the smart thing to do, you just want to do things where you do the least amount of damage if you're wrong. Right. And so that means including animals in our moral, um, circle, right. They can feel, if you just assume they can feel pleasure and pain, that solves a lot of problems. Yep.

Speaker 8:          01:02:26       Thank you so much for your time. So you mentioned at the end, I guess, um, to answer the question, if machines can experience emotion, the three things and then body was one of them. And then earlier on, or you also mentioned, I guess one purpose to have to create emotions is like to know what to do next. So my question is if a being without a body, like a machine really need that body element. If the purpose of that being is different than just, you know, knowing what to do next, therefore can be tagged that body out of the one of the three requirements based on a different purpose of that beam.

Speaker 2:          01:03:06       That's a great question. That is a great question. So if the, so can you give me an example? It would be help me,

Speaker 8:          01:03:18       well, I don't have an example by the way, I'm thinking amendment when I went to meet and I hear machines. Yeah. And you're modeling all based on in humans and we have your body and then you had a purpose or if you have a rule. Yup. Yup. We have a purpose to have emotions. Like we are creating them maybe at the beginning for survival. Maybe it's different social elements that, yeah, but if you take the body of, you can still have a brain octane oil intelligence. We thought nobody, which is a different bean or elements, Darfur, I'm questioning that model of three things needed. Yep.

Speaker 2:          01:03:49       You know, here's the thing you need to have. Um, so you'd need to have something that could tell the machine what it needs to pay attention to in the world and what it can ignore. So value. Right. So, I mean, let me back up and say it this way. I mean, I don't know how else to think about it except in organic terms, right? But for example, if you were to look at brain evolution, if I were to say it really simply like super simplistically, so I'm just glossing over like a ton of detail. Organisms first developed a motor, a sort of a rudimentary motor system with just a tube for like a gut. That's it. They used to just float around in the sea and kind of like filter food. And it wasn't until the Cambrian explosion when there was a, like a lot of oxygen and other things.

Speaker 2:          01:04:45       And so a lot of, um, uh, you know, an explosion of diversity of life that predation developed and predation was a selection pressure for the development of two things. Sensory systems. So these little like floating tubes had no visual system. They had no rudimentary visual system where auditory system or they had not, no sensory systems, they didn't really need them. And they also had no internal, uh, uh, nervous system to control any, any systems inside because they didn't have any systems really inside except the motor system and a gut. And that was it. Um, so they had to develop sensory systems, whether they're a Predator or prey, you, most predators, also our PR, our pray, right so that they could stay, could sense they have to develop distance census so they could detect what was I going to, what was out there. Um, but they also had to figure out what was meaningful and what wasn't.

Speaker 2:          01:05:45       What was valuable because it's expensive to run a system and learn the two most expensive things that any other humans or any Oregon ganache system can do is move and learn. And so, um, it needs, and so the development of the systems of the body sort of serve, serve the purpose of helping to determine the value to the organism. Now, it turned out that, you know, along the way that those systems also developed Sensei, you know, develop the capacity to sensation sensations to the brain, which had to be made sense of if you completely demolished that and you say, okay, well you have a um, um, a machine that um, um, it's purpose isn't to sense things in the world and make sense of them then, um, so that it can predict what to do next. Then maybe you don't need a body but then you're not even talking about something that is, I don't even know. I mean you'd have to give me an example for me to kind of reason through it in terms of the energetics and the,

Speaker 8:          01:06:51       I wonder if maybe body is throwing me off because yeah, like a AI, it's purpose can be also survive to exist and can be an they just very simplistically he needed it needs electric city or its connection to the cloud or something. But that can be its body.

Speaker 2:          01:07:05       But what's its function? What does it do?

Speaker 8:          01:07:09       Yeah, what do you mean? Like what? Like,

Speaker 2:          01:07:11       it doesn't just, you plug it in and it doesn't just sit there, it does something. What's its function? What does it do?

Speaker 8:          01:07:16       Uh, do you mean dots? Full intelligence? What does it do well,

Speaker 2:          01:07:22       so you're saying, um, okay, it gets its energy from a plug. I, I get that. But what is it actually attempting to recognize or do or what's its function?

Speaker 8:          01:07:30       I mean we can use it for, I don't know if some industrial experience or maybe so southwest driver car AI for example. And then what I'm saying is, okay,

Speaker 2:          01:07:39       okay, so it's driving a car. It's driving a car for you. It has to sense things in the world.

Speaker 8:          01:07:43       Right. And then the question is Ken, Ken, it experience emotion and then your three model. I agree the two of two. And then I was questioning about the body and then maybe the body is, what is it reflecting body is, is survival to create that value they thing we're talking about the second one. Yeah. I would

Speaker 2:          01:07:59       say, here's what I would say. I would say, okay, well let's, let's take this as an example. I don't know. I'm just doing this off the top of my head, but let's take as example, so, so sure you can just plug it in and it can get its energy from an electrical outlet, but still you want to have an efficient machine that uses electricity efficiently. Otherwise that would be like more expensive than it needs to be. And so, um, that means that you'd want it to do things predictively because that's actually more energy efficient. That's why the human body doesn't care or actually any at all. Brains are structured to work efficiently, not because of the, the, you know, the, the energy source there is glucose and other, other, um, organic sources. So it's the same principle basically, in fact, electrical machines. In fact, the whole idea of predictive coding, which is what I'm talking about, comes from cybernetics.

Speaker 2:          01:08:48       Um, and then human research researchers who were studying humans were like, oh, wait a minute. That actually could be really useful for explaining things here. So you'd still want it to be super efficient. Presumably if it's driving a car, it has to determine what's, what it has to pay attention to. And what it doesn't, it can't be energy. It can't be frivolous and it's a and its energy use, right? So it's gotta be predictive and it has to basically not pay attention to some things and it probably has a bunch of systems that it has to keep in balance so that it's working efficiently. Um, that's so far counts as there's nothing in there that actually violates anything that I've said. I think I was, I was trying to be really careful to say, when I say a body, I don't literally mean a flesh and blood body. I mean one of your brain's basic jobs is to keep the systems of your body in balance and that requirement, which is called allostasis, that requirement forces a lot of other things to be true about how the system works. Um, so if you want, um, if you want an AI to do anything like a human, it has to be put under the same selection pressures as a human, not literally with flesh and blood. If, however, you're talking about a function that a human can't do or that isn't relevant to humans then any, Nah, nothing I've said is relevant to you, probably I at all right because we're only talking about about humans, but could a car he could of could of computer that drives a car feel emotion.

Speaker 2:          01:10:32       Maybe if

Speaker 4:          01:10:35       it

Speaker 2:          01:10:35       had sensory inputs that it had to make sense of. But the problem is that I don't know that I would call that emotion because for human we make a distinction between the brain makes a distinction between the sensations from the body and the sensations from the world. If I were, if there, if you didn't have sensations from your body, you, you wouldn't have an effect. And so it just wouldn't be the same. But I don't know. I mean, may I maybe I can't really answer but um, maybe, maybe

Speaker 9:          01:11:16       actually can I try to kind of offer one idea that my combining both. Yeah. So what if emotionally just kind of characteristic for how your body feels like you don't have enough computing power to process ever since you summarize it and machine in that regard? Would neither Sam curistics. If it's not allowed from then it would be a motion. So like either heuristics will tell like substance Bra, Ronan, oh my view was an off. That can be in a way seen as emotions and for us it would be like Samsung is all from me. I feel pain. You don't really know her pain is, but yeah, that's a signal food for deeper investigation. Right. And might be one of the causes is, I mean I don't believe our brain is a pinnacle of engineering at least and causes, no, no. Like I correct me if I'm wrong, but let's say it's a frequency of our neurons was like hundred hertz. A second. So it was a band is you really like limited and is only thing that gives us a life is that you kind of like candid billions and machine might not need that because the frequency was I calling it.

Speaker 2:          01:12:22       Right. But I think it's, maybe I'm wrong, but it means, I think it comes down to a philosophical question, like, okay, so, so a machine that's driving a car would have, since we'd have sensory inputs that it has to make sense of and it would have to do it predictably and all of that, but so it would have to have category, would have to do ad hoc categorization and it would have to, um, or maybe not. But I think that would probably be the efficient way to do it. So it's making categories and it's perceiving things, but so when does that become an emotion and when doesn't it, I mean, you could also ask that of um, uh, humans, right? I mean, we, I mean, you know, nobody asked me this question, but yeah. Like what is the difference between an emotion category in any other kind of category that emit human [inaudible] can develop, you know, any kind of, any other kind of category that is of this sort, which is ad hoc and, um, of social reality.

Speaker 2:          01:13:13       And the answer is nothing. Nothing is different. Um, so I, you know, in some ways it's a not a question I think that science can answer because in this culture we've drawn a boundary and we've said, well, these things are emotions and these things aren't there. They're wrapped their thoughts and in half the world people don't make that distinction. So is it possible to um, to develop category, you know, to do ad hoc categorization? Do you do it predictively to make sense of the world or sensations, sensory input from the world without a body? Sure, sure. You could do it without a body, but then it probably wouldn't be what we would call human emotion or what feels to us like human emotion, but, but of course, you know, it, it would be, it could be similar, I guess to the experiences that our brains can make. Um, but I, I don't know. I have to think about it more. Actually, my iPad is speaking to me.

Speaker 10:         01:14:12       Yeah. Thanks Lisa for a great, a great presentation. I had a followup question. Do we believe that if the human brain and consciousness, good process, all of the intraceptive signals everything from the world, all the persons in real time, so there's no bandwidth issue, supposedly human brain could just process everything. The first question is, do you believe we would still have an effect like this sort of simple state of where we are supposedly could just represent every piece of information coming. And the followup question is that depending on their answer to that is how, how, how that relates to our notion of emotional experience.

Speaker 2:          01:14:49       So in order though, for us, so for us to have high dimensional, in order for us to have, let me, let me think about this for a second. So if we could sense everything, all of the neurons, everything, every, so our wiring changed, right? Because we, the reason why we can't is that we don't have the wiring to do it, but would we have an effect that's sound? Everything was at the same time, I think. Yes. I think we still would. And I'll tell you why we would, because the way the brain is structured, it's structured to do dimension reduction and compression of information. So if you were to look at the cortex and you were to sort of take the cortex off the subcortical parts of the brain and just lifted off like a and stretch it out like a Napkin and you were to look at it in cross section, what you would see is that you go from primary sensory regions like primary visual cortex or primary and Tara Septic Cortex, which is where the information from the body goes to.

Speaker 2:          01:15:56       There are a lot of little pyramidal cells, um, with few connections, which, and the information cascades to the front of the brain where there are fewer cells, which are much bigger with many, many, many connections. But the brain is doing with all sensory inputs is it's doing compression, it's doing dimension reduction. Um, that's how multimodal learning hat, that's how really all learn learning happens essentially. Um, so I think it happens in vision. It happens with audition. And so I think even if we could have, even if we had higher dimensional access to what's the, the sensory changes in the body, I still think given the way that the cortex is structured, we would still experience, we would still have an effect, which basically effect is just a low dimensional representation of the stuff going on inside your body.