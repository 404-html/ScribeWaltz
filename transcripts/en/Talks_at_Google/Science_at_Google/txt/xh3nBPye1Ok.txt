Speaker 1:          00:05          I need you to speak before this audience. So, uh, I guess the last week there has been lots of talk about opinion polls. And a funny, surprisingly this time the exit polls were on the dot. I almost, so, uh, they kind of made up a little bit for the messaging, the big Zit to opinion polls. Well, uh, I said I've been doing this for over about two decades in India and I give talks on this and, uh, using the talks are sometimes on the television show as well. Uh, people say, how can talking to 1200 waters tell you anything about what happens to the whole population? Uh, so, uh, when did I picked up? That's the typical size. I saw all of the pools in UK in India use really have favorable 6,000 for a state election. And maybe you went 20,000 for a national election because you're a lot of diversity, but still, uh, then the population is, are the number of voters is also 700 million.

Speaker 1:          01:12          And, uh, this question still remains, how can talking to such a small proportion, uh, give you any insight into the whole, uh, population. Uh, in dooly, everybody thinks about the proportion sample size versus the population size and, uh, the least, one of the tasks of convincing you is that that's not important. Uh, that's one of the tasks. The other thing which I'll talk about, everybody has an opinion on that, but can open in polls conducted say a week before the actual pool accurately predict outcome we saw here, uh, the recent poll while the exit polls got it right, even a one week earlier, they're still seeing a comfortable victory for conservatives. So there are occasions when they can, and there are occasions when they will not. So we'll talk about that.

Speaker 1:          02:03          Of course. Uh, the whole thing is a, as a probability statistics background. And, uh, this slide a deliberately, I've kept it blank is it doesn't mean no, but Magnolia, Munich. But, uh, we'll do it live. What background is needed? So what I have with me, it's just a cardboard box and what it has our hundred slips of paper. Okay. I didn't tickle in all aspects and uh, each one has a letter written on it, either a or a B. So you trust me on that. And then I mixed them thoroughly and uh, come to the audience and someone whom I have never met before, I asked you to pick up one. You can pick from the top, bottom anywhere. Okay. All right. Can you read it out and eat? So you, uh, you believe that we have not fixed this a event and uh, one was drawn out of this box after and that one led an eight.

Speaker 1:          03:01          Now, uh, it's just based on this information. If you were to make a call, uh, oh, I forgot to say that there are 99 of them which have one letter and exactly one which is the outlier. Okay. So there we are only two possibilities. 99 A's and one B or 99 B's and one eight. Just based on this observation that it was a one was drawn out after mixing, etc. Etc. And that was a a, if you're to make a guess between the two possibilities, which is the likely possibility, 99, eight one B on 99 [inaudible] 99 eight. So a few people answered, others may have made up their mind but didn't answer. But uh, if one tries to go at the background as to what went in your mind when you pick this all alternative, of course the simplest explanation is just common sense. What is there.

Speaker 1:          03:54          Okay. Uh, people who have had some statistics background or profitability background may compute probabilities of the two scenarios and compare them even a sold on the base in technology, uh, philosophy, they would computer put a prior and computer posterior and say that the posterior probability of getting it, this, this right is 99%. So they'll go with the whichever one turned up is your answer. Right? So, uh, now let's say, let's do a thought experiment. I don't have another box, but let's do a thought experiment that instead of 99 one, we had 95, five. Okay. So one letter is 95, another letter is five and they are all mixed up. And let's say I go to one person, you drought, read it out, put it back, we shake it up again. I come to the second, I come to the third. This time you will have three observations.

Speaker 1:          04:53          Okay. And we can still go. They all could be same or we could have two of one and one of the other. That's why I didn't pick from one to two, but three, no ties. Okay? So we can talk about, go with the majority rules with three. Okay. And here is a slide which convince us that uh, the chances of in their certain loosens chances of getting it right or again, nine over 99%. So the first one is, uh, all three are the same or two. Our of the dominant one and one is the outlier. Okay? So that's the possibilities. And uh, so by repeating the draws, three draws, we have increased our precision in a loose sense. Precision earlier was 99%. Now if you'd draw once it is 95 by three draws, we have improved the precision to 90% and [inaudible]

Speaker 2:          06:00          uh,

Speaker 1:          06:02          does, you can still go with this kind of a rule. Go with the majority out of three. Now if the gap is lesser, we can experiment and come up with what number is needed to reach 99% precision. Okay. Uh, and uh, I'll display the numbers. What kind of a scaling up happens. Okay? But, uh, let's do another thought experiment. Instead of hundreds slips, they are a bigger box and a thousand slips.

Speaker 2:          06:35          Okay?

Speaker 1:          06:39          And once I get 95% with one letter and 50 of 55% other ones, a 95, you have one letter, 95% have one letter, one 5% has the other. So nine 50 and 50. So 2009 50 50 is the breakup. And this time, again, we want to get an accuracy level of 99%. So with hundred and 95, five we saw it was three draws. So how would it scale up? Do we need three draws your or do we need 30? So does it scale up like multiplier or does it scale up like square root as some various things in statistics scale up? What is the factor of Scallop is the question? The answer to this is rather counter intuitive. Just look at these two equations. First one was 400 flips with 95 five second one is 4,000 slips with nine 50 and 50. And of course the second one, there's just a factor of 10 in each, uh, factor in each term and they would cancel out.

Speaker 1:          07:50          So, uh, with even if we are thousand slips with nine 50, 50 distribution, if we draw a three and go with the majority are accuracy level is still over 99% precision is still owed 99% and more in need, it scales up. It could be anything. Instead of thousand it would have been 100,000, it could have been a million, whatever. If the distribution is 95% and 5% we draw three go with the majority. Precision is 99. So this is somewhat counterintuitive that it is the number of draws and not total how many slips. Okay. That besides the precision.

Speaker 2:          08:37          Uh,

Speaker 1:          08:40          so I just read an equation. We don't have to read that, but if you have a, if he'd died down, the w is the support for when I'm just saying a two party contest, one constituency. We don't even need to know the constitution, the size, uh, and to candidates w percent support for the winner and hundred minus w for the loser. Okay. Uh, we'd draw a odd number of draws and accuracy of the rule. Go with the winner in the sample is this expression interesting thing. This expression doesn't request population say August size of the constituency, number of voters in the constituency.

Speaker 3:          09:19          And

Speaker 1:          09:23          this is, this is what is at the back of people when the population size to somehow enter the picture when you want precision. That's why they say that. You know, why so many, just the swimming out of so many. How can you say anything?

Speaker 3:          09:37          Okay, so

Speaker 1:          09:40          that's it. [inaudible] now here is this a table which I've computed that especially on the previous slide I've just computed explicitly, uh, far. Uh, so the column header is number of draws a spelling mistake. So column header is the number of draws. Uh, and the, the role harder is the percentage of support for the winner. So 51 is like very touch and go very close election and 53 54 is a comfortable election. So when it's comfortable, you won a 54%, you went to how and then one, if, so this day equivalent of draw is going to waters and seeking their opinion. So, uh, if it's a comfortable victory margin, uh, then even work the thousand people and getting their opinion should give you an insight into what is likely to happen. Uh, 99.4% accuracy exactly the same logic as the experiment of drawing one sleep, nothing more, nothing less. In fact, we can you imagine that we draw a night out slips with all possibilities of thousand voters from that constitutes put them in a box, shake it up and take it out. We don't have to do it as a thought experiment. That's exactly what

Speaker 3:          11:04          it is. And

Speaker 1:          11:08          you went 52%, a 52 48, five zero one five should suffice to get good procedure.

Speaker 3:          11:17          Yeah.

Speaker 1:          11:18          So, uh,

Speaker 3:          11:23          okay.

Speaker 1:          11:24          So another way of putting it is a where did I'm willing to do a poll in UK or in India where India, that voters are at least 10 times more if not 10 times more, uh, for a given procedure. And I need the same sample face, something which is very difficult to sell it to media houses. I mean, we do it for media houses in India and we do it for the whole country as well as for individual states. And they would say half or the whole country. We had forward budget. Now this state is less than a 10th of the whole population. Our budgets slashed by 10% slash two 10%. And we have to deliver, no, no, no, no, no. I mean, of course they're not going to buy this match. So that's a tough task. But uh,

Speaker 1:          12:14          okay. Uh, one uh, one correction in a sense that, uh, while in this experiment I could take, when we are drying three, we'll roll one slip, we put it back, shake it up again, go to the second person, put it back, shake and so on. So this is v three placements. So once you're taking it out, you're replaced. It in fact is you are never going to do that. You can't be going to the same person thing. Oh your name up. You're in my sample. So you know I last you two times whether face to face or by phone. That's stupid. So, uh, in practice what is implemented is what is called sampling without replacement. So you take out one slip, keep it aside, and then go to the next and take it out of that. It's makes very little difference. If anything, he don't leave improves this table, this accuracy of prediction.

Speaker 1:          13:07          If you live without replacement, if anything, you don't leave Bruce. As we can see if the tutor population was only two five zero one and we do without replacement, then at 25, two five zero and we'll get the exact answer no matter what is the proportion. So, uh, if the sample is a much larger proportion of the population, then the numbers change but only upwards. And when the sample is like 1% or less than these other numbers, even in practice. So this is less than one challenges me that uh, you know, it does make a difference. The population size, it makes a difference in practice because we do without replacement. And even then, uh, when the samples collected is less than 1% of the population. This is the table.

Speaker 3:          13:55          Okay?

Speaker 1:          13:57          Okay. So the, uh, then is how do we select this? A 1001 or two, five zero one. Respondence. So what we do should correspond to in some sense, writing out all possible list of that size, putting it in a big box, shaking it up, and then drawing one. Okay. If you don't do that, we may get, we may not get the right answer. Okay? So for example, if you were to pick, say [inaudible] people last week we had to do something about London city. It's a lot, all of you, but how are the voters in London doing? Okay? If someone said, okay, I'll just come and stand at a Google office in pen, uh, fix Pierre's office and first thousand people who enter, I just get their opinion. Okay. Got a sample of 5,000, uh, would that, do you want to literally talk would tell you that that would do, I mean, uh, it's what would be a very, very biased sample of a certain social economic profile, age group and Econ, socioeconomic profile. Everybody has something to do with Google. Most of them would be employed and fuel could be visitors, but there's something to do with Google and that already tells you a certain socioeconomic class. And this would represent the opinion of the entire city of London.

Speaker 2:          15:18          So

Speaker 1:          15:20          how do we select these thousand people? Uh, is the crux of this whole methodology or technology. And part of the problem happens is that in colloquial terms, people equate the word random with arbitrary. And if you say, no, no, we don't do well. Uh, what is ram in our computer systems, random access memory, it is mistakenly call, it should actually recall arbitrary access memory. That was to differentiate it from linear access memory as in old magnetic tapes where you could only access it linearly. They came up with this new thing where you could address any, you could ask any address. So, uh, three days actually arbitrary access memory, but it's got used to be random access memory. Now of course you'll need to call it rain. We don't call it ran. So this is just an example to say that how these two things are confused in, uh, somehow, uh, gender public's mind that, so that's why the thing comes. It just get some thousand people. No, not some thousand people. Uh, so sometimes when I say this, others argue with me, what the first thousand people to enter Google could be on one of the list. If you make all possible lists, this would be one of the list. How can you say this is not random sample?

Speaker 1:          16:38          Well, the answer to that is a random or not is not a property of the group chosen is the property of the method by which you chose. If you're telling me this group, I can't say that. No, this is not random because I don't know how you have chosen it. If you're telling me how you chosen it, then I can make a world whether or not it's a randomly chosen subgroup

Speaker 2:          17:01          subset. So, uh,

Speaker 1:          17:05          so especially on television, a lot of, uh, fireworks with the, especially from the representatives of the parties when we are saying are going to lose, uh, are based on just this approach that either sample size and this and I ended up saying,

Speaker 3:          17:21          okay,

Speaker 1:          17:23          so, uh, that, uh, the point is random sampling is a must if you do it.

Speaker 2:          17:29          Uh, no,

Speaker 1:          17:33          so I call it, it's a must. If for the statistical guarantee that the sample based prediction is accurate,

Speaker 3:          17:39          okay.

Speaker 1:          17:41          The choice of the sample should have been chosen randomly

Speaker 2:          17:44          then, uh,

Speaker 1:          17:48          now one could, some shortcuts

Speaker 3:          17:50          are, one could make additional requirements. Uh, for example, uh, the, uh, if you know that there is a rural urban divide or if you know that there's a divide across economic strata, then, uh, if you have a master database where we can differentiate very, we have data about, uh, age groups, thought about, uh, uh, income levels, uh, education. So one can create subgroups and the desire that I want to recall stratas in statistics and for each strata, we can pick them randomly. The required number of, but at the, so we can have a smarter strategy is to get a better representation. But final stage, we have to have a random choice in a ceratin way, uh, done either just a random number. Do you have a list and you generate randomly numbers and you'll pick those. That's easiest thing to do. Now we don't have to make slips and put them in a box. And so on so forth.

Speaker 1:          18:50          The, uh, implementing it, uh, varies from place to place. Uh, what is possible, what is not possible. So, uh, you, you can understand and probably, and us also, I, it typically these days it is done through a telephone survey. You randomly generate telephone numbers and, uh, you call up people and get their opinion.

Speaker 3:          19:16          Okay.

Speaker 1:          19:17          Now, uh, the sip, this methodology is fine. Now in the Western world, even in the Western world, going back a few decades, this was not fine because the penetration of telephones was not quite universal.

Speaker 3:          19:30          Okay?

Speaker 1:          19:32          In India, uh, is this still not okay? Because a, the penetration in the lower sections of the society, socioeconomic classes is still rather poor. So anything done from telephone surveys is good. Not going to represent the entire population. Uh,

Speaker 3:          19:51          yeah.

Speaker 1:          19:52          Now,

Speaker 1:          19:54          especially here, people could say that, why go through this thing of calling up people and so on, so forth. People are voluntarily pumping away at a fast pace on social media. Just pick that data and uh, use that after all advertisers have gone away from, uh, doing market research in the traditional way of sampling, but added going with the social media buzz. Why not elections is a very valid question. Uh, the answer to that is for elections, it won't do wildfire advertising for mo several products. It does work because if you're trying to address a market and that market is consist of people who are active on social media, uh, let's say somebody that advertising a foreign company that advertising a very good deal for wifi connectivity, white on the move, on the mobile, only targeting the segment. So for them it's enough to get the social media buzz.

Speaker 1:          20:52          But in a election scenario ago, uh, the day that differential usage across socioeconomic economic classes and uh, if one goes by social media, uh, one is likely to make errors. So we had three years ago in one of the high profile elections in India, yet looked at uh, opinion poll based thing as well as the social media buzz. We got to pick the winner correctly. You based on social media buzz and a, the ultimate answer was a hands down victory. What we would have had a much bigger victory margin if you had gone with social media. There was a buzz on one side. So the uh, made me one more time. One could do this, but certainly even in the western world, I don't think they can take this as one of the boots in the model, but the content entirely go with the social media buzz that will drive them away. So, uh, now let me come to another very interesting question on the surveys can do is to get estimate of what percentage, what percentage of people here we're rooting for labor and what we're waiting waiting for the Tories. And for the rest, the smaller parties. But uh, there's no magical formula which you can input the word percentages for the parties and output would give you how many seats because a lot depends on how these votes are distributed.

Speaker 1:          22:26          So, uh, just measuring the total vote is not good enough. On the other hand, if for every constituency you measure votes, then that is good enough. But then that's the possible exercise. Too expensive, too time consuming and almost not doable or, or 600 constituencies on each one. If you want the thousand, that's a tough task.

Speaker 1:          22:48          Nobody would pay for that kind of a thing. So that's not possible. So what do we do? On one hand, statistically you can measure public opinion in terms of overall what percentage is, but the interest for everyone, it doesn't help me if I said no, no, no, my work is, to me it was right, but the seats were wrong. What can I do? People would not listen to me if I'm wrong. Also the other way, if somehow I make a waiter's cancel each other out, our survey is wrong and my conversion is also wrong. And eventually we get right. If everybody public, I, I've got it right because that is the objective function. We are looking at fits. Uh, you went in the u s a it's the electoral college votes. Not Quite the votes, but uh, in the system like you gate is winner take all it a constituent filled level. Whoever gets the largest number of votes gets that constituency representative is there parking in USA, the consolidation happens at the state level. Winner take all at the state level. Uh, even if in the state, the one candidate in the head from ahead of the other with just a few hundred votes, but they get the entire state's electoral college votes. That's the system in us.

Speaker 2:          23:59          And

Speaker 1:          24:03          Buddhists do do this, namely convert votes to seats. What we need is a kind of a mathematical statistical model, which would as a input taken into account various things that we know about the ground reality.

Speaker 2:          24:16          Uh, the things like

Speaker 1:          24:22          how susceptible is the vote for change. They do are the volatility of public opinion. Now, traditionally, uh, I had been hearing an admin tool. I used to interact quite a bit with one project, Clive Bain, who used to do, uh, who are the psephologists for BBC for a long period professor in the field college. And he had told me that the volatility of public opinion in UK is very small from one election. If you look at from one election to the next, how many people change their world? That's very small. Now that scenario, it looks like it has been changing, uh, this tracking the opinion polls over the last three weeks. Uh, three weeks ago they had measured the gap between the Conservatives and Labor to be, or 20%. And it came down to 2% or 1% when it came to the election time. So the, uh, traditionally they used to spend a lot of effort on what they call the marginal seats and what was called fifth seats, fifth for predicting where the last time that gap was 7% or more when you can count.

Speaker 1:          25:29          This wasn't last time. We'll win again this time. Lot of such seats have changed hands. So, uh, the Martin had to take into account all such things that one knows about the video. Everyone is making. The prediction. Other thing is what kind of data is available. One knows that socioeconomic profile does matter when, when for voting intention, uh, overall and ventured balance across that, but at what level is the [inaudible] profiles available. So, uh, and you get their availability to fairly detailed level and perhaps that precinct level, uh, in India, they are not available at that level because it's available at district level in India, but not at the constituency level. So taking into account various such reality is how susceptible is the vote for change, what kind of other data auxiliary data is available, uh, taking into account all that, uh, one built a statistical model and, uh, then uses that to, can work votes to seats.

Speaker 1:          26:38          And so this is where I come in actually in our team in India where we do it, the others do the actual sampling and so on and so forth. And I come into the picture for this work to see conversion. So, indeed, if one looks at the last us election, uh, everybody got it wrong, but they got it wrong for, it was not the survey, we'd got it wrong. It was this word to sit conversion where they go to Trump. Hillary Clinton did get more votes than Trump, and then they'd been calling that the gap has narrowed. But there was still, everybody was Gung Ho about Hillary winning. And what went wrong was what in statistics would call their prior. So the, like I talked about UK in us too, they, uh, have classified the states into three groups, a democratic Republican and a swing states or marginal states.

Speaker 1:          27:33          And their entire effort was in looking at what is happening in the marginal stage. They had classified, okay, these 20 or this 17 going the democratic basket, this 12 going the republican basket, and then now let's focus on the rest. It was this classification which went wrong. Three of their democratic states, I guess, Michigan, Wisconsin, and one more, uh, which everybody believed cannot go out of my Democrats. And they change sites by narrow margin. And, uh, if they had not changed side, Hillary would have been precedent. So the uh, that's the other thing. How much weight do you give on your prior and how much weight do you give on your data in any statistical thing? So they prior model all that is together and then the data only data will not do because you need some other information and it's a, so it's not just a science, it's a science and the art you're to pick the right mix.

Speaker 3:          28:29          Okay.

Speaker 1:          28:30          So, um, no I I people willing to answer questions so I'm told that increasingly the UK, us, the answer is no. Only about 10% of phone calls that they make end up getting them, getting the answers they want. Most others just hang up as soon as they hear who, ah, okay. Okay. I know, I know. And they'll put it down. Interestingly in India a while we do not do a phone survey but actually do door to door survey lot more expensive. But then refusal rate is very small. Only about eight to 9%. If you can find the person, people are willing to talk. So telephone surveys much cheaper, but then there's this huge refusal rate and then there are lots of questions about is there a systematic bias in being introduced in refusal and non refuser is it people, well, nothing else to do, willing to answer. And people who are otherwise have some meaningful stuff to do are refusing to answer. Right? So, uh, these are questions but uh,

Speaker 3:          29:37          okay.

Speaker 1:          29:37          But even when people are willing to answer face to face, all the more, so they're very happy to answer. If you ask questions about how is the PM doing or was this policy right, what that policy, right? Everybody has an opinion and can do a lecture, not just answer of yes, no, but at the end, if you asked, okay, so whom are you willing to vote tomorrow or next week? Uh, answer is not going to, why should I tell you?

Speaker 2:          30:01          Okay.

Speaker 1:          30:02          Right. Because that's considered a private information also. They don't know how you might use it. So what we do is, uh, we carry a box, something like this, a bigger one with a slip. It's like the simulator, old style ballot box with a slit. And all other questions are answered face to face. This question, whom are you willing to vote? Voting intention. We give them a slip of paper with the names you market, put it in a away market, bring it back, fold it, put it in here. People are willing to do that.

Speaker 2:          30:34          Yeah.

Speaker 1:          30:34          So there's also what is acceptable to people.

Speaker 2:          30:38          Okay.

Speaker 1:          30:38          So it's a mix of various such things. Understanding ground realities, psychology of uh, that the population, all that goes into planning and doing the surgery next to the Duke. People tell the truth.

Speaker 2:          30:56          Yeah.

Speaker 1:          30:56          And this is where there's a lot of debate. Uh, in fact, uh, some, uh, people who do surveys to believe that in any case, people either may not answer the direct question, who are you going to vote? Or even when the answer we'd it meant be unreliable. So then what they do is they impute award based on answers to other questions. For example, in Uk, this election they could have had, do you think, are you in favor of Brexit or not? That would have been one of the questions plus various other questions or 10 questions. And based on that answer, you build a model to decide whether this respondent is likely to vote Labour or likely to work a of eight here. So that's one approach.

Speaker 2:          31:39          Yeah.

Speaker 1:          31:40          In India with a much larger proportion being illiterate, uh, this approach is unlikely to work. Uh, the afraid to experiment with. We do ask questions and we do ask a direct question and then try to correlate and try to see if some patterns we can find a no, it's not going to work there. It might well be working in the west. So, uh, not everybody follows this imputing methodology, but there are serious researchers are very searchers who do this that they asked 10 questions, 10 questions and impute a vote based on that. So my, my actual experience with data is only limited to India. It's not a, for UK, my interaction was limited to interactions with project life and that was also nearly 18 years ago.

Speaker 2:          32:29          So, uh,

Speaker 1:          32:34          another technique which is used by some researchers is, uh, using last food recalled to correct for bias. So along with other questions, you also ask, okay, how did you work last time? They were the elections. So, uh, and the idea is that if you see that, uh, let's say in your sample you are overestimating conservative would by 3% for the last four, three call. That means in spite of whatever you're trying, you have somehow got to buy a sample and you correct for the same by applying this correction. So you scale down, uh, the conservative vote by corresponding percentages, editing or multiplicating you can do that and scale of the labor work. So, uh, this is one common technique used by researchers and once again in India when we tried to do this, we are driven far away from the reality and again, something to do with psychology.

Speaker 1:          33:26          So there seems to be a tendency in India to identify with the winner. So, uh, so, so even even when they are very unhappy with the government and our voting it out, much larger proportion of people seem to remember that the, I don't want them now, but five years ago I voted for them. So we have found this invariant of space and time. Whoever was the winner last time seems to have much larger recall in vote. And uh, so we, we do not do that. So again, it is to do with understanding the psychology of the nation to which one is doing this. How do people behave collectively? So, so I have a favorite phrase which I use. So electrons spinning the same way in India is this do with vendors, but human minds when can't stay the same. So anything which is a physical phenomena, uh, one could use some model where Ted in the West just implanted in there and it will work. But when the phenomena involves public interaction, human choices being made by humans, that could address me a starting point and one needs to fine tune the model.

Speaker 2:          34:30          So, Huh.

Speaker 1:          34:35          Okay. The last one, uh, as we saw in UK itself recently, the predictive power of a poll done weeks ahead is not much good. They use probably similar methodology and exit polls. They came out with the right numbers. And so when there is a shifting opinion, clearly have an opinion poll can address, tell you what is the mood of the nation. At the time the vote is taken, it can not predict how it might evolve in the last week. Uh, so you know, uh, fluid situation where the public opinion is, uh, changing, uh, you can't quite trust a bolden ahead. There's another factor which probably played important role in a UK in recent one, the

Speaker 2:          35:22          okay

Speaker 1:          35:22          bolden a week ahead or a month ahead. Invariably of the whole population. Now the pollsters couldn't use their assumption, okay, this is the overall group, this word, this group is unlikely to go and would only 60% of this group will go and vote this group, 80%. Gwen would apply those factors and then take it together. Okay. But that's only a guess work. And presumably they may have done that in UK in the pre election polls. And uh, this time again reading newspaper reports, I have no other input. A younger voters turned up as much larger proportion this time around than they were traditionally hooting. And that could explain that for the, in the exit poll, they don't need to apply such imputed percentage of votes. They actually sample from the people who have come to work and they therefore cogness effect of younger people turning up in larger proportion than ever before. And that's how they get got a better result. So the predictive power of Paul is a question mark on two counts. One the opinion itself maybe changing and to address, you can be making a guesswork about who is going to turn up to work. You don't really know who is going to work. So that is what raises a question mark.

Speaker 3:          36:34          Okay.

Speaker 1:          36:34          So I guess let me stop

Speaker 3:          36:39          questions.

Speaker 4:          36:47          So you were saying that a in India pulling through telephone doesn't work whereas a new k. Dot. It tends to work. But I was wondering in Uk, I think a lot of young people are not really using home telephones these days. So they're using most mobile phones and a lot of my friends that don't have time to do you think that is representative?

Speaker 1:          37:03          Yeah, so I suppose I don't know what they're doing, but perhaps they are trying to somehow factoring this into account in there pulling method and maybe also calling mobile numbers. I really don't know what they're doing. But yes, if the old lipstick per landline numbers, they are going to miss out the younger group. Completely a point. Very well taken.

Speaker 5:          37:26          Have you looked into the question of what a public making the opinion polls public can actually influence the results for like tactical voting?

Speaker 1:          37:35          Yeah. Okay. Uh, so, uh, at least based on India experience, I do believe that there is a feedback loop and it does impact a tactical routine. And, uh, again it is to do with this what I call the psychology of going with the winner. So whoever the media is saying is winning a ends up actually increasing the, uh, the gap typically. So that's indirect evidence that uh, it does have a feedback loop. Uh, but, uh, some people ask me then why are you associated with it? But, uh, when others are working, whether I knew it or not, others are going to do it in any way. So it's good a proper methodological police done and released. That's my answer to that. Yes. But it does have a feedback loop. I don't know. Again, west, what is it?

Speaker 3:          38:26          Belief

Speaker 4:          38:30          isn't, so what capability does a company like have to, to address this problem? Because my, my understanding is, you know, they gather so much data on their users and they actually have a very good understanding of the political preferences of, of most of them. So as a company like Facebook actually in a position just to kind of replace a sample polling

Speaker 1:          38:57          maybe a water of time when they then it becomes more universal. But at this point, uh, I think not because, uh, it, it really, it can capture the opinion of the users that it has had, but the lower socioeconomic strata who, even though they may be using a mobile or be on Facebook, uh, that group will be completely missed. So, uh, I think, uh, they said if your mark, you are using this for marketing some product, which anyways, that segment, great, that's big inflammation. But for something like elections, but African has to wait maybe another few decades and uh, it might become almost to new earth then maybe are not Facebook, something else may come up. But social media more generally.

Speaker 6:          39:48          So, so my question is about the sample size, a dependence on the, on the uh, percentages. So as you said that it is relatively independent sample size. You need for the given prediction accuracy you is independent of the total population size. But I'm sure that it depends a bit more strongly on the number of choices. Uh, so like number of, uh, our possibilities by not binomial but multinomial yeah,

Speaker 1:          40:14          actually no, even that it doesn't depend what it, it depends a lot on for the country as a whole. It depends on how much is the diversity as a whole.

Speaker 6:          40:25          Right. So are you saying that for example, if there was not just two parties, but like in India later listen, 30 parties, right? So the choice, the choice that you have to make is between which of these 30 parties is going to win. Um, so you're saying that the size that I need does not actually depend on what they started, two or 300. That seems more counter intuitive.

Speaker 1:          40:48          But for vote to seat conversion is a homogeneous country or is it, so in India for example, each state has a kind of a different political structure and uh, so we don't read the country as a whole. We are predicting, we look at each state at least the largest state as a whole. And so I need a reasonable sample size at that state level because how people in the deep South I will not award has nothing to do with what people in the east, in Bengal. Uh, our thinking. It's a complete disconnect at least politically. And therefore, uh, so in that sense, I do need to do a poll at each state, not just getting a nationwide thing might give me what percentage, but it will not tell me at all for my word proceed conversion. So for what to see conversion, I do need a larger, but number of parties doesn't mess up.

Speaker 3:          41:34          There must be a defendant, sorry. Yep, Yep, Yep.

Speaker 6:          41:43          Because if you consider the limiting case of the number of choices being actually larger than the population right now, 10 days of the pigeon hole principal will say basically that you actually need to hold everybody.

Speaker 1:          41:54          Uh, when I said does it depend, you're still talking in terms of, you know, small, factual talking off a million times. Yes, exactly. To answer your question, once again, what estimated doesn't change, but from vote to seat, if the direct contacts, it's much easier when it's a three way contest, neck to neck, it's much more difficult.

Speaker 7:          42:17          Yeah. How does this mortals consider a turnout of, uh, there are different groups of voters since, uh, uh, if you will pull a PR people or my, my, my nose Zam our bill. Not Actual evolve.

Speaker 1:          42:34          Yeah. So, so, so that is based on anecdotal or historical evidence?

Speaker 7:          42:39          I am. I, yeah. Yeah. So, so, so, so, so a buy a bicycle for, uh, for example, if you say that your most of young people, they will vote labor, uh, and uh, uh, and also you have the HR. Is it when a young voters, a actual dot don't vote? Uh, okay. Uh, can you decrees, uh, as a, as a, as a, as a, as a prediction for Labor for

Speaker 1:          43:08          all right. Yeah. So that's what is done. So you have your prior information, believe whatever you call it, about voting in propo propensity toward across different socioeconomic groups and you scale it up or down based on that from what you get from the survey. Yeah.

Speaker 6:          43:25          Do people ever try and do adaptive sampling? In some sense? You sink that you have a certain prior belief that say people in Scotland always vote for one party and then you start sampling, you start sampling. Then when you find that actually they don't anymore. What people have a second round of sampling where they, uh Huh.

Speaker 1:          43:44          Yeah. I mean ideally that would be good to do, but uh, the time constraint comes in because you know, typically you would like to do it as close as possible to the publication date or telecast Stein and you may not have enough time to take in all this information and change sample sizes. But if you do see something very different from what you had expected, then a second round of Paul is indeed would add value. So in that sense the adaptive thing is desirable but maybe not always doable.

Speaker 6:          44:17          Uh, I was wondering, um, sorry to follow up on that. If there's, if you do a blended thing, cause you're talking about like there's a different methods of sampling and some are more expensive than others and if you, I don't know if you asked people on who would they'd vote for and if that's any different to phone poles versus like knocking on doors versus, I don't know, are there even more expensive approaches? Like just do sort of blend them together? Is that an approach?

Speaker 1:          44:44          Well I have not done so till now because uh, the, uh, the, the, the group which is on Twitter, Facebook and such in India is very small proportion of the overall thing. So even if I want to blend that in, I will have to give it a very tiny weight. So that's why we have not done it. But I guess another 10 years later, I'm sure we'll be forced to do that too. As it be here. They may be doing it. Okay. Here on us because another thing we just said is that, you know, uh, it's also opinion makers. So somebody is on the Facebook and he's giving his or her opinion. If they are able to influence, let's say they're people who they are come in touch with. Okay. So these are influenced makers. So typically these people, so each one may be influencing a few others choices. So therefore that should be taken into account is what one group is advocate. So or time, I'm sure it will be needed to be factored in.

Speaker 8:          45:39          Hi. Could you talk a little bit about people's motivations for actually wanting pull data? I guess one obvious example is, you know, it's a political party or a lobbyist use trying to actually influence the result and therefore they're probably interested in, you know, what are the marginal seats, you know, am I very vulnerable to let's say the youth vote and d slight fluctuations in the youth vote in this constituency. In which case they're not just going to be interested in the, the final, your final answer, your final prediction, but also some of the variables that went into it and what influence they have and where they should be focusing their attention. I mean, Dee Dee, do you, um, do the people who are commissioning your poles, are they interested in that type of information or do they have other motivations?

Speaker 1:          46:22          Uh, well, I'm sure that motivated was which one fed information, but the ones with whom I am associated with this purely a media pool and uh, it is guided out by a group of people who are social scientists and they are interested in doing it because this is valuable data for their own research into getting insight into why people vote the way they did. What were the reasons driving the vote. So a month after they are a television program is over, these guys would be coming the data and coming to more deeper answers to these questions. So, but I'm sure political parties do get a polling done on this basis by other agencies

Speaker 6:          47:05          in cases where the, uh, the, the opinions are very close or is changing. Then posts seems, you know, like you suggested earlier that, you know, they seem to be less accurate. Uh, apart from the solution of not doing polling and just ignoring all the polling data, what, what will you suggest kind of posters kind of do in the future to improve? What are we doing today that we

Speaker 9:          47:30          should be doing, you know, a year from now when, when we detect that there was a mood change, you know, in the, in the general population?

Speaker 1:          47:38          Uh, well, uh, a lot depends on what money is available to do the poll. Okay. Not for the opinion poll. Okay. So, uh, one thing could be to ask, uh, uh, not just the sample size, but even the questionnaire could be adapted. Uh, you know, if you, you know, we are tracking the same people and if you today say something different from what you said one week ago, then the questions I should ask you should be more probing. You know what happened during this week. So I will, everybody based on newspaper reports are media reports, would know five big events which happen. So then we should drill down into each of these influence you or change. Okay. Things like that can be done, but it requires a lot more money. So, uh, let me, uh, maybe one or two more comments then the other than media these days.

Speaker 1:          48:32          The other interesting opinion polls is the financial markets. So if you read the evening tabloids the day before the polling, they were giving various scenarios and how will the market react? And so the market players are very much interested in, uh, because finally the election outcome does it impact the market. So they are very much interested in anticipating this. And uh, in India has seen, uh, financial, uh, groups, uh, commissioning survey are trying to get us to come and talk to them and get bigger, better insight into what is happening. I'm sure it is happening here too. So the financial, so more time, more than the media, it could be the financial markets, which would be engaging pollsters perhaps.

Speaker 9:          49:16          I have a bit of a question about the, um, the use of opinion polls to influence the outcome of elections and whether there has been any examples of political parties trying to introduce biased opinion polls or introduce bias into the sampling methods or anything like that to try and, um, sort of shift the election in their direction and if that sort of thing does or could happen, what they might might be to, uh, to protect against that.

Speaker 1:          49:41          Yeah, so, uh, in India and I would want to have [inaudible] but there is enough circumstantial evidence that this kind of things have happened. And, uh, all I can say is that, you know, we had, we not been approached and we were with the media house for whom we were doing the polls. We had a very clear, uh, understanding, uh, contract that a, they cannot influence what we will say after commissioning. Paul. They can refuse to put us on here. That is their prerogative. But if they put us on air, we will only say what our surveys say even if it is wrong. But, uh, there are in fact in India two years ago, they were before this a high profile election, there was a sting operation. Somebody posing as a, an acting on behalf of certain political parties went act, talk to 10 different agencies and uh, uh, trying to indirectly say that, you know, what, if you know, we want to fine tune your findings in a certain way and seven out of the 10 did indicate that the EIT may cost. But it is possible we can debate and discuss such things. They were caught on camera. So

Speaker 1:          50:49          yeah,

Speaker 4:          50:50          in the, uh, in the u s the, there is a newspaper who did a completely different approach. They had like, uh, a group of 100 or 160 people of which they, the opinion and based on that data inferences on, on what they believed the election results would be. What is your opinion on, on such an approach instead of random sampling?

Speaker 3:          51:10          Uh,

Speaker 1:          51:12          I would say, uh, I won't go with that. I mean, they, uh, they may be right on occasions, but, uh, there's no reason why these hundred people alone would determine what is the entire mood in the nation, right? I mean, um, and we changed that hundred, 2000 doesn't Jay, of course, we know that e television rating system, PRP system is everywhere based on such an approach. Okay. But it's debated, uh, there, the question is what else can they do?

Speaker 3:          51:43          Okay.

Speaker 1:          51:43          But in our opinion, Paul, having a fixed sample could track, uh, I think would have a limited value, but when we'll have to see how their predictions evolve over time. Ultimately, that's the only measure overall thing.

Speaker 4:          52:01          Any more questions? If there are no questions anymore, then please thank our guest. Thank you very much.

Speaker 3:          52:10          Thank you.