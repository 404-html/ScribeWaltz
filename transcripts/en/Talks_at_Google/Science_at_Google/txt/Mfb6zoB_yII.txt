Speaker 1:          00:06          The niece is the founder and director of the wild dolphin project and spent I think 32 years now observing dolphins in the wild. Uh, she and I have a research project, a joint between Georgia tech and the wild dolphin project. We were trying to really, uh, look at two way communication with wild dolphins. She's gonna give us a overview today of her work and towards the end I got to show you some of the stuff that y'all can get into, especially if you have some background in machine learning. So thank you very much for coming to these, shall we?

Speaker 2:          00:39          Okay. Well it's

Speaker 3:          00:43          great to be here. I'm glad you're all here and listening. So like fad said, um, I'm going to share with you a little bit about our general dolphin work and how we work in the field and how we analyze data and then we're going to talk about our joint projects and see if we can intrigue any of you into the machine learning world of dolphin sounds. So every 20 years, national geographic covers our work in the field. And a couple of years ago they did an another story about thinking like a dolphin. And our work was highlighted, uh, very much because that, and I have been working on trying to crack the code of Dolphin communication in a couple different ways. Now the reason we really think dolphins have some potential, um, they're kind of like primates in the water. On the upper left you see an example of encephalization quotient, which is the, uh, physical measure of intelligence.

Speaker 3:          01:34          In some ways it's a brain to body ratio and humans have a queue of about seven and dolphins are just second to humans, even above the grade eight s. So we know we have the physical store, they have the physical structure for intelligence. Um, they use tools in the wild. They've been known to understand artificially presented languages comprehending both syntax and semantics and they recognize themselves in mirrors, which is no small feat lane. But one of the big questions still is do they have language, do any nonhuman animals have language? And my particular perspective is that we really haven't looked close enough. We suspect if, if you're a biologist you tend to suspect that animals are doing a lot of complex communication. We just haven't cracked the code because we haven't had the tools. Now collaboration with sad is based on two different strategies. The first is a decoding the dolphins natural sounds.

Speaker 3:          02:30          So like he was mentioning, we have a a 32 year underwater acoustic and a behavioral data set where we want to try to correlate sounds and behaviors and look at some of the really detailed categories of their sounds using um, kind of an anthropological framework. So knowing their society, the individuals, because it's a small resident group. And the second way is to interface with technology directly creating some kind of two way interface where we can exploring perhaps a mutual language and maybe learning back and forth from both these strategies. So first I'm going to tell you a little bit about the specific dolphins that we work with and how we're approaching the dolphin signals themselves. A little bit about how we analyze these signals. Some of the challenges working underwater with a dolphin communication and keeping in mind that we use computers all the time as biologists. And it's a really good marriage and it's really necessary tool. So dolphins have a lot of different senses. Of course they're great acquisitions, but they also have pretty good vision, especially a species that live in Clearwater. And they have cross modal abilities between a vision and sound. They have taste, they do not have smell, and they have touch. And in the, in the water, uh, tissue and water has about the same acoustic impedance so they can actually feel sound. So this becomes another way they communicate.

Speaker 3:          03:53          So this is a summary of the history of Dolphin communication using one of Gary Larson's slides showing that primarily we've worked in captivity. We've tried to look at acoustics and believe it or not, we've been looking for English, which is a little scary, a little antiquated. So here you have this slide and the scientists are hearing oblong to spend you'll and other Spanish phrases and they have no idea what they're hearing, but they're writing it down. So we've progressed, at least we want to look and see what the dolphins themselves are doing and how they might structure a communication, a system for a complex aquatic society. So, um, we basically correlate video. I'm Cindy Rogers and I'm the research assistant at the broad off and project. And I'm Michelle, I'm a graduate student here. So, you know, it's funny, I was thinking when I was in graduate school, uh, one of my teachers told me there were only a couple people in the world who could read a spectrogram for human speech, right? This is how old I am. And now of course computers do it, but we use the basic technique with dolphins. We take our video, we run the audio track that correlates with the behavior,

Speaker 2:          04:59          some dolphin aggression.

Speaker 3:          05:19          So it's, it's busy place. There's a lot of sounds. Um, there's a lot of dolphins speaking sounds and we don't always know who's making what sound, which is another issue. So our project is in The Bahamas. So here you see this coast to Florida across the Gulf stream are the shells sandbanks to The Bahamas and the dolphins lived there. It's great underwater visibility, which is why I chose to work there because I want it to see what these animals were doing underwater, not just looking at the surface, which is the case in most parts of the world. Uh, my nonprofit had a boat donated Yay can 1992, which is a great ocean going vessel. So we lived there for four to five months year. We do our work there, sleep there, we cook there, we analyze data out there and we go where the dolphins are basically.

Speaker 3:          06:02          And uh, my main tool is underwater video with a hydrophone underwater microphone of course simply to correlate Santa behavior and try to get as much data with that equipment. Um, our project is pretty noninvasive. We don't grab or tag the animals and we have about 300 individuals track over the decades and we're on our, actually now we're on our fourth generation, so we track that as well. Now the species is actually a quite convenient species to work with their Atlantic spotted dolphins and they don't have any spots when they're born. Um, so here you see a mother fully grown at, she's 35 years old here and we use id, plastic ID techniques. We look at Dorsal Fin Nixon notches, which most people use from surface work. But because we work underwater we can also look at body marks, full body marks because there are spots, we track their spots which are kind of like constellations of stars.

Speaker 3:          06:55          So we do this id work every summer cause they gained spots with age. And so we know, we actually know all our dolphins in most of which were born certain years. So we can, we kind of take the broad overview, the anthropological framework. So we sex are dolphins because again, we're in the water and if you want a little take home tidbit to tell your kids or your significant others, you can tell them, you now know that to sex. A female dolphin, you look at the mammary slits on the underside cause of course they nurse their young. Um, we know their reproductive status, they're pregnant for about a year. They'll have three to four years in between having calves. Uh, we know the females associate with each other by reproductive status, so some are pregnant, they'll hang out with other pregnant dolphins and the males have lifelong friendships that they're form when they're a juveniles. And their job is to not only try to mate with females but to protect the group and try to scout out sharks and that sort of thing. So we understand the society a fair amounts. We have a lot of metadata around our acoustic data as well. And we do know some things about dolphin sounds. We know they make whistles specifically signature whistles, which are,

Speaker 3:          08:08          so these are long distance communication signals that go three to five miles in the water for example, on they make burst pulses,

Speaker 2:          08:15          which are

Speaker 3:          08:21          very funny sounding, sounds very hard to categorize, but they're very social. This is probably the most common type of dolphin sound and the least studied.

Speaker 2:          08:31          Okay. Then

Speaker 3:          08:32          we had echolocation clicks, which of course we know a lot about from a navy work. Um, and then we have a compressed echolocation clicks, which we call buzzes. And these are social sounds, um, unlike clicks for navigation and there are these buses are using courtship and discipline. So we have some basic correlations about what sounds they make with kind of gross behaviors. But what we'd really like to do is look at the greater complexity potential. Now, Dolphin research is always about 20, 25 years behind terrestrial research. For example, we've known since I think 1992 that vervet monkeys use different kinds of alarm calls to communicate that different kinds of predators are coming. So very specific. There's eagle coming or a leopard coming, so they're, they're kin can take the appropriate action. Um, some fantastic work with prairie dogs that really shows the complexity of the kind of information that can be encoded in a very short alarm call and including things like a human with a yellow shirt on, walking across a field with a gun or not a gun, et Cetera, et cetera.

Speaker 3:          09:36          So there's a lot of things these animals are doing. And you can imagine a scenario where a dolphin would be pretty handy if you could communicate whether the hammer heart had shark was coming towards you or a tiger shark or great white shark. And my really determined if you survive or not. So again, we suspect there's a lot of information there we don't know about. So the big question really with dolphin sounds, at least our an an all animal communication for that matter. Are The signals referential meaning they refer to something, they label something like a name or word for an object or are they graded? Do the sounds run into each other? Do they just do they just show things like increasing intensity of sound? If I'm going to talk really loud it really fast, that would be kind of a graded system versus a discreet word or referential signal. So this is one really big debate in animal communication. And dolphins have both types of signals. We know signature whistles, for example, their names are technically a referential signal. They can call each other and they can broadcast their own name.

Speaker 3:          10:39          But it is at language and this is where we really fall short in our tools to look at is there more complexity in the order structure, potential grammar of their sounds. Now, Dolphin Communication has also a very complicated for the researcher because they're really good at what they do acoustically. Um, they have a pretty neat system for creating sound and receiving sound. And probably the big message here is they're very directional. So they send out very high frequency signals up to, uh, it looks like 240 kilohertz, high frequency out this way. Low frequencies drop off to the side and they receive the sound, um, through their lower jaw, through a fatty Oregon. They don't have external ears like we do, same thing. They receive sound directionally and uh, to make it more complicated for the researcher, if you want to get ahead on signal and get the full high frequency that dolphins can internally steer their sound 20 degrees off and you would have no physical signal that they're doing that.

Speaker 3:          11:35          So it's a little challenging to get ahead on signal. So when you're trying to localize a vocalize her and you see on the right side of the screen here a four hydrophone array that helps us triangulate who's making sounds and this technology actually we're just starting to incorporate with some new equipment. So lots of times we don't even know making the sound and these big groups. So very challenging for the data. Um, again, collecting ultrasounds has been pretty recent, at least in our work. On the lower left, you see an example of the Spectrogram, a little the whistle that you've been hearing. And the full picture is the ultrasound above a hundred, about 110 kilo hertz. And uh, hopefully you can see on the far right there's some harmonics and I'll start showing up in this, uh, high frequency sound spectrogram you see places where of course the clicks go broad band, we always knew that we just didn't have the equipment to record it. And you see some places where the narrowband signal wouldn't even be recognized. There's no information in the narrow band, but there's information in the high band.

Speaker 3:          12:40          Now, historically, we've used a lot of different techniques and whistles have been fairly well studied primarily, and I hate to say it, but they're easiest to measure, right? We can measure contours. We're pretty good at visual pattern recognition. Here you see examples of four signature whistles from different dolphins and you can tell they're quite different. You can imagine the dolphins can tell that as well, and we can measure their basic parameters. Um, early on we also use neural nets. Uh, well if we had enough samples to train the computer to tell us, um, if two signature whistles is red and the green, as you see are really different or the same if the computer had problems separating them or not separating them. And then it would give us a quantitative index of how different they were. A, for example, between a mother and a calf or two juveniles. So these are tools. We have used a very custom kind of program tools. Now these are signature whistles from a mom and a calf. Gemini is the mom in a geo is her son?

Speaker 2:          13:41          No.

Speaker 3:          13:44          And what we discovered when we were doing our neural network is that some of these signals are very messy. They were just sloppy and we couldn't track the contours of the whistle. So we had to throw them out of our data sets for the neural network, which was not really okay because we really want it to misery all the whistles of all the mother, calf pairs, that sort of thing. Um, now with some of these new programs, it'll actually extract these patterns in a different way and allow us to code these signals. So neural nets were great for a while. Um, they didn't get us very far. So the process we really go through is because we have underwater video and we have sound, we use a program called observer, which is a behavioral coding software for a bio behavioral biologists. And we basically take a timeline and we will code our body postures.

Speaker 3:          14:27          For example, she might see a head to head, a little peck rubbing another head to head, a courtship, little courtship behavior, and another, uh, reunion or peck peck. So we'll code that on a timeline. And then simultaneously we'll take our sounds and then you saw a little bit of this in the video and we'll walk through our vocalizations and we'll code them in very generic terms. Um, so here we might see a scream. I'm here, we have a signature whistle. Now we have a buzz, and then we have some other really messy kinds of sounds and we don't know how to categorize those. So one of our main impetus for connecting with dad's group was to really help us get at these categories of sound, specifically burst pulses to help categorize things that we human beings aren't very good at. Apparently. Again, match this with behavior and then look for some structure in order to really take a look at, you know, are there really any grammatical rules? Those are structures, anything akin to language. You're partially, uh, akin to human phonemes, for example. So here are three spectrograms tour humans and one is a dolphin. Take a quick guests, which is, which you guys are experts. I bet you're all going to get this right. Um, so anyway, uh, the top one is, is not a dolphin. That's a human speaking as is the bottom line. So the middle one are dolphin burst, pulse sounds,

Speaker 3:          15:55          and you can see they look a little bit like phonemes. We're not saying they are, but these are the structures were used to sort of looking at and analyzing. Right? Um, I know my computer can recognize my voice, but again, he's a language we don't know. You know, this is one way to start looking at it. It's not the only way because you still have to understand the function meaning of words if they're there. But we really want to take a look at it. Um, now that we have some potential tools. Um, so for example, one thing that we started with, with um, uh, the Georgia tech work was just asking, is a whistle, just one unit? That's the way we've always approached it as dolphin researchers, but in fact, could that be parts of whistles that are recombined? And of course this is critical if you're looking at language, right? We recombine phonemes to make different words in different orders. Um, so some of these tools are being applied through Thad. There's some dynamic time warping. This been applied with other researchers. But we're really pretty excited about these tools. I'm gonna let fed jump in here. Talk about,

Speaker 1:          16:56          so this is a tool called horror, which is something we've made a Georgia tech. This is a Daniel Close Doors, Phd dissertation. For those of you who are into machine learning, you can go look it up at Georgia Tech's dissertation database. Um, in particular we have the spectrogram up here. Um, we create a, we have about a 40 features here that were, that are learned features as being distinct in the database. I'm basically looking at convolutions see what those features are in the, in the spectrogram from those. We try to look for repeating a time series, these patterns here so you can get, so you taken raw audio, uh, look for the areas that have a dolphin vocalization in them, try to find these features. And then from those features, look at, see how they recurrent time and get a motif. They look to see where all those motifs are in time and make sure they look, uh, on the, uh, on the system from that.

Speaker 1:          17:54          We then, you know, take this code book and try to explain as much of the audio as we can. So you're just trying to basically we do such thing to a series of strings. Um, that data reduction really helps us move much more quickly and trying to find a repeated patterns. And from that, um, from these repeating motifs and labeling them, we start finding things that it turns out regular expressions. So we just look at how these things reoccur. And to our surprise, regular expressions turn out to be one of those predictive things for behavior that we know that the niece has observed in the wild. So what we did is took her 2012 field season, took the first half of it, trained up a system. So this is unsupervised learning where we got the motifs and got the regular expressions and we tried bag him for find the machine learning people.

Speaker 1:          18:49          Now for a second try bag of words, bag of grammars and bag of regular expressions. It turns out the most predictive thing for the behaviors were bag of regular expressions. Um, and so what we looked at is took the second half of her 2012 database. No, and looked at the behavior she, she tagged visually. So she actually saw foraging behavior. She actually saw aggression, McCaffrey play. Given that we then say, okay, what sort of motifs, what sort of regular expressions, um, are affiliated with these different types of visual behavior? Can we actually see a correlation with them acoustically? And the answer is yes. And the correlation within class is very high correlation across classes, very low. This is excellent news for us because then now starts telling us that yes, there is some stuff here that is discriminative in the acoustic environment. Now the question is how can you do this on a bigger scale? Doing this sort of stuff takes days of computer power for a roughly small dataset.

Speaker 3:          19:50          So, so these are bottlenose dolphins now very coordinated and synchronizing the sounds. And I'll let you disappeared for a second. Two dyads and nail dolphins, sea simply competing and you can hear, hear them supervising their sounds and see them orient. What are they saying? So, so, so what we basically want the computer to do is to help us automate categorizing and coding the sounds as the video section you just saw. So we're not manually going through it all the time and build a larger database.

Speaker 1:          21:04          So the system that we showed you before from Dale causers dissertation is I said quite power hungry at thanks. A lot of computations taking living a lot of the spectrogram at the same time. And so it's, it's something that, um, uh, has some problems being paralyzed though we can do it so they can kind of do a lot with bigger iron out there. Um, but, uh, we're hoping now to make some things a little faster. So that's where we were showing you next. So we'll let you again, unless you listen to this for a second. Okay.

Speaker 2:          21:34          You just spotted dolphins. Now this is aggression, typical spot. Okay.

Speaker 1:          21:55          And now this is a new tool where we're actually doing something similar to this Shizam our algorithm for those who are familiar with that were looking at the spectrogram as if it's a computer vision problem, looking for regions, uh, areas of interest. And they were starting to looking at these patterns are time. Now the problem is it before, we are looking at sort of the distribution of these things and how they change. Now we're looking for something very simple, which is just, you know, the, the main majority of the regions of interest. Are they increasing in frequency or decreasing your frequency? And that's what this is down here. And it turns out that's sufficient. Josh Gad an initial, uh, annotation of what's going on. So you can see here that it's, it's labeling the, the red dots are the, the, the uh, features. Um, this is sort of the general, uh, main feature.

Speaker 1:          22:38          We're using the cluster on. And you see at the top we have j, e, n, e, d, j, d, a, e, j, e, n, c layer. Later on you can see the, the Ejv as a commander, a combined thing. So the top is basically the individual, um, motifs, the ones that are combined together. Those are the, uh, starting to be the regular expressions were saying. And, uh, this is brand new stuff. This is just came out this month. We're trying to get this so that the niece can sit there and say, okay, I'm going to load in this database and in say minutes, as opposed to days, get back some results, initial results and be able to figure out where to throw more attention to more computational power on. Um, so, um, hopefully we're going to continue develop this and make it a little bit more sophisticated but allows her to really tune the parameters a bit and see where our best bets are for a future work.

Speaker 2:          23:34          Okay.

Speaker 3:          23:34          For example, if you're going to hear a synchronize squad,

Speaker 2:          23:41          okay,

Speaker 3:          23:42          so these are adult dolphins. Now you're going to hear juvenile, you're trying to synchronize your squawks. This is something they learn over time and we're hoping for example of these tools will help us pull out of our data the specific categories in order of sounds so we can differentiate and look at the process for example, of development of a vocal use in dolphins. Now the second way we've been interfacing is a researchers is to look at can we incorporate some underwater computer technology real time to try to look at Dolphin communication? And we asked the question, is it possible to directly real time communicate with another species? Well, in The Bahamas we actually have two species of dolphins. Bottlenose dolphins you see here there are about three feet larger than the spots and but they interact on a regular basis. Um, they, they forged together. Sometimes they babysit each other's young interspecies babysitting.

Speaker 3:          24:40          Um, there's some dominance dynamics between the males of both species, but they'll form temporary alliances when we get another intruder in like a shark or a unknown bottlenose for example. So they're are already communicating with each other to a certain extent. We've been trying to look at that and pick apart how much they understand perhaps of each other's sounds and or behavior. But if we look at the rest of nature, um, other species are actually decoding each other already. We just aren't in that circle. So we have sent in a warning calls that are used between species with birds and monkeys. Now with Cetaceans, with whales and dolphins, we have a couple interesting examples where they're not just listening and taking advantage of knowing an alarm call from another species. When they get together with each other, they actually create a short and send a mutual language, but they use mutual signals.

Speaker 3:          25:30          So it appears to be easier at least for whale and dolphin species to agree on some sounds they use when they're together versus their own communication system, which is pretty interesting. I always thought maybe the way to go is to just decode them. But if you're trying to communicate with another species, maybe you have to spend the time and maybe it's not culturally possible to learn the intricacies of, you know, a nonhuman animal communication system. So that's kind of approach we're taking. And uh, what started happening in The Bahamas, we work noninvasively and we try to do behavioral observations, but the dolphins are often interacted with us cause we're right in the water with them. We're close, we know them. And they started doing things like mimicking our postures, goofy things like a gosh. One time we had, I'll never forget this, a swimmer who was trying to do a dolphin kick, you know, the dolphin kick in the water and then the dolphin was behind the swimmer going just spazzing out because he was not a good dolphin kicker. So they have some funny sense of mimicry cause that's what they do with each other all the time. And they sometimes they would what I would call, tried to acoustically mimic us too. And we kept thinking, God, maybe we should really create a tool. I mean it's a pretty unique situation in the world. These dolphins have time, they're safe, they're pretty friendly

Speaker 3:          26:44          and they seem interested in humans. Now we have plenty of other examples, scientifically of interfaces that have been created. What there's other species, one of the best known of course as the work with Kanzi by sue savage, Rumba, uh, who over many decades and many different types of work designed a keyboard. So Kanzi could interface and they could go out in the woods and talk about things and share ideas and thoughts. Uh, probably the most technical, technically advanced a dolphin human interface actually happened at the Epcot Center in Orlando, Florida. It was actually an underwater, um, a keyboard that was created. So Delvin's and humans could both use it. Um, you basically either stuck your hand if you were human or you're a beak. If you were a dolphin into a hole, which broken infrared beam, which triggered a word in English or a whistle and Dolphin. And then the dolphin could go and say, let's go to the reef and get a fish, or the human could do that.

Speaker 3:          27:39          Um, but it turns out we humans, we're really slow in the water. So the dolphins would make a command for the humans and go over and they'd wait and they'd wait for the humans to get there. So part of the problem is they're so fast acoustically and physically in the water, of course. Um, so they had pretty good results, no big breakthroughs. But it was really the first example. And there've been historically other people that have developed keyboards for dolphins. But this is the true, really the first true underwater keyboard. So we kind of worked with them for a while on our ideas and our original vision was while we're out there in the boat, let's, you know, we have computers on the boat, let's put a keyboard under the water, we'll have the diver work with the dolphins, explore the keyboard together. Um, well we found out pretty quickly that the dolphins have a lot better things to do in the wild and sit at the boat and press the keyboard.

Speaker 3:          28:26          Right? Fish goes by, they're out of there. So we, we quickly created a portable keyboard. We get pushed through the water because we're often swimming hard to keep up with them to see what they're doing. Um, and it was a visual keyboard with a little acoustic sound that was correlated with specific signal, the ship place. So this is the whistle for scarf. Okay. Um, so the idea was we labeled for simple objects with whistles that they could mimic, but that were outside of their normal repertoire. Because we didn't want to say something inappropriate because we didn't know their language. Like your mother does something with Blowfish, I dunno, puffer fish. They do. Actually they, you know, they get high chewing and puffer fish. Did you know that? They do. Yeah. There were some great footage on the Internet not too long ago that yeah, they pass around the puffer fish and they have the, you know, the food, the drug from the liver of the puffer fish.

Speaker 3:          29:23          So, and in every animal has just enough to get high thinks you'd never tell me I SRE, I just thought of it. It's trying to make a joke, but, so anyway, sorry about that. So we labeled um, these four objects, scarf, rope, things that we as humans bring in the water they like to play with and drag around Sargassum which is a seaweed they play around with. And then a bow ride, which is really high motivation cause it like about rides. So we had the system was pretty archaic. This was in the late nineties. Um, and they were interested but they couldn't really trigger the system with their sound and, and, um, we couldn't do any real time, sound recognition. So it was really not good enough. So we said, oh, let's just wait. We'll find some computer geniuses to help us out. Fat actually came to my lab originally to try to decode the dolphin natural sounds using some of his tools and yeah.

Speaker 1:          30:14          Then I made the mistake of saying, oh, we can make your keyboard for you only take a semester five years later. Uh, we actually have a system that we field. We're doing this every summer now. Uh, I, most modern one is a stereo. How tried to kill her hurts a sampling system. Uh, she can actually, we can produce, um, uh, these whistles in the water. We can also listen for them as well. We're finding the stage where we have gps and this thing that we can do a convolutional, uh, filters and start picking up stuff at any frequency. The, they do it at a, one of the biggest problems with this box actually is, is believe or not the speaker, it turns out it's very hard to get enough power in the water to actually communicate. Um, whenever you see the navy do this, they have a, a hydrophone in the water with a big amplifier hooked up to a big generator.

Speaker 1:          31:04          Right? And they're pushing toward Watson. We can't exactly put 200 watts into a chest mounted box. Um, so we've been making our own custom speakers. Yes, I am a speaker manufacturer. You don't want my speakers though, cause they really do not have very good response, but they're very good at certain, you know, narrow bands and we can actually, you know, send out these things. I, while while the dolphins can communicate it three miles, uh, we're, we're sheet working on our signals to be, to work at 60 feet. Um, so now we're actually successful. We can actually get these things in the water with enough computational power, uh, enough, uh, sound power to actually do these, do a two way communication work and I'll let knees to tell you what we're trying to do with it.

Speaker 3:          31:44          Yeah. So the system basically as you see it, you know the box with a computer to hydrophones to receive the sound speakers to play the sound. And then the operator has a key pad with the preprogrammed, it sounds in it here you see pictures of us getting our boxes on and go in the water. Um, we actually practice in the water with the boxes without the dolphins because we want to be ready for how they try to mess us up when we're trying to do things with them. Um, and, and the whole, the whole system here is designed on modeling the communication system because the only thing that has worked with other species primates, I mean pepper Pepperberg work with birds is the idea that you want to show the a species how do use the system. It's like your kids are exposed to human communication as they're growing up, right?

Speaker 3:          32:28          They see you going to the refrigerator for milk, you talk about milk, you offered the mill. It's the same idea is that you don't want to throw the species in there and go, okay, perform, do what we want you to do. So if, if my colleague Adam wants to get the scarf from me in the water, he has to use his keyboard. He has to ask for scarf, has to come over and get it and then we exchange it. So, so we practice that. Um, we usually put a third person in to be a dolphin. And try to mess us up and ask for things we don't have and that sort of thing. And so we learn how to respond appropriately. Um, and then when the dolphins come around, we have small timeframes. I call them windows of opportunity where they aren't doing their normal behavior, they're there to play, they're ready to engage.

Speaker 3:          33:08          And we just try to tap into those moments. And we can be in the water, I don't know, an hour, two hours sometimes with them where we're exchanging these toys and playing sounds and, and having them see the system. So the idea is diaper a and dive ruby have a box on it with the computer diver a can play the scarf whistle so that diver B is going to hear it as a word. Where, where are these um, uh, bumped conducting earphones. So diver be would hear it as a scarf. The Dolphin would hear it as a whistle or diaper. Be Good, play a sound, a sargassum whistle. Same thing. Diver a hears it as an English word, so they're sure what went out. The Dolphin would hear it as a different, a whistle. And then the hope is that the dolphins will mimic the whistle and then dive or a, and diver B would be notified in the earphone that, Ooh, you know, that whistle was made sargassum or scarf and then the appropriate toi would be given to the dolphin and we'd swim off into the sunset together playing with their toys.

Speaker 3:          34:02          So it's a simple system, but you know, dolphins can mimic sounds really well, but to get them to understand the function of a sound is a whole other level. So it's all about exposure, repeating the sound to them, showing them how it's used so that they start, if they want the toy, they could ask us for the toy. Um, for example, we also in the system, we have our own signature whistles made. So we have names. So I have a Denise whistle for example. Um, we also have some of the dolphins names in the computer because we have a very small subset of dolphins that work with us on a regular basis. So we have their signature whistles in the computer. So if they come up, we can actually greet them with their name, which is what they do with each other, and then we can engage in this behavior. So it's a potential tool. This is actually something

Speaker 2:          34:50          underwater. Video of the word. There's two people in the water, both with boxes. I've got the yellow scarf. They can't hear you when the, yeah, sorry. I've got the yellow scarf. Just labeled it for the dolphins. I've just asked for the star using a whistle. Got, then it gives me the scar. Now I'm going to offer to the dolphins, I'm going to label it as I offer it.

Speaker 3:          35:46          And then we try to get the scarf beck, which we don't always, she loses a lot of scarves, but it's interesting. Just last year, you know, we try to ask for the scarf back too, and every once while they'll drop it. And so we're like, we're hopeful that there may be understanding that I've asked for the scarf, not sargassum or the rope, et cetera. Um, so, uh, this year we're going to have the new computers. And now the great thing about this next summer is we finally have data for what the dolphins have been trying to do with us. For example, they've been trying to mimic these whistles in ways that the computer wasn't set up to recognize. For example, they will put little additions to our whistle on it or they're doing some high frequency stuff, um, that the computer's not recognizing, but now we're going to use that.

Speaker 1:          36:31          Dana, we were sampling it and did you kill her as at first cause normally where between, you know, a a seven, 10, but it turns out that when we went back and looked, um, there was some evidence of mimicry, um, uh, double the frequency and we just simply didn't have a computer board fast enough to deal with it. And it does not till this year that we actually have a GPU based system where we can actually look for the same pattern no matter what frequency and sad. So, um, if they, we know that they can reduce whistles, um, uh, much higher, uh, than we're looking at before. But now we should be able to cover the whole, the whole arrangement, whistles, not the whole whole range of burst pulse for the whole whole ranger whistles and be able to process that and time

Speaker 3:          37:12          and to know what they're trying to do is really helpful. So now we can react more in human time. So the whole idea of this system is to empower the dolphins to communicate with us, so we're not just giving them orders or commands and to see how far it will go. And the hope is that with the decoding software of their natural sounds, we can eventually figure out some motifs or patterns. We can loop back into the two way system and see if we can use more of their signals in the system. Then our artificially created whistles. So breaking the communication barrier of humans and other animals, you know, it's, it's a pretty interesting concept. We haven't really tried it on a grand scale. Can machine learning help? I'm pretty convinced after working with dolphins and the sounds for 30 plus years that you guys are, our only hope is that we really need these power tools and we need the expertise that you've all done in your various fields for just recognizing human speech in many ways. And I really think that's going to help us really cracked the code with a lot of other animals, not just dolphins, but the tools will really be applicable I think across disciplines.

Speaker 1:          38:19          And that's one of the things that, you know, those of you who do machine learning, you know, part of the hardest part of the problem is just getting good data sets. We'll, Denise has three, two years of it. Um, we're slowly getting it to the stage where it's cleaned enough that that machine learning can really, um, uh, techniques to really be used on it. I'm really understanding it well enough that we can, you know, figure out when this dolphin vocalization when there's not. And, um, you know, we really think that these, uh, these machine learning techniques really can help. And we've done a first pass at which is really showing some promise. However, when the big problems is simply resources, right? We need more smart people to look at this and look at this Dataset and um, uh, um, to, uh, take, uh, um, to try different methods. So this is the method that we're using. Those of you who are familiar with, uh, the, um, machine learning approaches here, it's sort of a different take on, on it. And so we're really wanting to do is get some, get a, a little team together and see if we can, uh, get some, some progress here, um, in the next couple of years.

Speaker 3:          39:23          Yeah. So we're trying to put work in groups together. Um, if you want to help support our regular work, please go to our website while Dolphin project and we have memberships, we have the chats society,

Speaker 1:          39:33          and there's also some small projects out there at inner speech for those or you go to that conference. A summit specializes on an animal computer interaction. Um, this also an animal, a computer interaction, a conference that's a now part of ACM, a that has a yearly meeting. Uh, and so this now start to get a small cadre of researchers. They're not just looking at, um, uh, marine mammals, but also, uh, birds and primates and all sorts of animals. Seeing if we can come upon some common tools and common methods to go forward and see if we really can crack this code of what the key, the, these animals are doing with their communication. Great. So thank you very much. And uh, I'll run around with a microphone and take any questions. Thank you.

Speaker 3:          40:23          Thank you. That was really interesting. Do you have any idea what the dolphins think of you? Are they there for play? Are they, what are they maybe trying to communicate? Do you have any idea? You know, they, it's, it's a really unique situation because they have, I won't say an easy life, but they have a, a well-resourced life. So they have, you know, they have fish, they don't have a lot of predators. There's not a lot of human impact. So they have time to play with each other. And apparently humans when we're there, um, you know, I don't know. I don't know what they think of us. I think they think we're funny in the water. I think they think we provide bow rides and some interesting entertainment. But you know, I, I, I really think it intelligence seeks intelligence and I think they really recognize and other species of interests like they do with the bottlenose we're out there and other species. Um,

Speaker 1:          41:15          I think they make fun of me. Well that's for sure. I, I'm not a very strong swimmer. And so one day I was out there with a niece and got some water. My master, I was doing mass clearing and we're just floundering around and they all ignored her and came and just circled me and just all did you know, just all stood at the same way I'm standing in, the water's

Speaker 3:          41:34          going, are you going, are you dying? Can we watch suppressant and drag it back to the boat? We've had to had them in health, like a struggling swimmer before, like escort them back to the boat because they recognize that they definitely sense of play. Yeah. So we'll see. I like to know. You're fine.

Speaker 4:          41:52          I want to thank you very much for the wonderful talk, a very interesting project. I have a question for you and also questions with that. So you said your research does not require attaching any physical things to the dolphin. So when you acquire the data, you are always at some close proximity to the dolphins. So how do you know that the data that you have is all they're saying? Right. And also a question to that, uh, the, the approach that you used to, to cluster all of these, um, rejects expressions. Why don't you, or maybe you have like one idea would be to just run it on human voice and see whether it can find out patterns in what people are saying. Um,

Speaker 3:          42:46          yeah, sure. Thank you. So yeah, so we're in the water, we're in close proximity. It's pretty hard to build a dolphin blind, you know, and we've tried sticking a camera over a boat and, you know, but we feel that if, if they're habituated to us and we're not grabbing them and poking them, they basically start behaving and we try to record their behavior. Now it's most likely that we're not recording everything they say. Absolutely. And, uh, we've toyed with doing things like putting passive acoustic devices down on the sea floor, which is only 20, 40 feet deep just to record when we're not there, of course, but they move around a lot. So it's, that's a little challenging. We've thought about building some kind of drone. Like I'd love to do an underwater remora that could attach to them and just go with them when we can't. And record sounds. I mean, you could do that. You can do that kind of stuff now. Right. Um, that'd be pretty cool. Um, so yeah, I, I'm sure there's things they're saying that we haven't heard. And,

Speaker 1:          43:38          and the sound is directional too. They can actually, uh, uh, select, uh, one person like, you know, in this audience and the other people can't hear it, so they can be very directional with their high frequency stuff. So yes, we're missing stuff. We know we're missing stuff. Um, uh, hopefully this two way work, um, you know, you, you could hear him in the water too, right? Because you get a lower frequency stuff so you kind of know when they're aiming at Ya. Um, but uh, yeah, uh, it's a problem. Um, we just hoping to get enough data that, uh, we can deal with that issue. Now as far as, you know, running it on humans, human language, we have for I know sign language actually and a data set that I made as part of my master's thesis back in the mid nineties and a, it actually finds half those signs right from the, from the video features.

Speaker 1:          44:29          Um, I've done more recently with a connect data for sign language does the same thing. We're doing it on synthesize synthetic speech. Um, we're doing it on a MIT's opencourseware lectures. Uh, the timid Dataset, a OCR, um, lot of things where we know what the answer is and performs really well. Uh, we have an exercise database to where it's just exercise routines and it and it discovers all six of the exercises and the I'll make recognizes get nicer person accuracy. So the algorithms are, are you know, doing stuff, doing well on these well constrained problems. But this is the aspirational, where can we actually make it work on something we don't know the answer to. It can work well enough. We can get help Denise get somewhere on the state of set cause we don't have to be perfect. We don't even have to be that good. We used to have to accelerate what she's doing. That's are a hard thing.

Speaker 5:          45:22          So if I understood the correlation that was being done between alphabets and patterns or features being extracted, right. So it sounded like we were trying to take the English approach, the English language approach and equate pieces of sounds to alphabet. I'm wondering whether there's any room to take the Chinese language philosophy and take a character to a whole phrase or a meaning or a sentence. Right. And, and, and use that, especially when it comes to, um, it could, could there be portions of the language, and this is part of the second question that are actually singing. Nothing to do with communication, just plain singing, going, um, comfort noises, things like that. Or just singing in rhythmic dawn's hypnotic tons, right?

Speaker 1:          46:11          Yeah. That's a second question. Oh, so there's, there's a lot of, um, hopefully you'll get this, this algorithm. We get some of that, uh, both, both issues and there's a lot of work on Zebra finch, that sort of thing as far as, you know, just bird song and, uh, we can get, when we're doing, I'm a human speech, we get things like coughs and sneezes and, and uh, just a microphone against that person's lapel. Um, and a lot of that stuff is actually useful because he can get rid of the noise. You characterize the types of noise and get rid of it. Right. And there's anybody to speech recognition. It tells you that's important. Um, now as far as you know, how much of these individual units mean, um, is it, is it a composition or is it wholly on their own? Don't know regarding that whistle question.

Speaker 1:          46:57          So your question about is it Chinese mandarin where you have the different inflections of the tunnel? Right? So that would be something that the system could pick up as far as if it was a combination, you know, the tail end went up, it went down, etc. Now, decades ago, people thought about this issue and they looked at whistle languages in humans, in the mountainous areas, for example, Pyrenees. And I'm just a few other areas in the world to look at how much content goes between those mountains when one guy is whistling to another and coming home. So some of it ended up being contextual. Like they, it was Joe over

Speaker 3:          47:30          here and Fred was whistling and they knew it was mealtime, so he was probably gonna go get some bread and you know, get some food for dinner, whatever. But, um, that has been speculated on, but that the part of the trick here is that whistles are a really small part of their communication and these other kinds of sounds, which are the really hard ones to categorize. The burst pulses that are more like phonemes really, they are the ones that we were really lacking an understanding of their function, their categories and all that. So that's part of why we started because people have really gone through the whistle stuff a lot.

Speaker 1:          48:03          Historically, logic helps work on parade dogs. It's really quite astonishing in 200 milliseconds. Point two seconds, right? These, uh, uh, pray dogs are our encoding. What type of Predator is there? So you can actually, you can actually watch this yourself. Um, if it's a hawk, right? And they get along well, I call everybody goes into the borough. If it's a snake, they'll sit there and stand and stand up and look at it. Right? And so conn has shown that they can encode in this in this 0.2 seconds, the shape and size of the object that's impinging upon their territory. Right? Uh, so, so if these prairie dogs can they do this and this is higher harmonics, if these prairie dogs can do it right, what can these dolphins do? And so one of the things we've been looking at is, uh, one of the grand proposal we put together is trying to take all the stuff that you painfully put together over 20 years.

Speaker 1:          48:55          All these experiments and all this data is collected. Can you actually take a look at his data where we know what the answer is and rediscover it using our techniques? Um, we haven't gone there yet. That was part of a grant proposals being being evaluated right now. But, um, uh, we're hoping to, to not only just do Denise's work, but a lot of these other, uh, animal vocalizations specialists see if I can get their databases and see if we can accelerate stuff for them. And for the ones we know the answers to see if we can see if it actually pulls them out as well.

Speaker 6:          49:26          You mentioned in the dauphine diary that very occasionally orthodontics spottier dauphine lots to babysit the cuffs. You also mentioned it's not really a great idea.

Speaker 3:          49:39          I don't know that they will give us the, yeah.

Speaker 6:          49:42          So I don't know if like any incident has happened and like why do you print it? Do you,

Speaker 3:          49:48          so shark attacking diocese, what he's referring to is, um, we described, I described in my book how, cause we know that I've grown up with a lot of these dolphins, other grandmothers now, but I knew them when they were born and I know their grandkids and all that. And so occasionally, occasionally they'll come to the boat or will be anchored and we'll be in the water where the, and the moms will leave their calves with us and they'll take off. I like got, that might not be a very smart thing to do, but they figured, well, I got to go forward. So take care of junior for a while. We're like, yeah, right. We're going to, what are we going to do? You know, if a shark shows up as a walking. But um, I mean they don't go far. It's not like if the calf made a noise, they wouldn't come swimming and running. You know, but, uh, yeah, it's, what I find interesting about that is that it's kind of like they are incorporating you in their society and they're smart enough to not put their calf in danger or at least I hope so because I wouldn't be very survival driven. Um, but it is interesting and that, that's why, you know, I think the link is there to really go further. It's just said, you know, we have to meet them. We have to bridge the gap with her technology I think is the trip.

Speaker 7:          50:58          I'm wondering like, since being observing golfing's for such a long time, um, do you observe sound like young doffing um, like ball without such a full pattern of doffing for Kevin Murray? Then gradually they kind of learned this, um,

Speaker 3:          51:16          you know, uh, the watching young dolphins develop has actually been one of the biggest things for our work because when I first went out there, you know, I'd see the adults fighting or the adults meeting. And so you get a sense of, oh, that's what they do. And that's the ritual. But when you see the little guys, they're trying and they're fumbling and they're learning and they're getting feedback from their cohort. So yeah, absolutely. They're learning social rules. They're learning sound restrictions, I'm sure when, when to make sound, when to not like we've had a couple young calves that were real squeakers, we'd hear them coming from miles away, you know, and we could tell who it was a little freedom. They'd be like, shut up. You're shark bait. Stop it. You know. And sure enough, two months later she shows up with a big chunk out of her fluke, someone bitter.

Speaker 3:          51:59          And then it happened again. And it's like, come on, it's quiet down. And finally her mom, I guess convinced her to shut up and not be so vocal. But yeah, they absolutely go through the learning process and they don't always learn sufficiently and they do die and get in trouble of course. But yeah, absolutely. And, um, other researchers have taken a dolphin vocalization databases and apply information theory to them and looked at, do little dolphins babble like little human kids do, and then gradually refine their sounds. And it in fact they do. The information is constrained and given feedback. And so, yeah, absolutely. It's a process. I'm just wondering if you have any sense of the upper limit of the frequency dolphins are able to hear it. And also are there any differences in the way a dolphin might here relative to how a human hears and do you capture that?

Speaker 3:          52:49          So dolphins have been tested at least some species and had audiograms made and it looks like they're hearing has a couple of different sensitivity peaks. So they hear best around 30 to 40 kilohertz and then they have another bi-modal peak at about a hundred 120 kilo kilohertz. Now, researchers haven't measured much higher than that because it looks like it drops around 120 but on the recording side, just recently, last few years, um, people have recorded sounds up until about 240 kilohertz. So for some reason they're making sounds higher than we think they can hear. So quite sick question mark. Um, and yeah, of course their ears are completely different. They had mammalian ears, they have certain specializations. Of course, a lot of high frequency, you know, inner hair cells and a lot of turns in the Cochlea and stuff. Um, yeah, so there are adapted to their environment.

Speaker 3:          53:39          Certainly. Um, they can produce really loud sounds like 180 decibels, 220 decibels, which underwaters like a blasting cap, you know, it's a lot of energy because of the medium. Right. And they do crazy things with her sounds, which I could get into in some crazy deal till like they can phase out signals to block, you know, certain parts of the bandwidth. So there's some pretty complicated stuff going on there, which are just starting to get at, we're seeing some crazy stuff at work. Yeah. We don't understand yet. I was like, you know, was that intentional or not? Cause it's really, really on the stuff they're doing acoustically is really wacky.