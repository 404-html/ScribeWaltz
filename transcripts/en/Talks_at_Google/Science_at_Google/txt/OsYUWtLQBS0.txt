Speaker 1:          00:00:06       Thanks for coming out. It's a, it's good to be here. As Eric said, I'm a philosopher thinking about consciousness. Coming from a background in the sciences and math that always struck me that the most interesting and hardest unsolved problem in the sciences was the problem of consciousness and way back 25 years ago when I was in, when I was in Grad school. It seems to be the best way to come at this from a big picture perspective was to go into philosophy and think about the foundational issues that arise and thinking about consciousness from any number of different angles, including the angles of neuroscience and psychology and AI. In this talk, I'm going to present a slightly different perspective on the problem after laying out some background, the perspective of what I call the, uh, metal problem of consciousness. I always liked the idea that, you know, you approach a problem by stepping one level up, taking the, uh, the Metta perspective.

Speaker 1:          00:01:10       Um, I love this quote. Anything you can do, I can do Metta. I have no idea what the origins was. I liked the fact this is attributed to to Rudolph CONAP, one of my favorite philosophers, but anyone who knows condoms work, it's completely implausible. He would ever say anything so frivolous. It's also been attributed to my thesis, advisor. Dot. Hofstatter author of Godel, Escher, Bach, and a big, uh, a big fan of the Meta perspective. But he assures me he never said it either. Um, but the Meta perspective on anything is stepping up. I'm stepping up a level. The met a problem as I think about it is it's called the Meta problem because it's a problem about a problem. And met a theory is a theory about a theory. Met a problem is a problem about a problem in particular. It's the problem of explaining why we think there is a problem about consciousness.

Speaker 1:          00:02:02       So there's a first order problem, the problem of consciousness. Today I'm going to focus on a problem about it, but I'll start by introducing the first order problem itself. The first order problem is what we call the hard problem of consciousness. It's the problem of explaining why and how physical processes should give rise to conscious experience. Um, you know, you've got all of these neurons firing and you're in your brain bringing it about all kinds of sophisticated behavior. Um, we can get to beat on explaining our various responses. But there's this big question about how it feels from the first person point of view. That's the subjective experience. I'd like to see illustration of the hard problem of consciousness. It seems to show social show someone's hair catching fire, but it's, I guess it's a metaphorical illustration of the subjective perspective. So the hard problem is concerned with what philosophers call consciousness.

Speaker 1:          00:03:00       The word consciousness is ambiguous a thousand ways, but phenomenal consciousnesses, what it's like to be a subject from the first person point of view. So a system is phenomenally conscious. If there's something that's like to be it, a mental state is predominantly conscious. If there's something, it's like to be in it. So the thought is there are some systems, so there's something that's like to beat that system. There's something that's like to be me. I presume there's something gets like to be you, but presumably there's nothing. It's like to be this lectern. As far as we know, the lectern does not have a first person perspective. Ah, this, this phrase was made famous by my colleague Tom Nagle at Nyu who back in 1974 wrote an article called what is it like to be a bat and the general idea of is what's very hard to know what it's like to be a bat from the third person point of view.

Speaker 1:          00:03:55       Just looking at it as a human who has different kinds of experience, but presumably very plausibly there is something. It's like to be a bat. The Bat, his car, it's just, it's having subjective experiences just of a kind very different from ours. In human subjective experience, consciousness divides into any number of different kinds or aspects like different tracks of the inner movie of consciousness. We have visual experiences like the experience of seeing these colors blue and red and green from the first person point of view and the depth. There are sensory experiences like the experience of my voice, experiences of taste and smell, their experiences of your buddy, a feeling pain or orgasms or hunger or a tickle or something. They all have some distinctive first person quality mental images like recalled visual images, emotional experiences like uh, experience of happiness or anger. And indeed we all seem to have this stream of a current thought where at the very least we're kind of chattering away to ourselves and reflecting and deciding, oh, these are aspects of subjective experience.

Speaker 1:          00:05:10       Things we experience from the first person point of view. And I think these subjective experiences or at least on the face of it, data data for the science of consciousness to explain these are just facts about us that we're having these subjective experiences. If we ignore them, we're ignoring the data is if you catalog the data that say the science of consciousness needs to explain. There were certainly facts about our behavior and how we respond in situations or a fact about how brain facts about how our brains is working. There also facts about how subjective experiences and on the face of it their data and as these data, the pose, what I call the hard problem of consciousness. But this gets a contrasted move. The easy problems, the so called easy problems of consciousness, which are the problems of explaining behavioral and cognitive functions. Um, objective things you can measure from the third person point of view, typically tied to behavior, perceptual discrimination, say of a stimulus.

Speaker 1:          00:06:14       Um, I can discriminate to different things in my environment. I can say that's red and that's green. I can integrate the information, say about the color and the shape. I can use it to control my behavior, walk towards the red one rather than the green one. I can report it, say that's red and uh, and so on. Those are all data too for science to explain, but we've got a bead on how to explain those. They don't seem to pose as big a problem. Um, why we explain those easy problems by finding a mechanism, typically a neural or computational mechanism that performs the relevant function to explain, you know, how it is that I get to say there's a red thing over there or walk towards it. Will you find the mechanisms involving perceptual perceptual processes and action processes in my brain that leads to leads to that behavior, find the right mechanism that performs a function?

Speaker 1:          00:07:12       You've explained what needs to be explained with the easy problems of consciousness, but for the hard problem, for subjective experience, it's just not clear that this standard method works, that it looks like explaining all that behavior still leaves open. A further question, why does all that give you subjective experience? You know, explain the uh, the reacting, the responding, the controlling, the reporting and so on. It still leaves open the question, why is all that accompanied by subjective experience? Why doesn't that go on in the dark without consciousness, so to speak? That seems to be what the philosopher Joe Levine is cold a gap here and explanatory gap between physical processes and subjective experience. At least our standard kinds of explanations. Which worked really well for the easy problems of behavior and so on. Don't obviously give you a connection to the subjective aspects of experience. And there's been a vast amount of discussion of these things over, I mean, well for centuries really, but it's been particularly active in recent decades.

Speaker 1:          00:08:23       Philosophers, scientists, all kinds of different views. I mean, philosophically you can divide approaches to the hard problem into at least two classes. One is an approach on which consciousness is taken to be somehow irreducible and primitive. We can't explain it in more basic physical terms. So take it as a kind of primitive, and that might lead to dualist theories of consciousness. We are consciousness is somehow separate from an interacts with the brain. Recently, very popular has been the class of pence, Haikus, theories of consciousness. I know Galen Strawson was here a while back talking. Uh, he, he very much favors [inaudible] theories where consciousness is something basic in the universe underlying matter. And indeed, there are idealists theories where consciousness underlies the whole universe. Um, so these are all extremely speculative but interesting, uh, views that I've explored myself. There are also reductionist theories of consciousness from functionalist approaches where consciousness is just basically taken to be a, you know, a giant algorithm or computation by logical approaches to consciousness.

Speaker 1:          00:09:27       My colleague Ned Block was here. I know talking about a neuro biology, neuro biology based approach approaches versus not the algorithm that matters, but the biology it's implemented in. And indeed the kinds of quantum approaches that people like Roger Penrose and Stuart Hameroff have made famous. I mean, I think there's interesting things to say about all of these approaches. I think that right now at least most of the reductionist approaches leave a gap, but the number reductions to purchase have other problems and seeing how, how it all works today I'm going to take a different kind of approach, this approach through the metal problem. One way to motivate this, um, is too, I often get asked, you know, well, okay, you're a philosopher. It's fine. You get to think about, uh, these, these things like the hard problem of consciousness. How can I, as a scientist or an engineer or an AI researcher, how can I do something to kind of, uh, to contribute, um, to help get at this, at this hard problem of consciousness?

Speaker 1:          00:10:28       Is this just a problem for philosophy? I mean, for me to work on it, see, as an AI researcher, I need something I can operationalize. Um, something I can work with and try to program. And as it stands, it's just not clear how to do that with the, uh, with the hard problem. I mean, if you're a neuroscientist, there are some things you can do. Um, you can say work with humans and look at their brains and look for the neural correlates of consciousness, the bits of the brain that go along with being conscious. Because at least with humans, we can take as a background assumption, a plausible background assumption that the system is conscious for AI. We can't even do that. We don't know which AI systems we're working with. There are conscious, we need some operational criteria in AI. We mostly work on bottling things like, you know, behavior and objective functioning for consciousness.

Speaker 1:          00:11:13       Those are the easy problems. So how does someone coming from this perspective make a connection to the hard problem of consciousness? Well, one approach is to work on certain problems. Among the easy problems have behavior that shed particular light on the Hud problem. And that's going to be the approach that I look at today. So the uh, the guiding, the key idea here, is there a certain behavioral functions that seem to have a particularly close relation to the hard problem of consciousness in particular, we say things about consciousness. We make what philosophers call phenomenal reports, verbal reports of conscious experiences. So I'll say things like, I'm conscious, I'm feeling pain right now and so on. I mean, maybe the consciousness and the pain are subjective experiences, but the reports, the Adrienne says, I am conscious. Well that's a bit of behavior in principal explaining those is among the easy problems.

Speaker 1:          00:12:18       Subjectively measurable response. We can find a mechanism in the brain that produces it. And among our phenomenal reports, there's this special class we can call the problem reports, reports expressing our sense, the consciousness poses a problem. Now, admittedly, not everyone makes these reports, but they seem to be fairly widespread among, especially among philosophers and scientists thinking about these things. But furthermore, um, just centered, it's fairly easy to find and a very wide class of people who think about consciousness, people say things like, there is a problem of consciousness, a hard problem on the face of it. Explaining behavior doesn't explain consciousness. Consciousness seems nonphysical. How would you ever explain the subjective experience of red and so on? It's an objective fact about us, at least about some of us, um, that we make those reports and that's a fact about human behavior. So the matter problem of consciousness then at a second approximation is roughly the problem of explaining these problem reports.

Speaker 1:          00:13:29       Explaining, you might say the conviction that we're conscious and the consciousness is puzzling. And what's nice about this is that although the hard problem is this, you know, airy fairy problem about subjective experience, it's hard to pin down. This is a puzzle ultimately about behavior. So this is an easy problem. One that ought to be open to those standard methods of explanation in the cognitive and brain sciences. So there's a, there's a research program, um, as a research program here. So I like to think of the metal problem is something that can play that role I talked about earlier. If you say an AI researcher thinking about this, the metal problem is an easy problem. The problem about behavior, this is closely tied to the hog problem. So it's something we might be able to make some progress on using standard methods if they came out algorithms and computation.

Speaker 1:          00:14:15       So thinking about brain processes and behavior while still shedding some light, at least indirectly on the hard problem. It's more tractable than the hard problem, but solving it off to shed light on the hard problem. And today I'm just going to kind of lay out the research program and talk about some ways in which it might potentially shed some light. This is interesting to a philosopher because it's a, it looks like an instance of what people symptoms called Genia. Logical analysis goes back to Friedrich nature on the, you know, the genealogy of morals. Instead of thinking about what's good or bad, let's look at where our sense of good or bad came from the genealogy of it all and evolution or in culture or in religion and uh, you know, people think of Ge. Do you need a logical approach to God instead of thinking about does God exist or not?

Speaker 1:          00:15:01       Let's look at where our belief in God came from. Maybe there's some evolutionary reason for why people believe in God. There's Oftenly. It's not always, but often leads to a kind of debunking of our beliefs about those domains. Explain why we believe in God in evolutionary terms. No need for the God hypothesis anymore. Explain I'll moral beliefs and say evolutionary terms. Maybe no need to take morality quite so seriously. So some people at least are inclined to take an approach like this with consciousness too. If you think about the metal problem, explaining our beliefs about consciousness that might ultimately debunk our beliefs about consciousness. This leads to a philosophical view, which is recently attracted a lot of interest, a philosophical view called illusionism, which is the view that consciousness itself is an illusion. Or maybe that the problem of consciousness is an illusion, explained the illusion and we dissolve the problem.

Speaker 1:          00:16:03       I take the, in terms of the head of the Metro problem, that view roughly comes to solve the matter problem. It will dissolve the hard problem. Explain why it is that we say are these things about consciousness. While we say I am conscious, while we say consciousness is puzzling. If you can explain all that in say algorithmic terms, then you'll remove the underlying problem because you'll have explained why we were puzzled in the first place. Actually walking over here today, I noticed that just a couple of blocks away, we have the Museum of illusions so I'm going to check that out. That out later on we know if illusionism Israel added to all those perceptual illusions, it's going to be the problem of consciousness itself. It's roughly and illusion thrown up by having a weird sick kind of self model with a certain kind of algorithm that are attributes to ourselves, special properties that we don't have.

Speaker 1:          00:16:54       So one line on the matter problem is the uh, is the illusionist line solved the matter problem. You'll get to treat consciousness as an illusion. And that's actually a view that has many antecedents in the history of philosophy. One way or another, even a manual cans is great critique of pure reason had a section where we talked about the self or the soul as a transcendental illusion. We seem to have this in the visible soul, but that's the kind of illusion or not by our cognitive processes. The Australian philosophers, uh, Alan Place and David Armstrong had versions of this that I might touch on a bit later. Uh, Daniel Dennett, um, leading reductionist thinker about consciousness has been pushing for the last couple of decades. The idea that consciousness involves a certain kind of user illusion. And most recently the British philosopher Keith Frankish has been really pushing, um, illusion. There's as a theory of consciousness.

Speaker 1:          00:17:51       Here's a book, uh, that, uh, centering around the, uh, uh, pay for by Keith Frankish on illusionism as a theory of consciousness that I recommend to you. So one way to go with the metal problem is the direction of illusionism. But one nice thing about that many people find illusionism completely unbelievable. They find you, how could it be that consciousness is an illusion? Look, we just have these subjective experience. It's a datum about our nature. And I confess, I've got some sympathy with that reaction. So I'm not an illusion. This myself, I'm a realist about consciousness and the philosophers sense where a realist about something because someone who believes that thing is real. I think consciousness is real. I think it's not an illusion. I think that solving the metal problem does not dissolve the hard problem. But the nice thing about the met problem as you can proceed on it to some extent, to some extent, at least in the initial neutrality on that question, is conscious.

Speaker 1:          00:18:45       It's real or is it an illusion? It's just, it's a basic problem about our objective functioning and these reports. What explains, explains those? So there's a neutral research program here that both realists illusionists people of all kinds of different views of consciousness can explain and then we can come back and look at the, uh, the philosophical consequences. Why I'm not an illusion. So I think consciousness is real. They're going to say, if I do feel the temptation of illusionism I find as a really intriguing and in some ways attractive, you just fundamentally unbelievable. Um, nevertheless, I think that the matter problems should be attractable problem solving. It will shed my what's at the very least we'll shed much light on the hard problem of consciousness even if it doesn't solve it. If you could explain how conviction that we're conscious somehow this sauce, the roots of our conviction that we're conscious must have something to do with consciousness, especially if consciousness is real.

Speaker 1:          00:19:42       So I think it's very much, um, a good research program for people to explain. So then I'll move on now to just outlining the research program a little bit more and then talk a bit about potential solutions and an impact on theories of consciousness before wrapping up with, um, just a little bit more about illusionism. So there's metal problem which I've been pushing recently, opens up attractable empirical research program for everyone. Reductionists number actionists illusionists non illusionists. We can try to solve it and then think about the philosophical consequences. Um, now what is the matter of problem? Well, the way I'm going to put it is it's the problem of topic neutrally explaining problem intuitions or else explaining why that can't be done. And I'll unpack that now. Um, unpack all the pieces of that right now. First starting with problem intuitions, what our problem intuitions.

Speaker 1:          00:20:43       Well, those are, you know, there are the things we say, there are things we think I say hi consciousness seems irreducible. I might think consciousness is irreducible. People might be disposed, have a tendency to say or think those things. Problem and tuition is I'll take to be roughly that tendency. We have dispositions to uh, to say and think certain things about consciousness. What are the core problem? Intuitions? Well, I think they break down into a number of different kinds. There's the intuition that consciousness is nonphysical. We might think of that as a metaphysical intuition about the nature of consciousness. Their intuitions about explanation. Consciousnesses, hard to explain, explaining behavior. It doesn't explain consciousness. There are intuitions about knowledge of consciousness. Some of you may know the famous thought experiment of Mary and the Black and white room who knows all about the objective nature of color, vision and so on, but it still doesn't know what it's like to see red.

Speaker 1:          00:21:38       Do you see as red for the first time she learns something new. That's an intuition about knowledge of consciousness through what philosophers called modal intuitions about what's possible or imaginable. One famous case is the case of, um, of a Zombie, a creature who's physically identical to you and me, but not conscious or maybe an AI system, which is functionally identical to you and me, but not conscious. That at least seems conceivable to many people. So this is the philosophical Zombie. Unlike the zombies in movies which, you know, have weird behaviors and go after brains. And so on. The Philosophical Zombie is a creature that seems at least behaviorally, maybe physically like a normal human, but doesn't have any conscious experiences. All the physical states, none of the mental states. And it seems to many people that's at least conceivable. We're not zombies. I don't think anyone here is a Zombie.

Speaker 1:          00:22:28       I hope. But, um, nonetheless it seems that we can make sense of the idea. And one way to pose the hard problem is why are we not Zombie? So this imagine inability of zombies. It's one of the intuitions that gets the problem going and then you can go on and catalog more and more situations about the distribution of conscious, maybe the intuition that robots won't be conscious. That's an optional one. I think, uh, or consciousness matters morally in certain ways and the list goes on. So I think there's an interdisciplinary research program here of working on those intuitions about consciousness and trying to explain them. Experimental psychology and experimental philosophy and newly active area can study people's intuitions about consciousness. We can work on models of these things, computational models on neuro biological models of these intuitions and reports. And indeed, I think there's a lot of room for philosophical analysis.

Speaker 1:          00:23:22       And this is just starting to be a program of people doing these things in all these fields. I mean, it is an empirical question how widely these intuitions are shared. You might be sitting there thinking, come on. I don't have any of these integrations. Maybe this is just you. I mean, my sense is from the psychological study to date, it seems that some of these intuitions about consciousness or at least very widely shared at least as dispositions or intuitions, although they're often overwritten on reflection, but you know the current data on this is somewhat limited. There is a lot of empirical work on intuitions about the mind concerning things like belief, like when the kids get the idea that your beliefs about the world can be false concerning the way your self persist through time. You know, could you exist after the death of your buddy?

Speaker 1:          00:24:09       Well, consciousness is concerned. There's work on the distribution of consciousness. Could a robot be conscious? Could a group be conscious? He's a book by Paul Bloom, Descartes' baby, the catalogs. A lot of this interesting work making the case that many children are intuitive journalists thinks they're naturally inclined to think there's something nonphysical about the mind. So far. Most of this work has not been so much on these core problem intuitions about consciousness, but there's work developing in this direction. Now Sarah got leave and Tonya Lombroso have a very recent article called can science explain the human human mind on people's judgments about when various mental phenomena are hard to explain and they seem to find that yes, subjective experience and things which have to which people have privileged first person access seem to pose the problem big time. So there's the beginning of a recess program here.

Speaker 1:          00:25:00       I think there's room for a lot more. The topic neutrality part where we, when I say we're looking for a topic neutral explanation of problem integrations, that's roughly just say an explanation that doesn't mention consciousness itself. It's put in neutral terms is neutral on the existence of consciousness. The most obvious one would be something like an algorithmic explanation I get to here is the algorithm the brain is executing. That generates our conviction that we're conscious and a reports about consciousness. There may be some time between an algorithm and consciousness, but to specify the algorithm, you don't need to make claims about consciousness. So the algorithmic version of the matter problem is roughly fund the Algorithm that generates our problem integration. So that's I think in principle uh, uh, research program that, you know, maybe in AI researchers in combination with say psychologists, the psychologists could help isolate data about the way that uh, the human beings are doing it, how these things are generated in the humans and, and the I researched, I couldn't try and see about implementing that algorithm in machines and see what results and I'll talk about a little bit of research in this direction in just a moment, but okay, now I want to say something about potential solutions to the problem.

Speaker 1:          00:26:16       Like I said, this is a big research program. I don't claim to have the solution to the metal problem. I've got some ideas, but I'm not going to try and lay out a major solution or just to hear a few things, which I think might be parts of a solution to the problem. Many of which you've got antioxidants here and there in scientific and philosophical discussion. Some promising ideas include retrospective, muddles, phenomenal concepts, introspective capacity, the sense of acquaintance. Let me just say something about a few of these. We one starting idea that almost anyone's going to have here as somehow models of ourselves are playing a central role here. You know, human beings have models of the world, um, you know, naive physics and now you've psychology muddles of other people and so on. We also have models of ourselves. It makes sense for us to have models of ourselves and our own mental processes.

Speaker 1:          00:27:11       This is something that the psychologist, Michael Graziano has written a lot on. We have internal models of our own cognitive processes, including those tied to consciousness. And somehow something about our introspective models explains our sense a that we are conscious and B, that this is distinctively problematic. And I think you know, anyone thinking about the metal problem, this has got to be at least the first step. We have these, uh, these introspective models. If you were an illusionist, there'll be false modals. If you're a realist, they needn't be forced models. But at the very least these introspective modals are involved, which is fine. Okay. But the devil's in the details. How do they work to generate this, uh, this problem and number of philosophers visit have argued, we have special concepts of consciousness, introspective concepts of these special subjective states. People call these phenomenal concepts, concepts of phenomenal consciousness.

Speaker 1:          00:28:06       And one thing that's special is these concepts of somehow independent of our physical concepts. They explain, you know, we've got one set of physical concepts of modeling the external work world. We've got one set of introspective concepts from modeling our own mind. And these concepts just by virtue of the way they're designed is somewhat independent of each other. And that partly explains why consciousness seems to be independent of the physical world intuitively. And so maybe that independence of phenomenal concepts could go some distance to explaining our problem reports. And I think there's got to be something to this as well. At the same time, I don't think this goes nearly far enough because you know, we, we have concepts of many aspects of the mind, not just of the subjective experiential pass but things we believe and things we desire and so on. I believe that Paris is the capital of France.

Speaker 1:          00:28:59       That's part of my internal self model, but that doesn't seem to generate the hard problem in nearly the same way in which say the experience of red does. So a lot more needs to be said about what's going on in cases like having the experience of red and having the sense that that generates a gap. So it doesn't generalize to everything about the mind. Um, some people have thought that what we might call introspective or passively plays a role that when we introspect what's going on in our minds, we don't have access to the underlying physical states. We don't see the neurons in our brains. We don't see that consciousness is physical. So we see it as nonphysical. Most recently, um, uh, the physicist Max Tegmark has argued in this direction saying somehow consciousness is sub straight independent. We don't see the substrate. So then we think, oh, maybe it can flip free of the substrate.

Speaker 1:          00:29:52       Um, I'm strong, made an analogy with the, the case of someone in a circus where, um, the headless person illusion where you know, you don't see someone's there with the veil across their head, you don't see their head. So you see them as having no head here as a 19th century booth at a circus, so called headless woman. There's a veil over her head. You don't see the heads. So somehow it looks at least for a moment, like the person doesn't have a head. So I'm strongly as maybe that's how it is with consciousness. You don't see that it's physical, so you see it as nonphysical. Um, but still the question comes up, how do we make this a, how do we make this inference? I mean, there's something that special goes on in cases like say color and taste and so on. Color experience seems to attribute primitive properties.

Speaker 1:          00:30:41       Two objects like redness, greenness, and so on. When in fact, in the external world, at the very least, they have complex reduceable properly. Somehow. I'm internal, but our models of color treat colors like red and green as if they are primitive things. It turns out to be useful to have these models of things. We treat certain things as primitive even though they're reducible. And it sure seems that when we experience colors, we experienced green this as a primitive quality even though it may be a very, very complex reduceable property. That's something about our model of colors. The philosopher Wolfgang Schwartz has tried to make an analogy with sensor variables in say image processing. You've got a, you've got some visual sensors and a camera or something and you need to process the image. We've got some variables, some sense of variables to represent. Um, you know, the sensory inputs at the various sensors are getting and you might treat them as a primitive dimension because that's the most useful way to treat them.

Speaker 1:          00:31:38       You don't treat them as certain amounts of lights or photons fire and you don't need to know about that. Use The sensor variables and treat them as a primitive dimension and all that will play into a model of these things as primitive. Maybe taking that idea and extending it to introspection, you know, somehow these, these conscious states are somehow like sensor variables in our model of the mind and somehow these internal models give us the sense of being acquainted with primitive concrete qualities and of our awareness of them. Is it still just laying out? I don't think this is still the it actually explaining a whole lot, but it's laying out. It's narrowing down what it is that we need to explain to solve the major problem, but just to put the pieces together. Here's a little summary. One thing I like about this summary as you can read it in either an illusionist tone of voice as an account of the illusion of consciousness.

Speaker 1:          00:32:33       All of this is how our false introspective models work or in a realist tone of voice as an account of our, our true correct models of consciousness, but we can set it out in a way which is neutral on the two and then try and figure out later what are these models are correct as the realist says or incorrect. As the illusionist says, we have introspective, muddles deploying introspective concepts of our internal states that are largely independent of our physical concepts. These concepts are introspectively okayk not revealing any of the underlying mechanisms are perceptual muddles perceptually attribute primitive perceptual qualities to the world and are introspective muddles attribute primitive mental relations to those qualities. These models produced the sense of acquaintance both with those qualities and with our awareness of those qualities. Like I said, this is not a solution to the metal problem, but it's trying at least to pin down some some parts of the, of the roots of those intuitions and to narrow down what needs to be explained to go further.

Speaker 1:          00:33:45       You want, I think to test these explanations both with psychological studies to see if this is plausibly what's going on in humans. This is the kind of thing which is the basis of our intuitions and computational models to see if for example, you could program this kind of thing into an AI system and see if it can generate somehow qualitatively similar reports and intuitions. You might think that last thing is a bit far fetched right now. But I knew, I knew of at least one instance of this research program, which has been put into play by a Luke Millhauser and buck sluggers, two researchers at open philanthropy, very interested in um, in Ai and consciousness. They actually built, they took some ideas about the metal problem from something I'd written about it and from something that the philosopher from swap camera had written about it. It's a couple of basic ideas about where problems situations might come from.

Speaker 1:          00:34:40       And they tried to build them in to where computational model, they built a little a three, they built a little software agent which had certain axioms about cowers and how they work. You know, there's red and there's green and certain axioms about their own subjective experiences of colors. And then they combined it with little theorem prover and, and they saw what, what did this little software agent come up with and it came up with claims like, Hey, well my own, my experiences of color are distinct from any physical state and so on. I mean, okay, they cut a few corners. This is not a, this is not yeah, to truly, uh, uh, convincing sophisticated, uh, model of everything going on in the, uh, in the human mind. But it, but it shows that there's a research program here, um, of trying to find the algorithmic basis of these states.

Speaker 1:          00:35:33       And I think as more sophisticated models, uh, develop, we might be able to use these to kind of provide a way in for AI researchers in thinking about this topic. Of course there is the question, you model all this stuff better and better in a machine then is the machine actually going to be conscious or is it Jessica [inaudible] found self models that replicate what's going on in, uh, in humans. So, you know, some people have proposed an artificial consciousness test. Uh, Aaron Sloman, Susan Snyder at Turner have suggested that somehow that if a machine, it seems to be puzzled about consciousness in roughly the ways that we are. Maybe that's actually a sign that it's conscious. So you know, if a machine actually looks to ask is if it's puzzled by conscious consciousness, is that a sign of consciousness? These people, this is suggested as kind of Turing test for machine consciousness, find machines which are conscious like we are of course the opposing point of view is going to be another machine is not actually conscious.

Speaker 1:          00:36:31       It's just like the machine that studied up for the cheering test by reading the talk like a human book. I was like, damn, do I really need to confess to a convince those humans that I'm conscious by row by, uh, by replicating a little those ill-conceived confusions about consciousness? Well, I guess I can do it if I, uh, if I need to. Anyway, I'm not going to settle this question here, but I do think that if we somehow find machines being puzzled, it's not, it won't surprise me that once we actually have serious AI systems which engaging in natural language and muddling of themselves in the world, they might well be natural find themselves saying things like, well, yeah, I know in principle I'm just a set of silicon circuits. But I feel like so much more. Um, I think that might tell us something about consciousness.

Speaker 1:          00:37:18       Let me just say a little bit about theories of consciousness. I do think a solution to the metal problem and a solution to the hard problem ought to be closely connected. The allusionist says, solve the major problem. You'll dissolve the hard problem, but even if you're not an illusionist about consciousness, there ought to be some link. So here's the thesis. Whatever explains consciousness should also partly explain our judgments. Now reports about consciousness. The rationale here is we'd just be very strange if these things were independent, if the basis of consciousness played no role in our judgments about consciousness, so they can use this as a way of evaluating or testing. Theories of consciousness for theory of consciousness says mechanism. M is the basis of consciousness. That m should also partly explain our judgments about consciousness, whatever the basis is, or to explain the reports and you can use this.

Speaker 1:          00:38:19       You can bring this to bear on various extant theories of consciousness. Here's one famous current theory of consciousness. Integrated information theory developed by Giulio Tononi and colleagues at the University of Wisconsin. Um, Tony says, integrate the basis of consciousness is integrated information, a certain kind of integration of information, um, for which Tony has a measure that he calls fi basically when your fire as high enough you get, you get consciousness. A consciousness is high fi. Um, and you know, there's a mathematical definition but I won't go into it here, but uh, it's, it's a really interesting theory. So here's that basically analyzes and network property of, of systems of units and it's got a informational measure called five supposed to go with consciousness question. How does integrated information as the basis of consciousness, it ought to explain problem reports, at least in principle challenge. How does that work?

Speaker 1:          00:39:20       And it's at least far from obvious to me how integrated information we'll explain the problem reports. It seems pretty dissociated from them. I mean Antonis view, you can have simulations of systems with high fi that have zero fi they'll go about making exactly the same reports, but without consciousness at all. So fires at least somewhat to sociable. You get systems very high fi no tendency to um, to report. Maybe that's less worrying anyways. Here's a challenge for this theory. For other theories. Explain not just how high fi gives you consciousness, but how it plays a central role on the algorithms that generate problem report. Something similar goes for many other theories, biological theories, quantum theories, global workspace and so on. But let me just wrap up by saying something about the issue of illusionism that I was talking about near the start. Again, you might be inclined to think that this approach through the matter problem tens at least very naturally to lead to illusionism and I think it can be, it certainly provides, I think some motivation for illusionism the view that consciousness doesn't exist.

Speaker 1:          00:40:31       We just think it does on this view. Again, a solution to the matter problem dissolves the hard problem. So here's one way of putting the case for illusionism. If there's a solution to the metal problem, then there's an explanation of our beliefs about consciousness that's independent of consciousness. There's an algorithm that explains I beliefs about consciousness, doesn't mention consciousness, arguably could be in place without consciousness. Arguably that kind of explanation could debunk our beliefs about consciousness. The same way that perhaps explaining beliefs about God in evolutionary terms might debunk belief in God. It's certainly doesn't prove that God doesn't exist. You might think that if you can explain our beliefs in terms of evolution and somehow it removes the justification or the rational basis for those beliefs. So something like that I think can be applied to consciousness to, and there's a lot to be said about analyzing the extent to which this might debunk the beliefs.

Speaker 1:          00:41:30       On the other hand, the case against illusionism, is it very, very strong for many people and the underlying worry, is it something like illusion doesn't miss completely unbelievable. It's just a manifest fact about ourselves that we have conscious experience. We, uh, we experienced red, we feel pain and so on. And to deny those things is to deny the data. No, you know, the dielectric here is complicated. The illusion will come back and say yes, but I can explain why it illusionism is unbelievable. These models, we have these self models of consciousness as so strong that, you know, they were just wired into us by evolution and they're not, they're not models we can, we can get rid of. So my view predicts that my view is unbelievable and the question is what the diarrhea electrical situation is, uh, is complex and interesting. But maybe I could just wrap up with two expressions of absurdity on either sides of this question.

Speaker 1:          00:42:22       The illusionist and the, uh, the anti illusionists both finding absurdity in the other person's views, his, uh, his Galen Strawson, um, who was here, Galen, his view is very much that illusionism is totally absurd. In fact, he thinks it's the most absurd view that anyone has ever held there occurred in the 20th century, the most remarkable episode in the whole history of ideas. The whole history of human thought and number of thinkers denied the existence of something we know with certainty to exist consciousness. He thinks this is just a sign of incredible philosophical pathology. Uh, here's the rationalist philosopher Eliezer, you'd Koski in something he wrote a few years ago on a, on zombies and consciousness and uh, and the view, the Epi phenomenon, list views that consciousness plays no causal role where he was engaging some stuff. I wrote, um, a couple of decades ago, he said this Zombie argument, the idea, we can imagine zombies physically like us, but without consciousness, maybe a candidate for the most deranged idea in all our philosophy, the causally closed cognitive system of Traumas, internal narrative is malfunctioning in a way that not by necessity but just in our own universe miraculously happens to be correct.

Speaker 1:          00:43:40       And here he is expressing this to banking idea that, you know, on this view, like there's an algorithm that generates these intuitions about consciousness and that's all physical. And there's also this further layer of nonphysical stuff. And just by massive coincidence, the uh, the physical algorithm is a correct model of the non physical stuff. And these, that's a, that's a, that's a form of debunking here. It would take a miracle for this view to be correct. So I think both of these views are on to these objections are onto something and to make progress on this, when I decide we need to find a way of getting past these absurdities, I mean you might say, well there's middle ground between very strong illusionism and very strong Epi for nominalism. It tends to slide back to the same problems other forms of illusionism weaker forms don't help much with the highest problem.

Speaker 1:          00:44:29       Other forms of realism is still subject to this, uh, takes a miracle for this view to be correct critique. So I think to get beyond absurd as he hear both sides need to do something more. The illusion, this needs to do more to explain how having a mind could be like this time I'll just like this. Even though it's not at all the way that it seems, you need to find some way to recapture the data. Realists need to explain how it is that these metal problem processes co I'm not completely independent of consciousness. Realistically, to explain how Metta problem processes the ones that generate these intuitions and reports and convictions about consciousness are essentially grounded in consciousness. Even if it's possible somehow for them to, or conceivable for them to occur without consciousness anyway. So that's just to lay out a research program. I think a solution to the metal problem that meets these ambitions might just possibly solve the hard problem of consciousness or at the very least shed significant light on it. In the meantime, the metal problem is a potentially tractable research project for everyone and mine, I recommend to all of you. Thanks.

Speaker 2:          00:45:48       Uh, yes. I just want to say I think it's very interesting, this concept of we have these mental models, a collection of mental models and that this collection of mental models is con consciousness. Basically. Um, consciousness is defined as a collection of these mental models that we have. And the problem of consciousness is that we don't understand the physical phenomenon that, that causes these mental models, models or that stimulates these mental models. Um, so we just have this belief that it's, you know, if femoral or not real or something like that. Um, and it's, if you take that view, then what's interesting is that you could, you could simulate these mental models, uh, like robot could simulate this mental models. Um, and you could simulate consciousness, uh, as well. Um, and even if the underlying physical phenomenon that fuels these mental models is different, you know, robots have different sensors, et Cetera, um, you could still get the same consciousness effect, um, in both cases. Okay.

Speaker 1:          00:46:54       Yeah, I think that's that's right. Or at the very least you ought to be able to get, it looks like you ought to be able to get the same models, at least in a robot if the models themselves or something algorithmic and ought to be, ought to be able to design a robot that has, at the very least, let's say isomorphic models and some sense that is conscious. Of course it is a further question at least by my lights. What are then the robot will be conscious. And that was the question I alluded to and talking about the artificial consciousness tests. You might think that would at least be very good evidence that the robot is conscious. If it's got a model of consciousness just like ours, it seems very plausible. There ought be a very strong link if you're having a model like that and be unconscious.

Speaker 1:          00:47:32       I mean, I think probably, um, something like ned block who was here arguing against me, machines consciousness, but say, no, no, the model is not enough. The model has to be built with the right stuff. Say it's got to be built to biology and so on. But at least by my lights, I think if I have found that AI system that had a very serious, uh, version of our model of consciousness, I'd take that as a very good reason to believe it's conscious. In the IIT theory. Is there a estimate or plausible estimate for what the value of Phi is for people and for other systems? Basically, no. Um, it's extremely hard to measure, um, in systems of any size at all. I mean, because the way is to find that involves taking a som over every possible partition of a system. It turns out we a, it's hard to measure in the brain because you've got to involve the causal dependencies separate in different units on neurons.

Speaker 1:          00:48:27       But even for a pure algorithmic system, you've got to like a neural network laid out in a, in front of you. It's computationally intractable to measure the fire of one of those once I get to bigger than 15 units or, uh, also, so he had no, you'd like to say this isn't empirical theory and in principle empirically testable. But notice the, in principle, it's extremely difficult to, uh, to, uh, to measure it by some people. I'm Scott Aaronson. Uh, the computer scientist has, uh, has argued, has tried to put forward counterexamples to the theory, which are basically very, very simple systems like matrix multipliers, uh, that, you know, multiply two large matrix. He's turned out to have enormous fi, fi as big as you like. If the Matrix, these are big enough and therefore by Tony's theory will not just be conscious, but as conscious as a human being. And Aronson put this forward as a reductio ad absurdum of the Iot theory, I think to know any, basically we bit the bullet and said, Yo, yeah, those, those metrics modifiers are actually having some high degree of consciousness. So I think Iot is probably missing at least missing a few pieces of it's going to be developed. But it's a research program too.

Speaker 3:          00:49:36       You mentioned the leaf as an example of something where uh, does another mental quality, but people don't seem to have the same sense that it is very hard to explain it. In fact, it almost seems too easy where people like a belief about something sorta feels like just how things are. Like you have to kind of reflect on it, I believe, to notice it as a belief. Uh, do you think there's also, or has there been a research kind of related to this question and two, why is that different? Like, uh, it seems like another angle of attack on this problem. It was just like, why doesn't this generate the same hard problem?

Speaker 1:          00:50:13       Yeah. In terms, I'm not sure if there's been sort of research from the perspective of the metal problem or a theory of mine. Suddenly people have thought in their own right, what is the difference between belief and experience that makes them so different? This goes way back to David Hume who philosopher a few centuries ago who said, you know, I'm basically perception is vivid impression. Many impressions and ideas and impressions like experiencing collars are vivid, they have force and vivacity and ideas are merely up faint copy or something. But that's just the first order. And then there were contemporary versions of this kind of thing. Far more sophisticated ways of saying a similar thing. But yeah, you could in principle, um, explore that through the metal problem. Why does it seem to us that perception is so much more vivid? What, what about our models of the mind? Makes perception seems so much more vivid than belief. It makes it vivid. Beliefs seem kind of structural and empty, whereas perception is so full of light. But no, I don't know of work on that from the, uh, the metal problem perspective. Like I said, there's not that much work on these introspective models directly. There is work on theory of mind about beliefs tends to be about models of other people. It may be that something I could dig through my literature on belief that that says something about that. It's a good place to push. Thanks.

Speaker 4:          00:51:31       I wanted to bring up Kurt girdle. Uh, you mentioned your advisor wrote Godel Escher, Bach. Uh, there's something that seems very like girdle or deleon or whatever about this whole discussion and that so girdle showed that a given like a set of axioms and mathematics, it would either be consistent or complete, but not both. Um, and it seems like when Daniel Dennett, uh, Daniel Dennett seems to have like a set of axioms where he cannot construct consciousness from them. He seems to be very much in this sort of consistent camp. Like he, he wants to have a consistent framework, uh, but is okay with the incompleteness. Uh, and I wonder if a similar approach could be taken with consciousness where we could in fact prove that consciousness is independent of Daniel dentists set of axioms the same way they proved after girdle. They prove like the continuum hypothesis was independent of Zf set theory and then they added the axiom of choice, made it to the AFC set theory. So I wonder if we could show that like in Daniel Dennett swirled, we are essentially zombies or we are kind of either zombies or not. It doesn't matter. Either statement could be true and then find what is like the mini minimum axiom that has to be added to Dennis axioms in order to make con consciousness true. Hmm. Interesting. I thought thought

Speaker 1:          00:52:59       for a moment this was going to go in a different direction and you're gonna say Dennett is, uh, is consistent but incomplete. He doesn't have consciousness in this picture. I am complete. I've got consciousness. Yes, but inconsistent. That's why I say all these crazy things you're faced with a choice of not having, of not having consciousness and being incomplete or having consciousness and somehow getting this hard problem and being forced into at least puzzles and paradoxes. But now the way you put was friendlier to me. Yeah. Um, um, yeah, I mean certainly, um, ducoff tedder himself has written a lot on analogies between the good Elian paradoxes and the mind body problem. Anything is always our models. Our self models are always doomed to be incomplete in the galleon way. And he thinks that that might be somehow part of the explanation of our puzzlement at least about consciousness.

Speaker 1:          00:53:49       Someone like Roger Penrose of course takes this much more seriously and literally he thinks that he thinks that, uh, you know, that uh, the computational aspects of computational systems are always going to be limited in the girdle way. He thinks human beings are not so limited. He thinks we've got mathematical capacities to prove theorems are to know that, to see the truth of certain mathematical claims that no formal system could ever have. So he thinks that we somehow go beyond that, the incomplete good dealing. And I don't know if he actually thinks we're complete, but at least we're not incomplete in the way that find out computational systems are incomplete. And furthermore, he thinks that that extra thing that humans have is tied to consciousness. I mean, I never quite saw how that last step code, even if we go, it was even if we did have these special non algorithmic capacities to see the truth of mathematical theorems, how would that be tied to consciousness? But, um, but at the very least, they're there at least an hour. There were structural analogies to be drawn between those two cases about incompleteness of certain theories. How literally we should take the analogies. I'd have to think about. Okay.

Speaker 3:          00:54:54       Has there been some consideration that the problem of understanding consciousness sort of inherently must be difficult because we address the problem using consciousness? Uh, I'm, I'm reminded of the whole thing problem in computer science where we say that in the general case, a program cannot be written to tell whether a program will halt. Because what if you ran it on itself, it can't sort of be broad enough to include its own, uh, execution. So I wonder if there's a similar corollary in consciousness where we use consciousness to think about consciousness. And so therefore, you know, we may not have enough, uh, um, sort of equipment there to be able to unpack it.

Speaker 1:          00:55:32       Yeah, I mean it's tricky. It's like get six people say it's like a use a ruler to measure or a ruler and all. I can do this ruler to measure many other things, but it can't measure itself. It's not a, well on the other hand you can measure one rule or using another ruler. Um, maybe you could measure one consciousness using another, the brain, what the brain kind of study of the brain. But the brain actually has a pretty good job of, uh, of studying the brain. So there were some self referential, uh, paradoxes there. And I think that again, is at the heart of, of Hofstadter's approach. I think we'd have to look for very, very specific conditions under which systems can't study themselves. I didn't always like the idea that the mind was simple enough that we could understand it. We would be too simple to understand the mind. So maybe something like that could be true of consciousness. On the other hand, I actually think that if you start thinking of that consciousness can go along with simple systems, I think at the very least we ought to be able to study consciousness in other systems simpler than ourselves. And boy, if I could solve the hard problem, so even in dogs I'd be, I'd be satisfied.

Speaker 5:          00:56:34       Hey. Um, so I have a question about how the Neta problem research program might proceed sort of related to the last question. So I'm certainly things we believe about our own consciousness. Even if we all say them, probably some of them are false. Our brain has a tendency to hide what reality is like. Um, if you look at like visual perception, you know, there's what's called lightness constancy in our brains have tracks out the lighting and the environment. So we actually see more reliably what the colors of objects are. Like these viral examples of like the black and gold dress is an example of this. And when you're kind of presented with an explanation every, it's like, Huh, my brain does that. It's not something we have access to. Yeah. Relate the Jani Laurel, Laurel illusion is like another one where like when you hear the explanation, you know, the scientists that understand it, our own introspection doesn't include that. So how do you kind of proceed with, um, trying to get at what consciousness really is versus what our sort of, whatever is simplified or distorted view might be? Okay.

Speaker 1:          00:57:39       Yeah, I think, well one view here would be is that we never have access to the mechanisms that generate consciousness, but we still have access to the conscious states themselves. Actually, the coal Ashley said this decades, he said, no process of the brain is ever conscious. The processes that get you to the states or never conscious the states, they get you to our conscious to take your experience of the dress. For me it was, uh, it was about, um, white and gold. Um, so you know, and I knew that, you know, each of us, we're certain that I am, I was experiencing, I was certain that I was experiencing white and gold. Maybe you were suddenly you're experiencing blue and black a in Brazil. It was right.

Speaker 1:          00:58:20       You were sure that, yeah, those idiots can't be, can't be looking at this. Right. Um, but each of us, I think the natural way to describe this, at least it was a, each of us was certain what kind of conscious experience we were having. But what we had no idea about was the mechanisms by which we got there. So the mechanisms are completely opaque, but the states themselves where at least primary facie transparency, I think that would be the standard if you would even a realist about consciousness could go with that. They said, well, we know what conscious states where we know what those conscious states are. We don't know the processes by which they're generated. The allusionist I think wants to go much further and say, well, it seems to you that you know what conscious state you're having, it seem to you that you're experiencing yellow and gold.

Speaker 1:          00:59:03       Ah, sorry, yellow and white, whatever it was, golden golden words, whatever. Platinum, black and blue, I think, Ooh, gold and white. Um, it seems to you that your experiencing golden light, but in fact that too is just something thrown up by another model. The, the yellow gold was a perceptual model. Then there was an introspective model that said you're experiencing gold and why when maybe in fact you're just a Zombie or who knows what's actually going on in your conscious state. So the illusion of the few I think has to somehow take this further and say not just the processes that generate the conscious states, but maybe the conscious states themselves are somehow opaque to us.

Speaker 6:          00:59:39       Great. Thanks.

Speaker 1:          00:59:42       It feels like, uh, some discussion of generality of a problem is missing from this discussion. Yeah. The Matrix multiplier example of having high fi is still, it's not a general thing. Is there some someone exploring space, the sort of intersection of generality and complexity that leads to consciousness as an emergent behavior? We can just say generality. I mean the idea that a theory should be general that I should apply to every system. You mean mechanisms of generality of the agent, right. If I can write an arbitrarily complex program to play tic TAC toe and all it will ever be able to do is play tic TAC toe. It has no outputs to express anything else. Yeah, that's just a general in the sense of Agi, artificial general intelligence. I mean some aspects of consciousness seem to be domain general like for example maybe insofar as belief and reasoning as conscious.

Speaker 1:          01:00:33       Those were domain general but much of perception it doesn't seem, especially demand Gen, right? Collar is very domains. Taste is very domain specific so it's still conscious. If my agent can't express problem statements like if I don't give it an output by which it can express problem statements, you can never come to a conclusion about its consciousness. I like to distinguish intelligence and consciousness and even be able to even natural language. And, you know, be able to address a problem statement and analyze a problem that's already a very had pretty advanced form of intelligence. Um, I think it's very plausible that say a mouth has got some kind of consciousness, even if it's got no ability to address problem statements and many of its capacities maybe very specialized. I mean it's still a much more general than say a simple neural network that it can only do one thing. I can do a mouth can do many things, but I'm not, I'm not sure that I see in a central, I suddenly see a connection between intelligence in generality. We want to say, you know, somehow a high degree of generality is required for high intelligence. I'm not sure there's the same connection for consciousness. Consciousness can be extremely domain specific has a taste and maybe maybe vision or it can be domain general. So maybe those two cross cut each other a bit.

Speaker 6:          01:01:48       Yeah.

Speaker 5:          01:01:51       So, um, it seems to me like the, the metal problem as it's formulated, um, implies some amount of like separation or epiphanies nominalism tween like consciousness and brain states. And, um, one thing that I think underlies a lot of people's motivation to do say science is that it has, um, causal import. Like predicting behaviors is clearly a functionally useful thing to do. And if you can predict all of behavior without having to explain consciousness, their motivation for explaining consciousness sort of evaporates. And it sort of feels like, yeah, yeah. Well, what's the point of even in thinking about that because it's just not going to do anything for me. Um, what do you say to someone when they say that to you? What is the thing that they said to me again? That there's no, there's, maybe consciousness exists, maybe it doesn't, but if I can explain all of human behavior and all of the behavior of the, the world in general without recourse to such concepts, then I've done everything that there is that's useful. Like explaining consciousness isn't a useful thing to do. Um, and, and thus I'm not interested in this and it makes it not be real.

Speaker 1:          01:03:01       I mean, I'm certainly, I'm not, I mean, I think epi phenomenon isn't, it could be true. I certainly don't have any commitment to it though. It's quite possible that consciousness has a role to play in generating behavior that we don't yet understand. And maybe thinking hard about the metal problem can help us get clearer on those roles. I think if you're got any sympathy to panpsychism maybe consciousness is intimately involved with how physical processes get going and the uh, in the first place. And there are people who want to pursue interaction. This ideas where consciousness interacts with the brain or if you're a reductionist consciousness, maybe just a matter of the right algorithm on all of those views. Consciousness may have some role to play, but just say, it turns out that you can explain all of behavior including these problems without, um, without bringing in consciousness.

Speaker 1:          01:03:47       Then does that mean that consciousness is not something we should care about and not something that matters? I don't think that would follow. I mean maybe it wouldn't matter for certain engineering purposes. So you want to build a useful system. But um, you know, at least in my view, consciousness is really the only thing that matters. It's the thing that makes life worth living. It's what gives our lives meaning and value and so on. So it might turn out that, okay, the point of consciousness is not that useful for explaining other stuff, but it's, you know, but if it's the source of intrinsic difficult significance in the world, then understanding consciousness, it was still be absolutely essential to understanding ourselves. Furthermore, if it comes to developing other systems like say AI systems or dealing with nonhuman animals and so on, we absolutely want to know.

Speaker 1:          01:04:31       We need to know whether they're conscious because you know, if they are conscious, they presumably have moral status. If they can suffer, then it's very bad to mistreat them. If they're not conscious, then you might, I think it's very plausible. Treat nonconscious systems we can treat how we like and it doesn't really matter morally. So the question of, of whether say an AI system is conscious or not, it's gonna be absolutely vital for how we interact with it and how we build our society. That's not a question of engineering usefulness. That's a question of connecting with our most fundamental values.

Speaker 5:          01:05:02       I completely agree. I just, I haven't found that formulation to, to be very convincing to others necessarily. Hi. Uh, thank you so much for coming and chatting with us today. Um, I'm really interested in some of your earlier work. Uh, the extended

Speaker 7:          01:05:18       mind just trying to dig a cognition. Yeah. And a, you're at a company speaking with a bunch of people who do an incredibly cognitively demanding task. Most of literature that I've read on this topic, uh, uses relatively simple examples of tying like it's, it's difficult to think, um, just inside your head, uh, on these relatively simple things. And if you take a look at the programs that we build sort of like on a Monday and day to day basis, there are millions of lines long I've read people in the past say something like the Boeing triple seven was the most complicated thing that human beings have ever made. And I think most of us would look at that and say, we got that beat. You know, like the, the, the things that large internet companies do, the size of the complexity of that is staggering. And yet if we close our eyes, everyone in here is going to say, I'm going to have difficulty writing a 10 line program in my head. Okay. So I've just sort of as, as an open, like I'd be very interested in hearing your thoughts, uh, about, uh, how the activity of programming, uh, connects to the extended mind ideas.

Speaker 1:          01:06:17       Yeah. So this is like, this is a reference to, um, something that I got to talk to them about 20 years ago with my colleague Andy Clock. We wrote an article, a book called the extended mind about how processes in the mind can extend outside the brain when we become coupled to our tools. And actually I'll, I'll central example back then in the mid nineties was a, it was a notebook, someone writing stuff in a notebook. I mean, even then we knew about the, uh, we knew about the internet and we had some, uh, intent examples, I guess this company didn't exist yet and 95. Um, but, um, but now of course our minds are just become more and more extended and, um, you know, smart phones came up, came along a few years later and everyone has coupled very, very closely to their phones and their other devices, the couple of them very, very closely to the Internet.

Speaker 1:          01:07:05       And now it's certainly the case that a whole lot of my memory is now offloaded onto the, uh, the servers of your company somewhere, uh, somewhere or other, whether it's in the, uh, you know, um, a mail systems or not, or navigation mapping systems or, or, uh, or other systems. So, yeah, most of my navigation has been offloading, been offloaded to maps and much of my memory for you know, has been offloaded, well maybe that's in my phone, but, um, other bits of my memory or are offloaded into my file system on, on, um, you know, some cloud service. So certainly, um, yeah, a vast amounts of my mind are now existing in the, uh, in the cloud. And if I was talking how to lose access to those completely, then I'd lose an awful lot of my, uh, of my capacities. So I think we have now, we are now sort of extending into, uh, into the cloud, thanks to that, to you guys and others.

Speaker 1:          01:08:02       The questions specifically though about programming, programming is a kind of active interaction with our, our devices. I mean, I think of programming or something, it takes a little bit longer. It's a longer timescale. So the core cases of the extended mind involved sort of automatic use of our devices, which are always ready to hand. We can use them to get information to act in the moment, which is the kind of thing that, that the, uh, that the brain does. So we went so far as programming is a slower process, you know, and I remember from my, uh, programming days, all the, uh, the endless hours of debugging and uh, and uh, and so on. And then as at least going to be a slower time scale for the extended mind. But still, I mean, fine men talked about writing this way. Someone looked at fine men's work and um, bunch of notes he had about a physics problem he was thinking about.

Speaker 1:          01:08:57       And someone said to him, Oh, it's nice you have this record of your work and five minutes, that's not a record of my work. That's the work. That is the thinking as I was writing it down and so on. I think, you know, at least my recollection from my programming days was it is that, you know, when you're actually running a program that's not, it's not like he's just do a bunch of thinking and then, and then code your code, your thoughts. The programming is to have some very considerable extent your, uh, your thinking. So is that, is that the sort of thing here?

Speaker 7:          01:09:27       Yes. And uh, the, if we, I think as, as people that program start to reflect on what we do. And very few of us actually, uh, like if you're the tech lead of a system, maybe you've got it in your head. Okay. But you would agree that most of the people on the team who've come more recently only have a chunk of it in their head. And yet there's some house to label to contribute.

Speaker 1:          01:09:50       Oh yeah, this is, this is, and this is now distributed cognition of, I mean we talk the extended mind that extended cognition like starts with an individual and then extends out into extends the capacities out using their tools or their devices or even other people. So maybe my partner serves as my memory, but it's still centered on an individual. But then there's the closely related case of distributed cognition. We have a team of people who are, uh, who are doing something and making joint decisions and carrying out joint actions and absolutely seamless way. And I take it that a company like this that are going to be any number of instances of distributed cognition. I don't know whether the company has as a whole as a whole has one giant Google mind or maybe there's just like, and a near infinite number of separate Google minds for all the individual teams and divisions. Um, and so on. But I think, yeah, probably some anthropologist has already done it, a definitive analysis of distributed cognition in this, in this company. But if they haven't, they certainly need to. Thank you.

Speaker 6:          01:10:48       [inaudible].