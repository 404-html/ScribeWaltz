Speaker 1:          00:08          And thanks for your interest in this topic. Effective altruism, philanthropy as a for profit and ever and the Al shortly delve into what that means for profit thinking and charity, how that can be a meaningfully combined. Um, yeah, let's start off with a definition of what effective altruism is. It's a practical philosophy and social movement, which you might have heard of that uses empirical evidence and reason to determine the most effective ways to benefit others in global society. Now, that may sound good, but it doesn't tell us that much. Um, of course. Um, in any case, there are two conceptual components, altruism and effectiveness. Altruism is essentially, um, the conceptual opposite of, of egoism. So it means that we're sort of practically oriented in our action towards helping others as well, and not just considering our self interest and effectiveness means that we're trying to, of course optimally achieve our goals, including our altruistic goals.

Speaker 1:          01:15          So we're trying to maximize the probability of a, of goal achievement. Um, now why should we be altruistic at all? One might wonder, I mean, in economics there's also, I mean, there are various, uh, prevalent theories of course, but, um, there's a big strand at least in, uh, in, in western economics saying, well, you know, we should be selfish, a utility maximizers so that might raise the question, why be altruistic at all? Why be interested in, in, in benefiting others too. And there are various, uh, thought experiments and arguments to, to justify this practical orientation. So imagine you're in the unfortunate position of a firefighter faced with two buildings, uh, to burning buildings. One big one, small. And let's say there are a hundred people being trapped at the moment in the big building and, uh, just one person being trapped in the small building and they also received the information that unfortunately, it won't be possible to save all of these people.

Speaker 1:          02:19          So you have to, you're faced with this moral dilemma. You're either, um, can save these 100 people in the big building or the one person in the small building. That's the moral dilemma you're in. And you know, for simplicity, let's say that you have like in either case, the 100% success probability, so you'll succeed, uh, at saving these 100 or the one person depending on the choice you make. Now of course, there's always a third option as well if you're the firefighter, you could just decide to, to go have a beer instead and I'm and not be bothered with the situation. So this will be the non altruistic choice of course. And uh, yeah, maybe the selfish choice, the pure lust, selfish choice, just go have a beer and ignore the moral catastrophe, the suffering and the imminent deaths there. Um, but you know, if, if we agree that three is not an option as, um, most people do when faced with moral catastrophe, then yeah, we need to choose between one and two.

Speaker 1:          03:17          And yeah, I mean it's a, it's a tragic, it's a sad choice because somebody is going to suffer and die anyway. But it seems that clearly in such a situation, we would opt for the lesser evil and the lesser evil here seems to be saving the 100 and not the one. So here we'd probably go for a option one and save a 100 people. And um, this, so if we agree you sort of with the background reasoning here then, um, that's an argument for two things. One that we don't just have selfish or self interested goals. We have at least some altruistic goals as well, at least in situations of moral catastrophe. And two, we care about the numbers. So when helping others, it doesn't just matter that we are helping. It does also matter that we try and help, um, the greatest possible number of people.

Speaker 1:          04:10          Um, there are further arguments which you also may have heard of, um, originating in, in practical philosophy. This is a drowning child thought experiment going back to a philosopher, Peter Singer. So imagine you're walking past a lake or a shallow pond and you realize that a child, a small child is drowning in there. Um, you look around and you can't see any parents or anyone else that would be willing and able to help. And the realize that this child's life depends on what you do now. So you can walk into that pond or that lake and just safe the child. There are no complications, no risks or dangers involved to yourself. Um, so you could just walk in and save her life. Well, that's not entirely true. There is a little complication and it's the following. You're wearing pretty expensive clothes and expensive shoes. Maybe your, you happen to be on your way to an important meeting.

Speaker 1:          05:09          Let's say, uh, you're expensive suit and shoes cost, whatever, a total amount of $1,000. And they realize that, you know, if, if you're now running or jumping into that pond and lake and no risk to yourself, you're going to ruin the clothes and the shoes. Let's also assume that there will be no replacement. So you'll just have to buy a new suit and new shoes for $1,000. So then the question is, would you save the life, um, under these circumstances, in that situation? Or what do you not? Um, unsurprisingly, the vast majority of people say, ah, they would save the life. Of course, if they didn't really existentially need these $1,000, of course it can construct a situation where you're like, yeah, I'm going to be starving if I'll have to, you know, uh, spend 1000 in addition to what I'm spending anyway. But you know, if you're still going to be comfortably off, if that's just going to mean, um, a little less luxury in your life, then most people say, yeah, in that case, it's really a no brainer that they would save the life.

Speaker 1:          06:11          And this also shows that we do indeed, or most people do indeed have important altruistic goals as well, and they would be willing to sacrifice quite a bit of money in order to achieve these altruistic goals, to reduce the suffering of others and save the life of others, at least if that doesn't put themselves in, in sort of an existentially very uncomfortable or dangerous situation. But, so if we follow the reasoning behind these two thought experiments, the firefighter and the drowning child, um, that may have pretty significant practical implications because we can then ask, well, okay, now we do know that there are also burning buildings out there. There's a lot of suffering out there. A lot of people are dying maybe from, from preventable causes that we could do something about. So let's say if we have on our bank account $1,000 or, uh, you know, maybe many times as much that we could donate and we would still be comfortably off.

Speaker 1:          07:08          Um, why don't we do it? So I mean, if, if it's, if it's a no brainer to save a drowning child in that situation, then one can ask, well, wouldn't it also be just equally irrational and the no brainer actually to dominate a lot more if we're not already doing it? And then of course, try and figure out, yeah, where this money could go the longest way because it's important to save as many people as possible as we saw in the first thought experiment. And that's precisely what the effective altruism movement is trying to do. Of course, if we follow the spirit of these thought experiments, then one action point is, yeah, trying to check out the material, the philosophical material, the scientific, the economic, the empirical material on strategic do Gooding, um, where should we donate our time or our money if we want to make a difference?

Speaker 1:          08:00          Then one standard action point, um, within the effective altruism movement has been trying to donate at least 10% of one's income. Now, when I first heard of this idea, donate 10% of my income, I was like, yeah, but that's like a lot. And yes, I do have altruistic goals, but I'm not a moral saint. Um, listen, this too much of a challenge isn't this sort of over demanding, but then when you actually reason about it more and check out the happiness, psychology and happiness economics behind it, many people conclude that yes, you know, it's quite unlikely. So if you live in a, in a rich country and earn a good salary, it's quite likely that, that you'd be worse off, uh, when donating 10% of your income each month. And quite the opposite. There's quite solid happiness, economic and happiness, psychological research suggesting that donations often tend to make the donors happier as well.

Speaker 1:          08:55          So it can be a win win. And this is why many effective altruists have, um, decided to donate at least 10% of their income. And, uh, yeah, it's quite astounding what can be achieved in this way. So currently on a global level, there are a few thousand people at least, um, that strongly identify with effective altruism. I mean, maybe already tens of thousands that maybe loosely identify with the tenants by the few thousand that have decided to donate 10% or more and have taken the pledge. This pledge is not legally binding, but it's sort of personally and maybe socially binding a pledge to donate 10% over a lifetime. And in aggregate, these pledges are already worth, uh, several, several billion dollars in sort of promised donations. And these donations are now happening. So it seems possible. And the movement, this is only just starting essentially. So it seems possible to compete with, uh, with billionaires actually with this strategy, even if we're not billing there as ourselves.

Speaker 1:          09:57          Um, the other main action point, this third effective altruists of course don't just try to donate money but also time. Um, there is an organization called 80,000 hours, which provides career advice to people interested in making as much of an altruistic difference as possible. On average, we work about $80,000, uh, in our lifetime. And of course it's both in terms of the personal stakes and in terms of the stakes for the world at large, the altruistic stakes, this is a huge decision. What are we gonna do with these, um, $80,000. And it turns out that sort of standard, traditional, um, career advice that people have been receiving if they were interested in making an altruistic difference isn't very useful or very rational. So, I mean if you, if you Paul people and ask them for examples of like standardly altruistic careers, many will for instance, say doctor doctor in uh, the developing world, that's like a standard intuition.

Speaker 1:          11:04          But if you think about that, if the goal is to make as much of a difference as possible, you need to work on something that wouldn't happen otherwise. So if you take the job of a doctor and if it's the case that if you had not taken that job, somebody else would be doing it, then maybe you're not making much of a counter. The counter factual difference actually. And in some cases it can be a much better strategy to say, uh, decide to go and earn as much money as possible and then give quite a large fraction of that. For instance, in order to enable several other people to become doctors in developing countries. So with that sort of strategy often called earning to give, it could be possible to make a much bigger counterfactual difference and also to multiply once impact. And of course that's just one consideration. These considerations could go in either direction. It depends on what the biggest bottle necks are. I mean, if, if money isn't the big bottleneck, then probably earning to give won't be, uh, that promising strategy. So it, it depends on the specifics of course, but it turns out that traditional altruistic career advice, um, often isn't very useful. Now another question of course, is, well, what are the biggest, um, metaphorically speaking, burning buildings out there?

Speaker 1:          12:26          Um, uh, traditional focus of do gooders has been just turning to local problems, problems in, uh, in our own society. And there are sayings like charity begins at home and so on. Um, but if we strive to make the biggest possible difference, that's not a very plausible focus area because at least if we live in, if an enrich in developed countries, it's, it's quite hard to actually save a life for say, a few thousand dollars. And most of the time, totally impossible. So if we look into our health care systems, it usually takes several hundred thousand dollars in order to save a life. And health care economics, uh, can calculate that pretty precisely. But because of this dynamic of diminishing marginal utility of money, it's, it tends to be a lot easier to save lives and reduce suffering and advanced society in poorer countries. So I'm a more plausible focus area with, for instance, we are looking to the refugee crisis.

Speaker 1:          13:29          I mean we can roughly a look at them at the victim counts. People that are suffering turns out about 50 or 60 million people are currently a categorized us as refugees, but it seems that there are much bigger burning buildings still. For instance, global poverty, extreme poverty still affect about 800 million people. The standard definition is of of extreme global poverty is an individual living on less than $2 per day. That's purchasing power parity of course. And that means usually means permanent undernourishment I'm suffering from diseases that are actually totally treatable because we can't afford any, any medical treatment and so on. So global poverty and health is a really big um, burning building as it were, a really big cause area. Um, and other sort of general cause area, uh, that effective altruists tend to be interested in is, is global catastrophic risks. Because of course when we're dealing with risks are for potentially global scale, well literally everybody on earth could be affected and not just the present generation but also all future generations.

Speaker 1:          14:43          That's also a consideration and important to many effective altruists. So if we think that the numbers count, the victim counts are important, then um, it seems that at least if civilization goes on, most of the people that will ever live, we'll of course live in, in the future. So at this could then lead to an argument of the sword. Well, of course the present generation is, is intrinsically important and certainly very instrumental. Important. But all the future generations taken together could, could be enormously and overwhelmingly important if there's ways to positively affect their wellbeing. Um, and of course some global catastrophic risks are environmental, some are political, international warfare say some are technological. And there's also of course an overlapping area between all of these, I mean nuclear war of course at the intersection of technology and international politics. Um, but there are technological risks of various kinds and also some stemming from our action or are omission.

Speaker 1:          15:52          So, for instance, one cause area, some effective altruists are interested in is, is biotechnology in general. So biosecurity, of course here there are global risks from just harmful action. Yeah. If you consider synthetic biology, the possibility to create artificial artificially create bacteria or viruses that could cause pandemics. Um, so that, that's certainly a risk. But then some risk also could sort of stem from our mission, our failure to advance certain technologies. I'm abroad because area some effective altruists are interested in is transhumanism of thinking about the human condition from a far future perspective, biologically informed perspective. Um, and some sub cause area there is trying to fight and ultimately eliminate aging for instance. So if we take a fire, if you'd take a far future perspective and let's say it's, it's techno technologically possible to sort of fight diseases and the diseases of old age to a point where we're really no longer aging in a meaningful, uh, biological sense.

Speaker 1:          16:58          So if that is possible, it seems like if we take sufficient technological action to bring it about that could be a huge benefit. And in a sense maybe of course, you know, that's a, that's a separate long discussion. To what extent are biological condition also represents a catastrophe in the way. But this is a case where technology of course, you know, through the advancement of medicine has already brought us huge benefits and where failure to act now to invest in the right kinds of maybe sort of very visionary in the Utopian research could also mean that certain catastrophes go on for longer than would be necessary. So eliminating aging as maybe a speculative buy their cars area that some effective boundaries have been interested in and other cause areas, animal suffering. I mean this is also like a long philosophical and empirical at debate, but uh, so if we suppose that animals are conscious to can suffer as well, and if say intelligence is not super relevant for, for moral status.

Speaker 1:          17:58          And that seems to be what we believe for humans. I mean we're, when humans are concerned, we're usually not saying, well, you know, these humans say children, small children are less intelligent, therefore they're suffering matters less. If we're not going to go for such an Rdm and then we might reason, well, okay, you know, animals might be far less intelligent than we are, but if there's good evidence that they can suffer as well, their numbers are also enormous. Um, and maybe there's something we can do to, uh, to reduce their suffering as well. So some effective altruist have taken that perspective and, and are therefore trying to, uh, to do effective work in that course area. And there's more cause areas of course. But that's just the rough overview. And as you can see, unsurprisingly, I mean, the world is a hugely complex place, um, especially also when it comes to trying to improve it.

Speaker 1:          18:46          So that's just sort of one consideration. What are the biggest, um, burning buildings. But then of course, if we want to know more precisely and more specifically what the highest impact opportunities are for us specifically, more considerations will be relevant. So scope of the problem that's sort of the size of the burning building is just one, uh, one relevant consideration. I mean, another consideration, needless to say is the solvability or the tractability of a problem. So if you have like two problems, one big, one small, and if it turns out that the big problem is just not solvable, not realistic, less soluble then well, yes, you know, working on it won't be effective and it could be more effective to work on a small problem with highest solvability. So that's definitely another, um, relevant consideration. Um, a third one is neglect in society and this is based on the sort of economic assumption that activist resources, so donation of time, donations of time and money also tend to have diminishing marginal returns.

Speaker 1:          19:50          So if, if, say you're one of the very, if there's a big problem out there in society and the you happen to be one of the first people to help address it, then the chance is much higher than you're going to be able to make a big difference that maybe you're going to be able to make contributions of time and money of ideas, uh, that wouldn't happen otherwise. But if the problem isn't highly neglected in society, so if there are already a huge numbers of people addressing it, both in civil society and in politics may be in research and they're just one additional person there contributing money or time, then the probability of course it will be much lower than the are gonna make a huge difference. That wouldn't happen otherwise. So neglect and society is definitely another relevant consideration. Last but not least, personal fit and comparative advantage also of groups and organizations and then of individuals is, is relevant as well.

Speaker 1:          20:44          So, I mean, yeah, if you have like a big problem that's highly solvable and neglected in society, but it's just a very bad fit, you don't really have any like required skill, you can't really make a huge contributions to that problem. Maybe it's still better to work on something else. So that's definitely also an important consideration. Your interests, your skills, um, your motivation of course, you know, if, if you're an effective altruist, a core thing to work on is also minimizing the probability that you're going to burn out at some point because that that would of course hurt your impact big time. So all of these should factor into the overall assessment and that can of course get hugely complex but also very interesting. Um, now let's zoom in into specific cause areas and, and look at some more examples and also then specific data that enables us to make a cost effectiveness and impact evaluations.

Speaker 1:          21:39          Um, in terms of global health, of course that's strongly related to two general global poverty. Um, there are various diseases that affect a huge number and kill a huge number of people every year and every day. So for instance, the so called big three malaria, tuberculosis, HIV, killed many more people each and every day than say, political violence and oppression and warfare has tended to kill in a year. Now that's another thing. Of course. I mean I'm not saying that political action can be effective. I mean quite the opposite. So I mean, even in terms of trying to fix global health, ultimately if, if you can sort of go for concert at political action, systemic action, that can be hugely effective. There are also risks to that. But it does seem like many do gooders do seem to sort of prematurely jump into politics because that's been the traditional focus.

Speaker 1:          22:33          Yeah, we knew we need to address something politically or we need to address specifically political problems. But actually if we just look at the victim counts, it's not obvious that this should be the focus. So as I mentioned, if we consider these big diseases, the victim count at least has tended to be much higher. Um, the solvability is also seems very high, at least in principle. Uh, the medical technological solvability or preventability very high in principle. That's also often like a huge complication with politics. It's so messy and it's unclear whether your campaign will succeed in song neglect in society also comparatively high. So of course a lot of money is going into medical research, but the bulk of the money is going into research that aims to address diseases that are prevalent in western societies, in rich societies. Why? Well, one reason is that you can make a huge profit there because they're going to sell the treatment to people in rich countries, whereas it's much harder to make a great profit addressing malaria, which predominantly affects, uh, the poorest people.

Speaker 1:          23:40          But in terms of achieving our altruistic goals, if we care about helping as many people as possible, this can be a great focus area. Of course, you know, trying to address just the diseases that tend to affect the poorest of the poor. Um, now, interestingly, empirical studies, empirical data shows that there are massive differences in cost effectiveness. So in terms of amount of lives saved, amount of suffering reduced between global health, different global health, um, interventions. And um, so the title of the talk said that effective altruism is sort of trying to approach, um, altruism in charity in a for profit way. And I didn't mean to say that we're attempting to make a monetary profit, not at all, but it's sort of the mindset of trying to maximize something, maximize a profit. But here of course the profit is being defined in terms of lives saved and suffering reduced.

Speaker 1:          24:41          And interestingly, this hasn't really been the traditional focus of charity because in traditional charity, what gets emphasized as sort of more the emotional side of it, right? I mean, maybe you have a relative that died from cancer and of course that affects you deeply, emotionally and very legitimately of course. And then you're emotionally moved with your sort of immediate compassion to do something about that. And uh, effective altruism missing, trying to counter that. It's just saying, yes, let's take our compassion. Let's take our emotions, let's take our heart. But let's also combine that with our head, with our rationality or optimization power that we are of course standardly applying when it comes to actual for profit investments. You know, in terms of monetary and personal profit. Now when we're investing there for monetary profit, it's totally obvious that of course we're going to be interested in the data and where we're going to be highly interested to know, well, you know, if, if I'm gonna Invest into that particular option, what's the probability of success and so on.

Speaker 1:          25:42          How much am I going to achieve? And this is precisely the kind of mindset that effective altruism tries to apply to the domain of altruism and charity as well. Let's look at HIV for instance. So this is data from the World Health Organization have like various interventions that a one could go for antiretroviral therapy, condom distribution treatment, and then uh, at the bottom prevention through education for high risk groups and mass media education. And you can see the estimated the impacts based on a randomized controlled trials in many cases can vary. It can vary a lot. As you can also see by like the, the varying shades of the bars. Um, the uncertainty is also quite huge. So these traits there, um, and the faint depart that represents the uncertainty, the intervals that we should probably rationally have based on the studies. So the uncertainty is huge, but we can also see that despite this uncertainty, which should of course factor in the expect, the differences in, in impact are still enormous. And so if we follow the initial thought experiment of the firefighter and the greed that the numbers count in altruism as well, then of course it's crucial to factor this information in when making donation choices.

Speaker 1:          27:05          Another example is MOLAP malaria prevention and uh, this is actually a, a cause area or a sub cause area that does particularly well, tends to do better, at least at the moment. I mean this can also change, you know, based on the situation on the ground changing the available evidence, changing a bout. But malaria prevention is a standard intervention that's currently recommended by effective altruists organizations in the space of a world poverty and global health. So with organizations such as the against malaria foundation, you can distribute, purchase, distribute one bed net for just $5 and this is going to protect two people that can then sleep under these nets. And this offers really strong protection. Of course, most infections happen at night when when people are sleeping and uh, yes, in many, many randomized, dozens of randomized control trials, which is really good evidence. I mean this is not physics of course, but it's not the hard science by many standards.

Speaker 1:          28:04          But, uh, it's, I mean this is backed by really good evidence, uh, comparatively really good evidence in that space. Um, and so yeah, of course, if you scale up these numbers, if you donate 100 k a then you can protect for decay people. That means many villages or like my whole football stadium say and donating 100K is quite easy for most people actually in their western, in rich societies. I mean, you don't need to donate 100 k one goal, but you can maybe if you donate five k a year, well then you're going to get up there. If you donate 10 k a year anyway. Um, and so it's quite amazing what we can actually do. Even just justice individuals. Um, the estimated cost per life year saved is just $150 with that kind of intervention. So that's pretty amazing with an, with a donation of $150, that doesn't affect many or most people in rich societies at all.

Speaker 1:          29:04          We can actually give a poor person one more health for Year of life. And then of course, yeah, we can calculate of course based on that, the cost, the average cost of saving a whole life. Now, of course, strictly speaking, we can't save a life at the moment because everybody is going to die. So we'll need a, the abolition of aging there through biotechnology. Um, but what health care economists do is to stay. They'd talk about what the cost of one life saved, uh, as equivalent to saving 30 years of healthy life. So that's usually what health care economists mean when they say saving a life. So 30 years by the, yeah, this is the rough cost here for one year, one healthy year of life. Um, there are various organizations doing sort of in depth charity research, um, collecting data, analyzing it, um, doing the relevant calculations and uh, yeah, the leading organization currently there is gift well Gif world at work and uh, yes they are extremely transparent.

Speaker 1:          30:02          You can check out all of their research, um, on the website if you're interested and they're coming up with top recommendations for where to donate based on their calculations every year. And of course there's some fluctuations if the evidence changes. Um, generally speaking, global health seems to be a really promising cause area because there are these huge medical short run benefits. And then as some studies suggest, of course there are also tend to be long run societal benefits. Um, it's our one randomized controlled trial suggested that kids that were able to sleep under bed nets and we're protected from malaria, laid there on 10th tended to earn, they missed much for your school days and later on for instance tended to earn 20% more than kids that didn't have the privilege to sleep under these bent nets. And another advantage is that, I mean these are some worries that are often discussed in these debates where I'll, you know, but in terms of development aid of course there have been many failures as well.

Speaker 1:          31:04          So many people are actually rightly so quite cynical about the impact of development aid. And indeed there have been many failures. But I'd say, well that's just an additional argument for effective altruism for like being serious about the data and trying to figure out what actually helps and what may harm people. So even if we believe that's sort of all of the uh, development aid, maybe also the state sponsor development aid that's coming from western societies was like maybe net neutral or maybe even net negative. Just statistically speaking, there must be at least some interventions that are positive, right? So if we assume that all of the interventions that we've sponsored in terms of development aid where like net neutral, then probably there's some distribution where some interventions are like harmful, many are neutral ish and some are really good. And what effective altruism is about is sort of trying to find an, isolate the really good ones and scale them up.

Speaker 1:          31:58          And of course in our, and the advantage of working in health is that this seems to be like maybe the paradigm example of a universal good. I mean even if people's cultural preferences vary a lot and people's conceptions of the good life very a lot. Well, you know, everybody needs to be healthy in order to achieve any other goal. So this is kind of a, of a universal goats that seems safe to promote. Um, now taking this further, what about uncertainty and risk? I mean, yes, it's totally legitimate to ask. Well, you know, what's the probability that the certain antipoverty intervention fails to have the intended effect? Or maybe there's even a probability that it will have a harmful effect. Uh, and of course we should factor that in as well. And we can start, you know, at least in theory to address that sort of issue by modifying the initial thought experiments.

Speaker 1:          32:52          I mean, what, what if you have like a 50% probability to save a drowning child, you know, you know that maybe you're not going to make it in time, time is short. Maybe the child will still die despite their effort. I mean, in that case, if the situation is, you know, either not do anything or do something and have a 50% chance of success or even maybe just a 10% chance of saving their life, we will probably still say, yeah, it's worth it. And then of course we can further, we can introduce further complications while if we have, let's say an 80% chance to save the life, but the 1% chance to cause harm in some way. Maybe there are other children in the round and maybe we would, whatever, you know, unintentionally push a child into the lake or whatever. I mean complications could happen with harmful unintended side effects.

Speaker 1:          33:38          And then f and then we can try and reason. Okay. Under what circumstances with wild probability distributions would we still go for it? And uh, we can also consider dilemmas like what if we have a 10% chance to save 100 lives versus a 100% chance to save five? What would you do? What should you do? And um, what are the thresholds maybe? So maybe we could start with a 100% probability to save a certain number, go down to 90% and compare it to the, to another post decision possibility. And a standard framework for addressing this kind of situation is the expected value framework. So just taking the probability times the stakes. So here at 10% chance times 100 versus a 100% chance. So one times five and the expected value here would recommend the first option. So all 0.1 times 100 equals 10. But of course we could ask, well, but shouldn't we be risk averse at least a bit?

Speaker 1:          34:44          Doesn't risk aversion makes sense. So if we have a one 100% probability to save five versus just a 10% chance to save a hundred, we might feel uneasy about going for the 10% option. I mean, one replies, if that's not a one shot game, but if many people do the same or if you do the same many times over, well then you know, uh, due to the law of large numbers, of course you are going to save more people. In fact, if you go for the highest expected value, uh, so that's one reply. Um, but yeah, even if it's one shot, I think a pretty good case can be made here for going for risk neutrality. So expected value as opposed to some risk averse function. I think our risk aversion comes from our being accustomed to thinking about decisions, situations that are about earning money or making money for personal utility.

Speaker 1:          35:35          And if it's, if, if, if we're about making money for personal utility, then risk aversion seems to make perfect sense. Why? Because money tends to have diminishing marginal utility, at least in terms of personal gain. So the first $10,000 that you earn per year are the hugely important. They enable you to survive the second 20 k that you add our bid list and so on and so forth. So that's, that seems to be the dynamic when it comes to personal gain. But that same argument doesn't seem to apply when it comes to altruism because there, yeah, if you save the first life, great. If you safe the second one, it's not like the second one is less valuable. It's similar, really valuable to the person, um, themselves. And so if we're being altruistic, I think we should not, we should not assume that they're already made, that there's diminishing marginal utility there.

Speaker 1:          36:26          And so this kind of argument for risk aversion does no longer apply. So I would probably argue for going for expected value maximization. And when it comes to altruism and taking the probability times the stakes, and this leads to a concept that a effective altruists call hits based philanthropy or hit spaced giving. I mean, let's consider an example. In the 20th century. It's again from the health area, smallpox killed more than 300 million people up to its eradication in the 70s so a huge evil in terms of the consequences. That's more than all wars, all genocides, all political violence and famines combined. So an insane, uh, victim count. But we were able to beat it and have since saved an estimated 60 to 120 some uncertainty. There are 60 to 120 million lives since the eradication. And so there's a pretty interesting and complex story behind how that happened and x untie it also seems quite unlikely to succeed in our many people are saying now it's, these are crazy plans snuck on to succeed in terms of smallpox eradication.

Speaker 1:          37:37          But the point is if the stakes are so high, then even if the success chance is very low, the expected value can still be enormous and it can be extremely worthwhile to pursue something that's most likely to fail. So you're pursuing a plan, deliberately pursuing an altruistic plan that's most likely to fail, maybe only has a 5% success success chance, but because the stakes are so high, the expected value can still be enormous. And so that's the concept of sort of headspace giving, trying to do maybe many things at once, trying to have various individuals and various groups or organizations work on such situations of low probability of success and high stakes of course the for profit analogy, again, this, his headspace investment. I mean you can do that with your investment portfolio. Uh, it's if you have, if you're pursuing many such situations or opportunities at once with like low success probability but high stakes, that's also a form of hedging.

Speaker 1:          38:35          Needless to say, due to law of large numbers again, and this concept can be transferred to, to philanthropy in very productive ways. Now some applications, um, off headspace giving are to be found specifically in the course of the area of global catastrophic risks. Some of them being the worst case climate change scenarios, which might be like, you know, sort of really extreme global chaos and maybe even some extinction risk. But that seems, I mean climate change seems pretty bad, but the absolute worst case scenario seem pretty unlikely, but still the stakes are enormous. Maybe nuclear war seems unlikely at least on a, on an extinction level scale. But the stakes, again, extreme buyer security, maybe unlikely, but it's not clear. I mean I'm, I'm also not claiming that with these risks to situation is necessary necessarily, uh, off the sword that the risk is low probability.

Speaker 1:          39:39          I mean, if the risk is high probability, then we should be addressing it all the more. But the argument is, even if the risk is low probability and even if our success chance of doing something to mitigate the risk is also low, the expected value of trying hard can still be pretty high and a further cost area that some effective altruists have been thinking about and researching is, um, artificial intelligence opportunities and risks from a possible transition to a superhuman intelligence and how to maybe steer, um, such a transition to maximize the benefits. And in order to conclude, let's zoom in on this cause area briefly. Um, I should maybe premise it by saying that I'm not myself and a, an AI expert. I'm a trained philosopher and a half done a lot of entrepreneurship. And if you're interested in what the AI experts in effective altruism have to say more specifically, then, uh, I'd encourage you to check out the organizations and the websites that I'll mention at the end.

Speaker 1:          40:45          So this will just be a brief, um, intro and philosophical overview. Um, so many technical experts do seem to predict the emergence of artificial superintelligence. So machines that outperform humans in literally every, a domain of cognitive interest. Uh, later this century that seems to be a prediction that many technical experts are willing to make. And of course if that materializes, it's likely to be a change that's more disruptive on a global scale than the evolutionary sort of ape to human transition has been. And this has been an extremely disruptive transition of course. So the transition from apelike brains to human brains, I mean up to the point where you know, the goal achievement and the very survival of apes of chimps depends a lot more on us humans than it depends on them. So if artificial superintelligence or super intelligence is emerge and I mean, you know, there are various scenarios, maybe there will also be like a, a corresponding enhancement of our human brains.

Speaker 1:          41:53          There is scenarios there that are possible. But I mean then it's, yes, it seems quite likely that our goal achievement and our very survival will depend more on on the super intelligences. Then it would depend on us. And this raises the question well, um, will these AI's goals be stably aligned with our goals or not? And it seems that if they are, if they are stable, aligned with our goals, then that could be the biggest opportunity for humanity ever. I mean, of course, uh, technological progress has brought humanity a great many benefits and this could sort of be the ultimate benefit. Um, maybe the last one mentioned we need to make and in principle it could solve all our problems because the tool that we have been using of course in history to solve problems is our intelligence. And if we end up with a human with a superhuman intelligence that's beneficial, uh, meaning, uh, that's pursuing goals that are identical with ours, uh, goals that we'd consider good and valuable, uh, then that could be, yes, really the biggest opportunity for humanity ever.

Speaker 1:          43:05          But there could also be some risks, namely if, um, the superintelligence his goals are not aligned or not stapling aligned. I mean, it could also be the case that they are aligned at first, but then sort of, uh, because there's further developments become misaligned, that could also be a problem. So that could also be the risks, the risks that we're going to be faced with a kind of super intelligent power that's not stable goal aligned with us. So it seems that this transition could be extremely disruptive. The s the stakes could be enormous, literally the biggest stakes ever. And depending on whether, um, goal alignment will be the case, the outcome could be extremely good or quite dangerous. And that's basically the problem context that many effective altruists have been thinking about. Now from a sort of more philosophical, non technical point of view also in the public debate, uh, some people have been like dismissing the whole argument for various reasons and well their common denominator is often that they think that, well, the probability either of these scenarios materializing is super low.

Speaker 1:          44:15          And even if the probability were high, then the probability would be low of, uh, are, are being successfully able to steer the development in any, in any direction that we would prefer. So yeah, some people say, well, the probability of, you know, any fears of goal misalignment applying is extremely low. Um, and the probability of goal alignments succeeding through sort of a strategic effort on our part is also extremely low. And from this day conclude that yeah, it's not a cause area worth pursuing in any serious way, but sort of from a decision theoretic and philosophical point of view, I'd say, well, um, even if even if that's completely correct, if that were totally accurate, that let's say it were extremely unlikely that superintelligence would emerge at all or like they're very far into the future only. And if we also add that it's in any case, extremely improbable that we would sort of succeed with a strategic effort to try and maximize the probability of an awesome, a utopian outcome, even if both of these points apply, well the expected value could still be enormous.

Speaker 1:          45:26          So it even, even even knowing that our deliberate effort at steering the development is unlikely to succeed, even if that were the case, the expected value of trying hard could still be high. So that's I think, an important point to make from a sort of decision theoretic perspective. Um, and that brings me to the second part. Last slide already. Uh, what I've tried to do in this talk is give a brief intro, like a justification for the spirit of ea and an overview of like main strategies and cause areas pursuit. And last but not least, I'd like to mention some organizations and their websites if you're interested in checking out the material in greater detail. I uh, did mention Gif, well, um, the charity research think tank mainly focusing on the cause area of world poverty in global health. Then there's giving what we can, which is an organization where you can sort of take a pledge to take this 10% pledge the dimension than several thousand people have already taken this pledge to donate 10% to the most effective cause areas they can find.

Speaker 1:          46:33          There's $80,000 the organization providing career advice in the German area. There's the effective altruism foundation doing various things. Also providing some career advice and donation advice. And for the cause area of Ai Safety specifically, there's the Berlin based Foundational Research Institute whose research you can check out or a the u s based, um, machine intelligence research institute mirror, which you may have heard of. Um, and uh, yes, so as I said, I'm a philosopher by training, but uh, you can find the computer science and machine learning literature by people who believe that this is an important cause area, um, on these websites for instance. And to conclude, so this is sort of the animating spirit behind all of these organizations are keen awareness that this is probably in the very real and urgent sense. The situation we are in globally. It may not feel that way evolutionarily because we are still running on sort of stone age brains and emotionally we have a hard time grasping the situation of a global village, which is really an interconnected global village.

Speaker 1:          47:44          That's a historical first. And so emotionally we're not really able to grasp that. But intellectually we are, and it's probably through that this is our situation, but it's also true that having a beer from time to time is supreme. We strategically important, we need to minimize burnout risk and a, yeah, the effective altruist communities all around the world are very welcoming. And there's also community in, uh, in Zurich here. Pretty big community in Switzerland and Germany. And Yeah, if you're interested, these are also just the kind of people that like to go grab a beer and, um, and have a philosophical and also technical discussions. And with that being said, thanks again for your interest and your attention and I'm looking forward to your questions and comments.

Speaker 2:          48:27          Thank [inaudible].