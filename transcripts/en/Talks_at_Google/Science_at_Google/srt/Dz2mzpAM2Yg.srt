1
00:00:06,100 --> 00:00:10,290
Please join me and warmly welcoming
to Google Brendan Nyhan and Dj flame.

2
00:00:17,120 --> 00:00:17,241
Well,

3
00:00:17,241 --> 00:00:20,480
thank you Laura for organizing this and
thanks to all of you for being here.

4
00:00:20,481 --> 00:00:23,400
It's super exciting to
be here and to talk, um,

5
00:00:23,420 --> 00:00:26,900
talk with you all about misinformation
today. Um, so our plan is,

6
00:00:26,901 --> 00:00:28,660
I'm going to talk, uh, first, uh,

7
00:00:28,670 --> 00:00:33,200
that's the top half of the slide here
about the political system today and why,

8
00:00:33,400 --> 00:00:38,030
um, different aspects of American
politics today are conducive to creating

9
00:00:38,031 --> 00:00:42,710
misperceptions that influence people's
attitudes and behaviors and often persist

10
00:00:43,220 --> 00:00:46,760
for a long period despite the
provision of accurate information.

11
00:00:46,761 --> 00:00:48,440
So I'm going to talk about that first.
Um,

12
00:00:48,560 --> 00:00:50,870
and then I'm going to turn it over to
Brendan who's going to talk about it a

13
00:00:50,871 --> 00:00:55,730
little bit more specifically about the
role of fake news and fake news in 2016

14
00:00:56,060 --> 00:01:00,170
in particular, does a roadmap
of, of my part of the talk.

15
00:01:00,171 --> 00:01:04,430
Today I'm going to begin by talking about
some contextual elements in American

16
00:01:04,431 --> 00:01:07,760
politics that are conducive to
misinformation as I mentioned. Um,

17
00:01:07,890 --> 00:01:11,690
I'll then give some examples to give
you a sense of the extent of partisan

18
00:01:11,691 --> 00:01:14,820
differences in factual
beliefs about politics. Um,

19
00:01:14,840 --> 00:01:18,240
I'll talk about some institutional
responses that we've seen, uh,

20
00:01:18,260 --> 00:01:21,080
to misinformation mostly
in the United States. Um,

21
00:01:21,081 --> 00:01:25,730
and then I'll outline some challenges
and opportunities moving forward. Um,

22
00:01:25,850 --> 00:01:29,110
so to begin with the
contextual discussion, um,

23
00:01:29,180 --> 00:01:33,710
I think any discussion of
American politics today has
to start by acknowledging

24
00:01:33,711 --> 00:01:38,270
that the huge and really historic levels
of partisan polarization that we see at

25
00:01:38,271 --> 00:01:40,530
the elite level in the United States.
Um,

26
00:01:40,550 --> 00:01:45,550
so this is a plot showing a roll call
ideology spores in Congress for members of

27
00:01:46,161 --> 00:01:50,030
the Democratic Party, uh, in the
Republican Party over time. Um,

28
00:01:50,090 --> 00:01:54,320
so higher scores here are more liberal
voting records in Congress and lower

29
00:01:54,321 --> 00:01:57,360
scores on the y axis are more
conservative records. Um,

30
00:01:57,370 --> 00:02:00,470
and as you can see by this
increasing divergence over time,

31
00:02:00,590 --> 00:02:04,940
we have a dramatic polarization in
ideology among political elites.

32
00:02:05,240 --> 00:02:08,670
That is to say Democrats are becoming
more and more liberal in their roll call.

33
00:02:08,671 --> 00:02:11,240
Voting, Republicans becoming
more and more conservative,

34
00:02:11,480 --> 00:02:15,600
and there's just less area of ideological
agreement at the elite level. Um,

35
00:02:15,890 --> 00:02:16,730
which as we'll see,

36
00:02:16,731 --> 00:02:19,790
has important implications for
how partisans in the mass public,

37
00:02:19,791 --> 00:02:24,791
how ordinary Democrats and Republicans a
form opinions and factual beliefs about

38
00:02:25,041 --> 00:02:27,900
politics. Um, another, uh,

39
00:02:27,950 --> 00:02:31,790
really important institutional change
that we've seen in recent in recent

40
00:02:31,790 --> 00:02:34,430
decades is the resurgence
of partisan media. Um,

41
00:02:34,460 --> 00:02:39,460
so we now see explicitly ideological
and sometimes partisan outlets providing

42
00:02:39,801 --> 00:02:43,580
information. Um, this is important
for a huge number of reasons,

43
00:02:43,581 --> 00:02:47,760
but I'll focus just on one, it's
relevant for the discussion today. Um,

44
00:02:47,870 --> 00:02:52,670
and that is that when you have a highly
partisan, highly fragmented media system,

45
00:02:52,980 --> 00:02:53,570
um,

46
00:02:53,570 --> 00:02:58,100
what you have is you have people
watching certain channels and coming away

47
00:02:58,160 --> 00:03:01,720
watching different of channels and
coming away with fundamentally different

48
00:03:01,721 --> 00:03:04,900
understandings of the same issue or event.
Okay.

49
00:03:04,901 --> 00:03:09,100
So if we think about events that are in
the news today and how the same event is

50
00:03:09,101 --> 00:03:13,720
covered by Sean Hannity on Fox, for
example, in Rachael Maddow on MSNBC,

51
00:03:14,090 --> 00:03:17,080
um, they might be covering
the same sort of larger story,

52
00:03:17,380 --> 00:03:21,160
like the potential link between
the Trump campaign and Russia,

53
00:03:21,670 --> 00:03:24,190
but they cover it from very,
very different angles.

54
00:03:24,400 --> 00:03:28,540
And so partisan viewers of each network
come away with very different factual

55
00:03:28,541 --> 00:03:33,370
understandings of the same issue. Um,
this is, uh, this is a relatively new,

56
00:03:33,620 --> 00:03:38,170
um, aspect of, of the media system,
right? So, um, up until recent decades,

57
00:03:38,171 --> 00:03:43,030
we had a relatively homogeneous media
system with a small number of channels,

58
00:03:43,031 --> 00:03:46,690
right? Not as much partisan choice.
And that's important because, uh,

59
00:03:46,691 --> 00:03:48,850
it led to the opposite of
what we have today, right?

60
00:03:48,851 --> 00:03:51,840
People had a smaller selection of,
uh,

61
00:03:52,000 --> 00:03:55,990
news channels to watch and they came
away with the same basic set of facts,

62
00:03:56,380 --> 00:04:00,670
which we increasingly no
longer no longer half. Now,

63
00:04:00,671 --> 00:04:05,140
one important result of this resurgence
in partisan media and increase in

64
00:04:05,141 --> 00:04:09,220
partisanship and the mass public is
a large increase in what we political

65
00:04:09,221 --> 00:04:12,130
scientists call an affective polarization.
Um,

66
00:04:12,131 --> 00:04:16,210
so AFX rates just simply
how you feel about a certain
group, usually an outgroup.

67
00:04:16,690 --> 00:04:20,260
Um, so there are lots of different ways
of measuring an affective polarization in

68
00:04:20,261 --> 00:04:23,620
politics. Um, this is one of the
more clever ones on the top here.

69
00:04:23,621 --> 00:04:28,030
This shows the trend in the percent of
people who would be upset if a member of

70
00:04:28,031 --> 00:04:32,990
their immediate family married a member
of the other political party. Um,

71
00:04:33,040 --> 00:04:35,410
there are lots of measures
of effect of polarization,

72
00:04:35,411 --> 00:04:38,620
but this is one of the more
interesting ones I think. Um,

73
00:04:38,621 --> 00:04:42,340
and you see the pretty
steady linear linear increase
over the time series on the

74
00:04:42,341 --> 00:04:46,750
x axis and his huge spike
around 2008, 2009. Okay.

75
00:04:48,370 --> 00:04:51,930
I'm sorry.
Um,

76
00:04:53,500 --> 00:04:56,560
the line might be smooth. I'm not,
I'm not sure. These are my data. Yeah,

77
00:04:56,740 --> 00:04:58,570
I think there's a lowest
smoother on the line.

78
00:04:59,320 --> 00:05:03,130
Another sort of more straight forward,
a more straightforward measure of this,

79
00:05:03,131 --> 00:05:06,100
right, is just how do you
feel sort of a zero to 100.

80
00:05:06,101 --> 00:05:09,400
Do you feel cool toward the opposing
party or warm toward the opposing party?

81
00:05:09,640 --> 00:05:12,960
And we see pretty steady decreases among,
uh,

82
00:05:13,180 --> 00:05:16,450
people across the partisan spectrum in
terms of how they feel about members of

83
00:05:16,451 --> 00:05:18,580
the opposing the opposing party.

84
00:05:18,581 --> 00:05:21,670
And that that negative trend
is especially dramatic, right?

85
00:05:21,671 --> 00:05:26,260
Among strong Democrats and
strong Republicans. Um, now
that's important, right?

86
00:05:26,261 --> 00:05:29,530
Because people who tend to be
strongly partisan participate more,

87
00:05:29,890 --> 00:05:32,920
they're more likely to vote, they're more
likely to contact members of Congress.

88
00:05:33,100 --> 00:05:35,890
And so those, um, highly
politically active,

89
00:05:35,920 --> 00:05:40,120
influential people are more and more as
effectively polarized towards members

90
00:05:40,121 --> 00:05:43,570
the other party. Okay. Um,
and these are some of the,

91
00:05:43,571 --> 00:05:46,240
these are some of the
contextual changes in,

92
00:05:46,250 --> 00:05:49,840
in the US political system in recent
years that have led to really dramatic

93
00:05:49,900 --> 00:05:53,440
partisan differences in factual
beliefs. Okay. So I, um,

94
00:05:53,500 --> 00:05:56,530
I'm showing some examples here that
I think many of us will recognize,

95
00:05:56,531 --> 00:05:59,680
but I think the data, I'm
really help us understand, uh,

96
00:05:59,720 --> 00:06:04,310
how much factual polarization we've seen
on these issues over time. Um, which is,

97
00:06:04,311 --> 00:06:09,080
uh, which is, uh, um, that
time trend often gets, often
gets lost, right. Um, in,

98
00:06:09,081 --> 00:06:13,790
in, in media coverage. Um, and so we see
partisan differences in factual beliefs.

99
00:06:13,791 --> 00:06:17,420
About major social and
economic problems, like, uh,

100
00:06:17,450 --> 00:06:21,860
beliefs about the cause of global
warming. Um, and, and lots of,

101
00:06:21,861 --> 00:06:25,910
lots of other, a really hugely
influential policy issues. Um,

102
00:06:25,911 --> 00:06:27,140
so this is just showing,
right?

103
00:06:27,141 --> 00:06:30,620
The percent of people who believe that
the rise of the Earth's temperature is

104
00:06:30,621 --> 00:06:34,280
due mostly to human activity among
Democrats, Republicans and independents.

105
00:06:34,460 --> 00:06:38,620
And you see this increasing
divergence over time. Okay.

106
00:06:39,080 --> 00:06:43,460
Um, we also see these partisan differences
in factual beliefs on the content of

107
00:06:43,461 --> 00:06:46,710
major legislation, right? Like the
affordable care act or tax reform.

108
00:06:46,711 --> 00:06:50,620
And it's being debated now, or Aca
repeal that's being debated now. Um,

109
00:06:50,630 --> 00:06:55,010
so this is a really a really cool a graph
from a paper that Brendan Road in 2010

110
00:06:55,011 --> 00:06:58,040
which I often a steal for talks like this.
Um,

111
00:06:58,041 --> 00:07:02,140
so this shows the percent of people
who believe in the left panel, um,

112
00:07:02,240 --> 00:07:06,560
that the Clinton healthcare
reform proposal in the
early 1990s would result in

113
00:07:06,561 --> 00:07:10,490
people losing their choice of
doctor. Um, and it shows the belly,

114
00:07:10,491 --> 00:07:13,820
it shows belief in that, in that claim
by different partisan groups, right?

115
00:07:13,821 --> 00:07:17,570
Democrats and Blue Republicans
and red independence in green. Um,

116
00:07:17,750 --> 00:07:21,290
and what's even more interesting here,
and maybe a little bit depressing, right?

117
00:07:21,410 --> 00:07:22,640
It's,
you'll notice there are two,

118
00:07:22,641 --> 00:07:25,870
there are two points for
each party corresponding, uh,

119
00:07:25,880 --> 00:07:30,800
down here on the x axis to people who
report that they have, um, uh, two,

120
00:07:30,801 --> 00:07:34,370
it's too early or they don't know very
much about the plan versus partisans who

121
00:07:34,371 --> 00:07:36,440
say they have a really good
understanding of the plan.

122
00:07:36,470 --> 00:07:39,920
So these are people who self report that
they know a lot about the plan and we

123
00:07:39,921 --> 00:07:41,690
see that the more people think they know,

124
00:07:41,720 --> 00:07:45,020
the more misinformed they
actually are in some cases, right?

125
00:07:45,021 --> 00:07:49,460
So we see huge partisan disagreement
among partisans who claim to have a really

126
00:07:49,461 --> 00:07:53,900
good factual understanding
of these issues. Um, and we
see a similar pattern, uh,

127
00:07:53,980 --> 00:07:58,160
about the death panel claim and the
affordable care act in 2009 on the right

128
00:07:58,161 --> 00:08:02,510
side. Okay. Um, this is perhaps
the most famous example, right,

129
00:08:02,511 --> 00:08:07,010
of partisan differences in factual beliefs
in recent years. This is the shows,

130
00:08:07,070 --> 00:08:09,620
right? Differences in the belief
that Barack Obama was not born.

131
00:08:09,621 --> 00:08:13,880
It was or was not born in the United
States. Uh, not surprisingly, right? Huge,

132
00:08:13,881 --> 00:08:18,770
huge differences here. Okay. Um,
now, so we have, we have these,

133
00:08:18,830 --> 00:08:19,430
uh,

134
00:08:19,430 --> 00:08:23,060
this partisan disagreement and on all
these different factors that play a large

135
00:08:23,061 --> 00:08:24,700
role in policy debates.
Um,

136
00:08:24,740 --> 00:08:29,000
and we've seen a lot of institutional
responses to this and attempts to correct

137
00:08:29,001 --> 00:08:30,890
misperceptions and bring,

138
00:08:31,040 --> 00:08:34,550
bring partisans together and sort of
their factual understanding of politics.

139
00:08:34,830 --> 00:08:37,130
I'm the first one that I think
is really important, right,

140
00:08:37,131 --> 00:08:39,650
is the rise of political fact checking.
Um,

141
00:08:39,770 --> 00:08:44,000
so there's this nice new report out by
the Duke reporters' lab that shows that

142
00:08:44,001 --> 00:08:49,001
tracks the trend and the growth of fact
checking or for the past 10 or so years,

143
00:08:49,460 --> 00:08:53,650
and we've seen about 150% explosion
in fact checkers across, um,

144
00:08:53,700 --> 00:08:58,340
across the globe in the last three years
in response to this, uh, this upsurge in,

145
00:08:58,341 --> 00:09:03,240
in misinformation and democracies
across the world. Right?

146
00:09:03,440 --> 00:09:07,680
Um, really a related point, right? We've
seen an uptick in watchdog journalism.

147
00:09:07,830 --> 00:09:11,010
So even as, um, a lot
of newspapers, right,

148
00:09:11,011 --> 00:09:13,590
face financial burdens and they're
shrinking their news teams,

149
00:09:13,591 --> 00:09:17,250
we've seen a lot of papers sort of re uh,
recalibrate their,

150
00:09:17,251 --> 00:09:19,690
their focus and focus a lot more on,
um,

151
00:09:19,710 --> 00:09:24,450
investigative watchdog style journalism.
Um, so these are some headlines from,

152
00:09:24,480 --> 00:09:26,310
from very recent months,
right,

153
00:09:26,311 --> 00:09:31,311
about fact checking claims made by
Hillary Clinton and Donald Trump played a

154
00:09:31,681 --> 00:09:35,640
large role, uh, in, in the campaign
and still play a large role today.

155
00:09:36,510 --> 00:09:38,550
Okay. Um, now of course,

156
00:09:38,551 --> 00:09:42,390
there are still a lot of challenges ahead
for fact checkers and other people who

157
00:09:42,391 --> 00:09:45,520
seek to mitigate
misinformation in politics. Um,

158
00:09:45,690 --> 00:09:50,280
the first is declining trust in media
over time, right? So people, uh,

159
00:09:50,340 --> 00:09:51,140
people,
uh,

160
00:09:51,140 --> 00:09:56,010
self reported trust in media has declined
pretty steadily in recent decades.

161
00:09:56,220 --> 00:10:00,180
Um, it doesn't help when
you have a prominent members
of both parties, frankly,

162
00:10:00,181 --> 00:10:03,930
who regularly make a
arguments in media media,

163
00:10:03,931 --> 00:10:05,430
can't be trusted and can't be,

164
00:10:05,940 --> 00:10:10,580
can't be trusted to adjudicate these
factual disputes that we see. Okay.

165
00:10:11,060 --> 00:10:12,400
Um,
there's also this,

166
00:10:12,401 --> 00:10:15,630
this echo chamber problem which Brendan's
going to talk about in a moment.

167
00:10:15,970 --> 00:10:17,640
And this is the idea that,
um,

168
00:10:17,670 --> 00:10:21,510
given that explosion and media choice
that I talked about in the beginning, um,

169
00:10:21,540 --> 00:10:26,310
people sometimes a expose themselves to
information selectively. That is to say,

170
00:10:26,520 --> 00:10:29,970
Democrats for instance, might watch
MSNBC or read the Huffington Post.

171
00:10:29,971 --> 00:10:34,770
Republicans might watch box or read
Breitbart. Um, you know, there's a lot of,

172
00:10:34,800 --> 00:10:35,340
uh,

173
00:10:35,340 --> 00:10:38,190
new research with really cool data that
you're going to hear about in a second,

174
00:10:38,350 --> 00:10:42,720
uh, tracking the extent of this and
who actually engages in this. Um,

175
00:10:42,840 --> 00:10:47,040
but this is, this is a
challenge to be sure. Um,

176
00:10:47,400 --> 00:10:50,580
another, another sort of disclaimer
that I just want to give before I,

177
00:10:50,581 --> 00:10:55,581
before I conclude is that a lot of the
examples that I've focused on in the talk

178
00:10:55,890 --> 00:10:58,440
are highly partisan facts,
right?

179
00:10:58,441 --> 00:11:02,550
By definition that are at the center of
partisan disagreement. And you might say,

180
00:11:02,551 --> 00:11:03,241
Oh, well, you know,

181
00:11:03,241 --> 00:11:06,660
you're always going to see partisan
disagreement on these sorts of issues.

182
00:11:06,661 --> 00:11:09,130
Perhaps that's not super troubling.
Um,

183
00:11:09,180 --> 00:11:12,690
but I want to highlight that
we see a lot of misperceptions,

184
00:11:12,691 --> 00:11:17,130
factual misperceptions that are widely
held by members of both parties. Okay.

185
00:11:17,220 --> 00:11:21,390
It means exist on really important issues
like foreign aid. This is one example.

186
00:11:21,650 --> 00:11:22,970
Um,
uh,

187
00:11:23,040 --> 00:11:28,040
people tend to vastly overestimate the
size of outgroups like immigrants and

188
00:11:28,261 --> 00:11:31,560
minority groups in general. Um, there,

189
00:11:31,580 --> 00:11:35,400
there are huge misperceptions
about, uh, tax policy,

190
00:11:35,401 --> 00:11:37,080
which is being debated right now,

191
00:11:37,081 --> 00:11:39,840
right about the percent of the budget
that goes to various things like,

192
00:11:40,070 --> 00:11:44,550
like foreign aid, uh, defense and
that sort of thing. Um, so it's,

193
00:11:44,551 --> 00:11:46,590
it's not just that we see,
um,

194
00:11:46,650 --> 00:11:49,260
partisan disagreement on these
highly charged issues, right?

195
00:11:49,261 --> 00:11:53,540
We see them broadly across lots of
different issues that are coming up. Um,

196
00:11:54,130 --> 00:11:58,900
now there are of course opportunities to,
to address some of this.

197
00:11:59,130 --> 00:12:03,370
Um, so I'll just, I'll just highlight
two, uh, in closing here. Um,

198
00:12:03,400 --> 00:12:07,000
so the first potentially useful
strategy for practitioners, right,

199
00:12:07,001 --> 00:12:10,810
who were interested in combating this
information made it's to call people out,

200
00:12:10,930 --> 00:12:14,140
which is largely what fact checkers do
and try to increase the reputational

201
00:12:14,141 --> 00:12:16,310
costs of false statements.
Um,

202
00:12:16,450 --> 00:12:19,960
so if politicians feel that they have to
pay a price when they make an accurate

203
00:12:19,961 --> 00:12:20,950
statements,
uh,

204
00:12:20,951 --> 00:12:24,910
we can try to attack this sort of supply
side of misinformation rather than

205
00:12:24,911 --> 00:12:27,140
focusing on what happens sort of post,
uh,

206
00:12:27,310 --> 00:12:30,790
post fall statement and
correcting people's misperceptions
after they hold them.

207
00:12:31,160 --> 00:12:34,060
Um,
this is sort of the fact checking model,

208
00:12:34,240 --> 00:12:37,540
but I want to emphasize that of
course you don't have to be in sort of

209
00:12:37,541 --> 00:12:42,050
institutionalized fact checking
organization to do this. Um, uh,

210
00:12:42,460 --> 00:12:46,240
uh, tools, uh, at places like
Google and Facebook and elsewhere.

211
00:12:46,241 --> 00:12:49,810
We've made efforts at flagging
false claims at the point of,

212
00:12:49,930 --> 00:12:54,760
at the point of communication. Try this
the same, the same strategy, right? Um,

213
00:12:54,970 --> 00:12:55,691
in the second,

214
00:12:55,691 --> 00:12:59,620
the second opportunity I want to
highlight is there a lot of really highly

215
00:12:59,621 --> 00:13:02,590
credible sources, um, that can, uh,

216
00:13:02,650 --> 00:13:06,400
combat miss misperceptions even when
they're highly polarized along political

217
00:13:06,401 --> 00:13:09,490
lines. Um, one, uh, situation,

218
00:13:09,491 --> 00:13:13,180
which misperceptions are often a little
bit easier to correct a is when you can

219
00:13:13,181 --> 00:13:17,590
get a partisan or ideological actors
speaking against his or her own self

220
00:13:17,591 --> 00:13:21,460
interest, that's often highly persuasive,
right? There are lots of experiments,

221
00:13:21,510 --> 00:13:24,220
for instance, that show
that if you can get, um,

222
00:13:24,340 --> 00:13:27,250
a Republican to combat the plane,

223
00:13:27,251 --> 00:13:29,380
that there were death panels
in the affordable care act,

224
00:13:29,560 --> 00:13:31,360
that that's a much more persuasive Q.

225
00:13:31,361 --> 00:13:34,510
Then if you can get somebody who is just
totally in support of the affordable

226
00:13:34,511 --> 00:13:38,410
care act for instance to do that. And
there are lots of other examples. Um,

227
00:13:38,920 --> 00:13:39,551
and finally,

228
00:13:39,551 --> 00:13:44,080
unexpected sources are often really
highly persuasive in correcting this

229
00:13:44,081 --> 00:13:46,770
perception. So there's a neat
new paper that shows, uh,

230
00:13:46,850 --> 00:13:51,310
that when corrected information about
the human causes of global warming is

231
00:13:51,311 --> 00:13:55,480
provided by military organizations or
defense related entities as opposed to

232
00:13:55,481 --> 00:13:56,314
scientists.

233
00:13:56,440 --> 00:14:00,550
It's just a more surprising source
and it sort of disarms people and has,

234
00:14:00,551 --> 00:14:02,700
it has a much more powerful impact.
Um,

235
00:14:02,710 --> 00:14:05,860
so these are just two opportunities that
I wanted to flag that have sort of come

236
00:14:05,861 --> 00:14:10,720
up in recent research. There are certainly
others. Um, uh, but with that I'll,

237
00:14:10,721 --> 00:14:11,800
I'll uh,
turn it over to Brendan.

238
00:14:13,070 --> 00:14:13,730
All right.

239
00:14:13,730 --> 00:14:18,730
So what I want to do now is turn more
specifically to the 2016 election that

240
00:14:19,430 --> 00:14:20,091
what Djs don't.

241
00:14:20,091 --> 00:14:23,780
Hopefully it's given you an overview
of why we might be especially concerned

242
00:14:23,781 --> 00:14:27,860
about people's vulnerability to miss
perceptions in the partisan age that we

243
00:14:27,861 --> 00:14:29,330
live in, in this country. Um,

244
00:14:29,331 --> 00:14:33,920
so I'm going to talk specifically about
the 2016 election focusing on fake news.

245
00:14:34,100 --> 00:14:35,390
But you should of course think of that.

246
00:14:35,391 --> 00:14:37,280
And I'm going to say what I mean
by fake news in a moment, but,

247
00:14:37,281 --> 00:14:39,650
but you should think of that as a
subset of the larger problem with

248
00:14:39,651 --> 00:14:40,520
misinformation.

249
00:14:40,790 --> 00:14:44,900
The DJ has been talking about fake news
is one particular manifestation of the

250
00:14:44,901 --> 00:14:48,680
ways in which people's
political commitments can
influence what they choose to

251
00:14:48,681 --> 00:14:52,850
believe is true in the information they
consume. But it's hardly the only one.

252
00:14:52,880 --> 00:14:56,300
And in some ways it's important to
maybe overstated as I'll talk about the

253
00:14:56,301 --> 00:15:01,100
concern that a lot of people have had
have expressed since the election is

254
00:15:01,101 --> 00:15:05,630
weather. Uh, people are not just
living in an echo chambers of opinion,

255
00:15:05,631 --> 00:15:08,210
but echo chambers of fact,
right? In other words,

256
00:15:08,300 --> 00:15:13,040
it's not just that we tend to interact
both online and off with people who share

257
00:15:13,041 --> 00:15:14,480
our political commitments,

258
00:15:14,630 --> 00:15:18,710
but that we may actually be only hearing
from or disproportionately hearing from

259
00:15:18,711 --> 00:15:21,710
people who share our same
understanding of the world. Right?

260
00:15:21,860 --> 00:15:24,080
And that's not just from
your social counterparts,

261
00:15:24,081 --> 00:15:29,060
but from the information you consume. And
that's very worrisome to people, right?

262
00:15:29,061 --> 00:15:33,560
Even if we disagree about opinions,
uh, about what we should do, uh,

263
00:15:33,920 --> 00:15:37,700
you'd like to think that we could have
a debate as a democracy based on some

264
00:15:37,701 --> 00:15:41,690
shared set of facts. And the fear that's
come out of this election of course,

265
00:15:41,691 --> 00:15:44,850
is that that's no longer the case.
So I don't want to talk about, um,

266
00:15:45,140 --> 00:15:49,370
fake news in 2016 election in the context
of that question and help you try to

267
00:15:49,371 --> 00:15:53,150
get you, give you some intuition on
how you might think about that problem.

268
00:15:54,530 --> 00:15:54,711
Okay.

269
00:15:54,711 --> 00:15:58,070
So this is my technical analysis of the
state of a factual discourses in the

270
00:15:58,071 --> 00:16:01,910
2016 election. Right? If it seemed
terrible to you, it was terrible.

271
00:16:01,940 --> 00:16:04,700
We studied this full time
and, uh, we agree. I mean,

272
00:16:04,701 --> 00:16:09,701
there's no systematic way to say how
factual a political debate was over time.

273
00:16:10,011 --> 00:16:12,440
Historically, there's no agreed
upon set of standards. Uh,

274
00:16:12,680 --> 00:16:15,100
but I think it's fair to say
a qualitative judgment, uh,

275
00:16:15,200 --> 00:16:19,050
is that this election was
especially poor. Okay. Um,

276
00:16:19,760 --> 00:16:24,200
the reason for that starts
with the unprecedented level
of elite misinformation

277
00:16:24,440 --> 00:16:28,040
that came out during this
campaign. There's no way around
that fact. Okay. And the,

278
00:16:28,041 --> 00:16:28,311
you know,

279
00:16:28,311 --> 00:16:31,490
both candidates obviously said many
things that were false or unsupported.

280
00:16:31,700 --> 00:16:35,380
One of them said that said it with
historic frequency and was his,

281
00:16:35,450 --> 00:16:36,920
it was unprecedented,

282
00:16:36,921 --> 00:16:41,840
his unwillingness to retract or correct
the record when a misstatement was made.

283
00:16:42,080 --> 00:16:43,700
Okay.
We'll set that aside.

284
00:16:45,500 --> 00:16:47,690
The question of interest for
us today though, you know,

285
00:16:47,691 --> 00:16:51,050
in the second part of the talk
is what happened with fake news.

286
00:16:51,051 --> 00:16:55,070
So here are five of the most widely
shared fake news stories of the 2016

287
00:16:55,071 --> 00:17:00,071
campaign according to a sharing data
from Facebook compiled by Craig Silverman

288
00:17:00,621 --> 00:17:01,850
at buzzfeed.
Okay.

289
00:17:02,060 --> 00:17:07,060
And you can see that they're often
quite outlandish and over the top right.

290
00:17:07,971 --> 00:17:10,040
So Pope Francis Endorses Donald Trump.

291
00:17:10,190 --> 00:17:15,050
Hillary Clinton is disqualified from
office. Hillary sold weapons to isis. Um,

292
00:17:15,080 --> 00:17:20,080
and the heel FBI agent involved in the
email investigation is dead in a murder

293
00:17:20,121 --> 00:17:24,890
suicide. Okay. Um, but there are
many, many more cases like this.

294
00:17:24,950 --> 00:17:29,950
There's a whole industry of fake news
that sprung up during the campaign.

295
00:17:31,400 --> 00:17:34,700
Okay. Um, to give you a sense
of the magnitude of this,

296
00:17:34,730 --> 00:17:38,030
silvermen found that the most
shared fake news stories,

297
00:17:38,390 --> 00:17:41,600
according to the public Facebook
sharing data, of course,

298
00:17:41,601 --> 00:17:44,180
the Facebook data science team knows
more about this than than I do.

299
00:17:44,420 --> 00:17:47,410
But I can tell you from the
public sharing data that he,

300
00:17:47,430 --> 00:17:51,780
he found the most shared fake news stories
were shared more often than the most

301
00:17:51,781 --> 00:17:55,380
shared mainstream news stories in
that period before the election. Now,

302
00:17:55,381 --> 00:17:58,080
that doesn't mean that the audience
were fake news was greater. Okay.

303
00:17:58,320 --> 00:18:03,180
Because there were a few viral hits
and a lot of duds in the fake news,

304
00:18:03,450 --> 00:18:05,700
uh, in the set of fake news
articles that were published, right?

305
00:18:05,701 --> 00:18:10,040
So it's still the case that the
mainstream media was far more, uh,

306
00:18:10,110 --> 00:18:13,350
widely shared during that period.
But it does suggest how,

307
00:18:13,780 --> 00:18:14,613
uh,

308
00:18:15,350 --> 00:18:19,460
quickly these kinds of stories can
reach a very large audience. Right?

309
00:18:19,461 --> 00:18:22,880
So if you imagine this many million
shares happening on Facebook and how many

310
00:18:22,881 --> 00:18:25,070
times people are at least
passing by the headline,

311
00:18:25,310 --> 00:18:28,430
if not necessarily reading the
article, you know, there's,

312
00:18:28,880 --> 00:18:30,930
these seem to have been very widespread,
right?

313
00:18:30,931 --> 00:18:35,780
And many of you probably saw these at
some point during the election campaign.

314
00:18:36,710 --> 00:18:39,860
Okay, now it's hard to say
exactly how many, again,

315
00:18:39,980 --> 00:18:42,050
the Facebook data science
team knows I don't,

316
00:18:42,260 --> 00:18:45,470
but we can try to get answered this
question in some different ways.

317
00:18:45,680 --> 00:18:49,640
So silvermen and his colleague at
buzzfeed went back and asked people,

318
00:18:49,910 --> 00:18:53,750
did you remember seeing or hearing
about this story in the last few weeks?

319
00:18:53,930 --> 00:18:57,320
And they find somewhere
between approximately 10 and
20% of Americans said they

320
00:18:57,321 --> 00:19:00,320
did. This is a survey fielded
very soon after the election.

321
00:19:00,590 --> 00:19:02,660
Now the problem with this is you're
probably thinking is, of course,

322
00:19:02,870 --> 00:19:06,830
it's hard to remember every new
story I saw. And that's true. Okay.

323
00:19:06,860 --> 00:19:11,030
So what we'd really like to have is who
click through and who read this using

324
00:19:11,031 --> 00:19:15,140
behavioral data rather than a self
reported recall measure like this, right?

325
00:19:15,141 --> 00:19:17,840
Some of these people may be falsely
believing they saw a story and they never

326
00:19:17,841 --> 00:19:20,000
actually saw it and vice versa.
Okay.

327
00:19:20,210 --> 00:19:22,350
But it gives you a sense of
the approximate magnitude.

328
00:19:23,060 --> 00:19:27,440
And what they did find was among those
folks who said they had seen or heard

329
00:19:27,441 --> 00:19:32,090
about these stories that somewhere
around two thirds to for fis,

330
00:19:32,330 --> 00:19:35,060
uh, believe that they, these
stories were accurate. Okay.

331
00:19:35,180 --> 00:19:40,180
So it seems like there was a nontrivial
number of Americans who encounter these

332
00:19:40,851 --> 00:19:42,350
stories,
remembered them and believe them.

333
00:19:43,030 --> 00:19:43,863
Okay.

334
00:19:46,170 --> 00:19:50,880
So similar data here, uh, from two
economists, uh, Al Cotton Gins,

335
00:19:50,881 --> 00:19:52,950
Carol,
it's a little bit small on the slide.

336
00:19:52,951 --> 00:19:57,450
The grouping of interests is in
the middle portion of the y axis.

337
00:19:57,451 --> 00:20:00,780
The false stories where they asked people
again, um, do you think this was true?

338
00:20:00,781 --> 00:20:03,600
And they showed them a bunch of false
headlines as well as some true ones and

339
00:20:03,601 --> 00:20:05,040
simple CBO headlines they made up.

340
00:20:05,640 --> 00:20:09,030
And what you can see as again around 10
and 20% of Americans for any given fake

341
00:20:09,031 --> 00:20:12,270
news headline said they
believed it was true.

342
00:20:12,720 --> 00:20:16,230
A very large number of those said they
didn't know or they weren't sure, right.

343
00:20:16,231 --> 00:20:18,870
Often 50% of the public or more,

344
00:20:18,960 --> 00:20:22,230
which is troubling if you look at what
the content of these sorts of fake news

345
00:20:22,231 --> 00:20:25,140
headlines was.
For example,

346
00:20:25,260 --> 00:20:30,240
the pizzagate conspiracy
theory that Hillary Clinton
is involved in a pedophilia

347
00:20:30,241 --> 00:20:33,540
ring being run out of a pizza
parlor in Washington DC, right?

348
00:20:33,690 --> 00:20:37,740
So somewhere around, if you look at
the, the line there that, you know,

349
00:20:37,741 --> 00:20:42,270
40 to 50% of Americans are saying
either that's true or they're not sure,

350
00:20:42,900 --> 00:20:47,620
right? So we'd really like the answer
to be no, right. That it's false. Okay.

351
00:20:47,800 --> 00:20:51,520
Um, so these things can, again, even
for those people who don't believe them,

352
00:20:51,580 --> 00:20:56,140
they raise questions and
doubts, potentially there
may be worrisome, right? Um,

353
00:20:56,170 --> 00:20:58,300
people may be vulnerable to,
um,

354
00:20:58,570 --> 00:21:01,420
at least calling into question things
that we wouldn't like to be called into

355
00:21:01,421 --> 00:21:04,840
question because they are in fact falls.
So

356
00:21:06,340 --> 00:21:07,630
why did this happen?
Okay.

357
00:21:07,631 --> 00:21:11,080
So I want to just identify two factors
you might think about and it's obviously

358
00:21:11,081 --> 00:21:14,260
relevant to how people interact with
Google in its various forms, right?

359
00:21:14,500 --> 00:21:16,110
Why were vulnerable to this,
right?

360
00:21:16,120 --> 00:21:18,580
The first is the choice of
the information we consume,

361
00:21:18,581 --> 00:21:22,080
which is often called selective exposure.
That in some context,

362
00:21:22,081 --> 00:21:25,630
in some circumstances we pick and choose
the information we consume according to

363
00:21:25,631 --> 00:21:28,870
the extent to which is consistent
with our political preferences. Okay.

364
00:21:29,320 --> 00:21:32,860
That's going to vary depending on the
context you're in and the motivation you

365
00:21:32,861 --> 00:21:36,430
have to seek out that sort of information
as I'll show you in his Dj suggested.

366
00:21:36,640 --> 00:21:40,780
It's not universal,
but it's very much a threat,

367
00:21:40,990 --> 00:21:43,060
especially in the context,
uh,

368
00:21:43,210 --> 00:21:46,210
of something like a campaign
when people's political, uh,

369
00:21:46,630 --> 00:21:49,580
emotions are at a fever
pitch, right? That was as,

370
00:21:49,780 --> 00:21:52,060
as passionate as po about
politics as people get.

371
00:21:52,300 --> 00:21:55,330
And that was a time when people might be
especially interested in something that

372
00:21:55,331 --> 00:21:59,320
seemed to confirm how they felt about
a candidate or to disconfirm something

373
00:21:59,410 --> 00:22:00,250
they'd like to hear.

374
00:22:00,280 --> 00:22:04,300
Just the second problem is what's called
directionally motivated reasoning.

375
00:22:04,301 --> 00:22:08,080
So all of those factors that are
causing people to feel so strongly about

376
00:22:08,081 --> 00:22:11,110
politics and become so polarized me
that when we encounter that information,

377
00:22:11,320 --> 00:22:16,320
our directional preferences about what's
true often override our motivation to

378
00:22:17,051 --> 00:22:21,550
hold an accurate belief, right? The
side we'd like to be a correct, right?

379
00:22:21,551 --> 00:22:24,010
We often conclude they have the
stronger argument, they have,

380
00:22:24,011 --> 00:22:27,310
the more persuasive claim. Okay.
Even when that's not the case.

381
00:22:27,850 --> 00:22:32,440
So this raises a real challenge for fact
checkers or for information providers

382
00:22:32,441 --> 00:22:35,350
of any sort, including,
uh, everyone here. Right?

383
00:22:35,351 --> 00:22:38,350
So the first question you might think
about in terms of fact checking as a

384
00:22:38,351 --> 00:22:42,940
potential response to fake
news and misinformation more
generally is in terms of

385
00:22:42,941 --> 00:22:47,140
selective exposure, do people
read them? Right? And it's
not just a, it's, you know,

386
00:22:47,141 --> 00:22:49,210
so the fact checkers will tell you and I,
I should say,

387
00:22:49,211 --> 00:22:52,360
I know the fact checkers and they're
great and they're, they weren't very hard.

388
00:22:52,780 --> 00:22:53,440
Okay.

389
00:22:53,440 --> 00:22:56,950
And they do have very large audiences
now compared to where they were not very

390
00:22:56,951 --> 00:22:57,784
long ago.

391
00:22:58,120 --> 00:23:01,210
But what you might think about is are
the right people reading fact checks and

392
00:23:01,211 --> 00:23:03,820
are they reading fact checks
of the right claims? Right?

393
00:23:04,210 --> 00:23:08,710
It may be the case that the people reading
fact checks are the not necessarily

394
00:23:08,711 --> 00:23:12,000
the ones who are holding the given
misperception and question, right?

395
00:23:12,130 --> 00:23:14,140
And it might even be the case
that you read fact checks,

396
00:23:14,141 --> 00:23:17,110
but you read the fact check of the claim
by the other side that you don't want

397
00:23:17,111 --> 00:23:21,490
it to be true, right? So it's not
just, it's not good enough to say,

398
00:23:21,491 --> 00:23:23,530
well fact checks are out there
and some people were reading them.

399
00:23:23,531 --> 00:23:27,340
You really want to think about targeting
is the person who is mind you think

400
00:23:27,341 --> 00:23:30,610
could be changed by the fact check
being reached by that fact check. Okay.

401
00:23:31,690 --> 00:23:32,860
The problem with fact checking,

402
00:23:32,861 --> 00:23:35,740
and I'm sure those of you who've worked
with surfacing fact checks in Google

403
00:23:35,741 --> 00:23:38,950
have thought about this problem is that
everyone thinks everyone else needs them,

404
00:23:39,370 --> 00:23:42,900
right? We don't think we need them
ourselves. We think, well, you know, I, I,

405
00:23:42,910 --> 00:23:44,260
I'm well informed with those other people.

406
00:23:44,261 --> 00:23:46,310
They really should read the
fact checks more. Right?

407
00:23:46,490 --> 00:23:50,960
And that's been an obstacle to all sorts
of technological solutions to this,

408
00:23:50,961 --> 00:23:51,171
right?

409
00:23:51,171 --> 00:23:54,620
This is something where the demand among
the people we like to read the fact

410
00:23:54,621 --> 00:23:57,140
checks is an often as strong as we'd like.
Okay.

411
00:23:58,010 --> 00:24:00,620
And even if they do happen to read
the fact check or encounter it,

412
00:24:00,740 --> 00:24:02,930
will they find it persuasive?
That's the next question. Right?

413
00:24:02,931 --> 00:24:05,690
So even if we overcome
selective exposure and they, uh,

414
00:24:06,080 --> 00:24:09,830
choose to consume that piece of
information, is it persuasive to them?

415
00:24:09,831 --> 00:24:14,030
Do they evaluate that factcheck and
conclude, okay, this, this claim, uh,

416
00:24:14,090 --> 00:24:16,160
I thought was true, was
actually false. Right?

417
00:24:16,161 --> 00:24:18,680
And that's where we were directly
motivated reasoning would get in a way,

418
00:24:21,050 --> 00:24:24,080
when you think about fake
news in 2016 election, though,

419
00:24:24,200 --> 00:24:28,760
I want to encourage you to go beyond
the psychological factors we've been

420
00:24:28,761 --> 00:24:29,330
talking about.

421
00:24:29,330 --> 00:24:33,380
And think about the supply side is Dj
alluded to not just the demand side. Okay?

422
00:24:33,470 --> 00:24:35,810
Human psychology isn't going to change.
All right?

423
00:24:35,930 --> 00:24:39,500
Political scientists can tell you
polarization is not going away. Okay?

424
00:24:39,620 --> 00:24:41,540
So for both of those reasons,

425
00:24:41,780 --> 00:24:45,380
I'm trying to change the extent to which
people engage in selective exposure.

426
00:24:45,381 --> 00:24:47,930
Indirection of motivated reasoning
is going to be very difficult. Okay.

427
00:24:48,050 --> 00:24:51,140
There may be ways to mitigate it on the
margin and Dj gave you some suggestions

428
00:24:51,290 --> 00:24:55,190
of how we might do that, but we also have
to think about the supply side. Okay.

429
00:24:55,520 --> 00:24:58,190
There's a reason that people
enter this market, right?

430
00:24:58,191 --> 00:25:00,830
There were entrepreneurs who are
attracted to this market. You've all read,

431
00:25:00,870 --> 00:25:05,870
I assume of the mat about the Macedonian
teenagers who across the Ocean said I

432
00:25:06,201 --> 00:25:09,620
can buy a new guitar if I write fake
news stories for American audiences and

433
00:25:09,621 --> 00:25:12,810
sell and sell ads against them. Right. Um,

434
00:25:13,010 --> 00:25:16,610
so the financial incentives
played a really big role here.

435
00:25:16,611 --> 00:25:21,140
The partisanship that people had made
it financially attractive to create

436
00:25:21,141 --> 00:25:23,510
content that would attract audiences
you could sell ads against. Right?

437
00:25:23,511 --> 00:25:27,410
So the ad that works are playing
an important role here. Okay. Um,

438
00:25:28,490 --> 00:25:32,060
the second factor is really important is,
is, is distribution mechanisms. Right?

439
00:25:32,061 --> 00:25:34,100
And Facebook seems to have play as,

440
00:25:34,101 --> 00:25:36,020
I'll show you some more direct
evidence for this a moment,

441
00:25:36,021 --> 00:25:40,040
but Facebook in particular seems to have
played a key role in distributing fake

442
00:25:40,041 --> 00:25:40,910
news.
Okay.

443
00:25:41,210 --> 00:25:45,890
The reporting we have about the publishers
of fake news is that they would go

444
00:25:45,891 --> 00:25:50,720
see those articles into pro Trump Facebook
groups as a distribution mechanism

445
00:25:50,721 --> 00:25:53,300
and, and people in those groups
would then push them out.

446
00:25:53,510 --> 00:25:57,620
And that was part of how they would get
these stories to a wide distribution.

447
00:25:58,070 --> 00:26:02,390
Okay. So, uh, those
sorts of platforms are,

448
00:26:02,920 --> 00:26:06,760
you know, potentially enabling the spread
of bad information, not just good. Okay.

449
00:26:07,170 --> 00:26:08,990
Um,
and the final factor of course,

450
00:26:08,991 --> 00:26:12,620
is that negative partisanship is DJ
emphasize to you is so powerful now.

451
00:26:12,800 --> 00:26:14,810
It's not just polarization,
it's not just partisanship,

452
00:26:14,811 --> 00:26:17,810
it's how much people don't
like the other side. Right?

453
00:26:17,930 --> 00:26:21,650
And that's creating a demand
for information about why
the other side is bad or

454
00:26:21,651 --> 00:26:24,130
the other side is evil. Why
the other side? And that's,

455
00:26:24,180 --> 00:26:28,760
you should obviously that's terribly
destructive to the kinds of compromise and

456
00:26:28,761 --> 00:26:31,550
living together that we need
for a democracy to work. Okay.

457
00:26:31,551 --> 00:26:33,860
So this is a deep and fundamental problem,
but,

458
00:26:33,920 --> 00:26:36,830
and this is one expression of it that's
especially pernicious because now we're

459
00:26:36,831 --> 00:26:39,830
not only potentially intensifying
that negative partisanship,

460
00:26:40,160 --> 00:26:44,000
but we're misleading people
about the facts as well. Okay.

461
00:26:45,060 --> 00:26:46,160
But not, you know, I,

462
00:26:46,161 --> 00:26:49,650
I suggested you earlier and I want to
give you a little bit more of a sense of

463
00:26:49,651 --> 00:26:52,610
this, that not everyone is engaging
in this all the time, right?

464
00:26:52,690 --> 00:26:53,431
You're probably saying,
well,

465
00:26:53,431 --> 00:26:56,760
I don't go seek out fake news that
conforms to my political viewpoints at all

466
00:26:56,761 --> 00:26:59,160
times. And that's probably true. Okay.

467
00:26:59,220 --> 00:27:02,730
So this is data from my
coauthor Andy guests at Nyu.

468
00:27:03,000 --> 00:27:07,050
I'm looking at a near representative
sample of Americans who've provided an

469
00:27:07,051 --> 00:27:11,970
anonymous who provided consent for, uh,
sharing their online web traffic data.

470
00:27:12,240 --> 00:27:14,220
Okay?
And there's two plots here.

471
00:27:14,221 --> 00:27:16,230
I'm going to talk you through
the differences between them.

472
00:27:16,410 --> 00:27:21,000
What he's doing is he's looking at the
estimated slant of the media outlets they

473
00:27:21,001 --> 00:27:22,960
visit in terms of websites.
Okay.

474
00:27:23,100 --> 00:27:26,610
And the slant estimate comes from the
average ideology of the people who share

475
00:27:26,611 --> 00:27:29,250
these sites on Facebook.
Okay? So it's, uh,

476
00:27:29,280 --> 00:27:32,010
it's from an article by some
Facebook data scientists.

477
00:27:32,190 --> 00:27:35,970
So what you can see in the left panel
was that the average person's average

478
00:27:35,971 --> 00:27:39,420
media outlet is quite centers
regardless of party. Okay?

479
00:27:39,480 --> 00:27:44,480
So the average person is not seeking
out a steady diet of skewed Pars and

480
00:27:44,971 --> 00:27:47,280
information. Most people don't
care about politics that much,

481
00:27:47,281 --> 00:27:51,800
especially outside of a campaign
context. This is 2015 data. Okay Boys,

482
00:27:51,820 --> 00:27:55,080
you can see in the right panels that the
distribution at the audience level in

483
00:27:55,081 --> 00:27:57,960
terms of where the overall set
of traffic is going by party,

484
00:27:57,961 --> 00:28:00,240
it looks very different, right?
That's what you're seeing.

485
00:28:00,241 --> 00:28:03,600
That big partisan polarization between,
uh,

486
00:28:03,630 --> 00:28:05,910
the kinds of sites Democrats
go to in the left, in blue,

487
00:28:05,911 --> 00:28:08,550
in the kind of sites it
Republicans go to on the right,

488
00:28:08,640 --> 00:28:13,050
and that there's a smaller set of folks
on both sides of the aisle who are,

489
00:28:13,380 --> 00:28:18,380
who are consuming large amounts of
information disproportionately from

490
00:28:18,810 --> 00:28:20,310
likeminded media outlets.

491
00:28:20,480 --> 00:28:21,313
Okay.

492
00:28:24,480 --> 00:28:27,250
Uh, oh, I'm sure. Yes. From
left to right, daily coast,

493
00:28:27,520 --> 00:28:30,190
the Huffington Post and
the New York Times, CNN,

494
00:28:30,191 --> 00:28:33,820
just to the left of the red spike.
Msn is under the middle.

495
00:28:33,880 --> 00:28:35,940
There's still a remarkable number
of people who go to the, the,

496
00:28:35,941 --> 00:28:37,990
the main portals like MSN and Yahoo.

497
00:28:38,710 --> 00:28:40,840
And then on the right
Fox News and Breitbart.

498
00:28:43,510 --> 00:28:48,250
So, um, I mentioned this to you
earlier about the role of Facebook.

499
00:28:48,251 --> 00:28:51,250
Here's some partial evidence
that's consistent with that.

500
00:28:51,490 --> 00:28:53,920
I might or might not have more information
about this that I could talk to you

501
00:28:53,921 --> 00:28:58,900
in a non record about an a
non-recorded format. Um, the, what,

502
00:28:58,930 --> 00:29:03,280
uh, these economists Alcott against
cofound is that fake news sites were

503
00:29:03,580 --> 00:29:08,380
differentially likely to have
been reached via social media.

504
00:29:08,470 --> 00:29:11,680
Okay. So you can see in the
bottom, in the bottom bar here,

505
00:29:11,681 --> 00:29:15,700
more than 40% of their visits originated
in social media compared to only about

506
00:29:15,701 --> 00:29:18,880
10% for top news sites.
Okay.

507
00:29:18,940 --> 00:29:22,240
So it seems to have been a really
important traffic driver. Um,

508
00:29:22,300 --> 00:29:26,620
search a smaller traffic
driver relative to a top news,

509
00:29:26,621 --> 00:29:29,770
but a nontrivial one,
right? About 22% there.

510
00:29:32,330 --> 00:29:37,230
So in terms of fact checking and its
role in countering fake news, um,

511
00:29:37,970 --> 00:29:40,490
one of the problems was that
no one really expected this.

512
00:29:40,880 --> 00:29:44,320
And so the fact checkers
weren't, uh, on top of it as a,

513
00:29:44,321 --> 00:29:46,240
as a target for fact checking,

514
00:29:46,241 --> 00:29:49,010
and they typically have
focused on elite statements,

515
00:29:49,050 --> 00:29:52,390
statements made by politicians,
party members, right?

516
00:29:52,391 --> 00:29:54,490
The president members of Congress,
people like that.

517
00:29:54,850 --> 00:29:59,410
And the only exception,
you know for the most part was Snopes,

518
00:29:59,411 --> 00:30:00,970
which focuses more on rumors and hoaxes,

519
00:30:00,971 --> 00:30:05,290
which fake news seem to fall more
squarely within. But even still,

520
00:30:05,291 --> 00:30:09,430
I would say that, you know, everyone
was caught off guard by this. Okay.

521
00:30:09,550 --> 00:30:12,880
So now there's going to be more
fact checking and fake news.

522
00:30:12,910 --> 00:30:15,640
I think it's fair to say, and the
question is how effective will it be?

523
00:30:15,641 --> 00:30:19,510
And I just want to raise some questions
that you should think about as you think

524
00:30:19,511 --> 00:30:23,250
about how effective these
kinds of approaches might be.
Okay. Um, the first one is,

525
00:30:23,251 --> 00:30:24,250
I mentioned you earlier,

526
00:30:24,370 --> 00:30:28,430
is that the people who read fake news
may or may not actually see a fact check

527
00:30:28,480 --> 00:30:29,230
of it,
right?

528
00:30:29,230 --> 00:30:33,000
It's entirely possible to have a world
where those are two disjoint sets, okay.

529
00:30:33,100 --> 00:30:35,400
Or ones with very little overlap.
Okay.

530
00:30:35,710 --> 00:30:39,790
And so we should worry a lot about the
extent of which the fact checking of fake

531
00:30:39,791 --> 00:30:43,510
news that's being done right now is
reaching the people we'd like it to reach.

532
00:30:45,460 --> 00:30:49,060
The second point is I would just encourage
you to be very specific about what

533
00:30:49,061 --> 00:30:50,530
you mean by the effects of fake news.

534
00:30:50,531 --> 00:30:54,220
This is something that people talk
about very loosely and I think, uh,

535
00:30:55,210 --> 00:30:59,890
it's often fake news is being used as
it's kind of catch all term for all sorts

536
00:30:59,891 --> 00:31:02,860
of misinformation and people are making
sweeping claims about its effects,

537
00:31:03,040 --> 00:31:06,370
but I don't think are well supported by
the facts. So in the case of fake news,

538
00:31:06,371 --> 00:31:09,310
you'll hear people say fake news
affected the outcome of the election.

539
00:31:09,340 --> 00:31:13,540
I don't think there's any evidence for
that in terms of changing who won. Okay.

540
00:31:13,810 --> 00:31:15,160
Um, it's, it's,

541
00:31:15,270 --> 00:31:17,920
it's very difficult to show that you'd
have to make very strong assumptions that

542
00:31:17,921 --> 00:31:20,650
Alcott and gins, cow
article I mentioned, uh,

543
00:31:20,860 --> 00:31:23,920
walks you through the kinds of assumptions
you'd have to make to get to an

544
00:31:23,921 --> 00:31:28,510
outcome like that. Similarly,
fact checking itself may
not change that many votes.

545
00:31:28,630 --> 00:31:30,970
Right? Even if you've read a
fact, I think of your own life.

546
00:31:30,971 --> 00:31:33,220
If you read a fact check that
change your mind about something,

547
00:31:33,460 --> 00:31:35,590
did it change how you were going to
vote on which party you preferred?

548
00:31:35,591 --> 00:31:36,760
Probably not.
Right?

549
00:31:36,761 --> 00:31:40,480
So we have to be realistic about what
fact checking can and can't accomplish.

550
00:31:40,690 --> 00:31:41,710
Right?
Um,

551
00:31:41,920 --> 00:31:46,150
and then finally I just encourage you
to think about misinformation in general

552
00:31:46,151 --> 00:31:49,480
and not focus on fake news to the
exclusion of this larger problem,

553
00:31:49,481 --> 00:31:53,580
which is a much more systematic one.
Then these particular, you know,

554
00:31:53,590 --> 00:31:58,180
sites that have popped up that are wholly
fake, non journalistic, right? The, the,

555
00:31:58,190 --> 00:32:02,380
the Macedonian teenager version
of fake news. All right.

556
00:32:02,381 --> 00:32:04,870
So I want to conclude with a few,

557
00:32:04,871 --> 00:32:08,590
with a brief discussion to get us moving
towards the Qa and your thoughts about

558
00:32:08,591 --> 00:32:10,240
what to do about this,
uh,

559
00:32:10,241 --> 00:32:14,230
in terms of the policy responses that are
being pursued out there and what might

560
00:32:14,231 --> 00:32:17,770
be effective. Okay. So the first
thing is to think about ad networks.

561
00:32:17,800 --> 00:32:21,430
No one has any constitutional right to
be part of an ad network, right? In fact,

562
00:32:21,700 --> 00:32:22,421
as a company,

563
00:32:22,421 --> 00:32:26,950
you should wish to protect your brand
from any association with these terrible

564
00:32:26,951 --> 00:32:28,450
misleading sites.
Okay.

565
00:32:28,510 --> 00:32:32,350
So it seems as though ad networks are
starting to crack down and who has,

566
00:32:32,430 --> 00:32:35,800
who is allowed to be a member of them
and they're starting to even be white

567
00:32:35,801 --> 00:32:39,490
listing policies put into place, right?
So then rather than stripping out, uh,

568
00:32:39,740 --> 00:32:44,600
the worst sites, they're selectively
adding the best. Okay. Um,

569
00:32:44,990 --> 00:32:47,960
so I think that's a promising approach
that's also going to be driven by the

570
00:32:47,961 --> 00:32:51,230
advertisers who are increasingly worried
about what their ad is showing that

571
00:32:51,231 --> 00:32:52,910
what content or ad is showing up against.

572
00:32:52,911 --> 00:32:56,150
Obviously Youtube is dealing with
this on a massive scale right now.

573
00:32:57,500 --> 00:33:00,710
Second, thinking about the
algorithm, I know this,

574
00:33:01,100 --> 00:33:03,440
there's probably like 10 to search
people here who don't want to hear my

575
00:33:03,441 --> 00:33:04,790
thoughts on the algorithm,

576
00:33:04,791 --> 00:33:08,000
but let me just say there was an
announcement made that fake news was being

577
00:33:08,001 --> 00:33:10,610
downward in the algorithm and I
think that's appropriate given

578
00:33:12,170 --> 00:33:15,080
the concerns about the quality of
this kind of information. Right?

579
00:33:15,081 --> 00:33:18,620
You were not serving your users well.
If you are surfacing that information up,

580
00:33:19,070 --> 00:33:23,090
uh, it is not high quality.
Right. And then finally,

581
00:33:23,120 --> 00:33:26,630
the most developed response we've seen
has been the fact checking partnership

582
00:33:26,631 --> 00:33:27,860
with the,
uh,

583
00:33:27,890 --> 00:33:32,890
the fact checkers partnership with
Facebook where they're going to identify

584
00:33:33,590 --> 00:33:38,420
stories that have been rated as
false by established fact checkers.

585
00:33:38,510 --> 00:33:41,030
They're going to give you a nudge
first one. You see it. And again,

586
00:33:41,031 --> 00:33:43,070
if you'd like to share it
saying this, just, you know,

587
00:33:43,071 --> 00:33:46,790
this is disputed by this fact
checker in that fact checker. Okay.

588
00:33:46,910 --> 00:33:51,090
And that's a more deliberate kind of
intervention to that provides, uh,

589
00:33:51,230 --> 00:33:52,940
information to people,
but in it,

590
00:33:53,000 --> 00:33:55,970
I think an appropriately hands off way
that keeps Facebook out of the business

591
00:33:55,971 --> 00:33:58,070
of trying to adjudicate
what's true and false. Okay.

592
00:33:58,080 --> 00:34:02,780
Leaves that to the journalist where
that's their core competency. Okay. Um, so

593
00:34:04,520 --> 00:34:08,810
one last point is just simply to say
this is an evolving problem. Okay.

594
00:34:08,840 --> 00:34:11,540
So if you follow the political
debate since the election,

595
00:34:11,630 --> 00:34:14,270
the kinds of misinformation you see
out there, it looks quite different.

596
00:34:14,570 --> 00:34:17,570
The form it's taking, it's
is different. Okay. Um,

597
00:34:17,630 --> 00:34:22,250
there are all these conspiracy
minded folks on Twitter. Uh,

598
00:34:22,550 --> 00:34:26,710
writing and circulating fairly crazy
things I won't get into too much of,

599
00:34:26,720 --> 00:34:28,430
but let me just say that I'm a,

600
00:34:28,460 --> 00:34:31,670
there isn't just an article about
a Harvard University professor,

601
00:34:31,700 --> 00:34:36,170
very famous law professor as well as
the senator who are citing the most, uh,

602
00:34:36,320 --> 00:34:36,830
you know,

603
00:34:36,830 --> 00:34:41,830
non-credible conspiracy sites on the
left to make sweeping unsupported claims

604
00:34:42,021 --> 00:34:43,280
about Trump and Russia.

605
00:34:43,280 --> 00:34:48,280
So this misinformation problem is evolving
and changing and you should be aware

606
00:34:50,271 --> 00:34:54,980
of how rapidly it evolves and avoid the
mistake of fighting the last war. Okay.

607
00:34:55,040 --> 00:34:56,490
So with that note,
we,

608
00:34:56,590 --> 00:35:00,620
we would love your questions and I just
want to say please don't give up. Right.

609
00:35:00,680 --> 00:35:03,500
I remember my, you know, we may not
be able to put out the dumpster fire,

610
00:35:03,501 --> 00:35:06,110
but we can at least like
bring it down a little. Okay.

611
00:35:06,111 --> 00:35:10,130
And it's important not to give into this
temptation to say we're in a post truth

612
00:35:10,131 --> 00:35:12,560
and post facts society and there's
nothing we can do about it.

613
00:35:12,680 --> 00:35:13,940
There is something we can do about it.

614
00:35:14,060 --> 00:35:18,440
We should defend these norms and you
at the platforms Google like Google and

615
00:35:18,441 --> 00:35:21,660
Facebook can in some ways do
more than almost anybody. So, um,

616
00:35:21,710 --> 00:35:23,480
I think we'd love your
questions and thanks very much.

617
00:35:31,870 --> 00:35:34,150
Hi. So, uh, I came to this talk, uh,

618
00:35:34,190 --> 00:35:37,350
part of the reason I came was because
that was a little bit worried about my own

619
00:35:37,440 --> 00:35:42,300
personal bubble that I might be in and
how would I know about this? And uh,

620
00:35:42,301 --> 00:35:46,230
I'm a liberal and I noticed that most
of the fake news stories that you gave

621
00:35:46,231 --> 00:35:48,240
were aim towards Republicans.

622
00:35:48,510 --> 00:35:52,530
So I wanted to know is it the case that
most fake news stories were aimed at

623
00:35:52,531 --> 00:35:56,700
Republicans or can you give some examples
of fake news stories that were aimed

624
00:35:56,701 --> 00:35:59,670
at Democrats during the election?

625
00:35:59,730 --> 00:36:01,950
The fake news was
overwhelmingly pro Trump.

626
00:36:02,160 --> 00:36:06,180
There were only a handful of pro
Clinton fake news stories that took off.

627
00:36:06,330 --> 00:36:09,960
Some people tried it, it wasn't
as successful in the marketplace.

628
00:36:10,050 --> 00:36:12,480
So at least in that particular
format it seemed to not work.

629
00:36:12,481 --> 00:36:13,890
But as I'm suggesting to you,

630
00:36:13,891 --> 00:36:17,790
some of these dubious conspiracy sites
have gotten more traction among Democrats

631
00:36:17,791 --> 00:36:21,600
in the time since the election when
Trump is in office and creates,

632
00:36:21,660 --> 00:36:25,230
he's the kind of scary
opponent who motivates belief
and misinformation the way

633
00:36:25,231 --> 00:36:26,060
Clinton was drinking

634
00:36:26,060 --> 00:36:28,890
campaign. Yeah, I would just,

635
00:36:29,290 --> 00:36:32,240
I would totally concur and I would just
say that it's important to keep in mind

636
00:36:33,400 --> 00:36:38,400
that it's important to keep in mind that
misinformation exist within this larger

637
00:36:38,661 --> 00:36:42,770
political context and the nature
of party competition and who's in,

638
00:36:42,771 --> 00:36:44,840
who's in the White House and
who's in control of Congress.

639
00:36:44,841 --> 00:36:47,420
These are all things that shift over
time and are going to continue to shift

640
00:36:47,421 --> 00:36:51,500
over time. Right? So if we think about
the demand for misinformation about just,

641
00:36:51,620 --> 00:36:54,200
you know, the president for example,
this is the most salient, right?

642
00:36:54,201 --> 00:36:57,220
Political figure in the United
States. Of course, you know, the, the,

643
00:36:57,530 --> 00:36:59,330
the partisan coalitions who are dead,

644
00:36:59,360 --> 00:37:02,160
who are demanding that information is
going to shift over time and you're going

645
00:37:02,161 --> 00:37:03,780
to see the flow and the supply of,

646
00:37:03,781 --> 00:37:07,250
of fake news and misinformation
more generally be responsive to,

647
00:37:07,640 --> 00:37:10,730
to sort of who to to the shift in
conditions that I talked about.

648
00:37:11,960 --> 00:37:12,793
I'm going to hand the mic,

649
00:37:13,070 --> 00:37:16,720
John, who's picking up some
questions from the dory. Sure. Um,

650
00:37:16,721 --> 00:37:18,990
so this is from the London office.
Um,

651
00:37:19,120 --> 00:37:23,620
do you have anything particular to say
about fake science or is it all the same

652
00:37:23,680 --> 00:37:27,310
no matter what the subject? So I've done
research on vaccine misinformation and,

653
00:37:27,311 --> 00:37:29,800
and that's certainly, you know,
Dj mentioned climate change.

654
00:37:29,950 --> 00:37:32,740
So this is certainly something
that's not specific to politics.

655
00:37:32,920 --> 00:37:34,810
It extends the health and science as well.

656
00:37:34,910 --> 00:37:38,500
I in vaccines is a good example where
it's not an ideological or partisan issue

657
00:37:38,501 --> 00:37:39,011
as such.

658
00:37:39,011 --> 00:37:42,850
And misinformation can still be quite
widespread and people can still engage in

659
00:37:42,851 --> 00:37:45,880
selective exposure and are actually
motivated reasoning too. Um,

660
00:37:46,090 --> 00:37:46,810
so I think there's,

661
00:37:46,810 --> 00:37:49,750
there's reason to question in all of
these domains where people have strong

662
00:37:49,751 --> 00:37:54,250
views and about an issue that seems to
implicate some aspect of their identity.

663
00:37:54,450 --> 00:37:55,360
Uh,
there's,

664
00:37:55,600 --> 00:38:00,600
there's a real potential
to have counterproductive
effects or for people to seek

665
00:38:00,941 --> 00:38:03,160
out information that seems to
confirm their views. All right.

666
00:38:03,161 --> 00:38:06,550
So just to give you an
example, um, there is, uh,

667
00:38:06,670 --> 00:38:08,780
a measles outbreak going
on right now in Minnesota,

668
00:38:08,781 --> 00:38:10,990
in the Somali American
community predominantly.

669
00:38:11,290 --> 00:38:14,860
And the story that's been told by our
journalists there is that there was some

670
00:38:14,861 --> 00:38:18,340
children had autism and the parents
didn't know why that wasn't a concept that

671
00:38:18,341 --> 00:38:22,660
we're familiar with and people started
to search and they found their way to

672
00:38:22,661 --> 00:38:26,720
some of the really terrible information
out there and have since been tar.

673
00:38:26,770 --> 00:38:28,600
And as they became more
interested in that,

674
00:38:28,601 --> 00:38:32,530
some folks invited the anti-vaccine
activists who present as misleading and

675
00:38:32,531 --> 00:38:33,780
false science.
Um,

676
00:38:33,781 --> 00:38:36,970
and they've cultivated that to the point
that their vaccination rates from very

677
00:38:36,971 --> 00:38:38,000
low,
um,

678
00:38:38,050 --> 00:38:42,700
and made them vulnerable to
what is now an ongoing outbreak.

679
00:38:43,120 --> 00:38:46,600
And measles is a very dangerous disease.
Explosive. We contagious, right?

680
00:38:46,601 --> 00:38:50,800
So this is a good, it's a good reminder
that misinformation doesn't have to be,

681
00:38:51,070 --> 00:38:54,730
um, partisan and it can have very
immediate and dangerous consequence.

682
00:38:55,780 --> 00:38:59,710
I have a question about, uh, the sort
of the psychology side of the, uh,

683
00:39:00,230 --> 00:39:01,840
the recipients of the fake news.

684
00:39:02,560 --> 00:39:06,910
I know people who love
to be no at all and so,

685
00:39:07,060 --> 00:39:11,890
and they will happily fill in the gaps
in their knowledge as they're explaining

686
00:39:11,891 --> 00:39:13,300
something.
Uh,

687
00:39:13,570 --> 00:39:17,800
I wonder if any investigations be done
about whether they are particularly

688
00:39:17,801 --> 00:39:21,970
vulnerable to, to the
misinformation or, you know,

689
00:39:22,420 --> 00:39:26,560
because they are,
it was like once they've accepted it did,

690
00:39:26,561 --> 00:39:30,140
they're going to tout it
enthusiastically. I actually, I'm,

691
00:39:30,190 --> 00:39:33,220
I'm inferring that they will do that.
I suppose that you should leave it to you.

692
00:39:33,920 --> 00:39:37,100
Um, so the, the closest sort of
individual level factor that,

693
00:39:37,190 --> 00:39:40,060
that I can think about there
that's been studied as, um, there,

694
00:39:40,160 --> 00:39:44,030
there are psychological studies on the
need for need for closure or the need for

695
00:39:44,031 --> 00:39:47,560
certainty. And there's, there's a lot
of variation across people in, in,

696
00:39:47,660 --> 00:39:48,830
in their score on that,
right?

697
00:39:48,831 --> 00:39:51,410
Like some people to your point
need to need to know things.

698
00:39:51,411 --> 00:39:56,180
And need to fill in the blanks. Now to the
extent that those, um, those people are,

699
00:39:56,181 --> 00:39:59,370
are highly partisan, highly
political people. Um,

700
00:39:59,420 --> 00:40:02,210
directionally motivated reasoning, right?
Would suggest that they're going to put,

701
00:40:02,390 --> 00:40:05,960
they're gonna apply their abilities
to forming partisan or ideologically

702
00:40:05,961 --> 00:40:10,610
consistent views. And I think that's that
I could definitely see that happening.

703
00:40:10,611 --> 00:40:14,630
I'm not aware of any studies on sort
of individual the effect of individual

704
00:40:14,631 --> 00:40:17,180
level differences like that. Um, on,

705
00:40:17,240 --> 00:40:19,610
on the propensity to have
misperceptions or something.

706
00:40:19,820 --> 00:40:21,650
I don't know if you would add yeah,

707
00:40:21,750 --> 00:40:25,410
no, that's it. That sounds right. And
I would just say that, um, people,

708
00:40:25,440 --> 00:40:27,870
you know what you were saying about
people kind of filling in the gaps, right?

709
00:40:27,871 --> 00:40:31,020
Another thing you might think about here
is conspiracy theories often have that

710
00:40:31,021 --> 00:40:33,630
function and kind of what you can think
of as a kind of subset of misinformation

711
00:40:33,631 --> 00:40:35,940
or misperceptions. Sorry,
my microphone is off.

712
00:40:36,870 --> 00:40:38,360
You can think of conspiracy theories,

713
00:40:38,361 --> 00:40:40,830
the kind of subset of misinformation or
misperceptions where people who want to

714
00:40:41,070 --> 00:40:44,190
understand what's going on or think they
understand what's going on right now

715
00:40:44,400 --> 00:40:48,630
are filling in gaps. Okay.
So we don't know why, uh,

716
00:40:48,660 --> 00:40:52,020
we don't really understand
why autism happens, right?

717
00:40:52,050 --> 00:40:54,980
The vaccine story provides
a simple explanation, right?

718
00:40:55,500 --> 00:40:59,160
There are chaotic and
conflicting information about
what happened in nine 11.

719
00:40:59,161 --> 00:41:02,970
The inside job miss seems to explain what
happened and you go through the list.

720
00:41:03,210 --> 00:41:03,991
These misperceptions,

721
00:41:03,991 --> 00:41:08,991
we'll often have that flavor of
seeming to provide a compelling simple

722
00:41:09,421 --> 00:41:10,680
explanation for what's going on.

723
00:41:10,681 --> 00:41:13,230
And some people I think do
intuitively gravitate to those.

724
00:41:13,410 --> 00:41:16,170
There's definitely evidence that people
who are predisposed to see the world in

725
00:41:16,171 --> 00:41:19,440
that way will disproportionately
believing conspiracy theories.

726
00:41:20,050 --> 00:41:22,420
Here's another question.
This one's from mountain view.

727
00:41:22,870 --> 00:41:26,620
There been some discussion that the
issue of misinformation is exacerbated by

728
00:41:26,621 --> 00:41:30,640
readers not knowing how to critically
be news and or navigate the overwhelming

729
00:41:30,641 --> 00:41:33,980
nature of the current high choice
media environment. How might

730
00:41:33,980 --> 00:41:37,410
this concern and be relevant to
Google? Okay, that's a good point.

731
00:41:39,470 --> 00:41:41,890
Let me say a couple of things. There's
been a lot of interest in steel action in,

732
00:41:41,891 --> 00:41:46,310
in news and media literacy and I think
educators are thinking about this as an

733
00:41:46,311 --> 00:41:47,540
issue.
Um,

734
00:41:47,630 --> 00:41:51,740
so one of the things Google might be
able to do is support news and media

735
00:41:51,741 --> 00:41:55,320
literacy as a component
of civics education, um,

736
00:41:55,580 --> 00:41:58,070
as something that librarians could teach.
Um,

737
00:41:58,100 --> 00:42:01,670
there's lots of ways in which we might
help equip people to become better

738
00:42:01,671 --> 00:42:04,330
citizens. Right. The
educational approach though is,

739
00:42:04,331 --> 00:42:06,410
is pretty limited in the sense that,
you know,

740
00:42:06,411 --> 00:42:08,690
you're only getting those
younger folks as they,

741
00:42:08,691 --> 00:42:12,770
as they come through the educational
system year by year. Right?

742
00:42:12,771 --> 00:42:16,340
But we have a very large set of adults
who are not in the educational system

743
00:42:16,341 --> 00:42:20,720
anymore and we might think about how we
could reach them more effectively. Um,

744
00:42:20,990 --> 00:42:25,040
I think this is an area where libraries
are probably the most effective vehicle.

745
00:42:25,041 --> 00:42:28,790
We have all these institutions full
of books and people and Librarians are

746
00:42:28,791 --> 00:42:32,720
trying to repurpose themselves as
helping people navigate the world of

747
00:42:32,721 --> 00:42:36,410
information. Right? They don't, we don't
need them to help us find books anymore,

748
00:42:36,740 --> 00:42:39,990
but they want to be information navigator.
So I think there,

749
00:42:40,070 --> 00:42:44,570
there might be opportunities for things
like partnerships. Um, at the same time,

750
00:42:44,571 --> 00:42:47,270
I think we have to think about how to
make it easier for people in the first

751
00:42:47,271 --> 00:42:51,620
place. Right. It, I worry
there's too much. Uh, you know,

752
00:42:51,621 --> 00:42:53,660
and I don't mean to attribute this
to your question, but in general,

753
00:42:53,661 --> 00:42:54,494
out in the world,

754
00:42:54,890 --> 00:42:57,530
there's too much kind of lecturing
people for not being sophisticated.

755
00:42:57,860 --> 00:43:02,690
We need to help them make
better decisions, right? In
a responsible, ethical way.

756
00:43:02,920 --> 00:43:04,610
Well, we don't tell them
we know the right answer.

757
00:43:04,640 --> 00:43:08,420
We're going to in a condescending way,
give it to them, but that we create,

758
00:43:08,650 --> 00:43:08,900
you know,

759
00:43:08,900 --> 00:43:13,340
a kind of information architecture that
helps people make better decisions and

760
00:43:13,341 --> 00:43:14,810
get better information.
Yeah.

761
00:43:15,090 --> 00:43:15,891
And this is where the,

762
00:43:15,891 --> 00:43:20,400
the adjustments to search algorithms and
different tools that you all can offer

763
00:43:20,401 --> 00:43:22,590
are extremely helpful because right.

764
00:43:22,710 --> 00:43:26,010
We do have this proliferation
of news sources, many of many,

765
00:43:26,011 --> 00:43:30,690
most of which are high quality and
are credible. Um, and so, you know,

766
00:43:30,691 --> 00:43:34,290
to the extent that we're highlighting
those and we're training people over time

767
00:43:34,291 --> 00:43:34,531
as,

768
00:43:34,531 --> 00:43:39,270
as they become more and more use to
searching and as they increasingly rely on

769
00:43:39,271 --> 00:43:41,790
Google, right. For everything. In
their lives, the more, the more,

770
00:43:41,791 --> 00:43:45,270
the better they can get a distinguishing
between high and low quality.

771
00:43:45,500 --> 00:43:47,760
Um, it's going to be really
important. Had a quick question

772
00:43:47,900 --> 00:43:52,460
about, uh, the, uh, polarization data
that you showed at the beginning.

773
00:43:52,560 --> 00:43:56,450
Especially, you know, that climate
change chart is very striking. Uh,

774
00:43:56,780 --> 00:44:00,170
and it's easy to read that as the same
cohort if people changing their minds

775
00:44:00,171 --> 00:44:01,004
over time.

776
00:44:01,250 --> 00:44:05,990
But the set of adults voting
Republicans is not the same.

777
00:44:05,991 --> 00:44:07,160
In 2014,

778
00:44:07,340 --> 00:44:11,270
I myself shifted from one of those lines
to another one and [inaudible] taking

779
00:44:11,271 --> 00:44:14,690
my climate change beliefs
with me partially because
I felt more comfortable on

780
00:44:14,691 --> 00:44:17,900
the other line with my current
set of climate change beliefs. Uh,

781
00:44:17,901 --> 00:44:21,860
and so do you have a feeling for how
much of this is people changing minds and

782
00:44:21,861 --> 00:44:25,310
how much of this is mines
shifting their identification?

783
00:44:26,610 --> 00:44:30,600
Uh, that's a great question. I
mean, so, so given the polarization,

784
00:44:30,630 --> 00:44:33,680
given polarization of politics today,

785
00:44:33,770 --> 00:44:38,290
you see for most people write
a strengthening of, of party
attachment and, and, um,

786
00:44:38,670 --> 00:44:42,390
uh, people are less likely to, to switch
sides and this, and this is interesting.

787
00:44:42,420 --> 00:44:44,840
Um, yeah, I mean, so the,
the question whenever it,

788
00:44:44,841 --> 00:44:47,640
whenever you see partisan changes
like that, like the ones I showed,

789
00:44:47,641 --> 00:44:51,000
it always comes down to this question of
is it persuasion or replacement? Right?

790
00:44:51,001 --> 00:44:55,200
Are People, people forming new
coalitions? Um, in the case,

791
00:44:55,201 --> 00:44:59,070
in the case of climate change mean climate
change has just become so politicized

792
00:44:59,250 --> 00:45:01,830
and how it's how it's communicated in,
in media that it,

793
00:45:01,831 --> 00:45:03,930
it sort of is a political issue now.

794
00:45:03,931 --> 00:45:07,500
It's almost covered as a political
issue in a sort of left versus right

795
00:45:07,501 --> 00:45:11,130
framework as opposed to a scientific
framework that other issues are,

796
00:45:11,131 --> 00:45:15,010
are disgusting. Um, so yeah, I mean, I,

797
00:45:16,020 --> 00:45:18,930
it's hard for me to imagine a lot of,
um,

798
00:45:19,290 --> 00:45:21,850
within party persuasion now on that issue.
Right?

799
00:45:21,960 --> 00:45:24,450
My sense would be that it's mostly,
it's mostly people,

800
00:45:24,570 --> 00:45:27,810
people moving to a Gi tend to
adjust their views with their party

801
00:45:28,630 --> 00:45:29,770
in general.
In general,

802
00:45:29,771 --> 00:45:33,460
the evidence suggests that people do go
along with their party on these kinds of

803
00:45:33,461 --> 00:45:35,890
cues. Right? I mean, if you
think about it, right? You know,

804
00:45:35,940 --> 00:45:37,360
in the seventies and eighties,

805
00:45:37,510 --> 00:45:41,380
the idea that it was somehow part isn't
what you thought about the climate with

806
00:45:41,381 --> 00:45:45,310
preposterous that wasn't on anyone's
agenda whatsoever, right? And,

807
00:45:45,490 --> 00:45:48,880
but the set of elite cues that people
were getting started to diverge,

808
00:45:48,881 --> 00:45:53,881
and then we saw this kind of tribalistic
movement along those lines and people

809
00:45:54,191 --> 00:45:58,000
falling, right? There are those folks
who might switch lines as you suggest,

810
00:45:58,130 --> 00:45:59,680
but I'd say far more just,
you know,

811
00:46:00,130 --> 00:46:02,260
they just to extend that they've
thought about climate change,

812
00:46:02,261 --> 00:46:05,020
they're taking cues from people they
trust as to what they should believe.

813
00:46:05,500 --> 00:46:09,670
If you think about how most people write
form opinions on most issues, right?

814
00:46:09,720 --> 00:46:13,210
Most, most people on most issues are not
very informed enough, very interested.

815
00:46:13,600 --> 00:46:18,440
And so it's a really easy heuristic
to use if people, right. If,

816
00:46:18,450 --> 00:46:22,000
if, if the media just presents an
issue as, as a left versus right issue,

817
00:46:22,001 --> 00:46:25,930
it's a very easy cue for people to take
as opposed to engaging in war effort,

818
00:46:25,931 --> 00:46:28,300
fall sort of info search
about global warming. Right?

819
00:46:28,301 --> 00:46:29,770
And you just go to your team.
Yeah.

820
00:46:30,680 --> 00:46:34,490
Hi. Um, my question's around
you, you presented kind of some,

821
00:46:34,520 --> 00:46:38,900
some findings of how people
are discovering, uh, fake news,

822
00:46:38,901 --> 00:46:41,060
whether it's from search or social media,

823
00:46:41,090 --> 00:46:46,090
but has there been any work that kind of
understands how they make it into those

824
00:46:47,181 --> 00:46:49,460
environments ranging because I mean,

825
00:46:49,461 --> 00:46:52,780
things get created and there
must be some strategies or,

826
00:46:53,260 --> 00:46:58,070
or maybe common attributes that help
these things get very wide distribution

827
00:46:58,071 --> 00:47:01,460
because these things don't get widely
distributed if just if they were just

828
00:47:01,461 --> 00:47:05,150
graded. There must be some
strategies that, that, that might,

829
00:47:05,210 --> 00:47:06,980
some patterns that are emerging.

830
00:47:06,981 --> 00:47:11,810
And I was wondering if you had thoughts
or there's some research that's on this

831
00:47:11,811 --> 00:47:12,644
particular topic.

832
00:47:13,590 --> 00:47:15,420
The mechanism we know best is,

833
00:47:15,421 --> 00:47:20,421
is the seeding of these fake news
articles into Facebook groups.

834
00:47:21,150 --> 00:47:22,440
Um,
you know,

835
00:47:22,740 --> 00:47:25,320
as you guys know about a lot of answers
don't write Facebook is orders of

836
00:47:25,321 --> 00:47:29,040
magnitude larger than Twitter. It's the
most important traffic driver of, of,

837
00:47:29,350 --> 00:47:33,220
of news audiences, online, bar none.

838
00:47:33,790 --> 00:47:36,280
And so it seems like
that was where people,

839
00:47:36,281 --> 00:47:39,520
how people were getting
from social media to, uh,

840
00:47:39,670 --> 00:47:43,180
to the fake news sites
was the seating happened.

841
00:47:43,181 --> 00:47:45,610
And that was starting a diffusion process.
Now again,

842
00:47:45,611 --> 00:47:48,910
it's important to think about this,
like this is a very skewed distribution,

843
00:47:48,911 --> 00:47:52,300
right? There's a very large number of fake
news articles that almost no one read.

844
00:47:52,480 --> 00:47:54,970
And if you go to these fake news sites,
there's tons and tons of these.

845
00:47:54,971 --> 00:47:58,120
They were really mass produced in
a very slipshod way. And you know,

846
00:47:58,121 --> 00:48:01,300
any kind of monkeys on typewriters,
fashion,

847
00:48:01,480 --> 00:48:03,610
every so often they hit on a big one.
Right?

848
00:48:03,611 --> 00:48:08,611
And that one went hugely viral and
seems to have disseminated often via

849
00:48:09,460 --> 00:48:12,130
primarily via social media. But that's
certainly not the only thing is you can,

850
00:48:12,220 --> 00:48:13,060
as you can see here,

851
00:48:14,770 --> 00:48:17,960
follow up on that one. Um, you've been,

852
00:48:18,410 --> 00:48:21,920
I think scrupulously neutral about, you
know, the source of this stuff, you know,

853
00:48:21,921 --> 00:48:24,950
is it stops just out there and
the Darwinian sense, you know,

854
00:48:24,951 --> 00:48:26,540
some of this seems to,
you know,

855
00:48:26,541 --> 00:48:30,020
get viral and take off just because
it resonates with people or is there

856
00:48:30,021 --> 00:48:33,320
something more purposeful going
on, whether it's, you know,

857
00:48:33,890 --> 00:48:38,540
particular groups that have,
you know, particular agendas,
state actors, whatever.

858
00:48:38,541 --> 00:48:39,374
Do you study that?

859
00:48:41,040 --> 00:48:45,330
So the state actor part is hard
to assess. Um, I, you know,

860
00:48:45,390 --> 00:48:47,640
you should ask the intelligence community,
um,

861
00:48:48,690 --> 00:48:53,520
the reporting on fake news specifically
right now suggest a more decentralized

862
00:48:53,521 --> 00:48:57,000
process where people were being
attracted largely by the profit motive.

863
00:48:57,210 --> 00:49:00,000
In some cases they might have
ideological partisan motives as well,

864
00:49:00,001 --> 00:49:03,660
but there's very little evidence of, of,
of kind of coordinated effort. Right?

865
00:49:03,661 --> 00:49:06,180
So I showed you that FBI agent story,
uh,

866
00:49:06,240 --> 00:49:09,540
earlier the Hillary Clinton investigation,
you died in a murder suicide.

867
00:49:09,541 --> 00:49:13,140
The guy who wrote that was literally
unemployed, living at home,

868
00:49:13,230 --> 00:49:14,310
trying to figure out how to make money.

869
00:49:14,311 --> 00:49:17,240
He's since come forward and
explained his story. Um,

870
00:49:17,640 --> 00:49:20,880
the Macedonian teenagers didn't have to
seem to have any particular ax to grind

871
00:49:20,881 --> 00:49:22,380
as far as anyone can tell and so forth.

872
00:49:22,470 --> 00:49:24,990
So it's possible there's
coordination or state actors,

873
00:49:25,110 --> 00:49:28,620
some of those other kinds of
state sponsored misinformation
online we can talk

874
00:49:28,621 --> 00:49:29,810
about.
But in terms of the,

875
00:49:29,900 --> 00:49:32,970
the set of sites that were being
widely distributed during the campaign,

876
00:49:32,971 --> 00:49:36,300
I think it's fair to say that there's
not a lot of hard evidence of the record

877
00:49:36,420 --> 00:49:37,290
of that at this point.

878
00:49:37,990 --> 00:49:38,590
So,

879
00:49:38,590 --> 00:49:41,950
so journalists are constantly thinking
about how they can rewrite headlines or

880
00:49:41,951 --> 00:49:45,430
taglines to get to make
their content real news.

881
00:49:45,431 --> 00:49:48,160
We like to call it a go viral.
Um,

882
00:49:48,400 --> 00:49:52,280
were there particular
headline structures or, um,

883
00:49:52,960 --> 00:49:57,910
pictures added or structures to specific
Facebook posts that kind of stood out

884
00:49:57,911 --> 00:49:59,830
in a common thread for
these posts that went viral?

885
00:50:00,760 --> 00:50:05,050
I mean, I would say the common thread
was mostly Hillary is evil, right?

886
00:50:05,051 --> 00:50:07,690
If you had to boil it down to three words,
um,

887
00:50:07,720 --> 00:50:11,780
but there were certainly lots of stories
like that that didn't resonate. Um,

888
00:50:11,860 --> 00:50:15,010
it does seem to have something
of a Darwinian feel to it.

889
00:50:15,040 --> 00:50:17,840
Like there's lots and lots
and lots of other, uh,

890
00:50:18,050 --> 00:50:22,150
articles that say Hillary is evil that
didn't take off. And for whatever reason,

891
00:50:22,151 --> 00:50:25,150
these resonated.
These five examples aren't the best.

892
00:50:25,151 --> 00:50:30,151
But some of these are kind of in a way
that Parsons might enjoy laughing and

893
00:50:31,670 --> 00:50:34,360
being like, ha ha ha, look, you know,
I don't really believe this bill.

894
00:50:34,361 --> 00:50:35,440
Look how funny it is.
Right?

895
00:50:35,450 --> 00:50:39,170
So one of the most widely shared pro
Clinton fake news stories said Ireland is

896
00:50:39,171 --> 00:50:43,550
taking a political asylum. Refugees
from the United States because of Trump.

897
00:50:43,910 --> 00:50:44,151
Right.

898
00:50:44,151 --> 00:50:47,330
And so how many people were sharing
that because they actually believed it?

899
00:50:47,331 --> 00:50:48,290
How many were sharing?

900
00:50:48,291 --> 00:50:51,710
It is a kind of expression of
anti-Trump feelings or identity, right.

901
00:50:51,711 --> 00:50:54,140
Is less clear. Um,
another one from online.

902
00:50:54,260 --> 00:50:57,220
So if raw fact checking
doesn't convince people, uh,

903
00:50:57,300 --> 00:50:59,030
and might even reinforce false beliefs,

904
00:50:59,090 --> 00:51:02,760
are there any strategies we have
evidence for that do work? You know,

905
00:51:03,380 --> 00:51:07,290
there is, there is evidence in fact
checking works right? So I don't want to,

906
00:51:07,291 --> 00:51:11,110
I don't want to be totally pessimistic
here and say that this problem isn't,

907
00:51:11,250 --> 00:51:14,210
isn't solvable under, under
any conditions. I mean, so, um,

908
00:51:14,340 --> 00:51:17,280
there's a lot of experimental
work now looking at the, uh,

909
00:51:17,281 --> 00:51:21,120
the influence of information
from nonpartisan fact
checking organizations like

910
00:51:21,210 --> 00:51:25,620
politifact and, um, uh, politik
track and other, other organizations,

911
00:51:25,950 --> 00:51:29,640
um, uh, when it comes to, but when
it goes outside of that framework,

912
00:51:29,641 --> 00:51:33,030
when it comes to just kind of watchdog
journalism and trying to correct

913
00:51:33,031 --> 00:51:36,240
misperceptions and more
widely read formats, um, yeah,

914
00:51:36,241 --> 00:51:39,900
there are all sorts of best practices
that, that people can engage it.

915
00:51:39,901 --> 00:51:43,440
And I'm so one of the most widely studied
ones that I would just point out as as

916
00:51:43,441 --> 00:51:45,600
one example. Um, is it people,

917
00:51:45,720 --> 00:51:49,860
people are really bad at processing
negations of planes. Okay.

918
00:51:49,861 --> 00:51:53,970
So Brendan's written written on
this. Um, and, uh, people, uh,

919
00:51:53,971 --> 00:51:56,630
over time tend to remember information.
Uh,

920
00:51:56,640 --> 00:51:58,320
if you tell someone
that something is false,

921
00:51:58,321 --> 00:52:00,540
they tend to remember the
information over time,

922
00:52:00,541 --> 00:52:04,080
but they often forget that it's
false, right? So this is an illusion.

923
00:52:04,081 --> 00:52:07,050
It's called the illusion of
truth, the fact, right? So
you might correct something,

924
00:52:07,051 --> 00:52:08,530
it might,
you know,

925
00:52:08,540 --> 00:52:11,670
might change their factual beliefs
that at the point of the correction,

926
00:52:11,820 --> 00:52:14,750
but then over time people
are bad at processing, uh,

927
00:52:14,850 --> 00:52:17,640
cognitively that things are not true,
right?

928
00:52:17,641 --> 00:52:20,250
And so they might come to actually
hold that belief more over time.

929
00:52:20,251 --> 00:52:24,660
So avoiding negations and framing things
in the affirmative, right? So the,

930
00:52:24,840 --> 00:52:27,330
the classic example of this
from recent years is, um,

931
00:52:27,690 --> 00:52:31,080
Obama is a Christian versus
Obama is not a Muslim.

932
00:52:32,130 --> 00:52:35,690
People just become, uh, especially
when they're motivated, right?

933
00:52:35,720 --> 00:52:37,020
Like they might be with that when people,

934
00:52:37,260 --> 00:52:41,340
people are oftentimes susceptible
to losing sight of, of,

935
00:52:41,341 --> 00:52:42,570
of litigation over time.

936
00:52:43,150 --> 00:52:44,740
And unfortunately that's
all we have time for.

937
00:52:44,741 --> 00:52:45,960
Thank you so much for
coming to [inaudible].

938
00:52:46,010 --> 00:52:46,170
Okay.

