1
00:00:08,130 --> 00:00:12,660
And thanks for your interest in
this topic. Effective altruism,

2
00:00:12,720 --> 00:00:17,720
philanthropy as a for profit and ever
and the Al shortly delve into what that

3
00:00:18,121 --> 00:00:22,800
means for profit thinking and charity,
how that can be a meaningfully combined.

4
00:00:24,870 --> 00:00:29,850
Um, yeah, let's start off with a
definition of what effective altruism is.

5
00:00:29,851 --> 00:00:32,520
It's a practical philosophy
and social movement,

6
00:00:32,550 --> 00:00:37,380
which you might have heard of that
uses empirical evidence and reason to

7
00:00:37,381 --> 00:00:42,240
determine the most effective ways to
benefit others in global society. Now,

8
00:00:42,330 --> 00:00:46,530
that may sound good, but it doesn't
tell us that much. Um, of course.

9
00:00:47,070 --> 00:00:51,870
Um, in any case, there are two conceptual
components, altruism and effectiveness.

10
00:00:51,871 --> 00:00:56,310
Altruism is essentially, um,
the conceptual opposite of,

11
00:00:56,370 --> 00:00:57,240
of egoism.

12
00:00:57,600 --> 00:01:02,600
So it means that we're sort of
practically oriented in our action towards

13
00:01:02,790 --> 00:01:03,930
helping others as well,

14
00:01:03,931 --> 00:01:08,931
and not just considering our self interest
and effectiveness means that we're

15
00:01:09,871 --> 00:01:13,170
trying to,
of course optimally achieve our goals,

16
00:01:13,171 --> 00:01:15,000
including our altruistic goals.

17
00:01:15,000 --> 00:01:19,650
So we're trying to maximize the
probability of a, of goal achievement.

18
00:01:20,120 --> 00:01:20,953
Um,

19
00:01:21,900 --> 00:01:26,790
now why should we be altruistic
at all? One might wonder, I mean,

20
00:01:26,820 --> 00:01:30,090
in economics there's also, I
mean, there are various, uh,

21
00:01:30,120 --> 00:01:34,980
prevalent theories of course, but, um,
there's a big strand at least in, uh, in,

22
00:01:34,981 --> 00:01:38,880
in western economics saying, well,
you know, we should be selfish,

23
00:01:39,180 --> 00:01:42,720
a utility maximizers so that
might raise the question,

24
00:01:42,721 --> 00:01:47,100
why be altruistic at all?
Why be interested in, in,
in benefiting others too.

25
00:01:47,101 --> 00:01:50,910
And there are various, uh, thought
experiments and arguments to,

26
00:01:50,930 --> 00:01:53,760
to justify this practical orientation.

27
00:01:53,761 --> 00:01:58,761
So imagine you're in the unfortunate
position of a firefighter faced with two

28
00:02:00,181 --> 00:02:04,050
buildings, uh, to burning
buildings. One big one, small.

29
00:02:04,051 --> 00:02:08,370
And let's say there are a hundred people
being trapped at the moment in the big

30
00:02:08,371 --> 00:02:10,260
building and,
uh,

31
00:02:10,350 --> 00:02:15,310
just one person being trapped in the
small building and they also received the

32
00:02:15,320 --> 00:02:19,380
information that unfortunately, it won't
be possible to save all of these people.

33
00:02:19,410 --> 00:02:24,030
So you have to, you're faced with
this moral dilemma. You're either, um,

34
00:02:24,420 --> 00:02:29,190
can save these 100 people in the big
building or the one person in the small

35
00:02:29,191 --> 00:02:33,030
building. That's the moral dilemma
you're in. And you know, for simplicity,

36
00:02:33,031 --> 00:02:37,800
let's say that you have like in either
case, the 100% success probability,

37
00:02:37,801 --> 00:02:39,880
so you'll succeed,
uh,

38
00:02:39,990 --> 00:02:44,130
at saving these 100 or the one person
depending on the choice you make.

39
00:02:44,610 --> 00:02:48,170
Now of course, there's always a third
option as well if you're the firefighter,

40
00:02:48,180 --> 00:02:49,160
you could just decide to,

41
00:02:49,161 --> 00:02:53,880
to go have a beer instead and I'm and
not be bothered with the situation.

42
00:02:53,910 --> 00:02:58,020
So this will be the non altruistic
choice of course. And uh, yeah,

43
00:02:58,021 --> 00:03:00,670
maybe the selfish choice, the
pure lust, selfish choice,

44
00:03:00,671 --> 00:03:03,880
just go have a beer and
ignore the moral catastrophe,

45
00:03:03,881 --> 00:03:08,630
the suffering and the imminent
deaths there. Um, but you know, if,

46
00:03:08,660 --> 00:03:11,840
if we agree that three
is not an option as, um,

47
00:03:12,100 --> 00:03:15,820
most people do when faced with
moral catastrophe, then yeah,

48
00:03:15,821 --> 00:03:20,080
we need to choose between one and two.
And yeah, I mean it's a, it's a tragic,

49
00:03:20,081 --> 00:03:23,080
it's a sad choice because somebody
is going to suffer and die anyway.

50
00:03:23,081 --> 00:03:25,810
But it seems that clearly
in such a situation,

51
00:03:25,811 --> 00:03:30,811
we would opt for the lesser evil and the
lesser evil here seems to be saving the

52
00:03:30,851 --> 00:03:32,260
100 and not the one.

53
00:03:32,470 --> 00:03:37,330
So here we'd probably go for a
option one and save a 100 people.

54
00:03:37,870 --> 00:03:42,730
And um, this, so if we agree you sort of
with the background reasoning here then,

55
00:03:43,170 --> 00:03:45,400
um,
that's an argument for two things.

56
00:03:45,401 --> 00:03:50,401
One that we don't just have
selfish or self interested goals.

57
00:03:50,891 --> 00:03:53,590
We have at least some
altruistic goals as well,

58
00:03:53,860 --> 00:03:58,450
at least in situations of
moral catastrophe. And two,
we care about the numbers.

59
00:03:58,451 --> 00:04:02,380
So when helping others, it doesn't
just matter that we are helping.

60
00:04:02,470 --> 00:04:05,480
It does also matter that we try and help,
um,

61
00:04:06,280 --> 00:04:10,690
the greatest possible number of people.
Um,

62
00:04:10,691 --> 00:04:15,420
there are further arguments which you also
may have heard of, um, originating in,

63
00:04:15,421 --> 00:04:16,840
in practical philosophy.

64
00:04:17,320 --> 00:04:21,190
This is a drowning child thought
experiment going back to a philosopher,

65
00:04:21,191 --> 00:04:21,821
Peter Singer.

66
00:04:21,821 --> 00:04:26,821
So imagine you're walking past a lake
or a shallow pond and you realize that a

67
00:04:28,391 --> 00:04:31,590
child, a small child is
drowning in there. Um,

68
00:04:31,630 --> 00:04:36,630
you look around and you can't see any
parents or anyone else that would be

69
00:04:37,421 --> 00:04:38,560
willing and able to help.

70
00:04:38,830 --> 00:04:43,090
And the realize that this child's
life depends on what you do now.

71
00:04:43,150 --> 00:04:48,150
So you can walk into that pond or
that lake and just safe the child.

72
00:04:48,460 --> 00:04:52,660
There are no complications,
no risks or dangers involved to yourself.

73
00:04:53,160 --> 00:04:57,490
Um, so you could just walk
in and save her life. Well,

74
00:04:57,550 --> 00:05:01,120
that's not entirely true.
There is a little complication
and it's the following.

75
00:05:01,121 --> 00:05:05,470
You're wearing pretty expensive clothes
and expensive shoes. Maybe your,

76
00:05:05,710 --> 00:05:10,120
you happen to be on your way to
an important meeting. Let's say,

77
00:05:10,180 --> 00:05:13,360
uh, you're expensive suit
and shoes cost, whatever,

78
00:05:13,361 --> 00:05:17,650
a total amount of $1,000. And
they realize that, you know, if,

79
00:05:17,651 --> 00:05:22,330
if you're now running or jumping into that
pond and lake and no risk to yourself,

80
00:05:22,331 --> 00:05:24,430
you're going to ruin the
clothes and the shoes.

81
00:05:24,730 --> 00:05:27,100
Let's also assume that there
will be no replacement.

82
00:05:27,101 --> 00:05:32,101
So you'll just have to buy a new
suit and new shoes for $1,000.

83
00:05:32,590 --> 00:05:36,220
So then the question is,
would you save the life, um,

84
00:05:36,550 --> 00:05:41,110
under these circumstances, in that
situation? Or what do you not? Um,

85
00:05:41,380 --> 00:05:45,730
unsurprisingly, the vast majority of
people say, ah, they would save the life.

86
00:05:45,731 --> 00:05:50,200
Of course, if they didn't really
existentially need these $1,000,

87
00:05:50,201 --> 00:05:52,270
of course it can construct a
situation where you're like, yeah,

88
00:05:52,271 --> 00:05:55,690
I'm going to be starving if
I'll have to, you know, uh,

89
00:05:55,720 --> 00:05:59,870
spend 1000 in addition to what
I'm spending anyway. But you know,

90
00:05:59,871 --> 00:06:04,360
if you're still going to be comfortably
off, if that's just going to mean, um,

91
00:06:04,640 --> 00:06:08,480
a little less luxury in your life, then
most people say, yeah, in that case,

92
00:06:08,481 --> 00:06:10,670
it's really a no brainer that
they would save the life.

93
00:06:11,150 --> 00:06:14,120
And this also shows that we do indeed,

94
00:06:14,180 --> 00:06:17,540
or most people do indeed have
important altruistic goals as well,

95
00:06:17,690 --> 00:06:22,640
and they would be willing to sacrifice
quite a bit of money in order to achieve

96
00:06:22,641 --> 00:06:23,540
these altruistic goals,

97
00:06:23,541 --> 00:06:26,960
to reduce the suffering of others
and save the life of others,

98
00:06:27,350 --> 00:06:30,320
at least if that doesn't
put themselves in,

99
00:06:30,380 --> 00:06:35,030
in sort of an existentially very
uncomfortable or dangerous situation.

100
00:06:36,140 --> 00:06:40,460
But, so if we follow the reasoning
behind these two thought experiments,

101
00:06:40,461 --> 00:06:43,260
the firefighter and the drowning child,
um,

102
00:06:43,280 --> 00:06:47,060
that may have pretty significant practical
implications because we can then ask,

103
00:06:47,061 --> 00:06:51,350
well, okay, now we do know that there
are also burning buildings out there.

104
00:06:51,351 --> 00:06:55,130
There's a lot of suffering out there.
A lot of people are dying maybe from,

105
00:06:55,160 --> 00:06:58,940
from preventable causes that
we could do something about.

106
00:06:59,390 --> 00:07:04,190
So let's say if we have on our bank
account $1,000 or, uh, you know,

107
00:07:04,220 --> 00:07:07,730
maybe many times as much that we could
donate and we would still be comfortably

108
00:07:07,731 --> 00:07:12,500
off. Um, why don't we do
it? So I mean, if, if it's,

109
00:07:12,501 --> 00:07:16,010
if it's a no brainer to save a
drowning child in that situation,

110
00:07:16,370 --> 00:07:17,360
then one can ask,
well,

111
00:07:17,361 --> 00:07:20,690
wouldn't it also be just equally
irrational and the no brainer actually to

112
00:07:20,691 --> 00:07:24,290
dominate a lot more if we're not
already doing it? And then of course,

113
00:07:24,291 --> 00:07:25,730
try and figure out,
yeah,

114
00:07:25,731 --> 00:07:30,440
where this money could go the longest way
because it's important to save as many

115
00:07:30,441 --> 00:07:32,840
people as possible as we saw in
the first thought experiment.

116
00:07:33,200 --> 00:07:38,200
And that's precisely what the effective
altruism movement is trying to do.

117
00:07:38,631 --> 00:07:42,680
Of course, if we follow the spirit
of these thought experiments,

118
00:07:42,681 --> 00:07:47,450
then one action point is, yeah,
trying to check out the material,

119
00:07:47,451 --> 00:07:50,480
the philosophical material,
the scientific, the economic,

120
00:07:50,510 --> 00:07:54,260
the empirical material on
strategic do Gooding, um,

121
00:07:54,380 --> 00:07:59,380
where should we donate our time or our
money if we want to make a difference?

122
00:08:00,170 --> 00:08:02,890
Then one standard action point,
um,

123
00:08:02,930 --> 00:08:07,760
within the effective altruism movement
has been trying to donate at least 10% of

124
00:08:07,761 --> 00:08:10,880
one's income. Now, when I
first heard of this idea,

125
00:08:10,881 --> 00:08:15,620
donate 10% of my income, I was
like, yeah, but that's like a lot.

126
00:08:15,650 --> 00:08:20,630
And yes, I do have altruistic goals,
but I'm not a moral saint. Um,

127
00:08:21,080 --> 00:08:24,140
listen, this too much of a challenge
isn't this sort of over demanding,

128
00:08:24,410 --> 00:08:28,760
but then when you actually reason about
it more and check out the happiness,

129
00:08:28,790 --> 00:08:33,470
psychology and happiness economics
behind it, many people conclude that yes,

130
00:08:33,471 --> 00:08:35,840
you know, it's quite
unlikely. So if you live in a,

131
00:08:35,841 --> 00:08:39,090
in a rich country and earn a good salary,
it's quite likely that,

132
00:08:39,190 --> 00:08:41,660
that you'd be worse off,
uh,

133
00:08:41,661 --> 00:08:46,460
when donating 10% of your income
each month. And quite the opposite.

134
00:08:46,470 --> 00:08:49,880
There's quite solid happiness,
economic and happiness,

135
00:08:49,881 --> 00:08:54,881
psychological research suggesting that
donations often tend to make the donors

136
00:08:55,140 --> 00:08:57,570
happier as well.
So it can be a win win.

137
00:08:58,110 --> 00:09:00,570
And this is why many
effective altruists have, um,

138
00:09:00,750 --> 00:09:05,340
decided to donate at least 10%
of their income. And, uh, yeah,

139
00:09:05,341 --> 00:09:07,920
it's quite astounding what
can be achieved in this way.

140
00:09:07,921 --> 00:09:10,410
So currently on a global level,

141
00:09:10,440 --> 00:09:13,510
there are a few thousand people at least,
um,

142
00:09:13,740 --> 00:09:16,950
that strongly identify with
effective altruism. I mean,

143
00:09:17,670 --> 00:09:22,500
maybe already tens of thousands that
maybe loosely identify with the tenants by

144
00:09:22,501 --> 00:09:26,730
the few thousand that have decided to
donate 10% or more and have taken the

145
00:09:26,731 --> 00:09:29,730
pledge.
This pledge is not legally binding,

146
00:09:29,731 --> 00:09:34,731
but it's sort of personally and maybe
socially binding a pledge to donate 10%

147
00:09:34,771 --> 00:09:39,450
over a lifetime. And in aggregate,
these pledges are already worth,

148
00:09:39,470 --> 00:09:44,040
uh, several, several billion dollars
in sort of promised donations.

149
00:09:44,041 --> 00:09:47,530
And these donations are now happening.
So it seems possible. And the movement,

150
00:09:47,531 --> 00:09:52,110
this is only just starting essentially.
So it seems possible to compete with, uh,

151
00:09:52,111 --> 00:09:54,450
with billionaires actually
with this strategy,

152
00:09:54,451 --> 00:09:59,010
even if we're not billing
there as ourselves. Um, the
other main action point,

153
00:09:59,011 --> 00:10:03,900
this third effective altruists of course
don't just try to donate money but also

154
00:10:03,901 --> 00:10:05,550
time.
Um,

155
00:10:06,150 --> 00:10:09,240
there is an organization
called 80,000 hours,

156
00:10:09,540 --> 00:10:13,980
which provides career advice to people
interested in making as much of an

157
00:10:13,981 --> 00:10:17,700
altruistic difference as possible.
On average,

158
00:10:17,730 --> 00:10:21,960
we work about $80,000,
uh, in our lifetime.

159
00:10:22,350 --> 00:10:27,350
And of course it's both in terms of
the personal stakes and in terms of the

160
00:10:27,391 --> 00:10:30,330
stakes for the world at large,
the altruistic stakes,

161
00:10:30,900 --> 00:10:34,770
this is a huge decision. What
are we gonna do with these, um,

162
00:10:35,190 --> 00:10:40,040
$80,000. And it turns out that
sort of standard, traditional, um,

163
00:10:40,290 --> 00:10:45,290
career advice that people
have been receiving if they
were interested in making

164
00:10:46,561 --> 00:10:51,030
an altruistic difference isn't very useful
or very rational. So, I mean if you,

165
00:10:51,031 --> 00:10:56,031
if you Paul people and ask them for
examples of like standardly altruistic

166
00:10:56,730 --> 00:11:00,870
careers, many will for instance,
say doctor doctor in uh,

167
00:11:01,260 --> 00:11:04,560
the developing world,
that's like a standard intuition.

168
00:11:04,890 --> 00:11:06,120
But if you think about that,

169
00:11:06,930 --> 00:11:10,830
if the goal is to make as much
of a difference as possible,

170
00:11:11,700 --> 00:11:14,520
you need to work on something
that wouldn't happen otherwise.

171
00:11:14,521 --> 00:11:19,410
So if you take the job of a doctor and
if it's the case that if you had not

172
00:11:19,411 --> 00:11:21,450
taken that job,
somebody else would be doing it,

173
00:11:21,451 --> 00:11:23,460
then maybe you're not
making much of a counter.

174
00:11:23,520 --> 00:11:25,410
The counter factual difference actually.

175
00:11:25,920 --> 00:11:29,950
And in some cases it can be a
much better strategy to say, uh,

176
00:11:30,000 --> 00:11:35,000
decide to go and earn as much money as
possible and then give quite a large

177
00:11:35,491 --> 00:11:37,980
fraction of that.
For instance,

178
00:11:37,981 --> 00:11:42,600
in order to enable several other
people to become doctors in developing

179
00:11:42,601 --> 00:11:47,160
countries. So with that sort of
strategy often called earning to give,

180
00:11:47,250 --> 00:11:50,610
it could be possible to make a much bigger
counterfactual difference and also to

181
00:11:50,611 --> 00:11:54,760
multiply once impact. And of course
that's just one consideration.

182
00:11:55,000 --> 00:11:57,490
These considerations could
go in either direction.

183
00:11:57,520 --> 00:12:00,010
It depends on what the biggest
bottle necks are. I mean, if,

184
00:12:00,310 --> 00:12:05,090
if money isn't the big bottleneck, then
probably earning to give won't be, uh,

185
00:12:05,140 --> 00:12:08,890
that promising strategy. So it, it
depends on the specifics of course,

186
00:12:09,190 --> 00:12:12,860
but it turns out that traditional
altruistic career advice, um,

187
00:12:13,660 --> 00:12:18,490
often isn't very useful.
Now another question of course,

188
00:12:18,491 --> 00:12:22,660
is, well, what are the biggest,
um, metaphorically speaking,

189
00:12:22,661 --> 00:12:27,350
burning buildings out there?
Um,

190
00:12:28,660 --> 00:12:28,790
uh,

191
00:12:28,790 --> 00:12:33,790
traditional focus of do gooders has
been just turning to local problems,

192
00:12:34,810 --> 00:12:37,660
problems in, uh, in our own society.

193
00:12:37,661 --> 00:12:42,400
And there are sayings like charity
begins at home and so on. Um,

194
00:12:42,550 --> 00:12:45,550
but if we strive to make the
biggest possible difference,

195
00:12:45,551 --> 00:12:50,170
that's not a very plausible focus
area because at least if we live in,

196
00:12:50,190 --> 00:12:53,260
if an enrich in developed countries,
it's,

197
00:12:53,290 --> 00:12:57,220
it's quite hard to actually save a
life for say, a few thousand dollars.

198
00:12:57,240 --> 00:12:59,860
And most of the time,
totally impossible.

199
00:13:00,280 --> 00:13:02,440
So if we look into our
health care systems,

200
00:13:02,441 --> 00:13:06,490
it usually takes several hundred
thousand dollars in order to save a life.

201
00:13:06,491 --> 00:13:10,900
And health care economics, uh, can
calculate that pretty precisely.

202
00:13:11,410 --> 00:13:16,300
But because of this dynamic of
diminishing marginal utility of money,

203
00:13:16,600 --> 00:13:16,900
it's,

204
00:13:16,900 --> 00:13:21,900
it tends to be a lot easier to save
lives and reduce suffering and advanced

205
00:13:22,090 --> 00:13:26,680
society in poorer countries.
So I'm a more plausible focus area with,

206
00:13:26,681 --> 00:13:29,200
for instance,
we are looking to the refugee crisis.

207
00:13:29,200 --> 00:13:33,130
I mean we can roughly a look
at them at the victim counts.

208
00:13:33,131 --> 00:13:38,131
People that are suffering turns out about
50 or 60 million people are currently

209
00:13:38,960 --> 00:13:40,930
a categorized us as refugees,

210
00:13:41,260 --> 00:13:45,550
but it seems that there are much bigger
burning buildings still. For instance,

211
00:13:45,551 --> 00:13:46,600
global poverty,

212
00:13:46,780 --> 00:13:50,890
extreme poverty still affect
about 800 million people.

213
00:13:51,310 --> 00:13:56,310
The standard definition is of of extreme
global poverty is an individual living

214
00:13:56,441 --> 00:14:01,210
on less than $2 per day.
That's purchasing power parity of course.

215
00:14:01,600 --> 00:14:06,600
And that means usually means permanent
undernourishment I'm suffering from

216
00:14:08,440 --> 00:14:11,800
diseases that are actually totally
treatable because we can't afford any,

217
00:14:11,830 --> 00:14:13,690
any medical treatment and so on.

218
00:14:13,990 --> 00:14:18,790
So global poverty and health is a really
big um, burning building as it were,

219
00:14:18,791 --> 00:14:22,060
a really big cause area.
Um,

220
00:14:22,330 --> 00:14:26,890
and other sort of general cause area,
uh,

221
00:14:26,950 --> 00:14:29,240
that effective altruists
tend to be interested in is,

222
00:14:29,241 --> 00:14:31,360
is global catastrophic risks.

223
00:14:31,720 --> 00:14:35,770
Because of course when we're dealing with
risks are for potentially global scale,

224
00:14:36,430 --> 00:14:41,020
well literally everybody on earth could
be affected and not just the present

225
00:14:41,021 --> 00:14:43,480
generation but also
all future generations.

226
00:14:43,480 --> 00:14:48,040
That's also a consideration and
important to many effective altruists.

227
00:14:48,370 --> 00:14:50,830
So if we think that the numbers count,

228
00:14:50,930 --> 00:14:55,140
the victim counts are important,
then um,

229
00:14:55,370 --> 00:14:58,670
it seems that at least
if civilization goes on,

230
00:14:59,180 --> 00:15:03,560
most of the people that will ever live,
we'll of course live in,

231
00:15:03,561 --> 00:15:07,910
in the future. So at this could then
lead to an argument of the sword. Well,

232
00:15:08,180 --> 00:15:09,690
of course the present generation is,

233
00:15:09,710 --> 00:15:12,480
is intrinsically important and
certainly very instrumental. Important.

234
00:15:12,481 --> 00:15:15,770
But all the future generations
taken together could,

235
00:15:15,890 --> 00:15:20,890
could be enormously and overwhelmingly
important if there's ways to positively

236
00:15:21,830 --> 00:15:24,380
affect their wellbeing.
Um,

237
00:15:25,100 --> 00:15:28,460
and of course some global
catastrophic risks are environmental,

238
00:15:29,240 --> 00:15:33,650
some are political, international
warfare say some are technological.

239
00:15:34,040 --> 00:15:37,400
And there's also of course an
overlapping area between all of these,

240
00:15:37,730 --> 00:15:42,530
I mean nuclear war of course
at the intersection of
technology and international

241
00:15:42,531 --> 00:15:44,090
politics.
Um,

242
00:15:44,150 --> 00:15:49,100
but there are technological risks of
various kinds and also some stemming from

243
00:15:49,250 --> 00:15:53,570
our action or are omission. So,
for instance, one cause area,

244
00:15:53,571 --> 00:15:57,890
some effective altruists are interested
in is, is biotechnology in general.

245
00:15:58,130 --> 00:15:59,360
So biosecurity,

246
00:15:59,660 --> 00:16:04,070
of course here there are global
risks from just harmful action. Yeah.

247
00:16:04,071 --> 00:16:06,050
If you consider synthetic biology,

248
00:16:06,470 --> 00:16:11,470
the possibility to create artificial
artificially create bacteria or viruses

249
00:16:13,221 --> 00:16:17,480
that could cause pandemics. Um,
so that, that's certainly a risk.

250
00:16:17,900 --> 00:16:21,530
But then some risk also could
sort of stem from our mission,

251
00:16:21,560 --> 00:16:24,290
our failure to advance
certain technologies.

252
00:16:24,770 --> 00:16:27,980
I'm abroad because area some effective
altruists are interested in is

253
00:16:28,080 --> 00:16:33,080
transhumanism of thinking about the
human condition from a far future

254
00:16:33,261 --> 00:16:36,710
perspective, biologically
informed perspective. Um,

255
00:16:36,770 --> 00:16:41,270
and some sub cause area there is trying
to fight and ultimately eliminate aging

256
00:16:41,271 --> 00:16:43,310
for instance.
So if we take a fire,

257
00:16:43,311 --> 00:16:46,370
if you'd take a far future
perspective and let's say it's,

258
00:16:46,400 --> 00:16:51,400
it's techno technologically possible to
sort of fight diseases and the diseases

259
00:16:52,011 --> 00:16:56,570
of old age to a point where we're
really no longer aging in a meaningful,

260
00:16:56,990 --> 00:17:00,080
uh, biological sense.
So if that is possible,

261
00:17:00,081 --> 00:17:04,280
it seems like if we take
sufficient technological
action to bring it about that

262
00:17:04,281 --> 00:17:08,740
could be a huge benefit. And in a sense
maybe of course, you know, that's a,

263
00:17:08,830 --> 00:17:10,790
that's a separate long discussion.

264
00:17:10,791 --> 00:17:15,350
To what extent are biological condition
also represents a catastrophe in the way.

265
00:17:15,890 --> 00:17:18,200
But this is a case where
technology of course, you know,

266
00:17:18,201 --> 00:17:22,820
through the advancement of medicine has
already brought us huge benefits and

267
00:17:22,821 --> 00:17:26,480
where failure to act now to invest in
the right kinds of maybe sort of very

268
00:17:26,481 --> 00:17:31,340
visionary in the Utopian research could
also mean that certain catastrophes go

269
00:17:31,341 --> 00:17:33,620
on for longer than would be necessary.

270
00:17:33,950 --> 00:17:37,910
So eliminating aging as maybe a
speculative buy their cars area that some

271
00:17:37,911 --> 00:17:42,911
effective boundaries have been
interested in and other cause areas,

272
00:17:42,921 --> 00:17:43,731
animal suffering.

273
00:17:43,731 --> 00:17:48,530
I mean this is also like
a long philosophical and
empirical at debate, but uh,

274
00:17:48,531 --> 00:17:52,320
so if we suppose that animals are
conscious to can suffer as well,

275
00:17:52,850 --> 00:17:56,610
and if say intelligence
is not super relevant for,

276
00:17:56,850 --> 00:18:01,200
for moral status. And that seems to be
what we believe for humans. I mean we're,

277
00:18:01,570 --> 00:18:04,200
when humans are concerned, we're
usually not saying, well, you know,

278
00:18:04,201 --> 00:18:07,830
these humans say children,
small children are less intelligent,

279
00:18:07,860 --> 00:18:10,050
therefore they're suffering matters less.

280
00:18:10,350 --> 00:18:13,500
If we're not going to go for such an Rdm
and then we might reason, well, okay,

281
00:18:13,501 --> 00:18:16,200
you know, animals might be far
less intelligent than we are,

282
00:18:16,201 --> 00:18:18,450
but if there's good evidence
that they can suffer as well,

283
00:18:18,690 --> 00:18:22,890
their numbers are also enormous. Um, and
maybe there's something we can do to, uh,

284
00:18:22,920 --> 00:18:24,570
to reduce their suffering as well.

285
00:18:24,571 --> 00:18:27,190
So some effective altruist have
taken that perspective and,

286
00:18:27,610 --> 00:18:32,250
and are therefore trying to, uh, to
do effective work in that course area.

287
00:18:33,030 --> 00:18:36,720
And there's more cause areas of course.
But that's just the rough overview.

288
00:18:37,200 --> 00:18:39,960
And as you can see,
unsurprisingly, I mean,

289
00:18:39,961 --> 00:18:43,650
the world is a hugely complex place,
um,

290
00:18:43,740 --> 00:18:46,560
especially also when it comes
to trying to improve it.

291
00:18:46,830 --> 00:18:49,930
So that's just sort of one
consideration. What are the biggest, um,

292
00:18:50,310 --> 00:18:52,710
burning buildings.
But then of course,

293
00:18:52,711 --> 00:18:57,300
if we want to know more precisely and
more specifically what the highest impact

294
00:18:57,301 --> 00:19:02,100
opportunities are for us specifically,
more considerations will be relevant.

295
00:19:02,101 --> 00:19:06,810
So scope of the problem that's sort of
the size of the burning building is just

296
00:19:06,811 --> 00:19:10,950
one, uh, one relevant consideration.
I mean, another consideration,

297
00:19:10,951 --> 00:19:14,010
needless to say is the solvability
or the tractability of a problem.

298
00:19:14,011 --> 00:19:16,890
So if you have like two
problems, one big, one small,

299
00:19:16,891 --> 00:19:20,400
and if it turns out that the big problem
is just not solvable, not realistic,

300
00:19:20,401 --> 00:19:23,070
less soluble then well, yes, you know,

301
00:19:23,071 --> 00:19:26,370
working on it won't be effective and
it could be more effective to work on a

302
00:19:26,371 --> 00:19:30,900
small problem with highest solvability.
So that's definitely another, um,

303
00:19:30,930 --> 00:19:33,810
relevant consideration.
Um,

304
00:19:34,050 --> 00:19:39,050
a third one is neglect in society and
this is based on the sort of economic

305
00:19:40,201 --> 00:19:44,190
assumption that activist resources,
so donation of time,

306
00:19:44,191 --> 00:19:49,191
donations of time and money also tend
to have diminishing marginal returns.

307
00:19:50,160 --> 00:19:52,050
So if, if, say you're one of the very,

308
00:19:52,260 --> 00:19:56,640
if there's a big problem out there in
society and the you happen to be one of

309
00:19:56,641 --> 00:19:59,040
the first people to help address it,

310
00:19:59,580 --> 00:20:03,030
then the chance is much higher than
you're going to be able to make a big

311
00:20:03,031 --> 00:20:06,420
difference that maybe you're going to be
able to make contributions of time and

312
00:20:06,421 --> 00:20:09,810
money of ideas, uh, that
wouldn't happen otherwise.

313
00:20:10,290 --> 00:20:13,180
But if the problem isn't
highly neglected in society,

314
00:20:13,181 --> 00:20:16,620
so if there are already a huge
numbers of people addressing it,

315
00:20:16,650 --> 00:20:20,880
both in civil society and in politics
may be in research and they're just one

316
00:20:20,881 --> 00:20:23,280
additional person there
contributing money or time,

317
00:20:23,700 --> 00:20:27,690
then the probability of course it will
be much lower than the are gonna make a

318
00:20:27,691 --> 00:20:30,240
huge difference.
That wouldn't happen otherwise.

319
00:20:30,241 --> 00:20:34,590
So neglect and society is definitely
another relevant consideration.

320
00:20:35,520 --> 00:20:36,480
Last but not least,

321
00:20:36,481 --> 00:20:41,481
personal fit and comparative advantage
also of groups and organizations and then

322
00:20:41,731 --> 00:20:44,970
of individuals is, is relevant
as well. So, I mean, yeah,

323
00:20:44,971 --> 00:20:48,220
if you have like a big problem that's
highly solvable and neglected in society,

324
00:20:48,221 --> 00:20:52,840
but it's just a very bad fit, you don't
really have any like required skill,

325
00:20:53,110 --> 00:20:55,840
you can't really make a huge
contributions to that problem.

326
00:20:55,841 --> 00:20:57,730
Maybe it's still better
to work on something else.

327
00:20:58,150 --> 00:21:03,100
So that's definitely also
an important consideration.
Your interests, your skills,

328
00:21:03,340 --> 00:21:06,820
um, your motivation of
course, you know, if,

329
00:21:06,850 --> 00:21:08,590
if you're an effective altruist,

330
00:21:09,120 --> 00:21:13,480
a core thing to work on is also minimizing
the probability that you're going to

331
00:21:13,481 --> 00:21:17,470
burn out at some point because that that
would of course hurt your impact big

332
00:21:17,471 --> 00:21:17,771
time.

333
00:21:17,771 --> 00:21:22,000
So all of these should factor into the
overall assessment and that can of course

334
00:21:22,001 --> 00:21:26,840
get hugely complex but
also very interesting. Um,

335
00:21:27,040 --> 00:21:30,140
now let's zoom in into
specific cause areas and,

336
00:21:30,141 --> 00:21:34,570
and look at some more examples and also
then specific data that enables us to

337
00:21:34,571 --> 00:21:38,230
make a cost effectiveness
and impact evaluations.

338
00:21:39,670 --> 00:21:42,040
Um,
in terms of global health,

339
00:21:42,490 --> 00:21:47,110
of course that's strongly related
to two general global poverty. Um,

340
00:21:47,140 --> 00:21:52,140
there are various diseases that affect
a huge number and kill a huge number of

341
00:21:52,211 --> 00:21:57,130
people every year and every day. So for
instance, the so called big three malaria,

342
00:21:57,160 --> 00:22:02,110
tuberculosis, HIV, killed many more
people each and every day than say,

343
00:22:02,111 --> 00:22:07,111
political violence and oppression and
warfare has tended to kill in a year.

344
00:22:08,320 --> 00:22:09,730
Now that's another thing.
Of course.

345
00:22:10,120 --> 00:22:13,390
I mean I'm not saying that
political action can be effective.

346
00:22:13,391 --> 00:22:14,890
I mean quite the opposite.
So I mean,

347
00:22:14,891 --> 00:22:19,140
even in terms of trying to fix
global health, ultimately if,

348
00:22:19,240 --> 00:22:23,590
if you can sort of go for concert at
political action, systemic action,

349
00:22:23,591 --> 00:22:26,620
that can be hugely effective.
There are also risks to that.

350
00:22:27,040 --> 00:22:31,120
But it does seem like many do gooders
do seem to sort of prematurely jump into

351
00:22:31,121 --> 00:22:34,090
politics because that's been
the traditional focus. Yeah,

352
00:22:34,091 --> 00:22:37,660
we knew we need to address something
politically or we need to address

353
00:22:37,661 --> 00:22:39,490
specifically political problems.

354
00:22:39,730 --> 00:22:41,890
But actually if we just
look at the victim counts,

355
00:22:41,891 --> 00:22:46,330
it's not obvious that this should
be the focus. So as I mentioned,

356
00:22:46,390 --> 00:22:48,670
if we consider these big diseases,

357
00:22:49,150 --> 00:22:53,160
the victim count at least has
tended to be much higher. Um,

358
00:22:53,680 --> 00:22:57,910
the solvability is also seems very
high, at least in principle. Uh,

359
00:22:57,911 --> 00:23:02,200
the medical technological solvability or
preventability very high in principle.

360
00:23:02,350 --> 00:23:04,870
That's also often like a huge
complication with politics.

361
00:23:05,170 --> 00:23:09,460
It's so messy and it's unclear whether
your campaign will succeed in song

362
00:23:10,540 --> 00:23:13,690
neglect in society also
comparatively high.

363
00:23:13,691 --> 00:23:17,320
So of course a lot of money is
going into medical research,

364
00:23:17,350 --> 00:23:22,240
but the bulk of the money is going into
research that aims to address diseases

365
00:23:22,241 --> 00:23:26,350
that are prevalent in western
societies, in rich societies. Why? Well,

366
00:23:26,351 --> 00:23:29,950
one reason is that you can make a huge
profit there because they're going to

367
00:23:29,951 --> 00:23:32,500
sell the treatment to
people in rich countries,

368
00:23:32,950 --> 00:23:36,280
whereas it's much harder to make
a great profit addressing malaria,

369
00:23:36,340 --> 00:23:39,550
which predominantly affects,
uh, the poorest people.

370
00:23:40,180 --> 00:23:43,660
But in terms of achieving
our altruistic goals,

371
00:23:43,661 --> 00:23:46,940
if we care about helping
as many people as possible,

372
00:23:47,810 --> 00:23:50,780
this can be a great focus
area. Of course, you know,

373
00:23:50,870 --> 00:23:55,870
trying to address just the diseases that
tend to affect the poorest of the poor.

374
00:23:57,210 --> 00:24:00,110
Um, now, interestingly, empirical studies,

375
00:24:00,111 --> 00:24:04,910
empirical data shows that there are
massive differences in cost effectiveness.

376
00:24:04,911 --> 00:24:08,300
So in terms of amount of lives saved,

377
00:24:08,310 --> 00:24:12,910
amount of suffering reduced between global
health, different global health, um,

378
00:24:12,950 --> 00:24:15,230
interventions.
And um,

379
00:24:15,260 --> 00:24:19,550
so the title of the talk said that
effective altruism is sort of trying to

380
00:24:19,551 --> 00:24:24,260
approach, um, altruism in
charity in a for profit way.

381
00:24:24,920 --> 00:24:29,900
And I didn't mean to say that we're
attempting to make a monetary profit,

382
00:24:29,901 --> 00:24:34,220
not at all, but it's sort of the
mindset of trying to maximize something,

383
00:24:34,221 --> 00:24:35,300
maximize a profit.

384
00:24:35,301 --> 00:24:39,770
But here of course the profit is being
defined in terms of lives saved and

385
00:24:39,800 --> 00:24:42,620
suffering reduced.
And interestingly,

386
00:24:42,621 --> 00:24:46,910
this hasn't really been the traditional
focus of charity because in traditional

387
00:24:46,911 --> 00:24:51,470
charity, what gets emphasized as sort
of more the emotional side of it, right?

388
00:24:51,500 --> 00:24:52,070
I mean,

389
00:24:52,070 --> 00:24:57,070
maybe you have a relative that died from
cancer and of course that affects you

390
00:24:57,171 --> 00:25:00,740
deeply, emotionally and
very legitimately of course.

391
00:25:00,770 --> 00:25:04,580
And then you're emotionally moved with
your sort of immediate compassion to do

392
00:25:04,581 --> 00:25:08,920
something about that. And uh, effective
altruism missing, trying to counter that.

393
00:25:08,990 --> 00:25:12,470
It's just saying, yes, let's take our
compassion. Let's take our emotions,

394
00:25:12,471 --> 00:25:16,070
let's take our heart.
But let's also combine that with our head,

395
00:25:16,130 --> 00:25:20,420
with our rationality or optimization
power that we are of course standardly

396
00:25:20,421 --> 00:25:25,130
applying when it comes to
actual for profit investments.

397
00:25:25,160 --> 00:25:27,740
You know,
in terms of monetary and personal profit.

398
00:25:27,741 --> 00:25:30,680
Now when we're investing
there for monetary profit,

399
00:25:31,190 --> 00:25:34,880
it's totally obvious that of course we're
going to be interested in the data and

400
00:25:34,881 --> 00:25:38,320
where we're going to be highly
interested to know, well, you know, if,

401
00:25:38,340 --> 00:25:40,460
if I'm gonna Invest into
that particular option,

402
00:25:40,461 --> 00:25:44,000
what's the probability of success and
so on. How much am I going to achieve?

403
00:25:44,001 --> 00:25:48,260
And this is precisely the kind of mindset
that effective altruism tries to apply

404
00:25:48,261 --> 00:25:50,360
to the domain of altruism
and charity as well.

405
00:25:51,590 --> 00:25:53,780
Let's look at HIV for instance.

406
00:25:53,781 --> 00:25:57,890
So this is data from the World
Health Organization have like various

407
00:25:57,950 --> 00:26:02,090
interventions that a one could
go for antiretroviral therapy,

408
00:26:02,091 --> 00:26:06,380
condom distribution treatment,
and then uh,

409
00:26:06,381 --> 00:26:11,381
at the bottom prevention through education
for high risk groups and mass media

410
00:26:11,661 --> 00:26:12,494
education.

411
00:26:12,710 --> 00:26:17,710
And you can see the estimated the
impacts based on a randomized controlled

412
00:26:17,751 --> 00:26:22,720
trials in many cases can vary. It can vary
a lot. As you can also see by like the,

413
00:26:22,721 --> 00:26:26,280
the varying shades of the bars.
Um,

414
00:26:26,570 --> 00:26:31,160
the uncertainty is also quite
huge. So these traits there, um,

415
00:26:31,220 --> 00:26:33,800
and the faint depart that
represents the uncertainty,

416
00:26:33,801 --> 00:26:38,090
the intervals that we should probably
rationally have based on the studies.

417
00:26:38,240 --> 00:26:42,770
So the uncertainty is huge, but we can
also see that despite this uncertainty,

418
00:26:42,771 --> 00:26:46,060
which should of course factor in
the expect, the differences in,

419
00:26:46,061 --> 00:26:48,210
in impact are still enormous.

420
00:26:48,660 --> 00:26:53,010
And so if we follow the initial thought
experiment of the firefighter and the

421
00:26:53,011 --> 00:26:55,920
greed that the numbers
count in altruism as well,

422
00:26:56,370 --> 00:27:01,370
then of course it's crucial to factor
this information in when making donation

423
00:27:02,191 --> 00:27:03,024
choices.

424
00:27:05,490 --> 00:27:10,070
Another example is MOLAP malaria
prevention and uh, this is actually a,

425
00:27:10,120 --> 00:27:14,280
a cause area or a sub cause area that does
particularly well, tends to do better,

426
00:27:14,281 --> 00:27:16,680
at least at the moment. I mean
this can also change, you know,

427
00:27:16,681 --> 00:27:20,490
based on the situation on the ground
changing the available evidence,

428
00:27:20,520 --> 00:27:21,850
changing a bout.

429
00:27:21,870 --> 00:27:25,710
But malaria prevention is a standard
intervention that's currently recommended

430
00:27:25,711 --> 00:27:30,711
by effective altruists organizations in
the space of a world poverty and global

431
00:27:30,991 --> 00:27:35,550
health. So with organizations such
as the against malaria foundation,

432
00:27:35,551 --> 00:27:37,110
you can distribute,
purchase,

433
00:27:37,111 --> 00:27:42,111
distribute one bed net for just $5 and
this is going to protect two people that

434
00:27:44,971 --> 00:27:48,840
can then sleep under these nets.
And this offers really strong protection.

435
00:27:48,841 --> 00:27:49,180
Of course,

436
00:27:49,180 --> 00:27:53,880
most infections happen at night when
when people are sleeping and uh,

437
00:27:53,940 --> 00:27:58,170
yes, in many, many randomized,
dozens of randomized control trials,

438
00:27:58,171 --> 00:28:01,590
which is really good evidence.
I mean this is not physics of course,

439
00:28:01,680 --> 00:28:05,640
but it's not the hard science by
many standards. But, uh, it's,

440
00:28:05,700 --> 00:28:08,260
I mean this is backed by
really good evidence, uh,

441
00:28:08,280 --> 00:28:11,850
comparatively really good
evidence in that space. Um,

442
00:28:12,870 --> 00:28:15,510
and so yeah, of course, if
you scale up these numbers,

443
00:28:15,511 --> 00:28:20,430
if you donate 100 k a then you
can protect for decay people.

444
00:28:21,360 --> 00:28:26,100
That means many villages or like my whole
football stadium say and donating 100K

445
00:28:26,280 --> 00:28:30,390
is quite easy for most people
actually in their western,

446
00:28:30,680 --> 00:28:34,860
in rich societies. I mean, you
don't need to donate 100 k one goal,

447
00:28:34,880 --> 00:28:37,500
but you can maybe if you
donate five k a year,

448
00:28:37,710 --> 00:28:42,510
well then you're going to get up there.
If you donate 10 k a year anyway. Um,

449
00:28:42,550 --> 00:28:46,740
and so it's quite amazing
what we can actually do. Even
just justice individuals.

450
00:28:47,350 --> 00:28:47,850
Um,

451
00:28:47,850 --> 00:28:52,850
the estimated cost per life year
saved is just $150 with that kind of

452
00:28:54,601 --> 00:28:58,860
intervention. So that's pretty amazing
with an, with a donation of $150,

453
00:28:58,861 --> 00:29:03,750
that doesn't affect many or most
people in rich societies at all.

454
00:29:04,590 --> 00:29:08,850
We can actually give a poor person
one more health for Year of life.

455
00:29:09,390 --> 00:29:13,020
And then of course, yeah, we can calculate
of course based on that, the cost,

456
00:29:13,021 --> 00:29:16,440
the average cost of saving a whole
life. Now, of course, strictly speaking,

457
00:29:16,441 --> 00:29:19,260
we can't save a life at the moment
because everybody is going to die.

458
00:29:19,830 --> 00:29:24,290
So we'll need a, the abolition of
aging there through biotechnology. Um,

459
00:29:24,750 --> 00:29:27,050
but what health care
economists do is to stay.

460
00:29:27,090 --> 00:29:30,710
They'd talk about what the
cost of one life saved, uh,

461
00:29:30,711 --> 00:29:33,960
as equivalent to saving
30 years of healthy life.

462
00:29:33,961 --> 00:29:37,560
So that's usually what health
care economists mean when
they say saving a life.

463
00:29:37,561 --> 00:29:42,040
So 30 years by the, yeah, this is
the rough cost here for one year,

464
00:29:42,100 --> 00:29:45,150
one healthy year of life.
Um,

465
00:29:45,250 --> 00:29:49,600
there are various organizations doing
sort of in depth charity research, um,

466
00:29:49,810 --> 00:29:52,180
collecting data, analyzing it, um,

467
00:29:52,810 --> 00:29:55,900
doing the relevant calculations and uh,
yeah,

468
00:29:55,901 --> 00:30:00,790
the leading organization currently there
is gift well Gif world at work and uh,

469
00:30:00,880 --> 00:30:04,480
yes they are extremely transparent. You
can check out all of their research, um,

470
00:30:04,750 --> 00:30:08,290
on the website if you're interested
and they're coming up with top

471
00:30:08,291 --> 00:30:12,730
recommendations for where to donate
based on their calculations every year.

472
00:30:13,060 --> 00:30:17,690
And of course there's some fluctuations
if the evidence changes. Um,

473
00:30:18,160 --> 00:30:19,000
generally speaking,

474
00:30:19,001 --> 00:30:22,930
global health seems to be
a really promising cause
area because there are these

475
00:30:22,931 --> 00:30:27,880
huge medical short run benefits.
And then as some studies suggest,

476
00:30:27,940 --> 00:30:32,440
of course there are also tend to
be long run societal benefits. Um,

477
00:30:32,470 --> 00:30:37,180
it's our one randomized controlled trial
suggested that kids that were able to

478
00:30:37,181 --> 00:30:39,850
sleep under bed nets and
we're protected from malaria,

479
00:30:40,050 --> 00:30:42,160
laid there on 10th tended to earn,

480
00:30:42,650 --> 00:30:46,450
they missed much for your school days
and later on for instance tended to earn

481
00:30:46,451 --> 00:30:51,451
20% more than kids that didn't have
the privilege to sleep under these bent

482
00:30:51,550 --> 00:30:54,310
nets.
And another advantage is that,

483
00:30:54,550 --> 00:30:58,010
I mean these are some worries that are
often discussed in these debates where

484
00:30:58,011 --> 00:30:58,361
I'll,
you know,

485
00:30:58,361 --> 00:31:03,361
but in terms of development aid of course
there have been many failures as well.

486
00:31:04,300 --> 00:31:09,300
So many people are actually rightly
so quite cynical about the impact of

487
00:31:09,790 --> 00:31:13,420
development aid. And indeed there
have been many failures. But I'd say,

488
00:31:13,421 --> 00:31:16,930
well that's just an additional argument
for effective altruism for like being

489
00:31:16,931 --> 00:31:20,800
serious about the data and
trying to figure out what
actually helps and what may

490
00:31:20,801 --> 00:31:25,240
harm people. So even if we believe
that's sort of all of the uh,

491
00:31:25,270 --> 00:31:26,230
development aid,

492
00:31:26,290 --> 00:31:29,590
maybe also the state sponsor development
aid that's coming from western

493
00:31:29,591 --> 00:31:34,000
societies was like maybe net
neutral or maybe even net negative.

494
00:31:34,270 --> 00:31:35,710
Just statistically speaking,

495
00:31:35,711 --> 00:31:38,650
there must be at least some
interventions that are positive, right?

496
00:31:38,651 --> 00:31:42,250
So if we assume that all
of the interventions that
we've sponsored in terms of

497
00:31:42,251 --> 00:31:44,620
development aid where like net neutral,

498
00:31:45,100 --> 00:31:48,400
then probably there's some distribution
where some interventions are like

499
00:31:48,401 --> 00:31:51,760
harmful, many are neutral
ish and some are really good.

500
00:31:52,210 --> 00:31:55,240
And what effective altruism is
about is sort of trying to find an,

501
00:31:55,241 --> 00:31:58,970
isolate the really good ones and
scale them up. And of course in our,

502
00:31:58,980 --> 00:32:03,640
and the advantage of working in health
is that this seems to be like maybe the

503
00:32:03,641 --> 00:32:05,740
paradigm example of a universal good.

504
00:32:05,800 --> 00:32:10,800
I mean even if people's
cultural preferences vary a
lot and people's conceptions

505
00:32:11,111 --> 00:32:13,600
of the good life very
a lot. Well, you know,

506
00:32:13,601 --> 00:32:16,780
everybody needs to be healthy in
order to achieve any other goal.

507
00:32:16,960 --> 00:32:20,680
So this is kind of a, of a universal
goats that seems safe to promote.

508
00:32:22,100 --> 00:32:24,220
Um,
now taking this further,

509
00:32:24,280 --> 00:32:28,480
what about uncertainty
and risk? I mean, yes,

510
00:32:28,481 --> 00:32:30,610
it's totally legitimate
to ask. Well, you know,

511
00:32:30,611 --> 00:32:35,611
what's the probability that the certain
antipoverty intervention fails to have

512
00:32:35,801 --> 00:32:36,760
the intended effect?

513
00:32:36,761 --> 00:32:40,790
Or maybe there's even a probability
that it will have a harmful effect. Uh,

514
00:32:41,090 --> 00:32:45,830
and of course we should factor
that in as well. And we can start,

515
00:32:45,890 --> 00:32:46,250
you know,

516
00:32:46,250 --> 00:32:51,250
at least in theory to address that
sort of issue by modifying the initial

517
00:32:51,441 --> 00:32:52,760
thought experiments. I mean, what,

518
00:32:53,480 --> 00:32:57,740
what if you have like a 50% probability
to save a drowning child, you know,

519
00:32:57,741 --> 00:33:01,700
you know that maybe you're not going
to make it in time, time is short.

520
00:33:01,850 --> 00:33:05,390
Maybe the child will still die despite
their effort. I mean, in that case,

521
00:33:05,391 --> 00:33:06,520
if the situation is,
you know,

522
00:33:06,530 --> 00:33:11,060
either not do anything or do something
and have a 50% chance of success or even

523
00:33:11,061 --> 00:33:15,260
maybe just a 10% chance of saving their
life, we will probably still say, yeah,

524
00:33:15,261 --> 00:33:17,840
it's worth it.
And then of course we can further,

525
00:33:17,870 --> 00:33:20,420
we can introduce further
complications while if we have,

526
00:33:20,421 --> 00:33:23,510
let's say an 80% chance to save the life,

527
00:33:23,810 --> 00:33:26,510
but the 1% chance to
cause harm in some way.

528
00:33:26,690 --> 00:33:30,050
Maybe there are other children in the
round and maybe we would, whatever,

529
00:33:30,051 --> 00:33:33,830
you know, unintentionally push a
child into the lake or whatever.

530
00:33:33,831 --> 00:33:38,300
I mean complications could happen
with harmful unintended side effects.

531
00:33:38,900 --> 00:33:40,970
And then f and then we can try and reason.
Okay.

532
00:33:40,971 --> 00:33:44,810
Under what circumstances
with wild probability
distributions would we still go

533
00:33:44,811 --> 00:33:47,360
for it?
And uh,

534
00:33:47,390 --> 00:33:52,100
we can also consider dilemmas like what
if we have a 10% chance to save 100

535
00:33:52,101 --> 00:33:56,390
lives versus a 100% chance to save five?

536
00:33:57,020 --> 00:34:00,630
What would you do? What
should you do? And um,

537
00:34:01,820 --> 00:34:03,320
what are the thresholds maybe?

538
00:34:03,380 --> 00:34:08,150
So maybe we could start with a 100%
probability to save a certain number,

539
00:34:08,180 --> 00:34:12,440
go down to 90% and compare it to the,
to another post decision possibility.

540
00:34:13,220 --> 00:34:18,220
And a standard framework for addressing
this kind of situation is the expected

541
00:34:18,651 --> 00:34:22,940
value framework. So just taking
the probability times the stakes.

542
00:34:23,150 --> 00:34:28,150
So here at 10% chance times
100 versus a 100% chance.

543
00:34:28,801 --> 00:34:33,801
So one times five and the expected value
here would recommend the first option.

544
00:34:34,280 --> 00:34:38,180
So all 0.1 times 100 equals 10.

545
00:34:38,660 --> 00:34:40,070
But of course we could ask,
well,

546
00:34:40,071 --> 00:34:43,760
but shouldn't we be risk
averse at least a bit?

547
00:34:44,030 --> 00:34:46,070
Doesn't risk aversion makes sense.

548
00:34:46,340 --> 00:34:51,340
So if we have a one 100% probability to
save five versus just a 10% chance to

549
00:34:51,951 --> 00:34:56,720
save a hundred, we might feel uneasy
about going for the 10% option.

550
00:34:57,350 --> 00:35:00,050
I mean, one replies, if
that's not a one shot game,

551
00:35:00,051 --> 00:35:03,860
but if many people do the same or
if you do the same many times over,

552
00:35:04,190 --> 00:35:07,250
well then you know, uh, due
to the law of large numbers,

553
00:35:07,280 --> 00:35:09,920
of course you are going to
save more people. In fact,

554
00:35:09,921 --> 00:35:14,510
if you go for the highest expected
value, uh, so that's one reply. Um,

555
00:35:14,540 --> 00:35:16,310
but yeah,
even if it's one shot,

556
00:35:16,311 --> 00:35:21,020
I think a pretty good case can be made
here for going for risk neutrality.

557
00:35:21,021 --> 00:35:24,440
So expected value as opposed
to some risk averse function.

558
00:35:24,740 --> 00:35:29,150
I think our risk aversion comes from
our being accustomed to thinking about

559
00:35:29,151 --> 00:35:29,531
decisions,

560
00:35:29,531 --> 00:35:34,531
situations that are about earning money
or making money for personal utility.

561
00:35:35,870 --> 00:35:40,530
And if it's, if, if, if we're about
making money for personal utility,

562
00:35:40,531 --> 00:35:43,380
then risk aversion seems
to make perfect sense. Why?

563
00:35:43,980 --> 00:35:47,310
Because money tends to have
diminishing marginal utility,

564
00:35:47,311 --> 00:35:49,560
at least in terms of personal gain.

565
00:35:49,890 --> 00:35:54,810
So the first $10,000 that you earn
per year are the hugely important.

566
00:35:54,840 --> 00:35:59,580
They enable you to survive the second
20 k that you add our bid list and so on

567
00:35:59,581 --> 00:36:00,900
and so forth.
So that's,

568
00:36:00,910 --> 00:36:03,630
that seems to be the dynamic
when it comes to personal gain.

569
00:36:04,050 --> 00:36:08,850
But that same argument doesn't seem to
apply when it comes to altruism because

570
00:36:08,851 --> 00:36:12,690
there, yeah, if you save the first
life, great. If you safe the second one,

571
00:36:12,691 --> 00:36:16,560
it's not like the second one
is less valuable. It's similar,

572
00:36:16,561 --> 00:36:19,890
really valuable to the
person, um, themselves.

573
00:36:20,280 --> 00:36:22,740
And so if we're being altruistic,
I think we should not,

574
00:36:23,010 --> 00:36:24,520
we should not assume that
they're already made,

575
00:36:24,560 --> 00:36:26,490
that there's diminishing
marginal utility there.

576
00:36:26,790 --> 00:36:30,720
And so this kind of argument for
risk aversion does no longer apply.

577
00:36:31,080 --> 00:36:35,440
So I would probably argue for going
for expected value maximization.

578
00:36:35,441 --> 00:36:39,780
And when it comes to altruism and
taking the probability times the stakes,

579
00:36:41,370 --> 00:36:46,370
and this leads to a concept that a
effective altruists call hits based

580
00:36:46,381 --> 00:36:50,250
philanthropy or hit spaced giving.
I mean, let's consider an example.

581
00:36:50,251 --> 00:36:53,070
In the 20th century.
It's again from the health area,

582
00:36:53,490 --> 00:36:58,490
smallpox killed more than 300 million
people up to its eradication in the 70s so

583
00:37:00,050 --> 00:37:03,720
a huge evil in terms of the consequences.
That's more than all wars,

584
00:37:03,721 --> 00:37:07,650
all genocides, all political
violence and famines combined.

585
00:37:08,430 --> 00:37:10,620
So an insane, uh, victim count.

586
00:37:11,190 --> 00:37:16,190
But we were able to beat it and have
since saved an estimated 60 to 120 some

587
00:37:18,511 --> 00:37:22,320
uncertainty. There are 60 to 120
million lives since the eradication.

588
00:37:22,860 --> 00:37:27,860
And so there's a pretty interesting and
complex story behind how that happened

589
00:37:28,590 --> 00:37:32,280
and x untie it also seems quite unlikely
to succeed in our many people are

590
00:37:32,281 --> 00:37:33,114
saying now it's,

591
00:37:33,180 --> 00:37:37,020
these are crazy plans snuck on to
succeed in terms of smallpox eradication.

592
00:37:37,020 --> 00:37:40,590
But the point is if
the stakes are so high,

593
00:37:40,591 --> 00:37:43,740
then even if the success
chance is very low,

594
00:37:43,770 --> 00:37:48,390
the expected value can still be enormous
and it can be extremely worthwhile to

595
00:37:48,391 --> 00:37:53,070
pursue something that's most likely
to fail. So you're pursuing a plan,

596
00:37:53,071 --> 00:37:56,370
deliberately pursuing an altruistic
plan that's most likely to fail,

597
00:37:56,371 --> 00:37:59,370
maybe only has a 5%
success success chance,

598
00:37:59,730 --> 00:38:03,510
but because the stakes are so high,
the expected value can still be enormous.

599
00:38:04,100 --> 00:38:07,020
And so that's the concept
of sort of headspace giving,

600
00:38:07,080 --> 00:38:09,600
trying to do maybe many things at once,

601
00:38:09,601 --> 00:38:13,890
trying to have various individuals and
various groups or organizations work on

602
00:38:13,891 --> 00:38:18,891
such situations of low probability of
success and high stakes of course the for

603
00:38:20,241 --> 00:38:22,860
profit analogy, again, this,
his headspace investment.

604
00:38:22,861 --> 00:38:27,840
I mean you can do that with
your investment portfolio.
Uh, it's if you have,

605
00:38:27,900 --> 00:38:32,280
if you're pursuing many such situations
or opportunities at once with like low

606
00:38:32,281 --> 00:38:35,890
success probability but high stakes,
that's also a form of hedging.

607
00:38:35,890 --> 00:38:38,410
Needless to say,
due to law of large numbers again,

608
00:38:38,890 --> 00:38:41,240
and this concept can be transferred to,

609
00:38:41,360 --> 00:38:44,500
to philanthropy in very productive ways.

610
00:38:45,220 --> 00:38:47,740
Now some applications,
um,

611
00:38:48,220 --> 00:38:52,990
off headspace giving are to be found
specifically in the course of the area of

612
00:38:52,991 --> 00:38:55,420
global catastrophic risks.

613
00:38:56,170 --> 00:39:00,460
Some of them being the worst case climate
change scenarios, which might be like,

614
00:39:00,840 --> 00:39:01,090
you know,

615
00:39:01,090 --> 00:39:06,090
sort of really extreme global chaos
and maybe even some extinction risk.

616
00:39:06,701 --> 00:39:09,610
But that seems,
I mean climate change seems pretty bad,

617
00:39:10,090 --> 00:39:13,840
but the absolute worst case
scenario seem pretty unlikely,

618
00:39:14,740 --> 00:39:16,720
but still the stakes are enormous.

619
00:39:16,721 --> 00:39:20,230
Maybe nuclear war seems
unlikely at least on a,

620
00:39:20,231 --> 00:39:24,130
on an extinction level
scale. But the stakes, again,

621
00:39:24,460 --> 00:39:28,690
extreme buyer security,
maybe unlikely,

622
00:39:28,691 --> 00:39:30,430
but it's not clear.
I mean I'm,

623
00:39:30,790 --> 00:39:34,720
I'm also not claiming that with
these risks to situation is necessary

624
00:39:34,780 --> 00:39:38,980
necessarily, uh, off the sword
that the risk is low probability.

625
00:39:39,670 --> 00:39:41,320
I mean,
if the risk is high probability,

626
00:39:41,321 --> 00:39:43,570
then we should be addressing it
all the more. But the argument is,

627
00:39:43,571 --> 00:39:48,220
even if the risk is low probability
and even if our success chance of doing

628
00:39:48,221 --> 00:39:51,040
something to mitigate
the risk is also low,

629
00:39:51,610 --> 00:39:56,610
the expected value of trying hard can
still be pretty high and a further cost

630
00:39:56,751 --> 00:40:00,880
area that some effective altruists have
been thinking about and researching is,

631
00:40:00,940 --> 00:40:01,330
um,

632
00:40:01,330 --> 00:40:06,330
artificial intelligence opportunities
and risks from a possible transition to a

633
00:40:08,020 --> 00:40:12,200
superhuman intelligence
and how to maybe steer, um,

634
00:40:12,430 --> 00:40:15,700
such a transition to
maximize the benefits.

635
00:40:15,701 --> 00:40:20,350
And in order to conclude,
let's zoom in on this cause area briefly.

636
00:40:20,930 --> 00:40:25,710
Um, I should maybe premise it by
saying that I'm not myself and a,

637
00:40:25,720 --> 00:40:27,220
an AI expert.

638
00:40:27,340 --> 00:40:31,960
I'm a trained philosopher and a
half done a lot of entrepreneurship.

639
00:40:32,470 --> 00:40:37,470
And if you're interested in what the AI
experts in effective altruism have to

640
00:40:37,871 --> 00:40:40,420
say more specifically, then, uh,

641
00:40:40,690 --> 00:40:44,110
I'd encourage you to check out the
organizations and the websites that I'll

642
00:40:44,111 --> 00:40:47,900
mention at the end. So this
will just be a brief, um,

643
00:40:48,280 --> 00:40:51,310
intro and philosophical overview.
Um,

644
00:40:51,910 --> 00:40:56,890
so many technical experts do seem to
predict the emergence of artificial

645
00:40:56,891 --> 00:40:57,940
superintelligence.

646
00:40:57,970 --> 00:41:02,290
So machines that outperform
humans in literally every,

647
00:41:02,740 --> 00:41:05,860
a domain of cognitive interest.
Uh,

648
00:41:05,890 --> 00:41:10,240
later this century that seems to be a
prediction that many technical experts are

649
00:41:10,570 --> 00:41:15,280
willing to make.
And of course if that materializes,

650
00:41:15,281 --> 00:41:20,281
it's likely to be a change that's more
disruptive on a global scale than the

651
00:41:20,681 --> 00:41:24,190
evolutionary sort of ape to
human transition has been.

652
00:41:24,460 --> 00:41:27,010
And this has been an extremely
disruptive transition of course.

653
00:41:27,011 --> 00:41:30,370
So the transition from apelike
brains to human brains,

654
00:41:30,880 --> 00:41:32,650
I mean up to the point where you know,

655
00:41:32,740 --> 00:41:37,740
the goal achievement and the very survival
of apes of chimps depends a lot more

656
00:41:38,750 --> 00:41:41,120
on us humans than it depends on them.

657
00:41:41,720 --> 00:41:46,720
So if artificial superintelligence or
super intelligence is emerge and I mean,

658
00:41:47,181 --> 00:41:50,540
you know, there are various scenarios,
maybe there will also be like a,

659
00:41:50,550 --> 00:41:53,510
a corresponding enhancement
of our human brains.

660
00:41:53,660 --> 00:41:58,010
There is scenarios there that are
possible. But I mean then it's, yes,

661
00:41:58,011 --> 00:42:03,011
it seems quite likely that our goal
achievement and our very survival will

662
00:42:03,381 --> 00:42:08,150
depend more on on the super intelligences.
Then it would depend on us.

663
00:42:09,170 --> 00:42:11,960
And this raises the question well,
um,

664
00:42:12,830 --> 00:42:17,690
will these AI's goals be stably
aligned with our goals or not?

665
00:42:17,691 --> 00:42:21,690
And it seems that if they are,
if they are stable,

666
00:42:21,700 --> 00:42:22,790
aligned with our goals,

667
00:42:22,820 --> 00:42:27,560
then that could be the biggest opportunity
for humanity ever. I mean, of course,

668
00:42:27,620 --> 00:42:28,430
uh,

669
00:42:28,430 --> 00:42:33,230
technological progress has
brought humanity a great
many benefits and this could

670
00:42:33,231 --> 00:42:35,840
sort of be the ultimate benefit.
Um,

671
00:42:36,020 --> 00:42:40,550
maybe the last one mentioned we need to
make and in principle it could solve all

672
00:42:40,551 --> 00:42:44,480
our problems because the tool that we
have been using of course in history to

673
00:42:44,481 --> 00:42:46,250
solve problems is our intelligence.

674
00:42:46,610 --> 00:42:51,590
And if we end up with a human with a
superhuman intelligence that's beneficial,

675
00:42:52,130 --> 00:42:56,780
uh, meaning, uh, that's pursuing
goals that are identical with ours,

676
00:42:57,580 --> 00:43:02,030
uh, goals that we'd consider good and
valuable, uh, then that could be, yes,

677
00:43:02,300 --> 00:43:05,210
really the biggest
opportunity for humanity ever.

678
00:43:05,660 --> 00:43:08,880
But there could also be
some risks, namely if, um,

679
00:43:09,140 --> 00:43:13,070
the superintelligence his goals are not
aligned or not stapling aligned. I mean,

680
00:43:13,071 --> 00:43:17,330
it could also be the case that they are
aligned at first, but then sort of, uh,

681
00:43:17,990 --> 00:43:20,960
because there's further
developments become misaligned,

682
00:43:21,020 --> 00:43:23,990
that could also be a problem.
So that could also be the risks,

683
00:43:24,110 --> 00:43:28,430
the risks that we're going to be faced
with a kind of super intelligent power

684
00:43:28,910 --> 00:43:31,910
that's not stable goal aligned with us.

685
00:43:31,970 --> 00:43:35,630
So it seems that this transition
could be extremely disruptive.

686
00:43:35,670 --> 00:43:40,010
The s the stakes could be enormous,
literally the biggest stakes ever.

687
00:43:40,490 --> 00:43:44,750
And depending on whether, um,
goal alignment will be the case,

688
00:43:44,780 --> 00:43:48,230
the outcome could be extremely
good or quite dangerous.

689
00:43:49,070 --> 00:43:53,330
And that's basically the problem context
that many effective altruists have been

690
00:43:53,360 --> 00:43:57,200
thinking about.
Now from a sort of more philosophical,

691
00:43:57,201 --> 00:44:01,500
non technical point of view
also in the public debate, uh,

692
00:44:01,580 --> 00:44:06,580
some people have been like dismissing the
whole argument for various reasons and

693
00:44:07,760 --> 00:44:11,120
well their common denominator is
often that they think that, well,

694
00:44:11,121 --> 00:44:14,630
the probability either of these
scenarios materializing is super low.

695
00:44:15,110 --> 00:44:20,030
And even if the probability were high,
then the probability would be low of, uh,

696
00:44:20,031 --> 00:44:24,020
are, are being successfully able
to steer the development in any,

697
00:44:24,021 --> 00:44:28,940
in any direction that we would
prefer. So yeah, some people say,

698
00:44:28,941 --> 00:44:31,040
well, the probability of, you know,

699
00:44:31,050 --> 00:44:35,130
any fears of goal misalignment
applying is extremely low.

700
00:44:35,660 --> 00:44:36,480
Um,

701
00:44:36,480 --> 00:44:40,740
and the probability of goal alignments
succeeding through sort of a strategic

702
00:44:40,741 --> 00:44:43,680
effort on our part is also extremely low.

703
00:44:44,160 --> 00:44:46,410
And from this day conclude that yeah,

704
00:44:46,411 --> 00:44:49,500
it's not a cause area worth
pursuing in any serious way,

705
00:44:50,070 --> 00:44:54,000
but sort of from a decision theoretic
and philosophical point of view, I'd say,

706
00:44:54,990 --> 00:44:58,380
well, um, even if even if
that's completely correct,

707
00:44:58,381 --> 00:45:00,000
if that were totally accurate,

708
00:45:00,500 --> 00:45:04,650
that let's say it were extremely unlikely
that superintelligence would emerge at

709
00:45:04,651 --> 00:45:07,260
all or like they're very
far into the future only.

710
00:45:07,770 --> 00:45:11,250
And if we also add that it's in any case,

711
00:45:11,251 --> 00:45:16,110
extremely improbable that we would sort
of succeed with a strategic effort to

712
00:45:16,111 --> 00:45:20,310
try and maximize the probability
of an awesome, a utopian outcome,

713
00:45:20,760 --> 00:45:22,620
even if both of these points apply,

714
00:45:23,040 --> 00:45:27,450
well the expected value could
still be enormous. So it even,

715
00:45:27,451 --> 00:45:31,890
even even knowing that our deliberate
effort at steering the development is

716
00:45:32,220 --> 00:45:34,290
unlikely to succeed,
even if that were the case,

717
00:45:34,620 --> 00:45:38,580
the expected value of trying hard
could still be high. So that's I think,

718
00:45:38,581 --> 00:45:42,990
an important point to make from a sort
of decision theoretic perspective.

719
00:45:44,130 --> 00:45:48,030
Um,
and that brings me to the second part.

720
00:45:48,031 --> 00:45:50,160
Last slide already.
Uh,

721
00:45:50,161 --> 00:45:53,190
what I've tried to do in this
talk is give a brief intro,

722
00:45:53,191 --> 00:45:58,191
like a justification for the spirit
of ea and an overview of like main

723
00:45:58,621 --> 00:46:02,700
strategies and cause areas pursuit.
And last but not least,

724
00:46:02,701 --> 00:46:06,930
I'd like to mention some organizations
and their websites if you're interested

725
00:46:06,940 --> 00:46:11,590
in checking out the material in
greater detail. I uh, did mention Gif,

726
00:46:11,591 --> 00:46:12,870
well,
um,

727
00:46:12,930 --> 00:46:17,850
the charity research think tank mainly
focusing on the cause area of world

728
00:46:17,851 --> 00:46:20,520
poverty in global health.
Then there's giving what we can,

729
00:46:20,880 --> 00:46:25,140
which is an organization where you can
sort of take a pledge to take this 10%

730
00:46:25,170 --> 00:46:28,830
pledge the dimension than several thousand
people have already taken this pledge

731
00:46:29,430 --> 00:46:33,210
to donate 10% to the most effective
cause areas they can find.

732
00:46:33,720 --> 00:46:38,720
There's $80,000 the organization providing
career advice in the German area.

733
00:46:39,390 --> 00:46:43,830
There's the effective altruism
foundation doing various things.

734
00:46:43,831 --> 00:46:47,490
Also providing some career
advice and donation advice.

735
00:46:48,360 --> 00:46:51,090
And for the cause area of
Ai Safety specifically,

736
00:46:51,570 --> 00:46:56,370
there's the Berlin based Foundational
Research Institute whose research you can

737
00:46:56,371 --> 00:46:59,870
check out or a the u s based,
um,

738
00:47:00,450 --> 00:47:05,340
machine intelligence research institute
mirror, which you may have heard of. Um,

739
00:47:05,520 --> 00:47:10,200
and uh, yes, so as I said,
I'm a philosopher by training,

740
00:47:10,290 --> 00:47:10,980
but uh,

741
00:47:10,980 --> 00:47:15,980
you can find the computer science and
machine learning literature by people who

742
00:47:16,291 --> 00:47:20,580
believe that this is an important cause
area, um, on these websites for instance.

743
00:47:20,760 --> 00:47:21,810
And to conclude,

744
00:47:22,170 --> 00:47:25,860
so this is sort of the animating spirit
behind all of these organizations are

745
00:47:25,861 --> 00:47:29,920
keen awareness that this is probably
in the very real and urgent sense.

746
00:47:29,921 --> 00:47:32,080
The situation we are in globally.

747
00:47:32,230 --> 00:47:35,530
It may not feel that way evolutionarily
because we are still running on sort of

748
00:47:35,531 --> 00:47:40,531
stone age brains and emotionally we have
a hard time grasping the situation of a

749
00:47:41,171 --> 00:47:44,020
global village, which is really
an interconnected global village.

750
00:47:44,020 --> 00:47:45,280
That's a historical first.

751
00:47:45,740 --> 00:47:47,920
And so emotionally we're not
really able to grasp that.

752
00:47:47,921 --> 00:47:51,730
But intellectually we are,
and it's probably through
that this is our situation,

753
00:47:52,000 --> 00:47:55,260
but it's also true that having a
beer from time to time is supreme.

754
00:47:55,261 --> 00:48:00,040
We strategically important, we need
to minimize burnout risk and a, yeah,

755
00:48:00,041 --> 00:48:03,430
the effective altruist communities all
around the world are very welcoming.

756
00:48:04,030 --> 00:48:07,120
And there's also community
in, uh, in Zurich here.

757
00:48:07,660 --> 00:48:11,770
Pretty big community in Switzerland and
Germany. And Yeah, if you're interested,

758
00:48:11,771 --> 00:48:16,420
these are also just the kind of people
that like to go grab a beer and, um,

759
00:48:16,480 --> 00:48:19,810
and have a philosophical and
also technical discussions.

760
00:48:20,230 --> 00:48:21,430
And with that being said,

761
00:48:21,760 --> 00:48:24,820
thanks again for your interest and your
attention and I'm looking forward to

762
00:48:24,821 --> 00:48:26,470
your questions and comments.

763
00:48:27,460 --> 00:48:32,460
Thank [inaudible].

