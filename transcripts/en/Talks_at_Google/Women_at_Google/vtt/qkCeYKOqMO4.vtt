WEBVTT

1
00:00:08.250 --> 00:00:12.400
So hello to friends in the room.
Friends who I know are joining us virtually.

2
00:00:12.680 --> 00:00:16.580
I'm really excited that you're here and thank you Justin for having me.

3
00:00:16.581 --> 00:00:18.620
It's great to be here.
Um,

4
00:00:20.300 --> 00:00:25.010
this is part of a big bunch of research that I started some way or another 13

5
00:00:25.011 --> 00:00:28.400
years ago in some way or another in 1996 even.

6
00:00:28.880 --> 00:00:32.060
And I want to start by making the claim that architecture,
AI,

7
00:00:32.061 --> 00:00:34.880
and design are all connected.
Uh,

8
00:00:34.940 --> 00:00:39.940
these 60 these connections go back some 60 or 70 years to 1956 or even 1948.

9
00:00:42.380 --> 00:00:46.370
And newness seems to be central to how we talk about artificial intelligence

10
00:00:46.371 --> 00:00:51.290
today.
We say things like AI is the new black

11
00:00:54.370 --> 00:00:55.840
AI is the new Ui.

12
00:00:56.740 --> 00:00:57.220
<v 1>Okay.</v>

13
00:00:57.220 --> 00:01:00.880
<v 0>Artificial intelligence is the next digital frontier according to our friends at</v>

14
00:01:00.881 --> 00:01:01.714
Mckinsey.

15
00:01:02.850 --> 00:01:03.000
<v 1>Okay.</v>

16
00:01:03.000 --> 00:01:06.140
<v 0>Andrew Ng's stated that AI is the new electricity.</v>

17
00:01:06.930 --> 00:01:07.200
<v 1>Okay.</v>

18
00:01:07.200 --> 00:01:12.000
<v 0>But also AI is the new moon because he will be an online course that he's</v>

19
00:01:12.001 --> 00:01:15.550
running.
You need data for AI.

20
00:01:15.551 --> 00:01:18.040
So data is the new oil.

21
00:01:19.310 --> 00:01:19.790
<v 1>Yeah.</v>

22
00:01:19.790 --> 00:01:23.030
<v 0>The future of computers is the mind of a disembodied toddler head.</v>

23
00:01:24.300 --> 00:01:25.133
<v 1>Okay.</v>

24
00:01:26.050 --> 00:01:31.050
<v 0>And apparently I am also the new AI and I will spare you this,</v>

25
00:01:31.691 --> 00:01:35.560
uh,
what this says,
but it's really,
really sexist and really awful.

26
00:01:36.220 --> 00:01:39.340
And if you want to get an idea of how much the word new gets used when we talk

27
00:01:39.341 --> 00:01:43.450
about Ai,
you could look at something like this MIT technology review article.
Um,

28
00:01:43.570 --> 00:01:48.060
I just highlighted the word,
knew where I could find it.
And,
uh,

29
00:01:48.100 --> 00:01:50.740
there were six mentions and the first two paragraphs,
of course,

30
00:01:50.741 --> 00:01:54.010
I think one of them was New York.
So I don't think that that actually counts.

31
00:01:55.690 --> 00:01:59.920
And this gentleman was a part of a conversation about tech moguls declaring an

32
00:01:59.921 --> 00:02:02.440
era of artificial intelligence.

33
00:02:05.110 --> 00:02:08.290
But AI isn't the new because AI isn't new.

34
00:02:08.960 --> 00:02:09.430
<v 1>Okay.</v>

35
00:02:09.430 --> 00:02:13.030
<v 0>Here's how I knew.
It's not 1955,
uh,
John McCarthy,</v>

36
00:02:13.031 --> 00:02:16.840
the founder of the Stanford Ai Lab.
And before that,

37
00:02:16.841 --> 00:02:20.130
the MIT Ai lab,
um,
referred to Artefill,

38
00:02:20.680 --> 00:02:24.100
coined the term artificial intelligence to me and making machines do things that

39
00:02:24.101 --> 00:02:26.170
would require intelligence if done by man.

40
00:02:27.040 --> 00:02:27.820
<v 1>Okay.</v>

41
00:02:27.820 --> 00:02:32.820
<v 0>In 1956 he brought together a group of researchers at Dartmouth College in New</v>

42
00:02:33.550 --> 00:02:36.450
Hampshire and they spent the summer hashing out what they thought the platform

43
00:02:36.460 --> 00:02:41.460
of research on AI should be and it included things like neural nets and it

44
00:02:41.471 --> 00:02:46.200
included,
um,
oh,
what did,
what does he say?
He says an attempt will be made to,
uh,

45
00:02:46.270 --> 00:02:50.020
find how to make machines use language,
form abstractions and concepts,

46
00:02:50.380 --> 00:02:53.860
self kinds of problems,
no reserve for humans and improve themselves.

47
00:02:54.130 --> 00:02:57.850
We think that a significant advance can be made in one or more of these problems

48
00:02:57.851 --> 00:03:01.390
if a carefully selected of scientists can work together on it for a summer.

49
00:03:01.930 --> 00:03:04.720
1956,
um,

50
00:03:04.780 --> 00:03:08.390
herb Simon Allan Newell and JC shot,
uh,
uh,
Simon annual,

51
00:03:08.440 --> 00:03:11.420
very important to Carnegie Mellon's history,
um,

52
00:03:11.440 --> 00:03:15.910
wrote in 1958 that they pretty much figured out this problem of modeling the

53
00:03:15.911 --> 00:03:18.220
human brain in computing.

54
00:03:18.221 --> 00:03:21.280
They said intuition into salient and learning are no longer exclusive

55
00:03:21.281 --> 00:03:23.680
possessions of humans.
Any large scale,

56
00:03:23.681 --> 00:03:27.670
high speed computer can be mean to pro and made to sit with them.
Also,

57
00:03:27.671 --> 00:03:31.390
they believed that by the 1960s they'd have it figured it out.
Figured out.

58
00:03:32.850 --> 00:03:33.180
<v 1>Yeah.</v>

59
00:03:33.180 --> 00:03:35.940
<v 0>And of course Marvin Minsky,
who in 1961 said,</v>

60
00:03:35.941 --> 00:03:38.700
I believe that we are on the threshold of an era that will be strongly

61
00:03:38.701 --> 00:03:42.690
influenced and quite possibly dominated by intelligent problem solving machines.

62
00:03:45.070 --> 00:03:47.080
Prodo machine learning.
Um,

63
00:03:47.081 --> 00:03:51.830
you might find in the world of perceptrons in 1958,
um,

64
00:03:51.880 --> 00:03:56.290
the New York Times published an article talking about this new navy device that

65
00:03:56.291 --> 00:04:00.640
learns by doing and said that the navy revealed the embryo of an electronic

66
00:04:00.641 --> 00:04:04.300
computer today that it expects will be able to walk,
talk,
see right,

67
00:04:04.330 --> 00:04:07.150
reproduce itself and be conscious of its existence.

68
00:04:07.720 --> 00:04:11.590
The navy said the perceptron we'd be the first non living mechanism capable of

69
00:04:11.591 --> 00:04:12.071
receiving,

70
00:04:12.071 --> 00:04:15.970
recognizing and identifying its surroundings without any human human training or

71
00:04:15.971 --> 00:04:16.804
control.

72
00:04:18.040 --> 00:04:18.340
<v 1>Okay.</v>

73
00:04:18.340 --> 00:04:22.930
<v 0>So all of these ideas are really old.
Um,
they're older than me by far,</v>

74
00:04:22.990 --> 00:04:27.850
and they're older than a lot of what we work on.
But in addition to this,

75
00:04:27.851 --> 00:04:31.810
I want to point out that AI and architecture are old friends and kind of

76
00:04:31.811 --> 00:04:32.980
cocreated each other.

77
00:04:32.981 --> 00:04:37.660
At a certain point in time that in ways that has very direct impact on the kind

78
00:04:37.661 --> 00:04:41.260
of engineering and design work that we do today and the problems that we focus

79
00:04:41.261 --> 00:04:45.910
on.
I'm going to introduce three of the characters in my book.

80
00:04:45.940 --> 00:04:49.360
Um,
artifice or architectural intelligence.

81
00:04:49.390 --> 00:04:53.140
The three people I'm going to introduce in this talk are Christopher Alexander,

82
00:04:53.470 --> 00:04:56.170
Cedric price and Negroponte.
Nicholas Negroponte,

83
00:04:56.440 --> 00:04:59.290
who here has heard of Christopher Alexander.

84
00:05:00.180 --> 00:05:00.630
<v 1>Okay.</v>

85
00:05:00.630 --> 00:05:03.030
<v 0>Couple.
Anyone?
Cedric price.</v>

86
00:05:03.470 --> 00:05:03.800
<v 1>Okay.</v>

87
00:05:03.800 --> 00:05:06.560
<v 0>Right on.
And Nicholas Negroponte.</v>

88
00:05:07.110 --> 00:05:07.380
<v 1>Okay.</v>

89
00:05:07.380 --> 00:05:08.490
<v 0>Couple more.
All right.</v>

90
00:05:08.730 --> 00:05:13.730
So they're all architects who worked very closely with technology and different

91
00:05:14.191 --> 00:05:15.270
kinds of ways,
some,

92
00:05:15.271 --> 00:05:18.360
so much so that you don't probably think of Nicholas Negroponte,

93
00:05:18.600 --> 00:05:21.450
the founder of the MIT media lab as an architect.

94
00:05:21.810 --> 00:05:24.540
And I'm going to talk about the ways that they worked with cybernetics and

95
00:05:24.541 --> 00:05:28.110
artificial intelligence to build new kinds of worlds and new kinds of approaches

96
00:05:28.111 --> 00:05:32.610
and ways that affect what we do today.
I'll start with Christopher Alexander.

97
00:05:33.910 --> 00:05:34.400
<v 1>Yeah.</v>

98
00:05:34.400 --> 00:05:35.480
<v 0>Christopher Alexander.</v>

99
00:05:35.481 --> 00:05:40.481
If you do know of his work and even if you don't buy me putting up this book

100
00:05:40.521 --> 00:05:41.354
right here,

101
00:05:41.360 --> 00:05:44.630
I've probably begun to bring him into something you've heard of before.

102
00:05:44.990 --> 00:05:48.830
He's written many,
many books.
He's a mathematician,
born in Vienna,

103
00:05:48.831 --> 00:05:52.940
raised in England,
um,
came to the United States in the 1960s.

104
00:05:52.941 --> 00:05:54.530
He's quite old but still alive.

105
00:05:54.560 --> 00:05:59.300
Living in England again was a professor at Berkeley and,
um,
he,
I,

106
00:05:59.330 --> 00:06:01.210
I'll mention three books today.
Uh,

107
00:06:01.220 --> 00:06:05.030
notes on the synthesis of form a pattern language and the timeless way of

108
00:06:05.031 --> 00:06:08.480
building notes on the synthesis of form was his,
uh,
his thesis.

109
00:06:08.481 --> 00:06:11.750
And if you've ever heard the idea of design being,
um,

110
00:06:11.810 --> 00:06:14.770
good fit or fitness,
um,
you know,

111
00:06:14.780 --> 00:06:19.780
trying to shim a table until it kind of works or putting a stable system in

112
00:06:20.151 --> 00:06:24.680
place.
That's where that idea comes from.
Those of you who've used patterns,

113
00:06:24.681 --> 00:06:28.010
whether in Ux or in software,
um,

114
00:06:28.011 --> 00:06:31.220
are are right in the lineage of Christopher Alexander.

115
00:06:31.250 --> 00:06:34.010
His idea of pattern languages have influenced this.

116
00:06:34.011 --> 00:06:35.630
And then his philosophy of design,

117
00:06:35.631 --> 00:06:37.760
which he outlined in the timeless way of building.

118
00:06:38.090 --> 00:06:41.240
If you've ever used a Wiki or you use agile processes,

119
00:06:41.690 --> 00:06:44.960
you have used something that,
um,
that he's done.

120
00:06:45.620 --> 00:06:49.460
My own research with Alexander started when I was reading notes on the synthesis

121
00:06:49.461 --> 00:06:54.050
of form a and this question of Ai came up when I realized in a footnote,

122
00:06:54.290 --> 00:06:58.100
he was referring to Shannon and a Kludge Hannon,

123
00:06:58.430 --> 00:07:02.210
John McCarthy and Marvin Minsky.
And I wondered,
why is this architect,

124
00:07:02.720 --> 00:07:07.470
what does he have to say about artificial intelligence in 1964,
um,

125
00:07:08.120 --> 00:07:10.370
13 years later,
I have a book about that

126
00:07:11.930 --> 00:07:14.900
he was interested in finding ways to break down design problems so they could be

127
00:07:14.901 --> 00:07:17.280
easily understood and design.
Um,

128
00:07:17.330 --> 00:07:22.330
anybody who has created tree diagrams in order to explain the function of a

129
00:07:23.270 --> 00:07:27.910
piece of software or a website is working in this lineage.
Um,

130
00:07:27.950 --> 00:07:28.371
in fact,

131
00:07:28.371 --> 00:07:32.720
this is the outputs of one of his programs from this as a something that's in

132
00:07:32.721 --> 00:07:36.950
the Berkeley archives that shows how you break down the elements of a design

133
00:07:36.951 --> 00:07:38.690
problem for how you design high waves.

134
00:07:40.400 --> 00:07:44.540
He also applied these ideas to the bart system in 1964.
This is,
um,

135
00:07:44.660 --> 00:07:48.900
390 requirements that he and his team outlined.
And,
uh,

136
00:07:48.930 --> 00:07:51.680
this is before bark was built.
Um,

137
00:07:51.710 --> 00:07:54.110
I'm not sure how many of these are in place today.

138
00:07:54.111 --> 00:07:56.090
I think we probably need new requirements.

139
00:07:56.091 --> 00:07:58.940
My ears are still ringing from the Bart ride.
I took this morning

140
00:08:01.550 --> 00:08:04.760
pattern,
language and timeless way of building.
I already mentioned briefly,

141
00:08:04.761 --> 00:08:07.820
but I want to tell you a little bit more about them.
Um,

142
00:08:07.910 --> 00:08:12.320
the pattern language as he puts it in,
it's not the only,
it's a pattern language.

143
00:08:12.380 --> 00:08:17.240
Um,
it's 253 patterns from large to small scale and then the timeless way of

144
00:08:17.241 --> 00:08:19.130
buildings as a philosophy of patterns.

145
00:08:19.130 --> 00:08:23.510
And these patterns are anything from how to make a nation state down to how to

146
00:08:23.511 --> 00:08:27.190
organize your bedroom.
And is he,
uh,

147
00:08:27.200 --> 00:08:30.110
in his and his collaborators,
right?

148
00:08:30.410 --> 00:08:33.320
Each pattern describes a problem which occurs over and over again in our

149
00:08:33.321 --> 00:08:37.130
environment and then describes the core of the solution to that problem in such

150
00:08:37.131 --> 00:08:40.430
a way that you can use the solution a million times over without ever doing it

151
00:08:40.431 --> 00:08:45.380
the same way twice.
So in the book,
here's an example of a pattern.

152
00:08:45.381 --> 00:08:50.360
I live in Pittsburgh and I live in a row house and so it's very dense where I

153
00:08:50.361 --> 00:08:53.330
live.
And these are patterns about the design of a row house.

154
00:08:53.331 --> 00:08:56.640
You see a number of statement picture,
um,

155
00:08:56.700 --> 00:09:00.960
a statement about the design problem,
illustrations to carry out to,

156
00:09:01.050 --> 00:09:04.890
to explain some of what that design problem is doing and then a connection to

157
00:09:04.891 --> 00:09:09.330
other patterns,
um,
in the system.
And it works sort of like an operating system.

158
00:09:09.360 --> 00:09:11.340
The patterns are a network.

159
00:09:11.341 --> 00:09:16.341
So you see here are a number of them in their order explaining what each of them

160
00:09:16.381 --> 00:09:18.540
do.
Um,
you know,

161
00:09:18.541 --> 00:09:23.541
this is going from a mosaic of some cultures all the way down to a household mix

162
00:09:23.700 --> 00:09:26.430
and it goes further and further into more detail.

163
00:09:28.030 --> 00:09:28.863
<v 2>MMM.</v>

164
00:09:30.080 --> 00:09:34.670
<v 0>Alexander is someone that a lot of technologists have read.
Um,</v>

165
00:09:34.671 --> 00:09:38.180
if you know Alan Cooper,
the a interaction designer,

166
00:09:38.510 --> 00:09:42.710
he used to ferret out Christopher Alexander's books in his high school library

167
00:09:42.770 --> 00:09:45.950
and read them there.
Um,
he wanted to be an architect.

168
00:09:45.951 --> 00:09:49.850
So did Kent Beck and then realized he didn't want to build buildings.

169
00:09:49.851 --> 00:09:54.230
He wanted to build software and there was a lot more that you could do within

170
00:09:54.231 --> 00:09:59.030
that world.
And a word cutting and ham ward Cunningham and Kent Beck,

171
00:09:59.420 --> 00:10:03.230
we're curious about how they could apply this idea of patterns to the idea of

172
00:10:03.231 --> 00:10:04.830
object oriented programming language or this.

173
00:10:04.831 --> 00:10:09.530
So they applied it to 'em interface patterns in small talk in the late eighties

174
00:10:09.531 --> 00:10:14.531
and about 1987 and a community of programmers got together and over the years

175
00:10:14.751 --> 00:10:19.130
built up what was called,
um,
the design patterns,
um,
movement.

176
00:10:19.131 --> 00:10:24.131
And this group of four men who called themselves the gang of four are the ones

177
00:10:24.261 --> 00:10:28.340
who created this kind of network of design patterns in software.

178
00:10:28.550 --> 00:10:32.480
And it's an idea that you see millions and millions of times.

179
00:10:32.481 --> 00:10:35.900
I think there's something like 1200 bucks on Amazon that refer to design

180
00:10:35.901 --> 00:10:40.430
patterns in,
um,
in software and in interfaces and games.

181
00:10:40.930 --> 00:10:41.260
<v 2>Okay?</v>

182
00:10:41.260 --> 00:10:44.560
<v 0>Whatever.
Okay.
Now does this,
how many people have heard of design patterns?</v>

183
00:10:45.130 --> 00:10:48.550
There you go.
Um,
also if you've ever,

184
00:10:48.880 --> 00:10:53.350
if you've ever used agile processes or,
um,
followed extreme programming,

185
00:10:53.650 --> 00:10:58.650
Kent Beck and board coming Cunningham and a number of other people applied these

186
00:10:58.811 --> 00:11:03.811
ideas about the philosophy of design from Christopher Alexander and the timeless

187
00:11:04.331 --> 00:11:09.331
way of building to up end and change the politics of the design process and to

188
00:11:09.581 --> 00:11:13.510
give more flexibility to users and to people.
And when I interviewed Kent Beck,

189
00:11:13.511 --> 00:11:16.450
he told me that it was a rearrangement of the political power in the design and

190
00:11:16.451 --> 00:11:17.470
building process.

191
00:11:19.020 --> 00:11:19.840
<v 2>Okay.</v>

192
00:11:19.840 --> 00:11:24.840
<v 0>Everyone here has used a wiki and the Wiki format was developed by Ward</v>

193
00:11:25.300 --> 00:11:28.930
Cunningham.
He was developing it in,
in hypercard in the early nineties.
And then,

194
00:11:29.410 --> 00:11:33.670
um,
it was suggested that he could maybe build this on this new thing called the

195
00:11:33.671 --> 00:11:34.540
worldwide web.

196
00:11:34.930 --> 00:11:39.190
And what he wanted to do was have a conversation that never had an end if he

197
00:11:39.191 --> 00:11:43.660
wanted to map all of the knowledge he had of his,
that he and his team had.

198
00:11:43.870 --> 00:11:47.890
He didn't want to say it ends here.
He wanted it to be able to go and go and go.

199
00:11:48.250 --> 00:11:52.720
He never patented,
um,
the Wiki format.
And,
um,

200
00:11:52.930 --> 00:11:56.830
it was picked up in the early two thousands to run Wikipedia and now we've all

201
00:11:56.831 --> 00:11:59.980
used wikis one way or another.
Um,
but this is again,

202
00:12:00.010 --> 00:12:02.210
a direct inspiration or,

203
00:12:02.270 --> 00:12:06.790
or a director influenced by Christopher Alexander,
uh,
project.

204
00:12:08.950 --> 00:12:12.010
Okay.
So that's Christopher Alexander.
And the other thing to say about him,

205
00:12:12.011 --> 00:12:15.400
his architects tend to hate him and technologists tend to love him.

206
00:12:15.610 --> 00:12:18.610
I'm not sure why conundrum,
but kind of interesting.

207
00:12:18.850 --> 00:12:20.920
So I'd like to introduce you to a different kind of architect.

208
00:12:20.921 --> 00:12:23.430
This is Cedric price.
Um,

209
00:12:23.530 --> 00:12:26.710
I've called him the secret patron saint of interaction designers.

210
00:12:26.711 --> 00:12:29.180
He changed the way,
um,

211
00:12:29.230 --> 00:12:33.340
any number of people in the UK understood architecture and buildings and what

212
00:12:33.341 --> 00:12:38.140
they could do.
Uh,
his life partner is,
uh,
was Eleanor Bron.

213
00:12:38.141 --> 00:12:39.370
He died in 2003.

214
00:12:39.371 --> 00:12:43.840
Eleanor Bron was in the Beatles movies and was responsible,
actually,

215
00:12:43.841 --> 00:12:46.090
she was the inspiration for the song Eleanor Rigby.

216
00:12:46.840 --> 00:12:51.340
So just a very curious and very funny person who liked to ask things like,

217
00:12:51.341 --> 00:12:54.580
technology's the answer.
But what was the question?

218
00:12:57.580 --> 00:13:02.320
And Cedric price was best known for,
um,
things that were never built.
So,

219
00:13:02.321 --> 00:13:02.861
for instance,

220
00:13:02.861 --> 00:13:07.090
the fun palace was designed in the 1960s and it was supposed to be a big

221
00:13:07.091 --> 00:13:09.700
cybernetic theater,
um,

222
00:13:09.760 --> 00:13:14.480
movable kind of place where all sorts of things could happen.
And,
um,

223
00:13:14.710 --> 00:13:18.400
the space was going to learn from it's users over time and adapt to their

224
00:13:18.401 --> 00:13:22.420
interests and needs and you could change it over time and use it kind of as a

225
00:13:22.421 --> 00:13:27.220
big leisure center to learn and experience whatever it is you might want to

226
00:13:27.221 --> 00:13:31.930
learn.
When you worked with a 25 member,
27 members,
cybernetic community,

227
00:13:31.931 --> 00:13:35.200
which is what you see here on the right.
And in this particular image,

228
00:13:35.201 --> 00:13:38.140
it's kind of hard to,
to read,
but,
um,

229
00:13:38.200 --> 00:13:43.200
it talks about input of unmodified people and then output of modified people.

230
00:13:44.471 --> 00:13:47.080
So they thought through this like a system diagram.

231
00:13:47.620 --> 00:13:51.280
He did this project with Joan Littlewood who was a radical theater director and

232
00:13:51.281 --> 00:13:54.210
a protege of the German playwright bear pressure.

233
00:13:55.690 --> 00:14:00.430
He also was really inspired by Gordon Pask and Gordon Pask is a cybernetic

234
00:14:00.431 --> 00:14:05.080
assist,
um,
who also had a big influence on a lot of,
um,

235
00:14:05.110 --> 00:14:10.110
a lot of architects and a piece that he wrote in 1969 talked about changing this

236
00:14:11.051 --> 00:14:15.610
a design process and some really interesting in some interesting ways.

237
00:14:18.590 --> 00:14:22.970
And he suggested that we turned the design paradigm in on itself and let us

238
00:14:22.971 --> 00:14:25.370
apply it to the interaction between the designer and the system.

239
00:14:25.371 --> 00:14:28.910
She designs rather than the intersection between the system and the people who

240
00:14:28.911 --> 00:14:33.080
inhabit it.
So rather than the system you designing,
doing your bidding,

241
00:14:33.110 --> 00:14:34.610
maybe it does something different.

242
00:14:35.000 --> 00:14:40.000
Maybe it augments and create something different than what the two people put

243
00:14:40.101 --> 00:14:40.934
together.

244
00:14:41.690 --> 00:14:41.920
<v 1>Okay.</v>

245
00:14:41.920 --> 00:14:46.330
<v 0>Um,
this is a trying,
he gave him second order,
cybernetics.
Uh,</v>

246
00:14:46.331 --> 00:14:47.380
he wrote things like,

247
00:14:47.740 --> 00:14:52.130
assume for the moment I'm the successful businessman with the bowler hat and I

248
00:14:52.131 --> 00:14:55.400
insist that I'm the sole reality where everything else appears only in my

249
00:14:55.401 --> 00:14:56.330
imagination.

250
00:14:56.780 --> 00:14:59.990
I cannot deny that in my imagination there were a little purer other people,

251
00:14:59.991 --> 00:15:03.920
scientists,
other successful businessmen,
et cetera.
As for instance,

252
00:15:03.921 --> 00:15:06.590
in this very conference,
since I had the,

253
00:15:06.650 --> 00:15:10.010
since I find these apparitions in many ways similar to myself,

254
00:15:10.011 --> 00:15:12.980
I have to grant them the privilege that they themselves may insist that they are

255
00:15:12.981 --> 00:15:16.250
the soul reality and everything else is a condition of their imagination.

256
00:15:16.760 --> 00:15:17.331
Another hand,

257
00:15:17.331 --> 00:15:20.600
they cannot deny that they're fantasies will be populated by people and one of

258
00:15:20.601 --> 00:15:22.970
them may be I with bowler hat and everything.

259
00:15:23.790 --> 00:15:26.960
So this is second order cybernetics where the fact that you're engaging with the

260
00:15:26.961 --> 00:15:30.920
system changes to the system and if you start looking at the design process is

261
00:15:30.921 --> 00:15:33.230
something that needs to take that into consideration.

262
00:15:33.230 --> 00:15:36.820
Design begins to look a little bit different.
Um,

263
00:15:36.860 --> 00:15:39.290
Cedric price was again very,

264
00:15:39.500 --> 00:15:44.030
very inspired by his interactions and collaborations with Gordon Pask and one of

265
00:15:44.031 --> 00:15:45.320
the buildings that,

266
00:15:45.321 --> 00:15:48.350
or one of the projects that he designed was something called generator.

267
00:15:48.680 --> 00:15:50.460
This was never built.
Um,

268
00:15:50.500 --> 00:15:55.500
but it was a project that was in existence from 76 to 79 and it was a set of 12

269
00:15:56.451 --> 00:15:59.030
foot cubes,
150 cubes,
walkways,

270
00:15:59.031 --> 00:16:03.290
boardwalks that could be moved around and um,

271
00:16:03.710 --> 00:16:06.710
and recombined for whatever purposes someone might like.

272
00:16:06.711 --> 00:16:10.310
This was an arts retreat center for a wealthy patron in Florida.

273
00:16:11.420 --> 00:16:13.370
He always really liked moving cranes.

274
00:16:13.371 --> 00:16:17.870
So there's always a moving crane here and generator would come together like

275
00:16:17.871 --> 00:16:22.400
this.
There was grid on the ground,
he put cubes over it,
roofs on top,

276
00:16:22.401 --> 00:16:26.930
connect the parts,
and then you could put up the side baffles and,
um,

277
00:16:27.020 --> 00:16:29.840
the stairways and do what you'd like with this space.

278
00:16:30.170 --> 00:16:33.790
You could also model it with these little cubit parts,
um,
that,

279
00:16:33.850 --> 00:16:36.920
that you would see.
He thought that maybe fun things might happen.

280
00:16:36.921 --> 00:16:38.990
Like you could take a bleep or walk.
I will remind you,

281
00:16:38.991 --> 00:16:42.950
this is 1977 or so when this idea came up.
Um,

282
00:16:42.970 --> 00:16:45.530
and something strange about a mouse rolling around a cube,

283
00:16:45.920 --> 00:16:50.210
but it's strange and weird and funny all the time.

284
00:16:50.840 --> 00:16:53.590
And again,
we look at Gordon Pask.
Um,

285
00:16:53.630 --> 00:16:58.550
Gordon Pask did a project first in 1953 and then in 1968 called,

286
00:16:58.970 --> 00:17:01.220
which became the Colloquia of Mel mobiles.

287
00:17:01.490 --> 00:17:05.710
And what happens with these mobiles are you interact with it and it an internet

288
00:17:05.711 --> 00:17:09.080
or interacts back with you until it gets bored.

289
00:17:09.650 --> 00:17:13.390
And Paul Pangero who is a cybernetic assist and um,

290
00:17:13.450 --> 00:17:18.450
and a professor at the college for Creative Studies in Detroit has just rebuilt,

291
00:17:19.270 --> 00:17:19.880
um,

292
00:17:19.880 --> 00:17:23.720
the Colloquia of Mill Mobiles for the 50th Anniversary of the original project.

293
00:17:23.720 --> 00:17:26.450
He's Gordon Pask archivist and a,

294
00:17:26.451 --> 00:17:29.210
it's up right now as we speak in Detroit.

295
00:17:30.800 --> 00:17:34.790
So this is what,
what does this have to do with generator?
Well,

296
00:17:34.791 --> 00:17:35.720
it's hundred price realized.

297
00:17:35.721 --> 00:17:37.700
It was unlikely that people are going to want to move around,

298
00:17:37.701 --> 00:17:41.510
generator enough and learn from it and do the kind of funny things that it would

299
00:17:41.511 --> 00:17:44.630
expect.
So he began to work with John Frazier and Julia Frazier,

300
00:17:44.631 --> 00:17:48.350
who were computer scientists as well as architects and um,

301
00:17:48.470 --> 00:17:53.470
wanted put together a set of programs that people could use that would be used

302
00:17:53.751 --> 00:17:54.600
by generators.

303
00:17:54.601 --> 00:17:59.010
So there would be micro controllers all on all of its parts and inventory

304
00:17:59.011 --> 00:18:01.530
program,
a design program.
That's what you see here.

305
00:18:01.531 --> 00:18:05.890
You can pick up and move these cubes and they plot in Printout,
um,

306
00:18:06.300 --> 00:18:09.840
and a boarding program.
And the boredom program,
uh,

307
00:18:09.841 --> 00:18:14.400
said the following in the event of the site not being reorganized or changed for

308
00:18:14.401 --> 00:18:18.690
some time,
the computer starts generating unsolicited plans and improvements.

309
00:18:20.320 --> 00:18:21.153
<v 1>Okay.</v>

310
00:18:21.160 --> 00:18:24.730
<v 0>And this is something that John Frazier wrote in his letter to Cedric price</v>

311
00:18:24.731 --> 00:18:26.800
introducing these programs.
So if we kick a system,

312
00:18:26.801 --> 00:18:29.440
the very least you would expect it to do is kick you back.

313
00:18:30.880 --> 00:18:31.713
<v 1>And then</v>

314
00:18:32.410 --> 00:18:32.921
<v 0>he said,</v>

315
00:18:32.921 --> 00:18:36.220
you seem to imply this as a handwritten footnote that you can kind of see down

316
00:18:36.221 --> 00:18:36.671
here in the letter.

317
00:18:36.671 --> 00:18:40.210
You seem to imply that we were only useful if we produce results that you did

318
00:18:40.211 --> 00:18:42.100
not expect.
I think this was,

319
00:18:42.190 --> 00:18:46.510
this leads to some definition of computer AIDS in general.
At least.

320
00:18:46.511 --> 00:18:50.380
One thing that you would expect from any half decent program is that it should

321
00:18:50.381 --> 00:18:53.290
produce at least one plan,
which you did not expect.

322
00:18:54.370 --> 00:18:58.630
So this is where something like Gordon Pasts Colloquia mobiles makes its way

323
00:18:58.990 --> 00:19:01.420
into Cedric price's architecture.

324
00:19:01.930 --> 00:19:05.620
And these are ideas that we pick up today in some really lovely ways.

325
00:19:05.630 --> 00:19:06.700
I'm on the next slide.

326
00:19:06.701 --> 00:19:11.200
I'm going to introduce a project by the roboticist Madeline Gannon.

327
00:19:11.201 --> 00:19:13.660
She just finished her phd at Carnegie Mellon.

328
00:19:14.060 --> 00:19:19.060
I'm in architecture and she did an exhibition at the London Design Museum with a

329
00:19:20.321 --> 00:19:24.230
robot,
a robotic arm that would um,

330
00:19:24.400 --> 00:19:28.840
play with it's play with somebody in its midst until it got bored and went away,

331
00:19:29.540 --> 00:19:34.090
<v 3>comes together and you're in the space with the robot and you just have a very</v>

332
00:19:34.091 --> 00:19:39.091
raw experience with this animal like machine responding to your every move,

333
00:19:40.120 --> 00:19:43.630
all the technical aspects sort of melt away into the background.

334
00:19:44.680 --> 00:19:49.680
It's incredibly important to have opportunities and spaces to come in and

335
00:19:50.321 --> 00:19:54.910
experiments and misuse these existing technologies.

336
00:19:55.760 --> 00:19:56.290
<v 1>Okay.</v>

337
00:19:56.290 --> 00:20:00.670
<v 0>She didn't know about garden pastor Cedric price when she did this project,</v>

338
00:20:00.671 --> 00:20:03.520
but I find it compelling that many years later,

339
00:20:03.790 --> 00:20:08.050
this is exactly what she is trying to do and trying to get us to rethink our

340
00:20:08.051 --> 00:20:11.710
relationship with our machines.
She calls herself the robot whisperer.

341
00:20:15.140 --> 00:20:18.470
Third Person I'd like to talk about is Nicholas Negroponte.
Um,

342
00:20:18.560 --> 00:20:22.700
and you'll notice here he always seems to have his fingers in the picture.

343
00:20:22.701 --> 00:20:26.480
He always seems to be gesturing at something.
Um,

344
00:20:26.540 --> 00:20:30.290
you would probably know him as the founder of the MIT media lab,

345
00:20:30.320 --> 00:20:32.180
but before the media lab,

346
00:20:32.510 --> 00:20:36.260
he ran something called the architecture machine group at Mit.

347
00:20:36.590 --> 00:20:39.020
And that's what I want to want to talk about here.

348
00:20:39.410 --> 00:20:42.800
The Mit architecture machine group was in the school of architecture,

349
00:20:43.070 --> 00:20:47.800
but worked very closely with the AI lab and developed interfaces and

350
00:20:47.801 --> 00:20:52.030
environments for,
um,
for artificial intelligence.

351
00:20:53.750 --> 00:20:58.460
The gesture thing.
He,
um,
he's the author of a number of books.
Um,

352
00:20:58.461 --> 00:21:01.490
the book that you'd probably be most likely to know would be called being

353
00:21:01.491 --> 00:21:06.491
digital and being digital came out in 1995 but in 1970 he wrote a book called

354
00:21:06.981 --> 00:21:10.250
the architecture machine and he dedicated it to the first machine that can

355
00:21:10.251 --> 00:21:11.450
appreciate the gesture.

356
00:21:14.580 --> 00:21:19.020
Nicholas Negroponte is a clever man.
Um,
but this book I think is actually,
um,

357
00:21:19.230 --> 00:21:20.880
fascinating and really important.

358
00:21:20.910 --> 00:21:25.910
It was a theory of design for artificial intelligence that I think still holds

359
00:21:26.101 --> 00:21:26.934
weight.
Today.

360
00:21:29.160 --> 00:21:32.190
I want to point out a little bit about the funding structures of the

361
00:21:32.191 --> 00:21:37.050
architecture machine group and a of artificial intelligence in general.

362
00:21:37.440 --> 00:21:41.340
Um,
the architecture machine group was funded by the information processing

363
00:21:41.341 --> 00:21:45.780
technology office of Darpa and the office of naval research.
Um,

364
00:21:46.140 --> 00:21:50.860
the office of naval research was so important for defining the,
um,

365
00:21:51.000 --> 00:21:55.320
funding for artificial intelligence from pretty much its inception onward

366
00:21:55.321 --> 00:21:58.450
through the 80s.
Um,
so vital that,
um,

367
00:21:58.500 --> 00:22:02.960
I think that Marvin Minsky has referred to Marvin Denekov,

368
00:22:02.970 --> 00:22:07.970
who was the program officer as the grand old man of artificial intelligence.

369
00:22:08.460 --> 00:22:13.230
So the thing to point out is something that,
that I'm Paul Edwards,

370
00:22:13.231 --> 00:22:17.730
the historian has referred to as the closed world.
That artificial intelligence,

371
00:22:17.760 --> 00:22:18.210
um,

372
00:22:18.210 --> 00:22:23.210
was primarily developed within department of defense funded entities because it

373
00:22:23.971 --> 00:22:27.720
meant a small group of people could continue to work on these projects through

374
00:22:27.721 --> 00:22:31.860
the same social network friendship networks and professional networks that had

375
00:22:31.861 --> 00:22:36.861
been developed since the end of World War II where NSF funding national science

376
00:22:37.021 --> 00:22:41.070
foundation funding kind of automatically and invites very spirit,

377
00:22:41.071 --> 00:22:45.300
tried to throw things open to the world and open things up and bring more people

378
00:22:45.301 --> 00:22:49.230
in.
So it was desired by Darpa,
Darpa,

379
00:22:49.310 --> 00:22:54.310
depto and ONR to keep funding and development in these small networks of people

380
00:22:56.240 --> 00:22:57.660
and to get things done better.

381
00:22:58.260 --> 00:23:02.580
And it meant that the architecture machine group started to structure their work

382
00:23:02.581 --> 00:23:07.581
like the AI lab did at MIT and work very closely on interfaces and small

383
00:23:09.271 --> 00:23:11.880
machines and environments for artificial intelligence.

384
00:23:12.900 --> 00:23:16.930
One of these first projects was funded by IBM.
It was called urban to uh,

385
00:23:16.960 --> 00:23:19.080
or urban urban too.
And then later urban five.

386
00:23:19.081 --> 00:23:23.760
And it was a conversational user interface for urban design.
Um,
this is 1970.

387
00:23:23.830 --> 00:23:27.460
I actually started a little bit before that.
And,
um,

388
00:23:27.960 --> 00:23:31.590
what happens here is user chooses a set of blocks.

389
00:23:31.591 --> 00:23:35.010
You see these squares down at the bottom and then there's a conversation going

390
00:23:35.011 --> 00:23:40.011
on the side using this light pen to select questions and answers and it asks you

391
00:23:40.441 --> 00:23:41.840
questions and you,

392
00:23:41.880 --> 00:23:46.880
you use the conversational dialogue and the buttons to determine,

393
00:23:46.960 --> 00:23:50.270
um,
what the attributes are of these,
um,

394
00:23:50.300 --> 00:23:53.480
these areas that you are designing.
So it's for urban design

395
00:23:55.200 --> 00:23:56.033
<v 2>and</v>

396
00:23:56.490 --> 00:24:01.320
<v 0>the architecture machine group book,
um,
the architecture machine book outlines,</v>

397
00:24:01.321 --> 00:24:04.500
um,
and critiques their,
their study here.

398
00:24:06.390 --> 00:24:09.720
You might notice this last one that says Ted,

399
00:24:09.780 --> 00:24:14.430
many conflicts are occurring.
And they said,
you know,

400
00:24:14.820 --> 00:24:18.900
it was charming but it printed garbage,
but at least it was friendly garbage.

401
00:24:18.980 --> 00:24:22.520
So the words that they used and um,
it,

402
00:24:22.650 --> 00:24:25.230
I think it points out if anyone here has tried to,

403
00:24:25.650 --> 00:24:28.440
has tried to design a conversational user interface.
You know,

404
00:24:28.441 --> 00:24:33.441
how easy it's going to seem on the surface and how hard it is in reality.

405
00:24:33.960 --> 00:24:34.890
And even Justin,

406
00:24:34.920 --> 00:24:39.900
what we could look back to someone like Joseph Weizenbaum in 1966 who created

407
00:24:39.901 --> 00:24:44.130
Eliza,
the therapy program.
Um,
it's a question and answer,

408
00:24:44.340 --> 00:24:48.110
user conversational user interface and um,

409
00:24:48.150 --> 00:24:51.750
very difficult and problematic in ways that caused Joseph Weizenbaum to rethink

410
00:24:51.751 --> 00:24:56.700
his own total engagement with computation and artificial intelligence.
Um,

411
00:24:56.701 --> 00:25:00.900
so everybody knows that this is difficult to do and yet we continual continually

412
00:25:00.901 --> 00:25:01.950
tried to do it anyway.

413
00:25:03.480 --> 00:25:04.313
<v 2>MMM.</v>

414
00:25:05.440 --> 00:25:05.861
<v 0>Nicholas,</v>

415
00:25:05.861 --> 00:25:10.861
Negroponte and Marvin Minsky were friends throughout their entire throat.

416
00:25:12.000 --> 00:25:16.330
Nicholas,
his entire professional life until Marvin stuff a couple of years ago.

417
00:25:16.810 --> 00:25:21.010
And here you see a,
uh,
a robotic arm stacking blocks.

418
00:25:21.011 --> 00:25:22.090
This is um,

419
00:25:22.210 --> 00:25:26.620
a lot of AI problems were developed in what we're calling micro worlds,
right?

420
00:25:26.621 --> 00:25:30.790
So you focus in on some small part of a project,

421
00:25:30.791 --> 00:25:33.520
some small issue,
right?
You're looking at edge finding,

422
00:25:33.521 --> 00:25:35.380
you're looking at computer vision.
In fact,

423
00:25:35.381 --> 00:25:38.950
you're still today thinking about computer vision in some very specific small

424
00:25:38.951 --> 00:25:43.450
ways.
Um,
I know Justin was just taking a computer vision class at Stanford,

425
00:25:43.900 --> 00:25:44.620
um,

426
00:25:44.620 --> 00:25:48.970
and you try to throw away the rest of the noise and just focusing on that

427
00:25:48.971 --> 00:25:53.530
problem.
And that's appropriate for some period of time.

428
00:25:53.531 --> 00:25:57.790
But after a while and micro worlds didn't actually scale up.

429
00:25:57.850 --> 00:26:01.510
And this gets to be a problem.
Um,
but as I mentioned,

430
00:26:01.511 --> 00:26:04.690
the architecture machine group worked in micro worlds as well.

431
00:26:04.691 --> 00:26:07.510
An Ai Ai interfaces.

432
00:26:07.511 --> 00:26:12.070
And this is a project that they did in 1974 the software show.

433
00:26:12.071 --> 00:26:15.800
It was a museum show at the Jewish Museum and um,

434
00:26:15.850 --> 00:26:18.370
information technology.
It's new meaning for art.

435
00:26:18.371 --> 00:26:21.490
And so here you see all of these cubes,
isn't it funny?

436
00:26:21.491 --> 00:26:25.300
Like generator has cubes and urban five has cubes more cubes.

437
00:26:25.810 --> 00:26:29.410
And you've got this set of mirrored blocks,

438
00:26:29.411 --> 00:26:34.411
400 mirrored blocks of five foot by eight foot pen and a durable hoard.

439
00:26:35.930 --> 00:26:36.330
<v 2>Okay.</v>

440
00:26:36.330 --> 00:26:40.280
<v 0>Dribble Colony,
um,
living there.
And it's going to be difficult to read this,</v>

441
00:26:40.281 --> 00:26:42.800
but it says dribbles match wits with computer built.

442
00:26:44.700 --> 00:26:49.700
And what seek did with stack blocks and what turbos did was move them around and

443
00:26:50.131 --> 00:26:51.000
knock them over.

444
00:26:52.760 --> 00:26:56.400
And this is the gatefold of the software catalog,

445
00:26:56.401 --> 00:26:58.800
the life in a computerized environment.

446
00:26:59.340 --> 00:27:04.140
And there's this great quote from a Ted Nelson who says,
our bodies,
our hardware,

447
00:27:04.141 --> 00:27:05.400
our behavior software.

448
00:27:05.940 --> 00:27:10.940
And it's Paul Pangero who made the contemporary Colloquia of mobiles and a

449
00:27:11.520 --> 00:27:13.320
member of the architecture machine group who told me,

450
00:27:14.280 --> 00:27:14.570
<v 1>okay,</v>

451
00:27:14.570 --> 00:27:18.140
<v 0>it was a little bit too true because seek tended to kill the dribbles</v>

452
00:27:21.410 --> 00:27:22.310
in later days.

453
00:27:22.340 --> 00:27:25.790
The architecture machine group turned to commanding control interfaces that were

454
00:27:25.791 --> 00:27:27.310
explicitly,
um,

455
00:27:27.380 --> 00:27:31.040
that had explicit military connections and applications.

456
00:27:31.041 --> 00:27:36.041
And this is because the only way past 1974 that you could get work in artificial

457
00:27:36.261 --> 00:27:41.261
intelligence funded was to align it with commanding control questions and direct

458
00:27:41.931 --> 00:27:44.060
tactical military applications.

459
00:27:44.061 --> 00:27:48.590
This is due to some legislative things that had happened in the 70s and the wake

460
00:27:48.591 --> 00:27:52.850
of the Vietnam War.
And so this is the aspen movie map.
Um,

461
00:27:52.880 --> 00:27:54.740
it is kind of what it looks like.

462
00:27:54.770 --> 00:27:58.130
Google street view that you experienced sitting in a room in a chair.

463
00:27:58.131 --> 00:27:59.930
You zoomed down the streets of Aspen,

464
00:27:59.931 --> 00:28:04.931
Colorado in an Eames lounge chair with joy pads and a video disc produces the

465
00:28:07.881 --> 00:28:12.620
images for you.
They had a prototype video disc player,
uh,
in the 1970s.

466
00:28:12.621 --> 00:28:16.010
And the way you get the images is everybody goes to aspen on vacation and you

467
00:28:16.011 --> 00:28:19.760
load up the jeep and that's what you have right here.
Um,

468
00:28:19.761 --> 00:28:21.650
so you zoom down the streets and there are other,

469
00:28:21.680 --> 00:28:26.120
there are other photos that show what's in front of the,
um,
the,

470
00:28:26.121 --> 00:28:30.320
the two smaller screens are usually maps and are on touch screens.

471
00:28:32.230 --> 00:28:32.810
<v 1>Okay.</v>

472
00:28:32.810 --> 00:28:37.070
<v 0>I want to point out that part of the reason this project was funded was it was</v>

473
00:28:37.071 --> 00:28:41.420
in the wake of the,
um,
the jet rescue,

474
00:28:41.460 --> 00:28:45.350
the hostage rescue in Entebbe,
Uganda.
Um,

475
00:28:45.410 --> 00:28:48.830
a number of Israelis and Israeli jet had been,
um,

476
00:28:49.340 --> 00:28:54.340
had been hijacked and the rescue of the hijacking was performed by building a

477
00:28:57.141 --> 00:29:01.700
model desert or a model airport in the Negev desert and pre-death,

478
00:29:01.880 --> 00:29:06.350
um,
practicing the rescue under cover of darkness.

479
00:29:06.680 --> 00:29:09.170
Um,
in fact,
um,
this,

480
00:29:09.320 --> 00:29:13.130
I think this is the subject now of a movie that's about to come out or as coming

481
00:29:13.131 --> 00:29:17.510
out later this year.
Um,
and so the question here became what could be,

482
00:29:17.540 --> 00:29:22.430
what would happen if you were able to actually simulate and do remote moving
and,

483
00:29:22.460 --> 00:29:26.720
uh,
move down these streets?
So that would be the stipulation for this project.

484
00:29:28.300 --> 00:29:28.710
<v 1>Yeah.</v>

485
00:29:28.710 --> 00:29:31.320
<v 0>In later years they were really explicit about this and some of them,</v>

486
00:29:31.680 --> 00:29:33.220
some of the reports they wrote,
um,

487
00:29:33.270 --> 00:29:36.270
Minsky and his colleague Richard Bolt wrote that we are proposing to develop

488
00:29:36.271 --> 00:29:37.830
human computer interfaces.

489
00:29:38.100 --> 00:29:42.100
On the one hand is sophisticated and conception as a cockpit and on the other

490
00:29:42.101 --> 00:29:45.460
hand is operationally simple as a TV.
From either perspective,

491
00:29:45.461 --> 00:29:48.370
the objective is the same supreme usability.

492
00:29:50.640 --> 00:29:51.460
<v 1>Okay.</v>

493
00:29:51.460 --> 00:29:54.800
<v 0>Later on they started delving into um,</v>

494
00:29:55.150 --> 00:29:59.260
virtual reality in certain ways and questions of augmented reality.

495
00:29:59.261 --> 00:30:04.261
So this provocation and mapping by herself is a postcard held up in front of the

496
00:30:06.400 --> 00:30:09.160
Queen's Palace in Stockholm and imagining what it would be like to have a screen

497
00:30:09.370 --> 00:30:11.260
that overlaid information upon the world.

498
00:30:11.890 --> 00:30:16.870
On the other side is a Westinghouse map Westing house window,

499
00:30:16.930 --> 00:30:20.560
uh,
basically,
uh,
very early.
Um,

500
00:30:21.050 --> 00:30:26.050
I pad like thing that a was still connected to various technologies and you can

501
00:30:26.501 --> 00:30:30.050
kind of see in the background that there's a Selectric typewriter and a

502
00:30:30.060 --> 00:30:31.930
dictaphone.
Um,

503
00:30:31.990 --> 00:30:36.220
but this was a mapping window and intended to substitute from apps in the

504
00:30:36.221 --> 00:30:36.910
battlefield.

505
00:30:36.910 --> 00:30:41.910
It had two dimensional and two and a half d capabilities depending on how you

506
00:30:42.011 --> 00:30:45.880
would flip it.
And this is apparently the first example of a layer digital map.

507
00:30:48.380 --> 00:30:49.030
<v 1>Okay.</v>

508
00:30:49.030 --> 00:30:51.530
<v 0>I like the fact that Nicholas Negroponte,
um,</v>

509
00:30:51.550 --> 00:30:56.550
has always understood what he has been working on and the conundrums and

510
00:30:56.831 --> 00:31:00.610
contradictions about it.
And in his 1975 books,

511
00:31:00.611 --> 00:31:02.530
soft architecture machines,
he said,

512
00:31:02.770 --> 00:31:05.560
I strongly believe that it is very important to play with these ideas

513
00:31:05.561 --> 00:31:08.830
scientifically and explore applications of machine learning,

514
00:31:08.831 --> 00:31:12.850
that totter between being unimaginably impressive and unbelievably exciting.

515
00:31:15.580 --> 00:31:16.400
<v 1>Okay.</v>

516
00:31:16.400 --> 00:31:17.450
<v 0>And if I myself as,</v>

517
00:31:17.460 --> 00:31:21.170
as I look at these historical examples and where we are today,

518
00:31:21.171 --> 00:31:25.310
I find myself wondering if we need new cliches because when I do Google image

519
00:31:25.610 --> 00:31:27.710
search and pop in Ai,
this is what I get.

520
00:31:30.160 --> 00:31:34.440
There's always this,
you know,
the way B
or the,

521
00:31:34.441 --> 00:31:37.750
the behold the hand and it's always that color blue.
It's like the,

522
00:31:37.751 --> 00:31:40.640
the blue of the projector when no one's in the room.

523
00:31:42.010 --> 00:31:46.880
Chase on lady.
I think there are no,
this is j query,
right?
Yeah,

524
00:31:47.600 --> 00:31:49.730
yeah.
Cyborg lady.
Um,

525
00:31:50.450 --> 00:31:54.530
if these are our views of AI in the future,
I think we're in trouble.

526
00:31:55.160 --> 00:31:58.160
But I appreciate some of the subversive ways that we can look at this and some

527
00:31:58.161 --> 00:32:01.550
of the fun ways that we can as well.
Um,

528
00:32:01.640 --> 00:32:04.610
is anyone here a fan of Chanel Shane and AI weirdness?

529
00:32:05.180 --> 00:32:09.650
I nerded out so hard at io this year.
When I got to meet her,

530
00:32:09.890 --> 00:32:12.920
she,
uh,
she does very funny things with AI that I'll talk about,

531
00:32:12.921 --> 00:32:16.310
but I really appreciate that she understands what she's doing.
And she said,

532
00:32:16.311 --> 00:32:19.310
you know,
if life place by the rule image,
the recognition works well.

533
00:32:19.580 --> 00:32:22.070
But as soon as people are sheep do something unexpected,

534
00:32:22.071 --> 00:32:23.720
the algorithms show their weaknesses.

535
00:32:24.170 --> 00:32:28.010
So are these orange flowers in a field or sheep because she colored them orange,

536
00:32:28.310 --> 00:32:32.530
it's not sure anymore.
Um,
Chanel is also done.

537
00:32:32.531 --> 00:32:35.560
Things like this is 7,000.
Um,

538
00:32:35.590 --> 00:32:40.590
paint colors fed to a neural net and it begins to create new colors,

539
00:32:43.010 --> 00:32:47.630
all of which are terrible and color names.
Exactly.

540
00:32:47.640 --> 00:32:52.550
You know,
I've,
I've been reduced to tears multiple times.
Caring can verbal,

541
00:32:52.551 --> 00:32:54.200
simp turn ugly

542
00:32:56.510 --> 00:33:00.110
ranching Blue Tondar.
Um,

543
00:33:00.410 --> 00:33:05.410
she fed the names to the names of Guinea pigs for the Portland Guinea pig rescue

544
00:33:06.441 --> 00:33:08.330
and came up with some names for Guinea pigs.

545
00:33:09.320 --> 00:33:12.800
Here we have hanger Dan and Princess Pow.

546
00:33:14.240 --> 00:33:15.530
And my husband has said,
you know,

547
00:33:15.531 --> 00:33:17.750
I think any kid could come up with princess power,

548
00:33:17.751 --> 00:33:21.800
but it takes a neural net to come up with hanger down only meets machine

549
00:33:21.801 --> 00:33:26.090
learning can produce now and again,
when I think about these kinds of things,

550
00:33:26.091 --> 00:33:29.460
I also find myself thinking about this,
which is actually it,

551
00:33:29.780 --> 00:33:34.780
it apparently was not Marcel Duchamp who delivered this to the art show in 1917

552
00:33:35.361 --> 00:33:35.721
this year,

553
00:33:35.721 --> 00:33:40.520
rental as his object to pet rather a woman whose name I can't remember,

554
00:33:40.521 --> 00:33:42.440
they've just surfaced that it wasn't him,

555
00:33:42.470 --> 00:33:47.450
but this was part of data at some where the world was going to hell.
Um,

556
00:33:47.480 --> 00:33:50.750
it was world war one and things were really,
really dark.

557
00:33:51.140 --> 00:33:54.980
And so what does art mean when things are really dark means that you've got to

558
00:33:54.981 --> 00:33:57.530
turn to absurdity,
you've got to turn it upside down.

559
00:33:57.590 --> 00:33:59.270
And so art starts to look different.

560
00:34:00.230 --> 00:34:05.230
Or you look at someone like Eugene Unesco who wrote the play Rhinoceros in 1959.

561
00:34:06.140 --> 00:34:06.531
Um,

562
00:34:06.531 --> 00:34:11.000
it's an absurdist play and it's where a group of people in a French town bit by

563
00:34:11.001 --> 00:34:15.080
bit turned into rhinoceroses.
And of course he's talking about totalitarianism.

564
00:34:15.590 --> 00:34:20.180
A UNESCO wrote a play called the bald soprano because he had started to try to

565
00:34:20.181 --> 00:34:24.270
learn English.
He is a French Romanian.
And,
um,

566
00:34:24.680 --> 00:34:26.030
in order to learn English,

567
00:34:26.031 --> 00:34:29.720
he would type phrase after phrase from this kind of English book.

568
00:34:29.721 --> 00:34:32.720
John and Sally go to the store and over and over,

569
00:34:32.721 --> 00:34:35.330
and he began to question the nature of these meanings.

570
00:34:35.660 --> 00:34:39.740
It's almost like he became his own algorithm in order to learn this.

571
00:34:39.741 --> 00:34:44.060
And he produced a very funny play called the bald soprano about it.

572
00:34:44.840 --> 00:34:48.830
Or You could even look at something like,
um,
booneyard Louie balloon,
Boone Yell,

573
00:34:48.840 --> 00:34:53.780
who did,
um,
ah,
Shyanne under Lou in the 1920s,
19 teens,

574
00:34:54.140 --> 00:34:58.790
but in 1970,
71,
something like that,
he did the discreet charm of the bourgeoisie,

575
00:34:58.791 --> 00:35:00.830
which is about a group of people trying to have dinner.

576
00:35:00.831 --> 00:35:03.950
And they just keep getting thwarted and it's a container,
well,

577
00:35:03.980 --> 00:35:07.520
somewhat contemporary surrealist film,
but all of these things are ways to win.

578
00:35:07.521 --> 00:35:09.800
The world is on its head.

579
00:35:09.920 --> 00:35:13.100
You append it in art and I want to suggest that maybe we should be doing the

580
00:35:13.101 --> 00:35:16.100
same things more frequently with our algorithms.

581
00:35:18.260 --> 00:35:23.110
Can anyone name what this is?
Thank you.

582
00:35:24.310 --> 00:35:28.780
Okay with saying that.
But yeah,
it's exactly what it is.
And um,

583
00:35:28.810 --> 00:35:33.730
the uncanny valley is an idea from 1970 from Masahiro Mori.
And what I'm struck,

584
00:35:34.090 --> 00:35:37.980
this is the idea that when robots are too similar to us,
they freak us out.

585
00:35:38.100 --> 00:35:41.220
And he describes the eeriness that that produces,

586
00:35:41.221 --> 00:35:44.050
but he says something else in his piece in,

587
00:35:44.060 --> 00:35:46.500
in the article he wrote in 1970,

588
00:35:46.501 --> 00:35:49.920
which is that it's about how we understand what makes us human.

589
00:35:50.430 --> 00:35:53.490
We should begin to build an accurate map of the uncanny valley so that through

590
00:35:53.491 --> 00:35:56.220
robotics research we can understand what makes us human.

591
00:36:01.090 --> 00:36:01.811
Until recently,

592
00:36:01.811 --> 00:36:06.460
Manuela Veloso was the head of the machine learning program at Carnegie Mellon

593
00:36:06.490 --> 00:36:11.110
and I was at a conference that she was speaking at and she was talking about

594
00:36:11.111 --> 00:36:14.080
machine learning is successful when it does what you expect it to do.

595
00:36:14.650 --> 00:36:15.910
And for Cedric Price,

596
00:36:16.120 --> 00:36:19.660
we saw the goodness of one something doesn't expect,

597
00:36:19.840 --> 00:36:24.040
do what you expected to do,
but there's some dark things too.
Um,

598
00:36:24.041 --> 00:36:27.550
the stories that came out earlier this spring about Poland tears,
predictive,

599
00:36:28.240 --> 00:36:30.790
predictive policing in north,
in New Orleans,

600
00:36:31.330 --> 00:36:36.070
and the fact that that had not been known,
not even to,

601
00:36:36.180 --> 00:36:39.550
uh,
most people in the city of New Orleans,
New Orleans or city officials,

602
00:36:41.020 --> 00:36:44.530
I liked the work a lot of Mimi Onuoha.
Um,
some of you may know her work,

603
00:36:44.531 --> 00:36:48.970
she does a project called missing datasets and you can't,

604
00:36:49.030 --> 00:36:52.420
you can't parse what you don't collect.
And so if you don't have the data,

605
00:36:52.421 --> 00:36:57.240
you can't begin to Parse it.
She does this both as a tech piece and a,

606
00:36:57.241 --> 00:36:59.200
a critical piece and an art piece.

607
00:36:59.201 --> 00:37:03.970
So here you see a lot of folders full of missing data sets.

608
00:37:03.971 --> 00:37:07.450
And of course if you go look through them,
they're all empty.
Um,

609
00:37:07.510 --> 00:37:12.510
she keeps a get hub repository where she has these missing datasets.

610
00:37:13.810 --> 00:37:15.080
And um,
this one,

611
00:37:15.081 --> 00:37:19.060
civilians killed in encounters with police or law enforcement agencies is no

612
00:37:19.061 --> 00:37:22.300
longer a missing dataset.
It's been collected.
Um,

613
00:37:22.330 --> 00:37:26.660
and so she'd like to see more of those and you know,
kissing points.

614
00:37:26.680 --> 00:37:31.030
Sometimes there's an activist angle to her work,
but there's also,

615
00:37:31.390 --> 00:37:36.280
um,
there's also work that happens as a result and group of,

616
00:37:36.300 --> 00:37:36.730
um,

617
00:37:36.730 --> 00:37:41.730
Asian actors on Broadway contacted her about the roles that are available for

618
00:37:42.820 --> 00:37:44.290
Asians on Broadway.

619
00:37:44.320 --> 00:37:48.550
And that Blue Line is the Asians,

620
00:37:48.580 --> 00:37:51.940
the Asian roles in the play,
the king and I,
and a,

621
00:37:51.941 --> 00:37:56.941
the other bright light blue that you see are Asian roles elsewhere in the 2014,

622
00:37:57.671 --> 00:37:59.440
20,
15 season.
Um,

623
00:37:59.441 --> 00:38:03.910
similarly you can see the problems for black actors are Hispanic actors,

624
00:38:03.911 --> 00:38:07.630
just not a lot available,
um,
at that point.

625
00:38:07.631 --> 00:38:11.710
But by collecting this data,
it became possible to change the equation.

626
00:38:11.920 --> 00:38:14.680
And now there are more Asian actors on Broadway.

627
00:38:15.160 --> 00:38:16.750
This is not a matter of an algorithm,

628
00:38:16.751 --> 00:38:21.751
but it's just quite simply a matter of the collection of data and collecting the

629
00:38:22.511 --> 00:38:24.520
Neda and naming it,
making changes.

630
00:38:29.110 --> 00:38:32.660
Oprah.
But I'm,

631
00:38:32.661 --> 00:38:34.840
I'm struck by the things that machines don't do,

632
00:38:34.900 --> 00:38:39.220
that we don't have good machines for picking raspberries and we kind of do for

633
00:38:39.221 --> 00:38:41.530
picking strawberries,
but it's very,

634
00:38:41.531 --> 00:38:45.820
very difficult to get this right to manipulate,
to,
to do this properly.

635
00:38:46.570 --> 00:38:49.600
And I'm struck by the fact that if you,
um,

636
00:38:50.750 --> 00:38:52.270
I think it was in Sweden,

637
00:38:52.271 --> 00:38:57.130
there was a mining operation that had a truck that had some very nice,

638
00:38:57.160 --> 00:38:57.993
um,

639
00:38:58.030 --> 00:39:02.740
machine learning algorithms operating it more efficiently and it was so

640
00:39:02.741 --> 00:39:06.040
efficient that it dug itself into a Rut and couldn't get back out.

641
00:39:06.400 --> 00:39:11.380
So they had to introduce noise into the system to allow for the,

642
00:39:11.460 --> 00:39:12.293
um,

643
00:39:12.550 --> 00:39:16.480
I love for the vehicle to not to expertly dig itself into the ground.

644
00:39:18.610 --> 00:39:21.190
So in closing,
I'll say that AI is new,

645
00:39:22.000 --> 00:39:25.960
but it's also old and that it is a matter of architecture.

646
00:39:26.590 --> 00:39:30.610
It's a matter of infrastructure and it's a matter of how we design.

647
00:39:32.540 --> 00:39:35.300
This is something I see in my neighborhood in Pittsburgh.

648
00:39:35.450 --> 00:39:40.370
I go running by Carnegie robotics.
Not totally sure how I feel about it.

649
00:39:40.680 --> 00:39:41.513
Um,

650
00:39:44.540 --> 00:39:45.373
<v 1>okay.</v>

651
00:39:46.190 --> 00:39:50.330
<v 0>Yeah,
I'm still not sure how I feel about it,
but I think it's pretty compelling.</v>

652
00:39:50.970 --> 00:39:54.230
Um,
but this is the thing that he did come back to over and over again,

653
00:39:54.500 --> 00:39:57.410
which is our man Cedric price just says technology is the answer.

654
00:39:58.010 --> 00:39:59.000
But what was the question?

655
00:40:00.160 --> 00:40:05.140
<v 1>Thank you.
Hi.</v>

656
00:40:05.400 --> 00:40:09.250
<v 4>Uh,
my name is Avi and Molina.
I'm an analyst here at Google.
Um,</v>

657
00:40:09.690 --> 00:40:11.450
and I've been getting really interested in,
uh,

658
00:40:11.670 --> 00:40:14.040
some of the things happening in the edge of this space,

659
00:40:14.050 --> 00:40:17.630
like generative design and some of the stuff that autodesk is working on.
Yeah.

660
00:40:18.060 --> 00:40:21.300
And I'm curious as to how,
uh,

661
00:40:21.510 --> 00:40:26.310
where are you see the space evolving,
uh,
sort of like fastest because you know,

662
00:40:26.311 --> 00:40:30.030
like three d printing,
you know,
they're usually industries,
but pick it up first.

663
00:40:30.031 --> 00:40:32.910
Like where do you see like the first sort of mass market,

664
00:40:33.230 --> 00:40:37.050
a case of an architectural,
you know,
AI or intelligence really happening?

665
00:40:37.450 --> 00:40:40.690
<v 0>That's a really good question.
Um,
I'm not sure if I,</v>

666
00:40:41.080 --> 00:40:43.390
if I have a well thought out answer.
Um,

667
00:40:43.600 --> 00:40:47.560
but I point out that the history of three d printing is already quite old.

668
00:40:47.561 --> 00:40:50.410
And so what we see the uptake in now,
um,

669
00:40:51.160 --> 00:40:56.160
at a consumer level or at a professional level as well.

670
00:40:57.070 --> 00:40:59.680
And the other thing is I don't think that three d printers,

671
00:41:00.250 --> 00:41:03.850
I think they became something that it was Sunday we're going to transform

672
00:41:03.851 --> 00:41:04.840
everything in the world.

673
00:41:04.841 --> 00:41:07.810
And so then everybody started getting Mayberg maker spaces.

674
00:41:07.811 --> 00:41:12.640
And I'm not sure that that has actually printing objects has made the world

675
00:41:12.641 --> 00:41:15.850
change.
But the question of generative design,

676
00:41:15.851 --> 00:41:20.350
I think that architecturally the primary focus has been on form and

677
00:41:20.351 --> 00:41:23.590
representation.
You know,
you,
you use,
um,

678
00:41:23.591 --> 00:41:28.550
any number of authoring programs and sign programs to produce,
um,

679
00:41:29.020 --> 00:41:31.870
produce work that you wouldn't be able to otherwise.

680
00:41:32.290 --> 00:41:36.470
I get very curious again about these old ideas that I still think,
um,

681
00:41:36.560 --> 00:41:41.560
are our vital about how they change the design process and the ingredients that

682
00:41:44.181 --> 00:41:49.180
go in that make us think of something that isn't necessarily just ending up in

683
00:41:49.181 --> 00:41:53.270
form but ending up in how we construed design.

684
00:41:54.830 --> 00:41:56.060
<v 1>But that's,
yeah.</v>

685
00:41:57.630 --> 00:42:00.330
<v 0>Thanks.
It's a good question.
Now I'm going to try and figure out the,</v>

686
00:42:02.200 --> 00:42:06.130
<v 1>uh,
so a row hit says</v>

687
00:42:06.700 --> 00:42:10.140
<v 0>any thoughts on Bill Mitchell's role?
A city of yeah,</v>

688
00:42:11.020 --> 00:42:15.710
or urban design on software.
Yes.
Other than Sim city.
Ah,

689
00:42:16.760 --> 00:42:18.800
chill.
Um,

690
00:42:19.150 --> 00:42:22.510
Bill Mitchell was at points,

691
00:42:22.540 --> 00:42:26.890
the dean of the school of architecture at Mit.
He ran the smart cities lab,

692
00:42:26.920 --> 00:42:28.360
absolutely vital individual.

693
00:42:28.361 --> 00:42:32.830
He's actually an alum of my master's program and he died about five years ago.

694
00:42:33.240 --> 00:42:36.490
Um,
and I didn't write about him very much in my book.

695
00:42:36.491 --> 00:42:40.390
I was focusing on a slightly earlier era.
Um,

696
00:42:40.450 --> 00:42:42.430
but he's,

697
00:42:42.460 --> 00:42:46.600
he's a totally vital figure and there's a woman named Anne Marie Brennan Right

698
00:42:46.601 --> 00:42:51.340
now.
Um,
she's a graduate of my phd program in Melbourne,

699
00:42:51.341 --> 00:42:56.170
Australia,
who's been working on some of his work and using his archive there.

700
00:42:56.171 --> 00:43:00.400
So I think we're gonna see some stuff forthcoming about Bill Mitchell,

701
00:43:00.760 --> 00:43:03.640
but you also see his work in,
you see his,

702
00:43:03.670 --> 00:43:07.360
how he's inspired people including like Anthony Townsend who wrote the book

703
00:43:07.361 --> 00:43:09.280
smart cities.
Um,

704
00:43:09.340 --> 00:43:13.030
and has been thinking a lot for the last 20 years on urban planning,

705
00:43:13.031 --> 00:43:17.860
smart cities and development.
Um,
I guess from a smart city perspective,

706
00:43:17.890 --> 00:43:22.890
I keep going back to two big concerns that smart cities tend to be.

707
00:43:24.130 --> 00:43:29.050
Um,
and again I get to say this cause I'm an academic I guess,
but there um,

708
00:43:29.260 --> 00:43:34.060
it seems to be viewed as the locus of really great big enterprise deployments.

709
00:43:34.061 --> 00:43:36.550
Like if we used to get excited about enterprises,

710
00:43:36.580 --> 00:43:39.520
the scale of the building or a campus,

711
00:43:39.790 --> 00:43:44.790
this is even yet still more tech and a lot of it hasn't been very graciously

712
00:43:46.661 --> 00:43:47.201
designed.

713
00:43:47.201 --> 00:43:51.760
I also think of that idea of micro worlds that the question of scale is really

714
00:43:51.761 --> 00:43:55.450
vital.
But when we say scale,
it means a lot of different things.

715
00:43:55.451 --> 00:43:58.270
Like is it one to a billion?
Is it tiny?

716
00:43:58.271 --> 00:44:03.271
Two great big is it has many more users is it has many more stakeholders.

717
00:44:04.060 --> 00:44:07.540
And I think designers and architects are really well positioned to talk about

718
00:44:07.541 --> 00:44:10.420
issues of scale.
Um,
but it's,

719
00:44:10.450 --> 00:44:13.990
it's the hardest thing I think for us to figure out what we mean when we mean

720
00:44:14.440 --> 00:44:17.110
deploying on that kind of skill.
Thank you,
Rohit.

721
00:44:18.340 --> 00:44:18.760
<v 1>Okay.</v>

722
00:44:18.760 --> 00:44:23.020
<v 0>Hi,
my name is Christine.
I just wanted to ask like,</v>

723
00:44:23.350 --> 00:44:26.190
what are you most excited about,
um,

724
00:44:26.530 --> 00:44:31.160
that's getting like research now or at an idea that's,
um,

725
00:44:31.230 --> 00:44:35.640
you might have just heard about like what in this field are you most excited

726
00:44:35.641 --> 00:44:40.590
about?
Thanks for that question.
That's really nice thing to be asked.
Um,

727
00:44:41.160 --> 00:44:45.900
well I recently received this named professorship and it's,
it's,

728
00:44:45.901 --> 00:44:47.490
uh,
the KNL gates professors,

729
00:44:47.580 --> 00:44:50.580
associate professors have been the ethics and computational technologies.

730
00:44:51.090 --> 00:44:52.320
I'm not an ethicist,

731
00:44:52.380 --> 00:44:57.380
I'm a designer and I'm a historian and I'm really curious about what we talk

732
00:44:58.351 --> 00:45:03.351
about when we talk about ethics because I don't think that it means ethics per

733
00:45:04.501 --> 00:45:08.550
se.
Um,
I like,

734
00:45:08.551 --> 00:45:13.380
you can't just sprinkle some ethics on top and make the food salty and,
and that,

735
00:45:13.381 --> 00:45:14.310
that doesn't work.

736
00:45:14.311 --> 00:45:17.790
And it makes me remember that time in like the nineties and two thousands when

737
00:45:17.791 --> 00:45:22.410
people discovered usability and said,
we need some usability on this website.
And,

738
00:45:22.950 --> 00:45:26.010
and like,
no,
you don't want usability.
You want it to work,

739
00:45:26.790 --> 00:45:28.110
you want it to be good.

740
00:45:28.140 --> 00:45:32.850
And I'm going to leave here and go to Stanford to meet up with Fred Turner,

741
00:45:33.360 --> 00:45:36.180
um,
who,
if you haven't read his,
um,

742
00:45:36.450 --> 00:45:41.250
recent pieces in logic and in zero 32 c you should,

743
00:45:41.251 --> 00:45:43.740
the zero 32 c piece came out this week.

744
00:45:44.790 --> 00:45:46.950
The piece and logic has a few months old.

745
00:45:46.951 --> 00:45:51.951
So he's the head of the communications department and he's a historian of,

746
00:45:52.950 --> 00:45:55.400
among other things,
silicon valley.
Um,

747
00:45:56.010 --> 00:46:01.010
and we're grappling with the question about education and he's,

748
00:46:02.070 --> 00:46:05.790
he's speaking about engineering,
writing about engineering education.
But I mean,

749
00:46:05.791 --> 00:46:08.490
I think anyone who's going to work with stuff,
how,

750
00:46:09.090 --> 00:46:13.680
how do we educate people?
Engineers are increasingly,
Fred will argue,

751
00:46:13.681 --> 00:46:18.390
stem oriented,
and then anything else has relegated to other departments.

752
00:46:19.050 --> 00:46:23.070
But how do you make an ethical approach to not be a blow off class?

753
00:46:23.580 --> 00:46:28.580
How do you make it be a part of the datasets that year or the problem sets

754
00:46:28.861 --> 00:46:30.450
you're working on?
Um,

755
00:46:31.410 --> 00:46:34.320
part of my interest in casting the role of it,

756
00:46:34.440 --> 00:46:38.420
of design to work in different kinds of areas,
um,

757
00:46:38.460 --> 00:46:41.520
is because of that.
I'd like to,
you can,

758
00:46:41.640 --> 00:46:43.860
you can work with designers on framing a problem,

759
00:46:44.940 --> 00:46:47.790
determining what data should be collected,
how it should be used,

760
00:46:47.791 --> 00:46:52.470
how it should be visualized or explained.
Um,
so I have questions about that.

761
00:46:52.471 --> 00:46:57.471
I have questions about the possibilities and impossibilities of explainable AI.

762
00:46:58.200 --> 00:46:59.070
And honestly,

763
00:46:59.071 --> 00:47:03.300
if you just sit me down with like a new history or a bunch of documents about

764
00:47:03.301 --> 00:47:04.170
the history of Ai,

765
00:47:04.171 --> 00:47:08.010
you won't see me for a while because I'll be very happy if I can come back into,

766
00:47:08.550 --> 00:47:09.390
into that stuff.

767
00:47:09.391 --> 00:47:13.680
But I think we have some very human questions that also meet the jobs of the

768
00:47:13.681 --> 00:47:17.580
people in this room and the people on my campus and probably the places that we

769
00:47:17.581 --> 00:47:19.530
all studied,
um,

770
00:47:20.100 --> 00:47:24.750
that need broader approaches than just the most efficacious and effective and

771
00:47:24.780 --> 00:47:27.300
efficient approach.
Thanks.

772
00:47:29.750 --> 00:47:30.130
So,

773
00:47:30.130 --> 00:47:34.100
<v 5>one more question from the dory.
Uh,
also from hit,
uh,</v>

774
00:47:34.600 --> 00:47:37.840
why aren't there more real architects,
uh,

775
00:47:37.841 --> 00:47:41.950
celebrated in the field of software architecture other than Christopher
Alexander

776
00:47:42.510 --> 00:47:46.360
<v 0>for asking.
This is so great.
Thank you.
Um,
yeah,</v>

777
00:47:46.361 --> 00:47:51.140
I asked Alan Cooper a couple of years ago,
can you name another architect?

778
00:47:52.360 --> 00:47:56.590
And I did this to Kent Beck too,
and silence for,

779
00:47:57.190 --> 00:48:00.450
and I think that Kent Beck said La Qorvis Yay.

780
00:48:00.460 --> 00:48:04.990
And I think Alan Cooper said,
uh,
oh,
I said John Portman,

781
00:48:05.020 --> 00:48:09.220
the guy who did the Embarcadero center and you know,
the,

782
00:48:09.221 --> 00:48:12.430
the Bonaventure in,
in La.
Um,

783
00:48:12.940 --> 00:48:15.100
it would be better if we had other architects,

784
00:48:15.101 --> 00:48:20.101
but I think the other thing is that Christopher Alexander as a mathematician,

785
00:48:21.070 --> 00:48:24.880
um,
and as something,
I mean,
it's something that everyone in the room deals with.

786
00:48:24.881 --> 00:48:28.990
Do you work on some very tiny part of an absolutely huge problem?

787
00:48:29.380 --> 00:48:32.320
How do you go back and forth?
How do you keep that in your head?

788
00:48:32.650 --> 00:48:37.510
And Christopher Alexander can explain that to people in a way that is really,

789
00:48:37.511 --> 00:48:40.660
really,
that really resonates with them.
The,

790
00:48:40.661 --> 00:48:45.661
the working on these two scales and then explaining why it actually matters on

791
00:48:46.721 --> 00:48:51.670
like a cosmological sense,
his late stuff is,
is very woo woo.
Um,
and,

792
00:48:52.230 --> 00:48:53.063
and that,

793
00:48:53.170 --> 00:48:57.370
that really resonates and it really doesn't resonate with most architects who

794
00:48:57.371 --> 00:49:00.340
build buildings.
So,
um,
I,

795
00:49:00.430 --> 00:49:03.910
I would like to see Chris or I'd like to see Cedric price get taken up by

796
00:49:03.911 --> 00:49:08.780
everyone else cause he's fun.
Um,
Alexandra is not much fun.
Um,

797
00:49:08.950 --> 00:49:11.590
but Cedric's and a blast.
Um,

798
00:49:11.650 --> 00:49:15.370
and to get to know other architects out there,

799
00:49:15.371 --> 00:49:19.300
I think that the classes of people over about the last four or five years to

800
00:49:19.301 --> 00:49:24.220
graduate from architecture school or design school or those of you were in the

801
00:49:24.221 --> 00:49:24.671
spaces,

802
00:49:24.671 --> 00:49:28.240
that crossover really doing interesting work and interesting ways of thinking

803
00:49:28.241 --> 00:49:31.450
about it.
Um,
if you want to look at something fun,

804
00:49:31.480 --> 00:49:34.390
look at my friend Fred Sherman's work.
He's an architect,

805
00:49:34.391 --> 00:49:36.700
he's a professor at Morgan State and uh,

806
00:49:36.730 --> 00:49:40.300
he has a book coming out on space architecture,

807
00:49:40.301 --> 00:49:43.930
like space exploration,
architectural histories next year.

808
00:49:43.931 --> 00:49:45.700
So fantastic stuff

809
00:49:47.310 --> 00:49:51.300
<v 5>through your talk.
I think,
um,
sometimes when you mentioned a,</v>

810
00:49:51.301 --> 00:49:52.650
I like,

811
00:49:52.950 --> 00:49:57.390
I would consider it as like more like human computer interaction.

812
00:49:58.130 --> 00:50:01.920
The reason I'm saying that is because like I'm an engineer,

813
00:50:01.921 --> 00:50:05.190
so like when I get my phd like more than 10 years ago.

814
00:50:05.340 --> 00:50:10.340
So like if you have a phd in Ai is not as easy to find a job as now.

815
00:50:11.820 --> 00:50:15.150
Yup.
Right.
So in my understanding led,

816
00:50:15.420 --> 00:50:18.060
we have a law breakthrough in AI,
like caught her Raji,

817
00:50:18.061 --> 00:50:21.990
I'm talking about like for example,
Alpha goal.
Yep.
Or like image recognition.

818
00:50:22.050 --> 00:50:24.810
Totally.
Or if you look at it like the IBM Watson,
right.

819
00:50:24.811 --> 00:50:28.580
So when they won't the jeopardy show that time.
Yep.

820
00:50:28.640 --> 00:50:32.030
So we're talking about like that a different type of a,

821
00:50:32.031 --> 00:50:37.031
I caught her algae advancement and that's reason it brings back the trend of Ai.

822
00:50:38.210 --> 00:50:41.900
But this is quite different from what you're talking about.

823
00:50:41.901 --> 00:50:43.580
Like the human computer interaction.
So,

824
00:50:43.581 --> 00:50:46.400
and I think IBM call it like cognitive computing.

825
00:50:47.030 --> 00:50:49.070
So would you,

826
00:50:49.340 --> 00:50:53.660
would you agree with me if like I tried to understand when you say AI is more

827
00:50:53.661 --> 00:50:55.310
like human computer interaction,

828
00:50:55.311 --> 00:50:57.920
which is not core AI ever from that we're talking about.

829
00:50:58.190 --> 00:51:02.690
<v 0>So,
um,
yes and no.
Um,</v>

830
00:51:02.960 --> 00:51:06.770
thank you for that.
That's great.
And to be,
to be clear,

831
00:51:06.771 --> 00:51:11.750
the focus of my research in this book is 1960.

832
00:51:12.240 --> 00:51:16.100
I mean to some extent,
1948 cause I look at cybernetics a lot,

833
00:51:16.490 --> 00:51:20.810
but um,
to really about 1977,

834
00:51:20.811 --> 00:51:22.760
1980.
Um,

835
00:51:23.510 --> 00:51:25.950
the parts of the book that are focused on,

836
00:51:26.030 --> 00:51:30.680
on later material are not focused on questions of Ai.

837
00:51:31.220 --> 00:51:36.170
And I agree with you that they are questions of human computer interaction.

838
00:51:36.830 --> 00:51:37.730
Um,

839
00:51:38.360 --> 00:51:43.360
but I would say that when Negroponte was writing this book,

840
00:51:45.231 --> 00:51:50.030
he was writing the book in 1970 and doing the work in 1970.

841
00:51:50.420 --> 00:51:55.420
He was developing interfaces with the AI lab using their technologies and their

842
00:51:56.991 --> 00:52:01.640
environments and basically tinkering and working in,
in that capacity.

843
00:52:01.940 --> 00:52:06.260
So yes,
they are.
It's input and output devices and um,

844
00:52:06.470 --> 00:52:09.280
and all kinds of ways that the lab over that,

845
00:52:09.310 --> 00:52:13.370
that period of time played with that it becomes human computer interaction.

846
00:52:13.370 --> 00:52:18.370
But I don't think HCI is the field really kind of comes together in that term

847
00:52:20.300 --> 00:52:22.910
until the 1980s.
I,

848
00:52:23.330 --> 00:52:27.860
I'd need to look at my notes to remember exactly when I will say that I'm very

849
00:52:27.861 --> 00:52:32.861
curious about what happens in the Hci community around 1990 because there's some

850
00:52:33.951 --> 00:52:36.920
interesting crossovers that I think have,

851
00:52:37.010 --> 00:52:42.010
have things to do with HCI and interaction design in ways that are in things

852
00:52:43.251 --> 00:52:46.130
that are really vital.
Um,
and yeah,

853
00:52:46.131 --> 00:52:51.080
I would also agree with you that things are way more complex today and I have

854
00:52:51.081 --> 00:52:54.920
skated by all of it.
So,
um,

855
00:52:55.400 --> 00:52:59.180
my understanding of it is more nuanced.
But again,
I'm also,

856
00:53:00.200 --> 00:53:05.200
I'm a historian of technology and architecture and I'm not someone who got my

857
00:53:05.451 --> 00:53:08.600
phd in Ai.
But yeah,

858
00:53:08.630 --> 00:53:13.610
when you start looking at it today and the implications,
um,

859
00:53:14.030 --> 00:53:18.290
and the different kind of terms that people are claiming,

860
00:53:18.291 --> 00:53:21.980
cognitive computing,
deep learning,
um,

861
00:53:23.110 --> 00:53:27.720
there was a whole different set things to start looking at and studying their,

862
00:53:27.721 --> 00:53:30.210
their histories.
They're going back to the 1980s,

863
00:53:30.290 --> 00:53:35.190
1970s that I haven't yet gotten into.
But it's,
you're,

864
00:53:35.191 --> 00:53:38.730
you're right and,
and yes,
but no,
but yes.

865
00:53:40.530 --> 00:53:45.120
Thank you for that.
Thank you so much everyone for coming out today.

866
00:53:47.090 --> 00:53:47.620
Thank you.

