Speaker 1:          00:08          I'm very excited today to introduce to you, Cathy O'Neil will be speaking to us about her book, weapons of math destruction. Um, Kathy is uniquely welcome to talk about the subject of mathematical models and how they can go bad. Uh, she has a phd in math from Harvard University. She has held positions at Barnard college and Mit. She's worked at the Hedge Fund de Shaw and uh, also as a data scientist at various startups. Um, she also is active in the occupy occupy Wall Street movement, uh, especially in their alternative baking group. She blogs@mathbabe.org. It's a great blog. You should check it out. Uh, as well as uh, participates in the slate money podcast. Um, and she is the author, coauthor of two other books doing data science and on being a dentist and being a data skeptic. Please join me introducing Cathy O'neil.

Speaker 2:          01:09          Hi Guys. Thank you so much for coming. Um,

Speaker 3:          01:14          so I'm a math nerd. I love math. Uh, I was attracted to math in high school because it was true. It was beautiful. Like you can disagree with somebody. I'm a politics and everything. About what manifest destiny really meant. But you'd have to agree on math because you guys were, you'd be so careful with your assumptions on the, in your logical arguments. I thought it was something, wouldn't it be beautiful if we could sort of port this honesty and truth and clarity to other, other fields of the real world. So after I became an assistant professor at Barnard College, I realized that it wasn't the right pace for me, that I wanted to have like more, more to do with the real world. And it was 2007 so I did what everybody else does. And I went to a hedge fund, um, and quickly realized that, well, the world blew up almost the moment I stepped into the hedge fund, um, was working with the experts like Larry Summers.

Speaker 3:          02:23          I worked with him on a couple of projects. Um, it was very disillusioned by what the experts seem to actually know about what was going on. But I was most disillusioned and actually ashamed to be honest about one of the most fundamental reasons that the financial crisis happened, which was the AAA ratings on the mortgage back securities, which I consider a mathematical lie. It was the opposite of how I had fallen in love with mathematics as something that would bring forth clarity. It was the, it was really, it was hiding corrupt practices behind mathematics, like trust the math. Be impressed by the fact that we have a bunch of PhDs in math who are supposedly, you know, carefully going through this data to dump double check, triple check that this, these mortgage backed securities are very unrisky. Trust us, there's math here and, and invest in these, in these, uh, these very opaque instruments.

Speaker 3:          03:23          And I'm not saying that's the only thing that happened. Obviously the mortgage broker is like made a bunch of bad mortgages, but the reason that they were able to create, like build this machine and scale it to the, to the scale of the massive international scale that it became was largely because of the AAA ratings, because of the trust. And that was trusted mathematics. And it was abused. I'm willing to, uh, I left finance after a small stint in risk, which I could go into if you guys are interested in the Q and a, um, where I was equally disillusioned by my, the part I was playing in particular, um, in in risk. Um, and I left finance altogether. I started my blog, math babe. Um, the idea was to sort of expose corrupt practices and mathematical abuse as I saw it, we should happen to finance.

Speaker 3:          04:15          In the meantime, I needed a day job. So I became a data scientist very easily. I should say. I just renamed myself a data scientist and got a job the next day, um, because, well, because I was qualified because what do you do when you're a data scientist? You predict people instead of predicting markets at had been predicting markets, futures and credit default swaps and stuff. So it wasn't that much of a stretch to predict people. In fact, it was very, very simple and I wanted to think that what I was doing was like less destructive than what I'd been, how I've been feeling and finance. But what I realized pretty quickly, within a year or so is that I was just very much part of something, um, that I still considered, um, not, not benign, like actually negative. And what I realized that was that the difference between the sort of the, the havoc that was that was created by the financial crisis where everybody had noticed and the financial systems of the entire universe of the entire world, we're sort of at risk.

Speaker 3:          05:20          And everyone was worried was that the Christ, the sort of the, the, the destruction that was being, um, that was the havoc that was being wreaked and the data science world was happening at the individual individual level, that individuals were being lost or they were being deemed losers or winners by this stuff. And Moreover, what was most disillusioning and, and kind of frightening for me because I was still kind of idealist, idealistic, was that these were all being, again, they were being sort of marketed as objective and fair. These algorithms, um, when in fact they were not that they were, their algorithms are nothing more than opinions embedded in code. So let me give you an example. And you guys are technical, so you'll understand this quite well. This is my son. I like to give the example of what is an algorithm by using the example of my, my own internal algorithm for building, for cooking dinner for my family.

Speaker 3:          06:25          So an algorithm is that has essentially has two big choices, lots of little choices, which I'll ignore. But the big choices are the data that you train your algorithm on and the definition of success or the objective function, which also includes your penalty for mistakes, but ignoring that for now. So what, what, what's the, what's the data going into my algorithm for cooking? I'm, yeah, well, it's the, the, the food I have in my kitchen, uh, the time I have the ambition I have, by the way, I should say, I curate that already. I curate, I carry the food. I do not consider ramen noodles food. My, my teenagers do. I have teenage sons as well. So I'm already imposing my definition of food. Okay. And then the definition of success for me for dinner is whether my kids ate vegetables. Now my seven year old, if he were in charge, this guy right here wilfy, he would define success for a meal as if he got a lot of Nutella.

Speaker 3:          07:21          And the reason this matters is because over time we optimize our algorithms for success. I mean that's why we define it, right? We want to get closer and closer to that, that success. So over time, that means I make more and more meals that have been successful in the past. I train my algorithm to success. That is a very different succession of meals. And my son's algorithm would be if we, if we optimize on the tele. So that's me, that's me imposing my agenda onto the algorithm. We always do that. There is no such thing as objective algorithm because the very least the person building the algorithm defined success and that's usually success for them. But there's often other stakeholders in the, in the play. And the question is, is that success for the stakeholders as well? It's not always it. Generally speaking, the patterns I've found looking through more and more data science was that we were more and more talking like marketing these things as fair and objective and following the numbers and mathematical, but there were people behind it who are building the definitions of success, these objective functions, and they were essentially invisible behind that mathematical shield.

Speaker 3:          08:34          A lot of these algorithms were benign or fine or great, but some of them were really not great and I focus on those in my book. I focus on those because I do not like vague discussions about what could go wrong with a bunch of big data. I want to know exactly what is going wrong right now. It's a kind of triage on this, on the world of Algorithms, so I focus specifically on the worst possible algorithms. If you're wondering why I'm so negative in my book, I'm not actually negative. I love data. I'm a data scientist still and I believe in data, but I do want to make it very clear to the public and this book is written for the public that some of the times this stuff is really fucked up. And I define that with three characteristics. A weapon of math destruction is one of these really terrible algorithms.

Speaker 3:          09:21          They have three characteristics. One is the widespread important, okay, so nobody cares about the owl. The algorithm for I make food for my kids with that is not important. What's important, what makes something important? And he said it happens to a lot of people and it makes big decisions for them. Of course, algorithms don't make big decisions to be clear. It's humans that have set up processes by which an a score of an algorithm and ends up being a decision. But I'm going to, you know, let's just put that aside for now. If people's life's options, important life options, like how whether they get a job, where they go to school, whether they get alone, how much they pay for insurance, how long they go to jail. If those kinds of things are determined in part by algorithms, essentially mostly scoring systems, then they're important. They're widespread and important. The second characteristic of things I'm really worried about is the mysteriousness. The secrecy. These scoring algorithms are in general, aren't things that the formulas are not available for people who are being scored by them.

Speaker 2:          10:21          Yeah.

Speaker 3:          10:21          Often they are not even aware that they are being scored.

Speaker 2:          10:27          Okay.

Speaker 3:          10:28          And because they are important, that is not okay because this, they are tantamount to laws. It is not okay for them to be secret. We have a constitutional right to know what the laws of our country are. And finally I care about algorithms not that are improving the world. I mean those are great, but that are destructive. So I'm really focused on things that are destructive. And I mean that in two ways. The first way, I mean that is that they ruin people's lives unfairly. So those people who's important, life decisions are being informed by these scores. They are, they're getting unfairly booted from opportunities. Opportunities are unfairly being taken from them. Moreover, they're destructive in a larger sense. They actually set out to solve the problem in a larger sense, usually often with good intentions, but not only do they fail to solve that problem, but they actually make the problem worse.

Speaker 3:          11:20          They create usually a destructive feedback loop and I'm going to give you a bunch of examples and then we'll have Q and. A. So the first example comes from teacher assessment. We've had basically two decades long war on teachers in this country. The idea is we have a problem with education, let's find the bad teachers and get rid of them. And I will put aside for the now the question of whether we could actually solve our problems and the big problem being the achievement gap between rich and poor kids. Could we solve that by getting rid of bad teachers? That's a different question. The point is that we have been looking for these bad so we can get rid of them for two decades. The first generation of teacher assessment tools was really, really stupid and unfair. And here's how it was.

Speaker 3:          12:05          Look for teachers, a majority of who students did not pass some proficiency standard and their standardized tests. Now this, the thing you need to know to understand this is that there's a strong correlation between scores on standardized tests and poverty. Um, so poor kids just don't do as well on standardized test. That's not specific to the United States. That's true over time. That's true internationally. But what it means is that if you, if you targeted teacher as if you've labeled them as bad, because a lot of their kids do not pass a certain threshold of proficiency, you're basically targeting kids, teachers of poor kids. So that was clearly unfair. So it was discarded. The second generation was meant to be more fair and it had good intentions, but it didn't work. And let me tell you what it was. It was called the value added teacher model.

Speaker 3:          12:57          And the idea here is that you can't really blame a teacher if a kid gets like a 60 in fourth grade and they're the fifth grade teacher, you can't blame them for it. They don't get a hundred at the end of fifth grade right there starting from 60 so it's just better if they get more than expected. Right? So the idea was there was a primary model, which is the expected score for each student in a class. And then there was the actual score that that kid got to the end of the year and the teacher was taken, put, basically held accountable for the difference between those two scores. So if they are expected score was 62 but they got a 65 the teacher was sort of given credit for those three points. If they got, they were supposed to get a 62 but they got a 55 they were dinged for those seven points that they didn't get.

Speaker 3:          13:43          Does that make sense? Now the problem with this, it actually makes sense, right? And if you had like a Google level tests of this where instead of having 25 kids in the class, you had 20,000 kids in this class, then the statistics, the statistical signal will be pretty clean. The problem is we do not do not have that, so we have instead is a pretty bad model, which is estimating of the expected score for a kid at the end of the year. It's actually really hard to take a kid at the end of fourth grade, estimate what they're going to get at the end of fifth grade. It's not a precise thing. There's a lot of uncertainty, especially for kids in poverty. Then the second thing, the second source of uncertainty is like what they actually got at the end of fifth grade. Like they could've gotten more if they've taken the test in the morning and said the afternoon or in an air conditioned room instead of a hot room or after they ate a meal because they were hungry.

Speaker 3:          14:35          So there's one certainty on both those numbers and then you're taking the difference. If you think about statistically you have to the noise term, it's called the error term of that expected score model. So you're holding teachers accountable for the error terms of their students. Turns out there, it's not a very good model. It's noisy, very, very noisy. Now I don't know this because I have the algorithm because I looked, I tried to get it. I tried, I did a freedom of Information Act request for the source code, but it was denied me. I ended up talking to someone in Wisconsin. This is for the New York City, a virgin. There's versions all over the country. But I, I focus on the New York City version. And um, I eventually talk to the people that in Wisconsin, in Madison, Wisconsin who built that model and they explained to me that I would never get the source code for that because in fact, nobody in that, in New York City got the source code for that cause by, by the, the contract, it was stipulated that it was proprietary. So no one in the department of Education understood how these teachers were being evaluated. And yet the scores were being used to deny people tenure. So that's why, so I should mention the reason why I thought I might be able to get the source code through a Freedom Information Act is because the New York Post actually did get the scores with all the teacher's names through a freedom of Information Act request and they publish them all as an act of shaming the teachers with that scores.

Speaker 3:          16:00          And I thought, okay, maybe I can get the source code if you can get the scores, I can get the way the scores were made right now I couldn't. But here's the thing that happened. This really smart high school math teacher at Stuyvesant high school, Gary Rubenstein, he took those same numbers that the New York Post had gotten and he found teachers that had two scores for the same year. You can get two scores if you teach seventh grade math and eighth grade math. And he figured if they're both supposed to be like the final say on whether you're a good teacher, they should be consistent, right? So if you get a 75 for seventh grade math, you should get a 78 maybe for eighth grade math, right? Or a 72 so he plotted them and a scatter plot, it looks almost like uniform distribution, which is to say almost random.

Speaker 3:          16:49          It's actually 24% correlation. There's some signal in it, which is why I said the thing about like if you had 10,000 kids in your class, you might actually know whether your teacher brings up grades, uh, scores, but it's simply not good enough to hold someone accountable for that at an individual level. But we trusted it anyway. When I say we, I mean the Goe essentially because they didn't understand it. In spite of that, Sarah was soaked with Saki, who's pictured here was fired after she got a bad teacher assessment, 50% of which was her teacher value added model score in Washington DC in 2011 I should say it's 50% of our score, but it's more than 50% of the variants of the score. The big complaint of these, of the way that teachers were assessed was that everybody got the same grade from their principal. The principals would categorize them all as acceptable teachers. So they were like, we need some spread in the scores so we can like rank the teacher isn't refined the worst 2% so they got spread. But the question was, is it meaningful? I climb, it's not meaningful from the, from the scatter plot we just saw, but it was particularly irksome for Sarah because guess what? She's teaching a fifth grade, a bunch of four fourth graders came in with very high scores at the end of fourth grade who couldn't read or write.

Speaker 3:          18:10          And it turns out that in the Washington d c a district Michelle ree had instituted both carrots and sticks. You get bonus if you get a high value added model score or high teacher assessment, you get fired if you get a bad one. So moreover she found out that this school where some of her kids came from, had exceptionally high eraser a ratio rates for their end of year standardized tests. So she has reason to believe that the previous teacher is actually cheated on the test. And if you think about what that would mean, those kids get elevated scores and they're expected to keep them elevated at the end of her class. But she just got them. She got, she taught them well, but they didn't get ridiculously high scores, so she got punished for the previous teachers cheating.

Speaker 3:          18:57          I should mention that she got a new job a couple of days after she got fired in an affluent suburb of Washington DC because most of the, most of these, this is where the value added model is being used. I think it's actually more than that at this point. Mostly an urban school districts. So that brings me to the, the failure. Okay. Remember I said there's, there's failure on, well, let me just go back over the three things. It's widespread secret and it's destructive at an individual level. So we saw Sarah getting la losing her job, and at a systemic level, I claim that the value added model does not get rid of good, bad teachers. I remember the whole point was get rid of the bad teachers, then we'll solve the problem with education. We're not getting rid of bad teachers. Were getting rid of good teachers.

Speaker 3:          19:45          We got rid of Sarah, which was just dumb, but they're talentless teachers who have quit, who have retired early, who have not gone into teaching. We have a national teacher shortage or have fled to the suburbs where they don't use this arbitrary and punitive regime for teachers. My next example comes from the world of personality tests. This is Kyle beam. He was a college student in the Atlanta area. He wanted to get a job after school at a grocery store, Kroger's and he was a, he took a personality test. He failed that. Most people never find out they failed it, but he found out because his friend worked at, Kroger's told him. Um, so he was unusual in finding out that he got red lighted from this algorithm was, which was built by Kronos. I'm a small, big data company actually in this neighborhood. The other thing that was unusual about Kyle is that his father's a lawyer.

Speaker 3:          20:42          Most people that are are are applying for minimum wage jobs, which this was do not have that are lawyers. So his father said, why? Well, what were the questions like on this test? And Kyle said there were a lot like the questions I got at the hospital when I was being treated for bipolar disorder, the five factor model it's called. And his father said, well that's illegal under the Americans with disability act, it's illegal to have a health exam including a mental health exam as part of a a job application. And he said, well can you apply to other places? Kyle ended up applying to six other places, Lowe's, yum foods, which owns Taco Bell, other big companies. They all had exactly the same personality test and he failed all of them. So his father is suing those seven companies on behalf as a class action lawsuit on behalf of anyone who's ever taken that test on the grounds that it is illegal, it violates the American's with disability act. So that's widespread. It's secret, I should mention personality tests are not new. Radio Shack, old, old sort of easily gamed personality tests looked a lot like this. Agree or disagree. I'm always happy. I think your PR, I think your employer wants you to agree with that. These are the more recent ones, much harder. What do you agree with more? I sometimes get confused my thought by my thoughts and feelings or I don't really like it when I have to do something I haven't done before.

Speaker 3:          22:13          Imagine doing 50 of those. So it's, it's absolutely inscrutable.

Speaker 3:          22:22          So it's secret and I would say it's destructive. It's destructive for Kyle because he didn't get a job at any of those big companies he applied to in the entire area of Atlanta. But it's also destructive in a larger sense if what we're worried about is true, that they're actually sort of systematically denying employment to an entire subpopulation. Um, people with mental health problems. Um, it's not relegated to minimum wage work. This idea of having algorithms that sort through resumes. Do you guys know who this is? This is Roger Ailes. He recently got kicked out of Fox News. He led Fox News for a long time. I got kicked out for like six basically sexually harassing women and keeping them from Prenay promoted. So it's doing a thought experiment. I don't know if Fox News uses an algorithm to hire people, but I'm just imagining that they do play along with me.

Speaker 3:          23:16          What does that algorithm taken? Well, it takes in data probably historical data from Fox News, so all the history of people who've applied to be, say anchors at Fox News and then it takes a definition of success. What does a good employee at [inaudible] successful employee at Fox News look like? Let's just say, let's stipulate a successful employee at Fox News stays for three years. It gets promoted at least once. Okay. That's typical for what, how these are are, are designed and how they're trained. So if you think about what that means, the algorithm that if you've trained it on this, a lot of historical data, what it will do is given a new set of applicants for our anchor job, it will say, who along these new applicants looks like someone who was successful in the past? And I would not be surprised if it filtered out women because not only did women, not, not only did women not necessarily get that job as often, but when they did get the job, they were systematically pushed out by Roger Ailes and the entire culture there.

Speaker 3:          24:21          Does that make sense? So the point here, and this is a very important point, is that we do not improve. We do not. We do not. Algorithms are not inherently objective or fair. All they are doing their, what they're really good at is picking out past patterns and repeating them, which is to say, if we had a perfect way of hiring people, we would want to codify that. We would want to automate that because it would save us money in time. But until we have a perfect way of doing this, all we're doing is we are literally caudifying past practices and propagating them.

Speaker 3:          25:04          My last example comes from criminal justice. There's actually two different kinds of algorithms that worry me a lot in the, in the criminal justice. The first is predictive policing. Uh, you guys know all about the black lives matter movement, which is objecting to two black people getting shot. But it's not just of course being shot, it's just the amount of over policing and uneven policing that's happening to poor black communities. Some evidence, there's um, whites and blacks smoking pot at a similar rates, white smoking pot a little bit more if they're young, but blacks get arrested a lot more, a lot more for that. In fact, depending on the jurisdiction, blacks can get arrested up to 10 times more often than whites for smoking pot. Okay. So the two things I wanted to take away from this are blacks. It's biased against blacks. And actually it depends very much on local conditions of how the police force is expected to act.

Speaker 3:          26:08          So when you hear about arrest records, I want you to think just as much about police practices as you do about crime, at least when it's nonviolent crime. And I do make a distinction between violent crime and nonviolent crime. The problem is that nonviolent crime is much more prevalent and it is much more predictable. Think about it. We have crime. We have people being arrested in this country for mental health problems when they're poor, not when they're rich. We have people being arrested for addiction problems when they're poor and we had people being arrested much more often for low level nonviolent crime like drug use. And for that, for that matter, for like crimes of poverty, just like literally not having a place to go to the bathroom. And that is, that goes into their arrest records. And the reason I'm mentioning this is because the way predictive policing works is very simple.

Speaker 3:          27:02          It's actually very stupid. It's Geo located arrest cr rest records. And the algorithm says, go put police where we saw crime in the past, which is really where we saw arrests in the past where we saw police arresting people for things in the past. This is just much as much a way of predicting the police as it is of predicting crime. I should just, I'm just reiterating that it might be different by the way, if we really focused only on violent crime, imagine that. Imagine a predictive policing algorithm where they only focus on violent crime like murder.

Speaker 2:          27:39          Okay.

Speaker 3:          27:40          The problem is that murder is really hard to predict. Like even if you did predict murder, would you like what Stan like outside of a house waiting for someone to get murdered? Like the problem is that the things that are actually easy to predict are things like poverty.

Speaker 2:          27:56          Okay.

Speaker 3:          27:56          So my claim is that predictive policing is more or less creating a feedback loop where you're having a scientific pseudo scientific basis for sending police back to neighborhoods that are already over policed. So that another way, another thought experiment, imagine that if after the financial crisis, all the cops had been told to go to Wall Street and the rest of the bankers and find out if they had cocaine in their, in their pockets because they all do not all of them. Then the predict, then the police records, the police data, the data, which is just a reflection of what police do, right? The data would tell them in these predictive policing algorithms, go back to Wall Street because that's where the crime is, but that's not what we did.

Speaker 2:          28:40          Okay,

Speaker 3:          28:41          so the next example in also in, in the criminal justice system is recidivism risk. This is a score. Recidivism by the way, is coming back to jail. So recidivism risk is the risk. Somebody comes back to jail. Recidivism risk scores are given to judges when they sentence criminal defendants. Now there's two kinds of data that goes into that. One is a rest records we just talked about a restaurant, they're very biased. The second is a questionnaire. The most commonly used recidivism risk algorithm is called the Lsir. And I'm going to show you a couple of the questions that are asked of the defendant in the Lsir.

Speaker 2:          29:21          Okay.

Speaker 3:          29:21          Number 29 do you live in a high crime neighborhood? It's a proxy for class and race because that is pointing to people who are already poor and black. And, and by the way, if you say yes or no to these things, it goes exactly as you imagine. If you say yes to I have a mental health problem, you're higher risk. Right. And Oh, I forgot to mention if you're a higher risk will a judge will sentence you to longer in jail.

Speaker 4:          29:53          Okay.

Speaker 3:          29:53          We've doesn't, I mean I mentioned that because it's not obvious that that's what you would do, right? Like if you're higher risk of recidivism you get put in jail longer. Um, is a little bit minority report ish because what you're doing is you're preemptively punishing someone for something they haven't done yet. But that is the practice that judges now have. Here's another set of questions. Um, have you been suspended from school number 17? I'll show you a applaud of just how much that is a proxy for race. Black girls and boys are much more likely to be suspended from school. But I think the thing that bothers me the most and I think should bother absolutely everyone is number 26 with somebody in your family in prison. This is something that in an open court would be thrown out by a judge as unconstitutional. If a lawyer said, your honor, please sentence this person to longer because their father was also a criminal. That is not how we do it, but because it's being embedded in a risk score, which has been claimed to be scientific, this somehow has the authenticity of mathematics and science and again is being used to send people to jail for longer.

Speaker 4:          31:11          Okay,

Speaker 3:          31:12          so I talk about failures. I should also mention this is important. It's being used in more than half the states. It's secret. People do not understand what they're, what they're getting into with these scores. And judges are actually very secretive about exactly how much weight they put on these scores. But my claim is that they are most destructive thing. You can imagine they create their own reality. If you're a higher risk, you're sentenced to jail for longer. And guess what? If you're sentenced to prison, if you're in prison longer, you don't tend to be to benefit from that experience. You end up out of jail. 97% of people eventually leave prison, I should mention. So this happens, but you end up with no resources, no connections to your community, very little wealth. You have a felony to your name often so you it's hard to get a job and then you end up back in prison partly because you got this high risk score so you were deemed high risk and then you end up back in prison. So I'm almost done. Those are my examples, but I do want to mention again that I don't hate data. I just really think we have not yet get started understanding what it means to build safe algorithms. It's like we are building cars and just putting them on the road without understanding that cars can kill people.

Speaker 4:          32:33          Okay.

Speaker 3:          32:34          So I want a data scientist to take their ethical responsibilities seriously, which means building some kind of ethical framework like a hippocratic oath for data scientists. I also think that we need to learn how to scrutinize these algorithms to monitor them, to audit them for safety, for safety, for fairness and discrimination and for meaning. Making sure that we're actually building meaningful things with the teacher value added model was not meaningful. And I also think that in situations where it's very, very important to a given person's life, how they're being scored and they should have the right to scrutinize that score. Like the teachers, like if they are being given a score that will make maybe make them fired, they should be able to understand exactly how that scoring system works. Because to be clear, the teachers were not, even when they appealed their score, they were not told how they were being actually evaluated. And that's not right.

Speaker 2:          33:33          Thank you guys.

Speaker 3:          33:40          If you have any questions, I'm here.

Speaker 5:          33:41          Hi. Um, what's the best way to account for and mitigate limitations in a model? So suppose you, you have to come up with a mathematical model to affect somebody's life. Um, how do you figure out where the boundaries of application are and how do you, I guess iteratively readdress that to see if it's doing what you expect it to?

Speaker 3:          34:03          I mean, it's really a vague question, so it's hard for me to answer I think. I think it's like, I'll tell you how, how frustrating it is as a data scientist. I, I'm a data scientist. Like I worked in the City Hall of New York and I was asked to use the data. They had to figure out how long a family was going to be in homeless services.

Speaker 2:          34:26          MMM.

Speaker 3:          34:27          And I had race, I have the number of children. I had, you know, out how many times, whether the children, whether the parents had been in social services. I had all this data, but they weren't telling me, they weren't telling me how they were going to use this algorithm if this scoring system once I had it. So in particular, one thing I didn't know whether I should use was the attribute race or all the other things that are proxies to raise or should I d correlate those other things from the, the, the race. How was this going to be used? Number one possibility. Um, if you were high risk, then you're going to be put into worse housing. If you were like, you know, expect it to be longterm housing, you're going to be put into wars housing. If I did that, if I, if I knew that that was how it was going to be used, then I would know that people at higher risk, there'll be a disparate impact racially.

Speaker 3:          35:15          Right? But if I, if instead it was being used to sort of figure out interventions, um, so that, you know, black family, they were, maybe they wanted to understand why black families were in homeless services for longer and try to intervene to try to make that discrepancy smaller than it would by all means make sense to have racist the attribute. Does that make sense? So like basically not answering your question, but I am making it clear that you cannot answer that question until you really know the use case for this algorithm. And I should also add that a given algorithm could have a positive or a negative effect on the world. It's really tricky. These things are, let me give you an example. Um, health, everyone's talking about how great it is that we can predict each other's health. Well that's great. If your doctor has that algorithm that can keep you well, but it's not great if an insurance company has an algorithm.

Speaker 3:          36:15          We could charge you more if you're about to get sick or if Walmart, and I'm not saying this is happening, but if like Walmart puts a health risk on top of everybody who's applying to their job and says, we don't want to spend money on our people's insurance, so we're going to not hire people with high health risks. I mean, I'm just saying same algorithm, different use cases could be used for good or bad. And so the answer, the answer is if there is no rule here, these are decision making algorithms or decision making processes that are as complicated as anything in the world. So we can't, we can't pretend that there are like formulas for how to use them and how to make them safe. They're very, very contextual.

Speaker 6:          36:56          Uh, do you have any sense of, um, this is probably a naive question, but, um, whether the scale of the problem is bigger or smaller in a country with stronger libel and slander laws, like for example, invalid.

Speaker 3:          37:14          Say more. Why would you imagine that?

Speaker 4:          37:18          Oh,

Speaker 6:          37:19          well why would I imagine it would be different? Yeah. Uh, well I guess the people who are victims probably don't have the resources to pursue slim, slender cases anyways. Okay.

Speaker 3:          37:29          I think it was one of the very important point. Thank you for making it. Which is that one of the, one of the commonalities of almost all these algorithms is that the people who are losing by these scoring systems are often the most vulnerable people in our society. So they do not have, generally speaking, they don't have lawyers to protect them. Right. Um, but I would also say the following, liking, anonymized, anonymization, which is a tool that people often bring up as like a way of solving some of these problems is not really a solution to me. Let's think about the medical, the medical model I just mentioned. Like let's say I'm Walmart and again, what Walmart's not really, I'm a large employer and I get longterm health data off of all my employees because I forced them to use fitbits or whatever I do.

Speaker 3:          38:18          Right. I get long term health outcomes and I don't charge them. I'm their hardy, my employees, I treat them well. Right? But the point is I can build the algorithm, I can build the neural network, what have have you, I can train it so that when someone applies my job, I only ask have to ask them six questions and I've already categorized, I've segmented them into a, into a risk group score and it is completely anonymous. In other words, you can build an algorithm anonymously. It's still applying it to someone in a very precise way and it still can be problematic. So it's really not for me, it's not about, um, anonymity.

Speaker 7:          39:03          So in a lot of the reporting that I have read about this sort of thing, and I'm thinking specifically, uh, for example, the pro bowl get along thing on the, um, criminal justice sentencing thing. They, I haven't seen a lot of talk about how these algorithms perform relative to, in this example, just a judge without this, you know, score or what the preexisting case was. Yeah.

Speaker 3:          39:27          Great Point. And I. Dot. I'm desperate. Desperate for that. Right. Because the point is that the justice system is very racist. Alrighty. This, the intention of this recidivism risk stuff was to make judges more objective and less racist. And it might be doing that. Like with all the flaws that I just mentioned, which I really believe in, it might actually still be better than what we have already. We do not have data for that. And the Department of Justice is not coughing up that data. I've tried, I've tried to get, I've tried to get data to do that. An audit of the DOJ. It should be, it's possible they did exist because some jurisdictions have this stuff in, in, in use and some of them don't. You could just compare them before and after or whatever and compare to each other. Um, I should also, but I should also add that like, here's the good news, like if we made those recidivism risk algorithms actually not racist, then that would definitely be better than the current judges. We just, it's like we threw them out there and we're just like, oh, they must be good. Because their algorithms, they're by definition good. No, they're not. They're not necessarily good. But again, you're right. They might be better than the existing system.

Speaker 7:          40:40          So in the other examples that you talked about, do you have similar sense of whether that data is out there like the teachers and you know, and maybe the teachers, is that right?

Speaker 3:          40:51          The teachers are, it's like, it's just terrible.

Speaker 7:          40:53          You're not even trying to solve the right problem.

Speaker 3:          40:55          You're not solving the room. And I don't think, by the way, I still don't think recidivism risk being high shouldn't necessarily mean you go to jail longer. Right? Maybe we should be like, why? How do, why do we sentence people the way we send his people? Like we should maybe make a Redo of that entire conversation. Um, like we should, if we were thinking datadriven Lee, if we're like, if we're Google thinking people like we should ask really basic questions. Like to what extent does GED training in, in a prison help people when they leave? To what extent does solitary confinement help or hurt people or sexual assault like should actually know what these things are doing to our final outcomes, which hopefully is a combination of public safety and the wellbeing of the actual prisoners. We don't have any of that.

Speaker 4:          41:45          Yup.

Speaker 8:          41:50          I entirely agree with your, uh, uh, or was he a wonderful notion that, that uh, that some of this data is disparate? I mean it hurts, hurts, hurts, vulnerable communities. But despite that, um, like the, the, the thing you most of the attention to with was the fact that, uh, people whose relatives are criminals are judged more likely to commit crimes. Well, people whose relatives are actors are more likely to become actors. People's lives was a plumber's, I'm more likely to become plumbers. And even though it's a horrible thing, I strongly suspect that someone who's who is someone whose parents grew up by stealing cars. No. A heck of a lot of us. It more about stealing cars and someone's who's Don,

Speaker 3:          42:26          what do you suggest? I suggest we follow the constitution, which privileges justice over anything like that.

Speaker 4:          42:35          Okay.

Speaker 8:          42:35          So just the way our constitution says written, send it, send the police to Wall Street because it's equally likely to have muggings

Speaker 3:          42:43          culture. We decide what is, what is against the law, but as, but the constitution declares that we have to care about fairness to the criminal of above all. And that this is not fairness to the criminal by saying you're implicated because your father was implicated. And I just, I also want to add that like I lived next to Columbia University and every couple of years there's a huge drug bust in the, in the fraternity's, right? Like a huge drug bust.

Speaker 4:          43:10          And then, and then

Speaker 3:          43:11          I followed the Nyp d and they, they're constantly boasting about their datadriven criminal justice stuff and how they peg people as gang members if they associated on social media with gang members. But those gang members are always in Harlem and they're black. You see what I mean? They're not Columbia students. So it's also an inconsistently defined association is, so there's two different kinds of problems. Okay. All right. That's it. Thank you.

Speaker 4:          43:41          Yup.

Speaker 9:          43:44          So I think it's, uh, it's very easy to see that if you're not careful about how you define what kinds of success you're optimizing for, you wind up with an algorithm that, um, very strongly reinforces the status quo. And if we're looking at things like our hiring practices fair, then if there is existing on fairness, we're going to perpetuate it. So that, that makes total sense to me. What, what I struggle with is if I'm asked to do something like that and I try to not, uh, incorporate the existing, um, structural biases into the algorithm, it's going to be my, do you know, recommended hiring practices thing? How do I deal with the challenge that I'm actually optimizing away from success conditions?

Speaker 4:          44:41          Yeah.

Speaker 3:          44:42          Okay. So there's two strategies and I suggest the first one is, um, basically a parable. Um, so there's this, they think all the blind audition for orchestras, right? Where the orchestra were trying, they acknowledged that they were being nepotistic and their practices of hiring. So they wanted to get rid of their nepotism. So they decided to put a sheet between their judges and the auditioner so they wouldn't know if the person who was behind the cause, she was their friend. Um, at first they saw the shoes and then they're like, wait a second, we can see if it's a matter of woman. So they brought the sheet down to the ground and they also installed rugs in the hallway walking up to the spot so they couldn't hear. If it's high heels are not, not only did they get rid of nepotism, but they increase the number of women in orchestras by a factor of five.

Speaker 4:          45:27          Yeah.

Speaker 3:          45:27          Um, I would, so that's the story. And like the character, the way I would characterize what they successfully did there is to two things. First they decided our priori, what was it that was a requirement for this job? How do you assess somebody for this job? And it was, the answer was sound. The second thing they did, which was absolutely as important as the first thing was to ignore everything else. The promise of big data that we get confused by because it sounds really convincing. What's wrong? The promise of big data is the more the data is, the better. Just throw all this data at the wall. Correlations are just as good as causation. Um, not true because if we did the orchestra example again, but we didn't have the sheet and we thought to ourselves, we're looking for a good sound, but we also knew that it was our friend or we also knew that it was a woman that creeps into us.

Speaker 3:          46:25          It creeps into our brain. That's excess information that we should be ignoring but we're not ignoring. So the first strategy I would suggest for hiring people is to think opera, right? To actually build a model that you buy, buy, buy construction is fair. So build the model and say, these are the things we actually want for this job. The second possibility is use machine learning algorithm that takes all these correlations of blah, blah blah. But then audit it for fairness than say, wait, does this basically filter out women check, check to see if women are filtered out. Check to see whether they're filtered out in a reasonable way. Sometimes they are. Sometimes sometimes you like, I've heard the story, I'll go on fee on foot or just the other day we're way more Asian people were filtered out of a certain job. Um, and people were complaining because so many of the applicants were Asian, but then people were saying, well, the people who they were more Asians that were applying that weren't actually qualified for this job. Like it's complicated and expect it to be complicated. That's my third suggestion is that the, the other promise of big data that I'm pushing back against and I hope you understand and agree with, is that big data is not a silver bullet. It's just a tool and is not automatically going to solve our problems. It's just a tool that we might be able to use to solve some of our problems, but we have to be careful about it. We have to check.

Speaker 10:         47:53          That's stories where you would go to a judge or a jurisdiction or a school system and they would say, oh my God, you're right. We have to fix this. And they do. Sorry, what, do you have any other success stories like the orchestra where you might go to someone and say, you know, what was this problem? And they said, oh my gosh, as opposed to, no, you can't see our data now. That's proprietary. Go Away. Don't bother us.

Speaker 3:          48:16          No. You'd be surprised how few people want me to see their source code. Um, although I should say that like someone asked me whether I thought it would just be impossible to ever get a good value out of it. Like a good teacher assessment tool using data and I said I'll never say never because like 10 years ago I didn't think we'd have self driving cars but I think now that we're going to save lives from drunk driving with self driving cars, but what is it going to take to get a good teacher assessment tool is w we're going to have to have a lot of evidence that this is actually working right now we have no ground truth for teacher value added model. Literally, there is no comparison to some other kind of qualitative assessment to see whether a teacher's good, a good teacher is getting a good score and a bad teacher getting bad scores.

Speaker 3:          48:59          It's just whatever the number is is your score, which is ridiculous, right? So we actually do have qualitative assessments for teachers, their political, because different people disagree about what makes a good teacher. But like let's say we had a agreement, we're going to, we're defining this kind of qualitative assessment to be important, to be the, the, the ground truth. And then we could try to find, you know, maybe we put a bunch of sensors in the classroom. We tried to replicate that using data, literally replicate it. And if we got an algorithm that after 4,000 teachers got almost exactly the same score using the qualitative assessment is using this data driven assessment, then we would build trust that this data driven assessment is doing a good job. It's very similar to saying, you know, we have this many miles of a self driving car without an accident. So we start trusting that car, that algorithm. Does that make sense? Well, right now we have nothing like that in the, in the field of teacher value. They just want a silver bullet. That's all they want. They don't want any prying eyes either.

Speaker 11:         50:00          Something that's kind of come up a bunch is uh, sort of as an undercurrent, like the idea of open sourcing, how you're doing things. And I can see that like potentially being really helpful. For example, now you can actually criticize because you can see the code, but I can also see it being somewhat ineffective if you're still doing, if you're still asking questions that are totally beside the point. Um, so I, I guess I'm curious your thoughts as

Speaker 1:          50:28          to how the effectiveness of things like open sourcing and also the limitations.

Speaker 3:          50:34          Um, that's a great question and you guys know that it's hard, um, because if, because it's religion, it's relatively easy to make something arbitrarily difficult to understand. If you know in advance that it's going to be open source, you can just make it impossible to understand. Um, and like the other example I give is like nobody would want the code for Google search because literally it probably will not work on any computer system except Google's computer system. Right? So there's a limit to what open source and can do. But I I don't that, that's not to say I think we should give up. I think what we should ask for is an auditing, a fairness auditing. And I think we should think about along the lines of the way sociologists auto things for fairness. So like sociologists will see whether a hiring practices racist by sending a bunch of applications with black names and white names and like similar kind of qualifications and see whether white people get more call backs so we can do that kind of thing to algorithms.

Speaker 3:          51:28          And Latanya Sweeney did that for Google search. Do you guys know that example with Latanya Sweeney, Google's her own name and she found the ad next to her. The search results was are you looking for the criminal arrest records for Latanya Sweetie? Then she googled the white name. It didn't happen. Then you did a comprehensive task cause she's a computer scientists and found that black names were way more likely to have a rest records act to them. This ad for arrest records. Now that's not in my opinion of what a weapon of math destruction because it's not as directly impactful in people's lives, but it's certainly not. Not Great.

Speaker 12:         52:03          Go ahead. Thanks. Um, I was just going to agree with you before when you were talking about there is no easy way to do the teacher evaluation. I think it goes back to your point of success, right? Cause is it we then it gets into like student evaluation and our, you know, the assessments that they do on students fair or is that just a proxy for success in long term and their long term career? So I think the teacher evaluation, there's probably no right answer.

Speaker 3:          52:28          Um, do you agree? Yeah, it's, it's a very tricky problem. So it's, I think the trickier, the question is that much the less likely it's going to be solved by some simple scoring system.

Speaker 4:          52:40          Yeah.

Speaker 1:          52:44          A lot of the machine learning I have mustard are very popular these days of written. It's like neural network switch. Not that I designed. There are pig, you know, without even batting better tensions. Like you know, if you click on the little white button about why it made it give a decision, this is say I did these 3000 matrix multiplies and there's no simpler answer than that. Uh, but they do give much better predictions then other things like decision trees, which are much more interpretable. Do you have any thoughts about how in the context of these problems to, should we, you know, do you have any deliberately worst predictions because we need them to be in trouble?

Speaker 3:          53:15          Yes, absolutely. Okay. That's probably the hardest thing that you guys are gonna hear from me today, but I definitely think we need to sacrifice accuracy for fairness. Absolutely. And one of the reasons is because we as technologists, we are not the ones that are at risk. So our, our concept of what looks fair to us not good enough, right? We might have some better understanding of how things work, but we're like, oh, but it makes it so much more accurate. Um, and I actually, I talked to somebody who does recidivism risk algorithms for a living. He does it for this for a state. Um, and I said, well, do you ever use race? And he was like, oh no, that would be wrong. And I said, well, do you ever use zip code? He was like, well, it's a sometimes because it makes it so much more accurate.

Speaker 3:          54:01          And I'm like, okay, but that's a proxy for race. So you're basically using race. Um, so another way of saying that is if it's interpretable, then people, it's much more easy for other people to say that's not fair. It's a transparency measure. There is precedent for this, by the way, if in credit card law, if your credit card company and so you denied someone a credit card, you have to be able to explain why, which strictly restricts people for two using decision trees for the most part. Um, they don't like it, but that's okay. There's actually lots of things about lending that are a trade off for fairness and accuracy. So Fico scores by law and discrimination laws called freedom, uh, fair credit reporting act and the equal credit opportunity act make it illegal for you to base fico scores on race or gender or a zip code.

Speaker 3:          54:58          Is that because it's La it's more accurate when you restrict know like it would, it's, it's actually, it's keeping these credit scores for being as accurate as they might be, but it was deemed more fair that way. As a society we, we, we care about public, the public, right? So the idea is, and this was, this was soon, like the basic, what's happened in the 70s when like, especially in a divorce, women were being denied loans. And the idea of there was if you'd never let women get loans, then they're never going to be able to build up their credit scores. So there'll be credit worthy. So it's like a feedback loop, right? And so this was, these antidiscrimination laws were specifically designed to prevent the feedback loop. Um, and we, we need that, we need that kind of thing. Because algorithms like that algorithm I just talked about that benefits the, the low the banks or the people that make loans to the detriment of the public. So we have to balance the benefits of the for the public versus the benefits for the private companies. I have time for one question.

Speaker 11:         56:10          Sorry. I was going to say I'm in the opposite boat where I work on a search feature that looks at lots of queries and uh, we have a model for English where, uh, the masculine pronouns are the, and some common names are the default to it. The model knows a lot about that and we're kind of wondering if queries that women like we do terrible on or worse or there's some class except we don't know anything about the people who ask the queries. We've tried really hard not to know, I don't know gender, I don't know, income. I could probably get zip code in bulk. So, um, I'm sitting here, I'm trying to think how I'd write a machine learning model that would do the reverse question and figure out what's the most unfair thing my current system is up to.

Speaker 3:          57:00          That's a good question to be asking yourself, but I would also add, and thank you for asking that she herself, I think that's what we all need to ask because one of the first questions we should be asking ourselves, but I do want to, you made the point yourself, but I'll just reiterate it and something that I'm certainly not the first person. It makes this point. Sometimes you actually do need to know these attributes like race and gender in order to measure your fairness. So I understand the desire to be race neutral or gender neutral, but that doesn't mean you should not collect that data because then you're basically saying, I'm not going to look, you know, like it must be fair because I don't, I don't collect that data. That's not proof that it's fair.

Speaker 4:          57:42          Okay.

Speaker 3:          57:42          Which isn't to say that you, you know, it's easy to collect the data. Okay. I'm saying maybe collected at least in and for some experiments as an audit. Thank you guys.

Speaker 4:          58:01          Okay.