1
00:00:08,200 --> 00:00:10,150
I'm very excited today
to introduce to you,

2
00:00:10,360 --> 00:00:14,620
Cathy O'Neil will be speaking to us about
her book, weapons of math destruction.

3
00:00:15,340 --> 00:00:16,173
Um,

4
00:00:16,690 --> 00:00:21,640
Kathy is uniquely welcome to talk about
the subject of mathematical models and

5
00:00:21,641 --> 00:00:25,690
how they can go bad. Uh, she has a
phd in math from Harvard University.

6
00:00:26,200 --> 00:00:29,830
She has held positions at
Barnard college and Mit.

7
00:00:30,430 --> 00:00:34,130
She's worked at the Hedge
Fund de Shaw and uh,

8
00:00:34,210 --> 00:00:37,360
also as a data scientist
at various startups. Um,

9
00:00:37,810 --> 00:00:41,750
she also is active in the occupy
occupy Wall Street movement, uh,

10
00:00:41,830 --> 00:00:44,350
especially in their
alternative baking group.

11
00:00:45,070 --> 00:00:49,060
She blogs@mathbabe.org. It's a great
blog. You should check it out. Uh,

12
00:00:49,120 --> 00:00:53,870
as well as uh, participates in
the slate money podcast. Um,

13
00:00:54,070 --> 00:00:55,200
and she is the author,

14
00:00:55,210 --> 00:01:00,210
coauthor of two other books doing data
science and on being a dentist and being

15
00:01:00,481 --> 00:01:04,450
a data skeptic.
Please join me introducing Cathy O'neil.

16
00:01:09,920 --> 00:01:12,990
Hi Guys. Thank you so much for coming. Um,

17
00:01:14,570 --> 00:01:19,080
so I'm a math nerd. I love math. Uh,

18
00:01:20,750 --> 00:01:24,650
I was attracted to math in high
school because it was true.

19
00:01:25,340 --> 00:01:27,890
It was beautiful.
Like you can disagree with somebody.

20
00:01:28,340 --> 00:01:32,300
I'm a politics and everything.
About what manifest destiny really meant.

21
00:01:33,230 --> 00:01:35,630
But you'd have to agree on
math because you guys were,

22
00:01:35,690 --> 00:01:39,260
you'd be so careful with your assumptions
on the, in your logical arguments.

23
00:01:40,040 --> 00:01:42,410
I thought it was something,

24
00:01:42,530 --> 00:01:46,160
wouldn't it be beautiful if we could
sort of port this honesty and truth and

25
00:01:46,161 --> 00:01:47,210
clarity to other,

26
00:01:47,960 --> 00:01:51,200
other fields of the real world.

27
00:01:51,890 --> 00:01:56,810
So after I became an assistant
professor at Barnard College,

28
00:01:57,680 --> 00:02:01,840
I realized that it wasn't the right pace
for me, that I wanted to have like more,

29
00:02:01,910 --> 00:02:03,140
more to do with the real world.

30
00:02:04,040 --> 00:02:07,220
And it was 2007 so I did
what everybody else does.

31
00:02:07,221 --> 00:02:10,670
And I went to a hedge fund,
um,

32
00:02:11,010 --> 00:02:14,180
and quickly realized that,
well,

33
00:02:14,181 --> 00:02:19,050
the world blew up almost the moment
I stepped into the hedge fund, um,

34
00:02:19,280 --> 00:02:23,780
was working with the
experts like Larry Summers.

35
00:02:23,780 --> 00:02:26,470
I worked with him on a couple of projects.
Um,

36
00:02:26,480 --> 00:02:30,740
it was very disillusioned by what the
experts seem to actually know about what

37
00:02:30,741 --> 00:02:31,574
was going on.

38
00:02:31,850 --> 00:02:36,650
But I was most disillusioned and actually
ashamed to be honest about one of the

39
00:02:36,710 --> 00:02:39,560
most fundamental reasons that
the financial crisis happened,

40
00:02:39,770 --> 00:02:43,070
which was the AAA ratings on
the mortgage back securities,

41
00:02:43,310 --> 00:02:45,590
which I consider a mathematical lie.

42
00:02:45,860 --> 00:02:49,940
It was the opposite of how I had fallen
in love with mathematics as something

43
00:02:49,941 --> 00:02:53,330
that would bring forth clarity.
It was the, it was really,

44
00:02:54,170 --> 00:02:58,340
it was hiding corrupt
practices behind mathematics,

45
00:02:58,930 --> 00:03:00,220
like trust the math.

46
00:03:00,730 --> 00:03:05,290
Be impressed by the fact that we have a
bunch of PhDs in math who are supposedly,

47
00:03:06,090 --> 00:03:09,340
you know, carefully going through
this data to dump double check,

48
00:03:09,341 --> 00:03:13,840
triple check that this, these mortgage
backed securities are very unrisky.

49
00:03:14,380 --> 00:03:19,150
Trust us, there's math here
and, and invest in these,

50
00:03:19,750 --> 00:03:23,020
in these, uh, these
very opaque instruments.

51
00:03:23,950 --> 00:03:25,390
And I'm not saying that's
the only thing that happened.

52
00:03:25,391 --> 00:03:30,190
Obviously the mortgage broker is
like made a bunch of bad mortgages,

53
00:03:30,760 --> 00:03:32,880
but the reason that they
were able to create,

54
00:03:32,950 --> 00:03:35,450
like build this machine
and scale it to the,

55
00:03:35,560 --> 00:03:39,640
to the scale of the massive international
scale that it became was largely

56
00:03:39,641 --> 00:03:42,070
because of the AAA ratings,
because of the trust.

57
00:03:42,071 --> 00:03:46,720
And that was trusted mathematics.
And it was abused. I'm willing to,

58
00:03:46,900 --> 00:03:51,490
uh, I left finance after
a small stint in risk,

59
00:03:51,820 --> 00:03:55,440
which I could go into if you guys
are interested in the Q and a, um,

60
00:03:55,540 --> 00:04:00,460
where I was equally disillusioned by my,
the part I was playing in particular, um,

61
00:04:00,490 --> 00:04:04,610
in in risk. Um, and I
left finance altogether.

62
00:04:04,650 --> 00:04:07,260
I started my blog, math babe. Um,

63
00:04:07,300 --> 00:04:12,300
the idea was to sort of expose corrupt
practices and mathematical abuse as I saw

64
00:04:13,451 --> 00:04:16,780
it, we should happen to finance. In
the meantime, I needed a day job.

65
00:04:16,781 --> 00:04:19,810
So I became a data scientist very easily.
I should say.

66
00:04:20,140 --> 00:04:24,870
I just renamed myself a data scientist
and got a job the next day, um,

67
00:04:25,390 --> 00:04:26,710
because,
well,

68
00:04:26,711 --> 00:04:30,760
because I was qualified because what
do you do when you're a data scientist?

69
00:04:30,761 --> 00:04:34,450
You predict people instead of predicting
markets at had been predicting markets,

70
00:04:34,660 --> 00:04:37,540
futures and credit
default swaps and stuff.

71
00:04:38,050 --> 00:04:41,080
So it wasn't that much of a stretch to
predict people. In fact, it was very,

72
00:04:41,081 --> 00:04:46,081
very simple and I wanted to think
that what I was doing was like less

73
00:04:46,181 --> 00:04:49,990
destructive than what I'd been,
how I've been feeling and finance.

74
00:04:51,010 --> 00:04:52,840
But what I realized pretty quickly,

75
00:04:52,870 --> 00:04:57,460
within a year or so is that I was
just very much part of something, um,

76
00:04:57,700 --> 00:05:02,380
that I still considered, um, not,
not benign, like actually negative.

77
00:05:03,130 --> 00:05:08,130
And what I realized that was that the
difference between the sort of the,

78
00:05:08,660 --> 00:05:13,660
the havoc that was that was created by
the financial crisis where everybody had

79
00:05:14,291 --> 00:05:19,000
noticed and the financial systems of
the entire universe of the entire world,

80
00:05:19,001 --> 00:05:23,560
we're sort of at risk. And everyone
was worried was that the Christ,

81
00:05:23,590 --> 00:05:28,010
the sort of the, the, the
destruction that was being, um,

82
00:05:28,270 --> 00:05:33,270
that was the havoc that was being
wreaked and the data science world was

83
00:05:34,331 --> 00:05:36,400
happening at the individual
individual level,

84
00:05:36,480 --> 00:05:41,480
that individuals were being lost or they
were being deemed losers or winners by

85
00:05:43,031 --> 00:05:46,450
this stuff. And Moreover, what
was most disillusioning and,

86
00:05:46,451 --> 00:05:50,440
and kind of frightening for me because I
was still kind of idealist, idealistic,

87
00:05:51,130 --> 00:05:53,830
was that these were all being,
again,

88
00:05:54,430 --> 00:05:58,490
they were being sort of
marketed as objective and fair.

89
00:05:58,640 --> 00:06:02,960
These algorithms, um, when in
fact they were not that they were,

90
00:06:03,150 --> 00:06:07,130
their algorithms are nothing more
than opinions embedded in code.

91
00:06:07,670 --> 00:06:10,310
So let me give you an example.
And you guys are technical,

92
00:06:10,311 --> 00:06:13,310
so you'll understand this quite well.
This is my son.

93
00:06:15,170 --> 00:06:18,980
I like to give the example of what is
an algorithm by using the example of my,

94
00:06:19,520 --> 00:06:24,260
my own internal algorithm for building,
for cooking dinner for my family.

95
00:06:25,520 --> 00:06:29,300
So an algorithm is that has
essentially has two big choices,

96
00:06:29,301 --> 00:06:30,760
lots of little choices,
which I'll ignore.

97
00:06:30,780 --> 00:06:34,760
But the big choices are the data that
you train your algorithm on and the

98
00:06:34,761 --> 00:06:36,950
definition of success or
the objective function,

99
00:06:38,000 --> 00:06:41,720
which also includes your penalty for
mistakes, but ignoring that for now.

100
00:06:42,110 --> 00:06:46,530
So what, what, what's the, what's the data
going into my algorithm for cooking? I'm,

101
00:06:46,540 --> 00:06:50,900
yeah, well, it's the, the, the
food I have in my kitchen, uh,

102
00:06:50,920 --> 00:06:54,860
the time I have the ambition I
have, by the way, I should say,

103
00:06:54,861 --> 00:06:58,160
I curate that already. I
curate, I carry the food.

104
00:06:58,220 --> 00:07:01,100
I do not consider ramen noodles
food. My, my teenagers do.

105
00:07:01,300 --> 00:07:02,510
I have teenage sons as well.

106
00:07:03,260 --> 00:07:07,670
So I'm already imposing my
definition of food. Okay.

107
00:07:07,880 --> 00:07:12,440
And then the definition of success for
me for dinner is whether my kids ate

108
00:07:12,441 --> 00:07:17,330
vegetables. Now my seven year old, if he
were in charge, this guy right here wilfy,

109
00:07:17,720 --> 00:07:20,450
he would define success for a meal
as if he got a lot of Nutella.

110
00:07:21,740 --> 00:07:26,060
And the reason this matters is because
over time we optimize our algorithms for

111
00:07:26,061 --> 00:07:28,100
success. I mean that's
why we define it, right?

112
00:07:28,101 --> 00:07:32,330
We want to get closer and closer to
that, that success. So over time,

113
00:07:32,480 --> 00:07:35,900
that means I make more and more meals
that have been successful in the past.

114
00:07:35,901 --> 00:07:40,850
I train my algorithm to success. That
is a very different succession of meals.

115
00:07:40,851 --> 00:07:45,140
And my son's algorithm would be if we,
if we optimize on the tele.

116
00:07:46,040 --> 00:07:49,670
So that's me, that's me imposing
my agenda onto the algorithm.

117
00:07:50,210 --> 00:07:51,110
We always do that.

118
00:07:51,770 --> 00:07:56,770
There is no such thing as
objective algorithm because
the very least the person

119
00:07:57,201 --> 00:08:01,550
building the algorithm defined success
and that's usually success for them.

120
00:08:02,180 --> 00:08:06,200
But there's often other stakeholders in
the, in the play. And the question is,

121
00:08:06,260 --> 00:08:09,170
is that success for the stakeholders
as well? It's not always

122
00:08:11,660 --> 00:08:12,890
it.
Generally speaking,

123
00:08:13,730 --> 00:08:18,380
the patterns I've found looking through
more and more data science was that we

124
00:08:18,410 --> 00:08:22,220
were more and more talking like marketing
these things as fair and objective and

125
00:08:22,221 --> 00:08:23,960
following the numbers and mathematical,

126
00:08:24,410 --> 00:08:29,240
but there were people behind it who are
building the definitions of success,

127
00:08:29,241 --> 00:08:30,290
these objective functions,

128
00:08:30,470 --> 00:08:33,740
and they were essentially invisible
behind that mathematical shield.

129
00:08:34,400 --> 00:08:37,040
A lot of these algorithms
were benign or fine or great,

130
00:08:38,090 --> 00:08:41,210
but some of them were really not
great and I focus on those in my book.

131
00:08:41,930 --> 00:08:46,220
I focus on those because I do not like
vague discussions about what could go

132
00:08:46,221 --> 00:08:47,780
wrong with a bunch of big data.

133
00:08:47,810 --> 00:08:50,510
I want to know exactly what
is going wrong right now.

134
00:08:51,170 --> 00:08:54,410
It's a kind of triage on this,
on the world of Algorithms,

135
00:08:55,680 --> 00:08:59,130
so I focus specifically on
the worst possible algorithms.

136
00:08:59,220 --> 00:09:02,760
If you're wondering why I'm so negative
in my book, I'm not actually negative.

137
00:09:02,761 --> 00:09:06,480
I love data. I'm a data scientist
still and I believe in data,

138
00:09:06,990 --> 00:09:11,250
but I do want to make it very clear to
the public and this book is written for

139
00:09:11,251 --> 00:09:15,000
the public that some of the times
this stuff is really fucked up.

140
00:09:15,960 --> 00:09:17,850
And I define that with
three characteristics.

141
00:09:17,880 --> 00:09:20,550
A weapon of math destruction is one
of these really terrible algorithms.

142
00:09:21,210 --> 00:09:24,060
They have three characteristics. One
is the widespread important, okay,

143
00:09:24,061 --> 00:09:25,410
so nobody cares about the owl.

144
00:09:25,440 --> 00:09:29,100
The algorithm for I make food for
my kids with that is not important.

145
00:09:30,060 --> 00:09:31,650
What's important,
what makes something important?

146
00:09:31,651 --> 00:09:34,890
And he said it happens to a lot of people
and it makes big decisions for them.

147
00:09:35,580 --> 00:09:38,730
Of course, algorithms don't
make big decisions to be clear.

148
00:09:38,760 --> 00:09:43,460
It's humans that have set up processes
by which an a score of an algorithm and

149
00:09:43,860 --> 00:09:46,560
ends up being a decision.
But I'm going to, you know,

150
00:09:46,590 --> 00:09:50,670
let's just put that aside for now.
If people's life's options,

151
00:09:50,671 --> 00:09:54,480
important life options, like how whether
they get a job, where they go to school,

152
00:09:54,750 --> 00:09:59,400
whether they get alone, how much they pay
for insurance, how long they go to jail.

153
00:09:59,430 --> 00:10:02,370
If those kinds of things are
determined in part by algorithms,

154
00:10:02,460 --> 00:10:05,910
essentially mostly scoring systems,
then they're important.

155
00:10:05,970 --> 00:10:07,200
They're widespread and important.

156
00:10:07,560 --> 00:10:10,260
The second characteristic of things
I'm really worried about is the

157
00:10:10,261 --> 00:10:14,430
mysteriousness. The secrecy. These
scoring algorithms are in general,

158
00:10:14,460 --> 00:10:18,780
aren't things that the formulas are
not available for people who are being

159
00:10:18,781 --> 00:10:19,680
scored by them.

160
00:10:21,360 --> 00:10:21,780
Yeah.

161
00:10:21,780 --> 00:10:25,980
Often they are not even aware
that they are being scored.

162
00:10:27,830 --> 00:10:28,200
Okay.

163
00:10:28,200 --> 00:10:31,980
And because they are important,
that is not okay because this,

164
00:10:32,040 --> 00:10:34,890
they are tantamount to laws.

165
00:10:35,280 --> 00:10:37,140
It is not okay for them to be secret.

166
00:10:37,470 --> 00:10:41,130
We have a constitutional right to
know what the laws of our country are.

167
00:10:41,910 --> 00:10:45,240
And finally I care about algorithms
not that are improving the world.

168
00:10:45,360 --> 00:10:47,490
I mean those are great,
but that are destructive.

169
00:10:48,150 --> 00:10:50,670
So I'm really focused on
things that are destructive.

170
00:10:50,671 --> 00:10:52,440
And I mean that in two ways.
The first way,

171
00:10:52,441 --> 00:10:55,190
I mean that is that they
ruin people's lives unfairly.

172
00:10:55,230 --> 00:10:57,840
So those people who's important,

173
00:10:57,841 --> 00:11:01,480
life decisions are being informed
by these scores. They are,

174
00:11:01,610 --> 00:11:05,220
they're getting unfairly
booted from opportunities.

175
00:11:05,970 --> 00:11:09,330
Opportunities are unfairly
being taken from them. Moreover,

176
00:11:09,540 --> 00:11:11,100
they're destructive in a larger sense.

177
00:11:11,101 --> 00:11:13,950
They actually set out to solve
the problem in a larger sense,

178
00:11:13,951 --> 00:11:16,110
usually often with good intentions,

179
00:11:16,920 --> 00:11:19,050
but not only do they fail
to solve that problem,

180
00:11:19,080 --> 00:11:20,610
but they actually make the problem worse.

181
00:11:20,700 --> 00:11:24,090
They create usually a destructive
feedback loop and I'm going to give you a

182
00:11:24,091 --> 00:11:26,070
bunch of examples and
then we'll have Q and. A.

183
00:11:27,330 --> 00:11:29,970
So the first example comes
from teacher assessment.

184
00:11:30,030 --> 00:11:34,800
We've had basically two decades long
war on teachers in this country.

185
00:11:34,801 --> 00:11:36,720
The idea is we have a
problem with education,

186
00:11:36,721 --> 00:11:38,460
let's find the bad teachers
and get rid of them.

187
00:11:38,670 --> 00:11:42,990
And I will put aside for the
now the question of whether
we could actually solve

188
00:11:42,991 --> 00:11:46,470
our problems and the big problem being
the achievement gap between rich and poor

189
00:11:46,471 --> 00:11:49,560
kids. Could we solve that by
getting rid of bad teachers?

190
00:11:49,770 --> 00:11:50,700
That's a different question.

191
00:11:50,940 --> 00:11:54,310
The point is that we have been looking
for these bad so we can get rid of them

192
00:11:54,311 --> 00:11:55,390
for two decades.

193
00:11:56,050 --> 00:12:00,730
The first generation of teacher
assessment tools was really,

194
00:12:00,731 --> 00:12:03,070
really stupid and unfair.
And here's how it was.

195
00:12:05,530 --> 00:12:06,520
Look for teachers,

196
00:12:06,670 --> 00:12:11,670
a majority of who students did not pass
some proficiency standard and their

197
00:12:12,371 --> 00:12:14,900
standardized tests.
Now this,

198
00:12:14,920 --> 00:12:18,160
the thing you need to know to
understand this is that there's a strong

199
00:12:18,161 --> 00:12:22,950
correlation between scores on
standardized tests and poverty. Um,

200
00:12:23,100 --> 00:12:25,660
so poor kids just don't do
as well on standardized test.

201
00:12:25,661 --> 00:12:28,900
That's not specific to the United States.
That's true over time.

202
00:12:28,901 --> 00:12:32,320
That's true internationally.
But what it means is that if you,

203
00:12:32,590 --> 00:12:35,710
if you targeted teacher as if
you've labeled them as bad,

204
00:12:35,711 --> 00:12:39,370
because a lot of their kids do not pass
a certain threshold of proficiency,

205
00:12:39,760 --> 00:12:42,640
you're basically targeting kids,
teachers of poor kids.

206
00:12:43,420 --> 00:12:46,450
So that was clearly unfair.
So it was discarded.

207
00:12:46,600 --> 00:12:51,600
The second generation was meant to be
more fair and it had good intentions,

208
00:12:51,881 --> 00:12:54,040
but it didn't work.
And let me tell you what it was.

209
00:12:54,640 --> 00:12:56,470
It was called the value
added teacher model.

210
00:12:57,760 --> 00:13:01,990
And the idea here is that you can't really
blame a teacher if a kid gets like a

211
00:13:01,991 --> 00:13:04,720
60 in fourth grade and they're
the fifth grade teacher,

212
00:13:04,721 --> 00:13:05,500
you can't blame them for it.

213
00:13:05,500 --> 00:13:08,590
They don't get a hundred at the end of
fifth grade right there starting from 60

214
00:13:09,370 --> 00:13:13,330
so it's just better if they
get more than expected. Right?

215
00:13:13,360 --> 00:13:15,310
So the idea was there was a primary model,

216
00:13:15,311 --> 00:13:18,790
which is the expected score
for each student in a class.

217
00:13:19,990 --> 00:13:23,380
And then there was the actual score that
that kid got to the end of the year and

218
00:13:23,381 --> 00:13:25,690
the teacher was taken,
put,

219
00:13:25,691 --> 00:13:30,430
basically held accountable for the
difference between those two scores.

220
00:13:30,970 --> 00:13:35,970
So if they are expected score was 62 but
they got a 65 the teacher was sort of

221
00:13:36,011 --> 00:13:38,610
given credit for those three points.
If they got,

222
00:13:38,660 --> 00:13:42,610
they were supposed to get a 62 but they
got a 55 they were dinged for those

223
00:13:42,611 --> 00:13:44,650
seven points that they didn't get.
Does that make sense?

224
00:13:45,190 --> 00:13:47,470
Now the problem with this, it
actually makes sense, right?

225
00:13:48,080 --> 00:13:53,080
And if you had like a Google level tests
of this where instead of having 25 kids

226
00:13:53,681 --> 00:13:58,420
in the class, you had 20,000 kids
in this class, then the statistics,

227
00:13:58,421 --> 00:14:00,130
the statistical signal
will be pretty clean.

228
00:14:00,730 --> 00:14:02,530
The problem is we do not do not have that,

229
00:14:03,310 --> 00:14:06,280
so we have instead is a pretty bad model,

230
00:14:06,340 --> 00:14:09,640
which is estimating of the expected
score for a kid at the end of the year.

231
00:14:10,090 --> 00:14:13,680
It's actually really hard to take
a kid at the end of fourth grade,

232
00:14:13,750 --> 00:14:15,460
estimate what they're going to
get at the end of fifth grade.

233
00:14:15,580 --> 00:14:17,560
It's not a precise thing.
There's a lot of uncertainty,

234
00:14:18,460 --> 00:14:22,060
especially for kids in poverty.
Then the second thing,

235
00:14:22,120 --> 00:14:25,300
the second source of uncertainty is like
what they actually got at the end of

236
00:14:25,301 --> 00:14:25,721
fifth grade.

237
00:14:25,721 --> 00:14:29,130
Like they could've gotten more if they've
taken the test in the morning and said

238
00:14:29,140 --> 00:14:32,950
the afternoon or in an air conditioned
room instead of a hot room or after they

239
00:14:32,951 --> 00:14:34,510
ate a meal because they were hungry.

240
00:14:35,200 --> 00:14:37,750
So there's one certainty on both those
numbers and then you're taking the

241
00:14:37,751 --> 00:14:41,170
difference. If you think about
statistically you have to the noise term,

242
00:14:41,171 --> 00:14:44,410
it's called the error term
of that expected score model.

243
00:14:45,070 --> 00:14:48,010
So you're holding teachers accountable
for the error terms of their students.

244
00:14:48,820 --> 00:14:53,000
Turns out there, it's not a very
good model. It's noisy, very,

245
00:14:53,270 --> 00:14:57,920
very noisy. Now I don't know this because
I have the algorithm because I looked,

246
00:14:58,040 --> 00:15:00,290
I tried to get it.
I tried,

247
00:15:00,291 --> 00:15:03,290
I did a freedom of Information
Act request for the source code,

248
00:15:03,740 --> 00:15:07,520
but it was denied me. I ended up
talking to someone in Wisconsin.

249
00:15:07,521 --> 00:15:10,130
This is for the New York City, a virgin.
There's versions all over the country.

250
00:15:10,131 --> 00:15:14,600
But I, I focus on the New
York City version. And um,

251
00:15:15,260 --> 00:15:18,170
I eventually talk to the people
that in Wisconsin, in Madison,

252
00:15:18,171 --> 00:15:20,900
Wisconsin who built that model and they
explained to me that I would never get

253
00:15:20,901 --> 00:15:23,480
the source code for that because in fact,
nobody in that,

254
00:15:23,540 --> 00:15:27,770
in New York City got the source code
for that cause by, by the, the contract,

255
00:15:27,980 --> 00:15:30,320
it was stipulated that it was proprietary.

256
00:15:31,370 --> 00:15:34,820
So no one in the department of Education
understood how these teachers were

257
00:15:34,821 --> 00:15:39,770
being evaluated. And yet the scores
were being used to deny people tenure.

258
00:15:41,210 --> 00:15:42,440
So that's why,

259
00:15:42,530 --> 00:15:46,160
so I should mention the reason why I
thought I might be able to get the source

260
00:15:46,161 --> 00:15:49,760
code through a Freedom Information Act
is because the New York Post actually did

261
00:15:49,761 --> 00:15:53,360
get the scores with all the teacher's
names through a freedom of Information Act

262
00:15:53,361 --> 00:15:58,100
request and they publish them all as an
act of shaming the teachers with that

263
00:15:58,101 --> 00:16:01,280
scores. And I thought, okay,

264
00:16:01,281 --> 00:16:03,140
maybe I can get the source
code if you can get the scores,

265
00:16:03,141 --> 00:16:05,390
I can get the way the scores
were made right now I couldn't.

266
00:16:05,540 --> 00:16:07,190
But here's the thing that happened.

267
00:16:07,220 --> 00:16:10,160
This really smart high school math
teacher at Stuyvesant high school,

268
00:16:10,700 --> 00:16:11,540
Gary Rubenstein,

269
00:16:12,080 --> 00:16:15,200
he took those same numbers that the
New York Post had gotten and he found

270
00:16:15,201 --> 00:16:18,260
teachers that had two
scores for the same year.

271
00:16:19,280 --> 00:16:22,310
You can get two scores if you teach
seventh grade math and eighth grade math.

272
00:16:23,180 --> 00:16:26,390
And he figured if they're both supposed
to be like the final say on whether

273
00:16:26,391 --> 00:16:28,970
you're a good teacher, they
should be consistent, right?

274
00:16:29,570 --> 00:16:32,330
So if you get a 75 for seventh grade math,

275
00:16:32,331 --> 00:16:35,030
you should get a 78 maybe
for eighth grade math, right?

276
00:16:35,480 --> 00:16:38,240
Or a 72 so he plotted them

277
00:16:39,860 --> 00:16:40,970
and a scatter plot,

278
00:16:44,900 --> 00:16:48,560
it looks almost like uniform distribution,
which is to say almost random.

279
00:16:49,940 --> 00:16:52,880
It's actually 24% correlation.
There's some signal in it,

280
00:16:53,180 --> 00:16:56,330
which is why I said the thing about like
if you had 10,000 kids in your class,

281
00:16:56,900 --> 00:17:01,100
you might actually know whether your
teacher brings up grades, uh, scores,

282
00:17:01,910 --> 00:17:06,200
but it's simply not good enough to
hold someone accountable for that at an

283
00:17:06,201 --> 00:17:10,130
individual level. But we trusted
it anyway. When I say we,

284
00:17:10,131 --> 00:17:12,920
I mean the Goe essentially
because they didn't understand it.

285
00:17:14,150 --> 00:17:16,850
In spite of that,
Sarah was soaked with Saki,

286
00:17:17,480 --> 00:17:21,740
who's pictured here was fired after
she got a bad teacher assessment,

287
00:17:21,741 --> 00:17:26,741
50% of which was her teacher value added
model score in Washington DC in 2011 I

288
00:17:28,581 --> 00:17:30,200
should say it's 50% of our score,

289
00:17:30,201 --> 00:17:35,120
but it's more than 50% of the variants
of the score. The big complaint of these,

290
00:17:35,210 --> 00:17:39,800
of the way that teachers were assessed
was that everybody got the same grade

291
00:17:39,801 --> 00:17:40,431
from their principal.

292
00:17:40,431 --> 00:17:42,980
The principals would categorize
them all as acceptable teachers.

293
00:17:44,000 --> 00:17:44,511
So they were like,

294
00:17:44,511 --> 00:17:48,710
we need some spread in the scores so we
can like rank the teacher isn't refined

295
00:17:48,770 --> 00:17:53,100
the worst 2% so they got spread.
But the question was,

296
00:17:53,160 --> 00:17:56,640
is it meaningful? I climb,
it's not meaningful from the,

297
00:17:56,760 --> 00:17:58,260
from the scatter plot we just saw,

298
00:17:58,560 --> 00:18:01,590
but it was particularly irksome
for Sarah because guess what?

299
00:18:02,250 --> 00:18:03,300
She's teaching a fifth grade,

300
00:18:03,720 --> 00:18:06,660
a bunch of four fourth graders came
in with very high scores at the end of

301
00:18:06,661 --> 00:18:08,160
fourth grade who couldn't read or write.

302
00:18:10,440 --> 00:18:15,440
And it turns out that in the Washington
d c a district Michelle ree had

303
00:18:15,761 --> 00:18:17,440
instituted both carrots and sticks.

304
00:18:18,220 --> 00:18:22,180
You get bonus if you get a high value
added model score or high teacher

305
00:18:22,181 --> 00:18:24,190
assessment,
you get fired if you get a bad one.

306
00:18:24,850 --> 00:18:28,690
So moreover she found out that this
school where some of her kids came from,

307
00:18:28,900 --> 00:18:33,900
had exceptionally high eraser a ratio
rates for their end of year standardized

308
00:18:33,911 --> 00:18:34,360
tests.

309
00:18:34,360 --> 00:18:38,080
So she has reason to believe that the
previous teacher is actually cheated on

310
00:18:38,081 --> 00:18:40,540
the test. And if you think
about what that would mean,

311
00:18:40,750 --> 00:18:44,620
those kids get elevated scores and they're
expected to keep them elevated at the

312
00:18:44,621 --> 00:18:49,480
end of her class. But she just got
them. She got, she taught them well,

313
00:18:49,481 --> 00:18:51,390
but they didn't get
ridiculously high scores,

314
00:18:51,391 --> 00:18:54,580
so she got punished for the
previous teachers cheating.

315
00:18:57,320 --> 00:19:02,320
I should mention that she got a new job
a couple of days after she got fired in

316
00:19:02,391 --> 00:19:05,720
an affluent suburb of Washington
DC because most of the,

317
00:19:06,620 --> 00:19:09,710
most of these, this is where the
value added model is being used.

318
00:19:09,980 --> 00:19:11,660
I think it's actually more
than that at this point.

319
00:19:12,830 --> 00:19:16,220
Mostly an urban school districts.
So that brings me to the,

320
00:19:16,610 --> 00:19:21,320
the failure. Okay. Remember I
said there's, there's failure on,

321
00:19:22,220 --> 00:19:23,900
well,
let me just go back over the three things.

322
00:19:23,901 --> 00:19:28,901
It's widespread secret and it's
destructive at an individual level.

323
00:19:29,900 --> 00:19:33,440
So we saw Sarah getting la losing her job,
and at a systemic level,

324
00:19:34,160 --> 00:19:38,010
I claim that the value added model
does not get rid of good, bad teachers.

325
00:19:38,011 --> 00:19:40,370
I remember the whole point was
get rid of the bad teachers,

326
00:19:40,430 --> 00:19:42,320
then we'll solve the
problem with education.

327
00:19:42,710 --> 00:19:45,140
We're not getting rid of bad teachers.
Were getting rid of good teachers.

328
00:19:45,830 --> 00:19:48,080
We got rid of Sarah,
which was just dumb,

329
00:19:48,770 --> 00:19:53,000
but they're talentless teachers who
have quit, who have retired early,

330
00:19:53,690 --> 00:19:54,920
who have not gone into teaching.

331
00:19:54,930 --> 00:19:58,850
We have a national teacher shortage or
have fled to the suburbs where they don't

332
00:19:58,851 --> 00:20:02,150
use this arbitrary and
punitive regime for teachers.

333
00:20:03,380 --> 00:20:07,100
My next example comes from the
world of personality tests.

334
00:20:07,820 --> 00:20:11,810
This is Kyle beam. He was a college
student in the Atlanta area.

335
00:20:12,080 --> 00:20:16,190
He wanted to get a job after
school at a grocery store,

336
00:20:16,580 --> 00:20:20,030
Kroger's and he was a,
he took a personality test.

337
00:20:21,470 --> 00:20:23,810
He failed that.
Most people never find out they failed it,

338
00:20:23,811 --> 00:20:27,620
but he found out because his friend
worked at, Kroger's told him. Um,

339
00:20:27,800 --> 00:20:31,490
so he was unusual in finding out that he
got red lighted from this algorithm was,

340
00:20:31,491 --> 00:20:35,150
which was built by Kronos.
I'm a small,

341
00:20:35,151 --> 00:20:37,580
big data company actually
in this neighborhood.

342
00:20:38,690 --> 00:20:41,330
The other thing that was unusual about
Kyle is that his father's a lawyer.

343
00:20:42,230 --> 00:20:45,050
Most people that are are are
applying for minimum wage jobs,

344
00:20:45,051 --> 00:20:49,690
which this was do not have that are
lawyers. So his father said, why? Well,

345
00:20:49,730 --> 00:20:51,370
what were the questions like on this test?

346
00:20:51,371 --> 00:20:54,460
And Kyle said there were a lot like the
questions I got at the hospital when I

347
00:20:54,461 --> 00:20:58,810
was being treated for bipolar disorder,
the five factor model it's called.

348
00:20:58,960 --> 00:21:01,960
And his father said, well that's illegal
under the Americans with disability act,

349
00:21:01,961 --> 00:21:06,961
it's illegal to have a health
exam including a mental
health exam as part of a a

350
00:21:08,290 --> 00:21:12,490
job application. And he said, well
can you apply to other places?

351
00:21:12,520 --> 00:21:16,540
Kyle ended up applying to six
other places, Lowe's, yum foods,

352
00:21:16,541 --> 00:21:19,210
which owns Taco Bell,
other big companies.

353
00:21:19,300 --> 00:21:22,570
They all had exactly the same personality
test and he failed all of them.

354
00:21:23,620 --> 00:21:28,180
So his father is suing those
seven companies on behalf
as a class action lawsuit

355
00:21:28,181 --> 00:21:32,680
on behalf of anyone who's ever taken
that test on the grounds that it is

356
00:21:32,681 --> 00:21:37,030
illegal, it violates the
American's with disability act.

357
00:21:38,260 --> 00:21:42,520
So that's widespread. It's secret,

358
00:21:44,050 --> 00:21:47,440
I should mention personality
tests are not new.

359
00:21:48,520 --> 00:21:49,840
Radio Shack,
old,

360
00:21:49,930 --> 00:21:53,710
old sort of easily gamed personality
tests looked a lot like this.

361
00:21:54,010 --> 00:21:57,960
Agree or disagree. I'm always
happy. I think your PR,

362
00:21:58,000 --> 00:22:00,070
I think your employer wants
you to agree with that.

363
00:22:00,820 --> 00:22:05,650
These are the more recent ones, much
harder. What do you agree with more?

364
00:22:05,651 --> 00:22:08,800
I sometimes get confused my thought
by my thoughts and feelings or I don't

365
00:22:08,801 --> 00:22:10,960
really like it when I have to do
something I haven't done before.

366
00:22:13,580 --> 00:22:16,810
Imagine doing 50 of those.
So it's,

367
00:22:16,830 --> 00:22:18,680
it's absolutely inscrutable.

368
00:22:22,800 --> 00:22:24,720
So it's secret and I would
say it's destructive.

369
00:22:24,721 --> 00:22:27,630
It's destructive for Kyle because he
didn't get a job at any of those big

370
00:22:27,631 --> 00:22:30,810
companies he applied to in
the entire area of Atlanta.

371
00:22:31,590 --> 00:22:34,710
But it's also destructive in a larger
sense if what we're worried about is true,

372
00:22:34,711 --> 00:22:39,711
that they're actually sort
of systematically denying
employment to an entire

373
00:22:40,200 --> 00:22:43,920
subpopulation. Um, people
with mental health problems.

374
00:22:45,540 --> 00:22:48,990
Um,
it's not relegated to minimum wage work.

375
00:22:49,560 --> 00:22:53,130
This idea of having algorithms
that sort through resumes.

376
00:22:54,750 --> 00:22:59,130
Do you guys know who this is?
This is Roger Ailes.

377
00:22:59,640 --> 00:23:03,270
He recently got kicked out of Fox News.
He led Fox News for a long time.

378
00:23:03,660 --> 00:23:07,170
I got kicked out for like six basically
sexually harassing women and keeping

379
00:23:07,171 --> 00:23:10,230
them from Prenay promoted.
So it's doing a thought experiment.

380
00:23:10,470 --> 00:23:12,870
I don't know if Fox News uses
an algorithm to hire people,

381
00:23:12,871 --> 00:23:15,720
but I'm just imagining that
they do play along with me.

382
00:23:16,590 --> 00:23:18,390
What does that algorithm taken?
Well,

383
00:23:18,391 --> 00:23:22,410
it takes in data probably
historical data from Fox News,

384
00:23:22,470 --> 00:23:25,290
so all the history of
people who've applied to be,

385
00:23:25,291 --> 00:23:29,700
say anchors at Fox News and then
it takes a definition of success.

386
00:23:29,970 --> 00:23:33,240
What does a good employee at [inaudible]
successful employee at Fox News look

387
00:23:33,241 --> 00:23:35,790
like?
Let's just say,

388
00:23:35,940 --> 00:23:39,240
let's stipulate a successful employee
at Fox News stays for three years.

389
00:23:39,241 --> 00:23:42,240
It gets promoted at least once.
Okay.

390
00:23:42,390 --> 00:23:45,920
That's typical for what,
how these are are,

391
00:23:46,100 --> 00:23:50,570
are designed and how they're trained.
So if you think about what that means,

392
00:23:50,660 --> 00:23:53,630
the algorithm that if you've trained
it on this, a lot of historical data,

393
00:23:53,631 --> 00:23:57,950
what it will do is given a new set
of applicants for our anchor job,

394
00:23:58,280 --> 00:23:58,940
it will say,

395
00:23:58,940 --> 00:24:03,140
who along these new applicants looks like
someone who was successful in the past?

396
00:24:04,980 --> 00:24:09,980
And I would not be surprised if it
filtered out women because not only did

397
00:24:10,251 --> 00:24:14,540
women, not, not only did women not
necessarily get that job as often,

398
00:24:14,930 --> 00:24:16,370
but when they did get the job,

399
00:24:16,910 --> 00:24:21,470
they were systematically pushed out by
Roger Ailes and the entire culture there.

400
00:24:21,800 --> 00:24:25,910
Does that make sense? So the point here,
and this is a very important point,

401
00:24:27,680 --> 00:24:32,660
is that we do not improve.
We do not. We do not.

402
00:24:33,050 --> 00:24:37,310
Algorithms are not inherently objective
or fair. All they are doing their,

403
00:24:37,311 --> 00:24:41,240
what they're really good at is picking
out past patterns and repeating them,

404
00:24:42,290 --> 00:24:45,920
which is to say,
if we had a perfect way of hiring people,

405
00:24:46,700 --> 00:24:48,050
we would want to codify that.

406
00:24:48,051 --> 00:24:51,170
We would want to automate that because
it would save us money in time.

407
00:24:52,220 --> 00:24:54,560
But until we have a
perfect way of doing this,

408
00:24:54,980 --> 00:24:59,570
all we're doing is we are literally
caudifying past practices and propagating

409
00:24:59,571 --> 00:25:00,404
them.

410
00:25:04,560 --> 00:25:07,590
My last example comes
from criminal justice.

411
00:25:08,250 --> 00:25:12,300
There's actually two different kinds of
algorithms that worry me a lot in the,

412
00:25:12,301 --> 00:25:16,350
in the criminal justice. The
first is predictive policing. Uh,

413
00:25:16,500 --> 00:25:19,170
you guys know all about the
black lives matter movement,

414
00:25:19,171 --> 00:25:22,290
which is objecting to two
black people getting shot.

415
00:25:23,190 --> 00:25:24,450
But it's not just of course being shot,

416
00:25:24,451 --> 00:25:28,440
it's just the amount of over policing
and uneven policing that's happening to

417
00:25:28,800 --> 00:25:32,510
poor black communities.
Some evidence, there's um,

418
00:25:32,940 --> 00:25:35,790
whites and blacks smoking
pot at a similar rates,

419
00:25:35,820 --> 00:25:38,610
white smoking pot a little
bit more if they're young,

420
00:25:40,040 --> 00:25:43,680
but blacks get arrested a lot more,
a lot more for that.

421
00:25:45,000 --> 00:25:46,770
In fact,
depending on the jurisdiction,

422
00:25:48,150 --> 00:25:53,150
blacks can get arrested up to 10 times
more often than whites for smoking pot.

423
00:25:54,900 --> 00:25:59,370
Okay. So the two things I wanted
to take away from this are blacks.

424
00:25:59,430 --> 00:26:01,200
It's biased against blacks.

425
00:26:01,530 --> 00:26:06,510
And actually it depends very much on local
conditions of how the police force is

426
00:26:06,570 --> 00:26:10,170
expected to act.
So when you hear about arrest records,

427
00:26:10,171 --> 00:26:15,171
I want you to think just as much about
police practices as you do about crime,

428
00:26:17,070 --> 00:26:18,780
at least when it's nonviolent crime.

429
00:26:20,280 --> 00:26:24,240
And I do make a distinction between
violent crime and nonviolent crime.

430
00:26:24,630 --> 00:26:29,630
The problem is that nonviolent crime is
much more prevalent and it is much more

431
00:26:29,761 --> 00:26:33,280
predictable. Think
about it. We have crime.

432
00:26:33,330 --> 00:26:37,410
We have people being arrested in this
country for mental health problems when

433
00:26:37,411 --> 00:26:38,730
they're poor,
not when they're rich.

434
00:26:39,360 --> 00:26:43,620
We have people being arrested
for addiction problems
when they're poor and we

435
00:26:43,621 --> 00:26:47,940
had people being arrested much more
often for low level nonviolent crime like

436
00:26:47,941 --> 00:26:51,750
drug use. And for that, for that
matter, for like crimes of poverty,

437
00:26:51,751 --> 00:26:55,680
just like literally not having a place
to go to the bathroom. And that is,

438
00:26:55,740 --> 00:26:57,330
that goes into their arrest records.

439
00:26:58,320 --> 00:27:01,650
And the reason I'm mentioning this is
because the way predictive policing works

440
00:27:01,651 --> 00:27:03,480
is very simple.
It's actually very stupid.

441
00:27:04,020 --> 00:27:07,230
It's Geo located arrest cr rest records.

442
00:27:08,100 --> 00:27:12,990
And the algorithm says, go put police
where we saw crime in the past,

443
00:27:14,280 --> 00:27:17,880
which is really where we saw arrests in
the past where we saw police arresting

444
00:27:17,881 --> 00:27:19,860
people for things in the past.

445
00:27:20,370 --> 00:27:25,050
This is just much as much
a way of predicting the
police as it is of predicting

446
00:27:25,080 --> 00:27:29,190
crime. I should just, I'm just reiterating
that it might be different by the way,

447
00:27:29,940 --> 00:27:33,990
if we really focused only on
violent crime, imagine that.

448
00:27:34,020 --> 00:27:36,960
Imagine a predictive policing algorithm
where they only focus on violent crime

449
00:27:36,961 --> 00:27:37,794
like murder.

450
00:27:39,550 --> 00:27:40,383
Okay.

451
00:27:40,770 --> 00:27:42,750
The problem is that murder
is really hard to predict.

452
00:27:43,980 --> 00:27:45,540
Like even if you did predict murder,

453
00:27:45,780 --> 00:27:49,800
would you like what Stan like outside
of a house waiting for someone to get

454
00:27:49,801 --> 00:27:50,220
murdered?

455
00:27:50,220 --> 00:27:53,760
Like the problem is that the things that
are actually easy to predict are things

456
00:27:53,761 --> 00:27:54,930
like poverty.

457
00:27:56,290 --> 00:27:56,970
Okay.

458
00:27:56,970 --> 00:28:01,410
So my claim is that predictive policing
is more or less creating a feedback loop

459
00:28:01,411 --> 00:28:06,210
where you're having a scientific pseudo
scientific basis for sending police back

460
00:28:06,211 --> 00:28:10,260
to neighborhoods that are already
over policed. So that another way,

461
00:28:10,800 --> 00:28:14,100
another thought experiment, imagine
that if after the financial crisis,

462
00:28:14,400 --> 00:28:18,540
all the cops had been told to go to Wall
Street and the rest of the bankers and

463
00:28:18,541 --> 00:28:19,770
find out if they had cocaine in their,

464
00:28:19,910 --> 00:28:23,970
in their pockets because they all do
not all of them. Then the predict,

465
00:28:24,040 --> 00:28:27,600
then the police records,
the police data, the data,

466
00:28:28,080 --> 00:28:30,330
which is just a reflection
of what police do, right?

467
00:28:30,570 --> 00:28:32,970
The data would tell them in these
predictive policing algorithms,

468
00:28:32,971 --> 00:28:34,950
go back to Wall Street because
that's where the crime is,

469
00:28:35,460 --> 00:28:36,420
but that's not what we did.

470
00:28:40,730 --> 00:28:41,440
Okay,

471
00:28:41,440 --> 00:28:43,320
so the next example in also in,

472
00:28:43,690 --> 00:28:48,640
in the criminal justice system is
recidivism risk. This is a score.

473
00:28:49,120 --> 00:28:53,040
Recidivism by the way, is coming back
to jail. So recidivism risk is the risk.

474
00:28:53,060 --> 00:28:54,100
Somebody comes back to jail.

475
00:28:54,730 --> 00:28:58,990
Recidivism risk scores are given to
judges when they sentence criminal

476
00:28:58,991 --> 00:29:03,040
defendants. Now there's two kinds
of data that goes into that.

477
00:29:03,550 --> 00:29:06,670
One is a rest records we just talked
about a restaurant, they're very biased.

478
00:29:07,630 --> 00:29:09,040
The second is a questionnaire.

479
00:29:10,270 --> 00:29:13,900
The most commonly used recidivism
risk algorithm is called the Lsir.

480
00:29:14,200 --> 00:29:18,040
And I'm going to show you a couple of
the questions that are asked of the

481
00:29:18,041 --> 00:29:19,540
defendant in the Lsir.

482
00:29:21,220 --> 00:29:21,930
Okay.

483
00:29:21,930 --> 00:29:24,120
Number 29 do you live in
a high crime neighborhood?

484
00:29:25,080 --> 00:29:29,730
It's a proxy for class and race because
that is pointing to people who are

485
00:29:29,731 --> 00:29:30,780
already poor and black.

486
00:29:35,950 --> 00:29:38,450
And, and by the way, if you
say yes or no to these things,

487
00:29:38,451 --> 00:29:43,210
it goes exactly as you imagine. If you
say yes to I have a mental health problem,

488
00:29:43,810 --> 00:29:46,730
you're higher risk. Right. And Oh,

489
00:29:46,780 --> 00:29:50,740
I forgot to mention if you're a higher
risk will a judge will sentence you to

490
00:29:50,741 --> 00:29:51,730
longer in jail.

491
00:29:53,400 --> 00:29:53,990
Okay.

492
00:29:53,990 --> 00:29:54,560
We've doesn't,

493
00:29:54,560 --> 00:29:58,010
I mean I mentioned that because it's not
obvious that that's what you would do,

494
00:29:58,011 --> 00:30:02,630
right? Like if you're higher risk of
recidivism you get put in jail longer.

495
00:30:03,300 --> 00:30:06,260
Um, is a little bit minority report
ish because what you're doing is you're

496
00:30:06,261 --> 00:30:08,990
preemptively punishing someone for
something they haven't done yet.

497
00:30:09,830 --> 00:30:12,080
But that is the practice
that judges now have.

498
00:30:13,550 --> 00:30:16,790
Here's another set of questions.
Um,

499
00:30:16,910 --> 00:30:20,000
have you been suspended
from school number 17?

500
00:30:20,540 --> 00:30:25,540
I'll show you a applaud of just
how much that is a proxy for race.

501
00:30:26,630 --> 00:30:30,140
Black girls and boys are much more
likely to be suspended from school.

502
00:30:31,790 --> 00:30:34,760
But I think the thing that bothers
me the most and I think should bother

503
00:30:34,761 --> 00:30:39,761
absolutely everyone is number 26 with
somebody in your family in prison.

504
00:30:42,590 --> 00:30:46,440
This is something that in an open
court would be thrown out by a judge as

505
00:30:46,610 --> 00:30:50,060
unconstitutional. If a
lawyer said, your honor,

506
00:30:50,061 --> 00:30:53,180
please sentence this person to longer
because their father was also a criminal.

507
00:30:53,720 --> 00:30:58,490
That is not how we do it, but because
it's being embedded in a risk score,

508
00:30:58,550 --> 00:31:00,380
which has been claimed to be scientific,

509
00:31:01,130 --> 00:31:06,130
this somehow has the authenticity of
mathematics and science and again is being

510
00:31:07,311 --> 00:31:09,230
used to send people to jail for longer.

511
00:31:11,510 --> 00:31:12,130
Okay,

512
00:31:12,130 --> 00:31:13,660
so I talk about failures.

513
00:31:15,540 --> 00:31:17,740
I should also mention this is important.

514
00:31:17,741 --> 00:31:21,160
It's being used in more than
half the states. It's secret.

515
00:31:21,490 --> 00:31:22,930
People do not understand what they're,

516
00:31:23,410 --> 00:31:25,030
what they're getting
into with these scores.

517
00:31:25,031 --> 00:31:29,830
And judges are actually very secretive
about exactly how much weight they put on

518
00:31:29,831 --> 00:31:34,780
these scores. But my claim is that
they are most destructive thing.

519
00:31:34,781 --> 00:31:38,380
You can imagine they create their
own reality. If you're a higher risk,

520
00:31:38,381 --> 00:31:40,570
you're sentenced to jail for longer.
And guess what?

521
00:31:40,571 --> 00:31:42,850
If you're sentenced to prison,
if you're in prison longer,

522
00:31:43,090 --> 00:31:48,010
you don't tend to be to benefit from
that experience. You end up out of jail.

523
00:31:48,490 --> 00:31:52,690
97% of people eventually leave prison,
I should mention. So this happens,

524
00:31:53,350 --> 00:31:56,260
but you end up with no resources,
no connections to your community,

525
00:31:56,261 --> 00:31:57,520
very little wealth.

526
00:31:58,480 --> 00:32:02,950
You have a felony to your name often so
you it's hard to get a job and then you

527
00:32:02,951 --> 00:32:07,951
end up back in prison partly because
you got this high risk score so you were

528
00:32:08,531 --> 00:32:11,080
deemed high risk and then
you end up back in prison.

529
00:32:12,490 --> 00:32:14,530
So I'm almost done.
Those are my examples,

530
00:32:14,770 --> 00:32:18,310
but I do want to mention
again that I don't hate data.

531
00:32:19,600 --> 00:32:24,460
I just really think we have not yet get
started understanding what it means to

532
00:32:24,461 --> 00:32:25,720
build safe algorithms.

533
00:32:27,220 --> 00:32:30,760
It's like we are building cars and
just putting them on the road without

534
00:32:30,761 --> 00:32:32,470
understanding that cars can kill people.

535
00:32:33,770 --> 00:32:34,560
Okay.

536
00:32:34,560 --> 00:32:38,340
So I want a data scientist to take their
ethical responsibilities seriously,

537
00:32:39,320 --> 00:32:43,970
which means building some kind of ethical
framework like a hippocratic oath for

538
00:32:44,450 --> 00:32:45,440
data scientists.

539
00:32:46,700 --> 00:32:50,750
I also think that we need to learn how
to scrutinize these algorithms to monitor

540
00:32:50,751 --> 00:32:52,910
them, to audit them
for safety, for safety,

541
00:32:52,911 --> 00:32:56,270
for fairness and
discrimination and for meaning.

542
00:32:56,540 --> 00:33:00,830
Making sure that we're actually building
meaningful things with the teacher

543
00:33:00,831 --> 00:33:02,300
value added model was not meaningful.

544
00:33:04,230 --> 00:33:07,740
And I also think that in
situations where it's very,

545
00:33:07,741 --> 00:33:10,650
very important to a given person's life,

546
00:33:10,680 --> 00:33:14,070
how they're being scored and they should
have the right to scrutinize that score.

547
00:33:14,790 --> 00:33:15,720
Like the teachers,

548
00:33:15,750 --> 00:33:19,680
like if they are being given a score
that will make maybe make them fired,

549
00:33:19,710 --> 00:33:22,560
they should be able to understand
exactly how that scoring system works.

550
00:33:22,561 --> 00:33:26,580
Because to be clear, the teachers were
not, even when they appealed their score,

551
00:33:26,581 --> 00:33:30,630
they were not told how they were being
actually evaluated. And that's not right.

552
00:33:33,560 --> 00:33:34,393
Thank you guys.

553
00:33:40,210 --> 00:33:41,290
If you have any questions,
I'm here.

554
00:33:41,450 --> 00:33:45,690
Hi. Um, what's the best way to account
for and mitigate limitations in a model?

555
00:33:47,580 --> 00:33:48,730
So suppose you,

556
00:33:48,780 --> 00:33:53,580
you have to come up with a mathematical
model to affect somebody's life. Um,

557
00:33:53,640 --> 00:33:57,930
how do you figure out where the boundaries
of application are and how do you,

558
00:33:58,530 --> 00:34:02,160
I guess iteratively readdress that to
see if it's doing what you expect it to?

559
00:34:03,890 --> 00:34:07,910
I mean, it's really a vague question,
so it's hard for me to answer I think.

560
00:34:08,840 --> 00:34:12,860
I think it's like, I'll tell you how, how
frustrating it is as a data scientist. I,

561
00:34:12,870 --> 00:34:13,800
I'm a data scientist.

562
00:34:13,820 --> 00:34:18,820
Like I worked in the City Hall of New
York and I was asked to use the data.

563
00:34:20,661 --> 00:34:24,560
They had to figure out how long a family
was going to be in homeless services.

564
00:34:26,390 --> 00:34:27,200
MMM.

565
00:34:27,200 --> 00:34:31,280
And I had race, I have the
number of children. I had,
you know, out how many times,

566
00:34:31,370 --> 00:34:33,980
whether the children, whether the
parents had been in social services.

567
00:34:33,981 --> 00:34:37,060
I had all this data,
but they weren't telling me,

568
00:34:37,100 --> 00:34:40,390
they weren't telling me how they were
going to use this algorithm if this

569
00:34:40,391 --> 00:34:43,880
scoring system once I had it.
So in particular,

570
00:34:43,881 --> 00:34:48,881
one thing I didn't know whether I should
use was the attribute race or all the

571
00:34:49,251 --> 00:34:52,340
other things that are proxies to raise
or should I d correlate those other

572
00:34:52,341 --> 00:34:56,630
things from the, the, the race.
How was this going to be used?

573
00:34:57,050 --> 00:35:00,080
Number one possibility.
Um, if you were high risk,

574
00:35:00,090 --> 00:35:03,260
then you're going to be put into worse
housing. If you were like, you know,

575
00:35:03,380 --> 00:35:06,260
expect it to be longterm housing,
you're going to be put into wars housing.

576
00:35:06,560 --> 00:35:10,040
If I did that, if I, if I knew that
that was how it was going to be used,

577
00:35:10,220 --> 00:35:12,710
then I would know that
people at higher risk,

578
00:35:13,400 --> 00:35:17,180
there'll be a disparate impact
racially. Right? But if I,

579
00:35:17,181 --> 00:35:20,600
if instead it was being used to
sort of figure out interventions,

580
00:35:21,830 --> 00:35:25,040
um, so that, you know,
black family, they were,

581
00:35:25,190 --> 00:35:29,150
maybe they wanted to understand why black
families were in homeless services for

582
00:35:29,151 --> 00:35:33,680
longer and try to intervene to try to
make that discrepancy smaller than it

583
00:35:33,681 --> 00:35:37,380
would by all means make sense
to have racist the attribute.
Does that make sense?

584
00:35:37,470 --> 00:35:40,440
So like basically not
answering your question,

585
00:35:40,710 --> 00:35:45,450
but I am making it clear that you cannot
answer that question until you really

586
00:35:45,451 --> 00:35:48,990
know the use case for this algorithm.

587
00:35:49,020 --> 00:35:54,020
And I should also add that
a given algorithm could have
a positive or a negative

588
00:35:54,391 --> 00:35:58,890
effect on the world. It's
really tricky. These things are,

589
00:35:59,220 --> 00:36:01,710
let me give you an example. Um, health,

590
00:36:02,340 --> 00:36:06,240
everyone's talking about how great it is
that we can predict each other's health.

591
00:36:06,570 --> 00:36:10,230
Well that's great. If your doctor has
that algorithm that can keep you well,

592
00:36:11,400 --> 00:36:15,030
but it's not great if an insurance
company has an algorithm.

593
00:36:15,030 --> 00:36:20,030
We could charge you more if you're
about to get sick or if Walmart,

594
00:36:20,250 --> 00:36:21,750
and I'm not saying this is happening,

595
00:36:21,780 --> 00:36:25,680
but if like Walmart puts a health risk
on top of everybody who's applying to

596
00:36:25,681 --> 00:36:29,610
their job and says, we don't want to
spend money on our people's insurance,

597
00:36:29,611 --> 00:36:32,610
so we're going to not hire people
with high health risks. I mean,

598
00:36:32,611 --> 00:36:34,620
I'm just saying same algorithm,

599
00:36:35,040 --> 00:36:37,740
different use cases could be used
for good or bad. And so the answer,

600
00:36:38,040 --> 00:36:40,680
the answer is if there is no rule here,

601
00:36:41,310 --> 00:36:45,780
these are decision making algorithms or
decision making processes that are as

602
00:36:45,781 --> 00:36:48,660
complicated as anything in the world.
So we can't,

603
00:36:48,900 --> 00:36:52,470
we can't pretend that there are like
formulas for how to use them and how to

604
00:36:52,471 --> 00:36:54,810
make them safe. They're
very, very contextual.

605
00:36:56,880 --> 00:37:01,270
Uh, do you have any sense of, um, this
is probably a naive question, but,

606
00:37:01,930 --> 00:37:02,763
um,

607
00:37:02,890 --> 00:37:07,890
whether the scale of the problem is bigger
or smaller in a country with stronger

608
00:37:10,001 --> 00:37:12,480
libel and slander laws,
like for example, invalid.

609
00:37:14,840 --> 00:37:16,610
Say more.
Why would you imagine that?

610
00:37:18,590 --> 00:37:19,330
Oh,

611
00:37:19,330 --> 00:37:22,240
well why would I imagine it
would be different? Yeah. Uh,

612
00:37:22,640 --> 00:37:25,730
well I guess the people who are victims
probably don't have the resources to

613
00:37:26,050 --> 00:37:28,960
pursue slim, slender cases anyways. Okay.

614
00:37:29,080 --> 00:37:31,780
I think it was one of the very important
point. Thank you for making it.

615
00:37:31,781 --> 00:37:35,410
Which is that
one of the,

616
00:37:35,440 --> 00:37:39,040
one of the commonalities of almost all
these algorithms is that the people who

617
00:37:39,041 --> 00:37:43,840
are losing by these scoring systems are
often the most vulnerable people in our

618
00:37:43,841 --> 00:37:48,670
society. So they do not
have, generally speaking,

619
00:37:48,671 --> 00:37:51,580
they don't have lawyers to
protect them. Right. Um,

620
00:37:51,610 --> 00:37:55,660
but I would also say the following,
liking, anonymized, anonymization,

621
00:37:55,661 --> 00:37:58,480
which is a tool that people often bring
up as like a way of solving some of

622
00:37:58,481 --> 00:38:01,270
these problems is not
really a solution to me.

623
00:38:02,080 --> 00:38:06,100
Let's think about the medical,
the medical model I just mentioned.

624
00:38:06,190 --> 00:38:11,020
Like let's say I'm Walmart and again,
what Walmart's not really,

625
00:38:11,021 --> 00:38:16,021
I'm a large employer and I get longterm
health data off of all my employees

626
00:38:16,841 --> 00:38:19,120
because I forced them to use
fitbits or whatever I do. Right.

627
00:38:19,450 --> 00:38:24,040
I get long term health outcomes
and I don't charge them.

628
00:38:24,041 --> 00:38:26,290
I'm their hardy, my employees,
I treat them well. Right?

629
00:38:26,770 --> 00:38:30,700
But the point is I can build the
algorithm, I can build the neural network,

630
00:38:30,701 --> 00:38:31,570
what have have you,

631
00:38:31,630 --> 00:38:35,830
I can train it so that when
someone applies my job,

632
00:38:35,980 --> 00:38:39,310
I only ask have to ask them six
questions and I've already categorized,

633
00:38:39,311 --> 00:38:40,540
I've segmented them into a,

634
00:38:41,080 --> 00:38:46,080
into a risk group score and
it is completely anonymous.

635
00:38:46,560 --> 00:38:49,330
In other words,
you can build an algorithm anonymously.

636
00:38:49,570 --> 00:38:54,570
It's still applying it to someone in
a very precise way and it still can be

637
00:38:55,241 --> 00:38:59,330
problematic. So it's really
not for me, it's not about, um,

638
00:38:59,680 --> 00:39:00,513
anonymity.

639
00:39:03,940 --> 00:39:07,330
So in a lot of the reporting that I
have read about this sort of thing,

640
00:39:07,331 --> 00:39:09,880
and I'm thinking
specifically, uh, for example,

641
00:39:09,881 --> 00:39:14,410
the pro bowl get along thing on the,
um, criminal justice sentencing thing.

642
00:39:14,920 --> 00:39:15,280
They,

643
00:39:15,280 --> 00:39:19,840
I haven't seen a lot of talk about how
these algorithms perform relative to,

644
00:39:20,250 --> 00:39:23,440
in this example, just a
judge without this, you know,

645
00:39:23,441 --> 00:39:27,220
score or what the preexisting case was.
Yeah.

646
00:39:27,750 --> 00:39:31,680
Great Point. And I. Dot. I'm
desperate. Desperate for that. Right.

647
00:39:31,681 --> 00:39:35,400
Because the point is that the justice
system is very racist. Alrighty.

648
00:39:36,990 --> 00:39:37,350
This,

649
00:39:37,350 --> 00:39:42,350
the intention of this recidivism risk
stuff was to make judges more objective

650
00:39:42,631 --> 00:39:45,090
and less racist.
And it might be doing that.

651
00:39:45,590 --> 00:39:48,690
Like with all the flaws that I just
mentioned, which I really believe in,

652
00:39:49,470 --> 00:39:52,170
it might actually still be
better than what we have already.

653
00:39:53,100 --> 00:39:54,420
We do not have data for that.

654
00:39:54,840 --> 00:39:58,140
And the Department of Justice is not
coughing up that data. I've tried,

655
00:39:58,770 --> 00:40:02,790
I've tried to get, I've tried to get
data to do that. An audit of the DOJ.

656
00:40:03,240 --> 00:40:03,780
It should be,

657
00:40:03,780 --> 00:40:08,160
it's possible they did exist because some
jurisdictions have this stuff in, in,

658
00:40:08,180 --> 00:40:09,330
in use and some of them don't.

659
00:40:09,331 --> 00:40:13,380
You could just compare them before and
after or whatever and compare to each

660
00:40:13,381 --> 00:40:16,860
other. Um, I should also, but
I should also add that like,

661
00:40:17,370 --> 00:40:18,420
here's the good news,

662
00:40:18,690 --> 00:40:23,690
like if we made those recidivism
risk algorithms actually not racist,

663
00:40:25,140 --> 00:40:28,800
then that would definitely be better
than the current judges. We just,

664
00:40:28,830 --> 00:40:31,320
it's like we threw them out there and
we're just like, oh, they must be good.

665
00:40:31,321 --> 00:40:35,680
Because their algorithms, they're
by definition good. No, they're not.

666
00:40:35,880 --> 00:40:38,310
They're not necessarily good.
But again, you're right.

667
00:40:38,311 --> 00:40:40,140
They might be better
than the existing system.

668
00:40:40,310 --> 00:40:42,470
So in the other examples
that you talked about,

669
00:40:42,471 --> 00:40:47,471
do you have similar sense of whether
that data is out there like the teachers

670
00:40:48,201 --> 00:40:51,190
and you know, and maybe the
teachers, is that right?

671
00:40:51,270 --> 00:40:53,370
The teachers are, it's
like, it's just terrible.

672
00:40:53,430 --> 00:40:55,290
You're not even trying to
solve the right problem.

673
00:40:55,460 --> 00:40:57,110
You're not solving the room.
And I don't think, by the way,

674
00:40:57,111 --> 00:41:01,970
I still don't think recidivism risk being
high shouldn't necessarily mean you go

675
00:41:01,971 --> 00:41:06,290
to jail longer. Right? Maybe
we should be like, why? How do,

676
00:41:06,500 --> 00:41:08,420
why do we sentence people
the way we send his people?

677
00:41:08,421 --> 00:41:12,830
Like we should maybe make a Redo of that
entire conversation. Um, like we should,

678
00:41:13,010 --> 00:41:16,100
if we were thinking datadriven Lee,
if we're like,

679
00:41:16,101 --> 00:41:21,101
if we're Google thinking people like
we should ask really basic questions.

680
00:41:21,320 --> 00:41:24,570
Like to what extent does GED training in,

681
00:41:24,680 --> 00:41:26,630
in a prison help people when they leave?

682
00:41:27,230 --> 00:41:32,230
To what extent does solitary confinement
help or hurt people or sexual assault

683
00:41:32,811 --> 00:41:37,730
like should actually know what these
things are doing to our final outcomes,

684
00:41:37,731 --> 00:41:42,260
which hopefully is a combination of
public safety and the wellbeing of the

685
00:41:42,261 --> 00:41:44,600
actual prisoners.
We don't have any of that.

686
00:41:45,970 --> 00:41:46,803
Yup.

687
00:41:50,320 --> 00:41:55,000
I entirely agree with your, uh, uh, or
was he a wonderful notion that, that uh,

688
00:41:55,050 --> 00:41:58,600
that some of this data is disparate?
I mean it hurts, hurts, hurts,

689
00:41:58,601 --> 00:42:02,540
vulnerable communities. But
despite that, um, like the, the,

690
00:42:02,541 --> 00:42:05,650
the thing you most of the attention
to with was the fact that, uh,

691
00:42:05,740 --> 00:42:08,890
people whose relatives are criminals
are judged more likely to commit crimes.

692
00:42:10,090 --> 00:42:13,090
Well, people whose relatives are actors
are more likely to become actors.

693
00:42:13,120 --> 00:42:15,580
People's lives was a plumber's,
I'm more likely to become plumbers.

694
00:42:15,670 --> 00:42:17,770
And even though it's a horrible thing,

695
00:42:18,310 --> 00:42:22,210
I strongly suspect that someone who's
who is someone whose parents grew up by

696
00:42:22,211 --> 00:42:23,870
stealing cars. No. A heck of a lot of us.

697
00:42:23,871 --> 00:42:25,480
It more about stealing cars
and someone's who's Don,

698
00:42:26,660 --> 00:42:30,080
what do you suggest?
I suggest we follow the constitution,

699
00:42:30,081 --> 00:42:34,490
which privileges justice
over anything like that.

700
00:42:35,410 --> 00:42:35,780
Okay.

701
00:42:35,780 --> 00:42:38,450
So just the way our constitution
says written, send it,

702
00:42:38,451 --> 00:42:42,350
send the police to Wall Street because
it's equally likely to have muggings

703
00:42:43,310 --> 00:42:47,750
culture. We decide what is,
what is against the law, but as,

704
00:42:47,840 --> 00:42:51,980
but the constitution declares that
we have to care about fairness to the

705
00:42:52,310 --> 00:42:53,450
criminal of above all.

706
00:42:53,720 --> 00:42:56,930
And that this is not fairness to the
criminal by saying you're implicated

707
00:42:56,931 --> 00:42:58,700
because your father was implicated.
And I just,

708
00:42:58,790 --> 00:43:03,140
I also want to add that like I lived
next to Columbia University and every

709
00:43:03,140 --> 00:43:07,850
couple of years there's a huge drug
bust in the, in the fraternity's, right?

710
00:43:07,880 --> 00:43:09,080
Like a huge drug bust.

711
00:43:10,290 --> 00:43:11,220
And then,
and then

712
00:43:11,890 --> 00:43:13,240
I followed the Nyp d and they,

713
00:43:13,250 --> 00:43:16,900
they're constantly boasting about their
datadriven criminal justice stuff and

714
00:43:16,901 --> 00:43:21,901
how they peg people as gang members if
they associated on social media with gang

715
00:43:23,801 --> 00:43:27,430
members. But those gang members are
always in Harlem and they're black.

716
00:43:27,960 --> 00:43:30,190
You see what I mean?
They're not Columbia students.

717
00:43:31,090 --> 00:43:35,080
So it's also an inconsistently
defined association

718
00:43:36,770 --> 00:43:40,180
is, so there's two different kinds of
problems. Okay. All right. That's it.

719
00:43:40,300 --> 00:43:41,040
Thank you.

720
00:43:41,040 --> 00:43:41,873
Yup.

721
00:43:44,800 --> 00:43:47,010
So I think it's,
uh,

722
00:43:47,320 --> 00:43:52,180
it's very easy to see that if you're not
careful about how you define what kinds

723
00:43:52,181 --> 00:43:56,770
of success you're optimizing for,
you wind up with an algorithm that,

724
00:43:56,830 --> 00:44:00,340
um,
very strongly reinforces the status quo.

725
00:44:00,341 --> 00:44:04,210
And if we're looking at things
like our hiring practices fair,

726
00:44:04,540 --> 00:44:09,070
then if there is existing on fairness,
we're going to perpetuate it. So that,

727
00:44:09,550 --> 00:44:11,800
that makes total sense to me.
What,

728
00:44:12,580 --> 00:44:17,580
what I struggle with is if I'm asked to
do something like that and I try to not,

729
00:44:20,550 --> 00:44:24,100
uh, incorporate the existing, um,

730
00:44:24,130 --> 00:44:28,540
structural biases into the algorithm,
it's going to be my,

731
00:44:28,541 --> 00:44:31,440
do you know,
recommended hiring practices thing?

732
00:44:31,950 --> 00:44:36,950
How do I deal with the challenge that
I'm actually optimizing away from success

733
00:44:38,610 --> 00:44:39,600
conditions?

734
00:44:41,120 --> 00:44:41,953
Yeah.

735
00:44:42,040 --> 00:44:46,100
Okay. So there's two strategies
and I suggest the first one is, um,

736
00:44:46,540 --> 00:44:49,390
basically a parable. Um, so there's this,

737
00:44:49,770 --> 00:44:52,570
they think all the blind
audition for orchestras, right?

738
00:44:53,530 --> 00:44:54,670
Where the orchestra were trying,

739
00:44:54,700 --> 00:44:58,060
they acknowledged that they were being
nepotistic and their practices of hiring.

740
00:44:58,090 --> 00:45:00,040
So they wanted to get
rid of their nepotism.

741
00:45:00,460 --> 00:45:05,350
So they decided to put a sheet between
their judges and the auditioner so they

742
00:45:05,351 --> 00:45:09,880
wouldn't know if the person who was behind
the cause, she was their friend. Um,

743
00:45:10,090 --> 00:45:12,520
at first they saw the shoes and
then they're like, wait a second,

744
00:45:12,521 --> 00:45:13,630
we can see if it's a matter of woman.

745
00:45:13,631 --> 00:45:16,630
So they brought the sheet down to the
ground and they also installed rugs in the

746
00:45:16,631 --> 00:45:20,290
hallway walking up to the spot so they
couldn't hear. If it's high heels are not,

747
00:45:20,950 --> 00:45:22,450
not only did they get rid of nepotism,

748
00:45:22,451 --> 00:45:25,990
but they increase the number of women
in orchestras by a factor of five.

749
00:45:27,180 --> 00:45:27,880
Yeah.

750
00:45:27,880 --> 00:45:32,420
Um, I would, so that's the
story. And like the character,

751
00:45:32,430 --> 00:45:36,880
the way I would characterize what they
successfully did there is to two things.

752
00:45:37,210 --> 00:45:39,970
First they decided our priori,

753
00:45:40,030 --> 00:45:42,760
what was it that was a
requirement for this job?

754
00:45:42,910 --> 00:45:46,150
How do you assess somebody for this
job? And it was, the answer was sound.

755
00:45:47,290 --> 00:45:48,130
The second thing they did,

756
00:45:48,131 --> 00:45:52,900
which was absolutely as important as
the first thing was to ignore everything

757
00:45:52,901 --> 00:45:53,734
else.

758
00:45:54,580 --> 00:45:58,510
The promise of big data that we get
confused by because it sounds really

759
00:45:58,511 --> 00:46:00,250
convincing.
What's wrong?

760
00:46:00,610 --> 00:46:04,060
The promise of big data is the
more the data is, the better.

761
00:46:04,630 --> 00:46:09,370
Just throw all this data at
the wall. Correlations are
just as good as causation.

762
00:46:10,090 --> 00:46:10,923
Um,

763
00:46:11,350 --> 00:46:15,340
not true because if we did
the orchestra example again,

764
00:46:15,341 --> 00:46:18,730
but we didn't have the sheet
and we thought to ourselves,

765
00:46:18,731 --> 00:46:20,050
we're looking for a good sound,

766
00:46:20,350 --> 00:46:24,010
but we also knew that it was our friend
or we also knew that it was a woman that

767
00:46:24,011 --> 00:46:26,440
creeps into us.
It creeps into our brain.

768
00:46:27,250 --> 00:46:30,430
That's excess information that we should
be ignoring but we're not ignoring.

769
00:46:31,420 --> 00:46:35,440
So the first strategy I would suggest for
hiring people is to think opera, right?

770
00:46:35,441 --> 00:46:39,080
To actually build a model that you buy,
buy,

771
00:46:39,430 --> 00:46:43,360
buy construction is fair.
So build the model and say,

772
00:46:43,361 --> 00:46:45,400
these are the things we
actually want for this job.

773
00:46:46,480 --> 00:46:50,650
The second possibility is use machine
learning algorithm that takes all these

774
00:46:50,651 --> 00:46:55,180
correlations of blah, blah blah. But
then audit it for fairness than say,

775
00:46:55,420 --> 00:46:58,870
wait, does this basically
filter out women check,

776
00:46:59,350 --> 00:47:00,820
check to see if women are filtered out.

777
00:47:01,150 --> 00:47:03,790
Check to see whether they're
filtered out in a reasonable way.

778
00:47:04,690 --> 00:47:09,580
Sometimes they are. Sometimes sometimes
you like, I've heard the story,

779
00:47:09,581 --> 00:47:14,380
I'll go on fee on foot or just the other
day we're way more Asian people were

780
00:47:14,381 --> 00:47:16,800
filtered out of a certain job.
Um,

781
00:47:16,960 --> 00:47:19,660
and people were complaining because
so many of the applicants were Asian,

782
00:47:19,661 --> 00:47:20,620
but then people were saying,
well,

783
00:47:20,621 --> 00:47:23,320
the people who they were more Asians
that were applying that weren't actually

784
00:47:23,321 --> 00:47:24,430
qualified for this job.

785
00:47:24,850 --> 00:47:28,390
Like it's complicated and
expect it to be complicated.

786
00:47:28,391 --> 00:47:31,450
That's my third suggestion is that the,

787
00:47:31,520 --> 00:47:34,360
the other promise of big data that I'm
pushing back against and I hope you

788
00:47:34,361 --> 00:47:38,440
understand and agree with,
is that big data is not a silver bullet.

789
00:47:39,550 --> 00:47:44,290
It's just a tool and is not automatically
going to solve our problems.

790
00:47:44,320 --> 00:47:48,070
It's just a tool that we might be able
to use to solve some of our problems,

791
00:47:48,100 --> 00:47:49,930
but we have to be careful about it.
We have to check.

792
00:47:53,570 --> 00:47:58,570
That's stories where you would go to
a judge or a jurisdiction or a school

793
00:47:58,871 --> 00:48:02,380
system and they would say, oh my God,
you're right. We have to fix this.

794
00:48:02,381 --> 00:48:05,060
And they do. Sorry, what,

795
00:48:05,330 --> 00:48:08,690
do you have any other success stories
like the orchestra where you might go to

796
00:48:08,691 --> 00:48:11,660
someone and say, you know, what was
this problem? And they said, oh my gosh,

797
00:48:11,720 --> 00:48:15,770
as opposed to, no, you can't see our
data now. That's proprietary. Go Away.

798
00:48:15,771 --> 00:48:16,610
Don't bother us.

799
00:48:16,730 --> 00:48:19,550
No. You'd be surprised how few people
want me to see their source code.

800
00:48:21,230 --> 00:48:22,063
Um,

801
00:48:23,450 --> 00:48:27,290
although I should say that like someone
asked me whether I thought it would just

802
00:48:27,291 --> 00:48:29,210
be impossible to ever get
a good value out of it.

803
00:48:29,211 --> 00:48:33,740
Like a good teacher assessment tool using
data and I said I'll never say never

804
00:48:33,741 --> 00:48:37,040
because like 10 years ago I didn't think
we'd have self driving cars but I think

805
00:48:37,041 --> 00:48:40,640
now that we're going to save lives from
drunk driving with self driving cars,

806
00:48:41,180 --> 00:48:45,340
but what is it going to take to get a
good teacher assessment tool is w we're

807
00:48:45,341 --> 00:48:49,040
going to have to have a lot of evidence
that this is actually working right now

808
00:48:49,041 --> 00:48:52,940
we have no ground truth for teacher
value added model. Literally,

809
00:48:52,941 --> 00:48:56,270
there is no comparison to some other
kind of qualitative assessment to see

810
00:48:56,271 --> 00:48:57,410
whether a teacher's good,

811
00:48:57,590 --> 00:48:59,930
a good teacher is getting a good score
and a bad teacher getting bad scores.

812
00:48:59,930 --> 00:49:03,710
It's just whatever the number is is
your score, which is ridiculous, right?

813
00:49:03,950 --> 00:49:07,790
So we actually do have qualitative
assessments for teachers, their political,

814
00:49:07,791 --> 00:49:10,790
because different people disagree
about what makes a good teacher.

815
00:49:10,820 --> 00:49:14,030
But like let's say we had a agreement,
we're going to,

816
00:49:14,150 --> 00:49:18,880
we're defining this kind of qualitative
assessment to be important, to be the,

817
00:49:18,881 --> 00:49:22,080
the, the ground truth. And then
we could try to find, you know,

818
00:49:22,090 --> 00:49:24,170
maybe we put a bunch of
sensors in the classroom.

819
00:49:24,171 --> 00:49:27,590
We tried to replicate that using data,
literally replicate it.

820
00:49:27,980 --> 00:49:32,980
And if we got an algorithm that after
4,000 teachers got almost exactly the same

821
00:49:33,141 --> 00:49:36,230
score using the qualitative assessment
is using this data driven assessment,

822
00:49:36,500 --> 00:49:40,490
then we would build trust that this data
driven assessment is doing a good job.

823
00:49:40,491 --> 00:49:42,380
It's very similar to saying,
you know,

824
00:49:42,381 --> 00:49:45,590
we have this many miles of a self
driving car without an accident.

825
00:49:45,710 --> 00:49:50,580
So we start trusting that car, that
algorithm. Does that make sense? Well,

826
00:49:50,810 --> 00:49:54,260
right now we have nothing like that
in the, in the field of teacher value.

827
00:49:54,770 --> 00:49:57,500
They just want a silver bullet.
That's all they want.

828
00:49:58,130 --> 00:49:59,780
They don't want any prying eyes either.

829
00:50:00,320 --> 00:50:04,790
Something that's kind of come up a
bunch is uh, sort of as an undercurrent,

830
00:50:04,791 --> 00:50:09,380
like the idea of open sourcing,
how you're doing things.

831
00:50:09,680 --> 00:50:13,670
And I can see that like potentially
being really helpful. For example,

832
00:50:13,730 --> 00:50:16,610
now you can actually criticize
because you can see the code,

833
00:50:16,970 --> 00:50:21,290
but I can also see it being somewhat
ineffective if you're still doing,

834
00:50:21,320 --> 00:50:26,140
if you're still asking questions that
are totally beside the point. Um, so I,

835
00:50:26,141 --> 00:50:28,070
I guess I'm curious your thoughts as

836
00:50:28,070 --> 00:50:32,450
to how the effectiveness of things like
open sourcing and also the limitations.

837
00:50:34,230 --> 00:50:38,860
Um, that's a great question and you guys
know that it's hard, um, because if,

838
00:50:39,910 --> 00:50:41,170
because it's religion,

839
00:50:41,230 --> 00:50:44,710
it's relatively easy to make something
arbitrarily difficult to understand.

840
00:50:45,760 --> 00:50:47,560
If you know in advance that
it's going to be open source,

841
00:50:47,561 --> 00:50:50,230
you can just make it
impossible to understand. Um,

842
00:50:50,250 --> 00:50:54,310
and like the other example I give is like
nobody would want the code for Google

843
00:50:54,311 --> 00:50:58,090
search because literally it probably will
not work on any computer system except

844
00:50:58,091 --> 00:50:59,290
Google's computer system.
Right?

845
00:50:59,770 --> 00:51:04,480
So there's a limit to what open
source and can do. But I I don't that,

846
00:51:04,530 --> 00:51:06,490
that's not to say I
think we should give up.

847
00:51:06,491 --> 00:51:10,750
I think what we should ask for is
an auditing, a fairness auditing.

848
00:51:11,080 --> 00:51:14,080
And I think we should think about along
the lines of the way sociologists auto

849
00:51:14,081 --> 00:51:14,801
things for fairness.

850
00:51:14,801 --> 00:51:19,090
So like sociologists will see whether
a hiring practices racist by sending a

851
00:51:19,091 --> 00:51:23,860
bunch of applications with black names
and white names and like similar kind of

852
00:51:23,861 --> 00:51:27,310
qualifications and see whether white
people get more call backs so we can do

853
00:51:27,311 --> 00:51:30,880
that kind of thing to
algorithms. And Latanya Sweeney
did that for Google search.

854
00:51:31,170 --> 00:51:33,790
Do you guys know that
example with Latanya Sweeney,

855
00:51:33,791 --> 00:51:37,080
Google's her own name and
she found the ad next to her.

856
00:51:37,300 --> 00:51:40,810
The search results was are you looking
for the criminal arrest records for

857
00:51:40,811 --> 00:51:43,840
Latanya Sweetie? Then she googled
the white name. It didn't happen.

858
00:51:44,380 --> 00:51:48,100
Then you did a comprehensive task cause
she's a computer scientists and found

859
00:51:48,101 --> 00:51:51,100
that black names were way more likely
to have a rest records act to them.

860
00:51:51,340 --> 00:51:52,930
This ad for arrest records.

861
00:51:53,610 --> 00:51:56,970
Now that's not in my opinion of what a
weapon of math destruction because it's

862
00:51:57,040 --> 00:52:01,510
not as directly impactful in people's
lives, but it's certainly not. Not Great.

863
00:52:03,080 --> 00:52:04,640
Go ahead. Thanks. Um,

864
00:52:04,700 --> 00:52:08,000
I was just going to agree with you before
when you were talking about there is

865
00:52:08,001 --> 00:52:11,000
no easy way to do the teacher evaluation.

866
00:52:11,270 --> 00:52:13,730
I think it goes back to your
point of success, right?

867
00:52:13,750 --> 00:52:18,260
Cause is it we then it gets into like
student evaluation and our, you know,

868
00:52:18,261 --> 00:52:22,370
the assessments that they do on students
fair or is that just a proxy for

869
00:52:22,371 --> 00:52:25,340
success in long term and
their long term career?

870
00:52:25,610 --> 00:52:28,700
So I think the teacher evaluation,
there's probably no right answer.

871
00:52:28,990 --> 00:52:33,580
Um, do you agree? Yeah, it's, it's
a very tricky problem. So it's,

872
00:52:33,910 --> 00:52:34,660
I think the trickier,

873
00:52:34,660 --> 00:52:38,950
the question is that much the less likely
it's going to be solved by some simple

874
00:52:39,040 --> 00:52:40,060
scoring system.

875
00:52:40,170 --> 00:52:41,003
Yeah.

876
00:52:44,580 --> 00:52:47,480
A lot of the machine learning I have
mustard are very popular these days of

877
00:52:47,481 --> 00:52:51,720
written. It's like neural network switch.
Not that I designed. There are pig,

878
00:52:51,750 --> 00:52:54,110
you know, without even batting
better tensions. Like you know,

879
00:52:54,120 --> 00:52:57,090
if you click on the little white button
about why it made it give a decision,

880
00:52:57,290 --> 00:53:00,660
this is say I did these 3000 matrix
multiplies and there's no simpler answer

881
00:53:00,661 --> 00:53:01,980
than that.
Uh,

882
00:53:02,610 --> 00:53:05,790
but they do give much better predictions
then other things like decision trees,

883
00:53:05,791 --> 00:53:07,140
which are much more interpretable.

884
00:53:07,470 --> 00:53:10,260
Do you have any thoughts about how
in the context of these problems to,

885
00:53:11,280 --> 00:53:12,180
should we,
you know,

886
00:53:12,690 --> 00:53:15,250
do you have any deliberately worst
predictions because we need them to be in

887
00:53:15,251 --> 00:53:15,730
trouble?

888
00:53:15,730 --> 00:53:17,470
Yes, absolutely. Okay.

889
00:53:17,530 --> 00:53:19,900
That's probably the hardest thing that
you guys are gonna hear from me today,

890
00:53:19,901 --> 00:53:24,220
but I definitely think we need to
sacrifice accuracy for fairness.

891
00:53:25,650 --> 00:53:29,940
Absolutely. And one of the reasons
is because we as technologists,

892
00:53:30,050 --> 00:53:34,260
we are not the ones that are at risk.
So our,

893
00:53:34,320 --> 00:53:38,250
our concept of what looks fair
to us not good enough, right?

894
00:53:38,251 --> 00:53:42,000
We might have some better understanding
of how things work, but we're like, oh,

895
00:53:42,001 --> 00:53:45,480
but it makes it so much more
accurate. Um, and I actually,

896
00:53:45,570 --> 00:53:48,750
I talked to somebody who does
recidivism risk algorithms for a living.

897
00:53:48,751 --> 00:53:53,460
He does it for this for a
state. Um, and I said, well,

898
00:53:53,461 --> 00:53:56,760
do you ever use race? And he was like, oh
no, that would be wrong. And I said, well,

899
00:53:56,761 --> 00:53:58,290
do you ever use zip
code? He was like, well,

900
00:53:58,370 --> 00:54:02,400
it's a sometimes because it makes it so
much more accurate. And I'm like, okay,

901
00:54:02,401 --> 00:54:06,360
but that's a proxy for race. So
you're basically using race. Um,

902
00:54:07,950 --> 00:54:11,880
so another way of saying that is
if it's interpretable, then people,

903
00:54:11,881 --> 00:54:15,030
it's much more easy for other
people to say that's not fair.

904
00:54:15,210 --> 00:54:19,110
It's a transparency measure. There
is precedent for this, by the way,

905
00:54:19,140 --> 00:54:20,580
if in credit card law,

906
00:54:20,760 --> 00:54:24,810
if your credit card company and so
you denied someone a credit card,

907
00:54:24,930 --> 00:54:26,430
you have to be able to explain why,

908
00:54:26,640 --> 00:54:30,780
which strictly restricts people for two
using decision trees for the most part.

909
00:54:31,740 --> 00:54:35,460
Um, they don't like it, but that's okay.

910
00:54:35,461 --> 00:54:40,461
There's actually lots of things about
lending that are a trade off for fairness

911
00:54:41,461 --> 00:54:42,294
and accuracy.

912
00:54:42,300 --> 00:54:47,220
So Fico scores by law and
discrimination laws called freedom, uh,

913
00:54:47,221 --> 00:54:52,221
fair credit reporting act and the equal
credit opportunity act make it illegal

914
00:54:53,101 --> 00:54:57,360
for you to base fico scores on
race or gender or a zip code.

915
00:54:58,650 --> 00:55:03,480
Is that because it's La it's more accurate
when you restrict know like it would,

916
00:55:04,230 --> 00:55:05,220
it's,
it's actually,

917
00:55:05,330 --> 00:55:09,870
it's keeping these credit scores for
being as accurate as they might be,

918
00:55:11,190 --> 00:55:15,300
but it was deemed more fair
that way. As a society we, we,

919
00:55:15,350 --> 00:55:19,890
we care about public, the public,
right? So the idea is, and this was,

920
00:55:20,130 --> 00:55:22,980
this was soon, like the basic,
what's happened in the 70s when like,

921
00:55:23,010 --> 00:55:26,760
especially in a divorce,
women were being denied loans.

922
00:55:27,540 --> 00:55:30,960
And the idea of there was if
you'd never let women get loans,

923
00:55:31,680 --> 00:55:34,140
then they're never going to be able
to build up their credit scores.

924
00:55:34,170 --> 00:55:38,940
So there'll be credit worthy. So it's like
a feedback loop, right? And so this was,

925
00:55:38,970 --> 00:55:43,230
these antidiscrimination laws
were specifically designed
to prevent the feedback

926
00:55:43,231 --> 00:55:46,770
loop. Um, and we, we need that,
we need that kind of thing.

927
00:55:46,980 --> 00:55:51,980
Because algorithms like that algorithm
I just talked about that benefits the,

928
00:55:52,520 --> 00:55:56,940
the low the banks or the people that make
loans to the detriment of the public.

929
00:55:56,941 --> 00:56:01,230
So we have to balance the benefits of the
for the public versus the benefits for

930
00:56:01,231 --> 00:56:02,160
the private companies.

931
00:56:05,080 --> 00:56:06,370
I have time for one question.

932
00:56:10,730 --> 00:56:10,941
Sorry.

933
00:56:10,941 --> 00:56:15,290
I was going to say I'm in the opposite
boat where I work on a search feature

934
00:56:15,291 --> 00:56:18,260
that looks at lots of queries and uh,

935
00:56:18,440 --> 00:56:22,700
we have a model for English where,
uh, the masculine pronouns are the,

936
00:56:23,440 --> 00:56:25,720
and some common names
are the default to it.

937
00:56:26,410 --> 00:56:31,410
The model knows a lot about that and
we're kind of wondering if queries that

938
00:56:31,511 --> 00:56:36,511
women like we do terrible on or worse or
there's some class except we don't know

939
00:56:36,671 --> 00:56:38,170
anything about the people
who ask the queries.

940
00:56:38,171 --> 00:56:42,820
We've tried really hard not to know,
I don't know gender, I don't know,

941
00:56:43,210 --> 00:56:46,720
income.
I could probably get zip code in bulk.

942
00:56:48,040 --> 00:56:50,830
So, um, I'm sitting here,

943
00:56:50,831 --> 00:56:54,910
I'm trying to think how I'd write a
machine learning model that would do the

944
00:56:54,911 --> 00:56:59,500
reverse question and figure out what's
the most unfair thing my current system

945
00:56:59,501 --> 00:57:00,300
is up to.

946
00:57:00,300 --> 00:57:03,000
That's a good question to be asking
yourself, but I would also add,

947
00:57:03,630 --> 00:57:05,400
and thank you for asking that she herself,

948
00:57:05,401 --> 00:57:08,340
I think that's what we all need to ask
because one of the first questions we

949
00:57:08,341 --> 00:57:11,850
should be asking ourselves, but I do
want to, you made the point yourself,

950
00:57:11,851 --> 00:57:15,590
but I'll just reiterate it and something
that I'm certainly not the first person.

951
00:57:15,591 --> 00:57:16,424
It makes this point.

952
00:57:17,610 --> 00:57:21,900
Sometimes you actually do need to know
these attributes like race and gender in

953
00:57:21,901 --> 00:57:23,730
order to measure your fairness.

954
00:57:24,510 --> 00:57:29,510
So I understand the desire to be
race neutral or gender neutral,

955
00:57:29,910 --> 00:57:34,140
but that doesn't mean you should not
collect that data because then you're

956
00:57:34,141 --> 00:57:36,480
basically saying, I'm not
going to look, you know,

957
00:57:36,481 --> 00:57:39,600
like it must be fair because I don't,
I don't collect that data.

958
00:57:39,630 --> 00:57:41,130
That's not proof that it's fair.

959
00:57:42,180 --> 00:57:42,500
Okay.

960
00:57:42,500 --> 00:57:45,230
Which isn't to say that you, you
know, it's easy to collect the data.

961
00:57:48,450 --> 00:57:49,283
Okay.

962
00:57:49,290 --> 00:57:54,290
I'm saying maybe collected at least in
and for some experiments as an audit.

963
00:57:56,430 --> 00:57:57,263
Thank you guys.

964
00:58:01,350 --> 00:58:01,490
Okay.

