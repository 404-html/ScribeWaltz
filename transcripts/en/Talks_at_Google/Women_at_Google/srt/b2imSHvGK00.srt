1
00:00:06,230 --> 00:00:11,120
Okay, thanks for coming. Uh,
I'll introduce our main speaker.

2
00:00:11,690 --> 00:00:16,690
Amy Kurzweil have known her for 30 years.

3
00:00:19,300 --> 00:00:23,170
It's a first vote, Huh? Yeah. That,

4
00:00:23,320 --> 00:00:24,420
that was planned.

5
00:00:28,030 --> 00:00:30,820
So she's written a great book.
Uh,

6
00:00:31,030 --> 00:00:35,980
she actually wrote it started
as a senior thesis at, uh,

7
00:00:36,010 --> 00:00:40,120
Stanford.
And finally three or four years later,

8
00:00:40,121 --> 00:00:43,810
it was finished and everybody
is finally relieved. Okay.

9
00:00:43,811 --> 00:00:46,200
You're going to get it out.
And she looked sitting there.

10
00:00:46,330 --> 00:00:49,420
The illustrations at the end are much
better than the ones at the beginning.

11
00:00:49,750 --> 00:00:54,550
So I have to do it over and everyone
is calm. I got this. Uh, so,

12
00:00:54,551 --> 00:00:59,170
but she got that done and then only
another three or four years. Uh,

13
00:00:59,290 --> 00:01:04,120
it's a very labor intensive
process. It's, it's a great book.

14
00:01:04,121 --> 00:01:08,990
Uh, if I say so from our
family's perspective, uh,

15
00:01:10,120 --> 00:01:11,800
but other people have liked it.
And New York,

16
00:01:11,830 --> 00:01:14,890
it was a editor's pick
by the New York Times.

17
00:01:15,490 --> 00:01:20,490
They put it on their list of the best
graphic novels or 2016 Kirkus reviews said

18
00:01:22,951 --> 00:01:26,830
it was one of the best memories of 2016.
Um,

19
00:01:27,760 --> 00:01:30,730
and so we,
we will talk about that,

20
00:01:30,731 --> 00:01:33,760
but we'll talk about some
other collaborative projects
cause we've actually

21
00:01:33,761 --> 00:01:35,350
worked,
um,

22
00:01:35,380 --> 00:01:39,310
quite a few things as we went
through it for this presentation.

23
00:01:40,270 --> 00:01:44,110
Uh, she's, uh, her comics
had been in the New Yorker,

24
00:01:44,180 --> 00:01:45,620
a New Yorker.

25
00:01:45,621 --> 00:01:50,440
I had a compilation of the best cartoons
in the New Yorker for 2016 and two of

26
00:01:50,441 --> 00:01:53,680
our cartoons when they are in, she'll
show, she'll share some of those with you.

27
00:01:54,610 --> 00:01:59,530
Uh, she's been a number
of other publications. Uh,

28
00:01:59,620 --> 00:02:02,530
Shenandoah, the toast, they all,

29
00:02:03,070 --> 00:02:07,090
she has a ba from Stanford University
and the MFA from the new school.

30
00:02:07,091 --> 00:02:12,091
She's an adjunct professor of writing
comics at Parsons and the Fashion

31
00:02:13,600 --> 00:02:15,590
Institute of Technology.
So welcome Amy as well.

32
00:02:21,740 --> 00:02:22,573
Thanks Dad.

33
00:02:25,550 --> 00:02:28,430
Thank you everyone for having
me. This is such a pleasure. Um,

34
00:02:28,460 --> 00:02:32,480
so I'm going to introduce the man
who needs no introduction. Uh,

35
00:02:32,481 --> 00:02:34,550
you probably know Ray Kurzweil.

36
00:02:34,670 --> 00:02:38,750
He's been director of engineering
here at Google for over four years.

37
00:02:39,140 --> 00:02:43,370
A director, a director, uh,
works on natural language,

38
00:02:43,371 --> 00:02:48,020
understanding his inventions
include, uh, the flatbeds, Danner,

39
00:02:48,260 --> 00:02:52,640
omnifont OCR, a large
vocabulary, speech recognition,

40
00:02:52,670 --> 00:02:56,960
realistic music synthesis. He's been
awarded the National Medal of Technology,

41
00:02:56,961 --> 00:03:00,100
inducted into the inventor's hall of fame.
He has 21

42
00:03:00,100 --> 00:03:04,990
honorary doctorates, a grammy, and he's
written five national bestselling books.

43
00:03:05,410 --> 00:03:06,243
Not Bad.

44
00:03:07,820 --> 00:03:08,653
Okay.

45
00:03:12,950 --> 00:03:14,300
You might not know,

46
00:03:14,301 --> 00:03:19,301
however that one of his lesser known but
still very significant achievements is

47
00:03:21,501 --> 00:03:25,520
that he plays a bit part in the
smash hit graphic memoir, flying cow,

48
00:03:29,810 --> 00:03:33,080
doing such important things
as helping me get to sleep.

49
00:03:33,520 --> 00:03:35,000
That's so thank you dad for that.

50
00:03:36,610 --> 00:03:39,830
I'm going to tell you a bit about flying
couch before we move on to other things.

51
00:03:40,160 --> 00:03:42,980
Uh, this is the story of
three generations of women,

52
00:03:43,310 --> 00:03:45,870
the first myself young artists.

53
00:03:46,490 --> 00:03:51,490
And the second is my mother also
happens to be the wife of Ray Kurzweil.

54
00:03:53,870 --> 00:03:57,950
Uh, Sonia Kurzwell. She's
a psychologist. And uh,

55
00:03:58,550 --> 00:04:02,210
last but not least, my grandmother,
my mother's mother, Lily Fenster,

56
00:04:02,540 --> 00:04:06,380
she's 90 years old. We just came
from seeing her in Naples, Florida,

57
00:04:06,381 --> 00:04:09,770
where I did a book event down there.
She was so excited.

58
00:04:10,190 --> 00:04:14,910
She hasn't changed a bit in 42 years.
Yeah.

59
00:04:15,080 --> 00:04:18,110
Here she is as a young
woman. Um, when I show up.

60
00:04:18,111 --> 00:04:19,940
So we did this presentation in Naples,
Florida,

61
00:04:19,941 --> 00:04:23,210
and she was there in the audience the
first time that she's ever been in the

62
00:04:23,211 --> 00:04:24,680
audience while I've been
talking about the book.

63
00:04:24,681 --> 00:04:28,880
And she was just interrupting every
five seconds. Exuberantly like she saw.

64
00:04:28,881 --> 00:04:30,920
She's like, where did you
get that picture? You know,

65
00:04:30,921 --> 00:04:35,840
she was so excited about everything.
And then also in some moments, uh,

66
00:04:36,020 --> 00:04:39,350
it was difficult for her to watch
because in this story flying couch,

67
00:04:39,351 --> 00:04:44,070
I'm telling about her early life during
the Holocaust when she left the worst

68
00:04:44,090 --> 00:04:47,690
ghetto survived on her own.
She was the only person who survived.

69
00:04:47,960 --> 00:04:51,650
So it's a difficult story but
also an exuberant story. Um,

70
00:04:51,680 --> 00:04:56,680
the best way to give you the flavor of
the book is to show you this animated

71
00:04:56,751 --> 00:05:01,550
book trailer that I slaved over. So,
um, those of you who are animators,

72
00:05:01,551 --> 00:05:03,260
I have much respect for your work

73
00:05:07,840 --> 00:05:12,480
so you understand what we're doing.
I am this then us questions.

74
00:05:12,481 --> 00:05:17,080
So I just say I'm lowly fence the night.
My whole look was her revive it. Okay.

75
00:05:17,081 --> 00:05:19,630
So tell everyone about yourself,
huh?

76
00:05:24,520 --> 00:05:26,320
How do you feel about being a character?
In my book,

77
00:05:26,890 --> 00:05:31,230
I love it mean be Dick head
and it's mostly like cut, cut,

78
00:05:31,640 --> 00:05:34,410
cut, kind of the at you. Paramount.

79
00:05:36,460 --> 00:05:39,820
So you know, but why did you
pick that? I have no idea.

80
00:05:40,450 --> 00:05:45,400
Who would light the number? Old Jewish
woman. Is it going to be successful

81
00:05:49,930 --> 00:05:54,430
showing you? I use a Ph d
child, 10 colleges for children.

82
00:05:54,820 --> 00:05:56,290
I arranged this.
My kids,

83
00:05:56,291 --> 00:06:00,500
I called Amy has a heart of gold.

84
00:06:00,950 --> 00:06:05,950
She goes my first granddaughter and he
wants to feel what they went through.

85
00:06:13,990 --> 00:06:18,370
Well, I'm thinking I would like
it to get married. One Kid Hammer.

86
00:06:18,390 --> 00:06:23,380
We should stick together.
Then chaplains that you flesh.

87
00:06:24,070 --> 00:06:28,630
If you gave a good family life,
you ever good light? You survive.

88
00:06:32,660 --> 00:06:33,330
Yeah.

89
00:06:33,330 --> 00:06:37,070
Do you want to sing something?
You want me to sit and get a thing?

90
00:06:37,320 --> 00:06:38,940
Should I sing in Jewish?

91
00:06:45,550 --> 00:06:48,460
If you want to hear my grandmother's
thing, that can be a ranch.

92
00:06:52,100 --> 00:06:52,933
Okay,

93
00:06:53,940 --> 00:06:57,360
so you started writing this
book eight years ago. Why?

94
00:06:57,361 --> 00:07:01,210
Why was this a story you want it
to tell them what took you so long?

95
00:07:02,770 --> 00:07:07,010
Well, as I said, I had to draw it twice.
So that's why it took so long. Um,

96
00:07:07,270 --> 00:07:11,130
so this project started when I
was in college and my mother, um,

97
00:07:11,530 --> 00:07:15,280
gave me this transcript that my
grandmother had done with a historian,

98
00:07:15,310 --> 00:07:19,630
Holocaust historian at the
University of Michigan. And she, uh,

99
00:07:20,020 --> 00:07:24,190
she painstakingly and in a lot of
detailed told about everything that had

100
00:07:24,191 --> 00:07:27,880
happened to her. Of course,
not in a totally linear way,

101
00:07:27,881 --> 00:07:32,620
but everything was recorded. All these
memories, things that I hadn't heard, um,

102
00:07:33,070 --> 00:07:37,420
all at once, but that I'd heard
pieces of for my whole life. And, uh,

103
00:07:38,170 --> 00:07:41,710
things like, um, her,
her early family life,

104
00:07:41,711 --> 00:07:45,610
things like that I never knew about or
the antisemitism that existed in Europe

105
00:07:45,611 --> 00:07:47,560
before the war.
Um,

106
00:07:48,470 --> 00:07:52,480
the story of her sneaking through
the wall and the Warsaw ghetto,

107
00:07:52,481 --> 00:07:56,860
there was a few bricks and she was skinny,
so she snuck out through the wall. Um,

108
00:07:56,920 --> 00:07:58,480
I'd heard fragments of these stories,

109
00:07:58,481 --> 00:08:02,350
but I've never really put
them into a coherent whole.

110
00:08:02,351 --> 00:08:06,220
And I never really understood exactly
what that might mean for me to grow up in

111
00:08:06,221 --> 00:08:09,700
the shadow of such a dramatic history.
Um,

112
00:08:10,120 --> 00:08:14,080
it affects you and it's
not exactly clear how, um,

113
00:08:14,830 --> 00:08:19,510
so I was a relatively normal kid,
um,

114
00:08:19,900 --> 00:08:24,670
sensitive and spent a lot of time
involved in my creative pursuits.

115
00:08:25,240 --> 00:08:28,060
Not technologically inclined necessarily,

116
00:08:28,090 --> 00:08:32,890
but I'm creating my own world.
And normal is irrelative.

117
00:08:34,500 --> 00:08:35,333
Yeah.

118
00:08:35,950 --> 00:08:40,840
I had some strange habits
or strange bends of mind,

119
00:08:41,020 --> 00:08:46,020
certain curiosities about dark things
and curiosities about this history.

120
00:08:46,330 --> 00:08:50,740
And it felt to me like I
had to look at it closely.

121
00:08:51,070 --> 00:08:55,590
Um, here's an example of maybe
being a little strange a kid.

122
00:08:55,591 --> 00:08:58,950
Did anybody use to do that thing where
you watch your eyes dilate in the mirror?

123
00:08:59,940 --> 00:09:03,810
Yeah. So you can learn about some
of my other neuroses in the book,

124
00:09:03,811 --> 00:09:05,840
but that's one of the,
um,

125
00:09:06,030 --> 00:09:09,720
but the main thing that
spurred my interest in the
story was when I left home to

126
00:09:09,721 --> 00:09:12,210
go to Stanford,
which was those of you who've been there,

127
00:09:12,211 --> 00:09:15,960
it's not that far from here. It's just
a paradise of comfort and privilege.

128
00:09:15,961 --> 00:09:20,370
But I was really very anxious about
leaving home and it didn't make a lot of

129
00:09:20,371 --> 00:09:23,160
sense to me. You know, my life
was relatively comfortable.

130
00:09:23,161 --> 00:09:26,160
Why would I have so much
anxiety about leaving home?

131
00:09:26,370 --> 00:09:29,550
And when I started to learn more about
my grandmother's story about how she left

132
00:09:29,551 --> 00:09:31,410
home under these certain circumstances,

133
00:09:31,740 --> 00:09:35,840
I started to think that there
was something I'd inherited,
some sort of fear. And,

134
00:09:35,860 --> 00:09:38,190
and writing this book was
the way that I explored that.

135
00:09:40,400 --> 00:09:42,190
I mean,
I also heard,

136
00:09:42,280 --> 00:09:47,280
I've heard a little fragments and bits
and pieces of lowly story over the years,

137
00:09:50,200 --> 00:09:52,870
but there were little fragments.
And until I read Amy's book,

138
00:09:52,871 --> 00:09:56,920
I really didn't have a coherent
linear narrative of it.

139
00:09:56,921 --> 00:09:59,740
So it was quite a revelation to
actually get the story in the,

140
00:10:00,340 --> 00:10:05,320
in the linear sequence.
Something that I like about the book,

141
00:10:05,321 --> 00:10:10,321
it's really combines the holocausts with
Umer Umer both of Amy's coming of age

142
00:10:10,511 --> 00:10:11,344
and,

143
00:10:11,620 --> 00:10:16,620
and humor of Lily herself in
her current circumstances.

144
00:10:17,200 --> 00:10:19,540
That's very hard combination to pull off.

145
00:10:21,390 --> 00:10:25,060
I think the only other time I've seen
that in the work of is it's a beautiful

146
00:10:25,061 --> 00:10:30,010
life, the movie, but, but they're,
the humor is part of the story.

147
00:10:30,011 --> 00:10:34,660
The farther uses humor to
keep his son Zane. But anyway,

148
00:10:34,661 --> 00:10:36,220
so it's a hard combination.

149
00:10:37,090 --> 00:10:42,090
So I remember Amy's early efforts in
animation and drawing when she was in,

150
00:10:44,740 --> 00:10:46,300
I think 10th grade I attended,

151
00:10:47,040 --> 00:10:50,020
it was bring your parents to school day.

152
00:10:50,020 --> 00:10:55,020
And so I attended her math class and
she's like drawing these little drawings

153
00:10:55,571 --> 00:10:57,550
and Casey and looking up at the board,

154
00:10:57,551 --> 00:11:01,510
well obviously that angle is equal to
that one. Minus. I won. I said, Amy,

155
00:11:01,511 --> 00:11:05,280
that's really good math thinking since
this, it's not math thinking, it's,

156
00:11:05,281 --> 00:11:09,220
it's obvious that angle has to be equal
to that one and yet subtract that and

157
00:11:09,550 --> 00:11:14,230
said, well, I said, well,

158
00:11:14,231 --> 00:11:17,740
when you understand something
it always seems obvious.

159
00:11:19,030 --> 00:11:24,030
So we're very concerned here with girls
dropping out of math and stem in high

160
00:11:24,071 --> 00:11:24,461
school,

161
00:11:24,461 --> 00:11:29,461
which seems to be a phenomena and
we'd like to see more girls and women,

162
00:11:30,170 --> 00:11:33,430
uh, you know, entering these fields.

163
00:11:33,431 --> 00:11:37,900
Amy Fortunately had and has other skills,

164
00:11:39,300 --> 00:11:41,320
but she is a good mathematician.
So

165
00:11:42,860 --> 00:11:47,400
we need more programmers than our day
card groups and we may recruit her and,

166
00:11:47,410 --> 00:11:52,330
but, but she is busy with other
projects that she'll tell you about.

167
00:11:53,260 --> 00:11:56,620
So, uh, but how did you
transition to comics?

168
00:11:56,621 --> 00:12:00,310
Cause you at Stanford do studied
writing and you had this background in

169
00:12:00,311 --> 00:12:03,490
illustration.
When did you come to combine those two?

170
00:12:04,010 --> 00:12:05,800
Yeah.
Well yeah,

171
00:12:06,350 --> 00:12:10,850
my drawing career before this book was
mostly just drawing during math class.

172
00:12:10,851 --> 00:12:15,851
That was the main way that I got to
be an artist because they need to do

173
00:12:16,311 --> 00:12:18,410
something about the way they teach math.
I think that's,

174
00:12:18,411 --> 00:12:21,830
but that's a whole other
panel. Um, so yeah,

175
00:12:21,831 --> 00:12:24,500
I mean I was interested in drawing and
I was interested in writing and I didn't

176
00:12:24,501 --> 00:12:29,030
think to put the two together until I read
certain books, like mouse for example,

177
00:12:29,031 --> 00:12:31,910
which is another book that it
combines humor in the Holocaust,

178
00:12:32,150 --> 00:12:35,010
but it's a short list of
things that do that. Um,

179
00:12:35,090 --> 00:12:39,140
but mouse is definitely on that list.
And there's something interesting about,

180
00:12:39,620 --> 00:12:42,350
um, about drawing. So we've,

181
00:12:42,351 --> 00:12:46,910
somebody taught me that storytelling
is one of our first technologies.

182
00:12:46,970 --> 00:12:48,770
I wonder who told me that.
Um,

183
00:12:49,220 --> 00:12:54,050
and there was oral storytelling and
then there was written storytelling.

184
00:12:54,350 --> 00:12:56,990
Um, and it, it feels like
one progressed to the other,

185
00:12:56,991 --> 00:13:00,830
but there's this other thing in the middle
there which is drawing. Um, there's,

186
00:13:00,870 --> 00:13:05,870
there have been cave drawings
discovered, you know, 50,000 BCE. Um,

187
00:13:06,080 --> 00:13:09,800
and so people, early humans were
recording their lives that way.

188
00:13:10,100 --> 00:13:14,870
And there's something about drawing and
comics that they get a little bit of a

189
00:13:14,871 --> 00:13:19,260
bad rap for being unsophisticated or
being associated with children, um,

190
00:13:19,370 --> 00:13:22,880
because they tap into something that I
think is more primitive and it really is

191
00:13:22,881 --> 00:13:26,900
more primitive. Um, but that's not
necessarily necessarily a dig against it.

192
00:13:27,290 --> 00:13:31,490
When you draw, you are much more, um,

193
00:13:31,790 --> 00:13:36,320
directly expressing something about
what you feel or what you see. Um,

194
00:13:36,380 --> 00:13:40,010
and so Scott McCloud, the writer of
understanding comics, it's a great book.

195
00:13:40,070 --> 00:13:41,420
People should read if they're interested.

196
00:13:41,720 --> 00:13:43,910
He talks about the
iconic obstruction scale.

197
00:13:44,660 --> 00:13:47,710
And over here you have images,
um,

198
00:13:47,750 --> 00:13:50,120
the least abstract way of
representing the world.

199
00:13:50,120 --> 00:13:53,960
So like a photograph for example,
it's very close to what we actually see.

200
00:13:54,260 --> 00:13:58,790
And then in the middle here you have
symbols and simplistic drawings, emojis.

201
00:13:59,000 --> 00:14:02,970
Um, and then over here on the
other spectrum, we have words, uh,

202
00:14:02,990 --> 00:14:05,390
which are the most
abstracted from reality.

203
00:14:05,391 --> 00:14:07,550
And we take for granted
that words are abstract,

204
00:14:08,120 --> 00:14:10,160
abstract because we dealt
with them all the time,

205
00:14:10,400 --> 00:14:14,630
but it requires more levels of abstraction
in order to understand the symbols

206
00:14:14,631 --> 00:14:16,400
that we call words.
Um,

207
00:14:16,850 --> 00:14:21,120
and so I think that's why drawing
gets associated with what is child or

208
00:14:21,140 --> 00:14:24,050
primitive because it actually is
associated with something that takes less

209
00:14:24,051 --> 00:14:26,270
levels of abstracting to understand.

210
00:14:26,600 --> 00:14:29,540
But what's cool about comics is
that it mixes the two together.

211
00:14:29,541 --> 00:14:33,830
And so that mix is so sophisticated
if people use the form correctly.

212
00:14:33,831 --> 00:14:36,740
And so once I discovered that it
was like a revelation, it was,

213
00:14:36,770 --> 00:14:39,080
it was so magical and I was just hooked.

214
00:14:39,590 --> 00:14:44,590
Is there a comics particularly good
at describing visceral emotional

215
00:14:45,050 --> 00:14:47,450
experiences? I mean, what's
your relationship there?

216
00:14:47,840 --> 00:14:52,310
I said storytelling is the technology
and I think storytelling is mostly a

217
00:14:52,311 --> 00:14:54,470
technology. Well, it does a lot of things,

218
00:14:54,471 --> 00:14:59,000
but one thing it does really well is
create empathy and drawing because it's so

219
00:14:59,001 --> 00:15:03,980
connected to the what's
visceral, um, and what's,

220
00:15:03,981 --> 00:15:06,800
you know, um, like this is a good example.

221
00:15:07,790 --> 00:15:12,740
What can't be expressed or,
or what's felt? Um, I think
it's that much better at,

222
00:15:12,890 --> 00:15:17,260
at inducing empathy and readers. Something
I noticed when I'm drawing is it's,

223
00:15:17,350 --> 00:15:21,620
and it's very difficult for me to
ever draw a facial expression. Um,

224
00:15:21,650 --> 00:15:25,850
like for example, trying to represent
something like fear or anxiety.

225
00:15:26,450 --> 00:15:30,050
I can't draw it without having
to form it on my own face.

226
00:15:30,290 --> 00:15:33,500
And you can experiment with
that. It's very difficult. Um,

227
00:15:33,560 --> 00:15:36,260
and I can't draw,
you know,

228
00:15:36,261 --> 00:15:41,240
tension or exuberance or some abstraction
without bringing the quality into my

229
00:15:41,241 --> 00:15:44,720
arm. And so in this really
immediate, interesting way,

230
00:15:45,200 --> 00:15:49,670
drawing brings feelings that you're
trying to represent into your own body.

231
00:15:49,850 --> 00:15:51,350
And so my grandmother talked about,

232
00:15:51,530 --> 00:15:54,890
I wanted to tell this story because I
want it to feel what she went through.

233
00:15:54,891 --> 00:15:58,010
I didn't just want to know it or
understand it. I want it to feel it.

234
00:15:58,100 --> 00:16:02,240
And drawing helped me do that. It helped
me access that direct empathetic mode.

235
00:16:02,960 --> 00:16:06,260
This makes me want to
ask you some questions.

236
00:16:06,261 --> 00:16:08,900
We were talking about how,
um,

237
00:16:08,930 --> 00:16:13,250
comics have this kind of
modus of sophistication, um,

238
00:16:14,450 --> 00:16:15,980
comics.
In order to read a comic,

239
00:16:15,981 --> 00:16:20,090
you have to do something that's
common cloud calls closure.

240
00:16:20,300 --> 00:16:23,930
So you see one image and you see another
image and you have to knit the two

241
00:16:23,931 --> 00:16:26,330
together in order to create
a sequence in your mind.

242
00:16:26,331 --> 00:16:30,170
And a lot of that's happening in unlike
film where that happens for you with

243
00:16:30,171 --> 00:16:32,360
comics has happening,
you have to do that work yourself.

244
00:16:32,780 --> 00:16:37,780
And so I've thought about what that
work is like and because you have taught

245
00:16:38,091 --> 00:16:39,980
machines to read,
um,

246
00:16:40,010 --> 00:16:42,910
first with the reading machine and now
with natural language understanding here,

247
00:16:42,930 --> 00:16:43,763
Google,

248
00:16:44,090 --> 00:16:48,350
how hard would it be to teach
a computer to read comics?

249
00:16:48,920 --> 00:16:50,150
Is that a harder problem?

250
00:16:50,440 --> 00:16:53,680
I've got a few members of
my team here and uh, yes,

251
00:16:53,681 --> 00:16:58,030
I think that would be quite
challenging. Uh, Brian,

252
00:16:58,031 --> 00:17:00,760
this smirking I think,
uh,

253
00:17:01,510 --> 00:17:04,030
realizing how difficult
a problem that would be.

254
00:17:04,780 --> 00:17:08,200
So we were making good progress
and understanding language.

255
00:17:08,201 --> 00:17:13,201
We can start to do things
like summarization and we
have different tasks that

256
00:17:14,800 --> 00:17:18,340
require actually understanding
semantics of language.

257
00:17:19,990 --> 00:17:24,040
And Google has very good image
recognition that people are familiar with.

258
00:17:24,850 --> 00:17:28,120
Big accusation against the
field was five or six years ago,

259
00:17:28,121 --> 00:17:31,300
is that you guys can't even tell the
difference between a dog and a cat.

260
00:17:32,020 --> 00:17:34,010
Now we can do that.
Uh,

261
00:17:34,650 --> 00:17:39,650
and turns out the essence of a dog and
a cat is at level 15 of a deep neural

262
00:17:39,791 --> 00:17:43,960
net. And we couldn't go beyond three or
four levels because of a math problem.

263
00:17:44,380 --> 00:17:49,380
Just a five or six years ago has to do
with keeping convicts ever era surfaces.

264
00:17:50,490 --> 00:17:54,690
But anyway, it's a math problem. We
could create 10 20 level neural nets,

265
00:17:54,691 --> 00:17:58,080
but the information disintegrated
around the fifth level.

266
00:17:58,800 --> 00:18:03,800
So that problem was solved and now we
could go to 2000 level neural nets.

267
00:18:04,141 --> 00:18:06,210
And once we pass a level 15,

268
00:18:06,211 --> 00:18:08,310
they could tell the difference
between a dog and a cat.

269
00:18:08,880 --> 00:18:11,190
And now thousands of other categories.

270
00:18:11,670 --> 00:18:16,670
And the Google image recognition program
can tell cats and dogs and cows and you

271
00:18:18,141 --> 00:18:23,070
know, 20 different varieties of
birds, but also things like sunsets,

272
00:18:23,100 --> 00:18:24,930
autumn love,

273
00:18:25,260 --> 00:18:28,950
other kind of more a femoral,

274
00:18:29,520 --> 00:18:33,960
uh, characteristics. And it
really does a very good job.

275
00:18:34,890 --> 00:18:39,450
This is not a Google project,
but it shows the power of deep learning.

276
00:18:40,410 --> 00:18:41,430
In our Descartes group,

277
00:18:41,431 --> 00:18:43,980
we have a different approach to
deep learning based on my book,

278
00:18:43,981 --> 00:18:48,870
how to create a mind, which is basic
hierarchies of models of sequences.

279
00:18:49,740 --> 00:18:52,230
This was done with a deep neural net,

280
00:18:53,130 --> 00:18:56,880
but you can give it an image like this
is just a photograph. And then say,

281
00:18:56,881 --> 00:19:01,000
we'll repaint that same age in
the style of Van Gogh and this,

282
00:19:01,550 --> 00:19:03,780
uh,
or Picasso

283
00:19:05,850 --> 00:19:09,710
or munch, uh, who did the scream. He's,

284
00:19:09,890 --> 00:19:12,750
that's an actual image. And, uh,

285
00:19:12,860 --> 00:19:16,670
so it's redoing that picture
of Ralph buildings, uh,

286
00:19:16,770 --> 00:19:18,000
in these different styles.

287
00:19:18,840 --> 00:19:23,840
So computers are taking
steps in the arts and uh,

288
00:19:25,410 --> 00:19:29,790
they're not getting human levels.
But, uh, I've been saying 20, 29,

289
00:19:30,660 --> 00:19:35,280
uh, the AI field, there's
been periodic polls.

290
00:19:35,580 --> 00:19:39,120
We're getting closer, but not
because I'm changing my view. Uh,

291
00:19:39,510 --> 00:19:44,130
when I first said 2029 and my 1999
book age of spiritual machines,

292
00:19:44,820 --> 00:19:45,180
uh,

293
00:19:45,180 --> 00:19:50,180
Stanford held a conference to deal with
this startling prediction and it took a

294
00:19:51,781 --> 00:19:56,260
poll and the median was about 500 years.
Uh,

295
00:19:56,310 --> 00:19:57,360
recently.

296
00:19:57,750 --> 00:20:02,020
I was just actually at a
seminar or conference, uh,

297
00:20:02,730 --> 00:20:05,580
not named after this room,
but uh,

298
00:20:06,030 --> 00:20:09,960
really to establish AI ethics
based on the original list. Uh,

299
00:20:09,990 --> 00:20:14,300
biotech is telemark conference,
which is now 40 years old. Uh,

300
00:20:14,760 --> 00:20:18,840
and there was a poll presented.
The median was 50 years.

301
00:20:19,830 --> 00:20:21,960
It's actually not because,
uh,

302
00:20:22,350 --> 00:20:25,260
I mean part of my thesis is
the acceleration of progress.

303
00:20:25,330 --> 00:20:29,880
Proctors keeps getting faster
and information technology
grows exponentially and

304
00:20:29,881 --> 00:20:34,200
I don't think there's actually a lot more
appreciation of that than they used to

305
00:20:34,201 --> 00:20:38,250
be, but people, the actual
rate of progress now has,

306
00:20:38,251 --> 00:20:41,580
is in fact a lot faster
than it was in 1999.

307
00:20:42,270 --> 00:20:45,390
So peep people look at the current
rate of progress and saying, well,

308
00:20:45,391 --> 00:20:49,300
at the current rate of pockets, I think
it's going to take 50 years. And in 1999,

309
00:20:49,930 --> 00:20:51,640
they looked at the current
rate of progress as well.

310
00:20:51,641 --> 00:20:54,220
I think at this rate of progress
is going to take about 500 years.

311
00:20:54,520 --> 00:20:59,140
I agreed with both of those. I think at
the rate of progress in 19 nine, nine,

312
00:20:59,141 --> 00:21:01,810
it would take 5,500 years and
it's right in practice today.

313
00:21:01,810 --> 00:21:02,830
It would take 50 years,

314
00:21:03,190 --> 00:21:05,850
but the rate of progress is
going to continue to get faster.

315
00:21:05,851 --> 00:21:10,180
We're going to make 50 years a progress
at today's rate of progress in 13 years.

316
00:21:10,650 --> 00:21:11,950
Um,
anyway,

317
00:21:12,190 --> 00:21:17,190
people are getting more optimistic and
these are sort of turning complete tasks

318
00:21:17,891 --> 00:21:22,180
or acquire all of human intelligence
in any implications of that,

319
00:21:22,210 --> 00:21:26,680
both in terms of promise
versus peril employment, uh,

320
00:21:26,950 --> 00:21:28,570
consciousness of computers.

321
00:21:28,571 --> 00:21:33,490
Should we worry about causing pain and
suffering to our computer software? Uh,

322
00:21:33,550 --> 00:21:38,320
people are more concerned today about
the opposite and software causing us pain

323
00:21:38,321 --> 00:21:40,240
and suffering. But yeah. Um,

324
00:21:41,380 --> 00:21:46,030
well speaking about the future of art,
um,

325
00:21:46,090 --> 00:21:50,140
we can talk a little bit about our first
collaboration that we did together.

326
00:21:50,310 --> 00:21:50,910
Yeah.

327
00:21:50,910 --> 00:21:55,660
Way Back in 2001 when I was
14 and still had braces,

328
00:21:55,990 --> 00:21:58,750
I was roped into doing this a while.

329
00:21:58,751 --> 00:22:03,751
I was granted the opportunity to take
part in this historic Ted Conference,

330
00:22:04,300 --> 00:22:04,870
um,

331
00:22:04,870 --> 00:22:09,870
where we were presenting the promise of
virtual reality and virtual spaces being

332
00:22:10,301 --> 00:22:13,630
places where you can be
whoever you want to be,

333
00:22:13,930 --> 00:22:17,380
which reminds me of writing, um, and the,

334
00:22:17,470 --> 00:22:20,110
the way we get to inhabit
different characters and writing.

335
00:22:20,500 --> 00:22:22,540
And so we did this project.

336
00:22:22,800 --> 00:22:26,800
My father transformed into
Ramona who's a rage rock singer.

337
00:22:27,100 --> 00:22:30,880
And I unfortunately do not have
as much say in my transformation.

338
00:22:31,420 --> 00:22:34,650
I was transformed into three.
Um,

339
00:22:34,660 --> 00:22:37,330
I got to expand my identity across bodies,

340
00:22:37,630 --> 00:22:42,550
three male backup dancers who were
middle aged and no bit overweight,

341
00:22:42,551 --> 00:22:45,280
I have to say,
and not wearing shirts and

342
00:22:47,230 --> 00:22:51,070
modeled in the image of Richard Saul
Wurman who is the Ted Empresario at the

343
00:22:51,071 --> 00:22:51,880
time.

344
00:22:51,880 --> 00:22:56,880
And so I can't find any images of this
online and I wonder if he's responsible

345
00:22:57,011 --> 00:23:01,620
for taking them down. So,
yeah. Do you want to talk,

346
00:23:01,750 --> 00:23:02,583
talk about this?

347
00:23:02,950 --> 00:23:07,200
I mean, you're in Vr. This
was in 2001 at 10. Uh,

348
00:23:07,240 --> 00:23:11,590
Vr was very primitive. We had a lot
of problems with the technology,

349
00:23:11,591 --> 00:23:16,591
but both Amy and I were wearing magnetic
sensors and as removed these life size

350
00:23:17,411 --> 00:23:20,370
avatars on a big screen,
moved in sync with us.

351
00:23:20,371 --> 00:23:24,310
They were kind of digital puppets of ours,
quite had,

352
00:23:26,140 --> 00:23:29,980
uh, there were technical issues,
but it worked well enough.

353
00:23:29,981 --> 00:23:31,420
We pulled it off the performance.

354
00:23:31,421 --> 00:23:36,130
So the idea was you can be
someone else in virtual reality.

355
00:23:36,940 --> 00:23:39,580
You don't have to be the same
boring person all the time.

356
00:23:39,581 --> 00:23:42,340
Present company excluded.
Um,

357
00:23:43,270 --> 00:23:46,460
and so I tried to fix him and
that was different from myself.

358
00:23:46,461 --> 00:23:48,620
And same with Amy.

359
00:23:49,010 --> 00:23:52,980
Richard Saul Wurman was not known
for his hip hop cakes, but uh,

360
00:23:53,090 --> 00:23:57,260
her avatar was actually very realistic.
Um,

361
00:23:57,890 --> 00:24:02,090
but uh, you can actually go
to go to the web, uh, if you,

362
00:24:02,480 --> 00:24:06,620
if you're just put in the,
uh, in Google Ramona songs,

363
00:24:07,100 --> 00:24:11,220
Kurzweil, you can these here, uh,

364
00:24:11,300 --> 00:24:12,200
the songs I sang,

365
00:24:12,201 --> 00:24:17,110
I sang white rabbit at Gracie slick
song and then a song I wrote myself a,

366
00:24:17,190 --> 00:24:21,570
and you can hear it both with and
without the gender modification. Uh,

367
00:24:21,830 --> 00:24:23,960
it came,
came out pretty well.

368
00:24:24,470 --> 00:24:27,140
I think this is one of the promises
of virtual reality right now.

369
00:24:27,141 --> 00:24:31,820
The VR that's coming out, it's fairly
static. You basically are in a VR world.

370
00:24:31,821 --> 00:24:33,200
There's some technical issues with it.

371
00:24:33,201 --> 00:24:37,420
I think the real promise is that you
can be an actor in it and interact with

372
00:24:37,790 --> 00:24:38,601
other people,

373
00:24:38,601 --> 00:24:42,380
but you don't actually have to be the
same person and you can actually change

374
00:24:42,410 --> 00:24:46,700
other people to be who you want them
to be. Uh, so these are some of them.

375
00:24:47,330 --> 00:24:50,790
We tried to show this in 2001 so that
was our first collaboration, Huh?

376
00:24:50,890 --> 00:24:52,630
Yeah. I mean, I'm curious,

377
00:24:52,631 --> 00:24:55,720
I want to ask you about this idea
of identity in virtual reality.

378
00:24:55,721 --> 00:24:59,620
Conceivably we might spend more and more
time in these spaces as they get more

379
00:24:59,920 --> 00:25:04,390
sophisticated and this idea that you
can be anyone that you want to be.

380
00:25:04,720 --> 00:25:08,590
Um, that's interesting to me because
theoretically that's true in writing,

381
00:25:08,860 --> 00:25:12,400
but in the writing world, there is
actually sensitivity about, for example,

382
00:25:12,401 --> 00:25:16,090
taking on identities that you don't really
have insight into, sir. So, you know,

383
00:25:16,091 --> 00:25:18,310
a white person writing in
the voice of a black person.

384
00:25:18,311 --> 00:25:21,750
Sometimes there's some questions
about that. Um, or appropriate.

385
00:25:21,751 --> 00:25:23,620
And cultures in general,
people are,

386
00:25:24,190 --> 00:25:27,070
they have questions and sometimes
get offended about those kinds of

387
00:25:27,071 --> 00:25:29,650
transformations.
I'm often rightfully so.

388
00:25:29,651 --> 00:25:34,210
And then sometimes there are times
in which transforming our identities,

389
00:25:34,211 --> 00:25:34,451
you know,

390
00:25:34,451 --> 00:25:39,310
like gender transitions is celebrated
and encouraged and virtual reality seems

391
00:25:39,311 --> 00:25:42,670
like a really promising place for,
um, something like that. You know,

392
00:25:42,671 --> 00:25:46,360
somebody who wants to really embody a
different gender and they don't have to

393
00:25:46,420 --> 00:25:47,800
have complicated surgery.

394
00:25:47,801 --> 00:25:52,330
So do you think that virtual reality will
have the same identity politics as we

395
00:25:52,331 --> 00:25:53,260
see in real reality?

396
00:25:53,390 --> 00:25:53,601
Well,

397
00:25:53,601 --> 00:25:58,040
it is interesting that people really
begin to identify with their avatar and

398
00:25:58,041 --> 00:26:02,120
they seem to transfer their
consciousness even if it's a simple game.

399
00:26:02,340 --> 00:26:06,860
Um, it was actually interesting, uh,

400
00:26:06,980 --> 00:26:10,850
incident recently I've been writing how
virtual reality is inherently safer.

401
00:26:10,851 --> 00:26:14,570
I mean, ultimately we can be in virtual
environments that are just as real.

402
00:26:14,630 --> 00:26:18,320
It's real reality. But one advantage
of virtual reality is you can hang up.

403
00:26:18,321 --> 00:26:22,560
So telephone call is virtual
reality as far as soon, uh,

404
00:26:23,150 --> 00:26:27,110
talking, uh, as far as far as
the auditory sense is concerned.

405
00:26:27,111 --> 00:26:29,840
But you can hang up on a phone call
if you don't like where it's going.

406
00:26:30,620 --> 00:26:34,310
There's usually an incident where a woman,
uh,

407
00:26:34,520 --> 00:26:39,520
claimed quite sincerely that she had
been assaulted in virtual reality.

408
00:26:41,480 --> 00:26:44,820
And I had been writing
how you really from that,

409
00:26:45,350 --> 00:26:49,650
I still think virtual reality is safer
because if Chavis he didn't like this

410
00:26:49,651 --> 00:26:53,970
interaction and happening quickly enough
that she wasn't able to leave and she

411
00:26:53,971 --> 00:26:58,971
was densified enough with her avatar that
she didn't like what this other avatar

412
00:26:58,981 --> 00:27:02,190
did to her avatar.
But it wasn't my avatar.

413
00:27:02,280 --> 00:27:06,330
She felt it was her because people
do transfer their consciousness.

414
00:27:06,630 --> 00:27:09,150
She could then leave that experience.

415
00:27:09,720 --> 00:27:14,720
So at least she could stop the
continuation of a interaction.

416
00:27:14,881 --> 00:27:18,190
She didn't like what you can't
always do in real reality.

417
00:27:18,191 --> 00:27:19,380
So I do think it's safer,

418
00:27:19,381 --> 00:27:23,220
but I thought it was interesting cause
I really hadn't expected that you could

419
00:27:23,221 --> 00:27:26,130
be assaulted in virtual reality,

420
00:27:26,131 --> 00:27:29,160
but cause you could be in a phone call
to when someone could say something that

421
00:27:29,550 --> 00:27:32,640
you feel is assaultive. And, uh,

422
00:27:32,700 --> 00:27:37,700
so virtual reality is the term I think
is unfortunate because virtual reality

423
00:27:38,510 --> 00:27:42,390
makes it sound like it's not
real reality. So you know, if,

424
00:27:42,540 --> 00:27:45,750
if you and I make an agreement on the
phone and say, oh, well, but that was,

425
00:27:45,960 --> 00:27:49,540
you know, virtual reality, that
wasn't a real agreement. Um,

426
00:27:50,190 --> 00:27:53,610
it is real reality. Yeah.
It's just in a different form.

427
00:27:53,970 --> 00:27:58,240
So the same rules might
apply. Yeah. So I mean, I,

428
00:27:58,410 --> 00:28:01,350
there are a ways that it's safer,

429
00:28:01,351 --> 00:28:04,620
but there's also ways it's more
dangerous because people can be there

430
00:28:04,621 --> 00:28:06,480
anonymously.
You don't even know who they are.

431
00:28:06,481 --> 00:28:09,660
And some people feel like it can take
more liberties and they're doing real

432
00:28:09,680 --> 00:28:11,670
reality.
Uh,

433
00:28:12,000 --> 00:28:16,560
both parties are sort of less protected,
but that can be,

434
00:28:17,460 --> 00:28:22,380
uh, I mean we see all the time people
anonymously communicate things that they

435
00:28:22,381 --> 00:28:25,780
wouldn't otherwise do if we
knew who they were. Yeah.

436
00:28:25,910 --> 00:28:27,470
Yeah.
I mean,

437
00:28:27,471 --> 00:28:32,471
it's interesting thinking
about storytelling as
something that induces empathy,

438
00:28:32,481 --> 00:28:33,740
a technology of empathy.

439
00:28:33,741 --> 00:28:37,460
It's interesting for me to think about
virtual reality experiences becoming a

440
00:28:37,461 --> 00:28:38,450
mode of storytelling.

441
00:28:38,451 --> 00:28:43,310
Like you literally enter somebody else's
life and you actually see what it's

442
00:28:43,311 --> 00:28:46,730
like to, you're not necessarily
saying, now I am this person,

443
00:28:46,731 --> 00:28:51,230
but you get this really immersive sense
of somebody's life and you have that

444
00:28:51,231 --> 00:28:55,220
experience and it's, it's sounds like, um,

445
00:28:57,080 --> 00:28:58,610
there's a,
there's,

446
00:28:58,850 --> 00:29:03,850
there's potential to identify so strongly
with that experience that you actually

447
00:29:05,030 --> 00:29:10,030
forget to have empathy for it or you end
up being traumatized by the experience.

448
00:29:11,050 --> 00:29:16,040
It's, it's interesting to me that in, in
art and storytelling and even in film,

449
00:29:16,340 --> 00:29:18,920
you still have this removal
from the experience.

450
00:29:18,921 --> 00:29:22,640
And I think that that distance from the
experience is important because it gives

451
00:29:22,641 --> 00:29:27,110
you the space to reflect and to say,
oh, I'm watching this thing and, um,

452
00:29:27,320 --> 00:29:30,740
I have this distance from it in order to
reflect on what it is and what it might

453
00:29:30,741 --> 00:29:35,030
mean. We can move you enough to, to cry
or it happened. I mean, yeah, definitely.

454
00:29:35,370 --> 00:29:36,470
Um,
it's,

455
00:29:36,500 --> 00:29:39,620
it seems interesting to me that in
virtual reality where we're losing and

456
00:29:39,621 --> 00:29:41,920
closing that distance,
and is that,

457
00:29:41,921 --> 00:29:46,450
is there any danger there for the ability
to really feel empathy for the people

458
00:29:46,451 --> 00:29:49,120
that we become in virtual reality?

459
00:29:49,760 --> 00:29:53,170
Well, I mean, potentially
being more immersive, uh,

460
00:29:53,210 --> 00:29:54,850
we can be more effective.

461
00:29:54,980 --> 00:29:59,750
And I mentioned this anecdote about this
woman feeling assaulted and that that

462
00:29:59,751 --> 00:30:04,580
was kind of startling to me. Um, but,

463
00:30:05,300 --> 00:30:06,200
uh,

464
00:30:06,740 --> 00:30:11,740
I think there is danger and that I think
we will move to an era where we're at

465
00:30:12,170 --> 00:30:16,310
least in augmented reality all the time.
Uh,

466
00:30:16,730 --> 00:30:21,730
and that has the potential of increasing
empathy because we'll be able to be

467
00:30:23,871 --> 00:30:28,380
with people that we want to be with.
But then people may enter the, uh,

468
00:30:28,760 --> 00:30:32,300
the, uh, situation that we didn't invite.

469
00:30:32,880 --> 00:30:34,730
We certainly see that.
And social media.

470
00:30:35,240 --> 00:30:39,450
Yeah. Well they're still
be experiences, um,

471
00:30:39,650 --> 00:30:40,880
that are called,
well,

472
00:30:40,881 --> 00:30:44,750
there'd be virtual reality experiences
that we call art and others that we just

473
00:30:44,751 --> 00:30:48,140
call something else,
like, just experience.

474
00:30:48,380 --> 00:30:50,690
Because what we talked
a little bit about art,

475
00:30:50,780 --> 00:30:54,020
the roots of the word art being
connected to the word artificial.

476
00:30:54,770 --> 00:30:58,360
And that route, meaning, um,

477
00:30:58,730 --> 00:31:03,730
trickery and crafts craftsmanship and
that an inherent in the concept of art is

478
00:31:04,401 --> 00:31:09,080
that we are, that it's separate
from us and that it's not real.

479
00:31:09,440 --> 00:31:12,980
And so I wonder, these VR experiences
that we call art, what will,

480
00:31:13,010 --> 00:31:14,870
what will make them art
if there's so immersive?

481
00:31:15,580 --> 00:31:20,200
Well, I think it's the same thing as, as
with, uh, anything else. I mean, you, uh,

482
00:31:20,230 --> 00:31:23,140
Gordon Bell invented
life pitch in the 80s,

483
00:31:23,680 --> 00:31:26,890
so you could record your whole life and
then you could do something like the

484
00:31:26,891 --> 00:31:31,891
movie John Being John Malcovich where
you could actually inhabit someone else's

485
00:31:32,471 --> 00:31:37,360
body. And that's actually just about
feasible today. Yeah. Um, let's see.

486
00:31:37,361 --> 00:31:40,270
Out of their eyes here,
out of their ears that,

487
00:31:40,271 --> 00:31:43,450
that's fairly straightforward
feel out of their skin. I mean,

488
00:31:43,451 --> 00:31:47,380
there's ways of beginning to do that.
Um,

489
00:31:47,890 --> 00:31:50,740
but that's not necessarily
art, uh, artists,

490
00:31:50,741 --> 00:31:53,920
when you select certain subset of,

491
00:31:54,020 --> 00:31:57,910
of our reality to give
you some insight into,

492
00:31:57,911 --> 00:32:01,000
into deeper truths about,
about life.

493
00:32:01,450 --> 00:32:04,420
Yeah.
It seems to me that in virtual reality,

494
00:32:04,421 --> 00:32:07,510
a whole concept of identity
will be totally different.

495
00:32:07,540 --> 00:32:12,100
And the things that are connected to our
physical bodies are going to be less.

496
00:32:12,530 --> 00:32:15,640
It was very interesting
when I became Ramona,

497
00:32:16,270 --> 00:32:20,680
how many just kind of interesting. But
I felt it was actually quite liberating.

498
00:32:20,681 --> 00:32:24,550
The glow. I really felt like I
transferred my consciousness.

499
00:32:24,551 --> 00:32:29,551
I was looking in a mirror and I was
somebody else and I felt more relative to,

500
00:32:30,071 --> 00:32:33,580
okay, I happen to have this
particular physical body,

501
00:32:34,390 --> 00:32:38,950
but that's not the only way to be.
I really could be someone else.

502
00:32:39,080 --> 00:32:43,730
People very much identify,
I'm this skinny teenage girl,

503
00:32:43,731 --> 00:32:47,510
I'm the so will wait, middle aged
man. But you can be someone else.

504
00:32:48,070 --> 00:32:50,960
And it'll be actually interesting
when you can have, you know,

505
00:32:50,961 --> 00:32:54,980
full immersive virtual experiences
with other people being someone else.

506
00:32:54,981 --> 00:32:56,010
And uh,

507
00:32:56,420 --> 00:32:59,540
I think that I could potentially increase
empathy because you can realize what

508
00:32:59,541 --> 00:33:00,770
it's like to be someone else.

509
00:33:02,130 --> 00:33:06,840
Yeah. Especially if you can experience
their stories and their histories.

510
00:33:07,050 --> 00:33:10,400
Because so much of identity isn't just
the way we look, but the history is like,

511
00:33:10,410 --> 00:33:11,730
as my book was exploring,

512
00:33:12,000 --> 00:33:15,420
the histories that we inherit are the
kinds of experiences we have because of

513
00:33:15,421 --> 00:33:18,720
the way we look and we can really
simulate those experiences.

514
00:33:18,721 --> 00:33:20,330
Then I think that relapse,
that's where are,

515
00:33:21,050 --> 00:33:23,530
cause you've got to be selective
and you tell that story.

516
00:33:23,531 --> 00:33:25,690
I mean you can't tell the
20th story in 20 years.

517
00:33:25,691 --> 00:33:29,890
So you pick the incidents that are,
they really make a point.

518
00:33:31,110 --> 00:33:34,410
Well, speaking of transferring
our consciousness,

519
00:33:34,890 --> 00:33:39,890
maybe you can also tell us a little bit
about your first foray into one kind of

520
00:33:40,591 --> 00:33:43,500
virtual world,
which is the world of fiction.

521
00:33:43,950 --> 00:33:46,830
Yeah. So in this Vr, some thing at Ted,

522
00:33:47,260 --> 00:33:50,430
Amy taught me how to actually move like a

523
00:33:52,400 --> 00:33:57,320
female,
which did not come naturally to me.

524
00:33:58,100 --> 00:34:01,370
Um,
but uh,

525
00:34:02,270 --> 00:34:05,510
I've been thinking a lot about,
uh,

526
00:34:06,440 --> 00:34:10,160
intelligence since I've written
books about nonfiction books,

527
00:34:10,161 --> 00:34:13,700
about intelligence. And I
think once computers, region,

528
00:34:13,760 --> 00:34:17,660
human intelligence say you automatically
will be super human intelligent.

529
00:34:18,380 --> 00:34:20,060
Um,
you know,

530
00:34:20,061 --> 00:34:23,480
we've been using computers as it is
to make up for weaknesses and our

531
00:34:23,481 --> 00:34:27,980
intelligence, we can't remember more
than the handful of phone numbers. Uh,

532
00:34:28,370 --> 00:34:30,710
most people here could
recite the alphabet.

533
00:34:30,711 --> 00:34:32,840
But if I ask you to recite it backwards,
you can't do that.

534
00:34:32,841 --> 00:34:34,730
That's a trivial thing
for computers to do.

535
00:34:35,190 --> 00:34:37,880
It's a computer's already do a
lot of things that we can't do.

536
00:34:37,881 --> 00:34:41,360
They can remember a billion or
trillion ton numbers easily.

537
00:34:42,230 --> 00:34:46,030
Um, but I thought really, uh,

538
00:34:46,460 --> 00:34:48,860
illustrate this a,

539
00:34:48,861 --> 00:34:53,060
I started to imagine what would
it be like if one person and, uh,

540
00:34:53,210 --> 00:34:57,020
a young girl happened to
be half superintelligence.

541
00:34:57,590 --> 00:35:00,860
And I don't actually explain that.
Uh,

542
00:35:00,890 --> 00:35:05,630
have a preface to the novel were saying,
well, this does someone like Danielle,

543
00:35:05,631 --> 00:35:10,310
who's, it's very super intelligent girl,
exists in the world today. And I said,

544
00:35:10,311 --> 00:35:14,510
well, from one perspective, anybody can
be a Danielle. And we have young people,

545
00:35:14,511 --> 00:35:18,530
certainly college students who do things
like start Google or apple or Microsoft.

546
00:35:19,130 --> 00:35:22,400
And we have high school kids now
that are doing amazing things.

547
00:35:22,401 --> 00:35:26,750
And I think anybody who has the courage
to pursue their ideas and to overcome

548
00:35:27,260 --> 00:35:29,680
sort of artificially,
uh,

549
00:35:29,960 --> 00:35:34,730
imposed limitations and our thinking in
our confidence in what we can do to the

550
00:35:34,731 --> 00:35:39,630
world can be a Danielle and make
dramatic changes in the world.

551
00:35:40,290 --> 00:35:42,530
Uh, then I said, well, does
anyone actually exists?

552
00:35:42,531 --> 00:35:47,531
It has the broad range of Danielle's
superpowers has superpowers not melting

553
00:35:48,900 --> 00:35:53,160
steel with our eyes. It's melting problems
with her intellect and her artistry.

554
00:35:53,730 --> 00:35:57,990
And I said, well, perhaps not yet.
So this book is a thought experiment.

555
00:35:57,991 --> 00:36:02,991
What if someone with Danielle's confidence
and courage and an intellect existed,

556
00:36:06,000 --> 00:36:09,210
what would that mean for the
world? Uh, and then I say,

557
00:36:09,211 --> 00:36:13,440
now imagine if we were all Danielle's
and that will happen by the year 20,

558
00:36:13,441 --> 00:36:17,220
45. So my publisher said, well,

559
00:36:17,221 --> 00:36:20,640
we need a backstory cause you
know, superman has a backstory. And

560
00:36:22,320 --> 00:36:22,861
I said,
well,

561
00:36:22,861 --> 00:36:26,820
I'd rather give my backstory as to why I
wrote this novel cause I didn't want to

562
00:36:26,821 --> 00:36:30,630
say, well she's a girl and she's fused
with AI and our brain or something.

563
00:36:30,680 --> 00:36:33,120
It would seem to pat.
But anyway,

564
00:36:33,121 --> 00:36:37,650
she's a girl who follows
her from zero to 22.

565
00:36:38,280 --> 00:36:41,100
Um,
and she does remarkable things.

566
00:36:41,101 --> 00:36:44,460
She goes to Zambia at age six and
solves part of the water problem.

567
00:36:44,940 --> 00:36:46,940
She becomes a worldwide
country music stars.

568
00:36:47,000 --> 00:36:50,940
Page h he overthrows Kadafi
was software viruses at age 10.

569
00:36:51,480 --> 00:36:53,550
She cures cancer at age 12.

570
00:36:54,120 --> 00:36:57,450
She brings democracy to
China at age 14 and becomes,

571
00:36:57,451 --> 00:37:00,960
it's first democratically
elected president. Um,

572
00:37:01,710 --> 00:37:05,430
she becomes present United States at
age 19 by defeating Hillary Clinton.

573
00:37:06,260 --> 00:37:10,920
Um, that'd be a better choice. So,

574
00:37:10,980 --> 00:37:11,813
uh,

575
00:37:12,030 --> 00:37:17,030
W W I made a VR trailer we can't show
you via yet for an audience of the site.

576
00:37:17,530 --> 00:37:19,420
We'll show it to you in two D.
Right?

577
00:38:00,810 --> 00:38:04,080
How long do I have? Precisely. 11 minutes.

578
00:38:05,180 --> 00:38:09,440
Really starting to sink in that I've
actually been elected chairman of the

579
00:38:09,441 --> 00:38:13,970
Communist Party of China. Some
people seem surprised because, well,

580
00:38:14,130 --> 00:38:18,560
I'm a 15 year old girl from Los Angeles.
I suppose part of me is surprised too,

581
00:38:18,980 --> 00:38:20,600
but the other part has been expecting it.

582
00:38:21,830 --> 00:38:23,660
It's been a complicated path to get here.

583
00:38:24,440 --> 00:38:26,840
I started out innocuously
as a child that didn't talk.

584
00:38:27,380 --> 00:38:30,170
I didn't really see the point of it
at least until I had something to say.

585
00:38:31,250 --> 00:38:35,060
I loved books I taught myself to read,
but I didn't want people to know.

586
00:38:35,650 --> 00:38:39,220
I purposefully held my book upside down
to throw my sister Claire off track.

587
00:38:39,810 --> 00:38:40,780
Clear by the way,

588
00:38:40,840 --> 00:38:44,170
was an orphan from the Haitian earthquake
that mom and dad adopted two years

589
00:38:44,171 --> 00:38:45,010
before I was born.

590
00:38:46,710 --> 00:38:47,150
Okay.

591
00:38:47,150 --> 00:38:50,860
Well, I finally joined the conversation
and said my first words when I was two.

592
00:38:51,510 --> 00:38:55,670
My mom was shocked at my insights into
the word on him on a pia and dropped the

593
00:38:55,671 --> 00:38:59,630
blueberry dessert all over. Claire
and me. The Dad took it in stride.

594
00:39:02,370 --> 00:39:04,790
It wasn't very good at making friends.
Well,

595
00:39:05,010 --> 00:39:07,650
at least the other kids had a good time.
At my fourth birthday party,

596
00:39:08,460 --> 00:39:11,490
I finally made my first friend
when I was six in Zambia,

597
00:39:11,820 --> 00:39:15,300
halfway around the world. I went there
with Claire to help with the drought.

598
00:39:15,780 --> 00:39:19,200
We ran into a lot of predicaments
like feuding warlords.

599
00:39:19,920 --> 00:39:23,160
By my calculation, we solved one
third of 1% of the water problem.

600
00:39:24,870 --> 00:39:28,230
The world discovered me at eight when
I was debuting at the country music

601
00:39:28,231 --> 00:39:32,370
festival,
but I learned that fame has a steep price.

602
00:39:33,270 --> 00:39:35,410
The peace conference that
clear organized in Libya.

603
00:39:35,420 --> 00:39:38,190
It turned out to be kind of a disaster.

604
00:39:38,880 --> 00:39:42,060
I ended up protecting the rebels and my
sister with software viruses from our

605
00:39:42,061 --> 00:39:43,350
outpost in the Libyan desert.

606
00:39:44,310 --> 00:39:48,210
Negotiating a Middle East peace agreement
was more complicated than I expected,

607
00:39:48,960 --> 00:39:50,970
but I had help from a
wise rabbi in Brooklyn.

608
00:39:52,410 --> 00:39:57,410
Tragedy struck when I was 14 I discovered
that there is an unrelenting banality

609
00:39:57,810 --> 00:40:02,580
to debt. Well, I better gather my
thoughts and my acceptance speech.

610
00:40:03,180 --> 00:40:06,630
I'm the first teenage girl to be elected
chairman of the Communist Party of

611
00:40:06,631 --> 00:40:09,150
China and I'm not even Chinese.

612
00:40:09,450 --> 00:40:12,720
I have to say I'm not really
comfortable with that title of chairman.

613
00:40:13,290 --> 00:40:15,750
How about chair woman? No,
that doesn't make sense.

614
00:40:15,751 --> 00:40:20,751
Either 15 someone suggested id just
called chair and I'm not a piece of

615
00:40:21,331 --> 00:40:23,670
furniture.
I've got a chair girl

616
00:40:27,810 --> 00:40:32,810
[inaudible].

617
00:40:40,500 --> 00:40:43,740
Amy was my writing coach for fiction,

618
00:40:43,770 --> 00:40:46,950
which is actually quite
different than nonfiction.

619
00:40:46,980 --> 00:40:51,900
You need is a character arcs and you need
a story arc. Every story is the same.

620
00:40:51,901 --> 00:40:53,900
It has a,
what's called a Christian arc.

621
00:40:54,940 --> 00:40:59,820
The character runs into problems and then
more problems and then all is lost and

622
00:40:59,821 --> 00:41:02,820
then the bottom falls out
and it's completely helpless.

623
00:41:03,210 --> 00:41:05,190
And then there's the
resurrection at the end.

624
00:41:05,250 --> 00:41:09,600
Let me look at stories you'll see
that they all follow that aren't, um,

625
00:41:09,780 --> 00:41:13,600
anyway that Amy did the illustrations.
Um,

626
00:41:13,710 --> 00:41:16,920
and there was an interplay between
writing the story and the illustration.

627
00:41:16,921 --> 00:41:18,840
Could she write an illustration that say,
oh,

628
00:41:19,210 --> 00:41:23,130
that's what Danielle was thinking and
what are you seeing this character in your

629
00:41:23,131 --> 00:41:27,930
mind does he wrote? Yes, actually it had
fantasies about Danielle and for years.

630
00:41:27,931 --> 00:41:29,340
And finally I'm one vacation,

631
00:41:29,341 --> 00:41:33,710
but six years ago decided to write them
down and had draft one of the novel.

632
00:41:33,740 --> 00:41:37,550
But I had the long way to go to
understand the craft of storytelling,

633
00:41:37,551 --> 00:41:42,130
which as you pointed out with the
first, our first technology. Um,

634
00:41:43,250 --> 00:41:46,670
but then your illustrations actually
helped me to understand the stories that

635
00:41:46,671 --> 00:41:51,450
there was an interaction there. What'd
you say a little bit about your next,

636
00:41:51,600 --> 00:41:53,640
yeah. Novel. Well, the next part of that,

637
00:41:53,641 --> 00:41:56,220
I'm working on the beginning
of our collaborations.

638
00:41:57,000 --> 00:42:01,410
This is Fred Kurzwell,
a father of my father.

639
00:42:01,740 --> 00:42:05,700
And some of you might know if you saw
the movie transcendent man that my father

640
00:42:05,701 --> 00:42:10,320
has this collection of documents and
artifacts that he saved from his father.

641
00:42:10,520 --> 00:42:13,320
And I'm in a storage unit.
I've actually been to this storage unit.

642
00:42:13,321 --> 00:42:18,180
At first it was like definitely
haunted. Um, it was in this,

643
00:42:18,181 --> 00:42:22,140
you know, dark, all these dark
alleys and nobody else was there.

644
00:42:22,141 --> 00:42:26,310
And like floors and floors of different
storage units and the lights would turn

645
00:42:26,311 --> 00:42:29,010
off if you weren't moving.
And it was the very horrifying,

646
00:42:29,540 --> 00:42:32,910
I had an interesting
experience there. Um, and,

647
00:42:33,000 --> 00:42:36,480
and now you've moved those documents
to a less haunted building,

648
00:42:36,481 --> 00:42:39,700
which makes her not as
good as story. But, uh,

649
00:42:40,560 --> 00:42:44,310
so it's interesting to me why you've
been saving all these documents.

650
00:42:44,340 --> 00:42:45,270
Theoretically,

651
00:42:45,480 --> 00:42:50,070
we will come to have such sophisticated
AI that we could take these documents

652
00:42:50,071 --> 00:42:55,020
along with memories and recreate an Avatar
of this person who died quite young.

653
00:42:55,380 --> 00:42:58,260
Um, and I'm really
interested in, you know,

654
00:42:58,470 --> 00:43:02,250
what that might look like
and why we might do that. Um,

655
00:43:02,460 --> 00:43:03,990
and for me store,
you know,

656
00:43:03,991 --> 00:43:07,660
stories especially in
flying couch had such a,

657
00:43:07,930 --> 00:43:12,600
a meaning to my present life, the past
and its relationship to the present, uh,

658
00:43:12,680 --> 00:43:14,130
has given my life a lot of meaning.

659
00:43:14,131 --> 00:43:16,470
And it's things like something
I've had a lot of questions about.

660
00:43:16,680 --> 00:43:17,490
And so with this,

661
00:43:17,490 --> 00:43:21,450
if you can actually take the past and
make it literally a part of the present by

662
00:43:21,451 --> 00:43:25,710
bringing somebody back, um,
what does that mean? What,

663
00:43:25,720 --> 00:43:28,800
what does that going to do for our
sense of self and our relationships?

664
00:43:28,801 --> 00:43:32,220
And so I'm curious to explore that in my,
in my next,

665
00:43:32,300 --> 00:43:35,070
well, I mean that was the theme
of the movie transcendent man.

666
00:43:35,880 --> 00:43:37,860
We can create a chat bot today.

667
00:43:38,010 --> 00:43:41,790
We have the technology to take a corpus
of writings and create a chat Bot that

668
00:43:41,791 --> 00:43:43,700
represents that person likes you planted.

669
00:43:43,860 --> 00:43:47,270
Do that with my father's writings
as we did that with Danielle.

670
00:43:47,280 --> 00:43:51,990
Actually you can talk to Danielle,
but it's not that human levels yet.

671
00:43:52,020 --> 00:43:54,810
I think ultimately wouldn't be x.
You can pass a chain test.

672
00:43:54,811 --> 00:43:59,430
One of the implications is we could
create a someone that is very much human

673
00:43:59,431 --> 00:44:02,760
like and that reflects all the
information we have about them,

674
00:44:02,820 --> 00:44:05,270
even their DNA and uh,

675
00:44:06,060 --> 00:44:09,030
recollections of other people
and all of their artifacts.

676
00:44:09,570 --> 00:44:09,791
I mean,

677
00:44:09,791 --> 00:44:14,050
what's interesting is as a writer or
something you're trying to do is to do as

678
00:44:14,051 --> 00:44:18,880
much research as you can and then trust
your intuition about how to write about

679
00:44:18,881 --> 00:44:22,060
real people who lived. Um, so
with my grandmother's story,

680
00:44:22,240 --> 00:44:25,810
I had to trust my intuition and I had a
certain amount of information and I was

681
00:44:25,811 --> 00:44:29,440
able to bring her to the
page and I'm wondering,

682
00:44:29,740 --> 00:44:33,450
will an AI one do that process
better than I can, you know,

683
00:44:33,451 --> 00:44:37,080
will they have access to more information
and potentially have more emotional

684
00:44:37,081 --> 00:44:42,000
nuance? Like, am I going to
be out of a job? Probably.

685
00:44:42,460 --> 00:44:44,680
My view is it's not us versus them.

686
00:44:44,681 --> 00:44:49,450
We're going to use AI to make
ourselves smarter. We do that now.

687
00:44:49,451 --> 00:44:53,890
We carry these ais on our belts and who
here could do their work or anyone in

688
00:44:53,891 --> 00:44:57,460
the world can they do their work or
their education without these brain

689
00:44:57,461 --> 00:45:00,070
extenders already have. And then we're
going to get more intimate with them.

690
00:45:00,071 --> 00:45:01,480
We're going to put them in our brains.

691
00:45:01,481 --> 00:45:03,010
We're going to connect
our brains to the cloud.

692
00:45:03,640 --> 00:45:06,610
So we'll be actually better at
doing these kinds of things.

693
00:45:07,210 --> 00:45:09,550
So if we could just take a
couple of minutes in the shares,

694
00:45:09,551 --> 00:45:12,910
some of your cartoons,
a similar one,

695
00:45:13,330 --> 00:45:14,800
the first one you had in the New Yorker.

696
00:45:15,520 --> 00:45:18,280
My first New Yorker cartoon
was Google inspired.

697
00:45:20,380 --> 00:45:21,520
This is the first one.

698
00:45:26,060 --> 00:45:29,130
Do you guys have any studious
Google cars around who are,

699
00:45:32,030 --> 00:45:35,510
um,
am I second cartoon was

700
00:45:37,310 --> 00:45:38,380
Rhode Island.

701
00:45:39,500 --> 00:45:41,920
I don't know. So probably
close to doing that, but uh,

702
00:45:42,660 --> 00:45:44,220
we both have robot cats.

703
00:45:44,430 --> 00:45:48,270
I got mine for Hanukkah
last year and it's very,

704
00:45:48,330 --> 00:45:50,210
robots are very funny to me.
I've,

705
00:45:50,220 --> 00:45:54,390
I've draw a lot of inspiration
from artificial intelligence, um,

706
00:45:54,960 --> 00:45:59,100
for my cartoons because there's just
something about robots in their current

707
00:45:59,101 --> 00:46:03,210
form that is like, you're, you're
like, you feel for it. You know,

708
00:46:03,211 --> 00:46:06,420
you want it to be more, you know, because
you're like, just try a little harder.

709
00:46:06,421 --> 00:46:11,160
You can be human or you can really be a
cat, you know. Um, and this robot car,

710
00:46:11,161 --> 00:46:15,450
we have it, it's pretty crude and, but
I was thinking, you know, to be a cat,

711
00:46:15,451 --> 00:46:19,520
it's not that hard actually. You just
have to just have to sit there. Um,

712
00:46:20,240 --> 00:46:21,360
so yeah,
I draw,

713
00:46:21,390 --> 00:46:26,390
I draw a lot of inspiration from are
my dad's worked for sources of humor as

714
00:46:27,061 --> 00:46:29,130
well as more profound thanks.
But you know,

715
00:46:29,131 --> 00:46:32,610
humor is humor and humor is
connected to what's profound.

716
00:46:32,611 --> 00:46:35,880
I think we find things funny because
they really resonate with something true

717
00:46:35,881 --> 00:46:38,700
about our condition. Um,
so I thought to wrap up,

718
00:46:38,701 --> 00:46:43,701
I'd show you guys some of my unpublished
cartoons and you can help me figure out

719
00:46:43,770 --> 00:46:46,950
which ones are the best.
So I can, you know,

720
00:46:46,951 --> 00:46:48,720
go to Bob Makeup at the
New Yorker and be like,

721
00:46:48,750 --> 00:46:53,550
everyone at Google said that this one.
And I know that there's a partnership,

722
00:46:53,910 --> 00:46:57,240
something, you know, a partnership
between the cartoon editor in Google.

723
00:46:57,241 --> 00:47:01,350
So maybe he'll, maybe you guys can help me
out. All right, so here's the first one.

724
00:47:06,130 --> 00:47:09,740
Good.
All right.

725
00:47:14,440 --> 00:47:19,240
The future of art.
Cool.

726
00:47:27,140 --> 00:47:28,220
How went,
took a second.

727
00:47:38,050 --> 00:47:38,883
Okay.

728
00:47:39,470 --> 00:47:40,760
That's how I would like to play golf.

729
00:48:02,650 --> 00:48:04,280
You never know who you're offending me.

730
00:48:24,830 --> 00:48:26,000
I think that's a good place to end.

731
00:48:29,500 --> 00:48:30,333
Okay.

732
00:48:30,740 --> 00:48:33,300
So cool questions,
comments.

733
00:48:34,580 --> 00:48:34,920
Hi.

734
00:48:34,920 --> 00:48:39,870
So it's interesting that you brought up
the one panel two panel inferring what

735
00:48:39,871 --> 00:48:44,871
happened in between the panels in a
comic for the things that people write or

736
00:48:44,941 --> 00:48:49,941
something like the panels in the story
of their life and the getting an AI to

737
00:48:52,141 --> 00:48:57,141
speak in the voice of someone else using
these panels involves inferring what's

738
00:48:58,410 --> 00:48:59,790
in between those two panels.

739
00:48:59,820 --> 00:49:04,560
There was actually a black mirror episode
about doing that and it hitting the

740
00:49:04,561 --> 00:49:05,520
uncanny valley.

741
00:49:06,630 --> 00:49:11,630
How do you think human storytelling as
a technology could be transferred into

742
00:49:12,961 --> 00:49:14,430
doing this for robots?

743
00:49:15,140 --> 00:49:16,160
It's interesting what you say.

744
00:49:16,161 --> 00:49:20,540
I can only speak from a
writer's perspective that you
do so much writing before.

745
00:49:20,541 --> 00:49:24,650
What you put on the page is if
you know, inked or is printed, um,

746
00:49:24,710 --> 00:49:28,460
you really have to have a full sense of
a character before you can write them

747
00:49:28,461 --> 00:49:29,121
convincingly.

748
00:49:29,121 --> 00:49:33,160
And so much of that is unsaid and that's
sort of what minimalism is about and

749
00:49:33,200 --> 00:49:34,670
it's what any good writing is about.

750
00:49:34,880 --> 00:49:38,540
And I think that's why people feel
that in order to ride really well,

751
00:49:38,600 --> 00:49:40,760
a certain character,
you need to know so much about them,

752
00:49:40,761 --> 00:49:45,620
which is why bridging racial or cultural
divides, it's so hard for writing. Um,

753
00:49:46,010 --> 00:49:48,020
and so yeah, that sounds
really hard. I mean,

754
00:49:48,021 --> 00:49:50,330
in order to really write convincingly,

755
00:49:50,331 --> 00:49:54,420
especially something like
fiction or nonfiction, something
more literary, you're,

756
00:49:54,440 --> 00:49:56,150
there's so much that you have to know.

757
00:49:56,720 --> 00:50:00,160
Dan. Kenny valley is a key issue.
I'm glad you brought that up.

758
00:50:00,161 --> 00:50:03,340
Particularly as we get to
more and more immersive taps,

759
00:50:03,390 --> 00:50:07,330
reality that if we create a
reality that's very different.

760
00:50:07,750 --> 00:50:11,900
So one of the first animated movies, it
was Shrek. It's not really quite humans.

761
00:50:11,901 --> 00:50:14,860
I didn't really attack the uncanny valley.

762
00:50:15,420 --> 00:50:20,020
We get to more realistic
portrayals of animals and humans.

763
00:50:20,021 --> 00:50:23,470
So we have that challenge.
Jungle Book.

764
00:50:23,471 --> 00:50:26,200
I though was good at actually
leaped over the uncanny valley,

765
00:50:26,201 --> 00:50:28,550
at least for animals.
It looked like a movie of animals,

766
00:50:28,551 --> 00:50:33,140
but they were talking and the acting
and so it was clearly animation,

767
00:50:33,920 --> 00:50:38,290
uh, can't quite do that
for humans yet. Uh,

768
00:50:38,300 --> 00:50:42,620
in rogue one, uh, this one
character they brought back,

769
00:50:42,890 --> 00:50:45,080
uh,
through animation.

770
00:50:46,550 --> 00:50:49,640
I thought it was still in the uncanny
valley. Some people thought it was cool,

771
00:50:49,670 --> 00:50:50,860
but I don't know.

772
00:50:50,950 --> 00:50:53,590
There's a kind of uncanny valley,
not just for visual.

773
00:50:53,680 --> 00:50:57,100
I think that's what you're getting at
is like there's some kind of stilted,

774
00:50:57,101 --> 00:51:00,340
emotional thing going on
with, you know, when you,

775
00:51:00,400 --> 00:51:04,810
if you talk to an AI and it claims to be
having this life, but you can just tell,

776
00:51:04,811 --> 00:51:08,050
it's like you don't really have that life,
you know, that's a sort of narrative,

777
00:51:08,051 --> 00:51:12,140
uncanny valley. And um, it
just sounds like levels.

778
00:51:12,570 --> 00:51:13,403
It was interesting.

779
00:51:13,830 --> 00:51:18,660
Dennis Hassabis at this beneficial
AI conference presented the Alphago.

780
00:51:19,230 --> 00:51:24,230
He spent the whole presentation on this
one move in this key game that that

781
00:51:24,421 --> 00:51:28,650
Alpha go played. Uh,
and it was a move off.

782
00:51:29,310 --> 00:51:32,700
It was early in the game and it
played a move near the corner.

783
00:51:33,000 --> 00:51:38,000
And any amateur knows you don't do that
because you need to control territory

784
00:51:38,251 --> 00:51:41,730
and having a move that's right,
right near the corner.

785
00:51:42,120 --> 00:51:45,590
Early in the game when you could really
try to begin to control some territory

786
00:51:45,600 --> 00:51:49,290
doesn't make sense.
And everybody groaned at this.

787
00:51:49,291 --> 00:51:50,280
And then at the end of the game,

788
00:51:50,281 --> 00:51:54,060
it turned out that that was the
most brilliant move. It was key.

789
00:51:54,061 --> 00:51:57,870
The AlphaGo wouldn't have one
without that amazing move.

790
00:51:59,180 --> 00:52:03,000
Now I've been talking about how I think
when computers pass the Turing test,

791
00:52:03,030 --> 00:52:04,230
they will be conscious.

792
00:52:04,231 --> 00:52:08,250
And I talk about consciousness and my
books and it's not a scientific concept.

793
00:52:08,250 --> 00:52:12,420
Do you need a philosophical leap of faith
as to who and what is conscious? But,

794
00:52:13,020 --> 00:52:17,520
uh, so some scientists like
my mentor, Marvin Minsky, at
least at times, would say,

795
00:52:17,910 --> 00:52:18,743
uh,
don't,

796
00:52:18,960 --> 00:52:23,520
don't worry about consciousness is just
an illusion because it's not scientific

797
00:52:23,550 --> 00:52:25,390
and there's no way to
have a falsifiable looks,

798
00:52:25,391 --> 00:52:27,570
prime minister who and what is conscious,

799
00:52:27,571 --> 00:52:30,960
my view is to can't abandon consciousness
because it's actually the fundamental

800
00:52:30,961 --> 00:52:31,860
value in the world.

801
00:52:31,861 --> 00:52:36,030
And our morality is based on
consciousness and unconscious entities,

802
00:52:36,031 --> 00:52:39,510
only important insofar as they affect
the conscious experience of conscious

803
00:52:39,511 --> 00:52:39,851
entities.

804
00:52:39,851 --> 00:52:43,740
So you really need to make that leap
of faith in my leap of faith as if it

805
00:52:44,130 --> 00:52:48,270
really is convincingly acts conscious.
Uh,

806
00:52:48,450 --> 00:52:52,260
and you really are convinced it is having
the subjective experience of claims to

807
00:52:52,261 --> 00:52:56,820
be, which is not the case yet. We'll
see how that they are conscious.

808
00:52:56,820 --> 00:52:58,200
That's my leap of faith.

809
00:52:58,230 --> 00:53:02,820
And so AI is when they really pass a valid
Turing test with emphasis on the word

810
00:53:02,821 --> 00:53:06,930
valid. And that's a whole nother
discussion. Uh, will be conscious.

811
00:53:07,920 --> 00:53:11,460
But my feeling was, I mean,
Alphago made this brilliant move.

812
00:53:12,060 --> 00:53:16,200
I didn't feel like when they
made that move with a Ha,

813
00:53:16,230 --> 00:53:18,380
I've got him here.
Uh,

814
00:53:19,010 --> 00:53:21,810
and that there was a conscious
objective experience.

815
00:53:21,811 --> 00:53:25,680
They went along with that which a
human would have had you saying, okay,

816
00:53:25,681 --> 00:53:26,880
I'm going here and boy,

817
00:53:26,881 --> 00:53:31,881
this is going to really shock them
and ultimately I'll be vindicated and

818
00:53:32,780 --> 00:53:35,930
well, didn't have an emotional
investment in its victory. Yeah.

819
00:53:36,410 --> 00:53:39,320
So, uh, it's interesting.

820
00:53:39,321 --> 00:53:44,210
I mean I think if it's acting human and
certainly in the, and the world to go,

821
00:53:44,690 --> 00:53:49,690
it was acting super human because not
even the human masters were shocked at

822
00:53:50,331 --> 00:53:53,150
this move and ultimately
turned out to be the right one.

823
00:53:53,720 --> 00:53:58,720
I didn't feel that Alphago
had a subjective experience
to go along with that.

824
00:54:00,440 --> 00:54:03,830
You wrote a memoir about
some very personal stuff.

825
00:54:04,370 --> 00:54:07,940
How do you make sure that when you're
writing about something so close to you,

826
00:54:08,390 --> 00:54:12,440
you also, uh, have enough
distance to make it good art?

827
00:54:13,560 --> 00:54:18,030
Yeah, that's a great question.
I mean that's probably part
of why it took so long.

828
00:54:18,090 --> 00:54:22,360
You know, a lot of the experiences in the
book I was writing about as I was, um,

829
00:54:22,890 --> 00:54:23,723
having them,

830
00:54:23,940 --> 00:54:27,690
and then you have to let a certain amount
of time go by before you can really

831
00:54:27,691 --> 00:54:29,130
say, okay, this is done.

832
00:54:29,430 --> 00:54:32,700
And so I think maybe comics,

833
00:54:32,730 --> 00:54:37,110
there's something about comics that
actually can be helpful because you put

834
00:54:37,111 --> 00:54:42,111
yourself a reflection and Avatar of
yourself on the page and then it creates,

835
00:54:42,780 --> 00:54:47,610
it takes on a life of its
own. Um, it's no longer you,

836
00:54:47,611 --> 00:54:51,630
it's like a character that you've created
that's reflecting certain true things

837
00:54:51,631 --> 00:54:56,070
about you, which is what fictional
characters do. Also. Um, maybe this,

838
00:54:56,190 --> 00:54:59,190
the degrees of separation are
just, you know, slightly less here.

839
00:54:59,460 --> 00:55:04,320
And so once I could really see
this character as separate from me,

840
00:55:04,380 --> 00:55:06,000
even though they were a reflection of me,

841
00:55:06,150 --> 00:55:09,780
that made it a lot easier
to edit and trim. Um,

842
00:55:09,900 --> 00:55:14,340
and then also having readers helps,
you know, people who can say, ah,

843
00:55:14,610 --> 00:55:18,060
this is bad. And you're
like, okay, you know,

844
00:55:18,061 --> 00:55:19,890
you have to be quite
humble I think to do it.

845
00:55:20,550 --> 00:55:23,240
Um,
and talking of consciousness,

846
00:55:23,720 --> 00:55:27,290
it often seems very
vague. I often ask, uh,

847
00:55:27,380 --> 00:55:30,190
what is consciousness but itself?
Um,

848
00:55:30,350 --> 00:55:35,240
so I was wondering about how we
can elucidate that idea with,

849
00:55:35,460 --> 00:55:40,180
uh, other elements like
reflective self awareness, uh, uh,

850
00:55:40,410 --> 00:55:45,410
anything more concrete that we
can use to sort of put around it.

851
00:55:46,700 --> 00:55:50,790
I've said that there's no machine you
could build that has a red light and a

852
00:55:50,791 --> 00:55:54,230
green light and you're sliding into
Vienna. Okay. This one's conscious.

853
00:55:54,231 --> 00:55:55,130
Now this one isn't,

854
00:55:55,550 --> 00:55:58,430
that doesn't have some philosophical
assumptions built into it.

855
00:55:58,431 --> 00:56:02,600
And so John's Searle at Berkeley
would want to make sure it's squirting

856
00:56:02,601 --> 00:56:05,750
biological neurotransmitters.
Otherwise the greenlight wouldn't gone.

857
00:56:06,290 --> 00:56:08,990
Dan Dennett would do moral
about what you've said,

858
00:56:08,991 --> 00:56:13,920
which is to have a model of
its own thinking and to be
able to self reflect. Uh,

859
00:56:14,330 --> 00:56:19,010
but you could imagine, uh, you
know, a, a Zombie that did that,

860
00:56:19,011 --> 00:56:22,850
that was able to convincingly talk about
its own experience and you could build

861
00:56:22,851 --> 00:56:26,620
a model of that today doesn't
necessarily mean it's conscious.

862
00:56:26,620 --> 00:56:30,610
And so my conclusion is
it's a philosophical issue,

863
00:56:30,611 --> 00:56:35,050
this to actually a role for
philosophy beyond science,

864
00:56:35,590 --> 00:56:40,360
uh, but that we actually need to make
that philosophical leap of faith.

865
00:56:40,840 --> 00:56:42,640
And we all make it to some extent.
Three,

866
00:56:42,641 --> 00:56:47,500
assume other humans or conscious
is leads. Most other humans. Um,

867
00:56:47,620 --> 00:56:52,560
most of the time, uh, people disagree
about animals. The whole, uh,

868
00:56:52,690 --> 00:56:55,930
animal rights argument is a,
is about what,

869
00:56:56,470 --> 00:57:00,760
whether animals are conscious
and to what degree. And uh,

870
00:57:01,210 --> 00:57:05,320
I think Mike Cat was conscious.
Uh, not everybody agrees.

871
00:57:05,321 --> 00:57:06,970
I think they didn't meet my cat,

872
00:57:09,210 --> 00:57:13,740
but then we'll have arguments about
computers. That's the key thing. Uh,

873
00:57:14,220 --> 00:57:16,870
John,
I'll make this a detailed argument.

874
00:57:16,871 --> 00:57:21,320
How ridiculous it would be that a
computer program could be conscious and

875
00:57:21,321 --> 00:57:23,830
there's was really nothing more
to his Chinese room argument,

876
00:57:23,831 --> 00:57:26,030
which I won't recite now,
uh,

877
00:57:26,040 --> 00:57:30,220
other than the absurdity that a set
of rules could, could be conscious.

878
00:57:30,640 --> 00:57:35,640
But then I took his essay and I simply
substituted a symbolic manipulation for,

879
00:57:39,280 --> 00:57:41,610
uh,
uh,

880
00:57:42,400 --> 00:57:46,960
manipulating neurotransmitter
concentrations and made a
very convincing argument

881
00:57:46,961 --> 00:57:51,580
that the human brain has no
consciousness, no understanding. And uh,

882
00:57:52,450 --> 00:57:52,961
ultimately,

883
00:57:52,961 --> 00:57:56,170
I mean the human brain is made of neurons
and we're understanding neurons is

884
00:57:56,200 --> 00:57:58,990
machines somewhat complicated
but not that complicated.

885
00:57:58,991 --> 00:58:02,500
And so the whole brain is a machine.
And where's the consciousness?

886
00:58:02,501 --> 00:58:06,710
Where's the free? Well, um, so it,

887
00:58:06,740 --> 00:58:10,870
it is a philosophical issue and I don't
think you have to be biological to be

888
00:58:10,871 --> 00:58:11,704
conscious.

889
00:58:11,750 --> 00:58:12,421
I mean,
it's also,

890
00:58:12,421 --> 00:58:17,420
it comes down to being a kind of a
showing of art and empathy. Like if,

891
00:58:17,421 --> 00:58:21,260
uh,
if the AI is convincing enough,

892
00:58:21,261 --> 00:58:25,130
if it's like good enough of a writer
or you know, good enough of an artist,

893
00:58:25,400 --> 00:58:28,400
it convinces you, and
that's what's important. Um,

894
00:58:28,401 --> 00:58:30,180
and I think that's true for humans too.
I mean,

895
00:58:30,230 --> 00:58:32,960
we read a lot of things and
we're not convinced. And, uh,

896
00:58:32,961 --> 00:58:35,600
it doesn't mean that we think
that person deserves to die,

897
00:58:35,601 --> 00:58:39,950
but we do discount how much credit we
give them and how much we're willing to

898
00:58:39,951 --> 00:58:43,810
respect people if we aren't convinced
by how they communicate. And so it,

899
00:58:43,811 --> 00:58:47,710
it seems like it's, you know,
that just in another substrate.

900
00:58:48,080 --> 00:58:49,190
All right, well, thank you very much,

901
00:58:49,240 --> 00:58:49,700
chuck.

