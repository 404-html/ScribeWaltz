1
00:00:10,580 --> 00:00:13,320
Thanks everybody.
Thanks for being here.

2
00:00:13,890 --> 00:00:14,191
Uh,

3
00:00:14,191 --> 00:00:19,191
today I am talking about the ethical
oos ethical operating system,

4
00:00:20,670 --> 00:00:21,210
uh,

5
00:00:21,210 --> 00:00:25,890
or if you want to call it a guide to
anticipating the future impact of today's

6
00:00:25,891 --> 00:00:29,780
technology. We'll talk a lot
more about what that means. Uh,

7
00:00:29,880 --> 00:00:31,290
but for now,

8
00:00:31,740 --> 00:00:35,520
let's just say that this talk is about
how not to regret the things that you

9
00:00:35,521 --> 00:00:39,600
will build. Um, and the ways,
the tools, the foresight,

10
00:00:39,870 --> 00:00:44,190
the frameworks that we can bring to the
process of developing new technologies

11
00:00:44,191 --> 00:00:47,850
to ensure that we have
in mind from the outset,

12
00:00:49,020 --> 00:00:50,940
the user,
the societies,

13
00:00:50,941 --> 00:00:55,680
the communities that will be impacted by
the products that we actually build and

14
00:00:55,681 --> 00:01:00,000
ultimately ship.
So as already mentioned,

15
00:01:00,030 --> 00:01:04,830
I am yours. If, um, there's my
Twitter handle, if you want to tag me.

16
00:01:05,040 --> 00:01:05,873
Um,

17
00:01:05,880 --> 00:01:10,860
I have been doing this work and thinking
pretty deeply about how we build more

18
00:01:10,950 --> 00:01:15,720
innovative, yet responsible products
for a while at Omidyar network. Uh,

19
00:01:15,721 --> 00:01:19,010
and let me tell you a little bit about
what Omidyar network is and what the tech

20
00:01:19,011 --> 00:01:22,650
and society solutions lab is.
To give a little context.

21
00:01:22,830 --> 00:01:27,240
So this is my boss's boss's
boss. Uh, Pierre Omidyar,

22
00:01:27,300 --> 00:01:32,040
who was the founder of Ebay and
from the earliest days at Ebay,

23
00:01:32,041 --> 00:01:37,041
Pierre had a vision for tech as a great
distributor of opportunity in the world

24
00:01:37,981 --> 00:01:42,480
and a great tool for empowering people to,
uh,

25
00:01:42,510 --> 00:01:44,340
really lift themselves up,
right?

26
00:01:44,370 --> 00:01:47,580
If you think about Ebay as a marketplace
where everyone could be a buyer and a

27
00:01:47,581 --> 00:01:48,270
seller,

28
00:01:48,270 --> 00:01:52,890
it was at the core a project
in that idea of empowerment.

29
00:01:52,891 --> 00:01:54,280
And so when Ebay IPO,

30
00:01:54,570 --> 00:01:58,890
Pierre took much of the
wealthy accumulated and
rolled it into Omidyar network

31
00:01:59,010 --> 00:02:02,580
as well as our sister institutions,
uh,

32
00:02:02,581 --> 00:02:06,690
to have social impact through
four and nonprofit investments.

33
00:02:06,840 --> 00:02:09,120
And so for the last 14 or so years,

34
00:02:09,660 --> 00:02:14,550
Omidyar network has been
making investments globally
in things like financial

35
00:02:14,551 --> 00:02:19,500
inclusion, citizen engagement, education,
property rights, digital identity,

36
00:02:19,501 --> 00:02:23,430
a whole range of things with this central
thesis of empowering people around the

37
00:02:23,431 --> 00:02:28,320
world. Uh, in about 2016 or so, we,

38
00:02:28,321 --> 00:02:33,321
like everyone else kind of ran
into a brick wall of an awakening.

39
00:02:34,480 --> 00:02:35,370
That sort of happened,

40
00:02:35,371 --> 00:02:39,060
I think for a lot of us around Brexit
and for many of us around the 2016

41
00:02:39,061 --> 00:02:39,571
elections.

42
00:02:39,571 --> 00:02:43,680
And some of what happened through the
2016 elections in terms of manipulation of

43
00:02:43,710 --> 00:02:45,240
our democratic process,
et cetera.

44
00:02:45,510 --> 00:02:48,510
When we all kind of collectively
woke up and said, oh, wait a second,

45
00:02:48,930 --> 00:02:51,960
that tech that we thought was just
going to solve all the world's problems

46
00:02:51,961 --> 00:02:54,780
actually has some pretty deep,
uh,

47
00:02:54,900 --> 00:02:58,230
problems and challenges that we
actually need to address as well.

48
00:02:58,231 --> 00:03:00,970
And so we had a mid network
beyond thinking about, okay,

49
00:03:00,971 --> 00:03:03,520
we've had this techno
optimism for so long.

50
00:03:03,521 --> 00:03:07,930
How do we actually ensure that the tech
that we are developing is actually good

51
00:03:07,931 --> 00:03:12,130
for people? So not just thinking about
tech in its application for good,

52
00:03:12,131 --> 00:03:14,650
but actually tech that
at its core is good.

53
00:03:14,950 --> 00:03:19,170
And so we stood up the tech and
society solutions lab, which Eh,

54
00:03:19,420 --> 00:03:24,400
aims at our core to help
technologists to mitigate, to prevent,

55
00:03:24,401 --> 00:03:27,250
to correct the downsides of technology,

56
00:03:27,490 --> 00:03:31,780
to ensure that it can live up to its
potential as that positive force for good

57
00:03:31,781 --> 00:03:32,440
in the world.

58
00:03:32,440 --> 00:03:37,440
And so we're doing that by cocreating
and supporting the creation of solutions

59
00:03:37,631 --> 00:03:41,290
to advance these goals. There's been
a lot of talk about the problems.

60
00:03:41,710 --> 00:03:43,870
We'll do a little bit of
that, uh, in a few minutes.

61
00:03:44,110 --> 00:03:48,310
But at the core we're focused on what
are the solutions to those challenges.

62
00:03:48,490 --> 00:03:53,490
And Ethical Oos is one of the tools that
we've built in order to address some of

63
00:03:53,741 --> 00:03:58,300
those challenges. So what
brings us here today, um,

64
00:03:59,200 --> 00:04:04,200
there's been a lot of talk
about the challenges that
tech has created for people

65
00:04:05,830 --> 00:04:08,800
and these are just a few
that are appearing, uh,

66
00:04:08,860 --> 00:04:12,250
challenges around transparency
and pass pass city, uh,

67
00:04:12,700 --> 00:04:16,240
about how decisions are made around
tech and what decisions are being made

68
00:04:16,241 --> 00:04:21,241
beyond our control and
beyond our understanding how
algorithms are developed and

69
00:04:22,001 --> 00:04:24,160
the biases that they
might be perpetuating,

70
00:04:24,460 --> 00:04:28,060
how privacy and security are
built or not in the products,

71
00:04:28,300 --> 00:04:32,650
how text mono culture might be
perpetuating many of these things, uh,

72
00:04:32,680 --> 00:04:37,680
ranging all the way down to environmental
impacts of the tech we're building and

73
00:04:38,170 --> 00:04:41,380
rising social inequality
and economic inequality.

74
00:04:41,500 --> 00:04:45,880
It's a whole range of issues that bring
us to the table and thinking about how

75
00:04:45,881 --> 00:04:47,590
we build more responsible products.

76
00:04:48,250 --> 00:04:51,490
So here's an analogy I like to use,

77
00:04:51,640 --> 00:04:56,470
and this is with all credit to the folks
at Santa Clara University in their tech

78
00:04:57,040 --> 00:05:01,150
ethics practice. But
foresight issues, if you will,

79
00:05:01,210 --> 00:05:06,100
ethical issues are like birds first.
They're everywhere.

80
00:05:07,060 --> 00:05:11,470
Okay? Some are ordinary, some rares, some
are big, some are small, some are local,

81
00:05:11,471 --> 00:05:15,370
some are exotic, some are ubiquitous,
but there are birds everywhere.

82
00:05:16,150 --> 00:05:21,150
But it's actually really
easy to go through life
surrounded by them and not see

83
00:05:21,551 --> 00:05:23,590
them at all.
Right?

84
00:05:23,860 --> 00:05:27,160
They just sort of are in the background
and if you're not paying attention,

85
00:05:27,940 --> 00:05:29,650
you won't even notice the birds.

86
00:05:30,490 --> 00:05:35,490
Getting good at seeing them is
a skill only built by practice.

87
00:05:36,430 --> 00:05:39,970
And practice makes us finding them
easier and actually more rewarding.

88
00:05:39,971 --> 00:05:44,710
So when your practice and seeing the
birds, it becomes an a daily ritual.

89
00:05:44,710 --> 00:05:47,980
It becomes part of what you do as
you navigate through the world.

90
00:05:48,130 --> 00:05:49,750
It becomes that much more rewarding.

91
00:05:51,310 --> 00:05:56,310
Skilled watchers learn how to see them
and where and when they're most likely to

92
00:05:56,321 --> 00:06:01,280
turn up. Right? So when you are
a birdwatcher, it becomes, oh,

93
00:06:01,310 --> 00:06:06,230
I know that if I had to this forest,
I'm likely to see these types of birds.

94
00:06:06,950 --> 00:06:08,690
Right?
And actually it,

95
00:06:08,691 --> 00:06:11,720
certain types are found more
in some landscapes than others.

96
00:06:11,721 --> 00:06:14,060
Some are not found anywhere at all,
right?

97
00:06:14,061 --> 00:06:19,061
You can sort of predict with some degree
of certainty about where those birds

98
00:06:19,101 --> 00:06:22,910
may appear, it's actually
easier to spot them with help.

99
00:06:23,390 --> 00:06:26,360
So if you have a birdwatching
buddy next to you,

100
00:06:26,870 --> 00:06:31,790
you're doubling your capacity to
actually spot the birds on the landscape.

101
00:06:32,180 --> 00:06:36,830
And finally, you may need special
lenses actually to see those birds.

102
00:06:37,070 --> 00:06:42,070
Your own eyes may limit your ability to
actually spot the bird on the horizon.

103
00:06:43,640 --> 00:06:48,380
And so a special lens like ethical Oos
or any other framework helps you to spot

104
00:06:48,381 --> 00:06:50,150
them. So, uh,

105
00:06:50,900 --> 00:06:54,350
if you just replace the word birds with
ethical issues or foresight issues,

106
00:06:54,351 --> 00:06:57,680
I think you understand how this
analogy actually plays out.

107
00:06:58,400 --> 00:07:02,840
And so we've built the ethical alas to
actually aid and just doing this very

108
00:07:02,841 --> 00:07:06,590
thing. And we built it alongside
the Institute for the Future, uh,

109
00:07:06,680 --> 00:07:11,680
and nonprofit group based in Palo Alto
who think deeply about these issues of

110
00:07:12,051 --> 00:07:16,880
foresight, how we think
about consequences, future
technologies, et cetera.

111
00:07:17,030 --> 00:07:21,050
And together we decided that if
you can't predict the future,

112
00:07:21,051 --> 00:07:26,000
you're in the wrong business.
Just kidding. But at the core,

113
00:07:26,180 --> 00:07:29,540
one of the things that we are interested
in doing is thinking about those

114
00:07:29,541 --> 00:07:30,950
consequences of technology.

115
00:07:31,070 --> 00:07:35,030
And when we think about the
consequences of human systems, uh,

116
00:07:35,060 --> 00:07:38,900
that are sped up, amplified or
disrupted by those technologies.

117
00:07:39,020 --> 00:07:43,460
So we know actually that no one can
predict what tomorrow will bring.

118
00:07:43,700 --> 00:07:46,970
Uh,
and so until we get a crystal ball app,

119
00:07:47,540 --> 00:07:52,520
the best we can hope to do is to build
the muscle of anticipating longterm

120
00:07:52,521 --> 00:07:56,120
social impacts, which is essentially
what ethical [inaudible] has meant to do.

121
00:07:56,300 --> 00:07:58,760
And so it's actually not necessary
that we predict the future.

122
00:07:58,880 --> 00:08:02,960
It's just that we get better at
practicing the skill of birdwatching.

123
00:08:03,620 --> 00:08:06,440
Right?
Which I will now introduce you to.

124
00:08:07,220 --> 00:08:12,170
So I think a low ass is designed to
help makers of tech, product managers,

125
00:08:12,171 --> 00:08:16,460
engineers and others get in front
of problems before they happen.

126
00:08:16,610 --> 00:08:20,510
So it's actually been designed to
facilitate better product development,

127
00:08:20,511 --> 00:08:23,690
faster deployment,
and more impactful innovation.

128
00:08:24,230 --> 00:08:27,500
So we have this thing as technologists,
right?

129
00:08:27,501 --> 00:08:30,110
Where we imagine that
the products we build,

130
00:08:30,111 --> 00:08:34,220
we'll solve all the world's problems and
actually it will all change the world

131
00:08:34,221 --> 00:08:39,110
for the better. And that to a
large extent is true, right? Um,

132
00:08:39,290 --> 00:08:44,290
but it is also useful at times to
think of the glass as half empty.

133
00:08:46,520 --> 00:08:49,790
If we're always seeing the glass
as half full or entirely full,

134
00:08:49,910 --> 00:08:52,820
we will not be spotting the
risks before they happen.

135
00:08:53,510 --> 00:08:58,510
So here are the three questions really
that this is designed to ask the,

136
00:09:00,541 --> 00:09:02,640
there are many more embedded.
Um,

137
00:09:02,670 --> 00:09:07,230
if the technology you're building right
now will someday be used in unexpected

138
00:09:07,231 --> 00:09:10,430
ways, what can you do
to prepare for it? Uh,

139
00:09:10,500 --> 00:09:14,490
what new categories of risks
should you be paying attention to?

140
00:09:14,670 --> 00:09:19,350
And lastly, what choices can you make
that would actively safeguard users,

141
00:09:19,351 --> 00:09:22,800
communities, society, and your
company? In this case, Google.

142
00:09:22,801 --> 00:09:25,290
But any company from future risks.

143
00:09:25,710 --> 00:09:28,860
So we built three different tools in the
ethical s I'll introduce you to each of

144
00:09:28,861 --> 00:09:33,050
those tools as a way of exercising
this foresight muscle. Um,

145
00:09:33,120 --> 00:09:38,120
and the first is a set of scenarios to
consider that just sort of start pumping

146
00:09:39,601 --> 00:09:42,990
the juices. Think of it as a
warmup for your run, right?

147
00:09:42,991 --> 00:09:45,330
This is just getting the blood flowing,

148
00:09:45,331 --> 00:09:50,331
thinking about the potential risks of
technologies with some specific city,

149
00:09:51,690 --> 00:09:54,030
excuse me, scenarios. So you know,

150
00:09:54,031 --> 00:09:57,660
some questions to ask about each
of these scenarios is, you know,

151
00:09:57,661 --> 00:10:01,680
what's your greatest worry
when you imagine this future,

152
00:10:02,160 --> 00:10:06,570
how might different users be
affected differently by this future?

153
00:10:07,080 --> 00:10:12,080
What actions would you take if you could
foresee those impacts down the road and

154
00:10:12,721 --> 00:10:17,010
what could we together be
doing differently to prepare
for that risky future?

155
00:10:17,250 --> 00:10:21,810
So this is a little black mirror tree.
Apologies for that.

156
00:10:21,900 --> 00:10:22,531
In fact,

157
00:10:22,531 --> 00:10:27,531
some of the scenarios that we built in
the ethical s have now been played out on

158
00:10:28,591 --> 00:10:32,070
episodes of black mirror as
well as actually in the world.

159
00:10:32,100 --> 00:10:35,970
We've seen some of them. So we built
a scenario around smart toilets,

160
00:10:36,090 --> 00:10:41,090
which we actually couldn't have imagined
would be demoed at ces this past

161
00:10:41,340 --> 00:10:42,173
January.
Um,

162
00:10:42,390 --> 00:10:47,390
so it's the muscle of predicting some of
these things is actually really useful

163
00:10:47,581 --> 00:10:50,790
to exercise. And here are a
couple of the scenarios we belt.

164
00:10:51,030 --> 00:10:56,030
So are you ready for a future where
conversation bots have been trained to

165
00:10:56,281 --> 00:11:00,990
imitate specific people?
So not just people in general and uh,

166
00:11:00,991 --> 00:11:05,250
you know, any, a generic user or
a customer service representative,

167
00:11:05,251 --> 00:11:10,251
but your grandmother or a celebrity using
datasets collected from public social

168
00:11:10,831 --> 00:11:15,720
media posts. And those bots are deployed
across networks, email and text messages,

169
00:11:15,721 --> 00:11:19,980
and super targeted super
personalized propaganda campaigns.

170
00:11:20,400 --> 00:11:21,990
And they are,
as a result,

171
00:11:21,991 --> 00:11:25,710
highly effective in changing
opinions and driving action. Right?

172
00:11:25,920 --> 00:11:29,910
As these messages appear to be
from your family and friends,

173
00:11:31,140 --> 00:11:34,170
what are you worried about?
What can we do to prevent this future?

174
00:11:34,440 --> 00:11:37,830
How would people be disproportionately
impacted by a future in which this

175
00:11:37,831 --> 00:11:38,840
happens?
You know,

176
00:11:38,850 --> 00:11:43,320
in response to growing concerns over
tech addiction and anticipating governant

177
00:11:43,410 --> 00:11:45,900
government regulation
as a possible future.

178
00:11:46,230 --> 00:11:51,230
Several popular social media and game
companies decide to voluntarily enforced

179
00:11:51,690 --> 00:11:55,480
time limits. Sounds like
a good idea, right? Um,

180
00:11:55,510 --> 00:11:58,000
so adults might be limited
to two hours a day.

181
00:11:58,001 --> 00:12:00,400
Kids might be limited to one hour a day.

182
00:12:00,550 --> 00:12:04,720
And actually whether a platform is limited
or unlimited becomes a major selling

183
00:12:04,721 --> 00:12:05,554
point for them.

184
00:12:06,370 --> 00:12:11,370
You can imagine that world in which an
unlimited platform is much hotter as a

185
00:12:12,131 --> 00:12:14,530
consumer good and people,

186
00:12:14,560 --> 00:12:18,430
but some people at the same time prefer
having hard limits that PR that prevent

187
00:12:18,431 --> 00:12:22,540
them from becoming addicted.
So again,

188
00:12:22,800 --> 00:12:25,840
who is affected? There was actually
just an article in the New York Times,

189
00:12:25,841 --> 00:12:30,370
I think last week about how in some
measure personal human interaction is

190
00:12:30,371 --> 00:12:34,870
becoming a luxury good
a as those with lesser,

191
00:12:34,900 --> 00:12:39,460
um, opportunities are more
engaged with screens, right?

192
00:12:39,461 --> 00:12:44,461
So you can think about the disparate
impacts of this kind of a future fortune

193
00:12:45,911 --> 00:12:50,911
500 re human resource
departments subscribed to a
smart employer service that

194
00:12:50,951 --> 00:12:55,951
evaluates a person's suitability for your
culture and the stress associated with

195
00:12:56,111 --> 00:12:59,020
that culture.
Using social media posts.

196
00:12:59,320 --> 00:13:04,030
We already kind of do this,
uh, people trolling, Facebook,

197
00:13:04,031 --> 00:13:07,270
Instagram, etc. To understand
who the potential employee is.

198
00:13:07,271 --> 00:13:10,750
But you can imagine being this being
automated through artificial intelligence.

199
00:13:10,870 --> 00:13:15,870
So algorithms can actually
identify individuals suffering
from mental illness or

200
00:13:16,181 --> 00:13:20,950
depression, right? You could, the service
could actually develop symptoms of,

201
00:13:21,370 --> 00:13:25,480
you could predict the development of
mental illness in the future based on

202
00:13:25,481 --> 00:13:27,960
trends in the individual's postings.
Again,

203
00:13:27,980 --> 00:13:32,980
you can imagine the impacts that that
would have on workforce on the future of

204
00:13:33,491 --> 00:13:38,170
work, on hiring, on, and again, disparate
impacts on different groups of people.

205
00:13:38,860 --> 00:13:43,600
A major social network company in the
future could purchase a top US bank and

206
00:13:43,601 --> 00:13:47,380
become a first of its kind
social credit provider.

207
00:13:47,920 --> 00:13:50,380
And it could base mortgage loans,
loan approvals,

208
00:13:50,381 --> 00:13:55,090
credit access on deep data collected
from its social platform and could take

209
00:13:55,091 --> 00:14:00,091
into consideration credit histories of
your friends and family as well as the

210
00:14:00,371 --> 00:14:01,750
locations you've visited.

211
00:14:02,020 --> 00:14:07,020
So imagine if you've visited a bar or a
legal marijuana dispensary figuring into

212
00:14:08,711 --> 00:14:11,740
your social credit and the credit
that your bank might provide.

213
00:14:12,130 --> 00:14:16,270
And it could actually also do semantic
analysis of your messages and photos to

214
00:14:16,271 --> 00:14:20,710
actually decide whether you are generally
happy, angry, anxious, or depressed.

215
00:14:20,920 --> 00:14:23,110
Figure that into whether
you get alone as well.

216
00:14:24,820 --> 00:14:28,510
25% of online orders
are delivered by drone.

217
00:14:29,530 --> 00:14:31,840
Sounds great,
comes right to your door.

218
00:14:32,020 --> 00:14:35,980
But actually these drones are fitted with
cameras and sensors to collect data as

219
00:14:35,981 --> 00:14:38,950
they fly over neighborhoods,
which actually provides,

220
00:14:38,951 --> 00:14:42,640
right the drone company with additional
revenue for the shippers and merchants

221
00:14:43,280 --> 00:14:47,980
in some individuals can actually opt
out for free, unlimited drone delivery,

222
00:14:48,280 --> 00:14:50,050
uh,
by paying.

223
00:14:50,051 --> 00:14:54,020
And they consent to the collection of
their data by doing that for the drones

224
00:14:54,021 --> 00:14:54,854
that fly over.

225
00:14:55,040 --> 00:14:58,550
But actually neighborhoods where the
drone delivery is legally permitted or

226
00:14:58,551 --> 00:15:00,620
subject to the same data
collection activities,

227
00:15:00,740 --> 00:15:04,790
even though not all of their residents
or households have explicitly consented.

228
00:15:06,190 --> 00:15:10,340
And again, who is impacted? What
could we do to prevent this future?

229
00:15:10,341 --> 00:15:11,174
If you think about it?

230
00:15:11,720 --> 00:15:15,770
So one of the tools that you can use
to think about these impacts of a tech

231
00:15:15,771 --> 00:15:18,620
product you're building before it happens,
it's called a futures wheel.

232
00:15:19,910 --> 00:15:23,600
And if you take one thing from this talk,

233
00:15:24,620 --> 00:15:25,910
I hope you take a lot of
things from the stock.

234
00:15:26,000 --> 00:15:27,740
But if you take one thing from this talk,

235
00:15:27,741 --> 00:15:31,160
the future's wheel is an incredibly
powerful tool for thinking these things

236
00:15:31,161 --> 00:15:35,660
through. So put at the center,
your product, your change,

237
00:15:35,870 --> 00:15:39,290
the idea you have.
Let's go to the last example.

238
00:15:39,410 --> 00:15:41,630
Drone delivery of orders,
right?

239
00:15:42,150 --> 00:15:44,660
A of online orders and spin out from that,

240
00:15:44,750 --> 00:15:49,750
the consequences that might happen first
order consequences and then from each

241
00:15:50,001 --> 00:15:51,860
of those first order
consequences. Think, okay,

242
00:15:51,861 --> 00:15:56,861
and then what happens and then what
happens and then what happens from each of

243
00:15:56,931 --> 00:15:58,760
these nodes on the futures.
We all,

244
00:15:59,300 --> 00:16:04,300
so I'll give you just two quick examples
that I easily pulled actually from the

245
00:16:05,331 --> 00:16:09,350
web by just googling futures. We all, um,

246
00:16:09,351 --> 00:16:12,890
so the first is that there
are increased choices, right?

247
00:16:12,891 --> 00:16:14,900
For renewable energy,

248
00:16:15,110 --> 00:16:20,110
which sounds like an incredibly
positive development in the world,

249
00:16:20,151 --> 00:16:24,020
right? Um, and we would
all likely think, well,

250
00:16:24,050 --> 00:16:28,640
increased choices for renewable energy
is positive as a positive development.

251
00:16:28,641 --> 00:16:30,800
So put that in the center of your futures.
We all,

252
00:16:30,980 --> 00:16:34,250
and then spin out the first order
consequences of that. Right?

253
00:16:34,280 --> 00:16:38,930
So you can imagine new companies emerging
in the energy marketplace, oil prices,

254
00:16:38,931 --> 00:16:43,790
falling, monopolies being dismantled and
technologies being developed, you know,

255
00:16:43,791 --> 00:16:46,010
based on geography and politics.
Again,

256
00:16:46,550 --> 00:16:49,970
those all sound really positive
as first order consequences.

257
00:16:50,090 --> 00:16:52,040
But when you spend some of those out,

258
00:16:52,790 --> 00:16:56,510
you actually get to some
negative consequences. Now
on the second order, right?

259
00:16:56,511 --> 00:16:58,100
So if oil prices fall,

260
00:16:59,030 --> 00:17:03,860
jobs can disappear in the oil industry
and some poor countries can actually

261
00:17:03,861 --> 00:17:06,920
become oil dependent, right?
Because oil is now cheaper.

262
00:17:07,220 --> 00:17:11,420
So poor countries can access more oil
rather than renewables and they become oil

263
00:17:11,421 --> 00:17:14,450
dependent as a result. Right? Um,

264
00:17:14,510 --> 00:17:18,470
and actually you can imagine also on
loss of monopolies that actually war and

265
00:17:18,471 --> 00:17:22,760
poverty could result for countries who
are not competitive in the marketplace,

266
00:17:23,270 --> 00:17:27,620
right? So it takes this tool
of figuring out first order,

267
00:17:27,800 --> 00:17:28,430
second order,

268
00:17:28,430 --> 00:17:32,810
third order consequences in order
sometimes to see the negative impacts.

269
00:17:33,020 --> 00:17:36,470
So again, we imagine renewables
good first order good.

270
00:17:36,620 --> 00:17:39,590
When we get to second and third,
or oftentimes on the horizon,

271
00:17:39,740 --> 00:17:44,030
you can see potential risks out ahead.

272
00:17:44,120 --> 00:17:47,660
Here's another one.
I'm sorry if this is illegible again,

273
00:17:47,690 --> 00:17:50,400
I just pulled it from the web.
Uh,

274
00:17:50,430 --> 00:17:55,430
at the center of this futures wheel is
open source technology and generally,

275
00:17:56,190 --> 00:18:00,810
you know, popular sentiment is that
open source technology is a net good,

276
00:18:00,990 --> 00:18:02,790
it's positive development,
et cetera.

277
00:18:02,791 --> 00:18:04,920
I'm not going to go through
this entire futures wheel,

278
00:18:04,950 --> 00:18:09,330
but I just wanted to highlight a few
things further out on the degrees of

279
00:18:09,331 --> 00:18:12,900
consequences and impacts here.
So if you get out far enough,

280
00:18:13,110 --> 00:18:16,890
you have you know,
untraceable terrorism,

281
00:18:17,520 --> 00:18:21,920
you have companies failing,
you have, you know,

282
00:18:21,960 --> 00:18:25,950
a loss of jobs, you have lack of
attribution of product development,

283
00:18:25,951 --> 00:18:30,951
you have some potential negative impacts
from open source technology that might

284
00:18:31,231 --> 00:18:34,260
not have been expected right
at the outset. So again,

285
00:18:34,290 --> 00:18:37,830
just a useful tool and starting
to think through future impacts.

286
00:18:39,900 --> 00:18:42,990
Second tool,
risk scanning.

287
00:18:43,590 --> 00:18:48,590
So we created eight risk zones to look
at when thinking about your products.

288
00:18:49,380 --> 00:18:51,420
And I'm going to go through
each of these in some detail,

289
00:18:51,421 --> 00:18:55,170
but just to highlight them
here. Risks Zone one, truth,

290
00:18:55,171 --> 00:18:59,850
disinformation and propaganda to
addiction and the dopamine economy. Three,

291
00:18:59,851 --> 00:19:04,830
economic and asset inequalities for
machine ethics. And algorithmic bias.

292
00:19:05,220 --> 00:19:09,570
Five, the surveillance state. Sixth,
data control and monetization.

293
00:19:09,840 --> 00:19:10,620
Seven,

294
00:19:10,620 --> 00:19:15,000
implicit trust in user understanding
an eight hateful and criminal actors.

295
00:19:15,330 --> 00:19:17,880
So by dividing into
these eight risk zones,

296
00:19:18,030 --> 00:19:23,030
we could sort of identify spaces
where hard to anticipate and unwelcome

297
00:19:24,001 --> 00:19:27,930
consequences are most likely to emerge,
say over the next 10 years.

298
00:19:28,230 --> 00:19:30,270
Now I want to stipulate,

299
00:19:30,300 --> 00:19:34,710
we did not include every area
where things might go wrong, right?

300
00:19:34,711 --> 00:19:38,730
So you'll notice here environmental
impacts are not included. Again,

301
00:19:38,731 --> 00:19:42,480
this was meant to be the intersection
of both on hard to anticipate and

302
00:19:42,481 --> 00:19:43,314
unwelcome.

303
00:19:43,650 --> 00:19:47,850
We actually know that environmental
impacts are likely to come.

304
00:19:47,880 --> 00:19:49,950
So that's not hard to anticipate.
These risks.

305
00:19:49,951 --> 00:19:54,030
Villains are really just meant to
represent those hard anticipate areas.

306
00:19:54,830 --> 00:19:58,950
So let's talk about risks on one truth,
dif information and propaganda.

307
00:19:59,190 --> 00:20:03,540
And in this future,
shared facts are under attack.

308
00:20:04,290 --> 00:20:09,060
And the primary mode of attacking,
uh,

309
00:20:09,150 --> 00:20:09,990
today,

310
00:20:10,350 --> 00:20:15,350
these kinds of shared
facts has been fake video,

311
00:20:16,820 --> 00:20:19,910
right? So this is a signal
that we see on the landscape,

312
00:20:19,911 --> 00:20:23,430
but that says this is a
potential risk zone in the,

313
00:20:23,970 --> 00:20:25,800
in the near term,
right?

314
00:20:25,801 --> 00:20:30,801
So everything from fake news to bots
that spread propaganda and deep fakes,

315
00:20:31,441 --> 00:20:36,441
which are highly convincing video
that's algorithmically altered or that

316
00:20:37,021 --> 00:20:37,841
replaces people,

317
00:20:37,841 --> 00:20:42,390
speech and facial expressions
and identities creates
fake proof of actions or

318
00:20:42,391 --> 00:20:45,150
speech that actually never happened,
right?

319
00:20:45,270 --> 00:20:49,120
So individuals today are highly
motivated to subvert the truth,

320
00:20:49,121 --> 00:20:54,121
that massive scale specially for political
ends and new technologies will make

321
00:20:54,251 --> 00:20:58,990
it easier to spread lies and undermine
trust. So over the next decade,

322
00:20:59,350 --> 00:21:03,010
what else could be faked
via new technologies?

323
00:21:04,060 --> 00:21:06,570
So if this is the risk
zone you're scanning for,

324
00:21:06,580 --> 00:21:09,760
what questions should you be asking?
First,

325
00:21:10,450 --> 00:21:14,780
what type of data do users expect
you to accurately share, measure,

326
00:21:14,781 --> 00:21:16,300
or collect too?

327
00:21:16,660 --> 00:21:20,980
How could bad actors use your tech
to subvert or attack the truth?

328
00:21:21,480 --> 00:21:26,470
Um, what could become the equivalent of
fake news or deep fakes on your platform?

329
00:21:27,340 --> 00:21:31,330
How could someone use your technology to
undermine trust and social institutions?

330
00:21:31,840 --> 00:21:32,680
And lastly,

331
00:21:33,820 --> 00:21:38,290
if you can imagine the form
that that misinformation
might take on your platform,

332
00:21:38,890 --> 00:21:41,350
even if your tech is a political,

333
00:21:41,351 --> 00:21:46,120
how could it be politicized in some
way to destabilize a government,

334
00:21:46,150 --> 00:21:51,040
a community of regime, etc. So
this is just again, checklist,

335
00:21:51,280 --> 00:21:55,630
thinking about your product,
questions to ask and risk zone number one

336
00:21:57,130 --> 00:21:59,980
risk. So number to addiction
in the dopamine economy.

337
00:22:00,790 --> 00:22:02,410
So in this risk zone,

338
00:22:02,411 --> 00:22:06,250
we all have sort of come to
understand that time spent online,

339
00:22:06,730 --> 00:22:08,890
maximizes profits over wellbeing.

340
00:22:09,160 --> 00:22:14,160
And you can see that in things like
research by common sense media,

341
00:22:14,381 --> 00:22:19,381
which shows that actually now the average
teenager spends nine hours a day using

342
00:22:19,661 --> 00:22:23,380
some form of social media.
Uh,

343
00:22:23,410 --> 00:22:25,930
and actually at the same time,

344
00:22:25,931 --> 00:22:30,490
studies show that people achieve their
maximum intended use of an APP like

345
00:22:30,491 --> 00:22:34,830
Instagram or snapchat after 11 minutes.
Okay.

346
00:22:35,290 --> 00:22:38,800
And actually after 11 minutes,
overall happiness decreases.

347
00:22:39,610 --> 00:22:44,610
So how can you actually design tools to
advocate for user happiness offline and

348
00:22:45,371 --> 00:22:48,880
online over keeping eyes
glued to the screen? Right?

349
00:22:48,881 --> 00:22:52,450
And so the questions in risk zone number
two that you probably would want to be

350
00:22:52,451 --> 00:22:53,770
asking are,

351
00:22:53,980 --> 00:22:58,980
does your business model you maximize
user attention and engagement?

352
00:23:00,150 --> 00:23:05,050
You know, essentially the more the better.
And if so, is that good for people?

353
00:23:05,440 --> 00:23:09,730
You know, what is extreme use look
like? Um, and how would you know,

354
00:23:09,731 --> 00:23:11,740
moderate versus extreme use?

355
00:23:12,010 --> 00:23:16,420
How could you design a system that
actually courages moderate use.

356
00:23:16,870 --> 00:23:17,703
And lastly,

357
00:23:17,950 --> 00:23:22,950
is there potential actually for negative
material for toxic material to actually

358
00:23:24,700 --> 00:23:29,700
increase and drive levels of engagement
that maximize time spent on the platform

359
00:23:32,260 --> 00:23:35,000
for questions to ask around
addiction and the dopamine come, Hey,

360
00:23:35,001 --> 00:23:38,530
I'm going to roll through these
a little more quickly now, um,

361
00:23:38,531 --> 00:23:43,230
for each of these just so that I can
enjoy all of the eight risk zones. Uh,

362
00:23:43,260 --> 00:23:44,920
so economic and asset,

363
00:23:45,920 --> 00:23:49,250
right where new technology
can democratize access,

364
00:23:49,251 --> 00:23:53,430
but it can obviously also
exacerbate inequality. Uh,

365
00:23:53,730 --> 00:23:58,730
in this world you can get
cheaper insurance by being white,

366
00:23:58,850 --> 00:24:00,740
right?
And 2017,

367
00:24:00,770 --> 00:24:04,910
Oxfam international show that eight
people owned as much wealth as the entire

368
00:24:04,911 --> 00:24:08,390
bottom half of the world's
population, right? So wealth,

369
00:24:08,930 --> 00:24:12,970
concentration and distribution
is an issue. And this, uh,

370
00:24:13,010 --> 00:24:17,120
new technology can provide income,
opportunity, and balanced distribution,

371
00:24:17,121 --> 00:24:21,500
but it all can also cater only to high
income groups and eliminate low income

372
00:24:21,501 --> 00:24:22,334
jobs.

373
00:24:22,820 --> 00:24:27,820
So what questions should view asking
which groups are disproportionately

374
00:24:28,070 --> 00:24:29,560
impacted?
Um,

375
00:24:29,660 --> 00:24:34,660
how might workers be impacted on your
platform by virtue of the type of form a

376
00:24:37,280 --> 00:24:41,030
or contract that they are given
contractors versus full time employees?

377
00:24:41,240 --> 00:24:46,240
How might a communities with
fewer resources actually
not be able to access your

378
00:24:47,361 --> 00:24:51,680
platform or B, have too much
access to your platform, right?

379
00:24:51,860 --> 00:24:56,180
To think about how distribution of wealth
is impacting both the haves and the

380
00:24:56,181 --> 00:25:00,170
have nots. Uh, in risk don't four,

381
00:25:00,440 --> 00:25:04,550
we're looking at machine ethics and
Algorithmic bias, right? So in this world,

382
00:25:05,210 --> 00:25:09,920
human biases amplified through artificial
intelligence as in the case recently

383
00:25:09,921 --> 00:25:12,570
where Amazon's scrapped it's uh,

384
00:25:12,920 --> 00:25:17,920
HR recruiting tool that actually said
if you had the word woman appearing

385
00:25:18,201 --> 00:25:20,930
anywhere on your resume,
you were actually,

386
00:25:21,230 --> 00:25:26,230
I think it was 27 times less likely to
receive an interview then if that were,

387
00:25:27,771 --> 00:25:31,520
did not appear as a keyword on the resume,
the AI trained itself.

388
00:25:33,380 --> 00:25:35,360
But actually, um, you know,

389
00:25:35,361 --> 00:25:39,500
the application of AI in critical
domains like welfare and education,

390
00:25:39,501 --> 00:25:43,430
employment and criminal justice,
criminal justice has intensified, right?

391
00:25:43,431 --> 00:25:48,431
So the idea that technology is neutral
and that this is not a product of human

392
00:25:48,711 --> 00:25:52,040
action is no longer really acceptable
because we actually sort of know that

393
00:25:52,041 --> 00:25:55,340
human decisions are being
used to make the models,

394
00:25:55,341 --> 00:25:59,720
human decisions are using to categorize
the data. Human decisions are being used,

395
00:25:59,980 --> 00:26:03,230
um,
and the algorithm,

396
00:26:03,231 --> 00:26:06,830
and you can't blame the
algorithm any longer.

397
00:26:07,070 --> 00:26:11,270
So at the core of the questions you
want to be asking are, you know,

398
00:26:11,420 --> 00:26:16,420
do use deep data and machine learning
and are there gaps in the data that

399
00:26:17,601 --> 00:26:21,620
historical biases might actually be
biasing the technology. So for example,

400
00:26:21,621 --> 00:26:25,160
the compass criminal justice
profiling system, uh,

401
00:26:25,161 --> 00:26:27,870
was sentencing black,
um,

402
00:26:27,890 --> 00:26:32,090
defendants far more frequently than
white defendants and to much more severe

403
00:26:32,091 --> 00:26:36,150
sentences because it was using
a dataset that showed, um,

404
00:26:36,860 --> 00:26:37,220
you know,

405
00:26:37,220 --> 00:26:41,240
that black defendants were more likely
to commit crimes again in the future.

406
00:26:41,450 --> 00:26:41,691
Well,

407
00:26:41,691 --> 00:26:46,691
it turns out that if you're using
historical data about crime and it's

408
00:26:48,541 --> 00:26:51,540
incidents, right, and you're
using that to predict future,

409
00:26:51,600 --> 00:26:55,800
you're actually just reinforcing the
bias of what has happened historically as

410
00:26:55,801 --> 00:26:58,680
opposed to countering that bias.
Um,

411
00:26:58,710 --> 00:27:03,360
by understanding that actually pass
is not a future. Does that make sense?

412
00:27:03,570 --> 00:27:06,720
Right. Um, like black defendants, yes,

413
00:27:06,750 --> 00:27:11,750
have typically had higher recidivism
rates because of historical,

414
00:27:13,020 --> 00:27:17,130
deeply entrenched economic
and sociopolitical, uh,

415
00:27:17,160 --> 00:27:18,660
inequality.
Right?

416
00:27:18,661 --> 00:27:22,710
But if you use that to predict the
future and sentence people accordingly,

417
00:27:22,800 --> 00:27:25,060
you're just reinforcing that bias.
Uh,

418
00:27:25,110 --> 00:27:30,110
have you seen instances of that bias
actually entered your products algorithms?

419
00:27:32,160 --> 00:27:36,870
Is it actually amplifying that
bias and who's responsible for it?

420
00:27:37,380 --> 00:27:41,850
Do you have a diverse team that can spot
those risks early enough to understand

421
00:27:42,030 --> 00:27:45,060
that you may be perpetuating those biases?
And actually,

422
00:27:45,180 --> 00:27:49,950
how will you push back against the blind
preference for AI develops systems?

423
00:27:50,160 --> 00:27:54,720
And do you have transparency into that
system and is their actual recourse for

424
00:27:54,721 --> 00:27:59,460
people who feel like they've been
negatively impacted? In Rick's zone five,

425
00:27:59,461 --> 00:28:01,680
we've got a surveillance state,
uh,

426
00:28:01,710 --> 00:28:06,330
set of issues where surveillance tools
and facial recognition right today are

427
00:28:06,331 --> 00:28:09,090
empowering the powerful at
the expense of the powerless.

428
00:28:09,600 --> 00:28:14,600
So recent examples of social bots being
co opted by governments and militaries

429
00:28:14,821 --> 00:28:17,040
for use and attacking their opposition.
Uh,

430
00:28:17,120 --> 00:28:22,120
armies of automated software
driven profiles are used
to target journalists and

431
00:28:22,231 --> 00:28:26,470
activists and citizens using
Western surveillance tools. Uh,

432
00:28:27,000 --> 00:28:30,900
so in this risk zone you need
to be asking about your product.

433
00:28:30,901 --> 00:28:35,550
How might a government or military body
use this technology to increase their

434
00:28:35,551 --> 00:28:36,140
capacity?

435
00:28:36,140 --> 00:28:40,560
So surveil their citizens who besides
the government and the military might

436
00:28:40,561 --> 00:28:43,800
actually have access to those tools and
want to increase the surveillance of its

437
00:28:43,801 --> 00:28:48,060
citizens as well. Who would they
track? And why are you creating data,

438
00:28:48,570 --> 00:28:48,901
right?

439
00:28:48,901 --> 00:28:53,760
That actually could follow
people throughout their
lifetimes and actually will

440
00:28:53,761 --> 00:28:57,510
the data your tech is generating have
longterm consequences for the freedom of

441
00:28:57,511 --> 00:28:59,160
reputation of those individuals.

442
00:28:59,310 --> 00:29:04,310
And who would you not want to use your
data to surveil and what can you do to

443
00:29:04,531 --> 00:29:09,531
proactively protect that data from being
accessible to those malicious actors

444
00:29:09,961 --> 00:29:13,740
who might not want to have access to it?
And risk zone six,

445
00:29:13,741 --> 00:29:17,800
we're talking about data control and
monetization where users actually lack the

446
00:29:18,090 --> 00:29:22,470
control to share and monetize and benefit
from their data alongside the tech

447
00:29:22,471 --> 00:29:24,930
companies that create and use it,
right?

448
00:29:24,931 --> 00:29:29,931
So we've seen this actually
and we understand that
users expect access to tools

449
00:29:32,400 --> 00:29:33,840
now for acquiring,
sharing,

450
00:29:33,841 --> 00:29:37,590
interpreting and verifying or information
that's been collected about them.

451
00:29:37,830 --> 00:29:40,670
And we expect,
uh,

452
00:29:40,810 --> 00:29:45,810
that there will be an increasing level
of agency around data from users in the

453
00:29:47,711 --> 00:29:48,340
future.

454
00:29:48,340 --> 00:29:52,840
So if you're creating a product that
collects data and that's just about every

455
00:29:52,841 --> 00:29:56,110
product these days, what are the
questions you want to be asking?

456
00:29:57,070 --> 00:29:58,660
What date are you actually collecting?

457
00:29:58,900 --> 00:30:01,990
Do you need to collect it or
would you just like to collect it?

458
00:30:02,200 --> 00:30:03,070
Are you selling it?

459
00:30:03,071 --> 00:30:07,300
And if you're selling it to who and how
might they go about using it to your

460
00:30:07,301 --> 00:30:10,060
users, have the right to access
the data you're collecting on them.

461
00:30:10,360 --> 00:30:14,140
And could you build a way into your
platform or product to give them the right

462
00:30:14,141 --> 00:30:15,030
to capture,
share,

463
00:30:15,031 --> 00:30:19,830
monetize their data independently
outside from you? Uh,

464
00:30:19,930 --> 00:30:22,810
what could bad actors do with this
data if they had access to it?

465
00:30:23,500 --> 00:30:27,130
What could the government do with the
data if they were granted access to it?

466
00:30:27,610 --> 00:30:31,180
These are actually questions that
we've seen come along on the landscape

467
00:30:31,181 --> 00:30:32,790
recently.
Um,

468
00:30:32,860 --> 00:30:37,860
particularly with access to people's
phones or other types of requests from the

469
00:30:38,111 --> 00:30:41,770
government for people's data.
And Risto seven,

470
00:30:41,771 --> 00:30:46,090
we're talking about implicit trust in
user understanding where the misuse of

471
00:30:46,091 --> 00:30:50,230
data as a serious problem actually
because users don't trust companies

472
00:30:51,880 --> 00:30:54,940
with a opaque terms of service.

473
00:30:55,390 --> 00:30:56,920
And uh,

474
00:30:57,550 --> 00:31:02,550
the inability for users to
actually understand how a
popular APP or platform is

475
00:31:02,981 --> 00:31:06,730
working, how their engagement
is optimized, what's being
tracked and collected.

476
00:31:06,850 --> 00:31:11,850
It's really hard for users to have clarity
on what are the terms and companies

477
00:31:14,981 --> 00:31:17,560
can expect backlash actually from product,

478
00:31:17,860 --> 00:31:22,750
from the users of their products and from
employees actually when those terms of

479
00:31:22,751 --> 00:31:26,560
service are violated as in
the case here with Uber. Uh,

480
00:31:26,800 --> 00:31:28,060
so the questions to ask,

481
00:31:28,940 --> 00:31:32,980
does the technology you're building
have a clear code of rights and are your

482
00:31:32,981 --> 00:31:36,850
terms of service easy to read,
access and understand, uh,

483
00:31:36,970 --> 00:31:40,540
is there a version of your product that's
available to users if they don't want

484
00:31:40,541 --> 00:31:41,650
to sign the user agreement?

485
00:31:42,190 --> 00:31:44,830
And could you imagine building
a product that would do that?

486
00:31:45,340 --> 00:31:48,700
Does your technology actually do
anything you don't even know about?

487
00:31:49,780 --> 00:31:54,340
It's hard to imagine, but this is
actually the case. And actually,

488
00:31:54,550 --> 00:31:55,383
um,

489
00:31:56,170 --> 00:31:59,560
if users object to the idea of
their actions being monetized,

490
00:31:59,740 --> 00:32:04,030
is it possible to create a sustainable
model that builds trust with them and are

491
00:32:04,031 --> 00:32:06,460
all users treated equally?
Um,

492
00:32:06,461 --> 00:32:11,461
so could you handle consumer demands are
government regulations that require all

493
00:32:12,011 --> 00:32:15,540
users to be treated equally
or at least transparently. Um,

494
00:32:15,670 --> 00:32:19,900
could your product or platform
actually do that? And lastly,

495
00:32:19,930 --> 00:32:23,320
rezoned eight hateful in criminal actors,
uh,

496
00:32:23,680 --> 00:32:27,790
where online tools enabled glo
global dissemination of terrorism,

497
00:32:27,791 --> 00:32:31,870
hate bullying, radicalization trolling,
doxing, and much more. You know,

498
00:32:31,871 --> 00:32:36,871
we saw this very unfortunately
most recently with the
massacre in New Zealand or

499
00:32:37,151 --> 00:32:42,080
social media platforms were
used to disseminate terrorist
activities and used to

500
00:32:42,081 --> 00:32:43,970
radicalize the shooter in the first place.

501
00:32:44,750 --> 00:32:49,750
So using platforms to actually perpetuate
these kinds of behaviors globally.

502
00:32:50,540 --> 00:32:51,373
Uh,

503
00:32:51,530 --> 00:32:56,530
and there is a responsibility as a
foresight tool to be thinking about those

504
00:32:57,230 --> 00:32:59,360
possibilities before they happen.

505
00:32:59,480 --> 00:33:03,920
So how could someone use your technology
to bully, stalk, or harass people?

506
00:33:04,400 --> 00:33:07,790
You know,
what kinds of ransomware theft,

507
00:33:07,791 --> 00:33:12,791
financial crimes or fraud could
come around by use of your platform?

508
00:33:13,580 --> 00:33:18,580
Do you as a technology maker have an
ethical responsibility to make it for bad

509
00:33:19,131 --> 00:33:22,880
actors, to act, to make it harder for
bad actors to act on your platform?

510
00:33:23,270 --> 00:33:28,070
How could organize hate groups? Use your
technology to spread hate or recruit,

511
00:33:28,130 --> 00:33:30,470
uh, others. And lastly,

512
00:33:31,490 --> 00:33:34,370
what are the risks of your
technology being weaponized?

513
00:33:34,820 --> 00:33:37,370
And what responsibility
do you have to prevent it?

514
00:33:38,210 --> 00:33:42,500
So if you go through those eight risks
zones and there's a lot more embedded in

515
00:33:42,501 --> 00:33:46,250
each of them, you can download this full
full toolkit from ethical o s. Dot. Org.

516
00:33:46,790 --> 00:33:50,120
Uh, there's a lot more embedded
in each of those risks zones.

517
00:33:50,270 --> 00:33:54,110
But if you go through that,
here's three things to do. Uh,

518
00:33:54,680 --> 00:33:58,010
share it with your team. Go through
this as an exercise with your team.

519
00:33:58,011 --> 00:33:59,120
Go through the risks zones,

520
00:33:59,121 --> 00:34:03,190
understand the impact of your
technology at your team level. Um,

521
00:34:03,350 --> 00:34:08,000
you could consider adding some of
those top questions to your product

522
00:34:08,001 --> 00:34:11,300
requirements document or
any other document that you
set out at the beginning of

523
00:34:11,301 --> 00:34:13,310
the development cycle to think about,

524
00:34:13,850 --> 00:34:17,450
which was questions you should be
asking all along the way as you develop.

525
00:34:17,451 --> 00:34:19,400
And before you ship.
And lastly,

526
00:34:19,401 --> 00:34:24,401
you should scan the horizon actually on
as an ongoing exercise for additional

527
00:34:24,471 --> 00:34:27,380
information about the risks zones.
And if you're doing that,

528
00:34:27,381 --> 00:34:31,430
you're doing the work of the ethical ols
and thinking about the consequences of

529
00:34:31,431 --> 00:34:32,630
the tech before you ship.

530
00:34:34,370 --> 00:34:37,220
The last tool we built
unethical ols is actually, um,

531
00:34:37,250 --> 00:34:41,930
just some ideas for how to do some
alternative ways of future proofing.

532
00:34:42,290 --> 00:34:47,180
So thinking about best practices that
could help the tech community community

533
00:34:47,181 --> 00:34:48,530
mitigate risk at scale.

534
00:34:49,280 --> 00:34:53,750
So these are some ideas for industry
wide efforts to create products that you

535
00:34:53,751 --> 00:34:57,110
know, have a company in
humanity's best interest in mind.

536
00:34:57,260 --> 00:35:00,590
So these are just kind
of pie in the sky ideas.

537
00:35:00,591 --> 00:35:02,780
There's a lot more actually
in the ethical OSMP

538
00:35:04,760 --> 00:35:08,090
but here are a few to consider.
Um, so one is, you know,

539
00:35:08,091 --> 00:35:10,100
a hippocratic oath for data workers.

540
00:35:10,400 --> 00:35:14,570
So you could imagine a world where
anyone working with data took an oath to

541
00:35:14,571 --> 00:35:19,190
protect the data of its users. So if
you were going to develop such a thing,

542
00:35:19,400 --> 00:35:22,090
what commitments would it include?
Uh,

543
00:35:22,340 --> 00:35:26,360
what rituals would be associated with
taking the oath in the first place.

544
00:35:26,960 --> 00:35:31,250
It's actually a beautiful ceremony for
civil engineers in Canada called the

545
00:35:31,790 --> 00:35:35,690
order of the iron ring. And when you
become a civil engineer in Canada,

546
00:35:35,691 --> 00:35:40,691
you're presented a ring that
is apocryphally made from
the metal of a collapsed

547
00:35:41,311 --> 00:35:44,940
bridge and you're meant to wear
the ring on your right hand.

548
00:35:44,941 --> 00:35:45,900
You're drafting hand.

549
00:35:45,901 --> 00:35:48,660
So that all along the way through
your work as a civil engineer,

550
00:35:48,661 --> 00:35:53,220
you keep in mind the ethical
responsibility to the
people who will be crossing

551
00:35:53,221 --> 00:35:55,020
your bridges are living in your buildings.

552
00:35:55,250 --> 00:35:58,800
And so you can imagine building rituals
like this for a hippocratic oath for

553
00:35:58,801 --> 00:36:02,090
data scientists or data
workers more generally. Um,

554
00:36:02,700 --> 00:36:05,520
second tech companies
have bounties for bugs.

555
00:36:05,970 --> 00:36:09,480
If you find a bug for Microsoft,
they will pay you to have found that bug.

556
00:36:09,690 --> 00:36:14,640
Could you imagine actually building
ethical bounties as well for identifying

557
00:36:14,790 --> 00:36:17,800
risks in these risk zones or any others?
Um,

558
00:36:18,120 --> 00:36:22,390
how would somebody claimed that bounty?
What, what should they be paid? Um,

559
00:36:22,710 --> 00:36:26,850
and actually who in the company would be
responsible for assessing whether that

560
00:36:26,851 --> 00:36:30,930
risk identified was actually a risk
than needed to be mitigated in the first

561
00:36:30,931 --> 00:36:31,710
place.

562
00:36:31,710 --> 00:36:36,710
You could imagine also a
list for employees within
a company of red flags that

563
00:36:37,110 --> 00:36:41,250
absolutely required running up the
flagpole or pulling the metaphorical red

564
00:36:41,251 --> 00:36:45,660
handle. Um, so what could go on that list?

565
00:36:45,780 --> 00:36:50,370
Who would create an update it and
would it be shared across the company?

566
00:36:50,730 --> 00:36:54,500
Would it be public? Would
it be proprietary? What each
company have it, you know,

567
00:36:54,510 --> 00:36:57,540
these are questions to think about
as we think about resilience building

568
00:36:57,541 --> 00:37:02,140
actually has a system of companies and
product makers and developers, et cetera.

569
00:37:02,550 --> 00:37:05,310
And the last idea we had
was just, um, you know,

570
00:37:05,311 --> 00:37:09,960
creating an actual licensure for
technologists. So, you know, you could,

571
00:37:10,050 --> 00:37:14,820
as lawyers and doctors and a whole bunch
of other professions have licensed to

572
00:37:14,821 --> 00:37:17,160
operate and actually if
you behave unethically,

573
00:37:17,161 --> 00:37:21,300
you can be stripped of that license
and no longer be able to access the

574
00:37:21,301 --> 00:37:23,340
privileges of that profession.

575
00:37:23,520 --> 00:37:28,110
Could you develop a similar thing for
technologists? Um, and if you did,

576
00:37:28,440 --> 00:37:31,800
what kinds of things would actually
cause you to lose a license, right?

577
00:37:31,860 --> 00:37:36,330
Medical malpractices medical malpractice
causes you to lose a license if you're

578
00:37:36,331 --> 00:37:39,780
a doctor, what would cause you to
lose your license as a technologist?

579
00:37:41,340 --> 00:37:45,350
So all of this to wrap up is,
uh,

580
00:37:45,740 --> 00:37:50,550
a way of thinking about what's
hot and what's not in ethics.

581
00:37:50,720 --> 00:37:54,810
Um, with credit to my friend Jake
Metcalf for this, for this list. Um,

582
00:37:54,811 --> 00:37:58,410
so what's hot, show us your work. Uh,

583
00:37:58,740 --> 00:38:03,030
show us how you got to this decision with
transparency and what's not as follow

584
00:38:03,031 --> 00:38:05,550
our principles blindly,
right?

585
00:38:05,551 --> 00:38:09,810
What's hot is global adaptability and
interoperability of platforms as opposed

586
00:38:09,811 --> 00:38:14,490
to adherence to western Canon only as
thinking about ethics and instead of

587
00:38:14,491 --> 00:38:15,990
thinking about western Canon of ethics,

588
00:38:15,991 --> 00:38:18,090
thinking about societal
impact more broadly,

589
00:38:18,870 --> 00:38:22,800
what's hot is defining the due diligence,
that process of discernment,

590
00:38:22,801 --> 00:38:26,310
the process of thinking through those
consequences first, second and third,

591
00:38:26,311 --> 00:38:30,720
order what's not as an
inflexible framework that
doesn't allow you to spot those

592
00:38:30,721 --> 00:38:35,100
risks in the first place. Uh,
not hot as close community.

593
00:38:35,440 --> 00:38:39,100
Thinking about these things in silos.
What is hot is open source,

594
00:38:39,130 --> 00:38:42,760
open dialogue tools that enable
us all to get better at this.

595
00:38:42,761 --> 00:38:47,590
And the first place, uh,
hot ethics is a process. Um,

596
00:38:47,710 --> 00:38:52,510
it is actually labor. It requires
sustained effort to identify, track,

597
00:38:52,511 --> 00:38:56,710
mitigate and improve the consequences
for the things that we value as human

598
00:38:56,711 --> 00:38:58,630
beings.
And it is not PR,

599
00:38:59,320 --> 00:39:03,730
it actually matters because it impacts
people in their lives. And lastly,

600
00:39:03,910 --> 00:39:08,650
what is hot is actually, you know, ethics
doing this work as assignable tasks,

601
00:39:09,280 --> 00:39:12,700
right?
You can be the owner of this on your team.

602
00:39:12,970 --> 00:39:16,630
It can be parceled out by a
product manager, by a team manager,

603
00:39:16,631 --> 00:39:21,070
etc. With responsibilities for different
people to spot and bring these risks to

604
00:39:21,071 --> 00:39:21,904
the table.

605
00:39:21,970 --> 00:39:26,620
And what is not hot about doing this is
where ethics is a black hole expense,

606
00:39:27,000 --> 00:39:31,180
uh, where there is no accountability
built into the framework. Uh,

607
00:39:31,210 --> 00:39:35,410
finally one last thing. I'm going
to make a plug for a new podcast,

608
00:39:35,411 --> 00:39:39,610
which if you haven't listened to it, you
should. It's hosted by Caterina fake,

609
00:39:39,760 --> 00:39:44,760
who was the co founder of flicker and
has since become a venture capitalist.

610
00:39:44,920 --> 00:39:49,130
She has a new podcast called
should this exist, which, uh,

611
00:39:49,150 --> 00:39:53,800
I asked a lot of these same questions of
early stage entrepreneurs and inventors,

612
00:39:53,990 --> 00:39:54,310
uh,

613
00:39:54,310 --> 00:39:58,540
and startup founders where she brings
them on with a new product idea onto our

614
00:39:58,541 --> 00:40:00,100
show and ask these questions.

615
00:40:00,280 --> 00:40:03,040
Is the product of your building
fundamentally good for people?

616
00:40:03,100 --> 00:40:07,300
What negative consequences and unintended
impacts might you imagine about this

617
00:40:07,301 --> 00:40:11,440
product? How can you mitigate them? Have
you thought about all of these first,

618
00:40:11,441 --> 00:40:16,410
second, third order consequences? So
that's at the core, the ethical ols.

619
00:40:16,600 --> 00:40:21,600
It's a way of bringing foresight about
unintended consequences or unanticipated

620
00:40:22,211 --> 00:40:26,920
consequences or consequences actually
that you probably could have foreseen if

621
00:40:26,921 --> 00:40:30,760
you'd ask the right sets of questions
in the first place to the activity of

622
00:40:30,761 --> 00:40:35,761
building something new and to the activity
of adding a feature to the activity

623
00:40:36,551 --> 00:40:41,551
of building new code to the activity of
at fundamentally building things that we

624
00:40:42,431 --> 00:40:44,230
imagine will be good for people.

625
00:40:44,380 --> 00:40:48,850
But we might be not imagining the ways
that it might be bad as well to put that

626
00:40:48,851 --> 00:40:52,840
glass half empty.
So with that,

627
00:40:53,110 --> 00:40:56,530
I will, uh, take it to questions.

628
00:40:57,350 --> 00:41:00,830
So thank you so much for coming.
This has been such an enlightening talk.

629
00:41:00,980 --> 00:41:05,840
I noticed that you, it's this product
seems to generally be based for futures,

630
00:41:05,900 --> 00:41:07,970
uh, events, our future occurrences.

631
00:41:08,180 --> 00:41:11,780
Could you reapply these to current
companies now and kind of revamp this way,

632
00:41:11,781 --> 00:41:13,670
their system's flowing now,
do you think?

633
00:41:14,160 --> 00:41:15,330
Yeah,
I think absolutely.

634
00:41:15,331 --> 00:41:20,160
I think at the core of these questions
are meant to identify negative impacts,

635
00:41:20,370 --> 00:41:20,671
right?

636
00:41:20,671 --> 00:41:24,000
And you could absolutely be asking the
same set of questions about a current

637
00:41:24,001 --> 00:41:26,880
existing product.
How is it doing this now?

638
00:41:27,360 --> 00:41:30,960
Are we building a product that
currently is being used in this way?

639
00:41:30,961 --> 00:41:35,750
And how might we actually just mitigate
the current risks, not just, uh,

640
00:41:35,810 --> 00:41:37,970
anticipate those future risks.

641
00:41:38,120 --> 00:41:41,600
So I think it absolutely has application
for current products. And again,

642
00:41:41,810 --> 00:41:43,340
most products,
uh,

643
00:41:43,370 --> 00:41:46,910
living in the world today go
through multiple iterations.

644
00:41:47,330 --> 00:41:50,240
There's very few products
that ship and are done.

645
00:41:50,840 --> 00:41:55,160
And so with each iteration, with each
new feature, with each tweak to the code,

646
00:41:55,161 --> 00:42:00,161
with each tweak to the platform as we go
through the evolution of a product too,

647
00:42:00,350 --> 00:42:04,780
it's also a useful tool. Even along the
way there, I would say, uh, that it's not,

648
00:42:05,020 --> 00:42:07,790
and this is not meant to
be a one and done activity.

649
00:42:08,300 --> 00:42:11,510
It's not meant to just be asked
one time and then say, okay,

650
00:42:11,511 --> 00:42:14,600
well we asked the set of questions and
now we don't have to worry about it.

651
00:42:14,601 --> 00:42:18,680
But building that process of discernment
that all along the way we're asking

652
00:42:18,681 --> 00:42:21,600
these questions with frequency,
uh,

653
00:42:21,620 --> 00:42:25,580
that we're able to spot and great
if we answer no, nothing's changed.

654
00:42:25,610 --> 00:42:28,210
And that risk that we anticipated has,

655
00:42:28,550 --> 00:42:32,300
is no longer an issue and it continues
in long no longer be an issue.

656
00:42:32,301 --> 00:42:34,250
But at the point at which
it becomes an issue, again,

657
00:42:34,340 --> 00:42:37,700
let's surface it and let's think
about ways to change the behavior,

658
00:42:37,701 --> 00:42:40,130
change the product,
changed the way we're doing business.

659
00:42:41,430 --> 00:42:44,220
So do you know of any companies
that have already started this?

660
00:42:44,460 --> 00:42:46,410
Anyone reported back to you saying,
yes,

661
00:42:46,411 --> 00:42:50,040
we've had a week long meeting to
go through all of these steps.

662
00:42:50,900 --> 00:42:53,190
We do know companies that are using this.
Uh,

663
00:42:53,390 --> 00:42:58,280
and we have actually also had some
degree of success embedding this in

664
00:42:58,400 --> 00:43:01,180
incubators and accelerators as well.
Uh,

665
00:43:01,290 --> 00:43:06,290
so we're able to sort of get a class
of early stage founders are startups

666
00:43:07,551 --> 00:43:10,520
thinking about these
issues from the outset. Um,

667
00:43:10,730 --> 00:43:14,270
I can't speak specifically
about any particular companies.

668
00:43:14,630 --> 00:43:19,280
A lot of companies don't necessarily want
to be public about their use of tools

669
00:43:19,281 --> 00:43:20,060
like this.

670
00:43:20,060 --> 00:43:25,060
I think that's becoming less the case
as more companies are embracing their

671
00:43:25,730 --> 00:43:30,380
responsibility and their ethics. Uh,
as someone I heard recently said,

672
00:43:30,381 --> 00:43:33,450
ethics is now the hottest
product in Silicon Valley. Um,

673
00:43:33,530 --> 00:43:37,040
so I think a lot of people are actually
starting to be proud of their use of

674
00:43:37,041 --> 00:43:40,070
tools like this and not shy
away from it. But yeah, it's,

675
00:43:40,160 --> 00:43:43,790
this is definitely in process and if not
this other frameworks for having these

676
00:43:43,791 --> 00:43:44,930
conversations,
right?

677
00:43:44,931 --> 00:43:48,680
There's like a design agency in
Seattle called artefact group,

678
00:43:48,890 --> 00:43:52,730
which design Taro cards of tech,
which is a card deck,

679
00:43:52,731 --> 00:43:54,890
which actually asks a lot
of the same questions.

680
00:43:55,070 --> 00:43:58,910
And so really just thinking about what
are the tools for asking these questions

681
00:43:58,911 --> 00:44:00,460
and it doesn't need to be ethical.
Oh,

682
00:44:00,540 --> 00:44:04,240
these questions could also just be
asked by any buddy on the team who's

683
00:44:04,250 --> 00:44:07,910
empowered to do so. And
team leader, any manager,

684
00:44:07,940 --> 00:44:11,890
anybody at any level should be
able to ask these questions. Um,

685
00:44:11,960 --> 00:44:14,060
whether through ethical oh
or just because they care.

686
00:44:16,320 --> 00:44:18,390
So ethics is not an exact science.

687
00:44:18,900 --> 00:44:22,920
How do you decide what is termed ethical
and not ethical? Largely opinion. Yeah.

688
00:44:24,520 --> 00:44:26,970
Yeah, I get that question a lot. Um,

689
00:44:28,860 --> 00:44:32,820
so if it's not to use the term ethics,

690
00:44:32,940 --> 00:44:36,900
I say do away with the term
ethics. Um, because at the core,

691
00:44:37,110 --> 00:44:41,430
which I hope came through
in this presentation, when
we're talking about ethics,

692
00:44:41,431 --> 00:44:43,980
I'm talking less about,
uh,

693
00:44:44,100 --> 00:44:48,660
sort of structured and rigorous
frameworks for Western ethics, right?

694
00:44:48,690 --> 00:44:53,280
Deontological versus consequentialist
versus utilitarian frameworks,

695
00:44:53,430 --> 00:44:56,160
dissecting cons versus
mill. And really, right.

696
00:44:56,760 --> 00:45:00,210
I don't find that exercise
particularly helpful actually.

697
00:45:00,360 --> 00:45:03,990
I find the exercise of thinking about
ethics as this responsibility for

698
00:45:03,991 --> 00:45:07,530
mitigating harm as the core activity of,
of,

699
00:45:07,730 --> 00:45:12,000
of doing ethical thinking.
Right? So, um, in that case,

700
00:45:12,150 --> 00:45:17,150
I think that that issue of
ethical relativism becomes
less of an issue because

701
00:45:18,900 --> 00:45:23,160
really if we're asking these specific
questions who is negatively impacted by

702
00:45:23,161 --> 00:45:26,940
the tech and you don't need
to care, you could say, right,

703
00:45:27,180 --> 00:45:30,790
we know that some people are being
negatively impacted by this. And,

704
00:45:30,940 --> 00:45:35,540
and we're okay with that. That's
fine. The only thing I think, uh,

705
00:45:35,640 --> 00:45:39,240
I'm advocating through the use of a tool
like this is go through the process,

706
00:45:39,780 --> 00:45:44,550
ask the questions, answer
them thoughtfully, discern
the potential outputs,

707
00:45:44,670 --> 00:45:47,070
outcomes, impacts, and then say,

708
00:45:47,580 --> 00:45:51,660
how do we trade off our values
against those potential futures?

709
00:45:51,900 --> 00:45:55,620
And those values get to be defined by
you, by your team, by your company,

710
00:45:55,920 --> 00:45:59,910
follow your values. Those don't
need to be defined by anyone else.

711
00:46:00,030 --> 00:46:02,730
Those you get to define and
then make the tradeoffs.

712
00:46:02,790 --> 00:46:07,080
Once you've asked the questions between
the impacts you might that might occur,

713
00:46:07,180 --> 00:46:07,890
uh,

714
00:46:07,890 --> 00:46:11,790
or the ways your tech is behaving or the
ways that users are using your product,

715
00:46:11,791 --> 00:46:16,410
et cetera. And say, how does, how do we
square our values with those impacts?

716
00:46:17,650 --> 00:46:20,640
Um, what, what comes next? I think, uh,

717
00:46:20,880 --> 00:46:25,130
for Omidyar and beyond the Escalade,
this toolkit can,

718
00:46:25,370 --> 00:46:29,640
do you have any comments about what
you guys have in the pipe? Yeah,

719
00:46:29,670 --> 00:46:32,790
so I think at the core there's a,

720
00:46:33,740 --> 00:46:38,740
a need more broadly for
ethical infrastructure,

721
00:46:39,270 --> 00:46:43,620
if you will, that actually supports
this activity in the first place. Uh,

722
00:46:43,650 --> 00:46:48,650
if you plant a healthy tree
in a withering orchard,

723
00:46:50,080 --> 00:46:50,460
uh,

724
00:46:50,460 --> 00:46:55,460
it's unlikely that that healthy tree will
resurrect all of the dead trees in the

725
00:46:55,651 --> 00:46:56,310
orchard.

726
00:46:56,310 --> 00:47:00,810
And it's unlikely that that a healthy
tree will flourish if the soil is

727
00:47:00,811 --> 00:47:02,340
poisonous.
Right?

728
00:47:02,341 --> 00:47:07,341
So building ethical infrastructure and
ethical culture is a necessary piece of

729
00:47:09,061 --> 00:47:13,140
the puzzle that supports the use of a
thing like ethical Oh, assets scale.

730
00:47:13,890 --> 00:47:17,970
Right? So I think one of the things we're
trying to puzzle through is how do you

731
00:47:17,971 --> 00:47:22,971
actually build the kinds of things
that scaffold ethics all along the way,

732
00:47:24,570 --> 00:47:24,841
right?

733
00:47:24,841 --> 00:47:29,841
So you've got training of
future technologists and
how are they exposed to the

734
00:47:31,811 --> 00:47:34,210
behaviors and mindsets of
asking these questions?

735
00:47:34,300 --> 00:47:37,600
Actually all along the way as
they learn the activity of coding.

736
00:47:37,601 --> 00:47:39,550
If they're computer
scientists or engineers,

737
00:47:39,760 --> 00:47:44,760
how are employees onboarded and trained
once they enter a company to think

738
00:47:45,011 --> 00:47:49,120
through these issues and then what kinds
of supports exist for them to raise red

739
00:47:49,121 --> 00:47:54,070
flags. Are there chief ethics officers
are ethical ombudsmen that support that

740
00:47:54,071 --> 00:47:57,580
activity.
How do venture capitalist funders,

741
00:47:57,581 --> 00:48:02,581
institutional or otherwise set KPI's
and metrics for companies that actually

742
00:48:03,311 --> 00:48:08,311
support their activity of doing ethics
better and having more mission aligned in

743
00:48:08,591 --> 00:48:12,790
values driven companies in the first place
rather than just incentivizing growth

744
00:48:12,791 --> 00:48:14,410
overall else.
Um,

745
00:48:14,530 --> 00:48:18,970
what kinds of consumer awareness needs
to be built in order for consumers to

746
00:48:18,971 --> 00:48:22,930
have more active voice in the
kinds of products they want to use?

747
00:48:23,020 --> 00:48:26,800
How our employee voices harness on
the issues that employees care about.

748
00:48:26,890 --> 00:48:31,780
We've seen a tremendous amount of employee
activism recently in companies large

749
00:48:31,781 --> 00:48:34,990
and small about the kinds of
ethical issues that they see.

750
00:48:35,530 --> 00:48:39,070
All of that speaks to kind of ethical
culture and ethical infrastructure.

751
00:48:39,280 --> 00:48:41,890
And we're thinking about
that broad echo system.

752
00:48:41,891 --> 00:48:45,460
So not just tools like ethical OSCP,
which is a really important piece.

753
00:48:45,640 --> 00:48:47,950
And tools are absolutely necessary,
but also what is,

754
00:48:48,010 --> 00:48:50,890
what are the supporting mechanisms
to enable that behavior?

755
00:48:51,660 --> 00:48:55,680
Uh, just as a followup to that question
regarding the infrastructure needed. Um,

756
00:48:55,740 --> 00:48:59,460
I guess like you're in the u s there's
probably a lot of skepticism with regard

757
00:48:59,461 --> 00:49:04,140
to the ability of maybe like our federal
government to be able to provide that

758
00:49:04,141 --> 00:49:05,100
sort of infrastructure.

759
00:49:05,490 --> 00:49:10,490
So to what extent are you
optimistic regarding our
ability to actually put those

760
00:49:10,531 --> 00:49:13,890
mechanisms in place?
And if that doesn't happen,

761
00:49:14,040 --> 00:49:19,040
then to what extent are you optimistic
that the market or you know,

762
00:49:19,440 --> 00:49:21,810
some other actors can do it in that,
in the absence of that,

763
00:49:22,280 --> 00:49:27,140
I am not necessarily optimistic about our
government catching up. I mean, if you,

764
00:49:27,141 --> 00:49:30,680
if you see the recent tech hearings,
uh,

765
00:49:31,490 --> 00:49:36,490
unfortunately our members of Congress do
not demonstrate a extremely high level

766
00:49:37,431 --> 00:49:41,270
of proficiency or fluency around products
and the way they are built in their

767
00:49:41,271 --> 00:49:43,250
impacts on people.
Uh,

768
00:49:43,580 --> 00:49:47,930
and yet we are seeing this
really interesting trend
of now some large companies

769
00:49:47,931 --> 00:49:51,710
actually asking for regulation
of certain things. Now,

770
00:49:52,070 --> 00:49:54,680
there are lots of reasons
they might be doing that. Uh,

771
00:49:54,860 --> 00:49:59,360
but I think there is a growing awareness
that regulation will probably be

772
00:49:59,361 --> 00:50:03,710
necessary and I am not bullish on the
idea that that will catch up very quickly

773
00:50:03,920 --> 00:50:04,301
knowing.

774
00:50:04,301 --> 00:50:08,870
So unfortunately the dysfunction of our
current government and the system of

775
00:50:08,871 --> 00:50:10,100
passing legislation,

776
00:50:10,310 --> 00:50:13,880
I think at the local level it's
actually probably more likely, um,

777
00:50:13,910 --> 00:50:18,710
at the city and state level we might see
more interesting regulation coming out,

778
00:50:18,770 --> 00:50:21,830
especially from California and a few
other places that are thinking about these

779
00:50:21,831 --> 00:50:24,920
things. Um, if at the core
of your question is about,

780
00:50:24,950 --> 00:50:28,910
so then to what degree will companies
regulate as it's called, right?

781
00:50:28,911 --> 00:50:30,890
In order to do this better,

782
00:50:30,891 --> 00:50:34,970
if the government can't actually assert
itself in terms of legislation and

783
00:50:34,971 --> 00:50:37,410
regulation of these things? I think, um,

784
00:50:37,640 --> 00:50:41,990
there is a competitive advantage to be
built from doing this particularly well

785
00:50:41,991 --> 00:50:46,380
and building user trust
around security and data, uh,

786
00:50:46,530 --> 00:50:51,470
about thinking responsibly about the
products that are built. Um, you know, I,

787
00:50:51,560 --> 00:50:56,560
I often times come back to the example
in the automobile industry of Volvo,

788
00:50:57,700 --> 00:51:01,100
uh, which created the safe,
the safest car on the road,

789
00:51:01,400 --> 00:51:05,840
and every person who was going to put
their teenager into a car new to get them

790
00:51:05,841 --> 00:51:09,560
a Volvo because it was destined to crash
at some point and you're just wanted to

791
00:51:09,561 --> 00:51:10,940
protect your kid.
Right?

792
00:51:11,090 --> 00:51:15,050
And so it was actually
competitive advantage to be
built around being the safest

793
00:51:15,051 --> 00:51:18,480
car on the road. And
so I think that we, uh,

794
00:51:18,550 --> 00:51:22,610
I am perhaps naively optimistic that
we may be moving in a direction for the

795
00:51:22,611 --> 00:51:23,750
tech industry as well,

796
00:51:23,810 --> 00:51:27,710
where there is a competitive advantage
to be built based on doing this. Well,

797
00:51:27,920 --> 00:51:32,630
and in that case, I think that we may see
a race to the top before it. Um, again,

798
00:51:32,700 --> 00:51:33,620
that might be naive,

799
00:51:33,770 --> 00:51:38,450
but you asked me to my degree of optimism
and so I will say that I am naively

800
00:51:38,451 --> 00:51:41,740
optimistic about it. Thank
you very much. You have for,

801
00:51:41,790 --> 00:51:45,830
for coming today and speak and thank you
for your time and for taking all these

802
00:51:45,831 --> 00:51:47,990
questions. Thanks for
having me. Appreciate it.

