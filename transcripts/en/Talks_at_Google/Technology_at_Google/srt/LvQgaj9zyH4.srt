1
00:00:05,530 --> 00:00:09,260
Thanks again for coming and please join
me as we give a warm welcome to Kate on.

2
00:00:09,270 --> 00:00:10,103
Yo.

3
00:00:15,210 --> 00:00:19,240
Thank you Tom. Thank you
everyone. Hello, fellow humans.

4
00:00:20,710 --> 00:00:25,060
Wait, actually, I guess I should check
and make sure. Are there any robots here?

5
00:00:25,950 --> 00:00:30,550
Any robots? I raise your hand if
you're a robot in the audience. Yeah,

6
00:00:31,530 --> 00:00:34,590
I don't see any, so I think we're
safe to proceed. But you know,

7
00:00:34,591 --> 00:00:37,590
I asked this question every once in a
while cause I figured one of these days

8
00:00:37,980 --> 00:00:41,670
there's going to be kind of
a little spindly mechanical
arm that comes up when I

9
00:00:41,671 --> 00:00:42,504
asked that question.

10
00:00:43,110 --> 00:00:47,040
I'm not really sure what I'm supposed
to do at that moment if I'm supposed to

11
00:00:47,041 --> 00:00:49,620
invite the robot,
come to come up here and take my job.

12
00:00:49,740 --> 00:00:53,580
Because that's kind of how we talk
about robots and automation and AI and

13
00:00:53,581 --> 00:00:55,980
everything these days
is like with this fear,

14
00:00:55,981 --> 00:01:00,900
this dread about what it's going to
mean for human jobs, for humanity,

15
00:01:00,901 --> 00:01:04,650
for kind of existential
reality as we know it.

16
00:01:05,490 --> 00:01:10,490
So my premise has been to think about
how we can make technology better for

17
00:01:12,301 --> 00:01:13,890
humanity,
better for our future.

18
00:01:14,190 --> 00:01:17,970
And of course better for serving
business purposes. But in doing so,

19
00:01:17,971 --> 00:01:20,680
I think we have to start
back from one of the square,

20
00:01:20,690 --> 00:01:21,930
sort of one of the foundations.

21
00:01:21,931 --> 00:01:25,170
And that is to think about
what it is that makes us human.

22
00:01:25,560 --> 00:01:27,340
What is it that makes humans human?

23
00:01:27,341 --> 00:01:32,010
So I'll ask you to indulge me and
just think for a moment of a word or a

24
00:01:32,011 --> 00:01:36,990
characteristic that you feel like
really captures the human experience.

25
00:01:36,991 --> 00:01:41,880
Like what one characteristic is it? And
I won't have you call it out or anything,

26
00:01:41,881 --> 00:01:43,560
but just hold it in
your head for a moment.

27
00:01:44,190 --> 00:01:48,210
What do you feel like it is
that really makes humans human?

28
00:01:49,680 --> 00:01:50,513
And so let me ask,

29
00:01:50,700 --> 00:01:54,510
how many of you by a show of hands
thought of something like creativity or

30
00:01:54,511 --> 00:01:59,180
problem solving or innovation or
something like that? Anyone who one? Okay.

31
00:01:59,220 --> 00:02:03,710
A couple people in the room. Good. Okay.
So that's a pretty common answer. Um,

32
00:02:03,720 --> 00:02:05,850
how about,
this is a more common answer,

33
00:02:05,851 --> 00:02:10,590
I think empathy or love or
compassion? Anyone? Yeah. Okay.

34
00:02:10,591 --> 00:02:13,380
A few more hands.
Those are both great.

35
00:02:13,470 --> 00:02:17,250
I think great characteristics and
admirable qualities of humans.

36
00:02:17,610 --> 00:02:21,510
I didn't necessarily specify that these
needed to be uniquely human attributes.

37
00:02:21,511 --> 00:02:23,160
But if you think,
if we think about those,

38
00:02:23,370 --> 00:02:25,710
they don't feel like they
are uniquely human, right?

39
00:02:25,711 --> 00:02:30,390
Like we've seen creativity and problem
solving and nonhuman animals like Otters,

40
00:02:30,570 --> 00:02:30,931
Bang,

41
00:02:30,931 --> 00:02:35,931
Mollis fun rocks to open them and Ravens
use tools and we've seen a compassion

42
00:02:36,001 --> 00:02:40,020
and love from a elephants and
dogs and other other species.

43
00:02:40,050 --> 00:02:43,380
So we know that those are
exhibited by other animals.

44
00:02:43,410 --> 00:02:47,220
And I don't think it's too far fetched
to imagine that in the not too distant

45
00:02:47,221 --> 00:02:47,551
future,

46
00:02:47,551 --> 00:02:52,290
we might see at least
superficial indications of
machines exhibiting those kinds

47
00:02:52,291 --> 00:02:54,360
of qualities in their behavior,

48
00:02:54,370 --> 00:02:58,380
interactions with humans and maybe
even eventually everyone machines,

49
00:02:58,560 --> 00:03:01,420
which will very interesting
at a surface level.

50
00:03:02,860 --> 00:03:03,611
But how many of you,

51
00:03:03,611 --> 00:03:08,611
when you think about what that most
human of characteristics is thought of

52
00:03:08,801 --> 00:03:12,970
checking a box?
Anyone by show of hands?

53
00:03:14,020 --> 00:03:17,800
Of course you didn't cause it's absurd.
But this is the sort of premise.

54
00:03:17,801 --> 00:03:21,910
The problem that we encounter
in technology a lot of
the time is that we don't

55
00:03:21,911 --> 00:03:24,790
necessarily think through this kind
of foundational experience and we are

56
00:03:24,791 --> 00:03:29,380
presenting absurdities as if they
are sort of foundational truths. Uh,

57
00:03:29,381 --> 00:03:30,370
and besides which,

58
00:03:30,371 --> 00:03:34,000
if we were to try to claim that this
is a uniquely human characteristic,

59
00:03:34,420 --> 00:03:39,420
we get beat out anyway by machines
can also do this characteristic.

60
00:03:41,110 --> 00:03:41,501
Uh,

61
00:03:41,501 --> 00:03:46,501
so I don't know how many of you have seen
this little goofy guy but he's kind of

62
00:03:47,231 --> 00:03:48,064
fun.

63
00:03:53,050 --> 00:03:57,610
So I have come to be known as,
as Tim mentioned,

64
00:03:57,640 --> 00:04:00,190
the tech humanist and I take this,
uh,

65
00:04:00,191 --> 00:04:04,420
this sort of moniker a pretty seriously
cause I feel like there is this,

66
00:04:04,450 --> 00:04:09,450
this area around which technology
does have the capacity to solve human

67
00:04:10,001 --> 00:04:13,240
problems.
It also has the capacity to scale.

68
00:04:13,710 --> 00:04:17,560
Like we are experiencing automation
and AI and all kinds of other emerging

69
00:04:17,561 --> 00:04:22,390
technologies bringing scale to
the types of solutions we create.

70
00:04:22,600 --> 00:04:23,860
Like never before.

71
00:04:24,070 --> 00:04:28,390
And so I think it behooves us to really
think about what the human experience

72
00:04:28,391 --> 00:04:32,740
around that scale is going to be and what
the human experience around technology

73
00:04:32,741 --> 00:04:35,050
is going to be.
And how we can make technology better,

74
00:04:35,051 --> 00:04:39,670
solve business problems and solve
human problems at the same time.

75
00:04:40,180 --> 00:04:42,100
So as I talk about being a tech humanist,

76
00:04:42,101 --> 00:04:44,140
and as I think about
solving those challenges,

77
00:04:44,141 --> 00:04:48,220
I'm excited that you all are in this room
and on the live stream and watching on

78
00:04:48,221 --> 00:04:49,870
the video later. I hope, uh,

79
00:04:49,871 --> 00:04:53,950
and then I want to offer that
perhaps that that is also you,

80
00:04:53,980 --> 00:04:56,560
that you may be also our attack humanist.

81
00:04:56,561 --> 00:05:00,520
And I'd like to offer that term to
you so that when you see my book, uh,

82
00:05:00,550 --> 00:05:04,720
as Tim mentioned just came out
September 24th tech humanist, uh,

83
00:05:04,721 --> 00:05:09,520
that you will see that title and think
I'm describing you as well because that

84
00:05:09,521 --> 00:05:10,151
is the truth.

85
00:05:10,151 --> 00:05:15,151
I'd like to see us all sort of join hands
in this movement to create more human

86
00:05:15,191 --> 00:05:20,191
technology and more wide scale human
experiences that are more meaningful and

87
00:05:20,741 --> 00:05:24,280
more integrated and more dimensional
with the technology we create.

88
00:05:24,281 --> 00:05:29,281
So the premise there is how can we both
make technology better for business to

89
00:05:30,611 --> 00:05:35,590
solve business challenges and make it
better for humans? Uh, and I think that,

90
00:05:35,591 --> 00:05:38,860
that, that both and framing
is the key to the whole thing.

91
00:05:38,861 --> 00:05:41,050
We need to understand how to,
uh,

92
00:05:41,051 --> 00:05:44,020
accept that these things do
need to be integrated together.

93
00:05:44,680 --> 00:05:49,680
And I would propose that the way to
accomplish this at scale is to focus on

94
00:05:50,351 --> 00:05:54,670
creating more meaningful
human experiences at scale.

95
00:05:54,790 --> 00:05:59,420
So how do we focus on getting
meaning into the human experiences?

96
00:05:59,421 --> 00:06:03,290
So the way that that looks in the
model that I proposed is this,

97
00:06:03,530 --> 00:06:04,460
on one hand,

98
00:06:04,700 --> 00:06:08,900
how can we think about scaling
business meaningfully through data,

99
00:06:08,930 --> 00:06:10,730
through strategic
alignment and automation?

100
00:06:10,731 --> 00:06:14,480
How can we think about using the tools
at our disposal to make business more

101
00:06:14,481 --> 00:06:19,481
effective while also creating more
meaningful human experiences and scaling

102
00:06:20,181 --> 00:06:21,830
those through data and automation?

103
00:06:22,940 --> 00:06:26,750
I've had the opportunity to test this
idea with a lot of different companies

104
00:06:26,751 --> 00:06:28,940
that have consulted with,
spoken, with, advised,

105
00:06:28,941 --> 00:06:31,130
worked with on different
projects over the years.

106
00:06:31,400 --> 00:06:35,480
And I'm excited to say that it works in
almost every industry I've encountered.

107
00:06:35,720 --> 00:06:36,650
Uh, it, it's,

108
00:06:36,651 --> 00:06:40,850
it's sort of provides great results no
matter who you are or what you're trying

109
00:06:40,851 --> 00:06:45,620
to accomplish. Every company is
trying to achieve profit, right?

110
00:06:45,621 --> 00:06:49,190
Every company is trying to
achieve revenue based metrics in,

111
00:06:49,191 --> 00:06:53,000
in what they're going about.
Even if you're a nonprofit organization,

112
00:06:53,150 --> 00:06:56,990
you still have to be accountable to
some sort of profit and loss scenario.

113
00:06:56,991 --> 00:07:00,500
There's some sort of breakdown of the
financials that you need to be accountable

114
00:07:00,501 --> 00:07:04,730
for. And I'm happy to tell you that the
work of creating meaningful experiences

115
00:07:04,731 --> 00:07:08,120
actually does lead to
increased employee retention,

116
00:07:08,121 --> 00:07:11,060
decreased customer acquisition costs,
increased loyalty,

117
00:07:11,420 --> 00:07:15,980
and all kinds of other directional metrics
that lead to more profit. Of course,

118
00:07:15,981 --> 00:07:17,390
it's also the right thing to do.

119
00:07:17,391 --> 00:07:19,370
It also creates a better
experience for all of us.

120
00:07:19,371 --> 00:07:22,600
And I want everybody to be motivated
by, you know, kind of this, uh,

121
00:07:22,760 --> 00:07:26,600
this aesthetic of wanting the world to
be better and creating more meaningful

122
00:07:26,601 --> 00:07:30,380
experiences being its own end.
But if we have to be motivated by profit,

123
00:07:30,381 --> 00:07:32,960
we can be.
And that's all a good thing too.

124
00:07:33,560 --> 00:07:35,390
So let's unpack this just a little bit.

125
00:07:35,391 --> 00:07:40,391
What I mean when I talk about creating
meaningful human experiences at scale,

126
00:07:41,000 --> 00:07:45,380
what does that entail? So first let's
think about what meaningful really is.

127
00:07:45,950 --> 00:07:50,630
So the example of the, uh, click
the box to confirm your humanity.

128
00:07:51,140 --> 00:07:51,711
I mentioned,

129
00:07:51,711 --> 00:07:56,711
I think that that's an absurd example
and they had this kind of running a hobby

130
00:07:57,141 --> 00:08:01,820
of appreciating the tension between
meaning and absurdity in the world.

131
00:08:02,150 --> 00:08:04,850
But I feel like anywhere
there is a lack of meaning,

132
00:08:05,060 --> 00:08:08,450
it sort of opens up this void
into which absurdity can flow.

133
00:08:09,020 --> 00:08:11,720
So where we don't create enough meaning,

134
00:08:11,900 --> 00:08:16,850
where are we don't describe enough
meaning we allow absurdity to flourish.

135
00:08:17,210 --> 00:08:20,690
So there's enough opportunity
for that in technology as it is.

136
00:08:20,691 --> 00:08:24,230
And business really. I think you
all probably have this experience.

137
00:08:24,231 --> 00:08:27,440
I'm guessing that there are areas where,

138
00:08:28,040 --> 00:08:32,690
let's say you talk about work things in
ways that you wouldn't talk about with

139
00:08:32,691 --> 00:08:33,980
your friends outside of work.

140
00:08:34,020 --> 00:08:38,030
You use language or terminology that
your friends who don't work with you

141
00:08:38,430 --> 00:08:40,490
wouldn't, would not understand, uh,

142
00:08:40,491 --> 00:08:45,380
or there are things that you do at work
that just don't, that are kind of like,

143
00:08:45,440 --> 00:08:48,650
that's the way we've always done it.
But anytime you think to yourself,

144
00:08:48,651 --> 00:08:49,940
this doesn't make sense,

145
00:08:50,240 --> 00:08:53,840
that's a really big clue because making
things make sense is what meaning does.

146
00:08:54,230 --> 00:08:58,520
So we have an opportunity to step back
and assess absurdity and recognize that

147
00:08:58,521 --> 00:09:03,090
that we can infuse meaning into those
structures and create more opportunity to

148
00:09:03,091 --> 00:09:07,650
avoid meaning. Two, to keep
away from meaning. So the,
the reason that that works,

149
00:09:07,651 --> 00:09:12,120
I believe is because humans crave meaning
more than any other characteristics.

150
00:09:12,121 --> 00:09:16,470
So if you were to ask me what I think
makes humans human, this is what it is,

151
00:09:16,680 --> 00:09:20,490
is that we seek meaning in,
in all areas of life.

152
00:09:20,491 --> 00:09:25,380
We are compelled by meaning. If you
offer us a meaningful answer or solution,

153
00:09:25,650 --> 00:09:30,420
we are compelled by it. How many of
you are a Douglas Adams fans? Anyone?

154
00:09:30,810 --> 00:09:31,770
A few?
Okay,

155
00:09:31,920 --> 00:09:36,220
so you already know where I'm going to
go with this in life and the hitchhiker's

156
00:09:36,240 --> 00:09:39,300
guide to the galaxy series.
The answer to the great question of life,

157
00:09:39,301 --> 00:09:42,960
the universe and everything
was 42 of course.

158
00:09:44,310 --> 00:09:48,780
So Douglas Adams wrote per
set in interviews that he
chose 42 because it was not

159
00:09:48,781 --> 00:09:51,840
too high and not too low of a number.
And because it was just funny and it is,

160
00:09:52,560 --> 00:09:54,880
but I don't know if you
know this, but in, in uh,

161
00:09:55,110 --> 00:09:59,280
on reddit you can find this and a few
other places a collected around the web,

162
00:09:59,281 --> 00:10:00,630
there are,
uh,

163
00:10:00,660 --> 00:10:05,270
kind of collections of
alternate explanations for
why 42 actually kind of makes

164
00:10:05,271 --> 00:10:09,900
sense as the explanation of meaning
in the world. So for example,

165
00:10:10,080 --> 00:10:13,170
there are 42 characters in the phrase,
it's the answer to life,

166
00:10:13,171 --> 00:10:17,370
the universe and everything. So, right.

167
00:10:17,700 --> 00:10:21,660
You're convinced now, right? No. Are also,
there's like 42 dots on a pair of dice.

168
00:10:22,320 --> 00:10:24,900
So that answers everything.
Uh,

169
00:10:24,930 --> 00:10:27,900
which I thought was just kind
of a throwaway explanation,
but my husband said,

170
00:10:27,901 --> 00:10:30,810
well, life has kind of like a roll of
the dice. So I thought, well, all right,

171
00:10:31,260 --> 00:10:33,300
fair enough. Others, this,

172
00:10:33,330 --> 00:10:38,330
my favorite of them is
this 42 is apparently the
unicode character can unicode

173
00:10:40,291 --> 00:10:44,190
value for the asterisk character,
which as you may know,

174
00:10:44,220 --> 00:10:47,070
is a wildcard symbol.
Often in computing,

175
00:10:47,400 --> 00:10:51,030
which means it can mean anything,
right?

176
00:10:53,710 --> 00:10:54,481
But okay,

177
00:10:54,481 --> 00:10:57,300
so in the end it's obviously it just a
coincidence because Douglas Adams didn't

178
00:10:57,301 --> 00:10:58,134
mean it that way.

179
00:11:03,110 --> 00:11:05,400
But that's the important point is that,
uh,

180
00:11:05,430 --> 00:11:09,780
even though Douglas Adams didn't mean it
that way and it is just a coincidence,

181
00:11:09,781 --> 00:11:12,870
it is an absurd and poetic
and beautiful coincidence,

182
00:11:13,260 --> 00:11:17,280
but we always make meaning the way
we have always done and always will,

183
00:11:17,370 --> 00:11:22,020
which is by a scribing
different significance to
different events based on how

184
00:11:22,021 --> 00:11:26,370
and how much we value them or in other
words, by making it up as we go along.

185
00:11:27,810 --> 00:11:32,160
And I think that's the encouraging thing
about this is that even though we talk

186
00:11:32,161 --> 00:11:37,161
about robots and automation and AI in
the broad mainstream in a scary way,

187
00:11:38,880 --> 00:11:43,020
what this suggests is that there is
this kind of open interpretation to the

188
00:11:43,021 --> 00:11:46,980
future. We get to make meaning
for the future as we go along,

189
00:11:46,981 --> 00:11:49,530
we get to decide the
future as we go along.

190
00:11:49,560 --> 00:11:53,010
You get to decide the
future as you go along.

191
00:11:53,890 --> 00:11:58,060
And that's really incredible because
right now the, the possibilities,

192
00:11:58,061 --> 00:12:02,620
the power of what's happening
within technology and
within the scale of emerging

193
00:12:02,621 --> 00:12:07,621
technologies means that we
have the capacity to create
the best futures for the

194
00:12:07,871 --> 00:12:11,320
most people. There's really
this Po, this potential,

195
00:12:11,321 --> 00:12:15,760
and I think even an ethical responsibility
to think about how solutions can

196
00:12:15,761 --> 00:12:18,220
scale to that sort of level.

197
00:12:20,470 --> 00:12:21,820
So let's go back to unpacking.

198
00:12:21,821 --> 00:12:25,330
Create meaningful human experiences
at scale and what is it that human

199
00:12:25,331 --> 00:12:27,190
experience is really describes.

200
00:12:30,040 --> 00:12:33,430
We talk a lot in business about
customer experience, user experience,

201
00:12:33,431 --> 00:12:36,970
or depending on your industry and maybe
patient or gastro visitor experience of

202
00:12:36,971 --> 00:12:37,870
student experience.

203
00:12:38,530 --> 00:12:43,530
We don't often in many industries talk
about human experience in this integrated

204
00:12:44,381 --> 00:12:48,190
way and this way that brings all of
those roles together and appreciates the

205
00:12:48,191 --> 00:12:50,650
fact that there is this kind of holistic,

206
00:12:51,040 --> 00:12:55,450
a human experience that that transcends
any of those roles that you are,

207
00:12:55,630 --> 00:12:58,990
we are all of those roles
at any given point in time.

208
00:12:59,410 --> 00:13:04,030
And so even though you may be performing
as a customer and a customer experience,

209
00:13:04,180 --> 00:13:06,760
you are still a human coming
into that customer experience.

210
00:13:07,320 --> 00:13:11,260
And the important thing about that is
the transcendent empathy that can come

211
00:13:11,261 --> 00:13:13,210
from understanding the baggage,

212
00:13:13,211 --> 00:13:16,180
the context that someone
brings to that interaction.

213
00:13:16,960 --> 00:13:20,080
So there's an opportunity to create
these more dimensional interactions,

214
00:13:20,081 --> 00:13:21,610
these more integrated than interactions.

215
00:13:21,610 --> 00:13:24,310
And so to create more
meaningful human interactions,

216
00:13:24,311 --> 00:13:28,570
it turns out we need to design
more integrated human experiences.

217
00:13:29,020 --> 00:13:32,920
You'd think about how to blend all of
those roles and understandings together

218
00:13:33,280 --> 00:13:35,560
and bring an understanding
of where someone has been,

219
00:13:35,561 --> 00:13:36,940
where are you meeting them in the world,

220
00:13:36,941 --> 00:13:41,470
and how you can create these kinds of
senses of dimension, uh, and wholesome.

221
00:13:42,310 --> 00:13:47,310
So I promised a venn diagram to a friend
earlier and I have it the best venn

222
00:13:48,701 --> 00:13:53,470
diagram in the entire world. I'm sure
you all have seen this. If you haven't,

223
00:13:53,710 --> 00:13:56,260
you'll want to rush out and
get this tee shirt right away.

224
00:13:56,620 --> 00:13:59,050
A tensile graphics
makes this Venn Diagram.

225
00:13:59,620 --> 00:14:04,180
But the illustration here really I think
gets at the point then when you think

226
00:14:04,181 --> 00:14:07,240
about what is possible on
one side of an equation,

227
00:14:07,241 --> 00:14:12,241
such as the best technology
or the technology to make
business better and what's

228
00:14:12,761 --> 00:14:14,380
possible and that other side of equation,

229
00:14:14,381 --> 00:14:18,100
like what's possible to make
technology better for humans.

230
00:14:18,670 --> 00:14:22,330
It's only by really thinking about the
intersection of those things that you

231
00:14:22,331 --> 00:14:26,890
really come at the best
solutions like platypus.

232
00:14:26,891 --> 00:14:30,460
Kutar or you don't get platypus Kitara
until you're doing some serious both

233
00:14:30,461 --> 00:14:33,250
ending. So that's, that's the opportunity.

234
00:14:33,251 --> 00:14:37,750
And really what we're talking about is
augmenting human experience with data and

235
00:14:37,751 --> 00:14:38,584
context.

236
00:14:38,680 --> 00:14:42,790
So the broader opportunity in a technology
science is to really think about

237
00:14:43,210 --> 00:14:45,460
where does somewhat, where are
you meeting someone in the world?

238
00:14:45,461 --> 00:14:48,820
What data do you have to understand
and appreciate where they come from,

239
00:14:48,821 --> 00:14:51,130
what their preferences are,
what their tastes are,

240
00:14:51,710 --> 00:14:54,020
and how can you create context that,

241
00:14:54,050 --> 00:14:57,860
that addresses the objectives that you
have as a business and the objectives

242
00:14:57,861 --> 00:15:02,000
they have as a human and the role that
you're meeting them in and the alignment

243
00:15:02,030 --> 00:15:05,540
of those objectives. Well, how can
we come at that in a way that, uh,

244
00:15:05,541 --> 00:15:06,710
that provides that?

245
00:15:07,220 --> 00:15:11,000
And so I actually kind of think of this
in a way as being meaning as a service

246
00:15:11,150 --> 00:15:11,983
in a sense.

247
00:15:12,160 --> 00:15:17,000
And so an opportunity to think about
offering up in meaningful construct that

248
00:15:17,001 --> 00:15:21,350
aligns your objective and their
objective and providing the, the uh,

249
00:15:21,351 --> 00:15:24,770
the hooks in a sense to be
able to expand upon that.

250
00:15:24,771 --> 00:15:26,870
And I really mean any meaning of meaning.

251
00:15:27,170 --> 00:15:30,380
So meaning as we talk about
it could be at any level.

252
00:15:30,381 --> 00:15:33,080
We talk about meaning as it
relates to communication.

253
00:15:33,710 --> 00:15:37,840
So I'm a linguist by education, so I
think about the semantic layer, you know,

254
00:15:37,850 --> 00:15:41,130
how we communicate with one another,
what we convey across our,

255
00:15:41,131 --> 00:15:44,620
our communications with one another. But
it can be all the way through. You know,

256
00:15:44,630 --> 00:15:47,720
you probably spend a lot of your time
if you do a lot of development or

257
00:15:47,721 --> 00:15:50,150
engineering in patterns and significance,

258
00:15:50,151 --> 00:15:52,910
that's probably a layer that
you spend a lot of time in.

259
00:15:53,120 --> 00:15:56,270
But it could be all the way out to sort
of the existential and cosmic layer.

260
00:15:56,271 --> 00:15:59,360
And what is it all about? Alfie,
you know, that sort of thing.

261
00:16:00,680 --> 00:16:03,560
I think in a sense it's almost
like Api thinking for everything.

262
00:16:03,561 --> 00:16:08,561
You can really sort of think about how
one idea integrates with another and how,

263
00:16:08,810 --> 00:16:11,060
what does it,
what does meaningful on one side,

264
00:16:11,090 --> 00:16:15,290
like the business side can be meaningful
on the human side of the equation.

265
00:16:15,320 --> 00:16:16,670
How do you provide,
you know,

266
00:16:16,671 --> 00:16:21,350
sorta hooks and intelligence across those
different parts of the experience and

267
00:16:21,351 --> 00:16:24,200
make sure that that meaning is being
transferred through that layer.

268
00:16:27,150 --> 00:16:32,150
The integration that most brought me to
this realization is thinking about how

269
00:16:32,551 --> 00:16:37,170
the design of experiences online now
regularly intersects with the design of

270
00:16:37,171 --> 00:16:41,820
experiences offline. But more and more
as we think about physical experiences,

271
00:16:41,821 --> 00:16:46,050
they come with some sort of digital
component or some sort of trackability or

272
00:16:46,051 --> 00:16:47,650
traceability with that,

273
00:16:47,720 --> 00:16:50,400
that physical experience or when
we think about digital experiences,

274
00:16:50,401 --> 00:16:53,970
we have to think about the physical
context somebody might be in as they

275
00:16:53,971 --> 00:16:57,540
encounter those interactions.
So I wrote about this in my last book,

276
00:16:57,541 --> 00:16:58,590
pixels in place.

277
00:16:59,160 --> 00:17:02,190
So thinking about things like the Internet
of things and wearables and beacons

278
00:17:02,191 --> 00:17:07,080
and sensors and all kinds of connected
smart devices and how are those bring

279
00:17:07,081 --> 00:17:10,620
that sort of connective layer
of between those two worlds.

280
00:17:10,621 --> 00:17:15,621
But the important point about that is
that just about everywhere interesting

281
00:17:16,440 --> 00:17:18,900
that the physical world
and digital world connect,

282
00:17:18,960 --> 00:17:21,960
that connection layer happens through us,
through humans,

283
00:17:21,961 --> 00:17:26,160
through our human experience. It's
our movements. That's our behavior.

284
00:17:26,640 --> 00:17:29,130
It's our patterns, it's
what we want, what we do,

285
00:17:29,310 --> 00:17:34,200
what we indicate that really
creates that connection. So again,

286
00:17:34,201 --> 00:17:38,010
it comes back to sort of thinking
about that integrated human experience.

287
00:17:38,460 --> 00:17:42,300
So I proposed this model
in pixels in place, uh,

288
00:17:42,301 --> 00:17:44,520
which is integrated
human experience design.

289
00:17:44,940 --> 00:17:49,080
Thinking about how to blend those
online and offline contexts,

290
00:17:49,440 --> 00:17:54,240
thinking about how to kind of come across
all the different levels and roles of

291
00:17:54,241 --> 00:17:55,830
humanity that you might encounter.

292
00:17:56,460 --> 00:17:59,430
Thinking about how to think about
experience in an integrated way,

293
00:17:59,431 --> 00:18:02,520
interactions and transactions across
all the different touch points that you

294
00:18:02,521 --> 00:18:03,354
might have.

295
00:18:03,750 --> 00:18:07,590
And know it the way I'm
defining the word design,

296
00:18:08,250 --> 00:18:12,210
which is the adaptive execution
of strategic intent. So you know,

297
00:18:12,211 --> 00:18:14,220
you have an intention,

298
00:18:14,221 --> 00:18:19,221
you have a purpose to what you're trying
to do with any given design initiative

299
00:18:19,770 --> 00:18:23,790
and you know that you're going to probably
not get it exactly where you want it

300
00:18:23,791 --> 00:18:25,110
to be on the first go.

301
00:18:25,680 --> 00:18:29,280
So we need to build an adaptive
iterative process to this.

302
00:18:29,910 --> 00:18:33,360
And the more we do this around a
framework of creating that meaningful

303
00:18:33,361 --> 00:18:38,361
interaction and that
dimensional relationship between
business entity and human

304
00:18:39,481 --> 00:18:41,250
that's consuming that experience,

305
00:18:41,400 --> 00:18:44,550
the more we stand a chance of conveying
some sort of meaningful truth.

306
00:18:45,510 --> 00:18:50,310
So the elements of integrated
human experience design
as described in pixels in

307
00:18:50,311 --> 00:18:50,820
place,

308
00:18:50,820 --> 00:18:54,330
I'll go through it really quickly because
what I want to get to is that with

309
00:18:54,331 --> 00:18:55,200
tech humanist,

310
00:18:55,260 --> 00:18:59,250
I've actually built out upon this
to a more automated understanding of

311
00:18:59,251 --> 00:19:04,230
experience, but within integrated human
experience design in pixels in place,

312
00:19:05,100 --> 00:19:08,310
we look at integration. Of course,
that kind of comes along for the ride.

313
00:19:08,311 --> 00:19:11,700
So we're already talking about all
these layers that are being integrated,

314
00:19:11,701 --> 00:19:15,810
the online and offline contexts. Uh,
we're talking about dimensionality.

315
00:19:15,840 --> 00:19:20,730
So how does something come to life across
different sort of touch points or ways

316
00:19:20,731 --> 00:19:23,880
in which you interact with people?
How does,

317
00:19:24,000 --> 00:19:26,910
how did the metaphors and cognitive
associations come to life?

318
00:19:26,940 --> 00:19:29,910
What sorts of intentional things are
you communicating through all of the

319
00:19:29,911 --> 00:19:32,680
choices that you're making
about the language that you use,

320
00:19:32,681 --> 00:19:34,140
the iconography you use,

321
00:19:34,410 --> 00:19:37,050
and the cognitive assertions you're
bringing along with that cognitive

322
00:19:37,051 --> 00:19:41,400
associations. Uh, you're bringing along
with that intentionality and purpose.

323
00:19:41,401 --> 00:19:43,920
So how have you defined what it
is you're trying to accomplish?

324
00:19:43,921 --> 00:19:48,030
And that comes into play at a,
at a more holistic macro level as well,

325
00:19:48,031 --> 00:19:51,450
which we'll get to in just a moment
and a value and emotional load.

326
00:19:51,451 --> 00:19:55,020
Like where are you meeting someone in the
world? How challenging is that context?

327
00:19:55,200 --> 00:19:58,650
If you're designing for an encounter
in a hospital, for example,

328
00:19:58,651 --> 00:20:03,090
it's going to be a very different type
of a valuer emotional load than if you

329
00:20:03,091 --> 00:20:07,930
encounter somebody, uh, at
a, at a children's museum
where they're having fun. Uh,

330
00:20:08,130 --> 00:20:13,130
hopefully having fun alignment is of
course that that sort of foundational

331
00:20:13,740 --> 00:20:17,520
principle of understanding
what the business objective
is and understanding the

332
00:20:17,521 --> 00:20:21,000
human objective and making sure that
they are as tightly aligned as possible.

333
00:20:21,570 --> 00:20:24,210
And then adaptation and
iteration being of course,

334
00:20:24,540 --> 00:20:29,100
that that process of making sure that
we are building upon what we've learned,

335
00:20:29,101 --> 00:20:31,410
we're using experimentation and,
and that,

336
00:20:31,440 --> 00:20:35,460
that mental model of building
our learnings as we go.

337
00:20:36,540 --> 00:20:41,540
There's also this premise that experience
has a sort of to in a sense two layers

338
00:20:44,520 --> 00:20:45,690
to it.
Um,

339
00:20:46,530 --> 00:20:51,530
if you think about human nature
as this sort of ongoing truism,

340
00:20:52,511 --> 00:20:56,980
like we all have throughout time
needed to drink water for example.

341
00:20:57,580 --> 00:21:00,820
But then there's the shape
of that experience and how
it kind of gets packaged

342
00:21:00,821 --> 00:21:02,020
up and dimensionalized.

343
00:21:02,021 --> 00:21:06,580
And so you can see this bottle of
water a is an example of saying, well,

344
00:21:07,120 --> 00:21:11,380
if I were to put that water into sort of
a heavy glass bottle and label it with

345
00:21:11,381 --> 00:21:14,770
some sort of minimalistic type
face sort of brands and you know,

346
00:21:14,771 --> 00:21:18,400
create that whole aesthetic and it
has this kind of hipster vibe to it.

347
00:21:18,400 --> 00:21:22,420
Maybe I feel like I'm being a more
aspirational version of myself because I'm

348
00:21:22,421 --> 00:21:27,190
drinking maybe even the same water out
of this cool bottle and I feel like a

349
00:21:27,191 --> 00:21:29,400
better version of who I am.
Uh,

350
00:21:29,440 --> 00:21:32,410
then if I just drank it out of the
tap water tap glass or whatever.

351
00:21:33,230 --> 00:21:37,660
But so there's this kind of ongoing
way in which, uh, shapes evolve.

352
00:21:38,380 --> 00:21:41,320
And it's important I think to recognize
that as we create these integrated

353
00:21:41,321 --> 00:21:46,321
experiences that human experiences do
evolve with the shapes will always change

354
00:21:46,691 --> 00:21:50,800
more readily than the nature and helps
us get into contact and sort of create

355
00:21:50,801 --> 00:21:55,801
this continuity across time with the
human nature that persists throughout the

356
00:21:56,291 --> 00:22:01,291
experiences that we're designing for and
yet be ready to adapt to the changing

357
00:22:01,301 --> 00:22:06,250
shape of experiences. So with
that, that leads us into this,

358
00:22:06,280 --> 00:22:11,280
this opportunity to think
about how machine led
experiences can actually be more

359
00:22:12,191 --> 00:22:12,820
meaningful.

360
00:22:12,820 --> 00:22:17,320
The more we're thinking about automated
experiences and artificially intelligent

361
00:22:17,321 --> 00:22:18,160
experiences,

362
00:22:18,310 --> 00:22:22,060
how can we think about making sure that
the humans that interact with those are

363
00:22:22,061 --> 00:22:26,140
having as much of a sense of
meaning and significance and uh,

364
00:22:26,141 --> 00:22:31,090
and dimension to those.
So what I proposed in tech humanist is,

365
00:22:31,091 --> 00:22:35,170
uh, that we don't just
automate the meaning menial,
we automate the meaningful,

366
00:22:35,171 --> 00:22:37,580
I'll go through each one of
these in detail of course, uh,

367
00:22:37,660 --> 00:22:38,950
that we automate empathy,

368
00:22:38,980 --> 00:22:43,540
that we use human data respectfully and
that we reinvest the gains in efficiency

369
00:22:43,541 --> 00:22:48,430
that we get in business from automation
back into humanity and human experiences

370
00:22:48,431 --> 00:22:51,640
at least at some level.
And so we'll talk about each one of these.

371
00:22:52,270 --> 00:22:53,200
I'll start with this.

372
00:22:53,740 --> 00:22:56,930
I think a lot of times when we
talk about automation are our,

373
00:22:56,940 --> 00:23:00,100
our base understanding is that
we should automate menial,

374
00:23:00,220 --> 00:23:04,000
meaningless things so that
humans can do higher order tasks,

375
00:23:04,270 --> 00:23:08,950
which is a nice enough premise until you
start thinking about that at scale and

376
00:23:08,960 --> 00:23:12,790
start imagining a world in which all
kinds of functions have been automated.

377
00:23:12,790 --> 00:23:15,730
And most of our world is automated
and most of our interactions are with

378
00:23:15,731 --> 00:23:19,030
machines and they've all been
automated to the meaningless.

379
00:23:20,050 --> 00:23:23,110
So I think it's a, it's a yes
and a both and sort of scenario.

380
00:23:23,111 --> 00:23:25,870
We do need to think about
automating the menial,

381
00:23:25,871 --> 00:23:30,340
meaningless functions to free ourselves
up to think about higher order things.

382
00:23:30,950 --> 00:23:33,100
What we also need to think
about what's working,

383
00:23:33,101 --> 00:23:37,770
what are human interactions that conveys
some level of empathy and nuance that

384
00:23:38,050 --> 00:23:42,130
create some sort of significance and
dimension and how can we kind of work to

385
00:23:42,131 --> 00:23:43,450
automate those as well?

386
00:23:43,451 --> 00:23:46,940
How can we capture some of that
significance in those automations?

387
00:23:47,730 --> 00:23:51,350
So in this way, we're talking about
using data and technology to scale,

388
00:23:51,351 --> 00:23:52,370
not just for efficiency,

389
00:23:52,371 --> 00:23:56,030
but from meaning to think about ways
that we can actually create a sense of

390
00:23:56,031 --> 00:23:57,590
dimension in the world around us.

391
00:23:58,520 --> 00:24:02,960
One way that that works is I like to think
about this model of this relationship

392
00:24:02,961 --> 00:24:04,580
between metaphor and metadata.

393
00:24:05,630 --> 00:24:09,550
And I think the easiest
way to explain this is a,

394
00:24:09,790 --> 00:24:14,630
is a slide I stole from Brian Chesky,
the CEO of Airbnb,

395
00:24:14,870 --> 00:24:17,060
when he was demoing a couple of years ago,

396
00:24:17,300 --> 00:24:20,060
the new campaign that they were
launching at the time, which was the,

397
00:24:20,061 --> 00:24:24,560
don't go there, don't go there, live there
campaign. Anybody familiar with that?

398
00:24:24,650 --> 00:24:27,650
Because we run across that at
all. So the idea was, you know,

399
00:24:27,680 --> 00:24:29,420
even if it's only for one night,

400
00:24:29,780 --> 00:24:32,720
go to every place you visit as
if you're a local, you know,

401
00:24:32,721 --> 00:24:34,430
treat that city like you're a local.

402
00:24:35,630 --> 00:24:40,630
And this slide was an illustration of
how you could experience a different type

403
00:24:40,971 --> 00:24:41,810
of,
um,

404
00:24:42,380 --> 00:24:47,380
of approach on TripAdvisor versus with
the airbnb approach of trusting the local

405
00:24:48,411 --> 00:24:50,660
experts.
But note that extend.

406
00:24:50,670 --> 00:24:55,670
So this is obviously Paris and note that
every thing on each list is different

407
00:24:56,271 --> 00:24:58,160
except for one,
which was the Luxembourg garden,

408
00:24:58,161 --> 00:25:02,120
which is my favorite place in
Paris. So yay me. Uh, but the, he,

409
00:25:02,450 --> 00:25:05,300
each of the other things
on the TripAdvisor list,

410
00:25:05,630 --> 00:25:09,650
it's really just a popularity contest,
right?

411
00:25:09,770 --> 00:25:14,770
It's all just what are the most sort
of bucket list items that someone would

412
00:25:15,171 --> 00:25:19,950
associate with Paris? And on
the Airbnb side, it's who has,

413
00:25:20,000 --> 00:25:23,270
who has the most significant
understanding of the city of Paris?

414
00:25:23,420 --> 00:25:27,830
What did they recommend as being the
places that you'd sort of must visit and

415
00:25:27,831 --> 00:25:29,450
must experience in Paris?

416
00:25:30,260 --> 00:25:33,200
And what I think is interesting is when
you think about the metaphor that's

417
00:25:33,201 --> 00:25:34,370
really underlying this,

418
00:25:34,400 --> 00:25:39,400
it's clear that the TripAdvisor metaphor
is much more about this kind of a

419
00:25:40,430 --> 00:25:44,120
casual tourist experience of the world.
This kind of conventional understanding,

420
00:25:44,121 --> 00:25:47,480
right? Whereas the airbnb thing is that
sort of don't go there, live there,

421
00:25:47,481 --> 00:25:51,050
this knowledge of the expertise and then
the metadata clearly is it's like the

422
00:25:51,051 --> 00:25:53,030
same city.
These are all the same landmarks.

423
00:25:53,031 --> 00:25:58,031
They exist in either case but it's one
is being rated for popularity and one is

424
00:25:58,041 --> 00:26:01,010
being ranked for this
expertise where authority.

425
00:26:01,550 --> 00:26:04,430
So the way that these two sort
of dimensions interact with them,

426
00:26:04,431 --> 00:26:08,030
then a creates this more
meaningful understanding of uh,

427
00:26:08,031 --> 00:26:11,780
of what the company is trying to achieve
and how it brings it to dimensional

428
00:26:11,781 --> 00:26:12,830
life for,

429
00:26:12,860 --> 00:26:16,580
for the person that's interacting with
it because that meaning and forms the

430
00:26:16,581 --> 00:26:21,581
purpose that the company is bringing
to life in their experiences and the

431
00:26:22,101 --> 00:26:23,990
purpose of the company that
they're trying to bring.

432
00:26:23,991 --> 00:26:28,940
The life sort of fosters the meaning that
the person is going to experience when

433
00:26:28,941 --> 00:26:29,691
they interact with,

434
00:26:29,691 --> 00:26:32,600
with the touch points that the group for
the company creates if they've done it

435
00:26:32,601 --> 00:26:33,434
well.

436
00:26:33,650 --> 00:26:38,000
And the Nice thing about this when you
think about how this really comes to life

437
00:26:38,001 --> 00:26:40,550
in an automated machine led way,

438
00:26:40,940 --> 00:26:45,930
is that humans really, I think when
you think about what, what the, uh,

439
00:26:45,931 --> 00:26:50,931
the research shows what we most thrive
on is a sense of meaning and common goals

440
00:26:51,901 --> 00:26:54,330
and a sense of fulfilling
something bigger than ourselves.

441
00:26:54,690 --> 00:26:58,530
Whereas machines thrive on this
sense of clear instruction, right?

442
00:26:59,280 --> 00:27:01,860
And what leads to both of
those things is purpose.

443
00:27:01,920 --> 00:27:04,980
I'm not talking about perfect purpose,
like in this kind of touchy feely,

444
00:27:05,250 --> 00:27:07,260
you know,
spiritual sense necessarily.

445
00:27:07,500 --> 00:27:12,420
I'm talking about purpose as a set of
clear instructions or as a sense of

446
00:27:12,421 --> 00:27:15,330
clarity about what it is
you're trying to achieve.

447
00:27:15,750 --> 00:27:20,490
And what that does is leads
to this ability to kind of
bring all your resources

448
00:27:20,491 --> 00:27:24,780
to bear in a very efficient way and
to align all those resources to set

449
00:27:24,781 --> 00:27:28,740
priorities very effectively and make sure
that everybody's kind of rowing in the

450
00:27:28,741 --> 00:27:32,760
same direction.
My favorite example of this,

451
00:27:32,820 --> 00:27:37,320
of companies setting a strategic purpose
and really using it to operationalize

452
00:27:37,321 --> 00:27:40,950
around is Disney theme
parks. Uh, and you know,

453
00:27:40,951 --> 00:27:44,190
from a digital transformation
perspective than my magic band program.

454
00:27:44,490 --> 00:27:48,150
How many of you then to Disney world
or one of the Disney theme parks since

455
00:27:48,151 --> 00:27:51,240
they've introduced this, it
is pretty magical, right?

456
00:27:51,240 --> 00:27:55,740
Like so they have articulated their
purpose statement as create magical

457
00:27:55,741 --> 00:28:00,510
experiences. It's really just those
three words create magical experiences.

458
00:28:01,380 --> 00:28:05,820
And so you think about
cross the organization just
about anyone in any function

459
00:28:06,000 --> 00:28:10,650
can understand how they can solve problems
relative to their scope of their work

460
00:28:11,250 --> 00:28:14,760
as it relates to creating more magical
experiences like a problem that's brought

461
00:28:14,761 --> 00:28:16,830
to them. They can just go
along. I know how to solve this.

462
00:28:17,190 --> 00:28:21,000
As long as the company actually sort of
gets in line behind that and allows them

463
00:28:21,001 --> 00:28:23,550
the autonomy to solve the
problem the way they need to.

464
00:28:24,240 --> 00:28:28,920
But think about that as it relates to
digital transformation and deploying $1

465
00:28:28,920 --> 00:28:29,753
billion program,

466
00:28:29,880 --> 00:28:34,320
which this was that investment
scale for the company.

467
00:28:34,740 --> 00:28:38,850
And you can do that with complete
confidence knowing that this magic band is

468
00:28:38,851 --> 00:28:42,830
going to allow people to be able to go
around the park and use it as payment,

469
00:28:42,840 --> 00:28:45,150
use it as access,
uh,

470
00:28:45,151 --> 00:28:49,620
use it as a sort of preferences and all
kinds of information that are tracking

471
00:28:49,621 --> 00:28:53,730
that certainly gives a lot of useful
information to the company as they sort of

472
00:28:53,731 --> 00:28:57,450
merchandise more effectively
and so on. But that, that,

473
00:28:57,480 --> 00:29:02,480
that ability to translate the purpose
into a deployment at $1 billion scale is

474
00:29:04,651 --> 00:29:07,890
very clear from,
from that that program.

475
00:29:08,610 --> 00:29:12,930
So we can design experiences that are
aligned with strategic purpose so that we

476
00:29:12,931 --> 00:29:15,330
could actually see that
understanding of purpose,

477
00:29:15,331 --> 00:29:20,331
scale to massive levels and purpose is
the shape meaning takes in business.

478
00:29:20,790 --> 00:29:25,170
So that's how we get that meaning to, to
be felt and understood at a human level.

479
00:29:25,410 --> 00:29:25,801
By the way,

480
00:29:25,801 --> 00:29:30,660
I keep talking about scale and so I
want to unpack that a little bit too. So

481
00:29:32,370 --> 00:29:35,280
when we think about creating
meaningful experiences at scale,

482
00:29:36,360 --> 00:29:39,990
normally when we talk about scale and like
a startup, we're a corporate business,

483
00:29:40,050 --> 00:29:42,220
a corporate, uh, growth sort of scenario.

484
00:29:42,580 --> 00:29:46,840
We are talking about like removing hard
limits so that growth opportunities can

485
00:29:46,841 --> 00:29:50,740
flourish. And usually we're talking about
that in terms of multiples, let's say,

486
00:29:50,741 --> 00:29:54,790
right, like three x or four x or five
x or 10 x if you're very, very lucky.

487
00:29:55,510 --> 00:29:56,343
Um,

488
00:29:56,740 --> 00:30:01,740
but what happens when a
notion meets nearly unlimited
expansion possibility when

489
00:30:02,531 --> 00:30:07,060
data can model it and
software can accelerate it
and automation can amplify it

490
00:30:07,420 --> 00:30:08,830
and culture can adopt it.

491
00:30:09,340 --> 00:30:12,370
And that's what really we're talking
about with machine let experiences,

492
00:30:12,640 --> 00:30:16,120
and that's why it's so important that
we think about creating these in a more

493
00:30:16,121 --> 00:30:19,900
meaningful way. Because if we don't
create the meaning into the system,

494
00:30:20,440 --> 00:30:23,200
what are we doing? We are allowing
absurdity to encroach, right?

495
00:30:23,201 --> 00:30:24,940
And we don't want absurdity to scale.

496
00:30:26,440 --> 00:30:30,300
So my favorite example of absurdity,
it's scale. It's one that I,

497
00:30:30,301 --> 00:30:35,050
I don't mean to knock the program or the
product because I think it's incredible.

498
00:30:35,350 --> 00:30:38,680
The Amazon ghost or how many of
you have experienced it in person.

499
00:30:39,310 --> 00:30:40,750
It's pretty cool,
right?

500
00:30:40,751 --> 00:30:43,600
Like the idea that you can actually
just walk into a grocery store,

501
00:30:43,960 --> 00:30:48,040
you scan your app as you go in and uh,

502
00:30:48,160 --> 00:30:52,300
and just pick up whatever you need and
walk right out. And there's no, you know,

503
00:30:52,390 --> 00:30:55,330
sort of checking out process.
It just kind of knows it.

504
00:30:55,331 --> 00:30:57,190
But through cameras and sensors and so on,

505
00:30:57,191 --> 00:31:01,000
it knows what you've picked up and
what to associate with your account and

506
00:31:01,001 --> 00:31:01,834
you're good to go.

507
00:31:02,530 --> 00:31:07,150
So obviously we have to talk about a
cashier jobs and what that means for the

508
00:31:07,151 --> 00:31:09,880
future of human work
as that goes to scale.

509
00:31:10,660 --> 00:31:13,660
But let's leave that set aside for just
another moment because right now what

510
00:31:13,661 --> 00:31:15,460
I'm focused on is something else.
What,

511
00:31:16,120 --> 00:31:20,830
what happens when you open the APP
for the first time and you get this

512
00:31:20,831 --> 00:31:25,750
onboarding that explains that as you pick
things up off the shelf, the sensors,

513
00:31:25,751 --> 00:31:25,991
no.

514
00:31:25,991 --> 00:31:29,200
You know what you've picked up and as you
put it into your basket or in your bag

515
00:31:30,040 --> 00:31:34,720
that you'll be charged for it. So it says,
don't pick up anything for anyone else.

516
00:31:35,530 --> 00:31:36,170
Okay.

517
00:31:36,170 --> 00:31:39,140
Which is fine except that you start
thinking about, I don't know about you,

518
00:31:39,141 --> 00:31:42,350
but I get asked all the time to help
people in stories. You seem pretty tall.

519
00:31:42,351 --> 00:31:45,750
You probably get help ask for that.
Uh, you know, you get asked and,

520
00:31:45,751 --> 00:31:46,700
and now it's like,
okay,

521
00:31:46,701 --> 00:31:50,660
well I can't really help that person get
the thing off the shelf cause it might

522
00:31:50,661 --> 00:31:54,020
charge me for it. And there's a
way to get it charged back. And,

523
00:31:54,190 --> 00:31:56,450
and Amazon might fix this
before it goes to scale,

524
00:31:56,451 --> 00:32:01,451
but really like 3000 Amazon go stores
have been announced before 2021 so if this

525
00:32:03,561 --> 00:32:05,210
doesn't get fixed,
and if it is,

526
00:32:05,211 --> 00:32:09,800
it's something that we all start adjusting
our behavior and not helping other

527
00:32:09,801 --> 00:32:13,640
people in the Amazon go store.
Well that's the future of retail.

528
00:32:13,641 --> 00:32:18,641
We're talking about 3000 Amazon go stores
by 2021 is going to mean that retail

529
00:32:19,731 --> 00:32:23,750
environments are going to be this
cashier list environment before too long.

530
00:32:24,170 --> 00:32:26,240
And so we won't help
each other in any stores.

531
00:32:26,720 --> 00:32:29,600
And how long is it before we
don't help each other at all.

532
00:32:29,710 --> 00:32:31,940
And I know that sounds like
hyperbole at some level,

533
00:32:32,150 --> 00:32:37,150
but what I mean to suggest is that the
idea that experience at scale does change

534
00:32:37,461 --> 00:32:38,294
culture.

535
00:32:38,570 --> 00:32:43,220
And I that's important to recognize
because really experienced that scale is

536
00:32:43,221 --> 00:32:44,054
culture.

537
00:32:44,270 --> 00:32:48,230
What we sort of all collectively agreed
to do with each other and how we agree

538
00:32:48,231 --> 00:32:52,760
to interact with each other is culture
and are all of the work that we do.

539
00:32:52,790 --> 00:32:57,770
Creating human experiences sets that
context and creates that, that modality.

540
00:32:57,771 --> 00:33:01,280
So that understanding is,
is super, super important.

541
00:33:01,850 --> 00:33:04,760
So I do have a slide here and
if anybody needs these slides,

542
00:33:05,000 --> 00:33:07,070
I'm happy to share them. Uh, but,

543
00:33:07,071 --> 00:33:09,770
but a slide that asks questions
and it's in the book as well.

544
00:33:10,040 --> 00:33:13,430
If we were to try to, uh, the program,

545
00:33:13,460 --> 00:33:16,400
the absurdity of not helping each other,

546
00:33:16,640 --> 00:33:19,910
we could ask some questions to
step back from that and think like,

547
00:33:19,911 --> 00:33:24,440
how do we not create experience at scale?
That's going to be absurd.

548
00:33:24,590 --> 00:33:27,740
How do we make sure that the brand
isn't going to be impacted if we create

549
00:33:27,741 --> 00:33:31,880
products or solutions that might scale
in ways that are unexpected? How do we,

550
00:33:31,970 --> 00:33:34,370
how do we pivot to deal with that?
You know, what does that look like?

551
00:33:34,640 --> 00:33:38,030
So there's some questions we
can ask to anticipate that.

552
00:33:38,270 --> 00:33:43,160
But primarily I think the challenges
or the opportunity is to think about

553
00:33:43,310 --> 00:33:46,490
meaning and to think about
keeping absurdity from scaling.

554
00:33:46,790 --> 00:33:51,230
This is a comic that was drawn from
me by my friend Rob Cottingham, uh,

555
00:33:51,260 --> 00:33:54,560
to illustrate the sort of
opposite of tech humanism.

556
00:33:55,250 --> 00:33:56,061
I don't know if you can read it.

557
00:33:56,061 --> 00:33:59,570
It says it's getting harder and harder
to hold onto my humanity, but wow,

558
00:33:59,571 --> 00:34:01,730
is it easy to track my Amazon deliveries?

559
00:34:03,090 --> 00:34:04,760
So of course that's absurd,

560
00:34:05,780 --> 00:34:09,980
but it's the idea that we aren't
thinking about what do we really want

561
00:34:09,981 --> 00:34:11,510
meaningful experiences to look like?

562
00:34:11,510 --> 00:34:14,060
What do we really want our
future humanity to look like?

563
00:34:14,240 --> 00:34:19,190
How do we create technology solutions
that amplifier humanity and don't get in

564
00:34:19,191 --> 00:34:22,910
the way of humanity? I think that's
really what we're talking about.

565
00:34:24,320 --> 00:34:28,490
The second, uh, premise of
machine lead machine, uh,

566
00:34:28,520 --> 00:34:31,970
meaningful human experiences
as to automate empathy,

567
00:34:32,270 --> 00:34:34,310
which again may sound
like it's a contradiction,

568
00:34:35,060 --> 00:34:38,990
but I think there's an opportunity to
think about the ways that any kind of

569
00:34:38,991 --> 00:34:43,610
experience that we design creates
some kind of connection and to,

570
00:34:44,450 --> 00:34:46,310
to create as meaningful
a connection as possible.

571
00:34:46,311 --> 00:34:51,311
So how many of you remember the Seinfeld
episode where Kramer got a new phone

572
00:34:51,321 --> 00:34:54,980
number and it was one digit off from
movie phone? Anybody remember this?

573
00:34:56,060 --> 00:34:57,260
So anybody remember?

574
00:34:57,261 --> 00:35:02,060
Moviefone I know the APP just sort
of ended as of like a week ago,

575
00:35:02,061 --> 00:35:05,660
but um, but in the eighties
and nineties or whatever,

576
00:35:05,661 --> 00:35:09,080
we all had to pick up the phone and
actually call a service to tell us what

577
00:35:09,081 --> 00:35:13,310
movies were playing. And in this episode,
Cramer had gotten a new phone number.

578
00:35:13,430 --> 00:35:15,890
It was one digit off or
move your phone. And, uh,

579
00:35:15,891 --> 00:35:17,930
it was a obviously touch tone service.

580
00:35:17,931 --> 00:35:21,020
So he couldn't understand the touchstones
that he decided that he was going to

581
00:35:21,021 --> 00:35:24,220
impersonate movie phone, uh, but he
couldn't understand the touchstones.

582
00:35:24,250 --> 00:35:28,100
So he ends up just saying, why don't you
just tell me what movie you want to see?

583
00:35:29,060 --> 00:35:34,060
And I always find this to view such a
prescient example of how we think about

584
00:35:34,610 --> 00:35:39,090
machine driven interactions and how we
think about human based interactions look

585
00:35:39,091 --> 00:35:41,460
like the, the sort of
relationship between those two.

586
00:35:41,790 --> 00:35:46,790
So I think of the movie phone Kramer
model as a sort of agile deployment of

587
00:35:47,580 --> 00:35:51,330
emerging technology that
you can think about the uh,

588
00:35:51,360 --> 00:35:56,360
the robotic interaction and the
human interaction as being somehow

589
00:35:57,031 --> 00:36:00,840
interchangeable with one another so that
you can actually use human interaction

590
00:36:00,841 --> 00:36:05,841
to gather patterns that you will
encode as a chat bots or other types of

591
00:36:06,301 --> 00:36:10,120
automation. And not just suggest that
you would lie, that you would print,

592
00:36:10,140 --> 00:36:15,140
present a human a and have
it be posing as moviefone or,

593
00:36:15,210 --> 00:36:16,860
or whatever your equivalent is,

594
00:36:17,340 --> 00:36:21,840
but rather that you would have some kind
of agile human based interaction that

595
00:36:21,841 --> 00:36:26,070
gives you the insights to be able to
create scripts and create patterns that

596
00:36:26,071 --> 00:36:28,770
develop or help you develop
frameworks for automation.

597
00:36:30,300 --> 00:36:32,370
And of course you're starting
with if then statements,

598
00:36:32,371 --> 00:36:36,120
but you're quickly trying to work
beyond the if then to get to the nuance.

599
00:36:36,360 --> 00:36:38,160
So if that is easy to anticipate,
right?

600
00:36:38,161 --> 00:36:42,480
And in the kind of a frequently
asked questions model of automation,

601
00:36:42,481 --> 00:36:46,440
if you're saying like that,
if you're automating, let's
say a chat Bot for a bank,

602
00:36:46,620 --> 00:36:50,370
you know that a lot of your interactions
are going to be about how to change

603
00:36:50,371 --> 00:36:53,190
your password, for example, or
how to set up a new account.

604
00:36:53,310 --> 00:36:55,860
So if someone wants to
create a new account,

605
00:36:55,920 --> 00:36:57,570
then here's the answer and here's the,

606
00:36:57,571 --> 00:37:00,240
the sort of flow diagram that
you can walk them through.

607
00:37:00,780 --> 00:37:05,780
But the nuance beyond that is I need
to change my password because my ex is

608
00:37:06,481 --> 00:37:07,380
stalking me.

609
00:37:07,410 --> 00:37:11,760
And it's a dangerous situation and there
needs to be some human interaction.

610
00:37:11,761 --> 00:37:14,910
There needs to be some human
nuance to that experience.

611
00:37:15,330 --> 00:37:19,680
So that's more where the empathy gets
automated into the process is finding

612
00:37:19,860 --> 00:37:23,730
those types of interactions and finding
the opportunity to build out the

613
00:37:23,731 --> 00:37:26,310
relationship between the
automated and the human.

614
00:37:26,940 --> 00:37:30,180
And also when we're looking for
patterns that we're not just looking for

615
00:37:30,181 --> 00:37:35,181
arbitrary patterns and encoding those
arbitrary Venus is also sits in opposition

616
00:37:35,521 --> 00:37:39,030
to meaningfulness in much the same way
that absurdity sets an opposition to

617
00:37:39,031 --> 00:37:39,571
meaningfulness.

618
00:37:39,571 --> 00:37:42,810
So we want to make sure that we're
finding meaningful patterns and automating

619
00:37:42,811 --> 00:37:47,490
those. Because in all of the
work that we do with this,

620
00:37:47,491 --> 00:37:50,430
we cannot leave meaning up to machines.

621
00:37:50,730 --> 00:37:52,530
Machines won't do meaning.

622
00:37:53,070 --> 00:37:56,070
That's just not something that
machines are really equipped for.

623
00:37:56,310 --> 00:38:00,040
So it has to be humans that determine
what, what is meaningful and that,

624
00:38:00,420 --> 00:38:01,350
I love this example.

625
00:38:01,351 --> 00:38:05,790
I know many of you probably work around
image recognition or AI and so you know

626
00:38:05,791 --> 00:38:08,070
this dilemma very, very well. It's,

627
00:38:08,400 --> 00:38:11,400
I know a lot of algorithms have
advanced since this day, but you know,

628
00:38:11,640 --> 00:38:16,640
the puppy versus muffin sort of a problem
is one of my faves and that it always

629
00:38:16,951 --> 00:38:20,210
gets a chuckle. I see some
smiles in the audience. Uh, but,

630
00:38:20,250 --> 00:38:24,810
but it's true that subtle nuances aren't
really where AI signs and many cases at

631
00:38:24,811 --> 00:38:28,740
this point, at least not at a
meaningful recognition level,

632
00:38:29,310 --> 00:38:33,030
not being able to say
that, uh, the muffins are,

633
00:38:33,031 --> 00:38:36,250
have this certain meaningful
characteristic and the human, the puppies,

634
00:38:36,251 --> 00:38:37,840
I'll have this certain
meaningful characteristic.

635
00:38:38,260 --> 00:38:42,090
Whereas I believe many of you are probably
able to determine which ones a muffin

636
00:38:42,100 --> 00:38:45,310
in which one's a puppy pretty well. Here's
some more. And by the way, uh, you know,

637
00:38:45,311 --> 00:38:48,580
which one's a barn owl and which one's
an apple and not have any trouble with

638
00:38:48,581 --> 00:38:50,710
that.
I bet which one's a croissant.

639
00:38:52,480 --> 00:38:56,150
But I think this an introduces
an idea that there may be,

640
00:38:57,130 --> 00:39:02,130
there may be opportunities for humans to
work alongside machines in ways that I

641
00:39:04,571 --> 00:39:09,571
add nuance and empathy and understanding
to the machine lab processes,

642
00:39:10,520 --> 00:39:12,880
uh, cause humans generally
do nuance pretty well.

643
00:39:12,910 --> 00:39:16,900
That's something that we are encoded for.
We get meaning. That's what we're about.

644
00:39:17,560 --> 00:39:20,320
So we're able to add that
into the value proposition.

645
00:39:20,321 --> 00:39:23,650
So when we think about the relationship
between machines and humans,

646
00:39:23,710 --> 00:39:25,650
as we move into the, the, uh,

647
00:39:25,651 --> 00:39:29,500
the future of work and the
future of that sort of economy,

648
00:39:29,770 --> 00:39:33,940
I think we're going to add the most
value by being human and understanding

649
00:39:34,000 --> 00:39:37,720
meaning and nuance and understanding
value and understanding each other and

650
00:39:37,721 --> 00:39:39,910
adding that layer to those interactions.

651
00:39:41,470 --> 00:39:45,910
So the third tenant of the machine led
meaningful experiences is to use human

652
00:39:45,911 --> 00:39:47,020
data respectfully.

653
00:39:49,300 --> 00:39:52,750
It comes from this idea that when we
talk about digital transformation,

654
00:39:52,751 --> 00:39:56,680
I kind of feel like that's a little bit
of a misnomer at some level because we

655
00:39:56,681 --> 00:39:58,720
already made a digital
transformation at the moment.

656
00:39:58,721 --> 00:40:02,200
We started spending all of our time in
front of screens transacting in front of,

657
00:40:02,201 --> 00:40:05,830
in bits and bytes with each other.
So that's kind of a done deal.

658
00:40:06,100 --> 00:40:10,360
And what's really more meaningful
than that is the data transformation.

659
00:40:10,510 --> 00:40:13,960
The fact that all of this is
happening with a data layer behind it.

660
00:40:14,170 --> 00:40:15,970
That business has all kinds of data,

661
00:40:15,971 --> 00:40:19,330
visibility and transparency through
the supply chain, through logistics,

662
00:40:19,331 --> 00:40:24,130
through operations and everything has
this kind of clarity and transparency

663
00:40:24,430 --> 00:40:27,670
about what kind of trackability
is going on, what's,

664
00:40:27,820 --> 00:40:31,850
what's measurable and all that.
So it's a really interesting, uh,

665
00:40:31,870 --> 00:40:36,160
layer to work with. But when we
talk about digital transformation,

666
00:40:36,670 --> 00:40:39,370
including automation and
digitization and all of that,

667
00:40:39,550 --> 00:40:41,440
all of the many nuances of that,

668
00:40:41,800 --> 00:40:44,980
fundamentally what we're talking
about is agility with data.

669
00:40:44,980 --> 00:40:49,270
As companies become more digitally
writing in, digitally transformed,

670
00:40:49,510 --> 00:40:52,480
are becoming more agile
with database decisions.

671
00:40:53,530 --> 00:40:57,880
And that data that we're talking
about is really our data.

672
00:40:58,210 --> 00:41:00,400
It's human data for the most part.

673
00:41:00,640 --> 00:41:05,050
Business data is largely about people.
It's our purchases,

674
00:41:05,051 --> 00:41:08,530
it's our movements through space. It's
our preferences. It's are, you know,

675
00:41:08,531 --> 00:41:12,430
all of our tastes and indications
that we've made. And it really,

676
00:41:12,431 --> 00:41:16,930
what I'm saying is analytics are
people, right? At some level,

677
00:41:16,931 --> 00:41:20,710
for the most part, when we are looking
at graphs and reports and so on,

678
00:41:20,980 --> 00:41:24,970
we're generally looking at the needs
and interests and motivations of real

679
00:41:25,030 --> 00:41:28,780
people that are, they're buying from
our companies and interacting with them.

680
00:41:29,170 --> 00:41:31,600
And driving all of these decisions for us.

681
00:41:32,320 --> 00:41:35,960
And I think the flip side of appreciating
that and treating that data with

682
00:41:35,961 --> 00:41:40,961
respect is understanding that what we
encode into machines is really about us,

683
00:41:42,141 --> 00:41:46,990
that we are putting ourselves
and our biases into, into the,

684
00:41:46,991 --> 00:41:50,120
the encoding, into the algorithms
and everything that we create.

685
00:41:51,290 --> 00:41:56,290
So the opportunity I think as we look at
this tech humanist future is to encode

686
00:41:57,321 --> 00:41:58,670
the best of ourselves,

687
00:41:58,880 --> 00:42:03,650
is to think about how we can create our
most egalitarian viewpoints and our most

688
00:42:03,651 --> 00:42:07,250
evolved understandings
into the data we model,

689
00:42:07,251 --> 00:42:12,230
into the algorithms we build and into
the automated experiences that we design

690
00:42:12,231 --> 00:42:15,680
and create.
So we can use our data,

691
00:42:15,681 --> 00:42:19,310
our human data to make
more meaning in the world.

692
00:42:21,200 --> 00:42:26,030
And we can recognize that the more
we create relevance, uh, in those,

693
00:42:26,031 --> 00:42:29,090
in the alignment between business
objectives and cumin objectives,

694
00:42:29,390 --> 00:42:33,680
the more we are creating a
form of respect. But that also,

695
00:42:33,681 --> 00:42:37,910
the sort of caveat to that
is that discretion is a form
of respect to that we're

696
00:42:37,911 --> 00:42:39,860
also allowing people to say,

697
00:42:39,861 --> 00:42:44,060
be forgotten by us and allowing
them to take their data with them.

698
00:42:44,061 --> 00:42:47,570
And that we can not make people feel like
we're creeping them out by knowing so

699
00:42:47,571 --> 00:42:52,571
much about them and that we
protect human data excessively,

700
00:42:53,240 --> 00:42:54,980
that we make sure we're being very,
very,

701
00:42:54,981 --> 00:42:58,880
very careful with the data that we
collect and use and business decisions

702
00:42:58,881 --> 00:43:03,020
because we recognize that it
is human data. The last point,

703
00:43:03,021 --> 00:43:07,970
and it's a quick one because this may or
may not be within scope for many of you,

704
00:43:08,300 --> 00:43:10,930
but that as we think about, uh, the,

705
00:43:10,931 --> 00:43:14,720
the gains that we make in our businesses
through automation and machine led

706
00:43:14,721 --> 00:43:18,830
experiences that we think
about reinvesting some of
those gains into how to

707
00:43:18,831 --> 00:43:22,120
create more meaningful human
experiences at scale. Uh,

708
00:43:22,150 --> 00:43:26,120
and I don't think it's really a
mystery why that's so important. Uh,

709
00:43:26,300 --> 00:43:28,730
there was a study done,
a couple of versions of,

710
00:43:28,731 --> 00:43:33,320
of a study done on what jobs
are potentially considered
automateable and this is

711
00:43:33,321 --> 00:43:38,321
one visualization of the data
from that study that shows the,

712
00:43:39,051 --> 00:43:41,990
the different cities in the
United States and how likely you,

713
00:43:41,991 --> 00:43:45,680
the jobs that are there are to be
automated over the coming years.

714
00:43:45,980 --> 00:43:48,650
I zoomed in on New York,
which is where I'm from,

715
00:43:48,740 --> 00:43:53,430
and you see 55% of jobs are considered
potentially automate automateable and you

716
00:43:53,431 --> 00:43:55,730
have to think about the socio
economic impacts of that.

717
00:43:55,820 --> 00:43:58,160
You have to think about the
psychological impact of that,

718
00:43:58,310 --> 00:44:03,310
that humans have had a very deeply
connected experience to work that we've

719
00:44:03,411 --> 00:44:06,440
derived a lot of our sense of
meaning and identity from work.

720
00:44:06,680 --> 00:44:11,450
We say who we are in terms of what we do,
and we've had names like butcher,

721
00:44:11,451 --> 00:44:12,620
a baker,
a Tanner carpenter,

722
00:44:12,621 --> 00:44:17,420
and so on that derived from ancestral
jobs that have been carried down through

723
00:44:17,421 --> 00:44:21,410
our, through generations of our family.
And that's true across cultures.

724
00:44:21,710 --> 00:44:25,370
So it's a really important thing to
understand that jobs are kind of going to

725
00:44:25,371 --> 00:44:25,851
change.

726
00:44:25,851 --> 00:44:29,960
There's going to be job
displacement augmentation and
replacement by automation.

727
00:44:29,961 --> 00:44:32,190
And we don't yet know what
means for human meaning.

728
00:44:32,520 --> 00:44:34,680
And we don't yet know what
that means economically.

729
00:44:35,010 --> 00:44:38,280
We don't yet know what that means,
you know, sort of socio politically.

730
00:44:38,910 --> 00:44:42,510
And so there's a huge opportunity for
us to take the gains that we make an

731
00:44:42,511 --> 00:44:46,290
automation and have this ethical
contribution back into society,

732
00:44:46,291 --> 00:44:47,580
back to humanity and say like,

733
00:44:47,581 --> 00:44:51,900
what can we do to foster a
sense of meaning and a sense
of community in a sense

734
00:44:51,901 --> 00:44:56,160
of connectedness and a sense of more
humanity with that, with those games.

735
00:44:57,290 --> 00:45:01,050
So I think we can also think
about repurposing human
skills and qualities into

736
00:45:01,051 --> 00:45:05,640
higher value roles. So as we automate
that, uh, one of my, the executives,

737
00:45:05,641 --> 00:45:06,960
it was another strategic workshop.

738
00:45:06,961 --> 00:45:11,961
I led a round on utilities company in
South America and he found through our

739
00:45:12,061 --> 00:45:16,230
work and opportunity to automate
a customer service function, uh,

740
00:45:16,320 --> 00:45:20,730
that was their most heavily access
customer support question and function.

741
00:45:21,060 --> 00:45:22,710
And once he saw that opportunity,

742
00:45:22,711 --> 00:45:26,550
he saw that there was a way to take the
humans that were working in that job and

743
00:45:26,551 --> 00:45:30,150
create oversight positions for them
so that they can continue training the

744
00:45:30,151 --> 00:45:32,970
algorithms that we're going to,
to create that automation.

745
00:45:33,300 --> 00:45:36,600
So obviously I'm very
straightforward kind of replacement.

746
00:45:36,900 --> 00:45:40,050
It may not be a one for one.
We may see job loss anyway,

747
00:45:40,350 --> 00:45:45,030
but some of that the investment is going
to uh, offer up human, human, higher,

748
00:45:45,060 --> 00:45:49,950
higher value human roles. So here
are those uh, four tenants again.

749
00:45:50,700 --> 00:45:51,240
And uh,

750
00:45:51,240 --> 00:45:55,380
I think the summary of this really comes
back to as you think about the work

751
00:45:55,381 --> 00:45:59,490
you're trying to do and how to create
these more meaningful experiences through

752
00:45:59,491 --> 00:46:02,160
automation and through artificial
intelligence and so on,

753
00:46:02,610 --> 00:46:07,050
it really comes down to this question of
what is it that you are trying to do at

754
00:46:07,051 --> 00:46:08,970
scale?
So that's the purpose statement.

755
00:46:08,971 --> 00:46:11,730
How can you articulate what it
is your company is trying to do,

756
00:46:11,731 --> 00:46:15,990
your team is trying to do, you are
trying to do at scale. And for me,

757
00:46:16,140 --> 00:46:20,220
the answer to that question is create
more meaningful human experiences.

758
00:46:20,310 --> 00:46:21,510
It's just as simple as that.

759
00:46:21,511 --> 00:46:24,660
But the way that I can do that is by
speaking with groups like yourself,

760
00:46:24,930 --> 00:46:26,850
working with executives,
working with leaders,

761
00:46:27,000 --> 00:46:31,230
and being able to help them hone in on
that purpose and really get clearer on

762
00:46:31,231 --> 00:46:34,920
how to create those more meaningful
experiences that do align the business

763
00:46:34,921 --> 00:46:39,090
objectives with the human objectives that
do bring the business results and that

764
00:46:39,091 --> 00:46:41,220
create a better future for humanity.

765
00:46:41,790 --> 00:46:45,510
So because business will have to scale
through digitization and automation,

766
00:46:45,511 --> 00:46:49,470
business won't be successful longterm
without it. It's table stakes,

767
00:46:49,980 --> 00:46:53,520
but humanity won't be
successful without meaning.

768
00:46:54,120 --> 00:46:58,800
So in for that, I thank all of you for
the work that you do. Thank you very much.

769
00:47:03,990 --> 00:47:04,823
Thanks Kate.

770
00:47:05,400 --> 00:47:06,180
We have a dory.

771
00:47:06,180 --> 00:47:09,660
There are no questions on it right now
so we can ask a few local questions and

772
00:47:09,661 --> 00:47:13,680
if anything shows up then we'll,
we'll get those included too.

773
00:47:13,980 --> 00:47:17,710
But I see a hand right here.
And can we get a mic over
here? Up The mic test, test,

774
00:47:17,711 --> 00:47:19,050
test. Okay, sounds good.

775
00:47:19,280 --> 00:47:20,113
Okay.

776
00:47:23,870 --> 00:47:25,640
Hi. Thank you for the top. First off.

777
00:47:26,150 --> 00:47:30,220
So for the rhetorical question you asked
at the very beginning of talk, I know

778
00:47:30,220 --> 00:47:33,370
a lot of people, if not most
people I know would answer a soul.

779
00:47:33,580 --> 00:47:35,980
What makes humans human is a soul.

780
00:47:36,310 --> 00:47:40,390
So what advice would you give
about how to automate religion?

781
00:47:40,750 --> 00:47:41,583
Hmm,

782
00:47:41,870 --> 00:47:43,240
that's a very interesting question.

783
00:47:43,660 --> 00:47:48,580
I actually have talked with a few
people about the work that they're doing

784
00:47:48,610 --> 00:47:53,560
around automation and creating experiences
for people at scale around religion.

785
00:47:54,160 --> 00:47:59,160
And I don't feel like I'm a in a really
good position to be an expert on that.

786
00:47:59,640 --> 00:48:01,240
Uh,
it's not the work that I do,

787
00:48:01,570 --> 00:48:05,470
but I do think that religion is
fundamentally offering meaning, right?

788
00:48:05,471 --> 00:48:09,100
So really we're talking about the same
principles we're talking about being able

789
00:48:09,101 --> 00:48:13,570
to offer people a lens into what is
meaningful and then helping to scale that.

790
00:48:13,870 --> 00:48:14,703
So if,

791
00:48:14,830 --> 00:48:18,610
if there is a solution that someone is
trying to build that is some sort of

792
00:48:18,611 --> 00:48:21,430
technology product for,
uh,

793
00:48:21,490 --> 00:48:25,450
creating like a religious experience
or a religious outlet for people or a

794
00:48:25,451 --> 00:48:28,660
community. And I think it really
comes down to the same principles.

795
00:48:28,870 --> 00:48:31,600
It's just like religion as
the industry in that sense.

796
00:48:31,601 --> 00:48:36,160
And we're trying to offer meaning through
those experiences. I would be, uh,

797
00:48:36,220 --> 00:48:38,830
probably my best to take on that,
but I think it's a,

798
00:48:38,831 --> 00:48:41,650
it's a more interesting question
than that at some fundamental layer.

799
00:48:41,651 --> 00:48:44,620
And it sounds like a discussion
over beers or something like that.

800
00:48:45,550 --> 00:48:49,690
So do we have another, another
question? Yes. To your,

801
00:48:50,300 --> 00:48:52,690
Yup. Hi. Thanks for the talk.

802
00:48:53,250 --> 00:48:58,250
I was really struck by your point about
shared experiences forming culture and

803
00:48:59,280 --> 00:49:03,360
um, obviously, um, we in the
technology world have a lot of,

804
00:49:03,510 --> 00:49:07,350
we sh increasingly kind of
shape, shared experience. Um,

805
00:49:07,380 --> 00:49:12,380
so in the Amazon go and Amazon go a point
about like people not helping you try

806
00:49:12,601 --> 00:49:12,721
there.

807
00:49:12,721 --> 00:49:16,680
That's something we can all probably agree
as a net negative and even Amazon I'm

808
00:49:16,681 --> 00:49:20,970
sure would agree. Um, but there's
a lot of cases where we have,

809
00:49:21,370 --> 00:49:22,040
uh,

810
00:49:22,040 --> 00:49:25,500
the potential to shape culture where the
answer really isn't clear what is the

811
00:49:25,501 --> 00:49:26,190
right thing to do.

812
00:49:26,190 --> 00:49:29,430
A filter bubbles being
like one controversial idea
of whether they're a good

813
00:49:29,431 --> 00:49:31,110
thing or a bad thing.
Um,

814
00:49:31,650 --> 00:49:35,640
so I'm wondering what's your take on
how we should approach these problems?

815
00:49:35,641 --> 00:49:40,350
Like what principles should we use in
deciding how to shape culture or what

816
00:49:40,680 --> 00:49:43,920
processes are institutions maybe
we need to make these decisions.

817
00:49:44,900 --> 00:49:48,870
Yeah, thank you. It's a really good
and big question. Uh, I'd like to,

818
00:49:49,130 --> 00:49:53,850
to cop out and say that the entire book
tech humanist sort of addresses that,

819
00:49:53,890 --> 00:49:56,360
but at some level,
um,

820
00:49:56,810 --> 00:50:00,350
what it comes down to is trying to
understand that that sort of strategic

821
00:50:00,351 --> 00:50:05,330
purpose, that alignment between business
objective and human objective. Uh,

822
00:50:05,331 --> 00:50:09,640
and I think if you're looking at a,
at a filter bubble type of example,

823
00:50:09,650 --> 00:50:12,830
for instance,
as one example of something where a,

824
00:50:12,840 --> 00:50:15,260
like a social platform or um,

825
00:50:15,620 --> 00:50:20,620
or an online community is fostering or
are a media company is fostering through

826
00:50:20,901 --> 00:50:23,210
algorithmic content filtering and so on,

827
00:50:23,450 --> 00:50:28,190
the sense of disparity between
people's collective of what is truth.

828
00:50:28,690 --> 00:50:33,680
Um, I think you can probably come to
some understanding at some level of,

829
00:50:33,740 --> 00:50:36,800
of view of that,
that the business objective,

830
00:50:36,801 --> 00:50:41,210
which may be advertising or something
along those lines and the human objective

831
00:50:41,211 --> 00:50:44,850
aren't aligned there.
So I do think that there,

832
00:50:44,851 --> 00:50:48,420
there is still a useful framework there.
Um,

833
00:50:48,560 --> 00:50:53,120
but I do offer some additional ones
in, in tech humanist as well. Um,

834
00:50:53,150 --> 00:50:57,770
it is a really good and important question
and it's an important point for us

835
00:50:57,771 --> 00:51:00,920
all I think to, to consider in
the work that we're doing. Um,

836
00:51:01,010 --> 00:51:06,010
because there's so many met positives
and net goods that come out of let's say

837
00:51:06,051 --> 00:51:10,450
with the social media and the
connectedness we have with each other, uh,

838
00:51:10,520 --> 00:51:15,520
and the way we're able to maintain
relationships with such ease versus,

839
00:51:15,951 --> 00:51:19,970
you know, 20 years ago. But of
course it does come with these,

840
00:51:20,030 --> 00:51:22,520
these sort of associated,
uh,

841
00:51:22,550 --> 00:51:26,210
difficulties and the challenges of making
sure that we're all sort of speaking

842
00:51:26,211 --> 00:51:28,820
the same language,
which at the moment I believe were not,

843
00:51:28,870 --> 00:51:32,780
we were having that question or
that discussion beforehand. Uh,

844
00:51:32,810 --> 00:51:36,110
so I'm going to leave it at that.
I think there's a lot in the book,

845
00:51:36,140 --> 00:51:39,110
which I'll just keep pointing
back to that, uh, that,

846
00:51:39,111 --> 00:51:41,030
that does unpack that a little further.

847
00:51:41,150 --> 00:51:44,450
But I genuinely think that that
framework of understanding what it is the

848
00:51:44,451 --> 00:51:48,440
business is trying to accomplish and
what it is that's good for humanity,

849
00:51:48,530 --> 00:51:50,630
how those things can be
in line and it doesn't,

850
00:51:50,720 --> 00:51:53,390
it doesn't have to come down
to a humanitarian purpose.

851
00:51:53,540 --> 00:51:58,130
It just has to mean that we're
not accelerating something
that is not ultimately

852
00:51:58,131 --> 00:52:02,330
good for humanity. That that I think
is where the alignment comes back to.

853
00:52:04,020 --> 00:52:04,231
Right.

854
00:52:04,231 --> 00:52:07,440
There's a question on the dory and it's
sort of similar to a question that I had,

855
00:52:07,441 --> 00:52:09,780
so I'm going to try to
merge them together. Um,

856
00:52:10,500 --> 00:52:12,450
the question is on the dory,

857
00:52:13,030 --> 00:52:18,030
it starts like this scale tends to force
humans to reduce their variety to adapt

858
00:52:18,871 --> 00:52:23,100
to machines instead of the
other way around. So further,

859
00:52:23,130 --> 00:52:27,870
what about ways to reduce scale that
are compatible with business? Um,

860
00:52:28,440 --> 00:52:33,440
you mentioned distributism
decentralization or something
else and I'll add that.

861
00:52:35,881 --> 00:52:40,110
I think that a lot of this
technological change is really coercive.

862
00:52:40,530 --> 00:52:43,590
Meaning either you get with
it or you get left out,

863
00:52:43,860 --> 00:52:48,840
particularly around the job changes that
you talked about and like how much is

864
00:52:48,841 --> 00:52:53,180
our responsibility to bring people
along. Like to offer the lifeboat. Okay.

865
00:52:53,400 --> 00:52:55,440
And how much do people really
need to get in the boat?

866
00:52:56,910 --> 00:52:57,870
Yeah,
I think that's a,

867
00:52:58,200 --> 00:53:02,880
that's a really difficult thing
to be able to break down in one,

868
00:53:02,881 --> 00:53:04,230
one side or the other,
right?

869
00:53:04,231 --> 00:53:06,870
Like I think that the change
is coming no matter what.

870
00:53:08,040 --> 00:53:13,040
What we find though is the change
is going to be disproportionately,

871
00:53:13,500 --> 00:53:14,670
uh,
felt.

872
00:53:15,090 --> 00:53:18,690
So jobs that are most
likely to be automated,

873
00:53:18,930 --> 00:53:22,500
our jobs like truck driver, cashier, um,

874
00:53:22,950 --> 00:53:24,840
these types of things.
And uh,

875
00:53:24,870 --> 00:53:29,490
what know is that statistically those
jobs are disproportionately how by people

876
00:53:29,491 --> 00:53:33,930
of Color, uh, so that, that uh, not fair,

877
00:53:33,960 --> 00:53:37,230
not equal distribution is happening.
So we,

878
00:53:37,231 --> 00:53:40,890
I think we do have an obligation if we're
trying to create the best futures for

879
00:53:40,891 --> 00:53:41,850
the most people,

880
00:53:42,060 --> 00:53:46,710
which is what I would say is one of the
underpinning ideas or underlying ideas

881
00:53:46,711 --> 00:53:50,700
of tech humanist that we have to be
thinking about how to create a more

882
00:53:50,701 --> 00:53:55,220
equitable distribution of opportunity
and how to make sure that the,

883
00:53:55,250 --> 00:54:00,250
the impact of automation is not going
to destroy one set of humans potential.

884
00:54:02,070 --> 00:54:07,070
And while the increases the potential
for enrichment of another so that that

885
00:54:07,771 --> 00:54:12,390
inequity is going to become even more
extreme than we've already experienced.

886
00:54:12,810 --> 00:54:13,690
So I think it's,
it's an,

887
00:54:13,740 --> 00:54:17,700
it's in our best interest as humans to
think about how to sort of shift that and

888
00:54:17,701 --> 00:54:21,090
how to, how to level that out. Not
that people can't become wealthy,

889
00:54:21,420 --> 00:54:25,800
but that we don't end create this even
more extreme distribution then we already

890
00:54:25,801 --> 00:54:27,780
have.
So I think to some extent it's,

891
00:54:27,810 --> 00:54:30,900
it's a imperative for anyone
who's creating experiences,

892
00:54:30,901 --> 00:54:35,700
which is pretty much everyone that works
in technology that works around, um,

893
00:54:36,150 --> 00:54:40,380
most fields that I've worked around,
healthcare, entertainment and so on.

894
00:54:40,590 --> 00:54:44,820
To think about that, the, to change that's
coming and how to make sure that it is,

895
00:54:45,090 --> 00:54:49,650
uh, that we are creating as much
opportunity there as possible. But yes,

896
00:54:49,651 --> 00:54:50,550
I think there's also,

897
00:54:50,940 --> 00:54:55,940
there's also this kind of new emerging
space around opportunities to retrain

898
00:54:56,641 --> 00:55:00,510
people and repurpose, you know, get people
to understand the new skills that they,

899
00:55:00,511 --> 00:55:02,910
that they might have a, they, I saw,

900
00:55:02,970 --> 00:55:06,640
I shared some really great stats
in tech humanist about, um,

901
00:55:07,200 --> 00:55:11,790
programs that were taking,
let's say, prisoners who were,
uh, who would come out of,

902
00:55:11,880 --> 00:55:16,860
of sort of prison programs and
been able to retrain them into, uh,

903
00:55:17,340 --> 00:55:20,040
into communities,
the jobs that they could keep.

904
00:55:20,040 --> 00:55:24,870
And there was a 0.1% recidivism
within this program. Uh,

905
00:55:24,871 --> 00:55:27,890
so I, I urge you to look
into that example. There's,

906
00:55:28,050 --> 00:55:32,340
there's just so many ways that I think
an ecosystem of answers is really what's

907
00:55:32,341 --> 00:55:32,941
going on here.

908
00:55:32,941 --> 00:55:37,500
We have to own the responsibility as
content creators and experience creators.

909
00:55:37,920 --> 00:55:40,270
And we also have to recognize that,
you know,

910
00:55:40,290 --> 00:55:43,590
this is going to be a broadly
distributed, broadly felt thing, uh,

911
00:55:43,690 --> 00:55:48,000
that is going to have inequitable
inequitability inequity to ways.

912
00:55:48,090 --> 00:55:52,440
So to bring back a word that Paul put
in his question, decentralization,

913
00:55:52,680 --> 00:55:57,680
how does maybe decentralization help
with this by spreading power around or

914
00:55:58,231 --> 00:56:02,100
control around? Maybe just talk
about the centralization for a bit?

915
00:56:02,650 --> 00:56:03,483
Well,

916
00:56:03,580 --> 00:56:07,490
I think the idea of spreading power
runs kind of spreading control around is

917
00:56:07,520 --> 00:56:11,960
interesting. I'm certainly,
we have seen, um, you know,

918
00:56:11,961 --> 00:56:14,440
through user generated content, user, uh,

919
00:56:14,600 --> 00:56:19,040
communities and platforms
like meetup for example.

920
00:56:19,041 --> 00:56:22,910
We were talking about earlier as one way
that they're sort of tools that we can

921
00:56:23,140 --> 00:56:27,070
in the hands of people that allow people
to kind of create communities amongst

922
00:56:27,071 --> 00:56:29,590
themselves, create more
human connection. A,

923
00:56:29,680 --> 00:56:32,080
those are going to be I
think increasingly important.

924
00:56:32,410 --> 00:56:34,810
And the technologies are there to,

925
00:56:34,811 --> 00:56:37,840
to sort of foster that and allowed
discovery within those communities,

926
00:56:38,110 --> 00:56:41,950
allow people to sort of find, you
know, find each other and, and um,

927
00:56:41,980 --> 00:56:44,410
connect more deeply.
But I think, you know,

928
00:56:44,411 --> 00:56:48,730
we just have to be thinking mindfully
about the, the challenge of not,

929
00:56:49,150 --> 00:56:50,650
um, you know, uh,

930
00:56:50,670 --> 00:56:55,090
amplifying those sort of net negatives
as your question was alluding to earlier

931
00:56:55,091 --> 00:56:59,400
with the, the filter
bubble and sign. Uh, so I,

932
00:56:59,420 --> 00:57:04,100
I'd love to hear more specifically,
what about decentralization might be the,

933
00:57:04,120 --> 00:57:06,430
what's sort of nagging my med,

934
00:57:06,450 --> 00:57:11,170
the person's mind that's ans asking
the question or on your mind there.

935
00:57:11,650 --> 00:57:13,390
Uh,
so whoever's asking that,

936
00:57:13,391 --> 00:57:17,440
feel free to ask a secondary
goal and if there's any other,

937
00:57:17,820 --> 00:57:21,330
or maybe for the sake of time we look
for one more question in the room before

938
00:57:21,331 --> 00:57:24,830
we wrap up. Anything else? Yes.

939
00:57:25,570 --> 00:57:29,580
So I have a kind of a thought
about some of this stuff, um,

940
00:57:29,640 --> 00:57:33,210
in terms of, you know,
do you ever take your,

941
00:57:33,211 --> 00:57:38,211
your work and look at it as a lens of
looking at humanity through the lens of

942
00:57:38,430 --> 00:57:41,310
what technology is revealing about people?

943
00:57:42,040 --> 00:57:43,840
Yeah,
I have looked at that,

944
00:57:43,841 --> 00:57:47,860
but I'm very curious as to what is
occurring to you as you think about that.

945
00:57:48,380 --> 00:57:53,240
Well, I mean, I used to do a lot of
community management and so I came out of,

946
00:57:53,270 --> 00:57:58,100
um, you know, be on the bbs is back
in the late eighties, early nineties.

947
00:57:58,101 --> 00:58:03,101
And so it's this kind of thing where
I realized that a lot of what happened

948
00:58:03,651 --> 00:58:07,070
online was just what happened offline.

949
00:58:07,071 --> 00:58:11,610
But at a different scale and
at different localities. Yeah.

950
00:58:12,050 --> 00:58:16,430
And that was sort of a an Aha moment
for me when I was a part of these little

951
00:58:16,431 --> 00:58:20,240
communities back in the bbs days.
So it's,

952
00:58:20,241 --> 00:58:24,230
it's just kind of like as as technology
has become more and more prevalent in

953
00:58:24,231 --> 00:58:28,340
our lives, it's something that
I kind of look at almost flip,

954
00:58:28,610 --> 00:58:32,750
flip the conversation a little bit in my
own head of like, oh, what does it mean?

955
00:58:32,780 --> 00:58:36,470
What does it say about
people given how we're using?

956
00:58:37,500 --> 00:58:40,260
Yeah, that's what I mean it does kind of
a little bit of that points back to the

957
00:58:40,261 --> 00:58:43,950
decentralization discussion as well. But
I liked the aspect that you brought up.

958
00:58:44,020 --> 00:58:44,853
Um,

959
00:58:44,910 --> 00:58:49,250
one of the aspects of this that I
have looked at is that it turns,

960
00:58:49,251 --> 00:58:52,380
it seems to me that our digital selves,

961
00:58:52,381 --> 00:58:57,381
that sort of aggregate set
of characteristics that
gets collected through our

962
00:58:57,631 --> 00:58:59,700
movements,
through our connections,

963
00:58:59,910 --> 00:59:03,000
through our interactions
in social spheres,

964
00:59:03,300 --> 00:59:06,480
that digital self is really
our aspirational self.

965
00:59:06,540 --> 00:59:10,170
Most mostly that we are
saying who we most want to be.

966
00:59:10,740 --> 00:59:15,270
And it seems ironic to me that
that digital self is the self,

967
00:59:15,271 --> 00:59:19,740
the version of ourselves that is
most commodified by business and most

968
00:59:19,950 --> 00:59:23,930
capitalized upon and
manipulated by by business.

969
00:59:24,050 --> 00:59:29,000
I think in our physical manifestations
we are a much less prone to that kind of

970
00:59:29,180 --> 00:59:32,240
manipulation and over capitalization.

971
00:59:32,840 --> 00:59:36,950
Yet this digital south, which is our
aspirational self is prone to that.

972
00:59:36,950 --> 00:59:40,580
So I think this is the opportunity for
us to kind of merge that understanding

973
00:59:40,581 --> 00:59:44,870
and say, well, it, it is a
human that we're looking at
in that digital, you know,

974
00:59:44,871 --> 00:59:48,440
sort of, uh, collected
aggregate data points.

975
00:59:48,800 --> 00:59:51,200
And so we need to be
respectful about that too.

976
00:59:51,470 --> 00:59:55,340
So I think that's flipped version is
to say and others this way that we're

977
00:59:55,341 --> 00:59:59,270
interacting with each other in a way that
represents who we most want to be and

978
00:59:59,271 --> 01:00:00,740
who we most feel we are.

979
01:00:01,040 --> 01:00:04,580
So it's all the more reason why we need
to be respectful with the data that we

980
01:00:04,581 --> 01:00:09,581
collect and monetize and use
within business to inform
our intelligent decisions

981
01:00:09,711 --> 01:00:13,700
in our systems.
So I'd say that that's exciting to me.

982
01:00:13,900 --> 01:00:18,440
Thank you Kate. And thanks Google for
being great audience for cake. Um,

983
01:00:18,580 --> 01:00:22,270
and I guess we owe us all a
round of applause. So thanks

984
01:00:23,810 --> 01:00:24,880
Kate.
Especially.

