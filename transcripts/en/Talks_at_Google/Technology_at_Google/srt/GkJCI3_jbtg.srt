1
00:00:06,060 --> 00:00:11,040
Thank you. So this is the, uh,
this a talk. This is the book

2
00:00:13,060 --> 00:00:16,410
shot a Taz one,
a my first ever clickbait title.

3
00:00:19,120 --> 00:00:21,960
And I liked the cover I've covered
because for two reasons, one,

4
00:00:21,961 --> 00:00:25,770
there's only one button that says okay,
when it's clearly not okay

5
00:00:27,270 --> 00:00:30,780
and it looks like this
thing's been throwing error
messages of the past hour and

6
00:00:30,781 --> 00:00:32,820
nobody has been paying
any attention to them.

7
00:00:36,210 --> 00:00:41,210
So this is a book is a computer and
what I'm writing about is security in a

8
00:00:43,711 --> 00:00:48,180
world where everything is a computer
and I think this is the way we need to

9
00:00:48,181 --> 00:00:52,920
conceptualize the world.
We're building this smart,

10
00:00:52,921 --> 00:00:57,210
this phone is not a phone,
it's a computer that makes phone calls.

11
00:00:57,960 --> 00:01:02,250
And similarly your microwave oven is a
computer that makes things hot and your

12
00:01:02,260 --> 00:01:04,800
refrigerators or computer
than make keeps things cold.

13
00:01:05,190 --> 00:01:09,990
And the ATM machines computer with money
inside and in a car is now a computer

14
00:01:09,991 --> 00:01:12,510
with four wheels and an engine,
actually that's wrong.

15
00:01:12,510 --> 00:01:16,650
A car is about a hundred plus distributed
system with four wheels and an engine.

16
00:01:17,360 --> 00:01:21,720
All right, so, and this is more than
the Internet. It's again at a things,

17
00:01:21,810 --> 00:01:26,370
but it's more than that as well. In
my book, I use the term Internet plus.

18
00:01:26,490 --> 00:01:29,010
I hate having to invent a term,

19
00:01:29,430 --> 00:01:34,430
but there really isn't a term
we have for the Internet,

20
00:01:35,400 --> 00:01:37,740
the computers,
the things,

21
00:01:37,980 --> 00:01:42,720
the big systems like power plants,
the data stores, the processes,

22
00:01:43,020 --> 00:01:43,853
the people,

23
00:01:45,030 --> 00:01:49,260
and it's that holistic system
that I think we need to look at.

24
00:01:49,740 --> 00:01:54,510
Let me look at security.
So if everything is becoming a computer,

25
00:01:55,320 --> 00:02:00,320
it means two things that are relevant
here that all internet security becomes

26
00:02:01,170 --> 00:02:06,170
everything security and all the lessons
and problems of Internet and computers

27
00:02:07,590 --> 00:02:09,810
become problems everywhere.

28
00:02:10,330 --> 00:02:15,330
So let me start with six sort of
quick lessons of computer security,

29
00:02:15,781 --> 00:02:17,670
which will be true about
everything and everywhere.

30
00:02:18,780 --> 00:02:22,500
Some them are obvious and
computers not so obvious elsewhere.

31
00:02:22,890 --> 00:02:26,640
I the first most software is
poorly written and insecure, right?

32
00:02:26,641 --> 00:02:31,641
We know this basic reason is the market
doesn't want to pay for quality software

33
00:02:32,340 --> 00:02:36,000
like good, fast, cheap, pick any two.
We have picked fast and cheap over good,

34
00:02:37,250 --> 00:02:37,530
right?

35
00:02:37,530 --> 00:02:42,450
With with very expensive exceptions
like avionics and the space shuttle.

36
00:02:43,260 --> 00:02:45,480
Most software is really lousy.

37
00:02:46,620 --> 00:02:50,760
It kind of just barely works
now for security, right?

38
00:02:50,761 --> 00:02:54,610
Lots of vulnerabilities, some of those
vulnerable. So I started lots of bugs,

39
00:02:54,611 --> 00:02:55,980
some of those bugs of vulnerabilities.

40
00:02:56,080 --> 00:02:57,750
Some of those vulnerabilities
are exploitable,

41
00:02:58,620 --> 00:03:03,370
which modern software has lots of
exploitable vulnerabilities and that's not

42
00:03:03,371 --> 00:03:04,570
going to change anytime soon.

43
00:03:05,890 --> 00:03:10,120
The second lesson is that the Internet
was never designed with security in mind.

44
00:03:10,330 --> 00:03:12,370
That seems ridiculous today,

45
00:03:13,150 --> 00:03:16,420
but have you think back to
the late seventies and early
eighties there were two

46
00:03:16,421 --> 00:03:20,860
things that were true. One, the Internet
was used for nothing important ever

47
00:03:22,510 --> 00:03:27,510
and two you had to be a member of a
research institution had access to it

48
00:03:29,660 --> 00:03:34,660
and you read the early designers and
they talked about how fear limiting

49
00:03:34,761 --> 00:03:39,761
physical access was a security measure
and in fact you would exclude bad actors

50
00:03:42,170 --> 00:03:45,170
meant that you didn't have
to worry much about security.

51
00:03:46,070 --> 00:03:50,120
So decision was made deliberately
to leave security end point,

52
00:03:50,210 --> 00:03:51,470
not put in the network

53
00:03:53,050 --> 00:03:58,050
but fast forward today and we are still
living with the results of that in Dum,

54
00:03:58,551 --> 00:04:03,530
Dum domain name system and routing and
packet security email addresses sort of

55
00:04:03,531 --> 00:04:08,360
again and again. The protocols don't
have security and we are stuck with them.

56
00:04:09,740 --> 00:04:10,573
Third lesson,

57
00:04:11,210 --> 00:04:15,500
the extensibility of computerized systems
means they can be used against us.

58
00:04:15,770 --> 00:04:19,130
Extensibility is not something that
non computer people are used to.

59
00:04:20,030 --> 00:04:24,140
Basically what I mean by that is you
can't constrain the functionality of a

60
00:04:24,141 --> 00:04:26,000
computer because it's software.

61
00:04:27,530 --> 00:04:31,460
When I was a kid I had a telephone,
big black thing attached to the wall,

62
00:04:31,700 --> 00:04:35,180
great device,
but no matter how hard I tried,

63
00:04:35,240 --> 00:04:37,550
I couldn't make it be anything
other than a telephone.

64
00:04:38,240 --> 00:04:42,170
This is a computer that makes phone
calls. It can do anything you want, right?

65
00:04:42,171 --> 00:04:47,171
There's an app for that because this can
be programmed because it's a computer.

66
00:04:48,050 --> 00:04:52,350
It can do anything. You can't
constrain this functionality, right?

67
00:04:52,380 --> 00:04:53,930
It means several things for security,

68
00:04:54,170 --> 00:04:59,120
hard to test this thing because what
it does changes how it's configured,

69
00:04:59,121 --> 00:05:03,710
changes and it can get additional
features you don't want.

70
00:05:03,860 --> 00:05:05,120
That's what malware is,

71
00:05:07,490 --> 00:05:12,440
so you can put malware on this phone or
on inner catheter to fridgerator in a

72
00:05:12,441 --> 00:05:17,180
way that you can't possibly ever do
it in an old electrical mechanical

73
00:05:17,181 --> 00:05:20,510
refrigerator because
they're not computers.

74
00:05:22,460 --> 00:05:26,810
Fourth lesson is about complexity.
A lot of ways I can say this,

75
00:05:26,811 --> 00:05:30,320
basically the complexity of computers
means attack is easier than defense.

76
00:05:30,830 --> 00:05:32,870
I could spend an hour on that sentence,

77
00:05:34,550 --> 00:05:38,310
but complex systems are hard to secure in.

78
00:05:38,360 --> 00:05:41,240
It's the most complex machine
mankind has ever built by a lot,

79
00:05:42,110 --> 00:05:46,610
which makes this incredibly
hard to secure, hard to design,

80
00:05:46,611 --> 00:05:49,250
securely hard to test,
so everything about it,

81
00:05:50,090 --> 00:05:52,910
it is easier to attack a
system than to defend it.

82
00:05:54,260 --> 00:05:57,170
Our fifth lesson is that
there are new vulnerabilities,

83
00:05:57,320 --> 00:06:00,770
the interconnections as we
connect things to each other,

84
00:06:00,860 --> 00:06:05,390
vulnerabilities and one thing affect
other things. I had lots of examples.

85
00:06:05,630 --> 00:06:10,630
The Dine Bot net at vulnerabilities
in a connected route was digital video

86
00:06:11,751 --> 00:06:16,751
recorders and Webcam is primarily allowed
an on to create a Bot net that dropped

87
00:06:16,851 --> 00:06:17,990
the domain name server.

88
00:06:18,470 --> 00:06:21,620
That intern dropped a couple
of dozen real popular websites.

89
00:06:22,770 --> 00:06:27,770
I 2013 target corporation attack through
a vulnerability in the Hvac contractor

90
00:06:29,271 --> 00:06:31,370
of several of their
mid Pennsylvania stores

91
00:06:32,870 --> 00:06:36,080
earlier this year is a story
of a casino in Las Vegas.

92
00:06:36,081 --> 00:06:37,340
We don't have the name of the casino.

93
00:06:38,030 --> 00:06:43,030
They had their high roller database
stolen and the hackers got in through,

94
00:06:44,070 --> 00:06:47,360
and I'm not making this up,
they're internet connected fish tank.

95
00:06:52,690 --> 00:06:55,040
So right vulnerabilities,

96
00:06:56,390 --> 00:07:00,410
this can be hard cause
sometimes nobody's at fault.

97
00:07:00,980 --> 00:07:05,750
I reblogged a few months ago about a
vulnerability that results from the way

98
00:07:06,050 --> 00:07:09,590
Google treats email addresses,
right?

99
00:07:09,591 --> 00:07:13,850
The dots don't matter for your name and
the way Netflix streets email addresses,

100
00:07:13,910 --> 00:07:18,620
the dots do matter. Turns out you
can play some games with that.

101
00:07:20,300 --> 00:07:23,600
Who Do we blame?
I'm not sure we blame anybody.

102
00:07:25,130 --> 00:07:28,100
There's a vulnerability in PGP, which
is actually not really vulnerability.

103
00:07:28,101 --> 00:07:30,890
Impeached, speeds, vulnerability,
and the way emailers handle PGP,

104
00:07:31,670 --> 00:07:33,650
which everyone's convinced
everyone else was at fault

105
00:07:35,680 --> 00:07:37,450
and these kinds of things is
going to happen more and more.

106
00:07:39,700 --> 00:07:42,880
The last lesson is that
attacks always get better.

107
00:07:44,530 --> 00:07:48,400
Tax always get easier,
faster, cheaper, right?

108
00:07:48,401 --> 00:07:51,190
Some of this is Moore's law.
Computers get faster,

109
00:07:51,580 --> 00:07:55,540
so password guessing gets faster
as computer, it gets faster,

110
00:07:55,541 --> 00:07:58,930
not because we're smarter about it,
but we also get smarter.

111
00:08:00,580 --> 00:08:05,200
Attackers adapt, attackers figure out
new things and expertise flows downhill.

112
00:08:06,160 --> 00:08:08,320
What today is a top secret NSA program?

113
00:08:08,321 --> 00:08:11,980
Tomorrow becomes a Phd thesis and
the next day as a common hacker tool,

114
00:08:12,640 --> 00:08:15,040
and you can see this again and again,

115
00:08:16,230 --> 00:08:20,800
and an example might be a Imsi catchers,
fake cell phone towers, sting rays,

116
00:08:22,780 --> 00:08:26,100
right?
Which when I mean the cause of them is a,

117
00:08:26,250 --> 00:08:31,120
that cell phones don't authenticate to
tower. They automatically trust any,

118
00:08:31,121 --> 00:08:34,300
anybody says, I'm a tower. So
if you put up a fake tower,

119
00:08:34,420 --> 00:08:36,490
you can now query phones and get their,
uh,

120
00:08:36,790 --> 00:08:41,350
addresses that sort of know who's
there. Now, this was something that, uh,

121
00:08:41,470 --> 00:08:45,010
the NSA, the FBI used big
government secret for awhile.

122
00:08:46,790 --> 00:08:49,480
Uh,
expertise flowed downhill a few years ago,

123
00:08:49,481 --> 00:08:53,260
I think it was motherboard
did looked around the DC,

124
00:08:53,261 --> 00:08:54,880
found a couple of dozen of them run by.

125
00:08:54,881 --> 00:08:57,540
We know who around US
government buildings.

126
00:08:58,230 --> 00:09:03,210
I mean right now you can go on alibaba.com
buy one of those things for about a

127
00:09:03,210 --> 00:09:06,660
thousand dollars in China.
They used to send spam two phones.

128
00:09:07,680 --> 00:09:09,330
You get a software defined radio card,

129
00:09:09,331 --> 00:09:13,950
you can download free software
and make your own right.

130
00:09:13,951 --> 00:09:17,520
What started out as something that
was hard to do is now easy to do.

131
00:09:19,260 --> 00:09:24,260
So those are my sort of six lessons that
are going to be true for everything and

132
00:09:25,230 --> 00:09:29,820
none of that is new. But up to now it's
been basically a manageable problem.

133
00:09:31,500 --> 00:09:33,720
But I think that's going to change.

134
00:09:34,620 --> 00:09:37,620
And the reasons are automation,

135
00:09:37,680 --> 00:09:39,780
autonomy and the physical agency,

136
00:09:40,050 --> 00:09:43,050
computers that can do things.

137
00:09:44,780 --> 00:09:49,760
So have you do computer security, you've
heard of the CIA triad, confidentiality,

138
00:09:49,790 --> 00:09:52,610
integrity and availability.
Three basic properties.

139
00:09:52,820 --> 00:09:57,360
We deal with insecurity by and large.
Most of what our issues are,

140
00:09:57,361 --> 00:10:01,250
our confidentiality.
Someone stole and misused our data.

141
00:10:02,660 --> 00:10:06,680
That's ecofacts.
That's office of personnel management.

142
00:10:06,860 --> 00:10:10,490
That's Cambridge Analytica.
That's all the data thefts ever.

143
00:10:13,990 --> 00:10:15,160
But when you get,

144
00:10:16,120 --> 00:10:21,070
when you have computers that can affect
the world in a direct, physical manner,

145
00:10:21,370 --> 00:10:24,250
integrity and availability
become much more serious

146
00:10:25,850 --> 00:10:29,480
because the computers can do stuff.
There's real risk to life and property.

147
00:10:29,930 --> 00:10:30,501
So yes,

148
00:10:30,501 --> 00:10:34,280
I am concerned that someone hacks my
hospital and steals my private patient and

149
00:10:34,281 --> 00:10:38,060
medical records, but I'm much more
concerned that they changed my blood type.

150
00:10:38,970 --> 00:10:39,530
Yeah.

151
00:10:39,530 --> 00:10:44,530
I don't want them to hack my car and use
the Bluetooth microphone to listen in

152
00:10:45,351 --> 00:10:48,140
on conversations, but I really
don't want them to disable a breaks.

153
00:10:49,960 --> 00:10:54,210
Those are data integrity and data
availability attacks respectively.

154
00:10:55,680 --> 00:10:58,350
So suddenly the effects are much greater.

155
00:10:59,640 --> 00:11:04,620
And this is cars, medical devices,
drones, any kind of weapon systems,

156
00:11:04,621 --> 00:11:08,910
thermostats, a power plants, smart city,

157
00:11:08,911 --> 00:11:10,890
anything.
Appliances.

158
00:11:12,610 --> 00:11:13,443
Okay.

159
00:11:13,800 --> 00:11:16,380
I blogged a couple of days ago about,
uh,

160
00:11:16,381 --> 00:11:20,730
an attack where someone just theoretical,
if you can hack enough,

161
00:11:20,731 --> 00:11:25,731
major appliances can turn power
on and off in synchronization and,

162
00:11:26,551 --> 00:11:31,110
uh, affect the load on power plants
and potentially cause blackouts.

163
00:11:31,860 --> 00:11:36,690
Now, very much a side effect. But once
I say that, you say, well yeah, Duh.

164
00:11:36,691 --> 00:11:41,220
Of course you can do that.
Very different sort of attack,

165
00:11:42,240 --> 00:11:46,020
right? There's a fundamental difference
between my spreadsheet crashes.

166
00:11:46,021 --> 00:11:50,940
I may lose my data and my
implanted defibrillator
crashes and I lose my life and

167
00:11:50,941 --> 00:11:55,000
it could be the same,
the same operating system,

168
00:11:55,001 --> 00:11:56,200
the same vulnerability,

169
00:11:56,380 --> 00:12:01,240
the same attack software because
of what the computer can do.

170
00:12:02,020 --> 00:12:03,190
The effects are much difference,

171
00:12:04,570 --> 00:12:07,540
so the same time we're getting
this increased functionality,

172
00:12:09,190 --> 00:12:14,190
there's some long standing
security paradigms that are
failing and I give three.

173
00:12:15,220 --> 00:12:20,220
The first one is patching and patching
is how we get security and it's now

174
00:12:22,151 --> 00:12:27,151
having trouble actually of there's two
reasons why our phones and computers are

175
00:12:27,911 --> 00:12:28,750
secure as they are.

176
00:12:29,170 --> 00:12:34,030
The first is that there are security
engineers at apple and Microsoft at Google

177
00:12:34,270 --> 00:12:37,750
that are designing it as secure as
they are in the first place and those

178
00:12:37,751 --> 00:12:42,751
engineers can quickly write and push
down patches when vulnerabilities are

179
00:12:42,911 --> 00:12:47,230
discovered. That's a pretty
good ecosystem. We do that well.

180
00:12:48,280 --> 00:12:52,930
The problem is it doesn't work for low
cost embedded systems like dvrs and

181
00:12:52,931 --> 00:12:53,764
routers.

182
00:12:54,580 --> 00:12:58,030
These are designed and built
off shore by third parties,

183
00:12:58,270 --> 00:13:01,990
by ad hoc teams that come together,
design them, and then split apart.

184
00:13:02,740 --> 00:13:07,740
I mean there are people who can write
those patches when a vulnerability is

185
00:13:07,871 --> 00:13:12,790
discovered. And even worse, a lot of
these devices have no way to patch them,

186
00:13:14,750 --> 00:13:18,550
right? If your DVR is
vulnerable to the, uh, to the,

187
00:13:19,000 --> 00:13:22,630
to the hack that, uh, allows
to be crucial with botnets,

188
00:13:22,810 --> 00:13:27,100
the only way you can patch it is
to throw it away and buy a new one.

189
00:13:27,820 --> 00:13:32,820
That's the mechanism we have no other
now actually throw it away and buy a new

190
00:13:33,581 --> 00:13:37,930
one is a reasonable security measure.
We do get security.

191
00:13:37,940 --> 00:13:39,790
The fact that the life cycle of,

192
00:13:39,791 --> 00:13:42,670
of phones and computers is
about three to five years.

193
00:13:44,230 --> 00:13:47,890
That's not true for consumer goods.

194
00:13:48,840 --> 00:13:53,050
You're going to replace your DVR
every 10 years, your refrigerator,

195
00:13:53,051 --> 00:13:54,580
every 25 years.

196
00:13:55,480 --> 00:13:59,950
I bought a programmable thermostats
last year. I expect her a place.

197
00:13:59,951 --> 00:14:04,720
It approximately never
think of it when,

198
00:14:04,760 --> 00:14:08,650
think about it in terms of a
car or you buy a car today.

199
00:14:08,820 --> 00:14:12,850
So let's say software is two years old,
you're going to drive it for 10 years,

200
00:14:12,910 --> 00:14:16,810
sell it, someone else buys it,
drives her 10 years, they sell it,

201
00:14:17,530 --> 00:14:20,260
someone else buys it, puts on a
boat, said to the South America,

202
00:14:20,830 --> 00:14:24,680
whereas someone else in their buys it
drives or another 10 to 20 years and you

203
00:14:24,681 --> 00:14:28,480
go home,
find a computer from 1976

204
00:14:30,100 --> 00:14:32,350
try to boot it, try to run
it, try to make it secure.

205
00:14:33,700 --> 00:14:38,700
We actually have no idea how to
secure 40 year old consumer software.

206
00:14:39,970 --> 00:14:44,470
We have the faintest clue
and we need to figure it out.

207
00:14:45,490 --> 00:14:46,960
So what does Christ will maintain?

208
00:14:46,961 --> 00:14:51,961
A test bed of 200 chassies
per vulnerability testing
and for patch testing.

209
00:14:53,780 --> 00:14:54,740
Is that the mechanism?

210
00:14:56,230 --> 00:15:00,230
We're not going to be able to treat
these goods like we treat phones and

211
00:15:00,231 --> 00:15:04,690
computers, you know, we
start forcing the computer,

212
00:15:04,730 --> 00:15:07,760
the computer lifecycle onto
all these other things.

213
00:15:08,050 --> 00:15:09,830
We are probably literally
gonna Cook the planet.

214
00:15:10,880 --> 00:15:12,860
So we need some other
way and we don't have it.

215
00:15:14,510 --> 00:15:17,450
Second thing that's failing as
authentication, right? It's,

216
00:15:17,690 --> 00:15:20,050
we've always been only
okay at authentication,

217
00:15:22,580 --> 00:15:23,413
but

218
00:15:23,700 --> 00:15:25,980
authentication is going to change.
Right now,

219
00:15:26,040 --> 00:15:31,040
fenestration tends to be me
authenticating to some object or service.

220
00:15:32,820 --> 00:15:36,840
What we're going to see an explosion
in his thing to thing authentication.

221
00:15:37,950 --> 00:15:42,480
We're objects into authenticate to
objects and it's going to be a lot of it.

222
00:15:42,481 --> 00:15:47,460
Imagine a a driverless car or even some
kind of computer assisted driving car.

223
00:15:47,880 --> 00:15:51,330
It want to authenticate to
thousands of other cars,

224
00:15:51,540 --> 00:15:56,310
road signs,
emergency vehicles and and signals.

225
00:15:56,520 --> 00:15:59,670
Lots of things.
And we don't know how to do that at scale,

226
00:16:00,460 --> 00:16:05,460
but are you might have a hundred iot
objects in your orbit to authenticate to

227
00:16:07,621 --> 00:16:11,670
each other. Let's 10,000
authentications, right? 1,000 objects,

228
00:16:11,730 --> 00:16:13,140
a million authentications.
I mean,

229
00:16:13,141 --> 00:16:16,980
right now this is tends to be our iot hub.

230
00:16:17,650 --> 00:16:21,690
Do you have an Iot? Anything you
like to control it via your phone?

231
00:16:22,440 --> 00:16:27,440
I'm not sure that scales to that many
things and while we can do things,

232
00:16:29,060 --> 00:16:33,600
think that's an occasion.
It's very much a deliberate.

233
00:16:33,780 --> 00:16:35,550
So right now when I get into my car,

234
00:16:35,760 --> 00:16:40,350
this phone authenticates the
car automatically, right?
That works. Bluetooth works,

235
00:16:40,560 --> 00:16:45,560
but it works because I was there to set
it up and I'll do that for 10 things for

236
00:16:47,011 --> 00:16:51,450
20 things on not doing it for a thousand.
I'm not doing it for a million.

237
00:16:52,710 --> 00:16:57,420
So we need some way to do this automatic
thing of the thing of authentication at

238
00:16:57,421 --> 00:16:58,980
scale and we don't have it.

239
00:17:00,660 --> 00:17:03,480
The third thing that's
failing is supply chain.

240
00:17:03,990 --> 00:17:08,990
Supply chain security is
actually insurmountably hard now.

241
00:17:09,120 --> 00:17:12,330
We've seen it.
You've seen the papers in the past year.

242
00:17:12,480 --> 00:17:15,720
It's been one of two stories.
It's Kaspersky, right?

243
00:17:15,721 --> 00:17:20,070
Should we trust a Russian
made antivirus program and uh,

244
00:17:20,160 --> 00:17:24,240
who ae and ZTE should we trust
Chinese made phone equipment.

245
00:17:25,140 --> 00:17:28,380
But that really is just
the tip of the iceberg.

246
00:17:29,220 --> 00:17:32,880
There are other stories,
not just the U s turns out in 2014,

247
00:17:32,910 --> 00:17:37,080
China banned Kaspersky. They
also ban SYMANTEC. By the way,

248
00:17:38,440 --> 00:17:40,830
a 2017 I started from India.

249
00:17:41,130 --> 00:17:45,780
Then if I'm 45 Chinese phone apps
that they say shouldn't be used, uh,

250
00:17:46,050 --> 00:17:51,050
in 1997 and the people remember there a
worries in the u s about checkpoint and

251
00:17:52,110 --> 00:17:56,010
Israeli made security product.
You know, should we trust it?

252
00:17:57,370 --> 00:18:02,280
I also, I like a member, a 2008
program called Musha secrets,

253
00:18:03,300 --> 00:18:08,190
which was an isis created encryption
program because of course you can't trust

254
00:18:08,191 --> 00:18:12,690
western encryption programs but you know,

255
00:18:12,720 --> 00:18:16,950
the country of origin of the product
is just the tip of the iceberg.

256
00:18:18,360 --> 00:18:22,050
Where are the chips made?
Where is the software written?

257
00:18:22,600 --> 00:18:27,420
Where is the device fab where
the programmers are far from.

258
00:18:29,930 --> 00:18:34,240
I mean this iPhone probably has won a
couple of a hundred different passports

259
00:18:35,320 --> 00:18:36,490
that are programming this thing.

260
00:18:36,520 --> 00:18:41,520
It's not need in the U S and every
part of the chain is a vulnerability.

261
00:18:44,230 --> 00:18:47,370
They are, they're a paper showing
how you can take a, you know,

262
00:18:47,380 --> 00:18:52,380
a good chip design that the masks and
maliciously put in another layer and

263
00:18:54,671 --> 00:18:58,090
calmed my security. The chip without the
designer's knowing it and it doesn't,

264
00:18:58,091 --> 00:19:03,070
it doesn't show up in testing. There was
another paper about two years ago. Uh,

265
00:19:03,071 --> 00:19:06,700
you can, uh, hack an iPhone through
a malicious replacement screen,

266
00:19:08,940 --> 00:19:12,520
right? You have to trust
every piece of the system.

267
00:19:14,440 --> 00:19:18,160
The distribution mechanisms.
We've,

268
00:19:18,161 --> 00:19:23,090
we've seen backdoors in a
Cisco equipment. Uh, Mohammed,

269
00:19:23,100 --> 00:19:26,090
the NSA intercepted, uh, the, uh,

270
00:19:26,470 --> 00:19:29,200
Cisco routers being sent to
the Syrian telephone company.

271
00:19:29,920 --> 00:19:32,140
That was one of the greatest
pictures from the stolen documents.

272
00:19:32,830 --> 00:19:35,050
We've seen fake apps in
the Google play store.

273
00:19:37,350 --> 00:19:41,150
We know that a Russia
attacked Ukraine through a,

274
00:19:41,151 --> 00:19:42,480
a software update mechanism.

275
00:19:43,980 --> 00:19:48,240
I think my favorite story is,
this is a hard one. In 2003,

276
00:19:48,690 --> 00:19:50,580
there was actually a very clever,

277
00:19:50,581 --> 00:19:53,910
very subtle backdoor that
almost made it into Linux.

278
00:19:55,050 --> 00:19:57,750
We caught it and we kind
of just barely caught it.

279
00:19:58,290 --> 00:20:02,490
We got very lucky there and,
and you look at the code,

280
00:20:03,480 --> 00:20:07,170
it's, it really is hard. You have
to look for it to see the back door.

281
00:20:08,110 --> 00:20:11,370
Now that could have easily gotten in.
We don't know what else has gotten in,

282
00:20:11,640 --> 00:20:15,000
in watts
and solving this is hard.

283
00:20:15,720 --> 00:20:20,580
No one wants a US only iPhone is probably
a impossible and beetle cost 10 x.

284
00:20:21,870 --> 00:20:26,100
Now our industry is at
every level international.

285
00:20:26,310 --> 00:20:27,840
It is deeply international

286
00:20:29,850 --> 00:20:34,080
from the programmers to the, to the,
to the, the objects to the cloud,

287
00:20:34,081 --> 00:20:38,940
the services. We will not be
able to solve the seasonally.

288
00:20:40,920 --> 00:20:41,700
So,

289
00:20:41,700 --> 00:20:46,500
and a lot of ways this is a perfect
storm and things are failing just as

290
00:20:46,501 --> 00:20:49,350
everything is interconnected and will,

291
00:20:49,360 --> 00:20:54,360
and I think we've been okay
with a unregulated tech
space because fundamentally

292
00:20:55,260 --> 00:20:57,550
didn't matter.
And that's changing.

293
00:20:59,320 --> 00:21:04,060
And I think this is primarily a
policy problem. And in my book,

294
00:21:04,061 --> 00:21:06,580
I spend most of the time on policy

295
00:21:08,230 --> 00:21:12,340
and I talk about a lot of different
policy levers we have to improve this.

296
00:21:13,540 --> 00:21:18,010
Talking about standards,
regulations, liabilities, courts,

297
00:21:18,040 --> 00:21:22,330
international treaties think it's
a very hard political battle.

298
00:21:22,950 --> 00:21:26,410
And I don't think we're going to have
in the u s until a catastrophic event.

299
00:21:28,130 --> 00:21:32,890
You know, I look more to Europe to
lead. I couldn't go through all of this,

300
00:21:32,891 --> 00:21:35,830
but I want it to want to give sort
of two principles I want to pull out.

301
00:21:36,370 --> 00:21:38,290
The first is that the fence must dominate.

302
00:21:38,950 --> 00:21:43,950
I think we as a national policy need to
decide that defense wins that no longer

303
00:21:46,151 --> 00:21:51,151
can we accept insecurity for offense
purposes that as these computers become

304
00:21:51,911 --> 00:21:55,180
more critical,
defense is more important.

305
00:21:56,430 --> 00:22:00,040
My gone are the days when you can
attack their stuff and defend our stuff.

306
00:22:00,280 --> 00:22:01,720
Everyone uses the same stuff.

307
00:22:02,290 --> 00:22:07,290
We all use TCP IP and Cisco Routers
and Microsoft Word and pdf files,

308
00:22:09,370 --> 00:22:12,220
and it's just one world,
one network, one answer.

309
00:22:12,760 --> 00:22:14,560
Either we secure our stuff,

310
00:22:15,250 --> 00:22:18,640
thereby incidentally
securing the bad guy's stuff,

311
00:22:19,540 --> 00:22:22,570
or we keep our stuff vulnerable
in order to attack the bad guys,

312
00:22:22,720 --> 00:22:27,400
thereby incidentally rendering us
vulnerable and that's our choice

313
00:22:30,190 --> 00:22:33,890
and it means it means a whole
bunch of things to disclose and fix

314
00:22:33,891 --> 00:22:37,660
vulnerabilities to design for security,
not for surveillance,

315
00:22:38,140 --> 00:22:43,140
encrypt as much as possible to
really separate security from spying,

316
00:22:45,190 --> 00:22:49,420
make law enforcement smarter so they can
actually solve crimes even though their

317
00:22:49,421 --> 00:22:53,660
security
and create better norms.

318
00:22:55,130 --> 00:22:58,100
What other principle is that we
need to build for resilience?

319
00:22:59,030 --> 00:23:00,560
When you start designing systems,

320
00:23:00,561 --> 00:23:04,880
assuming they will fail and
how do we contain failures?

321
00:23:04,881 --> 00:23:09,020
How do we avoid catastrophes?
How do we fail safe for fail secure?

322
00:23:10,130 --> 00:23:12,770
Where can we remove
functionality or delete data?

323
00:23:13,760 --> 00:23:18,100
How do we have systems monitor other
systems to try to provide, you know,

324
00:23:18,250 --> 00:23:19,940
some level of redundancy.

325
00:23:21,950 --> 00:23:26,950
And I think the missing piece here has
government that the market will not do

326
00:23:27,411 --> 00:23:31,950
this on its own.
But I have a problem,

327
00:23:32,930 --> 00:23:33,211
you know,

328
00:23:33,211 --> 00:23:37,170
handing us to government because there
really isn't an existing regulatory

329
00:23:37,171 --> 00:23:41,280
structure that could tackle this at a
systemic level grads because there's a

330
00:23:41,281 --> 00:23:45,650
mismatch between the way government works
and the way tech works at government

331
00:23:45,651 --> 00:23:49,550
operates in silos.
The FAA regulate aircraft,

332
00:23:49,551 --> 00:23:54,551
the FDA regulates medical devices
or the FTC regulates consumer goods.

333
00:23:55,040 --> 00:23:57,230
Someone else does those cars

334
00:23:59,450 --> 00:24:03,620
and each agency will have its own
rules and on approach and own systems.

335
00:24:04,380 --> 00:24:08,750
And that's not the internet or the
Internet is this freewheeling system of

336
00:24:08,751 --> 00:24:13,730
integrated objects and and networks and
it grows horizontally and it kicks down

337
00:24:13,731 --> 00:24:18,731
barriers and it makes people able
to do things never could do before.

338
00:24:19,640 --> 00:24:22,600
And all of that rhetoric is true.
I mean,

339
00:24:22,610 --> 00:24:27,610
right now this device logs
my health information,

340
00:24:29,360 --> 00:24:31,520
communicates with my car,

341
00:24:33,450 --> 00:24:37,800
monitors my energy use and
makes phone calls, right?

342
00:24:37,801 --> 00:24:41,100
That's four different, probably
five different regulatory agencies.

343
00:24:41,910 --> 00:24:45,780
And this is just getting started,
right?

344
00:24:45,850 --> 00:24:48,200
We're not sure how to do this.

345
00:24:50,080 --> 00:24:55,010
So in my book I talk about a
bunch of options and what I have,

346
00:24:56,540 --> 00:25:00,030
and I think we're going to, we're
going to get eventually is a, a,

347
00:25:00,040 --> 00:25:05,040
a new route and a new government agency
that will have some jurisdiction over

348
00:25:05,421 --> 00:25:09,160
computers. Uh, this is a
hard sell to a, you know,

349
00:25:09,290 --> 00:25:13,970
low government crowd.
But there is a lot of precedent for this.

350
00:25:14,270 --> 00:25:15,260
In the last century,

351
00:25:15,410 --> 00:25:18,620
pretty much all major technologies
led to the formation of new government

352
00:25:18,621 --> 00:25:23,270
agencies. All right? Cars
did, planes did, radio did,

353
00:25:23,480 --> 00:25:28,480
nuclear power did because government
needs to consolidate its expertise.

354
00:25:31,890 --> 00:25:36,210
So, and that's what happens first.
And then there is need to regulate.

355
00:25:37,930 --> 00:25:39,340
I don't think markets solved us,

356
00:25:39,820 --> 00:25:44,530
Marcus or short term markets or profit
motivated markets don't take society into

357
00:25:44,531 --> 00:25:48,850
account. Markets can't solve collection
at the collective action problems.

358
00:25:51,020 --> 00:25:54,360
So of course there are lots
of problems with this, right?

359
00:25:54,370 --> 00:25:56,570
Governments are terrible.
Being proactive,

360
00:25:58,300 --> 00:26:01,070
a regulatory capture is,
are we is a real issue.

361
00:26:03,490 --> 00:26:07,420
I think there are differences between
security and safety that matter here.

362
00:26:07,930 --> 00:26:12,040
It's safety against things
like a hurricane and secure
it against an adaptive,

363
00:26:12,041 --> 00:26:16,120
malicious, intelligent adversary are,
are sort of very different things.

364
00:26:17,020 --> 00:26:18,130
And you know,

365
00:26:18,131 --> 00:26:22,900
we live in a fast moving technological
environment and it's hard to see how

366
00:26:22,901 --> 00:26:25,510
government can stay ahead of tech.

367
00:26:26,640 --> 00:26:29,800
There's something that's changed in the
past couple of decades or tech moves

368
00:26:29,801 --> 00:26:34,600
faster than policy.
The Devil's in the details and,

369
00:26:34,601 --> 00:26:37,210
and I, I don't have them, but

370
00:26:38,250 --> 00:26:38,910
yeah,

371
00:26:38,910 --> 00:26:43,200
this is a conversation that we need to
have because I believe that governments

372
00:26:43,320 --> 00:26:48,150
get involved regardless that the risks
are too great and the stakes are too

373
00:26:48,151 --> 00:26:52,830
high, right? Or governments are
ready involved in physical systems.

374
00:26:53,160 --> 00:26:58,160
They already regulate cars and
appliances and toys and power plants

375
00:27:01,450 --> 00:27:02,650
and medical systems.

376
00:27:04,440 --> 00:27:09,440
So they already have this ability and
need and desire to regulate those things

377
00:27:11,431 --> 00:27:15,970
as computers. But how do we give
them the expertise to do it right?

378
00:27:16,730 --> 00:27:17,080
Uh,

379
00:27:17,080 --> 00:27:22,080
my guesses are the courts are going to
do some things relatively quickly because

380
00:27:23,051 --> 00:27:27,700
cases will appear and that the
regulatory agencies will follow.

381
00:27:28,780 --> 00:27:33,780
I think Congress comes last but don't
count them out and nothing motivates a,

382
00:27:34,311 --> 00:27:36,070
a, a government. The US
government like fear.

383
00:27:37,340 --> 00:27:37,820
Okay.

384
00:27:37,820 --> 00:27:40,410
When you think back to the
terrorist attacks of September 11th,

385
00:27:40,860 --> 00:27:45,860
we had a very small government
administration create a
massive bureaucracy kind

386
00:27:46,521 --> 00:27:50,430
of out of thin air.
And that was all fear motivated.

387
00:27:52,260 --> 00:27:57,260
And when something happens that will
be a push that something must be done.

388
00:27:58,770 --> 00:28:03,210
And we are past the choice of government
involvement versus no government

389
00:28:03,211 --> 00:28:03,870
involvement.

390
00:28:03,870 --> 00:28:07,520
Our choice now is smart governor
involvement versus stupid government

391
00:28:07,521 --> 00:28:10,680
involvement.
And the more we can talk about this now,

392
00:28:11,040 --> 00:28:14,610
the more we could make sure it's smart.
Uh,

393
00:28:14,800 --> 00:28:16,770
am I guess as any good regulation will,
uh,

394
00:28:17,090 --> 00:28:21,690
send private industry that I think the
reason we have such bad security is not

395
00:28:22,020 --> 00:28:24,990
technological,
it's more economic.

396
00:28:25,770 --> 00:28:28,710
There's lots of good tech and you know,

397
00:28:28,711 --> 00:28:33,690
while some of these problems are hard,
they're like send a man to the moon hard.

398
00:28:33,691 --> 00:28:35,610
They're not Fastenal.
I travel hard.

399
00:28:36,610 --> 00:28:37,300
Yeah.

400
00:28:37,300 --> 00:28:41,350
And once the incentives
are in place in this story,

401
00:28:41,351 --> 00:28:43,600
we'll figure out how to do it right.
I mean a good example,

402
00:28:43,601 --> 00:28:47,680
it might be credit cards.
In the early days of credit cards,

403
00:28:47,710 --> 00:28:52,710
we were all liable for a for fraud and
losses that change in 1978 the fair

404
00:28:53,441 --> 00:28:54,400
credit reporting act,

405
00:28:54,940 --> 00:28:58,840
that's what a mandated the maximum
liability for credit card fraud for the

406
00:28:58,841 --> 00:29:02,440
consumer is $50 and he
understood what that means.

407
00:29:02,441 --> 00:29:05,380
That means I could take my card
flinging in the middle of this room,

408
00:29:05,800 --> 00:29:10,800
give you all lessons on forging my
signature and my maximum liability is $50

409
00:29:11,260 --> 00:29:12,970
right?
It might be worth it for the fun,

410
00:29:16,210 --> 00:29:21,040
but what that meant, right? That change
that even if the consumer is at fault,

411
00:29:21,250 --> 00:29:23,230
the credit card company is liable.

412
00:29:24,940 --> 00:29:29,940
That led to all sorts of security measures
that led to online verification of

413
00:29:31,721 --> 00:29:32,160
uh,

414
00:29:32,160 --> 00:29:37,160
of credit and card validity that led to
anti forgery measures like the holograms

415
00:29:39,401 --> 00:29:44,290
and the micro printing blood to mailing
the card and the activation information

416
00:29:44,291 --> 00:29:47,170
separately and requiring you to
call from a known phone number.

417
00:29:48,100 --> 00:29:49,360
And actually most importantly,

418
00:29:49,480 --> 00:29:53,540
that enabled the backend expert
systems that troll the, uh,

419
00:29:54,040 --> 00:29:57,820
credit, the transaction database,
looking unfortunately spending patterns,

420
00:29:58,300 --> 00:30:03,300
none of that would've happened if the
consumers reliable because the consumers

421
00:30:03,550 --> 00:30:06,340
had no ability to implement any of that.

422
00:30:07,090 --> 00:30:11,920
You want the entity that can fix the
problem to be responsible for the problem.

423
00:30:12,040 --> 00:30:13,360
That is just smart policy.

424
00:30:15,100 --> 00:30:19,090
So I see a lot of innovation that's not
happening because the incentives are

425
00:30:19,091 --> 00:30:23,170
mismatched. So I think Europe
is moving in this direction,

426
00:30:24,580 --> 00:30:25,030
right?

427
00:30:25,030 --> 00:30:29,410
The EU is right now the regulatory
superpower on the planet and they are not

428
00:30:29,411 --> 00:30:34,030
afraid to use their power. We've seen
that in the Gdpr in the privacy space.

429
00:30:34,510 --> 00:30:38,020
I think they're going to turn
to security next. I mean,

430
00:30:38,021 --> 00:30:41,940
they're already working on what
responsible disclosure means. Uh,

431
00:30:42,280 --> 00:30:46,600
there's that you have to
see on manufactured goods.
Does that label called CE?

432
00:30:47,080 --> 00:30:50,070
That's an EU label basically means uh,

433
00:30:50,320 --> 00:30:52,570
meets all applicable standards.

434
00:30:53,320 --> 00:30:58,280
They're working on standards for
cybersecurity and you know, useful.

435
00:30:58,390 --> 00:31:01,450
See them get incorporated
trade agreements into Gat.

436
00:31:02,950 --> 00:31:07,240
And there's an interesting,
a rising tide effect.

437
00:31:08,530 --> 00:31:12,580
It's not necessarily obvious the the car
you buy a United States is not the car

438
00:31:12,581 --> 00:31:13,690
you buy in Mexico.
Right?

439
00:31:13,691 --> 00:31:16,840
Environmental laws are different and
the cars are tuned to the different laws

440
00:31:17,980 --> 00:31:21,550
but not true in the computer space.

441
00:31:22,930 --> 00:31:23,763
The uh,

442
00:31:24,460 --> 00:31:26,860
the Facebook you get is pretty
much the same everywhere.

443
00:31:28,480 --> 00:31:31,990
And if you can imagine there's
some security regulation on a toy,

444
00:31:32,240 --> 00:31:33,580
the manufacturer meets it.

445
00:31:33,730 --> 00:31:35,710
They're not going to have a
separate build for United States.

446
00:31:35,711 --> 00:31:37,630
They're going to sell it
everywhere cause it's easier

447
00:31:40,360 --> 00:31:43,240
that, and they'll, there'll
be times when that's not true.

448
00:31:43,720 --> 00:31:48,100
I think Facebook would like to be able
to differentiate between someone who is

449
00:31:48,370 --> 00:31:52,060
subject to the Gdpr is somebody who's
not cause there's more revenue to be

450
00:31:52,061 --> 00:31:56,500
gained through them, the greater
surveillance. But when you get to things,

451
00:31:56,501 --> 00:32:00,370
I think it's more likely that it will
be a rising tide and we all benefit.

452
00:32:01,900 --> 00:32:06,220
United States are look to the states more
specifically New York, Massachusetts,

453
00:32:06,221 --> 00:32:10,420
California, which are more
aggressive, uh, in this space.

454
00:32:13,540 --> 00:32:14,530
But I think this is coming

455
00:32:17,950 --> 00:32:18,783
and

456
00:32:19,080 --> 00:32:22,110
I want to close with a,
I guess a call.

457
00:32:23,820 --> 00:32:28,170
What we need to do is to
get involved in policy.

458
00:32:29,100 --> 00:32:32,700
Technologists need to get in
policy to get involved in policy.

459
00:32:32,940 --> 00:32:35,520
As Internet security becomes everything.
Security,

460
00:32:36,330 --> 00:32:41,090
Internet security technology becomes more
important to overall security policy.

461
00:32:42,230 --> 00:32:47,060
And all of the security policy
debates will have strong technological

462
00:32:47,061 --> 00:32:47,894
components.

463
00:32:48,770 --> 00:32:53,720
We will never get the policy right if
the policy makers get the tech wrong,

464
00:32:54,050 --> 00:32:58,850
right? It will all look like the Facebook
hearings, which were embarrassing.

465
00:33:00,320 --> 00:33:03,620
And you see it even in some of you
see it in the going dark debate.

466
00:33:04,010 --> 00:33:05,810
You see it in the equities debate,

467
00:33:05,990 --> 00:33:10,990
you see it and voting machine debates in
driverless car security debates that we

468
00:33:12,591 --> 00:33:17,470
need technologists in the room
during policy discussions, right?

469
00:33:17,560 --> 00:33:18,590
We have to fix this.

470
00:33:19,950 --> 00:33:24,570
We need technologists on congressional
staffs at Ngos doing investigative

471
00:33:24,571 --> 00:33:28,500
journalism in the government
agencies in the White House,

472
00:33:31,240 --> 00:33:32,073
right?

473
00:33:32,250 --> 00:33:34,020
We need to make this happen.

474
00:33:35,040 --> 00:33:39,000
And right now you just
don't have that ecosystem.

475
00:33:39,720 --> 00:33:44,160
So you think about a public interest law
1970s there was no such thing as public

476
00:33:44,161 --> 00:33:46,320
interest law.
They would actually wasn't.

477
00:33:46,860 --> 00:33:50,370
It was created primarily by the
Ford Foundation. Oddly enough,

478
00:33:51,120 --> 00:33:53,370
that funded law clinics,

479
00:33:53,490 --> 00:33:58,490
funded internships in different NGOs and
now you want to make partner at a major

480
00:34:00,211 --> 00:34:03,150
law firm. You are expected
to do public interest work

481
00:34:06,090 --> 00:34:09,270
today at Harvard,
Harvard Law School,

482
00:34:09,420 --> 00:34:14,420
20% of the graduating class doesn't
go into corporations or law firms.

483
00:34:14,730 --> 00:34:19,620
They go into public interest law and the
university has soul searching seminars

484
00:34:19,621 --> 00:34:21,330
because that percentage is so low.

485
00:34:22,990 --> 00:34:27,790
Percentage of computer science
graduates is probably zero. Right?

486
00:34:27,791 --> 00:34:31,180
We need to fix that. And that's
more than just, you know,

487
00:34:32,110 --> 00:34:35,590
every Google or needs to do an internship
because there aren't spaces for those

488
00:34:35,591 --> 00:34:38,980
people.
So we got to fix the supply,

489
00:34:39,080 --> 00:34:41,980
got to fix the demand and the
ecosystem to link the two.

490
00:34:43,080 --> 00:34:44,770
And this is of course
the bigger than security.

491
00:34:45,220 --> 00:34:49,660
I think pretty much all the major
societal problems of this century have a

492
00:34:49,661 --> 00:34:53,650
strong tech component or climate change,
future of work,

493
00:34:54,070 --> 00:34:55,270
farm policy.

494
00:34:57,940 --> 00:35:02,940
And we need to be in the room
or bad policy happens to us.

495
00:35:06,080 --> 00:35:07,250
So that's my talk.

496
00:35:07,251 --> 00:35:10,880
There's of course a lot more in the book
that I didn't say and I'm happy to take

497
00:35:10,881 --> 00:35:11,714
questions.

498
00:35:13,820 --> 00:35:18,820
[inaudible] alright.

499
00:35:24,970 --> 00:35:28,900
Do you imagine that some
of the sociopolitical,

500
00:35:29,580 --> 00:35:33,640
uh, things that we're seeing crop
up fit within this framework? Uh,

501
00:35:33,670 --> 00:35:37,180
or do you think that that
might be an entirely separate,

502
00:35:37,590 --> 00:35:39,930
that needs an entirely
separate set of solutions?

503
00:35:40,160 --> 00:35:42,250
I think it's related.
I mean there's problems,

504
00:35:42,260 --> 00:35:44,570
I'm talking about a
pretty purely technical,

505
00:35:44,870 --> 00:35:49,870
the problem is of a internet as proper
as a propaganda vehicle are I think much

506
00:35:50,601 --> 00:35:52,990
more systemic and societal.
Uh,

507
00:35:53,210 --> 00:35:57,020
I do blame surveillance capitalism
for, for a bunch of it, right?

508
00:35:57,021 --> 00:36:02,021
The business model that a prioritizes
engagement rather than quality has learned

509
00:36:03,801 --> 00:36:08,490
that if you're pissed off,
you stay on Facebook more so.
So I think there are, are,

510
00:36:08,520 --> 00:36:13,340
are pieces that fit in. So
some related, some different,

511
00:36:13,810 --> 00:36:17,180
I mean, you shouldn't be talking a lot
about policy that the United States,

512
00:36:17,480 --> 00:36:18,950
it's to some extent the EU can do.

513
00:36:18,951 --> 00:36:22,280
But I wonder what do you think
will happen as policy everywhere?

514
00:36:22,640 --> 00:36:26,080
We policies local, the rns global.
Right? How's that going to play out?

515
00:36:26,180 --> 00:36:30,710
So I think that never goes away. And
some that's going to be the rising tide.

516
00:36:30,711 --> 00:36:34,820
I talked about that especially
with, you know, less about privacy.

517
00:36:34,821 --> 00:36:36,260
But when you get to safety,

518
00:36:36,740 --> 00:36:41,740
I think it's more likely that we benefit
from a European regulation that ensures

519
00:36:43,251 --> 00:36:44,330
that,
you know,

520
00:36:44,331 --> 00:36:48,170
the smart vacuum cleaner you bought
can't be taken over by somebody and then

521
00:36:48,171 --> 00:36:49,500
like attack you and trip you.
Right.

522
00:36:49,640 --> 00:36:53,090
Well W W we likely the benefit
of that more than, look,

523
00:36:53,091 --> 00:36:56,510
you can't have a microphone
on the thing. Okay. Uh,

524
00:36:57,680 --> 00:37:02,680
we have to assume that there will be
malicious things in whatever system we

525
00:37:03,231 --> 00:37:06,410
have.
And so if we have a US only regulation,

526
00:37:06,620 --> 00:37:09,380
it'll clean up a lot of the problem
because Walmart and we'll be able to sell

527
00:37:09,381 --> 00:37:13,550
the bad stuff. But you can still buy it
and mail order from alibaba.com right?

528
00:37:13,551 --> 00:37:17,180
So there will be some stuff in
the network that is malicious,

529
00:37:17,480 --> 00:37:19,520
much lower percentage is your problem.

530
00:37:20,000 --> 00:37:21,620
We're still going to
have to deal with that.

531
00:37:21,810 --> 00:37:25,070
And I don't think it ever goes away
because we're not going to have a world

532
00:37:25,071 --> 00:37:30,071
government though will be a jurisdiction
or they will be homebrew stuff that

533
00:37:30,771 --> 00:37:33,830
doesn't meet whatever regs we have that.
But that will always happen.

534
00:37:34,600 --> 00:37:38,290
You talk about the need for intelligent
technologists to get involved with main

535
00:37:38,291 --> 00:37:43,030
policy, but there are only so many
hours in a day and we'd probably,

536
00:37:43,031 --> 00:37:46,490
most of us would be taking a huge pay
cut to go work in government and under

537
00:37:46,510 --> 00:37:48,940
expertise there. So what, how
can we fix the incentives there?

538
00:37:49,050 --> 00:37:51,470
So some of it is desire.
I mean,

539
00:37:51,510 --> 00:37:55,980
I know ACLU attorneys that are making a
third of what they would make at a big

540
00:37:55,981 --> 00:38:00,280
law firm and they get more resumes
than they have positions, right?

541
00:38:00,281 --> 00:38:01,500
So it works in law.

542
00:38:02,040 --> 00:38:06,600
The desire to actually make the world
better turns out to be a prime motivator.

543
00:38:07,170 --> 00:38:10,890
So I think once we have the ecosystem,
we will get the supply.

544
00:38:11,820 --> 00:38:15,150
I think that enough of us will say,
you know, we've had great careers.

545
00:38:15,151 --> 00:38:20,151
We're going to take a break or are we
going to do something before we go work at

546
00:38:20,251 --> 00:38:25,110
a startup or a big company or maybe
that mean there will be a use for

547
00:38:25,111 --> 00:38:29,880
sabbaticals like you see in law firms
or you know your bits of pro bono work,

548
00:38:30,210 --> 00:38:34,170
you know like a 20% project.
So I mean, yes you will.

549
00:38:34,200 --> 00:38:35,470
People will be making less money.

550
00:38:35,920 --> 00:38:40,920
I don't think that is going
to harm the system or you can,

551
00:38:41,071 --> 00:38:42,100
we just need to get the system working.

552
00:38:42,720 --> 00:38:47,720
The most jarring thing I saw you write
as a Googler was a date day is a toxic

553
00:38:49,371 --> 00:38:50,204
assets.

554
00:38:50,570 --> 00:38:53,260
Uh, what, what can I know a lot of.

555
00:38:56,610 --> 00:38:57,443
So

556
00:38:59,370 --> 00:39:03,450
the promise of big data has been save
it all figured out what to do with it

557
00:39:03,451 --> 00:39:08,451
later and that's been driven by the
marginal cost of saving it has dropped to

558
00:39:09,871 --> 00:39:13,920
zero is basically cheaper now to save
it all and to figure out what to save.

559
00:39:15,000 --> 00:39:18,630
This storage is free processing is free,
transport is free.

560
00:39:19,500 --> 00:39:24,500
But it turns out that data is a toxic
asset that for most companies having it is

561
00:39:26,770 --> 00:39:29,790
enormous liability because
someone's going to hack it.

562
00:39:29,791 --> 00:39:31,740
So it's going to get stolen,
you're going to lose it.

563
00:39:32,610 --> 00:39:37,610
And I think we need to start talking
about data not as this sort of magic

564
00:39:38,041 --> 00:39:38,874
goodness,

565
00:39:39,720 --> 00:39:44,720
but it no decays and value the end that
there are dangers and storing it or the

566
00:39:46,390 --> 00:39:50,550
the best way to secure your data is to
delete it and you're going to delete it

567
00:39:50,730 --> 00:39:53,190
if you don't know if you
know you don't need it.

568
00:39:54,400 --> 00:39:57,150
I swear I've seen lots of studies on data,

569
00:39:57,390 --> 00:40:01,230
on and shopping preferences and it
turns out some pieces of data are very

570
00:40:01,231 --> 00:40:03,480
valuable in a lot of it
just isn't very valuable.

571
00:40:03,870 --> 00:40:08,520
So is it worth the extra quarter percent
of accuracy to have this day that is

572
00:40:08,521 --> 00:40:12,660
potentially dangerous and we'll get you
find or embarrassed and you'll take a

573
00:40:12,661 --> 00:40:14,160
stock,
takes a hit if it gets stolen.

574
00:40:14,880 --> 00:40:18,990
So I think we need to make more of those
decisions that the data is radioactive.

575
00:40:18,991 --> 00:40:23,520
It's toxic, right? We keep it
if we need to, but if we don't,

576
00:40:23,521 --> 00:40:26,010
we get rid of it and we figure how
to get rid of it safely and securely.

577
00:40:29,010 --> 00:40:33,000
Hey, I mean take, oh, I don't know ways,
right ways is a surveillance based system.

578
00:40:34,470 --> 00:40:35,610
Very personal data,

579
00:40:36,210 --> 00:40:40,290
but probably only valuable if for
like 10 minutes or at least you know,

580
00:40:40,350 --> 00:40:43,920
can can be sampled in lots of ways.

581
00:40:43,921 --> 00:40:47,520
I can treat that data
understanding it's a toxic asset,

582
00:40:47,550 --> 00:40:51,780
get my value at much less
risk to my organization.

583
00:40:53,010 --> 00:40:54,240
And that's what I mean by that.

584
00:40:54,490 --> 00:40:58,840
It's interesting that there are
ways to anonymize stuff. Uh,

585
00:40:58,900 --> 00:41:02,410
but there seems to be
no demand and no supply.

586
00:41:02,510 --> 00:41:07,010
He said there are marginally more
expensive to do federated machine learning

587
00:41:07,011 --> 00:41:10,480
than do everything in the center.
But companies don't care.

588
00:41:10,720 --> 00:41:12,920
And consumers a decidedly.
Yeah,

589
00:41:13,190 --> 00:41:15,290
consumers, consumers
don't care. That's why,

590
00:41:15,560 --> 00:41:20,270
I mean you need these decisions made not
by consumers, but by citizens. Right?

591
00:41:20,280 --> 00:41:21,590
Consumers don't care,
right?

592
00:41:21,591 --> 00:41:26,591
Consumers are buying the big Mac at 10%
off because you was truly don't care.

593
00:41:26,691 --> 00:41:30,770
The point of purchase, nobody
cares. At the point of reflection,

594
00:41:30,830 --> 00:41:32,960
people care a lot.
And that's,

595
00:41:34,160 --> 00:41:35,990
you don't want the market doing this.

596
00:41:35,991 --> 00:41:39,830
You want us as our best selves doing this.

597
00:41:40,910 --> 00:41:45,260
So and about an entity,
it is harder than you think.

598
00:41:45,800 --> 00:41:50,210
You most of our ways of Anna
and anonymizing data fails.

599
00:41:50,870 --> 00:41:55,700
It is a very hard problem. We
don't, the anonymity research is,

600
00:41:55,701 --> 00:41:56,930
is,
is really,

601
00:41:57,350 --> 00:42:01,550
I mean the breaking any research is very
good these days and outstripping the

602
00:42:01,551 --> 00:42:03,470
anonymity research.
Go to the next,

603
00:42:03,830 --> 00:42:06,680
one of the things that I'm thinking about
is a lot of times when you see like a

604
00:42:06,710 --> 00:42:09,830
big vulnerability, so say there's a
big operating system vulnerability,

605
00:42:09,860 --> 00:42:13,160
it's actually a genuine mistake of
it's not that someone, you know,

606
00:42:13,250 --> 00:42:15,500
put it in there on purpose,
it's they missed something.

607
00:42:15,501 --> 00:42:19,100
So how does regulation solve that problem
of surely you could have some great

608
00:42:19,101 --> 00:42:22,010
regulation in place that something
supposed to be done a certain way, but oh,

609
00:42:22,011 --> 00:42:25,790
the implementation was slightly offer,
slightly broken. How do you fix that?

610
00:42:25,850 --> 00:42:27,230
You know,
genuine mistake.

611
00:42:27,710 --> 00:42:30,770
Even if they were trying to do what the
regulations specified as this would be a

612
00:42:30,771 --> 00:42:31,604
secure system.

613
00:42:31,780 --> 00:42:33,010
So you'd be surprised.

614
00:42:33,011 --> 00:42:37,570
But a financial motive [inaudible]
money motivates companies.

615
00:42:38,290 --> 00:42:42,310
If companies will be fine, a lot of
money, if their employees make a mistake,

616
00:42:42,490 --> 00:42:44,470
they figured out a way to their
employees to make fewer mistakes.

617
00:42:44,690 --> 00:42:48,250
But doesn't that only take effect after
the mistake has already been, you know,

618
00:42:48,330 --> 00:42:49,820
like the evil has already been,

619
00:42:51,140 --> 00:42:53,670
but there's a deterrence
effect. Okay. And so, so yes,

620
00:42:53,671 --> 00:42:57,420
I mean like arresting someone for
murder only takes effect after he's done

621
00:42:57,421 --> 00:42:58,254
murder.

622
00:42:58,260 --> 00:43:02,730
But the goal is that the threat of being
arrested for murder will keep you from

623
00:43:02,731 --> 00:43:07,660
hurting someone tomorrow. And so, so
we, we want this deterrence effect, uh,

624
00:43:08,740 --> 00:43:11,280
how to produce software mistakes.

625
00:43:11,730 --> 00:43:15,540
We actually know a lot of techniques that
pretty much all software manufacturers

626
00:43:15,541 --> 00:43:18,030
and never do because it would
be slightly more expensive.

627
00:43:18,720 --> 00:43:23,550
But if it's a lot more expensive not to
do them, it's suddenly the math changes.

628
00:43:24,440 --> 00:43:26,430
And I need the math to change.

629
00:43:26,460 --> 00:43:30,360
I need security to be cheaper
than insecurity right now.

630
00:43:30,361 --> 00:43:33,480
The market rewards,
let's just take the chance,

631
00:43:36,020 --> 00:43:40,350
right? You know, let's hope for
the best. Okay. And then it,

632
00:43:40,351 --> 00:43:44,310
no industry, I say this already,
remind me. Yes, no, no. Okay.

633
00:43:44,311 --> 00:43:49,040
No industry in the past a hundred and
something years has improved security and

634
00:43:49,041 --> 00:43:52,700
safety without being forced to
cars,

635
00:43:53,120 --> 00:43:57,440
planes, pharmaceuticals, medical devices,

636
00:43:57,710 --> 00:44:02,330
food production,
restaurants, consumer goods,

637
00:44:02,880 --> 00:44:05,900
workplace.
Most recently financial products,

638
00:44:07,130 --> 00:44:11,360
the market rewards, doing a
bad job, hoping for the best.

639
00:44:12,890 --> 00:44:16,070
And I think it's too risky
to allow that anymore.

640
00:44:17,320 --> 00:44:17,610
Uh,

641
00:44:17,610 --> 00:44:21,780
so you mentioned that you were embarrassed
by the Zuckerberg hearings and I was

642
00:44:21,781 --> 00:44:26,640
embarrassed by the questions at the
Zuckerberg decides to be fair look,

643
00:44:26,641 --> 00:44:30,720
congressmen embarrassment. So I,
yeah, so I, I so I assumed correctly,

644
00:44:30,721 --> 00:44:34,490
I assume were embarrassed
by the senator's. Yes, yes.

645
00:44:34,790 --> 00:44:39,310
Whereas I have the opposite problem. I
was embarrassed by it. Zuckerberg, what,

646
00:44:39,460 --> 00:44:42,090
what I give me fair. There's a
lot of embarrassment to go around,

647
00:44:43,060 --> 00:44:46,560
but we can both be writers. Stan. Yeah.
But I, I have a serious point here though,

648
00:44:46,561 --> 00:44:49,860
is that, uh, while the
senators don't know about tech,

649
00:44:50,220 --> 00:44:54,780
I think the tech doesn't
know about law ethics,

650
00:44:55,020 --> 00:44:58,110
political science philosophy.
Like,

651
00:44:58,260 --> 00:45:03,120
do you think mark Zuckerberg can even
teach an introductory college course on

652
00:45:03,121 --> 00:45:08,070
free speech? Like has even read like
what anyone has ever said about it.

653
00:45:08,100 --> 00:45:09,930
So like shouldn't we all be learning

654
00:45:10,670 --> 00:45:15,510
about the world around this? This
has to go in both directions. Yes.

655
00:45:15,530 --> 00:45:18,260
Right.
I want techies and politicians.

656
00:45:18,530 --> 00:45:23,240
I won policy people in tech companies.
So yes, I think we need both.

657
00:45:23,480 --> 00:45:27,140
We need both sides talking
to each other. Right. And so,

658
00:45:27,141 --> 00:45:30,320
so I agree with you 100% good.
Okay.

659
00:45:33,950 --> 00:45:34,690
Alright.

660
00:45:34,690 --> 00:45:39,260
So right now I teach Internet security
at the Harvard Kennedy School at a public

661
00:45:39,261 --> 00:45:44,030
policy institution. So I'm trying to go,
you know, push people in that direction.

662
00:45:44,480 --> 00:45:45,231
At the same time,

663
00:45:45,231 --> 00:45:49,010
there are people at Harvard computer
science department trying to teach policy

664
00:45:49,011 --> 00:45:52,710
issues to go the other direction.
I think you probably know a lot of stuff.

665
00:45:54,740 --> 00:45:55,910
I know,
but I'm not in charge.

666
00:45:57,080 --> 00:46:00,440
You mentioned like sort
of shock events, uh, as,

667
00:46:00,441 --> 00:46:02,330
as things that drive government policy.

668
00:46:02,331 --> 00:46:07,310
And I thought the example of nine 11 was
like instructive maybe in a way you did

669
00:46:07,311 --> 00:46:11,240
or did not intend in that
like the government response
to nine 11 was to launch

670
00:46:11,300 --> 00:46:14,870
two illegal wars and create a
surveillance state that violates our civil

671
00:46:14,871 --> 00:46:18,020
liberties on a date. So I did
intend, not sure. That's a good,

672
00:46:18,050 --> 00:46:20,840
like I guess I'm curious how you,
terrible life.

673
00:46:20,900 --> 00:46:24,800
So how do you see the reaction to the
reaction to the mounting threats in

674
00:46:24,801 --> 00:46:26,390
technology as being different?

675
00:46:26,391 --> 00:46:28,370
What's going to prevent the same
sort of thing from absolutely

676
00:46:28,540 --> 00:46:29,810
nothing. Okay. And then that's the,

677
00:46:29,870 --> 00:46:33,250
and that's my fear that
something bad will happen.

678
00:46:34,150 --> 00:46:38,140
Congress will say something must be done.
This is something,

679
00:46:38,500 --> 00:46:41,180
therefore we must do it.
All right.

680
00:46:41,770 --> 00:46:46,770
So my goal of having this conversation
now before this happens is that we will,

681
00:46:47,980 --> 00:46:52,980
as a community figure out what should
be done when we have the luxury of time

682
00:46:55,120 --> 00:46:57,970
and insights and patients.

683
00:46:58,360 --> 00:47:01,420
And because I agree with you
that there is a disaster,

684
00:47:01,900 --> 00:47:06,130
we will get a disaster as a
response and it will be just as bad.

685
00:47:06,460 --> 00:47:10,810
So let's get ahead of it this
time. Let's do better. How do you,

686
00:47:11,480 --> 00:47:11,651
uh,

687
00:47:11,651 --> 00:47:15,110
envision preventing everything
a degenerating to the
lowest common denominator?

688
00:47:15,111 --> 00:47:16,430
Like you said,
client side,

689
00:47:16,431 --> 00:47:19,280
you can't really restrict people
from doing what they want. You know,

690
00:47:19,281 --> 00:47:20,270
even if we say,
okay,

691
00:47:20,271 --> 00:47:23,840
any company that wants to make money in
the u s has to follow these provisions,

692
00:47:24,170 --> 00:47:27,020
I'm just going to encrypt my data
and send it to Ali Baba Plate.

693
00:47:27,021 --> 00:47:29,980
It's a third of the price of Google
translate. But they all my data,

694
00:47:30,340 --> 00:47:34,960
like how do we prevent this?
Is there anything we can do?

695
00:47:35,010 --> 00:47:37,510
You know, some of it in the answer's
gonna be no psalm. It's gonna be yes.

696
00:47:38,200 --> 00:47:41,650
So if you think about uh,
other consumer goods,

697
00:47:42,040 --> 00:47:44,710
we do make it hard for
consumers to modify something.

698
00:47:44,711 --> 00:47:49,300
It's actually hard to modify your car
to violate emissions control, right?

699
00:47:49,630 --> 00:47:54,490
You can do it, but it's hard. And
then we try to have spot checks.

700
00:47:55,000 --> 00:47:56,950
You can imagine some sort of regime,

701
00:47:57,490 --> 00:48:01,570
you can imagine some system that tries
to maintain security anyway cause it will

702
00:48:01,571 --> 00:48:03,010
be up and just a minority doing that.

703
00:48:04,180 --> 00:48:08,410
I think once we start
hitting the problem for real,

704
00:48:08,980 --> 00:48:13,780
we'll come up with tech solutions and
ways the system to self to watch itself,

705
00:48:14,230 --> 00:48:17,830
other systems to watch each other.
What can we do this noninvasively?

706
00:48:17,950 --> 00:48:21,100
I think we have to figure it out. All
right. So I don't have the answers here,

707
00:48:21,460 --> 00:48:23,710
but I mean these are
certainly the problems.

708
00:48:24,440 --> 00:48:25,100
I,
uh,

709
00:48:25,100 --> 00:48:30,100
I really liked your phrasing
of the problem of we need
to give up on offense so

710
00:48:30,741 --> 00:48:34,050
we can go all in on defense. And, um, I,

711
00:48:34,080 --> 00:48:38,900
I think it's pretty clear to me where a
lot of the offensive focuses in terms of

712
00:48:38,901 --> 00:48:43,110
law enforcement, but I think one
thing that sort of remains, uh,

713
00:48:43,400 --> 00:48:48,400
mostly an unknown is on the military
side and how there is a ton of investment

714
00:48:49,010 --> 00:48:52,070
in military oftens of stuff.
We kind of know, you know,

715
00:48:52,400 --> 00:48:55,000
a little bit more maybe about
like what Russia and China are,

716
00:48:55,050 --> 00:48:59,390
are using offensively against us.
We're not seeing the good stuff yet.

717
00:49:00,770 --> 00:49:05,020
I hope not anyway. Right. Yeah.

718
00:49:05,780 --> 00:49:10,170
Uh, but, but do we, do we have a
sense of, of what the military did,

719
00:49:10,280 --> 00:49:12,620
the u s military,
let's say,

720
00:49:12,621 --> 00:49:17,570
would be giving up to give up
this, this offensive, uh, idea.

721
00:49:17,571 --> 00:49:19,070
And,
and uh,

722
00:49:19,670 --> 00:49:23,330
I don't know how willing they would be
to go with that direction. They wouldn't

723
00:49:23,330 --> 00:49:25,310
be willing,
but it's not their job to be willing.

724
00:49:25,950 --> 00:49:29,630
I mean it's why you don't want the NSA
in charge of your privacy policy because

725
00:49:30,830 --> 00:49:35,510
it's not their job. You know, we
need people above the military,

726
00:49:35,511 --> 00:49:40,511
the NSA to make these trade offs because
they are security versus security trade

727
00:49:41,160 --> 00:49:42,380
offs.
Right.

728
00:49:42,410 --> 00:49:47,410
Is the security we get from being able
to spy out and hack the bad guys greater

729
00:49:47,541 --> 00:49:51,650
or less than the security we get from
the bad guys being unable to spy on and

730
00:49:51,651 --> 00:49:55,480
hack us. I that's great. It's so,

731
00:49:55,510 --> 00:49:58,520
I mean it's security versus surveillance
was the wrong way to describe it as

732
00:49:58,521 --> 00:50:02,300
security versus security.
So someone above this, the,

733
00:50:02,480 --> 00:50:06,050
the military needs to decide that it
can't be the military because the military

734
00:50:06,051 --> 00:50:09,410
is not in charge of overall policy.
They don't charge of the military part.

735
00:50:09,920 --> 00:50:12,410
And what we know about the
capabilities is very little.

736
00:50:12,411 --> 00:50:17,090
I mean we get some shadows of it here
or there and it seems to be, you know,

737
00:50:17,300 --> 00:50:20,570
we, you know, on the one hand cruder
than we were then we'd like it to be.

738
00:50:21,200 --> 00:50:25,610
On the other hand, you know, stuxnet
was pretty impressive. You know,

739
00:50:25,611 --> 00:50:29,120
in general the stuff you see
as sort of the minimum tech,

740
00:50:29,121 --> 00:50:33,200
it has to be to succeed. Right?
This sort of this myth of this,

741
00:50:33,680 --> 00:50:35,090
these, uh, you know,

742
00:50:35,540 --> 00:50:39,500
super powerful cyber attacks
that basically, you know,

743
00:50:39,501 --> 00:50:42,830
one I owed them more than just
barely necessary to succeed.

744
00:50:44,610 --> 00:50:44,881
You know,

745
00:50:44,881 --> 00:50:49,830
you don't need to do more if he could
take out the DNC with a pretty sloppy

746
00:50:49,831 --> 00:50:53,880
phishing campaign. And why, I mean,
why bother using your good stuff?

747
00:50:54,570 --> 00:50:55,950
So a lot we just don't know

748
00:50:57,440 --> 00:50:59,510
at risk of revisiting an earlier question.
Um,

749
00:51:00,050 --> 00:51:02,000
I was interested in
what you thought about,

750
00:51:02,001 --> 00:51:04,520
so like one of the things that's often
you'll see cynical in the finance

751
00:51:04,521 --> 00:51:07,820
industry is that they think that people
in finance can out maneuver all the

752
00:51:07,940 --> 00:51:10,790
people who are regulating them in
part because of the lesser paid.

753
00:51:10,791 --> 00:51:14,120
So I was wondering if you could revisit
that cause I think La has maybe the

754
00:51:14,121 --> 00:51:17,960
exception because it's a little bit more
directly related to human rights and

755
00:51:17,961 --> 00:51:18,794
things like that that,

756
00:51:19,150 --> 00:51:23,590
yeah, I don't know. I mean, certainly,
uh, I worry about regulatory capture,

757
00:51:23,910 --> 00:51:28,510
uh, regulations being evaded. I think
the, all of those are real risks. I mean,

758
00:51:28,511 --> 00:51:31,780
this is not a great answer. I have,
it's just the best one I have.

759
00:51:32,740 --> 00:51:33,290
Yeah.

760
00:51:33,290 --> 00:51:37,380
Cause I don't see any way
to put a backstop against,

761
00:51:37,381 --> 00:51:40,580
so this massive corporate power
other than government power.

762
00:51:40,970 --> 00:51:44,060
Now in a sense out of Keto one,
either power,

763
00:51:44,780 --> 00:51:49,780
but tech naturally concentrates power
at least as it's configured today.

764
00:51:51,110 --> 00:51:54,170
So that's my missing piece.

765
00:51:54,320 --> 00:51:58,940
I think you're right that that is a
serious problem in worry and something we

766
00:51:58,941 --> 00:52:03,080
just have to deal with. Okay.
Policies iterative as techies.

767
00:52:03,081 --> 00:52:04,010
It's hard to,

768
00:52:04,480 --> 00:52:07,750
to accept that we like to get the answer
right and then implemented where's

769
00:52:07,760 --> 00:52:11,510
policy gets the answer. Like slightly
relaxed wrong every few months.

770
00:52:12,630 --> 00:52:13,463
Hm.

771
00:52:14,040 --> 00:52:17,700
But that's, you know, that's
the way it works. I mean,

772
00:52:17,701 --> 00:52:21,060
the real question is can we do this
at tech speed? And that it really is,

773
00:52:21,070 --> 00:52:25,560
I think it's an open question
so that I'm going to end.

774
00:52:25,561 --> 00:52:27,620
Thank you all. Thanks for
filling the room. Thanks.

775
00:52:27,790 --> 00:52:32,790
[inaudible].

