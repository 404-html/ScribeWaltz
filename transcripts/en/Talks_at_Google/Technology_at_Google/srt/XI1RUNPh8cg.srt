1
00:00:09,450 --> 00:00:12,030
Thanks so much for, uh, for having me. Um,

2
00:00:12,450 --> 00:00:15,180
if the theme of the session
is a cops and nerds,

3
00:00:15,181 --> 00:00:19,480
I think I'm going to force the audience
to guess which of those I am. Um,

4
00:00:20,040 --> 00:00:23,040
so here's what I wanted to talk about.
Thanks so much for having me.

5
00:00:23,041 --> 00:00:26,040
What I wanted to talk about
is, um, you know, you realize,

6
00:00:26,041 --> 00:00:28,410
especially when you live
in a big city like Chicago,

7
00:00:28,890 --> 00:00:31,970
how many heartbreaking social problems,
uh,

8
00:00:32,010 --> 00:00:34,050
that we are trying to solve right now.

9
00:00:34,620 --> 00:00:39,390
And you also realize that
we are increasingly living
in a world of, uh, of data,

10
00:00:39,391 --> 00:00:40,260
of big data.

11
00:00:40,290 --> 00:00:45,030
And so it seems like there's a very
natural and easy intellectual arbitrage

12
00:00:45,031 --> 00:00:48,390
opportunity where we can take this
sort of machine learning tools that we

13
00:00:48,391 --> 00:00:52,560
encounter all the time in the commercial
sector and just kind of plugin,

14
00:00:52,590 --> 00:00:57,080
plugin play and apply them
self policy problems. Uh, it's,

15
00:00:57,230 --> 00:01:00,390
it looks like an easy
intellectual arbitrage
opportunity for making the world a

16
00:01:00,391 --> 00:01:04,370
better place. And so what I wanted
to do is I wanted to talk about, um,

17
00:01:04,740 --> 00:01:09,240
how I think social progress on some of
these really difficult policy problems is

18
00:01:09,241 --> 00:01:12,570
going to require a lot more, I think,
than just off the shelf machine learning.

19
00:01:12,630 --> 00:01:15,300
So that's the, the theme of
the, the theme of the talk.

20
00:01:16,410 --> 00:01:18,740
So let me talk about that. Um, uh,

21
00:01:18,840 --> 00:01:23,190
a little bit more concretely
within the context of, uh,

22
00:01:23,280 --> 00:01:26,370
one particularly important problem
that we have here in Chicago.

23
00:01:26,371 --> 00:01:31,230
But this is obviously not just a Chicago
problem. So if you're hearing Chicago,

24
00:01:31,231 --> 00:01:34,980
you drive down to 26th and California
on the southwest side of the city,

25
00:01:35,310 --> 00:01:39,060
you will see the Cook County
jail, uh, which has, you know,

26
00:01:39,061 --> 00:01:43,530
depending on the day between six
or eight or 10,000 residents,

27
00:01:43,980 --> 00:01:48,870
some of the most economically
disadvantaged people in
the city of Chicago, uh,

28
00:01:48,871 --> 00:01:52,590
80 to 90% of whom are either
African American or Hispanic.

29
00:01:53,430 --> 00:01:55,170
So we can back up and say,

30
00:01:55,171 --> 00:01:58,680
how did we wind up with so many
people in the Cook County jail?

31
00:01:58,681 --> 00:01:59,700
So here's a little bit,

32
00:01:59,970 --> 00:02:02,190
a little primmer on how the
criminal justice system works.

33
00:02:02,191 --> 00:02:06,270
So you get arrested in the United States
and the constitution says within 48

34
00:02:06,271 --> 00:02:06,990
hours,

35
00:02:06,990 --> 00:02:10,740
you've got to go in front of a judge who
makes the decision about where you're

36
00:02:10,741 --> 00:02:12,990
going to await adjudication of your case.

37
00:02:13,170 --> 00:02:17,700
Do you get to go home or do you have to
sit in jail waiting for your case to be

38
00:02:17,701 --> 00:02:20,380
resolved? And, um,

39
00:02:20,490 --> 00:02:23,970
the law says that the judge is
supposed to make that decision,

40
00:02:24,270 --> 00:02:27,840
not based on whether you're guilty or
not or what the punishment is for the

41
00:02:27,841 --> 00:02:29,580
crime that you were
alleged to have committed.

42
00:02:29,970 --> 00:02:33,840
But the law says the judge is supposed
to make that decision entirely based on a

43
00:02:33,841 --> 00:02:36,600
prediction,
a prediction of your safety risk,

44
00:02:36,720 --> 00:02:40,950
and a prediction of your
flight risk. And, uh,

45
00:02:41,010 --> 00:02:45,050
this is an enormously consequential
decision as you can imagine.

46
00:02:45,060 --> 00:02:47,850
So if the judge gels you, uh, on average,

47
00:02:47,851 --> 00:02:51,210
you'll spend two to four months in
a place like the Cook County jail.

48
00:02:51,211 --> 00:02:55,320
You could imagine what that
does to your, uh, job prospects.

49
00:02:55,380 --> 00:02:58,030
You can imagine what that
does to your family. Uh,

50
00:02:58,080 --> 00:03:02,890
you can imagine what that to the
local county, a budget to keep six,

51
00:03:02,920 --> 00:03:04,480
eight,
10,000 people in jail,

52
00:03:05,140 --> 00:03:09,370
releasing people also has a
serious downside risk as well.

53
00:03:09,880 --> 00:03:10,151
Uh,

54
00:03:10,151 --> 00:03:13,780
the person that the judge releases might
go on to commit another crime and crime

55
00:03:13,781 --> 00:03:17,140
itself is also very
regressive in its impact.

56
00:03:17,170 --> 00:03:20,980
So super high stakes decision.
Uh, if you're a tech person,

57
00:03:20,981 --> 00:03:22,630
you look at this and say,
well,

58
00:03:22,631 --> 00:03:26,440
what is the criminal justice system
currently doing to help judges make this

59
00:03:26,441 --> 00:03:31,420
high stakes? Very difficult prediction.
And this is status quo in most cities.

60
00:03:31,420 --> 00:03:35,590
This is the same technology that we
would have used in the 1950s or maybe the

61
00:03:35,591 --> 00:03:39,640
1850s ye olde, um, decision aid. Um,

62
00:03:39,940 --> 00:03:43,370
and so it's natural to
think, well, you know,

63
00:03:43,390 --> 00:03:48,390
why not instead use this sort
of technology that places
like Google are applying

64
00:03:48,551 --> 00:03:51,640
to all sorts of commercial applications.

65
00:03:52,990 --> 00:03:56,920
And so this seems like
super straight forward, uh,

66
00:03:56,950 --> 00:04:00,730
because it seems like we have most of
the important ingredients that we need to

67
00:04:00,731 --> 00:04:05,020
make progress. So for starters, we have
these amazing machine learning tools.

68
00:04:05,021 --> 00:04:09,040
And I think it's, it's easy
because they're so ubiquitous now,

69
00:04:09,041 --> 00:04:12,310
it's easy to overlook exactly
how amazing these tools are.

70
00:04:12,311 --> 00:04:16,360
So let me just spend a minute reminding
us of this within the context of one

71
00:04:16,361 --> 00:04:20,800
very common kind of machine learning
application, which is sentiment analysis.

72
00:04:21,010 --> 00:04:22,830
So if you're a computer scientist,
you know,

73
00:04:22,960 --> 00:04:27,310
one of the canonical
applications of machine learning
is to take a piece of text

74
00:04:27,311 --> 00:04:28,570
that a author is a human,

75
00:04:28,571 --> 00:04:32,530
is written and try and infer what the
effect was that the author is trying to

76
00:04:32,531 --> 00:04:33,070
communicate.

77
00:04:33,070 --> 00:04:38,070
So here's a example from a more or less
randomly selected a consumer product off

78
00:04:38,471 --> 00:04:41,260
of Amazon,
the Hustler five 71 banana slicer.

79
00:04:41,740 --> 00:04:45,010
And so what we have here is
a product review, some text,

80
00:04:45,011 --> 00:04:48,250
and then we also have a starred review,
which gives us sort of ground truth.

81
00:04:48,520 --> 00:04:51,690
What actually was the author intending?
And so here's an example.

82
00:04:51,710 --> 00:04:54,640
This is from someone named thrifty.
He who says,

83
00:04:54,641 --> 00:04:57,670
I bought this in order to speed up
cutting up a banana for my cereal.

84
00:04:58,030 --> 00:05:01,180
And a time that I saved in that endeavor
was spent cleaning this implement.

85
00:05:01,210 --> 00:05:02,230
It's not easy to clean.

86
00:05:02,231 --> 00:05:04,510
You have to scrub between every
rung to thoroughly clean it.

87
00:05:04,840 --> 00:05:08,080
You can see that's it's not a great
review. Two out of five stars.

88
00:05:08,650 --> 00:05:11,020
You can look at another one
by uncle pooky who says,

89
00:05:11,021 --> 00:05:14,620
once I figured out I had to peel the
banana before using it works much better.

90
00:05:15,370 --> 00:05:18,760
Ordering one for my nephew who's in the
air force in California and he's been

91
00:05:18,761 --> 00:05:20,140
using an old slinky to slice.

92
00:05:20,141 --> 00:05:24,250
He's been in as he should really enjoy
this product. Five out of five stars,

93
00:05:25,610 --> 00:05:27,940
a confusing by Qtip.

94
00:05:27,970 --> 00:05:30,880
There's no way to tell if this is
a standard or metric banana slicer.

95
00:05:30,881 --> 00:05:32,770
Additional markings
audit would help greatly.

96
00:05:33,130 --> 00:05:38,130
One out of five stars and finally
from Jay Anderson Angle is wrong.

97
00:05:38,440 --> 00:05:42,330
I tried the banana slicer and found that
unacceptable as shown in the picture of

98
00:05:42,331 --> 00:05:44,200
the slices is curved from left to right.

99
00:05:44,201 --> 00:05:45,850
All of my bananas are bent the other way,

100
00:05:46,720 --> 00:05:50,170
so this is an example also of how
machine learning turns out to have such

101
00:05:50,171 --> 00:05:51,790
powerful commercial applications.

102
00:05:51,791 --> 00:05:56,080
This helped a hustler company figure out
what their five 72 banana slicer should

103
00:05:56,081 --> 00:06:00,890
look like. No. Here's, I think the
sort of the key lesson from this is,

104
00:06:00,891 --> 00:06:01,101
you know,

105
00:06:01,101 --> 00:06:05,360
you're reading these narrative reviews
and it is super easy for you to figure

106
00:06:05,361 --> 00:06:09,500
out what the author was intending.
And it's so easy that, you know,

107
00:06:09,501 --> 00:06:12,800
when the computer scientist in the
middle of the 20th century were initially

108
00:06:12,801 --> 00:06:15,080
working on artificial intelligence,

109
00:06:15,440 --> 00:06:19,790
this led to a natural sort of conclusion
about how you would try and get a

110
00:06:19,791 --> 00:06:21,650
computer to do what humans do,

111
00:06:21,890 --> 00:06:24,290
especially for something
that is so easy for us to do,

112
00:06:24,291 --> 00:06:28,340
which is just program them to
do something, how we do it.

113
00:06:28,790 --> 00:06:32,750
Um, so introspect on what you do and
write a program that does exactly the same

114
00:06:32,751 --> 00:06:37,640
thing. And here's sort of the problem
with that kind of approach, uh, that,

115
00:06:37,670 --> 00:06:39,770
that the computer scientists
found in practice.

116
00:06:39,771 --> 00:06:44,771
This is from a study done at Cornell
doing sentiment analysis for a movie

117
00:06:45,141 --> 00:06:45,591
review.

118
00:06:45,591 --> 00:06:50,591
So they get a bunch of Cornell nerds in
the basement to introspect on what words

119
00:06:50,811 --> 00:06:53,810
they would think would show up in a
positive or negative movie review.

120
00:06:53,870 --> 00:06:57,530
They write a program that looks for those
words and then they classify positive

121
00:06:57,531 --> 00:06:59,900
or negative reviews on that basis.

122
00:07:00,260 --> 00:07:04,250
Usually we set up the test set here
so that we have 50% positive reviews,

123
00:07:04,251 --> 00:07:05,810
50% negative reviews.

124
00:07:05,840 --> 00:07:09,710
So accuracy of 50% would
be like random guessing.

125
00:07:10,340 --> 00:07:15,200
And so these are the words that the
Cornell nerds, um, picked. Uh, I dunno,

126
00:07:15,201 --> 00:07:18,290
why suck. And sucks are both
separate words there. It's Cornell,

127
00:07:18,291 --> 00:07:22,340
not the University of Chicago.
So one of my coauthors is from Cornell.

128
00:07:22,341 --> 00:07:25,170
So I see that with, uh, with love. Um,

129
00:07:25,850 --> 00:07:30,020
and so here's what we saw with this sort
of programming approach is, you know,

130
00:07:30,021 --> 00:07:31,910
we're doing better than random guessing,

131
00:07:32,150 --> 00:07:35,930
but not much better than random guessing.
Not much better than random guessing.

132
00:07:35,931 --> 00:07:40,931
And the breakthrough here with these
machine learning tools came from when the

133
00:07:41,000 --> 00:07:44,300
computer scientists realized that we
just need to forget that we do these,

134
00:07:44,360 --> 00:07:48,440
that we know how to do these things,
treat the known like the unknown,

135
00:07:48,441 --> 00:07:53,441
and just start treating this like a brute
force empirical exercise and basically

136
00:07:53,841 --> 00:07:55,460
mined the data.

137
00:07:55,490 --> 00:07:59,870
Mine dumb movie reviews themselves for
information about what words turn up more

138
00:07:59,871 --> 00:08:03,860
frequently in positive and
negative reviews. And um, you know,

139
00:08:03,861 --> 00:08:05,120
once we start doing that,

140
00:08:05,121 --> 00:08:09,950
we start to get up to 90 a accuracy
rates on the order of like 95%.

141
00:08:10,160 --> 00:08:14,960
So this really was the big breakthrough
and the tools that enable us to do this

142
00:08:14,961 --> 00:08:16,550
really are incredibly amazing.

143
00:08:17,090 --> 00:08:20,240
I think what's also particularly exciting
from a public policy perspective,

144
00:08:20,241 --> 00:08:23,990
if you think about like pretrial release
decisions is that there is an enormous

145
00:08:23,991 --> 00:08:28,440
amount of data to be had. So this is what
we discovered when we were working, uh,

146
00:08:28,490 --> 00:08:33,490
building a pretrial risk prediction
tool ourselves using data from a large

147
00:08:33,700 --> 00:08:38,150
anonymous American city of eight
and a half million people. Um,

148
00:08:38,240 --> 00:08:42,110
and you know, you have millions
and millions of observations,

149
00:08:42,111 --> 00:08:45,320
which doesn't sound like a lot when
you're talking to an audience at Google.

150
00:08:45,320 --> 00:08:49,100
But I think in a policy
environment, uh, this is, you know,

151
00:08:49,130 --> 00:08:53,030
this is a new development to have so much
information that we can bring to bear

152
00:08:53,031 --> 00:08:56,130
on these problems and for each
case goes through bond court.

153
00:08:56,131 --> 00:08:59,910
We have lots of information about the
current charge and their prior criminal

154
00:08:59,911 --> 00:09:03,180
record. So lots of information
that can be brought to bear.

155
00:09:03,480 --> 00:09:06,720
And these machine learning tools
now are increasingly accessible.

156
00:09:06,721 --> 00:09:11,721
So anybody with an Internet connection
can download our free software and just

157
00:09:12,091 --> 00:09:15,720
basically build a machine learning
algorithm and they're off to the races.

158
00:09:16,860 --> 00:09:20,250
Now normally in a machine
learning exercise,

159
00:09:20,760 --> 00:09:22,440
the final step is also easy,

160
00:09:22,441 --> 00:09:27,030
which is determining how good of a job
your algorithm is actually doing. Right.

161
00:09:27,031 --> 00:09:30,960
And so imagine that I'm doing some sort
of commercial application of machine

162
00:09:30,961 --> 00:09:35,010
learning and I am trying to decide
like how good is my facial recognition

163
00:09:35,340 --> 00:09:36,390
software doing.

164
00:09:36,390 --> 00:09:40,830
And I wanted give it a bunch of new face
pictures that it hasn't seen and I want

165
00:09:40,831 --> 00:09:45,180
it to tell me if a normal human faces
in the picture. And so you know,

166
00:09:45,181 --> 00:09:49,710
you can look at that face, no face. It's
easy to tell and score the algorithm,

167
00:09:50,730 --> 00:09:53,340
you know.
Uh,

168
00:09:54,090 --> 00:09:57,180
I think one of the things I learned to
do it for this slide show is huskies have

169
00:09:57,181 --> 00:09:59,880
the funniest dog faces.
Um,

170
00:09:59,940 --> 00:10:03,120
and whether this one is a normal
human face I think is sort of a deeper

171
00:10:03,121 --> 00:10:07,080
philosophical question.
But this is easy, right?

172
00:10:07,230 --> 00:10:08,250
This is the easy part.

173
00:10:08,580 --> 00:10:13,580
This turns out to be the first point in
policy applications where you realize

174
00:10:15,360 --> 00:10:19,140
that this is very different
from commercial, from
commercial machine learning.

175
00:10:19,770 --> 00:10:24,600
And part of the issue here is that
we really, at some fundamental level,

176
00:10:24,840 --> 00:10:26,970
we really don't care
about prediction quality.

177
00:10:27,720 --> 00:10:30,630
The thing that we really care
about instead is decision quality.

178
00:10:30,960 --> 00:10:35,960
What I want to know is can I take a
machine learning algorithm and build some

179
00:10:35,971 --> 00:10:40,971
new release rule that I can give to a
judge and we'll let actually turn into the

180
00:10:41,821 --> 00:10:45,870
world becoming a better place.
Now, why is that complicated? Okay.

181
00:10:46,410 --> 00:10:50,130
Why is that complicated? Think
about, um, how an algorithm,

182
00:10:50,210 --> 00:10:53,790
an algorithmic release role could
potentially make the world a better place.

183
00:10:54,270 --> 00:10:58,590
The algorithm might want to detain someone
that the judge releases or it might

184
00:10:58,591 --> 00:11:01,440
want to release someone
that the judge to teens.

185
00:11:01,470 --> 00:11:02,970
And so then we can ask ourselves,

186
00:11:02,971 --> 00:11:07,530
how do we score whether the algorithmic
release rule is better than the judge or

187
00:11:07,531 --> 00:11:11,970
not in a world in which the data that
we have available to us to evaluate the

188
00:11:11,971 --> 00:11:16,020
algorithm is generated by
the human decisions generated
by the judge's decisions.

189
00:11:16,020 --> 00:11:21,000
And so here's what the issue is, right?
If the judge releases a defendant,

190
00:11:21,360 --> 00:11:24,360
right? If the judge releases a defendant,
we don't have any sort of problem,

191
00:11:24,810 --> 00:11:29,100
I can observe what the crime outcome
is under the judge's actual decision.

192
00:11:29,430 --> 00:11:33,870
And I know what the crime outcome is
under the counterfactual algorithmic

193
00:11:33,871 --> 00:11:34,351
decision.

194
00:11:34,351 --> 00:11:38,730
If the algorithm wants to detain the
defendant because putting someone in jail

195
00:11:38,970 --> 00:11:42,260
by definition incapacitates them
and prevents them from Aig, uh,

196
00:11:42,300 --> 00:11:44,250
engaging in crime.
So that's easy.

197
00:11:45,000 --> 00:11:48,420
But what if the judge
detains the defendant, right?

198
00:11:48,421 --> 00:11:52,500
What if the judge detains the defendant
and the Algorithm wants to release them?

199
00:11:52,501 --> 00:11:56,680
How do I, whether the algorithm is
right or whether the judge was right.

200
00:11:57,160 --> 00:12:01,750
Okay. Now if you're a computer scientist,
you look at this and say, well,

201
00:12:01,751 --> 00:12:03,940
this doesn't feel like a hard problem,
right?

202
00:12:03,941 --> 00:12:05,860
At some level it doesn't
feel like a hard problem.

203
00:12:06,130 --> 00:12:09,190
We have a bunch of data on the
people who the judge released.

204
00:12:09,610 --> 00:12:13,090
We have a bunch of background information
on the people that the judge released

205
00:12:13,091 --> 00:12:14,230
and the judge detained.

206
00:12:14,740 --> 00:12:18,760
Why don't we just assume that the
crime outcomes for the people the judge

207
00:12:18,761 --> 00:12:23,620
released would be like a crime outcomes
for the people the judge detained who

208
00:12:23,621 --> 00:12:27,760
have observably similar current
Arestin prior criminal record, right.

209
00:12:27,761 --> 00:12:31,780
It seems like a very, very straightforward
imputation exercise on its face.

210
00:12:32,800 --> 00:12:34,900
Here's the problem and
a policy application,

211
00:12:34,930 --> 00:12:39,100
which is the judge sees things
that no algorithm will ever see.

212
00:12:40,180 --> 00:12:43,100
So this is a,
this is an example of um,

213
00:12:43,300 --> 00:12:47,310
this is an example of a
defendant, uh, mid 20,

214
00:12:47,311 --> 00:12:52,000
it's called it a 25 year
old guy who report reported
as a occupation when he was

215
00:12:52,001 --> 00:12:56,860
arrested as tattoo model. I didn't
know there was such a thing. Uh,

216
00:12:56,861 --> 00:12:59,940
he gets arrested two different times,
uh,

217
00:13:00,040 --> 00:13:04,420
one time that's him on the left. Uh,
and then this is him again on the right.

218
00:13:05,050 --> 00:13:10,050
So you'll notice that the 25 year old
tattoo model on the right has decided that

219
00:13:10,451 --> 00:13:13,180
it was a good idea to get his
face tattooed. Like joker,

220
00:13:13,181 --> 00:13:15,070
the Super Villain from
the dark knight moving,

221
00:13:15,400 --> 00:13:18,850
but you can't see because it's on the
other side of the face is on this side.

222
00:13:18,851 --> 00:13:23,050
You can see he's got the joker
tattooed on a, on his right,

223
00:13:23,051 --> 00:13:24,190
on his left it says,

224
00:13:24,191 --> 00:13:27,490
just in case there was any ambiguity
about what he's about on his left,

225
00:13:27,491 --> 00:13:29,170
it says in giant letters.
Fuck Batman.

226
00:13:30,610 --> 00:13:32,860
Now to the Algorithm,

227
00:13:33,040 --> 00:13:37,210
the 25 year old tattoo model on the left
and the 25 year old tattoo model on the

228
00:13:37,211 --> 00:13:40,450
right look exactly the same. The Algorithm
looks at the guy in the left and say,

229
00:13:40,451 --> 00:13:43,540
he doesn't look no prior record,
current offenses and misdemeanor.

230
00:13:43,541 --> 00:13:45,280
This does not look like
a very high risk guy.

231
00:13:46,270 --> 00:13:51,270
The judge sees that can see
something extra about the defendant.

232
00:13:51,940 --> 00:13:55,360
Now imagine what happens.
The algorithm goes to the judge and says,

233
00:13:55,361 --> 00:13:57,820
why are you detaining these
25 year old tattoo models?

234
00:13:57,821 --> 00:13:59,950
These are all low risk guys.
You should just let them go.

235
00:14:00,730 --> 00:14:05,470
You look the guy on the right go and
crime goes up by a lot more than the

236
00:14:05,471 --> 00:14:10,180
algorithm had anticipated and we wind up
inadvertently making the world a worse

237
00:14:10,181 --> 00:14:12,550
place while other than a better place.
Okay,

238
00:14:12,610 --> 00:14:15,120
and the key here is that
in these sorts of social,

239
00:14:15,121 --> 00:14:17,290
social science or policy applications,

240
00:14:18,130 --> 00:14:22,840
we have to take seriously the possibility
that the judge has private information

241
00:14:22,841 --> 00:14:24,190
that the algorithm doesn't have,

242
00:14:24,191 --> 00:14:26,980
which makes this evaluation
problem enormously difficult.

243
00:14:27,310 --> 00:14:31,510
And if we ignore that evaluation problem
when we're comparing the algorithm to

244
00:14:31,511 --> 00:14:36,100
the judge where basically stacking the
deck in favor of saying the algorithm,

245
00:14:36,580 --> 00:14:38,890
the algorithm almost has to
do better than the judge.

246
00:14:38,890 --> 00:14:42,670
If you ignore the possibility that
the judge has as extra information.

247
00:14:43,600 --> 00:14:47,200
Okay.
So what is the solution to a,

248
00:14:47,260 --> 00:14:50,470
what is the solution to
that problem? Well, um,

249
00:14:50,530 --> 00:14:54,170
the solution to that problem starts,
has two comes from two insights.

250
00:14:54,171 --> 00:14:57,680
The first insight is to recognize
that the problems one sided.

251
00:14:57,710 --> 00:15:02,360
So when the Algorithm wants to release
a GL defendant, that's a problem.

252
00:15:02,361 --> 00:15:04,400
But there's no problem if the algorithm,

253
00:15:04,430 --> 00:15:08,640
if the algorithmic decision rule
wants to jail someone that the,

254
00:15:08,690 --> 00:15:09,860
the judge released.

255
00:15:10,550 --> 00:15:15,550
And the second insight is to rely on a
common trick that we have in econometrics

256
00:15:15,650 --> 00:15:17,210
or the causal inference literature,

257
00:15:17,211 --> 00:15:21,650
which takes advantage of the fact that
in this case we have something like

258
00:15:21,651 --> 00:15:23,660
random assignment of cases to judges.

259
00:15:24,320 --> 00:15:28,850
So what we wind up having his multiple
judges seeing case loads of defendants

260
00:15:28,851 --> 00:15:30,470
that are similar on average.

261
00:15:30,830 --> 00:15:35,060
And yet the judges differ a lot with
respect to their leniency rates.

262
00:15:35,060 --> 00:15:38,330
And so here's what we can do in that case,
here's what we can do in that case.

263
00:15:39,260 --> 00:15:41,120
So imagine that we have two judges,

264
00:15:41,540 --> 00:15:45,350
one of them with the 90% release rate
and one of them with an 80% release rate

265
00:15:45,560 --> 00:15:48,020
and they're seeing case loads
that are similar on average.

266
00:15:48,021 --> 00:15:51,950
And so here on the left we've got
the judge with the 90% release rate,

267
00:15:51,951 --> 00:15:53,990
we can go into the pool,

268
00:15:54,230 --> 00:15:58,910
the remaining 90% of defendants that the
lenient judge has released and weekend

269
00:15:59,150 --> 00:16:03,140
rank order defendants by our algorithms,
predictions of their crime risk.

270
00:16:03,950 --> 00:16:05,930
And then weekend countdown,

271
00:16:05,931 --> 00:16:10,931
that rank ordered list of defendants by
predicted risk and pick another 10% of

272
00:16:12,561 --> 00:16:13,670
defendants to detain.

273
00:16:13,700 --> 00:16:17,660
So this is what the Algorithm's guests
is for the most socially productive

274
00:16:17,661 --> 00:16:22,160
marginal 10% of defendants to detain.
And then when we can do,

275
00:16:22,161 --> 00:16:25,100
and that gets us down effectively
to an 80% release rate.

276
00:16:25,100 --> 00:16:29,390
And then what we can do in that case is
compare the crime rate that we get in

277
00:16:29,391 --> 00:16:34,040
that case with the actual crime rate
that we get from the 80% released judge.

278
00:16:34,590 --> 00:16:34,880
Right.

279
00:16:34,880 --> 00:16:39,880
And that turns out to be a credible way
and a fair way to compare whether the

280
00:16:40,881 --> 00:16:45,881
algorithm really is doing a better job
than than the judge and what you see when

281
00:16:46,191 --> 00:16:47,090
you do that.
So that,

282
00:16:47,120 --> 00:16:51,500
so when you see when you do that is it
is indeed the case that the algorithm is

283
00:16:51,501 --> 00:16:55,250
doing better than the judge and the
algorithm is doing a lot better than the

284
00:16:55,251 --> 00:16:55,581
judge.

285
00:16:55,581 --> 00:16:59,570
And so what you can see is if we were
to hold the size of the jail population

286
00:16:59,571 --> 00:17:03,830
constant and just use the algorithmic
release role to decide who to detain,

287
00:17:04,250 --> 00:17:09,110
we could reduce crime rates by 25%
without having the change of the jail

288
00:17:09,111 --> 00:17:12,260
population.
Now you might be sitting there thinking,

289
00:17:12,261 --> 00:17:15,950
given that we've ramped
up the incarceration papa
population in the United

290
00:17:15,950 --> 00:17:16,783
States so much,

291
00:17:17,210 --> 00:17:21,110
maybe you think the big problem in the
United States now is not crime so much as

292
00:17:21,111 --> 00:17:25,610
incarceration itself. And
so alternatively, what you
can imagine doing is saying,

293
00:17:25,790 --> 00:17:27,920
let's hold the crime rate constant.

294
00:17:28,310 --> 00:17:33,310
How many fewer people do we need to detain
in order to achieve the same level of

295
00:17:33,951 --> 00:17:37,640
crime reduction as the judges
current decisions achieve.

296
00:17:38,120 --> 00:17:39,230
And if you do that,

297
00:17:39,231 --> 00:17:44,231
we can reduce the jail population by
fully 42% with no increase in crime right

298
00:17:45,230 --> 00:17:47,630
now.
That's completely amazing.

299
00:17:47,720 --> 00:17:51,660
That's completely amazing part because
if you think about how hard these social

300
00:17:51,661 --> 00:17:53,280
problems are to solve,

301
00:17:53,520 --> 00:17:57,030
usually in those rare cases where
we find an effective solution,

302
00:17:57,240 --> 00:18:01,500
they turn out to be either very expensive
to implement or very difficult to

303
00:18:01,501 --> 00:18:05,700
scale up successfully. That's not the
case with the machine learning algorithm.

304
00:18:05,700 --> 00:18:06,570
Once the thing is built,

305
00:18:06,571 --> 00:18:09,360
the marginal cost of running this
over and over again is near zero.

306
00:18:09,720 --> 00:18:11,610
And because it's an automated tool,

307
00:18:11,820 --> 00:18:15,330
it scales nearly perfectly at least over
the range that we have for something

308
00:18:15,331 --> 00:18:17,490
like pretrial release decisions.
Um,

309
00:18:17,700 --> 00:18:20,310
there is something that you'd be
worried about, which is, you know,

310
00:18:20,311 --> 00:18:23,820
we don't care just about crime
and a detention outcomes,

311
00:18:23,821 --> 00:18:26,160
especially in the criminal justice system.

312
00:18:26,430 --> 00:18:29,490
We care a lot about things
like fairness as well.

313
00:18:30,000 --> 00:18:34,560
And so maybe what's also amazing about
the machine learning application in this

314
00:18:34,561 --> 00:18:39,561
case is in addition to the enabling us
to simultaneously reduce crime and jail

315
00:18:39,961 --> 00:18:42,180
populations,
at the same time,

316
00:18:42,181 --> 00:18:45,510
we can also reduce disparities within
the detention populations as well.

317
00:18:45,870 --> 00:18:50,870
And the answer to that is easy to see
why the algorithm can do that as easy to

318
00:18:51,151 --> 00:18:54,150
see once you recognize what the
alternative to the algorithm is,

319
00:18:54,540 --> 00:18:58,440
which is the same human decisions that
gave us the current criminal justice

320
00:18:58,441 --> 00:19:03,441
system that is so skewed in the direction
of over representing racial minorities

321
00:19:03,841 --> 00:19:07,770
and low income populations,
uh, behind bars. Okay.

322
00:19:08,400 --> 00:19:12,720
Now if we were in a,
if we were in a commercial setting,

323
00:19:12,721 --> 00:19:17,190
you would build an algorithm like
this. You have your face detector, uh,

324
00:19:17,250 --> 00:19:21,240
software, you'd put it on a phone
and you'd be off to the races.

325
00:19:21,241 --> 00:19:25,300
The social benefits of the algorithm
would be automatically realized. Um,

326
00:19:25,590 --> 00:19:28,620
if you sort of think about the
pretrial release application,

327
00:19:29,220 --> 00:19:32,580
there's another backend step,
right? There's another backend step,

328
00:19:32,581 --> 00:19:37,080
which is that nobody imagines that the
algorithm is actually ever going to be

329
00:19:37,200 --> 00:19:41,820
the decision maker. Instead,
we imagine, so realistically,

330
00:19:42,090 --> 00:19:47,090
the Algorithm for predicting defendant
risk is inevitably just going to be a

331
00:19:47,281 --> 00:19:51,540
decision aid for a human being that
then has to translate the algorithms

332
00:19:51,541 --> 00:19:56,340
predictions into decisions. And if the
human being is getting things wrong,

333
00:19:56,341 --> 00:20:00,990
they can undo whatever the social
good potential is for the algorithm

334
00:20:01,050 --> 00:20:05,700
algorithms, predictions
to achieve. Okay. Um,

335
00:20:06,510 --> 00:20:09,930
and so, you know, put differently,

336
00:20:09,931 --> 00:20:13,980
the robot in this case is not
going to replace the human,

337
00:20:13,981 --> 00:20:15,750
we think of the robot as a compliment,

338
00:20:15,751 --> 00:20:19,200
not substitute for a human
judgment at the end of the day.

339
00:20:19,680 --> 00:20:24,510
So here's one final, I think
really interesting, uh, uh,

340
00:20:24,540 --> 00:20:28,560
application or potential application of
machine learning that I think looks very

341
00:20:28,561 --> 00:20:32,190
different from what we're used to seeing
in, uh, in the commercial setting.

342
00:20:32,760 --> 00:20:37,230
And that is to basically use machine
learning is a behavioral diagnostic to

343
00:20:37,231 --> 00:20:41,160
better understand what the human
beings themselves are doing right now,

344
00:20:41,310 --> 00:20:46,310
partly as a way to help solve this problem
of how we optimally combine human and

345
00:20:46,990 --> 00:20:50,950
machine intelligence in
designing a decision aid that
we can give the judges and

346
00:20:50,951 --> 00:20:55,660
realize the potential for social good.
So you know, normally what we do,

347
00:20:55,690 --> 00:20:58,210
normally what we do is we take
a machine learning algorithm,

348
00:20:58,211 --> 00:20:59,830
we predict the defendant's risk,

349
00:21:00,160 --> 00:21:02,890
we do a horse race between
the algorithm and the judge.

350
00:21:02,920 --> 00:21:06,340
So the defendant's behavior
is the object of interest.

351
00:21:06,640 --> 00:21:09,990
The other thing that we can do is
basically turn the algorithm on the judge

352
00:21:10,010 --> 00:21:10,843
themselves.

353
00:21:10,990 --> 00:21:14,800
That is basically predict what the
judge is going to do in a given case.

354
00:21:14,830 --> 00:21:18,850
But the judge is going to do as a function
of the defendant's characteristics.

355
00:21:19,240 --> 00:21:20,073
Okay.

356
00:21:20,440 --> 00:21:25,440
And that turns out to give us lots of
really interesting insights into what

357
00:21:26,621 --> 00:21:28,990
humans are doing.
So let me just give you a couple,

358
00:21:29,040 --> 00:21:32,920
a couple of quick examples of that.
So one thing,

359
00:21:32,921 --> 00:21:34,810
notice what this lets us do for starters.

360
00:21:34,811 --> 00:21:37,900
So if you look at the
actual judge decisions,

361
00:21:37,960 --> 00:21:42,520
you know the only thing that you observed
from judges is whether they detain or

362
00:21:42,521 --> 00:21:44,680
release a defendant,
right?

363
00:21:45,460 --> 00:21:49,800
But if you think about our
prediction of the judge's behavior,

364
00:21:50,110 --> 00:21:52,570
that's a continuous release
probability at that point.

365
00:21:52,900 --> 00:21:57,900
And what we can do then is we can look
across the defendant risk distribution to

366
00:21:59,321 --> 00:22:04,240
see where judges are at least certain.
Okay. And you can see in this graph here,

367
00:22:04,241 --> 00:22:09,241
what we've done is we have the judges
predicted released probability on the y

368
00:22:10,211 --> 00:22:15,010
axis here and on the x axis,
uh,

369
00:22:15,040 --> 00:22:15,320
sorry,

370
00:22:15,320 --> 00:22:19,710
the judges detained probability predicted
detain power probability on the exit

371
00:22:19,750 --> 00:22:23,790
on the y axis. And then what we've
done is we've been defending,

372
00:22:23,791 --> 00:22:28,791
so we've grouped defendants together based
on the algorithms predicted defendant

373
00:22:29,441 --> 00:22:30,610
crime risk.
Okay.

374
00:22:30,611 --> 00:22:35,611
So we're looking at the dispersion of
judges release probabilities as a function

375
00:22:37,270 --> 00:22:40,870
of how serious the defendant's
crime risk actually is.

376
00:22:41,170 --> 00:22:45,700
And what you can see here is it's really
the highest risk defendants that are

377
00:22:45,701 --> 00:22:49,390
the ones where the judges have the most
uncertainty and the most difficulty

378
00:22:49,391 --> 00:22:51,880
making decisions.
And it's not,

379
00:22:51,970 --> 00:22:55,240
it's not inevitable that that's what
we would have seen. So for instance,

380
00:22:55,241 --> 00:22:57,580
if you look at like
education applications,

381
00:22:57,970 --> 00:23:02,970
what you see is that
principals do amazingly well
at predicting which teachers

382
00:23:05,290 --> 00:23:07,810
are amazingly good and amazingly bad.

383
00:23:08,260 --> 00:23:13,210
And principles have a huge difficulty
distinguishing the quality of all the

384
00:23:13,211 --> 00:23:15,400
teachers in the middle,
right?

385
00:23:15,730 --> 00:23:19,990
So it didn't need to turn out
like this in the crime setting.

386
00:23:20,050 --> 00:23:21,790
It does turn out though that the judge,

387
00:23:21,820 --> 00:23:25,000
it turns out to be the case that the
judges seem to have the most difficulty

388
00:23:25,390 --> 00:23:26,710
with the highest risk cases.

389
00:23:26,710 --> 00:23:30,400
And one of the things that we're learning
from psychology is that judges have

390
00:23:30,401 --> 00:23:31,960
very constrained judges.

391
00:23:31,961 --> 00:23:36,850
Like all human beings have
very constrained bandwidth
for judgment and decision

392
00:23:36,851 --> 00:23:37,241
making.

393
00:23:37,241 --> 00:23:41,320
And so one of the things for starters we
can use this information about is sort

394
00:23:41,321 --> 00:23:44,230
of a bandwidth triage tool for judges.

395
00:23:44,710 --> 00:23:49,710
Here are the cases where you
need to devote extra time
and being expert shore in

396
00:23:50,001 --> 00:23:52,790
mapping the algorithms
predictions into a decision,

397
00:23:52,880 --> 00:23:57,280
we know that these are going to be the
cases that are hardest for you. Um,

398
00:23:58,040 --> 00:24:02,840
okay. And let me give you one more.
Uh, one more example of this is,

399
00:24:02,860 --> 00:24:03,200
uh,

400
00:24:03,200 --> 00:24:08,150
the other thing that we can do then is
we can look at the crime and detention

401
00:24:08,151 --> 00:24:13,151
outcomes of the actual judge decisions
versus the predicted judge to decisions.

402
00:24:14,630 --> 00:24:16,910
So if you think about what the
predicted judge decisions are,

403
00:24:16,911 --> 00:24:20,480
this is for a case with a
given set of characteristics.

404
00:24:20,660 --> 00:24:22,460
What does the judge do you on average?

405
00:24:22,970 --> 00:24:27,970
And how does the average judge do compared
to the actual judge decisions that

406
00:24:28,460 --> 00:24:31,730
very around that, that average, okay,

407
00:24:31,790 --> 00:24:35,810
now economists would look
at this and say it's clear.

408
00:24:35,840 --> 00:24:40,040
It's clear that the actual judge decisions
have to do better because the judge

409
00:24:40,041 --> 00:24:41,960
has more information.

410
00:24:42,080 --> 00:24:45,860
The actual judge has more information
than the algorithm because the judge sees

411
00:24:45,861 --> 00:24:47,510
more things than the algorithm sees.

412
00:24:48,130 --> 00:24:53,030
A psychologists look at this and say it's
not obvious. We know that, uh, judges,

413
00:24:53,420 --> 00:24:55,360
there can be a lot of
noise in the system too,

414
00:24:55,361 --> 00:24:57,140
and the extra information the judges have.

415
00:24:57,141 --> 00:25:02,141
And so what we actually see in the data
is that the predicted judge that is the

416
00:25:03,080 --> 00:25:08,080
inflammation reduced version of the judge
winds up leading do better crime and

417
00:25:08,451 --> 00:25:12,470
detention outcomes than the actual judge.
Okay.

418
00:25:13,190 --> 00:25:15,970
Now why might that be,
um,

419
00:25:16,570 --> 00:25:21,170
did you judges Miss Miss Predict?
Um, let me just tell you a,

420
00:25:21,280 --> 00:25:26,060
uh, a quick story about what I
think might be going on here.

421
00:25:26,300 --> 00:25:28,420
Um, and it's a, uh,

422
00:25:28,460 --> 00:25:33,140
it's a story that comes from
a conversation that I had
with a friend of mine who

423
00:25:33,141 --> 00:25:34,460
is an emergency room doctor.

424
00:25:34,461 --> 00:25:38,610
And he was like the attending resident
one night in the Er that he's working in

425
00:25:38,611 --> 00:25:43,250
and a patient comes in complaining of
signs that look like he's having a heart

426
00:25:43,251 --> 00:25:43,611
attack.

427
00:25:43,611 --> 00:25:47,870
And so this is a common situation that
they have in the Er and the Er docs.

428
00:25:47,871 --> 00:25:48,441
In that case,

429
00:25:48,441 --> 00:25:52,400
their key goal is to try and predict
whether the patient is actually having a

430
00:25:52,401 --> 00:25:56,990
heart attack or not. And if they are,
then they send them to the ICU for a,

431
00:25:56,991 --> 00:26:01,190
for immediate treatment.
And so the, uh, you know,

432
00:26:01,191 --> 00:26:05,210
everybody else on the team, they go in,
they do this sort of cardiac enzyme test,

433
00:26:05,300 --> 00:26:09,830
uh, to as part of the prediction.
And if it's above some sort of level,

434
00:26:10,040 --> 00:26:13,610
then that's sort of one diagnostic.
And so they, uh, they do that.

435
00:26:14,120 --> 00:26:18,200
And then my friend goes to the, sort
of, the doctor and nurse's station says,

436
00:26:18,201 --> 00:26:22,220
what do we have here? And everybody
at the station says, you know,

437
00:26:22,221 --> 00:26:25,900
we've got this guy, he's got chest pains,
we administered the cardiac enzyme test.

438
00:26:26,360 --> 00:26:29,420
It's above the threshold level,
a lot above the threshold level.

439
00:26:29,421 --> 00:26:31,970
We got to get this guy
into the ICU immediately.

440
00:26:33,620 --> 00:26:38,040
So then my friend goes
in to see the guy in, uh,

441
00:26:38,090 --> 00:26:43,010
in the waiting room and the guy
sitting there and he looks up at, um,

442
00:26:43,080 --> 00:26:47,340
at my friend, the guy sitting there
eating a snack. He's having this,

443
00:26:47,430 --> 00:26:52,430
a slice of watermelon and he's talking
to my friend and my friend's like,

444
00:26:53,041 --> 00:26:57,150
what's going on? And the guy's like,
yeah, I'm having some chest pains.

445
00:26:57,150 --> 00:27:00,330
I don't feel great. My friend talks
to him for a couple of minutes,

446
00:27:00,690 --> 00:27:02,580
goes back out to the,
uh,

447
00:27:02,640 --> 00:27:07,500
to the station and everyone's
like, okay, we're ready to go,

448
00:27:07,501 --> 00:27:12,080
right? We're going to get this guy up to
the ICU. And my friend says, uh, no, I,

449
00:27:12,810 --> 00:27:16,410
I, you know, I think he's all right. I
talked to him, he's pretty chill. Uh,

450
00:27:16,411 --> 00:27:19,950
I'm not worried about it. And
everybody else is like, no, no, no.

451
00:27:19,951 --> 00:27:22,050
We got to get them to the
ICU. And my friend says, look,

452
00:27:22,051 --> 00:27:25,590
I'm the only one who actually
saw the patient in person.

453
00:27:25,620 --> 00:27:28,380
You guys have just seen his medical
test results. And I'm telling you,

454
00:27:28,381 --> 00:27:31,530
like I talked to the guy, he's hanging
out, he's having a snack. There's,

455
00:27:31,680 --> 00:27:32,513
there's no big deal.

456
00:27:33,360 --> 00:27:37,320
15 minutes later the guy goes into cardiac
arrest and they race them to the ICU.

457
00:27:38,550 --> 00:27:43,350
I think what's going on there,
what's going on there is, you know,

458
00:27:43,440 --> 00:27:48,440
the human brain is designed to be
sensitive to very salient information,

459
00:27:50,821 --> 00:27:51,960
even if it's irrelevant.

460
00:27:53,040 --> 00:27:57,960
We don't normally associate having a
snack with having a major health disorder.

461
00:27:58,500 --> 00:28:00,960
Right.
And so you can see how my friend,

462
00:28:01,200 --> 00:28:06,200
like the fact of having the snack was
an enormously salient detail that turned

463
00:28:06,211 --> 00:28:09,330
out to be totally irrelevant.
Um,

464
00:28:09,450 --> 00:28:13,800
I think the data that we're seeing are
consistent with lots of things like that

465
00:28:13,801 --> 00:28:17,100
distracting judges in the
court setting as well. Right.

466
00:28:17,101 --> 00:28:22,101
And so the key goal for realizing the
social potential of these tools is going

467
00:28:23,491 --> 00:28:28,290
to be to figure out what exactly
those salient but irrelevant pieces of

468
00:28:28,291 --> 00:28:30,630
information are that are
leading the judges stray.

469
00:28:31,290 --> 00:28:33,480
That is not a computer science problem,
right?

470
00:28:33,481 --> 00:28:36,750
In some sense that's kind of like the
last mile problem of building these

471
00:28:36,751 --> 00:28:40,590
algorithms and helping judges and that
last mile problem is not a computer

472
00:28:40,591 --> 00:28:43,260
science problem. That's really
a behavioral science problem.

473
00:28:44,010 --> 00:28:48,450
So as I sort of look at what
is, um, if I sort of look at,

474
00:28:48,750 --> 00:28:53,370
I'm sorry, the other thing to say is, you
know, I'm using bail as an example here,

475
00:28:53,610 --> 00:28:57,450
but nothing that I've said
is unique to bill, right?

476
00:28:57,510 --> 00:29:02,510
There are tons and tons of super important
policy decisions that policymakers

477
00:29:03,510 --> 00:29:08,510
and private citizens make every day with
really important policy consequences

478
00:29:08,941 --> 00:29:13,620
that hinge on a prediction where the
person themselves are currently making the

479
00:29:13,621 --> 00:29:16,740
prediction. But in principle we could
be doing that with data and stead,

480
00:29:17,160 --> 00:29:20,520
you know what college major or
course is best for me in college,

481
00:29:20,521 --> 00:29:23,610
students are millions of college students
are trying to answer that every year.

482
00:29:23,880 --> 00:29:27,780
How long am I going to be unemployed is
I'm trying to think about how much to

483
00:29:27,781 --> 00:29:32,781
scale back my consumption or where to
set my reservation quality for some new

484
00:29:34,201 --> 00:29:37,560
job offer that I should be taking or not,
or how much house can I afford?

485
00:29:37,740 --> 00:29:41,260
What's going to happen to my
income stream over the future? Um,

486
00:29:41,470 --> 00:29:44,380
behavioral science tells us that
these are prediction problems.

487
00:29:44,381 --> 00:29:48,940
They're going to be hugely difficult for
us. So what does this teach us? Well,

488
00:29:48,941 --> 00:29:53,941
I think one thing that we can see is
that prediction can be super useful for

489
00:29:55,331 --> 00:29:59,800
making progress on these policy
problems. Um, at the same time,

490
00:29:59,801 --> 00:30:01,060
there are potential pitfalls.

491
00:30:01,750 --> 00:30:04,870
Now I think what is really
important about this,

492
00:30:04,930 --> 00:30:09,250
what's really important about this is
that we know that there are, so it's,

493
00:30:09,251 --> 00:30:12,790
what's not news is that there are
pitfalls in applying machine learning to

494
00:30:12,791 --> 00:30:14,410
policy problems,
right?

495
00:30:14,830 --> 00:30:19,360
There are probably 10,000 papers and
computer science published about,

496
00:30:19,510 --> 00:30:22,210
for instance, fairness. Okay.

497
00:30:22,600 --> 00:30:27,600
I am reasonably optimistic that the
field will eventually solve that sort of

498
00:30:28,361 --> 00:30:32,050
problem because we recognize it
and so many people are devoting,

499
00:30:32,290 --> 00:30:36,370
understandably inappropriately devoting
so much energy to solving that problem.

500
00:30:36,850 --> 00:30:40,720
What worries me about this is that there
are lots of other challenges here that

501
00:30:40,721 --> 00:30:45,490
are not even on anybody's radar screen
and that could lead us to inadvertently

502
00:30:45,910 --> 00:30:47,140
take these tools,

503
00:30:47,470 --> 00:30:52,420
import them into a policy setting and
accidentally make things worse rather than

504
00:30:52,421 --> 00:30:57,190
better. And I think the solution
to those sorts of problems,

505
00:30:57,670 --> 00:31:00,820
they're not really about
machine learning engineering.

506
00:31:01,030 --> 00:31:05,230
I think progress on those
sorts of problems really
are going to require taking

507
00:31:05,231 --> 00:31:10,231
these amazing machine learning walls
and combining them with insights from

508
00:31:10,481 --> 00:31:13,150
social science and behavioral science.
And I think the,

509
00:31:13,450 --> 00:31:18,450
the payoff to getting that
right is potentially enormous
positive social impact.

510
00:31:19,420 --> 00:31:20,253
Thank you very much.

511
00:31:22,960 --> 00:31:25,140
[inaudible]

512
00:31:27,070 --> 00:31:27,260
uh,

513
00:31:27,260 --> 00:31:30,890
you mentioned that you do things or you're
able to do things like reduce racial

514
00:31:30,891 --> 00:31:34,070
disparities and um,
in bail setting and things like that.

515
00:31:34,071 --> 00:31:36,530
And I can understand where,
um,

516
00:31:36,920 --> 00:31:40,060
there's been obviously a lot
of consternation about, uh,

517
00:31:41,030 --> 00:31:44,930
accidentally in coding
demographic characteristics
into the Dataset that didn't

518
00:31:44,931 --> 00:31:45,860
otherwise express them.

519
00:31:45,861 --> 00:31:48,710
But there may be a high correlation and
so you could say a low income area is

520
00:31:49,430 --> 00:31:53,380
higher likelihood of missing bail,
therefore imputed flight risk.

521
00:31:53,381 --> 00:31:55,430
So can you comment on how you are,

522
00:31:55,431 --> 00:31:58,790
how those things get controlled for and
how that I understand the human bias

523
00:31:58,791 --> 00:32:00,560
element obviously,
and snap judgments can,

524
00:32:00,950 --> 00:32:02,720
on the one hand it's
probably a very bad thing.

525
00:32:02,960 --> 00:32:07,960
I think it's less well understood how a
ml can be made to be more fair in a way

526
00:32:09,741 --> 00:32:12,920
that explicitly excludes a sort
of inadvertent characteristics.

527
00:32:13,320 --> 00:32:18,070
Yeah, yeah, it's a great, um, it's a
great question. You know, I think that,

528
00:32:19,090 --> 00:32:22,270
um, one of the, uh,

529
00:32:22,810 --> 00:32:25,460
I think one of the,
uh,

530
00:32:26,740 --> 00:32:28,190
challenges in,

531
00:32:28,191 --> 00:32:33,191
in my mind in taking these tools and
applying them to policy settings now is

532
00:32:33,400 --> 00:32:38,330
that there is a wedge between,
uh,

533
00:32:38,410 --> 00:32:42,060
what I think the statistics
would tell us about what it,

534
00:32:42,100 --> 00:32:45,760
what promotes fairness and the way
that our current legal structures,

535
00:32:45,770 --> 00:32:50,000
which were built around dealing with
human predictions. And decisions allow.

536
00:32:50,001 --> 00:32:53,300
And so I might turn your
question and turn on,

537
00:32:53,420 --> 00:32:57,260
turn it on its head just a little bit,
which is to say isn't necessarily,

538
00:32:57,261 --> 00:33:01,010
so if a human being is making something
like a hiring decision or a jail

539
00:33:01,011 --> 00:33:01,730
decision,

540
00:33:01,730 --> 00:33:06,730
you would say we ideally would have
that info that human being completely

541
00:33:07,341 --> 00:33:11,720
blinded to defendant race or job
applicant race. Like for sure that's true.

542
00:33:12,380 --> 00:33:17,090
I think it's less self evident that
that is true for the algorithm. Right.

543
00:33:17,360 --> 00:33:21,710
And I think part of the reason for that
is because like unlike a human being,

544
00:33:21,711 --> 00:33:26,711
the algorithm has no preferences and
has no capacity for implicit bias.

545
00:33:27,621 --> 00:33:32,621
The way that the human brain does the
algorithm really is like a dog that chases

546
00:33:32,751 --> 00:33:34,340
the car that you tell it to chase.

547
00:33:34,670 --> 00:33:39,050
Like it's just very narrowly monomaniacal.
Right.

548
00:33:39,500 --> 00:33:43,610
And sort of imagine, let me give you a
stylized example to highlight what I mean.

549
00:33:43,640 --> 00:33:44,030
Right.

550
00:33:44,030 --> 00:33:48,800
So imagine that we're living in a city
where the rate at which you know whites

551
00:33:48,801 --> 00:33:53,210
and minorities engage in crime, that
true offending rate is exactly the same.

552
00:33:53,660 --> 00:33:58,400
Okay. Suppose assist the city
is 50% white, 50% minority.

553
00:33:58,610 --> 00:34:02,900
Suppose that the police never make a
false arrest to a white city resident,

554
00:34:03,530 --> 00:34:07,910
but that half the arrests to
minorities are false arrests, right,

555
00:34:07,911 --> 00:34:11,180
and the chances that you get arrested
if you actually engage in crime at the

556
00:34:11,181 --> 00:34:13,340
same for whites and minorities,
right?

557
00:34:14,000 --> 00:34:19,000
If I take that Dataset and I give it
to an algorithm that's blinded to race,

558
00:34:20,180 --> 00:34:21,590
what's the algorithm going to do?

559
00:34:21,650 --> 00:34:26,650
The algorithm has no choice but to assume
that each additional arrest on your

560
00:34:27,591 --> 00:34:28,131
rap sheet.

561
00:34:28,131 --> 00:34:31,820
Suppose that I'm predicting an outcome
like failure to appear in court and for

562
00:34:31,821 --> 00:34:34,910
the moment, let's assume that
that's not susceptible to bias.

563
00:34:34,940 --> 00:34:36,560
We can have a separate
conversation about that,

564
00:34:36,561 --> 00:34:40,340
but just suppose that we're willing to
assume that the outcome itself is not

565
00:34:40,341 --> 00:34:45,200
susceptible to buy us, right? If the
algorithm is, is blinded to race,

566
00:34:45,201 --> 00:34:48,860
it has no choice but to assume each arrest
on your rap sheet is exactly the same,

567
00:34:48,861 --> 00:34:52,940
no matter who you are and what's gonna
wind up happening in that case is that

568
00:34:52,941 --> 00:34:55,880
African Americans on average, you're
going to have much higher predicted.

569
00:34:56,060 --> 00:34:59,150
Failure to appear risk than whites.

570
00:34:59,540 --> 00:35:04,540
And if judges are instructed by the law
to make detention decisions based on a

571
00:35:05,840 --> 00:35:07,130
failure to appear of risk,

572
00:35:07,400 --> 00:35:10,850
we're going to increase incarceration
for minorities relative to whites.

573
00:35:11,120 --> 00:35:15,080
But now notice what happens if I use
a machine learning algorithm that has

574
00:35:15,081 --> 00:35:19,730
access to race and is allowed to
search or mine the data for evidence of

575
00:35:19,731 --> 00:35:21,740
interactivity between the predictors.

576
00:35:21,741 --> 00:35:26,570
What the algorithm does and that case
is it would immediately recognize that

577
00:35:26,600 --> 00:35:31,600
each additional arrest for a white has
twice as much signal as each additional

578
00:35:32,781 --> 00:35:35,550
arrest for a minority in this city,
uh,

579
00:35:35,660 --> 00:35:40,410
for risk of FTA. And in that case,
if you sort of think about it,

580
00:35:40,411 --> 00:35:42,000
a little linear regression,
in that case,

581
00:35:42,001 --> 00:35:45,600
the slope will be half as large for
African Americans as for whites.

582
00:35:46,020 --> 00:35:50,550
And a average predicted risk in this
scenario will wind up being exactly the

583
00:35:50,551 --> 00:35:55,551
same for whites versus minorities with
only with the algorithm that has access

584
00:35:55,981 --> 00:35:56,671
to race.
No,

585
00:35:56,671 --> 00:36:01,671
I was at a national academy of Science's
panel last fall were a question like

586
00:36:01,681 --> 00:36:05,700
this came up and every computer scientist,
social scientist,

587
00:36:05,701 --> 00:36:08,040
data scientist in the room,
it was like,

588
00:36:08,460 --> 00:36:11,970
for sure we need to think about ways
of letting the algorithm, you know,

589
00:36:12,690 --> 00:36:15,720
thinking about the conditions under
which the algorithm should have access to

590
00:36:15,721 --> 00:36:20,721
race because it can in some circumstances
promote fairness and every lawyer in

591
00:36:21,541 --> 00:36:21,961
the room,

592
00:36:21,961 --> 00:36:26,730
just one completely bananas and you can
see exactly why the lawyers have that

593
00:36:26,731 --> 00:36:27,180
view.

594
00:36:27,180 --> 00:36:32,180
And so I think this is an example where
like the technology is outrunning the

595
00:36:32,191 --> 00:36:36,510
legal structures and I think that we're
gonna, this is gonna have to be a big,

596
00:36:37,260 --> 00:36:41,730
a big area that we're going to need to
think about resolving in the future as

597
00:36:41,731 --> 00:36:46,410
these sorts of algorithms become baked
into our policy systems more and more.

598
00:36:47,310 --> 00:36:48,930
Great.
Joe,

599
00:36:50,350 --> 00:36:52,670
thanks for coming in. This is awesome. Uh,

600
00:36:52,690 --> 00:36:57,250
you talked earlier about adjusting the
algorithm to choose between either crime

601
00:36:57,251 --> 00:36:58,450
rate or release rate,

602
00:36:58,660 --> 00:37:02,890
and is it truly just one of the others
there way to kind of balance both of them?

603
00:37:03,240 --> 00:37:06,420
Oh yeah. Sorry. So let me, um, uh,

604
00:37:07,050 --> 00:37:10,200
I should've given you a better graph
for this. One way to think about this.

605
00:37:10,201 --> 00:37:11,670
As you can imagine like a,

606
00:37:12,080 --> 00:37:15,750
a two dimensional space where you have
crime rate on one axis of the detention

607
00:37:15,751 --> 00:37:20,751
rate on the other axis and the judges
current decision is a point in that space,

608
00:37:21,750 --> 00:37:22,081
right?

609
00:37:22,081 --> 00:37:26,550
And what the Algorithm let's you do the
algorithm winds up predicting crime risk

610
00:37:26,551 --> 00:37:28,290
much more accurately than the judge.

611
00:37:28,920 --> 00:37:33,920
And so the algorithm let's
you have lots of both,

612
00:37:35,220 --> 00:37:39,750
lots of one without increasing the like
less of one holding the other one fixed

613
00:37:39,751 --> 00:37:40,320
door.

614
00:37:40,320 --> 00:37:44,640
So you have a range of options that you
can choose from in terms of how you want

615
00:37:44,641 --> 00:37:46,410
to realize those gains,
right?

616
00:37:46,411 --> 00:37:49,650
So one extreme is I hold the
jail population constant.

617
00:37:49,800 --> 00:37:52,110
I take all of the gain in
the form of reduced crime.

618
00:37:52,440 --> 00:37:54,840
The other extreme is I take
the crime rate constant. I,

619
00:37:54,990 --> 00:37:59,990
I realize all of the game from reduced
jail or I can pick any point in between

620
00:38:00,121 --> 00:38:01,470
there and get some of both.

621
00:38:01,740 --> 00:38:06,740
And where you fall on that potential
outcome space is entirely a policy

622
00:38:06,781 --> 00:38:09,750
decision. Not a, not a social
science or computer science decision.

623
00:38:11,520 --> 00:38:12,353
Thank you.

624
00:38:15,720 --> 00:38:18,300
Hey, so are you doing, uh,
I'm one of those engineers,

625
00:38:18,330 --> 00:38:22,040
computer scientists who is going
bananas, right? Because there,

626
00:38:22,070 --> 00:38:23,640
there are a ton of concerns here.
Oh.

627
00:38:23,641 --> 00:38:26,970
And I've worked on machine learning
models and we know that the data makes a

628
00:38:26,971 --> 00:38:29,820
difference, right? The data
that we're ingesting. Yup.

629
00:38:30,740 --> 00:38:32,700
It's pretty easy to beat,

630
00:38:33,090 --> 00:38:36,460
I think Chicago rates around racism,
right?

631
00:38:36,461 --> 00:38:41,020
You're on racist decisions around
socioeconomic status, et Cetera. Right?

632
00:38:41,030 --> 00:38:43,840
I mean, Chicago is when the most
racist segregated places there are.

633
00:38:44,200 --> 00:38:47,320
And maybe it's because I grew up a
few blocks from that jail, right.

634
00:38:47,380 --> 00:38:49,120
Just seen it over and over and over again.
Right?

635
00:38:49,330 --> 00:38:54,330
So the fact that the academy that the
algorithm can beat Chicago and Cook County

636
00:38:55,091 --> 00:38:58,960
judge rates is, um, it's
great to see, right?

637
00:38:58,961 --> 00:39:01,570
And it's great that we're making
better progress. But in and of itself,

638
00:39:01,571 --> 00:39:05,230
I don't think actually says that the
algorithm is the way we should go. Right.

639
00:39:05,260 --> 00:39:07,580
There's other human factors that
have to be taken account. Yep.

640
00:39:07,840 --> 00:39:08,980
So part of it is yes,

641
00:39:08,981 --> 00:39:12,700
the data is messy because we have
so much embedded systemic racism.

642
00:39:13,210 --> 00:39:16,900
And the last people I want to turn more
data to is the Chicago cops because of

643
00:39:16,901 --> 00:39:20,060
everything that they do in the
city to our communities. Right. Uh,

644
00:39:20,200 --> 00:39:23,710
so there's a lot of danger at play in how
we use the data and you just mentioned

645
00:39:23,711 --> 00:39:24,544
as well.
Um,

646
00:39:24,610 --> 00:39:27,340
and answer to the previous question is
taking into account the demographics are

647
00:39:27,341 --> 00:39:29,420
not.
And we know from Kathy O'malley,

648
00:39:29,421 --> 00:39:33,790
he spoke from the weapons
of math destruction book,
right. That we, you know,

649
00:39:33,850 --> 00:39:38,410
whether it's credit card rates, there's
other ways to proxy race, right.

650
00:39:38,411 --> 00:39:43,330
That end up also embedding the same
conditions that led to this situation

651
00:39:43,360 --> 00:39:46,540
throughout. So I don't want to make a, I
understand that we can do a lot better.

652
00:39:46,570 --> 00:39:48,520
Right.
And the algorithm could do better by also,

653
00:39:49,060 --> 00:39:52,480
I definitely want to challenge the
dichotomy that says the algorithm and the

654
00:39:52,481 --> 00:39:54,580
technology in and of itself.
Yes.

655
00:39:54,610 --> 00:39:58,020
Better without taking into account
all the other embedded, uh,

656
00:39:58,080 --> 00:40:02,540
yeah, concern. Here's the,
here's the what. So it's a
very fair question. Um, or, uh,

657
00:40:02,690 --> 00:40:06,010
here's a way that I would, I would
sort of think about it is, um,

658
00:40:06,720 --> 00:40:10,800
I think of the role as a,
of people like me and people like you,

659
00:40:10,801 --> 00:40:14,970
the other people in the,
in the room is to, um,

660
00:40:15,570 --> 00:40:19,710
build tools and honestly and accurately
describe what they're capable of doing.

661
00:40:20,570 --> 00:40:23,430
A and in a policy set, you
know, in a commercial setting,

662
00:40:23,431 --> 00:40:27,360
you build these tools and then if you
think they can, they can make money.

663
00:40:27,690 --> 00:40:31,530
You deploy them and if they're successful,
they keep going.

664
00:40:31,531 --> 00:40:33,960
And if they're not successful,
they get scrapped. Right?

665
00:40:33,961 --> 00:40:37,650
In the policy setting it's different.
It's not my decision or you just,

666
00:40:37,651 --> 00:40:41,580
your decision to make,
it's like some larger collective society,

667
00:40:41,610 --> 00:40:45,000
societal decision about whether we
want to use these tools are not.

668
00:40:45,690 --> 00:40:49,190
And I think in this case it really is a,
um,

669
00:40:50,010 --> 00:40:53,760
I think it is something, it is a question
that is not easily dismissed, right?

670
00:40:53,761 --> 00:40:56,610
So I come to you and say,
I have a policy,

671
00:40:56,640 --> 00:41:01,110
let's call it a black box black
box policy for the moment, right?

672
00:41:01,620 --> 00:41:06,620
I have a jail system that is 90% minority
in this large anonymous city of eight

673
00:41:07,111 --> 00:41:09,570
and a half million people
that were using the data from,

674
00:41:09,870 --> 00:41:14,280
I can assure you that the city population
is not anywhere near 90% minority.

675
00:41:14,610 --> 00:41:15,443
Okay.

676
00:41:15,540 --> 00:41:20,520
I've got a black box policy got can
reduce the size of the jail population by

677
00:41:20,521 --> 00:41:23,060
42% right?

678
00:41:23,061 --> 00:41:26,370
Suppose for the moment we can get
judges to follow the algorithmic

679
00:41:26,820 --> 00:41:29,610
recommendations perfectly or
the black box ballsy record.

680
00:41:30,030 --> 00:41:32,880
I've got a policy lever that lets
you reduce the size of the pot,

681
00:41:32,930 --> 00:41:37,930
the jail by 42% who benefits massively
from a 42% reduction in the jail

682
00:41:40,581 --> 00:41:45,410
population in a world in which the
jail population is 90% minority.

683
00:41:45,650 --> 00:41:48,830
90% of the people that the
algorithm that the black box policy,

684
00:41:48,831 --> 00:41:50,870
in that case we're doing releases,

685
00:41:51,050 --> 00:41:55,250
so benefit from this are minorities.
Right?

686
00:41:55,370 --> 00:42:00,370
And then on top of that
we can disproportionately
further reduce the racial

687
00:42:00,411 --> 00:42:05,030
composition of the jail. Now it is
true that you can look at that and say,

688
00:42:05,720 --> 00:42:10,400
yeah, but the algorithms not
perfect because it's still
relying on data that have

689
00:42:10,401 --> 00:42:14,000
baked in biases. Those are the same
data that we would hand to the judge.

690
00:42:14,930 --> 00:42:19,250
I think the flip side is the alternative
is the judge using exactly the same

691
00:42:19,251 --> 00:42:24,251
bias data in the current status quo system
that gives us a jail population that

692
00:42:24,441 --> 00:42:28,580
is much, much larger and has a higher
chair my than it currently has.

693
00:42:28,940 --> 00:42:33,140
And I feel like this is one
where reasonable people can
argue both sides of this.

694
00:42:33,170 --> 00:42:37,190
And to me it's not,
it's not a crazy position to say,

695
00:42:37,550 --> 00:42:41,810
I will capitalize on the fairness
gains that I have in front of me,

696
00:42:41,811 --> 00:42:46,811
which is the 43% reduction in the jail
and a reduction in a share minority by 10

697
00:42:47,241 --> 00:42:48,074
percentage point.

698
00:42:50,540 --> 00:42:54,340
Thank you. Yep. Time for
one more question. Um, yeah,

699
00:42:54,341 --> 00:42:58,960
I had a question around also kind of
we're using data around the police and

700
00:42:58,961 --> 00:42:59,531
things like that.

701
00:42:59,531 --> 00:43:04,531
So are there projects that you're doing
to analyze a ras and like the geographic

702
00:43:05,541 --> 00:43:05,940
breakdown,

703
00:43:05,940 --> 00:43:09,490
it was racial breakdowns of things like
that that the police are doing versus

704
00:43:09,850 --> 00:43:13,720
you know, kind of continuing this kind of
work. But just on the opposite side. Um,

705
00:43:14,260 --> 00:43:16,390
and then also,
I think you may have touched on it,

706
00:43:16,391 --> 00:43:21,391
but how you're using this ml model
in real time with judges and how,

707
00:43:22,600 --> 00:43:23,560
what you've seen with that,

708
00:43:23,561 --> 00:43:28,120
if that legal system is
somewhat open to these tools,

709
00:43:28,480 --> 00:43:30,640
just to speak a little bit
more on that if you could.

710
00:43:31,200 --> 00:43:35,460
Yeah, yeah. I think, um, the, uh,

711
00:43:36,570 --> 00:43:39,840
for the, um, uh,

712
00:43:40,530 --> 00:43:42,720
let me take the second
question for starters.

713
00:43:42,721 --> 00:43:47,721
So we are working with a large American
anonymous city of eight and a half

714
00:43:48,150 --> 00:43:52,470
million people to think about whether
there are ways to actually, uh, you know,

715
00:43:52,471 --> 00:43:55,950
so the tool that I, the results
that I described so far, you know,

716
00:43:55,951 --> 00:43:59,160
we did that as an academic study
as a kind of proof of concept.

717
00:43:59,850 --> 00:44:04,770
And the large anonymous cities
said, uh, their policy decision,

718
00:44:04,860 --> 00:44:06,180
or you might disagree with it,

719
00:44:06,181 --> 00:44:10,620
but there are policy decision was if we
could really reduce our jail population

720
00:44:10,621 --> 00:44:14,550
by 42% without increasing crime,
that's something that we want to do.

721
00:44:14,551 --> 00:44:18,210
And that's something that we view as
a step towards promoting fairness.

722
00:44:18,211 --> 00:44:22,290
And so we're working with them
now to build this thing. Um,

723
00:44:22,710 --> 00:44:26,400
we're seeing sort of similarly encouraging
results in terms of the predictive

724
00:44:26,401 --> 00:44:27,234
accuracy.

725
00:44:27,480 --> 00:44:32,480
I think the frontier science problem that
we're encountering is how do you give

726
00:44:32,791 --> 00:44:36,720
this to the judges in a way where
they're able to learn their comparative

727
00:44:36,721 --> 00:44:41,190
advantage versus when the Algorithm
has the comparative advantage in an

728
00:44:41,191 --> 00:44:44,370
environment in which,
right now there's not much feedback.

729
00:44:44,490 --> 00:44:49,490
And so we need to build a
bunch of scaffolding around
what the judge is doing to

730
00:44:50,281 --> 00:44:55,281
make sure that that risk tool itself
translates into real better decisions.

731
00:44:57,330 --> 00:44:58,163
Thanks.

732
00:44:59,050 --> 00:45:01,510
Well, thank you all for coming.
That concludes our talk.

733
00:45:01,810 --> 00:45:03,730
Let's give you guys a round of applause.

