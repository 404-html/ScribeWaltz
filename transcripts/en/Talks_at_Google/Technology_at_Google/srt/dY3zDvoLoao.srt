1
00:00:06,240 --> 00:00:10,490
A Neat Suarez is the executive director
of the Machine Intelligence Research

2
00:00:10,490 --> 00:00:12,350
Institute and leads the research program,

3
00:00:13,460 --> 00:00:16,400
Nate as the primary author of
most of Mary's technical agenda,

4
00:00:16,520 --> 00:00:20,480
including the overview document agent
foundations for aligning superintelligence

5
00:00:20,481 --> 00:00:25,130
with human interests and are
the Aai paper or curability Oh,

6
00:00:25,190 --> 00:00:26,960
he's here today to discuss
his work with Mary.

7
00:00:27,290 --> 00:00:31,170
And why thinking about the impact of
Ai, uh, not just your Chromecast is,

8
00:00:31,171 --> 00:00:34,640
is an important idea. So thanks so
much. Take Nay Suarez. Everyone.

9
00:00:39,350 --> 00:00:40,183
Okay.

10
00:00:40,820 --> 00:00:44,360
Hello. Uh, I'll try and
go through this on the,

11
00:00:44,390 --> 00:00:47,900
on the faster end so that we
can have time for Q and. A. Uh,

12
00:00:48,410 --> 00:00:50,270
so as mentioned,

13
00:00:50,330 --> 00:00:53,510
I'm the director of the machine
intelligence research institute.

14
00:00:54,220 --> 00:00:59,220
A very roughly speaking, we're a
group that's thinking in the longterm,

15
00:00:59,380 --> 00:00:59,650
uh,

16
00:00:59,650 --> 00:01:04,140
about artificial intelligence and trying
to make sure that by the time we have a

17
00:01:04,200 --> 00:01:06,560
advanced AI systems,
we also know how to aim them.

18
00:01:07,460 --> 00:01:12,280
I'll say more about what I mean by that.
Uh, throughout the course of the talk, uh,

19
00:01:12,530 --> 00:01:17,000
I, I'm sort of going to assume that
people here know what I mean by artificial

20
00:01:17,001 --> 00:01:21,710
intelligence. That said, it's a fairly
vaguely defined term. There's not a,

21
00:01:22,070 --> 00:01:24,450
there's not one concrete definition,
uh,

22
00:01:24,470 --> 00:01:29,120
but to sort of wave in the direction of
this intelligence concept, I'll say that,

23
00:01:29,510 --> 00:01:33,680
uh, if you look all around us at,
uh, you know, the room we're in,

24
00:01:33,710 --> 00:01:37,340
the technology we're using,
this is not a product of our brown.

25
00:01:37,340 --> 00:01:39,430
This is a product of our brains.
Uh,

26
00:01:39,440 --> 00:01:44,390
the reason that humans are the dominant
animal on the face of the planet is not

27
00:01:44,391 --> 00:01:48,470
because we are faster than bears. Uh, it's
not because we are stronger than bears.

28
00:01:48,850 --> 00:01:52,910
Uh, it's because we are
smarter than bears. And, uh,

29
00:01:53,360 --> 00:01:54,560
over the course of time,

30
00:01:54,561 --> 00:01:59,360
if you look at the largest drivers
of change in technical, uh,

31
00:01:59,420 --> 00:02:03,270
or if you look at the largest
drivers of change in, uh, uh,

32
00:02:03,440 --> 00:02:06,020
human and animal welfare over
the last couple of centuries,

33
00:02:06,050 --> 00:02:08,840
it has been due to technological
and scientific innovation. Uh,

34
00:02:09,010 --> 00:02:12,650
and if we can automate technological
and scientific innovation that has the

35
00:02:12,651 --> 00:02:15,260
potential to change the world,
uh,

36
00:02:15,980 --> 00:02:20,600
on a scale not seen since maybe the
industrial evolution, perhaps earlier. Uh,

37
00:02:20,601 --> 00:02:24,460
so this is, uh, you know, Ai, uh, uh,

38
00:02:24,560 --> 00:02:27,530
artificially intelligent machines that
exceed humans in this sort of general

39
00:02:27,531 --> 00:02:30,170
listening capability are
not coming next year. Uh,

40
00:02:30,171 --> 00:02:34,430
but they probably are coming in our
lifetime. A Ai is now making humans money.

41
00:02:34,460 --> 00:02:36,290
And when something starts
making humans money,

42
00:02:36,291 --> 00:02:39,980
they tend to put a lot of
resources into it. Uh, so there's,

43
00:02:40,040 --> 00:02:44,650
there's lots of brilliant people working
on it, many of them here. And, uh, uh,

44
00:02:44,660 --> 00:02:46,550
I think it's a decent bet that it
comes in their lifetime. Again,

45
00:02:46,551 --> 00:02:48,970
I wouldn't worry about it coming around
the corner, but this does mean that, uh,

46
00:02:49,010 --> 00:02:53,420
it's something we should, we should
think very seriously about. Now,

47
00:02:53,690 --> 00:02:56,960
when people start talking about how
we should think very seriously about

48
00:02:56,961 --> 00:03:01,350
artificial intelligence, they'll often
raise a number of concerns. The ways,

49
00:03:01,360 --> 00:03:06,070
concerns, like, uh, what happens
when the computer becomes conscious?

50
00:03:06,570 --> 00:03:10,570
Uh, the Alaska. Like, will it like
humans? Will it hate humans? Uh,

51
00:03:10,780 --> 00:03:14,050
will it reflect on his goals and decide
that they're silly and then decided that

52
00:03:14,051 --> 00:03:16,780
it should kill us? They'll get
concerned about these sorts of,

53
00:03:16,840 --> 00:03:21,400
these sorts of questions. These are
not the right questions. Uh, the,

54
00:03:21,760 --> 00:03:22,840
there is no,

55
00:03:23,170 --> 00:03:26,980
there is no level of complexity and
a computer program where the computer

56
00:03:26,981 --> 00:03:31,210
program is suddenly imbibed
with a spirit. Uh, there's no,

57
00:03:31,690 --> 00:03:34,690
uh,
there's no amount of code.

58
00:03:34,691 --> 00:03:36,700
You can run in a CPU or the CPU will,

59
00:03:36,730 --> 00:03:39,760
will be inhabited by a ghost and the
ghost will look down at the instructions

60
00:03:39,761 --> 00:03:42,580
and decide whether or not to execute them.
Uh,

61
00:03:42,700 --> 00:03:44,620
the CPU just keeps on
executing the next instruction.

62
00:03:44,650 --> 00:03:49,090
You can of course program something that,
uh, you know, rewrites its own code,

63
00:03:49,660 --> 00:03:53,650
but there's no, there's no ghost
that comes down when the, uh,

64
00:03:53,740 --> 00:03:57,340
there's no ghost that comes down and
questions the goals that you gave it to

65
00:03:57,341 --> 00:04:02,040
unless you program something that is,
you know, reading as goals and uh,

66
00:04:02,260 --> 00:04:05,710
rewriting them on purpose. Um, so the,

67
00:04:05,711 --> 00:04:09,670
the concerns of the questions we should
be asking to quote Stuart Russell,

68
00:04:09,671 --> 00:04:12,610
who's the author of the coauthor
of one of the leading AI textbooks,

69
00:04:13,060 --> 00:04:15,250
which many of you may have read. Uh, the,

70
00:04:15,251 --> 00:04:17,920
the concerns with AI are not about
spooky merchant consciousness.

71
00:04:17,921 --> 00:04:21,520
The concerns with AI are about the
ability to make high quality decisions,

72
00:04:21,521 --> 00:04:25,480
which is exactly the thing we're trying
to do with Ai. So, uh, the concerns,

73
00:04:25,510 --> 00:04:29,470
the sort of double edged sword, not sort
of the spooky questions, but the, uh,

74
00:04:29,500 --> 00:04:31,270
the sort of what if we succeed questions.

75
00:04:32,650 --> 00:04:36,040
So many people when they start talking
about concerns about AI will throw up a

76
00:04:36,041 --> 00:04:38,050
picture of the terminator.
Uh,

77
00:04:38,110 --> 00:04:41,770
I was once quoted in a news article
making fun of people who've put up

78
00:04:41,771 --> 00:04:45,310
terminator pictures in all their articles
about AI next to a terminator picture.

79
00:04:46,920 --> 00:04:50,600
Uh, I learned something
about the media that day. Uh,

80
00:04:50,750 --> 00:04:52,030
I think there's a much better picture.

81
00:04:52,180 --> 00:04:55,660
So this is Mickey Mouse
in the movie Fantasia, uh,

82
00:04:55,690 --> 00:05:00,580
who has very cleverly and chanted a
broom to do his chore of Filling the

83
00:05:00,580 --> 00:05:03,760
Cauldron. Uh, so how
might make you do this?

84
00:05:04,510 --> 00:05:08,740
We can imagine that Mickey does this
by writing a computer program and then

85
00:05:08,741 --> 00:05:11,340
having the berm execute
that computer program, uh,

86
00:05:11,530 --> 00:05:15,460
and is thereby automating his
chores. Very, very clever. Uh,

87
00:05:15,461 --> 00:05:17,360
so what might this program looked like?
Well making,

88
00:05:17,361 --> 00:05:20,140
you might start by writing down some
sort of scoring function or objective

89
00:05:20,141 --> 00:05:21,430
function,
uh,

90
00:05:21,460 --> 00:05:25,180
where in this case the Broman gets one
point if the cauldron is full and zero

91
00:05:25,181 --> 00:05:28,000
points. If the cauldron is
empty, this seems simple enough,

92
00:05:29,380 --> 00:05:33,490
then Micky might write a program
that works as follows. I,

93
00:05:33,491 --> 00:05:37,780
it takes some set of available
actions, uh, and they're,

94
00:05:37,810 --> 00:05:41,950
Mickey writes a program that can take one
of these actions that's input and then

95
00:05:41,951 --> 00:05:45,220
calculate how high the score is expected
to be if the broom takes that action.

96
00:05:45,610 --> 00:05:49,120
So this is a predictor
of, uh, this score, uh,

97
00:05:49,180 --> 00:05:51,700
given that the is taking,
uh,

98
00:05:51,750 --> 00:05:55,460
they give an action and then Mickey can,
uh,

99
00:05:55,570 --> 00:05:59,840
write a function that just looks at all
the actions for a certain amount of time,

100
00:06:00,140 --> 00:06:04,730
uh, predicts which ones lead to high
scores and then finds one that leads to a

101
00:06:04,731 --> 00:06:05,660
pretty high score.

102
00:06:06,140 --> 00:06:09,980
The reason this is a sort of Arg Max
here is that we don't need to assume that

103
00:06:09,981 --> 00:06:14,780
the, uh, that the broom is able to
predict every single action. Uh,

104
00:06:14,990 --> 00:06:17,930
it may, there may be some actions
where it can predict the consequences.

105
00:06:17,931 --> 00:06:20,750
There may be some actions that
doesn't have time to think about, uh,

106
00:06:20,840 --> 00:06:24,260
but it's sort of, uh, using,
using some ethernet or another.

107
00:06:24,261 --> 00:06:29,060
Mickey will program it to, uh, uh, uh,

108
00:06:29,090 --> 00:06:33,170
in a given set of time. Look at a bunch of
actions, see which ones it predicts, uh,

109
00:06:33,230 --> 00:06:36,230
will lead to a high score and then
execute the one that leads to the highest

110
00:06:36,231 --> 00:06:38,090
score can find with
the allotted resources.

111
00:06:38,720 --> 00:06:41,450
So this is a pretty simple description
of a program. Uh, you know,

112
00:06:41,451 --> 00:06:45,230
the devil's in the details, uh, writing
this sort of predictor and this sort of,

113
00:06:45,530 --> 00:06:49,940
uh, uh, smart search through action
space is basically the whole field of Ai.

114
00:06:50,300 --> 00:06:53,990
Uh, and you know that that is not
proven easy problem. But conceptually,

115
00:06:53,991 --> 00:06:58,490
this is a simple program. And let's say
that Mickey does this and runs the broom,

116
00:06:59,000 --> 00:07:02,690
uh, seems, seems easy enough.
It seems like it should
work, right? So what happens?

117
00:07:03,920 --> 00:07:06,080
Well,
what happens in Fintech fantasia is this,

118
00:07:06,560 --> 00:07:11,480
the broom starts overflowing the cauldron.
This raises two questions. First,

119
00:07:11,990 --> 00:07:14,990
why did the broom start overflowing
the cauldron? And secondly,

120
00:07:15,020 --> 00:07:17,630
why is there a person standing in
front of you telling you that this is

121
00:07:17,631 --> 00:07:20,600
realistic?
Uh,

122
00:07:20,630 --> 00:07:24,920
so the first difficulty is that the,

123
00:07:24,940 --> 00:07:28,610
the objective function that
Mickey gave this brim is not,

124
00:07:28,611 --> 00:07:33,560
in fact the objective function that
Mickey, uh, we'll sort of intending, uh,

125
00:07:33,590 --> 00:07:36,050
Mickey wanted the culture and full and
didn't want to call it an that part was

126
00:07:36,051 --> 00:07:38,660
right, but Mickey, uh, uh,

127
00:07:38,720 --> 00:07:42,710
sort of didn't want the workshop flooded
the workshop being flooded as far worse

128
00:07:42,711 --> 00:07:44,710
than the workshop being empty.
Uh,

129
00:07:44,720 --> 00:07:46,580
and once you realize that
Micky left that term out,

130
00:07:46,581 --> 00:07:48,740
you realize that there's a bunch of
other terms. Mickey also left out,

131
00:07:49,130 --> 00:07:51,750
for example, flooding the
workshop is funny. Uh,

132
00:07:52,070 --> 00:07:56,240
but it's not funny enough
to justify flooding it. Uh,

133
00:07:56,300 --> 00:07:59,500
and uh, killing someone is way worse. Uh,

134
00:07:59,510 --> 00:08:02,540
you should do something as bad as flooding
the workshop many times over in order

135
00:08:02,541 --> 00:08:06,260
to save a life. Uh, and, and, uh,

136
00:08:06,500 --> 00:08:10,220
we realized when we look at
this that uh, Mickey, the,

137
00:08:10,260 --> 00:08:12,620
the problem was not so much the broom,
you know,

138
00:08:12,621 --> 00:08:14,420
gaining a mind of its
own and define Mickey.

139
00:08:14,420 --> 00:08:17,090
The trouble here is that the brown did
exactly as it what it was programmed to

140
00:08:17,091 --> 00:08:20,120
do all too well and Mickey didn't
understand the consequences of what he was

141
00:08:20,121 --> 00:08:20,954
programming.

142
00:08:22,160 --> 00:08:26,790
The second difficulty is that
Mickey Program, the Berm to uh,

143
00:08:27,020 --> 00:08:30,410
maximize as hard as it could,
the expectation of its score.

144
00:08:30,950 --> 00:08:35,720
What this means is if the broom
currently assesses a subjective,

145
00:08:35,780 --> 00:08:40,780
a 99.9% probability that the cauldron
will be full a and it has all these extra

146
00:08:41,481 --> 00:08:42,440
resources lying around,

147
00:08:43,400 --> 00:08:47,960
then the action that most maximize the
score is the one that uses all those

148
00:08:47,961 --> 00:08:49,640
spare resources to push the probability.

149
00:08:49,641 --> 00:08:53,450
So it's subjective probability that
the bucket is full up to 99.9%.

150
00:08:54,470 --> 00:08:58,650
Uh, once it has very high confidence
that the bucket's full or that the,

151
00:08:58,651 --> 00:09:02,460
that their cauldron's full well, there
might be a leak in the cauldron. It might,

152
00:09:02,490 --> 00:09:04,770
it's sensors might not
be working correctly.

153
00:09:04,800 --> 00:09:08,310
There might be someone trying to
take the water out of the cauldron.

154
00:09:08,520 --> 00:09:10,420
Presumably Mickey wanted the
cauldron full for a reason,

155
00:09:10,421 --> 00:09:13,680
then someone's going to get the water out.
So there's all these,

156
00:09:13,681 --> 00:09:17,000
there's all these small probabilities
that the color is not quite full and you

157
00:09:17,001 --> 00:09:19,950
know, there's nothing else,
uh, you know, the, the,

158
00:09:19,951 --> 00:09:22,600
the sort of default actions of sit around
and do nothing after they called her

159
00:09:22,610 --> 00:09:24,810
and looks pretty full, isn't it
going to make the color any fuller?

160
00:09:25,440 --> 00:09:29,560
So may as well the action that
the action that maximizes, uh,

161
00:09:29,561 --> 00:09:32,760
this score uses the spare resources
that are available to increase the

162
00:09:32,761 --> 00:09:35,730
probability even a tiny bit.
The rum has nothing better to do.

163
00:09:35,731 --> 00:09:37,260
That increases then the score.

164
00:09:37,261 --> 00:09:40,440
So even if even if the remaining resources
can increase the score a tiny bit,

165
00:09:40,740 --> 00:09:41,250
this bro,

166
00:09:41,250 --> 00:09:44,670
we'll put those resources towards eking
out that tiny little bit of probably,

167
00:09:46,340 --> 00:09:49,860
uh, the, we in the, in
the AI alignment business,

168
00:09:49,861 --> 00:09:53,790
we refer to this as an open ended goal.
There's the sort of goal where, uh, it's,

169
00:09:53,800 --> 00:09:56,410
it's resource hungry,
it's effort hungry. Uh,

170
00:09:56,610 --> 00:09:58,320
whenever there are spare
resources lying around,

171
00:09:58,321 --> 00:10:01,110
you might as well repurpose them
in towards the filling your goal.

172
00:10:01,440 --> 00:10:06,430
A contrast this with a task like goal
where uh, there's, there's sort of a,

173
00:10:06,480 --> 00:10:10,410
the goal we had in our heads for filling
the cauldron was much more task task.

174
00:10:10,411 --> 00:10:13,230
Like we sort of wanted to fill
the cultured enough and then stop.

175
00:10:14,220 --> 00:10:17,130
And you could try to do that by putting
in all these terms on the objective

176
00:10:17,131 --> 00:10:20,160
function that are like, uh, uh, you know,

177
00:10:20,240 --> 00:10:22,200
penalizing for too much
resources or something. I,

178
00:10:22,320 --> 00:10:26,520
there are many ways you could
try to do this, but uh, uh, uh,

179
00:10:26,521 --> 00:10:27,670
in essence the,

180
00:10:27,720 --> 00:10:32,280
the question or the difficulty that make
you run into here is that he created an

181
00:10:32,281 --> 00:10:37,050
open ended goal rather than a
task like goal. Uh, and the,

182
00:10:37,080 --> 00:10:41,550
this, making a test like goal is
something of a subtle problem. Uh,

183
00:10:41,551 --> 00:10:45,120
in this example, the original objective
function and looked pretty task like a,

184
00:10:45,121 --> 00:10:49,830
it was bounded. There was no way to get
more and more and more utility. Uh, uh,

185
00:10:50,080 --> 00:10:53,010
uh, like it's not like he got one point
for every bucket of water report and then

186
00:10:53,011 --> 00:10:56,100
there would clearly be a, an incentive
to overfill the color. And it was,

187
00:10:56,101 --> 00:10:58,770
it was founded, uh,
and it was very simple.

188
00:10:58,800 --> 00:11:01,440
You can look at the objective function
and you didn't see any spare terms.

189
00:11:01,840 --> 00:11:04,380
And what made this into
an open ended goal, uh,

190
00:11:04,381 --> 00:11:06,420
was the expected utility maximizing,

191
00:11:06,920 --> 00:11:10,840
taking subjective probabilities
that couldn't go to one, uh, and,

192
00:11:10,880 --> 00:11:13,150
and maximizing the expectation.
Uh,

193
00:11:13,151 --> 00:11:17,920
and there are actually a number of
different ways that a goal that looks, uh,

194
00:11:18,060 --> 00:11:21,820
uh, task like can turn
out to be open ended. Um,

195
00:11:22,860 --> 00:11:26,070
and, uh, one example is that if the,

196
00:11:26,071 --> 00:11:27,870
if the system has lots of sub processes,

197
00:11:28,050 --> 00:11:30,330
some of which are pursuing
these sort of open ended goals,

198
00:11:30,331 --> 00:11:34,470
you might be in trouble,
even if the overarching goal,
uh, is sort of task like,

199
00:11:36,150 --> 00:11:40,170
uh, so the, the standard objection at
this point is a, okay, this is not a,

200
00:11:40,171 --> 00:11:43,080
it's not a thing we need to worry
about a ton. And Ai, like, yeah,

201
00:11:43,081 --> 00:11:44,190
the system might not,

202
00:11:44,280 --> 00:11:49,230
there might be unintended consequences
of the objective function. Uh, and, uh,

203
00:11:49,290 --> 00:11:51,270
and the goal might be more
open ended than we thought,

204
00:11:51,271 --> 00:11:54,250
but we can always sort of pull the plug.
Right? Can we just hit the stop button?

205
00:11:55,400 --> 00:11:59,830
So Mickey tries to this,
uh, and it doesn't work.

206
00:12:00,510 --> 00:12:03,160
Uh, and again, I claim that
this is very realistic, right?

207
00:12:03,161 --> 00:12:05,320
Why is it not working Mickey's case?
It doesn't work because Mickey,

208
00:12:05,350 --> 00:12:09,340
because Mickey's broom has repurposed
lots of spare material, uh,

209
00:12:09,400 --> 00:12:13,660
towards things like making more brooms
that can help make sure that the

210
00:12:13,661 --> 00:12:16,910
cauldron's full, what else
was the resource going to
do? Like the action that,

211
00:12:16,940 --> 00:12:20,830
that maximize this score, uh, put
those spare resources to use. Uh,

212
00:12:20,831 --> 00:12:24,160
also if the broom was clever enough to
know that sometimes Mickey tries to get

213
00:12:24,161 --> 00:12:29,110
water out of the cauldron and Mickey,
you will when make, he realizes, uh,

214
00:12:29,470 --> 00:12:31,300
that the objective function
is not what he wanted.

215
00:12:31,480 --> 00:12:36,010
Attempted to hack me apart with an
ax. Uh, the broom like the, the score,

216
00:12:36,011 --> 00:12:41,011
maximizing a actions in something that's
predicting those aspects of the world,

217
00:12:41,980 --> 00:12:46,700
uh, will, uh, automatically
by default. Uh,

218
00:12:47,020 --> 00:12:50,650
do things like make copies,
resist,

219
00:12:50,651 --> 00:12:55,090
shutdown a, this has an
analog I claim in, uh,

220
00:12:55,930 --> 00:13:00,550
in the real world if we tried to
design sufficiently advanced AI systems

221
00:13:02,050 --> 00:13:06,320
by default, if this
system has a model of, uh,

222
00:13:06,370 --> 00:13:09,910
your ability to shut it down and the
fact, and it has a model of, for example,

223
00:13:10,140 --> 00:13:13,180
so let's say that you build an AI system
that has some objective and models,

224
00:13:13,181 --> 00:13:18,040
the fact that, uh, that you're going
to attempt to shut it down animals,

225
00:13:18,041 --> 00:13:21,700
the fact that this will lead to it not
fulfilling its objective, then by default,

226
00:13:21,701 --> 00:13:24,230
if you haven't done something clever
with your objective function, uh,

227
00:13:24,470 --> 00:13:27,950
it now has an incentive to attempt to
subvert you shutting it down or they're,

228
00:13:27,970 --> 00:13:30,670
I can as a separate problem.
But this is the default incentive.

229
00:13:31,060 --> 00:13:33,700
You have to put in a extra work yourself.

230
00:13:34,210 --> 00:13:38,440
If you want to subvert that as a
incentive. That's not to say you can't,

231
00:13:38,470 --> 00:13:42,940
this is not a, is not a oral
doomed talk. Uh, it's just, uh,

232
00:13:42,970 --> 00:13:46,300
by default if you build a
powerful optimizer and send
it towards an objective,

233
00:13:46,590 --> 00:13:48,340
uh,
and then try to turn it off the,

234
00:13:48,540 --> 00:13:50,620
the first step you built something
that was gonna try to subvert you,

235
00:13:50,621 --> 00:13:53,380
turning it off in the second
step. You are, you've now
made yourself an adversary.

236
00:13:53,670 --> 00:13:57,520
Uh, and if ever you're in a situation
where you think your programming,

237
00:13:57,630 --> 00:14:00,910
if you think you're writing a
computer program to be your adversary,

238
00:14:00,911 --> 00:14:05,810
you should probably stop and write a
different computer program. Uh, so, uh,

239
00:14:05,840 --> 00:14:08,990
as an example of of ways you might
try to try to solve this problem, uh,

240
00:14:09,100 --> 00:14:12,650
here's one way Mickey. You
could go back and try to, uh,

241
00:14:12,700 --> 00:14:15,740
address this shutdown
difficulty with the broom.

242
00:14:16,420 --> 00:14:18,730
He could change the score
function by adding a button.

243
00:14:18,760 --> 00:14:20,740
The button doesn't even really
need to be attached to the room.

244
00:14:20,741 --> 00:14:24,670
You can just have a button on his desk
and you can define a score function, uh,

245
00:14:24,910 --> 00:14:27,550
which says if the button's off,
you have the old score function.

246
00:14:27,551 --> 00:14:29,400
But if the button's on,
uh,

247
00:14:29,500 --> 00:14:34,420
you now get appointed few suspend and
zero points if you don't suspend. Now,

248
00:14:34,421 --> 00:14:38,950
this is assuming that, uh,
we have some way to write a,

249
00:14:39,220 --> 00:14:43,030
uh, an objective function and
that sort of detects suspension,

250
00:14:43,810 --> 00:14:46,870
which is very, uh, very
difficult. Uh, like for example,

251
00:14:46,871 --> 00:14:49,780
if there are loopholes in the
suspension detection mechanism,

252
00:14:50,050 --> 00:14:52,790
then the thing might try to
both technically suspend.

253
00:14:52,850 --> 00:14:57,050
We'll also filling the cauldron to get
both like just to, just to hedge his bets.

254
00:14:57,500 --> 00:14:59,240
Uh, but, but let's
assume way that problem,

255
00:14:59,241 --> 00:15:01,850
let's assume that we have sort of an
airtight suspension detection. Uh,

256
00:15:01,851 --> 00:15:05,390
you could try this utility function.
Now,

257
00:15:06,230 --> 00:15:10,790
does anyone have a guess as to whether
this works and if not work is wrong?

258
00:15:12,590 --> 00:15:16,610
Exactly. Yeah. So, uh, the, uh,

259
00:15:16,730 --> 00:15:21,090
it's filling the cauldron and requires
like getting water, pouring it, uh, uh,

260
00:15:21,140 --> 00:15:23,330
getting water in the buckets, carrying
those books along, way pouring it in.

261
00:15:23,331 --> 00:15:25,190
And that's like a, that's
like a difficult task, right?

262
00:15:25,280 --> 00:15:27,720
The spending is a pretty easy task.
Uh,

263
00:15:27,860 --> 00:15:30,860
so it can either get a point the hard
way or it can convince you that it's

264
00:15:30,861 --> 00:15:33,670
terrible and get a point the easy way.
Uh,

265
00:15:33,720 --> 00:15:36,440
so you have now built a broom that
attempts to scare the shit out of you,

266
00:15:38,120 --> 00:15:42,070
which might be fine. Hopefully,
hopefully it, it, it, uh,

267
00:15:42,830 --> 00:15:44,480
I mean you can't press the
button if you're dead, right?

268
00:15:44,480 --> 00:15:49,130
So it's probably just gonna, you know,
make scary noises. But, uh, this is,

269
00:15:49,131 --> 00:15:53,900
this is sort of indicative of the
problems, not a, it's not trivial,

270
00:15:54,200 --> 00:15:57,110
right? There's a, there's a
default incentive problem if you,

271
00:15:57,111 --> 00:15:58,550
if you set up a thing,
uh,

272
00:15:58,940 --> 00:16:03,790
to optimize objectives that you may
then want to change and, uh, uh,

273
00:16:03,820 --> 00:16:07,850
addressing it is not a trivial
problem. Uh, and this is, we're, we're,

274
00:16:07,851 --> 00:16:09,520
we're right now in this discussion,
surprisingly,

275
00:16:09,521 --> 00:16:12,750
we're pretty close to the state of the
art on this problem. There's maybe a,

276
00:16:12,870 --> 00:16:17,840
there's maybe three more inferential
steps. There's maybe three more, uh, uh,

277
00:16:17,900 --> 00:16:20,230
pieces of research that are
built on this problem. Uh,

278
00:16:20,480 --> 00:16:24,410
one of them I did a couple
of years ago, uh, with, uh,

279
00:16:25,130 --> 00:16:28,640
with, you'd Koski Fallen Stein
and Armstrong, and then, uh,

280
00:16:28,641 --> 00:16:33,520
another was a, was built on
that about a year ago, uh,

281
00:16:33,590 --> 00:16:38,510
which was the big red button paper, uh,
that you probably saw the hype about.

282
00:16:38,630 --> 00:16:42,530
It was actually very small result building
on this, but boy wasn't hyped. Uh,

283
00:16:42,531 --> 00:16:45,950
and that was, um, that was,

284
00:16:46,170 --> 00:16:48,710
you probably saw something about that
that was a collaboration between, uh,

285
00:16:48,711 --> 00:16:52,510
one of our associates who work
in the future of [inaudible]
and, uh, learn after,

286
00:16:52,520 --> 00:16:56,000
so his at deepmind here. Uh, and that's,
that's pushing this problem forward.

287
00:16:56,001 --> 00:17:00,860
But this problem is still more or less
unsolved. Uh, uh, will you, you, you can,

288
00:17:01,580 --> 00:17:02,510
the problem here is sort of,

289
00:17:02,511 --> 00:17:06,530
if you have two utility functions and you
want to combine them in such a way, uh,

290
00:17:06,860 --> 00:17:10,070
that, uh, you, you sort of have
three constraints you want,

291
00:17:10,071 --> 00:17:13,190
you want it to follow the first utility
function before the buttons pushed.

292
00:17:13,191 --> 00:17:14,540
And the second one after
the button's pushed,

293
00:17:14,810 --> 00:17:17,540
you want it to not have incentives to
cause or prevent you from pressing the

294
00:17:17,541 --> 00:17:22,430
button. Uh, Andy wanted to
still have incentives to
propagate that button forward.

295
00:17:22,870 --> 00:17:25,160
If it spins up sub
processes, uh, or if it,

296
00:17:25,190 --> 00:17:27,610
if it spins up subagents
or if it does anything, uh,

297
00:17:27,890 --> 00:17:32,390
that sort of extends beyond
the sort of original program.

298
00:17:32,391 --> 00:17:35,150
You sort of like don't want it to
delete the button as dead code.

299
00:17:35,510 --> 00:17:38,210
It turns out the getting all three of
those at once is still a pretty hard

300
00:17:38,211 --> 00:17:40,340
problem. There's a bit more than this,
but it's still still fairly open.

301
00:17:41,930 --> 00:17:45,170
So the moral of the story here is
that this is the sort of work, uh,

302
00:17:45,440 --> 00:17:49,040
that it sure would be nice if we knew how
to do this before we tried to estimate

303
00:17:49,380 --> 00:17:52,560
to fill cauldrons for us. Otherwise
we will end up like, Mickey,

304
00:17:52,561 --> 00:17:55,350
this is Mickey realizing that he should
have done this research a long time ago.

305
00:17:56,950 --> 00:17:59,690
Uh, so let me now step back. I'm,

306
00:17:59,691 --> 00:18:02,670
I'm doing this talk a bit backwards
cause I think in these sorts when we're

307
00:18:02,671 --> 00:18:03,450
talking about Ai,

308
00:18:03,450 --> 00:18:08,400
I think it's important to get concrete
examples of what the real problems are

309
00:18:08,580 --> 00:18:10,830
before talking with the big picture.

310
00:18:10,860 --> 00:18:13,140
Because when people try to talk
about the big picture, immediately,

311
00:18:13,141 --> 00:18:15,210
they sort of often get lost in,

312
00:18:15,211 --> 00:18:18,450
in philosophical discussions
and ethical discussions. Uh,

313
00:18:18,570 --> 00:18:21,920
so now that we've another, we sort of
grounded our discussion in some, uh,

314
00:18:21,990 --> 00:18:24,330
in some of them problems.
Uh,

315
00:18:24,360 --> 00:18:28,140
let's take a step back and talk about
what it takes to sort of align an AI

316
00:18:28,141 --> 00:18:33,141
system here is a dramatically simplified
pipeline of how you might try to get an

317
00:18:33,811 --> 00:18:37,310
aligned AI system. Uh, you
have some humans who, uh,

318
00:18:37,470 --> 00:18:39,370
who are thinking up the,

319
00:18:39,400 --> 00:18:42,210
the goals and values or the objectives
or whatever that they sort of want the

320
00:18:42,211 --> 00:18:43,440
system to have a,

321
00:18:43,441 --> 00:18:46,320
then they do some attempt to translate
that into an objective function.

322
00:18:47,040 --> 00:18:50,880
This may or may not succeed in Mickey's
case. Uh, it didn't quite succeed. Uh,

323
00:18:51,040 --> 00:18:55,860
then you have the system,
uh, uh, uh, attempt. Uh,

324
00:18:55,861 --> 00:19:00,600
so there's, there's, there's sort of
two subtly different, uh, parts here.

325
00:19:00,630 --> 00:19:05,040
One is you specifying formally, uh, what
you want. And the other is the system.

326
00:19:05,070 --> 00:19:07,620
Like getting that into the system.

327
00:19:08,010 --> 00:19:12,720
So many of the goals that we actually
want are significantly more complicated

328
00:19:12,750 --> 00:19:17,430
than fill a cauldron. Uh, and if you say
something like, well, I want cancer cured,

329
00:19:17,850 --> 00:19:21,990
uh, you might, uh, you might, uh,

330
00:19:22,020 --> 00:19:26,160
this might go wrong if a
cancer was in fact good.

331
00:19:26,280 --> 00:19:30,000
And like that's surprising you.
I don't need to worry too much about that,

332
00:19:30,430 --> 00:19:34,770
but it might also go wrong if you sort
of create a metric for measuring, uh,

333
00:19:34,771 --> 00:19:37,530
amount of cancer reduced where that
metric, if you actually optimize it,

334
00:19:37,531 --> 00:19:41,070
has bad consequences, uh, for example,
something killing cancer patients.

335
00:19:41,610 --> 00:19:46,350
Um, and then there's also the issue of,
uh, of the learning framework. This is,

336
00:19:46,380 --> 00:19:50,300
you might, you might have a, uh, uh, uh,

337
00:19:50,880 --> 00:19:54,630
like in the same way that
it's very difficult to write
a computer program that,

338
00:19:54,720 --> 00:19:58,220
uh, by hand that
recognizes a cat. Uh, it's,

339
00:19:58,221 --> 00:20:01,980
it's sort of much more difficult to
write a computer program that by hand

340
00:20:02,160 --> 00:20:04,130
recognizes a complicated goal.
Uh,

341
00:20:04,300 --> 00:20:07,620
you're almost surely going to going to
have machine learning systems that are,

342
00:20:07,880 --> 00:20:08,940
uh,
uh,

343
00:20:09,540 --> 00:20:12,300
attempting to learn the objectives that
you're giving them in some fashion.

344
00:20:12,450 --> 00:20:16,850
That framework also needs to
work. Otherwise, all of the, uh,

345
00:20:17,880 --> 00:20:19,290
all of the right objectives,

346
00:20:19,291 --> 00:20:22,800
function specification in the world
won't get that sort of into this system.

347
00:20:23,910 --> 00:20:26,130
And then the other components
that you sort of need working, uh,

348
00:20:26,131 --> 00:20:28,770
its predictions about what consequences
follow from what actions need to be

349
00:20:28,771 --> 00:20:30,600
pretty accurate. Uh, if it,

350
00:20:30,660 --> 00:20:34,200
if it is a predicting the
opposite of how get an action is,

351
00:20:34,201 --> 00:20:37,220
and it doesn't matter how
good its objectives are, uh,

352
00:20:37,470 --> 00:20:40,510
it's going to keep making things worse.
Uh, and then also needs to be able to,

353
00:20:40,630 --> 00:20:42,820
I mean, these, these two
at the bottom are more, uh,

354
00:20:42,960 --> 00:20:44,550
sort of classic capabilities research,

355
00:20:44,551 --> 00:20:48,750
but it also is still needs to work in
order to, uh, Devin AI that works, um,

356
00:20:49,510 --> 00:20:53,740
and does what you intend. So, uh, most,

357
00:20:53,950 --> 00:20:57,010
most media when it focuses on
these problem focus on one of,

358
00:20:57,310 --> 00:21:01,060
I sort of two of these problems,
uh, one of which is fictitious.

359
00:21:01,390 --> 00:21:05,770
They'll focus on the problem of what if
the wrong person is behind the AI. Uh,

360
00:21:06,040 --> 00:21:06,281
you know,

361
00:21:06,281 --> 00:21:09,100
what if the North Koreans go first or
something and they'll focus on this

362
00:21:09,101 --> 00:21:09,911
problem of like,

363
00:21:09,911 --> 00:21:14,260
what if the AI has natural desires that
cause it to not do the things that we

364
00:21:14,261 --> 00:21:18,460
programmed it to do? I think the
ladders largely fictitious, uh,

365
00:21:18,461 --> 00:21:23,130
the former, uh, I think, you know, the
former is going to be an issue, uh,

366
00:21:23,360 --> 00:21:28,150
uh, someday. But currently we are in a
situation where well intentioned people,

367
00:21:28,570 --> 00:21:30,100
uh,
could not,

368
00:21:30,101 --> 00:21:35,101
like if you handed me a box that was
an extraordinarily powerful function

369
00:21:35,321 --> 00:21:36,154
optimizer,

370
00:21:36,310 --> 00:21:40,210
I could put in a description of some
mathematical function and it would give me

371
00:21:40,211 --> 00:21:41,980
an output that made that function.
Uh,

372
00:21:42,280 --> 00:21:45,070
it would give me an input that made
that functions output really large.

373
00:21:45,520 --> 00:21:49,660
I would not know how to use that to
reliably have a very good impact on the

374
00:21:49,661 --> 00:21:50,830
world if this was,

375
00:21:50,860 --> 00:21:53,890
if this system was powerful enough to
build models of the world where it could,

376
00:21:54,450 --> 00:21:55,530
uh,

377
00:21:55,750 --> 00:22:00,750
figure out the effects of my beliefs
on it's a future actions and so on.

378
00:22:02,420 --> 00:22:02,670
Uh,

379
00:22:02,670 --> 00:22:06,250
so we're currently in a situation where
well intentioned people couldn't do good

380
00:22:06,251 --> 00:22:11,170
things if they tried. Uh, and I think
that when we're in this situation,

381
00:22:11,230 --> 00:22:15,040
sort of from an academic or a scientific
standpoint, you know, our first,

382
00:22:15,520 --> 00:22:17,830
our first objective should
be let's make sure that,

383
00:22:17,831 --> 00:22:20,260
well intention people could do good things
that they tried and then we can sort

384
00:22:20,261 --> 00:22:23,280
of worry about these more
political concerns. Uh,

385
00:22:23,710 --> 00:22:28,060
the politicians that I've spoken to are,
uh, at least the smarter ones are, uh,

386
00:22:28,270 --> 00:22:33,160
are pretty good at figuring out that
this natural desires thing is bunk. Uh,

387
00:22:33,520 --> 00:22:35,230
but they,
uh,

388
00:22:35,260 --> 00:22:37,960
they tend to think that this means they
should focus only on the other thing

389
00:22:37,961 --> 00:22:40,100
that the media is talking
about, which is, uh,

390
00:22:40,750 --> 00:22:43,650
whether or not good humans are
standing closest to the system. Uh,

391
00:22:43,750 --> 00:22:46,960
they also focus on things like,
is this going to cause mass unemployment?

392
00:22:46,961 --> 00:22:50,080
And so on. Which I again, I think are
important issues, but I think, uh,

393
00:22:50,350 --> 00:22:54,700
sort of Pale by comparison
to this possibility of a
industrial revolution style

394
00:22:54,701 --> 00:22:59,530
or larger, uh, change to the
way that society operates. Uh,

395
00:22:59,740 --> 00:23:03,680
so, uh, I think these, I think these
are useful things to think about that,

396
00:23:03,760 --> 00:23:07,800
but just they're not done other
things that, uh, that we do at myriad.

397
00:23:07,801 --> 00:23:10,840
There are other things that I think are
sort of most worth technical attention

398
00:23:11,140 --> 00:23:15,160
at this point. Uh, early science
fiction. You know, another,

399
00:23:15,161 --> 00:23:19,210
another thing that we often talk about
whenever we're talking about AI is Isaac

400
00:23:19,210 --> 00:23:22,090
Asimov's three laws of robotics.
Uh, people, people often ask,

401
00:23:22,091 --> 00:23:24,970
why don't we just do the field
last robotics? Well, okay, a,

402
00:23:25,000 --> 00:23:28,490
they're not formally specified,
right? Like in a, in Isaac Asimov,

403
00:23:28,520 --> 00:23:33,100
they're like built into the positronic
brains, whatever that means. A, B,

404
00:23:33,130 --> 00:23:36,580
if they were fully specified, you sort of
want, want them optimized, right? Like,

405
00:23:36,581 --> 00:23:39,130
don't throw an action. Let humans
come to harm. Like step one,

406
00:23:39,131 --> 00:23:42,230
put all the humans on heroin drips
and underground bunkers, right? Uh,

407
00:23:42,820 --> 00:23:45,950
and then see the books are about
how the laws don't work, right?

408
00:23:45,980 --> 00:23:48,650
Like they're like, hey,
here's these laws. Oh look,

409
00:23:48,651 --> 00:23:51,810
it leads to very interesting
failures, right? If they
worked at one p books, right?

410
00:23:53,240 --> 00:23:56,030
So, you know, I think these
make for interesting stories.

411
00:23:56,120 --> 00:23:59,930
I think they are maybe depressingly
more realistic than I'd like them to be.

412
00:23:59,931 --> 00:24:02,990
I think that often humans don't think for
five minutes about the consequences of

413
00:24:02,991 --> 00:24:05,850
what they're about to do.
But I don't think this is where,

414
00:24:05,880 --> 00:24:08,960
where the real issues lie. I think that
you can, if you do think for five minutes,

415
00:24:09,230 --> 00:24:12,320
realize someone's problems there.
I think that the,

416
00:24:12,321 --> 00:24:15,740
that the real places
to look if you want to,

417
00:24:15,741 --> 00:24:18,880
if you want to address some
of these issues are in, uh,

418
00:24:19,220 --> 00:24:21,650
making sure that the system
can learn the intended values.

419
00:24:22,430 --> 00:24:24,380
The better your value
learning framework is,

420
00:24:24,730 --> 00:24:27,470
the less you need to precisely
specify the right, uh,

421
00:24:27,471 --> 00:24:29,960
the right utility function or
the right objective function.

422
00:24:29,961 --> 00:24:33,570
And the more you can build a system
that sort of figures out, uh,

423
00:24:33,680 --> 00:24:35,950
like builds a good model of you,
figures out what your intentions were,

424
00:24:36,090 --> 00:24:39,660
trash and do those things. Uh, and then
I think it's also very important, uh,

425
00:24:39,661 --> 00:24:44,420
to get the optimization hooked up
correctly. Uh, two of the objectives.

426
00:24:44,450 --> 00:24:47,060
So that, so an instance of this,
this is a sort of abstract one,

427
00:24:47,660 --> 00:24:51,710
but an example of this really quick,
if you look at natural selection,

428
00:24:52,100 --> 00:24:55,790
natural selection is
the only known, uh, uh,

429
00:24:55,850 --> 00:24:59,240
engineering and in scare quotes process,
uh,

430
00:24:59,270 --> 00:25:02,870
that we know of that has ever led to
a generally intelligent artifact. Uh,

431
00:25:02,930 --> 00:25:06,500
it has led to quite a bunch of generally
intelligent artifacts, namely brains.

432
00:25:07,220 --> 00:25:10,490
And, uh, if you look at
natural selection, it's a,

433
00:25:10,491 --> 00:25:13,460
it's a sort of very stupid
hill climbing approach.

434
00:25:13,610 --> 00:25:16,880
So one thing we know from this is that
you can get to general intelligence with

435
00:25:16,881 --> 00:25:19,730
enough brute force and
he'll coming approach. Uh,

436
00:25:20,420 --> 00:25:23,750
and we also see in the case of natural
selection is natural selection.

437
00:25:23,751 --> 00:25:26,930
Optimized brains very,
very hard for genetic fitness.

438
00:25:27,800 --> 00:25:31,760
So the external pressure
on the brain, uh, was,

439
00:25:31,790 --> 00:25:34,580
was selecting brands for
being an agentic fitness.

440
00:25:35,030 --> 00:25:39,920
The internal objectives that humans
internally represent as their goals,

441
00:25:40,570 --> 00:25:44,880
uh, are not genetic fitness.
So we have, we have, uh,

442
00:25:45,020 --> 00:25:49,760
goals that were instrumental for survival,
sort of hardwired into our brains.

443
00:25:49,761 --> 00:25:54,590
We have like hunger instincts and we
have a survival instincts. And, uh,

444
00:25:54,591 --> 00:25:56,630
and then we explicitly
represent his goals.

445
00:25:56,631 --> 00:26:01,100
Things like love and justice and
beauty and mercy and you know,

446
00:26:01,220 --> 00:26:04,580
fun times and, and all
this, all this stuff that,

447
00:26:04,610 --> 00:26:09,200
that correlated with survival and
fitness in the ancestral Savannah.

448
00:26:09,890 --> 00:26:10,723
Uh,

449
00:26:10,880 --> 00:26:13,700
but it was things that correlated in the
past with what we were being selected

450
00:26:13,701 --> 00:26:18,420
for. Uh, that that sort
of became our, uh, uh,

451
00:26:18,560 --> 00:26:21,440
the things that we internally optimize
for. Right. So this was a case where the,

452
00:26:21,940 --> 00:26:25,900
uh, the external optimization
pressure on the artifact, uh,

453
00:26:26,150 --> 00:26:29,350
resulted in a generally intelligent
artifact with internal objectives that did

454
00:26:29,351 --> 00:26:33,830
not match to the external
selection pressure. Hopefully
that all made sense. Uh,

455
00:26:34,220 --> 00:26:38,810
similarly, if you are, uh,
uh, applying gradient descent,

456
00:26:39,010 --> 00:26:43,190
uh, to a black box trying to get it to
be very good at maximizing come objective

457
00:26:43,620 --> 00:26:47,130
eye. If you are, if you are
doing this blindly enough,

458
00:26:47,160 --> 00:26:49,740
you might get something very much
like what natural selection got,

459
00:26:49,741 --> 00:26:51,930
which was a generally intelligent
artifact that does pretty well,

460
00:26:51,931 --> 00:26:55,050
but it's actual goals are things that
correlated in the test environment with

461
00:26:55,051 --> 00:26:59,760
your objective and which will not
sort of continue to correlate with the

462
00:26:59,761 --> 00:27:04,180
objective when it undergoes a context
change, uh, or gets more resources or a,

463
00:27:04,230 --> 00:27:07,200
or a different setting. Uh,
so, so I think one of the,

464
00:27:07,230 --> 00:27:10,980
one of the real important things to do
in this work is make sure that whatever

465
00:27:10,981 --> 00:27:15,870
methods we're using for optimizing
whatever objectives are given,

466
00:27:15,871 --> 00:27:18,500
hopefully something like a, like
a value learning framework, uh,

467
00:27:18,530 --> 00:27:22,320
that the internal targets the system's
optimizing for are the same sorts of

468
00:27:22,321 --> 00:27:25,680
things that we are externally selecting
the system for using something like

469
00:27:25,681 --> 00:27:30,390
gradient descent. Abstract point.
I'll, uh, I'll move on now.

470
00:27:31,020 --> 00:27:31,641
Uh,
the,

471
00:27:31,641 --> 00:27:35,040
the take home message here is that we
expect that these things are going to be a

472
00:27:35,070 --> 00:27:39,150
technically difficult.
Uh,

473
00:27:39,180 --> 00:27:42,040
and if we, if we can't get
these things right, it's hard.

474
00:27:42,050 --> 00:27:44,610
It doesn't matter who's
standing closest, right? Uh,

475
00:27:44,720 --> 00:27:48,090
good intentions are not sneezed
on to computer programs, uh,

476
00:27:48,120 --> 00:27:50,100
by the intentions of the person,

477
00:27:50,130 --> 00:27:54,690
like a nice person standing nearby does
not make the program act better. Uh,

478
00:27:54,691 --> 00:27:56,130
if you want the program
back to certain way,

479
00:27:56,131 --> 00:28:00,540
you've got a program at tech that way.
So let's take one more step back.

480
00:28:00,720 --> 00:28:04,440
I sat in the last slide that we're
expecting these things to be somewhat

481
00:28:04,441 --> 00:28:04,891
difficult.

482
00:28:04,891 --> 00:28:08,280
So I'm now going to spend the remainder
of the talk saying why we expect these

483
00:28:08,281 --> 00:28:10,380
things to be somewhat difficult.
First,

484
00:28:10,560 --> 00:28:14,670
I'm going to give a four key propositions
that mean we should care and then I'm

485
00:28:14,671 --> 00:28:16,860
going to go in defending
that. Uh, it's all,

486
00:28:16,890 --> 00:28:18,060
it's not going to be a trivial problem.

487
00:28:19,290 --> 00:28:23,880
So the first proposition is what's
known as the orthogonality thesis. Uh,

488
00:28:23,881 --> 00:28:28,230
this was put forth by Nick
Bostrom. Uh, I think Steve,

489
00:28:28,231 --> 00:28:33,210
I'm a hunter Rowe also had had,
uh, uh, a lot of influence on this.

490
00:28:33,730 --> 00:28:36,180
Uh, more so in the next,
on the next point. But, uh,

491
00:28:36,210 --> 00:28:40,380
this basically says that you can in
theory build an AI system to pursue almost

492
00:28:40,381 --> 00:28:41,214
any goal.

493
00:28:41,220 --> 00:28:43,980
I think most programmers sort of
intuitively intuitively understand this.

494
00:28:43,981 --> 00:28:46,410
There's many people who are
non programmers who say, well,

495
00:28:46,411 --> 00:28:48,380
you couldn't program something
to just fill a cauldron.

496
00:28:48,381 --> 00:28:52,080
Cause when it gets smart enough it'll
decide to fill in the cauldron's dumb and

497
00:28:52,081 --> 00:28:56,570
go do something else. Right? You could
program something that, you know, uh,

498
00:28:56,640 --> 00:29:00,750
once it passes a certain
resource specialed, uh, checks
whether or not it thinks,

499
00:29:01,020 --> 00:29:03,840
uh, filling the cauldron is
dumb and then acts accordingly.

500
00:29:03,870 --> 00:29:06,960
But you could also program something
that just always executes the action that

501
00:29:06,961 --> 00:29:09,720
it predicts leads to the highest score,
where the score is the cauldron fullness.

502
00:29:10,230 --> 00:29:11,460
Uh,
so,

503
00:29:11,550 --> 00:29:15,270
so this proposition
says you can in theory,

504
00:29:15,300 --> 00:29:19,230
build an AI to pursue almost any
objective. The next proposition,

505
00:29:19,410 --> 00:29:24,240
instrumental convergence says that
most objectives imply subgoals such as

506
00:29:24,241 --> 00:29:27,240
survival, acquisition of
resources, and so on. By default,

507
00:29:27,600 --> 00:29:31,740
if you build something that's executing
actions to optimize some objective,

508
00:29:31,980 --> 00:29:36,180
it will, uh, have incentives to try
and prevent it from being itself,

509
00:29:36,181 --> 00:29:39,090
from being shut down because it's the
thing that's leading to the objective

510
00:29:39,091 --> 00:29:42,100
being fulfilled.
It by default has to get more resources.

511
00:29:42,101 --> 00:29:44,410
If those resources better
helping a few of the goal. Uh,

512
00:29:44,590 --> 00:29:49,590
the intuition here is that if you program
a robot to go buy you some milk and

513
00:29:50,321 --> 00:29:54,290
there are two paths, one of which is
dangerous and one of which is safe. Uh,

514
00:29:54,520 --> 00:29:55,960
and it's only objective,

515
00:29:55,990 --> 00:29:57,850
it's scored on whether or not
it gets back to you with milk.

516
00:29:58,240 --> 00:30:00,130
That's the only thing
you're scoring it on,

517
00:30:00,820 --> 00:30:02,380
then it will automatically
take the safe path.

518
00:30:03,100 --> 00:30:07,060
There's not because it has a Annamalai
and fear of the person on the dangerous

519
00:30:07,061 --> 00:30:10,440
path. There's not because
it feels like you do. Uh,

520
00:30:10,480 --> 00:30:14,500
the danger of a gun is cause you
can't get milk if you're dead. Right.

521
00:30:14,501 --> 00:30:17,560
So if it is capable of modeling that
one path is safer and the other path is

522
00:30:17,561 --> 00:30:22,450
more dangerous than, uh, by having
an attempt to optimize and objective,

523
00:30:22,750 --> 00:30:27,610
you sort of naturally get these
instrumental instrumental subgoals. Uh,

524
00:30:28,060 --> 00:30:31,290
these, these follow naturally is called
the instrumental convergence thesis. Uh,

525
00:30:31,330 --> 00:30:32,680
because,
uh,

526
00:30:32,710 --> 00:30:37,030
these are instrumental goals and almost
all objectives converge on these goals

527
00:30:37,031 --> 00:30:41,060
unless you do work to make them not
conversion of these goals. Third Point,

528
00:30:41,061 --> 00:30:42,040
capability gain,
uh,

529
00:30:42,100 --> 00:30:46,480
are potential ways for these systems
to gain greatly cognitive power of

530
00:30:46,481 --> 00:30:50,140
strategic options. Uh,
there's, there's a lot of the,

531
00:30:50,141 --> 00:30:53,070
there's a lot of ways that
this could be true. Uh,

532
00:30:53,170 --> 00:30:55,300
one of the reasons that I think there's
proposition is true is because highly

533
00:30:55,301 --> 00:30:57,070
destructive.
Uh,

534
00:30:57,100 --> 00:31:02,100
but an example of a way for an AI group
to have a rapid gaining capability.

535
00:31:03,431 --> 00:31:06,270
Their system is there some startup
that has very promising, uh,

536
00:31:06,280 --> 00:31:11,230
AI techniques and then Google buys them
and throws a boatload of a GPU is at

537
00:31:11,231 --> 00:31:16,180
them and invents tps for
them. Um, all hypothetical,

538
00:31:16,181 --> 00:31:20,470
I assure you, uh, uh,

539
00:31:20,471 --> 00:31:24,700
another way you could get rapid capability
gain. Uh, uh, so if the system is,

540
00:31:24,701 --> 00:31:24,851
you know,

541
00:31:24,851 --> 00:31:27,610
for the first time getting onto the
internet to where it hadn't been allowed

542
00:31:27,730 --> 00:31:31,630
massive Internet access before, uh,
other ways get rapid capability gain.

543
00:31:31,631 --> 00:31:34,660
If the system is a lot to do hardware
design and is able to find Harvard designs

544
00:31:34,661 --> 00:31:37,360
that are much better than
a human hardware designs,

545
00:31:37,570 --> 00:31:42,310
this is Jim's able to optimize
its own code in some way that, uh,

546
00:31:42,311 --> 00:31:46,060
if it, if it gets better at things like
computer programming than humans, then it,

547
00:31:46,120 --> 00:31:49,830
and you could get, you know, the
Apocryphal recursive feedback loop. Uh,

548
00:31:49,950 --> 00:31:51,800
so there's all sorts of ways that,
uh,

549
00:31:51,910 --> 00:31:55,930
we can sort of imagine a AI systems
getting dramatically more capable than

550
00:31:55,931 --> 00:31:56,621
humans.
I mean,

551
00:31:56,621 --> 00:31:59,080
it could also just be that humans aren't
very high up on the intelligence food

552
00:31:59,081 --> 00:32:04,000
chain and that early AI systems
are as naturally, uh, you know,

553
00:32:04,060 --> 00:32:07,120
in the same way that when humans started
beading or when computers started

554
00:32:07,121 --> 00:32:09,580
beating humans at chess.
What's the statistic?

555
00:32:09,581 --> 00:32:13,870
I think it's something like a 1996 was
the first time a computer beat human at

556
00:32:13,871 --> 00:32:18,250
chess and like 2002 or something was the
last time that a human being a computer

557
00:32:18,251 --> 00:32:23,020
at chess. Uh, I, I, there's probably
not quite right, but it's, it's,

558
00:32:23,021 --> 00:32:24,310
uh,
the window is fairly small,

559
00:32:24,311 --> 00:32:28,180
so you could just expect that a human's
aren't great and it's whole intelligence

560
00:32:28,181 --> 00:32:30,890
thing. Uh, this whole
like laziness thing and,

561
00:32:30,930 --> 00:32:35,800
and needing to eat three
times a day thing. Uh, brains
are very calorie intensive.

562
00:32:36,220 --> 00:32:36,970
Uh,

563
00:32:36,970 --> 00:32:39,440
it could be that the human brain is
optimized for very different sort of thing

564
00:32:39,441 --> 00:32:43,050
then what we would optimize brands for
in the same way that an airplane, uh,

565
00:32:43,310 --> 00:32:45,680
cannot heal his injuries
nor make baby airplanes.

566
00:32:45,681 --> 00:32:48,610
But it can fly a lot further
and faster than a bird. Uh,

567
00:32:48,770 --> 00:32:53,770
it might be the case that early AI
systems lack certain human capabilities,

568
00:32:54,351 --> 00:32:59,351
barriers dramatically beyond a human
capabilities in the axes that matter.

569
00:33:00,471 --> 00:33:02,480
And in fact, we already see
something kind of like this today.

570
00:33:03,730 --> 00:33:06,680
And then the fourth point is that
AI alignment is pretty difficult,

571
00:33:06,710 --> 00:33:10,210
which I'll argue in the remainder of
the talk. Uh, but before I go there,

572
00:33:10,240 --> 00:33:14,700
take a moment to, uh, to think
about these three propositions. Uh,

573
00:33:14,780 --> 00:33:16,740
if you believe these three propositions,
uh,

574
00:33:16,860 --> 00:33:21,350
the first one says it's possible to
build AI systems that are utterly

575
00:33:21,351 --> 00:33:26,120
indifferent to, uh, to human
objectives. The second one says that,

576
00:33:26,210 --> 00:33:27,750
uh,
uh,

577
00:33:27,890 --> 00:33:32,360
those systems by default will be in
competition with humans for resources.

578
00:33:32,840 --> 00:33:36,260
Uh, and the third one says that, uh,
they're gonna, they're gonna win.

579
00:33:38,750 --> 00:33:42,650
Uh, so if these three are, if
these two propositions are true,

580
00:33:43,310 --> 00:33:45,710
then this means that we had better
not screw it up. It doesn't mean that,

581
00:33:45,740 --> 00:33:47,750
you know,
it doesn't mean that we're screwed.

582
00:33:47,751 --> 00:33:50,720
It just means that we
could be screwed if we, uh,

583
00:33:50,750 --> 00:33:52,640
if we deal with the situation poorly,
right?

584
00:33:52,641 --> 00:33:54,500
So if you believed all
those three propositions,

585
00:33:54,770 --> 00:33:58,730
that means that this is really important
to get right. Uh, if we get it wrong,

586
00:33:58,731 --> 00:34:00,950
it could go very poorly. And of
course, it goes without saying,

587
00:34:00,951 --> 00:34:05,390
almost I'll say in any way that if we
get it right, I had, could go super well.

588
00:34:05,570 --> 00:34:09,890
Uh, the, the whole point that we're doing
AI research is because, uh, you know,

589
00:34:09,891 --> 00:34:13,250
the greatest drivers of improvements
in welfare have been technological and

590
00:34:13,251 --> 00:34:15,990
scientific. And if you
can automate those, uh,

591
00:34:16,070 --> 00:34:19,010
you can go a lot faster
than we going today. Uh, so,

592
00:34:19,011 --> 00:34:21,730
so there's great potential for upside.
Uh,

593
00:34:21,980 --> 00:34:25,220
and if you believe these three
propositions, then there's also, you know,

594
00:34:25,230 --> 00:34:27,970
there's a really important
thing to get right. Uh,

595
00:34:28,190 --> 00:34:31,100
if in fact this fourth point stands,
which are argue shortly,

596
00:34:31,130 --> 00:34:34,010
that means that we need to start
seriously focus focusing on these sorts of

597
00:34:34,011 --> 00:34:36,650
problems.
So without further ado,

598
00:34:37,010 --> 00:34:40,940
why do I think that element is
somewhat difficult? Uh, I mean,

599
00:34:40,970 --> 00:34:43,640
the real reason is that I worked on
the problems are a fair bit and uh,

600
00:34:43,670 --> 00:34:46,190
they seem difficult to me, but, uh,

601
00:34:46,220 --> 00:34:49,520
also draw some analogies
and also encourage you to
look at some of the problems

602
00:34:49,550 --> 00:34:53,720
and try them yourself. Uh, we could
use more people working on these. Uh,

603
00:34:53,750 --> 00:34:58,520
one of the reasons to expect that,
uh, uh, AI alignment is difficult,

604
00:34:58,580 --> 00:35:01,310
especially when you're looking at these
really advanced systems is for the same

605
00:35:01,311 --> 00:35:01,550
reason.

606
00:35:01,550 --> 00:35:05,330
You should sort of expect a priori that
rocket a rocket engineering is more

607
00:35:05,331 --> 00:35:09,110
difficult than airplane engineering.
Uh, when you're first designing rockets,

608
00:35:09,111 --> 00:35:13,100
you might argue, oh, well the rockets are
going to these basically just airplanes,

609
00:35:13,400 --> 00:35:14,750
right? Like, uh,

610
00:35:14,751 --> 00:35:17,750
it's all just like material science
and aerodynamics at the end of the day.

611
00:35:17,990 --> 00:35:20,510
Right? Like, what, like how
our rockets, how could they,

612
00:35:20,570 --> 00:35:23,450
how could they end up being like more
difficult to get right than airplanes?

613
00:35:23,870 --> 00:35:24,081
Well,

614
00:35:24,081 --> 00:35:26,330
the reason the Rockies are more difficult
to get right than airplanes is that

615
00:35:26,331 --> 00:35:29,630
they undergo a extreme stresses,
extreme context changes,

616
00:35:29,631 --> 00:35:34,130
and they're pack chock full of
explosives. Uh, I argue that, uh,

617
00:35:34,160 --> 00:35:36,800
that there's a very similar analogy to
when you're, when you're trying to build,

618
00:35:37,280 --> 00:35:38,940
uh, an AI system. There's a,

619
00:35:38,941 --> 00:35:42,000
there's a difference when you're trying
to build an AI system that's going to

620
00:35:42,420 --> 00:35:45,570
drive a car versus when you're trying
to build an AI system that's intended to

621
00:35:45,571 --> 00:35:49,830
go beyond the human regime in general
intelligence, whatever that means.

622
00:35:50,270 --> 00:35:54,810
Uh, and in the, in the first case,
you know, it's safety critical.

623
00:35:54,811 --> 00:35:56,550
It's very important. You've got to,
you've got to get the job right.

624
00:35:56,551 --> 00:36:01,350
In the second case, uh, you're going
somewhere sort of fundamentally new.

625
00:36:01,490 --> 00:36:04,710
You're, you're pushing beyond
the human region. Uh, you are,

626
00:36:04,740 --> 00:36:09,270
you are building something that
if it is, uh, a adversarial,

627
00:36:09,271 --> 00:36:14,250
you're going to have a
bad day. Uh, and you, uh,

628
00:36:14,280 --> 00:36:17,040
you're building something that, you know,
it's not going to start out brilliant,

629
00:36:17,130 --> 00:36:21,000
but it's going to be building models
of the world, uh, where you know,

630
00:36:21,001 --> 00:36:23,940
at some point it's going to realize that
you have beliefs and that your beliefs

631
00:36:24,120 --> 00:36:29,010
affect, uh, w its ability to achieve
its goals. And at that point,

632
00:36:29,370 --> 00:36:33,870
uh, it's outputs will be in part
chosen to affect your beliefs,

633
00:36:34,380 --> 00:36:39,150
right? So you are now, you know, working
with a system that if you built it wrong,

634
00:36:39,151 --> 00:36:43,500
has an active incentive to
deceive you. Uh, you're, uh,

635
00:36:44,220 --> 00:36:48,960
so, so you're sort of, uh,
potentially putting the system under,

636
00:36:49,260 --> 00:36:54,150
uh, uh, much more stress,
under much more ability to, to,

637
00:36:54,530 --> 00:36:56,210
uh,
uh,

638
00:36:56,510 --> 00:37:00,240
it's much more likely to undergo a big
context change where it dramatically, uh,

639
00:37:00,270 --> 00:37:01,440
models of the world in different way.

640
00:37:01,441 --> 00:37:05,950
Or if you give it a lot more resources
than it used to have and uh, uh,

641
00:37:06,150 --> 00:37:08,370
and it could go real wrong if you,
if you do it wrong,

642
00:37:08,580 --> 00:37:12,330
this is not too unlike rockets.
Uh, in the same way that a priori,

643
00:37:12,331 --> 00:37:15,750
you should go into rocket design
more cautious than airplane design.

644
00:37:16,170 --> 00:37:20,220
I recommend going into AI alignment much
more cautiously, uh, for the advanced,

645
00:37:20,250 --> 00:37:22,110
for the,
for the sort of long term AI alignment.

646
00:37:22,111 --> 00:37:24,900
I recommend going in much
more cautiously than in, uh,

647
00:37:24,930 --> 00:37:27,840
dealing with narrow systems where I think
many of these concerns I don't really

648
00:37:27,841 --> 00:37:29,940
apply. Second reason, uh,

649
00:37:30,000 --> 00:37:33,300
if you look at sort of
interesting engineering practices,

650
00:37:33,301 --> 00:37:37,830
some of the most entertaining interesting
engineering practices are at NASA

651
00:37:38,130 --> 00:37:41,010
where they do things like
take three independent teams,

652
00:37:41,011 --> 00:37:44,600
give each of the three teams a
different engineering spec or sorry,

653
00:37:44,620 --> 00:37:47,400
the same engineering spec and tell
him to design the same system.

654
00:37:47,730 --> 00:37:51,390
And then they actually
implement the system by taking
all three of those systems

655
00:37:51,570 --> 00:37:55,530
using a majority vote, uh, with the idea
that each of the systems will have bugs,

656
00:37:55,531 --> 00:37:59,880
but hopefully they won't. All three have
a bug in the same place. Uh, this is,

657
00:38:00,270 --> 00:38:03,530
uh, significantly more caution, uh,

658
00:38:03,600 --> 00:38:07,290
going into the deployment of things like
space probes and space shuttles then

659
00:38:07,291 --> 00:38:11,960
goes into the deployment of,
uh, say the new hangouts, uh,

660
00:38:14,310 --> 00:38:18,910
no offense, uh, and,

661
00:38:19,000 --> 00:38:19,833
and the,
you know,

662
00:38:19,920 --> 00:38:24,090
we can ask why is it that space probes
get so much more engineering attention?

663
00:38:24,310 --> 00:38:27,290
Uh, and I think there's a, there's
a bunch of reasons, but, uh, uh,

664
00:38:27,410 --> 00:38:31,220
I think some of the reasons are historical
and I think some of the reasons, uh,

665
00:38:31,250 --> 00:38:35,040
or maybe not quite what you'd expect at a
first glance, you might say, okay, well,

666
00:38:35,050 --> 00:38:37,840
space probes cost a real lot of money.
I think that's part of it.

667
00:38:37,841 --> 00:38:42,700
But I think that, uh, uh, if
you look at big tech companies,

668
00:38:42,870 --> 00:38:46,030
uh, many of their
deployments are also, uh,

669
00:38:46,300 --> 00:38:51,300
two systems that move much more money
than a space probe costs over the course

670
00:38:51,671 --> 00:38:56,390
of a year. Why is it that it's,
it's sort of much less, uh,

671
00:38:56,710 --> 00:39:01,000
uh, it takes much less caution to, uh,

672
00:39:01,060 --> 00:39:04,240
deploy a new feature on a web
app than a space probe? Well,

673
00:39:04,241 --> 00:39:09,190
one of the reasons I claim is that,
uh, the space probe goes to space.

674
00:39:10,650 --> 00:39:12,230
Uh, it's, it's sort of very, it's,

675
00:39:12,231 --> 00:39:16,570
it's much easier to be a cavalier about
your deployments when you can roll them

676
00:39:16,571 --> 00:39:19,000
back easily. Uh, with a space probe,

677
00:39:19,270 --> 00:39:24,270
you can correct bugs if the antenna
works and the receiver works,

678
00:39:25,290 --> 00:39:25,480
you know,

679
00:39:25,480 --> 00:39:28,150
whatever codes during the receiving and
whatever code's doing the decoding of

680
00:39:28,151 --> 00:39:31,570
the message you sent and whatever it
goes, applying the patch, right? Uh,

681
00:39:31,571 --> 00:39:34,720
but the code that runs the receiver
decodes the message and applies the patch.

682
00:39:34,750 --> 00:39:38,770
If that doesn't work, your
space probe is dead. It's gone.

683
00:39:38,890 --> 00:39:41,310
You can't go fix it.
Uh,

684
00:39:41,320 --> 00:39:45,340
in a very similar fashion with
highly advanced AI systems. Uh,

685
00:39:45,700 --> 00:39:48,970
if they start going wrong,
you'll be fine.

686
00:39:49,210 --> 00:39:53,740
If the code that, uh, uh,
make sure you go find it,

687
00:39:53,800 --> 00:39:56,920
make sure that the
system, you know, uh, uh,

688
00:39:57,310 --> 00:40:01,780
assist you in correcting it
works on the first try, right?

689
00:40:01,810 --> 00:40:05,680
But if you build a system that's
sufficiently capable that it could, uh,

690
00:40:05,800 --> 00:40:08,740
do bad things, then, uh,

691
00:40:10,840 --> 00:40:13,600
then the code,
like you'll only,

692
00:40:13,720 --> 00:40:18,370
you'll only be okay in this scenario a
if either the code that works entirely

693
00:40:18,371 --> 00:40:22,090
correctly on the first try or the, uh, uh,

694
00:40:22,120 --> 00:40:25,300
the code that helps you correct errors
works entirely correctly on the first try,

695
00:40:25,780 --> 00:40:27,640
right? So you need the whole system
to work well on the first try,

696
00:40:27,641 --> 00:40:30,880
but you need a component to work on
the first try and getting any code that

697
00:40:30,881 --> 00:40:35,290
works like exactly correctly
in the first deploy. Uh, this,

698
00:40:35,320 --> 00:40:39,000
if nothing I've said yet has sent
fear through your heart, this,

699
00:40:39,001 --> 00:40:42,220
this should I.
And then the third,

700
00:40:42,221 --> 00:40:46,060
the third reason to sort of expect that
this is a, uh, somewhat difficult task.

701
00:40:46,750 --> 00:40:50,170
It's difficult in part for
the same reasons that computer
security is difficult.

702
00:40:50,340 --> 00:40:54,610
Uh, now if you've done your job right, you
sort of, as I said before, you, you, you,

703
00:40:54,611 --> 00:40:58,840
you really, I really recommend against
a designing a computer program,

704
00:40:58,841 --> 00:41:02,210
the expected both to be smarter than
you and adversarial towards you. Uh,

705
00:41:02,510 --> 00:41:03,730
if you're writing that program,

706
00:41:04,060 --> 00:41:09,060
I suggest that you reconsider your
choices and write a different program.

707
00:41:09,790 --> 00:41:14,470
Uh, so ideally you sort of never want to
be in an adversarial situation against,

708
00:41:14,620 --> 00:41:16,700
against anything that you're writing.
Uh,

709
00:41:17,320 --> 00:41:22,000
but there is a sense in which the same
sorts of things that make computer

710
00:41:22,001 --> 00:41:25,300
security difficult make
a AI alignment difficult.

711
00:41:26,560 --> 00:41:29,440
Uh, and so what makes it
computer security so difficult?

712
00:41:29,560 --> 00:41:33,920
What makes computer security so
difficult is intelligent adversaries. Uh,

713
00:41:34,010 --> 00:41:38,420
so in computer security,
if you have a,

714
00:41:39,050 --> 00:41:43,730
you might have a dozen different
vulnerabilities in your code,

715
00:41:43,850 --> 00:41:48,200
none of which is itself fatal, none of
which is itself a easy to recognize.

716
00:41:48,500 --> 00:41:49,333
Uh,

717
00:41:49,670 --> 00:41:53,870
and you might have a very intelligent
attacker who can find all 12 and chain

718
00:41:53,871 --> 00:41:57,170
them together in a really weird and
confusing way to cause like an edge case

719
00:41:57,171 --> 00:42:01,220
scenario, which are less them a break
into your system or just make your system.

720
00:42:01,940 --> 00:42:06,230
Uh, even if you're not dealing
with adversarial systems,

721
00:42:06,290 --> 00:42:09,140
when you're dealing with a highly
advanced artificial intelligence,

722
00:42:09,340 --> 00:42:13,160
you are still dealing with
this, uh, intelligent search.

723
00:42:13,520 --> 00:42:17,720
The search space, you're much more
likely to end up in the edge cases. Uh,

724
00:42:17,721 --> 00:42:22,280
so as a example, there's a very
interesting paper by burden Lasalle,

725
00:42:22,281 --> 00:42:23,930
I think 2007,
uh,

726
00:42:23,970 --> 00:42:28,640
or they attempted to use a very
simple genetic algorithm to evolve a,

727
00:42:28,910 --> 00:42:33,740
uh, an oscillating circuit. And
they did and it worked. And, uh,

728
00:42:34,040 --> 00:42:36,780
they were astonished by the, uh, uh,

729
00:42:36,860 --> 00:42:40,250
very small size of the resulting program
and they were astonished also that it

730
00:42:40,251 --> 00:42:43,130
did not use the capacitor on the
chip. And there were like, well,

731
00:42:43,131 --> 00:42:46,670
how is it making an oscillating signal?
Uh,

732
00:42:46,671 --> 00:42:50,920
so they went in and spent a while
figuring out what this, uh, uh, uh,

733
00:42:50,990 --> 00:42:53,620
evolved program, uh, was doing. And it,

734
00:42:53,621 --> 00:42:57,620
what was doing as it happens was
repurposing the circuit tracks on the

735
00:42:57,621 --> 00:43:02,390
motherboard as a radio to replay the
oscillating signal from the test device

736
00:43:03,500 --> 00:43:07,940
back to the test device. Uh, this is,

737
00:43:07,970 --> 00:43:11,150
uh, and this was not a very smart
program, right? This is just, you know,

738
00:43:11,151 --> 00:43:15,950
use hill climbing on a, on a pretty
small solution space. And the, the, uh,

739
00:43:15,951 --> 00:43:19,610
the solution that it found was sort of
outside the space of solutions to the

740
00:43:19,620 --> 00:43:22,880
programmers were themselves visualizing,
right? There are visualizing things.

741
00:43:22,881 --> 00:43:25,220
They use the capacitor and,
and uh,

742
00:43:25,221 --> 00:43:29,210
and used a programming the way we would
think of programming when the thing

743
00:43:29,211 --> 00:43:30,350
actually used,
uh,

744
00:43:30,500 --> 00:43:33,980
features of the hardware properties of
the hardware that they wouldn't have

745
00:43:33,981 --> 00:43:36,350
modeled in say computer simulation
in a computer simulation.

746
00:43:36,351 --> 00:43:39,650
This thing might've done more
like what they intended. Uh,

747
00:43:39,710 --> 00:43:43,940
but the actual solution space in the real
world when they ran the code was wider

748
00:43:43,941 --> 00:43:48,290
than the one that they were visualizing.
It. Even this fairly stupid, uh,

749
00:43:48,291 --> 00:43:51,550
optimization algorithm found a solution
that was outside the space that are

750
00:43:51,551 --> 00:43:56,320
visualizing a, in the case of
an intelligence system that, uh,

751
00:43:56,900 --> 00:43:59,930
that's significantly smarter
than humans on certain axes,

752
00:43:59,931 --> 00:44:03,620
on whatever axes you're measuring a
significant better at humans say that

753
00:44:03,621 --> 00:44:06,080
predicting consequences of actions or
it finding actions that lead to certain

754
00:44:06,081 --> 00:44:07,580
outcomes.
Uh,

755
00:44:08,480 --> 00:44:11,930
you should by default expect it to be
pushing towards these sort of weird

756
00:44:11,931 --> 00:44:13,850
situations.
If there are weird hacks,

757
00:44:13,970 --> 00:44:18,610
if there are weird edge cases that let
it get a dramatically higher score, uh,

758
00:44:18,660 --> 00:44:20,840
you're just expected to
find those, right? So, uh,

759
00:44:21,020 --> 00:44:25,730
if it's playing Atari Games and it
eventually realizes that there's a, uh,

760
00:44:25,850 --> 00:44:28,850
a certain series of moves
that Mario can make, uh,

761
00:44:29,240 --> 00:44:34,240
that led it a rewrite the code of Mario
such that it can make it score really

762
00:44:35,641 --> 00:44:40,410
high. Uh, you should expect
performance of the system to sort of,

763
00:44:40,680 --> 00:44:41,010
uh,

764
00:44:41,010 --> 00:44:44,070
as measured by whether or not it plays
of the game super well to go up and then

765
00:44:44,071 --> 00:44:48,690
go down when it gains a, this insight and
the ability to actually rewrite Mario.

766
00:44:48,810 --> 00:44:50,160
Mario is in fact a Turing complete.

767
00:44:50,190 --> 00:44:55,110
There are in fact a series of jumps you
can do, uh, to inject your own code. Um,

768
00:44:56,190 --> 00:44:59,190
uh, I think someone wrote
flappy bird in Mario. It's

769
00:45:01,200 --> 00:45:05,790
really wish that person was doing
AI alignment work instead. But,

770
00:45:05,791 --> 00:45:06,990
uh,
uh,

771
00:45:07,020 --> 00:45:09,330
you should expect these sorts of things
you should expect when you're building

772
00:45:09,331 --> 00:45:11,340
something intelligent, you should
expect it to find the edge cases.

773
00:45:11,370 --> 00:45:15,030
You're sort of building something like
the whole purpose of intelligence is

774
00:45:15,031 --> 00:45:19,170
defined clever ways to solve problems and
we can find solutions that we couldn't

775
00:45:19,171 --> 00:45:23,640
ourselves pick out of the
solution space. Uh, and, uh,

776
00:45:23,670 --> 00:45:26,820
the only difference between
a solution that, uh,

777
00:45:27,060 --> 00:45:30,060
is very clever and you can plug into
the source solution space the humans can

778
00:45:30,061 --> 00:45:32,120
see versus a,

779
00:45:32,320 --> 00:45:36,420
a solution that is extremely perverse
and that we really didn't want a,

780
00:45:36,421 --> 00:45:39,450
that was obviously not what we
intended by the objective function. Uh,

781
00:45:39,540 --> 00:45:43,620
the only difference between these
is sort of our feelings, right?

782
00:45:43,680 --> 00:45:47,850
From the perspective of a system that is
trying to optimize score. Uh, you know,

783
00:45:47,851 --> 00:45:49,530
an action that leads to a
high score as an action,

784
00:45:49,531 --> 00:45:51,300
at least Ohi scores and
leads to a high score.

785
00:45:52,290 --> 00:45:54,540
Unless you get a good value learning
framework that is predicting how you would

786
00:45:54,541 --> 00:45:55,374
feel about it.

787
00:45:55,620 --> 00:45:57,930
And then you've got to make sure it's
not trying to affect how you feel about

788
00:45:58,020 --> 00:46:01,950
actions. It could make you really like
the easy actions. And then, uh, anyway,

789
00:46:02,640 --> 00:46:03,473
uh,

790
00:46:03,480 --> 00:46:08,310
so for the same reason that a
computer security gets way harder,

791
00:46:08,311 --> 00:46:11,200
when you have intelligent adversaries
trying to chain together, uh,

792
00:46:11,280 --> 00:46:15,120
lots of weaknesses, you
should expect, uh, uh,

793
00:46:15,810 --> 00:46:19,860
advanced AI systems to, uh, pick out,

794
00:46:20,460 --> 00:46:25,320
uh, to push the weird reasons to the
search space. And this means that, um,

795
00:46:26,270 --> 00:46:28,890
uh, it's not just the,
your, you know, sort of,

796
00:46:28,891 --> 00:46:31,410
it sort of much easier to make code that
works well on the path that you were

797
00:46:31,411 --> 00:46:31,941
visualizing.

798
00:46:31,941 --> 00:46:33,750
And then to make code that works
well on all the past that you weren't

799
00:46:33,751 --> 00:46:35,940
visualizing.
And with AI alignment,

800
00:46:36,390 --> 00:46:41,310
it's going to be using all the
paths you were visualizing. So,

801
00:46:41,340 --> 00:46:45,570
uh, uh, uh, uh, gut intuitive
reason to think of this role.

802
00:46:45,571 --> 00:46:48,360
It might be hard when you haven't
yourselves worked on the problems, uh,

803
00:46:48,361 --> 00:46:52,710
is treat it like a secure rocket probe.
Uh,

804
00:46:53,100 --> 00:46:57,750
so take the problem seriously. Don't
expect the problem to be easy, uh,

805
00:46:58,230 --> 00:47:03,090
and, uh, start thinking about it
sooner rather than later. And also,

806
00:47:03,091 --> 00:47:06,090
I think it was very important,
uh, in this, in this line of work,

807
00:47:06,091 --> 00:47:08,640
it's important to sort of
formalize your ideas. Uh,

808
00:47:09,540 --> 00:47:12,480
such that others can critique
them, uh, and build upon them.

809
00:47:12,540 --> 00:47:15,990
So in this example with the shutdown
button, uh, it's one thing to be like, ah,

810
00:47:15,991 --> 00:47:17,640
we can probably make it so that,
you know,

811
00:47:17,641 --> 00:47:20,340
if we have less that we can
make it shut down correctly. Uh,

812
00:47:20,341 --> 00:47:23,760
and once someone actually writes down
how they would combine the sort of normal

813
00:47:23,761 --> 00:47:27,210
operation utility function when the
shutdown operation until the function, uh,

814
00:47:27,510 --> 00:47:31,450
you can see that it will
either have incentives to
cause you to press the button,

815
00:47:31,570 --> 00:47:35,050
have incentives to defend the button
against you, uh, and so on and so forth.

816
00:47:35,050 --> 00:47:38,020
But if people aren't writing
down their actual ideas,

817
00:47:38,021 --> 00:47:40,330
if they sort of aren't treating them
like math problems or cs problems or

818
00:47:40,331 --> 00:47:41,164
technical problems,

819
00:47:41,850 --> 00:47:44,710
then you get lost in the philosophical
discussions and people arguing about what

820
00:47:44,711 --> 00:47:46,080
they could or couldn't do.
Uh,

821
00:47:46,300 --> 00:47:50,680
rather than actually building the thing
in such a way that if it's wrong, uh,

822
00:47:50,710 --> 00:47:52,990
the person who made it can be convinced
that it's wrong because it's actually

823
00:47:52,991 --> 00:47:55,990
math and you can check, uh, and such
that if it's right, we can build on it.

824
00:47:56,800 --> 00:48:00,320
So this is a, this is super important
and I encourage all of you, uh,

825
00:48:00,460 --> 00:48:04,850
if you're interested in these problems,
uh, to do some of this work. Uh,

826
00:48:04,940 --> 00:48:07,600
so that's all that I
have. I will say that, uh,

827
00:48:07,690 --> 00:48:11,530
there are ample resources
online for figuring out what
some of the open problems

828
00:48:11,531 --> 00:48:12,350
are.
Uh,

829
00:48:12,350 --> 00:48:16,090
there's actually a great paper that came
out of a Google brain called concrete

830
00:48:16,091 --> 00:48:20,280
problems in AI safety that has
I think eight open problems, uh,

831
00:48:20,350 --> 00:48:23,620
that are concrete as the title says
that you can try your hand at a,

832
00:48:23,700 --> 00:48:26,560
there were also, uh, some, uh,

833
00:48:26,590 --> 00:48:30,030
some other sets of problems you can find
on intelligence.org, which is where, uh,

834
00:48:30,340 --> 00:48:33,250
uh, machine intelligence research
institute host their website.

835
00:48:34,270 --> 00:48:38,560
And I encourage anyone who's interested
to look at these problems and help us

836
00:48:38,561 --> 00:48:41,380
make progress. Uh, because as I said,

837
00:48:41,410 --> 00:48:43,810
I think this is real important and I
think we shouldn't defer thinking about it

838
00:48:43,840 --> 00:48:45,430
until later.
That's all.

839
00:48:47,780 --> 00:48:50,260
[inaudible]

840
00:48:52,430 --> 00:48:55,400
thanks Nate. We're going to move
into the questions from the audience.

841
00:48:55,670 --> 00:48:59,570
An alternative to the tiering test that
was proposed by Ben [inaudible] at one

842
00:48:59,571 --> 00:49:02,570
point was send a robot to college.
If it comes back with a decree.

843
00:49:02,571 --> 00:49:07,130
It's intelligent and I liked this problem
because it illustrates a particular

844
00:49:07,131 --> 00:49:09,440
interesting case of utility functions.

845
00:49:09,830 --> 00:49:14,450
It's impractical to gather enough
training data to train that robot on the

846
00:49:14,451 --> 00:49:15,920
explicit utility function.

847
00:49:16,340 --> 00:49:20,720
It would take millions of instances
of sending it to college. Also,

848
00:49:20,750 --> 00:49:24,440
you can't simulate training
data because going to college,

849
00:49:24,441 --> 00:49:27,860
you're surrounded by other capable
students simulating those students would

850
00:49:27,861 --> 00:49:30,160
require solving the problem
you're training for.

851
00:49:30,650 --> 00:49:35,650
Has Miri done any investigations into
how to train systems where the ultimate

852
00:49:36,771 --> 00:49:41,450
utility function cannot be hill
climbed against for either practical or

853
00:49:41,451 --> 00:49:43,100
logically possible reasons?

854
00:49:43,890 --> 00:49:48,630
Uh, so there's a bunch of pizza. Did
that first I'll say, uh, you know,

855
00:49:48,631 --> 00:49:51,120
I think if the actual objective
function is get a degree from college,

856
00:49:51,121 --> 00:49:53,660
he'll do it in much faster than
four years and without, uh,

857
00:49:53,760 --> 00:49:55,740
sitting around other capable students,

858
00:49:56,070 --> 00:49:59,460
I think that I could get a
degree from a college, uh,

859
00:50:00,150 --> 00:50:03,450
faster than that. I guess it depends how
you're, what system's capabilities are.

860
00:50:03,840 --> 00:50:06,360
Uh,
uh,

861
00:50:06,480 --> 00:50:10,610
in terms of utility functions
that sort of don't have a,

862
00:50:10,620 --> 00:50:11,820
these perverse instantiation is,

863
00:50:11,821 --> 00:50:16,821
I would say that a defining one
of those is a large sheriff.

864
00:50:17,370 --> 00:50:20,190
The problem, uh, now I will say that, uh,

865
00:50:20,191 --> 00:50:23,130
I think many people think that
the problem is all about, uh,

866
00:50:23,160 --> 00:50:24,870
how did you get the objective
function just right.

867
00:50:24,900 --> 00:50:27,680
And I think that that's actually
much less of the problem. Uh,

868
00:50:27,681 --> 00:50:32,300
I think that building something
that models your intentions
and tries to extract,

869
00:50:32,870 --> 00:50:34,820
uh,
uh,

870
00:50:35,630 --> 00:50:40,160
what type of action it's going to do
by modeling your preferences somehow,

871
00:50:40,360 --> 00:50:43,240
uh, is a much more, uh,

872
00:50:43,340 --> 00:50:47,840
likely to work approach in the same way
that training a cla a cat classifier is

873
00:50:47,841 --> 00:50:51,350
a much more workable approach than trying
to write by hand the cat recognizing

874
00:50:51,351 --> 00:50:55,490
program. Uh, so I'm not, I'm
not very optimistic about the,

875
00:50:55,730 --> 00:50:59,300
let's try and write down utility
functions by hand, a approach.

876
00:50:59,301 --> 00:51:03,560
And I also think that the
problem is in fact more about,

877
00:51:03,800 --> 00:51:08,540
uh, getting the internal targets
of the system to match the,

878
00:51:08,650 --> 00:51:10,890
so it was externally selected for,
uh,

879
00:51:10,940 --> 00:51:15,770
which I need a better name for this
general class of problem. Uh, but, uh,

880
00:51:15,771 --> 00:51:19,520
I mentioned earlier, uh, humans, uh,

881
00:51:19,550 --> 00:51:24,530
humans not internally valuing
genetic fitness as their
top and only priority, uh,

882
00:51:24,560 --> 00:51:29,510
despite being selected only for
genetic fitness. Uh, uh, so,

883
00:51:29,511 --> 00:51:31,970
so I would say that getting utility
function right now, the whole problem,

884
00:51:32,000 --> 00:51:36,830
but maybe a fifth of the problem or
something. Um, in terms of that fifth,

885
00:51:37,370 --> 00:51:41,750
uh, I'm not super optimistic
about, uh, for example,

886
00:51:41,751 --> 00:51:45,810
the college, a utility
function. And I would say, uh,

887
00:51:45,860 --> 00:51:50,860
finding objectives that cannot
be a hill climbed against,

888
00:51:52,040 --> 00:51:54,800
you know, I haven't found, I haven't
found any good things in this direction.

889
00:51:54,801 --> 00:51:59,090
I would say that the thing
to look at is less, uh, uh,

890
00:51:59,480 --> 00:52:03,620
unhackable utility functions and more
something like, can we make it task based,

891
00:52:03,621 --> 00:52:08,120
limited, uh, uh, limited. It's, uh,

892
00:52:08,510 --> 00:52:11,270
yeah, there's, there's, some of these are
discussed in the concrete problems paper.

893
00:52:11,370 --> 00:52:12,440
Uh,
so I take a look there.

894
00:52:13,330 --> 00:52:17,530
What are your thoughts on using formal
verification methods in approaching this

895
00:52:17,531 --> 00:52:18,364
problem?

896
00:52:18,900 --> 00:52:19,830
Well,
verification,

897
00:52:19,860 --> 00:52:24,270
I sort of wouldn't bet on getting very
much in the way of formal verification of

898
00:52:24,271 --> 00:52:28,380
an advanced AI system.
Uh, uh, you know, if you,

899
00:52:28,381 --> 00:52:33,381
if you talk to a program Verification
Community folks today and you say,

900
00:52:34,771 --> 00:52:37,300
Hey, we're going to let the
code alter the code, uh,

901
00:52:37,560 --> 00:52:41,550
they will either shut her or yell at
you to get out of their office. Uh,

902
00:52:41,610 --> 00:52:45,810
more or less, that's not quite true.
Uh, we have, uh, so out of Mary,

903
00:52:45,811 --> 00:52:49,800
we do have some work
that, um, we have, uh,

904
00:52:49,830 --> 00:52:53,030
we have a paper showing how,
uh,

905
00:52:53,610 --> 00:52:56,220
you can write a secure colonel,
uh,

906
00:52:56,280 --> 00:53:01,130
that can replace any component of the
kernel, including the verifier component,

907
00:53:01,860 --> 00:53:06,660
uh, which is very hard to do,
um, for a go deleon reasons. A,

908
00:53:06,661 --> 00:53:09,730
you want the old verify or to
verify the new verifier, uh,

909
00:53:09,760 --> 00:53:11,520
and as easy to do if you
have loss of strength,

910
00:53:12,180 --> 00:53:14,310
but it's very hard to do if you don't
have a loss of strength because you'd run

911
00:53:14,311 --> 00:53:17,610
into, into a verifier
consistency, uh, issues. Uh,

912
00:53:17,611 --> 00:53:21,120
so we have a way to do
this with, with, uh,

913
00:53:21,300 --> 00:53:25,890
more or less no loss of strength,
which is very surprising. Um,

914
00:53:26,610 --> 00:53:31,440
that paper is called a hall and hall. It
uses the higher order logic, a theorem.

915
00:53:31,441 --> 00:53:34,470
Prover. Uh, so, so we have
done some thinking about this.

916
00:53:34,800 --> 00:53:38,310
Um,
uh,

917
00:53:38,660 --> 00:53:43,660
I would say that I think figuring out
how you would go about doing this sort of

918
00:53:43,791 --> 00:53:46,190
program verification can be helpful.
Uh,

919
00:53:46,220 --> 00:53:48,700
but it's mostly helpful
conceptually to understand,

920
00:53:49,190 --> 00:53:51,470
uh,
uh,

921
00:53:51,620 --> 00:53:54,130
like what sorts of things
you can and can't do. Uh,

922
00:53:54,160 --> 00:53:58,940
and I wouldn't expect early
AI systems to have properties.

923
00:53:58,941 --> 00:53:59,840
You can usually verify.

924
00:54:00,370 --> 00:54:05,370
I think best case you can get
statistical guarantees a something like,

925
00:54:05,510 --> 00:54:07,580
you know,
with probability one given enough time,

926
00:54:07,581 --> 00:54:11,720
this part of the network will converge
on the best policy or something.

927
00:54:12,260 --> 00:54:17,150
Uh, and I think that best case, you also
might be able to get something like, uh,

928
00:54:17,780 --> 00:54:19,910
you know, assuming that these
components work like this,

929
00:54:19,911 --> 00:54:24,470
then this component will work
fine. But by and large, uh, I'm,

930
00:54:24,530 --> 00:54:29,530
I'm pessimistic about ability to verify
large components were fully general AI.

931
00:54:30,710 --> 00:54:31,050
Okay.

932
00:54:31,050 --> 00:54:34,110
Oh, dude. Question from the
door. How the partnership on AI,

933
00:54:34,111 --> 00:54:36,990
recently founded by Google and other
big machine learning companies,

934
00:54:37,290 --> 00:54:40,880
has that has a set of tenants that emit
any mention of the existential risks

935
00:54:40,890 --> 00:54:44,850
that Asi, is this a case of divide
and conquer, Miri covers Ai,

936
00:54:44,940 --> 00:54:48,990
Asi risk partnership covers other issues
or a matter of disagreement about the

937
00:54:48,991 --> 00:54:49,824
risks?

938
00:54:50,570 --> 00:54:54,040
I suspect that it's none of the
above. I suspect that it's, uh,

939
00:54:54,100 --> 00:54:55,090
for political reasons.

940
00:54:55,520 --> 00:54:58,370
Um,
uh,

941
00:54:59,410 --> 00:55:01,120
I'd say I,

942
00:55:01,130 --> 00:55:05,290
I'd say please don't trust Mary to
handle the superintelligence risks all on

943
00:55:05,291 --> 00:55:08,290
their own. Uh, there is,

944
00:55:08,410 --> 00:55:12,830
I think half a dozen or so full
time researchers at the moment. Uh,

945
00:55:12,850 --> 00:55:16,690
those aren't, those
aren't very good odds. Um,

946
00:55:18,650 --> 00:55:23,400
I also, uh, you know,

947
00:55:23,410 --> 00:55:24,130
I don't wanna,

948
00:55:24,130 --> 00:55:28,180
I don't wanna say too much about this
partnership without talking to the people

949
00:55:28,181 --> 00:55:29,520
running it.
Um,

950
00:55:30,430 --> 00:55:35,170
but I'll say that I know they are
consulting with people in the AI alignment

951
00:55:35,171 --> 00:55:36,400
community.
Uh,

952
00:55:36,401 --> 00:55:40,450
and many of the groups
involved in it are quite aware,

953
00:55:40,510 --> 00:55:44,950
aware of the problems. Uh, so I suspect
that insofar as it's not in the charter,

954
00:55:44,951 --> 00:55:48,800
this is a largely political,
and I think also largely, uh,

955
00:55:48,880 --> 00:55:51,910
many of the things that this particular
partnership we'll be looking at are

956
00:55:51,911 --> 00:55:56,680
related to things like make the cars
not kill people, make sure that the, uh,

957
00:55:56,700 --> 00:56:01,490
that the displace jobs
don't, you know, destroy, uh,

958
00:56:01,600 --> 00:56:04,960
lives, uh, or that insofar
as they destroy lives,

959
00:56:04,961 --> 00:56:08,050
those slides are made better in
some other way. Uh, we do, I think,

960
00:56:08,110 --> 00:56:10,600
I think these are important issues,
but again,

961
00:56:10,630 --> 00:56:13,910
I think they sort of Pale
by comparison to these, uh,

962
00:56:14,260 --> 00:56:18,010
questions about the transformative
possibility of highly advanced AI systems.

963
00:56:18,890 --> 00:56:22,760
How does Miri plan to deal with the agr
research groups that keep large parts of

964
00:56:22,761 --> 00:56:24,130
their work secret such

965
00:56:24,130 --> 00:56:29,120
as deep mind and possibly other unknown
groups and say Russia or China? Um,

966
00:56:29,350 --> 00:56:31,450
so I don't know.

967
00:56:32,380 --> 00:56:36,850
I'm also not sure I like this
whole deal with terminology. Again,

968
00:56:37,360 --> 00:56:39,520
please don't leave the whole
problem to us. We're still,

969
00:56:39,760 --> 00:56:41,530
we're still only about
half a dozen researchers.

970
00:56:41,531 --> 00:56:46,380
We haven't grown in the last
two minutes. Um, I'll ask, uh,

971
00:56:47,980 --> 00:56:51,420
I would say, uh, that the,

972
00:56:51,450 --> 00:56:55,690
the type of work that we're trying to
do is sort of the type of work where,

973
00:56:55,930 --> 00:56:56,471
uh,

974
00:56:56,471 --> 00:57:00,880
at the end of the day you fundamentally
understand how to do AI lineman better.

975
00:57:01,000 --> 00:57:03,940
What do I mean by this? Uh, um,

976
00:57:04,840 --> 00:57:07,240
so let's say,

977
00:57:09,730 --> 00:57:14,320
let's say that you're trying to make a
program that plays chess well enough to

978
00:57:14,321 --> 00:57:18,760
be the human, uh, and you
know, the chess program, uh,

979
00:57:19,210 --> 00:57:22,660
uh, it makes some good moves and make some
bad moves you and you tried to like by

980
00:57:22,661 --> 00:57:26,020
hand to patch the bad moves and then
someone comes to you and they say, well,

981
00:57:26,021 --> 00:57:30,580
do you know how to play chess? If I had
to a sufficiently large computer, uh,

982
00:57:30,581 --> 00:57:33,400
you can name how large you
want the computer to be and
I'll give you a computer

983
00:57:33,401 --> 00:57:37,360
that size. Could you play chess in
that case? Uh, if the answer is no.

984
00:57:37,720 --> 00:57:41,740
So the answer today is clearly yes. You
run, you make the whole search tree,

985
00:57:41,770 --> 00:57:45,400
you backtrack. Uh, you know, you see
whether white has a winning move.

986
00:57:45,690 --> 00:57:49,160
You then repurpose the computer to get
all of the bitcoin is a good time. Um,

987
00:57:50,020 --> 00:57:54,190
but uh, uh, if the answer is no,
if you don't know how to do it,

988
00:57:54,191 --> 00:57:55,990
even if I give you an arbitrary
large computer, then you're,

989
00:57:56,160 --> 00:57:58,300
you're fundamentally confused about chess.
Somehow.

990
00:57:58,690 --> 00:58:01,180
You're either missing the search tree
data structure or you're missing the

991
00:58:01,181 --> 00:58:04,720
backtracking algorithm or you're missing
some understanding of how chess works.

992
00:58:05,120 --> 00:58:09,490
Uh, there's actually a fun,
a fun historical read. Uh,

993
00:58:09,491 --> 00:58:13,120
you can read a essay by Edgar Allen Poe,
uh,

994
00:58:13,450 --> 00:58:16,870
on the Mechanical Turk where I'd growl
and Poe that mechanical Turk was a

995
00:58:16,871 --> 00:58:18,970
purported chess playing the
automaton in the 18 hundreds.

996
00:58:19,400 --> 00:58:21,610
Edgar Allen Poe called bullshit.
He was like,

997
00:58:21,730 --> 00:58:24,670
the Mechanical Turk is not a test playing
automaton. I was a stage magician.

998
00:58:24,730 --> 00:58:27,790
I know where the Midget's hiding.
I can show you.

999
00:58:27,791 --> 00:58:32,080
But first let me argue
that it is fundamentally
impossible for an Automaton to

1000
00:58:32,081 --> 00:58:36,580
play chess. And then he gives an
argument that's remarkably sophisticated.

1001
00:58:36,610 --> 00:58:39,130
There is fundamentally impossible for
an Automaton to play chess. He says,

1002
00:58:39,131 --> 00:58:43,360
some people are talking about the machine
and Mr Babbage and they say it can do

1003
00:58:43,390 --> 00:58:47,740
arithmetical facts could in
theory do chess. And I argue no,

1004
00:58:47,860 --> 00:58:50,320
because arithmetical
questions are deterministic.

1005
00:58:50,740 --> 00:58:54,010
Each step folds from the previous,
but chess is not deterministic.

1006
00:58:54,370 --> 00:58:58,710
Your opponent's move, uh, it does
not follow from your move. So the,

1007
00:58:58,711 --> 00:59:02,710
the mechanistic wheels and gears cannot
represent your opponent's move. Uh,

1008
00:59:02,770 --> 00:59:06,610
so the Automaton, uh, can't play chess.

1009
00:59:07,720 --> 00:59:11,830
And uh, this is, this is
remarkably sophisticated. A,

1010
00:59:11,831 --> 00:59:16,750
he knew about computers. B, he identified
the heartbeat of chess, which is like,

1011
00:59:16,751 --> 00:59:19,780
Holy Shit. How do you deal with all
of the opponents possibilities, right?

1012
00:59:19,890 --> 00:59:24,380
It does indeed why chess is
right, but he, he, he, uh,

1013
00:59:24,410 --> 00:59:29,150
he went wrong. It's sort of like the
last step and you know, then in 1950,

1014
00:59:29,400 --> 00:59:30,740
a Claude Shannon is like,
okay,

1015
00:59:30,741 --> 00:59:32,660
so how are we gonna make this
computers play chess? Clearly,

1016
00:59:32,661 --> 00:59:35,330
if you gave me a computer as large as,
you know, much bigger than the universe,

1017
00:59:35,331 --> 00:59:38,540
I could make the tree do the backtracking.
We'd be playing chess.

1018
00:59:38,750 --> 00:59:40,640
But like in practice,
that's going to be difficult.

1019
00:59:40,641 --> 00:59:42,170
So we need the evaluation functions,
right?

1020
00:59:42,590 --> 00:59:46,340
Something changed between
Edgar Allan Poe and Shannon.

1021
00:59:46,760 --> 00:59:49,310
What change was whether or not we were
fundamentally confused about what sort of

1022
00:59:49,311 --> 00:59:51,260
thing trusses or whether or not we
were fundamentally confused about what

1023
00:59:51,261 --> 00:59:52,094
computers can do.

1024
00:59:52,690 --> 00:59:54,910
Uh,
uh,

1025
00:59:55,220 --> 00:59:58,820
right now we're in the state where no
matter how large a computer you had me,

1026
00:59:59,000 --> 01:00:03,890
I could not make a, uh, a
highly capable AI system that,

1027
01:00:03,990 --> 01:00:08,120
uh, does even a very simple task like
turn literally the entire universe in the

1028
01:00:08,121 --> 01:00:09,290
diamond,
right?

1029
01:00:09,320 --> 01:00:12,140
We don't need to worry about any of the
open ended gold problems cause we're

1030
01:00:12,141 --> 01:00:15,440
trying to optimize an opened ended goal.
We don't need to worry about these. Like,

1031
01:00:15,770 --> 01:00:19,160
uh, uh, how do you reason
given computing resources,

1032
01:00:19,161 --> 01:00:22,640
limitations problems because we
can just assume those way, right?

1033
01:00:22,760 --> 01:00:25,040
Even if you gave me all the brute
force in the world in a simple goal,

1034
01:00:25,070 --> 01:00:26,990
we still don't really know
how to align an AI system.

1035
01:00:28,260 --> 01:00:32,270
This is a roundabout way
of saying, uh, that, uh,

1036
01:00:33,380 --> 01:00:34,140
that

1037
01:00:34,140 --> 01:00:38,070
the type of work that we're trying to
do is the type of work that went in

1038
01:00:38,071 --> 01:00:40,050
between Edgar Allan
Poe and Claude Shannon.

1039
01:00:40,380 --> 01:00:42,010
I've sort of getting some of
the fundamental algorithms them,

1040
01:00:42,011 --> 01:00:45,270
some of the fundamental ways to
think about it such that, uh,

1041
01:00:45,300 --> 01:00:48,300
you sort of don't make very clever,
very good sounding,

1042
01:00:48,301 --> 01:00:51,150
very sophisticated arguments there turned
out to be wrong for some subtle reason.

1043
01:00:51,510 --> 01:00:53,850
Uh, you instead just know
how you would do it if you,

1044
01:00:53,851 --> 01:00:55,760
if the problem were simpler in some way.
Uh,

1045
01:00:55,770 --> 01:00:58,410
we're one way to simplify it is
to assume you have a simpler goal.

1046
01:00:58,411 --> 01:01:00,540
One way to simplify as assume you
have lots of computing resources.

1047
01:01:00,541 --> 01:01:05,260
There are many other simplifications you
could do. A, and I would say that, uh, uh,

1048
01:01:05,310 --> 01:01:09,400
getting back to the actual
question, I, uh, the,

1049
01:01:09,410 --> 01:01:11,790
the idea here is to get these insights,

1050
01:01:13,050 --> 01:01:14,530
get a

1051
01:01:15,740 --> 01:01:19,280
hi to the AI researchers to the point
where they would sort of automatically

1052
01:01:19,281 --> 01:01:23,150
know how to align a system, uh,
with the intended objectives. Uh,

1053
01:01:23,570 --> 01:01:27,170
if there weren't practical limitations
and then be figuring out how to do it if

1054
01:01:27,171 --> 01:01:29,690
there were practical limitations.
Cause there are a,

1055
01:01:29,691 --> 01:01:33,410
and I would say in the sense we don't,
we don't really,

1056
01:01:33,980 --> 01:01:37,310
we're not really trying to deal with,
uh, people keeping their stuff secret.

1057
01:01:37,311 --> 01:01:40,310
Where's trying to find these algorithms,
these tools,

1058
01:01:40,311 --> 01:01:45,140
these ways of thinking and uh,
get them out to people who will take them.

1059
01:01:45,530 --> 01:01:46,700
Uh,
on that vein,

1060
01:01:46,730 --> 01:01:50,720
I will say that we have a very good
relationships with many of the leading AI

1061
01:01:50,721 --> 01:01:54,830
teams. Uh, Google invited me
to do a talk recently, um,

1062
01:01:56,570 --> 01:02:00,620
but, uh, we have much less
good relationships with
a Russian Chinese teams.

1063
01:02:00,650 --> 01:02:05,530
I don't speak very much Russian
nor Mandarin. Um, but, uh, uh,

1064
01:02:05,540 --> 01:02:08,050
so, so good relationships
I think help, uh,

1065
01:02:08,120 --> 01:02:10,520
propagate these ideas when you get them.
But by and large,

1066
01:02:10,670 --> 01:02:14,270
we don't see it as our prerogative
to go around, you know,

1067
01:02:14,271 --> 01:02:17,990
poking our noses into the capabilities
work that other teams are doing, uh,

1068
01:02:17,991 --> 01:02:19,350
and shake a stick at them.
Something.

1069
01:02:20,360 --> 01:02:23,200
How do you expect the economics of
artificial intelligence development to

1070
01:02:23,201 --> 01:02:26,780
effects its safety? Would you prefer
one company to develop AI or many?

1071
01:02:27,530 --> 01:02:31,390
Um, so I think that, uh, you know,

1072
01:02:31,391 --> 01:02:35,600
I sometimes say about the AI, the Ai
alignment problem that, uh, in order to,

1073
01:02:35,601 --> 01:02:39,500
in order to handle AI
alignment well, uh, we need,

1074
01:02:40,160 --> 01:02:44,030
uh, so it's much harder to handle AI
alignment if you're in an arms race.

1075
01:02:44,150 --> 01:02:47,360
Clearly many things are harder in
an arms race and safety, keeping,

1076
01:02:47,361 --> 01:02:50,450
keeping safety concerns and not cutting
corners are especially difficult than an

1077
01:02:50,451 --> 01:02:55,430
arms race. Uh, so in order to deal well
with, uh, with general intelligence,

1078
01:02:55,690 --> 01:03:00,260
a humanity needs to, uh, uh, you know,

1079
01:03:00,620 --> 01:03:05,330
exercise restraint and caution in the
face of large economic incentives, uh,

1080
01:03:05,540 --> 01:03:10,190
while handling a gigantic
moral hazard, uh, uh,

1081
01:03:10,460 --> 01:03:14,570
of the form where you have a small group
of people who have the ability to, uh,

1082
01:03:14,571 --> 01:03:19,130
affect what happens in human society.
On a, on a massive scale, uh, seem to,

1083
01:03:19,131 --> 01:03:22,190
you need to resist economic incentives
while dealing with a gigantic moral

1084
01:03:22,191 --> 01:03:26,510
hazard, uh, while writing code
that works well on the first try.

1085
01:03:27,860 --> 01:03:32,390
Uh, and this is this, this does
not bode well for team human. Uh,

1086
01:03:32,420 --> 01:03:36,890
those, those three types of tasks are
not things that humans are unowned,

1087
01:03:37,310 --> 01:03:39,020
uh, for being great at. Right.

1088
01:03:39,021 --> 01:03:41,930
One of the reasons that I think it's
important to work hard on this now is just

1089
01:03:41,931 --> 01:03:46,010
for once I'd like to see humans sort
of go prepared into one of these,

1090
01:03:46,730 --> 01:03:50,620
one of these giant technological
shifts. We'll see. Uh,

1091
01:03:51,380 --> 01:03:56,000
so how old economics, uh,
affect the safety? Uh,

1092
01:03:56,210 --> 01:03:59,940
you know, I think, I think the
economics are definitely making AI.

1093
01:03:59,941 --> 01:04:02,270
I go faster these days. We
have talent, we have, uh,

1094
01:04:02,300 --> 01:04:04,610
money pouring into the field and that's
going to cause things to speed up.

1095
01:04:05,030 --> 01:04:08,960
And what I prefer one company
or many to develop AI. Um,

1096
01:04:10,580 --> 01:04:14,270
I think this is less a question of
preference and more a question of how the

1097
01:04:14,271 --> 01:04:17,990
world looks. A, there's
lots of debate in our, uh,

1098
01:04:18,590 --> 01:04:21,340
in our circles of, of people, uh,

1099
01:04:21,350 --> 01:04:25,490
doing this sort of research
about whether or not, uh,

1100
01:04:25,700 --> 01:04:29,310
the development of Ai looks more like,
um,

1101
01:04:30,530 --> 01:04:33,200
the development of a single software
project or the development of like an open

1102
01:04:33,201 --> 01:04:35,810
source community, right? Where, uh,

1103
01:04:35,840 --> 01:04:38,480
if AI is sort of like the
open source community,

1104
01:04:38,481 --> 01:04:41,900
if it's like a broad base of things that
was worked on by a lot of people, uh,

1105
01:04:41,960 --> 01:04:46,960
then it's sort of sort of weird to ask
what if one group creates it and write it

1106
01:04:47,191 --> 01:04:49,460
sort of this ecosystem that's growing.
Uh,

1107
01:04:49,490 --> 01:04:52,910
and it's sort of hard to
imagine one group, uh,

1108
01:04:53,390 --> 01:04:55,010
under control of the AI or something.

1109
01:04:55,610 --> 01:04:59,570
Whereas if AI is more
like a search engine, uh,

1110
01:04:59,750 --> 01:05:03,720
then even when you have lots of
different groups trying to make the, uh,

1111
01:05:03,740 --> 01:05:08,180
search engine, uh, you're gonna have,
uh, empirically in the past, uh,

1112
01:05:08,181 --> 01:05:13,181
you have sort of monopoly tendencies
where it's not that Google was the only

1113
01:05:13,730 --> 01:05:15,200
team working on search engines,

1114
01:05:16,370 --> 01:05:20,440
but Google sure captured the
search engine market, right?

1115
01:05:20,470 --> 01:05:24,790
Like there was, uh, uh, there was
Yahoo, there was that fish thing.

1116
01:05:25,000 --> 01:05:26,020
Who remembers what it is anymore?

1117
01:05:26,460 --> 01:05:30,900
Uh, uh, and yeah,

1118
01:05:32,130 --> 01:05:33,990
and,
um,

1119
01:05:35,390 --> 01:05:39,930
uh, sort of control of search engines
like development of search engines with

1120
01:05:39,931 --> 01:05:42,890
something where uh,
uh,

1121
01:05:43,280 --> 01:05:47,690
tools didn't easily transfer when
someone made Google a little better.

1122
01:05:47,691 --> 01:05:51,470
It took a while before he got
Yahoo, got a little better. Uh,

1123
01:05:51,530 --> 01:05:54,340
and uh,
and the,

1124
01:05:54,341 --> 01:05:57,800
the economics were pushing
the direction of monopoly.

1125
01:05:58,070 --> 01:06:01,760
And in that case it's sort of hard to
imagine the world ending up with an open

1126
01:06:01,761 --> 01:06:05,390
source Google, right. Uh, like there's
a lot of things Google's doing,

1127
01:06:05,391 --> 01:06:08,780
they would just be real hard for an
open source community to navigate. Um,

1128
01:06:09,620 --> 01:06:14,620
so I think the question of whether or not
we're going to see AI that's sort of a

1129
01:06:14,780 --> 01:06:18,950
broadly distributed or, uh, very, very
locally controlled is a question of like,

1130
01:06:18,951 --> 01:06:19,890
which type of softwares it,

1131
01:06:19,910 --> 01:06:24,380
is it more like this monolithic
type search engine type thing where,

1132
01:06:24,620 --> 01:06:24,971
where you're,

1133
01:06:24,971 --> 01:06:28,880
you're building it and it's
hard to transmit specific
components to other teams?

1134
01:06:28,910 --> 01:06:31,640
Or is it gonna be more like this open
source thing where it's a big ecosystem

1135
01:06:31,641 --> 01:06:34,190
and it's hard to even central
area if you wanted to, uh,

1136
01:06:34,220 --> 01:06:37,580
my bet as to which world we're in. Is
that just more like the centralized world,

1137
01:06:38,150 --> 01:06:42,980
uh, as to which world I'd like to
be in? Um, I dunno. I, that's not,

1138
01:06:43,040 --> 01:06:46,460
that's not really how I model worlds.
Alright.

1139
01:06:46,670 --> 01:06:49,190
I'll just deal with what's ever wanted.
It turns out we live in

1140
01:06:50,030 --> 01:06:52,600
what strategy and tactics
for AI risk mitigation.

1141
01:06:52,630 --> 01:06:55,420
Would you recommend the Googlers
who share your views on the risk?

1142
01:06:55,990 --> 01:06:57,430
If you are a random Google engineer,

1143
01:06:57,460 --> 01:07:01,840
would you focus entirely on donations to
Miri, try internal cross team advocacy,

1144
01:07:02,080 --> 01:07:03,820
try to join deep mind or something else

1145
01:07:03,890 --> 01:07:06,740
entirely? Um, you know,

1146
01:07:06,741 --> 01:07:10,610
one thing that I would do
if you're interested in this
work is I'd reach out to

1147
01:07:10,611 --> 01:07:13,240
Chris Olah. I hope he doesn't
mind me saying this, uh,

1148
01:07:13,430 --> 01:07:15,170
basic Google or he wrote the,
uh,

1149
01:07:15,230 --> 01:07:19,940
he was a coauthor on the concrete
problems paper. Uh, and uh, yeah,

1150
01:07:19,941 --> 01:07:22,820
if you're interested in it,
look them up, email them. Uh,

1151
01:07:23,540 --> 01:07:25,220
there's probably interesting
stuff you can do,

1152
01:07:25,221 --> 01:07:27,750
especially if you're already a brain,
um,

1153
01:07:28,490 --> 01:07:32,020
in terms of how one causes
more risk mitigation.

1154
01:07:32,600 --> 01:07:33,433
MMM.

1155
01:07:35,090 --> 01:07:39,890
You know, it's, it's hard to say,
sort of depends on the person. Uh,

1156
01:07:39,920 --> 01:07:41,510
I think,
you know,

1157
01:07:41,570 --> 01:07:46,570
20 percenting on a interesting AI safety
stuff is probably a pretty good time.

1158
01:07:47,960 --> 01:07:52,760
Um, and I'd say you're always welcome to
get in touch if you have more specific

1159
01:07:52,761 --> 01:07:56,000
questions that we had a
link up earlier, but, uh,

1160
01:07:56,001 --> 01:07:59,570
it's contact@intelligence.org
very hard to remember.

1161
01:08:00,550 --> 01:08:02,590
Uh,
and

1162
01:08:02,630 --> 01:08:05,450
yeah, I think it, I think it
depends on a situational basis,

1163
01:08:05,750 --> 01:08:08,750
but more eyes on the problem, more
people solving the problems, uh,

1164
01:08:09,200 --> 01:08:13,170
is one of the key things I'd encourage
when you were talking about, uh,

1165
01:08:13,220 --> 01:08:15,820
the problems of,
um,

1166
01:08:16,040 --> 01:08:21,040
the analogy with natural selection and
the contextual changes that creatures

1167
01:08:21,350 --> 01:08:25,940
might or might not go through in
natural selection being pretty small.

1168
01:08:26,780 --> 01:08:29,270
Um, it occurred to me that, uh,

1169
01:08:29,780 --> 01:08:31,990
humans are very different
from cave salamanders,

1170
01:08:32,010 --> 01:08:35,120
not just because they're far more
intelligent than cave salamanders,

1171
01:08:35,121 --> 01:08:39,860
but because they've been dealing with
far more contextual changes for the past

1172
01:08:39,980 --> 01:08:44,510
20,000 generations than cave salamanders.
Perhaps I've been sitting in a cave.

1173
01:08:45,170 --> 01:08:47,750
Um, so, uh, you know,

1174
01:08:47,751 --> 01:08:50,780
the changing social circumstances and
the changing weather and the changing

1175
01:08:50,781 --> 01:08:54,560
Savannah conditions and all the rest
of probably programmed into have forced

1176
01:08:54,561 --> 01:08:58,100
natural selection to program into us an
ability to deal with those conceptual

1177
01:08:58,101 --> 01:09:00,080
changes that the cave salamander,
it doesn't have.

1178
01:09:00,890 --> 01:09:05,120
Would it be useful to do something similar
in creating the genetic algorithms or

1179
01:09:05,121 --> 01:09:08,180
the hill climbing algorithms that would,
uh,

1180
01:09:08,210 --> 01:09:11,310
get us closer to AI,
uh,

1181
01:09:11,360 --> 01:09:13,520
by programming and this ability to this,

1182
01:09:13,521 --> 01:09:16,820
this requirement and the simulation that
there's a continuous changing in the

1183
01:09:16,821 --> 01:09:20,930
contextual elements necessary to
get to that objective function.

1184
01:09:21,510 --> 01:09:25,980
Uh, so, uh, I'll caveat first bat. Uh,

1185
01:09:26,070 --> 01:09:27,840
I'm,
uh,

1186
01:09:28,050 --> 01:09:31,080
I'd want to push in the direction of
sort of more transparent algorithms and

1187
01:09:31,081 --> 01:09:34,810
less, less brute force hill
climbing algorithms, uh, uh,

1188
01:09:34,890 --> 01:09:39,570
given the opportunity to do so. Uh,
but I do think it's very useful to,

1189
01:09:39,720 --> 01:09:44,700
uh, train the system under many
contexts changes. Uh, so, you know,

1190
01:09:44,701 --> 01:09:48,870
you shouldn't, I think it would be
foolish to test the system quite a bit,

1191
01:09:48,871 --> 01:09:50,820
and then for the first time,
uh,

1192
01:09:50,850 --> 01:09:54,270
that you give it a lot more resources than
I've ever had in test environment. Uh,

1193
01:09:54,630 --> 01:09:58,890
you put it like out in the world, uh,
to do whatever. Right? That would be,

1194
01:09:58,891 --> 01:10:01,770
there'll be sort of
reckless. Uh, so one of the,

1195
01:10:01,800 --> 01:10:04,110
one of the things that happens when it
moves from the test environment to the

1196
01:10:04,111 --> 01:10:08,220
real world is that the space
of the world's modeling
becomes quite a bit larger.

1197
01:10:08,460 --> 01:10:13,170
I, it goes from your lab to
the entire universe. Uh, so
this is a pretty big shift.

1198
01:10:13,510 --> 01:10:16,560
Uh, I mean, you might be able to
mitigate this in some ways. And then, uh,

1199
01:10:17,280 --> 01:10:21,910
in terms of available resources, if it
starts thinking about things like, uh,

1200
01:10:21,930 --> 01:10:23,280
acquiring computation resources,

1201
01:10:23,281 --> 01:10:25,410
which you might not want to
design a system that does that,

1202
01:10:25,411 --> 01:10:28,860
but it starts thinking about things like
acquiring computational resources. Uh,

1203
01:10:28,890 --> 01:10:33,300
then once it leaves the
lab, it sort of has, uh, uh,

1204
01:10:34,050 --> 01:10:37,950
far more resources at his disposal and
it did in the lab. Uh, potentially.

1205
01:10:38,310 --> 01:10:39,690
And if the,
if,

1206
01:10:39,720 --> 01:10:43,320
if the first time that you're testing
its ability to handle dramatic changes in

1207
01:10:43,321 --> 01:10:47,370
what the world looks like
and what resources it has
available is deployment, uh,

1208
01:10:47,371 --> 01:10:51,240
you're probably gonna
have a bad time. Uh, so I,

1209
01:10:51,660 --> 01:10:56,070
I am very pro, uh, testing context
changes as well as you can.

1210
01:10:56,160 --> 01:10:56,993
One of the things in the lab,

1211
01:10:57,210 --> 01:11:01,710
I will also say that I think
this is not alone sufficient. Um,

1212
01:11:02,250 --> 01:11:06,210
if you imagine a,
if you imagine a human on a planet,

1213
01:11:06,300 --> 01:11:08,720
a very slow aliens,
uh,

1214
01:11:08,730 --> 01:11:12,030
or the very slow aliens are cubs in the
human to like go through a series of

1215
01:11:12,031 --> 01:11:13,980
tests before they released
human into the world,

1216
01:11:14,340 --> 01:11:16,140
it's pretty plausible
that the human can tell

1217
01:11:17,730 --> 01:11:22,650
which one's the final test
environment. Maybe not, but maybe, uh,

1218
01:11:23,010 --> 01:11:27,220
and to some extent, uh,
you might want, you know,

1219
01:11:27,240 --> 01:11:29,910
if aliens like put up like
some canvases that they paint,

1220
01:11:29,911 --> 01:11:31,860
like backdrops on and that they think,
you can't tell,

1221
01:11:31,861 --> 01:11:34,740
they think you might think it's
the real world, right? Like, uh,

1222
01:11:35,010 --> 01:11:38,550
you've either got to be really, really
sure that you've tested this system, uh,

1223
01:11:38,610 --> 01:11:41,940
in context changes where it believes
this is the last context change, uh,

1224
01:11:42,000 --> 01:11:44,090
or you've got to have reasons beyond,
uh,

1225
01:11:44,200 --> 01:11:48,120
your trial and error and your
tests to expect that, uh,

1226
01:11:48,150 --> 01:11:51,540
after the final context change that
it will continue to act as intended.

1227
01:11:51,910 --> 01:11:53,190
And I'd sort of suggest both,

1228
01:11:53,191 --> 01:11:55,680
when you're dealing with things
that have the ability to transform a

1229
01:11:55,720 --> 01:12:00,300
civilization, uh, you should
not be trying to skimp on,

1230
01:12:00,660 --> 01:12:05,460
uh, on your safety precautions. So I would
say both test the hell out of it, uh,

1231
01:12:05,490 --> 01:12:08,580
especially under context changes and
have additional reasons to believe it's

1232
01:12:08,581 --> 01:12:12,350
going to work. Thank you all
for attending and thanks, Nate.

1233
01:12:12,530 --> 01:12:13,363
We're talking to Google.

1234
01:12:13,890 --> 01:12:18,890
[inaudible].

