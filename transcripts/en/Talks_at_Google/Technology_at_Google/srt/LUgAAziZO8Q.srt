1
00:00:06,540 --> 00:00:06,871
Today,

2
00:00:06,871 --> 00:00:11,610
I'd like to welcome Richard Harris to
come up and speak with us about his new

3
00:00:11,611 --> 00:00:13,160
book rigor Mortis.
Um,

4
00:00:13,230 --> 00:00:17,950
I've always had a kind of
passing curiosity as to, uh,

5
00:00:19,680 --> 00:00:23,880
why we're not getting, um, you know,

6
00:00:23,881 --> 00:00:27,330
it's the same kind of super
advancements in medicine that, um,

7
00:00:27,690 --> 00:00:31,590
kind of expected us to have
by this century. And, uh,

8
00:00:31,591 --> 00:00:34,950
it turns out Richard's
written an excellent book to
explain some of the reasons

9
00:00:34,951 --> 00:00:38,910
why and, uh, I'm excited to welcome
here to Google to talk to us today.

10
00:00:43,480 --> 00:00:45,470
I thank you all very much for coming in.
Uh,

11
00:00:45,830 --> 00:00:49,520
having lunch over a topic that's a
little bit uncomfortable maybe for some

12
00:00:49,521 --> 00:00:52,160
people, but maybe not.
The people in this crowd,

13
00:00:52,510 --> 00:00:55,370
a biologist are not always
happy to hear this news,

14
00:00:55,371 --> 00:00:59,150
although I think a lot of
them acknowledged that this
is a significant problem

15
00:00:59,151 --> 00:01:02,640
and they're in there worrying
about. So, uh, let me, uh, uh,

16
00:01:02,690 --> 00:01:05,950
spend a few minutes sort of
just giving you a sense of, of,

17
00:01:06,250 --> 00:01:09,770
of how the book came about and, and some
of the highlights of the book. And, um,

18
00:01:10,220 --> 00:01:14,870
this, I've been at NPR for coming up 31
years now and I've covered practically

19
00:01:14,871 --> 00:01:18,440
everything in science and in
medicine and related fields,

20
00:01:18,441 --> 00:01:23,420
environment I had over that time.
And in 2014 my boss said, hey,

21
00:01:23,690 --> 00:01:25,310
we want to switch things
around a little bit.

22
00:01:25,311 --> 00:01:28,940
Would you mind going back and covering
biomedicine for awhile again? And I said,

23
00:01:28,970 --> 00:01:30,350
oh, that's fine, I'm happy to do that.

24
00:01:30,650 --> 00:01:34,050
And I realized I was a little bit out of
touch with what had been going on since

25
00:01:34,051 --> 00:01:38,510
I last paid attention in the,
around the 2000 [inaudible] late 1990s.

26
00:01:38,690 --> 00:01:41,020
So the first thing I did was I
said, well, let's see how, let's,

27
00:01:41,030 --> 00:01:44,120
let's take a look at the broader
trends in biomedicine. And one thing,

28
00:01:44,121 --> 00:01:48,860
the first thing I discovered was
a money is a problem. Uh, this,

29
00:01:48,890 --> 00:01:53,090
the, the lower graph there shows
how much funding has decreased in,

30
00:01:53,120 --> 00:01:58,120
in inflation adjusted terms since the
year 2003 for NIH funded research.

31
00:02:00,120 --> 00:02:00,230
Uh,

32
00:02:00,230 --> 00:02:03,500
but this graph actually only tells half
the story because if you actually look

33
00:02:03,501 --> 00:02:07,730
back before 2003,
between 1998 and 2003,

34
00:02:07,731 --> 00:02:09,680
the NIH budget had doubled.

35
00:02:09,740 --> 00:02:13,610
And so there was a huge cash infusion
for the increase of vibratory space in

36
00:02:13,611 --> 00:02:17,390
this country, about 50% for this
kind of research in academia.

37
00:02:17,570 --> 00:02:20,270
Just enormous growth. And
then Congress said, okay,

38
00:02:20,271 --> 00:02:24,260
we've taken care of biomedicine,
we'll hold the dollars more or less flat.

39
00:02:24,500 --> 00:02:27,770
But in terms of, uh, terms
of spending power, uh,

40
00:02:27,970 --> 00:02:30,410
it was declining rather substantially.

41
00:02:30,411 --> 00:02:33,760
So a whole bunch of more mouths to feed
and less and less money to do it. So,

42
00:02:34,080 --> 00:02:37,280
so I thought that's going to have some
sort of negative consequence for sure.

43
00:02:38,360 --> 00:02:41,630
The second thing I fairly quickly came
across was this paper which was published

44
00:02:41,631 --> 00:02:45,710
in nature. Actually in 2012.
Uh, Glenn Begley and Lee Ellis,

45
00:02:46,200 --> 00:02:46,671
uh,

46
00:02:46,671 --> 00:02:50,480
wrote a paper that caught a lot of people
by surprise or certainly caught their

47
00:02:50,481 --> 00:02:53,990
attention.
And what they had done was a lowly Alice,

48
00:02:54,020 --> 00:02:58,820
I'm not leaving Ellis Glen Begley a
was head of cancer research for Amgen.

49
00:02:58,850 --> 00:03:02,850
Big Company out in, in
the La area. And uh,

50
00:03:02,920 --> 00:03:07,360
they rely very heavily on the output of
academic labs to get leads for new drugs.

51
00:03:07,360 --> 00:03:08,081
And over the years,

52
00:03:08,081 --> 00:03:11,440
lots of stuff had come across the
transom and most of it didn't pan out.

53
00:03:11,470 --> 00:03:15,850
And Begley was sort of saying, okay,
I'm kind of wrapping up my time here,

54
00:03:16,000 --> 00:03:19,540
but I want to go back one more time
and look at some of the most promising

55
00:03:19,541 --> 00:03:22,690
studies and take one more run at them
and see if I can get them to work.

56
00:03:22,691 --> 00:03:27,100
And so he selected 53 studies that he
thought if these studies were really

57
00:03:27,101 --> 00:03:30,250
panned out, these were, these would
be very strong leads for new drugs.

58
00:03:30,251 --> 00:03:32,980
So he tried to retest
them in his own labs. Uh,

59
00:03:33,250 --> 00:03:35,320
most often he couldn't get them to work.

60
00:03:35,420 --> 00:03:38,380
He even took them back to the
original academic labs to say,

61
00:03:39,040 --> 00:03:42,820
we couldn't get this to work,
can you? And, and very often the,

62
00:03:42,821 --> 00:03:44,800
the scientists there couldn't
get it to work either.

63
00:03:45,010 --> 00:03:49,240
So in the end of these 53 studies,
you only got six to work.

64
00:03:49,260 --> 00:03:51,250
It's about 11%. Right? Very,

65
00:03:51,251 --> 00:03:55,540
very poor success rate of just replicating
a study that had been published in

66
00:03:55,541 --> 00:03:57,700
the literature that people hadn't
paid a lot of attention to.

67
00:03:57,850 --> 00:04:01,840
One of these papers had 2000 citations
people saying, look at this great result.

68
00:04:01,841 --> 00:04:03,340
And he couldn't replicate it.

69
00:04:03,670 --> 00:04:08,670
It was a similar study done the
previous year by scientists at bear in,

70
00:04:08,950 --> 00:04:12,040
in Germany. And they also came up
with a pretty low success rate,

71
00:04:12,041 --> 00:04:14,740
about 25% of the studies that
they were able to replicate.

72
00:04:14,980 --> 00:04:19,980
So this led to this idea of
the reproducibility crisis,

73
00:04:20,070 --> 00:04:25,000
uh, that was just bubbling up as a
result of these papers and similar work.

74
00:04:25,001 --> 00:04:29,390
So I realized I've got a book in this
and, uh, uh, and I decided to, um,

75
00:04:30,310 --> 00:04:31,240
uh,
to take some,

76
00:04:31,320 --> 00:04:35,680
take a year off of NPR and dig into these
issues to really try to understand the

77
00:04:35,681 --> 00:04:37,750
consequences of this. And, uh,

78
00:04:38,020 --> 00:04:41,440
one of the first things I was worried
about was we'll anyone want to talk to me

79
00:04:41,441 --> 00:04:43,600
about this, uh, in the
answer, surprisingly,

80
00:04:43,601 --> 00:04:47,350
was people were completely happy to talk
to me about this. And, uh, for example,

81
00:04:47,351 --> 00:04:48,850
this is a,
uh,

82
00:04:48,880 --> 00:04:52,810
Janet Woodcock who's the number one
person at the FDA in charge of drug

83
00:04:52,840 --> 00:04:56,320
approvals. And she said, I'm happy to
talk to you. This is a big problem.

84
00:04:56,560 --> 00:04:59,800
And I sat down with her and we had a
long conversation and she said, you know,

85
00:04:59,830 --> 00:05:03,910
the stuff we get from pharmaceutical
companies is reasonably good.

86
00:05:03,911 --> 00:05:05,530
We can rely on a fairly heavily,

87
00:05:05,531 --> 00:05:10,480
but if academics come directly to
the FDA with studies that it's,

88
00:05:10,570 --> 00:05:13,300
the, the quality of those
studies is very, very poor.

89
00:05:13,330 --> 00:05:15,730
And the quote that she has is, uh, uh,

90
00:05:15,760 --> 00:05:18,940
she says it's like nine out of 10
airplanes we designed fell out of the sky,

91
00:05:18,941 --> 00:05:21,820
or nine out of 10 bridges we
build failed to stand up. I mean,

92
00:05:21,821 --> 00:05:25,450
this is the failure rate she's talking
about for the academic studies trying to

93
00:05:25,780 --> 00:05:29,650
lead to drug development in a fairly
advanced area because they're already at

94
00:05:29,651 --> 00:05:31,960
the FDA at that point.
[inaudible] you know, she,

95
00:05:32,170 --> 00:05:35,590
she sort of laughed and disbelief
and said, we need rigorous science.

96
00:05:35,591 --> 00:05:40,230
We can rely on another person
who was quite surprisingly, uh,

97
00:05:40,480 --> 00:05:43,660
engaged in this topic. Oh,
should is Francis Collins,

98
00:05:43,661 --> 00:05:47,230
who's the director of the National
Institutes of health. And to his credit,

99
00:05:47,231 --> 00:05:50,980
he recognized this as a problem
early on and said, you know,

100
00:05:50,981 --> 00:05:53,500
we can't sweep this under the rug.
It's clearly,

101
00:05:53,501 --> 00:05:56,290
it's an embarrassment to the
enterprise that he's funding.

102
00:05:56,291 --> 00:06:00,560
But his take was we got to fix
this and we can and we will,

103
00:06:00,561 --> 00:06:03,920
we'll try to engage on these
issues as opposed to saying,
oh, nothing to see here,

104
00:06:03,921 --> 00:06:08,360
move along. Uh, and he
engaged his top deputy, uh,

105
00:06:08,390 --> 00:06:13,010
Larry Tabak to be the point
person for this issue. They both,

106
00:06:13,070 --> 00:06:16,840
they worked on it together a lot and,
and among other things they wrote in,

107
00:06:16,930 --> 00:06:21,560
in one of the, one of the journals,
uh, uh, Collins explained. Yeah,

108
00:06:21,561 --> 00:06:23,060
of course science is self correcting.

109
00:06:23,061 --> 00:06:26,600
That's the beauty of the scientific
method is that things that you don't get

110
00:06:26,601 --> 00:06:28,700
everything right the first time.
It shouldn't be expected.

111
00:06:28,940 --> 00:06:32,330
But there's a process that
eventually things are, you know,

112
00:06:32,540 --> 00:06:34,580
mistakes are discovered.
You know,

113
00:06:34,760 --> 00:06:38,600
blind blind alleys are identified
and the process corrects itself.

114
00:06:38,601 --> 00:06:41,120
And so he says in the long run,
science of self correcting, but,

115
00:06:41,121 --> 00:06:43,280
and here's the quote in the shorter term,

116
00:06:43,281 --> 00:06:47,210
the checks and balances that once ensured
scientific fidelity have been hobbled.

117
00:06:47,660 --> 00:06:49,680
Pretty strong words for the,
uh,

118
00:06:49,730 --> 00:06:53,000
to come out of the mouth of the head of
the National Institutes of health. And,

119
00:06:53,001 --> 00:06:57,020
uh, but you know, he is engaged on this
topic and he's taking it very seriously.

120
00:06:57,170 --> 00:07:00,770
And I think that is that that probably
helped me talk to a bunch of other people

121
00:07:00,771 --> 00:07:03,690
who were also very concerned
about it. And, uh, and,

122
00:07:03,730 --> 00:07:07,040
and I think that's a good sign that
people are engaged in that way.

123
00:07:07,160 --> 00:07:10,970
So what difference does this make? This
is a, uh, you'll appreciate this graph.

124
00:07:10,971 --> 00:07:14,830
It's called e rooms law of the
inverse of Moore's law, uh,

125
00:07:14,900 --> 00:07:19,090
because this is the shows that drug
development, unlike microchips, uh,

126
00:07:19,340 --> 00:07:22,490
costs more and more to develop and you
get less and less bang for the buck as

127
00:07:22,491 --> 00:07:26,190
time goes by. You can see in the 1950s
at the beginning of that graph, uh,

128
00:07:26,510 --> 00:07:31,280
it was relatively inexpensive. That's
a log scale, uh, to develop, uh, drugs.

129
00:07:31,281 --> 00:07:35,900
But you know, by 2010 things where, you
know, costing a huge amount of money for,

130
00:07:35,901 --> 00:07:38,120
to yield a single drug.
And as we know,

131
00:07:38,121 --> 00:07:40,820
a lot of those drugs are
fairly marginally effective.

132
00:07:40,820 --> 00:07:45,300
So even if they pass muster, a lot
of these anticancer drugs are, uh,

133
00:07:45,590 --> 00:07:48,380
so modest and what they achieve a,

134
00:07:48,470 --> 00:07:51,500
that if you look at the national
statistics for cancer mortality,

135
00:07:51,680 --> 00:07:52,371
you actually don't,

136
00:07:52,371 --> 00:07:55,580
you can't even see the effect of
all of these new cancer drugs on,

137
00:07:55,820 --> 00:07:59,480
on cancer mortality trends in this
country as yet. You know, maybe,

138
00:08:00,260 --> 00:08:05,120
maybe we will eventually, but
at this point for all the,
all the great talk about,

139
00:08:05,170 --> 00:08:08,510
uh, how much all this cancer
research is panning out.

140
00:08:08,511 --> 00:08:12,080
The reality is it's costing a lot of
money and it's not yielding a whole lot of

141
00:08:12,081 --> 00:08:14,300
results.
And that's a serious problem.

142
00:08:14,540 --> 00:08:17,710
This is not necessarily driven
by reproducibility, but the,

143
00:08:17,711 --> 00:08:22,250
but it underlines the fact that we really
need to make sure that we are being as

144
00:08:22,251 --> 00:08:24,200
effective as we can in,

145
00:08:24,320 --> 00:08:28,370
in working on these underlying
science projects to make sure that,

146
00:08:28,580 --> 00:08:33,260
that actually to inflect this curve so
it doesn't get worse and worse and worse.

147
00:08:33,590 --> 00:08:37,160
So how bad is it? Well,
what nature nature asked,

148
00:08:37,161 --> 00:08:42,161
is there a reproducibility crisis and
a f and in a survey last year and a 52%

149
00:08:43,101 --> 00:08:43,880
said yes,

150
00:08:43,880 --> 00:08:47,750
there is a significant reproducibility
crisis and other 38% yet said there's a

151
00:08:47,751 --> 00:08:52,120
yes, there's a slight crisis. Not exactly
sure what a slight crisis is, but,

152
00:08:52,260 --> 00:08:55,650
but we can't say it's only 10%
said, I don't know. I don't care.

153
00:08:55,651 --> 00:08:57,300
And I don't think that
this is a real issue. So,

154
00:09:00,900 --> 00:09:04,590
so clearly scientists recognize this is,
this is a real,

155
00:09:04,970 --> 00:09:08,700
as very significant issue and something
that they need to deal with and they're

156
00:09:08,701 --> 00:09:13,000
thinking about it. Uh, uh, this, I,

157
00:09:13,950 --> 00:09:18,870
this plays out in news coverage as well.
This is a paper that was in, uh, uh,

158
00:09:18,930 --> 00:09:22,800
one of the plos journals, uh, published
just a couple of months ago after,

159
00:09:22,801 --> 00:09:24,420
after my book went to press.

160
00:09:24,630 --> 00:09:27,330
This is a team of scientists at the
University of Bordeaux that asked the

161
00:09:27,331 --> 00:09:31,410
question, uh, how, how much
can we rely, for example, on,

162
00:09:31,680 --> 00:09:34,950
on newspaper reports of,
of scientific discoveries.

163
00:09:34,950 --> 00:09:39,750
And these did a survey of a database of
almost 200 English language newspapers.

164
00:09:39,751 --> 00:09:41,910
And they said,
how well do these uh,

165
00:09:42,060 --> 00:09:46,560
newspapers cover these results and how
accurate does it turn out to be in the

166
00:09:46,561 --> 00:09:51,320
long run? And they identified
156 studies that, uh,

167
00:09:51,540 --> 00:09:55,350
that had been reported on sometime in the
past. And they went back and they said,

168
00:09:55,351 --> 00:09:59,040
okay, uh, did these studies can out?
And there are things like, you know,

169
00:09:59,160 --> 00:10:04,160
linking pesticides to Parkinson's disease
or suggesting a mechanism for Adhd or

170
00:10:05,820 --> 00:10:06,660
looking at,
you know,

171
00:10:06,780 --> 00:10:11,700
potential new genes or putative new genes
for breast cancer and various other,

172
00:10:11,730 --> 00:10:14,730
you know, things that you see. We opened
up the paper new gene for breast cancer.

173
00:10:14,910 --> 00:10:15,061
Well,

174
00:10:15,061 --> 00:10:19,240
it turns out that if that's 156 studies
that had actually had enough followup

175
00:10:19,290 --> 00:10:24,090
that there was a, a reasonable amount of
data till sort of be able to look back.

176
00:10:24,090 --> 00:10:27,610
About half of them were, uh, uh,

177
00:10:27,690 --> 00:10:31,310
stood the test of time and the other
half turned out to be ephemeral. They,

178
00:10:31,500 --> 00:10:34,230
the bubble burst and the idea went away.
Uh,

179
00:10:34,231 --> 00:10:37,890
and if you look at these sort of those
for the first time kinds of studies where

180
00:10:37,891 --> 00:10:41,310
people are just looking at the exciting
new first time anyone has seen this,

181
00:10:41,490 --> 00:10:45,690
only about a third of those who are
confirmed in these further studies.

182
00:10:45,691 --> 00:10:49,230
So I started thinking, I the
bottom of my stories, I should,

183
00:10:49,260 --> 00:10:51,990
if I'm doing stories of this
nature, I should say, by the way,

184
00:10:51,991 --> 00:10:55,170
about half the time to two thirds of the
time these studies turn out not to be

185
00:10:55,171 --> 00:10:59,940
true. So are you feeling lucky
today as a sort of a warning to, to,

186
00:11:00,000 --> 00:11:04,410
to readers that these are not necessarily
reliable, uh, findings and it's,

187
00:11:04,411 --> 00:11:09,330
it's pretty, it's pretty sobering. And
journalists also tend not to report the,

188
00:11:09,331 --> 00:11:11,730
Oh, by the way, that study who
told you about three years ago,

189
00:11:11,731 --> 00:11:14,910
it turned out not to be
true. Partly because, uh,

190
00:11:15,120 --> 00:11:20,070
those sort of follow on studies tend to
be published in journals that are lower

191
00:11:20,071 --> 00:11:23,220
ranked. They're not the big
flashy journals that send
out press releases and draw

192
00:11:23,221 --> 00:11:24,210
attention to themselves.

193
00:11:24,390 --> 00:11:28,620
So we don't necessarily even see
those followup studies or if we do,

194
00:11:28,800 --> 00:11:30,720
it is kind of a funny storyline to say,
Hey,

195
00:11:30,721 --> 00:11:32,610
do you remember s stretch
way back in your memory?

196
00:11:32,611 --> 00:11:36,210
And remember three years ago we were
talking about genes for, for autism. Well,

197
00:11:36,450 --> 00:11:41,160
nevermind that wasn't right. So, so
these, so you don't hear the, the, the,

198
00:11:41,161 --> 00:11:45,630
the studies that they come along and take.
Yeah. Nevermind. Nothing to see here.

199
00:11:45,690 --> 00:11:48,180
So at any rate,
what's going on here?

200
00:11:48,450 --> 00:11:50,760
I'm going to go give you a quick
survey of the sorts of things that are

201
00:11:50,761 --> 00:11:54,490
happening and going to focus in this,
talk on for them, bad ingredients,

202
00:11:54,810 --> 00:11:58,720
a dubious design of experiments,
statistical errors, uh, that,

203
00:11:58,721 --> 00:12:02,560
that are the crop up quite commonly.
And then some of the funding pressures,

204
00:12:02,561 --> 00:12:05,500
as you might imagine from
looking at the initial graph,

205
00:12:05,501 --> 00:12:09,610
you could appreciate that, uh, that's
creating a bad culture in, in science.

206
00:12:09,850 --> 00:12:14,710
And I want to talk first about, uh,
uh, bad ingredients in this case cells,

207
00:12:14,740 --> 00:12:19,690
a cell lines. This is a book that
was actually written in 1986. Uh,

208
00:12:19,691 --> 00:12:24,550
about, uh, a woman named Henrietta lacks
familiar name, the issues. She was a,

209
00:12:24,760 --> 00:12:28,780
she's now a featured
in an in new HBO story.

210
00:12:28,781 --> 00:12:33,680
Her cervical cancer cells were
or isolated and cultured in a,

211
00:12:34,050 --> 00:12:36,340
at the Johns Hopkins Hospital in 1951.

212
00:12:36,341 --> 00:12:39,400
And they became the first
human cell line of, uh, uh,

213
00:12:39,410 --> 00:12:43,090
that are used as a tremendous
tool for studying cancer,

214
00:12:44,260 --> 00:12:45,430
cervical cancer in particular,

215
00:12:45,431 --> 00:12:49,540
but used in generally just as
perpetual cancer cells. And, uh,

216
00:12:49,541 --> 00:12:54,541
this book is about the fact that those
cells actually ended up getting a very

217
00:12:56,471 --> 00:12:59,470
widely distributed. And, and, uh,

218
00:13:00,340 --> 00:13:03,190
as far as the 19 set back as the 1970s,

219
00:13:03,520 --> 00:13:06,610
a scientist started to realize that they
were contaminating all sorts of other

220
00:13:06,611 --> 00:13:09,100
cell lines that grew like weeds.
Nobody could quite tell.

221
00:13:09,101 --> 00:13:10,990
They had thought they had
isolated other cell lines,

222
00:13:10,991 --> 00:13:15,760
but in fact very frequently they were,
they were studying, he lost cells. Uh,

223
00:13:16,120 --> 00:13:20,350
and there's now a list of
451 contaminated cell lines.

224
00:13:20,351 --> 00:13:22,510
And if you go to the list of says,
you think you're studying this,

225
00:13:22,511 --> 00:13:25,780
if you're using this cell line, guess
what? You're studying something else.

226
00:13:25,781 --> 00:13:27,760
And sometimes you're not
even studying human cells.

227
00:13:27,761 --> 00:13:29,230
You may be studying rat cells.

228
00:13:29,500 --> 00:13:33,700
But if the of this list
of 450 plus cell lines,

229
00:13:34,030 --> 00:13:36,550
more than 100 of them are
Hilo cells. Actually. So this,

230
00:13:36,551 --> 00:13:38,950
this thing has just spread like crazy.

231
00:13:39,160 --> 00:13:43,270
And this book is about the frustrating
efforts of one particular science

232
00:13:43,330 --> 00:13:47,680
scientist named Walter Nelson Reese to
go out and tell his colleagues, hey,

233
00:13:47,681 --> 00:13:51,040
watch out. You're not studying what you're
thinking. You're studying. Be careful.

234
00:13:51,070 --> 00:13:53,980
Take a look at yourselves, make sure
you're, you're know what you're doing.

235
00:13:53,981 --> 00:13:58,810
And he died in a, it's in
frustration because scientists said,

236
00:13:58,811 --> 00:14:03,190
yeah, yeah, yeah, whatever.
And they continue to use
these contaminated cell lines.

237
00:14:03,191 --> 00:14:04,870
And there, there are many, many others.

238
00:14:04,990 --> 00:14:08,740
One of the favorite stories that I have
all along this lines is a cell line that

239
00:14:08,741 --> 00:14:12,130
came out of MD Anderson Hospital in 1976.

240
00:14:12,480 --> 00:14:15,550
It's called MDA MB four 35.

241
00:14:15,910 --> 00:14:17,760
And it was a cell line
that the scientists had.

242
00:14:17,761 --> 00:14:22,480
I isolated from a woman who had breast
cancer, 31 year old woman who uh, uh, and,

243
00:14:22,600 --> 00:14:25,720
and they collected the cells
and they grew up the cell line.

244
00:14:25,930 --> 00:14:28,030
It became one of the most
celebrated cell lines.

245
00:14:28,060 --> 00:14:32,890
It was the national cancer institute
sort of has a list of 60 critical cell

246
00:14:32,891 --> 00:14:35,990
lines that are sort of
archetypal sell ons,

247
00:14:36,020 --> 00:14:39,670
the for cancer of all of all varieties.
And this was on that list of 60,

248
00:14:39,910 --> 00:14:44,910
and I'm considered a really important
saw line for studying breast cancer.

249
00:14:45,550 --> 00:14:49,840
And uh, in the year 2000, uh,
some scientists at Stanford said,

250
00:14:49,841 --> 00:14:54,841
let's take a close look at the NCI 60
a and s and look at gene expression in

251
00:14:55,731 --> 00:14:57,380
these cells,
which is seeing,

252
00:14:57,410 --> 00:15:00,740
look at each so on and say what genes
are turned on and turned off in each of

253
00:15:00,741 --> 00:15:03,680
these cell lines and see if we see
some patterns. And Lo and behold,

254
00:15:03,681 --> 00:15:08,030
they found that the lung cancer cells
all had the same sort of set of genes,

255
00:15:08,031 --> 00:15:10,310
certain set of genes turned
on or there's turned off.

256
00:15:10,490 --> 00:15:14,500
Same was true of melanoma cells.
They were, you know, there was a,

257
00:15:14,501 --> 00:15:16,970
a nice pattern of gene
expression in these cells.

258
00:15:17,180 --> 00:15:20,360
And when they got to this breast cancer
cell, it was like, wait a second,

259
00:15:20,390 --> 00:15:24,230
that gene expression pattern does not
match the other breast cancer cells.

260
00:15:24,260 --> 00:15:28,910
It actually has the same gene expression
pattern as in fact one of the other

261
00:15:28,911 --> 00:15:33,050
melanoma cell lines in, in the collection.
And they put a notice up on the,

262
00:15:33,350 --> 00:15:37,340
on the NCI website saying, Hey,
attention folks, uh, this is,

263
00:15:37,640 --> 00:15:42,380
this was misidentified cell line. This
is, it looks like a melanoma, not a, uh,

264
00:15:42,860 --> 00:15:47,330
not a breast cancer at all. And
since, since the year 2000 Iu, uh,

265
00:15:47,390 --> 00:15:50,610
there's still have been hundreds and
hundreds of papers published of this cell

266
00:15:50,611 --> 00:15:52,700
on referring to it as a
breast cancer cell line.

267
00:15:52,910 --> 00:15:55,300
People haven't been paying
attention. Uh, they're,

268
00:15:55,330 --> 00:15:59,060
they're not finding or paying attention
to the warnings that this is a

269
00:15:59,061 --> 00:16:02,210
misidentified cell line.
So it's a, it's a crazy,

270
00:16:03,070 --> 00:16:04,730
it's a crazy circumstance out there.

271
00:16:04,850 --> 00:16:08,000
And now there are a effective
tests that are inexpensive.

272
00:16:08,001 --> 00:16:09,050
You can take your cell lines,

273
00:16:09,051 --> 00:16:12,500
mail them off to a lab and have them to
have them verified to find out if they

274
00:16:12,501 --> 00:16:15,200
are what, what you
think they are. And, uh,

275
00:16:15,350 --> 00:16:19,400
it was only last year that the NIH said,
if you're getting NIH funding,

276
00:16:19,401 --> 00:16:24,340
we expect you to do that. So
let's hope this problem is
being resolved. But, uh, uh,

277
00:16:24,390 --> 00:16:27,290
remains to be seen how many people
are following through on that dictum.

278
00:16:27,440 --> 00:16:30,170
So that was bad ingredients.
This is bad.

279
00:16:30,350 --> 00:16:34,610
This illustrates bad experimental design.
This data actually comes from, uh,

280
00:16:35,000 --> 00:16:38,600
from, uh, the outfit called the
als therapy development institute,

281
00:16:38,601 --> 00:16:40,940
which is in your neighborhood
right around the corner.

282
00:16:41,410 --> 00:16:45,200
And what they did was they noticed
that many, many drugs for als,

283
00:16:45,201 --> 00:16:49,970
Lou Gehrig's disease weren't working
in, in people. And the question is why.

284
00:16:50,000 --> 00:16:54,440
And so some years ago they went back to
look at the original underlying mouse

285
00:16:54,441 --> 00:16:59,350
studies that had been done to that,
that suggested that these drugs were good.

286
00:16:59,590 --> 00:17:03,890
And the blue bars show increased
survival in the mouse studies for these

287
00:17:03,891 --> 00:17:05,210
original studies.

288
00:17:05,480 --> 00:17:09,740
But the folks at als Tdi recognize that
a lot of these studies were very poorly

289
00:17:09,741 --> 00:17:13,640
designed. That is very few mice.
They, uh, the, the results were,

290
00:17:14,680 --> 00:17:15,620
uh,
you know,

291
00:17:16,460 --> 00:17:19,280
really there's so much noise that they
weren't really believing what they were

292
00:17:19,281 --> 00:17:19,581
seeing.

293
00:17:19,581 --> 00:17:24,581
So they ran all of those experiments
with substantial number of mice watching

294
00:17:25,191 --> 00:17:27,140
them, making sure they got
the sex of the mice right,

295
00:17:27,141 --> 00:17:28,940
that the genetics were correct and so on.

296
00:17:29,300 --> 00:17:31,880
And the black bar show
what they ended up with.

297
00:17:31,881 --> 00:17:34,300
And as you can see almost nothing,
uh,

298
00:17:34,670 --> 00:17:38,080
it worked at all in, in mice, uh,

299
00:17:38,270 --> 00:17:41,670
when the mouse tests were done
correctly. And this is this, there's,

300
00:17:41,710 --> 00:17:45,140
this is a very common problem in,
in biomedicine.

301
00:17:45,380 --> 00:17:47,000
And one reason is that the,
uh,

302
00:17:47,001 --> 00:17:50,670
the als TDI studies each cost well
over a hundred thousand dollars to run.

303
00:17:50,850 --> 00:17:52,140
And if you're a university lab,

304
00:17:52,170 --> 00:17:54,840
you don't have that kind of
money to run a single experiment.

305
00:17:55,050 --> 00:17:59,550
So you run a small experiment. You say,
this is a pilot study. Fingers crossed.

306
00:17:59,551 --> 00:18:03,480
It means what it would mean if we actually
did it with, you know, 32 mice in a,

307
00:18:03,780 --> 00:18:07,560
in a,
in a cohort as opposed to five or 10,

308
00:18:07,561 --> 00:18:12,100
which is what the universities
tend to use. So, so this is a, uh,

309
00:18:12,480 --> 00:18:16,500
this is, uh, an issue that is driven
both by financial pressures and just by,

310
00:18:16,560 --> 00:18:19,650
you know, people hoping for the
best when they're doing experiments.

311
00:18:19,651 --> 00:18:23,790
But I think it's one of the
grave concern about how much, uh,

312
00:18:23,850 --> 00:18:27,960
we're really getting out of these, uh,
biomedical research labs if they're,

313
00:18:28,100 --> 00:18:30,180
you know, trying to do the
best they can on the cheap.

314
00:18:30,181 --> 00:18:33,090
It doesn't necessarily pan out.
And the, and you know, these,

315
00:18:33,120 --> 00:18:37,110
these experiments cost many millions of
dollars to run once they got to human

316
00:18:37,111 --> 00:18:40,570
beings. And essentially none
of these drugs work, uh,

317
00:18:40,580 --> 00:18:44,820
that the top drug has a very minor effect,
but it's nothing to get excited about.

318
00:18:44,820 --> 00:18:46,110
And all the rest of them,
you know,

319
00:18:46,530 --> 00:18:49,830
tens of millions of dollars have been
spent on each of those studies or many of

320
00:18:49,831 --> 00:18:54,720
those studies at any rate. And they
were all bused. So, uh, so, uh,

321
00:18:54,780 --> 00:18:57,630
the folks at als Tdi
are now trying to say,

322
00:18:57,780 --> 00:19:00,690
let's do the careful experiments first.
Let's run these,

323
00:19:00,691 --> 00:19:03,480
let's spend the couple of
hundred thousand dollars upfront.

324
00:19:03,960 --> 00:19:07,920
And if we find good stuff, then we
can move forward. But let's, let's,

325
00:19:07,950 --> 00:19:11,340
let's be a little bit more cautious with
our preclinical research before we take

326
00:19:11,341 --> 00:19:14,700
it into human beings. Uh, this guy,

327
00:19:15,240 --> 00:19:17,010
I've got lots of stories
about Keith Bagherli,

328
00:19:17,011 --> 00:19:20,460
his a biostatistician at
MD Anderson in Houston,

329
00:19:20,461 --> 00:19:25,461
but I'll just tell one
story about a ovarian cancer
tests that he discovered was

330
00:19:27,100 --> 00:19:31,170
a rather inappropriately
put on the market. Uh,

331
00:19:31,320 --> 00:19:34,100
it had been discovered by,
uh,

332
00:19:34,200 --> 00:19:37,620
actually the scientists at
the NCI and at the FDA. They,

333
00:19:37,890 --> 00:19:42,270
instead of looking for sort of your
standard molecule that would indicate the

334
00:19:42,271 --> 00:19:44,820
presence of ovarian cancer, uh, they said,

335
00:19:45,120 --> 00:19:47,790
let's try a different approach
because that had, that had failed.

336
00:19:48,030 --> 00:19:50,550
And what they did was they
used a mass spectrometer,

337
00:19:50,551 --> 00:19:53,670
which sort of looks at the masses
of various part of particles.

338
00:19:53,840 --> 00:19:56,280
And we'll look at the
spectrum of mass of these, uh,

339
00:19:56,520 --> 00:20:00,030
of these proteins in the blood
from women with ovarian cancer.

340
00:20:00,180 --> 00:20:03,450
And women who don't have ovarian cancer
and see if we can see some sort of

341
00:20:03,451 --> 00:20:06,420
different in that could be a powerful
new test. And they announced, hey,

342
00:20:06,421 --> 00:20:09,840
we found something that's really
works great. Uh, actually, uh,

343
00:20:10,110 --> 00:20:13,830
folks in Congress passed a resolution
saying this is fabulous work,

344
00:20:13,890 --> 00:20:16,590
keep it up in 2002 and a,

345
00:20:16,591 --> 00:20:19,410
and Keith Bagley and his colleagues
at MD Anderson tried to do it,

346
00:20:19,650 --> 00:20:23,310
couldn't get it to work. And
finally back, uh, Bagherli said,

347
00:20:23,311 --> 00:20:25,200
let me go back and look
at the original data.

348
00:20:25,380 --> 00:20:29,640
And what he noticed was all of the women
with ovarian cancer had their tests run

349
00:20:29,641 --> 00:20:32,850
on one particular day and the
controls were run on a different day.

350
00:20:33,090 --> 00:20:36,150
And then she was just tuned differently
on what from one day to the other.

351
00:20:36,151 --> 00:20:40,080
So this was, this is a example of what
scientists called the batch of fact.

352
00:20:40,140 --> 00:20:44,760
And this is another problem that that
keeps cropping up in biomedical research.

353
00:20:45,000 --> 00:20:48,970
Uh, and uh, and that's pretty,
pretty shocking that, uh,

354
00:20:49,180 --> 00:20:53,620
that that test got so far
along before people realized,
Oh, this is just, you know,

355
00:20:53,830 --> 00:20:57,520
completely false lead. Uh, another, uh,

356
00:20:58,000 --> 00:20:59,920
statistical problem is called Harking,

357
00:20:59,921 --> 00:21:02,080
which is hypothesizing
after the results are known,

358
00:21:02,450 --> 00:21:05,080
it sounds like fairly innocent
thing to do. You Run an experiment,

359
00:21:05,410 --> 00:21:07,240
you don't get the results
you expect, but you say, oh,

360
00:21:07,241 --> 00:21:11,320
but that's an unexpected result. I'm going
to say that was my hypothesis. And, uh,

361
00:21:11,560 --> 00:21:11,801
you know,

362
00:21:11,801 --> 00:21:15,130
that this gene is associated with the
cancer that I wasn't expecting it to be.

363
00:21:15,310 --> 00:21:20,140
It turns out most of the time those
results are wrong. And, uh, uh, just,

364
00:21:20,180 --> 00:21:24,330
just by nature of the kind of statistics
that people apply to those, the,

365
00:21:24,480 --> 00:21:29,020
the p values and so on, the
whole statistical mechanism,
it's perfectly good to,

366
00:21:29,250 --> 00:21:32,080
to do an experiment like that and to say,
uh,

367
00:21:32,260 --> 00:21:35,950
I've generated a new hypothesis in the
study and now I'm going to go test it in

368
00:21:35,951 --> 00:21:36,670
a new one.

369
00:21:36,670 --> 00:21:40,630
But it's really inappropriate
and unfortunately very
common for scientists to

370
00:21:40,631 --> 00:21:43,450
say, look at the results
I found. This is, this is,

371
00:21:43,660 --> 00:21:46,450
here's my new hypothesis and
here's data that support it.

372
00:21:46,451 --> 00:21:48,440
So it's another reason that,
uh,

373
00:21:48,560 --> 00:21:51,880
there's actually a famous paper written
by a guy named John John Ian eds,

374
00:21:51,881 --> 00:21:52,970
who's at Stanford,
who,

375
00:21:53,010 --> 00:21:56,920
who has a paper titled Why most
published research findings are false.

376
00:21:57,130 --> 00:22:02,070
And this is, uh, this is fundamentally
part of the reason for that. Uh,

377
00:22:02,530 --> 00:22:05,500
also, uh, behavior in, in,

378
00:22:05,680 --> 00:22:09,130
in science because of this
funding pressures. Uh,

379
00:22:09,340 --> 00:22:11,740
scientists are pressured to cut corners.

380
00:22:12,130 --> 00:22:14,840
They know that if they don't
publish their papers in the, in the,

381
00:22:15,020 --> 00:22:19,210
in the top journals, they're unlikely to
get more grants or promotions and so on.

382
00:22:19,390 --> 00:22:22,420
So there's huge incentives for people to,
you know,

383
00:22:22,450 --> 00:22:26,250
leave out the data that doesn't quite
help them tell their story or to, to,

384
00:22:26,320 --> 00:22:28,660
you know,
to just to nudge things around here.

385
00:22:28,750 --> 00:22:30,880
And this is a study from
the national academies.

386
00:22:30,881 --> 00:22:32,840
It just came out a couple of weeks ago,
uh,

387
00:22:32,920 --> 00:22:36,460
looking at these issues more
broadly about scientific integrity.

388
00:22:36,790 --> 00:22:41,790
And this is a follow up from a study that
was done 25 years ago where the focus

389
00:22:42,251 --> 00:22:46,810
was on scientists who, uh, sort
of the bad apples, if you will,

390
00:22:46,990 --> 00:22:51,990
looking at the actually fairly rare cases
of outright fraud and misconduct that,

391
00:22:53,050 --> 00:22:57,010
uh, that is, uh, responsible for some
of these cases, but not all that many.

392
00:22:57,190 --> 00:23:00,100
And they said, we're, we're
looking differently at this. Now.

393
00:23:00,101 --> 00:23:01,570
When I talked to one of the panelists,

394
00:23:01,571 --> 00:23:05,320
a CK can sailors who said we'd been
fond of the bad apple narrative,

395
00:23:05,321 --> 00:23:08,170
and now we're talking about switching
to the barrels and the barrel makers.

396
00:23:08,171 --> 00:23:12,870
In other words, trying to understand
the system that is setting people up to,

397
00:23:12,930 --> 00:23:16,330
to act appropriately, to, you
know, to shave, to, to make,

398
00:23:16,331 --> 00:23:19,090
to do questionable research practices.
These are not outright fraud,

399
00:23:19,091 --> 00:23:22,000
but there are things that
you know, that are, that are,

400
00:23:22,030 --> 00:23:25,570
that are questionable and that are
detrimental to the progress of research.

401
00:23:25,720 --> 00:23:30,640
So that's a, that's another very
significant and hard to measure a problem.

402
00:23:30,641 --> 00:23:31,180
But,
uh,

403
00:23:31,180 --> 00:23:35,990
but there have been a couple of surveys
and exploration's of that and uh,

404
00:23:36,020 --> 00:23:39,550
it's clearly part of the underwrite
an underlying problem here.

405
00:23:40,000 --> 00:23:42,850
So solutions validating ingredients.

406
00:23:42,851 --> 00:23:45,950
I already mentioned that now
there's this easy test for example,

407
00:23:45,980 --> 00:23:48,560
to send off your cell
lines and get them tested.

408
00:23:49,130 --> 00:23:52,070
Scientists are expected to do that.
It's only a year old.

409
00:23:52,340 --> 00:23:55,910
I'm really interested to see whether
people are following up on that and I hope

410
00:23:55,960 --> 00:23:57,470
I've hoped to find out,
but I don't know yet.

411
00:23:58,160 --> 00:24:01,820
Transparency of making your
data and your code available.

412
00:24:01,821 --> 00:24:04,570
I think this is an idea that's
getting a lot of traction.

413
00:24:04,571 --> 00:24:07,280
We'll talk a little more about that.
Better training for scientists.

414
00:24:07,460 --> 00:24:11,300
Very often scientists just learn from
the mentors and they don't necessarily

415
00:24:11,301 --> 00:24:16,100
realize that what they're doing is,
is detrimental to their, uh, to their,

416
00:24:16,250 --> 00:24:18,890
to what they're doing. And uh,
and if they had better training,

417
00:24:18,891 --> 00:24:19,850
maybe they would know more.

418
00:24:20,060 --> 00:24:23,840
And finally finding some way to use
this financial crunch because that's the

419
00:24:23,841 --> 00:24:25,520
underlying pressure
that's driving a lot of,

420
00:24:25,521 --> 00:24:30,020
this is my favorite story
about transparency. Uh,

421
00:24:30,021 --> 00:24:32,480
this is a paper published. The, the, the,

422
00:24:32,481 --> 00:24:37,010
the first draft of the human genome
published in nature in February,

423
00:24:37,010 --> 00:24:41,810
2001 this was a, you know, a milestone
in biological research, uh, in,

424
00:24:41,900 --> 00:24:44,020
in science in general. And, uh,

425
00:24:44,300 --> 00:24:49,130
and instead of publishing the entire 3
billion base pair code of DNA and in a,

426
00:24:49,140 --> 00:24:50,600
in nature, what they did was to say, well,

427
00:24:50,610 --> 00:24:54,770
we're highlighting about a dozen or so
sort of exciting and unexpected findings.

428
00:24:55,010 --> 00:24:59,930
And one of them was they reported finding
dozens of genes that were bacterial

429
00:24:59,931 --> 00:25:03,260
genes right in our human genome. And they
thought, that's weird. How could that be?

430
00:25:03,261 --> 00:25:04,460
It's really interesting how,

431
00:25:04,670 --> 00:25:08,360
how could bacterial genes
jumped into the human genome?

432
00:25:08,510 --> 00:25:11,450
And the answer was they
didn't, uh, Steven Salzberg,

433
00:25:11,451 --> 00:25:15,050
who was at the Institute for
Genome Research, uh, at the time,

434
00:25:15,110 --> 00:25:17,660
and some of his colleagues said,
I don't believe this is true,

435
00:25:17,990 --> 00:25:20,810
but the genome data were
all publicly available,

436
00:25:20,870 --> 00:25:24,500
a completely transparent and
Salzburg and his colleague said,

437
00:25:24,890 --> 00:25:28,100
let's do our own analysis.
They pulled it the same day to sit down.

438
00:25:28,370 --> 00:25:31,580
They looked it over and they had a
completely different explanation for those

439
00:25:31,581 --> 00:25:36,170
apparently microbial genes in the
genome. And they said, false alarm guys,

440
00:25:36,171 --> 00:25:40,250
this isn't, this didn't happen. So,
and this is, this was in June of 2001.

441
00:25:40,251 --> 00:25:44,770
It was just a few months later. It
was really quite a remarkable, uh, uh,

442
00:25:45,390 --> 00:25:49,820
uh, feedback loop for, for getting
results either verified or,

443
00:25:50,300 --> 00:25:51,920
or not verified in this case.

444
00:25:51,980 --> 00:25:56,980
And I think this is an incredibly
powerful example of how transparency can,

445
00:25:57,050 --> 00:26:00,590
can work and can really help resolve
these questions much more rapidly.

446
00:26:00,770 --> 00:26:03,200
Scientists of course,
guard their data jealously.

447
00:26:03,201 --> 00:26:04,870
They are concerned that
there'll be scooped,

448
00:26:04,871 --> 00:26:07,730
that their intellectual property will
be whisked away by other scientists.

449
00:26:07,731 --> 00:26:11,450
And so those are very real issues.
And I don't mean to minimize them,

450
00:26:11,660 --> 00:26:14,900
but it is also true that to the extent
that you can get people to put their data

451
00:26:14,901 --> 00:26:17,090
up,
their code up and to,

452
00:26:17,150 --> 00:26:20,030
and to share their ingredients when
asked than other scientists can,

453
00:26:20,060 --> 00:26:23,460
can rapidly come around and say, this is
working. Is this right? Is this right?

454
00:26:23,540 --> 00:26:24,373
Is this wrong?

455
00:26:24,500 --> 00:26:28,100
And I also think that if you know that
any that you have that much transparency

456
00:26:28,101 --> 00:26:30,890
in what you've done,
maybe just take a little more time to,

457
00:26:30,940 --> 00:26:34,820
to fact check it yourself and make sure
that if there are problems that you find

458
00:26:34,880 --> 00:26:38,810
it before your, before your, uh,
your, uh, your colleagues do.

459
00:26:38,930 --> 00:26:42,890
So I think that's a really powerful idea.
And I think the new culture,

460
00:26:42,891 --> 00:26:45,210
the culture of, of, uh,

461
00:26:45,211 --> 00:26:49,860
that comes out of your world of open
source and so on, I think has really led,

462
00:26:50,300 --> 00:26:54,180
uh, I think the younger
generation to appreciate this
and to be less resistant to

463
00:26:54,181 --> 00:26:59,070
sharing. And I, I think that's a positive
long term trend in, in biomedicine.

464
00:26:59,071 --> 00:27:03,360
But, uh, it will, it will not just
sweep through the field overnight,

465
00:27:03,361 --> 00:27:06,600
but I think it would be very
salutary in terms of education.

466
00:27:06,600 --> 00:27:10,040
This is a wonderful scientist
named Arturo Casa Duvall,

467
00:27:10,060 --> 00:27:12,450
who's at Johns Hopkins now and,
uh,

468
00:27:12,960 --> 00:27:16,350
and it's his view that basically the
educational system needs to be reformed.

469
00:27:16,560 --> 00:27:18,430
And, uh, there are essentially no growth,

470
00:27:18,450 --> 00:27:21,710
good methodology classes
anywhere in biomedicine. Uh,

471
00:27:22,110 --> 00:27:24,930
you sort of learn from your mentor
and if your mentor is Great,

472
00:27:24,931 --> 00:27:26,970
you learned a great technique and
if your mentor is not so good,

473
00:27:27,180 --> 00:27:30,090
well maybe you learn good laboratory
technique but you don't necessarily learn

474
00:27:30,420 --> 00:27:34,110
how to think, how to, how to sort of
think through problems and how to,

475
00:27:34,380 --> 00:27:37,550
how to deal with some of these broader
issues. So constant of all is uh,

476
00:27:37,860 --> 00:27:42,860
trying to think about how to retool a
science education and he's by far not

477
00:27:43,081 --> 00:27:44,940
alone. There are lots of other
people thinking about that.

478
00:27:45,540 --> 00:27:48,840
Want to just end the talk talking about
four people who I think have really

479
00:27:48,841 --> 00:27:51,290
interesting ideas. Uh, these are, uh,

480
00:27:51,340 --> 00:27:54,330
the first two are Carolyn
Compton and Anna Barker who are,

481
00:27:54,510 --> 00:27:58,620
we're both at the national cancer
institute and are now at Arizona State

482
00:27:58,621 --> 00:28:02,820
University. And Dr. Compton is a
pathologist and she noticed that,

483
00:28:02,880 --> 00:28:04,440
that that uh,

484
00:28:04,530 --> 00:28:09,530
tissues collected either for blood or
organ cancer tissue and so on is collected

485
00:28:09,691 --> 00:28:12,990
in a very willy nilly manner. And,
and everyone does it differently.

486
00:28:12,991 --> 00:28:16,320
No one thinks very much about
how long it has to sit around,

487
00:28:16,500 --> 00:28:20,190
how quickly it gets preserved, when
it gets frozen, when it gets or, or,

488
00:28:20,640 --> 00:28:23,490
or preserved with, fix it
over, whatever. And uh,

489
00:28:23,640 --> 00:28:26,820
and she realized that as we're moving
into the world of personalized medicine,

490
00:28:27,090 --> 00:28:31,470
if you're not doing collecting that
material in a very uniform way,

491
00:28:31,710 --> 00:28:35,160
you're going to end up with a lot of
variation that is going to be misleading.

492
00:28:35,190 --> 00:28:39,090
And so she's been working really hard
with pathologist to get them to develop

493
00:28:39,091 --> 00:28:43,770
and use uniform standards for tissue
collection so that when we start diving

494
00:28:43,771 --> 00:28:48,380
into this data, we can actually have some
faith that there's at least that, that,

495
00:28:48,410 --> 00:28:51,510
that very important source of, of, uh,

496
00:28:51,750 --> 00:28:56,340
variation is limited by careful
collecting of tissue. Uh,

497
00:28:56,400 --> 00:29:01,400
her colleague and a barker is sort of
taking the whole idea of reproducibility

498
00:29:02,091 --> 00:29:06,290
and, and a very rigorous approach
to, she's applying to, uh,

499
00:29:06,420 --> 00:29:10,080
studies of Glioblastoma,
a very difficult to treat brain cancer.

500
00:29:10,081 --> 00:29:12,660
And she sort of starting from the very
beginning and going all the way through

501
00:29:12,661 --> 00:29:16,950
clinical trials of Glioblastoma to say
we're going to do every single step as

502
00:29:17,010 --> 00:29:21,000
rigorously as we possibly can. And I
said, why did you choose Glioblastoma?

503
00:29:21,001 --> 00:29:23,340
It's one of the hardest
cancers to cure brain cancer.

504
00:29:23,341 --> 00:29:27,570
That that essentially is as a death
sentence almost always. And she said,

505
00:29:27,571 --> 00:29:29,730
well if I can make it
work with Glioblastoma,

506
00:29:30,000 --> 00:29:33,220
people will have to sit up and pay
attention. So that experiment is,

507
00:29:33,830 --> 00:29:37,440
it was just going into the clinical
trials right now and it could be very

508
00:29:37,441 --> 00:29:40,380
interesting to see whether,
I think it's an uphill battle for her,

509
00:29:40,381 --> 00:29:43,270
but if she can make work that
would be really quite remarkable.

510
00:29:43,900 --> 00:29:47,050
The final two guys I want to talk about
our Steve Goodman and John Ian who are

511
00:29:47,051 --> 00:29:50,020
at the Meta research innovation
center at Stanford metrics.

512
00:29:50,350 --> 00:29:55,350
And their idea basically is we need to
study how science is being conducted.

513
00:29:56,591 --> 00:30:00,940
This is the metal research of research
in order to understand why all of these

514
00:30:00,941 --> 00:30:01,750
things come up and,

515
00:30:01,750 --> 00:30:05,680
and what kind of solutions we can
come up with to resolve them. And

516
00:30:07,180 --> 00:30:11,800
they were both veterans of actually
addressing this very similar set of issues

517
00:30:11,801 --> 00:30:15,520
that came up with a human
based medicine in the 1990s.

518
00:30:15,521 --> 00:30:20,210
Many of the clinical trials that were
done back in that era or done, uh,

519
00:30:20,220 --> 00:30:22,240
with very poor record,
they were,

520
00:30:22,270 --> 00:30:26,950
they were not put together with s with
as much care and caution as they are

521
00:30:26,951 --> 00:30:28,960
today by no means perfect,

522
00:30:29,230 --> 00:30:33,910
but w but scientists are now thinking
much more carefully about sample sizes,

523
00:30:33,911 --> 00:30:34,661
about controls,

524
00:30:34,661 --> 00:30:38,680
about making sure that they're
avoiding bias as much as possible and,

525
00:30:39,070 --> 00:30:43,110
and avoiding things like, uh, like
hypothesizing after the results are known,

526
00:30:43,111 --> 00:30:47,980
which is a huge problem in biomedical
research and one tool that has been put to

527
00:30:47,981 --> 00:30:49,770
use and it's quite successful,
um,

528
00:30:50,800 --> 00:30:53,470
at least in addressing some of these
issues as a website called the clinical

529
00:30:53,471 --> 00:30:58,060
trials.gov if you're a scientist and
you want to develop a new drug and do a

530
00:30:58,061 --> 00:30:59,560
clinical trial on human beings,

531
00:31:00,040 --> 00:31:03,610
you have to put your data or you have
to register in advance on clinical

532
00:31:03,611 --> 00:31:06,550
trials.gov you have to say in
advance what your hypothesis is.

533
00:31:06,670 --> 00:31:10,000
So if you come up with findings
that support some other hypothesis,

534
00:31:10,001 --> 00:31:14,440
you have to admit that wasn't my
hypothesis and these results are not to be

535
00:31:14,560 --> 00:31:15,880
taken literally. But, uh,

536
00:31:16,120 --> 00:31:20,500
and you also need to post your results
once you've finished your study,

537
00:31:20,770 --> 00:31:21,220
uh,

538
00:31:21,220 --> 00:31:24,220
to make sure that you're not just sort
of hiding results that are disappointing

539
00:31:24,400 --> 00:31:29,040
cause the biomedical literature is, uh,
is skewed in terms of results that are,

540
00:31:29,060 --> 00:31:31,360
that are positive results.
People who find things that aren't,

541
00:31:31,390 --> 00:31:35,080
that don't work out tend not to spend
the time to publish them. They'd may not,

542
00:31:36,190 --> 00:31:40,540
it may just be the effort involved in
putting it together or whatever. But the,

543
00:31:40,541 --> 00:31:43,000
uh, but a lot, but the, as a result,

544
00:31:43,001 --> 00:31:45,010
when someone goes to survey
the literature and say, well,

545
00:31:45,011 --> 00:31:48,190
how many studies show this is true and
how many don't they get a very skewed

546
00:31:48,191 --> 00:31:52,750
sample. And, and so, uh, this,
a website is designed to,

547
00:31:53,290 --> 00:31:57,790
to, to at least encourage scientists to
publish their results, uh, to get a more,

548
00:31:58,090 --> 00:32:00,100
a more well rounded sample of that.

549
00:32:00,400 --> 00:32:03,400
Many scientists unfortunately
still are not publishing a posting.

550
00:32:03,401 --> 00:32:05,950
The results in here as is required by law.

551
00:32:06,250 --> 00:32:10,930
But at least you know that when you
go there and say no results found, uh,

552
00:32:10,960 --> 00:32:14,230
that you can see, you can at least
know that there, there was a study,

553
00:32:14,231 --> 00:32:15,820
there were results and uh,

554
00:32:16,150 --> 00:32:19,530
and if you really care about doing
a Meta analysis or something, uh,

555
00:32:19,570 --> 00:32:23,530
you better track him down. Uh, so at least
you know that they exist even if you,

556
00:32:23,531 --> 00:32:26,320
even if they aren't
public. So at any rate, um,

557
00:32:27,220 --> 00:32:30,880
I wanted to end on that note because
I think that it really is, um,

558
00:32:31,030 --> 00:32:34,000
I mean there are things that can
be done and there are patterns.

559
00:32:34,000 --> 00:32:38,920
There are examples from the past of taking
similar issues in reproducibility and

560
00:32:38,930 --> 00:32:42,710
finding ways to, to improve the
systems. And it's, you know,

561
00:32:42,950 --> 00:32:46,790
the scientists who are doing this don't
want to spend their careers, you know,

562
00:32:47,420 --> 00:32:49,970
wasting their time and coming
up with results that aren't,

563
00:32:50,150 --> 00:32:51,560
that aren't pushing science forward.

564
00:32:51,561 --> 00:32:56,240
So I think the best news is
that scientists would like
this problem to be solved.

565
00:32:56,480 --> 00:33:00,410
And the problem is with the financial
pressures and all the rest of that,

566
00:33:00,411 --> 00:33:04,610
the system is still not set up
to help them succeed. And so, uh,

567
00:33:04,611 --> 00:33:07,060
and those are hard problems to
solve. Obviously you, you're,

568
00:33:07,310 --> 00:33:10,280
you can't double the amount of
money in it, the Nh again, or are,

569
00:33:10,281 --> 00:33:12,210
you can't ask half the people who are,
uh,

570
00:33:12,770 --> 00:33:15,830
who are currently doing this kind of
research to leave and find some other kind

571
00:33:15,831 --> 00:33:19,280
of line of work.
So they're very difficult issues to solve,

572
00:33:19,281 --> 00:33:24,110
but that people are thinking about at
least how to make some improvements along

573
00:33:24,510 --> 00:33:26,130
along the way.
And,

574
00:33:26,230 --> 00:33:29,180
and there's still a group of people
thinking about how we can solve the

575
00:33:29,181 --> 00:33:32,850
underlying problem. I think that's the
hardest one of all. But I think, uh,

576
00:33:33,080 --> 00:33:36,470
I think that the fact that
people want to do good work,

577
00:33:37,520 --> 00:33:39,060
I think really helps,
uh,

578
00:33:39,620 --> 00:33:42,620
helps us move forward with
this and having the support,

579
00:33:42,621 --> 00:33:45,050
at least current of the
current leadership of the Nih,

580
00:33:45,051 --> 00:33:49,350
I think also really helps provide
a path forward, at least for,

581
00:33:49,460 --> 00:33:50,580
for some of these. So, yeah,

582
00:33:50,590 --> 00:33:52,880
at any rate at this point would
be delighted to Dick Questions.

583
00:33:52,881 --> 00:33:53,714
Thanks for your attention.

584
00:33:58,500 --> 00:33:59,333
Okay.

585
00:34:00,240 --> 00:34:02,220
So I, I have a quick, uh,

586
00:34:02,430 --> 00:34:06,180
kind of something that I thought of
as you were giving the talk that, uh,

587
00:34:08,000 --> 00:34:08,310
okay,

588
00:34:08,310 --> 00:34:13,080
much, uh, in, in the Internet pass there
was always this a question of like,

589
00:34:13,081 --> 00:34:17,160
if you go look for a particular reseller,

590
00:34:17,280 --> 00:34:19,890
do you know that they're
actually like providing hue?

591
00:34:20,160 --> 00:34:24,660
The things are advertising and a,
it was an early problem and uh,

592
00:34:24,960 --> 00:34:29,960
some clever people put together a
site that collected stories about each

593
00:34:30,031 --> 00:34:34,500
reseller. So if you found someone, you
kind of had to go to sites and says, okay,

594
00:34:34,680 --> 00:34:38,880
was there ever any follow up on
this? You know, was this happen?

595
00:34:38,910 --> 00:34:42,780
Is there something like that
going on maybe for, you know,

596
00:34:42,810 --> 00:34:44,700
this backlog of research papers,

597
00:34:44,701 --> 00:34:48,570
cause I know there's not a
lot of retractions that ever
reached the light of day,

598
00:34:49,200 --> 00:34:52,680
but if you're searching in the past for,
you know,

599
00:34:52,740 --> 00:34:55,350
some promising research,

600
00:34:56,700 --> 00:35:00,090
it's kind of hard to follow the trail of,
you know,

601
00:35:00,300 --> 00:35:03,260
how did that ever Pan out?
It is difficult. And I,

602
00:35:03,261 --> 00:35:07,400
and one example I encountered in my book
is there was a study that was linking

603
00:35:07,430 --> 00:35:12,280
gene expression, uh, and comparing
it between races and uh, and,

604
00:35:12,370 --> 00:35:15,440
and it was a paper that the asserted
that there was a very large difference in

605
00:35:15,441 --> 00:35:19,700
gene expression between Caucasians and
Asians and other scientists looked at the

606
00:35:19,701 --> 00:35:22,010
data and we're were highly critical of it.

607
00:35:22,250 --> 00:35:26,240
And they were able to publish a paper
in a completely different literature.

608
00:35:26,241 --> 00:35:28,160
I think it was nature methods
or something like that,

609
00:35:28,161 --> 00:35:30,860
which is not the human genetics
literature, but it was, you know,

610
00:35:30,861 --> 00:35:33,770
in the scientific literature. And
they said, we found these flaws.

611
00:35:34,040 --> 00:35:37,410
The scientists sort of responded and said,
yeah we did screw up a little bit, we,

612
00:35:37,411 --> 00:35:41,160
you know, whatever. But that whole
discussion took place off to the side.

613
00:35:41,161 --> 00:35:43,980
If you want to go and find it in
the medical literature you can.

614
00:35:44,280 --> 00:35:48,000
But there's still like hundreds and
hundreds of papers that site the original

615
00:35:48,001 --> 00:35:51,240
research and have no idea that this
discussion has taken place cause they were

616
00:35:51,241 --> 00:35:54,630
just looking in the, in the genetics
literature or whatever. So it's a,

617
00:35:55,080 --> 00:35:59,130
it's a very big problem of sort
of hide and seek if you will.

618
00:35:59,880 --> 00:36:01,290
There's actually a tool that has,

619
00:36:01,350 --> 00:36:05,460
that has come up that is helping this
a little bit, which is a, um, uh,

620
00:36:05,790 --> 00:36:08,130
called pub here,

621
00:36:08,370 --> 00:36:12,900
which is a basically an open
comments site that if you,

622
00:36:12,920 --> 00:36:16,860
and you can post comments about anything
you read in the literature and, uh,

623
00:36:16,920 --> 00:36:19,830
and I have an APP on my browser
that lets me, that alerts me.

624
00:36:19,831 --> 00:36:23,730
If I go to a page that has, this,
has this paper, it'll alert me.

625
00:36:23,880 --> 00:36:26,970
There are six comments about
this paper on pub pier. So,

626
00:36:27,000 --> 00:36:31,590
so there are tools that are starting
to be used to do that, but it's a,

627
00:36:32,070 --> 00:36:35,380
or if there's some similar system in a,
in,

628
00:36:35,410 --> 00:36:39,630
if you go to the NIH, the National
Library of medicine's database,

629
00:36:39,870 --> 00:36:44,040
they have a pub Med and
comments at pub med.

630
00:36:44,220 --> 00:36:45,720
And so you can see comments there.

631
00:36:45,840 --> 00:36:49,920
The difference being the pub
med comments are not anonymous,

632
00:36:49,921 --> 00:36:54,390
so people are a little bit more, uh, a
little less likely to engage their pub.

633
00:36:54,391 --> 00:36:57,570
Here. You can be anonymous
so it can also be, you know,

634
00:36:57,571 --> 00:37:01,260
be a little spicier as you can imagine
for anonymous comments. But yeah, it is a,

635
00:37:01,660 --> 00:37:04,460
it is a huge information
flow problem that,

636
00:37:04,710 --> 00:37:06,980
that scientists are
starting to chip away at.

637
00:37:06,990 --> 00:37:09,240
But there's a huge room
for improvement for sure.

638
00:37:13,150 --> 00:37:16,000
So one of the points he made,

639
00:37:16,090 --> 00:37:20,620
or a couple I guess you said one is
that some of this bad science is being

640
00:37:20,621 --> 00:37:23,080
shaped by pressure from a lack of funding.

641
00:37:23,081 --> 00:37:28,081
And so I think by implication if funding
were more readily available than maybe

642
00:37:28,301 --> 00:37:31,570
there'd be less bad science. And at the
same time you said, well I can't ask,

643
00:37:31,571 --> 00:37:33,760
you know,
if you can't double the budget,

644
00:37:33,761 --> 00:37:36,430
you also can't ask caps
the people to leave.

645
00:37:37,180 --> 00:37:40,870
But I really want to question that because
it seems that we can either trust and

646
00:37:40,871 --> 00:37:45,871
reverify that the public money we put
into science now is generating genuine

647
00:37:46,091 --> 00:37:50,920
science. A lot of it's bogus and we
don't even know which parts are how much.

648
00:37:51,790 --> 00:37:54,640
And there are certainly bad actors.
Some of them are very,

649
00:37:54,641 --> 00:37:56,140
very influential in fact.

650
00:37:56,560 --> 00:38:00,070
And some of them are known in the
community and some of them have,

651
00:38:00,520 --> 00:38:03,910
have concealed themselves, you
know, more effectively so far.

652
00:38:04,600 --> 00:38:08,690
And that's a huge problem. So the idea
that putting more money in to as a,

653
00:38:08,691 --> 00:38:11,140
it's a situation that's
generating bad science,

654
00:38:11,410 --> 00:38:14,860
it seems like throwing good money after
bad. And at the same time saying, well,

655
00:38:14,861 --> 00:38:18,010
we can't ask half the people to leave.
Well if half of them are bad actors,

656
00:38:18,011 --> 00:38:21,340
maybe we should. Except I would argue with
it. I wouldn't, let me push back. Oh go.

657
00:38:21,341 --> 00:38:24,010
I'm sorry. But let me, let me
go on to the people at large,

658
00:38:24,011 --> 00:38:28,130
the public at large who elect
the officials. You know, the,

659
00:38:28,131 --> 00:38:32,200
the government that sets the public policy
around this are furthermore not well

660
00:38:32,201 --> 00:38:37,201
informed because of the media celebrates
splashy findings even when they turn

661
00:38:38,261 --> 00:38:40,810
out to be bogus. And there are many
famous example of, for example,

662
00:38:40,811 --> 00:38:43,390
step is often talked about here,
um,

663
00:38:44,740 --> 00:38:47,860
and does not pursue with
the same sort of vigor,

664
00:38:47,861 --> 00:38:51,700
the sort of investigative journalists
love finding scandals at City Hall.

665
00:38:52,540 --> 00:38:53,500
But as you point out,

666
00:38:53,501 --> 00:38:56,700
there are these studies done in humans
with hundreds of millions of dollars and

667
00:38:56,710 --> 00:39:00,940
so on and so forth where there
was some really shady or shaky,

668
00:39:00,941 --> 00:39:05,941
let's say preclinical work beforehand
and this waste doesn't seem to be exposed

669
00:39:06,461 --> 00:39:07,540
to the same kind of vigor.

670
00:39:07,570 --> 00:39:10,150
So I'm sort of curious about
your response to those thoughts.

671
00:39:10,180 --> 00:39:13,740
Yeah, good. Good questions. And I
think first of all, uh, there are,

672
00:39:13,741 --> 00:39:17,290
I think if you look at actual cases of
outright fraud, the really bad actors,

673
00:39:17,291 --> 00:39:17,920
they're fairly,

674
00:39:17,920 --> 00:39:22,780
they're fairly small number of people who
are clearly just gaming the system who

675
00:39:22,781 --> 00:39:26,710
are, who are doing, who are
deliberately misleading and so on.

676
00:39:26,890 --> 00:39:31,330
I think the vast majority
of these problems are well
meaning scientists who, uh,

677
00:39:31,720 --> 00:39:36,610
who ended up taking shortcuts that they
shouldn't have the using too few mice or,

678
00:39:36,850 --> 00:39:40,180
or, or not really following
their procedures and so on.

679
00:39:40,181 --> 00:39:44,080
And so you can't say get rid of all
the bad apples. Cause many of you know,

680
00:39:44,081 --> 00:39:46,900
I have a story in the book about a
Nobel laureate scientist who made it,

681
00:39:46,901 --> 00:39:48,940
who has a paper that was wrong.
And she was like,

682
00:39:49,450 --> 00:39:53,710
I was a pressure to publish situation,
but you wouldn't want to tell her, sorry,

683
00:39:53,711 --> 00:39:58,570
you're out of science now. She, you know,
she was, has contributed a lot since then.

684
00:39:58,750 --> 00:40:02,080
So everyone can make these mistakes and
missteps in the end depending on the

685
00:40:02,081 --> 00:40:03,280
pressures and the circumstances.

686
00:40:03,281 --> 00:40:06,040
So it's not just a simply a matter
of weeding out the bad actors.

687
00:40:06,190 --> 00:40:09,480
It's changing the culture and telling
people maybe you should publish,

688
00:40:09,610 --> 00:40:11,950
you should be rewarded for
publishing less and making,

689
00:40:11,980 --> 00:40:15,250
taking more time to make sure what you do.
Publish is right. The pressure's right.

690
00:40:15,251 --> 00:40:19,210
Now actually there's a paper
that suggests that uh, the,

691
00:40:19,211 --> 00:40:23,050
the evolutionary pressures in science
are the favor of the labs that churn out

692
00:40:23,051 --> 00:40:26,860
the most stuff the fastest. And, and,
and, uh, and they replicate these, these,

693
00:40:26,890 --> 00:40:30,110
these labs then spin off, uh, you
know, their Grad students going,

694
00:40:30,170 --> 00:40:34,240
postdocs go on start labs that
use the same, you know, you know,

695
00:40:34,900 --> 00:40:39,130
fast matters most and in generate
large volumes of paper. So,

696
00:40:39,240 --> 00:40:44,020
so you can change the culture too to
discourage that without, you know,

697
00:40:44,050 --> 00:40:48,490
firing the people who are, who are, who
are doing things that you know, are, are,

698
00:40:48,940 --> 00:40:53,490
are clearly wrong, but they are
clearly not helpful either. So it's,

699
00:40:53,650 --> 00:40:56,260
it's a hard problem how,
how to get at it for sure.

700
00:40:58,760 --> 00:41:00,680
Thanks for coming today.
Um,

701
00:41:00,681 --> 00:41:05,300
so I know that in manufacturing
specifically there's a
large focus on quality

702
00:41:05,301 --> 00:41:10,070
control. Um, and again, healthcare
isn't my field personally.

703
00:41:10,130 --> 00:41:11,840
Um, so maybe there's an
obvious answer to this,

704
00:41:11,841 --> 00:41:14,900
but I'm curious to know if there's
anything to be learned or if there's any

705
00:41:14,901 --> 00:41:18,770
surveying that you did have the business
community on how they've improved

706
00:41:18,920 --> 00:41:19,820
quality control,

707
00:41:19,821 --> 00:41:23,240
whether it's actually assigning
quality control officers or empowering

708
00:41:23,241 --> 00:41:28,010
individuals to take a closer look at
quality. Um, are there any lessons there?

709
00:41:28,530 --> 00:41:31,230
There are for sure.
Cause I think industry,

710
00:41:31,470 --> 00:41:35,630
the drug industry and Pharma and so it
tends to do much better at these problems

711
00:41:35,631 --> 00:41:36,464
because they,

712
00:41:36,710 --> 00:41:39,500
their incentive structure is completely
different if they do something,

713
00:41:39,770 --> 00:41:43,250
if you're in, if you're in academia,
you're reward is getting a paper in a,

714
00:41:43,390 --> 00:41:45,810
you know, big name journals.
That's, that's your reward.

715
00:41:45,820 --> 00:41:47,690
It doesn't matter so much
if it's right or wrong.

716
00:41:47,930 --> 00:41:50,930
If you're at a drug company
that does it, that's nice, but,

717
00:41:51,020 --> 00:41:53,780
but what you really want to do
is have a product you can sell.

718
00:41:53,781 --> 00:41:57,740
And so the drug companies care a lot more
about making sure that the quality is

719
00:41:57,741 --> 00:41:58,430
high.

720
00:41:58,430 --> 00:42:01,550
And there are some people who argue that
you should take that degree of rigor

721
00:42:01,551 --> 00:42:03,170
that already exists in,

722
00:42:03,530 --> 00:42:08,330
in Pharma and tell people in academia
we expect you to follow something like

723
00:42:08,331 --> 00:42:10,970
this or have some similar
set of standards to do that.

724
00:42:11,240 --> 00:42:16,010
And it would slow them down.
It would, you know, uh, it's,

725
00:42:16,011 --> 00:42:19,810
it's a little bit more cumbersome, but
I think you get, you get better output,

726
00:42:19,860 --> 00:42:21,440
you get better results for doing that.

727
00:42:21,710 --> 00:42:26,360
And there's some universities that have
sort of found a middle ground, pardon me,

728
00:42:26,361 --> 00:42:30,200
they are, uh, letting the
academics be academics,

729
00:42:30,201 --> 00:42:32,660
but before the science goes forward,

730
00:42:32,840 --> 00:42:36,830
they then bring it into a lab where
we're s we're scientists who are sort of

731
00:42:36,831 --> 00:42:39,320
like, look at it from the standpoint
of drug development and say,

732
00:42:39,350 --> 00:42:41,660
let's see if we can
validate these results.

733
00:42:41,661 --> 00:42:45,770
If we can reproduce them before
we start sending them out, uh, to,

734
00:42:45,771 --> 00:42:47,480
to Pharma and so on.
So some,

735
00:42:47,481 --> 00:42:51,590
there are some fairly inexpensive ways
that universities can create these

736
00:42:51,591 --> 00:42:55,970
research units that help, uh,
sort of crank up the quality.

737
00:42:56,240 --> 00:43:00,080
But clearly, I mean, Pharma knows how
to do this, uh, but it's, you know,

738
00:43:00,081 --> 00:43:02,210
it's a different,
different world in academia.

739
00:43:04,430 --> 00:43:08,690
Hi. So earlier you mentioned how
a pipe paper can be published,

740
00:43:08,691 --> 00:43:12,370
it can make a big splash and they're
turning out to be problems with the paper.

741
00:43:12,380 --> 00:43:13,430
Maybe it's,
you know,

742
00:43:13,431 --> 00:43:18,260
its conclusions aren't as true as they
look and it can be very hard to find

743
00:43:18,290 --> 00:43:19,850
those sorts of,
the criticisms of it.

744
00:43:20,420 --> 00:43:25,420
What can a tool like Google scholar
do to make those sorts of things,

745
00:43:25,910 --> 00:43:28,190
those like those followup easier to find?

746
00:43:28,350 --> 00:43:31,940
Hmm. Um, a lie. If I would look at the,

747
00:43:31,960 --> 00:43:36,930
the pub pier example, because I think
that's a, I think that is a useful tool.

748
00:43:37,140 --> 00:43:39,700
Uh, just for, cause you have to,

749
00:43:39,710 --> 00:43:43,560
you rely on individuals who flagged a
paper and either have published something

750
00:43:43,561 --> 00:43:47,310
that's kind of contradictory or at least
published a comment someplace that says,

751
00:43:47,311 --> 00:43:50,250
I don't think this is right and
raises questions about it. Uh,

752
00:43:50,251 --> 00:43:52,770
I don't know how Google scholar could,

753
00:43:53,730 --> 00:43:58,290
could sort of index those, those cautions
in a, you know, in a fruitful way,

754
00:43:58,291 --> 00:44:02,460
but as opposed to just reporting it
842 citations. Well that doesn't,

755
00:44:02,461 --> 00:44:07,461
that is not as informative to me as
it would be 842 of which 30 strongly

756
00:44:08,251 --> 00:44:11,640
disagree and you know, and the
others we don't know about. So,

757
00:44:11,670 --> 00:44:16,670
so a tool like that that would
help not only say who is cited it,

758
00:44:17,251 --> 00:44:18,720
but what they have,
uh,

759
00:44:19,170 --> 00:44:22,440
what they have observed about those
papers in a general sense I think could be

760
00:44:22,441 --> 00:44:23,274
very useful.

761
00:44:23,800 --> 00:44:24,633
Okay.

762
00:44:25,660 --> 00:44:26,381
Thank you very much.

763
00:44:26,381 --> 00:44:29,590
I just had a quick question at looking
at these studies from a different angle.

764
00:44:29,591 --> 00:44:30,580
I know a lot of,

765
00:44:30,860 --> 00:44:35,190
we've talked about here are the tests
to try to find cures and what works and

766
00:44:35,191 --> 00:44:37,230
what doesn't work.
And the other side is,

767
00:44:37,290 --> 00:44:40,830
is the test to either prove what is
happening or what's causing these in the

768
00:44:40,831 --> 00:44:44,320
first place to find it, how to prevent
the disease from coming. Um, and,

769
00:44:44,321 --> 00:44:45,470
and I went through work,

770
00:44:45,540 --> 00:44:48,640
I guess going through a personal
scenario where I found out my,

771
00:44:48,660 --> 00:44:51,900
my house is gonna be on the corner of
a high voltage power line with two,

772
00:44:51,901 --> 00:44:52,860
I have two little kids in the house.

773
00:44:52,861 --> 00:44:56,610
So I dove into every single study on
EMS and the links to charter leukemia.

774
00:44:57,000 --> 00:44:57,950
And it started to,

775
00:44:58,080 --> 00:45:01,560
to see that anytime that you found a
promising study that showed this land corp

776
00:45:01,561 --> 00:45:02,730
even didn't show us link,

777
00:45:02,880 --> 00:45:06,300
it's very easy for the other sides are
poking holes in it because the sample

778
00:45:06,301 --> 00:45:09,150
size and,
and I'm just wondering from your opinion,

779
00:45:09,151 --> 00:45:12,210
how do you look at the other side
from like we started looking at the

780
00:45:12,211 --> 00:45:14,040
precautionary principle
and stuff like that,

781
00:45:14,041 --> 00:45:16,710
but to actually prevent
these when you have other,

782
00:45:17,040 --> 00:45:20,100
what they were calling hired scientists
to kind of poke holes in these studies

783
00:45:20,101 --> 00:45:22,920
that there very well may be
actual holes in the study,

784
00:45:23,100 --> 00:45:26,250
but then it's going to prevent us from
ever taking that step to actually prevent

785
00:45:26,251 --> 00:45:28,230
some of these diseases from taking form.

786
00:45:28,360 --> 00:45:30,070
Yeah.
And unfortunately those are all,

787
00:45:30,071 --> 00:45:32,530
most of those studies are
epidemiological studies,

788
00:45:32,531 --> 00:45:37,150
which are much harder to get to try to
ground truth because you have so many

789
00:45:37,151 --> 00:45:38,230
variables you're working with.

790
00:45:38,231 --> 00:45:41,800
It's not working in a laboratory
situation where you can reproduce these

791
00:45:41,801 --> 00:45:45,850
experiments and so on. So
yeah, that's uh, that's,

792
00:45:46,210 --> 00:45:47,140
I mean the quality,

793
00:45:47,141 --> 00:45:50,500
just the outright quality if to begin
with is fairly low on those. So a,

794
00:45:50,501 --> 00:45:55,390
and that confidence should be,
yeah, it should be low in those, uh,

795
00:45:56,080 --> 00:45:56,950
it's uh,

796
00:45:58,520 --> 00:45:59,220
yeah.

797
00:45:59,220 --> 00:46:02,970
How does, how to sort through that
and find the truth is, is tough.

798
00:46:03,000 --> 00:46:05,900
I mean the IOM in the case
of the EMS and power lines,

799
00:46:05,930 --> 00:46:08,610
there was a big national research council,

800
00:46:09,320 --> 00:46:13,920
a National Academy of Science's report
I think some years ago where they took,

801
00:46:13,950 --> 00:46:18,950
they took another run at these issues and
based on the first decade or two of us

802
00:46:19,820 --> 00:46:22,950
of studies on this and they said there
doesn't really seem to be anything here.

803
00:46:23,220 --> 00:46:23,880
Uh,

804
00:46:23,880 --> 00:46:27,750
and I think in that case we stepped back
and when we rely on expert opinion as

805
00:46:27,751 --> 00:46:31,920
opposed to a really expecting that
the science will yield us a completely

806
00:46:31,921 --> 00:46:36,150
concrete and believable answer, it's
not totally satisfactory, but, um,

807
00:46:36,420 --> 00:46:39,600
but eh, but I, that's, I think given the,

808
00:46:39,630 --> 00:46:43,140
given the realities of that kind of
science, I think that that's kind of what,

809
00:46:43,141 --> 00:46:45,990
where we're left with is what,
what is the expert opinion on this?

810
00:46:48,570 --> 00:46:49,403
Yeah.

811
00:46:49,840 --> 00:46:51,490
Concerning reproducibility.

812
00:46:52,620 --> 00:46:57,280
I have thought that reproducing results
is theoretically important for science

813
00:46:57,281 --> 00:46:57,611
to work,

814
00:46:57,611 --> 00:47:02,170
but unlikely to be actually done in
practice very much because I doubt that

815
00:47:02,171 --> 00:47:06,040
anyone gets a Nobel prize or even tenure
at the lower train switch community

816
00:47:06,041 --> 00:47:09,010
college for redoing something as
someone already didn't publish.

817
00:47:09,730 --> 00:47:13,240
You've told a few stories of people
actually trying to reproduce results.

818
00:47:13,241 --> 00:47:15,690
So clearly this happens
at least sometimes.

819
00:47:16,180 --> 00:47:20,470
Do you have any general observations
concerning the cases when it actually

820
00:47:20,471 --> 00:47:22,810
happens?
If someone tries to reproduce the result,

821
00:47:23,530 --> 00:47:25,000
what were the motivations typically?

822
00:47:25,001 --> 00:47:27,820
What's the distribution of reasons that
caused someone to actually bother to do

823
00:47:27,821 --> 00:47:30,220
this? Yeah, I think most common motivation

824
00:47:30,220 --> 00:47:32,500
is a,
if you're working in a,

825
00:47:32,710 --> 00:47:36,970
in a rival lab and you're a postdoc or
graduate student and your professor says,

826
00:47:37,240 --> 00:47:40,960
Hey, there's an exciting finding there,
see if you can build on that result,

827
00:47:41,380 --> 00:47:44,020
but before you build on it,
make sure it's correct.

828
00:47:44,140 --> 00:47:47,620
And so that's where I think a lot of
the reproducibility these attempts to

829
00:47:47,621 --> 00:47:50,980
reproduce happen is at the
bench. People saying, I want to,

830
00:47:50,981 --> 00:47:55,380
I want to take this on and I want
to see if it actually works. And uh,

831
00:47:55,650 --> 00:47:58,480
and so this people can waste,
you know,

832
00:47:58,481 --> 00:48:00,940
six months or a year of their
career chasing something that,

833
00:48:01,060 --> 00:48:04,690
that that doesn't pan out, but at least
they've done the attempt to reproduce.

834
00:48:04,691 --> 00:48:07,300
And sometimes that gets published
and people realize, oops,

835
00:48:07,420 --> 00:48:11,620
this didn't really work. And sometimes
it doesn't. I mean, for some big results,

836
00:48:11,621 --> 00:48:16,621
you were mentioning the report of be
at some amazing new way to produce stem

837
00:48:17,201 --> 00:48:21,240
cells. That was such a big finding that,
um, that everyone realized if that's a,

838
00:48:21,241 --> 00:48:26,050
if that's for real, that's an incredibly
important laboratory tools. So, you know,

839
00:48:26,051 --> 00:48:28,660
dozens and dozens of blabs
dived into that to see,

840
00:48:28,690 --> 00:48:33,430
can we make this work and can we
start using it ourselves? So, uh, so,

841
00:48:33,910 --> 00:48:37,660
so there are some very strong motivations
and they tend to be motivated to do

842
00:48:37,661 --> 00:48:39,590
those studies that are the
most interesting. So that,

843
00:48:39,600 --> 00:48:42,970
I'm sure there's tons of stuff in the
literature that somebody publishes.

844
00:48:43,210 --> 00:48:46,600
It just becomes a line on their CV and
that's it. Nobody ever looks at it again.

845
00:48:46,601 --> 00:48:49,030
And it's kind of just disappears into the,
uh,

846
00:48:49,350 --> 00:48:53,800
into the dust and nobody really notices.
And those are some ways,

847
00:48:53,801 --> 00:48:55,600
it doesn't matter if
those ones work or not,

848
00:48:55,720 --> 00:48:59,470
but the exciting and interesting finding
sis suggestions of a new direction and

849
00:48:59,471 --> 00:49:00,040
so on,

850
00:49:00,040 --> 00:49:04,510
those studies do get attention and people
do sit down and try to reproduce them.

851
00:49:04,750 --> 00:49:09,340
Sometimes it can take a long time though.
I've a story in my book, um, about, uh,

852
00:49:09,520 --> 00:49:14,360
example of this was a discovery
1999, 2000 called, uh,

853
00:49:14,380 --> 00:49:17,740
it was regarding a phenomena they
called trans differentiation,

854
00:49:17,741 --> 00:49:21,370
which was a couple of labs reported that
they discovered they could take blood

855
00:49:21,371 --> 00:49:23,230
cells, uh, uh,

856
00:49:23,380 --> 00:49:26,950
bone marrow cells and transform them into
any other kinds of cells they wanted,

857
00:49:26,951 --> 00:49:31,240
which was like sort of, that would be
incredibly powerful laboratory tool.

858
00:49:31,241 --> 00:49:33,490
And people took off on this idea of the,

859
00:49:33,770 --> 00:49:37,450
of trans differentiating these blood
cells, uh, to do it. And they, uh,

860
00:49:37,870 --> 00:49:41,380
and there were quite a number of studies
published before somebody at Stanford

861
00:49:41,381 --> 00:49:43,000
finally said,
wait a second,

862
00:49:43,600 --> 00:49:47,560
all these studies that are reporting on
it and people are doing this and so on.

863
00:49:47,561 --> 00:49:51,640
But no one has really done the careful
reproduction of this experiment or,

864
00:49:51,820 --> 00:49:55,270
or really tried to get to the bottom of
whether it's a really correct phenomenon.

865
00:49:55,540 --> 00:49:58,480
And they did a series of very
time consuming, elaborate,

866
00:49:59,020 --> 00:50:02,920
technically challenging and really careful
experiments and concluded the whole

867
00:50:02,921 --> 00:50:07,720
thing was, was a mirage. It was
actually not really happening. And uh,

868
00:50:08,110 --> 00:50:11,560
and there's a, and so, and this was,

869
00:50:11,561 --> 00:50:13,090
it only took about three
or four years I guess,

870
00:50:13,091 --> 00:50:15,160
before they published this paper and said,
guess what?

871
00:50:15,490 --> 00:50:18,520
The fluorescent marker you're seeing
means something completely different.

872
00:50:18,521 --> 00:50:22,150
It has nothing to do with the
cells changing cell types.
It's just moving around.

873
00:50:22,480 --> 00:50:26,830
And, uh, and the entire field sort
of disappeared like a soap bubble.

874
00:50:27,620 --> 00:50:31,100
But there were, there were many,
many studies that had been published,

875
00:50:31,400 --> 00:50:36,400
reporting seemingly validated in this
phenomenon before this lab at Stanford

876
00:50:36,621 --> 00:50:40,940
said, wait a second, no one's really
done the careful validation here it is.

877
00:50:41,030 --> 00:50:43,580
And guess what? It doesn't
work. So, so sometimes it,

878
00:50:43,610 --> 00:50:48,590
sometimes it takes a little while to
sort out. Uh, I just had one question.

879
00:50:48,591 --> 00:50:53,000
You mentioned you had looked at
medicine earlier on in your career.

880
00:50:53,001 --> 00:50:56,630
Like in the early two thousands,
was any of this stuff on your radar then?

881
00:50:57,800 --> 00:50:59,270
No, actually. And uh,

882
00:50:59,810 --> 00:51:03,680
I mean a lot of the problems in clinical
medicine with clinical trials and so on,

883
00:51:04,040 --> 00:51:04,191
uh,

884
00:51:04,191 --> 00:51:09,140
that had been regarded as significant
problems at that time got very little

885
00:51:09,141 --> 00:51:13,430
attention in the media. I think a lot of
this sort of happened below the radar.

886
00:51:13,610 --> 00:51:14,750
I mean,
I,

887
00:51:14,820 --> 00:51:17,870
there was maybe some of it written
up in some of the technical journals,

888
00:51:17,871 --> 00:51:20,300
but this was not an issue that really

889
00:51:21,820 --> 00:51:26,510
garner public attention. And,
uh, and I think some of these,

890
00:51:26,570 --> 00:51:30,380
these, these new stories were,
were surprising to people,

891
00:51:30,381 --> 00:51:33,530
but because the results were so
eye-popping and a lot of the,

892
00:51:33,830 --> 00:51:37,760
a lot of the clinical research stuff
was much more along the lines of, well,

893
00:51:37,761 --> 00:51:42,761
we know that the drug companies always
like to look at their results in the best

894
00:51:43,161 --> 00:51:45,230
possible light to try to
get their FDA approval.

895
00:51:45,231 --> 00:51:47,900
And so there's always this element of,
of,

896
00:51:48,260 --> 00:51:52,760
of knowledge that things are always being
interpreted in, in ways that are, uh,

897
00:51:53,060 --> 00:51:55,570
not necessarily just
completely straight forward.

898
00:51:55,640 --> 00:51:58,400
But I don't think there was
an awareness about how, uh,

899
00:51:58,430 --> 00:52:03,350
how much improvement was needed
in the world of clinical research,

900
00:52:03,351 --> 00:52:04,640
at least not in the lay press.

901
00:52:06,090 --> 00:52:06,940
All right.

902
00:52:07,810 --> 00:52:08,830
Thank you all for coming.

