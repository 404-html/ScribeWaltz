1
00:00:05,920 --> 00:00:08,470
Thank you very much. It's a pleasure
to be back here. Thank you for coming.

2
00:00:09,790 --> 00:00:13,930
Why don't you start by telling us
about about the book. I mean, they're,

3
00:00:13,931 --> 00:00:15,040
in some ways, my, um,

4
00:00:15,160 --> 00:00:20,160
my 2017 therapeutic writing
Trump mishegoss stories.

5
00:00:21,371 --> 00:00:25,720
They, I didn't plan any of
them. They, they, uh, all
just sort of blurted out. Uh,

6
00:00:25,990 --> 00:00:28,390
so unauthorized bread
is a story about people.

7
00:00:28,391 --> 00:00:32,080
I'm refugee housing where all the
appliances are designed to have DRM,

8
00:00:32,081 --> 00:00:37,080
to lock them into a vendor ecosystem so
you can only toast, authorize breading,

9
00:00:37,090 --> 00:00:40,750
your toaster. You can only
store authorized groceries
in your fridge and so on.

10
00:00:40,990 --> 00:00:45,070
And um, it, it, that is bad enough.

11
00:00:45,071 --> 00:00:49,510
But then the hedge fund that owns the
backend for all this stuff financially

12
00:00:49,511 --> 00:00:53,380
engineer's itself into bankruptcy
and so everything stops working.

13
00:00:53,680 --> 00:00:54,960
And so,
uh,

14
00:00:55,000 --> 00:00:58,750
they learn to jailbreak their appliances
out of necessity and then out of sheer

15
00:00:58,751 --> 00:01:01,300
joy of,
of seizing the means of information.

16
00:01:01,540 --> 00:01:05,110
But then they learned that the companies
are being rebooted out of bankruptcy

17
00:01:05,111 --> 00:01:08,830
and that very soon their telemetry is
going to detect that the devices were

18
00:01:08,850 --> 00:01:12,010
jailbroken and then they're going
to face the MCA criminal liability.

19
00:01:12,011 --> 00:01:14,470
And because they're
all in refugee housing,

20
00:01:14,740 --> 00:01:17,380
that will mean being
deported and possibly killed.

21
00:01:17,680 --> 00:01:22,680
And so it becomes this like very high
stakes fight where this woman who has

22
00:01:23,080 --> 00:01:25,030
created this kind of,
you know,

23
00:01:25,031 --> 00:01:29,470
youth brigade of kids who go around and
jail break everyone's devices now has to

24
00:01:29,471 --> 00:01:33,160
convince these kids who are completely
foursquare against it to go and restore

25
00:01:33,161 --> 00:01:36,280
everything to factory defaults
without scaring the pants off of them.

26
00:01:36,610 --> 00:01:38,950
They end up working with friendly texts
and trying to figure out if there's a

27
00:01:38,951 --> 00:01:42,580
way that they can use vms to cheat
the telemetry and so on. So it's a,

28
00:01:42,820 --> 00:01:46,810
it's a novel about, about
the kind of class dimension
and surveillance dimension,

29
00:01:47,050 --> 00:01:48,820
a vendor Lockin,
uh,

30
00:01:48,880 --> 00:01:52,420
and the way that that plays out depending
on what kind of privilege you have in

31
00:01:52,421 --> 00:01:53,710
the world.
Um,

32
00:01:53,740 --> 00:01:57,370
and then the next story is a story
called radicalize the title story.

33
00:01:58,000 --> 00:01:59,050
And it's about,
um,

34
00:01:59,230 --> 00:02:03,100
super entitled Middle Class White
Dudes who watched their loved ones die

35
00:02:03,101 --> 00:02:07,450
preventable illnesses because
their insurers won't cover
therapies and who find

36
00:02:07,451 --> 00:02:10,270
themselves on these darknet message boards
where they're radicalized into being

37
00:02:10,271 --> 00:02:12,610
suicide bombers who kill
healthcare executives.

38
00:02:13,030 --> 00:02:16,150
And in the end,

39
00:02:16,210 --> 00:02:19,450
a lot of it is about whether or not
America will ever call a white due to

40
00:02:19,451 --> 00:02:21,430
terrorist. Uh, and,

41
00:02:21,431 --> 00:02:26,120
and some of it is about how just the
cause actually turns out to be and, and,

42
00:02:26,170 --> 00:02:26,921
and so on.

43
00:02:26,921 --> 00:02:30,580
And also this very toxic observation
that many have made about the incell

44
00:02:30,670 --> 00:02:35,150
movement that, um, in normal support
message boards, you know, if you're,

45
00:02:35,230 --> 00:02:39,250
if you're a recovering alcoholic and
you're in a, an an online community,

46
00:02:39,550 --> 00:02:42,380
all the elder states,
people of that community are people who,

47
00:02:42,400 --> 00:02:45,650
who beat the thing you're struggling
with and are now staying alive,

48
00:02:45,670 --> 00:02:47,170
staying around to guide people.

49
00:02:47,320 --> 00:02:51,290
But people who recover from being in cells
don't hangout an insult message board.

50
00:02:51,291 --> 00:02:53,710
So all the elder states,
people have the incell message.

51
00:02:53,711 --> 00:02:58,540
Communities are the most
toxic, most broken people.
And they're the ones saying,

52
00:02:58,720 --> 00:03:02,230
yeah, a van, and drive it
through the streets and kill
as many people as you can.

53
00:03:02,231 --> 00:03:05,560
They deserve it. Right? And so these
communities don't get better over time.

54
00:03:05,561 --> 00:03:10,020
They get worse. And, and that's the kind
of community that I'm exploring. Um,

55
00:03:10,150 --> 00:03:12,190
the third story is called model minority.

56
00:03:12,191 --> 00:03:16,630
And it's about a thinly veiled analog
to superman intervening in a, um,

57
00:03:16,720 --> 00:03:21,380
a beating by the same cops who killed
Eric Garner on Staten Island, uh,

58
00:03:21,430 --> 00:03:26,290
who then discovers that the fact that
he's viewed as both a white and a human

59
00:03:26,500 --> 00:03:30,670
are super contingent and that there are
some things that America won't tolerate,

60
00:03:30,940 --> 00:03:35,120
uh, from the people who are
honorarily, white and human.
And it takes the form, uh,

61
00:03:35,260 --> 00:03:37,930
in large part of Socratic
dialogues with Bruce Wayne,

62
00:03:38,110 --> 00:03:42,350
who's a military contractor who provides
predictive policing software to, uh,

63
00:03:42,430 --> 00:03:46,600
the end YPD and who's in fact
basically responsible for all of this.

64
00:03:46,960 --> 00:03:50,380
And then the last story is a story called
the mask of the red death named after

65
00:03:50,381 --> 00:03:52,950
the Edgar Allan Poe
story. And it's about, um,

66
00:03:52,990 --> 00:03:57,990
preppers who build a luxury bunker just
outside of Phoenix and repair to it as

67
00:03:58,691 --> 00:04:03,340
soon as the, the catastrophe
hits and, um, uh, you know,

68
00:04:03,341 --> 00:04:07,720
fancy themselves living at a kind of
boy's own adventure of the elite surviving

69
00:04:07,721 --> 00:04:12,100
while all the useless takers a die in
Phoenix because they didn't have the

70
00:04:12,101 --> 00:04:14,020
wisdom to prepare these bolt holes,

71
00:04:14,230 --> 00:04:17,320
but who in fact ended up
dying of cholera because they,

72
00:04:17,410 --> 00:04:20,740
the useful people are the people who
stay behind in Phoenix and got the

73
00:04:20,830 --> 00:04:23,950
sanitation working again.
And these Uber mentioned have,

74
00:04:23,951 --> 00:04:27,790
have to discover that you
can't shoot germs. I was
originally going to be called,

75
00:04:27,791 --> 00:04:28,660
you can't shoot terms,

76
00:04:28,661 --> 00:04:31,300
but I think masque of the red death
has got the appropriate gravitas.

77
00:04:31,390 --> 00:04:34,260
So it's these four novellas, they're
all linked. They, they, they,

78
00:04:34,290 --> 00:04:36,460
they wrap around a lot of the same themes.

79
00:04:36,610 --> 00:04:40,930
They touch in a lot of the
same geographic locations. Um,

80
00:04:41,050 --> 00:04:43,120
some of the details Ricoeur writes.

81
00:04:43,121 --> 00:04:46,570
So some of the op sec that superman
uses to avoid being, having his,

82
00:04:46,600 --> 00:04:48,820
his secret identity added by the NSA,

83
00:04:48,970 --> 00:04:52,510
he's got like a randomizer that tells
him where to take off from so that you

84
00:04:52,511 --> 00:04:56,980
can't draw a map to see where superman is
always cited by, you know, ground radar.

85
00:04:57,250 --> 00:05:01,360
Uh, so he know he runs very quickly to
somewhere else and then takes off. Um,

86
00:05:01,390 --> 00:05:05,980
and this prepper has also got a randomizer
that makes him get in his disguise,

87
00:05:05,981 --> 00:05:10,530
armored up, uh, uh, like
F-150 with a camper bed, uh,

88
00:05:10,531 --> 00:05:14,170
that is full of all of his prepper gear
and put some fishing rods in the front

89
00:05:14,350 --> 00:05:19,350
and just drive it off of
his gated community at this
randomizer is interval so

90
00:05:19,361 --> 00:05:21,370
that there isn't someone
who goes, hey, that's weird.

91
00:05:21,370 --> 00:05:25,450
Why is that guy driving his F-150
out of this gate guard a community?

92
00:05:25,451 --> 00:05:27,490
Now he must be going to
his prepper hidey hole.

93
00:05:27,491 --> 00:05:30,490
We'll follow him there and kill him
and take his stuff. So you know,

94
00:05:30,491 --> 00:05:31,120
a lot of those,

95
00:05:31,120 --> 00:05:34,480
a lot of the same things kind of repeat
and repeat and the things wrapper and

96
00:05:34,481 --> 00:05:39,400
each other. It's, we're not using the
word collection, uh, for it. It's a,

97
00:05:39,430 --> 00:05:42,120
it's just a fiction
book. Um, and it's, uh,

98
00:05:42,230 --> 00:05:46,540
it's been likened by my publisher to
the book that Stephen King wrote. Um,

99
00:05:46,920 --> 00:05:48,400
where that,
that the body,

100
00:05:48,410 --> 00:05:51,400
the story that became a standby
me comes out of four seasons.

101
00:05:51,610 --> 00:05:56,610
That's just for thematically connected
but not continuity connected.

102
00:05:57,380 --> 00:05:58,340
Uh,
Tales.

103
00:05:58,850 --> 00:06:00,920
Had any of the story's
been published previously?

104
00:06:01,120 --> 00:06:06,030
No, this is all original stuff. Uh,
and as I say, wasn't planned. I,

105
00:06:06,090 --> 00:06:07,690
I, you know, the unauthorized bread,

106
00:06:07,691 --> 00:06:12,010
originally I sent it to my editor because
it was such a weird and awkward length

107
00:06:12,250 --> 00:06:14,860
that I didn't think, I didn't know
what to do with it when it was done.

108
00:06:14,861 --> 00:06:18,880
It's 30,000 words long. And I thought
that he was going to say, well,

109
00:06:18,881 --> 00:06:19,870
we don't know what to do with this.

110
00:06:19,871 --> 00:06:22,750
And instead he sent me an email
like the next day saying, oh my God,

111
00:06:22,751 --> 00:06:24,250
this is amazing and timely.

112
00:06:24,640 --> 00:06:28,150
We want to publish it in two months
as a standalone book. And I said,

113
00:06:28,151 --> 00:06:28,841
well that's great.

114
00:06:28,841 --> 00:06:31,600
Let's do that and we want to pay you
more than you got for your first three

115
00:06:31,601 --> 00:06:35,560
novels for, which was also great. And
then, and then I sent him the next one,

116
00:06:35,561 --> 00:06:38,110
I sent him the model minority. It
was like, this is also awesome.

117
00:06:38,111 --> 00:06:39,790
We'll do it the next month as
a standalone. And I'm like,

118
00:06:39,791 --> 00:06:40,990
I've got two more in the works.

119
00:06:41,170 --> 00:06:44,050
So that's how we came to weight
about seven months to do it and,

120
00:06:44,051 --> 00:06:47,160
and publish them all in one go topic.
Who are the,

121
00:06:47,190 --> 00:06:51,220
the TV and movie studio associated with
the same company that owns the intercept.

122
00:06:51,550 --> 00:06:54,110
A have bought the TV rights
for this inner developing and

123
00:06:54,170 --> 00:06:58,370
now, and you mentioned timeliness
and reading unauthorized bread.

124
00:06:58,580 --> 00:07:03,530
It seemed very much of the now and in
the book also struck me as being more

125
00:07:03,531 --> 00:07:05,360
political,
which is kind of a funny thing to say.

126
00:07:05,361 --> 00:07:08,370
Your work has always been political
in one way or another. Uh,

127
00:07:08,390 --> 00:07:10,790
freedom of access to information,
doing it yourself.

128
00:07:10,880 --> 00:07:14,840
But this one seems a little
more pointedly political.

129
00:07:14,960 --> 00:07:17,900
Two things have happened.
One is that politics would
be just become more salient,

130
00:07:17,960 --> 00:07:22,190
right? Like we are just in a moment
where you can't not talk about politics,

131
00:07:22,191 --> 00:07:23,600
you know,
for better or for worse.

132
00:07:23,840 --> 00:07:28,340
The other thing is that the political
dimension of information has now become

133
00:07:28,341 --> 00:07:30,050
much more obvious,
right?

134
00:07:30,051 --> 00:07:35,051
Like I don't know if any of you
work in like Ux or or or Ui design.

135
00:07:35,360 --> 00:07:39,290
There's this thing that happens with
Ui design UX where like when you start,

136
00:07:39,530 --> 00:07:43,490
you're trying to convey a bunch of ideas
that are really novel to the people who

137
00:07:43,491 --> 00:07:45,360
you're conveying them to write.
Like you know,

138
00:07:45,380 --> 00:07:49,340
if you think about it early gooeys like
just the idea that there is a file and

139
00:07:49,341 --> 00:07:51,560
then that a file is a thing that you save,
right?

140
00:07:51,561 --> 00:07:55,430
And that you have reversion and you have
control z and like all of those things

141
00:07:55,431 --> 00:07:56,450
are kind of novel ideas.

142
00:07:56,480 --> 00:08:00,550
Undo is not a thing that you just know
about if you've never used a computer and

143
00:08:00,551 --> 00:08:02,360
do as like it's a weird idea.

144
00:08:02,780 --> 00:08:06,680
And over time we got better at conveying
with those ideas mean but also the

145
00:08:06,681 --> 00:08:09,260
urgency of conveying them is diminished,
right?

146
00:08:09,261 --> 00:08:13,010
Because people are meeting you halfway
right now. Like just everybody,

147
00:08:13,011 --> 00:08:15,440
people who don't know what a
floppy disk is, knows that oh no,

148
00:08:15,441 --> 00:08:18,940
that a floppy disk means safe. Right?
And that just that icon like doesn't,

149
00:08:18,941 --> 00:08:21,470
it doesn't need a tool tip
anymore the way it used to.

150
00:08:21,740 --> 00:08:26,510
And in the same way like writing political
fiction about information doesn't

151
00:08:26,511 --> 00:08:28,100
require nearly so much handholding.

152
00:08:28,130 --> 00:08:30,290
You can just like jump
right in because you know,

153
00:08:30,710 --> 00:08:35,630
in an age of black lives matter
and mimetic warfare and um,

154
00:08:35,720 --> 00:08:39,220
uh, you know, questions about, um, uh,

155
00:08:39,380 --> 00:08:43,940
whether or not our power grid is
vulnerable to a cyber attack and so on,

156
00:08:44,180 --> 00:08:47,960
like talking about inflammation as being
this like important dimension to our

157
00:08:47,961 --> 00:08:51,140
politics is,
is not a radical idea anymore.

158
00:08:51,141 --> 00:08:53,590
And that means that you can dig
into like more nuance, right?

159
00:08:53,660 --> 00:08:56,340
Like more the can be centered

160
00:08:56,340 --> 00:09:00,390
more because the information dimension
is just obvious. That's interesting. It.

161
00:09:00,420 --> 00:09:05,040
It also made me remember that it struck
me while reading the story that seems

162
00:09:05,041 --> 00:09:09,120
slightly more educational than past
fiction has been there several sections.

163
00:09:09,600 --> 00:09:13,530
Uh, it's not overly heavy exposition
but you go pretty detailed into digital

164
00:09:13,531 --> 00:09:17,940
copyright kind of aside
how to jailbreak DRM. It,

165
00:09:17,960 --> 00:09:21,870
it seems like you were educating
through the story as well.

166
00:09:21,930 --> 00:09:23,550
So I think there's two dimensions to that.

167
00:09:23,551 --> 00:09:28,080
One is like I think people are tolerant
of exposition when it's news you can use,

168
00:09:28,081 --> 00:09:28,321
right?

169
00:09:28,321 --> 00:09:31,530
Like oh that's how that works now a
whole bunch of stuff in my life suddenly

170
00:09:31,531 --> 00:09:33,480
makes sense.
Like here's what a VM is or whatever.

171
00:09:33,720 --> 00:09:38,700
The other thing though is that there
is a longevity that comes from writing

172
00:09:38,701 --> 00:09:40,710
fiction where the technology is rigorous.

173
00:09:40,950 --> 00:09:45,180
Like we've had a certain amount of
technological breakthrough in all of our

174
00:09:45,270 --> 00:09:46,350
careers and lives,

175
00:09:46,560 --> 00:09:50,790
but there are some like fundamentals
that remain pretty fundamental like you

176
00:09:50,791 --> 00:09:51,151
know,

177
00:09:51,151 --> 00:09:54,300
complexity and [inaudible]
and our understanding of
complexity theory and things

178
00:09:54,301 --> 00:09:58,350
at scale and order and squared and things
that scale in polynomial time and so

179
00:09:58,351 --> 00:10:01,710
on. Like those are things that
are just semi immutable. Right.

180
00:10:01,711 --> 00:10:04,470
And so like talking about the kind of
problems we can solve and the kind of

181
00:10:04,471 --> 00:10:07,800
problems we can't solve.
If you make the plot turn on them,

182
00:10:08,040 --> 00:10:11,100
the plot doesn't get old in
the same way that it does.

183
00:10:11,250 --> 00:10:14,150
If the plot turns on how much
Ram your phone has, right?

184
00:10:14,190 --> 00:10:18,720
Like that's a moving target. But like
Turing completeness, not a moving target,

185
00:10:18,721 --> 00:10:18,901
right?

186
00:10:18,901 --> 00:10:21,900
Like writing about the fact that if you
try to design a computer that can run

187
00:10:21,901 --> 00:10:26,010
all the programs except one you are trying
to do something that runs counter to

188
00:10:26,011 --> 00:10:30,960
the really the only functional widespread
architecture we have for computers,

189
00:10:30,961 --> 00:10:35,230
which is the one that runs all the
programs. And so there will, uh,

190
00:10:35,340 --> 00:10:40,340
it will always be this like weird
mashed up Rube Goldberg under the hood.

191
00:10:40,980 --> 00:10:44,370
If you've got a computer that doesn't,
that, that runs everything except one.

192
00:10:44,370 --> 00:10:46,950
And it's always going to have like
certain recurring characteristics, right?

193
00:10:46,951 --> 00:10:51,951
Like if you design a computer that has
a mode that sits below super user mode,

194
00:10:52,951 --> 00:10:53,191
right?

195
00:10:53,191 --> 00:10:56,670
Like a ring minus one where programs
execute that even the administrator of the

196
00:10:56,671 --> 00:10:58,920
computer isn't supposed to be
able to inspect or terminate,

197
00:10:59,460 --> 00:11:02,940
then that will always be a zone that if
someone who is an attacker gets access

198
00:11:02,941 --> 00:11:07,380
to it, that they'll be able to operate
with total impunity and undetectable.

199
00:11:07,381 --> 00:11:10,320
He against you because it's
a computer design not to,

200
00:11:10,560 --> 00:11:15,560
not to introspect about processes that
are running in some ways and so like this

201
00:11:15,601 --> 00:11:16,650
just keeps coming up,
right?

202
00:11:16,651 --> 00:11:20,340
We keep designing computers that have
as part of their security model, oh,

203
00:11:20,341 --> 00:11:21,810
there are programs that the user,

204
00:11:21,840 --> 00:11:24,180
even if the user is the
administrator and owns the device,

205
00:11:24,330 --> 00:11:28,500
can't terminate her inspect and inevitably
someone who's a bad actor starts

206
00:11:28,501 --> 00:11:29,220
running code in it.

207
00:11:29,220 --> 00:11:34,220
Like maybe the politburo orders apple
to run processes that seek out and

208
00:11:34,861 --> 00:11:39,720
terminate VPNs that it can't
spy on for Ios devices in China,

209
00:11:39,721 --> 00:11:43,200
which is like a thing that happened,
right? Like the, you know, this is,

210
00:11:43,201 --> 00:11:47,700
this is the world's most predictable
outcome of putting that gun on the mantle

211
00:11:47,701 --> 00:11:48,810
piece in act one.

212
00:11:49,200 --> 00:11:52,200
But we're going to continue to put that
gun on the mantle piece in act one.

213
00:11:52,201 --> 00:11:56,110
And so if you write science fiction that
turns on the actual characteristics of

214
00:11:56,111 --> 00:11:59,800
computers like our theoretical
understanding computers
that science fiction

215
00:11:59,801 --> 00:12:04,750
remains futuristic for so long as we
continue to make dumb policy choices about

216
00:12:04,751 --> 00:12:06,880
computers, which you know, uh,

217
00:12:06,940 --> 00:12:11,940
to to my great dismay probably means
that the fiction will remain current in

218
00:12:12,221 --> 00:12:16,560
some way forever, right? Like little
brother is still being taught and read it.

219
00:12:16,660 --> 00:12:21,660
I wrote it 13 years ago now and like the
question of whether or not subjecting

220
00:12:21,731 --> 00:12:25,030
whole populations to surveillance are
using machine learning to make inferences

221
00:12:25,031 --> 00:12:26,250
about guilt or,

222
00:12:26,251 --> 00:12:30,820
or any of these other things creates
a bunch of path pathologies. Those,

223
00:12:30,850 --> 00:12:35,290
those facts are like still totally
in evidence. They show no sign of,

224
00:12:35,310 --> 00:12:39,700
of going away. We have totally
failed to learn the lessons of them.

225
00:12:39,701 --> 00:12:44,650
The stakes only get higher and every
year there's another news hook that that

226
00:12:44,680 --> 00:12:47,890
makes people read little brother.
And then come back to me and say, oh,

227
00:12:47,891 --> 00:12:50,270
this is just like that
thing that you and you know,

228
00:12:50,320 --> 00:12:53,350
how did you anticipate 13 years later
that we'd be doing this? And I'm like,

229
00:12:53,351 --> 00:12:56,410
I didn't, we were doing it
13 years ago. We, we just,

230
00:12:56,560 --> 00:13:00,310
we have like our literally still
beating our heads against that wall.

231
00:13:00,311 --> 00:13:02,890
We will are figuratively still beating
our heads against that wall and we will

232
00:13:02,891 --> 00:13:03,910
never stop.

233
00:13:04,240 --> 00:13:07,960
And so little brother will remain current
and relevant for so long as we are

234
00:13:07,961 --> 00:13:11,620
stupid about technology policy.
You mentioned the word route.

235
00:13:11,621 --> 00:13:13,930
What was the initial idea for the story?

236
00:13:13,930 --> 00:13:17,320
Like did it start with the phrase
unauthorized bread or did it come from

237
00:13:17,321 --> 00:13:20,260
somewhere else? Yeah, it actually,
it started with a short story.

238
00:13:20,261 --> 00:13:23,230
I wrote it as part of my guardian column,

239
00:13:23,231 --> 00:13:27,220
so I'd been arguing for a long time
with people about Ios and the APP store

240
00:13:27,221 --> 00:13:31,630
business model. And uh, there was this,

241
00:13:31,631 --> 00:13:34,720
it just works.
And I trust them element to,

242
00:13:34,750 --> 00:13:38,180
to people saying I want to work
within an ecosystem. And I,

243
00:13:38,240 --> 00:13:42,070
I d I d I trust apple and I don't care,
uh,

244
00:13:42,071 --> 00:13:45,820
if they have made choices about what I
can and can't use because I think those

245
00:13:45,821 --> 00:13:49,120
choices are good ones. They're good
proxies for my interest. And I would,

246
00:13:49,150 --> 00:13:54,150
and I was trying to tease
out the difference between
having a checkbox in your

247
00:13:54,980 --> 00:13:56,260
[inaudible] that says,

248
00:13:56,710 --> 00:14:00,790
I would like to do something that the
manufacturer has an approved and having

249
00:14:01,060 --> 00:14:06,060
legal liability attached to figuring out
how to reconfigure eos to do something

250
00:14:06,850 --> 00:14:11,850
that that hasn't been approved and
what kind of bad things crop up if it's

251
00:14:12,431 --> 00:14:14,870
actually a felony to modify your oh,

252
00:14:14,990 --> 00:14:16,960
to do things at the
manufacturer doesn't like.

253
00:14:17,230 --> 00:14:21,130
And so I wrote a little story called if
dishwashers where I phones and it was in

254
00:14:21,131 --> 00:14:25,840
the form of an open letter from a,
from a Steve Jobs,

255
00:14:25,841 --> 00:14:30,841
the and CEO of a next generation Iot
dishwasher company called disher who make

256
00:14:31,181 --> 00:14:32,440
an appearance in this story.

257
00:14:32,740 --> 00:14:37,690
And he was explaining that like foodborne
illness has killed more people than

258
00:14:37,720 --> 00:14:39,910
any other killer in human history.

259
00:14:40,090 --> 00:14:45,090
And how can you expect your dishwasher
to be truly effective at keeping you safe

260
00:14:45,550 --> 00:14:49,540
and making your dishes clean with the
least amount of water in an age of scarce

261
00:14:49,541 --> 00:14:50,374
resources.

262
00:14:50,560 --> 00:14:54,590
If you are able to put any old dish
that you want in your dishwasher.

263
00:14:55,100 --> 00:14:59,660
And that is why you shouldn't be bending
the prongs of your dishwasher or trying

264
00:14:59,661 --> 00:15:03,800
to take the RFIDs out of the dishes that
you bought from the disher store and

265
00:15:03,801 --> 00:15:06,260
putting them in grandma's China and so on.

266
00:15:06,260 --> 00:15:10,190
Because like you could buy a different
dishwasher if you wanted that. And,

267
00:15:10,220 --> 00:15:11,770
and if you, uh, and,

268
00:15:11,780 --> 00:15:15,860
and the deal that you got when you
bought this dishwasher was that you would

269
00:15:15,861 --> 00:15:18,220
only wash dishwashers
from the kitchen store.

270
00:15:18,410 --> 00:15:21,890
And the kitchen store works only with
licensed partners who make high quality

271
00:15:21,891 --> 00:15:23,480
gear and they, they, you know,

272
00:15:23,620 --> 00:15:28,080
anyone who wants can buy a $99 kitchen
store license and they can, they provided,

273
00:15:28,100 --> 00:15:29,960
they comply with the terms of service.

274
00:15:29,961 --> 00:15:32,840
They can make pottery that you
can put in your dishwasher,

275
00:15:32,841 --> 00:15:33,740
but it's not dishwasher.

276
00:15:33,770 --> 00:15:37,730
The fact that it says dishwasher safe
doesn't mean it's dishwasher safe, right?

277
00:15:37,731 --> 00:15:37,941
They,

278
00:15:37,941 --> 00:15:41,060
it's only when they're in our developer
program that we can tell you that you

279
00:15:41,061 --> 00:15:44,090
truly won't die of Listeria. Uh, if you,

280
00:15:44,091 --> 00:15:48,590
if you use their dishes and like
these are all like taken individually,

281
00:15:48,591 --> 00:15:50,120
not unreasonable statements,

282
00:15:50,690 --> 00:15:54,560
but they gang up to inkjet
printers for everything, right?

283
00:15:54,590 --> 00:15:58,580
Everything is tied into some ecosystem
where you can't, you, you know,

284
00:15:58,670 --> 00:16:03,230
it's not that you, you do trust
them, it's that you must trust them.

285
00:16:03,470 --> 00:16:07,730
And if they ever make a decision that
you don't like, um, you are out of luck.

286
00:16:07,760 --> 00:16:12,560
And the longer you go inside the
ecosystem, the more sunk costs you have.

287
00:16:12,920 --> 00:16:16,460
And the harder it is the, you know,
the more you have to give away,

288
00:16:16,570 --> 00:16:20,810
the more the switching cost is. If you
decide that you no longer trust them.

289
00:16:20,811 --> 00:16:24,920
And so you end up being beholden to a
series of commercial decisions being made

290
00:16:24,921 --> 00:16:26,930
in board rooms that you
have no insight into.

291
00:16:27,140 --> 00:16:30,020
And you just have to trust that no one
in the firm will ever do anything that

292
00:16:30,021 --> 00:16:33,450
runs counter to your interests.
And you know, like, I'm not, uh,

293
00:16:33,500 --> 00:16:35,000
on anti-apple person.

294
00:16:35,001 --> 00:16:39,410
I actually have a sad mac tattooed on my
arm from when I used to be a CIO and I

295
00:16:39,440 --> 00:16:43,550
used to order $1 million worth of
APP apple gear a year. But you know,

296
00:16:43,610 --> 00:16:47,510
as someone who's bought a fair number of
Apple Lemons and written pos for a fair

297
00:16:47,511 --> 00:16:47,730
number,

298
00:16:47,730 --> 00:16:51,470
a low number of apple lemons over the
years and seeing how the company can be

299
00:16:51,471 --> 00:16:54,440
wildly imperfect and will
be wildly and perfect again,

300
00:16:54,680 --> 00:16:58,790
I think the idea that you are trusting
them in this way that requires that you

301
00:16:58,791 --> 00:17:03,740
trust them and that you can't transfer
your trust out actually invites future

302
00:17:03,741 --> 00:17:08,741
leaders of the firm to make choices
because in the knowledge that if they,

303
00:17:09,051 --> 00:17:11,720
that they can be, uh, uh,

304
00:17:11,780 --> 00:17:15,650
they can betray your trust and you're
not gonna be able to leave, right.

305
00:17:15,651 --> 00:17:17,150
That you'll be, that, that they,

306
00:17:17,180 --> 00:17:20,750
they've got a lot of headroom in terms
of betrayed trust before you get to the

307
00:17:20,751 --> 00:17:24,920
point where people are going to give up
a whole ecosystem of devices and replace

308
00:17:24,921 --> 00:17:27,860
all of it. And certainly in
some customers may be never.

309
00:17:28,160 --> 00:17:31,760
And I saw this not being looked on
with horror by the rest of the world,

310
00:17:31,761 --> 00:17:35,780
but being looked on as kind of a,
uh, an excellent idea. So you know,

311
00:17:35,781 --> 00:17:36,830
you have for example,

312
00:17:36,831 --> 00:17:40,910
Johnson and Johnson getting approval for
an artificial pancreas. This is a uh,

313
00:17:41,030 --> 00:17:44,530
closed loop, a continuous
glucose monitor and a, an um,

314
00:17:44,710 --> 00:17:49,710
an insulin pump with some machine learning
to try and time the insulin dosing to

315
00:17:50,070 --> 00:17:52,770
keep your blood sugar within a safe range.
Um,

316
00:17:52,860 --> 00:17:57,060
and it uses proprietary insulin
cartridges, right? And you know,

317
00:17:57,350 --> 00:18:02,350
it making a tool to refill
those cartridges is a
potential DMC a violation with

318
00:18:02,840 --> 00:18:07,840
a $500,000 fine and a five year prison
sentence and revealing a defect in it if

319
00:18:09,241 --> 00:18:14,241
that defect would help someone bypass
the DRM is also a potential felony under

320
00:18:15,511 --> 00:18:16,141
the DMCA.

321
00:18:16,141 --> 00:18:20,340
And one of the things we know is that
security researchers are killed from

322
00:18:20,341 --> 00:18:23,250
coming forward with reports
of defects when they just,

323
00:18:23,251 --> 00:18:27,750
when there's a potential DMTA overlap
or computer fraud and abuse act overlap.

324
00:18:28,050 --> 00:18:33,050
And so now you have this spreading
attack surface of devices that because

325
00:18:33,091 --> 00:18:36,720
they're designed to be extractive of
their users are also off limits to

326
00:18:36,721 --> 00:18:40,560
independent scrutiny
and they are much more,

327
00:18:40,590 --> 00:18:45,090
they're more and more intimately
connected to our bodies. And so, you know,

328
00:18:45,091 --> 00:18:49,350
your car is a robot you put your body
into that then goes down the road at 60

329
00:18:49,351 --> 00:18:52,590
miles an hour or if it's like the five
that goes out into five miles an hour.

330
00:18:52,800 --> 00:18:55,410
But you know, it's still like, you know,

331
00:18:55,411 --> 00:18:58,650
and I'm not talking about a self driving
car, I'm just talking about like a car,

332
00:18:58,680 --> 00:19:02,370
right? Because you take all
the informatics out of a
car and it becomes a nerd,

333
00:19:02,660 --> 00:19:05,760
right? The most salient feature
of that car is it's informatics.

334
00:19:05,970 --> 00:19:08,720
Compromising those informatics allows the,

335
00:19:08,721 --> 00:19:13,721
the a compromiser to wreak havoc on the
person whose body is trapped inside this

336
00:19:15,270 --> 00:19:18,840
fast moving robot and on
the people around it. Uh,

337
00:19:18,841 --> 00:19:23,841
and so I wanted to illustrate this idea
that as our property interest in the

338
00:19:25,141 --> 00:19:29,310
things that we own is being eroded
and as our ability to independently

339
00:19:29,311 --> 00:19:30,870
scrutinize them are being eroded,

340
00:19:31,170 --> 00:19:36,170
that we are also magnifying all of the
imbalances and inequalities in our world

341
00:19:37,320 --> 00:19:41,250
and that it happens first to the
poorest and least powerful among us.

342
00:19:41,370 --> 00:19:43,500
But it spreads to everybody,
right? You know, the,

343
00:19:43,501 --> 00:19:48,420
the user adoption curve for controlling
technology is like refugees,

344
00:19:48,480 --> 00:19:53,370
prisoners, children, poor
people, uh, blue collar workers,

345
00:19:53,400 --> 00:19:57,390
white collar workers, right? Like that's
the, that's the adoption curve. And like,

346
00:19:57,540 --> 00:20:00,720
if you want to know what your life
is going to look like in 20 years,

347
00:20:01,170 --> 00:20:04,110
just look at what we're
doing to refugees today.

348
00:20:04,380 --> 00:20:08,790
And that's the technology that people
will expect you to use. You know,

349
00:20:08,791 --> 00:20:11,820
when people didn't have
CCTVS recording them,

350
00:20:12,690 --> 00:20:15,300
non consensually operated
by third parties,

351
00:20:15,810 --> 00:20:20,810
unless they were in prison
in our lifetime right now,

352
00:20:21,091 --> 00:20:22,470
it's ubiquitous.

353
00:20:22,860 --> 00:20:27,720
Illustrating that and using this specific
group of people to illustrate it, uh,

354
00:20:27,750 --> 00:20:32,610
was, was an intervention in that.
A way to try and interrupt that.

355
00:20:33,280 --> 00:20:36,260
Well, let's dig a little deeper into
that idea of inequality. It's a,

356
00:20:36,261 --> 00:20:38,010
a major part of this story.

357
00:20:38,460 --> 00:20:42,470
The main character Selema it's
an immigrant or refugee. Uh,

358
00:20:42,480 --> 00:20:47,320
the people that surround her are
largely poor and underclass. Um,

359
00:20:48,160 --> 00:20:50,830
what do you think that brings the work?

360
00:20:51,760 --> 00:20:55,780
One of the most salient facts
of our moment is inequality.
Uh, in part because it,

361
00:20:55,781 --> 00:20:58,780
it breaks apart the story
that we've told about markets,

362
00:20:59,140 --> 00:21:02,650
which is that markets are dying,

363
00:21:02,710 --> 00:21:06,760
odd dynamic way of finding people
whose ideas would create more general

364
00:21:06,761 --> 00:21:10,480
prosperity and allocating capital
to them so that they can do it.

365
00:21:11,050 --> 00:21:15,910
And when you see widening inequality
and stagnation in our, um,

366
00:21:16,300 --> 00:21:18,040
social relations,

367
00:21:19,000 --> 00:21:21,670
you're left with either one
of two conclusions, right?

368
00:21:21,700 --> 00:21:26,700
Either markets aren't working the way
they're supposed to or eugenics is real.

369
00:21:28,270 --> 00:21:31,330
And there is a 1% of people who are
so smart that they should just own

370
00:21:31,331 --> 00:21:34,060
everything. And if we would
just give them all the stuff,

371
00:21:34,061 --> 00:21:37,870
they will allocate capital so wisely
that we will get richer and richer and

372
00:21:37,871 --> 00:21:41,800
richer. And you know, the
history of antitrust is full
of counter examples, right?

373
00:21:41,800 --> 00:21:45,670
Like when we broke up the phone
company into six companies,

374
00:21:46,420 --> 00:21:51,130
they got bigger and aggregate than they
were as one from right when they broke

375
00:21:51,131 --> 00:21:54,760
up the railroad and to, you know, the
railroad monopoly institute companies,

376
00:21:54,970 --> 00:21:59,230
each one within a few years was as big
as the parent company had been. You know,

377
00:21:59,231 --> 00:22:03,460
the disaffection of scale are,
are actually pretty well understood.

378
00:22:03,820 --> 00:22:06,280
But you know,
as Upton Sinclair said,

379
00:22:06,281 --> 00:22:08,920
it's impossible to get someone to
understand something when their paycheck

380
00:22:08,921 --> 00:22:10,510
depends on them not understanding it.

381
00:22:10,810 --> 00:22:15,520
And if you are someone who benefits
from the concentration of wealth and a

382
00:22:15,521 --> 00:22:17,380
single firm,
uh,

383
00:22:17,410 --> 00:22:20,890
the fact that we as a society
will benefit more if that firm,

384
00:22:20,891 --> 00:22:24,830
we're broken up into smaller pieces and
each of them was allowed to compete and,

385
00:22:24,880 --> 00:22:29,860
and grow is not very relevant to you
because it means that you get a lot fewer

386
00:22:29,861 --> 00:22:33,260
ivory handled back scratchers. And so, um,

387
00:22:34,210 --> 00:22:38,530
I think that like one of the corrosive
effects of inequality is that it drives

388
00:22:38,650 --> 00:22:42,580
rich people into believing that
they have good blood, right.

389
00:22:42,610 --> 00:22:44,920
And to believing in eugenics.
Um,

390
00:22:44,950 --> 00:22:46,930
you actually heard this in
the last election cycle.

391
00:22:46,931 --> 00:22:51,270
Trump repeatedly talked about his good
blood, right? It's like, it's like,

392
00:22:51,340 --> 00:22:52,960
it is like, uh, uh, for,

393
00:22:52,961 --> 00:22:57,700
for people who remember the
horrors of the Holocaust or,

394
00:22:57,970 --> 00:23:02,110
uh, who've, who are Tuskeegee or,
you know, the eugenics movement,

395
00:23:02,710 --> 00:23:06,880
good blood is like a, it's a, it's a
scary thing to hear someone talk about,

396
00:23:07,390 --> 00:23:10,080
right? Uh, it is like, it is, um,

397
00:23:10,680 --> 00:23:12,550
it's like the smoke from a,

398
00:23:12,610 --> 00:23:17,560
from a horrible fire that is smoldering.
Uh, and I think that, you know,

399
00:23:17,590 --> 00:23:19,450
we're,
we're blowing on those coals.

400
00:23:19,690 --> 00:23:23,950
And so by writing about people who have,
uh,

401
00:23:23,980 --> 00:23:26,620
who would be really efficient
capital allocators, right,

402
00:23:26,621 --> 00:23:28,660
who are doing really interesting,
dynamic,

403
00:23:28,661 --> 00:23:32,170
exciting things that benefit other people,
but who have,

404
00:23:32,260 --> 00:23:33,580
who are denied access to capital,

405
00:23:33,581 --> 00:23:38,080
one of the things that you do is you start
to change the narrative we have about,

406
00:23:38,110 --> 00:23:43,110
about whether capital
allocation as efficient and
whether inequality means that

407
00:23:43,511 --> 00:23:46,400
we are actually, um, just

408
00:23:46,580 --> 00:23:51,580
taking random lottery winners and
heaping enormous riches on them to the

409
00:23:51,831 --> 00:23:55,340
detriment of their soul and to
the detriment of our society.

410
00:23:55,560 --> 00:23:59,300
Your Twitter handle currently
is son of an asylum seeker,

411
00:23:59,540 --> 00:24:01,070
father of an immigrant.
Yeah.

412
00:24:01,220 --> 00:24:05,390
Do you feel a personal connection to
that experience? I do. You know, I, um,

413
00:24:05,510 --> 00:24:07,700
I lived in England until three years ago,
so I was,

414
00:24:07,701 --> 00:24:11,750
I was in the UK until in the
run up to Brexit. And, uh,

415
00:24:11,770 --> 00:24:16,640
I would end up in a lot of cabs and
lung cab drivers are a notoriously right

416
00:24:16,641 --> 00:24:20,530
wing and notoriously Gabby.
And given that the,

417
00:24:20,590 --> 00:24:25,590
the impending crisis was about migration
in this debate was going on about

418
00:24:26,121 --> 00:24:29,450
migration,
like at least on a weekly basis,

419
00:24:29,451 --> 00:24:31,970
someone would start talking to me
about asylum seekers and migrants.

420
00:24:31,990 --> 00:24:35,570
And my dad was born when his parents were
living in a displaced persons camp in

421
00:24:35,580 --> 00:24:38,300
Azerbaijan. They were
Red Army deserters. Um,

422
00:24:38,330 --> 00:24:40,970
my grandmother had been a child
soldier in the siege of Leningrad.

423
00:24:41,330 --> 00:24:45,320
They came to Canada as asylum seekers.
And then I'm an immigrant, right?

424
00:24:45,321 --> 00:24:47,990
I'm a Canadian who then lived in the
United Kingdom and now I live here on my

425
00:24:47,991 --> 00:24:51,730
daughter is an immigrant.
And, and I found that, like,

426
00:24:51,770 --> 00:24:56,480
it gave me a place, a way to enter that
conversation. Right. And say like, I am,

427
00:24:56,810 --> 00:25:00,980
that's who I'm, I'm the person you talking
about. Right. I, that's, that's me. Uh,

428
00:25:00,981 --> 00:25:02,330
one generation that it, they'd say, oh,

429
00:25:02,331 --> 00:25:05,760
but you're the right kind of your
high skill to your, you know,

430
00:25:05,800 --> 00:25:08,540
whatever your white, uh,
and I would say like,

431
00:25:08,541 --> 00:25:10,640
so my grandparents were not high skills,
right?

432
00:25:10,641 --> 00:25:12,950
My grandmother stopped her
formal schooling at 12.

433
00:25:12,951 --> 00:25:15,220
My grandfather stopped his at 14.
Um,

434
00:25:15,470 --> 00:25:17,390
they came to Canada
without marketable skills.

435
00:25:17,391 --> 00:25:22,190
They learned skills when they got
there. And this is the, and you know,

436
00:25:22,460 --> 00:25:24,700
they had to display an
awful lot of plucking,

437
00:25:24,740 --> 00:25:28,790
get up and go to get from Azerbaijan
to Canada. Right. Like that,

438
00:25:28,791 --> 00:25:32,780
that they have the winnowing function
they went through was not passing a

439
00:25:32,781 --> 00:25:36,200
standardized test or
displaying a credential.

440
00:25:36,620 --> 00:25:40,380
It was crossing Europe.
Right. Like that is, uh, that,

441
00:25:40,430 --> 00:25:43,070
that is an awful lot of gumption.
Right?

442
00:25:43,100 --> 00:25:46,280
By the time you get to the port of
Hamburg and you present yourself to a

443
00:25:46,281 --> 00:25:49,070
Canadian immigration official,
you are like,

444
00:25:49,280 --> 00:25:54,110
you have already demonstrated an
enormous amount of pluck. Uh, and,

445
00:25:54,140 --> 00:25:56,840
you know, they were traumatized and they
had lots of problems in their lives,

446
00:25:56,841 --> 00:25:58,010
but they were also,

447
00:25:58,280 --> 00:26:03,050
they were also people who came to
contribute and whose families contributed

448
00:26:03,051 --> 00:26:04,130
through the generations.

449
00:26:04,580 --> 00:26:09,580
And so as the debate about migration and
asylum seekers and immigrants took off

450
00:26:09,861 --> 00:26:14,360
here, I felt like, uh, you
know, my passing privilege,

451
00:26:14,840 --> 00:26:18,770
I have an accent that sounds
like it could be from America.

452
00:26:18,771 --> 00:26:21,470
I have a skin tone that makes
me look like I could be white,

453
00:26:21,471 --> 00:26:23,810
even though the people I come from,

454
00:26:23,811 --> 00:26:26,880
we're not thought of as white when they
got to Canada. They certainly are, are,

455
00:26:26,940 --> 00:26:31,250
are white now. Uh, and, um, and,

456
00:26:31,280 --> 00:26:32,480
and so you could,

457
00:26:32,510 --> 00:26:36,080
you could think that you were talking
to someone who wasn't an asylum seeker,

458
00:26:36,110 --> 00:26:40,400
who wasn't, uh, the, uh, the father
of a migrant, you know, as they say,

459
00:26:40,401 --> 00:26:41,930
Canadians are like serial killers.

460
00:26:41,931 --> 00:26:44,250
They're everywhere and they
look just like everybody else.

461
00:26:44,660 --> 00:26:47,370
Uh, I wanted to wear
it on my sleeve, right.

462
00:26:47,460 --> 00:26:52,460
To make people confront that they were
among people who were the kind of people

463
00:26:53,281 --> 00:26:55,680
they were demonizing.
In the beginning of the talk,

464
00:26:55,830 --> 00:26:59,860
you mentioned how the stories and
radicalized aren't connected. Yeah.

465
00:26:59,910 --> 00:27:04,200
But you said that the,
the role of place mattered in the stories.

466
00:27:04,620 --> 00:27:08,820
This story takes place in suburban Boston.
Why Boston?

467
00:27:09,000 --> 00:27:11,640
Although it touches Phoenix, right?
Phoenix isn't all the stories.

468
00:27:11,760 --> 00:27:15,360
So Boston in part because I I
knew at reasonably well, uh,

469
00:27:15,361 --> 00:27:19,230
I'm a MIT research, uh, MIT
media lab research affiliate. Um,

470
00:27:19,260 --> 00:27:23,670
you and I hung out in Boston. I stayed
on your living room in Boston once. Uh,

471
00:27:23,730 --> 00:27:26,760
and Boston's a really interesting
town in that it's like,

472
00:27:27,110 --> 00:27:32,110
it's a crossroads for a bunch of
different kinds of industry and activity.

473
00:27:32,161 --> 00:27:35,670
So it's obviously an academic hub.
It's a tech hub. It's now biotech hub,

474
00:27:36,030 --> 00:27:40,290
but it's also a light industry
town. It's a port. Um, it has,

475
00:27:40,291 --> 00:27:44,410
it has all of these different
contradictions and it
at similar to La, right.

476
00:27:44,670 --> 00:27:46,650
It's not just,
it's not like the bay area,

477
00:27:46,651 --> 00:27:49,800
which although the bay area has a
bunch of industries and its history,

478
00:27:50,220 --> 00:27:52,320
it has been completely eclipse.
Now.

479
00:27:52,321 --> 00:27:54,840
There's just like one thing
that it's known for now,

480
00:27:55,110 --> 00:27:59,070
whereas Boston still is this very
diverse place. Um, and Arizona I,

481
00:27:59,100 --> 00:28:01,440
I'm really interested in,
because like on the one hand,

482
00:28:01,441 --> 00:28:05,400
it's a place that is not going to
survive climate change gracefully, right?

483
00:28:05,401 --> 00:28:09,480
It's like, it's really in the crosshairs
of climate change, but it's a red state.

484
00:28:09,481 --> 00:28:12,810
So it's also a state where
climate is officially denied,

485
00:28:13,050 --> 00:28:16,800
but it's also a state that is
majority minority. Um, and,

486
00:28:16,830 --> 00:28:20,880
but for a little bit of Gerrymandering
here in a little bit of voter suppression

487
00:28:20,881 --> 00:28:25,470
there, um, it would be a place that that
would have a very different politics.

488
00:28:25,620 --> 00:28:27,240
And it's also a retirement hub.

489
00:28:27,480 --> 00:28:31,260
So it's a place that's full of people
who aren't Arizona's making claims about

490
00:28:31,470 --> 00:28:35,120
their native rights right there.
They're indigenous rights as Arizona's,

491
00:28:35,160 --> 00:28:39,020
even though they're all transplants.
And so like the contradictions of,

492
00:28:39,040 --> 00:28:42,330
of Arizona are so, are
so vivid. And I, I'm,

493
00:28:42,331 --> 00:28:46,730
I'm an advisor to a center at Asu that
does science fiction to talk about, um,

494
00:28:46,920 --> 00:28:51,420
other disciplines. And so I go
to Phoenix periodically, uh, and,

495
00:28:51,450 --> 00:28:54,000
and also it's my hub because
I fly to Burbank airport,

496
00:28:54,001 --> 00:28:58,440
I live in Burbank and so everything
starts Phoenix and then somewhere else.

497
00:28:58,441 --> 00:29:03,300
So I really was interested in the role
that Arizona plays in the kind of the

498
00:29:03,301 --> 00:29:07,920
future of our politics. And
it's interesting leveling
effect if you talk about,

499
00:29:07,950 --> 00:29:08,281
you know,

500
00:29:08,281 --> 00:29:12,780
kind of redefining who we presumed to be
a refugee or an immigrant both places.

501
00:29:12,810 --> 00:29:15,540
Both of those places are also
places that despite their diversity,

502
00:29:15,720 --> 00:29:20,390
we don't necessarily think of
as an immigration. Yeah, yeah.

503
00:29:20,430 --> 00:29:22,110
I think we'll see more places like that.
And I mean,

504
00:29:22,111 --> 00:29:26,550
Greenpoint Brooklyn is still predominant
Polish entry point in the country,

505
00:29:26,940 --> 00:29:30,150
uh,
grilling among population in Wisconsin.

506
00:29:30,450 --> 00:29:32,190
What do you think that
means kind of to the,

507
00:29:32,191 --> 00:29:36,660
to the sense of place and then the
diversity of those communities?

508
00:29:37,500 --> 00:29:41,280
So this is a really interesting question.
We have a story about America,

509
00:29:41,320 --> 00:29:45,070
about kind of something between
assimilation and ladder kicking, right?

510
00:29:45,071 --> 00:29:49,370
Like once there were poles who were not
thought of as as white or American or

511
00:29:49,420 --> 00:29:50,110
Italians,

512
00:29:50,110 --> 00:29:53,650
and then they assimilated and then they
became respectable and then they kicked

513
00:29:53,651 --> 00:29:55,810
the ladder away and they turn on
the people who came after them.

514
00:29:56,020 --> 00:29:57,550
And there's a certain
amount of truth to that.

515
00:29:57,790 --> 00:30:00,400
Some of that though is associated
with economic dynamism.

516
00:30:00,640 --> 00:30:02,500
So one of the things that,
um,

517
00:30:02,980 --> 00:30:07,230
that made people go from
being other to being, uh,

518
00:30:07,270 --> 00:30:09,370
accepted as fully paid up Americans.

519
00:30:09,610 --> 00:30:14,260
It was an extraordinary degree of social
mobility at various times in America's

520
00:30:14,261 --> 00:30:15,270
history.
Uh,

521
00:30:15,271 --> 00:30:19,720
and Tom Piketty and his book capital in
the 21st century talks about America's

522
00:30:19,721 --> 00:30:22,600
dynamism and he chalks it up to these,
uh,

523
00:30:22,601 --> 00:30:27,160
reset events that America had in terms
of its wealth distribution. So in,

524
00:30:27,220 --> 00:30:28,480
before manumission,

525
00:30:28,990 --> 00:30:33,990
the vast majority of American wealth was
in people who were claimed as slaves by

526
00:30:34,151 --> 00:30:36,970
people who had enslaved them,
right? Like the, the, the,

527
00:30:37,030 --> 00:30:41,410
the gross national wealth of America
was primarily in human bodies.

528
00:30:41,800 --> 00:30:45,160
And so manumission in addition
to doing a lot of other things,

529
00:30:45,161 --> 00:30:47,020
politically had this huge economic,

530
00:30:47,140 --> 00:30:50,770
what was this huge economic moment in
that the greatest concentrations of wealth

531
00:30:50,771 --> 00:30:54,550
in America sees to be considered
as assets and became human beings,

532
00:30:54,551 --> 00:30:58,930
at least as a legal fiction,
not withstanding Jim Crow and whatever.

533
00:30:59,110 --> 00:31:03,730
But one of the effects of that is that
the grip that wealthy people had on

534
00:31:03,731 --> 00:31:08,731
political outcomes completely changed
as after the civil war because they just

535
00:31:09,251 --> 00:31:11,980
didn't have as much money to
spend, right. That the, the,

536
00:31:11,981 --> 00:31:14,230
their wealth was like
radically diminished.

537
00:31:14,410 --> 00:31:17,620
And then it happened again during
the crash. And so the, the,

538
00:31:18,010 --> 00:31:22,300
the 30 the warriors and also the echo
and the crash after the gilded age

539
00:31:22,930 --> 00:31:24,940
completely leveled out
the wealth distribution.

540
00:31:24,941 --> 00:31:28,240
So the amount of wealth control by
the top decile of Americans just,

541
00:31:28,260 --> 00:31:32,260
just like nosedives twice in
American industrial history.

542
00:31:32,530 --> 00:31:36,170
And it only happens once in Europe, right?
It only happens with the, with the, um,

543
00:31:36,400 --> 00:31:40,530
with the warriors, with the two
wars in the interwar years. And, uh,

544
00:31:40,660 --> 00:31:44,350
that when, when the capital
is more widely distributed,

545
00:31:44,890 --> 00:31:48,450
you have more pluralistic choices,
more pluralistic decisions. Um,

546
00:31:48,520 --> 00:31:52,300
which creates things like, um,
a better social safety net,

547
00:31:52,480 --> 00:31:56,440
a better quality of schools, cheaper
and wider access to education and so on.

548
00:31:56,590 --> 00:32:01,180
And this lifts up lots of people,
right? It helps people, um, uh,

549
00:32:01,270 --> 00:32:04,690
enter a kind of middle spot.
Middleclass respectability. Uh,

550
00:32:04,691 --> 00:32:06,850
and as we know from the crisis of 2008,

551
00:32:06,851 --> 00:32:11,851
the first people who get jettisoned when
the economy starts to sink are the last

552
00:32:12,011 --> 00:32:12,910
people who got in.

553
00:32:13,150 --> 00:32:16,930
And of course there was still historic
problems with this in terms of red lining

554
00:32:16,931 --> 00:32:20,470
that denied access to African Americans
to, to that shared prosperity.

555
00:32:20,471 --> 00:32:21,340
But nevertheless,

556
00:32:21,550 --> 00:32:26,260
you got people who became officially
American instead of others instead of

557
00:32:26,261 --> 00:32:29,410
perpetual immigrants.
And so there's this story that goes,

558
00:32:29,411 --> 00:32:31,380
if you come to America
and you just hang out,

559
00:32:31,450 --> 00:32:33,460
your kids or their kids will be Americans.

560
00:32:33,461 --> 00:32:36,340
They'll see thinking themselves as
Polish Americans are Italian Americans.

561
00:32:36,550 --> 00:32:40,370
But really what we're saying is
there'll become middle class, right? And

562
00:32:40,820 --> 00:32:43,190
right now it's looking
like that's ended right?

563
00:32:43,191 --> 00:32:46,760
That you don't become middle class.
If you show up in America with you know,

564
00:32:46,790 --> 00:32:51,530
nothing but nothing but the shoe
leather on your, on your feet, you just,

565
00:32:51,560 --> 00:32:56,000
you just stay outside of
wealth accumulation and
instead of social mobility,

566
00:32:56,001 --> 00:32:58,940
because all of the policy
levers that we're used to,

567
00:32:59,240 --> 00:33:04,240
to allow people who had aptitude to gain
access to education and capital and to

568
00:33:05,541 --> 00:33:07,070
start businesses and so on.
Those have,

569
00:33:07,071 --> 00:33:09,140
those have been snuffed
out one after another.

570
00:33:09,560 --> 00:33:14,000
As the concentration of wealth of
the top has grown. And as the Paula,

571
00:33:14,030 --> 00:33:18,190
they have more policy levers to
yank on to increase their, their,

572
00:33:18,290 --> 00:33:21,410
the concentration of their wealth
at the expense of everyone else.

573
00:33:21,411 --> 00:33:23,360
And if you think about it, you
know, the law of large numbers,

574
00:33:23,361 --> 00:33:27,500
the law of small numbers, making
the wealth of someone who, who,

575
00:33:27,530 --> 00:33:31,990
who owns almost everything
grow by even 1% is hard. Uh,

576
00:33:32,150 --> 00:33:35,090
making the wealth of someone who owns
almost nothing double is easy. Right?

577
00:33:35,091 --> 00:33:37,550
That's why I like startups
have incredible growth, right?

578
00:33:37,730 --> 00:33:42,660
We went from one user to two. That
is a very impressive growth, uh,

579
00:33:42,670 --> 00:33:44,690
number expressed as a percentage.

580
00:33:44,990 --> 00:33:48,590
It will take a lot longer for
Google to double its users, right?

581
00:33:48,620 --> 00:33:52,700
Then the startup that goes from one to
two, and so if you're making the rich,

582
00:33:52,701 --> 00:33:56,780
the super rich richer, even even a
little bit richer, measurably richer,

583
00:33:57,080 --> 00:34:02,080
it has to come at the expense of nearly
the total net worth of a huge number of

584
00:34:02,781 --> 00:34:04,310
people.
That's why that Oxfam,

585
00:34:04,311 --> 00:34:07,340
number of the number of billionaires in
a bus it would take to represent half

586
00:34:07,341 --> 00:34:10,850
the world's wealth is
so shocking because it,

587
00:34:10,880 --> 00:34:12,680
it doesn't just mean that they got richer,

588
00:34:12,681 --> 00:34:16,760
it means that giant number of people
had to get much poorer. Right?

589
00:34:16,790 --> 00:34:21,790
And so anytime you see the wealth of of
the top decile of the top percentile or

590
00:34:22,581 --> 00:34:27,500
the top 10th of a percentile increasing
measurably, you know that, that you know,

591
00:34:27,710 --> 00:34:28,880
it's not coming from growth.

592
00:34:28,881 --> 00:34:31,400
I mean there's some growth but it's
not coming primarily from growth.

593
00:34:31,401 --> 00:34:35,300
It's coming through through redistribution
and that redistribution is upwards

594
00:34:35,480 --> 00:34:37,460
and it's vastly asymmetrical.

595
00:34:37,610 --> 00:34:41,930
Everything you own going to make a tiny
difference to someone who already owns

596
00:34:41,931 --> 00:34:42,770
nearly everything.

597
00:34:42,980 --> 00:34:46,940
We've long thought that technology will
help where away the differences between

598
00:34:46,941 --> 00:34:47,481
class,

599
00:34:47,481 --> 00:34:52,190
between race and in the story the
poor have access to technology,

600
00:34:52,520 --> 00:34:55,640
but they don't have a choice of the
technology that they can access.

601
00:34:56,090 --> 00:34:59,520
And there's a moment in the piece when,
when I laughed where when,

602
00:34:59,600 --> 00:35:04,280
when asked what kind of toaster
the startup employee had, he said,

603
00:35:04,281 --> 00:35:05,750
well not that kind of toaster.

604
00:35:05,751 --> 00:35:08,660
And so the better off could
choose what tools they use your,

605
00:35:08,780 --> 00:35:11,750
could you talk a little bit about,
about that element of choice? Yeah,

606
00:35:11,751 --> 00:35:13,670
I mean I think that there's a,

607
00:35:13,790 --> 00:35:17,120
there's a common pathology
among technologists,

608
00:35:17,150 --> 00:35:19,580
especially those who work
on restrictive technologies,

609
00:35:19,700 --> 00:35:22,820
which is that we never imagined
ourselves using them. Uh,

610
00:35:22,850 --> 00:35:26,480
we imagine them being deployed on others.
We, we will always have root, right?

611
00:35:26,481 --> 00:35:27,950
We will always,
like this is,

612
00:35:28,180 --> 00:35:31,220
we were just talking before we came in
about this story today about youtubers

613
00:35:31,221 --> 00:35:36,140
being extorted by fraudsters who register
fake copyright strikes the fraudulent

614
00:35:36,141 --> 00:35:38,070
copyright strikes against
their youtube accounts and then

615
00:35:38,070 --> 00:35:39,450
say,
give me $150.

616
00:35:39,460 --> 00:35:43,110
I'll put a third strike on you because
everybody knows you can't get anyone at

617
00:35:43,111 --> 00:35:46,140
Youtube on the phone when you've
had a copyright strike against you.

618
00:35:46,141 --> 00:35:49,380
Because like literally anyone who
gets a copyright strike against them,

619
00:35:49,440 --> 00:35:51,960
things that they, that, that
it's illegitimate, right?

620
00:35:51,961 --> 00:35:55,560
So like youtube doesn't have any great
way to triage it and you just get stuck

621
00:35:55,561 --> 00:35:58,860
in these support email loops and
there isn't a support choice for,

622
00:35:59,070 --> 00:36:03,990
I'm being extorted by a petty grifter
who wants a millionth of a bitcoin to get

623
00:36:03,991 --> 00:36:06,810
away to get my, my, my
copyright strikes removed.

624
00:36:07,050 --> 00:36:09,390
And so since that doesn't exist in
the email loop, you can't get help.

625
00:36:09,391 --> 00:36:13,560
But if you are me and you know, Googlers
like that would never happen to me.

626
00:36:13,561 --> 00:36:14,100
Right?

627
00:36:14,100 --> 00:36:19,100
I would just call a Googler friend Fred
von Loman who runs copyright for Google

628
00:36:19,201 --> 00:36:22,140
with, with a couple other people
used to work with me at eff.

629
00:36:22,140 --> 00:36:25,020
I could just call him and he would
sort it out for me. So, you know,

630
00:36:25,021 --> 00:36:27,900
when we're in these situations,
when we design these systems,

631
00:36:28,110 --> 00:36:30,780
we always know that like, they're
not going to bind us. You know,

632
00:36:30,781 --> 00:36:35,781
I worked on this DRM standard trying to
fight it a for digital television called

633
00:36:35,941 --> 00:36:39,060
the broadcasting broadcast flag.
And it's the crazy idea,

634
00:36:39,061 --> 00:36:40,200
and I don't want to go
into the whole thing,

635
00:36:40,201 --> 00:36:44,460
but part of it was that all general
purpose computers would have to only have

636
00:36:44,461 --> 00:36:49,440
outputs that were approved by
movie studios. Uh, and uh, but,

637
00:36:50,160 --> 00:36:53,910
um, there was a professional
tools exemption, right?

638
00:36:53,911 --> 00:36:56,820
And so there were like people from all
the big tech companies and people from

639
00:36:56,821 --> 00:36:59,700
all the big movie studios and people from
all the big like consumer electronics

640
00:36:59,701 --> 00:37:01,810
companies in the room.
And as soon as they said it,

641
00:37:01,830 --> 00:37:04,380
but there'll be a professional tools
exemption. Everyone like, yeah,

642
00:37:04,560 --> 00:37:06,170
of course there'll be a
professional tools exemption.

643
00:37:06,360 --> 00:37:10,050
You think I'm going to use this stuff?
No Way. Right? Like, I'm going to have,

644
00:37:10,440 --> 00:37:14,910
I'm going to have the version of this
that isn't, you know, terrible. Right?

645
00:37:14,911 --> 00:37:19,230
The terrible version is for other
people. Um, and so, you know,

646
00:37:19,440 --> 00:37:21,810
trying to like kind of
tease that out, right? This,

647
00:37:21,811 --> 00:37:26,811
this idea that like when
we make technology that we
would never want to use,

648
00:37:27,270 --> 00:37:32,010
we're a dooming lots of
other people to using it,

649
00:37:32,280 --> 00:37:37,020
right? Uh, you know, the, a recurring
version of this as every time people says,

650
00:37:37,021 --> 00:37:37,321
well,
you know,

651
00:37:37,321 --> 00:37:41,760
all those people who work in Silicon
Valley don't let their kids on phones, uh,

652
00:37:41,790 --> 00:37:45,120
you know, or watch youtube
or whatever, you know,

653
00:37:45,121 --> 00:37:48,660
use it on a or are they all have
ad blockers installed or you know,

654
00:37:48,661 --> 00:37:53,661
all the other things that we know about
technologists and I'm guilty of a to uh,

655
00:37:53,820 --> 00:37:58,050
that, that um, you know, civilians
don't have access to or don't,

656
00:37:58,080 --> 00:37:58,920
don't get to do.

657
00:38:00,150 --> 00:38:03,180
And also reminds me of that when we were
talking about before the talk in terms

658
00:38:03,181 --> 00:38:06,240
of just how beautiful it is
when something just works.

659
00:38:06,630 --> 00:38:10,650
The book also addresses kind
of functionalism and there's
a couple of neat lines

660
00:38:10,710 --> 00:38:12,840
in the story.
This was a new kind of toaster,

661
00:38:13,170 --> 00:38:15,720
a toaster that took orders
rather than giving them.

662
00:38:16,260 --> 00:38:19,380
And then also if someone wants
to control you with the computer,

663
00:38:19,381 --> 00:38:22,530
they have to put the computer
where you are and they are not.

664
00:38:22,560 --> 00:38:24,750
So you can access the
computer without supervision.

665
00:38:24,751 --> 00:38:28,500
Can you talk a little bit
about functionalism and
how we might be moving away

666
00:38:28,501 --> 00:38:31,950
from that are closer to that. So the
nexus of control is the key thing, right?

667
00:38:31,951 --> 00:38:32,830
All that,

668
00:38:32,840 --> 00:38:36,550
the difference between Utopian dystopia
is who's got their finger on the button,

669
00:38:36,790 --> 00:38:40,540
right? Like having a, a fitbit that
tells you how many steps you've walked,

670
00:38:40,541 --> 00:38:43,000
leaving aside the problems with the
sensor and the fact that it might think

671
00:38:43,001 --> 00:38:46,190
you've walked 1500 steps before
you get it a bed. Um, but having a,

672
00:38:46,191 --> 00:38:50,380
a fitbit that helps you track
your own fitness levels.

673
00:38:50,740 --> 00:38:53,590
Maybe you could have a pathological
relationship with it, but there's no,

674
00:38:53,740 --> 00:38:58,090
there's nothing wrong with you
knowing more about yourself.

675
00:38:59,170 --> 00:39:01,510
Having your boss put a
fitbit on you and say,

676
00:39:01,511 --> 00:39:03,640
if you don't get your 10,000 steps in,
we're going to,

677
00:39:03,790 --> 00:39:07,300
we're going to take you out of
the company health plan. It's lit.

678
00:39:07,330 --> 00:39:11,380
It's the same technology. It's just
a different nexus of control, right?

679
00:39:11,590 --> 00:39:12,423
And if there's,
like,

680
00:39:12,850 --> 00:39:17,710
if there's a mechanism in the fitbit
that detects spurious sensor events in

681
00:39:17,711 --> 00:39:21,130
order to stop the fitbit from telling
you that you've walked 10,000 steps when

682
00:39:21,131 --> 00:39:23,980
you've really only just gotten out of bed,
that's great.

683
00:39:24,310 --> 00:39:27,880
If that same sensor is used to stop you
putting your fitbit and a sock and stick

684
00:39:27,881 --> 00:39:31,000
it in the tumble dryer to fool your
boss into thinking that you've done your

685
00:39:31,001 --> 00:39:34,570
10,000 steps, that's terrible.
Right? So the best example of this,

686
00:39:34,571 --> 00:39:37,680
Doug Rushkoff just wrote this
column about going to this um, um,

687
00:39:38,140 --> 00:39:42,160
hedge fund conference and one of the
panels was on what to do in the event,

688
00:39:42,190 --> 00:39:47,140
right? The, when the collapse comes and
the poorest come to eat you. And um, they,

689
00:39:47,200 --> 00:39:47,771
they,
they,

690
00:39:47,771 --> 00:39:50,920
they were talking about this problem
that they would have when currency

691
00:39:50,921 --> 00:39:51,461
collapsed,

692
00:39:51,461 --> 00:39:55,810
which is how do you stop your guards from
killing you if they don't need you to

693
00:39:55,811 --> 00:39:58,480
pay their paychecks? They said,
well, we've got it sorted out.

694
00:39:58,481 --> 00:40:02,710
We're going to have these like two
factor biometric and, and a secret, um,

695
00:40:02,920 --> 00:40:04,000
food lockers.

696
00:40:04,300 --> 00:40:07,480
And so they'll have to keep me alive to
unlock the food lockers so they don't

697
00:40:07,481 --> 00:40:08,470
starve to death.

698
00:40:08,800 --> 00:40:13,090
And that's going to be the thing that like
keeps my guards in line. And you know,

699
00:40:14,020 --> 00:40:17,800
I know people and I have been on many
diets and sometimes those diets involve

700
00:40:17,801 --> 00:40:21,760
not eating certain things. And if I put
a lock on my fridge that I can control,

701
00:40:21,761 --> 00:40:25,600
that didn't open until it was lunchtime.
Literally the same technology.

702
00:40:25,690 --> 00:40:27,970
And the only difference is
who's pushing the button, right?

703
00:40:27,971 --> 00:40:31,640
The difference between Utopian
dystopia is who gets to decide. It's,

704
00:40:31,730 --> 00:40:35,320
it's where the choices. Um, and so these,

705
00:40:35,350 --> 00:40:39,490
these people are moving from a situation
where the technology that they're in is

706
00:40:39,491 --> 00:40:41,890
a stricture and it becomes an enabler.

707
00:40:42,490 --> 00:40:47,080
And I think that there's a story that
like going back to the dishwashers and,

708
00:40:47,140 --> 00:40:48,910
and,
and the Steve Jobs figure,

709
00:40:49,000 --> 00:40:52,420
there's a story that we tell about
technological inevitability, right?

710
00:40:52,421 --> 00:40:57,421
Like we have to make it impossible and
illegal for you to reconfigure your

711
00:40:58,001 --> 00:41:01,390
devices or they won't work
or they won't keep you safe.

712
00:41:01,840 --> 00:41:03,940
And we talk about it as though it, it, it,

713
00:41:03,941 --> 00:41:07,330
like someone came down off a mountain
with two stone tablets that said that,

714
00:41:07,331 --> 00:41:10,240
right? Like, you know,
when we talk about, um, uh,

715
00:41:10,241 --> 00:41:14,710
online surveillance ad tech and, and
predictive markets and machine learning,

716
00:41:14,920 --> 00:41:15,131
right?

717
00:41:15,131 --> 00:41:19,180
Like I am old enough to remember when
we rotated our server logs instead of

718
00:41:19,181 --> 00:41:22,600
mining them for market intelligence,
right. There's like no reason.

719
00:41:22,601 --> 00:41:27,100
Like nobody will take away your cs degree
if you start rotating your logs again,

720
00:41:27,550 --> 00:41:31,180
right. Instead of saving them, right?
Like that is a choice someone made,

721
00:41:31,181 --> 00:41:34,460
but we disguise it as a
technological inevitability.

722
00:41:34,790 --> 00:41:39,500
And so by like showing that when
the nexus of control changes,

723
00:41:39,920 --> 00:41:43,220
the nature of your relationship to the
technology changes and the thing that you

724
00:41:43,221 --> 00:41:46,280
didn't like about the technology turns
out to be a thing you didn't like about

725
00:41:46,281 --> 00:41:49,400
the nexus of control.
That is, I think, again,

726
00:41:49,401 --> 00:41:54,401
a powerful lesson to encourage people
to understand stem and there and,

727
00:41:54,741 --> 00:41:57,920
and to become masters of their technology.

728
00:41:58,100 --> 00:42:02,450
We talk about stem education as though
it's just part of the like normal grift

729
00:42:02,451 --> 00:42:05,480
of like if you don't get your kids to
do this and give me money to teach your

730
00:42:05,481 --> 00:42:08,060
kids to do this,
your kids will be economic roadkill.

731
00:42:08,180 --> 00:42:10,790
And there's certainly an a certain
element of that. But again,

732
00:42:10,791 --> 00:42:14,840
like I think the origins of the stem
education movement are about digital self

733
00:42:14,841 --> 00:42:16,490
defense.
Not about,

734
00:42:16,520 --> 00:42:21,020
not about like creating an industrial
workforce right program or be programmed,

735
00:42:21,021 --> 00:42:23,990
learn how to use the technology
or the technology we'll use you.

736
00:42:24,440 --> 00:42:29,440
And that empowering message of technology
is one that I really firmly believe

737
00:42:30,321 --> 00:42:32,660
in. Right? I don't want
to get rid of computers.

738
00:42:32,750 --> 00:42:34,730
I just want to change how we use them.

739
00:42:35,390 --> 00:42:37,280
I want to talk a little
bit about the audio book.

740
00:42:37,430 --> 00:42:39,710
So this novella came out yesterday.
Yeah,

741
00:42:39,711 --> 00:42:44,711
there's stories are coming out in March
19 and the voice actor who did this

742
00:42:44,841 --> 00:42:49,460
one's pretty interesting, just herself and
so on. Ask you about her. Uh, let me see.

743
00:42:49,461 --> 00:42:54,140
SOC is a Palestinian American actress
and playwright and founded a production

744
00:42:54,141 --> 00:42:56,810
company dedicated to Middle
Eastern theater arts.

745
00:42:57,110 --> 00:43:00,470
How did you connect with her and
get her involved in the project?

746
00:43:00,500 --> 00:43:03,200
I'm lucky enough that I've been really
involved with the production of my last

747
00:43:03,201 --> 00:43:08,030
several audio books. We record them at a
studio near me in there in Hollywood, uh,

748
00:43:08,060 --> 00:43:12,590
called sky boat media. So they consult
me on casting choices and so on.

749
00:43:12,591 --> 00:43:17,591
And my hard and fast rule with them was
that I wanted a voice actor who was of

750
00:43:17,631 --> 00:43:19,010
Arab origin.
Uh,

751
00:43:19,011 --> 00:43:22,860
and I wanted a woman because the narrator
is a woman of Arab origin. And um,

752
00:43:23,180 --> 00:43:28,040
you know, I have a friend Lexi Alexander
who's a very interesting person.

753
00:43:28,041 --> 00:43:31,250
She's a director now, but she
was a world champion. Kickboxer.

754
00:43:31,520 --> 00:43:35,390
She's a German Palestinian woman who
was brought here by Chuck Norris to help

755
00:43:35,391 --> 00:43:39,620
train the army. Uh, she became a
stunt woman, then became a director.

756
00:43:39,621 --> 00:43:42,650
She directed punisher.
She directed a couple of super girls.

757
00:43:42,651 --> 00:43:43,610
She does all kinds of stuff.

758
00:43:43,790 --> 00:43:47,660
And one of the things that she's always
on about on Twitter is all of the

759
00:43:47,661 --> 00:43:52,661
talented Arab American woman actors and
other actors of color who could be doing

760
00:43:53,001 --> 00:43:56,270
the roles that for reasons
that completely baffled me,

761
00:43:56,630 --> 00:44:00,740
are not going to people who they
would be suited for. And so, um,

762
00:44:00,830 --> 00:44:04,670
I asked her for some names, none
of them could do it. Uh, but I,

763
00:44:04,680 --> 00:44:06,980
then I went to the,
to the directors and I said,

764
00:44:06,981 --> 00:44:09,230
we really need to cast an Arab American,

765
00:44:09,350 --> 00:44:13,520
a woman for this and um,
or a woman of Arabic origin.

766
00:44:13,550 --> 00:44:17,690
And we got the demo reels for four or
five of them and let me was the best one.

767
00:44:17,691 --> 00:44:21,170
And she was available.
And the arrest happened there.

768
00:44:21,171 --> 00:44:23,030
I was in the studio everyday
when we were recording.

769
00:44:23,031 --> 00:44:27,200
She did an amazing job and
it's a wonderful listen as
well as a wonderful read.

770
00:44:27,290 --> 00:44:28,880
Yeah.
Questions from the audience.

771
00:44:28,910 --> 00:44:32,160
If we can alternate between people who
identify as women and identify as men or

772
00:44:32,161 --> 00:44:36,780
nonbinary and can go at any time. But,
um, that way it's not just, um, dudes.

773
00:44:37,090 --> 00:44:40,680
Uh, so a lot of science fiction
writers, um, I'm thinking of, uh,

774
00:44:40,740 --> 00:44:43,540
like Ray Bradbury and Isaac Asimov
and so on, kind of like, right.

775
00:44:43,541 --> 00:44:45,360
A lot of stories that,

776
00:44:45,390 --> 00:44:50,340
that sort of come to imagine a quart of
shared world of the future, right? That,

777
00:44:50,380 --> 00:44:54,210
and they build up this kind of consistent
world of the future. Uh, uh, uh,

778
00:44:54,220 --> 00:44:57,720
Ray Bradbury did what they did that
with Martian chronicles and, and, uh,

779
00:44:57,850 --> 00:44:59,110
asthma for the foundation and so on.

780
00:44:59,350 --> 00:45:02,210
And just kind of curious if you see
yourself stumbling toward that or,

781
00:45:02,211 --> 00:45:04,300
and you might be doing that on
purpose or that kind of thing.

782
00:45:04,320 --> 00:45:05,010
I liked the way Brad

783
00:45:05,010 --> 00:45:07,050
free did it more than the way
asthma affected because it's,

784
00:45:07,051 --> 00:45:10,530
they're not all in the same continuity,
right? There are things that like,

785
00:45:10,560 --> 00:45:13,620
there are things that happen on Mars
in one story that if they happen,

786
00:45:13,621 --> 00:45:15,270
then the next story doesn't make sense,
right?

787
00:45:15,450 --> 00:45:17,760
But where it makes sense to overlap them,
he does.

788
00:45:17,761 --> 00:45:20,070
And another writer who does
that really well as John Farley,

789
00:45:20,280 --> 00:45:23,880
who has this whole long cycle of stories
that he's been writing since the 70s,

790
00:45:23,881 --> 00:45:28,850
they're amazing. Uh, about, um, I think
they're called the six world stories. Uh,

791
00:45:28,890 --> 00:45:33,780
earth becomes uninhabitable, are living
everywhere except earth. Uh, and when the,

792
00:45:33,840 --> 00:45:37,350
when continuity makes sense,
he's got continuity and
when it doesn't, it doesn't.

793
00:45:37,680 --> 00:45:42,580
And I love that because,
you know, uh, there,

794
00:45:42,660 --> 00:45:46,020
these aren't alternate history, right? Or
like these aren't history. They're not,

795
00:45:46,140 --> 00:45:49,110
they're not instructions.
They're not predictions.

796
00:45:49,290 --> 00:45:53,640
They're artistic works.
Who's effect is, yeah,

797
00:45:53,641 --> 00:45:58,290
there's some like continuity is, is
part of the effect of it. But, um,

798
00:45:58,830 --> 00:46:02,700
the themes of the characters and the
kinds of characters they are and the kinds

799
00:46:02,701 --> 00:46:07,020
of things they get up to are
more important than, you know,

800
00:46:07,021 --> 00:46:10,680
making sure that if someone does something
in one story and then they appear as

801
00:46:10,681 --> 00:46:15,120
a big character in another story, that
it's still crosses over. Uh, you know,

802
00:46:15,121 --> 00:46:18,840
I think of the way that the reboots
of the shared worlds of the comic book

803
00:46:18,841 --> 00:46:21,690
companies have worked where when
it makes sense, it makes sense.

804
00:46:21,691 --> 00:46:23,450
And when it doesn't, it
doesn't, and they, they,

805
00:46:23,451 --> 00:46:25,890
they can have multiple parallel timelines.
Like,

806
00:46:25,891 --> 00:46:28,110
does anyone think Batman
would be better if,

807
00:46:28,170 --> 00:46:33,120
if we were all in continuity since the
first detective comics, you know, uh,

808
00:46:33,590 --> 00:46:36,960
the, the reinvention is actually super
important and part of the way that we tell

809
00:46:36,961 --> 00:46:37,910
stories anyways,
right?

810
00:46:37,920 --> 00:46:41,550
Like if you look at say the
all the different versions
of the Icelandic stories,

811
00:46:41,551 --> 00:46:43,950
right? In the north stories,
they're like they are,

812
00:46:43,951 --> 00:46:47,160
there are multiple irreconcilable
versions of what the nurse did.

813
00:46:47,250 --> 00:46:49,740
There's two versions of
genesis in the Bible, right?

814
00:46:50,190 --> 00:46:54,420
Like the first two chapters of, of,
of genesis tell completely opposite,

815
00:46:54,480 --> 00:46:57,870
non reconcilable, mutually exclusive
stories of where the earth came from.

816
00:46:59,430 --> 00:47:00,950
Right?
So you see yourself as,

817
00:47:00,951 --> 00:47:04,170
as kind of interested in thematically
consistent culture but not necessarily a

818
00:47:04,171 --> 00:47:05,910
logically consistent.
You can't quite say it that way.

819
00:47:05,950 --> 00:47:07,800
And I'm a Pantser, not
a plotter, right eye.

820
00:47:08,200 --> 00:47:11,830
I'm not someone who does a lot of card
tricks in the dark to try and figure out

821
00:47:11,831 --> 00:47:14,020
how the plot's going to work.
I figured out as I go.

822
00:47:14,021 --> 00:47:17,650
So it doesn't really lend itself to
that kind of continuity. Who's next,

823
00:47:17,800 --> 00:47:21,880
what you were saying about digital
self defense? Um, I was wondering,

824
00:47:22,330 --> 00:47:26,350
do you see that as actually being doable?

825
00:47:26,380 --> 00:47:29,340
Because I'm pretty tech
savvy and you know,

826
00:47:29,341 --> 00:47:31,600
at home I run Linux and I have a,

827
00:47:31,870 --> 00:47:36,220
I have my hard drives are
encrypted with locks and so on and,

828
00:47:36,990 --> 00:47:41,230
but there is no way I could possibly run
down all the attack vectors even on my

829
00:47:41,231 --> 00:47:42,064
own machine. Sure, sure.

830
00:47:42,770 --> 00:47:44,960
Yeah.
And I think that um,

831
00:47:46,190 --> 00:47:51,190
there is a story we have about like
what the early days of the Internet

832
00:47:51,590 --> 00:47:52,410
Liberation Movement or a unit,
techno,

833
00:47:52,410 --> 00:47:54,830
techno politics movement
was that I think is wrong,

834
00:47:54,831 --> 00:47:56,460
but it's got a little kernel of rightness.

835
00:47:56,720 --> 00:48:00,650
The wrong version is we used to think
that technology couldn't be possibly used

836
00:48:00,770 --> 00:48:04,940
to do harm and so we wanted everyone to
use technology and we didn't care how it

837
00:48:04,941 --> 00:48:08,390
was used because we were sure that it
would just make everyone's life better.

838
00:48:08,750 --> 00:48:10,430
That's clearly not true.
Right?

839
00:48:10,431 --> 00:48:13,310
Like you don't found electronic
frontier foundation because you think

840
00:48:13,311 --> 00:48:14,480
everything's going to be great.

841
00:48:14,660 --> 00:48:17,450
You found it because you're worried about
how terrible it will go if it doesn't

842
00:48:17,451 --> 00:48:20,270
turn out great. Right. You, you, you
see on the one hand, the promising,

843
00:48:20,300 --> 00:48:21,290
on the other hand,
the peril,

844
00:48:21,560 --> 00:48:26,090
but the cypherpunk movement particularly
did have this idea that you could throw

845
00:48:26,091 --> 00:48:27,290
encrypted communications,

846
00:48:27,291 --> 00:48:32,291
create a parallel universe where even
if you lived in a totalitarian are

847
00:48:32,811 --> 00:48:37,040
unaccountable system where people could
operate with impunity and do terrible

848
00:48:37,041 --> 00:48:37,874
things to you.

849
00:48:37,970 --> 00:48:42,970
That because the ciphers worked and
the keys couldn't be brute forced,

850
00:48:43,520 --> 00:48:47,630
that you, that you could in some way
resist the state kind of indefinitely.

851
00:48:47,660 --> 00:48:51,200
Like you could have a demi moaned that,
that existed alongside of it.

852
00:48:51,650 --> 00:48:54,890
And I think that not every,
not every cyber cypherpunk felt that way,

853
00:48:54,891 --> 00:48:57,200
but a lot of them did.
And I think that that's wrong.

854
00:48:57,590 --> 00:49:00,590
But I think that what we've learned
about encryption and its relationship to

855
00:49:00,591 --> 00:49:05,591
unaccountable authority and illegitimate
authority is that encryption is,

856
00:49:06,410 --> 00:49:11,030
it's it, it's like a stop gap, right?

857
00:49:11,031 --> 00:49:16,031
The technology is a stop gap that you
can use to organize and to resist a

858
00:49:16,770 --> 00:49:20,840
coercion while you figure out how to
make the power structures that you're

859
00:49:20,841 --> 00:49:24,470
organizing about and that you're worried
about coercion from more accountable.

860
00:49:25,310 --> 00:49:26,900
Um, but that, you know,

861
00:49:27,530 --> 00:49:30,620
technology alone can't do it.

862
00:49:31,850 --> 00:49:34,430
But if you try to imagine the inverse,
right?

863
00:49:34,430 --> 00:49:39,430
Imagine creating a political movement
that holds power to account and demands

864
00:49:39,681 --> 00:49:43,190
accountable at legitimate
exercise of authority.

865
00:49:43,310 --> 00:49:45,620
But that doesn't use computers,
right.

866
00:49:45,621 --> 00:49:48,430
That would like you find
each other by like, I dunno,

867
00:49:48,440 --> 00:49:51,500
stapling photocopied posters to,

868
00:49:51,620 --> 00:49:56,620
to telephone poles and imagine how
easily you would be outmaneuvered by the

869
00:49:57,471 --> 00:50:01,110
force that you're trying to
resist. It seems like obvious
to me that that's, that,

870
00:50:01,160 --> 00:50:06,160
that the power relationship dynamic
that we're in now is using technology to

871
00:50:06,351 --> 00:50:09,800
open a space to make a political
change that gives us the space to make

872
00:50:09,801 --> 00:50:13,550
technology that opens the space to make
political change, lather, rinse, repeat.

873
00:50:13,940 --> 00:50:18,380
And you don't always win,
but not, uh, you know, not,

874
00:50:18,410 --> 00:50:21,440
not to just create this Debbie
Monde. And you know, the,

875
00:50:21,590 --> 00:50:26,590
the good news is that people's direct
experience of the way technology can,

876
00:50:27,230 --> 00:50:29,930
a force for liberation and
the way that technology,

877
00:50:29,931 --> 00:50:34,931
when abused can be a force of terror means
that people are actually caring about

878
00:50:36,350 --> 00:50:38,660
the problems a lot more. It's, you know,

879
00:50:38,661 --> 00:50:40,940
you can make an analogy here
like climate change, right?

880
00:50:40,941 --> 00:50:45,500
Like if you are the world's greatest
recycler, it wouldn't stop climate change.

881
00:50:45,650 --> 00:50:49,190
There is nothing you can personally
do that changes climate change,

882
00:50:49,230 --> 00:50:52,700
that that will change the facts of
climate change. But you and everyone else,

883
00:50:52,701 --> 00:50:53,001
you know,

884
00:50:53,001 --> 00:50:56,450
and everyone they know all working
together can do something about climate

885
00:50:56,451 --> 00:50:59,990
change, including collectively recycling
and making a lot of other choices,

886
00:50:59,991 --> 00:51:04,850
including choices about how we invest in
maybe a green, new deal and so on. Um,

887
00:51:04,910 --> 00:51:09,380
and for a long time our biggest problem
was convincing people that climate

888
00:51:09,381 --> 00:51:11,900
change was a problem.
And you know,

889
00:51:11,901 --> 00:51:15,920
we see today that even among the people
who've been historic climate deniers,

890
00:51:16,340 --> 00:51:17,720
that's not really a problem anymore.

891
00:51:17,721 --> 00:51:20,360
We're way we are like past the
point of peak indifference.

892
00:51:20,750 --> 00:51:24,020
But the reason we're past the point of
peak indifference is that the number of

893
00:51:24,021 --> 00:51:27,710
people for whom the reality of climate
change is undeniable because they,

894
00:51:27,711 --> 00:51:31,640
or someone they love has had their lives
harmed or destroyed by climate change

895
00:51:31,850 --> 00:51:32,780
is only growing,

896
00:51:32,990 --> 00:51:36,440
which means that we move from the
problem of convincing people to that that

897
00:51:36,441 --> 00:51:37,700
climate change is a problem.

898
00:51:37,910 --> 00:51:40,460
Now we have to convince them that it's
not too late to do something about it.

899
00:51:40,910 --> 00:51:45,230
And I think we are, we've gone through
that same trajectory with technology,

900
00:51:45,410 --> 00:51:45,770
right?

901
00:51:45,770 --> 00:51:49,700
Convincing people that how we regulate
technology matters that's taken care of

902
00:51:49,701 --> 00:51:50,630
itself,
right?

903
00:51:50,631 --> 00:51:55,631
We just the the largest
petition and Internet history
is the one to overrule the

904
00:51:56,030 --> 00:52:00,590
electoral college and make Hillary
president trailing it by like 1% is the

905
00:52:00,591 --> 00:52:04,070
petition from the European Union to not
pass the copyright directive that would

906
00:52:04,071 --> 00:52:07,610
mandate content id for all
public platforms. Right.

907
00:52:07,930 --> 00:52:10,040
And like that's crazy,
right?

908
00:52:10,041 --> 00:52:14,660
Like the number of people
who pay attention to American
presidential politics

909
00:52:14,750 --> 00:52:17,900
versus people who care about whether or
not we're going to have upload filters

910
00:52:18,050 --> 00:52:20,330
is now nearly a parody.
Right?

911
00:52:20,450 --> 00:52:23,480
So now we just have to convince them
that it's not too late to do something

912
00:52:23,481 --> 00:52:25,010
about it. And, and,

913
00:52:25,070 --> 00:52:29,030
and the doing something about it is the
making the power accountable and so on.

914
00:52:29,240 --> 00:52:29,810
Not that,

915
00:52:29,810 --> 00:52:32,720
not that we should ever be storing
passwords in the clear because everyone's

916
00:52:32,721 --> 00:52:35,510
accountable, but that, um,

917
00:52:36,440 --> 00:52:41,240
we can store passwords in an encrypted
form and also assume that the secret

918
00:52:41,241 --> 00:52:44,990
police aren't going to show up and make
you put it back door in to decrypt those

919
00:52:44,991 --> 00:52:49,220
passwords so they can man the middle or
otherwise disrupt the communications of

920
00:52:49,221 --> 00:52:53,390
your users. Right. And that's like the,
that's the cycle that we're in. Thank you.

921
00:52:54,190 --> 00:52:56,470
So obviously the, the, you know,

922
00:52:56,471 --> 00:53:01,060
this country has corporations
organized based on a profit motive.

923
00:53:01,090 --> 00:53:05,480
Like corporations have to be accountable
to their shirt, shareholders in peace,

924
00:53:05,490 --> 00:53:07,120
profits, et cetera. We've had,

925
00:53:07,180 --> 00:53:09,430
we've been lucky and unlucky
with the results of that.

926
00:53:09,431 --> 00:53:14,140
And in the grand scheme of things,
obviously, you know, from my perspective,

927
00:53:14,141 --> 00:53:16,600
and I'm sure a lot of people that are
your googles on the good side of that,

928
00:53:16,960 --> 00:53:19,840
hopefully, um, we'll
see. You know, I guess

929
00:53:20,060 --> 00:53:21,620
a lot of the time it is,
I think you're right.

930
00:53:22,250 --> 00:53:26,970
Um, but uh, but there's
nothing stopping, uh, you know,

931
00:53:27,600 --> 00:53:31,500
kind of the way things are going for
it going badly as well even for this

932
00:53:31,501 --> 00:53:33,140
company. Sure. Um,

933
00:53:33,540 --> 00:53:38,380
do you think there's a way to realign
the incentives like at a high level to,

934
00:53:38,860 --> 00:53:42,910
to make that less likely to make the good
outcomes more likely, let's say. Okay.

935
00:53:42,960 --> 00:53:45,720
I want to talk about that European Union
copyright directive because it's a good

936
00:53:45,721 --> 00:53:50,721
example of firms that are generally good
firms or firms that have done a lot of

937
00:53:51,001 --> 00:53:54,960
good that are doing bad and doing
it in a weirdly dysfunctional way.

938
00:53:55,410 --> 00:53:58,680
So one of the things that's weird about
the copyright directive is that the

939
00:53:58,681 --> 00:54:00,480
record labels are super in favor of it.

940
00:54:00,510 --> 00:54:02,310
And the movie studios
are super opposed to it,

941
00:54:02,460 --> 00:54:06,750
but they're the same companies like
literally universal music wants it and

942
00:54:06,751 --> 00:54:07,950
universal pictures doesn't,

943
00:54:08,220 --> 00:54:12,390
and they're both sending open letters
to the commission of the European Union

944
00:54:12,570 --> 00:54:15,660
demanding that they be heated.
Right.

945
00:54:15,990 --> 00:54:20,990
So this is actually a pretty common
problem in the theory and history of

946
00:54:22,321 --> 00:54:27,321
antitrust that beyond a certain scale
there are massive disaffiliate nccs of

947
00:54:27,841 --> 00:54:30,690
scale. And you probably encountered
them working in a large firm.

948
00:54:30,720 --> 00:54:34,830
Like there's a thing that's good
for the firm as far as you can see.

949
00:54:34,830 --> 00:54:39,330
But at Gore is the ox of someone who's
got a lot of power within the firm. I, uh,

950
00:54:39,360 --> 00:54:43,810
I had a personal relationship
with, with flicker when it
was founded. I, um, was, uh,

951
00:54:43,900 --> 00:54:47,190
an Alpha Tester for the game that
it came out of game never ending.

952
00:54:47,400 --> 00:54:49,800
And I was carrying on a long distance
relationship with a woman who's now my

953
00:54:49,801 --> 00:54:52,500
wife, who lived in San Francisco and
London. I lived in San Francisco.

954
00:54:52,620 --> 00:54:56,280
We are both Alpha testers and Stewart
Butterfield and Caterina fake who created

955
00:54:56,281 --> 00:54:58,950
the gang, came to San Francisco and we
had lunch and they said, how's it going?

956
00:54:58,951 --> 00:55:02,310
And I said, it's great, but we have
trouble sharing our images. And they said,

957
00:55:02,330 --> 00:55:04,780
we've got that coming in. The game will
just accelerate and product road map.

958
00:55:04,781 --> 00:55:07,830
And three months later they shut the
company down and called it [inaudible] and

959
00:55:07,831 --> 00:55:10,590
renamed it flicker just around
that one photo sharing thing.

960
00:55:10,590 --> 00:55:13,320
So I feel really close to flicker and I
watched really carefully what happened

961
00:55:13,321 --> 00:55:18,321
when Yahoo bought it and flicker was the
first mobile social photo sharing APP.

962
00:55:19,620 --> 00:55:24,600
And so it had the power to be a really
big money spinner for Yahoo. But Gord,

963
00:55:24,610 --> 00:55:28,320
the ox of Yahoo's nascent mobile division.
Yeah. Who's nascent social division,

964
00:55:28,321 --> 00:55:31,410
Yahoo's nascent, um, a
photo division and so on.

965
00:55:31,710 --> 00:55:36,710
And the great beasts of Yahoo who had
the ear of the senior management who had

966
00:55:37,141 --> 00:55:38,880
assembled power structures around them,

967
00:55:39,000 --> 00:55:43,320
we're able to head off and starve flicker.
So then now it limps along.

968
00:55:43,321 --> 00:55:46,350
It's just been popped by smug mug and
remains to be seen what its future is,

969
00:55:46,530 --> 00:55:48,720
but it represents this
like failed promise.

970
00:55:48,721 --> 00:55:52,590
And this is a really common disaffiliate
NC of scale, right? It's it, you know,

971
00:55:52,591 --> 00:55:56,310
these firms get got bought up by other
firms who then just poison them or do

972
00:55:56,311 --> 00:55:59,070
terrible things with
them. You know, as I say,

973
00:55:59,071 --> 00:56:00,660
I think Google has done a
lot of good in this world,

974
00:56:00,661 --> 00:56:04,530
but I think that really effectively
burying the Deja News Archive of early use

975
00:56:04,531 --> 00:56:09,360
net, it's something between a crime in
a shame. You know, those are, that's,

976
00:56:09,510 --> 00:56:12,750
that's a really important piece of history
and it's like a disaffiliate NC scale.

977
00:56:12,751 --> 00:56:16,230
Like someone just felt like it just
wasn't important to the core business.

978
00:56:16,231 --> 00:56:18,600
Well then why did you
buy it? And, and so on.

979
00:56:18,601 --> 00:56:21,660
And eventually it was just kind
of starved off and vanished in a,

980
00:56:21,661 --> 00:56:24,280
down the memory hole. And so, um,

981
00:56:24,880 --> 00:56:29,530
one of the ways that you make firms better
is by making the amount of harm that

982
00:56:29,531 --> 00:56:33,010
they can do less, right?
Because then they can't then,

983
00:56:33,040 --> 00:56:35,650
then the bad things they
do aren't as important.

984
00:56:35,680 --> 00:56:38,530
And one of the ways that you make the
f the harm that they can do less is by

985
00:56:38,531 --> 00:56:42,160
mandating that they be smaller.
And historically,

986
00:56:42,730 --> 00:56:47,030
anytime we had an industry that was
dominated by a few firms or anytime a firm

987
00:56:47,031 --> 00:56:51,170
tried to buy a competitor or
anytime affirm, tried to buy, um,

988
00:56:51,280 --> 00:56:54,580
people in its vertical supply chain to
dominate it's vertical supply chain,

989
00:56:54,790 --> 00:56:56,200
we looked askance at that,
we struck,

990
00:56:56,230 --> 00:56:59,920
we subjected it to very close
scrutiny and we often blocked it.

991
00:56:59,921 --> 00:57:01,980
Sometimes we over blocked it.
There's an argument that the,

992
00:57:01,981 --> 00:57:05,920
one of the ways that we got here was
that there was a constituency for this

993
00:57:05,921 --> 00:57:08,380
story that antitrust had been
overused because it had been,

994
00:57:08,381 --> 00:57:12,880
and they were willing to hear arguments
for why it should be dialed down. But,

995
00:57:13,560 --> 00:57:15,640
uh, you know, I think that like,

996
00:57:17,520 --> 00:57:20,670
so I have a friend who went to work for
Facebook and then he quit a year later

997
00:57:20,730 --> 00:57:23,010
and came to talk to me and said,
you know, you're right. I didn't,

998
00:57:23,070 --> 00:57:24,060
I didn't like it very much.

999
00:57:24,330 --> 00:57:27,660
And one of the things that I realized
when I got there that maybe makes me sleep

1000
00:57:27,661 --> 00:57:30,390
better is that no one there
is any smarter than I am.

1001
00:57:30,391 --> 00:57:34,080
They're just as dumb as I am. They're
not superheroes. Um, and I'm like,

1002
00:57:34,140 --> 00:57:36,780
but of course they are right there.
They're just like you and me.

1003
00:57:37,140 --> 00:57:42,140
And the reason that Facebook is a bad
custodian of 2 billion people's social

1004
00:57:42,151 --> 00:57:46,320
lives is because nobody
has a good custodian of 2
billion people's social lives.

1005
00:57:46,530 --> 00:57:48,390
Right? Like, how do you,

1006
00:57:48,450 --> 00:57:53,250
how do you minimize the harm that Facebook
has in the toxic ways that an enables

1007
00:57:53,251 --> 00:57:57,930
people lives? Don't make it in charge of
2 billion people. Socialize, break it up,

1008
00:57:57,960 --> 00:58:02,370
make it into different pieces, make it
sell off Instagram, make it, um, you know,

1009
00:58:02,371 --> 00:58:03,150
make it split out.

1010
00:58:03,150 --> 00:58:06,660
The two functions that has one is like
helping you find people to talk with and

1011
00:58:06,661 --> 00:58:08,970
the other one is helping
you talk with them. Make it,

1012
00:58:08,971 --> 00:58:13,200
split those into two pieces because it
really sucks at like letting you talk to

1013
00:58:13,201 --> 00:58:16,920
people. Uh, and it's really
good at helping you find
people to talk to, you know,

1014
00:58:16,921 --> 00:58:19,980
whether those are people that want to
carry Tiki torches to Charlottesville or

1015
00:58:19,981 --> 00:58:22,050
people who want to, you know, uh,

1016
00:58:22,080 --> 00:58:24,480
like Joe form a little
league with you or whatever.

1017
00:58:24,481 --> 00:58:26,880
It's really good at finding those
people or people who have the same rare

1018
00:58:26,881 --> 00:58:28,710
diseases you,
it's really good at that.

1019
00:58:28,711 --> 00:58:32,340
It just sucks as a place to carry on the
conversation afterwards mostly because

1020
00:58:32,341 --> 00:58:35,580
like there's not a lot of engagement to
be had. If you have a rare disease, right?

1021
00:58:35,581 --> 00:58:37,740
You check in every day, things
are okay, things are okay.

1022
00:58:37,890 --> 00:58:41,190
There aren't blockbuster blockbuster news
that keeps you hanging out on the rare

1023
00:58:41,191 --> 00:58:44,760
disease message board all day. So to
you know, up your engagement level.

1024
00:58:44,940 --> 00:58:47,970
Facebook's to sufficiency of scale as
they take people who have gathered to talk

1025
00:58:47,971 --> 00:58:51,390
about a rare disease and
they throw clickbait at them
so that they stay engaged

1026
00:58:51,391 --> 00:58:54,060
because their KPI and their
bonus is on engagement minutes,

1027
00:58:54,210 --> 00:58:56,760
not whether or not you're successfully
managing your rare disease,

1028
00:58:56,940 --> 00:58:59,820
make them split those two functions up.
You solve the problem.

1029
00:58:59,880 --> 00:59:03,420
Right now you have a business that just
gets monotonically better at helping you

1030
00:59:03,421 --> 00:59:07,260
find people to talk with and a business
that rises or syncs on its ability to

1031
00:59:07,261 --> 00:59:11,010
get you to talk with them there and,
and then you limit a lot of the harm.

1032
00:59:11,330 --> 00:59:16,040
It is a bit of a counter example. So at
Google for example, the primary income is,

1033
00:59:16,041 --> 00:59:20,800
is ads. Sure. Was historically, now
it's changing, but um, you know,

1034
00:59:20,801 --> 00:59:23,870
that has resulted in,
you know,

1035
00:59:23,871 --> 00:59:28,871
things like Gmail and docs and drive
essentially operating at a loss,

1036
00:59:30,140 --> 00:59:34,910
right? And being subsidized
by the successful parts of
the business. Yeah. Yeah.

1037
00:59:35,300 --> 00:59:37,870
But Google's not a breakeven
function, right? You've probably,

1038
00:59:37,930 --> 00:59:42,410
you probably have some like a
stock options as an employee.

1039
00:59:42,500 --> 00:59:46,940
So you've noticed that the company pays
dividends and also declares a profit

1040
00:59:46,941 --> 00:59:47,840
every year and it,

1041
00:59:47,960 --> 00:59:52,760
and so that tells you that even if the
ad business were curtailed by a breakup,

1042
00:59:53,270 --> 00:59:57,710
it would still not necessarily mean
that the company was unable to run those

1043
00:59:57,711 --> 01:00:01,400
other, those other, uh,
uh, loss leaders. Right.

1044
01:00:01,401 --> 01:00:05,570
It just might mean that the
shareholders took a, took a haircut. Um,

1045
01:00:05,600 --> 01:00:08,960
and you know, one of the, there are lots
of formal definitions of corruption,

1046
01:00:08,961 --> 01:00:11,600
but one of the formal definitions that
corruption is when you have privatized

1047
01:00:11,601 --> 01:00:15,620
gains and socialize losses, right. You
know, it's, it's cheap for me to pollute.

1048
01:00:15,621 --> 01:00:19,580
It's expensive for you to
get the pollution out of
your tap water. Uh, but that,

1049
01:00:19,610 --> 01:00:21,800
that expense is diffused
across everyone who's,

1050
01:00:21,801 --> 01:00:25,220
who's putting filters on their tap water
and the gains are concentrated in my

1051
01:00:25,221 --> 01:00:27,110
hands. Well, you know,

1052
01:00:27,111 --> 01:00:31,940
like if the costs of surveillance,

1053
01:00:31,941 --> 01:00:36,710
which are real, uh, are widely diffused
and the games are concentrated,

1054
01:00:37,070 --> 01:00:39,040
um, it may be that, uh,

1055
01:00:39,080 --> 01:00:43,550
making the firm's internalize some of
those costs dial down some of the other

1056
01:00:43,551 --> 01:00:44,630
things that they can do,

1057
01:00:44,930 --> 01:00:49,250
but it also reduces this drag
that the rest of us are feeling.

1058
01:00:49,251 --> 01:00:53,720
So maybe with the surplus that we gain
from being lifted out of the costs of

1059
01:00:53,721 --> 01:00:57,380
surveillance or market
domination or whatever, or the,
all the other things that,

1060
01:00:57,560 --> 01:00:59,480
that come as a result of it,
um,

1061
01:00:59,510 --> 01:01:04,510
that that surplus can be allocated to
make up for the losses that we get.

1062
01:01:04,610 --> 01:01:05,150
You know,

1063
01:01:05,150 --> 01:01:10,150
it's totally true that iPhone
locked ecosystems allow us to gain,

1064
01:01:12,050 --> 01:01:16,220
got some benefits that would be eroded
if we unlock that ecosystem by forced

1065
01:01:16,221 --> 01:01:19,280
mature.
But I'm willing to make that trade.

1066
01:01:19,310 --> 01:01:22,850
It's totally true that our printers are
cheaper because our inkjet cartridges

1067
01:01:22,851 --> 01:01:26,720
are designed to charge us more than
vocally co for water in town, you know,

1068
01:01:26,721 --> 01:01:29,100
water and pigment. Right. Um,

1069
01:01:29,220 --> 01:01:32,540
I'm willing to like roll
the dice on that one. Right.

1070
01:01:32,541 --> 01:01:37,541
And find out what happens if it turns
out that like we no longer charge for

1071
01:01:38,180 --> 01:01:41,250
carbon toner as though
what we're plutonium tone.

1072
01:01:42,410 --> 01:01:45,920
But do you have an eyeball on your sock?
I was trying to figure out, whoa. No,

1073
01:01:45,921 --> 01:01:49,850
it's clockwork orange.
Uh, okay. Yeah, yeah.

1074
01:01:49,880 --> 01:01:51,950
We're at like peak bookstore sock.

1075
01:01:52,340 --> 01:01:53,810
So I don't know if you've
been into a bookstore,

1076
01:01:53,811 --> 01:01:57,020
but like that's where all the margins
are now. You talk about ad subsidy,

1077
01:01:57,230 --> 01:02:00,830
like the entire literary world is being
subsidized by the fact that we can now

1078
01:02:00,831 --> 01:02:05,180
programmatically map bitmaps maps onto
socks using weaving machines and China

1079
01:02:05,570 --> 01:02:08,900
and I went on a couple of book
tours in the last two years.

1080
01:02:08,901 --> 01:02:09,890
I'm about to go on another one.

1081
01:02:09,891 --> 01:02:13,280
I spent a lot of time in indie
bookstores and I have all the socks.

1082
01:02:13,520 --> 01:02:15,920
Are their Doctorow socks a no.

1083
01:02:15,921 --> 01:02:19,980
Someone should make those and then I
can sue them for my right of publicity.

1084
01:02:22,800 --> 01:02:24,810
Thank you. Thank you.
And if you'd like socks,

1085
01:02:24,811 --> 01:02:29,100
there's a great sock store not far
from here near Angel City books.

1086
01:02:29,370 --> 01:02:31,680
That's in that,
that's in the Venice beach sock district.

1087
01:02:32,650 --> 01:02:37,290
It's in the old elk's lodge.
All right. Thanks again guys.

1088
01:02:37,640 --> 01:02:40,200
[inaudible].

