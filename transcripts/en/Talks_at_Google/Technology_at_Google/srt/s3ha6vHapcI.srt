1
00:00:06,030 --> 00:00:08,520
I assume since you're here,
uh,

2
00:00:08,550 --> 00:00:11,430
that most of you know who I am,

3
00:00:11,580 --> 00:00:14,490
but just for a little
bit of context setting,

4
00:00:14,491 --> 00:00:19,491
I am going to talk a little bit about
the background of my company and why

5
00:00:19,861 --> 00:00:20,694
that's relevant.

6
00:00:20,940 --> 00:00:25,140
Because the book that I've written a is
a combination of a memoir about my time

7
00:00:25,141 --> 00:00:28,230
in the technology industry,
a business book,

8
00:00:28,260 --> 00:00:31,770
and an economic call to
action now on the economics.

9
00:00:32,280 --> 00:00:36,740
How was my thesis advisor? Uh,
the book was, it was, it was, uh,

10
00:00:37,110 --> 00:00:39,990
uh, really, uh, uh, uh,

11
00:00:40,100 --> 00:00:43,650
I don't really know that much
about economics, although
a couple of people said,

12
00:00:43,651 --> 00:00:48,090
well, but your instincts are good. Uh, but
how, uh, corrected my instincts in many,

13
00:00:48,091 --> 00:00:49,080
many cases,
uh,

14
00:00:49,081 --> 00:00:53,250
but any errors in the economics
portions are not house responsibility,

15
00:00:53,550 --> 00:00:54,600
but he definitely would,

16
00:00:54,660 --> 00:00:57,720
it would read a passage he read
multiple times and he would say, no,

17
00:00:57,721 --> 00:01:00,540
you have to go read this paper. Oh,
you have to go read this paper. Oh,

18
00:01:00,541 --> 00:01:04,240
you have to read, go read this paper.
It was a wonderful education and, uh,

19
00:01:04,300 --> 00:01:07,830
I hope to continue it as I go
further down this path. Anyway,

20
00:01:07,850 --> 00:01:11,340
so I'm the founder and CEO of
O'reilly media. Uh, you know,

21
00:01:11,341 --> 00:01:16,260
most of you probably maybe
first, uh, new of us,

22
00:01:16,310 --> 00:01:20,040
uh, as a publisher of the iconic
animal books. This book now,

23
00:01:20,580 --> 00:01:23,290
I think it's still in its sixth edition,
VI and Van,

24
00:01:23,430 --> 00:01:27,800
the first edition was in 1985 was one
of the first books I ever published. Uh,

25
00:01:27,840 --> 00:01:30,690
I think it was in an
addition of 100 copies a,

26
00:01:30,691 --> 00:01:34,110
it's gone on to sell probably a million
copies since then. It's still in print.

27
00:01:34,490 --> 00:01:39,330
Uh, you know, however many years later,
32 years later. Uh, but you know,

28
00:01:39,331 --> 00:01:41,830
we continue to be a publisher with this.

29
00:01:41,880 --> 00:01:45,240
Here's a book that we did recently with
Google on site reliability engineering,

30
00:01:45,480 --> 00:01:46,313
another book on,

31
00:01:46,660 --> 00:01:50,730
on a hands on machine learning with
psychic learn and tensor flow. You know,

32
00:01:50,731 --> 00:01:55,260
these are kind of the thing that a
lot of people in the computer industry

33
00:01:55,570 --> 00:01:59,850
learned of us from. It's now only
about 20% of our business. Uh,

34
00:02:00,060 --> 00:02:05,060
but the thing I'm kind of the proudest
of is the role that I've had in spreading

35
00:02:05,401 --> 00:02:09,540
big ideas about where the industry
is going and what it ought to do.

36
00:02:09,960 --> 00:02:12,720
Um, so very early in the nineties, uh,

37
00:02:12,840 --> 00:02:15,060
my company created the
first commercial website,

38
00:02:15,600 --> 00:02:18,840
the site called the global network
navigator was sort of before Yahoo.

39
00:02:18,841 --> 00:02:21,900
It was the first web portal.
It was also the first, uh,

40
00:02:21,930 --> 00:02:24,480
advertising supported
site on the worldwide web.

41
00:02:24,900 --> 00:02:29,900
The Internet was still noncommercial
and did a lot of activism for, uh,

42
00:02:30,540 --> 00:02:33,230
uh, the commercial internet and, uh,

43
00:02:33,240 --> 00:02:36,240
also for the open commercial
internet in the very early days.

44
00:02:36,510 --> 00:02:39,570
For those of you who weren't around then
the Internet was a research network.

45
00:02:39,571 --> 00:02:42,090
And I still remember a conversation
I had with Steve Wolff,

46
00:02:42,390 --> 00:02:45,540
who at the time was the NSF overseer,
National Science Foundation,

47
00:02:45,541 --> 00:02:49,830
overseer of the Internet. And
he said, uh, I said, well,

48
00:02:49,860 --> 00:02:51,060
here's what we're planning on doing.

49
00:02:51,061 --> 00:02:53,490
We're going to build this
website and they will have ads.

50
00:02:53,491 --> 00:02:56,400
But the thing that's different about
the web is that people come to you.

51
00:02:56,401 --> 00:02:59,200
So we're not going to be sending out
anything unless people ask for it.

52
00:02:59,201 --> 00:03:02,710
So it's very different. And
he said, well, you know,

53
00:03:02,920 --> 00:03:07,660
the Internet is about research
and education and if you
guys aren't research and

54
00:03:07,661 --> 00:03:11,930
education, I don't know who is, so go for
it. And it was a wonderful moment. Anyway,

55
00:03:12,050 --> 00:03:16,090
later I organized a meeting where
the term open source software was,

56
00:03:16,150 --> 00:03:20,590
it was widely adopted and promoted it
and particularly told a story about it

57
00:03:20,591 --> 00:03:25,000
that was bigger than the political
movement about free software,

58
00:03:25,001 --> 00:03:28,900
a being against Microsoft by bringing in
the story of why the Internet was also

59
00:03:28,901 --> 00:03:30,550
built on top of open source software.

60
00:03:31,060 --> 00:03:36,060
And that was probably my first experience
of the power of ideas to change

61
00:03:36,281 --> 00:03:39,670
people's minds. You know,
I remember when I first,

62
00:03:40,270 --> 00:03:43,420
I held this press conference at the end
of this day that came to be called the

63
00:03:43,421 --> 00:03:44,320
open source summit.

64
00:03:44,710 --> 00:03:49,390
And I had all these guys up on a stage
and people who nobody had ever heard of,

65
00:03:49,750 --> 00:03:54,430
uh, most of them, you know, and, and
the story I told was so different,

66
00:03:54,550 --> 00:03:54,761
you know,

67
00:03:54,761 --> 00:03:59,200
which had been this story of free software
is this rebel movement that wants to

68
00:03:59,201 --> 00:04:02,890
bring down commercial software. Commercial
software is evil. And I said, hey,

69
00:04:03,130 --> 00:04:06,610
you know, if you guys have uh, you know,

70
00:04:06,660 --> 00:04:09,760
are on the internet and you have a domain
name, yeah. You from the New York Times,

71
00:04:09,761 --> 00:04:13,330
you from the Wall Street Journal or
whatever, you know, this guy over here,

72
00:04:13,331 --> 00:04:18,250
Paul Vicksey wrote the software and gave
it away so that that domain name, uh,

73
00:04:18,251 --> 00:04:20,980
you know, it can be recognized.
Uh, oh, if you send email,

74
00:04:20,981 --> 00:04:22,500
this Guy Eric Almont wrote,

75
00:04:22,501 --> 00:04:26,710
send mail the program that routes at
that point about 70% of all the email on

76
00:04:26,711 --> 00:04:29,050
the Internet, Oh, if you have a
website is probably a patchy. This guy,

77
00:04:29,051 --> 00:04:31,310
Brian Behlendorf started that.
So it kind of,

78
00:04:31,690 --> 00:04:35,080
and it was interesting because I did
about two weeks worth of interviews and in

79
00:04:35,081 --> 00:04:38,890
the beginning what the Internet
is based on free software.

80
00:04:39,350 --> 00:04:43,930
And it was this disbelief and it felt a
little bit like you're trying to push,

81
00:04:43,990 --> 00:04:47,710
you know, something really heavy and it
doesn't move and then it starts to move.

82
00:04:47,711 --> 00:04:51,840
And within two weeks it was just
the accepted wisdom and that, uh,

83
00:04:51,900 --> 00:04:56,650
in in a way is a backdrop to the story
of the book because I'm trying to change

84
00:04:56,651 --> 00:05:01,240
the accepted wisdom again with this book.
So I did it again with web two. Dot.

85
00:05:01,241 --> 00:05:02,200
O uh,

86
00:05:02,201 --> 00:05:07,201
which is really the story for me of
what came back after the s the.com bust.

87
00:05:07,270 --> 00:05:08,950
What was the second coming of the web?

88
00:05:09,310 --> 00:05:12,910
And they come to the conclusion from
thinking about open source in a very

89
00:05:12,911 --> 00:05:16,270
different way than other
people had that uh,

90
00:05:17,340 --> 00:05:21,070
the west, something very different
about the companies that survived.

91
00:05:21,520 --> 00:05:24,730
And of course this led me down the path
that we're really moving out of the

92
00:05:24,731 --> 00:05:26,490
world of,
uh,

93
00:05:26,560 --> 00:05:31,150
that we knew in the PC era where software
was an artifact to where software was

94
00:05:31,151 --> 00:05:33,340
increasingly a business process that is,

95
00:05:33,350 --> 00:05:37,690
were these vast cloud applications that
actually had people inside of them.

96
00:05:38,140 --> 00:05:40,960
And that data was going to be the
source of competitive advantage.

97
00:05:40,961 --> 00:05:42,960
And that was the heart of what I
talked about. Web Two. Dot. Oh,

98
00:05:43,390 --> 00:05:47,230
we also launched something called the
maker Movement with a magazine in 2004

99
00:05:47,231 --> 00:05:49,300
called make and maker faire.

100
00:05:50,020 --> 00:05:53,710
I spent a lot of time in the last
seven or eight years talking about how

101
00:05:53,711 --> 00:05:57,410
government also needs to learn about
platforms from the technology industry.

102
00:05:57,410 --> 00:06:01,040
And most recently I've been talking a
lot in the same way about how to think

103
00:06:01,041 --> 00:06:04,430
differently about AI and what
I called the next economy.

104
00:06:04,700 --> 00:06:08,510
It's a technology and the future
of work. So we also, you know,

105
00:06:08,960 --> 00:06:11,990
a big part of our business today or
conferences we just had last week,

106
00:06:12,020 --> 00:06:14,750
the O'Reilly Ai Conference
this week in New York,

107
00:06:14,751 --> 00:06:18,860
this strata of the business of data. We
doing a conference on Jupiter. We've,

108
00:06:18,920 --> 00:06:21,440
our original very first conference
started in [inaudible] 97,

109
00:06:21,441 --> 00:06:25,350
the O'Reilly open source software
summit. We run makerfair, uh,

110
00:06:25,400 --> 00:06:28,580
something called bootcamp, which Hal
talked about, which is an unconference.

111
00:06:29,090 --> 00:06:33,880
And then we have a platform of our
own, which we started in 2000, uh,

112
00:06:33,890 --> 00:06:36,530
it's called Safari was
originally an ebook platform,

113
00:06:36,531 --> 00:06:40,940
but it's increasingly a platform.
Uh, yes, 40,000 plus Ebooks,

114
00:06:41,090 --> 00:06:44,060
tens of thousands of hours of
video training, live training,

115
00:06:44,061 --> 00:06:47,000
millions of customers. It's really
a platform for knowledge exchange.

116
00:06:47,000 --> 00:06:50,630
And that's really the heart
of our business. We're
about 20% books, 30% events,

117
00:06:50,930 --> 00:06:54,410
and 50% this,
this online learning platform.

118
00:06:55,340 --> 00:06:58,820
So I spent a lot of time thinking about
platforms because that really is the

119
00:06:58,821 --> 00:06:59,750
heart of my business.

120
00:06:59,820 --> 00:07:03,800
And I think that Google also
obviously is a platform company,

121
00:07:04,040 --> 00:07:05,630
but I want to spend a lot of time,

122
00:07:05,690 --> 00:07:09,470
I spend a lot of time in the book thinking
about digital platforms and what they

123
00:07:09,471 --> 00:07:13,670
teach us about economies. I said,
I'm going to come back to that.

124
00:07:13,970 --> 00:07:18,030
So the title of the
Book Wtf, you know, uh,

125
00:07:18,350 --> 00:07:21,530
now I actually use this and the White
House frontiers conference and I was

126
00:07:21,531 --> 00:07:25,190
really proud that I got the White House
comms team to sign off on a talk called

127
00:07:25,191 --> 00:07:29,360
WTF. Uh, but I, and I did
it by kind of saying, well,

128
00:07:29,361 --> 00:07:31,130
it's stands for what's
the future of course.

129
00:07:31,220 --> 00:07:36,220
But really the reason why I wanted to use
the term WTF is because it's a term of

130
00:07:36,531 --> 00:07:41,531
astonishment that can be the astonishment
of delight or the astonishment of

131
00:07:41,871 --> 00:07:42,704
dismay.

132
00:07:43,340 --> 00:07:48,340
And I think that really encapsulates the
state of our dialogue about technology

133
00:07:50,781 --> 00:07:55,370
today is this source of enormous
astonishment and wonder.

134
00:07:55,550 --> 00:07:57,560
And it is also a source of fear.

135
00:07:58,220 --> 00:08:01,560
And I started worrying about this,
uh,

136
00:08:02,600 --> 00:08:04,230
probably two or three years ago,
I started a,

137
00:08:04,310 --> 00:08:07,940
an event called the next economy summit
because I was trying to get ahead of

138
00:08:07,941 --> 00:08:11,150
this issue. How do we think
about technology and the economy?

139
00:08:11,420 --> 00:08:16,420
How do we get technologists to think about
it and to talk about it in a way that

140
00:08:16,671 --> 00:08:18,200
doesn't make people afraid?

141
00:08:18,380 --> 00:08:23,380
How do we get business plans that are
focused on empowering people and building

142
00:08:24,170 --> 00:08:27,530
wealth for everyone rather than simply,
well, we're just going to disrupt,

143
00:08:28,040 --> 00:08:29,930
you know, we're going to
break things. You know,

144
00:08:29,931 --> 00:08:31,850
we're going to make ourselves really,

145
00:08:31,851 --> 00:08:34,970
really rich and we're not gonna really
worry about what happens to everybody

146
00:08:34,971 --> 00:08:38,390
else. And that's how the rest of the
world is starting to see Silicon Valley.

147
00:08:38,850 --> 00:08:43,850
And a lot of ways the book is an attempt
to address that narrative head on and

148
00:08:44,121 --> 00:08:48,800
to change it. Uh, and, and to
change it by actually inspiring,

149
00:08:49,610 --> 00:08:53,750
uh, software developers and entrepreneurs
to act differently, to talk differently,

150
00:08:53,960 --> 00:08:57,300
but also to persuade policy makers
to think and act differently.

151
00:08:57,630 --> 00:09:00,900
So I'm going to try to give you a few
sampling of some of the ideas from the

152
00:09:00,901 --> 00:09:05,040
book anyway,
back to this WTF of amazement or dismay.

153
00:09:05,340 --> 00:09:08,760
This is the world of techno
optimism that we all live in.

154
00:09:09,280 --> 00:09:12,210
That's a chart of life expectancy,

155
00:09:14,070 --> 00:09:16,800
life expectancy at birth,
pretty much flat. I mean,

156
00:09:16,801 --> 00:09:21,180
there's a few really bad times you can
kind of see where it really dropped,

157
00:09:21,450 --> 00:09:25,560
but then suddenly this magical thing
happened in the mid 18 hundreds when it

158
00:09:25,561 --> 00:09:29,970
started to climb. And then
you see additional countries
sort of come on stream.

159
00:09:30,320 --> 00:09:35,320
This is the modern world that we have
every reason to be so proud of, you know,

160
00:09:35,340 --> 00:09:39,330
that we, you know, there's a wonderful
site, our world and data, which is really,

161
00:09:39,331 --> 00:09:44,280
it's this incredible collection of
graphs and narratives about the way that

162
00:09:44,281 --> 00:09:47,370
technology is making the world better.
I'm standing in front of this,

163
00:09:47,460 --> 00:09:52,210
sorry about that guys. Um, so I
guess I'll be back over here. Uh,

164
00:09:53,280 --> 00:09:57,120
so, you know, but everyone
is not equally happy.

165
00:09:57,121 --> 00:10:01,530
We see all Brett's Brexit,
the rise of Trump. And here's,
here's this WTF, you know,

166
00:10:01,531 --> 00:10:06,030
on the Daily Telegraph,
you know, um, and you know,

167
00:10:06,060 --> 00:10:10,830
the question is this is not the first
time when we have had this kind of upset

168
00:10:10,831 --> 00:10:13,500
and worry about technology.
This is a, you know,

169
00:10:13,501 --> 00:10:15,840
an etching about the luddite rebellion.
Now,

170
00:10:15,841 --> 00:10:19,470
one of the things I learned that many
of you may not know is that Ned Lud did

171
00:10:19,471 --> 00:10:23,310
not actually exist. Uh, he, he was
not the leader of the Luddites.

172
00:10:23,460 --> 00:10:27,870
He was a mythical figure that this
particular revolution in 18, 11, 18,

173
00:10:27,871 --> 00:10:32,190
12 cited. He was somebody who
had apparently had smashed
Allume 30 years before.

174
00:10:32,580 --> 00:10:36,740
And it went down his story and they kind
of carried the banner of Ned Lud. Uh,

175
00:10:36,870 --> 00:10:37,591
but here's the point.

176
00:10:37,591 --> 00:10:42,000
These guys were right to be afraid
because the ensuing years were pretty bad.

177
00:10:42,090 --> 00:10:42,391
You know,

178
00:10:42,391 --> 00:10:47,391
and you think about the early years
of the industrial revolution and,

179
00:10:48,361 --> 00:10:52,780
uh, you know, William Blake's description
of the dark satanic mills, uh, you know,

180
00:10:52,800 --> 00:10:56,400
this was not a good time,
but you know,

181
00:10:56,580 --> 00:11:01,580
those weavers could not imagine
the wealth of modern society.

182
00:11:02,280 --> 00:11:05,960
They couldn't imagine that their
descendants would, you know,

183
00:11:05,970 --> 00:11:09,780
this production is mass
production of fabric, you know,

184
00:11:09,781 --> 00:11:14,781
would produce a world in
which their grandchildren
and great grandchildren would

185
00:11:15,061 --> 00:11:18,750
have more clothing than the kings
and Queens of Europe. Did, you know,

186
00:11:19,520 --> 00:11:23,510
the turn of the 18th century or
the 19th century rather, you know,

187
00:11:23,640 --> 00:11:27,930
they couldn't imagine, you know, their
descendants would have, you know,

188
00:11:28,530 --> 00:11:30,450
fruit in the middle of winter.

189
00:11:30,451 --> 00:11:35,040
They couldn't imagine that
we would actually build
skyscrapers half a mile high.

190
00:11:35,041 --> 00:11:40,041
That we dig a tunnel under
the English Channel to France,

191
00:11:40,171 --> 00:11:42,480
that we'd go into space,
that we'd fly through the air.

192
00:11:42,480 --> 00:11:47,480
All these things are amazing and they
couldn't imagine that their descendants

193
00:11:48,271 --> 00:11:52,380
would find so much meaningful work
bringing all these things to life.

194
00:11:52,950 --> 00:11:57,950
And so one of the questions I'm asking
in the face of today's world of AI and

195
00:12:00,190 --> 00:12:03,160
all of these new technologies
that were being told again, again,

196
00:12:03,161 --> 00:12:07,690
are going to destroy jobs.
What does our failure of imagination,

197
00:12:07,930 --> 00:12:12,460
what are we not able to imagine and what
world are we not able to paint for our

198
00:12:12,461 --> 00:12:15,850
grandchildren of the world
to come? And so, you know,

199
00:12:15,851 --> 00:12:19,780
one of the things that I start the book
with is a little excursion into this

200
00:12:19,781 --> 00:12:21,740
idea of fitness landscapes. Now, you know,

201
00:12:21,770 --> 00:12:25,600
in evolutionary biology is this idea
that genes contribute to survival and

202
00:12:25,601 --> 00:12:26,021
organism,

203
00:12:26,021 --> 00:12:29,930
that you can think about this as kind
of a landscape of peaks and valleys and

204
00:12:30,310 --> 00:12:34,960
organisms evolve towards the peaks which
are adapted to their environment or

205
00:12:34,961 --> 00:12:38,800
they die out. And so there's
this concept of a local maximum.

206
00:12:39,520 --> 00:12:40,353
And you know,

207
00:12:40,360 --> 00:12:45,360
a lot of what happens is society and
companies get comfortable with this local

208
00:12:45,401 --> 00:12:48,610
maximum and they don't
know how to move on.

209
00:12:48,790 --> 00:12:53,020
And often the only way to move off of it
is actually to go backwards. You know,

210
00:12:53,040 --> 00:12:56,010
you have to go down. And that's why
we have this cycle sometimes of,

211
00:12:56,210 --> 00:13:01,000
of revolutions of, of, you know, companies
you know, fall apart anyway. But,

212
00:13:01,030 --> 00:13:04,900
uh, you know, fitness landscapes
are, you know, dynamic. You know,

213
00:13:05,100 --> 00:13:07,300
when conditions are stable,
you can kind of just stay there.

214
00:13:07,301 --> 00:13:11,860
But if things are changing, uh, not,
not so, so good. And of course we have,

215
00:13:12,130 --> 00:13:14,380
uh, you know, radically
changing conditions. You know,

216
00:13:14,410 --> 00:13:16,330
climate change is a great example.

217
00:13:16,420 --> 00:13:18,970
If you look at the failure of
many civilizations in the past,

218
00:13:18,971 --> 00:13:20,350
they were driven by climate change.

219
00:13:20,440 --> 00:13:25,440
We may face a great deal of pressure on
our fitness landscape today as a result

220
00:13:27,641 --> 00:13:32,640
of that. Uh, and technology also as
a fitness landscape, you know, uh,

221
00:13:32,650 --> 00:13:34,240
you know, in my career
I watched, you know,

222
00:13:34,241 --> 00:13:37,660
this fitness landscape of the
personal computer, uh, you know,

223
00:13:37,661 --> 00:13:40,980
the big data and AI world,
uh, that, that, you know,

224
00:13:40,990 --> 00:13:45,390
Google lives in the smartphone, a
landscape dominated. That was, yes,

225
00:13:45,391 --> 00:13:49,360
a really broken open by apple and now
it was sort of a subject of fierce

226
00:13:49,361 --> 00:13:52,410
competition between Google and apple.
And you know,

227
00:13:52,450 --> 00:13:56,620
what's really interesting and when
you think about what happened,

228
00:13:56,621 --> 00:14:00,280
why was it hard for Microsoft to
get to the big data world? Well,

229
00:14:00,281 --> 00:14:02,160
they had too much,
you know,

230
00:14:02,170 --> 00:14:04,540
this is a perfect illustration
of the fitness landscape.

231
00:14:04,570 --> 00:14:09,460
They had a business model that really
worked for them and they fought.

232
00:14:09,490 --> 00:14:11,890
Yeah. There were people who were saying,
no, we got to get with the Internet.

233
00:14:11,891 --> 00:14:14,110
And they were kind of like, no,
we have to preserve windows.

234
00:14:14,110 --> 00:14:17,080
And that was their priority. And I think
it's something that we always have to,

235
00:14:17,110 --> 00:14:19,660
to uh, to think about the, this. Dot.

236
00:14:19,880 --> 00:14:23,290
The dominant companies
tend to be slow to adapt.

237
00:14:24,520 --> 00:14:27,120
So, um, you know, the,

238
00:14:27,180 --> 00:14:32,020
the thing is that one of the problems
is that those dominant companies tend to

239
00:14:32,021 --> 00:14:35,760
extract too much of the value from the
ecosystem for themselves. You know,

240
00:14:35,761 --> 00:14:38,650
Microsoft basically put other
companies out of business.

241
00:14:38,651 --> 00:14:41,170
They would come down to silicon valley,
meet with vcs and say,

242
00:14:41,171 --> 00:14:44,420
you can't invest there because
we're going to do that. Uh, and,

243
00:14:44,421 --> 00:14:49,000
and this is a real risk, I
think for Google as well. You
know, it, it, it's really,

244
00:14:49,180 --> 00:14:53,660
I think a mistake to think,
uh,

245
00:14:53,720 --> 00:14:56,090
just value for users,
you know,

246
00:14:56,091 --> 00:14:59,960
because Google will make a case what
we're doing this thing and it's better for

247
00:14:59,961 --> 00:15:04,010
our customers. But you actually have
to think about the whole ecosystem.

248
00:15:04,010 --> 00:15:07,910
And in particular, you have to think about
the ecosystem of developers. You know,

249
00:15:07,920 --> 00:15:09,390
the people you know,

250
00:15:09,400 --> 00:15:13,310
went to Lennox and the worldwide web
because there was no room left in the

251
00:15:13,311 --> 00:15:15,470
Microsoft ecosystem.
So they went somewhere,

252
00:15:15,471 --> 00:15:17,990
they didn't think they were going to
go make a lot of money over on the web.

253
00:15:17,991 --> 00:15:22,380
They just went, this is cool, this is
interesting. And it's free and open. And,

254
00:15:22,520 --> 00:15:26,990
and that sort of free and open was what
gave birth to the new fitness landscape.

255
00:15:26,991 --> 00:15:27,570
You know, the, the,

256
00:15:27,570 --> 00:15:31,820
the small mammals coming out from the
valleys into this new fitness peak.

257
00:15:32,090 --> 00:15:35,600
So again, it's always something to worry
about if you're as dominant as Google,

258
00:15:35,890 --> 00:15:38,210
uh, is, is to think about, you know,

259
00:15:38,390 --> 00:15:43,280
are we making enough value for these
developers who are part of our ecosystem,

260
00:15:43,281 --> 00:15:46,760
these people who are creating on top
of our platform and making it a rich,

261
00:15:46,761 --> 00:15:51,500
stable ecosystem? Or are we basically
saying, no, no, we're, you know,

262
00:15:51,501 --> 00:15:53,590
we, we have to do that
for ourselves. You know,

263
00:15:53,600 --> 00:15:56,270
I had that experience many times with
Microsoft, you know, it's like, oh,

264
00:15:56,271 --> 00:15:59,840
that's a great idea. We
have to do that. Sorry. Uh,

265
00:16:00,110 --> 00:16:04,940
so in our, in our political landscape,
we're seeing this same thing.

266
00:16:05,420 --> 00:16:10,250
You know, we've had this
idea that by making a small
number of people very, very,

267
00:16:10,251 --> 00:16:11,520
very wealthy,
uh,

268
00:16:11,570 --> 00:16:15,620
it would trickle down to the rest of the
society has an entirely work that way.

269
00:16:15,920 --> 00:16:20,450
And guess what, you know, the people are
moving somewhere else. They're saying,

270
00:16:20,451 --> 00:16:24,200
we don't like that consensus
about how to run the world. Uh,

271
00:16:24,201 --> 00:16:27,990
we're going to do it differently. They may
even be wrong. And maybe, you know, well,

272
00:16:28,950 --> 00:16:33,380
not in my opinion, I even a
May, but, uh, you know, uh,

273
00:16:33,590 --> 00:16:34,130
uh,

274
00:16:34,130 --> 00:16:39,130
there's a real serious risk that the
conditions that we have know that we

275
00:16:39,741 --> 00:16:42,470
silicon valley have used to thrive,

276
00:16:42,500 --> 00:16:46,910
are going to change radically because
of the political environment. And so,

277
00:16:48,040 --> 00:16:52,150
you know, one of the key pieces
of advice I give him my book is,

278
00:16:52,151 --> 00:16:56,720
is just to remember the
successful ecosystem creates
opportunity for everyone.

279
00:16:57,230 --> 00:17:00,530
And, and this obviously was
brought into the, you know,

280
00:17:00,590 --> 00:17:05,210
the political discourse by Thomas
Piketty's book capital in the 21st century

281
00:17:05,211 --> 00:17:08,780
where he really addressed this question
of inequality and got everybody talking

282
00:17:08,781 --> 00:17:10,190
about it. There had been
other people, you know,

283
00:17:10,191 --> 00:17:14,660
Joe Stieglitz had written about the 99%
versus the 1% before picket aid come out.

284
00:17:14,661 --> 00:17:18,070
But, and that, that almost
caught, but you know, you know,

285
00:17:18,280 --> 00:17:21,230
when [inaudible] book came out, it was
like everybody started talking about it.

286
00:17:21,650 --> 00:17:26,650
And so I decided to take a technological
angle on all of this to kind of tell

287
00:17:28,791 --> 00:17:30,950
the story from the point of
view of the tech industry.

288
00:17:30,951 --> 00:17:35,480
Because a lot of people talking about it
as economists and I was so in a lot of

289
00:17:35,481 --> 00:17:37,100
ways,
the heart of the book is,

290
00:17:37,460 --> 00:17:42,440
is a series of stories about what the
great technology platforms have to tell us

291
00:17:42,470 --> 00:17:44,660
about the future of
business and the economy.

292
00:17:45,230 --> 00:17:47,030
And the first is this
point I've already made.

293
00:17:47,031 --> 00:17:49,470
The platforms have to work
for all of their participants,

294
00:17:49,770 --> 00:17:54,770
not just for the users or the platform
owner is also the point that platforms

295
00:17:55,591 --> 00:18:00,591
today are no longer just about the
digital right there in mashed in the real

296
00:18:01,591 --> 00:18:05,850
world. You know, you think
about Uber or Lyft, you know,

297
00:18:06,030 --> 00:18:10,740
this is really this system. You think
about this huge algorithmic system.

298
00:18:11,010 --> 00:18:14,880
And sure, there are people at the end
points of all the touch points of Google.

299
00:18:14,881 --> 00:18:15,031
You know,

300
00:18:15,031 --> 00:18:18,870
people are uploading youtube videos or
they're clicking on links are there,

301
00:18:18,880 --> 00:18:21,990
they're submit, you know,
creating webpages. But boy,

302
00:18:21,991 --> 00:18:26,880
it becomes really obvious
that human beings are a,

303
00:18:26,970 --> 00:18:28,670
as this, uh, Sean McMullan,

304
00:18:28,680 --> 00:18:32,090
the science fiction authors are our
souls in the great machine. You know,

305
00:18:32,100 --> 00:18:36,990
when you have these cars dispatched
by algorithm with drivers,

306
00:18:36,991 --> 00:18:40,420
you have the swarming marketplace
where people are doing things in the,

307
00:18:40,440 --> 00:18:45,030
in the real world. You're at the
Beck and call of algorithms. So, uh,

308
00:18:45,060 --> 00:18:49,680
so, you know, we have to start
thinking about, you know,

309
00:18:49,681 --> 00:18:53,610
this interpenetration of the digital
into all of our business processes,

310
00:18:53,611 --> 00:18:57,000
into all of our world.
Uh, you know, as, uh,

311
00:18:57,030 --> 00:19:02,030
as we effectively take the principles
that we've been practicing in the purely

312
00:19:02,041 --> 00:19:04,180
digital realm for,
uh,

313
00:19:04,290 --> 00:19:08,100
the last couple of decades and start to
see them show up everywhere in the real

314
00:19:08,101 --> 00:19:09,660
world.
So it becomes even more important.

315
00:19:11,250 --> 00:19:16,250
And the third key point is
that the fundamental function
of every technology is

316
00:19:18,931 --> 00:19:22,500
that it augments people so they can do
things that were previously impossible.

317
00:19:23,400 --> 00:19:26,580
You know, we couldn't fly
through the air and now we can.

318
00:19:27,210 --> 00:19:31,830
And so when you think about, you
know, what AI is going to do,

319
00:19:31,910 --> 00:19:35,430
there's a strong narrative that it's
going to put people out of work.

320
00:19:36,240 --> 00:19:38,160
And instead,
I think the narrative,

321
00:19:38,460 --> 00:19:43,020
we need to be seeking out the narrative
of what will it let us do that we can't

322
00:19:43,021 --> 00:19:44,940
do today,
that will be wonderful,

323
00:19:44,941 --> 00:19:48,030
that will make us full
of that WTF of delight.

324
00:19:48,720 --> 00:19:51,960
And we have to tell that story
powerfully and we have to believe it.

325
00:19:51,961 --> 00:19:56,460
And we have to build products and
startups that deliver on that promise.

326
00:19:57,060 --> 00:19:58,710
So, um, you know,

327
00:19:58,711 --> 00:20:02,610
and there is kind of this interesting
thing that of course the WTF of delight

328
00:20:02,611 --> 00:20:06,290
becomes banal. I still remember once, uh,

329
00:20:06,360 --> 00:20:10,860
landing in Sydney airport in Australia
and they had this giant mural,

330
00:20:10,890 --> 00:20:14,490
and I wish I could find the name of the
photographer because it's long gone now.

331
00:20:14,491 --> 00:20:19,491
But it was thousands of people turned
out to see the first airplane come to

332
00:20:19,831 --> 00:20:24,630
Australia, you know, and it was just
this, these faces looking up in wonder.

333
00:20:24,960 --> 00:20:29,090
He has been a voyage of, you know, many
months, you know, for many of, you know,

334
00:20:29,130 --> 00:20:33,120
people that emigrate and then it's like
they're connected to the world and it's

335
00:20:33,121 --> 00:20:37,000
like, oh my God. And now you think about,
you know, Airlines, oh my God, you know,

336
00:20:37,430 --> 00:20:37,651
you know,

337
00:20:37,651 --> 00:20:42,150
so the WTF of wonder actually later
became the WTF of dismay and we have to

338
00:20:42,151 --> 00:20:46,410
watch out for that. But the, um, uh,

339
00:20:47,110 --> 00:20:50,320
you know, that, that source
of wonder is so important,

340
00:20:50,321 --> 00:20:52,870
but this idea of augmentation,

341
00:20:52,990 --> 00:20:56,390
let me kind of come back to Uber and Lyft.
You know,

342
00:20:56,391 --> 00:21:00,730
when you think about that experience,
there's a couple of things.

343
00:21:00,731 --> 00:21:04,690
The first is this magical
experience, which goes away
because you get used to it.

344
00:21:05,170 --> 00:21:09,310
Wow, I can, you know, I don't have to
call a taxi company and hope they show up.

345
00:21:09,460 --> 00:21:13,270
I don't have to stand out on the street
corner and wave another all full.

346
00:21:13,271 --> 00:21:15,640
Or there's nobody there.
I can summon a car,

347
00:21:15,641 --> 00:21:18,610
I get an estimate of when it's going
to be there and I walk out and it just

348
00:21:18,611 --> 00:21:20,300
comes magic,
you know,

349
00:21:20,980 --> 00:21:25,470
realizing the capability that
was hidden in our phones, uh, to,

350
00:21:25,840 --> 00:21:30,490
to match people in a real time
matching marketplace. Right? But also,

351
00:21:30,660 --> 00:21:34,330
and this is one of the things a lot of
people don't understand about Uber and

352
00:21:34,330 --> 00:21:36,790
Lyft, you know, it's like taxi companies
go, well we have to have an APP,

353
00:21:38,230 --> 00:21:41,590
but we hate this idea of part
time drivers who just, you know,

354
00:21:41,591 --> 00:21:44,530
show up and don't have licenses.
And you go, well guess what?

355
00:21:45,130 --> 00:21:48,440
All the parts of that business
model work together. Uh,

356
00:21:48,490 --> 00:21:52,030
because it's because you have those part
time drivers that you can have three

357
00:21:52,031 --> 00:21:56,010
minute pickup times, uh, all of the
time. You don't run out of cabs, right?

358
00:21:56,020 --> 00:22:00,040
Cause they're harnessing the
marketplace in this new algorithmic way.

359
00:22:00,610 --> 00:22:05,610
And the reason they can do that is because
the drivers are augmented with this

360
00:22:05,771 --> 00:22:10,690
new cognitive augmentation called
Google maps or ways or you know,

361
00:22:10,710 --> 00:22:12,460
it's built now into,
into the APP. You know,

362
00:22:12,461 --> 00:22:17,050
it's like people don't have to have the
knowledge of the streets and monuments

363
00:22:17,051 --> 00:22:21,760
of London. They can just turn on the
APP and it says, turn right here.

364
00:22:22,270 --> 00:22:23,940
Right. And you know, the, the,

365
00:22:23,950 --> 00:22:26,620
literally the knowledge is a
test where you are a human gps.

366
00:22:26,621 --> 00:22:30,220
You're like a men tat out
of Frank Herbert's Dune. You
know, where you literally,

367
00:22:30,221 --> 00:22:33,310
they give you two points in London and
you have to recite the turn by turn to

368
00:22:33,311 --> 00:22:37,240
get from one to the other. And you know,
now you don't have to do that. So that,

369
00:22:37,241 --> 00:22:41,590
that cognitive augmentation also is
going to make new things possible.

370
00:22:41,591 --> 00:22:43,860
Just like the steam shovel or the,
you know,

371
00:22:43,900 --> 00:22:47,140
steel girder or the steel
beam in the physical world.

372
00:22:47,170 --> 00:22:51,160
Cognitive augmentation allows people
to do new things that they couldn't do

373
00:22:51,161 --> 00:22:52,990
before. So, um,

374
00:22:54,430 --> 00:22:57,850
I also talk a lot about the fact that
these systems are algorithmic systems,

375
00:22:57,851 --> 00:22:59,920
at least today infused with AI,

376
00:22:59,921 --> 00:23:04,270
but these algorithmic systems have a sort
of a fitness function or an objective

377
00:23:04,271 --> 00:23:04,661
function.

378
00:23:04,661 --> 00:23:07,930
And I want to talk about that in the
context of the economy in a bit because I

379
00:23:07,931 --> 00:23:11,650
think the real, uh, Israel's
significance for society,

380
00:23:11,651 --> 00:23:14,500
the economy and the future of the human
race in the algorithms that we are

381
00:23:14,501 --> 00:23:17,440
building into our future. Then we're
going to talk about that in a minute.

382
00:23:17,680 --> 00:23:21,100
So there's a bunch of my writing on
the topic of it's not in the book.

383
00:23:21,100 --> 00:23:22,720
You can find out the site WTF,

384
00:23:22,721 --> 00:23:27,721
economy.com as well as some content
that's in the book anyway and come back to

385
00:23:28,271 --> 00:23:32,620
this notion of platform must create more
value than it captures. And that's been,

386
00:23:32,720 --> 00:23:37,550
uh, kind of the heart of what we try
to do it or Riley, um, uh, you know,

387
00:23:37,570 --> 00:23:41,110
we invited in our biggest competitors
into our platform when we launched it.

388
00:23:41,500 --> 00:23:46,430
We continue to think about how do we make
money for them as well as for us. Uh,

389
00:23:46,510 --> 00:23:48,240
and I think Google should,
I,

390
00:23:48,241 --> 00:23:53,241
I've loved what Hal has done with the
Google economic impact reports because

391
00:23:53,421 --> 00:23:57,650
it's starting to tell a story which should
be told by every business in America,

392
00:23:57,651 --> 00:23:59,060
every business in the world,

393
00:23:59,180 --> 00:24:03,620
which is how are we creating value
for other people, for others,

394
00:24:03,621 --> 00:24:07,760
not just for ourselves, all
of his financial reporting.
That's just about, wow,

395
00:24:07,790 --> 00:24:11,150
look how well we're doing.
Great. I go, well, how, you know,

396
00:24:11,180 --> 00:24:16,180
there are companies that are doing well
by making other people do badly and we

397
00:24:16,431 --> 00:24:20,780
have to actually have a
system wide accounting of
what people are putting in and

398
00:24:20,781 --> 00:24:24,440
what they're taking out.
In the alternative,

399
00:24:24,441 --> 00:24:29,310
if you are a Google like company where
you are a systematic platform companies,

400
00:24:29,320 --> 00:24:34,020
you'd become a regulated utility because
if people say, wow, you are not uh,

401
00:24:34,050 --> 00:24:37,580
you know, looking after your ecosystem,
well we'll look after it for you.

402
00:24:37,760 --> 00:24:40,810
So I think that's something you guys
should be very worried about weight.

403
00:24:40,820 --> 00:24:43,280
I want to kind of come back,
I've talked about a lot of this already,

404
00:24:43,281 --> 00:24:47,730
but this is something that out
of the book, this notion of a,

405
00:24:48,020 --> 00:24:53,020
I call a business model map business
model is the way that all the parts of a

406
00:24:53,520 --> 00:24:57,670
business work together to
create customer value. And uh,

407
00:24:57,740 --> 00:25:01,070
I first was introduced to this concept
by some consultants called Dan Meredith

408
00:25:01,071 --> 00:25:02,360
being back in 2000.

409
00:25:02,600 --> 00:25:06,830
And they actually used the example of
southwest airlines versus hub and spoke

410
00:25:06,831 --> 00:25:10,130
airlines. And they kind of went
through how all the parts of,

411
00:25:10,820 --> 00:25:12,680
of southwest,
uh,

412
00:25:12,710 --> 00:25:15,980
business actually make them very
different than the other airlines,

413
00:25:16,010 --> 00:25:18,360
even though they're all in the
airline business, you know,

414
00:25:18,380 --> 00:25:21,290
so they don't forward baggage.
They don't have assigned seats.

415
00:25:21,530 --> 00:25:25,580
This actually is different.
And so Google and Facebook,

416
00:25:25,581 --> 00:25:27,020
both in the advertising business,

417
00:25:27,140 --> 00:25:32,030
but you have very different parts of
your business model. So for example, uh,

418
00:25:32,350 --> 00:25:35,510
with, with Google, right? Somebody,

419
00:25:35,511 --> 00:25:39,380
your customer succeeds when they come to
Google, get what they want and go away,

420
00:25:40,430 --> 00:25:44,390
right? So your success is aligned with
people leaving Google coming and leaving

421
00:25:44,990 --> 00:25:49,040
Facebook. Success is aligned
with having people come and stay.

422
00:25:49,520 --> 00:25:53,690
And that's a huge difference in your
business model and a huge lever if you are

423
00:25:53,691 --> 00:25:57,110
competing with Facebook for example,
because you have to say, Oh wow,

424
00:25:57,230 --> 00:26:01,640
we have to find more reasons
to get people to go away

425
00:26:03,140 --> 00:26:06,540
and not to get people to come and stay
with us because then you're, you're,

426
00:26:06,550 --> 00:26:10,310
you're playing on Facebook's
uh, uh, you know, turf.

427
00:26:10,580 --> 00:26:14,120
And as Sun Tzu says, attack your enemy
where he is strong and you are weak.

428
00:26:14,150 --> 00:26:17,060
I mean, we're here as weekend. You
are strong. So anyway, back to this,

429
00:26:17,690 --> 00:26:22,690
I tried to draw a model of Uber because
it seemed to me to be a kind of a,

430
00:26:23,410 --> 00:26:27,170
uh, a kind of a template for what starting
to happen in the economy. You know,

431
00:26:27,171 --> 00:26:28,880
you have these companies
that are platforms,

432
00:26:28,881 --> 00:26:31,240
they're not just sort of
traditional companies, you know,

433
00:26:31,241 --> 00:26:35,150
they're replacing ownership with
access, you know, and they've got this,

434
00:26:35,360 --> 00:26:38,890
this marketplace managed by an algorithm,
you know,

435
00:26:38,900 --> 00:26:41,780
with passengers matching
up passengers and drivers.

436
00:26:41,781 --> 00:26:44,790
And there's a bunch of things,
know workers supplying their own cars,

437
00:26:44,791 --> 00:26:49,500
independent contractors, you know,
that are really part of this, uh,

438
00:26:49,650 --> 00:26:51,920
you know, this story. And, uh, you know,

439
00:26:52,190 --> 00:26:56,490
this augmented workers idea that we
talked about magical x user experience.

440
00:26:56,640 --> 00:26:58,590
So all of these things go together.

441
00:26:59,250 --> 00:27:01,230
And one of the things we
have to understand this,

442
00:27:01,231 --> 00:27:05,160
we think about technology and the
economy is what things go together in our

443
00:27:05,161 --> 00:27:05,994
business model.

444
00:27:06,750 --> 00:27:11,640
What things belong and what things
to fight against it. And so,

445
00:27:11,641 --> 00:27:14,060
for example, at a Riley, one of
the things that we've, you know,

446
00:27:14,340 --> 00:27:19,340
realized that made us able to transition
from being a publishing company to be a

447
00:27:20,011 --> 00:27:23,290
conference company in an online learning
company, was that we realized we were,

448
00:27:23,291 --> 00:27:26,730
we were really a company that was about
connecting people who knew something

449
00:27:26,910 --> 00:27:28,590
with people who wanted
to learn it from them.

450
00:27:29,400 --> 00:27:34,400
And so we were able to evolve how we did
that because we really understood that.

451
00:27:34,530 --> 00:27:36,980
Whereas a lot of our
competitors, uh, you know,

452
00:27:37,080 --> 00:27:39,990
most of them no longer really
in the business very much.

453
00:27:40,110 --> 00:27:43,410
They just thought their job was to
capture knowledge into books and put it on

454
00:27:43,411 --> 00:27:48,000
shelves. So anyway, so, uh, I, you know,

455
00:27:48,001 --> 00:27:52,500
I spent some time really trying to think
through business models and how they

456
00:27:52,501 --> 00:27:55,140
work and how the changing in
the age of the platform. Uh,

457
00:27:55,260 --> 00:28:00,260
I've talked a little bit about this
thick marketplace idea. A house, uh, uh,

458
00:28:00,390 --> 00:28:01,470
student Jonathan Hall,

459
00:28:01,471 --> 00:28:05,040
who's the chief economist at Uber
turned me onto this book. Alvin Roth.

460
00:28:05,041 --> 00:28:08,920
So who gets what and why, uh, which
is really about, uh, you know,

461
00:28:09,060 --> 00:28:11,760
how do you make a successful
matching marketplace,

462
00:28:11,761 --> 00:28:14,700
which is at the heart of so many
of the great platforms today.

463
00:28:15,420 --> 00:28:18,890
And so that kind of led me to
this, you know, this path of, of,

464
00:28:18,891 --> 00:28:21,540
of thinking about marketplaces.
But then again,

465
00:28:21,541 --> 00:28:24,210
I talked about this augmented
in a worker concept,

466
00:28:24,211 --> 00:28:26,900
but this is a really key part of it.
Uh,

467
00:28:27,030 --> 00:28:30,450
and that is really something I call
the arc of knowledge. You know,

468
00:28:30,451 --> 00:28:34,140
when we think about how do we communicate
knowledge, we first, you know, we, we,

469
00:28:34,320 --> 00:28:36,900
we spoke it to each other and
then we wrote it down, you know,

470
00:28:36,901 --> 00:28:38,450
things like maps and,

471
00:28:38,570 --> 00:28:43,570
and you thinking about the
first iterations of online
maps and they were kind of

472
00:28:44,311 --> 00:28:48,960
reproductions of printed maps just online,
which was kind of Nice.

473
00:28:49,140 --> 00:28:52,680
And then you got maps and directions
and you go all the way up to automated

474
00:28:52,681 --> 00:28:57,500
vehicles and you see that the knowledge
disappears into a service. And that's,

475
00:28:57,660 --> 00:29:02,660
it seems to me to be a key point to
remember when you think about how are we

476
00:29:02,881 --> 00:29:04,050
augmenting workers,

477
00:29:04,230 --> 00:29:09,230
we're basically building these cognitive
augmentations that will just disappear

478
00:29:09,601 --> 00:29:14,220
in the services where you
don't necessarily need to
know in advance things,

479
00:29:14,221 --> 00:29:17,670
you know, this is already
so true. You know, we were
talking over lunch about how,

480
00:29:17,730 --> 00:29:20,340
oh yeah, yeah. Some technique.
How do you learn it?

481
00:29:20,341 --> 00:29:23,910
You go to youtube and you watch the
video and before long maybe we will be in

482
00:29:23,911 --> 00:29:26,340
the place like trinity in
the Matrix where we can say,

483
00:29:26,610 --> 00:29:28,260
I need to know how to fly a helicopter.

484
00:29:28,490 --> 00:29:32,220
And I remember she downloads
the knowledge, you know,
but even without that,

485
00:29:32,221 --> 00:29:36,360
we're going to have these augmented
devices that capture and share human

486
00:29:36,361 --> 00:29:39,420
knowledge. So when I
think about, you know, AI,

487
00:29:39,421 --> 00:29:43,090
that's my starting point is that
it's a tool for human augmentation.

488
00:29:43,660 --> 00:29:46,990
It isn't this radical in a discontinuity.
You know,

489
00:29:46,991 --> 00:29:51,370
this machine from the future that's going
to make humans obsolete any more than,

490
00:29:51,580 --> 00:29:53,880
you know, uh, machines in our,

491
00:29:53,890 --> 00:29:58,360
of the industrial era were somehow going
to make humans obsolete, you know? Sure.

492
00:29:58,361 --> 00:30:02,870
Yeah. We're not as strong probably
as most of our, uh, you know, uh,

493
00:30:03,010 --> 00:30:06,390
even if we work out a lot,
we're probably not as strong or as fit as,

494
00:30:06,391 --> 00:30:10,660
as someone who worked every day on the
farm or doing, you know, prayer work.

495
00:30:10,661 --> 00:30:15,010
I still remember going to Hawaii once
and huffing and puffing up over some hill

496
00:30:15,011 --> 00:30:15,850
and it was like,
oh yeah,

497
00:30:15,851 --> 00:30:19,360
the ancient Hawaiians took him about
20 minutes to open over to the other,

498
00:30:19,390 --> 00:30:24,340
you know, waterfall and we
took us two hours, you know,
um, yeah, you know, we're not,

499
00:30:24,630 --> 00:30:28,660
you know, as fit, but we are more
capable if you've ever read guns,

500
00:30:28,661 --> 00:30:29,710
germs and steel,
you know,

501
00:30:29,711 --> 00:30:34,190
it opens through this wonderful
passage yet Yalies question, you know,

502
00:30:34,330 --> 00:30:38,740
guy in, in, in New Guinea saying,
you know, you guys are so stupid.

503
00:30:39,290 --> 00:30:42,400
You know, if I took you in the jungle
he'd be dead in two weeks. I'll come.

504
00:30:42,401 --> 00:30:45,670
You have all the cargo.
Yeah. And you know,

505
00:30:45,671 --> 00:30:48,640
and that is in fact the question
of our civilization. You know,

506
00:30:48,641 --> 00:30:53,320
it's like we are making, you
know, a ourselves smarter,
stronger, faster, better.

507
00:30:53,530 --> 00:30:58,150
And that should be how we frame and how
we think about what we're doing with

508
00:30:58,151 --> 00:31:01,990
technology. It's not disruptive. It's
not destroyed. It's not eliminate,

509
00:31:02,170 --> 00:31:06,550
it's empower, it's lift up,
its make things possible. So,

510
00:31:06,940 --> 00:31:09,820
you know, technology is
our superpower. You know,

511
00:31:09,821 --> 00:31:13,210
and this is another one of
the charts from, um, you know,

512
00:31:13,220 --> 00:31:15,340
our world and data of, uh, you know,

513
00:31:15,341 --> 00:31:19,000
the number of people in the world living
in absolute poverty and you wash that

514
00:31:19,001 --> 00:31:23,290
incredibly steep decline
and that's wonderful. But
inequality is our Kryptonite.

515
00:31:23,860 --> 00:31:26,110
And we have, you know, this
is a, you know, from the,

516
00:31:26,140 --> 00:31:30,560
we are the 99% story we have to confront
that. We have to think about it as a,

517
00:31:31,030 --> 00:31:34,180
particularly as a, as a society,
but also as an industry.

518
00:31:35,380 --> 00:31:39,220
So one of my interesting
questions, uh, is, uh,

519
00:31:39,250 --> 00:31:43,240
that I tried to explore in the book is
what keeps us from creating prosperity

520
00:31:43,241 --> 00:31:47,380
for everyone.
And here's the answer.

521
00:31:47,890 --> 00:31:51,910
And this is the connected taxicab circuit.
2005.

522
00:31:53,290 --> 00:31:57,310
Uh, you know, it's like, wow, we had
connected taxicabs. What'd we do?

523
00:31:57,311 --> 00:32:02,230
We put a screen in the back where people
could look up information where they

524
00:32:02,231 --> 00:32:07,210
can watch ads, you know, and
we totally missed, you know,

525
00:32:07,270 --> 00:32:11,590
even though actually Sunil Paul in 2000
had written a series of patents about

526
00:32:11,591 --> 00:32:15,520
what would be possible, uh, if you, you
know, you tried to connect, you know,

527
00:32:15,521 --> 00:32:18,670
cars with smartphones and it actually,
they weren't really even smartphones,

528
00:32:18,671 --> 00:32:21,850
but with gps and phones, it
was just too early, right?

529
00:32:22,120 --> 00:32:27,010
So there was this sort
of cognitive filter, this
cognitive blindness, you know,

530
00:32:27,090 --> 00:32:29,230
you kill actually sort
of framing blindness,

531
00:32:29,260 --> 00:32:33,160
maybe the right way to sit where we framed
the world in a particular way and we

532
00:32:33,161 --> 00:32:37,570
couldn't see what was possible.
And I think this is a critical problem.

533
00:32:37,571 --> 00:32:40,770
And I've watched it throughout
my career. You know, when I,

534
00:32:40,850 --> 00:32:44,590
in 2000 I had to lead this
a protest against, uh, uh,

535
00:32:44,720 --> 00:32:48,710
Jeff Bezos is one click patent. And
we went and we looked, uh, you know,

536
00:32:48,711 --> 00:32:51,630
Jeff kind of worked with
me to try to, uh, you know,

537
00:32:52,220 --> 00:32:55,640
turn it around from a PR point of view.
I, one of the things we did was we,

538
00:32:55,910 --> 00:32:58,730
we started a us,
we funded a startup called bounty quest,

539
00:32:58,731 --> 00:33:01,460
which looked for prior art on,
you know,

540
00:33:01,490 --> 00:33:06,020
one click shopping and we couldn't find
it because basically it was sort of

541
00:33:06,021 --> 00:33:09,320
unthinkable of the time people were afraid
to put their credit cards online and

542
00:33:09,321 --> 00:33:12,920
they had the Shah, the
shopping cart metaphor, they
kind of understood the world.

543
00:33:12,921 --> 00:33:15,830
It was very much like this
connecting taxi cab circa 2005.

544
00:33:16,370 --> 00:33:21,200
So I, the reason I tell you that story
actually, and then here's another one.

545
00:33:21,200 --> 00:33:24,650
You know, it's like I asked Tony
Fidel, why didn't, you know, uh,

546
00:33:24,740 --> 00:33:29,180
you guys do the connected speaker and he
says, well, can you imagine, you know, if,

547
00:33:29,360 --> 00:33:33,290
if, uh, people had had thought
Google is listening to me, you know,

548
00:33:33,291 --> 00:33:37,580
somebody had to make that possible first.
You know, because there was just this,

549
00:33:37,760 --> 00:33:41,480
you know, because of Google's
particular position in the market,

550
00:33:41,840 --> 00:33:43,310
you couldn't be the first mover,

551
00:33:43,610 --> 00:33:47,510
but you still have to understand that
these cognitive shifts do happen where

552
00:33:47,511 --> 00:33:50,480
people believe that something else
is possible. But more than that,

553
00:33:50,510 --> 00:33:55,510
I want to actually take that thinking
to the political and the economic realm.

554
00:33:55,701 --> 00:34:00,350
And this is where I start to get out of my
depth in the book and, but uh, but I, I,

555
00:34:00,351 --> 00:34:02,240
this is maybe it's just
a thought experiment,

556
00:34:02,241 --> 00:34:06,620
but maybe just maybe it's something
that's really worth thinking about.

557
00:34:06,890 --> 00:34:09,330
And this is the beginning of,

558
00:34:09,400 --> 00:34:14,400
of actually an argument that runs through
a central section of the book and it's

559
00:34:14,481 --> 00:34:18,740
about sort of AI and algorithmic systems
and their relationship to human society.

560
00:34:19,610 --> 00:34:21,080
And I asked the question,
you know,

561
00:34:21,260 --> 00:34:26,260
what is strong AI bears the same
relationship to today's narrow AI as

562
00:34:27,321 --> 00:34:31,010
multicellular life does to a
single celled forebears. Now,

563
00:34:31,040 --> 00:34:35,450
right now we keep thinking that we're
going to create a strong AI. Maybe,

564
00:34:35,480 --> 00:34:37,850
you know,
maybe it will be 1520 years from now,

565
00:34:37,851 --> 00:34:41,480
maybe 50 maybe it will be a hundred
but it will be like us, you know,

566
00:34:41,481 --> 00:34:45,600
a single self aware being. And, you know,

567
00:34:46,010 --> 00:34:49,130
I wonder if instead, you
know, it's a collective being,

568
00:34:49,580 --> 00:34:54,410
and it turns out that that is what
happened in this prokaryotic to eukaryotic

569
00:34:54,411 --> 00:34:59,411
cell transition because it turns out
that this is thing that Lynn Margulis in

570
00:34:59,691 --> 00:35:04,190
1967 actually it was, I think it was
originally in, in, uh, uh, again, uh,

571
00:35:04,220 --> 00:35:09,170
constant and marriage choskey in
2019 oh five at first proposed it,

572
00:35:09,171 --> 00:35:12,770
but she picked it up at 67 and it
actually proved it over, uh, you know,

573
00:35:13,810 --> 00:35:16,550
a lot of years of being
ridiculed and pushed back.

574
00:35:16,850 --> 00:35:19,700
And it was this idea that,
uh,

575
00:35:20,120 --> 00:35:24,530
that multicell organisms actually
incorporated bacteria into them. You know,

576
00:35:24,531 --> 00:35:28,580
so it turns out it was latest
substantiated by genetic evidence that

577
00:35:28,581 --> 00:35:33,581
Mitochondria in our body are actually
bacteria that came and lived inside of us.

578
00:35:35,120 --> 00:35:38,730
Right? Uh, same thing with
chloroplasts in plants. They're there.

579
00:35:38,731 --> 00:35:42,600
They have totally different genetic
material than the nucleus of the cell.

580
00:35:43,200 --> 00:35:45,150
And so in a similar way,
I started thinking, well,

581
00:35:45,151 --> 00:35:49,170
what if we are somehow like
that with our computers?

582
00:35:49,770 --> 00:35:53,490
And of course, biological
symbiosis doesn't start with that.

583
00:35:53,491 --> 00:35:56,820
Symbio Genesis that Lynn Margo was
talked about this also this increasing

584
00:35:56,821 --> 00:36:00,960
knowledge of the microbiome,
how we are really a colony organism.

585
00:36:01,230 --> 00:36:04,980
Ed Young in his wonderful book I
contain multitudes, has all zoology,

586
00:36:04,981 --> 00:36:09,450
is really ecology. You know, when
we look at beetles and elephants,

587
00:36:09,451 --> 00:36:14,230
sea urchins and earthworms parents and
friends, we see individuals, you know,

588
00:36:14,410 --> 00:36:18,120
uh, driven by a single brain and
operating with a single genome.

589
00:36:18,720 --> 00:36:22,170
And this is a pleasant fiction.
You know, we're legion, you know,

590
00:36:22,180 --> 00:36:25,560
he says he'd Walt Whitman, I
am large. I contain multitudes.

591
00:36:25,980 --> 00:36:30,150
So think about this in humans too.
You know, when we talk about, uh,

592
00:36:30,780 --> 00:36:35,400
you know, unsupervised learning as the
holy grail of Ai, I go, yeah, you know,

593
00:36:35,401 --> 00:36:37,770
humans have some amount
of unsupervised learning.

594
00:36:37,800 --> 00:36:40,840
We have a shit ton of supervised learning.
You know,

595
00:36:40,880 --> 00:36:44,310
we learn our language from our
parents. Uh, everything we, you know,

596
00:36:44,311 --> 00:36:46,060
we know that makes us able to,
you know,

597
00:36:46,061 --> 00:36:49,950
to do the kinds of things that we do here
at Google. We were taught by somebody.

598
00:36:49,951 --> 00:36:53,550
And then on top of that, yes, we're
able to do unsupervised learning,

599
00:36:53,880 --> 00:36:56,940
but we have a long way to
go. Uh, you know, I think in,

600
00:36:56,970 --> 00:37:01,260
in the supervised learning of this new
organism before we can kind of even

601
00:37:01,261 --> 00:37:05,850
expect to see it start to
do on supervised. Not like
you start necessarily there,

602
00:37:06,360 --> 00:37:11,100
but, um, you know, so, so
thinking about this as a metaphor,

603
00:37:11,101 --> 00:37:14,040
I started thinking about, I've been
really thinking about this for the last,

604
00:37:14,370 --> 00:37:17,730
you know, 10, 15 years, you know,
how increasingly we're, all,

605
00:37:17,731 --> 00:37:22,670
all of humanity is being woven into this
new global mind, the Super Ai, you know,

606
00:37:22,770 --> 00:37:25,950
and we're seeing in things like fake news,

607
00:37:25,980 --> 00:37:30,970
kind of the equivalent of our neuroses,
you know, the equivalent of, of, uh, of,

608
00:37:31,370 --> 00:37:36,060
of bad ideas, you know, the spread
incredibly quickly now and are adopted by,

609
00:37:36,260 --> 00:37:36,510
you know,

610
00:37:36,510 --> 00:37:40,830
millions of people and then encoded
further into the systems that we build.

611
00:37:41,190 --> 00:37:41,400
You know,

612
00:37:41,400 --> 00:37:46,170
so we're this really interesting phase
of symbiosis and we have to come and

613
00:37:46,171 --> 00:37:49,560
understand it. And, uh, so,

614
00:37:50,490 --> 00:37:54,450
and we think that our emergent
Prodo AI's are also compound being.

615
00:37:54,451 --> 00:37:57,210
It's just like, we are, what
would we do differently?

616
00:37:57,210 --> 00:38:01,470
How would we think about them and
what do we know about them today? And,

617
00:38:02,050 --> 00:38:02,970
uh,
you know,

618
00:38:03,090 --> 00:38:08,010
there's a lot of fear of some kind
of future hostile AI. And I go, well,

619
00:38:08,040 --> 00:38:10,960
if there is one,
we are already training it.

620
00:38:10,980 --> 00:38:15,480
We already teaching it and what
are we teaching it? So, you know,

621
00:38:15,481 --> 00:38:18,030
when you think about this further,
you know, we, we're seeing, you know,

622
00:38:18,060 --> 00:38:21,870
this compound being, you know, this,
we see the rise of specialized chips,

623
00:38:21,871 --> 00:38:23,250
which actually, uh, you know,

624
00:38:23,251 --> 00:38:26,040
is actually a pretty interesting
imitation of neuroscience.

625
00:38:26,280 --> 00:38:29,010
But here it is a great one. You're
here. Here's one of the, uh, you know,

626
00:38:29,011 --> 00:38:33,450
the Mitochondria inside the
Google brain, right? Uh,

627
00:38:33,510 --> 00:38:35,510
and, uh, you know, it,

628
00:38:35,590 --> 00:38:40,590
here's this thing we've even defined
in our legal system starting in 1888 we

629
00:38:40,721 --> 00:38:43,930
said that there's this thing
called a corporate person.

630
00:38:44,290 --> 00:38:48,790
It's really just this collective organism
that was the case called consolidated

631
00:38:48,791 --> 00:38:53,400
silver mining versus Pennsylvania.
It's like under the designation of person.

632
00:38:53,410 --> 00:38:56,650
There's no doubt that a
private corporation is included
in the 14th amendment,

633
00:38:56,980 --> 00:38:57,880
such corporations,

634
00:38:57,881 --> 00:39:01,870
Emilia associations of individuals who
United for special purpose, right? So,

635
00:39:02,020 --> 00:39:05,470
you know, special purpose
organism, you know, and guess what,

636
00:39:05,500 --> 00:39:08,830
we now have these special purposes of
organisms are increasingly digital,

637
00:39:08,980 --> 00:39:12,130
increasingly quick,
increasingly algorithmic.

638
00:39:12,350 --> 00:39:16,130
We're making these special
purpose organisms. Uh, uh, and,

639
00:39:16,150 --> 00:39:17,980
and what are we asking them to do?

640
00:39:18,580 --> 00:39:23,580
And I believe that one of the things that
we have to come to grips with is that

641
00:39:23,771 --> 00:39:25,600
in some ways these ais,

642
00:39:25,601 --> 00:39:29,080
that our children are
already ruling human society.

643
00:39:29,980 --> 00:39:33,850
And we have to ask ourselves, what is
the motivation of this new species?

644
00:39:33,940 --> 00:39:37,960
And this is obviously, you know, kind of
out there, but think about it, you know,

645
00:39:38,050 --> 00:39:43,050
how does Google decide
what it's going to do?

646
00:39:44,020 --> 00:39:48,640
Well, we actually, we collectively, you
know, starting with Larry and Sergei and,

647
00:39:49,120 --> 00:39:51,220
uh, you know, all the early founders,

648
00:39:51,221 --> 00:39:55,510
but people like Hal and all of you
have actually put thoughts into this,

649
00:39:55,710 --> 00:39:58,150
you know, shared Google
brain, you've trained that,

650
00:39:58,151 --> 00:40:03,151
you've taught it and you've taught it
that it should optimize for relevance.

651
00:40:04,030 --> 00:40:07,720
And this is this sort
of master organizing,

652
00:40:07,721 --> 00:40:10,300
objective function of
everything that Google does,

653
00:40:10,301 --> 00:40:12,280
whether it's in search or advertising,

654
00:40:12,281 --> 00:40:14,620
it's like find the thing
that people are looking for.

655
00:40:14,950 --> 00:40:19,780
So you've kind of built this
special purpose organism that has,

656
00:40:19,840 --> 00:40:23,500
you know, these, these goals
built into it, baked into it.

657
00:40:24,100 --> 00:40:28,390
And those goals actually now exist
independently of any of the people who

658
00:40:28,391 --> 00:40:32,710
created it. Like again, they, they, they
will, you know, it will die just like, uh,

659
00:40:32,730 --> 00:40:37,210
you know, uh, you know, a
biological organism will die
under certain conditions,

660
00:40:37,750 --> 00:40:42,750
but it doesn't actually depend on
any individual cells to remain.

661
00:40:42,790 --> 00:40:43,031
You know,

662
00:40:43,031 --> 00:40:47,680
we've all been told that our bodies
can completely recycle the completely

663
00:40:47,681 --> 00:40:52,390
different cells. Uh, yeah,
every seven years I think it
is. And, you know, same thing,

664
00:40:52,391 --> 00:40:56,440
all the people can cycle out and this
organism would continue with the same

665
00:40:56,441 --> 00:41:00,250
programming, with this continually
adapting program. So think about that.

666
00:41:00,251 --> 00:41:04,240
So Google relevance,
Facebook engagement,

667
00:41:04,720 --> 00:41:08,800
and we saw with Facebook and fake
news how that can go wrong, you know,

668
00:41:08,801 --> 00:41:10,810
and it basically reminds me,
I guess,

669
00:41:10,811 --> 00:41:15,580
a of the story of Mickey
Mouse in the broomsticks. And
fantasia, you know, I mean,

670
00:41:15,610 --> 00:41:16,810
all of these systems,

671
00:41:16,900 --> 00:41:21,580
we tell them what to do when we set them
going and they do what we told them to

672
00:41:21,581 --> 00:41:23,890
do, but we don't necessarily
quite understand it.

673
00:41:24,070 --> 00:41:27,190
And this is the story that comes out
again and again in Arabian mythology,

674
00:41:27,191 --> 00:41:31,140
you know, with genies and uh,
you know, you, you told them, uh,

675
00:41:31,150 --> 00:41:34,880
this is actually an illustration by
Edmund Dulac from the thousand and one

676
00:41:34,881 --> 00:41:38,240
nights. Uh, but it's sort
of a rising of great power,

677
00:41:38,450 --> 00:41:41,730
but what's it going to do is going
to do what we told it. But we made,

678
00:41:41,740 --> 00:41:44,150
we didn't understand and we
didn't tell it quite right.

679
00:41:44,450 --> 00:41:49,450
Mark thought that by building this
social platform that was focused on

680
00:41:49,641 --> 00:41:50,890
engagement and you know,

681
00:41:50,900 --> 00:41:54,620
he would build real community and then
he found instead that we've created this

682
00:41:54,621 --> 00:41:58,180
monster, which he is now trying to
bring back under control. You know,

683
00:41:58,250 --> 00:42:01,940
that's actually part of
the experience. So, uh,

684
00:42:02,360 --> 00:42:04,910
many years ago,
a friend of mine said to me,

685
00:42:04,911 --> 00:42:07,520
this is in the early days of
mackintosh programming. He said,

686
00:42:07,521 --> 00:42:10,670
the art of debugging is figuring out
what you really told you programmed to do

687
00:42:10,671 --> 00:42:12,470
rather than what you
thought you told it to do.

688
00:42:12,890 --> 00:42:16,580
And we are right now engaged
in that process with these,

689
00:42:16,970 --> 00:42:21,320
these new digital gin that we
have created. We are saying,

690
00:42:21,560 --> 00:42:25,640
are they doing what we meant?
Are we doing? Are they doing,

691
00:42:26,460 --> 00:42:29,690
you know what we told them to do?
But it wasn't quite what we had asked.

692
00:42:30,020 --> 00:42:32,600
And how do we fix that?
That's what you do every day at Google.

693
00:42:32,600 --> 00:42:36,770
You try to actually go, yes, is it
doing what we really meant it to do?

694
00:42:37,400 --> 00:42:42,260
And now my question is, are we doing
that same thing in our broader society?

695
00:42:43,010 --> 00:42:43,251
You know,

696
00:42:43,251 --> 00:42:48,251
because here's the data that we see that
is at the heart of this economic unease

697
00:42:49,731 --> 00:42:53,330
that we're facing,
which is that this wonderful gift,

698
00:42:53,331 --> 00:42:58,190
the WTF of amazement is that
productivity has continued to go up.

699
00:42:58,230 --> 00:42:58,851
You know,
if you look,

700
00:42:58,851 --> 00:43:03,851
this is from 1945 up
through 2015 is pretty,

701
00:43:04,250 --> 00:43:07,300
my God, it's a graph that ray
Kurzweil would be proud of.

702
00:43:07,320 --> 00:43:09,860
It's just up until the right,
right?

703
00:43:09,861 --> 00:43:13,670
And then you see this divergence
starting around 1970s,

704
00:43:14,230 --> 00:43:19,130
a real family income. Somehow that
productivity was not getting through,

705
00:43:19,160 --> 00:43:21,200
not being distributed to ordinary people.

706
00:43:21,620 --> 00:43:24,490
Now it's a lot of reasons why that
may have gone wrong is you know,

707
00:43:25,010 --> 00:43:28,680
people like Hal study this, uh,
and, and, and his, his, his, uh,

708
00:43:28,700 --> 00:43:31,480
his ill cause is what economists do.
Uh,

709
00:43:32,480 --> 00:43:35,300
and I'm not saying what I'm
proposing is the only cause,

710
00:43:35,301 --> 00:43:38,180
but I think it is one of them.
And that is this,

711
00:43:38,210 --> 00:43:42,430
here's another one of these
ais, these pro ais, you know,

712
00:43:42,530 --> 00:43:47,060
that we tell it what to do and is it
really doing what we think and this is the

713
00:43:47,061 --> 00:43:51,380
Equinix and why for data center,
which is sort of one of the hearts of the,

714
00:43:51,770 --> 00:43:53,690
of our financial trading system.

715
00:43:54,410 --> 00:43:59,410
And this is kind of the skynet of the
story because ultimately we told these are

716
00:44:04,820 --> 00:44:07,580
financial system what its
objective function should be.

717
00:44:08,810 --> 00:44:13,810
Milton Friedman in 1970 wrote an article
in the New York Times called the social

718
00:44:13,971 --> 00:44:17,000
responsibility of business
is to increase its profits.

719
00:44:17,570 --> 00:44:20,270
The businesses should not think
about anything other than that.

720
00:44:20,990 --> 00:44:22,160
And if they did that,

721
00:44:22,161 --> 00:44:25,160
they would pass along the profits to the
shareholders and the shareholders could

722
00:44:25,161 --> 00:44:28,970
make up their own mind.
This is a perfectly reasonable thesis,

723
00:44:29,330 --> 00:44:32,010
just like some of the theses
that you put into your code.

724
00:44:33,030 --> 00:44:35,410
But when you build code and you watch it,
you do,

725
00:44:35,411 --> 00:44:37,980
you actually do that debugging process.
You say,

726
00:44:37,981 --> 00:44:40,710
did it do what I asked it to do?

727
00:44:41,520 --> 00:44:45,050
And we have to ask ourselves when we said,
you know,

728
00:44:45,090 --> 00:44:47,860
optimize for share price,
you know,

729
00:44:48,390 --> 00:44:51,240
in particular optimized for
profits and the profits.

730
00:44:51,241 --> 00:44:55,650
This proxy is the share price
of companies optimize for that,

731
00:44:55,680 --> 00:44:59,250
not for people, you know? Yeah.
If it makes sense, you know,

732
00:44:59,280 --> 00:45:03,450
outsourced to factories. If it makes
sense. Got that community, you know,

733
00:45:03,451 --> 00:45:07,620
because you know, your, your master
fitness function, the thing that we,

734
00:45:07,621 --> 00:45:12,621
the wish that we expressed to the genie
we built was make my share price go up.

735
00:45:14,250 --> 00:45:15,960
And uh, you know, this is actually,

736
00:45:15,990 --> 00:45:18,930
there are a lot of people in the financial
industry who are starting to take

737
00:45:18,931 --> 00:45:22,320
note of this. I mean, you know,
uh, Larry Fink, who runs blackrock,

738
00:45:22,321 --> 00:45:25,680
the largest asset manager in the world,
has been railing about stock buy backs,

739
00:45:25,880 --> 00:45:28,590
uh, Warren Buffet questioning. And
then there's a number of books.

740
00:45:28,591 --> 00:45:33,180
This one makers and takers is really
good other than the golden passport about

741
00:45:33,750 --> 00:45:37,020
how Harvard business school
kind of wreck the economy. But,

742
00:45:37,021 --> 00:45:39,270
so this idea that the financial markets,

743
00:45:39,580 --> 00:45:44,160
which are really meant to support the
human economy and become this extractive

744
00:45:44,161 --> 00:45:48,720
platform that basically says, no,
no, no, gives the money to us.

745
00:45:48,810 --> 00:45:51,330
You know? And, and we have
to actually reverse that.

746
00:45:51,630 --> 00:45:55,800
And that's what I want to come
back to this story about the,

747
00:45:56,110 --> 00:46:01,110
a little screen in the back of the
taxi cab and the failure of imagination

748
00:46:02,190 --> 00:46:06,690
because it's very hard for us to
imagine a completely different world,

749
00:46:07,590 --> 00:46:10,290
you know, so when we talk about
tax reform will, it's like,

750
00:46:10,291 --> 00:46:14,580
we'll push this rate up in that rate
down. You know, that's not how it happens.

751
00:46:14,610 --> 00:46:18,720
The great revolutions are ones where
you really imagined the world in new.

752
00:46:19,050 --> 00:46:22,440
And we're good at that in technology
and we think about that all the time.

753
00:46:23,010 --> 00:46:27,900
And we need to do it also in the
world of politics and the economy.

754
00:46:28,440 --> 00:46:30,570
And we have done that before.
You know,

755
00:46:30,571 --> 00:46:34,950
when the founders of this country,
you know,

756
00:46:35,130 --> 00:46:37,350
got together and said,

757
00:46:37,440 --> 00:46:41,970
we're going to try to build a new
country with a new set of ideas,

758
00:46:42,780 --> 00:46:44,000
you know, oh my God. You know,

759
00:46:44,140 --> 00:46:48,720
there's that famous story
about King George when George
Washington stepped down,

760
00:46:49,290 --> 00:46:50,660
you know,
uh,

761
00:46:51,480 --> 00:46:55,080
basically originally and he won the war.
You know,

762
00:46:55,081 --> 00:46:58,200
all the Europeans expected that George
Washington would become the king of

763
00:46:58,201 --> 00:47:02,850
America. And, and George the
third is reported reporter said,

764
00:47:03,120 --> 00:47:07,410
yeah. And he went back to his farm and
said, and then George King George said,

765
00:47:07,560 --> 00:47:10,800
if he has done that, now he has the
greatest man in the world has ever seen.

766
00:47:10,800 --> 00:47:13,080
It was this unthinkable thing,
you know,

767
00:47:13,081 --> 00:47:16,290
so much more on thinkable than
Uber was to a taxi company.

768
00:47:16,500 --> 00:47:20,610
Or that one click was through,
uh, an ecommerce site. And
it was amazing, you know,

769
00:47:20,611 --> 00:47:22,500
and we can do that,
you know?

770
00:47:22,501 --> 00:47:27,501
So if we are entering a world where AI
can do so much more of the jobs that we

771
00:47:28,351 --> 00:47:32,230
do today, you know, we should
not be content to say, well,

772
00:47:32,260 --> 00:47:35,830
we'll just kind of keep feeding
the current system. You know,

773
00:47:35,831 --> 00:47:39,610
I have Cory Doctorow, the science
fiction writer had this great, uh, uh,

774
00:47:39,640 --> 00:47:41,620
statement on social media
recently where he said,

775
00:47:41,621 --> 00:47:46,090
a economists use equations to
justify the divine right of capital.

776
00:47:46,270 --> 00:47:48,640
The way that corridor astrologers used,
uh,

777
00:47:48,880 --> 00:47:52,140
the stars to justify the divine
right of kings. And yeah,

778
00:47:52,150 --> 00:47:56,170
whether it's true or not, it's true in
the spiritual world. This idea that,

779
00:47:56,171 --> 00:47:56,541
you know,
we,

780
00:47:56,541 --> 00:48:01,450
we basically justify the system as it is
rather than imagining the system as it

781
00:48:01,451 --> 00:48:02,284
could be.

782
00:48:02,380 --> 00:48:07,380
And I see this enormous opportunity to
make a more prosperous world with all of

783
00:48:07,961 --> 00:48:11,400
these technologies we have,
you know, so, you know, uh,

784
00:48:11,460 --> 00:48:14,320
the what makes me hopeful is first of all,

785
00:48:14,350 --> 00:48:16,630
that when we build these
algorithmic systems,

786
00:48:16,750 --> 00:48:21,750
we can start to see ourselves better
bias and coded and taken to scale becomes

787
00:48:22,541 --> 00:48:27,490
visible. You know, when,
you know, when we have been,

788
00:48:27,820 --> 00:48:28,360
uh,

789
00:48:28,360 --> 00:48:32,950
arresting blacks at a much
higher rate and not seeing it,

790
00:48:33,010 --> 00:48:37,360
it's just kind of in the woodwork. You
know, we'd go wells, human bias, you know.

791
00:48:37,361 --> 00:48:39,670
Sure. But then all of a
sudden when you're going, wow,

792
00:48:39,671 --> 00:48:42,970
we're baking it into the end of the
algorithms because we're using these

793
00:48:42,971 --> 00:48:46,840
training datasets that are based
on decades of bias policing.

794
00:48:47,200 --> 00:48:51,370
We can suddenly see it, we can
fix it. We can debug our society.

795
00:48:52,150 --> 00:48:55,510
You know? So that's the first thing, you
know, it's like this whole engagement,

796
00:48:55,511 --> 00:49:00,190
this digitalization of our world is
going to help us understand our world and

797
00:49:00,191 --> 00:49:03,400
make it better. And the
second thing, you know,

798
00:49:03,401 --> 00:49:08,401
AI is going to help us to understand more
deeply what is human and it can create

799
00:49:08,651 --> 00:49:11,410
new kinds of beauty. And I love this, uh,

800
00:49:11,411 --> 00:49:16,360
this a statement from the 37th move
of the second game. Uh, it was a fan.

801
00:49:16,361 --> 00:49:19,510
Who are you talking about?
He says it's not a human move.

802
00:49:19,540 --> 00:49:23,230
I've never seen a human play
this move so beautiful. You know,

803
00:49:23,231 --> 00:49:28,231
this idea that something new that we
have not imagined can be beautiful,

804
00:49:28,660 --> 00:49:30,790
that it can not be something
you'd be afraid of.

805
00:49:30,791 --> 00:49:34,630
That it can be something wonderful
that's the world that I think we and the

806
00:49:34,631 --> 00:49:38,740
technology industry should be
committed to creating. And finally,

807
00:49:39,910 --> 00:49:44,440
just want to remember that
it isn't technology that
wants to eliminate jobs. Uh,

808
00:49:44,470 --> 00:49:47,860
my friend Nick Hanauer or said technology
is the solution to human problems.

809
00:49:48,130 --> 00:49:51,790
We won't run out of work until
we run out of problems. So like,

810
00:49:51,791 --> 00:49:55,990
I think we all in our industry have
to commit ourselves to solving human

811
00:49:55,991 --> 00:50:00,580
problems, make it work, make
the world a better place.

812
00:50:01,150 --> 00:50:05,200
So that's the master design pattern of
technology and really the message of the

813
00:50:05,201 --> 00:50:06,250
book,
you know,

814
00:50:06,490 --> 00:50:09,850
our job is to augment people so they
can do things that were previously

815
00:50:09,851 --> 00:50:12,250
impossible.
Thanks.

816
00:50:18,420 --> 00:50:21,890
Well Tim, thanks so much for coming. I'm
really excited to read your book. Uh,

817
00:50:22,210 --> 00:50:23,140
I was just curious,
you know,

818
00:50:23,141 --> 00:50:26,290
as someone who's been in the publishing
and content industry for the past 20

819
00:50:26,291 --> 00:50:27,800
years,
why,

820
00:50:27,830 --> 00:50:31,880
when you think about it sort of like
transmitting these ideas about the future,

821
00:50:31,881 --> 00:50:35,360
why you chose to do it in the
form of a book, when there's like,

822
00:50:35,600 --> 00:50:37,900
there's lots of different
tools out there today and like,

823
00:50:38,090 --> 00:50:42,170
so as someone who's thought a lot about
this, I'm just kind of curious, like what,

824
00:50:42,171 --> 00:50:43,910
like why do you choose to,
cannot close it?

825
00:50:44,770 --> 00:50:46,270
That's really good point actually.
In a,

826
00:50:46,290 --> 00:50:50,870
probably the trigger was that I had,

827
00:50:51,710 --> 00:50:55,880
I've been running this event called the
next economy summit and I was trying to

828
00:50:55,881 --> 00:50:58,640
get on Michael Crafts, uh, you know,

829
00:50:58,641 --> 00:51:02,420
show the local PBS show and I
just was having a lot of trouble.

830
00:51:02,620 --> 00:51:05,570
And then somebody who'd written a book
that I really didn't respect, sort of,

831
00:51:05,870 --> 00:51:08,450
you know, appears on the show. And
I'm like, Damn, you know, it's like,

832
00:51:08,510 --> 00:51:13,280
it's free for a certain kind of
intellectual discussion. It is still,

833
00:51:13,640 --> 00:51:17,450
uh, you know, the, the
little entry card, you know,

834
00:51:17,451 --> 00:51:20,570
like I couldn't get in this room without
the Google entry card. And you know,

835
00:51:20,571 --> 00:51:25,310
writing the book gets you discussed
by the policymakers because that's the

836
00:51:25,311 --> 00:51:26,031
thing,
you know,

837
00:51:26,031 --> 00:51:29,450
all of these people who
shouldn't be deeply involved
in the digital because it's

838
00:51:29,451 --> 00:51:33,950
the center of our world, you know, I
mean this is a book that I can send,

839
00:51:33,951 --> 00:51:37,610
I can put in front of a
policymaker, have them read, uh,

840
00:51:37,760 --> 00:51:40,760
have the think tanks in DC.
Talk about,

841
00:51:40,761 --> 00:51:43,850
I can give talks there because
I wrote a book, you know,

842
00:51:44,010 --> 00:51:47,540
and otherwise I'm some
weird new media guy.

843
00:51:49,520 --> 00:51:52,750
And of course you've been a publisher
for 30 years. Yeah. Which is ironic. And,

844
00:51:52,751 --> 00:51:54,560
and actually I didn't publish it myself.

845
00:51:54,561 --> 00:51:58,160
I actually went to one of the traditional
business publishing, uh, you know,

846
00:51:58,290 --> 00:52:02,540
companies because again,
it's sort of, it's, it's just
this credentialing function,

847
00:52:04,430 --> 00:52:09,050
excellent set of hypotheses. But one of
them is the, who are the actors of change.

848
00:52:09,051 --> 00:52:11,000
Obviously we talk like break big things.

849
00:52:11,001 --> 00:52:12,980
Like Uber today has started
as very small things.

850
00:52:13,430 --> 00:52:15,000
Do you have any conclusions
you draw with this?

851
00:52:15,050 --> 00:52:18,950
Whether innovation is possible as they
can to get more and multicellular at the

852
00:52:18,951 --> 00:52:21,410
small stage garage date
and rural startup stage?

853
00:52:21,411 --> 00:52:25,190
Or would you be thinking that as data
gravity of its own that we're going to see

854
00:52:25,191 --> 00:52:29,750
a return to bigness for its own sake
and that potential for startups in this

855
00:52:29,751 --> 00:52:30,440
world?

856
00:52:30,440 --> 00:52:33,110
Well,
I do think first of all,

857
00:52:33,140 --> 00:52:38,120
the power of ideas is
profound. Uh, you know, uh,

858
00:52:38,210 --> 00:52:43,160
even, you know, if a
startup doesn't survive,

859
00:52:43,760 --> 00:52:48,760
it can actually place a new idea into
the world and that idea can be taken up.

860
00:52:49,520 --> 00:52:52,720
And um, you know, that's,
that is, you know,

861
00:52:52,910 --> 00:52:56,990
why create more value than you capture
is sort of part of my motto. You know,

862
00:52:56,991 --> 00:53:01,340
I just believe, yes, you know, in
the end we're all going to fail.

863
00:53:01,850 --> 00:53:05,060
You know, and I, I've often quoted
this wonderful poem of Roca.

864
00:53:05,061 --> 00:53:08,510
What we fight with is so small and
when we win, it makes us small.

865
00:53:08,720 --> 00:53:12,440
What we want is to be defeated decisively
by successively greater beings.

866
00:53:12,620 --> 00:53:16,100
He's talking about wrestling with
the angels. And I think that,

867
00:53:17,330 --> 00:53:19,910
so yes,
that may be a risk of bigness,

868
00:53:20,090 --> 00:53:25,090
but I think actually if you follow the
job of doing something that needs doing,

869
00:53:29,430 --> 00:53:31,440
you know,
like one of my,

870
00:53:31,470 --> 00:53:34,140
one of the companies they talked about
at the end of the book where I kind of

871
00:53:34,141 --> 00:53:36,690
try and give some, hopefully
it's companies zip line, right?

872
00:53:36,810 --> 00:53:41,490
So here's a company that's using drones
and on demand and they're not like,

873
00:53:41,491 --> 00:53:45,570
we're going to be the Uber of dry cleaning
there. They're like, uh, you know,

874
00:53:46,110 --> 00:53:51,090
we're going to be the Uber of blood
delivery in co in a country where,

875
00:53:51,250 --> 00:53:52,630
uh, people, you know,

876
00:53:52,650 --> 00:53:57,400
with the leading cause of death in women
is postpartum hemorrhage. You know,

877
00:53:57,410 --> 00:54:02,370
it was no developed hospital
infrastructure. There's, uh, you know,

878
00:54:03,120 --> 00:54:04,350
bad roads,

879
00:54:04,560 --> 00:54:09,330
but you can get a drone anywhere in
the country and 20 minutes, you know,

880
00:54:09,540 --> 00:54:13,440
and Bang, you know, it's like, that's
astonishing. You know, that's like, wow,

881
00:54:13,590 --> 00:54:18,480
we can solve a problem that's a real
problem with this magical technology.

882
00:54:18,690 --> 00:54:21,870
And guess what? They're kind of
not, they're in there, you know,

883
00:54:22,560 --> 00:54:24,870
in that normal competitive landscape.

884
00:54:25,020 --> 00:54:28,140
Because they've actually found
this green field of opportunity,

885
00:54:28,141 --> 00:54:32,280
which is solving a real problem.
And I think, uh, you know,

886
00:54:32,400 --> 00:54:36,990
if, when, you know, sure, the
[inaudible] space pretty crowded.

887
00:54:36,990 --> 00:54:40,830
And if you want to go, yeah, can I make
another social network kind of hard,

888
00:54:40,860 --> 00:54:43,890
you know, problem. But there's another
piece to it too, which I've been,

889
00:54:43,950 --> 00:54:47,610
I had a debate with Reid Hoffman recently
cause he's got a book coming out on

890
00:54:47,730 --> 00:54:51,450
blitz scaling. And I was like,
look, read it. If, you know,

891
00:54:51,451 --> 00:54:54,240
it's like you got to raise lots of money
because you have to grow faster than

892
00:54:54,241 --> 00:54:57,570
your competitors and so on and so forth.
It's still a winner.

893
00:54:57,571 --> 00:55:00,750
Takes all somebody going to win
and somebody's going to lose.

894
00:55:01,050 --> 00:55:04,970
And one of the things I want
to hear more is, uh, um,

895
00:55:05,180 --> 00:55:08,980
like how are these platforms
enabling an ecosystem? You know,

896
00:55:09,000 --> 00:55:11,640
it's kind of goes back to this sort
of platform thinking, you know,

897
00:55:11,641 --> 00:55:13,650
and you look at,
uh,

898
00:55:13,890 --> 00:55:18,660
you know how Google has been an enabler,
how youtube is an enabler.

899
00:55:18,690 --> 00:55:22,350
You know, there's people getting
work from this. And I think, uh,

900
00:55:22,470 --> 00:55:26,460
one of the things that I think in a
world of winner takes all platforms,

901
00:55:26,700 --> 00:55:30,690
the platforms themselves have to think
more about how are we going to enable a

902
00:55:30,691 --> 00:55:32,040
network of small business

903
00:55:32,510 --> 00:55:36,860
[inaudible].

