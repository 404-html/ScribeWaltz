1
00:00:06,060 --> 00:00:10,740
We are excited to have our speaker finale.
Doshi Vela is here today.

2
00:00:11,130 --> 00:00:16,130
She is an assistant professor in computer
science at the Harvard Paulson School

3
00:00:16,290 --> 00:00:17,550
of Engineering and Applied Sciences.

4
00:00:17,880 --> 00:00:22,830
Her research focuses on the intersection
of machine learning and healthcare and

5
00:00:22,831 --> 00:00:27,270
today she'll be speaking to us about
interpretability towards more rigorous

6
00:00:27,300 --> 00:00:28,540
rigorous evaluation.

7
00:00:29,150 --> 00:00:29,983
Thank you.

8
00:00:33,400 --> 00:00:34,380
Hi everyone.
It's

9
00:00:34,420 --> 00:00:39,310
exciting to be here. Um, I like
to keep talks fairly informal,

10
00:00:39,311 --> 00:00:42,480
so if you have questions along
the way, please shout out. Um,

11
00:00:42,670 --> 00:00:46,510
and we'll also have questions toward the
end. I one to, I want to point out, um,

12
00:00:46,520 --> 00:00:49,690
I'm going to share a little bit if the
work that we're doing currently in our

13
00:00:49,691 --> 00:00:53,180
group, but more importantly, uh, the,

14
00:00:53,190 --> 00:00:57,940
the thing that I want to chat about is
how we can make interpretability more

15
00:00:57,941 --> 00:00:59,640
rigorous.
So it's a bit this,

16
00:00:59,641 --> 00:01:03,610
this talk is going to have
more of a philosophical or
forward looking component,

17
00:01:04,060 --> 00:01:04,690
um,

18
00:01:04,690 --> 00:01:09,690
rather than a ton of results that
are already completed components.

19
00:01:09,701 --> 00:01:11,800
And that's why I encourage everyone,

20
00:01:12,130 --> 00:01:15,460
if you have ideas to like really engage
in the discussion because that's what

21
00:01:15,550 --> 00:01:20,080
I'm really interested in having.
So just to get started,

22
00:01:20,140 --> 00:01:24,830
um, as a, you wouldn't be
here if you weren't interested
in interpretability. Um,

23
00:01:24,880 --> 00:01:29,800
it's definitely in vogue,
tons of publications coming
out, especially lately.

24
00:01:30,170 --> 00:01:32,530
Um,
if you just look at Google scholar hits,

25
00:01:32,650 --> 00:01:36,460
obviously the rate of publications
in general has been increasing,

26
00:01:36,700 --> 00:01:40,800
but you see there's huge
numbers of publications in
the last several years, um,

27
00:01:40,920 --> 00:01:44,260
under this guise of interpretability
in machine learning.

28
00:01:44,440 --> 00:01:47,860
And part of that interest comes from the
fact that machine learning systems are

29
00:01:47,861 --> 00:01:52,861
being deployed in more and more settings
and as part of more and more systems

30
00:01:54,040 --> 00:01:57,070
and now we are curious about,
well,

31
00:01:57,130 --> 00:01:59,770
are these system's behaving
in ways that we expect? Sorry,

32
00:01:59,771 --> 00:02:03,610
they latching on to signals
that we didn't expect.

33
00:02:04,000 --> 00:02:08,080
How can we ensure that our systems are
doing what we want them to do and how can

34
00:02:08,081 --> 00:02:11,320
we debug them when they're not doing
things that we want them to do?

35
00:02:11,410 --> 00:02:15,190
Hence this growing interest
in interpretability. Um,

36
00:02:15,220 --> 00:02:17,980
but the big question is what
exactly is interpretability?

37
00:02:18,940 --> 00:02:20,740
And that's the thing that
I want to chat about today.

38
00:02:21,010 --> 00:02:25,810
And also how should we
go about quantifying and
measuring it both from a human

39
00:02:25,811 --> 00:02:30,811
perspective and also is there a way to
distill it down so that we can optimize

40
00:02:32,381 --> 00:02:36,040
for interpretability maybe without
all of those expensive human subject

41
00:02:36,041 --> 00:02:37,140
evaluations.
That's it.

42
00:02:37,141 --> 00:02:40,570
Those are the sorts of things that
we're gonna be chatting about. So first,

43
00:02:40,571 --> 00:02:43,090
let's start out with this question
of what is interpretability.

44
00:02:43,420 --> 00:02:46,870
So if you look at a dictionary
definition of the word interpret,

45
00:02:46,900 --> 00:02:49,120
which is the root word
for interpretability,

46
00:02:49,150 --> 00:02:52,090
it's to give or provide
meaning to explain.

47
00:02:52,390 --> 00:02:57,390
And I think that explanation is actually
a much more like human tangible word

48
00:02:57,551 --> 00:02:59,770
than interpretability, right? It's,

49
00:02:59,800 --> 00:03:03,190
it's easier to think about
what is a good explanation.

50
00:03:03,191 --> 00:03:05,830
Have I explained something to you?
Because you can think about whether,

51
00:03:05,831 --> 00:03:07,780
do you understand it or not?
All right,

52
00:03:07,781 --> 00:03:10,870
so that's the term that I'm really going
to be thinking of and I'm encouraging

53
00:03:10,930 --> 00:03:14,320
you all to think about when you think
of EC interpretability is the question,

54
00:03:15,070 --> 00:03:19,600
what's the quality of this explanation?
And before I go forward,

55
00:03:19,601 --> 00:03:23,830
I also want you distinguish the word
interpretability or the process of

56
00:03:23,831 --> 00:03:28,831
providing explanation from a lot of
other words that we also see as auxiliary

57
00:03:29,231 --> 00:03:32,140
criteria for our machine learning systems.
So there,

58
00:03:32,170 --> 00:03:35,530
there was a time when we were
just interested in prediction.

59
00:03:35,590 --> 00:03:39,760
So we just need a system to predict
and now we're often interested in other

60
00:03:39,761 --> 00:03:40,660
criteria as well.

61
00:03:40,840 --> 00:03:45,490
So there's been a lot of recent interest
in transparent systems in accountable

62
00:03:45,491 --> 00:03:49,030
systems, trustable systems, fair systems,

63
00:03:49,360 --> 00:03:51,670
systems that produce actionable outputs,

64
00:03:51,880 --> 00:03:54,970
systems that are robust or
reliable systems that are safe.

65
00:03:55,780 --> 00:04:00,760
And some of these areas I forgot,
privacy, privacy is another one. Um,

66
00:04:00,820 --> 00:04:01,241
so there,

67
00:04:01,241 --> 00:04:05,230
there's a lot of these sorts of terms
that are being bandied about and some of

68
00:04:05,231 --> 00:04:06,100
them like privacy,

69
00:04:06,101 --> 00:04:10,000
they have very clear there are some
at least attempt at making a rigorous

70
00:04:10,001 --> 00:04:11,110
definition.
For example,

71
00:04:11,111 --> 00:04:15,150
differential privacy has a very clear
mathematical formalism. You can, I,

72
00:04:15,340 --> 00:04:17,950
you can verify whether a system
is differentially private or not.

73
00:04:17,980 --> 00:04:19,870
And we don't have that
in interpretability.

74
00:04:20,410 --> 00:04:25,360
And one of my goals is to get us closer
to thinking about what those sorts of

75
00:04:25,361 --> 00:04:26,470
definition should be.

76
00:04:27,700 --> 00:04:32,110
The reason I distinguish the process of
explanation from these other criteria is

77
00:04:32,111 --> 00:04:36,070
it oftentimes we fall back
on interpretability or
the need for explanation

78
00:04:36,400 --> 00:04:40,180
because some of these other criteria are
also not particularly well quantified.

79
00:04:40,181 --> 00:04:42,700
So we want us to send to be safe.
For example,

80
00:04:42,910 --> 00:04:46,980
what if we can't list out all the
situations in which an unsafe, uh,

81
00:04:47,080 --> 00:04:49,660
scenario might happen, we
might fall back on, well,

82
00:04:49,661 --> 00:04:52,150
if the system can at least
explain what it's doing,

83
00:04:52,360 --> 00:04:57,100
then we're in a good position to identify
whether something might be safe or

84
00:04:57,101 --> 00:04:59,050
unsafe with a bit of human checking.
Right?

85
00:04:59,051 --> 00:05:02,650
So interpretability is this
thing that you do mostly in the,

86
00:05:02,651 --> 00:05:06,250
probably in the service of something
else, right? So let's, let's, uh,

87
00:05:06,340 --> 00:05:09,640
let's actually think about what reasons
you might need for interpretability.

88
00:05:11,080 --> 00:05:14,380
So one area that I work in
quite a bit is in healthcare.

89
00:05:14,770 --> 00:05:18,460
So we are interested in
deriving disease subtypes.

90
00:05:18,490 --> 00:05:22,060
We are also interested in identifying
what treatments work best for what

91
00:05:22,061 --> 00:05:22,894
patients.

92
00:05:23,200 --> 00:05:27,250
And in this case there is
a clear scientific question
of like what is disease,

93
00:05:27,251 --> 00:05:31,150
how does it progress, how to drugs
interact with a disease process.

94
00:05:31,480 --> 00:05:35,650
And interpretability is really important
because we are looking at large amounts

95
00:05:35,651 --> 00:05:36,011
of data.

96
00:05:36,011 --> 00:05:39,430
But at the end of the day we actually
want to have some new insights about the

97
00:05:39,431 --> 00:05:42,670
mechanism of human physiology
and pathophysiology.

98
00:05:44,170 --> 00:05:47,410
Another reason why you might
want interpretability is
to be able to debug your

99
00:05:47,411 --> 00:05:50,590
system. You have no idea why
the system is not working.

100
00:05:50,590 --> 00:05:53,470
But if the system can give you an
explanation of how it's coming to its

101
00:05:53,471 --> 00:05:54,011
decisions,

102
00:05:54,011 --> 00:05:58,400
maybe you can identify why that decision
is wonky and find the the issue in the

103
00:05:58,401 --> 00:06:02,180
system. I also met, I already
mentioned the issue of safety. Again,

104
00:06:02,181 --> 00:06:07,181
maybe you can't identify all the scenarios
under which a system might be unsafe,

105
00:06:07,400 --> 00:06:11,660
but at least if it tells you under at any
point what you might want to do or why

106
00:06:11,661 --> 00:06:12,471
it's making a decision,

107
00:06:12,471 --> 00:06:17,210
you can decide whether what you want to
do with that decision. Another big area,

108
00:06:17,270 --> 00:06:21,410
um, category that I put a
here is mismatched objectives.

109
00:06:22,190 --> 00:06:24,110
So,
going back to the clinical scenario,

110
00:06:24,380 --> 00:06:28,250
sometimes we have a thing that
we're interested in terms of output.

111
00:06:28,251 --> 00:06:29,330
So for example,

112
00:06:29,600 --> 00:06:33,470
if you have a patient who is depressed
and you want to manage their depression,

113
00:06:34,340 --> 00:06:37,880
then that's the criteria and maybe
that you're trying to optimize.

114
00:06:38,690 --> 00:06:43,100
But it might also be the case that
that patient is averse to weight gain.

115
00:06:43,610 --> 00:06:48,170
And so there might be this auxiliary or
secondary objective that you were not

116
00:06:48,171 --> 00:06:49,004
aware of,

117
00:06:49,310 --> 00:06:53,240
or maybe there's a particular drug that
requires being taken three times a day

118
00:06:53,450 --> 00:06:57,440
versus a drug that being taken once a day.
And depending on the patient,

119
00:06:58,070 --> 00:07:01,620
the probability of adherence may be
different for those two regimens.

120
00:07:02,120 --> 00:07:04,340
And if you have those written out,

121
00:07:04,370 --> 00:07:06,860
then maybe you could write them all
down and you could put them into an

122
00:07:06,861 --> 00:07:10,520
objective function or just solve the
multi objective problem. You can,

123
00:07:10,521 --> 00:07:14,210
we know how to find Paredo fronts when we
have multiple objectives, right? Again,

124
00:07:14,211 --> 00:07:17,300
there's no interpretability
required there. Um,

125
00:07:17,360 --> 00:07:20,900
but if we don't actually have the full
objective at the time and maybe the human

126
00:07:20,901 --> 00:07:23,150
is not even aware of
exactly what they want,

127
00:07:23,930 --> 00:07:27,260
then that you have run into this case
of mismatch objectives where you need a

128
00:07:27,261 --> 00:07:30,920
little bit of explanation that
might suggest why this was chosen,

129
00:07:31,160 --> 00:07:35,900
but maybe you can also decide that you
want to do something else. Um, and,

130
00:07:36,080 --> 00:07:38,480
and change the,
the final decision.

131
00:07:39,310 --> 00:07:42,560
And the final one that I'm going
to point out is in a legal setting.

132
00:07:42,561 --> 00:07:47,561
So EU regulation is going to require all
systems algorithmic systems to provide

133
00:07:48,321 --> 00:07:52,730
explanation starting in 2018 2019.

134
00:07:53,300 --> 00:07:56,330
And I think we as a community,

135
00:07:56,750 --> 00:08:01,040
as computer scientists, as people who
are interested in interpretability,

136
00:08:01,400 --> 00:08:06,380
have a real opportunity here to
make sure that that regulation,

137
00:08:06,410 --> 00:08:09,260
like the definition of that is
going to be decided in courts,

138
00:08:09,530 --> 00:08:12,800
but we need to be able to come up
with proposals of what we think

139
00:08:12,801 --> 00:08:16,790
interpretability should been and what
the reasonable definition should be.

140
00:08:16,970 --> 00:08:19,340
Otherwise it's going to be decided for us.
Right?

141
00:08:19,370 --> 00:08:23,660
So there's kind of a pressing need here
from a legal perspective to come up with

142
00:08:23,661 --> 00:08:26,210
operational definitions
of interpretability.

143
00:08:27,890 --> 00:08:30,470
So in all of these cases that I described,

144
00:08:31,280 --> 00:08:36,020
I would say the common thread is that
there's a fundamental incompleteness in

145
00:08:36,021 --> 00:08:37,370
the problem specification.

146
00:08:38,310 --> 00:08:42,320
And I want to distinguish that from the
concept of uncertainty. For example,

147
00:08:42,440 --> 00:08:47,180
if you are trying to track a
missile on a radar, you know the,

148
00:08:47,181 --> 00:08:50,450
the radar does not exactly
pick up where the missile is.

149
00:08:50,451 --> 00:08:53,420
There's going to be some uncertainty and
you have this ellipse about where you

150
00:08:53,421 --> 00:08:54,254
think the missile is.

151
00:08:54,410 --> 00:08:57,810
You get your set of measurements
and it decreases your uncertainty.

152
00:08:57,811 --> 00:09:00,450
Or maybe it increases it because
it's something unexpected.

153
00:09:00,780 --> 00:09:05,610
And now maybe you can turn on different
kinds of radar and you can use that to

154
00:09:05,640 --> 00:09:10,050
determine further hone your estimates.
But this is a closed system, right?

155
00:09:10,051 --> 00:09:15,051
You have a set of measurements and you
can very well quantify the uncertainty,

156
00:09:15,990 --> 00:09:19,410
right? So in that sense, the
problem specification is uncertain,

157
00:09:19,411 --> 00:09:21,600
but it's not incomplete.
And to the end of the day,

158
00:09:21,601 --> 00:09:24,600
you're going to have to make a
decision with that uncertainty.

159
00:09:24,601 --> 00:09:27,930
And we have rigorous ways of
decision making under uncertainty.

160
00:09:27,930 --> 00:09:32,930
That's an entire branch of cs so that
if you're living in that scenario,

161
00:09:33,330 --> 00:09:34,920
you don't really need interpretability.

162
00:09:34,921 --> 00:09:38,160
There's no human in the loop necessary
required because at the end of the day,

163
00:09:38,161 --> 00:09:40,500
you're just going to have to make
the best decision you can given the

164
00:09:40,501 --> 00:09:44,880
uncertainty that you have and the
examples that I've described here.

165
00:09:45,450 --> 00:09:48,990
In contrast,
you have this fundamental incompleteness.

166
00:09:48,991 --> 00:09:53,991
There's no way around the fact that
if you are trying to create scientific

167
00:09:54,361 --> 00:09:55,260
hypotheses,

168
00:09:55,920 --> 00:09:59,610
you don't know exactly what you're
looking for and you need some sort of

169
00:09:59,611 --> 00:10:01,950
explanation to help you
generate those hypotheses.

170
00:10:02,460 --> 00:10:07,460
Or if you cannot write down all of your
safety scenarios in terms of unit tests,

171
00:10:08,280 --> 00:10:09,750
then you need the explanation.

172
00:10:09,751 --> 00:10:12,060
There is a fundamental thing
that you can just cannot,

173
00:10:12,061 --> 00:10:16,830
you cannot unit test your way out of
this, um, particular situation. Right. So,

174
00:10:16,831 --> 00:10:18,880
so that's one thing to
keep in mind because I,

175
00:10:19,030 --> 00:10:22,650
I guess there's various debate about
whether interpretability is needed or when

176
00:10:22,651 --> 00:10:23,484
it is needed.

177
00:10:23,670 --> 00:10:28,670
And this is the definition that I like
to stick with or reasoning that I like to

178
00:10:29,311 --> 00:10:31,290
stick with in terms of
fundamental incompleteness.

179
00:10:32,460 --> 00:10:36,340
So before I go into talking a
little bit more about, um, uh,

180
00:10:37,030 --> 00:10:38,490
ways to measure interpretability,

181
00:10:38,670 --> 00:10:41,760
I'm just going to give you a little
teaser or a taste of the sorts of things

182
00:10:41,761 --> 00:10:46,680
that we're doing in our group. Just one
little short Vignette, a little story. Um,

183
00:10:46,820 --> 00:10:50,720
uh, so this is work that is, was
just accepted to Itch Chi, um,

184
00:10:50,760 --> 00:10:55,360
which we are calling right for the
right reasons. And here's, I mean,

185
00:10:55,620 --> 00:10:58,260
here's the main idea. Um, so we have,

186
00:10:58,500 --> 00:11:01,260
let's say we're interested in
creating some sort of different,

187
00:11:01,261 --> 00:11:03,750
we have a differentiable Cli
classifier such as neural network.

188
00:11:04,530 --> 00:11:08,490
And so we have some set of
outputs and we have some inputs.

189
00:11:08,910 --> 00:11:10,320
And what,
again,

190
00:11:10,590 --> 00:11:14,110
the choice of this function can be
anything like a neural network. Um,

191
00:11:14,130 --> 00:11:17,100
and then we have some set of
weights that it's parameterized by.

192
00:11:19,200 --> 00:11:22,740
And then if we have that,
uh, the question could be,

193
00:11:23,130 --> 00:11:28,130
are there multiple qualitatively different
classifiers and why it might be we

194
00:11:28,831 --> 00:11:29,730
interested in that.
Well,

195
00:11:29,731 --> 00:11:34,731
maybe there's these reasons over here
where one of them might be unsafe or it

196
00:11:35,251 --> 00:11:37,780
might not be reliable.
Um,

197
00:11:37,980 --> 00:11:42,510
it may be the case that one has some
subtle subtle issue with it due to a

198
00:11:42,511 --> 00:11:43,344
confounder.

199
00:11:43,740 --> 00:11:48,210
And so without the information
of why it's making the decision,

200
00:11:48,570 --> 00:11:50,700
um, we, we can't tell
for those sort of things.

201
00:11:50,701 --> 00:11:54,970
But if a classifier can tell you why
it's making the decision and we can find

202
00:11:54,971 --> 00:11:58,570
multiple classifiers who make the
same decision for different reasons,

203
00:11:58,930 --> 00:12:02,920
then maybe we can try to give those to
someone and have them decide which is the

204
00:12:02,921 --> 00:12:04,420
best one,
which is the one that they like.

205
00:12:04,660 --> 00:12:08,170
And maybe just exposing the fact that
there's multiple different ways to get the

206
00:12:08,171 --> 00:12:12,220
same level of prediction accuracy with
different types of classifiers is also

207
00:12:12,221 --> 00:12:15,430
kind of enlightening to them about the
nature of the Dataset that you have on

208
00:12:15,431 --> 00:12:16,500
hand.
Uh,

209
00:12:16,960 --> 00:12:20,110
the way we do this is we're going to
define our explanations in terms of the

210
00:12:20,111 --> 00:12:20,944
gradient.

211
00:12:21,100 --> 00:12:25,570
So if you tried to change your
value of the input by a little bit,

212
00:12:25,571 --> 00:12:27,310
remember that x is the input.

213
00:12:27,340 --> 00:12:29,800
If you change the value of
the input by some amount,

214
00:12:30,160 --> 00:12:32,440
how much does the output change?
Um,

215
00:12:32,441 --> 00:12:35,770
so if you have an input
that is very sensitive to,

216
00:12:36,280 --> 00:12:41,050
then that is something that should be
upgraded and your explanation or be part

217
00:12:41,051 --> 00:12:42,630
of your explanation.
Um,

218
00:12:42,700 --> 00:12:46,780
whereas if it's something where changing
this x doesn't really change your

219
00:12:46,781 --> 00:12:50,380
output,
then maybe it's not an important feature.

220
00:12:51,310 --> 00:12:55,540
So ingredients have been used as a form
of explanation. This is not new to us.

221
00:12:55,570 --> 00:12:58,570
It had been proposed as a form
of explanation in the past.

222
00:12:58,810 --> 00:13:00,890
And the Nice thing about
these is that they're,

223
00:13:00,930 --> 00:13:04,870
they do rely on derivatives
and derivatives are very
easy for us to compute.

224
00:13:04,870 --> 00:13:08,560
They're very fast compared to
perturbation based approaches.

225
00:13:08,561 --> 00:13:13,561
So there's a lot of other approaches
such as Lyme that take the x,

226
00:13:13,811 --> 00:13:14,890
perturb the x,

227
00:13:14,920 --> 00:13:18,880
and see what perturbations of x
result in the biggest changes in why.

228
00:13:19,250 --> 00:13:21,670
I said it's another way of doing
your sensitivity analysis through to

229
00:13:21,671 --> 00:13:25,840
perturbations. We're doing that now. An
infinitesimal scale with derivatives.

230
00:13:26,050 --> 00:13:27,970
The advantage being that they're,
again, they're super fast.

231
00:13:28,810 --> 00:13:32,950
And now what we can do is we can say,
if you're given one set of weights,

232
00:13:33,340 --> 00:13:38,340
let's find a different set of weights
that are not allowed to use the gradients

233
00:13:39,551 --> 00:13:41,890
that we're large in the next round.

234
00:13:42,310 --> 00:13:47,310
So round one you create
your own classifier round
to either someone provides

235
00:13:47,351 --> 00:13:49,600
some annotations and say's
actually these were bad,

236
00:13:49,690 --> 00:13:53,770
like don't use this information when
making this decision or you just try to

237
00:13:53,771 --> 00:13:57,100
find a completely different
solution. Right. Um,

238
00:13:57,850 --> 00:13:59,950
so I'm just going to give you,
again, this is a short story,

239
00:13:59,951 --> 00:14:02,590
so I'm just going to give you some
examples of how this sort of system might

240
00:14:02,591 --> 00:14:07,390
work. So here we have an example where
we're trying to classify these images,

241
00:14:07,420 --> 00:14:10,840
these five by five images
into two categories.

242
00:14:11,440 --> 00:14:15,190
And the choice of the category
can be made in two different ways.

243
00:14:15,460 --> 00:14:20,460
So it can be made depending on whether
the corners of the square match in color

244
00:14:21,191 --> 00:14:22,620
or not.
Uh,

245
00:14:22,760 --> 00:14:27,760
and it can also be made based on whether
these top three elements of the square

246
00:14:27,880 --> 00:14:28,713
match or not.

247
00:14:29,380 --> 00:14:34,380
And so there's two different ways of
doing the classification that are going to

248
00:14:34,481 --> 00:14:38,930
lead you to exactly the same
classification accuracy. So the, the,

249
00:14:38,980 --> 00:14:41,470
in the first round, uh,
the of the class of Arthur,

250
00:14:41,471 --> 00:14:46,180
this is accuracy in the first round of
the classifier. Um, we're doing it again,

251
00:14:46,181 --> 00:14:48,470
this is a simple classification problems.

252
00:14:48,490 --> 00:14:51,920
So you get a hundred percent performance
and it's mostly picking out these

253
00:14:51,921 --> 00:14:52,754
corners.

254
00:14:52,940 --> 00:14:56,660
Second round we say you're not allowed
to use the information that was highly

255
00:14:56,661 --> 00:14:57,770
weighted in the first round.

256
00:14:58,100 --> 00:15:01,100
Find me a completely different
classifier or as best as you can,

257
00:15:01,101 --> 00:15:05,150
you have a parameter that you can tune.
And so the second round,

258
00:15:05,151 --> 00:15:08,690
while there is actually another example
that exists, another choice that exists.

259
00:15:09,140 --> 00:15:11,120
So it gives you that choice out here.

260
00:15:11,480 --> 00:15:14,540
And then you go to the third round and
you say, actually, now I want one more.

261
00:15:14,750 --> 00:15:16,350
Can you, can you do it for me? Um,

262
00:15:16,510 --> 00:15:20,590
and it tries and it's mostly
picking up noise and the,

263
00:15:20,620 --> 00:15:24,800
the accuracy goes down. So this,
this is a very simple approach,

264
00:15:24,810 --> 00:15:28,370
very fast approach. Because again,
it's just based on gradients,

265
00:15:28,400 --> 00:15:32,450
competing gradients, twice. Essentially.
You need to be able to take your function,

266
00:15:32,451 --> 00:15:33,171
differentiate it,

267
00:15:33,171 --> 00:15:36,290
and then you need to differentiate again
to be able to compute sensitivity to

268
00:15:36,291 --> 00:15:38,480
the input gradients.
But that's all you need to do.

269
00:15:39,770 --> 00:15:41,690
And then we can do this on
more complicated setting.

270
00:15:41,690 --> 00:15:45,650
So here we played a similar
game with two UCI data sets,

271
00:15:45,651 --> 00:15:46,790
Iris and breast cancer.

272
00:15:46,791 --> 00:15:51,650
And what we did is we took the outputs
and we glued together examples from these

273
00:15:51,651 --> 00:15:53,800
two different data sets
based on the outputs.

274
00:15:53,801 --> 00:15:57,560
So if it's like flour of
type A and tumor of type B,

275
00:15:57,770 --> 00:16:00,680
those are all get linked together
concatenated together. So now again,

276
00:16:00,681 --> 00:16:02,750
there's two rules for making the decision.

277
00:16:03,470 --> 00:16:07,520
And here what we find is that this
is much more complicated dataset.

278
00:16:07,550 --> 00:16:11,720
There's many ways in which you can,
so here iterations,

279
00:16:11,870 --> 00:16:15,500
there's actually many ways you can get
very good prediction accuracy and this

280
00:16:15,501 --> 00:16:18,350
isn't going to useful thing to keep in
mind that from any real world data sets

281
00:16:18,351 --> 00:16:23,300
this happens. Um, but you see that the
test accuracy test accuracy, there you go,

282
00:16:23,600 --> 00:16:26,620
um, is lower sometimes because
it's picking up. So in the,

283
00:16:26,780 --> 00:16:30,650
this was in the training and then
in the tests that we took away,

284
00:16:30,651 --> 00:16:33,440
basically the flower information,
like we permuted the labels.

285
00:16:33,470 --> 00:16:37,610
So the right thing to focus on
was the cancer tissue question.

286
00:16:38,240 --> 00:16:41,150
So initially you see that it had
picked up on the wrong stuff.

287
00:16:41,390 --> 00:16:44,870
And as you go through fewer,
as you go through additional iterations,

288
00:16:44,871 --> 00:16:46,550
it ends up picking up on the right things.

289
00:16:47,480 --> 00:16:50,180
And there's no guarantee that this is
going to happen in that particular order.

290
00:16:50,181 --> 00:16:52,280
But the ability to be
able to produce quickly,

291
00:16:52,550 --> 00:16:55,640
qualitatively different solutions helps
you sift through them and try to find

292
00:16:55,641 --> 00:16:56,480
ones that are reasonable.

293
00:16:57,800 --> 00:17:00,770
And this is the last example
I'll give you along these lines.

294
00:17:01,460 --> 00:17:06,460
So here we have an example
from 20 newsgroups and uh,

295
00:17:06,530 --> 00:17:10,040
these are examples of words that
it's picking out, uh, in different,

296
00:17:10,100 --> 00:17:12,710
in different iterations. So this is, uh,

297
00:17:12,860 --> 00:17:16,340
being able to distinguish the
atheism and Christianity threads,

298
00:17:16,370 --> 00:17:20,930
which was used in previous papers, uh,
on interpretability. Hence we chose it.

299
00:17:21,560 --> 00:17:26,390
Um, you, what you see
again is that there's many
classifiers that have reasonable

300
00:17:26,391 --> 00:17:27,850
performance.
Um,

301
00:17:27,890 --> 00:17:32,270
and you see that the test performance
varies quite a bit depending on the

302
00:17:32,271 --> 00:17:33,860
different choices of classifiers.

303
00:17:33,890 --> 00:17:37,220
Cause like sometimes they end up picking
up weird confounds in the data. This,

304
00:17:37,250 --> 00:17:41,030
this data set has a lot of strange
confounds that you can pick up on because

305
00:17:41,031 --> 00:17:45,170
sometimes the person posting their name
actually be ends up being like a really

306
00:17:45,350 --> 00:17:50,190
key signal and on and all sorts of things.
And the point here is that in this,

307
00:17:50,370 --> 00:17:53,700
so this is a much more real setting.
You might not know what the right one is,

308
00:17:54,090 --> 00:17:55,220
but at least you can cut,

309
00:17:55,260 --> 00:17:58,760
you can skim through the factors
that are being used and look at um,

310
00:17:58,770 --> 00:18:02,430
the prediction performances. And you
can decide, you can say, you know,

311
00:18:02,431 --> 00:18:05,340
I really just don't like that classifier.
I want a different one.

312
00:18:05,640 --> 00:18:07,770
And you can keep asking it
to produce different ones.

313
00:18:08,910 --> 00:18:12,870
So this is just an example of the sort
of things that we do in our group where

314
00:18:12,871 --> 00:18:17,871
we're interested in creating these sorts
of systems that assist in projects that

315
00:18:18,661 --> 00:18:20,820
are ultimately going to require a human.

316
00:18:21,210 --> 00:18:25,230
Because there is a fundamental in
completeness in the problem specification.

317
00:18:25,560 --> 00:18:28,770
We know that the data that
we have is not enough.

318
00:18:29,310 --> 00:18:33,180
The data combined with the
problem specification that
we have is not enough that

319
00:18:33,181 --> 00:18:36,660
a system can just solve this problem
on its own, right. There's going to be,

320
00:18:36,661 --> 00:18:40,110
have to be some sort of interaction and
because there's going to be some sort of

321
00:18:40,111 --> 00:18:42,570
interaction,
we need to be able to provide a,

322
00:18:42,571 --> 00:18:43,860
what is it useful things for us to do.

323
00:18:43,861 --> 00:18:47,490
It's useful for us to provide explanation
is useful for us to provide multiple

324
00:18:47,491 --> 00:18:51,720
explanations. Cool. So
let's now go back to,

325
00:18:52,200 --> 00:18:55,890
um, the, the evaluation question. Yup.

326
00:18:56,580 --> 00:18:57,413
First,
with this work,

327
00:18:57,710 --> 00:19:00,050
have you looked at reading and
stone bowls with these different,

328
00:19:02,330 --> 00:19:05,850
so, so the question was, have we looked
at creating ensembles of these networks?

329
00:19:06,090 --> 00:19:06,960
And one could certainly do,

330
00:19:06,961 --> 00:19:10,770
so I guess we've been in the mindset
that some of these are going to be right

331
00:19:10,771 --> 00:19:14,070
for the wrong reasons and we just
wouldn't want to use them at all because

332
00:19:14,071 --> 00:19:15,630
they're,
they're going to pick up on a confounder.

333
00:19:15,930 --> 00:19:19,410
But certainly you could use this to
create just more robust decision makers,

334
00:19:19,411 --> 00:19:21,930
like a way to propose an ensemble
that you could then do, stir,

335
00:19:21,960 --> 00:19:22,793
do whatever you want it

336
00:19:25,170 --> 00:19:30,160
are questions. Yup. Yeah. Any one of the

337
00:19:33,290 --> 00:19:34,123
interpreter.

338
00:19:36,060 --> 00:19:40,170
So this is, we didn't do a user
study, but just among ourselves. Um,

339
00:19:40,200 --> 00:19:43,020
there were definitely ones that
made a lot of sense. I mean,

340
00:19:43,050 --> 00:19:47,220
so the first one is essentially picking
out words like Christian and atheism.

341
00:19:47,730 --> 00:19:52,440
It seems very reasonable for the
classification task at hand. Um,

342
00:19:52,620 --> 00:19:56,310
some of the later ones end up with names
of people and we could look at those

343
00:19:56,311 --> 00:20:00,110
and say these are clearly not
making sense because it was,

344
00:20:00,150 --> 00:20:02,550
they were picking up
users and such like this

345
00:20:03,670 --> 00:20:06,180
Qca or something.
Same thing.

346
00:20:07,950 --> 00:20:09,900
There might be one in your room.

347
00:20:14,590 --> 00:20:16,030
So, uh, the, the,

348
00:20:16,040 --> 00:20:19,170
the comment was some of the axes end up
being interpretable and some of them may

349
00:20:19,171 --> 00:20:23,940
be less interpretable. Um, and so
the, the, that, that's a fair point.

350
00:20:24,180 --> 00:20:29,070
And one thing that we're working on now
is can we at least limit the number of

351
00:20:29,071 --> 00:20:33,030
things that are sensitive to so you can
at least look at them and so not only

352
00:20:33,031 --> 00:20:35,790
give us multiple cloud
qualitatively different classifiers,

353
00:20:36,030 --> 00:20:40,260
but can you produce a classifier that
for any test point is going to give you a

354
00:20:40,261 --> 00:20:44,070
succinct explanation and that's something
that we're actively working on right

355
00:20:44,071 --> 00:20:47,890
now and is relatively easy to build into
the loss function because now you want

356
00:20:47,891 --> 00:20:51,670
to say that the input gradient should
not be large but for more than a certain

357
00:20:51,671 --> 00:20:55,330
number of elements for every
input and for different inputs.

358
00:20:55,660 --> 00:21:00,430
You may allow different things to be
important, but for any particular input,

359
00:21:00,431 --> 00:21:03,190
we only want a few things to be important
and this is something that you can

360
00:21:03,191 --> 00:21:04,630
again easily put into this framework.

361
00:21:06,410 --> 00:21:07,920
I'm just trying to understand it.

362
00:21:07,950 --> 00:21:12,500
When you say you do a first pass,
you find some classifier,

363
00:21:12,510 --> 00:21:15,150
this seems to work and you say,
oh, I don't like that classifier.

364
00:21:15,151 --> 00:21:17,400
It's fight for the wrong reason.
I'm just curious,

365
00:21:17,401 --> 00:21:19,830
does that mean it's not really right,

366
00:21:19,831 --> 00:21:23,100
like it's not predictive or it's
predictive, but for some policy reason,

367
00:21:23,101 --> 00:21:23,940
I don't want to use it.

368
00:21:24,520 --> 00:21:27,220
It's predictive, but first some
policy reason I don't want to use it.

369
00:21:27,550 --> 00:21:30,430
And the one example could be that,

370
00:21:30,431 --> 00:21:33,050
let's say that it's picking
up a confound in the data set.

371
00:21:33,051 --> 00:21:36,460
So there's this really popular example
and the interpretability world,

372
00:21:36,461 --> 00:21:41,080
if this wolf where says Huskey the
wolves are usually out in nature scenes.

373
00:21:41,081 --> 00:21:44,320
The huskies are usually like an
indoor scenes and so you could build a

374
00:21:44,321 --> 00:21:47,590
classifier and say's if there's a tree
in the picture, it's probably a wolf.

375
00:21:48,130 --> 00:21:51,760
And that's a classifier that on your
data will do really well because your

376
00:21:51,761 --> 00:21:54,730
data's had happened to have this bias
in it that you were not aware of.

377
00:21:55,180 --> 00:21:57,400
And so that's an example of where
you might want to throw that away.

378
00:21:57,401 --> 00:22:00,820
And you might want to say that that's
actually not the explanation that I want.

379
00:22:00,821 --> 00:22:01,810
I want a different one.

380
00:22:01,990 --> 00:22:02,830
So in some sense,

381
00:22:02,831 --> 00:22:06,580
are you repairing the specification
that you gave the AML system?

382
00:22:06,820 --> 00:22:10,990
You said optimize this, it did it did
it correctly and now you're just saying,

383
00:22:11,010 --> 00:22:13,750
oh no, I didn't want to use that.
I didn't properly specify it.

384
00:22:13,751 --> 00:22:16,930
I meant find wolves independent
of whether or not there are trees.

385
00:22:17,240 --> 00:22:20,180
Exactly. It's not so, so it, right.
So you hit it right on the head.

386
00:22:20,210 --> 00:22:24,680
But I think the case where you need
interpretability is where the problem,

387
00:22:24,710 --> 00:22:26,780
there's something wrong
with the specification. Yup.

388
00:22:26,910 --> 00:22:28,750
That was, and that's a
question I'm trying to get to.

389
00:22:28,751 --> 00:22:31,090
I'm just trying to understand.
So I understand that task,

390
00:22:31,240 --> 00:22:34,630
which is for policy reasons, I don't
want to use that. Yes, that classifier.

391
00:22:34,960 --> 00:22:36,940
But now I'm trying to connect back
to the title of the talk around

392
00:22:36,941 --> 00:22:41,070
interpretability, which is the classifier
was doing its job and if I had to,

393
00:22:41,200 --> 00:22:43,540
if I just had to explain what
it was doing, I could say,

394
00:22:43,541 --> 00:22:47,050
Oh yes and here's why I find that it
used the tree. Yes. Right. Or you know,

395
00:22:47,051 --> 00:22:51,420
it used this blood test to decide this
treatment or use this factor to decide

396
00:22:51,430 --> 00:22:54,190
the credit worthiness of this
European Union resident or something.

397
00:22:54,490 --> 00:22:58,330
It was doing an accurate, it was
solving the problem. I just, for a Po,

398
00:22:58,390 --> 00:23:02,130
I'm trying to decide which is it that
we wanted select a different set of,

399
00:23:02,320 --> 00:23:05,200
of Mol Program is for,
for policy reasons and which is it,

400
00:23:05,201 --> 00:23:06,310
we want to know what they're doing.

401
00:23:06,640 --> 00:23:09,700
So is it we want to understand them or
is it we we only want to use certain

402
00:23:09,701 --> 00:23:10,534
types of,

403
00:23:11,010 --> 00:23:13,900
right. So, so the focus of this
talk is going to be around the, the,

404
00:23:13,901 --> 00:23:18,430
the definition and the evaluation
of interpretability. And this, this,

405
00:23:18,431 --> 00:23:22,180
this is a short vignette to explain just,
just to make concrete,

406
00:23:22,210 --> 00:23:25,390
like I gave a bunch of reasons why
you want might want interpretability.

407
00:23:25,540 --> 00:23:29,710
So this is an example of a task where
you wanted interpretability to be able to

408
00:23:29,711 --> 00:23:33,850
check whether the classifier was making
its decisions for the right reasons and

409
00:23:33,851 --> 00:23:36,490
be able to reject it if it was
making it for the wrong reasons.

410
00:23:36,730 --> 00:23:39,940
So this is just an example of a situation
where you might want interpretability

411
00:23:40,120 --> 00:23:41,860
but there are many.
And so we're going to move out.

412
00:23:41,861 --> 00:23:44,390
We're going to zoom back out
into the general question. Uh,

413
00:23:44,430 --> 00:23:48,710
and this was just one specific example
of a situation where you might want this

414
00:23:49,450 --> 00:23:50,283
[inaudible].

415
00:23:51,350 --> 00:23:54,500
All right. So, so the big
question is like in this case,

416
00:23:54,501 --> 00:23:56,660
maybe you had someone in the loop,
right?

417
00:23:56,690 --> 00:24:00,800
Maybe you had someone providing that
feedback about, I like this explanation.

418
00:24:00,801 --> 00:24:04,730
I don't like this explanation. And you
had a system, right? You, you were,

419
00:24:04,731 --> 00:24:09,170
that you were trying to build for
certain policy related task, right? But,

420
00:24:09,200 --> 00:24:11,660
and so presumably if you
were trying to do that,

421
00:24:11,840 --> 00:24:15,320
then you have a measure of how well it
works because you have a task in mind,

422
00:24:15,321 --> 00:24:16,850
you have an expert in mind,

423
00:24:17,120 --> 00:24:21,050
that expert can check and you can
can request additional solutions.

424
00:24:22,360 --> 00:24:22,680
Okay.

425
00:24:22,680 --> 00:24:25,320
But now let's think about it
more broadly, right? What,

426
00:24:25,321 --> 00:24:28,410
how can you measure interpretability

427
00:24:29,150 --> 00:24:29,450
okay.

428
00:24:29,450 --> 00:24:33,470
In the APP, not necessarily any absence,
but in the abstract, right? In the,

429
00:24:33,471 --> 00:24:37,790
in the more general sense. Uh,
so one option is to say, well,

430
00:24:37,791 --> 00:24:41,920
interpretability it's like porn. You
know it when you see it. Um, and so it,

431
00:24:41,960 --> 00:24:43,940
or this is a kind of
related way of saying it,

432
00:24:43,941 --> 00:24:45,770
is that we have interpretability by Fiat,
right?

433
00:24:45,771 --> 00:24:49,370
Or just going to call this
thing interpretable cause
we've just decided that

434
00:24:49,390 --> 00:24:51,740
this thing is interpretable.
And so if we look at various,

435
00:24:51,810 --> 00:24:53,630
and these are like really
good papers by the way,

436
00:24:53,631 --> 00:24:55,220
so I'm not making fun of any of them.

437
00:24:55,490 --> 00:24:58,790
But what I want to point out is that
you often see statements in machine

438
00:24:58,791 --> 00:25:02,840
learning papers and I'm guilty of this
as well. Um, uh, of the form, you know,

439
00:25:03,050 --> 00:25:08,020
certain type of model is a gold standard
for intelligibility. Um, you know, the,

440
00:25:08,120 --> 00:25:11,460
our rationales are simply subsets of
words. You know, this is just saying like,

441
00:25:11,570 --> 00:25:13,790
clearly this thing is interpretable,
right?

442
00:25:13,920 --> 00:25:17,210
These sentences are just telling you
that of course this is the right way to

443
00:25:17,211 --> 00:25:18,340
solve this problem.
And,

444
00:25:18,430 --> 00:25:21,050
and the thing that when we read
this and it seems reasonable to us,

445
00:25:21,051 --> 00:25:24,800
there is a face validity to this because
the whole point about interpretability

446
00:25:24,840 --> 00:25:26,930
that didn't make sense to human beings.
We're all human beings.

447
00:25:26,931 --> 00:25:31,160
So if this explanation kind of makes
sense to us, that is cool. Right? Um,

448
00:25:31,190 --> 00:25:35,720
but at the same time, um, it doesn't
really let us go forward, right?

449
00:25:35,721 --> 00:25:38,480
Because it gives us a set of
classes that are interpretable,

450
00:25:38,481 --> 00:25:40,130
but then then what do we do with that?
Right?

451
00:25:40,131 --> 00:25:41,810
We can just optimize within that class.

452
00:25:42,330 --> 00:25:43,163
Right?

453
00:25:43,310 --> 00:25:46,920
Um, right. So this is what I just
said. So we agree that this, you know,

454
00:25:47,030 --> 00:25:50,490
a jam is an interpretable model
then than now the problem is done.

455
00:25:50,520 --> 00:25:52,380
The interpretability problem
is sometimes it's done,

456
00:25:52,381 --> 00:25:55,020
all we have to do is optimized yams.
Um,

457
00:25:55,500 --> 00:25:59,520
but we want to make sure that there's
some evidence for these different forms of

458
00:25:59,521 --> 00:26:01,170
models actually being interpretable.

459
00:26:04,060 --> 00:26:08,740
Another way of doing this is saying that
interpretability has to be evaluated in

460
00:26:08,741 --> 00:26:11,860
the context of an event, uh,
in terms of a real application.

461
00:26:12,190 --> 00:26:15,970
So this goes back to my story earlier
that maybe you had this application in

462
00:26:15,971 --> 00:26:20,971
mind where you were trying to separate
those news group posts or understand

463
00:26:21,641 --> 00:26:24,550
different tumor types, or in my case, um,

464
00:26:24,580 --> 00:26:29,020
predict meds for different patients.
And if you're trying to do this task,

465
00:26:29,021 --> 00:26:33,550
you have a clear goal that you want to
avoid confounds and interpretability has

466
00:26:33,551 --> 00:26:37,210
succeeded if it allows you to do the
higher level task. Right. There was,

467
00:26:37,211 --> 00:26:41,510
there was some task in service of which
you needed the interpretability and the,

468
00:26:41,511 --> 00:26:44,400
the HCI, the, the human factors.

469
00:26:44,401 --> 00:26:48,750
Communities have definitely embraced
this sort of view. You know, the, the,

470
00:26:48,960 --> 00:26:53,960
the airplane controls are good if it
allows the pilot to fly the plane safely,

471
00:26:54,240 --> 00:26:56,160
right?
That's your ultimate criterion.

472
00:26:57,360 --> 00:27:00,990
So this sort of criterion is useful
because it's, it's a real evaluation.

473
00:27:00,991 --> 00:27:01,711
Our real problem,

474
00:27:01,711 --> 00:27:05,490
like no one's trying to like make up
something and say clearly this form of

475
00:27:05,491 --> 00:27:10,300
model is interpretable. Um, you have
hard evidence as to whether or not the,

476
00:27:10,301 --> 00:27:14,970
the model is actually interpretable.
Um, but it may be costly,

477
00:27:15,350 --> 00:27:15,870
um,

478
00:27:15,870 --> 00:27:18,900
to perform those evaluations because you
have to do it in the context of a real

479
00:27:18,901 --> 00:27:19,734
application.

480
00:27:20,130 --> 00:27:24,480
And you might be curious about whether
or not to generalize is, for example,

481
00:27:24,481 --> 00:27:29,040
the human factors community in aerospace
has spend a ton of effort trying to

482
00:27:29,041 --> 00:27:32,340
understand how flight controls
should be set up, right?

483
00:27:32,341 --> 00:27:34,860
There's a lot of really
great science behind that.

484
00:27:35,430 --> 00:27:38,670
But now if I'm trying to build
some sort of other user interface,

485
00:27:38,700 --> 00:27:42,810
what can I learn about the
airline flight controls?

486
00:27:43,020 --> 00:27:46,590
Save for a, a self drive or
semi autonomous car. Right?

487
00:27:46,591 --> 00:27:51,150
It's not necessarily clear exactly
how the transfer happens. Right.

488
00:27:51,151 --> 00:27:54,690
And that's the part that maybe we want
to make a little bit more explicit.

489
00:27:55,750 --> 00:27:56,583
Yeah.

490
00:27:57,160 --> 00:28:01,720
So the question that we want to get to,
um, is it, are there more general proxies?

491
00:28:02,110 --> 00:28:06,340
Um, and so, uh, things like
sparsity are often brought up.

492
00:28:06,580 --> 00:28:11,160
People often cite this, um, seven
plus or minus two rule that says like,

493
00:28:11,170 --> 00:28:15,990
how much can people hold in
cognitive memory. Um, there,

494
00:28:16,000 --> 00:28:20,020
there may also be I ideas
of monitor unicity that, oh,

495
00:28:20,021 --> 00:28:23,890
I can understand like something that if
I turned the dial, like my accelerator,

496
00:28:23,891 --> 00:28:26,670
if I push on the accelerator,
I know I always go faster and that,

497
00:28:26,671 --> 00:28:28,060
that kind of makes sense to me.

498
00:28:28,330 --> 00:28:31,960
And if I'm going up a hill or there's
a bump in the road, maybe the,

499
00:28:31,970 --> 00:28:34,330
the rate of acceleration
might be slightly different,

500
00:28:34,331 --> 00:28:37,780
but at least I kind of understand it and
he pushed on the accelerator and then

501
00:28:37,810 --> 00:28:41,680
my car, it will move forward or
increase in speed. So are there,

502
00:28:41,681 --> 00:28:44,650
are there fundamental
notions here? Um, and again,

503
00:28:45,370 --> 00:28:48,430
the nice thing about these sort of having
these sort of fundamental notions is

504
00:28:48,431 --> 00:28:51,470
that as computer sciences, we can
optimize these things, right? We,

505
00:28:51,480 --> 00:28:55,000
we love this because now it's
like a mathematical function
that we can work with.

506
00:28:55,500 --> 00:28:56,470
Um,
but again,

507
00:28:56,471 --> 00:28:59,800
the issue is we need to make sure that
it's evidence based that we need to be

508
00:28:59,801 --> 00:29:04,210
able to understand where is
this assumption coming from?

509
00:29:04,211 --> 00:29:07,960
Or is it just something that's reasonable
that looks reasonable to us or is it

510
00:29:07,961 --> 00:29:11,800
something deeper? And this is
important. Um, not only just from a,

511
00:29:11,801 --> 00:29:15,040
like a philosophical,
I care about science perspective,

512
00:29:15,170 --> 00:29:18,820
but I think it's also important for
being able to make comparisons, right?

513
00:29:18,850 --> 00:29:23,410
Like if something is just a little bit
sparser is it actually more useful for

514
00:29:24,010 --> 00:29:24,970
some downstream task?

515
00:29:27,240 --> 00:29:28,073
Okay.

516
00:29:28,180 --> 00:29:32,440
So what I'm going to propose here
is a spectrum for evaluation.

517
00:29:32,980 --> 00:29:35,650
So I'm going to split it into two pieces,
um,

518
00:29:35,680 --> 00:29:39,010
in terms of quantitative and
qualitative forms of evaluation.

519
00:29:39,011 --> 00:29:43,690
And I think the qualitative part do
that. Absolutely. Um, we eventually,

520
00:29:44,020 --> 00:29:47,860
I'll probably want to live in this
space over here because then again,

521
00:29:47,861 --> 00:29:52,330
we can optimize it on a computer,
but before we get to that stage,

522
00:29:52,540 --> 00:29:54,250
we need to think about the other elements.

523
00:29:55,450 --> 00:29:57,940
So there's a level of the
real application, right?

524
00:29:57,941 --> 00:30:00,210
So that's the level that
I talked about that again,

525
00:30:00,211 --> 00:30:04,510
you might be thinking of from a human
factor standpoint or from a HCI standpoint

526
00:30:04,780 --> 00:30:07,840
and you can measure did we actually
improve some patient outcomes?

527
00:30:08,560 --> 00:30:12,490
Or we can just ask scientists,
you know, did you like this?
Right? Was this helpful?

528
00:30:12,491 --> 00:30:16,570
Again, we're measuring something in
the context of a real application. Um,

529
00:30:16,630 --> 00:30:20,850
and then we can think of it one level
more abstractly. We could say, um,

530
00:30:21,190 --> 00:30:25,990
it can people describe what factors
should be changed in order to change the

531
00:30:25,991 --> 00:30:28,600
outcome, right. That seems like
a reasonable sort of question.

532
00:30:28,601 --> 00:30:31,360
If the explanation I've provided
you is a good one, then you might,

533
00:30:31,450 --> 00:30:35,200
you should be able to forward simulate
what might happen with counterfactuals.

534
00:30:35,530 --> 00:30:39,070
And then you may also ask more
qualitative things about the model itself.

535
00:30:39,820 --> 00:30:43,480
And then finally you have the level of
what I'm calling proxies are things that

536
00:30:43,481 --> 00:30:47,440
we can think of in more very abstract
terms like functions. Um, you know,

537
00:30:47,441 --> 00:30:51,610
are the feature sparse, does it
look reasonable? And, uh, my,

538
00:30:51,630 --> 00:30:56,110
my claim is that we need to move in
this direction of real stuff should be

539
00:30:56,111 --> 00:31:00,580
informing the, the functions that
we are using for optimization. Um,

540
00:31:00,640 --> 00:31:01,473
and uh,

541
00:31:01,510 --> 00:31:05,320
keeping in mind that as we go up the
chain there things are going to get more

542
00:31:05,321 --> 00:31:06,154
specific.

543
00:31:07,030 --> 00:31:12,030
So I think that the top layer and the
bottom layer are pretty clear to mo

544
00:31:12,670 --> 00:31:15,250
probably a pretty familiar
comfortable to most people.

545
00:31:15,490 --> 00:31:19,590
You can think about optimizing
for like l one regularly,

546
00:31:19,591 --> 00:31:23,570
like sparsity that's like really
conceptually simple for us. Um,

547
00:31:23,590 --> 00:31:25,240
we can also think of measuring,
you know,

548
00:31:25,241 --> 00:31:27,680
whether patient outcomes improved or not.
And the,

549
00:31:27,681 --> 00:31:31,000
the part that I think is
really understudied and I
think that we should all be

550
00:31:31,001 --> 00:31:35,140
thinking about and investing in is really
understanding the glue between those,

551
00:31:35,141 --> 00:31:38,120
which is the cognitive science layer.
So I'm going to talk about, um,

552
00:31:38,350 --> 00:31:42,910
how we might think about this layer and
some specific hypotheses that I think

553
00:31:42,911 --> 00:31:46,510
are useful for us to, to think
about as a community in the space.

554
00:31:48,610 --> 00:31:53,230
So in terms of a general form for
human evaluation of explanation.

555
00:31:53,350 --> 00:31:57,970
So here we have, um, uh, this
very specific question, right?

556
00:31:57,971 --> 00:32:00,040
Did we improve patient outcomes?

557
00:32:00,041 --> 00:32:04,270
Like what's the equivalent of that
for just quality of explanation?

558
00:32:04,540 --> 00:32:08,020
And in this case we're not interested
in whether the explanation is correct,

559
00:32:08,590 --> 00:32:13,570
right? So keep in mind that my explanation
for like any prediction could be,

560
00:32:13,900 --> 00:32:17,530
um, you know, because because
the sky is blue, right?

561
00:32:17,580 --> 00:32:22,270
And the thing is that explanation does
not provide, is not predictive at all,

562
00:32:22,300 --> 00:32:25,360
but maybe it's easy, but it's still easy
for you to understand, right? Like you,

563
00:32:25,361 --> 00:32:27,580
you understand what I'm saying
when I say, because this, you know,

564
00:32:27,760 --> 00:32:30,860
I predicted this tumor type because
the sky is blue, right? Like you,

565
00:32:31,020 --> 00:32:33,400
you can understand it.
It's very understandable.

566
00:32:33,760 --> 00:32:38,080
And the reason why we're going to focus
on understanding is that the accuracy

567
00:32:38,081 --> 00:32:41,510
part of an explanation is actually
very easy to quantify that.

568
00:32:41,510 --> 00:32:46,460
If I have a classifier that
predicts malignant tumor
every time the sky is blue,

569
00:32:46,610 --> 00:32:49,880
I can check that and see how
good a classifier that is. Right?

570
00:32:49,910 --> 00:32:52,890
That's a quantitative thing that
I can check and therefore we,

571
00:32:52,970 --> 00:32:56,270
it's not a complicated problem in
terms of evaluations, very easy to do.

572
00:32:56,570 --> 00:33:01,400
The hard part is measuring whether the
explanation is good in terms of does it

573
00:33:01,490 --> 00:33:04,370
allow it, does it convey some
sort of understanding, right?

574
00:33:04,371 --> 00:33:06,590
This a human understand what
the person is talking about.

575
00:33:08,180 --> 00:33:12,640
So in terms of a general form for a
human evaluation of explanation, um,

576
00:33:12,860 --> 00:33:17,810
what we're proposing is being able to
evaluate some sort of counterfactual. So

577
00:33:19,310 --> 00:33:22,940
if you ask the person, how do I
need to change the inputs, right?

578
00:33:22,941 --> 00:33:25,950
What if I change the inputs,
um,

579
00:33:26,400 --> 00:33:30,110
to change some predictions. So I have,
maybe I have a specific data point.

580
00:33:30,111 --> 00:33:32,810
I want to change the output,
right?

581
00:33:32,810 --> 00:33:35,900
What inputs do I need to change
to get the output to change?

582
00:33:36,200 --> 00:33:39,530
That might be a very general sort
of question that I could ask.

583
00:33:39,770 --> 00:33:42,140
I could also ask about the
internals of the system.

584
00:33:42,141 --> 00:33:44,210
I could say actually the
input stays the same.

585
00:33:44,660 --> 00:33:49,040
Is there an element of the explanation
that I would need to change in order to

586
00:33:49,041 --> 00:33:53,500
get the system to predict some different
output? And finally I could say, uh,

587
00:33:53,570 --> 00:33:57,440
we could also think about this mismatch
objective thing saying that it actually,

588
00:33:57,441 --> 00:34:01,880
what would I need to change in the
criteria to be able to get a different

589
00:34:01,881 --> 00:34:04,580
output, uh, of interest? Right?

590
00:34:05,270 --> 00:34:07,310
So there's all sorts of
permutations of this,

591
00:34:07,460 --> 00:34:10,430
but this gets to the fundamental
idea that's very common. I mean,

592
00:34:10,431 --> 00:34:14,270
when you're in school, how do they
test you on whether, you know,

593
00:34:14,420 --> 00:34:17,720
explanations work. It's like whether
you can predict things, right?

594
00:34:17,990 --> 00:34:21,170
You do a science experiment and they ask
you now what's going to happen when you

595
00:34:21,171 --> 00:34:24,920
mix chemical a with chemical B? Can you
tell me the formula that came out? Right?

596
00:34:25,280 --> 00:34:29,660
So we're essentially asking humans to
be able to answer similar questions.

597
00:34:29,810 --> 00:34:31,370
But now instead of grading the humans,

598
00:34:31,371 --> 00:34:36,371
we're grading the system because if the
human can answer the question that it

599
00:34:36,411 --> 00:34:39,380
means a system did a good job of
explaining whatever it's trying to do.

600
00:34:40,400 --> 00:34:43,940
So now we can move on to some
specific hypotheses. And again, I'm,

601
00:34:43,990 --> 00:34:47,810
I'm really curious to get people's
feedback and input and I think it's really

602
00:34:47,811 --> 00:34:51,050
important for us as a community to be
thinking about this layer of cognitive

603
00:34:51,051 --> 00:34:54,740
science, which is not really
well thought through. Um, and,

604
00:34:54,840 --> 00:34:59,840
and so one hypothesis is that in Wa in
terms of being able to generalize tasks,

605
00:35:00,021 --> 00:35:04,070
how certain covariates so we can
think about where is the fundamental

606
00:35:04,071 --> 00:35:08,030
incompleteness and the problem. Is it
in the structure of the data, right?

607
00:35:08,031 --> 00:35:11,550
Are there just weird,
um, confounds, um, eh,

608
00:35:11,551 --> 00:35:16,400
that may also be in the inputs as well.
Is it in the reward functions?

609
00:35:16,880 --> 00:35:19,760
Is it in the internal
features and maybe different,

610
00:35:20,150 --> 00:35:22,580
all of those cases might
have different needs, right?

611
00:35:22,581 --> 00:35:25,940
It's important for us to think about the
fact that interpretability might not be

612
00:35:25,941 --> 00:35:30,110
a one, there might not be one thing
that is interpretable in all scenarios.

613
00:35:30,530 --> 00:35:33,890
And the form of the explanation you need
may be different for different settings.

614
00:35:34,450 --> 00:35:37,320
Um, another important
distinction is the scope. Like,

615
00:35:37,321 --> 00:35:41,460
are we trying to understand
the full model? In many
science cases we are, right?

616
00:35:41,461 --> 00:35:45,960
You have a big batch of data and I want
to understand what does this tell me,

617
00:35:45,961 --> 00:35:47,280
this big batch of tea data.

618
00:35:47,281 --> 00:35:51,480
Tell me about disease progression and
different subtypes and autism, right?

619
00:35:51,690 --> 00:35:53,730
I'm not interested in
any particular patient.

620
00:35:53,730 --> 00:35:56,310
I want to understand the overall trends.
So there's that level.

621
00:35:56,311 --> 00:36:00,060
I'm trying to understand explanations
for the whole system. On the other hand,

622
00:36:00,061 --> 00:36:02,490
if I need to make a decision
for a specific patient and say,

623
00:36:02,491 --> 00:36:04,860
do I treat this patient with this drug,

624
00:36:05,520 --> 00:36:08,340
then the form of explanation
I need may be very different.

625
00:36:08,760 --> 00:36:13,260
And in particular the need of how quickly
the explanation must be utilized as

626
00:36:13,390 --> 00:36:15,300
could be quite different in
these different scenarios.

627
00:36:15,301 --> 00:36:19,050
I might be willing to spend a couple
of hours looking at the output of some

628
00:36:19,051 --> 00:36:23,280
global science discovery sort of system
where it's like if there's something

629
00:36:23,281 --> 00:36:25,830
that needs to be made, a decision
that needs to be made at the bedside,

630
00:36:25,831 --> 00:36:28,860
maybe you only have on the order
of minutes, uh, or, or less,

631
00:36:28,920 --> 00:36:30,300
two to properly understand it.

632
00:36:32,040 --> 00:36:35,480
So this seems all very
reasonable. And what I, uh,

633
00:36:35,580 --> 00:36:38,190
again I said this talk is going
to be a lot of philosophy.

634
00:36:38,940 --> 00:36:42,570
I'm encouraging people to maybe think
about that as you're building your systems

635
00:36:42,571 --> 00:36:45,690
and designing your systems and thinking
about what are the natural ways in which

636
00:36:45,691 --> 00:36:49,170
you can bend these systems and think
about the different needs that are

637
00:36:49,171 --> 00:36:50,004
associated with them.

638
00:36:52,800 --> 00:36:57,690
The other hypothesis that I like to float
out there, um, and how people consider,

639
00:36:57,930 --> 00:37:02,550
so a little more abstract
is that explanation actually
has two components of it

640
00:37:02,700 --> 00:37:06,960
that we maybe can think about distinctly
indistinct in, uh, in separately.

641
00:37:07,680 --> 00:37:09,210
So the first component,

642
00:37:10,130 --> 00:37:10,730
mmm,

643
00:37:10,730 --> 00:37:12,590
it's pretty light, uh, is structure.

644
00:37:13,160 --> 00:37:17,510
So if we think about agents'
actions and outcomes, uh,

645
00:37:17,600 --> 00:37:20,880
maybe we have some explanation
that say's that, um,

646
00:37:20,930 --> 00:37:25,430
here a goes to B goes to c, right?
That's the form of my explanation,

647
00:37:25,431 --> 00:37:29,690
my explanation to, and did ground
that into a specific example.

648
00:37:29,900 --> 00:37:33,650
Maybe you give a vasopressor it constricts
blood vessels and increases blood

649
00:37:33,651 --> 00:37:34,484
pressure.

650
00:37:34,730 --> 00:37:38,810
So if I'm trying to increase blood
pressure and the system says give

651
00:37:38,811 --> 00:37:42,230
vasopressors, this might be the
explanation that comes out, right?

652
00:37:42,620 --> 00:37:45,650
And that explanation has a certain
structure which is a to B to c.

653
00:37:46,510 --> 00:37:46,900
Yeah.

654
00:37:46,900 --> 00:37:51,310
And it might have all sorts of different
forms and different structures here,

655
00:37:51,550 --> 00:37:53,980
maybe more or less understandable,
right?

656
00:37:53,981 --> 00:37:57,730
Here's where we might run into issues
of like if too many factors are involved

657
00:37:57,731 --> 00:38:00,310
in this explanation, we
just can't follow it, right?

658
00:38:00,311 --> 00:38:02,470
We can do those counterfactual reasoning.

659
00:38:02,740 --> 00:38:04,960
Like if there was some
other factor going in here,

660
00:38:05,170 --> 00:38:07,420
maybe we wouldn't be able to do the
reasoning about what would happen if

661
00:38:07,421 --> 00:38:09,790
something else changed and
something else changed, et Cetera.

662
00:38:09,820 --> 00:38:10,840
Maybe he's going to get too hard.

663
00:38:11,500 --> 00:38:14,380
And maybe that's a fundamental
quantity about humans, right?

664
00:38:14,381 --> 00:38:18,100
That there are certain types of these
structures that we can easily handle.

665
00:38:18,250 --> 00:38:22,880
And there's other structures
that we can't handle. So if,

666
00:38:22,910 --> 00:38:27,320
if we're in that regime, we can think
abstractly in terms of like a to B to c,

667
00:38:28,220 --> 00:38:29,053
right?

668
00:38:29,310 --> 00:38:33,540
There's another key component and that
other key component is thinking about the

669
00:38:33,541 --> 00:38:34,950
cognitive chunks.
So that what,

670
00:38:35,260 --> 00:38:39,070
when we ground each of these
variables and like a specific concept,

671
00:38:40,060 --> 00:38:42,610
and here it's important to remember.
So there again this,

672
00:38:42,611 --> 00:38:45,520
this offsite is seven
plus or minus two rule.

673
00:38:46,000 --> 00:38:50,320
It's not seven plus or
minus two. Well, some,

674
00:38:50,350 --> 00:38:55,180
well actually seven plus or minus two.
What is the question? So if I give you,

675
00:38:55,400 --> 00:38:58,750
um, the, the acronym Caia,

676
00:38:59,200 --> 00:39:02,010
is that one thing to you or is
that three letters that you know,

677
00:39:02,020 --> 00:39:02,920
you have to remember?

678
00:39:03,280 --> 00:39:06,580
It's probably one thing because
you have a concept for that.

679
00:39:06,581 --> 00:39:07,480
It's the single item.

680
00:39:08,560 --> 00:39:12,610
And so one thing that's really important
in these very large data sets is first

681
00:39:12,611 --> 00:39:15,190
identifying what are the cognitive chunks.

682
00:39:15,730 --> 00:39:19,330
So clinicians might have a word
for a collection of diseases.

683
00:39:19,900 --> 00:39:24,580
We work with billing codes a lot and
billing codes are like insanely specific.

684
00:39:25,120 --> 00:39:29,920
And so you may how 25 or 30 codes
that exact correspond to epilepsy.

685
00:39:30,370 --> 00:39:33,490
And so if the clinician has
a word for epilepsy, so,

686
00:39:33,580 --> 00:39:35,560
so an explanation does not need to,

687
00:39:35,620 --> 00:39:40,620
we don't need a separate a and a B and a
c out to some large fraction for all of

688
00:39:40,931 --> 00:39:44,680
the different forms of epilepsy. We
could collapse it into one thing, right?

689
00:39:45,370 --> 00:39:47,020
Similarly,
um,

690
00:39:47,470 --> 00:39:51,820
we may have situations where certain
relationships might be really well known.

691
00:39:51,850 --> 00:39:52,580
For example,

692
00:39:52,580 --> 00:39:57,280
a vasopressor I'm constricting blood flow
blood vessels that that might be known

693
00:39:57,281 --> 00:40:01,250
science. Right. So, so
the, the question was, um,

694
00:40:01,300 --> 00:40:04,390
if you end up providing a
more succinct explanation, um,

695
00:40:04,391 --> 00:40:07,510
but there were multiple fat that
only pulls out the biggest factors,

696
00:40:07,511 --> 00:40:11,980
what about all the multiple other tiny
factors that that may also be affecting

697
00:40:11,981 --> 00:40:14,980
the explanation and do you have
an ethical reason to expose them?

698
00:40:15,370 --> 00:40:18,250
I think that's very problem dependent.
For example,

699
00:40:18,430 --> 00:40:22,930
if you just want to be able to overall
be able to say like if changing none of

700
00:40:22,931 --> 00:40:26,560
those factors is going to
change the outcome, uh, the,

701
00:40:26,860 --> 00:40:30,880
the final prediction, maybe it's
safe to ignore them. If those final,

702
00:40:30,910 --> 00:40:33,580
if those other factors
can co-vary as a chunk,

703
00:40:33,610 --> 00:40:37,000
then maybe it's important to expose that
to say that actually if some of this

704
00:40:37,001 --> 00:40:40,600
larger cognitive item changed or
this larger correlated item changed,

705
00:40:40,840 --> 00:40:42,870
then actually the output
could be different. Um,

706
00:40:43,060 --> 00:40:46,150
I think that comes back to
this accuracy tradeoff that I,

707
00:40:46,151 --> 00:40:48,340
that I mostly said I'm
not going to worry about,

708
00:40:48,640 --> 00:40:53,470
but it's this question of any form of
explanation is not going to be perfectly

709
00:40:53,471 --> 00:40:54,760
faithful to your model.

710
00:40:54,761 --> 00:40:57,910
Assuming that your model is some
really large complicated system.

711
00:40:58,390 --> 00:41:02,790
And so that's going to have to be another
user defined threshold of like how,

712
00:41:02,820 --> 00:41:07,270
how much accuracy am I willing
to sacrifice. Um, and you,

713
00:41:07,271 --> 00:41:12,271
if you are not willing to sacrifice a
lot of accuracy and your explanations,

714
00:41:13,600 --> 00:41:15,310
um,
you still need your explanations,

715
00:41:15,311 --> 00:41:18,790
maybe they're going to suffer in quality
in the sense that maybe you are going

716
00:41:18,791 --> 00:41:20,530
to have to include more things.

717
00:41:21,190 --> 00:41:26,190
So all of this stuff I see in the setting
where you have a threshold on accuracy

718
00:41:26,590 --> 00:41:29,590
and you say the explanation actually has,
has to be at least this good.

719
00:41:29,950 --> 00:41:33,770
And given that the explanation is
this good, how can we limit, uh,

720
00:41:33,800 --> 00:41:35,780
how can we make it more of higher quality?

721
00:41:36,080 --> 00:41:39,920
So maybe the case that certain instructors
are easier for people to understand.

722
00:41:40,380 --> 00:41:40,880
Um,

723
00:41:40,880 --> 00:41:45,880
and it may be the case that like a can
be grounded in something that actually

724
00:41:46,340 --> 00:41:51,340
consists of a lot of items and
maybe even this link from a to B.

725
00:41:52,460 --> 00:41:55,820
If we are finding that it takes people
a lot of time to traverse things of a

726
00:41:55,821 --> 00:41:59,300
certain length, but maybe it takes
people less time to traverse things,

727
00:41:59,301 --> 00:42:02,150
have a shorter length, uh, or, or sorry,

728
00:42:02,151 --> 00:42:05,450
the of things that they know that maybe
that effectively reduces the length,

729
00:42:05,840 --> 00:42:06,500
right?
So here,

730
00:42:06,500 --> 00:42:09,860
these are hypotheses to keep in mind
and as you're building your systems,

731
00:42:09,861 --> 00:42:11,660
one thing you can think about is like,

732
00:42:11,870 --> 00:42:14,630
since interpretability is
such a human concept, right,

733
00:42:14,631 --> 00:42:18,320
we can't really think about it without
thinking about what is the language of

734
00:42:18,380 --> 00:42:20,590
the human.
I think that's maybe the,

735
00:42:20,591 --> 00:42:24,980
another key idea to keep in mind is
that we need to have like basically a

736
00:42:24,981 --> 00:42:27,920
dictionary of what is the language of
the human to be thinking about this.

737
00:42:29,240 --> 00:42:33,810
All right. Um, so I'm going
to say mostly rapid there. Um,

738
00:42:34,190 --> 00:42:34,670
uh,

739
00:42:34,670 --> 00:42:38,690
I'm the investigating these hypotheses
and these sorts of questions I think are

740
00:42:38,691 --> 00:42:43,610
really important because they're
going to lead us to understand,

741
00:42:43,940 --> 00:42:48,410
um, are there model classes? It seemed
to work well for certain settings.

742
00:42:48,440 --> 00:42:52,340
So that was the first hypothesis was
a task Atherton and covariates, right?

743
00:42:52,610 --> 00:42:53,720
So those are our settings.

744
00:42:54,410 --> 00:42:59,210
And then are there model classes or
regularizes and maybe that has to do with

745
00:42:59,211 --> 00:43:03,590
the forms of the structures that
we consider. Okay. Or not okay, um,

746
00:43:03,800 --> 00:43:05,510
that are best for certain settings.

747
00:43:06,410 --> 00:43:09,410
And these are a set of experiments
that we could do that we should do,

748
00:43:09,500 --> 00:43:14,150
that we should be supporting each
other as a community, um, to do. Um,

749
00:43:14,210 --> 00:43:19,210
because I think that until we really
get a solid sense of like the cognitive

750
00:43:19,791 --> 00:43:24,350
science of explanation as it relates to
really specific machine learning tasks,

751
00:43:25,370 --> 00:43:29,210
we're not going to be able to make
comparisons between different methods very

752
00:43:29,211 --> 00:43:33,020
rigorously. And we also may end up just
optimizing the wrong things, right?

753
00:43:33,021 --> 00:43:36,130
You may end up optimizing
for some form of, uh,

754
00:43:36,560 --> 00:43:41,110
some function and find that it's not
actually the function that you needed to,

755
00:43:41,120 --> 00:43:44,260
to do the task at hand.
Um,

756
00:43:45,020 --> 00:43:48,620
so in terms of the big takeaways that
I want us to, to be thinking about,

757
00:43:48,650 --> 00:43:52,330
and again, leaving plenty
of time for discussion, um,

758
00:43:53,030 --> 00:43:55,940
the first is that when
do we need explanations?

759
00:43:56,450 --> 00:43:59,540
So we need explanations when there's
a fundamental incompleteness and the

760
00:43:59,541 --> 00:44:01,720
problem specification. And that's, and,

761
00:44:01,730 --> 00:44:05,060
and it's important to be honest about
that because there's many cases where that

762
00:44:05,061 --> 00:44:09,800
just doesn't happen, right? Like if
the system is operating independently,

763
00:44:09,860 --> 00:44:14,750
does not need any humans, the problem as
well as specified as well as it can be,

764
00:44:15,530 --> 00:44:19,070
then it's great to just throw
like a super predictive,

765
00:44:19,550 --> 00:44:23,270
deep model at it. And that's
the best you can do. Right?

766
00:44:23,271 --> 00:44:25,840
And it will and that that's
what you should do. Um,

767
00:44:25,841 --> 00:44:29,810
so thinking about when do you
actually need explanation? Um,

768
00:44:29,860 --> 00:44:30,980
and also this point,

769
00:44:31,710 --> 00:44:35,140
there's a spectrum of valid evaluation
going from just like, you know,

770
00:44:35,160 --> 00:44:39,270
what's my right, what's my l
one sparsity score to, uh, uh,

771
00:44:39,300 --> 00:44:43,710
how well does it perform in a
particular embedded task setting? Um,

772
00:44:43,770 --> 00:44:45,270
and to do this properly,

773
00:44:45,271 --> 00:44:48,330
we're going to need more
connections it to cognitive science.

774
00:44:48,600 --> 00:44:53,600
And my claim is that a relatively modest
effort in terms of doing some of these

775
00:44:55,021 --> 00:44:59,400
experiments could have huge
payoffs because maybe we're
optimizing too much for

776
00:44:59,401 --> 00:45:04,230
the wrong things. Where it, you, if
you have papers that say, you know,

777
00:45:04,231 --> 00:45:08,400
I can get something that's five sparse
and somebody else can get something

778
00:45:08,401 --> 00:45:12,480
that's forced bars, does it, is it
actually help? Right? Like, it was worth,

779
00:45:12,481 --> 00:45:14,790
it was that extra effort worth it.
Um,

780
00:45:15,520 --> 00:45:18,930
and maybe there's something completely
different that we should be focusing on

781
00:45:18,990 --> 00:45:22,310
that we're missing out on because we
haven't thought about it. Um, so the,

782
00:45:22,311 --> 00:45:25,860
the main thing that I want to encourage
us all to do is let's be rigorous about

783
00:45:25,861 --> 00:45:29,460
this question of interpretability that
there's ton of research going on in this

784
00:45:29,461 --> 00:45:31,060
area.
Um,

785
00:45:31,230 --> 00:45:35,520
there's not really consensus on how to
evaluate it and how to think about it.

786
00:45:35,790 --> 00:45:37,110
But as we go forward,

787
00:45:37,140 --> 00:45:40,650
we're going to need to be able to make
decisions about which system is better

788
00:45:40,651 --> 00:45:42,170
for a particular task and when,

789
00:45:42,210 --> 00:45:46,080
when have we reached satisficing cause
interpretability I think is definitely

790
00:45:46,081 --> 00:45:50,430
the sort of thing where you reach a
level where people can understand it and

791
00:45:50,431 --> 00:45:51,900
once people can understand it,

792
00:45:51,901 --> 00:45:55,620
then in some instances your task is done
because you've managed to communicate.

793
00:45:56,330 --> 00:45:59,610
It's a cool, so I'll leave it at
that. Um, I'd love to hear questions,

794
00:45:59,611 --> 00:46:00,810
opinions and discussion.

795
00:46:09,920 --> 00:46:11,600
Thank you very much.
That was wonderful talk.

796
00:46:11,601 --> 00:46:15,170
I think the connections with Congress
slants or just very fascinating and raise

797
00:46:15,171 --> 00:46:16,720
lots of questions have one that can point.

798
00:46:16,721 --> 00:46:21,080
Mine was we know that people are
very sensitive to framing effects.

799
00:46:21,110 --> 00:46:23,180
The same information
presented two different ways.

800
00:46:23,400 --> 00:46:26,480
It can be very different in
how understandable they are.

801
00:46:27,290 --> 00:46:30,500
How do we control for that when we're
evaluating different model classes?

802
00:46:31,190 --> 00:46:34,550
And does there need to be a, you know, a
little subfield of this or like, you know,

803
00:46:34,700 --> 00:46:38,120
let's try to find the most best
framing for, you know, these,

804
00:46:38,121 --> 00:46:40,210
these narrow networks or models.
Okay.

805
00:46:40,280 --> 00:46:42,440
I think that's another
really important question.

806
00:46:42,441 --> 00:46:44,570
And so I didn't really get into the,

807
00:46:44,600 --> 00:46:48,950
like how do you set up the system that
provides the explanation and just also

808
00:46:48,951 --> 00:46:52,550
just the more general Ui issue of how,
how you present this.

809
00:46:53,420 --> 00:46:54,061
I envisioned it,

810
00:46:54,061 --> 00:46:58,190
or like early experiments can probably
be done in sufficiently abstract settings

811
00:46:58,191 --> 00:47:01,610
that you can try to control for as
much of these effects as possible.

812
00:47:01,611 --> 00:47:04,940
But certainly in a real situation that's
going to be another access that needs

813
00:47:04,941 --> 00:47:06,320
to be carefully discovered.

814
00:47:09,570 --> 00:47:13,670
So it seems that there is a
inverse relationship between, uh,

815
00:47:13,740 --> 00:47:17,040
the performance of a model and the
interpretability of that model.

816
00:47:17,250 --> 00:47:21,160
So for example, logistic
regression is extremely simple. Uh,

817
00:47:21,210 --> 00:47:24,660
doesn't often perform that
well. It does well, but, um,

818
00:47:25,170 --> 00:47:27,990
but most importantly it's extremely
interpretable because each of the

819
00:47:27,991 --> 00:47:30,730
coefficients in the model,
you can say, okay, this is a,

820
00:47:30,820 --> 00:47:33,910
this is a big positive effect, but
this is a big negative effect. Uh,

821
00:47:34,090 --> 00:47:38,830
whereas more complicated model is deep
model is they tend to be very hard to

822
00:47:38,831 --> 00:47:43,390
interpret. Uh, do you think that
inverse relationship is inevitable?

823
00:47:43,420 --> 00:47:44,370
Is,
is that,

824
00:47:44,371 --> 00:47:48,640
is there a law saying that the better
model performs the more unintelligible it

825
00:47:48,641 --> 00:47:52,630
becomes or you know, is there
some way or you know, around that?

826
00:47:53,250 --> 00:47:56,050
I actually, I don't agree with that. Um,

827
00:47:56,090 --> 00:47:59,730
there's so Cynthia Ruden quotes this
paper, which I'm forgetting, um,

828
00:48:00,020 --> 00:48:03,530
that let's sees it many times there's
many models that perform reasonably well,

829
00:48:03,890 --> 00:48:07,120
um, that, that oftentimes that
you can find multiple models.

830
00:48:07,340 --> 00:48:11,570
And certainly our anecdotal evidence with
like training deep models is that you

831
00:48:11,571 --> 00:48:15,410
can get multiple deep models that
produce similar predictive scores.

832
00:48:15,411 --> 00:48:16,280
And the question is,

833
00:48:16,340 --> 00:48:20,120
can you make some of those
more interpretable than
others by say involving fewer

834
00:48:20,121 --> 00:48:22,040
dimensions?
So I would act.

835
00:48:22,070 --> 00:48:24,800
And I also pushed back on the fact
that like it's something like logistic

836
00:48:24,801 --> 00:48:28,250
regression interpreter wall because if
you've got 100,000 dimensions in your

837
00:48:28,251 --> 00:48:32,960
logistic regression and they are covariate
and all sorts of funny ways that you

838
00:48:32,961 --> 00:48:35,310
know, the top leader things
might mean nothing, right? And,

839
00:48:35,320 --> 00:48:39,740
and so I actually don't agree with
the fact that it's simpler models,

840
00:48:39,770 --> 00:48:42,470
simpler models like linear model,

841
00:48:42,510 --> 00:48:45,830
like a linear model or generalized linear
model are actually more interpretable

842
00:48:46,250 --> 00:48:49,910
than deep models. And once you're in
high dimensions, and one thing that,

843
00:48:49,911 --> 00:48:53,810
one direction that I'm really excited
about in terms of avenues is this notion

844
00:48:53,811 --> 00:48:58,811
of local interpretability is that is for
any particular explanation or instance

845
00:48:59,541 --> 00:49:03,410
input. Can you provide me an explanation
that is succinct? And I think there,

846
00:49:03,411 --> 00:49:07,320
there is hope even with deep
models because you can, um,

847
00:49:07,340 --> 00:49:10,820
because it also with deep models you
can pick out intermediate layers, right?

848
00:49:10,821 --> 00:49:12,440
You can think about the cognitive chunk.

849
00:49:12,830 --> 00:49:16,790
So if you can identify a layer in the
network that largely activates when

850
00:49:16,791 --> 00:49:18,140
there's a cat in the picture,

851
00:49:18,620 --> 00:49:23,120
then you can say that the reason why
I've done this classification, um,

852
00:49:23,150 --> 00:49:26,870
for, for this being like the
wizard of Oz as the, or, you know,

853
00:49:27,240 --> 00:49:30,980
a scene from wizard of Oz is because I
see the lion here and at like I've picked

854
00:49:30,981 --> 00:49:32,510
out the lion in the picture,
right?

855
00:49:33,290 --> 00:49:35,990
Same thing with two things that we
have going for us is that we only,

856
00:49:35,991 --> 00:49:40,370
we may only need to explain one instance
at a time and that we have access to a

857
00:49:40,371 --> 00:49:40,610
lot.

858
00:49:40,610 --> 00:49:43,760
We're kind of assuming that we have
access to a library of cognitive concepts.

859
00:49:44,210 --> 00:49:45,260
And I think so there's,

860
00:49:45,290 --> 00:49:49,490
there's absolutely hope for being able
to explain deep models or finding models.

861
00:49:49,870 --> 00:49:50,420
Um,

862
00:49:50,420 --> 00:49:54,110
it such that these properties are also
true and you don't have to sacrifice on

863
00:49:54,111 --> 00:49:54,860
prediction.

864
00:49:54,860 --> 00:49:59,420
You might also argue that if the model
is getting you tons more predictive

865
00:49:59,421 --> 00:50:01,100
accuracy in a certain situation,

866
00:50:01,101 --> 00:50:04,700
like I can't think of situations where
that would happen where you're not maybe

867
00:50:04,701 --> 00:50:07,410
picking up like a weird confound,
right? Like, maybe you're at,

868
00:50:07,411 --> 00:50:09,410
you're getting a load more accuracy,

869
00:50:09,860 --> 00:50:14,090
but it's because you picked out some weird
correlation that maybe maybe it's not

870
00:50:14,690 --> 00:50:15,523
true.
Yeah.

871
00:50:15,870 --> 00:50:19,160
So it would seem like there's a difference
between experts and non experts in

872
00:50:19,161 --> 00:50:22,220
terms of how explainable something is
where like the person who's flying the

873
00:50:22,221 --> 00:50:25,430
plane every day knows exactly where to
look and what they're looking for and

874
00:50:25,431 --> 00:50:26,990
they want to make the
decision really quickly.

875
00:50:27,410 --> 00:50:31,910
Do you think that if the distinction
expert non-expert is enough or do we need

876
00:50:31,911 --> 00:50:35,990
like gradations are experts in different
fields in terms of understanding how

877
00:50:35,991 --> 00:50:40,220
these proxies would generalize to
other fields or other situations?

878
00:50:40,250 --> 00:50:42,830
I think that's a great question.
And one of the thing,

879
00:50:42,890 --> 00:50:46,070
so this is a hypothesis and empirical
thing that needs to be tested.

880
00:50:46,370 --> 00:50:50,300
And I should say this is all
in discussion with being um,

881
00:50:50,330 --> 00:50:53,690
Jesse Johnson here.
Sam Grossman.

882
00:50:54,050 --> 00:50:57,920
I'm at Harvard where we've been talking
about like experiments and hypotheses.

883
00:50:58,850 --> 00:51:03,680
We believe we are a hypothesis that we
think is testable is that there are still

884
00:51:03,681 --> 00:51:08,681
fundamental aspects of like control
processes that humans can understand in

885
00:51:08,961 --> 00:51:10,080
terms of this uh,

886
00:51:10,190 --> 00:51:15,190
chains of causation or agents and
actors and that part and the difference,

887
00:51:15,680 --> 00:51:18,770
the hypothesis is a difference
between experts and non experts.

888
00:51:19,010 --> 00:51:22,640
My is not necessarily that the expert
can hold more in memory or a whole

889
00:51:22,641 --> 00:51:23,690
different structures and memory,

890
00:51:23,720 --> 00:51:25,340
although certainly people
get trained to do that.

891
00:51:25,341 --> 00:51:26,320
So that's the thing that we could,

892
00:51:26,330 --> 00:51:30,560
we should be absolutely aware of but
we're curious how far you can go with this

893
00:51:30,561 --> 00:51:33,320
hypothesis that maybe experts
just have different chunks.

894
00:51:33,980 --> 00:51:37,790
So we figured out like a
general structures that are
generally explainable and

895
00:51:37,791 --> 00:51:39,950
then we can sub in
chunks that experts have,

896
00:51:40,190 --> 00:51:44,510
cause the doctor might have a name for
something or a way of understanding a

897
00:51:44,511 --> 00:51:48,200
particular device that somebody
else doesn't have access to.

898
00:51:48,530 --> 00:51:51,440
And can we plug that in
and it's an open question.

899
00:51:53,120 --> 00:51:56,300
Squeeze in one last question.
I wonder if there are fundamental,

900
00:51:56,301 --> 00:52:00,530
the fundamental differences in the area
of interpretability between domains that

901
00:52:00,531 --> 00:52:04,370
are deeply studied like healthcare
and domains that are brand new,

902
00:52:04,371 --> 00:52:09,110
like how to maximize clicks
on ads, um, that the,

903
00:52:09,190 --> 00:52:10,023
um,

904
00:52:10,290 --> 00:52:14,480
the existence of a deep spectrum
of human expertise on the one hand,

905
00:52:14,960 --> 00:52:15,793
um,

906
00:52:15,830 --> 00:52:20,180
puts very different pressures on our
machine learning systems in that context.

907
00:52:20,950 --> 00:52:22,280
It just to make sure I understand.

908
00:52:22,281 --> 00:52:25,340
So you're saying that there are some
cases where there's a lot of domain

909
00:52:25,341 --> 00:52:26,900
expertise,
um,

910
00:52:26,990 --> 00:52:30,080
and there's cases where there is less
domain expertise in terms of what the

911
00:52:30,081 --> 00:52:31,190
explanation would provide?

912
00:52:31,460 --> 00:52:35,390
Yeah, absolutely. That you buy,
you might observe that a lot of the

913
00:52:36,980 --> 00:52:41,690
most interesting applications of machine
learning recently have been in areas

914
00:52:41,691 --> 00:52:43,760
that are not that deeply studied.
Whereas we've,

915
00:52:44,030 --> 00:52:47,790
we've struggled to have equal
impact in healthcare, uh,

916
00:52:47,900 --> 00:52:52,340
maybe because of the amount of, of,
of learning that's already happened.

917
00:52:53,820 --> 00:52:56,070
So I, I think to, so there's the,

918
00:52:56,550 --> 00:52:59,340
there's potentially the claim that
maybe there's a lot of learning that has

919
00:52:59,341 --> 00:53:03,270
already happened in healthcare,
so maybe there's less to advance on.

920
00:53:03,990 --> 00:53:07,650
I think it actually does come down
to this incompleteness and problems

921
00:53:07,651 --> 00:53:10,320
specification that makes it
a lot of these real tasks,

922
00:53:10,321 --> 00:53:15,321
harder real world task harder than tasks
that we have created like somehow human

923
00:53:16,111 --> 00:53:16,770
created.

924
00:53:16,770 --> 00:53:21,770
So like the click question is something
that's very closed in the form of we

925
00:53:23,370 --> 00:53:28,170
have a system we can measure whether
something changes, click rates are not.

926
00:53:28,650 --> 00:53:32,640
And so there's a way in which the problems
and so to the extent that you defined

927
00:53:32,641 --> 00:53:36,630
the problem as we want to
increase certain rates, right?

928
00:53:36,631 --> 00:53:39,330
It's very measurable and it can
be done in a closed loop fashion.

929
00:53:39,600 --> 00:53:43,680
And I'm not sure if it
requires interpretability if
you're asking the question

930
00:53:43,681 --> 00:53:44,161
more broadly,

931
00:53:44,161 --> 00:53:48,210
if like uptake of machine learning or
like value provided by machine learning in

932
00:53:48,211 --> 00:53:49,044
certain settings.

933
00:53:49,560 --> 00:53:53,700
And I think it is entirely fair to say
that the value that machine learning is

934
00:53:53,701 --> 00:53:56,920
added incentive settings where um,
the,

935
00:53:56,950 --> 00:53:58,860
the problem is not fully specified.

936
00:53:58,861 --> 00:54:00,780
And where there is this
incompleteness where you,

937
00:54:00,781 --> 00:54:04,980
you're measuring things in the
EHR and the EHR, super messy, um,

938
00:54:05,230 --> 00:54:07,860
has not necessarily been there because
we haven't really figured out how to

939
00:54:07,861 --> 00:54:09,060
communicate and measure

940
00:54:09,290 --> 00:54:12,380
in those systems.
Thank you.

