1
00:00:09,450 --> 00:00:09,870
So again,

2
00:00:09,870 --> 00:00:13,140
thank you for taking the risk to come
to a talk where there was a mysterious

3
00:00:13,141 --> 00:00:17,900
word in the title. I will explain
it over the course of this talk. Um,

4
00:00:17,910 --> 00:00:21,930
so the first place I want to
start is with this person. Um,

5
00:00:21,960 --> 00:00:23,940
I've already given a clue,
but does anyone know who this is?

6
00:00:25,190 --> 00:00:25,800
Yeah,

7
00:00:25,800 --> 00:00:30,790
it is. It is eight 11 list. Um, and the
reason I bring her up on screen, um,

8
00:00:30,840 --> 00:00:35,050
is uh, not because of her stature. Um,

9
00:00:35,080 --> 00:00:38,100
and not because she was the world's
first computer programmer working with

10
00:00:38,101 --> 00:00:39,260
Charles Babbage.
Um,

11
00:00:39,420 --> 00:00:44,420
but because it was in her
memoirs of working with Babbage,

12
00:00:45,110 --> 00:00:49,720
um, that she included this quote. Um,

13
00:00:49,950 --> 00:00:53,800
Babbage of course, did both
the differential engine
and the analytical engine.

14
00:00:53,830 --> 00:00:55,990
And in her memoir she put the sentence,

15
00:00:55,991 --> 00:01:00,220
the analytical engine has no
pretentions to originate anything.

16
00:01:00,221 --> 00:01:03,460
It can do whatever we know
how to order it to do.

17
00:01:04,480 --> 00:01:08,100
I'm Alan turning in his
understated,

18
00:01:08,130 --> 00:01:12,850
a seminal paper computing machinery
and intelligence called this lady, uh,

19
00:01:13,050 --> 00:01:14,670
level laces.
Objection.

20
00:01:16,270 --> 00:01:17,020
Okay.

21
00:01:17,020 --> 00:01:20,250
This caught my eye when I read
this, partially because, um,

22
00:01:20,500 --> 00:01:25,210
she was objecting against something
that nobody had objected to yet.

23
00:01:25,450 --> 00:01:28,330
They didn't have time.
The computer didn't exist.

24
00:01:28,810 --> 00:01:33,400
So why was she sort of forestalling an
argument here? Why was she saying no,

25
00:01:33,401 --> 00:01:36,590
it's cool. Computers
can't do anything. Um,

26
00:01:36,970 --> 00:01:39,130
I looked further back,

27
00:01:39,160 --> 00:01:44,160
obviously then Lovelace and Babbage and
found the answers probably in mythology,

28
00:01:46,000 --> 00:01:50,080
right all the way back. And I know this
is a screen grab from the Disney version.

29
00:01:50,420 --> 00:01:53,950
Um, but the original good to version of
the sorcerer's apprentice was a piece of

30
00:01:53,951 --> 00:01:57,880
mythology that said, hey, look,
tech can go way out of control.

31
00:01:58,150 --> 00:01:59,770
And in fact,
in that poem,

32
00:01:59,920 --> 00:02:03,010
it was only the presence of the sourcer
that saved the world from destruction,

33
00:02:03,011 --> 00:02:03,844
from tiny brooms.

34
00:02:05,090 --> 00:02:05,923
Sorry.

35
00:02:06,880 --> 00:02:10,030
Another myth comes from
the Golem of Prague.

36
00:02:10,860 --> 00:02:14,110
If you're not familiar with this,
this was a, a creature made of clay.

37
00:02:14,111 --> 00:02:17,710
It had a pretty cool little thing where
you put a gif on its forehead to bring

38
00:02:17,711 --> 00:02:21,700
it to life. Um, but then after that,
this one doesn't have a mouth. Um,

39
00:02:21,730 --> 00:02:24,880
but anything that you wrote as an
instruction and put it in its mouth,

40
00:02:25,030 --> 00:02:29,230
it would just follow mindlessly
as best as it could. Right?

41
00:02:29,231 --> 00:02:31,330
And it was a story of
sort of Hubris and terror.

42
00:02:31,510 --> 00:02:35,200
Watch the machine go crazy with this
instruction. I'm going, of course,

43
00:02:35,201 --> 00:02:40,201
we still tell terrifying tales of machines
that go awry for their instructions

44
00:02:40,331 --> 00:02:45,130
and, and rec human damage on
their, uh, on their path. Right?

45
00:02:45,131 --> 00:02:48,540
That's a how from 2001,
a space odyssey.

46
00:02:48,550 --> 00:02:51,790
So there are lots of
places across myth in past,

47
00:02:51,791 --> 00:02:56,020
before Lovelace's time and certainly
after where we're telling ourselves this,

48
00:02:56,290 --> 00:03:00,820
these tales of the technology is
terrifying and should not take initiative.

49
00:03:01,090 --> 00:03:03,010
Um,
and I'm going to

50
00:03:04,510 --> 00:03:08,980
take that question as we move forward,
um, partially as a narrative construct,

51
00:03:08,981 --> 00:03:13,810
but it's also the question that drove
some patterns that I had identified

52
00:03:13,930 --> 00:03:16,860
probably about an starting
about 10 years ago, but, uh,

53
00:03:16,930 --> 00:03:21,820
really started thinking about it pretty
hard about five years ago and that was a

54
00:03:21,821 --> 00:03:22,900
pattern that I saw in the world.

55
00:03:25,830 --> 00:03:28,530
So does anyone know what
the object on the left is?

56
00:03:30,280 --> 00:03:34,710
It's pretty easy to gimme right?
This is to encourage you to talk. Um,

57
00:03:34,890 --> 00:03:37,170
it's a DSLR camera. In fact,
that's the model that I have.

58
00:03:37,350 --> 00:03:41,460
Does anyone recognize the object on the
right? It looks like a tile. In fact,

59
00:03:41,461 --> 00:03:44,640
it's creepily like a tile,
but it's not a tile. Yes,

60
00:03:44,641 --> 00:03:48,240
that is the get narrative camera. Um,
and if you're not familiar with it,

61
00:03:48,241 --> 00:03:49,350
the way it works is you,

62
00:03:49,590 --> 00:03:53,010
you leave it plugged up and plugged in
overnight so it has a battery charge and

63
00:03:53,011 --> 00:03:57,660
then you unplug it.
As long as that Lens sees light,

64
00:03:57,780 --> 00:04:00,840
it takes a picture every 30 seconds
of whatever's in front of it.

65
00:04:01,800 --> 00:04:04,380
Then you come home at the end of the day,
the camera's taken about three,

66
00:04:04,381 --> 00:04:08,190
5,000 photos. You plug it in
and it does for cool things.

67
00:04:08,640 --> 00:04:11,790
The first is that automatically
uploads all those photos to a server.

68
00:04:12,080 --> 00:04:15,930
The second thing is it uses some smart
algorithms to divide your day into scenes.

69
00:04:16,350 --> 00:04:18,630
Here's when you were at breakfast,
here's when you were at lunch.

70
00:04:19,020 --> 00:04:20,370
Here's when you were
listening to Chris Talk.

71
00:04:21,660 --> 00:04:26,040
Third thing it does is it uses some
algorithms to detect what the best photo

72
00:04:26,041 --> 00:04:30,390
from that set is based on
things like level of clarity,

73
00:04:30,540 --> 00:04:31,650
contrast,
that sort of thing.

74
00:04:32,070 --> 00:04:35,700
And the last thing it does is it uploads
those photos to a phone for you to say,

75
00:04:35,701 --> 00:04:38,850
what do you want me to do with these? And
the APP is pretty clever. You can say,

76
00:04:38,880 --> 00:04:42,510
Oh yes, I love that one, post that
one to social media. Or you can say,

77
00:04:42,540 --> 00:04:44,160
I disagree with you on this one.

78
00:04:44,370 --> 00:04:47,490
Show me all of the images and let
you rifle through. And by the way,

79
00:04:47,491 --> 00:04:50,130
learn for next time what
I likened my images.

80
00:04:50,850 --> 00:04:53,460
You can tag folks and you can
delete it. You can say, wow,

81
00:04:53,940 --> 00:04:56,670
don't share any of these things.
Um,

82
00:04:57,900 --> 00:05:00,780
and so that's what the
[inaudible] narrative camera does.

83
00:05:00,781 --> 00:05:03,930
It's part of a category of cameras
called the life blogging camera.

84
00:05:04,830 --> 00:05:09,090
And for my money, what makes us really
interesting is that they're both cameras,

85
00:05:09,810 --> 00:05:13,020
but the one on the right is a
camera without a photographer.

86
00:05:16,590 --> 00:05:18,390
Does anyone know what
the thing on the left is?

87
00:05:19,620 --> 00:05:20,453
Thanks

88
00:05:21,570 --> 00:05:24,240
Dyson brand vacuum cleaner.
And the thing in the right,

89
00:05:24,241 --> 00:05:27,090
I probably don't have to
explain it's a Roomba. Right?

90
00:05:27,091 --> 00:05:29,820
And similarly to the get narrative camera,

91
00:05:30,300 --> 00:05:34,020
we have two objects that ostensibly
do the same thing for their users,

92
00:05:34,230 --> 00:05:38,340
but the one in the left requires you
to grab it with your hand. In fact,

93
00:05:38,341 --> 00:05:41,340
that's the only purpose that handles
serves. Right. And to use it,

94
00:05:41,341 --> 00:05:45,360
you step on a switch to unlock it and
you step on a second switch in order to

95
00:05:45,361 --> 00:05:49,530
turn it on and then you push the
vacuum around the house. Meanwhile,

96
00:05:49,531 --> 00:05:53,730
this thing does the same thing,
but there's no handle,

97
00:05:54,240 --> 00:05:57,560
there's no real step free
to step on to unlock it. Um,

98
00:05:57,561 --> 00:05:59,390
there is something where
you can turn it on manually.

99
00:05:59,391 --> 00:06:00,410
That's what that big button.
Yes.

100
00:06:01,400 --> 00:06:06,400
But the important thing here is that
it's a vacuum without a vacuum or pardon?

101
00:06:06,500 --> 00:06:09,140
That would be clever. Or if I was
working in another language than English,

102
00:06:10,070 --> 00:06:12,710
but right. There's no human
whose job it is to vacuum there.

103
00:06:13,040 --> 00:06:17,330
It's not quite automatic because if
I spill cocoa on my, on my floor,

104
00:06:17,331 --> 00:06:18,290
I can grab that thing,

105
00:06:18,530 --> 00:06:22,220
put it nearby and say clean here and
it'll focus on that spot for a little bit.

106
00:06:23,420 --> 00:06:27,350
Room has recently been blasted
this week in the news, um,

107
00:06:27,650 --> 00:06:31,520
for some of their privacy violations.
But let's bypass that for now.

108
00:06:31,521 --> 00:06:35,570
I just want to talk about the functions
and the thing on the left is what,

109
00:06:37,350 --> 00:06:41,280
it's a car, right? And the thing
on the right is that your car,

110
00:06:41,290 --> 00:06:43,720
at least the current model
that I'm aware of, um,

111
00:06:43,740 --> 00:06:47,820
and it's a similar in that in that same
pattern it's a car but without a driver,

112
00:06:48,630 --> 00:06:51,180
right? And when you look at
all three of those things,

113
00:06:51,181 --> 00:06:54,870
they are emblematic of a larger
pattern that I was seeing in my life.

114
00:06:55,050 --> 00:06:59,550
I had a, um, automatic cat
feeder, so when I traveled,

115
00:06:59,551 --> 00:07:02,920
my cat wouldn't starve or I didn't have
to ask a friend to come over. Um, and,

116
00:07:02,930 --> 00:07:05,760
and at the time that I began thinking
of these ideas really deeply,

117
00:07:05,761 --> 00:07:07,380
I was working in a Robo investor.

118
00:07:07,950 --> 00:07:10,590
And so this was something that you
would say how much money you had,

119
00:07:11,250 --> 00:07:12,630
how much money you would give each month,

120
00:07:12,660 --> 00:07:15,150
and what your financial goals
were from that point forward.

121
00:07:15,151 --> 00:07:16,530
The investor would manage your portfolio.

122
00:07:18,650 --> 00:07:20,840
And the thing when thinking
about these patterns,

123
00:07:20,841 --> 00:07:24,320
what they all sort of shared was that they
all did things on their users' behalf,

124
00:07:24,680 --> 00:07:28,940
but they weren't quite automated.
But to get narrowed a camera,

125
00:07:28,941 --> 00:07:33,650
you can actually tap it in order to take
a photo or take a video with the Roomba,

126
00:07:33,651 --> 00:07:36,230
you can pick it up and force
it to vacuum somewhere.

127
00:07:36,231 --> 00:07:40,430
That wasn't in its current plan,
I'm presuming I've never ridden in one,

128
00:07:40,431 --> 00:07:42,290
but that you can stop
the Google car and say,

129
00:07:42,350 --> 00:07:47,350
I've got a p or let's take a food break
or I want to go see what that is thing

130
00:07:47,391 --> 00:07:49,820
over there.
So they're not quite automated.

131
00:07:51,050 --> 00:07:55,550
And when I tried to dig deep and to sort
of the epistomological definition, um,

132
00:07:55,580 --> 00:07:59,960
it seemed to me that the major difference
was that you grant these things agency

133
00:07:59,961 --> 00:08:03,140
to act on your behalf.
So for my money,

134
00:08:03,410 --> 00:08:07,340
this pattern was rightly
called agent tive technology.

135
00:08:08,450 --> 00:08:12,500
I didn't make up that word, but I rescued
that word from obscurity. I think, uh,

136
00:08:12,560 --> 00:08:15,230
before I published it, the only place
it was used, it was in linguistics,

137
00:08:16,190 --> 00:08:17,240
but that's what I'm calling it.

138
00:08:17,240 --> 00:08:20,900
This weird space in between
automation and assistance.

139
00:08:20,901 --> 00:08:22,490
And we'll talk a little
bit more about that later.

140
00:08:23,060 --> 00:08:24,770
But that's the pattern I had identified.

141
00:08:26,450 --> 00:08:31,250
Now these are all first world problems,
admittedly,

142
00:08:31,280 --> 00:08:34,700
but they're super easy to understand
and that's why I go to these as examples

143
00:08:34,701 --> 00:08:37,010
first and quite commonly in the book.

144
00:08:38,000 --> 00:08:40,310
But I really wanted to push
the idea and say, okay,

145
00:08:40,311 --> 00:08:45,050
are there some third world problems or
other world problems that this technology

146
00:08:45,051 --> 00:08:48,170
can be applied to review
them kind of quickly.

147
00:08:49,010 --> 00:08:50,960
And the upper left hand
corner is ShotSpotter.

148
00:08:51,350 --> 00:08:52,670
If you're not familiar with this tech,

149
00:08:53,000 --> 00:08:57,300
what this service does is it will work
with a precinct and the United States in

150
00:08:57,301 --> 00:09:01,530
order to sprinkle microphones all over
a neighborhood. There's microphones, uh,

151
00:09:01,590 --> 00:09:02,610
aren't particularly great,

152
00:09:02,611 --> 00:09:06,150
but they do have fantastically accurate
clocks and they're connected to the

153
00:09:06,151 --> 00:09:08,160
network and they're trained
to listen for gunfire.

154
00:09:09,120 --> 00:09:10,760
The men of Nehemiah hear gunfire.

155
00:09:10,770 --> 00:09:14,070
They report the exact moment they heard
it back to a server and of course you

156
00:09:14,071 --> 00:09:18,480
can triangulate it and the cool thing is
the accuracy of that is down to a meter

157
00:09:19,910 --> 00:09:23,690
and doing an interview with
ShotSpotter for this book. Uh,

158
00:09:23,760 --> 00:09:28,760
they love to tell a tale of one pair
of officers who were able to arrive in

159
00:09:29,281 --> 00:09:32,550
winter
so quick.

160
00:09:32,580 --> 00:09:35,940
They were able to respond so
quickly to the gunshots that uh,

161
00:09:35,941 --> 00:09:38,040
even though there was snow in the ground
and they were worried they wouldn't be

162
00:09:38,041 --> 00:09:39,060
able to find the shell,

163
00:09:39,210 --> 00:09:43,560
they did find it by the smoking
hole that it left in the snow.

164
00:09:44,010 --> 00:09:47,010
Pretty cool. Right in the upper
right hand corner is Volvo.

165
00:09:47,370 --> 00:09:51,420
It can really with their self
breaking trucks in the grill,

166
00:09:51,421 --> 00:09:56,421
they have a series of sensors that not
only track motion but actually a extended

167
00:09:56,911 --> 00:09:57,744
that motion.

168
00:09:57,750 --> 00:10:02,750
And if the vector of any object in motion
intersects with the car and the driver

169
00:10:02,810 --> 00:10:04,230
is not breaking it,
they will break it.

170
00:10:04,290 --> 00:10:07,950
It's going to save a ton of lives even
before we have self driving trucks.

171
00:10:08,460 --> 00:10:09,600
That's a human augmentation

172
00:10:11,100 --> 00:10:14,760
in the lower left is a swarm
robot called Prospero, uh,

173
00:10:14,830 --> 00:10:19,260
designed and prototyped in Australia.
And with these little robots,

174
00:10:19,261 --> 00:10:22,650
you can actually set some seed in here,
show it a map,

175
00:10:22,860 --> 00:10:26,600
tell it what kind of seeds they are and
the swarm can go out and plant crops, um,

176
00:10:26,820 --> 00:10:29,370
accurately and quickly
in a matter of moments.

177
00:10:30,510 --> 00:10:33,450
Some major issues with what do we do
with the workers who used to do that sort

178
00:10:33,451 --> 00:10:36,480
of thing. But all of this technology
has that question underneath it.

179
00:10:37,390 --> 00:10:41,130
The nice thing about Prospero is that
it is not only faster but also safer.

180
00:10:41,220 --> 00:10:43,590
So dangerous terrain.
The robots, you know,

181
00:10:44,400 --> 00:10:47,310
they can handle it fine and we don't mind
if a robot breaks it's little league.

182
00:10:48,990 --> 00:10:51,780
And the lower right hand corner
is a service called scare crow.

183
00:10:52,290 --> 00:10:57,290
This is a drone that is trained to hover
high above endangered herds and watch

184
00:10:57,931 --> 00:11:01,140
for humans that are approaching.
And if it doesn't know those humans,

185
00:11:01,500 --> 00:11:03,900
it will drop down in
between the herd and humans.

186
00:11:04,140 --> 00:11:07,230
And the wasp like buzzing will scare
the herd in the opposite direction.

187
00:11:09,900 --> 00:11:11,640
Now I made that last one up.
It's a lie,

188
00:11:12,810 --> 00:11:16,260
but the reason I made it up is I thought
of a problem in the world that bugged

189
00:11:16,261 --> 00:11:20,970
me, animal rights. Um, and I tried to
apply this kind of thinking, this pattern,

190
00:11:20,971 --> 00:11:24,810
this template to the problem and I was
able to come up with something fairly

191
00:11:24,811 --> 00:11:27,900
quickly and fairly compelling.
In fact,

192
00:11:27,901 --> 00:11:32,250
I was giving this talk in Delft in the
Netherlands prelate fairly recently and

193
00:11:32,251 --> 00:11:35,300
one of the audience members raised his
hand and say, Oh yeah, we're a Dutch.

194
00:11:35,310 --> 00:11:38,220
People are already building that.
So it's a viable idea,

195
00:11:38,221 --> 00:11:40,740
at least for the Dutch
to seem to think so. Um,

196
00:11:40,770 --> 00:11:42,870
but all of these are
sort of to illustrate,

197
00:11:42,900 --> 00:11:44,460
even though they're a
little more complicated,

198
00:11:44,461 --> 00:11:47,550
a little more nuanced that
the pattern of agency,

199
00:11:47,630 --> 00:11:51,150
it doesn't just apply to first world
problems even though those are easier to

200
00:11:51,151 --> 00:11:54,430
understand.
So one assert that yes,

201
00:11:54,490 --> 00:11:59,290
I think it's as big as we can think.
Put that part down.

202
00:11:59,470 --> 00:12:00,520
Pattern established.

203
00:12:00,910 --> 00:12:05,110
I'm going to talk to you
about five reasons why you
think this pattern is nifty.

204
00:12:06,970 --> 00:12:08,980
The first is that it's new,

205
00:12:09,640 --> 00:12:11,980
but you can hear in my voice and you
should be able to see on the slide that

206
00:12:11,981 --> 00:12:14,170
there are air quotes around the word new.

207
00:12:14,590 --> 00:12:16,840
The reason why is because
that image in the background,

208
00:12:16,841 --> 00:12:18,190
does anyone recognize what that is?

209
00:12:20,500 --> 00:12:21,333
Yeah.

210
00:12:21,680 --> 00:12:25,640
Yes. A particular machine
within the cockpit autopilot.

211
00:12:26,930 --> 00:12:30,280
Anyone want to take a stab at when the
first autopilot was demonstrated to

212
00:12:30,281 --> 00:12:34,730
Chicago world's fair,
1914 103 years ago.

213
00:12:35,360 --> 00:12:37,910
Now, of course it was
electromechanical at the time.

214
00:12:38,300 --> 00:12:41,900
It took a lot of engineering effort, a
lot of money in order to make it. Um,

215
00:12:41,960 --> 00:12:44,090
but that's a piece of
agent tiff technology.

216
00:12:44,140 --> 00:12:48,320
You granted agency to fly the plane and
it doesn't just do it automatically.

217
00:12:48,321 --> 00:12:52,270
We still have pilots on those planes,
even with our modern autopilot's.

218
00:12:53,020 --> 00:12:57,260
But let's put that down between that
and the thermostat that you'll see in

219
00:12:57,261 --> 00:13:00,200
chapter one of the book.
Um, it's kind of new.

220
00:13:00,230 --> 00:13:02,630
There are lots of precedents
in the world ahead of this,

221
00:13:03,020 --> 00:13:07,130
but the reason I say that it's new that
public API Apis for neuro artificial

222
00:13:07,131 --> 00:13:10,700
intelligence have become available
within the past five years.

223
00:13:11,270 --> 00:13:15,810
IBM's yours open Ai's right?

224
00:13:15,830 --> 00:13:16,663
The

225
00:13:18,420 --> 00:13:23,420
public understanding and appreciation
of these services are on the rise.

226
00:13:24,001 --> 00:13:26,340
So we have a marketplace that
will sort of accept them.

227
00:13:26,341 --> 00:13:27,930
We don't have to train
them on this sort of thing.

228
00:13:28,500 --> 00:13:31,950
We have a lot of centers that are an
actuators that are available to us as

229
00:13:31,951 --> 00:13:34,530
designers and makers in order
to make these things happen.

230
00:13:34,950 --> 00:13:38,850
I have 32 examples in the book.
Here's a question mark there.

231
00:13:38,851 --> 00:13:42,150
I don't remember the exact
number. Um, but all of them,

232
00:13:42,151 --> 00:13:46,740
except for the autopilot or within
the past five years, so it is new.

233
00:13:47,130 --> 00:13:51,240
I believe that this is a way of
thinking that's ahead of a curve.

234
00:13:52,680 --> 00:13:57,090
So new, pretty cool. Second
reason is that it is different.

235
00:13:57,720 --> 00:14:00,330
If you've ever studied
interaction design like I did,

236
00:14:01,170 --> 00:14:06,170
we talk about things like affordances
and mapping and all those things and the

237
00:14:06,181 --> 00:14:08,820
canonical example for that as a hammer.

238
00:14:09,030 --> 00:14:12,600
How does a human know how to
use that hammer? Well, they
look at it and they say,

239
00:14:12,660 --> 00:14:17,280
oh well this looks like it fits a hand
and as long as I'm going to grab it at

240
00:14:17,281 --> 00:14:21,480
the end, that looks hard. So it should be
able to hit something and drive a nail.

241
00:14:22,410 --> 00:14:26,040
Those affordances, those mappings,
those systems states, uh,

242
00:14:26,050 --> 00:14:29,340
are all belonged to
something like a hammer.

243
00:14:30,150 --> 00:14:33,750
But that doesn't apply when you're
talking about an agent of model. Right?

244
00:14:33,780 --> 00:14:36,780
You don't need an affordance for the
Roomba because I don't have to grab the

245
00:14:36,781 --> 00:14:37,614
handle.

246
00:14:38,090 --> 00:14:38,710
Okay.

247
00:14:38,710 --> 00:14:41,830
I don't need mapping when it comes to the,

248
00:14:42,190 --> 00:14:44,500
I'm not going to say Google car cause
course you need mapping for a car.

249
00:14:45,220 --> 00:14:48,730
I don't need mapping when it
comes to my cat feeder. Right.

250
00:14:48,790 --> 00:14:52,160
I just need good controls to set the
thing up and some kind of feedback

251
00:14:52,161 --> 00:14:57,080
mechanisms to let me know that yes,
my cat is being fed. So for my money,

252
00:14:58,190 --> 00:15:01,790
if the hammer is not the right model
for an agent of technology, what is,

253
00:15:02,210 --> 00:15:07,160
and I think you can go turn to a
butler or a valid as a better model.

254
00:15:07,850 --> 00:15:12,320
You would tell this person what your
goals were and what your preferences were

255
00:15:12,321 --> 00:15:15,740
and from that point forward it would
manage as best as it could to those goals

256
00:15:15,741 --> 00:15:19,490
and preferences. It would come to
you and there was a problem, hey,

257
00:15:19,491 --> 00:15:23,990
the larder is empty or in case of a of
a valid, I don't have any clean clothes.

258
00:15:23,990 --> 00:15:27,530
Something's gone wrong here and you
would help it and you can even tweak it

259
00:15:27,531 --> 00:15:28,364
saying,
wow,

260
00:15:28,570 --> 00:15:31,570
I appreciate your bringing me that tie
but I never want to see it again. It's,

261
00:15:31,590 --> 00:15:35,880
it's hideous and ugly.
To illustrate this notion that the,

262
00:15:36,300 --> 00:15:40,100
the, the model must be different. Um,

263
00:15:40,101 --> 00:15:42,320
I built a model of interaction design.

264
00:15:42,440 --> 00:15:45,260
If you studied this should
be pretty familiar, right?

265
00:15:45,380 --> 00:15:49,460
Humans sees the state of the system. They
think that's not quite the right state.

266
00:15:49,461 --> 00:15:50,570
What can I do next?

267
00:15:50,780 --> 00:15:54,260
Judging the affordances of the system and
they do something to the system, right?

268
00:15:54,261 --> 00:15:59,261
They press a button way of a hand speak
of something and then if that's the red

269
00:15:59,301 --> 00:16:02,000
human part,
the blue is the familiar computer part.

270
00:16:02,300 --> 00:16:05,510
Computer takes that input goes through
some sort of algorithm and result in an

271
00:16:05,511 --> 00:16:09,290
output. This we know is a
oversimplified model of human cognition,

272
00:16:09,291 --> 00:16:13,460
but it's a very useful over simplified
model of human cognition and interaction

273
00:16:13,461 --> 00:16:14,294
design.

274
00:16:15,350 --> 00:16:19,430
But when we compare that to a
model for agents give technology,

275
00:16:19,760 --> 00:16:24,500
it gets different. So right. If the, if
the computer of course still down here,

276
00:16:24,650 --> 00:16:28,910
but if there's a computer running
the system with a human, not gone,

277
00:16:29,390 --> 00:16:34,160
but just on the outside of the system,
peaking in occasionally making requests,

278
00:16:34,190 --> 00:16:35,023
tweaking.

279
00:16:35,510 --> 00:16:40,160
Then we run into a whole lot of different
use cases and that's what this map is.

280
00:16:40,161 --> 00:16:44,450
It's in the book that you have a,
it's in piecemeal throughout section too,

281
00:16:44,451 --> 00:16:47,300
but it's in its whole version in
the very back on the appendix,

282
00:16:48,080 --> 00:16:51,260
but you can see here the unique use
cases for this involve a lot of setup.

283
00:16:51,410 --> 00:16:55,940
How do you give that agent of technology
your goals and your preferences towards

284
00:16:55,941 --> 00:16:58,490
getting to those goals?
Oh,

285
00:16:58,491 --> 00:17:01,070
I want my son to go to
college when he is of age.

286
00:17:02,300 --> 00:17:06,110
Is it going to be a state school or is
it going to be really expensive school?

287
00:17:06,320 --> 00:17:07,400
Right?
This preferences,

288
00:17:07,550 --> 00:17:11,540
do I want investments to go to vice
funds are specifically not to go to vice

289
00:17:11,541 --> 00:17:12,374
funds.

290
00:17:12,500 --> 00:17:15,770
All the sorts of things that one might
think about in the setup of like a Robo

291
00:17:15,771 --> 00:17:19,790
investor, use cases for seeing
or of course monitoring.

292
00:17:20,090 --> 00:17:23,360
How do I build trust that this thing is
going to do what I needed to do and it's

293
00:17:23,361 --> 00:17:27,980
in charge of something as important as my
money? How do I know that it's working?

294
00:17:28,940 --> 00:17:32,930
How do I know when it's running
out of things like resources,

295
00:17:34,190 --> 00:17:34,461
right?

296
00:17:34,461 --> 00:17:38,090
So there are some unique use cases for
seeing and then there's doing right?

297
00:17:38,091 --> 00:17:40,010
Pausing and restarting a,

298
00:17:40,011 --> 00:17:43,230
one of the great things from the research
that we did for the Robo investors,

299
00:17:43,250 --> 00:17:45,320
we wound up talking to a guy and we said,
okay,

300
00:17:45,321 --> 00:17:48,260
if you had $1 million and you wanted
to give it to the Robo investor,

301
00:17:48,410 --> 00:17:51,390
how would you that million
up? And he said, well,

302
00:17:51,391 --> 00:17:55,650
I give the Robo investor probably
900,000 but I keep 100,000 for myself.

303
00:17:56,160 --> 00:17:59,340
And we said, why? He said, I'm going to
see if I can beat it. And we were like,

304
00:18:00,180 --> 00:18:04,440
this is an algorithm that looks at the
entirety of the market. He was like, yeah,

305
00:18:04,441 --> 00:18:07,290
but I'm sure I have some
instinct that it does it right.

306
00:18:07,291 --> 00:18:10,920
So that need to play alongside
was sort of illustrated. Um,

307
00:18:11,460 --> 00:18:14,850
because we're talking about
narrow artificial intelligence
and we'll get to that

308
00:18:14,851 --> 00:18:17,340
in a little bit. Um, it's
going to get some things wrong.

309
00:18:17,760 --> 00:18:21,850
So the user needs tools in
order to tune it's black lists,

310
00:18:21,860 --> 00:18:24,510
it's white lists,
it's a play.

311
00:18:26,520 --> 00:18:28,050
And then of course for problems,

312
00:18:28,051 --> 00:18:30,460
there's a hand off and
take back problem or the A,

313
00:18:30,461 --> 00:18:32,960
and I runs into something
that it can't cope with. Um,

314
00:18:32,961 --> 00:18:37,680
and we need elegant ways to pass back
and control back and forth. And lastly,

315
00:18:37,681 --> 00:18:39,630
since almost all agents are persistent,

316
00:18:39,750 --> 00:18:43,950
trained to watch some data stream for a
trigger and then execute their behaviors,

317
00:18:44,490 --> 00:18:46,230
um,
we get into a problem with disengagement.

318
00:18:46,231 --> 00:18:48,360
How do we know when the
agent is no longer necessary?

319
00:18:49,410 --> 00:18:53,910
In the case of an invest a Robo
investor, it's significant. If I die,

320
00:18:54,390 --> 00:18:56,180
that money just doesn't
stay with the Robo investor.

321
00:18:56,181 --> 00:18:58,290
It needs to go to somebody that I need.

322
00:18:58,530 --> 00:19:00,450
So that disengagement
becomes really important.

323
00:19:01,050 --> 00:19:04,410
Not so much with the Roomba but
certainly in other circumstances.

324
00:19:05,430 --> 00:19:06,480
So this model,

325
00:19:06,481 --> 00:19:10,320
these unique use cases are sort of
underscoring what I say when I say, Hey,

326
00:19:10,620 --> 00:19:13,770
this is new and unique.

327
00:19:15,680 --> 00:19:17,600
Okay.
Number three,

328
00:19:18,500 --> 00:19:22,670
we have been studying usercenteredness
for a long time and part of what we are

329
00:19:22,671 --> 00:19:27,671
always doing as we apply designed towards
the problems at hand is we try and

330
00:19:27,981 --> 00:19:32,300
maximize the user value and minimize the
amount of work that they have to do to

331
00:19:32,301 --> 00:19:33,134
get there.

332
00:19:33,890 --> 00:19:38,890
And I can't imagine a better equation
then nearly zero input for maximum user

333
00:19:41,901 --> 00:19:42,734
value.

334
00:19:44,420 --> 00:19:45,253
Right?

335
00:19:45,280 --> 00:19:49,030
For familiar with pine and Gilmore in
1999 they published a book called the

336
00:19:49,031 --> 00:19:53,440
experience economy and they categorize
the types of products that can be pushed

337
00:19:53,441 --> 00:19:56,860
to market in one of four ways.
The first is of a commodity,

338
00:19:57,490 --> 00:20:00,850
barely differentiated,
cheapest dirt traded on the market.

339
00:20:00,851 --> 00:20:04,330
And if you wanted a cup of coffee and that
was their example. So I'm using it to,

340
00:20:05,440 --> 00:20:07,690
you would go and go to a wholesaler,

341
00:20:07,691 --> 00:20:10,590
bring a bag with you and a
scoop and scoop that stuff and,

342
00:20:10,591 --> 00:20:14,050
and you'd pay less than a
cent for that bag of beans.

343
00:20:15,070 --> 00:20:18,100
Commodities are really cheap because
they require the user to do a ton of work

344
00:20:19,410 --> 00:20:23,710
and next level up is called a product
where the company says, you know what?

345
00:20:23,800 --> 00:20:25,240
Don't worry about going to the wholesaler.

346
00:20:25,510 --> 00:20:28,120
We're going to grind these beans for you.
Put them in a really cool bag,

347
00:20:28,270 --> 00:20:31,840
will even design that bag to look really
cool, get it on grocery stores near you,

348
00:20:32,230 --> 00:20:35,800
and for that you will pay a
premium compared to the commodity,

349
00:20:36,850 --> 00:20:37,683
but hey,

350
00:20:38,590 --> 00:20:43,150
it's closer to you grocery stores
or Bodegas and it's pretty,

351
00:20:43,151 --> 00:20:45,010
it'll look good in your shelf.
It's already ground.

352
00:20:45,011 --> 00:20:48,990
You don't have to worry about grinding it.
Third Category,

353
00:20:48,991 --> 00:20:51,630
that head was this service where
a company would say, you know,

354
00:20:51,690 --> 00:20:53,760
don't even worry about
going to the grocery store.

355
00:20:54,330 --> 00:20:57,930
Come on into this space that we've
set aside. Let's call it a restaurant.

356
00:20:58,500 --> 00:21:01,380
And if you want a cup of coffee, you
just tell us. We will go into the back,

357
00:21:01,890 --> 00:21:04,680
grab our product, made up of
commodity, make you a cup of coffee,

358
00:21:04,681 --> 00:21:07,110
bringing out to you,
and we will even clean the dish.

359
00:21:07,800 --> 00:21:10,980
And for that you will pay a
premium compared to the product.

360
00:21:11,860 --> 00:21:14,820
The point of their book was that
there was a fourth layer that they had

361
00:21:14,821 --> 00:21:18,750
identified and they had been brought to
there that it had been brought to their

362
00:21:18,751 --> 00:21:21,660
attention by Starbucks.
So logo time,

363
00:21:23,500 --> 00:21:24,333
okay,

364
00:21:24,400 --> 00:21:27,450
Starbucks was a service,

365
00:21:28,170 --> 00:21:31,290
but how were they getting away with
charging as much as they were for a cup of

366
00:21:31,291 --> 00:21:34,620
coffee? And they said it was
because of the experience, right?

367
00:21:34,621 --> 00:21:37,380
You're just not going into a
diner and there's a ton of stuff.

368
00:21:37,381 --> 00:21:40,770
You're going into a coffee experience
where they have lots of wood,

369
00:21:40,800 --> 00:21:45,800
gorgeous wood paneling and lights and a
cool music playing and they abused the

370
00:21:45,811 --> 00:21:48,750
Italian language in order
to sell you this stuff. Um,

371
00:21:48,751 --> 00:21:50,970
and for that you consumers,

372
00:21:51,000 --> 00:21:55,050
it's pretty much been shown
in the marketplace that are
willing to pay a premium

373
00:21:55,051 --> 00:21:56,430
for that deep experience.

374
00:21:57,390 --> 00:22:01,380
The reason I spent a little bit of time
with this model is to show that I think

375
00:22:01,381 --> 00:22:06,381
there's another layer of
value that agency unlocks.

376
00:22:08,640 --> 00:22:13,640
All of these require
attention to extract value.

377
00:22:14,880 --> 00:22:17,760
It's either me with a scoop
me in a grocery store,

378
00:22:18,530 --> 00:22:22,530
me and a diner or me at Starbucks,
but in the case of the room,

379
00:22:22,531 --> 00:22:26,910
but I get value out of that object
when I'm at work. My cat is fed,

380
00:22:26,911 --> 00:22:29,700
what am I am traveling?
There is value there.

381
00:22:29,701 --> 00:22:32,280
That doesn't depend on the
16 hours of my attention,

382
00:22:32,580 --> 00:22:36,540
which is limited and a very
competitive competitive space, right?

383
00:22:36,541 --> 00:22:37,374
If you think about that,

384
00:22:38,040 --> 00:22:41,940
the opportunity for value that you can
provide your users, your customers,

385
00:22:41,941 --> 00:22:44,790
however you want to think about
them is comparatively infinite.

386
00:22:45,720 --> 00:22:50,010
So I believe there's a post attention
value that we can begin to capitalize on

387
00:22:50,190 --> 00:22:53,190
when we equip our products for this mode.

388
00:22:54,590 --> 00:22:55,423
Number four,

389
00:22:56,670 --> 00:23:00,100
PW singer wrote a scary book called uh,

390
00:23:01,180 --> 00:23:03,870
something of war. I
have lost it, but ended.

391
00:23:03,900 --> 00:23:06,390
He makes this argument that there
are certain technologies that once a

392
00:23:06,391 --> 00:23:09,600
civilization adopts that can't go back.

393
00:23:10,890 --> 00:23:12,870
He's thinking specifically
of drones in warfare.

394
00:23:12,871 --> 00:23:15,390
Once we send machines to do the
fighting against other machines.

395
00:23:15,540 --> 00:23:18,960
Why on earth would you send
your flesh and blood dark,

396
00:23:19,590 --> 00:23:23,160
good conversation worth having.
But in terms of agent of technology,

397
00:23:23,280 --> 00:23:27,450
I believe it's threshold. Once you
have the room, but in your life,

398
00:23:28,110 --> 00:23:30,420
how much are you going to
want to go back to the Dyson?

399
00:23:32,520 --> 00:23:37,520
Once you have a car that will drive you
and your kid to their school and you'd

400
00:23:38,631 --> 00:23:40,140
get all this quality time with them,

401
00:23:40,290 --> 00:23:42,870
how much are you going to want
to get behind the wheel and say,

402
00:23:42,871 --> 00:23:47,480
get in the back and I really can't
talk to you. We get there, right?

403
00:23:47,810 --> 00:23:51,950
The amount of value that you get from
an agent of technology is so great.

404
00:23:52,220 --> 00:23:56,630
I believe that going back becomes
drudgery and for that it's a threshold

405
00:23:56,631 --> 00:24:00,800
technology and for that reason it's a
massive competitive advantage to those

406
00:24:00,801 --> 00:24:04,520
companies that adopted first.
Last bit.

407
00:24:05,630 --> 00:24:09,320
It's Ai,
which I don't think it's the AI to fear,

408
00:24:09,321 --> 00:24:11,210
but it's certainly interesting to me.

409
00:24:11,390 --> 00:24:13,220
So let me take just a
moment and situate it.

410
00:24:13,760 --> 00:24:16,760
I include this as part of the talk
because I don't know how well you guys are

411
00:24:16,761 --> 00:24:20,420
versed in AI, but I may be
overstepping you baby, uh,

412
00:24:20,450 --> 00:24:24,110
much more knowledgeable about this
than I am. Bear with me if you are,

413
00:24:25,460 --> 00:24:27,050
but in the literature of Ai,

414
00:24:27,230 --> 00:24:31,010
AI as a field is broken up into three
primary categories depending on the

415
00:24:31,011 --> 00:24:34,730
capabilities of the AI itself.
The first one that,

416
00:24:34,760 --> 00:24:38,960
the one that most people think of
when they hear AI is general AI,

417
00:24:39,110 --> 00:24:43,400
so called because the AI can generalize
knowledge just like you or I can.

418
00:24:44,330 --> 00:24:48,590
Uh, I run a blog about
science fiction interfaces,

419
00:24:48,650 --> 00:24:50,360
super nerdy.
Um,

420
00:24:50,390 --> 00:24:53,720
and so I'm going to make a couple of
Scifi references and this is my first who

421
00:24:53,721 --> 00:24:57,470
does not know the movie war games.
Okay,

422
00:24:58,250 --> 00:25:01,400
spoiler alert. I'm going to tell you a
little bit about war games, but in this,

423
00:25:01,401 --> 00:25:06,401
there is an AI that has been trained
to play games and a lot of them are

424
00:25:07,341 --> 00:25:10,130
harmless games, but one of them
is global thermonuclear war.

425
00:25:10,280 --> 00:25:13,450
What the Ai doesn't know is that it's
actually tied into the nuclear arsenal of

426
00:25:13,460 --> 00:25:16,070
the United States. Oh my God.
And it's got this countdown.

427
00:25:16,670 --> 00:25:19,280
It's going to start playing this game
and ruined all our lives with the

428
00:25:19,281 --> 00:25:24,080
protagonist ends up getting an idea
and you have the end of the film, Hey,

429
00:25:24,200 --> 00:25:27,470
let's play tic TAC toe and it begins
to play tic tac toe with this AI called

430
00:25:27,471 --> 00:25:28,760
whopper.
Terrible name.

431
00:25:29,300 --> 00:25:34,300
Or Joshua a is the nickname and over
playing tic tac toe over and over again,

432
00:25:35,310 --> 00:25:39,290
the Ai goes suddenly realizes, oh
hey, this is a game that can't be one.

433
00:25:39,770 --> 00:25:43,280
And then it generalizes that
knowledge and thinks, Huh?

434
00:25:44,090 --> 00:25:46,880
And it begins to run through scenarios
of global thermonuclear war and says, oh,

435
00:25:46,881 --> 00:25:49,850
that's a game that cannot be won.
Why on Earth would we play it?

436
00:25:51,080 --> 00:25:54,410
The countdown timer stops. They have
saved, guy gets the girl movie over,

437
00:25:55,040 --> 00:25:57,800
but it's a pretty good
example of General Ai, right?

438
00:25:57,801 --> 00:26:01,340
That AI is able to generalize doing what
you and I have done since toddler hood

439
00:26:01,850 --> 00:26:05,420
when we took the physical things in the
abstractions in the world and then began

440
00:26:05,421 --> 00:26:09,200
to build them to the adult knowledges
that we move through the world with today.

441
00:26:10,280 --> 00:26:14,960
Once we have a general AI, one of the
first things that we're going to do,

442
00:26:14,990 --> 00:26:17,360
whoever gets that,
it's going to ask it to say,

443
00:26:17,361 --> 00:26:22,280
can you make a copy of yourself that
is a better predictor and has better

444
00:26:22,281 --> 00:26:27,080
outcomes and make sure that copy is deeply
interested in making a copy of itself.

445
00:26:27,770 --> 00:26:30,230
And that'll make a copy and that'll make
a copy and that will make a copy and

446
00:26:30,231 --> 00:26:33,440
eventually what will come out the other
end. And it depends on who you ask,

447
00:26:33,441 --> 00:26:35,540
how long that takes.
It could take a couple of hours,

448
00:26:35,541 --> 00:26:36,650
it could take a couple of months,

449
00:26:36,890 --> 00:26:41,510
but we'll be something so smart
that the running metaphor is,

450
00:26:41,630 --> 00:26:45,600
it's intelligence will be
to us as ours is to a birds.

451
00:26:46,860 --> 00:26:51,270
We will not have the language to think
in the questions it is interested in.

452
00:26:51,990 --> 00:26:53,220
If you want to be scared of an AI,

453
00:26:53,221 --> 00:26:58,170
be scared of that AI because if that
is not pre loaded with a care for human

454
00:26:59,100 --> 00:27:02,670
wellness, we're in trouble. And in fact,

455
00:27:02,671 --> 00:27:07,671
the mathematician and science fixture
fiction author Verner Venga probably

456
00:27:07,981 --> 00:27:11,790
murdering the pronunciation coined to
this moment as we pass from general AI to

457
00:27:11,791 --> 00:27:15,110
super AI is the singularity because
we don't know what life is like with a

458
00:27:15,120 --> 00:27:18,090
functioning God able to
answer our questions.

459
00:27:19,300 --> 00:27:20,030
Okay.

460
00:27:20,030 --> 00:27:24,800
I raise these because agent of tech
is a mode of interaction that does not

461
00:27:24,801 --> 00:27:27,290
involve these two.
It might set it up,

462
00:27:27,291 --> 00:27:28,790
but let's talk about that
at the end of the talk.

463
00:27:29,870 --> 00:27:33,200
The one that we have in the world now,
which is not as terrifying as these guys,

464
00:27:33,201 --> 00:27:35,030
isn't narrow artificial intelligence,

465
00:27:35,210 --> 00:27:40,210
so called because it's very good at one
or two things and can't generalize that

466
00:27:40,221 --> 00:27:43,400
knowledge. I can't ask the Roomba to
help me plan a Thanksgiving dinner.

467
00:27:43,700 --> 00:27:48,440
Not yet shirts and the roadmap,
but narrow AI is the one,

468
00:27:48,441 --> 00:27:52,100
I'm a very practical person and this
is the one I'm most interested in.

469
00:27:52,400 --> 00:27:54,350
Lots of exciting stuff to talk about here.

470
00:27:54,470 --> 00:27:57,070
What do we do with all the jobs
that are going to be lost? Um, what,

471
00:27:57,080 --> 00:28:01,160
what did we do? Who's got the last jobs?
I think it's designers and judges. Um,

472
00:28:01,220 --> 00:28:05,720
but let's talk about this because this
is what we have in the world now for my

473
00:28:05,721 --> 00:28:09,170
money. We talk a lot about AI
in terms of what it can do,

474
00:28:09,440 --> 00:28:14,440
but I'm most interested in
what the relationship is of
the user to the work that

475
00:28:14,451 --> 00:28:18,620
the AI is doing.
And I found three categories.

476
00:28:19,760 --> 00:28:22,010
I may be wrong if so,
let's chat about it.

477
00:28:22,760 --> 00:28:27,640
But the first one is automatic stuff
that should just happen, right? Uh,

478
00:28:27,660 --> 00:28:32,090
a pacemaker is a good example. I never
want a pacemaker to ask me in the morning,

479
00:28:32,510 --> 00:28:37,040
do you want me to make your pace?
Because the answer is always yes.

480
00:28:37,580 --> 00:28:42,200
Same thing with the grocery
store. Automatic door. I
never want that to ask, right?

481
00:28:42,201 --> 00:28:43,280
Just open,
just do it.

482
00:28:44,390 --> 00:28:47,630
This sort of thing does fit
narrow artificial intelligence,

483
00:28:47,631 --> 00:28:51,710
but only for well constrained
unpersonalized things.

484
00:28:55,760 --> 00:29:00,590
Assistance are things that you
would use to help you do a task,

485
00:29:01,520 --> 00:29:04,700
right? Like an angel on your
shoulder, whispering in your ear.

486
00:29:05,450 --> 00:29:09,680
Scifi referenced number two,
like Jarvis for iron man or now Friday,

487
00:29:10,490 --> 00:29:10,791
right?

488
00:29:10,791 --> 00:29:14,720
It's an assistant to him though I make
the case in the blog that Jarvis is the

489
00:29:14,721 --> 00:29:17,600
iron man. Um, and when I thought
about this, I was like, well,

490
00:29:17,601 --> 00:29:19,970
what do we parse as it,

491
00:29:20,270 --> 00:29:25,270
what do we give to the assistance of the
world and never want to hand off to a

492
00:29:25,821 --> 00:29:29,390
non human? And I found four and a half
categories and they're listed up there.

493
00:29:30,110 --> 00:29:31,730
The first one is our jobs.

494
00:29:32,030 --> 00:29:36,110
If you and I have an economic
agreement that I will do work for you,

495
00:29:37,220 --> 00:29:40,370
and instead I handed that to an
AI and spend my days on the beach,

496
00:29:40,640 --> 00:29:45,350
we'd probably an ethical problem. There's
an argument to be made of, Oh hey,

497
00:29:45,351 --> 00:29:48,080
why don't you sell me that AI so
that I can use it at my company?

498
00:29:48,081 --> 00:29:50,210
But for the most part,
you're not doing your job.

499
00:29:50,211 --> 00:29:51,510
You're sneaking away from your job.

500
00:29:51,511 --> 00:29:55,460
So jobs are something that we want
humans to be outfitted with assistance,

501
00:29:56,270 --> 00:30:00,800
not agents.
The second is human connection.

502
00:30:01,040 --> 00:30:06,040
I suspect that in about 10 years I'm going
to greatly prefer an AI for my doctor

503
00:30:09,740 --> 00:30:13,310
to be able to recognize and diagnose
the problems that I have with my health.

504
00:30:13,970 --> 00:30:14,690
Right.

505
00:30:14,690 --> 00:30:19,690
Already we know that Watson can read every
medical publication that is published

506
00:30:19,971 --> 00:30:24,890
around the world, pretty much
in real time. It's diagnosis.

507
00:30:25,760 --> 00:30:28,220
Despite the MD Anderson troubles of late,

508
00:30:29,120 --> 00:30:32,360
I'm going to become much more
confident in than a humans.

509
00:30:33,410 --> 00:30:38,060
That is not true for a nurse. Part of the
purpose of the nurses human connection.

510
00:30:38,090 --> 00:30:41,810
I don't want an AI waking me up and
saying, do you need more painkiller?

511
00:30:42,830 --> 00:30:46,670
That's not going to help me feel cared
for. Helped me feel loved not left.

512
00:30:47,030 --> 00:30:48,650
It's not going to get me
on the road to wellness.

513
00:30:49,640 --> 00:30:54,640
So if human connection is part of the
purpose of the thing that I don't think we

514
00:30:55,011 --> 00:30:57,890
ever want to hand that off to an agent
and we do want it to be an assistant

515
00:30:59,480 --> 00:31:02,990
the last year related, right.
If the point is physiology,

516
00:31:03,500 --> 00:31:05,650
then I can't send a robot to do the work.
Right?

517
00:31:05,651 --> 00:31:09,470
I can't send a robot to go
to the gym for me. I'd be,

518
00:31:09,471 --> 00:31:14,360
I'd be jacked if I could. Skills are
similar. If I'm trying to learn French,

519
00:31:14,361 --> 00:31:17,750
I can't certain send an agent to French
class in order to study the language for

520
00:31:17,751 --> 00:31:20,870
me, I have to be there so that
my brain acquires those skills.

521
00:31:22,670 --> 00:31:25,250
Lastly, there's a half
category of art. Um,

522
00:31:25,251 --> 00:31:29,540
I say it's a half category because there
are plenty of examples where a computer

523
00:31:29,541 --> 00:31:32,960
generated stuff we find
delightful until Arius,

524
00:31:33,430 --> 00:31:35,930
this is inspire robot popular of late,

525
00:31:36,140 --> 00:31:40,070
but it's a neural net that's trying to
create inspirational images and they're

526
00:31:40,071 --> 00:31:40,910
just hilarious.

527
00:31:44,060 --> 00:31:47,670
But the reason I say it's still
a half category is because, uh,

528
00:31:47,720 --> 00:31:50,570
when people find out that a poem
they love has been written by an AI,

529
00:31:50,571 --> 00:31:54,800
they feel betrayed.
Not True with visual imagery.

530
00:31:54,920 --> 00:31:58,310
So it's a half category and I'm sure
they're going to be artists who are going

531
00:31:58,311 --> 00:32:00,380
to rock out using AI in their work.

532
00:32:02,420 --> 00:32:05,660
But that leaves agent of tech,
right?

533
00:32:05,690 --> 00:32:09,770
That thing where we partner with a piece
of technology in order to do the things

534
00:32:09,950 --> 00:32:14,150
for us that we want it to do.
And that means everything else.

535
00:32:14,300 --> 00:32:16,460
If it's not one of those
four and a half categories,

536
00:32:17,980 --> 00:32:18,813
that's a lot.

537
00:32:20,210 --> 00:32:22,700
That includes like things
that we're not good at,

538
00:32:24,830 --> 00:32:25,310
right?

539
00:32:25,310 --> 00:32:29,570
The whole reason we had autopilot is
that humans have only about 20 minutes

540
00:32:29,571 --> 00:32:32,840
worth of vigilance. And I know you're
already past that in the audience,

541
00:32:32,841 --> 00:32:34,570
so thank you.
Um,

542
00:32:34,790 --> 00:32:37,370
but where we can only pay attention
to a signal for 20 minutes before our

543
00:32:37,371 --> 00:32:39,640
attention begins to drift.
Um,

544
00:32:39,860 --> 00:32:43,610
and so we need partners to help us
with the tasks that we must do that are

545
00:32:43,611 --> 00:32:46,460
longer,
that are beyond common human capabilities.

546
00:32:47,180 --> 00:32:49,460
It includes technologies
that were unwilling to do.

547
00:32:49,461 --> 00:32:53,450
We've known about the Pacific gyres
for the better part of two decades now.

548
00:32:54,030 --> 00:32:56,480
We haven't been cleaning them and yes,

549
00:32:56,481 --> 00:32:59,750
we could probably put some humans and
some boats to go out and clean that mess

550
00:32:59,751 --> 00:33:02,020
up. Um, but once we have robots doing,

551
00:33:02,060 --> 00:33:04,760
I think we're going to be hard pressed
to find humans that want to take those

552
00:33:04,761 --> 00:33:09,380
robots place. By the way, the Pacific
guard does not look like that. Just soupy,

553
00:33:09,381 --> 00:33:12,110
plastic mess. It's not actual
trash floating in the ocean.

554
00:33:13,610 --> 00:33:17,300
And it also includes stuff that
we just cannot do alone. Right?

555
00:33:17,301 --> 00:33:19,430
We know we only have about 4
billion years in this planet,

556
00:33:19,431 --> 00:33:23,630
which doesn't seem like a
lot, but boy, howdy getting
a, an entire civilization. Um,

557
00:33:23,631 --> 00:33:27,350
to some of the rock is a major undertaking
and we can send humans out into the

558
00:33:27,351 --> 00:33:28,700
void,
but we're fragile.

559
00:33:29,180 --> 00:33:33,710
Error prone sending robots
is a much saner idea,

560
00:33:33,711 --> 00:33:37,760
but the farther away from us they get,
the longer that communication time is.

561
00:33:38,630 --> 00:33:42,050
And we have to have some kind of smart
on those robots as they explore the

562
00:33:42,051 --> 00:33:45,620
galaxy to handle things that happen in
between the moment of communication.

563
00:33:46,100 --> 00:33:47,510
Fortunately,
NASA is already on this.

564
00:33:47,511 --> 00:33:52,280
They have something called the
NASA agent architecture for right.

565
00:33:52,281 --> 00:33:56,840
But we can't do this task alone that we
have to do and we're going to have to

566
00:33:57,230 --> 00:34:02,000
partner very in very smart ways with our
technology in order to make it happen.

567
00:34:04,390 --> 00:34:05,560
Which brings us back to Ada.

568
00:34:07,060 --> 00:34:09,880
You'll remember that eight is objection
or the Lovelace objection is touring

569
00:34:09,881 --> 00:34:10,231
call.

570
00:34:10,231 --> 00:34:15,231
That was Ken computers take initiative
and I think I've certainly shown that.

571
00:34:15,550 --> 00:34:19,520
Oh yes. The patterns that he showed
at the beginning where they can, um,

572
00:34:19,600 --> 00:34:22,900
even the examples that I showed
show that they do already,

573
00:34:23,110 --> 00:34:26,350
a few people have already started to
move their technology in this direction.

574
00:34:27,190 --> 00:34:31,000
I'm writing for both business
reasons and for ethical reasons.

575
00:34:31,030 --> 00:34:33,880
I think that they should in certain cases.

576
00:34:35,530 --> 00:34:36,190
Yeah.

577
00:34:36,190 --> 00:34:39,160
So part of the mission I've gotten writing
this book and going and making talks

578
00:34:39,161 --> 00:34:43,030
like this is to convince people that,
hey, let's move in this direction.

579
00:34:43,031 --> 00:34:46,420
Let's build our products such
they're equipped this way.

580
00:34:47,560 --> 00:34:51,130
And so I'm going to leave you with three
questions to ask for the products that

581
00:34:51,131 --> 00:34:51,964
you guys build.

582
00:34:52,410 --> 00:34:57,280
And the first is basic in that it works
from the micro interaction level all the

583
00:34:57,281 --> 00:34:59,950
way up to the strategic
directions of products and to say,

584
00:34:59,951 --> 00:35:04,951
are we asking users to do something that
we could do for them if they so wanted.

585
00:35:09,920 --> 00:35:14,600
Once you answer that question and you
begin to create the agency of your

586
00:35:14,601 --> 00:35:18,110
products or agent of modes of your
products, then you have to say, okay,

587
00:35:18,111 --> 00:35:20,000
well it just not purely agent IV.

588
00:35:20,180 --> 00:35:22,270
The rooms in the world I think
are going to be the exception.

589
00:35:22,300 --> 00:35:25,400
Something that just fits nicely
into one of those categories.

590
00:35:26,210 --> 00:35:31,180
How can our product support automation
when the confidence is super high agent

591
00:35:31,181 --> 00:35:34,550
tive when it's a task that the user
doesn't want to do and assist of modes

592
00:35:36,350 --> 00:35:38,370
because smart products products

593
00:35:38,370 --> 00:35:42,630
we'll have to work in all three.
And lastly,

594
00:35:42,660 --> 00:35:46,170
how does our product help the
user, right? Whether the product,

595
00:35:46,171 --> 00:35:49,350
the Ai is taking initiative or
the human is taking initiative,

596
00:35:49,830 --> 00:35:52,050
how do we flow smoothly
between those modes?

597
00:35:53,340 --> 00:35:56,290
I think once we answer those
questions, we will be, uh,

598
00:35:56,370 --> 00:36:00,630
many further steps on the way to getting
our technology to take advantage of

599
00:36:00,631 --> 00:36:04,350
this conceptual framework
of agent TIF technologies.

600
00:36:05,160 --> 00:36:05,671
Um,

601
00:36:05,671 --> 00:36:10,620
so you can follow me at agent tiff
tech or Chris Nossel but that's it.

602
00:36:10,770 --> 00:36:11,950
Let's have a chat about it.

603
00:36:16,200 --> 00:36:16,560
Um,

604
00:36:16,560 --> 00:36:21,560
so I had a question about where the
initiative of decision making falls into

605
00:36:23,011 --> 00:36:27,960
this pyramid. So you mentioned a
lot of tasks that, uh, agentic IV,

606
00:36:28,580 --> 00:36:31,950
uh, technology will do
what we're unwilling to do.

607
00:36:32,010 --> 00:36:36,870
But it sounded like that's where the
humans have to kind of give the initiative

608
00:36:36,900 --> 00:36:40,050
to these other devices to do.

609
00:36:40,470 --> 00:36:42,180
How do we kind of,

610
00:36:43,300 --> 00:36:48,150
it's like how do we seamlessly
move into a place where humans,

611
00:36:48,180 --> 00:36:53,100
where we can suggest things that we
think should be done and humans will,

612
00:36:53,310 --> 00:36:56,070
um, trust that suggestion. Right?

613
00:36:56,190 --> 00:36:59,220
I think that's a big problem that we
have now as we move into a world where

614
00:36:59,221 --> 00:37:01,230
things are getting more
and more intelligent. Yup.

615
00:37:01,590 --> 00:37:05,610
And people can kind of get scared of those
suggestions, which might sound simple,

616
00:37:05,611 --> 00:37:09,030
but they don't know all of the
algorithms that are going on behind it.

617
00:37:09,450 --> 00:37:12,340
And so that it's moving in that
direction. Do you have any, yeah.

618
00:37:12,800 --> 00:37:16,900
And in fact, earlier this year,
the EU passed a resolution, um,

619
00:37:16,940 --> 00:37:20,180
about AI that included a provision
called the right to explanation,

620
00:37:20,600 --> 00:37:24,350
which says that if you are subject
to an [inaudible] decision,

621
00:37:24,890 --> 00:37:27,530
you as a citizen have the right to
understand how that decision was made,

622
00:37:27,531 --> 00:37:31,200
which is going to be problematic in
the world of neural nets. But, um,

623
00:37:31,280 --> 00:37:35,990
in the book I describe a pattern called
a hood to look under and it's a trust

624
00:37:35,991 --> 00:37:39,770
building pattern. And the notion is
that for any decision you should be,

625
00:37:39,771 --> 00:37:41,780
and it's easier with narrow AI,

626
00:37:42,440 --> 00:37:45,290
you should give that user an opportunity
to open up the hood and see how that

627
00:37:45,291 --> 00:37:48,350
decision was made,
disagree with that decision,

628
00:37:48,710 --> 00:37:52,340
and either provide categorical
imperatives for future behavior,

629
00:37:52,790 --> 00:37:56,720
white lists, black lists, inclusions,
or even particular tweaks.

630
00:37:56,900 --> 00:38:00,650
So I think you should be able to go
down all the way, um, in common usage.

631
00:38:00,650 --> 00:38:04,580
I suspect that's going to be similar to
the get narrative camera. Wait a minute.

632
00:38:04,581 --> 00:38:07,340
Why did you show me that
image? Oh yeah, you're right.

633
00:38:07,341 --> 00:38:10,340
That's the only image from
the party that was clear.

634
00:38:10,460 --> 00:38:15,260
And the chode the people. Well and the
balance was right. Okay, I agree with you.

635
00:38:15,290 --> 00:38:19,340
And over time, having that hood to
look under will help me build trust.

636
00:38:19,580 --> 00:38:21,890
And when I don't trust it
and it's got it wrong to be,

637
00:38:21,891 --> 00:38:25,280
to influence it such that it
behaves the way I want it to.

638
00:38:25,820 --> 00:38:28,730
So check out that pattern in the book.
And I think it may answer your question.

639
00:38:29,210 --> 00:38:31,280
Um,
there was a second question implied there,

640
00:38:31,281 --> 00:38:36,281
which is about recommendations and I
think that any system should be able to

641
00:38:36,491 --> 00:38:39,550
recommend to users new behaviors.
Um,

642
00:38:39,640 --> 00:38:41,530
occasionally there's a,

643
00:38:41,531 --> 00:38:46,531
there's an ugly tension between agency
that goes off and does its work out of

644
00:38:47,261 --> 00:38:51,790
your attention and the brand importance
of not becoming a commodity. Right?

645
00:38:51,791 --> 00:38:54,520
Brands want to be first and foremost.
Um,

646
00:38:54,521 --> 00:38:58,060
so there's a weird tension there and I
believe that the corporate pressure we

647
00:38:58,061 --> 00:39:01,940
will have will be to over communicate
and our job as designers is to match what

648
00:39:01,941 --> 00:39:03,380
the humans want.
Um,

649
00:39:03,550 --> 00:39:07,360
but I don't have any problem with an
agent coming and making an occasional

650
00:39:07,361 --> 00:39:11,530
recommendation to me as a, as a user.
And I suspect users at large. Um,

651
00:39:11,620 --> 00:39:13,900
so I don't think the
decision itself is a problem.

652
00:39:14,110 --> 00:39:17,560
Understanding how it was made and then
being able to tweak that as the important

653
00:39:17,561 --> 00:39:18,394
part of the pattern.

654
00:39:19,240 --> 00:39:22,870
Why designers and judges,
why are they the last one set of jobs?

655
00:39:23,360 --> 00:39:27,950
Uh, yes, I suspect it's because
that we do the same thing,

656
00:39:28,190 --> 00:39:31,610
right?
We judge what's good and we know how to,

657
00:39:32,270 --> 00:39:36,080
we are good at understanding humans
and then designing systems, uh,

658
00:39:36,140 --> 00:39:39,680
depending on your definition of design
that optimized for a human set of effects,

659
00:39:40,080 --> 00:39:43,670
um, and Ai's will not know that
inherently from the very beginning. Um,

660
00:39:43,720 --> 00:39:46,340
and they'll come to us to
ask those questions. Um,

661
00:39:46,400 --> 00:39:50,390
similarly I think that a AI's are an
alien and tele or the generally I will be

662
00:39:50,391 --> 00:39:53,690
an alien intelligence and they'll need
to turn to judges to know in order to

663
00:39:53,691 --> 00:39:57,530
understand what to do or what humans
would ordinarily do in the edge cases.

664
00:39:57,760 --> 00:40:00,490
I'm in law as of course, all
about the edge cases. Um,

665
00:40:00,520 --> 00:40:02,450
so I think those are the reasons.
The last two,

666
00:40:02,840 --> 00:40:04,670
I didn't say this over
the course of the talk,

667
00:40:04,940 --> 00:40:08,660
but I'm actually pretty hopeful about
the role that agent of technologies would

668
00:40:08,661 --> 00:40:11,720
have in setting up a
general AI to be benign.

669
00:40:12,770 --> 00:40:15,260
I know it's tricky to
say, but, um, if the,

670
00:40:15,261 --> 00:40:20,261
if the output of a ton of agent of
technologies in the world is a set of

671
00:40:21,410 --> 00:40:26,180
instructions for how you want things
to behave in order to serve you well,

672
00:40:26,390 --> 00:40:27,590
what we'll have at the end of that,

673
00:40:27,591 --> 00:40:31,790
at the advent of general AI
is 4 million laws of robotics,

674
00:40:32,360 --> 00:40:33,800
right?
Where an AI,

675
00:40:33,801 --> 00:40:36,950
we can read all of these instructions
at lunch can begin to infer, oh,

676
00:40:37,100 --> 00:40:39,830
this is how humans generally
want to be treated.

677
00:40:40,130 --> 00:40:44,480
And that's a pretty important big data
piece of information to handle general AI.

678
00:40:45,380 --> 00:40:49,790
So I'm hopeful in that regard. Yes. Uh,

679
00:40:50,260 --> 00:40:50,861
thinking through this,

680
00:40:50,861 --> 00:40:55,861
but what are some examples of agent of
products that you've found have been able

681
00:40:56,531 --> 00:40:57,700
to close the gap?

682
00:40:57,940 --> 00:41:02,940
Maybe between the manual traditional way
of doing something and then handing off

683
00:41:03,671 --> 00:41:06,160
that service and uh,

684
00:41:06,910 --> 00:41:11,440
building off that maybe some Asian two
products were completely AI generated

685
00:41:11,441 --> 00:41:11,741
products.

686
00:41:11,741 --> 00:41:16,300
Like the poems that you've found people
just did not like she didn't having

687
00:41:16,301 --> 00:41:18,370
generated.
How,

688
00:41:19,150 --> 00:41:23,530
how have you seen ways that that gap can
be closed to where they do appreciate

689
00:41:23,531 --> 00:41:26,710
those poems in the future,
for instance?

690
00:41:27,140 --> 00:41:30,340
Um, so let's tackle the two. Uh,

691
00:41:30,350 --> 00:41:33,710
the first question was what are some
examples of products that have genuinely

692
00:41:33,711 --> 00:41:37,700
managed to work fine and go from
human distrust to human trust?

693
00:41:38,450 --> 00:41:42,200
Spam filters?
Spam filters are really great example,

694
00:41:42,530 --> 00:41:44,120
partially because when
they were first there,

695
00:41:44,121 --> 00:41:46,070
people don't necessarily trust them.
Um,

696
00:41:46,130 --> 00:41:49,790
partially because well done spam
filters and include Jimmy on this,

697
00:41:50,120 --> 00:41:52,850
I'm still give you the opportunity
to go in and review, open the hood.

698
00:41:53,030 --> 00:41:56,620
Do I agree with you on these things?
They give controls for blacklists.

699
00:41:56,630 --> 00:41:59,870
I never want to hear from this
person again. And white lists.

700
00:41:59,871 --> 00:42:04,660
I always wanted to hear from this person,
um, and incorporates deep, uh, uh,

701
00:42:04,820 --> 00:42:09,800
narrow artificial intelligences in
order to end and a group feedback.

702
00:42:09,801 --> 00:42:12,770
If you know 20 people all mark,
this is undesirable.

703
00:42:12,890 --> 00:42:15,650
You can pretty much guarantee you that
the other users that are similar to them,

704
00:42:15,740 --> 00:42:20,420
we'll find the same thing. So, uh, email
filters are a great, great example.

705
00:42:21,110 --> 00:42:23,480
Uh, and partially because
like, I dunno about you,

706
00:42:23,481 --> 00:42:27,650
but I haven't gone into my rejected folder
in a really long time because I trust

707
00:42:27,651 --> 00:42:31,520
them. Yeah. Does anyone
still go in? Right.

708
00:42:31,521 --> 00:42:33,290
So that's a great example as a model.

709
00:42:34,370 --> 00:42:38,450
The second part of your question
was, oh, a things that have failed.

710
00:42:39,890 --> 00:42:42,590
I don't have a great, uh, answer for
that right off the top of my head.

711
00:42:42,920 --> 00:42:45,200
The get narrative is a risky example,

712
00:42:45,410 --> 00:42:47,570
partially because that
company has been struggling,

713
00:42:47,630 --> 00:42:50,900
but I don't think it's because
of the agency of the product.

714
00:42:51,230 --> 00:42:54,710
I think it's actually because of
our concerns about privacy, um,

715
00:42:54,740 --> 00:42:59,210
that the domains in which I can wear a
camera and not worry about the privacy of

716
00:42:59,211 --> 00:43:02,090
the people in front of me. It's
actually pretty limited to cross my day.

717
00:43:02,270 --> 00:43:03,110
It's not my work.

718
00:43:03,180 --> 00:43:08,090
Certainly not my inside my family unless
I'm just sharing those to a small group.

719
00:43:08,480 --> 00:43:12,620
Um, like maybe a party, but still when I
wore one and I did for about four months,

720
00:43:12,621 --> 00:43:15,710
people would be like, oh,
what's that? Maybe hiking,

721
00:43:15,711 --> 00:43:18,680
maybe public things like
walking around the city. Um,

722
00:43:18,681 --> 00:43:21,440
but that's a small percentage of
certainly my time in the world.

723
00:43:21,680 --> 00:43:26,240
So I think that the problems that
they had weren't about the technology,

724
00:43:26,241 --> 00:43:29,720
but about bringing that technology
to that particular domain.

725
00:43:30,320 --> 00:43:32,150
I'll think about it a little more.
Um,

726
00:43:33,590 --> 00:43:37,520
I think Roomba is making a big mistake
with the announcement that they gave this

727
00:43:37,520 --> 00:43:41,660
week that they have slowly been
building up a model of people's homes.

728
00:43:42,150 --> 00:43:47,150
And even though they said we'll let users
opt in to sharing that home platform

729
00:43:47,691 --> 00:43:52,430
and the details that Roomba has
found with a co marketers, um,

730
00:43:52,431 --> 00:43:56,210
it's still super creepy knowing that it's
creating a model of your home that can

731
00:43:56,211 --> 00:44:01,160
be hacked by anyone.
So I don't know how that's going to fair.

732
00:44:01,161 --> 00:44:04,790
I suspect that the announcement this week
was a bit of a flag to see how people

733
00:44:04,791 --> 00:44:09,530
responded and I, it's being overshadowed,
overshadowed by modern politics,

734
00:44:09,650 --> 00:44:10,770
but I hope that they get,
um,

735
00:44:10,860 --> 00:44:13,780
some negative feedback because people
are going to be creeped out. Um,

736
00:44:13,880 --> 00:44:17,510
I can think of some other I examples, um,
and tweet them out there if you'd like.

737
00:44:19,820 --> 00:44:22,880
I will volunteer. Three of the
things, uh, for the things,

738
00:44:23,550 --> 00:44:28,550
the first is one of the questions that
I get asked a lot about is what do we do

739
00:44:29,091 --> 00:44:33,840
with the humans in the world of, and
I, I don't have a pat answer for that,

740
00:44:33,900 --> 00:44:37,920
but I am a big believer in universal
basic income. Um, I think it's the,

741
00:44:38,220 --> 00:44:41,970
we have always had technologies
that slowly obviate jobs,

742
00:44:42,120 --> 00:44:45,300
but the speed at which we are
going to replace entire fields,

743
00:44:45,301 --> 00:44:48,900
it's going to be massive and we have
to come at that from a cultural answer,

744
00:44:48,901 --> 00:44:53,310
not a, uh, not a, Oh, it'll work
itself out. Free market kind of answer.

745
00:44:54,430 --> 00:44:58,050
The other three are sort of the next
steps over the course of this book,

746
00:44:58,051 --> 00:45:01,110
and I'll give this to you guys because
you're pretty advanced audience.

747
00:45:02,460 --> 00:45:05,550
I talk about the world as if
there were one user and one agent.

748
00:45:06,750 --> 00:45:09,870
That's certainly not true.
We have to get good at that foundation,

749
00:45:10,140 --> 00:45:15,140
but over time it's going to be user one
user to many agents and even multiple

750
00:45:15,241 --> 00:45:17,940
users to multiple agents.
I don't talk about that here,

751
00:45:17,941 --> 00:45:20,640
but it's something that we're going to
have to solve as a community of practice

752
00:45:20,641 --> 00:45:24,130
working with this. Uh, the second
thing is that I didn't talk,

753
00:45:24,270 --> 00:45:26,940
I don't talk about the third category,
the assistive tech.

754
00:45:27,150 --> 00:45:32,150
I believe that the long history we have
of interaction design will fit us well

755
00:45:32,810 --> 00:45:34,050
to assist of technology,

756
00:45:34,470 --> 00:45:38,670
but I'm also quite concerned that that
becomes a human crutch for cognition as

757
00:45:38,671 --> 00:45:40,410
opposed to equipment that makes us better.

758
00:45:41,310 --> 00:45:46,310
I don't also talk about the flow between
agented and a system and automatic and

759
00:45:47,431 --> 00:45:48,300
this book,

760
00:45:48,750 --> 00:45:53,490
but it is something that if you begin
to incorporate agent of aspects of your

761
00:45:53,491 --> 00:45:53,821
product,

762
00:45:53,821 --> 00:45:58,530
and it's not purely agent of like the
Roomba that you have to work through.

763
00:45:58,531 --> 00:46:02,220
And I don't give any advice to that. I'm
working like with Vic and shines ladder.

764
00:46:02,221 --> 00:46:05,730
Right? Let's get this done first and
then we can build up those other skills.

765
00:46:06,780 --> 00:46:07,500
Thank you guys.

