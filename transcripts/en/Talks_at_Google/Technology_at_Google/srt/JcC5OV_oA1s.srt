1
00:00:06,170 --> 00:00:07,030
So,
uh,

2
00:00:07,220 --> 00:00:12,220
the book is called the sentient
machine and it really is,

3
00:00:12,750 --> 00:00:17,300
um, uh, varied book. You
know, it starts off with, um,

4
00:00:17,360 --> 00:00:22,360
some philosophical ponderings on what
the advent of Ai really means for us.

5
00:00:23,870 --> 00:00:25,490
There are,
as you know,

6
00:00:25,520 --> 00:00:30,520
some existential concerns regarding the
advent of more and more powerful Ai,

7
00:00:32,350 --> 00:00:33,380
uh,
Agi.

8
00:00:33,381 --> 00:00:38,381
And then Asi and lots of very worthy
scholars have written volumes,

9
00:00:38,890 --> 00:00:42,290
uh, about these, for example,
Nick Bostrom's superintelligence,

10
00:00:42,291 --> 00:00:45,110
which I'm sure many of you
have heard of if not red.

11
00:00:46,850 --> 00:00:50,760
And then beyond that, there's
also discussion around, uh,

12
00:00:50,810 --> 00:00:54,930
essentially these two fears that
keep sort of a rearing their head in,

13
00:00:54,980 --> 00:00:57,200
in different ways,

14
00:00:57,470 --> 00:01:02,470
but one that AI will take away all our
jobs and it might render us useless when

15
00:01:02,631 --> 00:01:05,960
it gets to a certain level
of complexity and capability.

16
00:01:06,770 --> 00:01:11,480
And the other is that it might kill us.
And of course that has many different,

17
00:01:11,700 --> 00:01:14,800
um,
aspects and,

18
00:01:14,830 --> 00:01:19,520
and situations under which that
fear manifests itself. But, uh,

19
00:01:19,610 --> 00:01:20,211
in a nutshell,

20
00:01:20,211 --> 00:01:24,710
those are two real conversations
that are happening these days.

21
00:01:25,460 --> 00:01:26,660
And for this audience,

22
00:01:26,661 --> 00:01:31,661
I'll also tell you that these are not
just hypothetical or philosophical

23
00:01:32,750 --> 00:01:34,700
quandaries.
And questions anymore.

24
00:01:35,120 --> 00:01:40,010
They are now being played out at the
highest levels of government. Uh,

25
00:01:40,011 --> 00:01:43,580
so spark cognition works
in three principle areas,

26
00:01:43,581 --> 00:01:47,540
national security industry
and Energy and finance.

27
00:01:48,020 --> 00:01:51,020
And as a concept, and I won't talk
much about our own work, this is,

28
00:01:51,080 --> 00:01:52,550
this is a talk about the book.

29
00:01:52,610 --> 00:01:56,890
This is not a talk about spark
cognition on my work per se. But, uh,

30
00:01:57,050 --> 00:01:58,520
because of that background,

31
00:01:58,521 --> 00:02:03,521
I ended up meeting with and having some
pretty interesting discussions with,

32
00:02:04,101 --> 00:02:06,350
uh,
the senior most military leadership,

33
00:02:06,351 --> 00:02:09,200
not just in this country
but also for example,

34
00:02:09,201 --> 00:02:12,680
in allied states in Europe
about two weeks ago,

35
00:02:12,681 --> 00:02:17,681
I addressed the NATO counsel on their
adapt Taishan a report that was about to

36
00:02:19,041 --> 00:02:21,680
come out and how,
believe it or not,

37
00:02:21,890 --> 00:02:26,780
AI will play a great role in the new
adaptation report that was just released

38
00:02:28,130 --> 00:02:31,040
based on the writings
that a general John Allen,

39
00:02:31,041 --> 00:02:35,620
who's my collaborator and also on
the spark cognition board and I, uh,

40
00:02:35,660 --> 00:02:36,980
published earlier this year.

41
00:02:37,520 --> 00:02:42,520
So that's just one example to say that
artificial intelligence is becoming real

42
00:02:43,430 --> 00:02:44,360
in many,

43
00:02:44,390 --> 00:02:48,530
many ways and perhaps in
narrow domains initially.

44
00:02:48,531 --> 00:02:50,600
But the capabilities are widening.

45
00:02:51,110 --> 00:02:56,090
And for some of these existential
concerns that people have expressed,

46
00:02:56,570 --> 00:03:00,010
um, for example, that
it will take jobs away.

47
00:03:00,100 --> 00:03:02,980
Realize that we don't need commander data,

48
00:03:03,310 --> 00:03:08,260
Agi level capability for those
sorts of things to be threats.

49
00:03:08,950 --> 00:03:13,950
Eni Capability implemented in many
different areas can lead to 30%,

50
00:03:15,461 --> 00:03:19,750
40%. God knows what percent
unemployment remains to be seen.

51
00:03:20,860 --> 00:03:24,700
And that's for developed countries.
On the other hand,

52
00:03:24,701 --> 00:03:27,520
for undeveloped countries
or developing countries,

53
00:03:27,910 --> 00:03:31,210
they have invested a lot in there,

54
00:03:31,510 --> 00:03:35,530
a burgeoning what they call their
demographic dividend. You know,

55
00:03:35,531 --> 00:03:40,270
people that have been brought
out from, uh, conditions of, uh,

56
00:03:40,480 --> 00:03:42,520
sort of, uh, you know,

57
00:03:43,060 --> 00:03:48,060
underprivileged and are now being educated
and are being made available to do

58
00:03:48,730 --> 00:03:51,310
complex tasks in the economy.
Well,

59
00:03:51,311 --> 00:03:56,290
some of those complex tasks might be
subsumed before those generations get an

60
00:03:56,291 --> 00:04:00,070
opportunity to really make
a mark. So there's that, uh,

61
00:04:00,460 --> 00:04:04,350
emerging sense of the fallacy of the,
uh,

62
00:04:04,420 --> 00:04:08,200
burgeoning middle class in developing
countries and whether they'll be able to

63
00:04:08,201 --> 00:04:11,650
play the role that we once thought
they would be able to play.

64
00:04:12,550 --> 00:04:15,910
We don't know. We'll see. At what
rate these technologies progress.

65
00:04:15,911 --> 00:04:18,550
But my point here is that all ready,

66
00:04:19,060 --> 00:04:24,060
we are at a point where the discussion
around artificial intelligence is partly

67
00:04:24,881 --> 00:04:29,650
technology, but it's
also partly policy. And,

68
00:04:29,740 --> 00:04:34,720
uh, in my own case, I've tried
to bring these things together.

69
00:04:34,721 --> 00:04:37,810
And in the book you see, uh, the science,

70
00:04:38,170 --> 00:04:43,000
the philosophy as well as elements of
policy because ultimately we have to do

71
00:04:43,001 --> 00:04:47,920
something about this. Um, and I'll tell
you later on as we get into this, uh,

72
00:04:48,160 --> 00:04:52,000
for example, some of the discussions
around bands and autonomous weapons,

73
00:04:52,001 --> 00:04:56,980
I've been quite deeply involved in all
of those debates and have, um, met with,

74
00:04:57,020 --> 00:05:01,210
uh, a lot of folks that really do
matter in that, in that debate.

75
00:05:01,570 --> 00:05:04,150
So we'll, we'll go through that. Um,

76
00:05:04,210 --> 00:05:08,560
the way I would structure the talk is
I'll start off with a very brief reading.

77
00:05:08,890 --> 00:05:11,230
Uh,
and you know,

78
00:05:11,231 --> 00:05:14,530
the beginning of all of these sessions
ends up being different just based on

79
00:05:14,531 --> 00:05:18,160
what chapter you choose. So here we
were talking about autonomous weapons.

80
00:05:18,161 --> 00:05:22,570
Maybe I'll start with, uh, the beginning
of a chapter called warfare in Ai.

81
00:05:23,010 --> 00:05:27,130
Um, and then we'll talk about more
broadly some of the content in the book,

82
00:05:27,400 --> 00:05:31,030
but I've also structured a presentation
that takes us a little bit beyond the

83
00:05:31,031 --> 00:05:31,450
book.

84
00:05:31,450 --> 00:05:35,920
There are some concepts here which for
accomplished computer scientists that are

85
00:05:35,921 --> 00:05:39,460
in the audience might seem
to be very basic and you,

86
00:05:39,490 --> 00:05:43,390
you may be familiar with them and I'll
go over those quickly if they become

87
00:05:43,420 --> 00:05:47,520
boring, but then we can get into
some of the other issues. Um,

88
00:05:47,860 --> 00:05:50,380
and some of the problems that
I think we still need to solve.

89
00:05:50,560 --> 00:05:52,210
So that's how I'll go.

90
00:05:52,211 --> 00:05:55,120
And of course I'm open to
questions at any time and comments.

91
00:05:55,121 --> 00:05:57,440
I would welcome that.
Okay.

92
00:05:58,040 --> 00:06:01,490
So for those of you who do
have the book on page 87,

93
00:06:01,700 --> 00:06:05,060
I'll start with a very brief
reading of this chapter

94
00:06:06,650 --> 00:06:09,020
is titled Warfare and Ai.

95
00:06:11,180 --> 00:06:16,180
Join me for the thought experiment
originally published in the U S Naval

96
00:06:17,240 --> 00:06:20,870
Institutes Proceedings
Journal and conceived off by
my friend and collaborator

97
00:06:21,050 --> 00:06:24,350
General John Allen of the
United States Marine Corps,

98
00:06:25,100 --> 00:06:29,930
a four star general and pass deputy
commander of US Central Command.

99
00:06:30,980 --> 00:06:35,980
It is January two 2018 and a captain is
contemplating damage to his ship after a

100
00:06:37,641 --> 00:06:42,020
surprise attack. This however,
was no ordinary attack.

101
00:06:42,530 --> 00:06:47,510
He is about to discover that this was a
massive widespread strategic surprise.

102
00:06:48,620 --> 00:06:53,620
Our captain and his crew
had not anticipated the
incoming swarm because neither

103
00:06:53,631 --> 00:06:58,631
he nor his ship recognize that their
systems were under cyber attack.

104
00:07:00,350 --> 00:07:04,550
The undetected cyber activity
not only compromise the sensors,

105
00:07:04,760 --> 00:07:09,760
but locked out defensive systems leaving
the ship almost entirely helpless.

106
00:07:11,210 --> 00:07:16,210
The kinetic strikes came in waves as a
complex swarm of drones door into the

107
00:07:17,001 --> 00:07:17,834
ship.

108
00:07:18,260 --> 00:07:23,090
It was attacked by a cloud of autonomous
systems moving together with purpose,

109
00:07:23,900 --> 00:07:28,900
yet also dynamically reacting
to one another and to the ship.

110
00:07:31,790 --> 00:07:36,170
More than anything, the speed of
the attacks stunned and overwhelmed.

111
00:07:36,171 --> 00:07:39,920
The sailors though the ID
specialists on board the ship,

112
00:07:39,921 --> 00:07:43,880
we're able to release some defensive
systems from the clutches of the cyber

113
00:07:43,881 --> 00:07:44,714
intrusion.

114
00:07:44,780 --> 00:07:49,780
The rest of the crew simply did not have
enough decision making time to react

115
00:07:50,660 --> 00:07:54,500
mere seconds.
And in these few seconds,

116
00:07:54,590 --> 00:07:59,030
some of the sailors ascertained with
their limited situational awareness that

117
00:07:59,031 --> 00:08:03,860
the enemy's autonomous cyber and
kinetic systems, we're collaborating.

118
00:08:05,000 --> 00:08:08,480
But in a matter of minutes,
the entire attack was over.

119
00:08:09,410 --> 00:08:13,220
The captain survived and
courageously remained on the bridge,

120
00:08:13,910 --> 00:08:15,960
but he was badly wounded.
As we're,

121
00:08:15,990 --> 00:08:20,990
as much of his crew fires were burning
out of control and the ship was already

122
00:08:21,471 --> 00:08:26,471
listing badly from flooding because of
the damage the captain was unable to

123
00:08:26,931 --> 00:08:31,880
communicate with the damage control
assistant who was herself badly wounded.

124
00:08:32,720 --> 00:08:37,160
It appeared that some of the autonomous
platforms knew exactly where to strike

125
00:08:37,161 --> 00:08:37,910
the ship,

126
00:08:37,910 --> 00:08:42,680
both to maximize the damage and
reduce the chances of survivability.

127
00:08:43,550 --> 00:08:48,050
The captain's ability to command his
ship was now badly compromised and the

128
00:08:48,051 --> 00:08:50,300
flooding was out of control.

129
00:08:51,290 --> 00:08:53,750
After surveying the entire situation,

130
00:08:53,870 --> 00:08:58,870
he realizes he must make a call that
no American skipper has made for

131
00:09:00,631 --> 00:09:01,740
generations.

132
00:09:02,520 --> 00:09:05,970
He issues the order to abandon ship.

133
00:09:08,190 --> 00:09:09,023
Okay,

134
00:09:11,180 --> 00:09:13,780
so going through this,
um,

135
00:09:14,180 --> 00:09:18,230
you might wonder if this is a fiction.
It is.

136
00:09:19,230 --> 00:09:23,840
You might wonder if this is entirely
imagined fiction with no grounding in

137
00:09:23,841 --> 00:09:28,540
truth. It is art. Um, it
was very interesting, uh,

138
00:09:28,550 --> 00:09:32,630
this past year I had the opportunity
to travel through the Middle East.

139
00:09:32,750 --> 00:09:33,590
And as you know,

140
00:09:33,970 --> 00:09:38,930
there are many numerous active conflicts
going on in the Middle East and many of

141
00:09:38,931 --> 00:09:42,710
them are conflicts that are
taking an asymmetric, uh,

142
00:09:43,460 --> 00:09:44,770
sort of, uh, uh,

143
00:09:45,020 --> 00:09:50,020
tilt where you've got somebody like the
who theories as an example in Yemen or

144
00:09:50,991 --> 00:09:55,991
you've got isis terror organizations and
Syria and some parts of Iraq and you've

145
00:09:58,101 --> 00:10:00,830
got then on the other
side for the most part,

146
00:10:00,831 --> 00:10:05,831
well armed military forces
with sophisticated radars
and Patriot missiles and so

147
00:10:06,741 --> 00:10:10,710
on and so forth. And um, let
me just tell you that, uh,

148
00:10:10,780 --> 00:10:15,710
I know for a fact that much
of what is described here,

149
00:10:15,711 --> 00:10:19,040
maybe not with this level of nation
states sophistication, you know,

150
00:10:19,041 --> 00:10:24,041
because here what we described is a swarm
of a improvised you calves coming in

151
00:10:25,550 --> 00:10:30,020
with pretty sophisticated vision and
other recognition capabilities going

152
00:10:30,021 --> 00:10:31,460
forward.
Well-Protected asset,

153
00:10:31,940 --> 00:10:36,940
but similar scenarios have actually
played out in the current conflict in the

154
00:10:37,850 --> 00:10:38,510
Middle East.

155
00:10:38,510 --> 00:10:43,510
The DIY AI improvised
flying IED is already here

156
00:10:49,330 --> 00:10:50,850
two weeks ago.
Uh,

157
00:10:50,860 --> 00:10:54,850
the Convention for Conventional Weapons,

158
00:10:54,851 --> 00:10:56,890
which is a little awkwardly named,

159
00:10:56,891 --> 00:11:01,891
but CCW at the UN got together I think
for the third or fourth time over three

160
00:11:03,851 --> 00:11:04,684
or four years.

161
00:11:05,260 --> 00:11:09,970
And they're 107 member states
of this organization and the UN.

162
00:11:10,750 --> 00:11:15,750
And every year they've gotten together
and expressed their dire concern over the

163
00:11:17,351 --> 00:11:21,640
potential spread of autonomous weapons
and what they must do. And of course,

164
00:11:21,641 --> 00:11:25,330
you may remember the famous what's
called the Elan Musk letter,

165
00:11:25,420 --> 00:11:27,790
which was really not Elon Musk's letter.

166
00:11:27,791 --> 00:11:32,380
It was the most recent one was
written by Professor Toby Walsh, uh,

167
00:11:32,381 --> 00:11:36,430
from the University of New
South Wales in Australia.

168
00:11:36,431 --> 00:11:40,120
He's an AI professor there. And of
course, Elon Musk was a signatory,

169
00:11:40,480 --> 00:11:45,310
and then the letter was presented
as a desire or a request for a band,

170
00:11:45,311 --> 00:11:48,490
which it was not.
It was a request for a discussion.

171
00:11:49,530 --> 00:11:51,550
And at the end of this
most recent session,

172
00:11:51,551 --> 00:11:56,551
107 countries even get together after
four years of debate and agree on what the

173
00:11:57,251 --> 00:12:01,930
definition of an autonomous weapon is.
In the meanwhile,

174
00:12:01,931 --> 00:12:05,020
the clashing cove bureau,
which many of you may have heard of,

175
00:12:05,021 --> 00:12:10,021
which is the Russian weapons manufacturer
announced that they were testing a UGV

176
00:12:10,631 --> 00:12:12,730
and unmanned ground vehicle,

177
00:12:13,000 --> 00:12:17,890
which in field tests had already
shown better than human performance.

178
00:12:18,580 --> 00:12:23,230
Now, you may doubt these claims.
You may think these are oversold,

179
00:12:23,590 --> 00:12:27,040
but wait a year or two.
Similarly,

180
00:12:27,041 --> 00:12:32,041
China announced that they were fielding
AI powered cruise missiles and the Mig

181
00:12:33,011 --> 00:12:38,011
bureau announced that the new next gen
meg aircraft would have AI autopilot

182
00:12:39,041 --> 00:12:43,960
operating, uh, to control the flight
envelope at hypersonic speeds.

183
00:12:44,620 --> 00:12:48,220
So this is just one vignette.
This is just one side of Ai,

184
00:12:48,221 --> 00:12:53,221
which is it is a technology that brings
or distributed autonomy at large scale

185
00:12:55,481 --> 00:12:56,620
to the field of battle.

186
00:12:57,280 --> 00:13:01,390
It is a strategic level up,
if you will,

187
00:13:01,600 --> 00:13:05,590
in terms of capability
and no significant player,

188
00:13:05,650 --> 00:13:10,080
no significant military is
going to ignore this. Uh,

189
00:13:10,300 --> 00:13:12,730
and just to give you
further evidence of this,

190
00:13:12,790 --> 00:13:17,290
of the hundred and seven countries
that were at the CC w session,

191
00:13:18,370 --> 00:13:22,570
there were only 22 that came out and said,
uh,

192
00:13:22,600 --> 00:13:27,550
we are in favor of a ban
of all the nuclear states.

193
00:13:27,551 --> 00:13:32,551
There was only one which supported the
band and of all significant militaries in

194
00:13:32,561 --> 00:13:35,260
the world, all states with
significant militaries in the world,

195
00:13:35,380 --> 00:13:39,640
only to supported the band,
the largest militaries,

196
00:13:39,880 --> 00:13:44,680
the countries with the largest number of
nuclear weapons all argued for further

197
00:13:44,681 --> 00:13:46,780
discussion.
Let's push this off to next year.

198
00:13:47,710 --> 00:13:52,530
So that's where we are. That's just
one vignette. But with that, uh,

199
00:13:52,690 --> 00:13:56,290
let me start talking about sort
of some of the things that we,

200
00:13:57,010 --> 00:13:58,090
that we cover in the book.

201
00:13:58,960 --> 00:14:03,960
So what's quite clear to me now is that
we've made enough progress in several

202
00:14:04,421 --> 00:14:09,130
areas where a new form of
intelligence really is coming. I mean,

203
00:14:09,131 --> 00:14:13,330
it's no longer sort of the
wizard of, uh, the wizard of Oz,

204
00:14:13,540 --> 00:14:15,340
a man hiding behind the curtain.

205
00:14:15,580 --> 00:14:19,690
It's no longer just large numbers
of if then else statements.

206
00:14:20,000 --> 00:14:24,400
And while we keep uncovering every
now and then company x and company y

207
00:14:24,401 --> 00:14:27,780
outsourcing some activity that
they call intelligent Petrolia,

208
00:14:27,790 --> 00:14:31,270
it's going out to uh, you know, the uh,

209
00:14:31,271 --> 00:14:34,240
Amazon Mechanical Turk type situation.

210
00:14:34,750 --> 00:14:36,910
Really aside from all of those things,

211
00:14:37,060 --> 00:14:41,320
intelligence in an increasing level
of intelligence is being built.

212
00:14:41,321 --> 00:14:44,080
And we've had,
with deep learning in particular,

213
00:14:44,081 --> 00:14:48,820
we've had great impact on perception
tasks. You know, where we want to,

214
00:14:48,821 --> 00:14:50,140
for example,
classify,

215
00:14:50,141 --> 00:14:55,141
we want to perceive something and extract
complex patterns and even patterns

216
00:14:55,701 --> 00:14:57,470
across the moral boundaries.

217
00:14:57,471 --> 00:15:01,730
We've been able to do that very well
with deep learning and now we're sort of

218
00:15:01,790 --> 00:15:06,770
running forward with reinforcement
learning with lots of new innovations.

219
00:15:06,771 --> 00:15:08,450
And, uh, the, the,

220
00:15:08,451 --> 00:15:12,050
the key thing to take away there is where
moving from the domain of perception

221
00:15:12,470 --> 00:15:16,550
to the domain of action and even
within reinforcement learning,

222
00:15:16,551 --> 00:15:19,250
now we have the ability to um,

223
00:15:19,400 --> 00:15:23,710
train systems up maybe in simulated
environments and with some of the break

224
00:15:23,711 --> 00:15:28,670
break break throughs that are taking place
and transfer learning. We take the, uh,

225
00:15:28,820 --> 00:15:32,900
learning that's done in a simulator and,
and translate that to the real world.

226
00:15:33,110 --> 00:15:38,110
So this is not a talk about the current
state of the art in all three of these

227
00:15:39,351 --> 00:15:39,831
areas,

228
00:15:39,831 --> 00:15:44,831
but just a couple of minutes to say that
this is becoming quite real and indeed

229
00:15:45,410 --> 00:15:49,670
a new form of intelligence
if not sanctions is coming.

230
00:15:49,671 --> 00:15:54,170
Sanctions I think is far
away. Um, so with this,

231
00:15:54,620 --> 00:15:58,710
just a quick background, uh,
one of the things that you know,

232
00:15:58,960 --> 00:16:03,960
I think I do differently in the book
is simply a consequence of my own

233
00:16:05,061 --> 00:16:05,661
background.

234
00:16:05,661 --> 00:16:09,800
So I'm a serial entrepreneur and I've
been based in Austin and I went to school

235
00:16:09,801 --> 00:16:14,240
at ut Austin computer science and have
done a number of software companies since

236
00:16:14,241 --> 00:16:16,650
then and spark cognition a,

237
00:16:16,690 --> 00:16:21,650
it was a company I founded back
in mid 2013 the company focuses,

238
00:16:21,651 --> 00:16:25,940
as I mentioned on national security
finance in industry. In fact,

239
00:16:25,941 --> 00:16:28,930
we are a Google partner on a
number of different things and,

240
00:16:29,110 --> 00:16:31,340
and the company's grown
really, really fast. In fact,

241
00:16:31,341 --> 00:16:35,000
it's the fastest growing company in
Austin. Now with that being said,

242
00:16:35,001 --> 00:16:36,680
that's the business side of things,
right?

243
00:16:36,710 --> 00:16:40,730
How you actually take this technology
and make it work and make it solve

244
00:16:40,731 --> 00:16:42,740
problems for the largest
companies in the world.

245
00:16:43,430 --> 00:16:47,600
But the other side of this is that I
come to this not just purely from a

246
00:16:47,601 --> 00:16:50,510
business background.
I am a computer scientist by training.

247
00:16:50,780 --> 00:16:54,590
I love computer science. I
love computer science. And, uh,

248
00:16:54,591 --> 00:16:58,850
I serve on the board of advisors of ut CS,
which is one of the great,

249
00:16:59,230 --> 00:17:03,830
uh, really pleasures, um, when I'm,
when I'm able to spend much time there.

250
00:17:04,940 --> 00:17:07,070
So it brings sort of the business aspect,

251
00:17:07,071 --> 00:17:11,750
the practicality of making these things
work with the science and attempting to

252
00:17:11,751 --> 00:17:14,030
advance the science.
And then finally,

253
00:17:14,090 --> 00:17:19,090
the center of new American security is
one of the premiere think tanks in DC.

254
00:17:20,600 --> 00:17:25,160
And I serve on their advisory
board for artificial intelligence.

255
00:17:25,190 --> 00:17:28,770
In fact, about two or three
weeks ago, I was in DC. We were,

256
00:17:28,800 --> 00:17:33,290
we had a scene as conference on AI and
what this would mean for autonomous

257
00:17:33,291 --> 00:17:37,660
weapons. And there were lots of generals
and a serving and otherwise and,

258
00:17:37,700 --> 00:17:40,730
and many, uh, policy
policymakers in the audience.

259
00:17:41,000 --> 00:17:45,980
But we also had Eric Schmidt there. And
a, I had an interesting discussion with a,

260
00:17:45,981 --> 00:17:50,340
with Eric and one of the topics
that came up was, well, uh,

261
00:17:50,700 --> 00:17:55,350
how long given the China just
announced their 20, 30 AI plan,

262
00:17:55,351 --> 00:17:57,060
which many of you may have seen,

263
00:17:57,900 --> 00:18:02,280
it's an investment of $150
billion of government spending.

264
00:18:02,760 --> 00:18:05,970
Uh, over the next five years. In 2015,

265
00:18:05,971 --> 00:18:10,770
the u s government spent one point
$1 billion on AI. And in 2016,

266
00:18:10,771 --> 00:18:15,750
we spent a whopping one
point $2 billion on AI.

267
00:18:16,650 --> 00:18:20,550
Again, the Chinese government has
committed just governmental spending,

268
00:18:20,610 --> 00:18:23,460
$150 billion over five years.

269
00:18:23,790 --> 00:18:27,660
And if you read the 2030 AI report,
it says by 2030,

270
00:18:27,661 --> 00:18:30,720
we will be the dominant AI player.

271
00:18:31,800 --> 00:18:35,100
In addition to that, they also talk
about all the applications of Ai.

272
00:18:35,101 --> 00:18:37,930
And a big chunk of that
is military application.

273
00:18:37,931 --> 00:18:42,780
So here we were in DC at this conference
and I asked Eric, I said, Eric,

274
00:18:42,830 --> 00:18:45,630
um, you know, I have a
view, but what's your view?

275
00:18:45,631 --> 00:18:50,510
How soon do you think China will be
able to overtake the U S in Korea?

276
00:18:50,511 --> 00:18:51,344
I capability?

277
00:18:52,920 --> 00:18:56,010
And it's on video and then they will a
lot a lot of articles written about it,

278
00:18:56,011 --> 00:18:58,920
but he said five years,
I don't disagree with them,

279
00:18:59,370 --> 00:19:01,440
it sounds very aggressive,

280
00:19:02,070 --> 00:19:06,720
but the rate at which progress is being
made, the rate at which, um, you know,

281
00:19:06,721 --> 00:19:09,810
just if you look at face plus plus send
the rate at which they're improving

282
00:19:09,811 --> 00:19:14,070
their vision algorithms. And a few years
ago I remember people used decide, well,

283
00:19:14,071 --> 00:19:15,720
you know,
AI papers and China show,

284
00:19:15,721 --> 00:19:18,870
they are publishing a lot of AI papers
but they're not as good as good as us.

285
00:19:19,830 --> 00:19:23,550
Well now that's really
not the case anymore.

286
00:19:23,551 --> 00:19:28,160
And it sort of reminds
me of the whole, um, the,

287
00:19:28,161 --> 00:19:32,190
the reaction that we've had too many other
countries that have been catching up,

288
00:19:33,030 --> 00:19:37,630
it probably just copied it. Oh, it's kind
of like a fake, uh, you know, sort of,

289
00:19:37,640 --> 00:19:41,100
kind of there, but it isn't the
same thing. And then suddenly,

290
00:19:41,101 --> 00:19:44,580
very quickly you realize that,
that folks are catching up.

291
00:19:44,610 --> 00:19:49,610
And where we are in our current situation
as a country is that we're doing

292
00:19:50,971 --> 00:19:55,971
things like preventing the spouses
of h one B immigrants from working,

293
00:19:56,311 --> 00:19:59,670
which means that fewer smart
people will be able to come in. Uh,

294
00:19:59,671 --> 00:20:03,510
we're trying to ban entry from
a number of countries and uh,

295
00:20:03,540 --> 00:20:08,460
limiting the number of smart people that
we'll be able to bring into the u s so

296
00:20:08,461 --> 00:20:13,461
while you have a near peer competitor
putting a 150 billion to your 1.2 billion,

297
00:20:14,850 --> 00:20:17,760
you're also then strangling,
uh,

298
00:20:17,790 --> 00:20:22,790
some of the core elements of a innovation
that have historically been so useful.

299
00:20:24,480 --> 00:20:28,380
A useful for you.
That I think is a bad timing.

300
00:20:30,630 --> 00:20:34,410
So moving onto another element
in a lot of these stocks,

301
00:20:34,411 --> 00:20:36,870
people ask just the simple question of,
well,

302
00:20:36,871 --> 00:20:38,580
what does it mean for a machine to think?

303
00:20:38,610 --> 00:20:41,460
This is obviously a very complex
question and there's many,

304
00:20:41,461 --> 00:20:43,680
many different ways in
which a machine can think.

305
00:20:43,980 --> 00:20:47,100
And we tend to describe things
in the context of a neural,

306
00:20:47,590 --> 00:20:50,020
where you take a neural network,
you give it a lot of data,

307
00:20:50,021 --> 00:20:53,710
and then you can ask it a question
about what you've trained it on.

308
00:20:53,711 --> 00:20:56,620
And it leads to classify
or in a regression sense,

309
00:20:56,621 --> 00:21:00,150
give you some sort of an
answer. But I figured that
from a visual perspective, no,

310
00:21:00,160 --> 00:21:03,790
there's many ways machines can think
and AI isn't just machine learning.

311
00:21:03,791 --> 00:21:07,690
There's many other things in
AI as well. Uh, for example,

312
00:21:07,760 --> 00:21:10,030
a search based optimization.
So here,

313
00:21:10,031 --> 00:21:15,031
one way that you can think in a problem
domain is let's just take the simplest

314
00:21:15,041 --> 00:21:18,880
example of tictactoe. Uh,
given just a couple of rules.

315
00:21:19,030 --> 00:21:24,030
You can pre generate all the possible
outcomes and then what is perceived by the

316
00:21:25,841 --> 00:21:30,841
human player to be a smart move is simply
a goal seeking behavior where I know

317
00:21:32,711 --> 00:21:36,430
what a win looks like and I've
generated the tree or the graph.

318
00:21:36,610 --> 00:21:40,390
And I'm trying to traverse the graph to
find the most efficient path to what I

319
00:21:40,391 --> 00:21:41,980
know to be a win.

320
00:21:42,760 --> 00:21:47,740
And that's one way in which you can
make machines, uh, appear to think.

321
00:21:48,130 --> 00:21:53,020
But we also know that not every
problem has such a small state space.

322
00:21:53,200 --> 00:21:54,640
So there are problems.

323
00:21:54,641 --> 00:21:59,350
I mean even games taken other game
like Batman where uh, you know,

324
00:21:59,440 --> 00:22:04,440
whereas miss pacman quite at this moment
and how many of the golden nuggets have

325
00:22:05,081 --> 00:22:08,080
been consumed and what's the
direction of each one of the photos?

326
00:22:08,081 --> 00:22:10,720
And did you eat the berry or not?
I mean,

327
00:22:10,721 --> 00:22:14,410
there's a lot of variability
in that state space.

328
00:22:14,411 --> 00:22:17,980
So that's the kind of thing that you
wouldn't want to just encode in this way.

329
00:22:19,060 --> 00:22:24,060
So then now we try to do things like a
reinforcement learning where this start

330
00:22:24,281 --> 00:22:28,090
off and start playing the game and you
end up and you've die pretty quick,

331
00:22:28,091 --> 00:22:32,350
but maybe you made 50 points and what
you are trying to remember is what

332
00:22:32,351 --> 00:22:35,170
sequence of tasks got
you to those 50 points.

333
00:22:35,590 --> 00:22:40,090
The first ones that you took are pretty
much worth the 50 points because they

334
00:22:40,091 --> 00:22:42,220
lead to you getting the 50 points.

335
00:22:42,430 --> 00:22:45,850
But as you move further
along in that stack of moves,

336
00:22:46,030 --> 00:22:51,030
you realize that the closer you
get to 50 the less valuable.

337
00:22:51,850 --> 00:22:55,520
Each one of those more recent
moves were because goddammit,

338
00:22:55,600 --> 00:22:58,390
the last one got you killed.
So that can't be very valuable.

339
00:22:58,840 --> 00:23:03,760
And so you can have this sense of, well
let me feel, and there's again many,

340
00:23:03,761 --> 00:23:05,800
many different approaches to this,

341
00:23:05,801 --> 00:23:09,190
but you can have an element
of randomness to where,

342
00:23:09,430 --> 00:23:11,620
let me try and get the maximum reward.

343
00:23:11,620 --> 00:23:14,470
But when the level of reward
goes below a certain threshold,

344
00:23:14,471 --> 00:23:17,860
I'm going to try different things and
maybe I find something that's more

345
00:23:17,861 --> 00:23:22,450
interesting. So this is sort of
like a self pruned search, you know,

346
00:23:22,451 --> 00:23:25,390
in reinforcement learning
and just layman's terms.

347
00:23:25,480 --> 00:23:29,440
It's sort of like a self pruned search
where you start off with something and

348
00:23:29,441 --> 00:23:32,710
you don't abandon that, but you just
look for improvements where you can,

349
00:23:32,711 --> 00:23:37,060
you can find them.
And we've seen great progress with this.

350
00:23:37,480 --> 00:23:41,500
Now another thing that I will
point out is that even in this, uh,

351
00:23:41,560 --> 00:23:44,830
idea where you are generating
entire states, by the way,

352
00:23:45,920 --> 00:23:48,800
stuff that's not fashionable
people stop thinking about.

353
00:23:48,801 --> 00:23:53,801
But there's a lot of problems that can
be smartly pruned to where these sorts of

354
00:23:56,331 --> 00:23:58,220
solutions are still pretty good solutions.

355
00:23:58,221 --> 00:24:01,370
I mean you don't hear
a star search much now,

356
00:24:01,550 --> 00:24:05,870
but if you apply data properly to a
star search and you come up with clever

357
00:24:05,871 --> 00:24:09,950
heuristics on how to prune what gets
generated and what gets searched,

358
00:24:10,190 --> 00:24:13,940
there's a lot of problems that you can
solve pretty cleverly with a star search.

359
00:24:14,270 --> 00:24:17,990
But anyway, uh, in this particular
case where you see the full tree,

360
00:24:18,020 --> 00:24:23,020
one thing to note is that generating
the states sometimes can be incredibly

361
00:24:23,661 --> 00:24:28,010
simple. Can, that's one concept
that I'll build on here. So here,

362
00:24:28,011 --> 00:24:33,011
what you had to know to generate this
whole tree is that for every progression

363
00:24:33,350 --> 00:24:37,100
you can only change or add
one symbol at a time, right?

364
00:24:37,101 --> 00:24:40,730
So if you've got knots and crosses,
either it can be, uh, you know,

365
00:24:40,731 --> 00:24:44,570
you can add one not or one cross,
you can't add two knots in one go.

366
00:24:45,260 --> 00:24:48,650
Um, you need to know what the
winning state is and that sort of,

367
00:24:48,651 --> 00:24:53,120
we all know a diagonal or line or
a horizontal bar. And then with,

368
00:24:53,150 --> 00:24:56,640
with every step here, as you
step through the tree, um,

369
00:24:56,660 --> 00:24:59,480
in every layer you are
alternating symbols.

370
00:24:59,630 --> 00:25:04,220
So first you get a knot, then you get
across, then you get a knot and so on.

371
00:25:05,630 --> 00:25:06,463
That's all.

372
00:25:06,560 --> 00:25:11,180
That's all that you need to know in
order to generate something like this.

373
00:25:11,570 --> 00:25:13,670
And so, uh, is that, you know,

374
00:25:13,700 --> 00:25:18,140
sort of this mind altering fact in
the context of knots and crosses?

375
00:25:18,350 --> 00:25:21,980
Not really,
but it does go to say that very,

376
00:25:21,981 --> 00:25:26,981
very simple things when
iterated upon when,

377
00:25:27,620 --> 00:25:28,820
uh, when, uh,

378
00:25:28,821 --> 00:25:33,821
dealt with with recursion can create
tremendous complexity that can be useful.

379
00:25:35,020 --> 00:25:39,590
Okay.
So a seed of specification can,

380
00:25:39,710 --> 00:25:44,480
can create something that
is very, very large, very,

381
00:25:44,481 --> 00:25:46,940
very useful and sometimes
a little unexpected.

382
00:25:47,150 --> 00:25:49,970
These are concepts that at
least in two different places,

383
00:25:49,971 --> 00:25:54,290
and we know we talk about the game of
life briefly in the sentient machine,

384
00:25:54,620 --> 00:25:59,540
but, uh, what I cover is just
the basic introduction. Uh,

385
00:25:59,570 --> 00:26:00,770
Stephen Wolfram,

386
00:26:00,860 --> 00:26:05,270
who's the creator of Mathematica and
somebody that I followed for many years,

387
00:26:05,690 --> 00:26:09,620
very interesting thinker in his book,
a new kind of science.

388
00:26:09,830 --> 00:26:11,390
He spends,
you know,

389
00:26:11,510 --> 00:26:15,860
almost 200 pages just
going over different forms,

390
00:26:15,861 --> 00:26:19,380
different variations of the game
of life. And these, again, if you,

391
00:26:19,640 --> 00:26:24,320
how many of you are familiar with the
game of life? All of you. Okay. Almost.

392
00:26:24,700 --> 00:26:28,040
Uh, and so again, these are
really, really very simple rules.

393
00:26:28,041 --> 00:26:33,041
And what from shows is that you can have
these incredible levels of complex non

394
00:26:34,491 --> 00:26:38,210
repeating patterns that come from very,
very basic rules.

395
00:26:39,810 --> 00:26:40,643
Uh,

396
00:26:40,870 --> 00:26:45,870
and other sort of more in lines of sort
of a continuous mathematics is this

397
00:26:47,730 --> 00:26:51,300
notion of, of fractals. And there's two
things that I wanted to quickly say.

398
00:26:51,500 --> 00:26:55,020
So one, uh, since you are
familiar with the game of life,

399
00:26:55,021 --> 00:26:57,570
I'm sure you've seen simulations
like this, but to me,

400
00:26:57,571 --> 00:27:01,340
every time I see stuff like this,
I'm amazed, you know, it's a,

401
00:27:01,890 --> 00:27:05,900
there's these creatures would
like distinct behavior that,

402
00:27:05,920 --> 00:27:08,160
that evolve every time.
And they,

403
00:27:08,190 --> 00:27:12,990
some of them find stability and
others oscillate between two states.

404
00:27:13,230 --> 00:27:16,520
And then you have some movement.
You have this, uh, you know,

405
00:27:16,560 --> 00:27:20,700
artifact called a glider that just
sort of walks across diagonally.

406
00:27:20,701 --> 00:27:25,350
Usually you have these
blobs that can combine.

407
00:27:25,351 --> 00:27:29,850
And then what comes out of
that is at least not visually,
immediately predictive.

408
00:27:29,851 --> 00:27:34,290
And these can be very complex sort of
behaviors. And looking at that, you know,

409
00:27:34,310 --> 00:27:36,780
does seem to be like there's
something going on here.

410
00:27:36,781 --> 00:27:40,800
And of course we know what's
going on here is just very simple.

411
00:27:40,801 --> 00:27:41,940
Three simple rules.

412
00:27:42,450 --> 00:27:47,450
But the manifest complexity is far
more than those three simple rules.

413
00:27:49,770 --> 00:27:53,790
Uh, an initial reading of those
three simple rules would imply

414
00:27:56,340 --> 00:28:01,260
the same as the case really when you
start thinking about fractals because, uh,

415
00:28:01,261 --> 00:28:05,820
I mean this one is the Mandelbrot fractal
and Belvoir Mandelbrot came up with an

416
00:28:05,821 --> 00:28:10,530
expression, which is Yay long. I
mean, that's it, right? That's the,

417
00:28:10,860 --> 00:28:15,540
the amount of math that's being
generated into the structure.

418
00:28:15,900 --> 00:28:18,720
And the reason why we're going
into this is that a, I mean, again,

419
00:28:18,990 --> 00:28:23,580
for those of you who've read Ender's game,
the book and not the movie,

420
00:28:23,970 --> 00:28:26,280
uh, you'll, you'll,

421
00:28:26,520 --> 00:28:31,520
you'll realize or recognize or remember
that there's one comment in there that

422
00:28:31,591 --> 00:28:35,970
was made where ender was at this training
facility and he was given access to a

423
00:28:35,971 --> 00:28:39,450
large computer and somebody pulls him
away and says, what are you doing?

424
00:28:39,451 --> 00:28:41,700
And he says,
I'm traveling through the fractal.

425
00:28:41,730 --> 00:28:46,500
And now the fractal is larger
than the known universe.

426
00:28:47,460 --> 00:28:51,270
So when I read that as a teenager,
it sort of stuck in my head.

427
00:28:51,300 --> 00:28:55,440
I don't think that when the book was
written that computers had actually

428
00:28:55,441 --> 00:28:59,850
generated a fractal that was larger
than the known universe, but now, uh,

429
00:28:59,851 --> 00:29:03,960
that have many examples of this.
So to think that this much math,

430
00:29:04,170 --> 00:29:05,580
this much specification,

431
00:29:05,880 --> 00:29:10,880
this much code can generate something
with unending complexity that is a

432
00:29:13,960 --> 00:29:18,510
unpredictable actually, uh, and
unique at so many different levels.

433
00:29:18,850 --> 00:29:21,360
Um,
to me is pretty amazing.

434
00:29:21,361 --> 00:29:25,740
And the two ingredients of that of
course are the specification and the

435
00:29:25,741 --> 00:29:27,030
iteration and the recursion.

436
00:29:27,510 --> 00:29:31,230
And that gets me to one of the points
that I make in the book also about the

437
00:29:31,231 --> 00:29:35,990
universe being computable in a
different way. So you've heard, um,

438
00:29:36,810 --> 00:29:41,440
Elon Musk recently talk about how the
universe might be a simulator actually

439
00:29:41,441 --> 00:29:44,740
came across that concept of in my youth,

440
00:29:44,770 --> 00:29:49,570
a gentlemen by the name of Ed,
Fred Kin had written extensively on this.

441
00:29:49,571 --> 00:29:53,310
And then when I started digging
deeper, I realized that Conrad, sir,

442
00:29:53,680 --> 00:29:58,180
even back in the forties,
had talked about these concepts and add,

443
00:29:58,181 --> 00:30:01,570
Fred can, um, you know, this
article my father gave it to me.

444
00:30:01,720 --> 00:30:05,920
It was published in a magazine whose
God is the universe of computer.

445
00:30:06,640 --> 00:30:09,850
And the idea there was not
so much Elan's mosque idea,

446
00:30:09,851 --> 00:30:11,980
which is that we're all
living inside the simulation,

447
00:30:12,370 --> 00:30:16,840
which is one type of simulator inquiry.
But the other idea was,

448
00:30:16,841 --> 00:30:20,500
is the universe fundamentally computable?
Like everything we see,

449
00:30:20,501 --> 00:30:22,330
is it a consequence of computation?

450
00:30:24,880 --> 00:30:29,700
Many years later,
my sister became a string theorist and um,

451
00:30:30,250 --> 00:30:33,570
I tried to get, uh, at least, uh, uh,

452
00:30:33,600 --> 00:30:37,930
a workable understanding of string theory
and my many conversations with her.

453
00:30:38,230 --> 00:30:41,020
And one day she sort of lost a
patients with me and said, listen,

454
00:30:41,350 --> 00:30:44,860
all of what we write in,
in words in English,

455
00:30:45,160 --> 00:30:48,820
it's just a sort of 0.2
roughly in the right direction.

456
00:30:49,090 --> 00:30:51,700
If you want to understand any
of what we are really saying,

457
00:30:51,701 --> 00:30:56,500
you have to work through the math.
None of this really translates in, uh,

458
00:30:56,560 --> 00:30:57,730
in,
in language.

459
00:30:58,420 --> 00:31:02,200
And the one thing of
course from string theory,

460
00:31:02,530 --> 00:31:04,770
which is very interesting, is the, the,

461
00:31:04,780 --> 00:31:09,640
the rediscovery potentially of
what the Greeks called the atom,

462
00:31:09,670 --> 00:31:13,480
you know, ah, Tom, that
which cannot be cut.

463
00:31:13,870 --> 00:31:18,820
They were in search of that final
particle that was truly in divisible.

464
00:31:19,540 --> 00:31:23,290
And perhaps with the Planck length,
it's not so much the particle,

465
00:31:23,740 --> 00:31:28,090
but it's the fact that we know how
small a particle can be. We have,

466
00:31:28,360 --> 00:31:30,400
if the universe is Minecraft,

467
00:31:30,550 --> 00:31:35,380
we know the smallest size
of the Pixel of the block,

468
00:31:35,980 --> 00:31:36,251
right?

469
00:31:36,251 --> 00:31:41,200
Within which there can be nothing else
other than just one symbol contained.

470
00:31:41,560 --> 00:31:45,250
And what does that symbol, that symbol
can be a configuration of a string.

471
00:31:45,730 --> 00:31:49,150
So in that sense, I started
thinking, well, if that's the case,

472
00:31:49,360 --> 00:31:54,360
then you essentially can
model the universe as a data
structure that has these

473
00:31:56,201 --> 00:31:59,080
fixed size cells that are
planned class size cells,

474
00:31:59,350 --> 00:32:03,490
which have a number of
these, uh, symbols in them.

475
00:32:03,910 --> 00:32:08,290
And in that sense, it's
computable, right? So who knows?

476
00:32:08,710 --> 00:32:12,430
Lots of different people are thinking
about this in different ways. Uh,

477
00:32:12,431 --> 00:32:16,960
Max Tegmark has his book. Uh,
and he talks about some of this,

478
00:32:17,330 --> 00:32:20,470
uh, of course, ed, Fred, Ken
and Conrad Zeus, or like I said,

479
00:32:20,471 --> 00:32:24,650
have been thinking about this
for many decades. Um, and, uh,

480
00:32:24,700 --> 00:32:28,660
Stephen Wolfram has his take on it,
but there's something here.

481
00:32:28,690 --> 00:32:33,490
There's something here about the fact
that computational constructs on very,

482
00:32:33,491 --> 00:32:35,560
very small,
um,

483
00:32:35,980 --> 00:32:40,980
recipes can create this sort of
a useful emergent complexity.

484
00:32:43,850 --> 00:32:47,690
And now as we know, you know, computers
can create computational realities.

485
00:32:47,691 --> 00:32:50,450
I can today basically build my own world.

486
00:32:50,451 --> 00:32:53,870
I start the book off by saying that I
got into computers because at the age of

487
00:32:53,871 --> 00:32:56,540
four I ran into a commodore 64.

488
00:32:56,690 --> 00:33:01,550
I saw hang man laying on the screen and
it blew my mind because I had never seen

489
00:33:01,610 --> 00:33:05,840
a TV screen play out what
I wanted it to play out.

490
00:33:06,080 --> 00:33:09,440
And yet there was a keyboard which I
could touch and suddenly everything was

491
00:33:09,441 --> 00:33:13,280
fungible. This notion of programmability
for a four year old mind was,

492
00:33:13,290 --> 00:33:16,400
was completely mind blowing.
And from there it went to, well,

493
00:33:16,401 --> 00:33:17,660
what can I not create?

494
00:33:17,960 --> 00:33:20,870
And now of course we know we can
pretty much create what we want.

495
00:33:21,170 --> 00:33:26,170
And even the physical dimensions of all
of this are not gated in any way by by

496
00:33:26,961 --> 00:33:27,530
reality.

497
00:33:27,530 --> 00:33:32,530
These are some just fundamental things
from computer science that we ought to

498
00:33:32,631 --> 00:33:37,520
realize that, uh, the, the basic
constructs of computer science,

499
00:33:37,610 --> 00:33:40,490
the magnification,
it'll constructs the iteration,

500
00:33:40,491 --> 00:33:42,110
the recursion and so on and so forth.

501
00:33:42,410 --> 00:33:46,640
Applied to very basic
specifications can yield a lot.

502
00:33:47,300 --> 00:33:50,150
And then if you start thinking
about the mind of a machine,

503
00:33:50,180 --> 00:33:53,270
which any such,
uh,

504
00:33:53,360 --> 00:33:57,140
thinking is partial because we don't know,
we, you know, there's a lot to be done.

505
00:33:57,141 --> 00:33:59,090
There's a lot of questions to be answered,

506
00:33:59,480 --> 00:34:03,230
but think about things just in
terms of differences with us. Well,

507
00:34:03,231 --> 00:34:06,250
one thing we know pretty well is that,
you know,

508
00:34:06,260 --> 00:34:11,260
our brain fits into a relatively small
cranium and consumes about 20 watts.

509
00:34:12,920 --> 00:34:16,340
It's very, very efficient. It has
a very large number of neurons.

510
00:34:16,370 --> 00:34:19,850
It has a very large number
of connections. But, uh,

511
00:34:19,910 --> 00:34:21,860
for all of its efficiency and size,

512
00:34:21,861 --> 00:34:26,150
etc. It's not really substantially going
to consume more than 20 watts in its

513
00:34:26,151 --> 00:34:28,340
present form.
Um,

514
00:34:29,720 --> 00:34:34,010
while we don't have a computer
that's as efficient as a brain yet,

515
00:34:34,190 --> 00:34:38,600
we do know that our computers can
consume much more than 20 watts.

516
00:34:38,600 --> 00:34:42,920
They don't have to sit inside a physical
cranium. That's the, the size of ours.

517
00:34:43,340 --> 00:34:45,260
Uh, perfect recall, which is that,

518
00:34:45,790 --> 00:34:49,360
and this is interesting because you know,

519
00:34:49,370 --> 00:34:51,800
we tend to sort of live in the moment.

520
00:34:51,801 --> 00:34:55,970
We get what we need from that
experience and we tend to forget.

521
00:34:56,150 --> 00:34:57,920
And in many ways this is good for us.

522
00:34:57,980 --> 00:35:00,920
It's good for us actually
because it avoids overload.

523
00:35:01,280 --> 00:35:03,380
And this is common with
another thing that we do,

524
00:35:03,381 --> 00:35:04,880
which is very aggressive pruning.

525
00:35:05,330 --> 00:35:10,330
So we tend to go down certain solutions
and we tend to discard things that sound

526
00:35:11,121 --> 00:35:12,260
ridiculous to us.

527
00:35:12,680 --> 00:35:17,680
So the reason why that move that Lisa
Dole and others found so magical,

528
00:35:19,790 --> 00:35:24,790
even all the commentators that were
great practitioners of go was because it

529
00:35:24,981 --> 00:35:27,980
just was one of those things that
they were willing to discard.

530
00:35:28,180 --> 00:35:31,190
Nobody's done this before, I've never
done this before in this situation.

531
00:35:31,190 --> 00:35:34,520
Why the hell would you ever do this
before? It becomes sort of common sense.

532
00:35:34,880 --> 00:35:39,880
It becomes the kind of thing that is
system one thinking in a few go back to

533
00:35:39,901 --> 00:35:42,450
[inaudible] and his
teary about how we think,

534
00:35:42,451 --> 00:35:45,540
thinking fast and slow and
we start pruning things.

535
00:35:45,840 --> 00:35:50,220
But in machine intelligence can actually
be in a place, can take everything in,

536
00:35:51,030 --> 00:35:53,010
can learn what it can at the time,

537
00:35:53,490 --> 00:35:55,950
but the original experiences
entirely preserved.

538
00:35:55,951 --> 00:36:00,630
So if its ability to extract
more knowledge from that
experience improves over

539
00:36:00,631 --> 00:36:05,550
time. The original data and full
fidelity is still available. In fact,

540
00:36:05,551 --> 00:36:08,310
uh, my colleague, Professor Bruce Porter,

541
00:36:08,311 --> 00:36:13,311
who's the chairman of the ut computer
science department is working on a long

542
00:36:13,561 --> 00:36:16,110
running project that
does something like this,

543
00:36:16,440 --> 00:36:19,100
which is that he's developing machine.
Um,

544
00:36:19,230 --> 00:36:23,610
so natural language understanding software
and he's been working in that area

545
00:36:23,611 --> 00:36:25,080
for 35 plus years.

546
00:36:25,410 --> 00:36:30,410
And his approach is that what I can't
understand what my current algorithms in

547
00:36:30,601 --> 00:36:32,910
the corpus will get
tagged in a special way.

548
00:36:33,570 --> 00:36:37,950
And every iteration of the algorithm will
go back and look at what the previous

549
00:36:37,951 --> 00:36:39,900
iteration was not able to understand.

550
00:36:40,500 --> 00:36:44,940
So this sort of constant learning with
the availability of the fully preserved

551
00:36:44,941 --> 00:36:48,180
information. That's not how
we usually think about things.

552
00:36:48,181 --> 00:36:49,530
We filter out a lot of stuff.

553
00:36:49,860 --> 00:36:52,650
And then of course there's other
things like being disembodied.

554
00:36:52,651 --> 00:36:56,790
I mean there's no need to
protect a physical a body at all.

555
00:36:56,791 --> 00:37:01,320
There's no need to comply
with a size limitation.

556
00:37:01,470 --> 00:37:05,010
And of course we know about the
faster processing. So, uh, you know,

557
00:37:05,070 --> 00:37:10,070
the question here is if human beings are
devolved with an eye also on the back

558
00:37:10,111 --> 00:37:14,970
of our head, would we be
fundamentally different? Probably.

559
00:37:15,600 --> 00:37:16,710
Probably.
I mean,

560
00:37:16,711 --> 00:37:21,690
a lot of the Amygdala driven response
that we have now in situations of fair

561
00:37:21,691 --> 00:37:24,060
where, you know, uh, we,

562
00:37:24,090 --> 00:37:28,830
we are aware that there could be something
pretty close behind us that's got to

563
00:37:28,831 --> 00:37:29,690
keep us,
uh,

564
00:37:29,740 --> 00:37:34,710
that could get us and therefore we have
to keep at a high state of readiness.

565
00:37:34,710 --> 00:37:39,510
Well, uh, maybe that we
would be less, I'm neurotic,

566
00:37:39,600 --> 00:37:43,110
you know, with an eye at the
back of our head. Who knows. Uh,

567
00:37:43,170 --> 00:37:46,330
so these are all the kinds of
things that, of course in, um,

568
00:37:47,550 --> 00:37:50,730
machine intelligence,
we get to experiment with.

569
00:37:52,680 --> 00:37:57,460
One other element, very practical element
of what's happening right now, um,

570
00:37:58,050 --> 00:37:59,830
is, you know, so,

571
00:37:59,831 --> 00:38:03,240
so we're talking about AI of the future
and the mind of a machine and so on.

572
00:38:03,241 --> 00:38:07,530
But what's happening right now,
you know, and in the valley,

573
00:38:07,531 --> 00:38:08,281
everybody knows,

574
00:38:08,281 --> 00:38:12,540
Marc Andreessen said awhile ago that
software's eating the world and that sort

575
00:38:12,541 --> 00:38:17,190
of a euphemistic thing. But for me it's
a, it's a very real physical thing.

576
00:38:17,550 --> 00:38:21,420
So this is, uh, you know, traditional, uh,

577
00:38:21,450 --> 00:38:25,380
combustion engine and you've
got an electric motor onto
the right hand side and

578
00:38:25,381 --> 00:38:28,850
you've got valves and, and, uh, uh,

579
00:38:29,300 --> 00:38:32,670
spark plugs and efs and carburetors,

580
00:38:32,671 --> 00:38:36,490
and you've got a block and you've
got all of things going on here.

581
00:38:36,850 --> 00:38:39,100
And each one of them
has a specific function.

582
00:38:39,580 --> 00:38:42,910
Each one of these mechanical elements
performs a specific function.

583
00:38:43,240 --> 00:38:46,750
And then you've got an electric motor
where most of the capability of the

584
00:38:46,751 --> 00:38:51,460
mechanical elements has been transformed
into software. The Efi, for example,

585
00:38:51,580 --> 00:38:55,330
how you gate energy is now all software.

586
00:38:55,480 --> 00:39:00,480
So the reduced number of physical
components in that picture on the right is

587
00:39:02,380 --> 00:39:04,990
when you subtract that from
the picture on the left.

588
00:39:05,230 --> 00:39:09,220
That is the amount of physical stuff
that software just eight. Okay.

589
00:39:09,400 --> 00:39:13,690
And there are similar pictures like
this across a whole host of areas.

590
00:39:13,990 --> 00:39:17,800
We work very closely with Boeing.
Um,

591
00:39:18,040 --> 00:39:20,050
I can tell you that
the future of aviation,

592
00:39:20,080 --> 00:39:23,560
even though it's not
going to be very imminent,

593
00:39:23,620 --> 00:39:28,620
but Boeing's invested in a company that's
doing electrical engines for proper

594
00:39:29,500 --> 00:39:33,790
commuter aircraft.
You're not the size of a triple seven yet,

595
00:39:33,970 --> 00:39:37,120
but you know,
multi seat commuter aircraft,

596
00:39:37,121 --> 00:39:40,780
short haul a electric,
that changes a lot of things.

597
00:39:40,990 --> 00:39:45,820
If you look at what companies
like [inaudible] he hang
hangar doing the Chinese

598
00:39:45,821 --> 00:39:50,710
company, he hang with autonomous
drones. Uh, the city of, uh, um,

599
00:39:50,800 --> 00:39:55,090
actually the country of UAA appointed
an AI minister recently and they've

600
00:39:55,091 --> 00:39:55,940
expressed their,

601
00:39:56,070 --> 00:40:01,070
their desire to build the world's
first autonomous flying a taxi service.

602
00:40:02,050 --> 00:40:06,280
And they in fact even signed a contract
with a Chinese company and they're now

603
00:40:06,281 --> 00:40:10,240
looking to move that to somebody else.
But they are committed to doing that.

604
00:40:10,390 --> 00:40:11,820
So these things,
uh,

605
00:40:12,070 --> 00:40:16,300
even in the narrow context are
happening and uh, the, they're,

606
00:40:16,301 --> 00:40:19,420
they're certainly coming to, to, to bear

607
00:40:21,040 --> 00:40:25,150
last point here I'll make and then we
will kind of stop for questions. Um,

608
00:40:25,480 --> 00:40:28,930
important thing is not so much,
uh,

609
00:40:29,410 --> 00:40:31,660
whether AI will do everything.

610
00:40:32,200 --> 00:40:35,580
The important thing I
think is what all can a,

611
00:40:35,581 --> 00:40:38,080
and I do in a given period of time.

612
00:40:38,650 --> 00:40:43,600
And if you look at the right there,
that's a study that was done recently,

613
00:40:43,630 --> 00:40:48,310
a poll. Many AI experts and
you may agree or disagree with

614
00:40:48,600 --> 00:40:51,840
those and some of them may be optimistic
and some of them may be pessimistic,

615
00:40:52,170 --> 00:40:53,430
but for example,

616
00:40:53,490 --> 00:40:58,490
the ability to assemble any lego
right in the next 10 years or so.

617
00:40:59,790 --> 00:41:03,390
Now we know that when you can assemble
any Lego, you're not just assembling Lego.

618
00:41:03,450 --> 00:41:08,190
It's a fairly general purpose capability
that you have a manufacturing robots

619
00:41:08,191 --> 00:41:11,250
have been increasing in huge ways.

620
00:41:11,251 --> 00:41:14,940
If you look at warehouse management just
five years ago and what we have now in

621
00:41:14,941 --> 00:41:17,730
terms of warehouse management capability,
it's tremendous.

622
00:41:18,000 --> 00:41:21,090
And nobody's going to want to discuss,
you know,

623
00:41:21,270 --> 00:41:25,080
the poetry of Robert Froster roomy
with a warehouse management robot.

624
00:41:25,290 --> 00:41:29,160
But it is going to take a, it is
going to have an impact on, on jobs.

625
00:41:29,400 --> 00:41:31,970
And so part of what my uh,

626
00:41:33,560 --> 00:41:38,210
push really has been a particularly
on the policy side has been, look,

627
00:41:38,310 --> 00:41:41,510
we hear it all the platitudes that uh,
you know,

628
00:41:41,511 --> 00:41:43,940
people say the machines are coming,
the machines are coming,

629
00:41:43,941 --> 00:41:46,460
but really it's like any other revolution.

630
00:41:46,460 --> 00:41:50,090
It's like any other
technological area of progress,

631
00:41:50,360 --> 00:41:54,710
era of progress rather where there'll be
new jobs and all these people that are

632
00:41:54,711 --> 00:41:58,550
displaced here,
we'll find things to go do there.

633
00:41:58,760 --> 00:42:02,660
And I think that's just complete hogwash.
I think that's nonsense.

634
00:42:02,870 --> 00:42:06,110
I think the two things that we've done,
one,

635
00:42:06,111 --> 00:42:09,110
we've replicated human
muscle with the steam engine,

636
00:42:09,320 --> 00:42:14,160
which basically we got ourselves out
of every job that requires a muscle.

637
00:42:14,720 --> 00:42:19,580
And now by replicating, not the entire
mine, but even parts of the mind,

638
00:42:19,581 --> 00:42:23,180
slivers of the mind that are good
enough to form a, to perform a function.

639
00:42:23,330 --> 00:42:27,470
We are at a point where we can automate
much of what we do in, in many,

640
00:42:27,471 --> 00:42:30,800
many professions.
And that's all we are.

641
00:42:31,010 --> 00:42:35,330
We are muscle and mind. I mean
that's what, that's what it is.

642
00:42:35,600 --> 00:42:39,980
So there's no third thing to go
and replicate. And with this,

643
00:42:40,010 --> 00:42:44,870
the impact may not be us
in Barka loungers you know,

644
00:42:44,960 --> 00:42:49,430
Allah, Wally, but it might
be 30, 40% unemployment.

645
00:42:49,790 --> 00:42:52,580
And who needs to start
thinking about this?

646
00:42:52,850 --> 00:42:56,510
The people that need to start thinking
about this are the people who make

647
00:42:56,511 --> 00:43:00,380
policies. And there are some
countries where, for example,

648
00:43:00,381 --> 00:43:04,520
they've gone and they've
started experimenting with
them. Uh, things like, uh,

649
00:43:04,521 --> 00:43:09,350
you know, a minimum wage of
essentially minimum guaranteed income.

650
00:43:09,830 --> 00:43:12,980
Um,
there are countries like France,

651
00:43:12,981 --> 00:43:17,720
which are progressively reducing the
number of workers so that automation can

652
00:43:17,721 --> 00:43:21,410
pick up the slack and people
can get time back. I don't know,

653
00:43:21,411 --> 00:43:24,590
Bill Gates has proposed a tax on robots.

654
00:43:24,830 --> 00:43:27,470
I am not proposing a specific solution,

655
00:43:27,710 --> 00:43:31,520
but what I am saying is that the level
at which this discussion is happening

656
00:43:32,280 --> 00:43:33,320
with the

657
00:43:35,240 --> 00:43:40,240
inevitability of the
impact right ahead of us.

658
00:43:40,490 --> 00:43:43,210
I think that level of discussion is uh,

659
00:43:43,340 --> 00:43:45,890
in significant and insufficient.

660
00:43:46,190 --> 00:43:49,430
And I started the stock
about, uh, you know,

661
00:43:49,431 --> 00:43:54,431
talking about autonomous weapons and
sharing with you the rate at which the CCW

662
00:43:54,900 --> 00:43:58,550
UN is making progress on even
defining what an autonomous weapon is.

663
00:43:58,700 --> 00:44:01,790
While at the same time autonomous
weapons are being deployed.

664
00:44:02,450 --> 00:44:05,420
And here again,
we might find ourselves in a,

665
00:44:05,421 --> 00:44:10,421
in a situation where more and more
automation will make it into factories,

666
00:44:11,061 --> 00:44:13,460
into retail spaces.
Uh,

667
00:44:13,770 --> 00:44:18,770
we're working on an engagement with a
large company in Dubai to do concierge

668
00:44:20,001 --> 00:44:24,440
intelligence based on natural
language processing in retail stores.

669
00:44:24,560 --> 00:44:29,450
It's going to start off being
interesting and sort of like a,

670
00:44:29,540 --> 00:44:29,751
you know,

671
00:44:29,751 --> 00:44:32,880
how do you get people in into the shop
and you would track them with something

672
00:44:32,881 --> 00:44:37,470
that's a new and shiny, but it'll get
pretty good pretty quickly. So, uh,

673
00:44:37,530 --> 00:44:41,190
that's kind of where we are in my,
my hope is that we can,

674
00:44:41,191 --> 00:44:45,060
with all of the work that we're doing
and all the conversations we're having in

675
00:44:45,061 --> 00:44:48,380
Brussels and DC and
elsewhere, is to get, uh,

676
00:44:48,420 --> 00:44:50,940
the leaders of our nation and frankly,

677
00:44:50,941 --> 00:44:55,941
the leaders of the Western world to take
notice and to truly focus themselves in

678
00:44:58,110 --> 00:45:02,550
developing policies that can sustain
this AI powered world of the future.

679
00:45:03,360 --> 00:45:07,200
So with that, I'll stop, um,
see if there are any questions.

680
00:45:15,690 --> 00:45:18,340
Thank you for your, uh, your talk. Um,

681
00:45:19,740 --> 00:45:22,710
my question was mostly about
like, you know, the ending,

682
00:45:22,740 --> 00:45:27,030
the last thing you talked about,
right? Um, I often hear about,

683
00:45:27,060 --> 00:45:31,920
um, the need for more discussion
policy, policy changes, right?

684
00:45:32,190 --> 00:45:36,450
But it really alarms me because when
you see the kind of discourse that our

685
00:45:36,451 --> 00:45:39,900
elected representatives often have about
even like really basic technological

686
00:45:39,901 --> 00:45:44,160
issues, you realize they have no idea what
they're talking about. Right. And like,

687
00:45:44,280 --> 00:45:47,940
it really worries me when they might
be having discussions about things like

688
00:45:47,941 --> 00:45:51,030
this, which are potentially
very complex and nuanced. Like,

689
00:45:51,031 --> 00:45:54,120
is there a solution to this? Like, you
know, you, you mentioned even that like,

690
00:45:54,330 --> 00:45:54,541
you know,

691
00:45:54,541 --> 00:45:57,210
they're having discussions about defining
things that are already happening,

692
00:45:57,240 --> 00:45:59,670
right? Like, what, what
can we do about this?

693
00:46:01,080 --> 00:46:03,360
That's a very difficult question.

694
00:46:03,390 --> 00:46:06,480
I think one thing that we
should do is to not give up.

695
00:46:06,870 --> 00:46:09,330
So what I'm personally doing,
uh,

696
00:46:09,390 --> 00:46:14,390
is that I try to the best of my ability
to insert myself into every forum where

697
00:46:16,831 --> 00:46:19,080
I can impact policymakers,

698
00:46:19,470 --> 00:46:23,820
where we can go and share the story and
explained to them the quantum of impact

699
00:46:23,821 --> 00:46:28,770
that is coming. Uh, two weeks ago I
was speaking at the Texas CEO Summit,

700
00:46:28,771 --> 00:46:29,970
which is an economic summit.

701
00:46:29,971 --> 00:46:33,780
So they talk about how the state's growing
and what the future of work will be,

702
00:46:34,020 --> 00:46:37,770
but you have leaders, the state
leadership, economic leaders,

703
00:46:37,771 --> 00:46:42,480
and so on present day. And I taught that
what I said may have been surprising,

704
00:46:42,750 --> 00:46:46,530
but it was well received. People
were willing to listen. Um,

705
00:46:46,620 --> 00:46:51,330
I brief pretty much everyone
at the Pentagon. Uh, and uh,

706
00:46:51,390 --> 00:46:53,700
I told you I met with Eric
Schmidt. Uh, you know,

707
00:46:53,730 --> 00:46:56,400
that was also in connection
with a seniors event.

708
00:46:56,820 --> 00:46:59,670
So that's the one thing I know how to do,

709
00:46:59,700 --> 00:47:04,020
which is to be out there and keep
repeating the message over and over.

710
00:47:04,021 --> 00:47:07,740
And then as a consequence of doing that,
you find allies,

711
00:47:07,770 --> 00:47:11,040
you find kindred spirits
you find in an exchange.

712
00:47:11,070 --> 00:47:13,410
I didn't know how Eric was
going to answer that question,

713
00:47:13,620 --> 00:47:18,030
but he happened to answer that question
in a way that supported the basic thrust

714
00:47:18,031 --> 00:47:22,620
of what we were talking about.
And that became 25 media articles,

715
00:47:22,980 --> 00:47:24,410
you know, uh, General Allen,

716
00:47:24,430 --> 00:47:29,430
I recently wrote in foreign policy that
got reprinted in the national newspaper

717
00:47:29,651 --> 00:47:33,820
of the UAE. It showed up in Canada,
showed up everywhere. So again,

718
00:47:34,930 --> 00:47:38,530
we got forced change physically,

719
00:47:38,890 --> 00:47:42,940
but what we can do is influence
huge numbers of mines.

720
00:47:43,030 --> 00:47:47,290
And in doing that we can find allies.
You know, this is a mind shift.

721
00:47:47,830 --> 00:47:52,830
So I don't expect people that have no
grounding and no interest in this area to

722
00:47:53,111 --> 00:47:54,820
suddenly see the light.

723
00:47:55,120 --> 00:47:58,870
But I do hope that we'll be
able to find around them,

724
00:47:58,900 --> 00:48:03,900
influencers and shapers that can at least
carry the day and move them forward in

725
00:48:04,541 --> 00:48:09,300
the direction that the country, the
world needs them to move in. It's a,

726
00:48:09,400 --> 00:48:14,400
it's not an easy process and it's not a
direct answer but that's the best I know

727
00:48:14,830 --> 00:48:15,663
how to do.

728
00:48:16,920 --> 00:48:18,480
Thank you.
Thank you.

729
00:48:22,820 --> 00:48:27,310
I the title of the Book and the talk
is the sentient machine and we haven't

730
00:48:27,490 --> 00:48:29,890
talked much about
sentients. Yeah, right here.

731
00:48:30,190 --> 00:48:34,630
But that is a goal of many of the teams
working towards this self aware machines

732
00:48:34,631 --> 00:48:38,470
with morality and ethics of their
own real viewpoints and perspectives.

733
00:48:38,471 --> 00:48:41,140
Now that might be five years away,
40 years away,

734
00:48:41,590 --> 00:48:45,520
but it would be a good idea to have some
kind of idea of how to deal with that

735
00:48:45,521 --> 00:48:47,740
when it gets here before it gets here.

736
00:48:48,010 --> 00:48:50,710
So in the circles that you talk in,

737
00:48:50,980 --> 00:48:53,620
is there any discussion
about ways to manage,

738
00:48:53,621 --> 00:48:57,220
regulate and interact with
AI other than as property?

739
00:48:58,070 --> 00:49:00,470
That's a very good question.
And by the way,

740
00:49:00,471 --> 00:49:04,760
I completely concur with your original
observation that the title of the book is

741
00:49:04,761 --> 00:49:06,740
the essential machine in this stock.

742
00:49:06,741 --> 00:49:10,640
We didn't really get into what ascension's
and so on. The book does cover that.

743
00:49:10,790 --> 00:49:14,710
My view of it. And just to
give you a very quick, uh,

744
00:49:15,230 --> 00:49:18,440
sort of response to that
in my view, uh, you know,

745
00:49:18,441 --> 00:49:23,090
intelligence and a lot of people
agree with this, but not all. Uh,

746
00:49:23,690 --> 00:49:28,580
intelligence is about goal directed
behavior and the larger your goals,

747
00:49:28,610 --> 00:49:32,420
usually the more intelligent
we assess that entity to be.

748
00:49:34,400 --> 00:49:39,400
And dummy sanctions is a combination
of intelligence and self awareness.

749
00:49:39,711 --> 00:49:42,590
So what I talk about
in the book is sort of,

750
00:49:42,740 --> 00:49:47,630
I think therefore I am that school
of thought of the principal,

751
00:49:47,900 --> 00:49:48,733
um,

752
00:49:49,790 --> 00:49:54,790
proof even to myself that I exist
is that I can externalize my,

753
00:49:56,270 --> 00:50:00,860
my myself and then observe
myself thinking and then say,

754
00:50:00,890 --> 00:50:05,780
Aha, I must be. And then
from there we go. We go on.

755
00:50:06,470 --> 00:50:11,100
But that being said, um, you
know, your question really is, uh,

756
00:50:14,600 --> 00:50:18,750
whether in these practical,
you know, domains,

757
00:50:19,200 --> 00:50:23,320
whether, um, there's a realization, uh,

758
00:50:23,430 --> 00:50:25,550
that so,

759
00:50:25,700 --> 00:50:30,270
so the trust of your really
is managing sanctions or, uh,

760
00:50:30,410 --> 00:50:34,070
not the, not creating sentients in
these machines. So there is nothing,

761
00:50:34,460 --> 00:50:38,370
no. So the government manages
and interacts with people, right?

762
00:50:38,620 --> 00:50:41,610
But the government does not manage
us as though we are property. Right.

763
00:50:42,250 --> 00:50:45,700
All of the talks that I've heard
are about managing ais property.

764
00:50:45,790 --> 00:50:49,180
Are there any alternative talks going
on that you've observed? Yes. I mean,

765
00:50:49,181 --> 00:50:51,400
I'm part of that alternative doc myself.

766
00:50:51,880 --> 00:50:55,750
What I say in this book is that
even if you go back to our, uh,

767
00:50:55,870 --> 00:50:58,750
religious traditions,
not to take them literally,

768
00:50:58,780 --> 00:51:00,760
but if you go back to
our religious traditions,

769
00:51:01,060 --> 00:51:06,060
what made Adam great was the fact that
he could take action on his own and up

770
00:51:07,271 --> 00:51:12,070
until the creation of Adam in our
Abrahamic system of, uh, religions,

771
00:51:12,430 --> 00:51:17,050
uh, the angels et Cetera, could do just
what they were told to do. Uh, and the,

772
00:51:17,051 --> 00:51:22,030
the fact that Adam could do what he,
what he wanted to was what made him great.

773
00:51:22,330 --> 00:51:27,070
If we now are after millennia
and millennia of evolution,

774
00:51:27,071 --> 00:51:31,420
poised at the juncture where we can
be the kind of creator that can create

775
00:51:31,421 --> 00:51:36,310
something that has its own with
two whatever limited degree,

776
00:51:36,580 --> 00:51:38,890
because we haven't figured out
how much free will we have,

777
00:51:39,190 --> 00:51:44,000
but to whatever limited degree, I don't
think that that's an automatic, um,

778
00:51:44,400 --> 00:51:48,460
uh, you know, a dampener on, on this
process. And I don't think we should stop.

779
00:51:48,461 --> 00:51:51,970
I also think there's a long ways to go,
a lot to learn.

780
00:51:52,030 --> 00:51:56,890
There's a lot to do with ethical systems
and safe AI and so on and so forth. So,

781
00:51:57,180 --> 00:52:01,000
um, uh, a new form of, of life.

782
00:52:01,270 --> 00:52:05,830
If it truly is sentient, should
we treat it as property? No.

783
00:52:07,840 --> 00:52:08,673
Thank you.
Yeah.

784
00:52:11,230 --> 00:52:15,970
I have another question.
Uh, in your book, um, you,

785
00:52:16,040 --> 00:52:18,070
you talked a lot about,
um,

786
00:52:18,250 --> 00:52:23,170
opportunities and dangers of AI
in all sorts of different aspects.

787
00:52:23,230 --> 00:52:23,711
And again,

788
00:52:23,711 --> 00:52:26,920
I want to encourage everyone to read
the book because it covers so much more

789
00:52:26,921 --> 00:52:30,030
subject matter then,
uh, this darkening, uh,

790
00:52:30,040 --> 00:52:33,210
one of the things they found the
missing was, uh, when you, um,

791
00:52:33,310 --> 00:52:35,980
write about mine hacking,
um,

792
00:52:36,160 --> 00:52:41,120
both on like on a personal level
and on a national level, right?

793
00:52:41,350 --> 00:52:45,130
Is there any like solution that,
that you see,

794
00:52:45,340 --> 00:52:47,170
like how to defend against that?

795
00:52:47,260 --> 00:52:51,850
Like sort of like an antivirus that will
say your mind is being hijacked or your

796
00:52:51,851 --> 00:52:53,410
democracy is being hijacked?
Yeah.

797
00:52:53,540 --> 00:52:54,500
Yes.
Uh,

798
00:52:54,530 --> 00:52:59,530
so in the book there's a section called
AI shields and that talks about how,

799
00:53:00,050 --> 00:53:03,990
uh, you would want to use, uh, AI, uh, to,

800
00:53:04,030 --> 00:53:06,250
to fend off that kind of Ai,
uh,

801
00:53:06,620 --> 00:53:10,130
because basically what's happening
now is that we will know once this

802
00:53:10,131 --> 00:53:11,720
investigation into the Russian

803
00:53:13,550 --> 00:53:16,280
involvement in an outer
election is full liter veal,

804
00:53:16,700 --> 00:53:21,700
but a tens and tens of thousands of
bots were using very simplistic NLG

805
00:53:23,480 --> 00:53:28,380
technology. Do not just retweet, but
come up with messages that were targeted.

806
00:53:29,310 --> 00:53:33,930
And the indent was to shift the prevailing
sentiment in the, in the election.

807
00:53:34,200 --> 00:53:36,840
So, uh, we've developed systems,

808
00:53:36,841 --> 00:53:41,070
others have developed systems that can
look at that kind of generated activity

809
00:53:41,430 --> 00:53:45,600
and a identified as distinct from
what somebody actually wrote.

810
00:53:45,930 --> 00:53:47,250
And even otherwise,

811
00:53:47,251 --> 00:53:51,840
looking at the pattern of post behaviors
and profiles and so on to detect what

812
00:53:51,841 --> 00:53:56,390
is might be a very sophisticated part.
But ultimately this is sort of a, uh,

813
00:53:56,400 --> 00:54:00,780
um, you know, sort of, uh, a
cycle. They build a better bar.

814
00:54:00,810 --> 00:54:04,110
Then you've got to find out ways to
detect that better bought and so on and so

815
00:54:04,111 --> 00:54:06,900
forth.
But I think that is very critical.

816
00:54:06,901 --> 00:54:11,700
The other thing which you may like or
not like given that this is Google, uh,

817
00:54:12,210 --> 00:54:13,650
is in my view,

818
00:54:13,740 --> 00:54:17,670
there's too much control
of Algorithms in the cloud.

819
00:54:18,240 --> 00:54:21,720
I think that, uh, just just
to give you the very simple,

820
00:54:21,750 --> 00:54:26,730
basic example is that I
want to control what I see.

821
00:54:26,820 --> 00:54:31,820
I've made a conscious decision now
that it is not good for a social media

822
00:54:32,491 --> 00:54:34,680
service and I won't name any specific one.

823
00:54:34,681 --> 00:54:39,681
All of them do this for a social media
service to decide what they want to show

824
00:54:39,781 --> 00:54:42,950
me, regardless of how good their
machine learning algorithms are.

825
00:54:43,140 --> 00:54:47,370
And regardless of how good
the collaborative filtering
is and regardless of what

826
00:54:47,371 --> 00:54:50,830
they think my cousin likes or
what my younger brother, you know,

827
00:54:50,850 --> 00:54:52,950
clicked on yesterday.
Um,

828
00:54:53,070 --> 00:54:58,070
I want to be an active participant
in the filtering of my feed.

829
00:54:58,741 --> 00:54:59,281
And in fact,

830
00:54:59,281 --> 00:55:04,281
what I would like is I would like
all of my posts from my network,

831
00:55:06,060 --> 00:55:11,060
totally raw feed and on my end I get
to decide what I want to see and what I

832
00:55:11,221 --> 00:55:16,200
don't want to see. If we don't do that,
we're in trouble. And if we don't do that,

833
00:55:16,201 --> 00:55:20,640
then you know, US engineers and
builders, etc. Should go to that.

834
00:55:21,330 --> 00:55:22,050
Um,

835
00:55:22,050 --> 00:55:27,050
I think having this notion of your
own AI shield and your own AI filter,

836
00:55:27,270 --> 00:55:28,470
that's very,
very important.

837
00:55:30,060 --> 00:55:33,630
Having the convenience of multiple data
centers all over the world and having

838
00:55:33,631 --> 00:55:37,080
the convenience of not having to buy
and configure computers and having the

839
00:55:37,081 --> 00:55:40,260
convenience of being able to get to
them from any point, uh, you know,

840
00:55:40,261 --> 00:55:45,180
on the world. Uh, is one thing,
but then also not controlled.

841
00:55:45,210 --> 00:55:46,980
Not even knowing what the,

842
00:55:47,310 --> 00:55:52,290
today I can't even tell what the hell
the raw feed is that I'm supposed to get.

843
00:55:52,320 --> 00:55:56,040
There's just no way to get to that.
It's such a glaring omission.

844
00:55:57,150 --> 00:55:59,160
That to me is ridiculous.

845
00:55:59,430 --> 00:56:04,200
And I think that's another area where
the algorithms need to be controlled more

846
00:56:04,201 --> 00:56:05,034
by us.

847
00:56:05,160 --> 00:56:10,160
Even if the cloud platforms
provide the data and now to say,

848
00:56:10,650 --> 00:56:11,790
Oh, you know, don't worry,

849
00:56:11,820 --> 00:56:15,110
you don't have the compute power to
filter your own feet. Nonsense. Come on.

850
00:56:16,140 --> 00:56:19,590
That's how many,
how many posts will I get in my feed?

851
00:56:20,400 --> 00:56:25,000
10,000 a day. I could probably do
that on her ass, but he buy, you know,

852
00:56:25,750 --> 00:56:30,250
so all of us can do that, Frederick. So
the technical arguments no longer apply.

853
00:56:30,251 --> 00:56:35,251
It's a control argument and it's very
important to let people manage the

854
00:56:36,190 --> 00:56:38,020
information the way
they want to manage it.

855
00:56:39,990 --> 00:56:43,540
So in the last slide where
we talk about a lot of STI,

856
00:56:43,780 --> 00:56:48,760
the loss of jobs and
those jobs potentially not
replaceable as has been in the

857
00:56:48,761 --> 00:56:53,020
past. Uh, see there, there's sort of the,

858
00:56:53,070 --> 00:56:54,480
the muscle where you're,

859
00:56:54,540 --> 00:56:59,540
you're talking about labor moving to
steam engines and those innovations at the

860
00:56:59,591 --> 00:57:01,270
factory innovation that came out of it.

861
00:57:01,990 --> 00:57:06,990
But nowadays the first level of AI
implementation that we are looking at like

862
00:57:07,451 --> 00:57:08,620
self driving cars.

863
00:57:08,950 --> 00:57:12,670
So in some ways they seem sort of muscle
plus plus there is a little bit of

864
00:57:12,671 --> 00:57:13,660
intelligence on top.

865
00:57:14,260 --> 00:57:19,260
Where do you see that heading to in
the next 10 to 20 years where uh,

866
00:57:19,540 --> 00:57:23,610
that muscle plus plus really
becomes as capable as,

867
00:57:23,670 --> 00:57:25,900
as an infant's mine for example.

868
00:57:26,260 --> 00:57:31,060
And what will be the impact of that on
unemployment. So there's, if you can,

869
00:57:31,610 --> 00:57:34,480
um,
if we can look into the future five years,

870
00:57:34,540 --> 00:57:37,690
10 years and maybe even 50
years and see where we end up at

871
00:57:37,990 --> 00:57:41,380
in terms of AI researcher directions.
I'll answer that question in two ways.

872
00:57:41,381 --> 00:57:43,180
One of the things that I'm actually very,

873
00:57:43,181 --> 00:57:47,230
very curious about and have been very
curious about for a long time is this

874
00:57:47,231 --> 00:57:49,570
whole idea of intrinsic motivation,

875
00:57:49,960 --> 00:57:53,230
hierarchical reinforcement
learning and intrinsic motivation.

876
00:57:53,500 --> 00:57:57,910
There are many challenges with this
because I mean ultimately if you, if you,

877
00:57:57,970 --> 00:58:02,650
if you, if you say this
simply, it sounds simple,

878
00:58:02,800 --> 00:58:05,530
it's not very simple to go do,
but the idea is,

879
00:58:05,540 --> 00:58:09,220
well you can have a
reinforcement learning pickup,

880
00:58:09,221 --> 00:58:13,330
one task and then you can have it pick
up another task and then one can be the

881
00:58:13,331 --> 00:58:14,650
sub task for the other end.

882
00:58:14,651 --> 00:58:17,720
So you can have this hierarchical tree
and if you keep building all these tasks,

883
00:58:17,721 --> 00:58:20,580
then you'll have a pretty
big coverage. But the, the,

884
00:58:20,581 --> 00:58:24,280
the challenges with that is whether you
can really do that and whether you can

885
00:58:24,281 --> 00:58:29,080
have these independent tasks and so on.
And second, it's intrinsic motivation.

886
00:58:29,140 --> 00:58:32,180
That's another topic that
I do cover in the book. Um,

887
00:58:32,230 --> 00:58:35,290
Andrew Bartow wrote about this
father of reinforcement learning.

888
00:58:35,320 --> 00:58:38,590
Andrew Barber wrote about this as well,
but that is a,

889
00:58:38,591 --> 00:58:42,190
where does that flame come from?
Where does that,

890
00:58:42,220 --> 00:58:44,830
so when you talk about
a three year old child,

891
00:58:45,250 --> 00:58:49,270
to me it's not so much that we can't build
a machine that does what a three year

892
00:58:49,271 --> 00:58:53,080
old child does lift the amount of weight
or can be driven to where a three year

893
00:58:53,081 --> 00:58:56,260
old child can be driven in that
very mechanistic kind of sense.

894
00:58:56,590 --> 00:59:00,940
The thing about a three year old child
is that that three year old child is born

895
00:59:00,941 --> 00:59:01,780
with a flame.

896
00:59:02,620 --> 00:59:07,600
And that flame of intrinsic motivation
is something that we need to figure out.

897
00:59:07,900 --> 00:59:10,750
Actually, some people have
hypothesized things like, well,

898
00:59:10,751 --> 00:59:15,580
you know what that flame really is, is uh,
emergent. And I talk about that in the,

899
00:59:15,581 --> 00:59:18,520
in the, in the book as
well, emergent purpose.

900
00:59:18,850 --> 00:59:22,850
I went looking in philosophy
for what that purpose is.

901
00:59:23,390 --> 00:59:27,170
Somewhere along my life I thought
I knew what I was supposed to do.

902
00:59:27,171 --> 00:59:30,350
And then one day I felt,
what the hell am I supposed to do now?

903
00:59:30,620 --> 00:59:33,950
And so I started reading
philosophy and you know,

904
00:59:33,951 --> 00:59:35,960
at that point I realized that,
you know,

905
00:59:35,990 --> 00:59:39,010
even even people like Camou Alphabet,

906
00:59:39,680 --> 00:59:44,570
the French philosopher and writer,
he said things that were, you know,

907
00:59:44,571 --> 00:59:49,050
the existential, the Hilus movement.
He said things like, well, uh,

908
00:59:49,280 --> 00:59:53,970
that leap of faith is,
um,

909
00:59:54,470 --> 00:59:58,940
philosophical suicide,
intellectual suicide because you,

910
00:59:59,540 --> 01:00:04,540
you accept that there came a time when
your knowledge couldn't take you over the

911
01:00:06,171 --> 01:00:10,280
humps. So you just assumed, in other
words, you did away with that gift,

912
01:00:10,281 --> 01:00:14,420
which was your biggest gift.
So I don't know whether I'm that extreme,

913
01:00:15,080 --> 01:00:17,840
but I do think that that,
that internal flame,

914
01:00:17,841 --> 01:00:20,870
that intrinsic motivation would
still be something that's missing.

915
01:00:21,200 --> 01:00:25,070
I think narrow systems that have,
uh,

916
01:00:25,610 --> 01:00:26,510
not a flame,

917
01:00:26,511 --> 01:00:31,150
but a set of criteria that they
are going to go and optimize.

918
01:00:31,490 --> 01:00:34,500
You'll see that much of,
I mean,

919
01:00:35,180 --> 01:00:39,230
all of our system one type stuff,
you can pretty much mechanize.

920
01:00:39,231 --> 01:00:41,840
And now if you look at
your day and you say, well,

921
01:00:41,841 --> 01:00:45,740
how much system to type stuff too?
I do. Uh, depending on who you are,

922
01:00:45,741 --> 01:00:50,360
it's going to be variable.
But I think for a lot of jobs,

923
01:00:50,600 --> 01:00:54,710
the job itself is a lot of system
one stuff, and that then goes away,

924
01:00:54,890 --> 01:00:58,880
three year old child, everything.
Other than that intrinsic motivation,

925
01:00:59,060 --> 01:01:03,050
I don't think we have an answer to
that. Intrinsic motivation. Thank you.

926
01:01:04,160 --> 01:01:05,990
Thank you so much. I'm
here. Thank you very much.

927
01:01:06,040 --> 01:01:11,040
[inaudible].

