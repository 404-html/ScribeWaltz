1
00:00:06,210 --> 00:00:07,980
Thank you all for coming.
I really appreciate it.

2
00:00:08,010 --> 00:00:10,440
Thanks for the opportunity
to come speak here. Uh,

3
00:00:10,441 --> 00:00:15,441
today I'm going to be speaking with
you about my book privacy's blueprints,

4
00:00:16,380 --> 00:00:20,070
the battle to control the
design of new technologies.

5
00:00:20,490 --> 00:00:24,190
And I really wants to,
uh,

6
00:00:24,330 --> 00:00:28,470
start off with a series of stories to
tell you a little bit about what the book

7
00:00:28,471 --> 00:00:32,280
is about. So many of
you, if not most of you,

8
00:00:32,281 --> 00:00:36,760
are familiar with the Cambridge
Analytica, Facebook, um,

9
00:00:37,470 --> 00:00:41,670
dust up as it were. Uh, and
there's a, it was a lot going on.

10
00:00:41,671 --> 00:00:46,080
There were lots of information
that was taken from Facebook, uh,

11
00:00:46,200 --> 00:00:49,230
that eventually made its way towards
Cambridge analytic and an outward.

12
00:00:49,231 --> 00:00:52,170
And it caused a huge,
um,

13
00:00:52,500 --> 00:00:56,430
public discussion about
privacy and platforms.

14
00:00:56,730 --> 00:00:58,440
And there was a lot going
on built in with that.

15
00:00:58,441 --> 00:01:01,400
But one of the most
interesting things was, uh,

16
00:01:01,440 --> 00:01:05,580
the fact that there was a big debate about
whether this was originally what they

17
00:01:05,581 --> 00:01:08,670
referred to as a breach or something else.
Ryan.

18
00:01:08,671 --> 00:01:11,630
And one of the things that sort of
came up and one of the things that,

19
00:01:11,730 --> 00:01:16,050
that indeed the platform actually said
is that that permission was actually

20
00:01:16,051 --> 00:01:17,990
granted, um, uh,

21
00:01:18,060 --> 00:01:22,230
through the privacy settings to have
this sort of information given to third

22
00:01:22,231 --> 00:01:24,860
party apps. And in fact
it wasn't just, um,

23
00:01:24,900 --> 00:01:27,420
the one app that was
collecting lots of information.

24
00:01:27,421 --> 00:01:30,960
This was actually a relatively common
practice that people were agreeing to.

25
00:01:31,110 --> 00:01:34,300
But if you actually start to
dig in and think about, well,

26
00:01:34,310 --> 00:01:37,830
what was actually agreed to and when did
it look like you realized it was sort

27
00:01:37,831 --> 00:01:42,120
of one small little option nested within
lots of other small little options that

28
00:01:42,121 --> 00:01:46,330
existed. And while you may technically
have agreed to it, um, it's,

29
00:01:46,331 --> 00:01:49,450
it was sort of binary and
relatively hidden. Um,

30
00:01:49,590 --> 00:01:53,400
and so it resulted in a lot of confusion
because the people that seem to be

31
00:01:53,401 --> 00:01:56,130
angry about it said, well, wait a
second. I didn't even agree to that.

32
00:01:56,131 --> 00:01:59,310
And then of course they were shown exactly
where they did and they said, okay,

33
00:01:59,311 --> 00:02:03,330
well maybe that wasn't the best thing.
The second story I want to talk about.

34
00:02:03,331 --> 00:02:08,331
This is a still taken from an
internet connected baby monitor.

35
00:02:10,020 --> 00:02:12,030
And the most disturbing thing about this,

36
00:02:12,031 --> 00:02:14,010
and this was disturbing in and of itself,

37
00:02:14,160 --> 00:02:16,590
isn't that that there was this one still,

38
00:02:16,591 --> 00:02:20,940
but rather there's an entire search
engine dedicated to finding these sorts of

39
00:02:20,941 --> 00:02:25,710
stills because these sorts of
devices are so routinely compromised.

40
00:02:25,980 --> 00:02:30,720
Um, and, and so easy to, to
sort of take control of that.

41
00:02:30,810 --> 00:02:33,250
There's actually an entire
collection of them online. Um,

42
00:02:33,440 --> 00:02:36,180
which I thought was also
relatively disturbing.

43
00:02:36,480 --> 00:02:40,470
And then the third story that I wanted
to talk about was what many of you may

44
00:02:40,471 --> 00:02:42,060
have been familiar with,

45
00:02:42,061 --> 00:02:46,860
which is the dispute between the Federal
Bureau of Investigation and apple over

46
00:02:46,861 --> 00:02:51,861
whether to build in some sort of
bypass to its encryption scheme and an

47
00:02:53,581 --> 00:02:58,190
authentication protocol to allow
it to access the phone, um,

48
00:02:58,320 --> 00:03:00,550
of the San Bernardino shooter.

49
00:03:01,210 --> 00:03:04,090
Now what are these three
things have in common?

50
00:03:05,320 --> 00:03:10,320
They all are stories about the
design of information technologies.

51
00:03:12,250 --> 00:03:17,250
And in the book I focus on how things are
built and the way in which they affect

52
00:03:19,211 --> 00:03:20,044
our privacy.

53
00:03:20,200 --> 00:03:23,350
And I want to make three points today and
I make try to make three larger points

54
00:03:23,351 --> 00:03:27,250
in the book. One design matters
for privacy and more than that,

55
00:03:27,460 --> 00:03:28,900
which may seem relatively evident,

56
00:03:29,110 --> 00:03:34,110
we need to have a better conversation
to explore why design matters and how

57
00:03:34,331 --> 00:03:37,810
design matters for our privacy because
I think it matters more than what most

58
00:03:37,811 --> 00:03:41,920
people regularly consider too.
And this is the main thesis of the book,

59
00:03:41,921 --> 00:03:43,180
which is why it's in bold,

60
00:03:43,420 --> 00:03:47,710
which is the privacy laws should take
design more seriously and not just the law

61
00:03:47,711 --> 00:03:52,000
but also uh, industry policy,
industry standards, industry norms.

62
00:03:52,180 --> 00:03:56,200
We focus a lot on personal data,

63
00:03:56,980 --> 00:03:59,890
which data, what data is being
collected, how is it being used,

64
00:03:59,891 --> 00:04:03,910
who is it being sharing shared with.
We focused a lot about transparency,

65
00:04:04,090 --> 00:04:08,080
but I argue that there's actually, there's
a, there's a gap, there's a, there's a,

66
00:04:08,110 --> 00:04:13,110
a huge area within which design affects
a lot of what people to be considered,

67
00:04:13,781 --> 00:04:18,730
their personal privacy interest that we
don't regularly talk about in law and in

68
00:04:18,731 --> 00:04:21,430
industry policy and three,

69
00:04:22,060 --> 00:04:26,110
and this is perhaps one of the more
controversial aspects of the book.

70
00:04:26,230 --> 00:04:30,430
A design agenda for law and policy should
have roots and consumer protection and

71
00:04:30,431 --> 00:04:31,600
surveillance law,

72
00:04:32,290 --> 00:04:37,290
not the standard data protection
framework that isn't becoming incredibly

73
00:04:37,901 --> 00:04:40,990
popular through frameworks like the Gdpr,

74
00:04:41,320 --> 00:04:44,020
which everyone has
probably at least Molly,

75
00:04:44,021 --> 00:04:48,730
where I've given the thousands of emails
that you received somewhere around late

76
00:04:48,730 --> 00:04:51,340
April that says we've updated our terms.
Um,

77
00:04:51,580 --> 00:04:56,580
and it's built around this model that
tends to prioritize certain kinds of data

78
00:04:56,771 --> 00:04:58,240
collection and processing rules.

79
00:04:58,450 --> 00:05:02,860
I think at the expense of ignoring some
nuances about the ways in which design

80
00:05:02,861 --> 00:05:06,340
actually controls our privacy.
Okay,

81
00:05:06,640 --> 00:05:11,260
so let me expand upon these three
points. One, design matters for privacy.

82
00:05:11,261 --> 00:05:12,370
What do I mean by that?

83
00:05:12,670 --> 00:05:17,380
I mean the design matters for privacy
because it is everywhere because it is

84
00:05:17,381 --> 00:05:20,200
power and because it is political.

85
00:05:20,680 --> 00:05:23,080
First design is everywhere.

86
00:05:24,860 --> 00:05:28,220
Many people might recognize what this is.
Does anyone know what this is?

87
00:05:30,240 --> 00:05:35,130
Has anyone seen this before?
It is a promo user interface,

88
00:05:35,400 --> 00:05:39,060
early user interface for
the APP. Snapchat. Now,

89
00:05:39,061 --> 00:05:43,010
if you were to look at this knowing
nothing, sometimes I give this talk and I,

90
00:05:43,011 --> 00:05:47,310
I speak with people that haven't used
technology that much or only sort of

91
00:05:47,311 --> 00:05:50,910
casual users have a lot of apps and
so they've never seen this at all.

92
00:05:50,911 --> 00:05:55,860
And so I like to sort of pause and I say,
if you had never seen this before,

93
00:05:56,100 --> 00:06:00,350
what would you,
this user interface is,

94
00:06:00,351 --> 00:06:05,351
it is designed to accomplish and
some of the usual answers I get are,

95
00:06:06,471 --> 00:06:11,471
well it's probably something that relates
to taking and sending of a picture,

96
00:06:13,730 --> 00:06:18,500
right? We can tell that exists
for, we saw the timer here.

97
00:06:18,501 --> 00:06:23,290
Where does the time or probably do
if we had to guess. Yeah, it, it,

98
00:06:23,320 --> 00:06:27,200
it makes the, the picture of disappear
after maybe a certain amount of time.

99
00:06:27,200 --> 00:06:30,680
We see the scroll wheel that we can
select, right? So we clearly have options,

100
00:06:31,010 --> 00:06:34,250
right? It's clearly meant not just
for taking photos and storing them,

101
00:06:34,610 --> 00:06:38,450
but for sharing them.
We can tell because we look at,

102
00:06:38,900 --> 00:06:43,200
we see a sin button, right? It's got
the little arrow. And then one of,

103
00:06:43,240 --> 00:06:46,760
one of my favorite things about
this user interface and one of the,

104
00:06:46,790 --> 00:06:51,320
one of the most demonstrative sort of
signals that we see about the way in which

105
00:06:51,321 --> 00:06:56,030
this is supposed to be used.
What do you think this particular app,

106
00:06:56,150 --> 00:06:57,620
just looking at this,

107
00:06:58,310 --> 00:07:02,300
what kinds of pictures do you think
the APP is inviting you to send?

108
00:07:03,800 --> 00:07:06,620
Naughty pictures, right? It originally,

109
00:07:06,621 --> 00:07:10,100
this is how snapchat was sort of
perceived publicly, right? Uh,

110
00:07:10,101 --> 00:07:11,630
and you can't blame them because look,

111
00:07:11,631 --> 00:07:16,631
we've got an incredibly carefully cropped
a little square here where we can't

112
00:07:16,821 --> 00:07:19,550
tell whether these young women are
clothed or not. They're having fun.

113
00:07:19,820 --> 00:07:24,820
And the implicit message built in through
every aspect of design of this user

114
00:07:25,281 --> 00:07:30,281
interface is that you can trust this app
with things that you might not trust in

115
00:07:31,101 --> 00:07:34,970
more permanent sorts of social
media, right? Because it goes away.

116
00:07:35,120 --> 00:07:38,860
It is safer than if you were to use,
say,

117
00:07:39,100 --> 00:07:42,560
a some other service that, that where
the pictures are stored forever.

118
00:07:42,680 --> 00:07:46,970
And all of this is conveyed
with less than three words.

119
00:07:47,120 --> 00:07:48,110
It's all signals.

120
00:07:48,111 --> 00:07:53,060
It's all things that we intuitively
seem to understand through the design of

121
00:07:53,780 --> 00:07:57,500
information technologies. And I started
looking for examples for this book.

122
00:07:57,680 --> 00:08:00,950
And once you start to look for examples
of the ways in which design affects your

123
00:08:00,951 --> 00:08:05,810
privacy, you see them all over the place
and, and not just in in obvious ways,

124
00:08:05,811 --> 00:08:06,710
but in subtle ways.

125
00:08:06,950 --> 00:08:11,060
So I don't have to tell everybody
here with this technology is,

126
00:08:11,061 --> 00:08:13,410
and there were some design decisions that,
um,

127
00:08:13,790 --> 00:08:17,900
that went into Google glass and I
thought were incredibly interesting.

128
00:08:18,080 --> 00:08:23,000
The most obvious privacy relevant
feature for Google glass is what

129
00:08:25,310 --> 00:08:26,960
the camera on the front,
right? The one that's,

130
00:08:26,961 --> 00:08:29,930
it's literally staring straight at us.
Um,

131
00:08:30,290 --> 00:08:32,840
but that's not the design feature.
I mean,

132
00:08:32,841 --> 00:08:34,850
I guess you could have designed
glass without the camera.

133
00:08:34,851 --> 00:08:37,410
That was a design decision
that implicated, um,

134
00:08:37,460 --> 00:08:41,660
people's privacy because it's able
to be surveilled. But of course,

135
00:08:41,960 --> 00:08:46,960
the counter argument to that is this
camera doesn't really do a lot that isn't

136
00:08:48,381 --> 00:08:51,770
already being done by another camera.
Right.

137
00:08:51,771 --> 00:08:54,170
The one that everyone has in their phones.

138
00:08:54,500 --> 00:08:58,410
And so the design decision that's
relevant for Google glass is not just that

139
00:08:58,411 --> 00:08:59,640
there's a camera,

140
00:09:00,420 --> 00:09:05,040
but the camera is always
potentially visible.

141
00:09:05,460 --> 00:09:09,600
Right now there's a small transaction
cost, but it's significant at scale,

142
00:09:09,720 --> 00:09:12,480
which is in order to take a
picture with our normal phones,

143
00:09:12,630 --> 00:09:16,320
we have to reach into our phone,
pull it out, right, open it up,

144
00:09:16,321 --> 00:09:19,830
we fit a little bit with it, we
aim it, and then we take the photo.

145
00:09:19,980 --> 00:09:23,040
Now that doesn't seem like
a lot on a per time basis,

146
00:09:23,041 --> 00:09:28,041
but that small transaction cost actually
I think has a tendency to make people

147
00:09:29,011 --> 00:09:32,730
feel more comfortable with the fact that
everyone is carrying around a sort of

148
00:09:32,731 --> 00:09:36,510
pres, a persistent a
surveillance device in their,

149
00:09:36,930 --> 00:09:41,100
in their pockets all the time. Whereas
glass was net with a much more resistance,

150
00:09:41,101 --> 00:09:41,371
right?

151
00:09:41,371 --> 00:09:46,371
People tended to be more uncomfortable
with it than they did with their phones.

152
00:09:46,770 --> 00:09:49,590
And think the reason why is because
the camera was always there, right?

153
00:09:49,591 --> 00:09:53,610
The transaction costs of having to dig
into your pocket was suddenly gone and I

154
00:09:53,611 --> 00:09:58,170
had to do, was utter a command or um, uh,

155
00:09:58,590 --> 00:09:59,730
sort of readily activated.

156
00:09:59,731 --> 00:10:03,120
There was another design decision that
I thought was really relevant to Google

157
00:10:03,121 --> 00:10:05,100
glass.
They didn't get a lot of attention,

158
00:10:05,430 --> 00:10:09,480
but it's my understanding mid glass
did not provide support for facial

159
00:10:09,481 --> 00:10:11,010
recognition technologies,

160
00:10:11,310 --> 00:10:15,130
which was an interesting design decision.
Um,

161
00:10:15,180 --> 00:10:17,790
and one that I think also
had privacy implications,

162
00:10:17,791 --> 00:10:19,320
positive privacy implications.

163
00:10:19,410 --> 00:10:22,620
I don't have to tell anyone here
with this symbol is as well, right?

164
00:10:22,621 --> 00:10:25,140
But for those that aren't familiar,

165
00:10:25,290 --> 00:10:28,800
this is the symbol of incognito mode.

166
00:10:29,370 --> 00:10:34,200
When you open up Google chrome browser
and you go into incognito mode,

167
00:10:34,201 --> 00:10:37,560
you see, or at one point I believe
that the icon is now changed.

168
00:10:37,650 --> 00:10:42,480
But at one point you saw this icon
and I always like to ask people,

169
00:10:42,570 --> 00:10:46,500
this is a, this is a design, right?
It's a, it's a, it's a signal to people.

170
00:10:46,650 --> 00:10:51,150
And if you were to just see this,
what would you think the signal was?

171
00:10:51,930 --> 00:10:56,310
What is this meant to convey?
I'm sorry.

172
00:10:57,210 --> 00:11:00,120
Concealment, right, which is
of course it's the purpose.

173
00:11:00,121 --> 00:11:02,640
I mean it's called incognito
mode and we've got our,

174
00:11:02,641 --> 00:11:05,580
it seems like a sort of classic 1940s gum.
She right.

175
00:11:05,581 --> 00:11:09,630
Someone that doesn't want to be seen, they
put the, they can put their collar up,

176
00:11:09,631 --> 00:11:13,470
they've got a hat that they can
pull down low, maybe glasses.

177
00:11:14,460 --> 00:11:15,130
Again,
the,

178
00:11:15,130 --> 00:11:20,130
the implicit message just from looking
at this is that you're safer if you want

179
00:11:20,551 --> 00:11:25,160
to hide. Now, of course, that's
not exactly how incognito
mode works, right? And,

180
00:11:25,270 --> 00:11:28,560
and indeed, when you open
up incognito mode, um,

181
00:11:29,010 --> 00:11:31,560
you've got a list of things they say,
by the way,

182
00:11:31,561 --> 00:11:35,220
here are all the people that can still
see you when using incognito mode,

183
00:11:35,221 --> 00:11:39,330
which is a very important disclosure.
But absent that, you might think, oh,

184
00:11:39,331 --> 00:11:41,970
well I'm using the same
browser, right? The browser,

185
00:11:41,971 --> 00:11:45,930
that doesn't record any history of me at
all. Right? It's completely incognito.

186
00:11:46,110 --> 00:11:47,970
And so without that,
I think important disclosure,

187
00:11:47,971 --> 00:11:51,810
this design would tend to
shape people's expectations.

188
00:11:51,811 --> 00:11:53,440
And so when you look around,

189
00:11:53,441 --> 00:11:58,441
you tend to see the sort of
design decisions everywhere
that tend to be relevant

190
00:11:58,871 --> 00:12:02,200
for people's decisions about what
to disclose, how much to disclose,

191
00:12:02,350 --> 00:12:05,440
and how safe they are in disclosing.
Now,

192
00:12:05,441 --> 00:12:09,910
the second point that I want to make
about design, if the design is power,

193
00:12:11,380 --> 00:12:16,000
one of my favorites experiments that I
talk a little bit about in the book was

194
00:12:16,001 --> 00:12:17,320
done by Leslie John.

195
00:12:17,650 --> 00:12:22,000
I'm Alessandro queasy and George
Loewenstein at Carnegie Mellon University.

196
00:12:22,480 --> 00:12:27,480
And they got several people in to answer
this series of questions in a survey in

197
00:12:28,271 --> 00:12:28,991
a on a screen.

198
00:12:28,991 --> 00:12:32,350
And they sat down and they were presented
with a screen like this and they asked

199
00:12:32,351 --> 00:12:34,960
him a number of different questions
and this is just a snapshot.

200
00:12:35,260 --> 00:12:40,260
And the questions ranged from sort of
banal to relatively intimate to incredibly

201
00:12:40,631 --> 00:12:44,290
intimate. And this is just a short
little snapshot here. They said,

202
00:12:44,350 --> 00:12:47,140
have you ever smoked marijuana?

203
00:12:47,860 --> 00:12:50,770
Have you ever cheated
while in a relationship?

204
00:12:51,190 --> 00:12:55,300
Have you ever driven when you were
pretty sure you were over the legal blood

205
00:12:55,301 --> 00:12:59,770
alcohol level asking you to
admit to committing a crime? Now,

206
00:12:59,771 --> 00:13:03,940
what's the first thing that you
notice when I put this screen up?

207
00:13:05,080 --> 00:13:08,590
How bad are you?
Right?

208
00:13:08,620 --> 00:13:11,520
And so there were a series of design
decisions that I think were particularly

209
00:13:11,521 --> 00:13:12,400
relevant here.

210
00:13:12,820 --> 00:13:17,110
One is the fact that bad is
sort of capitalized, right?

211
00:13:17,111 --> 00:13:20,740
And then they use the sort of cutesy,
you sort of tech speak, right?

212
00:13:20,741 --> 00:13:24,400
How bad are you?
Multiple question marks,

213
00:13:24,401 --> 00:13:27,490
which is indicative of what youth,

214
00:13:27,610 --> 00:13:29,410
right exuberance.

215
00:13:29,710 --> 00:13:33,100
What else do we have
relevant to design here?

216
00:13:34,600 --> 00:13:39,150
The logo, right? I said devil,
but it's not a bad devil.

217
00:13:39,151 --> 00:13:42,250
It's like an Emoji devil.
Like acute devil, right?

218
00:13:42,700 --> 00:13:46,150
There's one other design decision
that I thought was really interesting.

219
00:13:46,810 --> 00:13:47,680
The font,

220
00:13:48,040 --> 00:13:52,420
it is written in Comic Sans and no one
in the history of the world is taking

221
00:13:52,421 --> 00:13:56,410
anything seriously written
in Comic Sans Font, right?

222
00:13:56,920 --> 00:14:01,090
And the overall implication, of course,
of all of these design decisions,

223
00:14:01,091 --> 00:14:04,030
despite asking incredibly
intimate questions,

224
00:14:04,750 --> 00:14:09,220
questions that can implicate
you in a crime, is that you're,

225
00:14:09,310 --> 00:14:12,100
how bad are you? You're
bad. You know, you're bad.

226
00:14:12,101 --> 00:14:13,780
Everyone was just a little bit bad,
right?

227
00:14:13,781 --> 00:14:18,730
That's sort of the implication of every
single design feature built into this.

228
00:14:18,910 --> 00:14:22,030
And then of course the control group,
they had looked like this.

229
00:14:23,800 --> 00:14:26,450
Gone is the sand Saraf is,

230
00:14:26,550 --> 00:14:31,000
is the Comic Sans Font replaced with
this, you know, very formal sand Saraf.

231
00:14:31,210 --> 00:14:31,750
You know,

232
00:14:31,750 --> 00:14:36,750
funds gone is our cute little question
about how bad are you and now it's just

233
00:14:37,211 --> 00:14:40,240
replaced with the imprimatur
of Carnegie Mellon University,

234
00:14:40,241 --> 00:14:42,520
a very prestigious university.
Right.

235
00:14:43,090 --> 00:14:47,530
But the question's remains
exactly the same. Right?

236
00:14:48,250 --> 00:14:48,670
And,

237
00:14:48,670 --> 00:14:52,760
and I want to read directly because I
don't want to miss quotes the results of

238
00:14:52,761 --> 00:14:54,230
their study.
Um,

239
00:14:54,260 --> 00:14:59,260
but they call this the frivolous
looking interface in this,

240
00:15:00,261 --> 00:15:04,890
the non frivolous looking interface.
And uh,

241
00:15:05,070 --> 00:15:09,860
they found that relative to the non
frivolous interface participants in the

242
00:15:09,861 --> 00:15:13,040
frivolous looking survey that
asked identical questions,

243
00:15:13,310 --> 00:15:18,310
we're on average 1.7 times more likely
to admit having engaged in risky

244
00:15:18,651 --> 00:15:20,420
behaviors.
For example,

245
00:15:20,421 --> 00:15:25,421
a participant in the frivolous looking
survey was on average 2.03 times more

246
00:15:26,241 --> 00:15:30,650
likely to admit having ever taken
nude pictures of himself or a partner.

247
00:15:30,800 --> 00:15:32,660
And the authors conclude people,
it seems,

248
00:15:32,661 --> 00:15:37,190
feel more comfortable providing personal
information on unprofessional sites

249
00:15:37,370 --> 00:15:40,910
that are arguably, uh,
particularly likely to misuse it.

250
00:15:41,900 --> 00:15:46,900
Design is power design doesn't
necessarily dictate behavior,

251
00:15:47,871 --> 00:15:52,871
but it channels its every single design
decision makes a certain reality more or

252
00:15:54,141 --> 00:15:56,800
less likely, right? And so it,

253
00:15:56,801 --> 00:15:59,270
it provides an incredible amount of power.

254
00:15:59,510 --> 00:16:02,030
And that leads me to my third point,

255
00:16:02,330 --> 00:16:06,950
which is that design is political
design is always political.

256
00:16:06,951 --> 00:16:10,520
And when I say political, I don't mean
political in terms of Capitol Hill.

257
00:16:10,670 --> 00:16:13,610
I mean political in terms of
the distribution of power.

258
00:16:14,510 --> 00:16:18,620
So I give this talk a little
and sometimes in response,

259
00:16:18,621 --> 00:16:20,420
people will come up
afterwards and they'll say,

260
00:16:20,600 --> 00:16:25,190
why are you targeting design as something
that we should be focused about in

261
00:16:25,191 --> 00:16:30,140
law? Right? Instead of targeting
the tools that we use to create, to,

262
00:16:30,470 --> 00:16:33,830
to create problems, why don't we
target the harmful problems themselves?

263
00:16:34,100 --> 00:16:38,150
One of the examples that's been put
forth recently is a knife can be used for

264
00:16:38,151 --> 00:16:41,270
good or bad, but we don't
ban knives, Ryan and said,

265
00:16:41,271 --> 00:16:44,660
we say you can't use a knife to stab
someone, but you can use it to cut food.

266
00:16:45,320 --> 00:16:49,900
And so, um, and so it's always some
sort of version of the argument that,

267
00:16:49,901 --> 00:16:53,990
that there are no bad is only
bad uses of those technologies.

268
00:16:53,991 --> 00:16:58,790
And that should be the focus
of our policy and law efforts.

269
00:16:58,791 --> 00:17:03,380
But I think that that
actually, um, tends to, uh,

270
00:17:03,620 --> 00:17:07,970
cut against the fact that I don't think
that there's such a thing as a neutral

271
00:17:07,971 --> 00:17:12,971
technology given the fact that every
single design decision is meant to affect

272
00:17:14,211 --> 00:17:16,380
the world, right? That's what design is.

273
00:17:16,381 --> 00:17:21,381
It's a series of decisions that we get
reflected in some sort of substance that

274
00:17:21,471 --> 00:17:24,200
affects the world and
in some particular way,

275
00:17:24,201 --> 00:17:26,480
then there's no such thing
as neutral technologies.

276
00:17:26,660 --> 00:17:31,340
Even ignoring certain realities is an
acceptance of those realities in a certain

277
00:17:31,341 --> 00:17:32,610
sense.
Um,

278
00:17:32,630 --> 00:17:36,830
and so we cannot ignore
technology in the role of design,

279
00:17:37,070 --> 00:17:41,240
in law and policy. We have to
confront it head on. Um, and there,

280
00:17:41,300 --> 00:17:46,300
there may be reasons why we explicitly
decided not to have rules against

281
00:17:46,880 --> 00:17:50,550
legislating technology. We don't
want regulators, for example,

282
00:17:50,551 --> 00:17:55,290
dictating from top to bottom, every single
design decision in a ham fisted way,

283
00:17:55,560 --> 00:17:57,830
right? Because, you know, it's,

284
00:17:57,840 --> 00:18:00,810
it's difficult to think that they
would know better how to build a lot of

285
00:18:00,811 --> 00:18:04,020
particulars in technologies
and people that live it. Um,

286
00:18:04,350 --> 00:18:06,750
but that too has its own costs.
Right?

287
00:18:06,751 --> 00:18:10,380
And we need to be specific about
which costs we want to embrace, right?

288
00:18:10,381 --> 00:18:14,550
And how to better balance them with the
full range of concerns that we have.

289
00:18:14,551 --> 00:18:19,551
And the only way that we're going to
do that is if we explicitly embrace the

290
00:18:19,621 --> 00:18:24,621
design of technology and how that is
powerful in our laws and policies.

291
00:18:26,490 --> 00:18:29,460
So that leaves me to my next point,

292
00:18:29,461 --> 00:18:34,461
which is that privacy law should take
design more seriously because right now it

293
00:18:35,881 --> 00:18:40,770
doesn't, it may seem like it relatively
does and there may be some, uh,

294
00:18:40,771 --> 00:18:44,910
in this room are listening in online
that have had experience dealing with

295
00:18:45,060 --> 00:18:46,350
regulators and the,

296
00:18:46,380 --> 00:18:51,380
with the European Union and the general
data protection regulation that deal

297
00:18:51,841 --> 00:18:55,110
with privacy by design and
privacy by default. Um,

298
00:18:55,140 --> 00:19:00,140
but actually I will argue that largely
privacy law and privacy policy and the

299
00:19:00,811 --> 00:19:05,811
rules that we have even internally about
privacy have a major design gap turns

300
00:19:07,231 --> 00:19:07,471
out.

301
00:19:07,471 --> 00:19:12,471
So I teach privacy law at northeastern
university and it's a whole semester long

302
00:19:12,631 --> 00:19:12,961
course,

303
00:19:12,961 --> 00:19:16,830
but it turns out there are basically
only three privacy rules that you have to

304
00:19:16,831 --> 00:19:20,190
follow. Um, and so I can
save you all a lot of money.

305
00:19:20,191 --> 00:19:21,930
If you're interested in
taking a private slot course.

306
00:19:21,931 --> 00:19:23,310
I'm just gonna summarize it now.

307
00:19:23,610 --> 00:19:28,610
One of the rules is follow the
fair information practices.

308
00:19:28,860 --> 00:19:33,860
This is the underlying ethos of the
entire general data protection regulation.

309
00:19:35,610 --> 00:19:39,380
It's the, it's the idea behind, uh,

310
00:19:39,600 --> 00:19:42,210
statutes that you are
probably very familiar with,

311
00:19:42,211 --> 00:19:46,480
like the fair credit reporting
acts. Um, oh, even HIPAA has a,

312
00:19:46,481 --> 00:19:48,420
a sense of,
uh,

313
00:19:48,421 --> 00:19:52,380
follow the fair information practices
and the fair information practices are

314
00:19:52,381 --> 00:19:56,610
things that are so fundamental to our
understanding of fairness with data

315
00:19:56,611 --> 00:20:00,330
processing that you probably have already
internalized them without thinking

316
00:20:00,331 --> 00:20:01,560
about them.
For example,

317
00:20:01,710 --> 00:20:06,350
people should have control over
their information. They should, uh,

318
00:20:06,360 --> 00:20:09,690
databases should be transparent
about what they're collecting.

319
00:20:09,691 --> 00:20:13,110
They should give people notice about what
they're collecting and the ability to

320
00:20:13,111 --> 00:20:16,830
delete information or correct information
that you should only collect what you

321
00:20:16,831 --> 00:20:19,830
need and that you should delete it
when you're done with it. Right?

322
00:20:19,831 --> 00:20:21,540
The data minimization principle,

323
00:20:21,780 --> 00:20:24,900
there's the principle of accountability
that we should be accountable for these

324
00:20:24,901 --> 00:20:28,270
rules. There's the
principal, um, that, uh,

325
00:20:28,440 --> 00:20:32,400
people should have certain sort of access
rights for portability rights to their

326
00:20:32,401 --> 00:20:32,970
data.

327
00:20:32,970 --> 00:20:36,510
These are all things that we've been
talking about for a long time and they all

328
00:20:36,511 --> 00:20:39,570
started with the fair
information practices,

329
00:20:39,571 --> 00:20:44,070
which are actually originated in the mid
and late 1970s and then made their way

330
00:20:44,071 --> 00:20:49,071
through and now are as close as the
world has come to a common language of

331
00:20:51,821 --> 00:20:52,654
privacy,

332
00:20:52,990 --> 00:20:57,460
which is pretty remarkable
when you consider all the
varying different cultural

333
00:20:57,550 --> 00:21:00,730
connotations that play into
conceptions of privacy.

334
00:21:00,940 --> 00:21:05,770
The fact that we have a basic set of
rules on how to fairly collect and process

335
00:21:05,771 --> 00:21:09,490
data is remarkable and
the fips are important.

336
00:21:09,610 --> 00:21:13,660
They're necessary.
But here's the problem with the Phipps,

337
00:21:15,730 --> 00:21:17,350
they look like this,
right?

338
00:21:17,470 --> 00:21:21,220
This is the way in which
they get reflected because
if you look at the Phipps

339
00:21:21,400 --> 00:21:26,400
and almost always boils down
to this idea of control,

340
00:21:27,310 --> 00:21:32,310
if you ask the CEOs of every major tech
company in the United States in the

341
00:21:32,441 --> 00:21:35,830
world, how do you respect
the privacy of your users?

342
00:21:35,950 --> 00:21:40,950
I bet money that they were almost all
respond with some version of we put people

343
00:21:42,971 --> 00:21:45,940
in control of their own information.

344
00:21:46,810 --> 00:21:50,950
This is one of the most
dominant conceptualizations
of privacy in the United

345
00:21:50,950 --> 00:21:52,090
States and around the world.

346
00:21:52,300 --> 00:21:56,080
The idea being that if we give users
control of their information and they get

347
00:21:56,081 --> 00:22:00,400
to decide for themselves what they want
to do with their information and what

348
00:22:00,401 --> 00:22:01,300
they're okay with,

349
00:22:01,480 --> 00:22:05,860
they do a risk calculation and
if they are given that control,

350
00:22:06,010 --> 00:22:07,300
then if they exercise it,

351
00:22:07,301 --> 00:22:12,301
then presumably the giving of that control
is enough to justify certain sorts of

352
00:22:13,091 --> 00:22:16,120
data practices cause people approve
it, right? They have control over it.

353
00:22:16,450 --> 00:22:20,590
Data Subjects,
autonomy in theory is respected.

354
00:22:21,370 --> 00:22:24,220
The, the uh, platform.

355
00:22:24,221 --> 00:22:28,810
Collecting information has done its moral
duty under this conceptualization and

356
00:22:28,811 --> 00:22:31,720
my offering, the control and
we can all proceed together.

357
00:22:32,490 --> 00:22:35,260
And here's the problem with that.
And here's a problem with control.

358
00:22:35,280 --> 00:22:39,130
Think of thinking privacy in
terms of control generally,
which is really popular,

359
00:22:39,250 --> 00:22:43,870
not just an industry. Governments
conceptualize privacy is control.

360
00:22:43,990 --> 00:22:46,270
Advocates conceptualize privacy's control.

361
00:22:46,271 --> 00:22:51,271
It is by far probably the most
popular way to define privacy.

362
00:22:52,960 --> 00:22:57,960
I teach privacy law and on the first day
I asked my students to write out what

363
00:22:58,931 --> 00:23:01,330
privacy is and say,
what is privacy?

364
00:23:01,331 --> 00:23:05,920
And then we go around the room and I
have 20 to 30 students and I usually get

365
00:23:05,921 --> 00:23:08,470
around five to 10 different answers,

366
00:23:08,620 --> 00:23:13,150
but almost always the most popular
response is control over personal

367
00:23:13,151 --> 00:23:15,280
information.
But again,

368
00:23:16,390 --> 00:23:20,800
the way that control is reflected
in practice is through design.

369
00:23:21,160 --> 00:23:25,150
It is through buttons,
right? Toggle switches,

370
00:23:25,330 --> 00:23:29,410
which in theory is still fine.
Everyone knows what this is.

371
00:23:29,650 --> 00:23:33,970
Green means yes.
Gray means no and it's a toggle switch.

372
00:23:34,180 --> 00:23:38,590
Can Google maps collect your
Geo location information?

373
00:23:39,010 --> 00:23:43,090
Green means yes. Gray means no.
Yes, I have exercised control.

374
00:23:43,180 --> 00:23:46,730
I have made a decision. Privacy
is respected. Onward. We go,

375
00:23:47,420 --> 00:23:51,920
except of course it doesn't always work
out like that because it never looks

376
00:23:51,921 --> 00:23:53,840
just like that does it.
Right?

377
00:23:55,190 --> 00:24:00,190
We have a series of decisions to make
because we don't want the decision to be

378
00:24:00,741 --> 00:24:03,410
just binary.
If it's too binary,

379
00:24:03,411 --> 00:24:08,360
it's two abstracted and I
privacy is complicated. There
are nuances, right? Yes.

380
00:24:08,361 --> 00:24:12,260
No doesn't give enough choices. So
we say, okay, I need lots of choices.

381
00:24:12,261 --> 00:24:14,330
I want nuance, right? This is,

382
00:24:14,480 --> 00:24:19,160
this is the inherent tension in defining
privacy is control is give me nuance.

383
00:24:19,580 --> 00:24:22,310
So tech companies respond and they say,
okay,

384
00:24:22,370 --> 00:24:24,170
we'll give you lots of different choices,
right?

385
00:24:24,170 --> 00:24:27,650
Rather than just sort of one off by an
area, we'll give you lots of choices.

386
00:24:27,651 --> 00:24:32,651
Except of course that means that users
are now responsible for analyzing those

387
00:24:33,261 --> 00:24:35,470
choices. And you say, okay, I've got,

388
00:24:35,630 --> 00:24:38,900
I've got that for the find my iPad
button and in location based alerts,

389
00:24:38,901 --> 00:24:41,960
I guess that's good. Location based
I ads, I don't know what that means.

390
00:24:42,140 --> 00:24:46,030
Share my location green. Okay. I guess
I guess I'll try to figure that one out.

391
00:24:46,220 --> 00:24:49,910
Wifi network. I don't know. I don't know
what that but does, but I guess it's okay.

392
00:24:50,240 --> 00:24:54,290
Um, diagnostics and usage. Peep
popular near me. I guess. That's good.

393
00:24:54,500 --> 00:24:57,740
And we have to make all of these sorts
of implicit risk calculations over and

394
00:24:57,741 --> 00:25:02,390
over again, which is costly in
terms of personal resources, right?

395
00:25:02,391 --> 00:25:07,010
Because people have limited amounts
of time. So we go through, but okay.

396
00:25:07,011 --> 00:25:08,180
So let's say we look at this,

397
00:25:08,181 --> 00:25:12,740
we make our design is set up in this
way for us to to effectuate the control

398
00:25:12,741 --> 00:25:17,060
that we're given. We do it and
we say, Ooh, okay, I think I'm,

399
00:25:17,090 --> 00:25:21,260
I think I'm good. I think I've set my
settings the way I want them. And then we,

400
00:25:21,830 --> 00:25:24,950
we take a deep breath and we
back up and we say, Oh dear,

401
00:25:25,490 --> 00:25:28,820
I've got lots of buttons to
press over and over. Right?

402
00:25:29,490 --> 00:25:31,730
And when the settings change,
I've got to update them again.

403
00:25:32,450 --> 00:25:37,450
The problem with thinking about privacy
in terms of control is that people are

404
00:25:37,671 --> 00:25:42,470
gifted with so much control that they
choke on it and it becomes in practice

405
00:25:42,471 --> 00:25:47,450
away for a risk of loss
to be shifted onto users.

406
00:25:47,720 --> 00:25:50,270
Because if you fail to
exercise that control,

407
00:25:51,440 --> 00:25:54,770
it's not the problem of the company.
They gave you the control, right?

408
00:25:54,800 --> 00:25:58,050
They're giving you the options,
that's what you wanted. And so it's,

409
00:25:58,130 --> 00:26:02,540
there's an inherent tension but
almost as never going to be resolved,

410
00:26:02,750 --> 00:26:06,530
which is if you abstracted
away and make control easy,

411
00:26:06,590 --> 00:26:10,430
if you provide one button
for all of our our issues,

412
00:26:10,670 --> 00:26:15,080
then it becomes too generalized right
there too. Too much stuff washed in.

413
00:26:15,081 --> 00:26:20,000
And so our ability to do a,
a meaningful risk calculus is washed away.

414
00:26:20,510 --> 00:26:25,090
But if you provide every option under
the sun, then it becomes overburdensome,

415
00:26:25,100 --> 00:26:28,520
right? It's a, it's a d dos
attack on our brain and there's,

416
00:26:28,630 --> 00:26:30,770
there's no way that we can respond.
It's scale.

417
00:26:31,190 --> 00:26:35,370
And so control I think is actually one of,
uh,

418
00:26:35,480 --> 00:26:40,480
fundamentally one of the
misguided actually ways to
think about privacy in the

419
00:26:40,941 --> 00:26:45,090
modern data ecosystem because it's never
going to, it just doesn't scale. Right?

420
00:26:45,210 --> 00:26:48,480
And left we started having a conversation
about prioritize your control,

421
00:26:48,481 --> 00:26:51,000
which gets really interesting,
which we can talk about later.

422
00:26:52,380 --> 00:26:56,070
So there's this massive design
gap in follow the Phipps,

423
00:26:56,071 --> 00:26:59,850
this ethic that we follow in all privacy
law and and all and industry sort of

424
00:26:59,851 --> 00:27:00,211
rules,

425
00:27:00,211 --> 00:27:05,211
which is you should give people control
because design is a leveraged to instead

426
00:27:05,461 --> 00:27:09,990
of being autonomy enhancing and actually
it's autonomy, corrosive, right?

427
00:27:09,990 --> 00:27:12,960
Because we're given all of these choices
were sort of burdening them or it's

428
00:27:12,961 --> 00:27:17,040
abstracted away to the point where it's
meaningless. All right? That's rule one.

429
00:27:17,580 --> 00:27:22,200
Rule Two is do not lie
relatively simple rule, right?

430
00:27:22,440 --> 00:27:23,610
The Federal Trade Commission,

431
00:27:23,611 --> 00:27:28,611
which is the nation's top a regulator
of privacy has a authority to regulate

432
00:27:29,611 --> 00:27:34,230
unfair and deceptive trade practices
and as part of its regulation,

433
00:27:34,231 --> 00:27:36,990
it has one major rule is just
don't lie to people, right?

434
00:27:37,230 --> 00:27:41,880
That sounds relatively easy. The problem
with the do not lie ethos of course,

435
00:27:42,090 --> 00:27:46,560
is one of the similar problems to
transparency and control generally,

436
00:27:46,561 --> 00:27:50,220
which is you can put technical truths
in a place that nobody's going to read

437
00:27:50,221 --> 00:27:54,480
them and we all know where you put the
things where you nobody to read it.

438
00:27:54,810 --> 00:27:55,740
All right,
where do we put it?

439
00:27:57,330 --> 00:28:02,100
If I want to be sure to put something
in a place where I know no users ever

440
00:28:02,101 --> 00:28:03,660
going to see it,
where am I going to stick it?

441
00:28:04,530 --> 00:28:08,880
The privacy policy or the terms of
use, right. I do this for a living.

442
00:28:09,090 --> 00:28:14,090
I am a a a privacy law professor and I
cannot scroll through the terms of use

443
00:28:17,041 --> 00:28:19,230
quick enough to get to, I agree, right?

444
00:28:20,010 --> 00:28:23,760
Because of course there's no way that it
that at scale anyone could collectively

445
00:28:23,761 --> 00:28:24,594
read them.
Right?

446
00:28:24,640 --> 00:28:28,080
It'd be we'd have to take two days off
of vacation a year just to read all the

447
00:28:28,081 --> 00:28:31,470
privacy policies we come
into and so do not lie.

448
00:28:31,471 --> 00:28:33,870
Can also get circumvented through design,

449
00:28:34,140 --> 00:28:38,100
through putting it in a place where
technical truth is is ignored,

450
00:28:38,700 --> 00:28:42,180
and then there's the do not harm ethic.
That's the last one.

451
00:28:42,480 --> 00:28:45,150
So the Federal Trade
Commission also says don't do,

452
00:28:45,300 --> 00:28:48,600
don't engage in unfair trade practices.

453
00:28:48,601 --> 00:28:53,601
That's harm users in a way that's a not
avoidable by the users themselves and in

454
00:28:53,701 --> 00:28:56,970
a way that's not out balanced by
countervailing benefits to the consumer.

455
00:28:56,970 --> 00:29:01,860
That's the sort of tests they use. But
of course, the do not harm ethic is also,

456
00:29:02,030 --> 00:29:05,880
uh, easily subverted through
design because in the modern age,

457
00:29:06,570 --> 00:29:11,400
the threshold for harm to rise to a
legal violation is sort of high, right?

458
00:29:11,790 --> 00:29:16,410
Um, they, if, if you're going to
bring a lawsuit against the company,

459
00:29:16,411 --> 00:29:20,700
then you don't want the harm
to be a, this is creepy, right?

460
00:29:20,730 --> 00:29:22,960
Which is some of the things that
happen, right? And creepy. It's not a,

461
00:29:23,070 --> 00:29:26,700
it's not a real word that has boundaries
around, right. What is creepy?

462
00:29:26,701 --> 00:29:31,500
Lots of things could possibly creepy.
Um, but that varies wildly according,

463
00:29:31,501 --> 00:29:32,790
you know,
across the spectrum.

464
00:29:33,090 --> 00:29:36,570
And so the law and in many
ways rightly actually says,

465
00:29:36,571 --> 00:29:38,490
you've got to have a
real tangible harm here.

466
00:29:38,491 --> 00:29:42,280
You've got to have some sort of financial
harm, right? If someone, your identity,

467
00:29:42,281 --> 00:29:46,420
someone stole your money, or you have to
have some sort of clear emotional harm,

468
00:29:46,630 --> 00:29:51,190
right? And often it is your naked body
was exposed publicly, right? Or something,

469
00:29:51,310 --> 00:29:54,760
something very visceral and something
that, that has a clear boundary to it.

470
00:29:55,150 --> 00:29:56,050
But of course,

471
00:29:56,500 --> 00:30:00,580
many of the sorts of things that people
quantify as privacy harms in the modern

472
00:30:00,581 --> 00:30:04,960
age is not actually those
sorts of visceral harms,

473
00:30:04,961 --> 00:30:06,520
which can be relatively rare,

474
00:30:06,521 --> 00:30:10,150
but rather it's a little bit more of
a death by a thousand cuts, right?

475
00:30:10,240 --> 00:30:13,240
We reveal a little bit of information
here and we trickle a little bit of

476
00:30:13,241 --> 00:30:15,310
information there,
none of it,

477
00:30:15,640 --> 00:30:19,180
which rises to the level of what we
would consider to be a privacy harm.

478
00:30:19,540 --> 00:30:24,220
But collectively we look up one day
and we all become vulnerable, right?

479
00:30:24,250 --> 00:30:25,800
And we all see that that's,

480
00:30:25,810 --> 00:30:29,770
we've gone down this road without actually
having any one particular violation.

481
00:30:29,771 --> 00:30:33,430
Yeah. And here we are with lots of our
information sort of expose that can,

482
00:30:33,670 --> 00:30:35,920
that could ultimately
be leveraged against us.

483
00:30:36,130 --> 00:30:40,060
And so design also sort of does that
through sort of the way in which we

484
00:30:40,061 --> 00:30:44,670
encourage short little
amounts of information to be
disclosed here. And they, oh,

485
00:30:44,680 --> 00:30:47,470
it's just a little information here.
It's just a little information there.

486
00:30:47,740 --> 00:30:51,100
But collectively it becomes a big issue.
All right,

487
00:30:51,310 --> 00:30:54,760
so at this point you may be saying,
all right, smart Guy Wolf. This,

488
00:30:54,761 --> 00:30:56,410
we've got the problems,
what are we going to do about it?

489
00:30:56,411 --> 00:30:58,300
And that actually is the
next part of the book,

490
00:30:58,510 --> 00:31:00,610
which is why I proposed
a theory of privacy,

491
00:31:00,611 --> 00:31:05,380
law and design and the theory
is actually built around values,

492
00:31:05,381 --> 00:31:10,381
boundaries and tools and the values that
I advocate for or actually not control.

493
00:31:12,580 --> 00:31:14,290
Even though that's the dominant framework,

494
00:31:14,320 --> 00:31:17,590
I think there are better
values that we can embrace.

495
00:31:17,620 --> 00:31:22,620
Not only that will give us industry
in US law a much clearer identity with

496
00:31:24,431 --> 00:31:28,750
respect to protecting personal information
but also one that doesn't sort of

497
00:31:28,751 --> 00:31:33,010
inherently have this tension where you
have to choose between a meaningless

498
00:31:33,011 --> 00:31:38,011
abstraction or overwhelming nuance and
that's trusted security and autonomy.

499
00:31:38,410 --> 00:31:39,243
In other words,

500
00:31:39,640 --> 00:31:44,640
I argue that our law and policy should
encourage design that is trustworthy that

501
00:31:46,331 --> 00:31:50,290
promotes or at least values
obscurity and enhance his autonomy.

502
00:31:50,470 --> 00:31:51,490
Now what do I mean by that?

503
00:31:52,660 --> 00:31:57,250
Trust I think is one of the most
important values, uh, in the modern age.

504
00:31:57,251 --> 00:32:00,790
With respect to the disclosure of
personal information in platforms,

505
00:32:00,820 --> 00:32:02,860
trust is key for commerce.

506
00:32:02,950 --> 00:32:06,400
Trust is key for personal
relationships and intimacy.

507
00:32:06,550 --> 00:32:08,920
Trust is key for self exploration.

508
00:32:08,980 --> 00:32:12,260
We tend not to disclose
to other people in a,

509
00:32:12,270 --> 00:32:15,340
in an attempt to sort of figuring
out who we are unless we trust those,

510
00:32:15,490 --> 00:32:19,870
if everything we say can be used against
us and we tend not to sort of engage in

511
00:32:19,871 --> 00:32:21,040
self exploration.

512
00:32:21,490 --> 00:32:26,490
And so our design should reflect that
our design should reflect a sense of

513
00:32:26,591 --> 00:32:30,460
discretion, not necessarily
confidentiality, but just discretion.

514
00:32:30,580 --> 00:32:34,420
We don't disclose everything all the time,
right? This can be de identification,

515
00:32:34,630 --> 00:32:37,590
this is gonna be disclosure
within a limited community. Um,

516
00:32:37,690 --> 00:32:42,230
it should encourage a sense of protection.
Now this is a little more obvious,

517
00:32:42,231 --> 00:32:42,501
right?

518
00:32:42,501 --> 00:32:47,501
Which is a don't store people's passwords
in clear text and salt and Hash and

519
00:32:48,170 --> 00:32:50,390
uh, let's encrypt traffic.
We should protect the data.

520
00:32:50,391 --> 00:32:52,730
This is what we would refer to.
Maybe it's data security,

521
00:32:53,270 --> 00:32:57,650
a sense of honesty and honesty
is different from transparency.

522
00:32:58,310 --> 00:33:03,050
Transparency often gets touted as, look
at we, we've opened up the books to you,

523
00:33:03,410 --> 00:33:07,340
right? We, you can look inside and
you can see whatever is available.

524
00:33:07,341 --> 00:33:11,480
But that's actually different than what
I would consider to be acting honestly,

525
00:33:11,720 --> 00:33:16,720
honestly is affirmatively disclosing
the things that users want to know that

526
00:33:16,911 --> 00:33:21,170
maybe you would prefer not to tell them
that they should probably know. Right?

527
00:33:21,380 --> 00:33:24,950
In other words, it's, it's a little
bit more of affirmative obligation, um,

528
00:33:25,350 --> 00:33:30,080
as a warning rather than
full transparency. And then
finally, and this is the,

529
00:33:30,260 --> 00:33:33,650
this is the hard one with
respect to trust. Um,

530
00:33:33,750 --> 00:33:35,930
design should be loyal to the user.

531
00:33:36,230 --> 00:33:40,880
And what I mean by that is it shouldn't
elevate the interests of the platform

532
00:33:40,881 --> 00:33:45,560
unreasonably in or an unreasonable ways
over the interest of the data subject.

533
00:33:46,350 --> 00:33:49,400
And we can talk about what I mean
by that. Uh, Eh, in a minute. And,

534
00:33:49,401 --> 00:33:54,080
and that would be trustworthy design
design that is discrete, protective,

535
00:33:54,260 --> 00:33:55,640
honest and loyal.

536
00:33:56,810 --> 00:34:01,810
Now another sort of a
design choice which doesn't,

537
00:34:03,110 --> 00:34:08,110
isn't quite as established in law and
policy is the value of obscurity obscurity

538
00:34:09,531 --> 00:34:14,480
is the idea that when information is hard
are unlikely to be found or understood

539
00:34:14,840 --> 00:34:17,750
than it is to a relative degree.
Safe.

540
00:34:18,410 --> 00:34:23,180
We rely upon zones of obscurity
all the time in our everyday lives.

541
00:34:23,181 --> 00:34:24,500
And we don't even realize it.

542
00:34:24,740 --> 00:34:29,740
How many people here have eaten out at
a restaurant within the last two weeks?

543
00:34:32,630 --> 00:34:33,463
A number of people.

544
00:34:33,890 --> 00:34:38,390
Do any of you remember who was
sitting two tables away from you?

545
00:34:39,650 --> 00:34:42,590
Probably not. You were in
public. Everyone can see you.

546
00:34:42,680 --> 00:34:46,100
But of course we've delong
since deleted that information.

547
00:34:46,310 --> 00:34:48,440
I flew here yesterday on an airplane.

548
00:34:48,470 --> 00:34:51,860
I don't remember what the person
sitting next to me looked like, right?

549
00:34:51,890 --> 00:34:52,970
Even though it was in public.

550
00:34:52,971 --> 00:34:57,971
And we do this to help sort of ease to
prevent cognitive overburdening and we

551
00:34:58,131 --> 00:35:02,600
rely upon this risk calculus all
the time to make decisions. Um,

552
00:35:02,601 --> 00:35:07,220
when I'm walking out in public,
um, you know, if, if you know,

553
00:35:07,880 --> 00:35:09,650
you quickly want to sort of scratch in a,

554
00:35:09,651 --> 00:35:13,340
in a delicate place and you look around,
no one's looking right, you do it right.

555
00:35:13,341 --> 00:35:17,180
Even though maybe if that were
posted on the walls, the, uh,

556
00:35:17,240 --> 00:35:20,840
the Times Square Jumbo Tron, then you
would have second thoughts about it.

557
00:35:20,841 --> 00:35:22,910
But the odds of that are incredibly small.

558
00:35:23,390 --> 00:35:28,140
When you go shopping in a
grocery store or a drugstore,

559
00:35:28,590 --> 00:35:31,910
um, you're in public, you're picking
out things that are delicate,

560
00:35:31,911 --> 00:35:35,360
but the odds that someone is standing
right behind you writing down every single

561
00:35:35,361 --> 00:35:38,520
thing that you purchase is incredibly low.
Right?

562
00:35:38,521 --> 00:35:41,340
And the odds of that ever coming back
to hurt, she was also incredibly low.

563
00:35:41,550 --> 00:35:46,550
So we value obscurity and the
harm comes when our obscurity is,

564
00:35:46,921 --> 00:35:50,820
is taken away from us in dramatic ways.
In other words, they're lurches obscurity.

565
00:35:50,821 --> 00:35:52,800
Lurches um,
I've been,

566
00:35:52,801 --> 00:35:56,880
I've written a very clear critically
about facial recognition technologies and

567
00:35:56,881 --> 00:36:00,600
one of the reasons why as I view
it as an incredible obscurity,

568
00:36:00,601 --> 00:36:04,140
lurch our faces while public,
um,

569
00:36:05,250 --> 00:36:08,940
basically allow us still to be relatively
private as we walk into a crowd.

570
00:36:09,210 --> 00:36:12,510
But facial recognition technology is
threatened. That ability, uh, it, it,

571
00:36:12,511 --> 00:36:16,400
it represents a dramatic
obscurity, lurch and online. Uh,

572
00:36:16,410 --> 00:36:17,970
some of my research is focused on,

573
00:36:17,971 --> 00:36:21,930
there are things that can obscure you
or make you more obvious like search

574
00:36:21,931 --> 00:36:24,870
visibility, unprotected access, right?

575
00:36:24,871 --> 00:36:27,870
Whether we protect something
with a password, um,

576
00:36:27,871 --> 00:36:31,470
whether someone uses their
real name or a pseudonym or no,

577
00:36:31,620 --> 00:36:35,250
no identifier at all and
whether things are clear, right?

578
00:36:35,251 --> 00:36:38,970
Sometimes things are obscure because
people like the context to make sense of

579
00:36:38,971 --> 00:36:43,530
them. If I were to tell you all
right now, um, hey, by the way,

580
00:36:43,680 --> 00:36:46,320
I just got the test results back.
It's positive.

581
00:36:46,650 --> 00:36:49,500
You would have no idea what that means,
right?

582
00:36:49,530 --> 00:36:53,490
It's obscurity you and it's safe to me
because you have no idea what that means.

583
00:36:53,810 --> 00:36:57,150
Um,
but if you had the backstory,

584
00:36:57,151 --> 00:37:00,720
which is maybe that my wife
and I were trying to conceive,
then you would say, oh,

585
00:37:00,721 --> 00:37:04,780
I now understand, right? This is sort
of hiding in plain sight with content.

586
00:37:04,780 --> 00:37:07,980
And so there are ways in which we can
obscure information and preserve that

587
00:37:07,981 --> 00:37:12,270
obscurity. And then finally, the
value that I advocate for is autonomy.

588
00:37:12,640 --> 00:37:16,140
Now autonomy is difference than control.

589
00:37:16,440 --> 00:37:18,930
Control can serve autonomy,
right?

590
00:37:18,931 --> 00:37:22,740
The right to be free from
external interference, the
right to self determination,

591
00:37:23,520 --> 00:37:26,970
but, but if too much control
actually threatens that autonomy.

592
00:37:26,971 --> 00:37:31,971
So I argue that a better value to
embrace is autonomy rather than control.

593
00:37:34,120 --> 00:37:34,800
All right.

594
00:37:34,800 --> 00:37:38,830
And so that leads me to the final part
of the book where I argue that a design

595
00:37:38,831 --> 00:37:42,720
agenda should have roots and consumer
protection and surveillance law rather

596
00:37:42,721 --> 00:37:47,640
than saying, well, the fair information
practices, which everybody accepts, uh,

597
00:37:47,641 --> 00:37:51,750
are, uh, the thing that we should,
we should adhere to. If you look,

598
00:37:51,751 --> 00:37:53,730
I said there are actually
other areas of the law,

599
00:37:53,820 --> 00:37:56,610
but if thought this out that
if taken design seriously,

600
00:37:56,970 --> 00:38:01,290
then I proposed sorts of boundaries,
uh, to avoid deceptive design,

601
00:38:01,291 --> 00:38:06,180
abusive design and dangerous
design. Deceptive design, uh,

602
00:38:06,210 --> 00:38:10,620
is something that it,
it, we relatively, uh,

603
00:38:10,680 --> 00:38:12,150
are doing a good job with both,

604
00:38:12,180 --> 00:38:16,020
I think within industry and within
policy in terms of not outright deceiving

605
00:38:16,021 --> 00:38:16,770
people.
You know,

606
00:38:16,770 --> 00:38:21,720
if it says this is a path complaints filed
by the Federal Trade Commission where

607
00:38:21,721 --> 00:38:24,330
add friends actually didn't
just allow you to add France,

608
00:38:24,331 --> 00:38:29,100
but it actually automatically sort of
went in and scooped up the entire contact

609
00:38:29,101 --> 00:38:30,330
list.
A,

610
00:38:30,331 --> 00:38:35,331
and then there's also the idea that
maybe we should avoid abusive design,

611
00:38:36,010 --> 00:38:39,490
which is the leveraging of people's
own limitations against them.

612
00:38:40,060 --> 00:38:43,390
And this happens sometimes
in everyday life.

613
00:38:43,450 --> 00:38:46,450
We tend to sort of leverage people's
own limitations against them.

614
00:38:46,660 --> 00:38:50,410
But it's another thing entirely to build
an entire machine built to leverage

615
00:38:50,411 --> 00:38:52,150
people's own limitations against them.

616
00:38:52,810 --> 00:38:56,680
This is an example of um,
pricing,

617
00:38:56,681 --> 00:39:00,790
different differential pricing based on
people's personal characteristics or the

618
00:39:00,791 --> 00:39:05,050
thing that made my privacy law students
really mad when year, which is, uh,

619
00:39:05,080 --> 00:39:07,240
the fact that a certain travel,

620
00:39:07,390 --> 00:39:12,220
a website could tell whether using
a PC or a Mac and would show the,

621
00:39:12,280 --> 00:39:15,910
um, the cheaper, um, uh,

622
00:39:16,000 --> 00:39:20,110
hotel rooms to the people using windows
pcs and the more expensive hotel rooms

623
00:39:20,111 --> 00:39:24,940
for the people using MACs based on
the assumption that people that had

624
00:39:24,970 --> 00:39:27,760
Macintosh's we're actually
being more, were, had more,

625
00:39:27,790 --> 00:39:31,360
had more money and we're more affluence.
Um, but there are tons of examples.

626
00:39:31,361 --> 00:39:35,200
So one of the examples that pops up is,

627
00:39:35,230 --> 00:39:39,280
is when leveraging people's own
limitations against them is our desire for

628
00:39:39,281 --> 00:39:43,600
conformity, right? People have a
strong desire sort of not to be,

629
00:39:43,690 --> 00:39:47,050
to be with a group or to do what everyone
else does and sometimes that can be

630
00:39:47,051 --> 00:39:51,100
worked against this. So, uh, this shows
up in the literature around dark patterns,

631
00:39:51,101 --> 00:39:53,290
which is a,
a really interesting website.

632
00:39:53,530 --> 00:39:56,980
And this is one where it says get 10% off.
And then at the bottom it says,

633
00:39:56,981 --> 00:39:59,770
no thanks. I'd rather pay full
price for delicious tea, right?

634
00:39:59,920 --> 00:40:02,500
Which is sort of shaming people,
right? But like, oh, okay,

635
00:40:02,650 --> 00:40:04,480
well I don't want to be the sucker here.

636
00:40:04,481 --> 00:40:08,320
And there's several other examples
where you see, because people's, uh,

637
00:40:08,350 --> 00:40:11,970
po voting records tend to be
public in most states. Um,

638
00:40:12,070 --> 00:40:13,990
some people have actually
leveraged that and they said,

639
00:40:13,991 --> 00:40:17,410
all your neighbors voted
and you didn't write.

640
00:40:17,410 --> 00:40:20,410
Here are the people that voted
and you didn't votes. Look,

641
00:40:20,411 --> 00:40:24,820
look how irresponsible you are,
right? Which is sort of a shaming, um,

642
00:40:24,850 --> 00:40:29,800
or a a one here, which is,
um, at the bottom it says,

643
00:40:29,801 --> 00:40:33,220
no thanks, I'm fine with losing customers.
I mean, who wants to click that right?

644
00:40:33,340 --> 00:40:37,480
Now, again, this is a very subtle
sort of manipulative technique,

645
00:40:37,510 --> 00:40:39,520
but it's one that is at scale,

646
00:40:39,521 --> 00:40:42,850
could tend to be a
harmful in the right way.

647
00:40:43,030 --> 00:40:47,350
Another example of using people's
own limitations against them is, uh,

648
00:40:47,440 --> 00:40:51,310
the use of like double and triple
negatives, right? So here it's decline.

649
00:40:51,311 --> 00:40:56,140
Release of directory information. Note,
most parents do not choose this option,

650
00:40:56,350 --> 00:41:00,100
right? So we've got lots of knots here.
I do not want the release information.

651
00:41:00,640 --> 00:41:04,000
So when you start layering like double
and triple negatives, then it's not,

652
00:41:04,030 --> 00:41:07,720
it's not lying, right? It's not,
it's not deceitful, but it's,

653
00:41:07,890 --> 00:41:11,800
it's leveraging people's inability to
sort of process the double and triple

654
00:41:11,801 --> 00:41:16,030
negative. I don't not never want
this information released. Um,

655
00:41:16,630 --> 00:41:19,060
and then finally there's dangerous design.

656
00:41:19,090 --> 00:41:22,660
Some designs are just inherently
dangerous. This is a spy camera.

657
00:41:22,840 --> 00:41:26,080
It's designed for more or less one thing,
right?

658
00:41:26,260 --> 00:41:30,100
Which just to catch people in
various states of undress, right?

659
00:41:30,101 --> 00:41:31,810
Because it's a clothes hanger,
right?

660
00:41:31,811 --> 00:41:35,480
It's meant to be put in rooms where
people are taking clothes on and off.

661
00:41:35,600 --> 00:41:38,300
And I argue that that's
dangerous design is as,

662
00:41:38,480 --> 00:41:41,270
as dangerous as surveillance
that you know about is,

663
00:41:41,450 --> 00:41:44,120
I've used surveillance as you don't know
about it to be even more potentially

664
00:41:44,121 --> 00:41:47,960
dangerous and there are lots of potential
responses to this and I go into this

665
00:41:47,961 --> 00:41:50,510
and the second part of the book,
soft responses,

666
00:41:50,511 --> 00:41:52,880
moderate responses in robust responses,

667
00:41:53,240 --> 00:41:57,620
so a soft response is we don't have to
come in as regulators heavy handed and

668
00:41:57,621 --> 00:41:59,150
say we're regulating this to the hilt.

669
00:41:59,390 --> 00:42:02,510
Sometimes what privacy
needs is some funding,

670
00:42:02,690 --> 00:42:07,280
it needs some innovation, it needs some
opportunity. It needs standardization,

671
00:42:07,610 --> 00:42:09,860
right?
It needs educational opportunities.

672
00:42:10,290 --> 00:42:13,790
So I argue for a lot of different ways
to improve people's privacies without

673
00:42:13,791 --> 00:42:16,310
passing some really
heavy handed regulations.

674
00:42:16,610 --> 00:42:20,840
Sometimes we need to moderate response
is maybe judges just need to be a little

675
00:42:20,841 --> 00:42:21,674
more

676
00:42:23,480 --> 00:42:27,710
sensitive to the role that design plays
in people shaping people's expectations

677
00:42:27,711 --> 00:42:29,540
and their choices and maybe regulators.

678
00:42:29,541 --> 00:42:34,040
Do you see this is an example of the
padlock icon, which is ubiquitous online.

679
00:42:34,250 --> 00:42:39,230
We use it all the time. It's the, it is.
It is the the physical symbol of security,

680
00:42:39,350 --> 00:42:42,140
but we don't really question what
it means to people as much. Right?

681
00:42:42,410 --> 00:42:47,180
When people see the padlock icon,
they usually think they're safe or secure,

682
00:42:47,280 --> 00:42:49,970
that that's what a lock means.
But what does,

683
00:42:49,971 --> 00:42:52,730
what does it really mean on the practice?
And maybe there,

684
00:42:52,740 --> 00:42:56,600
there are ways in which these sort of
Paluch icons might act as as sort of

685
00:42:56,601 --> 00:42:57,710
implicit promises.

686
00:42:58,040 --> 00:43:01,760
And then finally there may be a need
for heavy handed regulations in certain

687
00:43:01,761 --> 00:43:02,594
instances.

688
00:43:02,660 --> 00:43:07,660
One of the things I've argued for is in
support of as the ban on spyware that we

689
00:43:07,821 --> 00:43:11,200
have right now, which is
I think relatively, um,

690
00:43:11,870 --> 00:43:13,460
can be used and really malicious ways.

691
00:43:13,461 --> 00:43:17,090
And then another thing that's a little
more controversial is I've argued in

692
00:43:17,091 --> 00:43:20,240
favor of moratoriums and bans on
facial recognition technology,

693
00:43:20,241 --> 00:43:24,680
which I view as one of the most uniquely
dangerous technology surveillance

694
00:43:24,681 --> 00:43:29,210
technologies ever invented. And
then the final part of the book, I,

695
00:43:29,480 --> 00:43:30,980
I lay out what these,

696
00:43:31,010 --> 00:43:34,850
this blueprint of this design agenda
might look in three different kinds of

697
00:43:34,851 --> 00:43:36,290
context,
social media,

698
00:43:36,440 --> 00:43:40,250
hide and seek technologies
and the Internet of things,

699
00:43:40,460 --> 00:43:45,010
which is basically just a
plea from me to stop for,

700
00:43:45,270 --> 00:43:48,020
to tech companies to stop connecting
people's underwear to the Internet,

701
00:43:48,170 --> 00:43:50,900
which is one of the things.
So, um, and with that,

702
00:43:50,901 --> 00:43:53,210
I would love to go ahead and go
to the question answer period.

703
00:43:53,211 --> 00:43:54,500
Thank you very much.
I appreciate it.

704
00:43:57,540 --> 00:44:01,880
Given some examples of bad designs, uh,
can you give an example of a good design,

705
00:44:01,881 --> 00:44:05,060
particularly in the area of providing
user autonomy as you described?

706
00:44:05,460 --> 00:44:09,910
Oh, okay. Um, that's, uh, uh,
actually there are several,

707
00:44:09,911 --> 00:44:13,990
I think very good
designs. Um, one might be,

708
00:44:14,110 --> 00:44:18,820
there are areas in which, so after
I sort of rail on control, um,

709
00:44:18,910 --> 00:44:22,270
a lot, there are areas in which control
might actually be really useful.

710
00:44:22,540 --> 00:44:26,200
I liked the data subject rights.
I also sort of critique the Gdpr,

711
00:44:26,201 --> 00:44:27,310
but now a backtrack on that.

712
00:44:27,311 --> 00:44:32,190
A little sue data subject rights that
allow you to log in, review profiles.

713
00:44:32,490 --> 00:44:33,420
Um,
uh,

714
00:44:33,421 --> 00:44:36,840
information that's kept a voucher I
think are very useful because they can be

715
00:44:36,841 --> 00:44:41,190
done at the election of the
user. Um, dashboards for example,

716
00:44:41,191 --> 00:44:45,660
that allow you to review histories,
locations. I think those are,

717
00:44:45,690 --> 00:44:47,520
those are very good.
And they,

718
00:44:48,450 --> 00:44:52,830
my general principle is that people should
be protected regardless of what they

719
00:44:52,831 --> 00:44:53,371
choose.

720
00:44:53,371 --> 00:44:57,870
But if there are ways in which they
can exercise autonomy on top of that

721
00:44:57,871 --> 00:45:02,210
baseline level of protection, then
that that should be encouraged. Um,

722
00:45:02,390 --> 00:45:05,820
there are also small little things that
I liked that a lot of people actually

723
00:45:05,821 --> 00:45:07,470
critique is not going far enough,

724
00:45:07,650 --> 00:45:11,640
but I like because they're obscurity
protective. Um, for example, uh,

725
00:45:11,641 --> 00:45:16,260
youtube released a tool that
allowed you to blur people's faces.

726
00:45:16,261 --> 00:45:19,090
I believe. I love that tool. Um,

727
00:45:19,110 --> 00:45:23,100
and one of the reasons I love it is
because it's obscurity protecting, right?

728
00:45:23,640 --> 00:45:26,160
It's one that if you happen to
another person who was in the video,

729
00:45:26,161 --> 00:45:29,040
then maybe you would recognize them.
But to the world,

730
00:45:29,400 --> 00:45:31,740
that's actually a pretty good
design. Right? So in other words,

731
00:45:31,741 --> 00:45:33,810
people are largely protected.

732
00:45:33,811 --> 00:45:37,320
We often don't want to be protected from
everybody just from certain sorts of

733
00:45:37,321 --> 00:45:41,370
risks, right? So, um, uh, people, uh,

734
00:45:41,870 --> 00:45:42,930
having the right to be forgotten,

735
00:45:42,931 --> 00:45:45,870
it's one of these things where
people often want their name.

736
00:45:46,070 --> 00:45:50,160
This is associated with being
found for certain results, right?

737
00:45:50,161 --> 00:45:51,660
They don't need the information gone.

738
00:45:51,780 --> 00:45:56,640
They just don't want the HR person who
is reviewing their application for a job.

739
00:45:56,641 --> 00:45:58,120
They want to find it.
Right?

740
00:45:58,121 --> 00:46:01,140
And so there are certain sorts of things
they can protect obscurity and design

741
00:46:01,350 --> 00:46:05,360
like that. And I think the youtube a
Facebook video is a great example of that.

742
00:46:06,360 --> 00:46:10,740
So how do you consider a metadata
like derived data for by data, right?

743
00:46:10,741 --> 00:46:14,070
How has that kind of plan
to this and other people?

744
00:46:14,160 --> 00:46:17,610
Other people's data of me.
So like there was the Facebook thing.

745
00:46:17,611 --> 00:46:19,230
I think we're right.

746
00:46:19,260 --> 00:46:24,150
Other people had data on me and I won't
be able to see their data of me, right?

747
00:46:24,151 --> 00:46:26,550
So for instance,
if they had a phone number of me,

748
00:46:27,090 --> 00:46:29,010
but I never authorized Facebook
to have my phone number,

749
00:46:29,130 --> 00:46:30,750
they had it Facebook and I,
my phone number.

750
00:46:30,751 --> 00:46:32,310
And I don't know that
Facebook has my phone number.

751
00:46:32,650 --> 00:46:35,770
The question is sort of what is the
role of Metadata and all of this?

752
00:46:35,950 --> 00:46:40,360
And this is what I think
really illustrates the limits
of the control conception

753
00:46:40,361 --> 00:46:43,660
of privacy because so much is
actually out of our control.

754
00:46:43,870 --> 00:46:48,040
A really good example is the genetic
information, right? So, so, um,

755
00:46:48,070 --> 00:46:51,780
my sister, uh, Eh got one of the,

756
00:46:51,781 --> 00:46:53,770
the popular sort of tests and,

757
00:46:53,830 --> 00:46:58,480
and gave away or genetic information and
I was really a little uneasy about it

758
00:46:58,481 --> 00:47:00,280
cause I was like, that's it.
That's my information too.

759
00:47:00,430 --> 00:47:03,000
And I've got no control over that at all.
Um,

760
00:47:03,070 --> 00:47:06,850
which is why I argued that we should
have less reliance on control because

761
00:47:07,090 --> 00:47:10,420
control ultimately sort of becomes
unraveled at this stage, right?

762
00:47:10,421 --> 00:47:14,650
It's inability to control it. This is
where argued for the importance of trust.

763
00:47:14,950 --> 00:47:19,410
Um, this is where relationships
of trust matter, um,

764
00:47:19,600 --> 00:47:22,660
now it's, there are
costs to that. So if we,

765
00:47:22,661 --> 00:47:25,420
if we think of privacy in terms of trust,

766
00:47:25,750 --> 00:47:30,400
then inherently we've just made
this conceptualization relational,

767
00:47:30,580 --> 00:47:33,090
right? So, um, so then, uh,

768
00:47:33,340 --> 00:47:37,990
there are advantages to that in that now
I can ask you if you have metadata on

769
00:47:37,991 --> 00:47:38,471
me,
right?

770
00:47:38,471 --> 00:47:42,520
Or for platform has metadata on multiple
people to act in a certain way. A loyal,

771
00:47:42,521 --> 00:47:47,180
for example, to not leverage that
information against me. Um, I'm,

772
00:47:47,500 --> 00:47:50,380
I don't think that just because
information is disclosed,

773
00:47:50,590 --> 00:47:52,510
your privacy isn't necessarily gone.

774
00:47:52,810 --> 00:47:55,990
I think that we disclosed
relationships within information,

775
00:47:55,991 --> 00:47:57,700
within relationships
with trust all the time.

776
00:47:57,880 --> 00:48:02,020
But what it does mean is that sometimes
there are our parties that have no

777
00:48:02,021 --> 00:48:04,660
relationship,
that have that information,

778
00:48:04,661 --> 00:48:07,090
that can then violate your privacy.

779
00:48:07,091 --> 00:48:10,000
So date of brokers who would be
an obvious example where, um,

780
00:48:10,210 --> 00:48:14,650
they don't have a direct relationship
with the data subjects, uh, in ways that,

781
00:48:14,651 --> 00:48:17,100
that platforms that collect
information directly do, but,

782
00:48:17,101 --> 00:48:20,320
but they get information
from various streams. Um,

783
00:48:20,380 --> 00:48:24,940
and so my general sense is that we should
have a baseline set of rules that even

784
00:48:24,941 --> 00:48:26,260
apply to data brokers,

785
00:48:26,530 --> 00:48:30,280
but that a lot of our issues with respect
to and metadata and a lot of these

786
00:48:30,281 --> 00:48:34,570
sort of unraveling problems can be solved
through thinking of privacy in terms

787
00:48:34,571 --> 00:48:35,404
of trust.

788
00:48:35,440 --> 00:48:39,190
So the just the fact that you have it
right or that someone else has it isn't

789
00:48:39,191 --> 00:48:40,001
the end of the story.

790
00:48:40,001 --> 00:48:43,270
Rather there are now things that you
shouldn't be able to do with that

791
00:48:43,271 --> 00:48:44,104
information.

792
00:48:45,510 --> 00:48:48,180
We have a question from
the dory from cane.

793
00:48:48,600 --> 00:48:50,820
I found that I quickly
learned to click on those.

794
00:48:50,940 --> 00:48:55,170
I'm fine with losing customers buttons
because the popup is blocking the task.

795
00:48:55,200 --> 00:48:56,280
I was trying to complete.

796
00:48:56,670 --> 00:49:00,120
What are some tips for making sure people
know how to go back and change their

797
00:49:00,121 --> 00:49:02,280
mind when it's no longer time critical?

798
00:49:03,050 --> 00:49:06,890
Again, I mean not to make everything in
illustration of the limits of the control

799
00:49:06,891 --> 00:49:11,870
conceptualization of privacy. But
um, often when we think of consent,

800
00:49:11,871 --> 00:49:16,190
consent is, is implemented in sort of
binary terms. Like I agreed to this.

801
00:49:16,370 --> 00:49:17,540
And then once that has gotten,

802
00:49:17,541 --> 00:49:21,290
then it sort of in perpetuity exist and
you don't have to re ask for it again,

803
00:49:21,291 --> 00:49:25,640
even though people often change their
minds or circumstances change. Um,

804
00:49:25,670 --> 00:49:30,670
this is where I do think that there is
some responsibility on the part of users

805
00:49:31,011 --> 00:49:34,400
to take advantage of the
tools that are given to them,

806
00:49:34,430 --> 00:49:38,150
if those tools are given to
them in an easy to use way. Um,

807
00:49:38,360 --> 00:49:43,360
I encourage everyone to at least
take a few minutes as part of your,

808
00:49:43,521 --> 00:49:46,760
maybe not daily but maybe weekly or
monthly routine to do a little digital

809
00:49:46,761 --> 00:49:49,420
hygiene. Um, you know,

810
00:49:49,550 --> 00:49:53,510
search your own name on various search
engines to see what's coming up. Um,

811
00:49:53,511 --> 00:49:56,650
look at the, if you have accounts, um,

812
00:49:56,770 --> 00:49:59,960
and sometimes you have accounts even
for only defensive purposes, right?

813
00:49:59,961 --> 00:50:02,570
So I sometimes people I get
resistant to it, I say, well,

814
00:50:02,571 --> 00:50:04,440
I don't want to sign up
for an account for x, y,

815
00:50:04,450 --> 00:50:06,860
z because then they'll have
even more information out of me,

816
00:50:06,861 --> 00:50:08,940
which I'm sympathetic to. But, um,

817
00:50:09,080 --> 00:50:13,130
if it allows you to then sort of log
in and regularly delete information or

818
00:50:13,131 --> 00:50:17,770
cleanse information, then that might
be a positive. And then, um, uh,

819
00:50:17,840 --> 00:50:22,360
take advantage of, of I
think popular ad-ons. Um,

820
00:50:22,400 --> 00:50:27,320
I use ad blockers, uh, which I know is
controversial, but I think that's, um,

821
00:50:27,800 --> 00:50:29,990
that that's another possible tool.
And then finally,

822
00:50:29,991 --> 00:50:32,960
and this is just the thing that I
throw out in generally is if two factor

823
00:50:32,961 --> 00:50:37,820
authentication has ever offered
anywhere, use it, um, and, and,

824
00:50:37,870 --> 00:50:42,340
uh, use it, uh, sort
of early and often is,

825
00:50:42,380 --> 00:50:47,300
is my example. But, but I do think
that that vanity searches, you know,

826
00:50:47,420 --> 00:50:51,710
learning how to, to work within
dashboards, um, is at least a key part.

827
00:50:51,711 --> 00:50:53,690
It can't be everything. Right. My, my,

828
00:50:54,020 --> 00:50:57,800
my argument is that often users are
being asked to do too much, but,

829
00:50:57,840 --> 00:51:00,750
but there is some sort of expectation
that I think we can have a views.

830
00:51:00,751 --> 00:51:04,040
There's two at least, sort
of try to, to exercise some,

831
00:51:04,520 --> 00:51:05,960
some diligence with the tool set given

832
00:51:07,200 --> 00:51:09,600
what are your thoughts on premium options?
Um,

833
00:51:09,630 --> 00:51:12,510
so we're starting to see a lot
of companies moving towards that.

834
00:51:12,511 --> 00:51:15,750
I know we have a youtube
premium option, etc. Um, as a,

835
00:51:15,751 --> 00:51:18,510
as a method for coming back to
your concept of control, right.

836
00:51:18,810 --> 00:51:19,950
Do you see that as valid?

837
00:51:22,570 --> 00:51:27,010
I'm of two minds for the premium
options. I mean on one hand, um,

838
00:51:27,700 --> 00:51:32,530
I liked the premium options because the
business model seems a little clearer,

839
00:51:32,870 --> 00:51:36,880
right? So one of the things that I've
always been drawn to is when you can,

840
00:51:37,150 --> 00:51:38,290
when you win,

841
00:51:38,440 --> 00:51:41,290
everybody's incentives are sort of out
in the open and we know sort of what

842
00:51:41,291 --> 00:51:45,370
we're dealing with it, it becomes a
lot easier and easier to trust. Um,

843
00:51:45,410 --> 00:51:48,730
I think for the premium models to work,
um,

844
00:51:48,731 --> 00:51:53,560
there has to be not just, um,
some sort of soft promises, but I,

845
00:51:53,590 --> 00:51:58,270
but I think really robust promises.
I would love the idea of,

846
00:51:58,360 --> 00:52:02,590
of platforms being able to opt in to
sort of the gold standard of privacy,

847
00:52:02,591 --> 00:52:06,500
which is the, the true level of
trustworthiness where, where they, they,

848
00:52:06,550 --> 00:52:09,040
they say we will be loyal,
we will be discreet,

849
00:52:09,041 --> 00:52:13,780
we will be honest to the sort of
highest standard. Um, and then,

850
00:52:13,870 --> 00:52:17,080
and then that can drive people
to sign up for companies. Right?

851
00:52:17,081 --> 00:52:19,090
It can be a competitive advantage.
And I've,

852
00:52:19,120 --> 00:52:22,180
I've always longed for privacy
to be a competitive advantage.

853
00:52:22,610 --> 00:52:25,690
And I think that we're seeing a
little traction on that model. But,

854
00:52:25,780 --> 00:52:29,050
but then the other side of me worries a
little about premium models because the

855
00:52:29,051 --> 00:52:32,230
other thing that I worry about is that,
um,

856
00:52:32,260 --> 00:52:37,260
privacy becomes something only that
people that have affordances can have a,

857
00:52:37,990 --> 00:52:42,850
and I really worry about the sort
of, uh, equities where we say,

858
00:52:42,851 --> 00:52:46,450
if you pay that,
if you pay the the gold standard option,

859
00:52:46,451 --> 00:52:49,990
you get privacy for everyone that can't
afford it. They know, they, you know,

860
00:52:50,440 --> 00:52:55,420
they don't have that. Um, and
so, so for that, for that reason,

861
00:52:55,421 --> 00:52:58,080
I'm, I'm still actually struggling
with, with a lot of the way that,

862
00:52:58,081 --> 00:53:02,950
that cashes out in, in, in
terms of policy, um, because it,

863
00:53:03,190 --> 00:53:04,210
I do worry about that.

864
00:53:06,800 --> 00:53:11,630
And we have one more question on the
dory from Keith. Uh, what are we,

865
00:53:11,960 --> 00:53:14,630
what are we all missing about dragon fly?
Um,

866
00:53:14,710 --> 00:53:18,770
and the context is a wired story
of Sundar at the wired summit.

867
00:53:19,280 --> 00:53:20,080
So,
so

868
00:53:20,080 --> 00:53:22,450
dragon fly,
as I understand it,

869
00:53:22,810 --> 00:53:27,810
is the initiative for Google to be,

870
00:53:28,200 --> 00:53:32,120
to go in to, to enter China's market.
Right. And I understand, I mean,

871
00:53:32,300 --> 00:53:35,550
if I'm understanding the stories correctly
and I haven't read a lot about it,

872
00:53:35,940 --> 00:53:40,260
so I actually can't provide a,
I think a nuanced discussion,

873
00:53:40,590 --> 00:53:45,590
but I have thought a lot about the
values that get reflected in technology.

874
00:53:46,860 --> 00:53:48,330
And so to that extent,

875
00:53:48,360 --> 00:53:53,340
I think it's worth having an
explicit conversation about,

876
00:53:53,700 --> 00:53:55,530
um,
what,

877
00:53:56,190 --> 00:54:01,140
what the affordances of
dragon fly would be. Um,

878
00:54:01,380 --> 00:54:06,380
if it facilitates it because
it doesn't exist in a vacuum.

879
00:54:07,890 --> 00:54:12,890
This is a browser that will get deployed
within a system that is actively

880
00:54:13,111 --> 00:54:15,000
leveraging,
um,

881
00:54:15,030 --> 00:54:19,530
things like social credit scores and the
social credit score, actually I will,

882
00:54:19,590 --> 00:54:22,400
I will say scares me a lot.
Um,

883
00:54:22,440 --> 00:54:26,460
the idea that we would be ranked based
on any number of different factors and

884
00:54:26,461 --> 00:54:29,970
then have benefits sort of denied
to us based upon those factors.

885
00:54:29,970 --> 00:54:32,100
So we wouldn't be able to
board a plane, um, that are,

886
00:54:32,101 --> 00:54:35,490
that we would have sort of demerits
taken away from us because we used up too

887
00:54:35,491 --> 00:54:39,780
many rolls of toilet paper. Um, the fact
that we would be sort of data flying,

888
00:54:39,781 --> 00:54:41,340
everything does,

889
00:54:41,760 --> 00:54:46,320
does this contribute to that effort
because that's a value that's implicated?

890
00:54:46,620 --> 00:54:50,940
Does it, will it help exacerbate the
spread of facial recognition technology,

891
00:54:51,030 --> 00:54:54,420
which is another thing that
I've worried about a lot. Um,

892
00:54:54,450 --> 00:54:59,400
because in order to gain the benefits of
facial recognition technology of which

893
00:54:59,401 --> 00:55:03,180
there are admittedly are many,
we can find the bad guy,

894
00:55:03,270 --> 00:55:08,160
we can find missing children,
we can, um, uh, help those,

895
00:55:08,430 --> 00:55:12,870
um, that, uh, uh, don't
have the ability to sense,

896
00:55:13,240 --> 00:55:17,780
uh, in, in ways of others and a
bit in order to get a lot of the,

897
00:55:17,781 --> 00:55:20,580
the real uses of facial
recognition technology,

898
00:55:20,581 --> 00:55:24,030
not just the sort of minor conveniences
of unlocking your phone with your face

899
00:55:24,031 --> 00:55:27,370
rather than your fingerprint, which
is fine, but I view as just a, uh,

900
00:55:27,520 --> 00:55:31,830
a very incremental benefits. Um, but
in order to get the real benefits,

901
00:55:32,250 --> 00:55:36,880
we will have to give up a
lot, almost everything, right?

902
00:55:36,930 --> 00:55:40,890
We have to have cameras everywhere.
We have to have databases that are shared.

903
00:55:41,070 --> 00:55:44,040
And I have facial recognition technology
depends upon the existence of a name,

904
00:55:44,041 --> 00:55:48,100
face database that's they
can recognize people. Um,

905
00:55:48,660 --> 00:55:51,870
and I think we have to think about
that, that value as well. And will this,

906
00:55:51,960 --> 00:55:55,920
will this aid that in the
aggregate and is it worth then,

907
00:55:56,310 --> 00:55:58,530
um,
uh,

908
00:55:59,010 --> 00:56:02,780
defining the values of the company
based on that, right? Because,

909
00:56:02,790 --> 00:56:05,310
because your values are reflected
not just in what you do but in,

910
00:56:05,490 --> 00:56:08,810
but in what you build. Um,
and I think that it's, it's,

911
00:56:09,120 --> 00:56:13,830
it merits a serious conversation about
the direction of a company and the ethos

912
00:56:13,831 --> 00:56:18,670
of the company, um, with
the, with that creation. But,

913
00:56:18,690 --> 00:56:19,760
but that being said, I, I'm,

914
00:56:19,761 --> 00:56:23,110
I'd be scared to comment anymore because
I actually don't know the specifics of

915
00:56:23,111 --> 00:56:24,730
the, of the project. So,

916
00:56:25,430 --> 00:56:28,550
uh, so terms and conditions
seems like a necessary evil.

917
00:56:28,970 --> 00:56:33,470
But clearly the way that it's done today
is pretty terrible because nobody reads

918
00:56:33,480 --> 00:56:36,770
dumb. And now people are,
I guess, legally bound.

919
00:56:36,771 --> 00:56:40,280
I'm not even sure if they're legally
blind in because nobody understands that

920
00:56:40,281 --> 00:56:43,100
nobody reads them is can you
agree to something you don't read?

921
00:56:43,460 --> 00:56:47,100
But besides the point, and there's no
expectation to meet them either. Right.

922
00:56:47,360 --> 00:56:50,930
Which is another issue. But, uh, how
do you think that could be done better?

923
00:56:51,680 --> 00:56:53,930
Craig, thank you very
much. I love that question.

924
00:56:53,931 --> 00:56:57,320
So I just wrote an essay a little
while back called user agreements are

925
00:56:57,321 --> 00:57:01,310
betraying you, uh, with the
idea that, that it, it's,

926
00:57:01,311 --> 00:57:05,510
it's this abstraction, nuance tension
that's unreal. Unresolvable right.

927
00:57:05,511 --> 00:57:10,370
We could either have a, someone proposed
as some point, a 100 word user agreement,

928
00:57:10,580 --> 00:57:14,330
which says that's, that's, that's, that's
not even an advertising slogan almost.

929
00:57:14,331 --> 00:57:16,510
Right? I mean it's, it's,
it's so sort of abstract.

930
00:57:16,540 --> 00:57:21,200
This did not really tell you anything.
Um, I think that user agreements, um,

931
00:57:21,310 --> 00:57:23,570
could be improved upon,
uh,

932
00:57:23,600 --> 00:57:28,600
if they are written in a way that
they are written for basically the

933
00:57:30,770 --> 00:57:34,040
transparency purposes because some
people do read them. Actually,

934
00:57:34,041 --> 00:57:38,770
there's a really good article,
um, uh, by, uh, Mike [inaudible],

935
00:57:39,050 --> 00:57:42,740
uh, called in defense of the long privacy
statements. And he says that people,

936
00:57:42,741 --> 00:57:47,090
some people actually do read privacy
policies, regulators do, advocates do,

937
00:57:47,580 --> 00:57:51,140
they serve in, Huh? Attorneys,
right? Attorneys do,

938
00:57:52,010 --> 00:57:55,550
they serve an important hygienic function
for companies, right? So they let you,

939
00:57:55,730 --> 00:57:59,480
they let you know, like internally like,
okay, here's where all our information is.

940
00:57:59,481 --> 00:58:01,300
We've got stock of it.
Like it's a,

941
00:58:01,301 --> 00:58:05,870
it's an incredibly good force functioning
tool. It's just not for users.

942
00:58:06,080 --> 00:58:09,050
Right? And so my, um,

943
00:58:09,710 --> 00:58:13,930
my preference would be to have a,

944
00:58:13,960 --> 00:58:15,290
a regulatory regime.

945
00:58:15,291 --> 00:58:17,660
And this is a little controversial
where there are certain things that you

946
00:58:17,661 --> 00:58:21,290
cannot trade away in our user agreement.
So in other words,

947
00:58:21,291 --> 00:58:25,370
there's a baseline set of protections
that cannot be compromised through terms

948
00:58:25,371 --> 00:58:28,610
and conditions. We have this
in other areas of the law. Um,

949
00:58:29,180 --> 00:58:31,970
and then that way users can agree to
them without worrying about them. Right?

950
00:58:31,971 --> 00:58:35,540
Because I always sort of have this, this
like small sense of dread when I Click,

951
00:58:35,541 --> 00:58:38,720
I agree. Like, I didn't read any of
this, but I don't know within, in there,

952
00:58:38,721 --> 00:58:42,380
but I agreed to it anyway.
And they actually are
enforceable largely, right. So,

953
00:58:42,381 --> 00:58:45,470
so the, the law, I teach
contracts sometimes too,

954
00:58:45,620 --> 00:58:47,510
and the law of contracts
is that if you click,

955
00:58:47,511 --> 00:58:51,500
I agreed in courts are
going to say you agreed. Um,

956
00:58:51,560 --> 00:58:56,180
and I would like to have a talk about
things that are unwavered will so that it

957
00:58:56,181 --> 00:58:58,850
basically, it didn't matter what was
in the terms of agreements, right?

958
00:58:58,860 --> 00:59:02,960
Because they are important risk mitigation
tools for companies and they could

959
00:59:02,961 --> 00:59:07,610
have nice, um, transparency and
accountability aspects to them.

960
00:59:07,611 --> 00:59:11,000
We just have to stop pretending that
they're for users because the things that

961
00:59:11,001 --> 00:59:15,210
shape user's expectations or the
design, it's the website. It's the,

962
00:59:15,290 --> 00:59:19,860
it's the padlock icon. It's the button.
Thank you so much. I really appreciate it.

963
00:59:19,861 --> 00:59:20,140
Everybody.

