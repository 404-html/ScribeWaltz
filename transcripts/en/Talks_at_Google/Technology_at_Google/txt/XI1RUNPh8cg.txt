Speaker 1:          00:09          Thanks so much for, uh, for having me. Um, if the theme of the session is a cops and nerds, I think I'm going to force the audience to guess which of those I am. Um, so here's what I wanted to talk about. Thanks so much for having me. What I wanted to talk about is, um, you know, you realize, especially when you live in a big city like Chicago, how many heartbreaking social problems, uh, that we are trying to solve right now. And you also realize that we are increasingly living in a world of, uh, of data, of big data. And so it seems like there's a very natural and easy intellectual arbitrage opportunity where we can take this sort of machine learning tools that we encounter all the time in the commercial sector and just kind of plugin, plugin play and apply them self policy problems.

Speaker 1:          00:56          Uh, it's, it looks like an easy intellectual arbitrage opportunity for making the world a better place. And so what I wanted to do is I wanted to talk about, um, how I think social progress on some of these really difficult policy problems is going to require a lot more, I think, than just off the shelf machine learning. So that's the, the theme of the, the theme of the talk. So let me talk about that. Um, uh, a little bit more concretely within the context of, uh, one particularly important problem that we have here in Chicago. But this is obviously not just a Chicago problem. So if you're hearing Chicago, you drive down to 26th and California on the southwest side of the city, you will see the Cook County jail, uh, which has, you know, depending on the day between six or eight or 10,000 residents, some of the most economically disadvantaged people in the city of Chicago, uh, 80 to 90% of whom are either African American or Hispanic.

Speaker 1:          01:53          So we can back up and say, how did we wind up with so many people in the Cook County jail? So here's a little bit, a little primmer on how the criminal justice system works. So you get arrested in the United States and the constitution says within 48 hours, you've got to go in front of a judge who makes the decision about where you're going to await adjudication of your case. Do you get to go home or do you have to sit in jail waiting for your case to be resolved? And, um, the law says that the judge is supposed to make that decision, not based on whether you're guilty or not or what the punishment is for the crime that you were alleged to have committed. But the law says the judge is supposed to make that decision entirely based on a prediction, a prediction of your safety risk, and a prediction of your flight risk.

Speaker 1:          02:39          And, uh, this is an enormously consequential decision as you can imagine. So if the judge gels you, uh, on average, you'll spend two to four months in a place like the Cook County jail. You could imagine what that does to your, uh, job prospects. You can imagine what that does to your family. Uh, you can imagine what that to the local county, a budget to keep six, eight, 10,000 people in jail, releasing people also has a serious downside risk as well. Uh, the person that the judge releases might go on to commit another crime and crime itself is also very regressive in its impact. So super high stakes decision. Uh, if you're a tech person, you look at this and say, well, what is the criminal justice system currently doing to help judges make this high stakes? Very difficult prediction. And this is status quo in most cities.

Speaker 1:          03:31          This is the same technology that we would have used in the 1950s or maybe the 1850s ye olde, um, decision aid. Um, and so it's natural to think, well, you know, why not instead use this sort of technology that places like Google are applying to all sorts of commercial applications. And so this seems like super straight forward, uh, because it seems like we have most of the important ingredients that we need to make progress. So for starters, we have these amazing machine learning tools. And I think it's, it's easy because they're so ubiquitous now, it's easy to overlook exactly how amazing these tools are. So let me just spend a minute reminding us of this within the context of one very common kind of machine learning application, which is sentiment analysis. So if you're a computer scientist, you know, one of the canonical applications of machine learning is to take a piece of text that a author is a human, is written and try and infer what the effect was that the author is trying to communicate.

Speaker 1:          04:33          So here's a example from a more or less randomly selected a consumer product off of Amazon, the Hustler five 71 banana slicer. And so what we have here is a product review, some text, and then we also have a starred review, which gives us sort of ground truth. What actually was the author intending? And so here's an example. This is from someone named thrifty. He who says, I bought this in order to speed up cutting up a banana for my cereal. And a time that I saved in that endeavor was spent cleaning this implement. It's not easy to clean. You have to scrub between every rung to thoroughly clean it. You can see that's it's not a great review. Two out of five stars. You can look at another one by uncle pooky who says, once I figured out I had to peel the banana before using it works much better.

Speaker 1:          05:15          Ordering one for my nephew who's in the air force in California and he's been using an old slinky to slice. He's been in as he should really enjoy this product. Five out of five stars, a confusing by Qtip. There's no way to tell if this is a standard or metric banana slicer. Additional markings audit would help greatly. One out of five stars and finally from Jay Anderson Angle is wrong. I tried the banana slicer and found that unacceptable as shown in the picture of the slices is curved from left to right. All of my bananas are bent the other way, so this is an example also of how machine learning turns out to have such powerful commercial applications. This helped a hustler company figure out what their five 72 banana slicer should look like. No. Here's, I think the sort of the key lesson from this is, you know, you're reading these narrative reviews and it is super easy for you to figure out what the author was intending.

Speaker 1:          06:07          And it's so easy that, you know, when the computer scientist in the middle of the 20th century were initially working on artificial intelligence, this led to a natural sort of conclusion about how you would try and get a computer to do what humans do, especially for something that is so easy for us to do, which is just program them to do something, how we do it. Um, so introspect on what you do and write a program that does exactly the same thing. And here's sort of the problem with that kind of approach, uh, that, that the computer scientists found in practice. This is from a study done at Cornell doing sentiment analysis for a movie review. So they get a bunch of Cornell nerds in the basement to introspect on what words they would think would show up in a positive or negative movie review.

Speaker 1:          06:53          They write a program that looks for those words and then they classify positive or negative reviews on that basis. Usually we set up the test set here so that we have 50% positive reviews, 50% negative reviews. So accuracy of 50% would be like random guessing. And so these are the words that the Cornell nerds, um, picked. Uh, I dunno, why suck. And sucks are both separate words there. It's Cornell, not the University of Chicago. So one of my coauthors is from Cornell. So I see that with, uh, with love. Um, and so here's what we saw with this sort of programming approach is, you know, we're doing better than random guessing, but not much better than random guessing. Not much better than random guessing. And the breakthrough here with these machine learning tools came from when the computer scientists realized that we just need to forget that we do these, that we know how to do these things, treat the known like the unknown, and just start treating this like a brute force empirical exercise and basically mined the data.

Speaker 1:          07:55          Mine dumb movie reviews themselves for information about what words turn up more frequently in positive and negative reviews. And um, you know, once we start doing that, we start to get up to 90 a accuracy rates on the order of like 95%. So this really was the big breakthrough and the tools that enable us to do this really are incredibly amazing. I think what's also particularly exciting from a public policy perspective, if you think about like pretrial release decisions is that there is an enormous amount of data to be had. So this is what we discovered when we were working, uh, building a pretrial risk prediction tool ourselves using data from a large anonymous American city of eight and a half million people. Um, and you know, you have millions and millions of observations, which doesn't sound like a lot when you're talking to an audience at Google.

Speaker 1:          08:45          But I think in a policy environment, uh, this is, you know, this is a new development to have so much information that we can bring to bear on these problems and for each case goes through bond court. We have lots of information about the current charge and their prior criminal record. So lots of information that can be brought to bear. And these machine learning tools now are increasingly accessible. So anybody with an Internet connection can download our free software and just basically build a machine learning algorithm and they're off to the races. Now normally in a machine learning exercise, the final step is also easy, which is determining how good of a job your algorithm is actually doing. Right. And so imagine that I'm doing some sort of commercial application of machine learning and I am trying to decide like how good is my facial recognition software doing.

Speaker 1:          09:36          And I wanted give it a bunch of new face pictures that it hasn't seen and I want it to tell me if a normal human faces in the picture. And so you know, you can look at that face, no face. It's easy to tell and score the algorithm, you know. Uh, I think one of the things I learned to do it for this slide show is huskies have the funniest dog faces. Um, and whether this one is a normal human face I think is sort of a deeper philosophical question. But this is easy, right? This is the easy part. This turns out to be the first point in policy applications where you realize that this is very different from commercial, from commercial machine learning. And part of the issue here is that we really, at some fundamental level, we really don't care about prediction quality.

Speaker 1:          10:27          The thing that we really care about instead is decision quality. What I want to know is can I take a machine learning algorithm and build some new release rule that I can give to a judge and we'll let actually turn into the world becoming a better place. Now, why is that complicated? Okay. Why is that complicated? Think about, um, how an algorithm, an algorithmic release role could potentially make the world a better place. The algorithm might want to detain someone that the judge releases or it might want to release someone that the judge to teens. And so then we can ask ourselves, how do we score whether the algorithmic release rule is better than the judge or not in a world in which the data that we have available to us to evaluate the algorithm is generated by the human decisions generated by the judge's decisions.

Speaker 1:          11:16          And so here's what the issue is, right? If the judge releases a defendant, right? If the judge releases a defendant, we don't have any sort of problem, I can observe what the crime outcome is under the judge's actual decision. And I know what the crime outcome is under the counterfactual algorithmic decision. If the algorithm wants to detain the defendant because putting someone in jail by definition incapacitates them and prevents them from Aig, uh, engaging in crime. So that's easy. But what if the judge detains the defendant, right? What if the judge detains the defendant and the Algorithm wants to release them? How do I, whether the algorithm is right or whether the judge was right. Okay. Now if you're a computer scientist, you look at this and say, well, this doesn't feel like a hard problem, right? At some level it doesn't feel like a hard problem.

Speaker 1:          12:06          We have a bunch of data on the people who the judge released. We have a bunch of background information on the people that the judge released and the judge detained. Why don't we just assume that the crime outcomes for the people the judge released would be like a crime outcomes for the people the judge detained who have observably similar current Arestin prior criminal record, right. It seems like a very, very straightforward imputation exercise on its face. Here's the problem and a policy application, which is the judge sees things that no algorithm will ever see. So this is a, this is an example of um, this is an example of a defendant, uh, mid 20, it's called it a 25 year old guy who report reported as a occupation when he was arrested as tattoo model. I didn't know there was such a thing. Uh, he gets arrested two different times, uh, one time that's him on the left.

Speaker 1:          13:01          Uh, and then this is him again on the right. So you'll notice that the 25 year old tattoo model on the right has decided that it was a good idea to get his face tattooed. Like joker, the Super Villain from the dark knight moving, but you can't see because it's on the other side of the face is on this side. You can see he's got the joker tattooed on a, on his right, on his left it says, just in case there was any ambiguity about what he's about on his left, it says in giant letters. Fuck Batman. Now to the Algorithm, the 25 year old tattoo model on the left and the 25 year old tattoo model on the right look exactly the same. The Algorithm looks at the guy in the left and say, he doesn't look no prior record, current offenses and misdemeanor. This does not look like a very high risk guy.

Speaker 1:          13:46          The judge sees that can see something extra about the defendant. Now imagine what happens. The algorithm goes to the judge and says, why are you detaining these 25 year old tattoo models? These are all low risk guys. You should just let them go. You look the guy on the right go and crime goes up by a lot more than the algorithm had anticipated and we wind up inadvertently making the world a worse place while other than a better place. Okay, and the key here is that in these sorts of social, social science or policy applications, we have to take seriously the possibility that the judge has private information that the algorithm doesn't have, which makes this evaluation problem enormously difficult. And if we ignore that evaluation problem when we're comparing the algorithm to the judge where basically stacking the deck in favor of saying the algorithm, the algorithm almost has to do better than the judge.

Speaker 1:          14:38          If you ignore the possibility that the judge has as extra information. Okay. So what is the solution to a, what is the solution to that problem? Well, um, the solution to that problem starts, has two comes from two insights. The first insight is to recognize that the problems one sided. So when the Algorithm wants to release a GL defendant, that's a problem. But there's no problem if the algorithm, if the algorithmic decision rule wants to jail someone that the, the judge released. And the second insight is to rely on a common trick that we have in econometrics or the causal inference literature, which takes advantage of the fact that in this case we have something like random assignment of cases to judges. So what we wind up having his multiple judges seeing case loads of defendants that are similar on average. And yet the judges differ a lot with respect to their leniency rates.

Speaker 1:          15:35          And so here's what we can do in that case, here's what we can do in that case. So imagine that we have two judges, one of them with the 90% release rate and one of them with an 80% release rate and they're seeing case loads that are similar on average. And so here on the left we've got the judge with the 90% release rate, we can go into the pool, the remaining 90% of defendants that the lenient judge has released and weekend rank order defendants by our algorithms, predictions of their crime risk. And then weekend countdown, that rank ordered list of defendants by predicted risk and pick another 10% of defendants to detain. So this is what the Algorithm's guests is for the most socially productive marginal 10% of defendants to detain. And then when we can do, and that gets us down effectively to an 80% release rate.

Speaker 1:          16:25          And then what we can do in that case is compare the crime rate that we get in that case with the actual crime rate that we get from the 80% released judge. Right. And that turns out to be a credible way and a fair way to compare whether the algorithm really is doing a better job than than the judge and what you see when you do that. So that, so when you see when you do that is it is indeed the case that the algorithm is doing better than the judge and the algorithm is doing a lot better than the judge. And so what you can see is if we were to hold the size of the jail population constant and just use the algorithmic release role to decide who to detain, we could reduce crime rates by 25% without having the change of the jail population.

Speaker 1:          17:10          Now you might be sitting there thinking, given that we've ramped up the incarceration papa population in the United States so much, maybe you think the big problem in the United States now is not crime so much as incarceration itself. And so alternatively, what you can imagine doing is saying, let's hold the crime rate constant. How many fewer people do we need to detain in order to achieve the same level of crime reduction as the judges current decisions achieve. And if you do that, we can reduce the jail population by fully 42% with no increase in crime right now. That's completely amazing. That's completely amazing part because if you think about how hard these social problems are to solve, usually in those rare cases where we find an effective solution, they turn out to be either very expensive to implement or very difficult to scale up successfully. That's not the case with the machine learning algorithm.

Speaker 1:          18:05          Once the thing is built, the marginal cost of running this over and over again is near zero. And because it's an automated tool, it scales nearly perfectly at least over the range that we have for something like pretrial release decisions. Um, there is something that you'd be worried about, which is, you know, we don't care just about crime and a detention outcomes, especially in the criminal justice system. We care a lot about things like fairness as well. And so maybe what's also amazing about the machine learning application in this case is in addition to the enabling us to simultaneously reduce crime and jail populations, at the same time, we can also reduce disparities within the detention populations as well. And the answer to that is easy to see why the algorithm can do that as easy to see once you recognize what the alternative to the algorithm is, which is the same human decisions that gave us the current criminal justice system that is so skewed in the direction of over representing racial minorities and low income populations, uh, behind bars.

Speaker 1:          19:07          Okay. Now if we were in a, if we were in a commercial setting, you would build an algorithm like this. You have your face detector, uh, software, you'd put it on a phone and you'd be off to the races. The social benefits of the algorithm would be automatically realized. Um, if you sort of think about the pretrial release application, there's another backend step, right? There's another backend step, which is that nobody imagines that the algorithm is actually ever going to be the decision maker. Instead, we imagine, so realistically, the Algorithm for predicting defendant risk is inevitably just going to be a decision aid for a human being that then has to translate the algorithms predictions into decisions. And if the human being is getting things wrong, they can undo whatever the social good potential is for the algorithm algorithms, predictions to achieve. Okay. Um, and so, you know, put differently, the robot in this case is not going to replace the human, we think of the robot as a compliment, not substitute for a human judgment at the end of the day.

Speaker 1:          20:19          So here's one final, I think really interesting, uh, uh, application or potential application of machine learning that I think looks very different from what we're used to seeing in, uh, in the commercial setting. And that is to basically use machine learning is a behavioral diagnostic to better understand what the human beings themselves are doing right now, partly as a way to help solve this problem of how we optimally combine human and machine intelligence in designing a decision aid that we can give the judges and realize the potential for social good. So you know, normally what we do, normally what we do is we take a machine learning algorithm, we predict the defendant's risk, we do a horse race between the algorithm and the judge. So the defendant's behavior is the object of interest. The other thing that we can do is basically turn the algorithm on the judge themselves.

Speaker 1:          21:10          That is basically predict what the judge is going to do in a given case. But the judge is going to do as a function of the defendant's characteristics. Okay. And that turns out to give us lots of really interesting insights into what humans are doing. So let me just give you a couple, a couple of quick examples of that. So one thing, notice what this lets us do for starters. So if you look at the actual judge decisions, you know the only thing that you observed from judges is whether they detain or release a defendant, right? But if you think about our prediction of the judge's behavior, that's a continuous release probability at that point. And what we can do then is we can look across the defendant risk distribution to see where judges are at least certain. Okay. And you can see in this graph here, what we've done is we have the judges predicted released probability on the y axis here and on the x axis, uh, sorry, the judges detained probability predicted detain power probability on the exit on the y axis.

Speaker 1:          22:21          And then what we've done is we've been defending, so we've grouped defendants together based on the algorithms predicted defendant crime risk. Okay. So we're looking at the dispersion of judges release probabilities as a function of how serious the defendant's crime risk actually is. And what you can see here is it's really the highest risk defendants that are the ones where the judges have the most uncertainty and the most difficulty making decisions. And it's not, it's not inevitable that that's what we would have seen. So for instance, if you look at like education applications, what you see is that principals do amazingly well at predicting which teachers are amazingly good and amazingly bad. And principles have a huge difficulty distinguishing the quality of all the teachers in the middle, right? So it didn't need to turn out like this in the crime setting. It does turn out though that the judge, it turns out to be the case that the judges seem to have the most difficulty with the highest risk cases.

Speaker 1:          23:26          And one of the things that we're learning from psychology is that judges have very constrained judges. Like all human beings have very constrained bandwidth for judgment and decision making. And so one of the things for starters we can use this information about is sort of a bandwidth triage tool for judges. Here are the cases where you need to devote extra time and being expert shore in mapping the algorithms predictions into a decision, we know that these are going to be the cases that are hardest for you. Um, okay. And let me give you one more. Uh, one more example of this is, uh, the other thing that we can do then is we can look at the crime and detention outcomes of the actual judge decisions versus the predicted judge to decisions. So if you think about what the predicted judge decisions are, this is for a case with a given set of characteristics.

Speaker 1:          24:20          What does the judge do you on average? And how does the average judge do compared to the actual judge decisions that very around that, that average, okay, now economists would look at this and say it's clear. It's clear that the actual judge decisions have to do better because the judge has more information. The actual judge has more information than the algorithm because the judge sees more things than the algorithm sees. A psychologists look at this and say it's not obvious. We know that, uh, judges, there can be a lot of noise in the system too, and the extra information the judges have. And so what we actually see in the data is that the predicted judge that is the inflammation reduced version of the judge winds up leading do better crime and detention outcomes than the actual judge. Okay. Now why might that be, um, did you judges Miss Miss Predict?

Speaker 1:          25:19          Um, let me just tell you a, uh, a quick story about what I think might be going on here. Um, and it's a, uh, it's a story that comes from a conversation that I had with a friend of mine who is an emergency room doctor. And he was like the attending resident one night in the Er that he's working in and a patient comes in complaining of signs that look like he's having a heart attack. And so this is a common situation that they have in the Er and the Er docs. In that case, their key goal is to try and predict whether the patient is actually having a heart attack or not. And if they are, then they send them to the ICU for a, for immediate treatment. And so the, uh, you know, everybody else on the team, they go in, they do this sort of cardiac enzyme test, uh, to as part of the prediction.

Speaker 1:          26:07          And if it's above some sort of level, then that's sort of one diagnostic. And so they, uh, they do that. And then my friend goes to the, sort of, the doctor and nurse's station says, what do we have here? And everybody at the station says, you know, we've got this guy, he's got chest pains, we administered the cardiac enzyme test. It's above the threshold level, a lot above the threshold level. We got to get this guy into the ICU immediately. So then my friend goes in to see the guy in, uh, in the waiting room and the guy sitting there and he looks up at, um, at my friend, the guy sitting there eating a snack. He's having this, a slice of watermelon and he's talking to my friend and my friend's like, what's going on? And the guy's like, yeah, I'm having some chest pains.

Speaker 1:          26:57          I don't feel great. My friend talks to him for a couple of minutes, goes back out to the, uh, to the station and everyone's like, okay, we're ready to go, right? We're going to get this guy up to the ICU. And my friend says, uh, no, I, I, you know, I think he's all right. I talked to him, he's pretty chill. Uh, I'm not worried about it. And everybody else is like, no, no, no. We got to get them to the ICU. And my friend says, look, I'm the only one who actually saw the patient in person. You guys have just seen his medical test results. And I'm telling you, like I talked to the guy, he's hanging out, he's having a snack. There's, there's no big deal. 15 minutes later the guy goes into cardiac arrest and they race them to the ICU. I think what's going on there, what's going on there is, you know, the human brain is designed to be sensitive to very salient information, even if it's irrelevant.

Speaker 1:          27:53          We don't normally associate having a snack with having a major health disorder. Right. And so you can see how my friend, like the fact of having the snack was an enormously salient detail that turned out to be totally irrelevant. Um, I think the data that we're seeing are consistent with lots of things like that distracting judges in the court setting as well. Right. And so the key goal for realizing the social potential of these tools is going to be to figure out what exactly those salient but irrelevant pieces of information are that are leading the judges stray. That is not a computer science problem, right? In some sense that's kind of like the last mile problem of building these algorithms and helping judges and that last mile problem is not a computer science problem. That's really a behavioral science problem. So as I sort of look at what is, um, if I sort of look at, I'm sorry, the other thing to say is, you know, I'm using bail as an example here, but nothing that I've said is unique to bill, right?

Speaker 1:          28:57          There are tons and tons of super important policy decisions that policymakers and private citizens make every day with really important policy consequences that hinge on a prediction where the person themselves are currently making the prediction. But in principle we could be doing that with data and stead, you know what college major or course is best for me in college, students are millions of college students are trying to answer that every year. How long am I going to be unemployed is I'm trying to think about how much to scale back my consumption or where to set my reservation quality for some new job offer that I should be taking or not, or how much house can I afford? What's going to happen to my income stream over the future? Um, behavioral science tells us that these are prediction problems. They're going to be hugely difficult for us. So what does this teach us?

Speaker 1:          29:48          Well, I think one thing that we can see is that prediction can be super useful for making progress on these policy problems. Um, at the same time, there are potential pitfalls. Now I think what is really important about this, what's really important about this is that we know that there are, so it's, what's not news is that there are pitfalls in applying machine learning to policy problems, right? There are probably 10,000 papers and computer science published about, for instance, fairness. Okay. I am reasonably optimistic that the field will eventually solve that sort of problem because we recognize it and so many people are devoting, understandably inappropriately devoting so much energy to solving that problem. What worries me about this is that there are lots of other challenges here that are not even on anybody's radar screen and that could lead us to inadvertently take these tools, import them into a policy setting and accidentally make things worse rather than better. And I think the solution to those sorts of problems, they're not really about machine learning engineering. I think progress on those sorts of problems really are going to require taking these amazing machine learning walls and combining them with insights from social science and behavioral science. And I think the, the payoff to getting that right is potentially enormous positive social impact. Thank you very much.

Speaker 2:          31:22          [inaudible]

Speaker 3:          31:27          uh, you mentioned that you do things or you're able to do things like reduce racial disparities and um, in bail setting and things like that. And I can understand where, um, there's been obviously a lot of consternation about, uh, accidentally in coding demographic characteristics into the Dataset that didn't otherwise express them. But there may be a high correlation and so you could say a low income area is higher likelihood of missing bail, therefore imputed flight risk. So can you comment on how you are, how those things get controlled for and how that I understand the human bias element obviously, and snap judgments can, on the one hand it's probably a very bad thing. I think it's less well understood how a ml can be made to be more fair in a way that explicitly excludes a sort of inadvertent characteristics.

Speaker 1:          32:13          Yeah, yeah, it's a great, um, it's a great question. You know, I think that, um, one of the, uh, I think one of the, uh, challenges in, in my mind in taking these tools and applying them to policy settings now is that there is a wedge between, uh, what I think the statistics would tell us about what it, what promotes fairness and the way that our current legal structures, which were built around dealing with human predictions. And decisions allow. And so I might turn your question and turn on, turn it on its head just a little bit, which is to say isn't necessarily, so if a human being is making something like a hiring decision or a jail decision, you would say we ideally would have that info that human being completely blinded to defendant race or job applicant race. Like for sure that's true.

Speaker 1:          33:12          I think it's less self evident that that is true for the algorithm. Right. And I think part of the reason for that is because like unlike a human being, the algorithm has no preferences and has no capacity for implicit bias. The way that the human brain does the algorithm really is like a dog that chases the car that you tell it to chase. Like it's just very narrowly monomaniacal. Right. And sort of imagine, let me give you a stylized example to highlight what I mean. Right. So imagine that we're living in a city where the rate at which you know whites and minorities engage in crime, that true offending rate is exactly the same. Okay. Suppose assist the city is 50% white, 50% minority. Suppose that the police never make a false arrest to a white city resident, but that half the arrests to minorities are false arrests, right, and the chances that you get arrested if you actually engage in crime at the same for whites and minorities, right?

Speaker 1:          34:14          If I take that Dataset and I give it to an algorithm that's blinded to race, what's the algorithm going to do? The algorithm has no choice but to assume that each additional arrest on your rap sheet. Suppose that I'm predicting an outcome like failure to appear in court and for the moment, let's assume that that's not susceptible to bias. We can have a separate conversation about that, but just suppose that we're willing to assume that the outcome itself is not susceptible to buy us, right? If the algorithm is, is blinded to race, it has no choice but to assume each arrest on your rap sheet is exactly the same, no matter who you are and what's gonna wind up happening in that case is that African Americans on average, you're going to have much higher predicted. Failure to appear risk than whites. And if judges are instructed by the law to make detention decisions based on a failure to appear of risk, we're going to increase incarceration for minorities relative to whites.

Speaker 1:          35:11          But now notice what happens if I use a machine learning algorithm that has access to race and is allowed to search or mine the data for evidence of interactivity between the predictors. What the algorithm does and that case is it would immediately recognize that each additional arrest for a white has twice as much signal as each additional arrest for a minority in this city, uh, for risk of FTA. And in that case, if you sort of think about it, a little linear regression, in that case, the slope will be half as large for African Americans as for whites. And a average predicted risk in this scenario will wind up being exactly the same for whites versus minorities with only with the algorithm that has access to race. No, I was at a national academy of Science's panel last fall were a question like this came up and every computer scientist, social scientist, data scientist in the room, it was like, for sure we need to think about ways of letting the algorithm, you know, thinking about the conditions under which the algorithm should have access to race because it can in some circumstances promote fairness and every lawyer in the room, just one completely bananas and you can see exactly why the lawyers have that view.

Speaker 1:          36:27          And so I think this is an example where like the technology is outrunning the legal structures and I think that we're gonna, this is gonna have to be a big, a big area that we're going to need to think about resolving in the future as these sorts of algorithms become baked into our policy systems more and more.

Speaker 2:          36:47          Great. Joe,

Speaker 4:          36:50          thanks for coming in. This is awesome. Uh, you talked earlier about adjusting the algorithm to choose between either crime rate or release rate, and is it truly just one of the others there way to kind of balance both of them?

Speaker 1:          37:03          Oh yeah. Sorry. So let me, um, uh, I should've given you a better graph for this. One way to think about this. As you can imagine like a, a two dimensional space where you have crime rate on one axis of the detention rate on the other axis and the judges current decision is a point in that space, right? And what the Algorithm let's you do the algorithm winds up predicting crime risk much more accurately than the judge. And so the algorithm let's you have lots of both, lots of one without increasing the like less of one holding the other one fixed door. So you have a range of options that you can choose from in terms of how you want to realize those gains, right? So one extreme is I hold the jail population constant. I take all of the gain in the form of reduced crime. The other extreme is I take the crime rate constant. I, I realize all of the game from reduced jail or I can pick any point in between there and get some of both. And where you fall on that potential outcome space is entirely a policy decision. Not a, not a social science or computer science decision.

Speaker 2:          38:11          Thank you.

Speaker 4:          38:15          Hey, so are you doing, uh, I'm one of those engineers, computer scientists who is going bananas, right? Because there, there are a ton of concerns here. Oh. And I've worked on machine learning models and we know that the data makes a difference, right? The data that we're ingesting. Yup. It's pretty easy to beat, I think Chicago rates around racism, right? You're on racist decisions around socioeconomic status, et Cetera. Right? I mean, Chicago is when the most racist segregated places there are. And maybe it's because I grew up a few blocks from that jail, right. Just seen it over and over and over again. Right? So the fact that the academy that the algorithm can beat Chicago and Cook County judge rates is, um, it's great to see, right? And it's great that we're making better progress. But in and of itself, I don't think actually says that the algorithm is the way we should go.

Speaker 4:          39:04          Right. There's other human factors that have to be taken account. Yep. So part of it is yes, the data is messy because we have so much embedded systemic racism. And the last people I want to turn more data to is the Chicago cops because of everything that they do in the city to our communities. Right. Uh, so there's a lot of danger at play in how we use the data and you just mentioned as well. Um, and answer to the previous question is taking into account the demographics are not. And we know from Kathy O'malley, he spoke from the weapons of math destruction book, right. That we, you know, whether it's credit card rates, there's other ways to proxy race, right. That end up also embedding the same conditions that led to this situation throughout. So I don't want to make a, I understand that we can do a lot better. Right. And the algorithm could do better by also, I definitely want to challenge the dichotomy that says the algorithm and the technology in and of itself. Yes. Better without taking into account all the other embedded, uh,

Speaker 1:          39:58          yeah, concern. Here's the, here's the what. So it's a very fair question. Um, or, uh, here's a way that I would, I would sort of think about it is, um, I think of the role as a, of people like me and people like you, the other people in the, in the room is to, um, build tools and honestly and accurately describe what they're capable of doing. A and in a policy set, you know, in a commercial setting, you build these tools and then if you think they can, they can make money. You deploy them and if they're successful, they keep going. And if they're not successful, they get scrapped. Right? In the policy setting it's different. It's not my decision or you just, your decision to make, it's like some larger collective society, societal decision about whether we want to use these tools are not.

Speaker 1:          40:45          And I think in this case it really is a, um, I think it is something, it is a question that is not easily dismissed, right? So I come to you and say, I have a policy, let's call it a black box black box policy for the moment, right? I have a jail system that is 90% minority in this large anonymous city of eight and a half million people that were using the data from, I can assure you that the city population is not anywhere near 90% minority. Okay. I've got a black box policy got can reduce the size of the jail population by 42% right? Suppose for the moment we can get judges to follow the algorithmic recommendations perfectly or the black box ballsy record. I've got a policy lever that lets you reduce the size of the pot, the jail by 42% who benefits massively from a 42% reduction in the jail population in a world in which the jail population is 90% minority.

Speaker 1:          41:45          90% of the people that the algorithm that the black box policy, in that case we're doing releases, so benefit from this are minorities. Right? And then on top of that we can disproportionately further reduce the racial composition of the jail. Now it is true that you can look at that and say, yeah, but the algorithms not perfect because it's still relying on data that have baked in biases. Those are the same data that we would hand to the judge. I think the flip side is the alternative is the judge using exactly the same bias data in the current status quo system that gives us a jail population that is much, much larger and has a higher chair my than it currently has. And I feel like this is one where reasonable people can argue both sides of this. And to me it's not, it's not a crazy position to say, I will capitalize on the fairness gains that I have in front of me, which is the 43% reduction in the jail and a reduction in a share minority by 10 percentage point.

Speaker 4:          42:50          Thank you. Yep. Time for one more question. Um, yeah, I had a question around also kind of we're using data around the police and things like that. So are there projects that you're doing to analyze a ras and like the geographic breakdown, it was racial breakdowns of things like that that the police are doing versus you know, kind of continuing this kind of work. But just on the opposite side. Um, and then also, I think you may have touched on it, but how you're using this ml model in real time with judges and how, what you've seen with that, if that legal system is somewhat open to these tools, just to speak a little bit more on that if you could.

Speaker 1:          43:31          Yeah, yeah. I think, um, the, uh, for the, um, uh, let me take the second question for starters. So we are working with a large American anonymous city of eight and a half million people to think about whether there are ways to actually, uh, you know, so the tool that I, the results that I described so far, you know, we did that as an academic study as a kind of proof of concept. And the large anonymous cities said, uh, their policy decision, or you might disagree with it, but there are policy decision was if we could really reduce our jail population by 42% without increasing crime, that's something that we want to do. And that's something that we view as a step towards promoting fairness. And so we're working with them now to build this thing. Um, we're seeing sort of similarly encouraging results in terms of the predictive accuracy. I think the frontier science problem that we're encountering is how do you give this to the judges in a way where they're able to learn their comparative advantage versus when the Algorithm has the comparative advantage in an environment in which, right now there's not much feedback. And so we need to build a bunch of scaffolding around what the judge is doing to make sure that that risk tool itself translates into real better decisions.

Speaker 2:          44:57          Thanks.

Speaker 1:          44:59          Well, thank you all for coming. That concludes our talk. Let's give you guys a round of applause.