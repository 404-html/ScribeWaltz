Speaker 1:          00:06          Welcome to this presentation by the renowned David Eagleman. Um, and the presentation on can we create new senses for humans? David is a neuroscientist and a New York Times bestselling author as well as an adjunct professor at Stanford University. He's known for his work on sensory substitution, time perception, brain plasticity, synesthesia and neuro law. He's the writer and presenter of the PBS series, the brain with David Eagleman. And he said one of his most impressive credentials is that he is scientific advisor to Westworld. And so without further ado, David Eagleman,

Speaker 2:          00:53          what I had said is that the, uh, the scientific advisor of Westworld is the only thing anyone remembers, even though it's my least impressive credential. Okay. So here's what I want to talk about today. So I'm a neuroscientist and one of the things that's been of great interest to me for a long time is this issue that when we try to perceive the reality around us, we're only perceiving a little bit of it. So we're made at a very small stuff and we're embedded in this extremely large cosmos. And the fact is that human brains are really terrible at perceiving reality in either of these scales. And that's because we didn't evolve for that. We evolved to operate at the level of, you know, rivers and apples and mates and food and stuff like that right here in the middle. But the part that has always been strange to me is that even at this scale that we call home the scale that we perceive, we're actually quite bad at it.

Speaker 2:          01:46          We don't see most of the action that's going on. So an example of this, take the, uh, the colors of our world. So this is electromagnetic radiation that bounces off objects and hits specialized receptors in the back of our eyes. Um, and as many of you may know, um, the part that we call visible light is actually less than a 10 billions of the amount of light that's out there. So all this is electromagnetic radiation. It's just that we have receptors for this part and not for the rest of it. So you have radio waves and x rays and cosmic rays and microwaves and all this stuff is passing through your body and it's completely invisible to you. You have no idea that it's out there. There are thousands of cell phone conversations passing through your body right now and, and it's totally invisible to you.

Speaker 2:          02:35          Why? It's because you don't have the specialized receptors for that frequency instead of you only have it for this little range in between. Now it's not that this stuff is unseeable. So a rattle snakes for example, include part of the infrared range in their view of reality. And honeybees include some of the ultraviolet range and their view of reality. It's just that you can't see any of this, at least not yet. So what does leads to, I think it's a very counterintuitive idea that your experience of reality is actually constrained by your biology and, and that goes against the common sense notion that your eyes and your ears and your fingertips or just picking up the reality that is out there and all you need to do is open your eyes. Instead, what's happening is that we're sampling just a little bit of the world.

Speaker 2:          03:23          And what's interesting is that when you look across the animal kingdom, you find that different animals pick up on totally different signals. So they have different parts of reality that they're detecting. So just as an example, if you are the blind and deaf tick, then what you're picking up on is temperature and butyric acid, and that's the signals that you receive and that's how you figured out your world. That's your, that's the only signals that are telling you your reality. If you're the black ghost knife fish, you're in the pitch dark and all you're picking up on are perturbations in electrical fields. That's how you're figuring out what's around you. If you're the blind echo locating bat, all you're picking up on our air compression waves that are coming back to you from your chirps. And the idea is that that's, that's everything. That's your whole world.

Speaker 2:          04:13          And we have a word for this in science, it's called the Ou belt, which is the German word for the surrounding world. And presumably every animal thinks that there, Ooh, belt is the entire objective reality out there. Because why would you ever stop to imagine that there's something else beyond what you can sense. So let me do a consciousness raiser on this. Um, imagine that you are a blood hound dog. So your whole world is about smelling. You've got this very large snout. You have 200 million scent receptors in here. You have wet nostrils that attract and trap scent molecules. You have slits in your nostrils so you can a big giant nose full of air. You have floppy ears to kick up more scent. So everything for you is about smelling. It's your whole world. So one day you're walking along behind your master and you stop in your tracks with a revelation and you look at your masters knows and you think, what does it like to have the pitiful little nose of a human?

Speaker 2:          05:15          How could you not know that there's a cat a hundred yards away? Or how could you not know that your best friend was on this very spot six hours ago? But because we're humans, we are used to our [inaudible] velt. It's not like we have some, some sense that we're missing something. We're used to the reality that we have. We accept the reality that, that we're given with. The question is do we have to be stuck in our own belt? And so it was a neuroscientist. What I'm interested in is the way that our technology might expand our own belt and how that's going to change the experience of being human. So what many of you probably know is that there are hundreds of thousands people walking around now with artificial hearing, an artificial vision. So the way this works is, um, uh, with a cochlear implant and take a microphone.

Speaker 2:          06:06          You s you slipped an electrode strip into the inner ear, and you feed in this digitized signal into the inner ear. And the way it works with a, uh, a retinal implant is that you have a digital camera and that feeds into an electrode grid that plugs into the back of your eye. Now, this works. But what's interesting is that as recently is maybe 20 years ago, there were a lot of neuroscientists who thought this wouldn't work and the reason is because these things speak the language of silicon valley and that's not exactly the same dialect as your natural biological sense organs, and so they thought the reins, I can be able to understand these digital signals, but as it turns out, it works just fine. People plug these things in and they figure out how to be able to proceed with them. Now how do we understand that?

Speaker 2:          06:56          It's because here's the big secret. Your brain is not directly hearing or seeing any of this. Your brain is locked in a vault of silence and darkness and all it ever sees or electric chemical signals and that's it. So it has all these different cables that are plugged into it that they're bringing signals in. It doesn't know what those are. It has no idea what we would even mean by eyes or ears or nose and fingertips. All it knows is there's data coming in and what the brain is very good at doing is extracting patterns and assigning meaning to those and building your entire subjective world out of that. But the key thing is that your brain doesn't know and it doesn't care where the data's coming from. It just figures out what it's going to do with it. And this is really an extraordinary machine.

Speaker 2:          07:44          Um, essentially you can think about this like a general purpose compute device and you know, there's a lot of talk in silicon valley and hear about AI and all the great things that it's doing, but in fact we can't even scratch the surface yet of a system like this. Um, that just figures out all of the sensory information and figures out how to correlate senses with each other and correlate that with your motor movement and just, you know, make this world around you. So the point is, what, what I think a general purpose device like this allows for is that once mother nature has figured out these principles once, then she can mess around with the input channels. She hasn't have to figure out the principles of brain operation every time. And so this is what I call the Ph model of evolution. And I don't want to get too technical here, but Ph stands for potato head.

Speaker 2:          08:37          And, and I use this name to emphasize that all these sensors that we know and love, these are just peripheral plug and play devices. You stick them in and you're good to go. The brain just figures out what it's going to do with that information. And what's cool is that when you look across the animal, you find lots of different peripheral devices that can be plugged in, even though the brains across different animals all use the same principles. So just as an example, uh, with snake, you've got these heat pits. That's how it detects the infrared. And with the black ghost knife fish that I mentioned, it's body is covered with these electro receptors by which it picks up these perturbations in the electrical field. The, uh, the star knows mole has this funny nose with 22 fingers on it with which it feels out, the tunnels that it's boring through in the dark.

Speaker 2:          09:25          And uh, and that's how constructs a three dimensional representation of its tunnel world. Um, um, birds, this was just discovered last month, have cryptic chrome's which allow them to detect to the magnetic field of the earth. I mean, the fact that they could tell the magnetic fields we've known for a long time, but it's just discovered how they do it. Um, but you know, cows have this, uh, insects, most insects have this, they're all aligned with the magnetic fields, so it's called magneto reception. So the idea here that, that I've proposed is that, um, is that mother nature doesn't have to continually redesigned the brain with each animal. Instead, all she's doing is redesigning peripheral devices to pick up on information sources from the world, plug it in, and you're good to go. So the lesson that surfaces here is that there's nothing really special or fundamental about the senses that we happened to come to the table with.

Speaker 2:          10:20          It's just what we happened to have inherited from the long road of evolution, but it's not what we have to stick with. And I think the best proof of principle for this comes from what's called sensory substitution, which is the idea of feeding information to the brain via unusual channels in the brain. Figures out what it's going to do with it. Now that might sound speculative, but the first demonstration of this was published in the journal Nature in 1969 so there's a scientist named Paul Bucky, Rita, and he put blind people in a modified dental chair and the idea is that he had a video camera and he puts them in front of the camera and whatever was in front of the camera, you feel that poked into your back via this grid of solenoids here. So if I put a coffee cup in front of the camera and I feel that poked into my back, if I put a triangle, I feel that poked into my back and so on.

Speaker 2:          11:14          And blind people got pretty good at this. They were able to tell what was in front of the camera, just based on what they were feeling in the skin and the small of their back. So that's pretty amazing. And, and it turns out there've been many modern incarnations of this. So one of these is called the sauna classes. And the idea is that there's a, this is for blind people. Again, there's a camera here and whenever the camera's seeing that gets turned into an audio stream. So you here, this is a rule, rule, rule. And at first it sounds like a cacophony and you bump into things. And then after a little while, blind people get really good at being able to interpret all the stuff, the pitch and the volume and so on, to figure out how to navigate the world. So they're able to tell what is in front of them just based on what they're hearing through their ears.

Speaker 2:          11:59          And it doesn't have to meet through the ears. This is a version where there's an electro tactile grid on your forehead. And whenever the camera's seeing, you feel that poked onto your forehead with these little, these little shocks, why the forehead is cause you're not using it for anything else. Um, uh, the most modern incarnations called the brain port. Same thing for blind people to camera sees something and then it's put onto a little electro tactile grid on the tongue. So it feels like pop rocks on the tongue and blind people get so good at this that they can do things like throw a ball into a basket or um, or navigate a complex obstacle course. So if the sounds completely insane to be able to see through your tongue, just remember that's all that vision ever is. All vision ever is, is spikes coming from in, in the usual case coming from the retina.

Speaker 2:          12:51          It's just turned into spikes and sent back to the right of the brain, figures out what to do with it. Same thing here. Um, so in my lab, one of the things I got an interest in in many years ago was this interesting, uh, question of could I create sensory substitution for the deaf? And so the question is, if I, um, you know, had a person's space and they could have deaf person understand exactly what is being said, um, uh, just just with some sort of technology that we built it. So, so here's the idea we came up with. So first of all, let's say I have a phone that's picking up on the different frequencies in the room. So, you know, here, if I go, Ooh, you can see the thing picking up on the different frequencies and the idea is could I turn all those frequencies and do a pattern of vibration on the torso, let's say.

Speaker 2:          13:44          So that whatever sounds are being picked up, you're feeling that as a pattern of vibration on the torso. And so that's what we ended up building. And so this is the, uh, the vest that we built and the idea is I'm feeling the sonic world around me. So as I'm speaking, can you guys see the lights from where you are? I know it's sort of bright where I'm standing. Um, so as I'm speaking, the sound is getting translated into patterns of vibration on my torso. I'm feeling the sonic world around me as a pattern of vibrations. So we've been, uh, we've been working with the vest for awhile and um, and it turns out that people, deaf people can start understanding and feeling the, uh, what is being said this way. So let me just give you an example. This was a, this is actually our very first subject, Jonathan, 37 years old, born profoundly deaf and a, so we trained him on the vest for four days, two hours a day, and here is on his fifth day. Oh, can you turn the volume on? Let me start that over. So my graduate students, Scott says a word Jonathan, who's totally deaf, feels it on his vest and writes on the board what he's understanding

Speaker 3:          14:53          where, where touch, touch.

Speaker 2:          15:09          So the thing is, Jonathan's not doing this consciously, it's not a cost translation because the frames are 16 milliseconds and there's 32 motors and it's very complicated. Instead, his brain is unlocking the patterns. And the way to really understand this is to think about what your own ear does. I mean your own year is picking up on all the sound and breaking it up into frequencies from low to high and sending that to the brain and your brain is just figuring out, it sounds like, oh that's Eagleman is mellifluous voice that's going but in, but in fact your brain is busting it up into frequencies and doing all this work on it. And that's exactly what is doing. And you can think about this also with like when somebody is reading Braille, a blind person, you know it's just a bumps on the fingertip, but, but they can read a novel and laugh and cry because it has meaning to them.

Speaker 2:          15:57          The meaning has nothing to do with how it's getting in there. It has to do with how your brain is interpreting that centrally. And that's exactly what's going on here. But I'll just show you this and if this is useful because maybe this is brighter than what you can see on the stage, but here she's saying sound here. She's saying touch and you can just watch the pattern for a minute and you get, you get the difference here. So, just as an example of the word touch has a high frequency bit. Uh, when she says see h. And so you see touch and here she's saying sound. And so you can see how this works just by looking at it. And maybe that gives you a sense of how that, cause. Now the reason I think this is really, uh, important is because the only option for people who are deaf has a cochlear implant.

Speaker 2:          16:37          And that's, that's $100,000 and an invasive surgery. And we can make our vest for less than 500 bucks. And that opens up to the whole world. That means that deaf people anywhere don't have to worry about something of that. Obviously insurance typically covers this, but you still paid about $9,000 out of pocket. And so this is something that doesn't require surgery and as much less expensive. So that's why I think this matters a lot. Um, we recently had national geographic in our offices and we were filming, here's the guy who's deaf, but it's actually not because of him that we're filming. It's because of his daughter who's deaf and blind. And we made a miniature vest for her. We actually have a second subject now. Uh, another little girl who's deaf and blind. And, um, this is the only input she's getting. I mean, she, for the whole world is, is cut off to her that one belt is, um, this is not something she's receiving here.

Speaker 2:          17:29          Her grandmother is taking her around and touching her feet against things saying, okay, that's soft, that's hard. That's called whatever here. Uh, it's hard to see, but she's on a bed that's going up and down. And so the grandmother saying down, down, down, and then up, up, up, and she's just training her on these correlations, which is exactly how you learn how to use your ears just by understanding these sorts of correlations. So, um, this is work that will, uh, you know, over the next, over the next six months to a year, we'll have a lot more, um, participants on this and a lot more data about how that's going. But the key is that young brains are so plastic that this is where things are really going to the kind of fly. Um, we've also built a, uh, a wristband that does the same thing as the Vasper instead of 32 motors. It's got eight motors on it, so it's slightly low resolution, but it's, it's much less friction as far as people using it. This is, this is our first subject with the wristband. He happens to be the president of the San Francisco Deaf Association and he ended up crying when he wore this because, um, you know, the whole world was coming to him. And so he's just describing here what, what kinds of things he's able to do.

Speaker 2:          18:45          So anyway, we, so we're doing lots of stuff with this sensory substitution. We've been very, it's been very heartwarming and encouraging to us how all this is going. We're screaming along with this and if anyone's ever in Palo Alto in California, please come by and visit our offices. I'll show you what we're doing. But what I want to tell you about now is the stuff that we're doing, not just with sensory substitution, but I started thinking a lot about sensory addition. What if you took somebody who didn't have deafness or blindness or something like that and added something on. So for example, what if you took a real time stream of data from the Internet and fed it in? Could you come to have a direct perceptual experience is something that's new. So, um, here's an experiment that we did in my lab where this guy is feeling a real time feed of data for five seconds. [inaudible]

Speaker 2:          19:36          did it from the Internet and then two buttons appear yellow and a blue button and he chooses one in second. Half later he gets feedback, either a smiley face or a frowny face. Now he doesn't know that what we're doing is feeding in real time data from the stock market and he's making buy and sell decisions. And what we're seeing is whether he can tap into and understand or develop a direct perceptual experience of the stock market and the economic movements of the planet. This is a totally new kind of human ooh, belts, something that humans don't normally experience. Um, the, uh, another thing we're doing, we can obviously scrape the web for any kind of Hashtag and, um, and feel what's going on with the community on Twitter. And again, this is a new kind of experience for humans to be plugged into the consciousness of thousands or millions of people all at once and feel what's, what's happening with that.

Speaker 2:          20:28          It's a bigger experience than a human can normally have. We're doing lots of things like, um, taking a molecular odor detector, um, uh, and, and hooking it up to somebody so that you don't need the dog anymore so that you can experience the same sorts of smells that the dog can, um, and feel the different substances that way. We're working with robotic surgery so that a surgeon doesn't have to keep looking to understand what the data is with the patient in terms of blood pressure, how the patient's doing and so on, but instead can feel all that data. Um, we're working with patients with prosthetic legs where, you know, for somebody who has a prosthetic, it's actually hard to learn how to walk because you're not feeling your leg. You have to actually look where the leg is to understand where it is sending in all moments.

Speaker 2:          21:13          So we just hooked up pressure and angle sensors into a prosthetic and then you feel that on your torso and it turns out this is unbelievably helpful in getting someone to just use it and walk because just like your real leg, it's just like your real egg and you're feeling what your real leg is doing. It's just you feel it, uh, on a slightly different patch of skin and it turns out that's no problem. That's actually quite easy for the brain to figure out. Um, another thing that we're doing that's very easy for the brain to figure out is we did this collaboration is the collaboration that we did a with a Google team in the bay area where they have lidar set up in their office. So we came and tapped into the data stream so that we could tell the location of everything.

Speaker 2:          21:55          And then we brought in a blind participant, put the vest on him and he could tell where everybody was by feeling where people are around him. But then also we put in this navigation function where we say, okay, go to this conference room and he's never been here before. And he just follows, okay, go straight, go, left, go. Right. And he just follows along and gets right to where he's going this way. Um, I was at a conference two weeks ago, uh, that Jeff basis puts on. And last year at this conference, he, uh, gotten a mech suit. So this is a giant robot and he's sitting here and he can control this mech suit. And so, uh, what my team did this year is put together a, a, this is just in Vr, but we did this demo of, okay, if you were actually in the next suit, then what would you want to feel from the robot?

Speaker 2:          22:43          And specifically it's every time the robot steps, you feel that when the robots moving its arms, you feel that you feel all the data from the robot. If somebody throws something at the robot and it hits, you feel that. So the idea is if you're inside this mech suit, the thing that really ties you in and makes you one with the machine is feeling what the machine is doing. So we had a very cool demo that, uh, we're doing various things with VR where inside the VR world you are. In this case, it's a, it's just sort of a shooter game for entertainment. But the idea is you're getting shot at from different angles and you turn around and you see where people are shooting you from. But what we're doing with this now is we've just made this for social VR where you can, it's a haptic suit so that while you're in VR and people are touching you, um, you, you feel that, so someone touches you in Vr, you feel it in real life.

Speaker 2:          23:35          We feel the raindrops or bumping into a wall or somebody throwing a tomato at you or whatever the thing is in Vr, you're, you're actually feeling that. Have you guys seen ready player one who seemed ready? Player one. Okay. A few of you. So there's a haptic suit in there. Uh, and so we've, we've got that. Um, so we're launching this with, uh, we're launching this with high fidelity, which is if you guys, if you guys remember second life high fidelity is the, the guy started that sort of high fidelity. It's the new, um, you know, social world of Vr. So that's what we're doing with that. Um, as Sarah mentioned, I'm the advisor of Westworld and so the vest is in Westworld season two, which starts Sunday at 9:00 PM, and I'm calling it vast world now. Um, so we're doing various things. We have a, with drone pilots, we hooked it up so that the drone is passing the pitch y'all role orientation and heading to the person wearing the vest.

Speaker 2:          24:31          So it is essentially like extending your skin up there so you are feeling exactly what the drone is experiencing and the advantages that you can, um, uh, you can learn to fly in the, in the dark, in the fog, things like this because you are, it's just like what the Mech suit. You're becoming one with the machine and you're feeling it that way. Yeah. There's a lot of talk about brain computer interfaces where you're, you know, I mean two of my colleague and the friends are doing companies where they're thinking about, okay, how do we implant electrodes into the brain? But the fact is that planting electrodes in the brain has a lot of limitations. The main one being neurosurgeons don't want to do it cause there's always risk of infection and death on the table and consumers don't necessarily want to get a hole drilled in their head.

Speaker 2:          25:15          Um, so this is, this is a solution that's readily available right now. And where this is going by the way is with things like this. So this is what a modern cockpit looks like and it's an unbelievable number of gauges and things to look at. And the thing is our visual systems very sophisticated in certain ways. What they're good at is detecting motion and edges and blobs. What they're bad at is looking at high dimensional information. So what you have to do if you're a pilot is looking at each one of these individually. You can only attend to one thing at a time. It turns out that with the Somata sensory system, you can take in high dimensional information, which is why you can, you know, balance on one leg. There's information from all these different muscle groups coming in, but my brain has no problem integrating this high dimensional information to do that.

Speaker 2:          25:59          Whereas your visual system runs in a very different way. And it's very much about a serial focused process. And so the idea is we're living in a world of big data now, and is there a way to, instead of just having access to big data to experience it directly. So this is one of the places we're going with that we're a, our goal is to do this with factories as well. Instead of staring at monitors, just imagine feeling the state of, of the factory and this high dimensional system. And I'm not talking about alerts. Alerts are easy. You don't need something like this. But I'm talking about feeling how the whole system is going and where it needs, you know, where the pattern is moving in this high dimensional space. And you know, um, the, the key is I think with the right sorts of data compression, there's no limits to the kind of data that we will be able to take in.

Speaker 2:          26:48          And so, um, you know, just imagine an astronauts being able to float around instead of look at all the monitors to understand how the international space station is doing, just they, they feel it at all times or having access to the invisible states of your own health. So your, your blood pressure and the state of your microbiome and so on. All these things that are invisible to us. Imagine having the made explicit as you're feeling that or um, being able to see infrared or ultraviolet or be able to see in 360 degrees. So essentially there's no end to the possibilities on the horizon here. And I think the key is as we move into the future, we're going to increasingly be able to choose our own peripheral devices. So, um, you know, we don't have to wait for Mother Nature's sensory gifts on, on her timescales, eyes and ears and nose and fingertips and so on. We don't have to wait around for that anymore cause that takes several, a million or hundreds of millions of years for each new iteration. Um, but instead, like any good parent, what she's given us is the capacity to go out there and create our own trajectory. And so the question, especially with a smart audience like this is how do you want to experience your universe? Thank you very much.

Speaker 2:          28:14          The applause feels good on the best. So I'll take any questions about anything. I think I'm supposed to tell you guys, hear the microphone for that.

Speaker 4:          28:24          Uh,

Speaker 5:          28:25          so yeah, I'm wondering kind of like what are the, the limits of the haptic perception that you have or like, you know, where does it break down or is there, is there like, um, kind of like fatigue after a while? Like you just get tired or are you start getting numb to the perceptions are

Speaker 2:          28:43          great. Great. Let me answer this in two ways. So as far as the getting numb part goes, no. What's interesting is when, when I first put the vest on everyday or the wristband, you know, for the first, I dunno, let's say 60 seconds I'm feeling and I'm really aware of it and then it fades into background, but it's not because I'm getting numb because if anything happens is unexpected, I, I immediately feel it. So instead it's just like the feeling of your shoe on your left foot, you're not paying attention to, but if suddenly you get a pebble in it, then you're paying attention to it or you can attend to it right now and thinking about how your foot fields, um, so it's exactly like that with the vest. And the key thing about using the skin is that, you know, the skin is the largest organ of the body and it's incredibly sophisticated.

Speaker 2:          29:24          It's got all these receptor types in it. This is unbelievably useful Oregon, but we just don't use it for anything. The joke in the lab is that a, is that we don't call this the waste for nothing. It's just totally not used. And so, um, yeah. Anyway, you don't fatigue as far as the limits go of what kind of data we can pass into it. We don't know yet. What is clear is that some things you learn instantly. Just as an example, the thing we did with, with blind people where there's Lidar, which knows the location of everything and the guy who's wearing it, he could tell, okay, there's someone walking up on my left and now the person's walking around behind me and so on no learning. I mean it was, it was instantly he got it with something like deafness, people have to, people immediately do you get some things right away, like if we present to the wristband or the vest, you know, dog bark or smoke detector, a baby crying or whatever, they get that right away. Um, but other things are more challenging than it feels to me. Like the more removed the data set is like, let's say I'm doing factory data, uh, it just has to be something where you train and learn on it. Thanks. Thanks. Let's go over. Let's switch sides. Yeah.

Speaker 6:          30:35          Um, is there any kind of like, promise having your skin do double duty? Like do you like, could you get so used to like hearing through your skin that like if someone were to touch you, like Kinda hear something then

Speaker 2:          30:47          great question. The answer probably is no in the sense that the way that you hear is this very high dimensional pattern and so some would have to touch you in a very particular way, every 16 milliseconds. So that's why we haven't run into that yet. I don't, I don't, I don't foresee that happening. Um, yeah, that's the answer. And, and you know, the general story is that, like I said, because we all were clothes nowadays and so on, it's just, it's not really, we're not utilizing this for much of anything. By the way. Other people have come up with very clever ways of using, you know, a hearing or a hearing or sight or anything like that to pass the information. But the problem is those are senses that you're using. You actually need to use your, your vision and your hearing. Um, and the thing with that brain port that I showed you, the thing that sits on the tongue, it's a great proof of principle for sensory substitution, but it's really stupid as a device because you can't eat and you can't talk when it's in your mouth. So this is why I really wanted to do something that was totally unobtrusive as in you guys didn't even know I was wearing. It was just something worn under the clothing and something that takes advantage of all this skin that you're not using for anything. Yeah.

Speaker 6:          31:53          Yeah. Thanks a lot from doing this talk. This is extremely interesting. I was curious to learn more about the learning process because if you make a knowledge with machine learning, there's usually need to be some label data and he's a train. This prediction is extremely wrong. This is predispositions. Okay. So I was curious, have you started thinking of how to make the interpretation like how to make brain pick up the interpretation faster, better? Is there Craig? So how does it work?

Speaker 2:          32:17          Yeah, thank you. Great question. Um, so of course you know that the difference between artificial neural networks and brain are all that works is a miles a difference, right? Because with an artificial one, you need millions of exemplars and you just don't need that with the brain. But the way that we train, um, deaf people for example, is we'll present a, a, a word to let's say the wristband and invest. So you, and then you see four choices on the screen and you have to guess which word you just felt. And at first you have no idea. So you making guests and you're given feedback about what's right and wrong. This is just like these foreign language learning programs where you get feedback and you start getting better and better at it every day. Um, the reason we do those sorts of tests so that we can quantify exactly how things are going, but the real way that deaf people learn is two of them. One is they watch your lips and as they're watching your lips and feeling it, they're making the correlation that way between what they're seeing and what they're feeling. And the other way, which is even better is when they vocalize, they say something and they feel it. And that's by the way, how you train up your own ears when you were a baby. You know, you babble and you're hearing it and that closes the loop and you figure out how to use your ears. And that's what's going on here. Thank you. Yes.

Speaker 7:          33:30          So you talked a lot about substituting a new senses for an organ then maybe it doesn't exist for someone or introducing some new sense. Uh, do you know of any work about expanding a sense that you already have such as seeing wider range of lighter hearing, new things are getting better at touching things?

Speaker 2:          33:48          Yeah, so thank you for the question. Asked me this question again. In three months, I'll be able to tell you more than I can tell you right now. But, um, my deep interest is in, uh, for example, the, with the visual spectrum that I showed at the beginning, if we were born 500 years ago, it would have been a very different situation because the world was unmapped [inaudible] you would have been able to sail around and find new lands. Now we can't do that. Uh, everything is already known in the world, but it's not true for the visual spectrum. For the, let's say the em spectrum. Like I feel like I get to be a pioneer and walk around on this 10 billion size grid to find out what is meaningful to us as humans on that credit and no one's ever walked around. And then before that, obviously we build machines in our cars to pick up on radio waves.

Speaker 2:          34:40          We build machines and hospitals pickup on x rays and so on. We have various things that pick up on different parts here, but there's a difference when you're actually a human walking around in this spectrum. Just as an example, um, some friends of mine makes microwave cameras that sit on satellites for various reasons, but what they discovered totally accidentally is that you can tell if water is drinkable or polluted just by looking at it in the microwave range. But none of them knew that before. Why? Because you need to sort of be a human who cares about these things. Say, Oh look, there's this thing here. And this training said, the point is, I feel like there's, if I had to make a guess, I'd guess there's 30 Nobel prizes that are hidden along the spectrum for people to just make discoveries about cool stuff. So, so, uh, I should mention one of the things that we're doing is we're releasing the vest and the, and the wristband with an open API so people can put in whatever data streams they want. They can work cameras for different parts of the range, hearing for different parts of the hearing, anything like that and go around and see what's up

Speaker 8:          35:40          there in the world. Thank you. Thanks so much.

Speaker 1:          35:44          Hi. Um, I'm working on the entire section of Vr and empathy and I think a lot about perception, perception of emotion. And I was wondering if you think we could use a similar device to help people understand the other person's emotion or the emotions around them.

Speaker 2:          36:01          Thank you for the question. It would totally depend on having a sensor that can do that. In other words, if I had a machine that did, that's a facial recognition or pitch recognition of voice or whatever and could figure it out, the answer is that it's easy to feed that in so that I'd become aware, more aware of how somebody is feeling, but, but I, I would need the sensor in order to tell me what the right answer is to feed it in. Um, and the other thing is I've gotten, I've gotten a version of this question several times about whether this would be useful for autism, probably not. And the reason is many kids with autism have what's called sensory processing disorder where they can't stand the feel of things like the clothes are wearing or whatever. And so having all this buzzing probably wouldn't work for them unfortunately. Um, but anyway, that's the answer about empathy or anything else is if there's a way to sense it, then it's very easy to feed it in. So you're getting that data.

Speaker 1:          36:53          If I can have a second question. So you work on sensory data. Um, do you think it can help change the way brain works? Like if there's a brain disorder, can these devices be a compliment to buy? Have you process data? Okay.

Speaker 2:          37:09          Yeah, I totally think so. Um, I mean this is just one example of many, but the thing about the prosthetic leg, it's that you just don't have that data anymore coming from your leg. And it just took us, you know, two hours to be able to just fix that. So now somebody can feel their leg is though it's a real leg. Um, and I think, you know, one of the big problems with stroke with Parkinson's disease and so on is losing sensation in a limb. So forget prosthetics for a minute. Just, you know, just hooking this up so that you can feel what your limb is doing. So it doesn't just feel like this big numb thing, but you're feeling it. That's a, that's another example.

Speaker 8:          37:47          Thank you. Thanks very much.

Speaker 1:          37:50          I'm high. So I find the navigation applications are interesting and I, my question is, um, before we get to the, um, the airplanes and spacecraft navigation, not that that's an important, like is there an application to more immediates navigation needs? Uh, for example, like I can't tell you how many times I almost got hit by a car looking at Google maps on my phone, no offense to Google. Um, or like, um, I navigating back to safety. Like I do have a friend who got lost skiing cause he lost his way. Um, or like that episode of the Office by Michael drives his car into a lake. Uh, dups. Um, so I was wondering if there's like, I need more immediate applications that we can use in our daily lives.

Speaker 2:          38:35          Yeah. The

Speaker 9:          38:36          thing. Thank you for asking me. The thing that we did with the, the blind participant where he's getting navigation directions that way. No.

Speaker 2:          38:44          In the office. Right, exactly. And as I said, that's a product. We're doing a collaboration with Google. Um, I, I'm uh, now transferring that over to the wristband and so I built the risk man with eight motors. So you have the cardinal directions plus the in between directions and it doesn't have to be someone that's blind. It can be for any reason at all. Uh, first of all, if there's any kind of detection about what's around you, you can know, oh, there's someone to my right, there's someone behind me, there's whatever. Or you can be told, oh yeah, when you get up here, turn right, turn left, blah, blah. That sort of thing.

Speaker 9:          39:15          Cause I remember thinking like, I wish Google math had like a vibration thing, but, or like it just vibrated on my rest that I would know which way to turn instead of having to like look down on my phone.

Speaker 2:          39:24          Exactly. Right. That's exactly right. And you know, I'll just mention for clarification that, um, yeah, so as people say, oh, well wait, doesn't the like the apple Iwatch do stuff like this? But of course it doesn't. It just has a single motor in it. And so by having the, the spatial pattern and the motors, one of the things that's trivial for the brain to learn is, oh, you know, okay. Yeah. Got It. That's left. That's the right that's behind me. That's in front of me. Yeah. That's easy. So, okay. Okay. Thank you. Yeah, thanks. Yes.

Speaker 9:          39:52          Hi. Uh, how much does your team know about how difficult it is for someone to switch between different kinds of sensory augmentation? In other words, uh, will I be limited to a single sensory augmentation out in my vest?

Speaker 2:          40:05          That's a great question. We don't know the answer to that yet. Here's what I can tell you. The brain has what are called Schema where it's like, okay, in this situation, this is what this data stream is. And that situation, this is what I mean. I'll just give you an example. A few months ago if you wanted to go, I was throwing a football around with some friends and I, it hit my vehicle and knocked the rear view mirror off. So the next day or that afternoon, I got in my vehicle and I was driving and I noticed I kept making eye movements up here and I was seeing into the trees and I thought, what am I doing? And it's because of course I'm used to looking that way to see behind me. But I only do that when I'm sitting in my car seat. I would never do that walking around the street.

Speaker 2:          40:43          I've never suddenly look there to see behind me. So my brain had unconscious learned a Schema, which is when I'm in this context, then I've got these completely different, uh, you know, sensory capacities. So the point is the brain's always doing this. So it may be possible to learn more than one. We don't know. We just haven't tried that. My best guess for what would be easiest is to have like two wristbands or you know, an ankle bracelet or whatever. We're building all sorts of other uh, form factors too. Um, and so depending on the apps that you wanted, like if there were mainly two that you wanted possibly it would be easiest just to have them on separate parts of the body which go to separate parts of the brain. They can ask me that again in six months. I might have more data to tell you. Thanks so much.

Speaker 9:          41:29          Uh, I have two questions. What happens to the visual cortex of someone who's born blind? And second, if you're translating, uh, auditory signals to a or visual signals to auditory for someone who's blind, do you see activity in that area of the brain?

Speaker 2:          41:47          Great questions.

Speaker 8:          41:49          Okay.

Speaker 2:          41:49          And this is actually the topic of my next book that comes out next year called live wired, which is to say,

Speaker 8:          41:55          okay,

Speaker 2:          41:55          what you have are these cables that plug into the CORTEX. So from the eyes you have data cables that go and they plug him back here and then we say, oh, that's the visual cortex. But in fact, the only reason we ever think of that as the digital core text is just because that's where the information goes and becomes a visual cortex. But if you are born blind, it's no longer the visual cortex. Instead it gets taken over by hearing, by touch, by vocabulary words, by all of that stuff. Why? Because the Cortex is actually the same everywhere, all over the brain and what it looks like and what we call in textbooks. It's just a matter of what kind of data is plugging into it. So back in the early nineties, in fact, um, Rogowska Sore, a colleague of mine took the visual neurons that would normally go to the visual cortex and he rerouted things. So they plugged into what we'd normally call the auditory cortex and then that became the auditory cortex. Sorry, it became the visual cortex. In other words, if you plug in that data, and that's what shapes that area. Um, what we now know is that this is incredibly fast, this whole process. So, um, if you, uh, blindfold me tightly and stick me in a scanner within 90 minutes, my visual Cortex is starting to respond to sound and touch and things like that. So in other words, the takeover of these areas is extremely fluid. Um,

Speaker 8:          43:14          okay.

Speaker 2:          43:15          So that's the answer to the question is there's nothing special about visual cortex or wherever. It's just a matter of how much information is coming in and where the information is coming in. And if the brain finds it relevant and salient, then it devotes territory to it.

Speaker 8:          43:28          Thanks. Yeah.

Speaker 9:          43:30          Uh, so a lot of the applications that we saw, the vest had been for a strictly communicated purposes, uh, is there, uh, like also say, uh, like a possible emotional response. If you played a song to the best could, could you learn to perceive something like music through like the more tactile sensation and kind of get the same kind of response you got hearing it?

Speaker 2:          43:49          Yeah, it's a good question. So one thing we've discovered quite accidentally is that deaf people really like listening to music on these things. Um,

Speaker 9:          43:57          all right. It feels really good.

Speaker 2:          43:59          Exactly. It feels really good. And it in fact, one thing we've done is listened to, for example, the radio with this on and you know, it's broken up in all the different frequencies, the cigarettes, a high note and you're feeling it. It's an amazing good. And when you turn the vest off, the music feels sort of thin, like you're missing something now. Um, so it is terrific. Uh, one thing I'll just point out is that we only have 32 frequency bins on here. So you're not actually capturing all the possible notes. You just kept sort of lumping binning of those. Um, nonetheless, what you get out of it is, you know, the rhythm and the feeling of where the music's going and the highs and lows and all that. So people, people, even though I hadn't predicted this just, they liked that possibly more than anything else they do with the vest. Yeah. Thanks.

Speaker 10:         44:44          32 is a lead into my question. How do you, how do you characterize the richness of what you can input through the vest? Um, I mean, I suppose there's frequency and amplitude and special resolution and, and um, and how do, what are the dimensions and, and the, the follow up question then is how does that compare to the potential capability of the torso?

Speaker 2:          45:05          Um, so let me say three things about that. One of them is that we've also built a version with 64 motors on it. Um, and the only reason we're not using that is just because 32 seems to be totally sufficient and it's, you know, easier and cheaper to build. But, um, so there are several things. One is the how, what does the spatial resolution, how close can we get these motors together? There's something called two point discrimination which we measure, which is just at some point if you move signals on the skin too close together, your brain can't distinguish those. So we've carefully measured everything on the torso and published on this sort of thing about how far they need to be. Anyway. The point is 64 is easy. Um, we could probably fit, I don't know, up to 80 or 90 on the torso with no problem.

Speaker 2:          45:46          As far as what the motors represent, I probably with this audience, should have been more technical about it. The, each motors representing a different part of the frequency band from low to high. So in other words, um, this is the sound that is captured. We typically cut it off from like 300 hertz to let's say six to 8,000 hertz at the upper end because you don't actually need anything higher than that, even though ears can hear a little higher than that. Um, and so then each mode represents some beginning of the frequencies and just represents the amplitude. So if this bin has a lot of aptitude than that motor is harder. Yeah, I think that was all the questions that yes, it did. I miss something that, I mean, that's what we have. I'll just get into one of the thing. We've got a lot of sophisticated software, three years' worth of stuff that we've worked on to, to do all these other tricky things like you know, noise, Florence, and let's say we're talking and some of the air conditioner kicks on him within like 20 or 30 seconds, that'll get canceled out. So I'm not hearing noise in any different frequency band. And we have an adaptive ceiling and adaptive noise threshold and all kinds of other tricks we put in. But essentially think of it like a 48 transform with binning.

Speaker 10:         46:56          That answers the question for sound, but it doesn't really say is this equivalent to what's in ready player one

Speaker 2:          47:02          oh with the ready player. One thing that I'm almost embarrassed about that because that hardly utilizes all the capabilities we have ready player one, it's if there's a collision here, buzz that motor. So, so I'm just feeling where everything's going and there's all sorts of illusions that, that we implement about, you know, even though there's motor here and here, we can make it seem like any point along, anywhere in between has been touched. Um, I can explain how we do those. And so on. But that's simply, hey, where was my avatar touched? That's where I get touched. So that's the easy part.

Speaker 8:          47:36          Yeah.

Speaker 7:          47:36          Yes. Should I be worried that I'm too old and my brain isn't going to be able to like pick it up as well as a younger person?

Speaker 2:          47:43          Great question. No, we've been testing, we've tested it on 432 deaf people as an example. And uh, the oldest is probably around 70 or 75 and they can get it pretty easily as well.

Speaker 7:          47:52          Is there like a difference in them how quickly the yeah.

Speaker 2:          47:54          Yes, exactly. Very good. So, so if we plot things from like 16 year olds, this 75 year olds and we're looking at, let's just say how fast they pick it up, it does go down and it's essentially linear. Uh, so it just, it just goes down. So it just takes a 75 year old longer to learn it. They still learn it. It's just harder.

Speaker 7:          48:13          And do they get to the same level of math?

Speaker 2:          48:16          I think so. I think so. Um, ask me that again in about a month and I'll be able to tell you the data. But the cool part is on day one, right when people come in, when we present sounds to the wristband and they say, hey, was that a dog barking or footsteps or a microwave, Ding or whatever. People are pretty good at that straight away without ever having worn it before. It's sort of surprising the intuitive when you're feeling stuff, by the way, Andrew, who's ever around at the end, you can come and feel what it feels like. Yeah. Thanks. Yes.

Speaker 7:          48:46          Have you spent much time focusing yet on security and privacy in these, uh, privacy being, if someone could extract all the sounds that you heard or security being someone could just make it seem like you're hearing something else?

Speaker 2:          48:58          Yeah, a good question. The answer is yes. We've made sure this is really secure. So the, um, as far as, um, recording sounds, there's no recording that goes on. So, just as example of the wristband, the microphones are built into here and it's capturing the data and doing the 40 transforming all the other tricks that we're doing. And then, but it's not getting recorded anywhere. What is actually happening. And, and with this thing, we don't record anything either. So we're, we're sure about that. And then the other, and then for the passing the information, we're just making sure that it's all secure. Yeah. But, but we've, we've thought about that also. It would be a, you know, whatever this is a psi Phi story 50 years from now that somebody puts in information that, yeah. Hey Bob. And he turned around and there's an there. So thank you for that question. Yeah.

Speaker 11:         49:43          Ah, hi. I have two related questions. I'm one, what is the battery life on those things and to, can I buy one?

Speaker 2:          49:50          Great. The battery life is 16 hours and we want it to make it so it's just like a cell phone so that you wear this all day long for example, and then you plug it in at night. Um, and the answer is this, you will be able to buy in December. And this, we are, um, uh, about July. So this is art. This is available for preorder on the website, and this'll come out in the seven months, eight months from now. Thank you. Thanks very much. Any other questions?

Speaker 8:          50:20          Yeah.

Speaker 2:          50:23          Great. Thank you very much. Great. Thank you guys so much.

Speaker 11:         50:27          [inaudible].