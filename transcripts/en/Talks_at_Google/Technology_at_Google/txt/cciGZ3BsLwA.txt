Speaker 1:          00:09          So again, thank you for taking the risk to come to a talk where there was a mysterious word in the title. I will explain it over the course of this talk. Um, so the first place I want to start is with this person. Um, I've already given a clue, but does anyone know who this is?

Speaker 2:          00:25          Yeah,

Speaker 1:          00:25          it is. It is eight 11 list. Um, and the reason I bring her up on screen, um, is uh, not because of her stature. Um, and not because she was the world's first computer programmer working with Charles Babbage. Um, but because it was in her memoirs of working with Babbage, um, that she included this quote. Um, Babbage of course, did both the differential engine and the analytical engine. And in her memoir she put the sentence, the analytical engine has no pretentions to originate anything. It can do whatever we know how to order it to do. I'm Alan turning in his understated, a seminal paper computing machinery and intelligence called this lady, uh, level laces. Objection.

Speaker 2:          01:16          Okay.

Speaker 1:          01:17          This caught my eye when I read this, partially because, um, she was objecting against something that nobody had objected to yet. They didn't have time. The computer didn't exist. So why was she sort of forestalling an argument here? Why was she saying no, it's cool. Computers can't do anything. Um, I looked further back, obviously then Lovelace and Babbage and found the answers probably in mythology, right all the way back. And I know this is a screen grab from the Disney version. Um, but the original good to version of the sorcerer's apprentice was a piece of mythology that said, hey, look, tech can go way out of control. And in fact, in that poem, it was only the presence of the sourcer that saved the world from destruction, from tiny brooms.

Speaker 2:          02:05          Sorry.

Speaker 1:          02:06          Another myth comes from the Golem of Prague. If you're not familiar with this, this was a, a creature made of clay. It had a pretty cool little thing where you put a gif on its forehead to bring it to life. Um, but then after that, this one doesn't have a mouth. Um, but anything that you wrote as an instruction and put it in its mouth, it would just follow mindlessly as best as it could. Right? And it was a story of sort of Hubris and terror. Watch the machine go crazy with this instruction. I'm going, of course, we still tell terrifying tales of machines that go awry for their instructions and, and rec human damage on their, uh, on their path. Right? That's a how from 2001, a space odyssey. So there are lots of places across myth in past, before Lovelace's time and certainly after where we're telling ourselves this, these tales of the technology is terrifying and should not take initiative. Um, and I'm going to take that question as we move forward, um, partially as a narrative construct, but it's also the question that drove some patterns that I had identified probably about an starting about 10 years ago, but, uh, really started thinking about it pretty hard about five years ago and that was a pattern that I saw in the world.

Speaker 1:          03:25          So does anyone know what the object on the left is? It's pretty easy to gimme right? This is to encourage you to talk. Um, it's a DSLR camera. In fact, that's the model that I have. Does anyone recognize the object on the right? It looks like a tile. In fact, it's creepily like a tile, but it's not a tile. Yes, that is the get narrative camera. Um, and if you're not familiar with it, the way it works is you, you leave it plugged up and plugged in overnight so it has a battery charge and then you unplug it. As long as that Lens sees light, it takes a picture every 30 seconds of whatever's in front of it. Then you come home at the end of the day, the camera's taken about three, 5,000 photos. You plug it in and it does for cool things. The first is that automatically uploads all those photos to a server.

Speaker 1:          04:12          The second thing is it uses some smart algorithms to divide your day into scenes. Here's when you were at breakfast, here's when you were at lunch. Here's when you were listening to Chris Talk. Third thing it does is it uses some algorithms to detect what the best photo from that set is based on things like level of clarity, contrast, that sort of thing. And the last thing it does is it uploads those photos to a phone for you to say, what do you want me to do with these? And the APP is pretty clever. You can say, Oh yes, I love that one, post that one to social media. Or you can say, I disagree with you on this one. Show me all of the images and let you rifle through. And by the way, learn for next time what I likened my images. You can tag folks and you can delete it. You can say, wow, don't share any of these things. Um, and so that's what the [inaudible] narrative camera does. It's part of a category of cameras called the life blogging camera. And for my money, what makes us really interesting is that they're both cameras, but the one on the right is a camera without a photographer.

Speaker 1:          05:16          Does anyone know what the thing on the left is?

Speaker 2:          05:19          Thanks

Speaker 1:          05:21          Dyson brand vacuum cleaner. And the thing in the right, I probably don't have to explain it's a Roomba. Right? And similarly to the get narrative camera, we have two objects that ostensibly do the same thing for their users, but the one in the left requires you to grab it with your hand. In fact, that's the only purpose that handles serves. Right. And to use it, you step on a switch to unlock it and you step on a second switch in order to turn it on and then you push the vacuum around the house. Meanwhile, this thing does the same thing, but there's no handle, there's no real step free to step on to unlock it. Um, there is something where you can turn it on manually. That's what that big button. Yes. But the important thing here is that it's a vacuum without a vacuum or pardon?

Speaker 1:          06:06          That would be clever. Or if I was working in another language than English, but right. There's no human whose job it is to vacuum there. It's not quite automatic because if I spill cocoa on my, on my floor, I can grab that thing, put it nearby and say clean here and it'll focus on that spot for a little bit. Room has recently been blasted this week in the news, um, for some of their privacy violations. But let's bypass that for now. I just want to talk about the functions and the thing on the left is what,

Speaker 1:          06:37          it's a car, right? And the thing on the right is that your car, at least the current model that I'm aware of, um, and it's a similar in that in that same pattern it's a car but without a driver, right? And when you look at all three of those things, they are emblematic of a larger pattern that I was seeing in my life. I had a, um, automatic cat feeder, so when I traveled, my cat wouldn't starve or I didn't have to ask a friend to come over. Um, and, and at the time that I began thinking of these ideas really deeply, I was working in a Robo investor. And so this was something that you would say how much money you had, how much money you would give each month, and what your financial goals were from that point forward. The investor would manage your portfolio. And the thing when thinking about these patterns, what they all sort of shared was that they all did things on their users' behalf, but they weren't quite automated.

Speaker 1:          07:28          But to get narrowed a camera, you can actually tap it in order to take a photo or take a video with the Roomba, you can pick it up and force it to vacuum somewhere. That wasn't in its current plan, I'm presuming I've never ridden in one, but that you can stop the Google car and say, I've got a p or let's take a food break or I want to go see what that is thing over there. So they're not quite automated. And when I tried to dig deep and to sort of the epistomological definition, um, it seemed to me that the major difference was that you grant these things agency to act on your behalf. So for my money, this pattern was rightly called agent tive technology. I didn't make up that word, but I rescued that word from obscurity. I think, uh, before I published it, the only place it was used, it was in linguistics, but that's what I'm calling it.

Speaker 1:          08:17          This weird space in between automation and assistance. And we'll talk a little bit more about that later. But that's the pattern I had identified. Now these are all first world problems, admittedly, but they're super easy to understand and that's why I go to these as examples first and quite commonly in the book. But I really wanted to push the idea and say, okay, are there some third world problems or other world problems that this technology can be applied to review them kind of quickly. And the upper left hand corner is ShotSpotter. If you're not familiar with this tech, what this service does is it will work with a precinct and the United States in order to sprinkle microphones all over a neighborhood. There's microphones, uh, aren't particularly great, but they do have fantastically accurate clocks and they're connected to the network and they're trained to listen for gunfire.

Speaker 1:          09:09          The men of Nehemiah hear gunfire. They report the exact moment they heard it back to a server and of course you can triangulate it and the cool thing is the accuracy of that is down to a meter and doing an interview with ShotSpotter for this book. Uh, they love to tell a tale of one pair of officers who were able to arrive in winter so quick. They were able to respond so quickly to the gunshots that uh, even though there was snow in the ground and they were worried they wouldn't be able to find the shell, they did find it by the smoking hole that it left in the snow. Pretty cool. Right in the upper right hand corner is Volvo. It can really with their self breaking trucks in the grill, they have a series of sensors that not only track motion but actually a extended that motion.

Speaker 1:          09:57          And if the vector of any object in motion intersects with the car and the driver is not breaking it, they will break it. It's going to save a ton of lives even before we have self driving trucks. That's a human augmentation in the lower left is a swarm robot called Prospero, uh, designed and prototyped in Australia. And with these little robots, you can actually set some seed in here, show it a map, tell it what kind of seeds they are and the swarm can go out and plant crops, um, accurately and quickly in a matter of moments. Some major issues with what do we do with the workers who used to do that sort of thing. But all of this technology has that question underneath it. The nice thing about Prospero is that it is not only faster but also safer. So dangerous terrain. The robots, you know, they can handle it fine and we don't mind if a robot breaks it's little league. And the lower right hand corner is a service called scare crow. This is a drone that is trained to hover high above endangered herds and watch for humans that are approaching. And if it doesn't know those humans, it will drop down in between the herd and humans. And the wasp like buzzing will scare the herd in the opposite direction.

Speaker 1:          11:09          Now I made that last one up. It's a lie, but the reason I made it up is I thought of a problem in the world that bugged me, animal rights. Um, and I tried to apply this kind of thinking, this pattern, this template to the problem and I was able to come up with something fairly quickly and fairly compelling. In fact, I was giving this talk in Delft in the Netherlands prelate fairly recently and one of the audience members raised his hand and say, Oh yeah, we're a Dutch. People are already building that. So it's a viable idea, at least for the Dutch to seem to think so. Um, but all of these are sort of to illustrate, even though they're a little more complicated, a little more nuanced that the pattern of agency, it doesn't just apply to first world problems even though those are easier to understand. So one assert that yes, I think it's as big as we can think. Put that part down. Pattern established. I'm going to talk to you about five reasons why you think this pattern is nifty. The first is that it's new, but you can hear in my voice and you should be able to see on the slide that there are air quotes around the word new. The reason why is because that image in the background, does anyone recognize what that is?

Speaker 2:          12:20          Yeah.

Speaker 1:          12:21          Yes. A particular machine within the cockpit autopilot. Anyone want to take a stab at when the first autopilot was demonstrated to Chicago world's fair, 1914 103 years ago. Now, of course it was electromechanical at the time. It took a lot of engineering effort, a lot of money in order to make it. Um, but that's a piece of agent tiff technology. You granted agency to fly the plane and it doesn't just do it automatically. We still have pilots on those planes, even with our modern autopilot's. But let's put that down between that and the thermostat that you'll see in chapter one of the book. Um, it's kind of new. There are lots of precedents in the world ahead of this, but the reason I say that it's new that public API Apis for neuro artificial intelligence have become available within the past five years. IBM's yours open Ai's right? The

Speaker 1:          13:18          public understanding and appreciation of these services are on the rise. So we have a marketplace that will sort of accept them. We don't have to train them on this sort of thing. We have a lot of centers that are an actuators that are available to us as designers and makers in order to make these things happen. I have 32 examples in the book. Here's a question mark there. I don't remember the exact number. Um, but all of them, except for the autopilot or within the past five years, so it is new. I believe that this is a way of thinking that's ahead of a curve. So new, pretty cool. Second reason is that it is different. If you've ever studied interaction design like I did, we talk about things like affordances and mapping and all those things and the canonical example for that as a hammer. How does a human know how to use that hammer? Well, they look at it and they say, oh well this looks like it fits a hand and as long as I'm going to grab it at the end, that looks hard. So it should be able to hit something and drive a nail. Those affordances, those mappings, those systems states, uh, are all belonged to something like a hammer. But that doesn't apply when you're talking about an agent of model. Right? You don't need an affordance for the Roomba because I don't have to grab the handle.

Speaker 2:          14:38          Okay.

Speaker 1:          14:38          I don't need mapping when it comes to the, I'm not going to say Google car cause course you need mapping for a car. I don't need mapping when it comes to my cat feeder. Right. I just need good controls to set the thing up and some kind of feedback mechanisms to let me know that yes, my cat is being fed. So for my money, if the hammer is not the right model for an agent of technology, what is, and I think you can go turn to a butler or a valid as a better model. You would tell this person what your goals were and what your preferences were and from that point forward it would manage as best as it could to those goals and preferences. It would come to you and there was a problem, hey, the larder is empty or in case of a of a valid, I don't have any clean clothes.

Speaker 1:          15:23          Something's gone wrong here and you would help it and you can even tweak it saying, wow, I appreciate your bringing me that tie but I never want to see it again. It's, it's hideous and ugly. To illustrate this notion that the, the, the model must be different. Um, I built a model of interaction design. If you studied this should be pretty familiar, right? Humans sees the state of the system. They think that's not quite the right state. What can I do next? Judging the affordances of the system and they do something to the system, right? They press a button way of a hand speak of something and then if that's the red human part, the blue is the familiar computer part. Computer takes that input goes through some sort of algorithm and result in an output. This we know is a oversimplified model of human cognition, but it's a very useful over simplified model of human cognition and interaction design.

Speaker 1:          16:15          But when we compare that to a model for agents give technology, it gets different. So right. If the, if the computer of course still down here, but if there's a computer running the system with a human, not gone, but just on the outside of the system, peaking in occasionally making requests, tweaking. Then we run into a whole lot of different use cases and that's what this map is. It's in the book that you have a, it's in piecemeal throughout section too, but it's in its whole version in the very back on the appendix, but you can see here the unique use cases for this involve a lot of setup. How do you give that agent of technology your goals and your preferences towards getting to those goals? Oh, I want my son to go to college when he is of age. Is it going to be a state school or is it going to be really expensive school?

Speaker 1:          17:06          Right? This preferences, do I want investments to go to vice funds are specifically not to go to vice funds. All the sorts of things that one might think about in the setup of like a Robo investor, use cases for seeing or of course monitoring. How do I build trust that this thing is going to do what I needed to do and it's in charge of something as important as my money? How do I know that it's working? How do I know when it's running out of things like resources, right? So there are some unique use cases for seeing and then there's doing right? Pausing and restarting a, one of the great things from the research that we did for the Robo investors, we wound up talking to a guy and we said, okay, if you had $1 million and you wanted to give it to the Robo investor, how would you that million up?

Speaker 1:          17:50          And he said, well, I give the Robo investor probably 900,000 but I keep 100,000 for myself. And we said, why? He said, I'm going to see if I can beat it. And we were like, this is an algorithm that looks at the entirety of the market. He was like, yeah, but I'm sure I have some instinct that it does it right. So that need to play alongside was sort of illustrated. Um, because we're talking about narrow artificial intelligence and we'll get to that in a little bit. Um, it's going to get some things wrong. So the user needs tools in order to tune it's black lists, it's white lists, it's a play.

Speaker 1:          18:26          And then of course for problems, there's a hand off and take back problem or the A, and I runs into something that it can't cope with. Um, and we need elegant ways to pass back and control back and forth. And lastly, since almost all agents are persistent, trained to watch some data stream for a trigger and then execute their behaviors, um, we get into a problem with disengagement. How do we know when the agent is no longer necessary? In the case of an invest a Robo investor, it's significant. If I die, that money just doesn't stay with the Robo investor. It needs to go to somebody that I need. So that disengagement becomes really important. Not so much with the Roomba but certainly in other circumstances. So this model, these unique use cases are sort of underscoring what I say when I say, Hey, this is new and unique. Okay. Number three, we have been studying usercenteredness for a long time and part of what we are always doing as we apply designed towards the problems at hand is we try and maximize the user value and minimize the amount of work that they have to do to get there. And I can't imagine a better equation then nearly zero input for maximum user value.

Speaker 2:          19:44          Right?

Speaker 1:          19:45          For familiar with pine and Gilmore in 1999 they published a book called the experience economy and they categorize the types of products that can be pushed to market in one of four ways. The first is of a commodity, barely differentiated, cheapest dirt traded on the market. And if you wanted a cup of coffee and that was their example. So I'm using it to, you would go and go to a wholesaler, bring a bag with you and a scoop and scoop that stuff and, and you'd pay less than a cent for that bag of beans. Commodities are really cheap because they require the user to do a ton of work and next level up is called a product where the company says, you know what? Don't worry about going to the wholesaler. We're going to grind these beans for you. Put them in a really cool bag, will even design that bag to look really cool, get it on grocery stores near you, and for that you will pay a premium compared to the commodity, but hey, it's closer to you grocery stores or Bodegas and it's pretty, it'll look good in your shelf. It's already ground. You don't have to worry about grinding it.

Speaker 1:          20:48          Third Category, that head was this service where a company would say, you know, don't even worry about going to the grocery store. Come on into this space that we've set aside. Let's call it a restaurant. And if you want a cup of coffee, you just tell us. We will go into the back, grab our product, made up of commodity, make you a cup of coffee, bringing out to you, and we will even clean the dish. And for that you will pay a premium compared to the product. The point of their book was that there was a fourth layer that they had identified and they had been brought to there that it had been brought to their attention by Starbucks. So logo time,

Speaker 2:          21:23          okay,

Speaker 1:          21:24          Starbucks was a service, but how were they getting away with charging as much as they were for a cup of coffee? And they said it was because of the experience, right? You're just not going into a diner and there's a ton of stuff. You're going into a coffee experience where they have lots of wood, gorgeous wood paneling and lights and a cool music playing and they abused the Italian language in order to sell you this stuff. Um, and for that you consumers, it's pretty much been shown in the marketplace that are willing to pay a premium for that deep experience. The reason I spent a little bit of time with this model is to show that I think there's another layer of value that agency unlocks. All of these require attention to extract value. It's either me with a scoop me in a grocery store, me and a diner or me at Starbucks, but in the case of the room, but I get value out of that object when I'm at work. My cat is fed, what am I am traveling? There is value there. That doesn't depend on the 16 hours of my attention, which is limited and a very competitive competitive space, right? If you think about that, the opportunity for value that you can provide your users, your customers, however you want to think about them is comparatively infinite. So I believe there's a post attention value that we can begin to capitalize on when we equip our products for this mode.

Speaker 2:          22:54          Number four,

Speaker 1:          22:56          PW singer wrote a scary book called uh, something of war. I have lost it, but ended. He makes this argument that there are certain technologies that once a civilization adopts that can't go back. He's thinking specifically of drones in warfare. Once we send machines to do the fighting against other machines. Why on earth would you send your flesh and blood dark, good conversation worth having. But in terms of agent of technology, I believe it's threshold. Once you have the room, but in your life, how much are you going to want to go back to the Dyson?

Speaker 1:          23:32          Once you have a car that will drive you and your kid to their school and you'd get all this quality time with them, how much are you going to want to get behind the wheel and say, get in the back and I really can't talk to you. We get there, right? The amount of value that you get from an agent of technology is so great. I believe that going back becomes drudgery and for that it's a threshold technology and for that reason it's a massive competitive advantage to those companies that adopted first. Last bit. It's Ai, which I don't think it's the AI to fear, but it's certainly interesting to me. So let me take just a moment and situate it. I include this as part of the talk because I don't know how well you guys are versed in AI, but I may be overstepping you baby, uh, much more knowledgeable about this than I am.

Speaker 1:          24:22          Bear with me if you are, but in the literature of Ai, AI as a field is broken up into three primary categories depending on the capabilities of the AI itself. The first one that, the one that most people think of when they hear AI is general AI, so called because the AI can generalize knowledge just like you or I can. Uh, I run a blog about science fiction interfaces, super nerdy. Um, and so I'm going to make a couple of Scifi references and this is my first who does not know the movie war games. Okay, spoiler alert. I'm going to tell you a little bit about war games, but in this, there is an AI that has been trained to play games and a lot of them are harmless games, but one of them is global thermonuclear war. What the Ai doesn't know is that it's actually tied into the nuclear arsenal of the United States.

Speaker 1:          25:14          Oh my God. And it's got this countdown. It's going to start playing this game and ruined all our lives with the protagonist ends up getting an idea and you have the end of the film, Hey, let's play tic TAC toe and it begins to play tic tac toe with this AI called whopper. Terrible name. Or Joshua a is the nickname and over playing tic tac toe over and over again, the Ai goes suddenly realizes, oh hey, this is a game that can't be one. And then it generalizes that knowledge and thinks, Huh? And it begins to run through scenarios of global thermonuclear war and says, oh, that's a game that cannot be won. Why on Earth would we play it? The countdown timer stops. They have saved, guy gets the girl movie over, but it's a pretty good example of General Ai, right? That AI is able to generalize doing what you and I have done since toddler hood when we took the physical things in the abstractions in the world and then began to build them to the adult knowledges that we move through the world with today.

Speaker 1:          26:10          Once we have a general AI, one of the first things that we're going to do, whoever gets that, it's going to ask it to say, can you make a copy of yourself that is a better predictor and has better outcomes and make sure that copy is deeply interested in making a copy of itself. And that'll make a copy and that'll make a copy and that will make a copy and eventually what will come out the other end. And it depends on who you ask, how long that takes. It could take a couple of hours, it could take a couple of months, but we'll be something so smart that the running metaphor is, it's intelligence will be to us as ours is to a birds. We will not have the language to think in the questions it is interested in. If you want to be scared of an AI, be scared of that AI because if that is not pre loaded with a care for human wellness, we're in trouble. And in fact, the mathematician and science fixture fiction author Verner Venga probably murdering the pronunciation coined to this moment as we pass from general AI to super AI is the singularity because we don't know what life is like with a functioning God able to answer our questions.

Speaker 2:          27:19          Okay.

Speaker 1:          27:20          I raise these because agent of tech is a mode of interaction that does not involve these two. It might set it up, but let's talk about that at the end of the talk. The one that we have in the world now, which is not as terrifying as these guys, isn't narrow artificial intelligence, so called because it's very good at one or two things and can't generalize that knowledge. I can't ask the Roomba to help me plan a Thanksgiving dinner. Not yet shirts and the roadmap, but narrow AI is the one, I'm a very practical person and this is the one I'm most interested in. Lots of exciting stuff to talk about here. What do we do with all the jobs that are going to be lost? Um, what, what did we do? Who's got the last jobs? I think it's designers and judges. Um, but let's talk about this because this is what we have in the world now for my money.

Speaker 1:          28:05          We talk a lot about AI in terms of what it can do, but I'm most interested in what the relationship is of the user to the work that the AI is doing. And I found three categories. I may be wrong if so, let's chat about it. But the first one is automatic stuff that should just happen, right? Uh, a pacemaker is a good example. I never want a pacemaker to ask me in the morning, do you want me to make your pace? Because the answer is always yes. Same thing with the grocery store. Automatic door. I never want that to ask, right? Just open, just do it. This sort of thing does fit narrow artificial intelligence, but only for well constrained unpersonalized things.

Speaker 1:          28:55          Assistance are things that you would use to help you do a task, right? Like an angel on your shoulder, whispering in your ear. Scifi referenced number two, like Jarvis for iron man or now Friday, right? It's an assistant to him though I make the case in the blog that Jarvis is the iron man. Um, and when I thought about this, I was like, well, what do we parse as it, what do we give to the assistance of the world and never want to hand off to a non human? And I found four and a half categories and they're listed up there. The first one is our jobs. If you and I have an economic agreement that I will do work for you, and instead I handed that to an AI and spend my days on the beach, we'd probably an ethical problem.

Speaker 1:          29:43          There's an argument to be made of, Oh hey, why don't you sell me that AI so that I can use it at my company? But for the most part, you're not doing your job. You're sneaking away from your job. So jobs are something that we want humans to be outfitted with assistance, not agents. The second is human connection. I suspect that in about 10 years I'm going to greatly prefer an AI for my doctor to be able to recognize and diagnose the problems that I have with my health. Right. Already we know that Watson can read every medical publication that is published around the world, pretty much in real time. It's diagnosis. Despite the MD Anderson troubles of late, I'm going to become much more confident in than a humans. That is not true for a nurse. Part of the purpose of the nurses human connection.

Speaker 1:          30:38          I don't want an AI waking me up and saying, do you need more painkiller? That's not going to help me feel cared for. Helped me feel loved not left. It's not going to get me on the road to wellness. So if human connection is part of the purpose of the thing that I don't think we ever want to hand that off to an agent and we do want it to be an assistant the last year related, right. If the point is physiology, then I can't send a robot to do the work. Right? I can't send a robot to go to the gym for me. I'd be, I'd be jacked if I could. Skills are similar. If I'm trying to learn French, I can't certain send an agent to French class in order to study the language for me, I have to be there so that my brain acquires those skills. Lastly, there's a half category of art. Um, I say it's a half category because there are plenty of examples where a computer generated stuff we find delightful until Arius, this is inspire robot popular of late, but it's a neural net that's trying to create inspirational images and they're just hilarious.

Speaker 1:          31:44          But the reason I say it's still a half category is because, uh, when people find out that a poem they love has been written by an AI, they feel betrayed. Not True with visual imagery. So it's a half category and I'm sure they're going to be artists who are going to rock out using AI in their work. But that leaves agent of tech, right? That thing where we partner with a piece of technology in order to do the things for us that we want it to do. And that means everything else. If it's not one of those four and a half categories,

Speaker 2:          32:17          that's a lot.

Speaker 1:          32:20          That includes like things that we're not good at, right? The whole reason we had autopilot is that humans have only about 20 minutes worth of vigilance. And I know you're already past that in the audience, so thank you. Um, but where we can only pay attention to a signal for 20 minutes before our attention begins to drift. Um, and so we need partners to help us with the tasks that we must do that are longer, that are beyond common human capabilities. It includes technologies that were unwilling to do. We've known about the Pacific gyres for the better part of two decades now. We haven't been cleaning them and yes, we could probably put some humans and some boats to go out and clean that mess up. Um, but once we have robots doing, I think we're going to be hard pressed to find humans that want to take those robots place.

Speaker 1:          33:06          By the way, the Pacific guard does not look like that. Just soupy, plastic mess. It's not actual trash floating in the ocean. And it also includes stuff that we just cannot do alone. Right? We know we only have about 4 billion years in this planet, which doesn't seem like a lot, but boy, howdy getting a, an entire civilization. Um, to some of the rock is a major undertaking and we can send humans out into the void, but we're fragile. Error prone sending robots is a much saner idea, but the farther away from us they get, the longer that communication time is. And we have to have some kind of smart on those robots as they explore the galaxy to handle things that happen in between the moment of communication. Fortunately, NASA is already on this. They have something called the NASA agent architecture for right. But we can't do this task alone that we have to do and we're going to have to partner very in very smart ways with our technology in order to make it happen. Which brings us back to Ada. You'll remember that eight is objection or the Lovelace objection is touring call. That was Ken computers take initiative and I think I've certainly shown that. Oh yes. The patterns that he showed at the beginning where they can, um, even the examples that I showed show that they do already, a few people have already started to move their technology in this direction. I'm writing for both business reasons and for ethical reasons. I think that they should in certain cases.

Speaker 2:          34:35          Yeah.

Speaker 1:          34:36          So part of the mission I've gotten writing this book and going and making talks like this is to convince people that, hey, let's move in this direction. Let's build our products such they're equipped this way. And so I'm going to leave you with three questions to ask for the products that you guys build. And the first is basic in that it works from the micro interaction level all the way up to the strategic directions of products and to say, are we asking users to do something that we could do for them if they so wanted. Once you answer that question and you begin to create the agency of your products or agent of modes of your products, then you have to say, okay, well it just not purely agent IV. The rooms in the world I think are going to be the exception. Something that just fits nicely into one of those categories. How can our product support automation when the confidence is super high agent tive when it's a task that the user doesn't want to do and assist of modes because smart products products

Speaker 2:          35:38          we'll have to work in all three. And lastly, how does our product help the user, right? Whether the product, the Ai is taking initiative or the human is taking initiative, how do we flow smoothly between those modes? I think once we answer those questions, we will be, uh, many further steps on the way to getting our technology to take advantage of this conceptual framework of agent TIF technologies. Um, so you can follow me at agent tiff tech or Chris Nossel but that's it. Let's have a chat about it.

Speaker 3:          36:16          Um, so I had a question about where the initiative of decision making falls into this pyramid. So you mentioned a lot of tasks that, uh, agentic IV, uh, technology will do what we're unwilling to do. But it sounded like that's where the humans have to kind of give the initiative to these other devices to do. How do we kind of, it's like how do we seamlessly move into a place where humans, where we can suggest things that we think should be done and humans will, um, trust that suggestion. Right? I think that's a big problem that we have now as we move into a world where things are getting more and more intelligent. Yup. And people can kind of get scared of those suggestions, which might sound simple, but they don't know all of the algorithms that are going on behind it. And so that it's moving in that direction. Do you have any, yeah.

Speaker 1:          37:12          And in fact, earlier this year, the EU passed a resolution, um, about AI that included a provision called the right to explanation, which says that if you are subject to an [inaudible] decision, you as a citizen have the right to understand how that decision was made, which is going to be problematic in the world of neural nets. But, um, in the book I describe a pattern called a hood to look under and it's a trust building pattern. And the notion is that for any decision you should be, and it's easier with narrow AI, you should give that user an opportunity to open up the hood and see how that decision was made, disagree with that decision, and either provide categorical imperatives for future behavior, white lists, black lists, inclusions, or even particular tweaks. So I think you should be able to go down all the way, um, in common usage.

Speaker 1:          38:00          I suspect that's going to be similar to the get narrative camera. Wait a minute. Why did you show me that image? Oh yeah, you're right. That's the only image from the party that was clear. And the chode the people. Well and the balance was right. Okay, I agree with you. And over time, having that hood to look under will help me build trust. And when I don't trust it and it's got it wrong to be, to influence it such that it behaves the way I want it to. So check out that pattern in the book. And I think it may answer your question. Um, there was a second question implied there, which is about recommendations and I think that any system should be able to recommend to users new behaviors. Um, occasionally there's a, there's an ugly tension between agency that goes off and does its work out of your attention and the brand importance of not becoming a commodity.

Speaker 1:          38:51          Right? Brands want to be first and foremost. Um, so there's a weird tension there and I believe that the corporate pressure we will have will be to over communicate and our job as designers is to match what the humans want. Um, but I don't have any problem with an agent coming and making an occasional recommendation to me as a, as a user. And I suspect users at large. Um, so I don't think the decision itself is a problem. Understanding how it was made and then being able to tweak that as the important part of the pattern.

Speaker 4:          39:19          Why designers and judges, why are they the last one set of jobs?

Speaker 1:          39:23          Uh, yes, I suspect it's because that we do the same thing, right? We judge what's good and we know how to, we are good at understanding humans and then designing systems, uh, depending on your definition of design that optimized for a human set of effects, um, and Ai's will not know that inherently from the very beginning. Um, and they'll come to us to ask those questions. Um, similarly I think that a AI's are an alien and tele or the generally I will be an alien intelligence and they'll need to turn to judges to know in order to understand what to do or what humans would ordinarily do in the edge cases. I'm in law as of course, all about the edge cases. Um, so I think those are the reasons. The last two, I didn't say this over the course of the talk, but I'm actually pretty hopeful about the role that agent of technologies would have in setting up a general AI to be benign.

Speaker 1:          40:12          I know it's tricky to say, but, um, if the, if the output of a ton of agent of technologies in the world is a set of instructions for how you want things to behave in order to serve you well, what we'll have at the end of that, at the advent of general AI is 4 million laws of robotics, right? Where an AI, we can read all of these instructions at lunch can begin to infer, oh, this is how humans generally want to be treated. And that's a pretty important big data piece of information to handle general AI. So I'm hopeful in that regard. Yes. Uh,

Speaker 4:          40:50          thinking through this, but what are some examples of agent of products that you've found have been able to close the gap? Maybe between the manual traditional way of doing something and then handing off that service and uh, building off that maybe some Asian two products were completely AI generated products. Like the poems that you've found people just did not like she didn't having generated. How, how have you seen ways that that gap can be closed to where they do appreciate those poems in the future, for instance?

Speaker 1:          41:27          Um, so let's tackle the two. Uh, the first question was what are some examples of products that have genuinely managed to work fine and go from human distrust to human trust? Spam filters? Spam filters are really great example, partially because when they were first there, people don't necessarily trust them. Um, partially because well done spam filters and include Jimmy on this, I'm still give you the opportunity to go in and review, open the hood. Do I agree with you on these things? They give controls for blacklists. I never want to hear from this person again. And white lists. I always wanted to hear from this person, um, and incorporates deep, uh, uh, narrow artificial intelligences in order to end and a group feedback. If you know 20 people all mark, this is undesirable. You can pretty much guarantee you that the other users that are similar to them, we'll find the same thing.

Speaker 1:          42:17          So, uh, email filters are a great, great example. Uh, and partially because like, I dunno about you, but I haven't gone into my rejected folder in a really long time because I trust them. Yeah. Does anyone still go in? Right. So that's a great example as a model. The second part of your question was, oh, a things that have failed. I don't have a great, uh, answer for that right off the top of my head. The get narrative is a risky example, partially because that company has been struggling, but I don't think it's because of the agency of the product. I think it's actually because of our concerns about privacy, um, that the domains in which I can wear a camera and not worry about the privacy of the people in front of me. It's actually pretty limited to cross my day. It's not my work.

Speaker 1:          43:03          Certainly not my inside my family unless I'm just sharing those to a small group. Um, like maybe a party, but still when I wore one and I did for about four months, people would be like, oh, what's that? Maybe hiking, maybe public things like walking around the city. Um, but that's a small percentage of certainly my time in the world. So I think that the problems that they had weren't about the technology, but about bringing that technology to that particular domain. I'll think about it a little more. Um, I think Roomba is making a big mistake with the announcement that they gave this week that they have slowly been building up a model of people's homes. And even though they said we'll let users opt in to sharing that home platform and the details that Roomba has found with a co marketers, um, it's still super creepy knowing that it's creating a model of your home that can be hacked by anyone. So I don't know how that's going to fair. I suspect that the announcement this week was a bit of a flag to see how people responded and I, it's being overshadowed, overshadowed by modern politics, but I hope that they get, um, some negative feedback because people are going to be creeped out. Um, I can think of some other I examples, um, and tweet them out there if you'd like.

Speaker 1:          44:19          I will volunteer. Three of the things, uh, for the things, the first is one of the questions that I get asked a lot about is what do we do with the humans in the world of, and I, I don't have a pat answer for that, but I am a big believer in universal basic income. Um, I think it's the, we have always had technologies that slowly obviate jobs, but the speed at which we are going to replace entire fields, it's going to be massive and we have to come at that from a cultural answer, not a, uh, not a, Oh, it'll work itself out. Free market kind of answer. The other three are sort of the next steps over the course of this book, and I'll give this to you guys because you're pretty advanced audience. I talk about the world as if there were one user and one agent.

Speaker 1:          45:06          That's certainly not true. We have to get good at that foundation, but over time it's going to be user one user to many agents and even multiple users to multiple agents. I don't talk about that here, but it's something that we're going to have to solve as a community of practice working with this. Uh, the second thing is that I didn't talk, I don't talk about the third category, the assistive tech. I believe that the long history we have of interaction design will fit us well to assist of technology, but I'm also quite concerned that that becomes a human crutch for cognition as opposed to equipment that makes us better. I don't also talk about the flow between agented and a system and automatic and this book, but it is something that if you begin to incorporate agent of aspects of your product, and it's not purely agent of like the Roomba that you have to work through. And I don't give any advice to that. I'm working like with Vic and shines ladder. Right? Let's get this done first and then we can build up those other skills. Thank you guys.