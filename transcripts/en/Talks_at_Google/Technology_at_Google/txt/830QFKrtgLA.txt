Speaker 1:          00:10          Thanks everybody. Thanks for being here.

Speaker 2:          00:13          Uh, today I am talking about the ethical oos ethical operating system, uh, or if you want to call it a guide to anticipating the future impact of today's technology. We'll talk a lot more about what that means. Uh, but for now, let's just say that this talk is about how not to regret the things that you will build. Um, and the ways, the tools, the foresight, the frameworks that we can bring to the process of developing new technologies to ensure that we have in mind from the outset, the user, the societies, the communities that will be impacted by the products that we actually build and ultimately ship. So as already mentioned, I am yours. If, um, there's my Twitter handle, if you want to tag me. Um, I have been doing this work and thinking pretty deeply about how we build more innovative, yet responsible products for a while at Omidyar network.

Speaker 2:          01:15          Uh, and let me tell you a little bit about what Omidyar network is and what the tech and society solutions lab is. To give a little context. So this is my boss's boss's boss. Uh, Pierre Omidyar, who was the founder of Ebay and from the earliest days at Ebay, Pierre had a vision for tech as a great distributor of opportunity in the world and a great tool for empowering people to, uh, really lift themselves up, right? If you think about Ebay as a marketplace where everyone could be a buyer and a seller, it was at the core a project in that idea of empowerment. And so when Ebay IPO, Pierre took much of the wealthy accumulated and rolled it into Omidyar network as well as our sister institutions, uh, to have social impact through four and nonprofit investments. And so for the last 14 or so years, Omidyar network has been making investments globally in things like financial inclusion, citizen engagement, education, property rights, digital identity, a whole range of things with this central thesis of empowering people around the world.

Speaker 2:          02:24          Uh, in about 2016 or so, we, like everyone else kind of ran into a brick wall of an awakening. That sort of happened, I think for a lot of us around Brexit and for many of us around the 2016 elections. And some of what happened through the 2016 elections in terms of manipulation of our democratic process, et cetera. When we all kind of collectively woke up and said, oh, wait a second, that tech that we thought was just going to solve all the world's problems actually has some pretty deep, uh, problems and challenges that we actually need to address as well. And so we had a mid network beyond thinking about, okay, we've had this techno optimism for so long. How do we actually ensure that the tech that we are developing is actually good for people? So not just thinking about tech in its application for good, but actually tech that at its core is good.

Speaker 2:          03:14          And so we stood up the tech and society solutions lab, which Eh, aims at our core to help technologists to mitigate, to prevent, to correct the downsides of technology, to ensure that it can live up to its potential as that positive force for good in the world. And so we're doing that by cocreating and supporting the creation of solutions to advance these goals. There's been a lot of talk about the problems. We'll do a little bit of that, uh, in a few minutes. But at the core we're focused on what are the solutions to those challenges. And Ethical Oos is one of the tools that we've built in order to address some of those challenges. So what brings us here today, um, there's been a lot of talk about the challenges that tech has created for people and these are just a few that are appearing, uh, challenges around transparency and pass pass city, uh, about how decisions are made around tech and what decisions are being made beyond our control and beyond our understanding how algorithms are developed and the biases that they might be perpetuating, how privacy and security are built or not in the products, how text mono culture might be perpetuating many of these things, uh, ranging all the way down to environmental impacts of the tech we're building and rising social inequality and economic inequality.

Speaker 2:          04:41          It's a whole range of issues that bring us to the table and thinking about how we build more responsible products. So here's an analogy I like to use, and this is with all credit to the folks at Santa Clara University in their tech ethics practice. But foresight issues, if you will, ethical issues are like birds first. They're everywhere. Okay? Some are ordinary, some rares, some are big, some are small, some are local, some are exotic, some are ubiquitous, but there are birds everywhere. But it's actually really easy to go through life surrounded by them and not see them at all. Right? They just sort of are in the background and if you're not paying attention, you won't even notice the birds. Getting good at seeing them is a skill only built by practice. And practice makes us finding them easier and actually more rewarding. So when your practice and seeing the birds, it becomes an a daily ritual.

Speaker 2:          05:44          It becomes part of what you do as you navigate through the world. It becomes that much more rewarding. Skilled watchers learn how to see them and where and when they're most likely to turn up. Right? So when you are a birdwatcher, it becomes, oh, I know that if I had to this forest, I'm likely to see these types of birds. Right? And actually it, certain types are found more in some landscapes than others. Some are not found anywhere at all, right? You can sort of predict with some degree of certainty about where those birds may appear, it's actually easier to spot them with help. So if you have a birdwatching buddy next to you, you're doubling your capacity to actually spot the birds on the landscape. And finally, you may need special lenses actually to see those birds. Your own eyes may limit your ability to actually spot the bird on the horizon.

Speaker 2:          06:43          And so a special lens like ethical Oos or any other framework helps you to spot them. So, uh, if you just replace the word birds with ethical issues or foresight issues, I think you understand how this analogy actually plays out. And so we've built the ethical alas to actually aid and just doing this very thing. And we built it alongside the Institute for the Future, uh, and nonprofit group based in Palo Alto who think deeply about these issues of foresight, how we think about consequences, future technologies, et cetera. And together we decided that if you can't predict the future, you're in the wrong business. Just kidding. But at the core, one of the things that we are interested in doing is thinking about those consequences of technology. And when we think about the consequences of human systems, uh, that are sped up, amplified or disrupted by those technologies. So we know actually that no one can predict what tomorrow will bring.

Speaker 2:          07:43          Uh, and so until we get a crystal ball app, the best we can hope to do is to build the muscle of anticipating longterm social impacts, which is essentially what ethical [inaudible] has meant to do. And so it's actually not necessary that we predict the future. It's just that we get better at practicing the skill of birdwatching. Right? Which I will now introduce you to. So I think a low ass is designed to help makers of tech, product managers, engineers and others get in front of problems before they happen. So it's actually been designed to facilitate better product development, faster deployment, and more impactful innovation. So we have this thing as technologists, right? Where we imagine that the products we build, we'll solve all the world's problems and actually it will all change the world for the better. And that to a large extent is true, right?

Speaker 2:          08:38          Um, but it is also useful at times to think of the glass as half empty. If we're always seeing the glass as half full or entirely full, we will not be spotting the risks before they happen. So here are the three questions really that this is designed to ask the, there are many more embedded. Um, if the technology you're building right now will someday be used in unexpected ways, what can you do to prepare for it? Uh, what new categories of risks should you be paying attention to? And lastly, what choices can you make that would actively safeguard users, communities, society, and your company? In this case, Google. But any company from future risks. So we built three different tools in the ethical s I'll introduce you to each of those tools as a way of exercising this foresight muscle. Um, and the first is a set of scenarios to consider that just sort of start pumping the juices.

Speaker 2:          09:40          Think of it as a warmup for your run, right? This is just getting the blood flowing, thinking about the potential risks of technologies with some specific city, excuse me, scenarios. So you know, some questions to ask about each of these scenarios is, you know, what's your greatest worry when you imagine this future, how might different users be affected differently by this future? What actions would you take if you could foresee those impacts down the road and what could we together be doing differently to prepare for that risky future? So this is a little black mirror tree. Apologies for that. In fact, some of the scenarios that we built in the ethical s have now been played out on episodes of black mirror as well as actually in the world. We've seen some of them. So we built a scenario around smart toilets, which we actually couldn't have imagined would be demoed at ces this past January.

Speaker 2:          10:41          Um, so it's the muscle of predicting some of these things is actually really useful to exercise. And here are a couple of the scenarios we belt. So are you ready for a future where conversation bots have been trained to imitate specific people? So not just people in general and uh, you know, any, a generic user or a customer service representative, but your grandmother or a celebrity using datasets collected from public social media posts. And those bots are deployed across networks, email and text messages, and super targeted super personalized propaganda campaigns. And they are, as a result, highly effective in changing opinions and driving action. Right? As these messages appear to be from your family and friends, what are you worried about? What can we do to prevent this future? How would people be disproportionately impacted by a future in which this happens? You know, in response to growing concerns over tech addiction and anticipating governant government regulation as a possible future.

Speaker 2:          11:46          Several popular social media and game companies decide to voluntarily enforced time limits. Sounds like a good idea, right? Um, so adults might be limited to two hours a day. Kids might be limited to one hour a day. And actually whether a platform is limited or unlimited becomes a major selling point for them. You can imagine that world in which an unlimited platform is much hotter as a consumer good and people, but some people at the same time prefer having hard limits that PR that prevent them from becoming addicted. So again, who is affected? There was actually just an article in the New York Times, I think last week about how in some measure personal human interaction is becoming a luxury good a as those with lesser, um, opportunities are more engaged with screens, right? So you can think about the disparate impacts of this kind of a future fortune 500 re human resource departments subscribed to a smart employer service that evaluates a person's suitability for your culture and the stress associated with that culture.

Speaker 2:          12:56          Using social media posts. We already kind of do this, uh, people trolling, Facebook, Instagram, etc. To understand who the potential employee is. But you can imagine being this being automated through artificial intelligence. So algorithms can actually identify individuals suffering from mental illness or depression, right? You could, the service could actually develop symptoms of, you could predict the development of mental illness in the future based on trends in the individual's postings. Again, you can imagine the impacts that that would have on workforce on the future of work, on hiring, on, and again, disparate impacts on different groups of people. A major social network company in the future could purchase a top US bank and become a first of its kind social credit provider. And it could base mortgage loans, loan approvals, credit access on deep data collected from its social platform and could take into consideration credit histories of your friends and family as well as the locations you've visited.

Speaker 2:          14:02          So imagine if you've visited a bar or a legal marijuana dispensary figuring into your social credit and the credit that your bank might provide. And it could actually also do semantic analysis of your messages and photos to actually decide whether you are generally happy, angry, anxious, or depressed. Figure that into whether you get alone as well. 25% of online orders are delivered by drone. Sounds great, comes right to your door. But actually these drones are fitted with cameras and sensors to collect data as they fly over neighborhoods, which actually provides, right the drone company with additional revenue for the shippers and merchants in some individuals can actually opt out for free, unlimited drone delivery, uh, by paying. And they consent to the collection of their data by doing that for the drones that fly over. But actually neighborhoods where the drone delivery is legally permitted or subject to the same data collection activities, even though not all of their residents or households have explicitly consented.

Speaker 2:          15:06          And again, who is impacted? What could we do to prevent this future? If you think about it? So one of the tools that you can use to think about these impacts of a tech product you're building before it happens, it's called a futures wheel. And if you take one thing from this talk, I hope you take a lot of things from the stock. But if you take one thing from this talk, the future's wheel is an incredibly powerful tool for thinking these things through. So put at the center, your product, your change, the idea you have. Let's go to the last example. Drone delivery of orders, right? A of online orders and spin out from that, the consequences that might happen first order consequences and then from each of those first order consequences. Think, okay, and then what happens and then what happens and then what happens from each of these nodes on the futures.

Speaker 2:          15:58          We all, so I'll give you just two quick examples that I easily pulled actually from the web by just googling futures. We all, um, so the first is that there are increased choices, right? For renewable energy, which sounds like an incredibly positive development in the world, right? Um, and we would all likely think, well, increased choices for renewable energy is positive as a positive development. So put that in the center of your futures. We all, and then spin out the first order consequences of that. Right? So you can imagine new companies emerging in the energy marketplace, oil prices, falling, monopolies being dismantled and technologies being developed, you know, based on geography and politics. Again, those all sound really positive as first order consequences. But when you spend some of those out, you actually get to some negative consequences. Now on the second order, right? So if oil prices fall, jobs can disappear in the oil industry and some poor countries can actually become oil dependent, right?

Speaker 2:          17:05          Because oil is now cheaper. So poor countries can access more oil rather than renewables and they become oil dependent as a result. Right? Um, and actually you can imagine also on loss of monopolies that actually war and poverty could result for countries who are not competitive in the marketplace, right? So it takes this tool of figuring out first order, second order, third order consequences in order sometimes to see the negative impacts. So again, we imagine renewables good first order good. When we get to second and third, or oftentimes on the horizon, you can see potential risks out ahead. Here's another one. I'm sorry if this is illegible again, I just pulled it from the web. Uh, at the center of this futures wheel is open source technology and generally, you know, popular sentiment is that open source technology is a net good, it's positive development, et cetera. I'm not going to go through this entire futures wheel, but I just wanted to highlight a few things further out on the degrees of consequences and impacts here. So if you get out far enough, you have you know, untraceable terrorism, you have companies failing, you have, you know, a loss of jobs, you have lack of attribution of product development, you have some potential negative impacts from open source technology that might not have been expected right at the outset. So again, just a useful tool and starting to think through future impacts.

Speaker 2:          18:39          Second tool, risk scanning. So we created eight risk zones to look at when thinking about your products. And I'm going to go through each of these in some detail, but just to highlight them here. Risks Zone one, truth, disinformation and propaganda to addiction and the dopamine economy. Three, economic and asset inequalities for machine ethics. And algorithmic bias. Five, the surveillance state. Sixth, data control and monetization. Seven, implicit trust in user understanding an eight hateful and criminal actors. So by dividing into these eight risk zones, we could sort of identify spaces where hard to anticipate and unwelcome consequences are most likely to emerge, say over the next 10 years. Now I want to stipulate, we did not include every area where things might go wrong, right? So you'll notice here environmental impacts are not included. Again, this was meant to be the intersection of both on hard to anticipate and unwelcome.

Speaker 2:          19:43          We actually know that environmental impacts are likely to come. So that's not hard to anticipate. These risks. Villains are really just meant to represent those hard anticipate areas. So let's talk about risks on one truth, dif information and propaganda. And in this future, shared facts are under attack. And the primary mode of attacking, uh, today, these kinds of shared facts has been fake video, right? So this is a signal that we see on the landscape, but that says this is a potential risk zone in the, in the near term, right? So everything from fake news to bots that spread propaganda and deep fakes, which are highly convincing video that's algorithmically altered or that replaces people, speech and facial expressions and identities creates fake proof of actions or speech that actually never happened, right? So individuals today are highly motivated to subvert the truth, that massive scale specially for political ends and new technologies will make it easier to spread lies and undermine trust.

Speaker 2:          20:57          So over the next decade, what else could be faked via new technologies? So if this is the risk zone you're scanning for, what questions should you be asking? First, what type of data do users expect you to accurately share, measure, or collect too? How could bad actors use your tech to subvert or attack the truth? Um, what could become the equivalent of fake news or deep fakes on your platform? How could someone use your technology to undermine trust and social institutions? And lastly, if you can imagine the form that that misinformation might take on your platform, even if your tech is a political, how could it be politicized in some way to destabilize a government, a community of regime, etc. So this is just again, checklist, thinking about your product, questions to ask and risk zone number one risk. So number to addiction in the dopamine economy.

Speaker 2:          22:00          So in this risk zone, we all have sort of come to understand that time spent online, maximizes profits over wellbeing. And you can see that in things like research by common sense media, which shows that actually now the average teenager spends nine hours a day using some form of social media. Uh, and actually at the same time, studies show that people achieve their maximum intended use of an APP like Instagram or snapchat after 11 minutes. Okay. And actually after 11 minutes, overall happiness decreases. So how can you actually design tools to advocate for user happiness offline and online over keeping eyes glued to the screen? Right? And so the questions in risk zone number two that you probably would want to be asking are, does your business model you maximize user attention and engagement? You know, essentially the more the better. And if so, is that good for people?

Speaker 2:          23:05          You know, what is extreme use look like? Um, and how would you know, moderate versus extreme use? How could you design a system that actually courages moderate use. And lastly, is there potential actually for negative material for toxic material to actually increase and drive levels of engagement that maximize time spent on the platform for questions to ask around addiction and the dopamine come, Hey, I'm going to roll through these a little more quickly now, um, for each of these just so that I can enjoy all of the eight risk zones. Uh, so economic and asset, right where new technology can democratize access, but it can obviously also exacerbate inequality. Uh, in this world you can get cheaper insurance by being white, right? And 2017, Oxfam international show that eight people owned as much wealth as the entire bottom half of the world's population, right? So wealth, concentration and distribution is an issue.

Speaker 2:          24:11          And this, uh, new technology can provide income, opportunity, and balanced distribution, but it all can also cater only to high income groups and eliminate low income jobs. So what questions should view asking which groups are disproportionately impacted? Um, how might workers be impacted on your platform by virtue of the type of form a or contract that they are given contractors versus full time employees? How might a communities with fewer resources actually not be able to access your platform or B, have too much access to your platform, right? To think about how distribution of wealth is impacting both the haves and the have nots. Uh, in risk don't four, we're looking at machine ethics and Algorithmic bias, right? So in this world, human biases amplified through artificial intelligence as in the case recently where Amazon's scrapped it's uh, HR recruiting tool that actually said if you had the word woman appearing anywhere on your resume, you were actually, I think it was 27 times less likely to receive an interview then if that were, did not appear as a keyword on the resume, the AI trained itself.

Speaker 2:          25:33          But actually, um, you know, the application of AI in critical domains like welfare and education, employment and criminal justice, criminal justice has intensified, right? So the idea that technology is neutral and that this is not a product of human action is no longer really acceptable because we actually sort of know that human decisions are being used to make the models, human decisions are using to categorize the data. Human decisions are being used, um, and the algorithm, and you can't blame the algorithm any longer. So at the core of the questions you want to be asking are, you know, do use deep data and machine learning and are there gaps in the data that historical biases might actually be biasing the technology. So for example, the compass criminal justice profiling system, uh, was sentencing black, um, defendants far more frequently than white defendants and to much more severe sentences because it was using a dataset that showed, um, you know, that black defendants were more likely to commit crimes again in the future.

Speaker 2:          26:41          Well, it turns out that if you're using historical data about crime and it's incidents, right, and you're using that to predict future, you're actually just reinforcing the bias of what has happened historically as opposed to countering that bias. Um, by understanding that actually pass is not a future. Does that make sense? Right. Um, like black defendants, yes, have typically had higher recidivism rates because of historical, deeply entrenched economic and sociopolitical, uh, inequality. Right? But if you use that to predict the future and sentence people accordingly, you're just reinforcing that bias. Uh, have you seen instances of that bias actually entered your products algorithms? Is it actually amplifying that bias and who's responsible for it? Do you have a diverse team that can spot those risks early enough to understand that you may be perpetuating those biases? And actually, how will you push back against the blind preference for AI develops systems?

Speaker 2:          27:50          And do you have transparency into that system and is their actual recourse for people who feel like they've been negatively impacted? In Rick's zone five, we've got a surveillance state, uh, set of issues where surveillance tools and facial recognition right today are empowering the powerful at the expense of the powerless. So recent examples of social bots being co opted by governments and militaries for use and attacking their opposition. Uh, armies of automated software driven profiles are used to target journalists and activists and citizens using Western surveillance tools. Uh, so in this risk zone you need to be asking about your product. How might a government or military body use this technology to increase their capacity? So surveil their citizens who besides the government and the military might actually have access to those tools and want to increase the surveillance of its citizens as well. Who would they track?

Speaker 2:          28:45          And why are you creating data, right? That actually could follow people throughout their lifetimes and actually will the data your tech is generating have longterm consequences for the freedom of reputation of those individuals. And who would you not want to use your data to surveil and what can you do to proactively protect that data from being accessible to those malicious actors who might not want to have access to it? And risk zone six, we're talking about data control and monetization where users actually lack the control to share and monetize and benefit from their data alongside the tech companies that create and use it, right? So we've seen this actually and we understand that users expect access to tools now for acquiring, sharing, interpreting and verifying or information that's been collected about them. And we expect, uh, that there will be an increasing level of agency around data from users in the future.

Speaker 2:          29:48          So if you're creating a product that collects data and that's just about every product these days, what are the questions you want to be asking? What date are you actually collecting? Do you need to collect it or would you just like to collect it? Are you selling it? And if you're selling it to who and how might they go about using it to your users, have the right to access the data you're collecting on them. And could you build a way into your platform or product to give them the right to capture, share, monetize their data independently outside from you? Uh, what could bad actors do with this data if they had access to it? What could the government do with the data if they were granted access to it? These are actually questions that we've seen come along on the landscape recently. Um, particularly with access to people's phones or other types of requests from the government for people's data.

Speaker 2:          30:40          And Risto seven, we're talking about implicit trust in user understanding where the misuse of data as a serious problem actually because users don't trust companies with a opaque terms of service. And uh, the inability for users to actually understand how a popular APP or platform is working, how their engagement is optimized, what's being tracked and collected. It's really hard for users to have clarity on what are the terms and companies can expect backlash actually from product, from the users of their products and from employees actually when those terms of service are violated as in the case here with Uber. Uh, so the questions to ask, does the technology you're building have a clear code of rights and are your terms of service easy to read, access and understand, uh, is there a version of your product that's available to users if they don't want to sign the user agreement?

Speaker 2:          31:42          And could you imagine building a product that would do that? Does your technology actually do anything you don't even know about? It's hard to imagine, but this is actually the case. And actually, um, if users object to the idea of their actions being monetized, is it possible to create a sustainable model that builds trust with them and are all users treated equally? Um, so could you handle consumer demands are government regulations that require all users to be treated equally or at least transparently. Um, could your product or platform actually do that? And lastly, rezoned eight hateful in criminal actors, uh, where online tools enabled glo global dissemination of terrorism, hate bullying, radicalization trolling, doxing, and much more. You know, we saw this very unfortunately most recently with the massacre in New Zealand or social media platforms were used to disseminate terrorist activities and used to radicalize the shooter in the first place.

Speaker 2:          32:44          So using platforms to actually perpetuate these kinds of behaviors globally. Uh, and there is a responsibility as a foresight tool to be thinking about those possibilities before they happen. So how could someone use your technology to bully, stalk, or harass people? You know, what kinds of ransomware theft, financial crimes or fraud could come around by use of your platform? Do you as a technology maker have an ethical responsibility to make it for bad actors, to act, to make it harder for bad actors to act on your platform? How could organize hate groups? Use your technology to spread hate or recruit, uh, others. And lastly, what are the risks of your technology being weaponized? And what responsibility do you have to prevent it? So if you go through those eight risks zones and there's a lot more embedded in each of them, you can download this full full toolkit from ethical o s.

Speaker 2:          33:45          Dot. Org. Uh, there's a lot more embedded in each of those risks zones. But if you go through that, here's three things to do. Uh, share it with your team. Go through this as an exercise with your team. Go through the risks zones, understand the impact of your technology at your team level. Um, you could consider adding some of those top questions to your product requirements document or any other document that you set out at the beginning of the development cycle to think about, which was questions you should be asking all along the way as you develop. And before you ship. And lastly, you should scan the horizon actually on as an ongoing exercise for additional information about the risks zones. And if you're doing that, you're doing the work of the ethical ols and thinking about the consequences of the tech before you ship.

Speaker 2:          34:34          The last tool we built unethical ols is actually, um, just some ideas for how to do some alternative ways of future proofing. So thinking about best practices that could help the tech community community mitigate risk at scale. So these are some ideas for industry wide efforts to create products that you know, have a company in humanity's best interest in mind. So these are just kind of pie in the sky ideas. There's a lot more actually in the ethical OSMP but here are a few to consider. Um, so one is, you know, a hippocratic oath for data workers. So you could imagine a world where anyone working with data took an oath to protect the data of its users. So if you were going to develop such a thing, what commitments would it include? Uh, what rituals would be associated with taking the oath in the first place.

Speaker 2:          35:26          It's actually a beautiful ceremony for civil engineers in Canada called the order of the iron ring. And when you become a civil engineer in Canada, you're presented a ring that is apocryphally made from the metal of a collapsed bridge and you're meant to wear the ring on your right hand. You're drafting hand. So that all along the way through your work as a civil engineer, you keep in mind the ethical responsibility to the people who will be crossing your bridges are living in your buildings. And so you can imagine building rituals like this for a hippocratic oath for data scientists or data workers more generally. Um, second tech companies have bounties for bugs. If you find a bug for Microsoft, they will pay you to have found that bug. Could you imagine actually building ethical bounties as well for identifying risks in these risk zones or any others?

Speaker 2:          36:17          Um, how would somebody claimed that bounty? What, what should they be paid? Um, and actually who in the company would be responsible for assessing whether that risk identified was actually a risk than needed to be mitigated in the first place. You could imagine also a list for employees within a company of red flags that absolutely required running up the flagpole or pulling the metaphorical red handle. Um, so what could go on that list? Who would create an update it and would it be shared across the company? Would it be public? Would it be proprietary? What each company have it, you know, these are questions to think about as we think about resilience building actually has a system of companies and product makers and developers, et cetera. And the last idea we had was just, um, you know, creating an actual licensure for technologists. So, you know, you could, as lawyers and doctors and a whole bunch of other professions have licensed to operate and actually if you behave unethically, you can be stripped of that license and no longer be able to access the privileges of that profession.

Speaker 2:          37:23          Could you develop a similar thing for technologists? Um, and if you did, what kinds of things would actually cause you to lose a license, right? Medical malpractices medical malpractice causes you to lose a license if you're a doctor, what would cause you to lose your license as a technologist? So all of this to wrap up is, uh, a way of thinking about what's hot and what's not in ethics. Um, with credit to my friend Jake Metcalf for this, for this list. Um, so what's hot, show us your work. Uh, show us how you got to this decision with transparency and what's not as follow our principles blindly, right? What's hot is global adaptability and interoperability of platforms as opposed to adherence to western Canon only as thinking about ethics and instead of thinking about western Canon of ethics, thinking about societal impact more broadly, what's hot is defining the due diligence, that process of discernment, the process of thinking through those consequences first, second and third, order what's not as an inflexible framework that doesn't allow you to spot those risks in the first place.

Speaker 2:          38:32          Uh, not hot as close community. Thinking about these things in silos. What is hot is open source, open dialogue tools that enable us all to get better at this. And the first place, uh, hot ethics is a process. Um, it is actually labor. It requires sustained effort to identify, track, mitigate and improve the consequences for the things that we value as human beings. And it is not PR, it actually matters because it impacts people in their lives. And lastly, what is hot is actually, you know, ethics doing this work as assignable tasks, right? You can be the owner of this on your team. It can be parceled out by a product manager, by a team manager, etc. With responsibilities for different people to spot and bring these risks to the table. And what is not hot about doing this is where ethics is a black hole expense, uh, where there is no accountability built into the framework.

Speaker 2:          39:30          Uh, finally one last thing. I'm going to make a plug for a new podcast, which if you haven't listened to it, you should. It's hosted by Caterina fake, who was the co founder of flicker and has since become a venture capitalist. She has a new podcast called should this exist, which, uh, I asked a lot of these same questions of early stage entrepreneurs and inventors, uh, and startup founders where she brings them on with a new product idea onto our show and ask these questions. Is the product of your building fundamentally good for people? What negative consequences and unintended impacts might you imagine about this product? How can you mitigate them? Have you thought about all of these first, second, third order consequences? So that's at the core, the ethical ols. It's a way of bringing foresight about unintended consequences or unanticipated consequences or consequences actually that you probably could have foreseen if you'd ask the right sets of questions in the first place to the activity of building something new and to the activity of adding a feature to the activity of building new code to the activity of at fundamentally building things that we imagine will be good for people.

Speaker 2:          40:44          But we might be not imagining the ways that it might be bad as well to put that glass half empty. So with that, I will, uh, take it to questions.

Speaker 3:          40:57          So thank you so much for coming. This has been such an enlightening talk. I noticed that you, it's this product seems to generally be based for futures, uh, events, our future occurrences. Could you reapply these to current companies now and kind of revamp this way, their system's flowing now, do you think?

Speaker 2:          41:14          Yeah, I think absolutely. I think at the core of these questions are meant to identify negative impacts, right? And you could absolutely be asking the same set of questions about a current existing product. How is it doing this now? Are we building a product that currently is being used in this way? And how might we actually just mitigate the current risks, not just, uh, anticipate those future risks. So I think it absolutely has application for current products. And again, most products, uh, living in the world today go through multiple iterations. There's very few products that ship and are done. And so with each iteration, with each new feature, with each tweak to the code, with each tweak to the platform as we go through the evolution of a product too, it's also a useful tool. Even along the way there, I would say, uh, that it's not, and this is not meant to be a one and done activity.

Speaker 2:          42:08          It's not meant to just be asked one time and then say, okay, well we asked the set of questions and now we don't have to worry about it. But building that process of discernment that all along the way we're asking these questions with frequency, uh, that we're able to spot and great if we answer no, nothing's changed. And that risk that we anticipated has, is no longer an issue and it continues in long no longer be an issue. But at the point at which it becomes an issue, again, let's surface it and let's think about ways to change the behavior, change the product, changed the way we're doing business.

Speaker 4:          42:41          So do you know of any companies that have already started this? Anyone reported back to you saying, yes, we've had a week long meeting to go through all of these steps.

Speaker 2:          42:50          We do know companies that are using this. Uh, and we have actually also had some degree of success embedding this in incubators and accelerators as well. Uh, so we're able to sort of get a class of early stage founders are startups thinking about these issues from the outset. Um, I can't speak specifically about any particular companies. A lot of companies don't necessarily want to be public about their use of tools like this. I think that's becoming less the case as more companies are embracing their responsibility and their ethics. Uh, as someone I heard recently said, ethics is now the hottest product in Silicon Valley. Um, so I think a lot of people are actually starting to be proud of their use of tools like this and not shy away from it. But yeah, it's, this is definitely in process and if not this other frameworks for having these conversations, right? There's like a design agency in Seattle called artefact group, which design Taro cards of tech, which is a card deck, which actually asks a lot of the same questions. And so really just thinking about what are the tools for asking these questions and it doesn't need to be ethical. Oh, these questions could also just be asked by any buddy on the team who's empowered to do so. And team leader, any manager, anybody at any level should be able to ask these questions. Um, whether through ethical oh or just because they care.

Speaker 4:          44:16          So ethics is not an exact science. How do you decide what is termed ethical and not ethical? Largely opinion. Yeah.

Speaker 2:          44:24          Yeah, I get that question a lot. Um, so if it's not to use the term ethics, I say do away with the term ethics. Um, because at the core, which I hope came through in this presentation, when we're talking about ethics, I'm talking less about, uh, sort of structured and rigorous frameworks for Western ethics, right? Deontological versus consequentialist versus utilitarian frameworks, dissecting cons versus mill. And really, right. I don't find that exercise particularly helpful actually. I find the exercise of thinking about ethics as this responsibility for mitigating harm as the core activity of, of, of doing ethical thinking. Right? So, um, in that case, I think that that issue of ethical relativism becomes less of an issue because really if we're asking these specific questions who is negatively impacted by the tech and you don't need to care, you could say, right, we know that some people are being negatively impacted by this.

Speaker 2:          45:30          And, and we're okay with that. That's fine. The only thing I think, uh, I'm advocating through the use of a tool like this is go through the process, ask the questions, answer them thoughtfully, discern the potential outputs, outcomes, impacts, and then say, how do we trade off our values against those potential futures? And those values get to be defined by you, by your team, by your company, follow your values. Those don't need to be defined by anyone else. Those you get to define and then make the tradeoffs. Once you've asked the questions between the impacts you might that might occur, uh, or the ways your tech is behaving or the ways that users are using your product, et cetera. And say, how does, how do we square our values with those impacts? Um, what, what comes next? I think, uh, for Omidyar and beyond the Escalade, this toolkit can, do you have any comments about what you guys have in the pipe?

Speaker 2:          46:29          Yeah, so I think at the core there's a, a need more broadly for ethical infrastructure, if you will, that actually supports this activity in the first place. Uh, if you plant a healthy tree in a withering orchard, uh, it's unlikely that that healthy tree will resurrect all of the dead trees in the orchard. And it's unlikely that that a healthy tree will flourish if the soil is poisonous. Right? So building ethical infrastructure and ethical culture is a necessary piece of the puzzle that supports the use of a thing like ethical Oh, assets scale. Right? So I think one of the things we're trying to puzzle through is how do you actually build the kinds of things that scaffold ethics all along the way, right? So you've got training of future technologists and how are they exposed to the behaviors and mindsets of asking these questions?

Speaker 2:          47:34          Actually all along the way as they learn the activity of coding. If they're computer scientists or engineers, how are employees onboarded and trained once they enter a company to think through these issues and then what kinds of supports exist for them to raise red flags. Are there chief ethics officers are ethical ombudsmen that support that activity. How do venture capitalist funders, institutional or otherwise set KPI's and metrics for companies that actually support their activity of doing ethics better and having more mission aligned in values driven companies in the first place rather than just incentivizing growth overall else. Um, what kinds of consumer awareness needs to be built in order for consumers to have more active voice in the kinds of products they want to use? How our employee voices harness on the issues that employees care about. We've seen a tremendous amount of employee activism recently in companies large and small about the kinds of ethical issues that they see. All of that speaks to kind of ethical culture and ethical infrastructure. And we're thinking about that broad echo system. So not just tools like ethical OSCP, which is a really important piece. And tools are absolutely necessary, but also what is, what are the supporting mechanisms to enable that behavior?

Speaker 5:          48:51          Uh, just as a followup to that question regarding the infrastructure needed. Um, I guess like you're in the u s there's probably a lot of skepticism with regard to the ability of maybe like our federal government to be able to provide that sort of infrastructure. So to what extent are you optimistic regarding our ability to actually put those mechanisms in place? And if that doesn't happen, then to what extent are you optimistic that the market or you know, some other actors can do it in that, in the absence of that,

Speaker 2:          49:22          I am not necessarily optimistic about our government catching up. I mean, if you, if you see the recent tech hearings, uh, unfortunately our members of Congress do not demonstrate a extremely high level of proficiency or fluency around products and the way they are built in their impacts on people. Uh, and yet we are seeing this really interesting trend of now some large companies actually asking for regulation of certain things. Now, there are lots of reasons they might be doing that. Uh, but I think there is a growing awareness that regulation will probably be necessary and I am not bullish on the idea that that will catch up very quickly knowing. So unfortunately the dysfunction of our current government and the system of passing legislation, I think at the local level it's actually probably more likely, um, at the city and state level we might see more interesting regulation coming out, especially from California and a few other places that are thinking about these things.

Speaker 2:          50:22          Um, if at the core of your question is about, so then to what degree will companies regulate as it's called, right? In order to do this better, if the government can't actually assert itself in terms of legislation and regulation of these things? I think, um, there is a competitive advantage to be built from doing this particularly well and building user trust around security and data, uh, about thinking responsibly about the products that are built. Um, you know, I, I often times come back to the example in the automobile industry of Volvo, uh, which created the safe, the safest car on the road, and every person who was going to put their teenager into a car new to get them a Volvo because it was destined to crash at some point and you're just wanted to protect your kid. Right? And so it was actually competitive advantage to be built around being the safest car on the road.

Speaker 2:          51:16          And so I think that we, uh, I am perhaps naively optimistic that we may be moving in a direction for the tech industry as well, where there is a competitive advantage to be built based on doing this. Well, and in that case, I think that we may see a race to the top before it. Um, again, that might be naive, but you asked me to my degree of optimism and so I will say that I am naively optimistic about it. Thank you very much. You have for, for coming today and speak and thank you for your time and for taking all these questions. Thanks for having me. Appreciate it.