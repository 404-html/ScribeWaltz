Speaker 1:          00:06          Thanks for, thanks for everybody coming out. It's great to see so many of your hair. Uh, I'm just, I'm smart. I'm a engineer on the Google assistant team and I'm just going to introduce the Dan and Rosie who are thrilled to have with us today. Um, I'm curious how many of you guys got to see this piece while it was, uh, by the subway cars in outside five VB? Yeah. Cool. For those of you who haven't, I really encourage you to stick around afterwards, get right up close to it and you'll see all these incredible surprising details that the AI is imbued into this piece. And you'll hear all about that from Dan. Uh, my connection to it is just that, uh, we used to know each other from a previous company that I was in and when Dan started working on this computational photography stuff, he saw deep dream is an opportunity to work it in and kind of an artistic way and asked if I could help him.

Speaker 1:          00:46          And I didn't really know anything about the software and I thought, okay, you know, how hard could this be? And of course these things always turn out to be a lot more complicated than you thought. I'll go into a little bit of that afterwards. But what was really fascinating to me is, you know, this all kind of started because of New York City and that's why it's so exciting to have it here. About 50 years ago today, almost right there was maybe 51 by now. There was one of the very first art plus technology shows in New York City ever called nine evenings. And about 50 years later, a bunch of the people decided it would be really cool to do a retrospective and kind of a new show. And they decided to do it out in Seattle. And Dan was one of the artists that were commissioned to build a piece for nine evenings too.

Speaker 1:          01:20          And so he shot it in New York City. This is for those of you, it's not obvious, it's the Azalea walk inside of Central Park. And so it sort of coming from New York with New York content, but of course with a much more modern twist and after it finished touring a few places in Seattle, he thought, wouldn't it be cool to bring it back to Manhattan? And Google was excited to be able to take it and put it in the building. So it's kind of neat to kind of bring it full circle. And while Dan was in town, I thought it would be great for him to talk through because it's not just sort of release for me. It's really cool to hear about the neat photography tricks and the computer tricks that was necessary. But there's also a whole sort of art history that Dan has of how people over time figured out how to paint these compelling landscapes and compositional elements. That was really new to me. So I hope that'll be interesting to you guys as well. And afterwards we've got some refreshments and stick around for the reception. You get a chance to talk to Dan Moore and uh, we'll leave plenty of time for Q and. A. So with that I'll turn it over. Thanks a lot.

Speaker 2:          02:05          [inaudible]

Speaker 3:          02:09          thanks Joseph, and thanks to you all for being here. Um, my plan here is to take you through the art history, the technology and the psychology even behind our dreamscapes project. I hope you're going to find this fun and perhaps learn some new things along the way. Just a heads up that, um, all of the images in this presentation, unless otherwise noted, are my own work. So let's talk about special places. What I mean by that are precise locations in our world where something very powerful happens. And I'm not just talking about landscapes, but also city scapes and even indoor spaces. I mean, have you ever noticed that in certain places at certain times, the scene before you goes beyond a mere sight and become something you feel in your body? Does anyone ever had that experience hiking or skiing or something like that? Whenever I find myself in the midst of scenes like that, it's like a bell rings.

Speaker 3:          03:09          It just stops me dead in my tracks. Um, what starts then as a visual experience immediately becomes a visceral response. One that I feel right in the middle of my chest usually because it makes me gasp and when I've seen is powerful enough to make that happen. I found myself waxing philosophical, asking questions like, I wonder how a butterfly would see this, you know? Or what if my vision was many times sharper? What's real? Anyway, you know, Physics explains and tells us how we see the world through very limited filter, so I'm sure many of you have experienced something like this too, in the presence of special places, this powerful connection that's quickly formed between your eyes, your body and your mind, and if you're like me and many other people in the history of mankind, I suspect you then find yourself desperately wanting to bottle that experience with such fidelity that when you share it with others later, the same thing happens to them.

Speaker 3:          04:17          Right? Also like me, you may have found it very hard to do that with traditional photography. This drove me crazy. It had me wondering what constitutes this phenomenon and how can it be communicated? Is it even possible? The answer is yes. Let me show you how it turns out. We have an existence proof the great American painters of the Hudson River school in the early 18 hundreds and the European romanticist before them achieve this effect. They painted highly detailed scenes, some of which were 10 feet wider, more with great emotional impact. Painters like Thomas Cole, who's considered the founder of the Hudson River school and is incredibly skilled friend Asher, B Duran and several others created many powerful works like these coals. Protege Frederick Edmund Church was hugely successful at doing this. Let's talk about this painting for a minute is called the heart of the Andes. When this five and a half foot high by 10 foot wide masterpiece debuted in 1859 it was an absolute sensation.

Speaker 3:          05:31          It was first exhibited in New York City as a single painting exhibition. In four weeks. More than 12,000 people paid admission to view this painting. It was set up in this interesting little stage with a skylight putting light on it in a darkened room. They had benches set up, they handed people opera glasses. People were crazy about this thing. Um, the painting was widely acclaimed. Poetry was written in its honor. A composer dedicated a piece to it. Mark Twain wrote about it. The following quote proves my point that it is indeed possible to capture and convey the power of a special place. A contemporary witness wrote this, women felt faint. Both men and women succumb to the dizzy and combination of terror and vertigo that they recognized as the sublime. Many of them will later describe as sensation of becoming immersed in or absorbed by this painting who's dimensions, presentation and subject matters.

Speaker 3:          06:32          Speak of the divine power of nature. So by the way, this painting hangs up town at the met. Now, if you haven't already done done so, you should really see it in person. Joseph just sought for the first time on Sunday. What did you think? Amazing. Um, before I get into exactly how Frederic Church and his contemporaries did what they did, I want to mention one more panel who was very influential to me. Maxfield Parrish Parrish came along much later, in fact, well after the art world had moved on from representational landscape painting, but he brought a level of vibrancy and lucidity to his work that I absolutely love and which really hadn't been seen before. I don't have time to explain how he did what he did, but one thing I can relate to is that he did it by tinkering and experimenting with both old and new approaches to his medium.

Speaker 2:          07:24          Okay.

Speaker 3:          07:24          Okay. Now let's get into the how. I'll use my own work to illustrate this. Starting with this scene that I captured at Nymph Lake in Rocky Mountain National Park, Colorado in healthcare design, there's been a lot of research around what is known as healing art designers and administrators wanted to know.

Speaker 2:          07:44          Yeah.

Speaker 3:          07:45          What kind of art, if any, can reliably make patients feel better? Well, it turns out there is only one kind that does, and that's representational landscape scenes. Healthcare design consultant Barbara Lyons Stewart wrote every evidence based design study I found has proven that only viewing representational landscape scenes measurably decreases heart rate, blood pressure and pain while improving mood. Some people refer to this effect as the pastoral experience, but when you dig into this, you find out the pastoral experience is not achieved by subject matter alone. It actually requires four things. Subject matter, composition, technique and presentation. Let's briefly considered these one at a time. Starting with subject matter. The pastoral experience occurs with scenes featuring things like greenfields. It also includes water scenes,

Speaker 3:          08:44          Heelan, country scenes, sweeping views, coastal views, Marsha's farm land, groves of trees, beautiful Vestas, sunrises and sunsets. But like I said, it's not just about subject matter. It's also about composition and a guide to evidence based art. The author specifically mentioned immersion compatibility and extent immersion is best achieved with the presence of strong foreground elements and or wide fields of view. Compatibility refers to the resonance between the natural setting and human inclinations. Essentially, manmade elements subconsciously make the viewer feel safe. They sense that other people are thriving there and then there's extent which is characterized by scenes of distant wilderness trails and paths leading to idyllic destinations and a sense of being connected to a larger world. This quote really sums it up. The most fundamental feature of a restorative place is that people have to feel that they can move into those spaces and have a wonderful experience.

Speaker 3:          10:05          Okay. Now let's talk about technique. How do we achieve the pastoral effect with a digital camera? With the digital cameras? It'll start with, it starts with something I call x, Y, Z photography. You'll see why I call it that in just a minute. First, let me tell you about the gear. The Sony Rx one was the first full frame sensor compact camera when it was released a bit more than four years ago. It's got everything I need and nothing more and incredibly small foreign factor, a beautiful fix, 35 millimeter Carl Zeiss Lens, and the new version has a whopping 42.4 megapixel sensor with a small camera like this one, I can get away with a fairly compact tripod on top of which is a similarly compact but very important contraption called a panoramic tripod head, which gives me a precise way to capture overlapping views of the scene on the shooting and that's all I need.

Speaker 3:          10:58          With this compact setup, I then capture multiple views horizontally. Let's call that x multiple views vertically. Let's call that y and multiple exposures deep from dark to light. Let's call that Z. It's not really depth. It has nothing to do with distance. It's it's about capturing a higher dynamic range of exposures from underexposed, overexposed to more closely match how our IC instead of how the camera sees on average, it takes me about a minute to capture the entire scene. I then use three different commercial software packages to merge this cubic array of photos into a coherent scene. I blend them multiple exposures from front to back with Aurora HDR. I stitch together the multiple views with auto panel pro and then I crop in Sweden. The image with Photoshop and what I mean by Sweden is all I really do is adjust the tonal range just a little bit to take to reduce the haze.

Speaker 3:          11:55          Really now I'm calling these interim results because you may have noticed I haven't yet started talking about the artificial intelligence element that I introduced to my work, uh, last year, but we'll do that in a minute. With the technique of Xyz photography, you end up with a scene that's more immersive due to the wide angles, both the x and the lie I've seen that's more vivid due to the high dynamic range, the Z, and I've seen that as much higher fidelity containing up to 200 times as many pixels as the HDTV and up to 10 to 40 times as many pixels as you'll get with a pro DSLR camera. What you're seeing here is a non retouched photo of a four foot high by eight foot wide light box that shows the kind of detail and richness we're talking about. I put this graphic together to illustrate the Xyz difference.

Speaker 3:          12:47          The the inset photo you see here on the right is what you get with a high end camera capturing a single exposure with a 35 millimeter lens. You can see that the Xyz method deals an image that is much more immersive, more vibrant, and much higher resolution. So finally we have to talk about presentation to be most impactful. These scenes should be printed, in my opinion, no less than eight feet wide and they should be backlit. We are creatures of light. We're drawn to light. Um, that's why I've been so focused on producing light boxes these last few years. A big bonus about lightboxes is this, their containers. It's very easy to replace the image, uh, to keep things fresh.

Speaker 3:          13:32          So it turns out the entire time I was conducting my Xyz photography experiments, there's been this incredible revolution in artificial intelligence and deep learning. Thanks perhaps to some of the people in this room. I'll let Joseph speak more about this in depth after me, but in July, 2015, your company released a bit of open source software called deep dream that became a viral sensation. How many of you were aware of that when that happened? A few. Okay. So as I understand it, deep dream was originally created to help your researchers understand how their own software was working. And what they found when they applied deep dream to image recognition was that the machine was essentially hallucinating as it was trying to figure out what it was saying after they decided to release the software. The Internet went crazy per these headlines. What most people did with this was they turn their family photos and to psychedelic nightmares.

Speaker 3:          14:39          Now, these are not my own, but as you can see here, some of these results were quite intriguing. I, however, since an opportunity to do something a bit more subtle. So I tried applying deep dream with a light touch to some of my low resolution previews of my landscape images. The results were very well received, but really excited me was this opportunity to add a level of expressiveness to my last scapes that could be very rich. And just like the art world in the 18 hundreds began moving to more expressive, individualized modes of painting landscapes, starting with the pronto impressionism of JMW Turner and later the true impressionism seen for example, in the pointillism of Serah and the unique styles of folks like Cezanne and Monet. I thought I too might be able to, you know, take my landscapes in a new direction. Unfortunately, deep dream as released was simply not designed to operate on multi hundred megapixel images.

Speaker 3:          15:42          It would just crash and I was completely stuck. So that's when I thought of my friend at Google. Joseph Smart. Um, and much to my delight and amazement, Joseph actually agreed to help me and he interned, coerced, I mean convinced his friend Chris Lamb, uh, and video to help him. And this was a huge stroke of luck for me. Cause when you think about the kind of DNA you need to work on a project like this, no pun intended, Chris and Joseph were truly a dream team. So it turns out it took a lot of work to keep deep dream from crashing on my images. Uh, when I'm done, Joseph will share more details about the challenges they faced and how they overcame them. But after months of hacking on nights and weekends, by November, 2015, they finally got it working using a monster compute server with four graphics processing units up on the Amazon cloud.

Speaker 3:          16:38          My job would then be learn how to tune the parameters of deep dream for the best results on my landscape images. A couple of months after that, in January, 2016, they handed me this interface. So let me explain this a little and take you through my workflow first. There's the fact that I have for GPS graphics processing units at my disposal. This means I can experiment very quickly trying out four different dreaming tests at a time, at least on low resolution previous. This allows me to rapidly converge on a combination of parameter settings that I feel is most compatible with my source imagery. Speaking of parameters, there's five that I get to fiddle with. One main one and four sub parameters. The main parameter is the target layer name. Deep Dream has released has 84 of these layers and they tend to range from their effect. Every, everyone has a different sort of dreaming style and at the highest levels they tend to be abstract slash impressionistic where the algorithm is just starting to look at lines and edges and at the lowest levels it becomes more literal and even animalistic because the AI is now manipulating my image more deeply and its quest to find the things that's been trained to recognize things like animals and objects and buildings.

Speaker 3:          18:05          Then there are four sub parameters like number of octaves, octave scale factor and so on, which allowed me to fine tune the intensity and scale of the dreaming features. So when I find it promising mix, uh, I commit to the full resolution version. Uh, and these typically take anywhere from six to 20 hours of computing, depending upon the size of the image. The guys were kind enough to provide me with this progress bar to keep track of that. I try real hard not to press that button there, cancel dream. Um, but eventually I ended up with uh, the full resolution thing. Several hours later, Chris lamb calculated that a typical 10 hour job probably performs about 150, quadrillion floating point math operations just to manipulate one image. So it's working hard usually while I'm sleeping.

Speaker 3:          19:00          So when we first started to get results, I was delighted to see the following happen. I suspect that it would, but I wasn't totally sure this, this thing I call the museum experience. So have you guys noticed when you go like to the met into the landscape painting galleries that you walk in and on the opposite wall you may see some large sumptuous, almost photographic landscape, but when you get close up, you see all these brushstrokes and expressiveness and perhaps some hidden details, right? It's like having two different experiences this far versus near dichotomy. So besides the expressiveness, that deep dream lent to my work when I saw the results, another thing that excited me was to participate in another art tradition of camouflage and dual meaning, um, such as you find in the works of art and Boldo back in the renaissance who painted portraits composed entirely of fruits, vegetables, flowers, et cetera. And in the late 18 hundreds and early 19 hundreds, Abbott Thayer whose interest in protective coloration in nature ultimately led to the use of military camouflage and World War One little side note about that. And of course, the Great Salvador Dali whose works are filled with illusion and multiple meetings.

Speaker 4:          20:15          So

Speaker 3:          20:16          this then brings us to presenting dreamscapes with this kind of detail in the digital realm. These images are essentially in prison. Yes, you can zoom into them online, but the more you zoom into the image, the less of it you see. This can be a very frustrating experience and I can tell you, and hopefully if you haven't already seen, you will notice that something fundamentally better happens when you can walk right up to a scene like this, see all the crazy details and hold the entirety of the image in your periphery. That's one reason why I chose this large format medium, and as I said earlier, I'm all about back lighting. And the last thing I should point out is that the rich parameter space of deep dream enables me to commit to digital originals, meaning I never, I can get away with never printing the same exact image twice.

Speaker 3:          21:09          So another option for viewing dreamscapes is with large video walls. Actually, the first opportunity Joseph and I ever had to see our dream scapes at scale was down at the Qualcomm institute and Uc San Diego on the vroom display as it's called. This was a really fun day, especially after months of looking at these images like with blinders on through the limited window of a computer monitor. Um, so it was thrilling. Um, but I have to say even at 66 megapixels, which is a lot for video, a display, this powerful could only deliver a fraction of the pixels in our dreamscapes, some of which are over 400 megapixels. A lot of people ask me about viewing dreamscapes in virtual reality. I've had the good fortune of doing that in a couple of different VR research labs, including yours. And one thing that they all had to solve was how to deliver the pixels efficiently so that the system is only displaying full resolution.

Speaker 3:          22:10          Depending upon where you're looking at, how much you're zoomed in, not hard to solve. It's been that that problem has been solved for a long time. It's basically is exactly how Google Earth Vr works. As you're navigating the planet, you know, flying through the Swiss Alps, you're really just flying through a very large image dataset. So personal. I'm very excited about, uh, viewing dreamscapes and VR. I've had the experience now in a number of places and it's totally thrilling. It's, it's, it's amazing how immersive, uh, in fact, just yesterday uptown, I got to see this. Uh, it's amazing how immersive this experience is even though these images are only two dimensional and, and there's some great applications, especially for this content given the it's basis in healing art, uh, for things like Stress Reduction and distraction therapy for pain management and things like that. So just a quick stop observation I want to make about all this and maybe about technology in general, um, referring to this sort of saw tooth curve here.

Speaker 3:          23:14          When you look at the different technologies and methods for capturing and conveying the power of a special place, uh, you'll find that each time a paradigm shift occurs, like the transition from landscape painting to photography or from landscape photography to Vr, there's a lot of initial excitement and anxiety. I mean, I, I think, think about it and the, in the early 18 hundreds, mid 1800 so when the landscape painters were at the top of their game, how do you think they felt when a device comes along that can capture the actual photons in the scene? Um, you know, but the truth is it was a big step down and quality and impact. At first you couldn't make color, you couldn't make huge prints, the resolution wasn't there and so on, but it's steadily improved. And you had folks like Ansell Adams producing majestic black and white images in the first half of the 20th century and in the mid 19 hundreds, Eliot porter who's intimate landscapes did much to boost the acceptance of color photography as an artistic medium.

Speaker 2:          24:18          Okay.

Speaker 3:          24:19          And in the late 20th century, Peter Lik using panoramic film cameras, um, you know, to create works that at least in some respects, surpass those of the representational landscape painters.

Speaker 2:          24:32          Okay.

Speaker 3:          24:32          So, well, I think a similar thing is happening right now. They're there, you know, with the advent of virtual reality. Again, a lot of excitement, but a bit of a step down in quality and impact with lots of room for improvement and many problems to solve. But that will happen. And I for 1:00 AM not threatened by this. I quite the opposite. I'm excited to work with the our labs and startups to help make that happen. So back to dreamscapes just for fun, I'd like to pause here and show you a short video designed to illustrate the variety, the zooming power and the surprising details within our dream scapes.

Speaker 2:          29:26          [inaudible]

Speaker 3:          29:50          all right. So I'd like to end by coming back to my original aspirations when I originally set out to do and I hope I've been able to achieve with the help of my ingenious engineering collaborators, Joseph Smart and Chris Lamb. And the, of course tireless efforts of my new best friend deep dream is to share with you my experiences of special places. I want it to project their power and beauty through your eyes in such a way that you feel it deeply in your body

Speaker 3:          30:24          and it engages your mind. And as I reflect on this project and I realized that by teaming up with an AI is taking me beyond my initial ambitions in that it's essentially unlocked a superpower for me. I could never execute these images solely on my own, but it has required me giving up a degree of control and that I can't tell it exactly what to do. And in fact, I don't even fully understand how or why it's doing what it's doing. But as far as I know, the AI has no innate desire to create art nor any ability to discern which of the parameter settings are most aesthetically pleasing to humans. I say as far as I know, because Steve Jurvetson recently pointed out that a sufficiently advanced AI would choose to fail the Turing test. Let's help Steve is wrong, and that my AI is indeed just a tool.

Speaker 3:          31:26          I'll be at an incredibly powerful tool, one that, uh, might be somewhat beyond our comprehension. But ultimately I still make the decisions as to how to steer it and what to keep or discard. I imagine the future of art, science and technology will be increasingly like this. Where we use ais to process more data, see more complicated patterns and otherwise extend our natural capabilities, but working towards goals that we sat and direct. I hope that my dream scapes project serves as a clear example is that artificial intelligence can enhance not only our abilities and our productivity, but even our creativity. Thank you.

Speaker 1:          32:14          With that, I'm going to introduce Joseph. He's going to give a little talk about the tech behind the art. Um, and then we'll open it up to Q and a with as much time as we have left. Thanks a lot. Yeah, thanks Dan. That was great. Um, yeah, there's so much going on. Uh, you know, he sort of roped me into this a little while back and I thought, how hard could this be? This will be fun. You know, I'm, I'm a scrappy engineer. Let's just see what we can do. And then I was like, oh, actually I have no what I'm doing. Uh, and so I share this because ultimately obviously we were able to make it work. And I think that's kind of an empowering reminder that a lot of our sort of basic skills as engineers in terms of just trying things out and being willing to iterate and asking for help and so forth can get you surprisingly far.

Speaker 1:          32:54          And I, you know, it's been surprisingly moving from me to see how much has happened as a result of this collaboration, right? This went way beyond just the fun of problem solving and it's, uh, it's made me kind of appreciate that I don't have to just work on utility all day, that actually helping a expression as it can be just as deeply moving in terms of how we got there. So the, when Dan sent me these low Rez thumbnails, I forwarded them on the internal Google plus and said, hey, aren't these cool? You know, we were talking about scaling them up. What are you think we need to do? And, uh, some people, a bunch of people inside the company, Solomon, including some of the people on the deep Dream Team and they said, yeah, we've had some similar ideas. You know, you're going to have to do some sort of a breaking it up into tiling.

Speaker 1:          33:31          Here's some original, some ideas to get started and so forth. Um, and you know, obviously you can't just take an entire image, scale it down to the size, you know, the, the actual networks work on image is only a few hundred pixels wide. So if you do that and dream it and scale it back up, you lose all the details. But if you just naively chopped the image up into tiles and dream each child separately, you get weird seems between them. So you can't do that. And so it turns out to work best is to break the image down into tiles, but instead of actually chopping the image, you just sort of slide a window around the image and you, you know, in, in the sort of the RGB matrix, you, you wrap the ends around so you can kind of slide off the ends and you do a random offset and jitter every time you dream so that they don't line up perfectly.

Speaker 1:          34:11          And You keep doing that for multiple iterations. And you also sort of scale the image up and down a bit as you're doing it. And you can keep doing that for a while because it's dreaming on its own output. All the lines kind of disappeared and it ends up being kind of one coherent image. And it also gives you these parameters that Dan mentioned that I'll let you kind of control the feature size and the, and the intensity and also of course the layer name and he's found kind of different ways of applying that for different images, which was kind of a surprising plus from that. The other big challenge is memory management. And this again is sort of took me by surprise because there's sort of two layers to it. One is the actual dreaming all happens on the GPS. And I should say, by the way, in apologies to my Google colleagues that we did this on Amazon cause at the time the GPU compute instances weren't available on Google cloud.

Speaker 1:          34:52          So that's a, maybe we should, um, you know, fix that now anyway. But, uh, the GPU itself has memory on it and it's only a fixed amount of memory and so you can't overload the GPU with too much pixels or it'll crash. And so you have to kind of size the workload. And then a main memory actually turned out to be a bigger bottle neck then GPU memory for us, which was surprising to me because even at a few hundred megapixels, these like the jpegs themselves are only a few hundred megabytes. So I was like, how hard can this be? But I sort of forgotten. It's obvious in retrospect that when you're actually doing a dreaming or any kind of neural network on an image, you have to fully inflate it so that you know, each pixel is a 32 bit r n a g and a B. Right? So it's sort of, it's like an order of magnitude or more memory.

Speaker 1:          35:32          Um, and then these python libraries that are the sort of the standard ones for doing scientific computing and, and image manipulation and so forth, they ended up making them kind of a lot of like lazy array copies in the process, which is normally fine, but when you're input is several hundred megapixels, um, it's not so fine. And so a lot of the work ended up being a kind of very careful memory profiling and just sort of stepping through and figuring out at each step what's going on and trying to find clever ways to avoid as many of these memory copies as possible. Um, and you know, thank God for open source, right where a lot of these libraries that we were using themselves were open source. And so we could kind of go as far under the hood as we needed to. And eventually we actually got it working.

Speaker 1:          36:07          Um, and it's, you know, what's cool too is, I mean, this was kind of a night and weekend project is sort of, could barely get it work. And I'm, you know how it is when you first get your code running, you're kind of scared to touch it because you think you breathed on it wrong, it's going to fall over. Um, but we handed it off to Dan and he spent a week going crazy trying all these experiments and it actually held up pretty well. And so I guess, you know, you don't have to set your bar at something that is completely industrial strength, right? Just be able to get a tool that somebody can play with. I think it's, it's sort of surprising sometimes how far you can take that and you know, but it was python. So we just put a very simple like Django web Ui in front of it and we put all the dreaming tasks behind a celery queue so that you could get the progress bar and you didn't have to wait 10 hours for your page to load, but that was about it.

Speaker 1:          36:45          Um, and then the rest kind of took care of itself. And you know, we've made a few little tweaks along the way, but, uh, I, I would just say it's, this is something that I have got no formal training in. Um, but it was a really exciting thing to do and I'm sure you might find other opportunities to do things like that in the future. Maybe not with deep dreaming per se, but things like that. So I definitely encourage you to do that. Um, let's see. Other things that are worth talking about. I guess maybe let me just give you a quick, for those of you haven't seen it, a little explanation because people often ask about why deep dream creates these hallucinations in the first place. And then we'll, I'll bring back then up for the rest of the Q and a. So remember that what the way these networks work is you feed an image in, and this is like Google photos or Google Lens.

Speaker 1:          37:21          One of these things you're trying to figure out, is there a dog in it? Is there a building in it? What have you? And so the way it works is the, the pixels come in at the bottom and then you have a series of layers in the neural network. And each layer is trying to basically learn statistical patterns of the output of the previous layer. The first layer starts to look for patterns of Pixels, which might be a little like lines or blocks of color or shapes. And then the next layer we'll look for patterns of those patterns and so forth, right? And so you get higher and higher level concepts all the way up to eyes and faces and so forth at the top. And of course in the middle, all of these layers that you've trained, you've given lots of images where you've told it what's in there.

Speaker 1:          37:52          Um, they've just learned these weight matrices and that have optimized the target function, but it's completely opaque to you as a, as an engineer, you have no idea why it did what it did. And so when it gets something wrong, you want to figure out, well, like why did it misclassify this image? Um, you, you sort of don't know what to change. And so the deep Dream Team, what they were trying to do was gain some intuition for what these intermediate layers had actually learned. And so they have this very simple but very clever idea for how to do it, which is you run forward propagation in the network up to the some middle layer that you choose so that it's not, it hasn't fully recognize the object but it has some sort of patterns learned. And then you say, well how could I instead of normally know when you're training a neural network, you say, how can I change the weights to do a little bit better at recognize the image here you do the exact opposite.

Speaker 1:          38:35          You say, how can I change the image a little bit to have the weights have done a better job of recognizing whatever it was recognizing. Because remember that the image too is just a ends up being just a matrix of floats and so you can essentially do gradient space, gradient descent in Pixel space with respect to the weights as opposed to normally where you do gradient descent in, you know, wait space with respect to the pixels. And that's all it does. And then you just read that image back out. And so it's kind of like, I think about it like if you're staring at a cloud and you sort of like see like it looks like a horse or something. And if you'd like to think really hard to keep, like make it look leaving a little bit more like a horse, that's essentially what you're doing.

Speaker 1:          39:06          Um, and then what popped out where all of these surprising details. Um, and you know, so that's why you see this, you know, this is the normal inception network that we use in Google photos. So it sees dogs and faces and buildings and the things that we trained to recognize. But you can train a network on a different set of images and it would hallucinate totally different things in the image. And of course, depending on the layer, it hallucinates more or less than distract things. Um, and they actually did find under, if you've seen any of this, they found some cool things in the process. Um, like I guess the, whenever they were trying to learn a hammer, uh, the image that came out of the hallucination always had an arm attached to it and they had to go back and realize it and all their training data, they never gave it a picture of just like a hammer lying on the table.

Speaker 1:          39:41          And so the image had, you know, had always learned it. So it's things like that you wouldn't have otherwise realized. And it actually ended up helping with some of the training weights, but it had this sort of totally unexpected side effect. And for those of you who are around, you'll remember that. Um, there was some posts on I think internal gps as well of uh, the, the deep dream team being like, here's some weird images that have, uh, um, you know, that, that we got as a side effect of doing this, this diagnostic work. And of course Google is being Googlers and all the source code being opened internally, they like grabbed it and started doing it on their family photos and you know, it sort of went viral internally and that then prompted them to make an open source version that led to all those lovely pictures of the Dan showed you of the Internet going crazy with deep dream.

Speaker 1:          40:14          Um, so it's just kind of an amazing progression, right? Uh, to go from something that was really just a tool scratching an itch to something that's had this kind of transformational power. So I just wanted to share that anecdote because I think there's gonna be lots more opportunities for us all to do things like that. Um, let me stop right there. We still have about 10 minutes for questions. Uh, come to the mic if you've got one so we can capture it for the video and then please feel free to stick around afterwards for the reception and we can chat more. So thanks very much guys.

Speaker 2:          40:37          Thank you.

Speaker 1:          40:43          Questions? Who's got one? I know you haven't physically get it also clears yet. Microphone closer to the mic please.

Speaker 5:          40:53          All right. Well I love the fact that you gave that whole art history progression because it fits so well into their thank you. And I particularly like the way, if you look at like say saison and Monet, the way that the artists take the brushstrokes and manipulate them. So even though they're not very realistic, they really convey a different emotion than you would if you had something very realistic. So I love that. But the other question I had was about the images once you or the, the Ai images in the, in the photographs, did you pick and choose like multiple, um, you got different results? Did you kind of incorporate something that was

Speaker 3:          41:34          like more important to you in emotionally then than other images? And how many times do you have to run through that and what's cause that's totally different than what artists have done. So I found that really Greg, great question. So for sure given 84 different layers, I want to have some ideas of what I want to do, at least in terms of do I want this image to be cooked, raw, medium or well done? Meaning, you know, sort of more abstract or more animalistic. I tend to prefer the more impressionistic, but there are times where it makes sense to do it very animalistic, like the wooded scene that has all the creatures in it. You know, the woods we know are alive, but you walk through a peaceful grove and Ma might hear a few birds chirping. But other than that, it seems really still.

Speaker 3:          42:24          But we intellectually know that there are thousands of species all around us that we just don't see. So in that case, it really made sense to go that way. Now there's a whole bunch of layers that create different sort of animalistic effects and to some people, all of those are a little bit creepy, but some are really creepy. And so, yeah, I have to experiment. Conversely, on the impression aside, um, I had a canyon seen that, um, got printed for a couple of art fairs in Miami and, uh, down in Florida. And I really wanted to find a look as if the native Americans had carved every square inch of the canyon with sort of a petroglyph kind of look. And I found one, it took some experimentation. So typically when I say I'm doing four at a time, um, experiments on low rez previous, I'll do like six of those sets, all ended up doing like two dozen attempts now because I'm working on a low resolution previews, it hints at what the final thing is going to look like.

Speaker 3:          43:29          It's not a guarantee that I will have picked the right one. Um, I would say I've gotten good enough at it now so that when I finally do commit to the large resolution job and I do pay for time on the computer by the minute, by the hour and minute, whatever. Um, so I don't want to, you know, I gotta be kind of judicious about that to some degree, but I usually end up getting what I want on the first shot by that point. But not always. Sometimes I have to do the full resolution of few times to get what I really want. But especially when it comes to the intensity and the scale of the feature, sometimes I've got to really tweak with that.

Speaker 1:          44:07          I was just thinking that, you know, I, I did the simplest possible thing of expose all the parameters and Ui. That's like when you know, you know, you don't really understand your user needs, you just sort of give them all the controls. And normally that's not a good idea as an engineer, right? If you're building a really usable product. But it allowed Dan to do a lot more matching of the style to the picture, then I would have originally thought possible. And one that I love is, uh, he also got commissioned to build a piece for a startup in Guadalajara, Mexico. And so while we were there, we went to, there's this amazing old cathedral and the senator of Guadalajara and he took this interior, I think maybe we should have one of the videos. And he did this, he did this dreaming style, which it's kind of like spirally and it looks like you're having a religious experience and they're so like for me that like perfectly works with, you know,

Speaker 3:          44:44          yeah. Subject. That was a good example because we really want to be careful not to do anything that would be offensive to religious people and conversely hoped to find something that they would even be inspired by and felt spiritual or a theorial. Right. And we found, I think we found something. Uh, it took a lot of tries though. You amazing thing is we were able to do all of that to completion from our Western hotel and Guadalajara. That's right. Because the robot is up in the cloud and I could access it within that with any Internet connection. Yay. Robot overlords. More questions? We've got a, we've got five minutes left on five minutes. Any other questions,

Speaker 6:          45:26          Dan? You wouldn't, you wouldn't. Joe One's an engineer, one's a little bit further out than an engineering and Joe Mixes Joe Mixes or build something and it comes up structurally this way. You're a chemist from what I'm seeing in your, you're mixing it together and you're not sure what happens and what I started up, you're getting an idea now when it's coming out. What's the future you see for these? You had mentioned something where you wanted to use it for therapy and what some of the future you say.

Speaker 3:          45:56          Yeah, so I think there's a couple parts to that question. Um, one is where kind of the workflow goes from here, where kind of where I can take it next. And then there's the question of of how can it be applied. So in terms of what I'm doing with it now as an artist, in terms of trying to push it is I continue to go on photo expeditions and look for special places and capture them and ideally create full scene dreamscapes. But in the last year working with a number of art galleries and some really astute gallerists and curators. I've gotten some ideas for really sort of taken this to a place. At first I had never even thought of. In fact when I heard the idea I objected and realize later that this one curator was right. Probably helps it. She's Phd from the Sorbonne and everything else, but she gave me this idea, she said, what about what if you took, you know if this is eight feet high, what if you took a one foot square detail right and blew that up to 40 by 40 inches and maybe dreamed on it afresh with more intensity, what would you get?

Speaker 3:          47:07          It might be a really cool midsize piece and I gave her all sorts of reasons why it wasn't possible and that's not what I do and all this turned out to be the best thing I ever did it. I just last week released a new series called mining nightfall where I took that central park twilight scene. You might have noticed in the video and crew and, and minded for like I ended up with 11 of those scenes and it's a wonderful fanatic kind of set a, so that's like where I'm sure there are more ideas like that that others can suggest or I can come up with over time. In terms of the application, um, again, this does have its basis in healing. Art, uh, is not what I initially set out to do. What I set out to do was to, I want it to get that feeling across that I have.

Speaker 3:          47:59          And that feeling happens to be one of the reason we love to go hiking and going into nature and stuff like that is because it makes us feel so good. That's why they call them restorative places. It restores us. Um, and uh, so the application I'm most excited about is, is this sort of VR virtual reality application for stress reduction. And Like I said, a distraction therapy for pain management. Think of burn victims with this opiate opiate epidemic. You don't want to be handing out a lot more painkillers then you need. And it's known that the brain can only process so much stuff and if you're distracted doing something else, you're not going to feel the pain as much. So they've actually had good results experimenting with burn victims, putting them in a Vr headset and giving them something really intense to focus their concentration on and the need for opiate drugs and painkillers is, is significantly reduced now in addition to making it something cool and interactive, if you also make it something that's got its basis in healing, art, you know, is this is going to be a win win.

Speaker 1:          49:08          And I think more broadly, the WHO, there's this idea of collaborating with AI and having it extend your capabilities, but also as sort of a metaphor for the future of knowledge work where we don't necessarily have our hands completely on the controls and we have to kind of learn how to steer it and work with it and accept its limitations, but also let us go further than we've been before. And how it helps us think about the way we see reality. And I think there's just in, in music and in video and imagery, there's so much more opportunity there. So this definitely convinced me that we're just scratching the surface.

Speaker 3:          49:34          So we were told to wrap up at four o'clock in four seconds. We're done. Thank you. Very. Thank you very much.