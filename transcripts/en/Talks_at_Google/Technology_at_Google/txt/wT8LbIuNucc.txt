Speaker 1:          00:06          I was told I have to tell you who I am and why I'm here and not somebody else. And uh, that's very funny for me. I never do that. But at any rate, I should tell you, as was indicated, the first 50 years of my professional life was bent becoming a certified quantum mechanic. I know where from I speak when I talk about quantum mechanics, I started out being a particle physics phenomenologist. I moved into formal quantum field theory and in particular non-paternity additive techniques in quantum field theory. That led to my moving into a solid state physics and condensed matter theory. And then finally I ended up talking, working on the subject that I'll be talking to you about today. Now obviously I had no restrictions on what I did. Okay. I had the world's greatest job. So the real question is why did I leave it?

Speaker 1:          00:58          I mean I told people my job description when I got tenure at slack, it was a one liner and said, you are to do anything you feel like doing for the rest of your time here. As long as you think it's good science. So, I mean, why the hell did I retire to do this nonsense? And the answer is because six years ago I co-invented dynamic quantum clustering guard DQC and I created the company, I mean Stanford Patent that it right away. And there are three patents that surround this technology now and my company controls the exclusive rights to those patents. Um,

Speaker 2:          01:37          but

Speaker 1:          01:39          the reason I wanted to make the company go and it was clear Stanford wasn't gonna make it go. Their idea of how they develop something is they tell the person who invented it, go out and make a company. And, and it, you know, you can try to avoid that for a couple of years and then you say, it really isn't going to happen unless I do it. So I did it. And why? Because I'm not a bad physicist and I've done a lot of neat things. I never saved anybody's life. And I think this has the potential to do that. And that's why I'm here today to talk to you. And it's also why I've changed what I'm going to talk about. Okay. So what is dynamic quantum clustering? It has a really different way of looking at data. It opens up stuff that I don't think anybody really does except the QC.

Speaker 1:          02:30          Uh, it basically solves the problem of how do I find unexpected information. I know nothing about the data. I'm looking for unexpected information that I don't know how to look for and it's hidden in my large, noisy, messy, big dataset. I dimension. Okay. Or I liked the other quote, how can I find the needle in my multidimensional haystack if I have no clue what a needle is and there's no guarantee it's in my haystack. Okay? So I need a way of looking for things which can't use a predefined search strategy. Okay. One way to do this, Adam saws in the audience and we had a fight about this. I love losing fights. I lost one to Adam. He said, well, it's generally clustering. And I said, no, it's not. And then I went back and I said, well, it's been a long time since I went to Wikipedia and look up the definition of clustering.

Speaker 1:          03:22          I said, clustering didn't mean looking for a long complex, extended structures in the data. Well, it turns out that's wrong. I was wrong. Clustering since I looked last. Got that as part of the thing. Not too many things to do it, but that's what it is. But the typical ways of looking for unsupervised stuff will be hierarchical or k means clustering. There are things called DB scan, HDB scan, the current gold standard for biology work, t Sni, HDB scan, a Gaussian modeling. All of these things have several requirements which we have to get rid of if we're looking for unexpected stuff. One is they need to clean the data. They have tuneable parameters. They build assumptions as to what clusters are, how many clusters there are and what you're looking for. All of that gets in the way of letting the data tell you what's in the data.

Speaker 1:          04:16          Okay? We need a way of letting the data talk to itself. A DQC can do that. So how is DQC different? This is the 30,000 foot view of it. We can return to it towards the end of my talk if I don't run out of time when people let me talk over and that is uh, what DQC does is it uses the data itself to create a proxy function, a function that is a proxy for the density of the data. This function has the property wherever there's locally more data, it has a minimum. If there's a region of dense data along some curve it has a trough, which is, it has points of inflection in the car basically once you've got this function, the next problem which occurs, think of this as a topographical map. So I've drawn it in three dimensions and the data is distributed.

Speaker 1:          05:11          There are rivers in this map, river beds and the stream beds where the data is more dense and of course because of fluctuations in they just statistical fluctuations, systematic errors, the data will be distributed up along the banks of those streams. So the name of the game is we just let everybody rolled downhill into the stream. The nearest point in the stream, you will see a movie that does that. That is DQC is just the movie. The analysis is the movie. Okay. And then, uh, the stream beds will appear. You'll have understood the structure of density variation in the data. And then if there's more data at one end of the stream, then another things will slowly flow downstream to the ocean. The whole movie telling you stuff about the data, the time to reach the stream bed is a measure of how far the data was.

Speaker 1:          06:03          So just counting frames, you are able to get a measure on the data, uh, a metric on the data, which is how far you are from the stream bed. Okay. Uh, there are three knows that defined why DQC is really different. Okay. And why it's better, faster and cheaper if I were trying to pitch you and sell you something. Okay. So first no hypotheses. I mean, you know, the typical thing when somebody has a data project, they put a bunch of smart people in a room. They yell at each other for several days. They come up with a hypothesis and then they find out that hypothesis is wrong and they go back into the room and they hate each other at the end of this process because they've been yelling at each other. But it takes a lot of time. Analysts are expensive, they're time is money and so, but worse, a hypothesis biases the result you can find.

Speaker 1:          06:55          You've already gone in with an assumption of what the data has to tell you. You don't want to do that. The other thing is you don't clean the data. Now I've been warned and Google, I have to be very careful about saying that, so I'll be careful a little bit. Okay. The whole question of cleaning data is a huge issue. There are many things you can do to data including burning it that I can't fix. Okay? Uh, but in the sense that I only want your raw data. Okay? That's what I work with. People ask me, you know, do you want the raw data or the process data? And I said, I want the raw data. And the guy says to me, nobody works with the raw data. It's like, I want the wrong data. I don't want your fingerprints all over this data. You've already messed it up as far as I'm concerned.

Speaker 1:          07:41          Okay. So you don't preprocess the data to remove noise. You don't process it to try to get out systematic artifacts that you know about. You just live with them. It'll be fine. They'll take care of themselves in the process. Uh, if the data's gap in columns, think of the data generically is some big spreadsheet. If there are gaps in the columns, I don't care. I have plenty of examples where gap data work just fine. Okay. I mean, I'll put a minus one in for something like that. Any kind of substitution, I needed a number in there, but it doesn't have to be a sensible number. Okay. Uh, so if I don't have to clean my data, I've already saved the gigantic amount of time. Okay. That's typically the place where an enormous amount of time is wasted. And then, I mean, I know people where you say, first thing your analyst comes in and says, well, we have to clean your data.

Speaker 1:          08:35          And then he goes away for six months and then it comes back and says, I couldn't clean your data. So the project's over pay me. Okay? Uh, no. The analyst's time is money and you have the danger again of throwing the baby out with the bath water. If you don't know what you're looking for, starting to clean it means you're going to throw away everything but what you think you might be interested in. And the third thing is you don't need expert knowledge in the first stage of an analysis. That doesn't mean subject matter experts are needed. It means you run the analysis, you see what you see, and then you call them the subject matter expert to say, what the hell am I seeing over here? Okay? And so, uh, of course, subject matter experts or the most expensive analysts you're going to find.

Speaker 1:          09:21          So not wasting their time in the beginning is a big deal. Here you see three analyses, okay? Now this is fun, okay? This is one we completely understand. This was done as an exercise. This is Sloan digital sky survey data. These are 350,000 galaxies. The coordinates are the angles theta and Phi on the sky, and the red shift, meaning the distance of the galaxy from us. You've read the New York Times, I hope most of you or else Facebook showed it to you or something and they told you that galaxies are distributed in filaments. The elementary structure of high density and voids. This shows you at the beginning. It's not so easy to see. The way that's done is very complicated and took a long time. This is DQC running on that data, pulling the data, the galaxies back to the higher density regions. That is the right answer.

Speaker 1:          10:17          So number one, that's impressive, but number two, what's more impressive is the structure of what I'm showing you because I don't know of any other way to see a complex interlocking structure of filaments like that. The real structure of the data using any other cluster, the reality that's not possible in my knowledge here. You're looking at data from a hyperspectral camera. It starts out looking like a hairball and it contracts to this extended structure of filaments. That's the kind of thing I was telling you in this case. That stuff is related to things that are really going on on the ground, on the ground. This is a picture of a quarry in upstate New York and the camera is a Mega Patho megapixel camera that's taking a frequency spectrum for the half a million pixels and 512 frequency. So I have a half a million, 512 dimensional pieces of data.

Speaker 1:          11:13          And the question is which ones look more like one or nothing. Why is that interesting? Because light is reflected from the ground. I suppose. I am the military, which is not my favorite customer, but let's say I'm in the military and I have a tank in a desert and has been camouflage to look like the sand. Well, it's been camouflage to look like the sand and the visible frequencies, but it doesn't look like the sand and the nonvisible frequency. So if I look at who's like one another and area, the shape of the tank will appear in the data. Okay. And it will be visible to a hyperspectral camera, which can be mounted in a drone and flown over area. So that's pretty cool. It's also have used the Coca Cola and Pepsi Cola course. They're the biggest corn growers in the world and they, uh, have huge fields of corn.

Speaker 1:          12:04          They need the high fructose corn syrup, which is killing us all. But that's fine. This is business. And at any rate, this can find problems in the fields and it can do it in a very effective way. So that's a cool thing about that. On the very far left, there is cancer data. And that's what I want to talk to you about today. And why do I want to talk to you about the cancer data? Cancer is a problem that everybody understands is a big deal. We've all lost people. We love the cancer. Okay? Uh, you also all know cancer is a hard problem. So if I show you stuff you never knew about cancer data, I hope that impresses you, that we've seen something that nobody has seen before. Okay? Uh, and so that will be where the focus is. But before I do that, let me give you a little bit more of the story of what you see.

Speaker 1:          13:00          Okay. Because when you see the DQC movie and something like that appears, this is sort of the, uh, the progenitors of the zoo of shapes that, so up in the movie, if the data had little regions on the left, which were obviously separable clusters DQC we'll show you points. The points will be moved together to the centroid of the region. Okay. If the data has something which, let's say I, Ozzy would say, oh, that's a candidate for aggression. And then they call in their favorite regressor and say, please give me a model for this data. DQC We'll show you the line that it would have regressed them. Okay. And I'll, I'll show you how that could possibly happen cause it seems strange and if there is a flare, which people talk about places where the day to join showing you, oh, this has been going on this way for a long time and then two different things can happen and the data ends up in to slowly changing characteristics.

Speaker 1:          14:00          Then DQC we'll show you that and if you'll look carefully there, you're seeing it. Okay. The flares are just completely revealed and it happens automatically. You're not doing anything to extract that from the data. DQC is doing that. And finally finding circles and things, which are the problems everybody likes to talk about. It's a piece of cake. I don't care about it. Okay. It happens when it happens. All right. What's automatic regression? Cause that's a crazy idea. There is a posting on linkedin that I did, which I called Turbo charging, uh, predictive analytics. But what was it really about? It was really about how you could have knocked 200 years off the discovery of the ideal gas law had you had this technology. Okay. Why do I say that? Because Boyle and Charles 18 hundreds of, so they identify Boyle's law in Charles's law. They're looking either at how p and V are related.

Speaker 1:          14:59          If you hold teeth fixed or p and t are related, if you hold the fixed and then it's 200 years later, the Corinthia Dory Says No, no p Times v is Tate. Okay. And getting all that put together and dealing with the statistical problems in the data and everything else. It's a huge problem. So I did this as a fun exercise. I said, suppose I generate 6,000 independent values of p and V and then calculate t and if I was able to plot in three dimensions few hundred years ago, then I would have seen something that looks a lot like the red surface there. I can tell you what that red surfaces, it's a messed up version of what I would have seen. Now. That would be cool because once you see a two dimensional surface, you finished, you've said, oh there's a law of physics because not everyone is a law of physics.

Speaker 1:          15:53          If I'm now teaching in my class. Right. A Law of physics is the statement that not everything you can measure about a system is independent. If you know some of the variables, you can predict the other ones. That's what gives you control. That's what lets you make refrigerators. Okay. Had they just had the surface, they would have been in the position to start the engineering program to build the first refrigerator. Okay? That's all they needed. Okay, so you don't need the formula yet. The surface a look up table, you know, if you were my age, you would remember when science and co-signs, we're not cotton at the computer, but you actually went to look at tables of signed some coast signs and then you interpolated the values to get the number. You want it. Okay? So here you would have seen that right away, all of that power would be at your disposal.

Speaker 1:          16:45          The problem is data is never that good. Data is messy. So what I did was I took down 6,000 points in, generated another 66,000 points by taking each PV and t value and multiplying it by a number chosen from three independent random distribution centered at one with a width of about 0.1. So this 10% error is roughly in that data and the fuzzy blue surface is what you see. Now what's true if it is actually an underlying surface in the data, and I messed it up that way, that random distributions, the high density region is the surface. So what you're seeing is DQC coalescing the data down onto the nearest high density region, which is the surface and then it buries itself in the surface. Now it buries itself in the red surface. The red surface is the best answer I could have gotten because I took every 12 points that I had generated from a single point and I averaged them.

Speaker 1:          17:45          And because of course I have a small set of statistics, the mean isn't the surface, it's a little bit thing. So that bread surfers shows you the degree to which the data is messed up, which would have been improved by having more data. And then the only question is why is it pulling away from the edge in the later frames? By the way, you're looking at a thousand frames. All of the interesting Cathy happens in the first 60 then the thing stops moving. Why did it stop moving? Because the central region has roughly equal density. And so DQC has no reason to move the points around anymore. Okay? It's pulling away from the edges for the same reason the proxy function goes up where there's no data sharply. And so data that's out in that region rolls down and pulls in from the surface and then looks around and says, oh, no problem.

Speaker 1:          18:32          I'll stop right here. Okay. So that's what I mean by automatic regression. So that means basically if there are formulas, unknown relations hidden in your data, what will the QC do? It'll regress the stuff automatically to the formula. Then if there's a variation in the distribution of the data, it'll slowly change. That's why the whole movie matters. First. You see the stuff happen and then it'll slowly change in response to small variations. All right, so now I'm going to go to cancer data and I'm going to try to finish this all in the requisite amount of time. So basically, uh, this is a problem where, uh, I gave a talk at Seti and uh, someone who shall remain nameless in the audience noticed that this was sort of interesting. And he said, I have a biologist friend who would love to talk to you about this and it's on the end.

Speaker 1:          19:30          He said, well, this is a really great idea and I'd like to do stuff with you and collaborate. But first, in order to have my clients believe you, we need to do a problem that biologists will understand. I said, fine. So he went to an Nih database that had 25,000 tumors in it and uh, it had all kinds of DNA sequence information. It had MRN, a expression information for each tumor and a whole bunch of other things. He pulled out 10% of that data and they were five different kinds of tumors. There was a bladder cancer, Glioblastoma, multiforme or we all know what that is now cost. That's what John Mccain has. Okay. Low Grade Gliomas, which are same cell but a much less serious cancer. They're all serious ovarian cancer and thyroid cancer. The colors on the left are the colors I colored the data with. So he would see it.

Speaker 1:          20:26          You're looking at, well, when I first did this, I did the Svd de composition for the 73,800 MRN a jeans I was given. Okay. And I saw that shape, I clustered it, saw that the cancer separated in an obvious way. And from that DQC has an algorithm for pulling out the most important genes for that shape. Having done that, what you're seeing there is exactly the same starting point, uh, but for only the 48 genes that were identified as important and basically what do you see? You immediately see the brain cancer separate like crazy. The glioblastoma multiforme is very different from the low grade Gliomas, the thyroid cancers cluster, but they don't call as to a single disease. I mean, remember the colors are a histologist looked at the cell and said, this is thyroid cancer. And I'm telling you, no, it's not. These are all different thyroid cancers. If I had sensitivity information to specific drugs, all I have to do is recolor that plot. And I say, well, yeah, it's true. Your drug doesn't cure everybody's thyroid cancer, but it cures these siree cancers 100% of the time. Okay. This is the beginning of meeting the promise of precision medicine. Okay. When you can see that these things are not single diseases. Okay, sure. I like questions.

Speaker 1:          22:00          They're SVD axes. Okay. They're linear combinations of genes, okay. Uh, that doesn't mean anything. I mean, I can't, I every point is identifiable all the time as a particular point in the original Dataset. So we can reach in and say, I want to know about these guys and I'll pull out the raw data for you instantly. Sved sved is the Swiss army knife of all this stuff and I'm bringing coals to Newcastle, but I'll say it anyway. Um, all it's doing is finding a good set of coordinates, the plot, the data, the first three dimensions of the ones in which the data has the biggest that extent, the next three. It has less you, if you look at that at the beginning, there's no separation between these guys. They're, the separation occurs very quickly because not of what's going on in the first three dimensions, but what's going on in the other 48 to see everything.

Speaker 1:          22:56          We're searing here, all 48 genes and playing a role, okay? The reason these guys separate so fast and it looks so different from everybody else's, when you reach in and pull out the clusters, you find out instead of expressing 48 genes, they only express 24 48 dimensions. They live in a different place and that's reflected in the clustering in the first three dimensions, okay? So yes, if I wanted a bore, you have silly or maybe excite you, but run out of time, I could plot all 48 dimensions for you, but you're seeing basically what's going on. What I want to focus you on is the red guys and the ovarian cancer guys. The cyan guys are not moving very much, and the first question you'd like me to answer, I hope is why aren't they moving at all? They look like they're so close together.

Speaker 1:          23:45          Well, they're not close together and 48 dimensions. That's why they're very different. Okay. However, let's, uh, the first question you would have asked, right, is what happens if you pull 48 random genes out? Okay. And do the same game, 2,400 tumors, random genes. That's what happens. I don't know if you noticed it. Let's go back. Look very careful at the evolution. It's about 10 frames, emotion and then it all stops. Random data always does that. Random data doesn't cluster in any significant way. That is what DQC shows you when there's nothing to look at in the data. So this is not random genes that are going to do the job. That doesn't mean there's not something amazing going on in a lot of jeans. Okay? Here, just to show you what you can do with the data once you have the evolution. This, because I was challenged, somebody said, well, you have to tell people what the gold standard is doing on this date as how the gold points you see there.

Speaker 1:          24:48          Cause it's the gold standard is t SNI HDB scan when applied to this data, it finds brain cancer. But if you look carefully, it finds equal amounts of cluster in the Glioblastoma. Multiforme or, and in the low grade Gliomas, it's not distinguishing glial cell cancers, one from the other. Okay. And that's true of every cluster that HDB scan finds. So HDB scan can tell thyroid, it can tell ovarian, but it can't tell apart the brain cancers it fails on them. And there's a reason, some of which we understand for why that's happening. And it'll happen in a lot of data sets like this. And the reason is there's much more than one thing happening in this dataset and we have to be able to go through and pull out all the different things that are happening. Okay. But as you see, the competition doesn't do too well on this, whereas DQC separates the glial cancers immediately from one another.

Speaker 1:          25:47          They really are very different. If you average energy and expression and compare them, they're very different. Okay. These are different diseases. However, the thing you should notice, cause it's sort of fun and stuff about it, will be in the paper as it forms to begin with. You see an arch, their intermediate frames. The first thing that happened is there's a stream bed in which you see a low dimension structure that interpolate between Glioblastoma multiforme and low grade Glioma. Understanding that could be very, very important in treatment. Okay. Why is it important that in the beginning of the GLIOBLASTOMAS extended? I know this from personal sad experience. My first wife died of Glioblastoma multiforme and before at the stage where John Mccain is now, the doctors come to you and give you a choice. They say the choices, are you going to have radiation or not have radiation?

Speaker 1:          26:45          Well, it turns out Glioblastoma multiforme or can go one of two ways. When confronted with radiation, it can become much more malignant or it can get a little bit better. We ended up in the bad side of that day, but nobody knows how to tell them apart, but since initially this is an extended structure, the hope would be if we had the radiation information, we could give advice. Are you a candidate for radiation or you're not a candidate for radiation because that's also not completely one disease. There's just a few other little things that we know. We know something about the age of the patient. I have no information about uh, their response to the drugs. And I have no information about their response to radiation, but the Datasat has age. So always in these plots, the left hand side is the original analysis colored by diagnosis, the right hand thing, the red points are people over 40.

Speaker 1:          27:42          The blue points are people under 40. Maybe somebody is equal to 40. I don't know. At any rate, the interesting thing is the Glioblastoma multiforme is mostly people over 40. The low grade gliomas in this dataset or mostly people under 40. Okay. If we look out here, remember these guys didn't cluster well. Well, there's a reason these guys over here are red are older people. There are tumors. I mean why are older people interesting? As you get older, the errors in your genome crop up, the MRAA expression will become more random and therefore they should not look like younger people even for the same quote disease. They don't. And so the reason these guys don't coalesce and the blue ones do is the blue ones are younger people with a less messed up genome but still big variation in the genome. The only other piece I have is the the uh, stage of the cancer.

Speaker 1:          28:42          Okay. Cancer is a stage one, two, three, four, four, a four B. Okay. On this plot, the stage three and four cancers a yellow and red. So it's always the same movie. Oh, I've done has changed the coloring according to the information that I have from sources that have nothing to do with the genome. Well apparently I have nothing to do with the genome. And you notice the guys that I told you where the red and the scion, they ovarian, they don't move well. They also are the ones that are higher stage cancers more messed up genomes. You expect them to be different in 48 dimensions and they are, they don't cluster. These guys do. And this edge out here, the yellow ones are there they are again, the more messed up genomes and they shouldn't cluster with the blue ones and they don't. So this is how you just work your way through the analysis at the first stage. Okay. But then the question comes up, what happens? I mean, I'm just trying to lead you right through the questions as they appear. What happens if I take these 48 genes out? I do. I see anything. Okay. Do you want to vote?

Speaker 1:          29:55          Somebody wanted to take a guess. How am I going to see something? Am I going to see Confetti? I'm I going to see a mess. All right. Nobody wants to guess. That's what I see. Okay. Stunning. A totally different shape has appeared. The 48 genes are out. They were the most dominant ones. And defining the shape, it doesn't mean that went away. There's another shape.

Speaker 1:          30:19          Okay. And here's where I get my comeuppance. So I'll tell you about it. Okay. I looked at this and I said, oh look, that's pretty good. I think what I'm seeing, because these are not really separated, what I'm seeing, and I said this to my colleague, Alex felt this, uh, that, um, I'm seeing the, the ordinary behavior of healthy cells. And Alex said, I don't believe that. He said, I believe what we're seeing is that cancer infiltrates the cell and hijacks the cellular machinery and leaves, fingerprints all over the gene expression of the rest of the genes in the system. And I said, that's ridiculous. I know that the way that thing is going to cluster, here's the Glioblastoma and the others are going to be the same cluster. I have a lot of experience running this stuff. You should never say you understand that 48 dimensions later, the Glioblastoma and the low grade glioma separate from one another.

Speaker 1:          31:22          Beautifully. He's right. I'm wrong. I had a great day. I love being wrong. So if I look down here in dimensions four, five and six, this is what I was telling you. The red guys live in an entirely different place from the same guys. So in fact, we just found out ovarian cancer is not dark. It's not bladder cancer. But it takes other dimensions to see that fact. Okay? So, uh, it's just dangerous to guests. I don't care how much experience you've had. The good thing is I can put all the bias I want it to. What I think is going to happen. TQC does what it wants. That's what it did. That's what we saw. That was the end of the story. All right, so we're at, I'm told I have like 10 minutes left. Uh, is that about right? I promised John that I would pull back the curtain a little bit on the quantum mechanics. That means half of you should go to sleep. Okay. Okay. Uh, I'm sorry about that. Uh, the, I didn't do this to you. Blame him. Okay. Uh,

Speaker 1:          32:34          what do I first want to emphasize quantum mechanics in? This is a trick. I am abusing what I know about quantum theory to do a problem that should be done classically, but it wouldn't be computable if it was done. Classically, I need an approach, which is immensely paralyzable. If I have to do gradient descent to move things down the cliff and there's millions of points and hundreds of dimensions. I don't know how many of you have done gradient descent while you do in machine learning. So, uh, in fact, you know, not so easy to find 800 minima and characterize their shape. Okay, in 500 dimensions. So we need something else we need as the person who taught me quantum mechanics, Bob server would say we need a trick and in this case the trick is quantum mechanics. Okay, now do. I said we're going to create a function that's a proxy for the density of the data and we're going to do that for two reasons.

Speaker 1:          33:34          Then we're going to move the points. Okay, I'm going to move ahead a little bit. So here is what the manual Parson and infinite long time ago, but had you read basic books on data mining or statistical analysis you would have found at Parson in fact invented something called the Parson estimator. He said, for each data point in n dimensions, I'll write a Gaussian a Belker. I'll add them all up and I'll have a function and where there's more data, I should have a maximum and was less data. I should have a minimum. The problem is what you see here, the one parameter and the Gaussian Gamma varies by a few percent, and what you see in terms of maximum minima changes dramatically. The function is exquisitely sensitive to parameters anyway, characterizing lots of Maxima in a function in n dimensions is very, very difficult. So the first step that faced us was, how do we keep this idea but change what we do so that the function we have is not sensitive to changes the parameters.

Speaker 1:          34:42          You can change parameters by 20% and you'll see the same structure. Okay? This function has a lot of Nice properties. The first of it is it's positive everywhere. It's either a zero or positive that makes it a candidate for an Eigen function of a problem in quantum mechanics. Now, if you're a physicist like me, you know that problems in quantum mechanics, when you find the eigen functions of a potential of the force law for what's going on in the problem, wherever there is a maximum in the function or wherever there's a minimum and the potential, there'll be a small maximum of the eigen function. Okay? Or a point of inflection if the parameters are chosen. But the underlying potential will be much more stable representation of what's going on. Cause the reason for those little maxima is the uncertainty principle. It's the kinetic term in that problem, which is washing out the details. So we just faced with a problem. We want to solve the Schrodinger equation, Dell squared, Psi Plus v Time, PSI equals. It can be anything. So zero is perfectly good. That's a difference of a constant in the, and we just want to find the PSI. We know it's a sum of Gaussians, Dell squared, cy, we know how to compute. It's a sum of calciums multiplied by Paloma polynomials.

Speaker 1:          36:07          The even I can do this problem and I'm not great at math. Okay, I take this guy and I move them over here and I divide by this guy. And that's v okay, that's, that's the way we solve the problem. So now we have an analytic form for the potential function. We don't want to have to work with it, but it's a great form. Okay, so from that here I have three Gaussians and 20% variations. The red curve is the potential function. The green curve is the way function. Sigh. Okay. Notice the gas, the potential function has very sharp features even when the Parson estimator has essentially no features. So you can run through a huge range of parameters and nothing changes. You'll find the same set of minimum. Here we're looking at two very eight three very asymmetrical distribution's different numbers of points with as the parameters very, and again this guy is vanishing when this guy has enormously large features that we can exercise off.

Speaker 1:          37:15          So I've made my function. I had done what I promised you. I made my function immensely insensitive to parameters. Okay. And I've made it minima instead of Maxima, which is also cool cause with minimum I can just do gradient descent and find them. I can do gradient us sent, but that's even worse than gradient decent. This is how funny it can be. Okay, this is a 10% or 20% I can't a 20% variation and I'm showing you going from no visible features to what is apparently flat. The potential function has the structure we were looking for and the original function can't see anything. You wouldn't be able to make any money off that function. So I hope I've convinced you. Now we've got a good candidate for a proxy for density. Now we just have to run guys downhill as I promised you. I want to move the points for that.

Speaker 1:          38:12          I'll give you a name. I didn't write it down, so everybody take out your book. What I'm going to use as Erin fests, Erin fests theorem says, if I have a Gaussian sitting in a potential and I evolve it using the Hamiltonian, solve the Schrodinger equation for the motion of that Gaussian, the center of the Gaussian will move according to Newton's laws and the potential. If I don't go too far, okay? That's the heart of this trick. Okay? The center follows Newton's laws, but the Gaussian can be, but the Hamiltonian can be reduced if I choose a basis to a matrix. And so exponentiate the Gals, the Hamiltonian and moving things is matrix multiplication. And I can do that on every single data point. You give me a million cores and I'll do it totally in parallel. And so a million guys, we'll move delta t in one second.

Speaker 1:          39:12          Okay, now I never have a million is at my disposal, but if I had it, that would be a book you could do. Okay. And so where are we? Because you're geeks. This is a pseudo code for the algorithm. Okay. Step one, read reading data from a spreadsheet. I want Rosenthal, okay. To do SVD and make the first frame of the movie single DQC command. Make first, first trajectory, boom from the data that you did SVD on. Then you have to choose a basis. Now what am I mean by choosing a basis? So this is a long story and I'm not going to give you all the gory details, but basically all of these Gaussians I vectors and Hilbert's space, right? Everybody who, who's going to follow me from the rest of this point? How many people know of vectors in Hilbert space? Are All of you, none of you.

Speaker 1:          40:03          This is going deep. Okay, I'm already in trouble. Okay? Uh, they're vectors. They're independent, they're not independent. In fact, there's a look like this. So the question always is, how many vectors do I need to create all the other vectors as linear combinations of those vectors? The more important question is, if I only want to be accurate to 70% how many vectors do I need in order to get 70% coverage of the data that I can reconstruct everybody and get that right at that level. That's how you pick a basis. Okay. I mean it's a story about how you pick a basis, but that's it. Then you just have to evaluate in this basis the kinetic term, the potential term. And you, you try to keep the number of actors in the basis by adjusting gamma. So you don't need more than 1500 2000 vectors. Cause I don't want to be dealing with big, major cs.

Speaker 1:          40:55          And so by adjusting gamma, I can get very good coverage for a fixed number of actors. And by the way, that's an algorithm for picking gamma for the problem. So that takes my freedom of choice out. Now I can only say who I want, 60% coverage of 50% coverage and nothing changes very much. So I don't need to pick gamma. And then I built a Hamiltonian, I have to exponentiate that matrix, eat to a minus, I times a timestamp times the Hamiltonian. And then I just go around multiplying things. And every time I multiply I find where the centers of the packets are and I started again and I keep doing that. And so after that it's just a loop. At the end of that loop, I have a movie and then I just show you the movie. So what's going on is serious underneath complicated stuff.

Speaker 1:          41:45          But the movie shows you everything that's happening. We don't need to know what's happening, we only have to know what it's telling me. And it's telling me where the higher density locally, higher density regions are. And so now the fun is you just go off and do whatever else you're interested in. Cause you've seen the structure of the data. It's told you where the whales are hidden in the data. Okay, you're going to go down and do that. And so Dq sees benefits are basically defined by the three nos. No cleaning, no hypothesis, no expert knowledge. I hope I've convinced you it lets you find hidden unexpected insights and the big dense data. Even more so when there's multiple things going on in the data, it pulls out the levels of information just by following your nose. I mean nobody expected this data to have stuff going on in tens of thousands of genes.

Speaker 1:          42:41          But as you keep pulling more and more genes that you keep seeing that other pot, the second pattern that means tens of thousands of genes have been polluted by the cancer and just it's, which tells you right away, it's really easy to convince yourself that you can diagnose based on information that has nothing to do with the cost. Okay? You have to know the data has this so that you can go in and see which is more important because the information is in tens of thousands of genes. It's not in a few genes. Okay? So this is a way of looking at this kind of data. My colleague hasn't seen I who don't know much about this kind of, I mean, the neat thing about this, why do I love this? I get to be what I always wanted to be the consummate Dillard time. I get to work on a problem not knowing what it is.

Speaker 1:          43:31          I loaded in the data, I see something my friend comes and says, that's interesting. And then we start talking. And so I only learned about the problem, what I discovered in the data. Okay. And I get to work on in field after field after field. And it's why the answer to the question, why aren't other people using this? So I'm having such a good time, I'm not doing a good job of telling the story. And Burton Richter actually, who's one of my, uh, so he's a Nobel prize winning physicist. He yells at me all the time, he says, you're having too good a time.

Speaker 1:          44:05          He's on my board and he yells at me, you are having to go to time. You're not out there getting this into the public. So now I am engaged, uh, in at any rate, the insights are there and they're stranger than you might have guessed, which leads me to quote hamlet that there were more things and will therefore as a stranger give it welcomed. There are more things in heaven and earth, Horatio then I dreamt of in your philosophy. Okay. Datasets have more to say than you ever guessed they could have. And there is a tool that can pull the data apart in a way which lets you see this is information about the company. That's my information. You can get to me and we can have a conversation. Okay. I'm done.

Speaker 3:          45:02          Hi. The first question that was submitted beforehand is what are the most promising applications for your technique and how should Googlers think about potentially using it? I think you answered a little bit of the first one, but, um,

Speaker 1:          45:14          I answer to that. Yeah, it has to do with I'm a company now and I've been told by people not to do certain things. So the truth is we're not telling anybody this, right? So, uh, I don't care what the data comes from. I did, I don't use any knowledge of the data so you can imagine what it's good for. However, I have to tell you, I'm really interested in pursuing one vertical market because otherwise my advisers will tell me I'm doing a terrible thing. So while I'm pursuing a single vertical, my first thing frankly is I'd love to, if there is a market there, look at a bio, medicine and Pharma because I think I showed you that we have the potential to do precision medicine and then I can solve my dream of saving lives. Okay. Which is what I'd really like to do.

Speaker 1:          46:08          Uh, I have other examples and I welcome people who want to have a discussion. I can stay here till five o'clock. Okay. Uh, there are examples of the supply to market segmentation. Examples applied to dealing with very noisy data like finding contraband, nuclear material, driving around the city of Chicago with a lousy handheld detector. Um, there are examples with hyperspectral camera information. There are, so you tell me what it's good for. I leave it in your lap. A Googlers at the moment. How can they think about how to apply it? Talk to me. I mean it, it'll take some talking. Look, I'm telling you to do something that already violates everything that everybody's ever told you about looking at data, right? Uh, I'm telling you, don't clean your data. If you try to clean your data, I'm gonna slap your hand. It's not what you learn and use statistics course.

Speaker 1:          47:01          Okay? The next thing I'm telling you is don't make a hypothesis because you were an idiot. You don't know what you're asking. Okay? You're biasing the result. I mean, I have people who hate me for saying this, but that's the best way to take a first look at data. The other stuff comes in when you've understood what's in your data, when it's shown you what it has to say to you, what it's told you, which story. It has a story to tell, but you have to let it have a voice. Okay. And this is the way to let it have a voice. There may be other ways to let it have a voice. I just don't know them. Okay. At anyway. I love this one. It's my baby. Oh, quite freely admit to bias. Okay. But uh, I guess that's the answer to the question. And anybody who wants to talk to me, I'm more than happy to talk about it.

Speaker 4:          47:47          In the example of you have dealt with the cancer cells, it did you plot the also healthy cells?

Speaker 1:          47:53          I wish I knew that. Right. We only had six cells. However you saw I had a hypothesis. It was the healthy cell data, but I could nevertheless less prove I was wrong by only looking at the six cell information. Took me a little while to realize all I had to do was evolve it and see what would happen. And if they separated, it wasn't healthy cell inflammation because glioblastoma and low grade gliomas are the same cell. I couldn't tell from the other cells, but those two were an indicator. If they separated, the information was different for those two diseases and therefore it wasn't healthy cell information. It was pollution of the genome by the cancer. I wish I had the information. I am now looking for people with datasets that have medical information that, I mean, all I have is the histologists said this is thyroid cancer.

Speaker 1:          48:49          Great. So the histologists, it's a 140 different cancers. Okay. Even the Glioblastoma, which is a pretty tight cluster is still quite a while and extended structure. It's not one cancer. Okay. The low grade Gliomas for sure or not. And there's a connection between the mid early stages. Okay. So what is all of that damn define though? Is there more in this data that I didn't tell you that? Yes, but I'm outside of my wheelhouse. If I talk about it, it's deeply biological information. I don't understand it. My colleague tells me he doesn't understand that either, but the biologists so sure. To be interested in it because nobody's ever seen that before. Okay. So, uh, it, it just, it's so much fun to take a data set of park this way. I mean your notice I never calculated a thing. I just made movies. Yes.

Speaker 4:          49:49          As far as I understand you're creating this wave function, uh, uh, treating those data points as probability distribution of [inaudible].

Speaker 1:          50:00          They're just functions that are peaked at the data and then I'm adding them up and I'm hoping that the resulting function has maxima where there's more data and local minima where there's less data. So I'm plotting the density. If you start getting into probability questions and you try to understand this in some profound way about probability amplitudes, right? You're heading down a rabbit hole that you shouldn't be going down. Okay. It's a trick. This is normous problem to me because people love quantum mechanics. It's, it's, I love quantum mechanics. It's fabulous. But I'm abusing corner mechanics. I am taking what I know from quantum mechanics and all the years I spent knowing how to solve problems, not perturber differently. Okay. And I'm stealing them to do a problem, which is like, it's not 100% light gradient descent. You really want to be mystified a little bit.

Speaker 1:          50:57          Why can I work in a hundred or a thousand dimensions when other people fail in 10? Well, I happen to know why they fail. The reason they fail is if you would have formed this probability this, uh, so you've got me doing it, shame on you. At any rate, uh, if you were to take this, uh, potential function and look at it under a microscope, you would see that little ripples form because as the dimension goes up, statistical small statistical fluctuations start to happen a lot. Okay? That's what everybody gets stuck in. If they try doing kneeling or something like that, they find too many clusters. I want to see only the big shape then envelops that whole thing. So I didn't tell you about the other parameter in the problem. The other parameter which was set to one when I made the potential function is the mass of the particle. There's moving quantum mechanics. That's a wonderful thing called tunneling. If I make that mass lower than one like 0.1, it's not going to see the little minimum. They vanish it tunnels through them. So in fact you only CVN developing big potential and so I can work. I have worked in 1,024 dementia. Well it's two things are happening as the mask will go offline. Okay. Not Fair to other people for us to get a yes, but sometimes it just goes through.

Speaker 3:          52:26          Um, another question from the Dory. Early quantum computers won't have many cubits are long coherence times how you get big data sets into the machine before the cubits. Decohere

Speaker 1:          52:36          I'm not doing quantum computing, I'm doing classical computing. I don't need to answer that question. Next question.

Speaker 3:          52:49          Uh, what have you found DQC is more effective than other ML techniques? Have you found heuristics as to why that is? I think you also answered

Speaker 1:          52:56          a little bit. I tried to answer that all through this thing. I mean, I can say it again. So not only don't I care about the data, I don't care about the size of the Dataset. Okay. If you run this on small data sets, which it was done. Okay. There's something that people use for the small datasets called as your card score. I think some of you are familiar with that. I don't like it, but it's the main figure of mirror. So applied to small datasets, Nicu seizure card scores run from 0.8 seventh the 0.96 when I spoke to somebody at Hanford labs, he was a spook and he said, what kind of is your card scores? These you said? I said, I don't do that. Great. I do 0.8 6.9 he said I kill for 0.86. Okay. Uh, so we do very well, but why am I interested in small data sets and know why?

Speaker 1:          53:49          Small datasets, anybody can get an answer on when a Dataset has no separations in it. When you plot it earned him many methods that can touch that problem. Okay. The HDD scans as it can touch it and only works up to 10 dimensions. Number one and it degrades badly and 10 dimensions, but number two, you also saw kids the wrong answer for reasons having to do with the data set. It's complex and more than one thing that's going on, it doesn't know what he's looking for. DQC somehow avoids that. It's hidden in the density of the data in the high dimensions. Okay. I mean it just past that point, I already proved to you, I can visualize the data in 48 dimensions that tell you how the answer is going to come out. Thought I did, but I was wrong. Nice thing is I can't make the QC do what I want. It'll do what it's going to do.

Speaker 1:          54:41          I have a question. So if I have a large, a high dimensional data set, is the other use cases where you would not recommend to QC or I was killing these guys over lunch? I love that question. So I'm a physicist. I believe everything has a failure mode. Okay. Absolutely. Believe that with my gut, I spent five years trying to find the Dataset it doesn't work on. Okay. I thought I had it. Okay. I thought I had an Alzheimer's Dataset, which somebody gave me and it was 1500 Alzheimer's pay or 3000 I don't remember. And it was 250,000 snips. And they said, so the information we want to know, can you see people who would have laid out onset Alzheimer's and people who won't based on these snips? And so I ran the data, so I dunno where they ran it in, but when I looked, it was something like a hundred 150 dimensions was appropriate.

Speaker 1:          55:42          I could get reconstruct the whole Dataset to the 1% level by that. So there's no point in running and 250,000 dimensions. Okay, I'll run in a hundred and something. I saw lots of stuff. Unfortunately what I saw is what 23 [inaudible] or ancestry.com I saw what was probably, I didn't have the information was most certainly, uh, data that had to do with ethnicity. Okay. But when I looked at all of the clusters that I had, and I calculated the number of people who had, had been autopsied and shown to have Alzheimer's and the number that were autopsied insurance, so they own, unfortunately, they were all dead and I hope, and uh, they were shown to have Alzheimer's and not have Alzheimer's. The distribution in the general population and the distribution in the clusters up to 1% variations, it's the same. So finally, after knocking myself out, trying to vary the parameters I have, and of course I couldn't get a different answer. I called the guys up and said, I'm sorry you say this stuff there, I don't see a thing. There's just nothing else. I'm is related in your data. I think eva talked to the proteomics group at Stanford Hospital and they asked me this wonderful question, what's your failure mode? And I said, I failed a Bismal Leon [inaudible] data. And, and I was proud I had a failure mode. And he said, uh, I know the Dataset you just talked about. We worked on it for 10 years. There's nothing in that data center.

Speaker 1:          57:17          So, you know, my week spent looking at the Dataset four times was not in the end wasted, but I still don't know my thallium I'm sorry. I wish I knew it. No, I it has to be there. You can't work on everything. Nothing works on everything. I just haven't gotten unlucky or lucky. I don't know which it would be, but it would be nice to know what the limitations are. Density variations is such a robust thing. It's such a different way of looking at the data that it's hard to see when it fails.

Speaker 5:          57:50          Thank you so much.

Speaker 3:          57:53          Um, we were talking maybe for one more question. There's any live, we can also get one from the dory.

Speaker 5:          58:01          MMM.

Speaker 3:          58:03          Oh, this was sort of just ask, but what are the downsides? DQC Why isn't everyone using it right now? I guess this is part of, it's my fault.

Speaker 1:          58:13          My wife was sitting here, so I don't want to say it again. She'll yell at me. It's my fault. In some sense it's my fault. Okay. Uh, I don't think I've been the best spokesman for recruiting young people to jump on the bandwagon and start doing it cause I've been having too good at time using it and, and I guess that's been my problem through life. Okay. I mean, I always got yelled at by the head of the lab that you got to write another paper you're not sharing. He says you are having too good a time knowing what we don't know and you're not sharing. And uh, probably I don't share enough. Right. So you invited me and I'm sharing. Thank you.

Speaker 5:          58:56          Wow. Well

Speaker 1:          58:57          thank you for sitting through it.