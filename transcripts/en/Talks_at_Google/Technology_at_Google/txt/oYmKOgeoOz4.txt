Speaker 1:          00:00:09       Hello. My name is John [inaudible]. I work at verily, which is Google's life sciences company. I also lead a group called the singularity network, which is an internal organization, a competitive more than 3000 Googlers focused on topics about the future of artificial intelligence for which we are here today. And it's my pleasure to be here today with Dr Max Tegmark. As a brief introduction, Max Tegmark is a renowned scientific communicator and cosmologists and he's accepted donations from Elon Musk to investigate the existential risk, but Moondance artificial intelligence because research interests include consciousness, the multiverse advanced risk from Ai and formulating an ultimate ensemble theory of everything. Uh, Max was elected fellow of the American Physics Society in 2012 one scientist ms science magazines breakthrough of the year in 2003 and has written over 200 publications, nine of which have incited more than 500 times a Max Tegmark everyone

Speaker 2:          00:01:08       [inaudible]

Speaker 3:          00:01:13       he said it's a really great honor to be back here at Google and they get to talk in front though so many old friends and it's so much human level intelligence and idealism. Does anyone recognize

Speaker 4:          00:01:30       this? Was, this was of course, the Apollo 11 mission, the put Neil Armstrong, Buzz Aldrin, and Michael Collins on the moon was not only successful, but it was very inspiring because it showed that when we do is manage the technology wisely, our ancestors drama, right? Well,

Speaker 3:          00:02:00       the rest of the important lessons, I think we can learn from this as well. So I want to devote the rest of this talk to and other journey powered by something much more powerful than rocket engines where the passengers are not just three astronauts, but all of humanity. So let's talk about our collective journey into the future with Ai. My friend Jaan Tallinn likes to emphasize that just as with the rocketry, it's not enough to just make our technology powerful. We also have to focus on figuring out how to control it and on figuring out where we want to go with it. And that's where we can talk about, I think the opportunities are just so awesome if we get this right. During the past 13.8 billion years, our universe has transformed from dead and boring, too complex and interesting and it has the opportunity to get dramatically more interesting in the future if we don't screw up.

Speaker 3:          00:03:05       Oh for about 4 billion years ago, life first appeared here on earth, but it was pretty dumb stuff like bacteria. They couldn't really learn anything in their lifetime. I call that life 1.0 we are what I call life 2.0 because we can learn things, which of course in Geek speak means we can upload new software modules. If I want to learn Spanish, I can study Spanish and now I have all these new skills uploaded than I mine and it's precisely this ability of as humans to design their own software rather than be stuck with whatever the software evolution gave us, which has enabled us to dominate this earth and give us what we call cultural evolution were seemed to be gradually heading towards life 3.0 where we can, where, which is life that can design not just its software but also it's hardware. Maybe we're at 2.1 right now cause we can get cochlear implants and artificial knees and a few minor things like this.

Speaker 3:          00:04:05       But if you were a robots that are able to think is cleverly is right now of course there will be no limits whatsoever to how you could upgrade yourselves. So let's first talk about the power of the technology. Obviously the power of AI has improved dramatically recently. I'm going to define intelligence itself just very broadly as the ability to accomplish complex goals. I'm giving such a broad definition because I want to be really inclusive and include both all forms of biological intelligence and all forms of of artificial intelligence. And as you guys here at Google all know, obviously a subset of artificial intelligence is machine learning where systems can get, can improve themselves by using data from their environment, much like biological organisms can. And another subset of that is of course deep learning, which is where you use neural net architectures. And if you look at older breakthroughs in AI, like when Garry Kasparov, I've got his posterior kicked by IBM's deep blue, the intelligence here was of course mainly just put in by human programmers and deep blue beat Kasparov just because it could think faster and remember better.

Speaker 3:          00:05:18       Whereas in contrast, the recent stuff that you've done here at Google, like this work by ets suits givers group, there's almost no intelligence at all put in by the humans. They just trained the simple neuron feeds with, with uh, a bunch of data and you put in the numbers that represent the pixel colors and it puts out this caption, a group of young people playing game of Frisbee. Even though the software was never told any total taught anything about what a Frisbee is or what a human is or what the picture is. And the same stuff you put in other images gives other captions, which are often quite impressive. I find it even more striking how cool things can be done with video. So this is Google deep mind of course, learning to play an Atari Games. And, um, for those of you, those few of you who haven't seen this before, you need to remember that this neural network here with, with simple reinforcement learning built in, had no idea what a game was with a paddle was when a ball was or anything like that.

Speaker 3:          00:06:25       And just by practicing gradually starts to miss the ball as often and got the point where it hardly missed it all and play as much better than I, I could play this. And, and the, the real kicker is it of course the people at deep mind, they actually didn't know that, uh, there was this clever trick you can do when you play breakout, which is always aim for the corners and try to build a little, do a little tunnel there. So, so once this a little deep learning software, figure that out. It's just every single time the ball comes back, look how just uncannily precise it is just putting it right back there and the corner and playing. I can only dream to play this. Well now this is of course, it's very, very simple environment, that little two dimensional game world. But if you're a robot lie, you can think of life as a game, just a more complex one. And you could ask, you should ask yourself to what extent these sorts of techniques might enable you to learn more interesting things and deep mind more recently had three dimensional robots in a simulated world and that just asked to see if they could learn to do things like walk. And this is what happened.

Speaker 3:          00:07:53       This software had never ever seen any videos of walking. You knew nothing about the context, the concept of walking, right? All that the software was doing was sending random commands, the how to bend the different joints and it got rewarded every time. This treats them manage the move a little bit forward and it looks a bit funky, maybe a little bit awkward, but I actually learns, learns, interesting stuff. So this raises this very interesting question. Okay, how far can AI go? How much of what we humans can do, will machines ultimately be able to do if we use, not just the techniques that we know of so far but factory and all sorts of additional progress that you, people in the room and elsewhere are going to do. I like to think about this in terms of this landscape. I drew this, you've made this picture inspired by uh, a paragraph and the, one of my favorite books by Hans more Avec for many years ago where the height here represents how difficult is for a computer to do a certain task and the sea level represents how good computers are doing them right now.

Speaker 3:          00:09:01       So what we see here is that certain tasks like the chess playing and arithmetics have of course long been submerged by the slowly rising tide of, of machine intelligence. And there, there are some people who think that there are certain tasks like art and book writing or whatever, that machines will never be able to do. And then there are others who think that, uh, the old goal of AI to really solve intelligence and do everything that we do will mean that sea levels will eventually submerge everything. So, so what's going to happen, there've been a lot of interesting polls of site of Ai researchers and the conclusion is very clear. We don't know the more specifically though, what you find is there are some people in the techno skeptic camps who think that AI researchers ultimately doomed. We're never going to get there or maybe we're only going to get there hundreds of years from now.

Speaker 3:          00:09:50       Um, but actually most AI researchers think it's going to happen and more a matter of decades. And, uh, some people think that we don't have to worry so much about steering this rocket's metaphorically speaking because it's not going to happen. There will ever get powerful enough that we have to worry about this. But, but that's a minority. And, and then there are people who think we don't have to worry about steering because we're guaranteed that their outcome is going to be awesome. I call it such people digital utopians. And I respect that point of view. And there are also people who think it's guaranteed the things are going to suck. So there's no point in worrying about steering cause we're screwed anyway. But most of the people in surveys tend to land or here in the middle and what I've called the beneficial AI movement where you're really motivated actually to ask what can we do right now to steer things in a good direction because it could be awesome or it could be not so great.

Speaker 3:          00:10:44       And it depends on what we do. Now. Uh, I put this webpage up, age of AI to work and we did a survey there for people from the general public can answer these same questions and you can go there and do it too. And I was actually very interested that the general public responds almost exactly the same as AI researchers have done in recent polls. This is from something I analyze this weekend with 14,866 respondents. And you see, most people think maybe we're decades away from human level Ai. Maybe it will be good and maybe there'll be problems. So this is maximumly motivating to think about how we can steer this technology in a good direction. So let's talk about steering. How can we control, how can we learn to control AI to do what we wanted to do to help with this? Um, my wife may out who's sitting there and I and some other folks that founded the future of life institute.

Speaker 3:          00:11:38       And you can see we actually have the word steer up here in a mission statement. Our goal is simply to do what we can to help make sure that technology is beneficial for humanity. And I'm quite optimistic that we can create an really inspiring future with technology as long as we win this race between the growing power of the technology and the growing wisdom with which we manage it. But, um, I think if we're going to win this race, we actually have to shift strategies because technology is gradually getting more powerful. And when we invented less powerful tech like fire, we very successful use the strategy of learning from mistakes and entered fire oopsy and then went the fire extinguisher. We invented the car oopsy and then we invented the seatbelt. The airbag, the traffic light and things were more or less fine, but when you get beyond the certain point that the power of the technology, this idea of learning from mistakes, it's just really, really lousy.

Speaker 3:          00:12:37       Right? You don't want to make mistakes if one mistake is unacceptably many and when we get, when talking about nuclear weapons, synthetic biology and certainly super human AI, I feel we're at that point where we really don't want to. Mistakes make mistakes. We're going to shift strategy from being reactive to being proactive, which is exactly the slogan you said. You're also using it for your work here at Google earlier and the, I'm optimistic that we can do this if you really focused on it and I worked for it. Some people say, Nah, don't, don't talk about this because it's just luddite. Scaremongering when you're talking about things that could go wrong, I don't think it's luddite scaremongering. I think it's safety engineering. We started by talking about the Apollo Moon mission. You know when NASA thought through very carefully everything that could possibly go wrong. When you put three astronauts on top of this 100 meter tall rocket full of, of highly explosive fuel that wasn't luddite scaremongering.

Speaker 3:          00:13:39       What they were doing was precisely what ultimately led to the success of the mission. And this is where I think we want to be doing where the eye as well. I think so far what we've learned from other technologies here is that we need to up our game a little bit because we haven't really absorbed this idea that we have to switch to being uh, being, being proactive. Today is a very special day in terms of nuclear weapons because we came pretty close to September 26 being the 34th anniversary of world war three. In fact, um, it might have ended up this way if this guy's Stanislav Petrov hadn't just on gut instinct, there's ignore the fact that his early warning system said that there were five incoming minuteman us missiles there. It should be retaliated against. So how can we do better? How can we win this wisdom race?

Speaker 3:          00:14:31       I'm very, very happy that the AI community has really started to engage with these issues a lot in recent years. And um, thanks to a lot of people who are in this room here included Peter Norvig and where the future life is due to organized a couple of conferences and Puerto Rico and then in this year, earlier this year in the Sylmar California where the, it's really quite remarkable consensus around a number of very constructive things that we can do in terms of try to develop this wisdom and steer things in the right direction. And I want to spend just a little bit of tough time hitting some highlights of things here from this list of 23 Sylmar principles, which has now been signed by over a thousand AI researchers around the world. First of all, people, first of all there was, it says here on item one that we should define the goal of it, of Ai Research.

Speaker 3:          00:15:21       Not to be just making undirected intelligence but to make beneficial intelligence. So in other words, the steering of the rocket is part of the design specs, but and, and then there was also a very strong consensus that hey, we, if we have a bunch of unanswered questions that we need to answer, we shouldn't just say, Oh yeah, we should answer them well. We should answer them the way we scientifically know is the best way to answer hard questions. Then we to research them to work on them. Right. And we should fund this kind of research is just an integral part of computer science funding both in companies and an industry. I'm actually very, very proud of Google for being one of the founding members of the partnership on Ai, which aims very much to support this kind of AI research, AI safety research and other and other principle here that was very broad agreement was the shared prosperity principle that the economic prosperity created by AI should be shared broadly to benefit all of humanity.

Speaker 3:          00:16:25       What do I mean by that? Obviously technology has kept growing the economic pie. It's been growing our GDP. I'll opt in recent decades as you can see. If you look at the top line and this, this plot here, but as you're also generally aware of, this pie hasn't been divvied up quite equally. And in fact, if you look at the bottom 90% of income earners, their income has stayed flat almost since I was born. Actually. Maybe it's my fault. Um, and the 30% poorest in the US have actually gotten significantly poorer in real terms in recent decades. There's just created a great deal of anger, which has given us the election of Donald Trump. It's given us Brexit then has given us a more polarized society in general. And there's so, there was a very strong consensus amount among AI researchers, you know, if we can create so much more wealth and prosperity and have machines help produce all these wonderful goods and services, then if we can't make sure everybody gets better off from this, you know, shame on us. Some people say, well, this is just nonsense because something magical is going to change in these statistics soon. And the jobs that get automated away are going to be replaced by much better new jobs that don't exist yet.

Speaker 3:          00:17:43       But actually if you look at this data, it doesn't support that. We could have made that same argument to a hundred years ago when much more people worked in farming that all those jobs, they weren't in loss. We're going to replay, be replaced by new jobs that didn't exist yet. And this is what actually happened. This is, I made this little pie chart here of all the jobs in the u s by size and you can start going down list managers, drivers, retail, salespersons, cashiers, et cetera. Only when you get down to 21st place, you get to a job category that didn't exist a hundred years ago, namely software developers. Hi Guys. Um, so clearly what happened is not that most farmers became software developers. What instead happened with was people who lost generally from the industrial revolution and onward jobs where they were using their muscles to do work, went into other jobs where they could use their brains to the work and these jobs kind of to be better paid.

Speaker 3:          00:18:36       So this was a net win, but they were jobs that already existed before. Now what's happening today, which is driving the growth and income inequality is similarly that people are getting switched into other jobs that had existed before. It's just that this time since the jobs that are being automated away are largely jobs where they use their brains, they often switch to new jobs that existed before that pay less around and pay more. And I think it's really interesting challenge for all of us to think about how we can best make sure that this growing pie just makes everybody better off. Uh, and other item here on, on this list is principle number 18 to AI AI arms race. This was the one that had the highest agreement of all among the Sylmar participants.

Speaker 3:          00:19:30       An arms race in lethal autonomous weapons should be avoided. Why is that? Well, first of all, we're not talking about drones where, which are remote controlled vehicles where human is still deciding who to kill. We're talking here about about systems where the machine itself using machine learning or whatever decides exactly who is going to be killed. And then does, does the killing and um, first whatever you think about them, the fact is there's all other, has been of course a huge amount of investment in civilian uses of AI recently. It's actually Dorf by Tom talk about military spending here recently. So if you look in the pie, we, there's a real risk that the status quo will just mean that most of this loud sucking noise trying to recruit AI graduates from MIT and Stanford and elsewhere will be to go to military places rather than to places like Google and most Irish or should felt that that would be a great shame.

Speaker 3:          00:20:27       Here's how I think about it. If you look at any science, right, you can always use it to develop new ways of helping people or new ways of harming people. And the biologists fought really, really hard to make sure that their science is now known as new ways of curing people rather than for biological weapons. They fought very hard and they got an international ban on biological weapons past. Similarly, chemists managed to get the chemical weapons band. But by really speaking up as a community and persuading politicians around the world that this was good. And that's why you associate chemistry now mainly with new materials and it's very stigmatized to have bio weapons. So even if some countries cheat on them, it's so stigmatized that Assad even gave up as chemical weapons not get invaded. And if you want to buy some chemical weapons to uh, do something silly, you're going to find it really hard to find anyone who's going to sell them to you because it's so stigmatized.

Speaker 3:          00:21:24       And what the, what there's a very widespread support for in the Ai community is exactly the same thing here to try to negotiate an international treaty where the superpowers get together and say, hey, you know, the main winners of having an out of control arms race and AI weapons is not going to be the superpower as it's going to be isis and everybody else who can't afford expensive weapons. But we'd love to have a little cheap things that they can use to assassinate anybody with anonymously and basically drive the cost of a anonymous assassination down to zero. And this is something if you want to get involved in the United Nations, this is going to discuss this in November actually, and I think the more vocal the Ai community is on this issue, the more likely it is that that AI rocket here is going to veer in the same direction as the biology and chemistry rocket went.

Speaker 3:          00:22:16       Finally, let me say a little bit about the final SLR principles here. I find it really, really remarkable that even though a few years ago, if you started talking about superintelligence or existential risks or whatever, many people will dismiss you as some sort of clueless. A person who didn't know anything about Ai. These words are in here and yet this is signed by you, is a Saba, is the CEO of deep mind. It's, it's, it's um, signed by Peter Norvig who's just sitting over there by your very own Jeff Dean and by who really a WHO's who of of Ai researchers, over a thousand of them. So there's been a much greater acceptance of the fact that, hey, yeah, this is part of steering. If maybe AI is actually going to succeed. And maybe we need to take these sort of things into account. Let me just unpack a little bit what the deal is with all of this.

Speaker 3:          00:23:15       So, so first of all, why should we take it seriously at all? This idea of recursive self improvement and superintelligence we saw them, a lot of people expect we can get the human level AI and a few decades. But why would that mean that maybe we can get a, I'm much smarter than us, not just the little, the basic argument for this is very eloquently summarized and just this paragraph by Ij good from from 1965 and a mathematician who worked with Alan Turing to codes during World War Two and you mean, I think you're mostly heard this all before basically says that if we have a computer that a machine that can do everything as well as as we can, well one of the things we can do is design a AI systems. So then it can too. And then you can hire, instead of hiring 20,000 Google employees that work for, you can get 20 million little AI things working for you.

Speaker 3:          00:24:08       And they can work much faster. And the, the speed of, of Ai Development will no longer be set by the typical r and d timescale of humans a years. But by how fast machines can help you do this, which could be way, way faster. And that if it turns out that, um, that we have a, uh, hardware overhang where, where we've compensated for the fact that we really are kind of clueless about how to do the software, have human level AI by having massive amounts of extra hardware. Then it might be that you can get a lot of it through improvements first even. But just by changing the software, which is something that the, that, um, should be done very, very quickly by without even everything, build new stuff. And then from there on, things could get them. You might be able to get machines that are just dramatically larger, dramatically smarter than us.

Speaker 3:          00:24:59       We don't know this, this will happen, but peep. But basically what we see here is that poor linea researchers are viewing this is at least a possibility that we should, that we should take seriously. And other thing which you see here is existential risk. It's a more specifically, it says your risks posed by AI systems, especially existential risks as much to be subject to the planning and mitigation efforts commensurate with their expected impact. Next essential risk is the risk, which basically could include the humanity just getting wiped out all together. Why would you possibly worry about that? There are so many absolutely ridiculous Hollywood movies with Terminator Robots or whatever. You can't even watch like cringing. So what are the seaweeds reasons that people like this sign onto something that talks about that? Well, a common criticism that you hear is it. Well, you know, machines, there's no reason to think that intelligent machines would have human goals if we built them. And after all this, why should they have sort of weird alpha male goals of trying to get power or even self preservation? You know, my laptop doesn't protest when I tried to switch it off. Right?

Speaker 3:          00:26:13       But there's a very interesting argument here. I just want to share either in the form of this silly little fake computer game I drew for you here. Just imagine that you are this little blue friendly robot whose only goal is to save as many sheep as possible from the big bad wolf. Okay? You don't care. You have not put into this. This robot does not have the goal of surviving or getting resources or any stuff like that. Just sheep saving. It's all about these cute little cheapies. Okay. Um, it's gonna very thing if it's smart or figure out that if it walks into the bomb here and blows up, then it's not going to save any shape at all. So in a sub goal that it will derive is, well actually that's not get blown up and it's going to get a self preservation instinct.

Speaker 3:          00:26:57       This is a very generic inclusion. If you program, if you have a robot, then you program it to walk to the supermarket and buy your food and Cook you a nice dinner, you know, it's going to, again, develop the sub goal of self preservation because if it gets mugged and murdered on the way back with your food, it's going to not give you your dinner. So it's going to want to somehow avoid that. Right? Um, self preservation and there's an emergent goal of almost any goal that the machine might have because goals are hard to accomplish when you're broken. Uh, and also, uh, if, if the robot, the robot might develop finding, having an incentive to get a better model of the world it's in here and discover that there's actually a shortcut it can take to get to the, where the sheep are faster than it can save more.

Speaker 3:          00:27:42       So trying to understand more about how the world works is a natural sub goal you can get no matter whatever fundamental goal you program the machine to have. And then resource acquisition to can emerge. Because when this little row bought here, discovers that when a drink the potion, it can run twice as fast than it can save more sheeps. It's going to want the potion. It'll discover they want it takes the gun that can shoot the wolf and save all the sheep. Great. Um, this is going to want to have resources. And I've summarized in this pyramid here, this idea, which has been a very eloquently was mentioned first by Steve Omohundro who lives here in the area and just talked a lot about and Nick Bostrom's book, the idea is just the whatever fundamental goal you give a very intelligent machine. If it's pretty open ended, it's pretty natural to expect that it might develop sub goals of, of not wanting to be switched off and try to get resources. And that can be fine. There's no, not necessarily a problem being in the presence of more intelligent entities. We all did that as kids, right? With our parents. The reason it was fine was cause their goals were aligned with our goals. So there in lies the rub. We want to make sure that if we ever give a lot of power to machines of intelligence, he comparable or greater to ours, their goals are aligned with ours, otherwise we can be in trouble.

Speaker 3:          00:29:04       So to summarize, these are all the questions that we need to answer. Technical research questions. How can you make, how can you, how machines learn, uh, retain retainer goals, for example. And let me just show you a very short video up talking about these issues and superintelligence immense them. Let's see if we have better luck with video this time.

Speaker 2:          00:29:32       Okay.

Speaker 5:          00:29:33       Will artificial intelligence ever replace humans is a hotly debated question these days. Some people clean and computers will eventually gain superintelligence be able to outperform

Speaker 6:          00:29:42       humans on any task and destroy humanity. Other people say, don't worry, AI will just be another tool. We can use some control like our current computers. So we've got physicists and AI researcher Max Tegmark back again to share with us the collective takeaways from the recent sill of our conference on the future of AI that he helped organize and he's going to help separate AI myths from AI facts. Hello, first off, Max machines, including computers have long been better than us that many tasks like arithmetic or weaving, but those are often repetitive and mechanical operations. So why shouldn't I believe that there are some things that are simply impossible for machines to do as well as people say making minute physics videos or consoling a friend. Well, we've traditionally thought of intelligence as something mysterious that can only exist in biological organisms, especially humans. But from the perspective of modern physical science, intelligence is simply a particular kind of information processing and reacting performed by a particular random bits of elementary particles moving around.

Speaker 6:          00:30:34       And there's no law of physics that says it's impossible to do that kind of information processing better than humans already do. It's not a stretch to say that earthworms process information better than rocks and humans better than earthworms. And in many areas, machines are already better than humans. This suggest that we've liked the only seen the tip of the intelligence iceberg and there we're on track to unlock the full intelligence, this latent, the nature and use it to help humanity flourish or flounder. So how do we keep ourselves on the right side of the flourish or flounder balance? What, if anything, should we really be concerned about with superintelligent Ai Heroes? What has many top AI researchers concerned? Not Machines or computers turning evil, but something more subtle superintelligence that simply doesn't share our goals. If he'd seeking missile is homing in on you, you probably wouldn't think.

Speaker 6:          00:31:19       No need to worry. It's not evil. It's just following its programming. No. What matters to you is what the heat seeking missile does and how well it does it, not what it's feeling or whether it has feelings at all. The real worry isn't malevolence, but competence superintelligent AI is by definition very good at attaining it's gold. So the most important thing for us to do is to ensure that its goals are aligned with ours. As an analogy, humans are more intelligent and competent than the ads. And if we want to build a hydroelectric dam where there happens to be an anthill, there may be normal level and is involved, but well, too bad for the ants, cats and dogs. On the other hand, I've done a great job of aligning their goals with the goals of humans. I mean, even though I'm a physicist, I can't help think kittens are the cutest particle arrangements in our universe.

Speaker 6:          00:32:03       If we build super intelligence, we'd be better off in the position of cats and dogs and than that, or better yet, we'll figure out how to ensure that AI adopt our goals rather than the other way around. And when exactly is super intelligence is going to arrive. When do we need to start panicking? First of all, Henry superintelligence doesn't have to be something negative. In fact, if we get it right, AI might become the best thing ever to happen to humanity. Everything I love about civilization is the product of intelligence. So if AI amplifies our collective intelligence enough to solve today's and tomorrow's greatest problems, humanity might flourish like never before. Second, most AI researchers, things super intelligence is at decades away. But the research needed to ensure that it remains beneficial to humanity rather than harmful. It might also take decades. So we need to start right away.

Speaker 6:          00:32:48       For example, we'll need to figure out how to ensure a machine is learning the collective goals of humanity. Adopt these goals for themselves and repeating the goals as they get ever smarter. And what about when our goals disagree? Should we vote on what the machine's goals should be? Should we do whatever the president wants, whatever the creator of a super intelligence wants with the AI decide in a very real way. And then the question of how to live with super intelligence. It's a question of what sort of future we want to create, which humanity, which obviously shouldn't just be left to AI researchers as caring and then socially skilled as we are.

Speaker 3:          00:33:18       Okay, so that leads to the very final point I want to make here today to when this, where's the race trading an awesome future with Ai. In addition to doing these, these various things I've talked about, we really need to think about what kind of future we want, what sort of goal we want to have. We want to steer our technology. So just for fun, the survey I mentioned that we did, we asked people also to say what they want that for the future, and I'll just share with you here, these are from the analysis I did last weekend. Most people out of the 14,000, 866 years say they actually want they, I had to go all the way to superintelligence although some are saying no here. Um, most people, lot of people want humans to be in control. Most people actually want both humans and machines to being controlled together and a small fraction of super for the machines.

Speaker 3:          00:34:11       Uh, and then, uh, when asked about consciousness, a, a lot of people said, yeah, they will if they have machines are behaving as if there is intelligence zoom as they would like to have to have them have a subjective experience. All sorts of the machines can feel good, but some people said, Nah, they prefer having a Zombie robots. Don't feel conscious that people don't have to feel guilty, but switching them offer, give, giving them boring things to do in terms of what a future civilization should strive for. There was a large majority, you felt we should either try to maximize positive experiences or minimize suffering or something like that. Uh, the more people who said that feud, let, let the future civilization pick whatever goals they want, as long as it's reasonable. Some people said they didn't even care about if they thought the goal that future wanted was reasonable, even if it was pointless.

Speaker 3:          00:35:03       The banal like maybe turning our universe into paperclips. They were fine with just delegating it to humans. But most people actually felt that since we are creating this technology, we have the right to have some say as to where things should go. That the broadest agreement of all was on the quiz question that actually maybe we shouldn't just limit the future of life to forever be stuck on this little planet, give it the potential to spread and flourished throughout the cosmos and to get people thinking more about the different futures. My wife May, I likes to point out that, um, even though it's a good idea to visualize positive outcome is when you're planning your own career and then try to figure out how to get there. We kind of do the exact opposite. As a society, we just tend to think about everything that could possibly go wrong.

Speaker 3:          00:35:54       And then we like freak out about it. When you watch Hollywood movies, it's almost always this topic depictions of the future, right? So to get away from this a little bit in my book, the whole chapter five is a theories of thought. Experiments with different future scenarios trying to spread it span the whole range of what people have talked about and other. So you yourselves can ask what you would actually prefer. And the most striking thing from the survey was that people disagree very strongly and, and what sort of society they would like. And this is a fascinating discussion that would really encourage you all to what to join into. Um, did this, this is going to end by saying that the, I think when we look to the future, there's really a lot to be excited about. People sometimes ask me, Max, you know, all you for AI or against Ai. And I responded by asking them what about fire? Are you for it or against it?

Speaker 3:          00:36:56       Then of course they'll concede that there for fire to heat their homes in the winter and against the fire for arson. But it's the same with all technology. It's always a double edged sword. The difference with AI is just as much more powerful. So we need to put even more effort into into how, how are we steer it if you want life to, to exist for beyond the next election cycle and maybe hopefully for billions of years, uh, on earth and maybe beyond then just pressing pause on technology forever. That was actually just a really sucky idea because if we do that, the question isn't whether humanity is going to go extinct. The question is just what's going to wipe us out? Whether it's going to be the next massive asteroid strike, like the one that took the dinos out or the next super volcano or another one on the list of long things that we know are going to happen through earth that technology can create.

Speaker 3:          00:37:52       They just started that technology can prevent, but technology that we don't have yet that's going to require further development of our tech. So I for one think that it would be really foolish if we just run away from technology. I would, I would feel I'm much more excited about it in the Google spirit. And I love your old slogan, don't be evil. Asking what can we, what can we do to steer that development, that right technology in a direction so that life can really flourish not just for the next election cycle but for very, very long time on earth and maybe even throughout our cosmos. Thank you.

Speaker 2:          00:38:29       [inaudible] okay,

Speaker 1:          00:38:39       thanks so much Max. And then when we have time for questions from the audience, we have a mic over here which we can use for questions and I slept can pass this one around. And while we're doing that, I'll pull up the dory.

Speaker 3:          00:38:50       Great. And since you mentioned there were a lot of questions, make sure to keep the questions brief and make sure that they actually are questions.

Speaker 1:          00:39:00       Ai Risk seems to become a much more mainstream worry in the last few years. What changed to make that happen and why didn't we do it earlier?

Speaker 3:          00:39:10       I agree with you. I'm, I'm actually very, very happy that it's changing this way and trying to help make a change this way. It was the key reasonably founded the future of Life Institute and organize the Puerto Rico conference this summer conferences and so on because we felt that the, uh, up until a few years ago, the debate was kind of this dysfunctional and we read what I think is really, really changed things for the better. Is it the AI research community itself as really engage joined this debate and started the own it can. I think that's why it's become more mainstream and also much more sensible.

Speaker 2:          00:39:47       Okay.

Speaker 3:          00:39:49       Okay. So you were the boss's we alternate with online off nine questions. Do you want to read the questions? Sure.

Speaker 1:          00:39:56       Uh, what would you do to her? What would you most hope to see a company like Google to, to ensure safety as we transitioned to a more AI centric world?

Speaker 3:          00:40:04       So as I said, I, I think, uh, Google

Speaker 3:          00:40:10       already has the soul to do exactly what's needed. This don't be evil slogan of Larry and Sergey. I interpreted it as though we shouldn't just build technology because it's cool, but we should think about its uses. Know for those of you who know the Tall Tom Lira song about Verna Front Brown and that's not once the rockets go up as well as a calm down. That's not my department says van half on Brown. I view Google don't be evil. Slogan is exactly the opposite of that. Thinking mindfully about how to steer the technology to be good. And then I'm also really excited again, the Google is one of the founding partners in the partnership for Ai. Trying to make sure that we, that this happens not just in Google the Google what Google does but, but throughout the community. And um, I also think it's great if Google can pull all of its strings, the persuade politicians all around the world at CRSD fund the AI safety research.

Speaker 3:          00:41:10       Because the sad fact is even though there's a great, we'll for AI researchers to do this stuff now, there's almost no funding for it to tell. What do you Elon Musk help us give out? 2037 grants for is just a drop in the bucket of what's needed. And, um, it makes sense that Google and other private companies want to own the Ip on. Um, things would make AI more powerful and build products of it. But these same private companies, it's better for them all if nobody patents the way to make it safe and keeps others from using it. Right. That's something that's great if it's developed openly by companies or share it or in universities so that everybody can use the same best practices and raise the quality of safety everywhere.

Speaker 7:          00:41:57       Uh, all right. So, uh, Max actually he talked to you last night. How bout a lot like future, I really long, maybe a hundred years each, what's going to happen? But, uh, uh, if you look at the AI nowadays, not a lot of people focusing on today. I'll just aiming it's, uh, risks. So if you think about how, uh, how the Trump got elected and, uh, how those things went wrong in his last few years, uh, you cut really denying that a, I has contributed a lot. He specially interfect deal sad. Uh, he has like a suggested contents. So, um, he started like focusing on our all our energy into the future. So I felt there are really few people that's like looking into today. So do you think, cause that's a problem all these Inca would need to do battery on that?

Speaker 3:          00:42:48       Yeah, I think there's a really great opportunity for us nerds in the tech community to educate the broader public and politicians about the need to really engage with this. This is one of the reasons I want you to write this book. Um, I think, uh, when we watched, when I watched the presidential debates for the last election, for example, complete the aside from the front of the issue is they've talked about, I thought it was just absolutely astonishing what they didn't talk about. No, none of them talked about AI at all. Hello? Like they're talking about jobs, they're not mentioning Ai. They're talking about six international security and not talking about the, I like the biggest technology on out there. And uh, I think, um, in addition to just telling politicians to pay attention, I think it's incredibly valuable. Also, if a bunch of people from the tech community can actually go into government positions to add more human level intelligence in government. So, so the, the preventative with the world governments from, from being asleep at the wheel.

Speaker 7:          00:43:51       Uh, I mean, I actually,

Speaker 3:          00:43:52       maybe we should just, we can talk more afterwards, but everybody tends to ask first. Hello. Hi. Okay.

Speaker 5:          00:44:00       Um, when you introduced the concept of when you introduce the Cila Mar Treaty, uh, you mentioned the difference between undirected intelligence and benevolent intelligence. Um, don't you think that if human succeeded in creating controllable benevolent intelligence that they really have failed in creating intelligence?

Speaker 3:          00:44:24       Let me rephrase this question. Do you want to just repeat the punchline? I'll, I'll rephrase. Uh, do you that benevolent

Speaker 5:          00:44:32       intelligence, it would be the intelligence that we should strive towards, or should it be general intelligence that in perhaps cannot be controlled?

Speaker 3:          00:44:42       So that's a great question. You ask what I, what I think I am trying to be very open minded about what we actually want them. And I wrote the book not really, really avoiding saying, well I think the future should be because I think, um, this is such an important question. We just need everybody's wisdom on it. And yet again, I talk about all these different scenarios, some of which correspond to some of the different options or even listed there. And I'm incredibly interesting interested to hear what other people think would actually be good with these things. One, one, um, one thing that may I, and I found very striking when we discuss this was when I was writing the book was even though I tried quite hard to emphasize the upsides of each scenario, there wasn't a single one there that I didn't have at least some major misgivings about.

Speaker 5:          00:45:38       Do you think deep neural networks will be the way to get to artificial general intelligence? If not, do you see fundamental reasons why these do not have the potential for recursive self improvement? They can speed up the development of Agi or super indulgence.

Speaker 3:          00:45:51       All right. That's a great question. So I think that the, although, uh, yeah, let me say two things about this. This is the, first of all, our brain seems to be of course some kind of a of a recurrent neural network. It's very, very complicated and it has human level intelligence. But I think it would be a mistake to think that that's the only route there. I think you'd also be a mistake to think assuming that that's the fastest route there may I like to point out that, uh, even though finally a few years ago there was a beautiful tedtalk demonstrating the first ever successful mechanical bird that came a hundred years after the white with the Wright brothers built airplanes. And when I flew here yesterday, you'll be very surprised to hear this, but I didn't come into mechanical bird and it turned out there was a much easier, simpler way to build flying machines.

Speaker 3:          00:46:46       And I think we're going to find exactly the same thing with human level intelligent machines. The brain is just optimized for very different things. Then what your machines that you build are the brain is f a Darwinian evolution is obsessed about only building things that can self assemble. Who cares if your laptop can self assemble. The evolution is obsessed about creating things that can self repair. It would be nice if your laptop could self repair but it can't, then you're still using it. So, uh, and also evolution is doesn't care about simplicity for humans to understand how it works, but you care a lot about that. So maybe this is much more complicated than it needs to be just so it can self assemble on blah, blah, whatever. My guess is that the first human level, yeah. I will not be working exactly like the brain that it will be something much, much simpler and maybe we'll use that the gray later and figure out how human human brains work. Uh, that said, the deep neural networks are of course inspired by the brain. Now you are using some efforts if there's some very clever or computational techniques that evolution came up with. Um, my guess is that the fastest route to human level, I will actually use a combination of of deep neural networks with go five various good old fashioned AI techniques, more logic based things which have a lot of their own strength for billing, like for building a world model and and things like this.

Speaker 3:          00:48:15       And maybe I should just add one more thing about this also this pose is the increasing successes of neural networks Paul suppose is a really interesting challenge because when we put AI in charge of more and more infrastructure in our world, right? It's really important that it be reliable and robust. Raise your hand if your computer has ever crashed on you. All right. That wouldn't have been so fun if it was the the machine that was controlling your self driving car or your local nuclear power plant or your nation's nuclear power nuclear weapons system. Right. And what we need to transform today's buggy and hackable computers and the robust AI systems that we can really trust. What is trust? Where does trust come from? It comes from understanding how things work. And neural networks I think are a double edge sword. They are very powerful, but we understand the much less than than traditional software. So in my group at Mit, actually we're working very hard right now on a project that I call intelligible intelligence where we were trying to come up with algorithms where, where you can transform neural networks into things which you cannot really understand better how they work. I think this is a, this is a challenge that I would encourage you to all think about too. How can you combine the power of neural nets with stuff that you can really understand better and therefore trust.

Speaker 8:          00:49:35       So, uh, should we be afraid that uh, a Gi bill use it super intelligence to figure out that it's treatment by the humans is essentially slavery were just extra steps.

Speaker 3:          00:49:48       I, that's, well, that's a wonderful, wonderful question. I haven't talked at all about consciousness here, but um, the whole chapter eight in the book is about that. And a lot of people say things like, well,

Speaker 3:          00:50:03       machines can never have a subjective experience and feel anything at all because to feel something and you have to be made of excels or carbon atoms or whatever. As a scientist, I really hate this kind of carbon chauvinism. I'm made of the same kind of up corks down quarks and electrons is all the computers are, I'm didn't mind it. Just arrange in a slightly different way. And it's obviously something about the information processing. That's all that matters, right? So, uh, we've, and Pete, moreover this kind of a self self justifying arguments have been used by people throughout history. The say, oh, it's okay to torture slaves because they don't have souls. I don't feel anything. Oh, it's okay to torture chickens and, and uh, today and giant factories because they don't feel anything. You know, of course we're going to say that about our future computers too.

Speaker 3:          00:50:51       Uh, because it's convenient for us. But that doesn't mean it's true. And I think it's actually really, really interesting question. The first figure out what is it exactly that makes an information processing system have, have subjective experience? Uh, a lot of my, my colleagues whom I really respect, I think this is just bs, this whole question. This is what the Daniel Dennett says. I look up at the Mcmillan dictionary of psychology and it's said the consciousness is something, nothing worth reading he has ever been written on. But I really disagree with this. And actually let me just take one minute and explain why I think this is actually scientifically interesting question. So let's look at this. Okay. And Ask yourself why is it that when I show you 450 nanometer light on the left and 650 nanometer light on the right, why do you subjectively experience it like this?

Speaker 3:          00:51:45       And not like this? Why I like this and not like this I put to you that this is a really fair game science question that we simply don't have an answer to right now. There's nothing to do with wavelengths of light. There are neurons or anything that explains this, but it's an observational fact. Then I would like to understand and and why does it feel like anything? Why, why do we have this experience? You know, you might say, well look, we know that there are three kinds of light sensors in our retina, the cones and when I am with, uh, with a four 59 meter light to activate one kind of when I have the longer wavelength to activate the other Cayenne. And then you can see how they're connected to various newer ones in the back of your brain. But that just sharpens the question, the mystery of consciousness because this proves that it had nothing to do with light at all because you can experience colors even when you're dreaming, when different neurons, new brain activity, when we, when there is no light involved.

Speaker 3:          00:52:37       Right? So my guess is that consciousness, by which I mean subjective experience is simply the way information fields when it's being processed in certain complex ways. And I think there are some equations that we will one day discover that specify what those complex ways are. And once we can figure that out, it will both be very useful because we can put a consciousness detector and the emergency room and when an unresponsive patient comes in and you can figure out if they have locked in syndrome or not and it will also enable us to answer these. This is really good question. You ask about whether machines or should also be viewed as moral entities that can cause they can have feelings and above all, and I don't see Ray Kurzweil here today, but you know, if he can one day upload himself into the Ray Kurzweil robot then live on for for thousands of years and he talks like gray and it looks like gray and he acts like where you feel that that's great for Ray.

Speaker 3:          00:53:31       You know, now he's a mortal but, but suppose suppose it turns out that that machine is just a Zombie and doesn't feel like anything to be it. He would be pretty bummed wouldn't he? All right. And, and if in the future life spreads throughout our cosmos in some post biological form and we were like, this is so excited and our descendants are doing all these great things and we can die happy if it turns out that they're all just a bunch of zombies and all that cool stuff is just the play for empty benches would not suck.

Speaker 5:          00:54:05       Oh, do another question from the door. What do you think is the most effective way for individuals to embrace a promote a security engineering mentality? I you were not even one glitches tolerable when working on AI related project.

Speaker 3:          00:54:23       Well, first of all, I think we have a lot to learn from existing successes and safety engineering. That's why I started by showing the moon mission. This is not like that this is anything new to engineers. I think it's just that we're so used to the idea that AI didn't work, that we didn't need to worry about, uh, the impact of things. Um, and um, and now it is beginning to have an impact. So we should think it through. And then there are also a few challenges which are very unique and specific to AI. Some of the Sylmar principals talk about them. And, and um, this research agenda for AI safety research is a really a long list of specifics that of safety engineering challenges that we need. Smart people like you to work on it. And I hope we can support that.

Speaker 4:          00:55:12       Cool.

Speaker 5:          00:55:12       So also on the topic of security engineering, a lot of rockets blew up on the way to the moon. Yeah. And you know, given the intelligence explosion, it's like we're only gonna have one chance to be able to get the alignment problem correct. And you know, I think we couldn't even align on a set of values in this room, let alone a system that would govern the world effectively. Cause you know, there's certainly some drawbacks of capitalism. So I'm hopeful. I am glad that Ilan is hedging our bets by making a magic hat. But it seems like, you know, you and your group are focusing on the alignment problem. And I'm just, you know, kind of just curious, like, you know, what makes you optimistic that we're going to be able to get it right on the first time.

Speaker 3:          00:55:49       So, so first of all, yeah, a lot of rockets blew up. But, but you will note that most of the rockets that blew up, in fact, all our rock, that's the rock is that blew up in the moon mission, had no people in them. Right? So that would safety engineering, the high risk stuff, they did it in a controlled environment where the failure didn't matter so much. So if you've made some really advanced AI, you want understand it really well, maybe don't connect it to the Internet the first time. Right? So the down, so the downsides are small. There's a lot of things like this that you can do. And um,

Speaker 3:          00:56:24       I'm not saying that there's one thing that we should particularly focus on either. I think the community has brainstormed up really nice long list of things and we should really try to work on the mall. And then we will figure out some more challenges along the way. But that's the main thing we need to do is just all I just, yeah, this is valuable. Uh, let, let's, let's work on it. And then you asked also why I'm optimistic. Let me just clarify. There are two kinds of optimism. There's naive optimism, like my optimism that the sun is gonna rise over mountain view tomorrow morning regardless of what we do. That's not the kind of optimism I feel about the future of technology. Then there's a kind of optimism that you're optimistic that this can go well if we really, really plan and work for it. That's the kind of optimism I feel here. We have in our hands. It's to create an awesome future. But, uh, let's, let's roll up our sleeves and do it.

Speaker 9:          00:57:23       Hey Max. Um, in the paper that you wrote entitled to, why does cheap and deep learning works so well with Lynn and now we're all nick as well. Um, you ask the key question and you, you draw a lot of connections between, you know, a deep learning and then core parts of what we know about physics, a low polynomial order, I uncle processes, things like that. I'm just curious what are the reactions you received both from the physics community and then from the Ai community to that attempt to kind of draw some deep parallels?

Speaker 3:          00:57:51       Uh, just generally quite positive, uh, feedback. Uh, and then also people who are pointed out a lot of additional research questions related to that which are really worth doing. And just to bring everybody up to speed as the word we're talking about. Don't devolvement that this disgusting in Nordea research paper here. Uh, okay.

Speaker 4:          00:58:11       Okay.

Speaker 3:          00:58:12       We were very intrigued by the question of why deep learning works so well. Because if you, if you think about it, and I usually, I, even if I just want to classify, I take all the Google images and I want to have cats and dogs and I want to write it and I want to take in New Orleans or cause they will take in say 1 million pixels and output the probability that it's the cat, right? If you think about it just a little bit, you might convince yourself that it's impossible because how many such images are there, even if they're just black and white images. So each pixel can be black or white. There's two to the power of 1 million possible images are just much more images than there are atoms in our universe. There's only a 10th of the seventh, the eighth right? And for each image you have to output a probability.

Speaker 3:          00:58:57       So at the specifying arbitrary function of images, how many parameters do you need for that? Well, two to the 1000 would, you can't even fit if you store one parameter on each atom and in our cosmos. It's like how can it work so well and so it's always the basics wind solution we found there was was that the, of course the class of all functions that you can do well with a neural network that you can actually run. Is it almost infinitesimally tiny fraction of all functions but then physics tells us that the fraction of all functions that we actually care about because they're relevant to our world is also almost infinitesimally small fraction and then conveniently they're almost the same. I don't think this was luck. I think Darwinian evolution Davis, this particular kind of neural network based computer precisely because it's really well tuned for tapping into the kind of computational needs that are called masters dished out to us and we, I'll be delighted to chat more with you later about about loose ends to this cause I think there's a lot more interesting stuff to be done on that

Speaker 1:          01:00:03       take story. Being humans have the, in the age of AI, seems like an egocentric effort that gives an undeserved special status to our species. Why should we even bother to remain humans when we could get to push our boundaries and see where we get

Speaker 3:          01:00:20       all right.

Speaker 4:          01:00:22       Yeah.

Speaker 3:          01:00:27       Egocentric efforts. It gives us undeserved specialist status to our species.

Speaker 4:          01:00:36       Yeah.

Speaker 3:          01:00:38       Well first of all, you know, I'm, I'm totally fine with pushing our boundaries and I've, I've been advocating for doing this. I mean I find that very annoying human hubris when we go on a soapbox and we were like, well, you are the pinnacle of creation and nothing can ever be smarter than us and we try not to build our whole self worth on somehow human exceptionalism. I think that's kind of lame. Um, on the, on the other hand, we should probably make this the last questions. And on the other hand, you're centric efforts. Well, we are the only ones that it's only us. Humans are in this conversation right now. So, and somebody needs to have it. So it's really us to us up to us to talk about it. Right? We can't use this kind of thinking as an excuse to just not talk about it and just bumbled into some completely uncontrolled future art. I think we should take a firm grip on the rudder and stare in whatever direction we decided to steer steering. So let me thank you again so much for coming out. It's a wonderful pleasure to be here.

Speaker 4:          01:01:42       And, uh, if you had any more questions,

Speaker 3:          01:01:46       I get in, I'll be here signing books and I'm happy to chat more.

Speaker 1:          01:01:50       Thank you all for coming and thanks to Max for talking at Google

Speaker 3:          01:01:53       and thank you for having me.