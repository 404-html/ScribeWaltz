Speaker 1:          00:06          Thank you. So this is the, uh, this a talk. This is the book shot a Taz one, a my first ever clickbait title. And I liked the cover I've covered because for two reasons, one, there's only one button that says okay, when it's clearly not okay and it looks like this thing's been throwing error messages of the past hour and nobody has been paying any attention to them. So this is a book is a computer and what I'm writing about is security in a world where everything is a computer and I think this is the way we need to conceptualize the world. We're building this smart, this phone is not a phone, it's a computer that makes phone calls. And similarly your microwave oven is a computer that makes things hot and your refrigerators or computer than make keeps things cold. And the ATM machines computer with money inside and in a car is now a computer with four wheels and an engine, actually that's wrong.

Speaker 1:          01:12          A car is about a hundred plus distributed system with four wheels and an engine. All right, so, and this is more than the Internet. It's again at a things, but it's more than that as well. In my book, I use the term Internet plus. I hate having to invent a term, but there really isn't a term we have for the Internet, the computers, the things, the big systems like power plants, the data stores, the processes, the people, and it's that holistic system that I think we need to look at. Let me look at security. So if everything is becoming a computer, it means two things that are relevant here that all internet security becomes everything security and all the lessons and problems of Internet and computers become problems everywhere. So let me start with six sort of quick lessons of computer security, which will be true about everything and everywhere.

Speaker 1:          02:18          Some them are obvious and computers not so obvious elsewhere. I the first most software is poorly written and insecure, right? We know this basic reason is the market doesn't want to pay for quality software like good, fast, cheap, pick any two. We have picked fast and cheap over good, right? With with very expensive exceptions like avionics and the space shuttle. Most software is really lousy. It kind of just barely works now for security, right? Lots of vulnerabilities, some of those vulnerable. So I started lots of bugs, some of those bugs of vulnerabilities. Some of those vulnerabilities are exploitable, which modern software has lots of exploitable vulnerabilities and that's not going to change anytime soon. The second lesson is that the Internet was never designed with security in mind. That seems ridiculous today, but have you think back to the late seventies and early eighties there were two things that were true. One, the Internet was used for nothing important ever and two you had to be a member of a research institution had access to it

Speaker 1:          03:29          and you read the early designers and they talked about how fear limiting physical access was a security measure and in fact you would exclude bad actors meant that you didn't have to worry much about security. So decision was made deliberately to leave security end point, not put in the network but fast forward today and we are still living with the results of that in Dum, Dum domain name system and routing and packet security email addresses sort of again and again. The protocols don't have security and we are stuck with them. Third lesson, the extensibility of computerized systems means they can be used against us. Extensibility is not something that non computer people are used to. Basically what I mean by that is you can't constrain the functionality of a computer because it's software. When I was a kid I had a telephone, big black thing attached to the wall, great device, but no matter how hard I tried, I couldn't make it be anything other than a telephone. This is a computer that makes phone calls. It can do anything you want, right? There's an app for that because this can be programmed because it's a computer. It can do anything. You can't constrain this functionality, right? It means several things for security, hard to test this thing because what it does changes how it's configured, changes and it can get additional features you don't want. That's what malware is,

Speaker 1:          05:07          so you can put malware on this phone or on inner catheter to fridgerator in a way that you can't possibly ever do it in an old electrical mechanical refrigerator because they're not computers. Fourth lesson is about complexity. A lot of ways I can say this, basically the complexity of computers means attack is easier than defense. I could spend an hour on that sentence, but complex systems are hard to secure in. It's the most complex machine mankind has ever built by a lot, which makes this incredibly hard to secure,

Speaker 1:          05:45          hard to design, securely hard to test, so everything about it, it is easier to attack a system than to defend it. Our fifth lesson is that there are new vulnerabilities, the interconnections as we connect things to each other, vulnerabilities and one thing affect other things. I had lots of examples. The Dine Bot net at vulnerabilities in a connected route was digital video recorders and Webcam is primarily allowed an on to create a Bot net that dropped the domain name server. That intern dropped a couple of dozen real popular websites. I 2013 target corporation attack through a vulnerability in the Hvac contractor of several of their mid Pennsylvania stores earlier this year is a story of a casino in Las Vegas. We don't have the name of the casino. They had their high roller database stolen and the hackers got in through, and I'm not making this up, they're internet connected fish tank. So right vulnerabilities, this can be hard cause sometimes nobody's at fault. I reblogged a few months ago about a vulnerability that results from the way Google treats email addresses, right? The dots don't matter for your name and the way Netflix streets email addresses, the dots do matter. Turns out you can play some games with that. Who Do we blame? I'm not sure we blame anybody. There's a vulnerability in PGP, which is actually not really vulnerability. Impeached, speeds, vulnerability, and the way emailers handle PGP, which everyone's convinced everyone else was at fault

Speaker 1:          07:35          and these kinds of things is going to happen more and more. The last lesson is that attacks always get better. Tax always get easier, faster, cheaper, right? Some of this is Moore's law. Computers get faster, so password guessing gets faster as computer, it gets faster, not because we're smarter about it, but we also get smarter. Attackers adapt, attackers figure out new things and expertise flows downhill. What today is a top secret NSA program? Tomorrow becomes a Phd thesis and the next day as a common hacker tool, and you can see this again and again, and an example might be a Imsi catchers, fake cell phone towers, sting rays,

Speaker 1:          08:22          right? Which when I mean the cause of them is a, that cell phones don't authenticate to tower. They automatically trust any, anybody says, I'm a tower. So if you put up a fake tower, you can now query phones and get their, uh, addresses that sort of know who's there. Now, this was something that, uh, the NSA, the FBI used big government secret for awhile. Uh, expertise flowed downhill a few years ago, I think it was motherboard did looked around the DC, found a couple of dozen of them run by. We know who around US government buildings. I mean right now you can go on alibaba.com buy one of those things for about a thousand dollars in China. They used to send spam two phones. You get a software defined radio card, you can download free software and make your own right. What started out as something that was hard to do is now easy to do. So those are my sort of six lessons that are going to be true for everything and none of that is new. But up to now it's been basically a manageable problem. But I think that's going to change. And the reasons are automation, autonomy and the physical agency, computers that can do things.

Speaker 1:          09:44          So have you do computer security, you've heard of the CIA triad, confidentiality, integrity and availability. Three basic properties. We deal with insecurity by and large. Most of what our issues are, our confidentiality. Someone stole and misused our data. That's ecofacts. That's office of personnel management. That's Cambridge Analytica. That's all the data thefts ever. But when you get, when you have computers that can affect the world in a direct, physical manner, integrity and availability become much more serious because the computers can do stuff. There's real risk to life and property. So yes, I am concerned that someone hacks my hospital and steals my private patient and medical records, but I'm much more concerned that they changed my blood type.

Speaker 2:          10:38          Yeah.

Speaker 1:          10:39          I don't want them to hack my car and use the Bluetooth microphone to listen in on conversations, but I really don't want them to disable a breaks. Those are data integrity and data availability attacks respectively. So suddenly the effects are much greater. And this is cars, medical devices, drones, any kind of weapon systems, thermostats, a power plants, smart city, anything. Appliances.

Speaker 2:          11:12          Okay.

Speaker 1:          11:13          I blogged a couple of days ago about, uh, an attack where someone just theoretical, if you can hack enough, major appliances can turn power on and off in synchronization and, uh, affect the load on power plants and potentially cause blackouts. Now, very much a side effect. But once I say that, you say, well yeah, Duh. Of course you can do that. Very different sort of attack, right? There's a fundamental difference between my spreadsheet crashes. I may lose my data and my implanted defibrillator crashes and I lose my life and it could be the same, the same operating system, the same vulnerability, the same attack software because of what the computer can do. The effects are much difference, so the same time we're getting this increased functionality, there's some long standing security paradigms that are failing and I give three. The first one is patching and patching is how we get security and it's now having trouble actually of there's two reasons why our phones and computers are secure as they are.

Speaker 1:          12:29          The first is that there are security engineers at apple and Microsoft at Google that are designing it as secure as they are in the first place and those engineers can quickly write and push down patches when vulnerabilities are discovered. That's a pretty good ecosystem. We do that well. The problem is it doesn't work for low cost embedded systems like dvrs and routers. These are designed and built off shore by third parties, by ad hoc teams that come together, design them, and then split apart. I mean there are people who can write those patches when a vulnerability is discovered. And even worse, a lot of these devices have no way to patch them, right? If your DVR is vulnerable to the, uh, to the, to the hack that, uh, allows to be crucial with botnets, the only way you can patch it is to throw it away and buy a new one.

Speaker 1:          13:27          That's the mechanism we have no other now actually throw it away and buy a new one is a reasonable security measure. We do get security. The fact that the life cycle of, of phones and computers is about three to five years. That's not true for consumer goods. You're going to replace your DVR every 10 years, your refrigerator, every 25 years. I bought a programmable thermostats last year. I expect her a place. It approximately never think of it when, think about it in terms of a car or you buy a car today. So let's say software is two years old, you're going to drive it for 10 years, sell it, someone else buys it, drives her 10 years, they sell it, someone else buys it, puts on a boat, said to the South America, whereas someone else in their buys it drives or another 10 to 20 years and you go home, find a computer from 1976 try to boot it, try to run it, try to make it secure. We actually have no idea how to secure 40 year old consumer software. We have the faintest clue and we need to figure it out. So what does Christ will maintain? A test bed of 200 chassies per vulnerability testing and for patch testing. Is that the mechanism? We're not going to be able to treat these goods like we treat phones and computers, you know, we start forcing the computer, the computer lifecycle onto all these other things. We are probably literally gonna Cook the planet. So we need some other way and we don't have it. Second thing that's failing as authentication, right? It's, we've always been only okay at authentication,

Speaker 2:          15:22          but

Speaker 1:          15:23          authentication is going to change. Right now, fenestration tends to be me authenticating to some object or service. What we're going to see an explosion in his thing to thing authentication. We're objects into authenticate to objects and it's going to be a lot of it. Imagine a a driverless car or even some kind of computer assisted driving car. It want to authenticate to thousands of other cars, road signs, emergency vehicles and and signals. Lots of things. And we don't know how to do that at scale, but are you might have a hundred iot objects in your orbit to authenticate to each other. Let's 10,000 authentications, right? 1,000 objects, a million authentications. I mean, right now this is tends to be our iot hub. Do you have an Iot? Anything you like to control it via your phone? I'm not sure that scales to that many things and while we can do things, think that's an occasion.

Speaker 1:          16:30          It's very much a deliberate. So right now when I get into my car, this phone authenticates the car automatically, right? That works. Bluetooth works, but it works because I was there to set it up and I'll do that for 10 things for 20 things on not doing it for a thousand. I'm not doing it for a million. So we need some way to do this automatic thing of the thing of authentication at scale and we don't have it. The third thing that's failing is supply chain. Supply chain security is actually insurmountably hard now. We've seen it. You've seen the papers in the past year. It's been one of two stories. It's Kaspersky, right? Should we trust a Russian made antivirus program and uh, who ae and ZTE should we trust Chinese made phone equipment. But that really is just the tip of the iceberg. There are other stories, not just the U s turns out in 2014, China banned Kaspersky.

Speaker 1:          17:35          They also ban SYMANTEC. By the way, a 2017 I started from India. Then if I'm 45 Chinese phone apps that they say shouldn't be used, uh, in 1997 and the people remember there a worries in the u s about checkpoint and Israeli made security product. You know, should we trust it? I also, I like a member, a 2008 program called Musha secrets, which was an isis created encryption program because of course you can't trust western encryption programs but you know, the country of origin of the product is just the tip of the iceberg. Where are the chips made? Where is the software written? Where is the device fab where the programmers are far from.

Speaker 1:          18:29          I mean this iPhone probably has won a couple of a hundred different passports that are programming this thing. It's not need in the U S and every part of the chain is a vulnerability. They are, they're a paper showing how you can take a, you know, a good chip design that the masks and maliciously put in another layer and calmed my security. The chip without the designer's knowing it and it doesn't, it doesn't show up in testing. There was another paper about two years ago. Uh, you can, uh, hack an iPhone through a malicious replacement screen, right? You have to trust every piece of the system. The distribution mechanisms. We've, we've seen backdoors in a Cisco equipment. Uh, Mohammed, the NSA intercepted, uh, the, uh, Cisco routers being sent to the Syrian telephone company. That was one of the greatest pictures from the stolen documents. We've seen fake apps in the Google play store.

Speaker 1:          19:37          We know that a Russia attacked Ukraine through a, a software update mechanism. I think my favorite story is, this is a hard one. In 2003, there was actually a very clever, very subtle backdoor that almost made it into Linux. We caught it and we kind of just barely caught it. We got very lucky there and, and you look at the code, it's, it really is hard. You have to look for it to see the back door. Now that could have easily gotten in. We don't know what else has gotten in, in watts and solving this is hard. No one wants a US only iPhone is probably a impossible and beetle cost 10 x. Now our industry is at every level international. It is deeply international from the programmers to the, to the, to the, the objects to the cloud, the services. We will not be able to solve the seasonally.

Speaker 1:          20:40          So, and a lot of ways this is a perfect storm and things are failing just as everything is interconnected and will, and I think we've been okay with a unregulated tech space because fundamentally didn't matter. And that's changing. And I think this is primarily a policy problem. And in my book, I spend most of the time on policy and I talk about a lot of different policy levers we have to improve this. Talking about standards, regulations, liabilities, courts, international treaties think it's a very hard political battle. And I don't think we're going to have in the u s until a catastrophic event. You know, I look more to Europe to lead. I couldn't go through all of this, but I want it to want to give sort of two principles I want to pull out. The first is that the fence must dominate. I think we as a national policy need to decide that defense wins that no longer can we accept insecurity for offense purposes that as these computers become more critical, defense is more important. My gone are the days when you can attack their stuff and defend our stuff. Everyone uses the same stuff. We all use TCP IP and Cisco Routers and Microsoft Word and pdf files, and it's just one world, one network, one answer. Either we secure our stuff, thereby incidentally securing the bad guy's stuff, or we keep our stuff vulnerable in order to attack the bad guys, thereby incidentally rendering us vulnerable and that's our choice

Speaker 1:          22:30          and it means it means a whole bunch of things to disclose and fix vulnerabilities to design for security, not for surveillance, encrypt as much as possible to really separate security from spying, make law enforcement smarter so they can actually solve crimes even though their security and create better norms. What other principle is that we need to build for resilience? When you start designing systems, assuming they will fail and how do we contain failures? How do we avoid catastrophes? How do we fail safe for fail secure? Where can we remove functionality or delete data? How do we have systems monitor other systems to try to provide, you know, some level of redundancy. And I think the missing piece here has government that the market will not do this on its own.

Speaker 1:          23:30          But I have a problem, you know, handing us to government because there really isn't an existing regulatory structure that could tackle this at a systemic level grads because there's a mismatch between the way government works and the way tech works at government operates in silos. The FAA regulate aircraft, the FDA regulates medical devices or the FTC regulates consumer goods. Someone else does those cars and each agency will have its own rules and on approach and own systems. And that's not the internet or the Internet is this freewheeling system of integrated objects and and networks and it grows horizontally and it kicks down barriers and it makes people able to do things never could do before. And all of that rhetoric is true. I mean, right now this device logs my health information, communicates with my car, monitors my energy use and makes phone calls, right? That's four different, probably five different regulatory agencies. And this is just getting started,

Speaker 1:          24:45          right? We're not sure how to do this. So in my book I talk about a bunch of options and what I have, and I think we're going to, we're going to get eventually is a, a, a new route and a new government agency that will have some jurisdiction over computers. Uh, this is a hard sell to a, you know, low government crowd. But there is a lot of precedent for this. In the last century, pretty much all major technologies led to the formation of new government agencies. All right? Cars did, planes did, radio did, nuclear power did because government needs to consolidate its expertise. So, and that's what happens first. And then there is need to regulate. I don't think markets solved us, Marcus or short term markets or profit motivated markets don't take society into account. Markets can't solve collection at the collective action problems. So of course there are lots of problems with this, right? Governments are terrible. Being proactive, a regulatory capture is, are we is a real issue. I think there are differences between security and safety that matter here. It's safety against things like a hurricane and secure it against an adaptive, malicious, intelligent adversary are, are sort of very different things. And you know, we live in a fast moving technological environment and it's hard to see how government can stay ahead of tech. There's something that's changed in the past couple of decades or tech moves faster than policy. The Devil's in the details and, and I, I don't have them, but

Speaker 2:          26:38          yeah,

Speaker 1:          26:38          this is a conversation that we need to have because I believe that governments get involved regardless that the risks are too great and the stakes are too high, right? Or governments are ready involved in physical systems. They already regulate cars and appliances and toys and power plants and medical systems. So they already have this ability and need and desire to regulate those things as computers. But how do we give them the expertise to do it right? Uh, my guesses are the courts are going to do some things relatively quickly because cases will appear and that the regulatory agencies will follow. I think Congress comes last but don't count them out and nothing motivates a, a, a government. The US government like fear.

Speaker 2:          27:37          Okay.

Speaker 1:          27:37          When you think back to the terrorist attacks of September 11th, we had a very small government administration create a massive bureaucracy kind of out of thin air. And that was all fear motivated. And when something happens that will be a push that something must be done. And we are past the choice of government involvement versus no government involvement. Our choice now is smart governor involvement versus stupid government involvement. And the more we can talk about this now, the more we could make sure it's smart. Uh, am I guess as any good regulation will, uh, send private industry that I think the reason we have such bad security is not technological, it's more economic. There's lots of good tech and you know, while some of these problems are hard, they're like send a man to the moon hard. They're not Fastenal. I travel hard.

Speaker 2:          28:36          Yeah.

Speaker 1:          28:37          And once the incentives are in place in this story, we'll figure out how to do it right. I mean a good example, it might be credit cards. In the early days of credit cards, we were all liable for a for fraud and losses that change in 1978 the fair credit reporting act, that's what a mandated the maximum liability for credit card fraud for the consumer is $50 and he understood what that means. That means I could take my card flinging in the middle of this room, give you all lessons on forging my signature and my maximum liability is $50 right? It might be worth it for the fun, but what that meant, right? That change that even if the consumer is at fault, the credit card company is liable. That led to all sorts of security measures that led to online verification of uh, of credit and card validity that led to anti forgery measures like the holograms and the micro printing blood to mailing the card and the activation information separately and requiring you to call from a known phone number.

Speaker 1:          29:48          And actually most importantly, that enabled the backend expert systems that troll the, uh, credit, the transaction database, looking unfortunately spending patterns, none of that would've happened if the consumers reliable because the consumers had no ability to implement any of that. You want the entity that can fix the problem to be responsible for the problem. That is just smart policy. So I see a lot of innovation that's not happening because the incentives are mismatched. So I think Europe is moving in this direction, right? The EU is right now the regulatory superpower on the planet and they are not afraid to use their power. We've seen that in the Gdpr in the privacy space. I think they're going to turn to security next. I mean, they're already working on what responsible disclosure means. Uh, there's that you have to see on manufactured goods. Does that label called CE? That's an EU label basically means uh, meets all applicable standards. They're working on standards for cybersecurity and you know, useful. See them get incorporated trade agreements into Gat. And there's an interesting, a rising tide effect. It's not necessarily obvious the the car you buy a United States is not the car you buy in Mexico. Right? Environmental laws are different and the cars are tuned to the different laws but not true in the computer space.

Speaker 2:          31:22          The uh,

Speaker 1:          31:24          the Facebook you get is pretty much the same everywhere. And if you can imagine there's some security regulation on a toy, the manufacturer meets it. They're not going to have a separate build for United States. They're going to sell it everywhere cause it's easier that, and they'll, there'll be times when that's not true. I think Facebook would like to be able to differentiate between someone who is subject to the Gdpr is somebody who's not cause there's more revenue to be gained through them, the greater surveillance. But when you get to things, I think it's more likely that it will be a rising tide and we all benefit. United States are look to the states more specifically New York, Massachusetts, California, which are more aggressive, uh, in this space. But I think this is coming

Speaker 2:          32:17          and

Speaker 1:          32:19          I want to close with a, I guess a call. What we need to do is to get involved in policy. Technologists need to get in policy to get involved in policy. As Internet security becomes everything. Security, Internet security technology becomes more important to overall security policy. And all of the security policy debates will have strong technological components. We will never get the policy right if the policy makers get the tech wrong, right? It will all look like the Facebook hearings, which were embarrassing. And you see it even in some of you see it in the going dark debate. You see it in the equities debate, you see it and voting machine debates in driverless car security debates that we need technologists in the room during policy discussions, right? We have to fix this. We need technologists on congressional staffs at Ngos doing investigative journalism in the government agencies in the White House,

Speaker 2:          33:31          right?

Speaker 1:          33:32          We need to make this happen. And right now you just don't have that ecosystem. So you think about a public interest law 1970s there was no such thing as public interest law. They would actually wasn't. It was created primarily by the Ford Foundation. Oddly enough, that funded law clinics, funded internships in different NGOs and now you want to make partner at a major law firm. You are expected to do public interest work today at Harvard, Harvard Law School, 20% of the graduating class doesn't go into corporations or law firms. They go into public interest law and the university has soul searching seminars because that percentage is so low. Percentage of computer science graduates is probably zero. Right? We need to fix that. And that's more than just, you know, every Google or needs to do an internship because there aren't spaces for those people. So we got to fix the supply, got to fix the demand and the ecosystem to link the two. And this is of course the bigger than security. I think pretty much all the major societal problems of this century have a strong tech component or climate change, future of work, farm policy. And we need to be in the room or bad policy happens to us.

Speaker 1:          35:06          So that's my talk. There's of course a lot more in the book that I didn't say and I'm happy to take questions.

Speaker 2:          35:13          [inaudible] alright.

Speaker 3:          35:24          Do you imagine that some of the sociopolitical, uh, things that we're seeing crop up fit within this framework? Uh, or do you think that that might be an entirely separate, that needs an entirely separate set of solutions?

Speaker 1:          35:40          I think it's related. I mean there's problems, I'm talking about a pretty purely technical, the problem is of a internet as proper as a propaganda vehicle are I think much more systemic and societal. Uh, I do blame surveillance capitalism for, for a bunch of it, right? The business model that a prioritizes engagement rather than quality has learned that if you're pissed off, you stay on Facebook more so. So I think there are, are, are pieces that fit in. So some related, some different,

Speaker 3:          36:13          I mean, you shouldn't be talking a lot about policy that the United States, it's to some extent the EU can do. But I wonder what do you think will happen as policy everywhere? We policies local, the rns global. Right? How's that going to play out?

Speaker 1:          36:26          So I think that never goes away. And some that's going to be the rising tide. I talked about that especially with, you know, less about privacy. But when you get to safety, I think it's more likely that we benefit from a European regulation that ensures that, you know, the smart vacuum cleaner you bought can't be taken over by somebody and then like attack you and trip you. Right. Well W W we likely the benefit of that more than, look, you can't have a microphone on the thing. Okay. Uh, we have to assume that there will be malicious things in whatever system we have. And so if we have a US only regulation, it'll clean up a lot of the problem because Walmart and we'll be able to sell the bad stuff. But you can still buy it and mail order from alibaba.com right? So there will be some stuff in the network that is malicious, much lower percentage is your problem. We're still going to have to deal with that. And I don't think it ever goes away because we're not going to have a world government though will be a jurisdiction or they will be homebrew stuff that doesn't meet whatever regs we have that. But that will always happen.

Speaker 3:          37:34          You talk about the need for intelligent technologists to get involved with main policy, but there are only so many hours in a day and we'd probably, most of us would be taking a huge pay cut to go work in government and under expertise there. So what, how can we fix the incentives there?

Speaker 1:          37:49          So some of it is desire. I mean, I know ACLU attorneys that are making a third of what they would make at a big law firm and they get more resumes than they have positions, right? So it works in law. The desire to actually make the world better turns out to be a prime motivator. So I think once we have the ecosystem, we will get the supply. I think that enough of us will say, you know, we've had great careers. We're going to take a break or are we going to do something before we go work at a startup or a big company or maybe that mean there will be a use for sabbaticals like you see in law firms or you know your bits of pro bono work, you know like a 20% project. So I mean, yes you will. People will be making less money. I don't think that is going to harm the system or you can, we just need to get the system working.

Speaker 4:          38:42          The most jarring thing I saw you write as a Googler was a date day is a toxic assets.

Speaker 1:          38:50          Uh, what, what can I know a lot of. So the promise of big data has been save it all figured out what to do with it later and that's been driven by the marginal cost of saving it has dropped to zero is basically cheaper now to save it all and to figure out what to save. This storage is free processing is free, transport is free. But it turns out that data is a toxic asset that for most companies having it is enormous liability because someone's going to hack it. So it's going to get stolen, you're going to lose it. And I think we need to start talking about data not as this sort of magic goodness, but it no decays and value the end that there are dangers and storing it or the the best way to secure your data is to delete it and you're going to delete it if you don't know if you know you don't need it.

Speaker 1:          39:54          I swear I've seen lots of studies on data, on and shopping preferences and it turns out some pieces of data are very valuable in a lot of it just isn't very valuable. So is it worth the extra quarter percent of accuracy to have this day that is potentially dangerous and we'll get you find or embarrassed and you'll take a stock, takes a hit if it gets stolen. So I think we need to make more of those decisions that the data is radioactive. It's toxic, right? We keep it if we need to, but if we don't, we get rid of it and we figure how to get rid of it safely and securely. Hey, I mean take, oh, I don't know ways, right ways is a surveillance based system. Very personal data, but probably only valuable if for like 10 minutes or at least you know, can can be sampled in lots of ways. I can treat that data understanding it's a toxic asset, get my value at much less risk to my organization. And that's what I mean by that.

Speaker 4:          40:54          It's interesting that there are ways to anonymize stuff. Uh, but there seems to be no demand and no supply. He said there are marginally more expensive to do federated machine learning than do everything in the center. But companies don't care. And consumers a decidedly. Yeah,

Speaker 1:          41:13          consumers, consumers don't care. That's why, I mean you need these decisions made not by consumers, but by citizens. Right? Consumers don't care, right? Consumers are buying the big Mac at 10% off because you was truly don't care. The point of purchase, nobody cares. At the point of reflection, people care a lot. And that's, you don't want the market doing this. You want us as our best selves doing this. So and about an entity, it is harder than you think. You most of our ways of Anna and anonymizing data fails. It is a very hard problem. We don't, the anonymity research is, is, is really, I mean the breaking any research is very good these days and outstripping the anonymity research. Go to the next,

Speaker 5:          42:03          one of the things that I'm thinking about is a lot of times when you see like a big vulnerability, so say there's a big operating system vulnerability, it's actually a genuine mistake of it's not that someone, you know, put it in there on purpose, it's they missed something. So how does regulation solve that problem of surely you could have some great regulation in place that something supposed to be done a certain way, but oh, the implementation was slightly offer, slightly broken. How do you fix that? You know, genuine mistake. Even if they were trying to do what the regulations specified as this would be a secure system.

Speaker 1:          42:31          So you'd be surprised. But a financial motive [inaudible] money motivates companies. If companies will be fine, a lot of money, if their employees make a mistake, they figured out a way to their employees to make fewer mistakes.

Speaker 5:          42:44          But doesn't that only take effect after the mistake has already been, you know, like the evil has already been,

Speaker 1:          42:51          but there's a deterrence effect. Okay. And so, so yes, I mean like arresting someone for murder only takes effect after he's done murder. But the goal is that the threat of being arrested for murder will keep you from hurting someone tomorrow. And so, so we, we want this deterrence effect, uh, how to produce software mistakes. We actually know a lot of techniques that pretty much all software manufacturers and never do because it would be slightly more expensive. But if it's a lot more expensive not to do them, it's suddenly the math changes. And I need the math to change. I need security to be cheaper than insecurity right now. The market rewards, let's just take the chance, right? You know, let's hope for the best. Okay. And then it, no industry, I say this already, remind me. Yes, no, no. Okay. No industry in the past a hundred and something years has improved security and safety without being forced to cars, planes, pharmaceuticals, medical devices, food production, restaurants, consumer goods, workplace. Most recently financial products, the market rewards, doing a bad job, hoping for the best. And I think it's too risky to allow that anymore.

Speaker 6:          44:17          Uh, so you mentioned that you were embarrassed by the Zuckerberg hearings and I was embarrassed by the questions at the Zuckerberg decides to be fair look, congressmen embarrassment. So I, yeah, so I, I so I assumed correctly, I assume were embarrassed by the senator's. Yes, yes. Whereas I have the opposite problem. I was embarrassed by it. Zuckerberg, what, what I give me fair. There's a lot of embarrassment to go around, but we can both be writers. Stan. Yeah. But I, I have a serious point here though, is that, uh, while the senators don't know about tech, I think the tech doesn't know about law ethics, political science philosophy. Like, do you think mark Zuckerberg can even teach an introductory college course on free speech? Like has even read like what anyone has ever said about it. So like shouldn't we all be learning

Speaker 1:          45:10          about the world around this? This has to go in both directions. Yes. Right. I want techies and politicians. I won policy people in tech companies. So yes, I think we need both. We need both sides talking to each other. Right. And so, so I agree with you 100% good. Okay. Alright. So right now I teach Internet security at the Harvard Kennedy School at a public policy institution. So I'm trying to go, you know, push people in that direction. At the same time, there are people at Harvard computer science department trying to teach policy issues to go the other direction. I think you probably know a lot of stuff. I know, but I'm not in charge.

Speaker 7:          45:57          You mentioned like sort of shock events, uh, as, as things that drive government policy. And I thought the example of nine 11 was like instructive maybe in a way you did or did not intend in that like the government response to nine 11 was to launch two illegal wars and create a surveillance state that violates our civil liberties on a date. So I did intend, not sure. That's a good, like I guess I'm curious how you, terrible life. So how do you see the reaction to the reaction to the mounting threats in technology as being different? What's going to prevent the same sort of thing from absolutely

Speaker 1:          46:28          nothing. Okay. And then that's the, and that's my fear that something bad will happen. Congress will say something must be done. This is something, therefore we must do it. All right. So my goal of having this conversation now before this happens is that we will, as a community figure out what should be done when we have the luxury of time and insights and patients. And because I agree with you that there is a disaster, we will get a disaster as a response and it will be just as bad. So let's get ahead of it this time. Let's do better. How do you,

Speaker 8:          47:11          uh, envision preventing everything a degenerating to the lowest common denominator? Like you said, client side, you can't really restrict people from doing what they want. You know, even if we say, okay, any company that wants to make money in the u s has to follow these provisions, I'm just going to encrypt my data and send it to Ali Baba Plate. It's a third of the price of Google translate. But they all my data,

Speaker 1:          47:30          like how do we prevent this? Is there anything we can do? You know, some of it in the answer's gonna be no psalm. It's gonna be yes. So if you think about uh, other consumer goods, we do make it hard for consumers to modify something. It's actually hard to modify your car to violate emissions control, right? You can do it, but it's hard. And then we try to have spot checks. You can imagine some sort of regime, you can imagine some system that tries to maintain security anyway cause it will be up and just a minority doing that. I think once we start hitting the problem for real, we'll come up with tech solutions and ways the system to self to watch itself, other systems to watch each other. What can we do this noninvasively? I think we have to figure it out. All right. So I don't have the answers here, but I mean these are certainly the problems.

Speaker 3:          48:24          I, uh, I really liked your phrasing of the problem of we need to give up on offense so we can go all in on defense. And, um, I, I think it's pretty clear to me where a lot of the offensive focuses in terms of law enforcement, but I think one thing that sort of remains, uh, mostly an unknown is on the military side and how there is a ton of investment in military oftens of stuff. We kind of know, you know, a little bit more maybe about like what Russia and China are, are using offensively against us. We're not seeing the good stuff yet. I hope not anyway. Right. Yeah. Uh, but, but do we, do we have a sense of, of what the military did, the u s military, let's say, would be giving up to give up this, this offensive, uh, idea. And, and uh, I don't know how willing they would be to go with that direction. They wouldn't

Speaker 1:          49:23          be willing, but it's not their job to be willing. I mean it's why you don't want the NSA in charge of your privacy policy because it's not their job. You know, we need people above the military, the NSA to make these trade offs because they are security versus security trade offs. Right. Is the security we get from being able to spy out and hack the bad guys greater or less than the security we get from the bad guys being unable to spy on and hack us. I that's great. It's so, I mean it's security versus surveillance was the wrong way to describe it as security versus security. So someone above this, the, the military needs to decide that it can't be the military because the military is not in charge of overall policy. They don't charge of the military part. And what we know about the capabilities is very little. I mean we get some shadows of it here or there and it seems to be, you know, we, you know, on the one hand cruder than we were then we'd like it to be. On the other hand, you know, stuxnet was pretty impressive. You know, in general the stuff you see as sort of the minimum tech, it has to be to succeed. Right? This sort of this myth of this, these, uh, you know, super powerful cyber attacks that basically, you know, one I owed them more than just barely necessary to succeed.

Speaker 1:          50:44          You know, you don't need to do more if he could take out the DNC with a pretty sloppy phishing campaign. And why, I mean, why bother using your good stuff? So a lot we just don't know

Speaker 3:          50:57          at risk of revisiting an earlier question. Um, I was interested in what you thought about, so like one of the things that's often you'll see cynical in the finance industry is that they think that people in finance can out maneuver all the people who are regulating them in part because of the lesser paid. So I was wondering if you could revisit that cause I think La has maybe the exception because it's a little bit more directly related to human rights and things like that that,

Speaker 1:          51:19          yeah, I don't know. I mean, certainly, uh, I worry about regulatory capture, uh, regulations being evaded. I think the, all of those are real risks. I mean, this is not a great answer. I have, it's just the best one I have.

Speaker 2:          51:32          Yeah.

Speaker 1:          51:33          Cause I don't see any way to put a backstop against, so this massive corporate power other than government power. Now in a sense out of Keto one, either power, but tech naturally concentrates power at least as it's configured today. So that's my missing piece. I think you're right that that is a serious problem in worry and something we just have to deal with. Okay. Policies iterative as techies. It's hard to, to accept that we like to get the answer right and then implemented where's policy gets the answer. Like slightly relaxed wrong every few months.

Speaker 2:          52:12          Hm.

Speaker 1:          52:14          But that's, you know, that's the way it works. I mean, the real question is can we do this at tech speed? And that it really is, I think it's an open question so that I'm going to end. Thank you all. Thanks for filling the room. Thanks.

Speaker 2:          52:27          [inaudible].