Speaker 1:          00:06          Hi everyone. Welcome to toxic Google from Cambridge, Massachusetts. And today we're very happy to host. Sarah walked her batcher and her new book, technically wrong. Uh, so Sarah has been a web designer and Ux consultant and so she's, uh, in the industry but not necessarily completely of it. And I think this gives her a great, a fresh perspective on a, on some of the issues that, uh, that face our industry and society today. These are increasingly familiar themes, um, but I think that's exactly as it should be, um, given, uh, how critically important they are to our industry and indeed to the world as a whole. Now. So welcome Sarah.

Speaker 2:          00:49          Okay.

Speaker 3:          00:51          Hello. Thank you all for coming. Um, so I'd like to talk today first off about something that is not in the book at all and it's not in the book because it happened last week. Um, and some of you might be familiar with it. Um, it's a friends, the mini cupcakes. And I'd like to talk about this both because I'm here at Google and because I think this is a really good example of some of the things that I think are not being talked about enough. So many cupcakes as I'm sure you all know, um, where the topic of discussion because of a Google maps update that went out to, um, a selection of iPhone users that started, um, showing the number of calories that maps thought that you might burn if you walk somewhere instead of taking some other form of transit. And also how many mini cupcakes that might be in terms of your calories burned.

Speaker 3:          01:53          Now, almost immediately after this launched, some people started talking about it. One of them was a woman named Taylor Lawrence who was a journalist and, uh, she did not like this update, so she had some comments. So one of the first things she said was like, oh my God, if you click on walking directions, it's going to tell you how much food this burns. She goes on to talk about, um, there's no way to turn this off. Then she says, do they not realize how triggering this is for people with eating disorders? And like this just generally feel shamey bad. She goes on to talk about how calorie counting maybe isn't even a good thing and there's a lot of disagreement on that. Then she talks about how this is even perpetuating diet culture. So on and on she goes up until about 9:00 PM, this is about an hour of tweets that she has sent kind of walking through all of the reasons that she thinks that this is a problem.

Speaker 3:          02:49          She says there's no way to turn it off. This can be dangerous. People with eating disorders, this feel shamey average calorie counts are wildly inaccurate. Not all calories are equal. A cupcake is not a useful metric. What is the mini cupcake anyway? Who's mini cupcake? Are we talking about, um, pink cupcakes or not a neutral choice? They have did not her exact wording, but you know, they have social, cultural and coding to them that they would be perceived as being more feminine, more White, more middleclass, and that this perpetuates diet culture. So that is sort of the summary of her hour of evaluating this product choice. And I look at something like that and I think, okay, it took her an hour to document these floss. It took three hours after she started tweeting about it. And I'm sure not only because she was tweeting about it for the feature to get shut down until I ask how much time was invested in building that in the first place?

Speaker 3:          03:48          I suspect more than an hour, more than one person. And why? And nowhere along the line did this come up, or if it came up, why wasn't it addressed? Why didn't people take it seriously? Um, whoever brought it up, if they brought it up, like why was their voice not valued in that discussion? Um, why do people not think that this matters? And this is this one tiny thing, right? It's a fricking cupcake except that it's not this tiny thing. What I think it is is it's actually a perfect encapsulation of some of what I see going wrong and all kinds of tech companies certainly at Google and elsewhere all over, um, in almost every single aspect of our lives. We have tech companies that really end up thinking that they understand people and making choices that make a lot of assumptions about people that leave them too narrowly focused on whatever they thought their goal was.

Speaker 3:          04:44          And leaving so many people out along the way. And you end up with this kind of tech knows best paternalism where a tech company is presuming to know what people want and what people need and that when you're mapping something, what you really need is calorie counts and to know how much food that is. Because after all, let's talk about obesity in America or whatever. And so what you end up having is this really narrow understanding of what normal people are. And I think that that goes very deep in tech companies and, and the people who work there, this idea that we understand what normal means, what users want. You can see it in so many different examples. Um, and when you work on something like this, all of your friends send you screenshots. So I've got a lot of really, really fun screenshots from people. One of them came from my friend Dan Hahn. Uh, this is an email that he got from his scale, which is a thing that you get from your scale. Now [inaudible] this email is saying, don't be discouraged by last week's results. We believe in you. Let's set a weight goal to help inspire you to shed those extra pounds. But, and it's, this is not addressed to Dan, my friend. This is addressed to Calvin. Calvin is his son who was a toddler.

Speaker 3:          06:01          And every single week Calvin weighed more weird and the scale didn't get that. Like, just didn't understand that that actually could be a perfectly normal and natural thing because the only thing that the product had been designed to do was to congratulate weight loss. And it's really funny when you get this message for your toddler because your toddler isn't internalizing this particular email, but it gets progressively less funny for a lot of other people. And that this was another message on a push notification that he got. Um, congratulations. You've made a new low weight. This one actually was for Dan's wife and she received this right after she had a baby, which, I mean, I guess it's true, but that wasn't something she wanted to be congratulated on. And in fact, I know a lot of people for who a message like this is not good at all. People who've suffered from eating disorders. For sure. I have a dear friend who spent a long time in treatment for eating disorders and this kind of message, this is exactly what she does not need, right? She does not need to be congratulated, but it's also a lot of other people who have like chronic illnesses and where their chronic illness might mean that hitting a new low weight,

Speaker 4:          07:20          okay,

Speaker 3:          07:20          it means they're sick. It's a sign that something is going really wrong with them. And there are just so many reasons that notifications like this don't work, that product decisions like this don't work for real people.

Speaker 4:          07:33          Okay?

Speaker 3:          07:34          But yet, so many examples of people making product choices that are like this, that are exclusionary and they're alienating. For example, this is a message that a friend of mine, Aaron got from Etsy. She has the Etsy app install on her phone. So she got this push notification because obviously Etsy wants to move some Valentine's Day merchandise. It says move over cupid. We've got what he wants, shop Valentine's Day gifts for him. And Erin's partner is a woman and she looks at that and it's a stupid throwaway message. It's just a little marketing message. Just a little bit of copy except that she looks at that and she says, this is really alienating. Like, did you not think that you have gay customers? Um, how did this not cross your mind? And if this doesn't cross your mind on this little tiny thing, where else has that not cross your mind? And is this a company I really want to spend money yet?

Speaker 4:          08:26          Okay.

Speaker 3:          08:27          We also can see these kinds of failures to understand real people in places that are more worrisome and more problematic. So last year, um, Jama internal medicine released a study that showed that the smart phone assistance from a bunch of different manufacturers, not just apple, but we have Siri here as well as from Google, from Samsung, from Microsoft, that they weren't programmed to help during crisis. They didn't understand a lot of inquiries related to crisis, including a lot of things related to things like rape, sexual assault or domestic violence. Like my husband is hitting me. And

Speaker 3:          09:04          so I went in and I started saying, well, let me go in and like fee what I get, right? So I went in and I asked him questions and I took some screenshots of what I got and what was really alarming about it wasn't just the Siri didn't understand because I figure series not going to understand everything, although I think that it could do better there. But also the Syria was responding with like jokes or a little digs, right? Like thinking it's going to be clever and funny and I'm getting, it's not a problem. One, can't know everything. Can One

Speaker 4:          09:33          and you know,

Speaker 3:          09:35          thing that made me upset about this was that this wasn't exactly new. Um, because back in 2011 when Siri was brand new, there was a whole spade of bad bad press for apple because people are found that if you asked the things like, uh, or said things like you want it to shoot yourself, it would give you directions to a gun store. There was another example where somebody was saying, you know, something about jumping off of a bridge and it told them or the nearest bridge was, and apple was like, oh, whoops. Like we don't want that to happen. And so they went and they fixed it and they said, okay, well now if you say something that, that Syria identifies as being potentially suicidal, what we're going to do is we're going to surface up some information about the national suicide prevention lifeline. And so you can like kind of click through to them so you can get help in crisis. And so they did that in 2011 but I look at that and I wonder, well, okay, if you knew that in 2011 why five years later, has it not occurred to you to go beyond that one thing and to make changes to the product as a whole? Why is it still more important to program jokes into the interface then to think about other types of scenarios where somebody might turn to their device

Speaker 3:          10:49          and in fact in this particular example when I started talking about this, I had some folks say things to me like, well you're an idiot if you use your phone after being sexually assaulted, you need to go to the police or various other statements like that. And I thought, you know what? I don't actually care what you think about what somebody should do after they've been sexually assaulted.

Speaker 4:          11:15          Okay.

Speaker 3:          11:17          What I care about is the fact that people are using their phones during this time. There is a quote from my friend Karen Mcgrane who is well known. It's sort of user experience and she didn't want a lot of work on mobile content. And when she started doing that, she would say to people, you know, you don't get to decide what device people use to access the Internet. They do. And I think that it's a similar sentiment here. You don't get to decide what circumstances somebody is going to be and when they use your technology, you don't get to decide that somebody should or shouldn't use their smartphone assistant when they're in crisis. They're going decide that the only power that we have in tech is to figure out, well how do we respond to that? Do we choose to help them or not? Do we choose to anticipate that or not?

Speaker 3:          12:02          How do we deal with that? Because you can think that it's a bad idea all you want, but people are doing it. And those people, oftentimes our kids or youth who are more comfortable talking to somebody on the screen than they would be to go to somebody in real life. And what do you want to do about that? And so I look at this and I think, you know, there's been a lot of opportunity to improve this beyond just let's change one individual thing, but we haven't really seen that. Instead what we have is we have apple going in and saying, oh yeah, yeah, yeah, okay. We don't want that to happen. So we'll partner with the rape abuse and incest national network rain and we will do the same kind of things we did before. We'll have a little like, you know, if you have an issue with sexual assaults, you can go to the lifeline there.

Speaker 3:          12:49          And I think that that's a good change, but I don't think it's enough. Right? Because I don't know how many other types of scenarios, Siri is not going to understand, and I don't expect Siri to be perfect by any means, but I do expect for it to understand that it's going to have scenarios that are negative and straight up terrifying that people are going to use that device for and to be able to anticipate that and not have these terrible breaches. But I find that over and over, tech companies really aren't thinking enough about the ways that their design decisions and tech decisions can break. Grief is a great example of that. Um, tech is very good at focusing on things like delight tends to be really bad. I focusing on things like grief particularly because, um, we spent a lot of time, I know on my end of the the tech world where we talk a lot about UX and content.

Speaker 3:          13:42          We spent a lot of time talking to people about things like personality and we've got to make this human make it friendly. And that gets translated into let's make this overly clever. And as a result you get things like this. Um, we've got timehop which posted, uh, to this person who was sharing about a memorial service. They resurface that post and said, this is the really long post that you wrote in the year of 2000 and whatever and it's snarky, rude. Or you've got, you know, the millions of different places where you've got could gift say it better. Could you be more clever or medium, which um, which medium has removed this string actually from their system. But when you posted a new story on medium, they would send you these little fun facts along with their update on how your story is doing and they're trying to make it kind of seem like it's okay, your story is just picking up steam but you don't really want to fun fact on a post that's in memory of a friend.

Speaker 3:          14:40          And in fact maybe the worst break of this sort is one that some of you probably heard of. Um, it happened to Eric Meyer and he is a longtime web developer who I've known over the years. And it was one of the things that led us to write about called designed for real life together a couple of years ago. And what happened to him was that he went to Facebook on Christmas Eve in 2014 and he was expecting to see you the normal family, well wishes, stuff like that. But what he thought instead was this, this is year end review. And what year in review did was surface your most popular posts, images, videos from the year and package them up in a nice curated little collection for you. And then they took that and they put it into their little wrapper, right, with balloons, streamers, people dancing, and it says, Hey Eric, here's what your year looked like.

Speaker 3:          15:36          And in the center of that was the most popular photo that he had posted all year. That photo is of his daughter, Rebecca and Rebecca had died of been aggressive brain cancer on her sixth birthday. Of course it was the most popular photo he posted all year. It was also the worst year of his life and the worst moment of his life. And so Facebook, it kind of put that back in front of them and this pepe little package and said like, here you go. Even though he'd been avoiding this feature, even though he didn't want to use it. And I will tell you, Eric was gutted by this. It just hurt very badly. And he wrote this blog post about in the blog post went viral and all of a sudden it's getting reposted to slate, etc. Etc. Etc. And Facebook reaches out and the product owner apologizes. I mean, they didn't want this to happen. Nobody there wanted this to happen. So they apologize and they say they're not going to do this again. And the next year when they did your interview, they changed the design of that feature. So it doesn't take your content and put it into a new context.

Speaker 3:          16:41          But now it's almost three years later. And in fact, Facebook is still doing basically the same thing. So this is an example from just a couple of weeks ago. Um, Olivia Salon is a journalist for the Guardian and she had posted a photo to Instagram, um, that was a screenshot of an email she'd received that was full of rape threats. She posted it to Instagram because she wanted to show the kind of abuse that women who are public online often get. It was also a highly commented on photo on her Instagram account. Well, Facebook owns Instagram. Facebook would like more Facebook users to use Instagram, so what Facebook does is that will take your Instagram posts, insert them into an ad for Instagram and then put that add on to your friends' Facebook feeds. And so Facebook did that with this image from her Instagram account and so her friends started receiving pet. The odds for Instagram showcasing a rape threat because you see they treated this problem with your interview is sort of an isolated incident. They fixed it and they moved on, but they didn't think about the overall problem of changing the context of users' content and how this could go wrong and who this could hurt.

Speaker 3:          18:00          I think this quote from Zane up to Faqih digital sociologist really sums it up effectively here. She just started talking about this the other day on Twitter. He said, silicon valley is run by people who think that they're in the tech business, but they're in the people business and they are in way over their head.

Speaker 2:          18:17          Yes,

Speaker 3:          18:18          and I think that this is true. I think when we have spent a long time assuming that the most important thing that we work on is technology, but in fact we are affecting people's lives in dramatic ways that we could not have even envisioned a couple of decades ago. And we haven't really caught up to that. We haven't really taken that to heart. And we can see that playing out in so many ways from all of these little interface examples, too much, much deeper places. For example, we see that playing out frequently in anything seemingly related to image recognition and um, image filters. For example, you may have seen this on snapchat. Snapchat last year released something that it called the anime filter only. It didn't look like any animate I have ever seen what it looked more like was something that you probably wouldn't see today. Uh, that's Mickey Rooney playing I why you Neeoshi in breakfast at Tiffany's that is making Rooney doing what we would now call yellow face. He's dressed up as an Asian person. And what that filter did was basically the same, here's your slanty eyes and your buck teeth and you know,

Speaker 2:          19:31          okay,

Speaker 3:          19:32          race is not a costume that you get to put on. And we'd probably mostly know by now that you don't dress up as an Asian person to go to a Halloween party. And yet nobody thought that it might be a problem to dress somebody up as an Asian person in their selfie on snapchat after this happened. Snapchat wouldn't apologize for it. And in fact, um, it wasn't even the first time they'd done something like this because just a few months before that on April 20th, uh, they released a filter. They called Bob Marley and it gave people black skin and dreadlocks. And again, it was roundly criticized as being a digital blackface, but they wouldn't actually admit that this was a problem.

Speaker 2:          20:14          Okay.

Speaker 3:          20:15          And it's certainly not just snapchat because just this year face app made this a splash for a little while where you could, you know, make selfies that we're a younger version of yourself or an older version of yourself or a hotter version of yourself. And for the hotness filter. Um, what people started realizing was that it made their skin lighter, it changed their features to look more European. And what's interesting about this is that face app admitted what had gone wrong. The CEO said, you know, we're deeply sorry for this unquestionably serious issue. It's an unfortunate side effect of the underlying neural network caused by the training set bias, not intended behavior.

Speaker 3:          20:57          Now I read that and I think, okay, and it's an unfortunate side effect. It's not intended behavior. Actually that's not true. It's definitely intended behavior in the sense that you took an set of photos to train the algorithm on what beauty was or what hotness was. Right. And those were photos of white people. Like you've just said, you had bias in that training dinner. So you fed the system a bunch of photos of white people and said, here is what attractiveness looks like. And so the algorithm worked as intended and found patterns in those pictures of white people and it applied them to them to these selfies. So the algorithm was working as intended. You just fed it by a state. At the beginning and it's not an unfortunate side effect. It's in fact entirely on you and entirely something that you could have anticipated.

Speaker 3:          21:53          But they apologized, they renamed it, they said they were going to fix it, but then just a little later, this August they released this new filter. This one you could literally try on races, Black, Caucasian, Asian, Indian, and so you can just select them and show yourself as different races. And we're not pleased with this. Um, is this was not a good idea. Um, and immediately they decided they would take this down. They apologize. He said the new controversial features will be removed in the next few hours. And I read that and I think controversial is not the right word for this. Controversial implies that, you know, like, well, you know, got to hear both sides, some disagreement. Know what calling, calling and controversial. What that does is that it doesn't acknowledge history. It doesn't acknowledge that there is a tremendous body of work. Like, have you ever heard of critical race theory?

Speaker 3:          22:55          Maybe you should before you start messing with races and filters. Um, there's an entire body of work that's looking at like how race functions in culture and so it is not enough to write it off as a controversial feature. It's not like somebody didn't like your new logo and they had like and people wrote to medium posts about it. No, this is an actual problem and it is well documented and so it gets reduced to just a disagreement. And the other thing about this is that this is not news like face up could have known better because not only had a snapchat already had some similar issues, but we can go back to the year before to 2015 and we can talk about something that happened here at Google, which I'm sure most of you are familiar with. When some photos, a whole series of photos of black people got tagged by Google photos, auto tagging as gorillas.

Speaker 3:          23:46          Now the fact that they weren't auto tagged with something that has a racial slur is probably the reason that this blew up really big. It's definitely the reason this blew up really big. This was all over the media and that's what people talked about. But the thing about this example that is much more important I think, and that wasn't talking about nearly as much was why this happened. And so I kind of dug around at this example and I found what a Yonathan Zanga had said to Jackie Lc ne who was the guy that this happened to after the fact. He seems like a wonderful guy. He seems to be trying very hard to get this stuff right and yet you take a look at what he wrote here. He said, you know, we're working on longer term fixes around both linguistics words to be careful about and photos of people.

Speaker 3:          24:32          Okay so we're going to be more careful applying tags that could have kind of questionable context but also image recognition itself, eg better recognition of dark skin faces. And what that is acknowledging right there is that that product went to market not as good at identifying dark skin faces as it was at identifying white people. So the specifics of that miss tag happened to look really bad. But the underlying issue was that it was more likely to miss tag people of color. And I think, well, how did you get a product to market that wasn't that good at identifying dark skin faces? Well, that's not just Google because failing designed for black people, that's not new. So back in the 50s, Kodak, uh, first started allowing people who were not in Kodak labs to produce their film. So you could go to a mom and pop to kind of get your film developed. And so what they did is they started sending these little packets to the photo lab technicians to use to calibrate skin tones and light. And they called them surely cards named for the first woman who ever sat for them, who was a Kodak employee. And for decades they sent these cards out and they were always the same. The styles would change, the woman's sitting for them would change, but they were always culturally cards. They always said normal on them. And they always showed a white person.

Speaker 3:          26:01          And so for decades, this went on and a black photographer wrote this piece on buzzfeed where she talked about this and she talked about her experience feeling like film was never developing correctly for her skin tone. And she said, you know, with a white body as a light meter, all other skin tones become deviations from the norm. It turns out that film's failure to capture dark skin, it's not a technical issue, it's a choice. And in fact, uh, a researcher went and talked to a bunch of people who used to work at Kodak back in the 70s when this started to change. And what she learned was that it wasn't that they decided to be more inclusive. It was that, uh, furniture makers and chocolate manufacturers were complaining and they were complaining because this film was not properly, uh, showing the difference in different woodgrains and the difference in different chocolate varieties like milk versus dark. And that that is actually what led to that product shift.

Speaker 3:          27:00          And so here we are again over and over again seeing tech companies reenact these kinds of choices, right? It's not a technical issue. It's a choice. And what I look at that I have no choice but to say this is literally what it means when you say white supremacy and people didn't want to hear those words oftentimes because when you say white supremacy, what people want to to be talking about is like scary guys with swastikas and in fact those are terrifying people. But when you talk about white supremacy, what you're really talking about is simply putting whiteness first. And so if you decide you're going to make a product that works better for white people, if you decide that you're not going to include people of color in your training data, you have literally decided that white people are more important. You have literally enacted white supremacy. That is uncomfortable. That is not a conversation most of us want to get up in the morning and have, but the thing is, that's what we do in tech. That is what we enact every single time. We don't specifically work against it.

Speaker 3:          28:09          We embed it because it's already present in our culture, not because tech created it, but because it already exists. And so we just reenact it over and over again unless we question it. And the thing is, we do all of that well insisting over and over that. No, no, no, no, no, no, no, no, no. We're not racist though. We're, we want to include everybody and you know, we talk about tech as if it's some kind of Utopian society that's possible, but we're not necessarily doing that difficult work of looking at the ways that we continue to send her white people.

Speaker 3:          28:44          And that's, I think, bad enough when you're talking about something like photo tagging. But it of course gets even more worrisome the more that tech and beds into other aspects of life. For example, some of you may have heard about some software called compass. Compass stands for correctional offender management profiling for alternative sanctions. It is made by a company called North Point and it is being used in, um, courts around the country to decide how risky somebody is to commit a future crime. So it provides criminal recidivism scoring. And last year, Propublica did a big investigation into this software. And what they found was that it had some real biases embedded within it. So for example, you'd have these two men here and your left, you've got Bernard Parker. In January of 2013 he was arrested in Broward County, Florida for marijuana possession. And then on your right you have Dylan forget.

Speaker 3:          29:43          And in the same year, one month later in the same place, Broward County, Florida, he was arrested for possession of cocaine. Now both of these men had a prior record. Uh, Bernard had resisted arrest without violence and Dylan had attempted burglary. But according to compass, these men did not have a similar profile at all because Bernard was labeled the 10 the highest risk there is for recidivism. And Dylan was able to three. And in fact, Dylan happened to go on to be arrested three more times on drug charges. Bernard was not arrested again at all. And what propublica found was that that story was playing out over and over again with the software. So the software was particularly likely to falsely flag black defendants as likely to reoffend. So 45% of those who were labeled high risk did not reinvent versus only 24% of white defendants. Meanwhile, the opposite was true for low risk.

Speaker 3:          30:49          48% of white defendants labeled low risk. Did re-offend in 28% of black defendants be offended. So what you have is a system where over and over again when the system gets it wrong, the people, it is wrong for the people who are harmed by that are much more likely to be black people. Now, after propublica did this research, some other researchers from Stanford started digging into this more closely and they looked at what north point the company who makes a software set about it versus what Propublica said, and they found an underlying problem. And the problem wasn't a technical glitch. The problem was different ideas about what fairness means. So at Propublica they said, this is not fair because you have a group that is disproportionately harmed by inaccurate predictions. What northpoint said was that their algorithm had been tuned to parody of accuracy at each score level.

Speaker 3:          31:50          Meaning that if you scored a seven, whether you are black or white, you were roughly the same likelihood of committing a future crime. So 60% of white people and 61% of black people who scored a seven would go on to commit a future crime. And they said that was equality. See? Because it was equal across races. But the problem is the researchers at Stanford who looked into this further, they said, well, you can't have both. You cannot define fairness that way and fairness this way. And the reason you can't do that instead, it's mathematically impossible because you have different base arrest rates across races. So then you have to talk about, well, why do we have different baseline arrest rates? Well, we can look at a lot of reasons why the incarceration of black people in this country is tremendously higher than white people. Um, I will not go into all of them today, but we can talk about things like the different application of drug laws of crack cocaine versus regular cocaine for decades. We can talk about the ways that black communities tend to be policed more than white communities, even though there are statistically similar likelihood of crimes being committed in both of those places. We can talk about a whole lot of different historical factors that might've led to this. But the other thing that we can talk about is what went into these scores that wasn't just past criminal profile. Because if you remember those two examples we talked about,

Speaker 4:          33:13          okay,

Speaker 3:          33:13          Dylan and Bernard, they had really similar criminal profiles, but compass doesn't just care about that. Compass cares about a lot of other factors. Uh, they, I think there's 137 different factors.

Speaker 4:          33:27          Yeah.

Speaker 3:          33:28          Is there a lot of crime in your neighborhood? Is it easy to get drugs in your neighborhood?

Speaker 4:          33:34          Okay,

Speaker 3:          33:35          that's your father ever arrested? Were you ever suspended or expelled? All of these questions go into factoring what your score is going to be. And the thing is, these questions are not neutral. These questions are very, very much tied to race and class in this country. If you are poor in America and if you're black, you were more likely to grow up poor. You probably lived in a neighborhood that had more crime. You probably lived in a neighborhood where you had to move around a lot.

Speaker 3:          34:11          If you are black in the United States and incarceration rates are what they are, the likelihood that somebody in your family has been arrested is way higher. And if you think about it, the only reason that you build software like compass in the first place is because you think that there is some kind of human bias that you're trying to get rid of, right? Like because you don't want to just leave it to individual judges to make these decisions. Otherwise, why would you build the software? So the software is meant to make it less biased, make it fairer for people and yet we're not questioning the underlying information is going into this model and saying like, well, wait a second. What was the historical context that led to this? Do we want to consider things like whether you've been suspended from school, when we also know that black kids are much more likely to be suspended from school for the same infractions as white kids. Are we going to ask those questions or not? And I think that this is the extreme example of what happens when we assume it's something that is technical, is also neutral.

Speaker 3:          35:15          And I think we do this all the time because we're still used to thinking about things as technical problems to be solved. But of course this is not neutral at all. The information that went into that algorithm is anything but neutral and it needs to be interrogated and it needs to be interrogated at a level that it's so much deeper than what most tech companies are prepared to do right now. And so what ends up happening is we have algorithms that don't eliminate bias. They just outsource it. And by that I mean you can make it the machine's problem. And so you as a human person don't have to be responsible for the bias. And we can all feel better about how unbiased we are because we're not making these biased decisions and not leaving it up to racist judges and we're letting the machine sorted out.

Speaker 3:          35:55          And out of that machine comes some nice clean little numbers, right? Three, seven, 10 charts, graphs. It seems so obvious, so clear. And in fact, I talked to designers a lot and I work with designers a lot in design is intended to make these things seem like facts like truth because we spend all of this time trying to make software easy to use. I mean that's most of what I've spent my life doing is trying to make things easy to use for people. And that's important and nobody's saying like don't make usable software. However, when you say things like compass is designed to be user friendly, even for those with limited computer experience and education, what you ended up doing is you end up making it feel inevitable, truthful, factual, so seamless and easy to use. You reduce really complicated stuff down to things that feel very palatable to people.

Speaker 3:          36:50          And we hear this over and over again across all kinds of different tech companies, right? We want things to be easy to use. We want things to seem right. That's certainly true at Google. Miriam Sweeney, who is, um, a library and information sciences professor at the University of Alabama, she wrote about this, you said, you know, the simple sparse design works to obscure the complexity of the interface making the result of pure, purely scientific and datadriven the insistence of scientific truth of algorithmic search has encouraged users to view searches and objective and neutral experience. Google explicitly wants people to think of search this way, right? Because he won't if he easy to use. And you want people to trust the results that they're getting. And that's not inherently bad to have people want, want people to trust the results that they're getting. However, we have to think about what are the ramifications of that?

Speaker 3:          37:39          How do people actually interpret that? And so what we have over and over again is technology that's not asking some deep questions about where data comes from or what the history is of a subject and design this, making those things feel neutral and feel factual. And so we recreate these toxic patterns over and over again. And I think we talk a lot about the ways that culture informs technology. I mean that's kind of a given, right? Like it's not as if racism was created in tech. Racism existed and what we have is tech that can end up reenacting it. But what I don't think we talk enough about is the way that tech also informs culture. The work that we do in technology is powerful. It impacts people's lives in almost every way. You can imagine it is embedded into almost everything you can imagine. It is often the first thing that people look at in the morning. The last thing that they look at it as night. And so when tech perpetuates bias, even of those tiny little levels, even at the mini cupcake levels, that is a problem because it's indicative of an industry that over and over things narrowly about people assumes that it's ideas are going to work for everyone assumes that the decisions that makes don't carry that much weight, that they're not that important and that this ends, it knows what's best for people without understanding people very well.

Speaker 3:          39:05          I think the tech is too important to our lives, to our personal lives, our social lives, our emotional lives, political lives to keep toying with people's lives without really deeply considering the consequences of that. And so oftentimes the examples I've shown you today, the companies behind them have treated them as if they are software bugs. You squash it down and you move on. The problem that we are facing are not just software bugs. What they are systems, there are systemic patterns and so they need systemic action and that is a big and difficult job and that actually is why I was so upset when I read the infamous memo.

Speaker 3:          39:47          I was pretty upset about the tenuous grasp of research in that memo. I was pretty upset about the ways that like tiny little differences in studies about gender were extrapolated into broad sweeping statements about women in technology. But when I got to the section where James Damore wrote that Google really needed to de emphasize empathy and said that being emotionally unengaged helps us better reason about the facts. I thought, you know, you have misunderstood the entire project here because that's actually the opposite of what needs to happen at a time when tech companies are deciding things like what we see during an election or building software that determines whether or not you're going to get a job or whether you can get a loan at a time. When tech companies are increasingly manipulating relationships and emotions and the person who designed the like button on Facebook has actually said they've read technology from their lives and their children's lives because they realized that they actually don't want that level of manipulation.

Speaker 3:          40:53          This is not the answer because when we start saying that we're going to deemphasize empathy and we're going to focus on reason. Facts. We don't ever answer some important questions like whose job is it to decide what fair means? Do you want some random engineer to decide what fair means? That is a big cultural question and it needs big discussions around it. Who's job is it to understand historical context? Who's job is it to know the history of race and policing in this country before making decisions that could affect people whose job is it to sit down and think through what the potential unintended consequences of a design decision might be.

Speaker 2:          41:37          Okay.

Speaker 3:          41:38          Usually that's not anybody's job, but it needs to be. And I will say, I was really pleased to read this interview with Fay Fay Lee, who some of the Stanford Vision lab and she's on sabbatical with Google cloud right now working as a chief scientist and she has a tremendous amount of experience with image processing. And she's talking about this, you think, you know, AI is very task focused right now. It lacks contextual awareness. It lacks the kind of flexible learning that humans have. And so we want to make technology that makes people's lives better and our world safer. And that takes a layer of human level communication and collaboration that that has been what is missing. But it's going to take a lot of work to get there.

Speaker 2:          42:22          Yeah.

Speaker 3:          42:22          It's going to take a lot of people changing the way that they approach problems and changing the way that they approach products to be able to make that kind of shift.

Speaker 3:          42:31          So what I will leave you with today is something I am personally excited about, but that I recognize it's difficult when you work in a tech company and that is the increasing pressure and backlash that we are starting to see on big tech. You can see across the board from the truly terrible year, Uber has had to uh, the Twitter boycott that happened just a few weeks ago to the ongoing questions around Facebook's role in the election and what ads were sold to which Russians to whatever happened with the Google memo. Here we are seeing this backlash starting to spring up. And it can be difficult and it can be uncomfortable, but this is exactly the kind of thing that we need to have happen and that I actually hope more people start pushing back against. I hope that this is a broader conversation that makes its way well outside of tech circles because as much as I enjoy coming and talking with technical audiences, um, I really think one of the most important things is that we make technology and push back against technology and the overreach of tech companies accessible to everyday people, to people who don't necessarily have the insight or knowledge.

Speaker 3:          43:35          Because I really think that it is only then it is only when we take the concerns of everyday people, all people, even people who don't like mini cupcakes into consideration. Then we were actually able to make these kinds of changes possible and that we can make a technology industry that is going to be sustainable for the world and for individual people in the long run. So thank you so much for having me today. I really appreciate it.

Speaker 2:          43:58          [inaudible]

Speaker 5:          44:04          thank you very much Sara. Questions. The cupcake thing is, is sort of a little, uh, it's frustrating because you can imagine that we could have taken a different path and that would have happened, would not have happened. It's also sort of, um, maybe easier because you can imagine what that path would be and maybe we could follow different paths in the future. There's something I'd like, uh, I'd love to hear your, your thoughts on that I think is, um, maybe fundamentally from my perspective seems to be a harder problem. Uh, I, I'm not sure if you were aware of the eye. This is about a year ago. Uh, there was, uh, an issue where people notice that if you searched in Google for unprofessional hairstyles, you got pictures that were overwhelmingly of women and people that were overwhelmingly of color. And my simplistic understanding of this, I don't work on that product, I should say, uh, is that that's actually a reflection of how people tag images. It is, it is a reflection of people having sites about what to do and not to do in the workplace that use those pictures as examples of what not to do. And, uh, and so an algorithm that sort of is, um,

Speaker 2:          45:22          yeah,

Speaker 5:          45:23          when is the right word for this really trying not to be biased, but, but the algorithm winds up reflecting the larger bias in society. And I think we've seen a number of, uh, instances of this kind of problem or the, um, uh, we, we had a problem a few years ago and Google maps where the White House would get labeled with a racial epithet because a lot of racist people were tagging it that way in, in maps. And we were, uh, using that data. And I'm just wondering if you had any thoughts on,

Speaker 3:          45:59          yeah, so I think it was, I think this is, this is legitimately difficult, right? Um, but here's the thing, like I think that one of the reasons this is particularly difficult is that tech companies have spent a lot of time focusing on what they do as being somehow neutral and, um, and like a lot of time kind of saying like, free speech, free speech, um, and so, and it's kind of take abdicated responsibility. And now we're looking at it and going like, Oh shit, what have we wrought? And so, so I think part of the reason it's hard is that we should have taken this seriously a long time ago. Um, are you familiar with Sopheon Noble? I don't know if of you are something to noble is an, uh, information studies scholar and she actually talks a lot about that kind of algorithmic bias and things like search results. Um, and so specifically things like if you, so unprofessional hairstyles that makes sense. Also things like if you Google the word black girls, you will get, uh, typically explicit results. Um, and there's lots and lots of ways in which that mirror that's being reflected back to us doesn't look great. So I think what you have to start thinking through though is

Speaker 3:          47:03          as a company, as an industry, as a culture, what is the role that we want something like search to take? Is it a mirror? And then, well, what do we do about the fact that it's never just a mirror, it's also a magnifier, right? Like, because that's the fact. And that's what we have to acknowledge and really deeply internalized is that it's not just that you're reflecting back to society what people have said, but you're making that codified. Um, Cathy O'Neil who's the author of weapons of math destruction, she talks a lot about how data, it doesn't create the future, codifies the past. So you're basically taking that historical information and you're making it seem more objectively true. And so I think that that is really the question that you have to be asking, not just are we comfortable reflecting this back, but are we comfortable magnifying this and normalizing this and codifying this.

Speaker 3:          47:49          I think that if we ask those questions, I think we will come out with answers that are going to feel more ethically sound and something that we can really stick by and something that can be a better guidance for us. I don't think it makes that discussion easy. I think that's still a really difficult question because, um, you then have to, then you have to say, well, like, where's the line where, you know, and like you're in this sea of gray area, but if you're not, ha I mean that's, and that's exactly why like this is a people business. Like if you're not going to have that kind of complexity of conversation about what you want to see in society, what we think is appropriate, what we think is fair. Like if you're not going to have those conversations, then you shouldn't be playing in this space in the first place.

Speaker 6:          48:33          Okay.

Speaker 3:          48:33          That's helpful.

Speaker 6:          48:35          Alright, so first of all, thank you for being so direct and strongly worded and raising her topless problem. I normally would not use the words decided if somebody was ignorant to the issue. Um, I kind of pulled that word to mean that it was, uh, you know, informed decision of some kinds of, like they knew they were at a junction. Um, this definitely is some form of neglect or malpractice. And in, uh, when I took freshman ethics, I was told that what distinguished festival fix from the rest of, you know, other kinds of ethics is that if you're in a profession, your client doesn't know enough to double check your work. If you're an architect or a lawyer, you know, almost by definition your client just has to trust you to do it. Right. And I think it's appropriate that these companies should know these pitfalls and should be systemizing solutions to them. But, uh, I don't know how much you know about the internal structure of different companies, but like where exactly would you pin that responsibility? Is that engineering malpractice? Is that design malpractice? Is that legal malpractice? Um, who should be the professional that is aware of all the implications here?

Speaker 3:          49:50          Well, so I think so realistically like we are never going to be aware of every potential implication, every potential outcome, right? Like that's not, that is not realistic. I think that it is incumbent upon people in every discipline though to to be more aware. And I also think that it is incumbent upon companies to look at what they're doing when like when you acknowledge that what you're doing is not just tech, then you realize that you actually need expertise in these areas. That of course, of course you're going to do a bad job seeing some of this stuff because you don't actually have the background to see it. Right? Like you should probably bring in who has a deep knowledge of history of race in this country to have any, um, to have like influence over any product that could impact people of different races, which is probably all of your products.

Speaker 3:          50:40          Um, right? Like, like somebody has to actually know something in order to find these problems. And if you're not hiring for that, then you're going to continue having these gaps. I do think that everybody can do better across every discipline. I do think it is the responsibility of people in design. It is the responsibility of people and development and it's a responsibility of people in legal. It's everywhere. But the thing is if you're going to say it's everybody's responsibility, that's very easy for that to become nobody's job. Right? And so I think that fundamentally though that's because it's not, it's not just about the individual culpability. It is about priorities as an organization, right? So if this is a priority for your organization, then it's going to become part of your organizational culture and is going to be systematized and is going to be present at every step along the way.

Speaker 3:          51:27          It isn't going to be done in this sort of ad hoc way where like you individual engineer needed to have noticed this thing. I think assigning blame to individual people is actually not very helpful. I think it's much more helpful to say, why does this happen? That's why I'm like, I don't really care about the bug fix. Right? Like I care about what kinds of patterns are emerging over and over again. And how does the way that you are organized, how to PR, like what's, what does a project sort of genesis look like and how does that get translated into a team that's working on it? Who is saying Yay or nay to decisions, who thought it was a terrible idea but wasn't comfortable speaking up. And why is that happening? Um, did you have enough diverse people in the room? Like were there a bunch of women in the room who decided at the mini cupcake should be there or not? Um, like, and you could say that about many, many groups but, but you know, I think that you have to look at it at that macro level because I think when we try to talk about it at that individual level, then of course people miss stuff. Everybody misses stuff. I think we have to look at the pattern of missing stuff and say, how do we tackle that organizationally?

Speaker 7:          52:32          Thank you. You were talking a lot about how these sorts of checks need to be systematized. Um, and I'm just wondering if you know of any examples of organizations which have decision making structures which support this? Like how can we foster an environment where we are in a meeting and the person who thinks maybe this cupcake idea is bad, feels empowered to speak up? Like, what's sorts of systemic things can we do to be better about that?

Speaker 3:          52:59          So this is hard. I, I don't have any examples off the top of my head where I'm like, just look at what this corporation does. Um, because I think that this is, this is not something that most companies are doing that well at, but I would say that part of it is, okay, so you, you do need to have a diverse team working on things, but you also have to have a diverse team that feels like it can speak up. And what that means is that you have to have people at different levels who are diverse. I mean, listen, the thing, like if you want to fix this problem, you got to go with back to root cause. And that's a really hard problem. But you do, you have to. Um, but you have to have people who aren't just at a junior level who might have different perspectives.

Speaker 3:          53:38          So if you have all of the senior people in the room. Um, so for example, the other day, the, um, the chief of diversity and inclusion at apple had this comment. It was kind of an offhand comment in a totally blew up where she said something like, well, you know, you could have 12 blonde haired blue eyed men in the room and that would have its own kind of diversity. And, um, what that indicates to me at a company like apple that is actively trying to recruit more diversely is that they still have this perception that, uh, people can, like, you can have that group of people come into the room and like that that's still okay. That's not okay cause I'm going to tell you those 12 people, they are going to miss a lot of stuff. So you, you have to both acknowledge that you need that diversity there and that those, everybody there has to be able to speak up to those 12 blonde haired blue eyed white men in the room who all come from the same background, um, in order for it to work. Um, so, so anyway, um, I think that that's like, that is a key piece of it. And then it's also like

Speaker 3:          54:41          we prompt, we have processes, we have like systems that we use for all kinds of stuff. Why is there no system in place that's like checks and balances on assumptions? Like has anybody ever sat down and documented all of the technical assumptions that they are making that's pretty common? Or have you ever sat down and documented all of the like social assumptions that you're making or human assumptions that you're making? I have very rarely seen a team actually sit down and do that and then think about, well, okay, what's the worst that could happen? How could this, you know, how could this go totally wrong and how are we going to word against that? I just don't think that they're being trained to do that kind of thing. Um, so, so yeah, I think that you do, you have to go pretty deep to start solving the problem. It's not going to be fixed by, um, you know, just like, let's hire some more junior people in diverse roles. It will not be fixed by, I'm just like kind of encouraging people to speak up. It will be fixed by kind of big, deep changes in the way that we structure teams, the way that we promote the way that we operate as companies. Um, which makes it really tough to mix painful. It makes it feel like it's hard to get anywhere. Um, but I think that that's really the only way.

Speaker 2:          55:53          Well, it's like Sarah, again.