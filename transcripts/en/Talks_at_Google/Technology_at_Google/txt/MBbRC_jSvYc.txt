Speaker 1:          00:06          Good morning. Thank you all for coming out to meet, uh, Rob Califf, a former FDA commissioner under president Obama and barely science advisor. I'm Paul Varghese, the clinical lead for health informatics for verily life sciences. And today we're going to have a question and answer session. We had one of the most influential physician scientists in the United States and health care and internationally. And I'll be asking Dr Caleb, uh, about a range of topics related to tech enabled healthcare and they'll be an opportunity for the audience to all ask questions as well. Uh, so for those of you who may not be familiar with verily, we are a offshoot of Google X. We've been around since 2015 and our mission is really closely aligned with what Google has. We would like to make the world's healthcare information accessible and useful for the population at large. Um, and we do this in a variety of ways.

Speaker 1:          00:55          One is to collect information, organize it, and then make it actionable. You may have heard of some of our recent efforts in this field, which is uh, some of them are prominent public projects. One is called the baseline project, which is to enroll 10,000 people to collect evidence in a way that the world has not seen before. A broad range of healthcare information which we consider traditional and nontraditional things like behavioral information, activity information. Another more recent, uh, initiative is in the disease management space. We recently have a joint venture that was launched with Santa Fe called undue. It's for diabetes management, uh, initiatives and it's again trying to apply the best of what we know as a Google and alphabet associated company to the area of health care, which we think is right for improvement with some of the technologies take for granted. Um, that's a little bit about verily and with me is Robert Taylor, who's one of our barely science advisors.

Speaker 1:          01:48          There could be a whole separate talk about all the things that Dr Caleb has done. Um, but I will hit the highlights. Uh, he is a physician scientist. He's a cardiologist, trained primarily at Duke University where he spent the last 30 years becoming one more, a prominent medical researchers. I think. Uh, the blurb that I read was a 1200 peer reviewed journal articles, 50,000 citations and uh, um, that just shows the kind of work that he's done. It's a particularly, uh, meaningful to me. I'm trained as a cardiologist and when I did my training, uh, we learned Dr Caleb's works and, uh, one of them was a one most prominent large scale clinical trials that gave us true evidence based medicine and was called the Gusto trial. And when we talk large scale, I mean large scale 40,000 people. And uh, rob can, we'll have some opportunities to talk about what it was like to sort of do that kind of evidence generation and data collection.

Speaker 1:          02:41          Uh, in the era when the most sophisticated technology was a fax machine. Those are some of the reasons why, uh, he was tapped by President Obama to help run the FDA. For those of you who are not familiar with the FDA, it is the agency that is tasked with ensuring the safety and efficacy of the medications and medical devices that we take for in our lives. I think, uh, the blurb from Forbes was that Rob Califf was the most qualified FDA commissioner in the history of the organization. Given that sort of wide ranging background that he had. And for all of those reasons, uh, I've asked them to come here and talk a little bit about what it's like to be a clinician scientist educator and be on the government side of things when we are trying to find new and innovative ways to apply technology to healthcare. Um, so that's the background for Dr Kayla. Why come to technology? You have been on the government side, you've been on the clinical side. Uh, what is it about coming over to the technology era or the technology sector that you found appealing? Um, and the related question is something that Eric Schmidt posed to us about a year ago, which is, um, how does a technology centric company such as alphabet, such as Google, such as fairly, um, learn to become more healthcare centric?

Speaker 2:          03:59          Well. So first of all, let me just say I'm a hedging my bets as you know. So I'm halftime at verily and halftime at back at the old university and North Carolina. And I'm also chairing the board of an organization called the people centric research foundation, which is, um, an effort to generate evidence on behalf of patients and consumers about health policies and medical products that has been funded through the accountable care act actually. And now we have 130 million Americans participating through the ease of their health records and hopefully in clinical trial. So, um, I see it as a comprehensive effort, but you know, why spin at age 66 and two weeks in North Carolina in two weeks in California seems like an extreme thing to do. And the answer is because obviously to me, the technologies sphere as taking off now and has a huge amount of money.

Speaker 2:          05:05          So on one hand we've got tremendous talent, a revolution in technology, and then we've got a contrary, maybe exemplified by the South East and middle America, where life expectancy has gone the wrong direction three years in a row. So, you know, we should be concerned about the world, but, uh, we'd better be concerned about the United States because our health is deteriorating measurably, uh, under our feet. So what I would hope to do is to bring these worlds together because, um, the traditional healthcare sphere needs better technology and you know, amazing things are possible, but at the tech world has failed every single time it's tried to get into this space. And there are reasons for that which have to do with the human side of things that are, I think, fundamentally different between software and application, uh, to a very human thing like, uh, delivering healthcare and people making decisions about their healthcare. So, you know, at this stage of my career, I want to be helpful and I'm trying to keep a foot in each side of the equation.

Speaker 1:          06:16          Uh, so I'm going to ask you to play the role of physician. Um, you're used to giving patients a constructive information that can be tough to hear. What kind of constructive, uh, tough medicine would you give to an organization like Google, alphabet and barely, you mentioned the differences in how clinical medicine has done, uh, and software development.

Speaker 2:          06:43          So part of this, the part I'm very confident about giving advice on his pay a lot more attention to the human side of things. Uh, the best software in the world when it comes to people making everyday decisions about health will not do the job unless it's integrated with people. Um, uh, an individual, you know, it seems like a lot of what silicon valley is built on is an individual interacting with a computer, which is, you know, it's been great for me in my career, but when I make decisions about my health, often my wife is involved, my parents may be involved. Um, and for many things, by law I've got to deal with doctors and nurses and clinics and hospitals. So it's a, it's a complicated social human equation that needs to be dealt with. The part of it I'm a little less confident about, I'm sure that this is an issue that we need to pay attention now. It's just looking at what's happened with Facebook. You cannot assume that using people's information and selling it without a very strong governance, probably regulatory eventually, um, approach is going to be needed because ultimately this is a, I think a very powerful double edge sword that if it's not handled through channels that have governance, um, could end up being detrimental is I think, uh, this last election was, it certainly wasn't good for my career. I, you know, I was out of it, out of a job January 20th. Okay.

Speaker 1:          08:23          So you, you talked about, um, you touched on the role of what we do with people's data and so one of the promises, one of the potential promises is that as we do better data collection, that we have the advantage of technology being fairly ubiquitous. That we have sensors, um, that we're all carrying their smartphones. We're gathering information and we want to do the right thing by the patient. We want to do the right thing by the provider. Um, can you talk a little bit about where, see how we empower people to get that information? Uh, for example, you were recently at south by southwest talking on this very subject. Uh, um, can you share where you think, um, the obligation is to getting that information back to the patient and do we not run the risk of overwhelming people? Um, more information is not necessarily easier to manage.

Speaker 2:          09:18          A lot of wisdom in what you just said. In my opinion, this goes way back for me. I mean, we were doing studies and the eighties of, uh, end of life care and pretty easily demonstrated that uh, different people will take exactly the same information and make very different decisions. Oftentimes kind of shocking. Like there was a general view that if you had like a 1% chance of living the next six months, that people would rationally say, don't resuscitate me because you know, it's just going to add a couple of days of misery to, and I'd rather die peacefully under good social conditions. But a lot of Americans, we'll look at 1% say I'm going to be the 1% and often the families would have opinions about it. So we've also got to look at the cognitive capabilities of the population of very high proportion of Americans. Something like 15% can't read above a second grade level.

Speaker 2:          10:13          So the idea that we're going to empower people by giving them like their Gina and that circumstances just, it just doesn't make sense. On the other hand, you should get your Gina. So I think one of the beauties of a, what could be done with current technologies to tailor the information to the person according to their capabilities and desires. Also taking into account that they need to have a relationship, whether it's someone, particularly if it has an illness or a serious finding needs to have a relationship with a clinician of some type and a system. And like I say, very often, um, with family members, uh, I love the British term for this carers. So carers, someone is not paid but takes care of you. A caregiver or someone who is paid like a nurse or social worker. And those things are different because the carer often comes with a very emotional component. You know, it's, it's been sad that in a American families, the women make about 80% of the healthcare decisions. And I think that is verified by empirical research. So on average, if it's a man, if you're not dealing with a significant other in a heterosexual couple, a, you're probably making a mistake. On average.

Speaker 1:          11:32          You talked about, um, some of the new demands were placing on, on patients when they get access to that information and the people surrounding them, if that is changing for the, uh, patient and the people around them, where is this going to impact the people we traditionally associate the caregiver, the physician, the nurse, that there are things that we associate with those roles and responsibilities. In 2012 there was a lot of attention to what the known Coleslaw said about he didn't want to see doctor Varghese from doctor Kayla if he wanted to see doctor algorithm. And then four years later he kind of backtracked and he said, well, I think there's a role for the physician, but I think he's going to be like 20% physician. Not like, you know, 100%. Uh, a replacement. And, um, and so, um, the question for you is from your vantage point and perspective, both as a physician and an educator, where do you see the role of physician evolving? And I ask in particular because your son is actually doing his medical training in emergency room medicine. So what, what is it that you're going to see happening to your son and other people in their training with us?

Speaker 2:          12:48          You know, one thing that you learned at the FDA in a big way. So when people are healthy, they really do behave like consumers and that is, uh, very little risk tolerance. Um, a lot of desire for independence particularly and Americans, it's a little different. Other cultures. Um, but when people get sick, they become dependent fairly quickly. They're also willing to take more risk and they're more vulnerable to people selling them things when they're at risk. And so, um, I don't, I'm not worried about the role of the physician going away. I don't think that's going to happen. The physician becomes an enabler and an information, a transmitter. But there's also this other big thing happening, which is not, you know, we use the word physician. That's really, I must've met her for, for a clinician of some tie. More and more. Uh, the clinician may be a nurse or a pharmacist or a physician's assistant, all kinds of people working in teams to help people take better care of themselves and when the person desires it, making decisions for that person.

Speaker 2:          13:55          For example, as a cardiologist, pretty rare to have somebody coming in with an acute heart attack, making an argument about what treatment they want. You got 10 out of 10 chest pain, 30% chance of dying in the next couple of days. You're pretty dependent. And it's critical to have a doctors that can make decisions for people in healthcare teams that can do that. On the other hand, if you're totally healthy, you're trying to decide which vitamin to take or whether to take a prophylactic stat. And you know, the locus of control is much more in the hands of the individual and the family. So the role of the doctor will change, you know, the most imperil for their current roles would be those that look at images. So radiologists, dermatologists, because, uh, it's clear that machine learning can do better for the repetitive parts. But think about it right now. Those people make their living for the most part, looking at images serially and producing reports. This, we have a big problem and translating the findings of those reports into useful actions, there's a tremendous gap to be filled there, uh, for clinicians. So the jobs will just be different. Uh, but I would predict the better information we have, the more there's going to be a need for healthful people in health care.

Speaker 1:          15:13          That's seems to be a natural segue to, um, for those areas that are more amenable to these applications of machine learning and technology. How reliable should they be and who, uh, who is the person, who are the organizations that should be, uh, in charge of those things? And, uh, I asked in particular because the FDA is, has the purview of evaluating medical software as a device. Uh, there are some things that are exempt. There's some things that are not exempt. And, um, I think I as a physician to have a lot of comfort and a utility when I know that there's a tool that's reliable. Um, I, I joke with my staff that I can answer, uh, nearly every medical question they asked me with one of three answers. We don't know. It depends on my new favorite is I don't remember. And, um, uh, and I'm freely admitting that there are some things that, uh, are, um, just, I don't have time for that. I'm going to need to have a tool that helps me, but I know how to evaluate the effect safety and effectiveness of a drug or a medical traditional medical device. But I think we're really uncharted territory when it comes to the, the, uh, what we do with algorithms. And having sat at the highest seat in this arena, uh, what were your thoughts? Well, I mean, I feel compelled

Speaker 2:          16:38          to say my career started in 1975 with one of the first databases in medicine before the 1970s. Everything in medicine was on sheets of paper and scribbled handwriting is, you may know. And I just was fortunate as a medical student to be in a place that had a database in cardiology that measured everything that happened to people as part of clinical care, as part of the clinical record and then followed them for life. And so, um, we had algorithms that were used in a predictive way back then for one very specific condition. And the first paper I wrote that was published in peer review literature predicted though within five years all diseases would be treated with the assistance of a computer giving decision support, uh, with a doctor and patient looking together at the information. I was off by 40 years. We're still not there.

Speaker 2:          17:31          Um, because it's such a vast enterprise and so much to do. But if we fast forward to my time at FDA, um, you know, I'd seen this before, but at FDA you really see it. If you're really good in your field and you're very principled, you always do the right thing. It's hard to imagine that they're nefarious people in the world. And also that there are a lot of people are well intentioned but just don't get it right. And um, you see that at FDA, the mixture of those two, the proportion of the various people is actually quite small, but they can have very powerful negative effects and the history of the FDA as catastrophic public health events that lead to regulation. So it started with a horse named Jem milk wagon horse who was being used to develop any toxin and the horse got infected and it killed some children and Saint Louis, I was 1906 that led to the first biologic control act.

Speaker 2:          18:33          Then it was self an element. And there was a company that put arsenic in antibiotics. It was sold to children, believe it or not, uh, killed a number of children. So that led to safety regulation. Then there was the letter mind which, um, was being, uh, given us free samples to pregnant women to control nausea. Nick cause horrible birth defects that lead to key Farber Harris in 1962. And it's amazing to think prior to 1962, you didn't have to show a drug was effective to put it on the market. And that at that point they reviewed 3000 drugs that were being sold routinely in the United States that had no evidence that he had any benefit. They were all pulled off the market. So if you don't think there are people selling stuff that's bad. And then for devices there was a doll con shield, which was uh, uh, pregnancy prevention device.

Speaker 2:          19:24          It caused infections and uh, let's just say the company that, uh, made it didn't necessarily come clean with all the adverse event reports in a timely fashion. So that led to more device regulation. So now let's think about decision support and algorithms. Uh, it's understandable. There are a lot of smart people who say, you know, regulators are in the way, um, we can do this, right? It's going to help people we know are good. And, um, let's say that there is some decision support that a lot of people use. There's a little glitch in the program. So the wrong dose of the drug is recommended. You know, the cover right now, it would be to say, well, it's just a decision support the doctor makes the decision. But anybody that's been in a busy clinicians office knows that it 10 minutes a patient, you're going to use decision support if you think it's good.

Speaker 2:          20:17          And so this is part of my meeting with Obama. Just a quick story on that. When I got offered the job, people know I'm an enormous state basketball fan. I was captain of my high school team, I've had season tickets at Cameron indoor stadium for decades. And um, he spent the first 10 minutes of our one on one meeting in the Oval Office telling me how much he hated Duke and he loves you, loves you and see, and this went on for a while and, but then we, I mean like everybody here. Yeah, you got it. I'd say Duke and the Red Sox have a lot in common or more of the Patriots, probably terms of national hatred. But the last, the last 20 minutes, we're literally, um, about the importance of technology to the American people and how to the need, the absolute need to regulate it, but to do so in a way that, um, fostered creativity and innovation and allow this industry to thrive.

Speaker 2:          21:15          And you know, I loved it and I think, I think there are ways to do that. It starts with saying that for now the FDA is not going to regulate decision support heavily. It's gonna Watch it with something that's called um, enforcement discretion. The exception being if that decision support is attached to a real medical device, it has consequences like your cardiac defibrillator, you wouldn't want the algorithm to not quite be right with the cardiac defibrillator. So that's going to be regulated as part of a medical device. But you know, should you take a stat and decision support what you do with your blood pressure medicine, should you get a flu vaccine? Not going to be heavily regulated, but I've predicted publicly, and I'll just say it again, there will be some catastrophes that will occur and that's how we'll figure out what should be regulated in the meanwhile, what are the reasons that I came to verily as part of Google is my believes that accompany with these resources can afford to do it right?

Speaker 2:          22:16          Because so often what you see both in the academic world and in the industry world is you've got a company that's just trying to escape by, minimize expenses and get revenue as quickly as possible. There's a tendency to cut corners and often you can cut corners and get away with it and it doesn't affect anything. But sometimes bad things happen then, you know, this company can afford to do it right and I think should not back off. I do want to say one other thing. Um, I uh, advertising is a big part of this company and the role of advertising and all of this is a very complex issue that we better pay a lot of attention to. Um, because advertising is how you will get products to people both as, you know, doctors just like any other human being. If they don't know about it, they can't use it. On the other hand, most of advertising now is being used to sell things that aren't so good for people's health. And I would argue it's one of the reasons that our health statistics are going backwards now. We're not eating well when, you know, and we're spending a lot of money on things that are detrimental to health.

Speaker 1:          23:30          I'm thinking about the flip side of that, um, that with those types of interactions that people have with those things that they're getting prompted to look at, um, is this not an opportunity to sort of look at that as information that can help us make better decisions that, um, there is this concept of sort of pharmacol vigilance, which is, um, after a product, uh, pharmaceutical or medical device, um, uh, gets released to the market. It's only then when we realize, you know, what is its true effect? Because when you're doing the trial, it's hard to anticipate everything and that we get real world evidence now with people interacting with the things that they're looking for and searching for. Is this not an opportunity for us to be able to look at that as a, a valuable data stream to help us, you know, maybe counter act, you know, some of the, the things that we're not able to anticipate now. At the same time though, I think, um, it's nobody has the right answer for like the level of privacy associated with this. And I'm curious where you see, um, the, the benefit of that and the challenge of that.

Speaker 2:          24:35          Well, that's a, it's just, it is like the world's most important opportunity now to make a difference. You know, it's certainly fascinating if you look at the health statistics and we're going to have an amazing meeting of go parts of Google and the gates foundation and Chris Murray who produce these beautiful Geo spacial maps of the world. They've had a focus recently on the u s cause we're looking in particularly bad compared to other economically developed countries. And the breakdown of life expectancy and health status roughly looks like the election map. It's really fascinating. But if you go to the red counties where life expectancy is going backwards, mostly due to opioids, suicide and cardiometabolic disease, which is roughly eating too much and not exercising, everybody has a cell phone, everybody has access to information. Um, it's a question of what do we do with that information then how do we include people in an active way that leads to better health?

Speaker 2:          25:39          And I, you know, maybe people in the room, there are a lot of smart people here and maybe people here have the answer. But it seemed to me that a lot of what's going on with the technology now is, uh, you know, I think of them as a dick of circuits is the, you know, the neurobiology here is pretty complicated and not so simple, but most of it's built on immediate delight. And for almost everything related to your health, you need to have the opposite of immediate delight. You need to have executive function and you'd be able to say, no, I'm not going to eat that Bojangles biscuit from where I come from. That's a big deal. I'm going to hold off and maybe have one a week as a tree. Um, but what the, what the advertising is telling you to do is to eat the biscuit now.

Speaker 2:          26:23          And when you do it feels pretty darn good. And so, you know, how do you engineer, um, executive function using the technologies that currently, um, lead to repetitive behaviors that are detrimental? That I think is a huge question. It's one of the reasons I came, I was hoping we could figure out because it's not so not so obvious, but we got to involve people in, you know, what I'm telling the university side is I don't know of anything more important for universities right now then to have serious academic intellectual activity about, um, how societal expectations of technology, what are the, what should the rules of the game be that are good for society. You know, you need policy schools, law schools, medical schools, um, English department's history departments to come together and think this through. I'll just, I'll say one other thing cause I, I sort of make fun of Davos.

Speaker 2:          27:24          I went this year, I said, you know, it's where millionaires still billionaires what the middle class is like, but um, what are the themes? There is this fourth industrial revolution, which I think is very real. You know, the first was water power, the second was electricity, the third was an information technology. We're just recovering from that revolution. Now we've got the fourth, which is the, uh, merger of biological, physical and information sciences into a single sphere. It's dramatic and it has profound consequences for, um, our country and for people all over the world. I think we've learned we can't just leave this to chance. We've got to have serious people think about how it should be dealt with.

Speaker 1:          28:11          Uh, I'm going to get to a couple of others questions back. I just want to point out that this was Rob's official FDA portrait and uh, a couple of weeks after joining barely, this is his official, they're like, uh, so, uh, we, we, we got to him real quickly. I noticed. Yeah. Notice a Dr. Harrington sent you a Stanford t-shirt shirt. Yes, yes, yes. Yeah, we were, we were trained for UNC tee shirt, but it was last minute. Um, um, so you introduced me to this concept. Um, and I think it might be related to some of the things that you just touched on is like how do we sort of view this information differently? And this is a actually a university of Washington course calling bullshit in the age of big data and apparently it's incredibly oversubscribed. So how do we teach people to sort of understand what they're being put? What's put in front of front

Speaker 2:          29:03          end? Yeah. Well, I mean, I'm going to just talk about the health and medicine part of this. There's a whole other area in politics that we could spend a long time talking about. But I mean you could say the FDA's job basically in society as a called bullshit, but to do it with enforcement when it happens. And the problem we have is that, um, if you just say AI or machine learning in much of academia, now people step back and say it must be great, but there's a lot of bullshit and much of it, it comes from technically well done things that just from people who don't understand things like confounding and lead time bias and context of the research. Um, and it can lead to very tragic, wrong answers. And there are many, many, many examples of this. What fascinated me in this course is it's in the context of big data of all types.

Speaker 2:          29:54          It's a university course, not a health course, but essentially the curriculum is, is what you would learn in what clinical epidemiology one oh one. If you are in a school of public health or a medical school, it's how to understand context, how to know that you need the right teams of people who have a track record of producing reliable, uh, results. Not just accurate, but we're liable for the purpose of the analysis, particularly if it's going to be used to make a decision or derive an action. And then there's having the courage to call bullshit because, um, when you do it, I mean, the great thing about the FDA, you can, you, you know, you get vilified everyday by the press and by Congress, but you actually have the legal authority to call bullshit and do something about it. Um, you know, for people who are in everyday work and they see something going on in the workplace, this is very important for health research. That's not right. People cutting corners, uh, it's easier not to speak, uh, to not say anything about it and bad things happen when that occurs.

Speaker 1:          30:59          So, um, maybe you can hold us accountable. Um, I think we're all familiar with the phrase dog fooding that, um, one of the things that is a tremendous asset for us in this company and the alphabets fear is that we have a ready, willing pool of people who are willing to try the newest, the latest to contribute their information. Um, how do we avoid sort of building in those biases from your, your description, you are well versed in sort of looking for these confounding things coming through old school methods of epidemiology. But for many of us it's a, the lure of we have a ready pool, we can generate data quickly. Um, what is it that you think that we should be paying more attention to when it comes to this especially, uh, is this, uh, it might be internally valid, but it's not externally valid. Well, uh, the,

Speaker 2:          31:53          anything about the FDA, at least in the drug and device are right and we didn't get a chance to talk about food and cosmetics and animal health, which is, you know, the FDA regulates 20% of the economy. Half of it is further. We could have a long discussion about that. I'll avoid that for now, but still thought that the biscuits, sorry, by law, uh, companies have to demonstrate that their products are safe and effective in prospective studies. And so the, in health, um, it's actually pretty easy, but you have to have the rig, you have to have the rigor and the courage to do it right. And that is you can do all the stuff you want to do as you're developing your concepts and ideas. Ultimately, the test is a prospective study that measures health outcomes. And if your product doesn't improve health outcomes, she got to ask the question, why should anyone pay for it?

Speaker 2:          32:45          And in many cases, a surprising finding is that there's actually a detrimental effect or something that you thought was going to be great. Um, you know, and there, there are many, many examples I could give, but I think the same is going to be true of information technology. It looks logical. It makes sense that out of work then you deploy it in for reasons that you didn't understand the wrong things happen. Now, if you're selling shoes, it's probably okay. If people get shoes that are uncomfortable, they'll throw them away and buy some new shoes. If someone's making a life or death decision and they're now dead, you can't take it back and you cause, so, um, it's a different game. But the beauty of it is due the outcome studies. Figure out if what you're doing works and involve, um, people or don't have a financial interest in the outcome in the studies, which is another hard part.

Speaker 1:          33:43          It's one of these native tensions that, uh, we know that there are things that worked well in the healthcare arena. At the same time, we want the advantages of what we know works well in our, our Rena technology and that, um, it is this sort of uncomfortable tension that we, we sometimes think the adage move fast break things is, is the, the thing that makes us successful, but how does that translate into a more controlled, uh, arena where the consequences are much higher?

Speaker 2:          34:12          Like I said, I don't think anyone saw this yet. There's a reason that silicon valley has failed. I was 100% of the time in this arena. Okay. And part of it is this, um, you can't just take human lives and do move fast. You know, a brick in the face of a human being is not a trivial issue. On the other hand, you know, taking five years to get an answer for something I technology that can move faster. This is what President Obama was all over me about is how to do this. And I think the key is informing people of what you're doing. So if you're gonna develop a system that will figure out how to use technology quickly, make sure that the people who are getting healthcare in that system understand what you're doing. You're not doing it behind their back secretly back to Facebook again.

Speaker 2:          34:58          They need to understand it and participate. That's why we don't call research. We used to call them subjects when you did research. Like they're like inanimate objects that you're doing experiments. So we now call them participants because they have a right, a human right to be involved and understand what you're doing and why you're doing it. I believe based on what we're seeing in the baseline study that you mentioned, that if you do inform people, there'll be excited about it. Uh, they'll want to participate in, particularly if you give them feedback about what you're learning as you go along. So I, I think the transparency, the feedback and the being in it for the longterm is the other thing if you want to make about quickly now there's some pretty easy ways to do it. May Not be good for people in the longterm.

Speaker 1:          35:45          You've given us some bracing insights about what, what we can do better. I'm going to call up this guy named rob killer. And when he talked about what we can do with, uh, the clinical trial, this was a formative part of your contribution to the world of medicine, the large scale clinical trial that produces evidence based medicine that we rely on. Yet you, you're saying some bracing things and critical things about that. Can you elaborate a little bit about, uh, this? Yeah. You know, I just got screwed up early in my career

Speaker 2:          36:17          because I came along and someone asked me to run the cardiac care unit. At the time we didn't know what causes heart attacks. A 56 year old men are routinely dropping over dead from heart attacks. Everyone was smoking cigarettes, very high rates. And uh, an enterprising cardiologists did an angiogram and showed it was blood clots in the coronary arteries and the rice was on the develop treatments that worked. And for some strange reason, it became the norm all around the world to enter people into randomized trials. So we can do these very big randomized trials very quickly, get the answer and move on. And the risk of being dead now, if you have a heart attack, is less than half of what it was when we started because of this rapid learning system using randomization to get the right answer. Because most of what we tried actually didn't work.

Speaker 2:          37:07          People have forgotten about that. Most of our trials, the treatment wasn't effective even though we thought it should be. Um, but for some reason the whole thing got gummed up in bureaucracy. And collecting, uh, you know, part of the problem was at the time, uh, medical records are all written on paper. So you had to create a separate, uh, enterprise of data systems to collect data separately from the clinical care, very expensive. Then you had to fly nurses around airplanes to check to make sure the records don't match. It's a, you know, amazing amounts of bureaucratic human labor. The result is that even in cardiology, 85% of our major recommendations are not based on high quality evidence and that's the best there is. You know, God bless you. My little brother is an orthopedic surgeon. God bless you. If you need orthopedic surgery, it does a lot of good.

Speaker 2:          38:00          But you know, it's less than 5% of the major recommendations there are based on high quality evidence. So we need a different evidence generation system. I'm very excited that, um, you know, people in this room can come together and produce something that's far superior with much more ramp and learning, but we have to, I call it God's gift of randomization. Um, I, I don't think that just analyzing our way with a bunch of data because of all the complex biases that are involved is enough about that evidence generation. Um, are we trying to have it both ways that we want to make it easily accessible to produce this information, to use smartphones, other types of activity trackers at the same time as the, these aren't medical grade units. So are we trying to get more data, but at the same time we cannot fully trust that the quality of that data.

Speaker 2:          39:03          I mean, I think we, I think we got this ride at the FDA and it's playing out, you know, since I've left, I think in a very pleasing way. I mean, basically if you're otherwise healthy and there's a device, it's meant to remind you to exercise or count your heart rate or stuff, that's fine. That's a consumer device. Doesn't have to be perfect. You know, it'll get better and better if you've got a heart failure and you're at risk of dropping over dead. Uh, that device needs to be accurate and it, um, I think, uh, we need to have the discipline and rigor to perfecting engineering to the point where it really can be reliable and it's not so easy. I think we're finding, but I'm very optimistic. I think we will be able to do it. But it also means that we've got to involve like teams of people who really understand biomedical data and how it's used in the clinical setting, working with the engineers very closely so that, uh, the information can really be used to inform good decisions. And, um, I think the FDA is going to need to look carefully at a lot of the algorithms when they first come out, which will be a fascinating exercise. I would like to,

Speaker 3:          40:20          to give the opportunity for the audience to ask questions, Brian. Sure. So first as a pats fan, speaking to a Duke fan expression, I'll remind you of they hate us cause they ain't us. Um, so this, this is a little bit off the topic, but I'm sure you'll have an opinion on it. Uh, what do you think are, and this is more of a business question, hospitals today, I believe, uh, you know, health care hospitals in particular, I headed for a major disruption from a business model perspective. What are two or three things that you think hospitals need to start doing today from a business perspective to sustain themselves or to continue to provide the value that they provide for the population?

Speaker 4:          41:03          There was some interesting traffic on Twitter today about this related to an article came out, so I would make a big division between urban areas and rural areas. It turns out that rural hospitals need to become community centers. Basically, if you look at rural America where this demise that I talked about is happening in terms of health statistics is closely related to the economics and a loss of jobs and all that sort of stuff. It turns out at Duke we are affiliated with a group called lifepoint, which is a for profit system that buys rural hospitals and what's happening to those hospitals when they're well managed as they, they are a sort of a way station. They take care of people who need short essays or have a sort of, not quite nursing home care yet, but also become community centers because the main employment in these towns is a education and health care.

Speaker 4:          41:57          Um, and so that's, I think the rural hospitals need to be transformed. My friend Patrick Condom, I just brought a great Jamma editorial about the pathway for rural hospitals. Urban Hospitals I think, um, can't help themselves right now and there's no amount of efficiency in the current payment scheme that they can do that will fix the problem. This is just my opinion. So it's fair to say they ought to continue to be as a, to work more and more on getting more efficient, implementing decision support, et cetera. The fundamentally what's needed in a different payment system is to keep people out of the hospital. Um, and then to optimize the use of the hospital using very high grade, um, intelligence. I'm here, I'll just combined Clayton Christensen's a focus factory thing. It ought to be that you stay out of a hospital unless you really need one.

Speaker 4:          42:53          Then when you really need one, y'all to go to a place that's really good at what it does for the problem that you have, even if you have to travel, which is a disruption to the cultural things that we've just been talking about. But there's no way that in the current payment scheme, hospitals can afford to make that transition. So they're going to have to be forced to do it. And I hope, uh, the role of Silicon Valley here with all of its financial prowess as well as information will be to help push the country to the transition that needs away from fee for service. Madison, which I think is the culprit.

Speaker 5:          43:30          Hi. Um, my name is Virginia. I also work with hospitals, but Brian's, so I have another hospital's related question. But um, from one night, no, about machine learning and artificial intelligence is that you get better if you have bigger data sets and they're more interconnected and bigger, larger data set. So there have been great moves to do that. But what I observed with hospitals is that it's super siloed data that lives in every different spot. And while we have made major inroads with digitizing data, it's not connected to anything. And, um, data about our most vulnerable and needy populations actually live in a lot of these hospitals, which arguably are not technology companies are not led by technology people or people who understand data and again, have these siloed data structures. So what can be the next big step to connect data to allow for machine learning algorithms to get smarter that aren't just kind of like pockets of success that are really scaled interconnective systems

Speaker 4:          44:32          that allow for better evidenced based medicine for your little brother? A couple of things. I completely agree with you, um, about, uh, the nature of the problem, but I would urge a couple of changes in terminology. So it's not, you know, Americans coalescing into, it's actually quantifiable. It's a little over 650 health systems now that own the hospitals. Uh, it's been sad and I haven't seen the data yet, but a pretty reliable person told me, if you look, there are about 150 of these health systems that care for about 70% of the American population now. And, uh, uh, all of them have a business interest in, as you know, in aggregating their information in the data, whatever the right term is, warehouses, legs, pools, all that stuff. Um, so that they can conduct their business and stay viable financially because of fee for service. The data they are aggregating tends to be not quite accurate, but it's still usable for a lot of things.

Speaker 4:          45:34          I don't want to sound totally cynical about that. Um, but the culprit is a, they think there's great value in that data and so they're not very willing to share it or give it up often under the guise of HIPAA and, um, and, and privacy issues and concerns which are real but shouldn't pro prohibit this. Um, I'd say the second thing is, uh, in our, in our, uh, in our center at Duke for a data science, uh, I personally think we need to quit just saying machine learning. We need to say quantitative methods because a lot of what needs to be done doesn't require machine learning. It's just if you had the data. And so what I would do is focus a lot on the curation of the information to make, get it in usable form, and then have teams of people who use the information for the purpose using the right quantitative method for that purpose.

Speaker 4:          46:30          And then, um, what, what I'd, what I'd say is we've got to break down the data silos. And I think our best weapon, there's going to be patient advocacy groups. We really worked on this and the Obama administration as you know, to try to get over the hurdle, um, you know, if they're just been another four years, a lot of things would be different. But, um, you know, we couldn't get there. And you know, we are doing things, as I said, like the PCRF a sentinel. Um, they're, you know, in Pcrf we got a third of America's health systems, uh, sharing with PCRF is, it's a people centered research foundation. It was created through a funding from Pikori, which was, um, it's paid for out of, um, uh, attacks on the Medicare, um, fund in insurance companies. And there's a board that oversees it, which is a industry, academia government.

Speaker 4:          47:28          You know, the suspects, the goal is to do research using aggregate information, but what you want is something even much more granular than what we have in that. But we're showing that given the right incentives, people can share data on a massive scale. But I think actually the place to divert right now for a period of time is this thing that we were talking about. Um, getting the social roles right and having people understand, uh, what the rules of the game need to be so it can be shared because technically I don't think it's a big issue right now. It's all cultural.

Speaker 6:          48:09          One of the dominant ways to get paid in the researching development of, of drugs and of devices is to charge far based on a monopoly. Um, do you see our intellectual property laws as well calibrated to the human health space today and in the future?

Speaker 4:          48:27          I'd say the general principle, I don't know another way to do it. Then just say you've got to have some period of exclusivity to recoup the costs of development. When you say calibrated, that's a really important term because it's different for different sarco it should be different for different circumstances. Um, and uh, I don't know that if the current, if the current duration of time is right for the circumstances that we're currently in, um, you know, a simple way. I think what you're referring to, um, obviously the more rigorous we are about what gets on the market, the more, um, actually one argue with myself here, but in theory the more you'd have to spend on a development to get it right. But I would argue if we use the technology that you guys are developing and automate the research systems, we could radically reduce the cost of development because the biggest costs in error, the late phase, uh, clinical trials.

Speaker 4:          49:27          But then you just ask a question, if you have a drug that's life saving and you have a legal monopoly, which is a deal with society to give you a chance to recoup the costs, how do you set a price on that when the only way people can survive is if they get that drug. And that's where I think we've got it. Not Not right at this point. So you can think of that about that either as length of time in which you have that or some other mechanism of setting prices that's more fair to people who are vulnerable. So I think this is and you know, but let's also now think about the big thing looming as soon as we saw of all the short term problems we've got, um, Alzheimer's and other longterm chronic diseases where you will know that people are at risk loan before they have the first symptom.

Speaker 4:          50:14          So how do you develop a treatment and price it right when it would be preemptive and worth a whole lot, but you got to pay for it upfront. Uh, you, the Hepatitis C drug situation is a good example of that. Um, if you actually cost out over a lifetime, it's a pretty good deal at the price. But the fact that people have to pay it upfront at one time meant that whole state medicaid programs would have gone bankrupt, hadn't been, what is the price right now? It's coming down, but you know, it's in the tens of thousands per treatment as you know, for some rare diseases and can't types of cancers, three or 400,000 bucks a year. I think. I think that's what you're getting at. So I don't think we have it right. And I think there's a, it's another area where we need, once we get adults in the room in Washington and the universities, we need to have those, continue those discussions.

Speaker 5:          51:13          I have customers in the health care trying to solve some of these problems and as we work together with them and they have an interest in partnering with Google, how do we decide whether it's, uh, you know, the right conversation to have with you guys? Or are they, what are the criteria or how do we determine that process? Ah,

Speaker 4:          51:33          so, um, it's a little hard to come into this environment. I hate to tell you were from a structured, it's really stark for me because I'll step into my office in North Carolina and Duke University. It's very hierarchical. I've got this big staff that makes everything work and um, people know who I am. And there's a funnel of information that comes out to me. And my job here, I arrived out at verily I'm in a little glass cage with another guy's dean of a medical school, was sharing an office and um, I'm just another one of the people. So, um, and then you say how the decisions get made. And this is a place where I think the software culture has a lot to offer in terms of the flatness, but it won't work to be, it's not, it won't work like a software culture to make it work.

Speaker 4:          52:22          And I, based on everything I've seen, I think this is a major topic that the enterprise alphabet needs to spend some time on. It's good to have some internal competition. Harvard is probably one of the best examples of place where you can go a hundred yards away and see people competing on the same thing. But, uh, to the outside world, if you don't offer some cohesive approach, um, it's hard enough for them just to get by right now as you know. So I think we need to do some work to figure this out. And I hope I can be helpful with that.

Speaker 1:          52:56          Well, this has been wonderful. Uh, thank you so much for coming down and listening to rob and great questions. And, uh, please come up and visit us on the 12th floor and barely here in Cambridge, a if you want to continue the conversation and, uh, this was a great experience for us and, uh, we hope to replicate it in the future. Thank you.

Speaker 5:          53:16          [inaudible].