Speaker 1:          00:00:06       A Neat Suarez is the executive director of the Machine Intelligence Research Institute and leads the research program, Nate as the primary author of most of Mary's technical agenda, including the overview document agent foundations for aligning superintelligence with human interests and are the Aai paper or curability Oh, he's here today to discuss his work with Mary. And why thinking about the impact of Ai, uh, not just your Chromecast is, is an important idea. So thanks so much. Take Nay Suarez. Everyone.

Speaker 2:          00:00:39       Okay.

Speaker 3:          00:00:40       Hello. Uh, I'll try and go through this on the, on the faster end so that we can have time for Q and. A. Uh, so as mentioned, I'm the director of the machine intelligence research institute. A very roughly speaking, we're a group that's thinking in the longterm, uh, about artificial intelligence and trying to make sure that by the time we have a advanced AI systems, we also know how to aim them. I'll say more about what I mean by that. Uh, throughout the course of the talk, uh, I, I'm sort of going to assume that people here know what I mean by artificial intelligence. That said, it's a fairly vaguely defined term. There's not a, there's not one concrete definition, uh, but to sort of wave in the direction of this intelligence concept, I'll say that, uh, if you look all around us at, uh, you know, the room we're in, the technology we're using, this is not a product of our brown.

Speaker 3:          00:01:37       This is a product of our brains. Uh, the reason that humans are the dominant animal on the face of the planet is not because we are faster than bears. Uh, it's not because we are stronger than bears. Uh, it's because we are smarter than bears. And, uh, over the course of time, if you look at the largest drivers of change in technical, uh, or if you look at the largest drivers of change in, uh, uh, human and animal welfare over the last couple of centuries, it has been due to technological and scientific innovation. Uh, and if we can automate technological and scientific innovation that has the potential to change the world, uh, on a scale not seen since maybe the industrial evolution, perhaps earlier. Uh, so this is, uh, you know, Ai, uh, uh, artificially intelligent machines that exceed humans in this sort of general listening capability are not coming next year.

Speaker 3:          00:02:29       Uh, but they probably are coming in our lifetime. A Ai is now making humans money. And when something starts making humans money, they tend to put a lot of resources into it. Uh, so there's, there's lots of brilliant people working on it, many of them here. And, uh, uh, I think it's a decent bet that it comes in their lifetime. Again, I wouldn't worry about it coming around the corner, but this does mean that, uh, it's something we should, we should think very seriously about. Now, when people start talking about how we should think very seriously about artificial intelligence, they'll often raise a number of concerns. The ways, concerns, like, uh, what happens when the computer becomes conscious? Uh, the Alaska. Like, will it like humans? Will it hate humans? Uh, will it reflect on his goals and decide that they're silly and then decided that it should kill us?

Speaker 3:          00:03:15       They'll get concerned about these sorts of, these sorts of questions. These are not the right questions. Uh, the, there is no, there is no level of complexity and a computer program where the computer program is suddenly imbibed with a spirit. Uh, there's no, uh, there's no amount of code. You can run in a CPU or the CPU will, will be inhabited by a ghost and the ghost will look down at the instructions and decide whether or not to execute them. Uh, the CPU just keeps on executing the next instruction. You can of course program something that, uh, you know, rewrites its own code, but there's no, there's no ghost that comes down when the, uh, there's no ghost that comes down and questions the goals that you gave it to unless you program something that is, you know, reading as goals and uh, rewriting them on purpose.

Speaker 3:          00:04:04       Um, so the, the concerns of the questions we should be asking to quote Stuart Russell, who's the author of the coauthor of one of the leading AI textbooks, which many of you may have read. Uh, the, the concerns with AI are not about spooky merchant consciousness. The concerns with AI are about the ability to make high quality decisions, which is exactly the thing we're trying to do with Ai. So, uh, the concerns, the sort of double edged sword, not sort of the spooky questions, but the, uh, the sort of what if we succeed questions. So many people when they start talking about concerns about AI will throw up a picture of the terminator. Uh, I was once quoted in a news article making fun of people who've put up terminator pictures in all their articles about AI next to a terminator picture. Uh, I learned something about the media that day.

Speaker 3:          00:04:50       Uh, I think there's a much better picture. So this is Mickey Mouse in the movie Fantasia, uh, who has very cleverly and chanted a broom to do his chore of Filling the Cauldron. Uh, so how might make you do this? We can imagine that Mickey does this by writing a computer program and then having the berm execute that computer program, uh, and is thereby automating his chores. Very, very clever. Uh, so what might this program looked like? Well making, you might start by writing down some sort of scoring function or objective function, uh, where in this case the Broman gets one point if the cauldron is full and zero points. If the cauldron is empty, this seems simple enough, then Micky might write a program that works as follows. I, it takes some set of available actions, uh, and they're, Mickey writes a program that can take one of these actions that's input and then calculate how high the score is expected to be if the broom takes that action.

Speaker 3:          00:05:45       So this is a predictor of, uh, this score, uh, given that the is taking, uh, they give an action and then Mickey can, uh, write a function that just looks at all the actions for a certain amount of time, uh, predicts which ones lead to high scores and then finds one that leads to a pretty high score. The reason this is a sort of Arg Max here is that we don't need to assume that the, uh, that the broom is able to predict every single action. Uh, it may, there may be some actions where it can predict the consequences. There may be some actions that doesn't have time to think about, uh, but it's sort of, uh, using, using some ethernet or another. Mickey will program it to, uh, uh, uh, in a given set of time. Look at a bunch of actions, see which ones it predicts, uh, will lead to a high score and then execute the one that leads to the highest score can find with the allotted resources.

Speaker 3:          00:06:38       So this is a pretty simple description of a program. Uh, you know, the devil's in the details, uh, writing this sort of predictor and this sort of, uh, uh, smart search through action space is basically the whole field of Ai. Uh, and you know that that is not proven easy problem. But conceptually, this is a simple program. And let's say that Mickey does this and runs the broom, uh, seems, seems easy enough. It seems like it should work, right? So what happens? Well, what happens in Fintech fantasia is this, the broom starts overflowing the cauldron. This raises two questions. First, why did the broom start overflowing the cauldron? And secondly, why is there a person standing in front of you telling you that this is realistic?

Speaker 3:          00:07:20       Uh, so the first difficulty is that the, the objective function that Mickey gave this brim is not, in fact the objective function that Mickey, uh, we'll sort of intending, uh, Mickey wanted the culture and full and didn't want to call it an that part was right, but Mickey, uh, uh, sort of didn't want the workshop flooded the workshop being flooded as far worse than the workshop being empty. Uh, and once you realize that Micky left that term out, you realize that there's a bunch of other terms. Mickey also left out, for example, flooding the workshop is funny. Uh, but it's not funny enough to justify flooding it. Uh, and uh, killing someone is way worse. Uh, you should do something as bad as flooding the workshop many times over in order to save a life. Uh, and, and, uh, we realized when we look at this that uh, Mickey, the, the problem was not so much the broom, you know, gaining a mind of its own and define Mickey.

Speaker 3:          00:08:14       The trouble here is that the brown did exactly as it what it was programmed to do all too well and Mickey didn't understand the consequences of what he was programming. The second difficulty is that Mickey Program, the Berm to uh, maximize as hard as it could, the expectation of its score. What this means is if the broom currently assesses a subjective, a 99.9% probability that the cauldron will be full a and it has all these extra resources lying around, then the action that most maximize the score is the one that uses all those spare resources to push the probability. So it's subjective probability that the bucket is full up to 99.9%. Uh, once it has very high confidence that the bucket's full or that the, that their cauldron's full well, there might be a leak in the cauldron. It might, it's sensors might not be working correctly.

Speaker 3:          00:09:04       There might be someone trying to take the water out of the cauldron. Presumably Mickey wanted the cauldron full for a reason, then someone's going to get the water out. So there's all these, there's all these small probabilities that the color is not quite full and you know, there's nothing else, uh, you know, the, the, the sort of default actions of sit around and do nothing after they called her and looks pretty full, isn't it going to make the color any fuller? So may as well the action that the action that maximizes, uh, this score uses the spare resources that are available to increase the probability even a tiny bit. The rum has nothing better to do. That increases then the score. So even if even if the remaining resources can increase the score a tiny bit, this bro, we'll put those resources towards eking out that tiny little bit of probably, uh, the, we in the, in the AI alignment business, we refer to this as an open ended goal.

Speaker 3:          00:09:51       There's the sort of goal where, uh, it's, it's resource hungry, it's effort hungry. Uh, whenever there are spare resources lying around, you might as well repurpose them in towards the filling your goal. A contrast this with a task like goal where uh, there's, there's sort of a, the goal we had in our heads for filling the cauldron was much more task task. Like we sort of wanted to fill the cultured enough and then stop. And you could try to do that by putting in all these terms on the objective function that are like, uh, uh, you know, penalizing for too much resources or something. I, there are many ways you could try to do this, but uh, uh, uh, in essence the, the question or the difficulty that make you run into here is that he created an open ended goal rather than a task like goal.

Speaker 3:          00:10:35       Uh, and the, this, making a test like goal is something of a subtle problem. Uh, in this example, the original objective function and looked pretty task like a, it was bounded. There was no way to get more and more and more utility. Uh, uh, uh, like it's not like he got one point for every bucket of water report and then there would clearly be a, an incentive to overfill the color. And it was, it was founded, uh, and it was very simple. You can look at the objective function and you didn't see any spare terms. And what made this into an open ended goal, uh, was the expected utility maximizing, taking subjective probabilities that couldn't go to one, uh, and, and maximizing the expectation. Uh, and there are actually a number of different ways that a goal that looks, uh, uh, task like can turn out to be open ended.

Speaker 3:          00:11:21       Um, and, uh, one example is that if the, if the system has lots of sub processes, some of which are pursuing these sort of open ended goals, you might be in trouble, even if the overarching goal, uh, is sort of task like, uh, so the, the standard objection at this point is a, okay, this is not a, it's not a thing we need to worry about a ton. And Ai, like, yeah, the system might not, there might be unintended consequences of the objective function. Uh, and, uh, and the goal might be more open ended than we thought, but we can always sort of pull the plug. Right? Can we just hit the stop button? So Mickey tries to this, uh, and it doesn't work. Uh, and again, I claim that this is very realistic, right? Why is it not working Mickey's case? It doesn't work because Mickey, because Mickey's broom has repurposed lots of spare material, uh, towards things like making more brooms that can help make sure that the cauldron's full, what else was the resource going to do?

Speaker 3:          00:12:16       Like the action that, that maximize this score, uh, put those spare resources to use. Uh, also if the broom was clever enough to know that sometimes Mickey tries to get water out of the cauldron and Mickey, you will when make, he realizes, uh, that the objective function is not what he wanted. Attempted to hack me apart with an ax. Uh, the broom like the, the score, maximizing a actions in something that's predicting those aspects of the world, uh, will, uh, automatically by default. Uh, do things like make copies, resist, shutdown a, this has an analog I claim in, uh, in the real world if we tried to design sufficiently advanced AI systems by default, if this system has a model of, uh, your ability to shut it down and the fact, and it has a model of, for example, so let's say that you build an AI system that has some objective and models, the fact that, uh, that you're going to attempt to shut it down animals, the fact that this will lead to it not fulfilling its objective, then by default, if you haven't done something clever with your objective function, uh, it now has an incentive to attempt to subvert you shutting it down or they're, I can as a separate problem.

Speaker 3:          00:13:29       But this is the default incentive. You have to put in a extra work yourself. If you want to subvert that as a incentive. That's not to say you can't, this is not a, is not a oral doomed talk. Uh, it's just, uh, by default if you build a powerful optimizer and send it towards an objective, uh, and then try to turn it off the, the first step you built something that was gonna try to subvert you, turning it off in the second step. You are, you've now made yourself an adversary. Uh, and if ever you're in a situation where you think your programming, if you think you're writing a computer program to be your adversary, you should probably stop and write a different computer program. Uh, so, uh, as an example of of ways you might try to try to solve this problem, uh, here's one way Mickey.

Speaker 3:          00:14:09       You could go back and try to, uh, address this shutdown difficulty with the broom. He could change the score function by adding a button. The button doesn't even really need to be attached to the room. You can just have a button on his desk and you can define a score function, uh, which says if the button's off, you have the old score function. But if the button's on, uh, you now get appointed few suspend and zero points if you don't suspend. Now, this is assuming that, uh, we have some way to write a, uh, an objective function and that sort of detects suspension, which is very, uh, very difficult. Uh, like for example, if there are loopholes in the suspension detection mechanism, then the thing might try to both technically suspend. We'll also filling the cauldron to get both like just to, just to hedge his bets.

Speaker 3:          00:14:57       Uh, but, but let's assume way that problem, let's assume that we have sort of an airtight suspension detection. Uh, you could try this utility function. Now, does anyone have a guess as to whether this works and if not work is wrong? Exactly. Yeah. So, uh, the, uh, it's filling the cauldron and requires like getting water, pouring it, uh, uh, getting water in the buckets, carrying those books along, way pouring it in. And that's like a, that's like a difficult task, right? The spending is a pretty easy task. Uh, so it can either get a point the hard way or it can convince you that it's terrible and get a point the easy way. Uh, so you have now built a broom that attempts to scare the shit out of you, which might be fine. Hopefully, hopefully it, it, it, uh, I mean you can't press the button if you're dead, right?

Speaker 3:          00:15:44       So it's probably just gonna, you know, make scary noises. But, uh, this is, this is sort of indicative of the problems, not a, it's not trivial, right? There's a, there's a default incentive problem if you, if you set up a thing, uh, to optimize objectives that you may then want to change and, uh, uh, addressing it is not a trivial problem. Uh, and this is, we're, we're, we're right now in this discussion, surprisingly, we're pretty close to the state of the art on this problem. There's maybe a, there's maybe three more inferential steps. There's maybe three more, uh, uh, pieces of research that are built on this problem. Uh, one of them I did a couple of years ago, uh, with, uh, with, you'd Koski Fallen Stein and Armstrong, and then, uh, another was a, was built on that about a year ago, uh, which was the big red button paper, uh, that you probably saw the hype about.

Speaker 3:          00:16:38       It was actually very small result building on this, but boy wasn't hyped. Uh, and that was, um, that was, you probably saw something about that that was a collaboration between, uh, one of our associates who work in the future of [inaudible] and, uh, learn after, so his at deepmind here. Uh, and that's, that's pushing this problem forward. But this problem is still more or less unsolved. Uh, uh, will you, you, you can, the problem here is sort of, if you have two utility functions and you want to combine them in such a way, uh, that, uh, you, you sort of have three constraints you want, you want it to follow the first utility function before the buttons pushed. And the second one after the button's pushed, you want it to not have incentives to cause or prevent you from pressing the button. Uh, Andy wanted to still have incentives to propagate that button forward.

Speaker 3:          00:17:22       If it spins up sub processes, uh, or if it, if it spins up subagents or if it does anything, uh, that sort of extends beyond the sort of original program. You sort of like don't want it to delete the button as dead code. It turns out the getting all three of those at once is still a pretty hard problem. There's a bit more than this, but it's still still fairly open. So the moral of the story here is that this is the sort of work, uh, that it sure would be nice if we knew how to do this before we tried to estimate to fill cauldrons for us. Otherwise we will end up like, Mickey, this is Mickey realizing that he should have done this research a long time ago. Uh, so let me now step back. I'm, I'm doing this talk a bit backwards cause I think in these sorts when we're talking about Ai, I think it's important to get concrete examples of what the real problems are before talking with the big picture.

Speaker 3:          00:18:10       Because when people try to talk about the big picture, immediately, they sort of often get lost in, in philosophical discussions and ethical discussions. Uh, so now that we've another, we sort of grounded our discussion in some, uh, in some of them problems. Uh, let's take a step back and talk about what it takes to sort of align an AI system here is a dramatically simplified pipeline of how you might try to get an aligned AI system. Uh, you have some humans who, uh, who are thinking up the, the goals and values or the objectives or whatever that they sort of want the system to have a, then they do some attempt to translate that into an objective function. This may or may not succeed in Mickey's case. Uh, it didn't quite succeed. Uh, then you have the system, uh, uh, uh, attempt. Uh, so there's, there's, there's sort of two subtly different, uh, parts here.

Speaker 3:          00:19:00       One is you specifying formally, uh, what you want. And the other is the system. Like getting that into the system. So many of the goals that we actually want are significantly more complicated than fill a cauldron. Uh, and if you say something like, well, I want cancer cured, uh, you might, uh, you might, uh, this might go wrong if a cancer was in fact good. And like that's surprising you. I don't need to worry too much about that, but it might also go wrong if you sort of create a metric for measuring, uh, amount of cancer reduced where that metric, if you actually optimize it, has bad consequences, uh, for example, something killing cancer patients. Um, and then there's also the issue of, uh, of the learning framework. This is, you might, you might have a, uh, uh, uh, like in the same way that it's very difficult to write a computer program that, uh, by hand that recognizes a cat.

Speaker 3:          00:19:57       Uh, it's, it's sort of much more difficult to write a computer program that by hand recognizes a complicated goal. Uh, you're almost surely going to going to have machine learning systems that are, uh, uh, attempting to learn the objectives that you're giving them in some fashion. That framework also needs to work. Otherwise, all of the, uh, all of the right objectives, function specification in the world won't get that sort of into this system. And then the other components that you sort of need working, uh, its predictions about what consequences follow from what actions need to be pretty accurate. Uh, if it, if it is a predicting the opposite of how get an action is, and it doesn't matter how good its objectives are, uh, it's going to keep making things worse. Uh, and then also needs to be able to, I mean, these, these two at the bottom are more, uh, sort of classic capabilities research, but it also is still needs to work in order to, uh, Devin AI that works, um, and does what you intend.

Speaker 3:          00:20:51       So, uh, most, most media when it focuses on these problem focus on one of, I sort of two of these problems, uh, one of which is fictitious. They'll focus on the problem of what if the wrong person is behind the AI. Uh, you know, what if the North Koreans go first or something and they'll focus on this problem of like, what if the AI has natural desires that cause it to not do the things that we programmed it to do? I think the ladders largely fictitious, uh, the former, uh, I think, you know, the former is going to be an issue, uh, uh, someday. But currently we are in a situation where well intentioned people, uh, could not, like if you handed me a box that was an extraordinarily powerful function optimizer, I could put in a description of some mathematical function and it would give me an output that made that function.

Speaker 3:          00:21:41       Uh, it would give me an input that made that functions output really large. I would not know how to use that to reliably have a very good impact on the world if this was, if this system was powerful enough to build models of the world where it could, uh, figure out the effects of my beliefs on it's a future actions and so on. Uh, so we're currently in a situation where well intentioned people couldn't do good things if they tried. Uh, and I think that when we're in this situation, sort of from an academic or a scientific standpoint, you know, our first, our first objective should be let's make sure that, well intention people could do good things that they tried and then we can sort of worry about these more political concerns. Uh, the politicians that I've spoken to are, uh, at least the smarter ones are, uh, are pretty good at figuring out that this natural desires thing is bunk.

Speaker 3:          00:22:32       Uh, but they, uh, they tend to think that this means they should focus only on the other thing that the media is talking about, which is, uh, whether or not good humans are standing closest to the system. Uh, they also focus on things like, is this going to cause mass unemployment? And so on. Which I again, I think are important issues, but I think, uh, sort of Pale by comparison to this possibility of a industrial revolution style or larger, uh, change to the way that society operates. Uh, so, uh, I think these, I think these are useful things to think about that, but just they're not done other things that, uh, that we do at myriad. There are other things that I think are sort of most worth technical attention at this point. Uh, early science fiction. You know, another, another thing that we often talk about whenever we're talking about AI is Isaac Asimov's three laws of robotics.

Speaker 3:          00:23:20       Uh, people, people often ask, why don't we just do the field last robotics? Well, okay, a, they're not formally specified, right? Like in a, in Isaac Asimov, they're like built into the positronic brains, whatever that means. A, B, if they were fully specified, you sort of want, want them optimized, right? Like, don't throw an action. Let humans come to harm. Like step one, put all the humans on heroin drips and underground bunkers, right? Uh, and then see the books are about how the laws don't work, right? Like they're like, hey, here's these laws. Oh look, it leads to very interesting failures, right? If they worked at one p books, right? So, you know, I think these make for interesting stories. I think they are maybe depressingly more realistic than I'd like them to be. I think that often humans don't think for five minutes about the consequences of what they're about to do.

Speaker 3:          00:24:04       But I don't think this is where, where the real issues lie. I think that you can, if you do think for five minutes, realize someone's problems there. I think that the, that the real places to look if you want to, if you want to address some of these issues are in, uh, making sure that the system can learn the intended values. The better your value learning framework is, the less you need to precisely specify the right, uh, the right utility function or the right objective function. And the more you can build a system that sort of figures out, uh, like builds a good model of you, figures out what your intentions were, trash and do those things. Uh, and then I think it's also very important, uh, to get the optimization hooked up correctly. Uh, two of the objectives. So that, so an instance of this, this is a sort of abstract one, but an example of this really quick, if you look at natural selection, natural selection is the only known, uh, uh, engineering and in scare quotes process, uh, that we know of that has ever led to a generally intelligent artifact.

Speaker 3:          00:25:02       Uh, it has led to quite a bunch of generally intelligent artifacts, namely brains. And, uh, if you look at natural selection, it's a, it's a sort of very stupid hill climbing approach. So one thing we know from this is that you can get to general intelligence with enough brute force and he'll coming approach. Uh, and we also see in the case of natural selection is natural selection. Optimized brains very, very hard for genetic fitness. So the external pressure on the brain, uh, was, was selecting brands for being an agentic fitness. The internal objectives that humans internally represent as their goals, uh, are not genetic fitness. So we have, we have, uh, goals that were instrumental for survival, sort of hardwired into our brains. We have like hunger instincts and we have a survival instincts. And, uh, and then we explicitly represent his goals. Things like love and justice and beauty and mercy and you know, fun times and, and all this, all this stuff that, that correlated with survival and fitness in the ancestral Savannah.

Speaker 3:          00:26:09       Uh, but it was things that correlated in the past with what we were being selected for. Uh, that that sort of became our, uh, uh, the things that we internally optimize for. Right. So this was a case where the, uh, the external optimization pressure on the artifact, uh, resulted in a generally intelligent artifact with internal objectives that did not match to the external selection pressure. Hopefully that all made sense. Uh, similarly, if you are, uh, uh, applying gradient descent, uh, to a black box trying to get it to be very good at maximizing come objective eye. If you are, if you are doing this blindly enough, you might get something very much like what natural selection got, which was a generally intelligent artifact that does pretty well, but it's actual goals are things that correlated in the test environment with your objective and which will not sort of continue to correlate with the objective when it undergoes a context change, uh, or gets more resources or a, or a different setting. Uh, so, so I think one of the, one of the real important things to do in this work is make sure that whatever methods we're using for optimizing whatever objectives are given, hopefully something like a, like a value learning framework, uh, that the internal targets the system's optimizing for are the same sorts of things that we are externally selecting the system for using something like gradient descent. Abstract point. I'll, uh, I'll move on now. Uh, the, the take home message here is that we expect that these things are going to be a technically difficult.

Speaker 3:          00:27:38       Uh, and if we, if we can't get these things right, it's hard. It doesn't matter who's standing closest, right? Uh, good intentions are not sneezed on to computer programs, uh, by the intentions of the person, like a nice person standing nearby does not make the program act better. Uh, if you want the program back to certain way, you've got a program at tech that way. So let's take one more step back. I sat in the last slide that we're expecting these things to be somewhat difficult. So I'm now going to spend the remainder of the talk saying why we expect these things to be somewhat difficult. First, I'm going to give a four key propositions that mean we should care and then I'm going to go in defending that. Uh, it's all, it's not going to be a trivial problem. So the first proposition is what's known as the orthogonality thesis.

Speaker 3:          00:28:23       Uh, this was put forth by Nick Bostrom. Uh, I think Steve, I'm a hunter Rowe also had had, uh, uh, a lot of influence on this. Uh, more so in the next, on the next point. But, uh, this basically says that you can in theory build an AI system to pursue almost any goal. I think most programmers sort of intuitively intuitively understand this. There's many people who are non programmers who say, well, you couldn't program something to just fill a cauldron. Cause when it gets smart enough it'll decide to fill in the cauldron's dumb and go do something else. Right? You could program something that, you know, uh, once it passes a certain resource specialed, uh, checks whether or not it thinks, uh, filling the cauldron is dumb and then acts accordingly. But you could also program something that just always executes the action that it predicts leads to the highest score, where the score is the cauldron fullness.

Speaker 3:          00:29:10       Uh, so, so this proposition says you can in theory, build an AI to pursue almost any objective. The next proposition, instrumental convergence says that most objectives imply subgoals such as survival, acquisition of resources, and so on. By default, if you build something that's executing actions to optimize some objective, it will, uh, have incentives to try and prevent it from being itself, from being shut down because it's the thing that's leading to the objective being fulfilled. It by default has to get more resources. If those resources better helping a few of the goal. Uh, the intuition here is that if you program a robot to go buy you some milk and there are two paths, one of which is dangerous and one of which is safe. Uh, and it's only objective, it's scored on whether or not it gets back to you with milk. That's the only thing you're scoring it on, then it will automatically take the safe path.

Speaker 3:          00:30:03       There's not because it has a Annamalai and fear of the person on the dangerous path. There's not because it feels like you do. Uh, the danger of a gun is cause you can't get milk if you're dead. Right. So if it is capable of modeling that one path is safer and the other path is more dangerous than, uh, by having an attempt to optimize and objective, you sort of naturally get these instrumental instrumental subgoals. Uh, these, these follow naturally is called the instrumental convergence thesis. Uh, because, uh, these are instrumental goals and almost all objectives converge on these goals unless you do work to make them not conversion of these goals. Third Point, capability gain, uh, are potential ways for these systems to gain greatly cognitive power of strategic options. Uh, there's, there's a lot of the, there's a lot of ways that this could be true.

Speaker 3:          00:30:52       Uh, one of the reasons that I think there's proposition is true is because highly destructive. Uh, but an example of a way for an AI group to have a rapid gaining capability. Their system is there some startup that has very promising, uh, AI techniques and then Google buys them and throws a boatload of a GPU is at them and invents tps for them. Um, all hypothetical, I assure you, uh, uh, another way you could get rapid capability gain. Uh, uh, so if the system is, you know, for the first time getting onto the internet to where it hadn't been allowed massive Internet access before, uh, other ways get rapid capability gain. If the system is a lot to do hardware design and is able to find Harvard designs that are much better than a human hardware designs, this is Jim's able to optimize its own code in some way that, uh, if it, if it gets better at things like computer programming than humans, then it, and you could get, you know, the Apocryphal recursive feedback loop.

Speaker 3:          00:31:49       Uh, so there's all sorts of ways that, uh, we can sort of imagine a AI systems getting dramatically more capable than humans. I mean, it could also just be that humans aren't very high up on the intelligence food chain and that early AI systems are as naturally, uh, you know, in the same way that when humans started beading or when computers started beating humans at chess. What's the statistic? I think it's something like a 1996 was the first time a computer beat human at chess and like 2002 or something was the last time that a human being a computer at chess. Uh, I, I, there's probably not quite right, but it's, it's, uh, the window is fairly small, so you could just expect that a human's aren't great and it's whole intelligence thing. Uh, this whole like laziness thing and, and needing to eat three times a day thing.

Speaker 3:          00:32:33       Uh, brains are very calorie intensive. Uh, it could be that the human brain is optimized for very different sort of thing then what we would optimize brands for in the same way that an airplane, uh, cannot heal his injuries nor make baby airplanes. But it can fly a lot further and faster than a bird. Uh, it might be the case that early AI systems lack certain human capabilities, barriers dramatically beyond a human capabilities in the axes that matter. And in fact, we already see something kind of like this today. And then the fourth point is that AI alignment is pretty difficult, which I'll argue in the remainder of the talk. Uh, but before I go there, take a moment to, uh, to think about these three propositions. Uh, if you believe these three propositions, uh, the first one says it's possible to build AI systems that are utterly indifferent to, uh, to human objectives. The second one says that, uh, uh, those systems by default will be in competition with humans for resources. Uh, and the third one says that, uh, they're gonna, they're gonna win.

Speaker 3:          00:33:38       Uh, so if these three are, if these two propositions are true, then this means that we had better not screw it up. It doesn't mean that, you know, it doesn't mean that we're screwed. It just means that we could be screwed if we, uh, if we deal with the situation poorly, right? So if you believed all those three propositions, that means that this is really important to get right. Uh, if we get it wrong, it could go very poorly. And of course, it goes without saying, almost I'll say in any way that if we get it right, I had, could go super well. Uh, the, the whole point that we're doing AI research is because, uh, you know, the greatest drivers of improvements in welfare have been technological and scientific. And if you can automate those, uh, you can go a lot faster than we going today.

Speaker 3:          00:34:18       Uh, so, so there's great potential for upside. Uh, and if you believe these three propositions, then there's also, you know, there's a really important thing to get right. Uh, if in fact this fourth point stands, which are argue shortly, that means that we need to start seriously focus focusing on these sorts of problems. So without further ado, why do I think that element is somewhat difficult? Uh, I mean, the real reason is that I worked on the problems are a fair bit and uh, they seem difficult to me, but, uh, also draw some analogies and also encourage you to look at some of the problems and try them yourself. Uh, we could use more people working on these. Uh, one of the reasons to expect that, uh, uh, AI alignment is difficult, especially when you're looking at these really advanced systems is for the same reason.

Speaker 3:          00:35:01       You should sort of expect a priori that rocket a rocket engineering is more difficult than airplane engineering. Uh, when you're first designing rockets, you might argue, oh, well the rockets are going to these basically just airplanes, right? Like, uh, it's all just like material science and aerodynamics at the end of the day. Right? Like, what, like how our rockets, how could they, how could they end up being like more difficult to get right than airplanes? Well, the reason the Rockies are more difficult to get right than airplanes is that they undergo a extreme stresses, extreme context changes, and they're pack chock full of explosives. Uh, I argue that, uh, that there's a very similar analogy to when you're, when you're trying to build, uh, an AI system. There's a, there's a difference when you're trying to build an AI system that's going to drive a car versus when you're trying to build an AI system that's intended to go beyond the human regime in general intelligence, whatever that means.

Speaker 3:          00:35:50       Uh, and in the, in the first case, you know, it's safety critical. It's very important. You've got to, you've got to get the job right. In the second case, uh, you're going somewhere sort of fundamentally new. You're, you're pushing beyond the human region. Uh, you are, you are building something that if it is, uh, a adversarial, you're going to have a bad day. Uh, and you, uh, you're building something that, you know, it's not going to start out brilliant, but it's going to be building models of the world, uh, where you know, at some point it's going to realize that you have beliefs and that your beliefs affect, uh, w its ability to achieve its goals. And at that point, uh, it's outputs will be in part chosen to affect your beliefs, right? So you are now, you know, working with a system that if you built it wrong, has an active incentive to deceive you.

Speaker 3:          00:36:41       Uh, you're, uh, so, so you're sort of, uh, potentially putting the system under, uh, uh, much more stress, under much more ability to, to, uh, uh, it's much more likely to undergo a big context change where it dramatically, uh, models of the world in different way. Or if you give it a lot more resources than it used to have and uh, uh, and it could go real wrong if you, if you do it wrong, this is not too unlike rockets. Uh, in the same way that a priori, you should go into rocket design more cautious than airplane design. I recommend going into AI alignment much more cautiously, uh, for the advanced, for the, for the sort of long term AI alignment. I recommend going in much more cautiously than in, uh, dealing with narrow systems where I think many of these concerns I don't really apply.

Speaker 3:          00:37:28       Second reason, uh, if you look at sort of interesting engineering practices, some of the most entertaining interesting engineering practices are at NASA where they do things like take three independent teams, give each of the three teams a different engineering spec or sorry, the same engineering spec and tell him to design the same system. And then they actually implement the system by taking all three of those systems using a majority vote, uh, with the idea that each of the systems will have bugs, but hopefully they won't. All three have a bug in the same place. Uh, this is, uh, significantly more caution, uh, going into the deployment of things like space probes and space shuttles then goes into the deployment of, uh, say the new hangouts, uh, no offense, uh, and, and the, you know, we can ask why is it that space probes get so much more engineering attention?

Speaker 3:          00:38:24       Uh, and I think there's a, there's a bunch of reasons, but, uh, uh, I think some of the reasons are historical and I think some of the reasons, uh, or maybe not quite what you'd expect at a first glance, you might say, okay, well, space probes cost a real lot of money. I think that's part of it. But I think that, uh, uh, if you look at big tech companies, uh, many of their deployments are also, uh, two systems that move much more money than a space probe costs over the course of a year. Why is it that it's, it's sort of much less, uh, uh, it takes much less caution to, uh, deploy a new feature on a web app than a space probe? Well, one of the reasons I claim is that, uh, the space probe goes to space. Uh, it's, it's sort of very, it's, it's much easier to be a cavalier about your deployments when you can roll them back easily.

Speaker 3:          00:39:17       Uh, with a space probe, you can correct bugs if the antenna works and the receiver works, you know, whatever codes during the receiving and whatever code's doing the decoding of the message you sent and whatever it goes, applying the patch, right? Uh, but the code that runs the receiver decodes the message and applies the patch. If that doesn't work, your space probe is dead. It's gone. You can't go fix it. Uh, in a very similar fashion with highly advanced AI systems. Uh, if they start going wrong, you'll be fine. If the code that, uh, uh, make sure you go find it, make sure that the system, you know, uh, uh, assist you in correcting it works on the first try, right? But if you build a system that's sufficiently capable that it could, uh, do bad things, then, uh, then the code, like you'll only, you'll only be okay in this scenario a if either the code that works entirely correctly on the first try or the, uh, uh, the code that helps you correct errors works entirely correctly on the first try, right?

Speaker 3:          00:40:25       So you need the whole system to work well on the first try, but you need a component to work on the first try and getting any code that works like exactly correctly in the first deploy. Uh, this, if nothing I've said yet has sent fear through your heart, this, this should I. And then the third, the third reason to sort of expect that this is a, uh, somewhat difficult task. It's difficult in part for the same reasons that computer security is difficult. Uh, now if you've done your job right, you sort of, as I said before, you, you, you, you really, I really recommend against a designing a computer program, the expected both to be smarter than you and adversarial towards you. Uh, if you're writing that program, I suggest that you reconsider your choices and write a different program. Uh, so ideally you sort of never want to be in an adversarial situation against, against anything that you're writing.

Speaker 3:          00:41:16       Uh, but there is a sense in which the same sorts of things that make computer security difficult make a AI alignment difficult. Uh, and so what makes it computer security so difficult? What makes computer security so difficult is intelligent adversaries. Uh, so in computer security, if you have a, you might have a dozen different vulnerabilities in your code, none of which is itself fatal, none of which is itself a easy to recognize. Uh, and you might have a very intelligent attacker who can find all 12 and chain them together in a really weird and confusing way to cause like an edge case scenario, which are less them a break into your system or just make your system. Uh, even if you're not dealing with adversarial systems, when you're dealing with a highly advanced artificial intelligence, you are still dealing with this, uh, intelligent search. The search space, you're much more likely to end up in the edge cases.

Speaker 3:          00:42:17       Uh, so as a example, there's a very interesting paper by burden Lasalle, I think 2007, uh, or they attempted to use a very simple genetic algorithm to evolve a, uh, an oscillating circuit. And they did and it worked. And, uh, they were astonished by the, uh, uh, very small size of the resulting program and they were astonished also that it did not use the capacitor on the chip. And there were like, well, how is it making an oscillating signal? Uh, so they went in and spent a while figuring out what this, uh, uh, uh, evolved program, uh, was doing. And it, what was doing as it happens was repurposing the circuit tracks on the motherboard as a radio to replay the oscillating signal from the test device back to the test device. Uh, this is, uh, and this was not a very smart program, right?

Speaker 3:          00:43:10       This is just, you know, use hill climbing on a, on a pretty small solution space. And the, the, uh, the solution that it found was sort of outside the space of solutions to the programmers were themselves visualizing, right? There are visualizing things. They use the capacitor and, and uh, and used a programming the way we would think of programming when the thing actually used, uh, features of the hardware properties of the hardware that they wouldn't have modeled in say computer simulation in a computer simulation. This thing might've done more like what they intended. Uh, but the actual solution space in the real world when they ran the code was wider than the one that they were visualizing. It. Even this fairly stupid, uh, optimization algorithm found a solution that was outside the space that are visualizing a, in the case of an intelligence system that, uh, that's significantly smarter than humans on certain axes, on whatever axes you're measuring a significant better at humans say that predicting consequences of actions or it finding actions that lead to certain outcomes.

Speaker 3:          00:44:07       Uh, you should by default expect it to be pushing towards these sort of weird situations. If there are weird hacks, if there are weird edge cases that let it get a dramatically higher score, uh, you're just expected to find those, right? So, uh, if it's playing Atari Games and it eventually realizes that there's a, uh, a certain series of moves that Mario can make, uh, that led it a rewrite the code of Mario such that it can make it score really high. Uh, you should expect performance of the system to sort of, uh, as measured by whether or not it plays of the game super well to go up and then go down when it gains a, this insight and the ability to actually rewrite Mario. Mario is in fact a Turing complete. There are in fact a series of jumps you can do, uh, to inject your own code. Um, uh, I think someone wrote flappy bird in Mario. It's really wish that person was doing AI alignment work instead.

Speaker 3:          00:45:05       But, uh, uh, you should expect these sorts of things you should expect when you're building something intelligent, you should expect it to find the edge cases. You're sort of building something like the whole purpose of intelligence is defined clever ways to solve problems and we can find solutions that we couldn't ourselves pick out of the solution space. Uh, and, uh, the only difference between a solution that, uh, is very clever and you can plug into the source solution space the humans can see versus a, a solution that is extremely perverse and that we really didn't want a, that was obviously not what we intended by the objective function. Uh, the only difference between these is sort of our feelings, right? From the perspective of a system that is trying to optimize score. Uh, you know, an action that leads to a high score as an action, at least Ohi scores and leads to a high score.

Speaker 3:          00:45:52       Unless you get a good value learning framework that is predicting how you would feel about it. And then you've got to make sure it's not trying to affect how you feel about actions. It could make you really like the easy actions. And then, uh, anyway, uh, so for the same reason that a computer security gets way harder, when you have intelligent adversaries trying to chain together, uh, lots of weaknesses, you should expect, uh, uh, advanced AI systems to, uh, pick out, uh, to push the weird reasons to the search space. And this means that, um, uh, it's not just the, your, you know, sort of, it sort of much easier to make code that works well on the path that you were visualizing. And then to make code that works well on all the past that you weren't visualizing. And with AI alignment, it's going to be using all the paths you were visualizing.

Speaker 3:          00:46:40       So, uh, uh, uh, uh, gut intuitive reason to think of this role. It might be hard when you haven't yourselves worked on the problems, uh, is treat it like a secure rocket probe. Uh, so take the problem seriously. Don't expect the problem to be easy, uh, and, uh, start thinking about it sooner rather than later. And also, I think it was very important, uh, in this, in this line of work, it's important to sort of formalize your ideas. Uh, such that others can critique them, uh, and build upon them. So in this example with the shutdown button, uh, it's one thing to be like, ah, we can probably make it so that, you know, if we have less that we can make it shut down correctly. Uh, and once someone actually writes down how they would combine the sort of normal operation utility function when the shutdown operation until the function, uh, you can see that it will either have incentives to cause you to press the button, have incentives to defend the button against you, uh, and so on and so forth.

Speaker 3:          00:47:35       But if people aren't writing down their actual ideas, if they sort of aren't treating them like math problems or cs problems or technical problems, then you get lost in the philosophical discussions and people arguing about what they could or couldn't do. Uh, rather than actually building the thing in such a way that if it's wrong, uh, the person who made it can be convinced that it's wrong because it's actually math and you can check, uh, and such that if it's right, we can build on it. So this is a, this is super important and I encourage all of you, uh, if you're interested in these problems, uh, to do some of this work. Uh, so that's all that I have. I will say that, uh, there are ample resources online for figuring out what some of the open problems are. Uh, there's actually a great paper that came out of a Google brain called concrete problems in AI safety that has I think eight open problems, uh, that are concrete as the title says that you can try your hand at a, there were also, uh, some, uh, some other sets of problems you can find on intelligence.org, which is where, uh, uh, machine intelligence research institute host their website.

Speaker 3:          00:48:34       And I encourage anyone who's interested to look at these problems and help us make progress. Uh, because as I said, I think this is real important and I think we shouldn't defer thinking about it until later. That's all.

Speaker 2:          00:48:47       [inaudible]

Speaker 4:          00:48:52       thanks Nate. We're going to move into the questions from the audience. An alternative to the tiering test that was proposed by Ben [inaudible] at one point was send a robot to college. If it comes back with a decree. It's intelligent and I liked this problem because it illustrates a particular interesting case of utility functions. It's impractical to gather enough training data to train that robot on the explicit utility function. It would take millions of instances of sending it to college. Also, you can't simulate training data because going to college, you're surrounded by other capable students simulating those students would require solving the problem you're training for. Has Miri done any investigations into how to train systems where the ultimate utility function cannot be hill climbed against for either practical or logically possible reasons?

Speaker 3:          00:49:43       Uh, so there's a bunch of pizza. Did that first I'll say, uh, you know, I think if the actual objective function is get a degree from college, he'll do it in much faster than four years and without, uh, sitting around other capable students, I think that I could get a degree from a college, uh, faster than that. I guess it depends how you're, what system's capabilities are. Uh, uh, in terms of utility functions that sort of don't have a, these perverse instantiation is, I would say that a defining one of those is a large sheriff. The problem, uh, now I will say that, uh, I think many people think that the problem is all about, uh, how did you get the objective function just right. And I think that that's actually much less of the problem. Uh, I think that building something that models your intentions and tries to extract, uh, uh, what type of action it's going to do by modeling your preferences somehow, uh, is a much more, uh, likely to work approach in the same way that training a cla a cat classifier is a much more workable approach than trying to write by hand the cat recognizing program.

Speaker 3:          00:50:52       Uh, so I'm not, I'm not very optimistic about the, let's try and write down utility functions by hand, a approach. And I also think that the problem is in fact more about, uh, getting the internal targets of the system to match the, so it was externally selected for, uh, which I need a better name for this general class of problem. Uh, but, uh, I mentioned earlier, uh, humans, uh, humans not internally valuing genetic fitness as their top and only priority, uh, despite being selected only for genetic fitness. Uh, uh, so, so I would say that getting utility function right now, the whole problem, but maybe a fifth of the problem or something. Um, in terms of that fifth, uh, I'm not super optimistic about, uh, for example, the college, a utility function. And I would say, uh, finding objectives that cannot be a hill climbed against, you know, I haven't found, I haven't found any good things in this direction. I would say that the thing to look at is less, uh, uh, unhackable utility functions and more something like, can we make it task based, limited, uh, uh, limited. It's, uh, yeah, there's, there's, some of these are discussed in the concrete problems paper. Uh, so I take a look there.

Speaker 1:          00:52:13       What are your thoughts on using formal verification methods in approaching this problem?

Speaker 3:          00:52:18       Well, verification, I sort of wouldn't bet on getting very much in the way of formal verification of an advanced AI system. Uh, uh, you know, if you, if you talk to a program Verification Community folks today and you say, Hey, we're going to let the code alter the code, uh, they will either shut her or yell at you to get out of their office. Uh, more or less, that's not quite true. Uh, we have, uh, so out of Mary, we do have some work that, um, we have, uh, we have a paper showing how, uh, you can write a secure colonel, uh, that can replace any component of the kernel, including the verifier component, uh, which is very hard to do, um, for a go deleon reasons. A, you want the old verify or to verify the new verifier, uh, and as easy to do if you have loss of strength, but it's very hard to do if you don't have a loss of strength because you'd run into, into a verifier consistency, uh, issues. Uh, so we have a way to do this with, with, uh, more or less no loss of strength, which is very surprising. Um, that paper is called a hall and hall. It uses the higher order logic, a theorem. Prover. Uh, so, so we have done some thinking about this.

Speaker 5:          00:53:34       Um, uh,

Speaker 3:          00:53:38       I would say that I think figuring out how you would go about doing this sort of program verification can be helpful. Uh, but it's mostly helpful conceptually to understand,

Speaker 5:          00:53:49       uh, uh,

Speaker 3:          00:53:51       like what sorts of things you can and can't do. Uh, and I wouldn't expect early AI systems to have properties. You can usually verify. I think best case you can get statistical guarantees a something like, you know, with probability one given enough time, this part of the network will converge on the best policy or something. Uh, and I think that best case, you also might be able to get something like, uh, you know, assuming that these components work like this, then this component will work fine. But by and large, uh, I'm, I'm pessimistic about ability to verify large components were fully general AI.

Speaker 5:          00:54:30       Okay.

Speaker 1:          00:54:31       Oh, dude. Question from the door. How the partnership on AI, recently founded by Google and other big machine learning companies, has that has a set of tenants that emit any mention of the existential risks that Asi, is this a case of divide and conquer, Miri covers Ai, Asi risk partnership covers other issues or a matter of disagreement about the risks?

Speaker 3:          00:54:50       I suspect that it's none of the above. I suspect that it's, uh, for political reasons.

Speaker 5:          00:54:55       Um, uh,

Speaker 3:          00:54:59       I'd say I, I'd say please don't trust Mary to handle the superintelligence risks all on their own. Uh, there is, I think half a dozen or so full time researchers at the moment. Uh, those aren't, those aren't very good odds. Um, I also, uh, you know, I don't wanna, I don't wanna say too much about this partnership without talking to the people running it. Um, but I'll say that I know they are consulting with people in the AI alignment community. Uh, and many of the groups involved in it are quite aware, aware of the problems. Uh, so I suspect that insofar as it's not in the charter, this is a largely political, and I think also largely, uh, many of the things that this particular partnership we'll be looking at are related to things like make the cars not kill people, make sure that the, uh, that the displace jobs don't, you know, destroy, uh, lives, uh, or that insofar as they destroy lives, those slides are made better in some other way. Uh, we do, I think, I think these are important issues, but again, I think they sort of Pale by comparison to these, uh, questions about the transformative possibility of highly advanced AI systems.

Speaker 1:          00:56:18       How does Miri plan to deal with the agr research groups that keep large parts of their work secret such

Speaker 3:          00:56:24       as deep mind and possibly other unknown groups and say Russia or China? Um, so I don't know. I'm also not sure I like this whole deal with terminology. Again, please don't leave the whole problem to us. We're still, we're still only about half a dozen researchers. We haven't grown in the last two minutes. Um, I'll ask, uh, I would say, uh, that the, the type of work that we're trying to do is sort of the type of work where, uh, at the end of the day you fundamentally understand how to do AI lineman better. What do I mean by this? Uh, um, so let's say, let's say that you're trying to make a program that plays chess well enough to be the human, uh, and you know, the chess program, uh, uh, it makes some good moves and make some bad moves you and you tried to like by hand to patch the bad moves and then someone comes to you and they say, well, do you know how to play chess?

Speaker 3:          00:57:27       If I had to a sufficiently large computer, uh, you can name how large you want the computer to be and I'll give you a computer that size. Could you play chess in that case? Uh, if the answer is no. So the answer today is clearly yes. You run, you make the whole search tree, you backtrack. Uh, you know, you see whether white has a winning move. You then repurpose the computer to get all of the bitcoin is a good time. Um, but uh, uh, if the answer is no, if you don't know how to do it, even if I give you an arbitrary large computer, then you're, you're fundamentally confused about chess. Somehow. You're either missing the search tree data structure or you're missing the backtracking algorithm or you're missing some understanding of how chess works. Uh, there's actually a fun, a fun historical read.

Speaker 3:          00:58:09       Uh, you can read a essay by Edgar Allen Poe, uh, on the Mechanical Turk where I'd growl and Poe that mechanical Turk was a purported chess playing the automaton in the 18 hundreds. Edgar Allen Poe called bullshit. He was like, the Mechanical Turk is not a test playing automaton. I was a stage magician. I know where the Midget's hiding. I can show you. But first let me argue that it is fundamentally impossible for an Automaton to play chess. And then he gives an argument that's remarkably sophisticated. There is fundamentally impossible for an Automaton to play chess. He says, some people are talking about the machine and Mr Babbage and they say it can do arithmetical facts could in theory do chess. And I argue no, because arithmetical questions are deterministic. Each step folds from the previous, but chess is not deterministic. Your opponent's move, uh, it does not follow from your move. So the, the mechanistic wheels and gears cannot represent your opponent's move.

Speaker 3:          00:59:02       Uh, so the Automaton, uh, can't play chess. And uh, this is, this is remarkably sophisticated. A, he knew about computers. B, he identified the heartbeat of chess, which is like, Holy Shit. How do you deal with all of the opponents possibilities, right? It does indeed why chess is right, but he, he, he, uh, he went wrong. It's sort of like the last step and you know, then in 1950, a Claude Shannon is like, okay, so how are we gonna make this computers play chess? Clearly, if you gave me a computer as large as, you know, much bigger than the universe, I could make the tree do the backtracking. We'd be playing chess. But like in practice, that's going to be difficult. So we need the evaluation functions, right? Something changed between Edgar Allan Poe and Shannon. What change was whether or not we were fundamentally confused about what sort of thing trusses or whether or not we were fundamentally confused about what computers can do.

Speaker 5:          00:59:52       Uh, uh,

Speaker 3:          00:59:55       right now we're in the state where no matter how large a computer you had me, I could not make a, uh, a highly capable AI system that, uh, does even a very simple task like turn literally the entire universe in the diamond, right? We don't need to worry about any of the open ended gold problems cause we're trying to optimize an opened ended goal. We don't need to worry about these. Like, uh, uh, how do you reason given computing resources, limitations problems because we can just assume those way, right? Even if you gave me all the brute force in the world in a simple goal, we still don't really know how to align an AI system.

Speaker 5:          01:00:28       This is a roundabout way of saying, uh, that, uh, that

Speaker 3:          01:00:34       the type of work that we're trying to do is the type of work that went in between Edgar Allan Poe and Claude Shannon. I've sort of getting some of the fundamental algorithms them, some of the fundamental ways to think about it such that, uh, you sort of don't make very clever, very good sounding, very sophisticated arguments there turned out to be wrong for some subtle reason. Uh, you instead just know how you would do it if you, if the problem were simpler in some way. Uh, we're one way to simplify it is to assume you have a simpler goal. One way to simplify as assume you have lots of computing resources. There are many other simplifications you could do. A, and I would say that, uh, uh, getting back to the actual question, I, uh, the, the idea here is to get these insights,

Speaker 5:          01:01:13       get a

Speaker 3:          01:01:15       hi to the AI researchers to the point where they would sort of automatically know how to align a system, uh, with the intended objectives. Uh, if there weren't practical limitations and then be figuring out how to do it if there were practical limitations. Cause there are a, and I would say in the sense we don't, we don't really, we're not really trying to deal with, uh, people keeping their stuff secret. Where's trying to find these algorithms, these tools, these ways of thinking and uh, get them out to people who will take them. Uh, on that vein, I will say that we have a very good relationships with many of the leading AI teams. Uh, Google invited me to do a talk recently, um, but, uh, we have much less good relationships with a Russian Chinese teams. I don't speak very much Russian nor Mandarin. Um, but, uh, uh, so, so good relationships I think help, uh, propagate these ideas when you get them. But by and large, we don't see it as our prerogative to go around, you know, poking our noses into the capabilities work that other teams are doing, uh, and shake a stick at them. Something.

Speaker 1:          01:02:20       How do you expect the economics of artificial intelligence development to effects its safety? Would you prefer one company to develop AI or many?

Speaker 3:          01:02:27       Um, so I think that, uh, you know, I sometimes say about the AI, the Ai alignment problem that, uh, in order to, in order to handle AI alignment well, uh, we need, uh, so it's much harder to handle AI alignment if you're in an arms race. Clearly many things are harder in an arms race and safety, keeping, keeping safety concerns and not cutting corners are especially difficult than an arms race. Uh, so in order to deal well with, uh, with general intelligence, a humanity needs to, uh, uh, you know, exercise restraint and caution in the face of large economic incentives, uh, while handling a gigantic moral hazard, uh, uh, of the form where you have a small group of people who have the ability to, uh, affect what happens in human society. On a, on a massive scale, uh, seem to, you need to resist economic incentives while dealing with a gigantic moral hazard, uh, while writing code that works well on the first try.

Speaker 3:          01:03:27       Uh, and this is this, this does not bode well for team human. Uh, those, those three types of tasks are not things that humans are unowned, uh, for being great at. Right. One of the reasons that I think it's important to work hard on this now is just for once I'd like to see humans sort of go prepared into one of these, one of these giant technological shifts. We'll see. Uh, so how old economics, uh, affect the safety? Uh, you know, I think, I think the economics are definitely making AI. I go faster these days. We have talent, we have, uh, money pouring into the field and that's going to cause things to speed up. And what I prefer one company or many to develop AI. Um, I think this is less a question of preference and more a question of how the world looks. A, there's lots of debate in our, uh, in our circles of, of people, uh, doing this sort of research about whether or not, uh, the development of Ai looks more like, um, the development of a single software project or the development of like an open source community, right?

Speaker 3:          01:04:34       Where, uh, if AI is sort of like the open source community, if it's like a broad base of things that was worked on by a lot of people, uh, then it's sort of sort of weird to ask what if one group creates it and write it sort of this ecosystem that's growing. Uh, and it's sort of hard to imagine one group, uh, under control of the AI or something. Whereas if AI is more like a search engine, uh, then even when you have lots of different groups trying to make the, uh, search engine, uh, you're gonna have, uh, empirically in the past, uh, you have sort of monopoly tendencies where it's not that Google was the only team working on search engines, but Google sure captured the search engine market, right? Like there was, uh, uh, there was Yahoo, there was that fish thing. Who remembers what it is anymore?

Speaker 5:          01:05:26       Uh, uh, and yeah, and, um,

Speaker 3:          01:05:35       uh, sort of control of search engines like development of search engines with something where uh, uh, tools didn't easily transfer when someone made Google a little better. It took a while before he got Yahoo, got a little better. Uh, and uh, and the, the economics were pushing the direction of monopoly. And in that case it's sort of hard to imagine the world ending up with an open source Google, right. Uh, like there's a lot of things Google's doing, they would just be real hard for an open source community to navigate. Um, so I think the question of whether or not we're going to see AI that's sort of a broadly distributed or, uh, very, very locally controlled is a question of like, which type of softwares it, is it more like this monolithic type search engine type thing where, where you're, you're building it and it's hard to transmit specific components to other teams? Or is it gonna be more like this open source thing where it's a big ecosystem and it's hard to even central area if you wanted to, uh, my bet as to which world we're in. Is that just more like the centralized world, uh, as to which world I'd like to be in? Um, I dunno. I, that's not, that's not really how I model worlds. Alright. I'll just deal with what's ever wanted. It turns out we live in

Speaker 1:          01:06:50       what strategy and tactics for AI risk mitigation. Would you recommend the Googlers who share your views on the risk? If you are a random Google engineer, would you focus entirely on donations to Miri, try internal cross team advocacy, try to join deep mind or something else

Speaker 3:          01:07:03       entirely? Um, you know, one thing that I would do if you're interested in this work is I'd reach out to Chris Olah. I hope he doesn't mind me saying this, uh, basic Google or he wrote the, uh, he was a coauthor on the concrete problems paper. Uh, and uh, yeah, if you're interested in it, look them up, email them. Uh, there's probably interesting stuff you can do, especially if you're already a brain, um, in terms of how one causes more risk mitigation.

Speaker 5:          01:07:32       MMM.

Speaker 3:          01:07:35       You know, it's, it's hard to say, sort of depends on the person. Uh, I think, you know, 20 percenting on a interesting AI safety stuff is probably a pretty good time. Um, and I'd say you're always welcome to get in touch if you have more specific questions that we had a link up earlier, but, uh, it's contact@intelligence.org very hard to remember.

Speaker 5:          01:08:00       Uh, and

Speaker 3:          01:08:02       yeah, I think it, I think it depends on a situational basis, but more eyes on the problem, more people solving the problems, uh, is one of the key things I'd encourage when you were talking about, uh, the problems of, um,

Speaker 1:          01:08:16       the analogy with natural selection and the contextual changes that creatures might or might not go through in natural selection being pretty small. Um, it occurred to me that, uh, humans are very different from cave salamanders, not just because they're far more intelligent than cave salamanders, but because they've been dealing with far more contextual changes for the past 20,000 generations than cave salamanders. Perhaps I've been sitting in a cave. Um, so, uh, you know, the changing social circumstances and the changing weather and the changing Savannah conditions and all the rest of probably programmed into have forced natural selection to program into us an ability to deal with those conceptual changes that the cave salamander, it doesn't have. Would it be useful to do something similar in creating the genetic algorithms or the hill climbing algorithms that would, uh, get us closer to AI, uh, by programming and this ability to this, this requirement and the simulation that there's a continuous changing in the contextual elements necessary to get to that objective function.

Speaker 3:          01:09:21       Uh, so, uh, I'll caveat first bat. Uh, I'm, uh, I'd want to push in the direction of sort of more transparent algorithms and less, less brute force hill climbing algorithms, uh, uh, given the opportunity to do so. Uh, but I do think it's very useful to, uh, train the system under many contexts changes. Uh, so, you know, you shouldn't, I think it would be foolish to test the system quite a bit, and then for the first time, uh, that you give it a lot more resources than I've ever had in test environment. Uh, you put it like out in the world, uh, to do whatever. Right? That would be, there'll be sort of reckless. Uh, so one of the, one of the things that happens when it moves from the test environment to the real world is that the space of the world's modeling becomes quite a bit larger.

Speaker 3:          01:10:08       I, it goes from your lab to the entire universe. Uh, so this is a pretty big shift. Uh, I mean, you might be able to mitigate this in some ways. And then, uh, in terms of available resources, if it starts thinking about things like, uh, acquiring computation resources, which you might not want to design a system that does that, but it starts thinking about things like acquiring computational resources. Uh, then once it leaves the lab, it sort of has, uh, uh, far more resources at his disposal and it did in the lab. Uh, potentially. And if the, if, if the first time that you're testing its ability to handle dramatic changes in what the world looks like and what resources it has available is deployment, uh, you're probably gonna have a bad time. Uh, so I, I am very pro, uh, testing context changes as well as you can.

Speaker 3:          01:10:56       One of the things in the lab, I will also say that I think this is not alone sufficient. Um, if you imagine a, if you imagine a human on a planet, a very slow aliens, uh, or the very slow aliens are cubs in the human to like go through a series of tests before they released human into the world, it's pretty plausible that the human can tell which one's the final test environment. Maybe not, but maybe, uh, and to some extent, uh, you might want, you know, if aliens like put up like some canvases that they paint, like backdrops on and that they think, you can't tell, they think you might think it's the real world, right? Like, uh, you've either got to be really, really sure that you've tested this system, uh, in context changes where it believes this is the last context change, uh, or you've got to have reasons beyond, uh, your trial and error and your tests to expect that, uh, after the final context change that it will continue to act as intended. And I'd sort of suggest both, when you're dealing with things that have the ability to transform a civilization, uh, you should not be trying to skimp on, uh, on your safety precautions. So I would say both test the hell out of it, uh, especially under context changes and have additional reasons to believe it's going to work. Thank you all for attending and thanks, Nate. We're talking to Google.

Speaker 2:          01:12:13       [inaudible].