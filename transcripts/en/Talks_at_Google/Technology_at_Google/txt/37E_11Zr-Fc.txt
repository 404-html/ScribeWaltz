Speaker 1:          00:06          Thank you all for coming. I really appreciate it. Thanks for the opportunity to come speak here. Uh, today I'm going to be speaking with you about my book privacy's blueprints, the battle to control the design of new technologies. And I really wants to, uh, start off with a series of stories to tell you a little bit about what the book is about. So many of you, if not most of you, are familiar with the Cambridge Analytica, Facebook, um, dust up as it were. Uh, and there's a, it was a lot going on. There were lots of information that was taken from Facebook, uh, that eventually made its way towards Cambridge analytic and an outward. And it caused a huge, um, public discussion about privacy and platforms. And there was a lot going on built in with that. But one of the most interesting things was, uh, the fact that there was a big debate about whether this was originally what they referred to as a breach or something else.

Speaker 1:          01:08          Ryan. And one of the things that sort of came up and one of the things that, that indeed the platform actually said is that that permission was actually granted, um, uh, through the privacy settings to have this sort of information given to third party apps. And in fact it wasn't just, um, the one app that was collecting lots of information. This was actually a relatively common practice that people were agreeing to. But if you actually start to dig in and think about, well, what was actually agreed to and when did it look like you realized it was sort of one small little option nested within lots of other small little options that existed. And while you may technically have agreed to it, um, it's, it was sort of binary and relatively hidden. Um, and so it resulted in a lot of confusion because the people that seem to be angry about it said, well, wait a second.

Speaker 1:          01:55          I didn't even agree to that. And then of course they were shown exactly where they did and they said, okay, well maybe that wasn't the best thing. The second story I want to talk about. This is a still taken from an internet connected baby monitor. And the most disturbing thing about this, and this was disturbing in and of itself, isn't that that there was this one still, but rather there's an entire search engine dedicated to finding these sorts of stills because these sorts of devices are so routinely compromised. Um, and, and so easy to, to sort of take control of that. There's actually an entire collection of them online. Um, which I thought was also relatively disturbing. And then the third story that I wanted to talk about was what many of you may have been familiar with, which is the dispute between the Federal Bureau of Investigation and apple over whether to build in some sort of bypass to its encryption scheme and an authentication protocol to allow it to access the phone, um, of the San Bernardino shooter.

Speaker 1:          03:01          Now what are these three things have in common? They all are stories about the design of information technologies. And in the book I focus on how things are built and the way in which they affect our privacy. And I want to make three points today and I make try to make three larger points in the book. One design matters for privacy and more than that, which may seem relatively evident, we need to have a better conversation to explore why design matters and how design matters for our privacy because I think it matters more than what most people regularly consider too. And this is the main thesis of the book, which is why it's in bold, which is the privacy laws should take design more seriously and not just the law but also uh, industry policy, industry standards, industry norms. We focus a lot on personal data, which data, what data is being collected, how is it being used, who is it being sharing shared with.

Speaker 1:          04:01          We focused a lot about transparency, but I argue that there's actually, there's a, there's a gap, there's a, there's a, a huge area within which design affects a lot of what people to be considered, their personal privacy interest that we don't regularly talk about in law and in industry policy and three, and this is perhaps one of the more controversial aspects of the book. A design agenda for law and policy should have roots and consumer protection and surveillance law, not the standard data protection framework that isn't becoming incredibly popular through frameworks like the Gdpr, which everyone has probably at least Molly, where I've given the thousands of emails that you received somewhere around late April that says we've updated our terms. Um, and it's built around this model that tends to prioritize certain kinds of data collection and processing rules. I think at the expense of ignoring some nuances about the ways in which design actually controls our privacy. Okay, so let me expand upon these three points. One, design matters for privacy. What do I mean by that? I mean the design matters for privacy because it is everywhere because it is power and because it is political. First design is everywhere.

Speaker 1:          05:24          Many people might recognize what this is. Does anyone know what this is?

Speaker 1:          05:30          Has anyone seen this before? It is a promo user interface, early user interface for the APP. Snapchat. Now, if you were to look at this knowing nothing, sometimes I give this talk and I, I speak with people that haven't used technology that much or only sort of casual users have a lot of apps and so they've never seen this at all. And so I like to sort of pause and I say, if you had never seen this before, what would you, this user interface is, it is designed to accomplish and some of the usual answers I get are, well it's probably something that relates to taking and sending of a picture, right? We can tell that exists for, we saw the timer here. Where does the time or probably do if we had to guess. Yeah, it, it, it makes the, the picture of disappear after maybe a certain amount of time.

Speaker 1:          06:27          We see the scroll wheel that we can select, right? So we clearly have options, right? It's clearly meant not just for taking photos and storing them, but for sharing them. We can tell because we look at, we see a sin button, right? It's got the little arrow. And then one of, one of my favorite things about this user interface and one of the, one of the most demonstrative sort of signals that we see about the way in which this is supposed to be used. What do you think this particular app, just looking at this, what kinds of pictures do you think the APP is inviting you to send? Naughty pictures, right? It originally, this is how snapchat was sort of perceived publicly, right? Uh, and you can't blame them because look, we've got an incredibly carefully cropped a little square here where we can't tell whether these young women are clothed or not.

Speaker 1:          07:18          They're having fun. And the implicit message built in through every aspect of design of this user interface is that you can trust this app with things that you might not trust in more permanent sorts of social media, right? Because it goes away. It is safer than if you were to use, say, a some other service that, that where the pictures are stored forever. And all of this is conveyed with less than three words. It's all signals. It's all things that we intuitively seem to understand through the design of information technologies. And I started looking for examples for this book. And once you start to look for examples of the ways in which design affects your privacy, you see them all over the place and, and not just in in obvious ways, but in subtle ways. So I don't have to tell everybody here with this technology is, and there were some design decisions that, um, that went into Google glass and I thought were incredibly interesting.

Speaker 1:          08:18          The most obvious privacy relevant feature for Google glass is what the camera on the front, right? The one that's, it's literally staring straight at us. Um, but that's not the design feature. I mean, I guess you could have designed glass without the camera. That was a design decision that implicated, um, people's privacy because it's able to be surveilled. But of course, the counter argument to that is this camera doesn't really do a lot that isn't already being done by another camera. Right. The one that everyone has in their phones. And so the design decision that's relevant for Google glass is not just that there's a camera, but the camera is always potentially visible. Right now there's a small transaction cost, but it's significant at scale, which is in order to take a picture with our normal phones, we have to reach into our phone, pull it out, right, open it up, we fit a little bit with it, we aim it, and then we take the photo.

Speaker 1:          09:19          Now that doesn't seem like a lot on a per time basis, but that small transaction cost actually I think has a tendency to make people feel more comfortable with the fact that everyone is carrying around a sort of pres, a persistent a surveillance device in their, in their pockets all the time. Whereas glass was net with a much more resistance, right? People tended to be more uncomfortable with it than they did with their phones. And think the reason why is because the camera was always there, right? The transaction costs of having to dig into your pocket was suddenly gone and I had to do, was utter a command or um, uh, sort of readily activated. There was another design decision that I thought was really relevant to Google glass. They didn't get a lot of attention, but it's my understanding mid glass did not provide support for facial recognition technologies, which was an interesting design decision.

Speaker 1:          10:14          Um, and one that I think also had privacy implications, positive privacy implications. I don't have to tell anyone here with this symbol is as well, right? But for those that aren't familiar, this is the symbol of incognito mode. When you open up Google chrome browser and you go into incognito mode, you see, or at one point I believe that the icon is now changed. But at one point you saw this icon and I always like to ask people, this is a, this is a design, right? It's a, it's a, it's a signal to people. And if you were to just see this, what would you think the signal was? What is this meant to convey?

Speaker 1:          10:55          I'm sorry. Concealment, right, which is of course it's the purpose. I mean it's called incognito mode and we've got our, it seems like a sort of classic 1940s gum. She right. Someone that doesn't want to be seen, they put the, they can put their collar up, they've got a hat that they can pull down low, maybe glasses. Again, the, the implicit message just from looking at this is that you're safer if you want to hide. Now, of course, that's not exactly how incognito mode works, right? And, and indeed, when you open up incognito mode, um, you've got a list of things they say, by the way, here are all the people that can still see you when using incognito mode, which is a very important disclosure. But absent that, you might think, oh, well I'm using the same browser, right? The browser, that doesn't record any history of me at all.

Speaker 1:          11:44          Right? It's completely incognito. And so without that, I think important disclosure, this design would tend to shape people's expectations. And so when you look around, you tend to see the sort of design decisions everywhere that tend to be relevant for people's decisions about what to disclose, how much to disclose, and how safe they are in disclosing. Now, the second point that I want to make about design, if the design is power, one of my favorites experiments that I talk a little bit about in the book was done by Leslie John. I'm Alessandro queasy and George Loewenstein at Carnegie Mellon University. And they got several people in to answer this series of questions in a survey in a on a screen. And they sat down and they were presented with a screen like this and they asked him a number of different questions and this is just a snapshot. And the questions ranged from sort of banal to relatively intimate to incredibly intimate.

Speaker 1:          12:41          And this is just a short little snapshot here. They said, have you ever smoked marijuana? Have you ever cheated while in a relationship? Have you ever driven when you were pretty sure you were over the legal blood alcohol level asking you to admit to committing a crime? Now, what's the first thing that you notice when I put this screen up? How bad are you? Right? And so there were a series of design decisions that I think were particularly relevant here. One is the fact that bad is sort of capitalized, right? And then they use the sort of cutesy, you sort of tech speak, right? How bad are you? Multiple question marks, which is indicative of what youth, right exuberance. What else do we have relevant to design here? The logo, right? I said devil, but it's not a bad devil. It's like an Emoji devil.

Speaker 1:          13:40          Like acute devil, right? There's one other design decision that I thought was really interesting. The font, it is written in Comic Sans and no one in the history of the world is taking anything seriously written in Comic Sans Font, right? And the overall implication, of course, of all of these design decisions, despite asking incredibly intimate questions, questions that can implicate you in a crime, is that you're, how bad are you? You're bad. You know, you're bad. Everyone was just a little bit bad, right? That's sort of the implication of every single design feature built into this. And then of course the control group, they had looked like this. Gone is the sand Saraf is, is the Comic Sans Font replaced with this, you know, very formal sand Saraf. You know, funds gone is our cute little question about how bad are you and now it's just replaced with the imprimatur of Carnegie Mellon University, a very prestigious university.

Speaker 1:          14:42          Right. But the question's remains exactly the same. Right? And, and I want to read directly because I don't want to miss quotes the results of their study. Um, but they call this the frivolous looking interface in this, the non frivolous looking interface. And uh, they found that relative to the non frivolous interface participants in the frivolous looking survey that asked identical questions, we're on average 1.7 times more likely to admit having engaged in risky behaviors. For example, a participant in the frivolous looking survey was on average 2.03 times more likely to admit having ever taken nude pictures of himself or a partner. And the authors conclude people, it seems, feel more comfortable providing personal information on unprofessional sites that are arguably, uh, particularly likely to misuse it. Design is power design doesn't necessarily dictate behavior, but it channels its every single design decision makes a certain reality more or less likely, right?

Speaker 1:          15:56          And so it, it provides an incredible amount of power. And that leads me to my third point, which is that design is political design is always political. And when I say political, I don't mean political in terms of Capitol Hill. I mean political in terms of the distribution of power. So I give this talk a little and sometimes in response, people will come up afterwards and they'll say, why are you targeting design as something that we should be focused about in law? Right? Instead of targeting the tools that we use to create, to, to create problems, why don't we target the harmful problems themselves? One of the examples that's been put forth recently is a knife can be used for good or bad, but we don't ban knives, Ryan and said, we say you can't use a knife to stab someone, but you can use it to cut food.

Speaker 1:          16:45          And so, um, and so it's always some sort of version of the argument that, that there are no bad is only bad uses of those technologies. And that should be the focus of our policy and law efforts. But I think that that actually, um, tends to, uh, cut against the fact that I don't think that there's such a thing as a neutral technology given the fact that every single design decision is meant to affect the world, right? That's what design is. It's a series of decisions that we get reflected in some sort of substance that affects the world and in some particular way, then there's no such thing as neutral technologies. Even ignoring certain realities is an acceptance of those realities in a certain sense. Um, and so we cannot ignore technology in the role of design, in law and policy. We have to confront it head on.

Speaker 1:          17:40          Um, and there, there may be reasons why we explicitly decided not to have rules against legislating technology. We don't want regulators, for example, dictating from top to bottom, every single design decision in a ham fisted way, right? Because, you know, it's, it's difficult to think that they would know better how to build a lot of particulars in technologies and people that live it. Um, but that too has its own costs. Right? And we need to be specific about which costs we want to embrace, right? And how to better balance them with the full range of concerns that we have. And the only way that we're going to do that is if we explicitly embrace the design of technology and how that is powerful in our laws and policies. So that leaves me to my next point, which is that privacy law should take design more seriously because right now it doesn't, it may seem like it relatively does and there may be some, uh, in this room are listening in online that have had experience dealing with regulators and the, with the European Union and the general data protection regulation that deal with privacy by design and privacy by default.

Speaker 1:          18:54          Um, but actually I will argue that largely privacy law and privacy policy and the rules that we have even internally about privacy have a major design gap turns out. So I teach privacy law at northeastern university and it's a whole semester long course, but it turns out there are basically only three privacy rules that you have to follow. Um, and so I can save you all a lot of money. If you're interested in taking a private slot course. I'm just gonna summarize it now. One of the rules is follow the fair information practices. This is the underlying ethos of the entire general data protection regulation. It's the, it's the idea behind, uh, statutes that you are probably very familiar with, like the fair credit reporting acts. Um, oh, even HIPAA has a, a sense of, uh, follow the fair information practices and the fair information practices are things that are so fundamental to our understanding of fairness with data processing that you probably have already internalized them without thinking about them.

Speaker 1:          20:00          For example, people should have control over their information. They should, uh, databases should be transparent about what they're collecting. They should give people notice about what they're collecting and the ability to delete information or correct information that you should only collect what you need and that you should delete it when you're done with it. Right? The data minimization principle, there's the principle of accountability that we should be accountable for these rules. There's the principal, um, that, uh, people should have certain sort of access rights for portability rights to their data. These are all things that we've been talking about for a long time and they all started with the fair information practices, which are actually originated in the mid and late 1970s and then made their way through and now are as close as the world has come to a common language of privacy, which is pretty remarkable when you consider all the varying different cultural connotations that play into conceptions of privacy.

Speaker 1:          21:00          The fact that we have a basic set of rules on how to fairly collect and process data is remarkable and the fips are important. They're necessary. But here's the problem with the Phipps, they look like this, right? This is the way in which they get reflected because if you look at the Phipps and almost always boils down to this idea of control, if you ask the CEOs of every major tech company in the United States in the world, how do you respect the privacy of your users? I bet money that they were almost all respond with some version of we put people in control of their own information. This is one of the most dominant conceptualizations of privacy in the United States and around the world. The idea being that if we give users control of their information and they get to decide for themselves what they want to do with their information and what they're okay with, they do a risk calculation and if they are given that control, then if they exercise it, then presumably the giving of that control is enough to justify certain sorts of data practices cause people approve it, right?

Speaker 1:          22:15          They have control over it. Data Subjects, autonomy in theory is respected. The, the uh, platform. Collecting information has done its moral duty under this conceptualization and my offering, the control and we can all proceed together. And here's the problem with that. And here's a problem with control. Think of thinking privacy in terms of control generally, which is really popular, not just an industry. Governments conceptualize privacy is control. Advocates conceptualize privacy's control. It is by far probably the most popular way to define privacy. I teach privacy law and on the first day I asked my students to write out what privacy is and say, what is privacy? And then we go around the room and I have 20 to 30 students and I usually get around five to 10 different answers, but almost always the most popular response is control over personal information. But again, the way that control is reflected in practice is through design.

Speaker 1:          23:21          It is through buttons, right? Toggle switches, which in theory is still fine. Everyone knows what this is. Green means yes. Gray means no and it's a toggle switch. Can Google maps collect your Geo location information? Green means yes. Gray means no. Yes, I have exercised control. I have made a decision. Privacy is respected. Onward. We go, except of course it doesn't always work out like that because it never looks just like that does it. Right? We have a series of decisions to make because we don't want the decision to be just binary. If it's too binary, it's two abstracted and I privacy is complicated. There are nuances, right? Yes. No doesn't give enough choices. So we say, okay, I need lots of choices. I want nuance, right? This is, this is the inherent tension in defining privacy is control is give me nuance. So tech companies respond and they say, okay, we'll give you lots of different choices, right?

Speaker 1:          24:24          Rather than just sort of one off by an area, we'll give you lots of choices. Except of course that means that users are now responsible for analyzing those choices. And you say, okay, I've got, I've got that for the find my iPad button and in location based alerts, I guess that's good. Location based I ads, I don't know what that means. Share my location green. Okay. I guess I guess I'll try to figure that one out. Wifi network. I don't know. I don't know what that but does, but I guess it's okay. Um, diagnostics and usage. Peep popular near me. I guess. That's good. And we have to make all of these sorts of implicit risk calculations over and over again, which is costly in terms of personal resources, right? Because people have limited amounts of time. So we go through, but okay. So let's say we look at this, we make our design is set up in this way for us to to effectuate the control that we're given.

Speaker 1:          25:13          We do it and we say, Ooh, okay, I think I'm, I think I'm good. I think I've set my settings the way I want them. And then we, we take a deep breath and we back up and we say, Oh dear, I've got lots of buttons to press over and over. Right? And when the settings change, I've got to update them again. The problem with thinking about privacy in terms of control is that people are gifted with so much control that they choke on it and it becomes in practice away for a risk of loss to be shifted onto users. Because if you fail to exercise that control, it's not the problem of the company. They gave you the control, right? They're giving you the options, that's what you wanted. And so it's, there's an inherent tension but almost as never going to be resolved, which is if you abstracted away and make control easy, if you provide one button for all of our our issues, then it becomes too generalized right there too.

Speaker 1:          26:14          Too much stuff washed in. And so our ability to do a, a meaningful risk calculus is washed away. But if you provide every option under the sun, then it becomes overburdensome, right? It's a, it's a d dos attack on our brain and there's, there's no way that we can respond. It's scale. And so control I think is actually one of, uh, fundamentally one of the misguided actually ways to think about privacy in the modern data ecosystem because it's never going to, it just doesn't scale. Right? And left we started having a conversation about prioritize your control, which gets really interesting, which we can talk about later. So there's this massive design gap in follow the Phipps, this ethic that we follow in all privacy law and and all and industry sort of rules, which is you should give people control because design is a leveraged to instead of being autonomy enhancing and actually it's autonomy, corrosive, right?

Speaker 1:          27:09          Because we're given all of these choices were sort of burdening them or it's abstracted away to the point where it's meaningless. All right? That's rule one. Rule Two is do not lie relatively simple rule, right? The Federal Trade Commission, which is the nation's top a regulator of privacy has a authority to regulate unfair and deceptive trade practices and as part of its regulation, it has one major rule is just don't lie to people, right? That sounds relatively easy. The problem with the do not lie ethos of course, is one of the similar problems to transparency and control generally, which is you can put technical truths in a place that nobody's going to read them and we all know where you put the things where you nobody to read it. All right, where do we put it? If I want to be sure to put something in a place where I know no users ever going to see it, where am I going to stick it?

Speaker 1:          28:04          The privacy policy or the terms of use, right. I do this for a living. I am a a a privacy law professor and I cannot scroll through the terms of use quick enough to get to, I agree, right? Because of course there's no way that it that at scale anyone could collectively read them. Right? It'd be we'd have to take two days off of vacation a year just to read all the privacy policies we come into and so do not lie. Can also get circumvented through design, through putting it in a place where technical truth is is ignored, and then there's the do not harm ethic. That's the last one. So the Federal Trade Commission also says don't do, don't engage in unfair trade practices. That's harm users in a way that's a not avoidable by the users themselves and in a way that's not out balanced by countervailing benefits to the consumer.

Speaker 1:          28:56          That's the sort of tests they use. But of course, the do not harm ethic is also, uh, easily subverted through design because in the modern age, the threshold for harm to rise to a legal violation is sort of high, right? Um, they, if, if you're going to bring a lawsuit against the company, then you don't want the harm to be a, this is creepy, right? Which is some of the things that happen, right? And creepy. It's not a, it's not a real word that has boundaries around, right. What is creepy? Lots of things could possibly creepy. Um, but that varies wildly according, you know, across the spectrum. And so the law and in many ways rightly actually says, you've got to have a real tangible harm here. You've got to have some sort of financial harm, right? If someone, your identity, someone stole your money, or you have to have some sort of clear emotional harm, right?

Speaker 1:          29:46          And often it is your naked body was exposed publicly, right? Or something, something very visceral and something that, that has a clear boundary to it. But of course, many of the sorts of things that people quantify as privacy harms in the modern age is not actually those sorts of visceral harms, which can be relatively rare, but rather it's a little bit more of a death by a thousand cuts, right? We reveal a little bit of information here and we trickle a little bit of information there, none of it, which rises to the level of what we would consider to be a privacy harm. But collectively we look up one day and we all become vulnerable, right? And we all see that that's, we've gone down this road without actually having any one particular violation. Yeah. And here we are with lots of our information sort of expose that can, that could ultimately be leveraged against us.

Speaker 1:          30:36          And so design also sort of does that through sort of the way in which we encourage short little amounts of information to be disclosed here. And they, oh, it's just a little information here. It's just a little information there. But collectively it becomes a big issue. All right, so at this point you may be saying, all right, smart Guy Wolf. This, we've got the problems, what are we going to do about it? And that actually is the next part of the book, which is why I proposed a theory of privacy, law and design and the theory is actually built around values, boundaries and tools and the values that I advocate for or actually not control. Even though that's the dominant framework, I think there are better values that we can embrace. Not only that will give us industry in US law a much clearer identity with respect to protecting personal information but also one that doesn't sort of inherently have this tension where you have to choose between a meaningless abstraction or overwhelming nuance and that's trusted security and autonomy.

Speaker 1:          31:38          In other words, I argue that our law and policy should encourage design that is trustworthy that promotes or at least values obscurity and enhance his autonomy. Now what do I mean by that? Trust I think is one of the most important values, uh, in the modern age. With respect to the disclosure of personal information in platforms, trust is key for commerce. Trust is key for personal relationships and intimacy. Trust is key for self exploration. We tend not to disclose to other people in a, in an attempt to sort of figuring out who we are unless we trust those, if everything we say can be used against us and we tend not to sort of engage in self exploration. And so our design should reflect that our design should reflect a sense of discretion, not necessarily confidentiality, but just discretion. We don't disclose everything all the time, right?

Speaker 1:          32:32          This can be de identification, this is gonna be disclosure within a limited community. Um, it should encourage a sense of protection. Now this is a little more obvious, right? Which is a don't store people's passwords in clear text and salt and Hash and uh, let's encrypt traffic. We should protect the data. This is what we would refer to. Maybe it's data security, a sense of honesty and honesty is different from transparency. Transparency often gets touted as, look at we, we've opened up the books to you, right? We, you can look inside and you can see whatever is available. But that's actually different than what I would consider to be acting honestly, honestly is affirmatively disclosing the things that users want to know that maybe you would prefer not to tell them that they should probably know. Right? In other words, it's, it's a little bit more of affirmative obligation, um, as a warning rather than full transparency.

Speaker 1:          33:27          And then finally, and this is the, this is the hard one with respect to trust. Um, design should be loyal to the user. And what I mean by that is it shouldn't elevate the interests of the platform unreasonably in or an unreasonable ways over the interest of the data subject. And we can talk about what I mean by that. Uh, Eh, in a minute. And, and that would be trustworthy design design that is discrete, protective, honest and loyal. Now another sort of a design choice which doesn't, isn't quite as established in law and policy is the value of obscurity obscurity is the idea that when information is hard are unlikely to be found or understood than it is to a relative degree. Safe. We rely upon zones of obscurity all the time in our everyday lives. And we don't even realize it. How many people here have eaten out at a restaurant within the last two weeks?

Speaker 1:          34:32          A number of people. Do any of you remember who was sitting two tables away from you? Probably not. You were in public. Everyone can see you. But of course we've delong since deleted that information. I flew here yesterday on an airplane. I don't remember what the person sitting next to me looked like, right? Even though it was in public. And we do this to help sort of ease to prevent cognitive overburdening and we rely upon this risk calculus all the time to make decisions. Um, when I'm walking out in public, um, you know, if, if you know, you quickly want to sort of scratch in a, in a delicate place and you look around, no one's looking right, you do it right. Even though maybe if that were posted on the walls, the, uh, the Times Square Jumbo Tron, then you would have second thoughts about it. But the odds of that are incredibly small.

Speaker 1:          35:23          When you go shopping in a grocery store or a drugstore, um, you're in public, you're picking out things that are delicate, but the odds that someone is standing right behind you writing down every single thing that you purchase is incredibly low. Right? And the odds of that ever coming back to hurt, she was also incredibly low. So we value obscurity and the harm comes when our obscurity is, is taken away from us in dramatic ways. In other words, they're lurches obscurity. Lurches um, I've been, I've written a very clear critically about facial recognition technologies and one of the reasons why as I view it as an incredible obscurity, lurch our faces while public, um, basically allow us still to be relatively private as we walk into a crowd. But facial recognition technology is threatened. That ability, uh, it, it, it represents a dramatic obscurity, lurch and online.

Speaker 1:          36:16          Uh, some of my research is focused on, there are things that can obscure you or make you more obvious like search visibility, unprotected access, right? Whether we protect something with a password, um, whether someone uses their real name or a pseudonym or no, no identifier at all and whether things are clear, right? Sometimes things are obscure because people like the context to make sense of them. If I were to tell you all right now, um, hey, by the way, I just got the test results back. It's positive. You would have no idea what that means, right? It's obscurity you and it's safe to me because you have no idea what that means. Um, but if you had the backstory, which is maybe that my wife and I were trying to conceive, then you would say, oh, I now understand, right? This is sort of hiding in plain sight with content.

Speaker 1:          37:04          And so there are ways in which we can obscure information and preserve that obscurity. And then finally, the value that I advocate for is autonomy. Now autonomy is difference than control. Control can serve autonomy, right? The right to be free from external interference, the right to self determination, but, but if too much control actually threatens that autonomy. So I argue that a better value to embrace is autonomy rather than control. All right. And so that leads me to the final part of the book where I argue that a design agenda should have roots and consumer protection and surveillance law rather than saying, well, the fair information practices, which everybody accepts, uh, are, uh, the thing that we should, we should adhere to. If you look, I said there are actually other areas of the law, but if thought this out that if taken design seriously, then I proposed sorts of boundaries, uh, to avoid deceptive design, abusive design and dangerous design.

Speaker 1:          38:04          Deceptive design, uh, is something that it, it, we relatively, uh, are doing a good job with both, I think within industry and within policy in terms of not outright deceiving people. You know, if it says this is a path complaints filed by the Federal Trade Commission where add friends actually didn't just allow you to add France, but it actually automatically sort of went in and scooped up the entire contact list. A, and then there's also the idea that maybe we should avoid abusive design, which is the leveraging of people's own limitations against them. And this happens sometimes in everyday life. We tend to sort of leverage people's own limitations against them. But it's another thing entirely to build an entire machine built to leverage people's own limitations against them. This is an example of um, pricing, different differential pricing based on people's personal characteristics or the thing that made my privacy law students really mad when year, which is, uh, the fact that a certain travel, a website could tell whether using a PC or a Mac and would show the, um, the cheaper, um, uh, hotel rooms to the people using windows pcs and the more expensive hotel rooms for the people using MACs based on the assumption that people that had Macintosh's we're actually being more, were, had more, had more money and we're more affluence.

Speaker 1:          39:29          Um, but there are tons of examples. So one of the examples that pops up is, is when leveraging people's own limitations against them is our desire for conformity, right? People have a strong desire sort of not to be, to be with a group or to do what everyone else does and sometimes that can be worked against this. So, uh, this shows up in the literature around dark patterns, which is a, a really interesting website. And this is one where it says get 10% off. And then at the bottom it says, no thanks. I'd rather pay full price for delicious tea, right? Which is sort of shaming people, right? But like, oh, okay, well I don't want to be the sucker here. And there's several other examples where you see, because people's, uh, po voting records tend to be public in most states. Um, some people have actually leveraged that and they said, all your neighbors voted and you didn't write.

Speaker 1:          40:17          Here are the people that voted and you didn't votes. Look, look how irresponsible you are, right? Which is sort of a shaming, um, or a a one here, which is, um, at the bottom it says, no thanks, I'm fine with losing customers. I mean, who wants to click that right? Now, again, this is a very subtle sort of manipulative technique, but it's one that is at scale, could tend to be a harmful in the right way. Another example of using people's own limitations against them is, uh, the use of like double and triple negatives, right? So here it's decline. Release of directory information. Note, most parents do not choose this option, right? So we've got lots of knots here. I do not want the release information. So when you start layering like double and triple negatives, then it's not, it's not lying, right? It's not, it's not deceitful, but it's, it's leveraging people's inability to sort of process the double and triple negative.

Speaker 1:          41:12          I don't not never want this information released. Um, and then finally there's dangerous design. Some designs are just inherently dangerous. This is a spy camera. It's designed for more or less one thing, right? Which just to catch people in various states of undress, right? Because it's a clothes hanger, right? It's meant to be put in rooms where people are taking clothes on and off. And I argue that that's dangerous design is as, as dangerous as surveillance that you know about is, I've used surveillance as you don't know about it to be even more potentially dangerous and there are lots of potential responses to this and I go into this and the second part of the book, soft responses, moderate responses in robust responses, so a soft response is we don't have to come in as regulators heavy handed and say we're regulating this to the hilt. Sometimes what privacy needs is some funding, it needs some innovation, it needs some opportunity.

Speaker 1:          42:05          It needs standardization, right? It needs educational opportunities. So I argue for a lot of different ways to improve people's privacies without passing some really heavy handed regulations. Sometimes we need to moderate response is maybe judges just need to be a little more sensitive to the role that design plays in people shaping people's expectations and their choices and maybe regulators. Do you see this is an example of the padlock icon, which is ubiquitous online. We use it all the time. It's the, it is. It is the the physical symbol of security, but we don't really question what it means to people as much. Right? When people see the padlock icon, they usually think they're safe or secure, that that's what a lock means. But what does, what does it really mean on the practice? And maybe there, there are ways in which these sort of Paluch icons might act as as sort of implicit promises.

Speaker 1:          42:58          And then finally there may be a need for heavy handed regulations in certain instances. One of the things I've argued for is in support of as the ban on spyware that we have right now, which is I think relatively, um, can be used and really malicious ways. And then another thing that's a little more controversial is I've argued in favor of moratoriums and bans on facial recognition technology, which I view as one of the most uniquely dangerous technology surveillance technologies ever invented. And then the final part of the book, I, I lay out what these, this blueprint of this design agenda might look in three different kinds of context, social media, hide and seek technologies and the Internet of things, which is basically just a plea from me to stop for, to tech companies to stop connecting people's underwear to the Internet, which is one of the things. So, um, and with that, I would love to go ahead and go to the question answer period. Thank you very much. I appreciate it.

Speaker 2:          43:57          Given some examples of bad designs, uh, can you give an example of a good design, particularly in the area of providing user autonomy as you described?

Speaker 1:          44:05          Oh, okay. Um, that's, uh, uh, actually there are several, I think very good designs. Um, one might be, there are areas in which, so after I sort of rail on control, um, a lot, there are areas in which control might actually be really useful. I liked the data subject rights. I also sort of critique the Gdpr, but now a backtrack on that. A little sue data subject rights that allow you to log in, review profiles. Um, uh, information that's kept a voucher I think are very useful because they can be done at the election of the user. Um, dashboards for example, that allow you to review histories, locations. I think those are, those are very good. And they, my general principle is that people should be protected regardless of what they choose. But if there are ways in which they can exercise autonomy on top of that baseline level of protection, then that that should be encouraged.

Speaker 1:          45:01          Um, there are also small little things that I liked that a lot of people actually critique is not going far enough, but I like because they're obscurity protective. Um, for example, uh, youtube released a tool that allowed you to blur people's faces. I believe. I love that tool. Um, and one of the reasons I love it is because it's obscurity protecting, right? It's one that if you happen to another person who was in the video, then maybe you would recognize them. But to the world, that's actually a pretty good design. Right? So in other words, people are largely protected. We often don't want to be protected from everybody just from certain sorts of risks, right? So, um, uh, people, uh, having the right to be forgotten, it's one of these things where people often want their name. This is associated with being found for certain results, right? They don't need the information gone. They just don't want the HR person who is reviewing their application for a job. They want to find it. Right? And so there are certain sorts of things they can protect obscurity and design like that. And I think the youtube a Facebook video is a great example of that.

Speaker 3:          46:06          So how do you consider a metadata like derived data for by data, right? How has that kind of plan to this and other people? Other people's data of me. So like there was the Facebook thing. I think we're right. Other people had data on me and I won't be able to see their data of me, right? So for instance, if they had a phone number of me, but I never authorized Facebook to have my phone number, they had it Facebook and I, my phone number. And I don't know that Facebook has my phone number.

Speaker 1:          46:32          The question is sort of what is the role of Metadata and all of this? And this is what I think really illustrates the limits of the control conception of privacy because so much is actually out of our control. A really good example is the genetic information, right? So, so, um, my sister, uh, Eh got one of the, the popular sort of tests and, and gave away or genetic information and I was really a little uneasy about it cause I was like, that's it. That's my information too. And I've got no control over that at all. Um, which is why I argued that we should have less reliance on control because control ultimately sort of becomes unraveled at this stage, right? It's inability to control it. This is where argued for the importance of trust. Um, this is where relationships of trust matter, um, now it's, there are costs to that.

Speaker 1:          47:22          So if we, if we think of privacy in terms of trust, then inherently we've just made this conceptualization relational, right? So, um, so then, uh, there are advantages to that in that now I can ask you if you have metadata on me, right? Or for platform has metadata on multiple people to act in a certain way. A loyal, for example, to not leverage that information against me. Um, I'm, I don't think that just because information is disclosed, your privacy isn't necessarily gone. I think that we disclosed relationships within information, within relationships with trust all the time. But what it does mean is that sometimes there are our parties that have no relationship, that have that information, that can then violate your privacy. So date of brokers who would be an obvious example where, um, they don't have a direct relationship with the data subjects, uh, in ways that, that platforms that collect information directly do, but, but they get information from various streams. Um, and so my general sense is that we should have a baseline set of rules that even apply to data brokers, but that a lot of our issues with respect to and metadata and a lot of these sort of unraveling problems can be solved through thinking of privacy in terms of trust. So the just the fact that you have it right or that someone else has it isn't the end of the story. Rather there are now things that you shouldn't be able to do with that information.

Speaker 4:          48:45          We have a question from the dory from cane. I found that I quickly learned to click on those. I'm fine with losing customers buttons because the popup is blocking the task. I was trying to complete. What are some tips for making sure people know how to go back and change their mind when it's no longer time critical?

Speaker 1:          49:03          Again, I mean not to make everything in illustration of the limits of the control conceptualization of privacy. But um, often when we think of consent, consent is, is implemented in sort of binary terms. Like I agreed to this. And then once that has gotten, then it sort of in perpetuity exist and you don't have to re ask for it again, even though people often change their minds or circumstances change. Um, this is where I do think that there is some responsibility on the part of users to take advantage of the tools that are given to them, if those tools are given to them in an easy to use way. Um, I encourage everyone to at least take a few minutes as part of your, maybe not daily but maybe weekly or monthly routine to do a little digital hygiene. Um, you know, search your own name on various search engines to see what's coming up.

Speaker 1:          49:53          Um, look at the, if you have accounts, um, and sometimes you have accounts even for only defensive purposes, right? So I sometimes people I get resistant to it, I say, well, I don't want to sign up for an account for x, y, z because then they'll have even more information out of me, which I'm sympathetic to. But, um, if it allows you to then sort of log in and regularly delete information or cleanse information, then that might be a positive. And then, um, uh, take advantage of, of I think popular ad-ons. Um, I use ad blockers, uh, which I know is controversial, but I think that's, um, that that's another possible tool. And then finally, and this is just the thing that I throw out in generally is if two factor authentication has ever offered anywhere, use it, um, and, and, uh, use it, uh, sort of early and often is, is my example. But, but I do think that that vanity searches, you know, learning how to, to work within dashboards, um, is at least a key part. It can't be everything. Right. My, my, my argument is that often users are being asked to do too much, but, but there is some sort of expectation that I think we can have a views. There's two at least, sort of try to, to exercise some, some diligence with the tool set given

Speaker 4:          51:07          what are your thoughts on premium options? Um, so we're starting to see a lot of companies moving towards that. I know we have a youtube premium option, etc. Um, as a, as a method for coming back to your concept of control, right. Do you see that as valid?

Speaker 1:          51:22          I'm of two minds for the premium options. I mean on one hand, um, I liked the premium options because the business model seems a little clearer, right? So one of the things that I've always been drawn to is when you can, when you win, everybody's incentives are sort of out in the open and we know sort of what we're dealing with it, it becomes a lot easier and easier to trust. Um, I think for the premium models to work, um, there has to be not just, um, some sort of soft promises, but I, but I think really robust promises. I would love the idea of, of platforms being able to opt in to sort of the gold standard of privacy, which is the, the true level of trustworthiness where, where they, they, they say we will be loyal, we will be discreet, we will be honest to the sort of highest standard.

Speaker 1:          52:11          Um, and then, and then that can drive people to sign up for companies. Right? It can be a competitive advantage. And I've, I've always longed for privacy to be a competitive advantage. And I think that we're seeing a little traction on that model. But, but then the other side of me worries a little about premium models because the other thing that I worry about is that, um, privacy becomes something only that people that have affordances can have a, and I really worry about the sort of, uh, equities where we say, if you pay that, if you pay the the gold standard option, you get privacy for everyone that can't afford it. They know, they, you know, they don't have that. Um, and so, so for that, for that reason, I'm, I'm still actually struggling with, with a lot of the way that, that cashes out in, in, in terms of policy, um, because it, I do worry about that.

Speaker 4:          53:06          And we have one more question on the dory from Keith. Uh, what are we, what are we all missing about dragon fly? Um, and the context is a wired story of Sundar at the wired summit. So, so

Speaker 1:          53:20          dragon fly, as I understand it, is the initiative for Google to be, to go in to, to enter China's market. Right. And I understand, I mean, if I'm understanding the stories correctly and I haven't read a lot about it, so I actually can't provide a, I think a nuanced discussion, but I have thought a lot about the values that get reflected in technology. And so to that extent, I think it's worth having an explicit conversation about, um, what, what the affordances of dragon fly would be. Um, if it facilitates it because it doesn't exist in a vacuum. This is a browser that will get deployed within a system that is actively leveraging, um, things like social credit scores and the social credit score, actually I will, I will say scares me a lot. Um, the idea that we would be ranked based on any number of different factors and then have benefits sort of denied to us based upon those factors.

Speaker 1:          54:29          So we wouldn't be able to board a plane, um, that are, that we would have sort of demerits taken away from us because we used up too many rolls of toilet paper. Um, the fact that we would be sort of data flying, everything does, does this contribute to that effort because that's a value that's implicated? Does it, will it help exacerbate the spread of facial recognition technology, which is another thing that I've worried about a lot. Um, because in order to gain the benefits of facial recognition technology of which there are admittedly are many, we can find the bad guy, we can find missing children, we can, um, uh, help those, um, that, uh, uh, don't have the ability to sense, uh, in, in ways of others and a bit in order to get a lot of the, the real uses of facial recognition technology, not just the sort of minor conveniences of unlocking your phone with your face rather than your fingerprint, which is fine, but I view as just a, uh, a very incremental benefits.

Speaker 1:          55:29          Um, but in order to get the real benefits, we will have to give up a lot, almost everything, right? We have to have cameras everywhere. We have to have databases that are shared. And I have facial recognition technology depends upon the existence of a name, face database that's they can recognize people. Um, and I think we have to think about that, that value as well. And will this, will this aid that in the aggregate and is it worth then, um, uh, defining the values of the company based on that, right? Because, because your values are reflected not just in what you do but in, but in what you build. Um, and I think that it's, it's, it merits a serious conversation about the direction of a company and the ethos of the company, um, with the, with that creation. But, but that being said, I, I'm, I'd be scared to comment anymore because I actually don't know the specifics of the, of the project. So,

Speaker 3:          56:25          uh, so terms and conditions seems like a necessary evil. But clearly the way that it's done today is pretty terrible because nobody reads dumb. And now people are, I guess, legally bound. I'm not even sure if they're legally blind in because nobody understands that nobody reads them is can you agree to something you don't read? But besides the point, and there's no expectation to meet them either. Right. Which is another issue. But, uh, how do you think that could be done better?

Speaker 1:          56:51          Craig, thank you very much. I love that question. So I just wrote an essay a little while back called user agreements are betraying you, uh, with the idea that, that it, it's, it's this abstraction, nuance tension that's unreal. Unresolvable right. We could either have a, someone proposed as some point, a 100 word user agreement, which says that's, that's, that's, that's not even an advertising slogan almost. Right? I mean it's, it's, it's so sort of abstract. This did not really tell you anything. Um, I think that user agreements, um, could be improved upon, uh, if they are written in a way that they are written for basically the transparency purposes because some people do read them. Actually, there's a really good article, um, uh, by, uh, Mike [inaudible], uh, called in defense of the long privacy statements. And he says that people, some people actually do read privacy policies, regulators do, advocates do, they serve in, Huh?

Speaker 1:          57:49          Attorneys, right? Attorneys do, they serve an important hygienic function for companies, right? So they let you, they let you know, like internally like, okay, here's where all our information is. We've got stock of it. Like it's a, it's an incredibly good force functioning tool. It's just not for users. Right? And so my, um, my preference would be to have a, a regulatory regime. And this is a little controversial where there are certain things that you cannot trade away in our user agreement. So in other words, there's a baseline set of protections that cannot be compromised through terms and conditions. We have this in other areas of the law. Um, and then that way users can agree to them without worrying about them. Right? Because I always sort of have this, this like small sense of dread when I Click, I agree. Like, I didn't read any of this, but I don't know within, in there, but I agreed to it anyway.

Speaker 1:          58:39          And they actually are enforceable largely, right. So, so the, the law, I teach contracts sometimes too, and the law of contracts is that if you click, I agreed in courts are going to say you agreed. Um, and I would like to have a talk about things that are unwavered will so that it basically, it didn't matter what was in the terms of agreements, right? Because they are important risk mitigation tools for companies and they could have nice, um, transparency and accountability aspects to them. We just have to stop pretending that they're for users because the things that shape user's expectations or the design, it's the website. It's the, it's the padlock icon. It's the button. Thank you so much. I really appreciate it. Everybody.