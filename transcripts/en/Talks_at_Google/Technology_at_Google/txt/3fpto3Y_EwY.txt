Speaker 1:          00:06          On behalf of Google and the many Googlers here and those tuning in around the world. Welcome. It's great to have you here. We're also fortunate to be here in conversation with Clement Wolf, Clement leads, Google's global public policy strategy on a number of search and economic issues, notably including the economic impacts of machine learning. And with that I'll let you guys take it away. Jerry Kaplan, everyone.

Speaker 2:          00:33          Well, again, thank you so much for being here with us today. I don't think you could be in front of a crowd that cares more about the impacts of Ai and where it's going. Uh, I mean this is not your first Rodeo. You've been here before, so I know you know how this goes. Um, I think we have a, an amazing opportunity to chat with you for an hour and delve into some of the things you discussed your in, in your book as a start. It would be great to hear how that specific book came to be. What drove you to think about it? Why write about Ai Right now?

Speaker 3:          01:01          Well, the, um, the answer is not that interesting. Um, uh, Oxford University press has a series called what everyone needs to know. Just so you know, it's, it's such a pompous title. I did not make it up myself. And so they, they do it on every subject. You know, Islam, the Federal Reserve, uh, global warming, they have, uh, it's a book that's written. These books are short. They're easy to read, they're not dumb down, but a clear and simple explanation of a particular topic. So you know enough to go to a party and be a know it all. And so that's really what this about. So they asked me if I'd write this book and I thought that was a great opportunity. But after that, I realized that there's really a niche for this book that is unfilled. There really isn't a book that's a foreign intelligence, uh, literate audience, um, about artificial intelligence that really explains the whole field, what it means, how it works, et Cetera.

Speaker 3:          02:00          We're lucky for the people on camera. I can't see this, but Peter Norvig's here, luminary in the field and uh, you know, he, he's a real superstar and he's written the book that I learned from, uh, uh, a modern approach to artificial intelligence. If I had more time, I would've made it shorter. But yeah, I cut out the last thousand pages of my book to the, uh, the point is if you read that book, it's really a, it's almost a cookbook, a technical book covering all the different techniques and all that, but it doesn't really explain it in a way that, uh, for a person who is not an engineer or just not trained as a, uh, uh, in the, in that, that those kinds of disciplines. And so there isn't a book that I'm aware of. They really takes this and explains it in down to Earth. Simple language, but without, without, as I say, dumbing it down, it's, we hear it gets into what this is, is really all about. So that's why I wrote it. That's what it's for. And, uh, you're welcome to read it. It's simple and easy. And then you can go to any cocktail party and be the life of the party yet.

Speaker 2:          03:11          That's a good promise. And one of the things you started with a and which I think was what the, he is very interesting is the sort of constant ebb and flow of the, uh, history of Ai, uh, with, uh, a trend for ex ante over promising. And I think one of the examples you mentioned is a, this quotes, um, my, my colleague Mike McCarthy, I think in 1956, a that's a year from that first conference on Ai, uh, significant progress would have been made towards a actual machines emulating human thinking. Uh, so over promising quite constantly, but then again, um, at fun to expost trivialization, uh, with the idea that once it's sold, it's not AI anymore. Uh, and so a lot of hype cycles. And I guess the question for us here today is that you think what we're seeing here, uh, over the past few years, is one more of these hype cycle that's going to come down eventually or are we now in something that's really new or something that we're different? Is happening. Well, both of those, those

Speaker 3:          04:17          things are true. Um, we are definitely in a hype cycle. I've lived through them. This is exhausted. What they're like, you'll, you'll, you'll know it when, uh, when the bubble bursts, but because that's not, I can tell you that there isn't very valuable technology and a lot of incredible, uh, effects that this is going to have on society. And those are not really being talked about in the right way. Because the problem with AI is it's sort of sexy and exciting and it's full of Gee whiz. And we need as a group of scientists and engineers, which is, I assume mostly what we have here in the room to get the Gee whiz out of this field because it's distracting us from really understanding the societal impact that this kind of technology is going to happen have. So it's sucking the oxygen out of the conversation when we're talking about the singularity and machines rising up and taking over.

Speaker 3:          05:10          And of course, the main question I get when I speak is, you know, did you see Westworld? Uh, you know, and uh, we were joking before this, uh, which is true that every presentation has to have a picture of the terminator in it. So usually I just put up that picture, right, to start to say, here's, here's the picture of the chairman and around, let me get on with the talk. Because artificial intelligence, there are different ways to look at it in different ways to frame the problem. And I've come to the sad conclusion that the current framing, which is that we're building machines which are growing ever more intelligent and that these are rising up some kind of a linear, uh, objective scale toward a human intelligence. And Oh my God, what are we going to do when they start to exceed us? And then they are going to have do bad things to us potentially, or they'll get of our control.

Speaker 3:          05:58          This is not a helpful framing of the problem. And that's because there is no, they, they're a much better way to think about all of these things. And the problems and the benefits is to think about artificial intelligence is a natural continuation of the longstanding historical process of automating tasks that require human labor, effort and attention. And if you think of it that way, you can eliminate what's going to happen over the next couple of decades by going back and looking at the long history of automation over the past two or 300 years. That wasn't your question, but that's what I wanted to say. That was a great answer. Great answer to know the question question.

Speaker 2:          06:40          That's fine. And when you go into a different question then, okay. Um, you, you just mentioned how the idea off the scale with the evolution of the eye towards human intelligence maybe by us floods, uh, however, in the book, um, you certainly discuss the idea that even the idea of human intelligence maybe flowing in some way. Absolutely. Uh, and I think it's an interesting idea for this audience to discuss. Uh, could, would you mind elaborating a bit?

Speaker 3:          07:05          Sure. There is a mythology which has reinforced this whole hype cycle and AI, that intelligence is some kind of uh, uh, objective, linear, measurable quantity. And this person is Iq is 125 and this one's only 120. And so there's this person's smarter than that person. But let me just talk about Iq because that, that's a great illustration of why this is a myth. If you talk to the people who developed Iq, the psychologists and the deal with this, it's an intelligent quotient intelligence quotient they call it. And uh, it's really not about intelligence at all. What they'll tell you, they'll tell you it's a measure of developmental competence, which is why you mostly apply to young children. What they mean is do you have a certain set of skills and at what age did you gain those skills? Where you capable of doing that? And so after you were about 15 or 16 years old, Iq is functionally meaningless on now cause it's, it's a quotion.

Speaker 3:          08:05          You guys know what that means. You scored a bunch of tests and you divided by your age and that's your Iq. Well, I must be really dumb because I'm really old. And that's the development. Uh, so the hope concept of intelligence as it's understood by the general public is mistaken. Uh, it's really, uh, what we think of as intelligence. It's not an objective measurable quantity. It's more like our concept of say, beauty. You know, is this person more beautiful than that person? Now that's a question one can ask and you couldn't even have a, a meaningful answer. Yes, in general, this person's regarded as more beautiful net person, but we don't talk about that as a linear scale where a, if you've noticed, if you watch movies, which there are robots, they'd been becoming better and better looking. You know what they say? They start out, you know, rob, he's a robot and they've come along very far, but we don't have conferences where we're worrying about robots becoming so beautiful that we no longer want to meet with human beings because we want to meet with the robots.

Speaker 3:          09:10          Well, this is a corollary of the mistake. I'm staking path that were led down with this notion. The human intelligence is a, a sort of a flat linear scale. It's really a series of complicated capabilities and competencies that we have. I don't think of a spider is really smart because it knows how to spin a web. And so I don't think of a person is really smart because they know how to program in Java. It's uh, it's really just a set of competencies that one develops. So now as we apply that to machines, think about Iq. If I can continue for a minute, think about Iq is applied to a machine. How do you divide by its age? What happens if you given intelligence test to a machine? Well, it can perform calculations by way of example, one of the key elements of the Iq test, I mean millions of times faster than human being.

Speaker 3:          10:01          Oh my God. Is that mean it's going to take over the earth? No, of course not. And you, how do you divide by machines? Age? Oh, he built that one 15 minutes ago. So this is Iq must be, you know, 100,000, 22. Um, clearly we shouldn't be thinking about framing the problem in this particular way. We're using these machines precisely because they perform jobs or tasks, not jobs. That's another part of the conversation better than humans. Do. They do them faster or they do them and less expensively or they do them with a more accurately or whatever it might be. We use these machines because they exceed human capabilities. So with the general public worries about, Oh my God, what are we going to do when machines get smarter than us? They already are. And that's the whole point. So I'll stop there. Oh, don't stop. That's continuing. I go on for an hour on this list of points and it's like, we'll have you here and she knows all the yen smarter than us in some ways. Uh, one other upon your ways in which I think is, um, is, uh, a relevant level comparison is the idea of a machine freewill and machine consciousness. You see that as related in any way to intelligence. Do you see that as a, as a possibility?

Speaker 3:          11:17          Um, the, the real answer is of course, I don't know, but it does raise questions when we build machines with these capabilities. So whether they might ultimately we might ultimately regard them as having freewill or being conscious. And, uh, there's a lot of interesting work on this in the philosophical literature. I happen to be a follower of, uh, what's his name? Sam Harris. Thank you Sam Harris. Uh, terrible with names. The first thing that goes when you get to my age of the names, it's a pfeifle Q and a name street. They just strip them out. You know, it's, you know, it's a hash function just goes down. Uh, I can make jokes like that in front of this audience. Um, well basically with Sam Harris pointed out in terms of free will, and I happen to agree with him after a lot of thought, is that it's fundamentally our common sense notion of what it means to have free will is fundamentally in conflict with our science current scientific framework and worldview.

Speaker 3:          12:17          The two just don't fit. You have to introduce some kind of magic to the idea that independent of the world around you and independent of all the history of every event that took place up to a point in time you decided to turn left instead of right. And you could have done something differently that simply inconsistent with our scientific worldview. And I'm not just, I'm not suggesting that the world is entirely deterministic. I'm just saying that what does it mean for this mind to be somehow outside of that or separate from the physical world in such a way that it gets to stop and think and make a decision that is not in some sense determined by everything that has come before. Uh, so I don't think, I think there's a good argument that freewill is actually an illusion. Either that or we do not yet have the scientific framework or, or ideas that we can use to explain it in a reasonable way and as uncomfortable as that might be, I think that's the, uh, the conclusion that one has to reach.

Speaker 3:          13:21          And the same thing's true of consciousness. But underlying both of these questions, can a machine ever have freewill? Can a machine be conscious? Everybody here has seen, uh, how many people have seen Westworld? Okay, so I can, there's dozens of films and that basically around this thing. When does the machine come alive? When it does it become independent in its thinking? The really underlying issue in that is at what point, if ever, do we owe the courtesy of our empathy to human created or mechanical machines and devices? That's really the underlying question because immediately after you see another TV show like humans, how many of you have seen humans, British TV, one person, okay, well I'm not here to promote the humans, but the real question always becomes, are these simply tools which we can abuse and treat the way we treat any other tool?

Speaker 3:          14:25          I don't think of myself as abusing a hammer when I use it. Actually. I guess I could, I often break hammers and things was this form of abuse. Um, but at what point do they in some sense deserve, uh, having some semblance of rights in and of themselves. And, uh, that's the reason we worry about this issue. Whether a machine can have free will or not, I'm machine can certainly act as though it has free will. I know I've got, I've got a book and, and if that thing doesn't have freewill, nothing does. So, uh, but I don't feel like I owe it a, an opportunity go off and just have this little spinning wheel at anytime it wants to because it needs a break. I don't have that, that same sense of empathy that I do for, uh, not just for humans but for many other living things.

Speaker 2:          15:13          And that's a really good segue into the second part, I guess, of that conversation, which is we now have brushed up on the fact that, well, if maybe in a hive cycle, but things are changing fast either way. Uh, and we don't know if there's a huge difference. So what the difference is between the way humans are intelligence and the way meshing their intentions. We know that it's not the same thing. It's not even on the same scale. We don't know how to relate them. We know that consciousness. And that's a free maybe flawed notions. So what that makes a very advanced machine that different from a human and in a, in a, in a world where they keep getting well more prevalent, more pervasive and more sophisticated. What does that mean in terms of legal and economic impacts? And I think the first stage of that is a legal impact of course sits if we can say if a machine is really conscious, if we can't see if a machine is really intelligent, if we can't say if it's really, uh, if, if it has free will, then what can we do about questions like ownership, uh, which you raise questions like, uh, accountability.

Speaker 2:          16:15          Uh, how does that play out interview,

Speaker 3:          16:18          um, while even though I have been describing the, uh, the Ai World as merely an extension of a previous, uh, uh, types of technology, I do think it raises a number of new and very challenging questions for society. And we have different ways of dealing with it. I don't think we have any obligation to, uh, to machines. I don't my car, uh, you know, I may take care of it, but not because I think it has some rights. I think the same thing will be true of any machine that we're likely any of us are like interact with in our, in our lifetime. However, from a legal standpoint, there are certain legal fictions which are useful in sorting out the primary question of who's responsible for what. So as you may know, corporations have a concept of personhood that is applied to corporations. It doesn't mean they're people.

Speaker 3:          17:14          It means that there are certain collections of rights and responsibilities that go hand in hand. And that's what we call personhood, even if it's limited. And, uh, there's also interestingly enough, not all people have the same rights and responsibilities. Uh, you guys don't need a license to do machine learning, but there are in the law, for example, you need a license to practice law and you have certain responsibilities and then you're given certain rights to be able to work in the, in that profession. So, uh, this notion of a, of rights and responsibilities is a very powerful one. Very useful. In an illegal context. And I don't think it's at all unreasonable to the SPEC that courts will find that certain classes of devices, uh, could be given legal personhood in the same, uh, limited sense in, in the future. So that's something that we need to, that will have to be worked out.

Speaker 3:          18:09          Um, but here's the good news. Everybody's been worried about like, oh my God, what happens when, uh, self driving cars getting a crash? Who's responsible? Well, assigning responsibility is a very important issue, but it's not such a great philosophical mystery. There is a tremendous amount of history and work in the legal profession for sorting out who's responsible for various kinds of accidents from the manufacturer to the user, to the, uh, the person who sold you a particular device. Have they explained properly and told you about what the dangers are, et Cetera, et cetera. And the short answer to this question is, we don't need to worry about it because the lawyers are going to handle this. They'll sort it out and it's not going to be a big issue. Uh, in particular, if I can come back to self driving car, we don't need to worry about it because the insurance companies are going to drive the adoption of this technology for a variety of economic reasons.

Speaker 3:          19:05          And you won't care. You got into an accident, the insurance company will handle it. And whether they go back and say it was a flaw in the design or a flaw in something else, or, uh, you are drunk and you instructed your car to go run somebody over, you know, that's then your re you're responsible. So to me this is a, uh, uh, as a practical matter, it's a nonissue. We have experts who will take care of these things for us in reasonable ways through the legal system, but it will require a lot of precedent and, and future work.

Speaker 2:          19:36          Absolutely. And I guess as I'm one point, I will respond to that as these things will be handled ultimately, but they can be handled in more or less effective and more way, more or less beneficial ways for society and for individuals. And that's underneath one key consideration, especially when it comes to the, uh, the question of labor. Uh, and the question of the, uh, uh, impacts of automation on employments. Uh, there has been a lot of discussion about this of course, and I don't of considerations that there may not be as mean as many jobs 20 years from now as they are to date. Uh, what do you stand on that? Okay,

Speaker 3:          20:14          this is a broad and complex subject, but let me try to quickly summarize this in the spirit of my short answers in, in, in my book, if you view artificial intelligence not as the emergence of some new form of life that is heading towards singularity and we're all going to suddenly be raised up and reincarnated in mechanical form and instead think about it as a continuation of the longstanding process of automation, then you can look at a two effects the automation of had in the past. And what we're seeing if certainly continuation of impossibly in the first is automation changes the nature of work. So it's true when you read about my God, 50% of the jobs are, will be gone in 30 or 40 years. That's the first half of the sentence. The other half, that sentence that you never hear is that's normal it, but people did 20 or 30 years ago is not what they do today.

Speaker 3:          21:08          And there'll be a whole bunch of new jobs and an expansion of other kinds of jobs for very important economic reasons, uh, that will, uh, there'll be plenty of work. So if these two statements are nodding, not is inconsistent as they sound, 50% of today's jobs might go away and pick your number of decades and uh, there'll be still be plenty of work and people will still be employed. You know, if you just go back and look at everything from, uh, I'm going to give you an idea how old I am here. I remember barely, but I remember when I was a kid, you pick up the telephone, there was no dial and a person would get on the line, say, uh, what do you want? What number? Po Number Police, I think they used to say, and you would tell him Bigelow, eight to 4,200. I mean, this actually went on.

Speaker 3:          21:59          So there were people, I think there were a million people I could be mistaken on that employed as telephone operators and you would tell them and they would plug things. Right now, that switching function, as you're well aware, has been taken over by the, it's called Ess, the electronic switching systems and all those people are out of work. Oh my God. Well, the problem isn't that they're out of work. It's the nature of the work changed. And because of that, that these improvements make us wealthier as, or just society that creates a lot of demand, not just for new kinds of jobs, but for all kinds of jobs. Let me give you an example that I, I just noticed these as I go about my, my humdrum daily life. Um, right now getting a massage from a is a bit of a luxury. I mean, all of you of course, can afford this. Uh, in fact, I think they're free here. Google. That's one of the reasons I'd like to work here. I'll just do that part. I want, I want the job of testing the new messsage. That's my, yeah, it's going to be my job. It is.

Speaker 3:          23:05          Sorry. It doesn't look good on video. I've learned when you laugh, you know about that, these things. But that is funny. Okay. That totally threw me off. Okay. Now what happens when you no longer need to buy your own car or request of transportation drops by 75%. You have more money. Well, what are you going to do with that? A lot of people are going to step up and want to do things like get massages or go to the spa for a day, and that's gonna generate tremendous demand for that profession. I've been surprised locally. That's an incredibly well paid job. You know, talking about manual labor, you know, it's a, it's a very well paid job and there's going to be tremendous demand in the future for a massage therapists. Well, that's a side effect. We often think we're destroying a job. Uh, what, what's going to happen?

Speaker 3:          23:57          Well, the answer is that money goes to expanding and changing the complexion of the workforce. So that existing jobs of certain kinds become a much more prevalent and we need more and more people to do them, personal shoppers or people to do flower arranging or whatever it might be. Uh, so, uh, the way you, it's the labor markets work as they're very resilient and very, uh, dynamic. And so while it's true that jobs are going to go away, there will be new jobs and more importantly, there will be more of certain kinds of existing jobs. And I'm convinced that this pattern is going to play out in the future. As I thought about it. That said, we're putting people out of work. The other thing about automation to things that they never want to say when you're, uh, an IBM makes its presentations and other tech companies, we're not putting people out of work.

Speaker 3:          24:56          Uh, of course they are. That's the whole point of automation is to put people out of work. So you're putting people out of work and the question is, what do you do with those people? Do we have the, uh, social, uh, Po and policy frameworks in order to ensure that we're not, the costs of the automation is not falling disproportionately on certain groups. People, what are we going to do with the millions of drivers? We're going to be out of jobs in the next 10 to 20 years. We can just cut them loose. Maybe that's would be the current administration's point of view on something like this. I don't know. But, um, you know, we need to set up mechanisms and it, it's incumbent on us as a thinking human beings, empathetic human beings to, to figure out better and better ways to reincorporate people into the workforce in, in improper ways.

Speaker 3:          25:48          In a sense. This is what went wrong with globalization. Uh, the benefits accrued to a certain small group of people and the costs, I'm sorry, the benefits accrued broadly cross much of society and some people benefited tremendously, but there was a small group of people maybe like the steel industry or the coal industry there were devastated and we didn't pay attention to that and now we're suffering that and the backlash from that. The same thing is going to be true with the rollout of a lot of the work that's going on right here at Google. You guys are going to be fine. It's not going to be a problem, but you're going to be replacing people who may not have the opportunities to retrain or to become, continue to be productive members of society. And if we don't address this on a policy level, we're going to see the same kind of backlash as an [inaudible]. And that's one of the thing we spend enough time thinking about. So, uh, are they, let's we can do to help people of skill to,

Speaker 2:          26:43          uh, think of new types of social safety nets. And there's, of course, I don't think we've seen a compelling and a full proof on suggest yets. Uh, I think a lot of people around the world are thinking about that. It was interesting to me to see that. Um, I mean I'm a French of obviously that's my accent's uh, the um, the winner of the left wing primary for the French election urban while I'm on a one on the program that's based on Ubi, uh, in response to the prospects of technological change. What's interesting to me is that it comes from an optimistic perspective. His view is that it's a good thing if people work less working less generally. Okay. Uh, on the issue is how do we share the benefits of that? And so he believes ubi maybe a solution for that. That's a whole other conversation that I'd love to have with you, but it's already a 36 and I want to leave time for the audience. I guess the last question I would selfishly ask before opening it up is we've talked a lot about some of the states, some of the considerations you had about what's Ai will be an impact it will have. What's one thing that really gets you excited that you're really looking forward to it you think is going to happen? A breakthrough is coming soon.

Speaker 3:          27:54          Well, the most obvious things changes to our transportation and shipping infrastructures. Talk about preaching to the choir. How many of you, how many of you in the room are working on self driving technologies? We have a alphabet has split now. Okay. They're in, they're all working on it. And you guys, you guys all right. Well I mean w w when I was a little bit younger, uh, the, uh, they used to say, well, what are we gonna do when machines can program themselves? And in a way you hear that again, I'm sorry, I'm a little bit off of your, your topic. Uh, in the sense that we, that I programmed machines when I was a a working, working day engineer. Um, we have done that, you know, the languages are higher level, uh, the machines programs themselves, uh, today, uh, we don't lay out chips by hand anymore.

Speaker 3:          28:52          Obviously we have great, great tools to do that. So the nature of that work continues to change. And I somehow got completely off your, uh, that's the end point, which was, which was one thing that really gets you, it gets me excited. You want to see happen and you will see on the horizon or the, I don't think the impact of this revolution where we made a sensory perception with the ability of, with machines that can perform functions. There's almost existing technology on that side. The stuff that you guys work on a sensory perception. So we made those two together. You get a very powerful combination. Only one example is the self driving car. And when you do that, a whole variety of tasks that previously where machines could previously were confined to factory floors because they did, couldn't sense their environment. Now these machines are going to come out of those environments and be in and around people and that's going to have dramatic impact, not just on labor but on improving our lives, making us wealthier and allowing us to do things in, in, in greater safety or be better informed, uh, at, at a far lower cost than, than it ever could be done before.

Speaker 3:          30:03          And I don't think people realize how this revolution is going to, Eh, the, the enormous impact that this particular revolution it's going to have because it's, it's going to be quite dramatic. The problem is, I want to talk about this for you guys were working on the technologies, but there are tremendous barriers to the social acceptance of the kinds of systems and devices that were, uh, creating and building machines that, uh, abide by our normal social conventions, human social conventions when they're operating around. And with us is an area that is just in its infancy and we don't really have a good theory about this, you know, when is it okay, we'll be okay for a robot to go wait in line for you. Uh, is that acceptable or not? What does it mean? How are you going to feel the first time your self driving car, which could park itself, somebody else's self driving car can zoom into a spot and take a spot and there's nobody in it and you need to park your car. Is that okay or is that not okay? I go on and on with lots and lots of examples. But where we haven't paid adequate attention to is what kinds of theories of social behavior and normal conventional norms of society. How do we take those and codified them and put them into the, uh, behavior? If I could anthropomorphize a little bit on these machines in a way that people will find acceptable.

Speaker 4:          31:33          Okay.

Speaker 2:          31:36          Yeah. So in this age of, uh, polarities and people are able to see eye to eye, is it actually, uh, in, in a full, put the AI beginning to find a deep similarities between humans rather than the differences we tend to focus on. For example, a Google has found that in spite of language is being so different. There's a deep, a commonality at the heart of the language almost at the [inaudible] level. So if you think of mother and I think of Martha and somebody else thinks of Ma is same bio rhythm expressed in different ways as it's beginning to expose that through computation. So, um, and so it's what the expressions are different, but there's deep commonality between humans, right? So is there actually a hope that AI can break us together by finding more of the similarities versus, you know, it's getting us into being more reclusive?

Speaker 3:          32:32          Well, you know, there's a long history in linguistics of study on this and those who are I Chomsky's calm skins, Chompsky heights, uh, we'll, we'll recognize that. Uh, his, his whole theory is, is that there's a fundamental basis for, uh, for language whether or not that's true. I, you know, as a matter of some debate. But I think that to the extent that we've reflect human behaviors in this, uh, I'll call it a mechanical or electronic mirror of these machines that were building it can obviously help us to inform us about what our own capabilities are and what the commonalities are, uh, among different cultures in different, different people. Um, now that sort of the positive view on this, the other is that when we hold up this giant technology mirror that we're doing to society, for example, in, in social media, uh, it's going to adopt the same kinds of biases and, uh, negative aspects of a human conflict and behavior that, uh, that we see in ourselves.

Speaker 3:          33:39          So let's be very careful when we look into this mirror about what we're going to see. It may be about what's common and expanding our view of, uh, includes a wider and more inclusive, or it may become a tool for us to be a distinguished ourselves thinking about their people as other and not wanting to be concerned about, uh, like today the, the, uh, the role of immigrants versus a me first kind of attitude that, that we're, we're making a turn that's not necessarily good for all of the world. Uh, but, um, uh, I think that that, that's the way that I look at the, this problem. It's, it's informative. It will be helped to inform us about our own human nature. Machine nature will help to inform us about our human nature,

Speaker 5:          34:27          that there is a more or less equilibrium between demand and supply in the overall labor markets. And as automation changes, demands or reduces demand in one labor market, the assumptions that technology creates a new one, right? If you need less coal miners, maybe you need more solar panel installers. Well that assumption always hold true.

Speaker 3:          34:46          Well this isn't a law of nature, but you just have to look at it. Historically that has been the case. The concern that we're putting everybody out of work is hundreds of years old. And this question, this problem has been brought up repeatedly over time. Now that's not to diminish the tremendous personal cost that, uh, uh, cruise to people who are displaced by the new technology. But I just don't see any reason why this, this is not going to, uh, the same balance is just going to come back into it. We'll come back into balance. Uh, even if we just, even if we displace a large number of people, of course, the question is when and how long does that take and what are we going to do in order to minimize the social and economic impacts of that? You, the big picture is pretty simple. We're doing things that make us wealthier as a society, significantly wealthier.

Speaker 3:          35:43          Most people don't realize that the average household income in the United States has doubled reliably, uh, every 40 years for over 200 years. It's incredible how much wealthier we are that our parents were, even though you might not see it, uh, on a day to day basis, uh, and our grandparents and going all the way back, the average household income in the United States 200 years ago was $1,000 a year. It's about the same as it is in I think it was Gambia and a couple of, uh, uh, agrarian, uh, African countries. And yet we don't think back on, uh, uh, Ben Franklins time and think, my God, those people were dirt poor. Um, now what if that pattern is likely to continue? So we're going to have another hundred percent of wealth, uh, compared to today in 40 years. So we will double the amount of, well, the question is how do we, how do we distribute that, uh, properly among the workforce?

Speaker 3:          36:43          It may be, uh, you mentioned, uh, a universal basic income. You didn't use the term. So a lot of the people on the video probably will not know know what that is, but that's only one approach to this particular problem. I think it's self correcting, but we need to pay attention to it. We shouldn't be worrying about where the machines are going to come alive and take over the universe. We should be worried about whether or not the pace of automation is increasing or we're going through an another wave of increase in automation, primarily due to a machine learning today. And, uh, how is that going to affect different aspects, different portions of society and what do we do to mitigate the negative effects?

Speaker 2:          37:22          And if I just may I have one, a few, a few points about that actually. Um, one is, uh, to your coin, demand changes, uh, for employment. Employment itself itself changes. Uh, in 20 years from now what we think of as a job. Maybe there a difference, not just in terms of content but they have a structure. Maybe we'll have way more dig centered economy. Maybe you will have a much shorter work week of a rural because that's just how we think all work week as much shorter amount of grandparents. So it's just not the same thing. Uh, so that's one factor. It's important. Um, another one is the, you were mentioning the speed and the scale automation and how it impacts the economy. Uh, to your point about regulation being a key driver for that, even if the technology gets there early on, uh, if the rules and if society doesn't change at the same pace, there may be a buffer there. If we are underestimating now it may be a very bad thing for a lot of reasons because it stops innovation. It may be actually an okay thing from the perspective of jobs displacement in the sense that it gives more time for societies to adapt. So that's also permitted to have in mind.

Speaker 3:          38:24          I think the key issue is it's, it's a question of the speed of transformation. If you go back 200 years, as you may know, more than 90% of the u s population worked in agriculture. What it meant to work was to be on a farm and do farming work, that that's what jobs were only everything else was just 10% of that. Now, today, less than 2% and based on a lot of the work that's going on here, we're going to be able to make 75% of it, 2% pretty, pretty quickly. So, uh, basically all do people did, was grow in, in, and consume food and that represented I think 40% of the average person's budget. Today, it's way less than 10%, which is kind of kind of exciting, but we're not all out of a job. You know, we've got other things to do because our expectations continue to rise, uh, et cetera.

Speaker 3:          39:11          But, uh, I have, uh, maybe a radical statement to make here that I would not make elsewhere. This is not your problem. You guys should be out there generating the technology, which is going to make society wealthier. And as a slight side effect with mentioning you wealthier and the owners of Google stock wealthier. But that is not your problem to be worried about public policy. Actually it is clips job though. He, they let one guy kind of 50,000 and uh, he, he could do no, it's perfectly reasonable thing to be concerned with this, but the idea that we should not do this is silly. It's a little bit like saying now we can, uh, connect phone calls electronically. We'll we're gonna suppress that technology so that all of these telephone operators still have jobs. It's obviously the wrong approach to take. Do your jobs, generate that wealth, make the future, give us the opportunity to make the future better and let the people who worry about policy in a social cohesion figure out how best to address those side effects of security.

Speaker 6:          40:21          So, uh, so to follow up on that then, uh, one question, I always wonder from the perspective of anybody who's making a choice about what kinds of innovations to work on, an obvious question is do you, can you give any color around some aspects of automation just to change the jobs of people and they stay in place and become more powerful, like your example of the compiler. Other examples of automation though, do cause displacement. Do you have any thoughts about how those sorted out?

Speaker 3:          40:50          Um, well I would argue that they all cause displacement. Let me explain that. People worry about machines coming and taking our jobs. But machines don't take jobs, don't do jobs. They perform tasks. And if you look at what a particular individual does, usually there is some range of tasks that they are involved in. Now if you automate 50% of those, you make the argument correctly that you're making that person much more productive. You're freeing them up from routine work typically, uh, and give them time to focus on the things where they add the value the most. So that's great. We're making programmers more productive. What does that mean? We need fewer programs? Well, actually we can get a lot today, but in general, you know, that means we would need, if we were all programming in assembly language, this place would be the size of a, a major country in terms of what will be necessary to accomplish, uh, the, the, the things that you do.

Speaker 3:          41:42          So, uh, however, if your job consists of a single task, I lay bricks, I picked or drive a car, I pick up the brick, I put it here, I put the mortar on. And that's what I do. You can build a machine that does that. And if you do only tasks that are automateable, obviously you're going to be out of a job. So I think this is a false dichotomy that you hear a lot about. Are we making people more productive or are we putting them out of work? And it's somehow this bleeds into our design. I think that's not really the case. I think what is the case as we tackle tasks, you know, they're very important tasks with tremendous economic benefit. Like making a machine that plays go, that's a joke. It wasn't funny, but it was supposed to be a joke. Uh, you know, there's just, there's such a demand for this, a worldwide, you know, to save us from the drudgery of having to play, go ourselves. And I'm so glad that you guys have done such a fabulous job on, on that important societal issue. Um, so my point is that you're going to do these particular, if you automate tasks, that's what you guys do. Everybody here, if you think about what you're trying to do, it's always task oriented. It's not job oriented. And how the jobs fit into this kind of matrix, uh, is really the determining factor. Whether we are making people more productive or whether we are putting them out for,

Speaker 2:          43:03          yeah, there's an example actually that's pretty, uh, uh, often causes, which is that of ATM and bank tellers. I'm sure you guys have made before, which is that when Dan [inaudible] scheme up, uh, there was a notion that's welded, banked and [inaudible], uh, the one needed anyone to hand money at the counselor anymore and so it will be just less employment for these guys. And years later studies found that employment has actually remained constant. And that's because, uh, actually of course with Ata you needed less bank tellers in one bank, but then again, uh, you needed to Dennis still because you need the human face and they upscale that I'd be able to do more things. And because you saved money and you hyper activity, they just were more banks. What makes up things?

Speaker 3:          43:42          He was a general principle. I, I can put off you automation changes the nature of work. It's the same title, different job. What a, what a teller did 30 years ago is very different. I tell her is now a concierge to bank services before they used to sit there and Dole out money. So these things constantly change. My programming skills to describe them as obsolete would, would be too gracious and polite. You know, I mean, I remember the days before, uh, before, uh, object oriented pro, we had structured programming. Anybody who remembered that stuff. One guy, how old are you? 23. Okay. What structures? I love structured private. It was great advance. Um, you know, the whole, you look at the way database, I don't have time for this, but it's a fascinating thing to look at the history of how databases, uh, uh, were implemented. And what happened is we moved from networking hierarchical models to the relational model and how that changed the nature of, of databases, but it made it so much more efficient and easier to implement.

Speaker 3:          44:44          Everything is a database today, uh, used to be, if you wanted to store data in a computer, you had a higher group program is to figure out exactly what that particular application needed and program and structure at that particular way. Now we can do that in a much more generally. So the point is, even if we call them the same thing, the jobs change the job of a driver in the future. Maybe very much of a concierge in a, imagine a van that comes and picks you up. There may be somebody sitting there whose job it is to sell you a drink or to provide you with some other kind of service during the time that you are in this vehicle is it's picking up and dropping off other people. So there will be plenty of plenty of jobs. They'll just be a little bit different.

Speaker 3:          45:30          We had a question from the livestream. Why would insurance companies be motivated as a lobby for self driving cars? Industry is born of human mistakes. Isn't it reasonable to assume that one of the potential losers here would be insurance companies as accidents go from one per 100,000 miles driven to one per 1 million miles driven? Well, the, the advantages they work like all businesses on the spread, which is uh, you know, what is it costing you and what can you get for it? So the first thing that's going to happen, I'm making this up so I shouldn't, I shouldn't phrase like that. I expect one of the things that might happen is that when you, if you have a self driving car and I'm your insurance company, what I'm going to say in order to reduce my cost, and pat is if you get into an accident and uh, you're, the car was on automatic and it's, let's assume it can be driven manually or automatically, uh, which is controversial as you guys, you guys know whether that's a good idea or bad idea, it's a bad idea, but they'll say if, if it was on autopilot so to speak, then uh, uh, we'll waive your deductible.

Speaker 3:          46:40          You know, you don't have a deductible in the case of an accident. Now, uh, the reason for that is that the amount of money that there's going to save and the amount they're going to drop your insurance premium by is, is going to be less than the benefit to them. That's the spread. It's really, really that simple. So the insurance companies have a very strong incentive to, uh, to see a rollout of this kind of technology. It's true that it will transform their businesses as well, but people can get into all kinds of trouble. And I'm sure that we have many other things that, that uh, services that they can sell you in, in, in terms of, I'm not worried about the insurance. That's what I'm saying. I think there'll be, there'll be just fun.

Speaker 7:          47:18          So the current Ai Paradigm relies very heavily on training data and as we've seen in domains like bank lending and bro granting historic injustices and profiling can be perpetuated into the ais that are trained. We've done some work here on earth moving such biases from the networks. What do you think are the policy and legal issues that are going to get involved in inserting such value judgments into the utility functions that build these ais?

Speaker 3:          47:48          Well, here's an interesting way to look at this problem. I gave a talk to the mortgage bankers association, which of course is very concerned about exactly this issue. There are a lot of laws for very good social reasons that are in place to avoid red lining and discrimination based on race, et Cetera, et cetera. And they're subject to some very stringent, a statistical tests, uh, in the work that they do. Their problem is to the extent that human beings are involved in that decision making process. They need to train those people. And often those people do not perform in exactly the way that they want. So society has this problem today, but with respect to the people involved in that process, we are going to transfer that to the problem of these, these machine learning algorithms. Okay? We're going to say you can't discriminate on the basis of race, but as you know, it can find a correlated, a variable of some kind that has the same kind of effect.

Speaker 3:          48:46          You know, we're not banning Muslims were just stopping the immigration from certain countries. They just happened to be muscle. Okay? So there are ways to, uh, to, uh, that you wind up with the same effect of discriminatory and on satisfactory a social, uh, results. Uh, even though you've, you've, uh, all you've did, all I did was the, you know, program by my, a neural net, you know, to do this. So, um, it's very important that we put the kinds of hooks and controls entity systems that will allow us to ensure that they meet our social standards. I was talking about that in terms of like standing in line, but this is another example. Do they perform in ways that we find acceptable as a society that's going to be very, very important. Now if you think about this as automation as opposed to magic, you get a very different point of view about how to go about doing this.

Speaker 3:          49:39          This is a question of engineering standards and practices and I think we're, we're going to see, I hope we will see an emergence in the field of artificial intelligence, a, an approach to uh, the, uh, the development, the testing and the deployment of artificial intelligence systems that is, uh, similar to what like the I triple e does for many other areas or civil engineers, they have standards for building bridges. You know, it has to meet certain kinds of criteria or chip testing. There's was a standard now that you need to apply and make sure that you chip meets its specifications. While we don't have that science yet. And part of that is this idea that we're, so I'm sitting in the room here with wizards, with hats on, it's all magic. And so you couldn't possibly, uh, tell the machine what to do. It's going to make up its own mind or of course that's complete nonsense.

Speaker 3:          50:34          So I think what we need is a get the magic out of Ai, get the Gee whiz at it and start to talk about what kind of engineering practices and standards in techniques do we want to incorporate into these devices to ensure that they meet our societal values and that they abide by normal social conventions, which people find acceptable. Otherwise. My view isn't it, that the robot's ran a muck. It's that we built bad tools. You know, we don't, you know, automated lawnmowers that run children down or something. Uh, that's where that and that kind of a prompt comes in.

Speaker 8:          51:08          Uh, you mentioned earlier that there are multiple ways to, to deal with people whose jobs are getting displaced. One of them is a universal basic income. What are some of the others?

Speaker 3:          51:23          Well, the biggest problem that I see is that our education system isn't really designed to turn out people whose skills are needed by the marketplace. Uh, there, there are many roles of education, but one of the Mr train people to be productive members of society. And that means teaching them how to do certain kinds of jobs or, or, or tasks. And the problem we have is that the, the funder of our education, the lender of first resort, if you will, is the government and the government has no incentive. There's no incentive built into that structure to ensure that the things that are being taught are the things that people need to know. So by way of example, and this is the worst audience to use this example and Buddha and forks and other ones. I, we teach high school kids calculus. My view, that's mostly a waste of time.

Speaker 3:          52:13          They don't need calculus. I've never needed calculus. Now, many of you, how many people here use calculus? Okay, there you go. So this is why it's, this is where you need to do to learn calculus by, um, you know, the, the, the education that my children got at private schools here in the bay area is almost identical, surprisingly not only in content but in the way it was delivered, which is shocking to the education I had 40 or 50 years ago that hasn't been responsive to the needs of the marketplace. Now, you guys know people like Sebastian Thrun and interlinked, they're all, you know, they're doing things that close this loop by making the economics, uh, tied to the teaching, the stuff that people need to know or you just put it that way. And that the trick to making this work is to make sure that the investment that's being made has a likely payback.

Speaker 3:          53:05          And the way you do that is by introducing the discipline of the marketplace and of, uh, financial institutions who, uh, will only, if they need a return on investment in investing in you to take a course, they're only going to do that if they think there's a likelihood that's going to pay off. Like they don't loan you money to build, buy or build house anywhere. They do an appraisal on the house and see what it's worth and what's the neighborhood like. We need to do the same thing with education. One of the things that people really need to know in order to get a job and to be productive. So places like, uh, you, Udacity and Coursera have begun to focus in on by working with organizations like Google on, well, what do you guys really need? I talked to a young man, uh, just two days ago who took a machine learning course on one of these online.

Speaker 3:          53:52          He, it was like a three month course or something like that. He was, he was amazed. He took a three month course and it doubled his income and he got a new job and it was fantastic. Not that he was great at it, not like you guys, but you know, he, it was very valuable for him, whereas he could have gone back to school and gotten a masters degree in computer science and costs. I don't know what that costs today, you know, $100,000 or something and spent two years and it might not have been as valuable because it's going to be teaching and things which are not directly relevant to the work that he was going to be doing. So performing the educational system so that we close this loop between investment and payback is one another technique besides just handing out money. We just saw in the recent us election, uh, we have discovered some very negative social side effects of technology.

Speaker 3:          54:42          The Public Commons has moved online in social media companies, and I'll include Google in this, although they're not primary among this in such a way that it has fragmented public conversation and you are, you've read a lot about the fake news issues and all that. The problem is that the economic interests of these organizations, which is to keep you on for just a few seconds longer so they can get one more ad in front of you and that's worth millions of dollars is uh, in a tension with the needs of a, a society, a democratic society to have informed. And a vibrant debate. And there are a lot of the techniques that were used to affect the election by nefarious parties, both outside the United States, inside the United States, to abuse this new medium communications medium to sway things, uh, for it to benefit other people's interests and get people to do things that fundamentally we're not in there in their own interest.

Speaker 3:          55:42          Here's what I want to say about that. The were in that, oh my God moment. This was the classic Chernobyl moment as far as this thing goes. But the technology industry created this problem. And if you look at the history of every communications medium, a new communications medium throughout history, they've all had side effects, some of them like this or other negative side effects. And we have found ways to mitigate the negative effects of those communications meetings. This is a problem created by the technology industry and that's you guys and me. And we have a primary responsibility for dealing with these side effects in very much the way that the question over here was about, you know, can we build AI systems that, that meet our, our normal standards. So I don't think we should throw up our hands were responsible for it. We have to fix this and it can be done.

Speaker 3:          56:33          So I can go on at some length about this, but, but if you look at what we've done for email, many, I'll just give you one example. Email. As you may know, 90% of all email, it's sent as spam today. And yet we have built systems. They managed to weed most vet out. Uh, there are similar kinds of techniques appropriately applied to our social media space that can restore balance to the public. A discussion in the public space, uh, that I think is going to be very important over the next five to 10 years. Otherwise, we're, we're heading for some real trouble with, uh, the vibrancy and future possibly have our own democracy.

Speaker 4:          57:16          [inaudible].