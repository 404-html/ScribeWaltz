Speaker 1:          00:06          I'm very pleased to have sat and Steven's Davidowitz here. He has used data from the internet, particularly Google searches to get new insights into the human psyche. Today we'll be discussing his book and research. Everybody from everybody lies a SF has used Google searches to uh, measure racism, self induced abortion, depression, child abuse, hateful mobs, humor, sexual preference, anxiety and sexual insecurity among many other topics. Um, some a little less depressing then some of those, right. Um, Zach worked for one and a half years here as a data scientist at Google. Uh, so that's really exciting and is currently a contributing op ed writer for the New York Times. Uh, he's designing and teaching a course at the Wharton School, uh, where he like, he will be a visiting lecturer, uh, south received his Ba in philosophy from Stanford and a phd from Harvard in economics. So please, let's give Seth a warm welcome.

Speaker 2:          01:02          MMM.

Speaker 3:          01:07          All right. Thanks everybody for attending. Thank you. Megan is an introduction and it's great to be back here. I did work for one and a half years in mount and do Google. Uh, but I did come by here a while ago and, uh, I forgot how spectacular it is here. So it's a nice reminder, but, uh, it's, it's, it's, it's, it's nice to be, I'm talking about my book, everybody lies. Uh, which is how we can use data from the internet to understand who we really are. So for the last eight years, if you want to know what people want, a why people did the things they do, what people will do in the future, you had one main approach. You ask them, right? You conducted a survey, a Gallup or pew or Quinnipiac would go around and say, uh, what, what do you, what do you want?

Speaker 3:          01:52          What are you going to do? And a main problem with this is that people have been shown to lie to surveys, particularly on sensitive topics. They try to make themselves look good. They tell surveys what they think the survey or wants to hear and not necessarily the truth. So a classic example of this is if you ask people before an election or are you going to vote in the election? A huge percentage. The overwhelming majority of Americans say, sure, of course I'm going to exercise my civic duty and vote. And then when the election comes around, about 55% of Americans both. So people don't want to say that they're not voting. Uh, one of my favorite examples is the general social survey asks Americans how frequently they have sex, uh, and how frequently they use a condom. So according to woman, they have sex about on average, about once a week, uh, and use a condom 20% of the time.

Speaker 3:          02:44          Uh, so they say that they're using 1.1 billion condoms every year. And then they also will say whether it's gay or straight is they're using 1.1 billion condoms every year and heterosexual sexual encounters. And then they asked men the same questions. According to men, they're using 1.6 billion condoms every year. Uh, in heterosexual sexual encounters. I hope everyone realizes those by definition have to be the same, right? So we already know that someone's lying. Someone's not telling the truth here, uh, about how much sex they're having. And, uh, I got data from Nielsen. They track every condom sold in the United States. Only 600 million condoms are sold every year. So basically now everybody's lying about sex. Uh, just men are lying even more than woman. Uh, and this doesn't mean this could just be, they're lying about using a condom, not necessarily how much sex they have. But if you see how much unprotected sex woman of fertility age say they're having, if they really were having this much sex, they basically more pregnancies every year in the United States. So, uh, I think in our sex obsessed culture, there is now a pressure both on men and woman to say they're having more sex than they actually are hoping.

Speaker 2:          03:48          Yeah.

Speaker 3:          03:49          All right. Digital Truth Serum. Google. The thesis of my book basically on a thesis, my research for the last five years is that people are much more honest on Google than they are to basically any other source of thing. People tend to feel comfortable typing things into Google, uh, that they might not tell anybody else. And, uh, this data of course is all anonymous and aggregate. So nobody knows the searches that any particular person makes. But by aggregating it all and putting it all together, we can see different patterns in human behavior and human wants. So like the example people are on it do tell things, Google, they might not tell anyone else. There are more searches on average. This is using Google trends for pour. And then for weather though, if you ask people if they watch porn, only about 20% of men and 4% of women say they watch porn. So that's hard to reconcile. But people are clearly typing things into Google that they might not be comfortable telling to a survey.

Speaker 3:          04:47          Uh, so we can learn really, uh, lots of, uh, so, so why are people so honest on Google? Well, one thing, they're alone. They're online. That tends to make people more honest, but they also have an incentive to tell the truth to Google. So you, there's no reason for any person to tell a story, to tell a survey that they're voting about their voting behavior, whether they're actually voting or not voting. Uh, there's no reason to tell it to, to be honest about that. But if you're someone who doesn't always vote, or I just kind of a marginal voter, you may not know where the polling places are. So you have to tell Google, you have to say where to vote or how to vote or search, something like that. And it, it is clear in the data that this predicts turnout in different parts of the country, that when people aren't making a lot of searches for where to vote and how to vote, uh, people are much more likely to turn out to vote.

Speaker 3:          05:36          And if you're not having a lot of sex, you don't have a reason to tell that to a survey. There's no reason for you to do that. But you might have an incentive to search for this on Google. And the number one complaint about a marriage on Google by far is that it's a sexless marriage. Much more common than love lists are unhappy marriage. Uh, and we also start seeing in this data some things that maybe counterintuitive or surprising. Uh, the number one complaints about a partner on Google for husband, wife, boyfriend, or girlfriend is that the partner want to have sex with me. Uh, ease easily beats a second place complaint that the partner won't text me back. Uh, but, uh, the, the, the, the, they're, they're actually twice as many complaints on Google that my boyfriend won't have sex with me then that my girlfriend won't have sex with me, which goes completely against conventional wisdom about, uh, about, uh, who's avoiding, who's avoiding sex. Uh, so I think there are definitely surprising things in this data.

Speaker 3:          06:34          We can also use this search data to answer big, big questions. Now that I've kind of puzzled researchers for awhile. Uh, one of them that I've done a lot of research on is on racism. So this is kind of a classic area where it may be difficult to find the truth by using surveys. And for example, after the 2008 election, uh, one of the big questions was, would voters, did voters care that Barack Obama, the first major party, general election candidate who was African American, did they care that he was black? And if you ask in surveys, uh, the overwhelming majority of Americans, 98%, 99% of Americans said, no, I didn't care at all that Obama was black. It was not a factor in my voting decision. Uh, but of course this may be misleading because, uh, people may lie and not want to admit that they carried that Obama was black.

Speaker 3:          07:25          So what I did, uh, this is kind of the first study I did with this research is I studied racist searches that people make on Google. And, uh, this is obviously disturbing. Uh, this is, uh, a search for a very, very nasty, uh, worried about African Americans that you can kind of guess probably what it is or look at or read my book if you want to learn more. But, uh, it's, this is, uh, basically people searching for discharging jokes and mocking African Americans. Uh, so really, really, uh, nasty searches. And the first thing that struck me out about the searches was how frequent they were. Uh, in the time period I was looking a, this search was about as common as searches for Lakers and economist and Migraine. And daily show. So not by any stretch of imagination of fringe search. And, uh, the other thing that was striking about this, uh, when I first saw this data is that the Mac looks very different than the map that I would have guessed.

Speaker 3:          08:21          So if you had asked me before I did this research, whereas racism highest in the United States, I would have said south, right? Deep South, like that's when you think of the country's history. You think Mississippi and Louisiana and Alabama and South Carolina, and those areas definitely are among the highest, uh, but also among the highest, our West Virginia and western Pennsylvania and eastern Ohio and upstate New York and parts of Industrial Michigan and Rural Illinois. Uh, the real divide this map reveals and racism to a these days in the United States is not south versus north. It's east versus West. So which it's much higher resolution, much higher east of the Mississippi River in west of the Mississippi River.

Speaker 2:          09:02          Okay.

Speaker 3:          09:03          So how can you use this map to, uh, detect how much racism cost Obama? Basically, I compare it Obama to previous Democratic candidates such as John Kerry. The white candidate who was symbol of liberal in the previous candidate in the previous election. And you see a very, very clear relationship that in parts of the country that are making the most racist searches in western Pennsylvania and eastern Ohio and western Michigan, you see a clear relationship that Obama just does worse than previous Democratic candidates did. And you try to explain it by any other, uh, any other variable you have. And nothing else can really explain this relationship a except, except racism. So I think it was really, really clear in this data that despite what people were saying, uh, a significant fraction of Americans, I say about four percentage points overall, he lost. And about 10% of Americans and white Americans would not support a democratic candidate just because he was black.

Speaker 3:          09:55          I think that's, that's, that's what I picked up in this data. And then I kind of, uh, I kind of let this, this kind of languished and academic journals for a while, people weren't really paying much attention to it. But then in this recent election, uh, Donald Trump started saying some nasty things about, about black people, right? And we're still getting a lot of support. And this was kind of puzzling to a lot of people who thought that, uh, you're not really allowed to say those things, uh, in the United States these days. Uh, so not me, but actually, uh, Nate cone at the New York Times, a stats guy there, uh, he asked for this data. I said, can I see your racist search data? I have data on how Trump is doing in the primary, in all different parts of the country, and I want to see if it correlates with your racist search data. And he founds, uh, that it was the single highest variable that he could find, uh, that, uh, the, you know, for that I, it was higher than age and education and economic conditions and trade and policy positions and gun ownership. Basically nothing could explain, uh, support for Trump in the primary to the same degree as this racist search. Uh, did. So I think what happened is the same hidden racism that was hurting Obama but not being picked up in the data also helped carry a Trump to victory.

Speaker 3:          11:13          So I think, yeah, this is the digital truth serum and I'm, Megan is right. My book is, it's kind of depressing I think a little bit because yeah, like if you ask people what they're like a, they're going to give you one more positive view of, of themselves than they, than they unnecessarily really are. So I, yeah, I talk about racism and I talk about child abuse and do it yourself. Abortion, I think America is a do it yourself abortion crisis that isn't being picked up in the traditional data sources. So a really, yeah, dark, horrifying, terrifying, um, disturbing material. Uh, but I put jokes in it so you won't really notice just how miserable, uh, all the findings are. But, uh, I think there actually, so I think there actually is a lot of value to knowing some of this stuff, to knowing the truth, even if it is sometimes depressing and sometimes disturbing.

Speaker 3:          12:09          Uh, and I'll give you a couple of examples of that. So one of the studies I did is I just compared the searches that the Google searches that people make about sons and daughters. And I would have thought or hoped, uh, in the United States today that parents treated their sons and daughters the same way. Uh, but if you look at everybody's search data together and you see very, very different patterns where when American parents start a search is my son, there are about twice as likely to complete it with gifted or a genius. Then if they start a search is my daughter and they start a search is my daughter. There are much more likely to complete it with is my daughter overweight or even is my daughter ugly? So despite what I think parents might think, uh, there's clearly when you put together everybody's data, uh, parents on average are much more excited about the intellectual potential if their sons and much more concerned about the physical appearance of their daughters.

Speaker 3:          13:13          And I think that's one finding. So you talked about the racism thing. It's not clear that the people who are making these racist searches, uh, if you just tell them, they're going to be like, oh, okay, I didn't realize I was racist. Sorry about that. I'm going to stop making these, you know, uh, terrible searches, jokes, searches and stuff. But I think with his parenting finding, I think a lot of parents don't even realize that they're doing that. It's maybe a subconscious prejudice that they're not aware of. And if that, that's one where maybe just the information itself can actually help to change behavior. Where if we tell parents, oh, you know, you might not think so, but look, when we put together everybody's data, even in, you know, throughout the United States, there is this, there are these prejudices like think twice. Are you, are you paying enough attention to the report card that your daughter's bringing home?

Speaker 3:          13:57          Uh, are, are you paying enough attention to her intellectual interests? Uh, I, and I think a lot of parents have told me that, that has made them think twice, uh, about some of the questions they ask and some of the ways they treat their sons and daughters. So there is a lot of value in knowing things, knowing the truth, not what people think or what people say. And another example that I'm going to give, uh, is about Islamophobia. So if you can go back to the San Bernardino attacks in December, 2015, if people remember that it was two people with a Muslim sounding name, uh, shut up, basically one of the guys, coworkers. And uh, it was kind of a big, many, many people died. It was a big news story. And right after this, there was an explosion of Islamophobia. And you saw that really, really clearly in the Google searches where the number one search with the word Muslims in it immediately after the tech was kill Muslims.

Speaker 3:          14:54          And these are people just kind of, they're maniacs to some degree. These are kind of just not the most sane members of society. It's not even clear what exactly they're saying, but they're saying, ah, but they're, they're very angry and kind of just want to, I want to do something bad. And they also make searches like I hate Muslims. Uh, or you know, Muslims must die or really, really nasty, nasty, horrible, uh, searches. And these searches we've shown can predict hate crimes in the United, in the United States against Muslims. They're not, even though they're weird, they definitely contain, um, meaningful information. So a few days after the San Bernardino attack, uh, Barack Obama gave a talk to the nation. And the theme of this talk was both that we had to protect ourselves against terrorism, but also we had to fight this Islamophobia. We couldn't really allow ourselves to give into this hatred that some, a small, a small purse, but dangerous percentage of people are, we're at, we're letting themselves get into.

Speaker 3:          15:54          And, uh, the speech was nationally televised. It got a lot of attention. And it was a, one of the more beautiful speeches I'd heard Obama give. Uh, it was kind of classic Obama, but even better than classic Obama where he talked about, uh, how it's our responsibility to not give into fear and to appeal to freedom and how it's our responsibility to not reject someone just because of the religion they practice. And, uh, it got great reviews from all the serious sources, right? The New York Times said it was a great speech and the La Times said it was a great speech. And, or the Boston Globe said it was a great speech. Uh, sews the kind of all the conventional wisdom was that Obama had given this great speech about the rest of our, our responsibility to treat our neighbors a kindly, uh, our Muslim American neighbors kindly. So Google breaks down minute by minute, their search data.

Speaker 3:          16:49          And I wanted to see, did this beautiful speech, do it, serve its purpose. Did it calm down these Islamophobes. And I looked at the data and I found that not only did these crazy searches kill Muslims, hate Muslims, I hate Muslims. Thai Muslims, they didn't drop, they didn't even stay the same. They went way off. They exploded basically every time Obama was saying, uh, you know, all these responsibility, the, the, the, the importance of responsibility and this beautiful sermon just seems to backfire completely on all the bios on its main purpose. But there was one line that Obama gave at the speech that seems to have a different response. So he said that we had to remember that Muslim Americans are our friends and neighbors. Uh, there are athletes and they're our sports heroes and they're the men and women who will die for this country.

Speaker 3:          17:49          And right after he said that line, uh, for the first time in the last five years, the top word search with Muslims was not Muslim terrorists are Muslim refugees. It was Muslim athletes fall by Muslim soldiers. So, and that, these stayed up for about a week afterwards. And you saw throughout the Internet, people were talking about Shaquille O'Neal's, a Muslim, I didn't know she killed him, yell it. So it was a Muslim. And I think that does, you can kind of compare most of the speech versus what that line was. So the first part, the most of the speech, uh, was basically a lecture telling people not everything they'd heard a million times before and nothing new, right. Lecturing them that to be better people than they, than they were. And that seemed to totally backfire. But the line about the athletes and sports heroes was provoking their curiosity, giving them new information, changing what they might think of as the Muslim American.

Speaker 3:          18:48          And that seemed to be more successful. So we wrote this up in the New York Times and a column in the New York Times. And I don't think, uh, it's totally crazy though, when you're right in New York Times column, powerful people see that because a couple of weeks later Obama gave another speech. Uh, this time it was in a Baltimore mosque and again, it was on national TV. And again, it got a lot of attention and the content of the speech was totally different. Uh, basically he stopped with all this sermon, all the lectures, all the talk of responsibility, and he just doubled down or even quadruple down on the curiosity. So they said that Muslim Americans are not just our sports heroes and our soldiers. There are farmers and our merchants. And he talked about how Thomas Jefferson had a copy of the Qur'an and how Muslim Americans built the skyscrapers of Chicago.

Speaker 3:          19:44          So it was all these new images of Muslim Americans that we didn't previously have. And we look, I looked at the search data again after this speech and the in the hours following this speech and this time the searches for kill Muslims and I hate Muslims dropped. So I think, uh, that's only two speeches and I don't want to say that, uh, that, that we've solved the problem of, of hatred. But I do think that this is a radically new tool. And I think people don't realize just how revolutionary this data from the Internet is that we can actually peer into an angry mob and turn that into a science. Right? You can't, uh, in a survey of all Americans, you're not going to get, uh, necessarily these, uh, you know, these, uh, these people. And even if you do, they may not be honest and they're not going to agree to participate in a lab experiment at Princeton or Harvard. Uh, but because Google searches contains everybody's information, they're going to be on there and we can actually see, uh, what, you know, how they respond to big national events, uh, and maybe

Speaker 4:          20:48          learn a, you know, a lot of the things that we thought worked don't work, but here are things that actually work. So I think that kind of shows that this window into some parts of the psyche that are just starving but are usually missed, uh, can really serve a useful purpose where knowing the truth is maybe the first step towards improving the world. Uh, I think so. Uh, that's all I have to say. I'm going to now, I think we're going to take questions from Megan and other people about the book. All right.

Speaker 1:          21:18          I sat and that was great. The book's awesome. Um, it's really cool if you work at Google because you can see sort of what's going on here and, uh, sort of a different variation of, uh, what we do here every day. So I highly recommend it. Um, and that was just a few of the topics that you cover. You cover so many topics in the book, but I thought I'd start, uh, asking you a little bit about your experience working here and how that sort of led you to become a writer and if there's anything you miss about being here.

Speaker 4:          21:48          Yeah, no, I, I, I didn't miss it until I just had like the food right before I got here. I'm like, aw man, what was I thinking? Uh, and the views of all, they have a new floor here in the New York office where you can see the river and it's just like, oh, am I, and like the, all the furniture is so comfortable. Uh, so yeah. And like, yeah. And the people are all really smart. So I definitely, uh, I, uh, you know, you don't appreciate, appreciate what, what's the Song Joni Mitchell Song? You don't know what you got until it's gone. Yeah. Yeah. So that's, that kind of happened a little bit. Google, although I are, I'm enjoying the writing thing as well. But, and what was it like when you were working here and what were you working on? Can you give a little bit of color around?

Speaker 4:          22:26          Yeah, so I was working under how there and the chief economist at Google, he was the guy who initially hired me and then, uh, I was on his team and also in quantitative marketing, uh, how to out in mountain view. So, uh, kind of like a lot of in house data consulting, uh, but also some advertising effectiveness studies. Uh, and, and then also some research because I think Google was kind of getting interested in all this information we have. That's kind of one of the original reasons that hell hired me. Uh, that looked like that there is this powerful data and kind of how, how, how should we be using this information.

Speaker 1:          23:00          And that was sort of part of the inspiration for this book. Yes.

Speaker 4:          23:03          Yeah. I think, uh, I think the, yeah, I think, uh, I mean it's just the more I studied this data, the more I'm like, wow, this stuff is really important and there is a lot of important information so I kind of want it to get that message out.

Speaker 1:          23:15          Cool. So, so in talking about the information, sometimes you get an answer and you go, wow, that totally makes sense. And sometimes you get an answer that is sort of contrary to what you would think the data would predict. And so there's a lot of psychology involved in this can, can you talk a little bit about how often that sort of happens that, um, the answer is what you would expect and how do you kind of come to your conclusions when you get an answer that's totally different than what you, you thought would be?

Speaker 4:          23:46          Yeah, I think, uh, frequently things, things are just different than I expected. So I did, I've done a lot of research and anxiety and I thought anxiety was highest in New York City. Like, cause I'm from New Jersey, right? Like I'm from New Jersey, right outside New York City. I'm like Jewish. It was always like, oh, you're like a neurotic woody Allen type. I always thought I was really anxious, neurotic type in it. That was like a normal, uh, and that that would be like when we were all way more anxious than everybody else. But then you see in the search data, uh, that is not true at all. That anxiety is highest in Kentucky in you, upstate and Maine and rural areas way more than urban areas and places with lower levels of education more than higher levels of education. Uh, so I don't, yeah, I think it just, just over and over again the data, I think we're, we're just basically blind to the world.

Speaker 4:          24:33          I think a lot of times we think whatever's going on in our own head is much more general than it is a, or we just like, we jumped to conclusions very, very fast. Uh, so that's why I think tradition. I think the data is usually different than I expect. I mean, sometimes it's not like if you search where, uh, where do people search for Lakers? It's like Los Angeles and you probably didn't. Okay. That makes sense. It's like, I'm not going to like, uh, like, yeah, shock the world, like the Lakers and more popular in Minneapolis then Los Angeles, like that's not true.

Speaker 1:          25:02          So is that, helps control your anxiety because you're like, wow, those people in Maine, they've got to like way worse than I have it here.

Speaker 4:          25:08          Yeah, no, I think it, I, it just, it's just like changed how I thought about things. I'm like, Oh wow, that's not, yeah, it's just an even like, and that it's even like very particular types of anxiety, like anxiety about death. People make searches anxiety about death. And I'm like, oh, that's like the woody Allen neurotic like intellectual thing. But it's not true. It's like there are more of these searches and Kentucky and Alabama and Louisiana. So it's, uh, it's just kind of interesting and I definitely do to kind of go through the world differently than I did before.

Speaker 1:          25:36          That's super interesting. So, so you ask a lot of questions. I mean, we just talked about, you know, how many Americans are really racist, who cheats on their taxes, does advertising work. But I'm sure there are a lot of things that got left out in the book. So can you talk a little bit about sort of how you go about choosing your topics and if there was anything interesting that maybe didn't make the cut because there's so many interesting sort of factoid.

Speaker 4:          25:59          Yeah. Uh, I don't, I don't really, there's not a science to, to choosing a topic and just kind of go around the world and talks a lot of people and read a lot of things and then one thing sparks a question or you play around with data and it goes in a totally other direction. Uh, so I don't, so, uh, so I think that didn't make it, I have like all this stuff and anxiety that's just not like ready for prime time that got caught. Like I could have had, I wanted to have like three chapters on anxiety cause I just find it really, really interesting. So that's like a whole other book that we can look forward to. Yeah. Yeah. That's, that's, that's, that's maybe my next book, but, uh, but, uh, yeah, like, so I've done this research on if you break down the minute by minute when people make searches for panic attacks, uh, and it's not surprising when do people make search for panic attack?

Speaker 4:          26:43          Like 2:00 AM 3:00 AM right there like in a cold sweat in the middle of the night. Uh, but like what, what this basically means is that now, because this data we know on every given night, we have a pretty good estimate of how many people are having panic attacks in New York City and Boston and Los Angeles and Indianapolis. And we can basically say, okay, why are on some nights a lot of people having panic attacks or like, is there something that happened three days before, two days before the day before. So there's really just so much information here that it's, yeah, I think it's revolutionary, but there's some people criticized me for being too grandiose.

Speaker 1:          27:19          That's great. You got to sell it. Right. Um, yeah, I think that data is totally fascinating. I agree with you and, and that it applies to so many things. Um, so back to the election a little bit, you talked, you talked a little bit about that and you kind of out smarted Nate silver. Would you say that, that, that might be correct, and in some ways, um, are you especially, so I'm trying to get him to agree to, let me write a column for is, are you a, are you looking at data now and what, what are you seeing in terms of trends in politics now that Trump is in office? Like what's the followup to what we heard you say about this sort of wave of populism and the thing about the election? So there's a question just starting with politics. Can you predict,

Speaker 4:          28:05          uh, elections with Google searches? Because if he locked it, the survey, uh, surveys this year, they didn't really work so well, right? Every, the surveys told us that Hillary Clinton was gonna win and then Donald Trump won. So is there a way to use all this information on the Google where people are so honest to predict elections and it's not so simple? Uh, the top white, most people have tried to do it initially was, uh, you just see are you, are people searching for a candidate more. So if people are searching for Trump war, they're going to go Trump. And if people are searching for Clinton more and they're going to go Clinton. And you can probably think like yourself why that wouldn't really work, right? Because you're not saying whether you like the candidate or you hate the candidate. You could search for Trump because you like it or because you hate them.

Speaker 4:          28:51          So one of the indicators, uh, I've found with Stuart Gabriel who is a professor at Ucla, we found there is an indicator that has surprising the predictive power and it's the order in which people search candidates, which is pretty interesting. It's basically, uh, if people like 25% of the searches people make with Clinton also include the word Trump. So people search for Clinton, Trump polls or Clinton Trump debate or Clinton Trump election. But, but if people search Clinton before Trump, they're much more likely to go Clinton. And if they search Trump before Clinton, they're much more likely to go Trump. So it's like selling subconscious. If you're a Trump supporter, you're much more likely to, to think of it as a Trump Clinton election. If you're a Clinton supporter, you're much more likely to think the reverse. And then you could see that, uh, in general, uh, Trump came before Clinton more and then this was more true, a key Midwest battleground states, uh, which, which he, where he, he got, he got victorious. But I think it's going to take many, many more years of analyzing this data before we know exactly how to weight in and stuff. But there definitely is a lot of information in this data, uh, that will be missed by other sources.

Speaker 1:          29:56          That'll give you something to do for a long time to come. Yeah. There's no shortage of things to do with this data. Cool. I'm going to invite everyone here to start lining up at the mic if you have questions. So please come on up. Um, and in the meantime, I'll, I'll ask you one more question. Um, so some of the data it gets personal, right? You talk about being a mets fan or, um, or just, just a little, um, or you know, like you talk about, um, what women should do on a day to like get a second day. Um, so how much of this like came from sort of your own personal life and uh, you know, is, is that like a big way that you sort of select topics and, and does that make it feel like more justified in your everyday experiences?

Speaker 4:          30:45          Uh, no, no, a few. A few of the topics where my personal interests, a lot of the sports stuff. Oh, it was my personal curiosity to be a baseball player. Yes. Basketball player. I would have settled for a baseball, but uh, yeah, but I think, uh, I think, but like the racism stuff, I don't think I'm particularly racist or I did stuff on gay. There are a lot of closeted gay man. I'm not gay, but I thought that was really interesting too. So I don't know. That's not all. Uh, it's a, it's probably like 20% personal face, but

Speaker 1:          31:15          fair enough. Okay. I'm going to turn it over to you guys

Speaker 5:          31:18          at the mic and just be okay.

Speaker 1:          31:27          So I was struck by something you said, which was that people are partially more honest online and with Google searches because they have an incentive, they're trying to get something from their search. Um, but then some of the searches that you cited with, you know, the racism research and some of the others, like I hate Muslims and you know, things that didn't seem to be a question, which you would think as the incentive with a search to what do you attribute that type of honesty because it sounds like those, those people are not at the very least seeking additional information on a topic, which is our usual definition of a search.

Speaker 4:          32:02          Yeah, that's a great question. That's a so, yeah. So there, there are two reasons that Google is honest. The one is the incentive thing. So if you're talking about a sexless marriage or racist jokes or information on voting where they're sensitive topics, but you need the information and then there's this other class of searches, but totally shocked me. Uh, like it was one of those surprising things in the data, but it happens in big numbers. People just confess things to Google, so they say, I hate my boss, or like I'm sad or I'm drunk. And it's just like, okay, like why are you,

Speaker 4:          32:37          why are you, why are you telling? And I think it is, I think a lot of it is a, I think a lot of it is, uh, it's kinda similar to the confessional, right? The Catholic and p like you don't just somethings about it. I think people treat it as like, like a confessional. You don't, there's no purpose to sang things, but there's something about saying things that you wouldn't tell anybody else. Uh, and it's, it, it's, people seem to use it in that way. Uh, and yeah, it's, it's really surprising. My favorite example is, uh, that, uh, what I talk about how men are insecure about their bodies. Men. So what we usually think that woman a w they'd like. Bali insecurity is predominantly a female thing and it is majority female. But if you look around the web, like it's close, it's like 55, 45 a woman versus men and men are really insecure about their bodies as well. Uh, and a lot of it is insecurity, not surprisingly focuses on one particular body, part in the size of it in particular. Like men asks more questions about their whatever then then, and then any other body part. But then one of their top questions they ask about this body part is how big is my penis,

Speaker 1:          33:46          which is the strangest search engines. Right. Understand. Yeah. So it's a straight, so it was a stranger. Yeah.

Speaker 4:          33:51          Search ever. So people make like the weirdest searches on Google. It's very bizarre.

Speaker 1:          33:54          Follow up question if it, if I may. Um, have you done any sort of, or is there a data correlation analysis between, you know, for example, people confessing on various forums and reddit and sort of places where you would confess? I think with, uh, an expectation of other people like saying Oh, me too. Um, and like searches. Like do they spike at the same time maybe, or are they localized in the same way? Geographically?

Speaker 4:          34:23          I haven't seen it. It's a good, it's a good question. I think that some of the Google searches, I think that is it that you kind of, if you type on sad you might get message boards where people are saying, oh I'm sad too. Uh, so it might be that you're looking for people who are feeling the same way as you are in just by saying it in that way you get that information. Thank you.

Speaker 3:          34:44          Hi, thanks for coming in today. So when I do a search, often I'll have one thing in mind, but as I'm typing this search, I'll see the auto complete suggestion. And even though it's like not at all what I've meant to search, I might out of curiosity, complete that and just have that search anyway. I'm wondering how you account for that or how do you know that's true Vo, if a lot of people do that, you know, cause you don't want to count those like double count, right.

Speaker 4:          35:07          It's not great. You don't like downtown. I think it makes like a, I don't know what percent of searches somebody Google probably knows what percent of searches are you use auto complete. I don't know that number. I don't think it's publicly available, but uh, I think uh, it makes small differences. It could like magnify small, different magnify initial differences, right? If people initially have to search something to get there but then the winner will get potentially a more and more popular. So I don't think it totally changes the level of like the ranking of things, but it can make a bigger difference between the top and the other ones. Okay. So overall you're saying that, um, when you do look at the top search results, you do keep in mind to like scale down a little bit just to account for that? Yeah, and I think the regional ones are pretty, the regional differences are still pretty meaningful, uh, since from my understanding, they don't, uh, they don't get, they don't on average get very different. Autocompletes Nicolas, thank you.

Speaker 2:          36:03          Right

Speaker 6:          36:04          on your early chart that you had the comparison with racism and correlation to Donald Trump, my question would be what was that compared to Hillary? Because was that a flip or was it very similar in the two charts?

Speaker 4:          36:19          Uh, so that was the primary voting. Yes. So it wasn't, so you're saying what happened to general election? Yes. Yeah. I think the general elections a little more complicated because, uh,

Speaker 4:          36:30          it's like Democrats and Republicans differ, differ in general. So it's not the most of the, well, the reason that one area it goes Democrat in one area goes Republican, it's just, it's more Democrat and Republican area in general. So you can really want to compare it to previous elections. And that's also a little complicated because then Obama had this racism problem and then Trump, I think what it's basically that Trump, uh, an, uh, not a Republican candidate who didn't appeal to, uh, w who did an appeal to, uh, racism and the way that I think Trump did a would have lost a lot of votes in park quite in places relative to Obama in parts of the country with a lot of racist searches. But Trump didn't lose those votes. So places like West Virginia and western Pennsylvania and eastern Ohio, if it was a norm, if it was a different Republican candidate, those areas, they didn't like Obama, but they would have come. Uh, but, but they would have necessarily, they would have maybe come back to a democrat. But because Trump appealed to that same, those same feelings that made them mad about it, Obama, they went Trump's way again.

Speaker 6:          37:33          Okay. Because of the variation between the primary and the general election then probably has a great deal of effect that wouldn't have been in that, the correlation between that, those two charts might have been quite different if they were done at the same time.

Speaker 4:          37:49          Yeah. Well, like I said, the general elections a little more complicated because most of it is the map of any particular general election is very similar year to year. So really you just want to see the changes in behavior, the changes and votes in a general election. Okay. Very good. Thank you. Dot. Thanks.

Speaker 2:          38:07          Okay.

Speaker 7:          38:08          Yeah. Let's go on a quick question. You mentioned earlier how people lie on surveys and we have the product, Google consumer surveys. So I was wondering if there's any type of techniques or things you've seen from data collection via surveys like Okcupid, answer publicly versus answer privately. Have you seen anything that either yielded more consistent or reliable results? Uh, from a survey based format,

Speaker 4:          38:31          so they're all right. So I'm mine surveys 10 feet better than phone surveys, for example, because I think talking to someone, it makes people that much more dishonest. Uh, I think I, there are all these, there are all these games that, uh, uh, that scholars have been vented to try to trick people into telling the truth. Uh, and you can kind of look them up. It's random digit, a random digit, uh, examples or, uh, I list list experiments. They basically ask people 10 questions like one of them is, is embarrassing, and the other RN and ask how many is true are true yes or no, and then can kind of back out and then ask another group that except the embarrassing one. Kind of come back out the difference, if that makes sense. Uh, but my understanding is that these don't really work. Uh, although a lot of papers have been written about it that, uh, that, that, that they don't really work. I think Google consumer surveys has another pretty huge problem, which is that people just answer randomly. Uh, because, uh, like at least if you're answering a phone survey or you've gone through the time to not in it, not hang off right away, uh, you, you might give a take it somewhat seriously, but I think a Google consumer surveys, a big percentage of the people don't know, don't give a serious answer. Yes. Sorry.

Speaker 7:          39:47          Or for the APP, the screener questions, they always answer whatever they think that we want to screen it. Yeah, exactly. Yeah. Then also by the way, the buck amplification study you worked on, the clients really liked it.

Speaker 4:          39:58          Oh yeah. That was another thing that I, I work, yeah. Oh, okay.

Speaker 7:          40:04          Are you concerned at all about the Hawthorne effect by talking about what we can learn from the Google search results? Maybe people won't be as willing to put their dirty laundry out on Google.

Speaker 4:          40:15          Yeah. I don't know if it's what we can learn. There definitely are changes in behavior. So this is all based on the idea that people will tell anything to Google. But someone did a paper where they compared Google searches before and after a Snowden leak and they found that there was a big drop in searches that were either sensitive. So like, uh, you know, on sensitive topics or embarrassing topics. Actually, my favorite thing from the paper is that they had a list of them that they, uh, they had to figure out what an embarrassing search. So they asked people in Mechanical Turk like to rag the embarrassment of searches and one of the them they got classified is embarrassing, uh, was Nickelback and they found that those types of surgeons did drop including Nickelback after, after a Snowden Snowden's revelation. So yeah, I don't think, yeah, it may be that, uh, that this is a brief period of time where we can really see until the human psyche and uh, and then it will all die down or something. I hope not. But, uh, I think I always emphasize that like, everyone's like, have you, has your search behavior changed since you've done this research? And it hasn't at all because I'm like, like nobody knows my sir. Like, I don't, don't, I don't see why it, why it should affect me. If they know that someone in, uh, Brooklyn is making a search or something, it doesn't seem like a,

Speaker 8:          41:42          okay,

Speaker 9:          41:43          if I was a racist, I might not want to give my state of bed named by doing lots of racist searches. I don't know if I felt bad about being raised.

Speaker 4:          41:50          Yeah. So it could be like, yeah, that's like kind of a subtle, I'm, that's definitely bad. Definitely. It would be a, I mean, a problem in polls as well. So, uh, but I don't know if that's,

Speaker 8:          42:01          yeah. Thanks.

Speaker 9:          42:06          I, um, have you thought about comparing the Google search results? Was there like a, uh, uh, social network resolves or posts, um, are the social network results tend to be more or as honest as Google search terms or even more honest because people use that as a way to express themselves and not necessarily, uh, uh, exposing, exposing their true identity or people stay at all who was such as ours tend to be more on his,

Speaker 4:          42:43          no, I've done no question. Google searches are more, are more honest than social media. So I think that social media data you can't really trust because it's even in some sense worth than surveys because you have an incentive to make yourself look good to impress your friends. Uh, so if you compare, for example, one example I talk about is the popularity of the national enquirer versus the Atlantic monthly that uh, the national enquirer actually cells, it's kind of a low brow trashy magazine that actually sells more copies than the Atlantic monthly every year. But on a social media, the Atlantic monthly is 45 times more popular because everyone wants their friends to think they're really, really intellectual. Right. Uh, and then it actually is interesting if you compare the, uh, the social media posts in Google search post, uh, Google searches. It's a, so if you look at the top ways people describe their husband on social media, the top five descriptors.

Speaker 4:          43:41          My husband is a, it's my husband is amazing. So cute, awesome. The best and my best friend, which is probably misleading view of marriage to some degree at least. And then if you do Google the top five complete, my husband is on Google with salsa. Kind of like a weird search to make, but people do make, my husband is uh, one of them is also awesome. So that checks out. But the other ones are gay. A jerk meaning annoying. So it's really a, yeah, it's kind of interesting. I think you got, I'm not sure how if one of them's right or wrong on marriage otherwise, but uh, it's definitely very different because I'm one year trying to impress your friends and in one year not, thank you very much better. Just looking at that map, is there a way to tell or do you tell the difference between two regions where like

Speaker 1:          44:32          there are like one where there's a lot of people doing those? For example, racist searches or another area where it's like fewer people doing a lot of searches per person. Like do you account for multiple switches?

Speaker 4:          44:44          Uh, yeah. No, it's a good question. No, I, well that data is just not made available. Uh, I dunno if anyone at Google hazard, but uh, it's, it's uh, it's not made available except what Google does in the end. Google trends and they take out if someone makes a lot of searches in a short period of time, it just counts as one search. So it's not like someone just, you know, in a, in a half hour search this thing over and over again that it's driving the results. I think it is, it is an interesting comparison. I think in some ways it's an advantage of this data relative to surveys because I think surveys that we usually think of, not always, but usually think of as yes or no as binary. Either you are racist or you are and racist, but there clearly are degrees of it. Right? You're more, you're more or less likely. Uh, so it's on something that is an advantage that includes people who've done it a lot of times because they're probably even more even more racist. So, uh, yeah.

Speaker 1:          45:36          Cool. Okay. I just have a few more. Thank you everybody. Um, those were great questions. So I know you spent a lot of time thinking about naming your book and went through a bunch of sort of different names. Can you talk a little bit about the process of naming a book and how you came to everybody lies?

Speaker 4:          45:54          Uh, well, so yeah, so that's why I initially brought up the, that ridiculous question than men asked, uh, because that's what I wanted to call my book. It's called how big is my penis? What Google search I was going with that, what Google searches or feel about human nature, but then on my is like, people would be embarrassed to buy that in an airport. Uh, so I, I couldn't, I couldn't title with that, but, um, I think, uh, I though I still kind of think it, that was a better title, although I do like this, this title I didn't think of that was this was that they came up with this title. And I think it's good. I like it. I like it. I think it gets to, gets a lot of the point.

Speaker 1:          46:37          What about some of the other data sources you look at? I mean obviously you look at Facebook, you look at porn hub, you look at, you know, a variety of different sources. How do those rank compared to Google and, uh, are you planning on looking more at other sources or is Google for you sort of the old tough

Speaker 4:          46:56          place to be? I think Google's just way better than all the other ones because the honesty and then it just Kinda, it's so universal. A lot of these data sources, Twitter, like who uses Twitter? It's a very selected sample, but pretty much everybody uses Google. Uh, and then just any topic, there's information there, like can be a music or race or sexuality or you know, any, any, anything. There's probably at least some insights. Uh, [inaudible] pornhub is pretty specific. Yeah. And like, you know, or like they're kind of more one offs. You can find something interesting I in the, in the, in the Dataset, but it's not as comprehensive with googling and find something interesting on any topic or thing. So it's more of a, I think it's a, a more, uh, more like orders of magnitude better than the other data sources.

Speaker 1:          47:43          Well they like to hear that that keeps us in business. So what you mentioned before that you kind of have,

Speaker 4:          47:50          okay.

Speaker 1:          47:50          Grandiose, I actually think it's, it's um, are really sort of enlightening, um, kind of sense of where data science is going for both philosophy and medicine, sort of how it can be used to really help people in the future. And so I was sort of wondering if you had any advice, I know we have a lot of data scientists out there, both at Google and in the world. What advice would you give them in terms of data science, what they should be doing and where it's going?

Speaker 4:          48:17          I think it's, I, well, I got credit. I just think it's a really, really exciting area because, uh, just because of all this new data that's out there that, uh, you know, I think like the insights that are coming are going to be huge. I would say that some of that, uh, obviously if people work at Google, part of their job is probably going to be a lot of, a decent percentage of, of Google employees, uh, have to get people to Click on ads, which is not necessarily the most, uh, interesting, uh, in my opinion, uh, use of data, but a is important for Google's business. But like, I definitely think, I definitely think the health stuff is, uh, is really valuable. There's a, there was a study, how much time do we, you're fine, I'll go ahead. So, uh, so there was this study, uh, that, uh, was done by Microsoft researchers in collaboration with Professor at Columbia and they, uh, study, uh, pancreatic cancer.

Speaker 4:          49:09          I talk about in the book, they cite pancreatic cancer and they basically could figure out that they, they, they studied individuals whose a anonymized deidentified individuals, their search behavior over time. And they said that, and they could guess based on someone searches that they maybe got a diagnosis of pancreatic cancer cause people titling like duck just diagnosed with pancreatic cancer. What do I do? Or like very, very clear searches that they get pancreatic cancer. And then what they did is they compared these people to another group of users who are similar and never had such a diagnosis. And then they looked at the searches in the months leading up to that diagnosis, the symptoms that people were searching and they said, what symptoms do people search that tells us in two or three months they're going to have a diagnosis of pancreatic cancer. And the key to that, the reason this study's potentially powerful is if you get the earlier you get it, pancreatic cancer diagnosis, the higher your chances of survival.

Speaker 4:          50:05          So I, uh, so they, and they using this data, they found really subtle patterns to the point that if you search for indigestion before abdominal pain, that's a risk factor for pancreatic cancer. Whereas if you search just indigestion alone, that's not a risk factor for pancreatic cancer because they had so much data, they could pick up these really subtle patterns. I'm kind of scared whenever I hear the story as like whenever I, right after I read that paper I thought, I thought I had indigestion fall, but abdominal pain that's going to be in the anxiety. But yeah, so I don't want everyone to go home and be like, Oh crap, I've got pancreatic cancer. But, uh, I think, I think that's like a really impressive way to do medicine relative to what we usually do. That that's a pattern that I talked to the researchers.

Speaker 4:          50:50          Doctors don't know that series of symptoms, like if, you know, thank half of the way that doctors now diagnose diseases, it's not as sophisticated as like a time series of people symptoms over time and a huge sample to pick up patterns like that. So like, so I'm kind of a lot of people after that said, uh, so what are the HEPA building implications? Should a search company like if they know this information, should they tell you, you know, right below your button, I feel lucky you have pancreatic cancer or you might have pancreatic cancer. Like that's kind of a depressing thing to see on a search engine. But I think so. I think that they, they should, but they also, but that you should be able to opt into it. Say if I, if I'm the type that wants information, like you should be able to say, hey, like if you can mind these patterns and potentially if I have some series of symptoms that tell me that I'm at risk of a disease and if I am told that I'll increase my odds, I want to be told that.

Speaker 4:          51:44          But I go even further. I think that these businesses now have an obligation to be researching this, to be Tensley find patterns of symptoms, because I'm kind of pissed that there may be diseases that just because not enough data scientists, uh, at the, at these companies are looking into it. They may be able to pay, they may be picking up patterns and by symptoms, uh, that could potentially saved my life. So I think that, like I go even more extreme, and I think that there's an ethical obligation of companies who have this data to be really figuring out the health, the health, the health stuff there.

Speaker 1:          52:18          So big implications. And with that, uh, let's give a round of applause.