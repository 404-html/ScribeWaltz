Speaker 1:          00:06          We are excited to have our speaker finale. Doshi Vela is here today. She is an assistant professor in computer science at the Harvard Paulson School of Engineering and Applied Sciences. Her research focuses on the intersection of machine learning and healthcare and today she'll be speaking to us about interpretability towards more rigorous rigorous evaluation.

Speaker 2:          00:29          Thank you. Hi everyone. It's

Speaker 3:          00:34          exciting to be here. Um, I like to keep talks fairly informal, so if you have questions along the way, please shout out. Um, and we'll also have questions toward the end. I one to, I want to point out, um, I'm going to share a little bit if the work that we're doing currently in our group, but more importantly, uh, the, the thing that I want to chat about is how we can make interpretability more rigorous. So it's a bit this, this talk is going to have more of a philosophical or forward looking component, um, rather than a ton of results that are already completed components. And that's why I encourage everyone, if you have ideas to like really engage in the discussion because that's what I'm really interested in having.

Speaker 3:          01:18          So just to get started, um, as a, you wouldn't be here if you weren't interested in interpretability. Um, it's definitely in vogue, tons of publications coming out, especially lately. Um, if you just look at Google scholar hits, obviously the rate of publications in general has been increasing, but you see there's huge numbers of publications in the last several years, um, under this guise of interpretability in machine learning. And part of that interest comes from the fact that machine learning systems are being deployed in more and more settings and as part of more and more systems and now we are curious about, well, are these system's behaving in ways that we expect? Sorry, they latching on to signals that we didn't expect. How can we ensure that our systems are doing what we want them to do and how can we debug them when they're not doing things that we want them to do?

Speaker 3:          02:11          Hence this growing interest in interpretability. Um, but the big question is what exactly is interpretability? And that's the thing that I want to chat about today. And also how should we go about quantifying and measuring it both from a human perspective and also is there a way to distill it down so that we can optimize for interpretability maybe without all of those expensive human subject evaluations. That's it. Those are the sorts of things that we're gonna be chatting about. So first, let's start out with this question of what is interpretability. So if you look at a dictionary definition of the word interpret, which is the root word for interpretability, it's to give or provide meaning to explain. And I think that explanation is actually a much more like human tangible word than interpretability, right? It's, it's easier to think about what is a good explanation. Have I explained something to you?

Speaker 3:          03:04          Because you can think about whether, do you understand it or not? All right, so that's the term that I'm really going to be thinking of and I'm encouraging you all to think about when you think of EC interpretability is the question, what's the quality of this explanation? And before I go forward, I also want you distinguish the word interpretability or the process of providing explanation from a lot of other words that we also see as auxiliary criteria for our machine learning systems. So there, there was a time when we were just interested in prediction. So we just need a system to predict and now we're often interested in other criteria as well. So there's been a lot of recent interest in transparent systems in accountable systems, trustable systems, fair systems, systems that produce actionable outputs, systems that are robust or reliable systems that are safe.

Speaker 3:          03:55          And some of these areas I forgot, privacy, privacy is another one. Um, so there, there's a lot of these sorts of terms that are being bandied about and some of them like privacy, they have very clear there are some at least attempt at making a rigorous definition. For example, differential privacy has a very clear mathematical formalism. You can, I, you can verify whether a system is differentially private or not. And we don't have that in interpretability. And one of my goals is to get us closer to thinking about what those sorts of definition should be. The reason I distinguish the process of explanation from these other criteria is it oftentimes we fall back on interpretability or the need for explanation because some of these other criteria are also not particularly well quantified. So we want us to send to be safe. For example, what if we can't list out all the situations in which an unsafe, uh, scenario might happen, we might fall back on, well, if the system can at least explain what it's doing, then we're in a good position to identify whether something might be safe or unsafe with a bit of human checking.

Speaker 3:          04:58          Right? So interpretability is this thing that you do mostly in the, probably in the service of something else, right? So let's, let's, uh, let's actually think about what reasons you might need for interpretability. So one area that I work in quite a bit is in healthcare. So we are interested in deriving disease subtypes. We are also interested in identifying what treatments work best for what patients. And in this case there is a clear scientific question of like what is disease, how does it progress, how to drugs interact with a disease process. And interpretability is really important because we are looking at large amounts of data. But at the end of the day we actually want to have some new insights about the mechanism of human physiology and pathophysiology. Another reason why you might want interpretability is to be able to debug your system. You have no idea why the system is not working.

Speaker 3:          05:50          But if the system can give you an explanation of how it's coming to its decisions, maybe you can identify why that decision is wonky and find the the issue in the system. I also met, I already mentioned the issue of safety. Again, maybe you can't identify all the scenarios under which a system might be unsafe, but at least if it tells you under at any point what you might want to do or why it's making a decision, you can decide whether what you want to do with that decision. Another big area, um, category that I put a here is mismatched objectives. So, going back to the clinical scenario, sometimes we have a thing that we're interested in terms of output. So for example, if you have a patient who is depressed and you want to manage their depression, then that's the criteria and maybe that you're trying to optimize.

Speaker 3:          06:38          But it might also be the case that that patient is averse to weight gain. And so there might be this auxiliary or secondary objective that you were not aware of, or maybe there's a particular drug that requires being taken three times a day versus a drug that being taken once a day. And depending on the patient, the probability of adherence may be different for those two regimens. And if you have those written out, then maybe you could write them all down and you could put them into an objective function or just solve the multi objective problem. You can, we know how to find Paredo fronts when we have multiple objectives, right? Again, there's no interpretability required there. Um, but if we don't actually have the full objective at the time and maybe the human is not even aware of exactly what they want, then that you have run into this case of mismatch objectives where you need a little bit of explanation that might suggest why this was chosen, but maybe you can also decide that you want to do something else.

Speaker 3:          07:35          Um, and, and change the, the final decision. And the final one that I'm going to point out is in a legal setting. So EU regulation is going to require all systems algorithmic systems to provide explanation starting in 2018 2019. And I think we as a community, as computer scientists, as people who are interested in interpretability, have a real opportunity here to make sure that that regulation, like the definition of that is going to be decided in courts, but we need to be able to come up with proposals of what we think interpretability should been and what the reasonable definition should be. Otherwise it's going to be decided for us. Right? So there's kind of a pressing need here from a legal perspective to come up with operational definitions of interpretability. So in all of these cases that I described, I would say the common thread is that there's a fundamental incompleteness in the problem specification.

Speaker 3:          08:38          And I want to distinguish that from the concept of uncertainty. For example, if you are trying to track a missile on a radar, you know the, the radar does not exactly pick up where the missile is. There's going to be some uncertainty and you have this ellipse about where you think the missile is. You get your set of measurements and it decreases your uncertainty. Or maybe it increases it because it's something unexpected. And now maybe you can turn on different kinds of radar and you can use that to determine further hone your estimates. But this is a closed system, right? You have a set of measurements and you can very well quantify the uncertainty, right? So in that sense, the problem specification is uncertain, but it's not incomplete. And to the end of the day, you're going to have to make a decision with that uncertainty. And we have rigorous ways of decision making under uncertainty.

Speaker 3:          09:27          That's an entire branch of cs so that if you're living in that scenario, you don't really need interpretability. There's no human in the loop necessary required because at the end of the day, you're just going to have to make the best decision you can given the uncertainty that you have and the examples that I've described here. In contrast, you have this fundamental incompleteness. There's no way around the fact that if you are trying to create scientific hypotheses, you don't know exactly what you're looking for and you need some sort of explanation to help you generate those hypotheses. Or if you cannot write down all of your safety scenarios in terms of unit tests, then you need the explanation. There is a fundamental thing that you can just cannot, you cannot unit test your way out of this, um, particular situation. Right. So, so that's one thing to keep in mind because I, I guess there's various debate about whether interpretability is needed or when it is needed.

Speaker 3:          10:23          And this is the definition that I like to stick with or reasoning that I like to stick with in terms of fundamental incompleteness. So before I go into talking a little bit more about, um, uh, ways to measure interpretability, I'm just going to give you a little teaser or a taste of the sorts of things that we're doing in our group. Just one little short Vignette, a little story. Um, uh, so this is work that is, was just accepted to Itch Chi, um, which we are calling right for the right reasons. And here's, I mean, here's the main idea. Um, so we have, let's say we're interested in creating some sort of different, we have a differentiable Cli classifier such as neural network. And so we have some set of outputs and we have some inputs. And what, again, the choice of this function can be anything like a neural network. Um, and then we have some set of weights that it's parameterized by.

Speaker 3:          11:19          And then if we have that, uh, the question could be, are there multiple qualitatively different classifiers and why it might be we interested in that. Well, maybe there's these reasons over here where one of them might be unsafe or it might not be reliable. Um, it may be the case that one has some subtle subtle issue with it due to a confounder. And so without the information of why it's making the decision, um, we, we can't tell for those sort of things. But if a classifier can tell you why it's making the decision and we can find multiple classifiers who make the same decision for different reasons, then maybe we can try to give those to someone and have them decide which is the best one, which is the one that they like. And maybe just exposing the fact that there's multiple different ways to get the same level of prediction accuracy with different types of classifiers is also kind of enlightening to them about the nature of the Dataset that you have on hand.

Speaker 3:          12:16          Uh, the way we do this is we're going to define our explanations in terms of the gradient. So if you tried to change your value of the input by a little bit, remember that x is the input. If you change the value of the input by some amount, how much does the output change? Um, so if you have an input that is very sensitive to, then that is something that should be upgraded and your explanation or be part of your explanation. Um, whereas if it's something where changing this x doesn't really change your output, then maybe it's not an important feature. So ingredients have been used as a form of explanation. This is not new to us. It had been proposed as a form of explanation in the past. And the Nice thing about these is that they're, they do rely on derivatives and derivatives are very easy for us to compute.

Speaker 3:          13:04          They're very fast compared to perturbation based approaches. So there's a lot of other approaches such as Lyme that take the x, perturb the x, and see what perturbations of x result in the biggest changes in why. I said it's another way of doing your sensitivity analysis through to perturbations. We're doing that now. An infinitesimal scale with derivatives. The advantage being that they're, again, they're super fast. And now what we can do is we can say, if you're given one set of weights, let's find a different set of weights that are not allowed to use the gradients that we're large in the next round. So round one you create your own classifier round to either someone provides some annotations and say's actually these were bad, like don't use this information when making this decision or you just try to find a completely different solution. Right. Um, so I'm just going to give you, again, this is a short story, so I'm just going to give you some examples of how this sort of system might work.

Speaker 3:          14:03          So here we have an example where we're trying to classify these images, these five by five images into two categories. And the choice of the category can be made in two different ways. So it can be made depending on whether the corners of the square match in color or not. Uh, and it can also be made based on whether these top three elements of the square match or not. And so there's two different ways of doing the classification that are going to lead you to exactly the same classification accuracy. So the, the, in the first round, uh, the of the class of Arthur, this is accuracy in the first round of the classifier. Um, we're doing it again, this is a simple classification problems. So you get a hundred percent performance and it's mostly picking out these corners. Second round we say you're not allowed to use the information that was highly weighted in the first round.

Speaker 3:          14:58          Find me a completely different classifier or as best as you can, you have a parameter that you can tune. And so the second round, while there is actually another example that exists, another choice that exists. So it gives you that choice out here. And then you go to the third round and you say, actually, now I want one more. Can you, can you do it for me? Um, and it tries and it's mostly picking up noise and the, the accuracy goes down. So this, this is a very simple approach, very fast approach. Because again, it's just based on gradients, competing gradients, twice. Essentially. You need to be able to take your function, differentiate it, and then you need to differentiate again to be able to compute sensitivity to the input gradients. But that's all you need to do. And then we can do this on more complicated setting.

Speaker 3:          15:41          So here we played a similar game with two UCI data sets, Iris and breast cancer. And what we did is we took the outputs and we glued together examples from these two different data sets based on the outputs. So if it's like flour of type A and tumor of type B, those are all get linked together concatenated together. So now again, there's two rules for making the decision. And here what we find is that this is much more complicated dataset. There's many ways in which you can, so here iterations, there's actually many ways you can get very good prediction accuracy and this isn't going to useful thing to keep in mind that from any real world data sets this happens. Um, but you see that the test accuracy test accuracy, there you go, um, is lower sometimes because it's picking up. So in the, this was in the training and then in the tests that we took away, basically the flower information, like we permuted the labels.

Speaker 3:          16:33          So the right thing to focus on was the cancer tissue question. So initially you see that it had picked up on the wrong stuff. And as you go through fewer, as you go through additional iterations, it ends up picking up on the right things. And there's no guarantee that this is going to happen in that particular order. But the ability to be able to produce quickly, qualitatively different solutions helps you sift through them and try to find ones that are reasonable. And this is the last example I'll give you along these lines. So here we have an example from 20 newsgroups and uh, these are examples of words that it's picking out, uh, in different, in different iterations. So this is, uh, being able to distinguish the atheism and Christianity threads, which was used in previous papers, uh, on interpretability. Hence we chose it. Um, you, what you see again is that there's many classifiers that have reasonable performance.

Speaker 3:          17:27          Um, and you see that the test performance varies quite a bit depending on the different choices of classifiers. Cause like sometimes they end up picking up weird confounds in the data. This, this data set has a lot of strange confounds that you can pick up on because sometimes the person posting their name actually be ends up being like a really key signal and on and all sorts of things. And the point here is that in this, so this is a much more real setting. You might not know what the right one is, but at least you can cut, you can skim through the factors that are being used and look at um, the prediction performances. And you can decide, you can say, you know, I really just don't like that classifier. I want a different one. And you can keep asking it to produce different ones.

Speaker 3:          18:08          So this is just an example of the sort of things that we do in our group where we're interested in creating these sorts of systems that assist in projects that are ultimately going to require a human. Because there is a fundamental in completeness in the problem specification. We know that the data that we have is not enough. The data combined with the problem specification that we have is not enough that a system can just solve this problem on its own, right. There's going to be, have to be some sort of interaction and because there's going to be some sort of interaction, we need to be able to provide a, what is it useful things for us to do. It's useful for us to provide explanation is useful for us to provide multiple explanations. Cool. So let's now go back to, um, the, the evaluation question. Yup.

Speaker 4:          18:56          First, with this work, have you looked at reading and stone bowls with these different,

Speaker 3:          19:02          so, so the question was, have we looked at creating ensembles of these networks? And one could certainly do, so I guess we've been in the mindset that some of these are going to be right for the wrong reasons and we just wouldn't want to use them at all because they're, they're going to pick up on a confounder. But certainly you could use this to create just more robust decision makers, like a way to propose an ensemble that you could then do, stir, do whatever you want it

Speaker 4:          19:25          are questions. Yup. Yeah. Any one of the interpreter.

Speaker 3:          19:36          So this is, we didn't do a user study, but just among ourselves. Um, there were definitely ones that made a lot of sense. I mean, so the first one is essentially picking out words like Christian and atheism. It seems very reasonable for the classification task at hand. Um, some of the later ones end up with names of people and we could look at those and say these are clearly not making sense because it was, they were picking up users and such like this

Speaker 4:          20:03          Qca or something. Same thing. There might be one in your room.

Speaker 3:          20:14          So, uh, the, the, the comment was some of the axes end up being interpretable and some of them may be less interpretable. Um, and so the, the, that, that's a fair point. And one thing that we're working on now is can we at least limit the number of things that are sensitive to so you can at least look at them and so not only give us multiple cloud qualitatively different classifiers, but can you produce a classifier that for any test point is going to give you a succinct explanation and that's something that we're actively working on right now and is relatively easy to build into the loss function because now you want to say that the input gradient should not be large but for more than a certain number of elements for every input and for different inputs. You may allow different things to be important, but for any particular input, we only want a few things to be important and this is something that you can again easily put into this framework.

Speaker 5:          21:06          I'm just trying to understand it. When you say you do a first pass, you find some classifier, this seems to work and you say, oh, I don't like that classifier. It's fight for the wrong reason. I'm just curious, does that mean it's not really right, like it's not predictive or it's predictive, but for some policy reason, I don't want to use it.

Speaker 3:          21:24          It's predictive, but first some policy reason I don't want to use it. And the one example could be that, let's say that it's picking up a confound in the data set. So there's this really popular example and the interpretability world, if this wolf where says Huskey the wolves are usually out in nature scenes. The huskies are usually like an indoor scenes and so you could build a classifier and say's if there's a tree in the picture, it's probably a wolf. And that's a classifier that on your data will do really well because your data's had happened to have this bias in it that you were not aware of. And so that's an example of where you might want to throw that away. And you might want to say that that's actually not the explanation that I want. I want a different one.

Speaker 5:          22:01          So in some sense, are you repairing the specification that you gave the AML system? You said optimize this, it did it did it correctly and now you're just saying, oh no, I didn't want to use that. I didn't properly specify it. I meant find wolves independent of whether or not there are trees.

Speaker 3:          22:17          Exactly. It's not so, so it, right. So you hit it right on the head. But I think the case where you need interpretability is where the problem, there's something wrong with the specification. Yup.

Speaker 5:          22:26          That was, and that's a question I'm trying to get to. I'm just trying to understand. So I understand that task, which is for policy reasons, I don't want to use that. Yes, that classifier. But now I'm trying to connect back to the title of the talk around interpretability, which is the classifier was doing its job and if I had to, if I just had to explain what it was doing, I could say, Oh yes and here's why I find that it used the tree. Yes. Right. Or you know, it used this blood test to decide this treatment or use this factor to decide the credit worthiness of this European Union resident or something. It was doing an accurate, it was solving the problem. I just, for a Po, I'm trying to decide which is it that we wanted select a different set of, of Mol Program is for, for policy reasons and which is it, we want to know what they're doing. So is it we want to understand them or is it we we only want to use certain types of,

Speaker 3:          23:11          right. So, so the focus of this talk is going to be around the, the, the definition and the evaluation of interpretability. And this, this, this is a short vignette to explain just, just to make concrete, like I gave a bunch of reasons why you want might want interpretability. So this is an example of a task where you wanted interpretability to be able to check whether the classifier was making its decisions for the right reasons and be able to reject it if it was making it for the wrong reasons. So this is just an example of a situation where you might want interpretability but there are many. And so we're going to move out. We're going to zoom back out into the general question. Uh, and this was just one specific example of a situation where you might want this

Speaker 4:          23:49          [inaudible].

Speaker 3:          23:51          All right. So, so the big question is like in this case, maybe you had someone in the loop, right? Maybe you had someone providing that feedback about, I like this explanation. I don't like this explanation. And you had a system, right? You, you were, that you were trying to build for certain policy related task, right? But, and so presumably if you were trying to do that, then you have a measure of how well it works because you have a task in mind, you have an expert in mind, that expert can check and you can can request additional solutions.

Speaker 4:          24:22          Okay.

Speaker 3:          24:22          But now let's think about it more broadly, right? What, how can you measure interpretability

Speaker 4:          24:29          okay.

Speaker 3:          24:29          In the APP, not necessarily any absence, but in the abstract, right? In the, in the more general sense. Uh, so one option is to say, well, interpretability it's like porn. You know it when you see it. Um, and so it, or this is a kind of related way of saying it, is that we have interpretability by Fiat, right? Or just going to call this thing interpretable cause we've just decided that this thing is interpretable. And so if we look at various, and these are like really good papers by the way, so I'm not making fun of any of them. But what I want to point out is that you often see statements in machine learning papers and I'm guilty of this as well. Um, uh, of the form, you know, certain type of model is a gold standard for intelligibility. Um, you know, the, our rationales are simply subsets of words.

Speaker 3:          25:10          You know, this is just saying like, clearly this thing is interpretable, right? These sentences are just telling you that of course this is the right way to solve this problem. And, and the thing that when we read this and it seems reasonable to us, there is a face validity to this because the whole point about interpretability that didn't make sense to human beings. We're all human beings. So if this explanation kind of makes sense to us, that is cool. Right? Um, but at the same time, um, it doesn't really let us go forward, right? Because it gives us a set of classes that are interpretable, but then then what do we do with that? Right? We can just optimize within that class.

Speaker 4:          25:42          Right?

Speaker 3:          25:43          Um, right. So this is what I just said. So we agree that this, you know, a jam is an interpretable model then than now the problem is done. The interpretability problem is sometimes it's done, all we have to do is optimized yams. Um, but we want to make sure that there's some evidence for these different forms of models actually being interpretable.

Speaker 3:          26:04          Another way of doing this is saying that interpretability has to be evaluated in the context of an event, uh, in terms of a real application. So this goes back to my story earlier that maybe you had this application in mind where you were trying to separate those news group posts or understand different tumor types, or in my case, um, predict meds for different patients. And if you're trying to do this task, you have a clear goal that you want to avoid confounds and interpretability has succeeded if it allows you to do the higher level task. Right. There was, there was some task in service of which you needed the interpretability and the, the HCI, the, the human factors. Communities have definitely embraced this sort of view. You know, the, the, the airplane controls are good if it allows the pilot to fly the plane safely, right?

Speaker 3:          26:54          That's your ultimate criterion. So this sort of criterion is useful because it's, it's a real evaluation. Our real problem, like no one's trying to like make up something and say clearly this form of model is interpretable. Um, you have hard evidence as to whether or not the, the model is actually interpretable. Um, but it may be costly, um, to perform those evaluations because you have to do it in the context of a real application. And you might be curious about whether or not to generalize is, for example, the human factors community in aerospace has spend a ton of effort trying to understand how flight controls should be set up, right? There's a lot of really great science behind that. But now if I'm trying to build some sort of other user interface, what can I learn about the airline flight controls? Save for a, a self drive or semi autonomous car. Right? It's not necessarily clear exactly how the transfer happens. Right. And that's the part that maybe we want to make a little bit more explicit.

Speaker 4:          27:55          Yeah.

Speaker 3:          27:57          So the question that we want to get to, um, is it, are there more general proxies? Um, and so, uh, things like sparsity are often brought up. People often cite this, um, seven plus or minus two rule that says like, how much can people hold in cognitive memory. Um, there, there may also be I ideas of monitor unicity that, oh, I can understand like something that if I turned the dial, like my accelerator, if I push on the accelerator, I know I always go faster and that, that kind of makes sense to me. And if I'm going up a hill or there's a bump in the road, maybe the, the rate of acceleration might be slightly different, but at least I kind of understand it and he pushed on the accelerator and then my car, it will move forward or increase in speed. So are there, are there fundamental notions here?

Speaker 3:          28:43          Um, and again, the nice thing about these sort of having these sort of fundamental notions is that as computer sciences, we can optimize these things, right? We, we love this because now it's like a mathematical function that we can work with. Um, but again, the issue is we need to make sure that it's evidence based that we need to be able to understand where is this assumption coming from? Or is it just something that's reasonable that looks reasonable to us or is it something deeper? And this is important. Um, not only just from a, like a philosophical, I care about science perspective, but I think it's also important for being able to make comparisons, right? Like if something is just a little bit sparser is it actually more useful for some downstream task?

Speaker 4:          29:27          Okay.

Speaker 3:          29:28          So what I'm going to propose here is a spectrum for evaluation. So I'm going to split it into two pieces, um, in terms of quantitative and qualitative forms of evaluation. And I think the qualitative part do that. Absolutely. Um, we eventually, I'll probably want to live in this space over here because then again, we can optimize it on a computer, but before we get to that stage, we need to think about the other elements. So there's a level of the real application, right? So that's the level that I talked about that again, you might be thinking of from a human factor standpoint or from a HCI standpoint and you can measure did we actually improve some patient outcomes? Or we can just ask scientists, you know, did you like this? Right? Was this helpful? Again, we're measuring something in the context of a real application.

Speaker 3:          30:16          Um, and then we can think of it one level more abstractly. We could say, um, it can people describe what factors should be changed in order to change the outcome, right. That seems like a reasonable sort of question. If the explanation I've provided you is a good one, then you might, you should be able to forward simulate what might happen with counterfactuals. And then you may also ask more qualitative things about the model itself. And then finally you have the level of what I'm calling proxies are things that we can think of in more very abstract terms like functions. Um, you know, are the feature sparse, does it look reasonable? And, uh, my, my claim is that we need to move in this direction of real stuff should be informing the, the functions that we are using for optimization. Um, and uh, keeping in mind that as we go up the chain there things are going to get more specific.

Speaker 3:          31:07          So I think that the top layer and the bottom layer are pretty clear to mo probably a pretty familiar comfortable to most people. You can think about optimizing for like l one regularly, like sparsity that's like really conceptually simple for us. Um, we can also think of measuring, you know, whether patient outcomes improved or not. And the, the part that I think is really understudied and I think that we should all be thinking about and investing in is really understanding the glue between those, which is the cognitive science layer. So I'm going to talk about, um, how we might think about this layer and some specific hypotheses that I think are useful for us to, to think about as a community in the space.

Speaker 3:          31:48          So in terms of a general form for human evaluation of explanation. So here we have, um, uh, this very specific question, right? Did we improve patient outcomes? Like what's the equivalent of that for just quality of explanation? And in this case we're not interested in whether the explanation is correct, right? So keep in mind that my explanation for like any prediction could be, um, you know, because because the sky is blue, right? And the thing is that explanation does not provide, is not predictive at all, but maybe it's easy, but it's still easy for you to understand, right? Like you, you understand what I'm saying when I say, because this, you know, I predicted this tumor type because the sky is blue, right? Like you, you can understand it. It's very understandable. And the reason why we're going to focus on understanding is that the accuracy part of an explanation is actually very easy to quantify that.

Speaker 3:          32:41          If I have a classifier that predicts malignant tumor every time the sky is blue, I can check that and see how good a classifier that is. Right? That's a quantitative thing that I can check and therefore we, it's not a complicated problem in terms of evaluations, very easy to do. The hard part is measuring whether the explanation is good in terms of does it allow it, does it convey some sort of understanding, right? This a human understand what the person is talking about. So in terms of a general form for a human evaluation of explanation, um, what we're proposing is being able to evaluate some sort of counterfactual. So if you ask the person, how do I need to change the inputs, right? What if I change the inputs, um, to change some predictions. So I have, maybe I have a specific data point. I want to change the output, right?

Speaker 3:          33:32          What inputs do I need to change to get the output to change? That might be a very general sort of question that I could ask. I could also ask about the internals of the system. I could say actually the input stays the same. Is there an element of the explanation that I would need to change in order to get the system to predict some different output? And finally I could say, uh, we could also think about this mismatch objective thing saying that it actually, what would I need to change in the criteria to be able to get a different output, uh, of interest? Right? So there's all sorts of permutations of this, but this gets to the fundamental idea that's very common. I mean, when you're in school, how do they test you on whether, you know, explanations work. It's like whether you can predict things, right?

Speaker 3:          34:17          You do a science experiment and they ask you now what's going to happen when you mix chemical a with chemical B? Can you tell me the formula that came out? Right? So we're essentially asking humans to be able to answer similar questions. But now instead of grading the humans, we're grading the system because if the human can answer the question that it means a system did a good job of explaining whatever it's trying to do. So now we can move on to some specific hypotheses. And again, I'm, I'm really curious to get people's feedback and input and I think it's really important for us as a community to be thinking about this layer of cognitive science, which is not really well thought through. Um, and, and so one hypothesis is that in Wa in terms of being able to generalize tasks, how certain covariates so we can think about where is the fundamental incompleteness and the problem.

Speaker 3:          35:05          Is it in the structure of the data, right? Are there just weird, um, confounds, um, eh, that may also be in the inputs as well. Is it in the reward functions? Is it in the internal features and maybe different, all of those cases might have different needs, right? It's important for us to think about the fact that interpretability might not be a one, there might not be one thing that is interpretable in all scenarios. And the form of the explanation you need may be different for different settings. Um, another important distinction is the scope. Like, are we trying to understand the full model? In many science cases we are, right? You have a big batch of data and I want to understand what does this tell me, this big batch of tea data. Tell me about disease progression and different subtypes and autism, right? I'm not interested in any particular patient.

Speaker 3:          35:53          I want to understand the overall trends. So there's that level. I'm trying to understand explanations for the whole system. On the other hand, if I need to make a decision for a specific patient and say, do I treat this patient with this drug, then the form of explanation I need may be very different. And in particular the need of how quickly the explanation must be utilized as could be quite different in these different scenarios. I might be willing to spend a couple of hours looking at the output of some global science discovery sort of system where it's like if there's something that needs to be made, a decision that needs to be made at the bedside, maybe you only have on the order of minutes, uh, or, or less, two to properly understand it. So this seems all very reasonable. And what I, uh, again I said this talk is going to be a lot of philosophy. I'm encouraging people to maybe think about that as you're building your systems and designing your systems and thinking about what are the natural ways in which you can bend these systems and think about the different needs that are associated with them.

Speaker 3:          36:52          The other hypothesis that I like to float out there, um, and how people consider, so a little more abstract is that explanation actually has two components of it that we maybe can think about distinctly indistinct in, uh, in separately. So the first component,

Speaker 4:          37:10          mmm,

Speaker 3:          37:10          it's pretty light, uh, is structure. So if we think about agents' actions and outcomes, uh, maybe we have some explanation that say's that, um, here a goes to B goes to c, right? That's the form of my explanation, my explanation to, and did ground that into a specific example. Maybe you give a vasopressor it constricts blood vessels and increases blood pressure. So if I'm trying to increase blood pressure and the system says give vasopressors, this might be the explanation that comes out, right? And that explanation has a certain structure which is a to B to c.

Speaker 4:          37:46          Yeah.

Speaker 3:          37:46          And it might have all sorts of different forms and different structures here, maybe more or less understandable, right? Here's where we might run into issues of like if too many factors are involved in this explanation, we just can't follow it, right? We can do those counterfactual reasoning. Like if there was some other factor going in here, maybe we wouldn't be able to do the reasoning about what would happen if something else changed and something else changed, et Cetera. Maybe he's going to get too hard. And maybe that's a fundamental quantity about humans, right? That there are certain types of these structures that we can easily handle. And there's other structures that we can't handle. So if, if we're in that regime, we can think abstractly in terms of like a to B to c,

Speaker 4:          38:28          right?

Speaker 3:          38:29          There's another key component and that other key component is thinking about the cognitive chunks. So that what, when we ground each of these variables and like a specific concept, and here it's important to remember. So there again this, this offsite is seven plus or minus two rule. It's not seven plus or minus two. Well, some, well actually seven plus or minus two. What is the question? So if I give you, um, the, the acronym Caia, is that one thing to you or is that three letters that you know, you have to remember? It's probably one thing because you have a concept for that. It's the single item. And so one thing that's really important in these very large data sets is first identifying what are the cognitive chunks. So clinicians might have a word for a collection of diseases. We work with billing codes a lot and billing codes are like insanely specific.

Speaker 3:          39:25          And so you may how 25 or 30 codes that exact correspond to epilepsy. And so if the clinician has a word for epilepsy, so, so an explanation does not need to, we don't need a separate a and a B and a c out to some large fraction for all of the different forms of epilepsy. We could collapse it into one thing, right? Similarly, um, we may have situations where certain relationships might be really well known. For example, a vasopressor I'm constricting blood flow blood vessels that that might be known science. Right. So, so the, the question was, um, if you end up providing a more succinct explanation, um, but there were multiple fat that only pulls out the biggest factors, what about all the multiple other tiny factors that that may also be affecting the explanation and do you have an ethical reason to expose them?

Speaker 3:          40:15          I think that's very problem dependent. For example, if you just want to be able to overall be able to say like if changing none of those factors is going to change the outcome, uh, the, the final prediction, maybe it's safe to ignore them. If those final, if those other factors can co-vary as a chunk, then maybe it's important to expose that to say that actually if some of this larger cognitive item changed or this larger correlated item changed, then actually the output could be different. Um, I think that comes back to this accuracy tradeoff that I, that I mostly said I'm not going to worry about, but it's this question of any form of explanation is not going to be perfectly faithful to your model. Assuming that your model is some really large complicated system. And so that's going to have to be another user defined threshold of like how, how much accuracy am I willing to sacrifice.

Speaker 3:          41:06          Um, and you, if you are not willing to sacrifice a lot of accuracy and your explanations, um, you still need your explanations, maybe they're going to suffer in quality in the sense that maybe you are going to have to include more things. So all of this stuff I see in the setting where you have a threshold on accuracy and you say the explanation actually has, has to be at least this good. And given that the explanation is this good, how can we limit, uh, how can we make it more of higher quality? So maybe the case that certain instructors are easier for people to understand. Um, and it may be the case that like a can be grounded in something that actually consists of a lot of items and maybe even this link from a to B. If we are finding that it takes people a lot of time to traverse things of a certain length, but maybe it takes people less time to traverse things, have a shorter length, uh, or, or sorry, the of things that they know that maybe that effectively reduces the length, right?

Speaker 3:          42:06          So here, these are hypotheses to keep in mind and as you're building your systems, one thing you can think about is like, since interpretability is such a human concept, right, we can't really think about it without thinking about what is the language of the human. I think that's maybe the, another key idea to keep in mind is that we need to have like basically a dictionary of what is the language of the human to be thinking about this. All right. Um, so I'm going to say mostly rapid there. Um, uh, I'm the investigating these hypotheses and these sorts of questions I think are really important because they're going to lead us to understand, um, are there model classes? It seemed to work well for certain settings. So that was the first hypothesis was a task Atherton and covariates, right? So those are our settings. And then are there model classes or regularizes and maybe that has to do with the forms of the structures that we consider.

Speaker 3:          43:01          Okay. Or not okay, um, that are best for certain settings. And these are a set of experiments that we could do that we should do, that we should be supporting each other as a community, um, to do. Um, because I think that until we really get a solid sense of like the cognitive science of explanation as it relates to really specific machine learning tasks, we're not going to be able to make comparisons between different methods very rigorously. And we also may end up just optimizing the wrong things, right? You may end up optimizing for some form of, uh, some function and find that it's not actually the function that you needed to, to do the task at hand. Um, so in terms of the big takeaways that I want us to, to be thinking about, and again, leaving plenty of time for discussion, um, the first is that when do we need explanations?

Speaker 3:          43:56          So we need explanations when there's a fundamental incompleteness and the problem specification. And that's, and, and it's important to be honest about that because there's many cases where that just doesn't happen, right? Like if the system is operating independently, does not need any humans, the problem as well as specified as well as it can be, then it's great to just throw like a super predictive, deep model at it. And that's the best you can do. Right? And it will and that that's what you should do. Um, so thinking about when do you actually need explanation? Um, and also this point, there's a spectrum of valid evaluation going from just like, you know, what's my right, what's my l one sparsity score to, uh, uh, how well does it perform in a particular embedded task setting? Um, and to do this properly, we're going to need more connections it to cognitive science.

Speaker 3:          44:48          And my claim is that a relatively modest effort in terms of doing some of these experiments could have huge payoffs because maybe we're optimizing too much for the wrong things. Where it, you, if you have papers that say, you know, I can get something that's five sparse and somebody else can get something that's forced bars, does it, is it actually help? Right? Like, it was worth, it was that extra effort worth it. Um, and maybe there's something completely different that we should be focusing on that we're missing out on because we haven't thought about it. Um, so the, the main thing that I want to encourage us all to do is let's be rigorous about this question of interpretability that there's ton of research going on in this area. Um, there's not really consensus on how to evaluate it and how to think about it. But as we go forward, we're going to need to be able to make decisions about which system is better for a particular task and when, when have we reached satisficing cause interpretability I think is definitely the sort of thing where you reach a level where people can understand it and once people can understand it, then in some instances your task is done because you've managed to communicate. It's a cool, so I'll leave it at that. Um, I'd love to hear questions, opinions and discussion.

Speaker 6:          46:09          Thank you very much. That was wonderful talk. I think the connections with Congress slants or just very fascinating and raise lots of questions have one that can point. Mine was we know that people are very sensitive to framing effects. The same information presented two different ways. It can be very different in how understandable they are. How do we control for that when we're evaluating different model classes? And does there need to be a, you know, a little subfield of this or like, you know, let's try to find the most best framing for, you know, these, these narrow networks or models. Okay.

Speaker 3:          46:40          I think that's another really important question. And so I didn't really get into the, like how do you set up the system that provides the explanation and just also just the more general Ui issue of how, how you present this. I envisioned it, or like early experiments can probably be done in sufficiently abstract settings that you can try to control for as much of these effects as possible. But certainly in a real situation that's going to be another access that needs to be carefully discovered.

Speaker 7:          47:09          So it seems that there is a inverse relationship between, uh, the performance of a model and the interpretability of that model. So for example, logistic regression is extremely simple. Uh, doesn't often perform that well. It does well, but, um, but most importantly it's extremely interpretable because each of the coefficients in the model, you can say, okay, this is a, this is a big positive effect, but this is a big negative effect. Uh, whereas more complicated model is deep model is they tend to be very hard to interpret. Uh, do you think that inverse relationship is inevitable? Is, is that, is there a law saying that the better model performs the more unintelligible it becomes or you know, is there some way or you know, around that?

Speaker 3:          47:53          I actually, I don't agree with that. Um, there's so Cynthia Ruden quotes this paper, which I'm forgetting, um, that let's sees it many times there's many models that perform reasonably well, um, that, that oftentimes that you can find multiple models. And certainly our anecdotal evidence with like training deep models is that you can get multiple deep models that produce similar predictive scores. And the question is, can you make some of those more interpretable than others by say involving fewer dimensions? So I would act. And I also pushed back on the fact that like it's something like logistic regression interpreter wall because if you've got 100,000 dimensions in your logistic regression and they are covariate and all sorts of funny ways that you know, the top leader things might mean nothing, right? And, and so I actually don't agree with the fact that it's simpler models, simpler models like linear model, like a linear model or generalized linear model are actually more interpretable than deep models.

Speaker 3:          48:46          And once you're in high dimensions, and one thing that, one direction that I'm really excited about in terms of avenues is this notion of local interpretability is that is for any particular explanation or instance input. Can you provide me an explanation that is succinct? And I think there, there is hope even with deep models because you can, um, because it also with deep models you can pick out intermediate layers, right? You can think about the cognitive chunk. So if you can identify a layer in the network that largely activates when there's a cat in the picture, then you can say that the reason why I've done this classification, um, for, for this being like the wizard of Oz as the, or, you know, a scene from wizard of Oz is because I see the lion here and at like I've picked out the lion in the picture, right?

Speaker 3:          49:33          Same thing with two things that we have going for us is that we only, we may only need to explain one instance at a time and that we have access to a lot. We're kind of assuming that we have access to a library of cognitive concepts. And I think so there's, there's absolutely hope for being able to explain deep models or finding models. Um, it such that these properties are also true and you don't have to sacrifice on prediction. You might also argue that if the model is getting you tons more predictive accuracy in a certain situation, like I can't think of situations where that would happen where you're not maybe picking up like a weird confound, right? Like, maybe you're at, you're getting a load more accuracy, but it's because you picked out some weird correlation that maybe maybe it's not true. Yeah. So it would seem like there's a difference between experts and non experts in terms of how explainable something is where like the person who's flying the plane every day knows exactly where to look and what they're looking for and they want to make the decision really quickly.

Speaker 3:          50:27          Do you think that if the distinction expert non-expert is enough or do we need like gradations are experts in different fields in terms of understanding how these proxies would generalize to other fields or other situations? I think that's a great question. And one of the thing, so this is a hypothesis and empirical thing that needs to be tested. And I should say this is all in discussion with being um, Jesse Johnson here. Sam Grossman. I'm at Harvard where we've been talking about like experiments and hypotheses. We believe we are a hypothesis that we think is testable is that there are still fundamental aspects of like control processes that humans can understand in terms of this uh, chains of causation or agents and actors and that part and the difference, the hypothesis is a difference between experts and non experts. My is not necessarily that the expert can hold more in memory or a whole different structures and memory, although certainly people get trained to do that. So that's the thing that we could, we should be absolutely aware of but we're curious how far you can go with this hypothesis that maybe experts just have different chunks. So we figured out like a general structures that are generally explainable and then we can sub in chunks that experts have, cause the doctor might have a name for something or a way of understanding a particular device that somebody else doesn't have access to. And can we plug that in and it's an open question.

Speaker 8:          51:53          Squeeze in one last question. I wonder if there are fundamental, the fundamental differences in the area of interpretability between domains that are deeply studied like healthcare and domains that are brand new, like how to maximize clicks on ads, um, that the, um, the existence of a deep spectrum of human expertise on the one hand, um, puts very different pressures on our machine learning systems in that context.

Speaker 3:          52:20          It just to make sure I understand. So you're saying that there are some cases where there's a lot of domain expertise, um, and there's cases where there is less domain expertise in terms of what the explanation would provide?

Speaker 8:          52:31          Yeah, absolutely. That you buy, you might observe that a lot of the most interesting applications of machine learning recently have been in areas that are not that deeply studied. Whereas we've, we've struggled to have equal impact in healthcare, uh, maybe because of the amount of, of, of learning that's already happened.

Speaker 3:          52:53          So I, I think to, so there's the, there's potentially the claim that maybe there's a lot of learning that has already happened in healthcare, so maybe there's less to advance on. I think it actually does come down to this incompleteness and problems specification that makes it a lot of these real tasks, harder real world task harder than tasks that we have created like somehow human created. So like the click question is something that's very closed in the form of we have a system we can measure whether something changes, click rates are not. And so there's a way in which the problems and so to the extent that you defined the problem as we want to increase certain rates, right? It's very measurable and it can be done in a closed loop fashion. And I'm not sure if it requires interpretability if you're asking the question more broadly, if like uptake of machine learning or like value provided by machine learning in certain settings. And I think it is entirely fair to say that the value that machine learning is added incentive settings where um, the, the problem is not fully specified. And where there is this incompleteness where you, you're measuring things in the EHR and the EHR, super messy, um, has not necessarily been there because we haven't really figured out how to communicate and measure

Speaker 2:          54:09          in those systems. Thank you.