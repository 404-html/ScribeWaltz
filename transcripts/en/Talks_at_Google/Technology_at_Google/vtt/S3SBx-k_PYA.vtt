WEBVTT

1
00:00:06.140 --> 00:00:09.410
Thank you all for coming to our latest NASA Google talk.
I'm very,

2
00:00:09.411 --> 00:00:14.270
very pleased to welcome Keon Brad who is going to talk today about reducing the

3
00:00:14.271 --> 00:00:18.530
cost of impact her VNV for flight critical systems.
As you can see,

4
00:00:19.100 --> 00:00:22.370
the mouthfeel and it is,
it is.
Um,
so it was your bio.

5
00:00:23.220 --> 00:00:26.540
So Keon comes Ross from ut Austin where he has his phd.

6
00:00:26.750 --> 00:00:30.770
He is the area lead for robust software engineering in the intelligence systems

7
00:00:30.771 --> 00:00:31.604
division.

8
00:00:31.910 --> 00:00:35.360
He's also a technical lead for autonomous system assurance in the safe and

9
00:00:35.361 --> 00:00:39.800
autonomous systems operation project and for air traffic and vehicle software

10
00:00:39.801 --> 00:00:43.780
and system assurance research.
Thanks so much for your scale.

11
00:00:47.720 --> 00:00:48.620
<v 0>Thanks for coming.
So</v>

12
00:00:49.190 --> 00:00:51.330
<v 2>I mean I wasn't expecting that many people,</v>

13
00:00:51.810 --> 00:00:55.410
especially if we're talking about VNV usually done doesn't draw a crowd.

14
00:00:57.210 --> 00:01:00.360
Um,
so as Mary said,
I'm,
I'm,

15
00:01:00.420 --> 00:01:05.420
I'm leading a team at NASA ames that's about between 30 and 40 researchers and

16
00:01:07.861 --> 00:01:09.900
developers.
We have about 20,

17
00:01:09.901 --> 00:01:14.901
25 researchers that do mostly research on formal methods and their application

18
00:01:16.890 --> 00:01:21.890
to software verification and about between 10 to 15 engineers,

19
00:01:22.081 --> 00:01:26.460
depending on the time that are really focusing on developing.

20
00:01:26.850 --> 00:01:28.470
What do we call it?
Flight critical systems.

21
00:01:28.471 --> 00:01:32.880
So those could be a controlled system.
So spacecrafts mostly,

22
00:01:33.230 --> 00:01:38.110
uh,
on the space side,
the research I'll talk about is mostly,
uh,
uh,

23
00:01:38.160 --> 00:01:42.420
done on aeronautics.
So it's not so much applied anymore.

24
00:01:42.450 --> 00:01:46.260
On the space side,
we'll try to infuse in on the space side,

25
00:01:46.261 --> 00:01:50.850
but it's really targeting at a civil aviation.
Okay.

26
00:01:51.570 --> 00:01:55.200
Basically about six years ago,
um,

27
00:01:55.430 --> 00:01:59.400
and as I was told by,
uh,
by,
uh,
OMB,

28
00:01:59.730 --> 00:02:04.200
the White House to start addressing the cost,
what they see as the cost,

29
00:02:04.201 --> 00:02:07.410
the high cost of VNV in the civil aviation.

30
00:02:07.800 --> 00:02:12.800
So basically that's the manufacturers like Boeing and Breyer.

31
00:02:13.650 --> 00:02:18.100
I know those guys and the Oems,
and I'm working as a subcontractor to those.

32
00:02:18.310 --> 00:02:23.310
We are complaining that the VNV for software is becoming more and more of a cost

33
00:02:24.061 --> 00:02:28.340
for them and basically heating their lunch.
Um,

34
00:02:28.470 --> 00:02:32.790
so they wanted somebody to address it and basically that man that was passed

35
00:02:32.791 --> 00:02:36.400
onto us at NASA to try to address it.
Um,

36
00:02:36.510 --> 00:02:40.690
so we set up for a,
it was backed up by,
uh,

37
00:02:40.770 --> 00:02:44.130
several surveys that have been done by the dod,

38
00:02:44.131 --> 00:02:48.530
by external a Sayville,
uh,
agencies.
Uh,

39
00:02:48.540 --> 00:02:53.430
you can see at the top there is the [inaudible] survey for Saving Civil Arrow

40
00:02:53.790 --> 00:02:56.630
and it's pointing out to the fact that,
uh,
uh,

41
00:02:56.710 --> 00:02:59.800
uni new methods and models for assuring safety.

42
00:03:00.130 --> 00:03:04.060
So when we talk about VNV safety is the most,

43
00:03:04.240 --> 00:03:08.290
is the biggest target for us at NASA.
Okay.
Um,

44
00:03:08.360 --> 00:03:12.430
then there was the integrated plan for the implementation of next Gen,

45
00:03:12.670 --> 00:03:16.240
next gen stand for the next generation of fair traffic systems.

46
00:03:17.150 --> 00:03:20.730
And they also pointed out that there was a lack in Vnv,

47
00:03:20.870 --> 00:03:25.380
a lack of methods to really address those types of systems.
Um,
the,

48
00:03:25.381 --> 00:03:28.390
the air force did also,
um,

49
00:03:28.420 --> 00:03:32.530
their study and they actually identified the same need.

50
00:03:32.800 --> 00:03:37.000
There's not enough VNV,
uh,
techniques that are available to,

51
00:03:37.390 --> 00:03:41.170
uh,
to very,
very fine the type of systems that we're building.

52
00:03:41.530 --> 00:03:45.580
So we actually,
since then,
we've been kind of in lock step with them.

53
00:03:45.880 --> 00:03:50.590
The research was doing is we are doing is feeding into the program.

54
00:03:51.040 --> 00:03:55.630
Okay.
So,
as I mentioned a bit earlier,

55
00:03:56.110 --> 00:03:59.950
so they,
that was passed on to us by OMB and the White House Congress,

56
00:03:59.951 --> 00:04:03.370
all those big,
you know,
uh,
governmental agencies.

57
00:04:03.640 --> 00:04:08.560
But really what's happening is behind the scenes you have the CEOs of companies

58
00:04:08.561 --> 00:04:09.850
like Boeing,
Rockwell,

59
00:04:09.851 --> 00:04:14.851
Honeywell going to really say going to achieve an RMD and says,

60
00:04:17.740 --> 00:04:19.030
you know,
it really,

61
00:04:19.031 --> 00:04:22.930
it really is costing us way too much money to do the envy.

62
00:04:23.470 --> 00:04:28.470
And the specialty VNV for software and software is becoming an increasing a

63
00:04:28.781 --> 00:04:32.590
larger part of the cost of developing a plane.

64
00:04:33.040 --> 00:04:37.450
So what you're seeing here,
a pie chart that tells you pretty much,

65
00:04:37.930 --> 00:04:38.520
um,

66
00:04:38.520 --> 00:04:43.520
how much it costs to develop a new plane that's going from a major manufacturer,

67
00:04:44.261 --> 00:04:49.120
the biggest manufacturer of airplanes in this country,
which I won't name,

68
00:04:49.150 --> 00:04:51.380
but you can just,
uh,

69
00:04:51.460 --> 00:04:55.660
and if you see software is taking software and software,

70
00:04:55.661 --> 00:05:00.280
VNV is taking him like almost 70% of the cost.
Now,

71
00:05:00.910 --> 00:05:04.810
the part about,
you know,
the design of the plan and those kinds of things,

72
00:05:05.230 --> 00:05:09.520
it's 30% these days.
I explained to you,
if you want to quit Google,

73
00:05:09.521 --> 00:05:11.800
you can get a job in the aviation industry.

74
00:05:11.801 --> 00:05:16.030
They are hiring a lot of software engineers these days.
Okay.

75
00:05:16.510 --> 00:05:19.540
You just have,
Java is not the language that they will use.
But

76
00:05:22.690 --> 00:05:26.980
so,
um,
so really what we've been asked to address is that cost,

77
00:05:26.981 --> 00:05:29.710
especially for the,
what we call the flight critical systems.

78
00:05:29.710 --> 00:05:33.460
So that includes the flight management system,
the flight control system,

79
00:05:34.120 --> 00:05:39.120
the software that controlling engines and all the system integration on the

80
00:05:40.151 --> 00:05:43.450
plane.
Right?
Because,
uh,

81
00:05:43.900 --> 00:05:48.470
that part of the system integration is important to consider because,
uh,

82
00:05:48.820 --> 00:05:50.860
before Boeing for example,

83
00:05:50.861 --> 00:05:54.460
was building everything end to end.

84
00:05:54.820 --> 00:05:59.320
Nowadays they rely on,
on a lot of subcontractors and,

85
00:05:59.390 --> 00:06:03.590
and did the integrating a lot of cuts in their systems.

86
00:06:04.070 --> 00:06:08.540
So this is a new business for them and they don't know quite how to deal with it

87
00:06:08.600 --> 00:06:10.040
in terms of,
uh,
of VNB.

88
00:06:11.750 --> 00:06:16.100
So what drives the cost of VNV is mostly driven by,

89
00:06:16.370 --> 00:06:18.980
uh,
this,
uh,
certification standard.

90
00:06:18.981 --> 00:06:23.750
That software has to go through four airplane,
which is called [inaudible] 78.

91
00:06:24.100 --> 00:06:26.590
Um,
we add the,
uh,

92
00:06:27.020 --> 00:06:30.070
third interration of Don 78.
Now it's d one,

93
00:06:30.071 --> 00:06:35.071
one 78 C M and a deal once every ta B was all about doing the VNV through

94
00:06:38.361 --> 00:06:43.361
testing deal when 78 c now now allows you to take advantage of what's called

95
00:06:44.181 --> 00:06:49.181
formal methods and model based development to actually do some of the VNV

96
00:06:49.401 --> 00:06:53.680
upfront.
Okay.
So,
um,

97
00:06:55.370 --> 00:06:59.650
we are in 2010 we started to build a,
uh,

98
00:06:59.800 --> 00:07:03.750
a program that we're supposed to address it.
There's the software aspect,

99
00:07:03.751 --> 00:07:05.060
but there are many aspects,

100
00:07:05.270 --> 00:07:09.620
especially when you want to take into account also what's happening in terms of

101
00:07:09.710 --> 00:07:14.240
computer systems in their air traffic management part of this equation.

102
00:07:14.660 --> 00:07:18.350
So we had four foundational research elements.

103
00:07:19.680 --> 00:07:22.830
I'm going to start with the top left and this one,

104
00:07:22.831 --> 00:07:25.190
it's got argument based safety assurance.

105
00:07:25.670 --> 00:07:28.190
So we were in that one.

106
00:07:28.250 --> 00:07:33.250
We're looking at different ways of doing certification than deal one 78.

107
00:07:35.100 --> 00:07:39.650
Um,
if you're talking about applying for more methods,
the um,
it's,

108
00:07:39.680 --> 00:07:42.140
it's really formal methods.
When they're applied,

109
00:07:42.141 --> 00:07:47.141
they're retiring a specific risks and you software deal one 78 is all about

110
00:07:49.280 --> 00:07:53.960
building quality in new software rather than addressing a specific risk.

111
00:07:54.260 --> 00:07:55.093
Okay.

112
00:07:55.100 --> 00:07:59.240
So we want it to change that and it's a move that has been done in Europe or at

113
00:07:59.241 --> 00:08:02.360
least parts of Europe.
The UK in the UK,

114
00:08:02.361 --> 00:08:06.410
they're using a safety cases,
which are really argument based.

115
00:08:06.411 --> 00:08:08.760
They're addressing specific risks,
um,

116
00:08:08.960 --> 00:08:12.470
for almost all the defense contracts nowadays.
Okay.

117
00:08:13.250 --> 00:08:16.790
It's not used so much in the u s uh,

118
00:08:17.000 --> 00:08:22.000
except for when you want to introduce what they call an experimental plane.

119
00:08:23.420 --> 00:08:27.740
And this is where you guys might be a bit interested because a drone,

120
00:08:27.770 --> 00:08:32.000
for example,
a UAV is an experiment,
experimental plane.

121
00:08:32.300 --> 00:08:34.550
There is no really a,
a,

122
00:08:34.850 --> 00:08:39.640
a good guideline on how you go about doing the assurance of,
uh,

123
00:08:39.641 --> 00:08:43.140
of Jones these days.
So,
uh,
right now,

124
00:08:43.141 --> 00:08:45.850
DFA is requiring a safety case to,

125
00:08:45.980 --> 00:08:50.390
to be able to operate any a UABs.
Okay.
Um,

126
00:08:52.010 --> 00:08:56.640
now did the other four elements,
I'll,
I'll about those two together.

127
00:08:57.050 --> 00:08:57.780
Um,

128
00:08:57.780 --> 00:09:02.010
all those planes are parading and what we call the nest and national air space.

129
00:09:02.220 --> 00:09:03.310
Uh,
for,
uh,

130
00:09:03.360 --> 00:09:07.800
for the u s so think of it as the whole air traffic system within the US.

131
00:09:08.420 --> 00:09:10.500
Um,
when we were got that mandate,

132
00:09:10.830 --> 00:09:15.830
it was saying that the traffic would be basically multiplied by three by a

133
00:09:16.231 --> 00:09:18.930
factor of three within five years.

134
00:09:19.180 --> 00:09:21.900
They since then back down from that,

135
00:09:22.230 --> 00:09:25.290
but there's still a forecasting,

136
00:09:25.291 --> 00:09:28.950
a huge increase in traffic,
which means more density,

137
00:09:29.250 --> 00:09:32.920
which means the safety margins are going to be reduced a lot in a,

138
00:09:32.960 --> 00:09:33.793
in your traffic.

139
00:09:34.140 --> 00:09:38.940
So they wanted us to also look at that aspect of civil aviation.

140
00:09:39.930 --> 00:09:43.590
Um,
and if you look at the Nas and the way it's operating,

141
00:09:43.650 --> 00:09:47.430
it's basically a huge distributed system.

142
00:09:47.610 --> 00:09:50.790
It's a huge distribution system where you have automation,

143
00:09:51.210 --> 00:09:54.270
you have computers interacting with all kinds of humans.

144
00:09:54.271 --> 00:09:58.320
That includes controllers,
pilots,
um,

145
00:09:58.500 --> 00:10:01.620
the airline management operation center.

146
00:10:01.950 --> 00:10:06.950
So it's really a mix of computer systems and humans interacting with all the

147
00:10:09.061 --> 00:10:11.670
confusion that can arise from that mix.

148
00:10:12.510 --> 00:10:17.510
And we decided that the human element was so important that we pulled it in a

149
00:10:17.941 --> 00:10:20.850
separate element that we call authority and autonomy.

150
00:10:21.480 --> 00:10:26.040
So the attorney part is about increasing automation for those,
uh,

151
00:10:26.190 --> 00:10:27.390
air traffic systems.

152
00:10:27.840 --> 00:10:31.860
And the authority is as we introduce more and more automation,

153
00:10:31.861 --> 00:10:36.600
more and more autonomy,
we transforming the role of the human in that system.

154
00:10:37.170 --> 00:10:40.320
And the fundamental question in the end is woo,

155
00:10:40.380 --> 00:10:42.480
as the authority to make the decision.

156
00:10:43.110 --> 00:10:47.730
Do we leave it to the machine or do we still have the human that pushes a button

157
00:10:47.731 --> 00:10:50.670
and says,
okay,
go,
the machine was right.

158
00:10:50.700 --> 00:10:54.990
This is what we should be doing.
Okay.
Um,

159
00:10:55.410 --> 00:10:58.740
so,
uh,
and,
and the distributed same stem part,

160
00:10:58.741 --> 00:11:03.741
we kept focusing mostly on the complexity that's increasing within aircraft's.

161
00:11:06.240 --> 00:11:10.170
Aircraft's I've gone through a change.
Uh,
they,

162
00:11:10.190 --> 00:11:15.190
this used to be when they used to use what we call the federated architecture,

163
00:11:16.080 --> 00:11:20.040
meaning by,
you had one box per function for the software,

164
00:11:20.640 --> 00:11:25.640
which means that everything was well separated and there was a move to what's

165
00:11:25.651 --> 00:11:28.260
called now,
uh,
an Ima architecture,

166
00:11:29.190 --> 00:11:33.030
which is going stands for integrated modular avionics,

167
00:11:33.690 --> 00:11:38.690
which actually is a way of putting everything on the same boxes.

168
00:11:39.390 --> 00:11:41.550
But now the FAA has to,

169
00:11:41.580 --> 00:11:45.630
requires you to show that those functions are residing on the same physical

170
00:11:45.631 --> 00:11:48.540
boxes are separated from each others,

171
00:11:48.990 --> 00:11:53.920
either in terms of the memory they access or in of time.

172
00:11:53.921 --> 00:11:56.110
They're running at different times and they can interact.

173
00:11:56.410 --> 00:11:58.780
They can not interfere with each other's.
And that's,

174
00:11:58.840 --> 00:12:03.280
that's a lot trickier in terms of verification,
this type of,
uh,
architecture.

175
00:12:04.240 --> 00:12:05.020
Um,

176
00:12:05.020 --> 00:12:10.020
so they also have a tendency to put a lot more thing on the,

177
00:12:10.180 --> 00:12:11.110
on the plane.

178
00:12:11.440 --> 00:12:15.280
It used to be that most of the decisions were done on the ground and then you

179
00:12:15.281 --> 00:12:19.690
upload the,
you know,
the,
the sequence of factions to the pilot.

180
00:12:19.691 --> 00:12:21.970
You tell him them what you do nowadays,

181
00:12:21.971 --> 00:12:25.420
there's a lot of functions are Mig migrating from ground to air born.

182
00:12:25.750 --> 00:12:28.750
And that means the software on board in the end plane is a,

183
00:12:28.751 --> 00:12:32.350
is a lot more complicated.
Um,
that goes beyond that.

184
00:12:32.351 --> 00:12:35.500
If you look at the Boeing 787,
um,

185
00:12:36.280 --> 00:12:37.840
it has like a,

186
00:12:37.870 --> 00:12:42.870
something that made everybody nervous because they went from having two or three

187
00:12:42.911 --> 00:12:47.560
physical networks that were completely physically separated on the plane to one

188
00:12:47.561 --> 00:12:51.490
physical network where the,
the separation between the,

189
00:12:51.640 --> 00:12:55.150
the functions are controlling the airplane and the functions are controlling.

190
00:12:55.151 --> 00:12:59.290
Your entertainment system aren't using the same bus,

191
00:12:59.291 --> 00:13:03.880
but they basically a walled off by uh,
virtual,

192
00:13:04.210 --> 00:13:06.370
uh,
uh,
mechanisms.

193
00:13:06.730 --> 00:13:10.900
So that made DFA extremely nervous because they think you can hack and then

194
00:13:10.901 --> 00:13:12.300
crossover.
Um,

195
00:13:12.550 --> 00:13:17.550
and there is a famous guy that keeps sending that he's hacked into the control

196
00:13:18.041 --> 00:13:22.690
system for several United flights and kind of,
uh,
frankly it's a myth,

197
00:13:25.030 --> 00:13:28.540
but that makes the FAA nervous and they want better,

198
00:13:28.690 --> 00:13:32.440
better verification techniques to address those things.
Okay.

199
00:13:33.010 --> 00:13:34.390
So that's the setup one.

200
00:13:34.420 --> 00:13:38.290
The part I'm going to con concentrate on is the software intensive part.

201
00:13:38.830 --> 00:13:43.510
So really what are we doing about the software intensive system and mostly the

202
00:13:43.511 --> 00:13:47.800
part that's about airborne systems mean mean all the software that's on board

203
00:13:47.801 --> 00:13:49.360
the aircraft.
Okay.

204
00:13:50.770 --> 00:13:53.770
And hopefully by the end I'll make the,

205
00:13:54.100 --> 00:13:57.070
I'll make the transition to the new,
uh,

206
00:13:57.790 --> 00:14:01.710
things we suppose to address,
which is autonomy,
complex system.

207
00:14:02.710 --> 00:14:04.180
Okay.
So

208
00:14:06.130 --> 00:14:08.560
most people say VNV costs too much money.

209
00:14:08.570 --> 00:14:12.430
Actually that's what the manufacturers I telling or management,

210
00:14:12.790 --> 00:14:14.950
it's not really VNV let's be clear.

211
00:14:15.370 --> 00:14:20.370
It's not a VNV per se or doing the Vegan v that's costing you a lot of money.

212
00:14:22.150 --> 00:14:23.680
That's where you,
you're doing it.

213
00:14:24.220 --> 00:14:27.850
So if you look at the v Diagrams and um,
the,

214
00:14:28.450 --> 00:14:31.280
the phases of development are using in a,

215
00:14:31.370 --> 00:14:34.590
in a deviation industry goes for requirements,

216
00:14:34.630 --> 00:14:38.710
very careful requirement developments,
then design,
then coding,

217
00:14:38.890 --> 00:14:42.550
then you move on to testing,
you know,
unit testing,
integration testing,

218
00:14:42.551 --> 00:14:44.590
acceptance testing.
Finally,

219
00:14:44.591 --> 00:14:49.591
you deploy very long cycles and all the VNV and traditional innovation is done

220
00:14:51.741 --> 00:14:54.800
here on the right side,
on the testing part.

221
00:14:55.520 --> 00:14:59.360
So the problem with this is that not only it costs a little more to do it there,

222
00:14:59.690 --> 00:15:04.690
but all the faults I found here when actually the falls are being introduced way

223
00:15:06.291 --> 00:15:08.390
before.
So when you do this,

224
00:15:08.391 --> 00:15:12.050
that means that every time you want to fix a bug that you found,

225
00:15:12.380 --> 00:15:15.980
you have to go back to some of those earlier phases.

226
00:15:15.981 --> 00:15:19.280
If you go back only to coding but it doesn't cost you that much.

227
00:15:19.460 --> 00:15:21.500
If you go back to design or requirements,

228
00:15:21.860 --> 00:15:26.860
then that goes to a lot of money and it's usually where the cost is coming from

229
00:15:27.290 --> 00:15:31.490
and a in the additional costs that's coming from in Vietnamese.

230
00:15:32.720 --> 00:15:37.720
So the whole program was geared towards pushing the VNV to the left side of the

231
00:15:39.411 --> 00:15:43.340
v.
Make it available as soon as possible.

232
00:15:44.150 --> 00:15:47.900
And I want to be careful here.
It doesn't mean that we get rid of testing.

233
00:15:48.410 --> 00:15:53.360
It means that what we want is fly through testing and not find any bugs.
Okay.

234
00:15:53.660 --> 00:15:58.430
There shouldn't be anything left when we run the tests so that we don't have to

235
00:15:58.431 --> 00:16:01.480
go back to that of an that designer.
Okay.

236
00:16:03.050 --> 00:16:05.510
To give you two to really hammer the point.

237
00:16:06.950 --> 00:16:11.950
NTSB is the board in the u s that's investigating all the incident and accidents

238
00:16:12.501 --> 00:16:15.920
in aviation.
And I'm more than that in transportation actually.

239
00:16:16.400 --> 00:16:17.930
But innovation in particular,

240
00:16:18.590 --> 00:16:23.300
they've traced that 90% of the accidents are due to requirement problems.

241
00:16:24.020 --> 00:16:27.380
So really the,
the pot of gold is right here.

242
00:16:27.830 --> 00:16:30.440
We really need to be able to address this.
Okay.

243
00:16:31.160 --> 00:16:36.140
So our whole program was to try to push the VNV to the left.
Okay.

244
00:16:36.440 --> 00:16:38.820
Um,
what we started to do is,

245
00:16:38.880 --> 00:16:43.130
is by giving them better techniques in terms of testing,

246
00:16:43.250 --> 00:16:48.170
then we moved down to giving them techniques they could use during coding and

247
00:16:48.171 --> 00:16:52.670
that's mostly static analysis type of things.
And then to design.

248
00:16:52.671 --> 00:16:56.620
And it's like for you two guys are uh,

249
00:16:57.260 --> 00:17:01.400
familiar with from our metals,
this model checking type of technology.

250
00:17:01.880 --> 00:17:05.240
And then what can you do at requirements?
I'll save this,

251
00:17:05.241 --> 00:17:07.490
I don't have a slide on this,
but if I skip it,

252
00:17:07.880 --> 00:17:11.950
Mary flagged me because I need to talk about it a little bit.
Um,

253
00:17:13.340 --> 00:17:16.540
so it can go all the way to using theory,

254
00:17:16.541 --> 00:17:20.990
improving to formalize and proof formally that an algorithm is correct.

255
00:17:24.170 --> 00:17:27.540
Um,
so those,
that's what I,

256
00:17:27.600 --> 00:17:32.600
I just mentioned and those are the tools that we have developed at ames.

257
00:17:33.710 --> 00:17:38.710
You get to know that most of the 50% of the work in VNV and under that program

258
00:17:39.681 --> 00:17:44.090
is done here at Ames next door to you guys.
I can see your buildings.

259
00:17:45.890 --> 00:17:48.980
I'm kind of surprise.
I'm not in the building now.
I sit directly from my office,

260
00:17:48.990 --> 00:17:52.490
but
uh,
and uh,

261
00:17:52.560 --> 00:17:55.660
the other half is done at Nasa Langley in,
uh,

262
00:17:55.770 --> 00:18:00.710
Virginia and saw some invention.
Um,
but we,

263
00:18:00.711 --> 00:18:01.830
we are,

264
00:18:02.490 --> 00:18:07.490
I would say a little better than Langley and developing tools that are practical

265
00:18:07.561 --> 00:18:12.561
for use in a coding Monell checking and requirements.

266
00:18:13.950 --> 00:18:18.870
Okay.
So let me start with the,
uh,

267
00:18:18.900 --> 00:18:23.470
with the first one.
What did we do to improve testing?
Um,

268
00:18:25.050 --> 00:18:29.910
most of the testing is very well done.
It's really very well done.

269
00:18:30.240 --> 00:18:31.490
What,
uh,

270
00:18:31.590 --> 00:18:36.590
my killers and what we see sipping through is really the outliers.

271
00:18:37.200 --> 00:18:39.940
The defense that are extremely rare.
And I'm,

272
00:18:39.950 --> 00:18:42.960
Mike still cost you tons of money.
Okay.

273
00:18:43.890 --> 00:18:48.890
So we went after this and we went after this using statistical learning.

274
00:18:49.590 --> 00:18:54.450
The way it works is basically you use your typical test cases,

275
00:18:54.810 --> 00:18:55.680
you run them,

276
00:18:56.010 --> 00:19:00.810
and then you do a statistical analysis on your output to see if you have

277
00:19:00.811 --> 00:19:02.880
outliers in your data.

278
00:19:03.750 --> 00:19:07.920
So an outlier is not necessarily something bad but it could be indicative of a

279
00:19:07.921 --> 00:19:12.330
bug.
Okay.
So there is a human making that discriminate determination.

280
00:19:12.360 --> 00:19:16.440
Is that an outlier that's indicative of a safety issue or not?

281
00:19:16.770 --> 00:19:18.210
If it is a safety issue,

282
00:19:18.510 --> 00:19:23.510
what we want to do is try to learn all the other,

283
00:19:23.960 --> 00:19:27.030
the,
because it's not just one single data point,

284
00:19:27.360 --> 00:19:29.430
it's actually a region that's unsafe.

285
00:19:29.550 --> 00:19:34.410
So we want to learn what inputs can drive to that unsafe region.

286
00:19:34.680 --> 00:19:38.550
We want to identify that and safe region and find the inputs are going to drive

287
00:19:38.551 --> 00:19:42.110
there.
What that gives you is that after that,
uh,
way of,
uh,

288
00:19:42.390 --> 00:19:47.380
of telling I shouldn't operate my,
uh,
my plane at that type of a,

289
00:19:47.400 --> 00:19:51.060
in that type of situation,
avoid this because of probably my children.

290
00:19:51.570 --> 00:19:55.470
So if you don't believe that's a problem or it's a way of solving problems.

291
00:19:55.471 --> 00:19:59.850
Innovation.
Um,
Airbus I had a,
uh,

292
00:19:59.900 --> 00:20:04.480
an incident at one point where basically,
um,

293
00:20:05.120 --> 00:20:07.200
there,
um,
uh,

294
00:20:08.100 --> 00:20:12.630
they were data that was accumulating and creating overflow errors.

295
00:20:12.720 --> 00:20:13.553
Okay.

296
00:20:13.620 --> 00:20:17.850
And they found that they couldn't find it for the longest time because on sure

297
00:20:18.030 --> 00:20:21.780
it was happening only after 24 hours of flight basically.

298
00:20:22.350 --> 00:20:25.800
So for the longest time they couldn't find this and then he was showing up in

299
00:20:25.801 --> 00:20:26.634
some cases.

300
00:20:27.120 --> 00:20:30.840
So they decided that to fix this rather than redoing,

301
00:20:30.870 --> 00:20:35.370
redesigning the software,
revalidating it there inside the fix is easy.

302
00:20:35.880 --> 00:20:38.790
Now before we reached the 24 hour period,

303
00:20:38.820 --> 00:20:43.380
we reboot the system completely clean.
Right.
Is Effects.

304
00:20:43.470 --> 00:20:46.210
It's still in the system,
but they won't show up.
Right.

305
00:20:46.740 --> 00:20:50.140
But that was one that was extremely rare to find.
It was,

306
00:20:50.141 --> 00:20:52.450
it wasn't in very particular conditions.

307
00:20:52.900 --> 00:20:57.850
So this technique is trained to go after this kind of stuff.
Every time you go,

308
00:20:58.270 --> 00:20:59.980
every time you identified a problem,

309
00:21:00.190 --> 00:21:05.190
then you use statistical learning to refine your test cases and try to drive to

310
00:21:05.570 --> 00:21:10.090
this,
to this region.
That could be unsafe.
Okay.

311
00:21:11.020 --> 00:21:11.853
Um,

312
00:21:11.890 --> 00:21:16.720
and was done on single imperson we moved it to time series because when you try

313
00:21:16.721 --> 00:21:20.380
to apply it to air traffic control then you don't reason about single input.

314
00:21:20.381 --> 00:21:23.820
You rezone about trajectories.
So you need a time series,

315
00:21:24.220 --> 00:21:27.460
something that allows you to reason about time series and inputs and outputs.

316
00:21:27.790 --> 00:21:31.270
Okay.
Um,
there is a second use of this.

317
00:21:31.300 --> 00:21:35.260
Very often when you use formal methods and new applied to the verification of

318
00:21:35.261 --> 00:21:35.890
software,

319
00:21:35.890 --> 00:21:40.890
it's too complex and sometimes you want to be able to drive to a specific region

320
00:21:42.250 --> 00:21:45.130
of behaviors that you want to explore fully.

321
00:21:45.880 --> 00:21:50.620
You can use this type of technique to actually find a point,
uh,
much quicker.

322
00:21:51.370 --> 00:21:55.720
The point I will allow you to just focus on one single region.
Okay?

323
00:21:55.870 --> 00:21:59.170
So you will use this first and then once you have identified the region,

324
00:21:59.440 --> 00:22:02.410
you do an exhaustive search using some kind of formal methods.

325
00:22:04.790 --> 00:22:07.900
All right,
so that was the testing.
Like I said,

326
00:22:07.901 --> 00:22:12.250
after that we tried to give them tools that they could use at the code level.

327
00:22:13.000 --> 00:22:15.730
So I'm sure you guys are aware is plenty of static.

328
00:22:15.731 --> 00:22:19.900
Analyze a commercial static analyzers that are out there these days.
Um,

329
00:22:20.350 --> 00:22:24.070
to name a few tools.
There is grandma tech.
Um,

330
00:22:24.100 --> 00:22:27.880
the uh,
there is covering the,
um,

331
00:22:28.330 --> 00:22:31.690
there's been some consolidation.
The names might have changed three years ago,

332
00:22:31.691 --> 00:22:35.420
but those are the kinds of tools that are out there.
Um,

333
00:22:36.070 --> 00:22:39.130
here's the problem with those tools.
They're extremely fast.

334
00:22:39.160 --> 00:22:41.590
They can deal with really big software.

335
00:22:41.620 --> 00:22:44.170
We took in million lines of code very quickly,

336
00:22:44.920 --> 00:22:48.100
but all they are giving you are warnings

337
00:22:49.630 --> 00:22:54.520
because to be fast they have to cut corner and there unsound.

338
00:22:55.090 --> 00:22:59.620
They cannot tell you for sure that you have a bag or that you don't have a bug

339
00:22:59.650 --> 00:23:02.970
in part of your software.
Okay.
Um,

340
00:23:03.130 --> 00:23:07.990
so what you end up doing when you're using this is that you get lots of warnings

341
00:23:07.991 --> 00:23:11.950
for one thing and then you have to review every one of those to see if it's

342
00:23:11.951 --> 00:23:16.150
really a bug that you knew you should be addressing or not.
So that means very,

343
00:23:16.180 --> 00:23:20.240
a lot of manual work.
So what we went after,

344
00:23:20.241 --> 00:23:25.241
it was the type of static analyzer that could tell you exactly for sure that you

345
00:23:25.511 --> 00:23:26.344
have a bag.

346
00:23:26.650 --> 00:23:31.650
That's what we call as opposed to warnings or have mornings that can turn into

347
00:23:33.011 --> 00:23:34.630
false positives.
Okay.

348
00:23:35.560 --> 00:23:40.560
And the added benefit is this type of tool can also tell you when they are sure

349
00:23:41.830 --> 00:23:45.040
that you have no bugs on that instruction.

350
00:23:45.830 --> 00:23:49.570
So it doesn't tell you that for every type of bugs that you can have,

351
00:23:49.880 --> 00:23:52.580
it goes for what's called rent time errors.

352
00:23:52.850 --> 00:23:56.600
So the typical ones are buffer overflow,

353
00:23:57.110 --> 00:24:01.670
uninitialized variables and initialize pointers,
division by zero.

354
00:24:02.440 --> 00:24:06.430
It can catch some,
uh,

355
00:24:07.100 --> 00:24:11.480
floating point errors in terms of rounding golf and those kinds of things.
Okay.

356
00:24:12.710 --> 00:24:17.450
So we created a tool and I'm surprised,
I thought,
I don't know,
would show up,

357
00:24:17.540 --> 00:24:21.290
but because the guy who helped me create this tool was actually,

358
00:24:21.291 --> 00:24:24.200
it's actually working here now doing that for you guys.

359
00:24:25.250 --> 00:24:30.250
But we were doing it for CNC plus plus because those are the languages used in,

360
00:24:30.950 --> 00:24:33.440
uh,
in flight critical systems.
Okay.

361
00:24:34.070 --> 00:24:38.000
And that we showed in in all the applications that we've done,

362
00:24:38.001 --> 00:24:40.940
of those tools that we could go to a,
what we call it,

363
00:24:40.941 --> 00:24:44.180
precision rate of 98 to 99%,

364
00:24:44.900 --> 00:24:49.900
meaning that we had only one or 2% of the checks that we were making where we

365
00:24:50.571 --> 00:24:54.380
couldn't tell you for sure that you had a bug or we couldn't tell you for sure

366
00:24:54.381 --> 00:24:59.090
that it was safe in terms with respect to the classes affairs that we were

367
00:24:59.690 --> 00:25:01.940
tracking.
Okay.
Uh,

368
00:25:01.941 --> 00:25:05.570
which is much higher than any of the commercial tools.

369
00:25:05.571 --> 00:25:10.300
One that's coming close to doing this is a tool that used to be called police

370
00:25:10.301 --> 00:25:13.460
space.
There he fire,
which is now called,

371
00:25:15.230 --> 00:25:17.390
has been bought by the math work.
I can't remember the name,

372
00:25:17.391 --> 00:25:21.300
but it's folded into the met work,
uh,
suite of tools.
Um,

373
00:25:21.950 --> 00:25:25.370
and typically why I used it,
for example,
on space,

374
00:25:25.371 --> 00:25:28.280
on the control systems that are close to the avionics,

375
00:25:28.281 --> 00:25:29.600
but they were on the space side,

376
00:25:30.140 --> 00:25:33.560
I had between 20 to 50% of warnings.

377
00:25:34.100 --> 00:25:35.960
So imagine on 1 million lines of code,

378
00:25:35.961 --> 00:25:40.961
you almost have a check per line that gives you 200 to 500,000 checks that you

379
00:25:43.461 --> 00:25:46.580
have to go through and say,
is it really a bag when he's not a buck?

380
00:25:47.080 --> 00:25:51.830
It was the main reason for that why for the longest time people were trying out

381
00:25:51.831 --> 00:25:56.510
static analysis tools and web frying them away because it really wasn't getting

382
00:25:56.511 --> 00:25:59.840
enough useful information.
So what do we want it to show?

383
00:25:59.870 --> 00:26:04.870
We didn't set out to build a tool that we can sell and said we better than

384
00:26:04.971 --> 00:26:05.804
anybody else,

385
00:26:06.080 --> 00:26:10.580
but the goal is to show the industry in our building this type of tools that you

386
00:26:10.581 --> 00:26:11.240
can do it.

387
00:26:11.240 --> 00:26:16.240
It's possible air the techniques and we using use that in year two and then you

388
00:26:16.401 --> 00:26:20.960
can do better.
Okay.
Uh,
if you guys are interested in using it,

389
00:26:20.990 --> 00:26:25.990
it's been open source since 2014 we still work on it,

390
00:26:26.151 --> 00:26:30.530
trying to refine it.
If you download it and use it and find problems,

391
00:26:30.680 --> 00:26:35.610
please send us feedback because we relying on the external community to,
uh,

392
00:26:36.200 --> 00:26:39.850
uh,
to make the tool better.
Hi.

393
00:26:40.130 --> 00:26:43.800
So that was at the code level.
Now let's move to the,
uh,

394
00:26:43.830 --> 00:26:47.960
the design phase.
Um,
and they had,

395
00:26:47.961 --> 00:26:52.830
the goal is to try to enable model checking on the models are used at design

396
00:26:52.831 --> 00:26:54.060
time.
Okay.

397
00:26:55.110 --> 00:26:59.620
There we could have gone the way of picking a formal,
uh,

398
00:26:59.640 --> 00:27:04.640
formal language to represent models and use the model checkers and are built for

399
00:27:04.801 --> 00:27:05.634
this.

400
00:27:05.820 --> 00:27:09.840
The problem is that most of those formal languages are not used in industry.

401
00:27:10.260 --> 00:27:14.570
And our mandate is to really to have an impact on industry.
Um,

402
00:27:15.090 --> 00:27:20.010
it turns out industry for flight control systems and four engine control system

403
00:27:20.011 --> 00:27:25.011
at least are using mostly something called simulink or SCADE.

404
00:27:27.670 --> 00:27:30.360
Skate is a little bit more popular with engine control.

405
00:27:30.600 --> 00:27:34.590
Simulink is more prevalent with the flight control systems.
Okay.

406
00:27:35.190 --> 00:27:40.190
So we decided to provide something that could go from those languages that

407
00:27:40.891 --> 00:27:44.430
could,
that could apply model checking on those languages.

408
00:27:44.640 --> 00:27:46.020
So when I say model checking,

409
00:27:46.021 --> 00:27:51.021
that means I'm writing safety properties about my system that should hold on my

410
00:27:51.361 --> 00:27:56.130
system.
That could be,
uh,
the,

411
00:27:56.370 --> 00:28:01.200
uh,
landing gears should not be deployed while I'm in flight,

412
00:28:01.290 --> 00:28:04.880
for example.
That's a typical one.
Okay.
Um,

413
00:28:05.340 --> 00:28:09.810
and then it checks that property if that property holds on the models or not.

414
00:28:10.050 --> 00:28:13.740
So basically you get something like this,
you get either a true,
yes,

415
00:28:13.741 --> 00:28:18.741
it's true if you go with this design or it's false and here is why it's false

416
00:28:19.471 --> 00:28:22.730
and it's going to be violated.
It's giving you a trace to,

417
00:28:22.870 --> 00:28:27.680
to show how that property would be violated.
Okay.
Um,

418
00:28:28.290 --> 00:28:31.860
so we try to enable that from simulink and skate,

419
00:28:32.340 --> 00:28:37.340
but obviously we don't want to try to address every language that is used on

420
00:28:37.831 --> 00:28:39.270
there and with different language.

421
00:28:39.271 --> 00:28:44.271
So we actually hooked up amount of bunch of model checkers to a or own internal

422
00:28:47.191 --> 00:28:51.000
representation that's using the loosest lung language.

423
00:28:52.020 --> 00:28:54.860
So it turned out that makes it easy because for example,

424
00:28:54.880 --> 00:28:59.880
SCADE the underlying foundation for SCADE is list list is a data flow language

425
00:29:02.101 --> 00:29:05.550
that was developed in Europe for embedded systems.
Okay.

426
00:29:06.120 --> 00:29:10.350
And it turns out that simulink can also be mapped to that data flow language

427
00:29:10.351 --> 00:29:11.184
fairly easily.

428
00:29:11.460 --> 00:29:15.900
So what we have is that a bunch of translators that go from seem willing to
lose,

429
00:29:16.450 --> 00:29:20.130
um,
from simulink or scaled to loose.

430
00:29:20.760 --> 00:29:24.870
And then we apply all those types of Mughal checker on it.

431
00:29:25.920 --> 00:29:29.220
And this part is extensible.
You can add your own,

432
00:29:29.310 --> 00:29:34.310
all you have to do is be able to map the representation of uh,

433
00:29:35.700 --> 00:29:39.270
to our intermediate representation.
For example,

434
00:29:39.271 --> 00:29:42.310
Boeing is asking us to add new SMV,

435
00:29:42.311 --> 00:29:44.950
which is one of the fastest and most efficient model.

436
00:29:44.951 --> 00:29:48.220
Check her out there to this list.
Okay.

437
00:29:49.360 --> 00:29:53.050
So,
um,
we didn't want to stay here because the,

438
00:29:53.051 --> 00:29:58.051
the typical one when you use simulink is you develop your models in simulink and

439
00:29:58.781 --> 00:30:03.280
then you auto code and two C or c plus plus.

440
00:30:03.580 --> 00:30:04.360
Okay.

441
00:30:04.360 --> 00:30:09.360
So what we wanted to provide also is a way to doing that at the c c plus plus

442
00:30:10.451 --> 00:30:15.280
live on than was generated.
So we have another tool called Seehorn on the,

443
00:30:15.281 --> 00:30:20.220
on the bottom right there that actually can do this.
It's,

444
00:30:20.270 --> 00:30:20.471
it,

445
00:30:20.471 --> 00:30:25.360
think of it as a sort of static analyzers as static analyzer that can check on

446
00:30:25.361 --> 00:30:26.830
see assertions.

447
00:30:27.010 --> 00:30:31.150
And those assertions are representing the properties that you verified on the

448
00:30:31.151 --> 00:30:34.940
model.
So why do we do this?
You get,

449
00:30:34.941 --> 00:30:38.170
you get to most people that are looking at it as like you are already very fine

450
00:30:38.171 --> 00:30:41.170
on design level.
Why are you doing it at the code level?

451
00:30:41.500 --> 00:30:43.120
Because the FAA doesn't trust anything.

452
00:30:43.840 --> 00:30:47.440
They require you to check it at the design level and they're going to want to

453
00:30:47.441 --> 00:30:50.830
know that you'd done it on the code.
And if we were listening to them,

454
00:30:50.831 --> 00:30:53.170
they would require us to do it on the executive board.

455
00:30:53.860 --> 00:30:57.790
But as way too many target for us to,
to chase.
Okay.

456
00:30:59.140 --> 00:31:03.470
So again,
this one is available as available,

457
00:31:03.870 --> 00:31:08.410
um,
open source.
Uh,
it's been developed partly at NASA,

458
00:31:08.440 --> 00:31:12.040
partly at Cmu,
under a what's called an NRA.
Okay.

459
00:31:12.480 --> 00:31:17.140
And we're redoing a version of it that would be only NASA to make sure that we

460
00:31:17.141 --> 00:31:21.380
have a very clean license on it.
Same thing,
and be released open source.

461
00:31:21.880 --> 00:31:26.140
And what we want to do is make sure that we have clinic pis so that anybody that

462
00:31:26.141 --> 00:31:30.370
wants to add to it can add to it.
Okay.
Um,
and,

463
00:31:30.371 --> 00:31:34.180
and one of the main reasons we're doing this is because we have on this work,

464
00:31:34.181 --> 00:31:38.480
we have partners in France at an era when working on this type of,
uh,

465
00:31:38.570 --> 00:31:43.300
of system.
Okay.
Um,

466
00:31:44.870 --> 00:31:47.050
well before going to safety cases,

467
00:31:47.590 --> 00:31:51.280
I'm going to talk a little bit about requirements because the next step really

468
00:31:51.281 --> 00:31:55.780
should be an analysis of the requirements.
Okay.

469
00:31:56.770 --> 00:32:01.770
So air is the analysis we've done when we tried to see what we should be doing

470
00:32:02.290 --> 00:32:03.123
and that level,

471
00:32:03.790 --> 00:32:08.790
there are plenty of formal languages that have been created in the last 20 years

472
00:32:10.540 --> 00:32:14.170
to formulize requirement and to do analysis on requirement.

473
00:32:14.920 --> 00:32:15.753
Nobody,

474
00:32:15.820 --> 00:32:20.820
absolutely nobody in aviation I using totally nobody because they don't like the

475
00:32:22.991 --> 00:32:27.250
language because it's too mathematical because they have to learn something new.

476
00:32:27.910 --> 00:32:30.100
The way it's done in industry is still,

477
00:32:30.190 --> 00:32:35.190
I'm writing a natural English sentences to describe my requirements and I put

478
00:32:35.411 --> 00:32:38.800
that in the best tool that's available that's called doors,

479
00:32:39.080 --> 00:32:43.910
which is a glorified spreadsheet.
That's it.
There's no formalization behind it.

480
00:32:44.180 --> 00:32:47.030
Okay.
So the question is,

481
00:32:47.180 --> 00:32:50.870
do we go and pick did in one of those languages and trying to convince industry

482
00:32:50.871 --> 00:32:55.871
to use those or do we do something that allows them to go from natural English

483
00:32:56.210 --> 00:32:59.510
to form on requirements?
And that's what we're trying to do.

484
00:33:00.200 --> 00:33:05.200
We tried to provide the part that goes from natural English to uh,

485
00:33:05.330 --> 00:33:10.280
uh,
requirements.
There are a number of ways that have been investigated.

486
00:33:10.281 --> 00:33:14.720
One of them that's popular and that's post at the air force with our partners at

487
00:33:14.721 --> 00:33:19.070
the air force,
which is using English patterns.

488
00:33:19.490 --> 00:33:21.950
So using new brightening almost English,

489
00:33:21.951 --> 00:33:26.240
but you using only some patterns of English that can very,

490
00:33:26.390 --> 00:33:30.470
very easily translated into some kind of formal language.
Okay.

491
00:33:31.760 --> 00:33:36.650
It works up to a point.
Uh,
may really,
when we tried to use it,

492
00:33:36.651 --> 00:33:38.360
they will like plenty of requirements.

493
00:33:38.361 --> 00:33:42.120
We could and we could and formalize that way.
Okay.
Um,

494
00:33:42.530 --> 00:33:43.430
on the other hand,

495
00:33:43.910 --> 00:33:48.910
you guys can give us hope with all the advances in natural language processing

496
00:33:50.721 --> 00:33:53.240
where like,
wow,
maybe we can,
uh,
we can deal with it

497
00:33:55.250 --> 00:33:58.850
not being an expert as the round.
And they told me,
now you're dreaming.

498
00:33:58.851 --> 00:34:03.340
We're not there yet.
So we had to kind of,
um,

499
00:34:03.680 --> 00:34:08.330
you can tell me different if you can tell me different and I'd be happy to take

500
00:34:08.331 --> 00:34:09.810
your inputs.
Um,

501
00:34:09.860 --> 00:34:14.860
but we went for an intermediate way where basically we feel that,

502
00:34:15.980 --> 00:34:16.813
um,

503
00:34:17.330 --> 00:34:21.140
first to write requirements that will be from analyzable.

504
00:34:21.141 --> 00:34:24.380
You have to make sure that those requirements are not ambiguous.

505
00:34:24.980 --> 00:34:29.090
So you need an interaction and the dialogue between the tools that's going to

506
00:34:29.110 --> 00:34:33.800
formulize analyze and the people are actually writing the requirements.

507
00:34:33.830 --> 00:34:35.090
And that's what we're working on.

508
00:34:35.510 --> 00:34:39.200
And we're using some kind of a pattern language pattern,

509
00:34:39.890 --> 00:34:43.910
a domain specific pattern language to do this.

510
00:34:45.320 --> 00:34:48.510
So when I'm describing is the dream,
it's not,

511
00:34:49.260 --> 00:34:52.130
it's what we're working on for the next two or three years.

512
00:34:52.190 --> 00:34:55.590
We had the very beginning of this.
I don't know when it's a,

513
00:34:55.630 --> 00:34:57.800
it's going to be available.
Okay.

514
00:34:59.150 --> 00:35:02.360
But I've been told that you guys are not working so many requirements,

515
00:35:02.810 --> 00:35:06.950
so it might not be useful to you.
All right.

516
00:35:07.190 --> 00:35:09.380
So now we,
uh,
you know,

517
00:35:09.410 --> 00:35:14.110
what techniques we shooting for each phases of the lifecycle.
Okay.

518
00:35:14.910 --> 00:35:15.743
Um,

519
00:35:16.130 --> 00:35:21.130
I said at the beginning deal one 78 is a process that builds quality and safety

520
00:35:24.740 --> 00:35:25.910
and new programs,

521
00:35:25.940 --> 00:35:30.940
but really it doesn't demonstrate that you are addressing a specific risks and

522
00:35:31.671 --> 00:35:32.660
that's what we want to do.

523
00:35:33.350 --> 00:35:38.350
So we advocating the user safety case where you can have really uh,

524
00:35:39.270 --> 00:35:44.270
evolved to specify your risk explicitly and provide evidence says that you've

525
00:35:44.431 --> 00:35:47.130
written a retired those risks.
Okay.

526
00:35:48.090 --> 00:35:53.090
So the reason they went with their one 78 is that it's mostly enforced by a set

527
00:35:55.741 --> 00:35:57.150
of standards.
You know,

528
00:35:57.510 --> 00:36:02.350
I provide traceability throughout from requirements to testing.
Um,

529
00:36:02.760 --> 00:36:06.870
I do quality reviews,
those kinds of things.
And then you get a checkmark,

530
00:36:07.020 --> 00:36:10.560
nobody's really checking what you've done and if you follow the process,

531
00:36:11.370 --> 00:36:14.430
you're done,
you're good.
Right?
Um,

532
00:36:15.870 --> 00:36:19.500
in a safety case,
now if I,
um,

533
00:36:19.890 --> 00:36:24.890
if I really list off my risks and I'm gathering evidence is to retire those

534
00:36:25.051 --> 00:36:28.470
risks,
it can be huge.
Okay.

535
00:36:28.830 --> 00:36:32.790
Now I cannot have a guy at the FAA does this checkbox and says,
well,

536
00:36:32.800 --> 00:36:34.470
you follow this process.
Okay,
you're good.

537
00:36:34.980 --> 00:36:38.700
I have to have a guide that actually goes through the safety case and make sure

538
00:36:38.701 --> 00:36:43.530
that everything has been addressed properly.
Right?
So building one,

539
00:36:43.860 --> 00:36:48.210
inspecting one so that you get your certification at the end are the biggest,

540
00:36:48.270 --> 00:36:51.240
biggest obstacles to using this type of technology.

541
00:36:51.870 --> 00:36:56.870
So that's why you guys set up to actually try to work on this and build a tool

542
00:36:57.751 --> 00:36:59.280
that allows you to visualize,

543
00:36:59.281 --> 00:37:03.330
to create safety cases and visualize them and inspect them easily.

544
00:37:03.720 --> 00:37:05.370
It does some checking for you,

545
00:37:05.820 --> 00:37:10.820
but you can guide it and the human can use it to go very efficiently through it.

546
00:37:11.880 --> 00:37:14.820
Okay.
Uh,
for example,
to just to give you a flavor,

547
00:37:16.200 --> 00:37:18.210
if you manage to formalize your requirement,

548
00:37:19.170 --> 00:37:23.100
then they can build the skeleton of the safety case automatically from your

549
00:37:23.101 --> 00:37:25.650
safety requirements.
Okay.

550
00:37:27.180 --> 00:37:32.130
It's not complete,
but it gives you the big,
like 60 let's say 60 70% of it.

551
00:37:32.520 --> 00:37:35.640
Okay.
Ultimately,

552
00:37:36.210 --> 00:37:39.330
all the results of those tools I have talked about before,

553
00:37:40.860 --> 00:37:44.760
the results produced by those two should feed into that safety case.

554
00:37:45.030 --> 00:37:48.930
So that safety case is really the nexus point where everything is integrating.

555
00:37:49.410 --> 00:37:53.370
Okay.
And why?
I'm going through this,

556
00:37:53.371 --> 00:37:58.371
even though the FAA is not using this at the moment is because when I talk about

557
00:37:58.861 --> 00:38:02.730
autonomy,
there's going to be a central piece.
Okay.

558
00:38:04.230 --> 00:38:05.610
So let me step back.

559
00:38:05.670 --> 00:38:10.620
And what I talked about was mostly about addressing this bottom one,
okay.

560
00:38:11.190 --> 00:38:13.120
Addressing the,
uh,

561
00:38:13.170 --> 00:38:18.170
the cost of VNV inflight critical systems and then helping DFA,

562
00:38:19.290 --> 00:38:23.850
um,
have the right processes and the right training for their people so that they

563
00:38:23.851 --> 00:38:28.851
can actually accept what's coming out of those tools because they are one 78 C

564
00:38:30.541 --> 00:38:34.880
is not developed by DFA,
is developed by a consortium under our,

565
00:38:34.881 --> 00:38:37.330
our uh,
agency called Itca,

566
00:38:37.810 --> 00:38:40.430
which has people from DFA,

567
00:38:40.450 --> 00:38:45.430
people from a vacation and people from governmental agencies or the governmental

568
00:38:45.431 --> 00:38:49.700
agencies and all those guys are coming to a consensus,
um,

569
00:38:49.900 --> 00:38:52.450
writing a document and then the FDA says,
yes,

570
00:38:52.451 --> 00:38:55.250
we had done this as the new standard or not.
Okay,

571
00:38:56.080 --> 00:38:58.270
but that doesn't mean they're setting fires.

572
00:38:58.300 --> 00:39:00.970
I actually know what to do with that new documents.

573
00:39:01.360 --> 00:39:04.360
So a huge part of our,

574
00:39:04.540 --> 00:39:09.360
what our colleagues at Langley are doing is to try to help DFA,

575
00:39:09.800 --> 00:39:13.510
um,
build the training materials.
And educate,

576
00:39:13.511 --> 00:39:17.710
there's certifiers so that they can actually accept what's coming out of those

577
00:39:17.711 --> 00:39:22.210
tools.
Um,
the next thing is,
uh,
like I said,

578
00:39:22.211 --> 00:39:25.000
we were asked to look at their traffic management systems.

579
00:39:25.510 --> 00:39:30.430
So I don't know if you guys have ever seen an embedded system for uh,
for,

580
00:39:30.460 --> 00:39:35.460
for an airplane and a system for our traffic management from a software

581
00:39:35.921 --> 00:39:40.570
engineering,
uh,
point of view,
those are completely different beasts.

582
00:39:41.230 --> 00:39:43.150
So,
um,
for example,

583
00:39:43.170 --> 00:39:48.170
the use of pointers and those kinds of things in an embedded system is very

584
00:39:48.311 --> 00:39:49.144
restricted.

585
00:39:50.020 --> 00:39:53.920
Anything dynamic is extremely restricted in an embedded system.

586
00:39:54.340 --> 00:39:56.320
Air traffic control,
everything goes.

587
00:39:57.190 --> 00:40:00.520
So if you think just of static analysis,

588
00:40:00.790 --> 00:40:04.510
then suddenly what you have that's efficient for embedded system might not be

589
00:40:04.511 --> 00:40:07.150
efficient for air traffic system.

590
00:40:07.810 --> 00:40:11.950
So that's going to require us to actually do some adjustments in the tools that

591
00:40:11.951 --> 00:40:12.784
we have.

592
00:40:12.880 --> 00:40:16.750
And if you're talking about using a model based approach to building a traffic

593
00:40:16.751 --> 00:40:17.530
system,

594
00:40:17.530 --> 00:40:21.160
it's not going to be with simulink is not going to be with Kate is going to be

595
00:40:21.430 --> 00:40:25.450
most probably with [inaudible] or at the most.
Okay.

596
00:40:26.050 --> 00:40:29.680
So that's an or maybe a DL,

597
00:40:30.280 --> 00:40:34.450
but that means that you have to build translators to or loose language so that

598
00:40:34.451 --> 00:40:37.070
we can actually do the analysis of it.
Okay.

599
00:40:37.620 --> 00:40:42.620
And at the very top is what they want us to shoot for eventually is addressing

600
00:40:44.231 --> 00:40:45.064
autonomy.

601
00:40:46.540 --> 00:40:51.540
So one thing when you're talking to Arrow people and NASA about autonomy,

602
00:40:53.470 --> 00:40:57.910
um,
it can mean anything from automation to actually autonomy.

603
00:40:58.180 --> 00:40:58.900
Okay.

604
00:40:58.900 --> 00:41:03.460
It is not necessarily that you using AI in the system could be that you're

605
00:41:03.461 --> 00:41:04.510
automating a function.

606
00:41:05.260 --> 00:41:09.780
So we have to address anything in between,
uh,

607
00:41:10.120 --> 00:41:14.860
in between those.
Okay.
Um,

608
00:41:15.820 --> 00:41:19.930
oh,
one point before I go to,
it's an me,
um,

609
00:41:22.150 --> 00:41:25.750
this is not in the pie in the sky research,

610
00:41:26.780 --> 00:41:31.510
the days where we can play with the mouse checker and show it on a small system

611
00:41:31.511 --> 00:41:34.910
and say,
victory.
It worked.
It's gone.

612
00:41:35.660 --> 00:41:40.310
Now we have a mandate that we have to infuse doors,
any industry,

613
00:41:40.340 --> 00:41:45.340
those tools and actually demonstrate that we impacting the custom VNV the in the

614
00:41:46.941 --> 00:41:51.440
right way.
Okay.
So that's why we working with Boeing,
GE,
Rockwell,

615
00:41:51.441 --> 00:41:54.200
Collins,
Honeywell and all the,
and all of those.

616
00:41:54.400 --> 00:41:55.233
<v 0>Okay.</v>

617
00:41:57.470 --> 00:42:00.290
<v 2>Um,
and if you guys have any systems we can work with you.</v>

618
00:42:02.820 --> 00:42:06.390
So that gives you a sense of what was accomplished in terms of tools.

619
00:42:06.391 --> 00:42:10.320
I've talked about a few things that I haven't mentioned is that when you

620
00:42:10.321 --> 00:42:12.990
starting to talk about complex systems,

621
00:42:13.470 --> 00:42:16.990
then model checker is going to explode unless you use some kind of compositional

622
00:42:17.040 --> 00:42:20.370
reasoning behind it.
Okay.
Um,

623
00:42:20.850 --> 00:42:23.700
and then at the top it shows you that,
um,

624
00:42:24.090 --> 00:42:28.650
industry is starting to actually implement those tools,
pick up those tools,

625
00:42:28.651 --> 00:42:31.140
build on them,
and use them on real projects.

626
00:42:31.430 --> 00:42:32.263
<v 0>Okay.</v>

627
00:42:34.800 --> 00:42:39.360
<v 2>No,
like I said,
um,
we have to address autonomy.
Why?</v>

628
00:42:39.690 --> 00:42:43.140
Because no.
So in its,
in its infinite wisdom,

629
00:42:44.040 --> 00:42:44.790
um,

630
00:42:44.790 --> 00:42:49.770
at least the Irm d part decided we should have six strategic thrust that we need

631
00:42:49.771 --> 00:42:54.560
to address.
And if you look at the bottom,
I assured autonomy for innovation.

632
00:42:54.570 --> 00:42:58.170
Transformation is a big one.
Okay.
The other ones,

633
00:42:58.171 --> 00:43:02.610
and you might've had a talk by Kai goalball before is what's called real time

634
00:43:02.611 --> 00:43:04.770
system wide safety assurance,

635
00:43:05.160 --> 00:43:07.680
which is more about using data and analytics,

636
00:43:08.010 --> 00:43:11.640
real time on data to try to prevent or,

637
00:43:11.890 --> 00:43:16.320
or predict possible accidents.
Okay.
Um,

638
00:43:17.340 --> 00:43:22.340
so that means we have to adapt what we do in Vnv or charter right now is the

639
00:43:23.671 --> 00:43:28.110
flight critical systems.
But this is going to ram down at one point.

640
00:43:28.480 --> 00:43:31.170
I was going to say you're done with this,
you created the tools,

641
00:43:31.180 --> 00:43:33.060
you went as far as you could in infusion,

642
00:43:33.390 --> 00:43:38.390
move on to complex systems and autonomous systems and we have to start planning

643
00:43:39.691 --> 00:43:41.580
this.
So

644
00:43:42.500 --> 00:43:42.870
<v 0>okay.</v>

645
00:43:42.870 --> 00:43:44.100
<v 2>Um,
like I said,</v>

646
00:43:44.880 --> 00:43:49.880
the guys are working on safety cases and their tools and we're going to build on

647
00:43:49.951 --> 00:43:50.580
them.

648
00:43:50.580 --> 00:43:55.580
There have already been using them and building the safety case to get what's

649
00:43:56.011 --> 00:43:56.850
called [inaudible],

650
00:43:57.330 --> 00:44:01.560
which are the certificates you need to be able to operate and a flight the Jones

651
00:44:01.950 --> 00:44:05.700
to fund any kind of operations.
Um,

652
00:44:06.270 --> 00:44:11.270
and then we built on another work that we've done for the,

653
00:44:11.760 --> 00:44:13.430
for DFA,
which is kind of fun.

654
00:44:13.431 --> 00:44:17.640
The side which is trying to do v And v 40 classics.

655
00:44:17.970 --> 00:44:22.970
That's an interesting one for us for a very specific reason is because it's a

656
00:44:23.731 --> 00:44:28.710
very new way of building software for aviation and it hasn't been used before.

657
00:44:29.370 --> 00:44:33.280
What's happening at cast sex is,
is,
is the next generation for,
um,

658
00:44:33.540 --> 00:44:34.680
collision avoidance.

659
00:44:35.250 --> 00:44:39.120
So right now the ones that all the Aircrafts are using is called t cass.

660
00:44:39.710 --> 00:44:43.860
And if you look at the way tickets look inside,
it's just a bunch of,

661
00:44:43.920 --> 00:44:48.020
if then else rules,
if this happened,
then you do this.
If this happened,

662
00:44:48.021 --> 00:44:52.560
you do that.
Okay.
It's,
it's the reasoning is explicit in it,

663
00:44:52.561 --> 00:44:54.660
but it's a monster.
It's software.
Okay.

664
00:44:55.410 --> 00:44:57.900
And they never quite sure they haven't missed a case.

665
00:44:58.230 --> 00:45:01.980
So in the next generation they want to do it completely differently.

666
00:45:02.370 --> 00:45:07.370
They basically build a probabilistic model of all the collisions that happen or

667
00:45:07.711 --> 00:45:11.670
the whole possible collisions that happen given the history called data.

668
00:45:11.671 --> 00:45:12.504
And then they had,

669
00:45:13.230 --> 00:45:18.230
and then they do optimization on it for some specific parameters.

670
00:45:18.841 --> 00:45:21.030
And the typical one is called an Mac,

671
00:45:21.330 --> 00:45:25.410
which is minimizing the number of possible,
uh,

672
00:45:25.920 --> 00:45:28.050
collision.
Basically mid hair,

673
00:45:28.770 --> 00:45:32.550
near mid air collision is what it stands for.
Exactly.
Okay.

674
00:45:32.880 --> 00:45:37.290
So they solve this and that gives you a big table and they put that in a big

675
00:45:37.291 --> 00:45:37.691
table.

676
00:45:37.691 --> 00:45:41.970
And where if you have the state of bureaucrats and the state of the aircraft

677
00:45:41.971 --> 00:45:43.110
that might be in collusion with,

678
00:45:43.111 --> 00:45:48.020
do you feed that to the table and pops out an advisory and that's what's going

679
00:45:48.021 --> 00:45:49.980
to be on board pretty much.

680
00:45:50.820 --> 00:45:55.820
So what you have is a very simple software because really what it does is just

681
00:45:56.131 --> 00:45:58.380
reading off the table,
that's it.
Right?

682
00:45:58.770 --> 00:46:03.770
But all the intelligence or the rational behind the advisory is actually done in

683
00:46:06.001 --> 00:46:11.001
the model and in the optimization that was done to go from the model to the

684
00:46:11.071 --> 00:46:15.870
table.
Okay.
So in terms of VNV,
that's completely different.

685
00:46:15.900 --> 00:46:20.020
You can still apply testing.
That's still works.
All right.
But uh,

686
00:46:20.070 --> 00:46:24.090
the space for testing is extremely huge when you're talking about collision

687
00:46:24.091 --> 00:46:24.924
systems.

688
00:46:25.020 --> 00:46:29.460
So if you want to do anything smart or in thing that's like a bit clever,

689
00:46:29.880 --> 00:46:34.170
then you have to do the reasoning on the model and how the transformation from

690
00:46:34.171 --> 00:46:36.390
the model to the table is done.
Okay.

691
00:46:37.050 --> 00:46:39.900
And if you look at it with,

692
00:46:40.650 --> 00:46:44.220
I mean the past experience on one was done at NASA in autonomy.

693
00:46:45.630 --> 00:46:48.540
I mean it's new for Arrow,
but on the space side,

694
00:46:48.541 --> 00:46:53.040
it's been worked on for the past 15,
20 years.
Terry,
give a talk.
No,

695
00:46:53.880 --> 00:46:58.650
no,
it should,
you should ask him with the history of how engine.
Well,
it,

696
00:46:58.660 --> 00:47:02.250
when it goes way back,
Julian could testify to this.

697
00:47:03.610 --> 00:47:04.920
Uh,
so we,

698
00:47:05.610 --> 00:47:09.680
if you look at what was on Howard that was and was done,
Deuteronomy,

699
00:47:10.070 --> 00:47:15.070
there's almost always some kind of a model based execution that's at play there.

700
00:47:15.510 --> 00:47:17.070
It could be a model,

701
00:47:17.071 --> 00:47:21.120
could be a physics model of the spacecraft so that you can do health monitoring.

702
00:47:21.510 --> 00:47:24.930
You compare the data to that physics model.
When you have a mismatch,

703
00:47:25.440 --> 00:47:30.440
then you get an advisory or it could be using a planning and scheduling system

704
00:47:30.880 --> 00:47:34.870
or it could be new techniques like machine learning or reinforcement learning

705
00:47:34.871 --> 00:47:39.190
type of techniques.
Then your model is more in your reward or utility functions,

706
00:47:39.700 --> 00:47:44.560
right?
But the bottom line is that it's not in the code anymore.

707
00:47:45.130 --> 00:47:48.880
So that's why this type of work was a good stepping stone for us to think of

708
00:47:48.881 --> 00:47:51.640
what can be used in autonomy,
uh,
these days.

709
00:47:53.470 --> 00:47:57.700
So when you talk about autonomy,
most people tell you,
well,

710
00:47:57.701 --> 00:48:00.920
but what do I do about unknown nouns?
You know?

711
00:48:02.360 --> 00:48:03.193
And that's very true.

712
00:48:03.400 --> 00:48:08.400
I mean there is no way with the type of autonomy that's being discussed now that

713
00:48:08.591 --> 00:48:11.800
we'll be able to predict all the risks at design time.

714
00:48:13.270 --> 00:48:17.410
The environment is changing,
completely can,
can throw you off completely.

715
00:48:18.330 --> 00:48:20.290
There is no way you're going to be able to predict.

716
00:48:20.560 --> 00:48:24.460
So you're gonna still have to do some a design time,

717
00:48:24.580 --> 00:48:26.890
some VNV like the classical we've seen,

718
00:48:27.310 --> 00:48:31.240
but you're going to have to be able to do something at execution time.

719
00:48:31.990 --> 00:48:36.160
So most people are thinking,
when we think about this,
about rent,
time assurance,

720
00:48:36.190 --> 00:48:38.830
what's going on in time assurance,
which is rent,
time monitoring

721
00:48:40.540 --> 00:48:42.270
and being able to,
uh,

722
00:48:42.340 --> 00:48:47.260
basically a monitor a system and,
and,

723
00:48:47.270 --> 00:48:49.150
and do something in real time.

724
00:48:50.470 --> 00:48:55.470
The where it doesn't work with DFA is that you still have to kind of certified

725
00:48:56.611 --> 00:48:57.880
that you're doing the right thing.

726
00:48:58.720 --> 00:49:01.840
So you can certify the runtime monitoring system.

727
00:49:02.110 --> 00:49:03.970
But proving that your mum,

728
00:49:03.971 --> 00:49:07.720
you have enough money turns to catch all the possible problems.

729
00:49:08.740 --> 00:49:09.573
It's not possible.

730
00:49:10.210 --> 00:49:13.750
So somehow we'll have to come up with a system,

731
00:49:13.930 --> 00:49:17.020
especially if we're talking about drones all over the place,

732
00:49:17.021 --> 00:49:20.350
delivering packages and flying in urban environment.

733
00:49:20.770 --> 00:49:25.770
We'll have to have a sort of a very fication loop in real time that can provide

734
00:49:27.041 --> 00:49:28.240
you some insurance in it.

735
00:49:29.110 --> 00:49:33.430
So to do this quickly and to have a really time tight loop,

736
00:49:33.431 --> 00:49:36.490
I can't tell you when I tell that story to the traditional aviation,

737
00:49:36.491 --> 00:49:38.770
they look at me like you're nuts,

738
00:49:39.250 --> 00:49:43.150
but I'm hoping you guys would be more receptive if you're telling me you're
nuts,

739
00:49:43.230 --> 00:49:44.063
the numbness.

740
00:49:45.430 --> 00:49:49.780
But to get really a tight loop there that can do some kind of certification,

741
00:49:51.040 --> 00:49:56.020
that means behind it.
You have to be able to reason about risks.
Um,

742
00:49:56.650 --> 00:49:59.470
even though you have an identified those risks.

743
00:49:59.830 --> 00:50:04.830
So this is where we look at the safety case as something right now.

744
00:50:05.111 --> 00:50:09.520
Safety cases done a design time.
I check it,
I get my certification,

745
00:50:09.550 --> 00:50:13.990
it goes into a drawer and nobody looks at it anymore.
Right?
Or,
um,

746
00:50:14.560 --> 00:50:17.440
or a thesis is you keep this,

747
00:50:17.650 --> 00:50:22.060
and this is a live document throughout the life of the system all the way until

748
00:50:22.061 --> 00:50:23.650
it's retired.
Okay.

749
00:50:24.130 --> 00:50:27.500
And you use this to actually reason about risks.

750
00:50:28.270 --> 00:50:29.870
You add risks to it,

751
00:50:29.900 --> 00:50:34.900
but that means that you're able to reason and identified risk about your domain,

752
00:50:34.971 --> 00:50:36.050
your application domain.

753
00:50:36.920 --> 00:50:41.780
So this is what I'm not gonna I'm not expecting you guys to go for that whole

754
00:50:41.781 --> 00:50:46.250
picture,
but this is pretty much what this is saying in here.

755
00:50:46.580 --> 00:50:51.020
Okay.
And like I said,
the traditional industry is looking at us.

756
00:50:51.021 --> 00:50:54.230
I think you guys are nuts.
This,
you'll never get there,

757
00:50:55.880 --> 00:51:00.350
but that's what we're going to probably shoot for in the next,
uh,
five years.

758
00:51:00.770 --> 00:51:03.130
Okay.
And this is the research foundations.

759
00:51:03.131 --> 00:51:06.920
I will do a lot of form of domain theory,
uh,

760
00:51:06.950 --> 00:51:08.360
semantics driven safety.

761
00:51:08.361 --> 00:51:13.361
And then he says big one by directional transformations between the arguments in

762
00:51:13.821 --> 00:51:18.230
the artifact because DFA always wants traceability between everything.
Okay?

763
00:51:19.030 --> 00:51:23.300
Uh,
probabilistic analysis and,
uh,
and systems to actually help,

764
00:51:24.460 --> 00:51:28.400
uh,
provide the information.
And this is it.

765
00:51:28.700 --> 00:51:30.290
This is all I have today.

766
00:51:32.000 --> 00:51:34.250
I'll be happy to entertain questions.

767
00:51:34.970 --> 00:51:38.090
If it dives deep into a technology and I don't know know enough,

768
00:51:38.150 --> 00:51:40.940
I'll provide you the information of the guilty party.

769
00:51:43.080 --> 00:51:43.913
Okay.

770
00:51:44.340 --> 00:51:45.470
<v 3>Uh,
so,
uh,</v>

771
00:51:46.190 --> 00:51:50.120
how much of the problem of verification,

772
00:51:50.200 --> 00:51:52.070
especially at the requirements level,

773
00:51:52.100 --> 00:51:55.910
is modeling the system and how much is modeling the environment?

774
00:51:59.040 --> 00:52:02.860
<v 2>Uh,
truthfully in terms of the requirements,</v>

775
00:52:02.861 --> 00:52:07.570
you don't have that much about the environment.
It's mostly a bad the system.

776
00:52:08.350 --> 00:52:12.930
So yes,
that means that there is a part that's missing in there.
And,

777
00:52:13.250 --> 00:52:14.590
and that part,

778
00:52:15.130 --> 00:52:18.250
especially if I'm talking about addressing all the risks and the possible risk

779
00:52:18.640 --> 00:52:22.870
and as to come from your safety case because the beauty of the safety case.

780
00:52:23.370 --> 00:52:27.730
Um,
so I should say that,
um,
when I talk about certification Idfa,

781
00:52:27.790 --> 00:52:29.410
and when you do dear one 78,

782
00:52:29.710 --> 00:52:33.760
it gives you what you call what's called the air worthiness certificate.
Okay.

783
00:52:33.910 --> 00:52:37.270
It's not on the software.
It's on the system that's using the software.

784
00:52:37.810 --> 00:52:42.190
But you get air worthiness.
That means you worthy to go in the air.

785
00:52:42.430 --> 00:52:45.370
That doesn't mean that gives you the right to a period.

786
00:52:45.880 --> 00:52:48.980
There is a second stage that looked at year,
uh,

787
00:52:49.060 --> 00:52:52.810
operational part and how it's been one environment is going to be operating.

788
00:52:53.530 --> 00:52:58.360
If the beauty of the safety case is all of this is in the same place,

789
00:52:58.930 --> 00:53:02.350
the operational environment and the airworthiness part of it.

790
00:53:03.160 --> 00:53:07.070
So this is where the safety case,
we'll have to help us.
There it is.

791
00:53:07.350 --> 00:53:12.350
This is a problem because the current industry is not using when a reasoning in

792
00:53:12.431 --> 00:53:14.680
those terms,
they're usually separate teams

793
00:53:16.660 --> 00:53:18.670
<v 3>in the part of the coalition,</v>

794
00:53:19.180 --> 00:53:24.180
like a new system that basically you change the system from this bunch of easels

795
00:53:24.900 --> 00:53:29.490
to like a pre calculate data table that you upload and then,

796
00:53:30.050 --> 00:53:33.690
and then the system uses.
Huh.
Um,
as you said,

797
00:53:33.691 --> 00:53:37.950
the system and that was much easier to verify because it's a much simpler
system,

798
00:53:38.160 --> 00:53:41.280
but how can you verify that the actual data that you're producing,

799
00:53:41.340 --> 00:53:45.570
the table that you're uploading,
um,
is correct.
I mean,

800
00:53:45.571 --> 00:53:48.960
and now these actually is better than the other system.
I was

801
00:53:49.130 --> 00:53:50.990
<v 2>no more formally,</v>

802
00:53:52.010 --> 00:53:54.470
you're mixing two concepts I'm doing and say that was easy.

803
00:53:54.471 --> 00:53:57.860
That's actually a challenge.
The challenge is exactly what you stated.

804
00:53:58.390 --> 00:54:03.390
How'd you very far that what's in the table is actually what should be in the

805
00:54:03.891 --> 00:54:08.630
table.
Okay.
So it's,
it's multiple thing when you um,

806
00:54:08.960 --> 00:54:13.960
the first part is the model is built and we remembered that model will be frozen

807
00:54:14.841 --> 00:54:19.280
out.
Uh,
I'd execution time.
It wasn't a happy appear,
right?
It's,

808
00:54:19.281 --> 00:54:20.390
it's just used.

809
00:54:20.690 --> 00:54:25.690
The model again is produced from historical data and then it's going to be solve

810
00:54:27.500 --> 00:54:31.790
in terms of using some kind of optimization engine for certain criteria.

811
00:54:32.450 --> 00:54:36.140
That result of the optimization is what's causing to the table.
Okay,

812
00:54:36.470 --> 00:54:39.830
so you're right.
The model is at the core of the verification problem there.

813
00:54:40.310 --> 00:54:42.920
The first problem is I do you have the right data?

814
00:54:43.250 --> 00:54:47.870
This is the first bad part because it's all based on historical data.

815
00:54:48.770 --> 00:54:49.520
Okay.

816
00:54:49.520 --> 00:54:53.780
I haven't seen anything that says we're going to keep in pre running the mile to

817
00:54:53.781 --> 00:54:55.340
make sure that we have the right,

818
00:54:55.700 --> 00:55:00.020
the right model for the current density of traffic,
for example.
Okay.

819
00:55:00.650 --> 00:55:05.030
The second part is when you do the,
the,
the optimization and the model,

820
00:55:05.420 --> 00:55:07.550
you have to discreet ties that model.

821
00:55:07.551 --> 00:55:12.150
That's a model that's actually continues in,
in nature.
Um,

822
00:55:12.470 --> 00:55:13.690
eight has to be discreet.

823
00:55:14.840 --> 00:55:19.660
And the first verification question is what is the right,
uh,

824
00:55:20.030 --> 00:55:23.510
discretization a level for this.
Okay.

825
00:55:24.020 --> 00:55:29.020
And after that is I did you optimize for the right actually for the right power

826
00:55:30.930 --> 00:55:34.250
meters and your mom.
So it,

827
00:55:34.260 --> 00:55:39.260
it's a very different way of looking at the problem and,

828
00:55:39.370 --> 00:55:41.660
and I'm not going to claim we solving the whole thing.

829
00:55:42.530 --> 00:55:45.890
We've provide parts we provided part of the solution,

830
00:55:46.340 --> 00:55:50.830
not the whole solution to it.
And if you have suggestions,

831
00:55:52.840 --> 00:55:53.673
I'll take them.

832
00:55:57.910 --> 00:56:02.470
I don't know.
I'm not sure.
I mean this might be showing up in,
um,
um,

833
00:56:03.010 --> 00:56:06.700
well I live in mountain view right across the highway,
literally from Google.

834
00:56:07.360 --> 00:56:12.280
So I see d autonomous car going from my neighborhood.
Um,

835
00:56:12.460 --> 00:56:16.090
I don't have many details about,
uh,
what's happening,

836
00:56:16.091 --> 00:56:18.010
but for one,

837
00:56:18.011 --> 00:56:23.011
I've been gathering your guys every time are collecting about the current,

838
00:56:24.281 --> 00:56:25.210
right?
Right.

839
00:56:25.570 --> 00:56:30.570
And all goes down to a huge database that have like huge amount of scenarios and

840
00:56:32.591 --> 00:56:37.510
then you analyze those and that's how you modify whatever the software is doing.

841
00:56:37.511 --> 00:56:39.010
Right.
Um,

842
00:56:39.940 --> 00:56:43.300
in some sense it's a little bit the same problem.

843
00:56:43.750 --> 00:56:47.050
It's like you probably have a step where you look at then all that data,

844
00:56:47.560 --> 00:56:52.560
then you have to model it and then come up with the software after that that

845
00:56:53.411 --> 00:56:54.460
does it.
Okay.

846
00:56:54.730 --> 00:56:59.140
I very much doubt that you have if Denelle's type of software going through all

847
00:56:59.141 --> 00:57:03.790
the scenarios.
So I don't know enough about that part,

848
00:57:03.791 --> 00:57:07.060
but it's probably a similar problem that you have to,
to solve there.

849
00:57:08.630 --> 00:57:11.690
<v 3>I used to these kind of tools in particular in Missouri followed the safety</v>

850
00:57:11.691 --> 00:57:15.320
moods interaction with modeling especially well.
Um,

851
00:57:15.440 --> 00:57:18.950
but like does this give you a new perspective on how you would validate that

852
00:57:19.160 --> 00:57:22.430
your controllers responded appropriately to the dynamics that you're like

853
00:57:22.431 --> 00:57:24.020
crafting safety cases that say,

854
00:57:24.021 --> 00:57:28.670
like in this situation the controller should do this particular thing or not do

855
00:57:28.671 --> 00:57:29.780
this particular other things.

856
00:57:30.870 --> 00:57:35.870
<v 2>So that touches has a different area than we haven't really addressed.</v>

857
00:57:37.210 --> 00:57:42.030
Um,
that touches is how you come up with your risks given new system.

858
00:57:42.060 --> 00:57:46.950
In this case you're putting in,
in the system and control system,
uh,
uh,

859
00:57:47.280 --> 00:57:48.720
framework.
Um,

860
00:57:48.850 --> 00:57:52.500
they're like a lot of techniques that can be done for this.

861
00:57:52.530 --> 00:57:55.050
I mean the traditional me ea type of stuff,

862
00:57:55.440 --> 00:57:59.580
but you can be a lot more refined.
Truthfully.
We have an address.

863
00:57:59.880 --> 00:58:01.830
This part we haven't touched that part.

864
00:58:01.860 --> 00:58:06.570
We asked him that there is an end of tools to do this out there.

865
00:58:07.230 --> 00:58:10.200
Um,
the closest we've been doing,

866
00:58:10.201 --> 00:58:15.201
and this is work that we're doing with Oni in France where we looking at the

867
00:58:15.511 --> 00:58:15.931
code,

868
00:58:15.931 --> 00:58:20.931
implementing the controller and trying to prove controlled types of properties

869
00:58:23.401 --> 00:58:28.140
like stability and those kinds of things,
which is extremely hard to do.
Uh,

870
00:58:28.160 --> 00:58:30.870
our static analysis is based on abstract interpretation,

871
00:58:31.140 --> 00:58:34.470
which means to give you an answer but something we building an abstraction

872
00:58:34.471 --> 00:58:36.840
around the data points.
Okay.

873
00:58:37.230 --> 00:58:41.200
We as opposed to a static analysis where you would go,
uh,

874
00:58:41.460 --> 00:58:44.490
explore every path which is called the past sensitive or,

875
00:58:44.930 --> 00:58:49.890
or flow sensitive analysis.
Um,
because that doesn't scale that one.

876
00:58:50.340 --> 00:58:55.010
So if you reason about abstraction,
then it starts scaling.
But when you want it,

877
00:58:55.011 --> 00:58:58.920
it's too rough.
The ones that we have right now is too rough for a,
um,

878
00:58:59.050 --> 00:59:01.860
like stability properties for control theory.

879
00:59:02.400 --> 00:59:07.070
So we have to go to techniques that are much heavier,
um,
and,

880
00:59:07.071 --> 00:59:11.860
and truthfully,
we haven't implemented those in the tools yet.
Okay.

881
00:59:12.900 --> 00:59:17.460
<v 0>Thanks.
[inaudible].</v>

