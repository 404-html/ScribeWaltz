WEBVTT

1
00:00:06.210 --> 00:00:10.530
Hi everyone.
Welcome to toxic Google from Cambridge,
Massachusetts.

2
00:00:10.650 --> 00:00:13.560
And today we're very happy to host.

3
00:00:13.620 --> 00:00:17.670
Sarah walked her batcher and her new book,
technically wrong.
Uh,

4
00:00:17.671 --> 00:00:22.560
so Sarah has been a web designer and Ux consultant and so she's,

5
00:00:22.561 --> 00:00:25.740
uh,
in the industry but not necessarily completely of it.

6
00:00:25.770 --> 00:00:29.570
And I think this gives her a great,
a fresh perspective on a,

7
00:00:29.571 --> 00:00:34.350
on some of the issues that,
uh,
that face our industry and society today.

8
00:00:34.380 --> 00:00:37.050
These are increasingly familiar themes,
um,

9
00:00:37.051 --> 00:00:39.730
but I think that's exactly as it should be,
um,
given,
uh,

10
00:00:39.750 --> 00:00:43.800
how critically important they are to our industry and indeed to the world as a

11
00:00:43.801 --> 00:00:45.600
whole.
Now.
So welcome Sarah.

12
00:00:49.740 --> 00:00:50.573
<v 1>Okay.</v>

13
00:00:51.780 --> 00:00:54.720
<v 0>Hello.
Thank you all for coming.
Um,</v>

14
00:00:56.040 --> 00:01:01.040
so I'd like to talk today first off about something that is not in the book at

15
00:01:01.051 --> 00:01:05.270
all and it's not in the book because it happened last week.
Um,

16
00:01:05.400 --> 00:01:08.280
and some of you might be familiar with it.
Um,

17
00:01:10.270 --> 00:01:12.300
it's a friends,
the mini cupcakes.

18
00:01:13.420 --> 00:01:16.830
And I'd like to talk about this both because I'm here at Google and because I

19
00:01:16.831 --> 00:01:20.280
think this is a really good example of some of the things that I think are not

20
00:01:20.281 --> 00:01:25.230
being talked about enough.
So many cupcakes as I'm sure you all know,

21
00:01:25.590 --> 00:01:26.423
um,

22
00:01:26.620 --> 00:01:31.620
where the topic of discussion because of a Google maps update that went out to,

23
00:01:32.500 --> 00:01:36.150
um,
a selection of iPhone users that started,
um,

24
00:01:36.370 --> 00:01:41.370
showing the number of calories that maps thought that you might burn if you walk

25
00:01:42.601 --> 00:01:44.760
somewhere instead of taking some other form of transit.

26
00:01:45.360 --> 00:01:50.360
And also how many mini cupcakes that might be in terms of your calories burned.

27
00:01:53.310 --> 00:01:56.850
Now,
almost immediately after this launched,

28
00:01:57.060 --> 00:01:58.500
some people started talking about it.

29
00:01:58.980 --> 00:02:02.220
One of them was a woman named Taylor Lawrence who was a journalist and,
uh,

30
00:02:02.250 --> 00:02:06.120
she did not like this update,
so she had some comments.

31
00:02:06.121 --> 00:02:09.900
So one of the first things she said was like,
oh my God,

32
00:02:10.680 --> 00:02:12.270
if you click on walking directions,

33
00:02:12.271 --> 00:02:17.040
it's going to tell you how much food this burns.
She goes on to talk about,
um,

34
00:02:18.240 --> 00:02:21.300
there's no way to turn this off.
Then she says,

35
00:02:21.301 --> 00:02:24.420
do they not realize how triggering this is for people with eating disorders?

36
00:02:24.810 --> 00:02:27.210
And like this just generally feel shamey bad.

37
00:02:28.410 --> 00:02:31.950
She goes on to talk about how calorie counting maybe isn't even a good thing and

38
00:02:31.951 --> 00:02:33.210
there's a lot of disagreement on that.

39
00:02:34.230 --> 00:02:38.460
Then she talks about how this is even perpetuating diet culture.

40
00:02:39.180 --> 00:02:42.120
So on and on she goes up until about 9:00 PM,

41
00:02:43.110 --> 00:02:47.310
this is about an hour of tweets that she has sent kind of walking through all of

42
00:02:47.311 --> 00:02:49.020
the reasons that she thinks that this is a problem.

43
00:02:49.890 --> 00:02:52.350
She says there's no way to turn it off.
This can be dangerous.

44
00:02:52.351 --> 00:02:53.640
People with eating disorders,

45
00:02:53.940 --> 00:02:57.510
this feel shamey average calorie counts are wildly inaccurate.

46
00:02:57.570 --> 00:03:01.300
Not all calories are equal.
A cupcake is not a useful metric.

47
00:03:01.660 --> 00:03:06.640
What is the mini cupcake anyway?
Who's mini cupcake?
Are we talking about,
um,

48
00:03:06.820 --> 00:03:11.140
pink cupcakes or not a neutral choice?
They have did not her exact wording,

49
00:03:11.141 --> 00:03:12.310
but you know,
they have social,

50
00:03:12.311 --> 00:03:15.910
cultural and coding to them that they would be perceived as being more feminine,

51
00:03:16.090 --> 00:03:19.540
more White,
more middleclass,
and that this perpetuates diet culture.

52
00:03:20.740 --> 00:03:24.940
So that is sort of the summary of her hour of evaluating this product choice.

53
00:03:26.500 --> 00:03:28.930
And I look at something like that and I think,
okay,

54
00:03:30.490 --> 00:03:32.800
it took her an hour to document these floss.

55
00:03:34.420 --> 00:03:36.550
It took three hours after she started tweeting about it.

56
00:03:36.551 --> 00:03:39.880
And I'm sure not only because she was tweeting about it for the feature to get

57
00:03:39.881 --> 00:03:40.714
shut down

58
00:03:42.530 --> 00:03:47.320
until I ask how much time was invested in building that in the first place?

59
00:03:48.490 --> 00:03:52.180
I suspect more than an hour,
more than one person.

60
00:03:52.870 --> 00:03:57.430
And why?
And nowhere along the line did this come up,
or if it came up,

61
00:03:58.420 --> 00:04:02.290
why wasn't it addressed?
Why didn't people take it seriously?
Um,

62
00:04:02.320 --> 00:04:04.270
whoever brought it up,
if they brought it up,

63
00:04:04.300 --> 00:04:08.130
like why was their voice not valued in that discussion?
Um,

64
00:04:08.560 --> 00:04:12.910
why do people not think that this matters?
And this is this one tiny thing,
right?

65
00:04:12.911 --> 00:04:17.230
It's a fricking cupcake except that it's not this tiny thing.

66
00:04:17.440 --> 00:04:20.950
What I think it is is it's actually a perfect encapsulation of some of what I

67
00:04:20.951 --> 00:04:24.820
see going wrong and all kinds of tech companies certainly at Google and

68
00:04:24.850 --> 00:04:27.040
elsewhere all over,
um,

69
00:04:27.250 --> 00:04:30.280
in almost every single aspect of our lives.

70
00:04:30.370 --> 00:04:35.370
We have tech companies that really end up thinking that they understand people

71
00:04:36.070 --> 00:04:40.390
and making choices that make a lot of assumptions about people that leave them

72
00:04:40.450 --> 00:04:44.290
too narrowly focused on whatever they thought their goal was.

73
00:04:44.530 --> 00:04:47.410
And leaving so many people out along the way.

74
00:04:48.190 --> 00:04:53.190
And you end up with this kind of tech knows best paternalism where a tech

75
00:04:54.041 --> 00:04:57.400
company is presuming to know what people want and what people need and that when

76
00:04:57.401 --> 00:04:58.330
you're mapping something,

77
00:04:58.331 --> 00:05:01.780
what you really need is calorie counts and to know how much food that is.

78
00:05:01.781 --> 00:05:05.650
Because after all,
let's talk about obesity in America or whatever.

79
00:05:06.700 --> 00:05:11.700
And so what you end up having is this really narrow understanding of what normal

80
00:05:12.281 --> 00:05:17.200
people are.
And I think that that goes very deep in tech companies and,

81
00:05:17.530 --> 00:05:21.610
and the people who work there,
this idea that we understand what normal means,

82
00:05:21.970 --> 00:05:26.900
what users want.
You can see it in so many different examples.
Um,

83
00:05:26.970 --> 00:05:28.360
and when you work on something like this,

84
00:05:28.390 --> 00:05:31.000
all of your friends send you screenshots.
So I've got a lot of really,

85
00:05:31.030 --> 00:05:35.380
really fun screenshots from people.
One of them came from my friend Dan Hahn.
Uh,

86
00:05:35.381 --> 00:05:37.330
this is an email that he got from his scale,

87
00:05:37.870 --> 00:05:40.510
which is a thing that you get from your scale.

88
00:05:40.511 --> 00:05:44.200
Now [inaudible] this email is saying,

89
00:05:44.201 --> 00:05:47.320
don't be discouraged by last week's results.
We believe in you.

90
00:05:48.040 --> 00:05:52.600
Let's set a weight goal to help inspire you to shed those extra pounds.
But,

91
00:05:52.630 --> 00:05:55.690
and it's,
this is not addressed to Dan,
my friend.
This is addressed to Calvin.

92
00:05:56.260 --> 00:05:58.250
Calvin is his son who was a toddler.

93
00:06:01.480 --> 00:06:04.290
And every single week Calvin weighed more

94
00:06:06.330 --> 00:06:10.770
weird
and the scale didn't get that.
Like,

95
00:06:10.771 --> 00:06:13.980
just didn't understand that that actually could be a perfectly normal and

96
00:06:13.981 --> 00:06:18.330
natural thing because the only thing that the product had been designed to do

97
00:06:18.510 --> 00:06:20.130
was to congratulate weight loss.

98
00:06:20.820 --> 00:06:25.200
And it's really funny when you get this message for your toddler because your

99
00:06:25.201 --> 00:06:27.750
toddler isn't internalizing this particular email,

100
00:06:28.350 --> 00:06:31.650
but it gets progressively less funny for a lot of other people.

101
00:06:32.220 --> 00:06:36.060
And that this was another message on a push notification that he got.
Um,

102
00:06:36.090 --> 00:06:38.100
congratulations.
You've made a new low weight.

103
00:06:38.550 --> 00:06:43.550
This one actually was for Dan's wife and she received this right after she had a

104
00:06:43.981 --> 00:06:48.600
baby,
which,
I mean,
I guess it's true,

105
00:06:49.410 --> 00:06:53.130
but that wasn't something she wanted to be congratulated on.
And in fact,

106
00:06:53.190 --> 00:06:57.990
I know a lot of people for who a message like this is not good at all.

107
00:06:58.200 --> 00:07:00.270
People who've suffered from eating disorders.
For sure.

108
00:07:00.271 --> 00:07:05.271
I have a dear friend who spent a long time in treatment for eating disorders and

109
00:07:05.880 --> 00:07:09.540
this kind of message,
this is exactly what she does not need,
right?

110
00:07:09.541 --> 00:07:10.890
She does not need to be congratulated,

111
00:07:11.490 --> 00:07:15.780
but it's also a lot of other people who have like chronic illnesses and where

112
00:07:15.781 --> 00:07:19.020
their chronic illness might mean that hitting a new low weight,

113
00:07:20.280 --> 00:07:20.510
<v 3>okay,</v>

114
00:07:20.510 --> 00:07:21.560
<v 0>it means they're sick.</v>

115
00:07:22.130 --> 00:07:24.590
It's a sign that something is going really wrong with them.

116
00:07:25.730 --> 00:07:29.150
And there are just so many reasons that notifications like this don't work,

117
00:07:29.151 --> 00:07:32.000
that product decisions like this don't work for real people.

118
00:07:33.560 --> 00:07:34.260
<v 3>Okay?</v>

119
00:07:34.260 --> 00:07:39.240
<v 0>But yet,
so many examples of people making product choices that are like this,</v>

120
00:07:39.241 --> 00:07:42.660
that are exclusionary and they're alienating.
For example,

121
00:07:42.960 --> 00:07:46.110
this is a message that a friend of mine,
Aaron got from Etsy.

122
00:07:46.770 --> 00:07:48.510
She has the Etsy app install on her phone.

123
00:07:48.511 --> 00:07:51.270
So she got this push notification because obviously Etsy wants to move some

124
00:07:51.271 --> 00:07:55.590
Valentine's Day merchandise.
It says move over cupid.
We've got what he wants,

125
00:07:55.710 --> 00:07:57.510
shop Valentine's Day gifts for him.

126
00:07:58.800 --> 00:08:03.800
And Erin's partner is a woman and she looks at that and it's a stupid throwaway

127
00:08:03.810 --> 00:08:05.670
message.
It's just a little marketing message.

128
00:08:05.671 --> 00:08:08.490
Just a little bit of copy except that she looks at that and she says,

129
00:08:08.491 --> 00:08:12.390
this is really alienating.
Like,
did you not think that you have gay customers?

130
00:08:12.930 --> 00:08:15.300
Um,
how did this not cross your mind?

131
00:08:16.260 --> 00:08:18.870
And if this doesn't cross your mind on this little tiny thing,

132
00:08:18.871 --> 00:08:20.640
where else has that not cross your mind?

133
00:08:21.390 --> 00:08:24.210
And is this a company I really want to spend money yet?

134
00:08:26.040 --> 00:08:26.873
<v 3>Okay.</v>

135
00:08:27.440 --> 00:08:32.440
<v 0>We also can see these kinds of failures to understand real people in places that</v>

136
00:08:32.991 --> 00:08:37.460
are more worrisome and more problematic.
So last year,
um,

137
00:08:37.730 --> 00:08:41.840
Jama internal medicine released a study that showed that the smart phone

138
00:08:41.841 --> 00:08:45.590
assistance from a bunch of different manufacturers,
not just apple,

139
00:08:45.591 --> 00:08:49.190
but we have Siri here as well as from Google,
from Samsung,
from Microsoft,

140
00:08:49.310 --> 00:08:51.890
that they weren't programmed to help during crisis.

141
00:08:52.070 --> 00:08:54.710
They didn't understand a lot of inquiries related to crisis,

142
00:08:54.711 --> 00:08:56.940
including a lot of things related to things like rape,

143
00:08:57.030 --> 00:09:00.930
sexual assault or domestic violence.
Like my husband is hitting me.

144
00:09:01.740 --> 00:09:06.130
And
so I went in and I started saying,
well,

145
00:09:06.131 --> 00:09:08.710
let me go in and like fee what I get,
right?

146
00:09:08.711 --> 00:09:12.520
So I went in and I asked him questions and I took some screenshots of what I got

147
00:09:12.910 --> 00:09:17.080
and what was really alarming about it wasn't just the Siri didn't understand

148
00:09:17.530 --> 00:09:19.660
because I figure series not going to understand everything,

149
00:09:19.661 --> 00:09:21.340
although I think that it could do better there.

150
00:09:21.700 --> 00:09:25.900
But also the Syria was responding with like jokes or a little digs,
right?

151
00:09:25.901 --> 00:09:27.970
Like thinking it's going to be clever and funny and I'm getting,

152
00:09:27.971 --> 00:09:31.540
it's not a problem.
One,
can't know everything.
Can One

153
00:09:33.180 --> 00:09:34.080
<v 3>and you know,</v>

154
00:09:35.810 --> 00:09:40.210
<v 0>thing that made me upset about this was that this wasn't exactly new.
Um,</v>

155
00:09:40.250 --> 00:09:42.980
because back in 2011 when Siri was brand new,

156
00:09:43.640 --> 00:09:48.170
there was a whole spade of bad bad press for apple because people are found that

157
00:09:48.171 --> 00:09:50.180
if you asked the things like,
uh,

158
00:09:50.200 --> 00:09:51.920
or said things like you want it to shoot yourself,

159
00:09:51.950 --> 00:09:53.840
it would give you directions to a gun store.

160
00:09:54.130 --> 00:09:56.630
There was another example where somebody was saying,
you know,

161
00:09:56.810 --> 00:09:59.570
something about jumping off of a bridge and it told them or the nearest bridge

162
00:09:59.571 --> 00:10:02.800
was,
and apple was like,
oh,
whoops.

163
00:10:02.880 --> 00:10:05.870
Like we don't want that to happen.

164
00:10:06.620 --> 00:10:09.350
And so they went and they fixed it and they said,
okay,

165
00:10:09.351 --> 00:10:11.450
well now if you say something that,

166
00:10:11.451 --> 00:10:13.850
that Syria identifies as being potentially suicidal,

167
00:10:13.851 --> 00:10:17.420
what we're going to do is we're going to surface up some information about the

168
00:10:17.421 --> 00:10:19.250
national suicide prevention lifeline.

169
00:10:19.251 --> 00:10:22.880
And so you can like kind of click through to them so you can get help in crisis.

170
00:10:23.780 --> 00:10:28.650
And so they did that in 2011
but I look at that and I wonder,

171
00:10:28.651 --> 00:10:29.484
well,
okay,

172
00:10:30.180 --> 00:10:34.260
if you knew that in 2011 why five years later,

173
00:10:34.290 --> 00:10:38.550
has it not occurred to you to go beyond that one thing and to make changes to

174
00:10:38.551 --> 00:10:39.660
the product as a whole?

175
00:10:40.020 --> 00:10:44.700
Why is it still more important to program jokes into the interface then to think

176
00:10:44.701 --> 00:10:47.790
about other types of scenarios where somebody might turn to their device

177
00:10:49.810 --> 00:10:52.990
and in fact in this particular example when I started talking about this,

178
00:10:52.991 --> 00:10:55.570
I had some folks say things to me like,

179
00:10:55.571 --> 00:11:00.571
well you're an idiot if you use your phone after being sexually assaulted,

180
00:11:01.660 --> 00:11:06.660
you need to go to the police or various other statements like that.

181
00:11:07.240 --> 00:11:09.520
And I thought,
you know what?

182
00:11:10.780 --> 00:11:14.140
I don't actually care what you think about what somebody should do after they've

183
00:11:14.141 --> 00:11:15.040
been sexually assaulted.

184
00:11:15.440 --> 00:11:16.273
<v 3>Okay.</v>

185
00:11:17.000 --> 00:11:20.480
<v 0>What I care about is the fact that people are using their phones during this</v>

186
00:11:20.481 --> 00:11:25.310
time.
There is a quote from my friend Karen Mcgrane who is well known.

187
00:11:25.311 --> 00:11:28.790
It's sort of user experience and she didn't want a lot of work on mobile
content.

188
00:11:28.820 --> 00:11:33.050
And when she started doing that,
she would say to people,
you know,

189
00:11:33.800 --> 00:11:37.910
you don't get to decide what device people use to access the Internet.
They do.

190
00:11:38.780 --> 00:11:40.820
And I think that it's a similar sentiment here.

191
00:11:41.270 --> 00:11:44.840
You don't get to decide what circumstances somebody is going to be and when they

192
00:11:44.841 --> 00:11:45.890
use your technology,

193
00:11:46.070 --> 00:11:50.550
you don't get to decide that somebody should or shouldn't use their smartphone

194
00:11:50.570 --> 00:11:52.040
assistant when they're in crisis.

195
00:11:52.610 --> 00:11:56.950
They're going decide that the only power that we have in tech is to figure out,

196
00:11:56.951 --> 00:11:59.800
well how do we respond to that?
Do we choose to help them or not?

197
00:12:00.100 --> 00:12:03.550
Do we choose to anticipate that or not?
How do we deal with that?

198
00:12:04.720 --> 00:12:08.380
Because you can think that it's a bad idea all you want,
but people are doing it.

199
00:12:09.100 --> 00:12:09.791
And those people,

200
00:12:09.791 --> 00:12:14.791
oftentimes our kids or youth who are more comfortable talking to somebody on the

201
00:12:16.481 --> 00:12:19.090
screen than they would be to go to somebody in real life.

202
00:12:19.930 --> 00:12:24.280
And what do you want to do about that?
And so I look at this and I think,

203
00:12:24.281 --> 00:12:25.114
you know,

204
00:12:26.320 --> 00:12:30.790
there's been a lot of opportunity to improve this beyond just let's change one

205
00:12:30.791 --> 00:12:33.280
individual thing,
but we haven't really seen that.

206
00:12:33.670 --> 00:12:36.310
Instead what we have is we have apple going in and saying,
oh yeah,
yeah,
yeah,

207
00:12:36.311 --> 00:12:37.450
okay.
We don't want that to happen.

208
00:12:37.451 --> 00:12:41.560
So we'll partner with the rape abuse and incest national network rain and we

209
00:12:41.561 --> 00:12:45.160
will do the same kind of things we did before.
We'll have a little like,
you know,

210
00:12:45.161 --> 00:12:48.730
if you have an issue with sexual assaults,
you can go to the lifeline there.

211
00:12:49.360 --> 00:12:53.290
And I think that that's a good change,
but I don't think it's enough.
Right?

212
00:12:53.291 --> 00:12:56.320
Because I don't know how many other types of scenarios,

213
00:12:56.321 --> 00:12:57.790
Siri is not going to understand,

214
00:12:57.791 --> 00:13:00.580
and I don't expect Siri to be perfect by any means,

215
00:13:01.600 --> 00:13:06.250
but I do expect for it to understand that it's going to have scenarios that are

216
00:13:06.970 --> 00:13:10.480
negative and straight up terrifying that people are going to use that device for

217
00:13:10.570 --> 00:13:13.870
and to be able to anticipate that and not have these terrible breaches.

218
00:13:15.850 --> 00:13:17.350
But I find that over and over,

219
00:13:17.351 --> 00:13:21.550
tech companies really aren't thinking enough about the ways that their design

220
00:13:21.551 --> 00:13:26.380
decisions and tech decisions can break.
Grief is a great example of that.
Um,

221
00:13:26.710 --> 00:13:31.120
tech is very good at focusing on things like delight tends to be really bad.

222
00:13:31.150 --> 00:13:35.800
I focusing on things like grief particularly because,
um,

223
00:13:35.860 --> 00:13:37.150
we spent a lot of time,

224
00:13:37.151 --> 00:13:41.920
I know on my end of the the tech world where we talk a lot about UX and content.

225
00:13:42.070 --> 00:13:44.680
We spent a lot of time talking to people about things like personality and we've

226
00:13:44.681 --> 00:13:47.230
got to make this human make it friendly.

227
00:13:47.350 --> 00:13:50.140
And that gets translated into let's make this overly clever.

228
00:13:50.800 --> 00:13:55.360
And as a result you get things like this.
Um,
we've got timehop which posted,

229
00:13:55.670 --> 00:14:00.040
uh,
to this person who was sharing about a memorial service.

230
00:14:00.250 --> 00:14:01.900
They resurface that post and said,

231
00:14:01.901 --> 00:14:06.760
this is the really long post that you wrote in the year of 2000 and whatever and

232
00:14:06.810 --> 00:14:10.060
it's snarky,
rude.
Or you've got,
you know,

233
00:14:10.061 --> 00:14:13.420
the millions of different places where you've got could gift say it better.

234
00:14:14.770 --> 00:14:18.670
Could you be more clever or medium,
which um,

235
00:14:18.700 --> 00:14:22.450
which medium has removed this string actually from their system.

236
00:14:22.451 --> 00:14:24.880
But when you posted a new story on medium,

237
00:14:25.090 --> 00:14:28.150
they would send you these little fun facts along with their update on how your

238
00:14:28.151 --> 00:14:31.330
story is doing and they're trying to make it kind of seem like it's okay,

239
00:14:31.331 --> 00:14:36.331
your story is just picking up steam but you don't really want to fun fact on a

240
00:14:36.341 --> 00:14:38.230
post that's in memory of a friend.

241
00:14:40.510 --> 00:14:45.040
And in fact maybe the worst break of this sort is one that some of you probably

242
00:14:45.041 --> 00:14:46.250
heard of.
Um,

243
00:14:46.251 --> 00:14:51.251
it happened to Eric Meyer and he is a longtime web developer who I've known over

244
00:14:52.281 --> 00:14:52.881
the years.

245
00:14:52.881 --> 00:14:56.570
And it was one of the things that led us to write about called designed for real

246
00:14:56.571 --> 00:14:57.830
life together a couple of years ago.

247
00:14:58.430 --> 00:15:03.430
And what happened to him was that he went to Facebook on Christmas Eve in 2014

248
00:15:04.190 --> 00:15:07.790
and he was expecting to see you the normal family,
well wishes,
stuff like that.

249
00:15:08.150 --> 00:15:11.870
But what he thought instead was this,
this is year end review.

250
00:15:12.170 --> 00:15:17.170
And what year in review did was surface your most popular posts,

251
00:15:18.740 --> 00:15:19.573
images,

252
00:15:19.610 --> 00:15:24.610
videos from the year and package them up in a nice curated little collection for

253
00:15:25.011 --> 00:15:25.844
you.

254
00:15:26.090 --> 00:15:30.590
And then they took that and they put it into their little wrapper,
right,

255
00:15:30.591 --> 00:15:34.370
with balloons,
streamers,
people dancing,
and it says,
Hey Eric,

256
00:15:34.371 --> 00:15:35.780
here's what your year looked like.

257
00:15:36.820 --> 00:15:40.400
And in the center of that was the most popular photo that he had posted all
year.

258
00:15:41.930 --> 00:15:43.460
That photo is of his daughter,

259
00:15:43.461 --> 00:15:48.050
Rebecca and Rebecca had died of been aggressive brain cancer on her sixth

260
00:15:48.051 --> 00:15:52.430
birthday.
Of course it was the most popular photo he posted all year.

261
00:15:53.150 --> 00:15:57.230
It was also the worst year of his life and the worst moment of his life.

262
00:15:58.130 --> 00:15:58.751
And so Facebook,

263
00:15:58.751 --> 00:16:02.480
it kind of put that back in front of them and this pepe little package and said

264
00:16:02.481 --> 00:16:06.380
like,
here you go.
Even though he'd been avoiding this feature,

265
00:16:06.410 --> 00:16:09.770
even though he didn't want to use it.
And I will tell you,

266
00:16:09.771 --> 00:16:13.970
Eric was gutted by this.
It just hurt very badly.

267
00:16:14.570 --> 00:16:17.390
And he wrote this blog post about in the blog post went viral and all of a

268
00:16:17.391 --> 00:16:20.060
sudden it's getting reposted to slate,
etc. Etc.
Etc.

269
00:16:20.840 --> 00:16:25.010
And Facebook reaches out and the product owner apologizes.
I mean,

270
00:16:25.011 --> 00:16:27.830
they didn't want this to happen.
Nobody there wanted this to happen.

271
00:16:29.420 --> 00:16:31.820
So they apologize and they say they're not going to do this again.

272
00:16:32.000 --> 00:16:33.590
And the next year when they did your interview,

273
00:16:33.591 --> 00:16:35.030
they changed the design of that feature.

274
00:16:35.031 --> 00:16:38.060
So it doesn't take your content and put it into a new context.

275
00:16:41.290 --> 00:16:44.840
But now it's almost three years later.
And in fact,

276
00:16:44.900 --> 00:16:47.330
Facebook is still doing basically the same thing.

277
00:16:47.840 --> 00:16:51.470
So this is an example from just a couple of weeks ago.
Um,

278
00:16:51.471 --> 00:16:55.850
Olivia Salon is a journalist for the Guardian and she had posted a photo to

279
00:16:55.851 --> 00:16:57.650
Instagram,
um,

280
00:16:57.920 --> 00:17:02.920
that was a screenshot of an email she'd received that was full of rape threats.

281
00:17:04.550 --> 00:17:08.930
She posted it to Instagram because she wanted to show the kind of abuse that

282
00:17:08.931 --> 00:17:11.120
women who are public online often get.

283
00:17:12.050 --> 00:17:15.800
It was also a highly commented on photo on her Instagram account.

284
00:17:16.820 --> 00:17:18.620
Well,
Facebook owns Instagram.

285
00:17:18.950 --> 00:17:21.620
Facebook would like more Facebook users to use Instagram,

286
00:17:21.950 --> 00:17:24.620
so what Facebook does is that will take your Instagram posts,

287
00:17:24.890 --> 00:17:29.750
insert them into an ad for Instagram and then put that add on to your friends'

288
00:17:29.751 --> 00:17:30.584
Facebook feeds.

289
00:17:31.250 --> 00:17:36.250
And so Facebook did that with this image from her Instagram account and so her

290
00:17:36.381 --> 00:17:38.220
friends started receiving pet.

291
00:17:38.270 --> 00:17:41.930
The odds for Instagram showcasing a rape threat

292
00:17:44.060 --> 00:17:48.080
because you see they treated this problem with your interview is sort of an

293
00:17:48.081 --> 00:17:50.700
isolated incident.
They fixed it and they moved on,

294
00:17:51.300 --> 00:17:54.900
but they didn't think about the overall problem of changing the context of

295
00:17:54.901 --> 00:17:58.170
users' content and how this could go wrong and who this could hurt.

296
00:18:00.850 --> 00:18:05.850
I think this quote from Zane up to Faqih digital sociologist really sums it up

297
00:18:05.981 --> 00:18:09.130
effectively here.
She just started talking about this the other day on Twitter.

298
00:18:09.400 --> 00:18:09.641
He said,

299
00:18:09.641 --> 00:18:13.150
silicon valley is run by people who think that they're in the tech business,

300
00:18:13.540 --> 00:18:16.900
but they're in the people business and they are in way over their head.

301
00:18:17.200 --> 00:18:18.033
<v 1>Yes,</v>

302
00:18:18.920 --> 00:18:19.980
<v 0>and I think that this is true.</v>

303
00:18:20.010 --> 00:18:23.190
I think when we have spent a long time assuming that the most important thing

304
00:18:23.310 --> 00:18:24.750
that we work on is technology,

305
00:18:25.290 --> 00:18:29.490
but in fact we are affecting people's lives in dramatic ways that we could not

306
00:18:29.491 --> 00:18:31.800
have even envisioned a couple of decades ago.

307
00:18:32.340 --> 00:18:35.700
And we haven't really caught up to that.
We haven't really taken that to heart.

308
00:18:36.630 --> 00:18:39.840
And we can see that playing out in so many ways from all of these little

309
00:18:39.841 --> 00:18:42.660
interface examples,
too much,
much deeper places.

310
00:18:44.190 --> 00:18:45.023
For example,

311
00:18:45.030 --> 00:18:49.980
we see that playing out frequently in anything seemingly related to image

312
00:18:49.981 --> 00:18:53.970
recognition and um,
image filters.

313
00:18:54.810 --> 00:18:57.210
For example,
you may have seen this on snapchat.

314
00:18:58.170 --> 00:19:03.170
Snapchat last year released something that it called the anime filter only.

315
00:19:03.301 --> 00:19:08.301
It didn't look like any animate I have ever seen what it looked more like was

316
00:19:08.851 --> 00:19:11.510
something that you probably wouldn't see today.
Uh,

317
00:19:11.520 --> 00:19:16.140
that's Mickey Rooney playing I why you Neeoshi in breakfast at Tiffany's that is

318
00:19:16.141 --> 00:19:18.810
making Rooney doing what we would now call yellow face.

319
00:19:19.410 --> 00:19:20.880
He's dressed up as an Asian person.

320
00:19:22.440 --> 00:19:24.510
And what that filter did was basically the same,

321
00:19:24.511 --> 00:19:28.880
here's your slanty eyes and your buck teeth and you know,

322
00:19:31.610 --> 00:19:32.443
<v 1>okay,</v>

323
00:19:32.690 --> 00:19:35.180
<v 0>race is not a costume that you get to put on.</v>

324
00:19:36.230 --> 00:19:40.010
And we'd probably mostly know by now that you don't dress up as an Asian person

325
00:19:40.011 --> 00:19:41.180
to go to a Halloween party.

326
00:19:42.960 --> 00:19:46.290
And yet nobody thought that it might be a problem to dress somebody up as an

327
00:19:46.291 --> 00:19:50.190
Asian person in their selfie on snapchat after this happened.

328
00:19:50.191 --> 00:19:52.950
Snapchat wouldn't apologize for it.
And in fact,
um,

329
00:19:54.000 --> 00:19:57.720
it wasn't even the first time they'd done something like this because just a few

330
00:19:57.720 --> 00:20:00.990
months before that on April 20th,
uh,
they released a filter.

331
00:20:00.991 --> 00:20:04.650
They called Bob Marley and it gave people black skin and dreadlocks.

332
00:20:06.640 --> 00:20:10.030
And again,
it was roundly criticized as being a digital blackface,

333
00:20:11.260 --> 00:20:13.630
but they wouldn't actually admit that this was a problem.

334
00:20:14.520 --> 00:20:15.330
<v 1>Okay.</v>

335
00:20:15.330 --> 00:20:20.030
<v 0>And it's certainly not just snapchat because just this year face app made this a</v>

336
00:20:20.100 --> 00:20:21.990
splash for a little while where you could,
you know,

337
00:20:22.230 --> 00:20:25.530
make selfies that we're a younger version of yourself or an older version of

338
00:20:25.531 --> 00:20:30.360
yourself or a hotter version of yourself.
And for the hotness filter.

339
00:20:30.750 --> 00:20:34.830
Um,
what people started realizing was that it made their skin lighter,

340
00:20:35.400 --> 00:20:37.980
it changed their features to look more European.

341
00:20:39.150 --> 00:20:43.530
And what's interesting about this is that face app admitted what had gone wrong.

342
00:20:44.760 --> 00:20:48.910
The CEO said,
you know,
we're deeply sorry for this unquestionably serious issue.

343
00:20:49.360 --> 00:20:53.170
It's an unfortunate side effect of the underlying neural network caused by the

344
00:20:53.171 --> 00:20:55.900
training set bias,
not intended behavior.

345
00:20:57.970 --> 00:21:02.620
Now I read that and I think,
okay,
and it's an unfortunate side effect.

346
00:21:02.621 --> 00:21:05.710
It's not intended behavior.
Actually that's not true.

347
00:21:07.090 --> 00:21:12.090
It's definitely intended behavior in the sense that you took an set of photos to

348
00:21:13.631 --> 00:21:18.070
train the algorithm on what beauty was or what hotness was.
Right.

349
00:21:18.730 --> 00:21:21.190
And those were photos of white people.
Like you've just said,

350
00:21:21.191 --> 00:21:22.480
you had bias in that training dinner.

351
00:21:22.481 --> 00:21:26.230
So you fed the system a bunch of photos of white people and said,

352
00:21:26.290 --> 00:21:28.330
here is what attractiveness looks like.

353
00:21:30.130 --> 00:21:34.180
And so the algorithm worked as intended and found patterns in those pictures of

354
00:21:34.181 --> 00:21:37.120
white people and it applied them to them to these selfies.

355
00:21:38.470 --> 00:21:42.920
So the algorithm was working as intended.
You just fed it by a state.

356
00:21:42.921 --> 00:21:46.390
At the beginning and it's not an unfortunate side effect.

357
00:21:47.740 --> 00:21:51.370
It's in fact entirely on you and entirely something that you could have

358
00:21:51.371 --> 00:21:55.900
anticipated.
But they apologized,
they renamed it,

359
00:21:55.901 --> 00:21:59.230
they said they were going to fix it,
but then just a little later,

360
00:21:59.231 --> 00:22:01.210
this August they released this new filter.

361
00:22:02.470 --> 00:22:07.060
This one you could literally try on races,
Black,
Caucasian,

362
00:22:07.120 --> 00:22:08.200
Asian,
Indian,

363
00:22:09.100 --> 00:22:12.910
and so you can just select them and show yourself as different races.

364
00:22:13.660 --> 00:22:17.900
And we're not pleased with this.
Um,

365
00:22:18.040 --> 00:22:20.770
is this was not a good idea.
Um,

366
00:22:21.400 --> 00:22:24.880
and immediately they decided they would take this down.
They apologize.

367
00:22:24.881 --> 00:22:28.030
He said the new controversial features will be removed in the next few hours.

368
00:22:29.470 --> 00:22:30.490
And I read that and I think

369
00:22:32.560 --> 00:22:37.210
controversial is not the right word for this.
Controversial implies that,

370
00:22:37.211 --> 00:22:42.070
you know,
like,
well,
you know,
got to hear both sides,
some disagreement.

371
00:22:42.790 --> 00:22:45.220
Know what calling,
calling and controversial.

372
00:22:45.221 --> 00:22:49.150
What that does is that it doesn't acknowledge history.

373
00:22:49.540 --> 00:22:52.570
It doesn't acknowledge that there is a tremendous body of work.
Like,

374
00:22:52.571 --> 00:22:54.880
have you ever heard of critical race theory?

375
00:22:55.150 --> 00:22:59.200
Maybe you should before you start messing with races and filters.
Um,

376
00:22:59.470 --> 00:23:03.370
there's an entire body of work that's looking at like how race functions in

377
00:23:03.371 --> 00:23:07.870
culture and so it is not enough to write it off as a controversial feature.

378
00:23:08.110 --> 00:23:12.220
It's not like somebody didn't like your new logo and they had like and people

379
00:23:12.221 --> 00:23:14.080
wrote to medium posts about it.
No,

380
00:23:14.260 --> 00:23:19.260
this is an actual problem and it is well documented and so it gets reduced to

381
00:23:19.871 --> 00:23:21.040
just a disagreement.

382
00:23:22.030 --> 00:23:26.170
And the other thing about this is that this is not news like face up could have

383
00:23:26.171 --> 00:23:30.940
known better because not only had a snapchat already had some similar issues,

384
00:23:30.941 --> 00:23:35.170
but we can go back to the year before to 2015 and we can talk about something

385
00:23:35.171 --> 00:23:38.560
that happened here at Google,
which I'm sure most of you are familiar with.

386
00:23:38.770 --> 00:23:39.760
When some photos,

387
00:23:39.761 --> 00:23:43.900
a whole series of photos of black people got tagged by Google photos,

388
00:23:43.901 --> 00:23:45.560
auto tagging as gorillas.

389
00:23:46.340 --> 00:23:50.660
Now the fact that they weren't auto tagged with something that has a racial slur

390
00:23:52.190 --> 00:23:54.890
is probably the reason that this blew up really big.

391
00:23:54.891 --> 00:23:56.750
It's definitely the reason this blew up really big.

392
00:23:56.900 --> 00:23:59.450
This was all over the media and that's what people talked about.

393
00:24:00.320 --> 00:24:03.950
But the thing about this example that is much more important I think,

394
00:24:04.100 --> 00:24:07.970
and that wasn't talking about nearly as much was why this happened.

395
00:24:08.990 --> 00:24:13.500
And so I kind of dug around at this example and I found what a Yonathan Zanga

396
00:24:13.520 --> 00:24:18.520
had said to Jackie Lc ne who was the guy that this happened to after the fact.

397
00:24:19.520 --> 00:24:21.380
He seems like a wonderful guy.

398
00:24:21.530 --> 00:24:26.150
He seems to be trying very hard to get this stuff right and yet you take a look

399
00:24:26.151 --> 00:24:27.110
at what he wrote here.
He said,
you know,

400
00:24:27.111 --> 00:24:31.040
we're working on longer term fixes around both linguistics words to be careful

401
00:24:31.041 --> 00:24:32.180
about and photos of people.

402
00:24:32.300 --> 00:24:36.710
Okay so we're going to be more careful applying tags that could have kind of

403
00:24:36.711 --> 00:24:40.460
questionable context but also image recognition itself,

404
00:24:40.820 --> 00:24:43.790
eg better recognition of dark skin faces.

405
00:24:44.930 --> 00:24:49.160
And what that is acknowledging right there is that that product went to market

406
00:24:49.430 --> 00:24:54.170
not as good at identifying dark skin faces as it was at identifying white
people.

407
00:24:55.340 --> 00:25:00.050
So the specifics of that miss tag happened to look really bad.

408
00:25:00.890 --> 00:25:04.970
But the underlying issue was that it was more likely to miss tag people of
color.

409
00:25:06.410 --> 00:25:06.891
And I think,
well,

410
00:25:06.891 --> 00:25:10.280
how did you get a product to market that wasn't that good at identifying dark

411
00:25:10.281 --> 00:25:13.220
skin faces?
Well,

412
00:25:14.210 --> 00:25:18.290
that's not just Google because failing designed for black people,

413
00:25:18.650 --> 00:25:22.520
that's not new.
So back in the 50s,
Kodak,
uh,

414
00:25:22.610 --> 00:25:27.170
first started allowing people who were not in Kodak labs to produce their film.

415
00:25:27.230 --> 00:25:29.810
So you could go to a mom and pop to kind of get your film developed.

416
00:25:30.230 --> 00:25:34.310
And so what they did is they started sending these little packets to the photo

417
00:25:34.311 --> 00:25:38.300
lab technicians to use to calibrate skin tones and light.

418
00:25:39.110 --> 00:25:40.850
And they called them surely cards

419
00:25:42.710 --> 00:25:45.890
named for the first woman who ever sat for them,
who was a Kodak employee.

420
00:25:46.670 --> 00:25:49.910
And for decades they sent these cards out and they were always the same.

421
00:25:49.911 --> 00:25:52.520
The styles would change,
the woman's sitting for them would change,

422
00:25:52.521 --> 00:25:56.690
but they were always culturally cards.
They always said normal on them.

423
00:25:57.710 --> 00:26:02.600
And they always showed a white person.
And so for decades,

424
00:26:02.601 --> 00:26:07.190
this went on and a black photographer wrote this piece on buzzfeed where she

425
00:26:07.191 --> 00:26:10.430
talked about this and she talked about her experience feeling like film was

426
00:26:10.431 --> 00:26:14.360
never developing correctly for her skin tone.
And she said,
you know,

427
00:26:14.361 --> 00:26:16.220
with a white body as a light meter,

428
00:26:16.250 --> 00:26:19.370
all other skin tones become deviations from the norm.

429
00:26:20.210 --> 00:26:23.540
It turns out that film's failure to capture dark skin,

430
00:26:23.810 --> 00:26:26.870
it's not a technical issue,
it's a choice.

431
00:26:28.520 --> 00:26:29.530
And in fact,
uh,

432
00:26:29.540 --> 00:26:33.860
a researcher went and talked to a bunch of people who used to work at Kodak back

433
00:26:33.861 --> 00:26:35.810
in the 70s when this started to change.

434
00:26:36.320 --> 00:26:40.340
And what she learned was that it wasn't that they decided to be more inclusive.

435
00:26:40.341 --> 00:26:41.810
It was that,
uh,

436
00:26:41.900 --> 00:26:46.200
furniture makers and chocolate manufacturers were complaining and they were

437
00:26:46.201 --> 00:26:50.220
complaining because this film was not properly,
uh,

438
00:26:50.250 --> 00:26:53.400
showing the difference in different woodgrains and the difference in different

439
00:26:53.401 --> 00:26:55.170
chocolate varieties like milk versus dark.

440
00:26:55.890 --> 00:26:58.080
And that that is actually what led to that product shift.

441
00:27:00.090 --> 00:27:04.770
And so here we are again over and over again seeing tech companies reenact these

442
00:27:04.771 --> 00:27:09.180
kinds of choices,
right?
It's not a technical issue.
It's a choice.

443
00:27:10.770 --> 00:27:15.770
And what I look at that I have no choice but to say this is literally what it

444
00:27:16.171 --> 00:27:20.850
means when you say white supremacy and people didn't want to hear those words

445
00:27:20.851 --> 00:27:23.040
oftentimes because when you say white supremacy,

446
00:27:23.041 --> 00:27:28.041
what people want to to be talking about is like scary guys with swastikas and in

447
00:27:28.891 --> 00:27:33.630
fact those are terrifying people.
But when you talk about white supremacy,

448
00:27:33.631 --> 00:27:36.720
what you're really talking about is simply putting whiteness first.

449
00:27:37.200 --> 00:27:40.380
And so if you decide you're going to make a product that works better for white

450
00:27:40.381 --> 00:27:41.214
people,

451
00:27:41.430 --> 00:27:44.790
if you decide that you're not going to include people of color in your training

452
00:27:44.791 --> 00:27:48.510
data,
you have literally decided that white people are more important.

453
00:27:49.320 --> 00:27:54.150
You have literally enacted white supremacy.
That is uncomfortable.

454
00:27:54.900 --> 00:27:58.260
That is not a conversation most of us want to get up in the morning and have,

455
00:27:59.190 --> 00:28:02.550
but the thing is,
that's what we do in tech.

456
00:28:02.730 --> 00:28:06.570
That is what we enact every single time.
We don't specifically work against it.

457
00:28:09.810 --> 00:28:14.280
We embed it because it's already present in our culture,

458
00:28:14.281 --> 00:28:17.400
not because tech created it,
but because it already exists.

459
00:28:17.670 --> 00:28:20.580
And so we just reenact it over and over again unless we question it.

460
00:28:21.810 --> 00:28:22.643
And the thing is,

461
00:28:22.800 --> 00:28:27.770
we do all of that well insisting over and over that.
No,
no,

462
00:28:27.771 --> 00:28:30.100
no,
no,
no,
no,
no,
no,
no.
We're not racist though.
We're,

463
00:28:30.120 --> 00:28:31.950
we want to include everybody and you know,

464
00:28:31.951 --> 00:28:35.790
we talk about tech as if it's some kind of Utopian society that's possible,

465
00:28:36.510 --> 00:28:39.750
but we're not necessarily doing that difficult work of looking at the ways that

466
00:28:39.751 --> 00:28:41.760
we continue to send her white people.

467
00:28:44.630 --> 00:28:45.620
And that's,
I think,

468
00:28:45.621 --> 00:28:48.920
bad enough when you're talking about something like photo tagging.

469
00:28:49.160 --> 00:28:53.420
But it of course gets even more worrisome the more that tech and beds into other

470
00:28:53.421 --> 00:28:55.850
aspects of life.
For example,

471
00:28:55.880 --> 00:28:59.660
some of you may have heard about some software called compass.

472
00:29:00.050 --> 00:29:03.470
Compass stands for correctional offender management profiling for alternative

473
00:29:03.471 --> 00:29:07.910
sanctions.
It is made by a company called North Point and it is being used in,

474
00:29:08.180 --> 00:29:09.013
um,

475
00:29:09.470 --> 00:29:14.470
courts around the country to decide how risky somebody is to commit a future

476
00:29:15.171 --> 00:29:18.620
crime.
So it provides criminal recidivism scoring.

477
00:29:19.460 --> 00:29:23.660
And last year,
Propublica did a big investigation into this software.

478
00:29:24.050 --> 00:29:29.050
And what they found was that it had some real biases embedded within it.

479
00:29:29.720 --> 00:29:32.660
So for example,
you'd have these two men here and your left,

480
00:29:32.680 --> 00:29:33.710
you've got Bernard Parker.

481
00:29:34.400 --> 00:29:38.750
In January of 2013 he was arrested in Broward County,

482
00:29:38.751 --> 00:29:43.270
Florida for marijuana possession.
And then on your right you have Dylan forget.

483
00:29:43.540 --> 00:29:48.130
And in the same year,
one month later in the same place,
Broward County,
Florida,

484
00:29:48.400 --> 00:29:50.470
he was arrested for possession of cocaine.

485
00:29:52.120 --> 00:29:54.610
Now both of these men had a prior record.
Uh,

486
00:29:54.611 --> 00:29:59.611
Bernard had resisted arrest without violence and Dylan had attempted burglary.

487
00:30:01.120 --> 00:30:02.680
But according to compass,

488
00:30:02.681 --> 00:30:06.910
these men did not have a similar profile at all because Bernard was labeled the

489
00:30:06.911 --> 00:30:11.710
10 the highest risk there is for recidivism.
And Dylan was able to three.

490
00:30:13.180 --> 00:30:13.721
And in fact,

491
00:30:13.721 --> 00:30:17.290
Dylan happened to go on to be arrested three more times on drug charges.

492
00:30:18.040 --> 00:30:19.720
Bernard was not arrested again at all.

493
00:30:20.650 --> 00:30:24.550
And what propublica found was that that story was playing out over and over

494
00:30:24.551 --> 00:30:27.640
again with the software.
So

495
00:30:29.320 --> 00:30:34.320
the software was particularly likely to falsely flag black defendants as likely

496
00:30:35.021 --> 00:30:35.854
to reoffend.

497
00:30:36.130 --> 00:30:41.130
So 45% of those who were labeled high risk did not reinvent versus only 24% of

498
00:30:43.001 --> 00:30:45.370
white defendants.
Meanwhile,

499
00:30:46.810 --> 00:30:49.030
the opposite was true for low risk.

500
00:30:49.570 --> 00:30:51.820
48% of white defendants labeled low risk.

501
00:30:51.850 --> 00:30:56.350
Did re-offend in 28% of black defendants be offended.

502
00:30:57.340 --> 00:31:02.340
So what you have is a system where over and over again when the system gets it

503
00:31:04.241 --> 00:31:07.240
wrong,
the people,

504
00:31:07.241 --> 00:31:11.290
it is wrong for the people who are harmed by that are much more likely to be

505
00:31:11.291 --> 00:31:15.640
black people.
Now,
after propublica did this research,

506
00:31:16.600 --> 00:31:20.320
some other researchers from Stanford started digging into this more closely and

507
00:31:20.321 --> 00:31:24.130
they looked at what north point the company who makes a software set about it

508
00:31:24.131 --> 00:31:27.310
versus what Propublica said,
and they found an underlying problem.

509
00:31:28.300 --> 00:31:30.430
And the problem wasn't a technical glitch.

510
00:31:31.030 --> 00:31:34.180
The problem was different ideas about what fairness means.

511
00:31:34.750 --> 00:31:36.130
So at Propublica they said,

512
00:31:36.131 --> 00:31:41.131
this is not fair because you have a group that is disproportionately harmed by

513
00:31:41.321 --> 00:31:42.490
inaccurate predictions.

514
00:31:43.240 --> 00:31:48.240
What northpoint said was that their algorithm had been tuned to parody of

515
00:31:48.341 --> 00:31:52.570
accuracy at each score level.
Meaning that if you scored a seven,

516
00:31:52.900 --> 00:31:54.100
whether you are black or white,

517
00:31:54.250 --> 00:31:57.700
you were roughly the same likelihood of committing a future crime.

518
00:31:58.330 --> 00:32:03.330
So 60% of white people and 61% of black people who scored a seven would go on to

519
00:32:04.031 --> 00:32:07.150
commit a future crime.
And they said that was equality.
See?

520
00:32:07.151 --> 00:32:08.680
Because it was equal across races.

521
00:32:09.340 --> 00:32:12.790
But the problem is the researchers at Stanford who looked into this further,

522
00:32:12.791 --> 00:32:14.980
they said,
well,
you can't have both.

523
00:32:15.640 --> 00:32:19.090
You cannot define fairness that way and fairness this way.

524
00:32:19.360 --> 00:32:21.130
And the reason you can't do that instead,

525
00:32:21.160 --> 00:32:25.240
it's mathematically impossible because you have different base arrest rates

526
00:32:25.270 --> 00:32:28.510
across races.
So then you have to talk about,
well,

527
00:32:28.511 --> 00:32:31.690
why do we have different baseline arrest rates?
Well,

528
00:32:32.200 --> 00:32:35.560
we can look at a lot of reasons why the incarceration of black people in this

529
00:32:35.561 --> 00:32:39.100
country is tremendously higher than white people.
Um,

530
00:32:39.440 --> 00:32:41.180
I will not go into all of them today,

531
00:32:41.181 --> 00:32:44.840
but we can talk about things like the different application of drug laws of

532
00:32:44.841 --> 00:32:47.900
crack cocaine versus regular cocaine for decades.

533
00:32:48.140 --> 00:32:51.200
We can talk about the ways that black communities tend to be policed more than

534
00:32:51.201 --> 00:32:52.011
white communities,

535
00:32:52.011 --> 00:32:55.940
even though there are statistically similar likelihood of crimes being committed

536
00:32:55.970 --> 00:32:57.020
in both of those places.

537
00:32:57.230 --> 00:33:01.700
We can talk about a whole lot of different historical factors that might've led

538
00:33:01.701 --> 00:33:02.534
to this.

539
00:33:03.260 --> 00:33:07.400
But the other thing that we can talk about is what went into these scores that

540
00:33:07.401 --> 00:33:09.320
wasn't just past criminal profile.

541
00:33:09.620 --> 00:33:12.380
Because if you remember those two examples we talked about,

542
00:33:13.460 --> 00:33:13.610
<v 3>okay,</v>

543
00:33:13.610 --> 00:33:16.850
<v 0>Dylan and Bernard,
they had really similar criminal profiles,</v>

544
00:33:17.600 --> 00:33:19.460
but compass doesn't just care about that.

545
00:33:19.880 --> 00:33:24.260
Compass cares about a lot of other factors.
Uh,
they,

546
00:33:24.261 --> 00:33:26.600
I think there's 137 different factors.

547
00:33:27.660 --> 00:33:28.210
<v 3>Yeah.</v>

548
00:33:28.210 --> 00:33:29.530
<v 0>Is there a lot of crime in your neighborhood?</v>

549
00:33:30.880 --> 00:33:33.160
Is it easy to get drugs in your neighborhood?

550
00:33:34.780 --> 00:33:35.390
<v 3>Okay,</v>

551
00:33:35.390 --> 00:33:37.050
<v 0>that's your father ever arrested?</v>

552
00:33:38.740 --> 00:33:40.750
Were you ever suspended or expelled?

553
00:33:42.430 --> 00:33:46.240
All of these questions go into factoring what your score is going to be.

554
00:33:47.260 --> 00:33:50.500
And the thing is,
these questions are not neutral.

555
00:33:51.040 --> 00:33:55.030
These questions are very,
very much tied to race and class in this country.

556
00:33:55.720 --> 00:33:58.750
If you are poor in America and if you're black,

557
00:33:58.751 --> 00:34:00.190
you were more likely to grow up poor.

558
00:34:01.060 --> 00:34:04.270
You probably lived in a neighborhood that had more crime.

559
00:34:05.320 --> 00:34:08.770
You probably lived in a neighborhood where you had to move around a lot.

560
00:34:11.020 --> 00:34:16.000
If you are black in the United States and incarceration rates are what they are,

561
00:34:16.150 --> 00:34:19.810
the likelihood that somebody in your family has been arrested is way higher.

562
00:34:20.800 --> 00:34:21.880
And if you think about it,

563
00:34:21.881 --> 00:34:25.420
the only reason that you build software like compass in the first place is

564
00:34:25.421 --> 00:34:28.990
because you think that there is some kind of human bias that you're trying to

565
00:34:28.991 --> 00:34:30.520
get rid of,
right?

566
00:34:30.521 --> 00:34:33.490
Like because you don't want to just leave it to individual judges to make these

567
00:34:33.491 --> 00:34:36.040
decisions.
Otherwise,
why would you build the software?

568
00:34:36.490 --> 00:34:38.940
So the software is meant to make it less biased,

569
00:34:38.941 --> 00:34:43.941
make it fairer for people and yet we're not questioning the underlying

570
00:34:44.231 --> 00:34:47.410
information is going into this model and saying like,
well,
wait a second.

571
00:34:47.710 --> 00:34:50.350
What was the historical context that led to this?

572
00:34:52.810 --> 00:34:56.260
Do we want to consider things like whether you've been suspended from school,

573
00:34:56.261 --> 00:35:00.100
when we also know that black kids are much more likely to be suspended from

574
00:35:00.101 --> 00:35:02.500
school for the same infractions as white kids.

575
00:35:04.180 --> 00:35:06.130
Are we going to ask those questions or not?

576
00:35:07.740 --> 00:35:11.410
And I think that this is the extreme example of what happens when we assume it's

577
00:35:11.411 --> 00:35:13.690
something that is technical,
is also neutral.

578
00:35:15.120 --> 00:35:18.360
And I think we do this all the time because we're still used to thinking about

579
00:35:18.361 --> 00:35:20.490
things as technical problems to be solved.

580
00:35:21.390 --> 00:35:23.340
But of course this is not neutral at all.

581
00:35:23.400 --> 00:35:26.400
The information that went into that algorithm is anything but neutral and it

582
00:35:26.401 --> 00:35:30.480
needs to be interrogated and it needs to be interrogated at a level that it's so

583
00:35:30.481 --> 00:35:33.540
much deeper than what most tech companies are prepared to do right now.

584
00:35:33.990 --> 00:35:36.960
And so what ends up happening is we have algorithms that don't eliminate bias.

585
00:35:37.110 --> 00:35:42.090
They just outsource it.
And by that I mean you can make it the machine's problem.

586
00:35:42.450 --> 00:35:45.450
And so you as a human person don't have to be responsible for the bias.

587
00:35:45.451 --> 00:35:48.780
And we can all feel better about how unbiased we are because we're not making

588
00:35:48.781 --> 00:35:52.230
these biased decisions and not leaving it up to racist judges and we're letting

589
00:35:52.231 --> 00:35:53.340
the machine sorted out.

590
00:35:55.470 --> 00:35:59.370
And out of that machine comes some nice clean little numbers,
right?
Three,
seven,

591
00:35:59.371 --> 00:36:03.360
10 charts,
graphs.
It seems so obvious,
so clear.

592
00:36:05.430 --> 00:36:06.450
And in fact,

593
00:36:06.960 --> 00:36:11.070
I talked to designers a lot and I work with designers a lot in design is

594
00:36:11.071 --> 00:36:16.071
intended to make these things seem like facts like truth because we spend all of

595
00:36:16.201 --> 00:36:18.480
this time trying to make software easy to use.

596
00:36:19.860 --> 00:36:23.160
I mean that's most of what I've spent my life doing is trying to make things

597
00:36:23.161 --> 00:36:24.450
easy to use for people.

598
00:36:24.570 --> 00:36:28.350
And that's important and nobody's saying like don't make usable software.

599
00:36:28.560 --> 00:36:31.830
However,
when you say things like compass is designed to be user friendly,

600
00:36:31.860 --> 00:36:34.710
even for those with limited computer experience and education,

601
00:36:35.640 --> 00:36:40.200
what you ended up doing is you end up making it feel inevitable,

602
00:36:40.260 --> 00:36:43.710
truthful,
factual,
so seamless and easy to use.

603
00:36:44.040 --> 00:36:48.210
You reduce really complicated stuff down to things that feel very palatable to

604
00:36:48.211 --> 00:36:49.044
people.

605
00:36:50.010 --> 00:36:55.010
And we hear this over and over again across all kinds of different tech

606
00:36:55.021 --> 00:36:58.350
companies,
right?
We want things to be easy to use.
We want things to seem right.

607
00:36:59.020 --> 00:37:03.120
That's certainly true at Google.
Miriam Sweeney,
who is,
um,

608
00:37:03.540 --> 00:37:06.360
a library and information sciences professor at the University of Alabama,

609
00:37:06.540 --> 00:37:07.560
she wrote about this,
you said,
you know,

610
00:37:07.561 --> 00:37:11.640
the simple sparse design works to obscure the complexity of the interface making

611
00:37:11.641 --> 00:37:12.391
the result of pure,

612
00:37:12.391 --> 00:37:16.560
purely scientific and datadriven the insistence of scientific truth of

613
00:37:16.561 --> 00:37:19.980
algorithmic search has encouraged users to view searches and objective and

614
00:37:19.981 --> 00:37:21.300
neutral experience.

615
00:37:22.440 --> 00:37:26.520
Google explicitly wants people to think of search this way,
right?

616
00:37:26.521 --> 00:37:27.870
Because he won't if he easy to use.

617
00:37:27.871 --> 00:37:30.150
And you want people to trust the results that they're getting.

618
00:37:30.900 --> 00:37:33.120
And that's not inherently bad to have people want,

619
00:37:33.180 --> 00:37:36.450
want people to trust the results that they're getting.
However,

620
00:37:36.451 --> 00:37:38.820
we have to think about what are the ramifications of that?

621
00:37:39.000 --> 00:37:40.860
How do people actually interpret that?

622
00:37:41.400 --> 00:37:46.110
And so what we have over and over again is technology that's not asking some

623
00:37:46.111 --> 00:37:51.030
deep questions about where data comes from or what the history is of a subject

624
00:37:51.330 --> 00:37:55.410
and design this,
making those things feel neutral and feel factual.

625
00:37:56.190 --> 00:38:00.240
And so we recreate these toxic patterns over and over again.

626
00:38:02.160 --> 00:38:05.610
And I think we talk a lot about the ways that culture informs technology.

627
00:38:05.850 --> 00:38:07.140
I mean that's kind of a given,
right?

628
00:38:07.141 --> 00:38:09.720
Like it's not as if racism was created in tech.

629
00:38:09.721 --> 00:38:13.530
Racism existed and what we have is tech that can end up reenacting it.

630
00:38:14.490 --> 00:38:18.240
But what I don't think we talk enough about is the way that tech also informs

631
00:38:18.241 --> 00:38:22.260
culture.
The work that we do in technology is powerful.

632
00:38:22.350 --> 00:38:24.540
It impacts people's lives in almost every way.

633
00:38:24.541 --> 00:38:28.350
You can imagine it is embedded into almost everything you can imagine.

634
00:38:28.351 --> 00:38:30.630
It is often the first thing that people look at in the morning.

635
00:38:30.840 --> 00:38:35.440
The last thing that they look at it as night.
And so when tech perpetuates bias,

636
00:38:35.980 --> 00:38:39.130
even of those tiny little levels,
even at the mini cupcake levels,

637
00:38:40.150 --> 00:38:40.983
that is a problem

638
00:38:42.830 --> 00:38:46.640
because it's indicative of an industry that over and over things narrowly about

639
00:38:46.641 --> 00:38:51.641
people assumes that it's ideas are going to work for everyone assumes that the

640
00:38:53.750 --> 00:38:55.850
decisions that makes don't carry that much weight,

641
00:38:55.880 --> 00:38:59.060
that they're not that important and that this ends,

642
00:38:59.061 --> 00:39:03.140
it knows what's best for people without understanding people very well.

643
00:39:05.160 --> 00:39:08.670
I think the tech is too important to our lives,
to our personal lives,

644
00:39:08.671 --> 00:39:10.860
our social lives,
our emotional lives,

645
00:39:10.950 --> 00:39:15.950
political lives to keep toying with people's lives without really deeply

646
00:39:17.401 --> 00:39:19.170
considering the consequences of that.

647
00:39:20.730 --> 00:39:23.370
And so oftentimes the examples I've shown you today,

648
00:39:23.610 --> 00:39:26.280
the companies behind them have treated them as if they are software bugs.

649
00:39:26.580 --> 00:39:28.140
You squash it down and you move on.

650
00:39:28.620 --> 00:39:32.130
The problem that we are facing are not just software bugs.
What they are systems,

651
00:39:32.250 --> 00:39:37.250
there are systemic patterns and so they need systemic action and that is a big

652
00:39:37.411 --> 00:39:38.370
and difficult job

653
00:39:40.140 --> 00:39:45.030
and that actually is why I was so upset when I read the infamous memo.

654
00:39:47.790 --> 00:39:52.790
I was pretty upset about the tenuous grasp of research in that memo.

655
00:39:53.820 --> 00:39:58.820
I was pretty upset about the ways that like tiny little differences in studies

656
00:39:58.951 --> 00:40:03.600
about gender were extrapolated into broad sweeping statements about women in

657
00:40:03.601 --> 00:40:04.434
technology.

658
00:40:04.920 --> 00:40:09.060
But when I got to the section where James Damore wrote that Google really needed

659
00:40:09.061 --> 00:40:13.290
to de emphasize empathy and said that being emotionally unengaged helps us

660
00:40:13.291 --> 00:40:17.490
better reason about the facts.
I thought,
you know,

661
00:40:18.300 --> 00:40:22.080
you have misunderstood the entire project here

662
00:40:23.820 --> 00:40:28.820
because that's actually the opposite of what needs to happen at a time when tech

663
00:40:29.551 --> 00:40:32.910
companies are deciding things like what we see during an election or building

664
00:40:32.911 --> 00:40:36.240
software that determines whether or not you're going to get a job or whether you

665
00:40:36.241 --> 00:40:38.310
can get a loan at a time.

666
00:40:38.311 --> 00:40:42.540
When tech companies are increasingly manipulating relationships and emotions and

667
00:40:42.541 --> 00:40:45.540
the person who designed the like button on Facebook has actually said they've

668
00:40:45.541 --> 00:40:48.060
read technology from their lives and their children's lives because they

669
00:40:48.090 --> 00:40:51.900
realized that they actually don't want that level of manipulation.

670
00:40:53.430 --> 00:40:57.300
This is not the answer because when we start saying that we're going to

671
00:40:57.301 --> 00:41:00.750
deemphasize empathy and we're going to focus on reason.
Facts.

672
00:41:01.380 --> 00:41:06.240
We don't ever answer some important questions like whose job is it to decide

673
00:41:06.330 --> 00:41:07.530
what fair means?

674
00:41:08.490 --> 00:41:11.700
Do you want some random engineer to decide what fair means?

675
00:41:12.120 --> 00:41:16.440
That is a big cultural question and it needs big discussions around it.

676
00:41:18.240 --> 00:41:21.360
Who's job is it to understand historical context?

677
00:41:21.510 --> 00:41:26.510
Who's job is it to know the history of race and policing in this country before

678
00:41:26.671 --> 00:41:31.671
making decisions that could affect people whose job is it to sit down and think

679
00:41:32.131 --> 00:41:36.170
through what the potential unintended consequences of a design decision might
be.

680
00:41:37.540 --> 00:41:38.200
<v 1>Okay.</v>

681
00:41:38.200 --> 00:41:41.560
<v 0>Usually that's not anybody's job,
but it needs to be.</v>

682
00:41:42.520 --> 00:41:45.850
And I will say,
I was really pleased to read this interview with Fay Fay Lee,

683
00:41:45.880 --> 00:41:49.300
who some of the Stanford Vision lab and she's on sabbatical with Google cloud

684
00:41:49.310 --> 00:41:53.350
right now working as a chief scientist and she has a tremendous amount of

685
00:41:53.351 --> 00:41:57.460
experience with image processing.
And she's talking about this,
you think,

686
00:41:57.461 --> 00:42:01.960
you know,
AI is very task focused right now.
It lacks contextual awareness.

687
00:42:02.080 --> 00:42:04.840
It lacks the kind of flexible learning that humans have.

688
00:42:05.870 --> 00:42:09.250
And so we want to make technology that makes people's lives better and our world

689
00:42:09.490 --> 00:42:10.323
safer.

690
00:42:10.690 --> 00:42:15.130
And that takes a layer of human level communication and collaboration that that

691
00:42:15.131 --> 00:42:16.240
has been what is missing.

692
00:42:18.880 --> 00:42:21.340
But it's going to take a lot of work to get there.

693
00:42:22.410 --> 00:42:22.680
<v 1>Yeah.</v>

694
00:42:22.680 --> 00:42:25.860
<v 0>It's going to take a lot of people changing the way that they approach problems</v>

695
00:42:25.861 --> 00:42:28.860
and changing the way that they approach products to be able to make that kind of

696
00:42:28.861 --> 00:42:29.694
shift.

697
00:42:31.720 --> 00:42:34.840
So what I will leave you with today is something I am personally excited about,

698
00:42:34.841 --> 00:42:38.140
but that I recognize it's difficult when you work in a tech company and that is

699
00:42:38.141 --> 00:42:41.920
the increasing pressure and backlash that we are starting to see on big tech.

700
00:42:43.150 --> 00:42:47.440
You can see across the board from the truly terrible year,

701
00:42:47.470 --> 00:42:50.050
Uber has had to uh,

702
00:42:50.051 --> 00:42:55.051
the Twitter boycott that happened just a few weeks ago to the ongoing questions

703
00:42:55.391 --> 00:42:59.200
around Facebook's role in the election and what ads were sold to which Russians

704
00:42:59.590 --> 00:43:01.750
to whatever happened with the Google memo.

705
00:43:01.751 --> 00:43:05.020
Here we are seeing this backlash starting to spring up.

706
00:43:05.320 --> 00:43:07.570
And it can be difficult and it can be uncomfortable,

707
00:43:07.571 --> 00:43:10.900
but this is exactly the kind of thing that we need to have happen and that I

708
00:43:10.901 --> 00:43:13.960
actually hope more people start pushing back against.

709
00:43:13.970 --> 00:43:17.380
I hope that this is a broader conversation that makes its way well outside of

710
00:43:17.381 --> 00:43:21.430
tech circles because as much as I enjoy coming and talking with technical

711
00:43:21.431 --> 00:43:22.690
audiences,
um,

712
00:43:22.720 --> 00:43:26.680
I really think one of the most important things is that we make technology and

713
00:43:26.681 --> 00:43:30.790
push back against technology and the overreach of tech companies accessible to

714
00:43:30.791 --> 00:43:34.300
everyday people,
to people who don't necessarily have the insight or knowledge.

715
00:43:35.470 --> 00:43:38.890
Because I really think that it is only then it is only when we take the concerns

716
00:43:38.891 --> 00:43:41.110
of everyday people,
all people,

717
00:43:41.380 --> 00:43:44.620
even people who don't like mini cupcakes into consideration.

718
00:43:44.880 --> 00:43:47.890
Then we were actually able to make these kinds of changes possible and that we

719
00:43:47.891 --> 00:43:52.690
can make a technology industry that is going to be sustainable for the world and

720
00:43:52.691 --> 00:43:57.100
for individual people in the long run.
So thank you so much for having me today.

721
00:43:57.190 --> 00:43:58.023
I really appreciate it.

722
00:43:58.780 --> 00:44:03.780
<v 1>[inaudible]</v>

723
00:44:04.790 --> 00:44:09.440
<v 4>thank you very much Sara.
Questions.
The cupcake thing is,</v>

724
00:44:09.740 --> 00:44:11.030
is sort of a little,
uh,

725
00:44:11.060 --> 00:44:13.640
it's frustrating because you can imagine that we could have taken a different

726
00:44:13.641 --> 00:44:17.840
path and that would have happened,
would not have happened.
It's also sort of,
um,

727
00:44:18.260 --> 00:44:22.370
maybe easier because you can imagine what that path would be and maybe we could

728
00:44:22.371 --> 00:44:25.610
follow different paths in the future.
There's something I'd like,
uh,

729
00:44:25.630 --> 00:44:29.420
I'd love to hear your,
your thoughts on that I think is,
um,

730
00:44:30.900 --> 00:44:35.130
maybe fundamentally from my perspective seems to be a harder problem.

731
00:44:35.730 --> 00:44:40.020
Uh,
I,
I'm not sure if you were aware of the eye.
This is about a year ago.
Uh,

732
00:44:40.021 --> 00:44:40.590
there was,
uh,

733
00:44:40.590 --> 00:44:45.240
an issue where people notice that if you searched in Google for unprofessional

734
00:44:45.241 --> 00:44:46.110
hairstyles,

735
00:44:46.530 --> 00:44:49.920
you got pictures that were overwhelmingly of women and people that were

736
00:44:49.921 --> 00:44:51.390
overwhelmingly of color.

737
00:44:52.350 --> 00:44:56.790
And my simplistic understanding of this,
I don't work on that product,

738
00:44:56.791 --> 00:44:58.530
I should say,
uh,

739
00:44:58.680 --> 00:45:03.030
is that that's actually a reflection of how people tag images.
It is,

740
00:45:03.031 --> 00:45:07.440
it is a reflection of people having sites about what to do and not to do in the

741
00:45:07.441 --> 00:45:12.441
workplace that use those pictures as examples of what not to do.

742
00:45:13.650 --> 00:45:16.050
And,
uh,
and so

743
00:45:18.090 --> 00:45:21.450
an algorithm that sort of is,
um,

744
00:45:22.390 --> 00:45:23.220
<v 1>yeah,</v>

745
00:45:23.220 --> 00:45:28.110
<v 4>when is the right word for this really trying not to be biased,
but,</v>

746
00:45:28.140 --> 00:45:33.140
but the algorithm winds up reflecting the larger bias in society.

747
00:45:33.570 --> 00:45:36.070
And I think we've seen a number of,
uh,

748
00:45:36.720 --> 00:45:40.390
instances of this kind of problem or the,
um,
uh,
we,

749
00:45:40.391 --> 00:45:44.790
we had a problem a few years ago and Google maps where the White House would get

750
00:45:44.791 --> 00:45:49.791
labeled with a racial epithet because a lot of racist people were tagging it

751
00:45:50.941 --> 00:45:54.750
that way in,
in maps.
And we were,
uh,

752
00:45:54.810 --> 00:45:59.450
using that data.
And I'm just wondering if you had any thoughts on,

753
00:45:59.570 --> 00:46:03.470
<v 0>yeah,
so I think it was,
I think this is,
this is legitimately difficult,
right?
Um,</v>

754
00:46:03.650 --> 00:46:04.431
but here's the thing,

755
00:46:04.431 --> 00:46:08.210
like I think that one of the reasons this is particularly difficult is that tech

756
00:46:08.211 --> 00:46:13.211
companies have spent a lot of time focusing on what they do as being somehow

757
00:46:13.550 --> 00:46:17.060
neutral and,
um,
and like a lot of time kind of saying like,
free speech,

758
00:46:17.061 --> 00:46:21.290
free speech,
um,
and so,
and it's kind of take abdicated responsibility.

759
00:46:21.500 --> 00:46:26.390
And now we're looking at it and going like,
Oh shit,
what have we wrought?
And so,

760
00:46:26.560 --> 00:46:29.420
so I think part of the reason it's hard is that we should have taken this

761
00:46:29.421 --> 00:46:32.540
seriously a long time ago.
Um,
are you familiar with Sopheon Noble?

762
00:46:32.560 --> 00:46:34.730
I don't know if of you are something to noble is an,
uh,

763
00:46:34.820 --> 00:46:38.900
information studies scholar and she actually talks a lot about that kind of

764
00:46:38.901 --> 00:46:41.540
algorithmic bias and things like search results.
Um,

765
00:46:41.630 --> 00:46:44.030
and so specifically things like if you,

766
00:46:44.031 --> 00:46:45.830
so unprofessional hairstyles that makes sense.

767
00:46:45.831 --> 00:46:49.390
Also things like if you Google the word black girls,
you will get,
uh,

768
00:46:49.850 --> 00:46:52.260
typically explicit results.
Um,

769
00:46:52.280 --> 00:46:55.850
and there's lots and lots of ways in which that mirror that's being reflected

770
00:46:55.851 --> 00:46:57.650
back to us doesn't look great.

771
00:46:58.280 --> 00:47:01.640
So I think what you have to start thinking through though is

772
00:47:03.160 --> 00:47:05.650
as a company,
as an industry,
as a culture,

773
00:47:06.550 --> 00:47:10.000
what is the role that we want something like search to take?
Is it a mirror?

774
00:47:10.300 --> 00:47:13.060
And then,
well,
what do we do about the fact that it's never just a mirror,

775
00:47:13.061 --> 00:47:15.970
it's also a magnifier,
right?
Like,
because that's the fact.

776
00:47:15.971 --> 00:47:18.610
And that's what we have to acknowledge and really deeply internalized is that

777
00:47:18.820 --> 00:47:22.540
it's not just that you're reflecting back to society what people have said,

778
00:47:22.780 --> 00:47:25.310
but you're making that codified.
Um,

779
00:47:25.330 --> 00:47:27.910
Cathy O'Neil who's the author of weapons of math destruction,

780
00:47:27.911 --> 00:47:32.260
she talks a lot about how data,
it doesn't create the future,
codifies the past.

781
00:47:32.500 --> 00:47:36.190
So you're basically taking that historical information and you're making it seem

782
00:47:36.191 --> 00:47:37.360
more objectively true.

783
00:47:38.140 --> 00:47:40.900
And so I think that that is really the question that you have to be asking,

784
00:47:41.140 --> 00:47:43.450
not just are we comfortable reflecting this back,

785
00:47:43.451 --> 00:47:48.451
but are we comfortable magnifying this and normalizing this and codifying this.

786
00:47:49.270 --> 00:47:51.190
I think that if we ask those questions,

787
00:47:51.191 --> 00:47:55.390
I think we will come out with answers that are going to feel more ethically

788
00:47:55.391 --> 00:47:57.910
sound and something that we can really stick by and something that can be a

789
00:47:57.911 --> 00:48:00.940
better guidance for us.
I don't think it makes that discussion easy.

790
00:48:00.941 --> 00:48:04.510
I think that's still a really difficult question because,
um,

791
00:48:05.200 --> 00:48:08.800
you then have to,
then you have to say,
well,
like,
where's the line where,
you know,

792
00:48:08.801 --> 00:48:13.750
and like you're in this sea of gray area,
but if you're not,

793
00:48:13.751 --> 00:48:16.960
ha I mean that's,
and that's exactly why like this is a people business.

794
00:48:16.961 --> 00:48:20.680
Like if you're not going to have that kind of complexity of conversation about

795
00:48:20.860 --> 00:48:25.480
what you want to see in society,
what we think is appropriate,

796
00:48:25.481 --> 00:48:29.500
what we think is fair.
Like if you're not going to have those conversations,

797
00:48:29.680 --> 00:48:32.290
then you shouldn't be playing in this space in the first place.

798
00:48:33.310 --> 00:48:33.700
<v 5>Okay.</v>

799
00:48:33.700 --> 00:48:34.533
<v 0>That's helpful.</v>

800
00:48:35.880 --> 00:48:37.290
<v 5>Alright,
so first of all,</v>

801
00:48:37.291 --> 00:48:41.760
thank you for being so direct and strongly worded and raising her topless

802
00:48:41.761 --> 00:48:42.594
problem.

803
00:48:43.620 --> 00:48:48.620
I normally would not use the words decided if somebody was ignorant to the

804
00:48:50.341 --> 00:48:54.750
issue.
Um,
I kind of pulled that word to mean that it was,
uh,
you know,

805
00:48:54.870 --> 00:48:58.380
informed decision of some kinds of,
like they knew they were at a junction.
Um,

806
00:48:58.740 --> 00:49:03.430
this definitely is some form of neglect or malpractice.
And in,
uh,

807
00:49:03.660 --> 00:49:04.800
when I took freshman ethics,

808
00:49:04.830 --> 00:49:08.210
I was told that what distinguished festival fix from the rest of,
you know,

809
00:49:08.270 --> 00:49:11.010
other kinds of ethics is that if you're in a profession,

810
00:49:11.370 --> 00:49:14.970
your client doesn't know enough to double check your work.

811
00:49:15.210 --> 00:49:17.790
If you're an architect or a lawyer,
you know,

812
00:49:17.791 --> 00:49:21.150
almost by definition your client just has to trust you to do it.
Right.

813
00:49:21.570 --> 00:49:26.570
And I think it's appropriate that these companies should know these pitfalls and

814
00:49:27.931 --> 00:49:31.730
should be systemizing solutions to them.
But,
uh,

815
00:49:32.400 --> 00:49:34.260
I don't know how much you know about the internal structure of different

816
00:49:34.261 --> 00:49:37.860
companies,
but like where exactly would you pin that responsibility?

817
00:49:38.070 --> 00:49:42.120
Is that engineering malpractice?
Is that design malpractice?

818
00:49:42.180 --> 00:49:45.450
Is that legal malpractice?
Um,

819
00:49:45.660 --> 00:49:49.770
who should be the professional that is aware of all the implications here?

820
00:49:50.210 --> 00:49:54.680
<v 0>Well,
so I think so realistically like we are never going to be aware of every</v>

821
00:49:54.681 --> 00:49:57.680
potential implication,
every potential outcome,
right?
Like that's not,

822
00:49:57.681 --> 00:49:58.790
that is not realistic.

823
00:49:59.180 --> 00:50:03.110
I think that it is incumbent upon people in every discipline though to to be

824
00:50:03.111 --> 00:50:03.621
more aware.

825
00:50:03.621 --> 00:50:08.360
And I also think that it is incumbent upon companies to look at what they're

826
00:50:08.361 --> 00:50:12.320
doing when like when you acknowledge that what you're doing is not just tech,

827
00:50:12.530 --> 00:50:16.460
then you realize that you actually need expertise in these areas.

828
00:50:16.550 --> 00:50:17.840
That of course,

829
00:50:17.900 --> 00:50:22.280
of course you're going to do a bad job seeing some of this stuff because you

830
00:50:22.281 --> 00:50:25.070
don't actually have the background to see it.
Right?

831
00:50:25.071 --> 00:50:30.071
Like you should probably bring in who has a deep knowledge of history of race in

832
00:50:31.671 --> 00:50:34.640
this country to have any,
um,

833
00:50:35.360 --> 00:50:38.810
to have like influence over any product that could impact people of different

834
00:50:38.811 --> 00:50:42.080
races,
which is probably all of your products.
Um,
right?
Like,

835
00:50:42.130 --> 00:50:46.310
like somebody has to actually know something in order to find these problems.

836
00:50:46.311 --> 00:50:47.840
And if you're not hiring for that,

837
00:50:48.140 --> 00:50:50.000
then you're going to continue having these gaps.

838
00:50:50.150 --> 00:50:53.270
I do think that everybody can do better across every discipline.

839
00:50:53.271 --> 00:50:56.870
I do think it is the responsibility of people in design.

840
00:50:56.871 --> 00:50:59.450
It is the responsibility of people and development and it's a responsibility of

841
00:50:59.451 --> 00:51:02.060
people in legal.
It's everywhere.

842
00:51:02.330 --> 00:51:05.080
But the thing is if you're going to say it's everybody's responsibility,

843
00:51:05.081 --> 00:51:07.220
that's very easy for that to become nobody's job.
Right?

844
00:51:07.610 --> 00:51:11.240
And so I think that fundamentally though that's because it's not,

845
00:51:11.270 --> 00:51:13.880
it's not just about the individual culpability.

846
00:51:14.330 --> 00:51:18.320
It is about priorities as an organization,
right?

847
00:51:18.321 --> 00:51:20.660
So if this is a priority for your organization,

848
00:51:20.690 --> 00:51:23.870
then it's going to become part of your organizational culture and is going to be

849
00:51:23.871 --> 00:51:27.080
systematized and is going to be present at every step along the way.

850
00:51:27.620 --> 00:51:32.180
It isn't going to be done in this sort of ad hoc way where like you individual

851
00:51:32.181 --> 00:51:34.220
engineer needed to have noticed this thing.

852
00:51:34.430 --> 00:51:37.820
I think assigning blame to individual people is actually not very helpful.

853
00:51:37.940 --> 00:51:42.200
I think it's much more helpful to say,
why does this happen?
That's why I'm like,

854
00:51:42.260 --> 00:51:44.060
I don't really care about the bug fix.
Right?

855
00:51:44.061 --> 00:51:47.990
Like I care about what kinds of patterns are emerging over and over again.

856
00:51:48.200 --> 00:51:51.540
And how does the way that you are organized,
how to PR,
like what's,

857
00:51:51.710 --> 00:51:55.880
what does a project sort of genesis look like and how does that get translated

858
00:51:55.881 --> 00:52:00.500
into a team that's working on it?
Who is saying Yay or nay to decisions,

859
00:52:00.680 --> 00:52:03.830
who thought it was a terrible idea but wasn't comfortable speaking up.

860
00:52:03.831 --> 00:52:07.850
And why is that happening?
Um,
did you have enough diverse people in the room?

861
00:52:07.851 --> 00:52:11.600
Like were there a bunch of women in the room who decided at the mini cupcake

862
00:52:11.601 --> 00:52:16.370
should be there or not?
Um,
like,
and you could say that about many,

863
00:52:16.371 --> 00:52:18.500
many groups but,
but you know,

864
00:52:18.501 --> 00:52:21.740
I think that you have to look at it at that macro level because I think when we

865
00:52:21.741 --> 00:52:25.610
try to talk about it at that individual level,
then of course people miss stuff.

866
00:52:25.611 --> 00:52:26.930
Everybody misses stuff.

867
00:52:27.230 --> 00:52:29.450
I think we have to look at the pattern of missing stuff and say,

868
00:52:29.451 --> 00:52:31.190
how do we tackle that organizationally?

869
00:52:32.250 --> 00:52:33.083
<v 6>Thank you.</v>

870
00:52:33.180 --> 00:52:37.910
You were talking a lot about how these sorts of checks need to be systematized.

871
00:52:38.240 --> 00:52:38.630
Um,

872
00:52:38.630 --> 00:52:43.070
and I'm just wondering if you know of any examples of organizations which have

873
00:52:43.071 --> 00:52:45.500
decision making structures which support this?

874
00:52:45.501 --> 00:52:50.501
Like how can we foster an environment where we are in a meeting and the person

875
00:52:51.051 --> 00:52:55.460
who thinks maybe this cupcake idea is bad,
feels empowered to speak up?
Like,

876
00:52:55.461 --> 00:52:59.330
what's sorts of systemic things can we do to be better about that?

877
00:52:59.820 --> 00:53:00.601
<v 0>So this is hard.
I,</v>

878
00:53:00.601 --> 00:53:03.570
I don't have any examples off the top of my head where I'm like,

879
00:53:03.571 --> 00:53:06.710
just look at what this corporation does.
Um,
because I think that this is,

880
00:53:06.810 --> 00:53:10.350
this is not something that most companies are doing that well at,

881
00:53:10.710 --> 00:53:14.000
but I would say that part of it is,
okay,
so you,

882
00:53:14.001 --> 00:53:16.620
you do need to have a diverse team working on things,

883
00:53:16.860 --> 00:53:20.550
but you also have to have a diverse team that feels like it can speak up.

884
00:53:20.970 --> 00:53:25.200
And what that means is that you have to have people at different levels who are

885
00:53:25.201 --> 00:53:28.140
diverse.
I mean,
listen,
the thing,
like if you want to fix this problem,

886
00:53:28.141 --> 00:53:30.990
you got to go with back to root cause.
And that's a really hard problem.

887
00:53:31.290 --> 00:53:33.230
But you do,
you have to.
Um,

888
00:53:33.270 --> 00:53:37.860
but you have to have people who aren't just at a junior level who might have

889
00:53:37.861 --> 00:53:40.930
different perspectives.
So if you have all of the senior people in the room.
Um,

890
00:53:40.980 --> 00:53:43.830
so for example,
the other day,
the,
um,

891
00:53:44.370 --> 00:53:47.610
the chief of diversity and inclusion at apple had this comment.

892
00:53:47.611 --> 00:53:50.250
It was kind of an offhand comment in a totally blew up where she said something

893
00:53:50.251 --> 00:53:51.360
like,
well,
you know,

894
00:53:51.361 --> 00:53:54.060
you could have 12 blonde haired blue eyed men in the room and that would have

895
00:53:54.061 --> 00:53:57.770
its own kind of diversity.
And,
um,

896
00:53:58.260 --> 00:54:01.440
what that indicates to me at a company like apple that is actively trying to

897
00:54:01.441 --> 00:54:05.520
recruit more diversely is that they still have this perception that,
uh,

898
00:54:05.790 --> 00:54:07.410
people can,
like,

899
00:54:07.411 --> 00:54:10.830
you can have that group of people come into the room and like that that's still

900
00:54:10.831 --> 00:54:14.850
okay.
That's not okay cause I'm going to tell you those 12 people,

901
00:54:15.150 --> 00:54:18.480
they are going to miss a lot of stuff.
So you,

902
00:54:18.550 --> 00:54:22.710
you have to both acknowledge that you need that diversity there and that those,

903
00:54:22.770 --> 00:54:27.240
everybody there has to be able to speak up to those 12 blonde haired blue eyed

904
00:54:27.241 --> 00:54:30.150
white men in the room who all come from the same background,
um,

905
00:54:30.180 --> 00:54:35.010
in order for it to work.
Um,
so,
so anyway,
um,
I think that that's like,

906
00:54:35.340 --> 00:54:37.980
that is a key piece of it.
And then it's also like

907
00:54:41.680 --> 00:54:43.330
we prompt,
we have processes,

908
00:54:43.350 --> 00:54:46.210
we have like systems that we use for all kinds of stuff.

909
00:54:46.600 --> 00:54:51.490
Why is there no system in place that's like checks and balances on assumptions?

910
00:54:51.491 --> 00:54:55.690
Like has anybody ever sat down and documented all of the technical assumptions

911
00:54:55.691 --> 00:54:58.240
that they are making that's pretty common?

912
00:54:58.690 --> 00:55:03.520
Or have you ever sat down and documented all of the like social assumptions that

913
00:55:03.521 --> 00:55:05.650
you're making or human assumptions that you're making?

914
00:55:05.860 --> 00:55:09.250
I have very rarely seen a team actually sit down and do that and then think

915
00:55:09.251 --> 00:55:12.590
about,
well,
okay,
what's the worst that could happen?
How could this,
you know,

916
00:55:12.910 --> 00:55:16.240
how could this go totally wrong and how are we going to word against that?

917
00:55:16.480 --> 00:55:21.100
I just don't think that they're being trained to do that kind of thing.
Um,
so,

918
00:55:21.101 --> 00:55:22.060
so yeah,
I think that you do,

919
00:55:22.090 --> 00:55:24.370
you have to go pretty deep to start solving the problem.

920
00:55:24.730 --> 00:55:29.710
It's not going to be fixed by,
um,
you know,
just like,

921
00:55:29.711 --> 00:55:33.580
let's hire some more junior people in diverse roles.
It will not be fixed by,

922
00:55:33.910 --> 00:55:36.250
I'm just like kind of encouraging people to speak up.

923
00:55:36.280 --> 00:55:38.470
It will be fixed by kind of big,

924
00:55:38.471 --> 00:55:41.620
deep changes in the way that we structure teams,

925
00:55:41.621 --> 00:55:45.630
the way that we promote the way that we operate as companies.
Um,

926
00:55:45.640 --> 00:55:47.110
which makes it really tough to mix painful.

927
00:55:47.111 --> 00:55:50.460
It makes it feel like it's hard to get anywhere.
Um,

928
00:55:50.530 --> 00:55:51.730
but I think that that's really the only way.

929
00:55:53.550 --> 00:55:54.600
<v 1>Well,
it's like Sarah,
again.</v>

