WEBVTT

1
00:00:06.010 --> 00:00:10.350
So without further ado,
the first speaker today is a professor,

2
00:00:10.351 --> 00:00:14.830
Peter Baylor's from the neighborhood from Stanford University.

3
00:00:15.100 --> 00:00:19.040
He,
uh,
joined Stanford from Berkeley,
um,

4
00:00:19.920 --> 00:00:22.300
uh,
other buy tea,
right.

5
00:00:22.450 --> 00:00:26.950
You were a postdoc into MIT phd at Berkeley Postdoc at Mit.

6
00:00:27.220 --> 00:00:32.220
He joined Stanford last year and a is a rising star in the,

7
00:00:32.860 --> 00:00:36.820
in the area of database,
uh,
applied machine learning,

8
00:00:36.821 --> 00:00:41.170
large scale machine learning.
And today he's going to share with us his,

9
00:00:41.260 --> 00:00:45.100
his recent work,
which I've seen once,
but not enough.

10
00:00:45.101 --> 00:00:50.080
It's really exciting.
I love what they're doing.
So welcome Peter.
Great,
thanks.

11
00:00:53.740 --> 00:00:54.573
<v 1>Okay.</v>

12
00:00:54.650 --> 00:00:56.190
<v 0>Uh,
it,
it's really wonderful to be here.</v>

13
00:00:56.460 --> 00:01:01.170
Thank you all for showing up and for the untold number of you on Gvc,
uh,

14
00:01:01.171 --> 00:01:05.700
as well.
Um,
uh,
and thanks to the wonderful intro,
uh,
really excited,

15
00:01:05.780 --> 00:01:09.930
uh,
this opportunity to come speak with you.
Um,
so I'm a new faculty member,
uh,

16
00:01:10.000 --> 00:01:14.540
at Stanford,
um,
and with some new faces on campus.
Uh,

17
00:01:14.610 --> 00:01:18.320
we've actually started a,
a large scale initiative we call dawn,
uh,

18
00:01:18.540 --> 00:01:20.310
of which this macro based product is a,

19
00:01:20.311 --> 00:01:25.140
is it subcomponent a geared around infrastructure and tools for large scale

20
00:01:25.410 --> 00:01:28.790
machine learning and AI.
So,
so I'm coming here,
uh,

21
00:01:28.860 --> 00:01:31.830
under the guise of the Stanford dawn projects,
data analyst for what next,

22
00:01:31.860 --> 00:01:35.490
what's next?
If myself,
I studied largely stream processing data management,
uh,

23
00:01:35.491 --> 00:01:39.150
my colleague Chris Ray,
a certified genius by the Macarthur Foundation.

24
00:01:39.240 --> 00:01:41.210
So he's database and machine learning.
Um,

25
00:01:41.460 --> 00:01:44.760
and on the bottom half we actually have some systems in architecture folks.

26
00:01:44.760 --> 00:01:48.840
So Coonley Lakotan built some of the first multi core chips with a Sun Niagara

27
00:01:49.050 --> 00:01:50.550
and my Haria.
Yeah,

28
00:01:50.551 --> 00:01:55.510
you may be familiar with as the cocreator of spark and may sauce.
Um,

29
00:01:55.620 --> 00:02:00.420
so essentially this dawn project,
uh,
we're interested in,
uh,
what's a,

30
00:02:00.480 --> 00:02:04.410
what's a fairly exciting set of advances in day to day and that we've seen these

31
00:02:04.411 --> 00:02:07.830
really wonderful breakthroughs and amazing breakthroughs and conventionally

32
00:02:07.831 --> 00:02:12.330
difficult task spanning image recognition and LP planning,
information retrieval.

33
00:02:12.510 --> 00:02:16.790
And we're starting to see both in the private sector,
in the public sector,
um,

34
00:02:16.980 --> 00:02:18.960
really society scale impact,
uh,

35
00:02:19.020 --> 00:02:21.720
things like autonomous vehicles are now becoming a reality.
Uh,

36
00:02:21.721 --> 00:02:24.450
some of our work on personalized medicine shows that we can beat,
you know,

37
00:02:24.451 --> 00:02:28.550
Stanford doctors at their own game with enough training data.
Um,

38
00:02:28.680 --> 00:02:32.540
and some of Chris's work has even been used to make arrests and commonly,
uh,

39
00:02:32.660 --> 00:02:36.720
sort of in some sort of challenging tasks like a combating human trafficking.
So,

40
00:02:36.780 --> 00:02:37.531
so,
you know,

41
00:02:37.531 --> 00:02:40.680
I come today with a message of optimism that I think we all share in this room,

42
00:02:40.681 --> 00:02:43.980
which that there's no end in sight for Vance and machine learning,
uh,

43
00:02:43.981 --> 00:02:47.100
but also with a recognition that this is a bit of a,
uh,

44
00:02:47.101 --> 00:02:49.050
of a fairy tale right now.

45
00:02:49.051 --> 00:02:53.010
There's a bit of an asterisk next to this golden era of data and that it's

46
00:02:53.011 --> 00:02:56.490
really the golden era of data for the best funded in best train engineering

47
00:02:56.491 --> 00:02:59.350
teams.
Right?
So even within an organization like Google,
you know,

48
00:02:59.380 --> 00:03:04.380
AI and ml capabilities remain a fairly scarce commodity whereby to build a world

49
00:03:04.691 --> 00:03:09.691
class production quality data product requires a team of tens to hundreds of

50
00:03:10.181 --> 00:03:14.500
data engineers,
data scientists,
machine learning researchers,

51
00:03:14.590 --> 00:03:16.780
and ultimately ops people to put these things into prod.

52
00:03:17.200 --> 00:03:22.200
So our goal in this five year dawn project is to enable anyone with domain

53
00:03:22.781 --> 00:03:26.470
expertise to build their own production quality machine learning projects.

54
00:03:27.100 --> 00:03:30.040
Products without requiring that phd in machine learning are hiring,

55
00:03:30.041 --> 00:03:34.640
are wonderful students without becoming an expert in databases and systems and

56
00:03:34.641 --> 00:03:38.820
without understand the latest hardware to do so at scale and to run these models

57
00:03:38.821 --> 00:03:40.180
in a cost effective manner.
Right.

58
00:03:40.540 --> 00:03:43.480
And the reason why we think this is feasible is for two reasons.
Okay?

59
00:03:43.810 --> 00:03:46.480
The first reason for why we,
why we're so excited about this is that,

60
00:03:46.580 --> 00:03:47.950
and if we look to you,
all right,

61
00:03:47.951 --> 00:03:51.160
are some of your colleagues here at Google and one of our favorite papers from

62
00:03:51.161 --> 00:03:53.710
the last several years,
hidden tactical debt,
machine learning systems.

63
00:03:53.950 --> 00:03:54.491
First of all,

64
00:03:54.491 --> 00:03:58.150
only a fraction of real world machine learning systems are actually composed of

65
00:03:58.151 --> 00:03:59.840
machine learning code,
right?
That is,

66
00:03:59.841 --> 00:04:03.790
there's a large amount of work that goes into building predictive data products

67
00:04:03.870 --> 00:04:05.660
that that goes beyond just,
you know,

68
00:04:05.710 --> 00:04:08.770
scribbling equation on a whiteboard and come in with a new loss function in new

69
00:04:08.771 --> 00:04:10.150
neural network architecture,
right?

70
00:04:10.300 --> 00:04:14.050
It's about a configuration and data collection and then all the way to serving

71
00:04:14.051 --> 00:04:14.621
and monitoring.

72
00:04:14.621 --> 00:04:18.310
And there's these missing pieces like vast missing pieces in the machine

73
00:04:18.311 --> 00:04:22.150
learning life cycle that essentially not addressed by today's data management

74
00:04:22.330 --> 00:04:23.163
infrastructure.

75
00:04:24.190 --> 00:04:27.670
And the second reason why we think that this question is actually tractable is

76
00:04:27.671 --> 00:04:29.560
that,
you know,
if we look to history,

77
00:04:29.650 --> 00:04:33.190
similar advances have actually happened before,
right?

78
00:04:33.610 --> 00:04:36.790
My favorite example of this is the building where we're in today,
right?

79
00:04:36.970 --> 00:04:41.970
So search isn't that great example of a sort of core technology being

80
00:04:42.431 --> 00:04:45.700
democratized through both better systems and better interfaces.

81
00:04:46.060 --> 00:04:50.920
So the core algorithms,
uh,
before page rank,
things like TF,
IDF,

82
00:04:50.921 --> 00:04:54.010
and inverted indices and distributed query processing date back as early as

83
00:04:54.020 --> 00:04:58.480
1950s.
Right?
So TF IDF indices are from the night literally 1950s from IBM.

84
00:04:59.260 --> 00:05:02.320
And what it took was sort of concerted effort by a large number of folks,

85
00:05:02.470 --> 00:05:04.420
including the folks and this organization,

86
00:05:04.570 --> 00:05:08.290
but also a lot of folks in the developer community to make it so that today we

87
00:05:08.291 --> 00:05:12.240
can add search to any application simply by linking a library like slowly

88
00:05:12.241 --> 00:05:14.550
Stoller,
leucine and essentially getting searched.

89
00:05:14.550 --> 00:05:17.530
That kind of works or in most cases works relatively well.

90
00:05:17.560 --> 00:05:18.670
Sort of out of the box.

91
00:05:18.880 --> 00:05:23.020
And perhaps more importantly everyone that is these non expert users.
You know,

92
00:05:23.140 --> 00:05:25.290
my,
my siblings and my parents who are,

93
00:05:25.320 --> 00:05:27.910
who have never heard of distributed query processing or an inverted index,

94
00:05:28.030 --> 00:05:31.300
they can make use of search technology like Google and voice assistant that

95
00:05:31.301 --> 00:05:34.990
enables search,
uh,
without actually understanding the underlying concepts.

96
00:05:35.290 --> 00:05:38.710
And so we're asking this dawn product is why isn't machine learning the same
way?

97
00:05:39.670 --> 00:05:40.120
Well,

98
00:05:40.120 --> 00:05:42.970
what we think is necessary and what we're placing our bet over the next five

99
00:05:42.971 --> 00:05:46.270
years is that we need to fill out the remainder of that stack.

100
00:05:46.360 --> 00:05:49.390
Those tools required for building these production quality data products,

101
00:05:49.540 --> 00:05:54.540
going beyond just better models and better cost functions and loss functions to

102
00:05:54.611 --> 00:05:59.060
actually providing systems and tools that assess assistant all layers and all

103
00:05:59.120 --> 00:06:03.620
stages of the machine learning life cycle from left to right from data

104
00:06:03.621 --> 00:06:06.470
acquisition and feature engineering to model training and productionized

105
00:06:06.500 --> 00:06:10.100
productionization and then all the way from sort of new interfaces for non not

106
00:06:10.101 --> 00:06:13.400
expert users all the way down to new hardware that can exploit the statistical

107
00:06:13.420 --> 00:06:17.030
imprecision in these algorithms.
Do things like relax coherency,

108
00:06:17.031 --> 00:06:20.750
memory in order to make sure we can run these sorts of algorithms efficiently on

109
00:06:20.751 --> 00:06:21.584
modern hardware.

110
00:06:22.190 --> 00:06:25.490
So this is our vision and broadly where I'm coming to you today from.

111
00:06:25.730 --> 00:06:28.340
We think we have a huge opportunity to build systems and tools and make it

112
00:06:28.341 --> 00:06:32.270
radically easier and cheaper to build production quality data products.

113
00:06:32.271 --> 00:06:35.390
We're building an open source stack as part of this dawn project.
In fact,

114
00:06:35.391 --> 00:06:36.800
we're very excited about the potential.

115
00:06:36.801 --> 00:06:40.160
Having Google involved in our journey here.
And there's a whole website online.

116
00:06:40.460 --> 00:06:43.820
You can take a look at to learn more about the stack that we're putting together

117
00:06:44.300 --> 00:06:47.900
for this specific talk.
I'm going to Skype just one slice of this dawn vision,

118
00:06:48.170 --> 00:06:51.680
which is a new system we're building called macro based design to make it easier

119
00:06:51.770 --> 00:06:56.770
to extract value and performed classification and aggregation tasks over large

120
00:06:56.901 --> 00:06:58.340
scale telemetry streams.
Okay.

121
00:06:58.490 --> 00:07:02.480
So I'm missing the zoom in and we're going to give a deep dive into one narrow

122
00:07:02.481 --> 00:07:05.720
slice through the stack of work going on at Stanford.

123
00:07:05.721 --> 00:07:08.750
And this is joint work with a large number of students and faculty,

124
00:07:08.751 --> 00:07:13.400
both my pis who occupy spaces below and say compilers and hardware and also

125
00:07:13.401 --> 00:07:17.000
spaces above in terms of new interfaces as well as a wonderful team of Grad

126
00:07:17.001 --> 00:07:19.730
students who's actually,
you know,
building out large part of this,

127
00:07:19.731 --> 00:07:22.430
which is all available as open source.
So I'd encourage you,
uh,

128
00:07:22.450 --> 00:07:26.750
how have links in the talk to actually go and download systems like snorkel and

129
00:07:26.751 --> 00:07:30.140
weld as well as macro base.
Okay.
So,

130
00:07:30.710 --> 00:07:34.880
where are we coming from?
Essentially after the era of,
of big data,
right?

131
00:07:34.940 --> 00:07:38.150
Big Data sorta convinced every CTO on the planet that data had value.

132
00:07:38.151 --> 00:07:41.540
We should store this stuff and the cost of value went down precipitously.

133
00:07:41.540 --> 00:07:44.480
So instead of paying,
say $100,000 a terabyte for data,

134
00:07:44.481 --> 00:07:47.570
we can afford a store it for cents on the dollar and uh,
uh,

135
00:07:47.600 --> 00:07:51.120
commodity data warehouse like HDFS or s three or,
you know,

136
00:07:51.200 --> 00:07:55.880
maybe Google big query.
Uh,
and what we've seen is that there's,
you know,
a huge,

137
00:07:56.060 --> 00:07:57.940
uh,
ramp up and the collection of,

138
00:07:57.941 --> 00:08:02.480
of streaming telemetry largely driven by increases in automated data sources,

139
00:08:02.481 --> 00:08:02.721
right?

140
00:08:02.721 --> 00:08:06.650
So it'd be easier and cheaper than ever to instrument complex applications on

141
00:08:06.651 --> 00:08:10.610
the server,
uh,
on mobile phones and also tracking user behavior.

142
00:08:10.680 --> 00:08:13.880
And we're generating huge amounts of logs.
Okay.
So at least publicly,
right?

143
00:08:13.881 --> 00:08:16.760
The public numbers we hear from our friends down the street,
Facebook,
Twitter,

144
00:08:16.761 --> 00:08:19.910
linkedin,
they're collecting over 12 million events per second.
Um,

145
00:08:19.970 --> 00:08:24.530
I would be a shocked at the number,
wasn't higher,
uh,
internally at Google.

146
00:08:24.710 --> 00:08:27.500
And that sort of resulting challenge here as these data volumes continue to

147
00:08:27.630 --> 00:08:32.450
rocket is that,
you know,
for you and I,
um,
you know,
we're not getting any,

148
00:08:32.451 --> 00:08:35.120
any smarter,
right?
We're not going any faster and picking up this data,
right?

149
00:08:35.121 --> 00:08:37.350
So fundamentally human attentions,

150
00:08:37.370 --> 00:08:40.970
relatively scarce compared to these data volumes and you know,

151
00:08:40.971 --> 00:08:44.000
our friends over beers,
we'll report that for their,
you know,

152
00:08:44.001 --> 00:08:47.880
observability platforms that are sucking in all of this telemetry from say

153
00:08:47.881 --> 00:08:49.550
mobile devices and servers and so on.

154
00:08:49.700 --> 00:08:53.020
Less than 6% of the state has ever actually read,
right?
You might ask,
well,

155
00:08:53.021 --> 00:08:56.670
why is this being stored?
Well,
it turns out that when something goes wrong,

156
00:08:56.760 --> 00:08:59.490
you want to go back to the data and actually try to figure out what happened.

157
00:08:59.760 --> 00:09:00.511
But the question is,
you know,

158
00:09:00.511 --> 00:09:03.750
can we do better in order to increase the amount of data that we're,

159
00:09:03.780 --> 00:09:05.490
that we're actually looking at in these,

160
00:09:05.550 --> 00:09:10.470
in these sort of live to amateur teams and decrease the time from sort of event

161
00:09:10.500 --> 00:09:14.160
to detection and remediation.
So motivating example of a,

162
00:09:14.161 --> 00:09:16.650
of a company we work with very early on in this project.

163
00:09:17.190 --> 00:09:20.040
This was during some of my time at MIT that Faye mentioned.

164
00:09:20.670 --> 00:09:24.090
Cambridge mobile telematics is a spinoff from the MIT cartel project.

165
00:09:24.560 --> 00:09:27.870
Their products basically collects and analyzes telemetry and the form of driving

166
00:09:27.871 --> 00:09:29.790
behavior from end user devices.

167
00:09:30.570 --> 00:09:33.060
So they sell an APP that runs up predictive model and says,

168
00:09:33.061 --> 00:09:34.140
are you a good driver or not?

169
00:09:34.141 --> 00:09:37.260
And we'll give you actually sort of suggestions for how to improve your driving

170
00:09:37.261 --> 00:09:41.950
behavior.
Okay.
The CMT applique application operators who are included,
you know,

171
00:09:41.951 --> 00:09:44.970
two MIT professors,
Sam Madden,
Hardwell Christian,
great guys,

172
00:09:44.971 --> 00:09:47.010
incredibly smart wanna answer.
Simple question.

173
00:09:47.130 --> 00:09:50.640
Is the application behaving well on every platform?
Well,

174
00:09:50.940 --> 00:09:52.260
if any of you work on Android,

175
00:09:52.290 --> 00:09:55.050
you won't be surprised that this is a difficult question to answer,
right?

176
00:09:55.080 --> 00:09:58.320
So if we look at the android device ecosystem,
this is the device ecosystem.

177
00:09:58.321 --> 00:10:02.940
In 2015 we see that there are over 24,000 different android device types and the

178
00:10:02.941 --> 00:10:07.530
number has doubled since 2013 okay.
So,
so we have to understand,

179
00:10:07.620 --> 00:10:08.250
for instance,

180
00:10:08.250 --> 00:10:11.370
how our APP is behaving on each and every one of these hardware platforms and

181
00:10:11.371 --> 00:10:14.100
also each and every one of our releases of our application in each,

182
00:10:14.101 --> 00:10:16.290
every one of the android device releases.
Okay.

183
00:10:16.530 --> 00:10:18.530
So let's do a little bit of like mental math.
Um,

184
00:10:18.630 --> 00:10:22.200
if we were going to try to analyze the 24,000 different devices,

185
00:10:22.510 --> 00:10:26.970
android device in the wild and just the 25 different android API releases,

186
00:10:27.180 --> 00:10:31.080
just spending one second per combination here to check this right manually would

187
00:10:31.081 --> 00:10:33.870
require seven continuous days of effort.
Okay.

188
00:10:34.410 --> 00:10:38.400
Now this is clearly not going to work as we're releasing new versions of our

189
00:10:38.401 --> 00:10:41.880
application every day,
um,
or possibly multiple times a day.

190
00:10:42.330 --> 00:10:46.860
And it's actually important that we do inspect these things because in the rare

191
00:10:46.861 --> 00:10:50.770
combinations of say,
application version and um,
and uh,

192
00:10:50.850 --> 00:10:53.880
hardware version where there is some problematic interaction.
For instance,

193
00:10:53.881 --> 00:10:56.670
there's a buggy accelerometer or that a problem with the connection,

194
00:10:56.760 --> 00:11:00.360
especially when people were building sort of android soc for,
um,
you know,

195
00:11:00.600 --> 00:11:03.540
a couple cents or maybe tens of cents these days.
Um,

196
00:11:03.630 --> 00:11:06.660
we want to understand that these things matter.
And in fact,
at Cmt,
we found,

197
00:11:06.750 --> 00:11:10.420
you know,
for instance,
uh,
one problem with us,
fortunately not,

198
00:11:10.500 --> 00:11:13.230
not an android problem found other android problems,
but,
uh,
for the,

199
00:11:13.231 --> 00:11:14.700
for the Ios problems we found,
you know,

200
00:11:14.910 --> 00:11:19.910
Ios 9.0 Beta one dash five but not 9.0 0.1 had a buggy Bluetooth stack that

201
00:11:21.241 --> 00:11:24.990
prevented these ios devices from connecting to the incar car sensors.
Okay.

202
00:11:25.230 --> 00:11:28.650
So for a very small portion of the population that was running this particular,

203
00:11:28.780 --> 00:11:31.490
um,
thing,
essentially one very small cell,
uh,

204
00:11:31.660 --> 00:11:34.890
in the deployment and the deploy base for CMT application,

205
00:11:35.040 --> 00:11:38.280
there was actually a significant,
uh,
degraded sort of performance,
right?

206
00:11:38.790 --> 00:11:43.500
So what we want to do broadly in this work with this macro based engine is to

207
00:11:43.501 --> 00:11:47.400
enable us to take all of this sort of high dimensional streaming telemetry and

208
00:11:47.401 --> 00:11:51.180
actually,
uh,
automatically prioritize a end user attention.

209
00:11:51.600 --> 00:11:54.610
And specifically if you were going to do so by combining a large number of,

210
00:11:54.730 --> 00:11:58.900
or a small number of highly powerful statistical operators for classification

211
00:11:58.960 --> 00:12:02.290
and aggregation that we'll able to pop out these sorts of combinations

212
00:12:02.350 --> 00:12:03.610
automatically.
Okay.

213
00:12:04.300 --> 00:12:07.150
So little bit of a roadmap here for the remainder of the talk.

214
00:12:07.990 --> 00:12:10.700
I mean motivated dawn and the challenge of what we're calling a sort of fast

215
00:12:10.701 --> 00:12:10.961
data,

216
00:12:10.961 --> 00:12:14.650
more telemetry than you can steak shake a stick at and certainly more telemetry

217
00:12:14.651 --> 00:12:16.660
than you ever want to look at manual in your own.

218
00:12:17.260 --> 00:12:19.330
I'll spend a little of time describing,
you know,
what's missing,

219
00:12:20.650 --> 00:12:22.810
what's missing from streaming machine learning systems today,
right?

220
00:12:22.811 --> 00:12:24.640
Why do we need a new system like macro basis?

221
00:12:24.880 --> 00:12:28.030
I'll describe our approach in the macro based engine to building end to end

222
00:12:28.031 --> 00:12:30.760
model cascades that are able to filter and aggregate the stream.

223
00:12:31.150 --> 00:12:33.280
And I'll spend most of my time at,

224
00:12:33.281 --> 00:12:36.280
towards the end describing sort of how putting these operators together and an

225
00:12:36.281 --> 00:12:40.990
end to end system can enable new optimizations that essentially aren't,
uh,

226
00:12:41.170 --> 00:12:41.921
aren't possible,

227
00:12:41.921 --> 00:12:45.370
or it would never be sort of feasible if we just studied this statistical

228
00:12:45.810 --> 00:12:48.790
operators in isolation,
right?
So basically by putting things end to end,

229
00:12:49.000 --> 00:12:52.420
we can suddenly optimize entire pipelines and I can give us orders of magnitude

230
00:12:52.421 --> 00:12:54.850
speedups I'll discuss this as a form of,
uh,

231
00:12:54.860 --> 00:12:58.540
unsupervised density estimation and because everyone loves neural networks all

232
00:12:58.541 --> 00:13:01.870
to talk about some more people doing on accelerating inference over a video

233
00:13:01.871 --> 00:13:05.350
streams.
Okay.
So moving right along,

234
00:13:05.470 --> 00:13:09.850
what's missing with today's sort of streaming a ML systems?
Well,
you know,

235
00:13:09.880 --> 00:13:13.780
in theory we have really wonderful infrastructure,
right?
It's 2017.

236
00:13:13.781 --> 00:13:17.680
We can pat ourselves on the back that we have a large number of choices for

237
00:13:18.010 --> 00:13:20.800
deploying sort of functions on streams,
right?

238
00:13:20.920 --> 00:13:25.840
So one of my favorite articles from the last,
uh,
year is this article,

239
00:13:26.050 --> 00:13:28.780
all the patchy streeting projects and exploratory guides.

240
00:13:28.781 --> 00:13:33.700
There are over 20 different a data flow engines in the Apache software

241
00:13:33.701 --> 00:13:35.500
foundation on loan that we can run and download,

242
00:13:35.501 --> 00:13:37.570
including one of my favorites from,

243
00:13:37.600 --> 00:13:42.340
from you all that sort of the the beam project.
Right.
Um,
so this is really great.

244
00:13:42.400 --> 00:13:45.310
We can basically run a function over a bunch of streams on a bunch of servers.

245
00:13:45.340 --> 00:13:49.030
So we kind of have half of the equation required to be able to sort of winnowed

246
00:13:49.031 --> 00:13:52.150
down these streams.
Uh,
the problem here is that,
uh,

247
00:13:52.270 --> 00:13:55.270
the stream processing engines aren't exactly batteries included for a lot of

248
00:13:55.271 --> 00:13:58.180
these tasks.
As I mentioned in the dawn question we're trying to address,

249
00:13:58.510 --> 00:14:01.690
there's a lot of work that goes into actually building production quality data

250
00:14:01.691 --> 00:14:04.060
pipelines and a machine learning products.

251
00:14:04.061 --> 00:14:09.061
And so today if we wanted to say query for outlying device readings in a data

252
00:14:09.131 --> 00:14:12.000
stream,
right,
in telemetry stream using Apache spark,
you know,

253
00:14:12.030 --> 00:14:14.920
we sort of asked the Apache spark creators,
including Latte,
you know,

254
00:14:14.921 --> 00:14:17.230
spark would say,
well,
you know,
sure we can do that.

255
00:14:17.231 --> 00:14:20.770
Just write your own user defined function for kernel density estimation.

256
00:14:20.770 --> 00:14:23.890
You know,
read a textbook,
implement colonel NC estimation and you're fine.

257
00:14:24.640 --> 00:14:29.050
And what we find,
uh,
you know,
by and large that although this is a valid answer,

258
00:14:29.150 --> 00:14:34.070
uh,
the actual implementation of these statistical operators at scale remains a

259
00:14:34.300 --> 00:14:36.940
rarely completed a exercise for the end user,
right?

260
00:14:37.150 --> 00:14:41.140
What we essentially see as people by and large have the stream processing and

261
00:14:41.141 --> 00:14:45.490
use but ultimately rely on fairly brittle but relatively fast to execute sort of

262
00:14:45.491 --> 00:14:49.690
static rules instead.
So a lot of the goodness we could get from stats,
uh,

263
00:14:49.691 --> 00:14:52.790
essentially remains,
uh,
in the textbooks.
Okay.

264
00:14:53.150 --> 00:14:56.210
So we've got a lot of data flow engines you use got to implement their own

265
00:14:56.211 --> 00:14:58.590
functionality.
You can't just do the stuff with,
uh,

266
00:14:58.830 --> 00:15:01.010
with a joint or a selector project operator.

267
00:15:01.190 --> 00:15:03.830
There's a lot of stuff from statistics,
but it's unclear,
you know,

268
00:15:03.831 --> 00:15:07.610
what we should deploy and how it can compose and combine these things into an

269
00:15:07.611 --> 00:15:08.570
end to end engine.

270
00:15:08.810 --> 00:15:13.280
So you sort of need a really rare combination of disciplines in order to be able

271
00:15:13.281 --> 00:15:15.830
to build these types of,
of pipelines today.

272
00:15:15.980 --> 00:15:20.240
You need not just that domain expertise to know,
hey,
I'm an application builder.

273
00:15:20.750 --> 00:15:23.960
I know what my ios application,
you know,
what normal activity looks like,

274
00:15:24.020 --> 00:15:25.180
but I also need to know,
you know,

275
00:15:25.280 --> 00:15:28.340
statistics and machine learning and he know how to write a bunch of spark code,

276
00:15:28.341 --> 00:15:32.950
which essentially in our experience does not really exist in anyone or,

277
00:15:32.980 --> 00:15:33.850
or if it does,
it's,

278
00:15:33.870 --> 00:15:37.880
it's going to be the highest value organizations within sort of the most

279
00:15:38.060 --> 00:15:41.990
impressive companies like Google.
Okay.
So I'm gonna make it easier to do this.

280
00:15:42.140 --> 00:15:45.680
And to do so we're building essentially a new stream processing engine

281
00:15:45.860 --> 00:15:50.010
specifically designed to accelerate these sort of streaming ml workloads.
And,

282
00:15:50.060 --> 00:15:53.000
and here's where I'm gonna Start to drop down and abstraction to give you a

283
00:15:53.001 --> 00:15:56.510
little bit of insight into what we're actually doing a under the hood here.

284
00:15:56.570 --> 00:16:00.080
So the core questions we're asking in macro basis as part of dawn's,

285
00:16:00.081 --> 00:16:02.920
there's a five year project,
incidentally,
you know,

286
00:16:02.930 --> 00:16:05.390
five years corresponds to the end of my tenure clock.

287
00:16:05.540 --> 00:16:08.700
So it's a good sort of a timeline there.
Um,

288
00:16:08.990 --> 00:16:12.740
so I'm betting pretty big on these questions.
Um,
so firstly we want to know is,

289
00:16:12.741 --> 00:16:15.980
you know,
what should we actually run in this operator in this engine,
right?
So,

290
00:16:16.130 --> 00:16:18.840
so we knew from like sequel data warehouses,

291
00:16:18.860 --> 00:16:20.930
select project joint are really useful and we can,

292
00:16:20.931 --> 00:16:24.620
we can kind of string these operators together to build some pretty cool and

293
00:16:24.621 --> 00:16:27.800
interesting applications,
right?
We can do business analytics,

294
00:16:27.860 --> 00:16:29.420
we can do customer segmentation and so on.

295
00:16:29.421 --> 00:16:33.800
So what's the analog of these operators for this type of sort of fast data

296
00:16:33.801 --> 00:16:37.370
analysis where if I gave you like a data cube operator of the streams,

297
00:16:37.490 --> 00:16:41.120
you'd have way too many results,
right?
So what should we run instead?

298
00:16:42.080 --> 00:16:43.690
And then the second question is,
you know,

299
00:16:43.700 --> 00:16:47.510
how can we use techniques from sort of conventional database systems and sort of

300
00:16:47.511 --> 00:16:51.710
large scale data processing engines and the design of these operators that can

301
00:16:51.711 --> 00:16:55.490
actually achieve the scale we need to get up to say millions of events per

302
00:16:55.491 --> 00:16:56.330
second.
Okay.

303
00:16:56.870 --> 00:17:01.130
So those are the sort of two questions we're grappling with in this project.
Um,

304
00:17:01.430 --> 00:17:03.440
towards the first question of semantics,
uh,

305
00:17:03.470 --> 00:17:08.090
the way that macro based works is we actually combine a three core operators.

306
00:17:08.091 --> 00:17:12.200
Okay.
So we essentially execute cascades of statistical operators that sort of

307
00:17:12.201 --> 00:17:16.050
transform,
filter and aggregate the stream.
So just for a show of hands,

308
00:17:16.051 --> 00:17:20.330
and I read it,
I can't see people on the GVC,
but who here has sort of um,
uh,

309
00:17:20.360 --> 00:17:24.230
familiar with the statistical classification or unsupervised classification?

310
00:17:25.730 --> 00:17:28.100
Okay.
So maybe,
maybe half,
which is great.
Okay.

311
00:17:28.101 --> 00:17:31.250
So I'll give kind of the higher level over you and then a lower level overview.

312
00:17:31.280 --> 00:17:34.730
Right?
Um,
so the way to think about how we're going to process these streams,

313
00:17:34.820 --> 00:17:38.300
and I'll illustrate this through a demo,
is that we're going to,
first of all,

314
00:17:38.630 --> 00:17:41.690
taking these data streams and,
and using sort of domain specific information.

315
00:17:41.691 --> 00:17:42.650
This could be a neural network,

316
00:17:42.651 --> 00:17:44.840
this could be a set of rules or transformation functions.

317
00:17:44.990 --> 00:17:47.120
We're going to extract the features we're looking for from the stream.

318
00:17:47.121 --> 00:17:51.520
So if we're looking for say,
oscillations in a time series,
well employee,

319
00:17:51.580 --> 00:17:55.320
a sort of feature transformation operator that will pick out and record over the

320
00:17:55.321 --> 00:17:59.820
last say five minutes of the stream where the oscillations line will

321
00:17:59.821 --> 00:18:03.930
subsequently apply what's called classification or statistical classification to

322
00:18:03.931 --> 00:18:05.970
segment the stream into examples,

323
00:18:06.030 --> 00:18:09.540
often of good behavior and bad behavior for instance,
and say this is our,

324
00:18:09.541 --> 00:18:12.630
these are abnormally high oscillations are abnormally low oscillations.

325
00:18:13.050 --> 00:18:16.260
And finally,
instead of simply reporting all of the raw streams,
the users,

326
00:18:16.261 --> 00:18:18.330
let's say we're getting 10,000,
10 million events per second,

327
00:18:18.450 --> 00:18:21.060
we don't want report 10 million points to the end user.

328
00:18:21.240 --> 00:18:24.800
We're going to roll these things up by essentially aggregating them to give her

329
00:18:24.980 --> 00:18:29.820
a results like reading some android galaxy s five devices running application

330
00:18:29.821 --> 00:18:33.660
version 52 are 30 times more likely than others to have abnormally high

331
00:18:33.661 --> 00:18:37.520
frequency.
Okay,
so illustrate this through it through a quick example.
Um,

332
00:18:38.010 --> 00:18:41.960
as I mentioned,
macro basis,
a stream processing engine,
uh,
so it's essentially a,

333
00:18:41.990 --> 00:18:46.200
um,
a data flow engine,
kind of like spark or um,

334
00:18:46.830 --> 00:18:48.540
or storm or beam.

335
00:18:48.780 --> 00:18:53.780
But we've also using this data flow engine essentially built a couple of front

336
00:18:54.091 --> 00:18:57.780
ends on top of it.
Okay.
So,
so what I'm demonstrating here is a front end.

337
00:18:57.781 --> 00:19:01.170
We've built just a simple rest server that's going to call the core stream

338
00:19:01.171 --> 00:19:05.130
processing operators that will essentially enable us to run over historical
data,

339
00:19:05.160 --> 00:19:06.120
right?
So you need,

340
00:19:06.121 --> 00:19:10.110
this is kind of exploratory console that's going to exercise the core,
um,
uh,

341
00:19:10.260 --> 00:19:13.080
cascade functionality,
uh,
that I described earlier.

342
00:19:13.860 --> 00:19:15.300
So what does this look like?

343
00:19:15.420 --> 00:19:18.520
I've got a postgrest server running on my local machine.
Um,

344
00:19:18.750 --> 00:19:20.550
it's got a bunch of data from mobile devices.

345
00:19:20.551 --> 00:19:24.660
This is a simple or similar to what we saw with the CMT data.

346
00:19:25.170 --> 00:19:27.120
And if we just look at a sample of this,
uh,

347
00:19:27.150 --> 00:19:31.800
for instance from the database is what we might look at if we did a select is

348
00:19:31.801 --> 00:19:36.480
actually running a select star and dumping the results,
um,
on,
uh,
in the Ui,
right?

349
00:19:36.481 --> 00:19:40.500
So we've got a bunch of records from giving users and state make model firmware

350
00:19:40.510 --> 00:19:44.130
version,
APP version,
and we have some metrics like temperature,
battery drain,

351
00:19:44.131 --> 00:19:45.840
trip time.
Okay.
So,
you know,

352
00:19:45.841 --> 00:19:50.460
if you and I have say a SQL database or we have something like Tablo or a bi

353
00:19:50.461 --> 00:19:52.890
tool,
this is probably what we're working with,
right?

354
00:19:53.100 --> 00:19:57.390
And maybe we can identify some roses having say hi Tripp time or high battery

355
00:19:57.391 --> 00:20:01.400
drain.
Um,
there's a lot of rows in this table in particular.
So,
um,

356
00:20:01.500 --> 00:20:04.600
actually if we scroll through all of these,
we'd probably here be here till um,

357
00:20:04.710 --> 00:20:08.550
sometime tomorrow.
Um,
and,
and you know,
the challenge here is first of all,

358
00:20:08.551 --> 00:20:12.030
identifying,
you know,
which of these rows is abnormal compared to the population.

359
00:20:12.330 --> 00:20:12.811
And second of all,

360
00:20:12.811 --> 00:20:15.810
when we have this sort of proliferation of make and model and so on,

361
00:20:15.960 --> 00:20:17.910
it's going to take us a long time sort of piece together.

362
00:20:18.000 --> 00:20:20.490
Are there significant behaviors going on in the street?

363
00:20:21.120 --> 00:20:24.420
So to give an example of this type of functionality we're supporting in macro

364
00:20:24.421 --> 00:20:25.360
base,
um,

365
00:20:26.310 --> 00:20:29.970
what we've enabled is basically automating a process.

366
00:20:29.971 --> 00:20:33.030
And a lot of people do today when something goes wrong.
So let's say we,

367
00:20:33.031 --> 00:20:35.850
let's say we are key performance indicators.
Here's battery drain.

368
00:20:35.910 --> 00:20:38.800
So we care about say,
abnormally high battery drain.
Um,

369
00:20:38.930 --> 00:20:42.360
macro based can automatically apply a unsupervised density estimation.

370
00:20:42.361 --> 00:20:46.530
So here I'm just an apply a robust estimator of basically mad here if you're

371
00:20:46.531 --> 00:20:47.290
familiar with this,

372
00:20:47.290 --> 00:20:50.380
but I'm going to find the weirdest battery points with battery drain and the

373
00:20:50.381 --> 00:20:54.490
Stream.
Instead of reporting all of these individual points,
macro base,

374
00:20:54.491 --> 00:20:57.850
we'll do a roll up according to you particular attributes that I'm looking for.

375
00:20:57.851 --> 00:20:58.350
So,

376
00:20:58.350 --> 00:21:03.330
so I can say find me a APP application version and make and model,
uh,

377
00:21:03.530 --> 00:21:07.720
that,
that are unduly or highly correlated with,
with,
with extreme battery drain.

378
00:21:07.870 --> 00:21:11.180
Okay.
So basically I've done some really simple feature selection here,
uh,

379
00:21:11.530 --> 00:21:14.620
expressing a metric of interest and some attributes that I'd like to sort of

380
00:21:14.621 --> 00:21:18.250
roll up by this specifies a query.
Am I kicking analyze?

381
00:21:18.340 --> 00:21:22.000
I actually kick off the stream processor under the hood,
right?
So this basically,

382
00:21:22.040 --> 00:21:24.770
uh,
in this little,
little bit less than a second,
uh,

383
00:21:24.820 --> 00:21:28.540
sort of sucked in about a hundred thousand rows from the postgres database and,

384
00:21:28.570 --> 00:21:28.691
uh,

385
00:21:28.691 --> 00:21:33.370
applied classifier and an aggregation function and spit out a result as follows.

386
00:21:33.760 --> 00:21:38.760
So this combination of application version hardware make and hardware model was

387
00:21:38.890 --> 00:21:43.890
a 472 times more likely to result in a abnormally high battery drain than the

388
00:21:44.531 --> 00:21:47.110
overall population.
Okay.
Which seems somewhat problematic.

389
00:21:47.111 --> 00:21:49.990
There are 849 records match this particular filter.

390
00:21:50.380 --> 00:21:53.020
And if we look at the actual plot of these battery drain readings,

391
00:21:53.230 --> 00:21:56.170
the overall distribution of the population is here in blue.

392
00:21:56.860 --> 00:22:01.860
And this particular subgroup I identified by this sort of conjunct have app

393
00:22:01.991 --> 00:22:05.110
version harder making harder model has a distribution that significantly

394
00:22:05.111 --> 00:22:06.520
different than the overall population,

395
00:22:06.610 --> 00:22:10.430
which indicates there's something possibly going on with this particular,
uh,

396
00:22:10.450 --> 00:22:14.350
a subgroup and we can go in and we can drill in this exact data or we can

397
00:22:14.351 --> 00:22:18.700
generate a SQL query to dump this into a,
um,
uh,
you know,
downstream,
uh,

398
00:22:18.730 --> 00:22:22.720
engine like Tab Tablo in order to drill deeper or we just go and get the CSV and

399
00:22:22.721 --> 00:22:24.370
go play with this thing in pandas.
Okay.

400
00:22:24.610 --> 00:22:28.990
But the idea here is that essentially by highlighting a small number of target

401
00:22:28.991 --> 00:22:30.970
metrics and explanatory attributes,

402
00:22:31.180 --> 00:22:34.660
we can sort of sift through these very large data volumes.

403
00:22:34.810 --> 00:22:38.320
And actually get out of much smaller set of explanations or summaries that

404
00:22:38.321 --> 00:22:41.350
described the most unusual or abnormal behavior.
Okay.

405
00:22:41.770 --> 00:22:45.490
So you can think of this as sort of,
um,
doing what,
similar to what,
uh,

406
00:22:45.550 --> 00:22:46.840
from the sort of users productive.

407
00:22:46.841 --> 00:22:49.360
This is sort of similar what people do in root cause analysis,
right?

408
00:22:49.361 --> 00:22:51.310
You want to find out,
okay,
some alarm went off.

409
00:22:51.370 --> 00:22:53.580
What's common among what distinguishes the things that are,
you know,

410
00:22:53.680 --> 00:22:56.080
firing alerts versus things that are not right here.

411
00:22:56.081 --> 00:22:58.930
We're just automating that process.
When the ML side of the house,

412
00:22:58.931 --> 00:23:01.720
we're applying unsupervised density estimation,
uh,

413
00:23:01.750 --> 00:23:05.630
here in a univariate setting and we're applying essentially,
um,
common tutorial,

414
00:23:05.740 --> 00:23:10.060
a feature selection routines over the explanatory attributes in order to pull

415
00:23:10.061 --> 00:23:13.780
out these high level summaries that had non expert user can go and sort of dig

416
00:23:13.781 --> 00:23:16.570
into.
Now they're sort of cool part of the,
about this,

417
00:23:16.690 --> 00:23:18.610
about doing this in a unified engine.
As I said,

418
00:23:18.611 --> 00:23:21.130
we're kind of running a streaming query under the hood,
right?

419
00:23:21.430 --> 00:23:25.570
So when I actually run this through the Ui,
I also get a config file,

420
00:23:25.750 --> 00:23:28.180
which can then be used as a seed in a streaming job.

421
00:23:28.181 --> 00:23:30.750
So you can basically take the output of the exploratory you,

422
00:23:31.180 --> 00:23:34.780
we can basically get the config,
like if you say I liked that result,

423
00:23:34.781 --> 00:23:38.020
please keep it up to date and we can now run a stream processing job that will

424
00:23:38.021 --> 00:23:42.270
continually ingest the data and we'll keep this thing up to date.
Okay.
So,
um,

425
00:23:42.650 --> 00:23:45.920
benefits of having sort of a unified a system here.

426
00:23:47.900 --> 00:23:49.240
All right,
so

427
00:23:52.980 --> 00:23:54.520
great.
So what are we doing?
Um,

428
00:23:55.590 --> 00:23:56.030
<v 3>okay,</v>

429
00:23:56.030 --> 00:23:58.610
<v 0>under the hood,
right?
So,
um,</v>

430
00:23:59.480 --> 00:24:03.030
we're applying this transformation rule.
In the case of the,
of the,

431
00:24:03.031 --> 00:24:04.400
of the relational data I showed,

432
00:24:04.460 --> 00:24:09.370
we're basically just having users select columns,
like subsets of columns,
uh,

433
00:24:09.440 --> 00:24:11.270
for use it in downstream analytics.

434
00:24:11.670 --> 00:24:15.260
I'll show some examples of images and videos later.
But for instance,
uh,

435
00:24:15.350 --> 00:24:18.310
if you want,
this is what we did.
We did a test with satellite imagery where,

436
00:24:18.320 --> 00:24:21.020
you know,
clearly don't want to look for abnormal pixels,
but you can extract,

437
00:24:21.021 --> 00:24:23.270
you know,
domain specific features like human luminosity.

438
00:24:23.271 --> 00:24:24.470
So we were able to pick out,
you know,

439
00:24:24.590 --> 00:24:27.410
the bit using time varying satellite imagery.
You know,

440
00:24:27.500 --> 00:24:31.790
if we look at year over year differences in Hue,
right?

441
00:24:31.791 --> 00:24:34.490
You can pick out things like um,
uh,

442
00:24:34.491 --> 00:24:39.020
the bay area wasn't a drought in 20,
was it 2014 here,
right?

443
00:24:39.250 --> 00:24:42.170
Um,
this is pretty straight forward for time series,
right?

444
00:24:42.170 --> 00:24:44.180
If we don't want to just feed in each point as it comes in,

445
00:24:44.181 --> 00:24:48.620
we can run sort of an FFT or,
or segment the stream into,
into windows.
Okay.

446
00:24:49.220 --> 00:24:52.330
Um,
then we sort of apply once we have these sort of data points that,

447
00:24:52.331 --> 00:24:55.610
that specify what we're looking for in terms of KPIs or abnormal behavior.

448
00:24:55.730 --> 00:24:58.310
We apply a classifier,
right?
So our default behavior,

449
00:24:58.311 --> 00:25:01.400
because most users don't have labels for a lot of their streams,

450
00:25:01.760 --> 00:25:04.580
we apply what's known as sort of unsupervised tensey estimation.

451
00:25:04.581 --> 00:25:08.880
So we essentially over the stream,
um,
learn what the distribution of,

452
00:25:08.881 --> 00:25:13.340
of data points looks like,
um,
and essentially find points in the tail.

453
00:25:13.341 --> 00:25:17.000
So we measure,
you know,
basically a a mean and standard deviation.

454
00:25:17.001 --> 00:25:19.280
Then find things that are fairly far away from the mean.

455
00:25:20.090 --> 00:25:22.970
This is a bit of a cartoon picture and that,
you know,
in practice,

456
00:25:22.971 --> 00:25:25.280
some of our distributions look much stranger,
right?

457
00:25:25.280 --> 00:25:27.260
So we'll use something like a kernel density estimator.

458
00:25:27.410 --> 00:25:29.120
But the basic idea here is that,
you know,

459
00:25:29.210 --> 00:25:33.230
with relatively simple classifiers that have no information about the sort of,

460
00:25:33.260 --> 00:25:37.930
um,
labels,
we can actually pick out these sorts of abnormal,
uh,
events.
Um,

461
00:25:38.000 --> 00:25:38.780
and finally,

462
00:25:38.780 --> 00:25:42.470
instead of simply returning all of these points that we think are sort of,
uh,

463
00:25:42.650 --> 00:25:46.730
anomalous or unusual and asking users,
hey,
figure this out for us.
Right?

464
00:25:47.340 --> 00:25:48.500
And this is actually pretty important,
right?

465
00:25:48.501 --> 00:25:50.480
It's really a pain to actually go in and say,

466
00:25:50.600 --> 00:25:53.000
is there something going on with HTC devices?
There's something going on.

467
00:25:53.001 --> 00:25:56.510
TCT devices.
Are there combinations here?
Like humans are terrible combinations.

468
00:25:56.960 --> 00:25:59.420
Nothing wrong with us,
but we should let already know,
you know,
you know,

469
00:25:59.421 --> 00:26:03.710
robot overlords,
do the work for us.
Um,
macro basis essentially lets us,
you know,

470
00:26:03.770 --> 00:26:08.630
roll up the stream.
So if we get a a stream of say errors and non errors,
um,

471
00:26:08.960 --> 00:26:09.711
that looks like this,

472
00:26:09.711 --> 00:26:12.140
we can ask you to what makes the air is different than the non era.

473
00:26:12.140 --> 00:26:16.970
So this is sort of like running lasso over the,
um,
uh,
two classes,
but,
but the,

474
00:26:17.000 --> 00:26:20.200
but the basic sort of interpretation of this is we say,
what,
what,

475
00:26:20.201 --> 00:26:23.060
what makes these two different?
Right?
So if we look and we just say,
well,

476
00:26:23.180 --> 00:26:28.180
I phone six occurs three times and the errors and it occurs one,

477
00:26:28.701 --> 00:26:33.290
two,
three,
four cars many times.
And the Non Eris,
uh,
maybe not that interesting.

478
00:26:34.100 --> 00:26:35.750
If we look at Canada,
uh,

479
00:26:35.780 --> 00:26:39.140
we see her three times in the errors and doesn't care at all and the non errors.

480
00:26:39.170 --> 00:26:42.140
So we could say,
you know,
looks like Canada may have a problem,
right?

481
00:26:42.410 --> 00:26:47.250
And we can sort of use this measure of co-occurrence with this risk ratio as a

482
00:26:47.251 --> 00:26:52.170
measure of severity,
right?
Where we can say,
okay,
this is the most severe.

483
00:26:52.460 --> 00:26:57.370
A sort of combination of attributes it causes is problematic behavior.
So,

484
00:26:57.390 --> 00:27:02.390
so what we're doing is we're getting away from results that look like this right

485
00:27:02.521 --> 00:27:06.720
and tabular form and we're getting to results look like this where we're

486
00:27:06.721 --> 00:27:11.310
highlighting correlated attributes and uh,
also sort of,
you know,

487
00:27:11.370 --> 00:27:14.850
differences in distribution.
So user select our key performance metrics,

488
00:27:14.910 --> 00:27:16.500
metrics that are explanatory attributes.

489
00:27:16.680 --> 00:27:21.210
We classify a metrics and points that behave at that belong in the tails and

490
00:27:21.211 --> 00:27:24.420
then we generate sort of classifications or sorry,

491
00:27:24.450 --> 00:27:28.200
we generate explanations using these attributes by doing sort of hypothesis

492
00:27:28.201 --> 00:27:33.150
testing.
Okay.
Any questions so far?
Okay.
So,
uh,

493
00:27:33.180 --> 00:27:36.000
one thing that I'll point out that's kind of interesting here.
Um,
we've,

494
00:27:36.001 --> 00:27:38.190
we've essentially combined three different operators.

495
00:27:38.191 --> 00:27:41.550
This is a different view of the same pipeline I showed before a transformation

496
00:27:41.551 --> 00:27:42.900
classification explanation.

497
00:27:43.710 --> 00:27:46.920
And we found that by changing the transformation function and also change the

498
00:27:46.921 --> 00:27:48.510
inputs to each stage,
uh,

499
00:27:48.511 --> 00:27:52.200
we can actually do quite a bit with this with these three operators.
Right.
Um,

500
00:27:52.290 --> 00:27:55.740
so you know,
in terms of production uses,
I mentioned this is all sort of,
um,

501
00:27:55.980 --> 00:28:00.180
open source and publicly available.
Um,
you can use a couple of different ways.

502
00:28:00.180 --> 00:28:02.370
You can use the Ui as I showed in the demo,

503
00:28:02.910 --> 00:28:07.380
you can use our prebuilt sort of pipelines where we combine these things into

504
00:28:07.381 --> 00:28:09.030
sort of a recipes for you.

505
00:28:09.240 --> 00:28:12.770
Or you can just take the individual operators like our fast,
uh,

506
00:28:12.870 --> 00:28:15.850
explanation operators and plugged them into existing stream processing of them.

507
00:28:15.851 --> 00:28:19.140
So,
so we've,
we've sort of found people using this,
this technology,

508
00:28:19.230 --> 00:28:23.370
it's all available on get hub for different things.
So,
so in automotives,
uh,

509
00:28:23.371 --> 00:28:25.190
one of these engineers who works with me on,

510
00:28:25.200 --> 00:28:29.090
on electric car batteries looked at the fleet telemetry and,
uh,

511
00:28:29.130 --> 00:28:33.270
just you just use the Ui in order to augment their existing CQL queries for

512
00:28:33.271 --> 00:28:37.680
their daily reporting in order to identify a problematic interaction with a new

513
00:28:37.681 --> 00:28:41.130
firmware version and a subset of the vehicle making models that were in the

514
00:28:41.131 --> 00:28:45.300
fleet.
Okay.
So this was kind of like a,
a cubing on steroids style approach.

515
00:28:46.080 --> 00:28:50.360
Um,
in online services,
uh,
we're actually running this,
uh,
or we,

516
00:28:50.361 --> 00:28:53.780
we run this to identify slow containers.
Okay.
So this is like,
you know,
you have,

517
00:28:54.060 --> 00:28:57.300
imagine you have a process that's spitting out huge amounts of telemetry and you

518
00:28:57.301 --> 00:29:00.190
want to know,
does landing on a given container,
uh,

519
00:29:00.310 --> 00:29:03.840
in my data center making me more or less likely to be considered an outlier for

520
00:29:03.841 --> 00:29:07.500
my job?
Okay.
Um,
we've used as industrial manufacturing.

521
00:29:07.680 --> 00:29:10.470
Maybe the most interesting one coming back to our mobile application setting is,

522
00:29:10.471 --> 00:29:13.000
um,
you know,
if,
if you have say,
um,

523
00:29:13.110 --> 00:29:16.460
exception data coming back from an application,
right?
So,
um,
in,

524
00:29:16.470 --> 00:29:20.340
in one scenario we have someone who's running an application,
uh,
live on,
um,

525
00:29:21.090 --> 00:29:25.170
at least a couple of million devices.
Um,
whenever they get an exception,

526
00:29:25.380 --> 00:29:28.470
they want to know,
you know,
is there something you know,
correlated going on?

527
00:29:28.471 --> 00:29:28.621
Right?

528
00:29:28.621 --> 00:29:32.130
So let's say you try to open an asset in their mobile application asset fails a

529
00:29:32.131 --> 00:29:35.750
load.
Is it a problem with the user is a problem for users?

530
00:29:35.760 --> 00:29:37.520
ISP is a problem with it.

531
00:29:37.620 --> 00:29:41.610
That cache server the acids located on is a problem with the,
uh,

532
00:29:41.890 --> 00:29:44.320
hardware may Carter model,
application version,
so on.

533
00:29:44.530 --> 00:29:46.600
So we essentially do automated rollups,
uh,

534
00:29:46.720 --> 00:29:49.480
for them using this explanation operator.
And for these guys,

535
00:29:49.481 --> 00:29:52.330
it's pretty interesting.
They don't even use the first half of the stack,
right?

536
00:29:52.331 --> 00:29:56.230
They already have a stream processing engine set up.
It's all JVM based.

537
00:29:56.231 --> 00:29:59.290
So they can,
they can basically take their stream of exceptions.

538
00:29:59.440 --> 00:30:02.320
They run them through the macro based operator for explanation,

539
00:30:02.321 --> 00:30:03.340
which is the aggregation.

540
00:30:03.490 --> 00:30:06.760
And if there's a correlation in the stream that's particular severity,

541
00:30:06.761 --> 00:30:10.050
let's say this app version of this,
this hardware make,
uh,

542
00:30:10.270 --> 00:30:14.380
provide say three times or five times higher than usual exception rates.

543
00:30:14.410 --> 00:30:17.660
Someone gets page.
Okay.
So,
um,

544
00:30:17.920 --> 00:30:19.810
pretty cool love open source.
Like I said,

545
00:30:19.811 --> 00:30:21.130
all this stuff we're doing is open source.

546
00:30:21.131 --> 00:30:24.040
It's a great way to get sort of feedback on these types of tasks.
And you know,

547
00:30:24.041 --> 00:30:27.640
at Cmt,
which is when I can talk about particularly public,
um,
you know,

548
00:30:27.641 --> 00:30:29.650
we were able to find some pretty crazy device,

549
00:30:29.651 --> 00:30:34.180
civic battery problems and an issue with the application on startup that we're

550
00:30:34.181 --> 00:30:36.460
basically,
you know,
unknown to these users.
Okay.

551
00:30:37.150 --> 00:30:41.620
So we think this is a really cool sort of combination of physical operators

552
00:30:41.621 --> 00:30:44.920
where today you'd have to implement all of this yourself.
And we found is that,

553
00:30:44.921 --> 00:30:48.070
you know,
for these non expert users who are really fantastic,

554
00:30:48.071 --> 00:30:50.500
like they know everything about their mobile application,

555
00:30:50.501 --> 00:30:53.110
I know everything about their sort of data center deployment or know everything

556
00:30:53.111 --> 00:30:57.260
about um,
say automotive's uh,
you know,

557
00:30:57.310 --> 00:31:01.180
they don't have the background or the time to go and build up the streaming

558
00:31:01.181 --> 00:31:04.630
infrastructure so much.
So providing sort of batteries included,
um,
you know,

559
00:31:04.631 --> 00:31:09.220
you wise or pipelines or even the ability to deploy new custom data flow

560
00:31:09.221 --> 00:31:11.320
operators.
It's actually a fairly useful,

561
00:31:11.321 --> 00:31:15.100
I'm a piece of software and of course on the research side,
like I said,

562
00:31:15.101 --> 00:31:16.150
I got to get tenure.
Uh,

563
00:31:16.280 --> 00:31:18.730
there's a lot of things that we're able to do under the hood when we put these

564
00:31:18.731 --> 00:31:23.290
operators together that are really not possible,
uh,
in isolation,
right?

565
00:31:23.410 --> 00:31:28.280
So we're able to speed up conventionally a highly optimized operators for say

566
00:31:28.281 --> 00:31:31.540
classification by taking advantage of the fact that we're not running over say

567
00:31:31.570 --> 00:31:32.800
arbitrary data points,

568
00:31:32.950 --> 00:31:36.160
but that we're running over a particular user stream with a particular set of

569
00:31:36.370 --> 00:31:36.820
properties.

570
00:31:36.820 --> 00:31:41.800
So I'm going to give kind of three examples of this type of optimization that I

571
00:31:41.801 --> 00:31:46.660
think are particularly exciting.
So this brings us from the semantic side,

572
00:31:46.780 --> 00:31:48.070
uh,
down to the implementation.

573
00:31:48.070 --> 00:31:51.430
So how can we use techniques from systems in order to,
um,

574
00:31:51.610 --> 00:31:54.910
sort of speed up the execution of this,
of these types of operators?

575
00:31:55.120 --> 00:31:56.080
And the cool part,

576
00:31:56.081 --> 00:31:58.850
or the thing I'm most excited about right in the students are like just,

577
00:31:59.080 --> 00:32:02.050
you know,
not only crushing a bunch of papers,
pushing a bunch of papers out,

578
00:32:02.051 --> 00:32:04.750
but they're also sort of finding these things can really lead to quantify

579
00:32:04.780 --> 00:32:07.280
qualitatively different user experiences.
Um,

580
00:32:07.480 --> 00:32:09.280
is it by bringing these teams together,

581
00:32:09.281 --> 00:32:12.210
we get new sort of opportunities for end to end optimization.

582
00:32:12.240 --> 00:32:14.980
I'll just give you sort of three quick examples.
In new,

583
00:32:14.981 --> 00:32:17.170
we're going to get to the neural networks as the third example.
Okay.

584
00:32:17.350 --> 00:32:21.220
So as promised,
uh,
you thought you knew everything about,
um,
modeled distillation,

585
00:32:21.320 --> 00:32:23.470
turns out you can go even faster.
Okay.
All right.

586
00:32:23.471 --> 00:32:25.900
There's a teaser for those of you and owners in the audience.
Okay.

587
00:32:26.350 --> 00:32:28.720
So symbol warmup.
Okay,
this one's for,
this one's pretty cool.

588
00:32:28.810 --> 00:32:32.170
So let's say we wanted this explanation thing and macro based,

589
00:32:32.171 --> 00:32:35.230
we want to say what makes the good stuff different from the bad stuff.

590
00:32:35.231 --> 00:32:38.350
Let's say we have a bunch of bad bad stuff in red and a bunch of good stuff in

591
00:32:38.351 --> 00:32:40.880
green and the green probably goes all the way down until you know the center of

592
00:32:40.881 --> 00:32:42.890
the core.
Cause there's a lot more good stuff going on.

593
00:32:42.891 --> 00:32:45.620
Applications and bad stuff,
right?
If your application is on fire,

594
00:32:45.621 --> 00:32:48.890
you probably know about it.
Okay.
Um,
so,
so,

595
00:32:48.980 --> 00:32:51.620
so what sort of the canonical way that we go about doing this?
Well,

596
00:32:51.710 --> 00:32:55.670
we're sort of trying to look at what are highly correlated members of each of

597
00:32:55.671 --> 00:32:58.730
each class here.
So what makes the red or red and what makes the green and green?

598
00:32:58.940 --> 00:33:01.460
Okay.
So in this setting,
you know,

599
00:33:01.490 --> 00:33:05.450
we could very easily sort of compute correlations within each class

600
00:33:05.451 --> 00:33:06.320
independently,
right?

601
00:33:06.530 --> 00:33:09.920
So we go over the red and say a occurs 80% of the time be occurs.

602
00:33:10.010 --> 00:33:12.860
20% of the time we'd go over all the Greens and say,
okay,

603
00:33:12.861 --> 00:33:17.780
the A's occur 0.1% of the time,
uh,
bees occur 46% of the time and so on.

604
00:33:17.900 --> 00:33:20.410
Okay.
Um,
this would work.
Uh,

605
00:33:20.630 --> 00:33:23.570
but to be fairly expensive because if we want to compute not just individual

606
00:33:23.571 --> 00:33:24.500
element correlations,

607
00:33:24.501 --> 00:33:27.680
we want to compute pairwise order three or order for combinations.

608
00:33:27.830 --> 00:33:31.520
How often is ABCD occur?
Often is Abe occur,
you know,

609
00:33:31.521 --> 00:33:35.960
all of these sort of rare sort of combinations that may in fact be correlated

610
00:33:35.961 --> 00:33:40.190
with a bug.
This can be really,
really slow.
Okay.
So,
you know,

611
00:33:40.191 --> 00:33:41.170
think about this for a second.
You know,

612
00:33:41.171 --> 00:33:45.080
how can we go faster than simply running some sort of correlation,
you know,

613
00:33:45.110 --> 00:33:48.440
mining or detection procedure over each individual group?

614
00:33:50.510 --> 00:33:52.340
Well,
the answer's a little bit,
you know,
it,
it's,

615
00:33:52.341 --> 00:33:56.780
it's sort of implied on the slide.
No,
there's not that many red things.

616
00:33:57.410 --> 00:33:58.850
Yeah.
There's not much on the left.

617
00:33:59.150 --> 00:34:02.760
And so we found is that if we exploit this sort of what we call it,

618
00:34:02.870 --> 00:34:05.960
conventional database engineer,
the cardinality,
imbalanced Phoenix classes,

619
00:34:05.961 --> 00:34:08.750
we can go much faster.
It's almost like when you're running a join,

620
00:34:08.751 --> 00:34:13.160
if you want to join a with B and a is much,
much,
much,
much,
much smaller than B,

621
00:34:13.490 --> 00:34:17.150
you start by building a Hashtag on a and then you only go and touch the parts of

622
00:34:17.151 --> 00:34:19.790
B that you actually need to go look for,
right?

623
00:34:20.030 --> 00:34:21.590
So we do the same thing in macro base.

624
00:34:21.770 --> 00:34:26.600
We first detected sort of meaningful correlations and the red class and only

625
00:34:26.601 --> 00:34:27.080
once we,

626
00:34:27.080 --> 00:34:30.320
once we come up with a candidate set of correlations for elements that are

627
00:34:30.321 --> 00:34:35.210
highly correlated,
like,
hey,
then we go and actually touch the,
the green stuff,

628
00:34:35.240 --> 00:34:39.860
the in liars,
right?
So basically,
um,
we can avoid processing,
you know,

629
00:34:39.920 --> 00:34:43.850
the Bs and the Cs and the ds and also avoid processing,
you know,

630
00:34:43.851 --> 00:34:47.650
higher order combinations of these things without actually the full expense.
Um,

631
00:34:48.460 --> 00:34:49.191
I'm doing this not usually.

632
00:34:49.191 --> 00:34:52.250
So by knowing the fact that one stream is much bigger than the other,

633
00:34:52.280 --> 00:34:54.230
we can go much,
much faster.
And,
and you know,

634
00:34:54.231 --> 00:34:55.700
you can read the papers if you want to see this,
but you know,

635
00:34:55.701 --> 00:34:58.520
it's like orders of magnitude faster.
Uh,
it's like,
you know,

636
00:34:58.700 --> 00:35:01.790
two orders of magnitude faster than running a decision tree and three orders of

637
00:35:01.791 --> 00:35:04.870
magnitude faster than doing a lot of other stuff like building a data cube over

638
00:35:04.871 --> 00:35:06.680
your data here.
Okay.
Um,

639
00:35:06.710 --> 00:35:10.010
one of the cool things here from the stats side that I geek out about,
um,

640
00:35:10.070 --> 00:35:15.020
you know,
when we're testing all of these hypotheses,
right?
Um,
you might wonder,

641
00:35:15.080 --> 00:35:16.790
are you going to get false discoveries?
Right?

642
00:35:16.791 --> 00:35:18.230
So in medicine we sort of freak out.

643
00:35:18.380 --> 00:35:21.230
Most medical studies are false because people,
okay,

644
00:35:21.231 --> 00:35:23.780
so let's say you're a Grad student.
You finished five years of,

645
00:35:23.840 --> 00:35:26.570
of work on a clinical trial and you find out,
oh,
you know,

646
00:35:26.571 --> 00:35:31.370
bubble gum doesn't cause cancer.
Shoot.
Uh,
what am I gonna,
what am I going to do?

647
00:35:31.371 --> 00:35:33.350
I have a negative result.
No one likes negative results and science.

648
00:35:33.351 --> 00:35:36.110
So if you're a Grad state with your aunt equals 30 cohort,
you're gonna say,
well,

649
00:35:36.290 --> 00:35:39.600
I wonder if anyone eight seller,
oh,
nope.
Celery's not correlated.

650
00:35:39.720 --> 00:35:43.920
I want everyone to carrots.
Carrots aren't correlated.
Artichoke.
Oh,
artichokes.

651
00:35:43.950 --> 00:35:45.520
I find that significant correlation,

652
00:35:45.521 --> 00:35:48.030
which we artichokes and cancer in my population of size 30.
Right?

653
00:35:48.031 --> 00:35:50.700
So if you test those hypotheses,
just like the birthday paradox,

654
00:35:50.790 --> 00:35:52.500
you're going to get some false discoveries,
okay?

655
00:35:52.920 --> 00:35:55.680
So essentially in a conventional statistical setting,

656
00:35:55.830 --> 00:35:58.500
you have this problem where with limited sample size,

657
00:35:58.501 --> 00:36:01.890
you can only afford a testis limited number of hypotheses like these

658
00:36:01.891 --> 00:36:05.100
explanations for generating without getting due to random chance,

659
00:36:05.101 --> 00:36:08.220
something going wrong in the macro base setting,
right?

660
00:36:08.220 --> 00:36:11.250
With all this fast data coming in,
we have the opposite problem.

661
00:36:11.640 --> 00:36:15.940
Our problem is not that we are going to run out of some statistical budget,
uh,

662
00:36:16.110 --> 00:36:20.860
by testing too many hypotheses.
Our problem is that we have so many,
uh,

663
00:36:21.090 --> 00:36:23.340
hypotheses or we start,
we have so much data coming in,

664
00:36:23.490 --> 00:36:27.630
we need to use our limited computational budget as intelligently as possible,

665
00:36:27.631 --> 00:36:28.140
right?

666
00:36:28.140 --> 00:36:32.970
So by essentially being able to scale up and process a 500,000 events per second

667
00:36:32.971 --> 00:36:35.910
on a single core as we can with us optimize explanation operator,

668
00:36:36.090 --> 00:36:38.250
we can afford to test many,
many,

669
00:36:38.251 --> 00:36:41.910
many more possible hypotheses without compromising statistical validity.

670
00:36:42.150 --> 00:36:44.760
That is with simple corrections.
So if you're a stats nerd,

671
00:36:45.090 --> 00:36:47.490
it was simple corrections like the Bonferroni correction,

672
00:36:47.640 --> 00:36:51.730
right at 500,000 samples and a 1% sort of baseline,
um,

673
00:36:52.440 --> 00:36:56.820
uh,
exception rate.
You can afford to a test many,
many,
many different,

674
00:36:56.850 --> 00:37:00.060
many more hypothesis than you could with a sample size of say 30.
Okay.
So,

675
00:37:00.100 --> 00:37:02.950
so sort of surprising result.
Um,

676
00:37:03.420 --> 00:37:08.280
this was one example of how we can sort of explore this end to end process note

677
00:37:08.281 --> 00:37:09.560
that if we didn't have this class,

678
00:37:09.561 --> 00:37:12.990
if I operator and we just try to explain arbitrary sub arbitrary groups,

679
00:37:12.991 --> 00:37:14.970
we wouldn't have this card Nalley and balance.
We'll,

680
00:37:14.971 --> 00:37:18.270
because we know we're looking for sort of rare events and many pipelines,

681
00:37:18.390 --> 00:37:22.080
we need to exploit this aggressively to improve both result quality and

682
00:37:22.081 --> 00:37:24.090
scalability.
Right?
So,
and,
and to be,
to be clear,

683
00:37:24.091 --> 00:37:26.220
like three orders of magnitude of the difference between,
you know,

684
00:37:26.221 --> 00:37:28.050
waiting for the result,
um,
you know,

685
00:37:28.051 --> 00:37:31.080
all day versus being able to that interactive response in a browser.

686
00:37:31.081 --> 00:37:34.050
So this does qualitatively change,
uh,
one the,

687
00:37:34.051 --> 00:37:36.570
the sort of exploratory user experience,

688
00:37:36.660 --> 00:37:38.730
but also the ability to alert in real time.
Okay.

689
00:37:38.760 --> 00:37:42.270
So it's a pretty big deal despite a simple trick.
Um,

690
00:37:42.480 --> 00:37:44.180
moving up the pipeline,
uh,

691
00:37:44.190 --> 00:37:47.340
we've been looking a lot at classification as well and how we can make this

692
00:37:47.341 --> 00:37:51.030
faster for more complex distributions.
So I'll give another quick example.

693
00:37:51.960 --> 00:37:55.830
Um,
so I mentioned earlier,
we sometimes have distributions looking like this.

694
00:37:55.831 --> 00:38:00.690
This is a distribution from,
uh,
the,
uh,
UCI Dataset or UCI repository.

695
00:38:00.691 --> 00:38:04.440
It's temperature and carbon dioxide,
um,
in a,
a,
a building.

696
00:38:05.010 --> 00:38:07.080
And you might ask,
well,
how would I model this?

697
00:38:07.081 --> 00:38:10.920
How do I find the data and the tails for this particular distribution?
And a,

698
00:38:10.921 --> 00:38:13.180
if we apply something like a simple Gulshan,
uh,

699
00:38:13.200 --> 00:38:16.580
we're not going to fit this fine grain structure particularly well.
Right?
Um,

700
00:38:16.650 --> 00:38:18.030
it turns out that each of the white dots,

701
00:38:18.031 --> 00:38:21.420
if you can see them on a screen corresponds to a location or a time in which

702
00:38:21.570 --> 00:38:23.160
someone was in the room.
Okay.

703
00:38:23.161 --> 00:38:27.960
So the outliers actually are statistically at least correlated with,
with um,
uh,

704
00:38:27.990 --> 00:38:29.170
human activity.
Um,

705
00:38:29.430 --> 00:38:32.550
but we're not gonna pick that up at all if we use a simple but easy to compute

706
00:38:32.551 --> 00:38:34.020
model like a Gulshan I showed earlier,

707
00:38:35.170 --> 00:38:37.180
we use something like a Gaussian mixture model,
right?

708
00:38:37.181 --> 00:38:40.060
And say pull out 10 different Gaussians and plop them in space.

709
00:38:40.240 --> 00:38:43.090
We sort of start to resolve this finer grain structure a little bit better,

710
00:38:43.210 --> 00:38:45.340
but it's still pretty ugly,
right?
It's not really

711
00:38:46.840 --> 00:38:50.500
capturing all of the sort of structure out in this tale and so on.

712
00:38:50.501 --> 00:38:53.110
And we actually get pretty bad classification accuracy.
Uh,
if we,

713
00:38:53.111 --> 00:38:55.900
if we rely on this simple model.
So what we want,

714
00:38:55.901 --> 00:39:00.040
it's kind of the granddaddy of all unsupervised density estimators,

715
00:39:00.400 --> 00:39:01.420
which is this thing that looks like this.

716
00:39:01.421 --> 00:39:04.990
It's called the kernel density estimator where we can basically get a really

717
00:39:04.991 --> 00:39:07.270
nice heat map of the,

718
00:39:07.271 --> 00:39:10.720
of the distribution by essentially performing the following step.

719
00:39:11.680 --> 00:39:15.760
For each data point in our Dataset,
we kind of drop a Gao Shin on top of it,

720
00:39:16.150 --> 00:39:20.710
right?
And if we want to query the density of the Dataset at any given point,

721
00:39:20.711 --> 00:39:21.760
like let's say right here,

722
00:39:22.000 --> 00:39:25.990
we some of the contribution from all of the individual Gaussians and our
dataset.

723
00:39:26.460 --> 00:39:26.710
Okay.

724
00:39:26.710 --> 00:39:30.400
So it's like every single point contributes a little bit like a little bump to

725
00:39:30.401 --> 00:39:33.450
the overall estimate.
And this is wonderful because it's,
you know,

726
00:39:33.490 --> 00:39:38.250
provably ASM tonically optimal and we can essentially approximate for,
you know,

727
00:39:38.251 --> 00:39:41.560
on a very mild condition socially any distribution or an encounter in practice.

728
00:39:42.000 --> 00:39:45.280
Uh,
the problem here is that this is actually a [inaudible] squared operation,

729
00:39:45.490 --> 00:39:48.050
right?
To compute the exact density of each of these,
you know,

730
00:39:48.130 --> 00:39:52.300
points on the curve requires,
uh,
for a 500,000 point Dataset,

731
00:39:52.360 --> 00:39:54.430
two hours on a 2.4 gigahertz CPU.

732
00:39:54.431 --> 00:39:58.000
So this thing is clearly not going to scale as I mentioned,

733
00:39:58.090 --> 00:40:00.130
by essentially optimizing end to end here.

734
00:40:00.220 --> 00:40:03.330
By taking advantage of the fact that we're running over a full pipeline,
uh,

735
00:40:03.430 --> 00:40:04.750
we can do considerably better.

736
00:40:05.050 --> 00:40:09.140
And so we asked how can we take advantage of the fact that we're looking for,
um,

737
00:40:09.220 --> 00:40:11.830
uh,
data and the tails to basically run faster?

738
00:40:12.040 --> 00:40:14.860
And the answer is we can essentially apply it.

739
00:40:14.861 --> 00:40:18.430
You've got another favorite technique of mine from database systems to

740
00:40:18.431 --> 00:40:20.410
essentially prune computation.

741
00:40:20.830 --> 00:40:24.500
So the idea is that if we're just trying to identify whether or not appoint is

742
00:40:24.820 --> 00:40:28.180
above or below some cutoff to say,
is this a rare point or not,

743
00:40:28.720 --> 00:40:33.100
we don't need to waste our time computing density estimates for all of the stuff

744
00:40:33.130 --> 00:40:35.860
above the threshold illustrated.
Graphically.

745
00:40:35.861 --> 00:40:37.790
If you've got a distribution like this,
uh,

746
00:40:37.870 --> 00:40:41.140
we don't need to compute the exact color reading for each of these parts in
blue.

747
00:40:41.260 --> 00:40:44.080
We need to compute are we in the blue set or out of the blue set?

748
00:40:44.740 --> 00:40:48.810
And so we showed him a recent sigma paper that by essentially,
uh,

749
00:40:48.880 --> 00:40:53.350
computing exact estimates or bounds on densities as opposed to the exact

750
00:40:53.351 --> 00:40:55.900
densities.
We can take this kernel Nancy estimator,

751
00:40:55.930 --> 00:41:00.930
which takes two hours to run on a 2.4 gigahertz CPU and speed up by almost three

752
00:41:01.091 --> 00:41:05.650
orders of magnitude simply by recognizing that in the downstream analytics task,

753
00:41:05.770 --> 00:41:07.510
we only want to compute above or below.

754
00:41:07.540 --> 00:41:11.410
And so now this brings us KTE from the realm of sort of Nice to have

755
00:41:11.411 --> 00:41:11.981
statistically,

756
00:41:11.981 --> 00:41:16.210
but way too slow to run empirically to nice to have statistically and fast

757
00:41:16.211 --> 00:41:19.360
enough to run over over real data sets.
All right,

758
00:41:20.140 --> 00:41:22.060
one final example before I conclude,

759
00:41:22.061 --> 00:41:24.730
and I think this might be pretty exciting for some of the folks who are working

760
00:41:24.731 --> 00:41:27.310
on video and image processing here.
Um,

761
00:41:27.340 --> 00:41:29.380
we've recently been interested in asking how far can we,

762
00:41:29.410 --> 00:41:33.130
can we take these ideas and apply them to other domains beyond just streaming

763
00:41:33.140 --> 00:41:35.090
telemetry from say devices?

764
00:41:35.210 --> 00:41:38.900
What do we have richer data types like images and videos?

765
00:41:39.500 --> 00:41:43.550
So we have a project it'll appear in VLDB shortly called no scope.

766
00:41:44.330 --> 00:41:47.480
And we essentially said there are a large number of these neural networks are

767
00:41:47.490 --> 00:41:49.040
gonna run to basically feed in,

768
00:41:49.370 --> 00:41:53.030
say webcam feeds like the one I'm depicting here essentially extract.
Um,

769
00:41:53.180 --> 00:41:55.100
you know,
occurrences of objects in the world.

770
00:41:55.101 --> 00:41:58.050
So here we're taking a bus and some people and you know,

771
00:41:58.090 --> 00:41:59.810
if he smoothed the labels well enough,

772
00:41:59.960 --> 00:42:03.920
the sort of state of the art does pretty well.
And actually extracting,
you know,

773
00:42:03.950 --> 00:42:06.890
reasonable features that we could use sent downstream analytics tasks.

774
00:42:06.891 --> 00:42:07.311
So for instance,

775
00:42:07.311 --> 00:42:11.630
we can ask at what times was traffic heaviest of this intersection and Taipei.

776
00:42:12.080 --> 00:42:15.280
Okay.
Now the problem here is that uh,

777
00:42:15.350 --> 00:42:19.460
the state of the art neural networks run about 30 frames per second on $1,000

778
00:42:19.470 --> 00:42:22.790
Gpu.
Okay.
And that's going to be incredibly expensive.

779
00:42:22.791 --> 00:42:26.510
We want to monitor large numbers of,
of data streams specifically,
you know,

780
00:42:26.511 --> 00:42:31.230
tagging images with these deep nets doesn't scale the popular models that would,

781
00:42:31.231 --> 00:42:34.970
that,
that are sort of uh,
available,
are trained on still images.
So,
you know,

782
00:42:34.971 --> 00:42:39.180
work like face image net,
that was fantastic.
And letting people increase the,
the,

783
00:42:39.190 --> 00:42:43.610
the quality of their neural networks is only labeled labeled frames,
right?
It's,

784
00:42:43.670 --> 00:42:47.630
you know,
bunch of individual images.
And so if we want to run this on video,

785
00:42:47.840 --> 00:42:51.710
we essentially treat this video as a sequence of images and run it one,
one,
one,

786
00:42:51.711 --> 00:42:55.160
one,
one,
one image after the other.
And so as I mentioned for Yolo Vt,

787
00:42:55.161 --> 00:42:58.460
which is one of the fastest models on a p 100,
which is an $8,000 Gpu,

788
00:42:58.461 --> 00:43:01.910
it's 80 frames per second.
A titan x Gpu,
which is about $1,200,

789
00:43:01.911 --> 00:43:06.440
maybe a little cheaper.
Now it's 50 frames per second.
So for,
uh,
you know,

790
00:43:06.450 --> 00:43:09.590
a thousand cameras just in Gpu costs alone,
it's about a million dollars.

791
00:43:10.400 --> 00:43:12.350
So we want to know,
you know,
how can we do better here?

792
00:43:12.420 --> 00:43:15.980
It might be cheaper with tps.
Okay.
So we can talk about that,
but,
um,

793
00:43:16.130 --> 00:43:18.320
but it's definitely expensive.
So he said,
how can we do better?

794
00:43:18.440 --> 00:43:21.410
And I'll just leave you with one idea here.
Um,

795
00:43:22.430 --> 00:43:24.110
if we have a query like this,

796
00:43:24.170 --> 00:43:28.100
I want to know when buses passed by an intersection in Taipei using Yolo Vitu.

797
00:43:28.101 --> 00:43:32.090
So run Yolo Vitu over,
you know,
possibly historical data from this,

798
00:43:32.091 --> 00:43:36.110
from this scene.
Alternatively,
live as,
as data feeds arrive.

799
00:43:36.890 --> 00:43:41.210
Well,
Yolo v is amazing cause it can pick out these buses.
The problem is it's,

800
00:43:41.211 --> 00:43:44.720
it's also good at picking out a lot of other stuff.
So,
you know,

801
00:43:44.780 --> 00:43:48.320
Yolo Vitu can detect things like toilets and cats and skis.

802
00:43:48.320 --> 00:43:53.320
These are literal examples from the training data that Yolo is used to identify.

803
00:43:53.720 --> 00:43:56.870
And Moreover,
you know,
even for things like buses here,
there's a bus here,

804
00:43:56.871 --> 00:43:59.870
the bus here,
because we're looking at this particular video feed,

805
00:43:59.930 --> 00:44:03.370
we don't necessarily care about,
uh,

806
00:44:03.440 --> 00:44:07.070
sort of different orientations,
right.
When I want to know is,
again,

807
00:44:07.460 --> 00:44:11.870
I want to know when buses pass by this intersection,
right?
Uh,

808
00:44:11.990 --> 00:44:13.790
in Taipei,
right?
Uh,

809
00:44:13.791 --> 00:44:17.990
so I have a fixed angle and a fixed class and the buses are fairly similar from

810
00:44:17.991 --> 00:44:18.980
this video feed,
right?
I mean,

811
00:44:18.981 --> 00:44:22.130
these are five different buses and they all look relatively relatively similar.

812
00:44:22.160 --> 00:44:26.570
So the idea is that we can essentially exploit this observation by training a

813
00:44:26.571 --> 00:44:31.040
just in time,
sort of specialized neural networks where we,

814
00:44:31.320 --> 00:44:33.710
the big CNN over the,
over the,
uh,

815
00:44:33.750 --> 00:44:36.780
stream of interest in order to generate a large amount of training data.

816
00:44:36.810 --> 00:44:40.500
So we run that in real time and then we train a surrogate model or a specialized

817
00:44:40.501 --> 00:44:42.990
CNN,
uh,
that can run much faster.
Okay.

818
00:44:42.991 --> 00:44:45.420
So if you'll OVT runs on 80 frames per second,

819
00:44:45.510 --> 00:44:49.130
we use a much smaller network that achieves the same accuracy as Yolo.
Uh,

820
00:44:49.140 --> 00:44:51.480
but Ron's at 15,000 frames per second.
Okay.

821
00:44:52.080 --> 00:44:55.590
And the key idea here compared to things like modeled distillation is that we're

822
00:44:55.591 --> 00:44:59.160
throwing away the generality of the parent network.

823
00:44:59.970 --> 00:45:01.180
This network that we learn,

824
00:45:01.200 --> 00:45:06.000
the specialized network will not run accurately on anything but the Webcam that

825
00:45:06.001 --> 00:45:10.410
we've trained it on.
And it also won't run on anything but buses,
right?

826
00:45:10.530 --> 00:45:14.210
We're extending this to multi-class and counting and so on.
But really it's,
it's,

827
00:45:14.211 --> 00:45:17.220
it's,
it's taking this incredibly powerful network in saying,

828
00:45:17.550 --> 00:45:21.420
what if I want to squeeze it down to the minimum network size required for a

829
00:45:21.421 --> 00:45:22.590
particular query.

830
00:45:22.620 --> 00:45:25.470
So it's like lossy compression where we can't tell the difference between the

831
00:45:25.471 --> 00:45:29.860
original and the,
um,
and the,
uh,
the,
the new model.
Um,

832
00:45:30.300 --> 00:45:34.530
so this is pretty cool.
And by cascade in these models.
In the interest of time,

833
00:45:34.531 --> 00:45:37.650
I'll skip over how we handle differences in um,
in time.

834
00:45:37.860 --> 00:45:40.620
Instead of feeding the video directly to Yolo Vitu,

835
00:45:40.740 --> 00:45:44.880
we cascade a thing that tells us if something changed the specialized models

836
00:45:44.910 --> 00:45:49.050
with Yolo v two and we have wood running Yolo Vitu as much as possible.

837
00:45:50.010 --> 00:45:53.850
So when we run this on things like looking for buses and Taipei or cars and

838
00:45:53.851 --> 00:45:57.870
Amsterdam or a cars and Jackson hole bunch of stuff,
you can find the paper.

839
00:45:57.990 --> 00:46:00.990
We find that we can get speedups um,
you know,

840
00:46:00.991 --> 00:46:04.970
depending on the accuracy level that you'd like to maintain compared to Yolo

841
00:46:04.970 --> 00:46:09.780
Vitu,
um,
of up to 10,000 decks.
So if you want a 1% loss and accuracy,

842
00:46:09.870 --> 00:46:13.470
you can often get a 100 x speedup just for free.

843
00:46:13.590 --> 00:46:17.340
I'm by training a specialized model and not calling Yolo v two as much.

844
00:46:17.550 --> 00:46:19.230
And then if you're willing to go really,

845
00:46:19.231 --> 00:46:22.060
really fast or you can't afford to run over,
uh,
uh,

846
00:46:22.080 --> 00:46:25.080
you can't afford to run Yolo over all the study historical data,

847
00:46:25.650 --> 00:46:28.620
you can sacrifice even more.
So sort of our,
our,

848
00:46:28.680 --> 00:46:31.530
our thought here is that instead of just obsessing over,
you know,

849
00:46:31.531 --> 00:46:34.650
99.5% accuracy and going a little bit lower,

850
00:46:34.800 --> 00:46:38.310
we can train a much specialized model for a given task at hand and actually

851
00:46:38.311 --> 00:46:41.820
achieve dramatic speedups that actually make these type of large scale video

852
00:46:41.821 --> 00:46:46.210
analytics actually feasible at scale.
So,
uh,

853
00:46:46.320 --> 00:46:50.040
those were three examples of how we can use techniques from systems design of

854
00:46:50.041 --> 00:46:54.530
sort of efficient operators,
uh,
for online training and inference.
Um,

855
00:46:54.840 --> 00:46:56.790
I'm running out of time,
so I'll just leave you with,

856
00:46:56.791 --> 00:47:01.290
with a couple a things to think about.
Um,
macro basis as I said is,
is,

857
00:47:01.291 --> 00:47:03.360
is sort of a new open source engine,

858
00:47:03.361 --> 00:47:07.230
lots of projects going on inside of here around making it easier to make use of

859
00:47:07.231 --> 00:47:11.280
and derive value from online streaming data sources.

860
00:47:12.210 --> 00:47:14.850
And it's been really exciting to see what the open source community has done

861
00:47:14.851 --> 00:47:17.520
with this.
And I'd love to talk with you all about possible,
uh,

862
00:47:17.590 --> 00:47:20.010
use cases and extensions for this.
Uh,
and we,

863
00:47:20.011 --> 00:47:24.400
we spent a lot of time talking to developers and engineers actually valid and

864
00:47:24.430 --> 00:47:28.770
stuff in the field.
So that actually really drives the whole of dawn.
Uh,

865
00:47:28.771 --> 00:47:32.630
our sort of strategy is,
is really informed by current use cases,
uh,

866
00:47:32.710 --> 00:47:36.370
both on campus and off campus and sort of high volume,

867
00:47:36.530 --> 00:47:39.820
a high value,
um,
machine learning pipelines.
Uh,

868
00:47:39.821 --> 00:47:43.220
there's a ton of work on macro ways you can find on my website.
Uh,

869
00:47:43.221 --> 00:47:47.920
I think two of these are now in,
in Vldb if you're interested.
Um,
and the,
the,

870
00:47:47.921 --> 00:47:51.490
the message that I come to you with in conclusion is very optimistic that we

871
00:47:51.491 --> 00:47:53.170
have a lot more data than we know what to do with.

872
00:47:53.290 --> 00:47:56.800
We can do a lot of cool stuff with systems in order to make this data useful.

873
00:47:57.010 --> 00:47:57.843
Thanks for your time.

874
00:48:04.810 --> 00:48:09.330
<v 2>So for speeding up Yolo that work,
um,
did you care what kind of error,</v>

875
00:48:09.331 --> 00:48:13.630
if you're willing to be faster and sacrifice performance a little bit.
Did you,

876
00:48:14.290 --> 00:48:18.310
can you elaborate on,
do you get more recall or the precision?

877
00:48:18.311 --> 00:48:19.330
What type of error?

878
00:48:19.420 --> 00:48:22.600
<v 0>It's a great,
yeah.
So good question.
So,
um,</v>

879
00:48:23.260 --> 00:48:26.960
two things about our current experiments.
So right now we set a,
you can we up?

880
00:48:27.010 --> 00:48:27.843
So okay.

881
00:48:28.540 --> 00:48:32.860
We basically let use or specify a false positive false negative rate.

882
00:48:33.040 --> 00:48:37.060
And currently I think we're doing 50,
50 split on both.
So I think this is like,

883
00:48:37.080 --> 00:48:41.920
you know,
I say 1% would be 50% or 0.5% false positive 0.5% false negative.

884
00:48:42.190 --> 00:48:44.350
Um,
but uh,

885
00:48:44.710 --> 00:48:49.180
we like the code that we have basically in the optis optimizer we have lets you

886
00:48:49.181 --> 00:48:51.250
basically tune those parameters.
So certainly if I,

887
00:48:51.400 --> 00:48:54.150
if I have like no buses in the scene and I allow 100%,
you know,

888
00:48:54.310 --> 00:48:57.160
false negative rate,
then I'm fine.
But,
but it's kind of configurable.

889
00:48:57.370 --> 00:48:59.860
So I think with what the results I presented were from 50,
50.

890
00:49:00.350 --> 00:49:03.250
One thing also point out compared to your a Yolo and we're working on this right

891
00:49:03.251 --> 00:49:06.010
now is we're doing binary classification bus,
no bus.

892
00:49:06.070 --> 00:49:08.560
We're not doing bounding boxes.
So,
um,

893
00:49:08.800 --> 00:49:11.560
one of the summer interns of summers,

894
00:49:11.590 --> 00:49:13.870
it has extended no scope to do the bounding boxes,

895
00:49:13.871 --> 00:49:15.700
but you don't get the same speed up.
You still get up,

896
00:49:16.510 --> 00:49:17.440
I don't want to quote numbers,

897
00:49:17.441 --> 00:49:19.780
but it's still going to be more than like a two x speed up for this.
Yeah.

898
00:49:20.640 --> 00:49:24.960
<v 2>Okay.
Does that mean,
so if you really,
if we want total recall,</v>

899
00:49:25.240 --> 00:49:29.280
are you still gonna going to get that kind of a good speed up?

900
00:49:29.860 --> 00:49:33.370
<v 0>Yeah.
If you want total recall,
um,
you won't,</v>

901
00:49:34.300 --> 00:49:37.300
you'll definitely get some speed up.
Well,
okay.
It depends.
Right?

902
00:49:37.360 --> 00:49:40.330
So there's some cases like some of these sort of filtering steps.

903
00:49:40.331 --> 00:49:42.930
You can totally do like a one thing I didn't talk about what we do.

904
00:49:42.970 --> 00:49:46.540
We train a temporal,
you know,
different detectives says,
did this frame change?

905
00:49:46.541 --> 00:49:49.630
So what this looks like is basically I did my frame compared to the last thing I

906
00:49:49.631 --> 00:49:52.990
ran Yolo on and I basically,
uh,
tray,
you know,

907
00:49:52.991 --> 00:49:55.870
run an LR over the differences and logistic regression over the disturbances

908
00:49:56.020 --> 00:49:57.670
that says this thing.
Yeah.
Something likely changed.

909
00:49:57.910 --> 00:50:00.880
So if I'm running at 30 frames per second,
you know,

910
00:50:01.150 --> 00:50:05.890
it's highly likely that the label change rate is much lower than the frame

911
00:50:06.070 --> 00:50:09.010
change.
Right.
So,
you know,
that's a way to get,
um,
you know,

912
00:50:09.610 --> 00:50:13.690
10 plus x speedup basically for free for the specialization task.

913
00:50:14.110 --> 00:50:18.370
I believe it is possible to actually get,
um,
substantial speedups.

914
00:50:18.520 --> 00:50:22.000
I don't want to say,
you know,
three orders of magnitude,
but,
um,
you know,

915
00:50:22.001 --> 00:50:26.000
we've been,
we've been benchmarking things like,
um,
if I just train,
um,

916
00:50:26.050 --> 00:50:30.680
resonates on say cat versus dog versus cat versus insect and various forms of

917
00:50:30.681 --> 00:50:35.210
sort of
restricted input classes and restricted output classes,

918
00:50:35.300 --> 00:50:38.300
you can still get a five to 10 x performance increase right?

919
00:50:39.170 --> 00:50:42.320
Now note that this is not going to be a recall on the entire set of the,

920
00:50:42.410 --> 00:50:46.250
you know,
uh,
images but basically specializing forest fixed out of classes.

921
00:50:46.251 --> 00:50:48.690
Our preliminary results suggest you can get,
you know,

922
00:50:48.740 --> 00:50:51.770
much more than you can get from doing like training with quantitation and

923
00:50:51.771 --> 00:50:56.480
training of pruning.
Right?
So,
so,
and this is just because essentially w w well,

924
00:50:56.540 --> 00:50:58.250
yeah,
we can take it offline,
but,

925
00:50:58.340 --> 00:51:02.140
but I think the cool idea here is that if you restrict the,
you can even like to,

926
00:51:02.150 --> 00:51:04.370
you know,
the,
the manifold of things that you're looking for,

927
00:51:04.490 --> 00:51:07.510
you can train a much simpler classifier to distinguish between these,
um,

928
00:51:07.550 --> 00:51:11.450
if you restrict the,
the test data.
Yeah.
Thanks.
Yeah.

