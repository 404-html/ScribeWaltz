WEBVTT

1
00:00:09.950 --> 00:00:13.240
Hello.
My name is John [inaudible].
I work at verily,

2
00:00:13.241 --> 00:00:15.050
which is Google's life sciences company.

3
00:00:15.700 --> 00:00:17.690
I also lead a group called the singularity network,

4
00:00:17.691 --> 00:00:19.160
which is an internal organization,

5
00:00:19.550 --> 00:00:24.200
a competitive more than 3000 Googlers focused on topics about the future of

6
00:00:24.201 --> 00:00:27.140
artificial intelligence for which we are here today.

7
00:00:28.340 --> 00:00:31.190
And it's my pleasure to be here today with Dr Max Tegmark.

8
00:00:32.190 --> 00:00:33.350
As a brief introduction,

9
00:00:33.380 --> 00:00:37.730
Max Tegmark is a renowned scientific communicator and cosmologists and he's

10
00:00:37.731 --> 00:00:41.120
accepted donations from Elon Musk to investigate the existential risk,

11
00:00:41.270 --> 00:00:44.570
but Moondance artificial intelligence because research interests include

12
00:00:44.571 --> 00:00:45.404
consciousness,

13
00:00:45.410 --> 00:00:50.410
the multiverse advanced risk from Ai and formulating an ultimate ensemble theory

14
00:00:50.511 --> 00:00:52.200
of everything.
Uh,

15
00:00:52.280 --> 00:00:57.280
Max was elected fellow of the American Physics Society in 2012 one scientist ms

16
00:00:57.400 --> 00:01:01.850
science magazines breakthrough of the year in 2003 and has written over 200

17
00:01:01.851 --> 00:01:02.720
publications,

18
00:01:02.721 --> 00:01:07.250
nine of which have incited more than 500 times a Max Tegmark everyone

19
00:01:08.080 --> 00:01:13.050
<v 3>[inaudible]</v>

20
00:01:13.120 --> 00:01:17.330
<v 1>he said it's a really great honor to be back here at Google and they get to talk</v>

21
00:01:17.331 --> 00:01:21.830
in front though so many old friends and it's so much human level intelligence

22
00:01:21.831 --> 00:01:22.850
and idealism.

23
00:01:27.500 --> 00:01:28.580
Does anyone recognize

24
00:01:30.630 --> 00:01:33.950
<v 0>this?
Was,
this was of course,
the Apollo 11 mission,</v>

25
00:01:35.150 --> 00:01:36.910
the put Neil Armstrong,
Buzz Aldrin,

26
00:01:37.480 --> 00:01:42.480
and Michael Collins on the moon was not only successful,

27
00:01:45.041 --> 00:01:50.041
but it was very inspiring because it showed that when we do is manage the

28
00:01:50.471 --> 00:01:51.610
technology wisely,

29
00:01:53.710 --> 00:01:57.870
our ancestors drama,
right?
Well,

30
00:02:00.270 --> 00:02:03.690
<v 1>the rest of the important lessons,
I think we can learn from this as well.</v>

31
00:02:03.691 --> 00:02:08.691
So I want to devote the rest of this talk to and other journey powered by

32
00:02:10.051 --> 00:02:14.340
something much more powerful than rocket engines where the passengers are not

33
00:02:14.341 --> 00:02:16.620
just three astronauts,
but all of humanity.

34
00:02:16.621 --> 00:02:21.030
So let's talk about our collective journey into the future with Ai.

35
00:02:22.110 --> 00:02:27.110
My friend Jaan Tallinn likes to emphasize that just as with the rocketry,

36
00:02:30.090 --> 00:02:33.000
it's not enough to just make our technology powerful.

37
00:02:33.270 --> 00:02:38.270
We also have to focus on figuring out how to control it and on figuring out

38
00:02:39.240 --> 00:02:42.760
where we want to go with it.
And that's where we can talk about,

39
00:02:43.020 --> 00:02:47.160
I think the opportunities are just so awesome if we get this right.

40
00:02:47.540 --> 00:02:51.240
During the past 13.8 billion years,

41
00:02:51.420 --> 00:02:55.740
our universe has transformed from dead and boring,

42
00:02:56.490 --> 00:03:01.490
too complex and interesting and it has the opportunity to get dramatically more

43
00:03:01.930 --> 00:03:04.660
interesting in the future if we don't screw up.

44
00:03:05.330 --> 00:03:09.790
Oh for about 4 billion years ago,
life first appeared here on earth,

45
00:03:09.791 --> 00:03:11.950
but it was pretty dumb stuff like bacteria.

46
00:03:12.160 --> 00:03:14.410
They couldn't really learn anything in their lifetime.

47
00:03:14.500 --> 00:03:19.500
I call that life 1.0 we are what I call life 2.0 because we can learn things,

48
00:03:22.030 --> 00:03:26.740
which of course in Geek speak means we can upload new software modules.

49
00:03:27.460 --> 00:03:29.260
If I want to learn Spanish,

50
00:03:29.560 --> 00:03:33.910
I can study Spanish and now I have all these new skills uploaded than I mine and

51
00:03:34.000 --> 00:03:39.000
it's precisely this ability of as humans to design their own software rather

52
00:03:39.041 --> 00:03:41.770
than be stuck with whatever the software evolution gave us,

53
00:03:42.130 --> 00:03:46.990
which has enabled us to dominate this earth and give us what we call cultural

54
00:03:47.170 --> 00:03:52.170
evolution were seemed to be gradually heading towards life 3.0 where we can,

55
00:03:53.630 --> 00:03:54.310
where,

56
00:03:54.310 --> 00:03:58.330
which is life that can design not just its software but also it's hardware.

57
00:03:59.050 --> 00:04:03.700
Maybe we're at 2.1 right now cause we can get cochlear implants and artificial

58
00:04:03.701 --> 00:04:05.380
knees and a few minor things like this.

59
00:04:05.380 --> 00:04:10.210
But if you were a robots that are able to think is cleverly is right now of

60
00:04:10.211 --> 00:04:13.300
course there will be no limits whatsoever to how you could upgrade yourselves.

61
00:04:14.050 --> 00:04:17.410
So let's first talk about the power of the technology.

62
00:04:17.890 --> 00:04:19.780
Obviously the power of AI has

63
00:04:21.550 --> 00:04:23.050
improved dramatically recently.

64
00:04:23.230 --> 00:04:27.160
I'm going to define intelligence itself just very broadly as the ability to

65
00:04:27.161 --> 00:04:29.290
accomplish complex goals.

66
00:04:29.680 --> 00:04:32.710
I'm giving such a broad definition because I want to be really inclusive and

67
00:04:32.711 --> 00:04:37.711
include both all forms of biological intelligence and all forms of of artificial

68
00:04:37.811 --> 00:04:40.990
intelligence.
And as you guys here at Google all know,
obviously

69
00:04:43.210 --> 00:04:48.210
a subset of artificial intelligence is machine learning where systems can get,

70
00:04:49.750 --> 00:04:53.050
can improve themselves by using data from their environment,

71
00:04:53.051 --> 00:04:54.880
much like biological organisms can.

72
00:04:55.090 --> 00:04:57.310
And another subset of that is of course deep learning,

73
00:04:57.630 --> 00:05:00.280
which is where you use neural net architectures.

74
00:05:00.580 --> 00:05:04.940
And if you look at older breakthroughs in AI,
like when Garry Kasparov,

75
00:05:04.950 --> 00:05:08.080
I've got his posterior kicked by IBM's deep blue,

76
00:05:09.040 --> 00:05:14.040
the intelligence here was of course mainly just put in by human programmers and

77
00:05:15.040 --> 00:05:18.400
deep blue beat Kasparov just because it could think faster and remember better.

78
00:05:18.700 --> 00:05:21.460
Whereas in contrast,
the recent stuff that you've done here at Google,

79
00:05:21.670 --> 00:05:23.800
like this work by ets suits givers group,

80
00:05:25.810 --> 00:05:28.930
there's almost no intelligence at all put in by the humans.

81
00:05:30.170 --> 00:05:34.700
They just trained the simple neuron feeds with,
with uh,

82
00:05:35.200 --> 00:05:39.880
a bunch of data and you put in the numbers that represent the pixel colors and

83
00:05:39.881 --> 00:05:43.180
it puts out this caption,
a group of young people playing game of Frisbee.

84
00:05:43.420 --> 00:05:46.630
Even though the software was never told any total taught anything about what a

85
00:05:46.631 --> 00:05:48.820
Frisbee is or what a human is or what the picture is.

86
00:05:49.270 --> 00:05:52.150
And the same stuff you put in other images gives other captions,

87
00:05:52.151 --> 00:05:56.500
which are often quite impressive.
I find it even more striking

88
00:05:58.010 --> 00:06:02.660
how cool things can be done with video.
So this is Google deep mind

89
00:06:04.310 --> 00:06:08.150
of course,
learning to play an Atari Games.
And,
um,

90
00:06:10.130 --> 00:06:13.220
for those of you,
those few of you who haven't seen this before,

91
00:06:13.460 --> 00:06:18.460
you need to remember that this neural network here with,

92
00:06:18.570 --> 00:06:20.360
with simple reinforcement learning built in,

93
00:06:20.600 --> 00:06:24.380
had no idea what a game was with a paddle was when a ball was or anything like

94
00:06:24.381 --> 00:06:25.214
that.

95
00:06:25.870 --> 00:06:30.870
And just by practicing gradually starts to miss the ball as often and got the

96
00:06:31.881 --> 00:06:35.890
point where it hardly missed it all and play as much better than I,

97
00:06:35.950 --> 00:06:37.920
I could play this.
And,
and the,

98
00:06:38.150 --> 00:06:41.480
the real kicker is it of course the people at deep mind,

99
00:06:43.430 --> 00:06:46.130
they actually didn't know that,
uh,

100
00:06:46.131 --> 00:06:49.160
there was this clever trick you can do when you play breakout,

101
00:06:49.161 --> 00:06:52.640
which is always aim for the corners and try to build a little,

102
00:06:52.670 --> 00:06:57.140
do a little tunnel there.
So,
so once this a little deep learning software,

103
00:06:57.141 --> 00:06:57.974
figure that out.

104
00:06:58.220 --> 00:07:02.180
It's just every single time the ball comes back,

105
00:07:02.210 --> 00:07:07.210
look how just uncannily precise it is just putting it right back there and the

106
00:07:08.241 --> 00:07:09.140
corner and playing.

107
00:07:13.430 --> 00:07:18.320
I can only dream to play this.
Well now this is of course,

108
00:07:18.321 --> 00:07:21.860
it's very,
very simple environment,
that little two dimensional game world.

109
00:07:22.220 --> 00:07:26.150
But if you're a robot lie,
you can think of life as a game,

110
00:07:27.500 --> 00:07:30.560
just a more complex one.
And you could ask,

111
00:07:30.900 --> 00:07:35.090
you should ask yourself to what extent these sorts of techniques might enable

112
00:07:35.091 --> 00:07:40.091
you to learn more interesting things and deep mind more recently had three

113
00:07:40.251 --> 00:07:43.640
dimensional robots in a simulated world and that just asked to see if they could

114
00:07:43.880 --> 00:07:47.360
learn to do things like walk.
And this is what happened.

115
00:07:53.830 --> 00:07:56.390
This software had never ever seen any videos of walking.

116
00:07:56.391 --> 00:07:59.320
You knew nothing about the context,
the concept of walking,
right?

117
00:07:59.410 --> 00:08:03.850
All that the software was doing was sending random commands,

118
00:08:03.851 --> 00:08:06.730
the how to bend the different joints and it got rewarded every time.

119
00:08:06.731 --> 00:08:09.460
This treats them manage the move a little bit forward and

120
00:08:10.960 --> 00:08:15.850
it looks a bit funky,
maybe a little bit awkward,
but I actually learns,
learns,

121
00:08:15.851 --> 00:08:19.770
interesting stuff.
So this raises this very interesting question.
Okay,

122
00:08:19.780 --> 00:08:21.130
how far can AI go?

123
00:08:23.560 --> 00:08:25.540
How much of what we humans can do,

124
00:08:26.140 --> 00:08:29.380
will machines ultimately be able to do if we use,

125
00:08:29.381 --> 00:08:34.150
not just the techniques that we know of so far but factory and all sorts of

126
00:08:34.240 --> 00:08:38.500
additional progress that you,
people in the room and elsewhere are going to do.

127
00:08:38.800 --> 00:08:43.180
I like to think about this in terms of this landscape.
I drew this,

128
00:08:43.360 --> 00:08:46.370
you've made this picture inspired by uh,
a paragraph and the,

129
00:08:46.900 --> 00:08:51.280
one of my favorite books by Hans more Avec for many years ago where the height

130
00:08:51.550 --> 00:08:56.550
here represents how difficult is for a computer to do a certain task and the sea

131
00:08:58.441 --> 00:09:01.320
level represents how good computers are doing them right now.

132
00:09:01.920 --> 00:09:05.490
So what we see here is that certain tasks like the chess playing and arithmetics

133
00:09:05.491 --> 00:09:08.830
have of course long been submerged by the slowly rising tide of,

134
00:09:09.330 --> 00:09:11.980
of machine intelligence.
And there,

135
00:09:11.981 --> 00:09:16.981
there are some people who think that there are certain tasks like art and book

136
00:09:17.101 --> 00:09:19.440
writing or whatever,
that machines will never be able to do.

137
00:09:19.650 --> 00:09:21.560
And then there are others who think that,
uh,

138
00:09:21.620 --> 00:09:26.100
the old goal of AI to really solve intelligence and do everything that we do

139
00:09:26.400 --> 00:09:30.030
will mean that sea levels will eventually submerge everything.
So,

140
00:09:30.031 --> 00:09:31.090
so what's going to happen,

141
00:09:31.380 --> 00:09:34.500
there've been a lot of interesting polls of site of Ai researchers and the

142
00:09:34.501 --> 00:09:39.330
conclusion is very clear.
We don't know
the more specifically though,

143
00:09:39.331 --> 00:09:42.240
what you find is there are some people in the techno skeptic camps who think

144
00:09:42.241 --> 00:09:43.890
that AI researchers ultimately doomed.

145
00:09:44.430 --> 00:09:49.380
We're never going to get there or maybe we're only going to get there hundreds

146
00:09:49.380 --> 00:09:50.350
of years from now.
Um,

147
00:09:50.790 --> 00:09:54.630
but actually most AI researchers think it's going to happen and more a matter of

148
00:09:54.631 --> 00:09:56.320
decades.
And,
uh,

149
00:09:58.980 --> 00:10:02.370
some people think that we don't have to worry so much about steering this

150
00:10:03.780 --> 00:10:07.860
rocket's metaphorically speaking because it's not going to happen.

151
00:10:08.280 --> 00:10:11.300
There will ever get powerful enough that we have to worry about this.
But,

152
00:10:11.310 --> 00:10:13.430
but that's a minority.
And,

153
00:10:13.431 --> 00:10:15.780
and then there are people who think we don't have to worry about steering

154
00:10:15.781 --> 00:10:18.480
because we're guaranteed that their outcome is going to be awesome.

155
00:10:18.870 --> 00:10:23.310
I call it such people digital utopians.
And I respect that point of view.

156
00:10:23.520 --> 00:10:26.190
And there are also people who think it's guaranteed the things are going to
suck.

157
00:10:26.520 --> 00:10:29.280
So there's no point in worrying about steering cause we're screwed anyway.

158
00:10:29.700 --> 00:10:34.700
But most of the people in surveys tend to land or here in the middle and what

159
00:10:34.771 --> 00:10:38.370
I've called the beneficial AI movement where you're really motivated actually to

160
00:10:38.371 --> 00:10:41.670
ask what can we do right now to steer things in a good direction because it

161
00:10:41.671 --> 00:10:45.540
could be awesome or it could be not so great.
And it depends on what we do.
Now.

162
00:10:45.980 --> 00:10:48.510
Uh,
I put this webpage up,

163
00:10:48.511 --> 00:10:52.860
age of AI to work and we did a survey there for people from the general public

164
00:10:52.861 --> 00:10:55.320
can answer these same questions and you can go there and do it too.

165
00:10:55.620 --> 00:10:59.430
And I was actually very interested that the general public responds almost

166
00:10:59.431 --> 00:11:02.640
exactly the same as AI researchers have done in recent polls.

167
00:11:03.820 --> 00:11:08.820
This is from something I analyze this weekend with 14,866 respondents.

168
00:11:08.851 --> 00:11:12.360
And you see,
most people think maybe we're decades away from human level Ai.

169
00:11:14.280 --> 00:11:16.440
Maybe it will be good and maybe there'll be problems.

170
00:11:17.100 --> 00:11:21.750
So this is maximumly motivating to think about how we can steer this technology

171
00:11:21.751 --> 00:11:24.990
in a good direction.
So let's talk about steering.
How can we control,

172
00:11:25.950 --> 00:11:28.650
how can we learn to control AI to do what we wanted to do

173
00:11:30.720 --> 00:11:32.060
to help with this?
Um,

174
00:11:32.610 --> 00:11:37.380
my wife may out who's sitting there and I and some other folks that founded the

175
00:11:37.381 --> 00:11:38.550
future of life institute.

176
00:11:38.730 --> 00:11:42.060
And you can see we actually have the word steer up here in a mission statement.

177
00:11:42.300 --> 00:11:45.300
Our goal is simply to do what we can to help make sure that technology

178
00:11:48.930 --> 00:11:50.370
is beneficial for humanity.

179
00:11:50.730 --> 00:11:54.790
And I'm quite optimistic that we can create an really inspiring future with

180
00:11:54.791 --> 00:11:58.660
technology as long as we win this race between the growing power of the

181
00:11:58.661 --> 00:12:03.570
technology and the growing wisdom with which we manage it.
But,
um,

182
00:12:04.780 --> 00:12:06.340
I think if we're going to win this race,

183
00:12:06.400 --> 00:12:10.390
we actually have to shift strategies because technology is gradually getting

184
00:12:10.391 --> 00:12:14.590
more powerful.
And when we invented less powerful tech like fire,

185
00:12:15.010 --> 00:12:19.990
we very successful use the strategy of learning from mistakes and entered fire

186
00:12:20.020 --> 00:12:22.540
oopsy and then went the fire extinguisher.

187
00:12:23.020 --> 00:12:27.250
We invented the car oopsy and then we invented the seatbelt.
The airbag,

188
00:12:27.251 --> 00:12:30.160
the traffic light and things were more or less fine,

189
00:12:30.520 --> 00:12:34.480
but when you get beyond the certain point that the power of the technology,

190
00:12:34.481 --> 00:12:38.140
this idea of learning from mistakes,
it's just really,
really lousy.
Right?

191
00:12:38.260 --> 00:12:43.260
You don't want to make mistakes if one mistake is unacceptably many and when we

192
00:12:43.461 --> 00:12:44.830
get,
when talking about nuclear weapons,

193
00:12:44.831 --> 00:12:47.290
synthetic biology and certainly super human AI,

194
00:12:47.500 --> 00:12:51.070
I feel we're at that point where we really don't want to.
Mistakes make mistakes.

195
00:12:51.100 --> 00:12:56.080
We're going to shift strategy from being reactive to being proactive,

196
00:12:56.380 --> 00:12:57.850
which is exactly the slogan you said.

197
00:12:57.851 --> 00:13:01.780
You're also using it for your work here at Google earlier and the,

198
00:13:04.150 --> 00:13:06.970
I'm optimistic that we can do this if you really focused on it and I worked for

199
00:13:06.971 --> 00:13:08.710
it.
Some people say,
Nah,
don't,

200
00:13:09.220 --> 00:13:12.880
don't talk about this because it's just luddite.

201
00:13:12.881 --> 00:13:15.340
Scaremongering when you're talking about things that could go wrong,

202
00:13:17.050 --> 00:13:20.140
I don't think it's luddite scaremongering.
I think it's safety engineering.

203
00:13:21.430 --> 00:13:24.970
We started by talking about the Apollo Moon mission.

204
00:13:25.240 --> 00:13:29.440
You know when NASA thought through very carefully everything that could possibly

205
00:13:29.441 --> 00:13:30.041
go wrong.

206
00:13:30.041 --> 00:13:35.041
When you put three astronauts on top of this 100 meter tall rocket full of,

207
00:13:35.360 --> 00:13:39.010
of highly explosive fuel that wasn't luddite scaremongering.

208
00:13:39.190 --> 00:13:42.970
What they were doing was precisely what ultimately led to the success of the

209
00:13:42.971 --> 00:13:47.530
mission.
And this is where I think we want to be doing where the eye as well.

210
00:13:47.650 --> 00:13:51.550
I think so far what we've learned from other technologies here is that we need

211
00:13:51.551 --> 00:13:55.120
to up our game a little bit because we haven't really absorbed this idea that we

212
00:13:55.121 --> 00:13:58.270
have to switch to being uh,
being,
being proactive.

213
00:13:58.390 --> 00:14:03.390
Today is a very special day in terms of nuclear weapons because we came pretty

214
00:14:03.881 --> 00:14:08.881
close to September 26 being the 34th anniversary of world war three.

215
00:14:10.930 --> 00:14:11.870
In fact,
um,

216
00:14:12.730 --> 00:14:16.660
it might have ended up this way if this guy's Stanislav Petrov hadn't just on

217
00:14:16.661 --> 00:14:17.380
gut instinct,

218
00:14:17.380 --> 00:14:21.580
there's ignore the fact that his early warning system said that there were five

219
00:14:21.581 --> 00:14:25.390
incoming minuteman us missiles there.
It should be retaliated against.

220
00:14:27.010 --> 00:14:31.770
So how can we do better?
How can we win this wisdom race?
I'm very,

221
00:14:31.771 --> 00:14:36.771
very happy that the AI community has really started to engage with these issues

222
00:14:36.971 --> 00:14:39.290
a lot in recent years.
And um,

223
00:14:40.930 --> 00:14:45.100
thanks to a lot of people who are in this room here included Peter Norvig and

224
00:14:45.101 --> 00:14:48.310
where the future life is due to organized a couple of conferences and Puerto

225
00:14:48.310 --> 00:14:52.480
Rico and then in this year,
earlier this year in the Sylmar California where the,

226
00:14:52.510 --> 00:14:56.990
it's really quite remarkable consensus around a number of very constructive

227
00:14:56.991 --> 00:15:01.640
things that we can do in terms of try to develop this wisdom and steer things in

228
00:15:01.641 --> 00:15:02.474
the right direction.

229
00:15:02.570 --> 00:15:06.260
And I want to spend just a little bit of tough time hitting some highlights of

230
00:15:06.261 --> 00:15:10.430
things here from this list of 23 Sylmar principles,

231
00:15:10.520 --> 00:15:14.420
which has now been signed by over a thousand AI researchers around the world.

232
00:15:15.050 --> 00:15:17.540
First of all,
people,
first of all there was,

233
00:15:17.780 --> 00:15:21.410
it says here on item one that we should define the goal of it,
of Ai Research.

234
00:15:21.890 --> 00:15:26.840
Not to be just making undirected intelligence but to make beneficial

235
00:15:26.841 --> 00:15:28.220
intelligence.
So in other words,

236
00:15:28.221 --> 00:15:32.430
the steering of the rocket is part of the design specs,
but and,

237
00:15:32.640 --> 00:15:36.900
and then there was also a very strong consensus that hey,
we,

238
00:15:36.940 --> 00:15:39.920
if we have a bunch of unanswered questions that we need to answer,

239
00:15:40.550 --> 00:15:43.940
we shouldn't just say,
Oh yeah,
we should answer them well.

240
00:15:44.030 --> 00:15:48.020
We should answer them the way we scientifically know is the best way to answer

241
00:15:48.021 --> 00:15:51.280
hard questions.
Then we to research them to work on them.
Right.

242
00:15:51.440 --> 00:15:56.030
And we should fund this kind of research is just an integral part of computer

243
00:15:56.031 --> 00:15:59.930
science funding both in companies and an industry.
I'm actually very,

244
00:15:59.931 --> 00:16:04.931
very proud of Google for being one of the founding members of the partnership on

245
00:16:05.241 --> 00:16:09.530
Ai,
which aims very much to support this kind of AI research,

246
00:16:09.980 --> 00:16:14.980
AI safety research and other and other principle here that was very broad

247
00:16:16.221 --> 00:16:20.660
agreement was the shared prosperity principle that the economic prosperity

248
00:16:20.661 --> 00:16:25.580
created by AI should be shared broadly to benefit all of humanity.

249
00:16:25.580 --> 00:16:30.350
What do I mean by that?
Obviously technology has kept growing the economic pie.

250
00:16:30.500 --> 00:16:34.940
It's been growing our GDP.
I'll opt in recent decades as you can see.

251
00:16:34.941 --> 00:16:37.640
If you look at the top line and this,
this plot here,

252
00:16:38.090 --> 00:16:40.820
but as you're also generally aware of,

253
00:16:41.450 --> 00:16:44.600
this pie hasn't been divvied up quite equally.
And in fact,

254
00:16:44.601 --> 00:16:47.030
if you look at the bottom 90% of income earners,

255
00:16:47.540 --> 00:16:51.050
their income has stayed flat almost since I was born.
Actually.

256
00:16:51.051 --> 00:16:52.850
Maybe it's my fault.
Um,

257
00:16:53.750 --> 00:16:58.750
and the 30% poorest in the US have actually gotten significantly poorer in real

258
00:16:59.481 --> 00:17:03.440
terms in recent decades.
There's just created a great deal of anger,

259
00:17:04.100 --> 00:17:08.090
which has given us the election of Donald Trump.

260
00:17:08.091 --> 00:17:11.540
It's given us Brexit then has given us a more polarized society in general.

261
00:17:11.541 --> 00:17:14.900
And there's so,
there was a very strong consensus amount among AI researchers,

262
00:17:15.770 --> 00:17:15.981
you know,

263
00:17:15.981 --> 00:17:20.480
if we can create so much more wealth and prosperity and have machines help

264
00:17:20.481 --> 00:17:22.840
produce all these wonderful goods and services,

265
00:17:22.850 --> 00:17:26.060
then if we can't make sure everybody gets better off from this,
you know,

266
00:17:26.061 --> 00:17:29.390
shame on us.
Some people say,
well,

267
00:17:29.750 --> 00:17:33.620
this is just nonsense because something magical is going to change in these

268
00:17:33.680 --> 00:17:34.730
statistics soon.

269
00:17:34.731 --> 00:17:39.050
And the jobs that get automated away are going to be replaced by much better new

270
00:17:39.051 --> 00:17:40.790
jobs that don't exist yet.

271
00:17:43.640 --> 00:17:46.700
But actually if you look at this data,
it doesn't support that.

272
00:17:47.060 --> 00:17:50.070
We could have made that same argument to a hundred years ago when much more

273
00:17:50.071 --> 00:17:53.400
people worked in farming that all those jobs,
they weren't in loss.

274
00:17:53.430 --> 00:17:56.160
We're going to replay,
be replaced by new jobs that didn't exist yet.

275
00:17:57.540 --> 00:18:00.180
And this is what actually happened.
This is,

276
00:18:00.210 --> 00:18:05.210
I made this little pie chart here of all the jobs in the u s by size and you can

277
00:18:05.221 --> 00:18:09.660
start going down list managers,
drivers,
retail,
salespersons,
cashiers,
et cetera.

278
00:18:09.840 --> 00:18:11.820
Only when you get down to 21st place,

279
00:18:11.850 --> 00:18:15.630
you get to a job category that didn't exist a hundred years ago,

280
00:18:15.631 --> 00:18:18.620
namely software developers.
Hi Guys.
Um,

281
00:18:19.470 --> 00:18:23.040
so clearly what happened is not that most farmers became software developers.

282
00:18:23.041 --> 00:18:26.610
What instead happened with was people who lost generally from the industrial

283
00:18:26.611 --> 00:18:30.570
revolution and onward jobs where they were using their muscles to do work,

284
00:18:31.590 --> 00:18:35.310
went into other jobs where they could use their brains to the work and these

285
00:18:35.311 --> 00:18:38.760
jobs kind of to be better paid.
So this was a net win,

286
00:18:39.000 --> 00:18:43.020
but they were jobs that already existed before.
Now

287
00:18:44.790 --> 00:18:46.350
what's happening today,

288
00:18:46.410 --> 00:18:50.460
which is driving the growth and income inequality is similarly that people are

289
00:18:50.461 --> 00:18:53.370
getting switched into other jobs that had existed before.

290
00:18:54.570 --> 00:18:57.570
It's just that this time since the jobs that are being automated away are

291
00:18:57.571 --> 00:18:59.220
largely jobs where they use their brains,

292
00:18:59.430 --> 00:19:04.430
they often switch to new jobs that existed before that pay less around and pay

293
00:19:05.101 --> 00:19:05.880
more.

294
00:19:05.880 --> 00:19:09.450
And I think it's really interesting challenge for all of us to think about how

295
00:19:09.451 --> 00:19:13.830
we can best make sure that this growing pie just makes everybody better off.

296
00:19:14.420 --> 00:19:16.920
Uh,
and other item here on,

297
00:19:17.250 --> 00:19:22.250
on this list is principle number 18 to AI AI arms race.

298
00:19:23.670 --> 00:19:27.240
This was the one that had the highest agreement of all among the Sylmar

299
00:19:27.241 --> 00:19:28.074
participants.

300
00:19:30.060 --> 00:19:34.380
An arms race in lethal autonomous weapons should be avoided.
Why is that?
Well,

301
00:19:34.381 --> 00:19:38.010
first of all,
we're not talking about drones where,

302
00:19:38.040 --> 00:19:41.520
which are remote controlled vehicles where human is still deciding who to kill.

303
00:19:41.521 --> 00:19:46.521
We're talking here about about systems where the machine itself using machine

304
00:19:46.981 --> 00:19:50.730
learning or whatever decides exactly who is going to be killed.
And then does,

305
00:19:50.830 --> 00:19:55.190
does the killing and um,
first whatever you think about them,

306
00:19:55.320 --> 00:19:57.280
the fact is there's all other,

307
00:19:57.290 --> 00:20:02.160
has been of course a huge amount of investment in civilian uses of AI recently.

308
00:20:02.161 --> 00:20:07.161
It's actually Dorf by Tom talk about military spending here recently.

309
00:20:07.740 --> 00:20:09.400
So if you look in the pie,
we,

310
00:20:11.140 --> 00:20:16.140
there's a real risk that the status quo will just mean that most of this loud

311
00:20:16.201 --> 00:20:20.820
sucking noise trying to recruit AI graduates from MIT and Stanford and elsewhere

312
00:20:21.000 --> 00:20:25.470
will be to go to military places rather than to places like Google and most

313
00:20:25.490 --> 00:20:27.510
Irish or should felt that that would be a great shame.

314
00:20:27.930 --> 00:20:29.070
Here's how I think about it.

315
00:20:30.570 --> 00:20:34.050
If you look at any science,
right,

316
00:20:34.051 --> 00:20:37.890
you can always use it to develop new ways of helping people or new ways of

317
00:20:37.891 --> 00:20:41.370
harming people.
And the biologists fought really,

318
00:20:41.371 --> 00:20:43.380
really hard to make sure that their science

319
00:20:45.240 --> 00:20:50.240
is now known as new ways of curing people rather than for biological weapons.

320
00:20:51.820 --> 00:20:55.840
They fought very hard and they got an international ban on biological weapons

321
00:20:55.841 --> 00:21:00.120
past.
Similarly,
chemists managed to get the chemical weapons band.

322
00:21:00.121 --> 00:21:03.640
But by really speaking up as a community and persuading politicians around the

323
00:21:03.641 --> 00:21:04.660
world that this was good.

324
00:21:05.110 --> 00:21:08.170
And that's why you associate chemistry now mainly with new materials and it's

325
00:21:08.171 --> 00:21:11.560
very stigmatized to have bio weapons.
So even if some countries cheat on them,

326
00:21:11.561 --> 00:21:15.670
it's so stigmatized that Assad even gave up as chemical weapons not get invaded.

327
00:21:15.671 --> 00:21:19.990
And if you want to buy some chemical weapons to uh,
do something silly,

328
00:21:19.991 --> 00:21:22.750
you're going to find it really hard to find anyone who's going to sell them to

329
00:21:22.751 --> 00:21:25.660
you because it's so stigmatized.
And what the,

330
00:21:27.010 --> 00:21:30.340
what there's a very widespread support for in the Ai community is exactly the

331
00:21:30.341 --> 00:21:33.700
same thing here to try to negotiate an international treaty where the

332
00:21:33.710 --> 00:21:35.590
superpowers get together and say,
hey,
you know,

333
00:21:35.950 --> 00:21:39.760
the main winners of having an out of control arms race and AI weapons is not

334
00:21:39.761 --> 00:21:44.761
going to be the superpower as it's going to be isis and everybody else who can't

335
00:21:45.371 --> 00:21:47.440
afford expensive weapons.

336
00:21:47.441 --> 00:21:50.230
But we'd love to have a little cheap things that they can use to assassinate

337
00:21:50.231 --> 00:21:54.780
anybody with anonymously and basically drive the cost of a anonymous

338
00:21:55.300 --> 00:21:56.680
assassination down to zero.

339
00:21:57.040 --> 00:21:59.740
And this is something if you want to get involved in the United Nations,

340
00:21:59.741 --> 00:22:01.930
this is going to discuss this in November actually,

341
00:22:03.000 --> 00:22:06.340
and I think the more vocal the Ai community is on this issue,

342
00:22:06.700 --> 00:22:10.480
the more likely it is that that AI rocket here is going to veer in the same

343
00:22:10.900 --> 00:22:13.990
direction as the biology and chemistry rocket went.

344
00:22:16.510 --> 00:22:21.310
Finally,
let me say a little bit about the final SLR principles here.

345
00:22:21.640 --> 00:22:22.460
I find it really,

346
00:22:22.460 --> 00:22:27.460
really remarkable that even though a few years ago,

347
00:22:27.521 --> 00:22:31.420
if you started talking about superintelligence or existential risks or whatever,

348
00:22:32.110 --> 00:22:34.600
many people will dismiss you as some sort of clueless.

349
00:22:34.601 --> 00:22:38.950
A person who didn't know anything about Ai.

350
00:22:39.940 --> 00:22:44.890
These words are in here and yet this is signed by
you,

351
00:22:45.440 --> 00:22:49.910
is a Saba,
is the CEO of deep mind.
It's,
it's,
it's um,

352
00:22:50.440 --> 00:22:55.440
signed by Peter Norvig who's just sitting over there by your very own Jeff Dean

353
00:22:55.631 --> 00:22:59.530
and by who really a WHO's who of of Ai researchers,
over a thousand of them.

354
00:23:01.000 --> 00:23:03.900
So there's been a much greater acceptance of the fact that,
hey,
yeah,

355
00:23:03.970 --> 00:23:07.300
this is part of steering.
If maybe AI is actually going to succeed.

356
00:23:07.390 --> 00:23:09.760
And maybe we need to take these sort of things into account.

357
00:23:09.940 --> 00:23:11.440
Let me just unpack a little bit

358
00:23:13.990 --> 00:23:16.750
what the deal is with all of this.
So,
so first of all,

359
00:23:16.751 --> 00:23:19.060
why should we take it seriously at all?

360
00:23:19.270 --> 00:23:24.270
This idea of recursive self improvement and superintelligence we saw them,

361
00:23:25.120 --> 00:23:28.480
a lot of people expect we can get the human level AI and a few decades.

362
00:23:28.481 --> 00:23:32.980
But why would that mean that maybe we can get a,
I'm much smarter than us,

363
00:23:33.250 --> 00:23:34.083
not just the little,

364
00:23:35.170 --> 00:23:39.190
the basic argument for this is very eloquently summarized and just this

365
00:23:39.191 --> 00:23:44.191
paragraph by Ij good from from 1965 and a mathematician who worked with Alan

366
00:23:44.830 --> 00:23:47.650
Turing to codes during World War Two and you mean,

367
00:23:47.900 --> 00:23:52.700
I think you're mostly heard this all before basically says that if we have a

368
00:23:52.701 --> 00:23:57.200
computer that a machine that can do everything as well as as we can,

369
00:23:57.201 --> 00:24:01.580
well one of the things we can do is design a AI systems.
So then it can too.

370
00:24:02.030 --> 00:24:05.420
And then you can hire,
instead of hiring 20,000 Google employees that work for,

371
00:24:05.421 --> 00:24:08.930
you can get 20 million little AI things working for you.

372
00:24:08.930 --> 00:24:12.050
And they can work much faster.
And the,
the speed of,

373
00:24:12.320 --> 00:24:16.640
of Ai Development will no longer be set by the typical r and d timescale of

374
00:24:16.641 --> 00:24:21.320
humans a years.
But by how fast machines can help you do this,
which could be way,

375
00:24:21.321 --> 00:24:24.500
way faster.
And that if it turns out that,
um,

376
00:24:25.610 --> 00:24:28.750
that we have a,
uh,
hardware overhang where,

377
00:24:28.950 --> 00:24:32.900
where we've compensated for the fact that we really are kind of clueless about

378
00:24:32.901 --> 00:24:33.920
how to do the software,

379
00:24:33.921 --> 00:24:37.940
have human level AI by having massive amounts of extra hardware.

380
00:24:38.240 --> 00:24:41.720
Then it might be that you can get a lot of it through improvements first even.

381
00:24:41.721 --> 00:24:45.230
But just by changing the software,
which is something that the,
that,
um,

382
00:24:45.590 --> 00:24:48.560
should be done very,
very quickly by without even everything,
build new stuff.

383
00:24:49.190 --> 00:24:51.170
And then from there on,
things could get them.

384
00:24:52.880 --> 00:24:56.990
You might be able to get machines that are just dramatically larger,

385
00:24:57.260 --> 00:25:00.860
dramatically smarter than us.
We don't know this,
this will happen,
but peep.

386
00:25:00.920 --> 00:25:05.920
But basically what we see here is that poor linea researchers are viewing this

387
00:25:06.091 --> 00:25:11.000
is at least a possibility that we should,
that we should take seriously.

388
00:25:11.370 --> 00:25:14.630
And other thing which you see here is existential risk.

389
00:25:15.710 --> 00:25:18.860
It's a more specifically,
it says your risks posed by AI systems,

390
00:25:18.861 --> 00:25:21.530
especially existential risks as much to be subject to the planning and

391
00:25:21.531 --> 00:25:23.690
mitigation efforts commensurate with their expected impact.

392
00:25:23.840 --> 00:25:25.160
Next essential risk is the risk,

393
00:25:25.161 --> 00:25:30.161
which basically could include the humanity just getting wiped out all together.

394
00:25:30.770 --> 00:25:33.470
Why would you possibly worry about that?

395
00:25:33.770 --> 00:25:37.730
There are so many absolutely ridiculous Hollywood movies with Terminator Robots

396
00:25:37.731 --> 00:25:40.570
or whatever.
You can't even watch like cringing.

397
00:25:40.580 --> 00:25:45.450
So what are the seaweeds reasons that people like this sign onto something that

398
00:25:45.700 --> 00:25:50.500
talks about that?
Well,
a common criticism that you hear is it.

399
00:25:50.501 --> 00:25:53.300
Well,
you know,
machines,

400
00:25:54.530 --> 00:25:58.340
there's no reason to think that intelligent machines would have human goals if

401
00:25:58.370 --> 00:26:00.910
we built them.
And after all this,

402
00:26:00.950 --> 00:26:05.030
why should they have sort of weird alpha male goals of trying to get power or

403
00:26:05.440 --> 00:26:07.170
even self preservation?
You know,

404
00:26:07.250 --> 00:26:09.860
my laptop doesn't protest when I tried to switch it off.
Right?

405
00:26:13.190 --> 00:26:14.930
But there's a very interesting argument here.

406
00:26:14.931 --> 00:26:18.440
I just want to share either in the form of this silly little fake computer game

407
00:26:18.441 --> 00:26:19.274
I drew for you here.

408
00:26:19.670 --> 00:26:24.670
Just imagine that you are this little blue friendly robot whose only goal is to

409
00:26:26.901 --> 00:26:31.630
save as many sheep as possible from the big bad wolf.
Okay?
You don't care.

410
00:26:31.790 --> 00:26:33.170
You have not put into this.

411
00:26:33.230 --> 00:26:37.400
This robot does not have the goal of surviving or getting resources or any stuff

412
00:26:37.401 --> 00:26:42.100
like that.
Just sheep saving.
It's all about these cute little cheapies.
Okay.
Um,

413
00:26:42.380 --> 00:26:46.290
it's gonna very thing if it's smart or figure out that if it walks into the bomb

414
00:26:46.291 --> 00:26:49.650
here and blows up,
then it's not going to save any shape at all.

415
00:26:50.940 --> 00:26:53.640
So in a sub goal that it will derive is,

416
00:26:53.670 --> 00:26:57.300
well actually that's not get blown up and it's going to get a self preservation

417
00:26:57.301 --> 00:27:01.330
instinct.
This is a very generic inclusion.
If you program,
if you have a robot,

418
00:27:01.331 --> 00:27:04.020
then you program it to walk to the supermarket and buy your food and Cook you a

419
00:27:04.021 --> 00:27:06.480
nice dinner,
you know,
it's going to,
again,

420
00:27:06.510 --> 00:27:09.990
develop the sub goal of self preservation because if it gets mugged and murdered

421
00:27:09.991 --> 00:27:12.900
on the way back with your food,
it's going to not give you your dinner.

422
00:27:13.050 --> 00:27:16.350
So it's going to want to somehow avoid that.
Right?
Um,

423
00:27:17.040 --> 00:27:20.910
self preservation and there's an emergent goal of almost any goal that the

424
00:27:20.911 --> 00:27:25.580
machine might have because goals are hard to accomplish when you're broken.
Uh,

425
00:27:25.950 --> 00:27:29.670
and also,
uh,
if,
if the robot,

426
00:27:30.330 --> 00:27:31.910
the robot might develop finding,

427
00:27:32.000 --> 00:27:36.990
having an incentive to get a better model of the world it's in here and discover

428
00:27:36.991 --> 00:27:39.690
that there's actually a shortcut it can take to get to the,

429
00:27:40.230 --> 00:27:42.330
where the sheep are faster than it can save more.

430
00:27:42.750 --> 00:27:47.750
So trying to understand more about how the world works is a natural sub goal you

431
00:27:48.001 --> 00:27:50.880
can get no matter whatever fundamental goal you program the machine to have.

432
00:27:51.150 --> 00:27:53.610
And then resource acquisition to can emerge.

433
00:27:53.611 --> 00:27:57.720
Because when this little row bought here,
discovers that when a drink the potion,

434
00:27:57.721 --> 00:28:00.080
it can run twice as fast than it can save more sheeps.

435
00:28:00.081 --> 00:28:01.050
It's going to want the potion.

436
00:28:01.460 --> 00:28:04.500
It'll discover they want it takes the gun that can shoot the wolf and save all

437
00:28:04.501 --> 00:28:07.950
the sheep.
Great.
Um,
this is going to want to have resources.

438
00:28:07.960 --> 00:28:11.970
And I've summarized in this pyramid here,
this idea,

439
00:28:12.750 --> 00:28:17.400
which has been a very eloquently was mentioned first by Steve Omohundro who

440
00:28:17.401 --> 00:28:20.850
lives here in the area and just talked a lot about and Nick Bostrom's book,

441
00:28:21.090 --> 00:28:24.450
the idea is just the whatever fundamental goal you give a very intelligent

442
00:28:24.451 --> 00:28:26.160
machine.
If it's pretty open ended,

443
00:28:26.400 --> 00:28:30.120
it's pretty natural to expect that it might develop sub goals of,

444
00:28:30.180 --> 00:28:33.810
of not wanting to be switched off and try to get resources.

445
00:28:33.811 --> 00:28:36.580
And that can be fine.
There's no,

446
00:28:36.600 --> 00:28:40.710
not necessarily a problem being in the presence of more intelligent entities.

447
00:28:40.920 --> 00:28:43.800
We all did that as kids,
right?
With our parents.

448
00:28:44.670 --> 00:28:48.480
The reason it was fine was cause their goals were aligned with our goals.

449
00:28:49.110 --> 00:28:50.370
So there in lies the rub.

450
00:28:50.620 --> 00:28:55.290
We want to make sure that if we ever give a lot of power to machines of

451
00:28:55.291 --> 00:28:59.760
intelligence,
he comparable or greater to ours,
their goals are aligned with ours,

452
00:29:00.360 --> 00:29:02.550
otherwise we can be in trouble.

453
00:29:04.740 --> 00:29:07.080
So to summarize,

454
00:29:08.550 --> 00:29:12.120
these are all the questions that we need to answer.
Technical research questions.

455
00:29:13.110 --> 00:29:17.000
How can you make,
how can you,
how machines learn,
uh,

456
00:29:17.140 --> 00:29:18.660
retain retainer goals,
for example.

457
00:29:18.770 --> 00:29:22.770
And let me just show you a very short video up talking about these issues and

458
00:29:22.780 --> 00:29:24.650
superintelligence immense them.

459
00:29:28.560 --> 00:29:30.750
Let's see if we have better luck with video this time.

460
00:29:32.290 --> 00:29:33.123
<v 3>Okay.</v>

461
00:29:33.140 --> 00:29:37.510
<v 4>Will artificial intelligence ever replace humans is a hotly debated question</v>

462
00:29:37.520 --> 00:29:38.150
these days.

463
00:29:38.150 --> 00:29:41.440
Some people clean and computers will eventually gain superintelligence be able

464
00:29:41.441 --> 00:29:42.100
to outperform

465
00:29:42.100 --> 00:29:46.150
<v 5>humans on any task and destroy humanity.
Other people say,
don't worry,</v>

466
00:29:46.151 --> 00:29:49.510
AI will just be another tool.
We can use some control like our current computers.

467
00:29:49.540 --> 00:29:53.350
So we've got physicists and AI researcher Max Tegmark back again to share with

468
00:29:53.351 --> 00:29:56.590
us the collective takeaways from the recent sill of our conference on the future

469
00:29:56.591 --> 00:30:00.670
of AI that he helped organize and he's going to help separate AI myths from AI

470
00:30:00.671 --> 00:30:03.670
facts.
Hello,
first off,
Max machines,

471
00:30:03.671 --> 00:30:06.970
including computers have long been better than us that many tasks like

472
00:30:06.971 --> 00:30:10.840
arithmetic or weaving,
but those are often repetitive and mechanical operations.

473
00:30:10.870 --> 00:30:14.110
So why shouldn't I believe that there are some things that are simply impossible

474
00:30:14.111 --> 00:30:17.560
for machines to do as well as people say making minute physics videos or

475
00:30:17.561 --> 00:30:18.970
consoling a friend.
Well,

476
00:30:19.000 --> 00:30:22.240
we've traditionally thought of intelligence as something mysterious that can

477
00:30:22.241 --> 00:30:25.360
only exist in biological organisms,
especially humans.

478
00:30:25.450 --> 00:30:27.820
But from the perspective of modern physical science,

479
00:30:27.821 --> 00:30:31.610
intelligence is simply a particular kind of information processing and reacting

480
00:30:31.720 --> 00:30:34.780
performed by a particular random bits of elementary particles moving around.

481
00:30:34.780 --> 00:30:37.900
And there's no law of physics that says it's impossible to do that kind of

482
00:30:37.901 --> 00:30:40.300
information processing better than humans already do.

483
00:30:40.480 --> 00:30:43.960
It's not a stretch to say that earthworms process information better than rocks

484
00:30:43.961 --> 00:30:46.150
and humans better than earthworms.
And in many areas,

485
00:30:46.210 --> 00:30:47.950
machines are already better than humans.

486
00:30:48.070 --> 00:30:51.550
This suggest that we've liked the only seen the tip of the intelligence iceberg

487
00:30:51.580 --> 00:30:54.650
and there we're on track to unlock the full intelligence,
this latent,

488
00:30:54.660 --> 00:30:58.090
the nature and use it to help humanity flourish or flounder.

489
00:30:58.360 --> 00:31:02.410
So how do we keep ourselves on the right side of the flourish or flounder

490
00:31:02.411 --> 00:31:03.940
balance?
What,
if anything,

491
00:31:03.941 --> 00:31:07.240
should we really be concerned about with superintelligent Ai Heroes?

492
00:31:07.250 --> 00:31:09.550
What has many top AI researchers concerned?

493
00:31:09.760 --> 00:31:12.100
Not Machines or computers turning evil,

494
00:31:12.220 --> 00:31:15.790
but something more subtle superintelligence that simply doesn't share our goals.

495
00:31:15.910 --> 00:31:18.910
If he'd seeking missile is homing in on you,
you probably wouldn't think.

496
00:31:19.000 --> 00:31:22.620
No need to worry.
It's not evil.
It's just following its programming.
No.

497
00:31:22.630 --> 00:31:26.710
What matters to you is what the heat seeking missile does and how well it does

498
00:31:26.711 --> 00:31:29.470
it,
not what it's feeling or whether it has feelings at all.

499
00:31:29.500 --> 00:31:31.360
The real worry isn't malevolence,

500
00:31:31.420 --> 00:31:36.250
but competence superintelligent AI is by definition very good at attaining it's

501
00:31:36.251 --> 00:31:36.671
gold.

502
00:31:36.671 --> 00:31:40.060
So the most important thing for us to do is to ensure that its goals are aligned

503
00:31:40.061 --> 00:31:44.560
with ours.
As an analogy,
humans are more intelligent and competent than the ads.

504
00:31:44.561 --> 00:31:47.800
And if we want to build a hydroelectric dam where there happens to be an
anthill,

505
00:31:47.830 --> 00:31:51.610
there may be normal level and is involved,
but well,
too bad for the ants,

506
00:31:51.640 --> 00:31:53.260
cats and dogs.
On the other hand,

507
00:31:53.261 --> 00:31:56.950
I've done a great job of aligning their goals with the goals of humans.
I mean,

508
00:31:56.951 --> 00:31:58.320
even though I'm a physicist,

509
00:31:58.390 --> 00:32:02.890
I can't help think kittens are the cutest particle arrangements in our universe.

510
00:32:03.040 --> 00:32:04.480
If we build super intelligence,

511
00:32:04.481 --> 00:32:08.200
we'd be better off in the position of cats and dogs and than that,
or better yet,

512
00:32:08.230 --> 00:32:12.100
we'll figure out how to ensure that AI adopt our goals rather than the other way

513
00:32:12.101 --> 00:32:15.640
around.
And when exactly is super intelligence is going to arrive.

514
00:32:15.670 --> 00:32:18.100
When do we need to start panicking?
First of all,

515
00:32:18.101 --> 00:32:21.280
Henry superintelligence doesn't have to be something negative.
In fact,

516
00:32:21.281 --> 00:32:24.790
if we get it right,
AI might become the best thing ever to happen to humanity.

517
00:32:24.880 --> 00:32:28.420
Everything I love about civilization is the product of intelligence.

518
00:32:28.450 --> 00:32:31.990
So if AI amplifies our collective intelligence enough to solve today's and

519
00:32:31.991 --> 00:32:36.490
tomorrow's greatest problems,
humanity might flourish like never before.
Second,

520
00:32:36.520 --> 00:32:40.460
most AI researchers,
things super intelligence is at decades away.

521
00:32:40.490 --> 00:32:43.880
But the research needed to ensure that it remains beneficial to humanity rather

522
00:32:43.881 --> 00:32:47.930
than harmful.
It might also take decades.
So we need to start right away.

523
00:32:48.110 --> 00:32:48.681
For example,

524
00:32:48.681 --> 00:32:51.440
we'll need to figure out how to ensure a machine is learning the collective

525
00:32:51.441 --> 00:32:52.370
goals of humanity.

526
00:32:52.520 --> 00:32:55.580
Adopt these goals for themselves and repeating the goals as they get ever

527
00:32:55.581 --> 00:32:58.190
smarter.
And what about when our goals disagree?

528
00:32:58.310 --> 00:33:00.770
Should we vote on what the machine's goals should be?

529
00:33:00.870 --> 00:33:02.930
Should we do whatever the president wants,

530
00:33:02.990 --> 00:33:07.520
whatever the creator of a super intelligence wants with the AI decide in a very

531
00:33:07.521 --> 00:33:10.310
real way.
And then the question of how to live with super intelligence.

532
00:33:10.311 --> 00:33:13.400
It's a question of what sort of future we want to create,
which humanity,

533
00:33:13.460 --> 00:33:17.240
which obviously shouldn't just be left to AI researchers as caring and then

534
00:33:17.241 --> 00:33:18.680
socially skilled as we are.

535
00:33:18.820 --> 00:33:19.653
<v 1>Okay,</v>

536
00:33:20.290 --> 00:33:23.860
so that leads to the very final point I want to make here today to when this,

537
00:33:23.861 --> 00:33:28.120
where's the race trading an awesome future with Ai.
In addition to doing these,

538
00:33:28.810 --> 00:33:30.700
these various things I've talked about,

539
00:33:30.880 --> 00:33:33.040
we really need to think about what kind of future we want,

540
00:33:33.041 --> 00:33:37.240
what sort of goal we want to have.
We want to steer our technology.

541
00:33:37.480 --> 00:33:40.450
So just for fun,
the survey I mentioned that we did,

542
00:33:41.800 --> 00:33:44.920
we asked people also to say what they want that for the future,

543
00:33:44.921 --> 00:33:46.360
and I'll just share with you here,

544
00:33:46.390 --> 00:33:48.460
these are from the analysis I did last weekend.

545
00:33:49.690 --> 00:33:54.490
Most people out of the 14,000,
866 years say they actually want they,

546
00:33:54.500 --> 00:33:57.490
I had to go all the way to superintelligence although some are saying no here.

547
00:33:58.140 --> 00:34:02.140
Um,
most people,
lot of people want humans to be in control.

548
00:34:03.370 --> 00:34:06.880
Most people actually want both humans and machines to being controlled together

549
00:34:07.090 --> 00:34:09.420
and a small fraction of super for the machines.

550
00:34:11.960 --> 00:34:16.930
Uh,
and then,
uh,
when asked about consciousness,
a,
a lot of people said,

551
00:34:16.931 --> 00:34:17.141
yeah,

552
00:34:17.141 --> 00:34:22.141
they will if they have machines are behaving as if there is intelligence zoom as

553
00:34:22.451 --> 00:34:25.030
they would like to have to have them have a subjective experience.

554
00:34:25.031 --> 00:34:28.090
All sorts of the machines can feel good,
but some people said,
Nah,

555
00:34:28.300 --> 00:34:30.820
they prefer having a Zombie robots.

556
00:34:30.850 --> 00:34:33.100
Don't feel conscious that people don't have to feel guilty,

557
00:34:33.280 --> 00:34:35.640
but switching them offer,
give,

558
00:34:35.850 --> 00:34:40.210
giving them boring things to do in terms of what a future civilization should

559
00:34:40.240 --> 00:34:42.340
strive for.
There was a

560
00:34:44.530 --> 00:34:45.380
large majority,

561
00:34:45.381 --> 00:34:48.680
you felt we should either try to maximize positive experiences or minimize

562
00:34:48.681 --> 00:34:52.750
suffering or something like that.
Uh,
the more people who said that feud,
let,

563
00:34:52.780 --> 00:34:56.950
let the future
civilization pick whatever goals they want,

564
00:34:56.951 --> 00:34:58.120
as long as it's reasonable.

565
00:34:58.360 --> 00:35:01.530
Some people said they didn't even care about if they thought the goal that

566
00:35:01.531 --> 00:35:03.910
future wanted was reasonable,
even if it was pointless.

567
00:35:03.910 --> 00:35:06.900
The banal like maybe turning our universe into paperclips.

568
00:35:06.910 --> 00:35:08.860
They were fine with just delegating it to humans.

569
00:35:08.861 --> 00:35:12.520
But most people actually felt that since we are creating this technology,

570
00:35:13.000 --> 00:35:17.830
we have the right to have some say as to
where things should go.

571
00:35:18.070 --> 00:35:23.070
That the broadest agreement of all was on the quiz question that actually maybe

572
00:35:24.791 --> 00:35:29.650
we shouldn't just limit the future of life to forever be stuck on this little

573
00:35:29.651 --> 00:35:30.484
planet,

574
00:35:31.030 --> 00:35:35.020
give it the potential to spread and flourished throughout the cosmos and to get

575
00:35:35.021 --> 00:35:38.700
people thinking more about the different futures.
My wife May,

576
00:35:38.701 --> 00:35:39.980
I likes to point out that,
um,

577
00:35:41.490 --> 00:35:44.310
even though it's a good idea to visualize positive outcome is when you're

578
00:35:44.311 --> 00:35:47.880
planning your own career and then try to figure out how to get there.

579
00:35:48.780 --> 00:35:51.210
We kind of do the exact opposite.
As a society,

580
00:35:51.240 --> 00:35:54.150
we just tend to think about everything that could possibly go wrong.

581
00:35:54.150 --> 00:35:58.530
And then we like freak out about it.
When you watch Hollywood movies,

582
00:35:58.531 --> 00:36:01.410
it's almost always this topic depictions of the future,
right?

583
00:36:01.411 --> 00:36:04.350
So to get away from this a little bit in my book,

584
00:36:05.370 --> 00:36:08.070
the whole chapter five is a theories of thought.

585
00:36:08.071 --> 00:36:11.760
Experiments with different future scenarios trying to spread it span the whole

586
00:36:11.761 --> 00:36:13.800
range of what people have talked about and other.

587
00:36:14.040 --> 00:36:17.130
So you yourselves can ask what you would actually prefer.

588
00:36:17.430 --> 00:36:22.050
And the most striking thing from the survey was that people disagree very

589
00:36:22.051 --> 00:36:24.840
strongly and,
and what sort of society they would like.

590
00:36:25.140 --> 00:36:29.110
And this is a fascinating discussion that would really encourage you all to what

591
00:36:29.400 --> 00:36:31.730
to join into.
Um,
did this,

592
00:36:32.050 --> 00:36:34.650
this is going to end by saying that the,

593
00:36:35.280 --> 00:36:37.320
I think when we look to the future,

594
00:36:41.410 --> 00:36:43.650
there's really a lot to be excited about.

595
00:36:44.580 --> 00:36:47.760
People sometimes ask me,
Max,
you know,

596
00:36:47.761 --> 00:36:52.620
all you for AI or against Ai.
And I responded by asking them what about fire?

597
00:36:53.100 --> 00:36:54.360
Are you for it or against it?

598
00:36:56.500 --> 00:36:59.820
Then of course they'll concede that there for fire to heat their homes in the

599
00:36:59.821 --> 00:37:03.690
winter and against the fire for arson.
But it's the same with all technology.

600
00:37:03.691 --> 00:37:05.010
It's always a double edged sword.

601
00:37:05.130 --> 00:37:07.560
The difference with AI is just as much more powerful.

602
00:37:07.561 --> 00:37:10.560
So we need to put even more effort into into how,

603
00:37:10.950 --> 00:37:14.040
how are we steer it if you want life to,

604
00:37:14.400 --> 00:37:19.400
to exist for beyond the next election cycle and maybe hopefully for billions of

605
00:37:19.890 --> 00:37:21.090
years,
uh,

606
00:37:21.480 --> 00:37:26.480
on earth and maybe beyond then just pressing pause on technology forever.

607
00:37:27.010 --> 00:37:30.810
That was actually just a really sucky idea because if we do that,

608
00:37:30.811 --> 00:37:33.300
the question isn't whether humanity is going to go extinct.

609
00:37:33.960 --> 00:37:36.870
The question is just what's going to wipe us out?

610
00:37:37.080 --> 00:37:41.850
Whether it's going to be the next massive asteroid strike,

611
00:37:41.851 --> 00:37:46.851
like the one that took the dinos out or the next super volcano or another one on

612
00:37:48.241 --> 00:37:50.970
the list of long things that we know are going to happen through earth that

613
00:37:50.971 --> 00:37:54.330
technology can create.
They just started that technology can prevent,

614
00:37:54.840 --> 00:37:58.230
but technology that we don't have yet that's going to require further

615
00:37:58.231 --> 00:37:59.250
development of our tech.

616
00:37:59.251 --> 00:38:03.720
So I for one think that it would be really foolish if we just run away from

617
00:38:03.721 --> 00:38:04.820
technology.
I would,

618
00:38:04.940 --> 00:38:08.070
I would feel I'm much more excited about it in the Google spirit.

619
00:38:09.750 --> 00:38:13.320
And I love your old slogan,
don't be evil.
Asking what can we,

620
00:38:13.710 --> 00:38:15.960
what can we do to steer that development,

621
00:38:15.961 --> 00:38:19.380
that right technology in a direction so that life can really flourish

622
00:38:21.450 --> 00:38:24.090
not just for the next election cycle but for very,

623
00:38:24.091 --> 00:38:28.740
very long time on earth and maybe even throughout our cosmos.
Thank you.

624
00:38:29.820 --> 00:38:34.820
<v 3>[inaudible]</v>

625
00:38:37.770 --> 00:38:38.603
okay,

626
00:38:39.890 --> 00:38:43.040
<v 2>thanks so much Max.
And then when we have time for questions from the audience,</v>

627
00:38:43.550 --> 00:38:46.940
we have a mic over here which we can use for questions and I slept can pass this

628
00:38:46.941 --> 00:38:49.640
one around.
And while we're doing that,
I'll pull up the dory.

629
00:38:50.380 --> 00:38:53.410
<v 1>Great.
And since you mentioned there were a lot of questions,</v>

630
00:38:53.440 --> 00:38:56.980
make sure to keep the questions brief and make sure that they actually are

631
00:38:56.981 --> 00:38:57.814
questions.

632
00:39:00.560 --> 00:39:04.250
<v 2>Ai Risk seems to become a much more mainstream worry in the last few years.</v>

633
00:39:04.520 --> 00:39:08.270
What changed to make that happen and why didn't we do it earlier?

634
00:39:10.160 --> 00:39:11.960
<v 1>I agree with you.
I'm,
I'm actually very,</v>

635
00:39:11.961 --> 00:39:16.760
very happy that it's changing this way and trying to help make a change this
way.

636
00:39:16.761 --> 00:39:20.990
It was the key reasonably founded the future of Life Institute and organize the

637
00:39:20.991 --> 00:39:24.710
Puerto Rico conference this summer conferences and so on because we felt that

638
00:39:24.711 --> 00:39:27.200
the,
uh,
up until a few years ago,

639
00:39:27.210 --> 00:39:31.790
the debate was kind of this dysfunctional and we read what I think is really,

640
00:39:31.791 --> 00:39:33.470
really changed things for the better.

641
00:39:33.560 --> 00:39:38.360
Is it the AI research community itself as really engage joined this debate and

642
00:39:38.361 --> 00:39:40.120
started the own it can.

643
00:39:40.130 --> 00:39:43.390
I think that's why it's become more mainstream and also much more sensible.

644
00:39:47.000 --> 00:39:47.833
<v 3>Okay.</v>

645
00:39:49.100 --> 00:39:52.610
<v 1>Okay.
So you were the boss's we alternate with online off nine questions.</v>

646
00:39:53.530 --> 00:39:54.990
Do you want to read the questions?
Sure.

647
00:39:56.150 --> 00:39:57.080
<v 2>Uh,
what would you do to her?</v>

648
00:39:57.130 --> 00:39:59.680
What would you most hope to see a company like Google to,

649
00:39:59.740 --> 00:40:03.050
to ensure safety as we transitioned to a more AI centric world?

650
00:40:04.130 --> 00:40:07.580
<v 1>So as I said,
I,
I think,
uh,
Google</v>

651
00:40:10.160 --> 00:40:14.630
already has the soul to do exactly what's needed.

652
00:40:14.631 --> 00:40:18.200
This don't be evil slogan of Larry and Sergey.

653
00:40:18.201 --> 00:40:23.201
I interpreted it as though we shouldn't just build technology because it's cool,

654
00:40:23.811 --> 00:40:25.480
but we should think about its uses.

655
00:40:25.910 --> 00:40:30.560
Know for those of you who know the Tall Tom Lira song about Verna Front Brown

656
00:40:30.870 --> 00:40:35.770
and that's not once the rockets go up as well as a calm down.

657
00:40:35.771 --> 00:40:39.080
That's not my department says van half on Brown.

658
00:40:39.500 --> 00:40:43.520
I view Google don't be evil.
Slogan is exactly the opposite of that.

659
00:40:43.580 --> 00:40:47.090
Thinking mindfully about how to steer the technology to be good.

660
00:40:47.420 --> 00:40:49.550
And then I'm also really excited again,

661
00:40:49.551 --> 00:40:52.340
the Google is one of the founding partners in the partnership for Ai.

662
00:40:53.010 --> 00:40:54.200
Trying to make sure that we,

663
00:40:54.650 --> 00:40:58.940
that this happens not just in Google the Google what Google does but,

664
00:40:58.941 --> 00:41:02.070
but throughout the community.
And um,

665
00:41:02.780 --> 00:41:05.420
I also think it's great if Google can pull all of its strings,

666
00:41:05.421 --> 00:41:09.830
the persuade politicians all around the world at CRSD fund the AI safety

667
00:41:09.831 --> 00:41:13.040
research.
Because the sad fact is even though there's a great,

668
00:41:13.041 --> 00:41:15.620
we'll for AI researchers to do this stuff now,

669
00:41:15.950 --> 00:41:19.370
there's almost no funding for it to tell.
What do you Elon Musk help us give out?

670
00:41:19.460 --> 00:41:23.900
2037 grants for is just a drop in the bucket of what's needed.
And,
um,

671
00:41:25.220 --> 00:41:30.220
it makes sense that Google and other private companies want to own the Ip on.

672
00:41:31.890 --> 00:41:34.730
Um,
things would make AI more powerful and build products of it.

673
00:41:35.480 --> 00:41:37.460
But these same private companies,

674
00:41:37.550 --> 00:41:42.350
it's better for them all if nobody patents the way to make it safe and keeps

675
00:41:42.351 --> 00:41:43.670
others from using it.
Right.

676
00:41:43.760 --> 00:41:47.480
That's something that's great if it's developed openly by companies or share it

677
00:41:47.481 --> 00:41:51.260
or in universities so that everybody can use the same best practices and raise

678
00:41:52.070 --> 00:41:54.110
the quality of safety everywhere.

679
00:41:57.130 --> 00:42:00.520
<v 6>Uh,
all right.
So,
uh,
Max actually he talked to you last night.</v>

680
00:42:00.521 --> 00:42:04.590
How bout a lot like future,
I really long,

681
00:42:04.600 --> 00:42:07.690
maybe a hundred years each,
what's going to happen?
But,
uh,
uh,

682
00:42:07.691 --> 00:42:11.620
if you look at the AI nowadays,
not a lot of people focusing on today.

683
00:42:12.130 --> 00:42:16.930
I'll just aiming it's,
uh,
risks.
So if you think about how,
uh,

684
00:42:16.960 --> 00:42:19.240
how the Trump got elected and,
uh,

685
00:42:19.480 --> 00:42:22.900
how those things went wrong in his last few years,
uh,

686
00:42:22.930 --> 00:42:27.190
you cut really denying that a,
I has contributed a lot.

687
00:42:27.191 --> 00:42:29.530
He specially interfect deal sad.
Uh,

688
00:42:29.920 --> 00:42:34.150
he has like a suggested contents.
So,
um,

689
00:42:34.750 --> 00:42:39.490
he started like focusing on our all our energy into the future.

690
00:42:39.491 --> 00:42:44.350
So I felt there are really few people that's like looking into today.

691
00:42:44.560 --> 00:42:45.101
So do you think,

692
00:42:45.101 --> 00:42:47.910
cause that's a problem all these Inca would need to do battery on that?

693
00:42:48.490 --> 00:42:48.791
<v 1>Yeah,</v>

694
00:42:48.791 --> 00:42:53.590
I think there's a really great opportunity for us nerds in the tech community to

695
00:42:53.591 --> 00:42:58.591
educate the broader public and politicians about the need to really engage with

696
00:42:58.661 --> 00:43:03.010
this.
This is one of the reasons I want you to write this book.
Um,
I think,
uh,

697
00:43:03.520 --> 00:43:07.840
when we watched,
when I watched the presidential debates for the last election,

698
00:43:07.841 --> 00:43:08.530
for example,

699
00:43:08.530 --> 00:43:12.040
complete the aside from the front of the issue is they've talked about,

700
00:43:12.670 --> 00:43:16.120
I thought it was just absolutely astonishing what they didn't talk about.
No,

701
00:43:16.121 --> 00:43:19.570
none of them talked about AI at all.
Hello?

702
00:43:19.571 --> 00:43:22.480
Like they're talking about jobs,
they're not mentioning Ai.

703
00:43:22.750 --> 00:43:26.080
They're talking about six international security and not talking about the,

704
00:43:26.081 --> 00:43:30.790
I like the biggest technology on out there.
And uh,
I think,
um,

705
00:43:32.740 --> 00:43:35.500
in addition to just telling politicians to pay attention,

706
00:43:35.501 --> 00:43:37.920
I think it's incredibly valuable.
Also,

707
00:43:38.010 --> 00:43:40.750
if a bunch of people from the tech community can actually go into government

708
00:43:40.751 --> 00:43:45.110
positions to add more human level intelligence in government.
So,

709
00:43:45.910 --> 00:43:48.310
so the,
the preventative with the world governments from,

710
00:43:48.311 --> 00:43:49.990
from being asleep at the wheel.

711
00:43:51.130 --> 00:43:52.490
<v 6>Uh,
I mean,
I actually,</v>

712
00:43:52.990 --> 00:43:55.540
<v 1>maybe we should just,
we can talk more afterwards,</v>

713
00:43:55.541 --> 00:44:00.340
but everybody tends to ask first.
Hello.
Hi.

714
00:44:00.620 --> 00:44:00.820
Okay.

715
00:44:00.820 --> 00:44:05.800
<v 4>Um,
when you introduced the concept of when you introduce the Cila Mar Treaty,
uh,</v>

716
00:44:05.801 --> 00:44:10.690
you mentioned the difference between undirected intelligence and benevolent

717
00:44:10.691 --> 00:44:12.220
intelligence.
Um,

718
00:44:12.640 --> 00:44:17.020
don't you think that if human succeeded in creating controllable benevolent

719
00:44:17.021 --> 00:44:20.410
intelligence that they really have failed in creating intelligence?

720
00:44:24.890 --> 00:44:28.710
<v 1>Let me rephrase this question.
Do you want to just repeat the punchline?
I'll,</v>

721
00:44:28.711 --> 00:44:32.400
I'll rephrase.
Uh,
do you that benevolent

722
00:44:32.400 --> 00:44:37.230
<v 4>intelligence,
it would be the intelligence that we should strive towards,</v>

723
00:44:37.290 --> 00:44:41.910
or should it be general intelligence that in perhaps cannot be controlled?

724
00:44:42.900 --> 00:44:44.550
<v 1>So that's a great question.
You ask what I,</v>

725
00:44:44.790 --> 00:44:49.790
what I think I am trying to be very open minded about what we actually want

726
00:44:50.771 --> 00:44:54.810
them.
And I wrote the book
not really,

727
00:44:54.900 --> 00:44:56.130
really avoiding saying,

728
00:44:56.131 --> 00:45:00.950
well I think the future should be because I think,
um,

729
00:45:01.590 --> 00:45:04.650
this is such an important question.
We just need everybody's wisdom on it.

730
00:45:04.651 --> 00:45:08.070
And yet again,
I talk about all these different scenarios,

731
00:45:08.940 --> 00:45:11.880
some of which correspond to some of the different options or even listed there.

732
00:45:11.881 --> 00:45:16.230
And I'm incredibly interesting interested to hear what other people think would

733
00:45:16.231 --> 00:45:19.050
actually be good with these things.
One,
one,
um,

734
00:45:20.190 --> 00:45:21.300
one thing that may I,

735
00:45:21.301 --> 00:45:25.920
and I found very striking when we discuss this was when I was writing the book

736
00:45:25.921 --> 00:45:29.730
was even though I tried quite hard to emphasize the upsides of each scenario,

737
00:45:29.970 --> 00:45:34.410
there wasn't a single one there that I didn't have at least some major

738
00:45:34.411 --> 00:45:35.340
misgivings about.

739
00:45:38.770 --> 00:45:42.310
<v 4>Do you think deep neural networks will be the way to get to artificial general</v>

740
00:45:42.311 --> 00:45:43.990
intelligence?
If not,

741
00:45:44.020 --> 00:45:47.320
do you see fundamental reasons why these do not have the potential for recursive

742
00:45:47.321 --> 00:45:51.250
self improvement?
They can speed up the development of Agi or super indulgence.

743
00:45:51.440 --> 00:45:56.090
<v 1>All right.
That's a great question.
So I think that the,
although,
uh,</v>

744
00:45:56.760 --> 00:46:00.360
yeah,
let me say two things about this.
This is the,
first of all,

745
00:46:01.590 --> 00:46:06.210
our brain seems to be of course some kind of a of a recurrent neural network.

746
00:46:06.240 --> 00:46:10.920
It's very,
very complicated
and it has human level intelligence.

747
00:46:12.900 --> 00:46:17.430
But I think it would be a mistake to think that that's the only route there.

748
00:46:17.850 --> 00:46:20.880
I think you'd also be a mistake to think assuming that that's the fastest route

749
00:46:20.881 --> 00:46:24.960
there
may I like to point out that,
uh,

750
00:46:25.710 --> 00:46:29.250
even though finally a few years ago there was a beautiful tedtalk demonstrating

751
00:46:29.251 --> 00:46:34.230
the first ever successful mechanical bird that came a hundred years after the

752
00:46:34.231 --> 00:46:38.130
white with the Wright brothers built airplanes.
And when I flew here yesterday,

753
00:46:38.970 --> 00:46:40.170
you'll be very surprised to hear this,

754
00:46:40.171 --> 00:46:43.890
but I didn't come into mechanical bird and it turned out there was a much
easier,

755
00:46:43.950 --> 00:46:46.200
simpler way to build flying machines.

756
00:46:46.200 --> 00:46:49.260
And I think we're going to find exactly the same thing with human level

757
00:46:49.261 --> 00:46:53.580
intelligent machines.
The brain is just optimized for very different things.

758
00:46:53.581 --> 00:46:58.581
Then what your machines that you build are the brain is f a Darwinian evolution

759
00:46:59.731 --> 00:47:03.330
is obsessed about only building things that can self assemble.

760
00:47:04.110 --> 00:47:06.330
Who cares if your laptop can self assemble.

761
00:47:06.540 --> 00:47:09.740
The evolution is obsessed about creating things that can self repair.

762
00:47:09.870 --> 00:47:14.870
It would be nice if your laptop could self repair but it can't,

763
00:47:15.110 --> 00:47:17.140
then you're still using it.
So,
uh,

764
00:47:18.450 --> 00:47:23.450
and also evolution is doesn't care about simplicity for humans to understand how

765
00:47:24.361 --> 00:47:26.550
it works,
but you care a lot about that.

766
00:47:27.030 --> 00:47:30.010
So maybe this is much more complicated than it needs to be just so it can self

767
00:47:30.011 --> 00:47:34.630
assemble on blah,
blah,
whatever.
My guess is that the first human level,
yeah.

768
00:47:34.720 --> 00:47:36.280
I will not be

769
00:47:38.140 --> 00:47:39.880
working exactly like the brain that it will be something much,

770
00:47:39.881 --> 00:47:43.000
much simpler and maybe we'll use that the gray later and figure out how human

771
00:47:43.030 --> 00:47:45.430
human brains work.
Uh,
that said,

772
00:47:45.760 --> 00:47:49.090
the deep neural networks are of course inspired by the brain.

773
00:47:49.440 --> 00:47:53.320
Now you are using some efforts if there's some very clever or computational

774
00:47:53.321 --> 00:47:55.410
techniques that evolution came up with.
Um,

775
00:47:55.480 --> 00:47:57.910
my guess is that the fastest route to human level,

776
00:47:57.911 --> 00:48:02.911
I will actually use a combination of of deep neural networks with go five

777
00:48:03.580 --> 00:48:05.500
various good old fashioned AI techniques,

778
00:48:05.530 --> 00:48:09.260
more logic based things which have a lot of their own strength for billing,

779
00:48:09.261 --> 00:48:12.250
like for building a world model and and things like this.

780
00:48:15.030 --> 00:48:19.840
And maybe I should just add one more thing about this also this pose is the

781
00:48:19.940 --> 00:48:23.050
increasing successes of neural networks Paul suppose is a really interesting

782
00:48:23.051 --> 00:48:27.160
challenge because when we put AI in charge of more and more infrastructure in

783
00:48:27.161 --> 00:48:31.300
our world,
right?
It's really important that it be reliable and robust.

784
00:48:32.020 --> 00:48:34.760
Raise your hand if your computer has ever crashed on you.
All right.

785
00:48:35.140 --> 00:48:38.920
That wouldn't have been so fun if it was the the machine that was controlling

786
00:48:38.921 --> 00:48:43.090
your self driving car or your local nuclear power plant or your nation's nuclear

787
00:48:43.091 --> 00:48:45.130
power nuclear weapons system.
Right.

788
00:48:45.520 --> 00:48:50.520
And what we need to transform today's buggy and hackable computers and the

789
00:48:50.951 --> 00:48:54.280
robust AI systems that we can really trust.
What is trust?

790
00:48:54.310 --> 00:48:57.640
Where does trust come from?
It comes from understanding how things work.

791
00:48:58.240 --> 00:49:02.320
And neural networks I think are a double edge sword.
They are very powerful,

792
00:49:02.800 --> 00:49:05.920
but we understand the much less than than traditional software.

793
00:49:05.921 --> 00:49:08.050
So in my group at Mit,

794
00:49:08.051 --> 00:49:11.170
actually we're working very hard right now on a project that I call intelligible

795
00:49:11.171 --> 00:49:16.171
intelligence where we were trying to come up with algorithms where,

796
00:49:18.130 --> 00:49:21.950
where you can transform neural networks into things which you cannot really

797
00:49:21.970 --> 00:49:23.920
understand better how they work.
I think this is a,

798
00:49:24.190 --> 00:49:26.650
this is a challenge that I would encourage you to all think about too.

799
00:49:27.170 --> 00:49:30.760
How can you combine the power of neural nets with stuff that you can really

800
00:49:30.761 --> 00:49:33.340
understand better and therefore trust.

801
00:49:35.880 --> 00:49:38.940
<v 7>So,
uh,
should we be afraid that uh,</v>

802
00:49:38.990 --> 00:49:43.290
a Gi bill use it super intelligence to figure out that it's treatment by the

803
00:49:43.291 --> 00:49:47.370
humans is essentially slavery were just extra steps.

804
00:49:48.170 --> 00:49:50.420
<v 1>I,
that's,
well,
that's a wonderful,
wonderful question.</v>

805
00:49:50.480 --> 00:49:53.900
I haven't talked at all about consciousness here,
but um,

806
00:49:54.680 --> 00:49:57.140
the whole chapter eight in the book is about that.

807
00:49:57.630 --> 00:50:00.740
And a lot of people say things like,
well,

808
00:50:03.120 --> 00:50:07.140
machines can never have a subjective experience and feel anything at all because

809
00:50:07.530 --> 00:50:11.850
to feel something and you have to be made of excels or carbon atoms or whatever.

810
00:50:12.030 --> 00:50:14.940
As a scientist,
I really hate this kind of carbon chauvinism.

811
00:50:16.050 --> 00:50:19.740
I'm made of the same kind of up corks down quarks and electrons is all the

812
00:50:19.741 --> 00:50:22.320
computers are,
I'm didn't mind it.
Just arrange in a slightly different way.

813
00:50:22.350 --> 00:50:25.080
And it's obviously something about the information processing.

814
00:50:25.230 --> 00:50:29.240
That's all that matters,
right?
So,
uh,
we've,
and Pete,

815
00:50:29.241 --> 00:50:34.190
moreover this kind of a self self justifying arguments have been used by people

816
00:50:34.191 --> 00:50:35.390
throughout history.
The say,
oh,

817
00:50:35.391 --> 00:50:39.320
it's okay to torture slaves because they don't have souls.
I don't feel anything.

818
00:50:39.500 --> 00:50:41.920
Oh,
it's okay to torture chickens and,
and uh,

819
00:50:42.380 --> 00:50:46.760
today and giant factories because they don't feel anything.
You know,

820
00:50:47.060 --> 00:50:51.530
of course we're going to say that about our future computers too.
Uh,

821
00:50:51.800 --> 00:50:54.710
because it's convenient for us.
But that doesn't mean it's true.

822
00:50:55.100 --> 00:50:57.650
And I think it's actually really,
really interesting question.

823
00:50:58.340 --> 00:51:01.520
The first figure out what is it exactly that makes an information processing

824
00:51:01.521 --> 00:51:05.750
system have,
have subjective experience?
Uh,
a lot of my,

825
00:51:05.780 --> 00:51:10.580
my colleagues whom I really respect,
I think this is just bs,
this whole question.

826
00:51:11.150 --> 00:51:13.190
This is what the Daniel Dennett says.

827
00:51:13.570 --> 00:51:17.590
I look up at the Mcmillan dictionary of psychology and it's said the

828
00:51:17.610 --> 00:51:20.330
consciousness is something,
nothing worth reading he has ever been written on.

829
00:51:20.331 --> 00:51:21.500
But I really disagree with this.

830
00:51:21.501 --> 00:51:25.700
And actually let me just take one minute and explain why I think this is

831
00:51:25.701 --> 00:51:29.990
actually scientifically interesting question.
So let's look at this.

832
00:51:31.220 --> 00:51:31.850
Okay.

833
00:51:31.850 --> 00:51:36.850
And Ask yourself why is it that when I show you 450 nanometer light on the left

834
00:51:39.710 --> 00:51:42.170
and 650 nanometer light on the right,

835
00:51:42.440 --> 00:51:46.700
why do you subjectively experience it like this?
And not like this?

836
00:51:47.780 --> 00:51:52.780
Why I like this and not like this I put to you that this is a really fair game

837
00:51:53.631 --> 00:51:56.810
science question that we simply don't have an answer to right now.

838
00:51:57.290 --> 00:51:59.210
There's nothing to do with wavelengths of light.

839
00:51:59.211 --> 00:52:02.810
There are neurons or anything that explains this,
but it's an observational fact.

840
00:52:02.830 --> 00:52:05.440
Then I would like to understand and and why does it feel like anything?
Why,

841
00:52:05.441 --> 00:52:08.630
why do we have this experience?
You know,
you might say,
well look,

842
00:52:08.840 --> 00:52:12.200
we know that there are three kinds of light sensors in our retina,

843
00:52:12.230 --> 00:52:14.720
the cones and when I am with,
uh,

844
00:52:14.780 --> 00:52:18.920
with a four 59 meter light to activate one kind of when I have the longer

845
00:52:18.921 --> 00:52:20.390
wavelength to activate the other Cayenne.

846
00:52:20.391 --> 00:52:23.360
And then you can see how they're connected to various newer ones in the back of

847
00:52:23.361 --> 00:52:25.670
your brain.
But that just sharpens the question,

848
00:52:25.760 --> 00:52:29.330
the mystery of consciousness because this proves that it had nothing to do with

849
00:52:29.331 --> 00:52:33.560
light at all because you can experience colors even when you're dreaming,

850
00:52:34.430 --> 00:52:35.990
when different neurons,
new brain activity,
when we,

851
00:52:35.991 --> 00:52:40.340
when there is no light involved.
Right?
So my guess is that consciousness,

852
00:52:41.060 --> 00:52:44.750
by which I mean subjective experience is simply the way information fields when

853
00:52:44.751 --> 00:52:47.300
it's being processed in certain complex ways.

854
00:52:47.510 --> 00:52:51.170
And I think there are some equations that we will one day discover that specify

855
00:52:51.171 --> 00:52:55.280
what those complex ways are.
And once we can figure that out,

856
00:52:55.660 --> 00:52:59.000
it will both be very useful because we can put a consciousness detector and the

857
00:52:59.001 --> 00:53:02.720
emergency room and when an unresponsive patient comes in and you can figure out

858
00:53:02.721 --> 00:53:07.460
if they have locked in syndrome or not and it will also enable us to answer

859
00:53:07.461 --> 00:53:08.870
these.
This is really good question.

860
00:53:08.871 --> 00:53:13.700
You ask about whether machines or should also be viewed as moral entities that

861
00:53:13.701 --> 00:53:16.910
can cause they can have feelings and above all,

862
00:53:16.960 --> 00:53:19.040
and I don't see Ray Kurzweil here today,
but you know,

863
00:53:19.580 --> 00:53:24.580
if he can one day upload himself into the Ray Kurzweil robot then live on for

864
00:53:25.570 --> 00:53:29.610
for thousands of years and he talks like gray and it looks like gray and he acts

865
00:53:29.611 --> 00:53:33.370
like where you feel that that's great for Ray.
You know,
now he's a mortal but,

866
00:53:33.400 --> 00:53:37.920
but suppose suppose it turns out that that machine is just a Zombie and doesn't

867
00:53:38.010 --> 00:53:42.660
feel like anything to be it.
He would be pretty bummed wouldn't he?
All right.

868
00:53:42.860 --> 00:53:43.140
And,

869
00:53:43.140 --> 00:53:48.140
and if in the future life spreads throughout our cosmos in some post biological

870
00:53:49.171 --> 00:53:49.921
form and we were like,

871
00:53:49.921 --> 00:53:54.000
this is so excited and our descendants are doing all these great things and we

872
00:53:54.001 --> 00:53:57.300
can die happy if it turns out that they're all just a bunch of zombies and all

873
00:53:57.301 --> 00:54:01.470
that cool stuff is just the play for empty benches would not suck.

874
00:54:05.320 --> 00:54:06.670
<v 4>Oh,
do another question from the door.</v>

875
00:54:08.170 --> 00:54:11.530
What do you think is the most effective way for individuals to embrace a promote

876
00:54:11.880 --> 00:54:13.780
a security engineering mentality?

877
00:54:14.440 --> 00:54:18.760
I you were not even one glitches tolerable when working on AI related project.

878
00:54:23.280 --> 00:54:24.031
<v 1>Well,
first of all,</v>

879
00:54:24.031 --> 00:54:29.031
I think we have a lot to learn from existing successes and safety engineering.

880
00:54:29.821 --> 00:54:31.350
That's why I started by showing the moon mission.

881
00:54:31.580 --> 00:54:34.540
This is not like that this is anything new to engineers.

882
00:54:36.390 --> 00:54:40.470
I think it's just that we're so used to the idea that AI didn't work,

883
00:54:41.030 --> 00:54:45.310
that we didn't need to worry about,
uh,
the impact of things.
Um,
and um,

884
00:54:46.380 --> 00:54:50.070
and now it is beginning to have an impact.
So we should think it through.

885
00:54:50.640 --> 00:54:54.750
And then there are also a few challenges which are very unique and specific to

886
00:54:54.751 --> 00:54:59.440
AI.
Some of the Sylmar principals talk about them.
And,
and um,

887
00:55:00.510 --> 00:55:04.530
this research agenda for AI safety research is a really a long list of specifics

888
00:55:04.531 --> 00:55:06.540
that of safety engineering challenges that we need.

889
00:55:06.541 --> 00:55:10.770
Smart people like you to work on it.
And I hope we can support that.

890
00:55:12.030 --> 00:55:12.540
<v 0>Cool.</v>

891
00:55:12.540 --> 00:55:15.120
<v 4>So also on the topic of security engineering,</v>

892
00:55:15.250 --> 00:55:19.560
a lot of rockets blew up on the way to the moon.
Yeah.
And you know,

893
00:55:19.561 --> 00:55:21.000
given the intelligence explosion,

894
00:55:21.001 --> 00:55:23.790
it's like we're only gonna have one chance to be able to get the alignment

895
00:55:23.791 --> 00:55:25.980
problem correct.
And you know,

896
00:55:26.040 --> 00:55:28.890
I think we couldn't even align on a set of values in this room,

897
00:55:28.950 --> 00:55:32.250
let alone a system that would govern the world effectively.
Cause you know,

898
00:55:32.251 --> 00:55:36.090
there's certainly some drawbacks of capitalism.
So I'm hopeful.

899
00:55:36.091 --> 00:55:40.170
I am glad that Ilan is hedging our bets by making a magic hat.
But it seems like,

900
00:55:40.171 --> 00:55:43.080
you know,
you and your group are focusing on the alignment problem.
And I'm just,

901
00:55:43.410 --> 00:55:44.920
you know,
kind of just curious,
like,
you know,

902
00:55:44.940 --> 00:55:47.460
what makes you optimistic that we're going to be able to get it right on the

903
00:55:47.461 --> 00:55:48.294
first time.

904
00:55:49.050 --> 00:55:51.480
<v 1>So,
so first of all,
yeah,
a lot of rockets blew up.
But,</v>

905
00:55:51.481 --> 00:55:55.740
but you will note that most of the rockets that blew up,
in fact,
all our rock,

906
00:55:55.770 --> 00:55:59.400
that's the rock is that blew up in the moon mission,
had no people in them.
Right?

907
00:55:59.550 --> 00:56:02.220
So that would safety engineering,
the high risk stuff,

908
00:56:02.250 --> 00:56:05.310
they did it in a controlled environment where the failure didn't matter so much.

909
00:56:05.550 --> 00:56:08.820
So if you've made some really advanced AI,
you want understand it really well,

910
00:56:08.821 --> 00:56:13.230
maybe don't connect it to the Internet the first time.
Right?
So the down,

911
00:56:13.260 --> 00:56:18.120
so the downsides are small.
There's a lot of things like this that you can do.

912
00:56:18.450 --> 00:56:19.283
And um,

913
00:56:24.110 --> 00:56:26.930
I'm not saying that there's one thing that we should particularly focus on

914
00:56:26.931 --> 00:56:27.201
either.

915
00:56:27.201 --> 00:56:32.201
I think the community has brainstormed up really nice long list of things and we

916
00:56:32.391 --> 00:56:34.120
should really try to work on the mall.

917
00:56:34.200 --> 00:56:36.740
And then we will figure out some more challenges along the way.

918
00:56:36.890 --> 00:56:39.380
But that's the main thing we need to do is just all I just,
yeah,

919
00:56:39.381 --> 00:56:42.830
this is valuable.
Uh,
let,
let's,
let's work on it.

920
00:56:42.930 --> 00:56:45.920
And then you asked also why I'm optimistic.
Let me just clarify.

921
00:56:45.921 --> 00:56:49.400
There are two kinds of optimism.
There's naive optimism,

922
00:56:49.880 --> 00:56:53.810
like my optimism that the sun is gonna rise over mountain view tomorrow morning

923
00:56:54.140 --> 00:56:55.370
regardless of what we do.

924
00:56:56.120 --> 00:56:59.600
That's not the kind of optimism I feel about the future of technology.

925
00:57:00.380 --> 00:57:03.530
Then there's a kind of optimism that you're optimistic that this can go well if

926
00:57:03.531 --> 00:57:08.000
we really,
really plan and work for it.
That's the kind of optimism I feel here.

927
00:57:09.050 --> 00:57:12.600
We have in our hands.
It's to create an awesome future.
But,
uh,
let's,

928
00:57:12.850 --> 00:57:14.120
let's roll up our sleeves and do it.

929
00:57:23.400 --> 00:57:26.690
<v 8>Hey Max.
Um,
in the paper that you wrote entitled to,</v>

930
00:57:26.700 --> 00:57:29.970
why does cheap and deep learning works so well with Lynn and now we're all nick

931
00:57:29.971 --> 00:57:32.700
as well.
Um,
you ask the key question and you,

932
00:57:32.840 --> 00:57:35.520
you draw a lot of connections between,
you know,

933
00:57:35.730 --> 00:57:39.080
a deep learning and then core parts of what we know about physics,

934
00:57:39.081 --> 00:57:41.760
a low polynomial order,
I uncle processes,
things like that.

935
00:57:41.761 --> 00:57:44.730
I'm just curious what are the reactions you received both from the physics

936
00:57:44.731 --> 00:57:49.470
community and then from the Ai community to that attempt to kind of draw some

937
00:57:49.471 --> 00:57:50.370
deep parallels?

938
00:57:51.080 --> 00:57:55.540
<v 1>Uh,
just generally quite positive,
uh,
feedback.
Uh,</v>

939
00:57:56.350 --> 00:57:59.590
and then also people who are pointed out a lot of additional research questions

940
00:57:59.800 --> 00:58:01.870
related to that which are really worth doing.

941
00:58:02.050 --> 00:58:04.780
And just to bring everybody up to speed as the word we're talking about.

942
00:58:05.110 --> 00:58:10.050
Don't devolvement that this disgusting in Nordea research paper here.
Uh,
okay.

943
00:58:11.240 --> 00:58:12.073
<v 0>Okay.</v>

944
00:58:12.210 --> 00:58:15.990
<v 1>We were very intrigued by the question of why deep learning works so well.</v>

945
00:58:15.991 --> 00:58:18.570
Because if you,
if you think about it,
and I usually,
I,

946
00:58:18.880 --> 00:58:20.610
even if I just want to classify,

947
00:58:20.860 --> 00:58:24.690
I take all the Google images and I want to have cats and dogs and I want to

948
00:58:24.691 --> 00:58:28.710
write it and I want to take in New Orleans or cause they will take in say 1

949
00:58:28.710 --> 00:58:33.150
million pixels and output the probability that it's the cat,
right?

950
00:58:34.710 --> 00:58:36.000
If you think about it just a little bit,

951
00:58:36.001 --> 00:58:40.290
you might convince yourself that it's impossible because how many such images

952
00:58:40.291 --> 00:58:43.230
are there,
even if they're just black and white images.

953
00:58:43.231 --> 00:58:45.810
So each pixel can be black or white.

954
00:58:45.811 --> 00:58:49.560
There's two to the power of 1 million possible images are just much more images

955
00:58:49.830 --> 00:58:52.740
than there are atoms in our universe.
There's only a 10th of the seventh,

956
00:58:52.741 --> 00:58:57.180
the eighth right?
And for each image you have to output a probability.

957
00:58:57.810 --> 00:59:00.420
So at the specifying arbitrary function of images,

958
00:59:00.960 --> 00:59:02.630
how many parameters do you need for that?
Well,

959
00:59:03.140 --> 00:59:06.120
two to the 1000 would,

960
00:59:06.121 --> 00:59:09.310
you can't even fit if you store one parameter on each atom and in our cosmos.

961
00:59:09.311 --> 00:59:12.840
It's like how can it work so well and so it's always the basics wind solution we

962
00:59:12.841 --> 00:59:14.760
found there was was that the,

963
00:59:15.120 --> 00:59:18.630
of course the class of all functions that you can do well with a neural network

964
00:59:18.631 --> 00:59:19.680
that you can actually run.

965
00:59:20.100 --> 00:59:25.010
Is it almost infinitesimally tiny fraction of all functions but then physics

966
00:59:25.011 --> 00:59:29.690
tells us that the fraction of all functions that we actually care about because

967
00:59:29.691 --> 00:59:34.060
they're relevant to our world is also almost infinitesimally small fraction and

968
00:59:34.061 --> 00:59:37.610
then conveniently they're almost the same.
I don't think this was luck.

969
00:59:38.270 --> 00:59:41.210
I think Darwinian evolution Davis,

970
00:59:41.220 --> 00:59:45.050
this particular kind of neural network based computer precisely because it's

971
00:59:45.051 --> 00:59:49.490
really well tuned for tapping into the kind of computational needs that are

972
00:59:49.491 --> 00:59:52.140
called masters dished out to us and we,

973
00:59:52.220 --> 00:59:56.060
I'll be delighted to chat more with you later about about loose ends to this

974
00:59:56.090 --> 00:59:59.960
cause I think there's a lot more interesting stuff to be done on that

975
01:00:03.850 --> 01:00:08.220
<v 2>take story.
Being humans have the,</v>

976
01:00:08.221 --> 01:00:09.330
in the age of AI,

977
01:00:09.360 --> 01:00:13.110
seems like an egocentric effort that gives an undeserved special status to our

978
01:00:13.111 --> 01:00:13.944
species.

979
01:00:14.940 --> 01:00:18.270
Why should we even bother to remain humans when we could get to push our

980
01:00:18.271 --> 01:00:19.920
boundaries and see where we get

981
01:00:20.960 --> 01:00:21.793
<v 1>all right.</v>

982
01:00:22.830 --> 01:00:23.663
<v 0>Yeah.</v>

983
01:00:27.610 --> 01:00:29.050
<v 1>Egocentric efforts.</v>

984
01:00:29.051 --> 01:00:33.190
It gives us undeserved specialist status to our species.

985
01:00:36.990 --> 01:00:37.823
<v 0>Yeah.</v>

986
01:00:38.320 --> 01:00:39.460
<v 1>Well first of all,
you know,
I'm,</v>

987
01:00:39.461 --> 01:00:42.540
I'm totally fine with pushing our boundaries and I've,

988
01:00:42.550 --> 01:00:45.190
I've been advocating for doing this.

989
01:00:45.191 --> 01:00:50.191
I mean I find that very annoying human hubris when we go on a soapbox and we

990
01:00:51.721 --> 01:00:52.300
were like,
well,

991
01:00:52.300 --> 01:00:56.490
you are the pinnacle of creation and nothing can ever be smarter than us and we

992
01:00:56.500 --> 01:01:00.370
try not to build our whole self worth on somehow human exceptionalism.

993
01:01:00.610 --> 01:01:04.660
I think that's kind of lame.
Um,
on the,
on the other hand,

994
01:01:05.590 --> 01:01:08.830
we should probably make this the last questions.
And on the other hand,

995
01:01:09.800 --> 01:01:12.970
you're centric efforts.
Well,
we are the only ones that it's only us.

996
01:01:12.971 --> 01:01:17.020
Humans are in this conversation right now.
So,
and somebody needs to have it.

997
01:01:18.250 --> 01:01:20.830
So it's really us to us up to us to talk about it.
Right?

998
01:01:20.890 --> 01:01:24.910
We can't use this kind of thinking as an excuse to just not talk about it and

999
01:01:24.911 --> 01:01:28.450
just bumbled into some completely uncontrolled future art.

1000
01:01:28.630 --> 01:01:32.650
I think we should take a firm grip on the rudder and stare in whatever direction

1001
01:01:32.651 --> 01:01:37.570
we decided to steer steering.
So let me thank you again so much for coming out.

1002
01:01:37.571 --> 01:01:39.160
It's a wonderful pleasure to be here.

1003
01:01:42.750 --> 01:01:46.290
<v 0>And,
uh,
if you had any more questions,</v>

1004
01:01:46.510 --> 01:01:49.690
<v 1>I get in,
I'll be here signing books and I'm happy to chat more.</v>

1005
01:01:50.350 --> 01:01:53.200
<v 2>Thank you all for coming and thanks to Max for talking at Google</v>

1006
01:01:53.460 --> 01:01:54.430
<v 1>and thank you for having me.</v>

