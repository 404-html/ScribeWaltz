WEBVTT

1
00:00:08.810 --> 00:00:12.480
Um,
I am really delighted to be able to welcome our guest today,

2
00:00:13.230 --> 00:00:17.400
Dr. Lead Chata Polly who is both an MD and an mph.

3
00:00:17.800 --> 00:00:19.500
Uh,
doctor Chata.

4
00:00:19.501 --> 00:00:23.970
Polly is a pioneer I think intersection of two things that we all find very

5
00:00:23.971 --> 00:00:27.300
interesting here at Google,
which is artificial intelligence and healthcare.

6
00:00:27.930 --> 00:00:32.930
He's the Co founder and Cto of Kaiser Permanente's crest network where he's

7
00:00:33.541 --> 00:00:37.470
received the pioneer award from Kaiser Permanente innovations for his

8
00:00:37.471 --> 00:00:41.820
groundbreaking use of clinical decision support technologies.

9
00:00:42.810 --> 00:00:47.130
He's presented at many scientific meetings and industry events such as big data

10
00:00:47.131 --> 00:00:51.000
three 60 medical m e m.
S.
I'm going to say these all wrong,
uh,

11
00:00:51.001 --> 00:00:56.001
and sensors connected healthcare maxim integrated health technology forum and

12
00:00:56.311 --> 00:00:57.980
others.
Um,

13
00:00:57.990 --> 00:01:02.990
and he's an assistant clinical professor of medicine at UCF as well as a

14
00:01:03.031 --> 00:01:03.950
clinical researcher,

15
00:01:03.970 --> 00:01:08.580
the Kaiser Foundation Research Institute and has published articles in a variety

16
00:01:08.581 --> 00:01:12.880
of peer review journals.
He received his medical training from the Charles R.
Dot.

17
00:01:12.881 --> 00:01:17.881
Drew University of Medicine and science in Los Angeles and has his masters in

18
00:01:18.451 --> 00:01:20.850
public health from Ucla.
Uh,

19
00:01:20.851 --> 00:01:25.140
also we hope that you will keep an eye out for his forthcoming book,

20
00:01:25.141 --> 00:01:26.310
which is coming out this year,

21
00:01:26.311 --> 00:01:29.220
which we're hoping that he will tell you more about during this presentation.

22
00:01:29.221 --> 00:01:32.690
So a round of applause for Dr Chet Opale and

23
00:01:36.700 --> 00:01:37.533
<v 1>thank you.</v>

24
00:01:41.300 --> 00:01:46.130
<v 0>Thank you MS drought.
Uh,
and thanks for everybody who came in,</v>

25
00:01:46.220 --> 00:01:50.630
uh,
remotely and otherwise in this room.
Um,

26
00:01:50.870 --> 00:01:52.940
it's a pleasure to be here.
Uh,

27
00:01:52.970 --> 00:01:57.820
a always bought it to come here and talk about a little bit about healthcare,

28
00:01:58.910 --> 00:02:02.390
um,
because the technology piece you already,
you guys already know.

29
00:02:02.750 --> 00:02:06.050
So I'm not going to touch too much about the technology side.

30
00:02:06.470 --> 00:02:11.470
I'm more about how healthcare has changed and how technology can help,

31
00:02:13.580 --> 00:02:14.413
um,

32
00:02:14.540 --> 00:02:19.540
get better at saving patients' lives and also hopefully protecting physicians

33
00:02:21.861 --> 00:02:26.450
from burnout and the all the difficult situations they're in right now.

34
00:02:27.750 --> 00:02:31.640
Um,
as,
uh,
describe,
uh,
this drawl.

35
00:02:31.670 --> 00:02:34.370
My name is Uli Cheddar Polly A.

36
00:02:34.371 --> 00:02:39.371
I'm a physician and a researcher and as a scientist at a Kaiser Permanente,

37
00:02:40.060 --> 00:02:43.670
uh,
I see patients in the emergency department,
a,

38
00:02:43.671 --> 00:02:47.780
that's my main profession.
Uh,

39
00:02:47.960 --> 00:02:50.090
but I also do research.
Um,

40
00:02:50.180 --> 00:02:54.590
one of the key things that I found was that,
you know,

41
00:02:54.591 --> 00:02:59.120
when you're seeing patients,
you can make a difference in one person,
one patient,

42
00:02:59.230 --> 00:03:02.860
a time.
When you work in research and technology,

43
00:03:02.861 --> 00:03:07.840
you can make a difference and hundreds and thousands and probably millions of

44
00:03:07.841 --> 00:03:11.950
patients.
And so that's what gets me really excited about,
uh,

45
00:03:12.460 --> 00:03:16.420
about working in research and technology.
Um,

46
00:03:17.140 --> 00:03:20.490
my group,
uh,
crest network is a,
uh,

47
00:03:20.550 --> 00:03:24.970
a group of 12 or 13 physician scientists.

48
00:03:25.450 --> 00:03:30.450
All of us are trained in emergency medicine and all of us are researchers.

49
00:03:31.480 --> 00:03:35.290
So we see patients and also we,
um,
you know,

50
00:03:35.291 --> 00:03:38.230
we do research on,
on the things that interest us.

51
00:03:39.700 --> 00:03:44.020
We have a staff of about four or five people and criteria,

52
00:03:44.021 --> 00:03:48.610
division of research in Oakland and a,
we tackle some of the big,

53
00:03:48.730 --> 00:03:51.640
big ticket items like stroke,
heart attacks,

54
00:03:52.480 --> 00:03:56.560
and a sepsis and things like that.
Um,

55
00:03:56.590 --> 00:04:01.590
so I'm one of the cofounders of that group and also I help design the technology

56
00:04:01.751 --> 00:04:04.650
for that.
Let's see.
I,

57
00:04:04.651 --> 00:04:07.920
I teach medical students and residents at UCF.

58
00:04:08.320 --> 00:04:11.530
They come to us from,
um,
uh,

59
00:04:11.560 --> 00:04:14.890
from their medical for their medical rotations in the emergency department.

60
00:04:15.640 --> 00:04:19.810
And I also run a group called the society of physician entrepreneurs.

61
00:04:19.811 --> 00:04:24.300
It's a large,
um,
uh,

62
00:04:24.650 --> 00:04:27.070
uh,
group of,
uh,
entrepreneurs,

63
00:04:27.310 --> 00:04:31.540
healthcare entrepreneurs who are interested in changing healthcare.
Um,

64
00:04:31.960 --> 00:04:36.960
I run the San Francisco Bay area chapter and we meet every third Thursday.

65
00:04:37.661 --> 00:04:41.000
And so all of you are welcome.
It's open to everybody.
Uh,

66
00:04:41.050 --> 00:04:44.380
you can check out the invitation on Linkedin.

67
00:04:46.120 --> 00:04:48.370
Um,
let's see,

68
00:04:50.800 --> 00:04:55.150
disclosures.
Uh,
I have nothing to disclose right now.

69
00:04:55.151 --> 00:04:58.360
I'm employed by Kaiser Permanente and I do research,

70
00:04:58.750 --> 00:05:03.700
but the views that I'm expressing here are totally mine.
And,
uh,
you know,

71
00:05:03.701 --> 00:05:08.500
it doesn't,
um,
uh,
I,
I don't get any industry money or anything like that.

72
00:05:10.750 --> 00:05:15.710
Um,
when I,
when I was thinking about doing this talk,
um,

73
00:05:15.850 --> 00:05:18.130
I know you guys know technology very well,

74
00:05:18.490 --> 00:05:21.940
but I thought I would give you a little brief introduction to healthcare in the

75
00:05:22.000 --> 00:05:26.630
United States,
uh,
and how it has changed over time.
Um,
and,

76
00:05:26.810 --> 00:05:28.640
and what are the challenges now and,

77
00:05:28.641 --> 00:05:31.720
and how to deal with some of those challenges.

78
00:05:32.620 --> 00:05:37.030
So I'm going to divide my talk into three areas.
Um,
uh,

79
00:05:37.240 --> 00:05:40.630
three Eyras,
uh,
the golden era where,
you know,

80
00:05:40.750 --> 00:05:42.790
but a hundred years ago or so,

81
00:05:43.180 --> 00:05:46.890
when a science medical science started,
uh,

82
00:05:47.140 --> 00:05:52.090
revealing to us the secrets of disease and how disease happens and how to

83
00:05:52.091 --> 00:05:55.410
prevent it.
And I called the second era.

84
00:05:55.411 --> 00:06:00.411
I call the Steel Stelara era is where you're looking at industrialization of,

85
00:06:01.581 --> 00:06:05.360
uh,
of,
uh,
countries and a huge growth,

86
00:06:05.840 --> 00:06:10.790
um,
of,
of large companies,
large industries,
uh,

87
00:06:10.910 --> 00:06:15.220
large hospital systems,
uh,
diagnostic,
uh,

88
00:06:15.290 --> 00:06:17.180
abilities and surgeries.

89
00:06:17.540 --> 00:06:21.470
So everything that has to do with the know building large things,

90
00:06:21.530 --> 00:06:26.270
and that's the steel era that we talk about,
uh,
today.

91
00:06:26.840 --> 00:06:31.670
And then a comes to the digital era that more or more recent times,
you know,

92
00:06:31.671 --> 00:06:35.810
how,
how digitization of medicine,
uh,

93
00:06:36.080 --> 00:06:38.990
has changed and will change in the future.

94
00:06:39.080 --> 00:06:42.980
So that's talking more about the future and how technology can help.

95
00:06:43.940 --> 00:06:48.740
Um,
the golden era,
this is,
you know,

96
00:06:48.800 --> 00:06:53.540
probably how,
you know,
1850s or so,
it started around that time where,
you know,

97
00:06:53.570 --> 00:06:58.570
the germ theory came about where we're scientists figured out that germs are the

98
00:06:59.511 --> 00:07:03.710
ones that cause disease and how to treat them,

99
00:07:03.740 --> 00:07:08.050
how to,
uh,
you know,
kill them with antibiotics and,

100
00:07:08.320 --> 00:07:13.320
and a lot of causes of diseases were found in that I would say about 100 years,

101
00:07:14.180 --> 00:07:18.410
uh,
timeframe.
And I have a lot of my heroes,

102
00:07:18.760 --> 00:07:22.370
um,
you know,
being an epidemiologist by training.

103
00:07:22.430 --> 00:07:24.890
A lot of my heroes are from this era.

104
00:07:25.460 --> 00:07:28.350
So I'll talk about a few of those.
Uh,

105
00:07:28.520 --> 00:07:31.490
first one that comes to my mind is John Snow.

106
00:07:31.550 --> 00:07:35.150
He was a physician and in,
uh,
England.

107
00:07:36.320 --> 00:07:40.520
And uh,
during the 1950s,
you know,
there was a,
a bad,

108
00:07:40.521 --> 00:07:44.780
a cholera epidemic in,
in,
uh,
in,
in England.

109
00:07:45.230 --> 00:07:49.870
And so he was the one who started investigating this.
Um,

110
00:07:50.270 --> 00:07:54.980
he was one of the earliest scientists or data scientists I could say in

111
00:07:55.110 --> 00:07:55.910
healthcare.

112
00:07:55.910 --> 00:08:00.910
So what he did was he took a map and started putting dots on where people were

113
00:08:04.071 --> 00:08:06.590
dying of cholera.
And so this is a map.

114
00:08:06.591 --> 00:08:10.460
This is the original map that he worked on and you can see those black dots

115
00:08:10.740 --> 00:08:14.270
along the streets.
And those are the places where people died.

116
00:08:15.080 --> 00:08:18.920
And then what he did was he looked at one area,

117
00:08:19.490 --> 00:08:24.490
which was a very dense area where there were a lot of deaths happening and

118
00:08:25.971 --> 00:08:30.050
that's,
that's where that's on broad street.
Um,

119
00:08:30.230 --> 00:08:33.620
and then he figured out that there was a pump,

120
00:08:33.710 --> 00:08:37.370
a water pump called the broad street pump,

121
00:08:38.420 --> 00:08:43.420
which he felt that somehow that water coming from that pump was the one that is

122
00:08:45.291 --> 00:08:49.440
causing this disease.
And so he put out this theory first,
uh,

123
00:08:49.450 --> 00:08:54.200
until that time people were thinking that the air is what causes cholera.

124
00:08:54.410 --> 00:08:59.410
And so eventually what he did was he removed the handle on the pump.

125
00:08:59.550 --> 00:09:03.720
You know,
these pumps had handles where you,
you have to pump to get water.

126
00:09:04.200 --> 00:09:08.190
He removed the handle on that pump and lo and behold,

127
00:09:08.650 --> 00:09:11.710
cholera was a declining.
Um,

128
00:09:12.150 --> 00:09:14.950
and then later on they found out that it was the,

129
00:09:15.140 --> 00:09:19.230
the source of water that was contaminated and that was causing cholera.

130
00:09:19.680 --> 00:09:24.210
And a,
that pump still is up there in,
in England,

131
00:09:25.320 --> 00:09:29.910
in London.
Um,
the second one,
the,
I'm sure you know this person,

132
00:09:29.940 --> 00:09:34.170
Florence Nightingale,
she was a duchess.

133
00:09:35.260 --> 00:09:38.310
I actually,
they call her Dutchie.
Um,

134
00:09:38.580 --> 00:09:42.260
she was a daughter of a rich person in England.
Again,
um,

135
00:09:42.870 --> 00:09:45.460
she was good at mathematics,
um,

136
00:09:45.630 --> 00:09:49.800
another data scientist and she went and worked with,
um,

137
00:09:50.940 --> 00:09:55.350
with the,
with the wounded,
uh,
wetlands,
uh,
during the war.

138
00:09:55.650 --> 00:10:00.240
And this was in the 1950s also.
And what she did was she did,
uh,

139
00:10:00.960 --> 00:10:05.960
enumerate the causes of death in these soldiers and you have this and she

140
00:10:07.381 --> 00:10:12.330
prepared this beautiful graphs or you know,
this is one of the first,
I guess,

141
00:10:12.331 --> 00:10:15.540
infographics,
uh,
where she described.

142
00:10:15.750 --> 00:10:20.700
So each of those slices is a month's time.
And,

143
00:10:20.701 --> 00:10:21.990
uh,
the,
the,

144
00:10:21.991 --> 00:10:26.991
the gray area or the blue area as she describes it as preventable deaths.

145
00:10:27.690 --> 00:10:32.690
The black area is where a other causes are unknown causes.

146
00:10:33.300 --> 00:10:38.300
And the pink areas are the red areas in her description or the debts from the

147
00:10:38.731 --> 00:10:42.720
wounds from bleeding.
Um,
and so she,
she,

148
00:10:43.390 --> 00:10:47.970
and remember this was even before there was antibiotics are,

149
00:10:47.971 --> 00:10:51.570
there was the germ theory was it wasn't even published yet.

150
00:10:52.260 --> 00:10:54.100
So she,
yeah,

151
00:10:54.720 --> 00:10:58.570
just by observation she was able to figure out a lot of things.
Uh,

152
00:10:58.590 --> 00:10:59.970
using data science.

153
00:11:01.500 --> 00:11:05.460
Luther Terry was a physician in the United States.
Uh,
and,
uh,

154
00:11:05.490 --> 00:11:08.680
in the JFK era,
uh,

155
00:11:08.681 --> 00:11:12.420
he was the surgeon general of the United States.

156
00:11:13.530 --> 00:11:17.290
He had a tough choice of,
uh,
of,
uh,

157
00:11:17.370 --> 00:11:21.870
putting out a warning that you see on the cigarette labels.

158
00:11:22.380 --> 00:11:26.280
And this was the first time that,
uh,
people,
uh,

159
00:11:26.281 --> 00:11:29.250
realize that cigarette smoking was injurious to health.

160
00:11:29.730 --> 00:11:34.320
And that was the warning that,
uh,
he,
he's the one that is,
um,
uh,

161
00:11:34.350 --> 00:11:35.183
um,

162
00:11:35.520 --> 00:11:39.690
the person behind this committee that sat together and looked at all the

163
00:11:39.691 --> 00:11:42.720
evidence coming from,
from the public health data.

164
00:11:43.590 --> 00:11:47.310
And as soon they realized that,
wow,
you know,
we are all smoking.

165
00:11:47.310 --> 00:11:51.840
And probably 75% of those physicians,
uh,

166
00:11:51.841 --> 00:11:53.590
including Terry was smoker.

167
00:11:53.680 --> 00:11:57.730
And imagine how hard it must have been trying to look at that evidence.

168
00:11:57.731 --> 00:12:02.200
And we shows that,
uh,
smoking is causing cancer and other diseases.

169
00:12:02.590 --> 00:12:07.420
And so he's,
he was one of my heroes.
Um,
Thomas Dobber,

170
00:12:07.450 --> 00:12:11.180
uh,
was another physician who is um,

171
00:12:12.310 --> 00:12:14.660
uh,
who was the lead,
uh,

172
00:12:14.870 --> 00:12:18.940
investigator in the Framingham study.
Framingham is a,

173
00:12:18.941 --> 00:12:22.810
is a town in Massachusetts.
And so in the 50s,

174
00:12:22.811 --> 00:12:27.250
what they did was they picked this town of an enrolled all the people in the

175
00:12:27.251 --> 00:12:30.550
town,
all the adults in the town of about 5,000 of them.

176
00:12:30.940 --> 00:12:34.990
And then track their health histories,
track their diet,

177
00:12:34.991 --> 00:12:39.190
track their activity,
track their blood tests,
blood pressures and everything.

178
00:12:39.730 --> 00:12:41.270
And um,
Mo,

179
00:12:42.400 --> 00:12:46.540
there were about a thousand papers that came out of that study and they studied

180
00:12:46.541 --> 00:12:50.920
for years and years and years and they're still studying those,

181
00:12:51.430 --> 00:12:54.370
that population,
which is now in the third generation.

182
00:12:54.730 --> 00:12:59.730
And so this is where we got a lot of the good data about what causes heart

183
00:13:00.551 --> 00:13:03.640
disease and these are the risk factors that they found,

184
00:13:04.180 --> 00:13:06.220
which we still practice today.

185
00:13:06.720 --> 00:13:11.520
So those were all the great things that happened in those days.
Um,

186
00:13:11.560 --> 00:13:15.190
that changed the lives of millions of people.

187
00:13:15.400 --> 00:13:20.040
That actually saved a lot of people to,
uh,

188
00:13:20.530 --> 00:13:24.910
coming to the steel era.
This is the industrialized,
uh,
nations,

189
00:13:25.360 --> 00:13:27.850
uh,
starting anywhere from the 50s,

190
00:13:27.851 --> 00:13:32.000
sixties to a 19,
well,
2000,

191
00:13:32.230 --> 00:13:34.900
early two thousands.
So this is where,
you know,
you are,

192
00:13:35.080 --> 00:13:38.440
everything is growing in a big hospitals,
big hospital systems,

193
00:13:38.740 --> 00:13:41.830
electronic health records were introduced.
Um,

194
00:13:42.130 --> 00:13:46.290
and they were doing all kinds of surgeries,
saving,
saving lives and,

195
00:13:46.291 --> 00:13:51.040
and a very complicated procedures and everything blossomed.

196
00:13:51.070 --> 00:13:55.660
And healthcare became the,
became this huge,
huge industry.

197
00:13:56.200 --> 00:13:59.650
Um,
uh,
the image that is all great.
Uh,

198
00:13:59.651 --> 00:14:04.480
except that it was a,
uh,
it was getting out of hand.

199
00:14:05.110 --> 00:14:09.010
And so I'll show you some of the things I think

200
00:14:10.810 --> 00:14:13.300
based on my 25 years of experience,

201
00:14:13.630 --> 00:14:18.630
50% of what we do in the clinics in the u s he's a total waste of money and

202
00:14:20.741 --> 00:14:25.690
effort.
It's unnecessary,
ineffective or dangerous.
Now you must be thinking,

203
00:14:26.740 --> 00:14:31.120
you know,
looking at that data,
you know,
you know,
I,
I don't believe it.

204
00:14:31.690 --> 00:14:33.070
I don't think that's true.

205
00:14:34.420 --> 00:14:37.030
I don't think this guy has 25 years of experience.

206
00:14:38.950 --> 00:14:42.390
But it's true.
I do have 25 years of experience and I,

207
00:14:42.540 --> 00:14:46.900
I think it is a 50% of it is,
is um,
unnecessary.

208
00:14:47.110 --> 00:14:51.970
So you must be a,
you know,
when you go to your doctor,
I'm sure you be thinking,

209
00:14:52.130 --> 00:14:54.710
wow,
what part of this 50%,

210
00:14:55.010 --> 00:15:00.010
what part of this care is not necessary or dangerous?

211
00:15:00.590 --> 00:15:05.480
So I'll go over a few things to show you what is happening in the industry right

212
00:15:05.481 --> 00:15:10.100
now.
Um,
this is,
uh,
uh,
from,
um,

213
00:15:10.430 --> 00:15:14.540
Harvard business review.
The population is aging,
right?

214
00:15:15.140 --> 00:15:20.140
So one of the problems with aging population is that the cost of care goes up.

215
00:15:21.560 --> 00:15:23.030
You know,
if you're a 30 year old,

216
00:15:23.210 --> 00:15:26.600
maybe it'll cost about $5,000 a year to take care of you.

217
00:15:27.320 --> 00:15:31.970
But if you're a 65 year old,
you'll cost $20,000.

218
00:15:32.390 --> 00:15:34.310
And so if everybody is getting older,

219
00:15:34.311 --> 00:15:37.820
that means the cost of care we'll get higher and higher.

220
00:15:39.330 --> 00:15:43.880
The,
this other graph I love because it shows the,

221
00:15:43.881 --> 00:15:47.450
the largest economies in the world.
You know,
the duck tall,

222
00:15:47.451 --> 00:15:50.950
big blue one is the United States.
Uh,

223
00:15:51.170 --> 00:15:52.610
and as you guessed right,

224
00:15:52.780 --> 00:15:57.680
the second orange one is China and then comes Japan and the gray one and the

225
00:15:57.681 --> 00:15:59.300
yellow one is Germany.

226
00:15:59.840 --> 00:16:04.400
And then of course UK and France and other countries follow.

227
00:16:05.900 --> 00:16:10.220
Now healthcare,
if you look at it,
it'll fit right there.

228
00:16:10.250 --> 00:16:14.420
The US health care,
that's the red one in the middle.
Um,

229
00:16:16.370 --> 00:16:20.120
we spend that much money on healthcare every year.

230
00:16:20.480 --> 00:16:25.480
We spend almost as much as the whole economy of Germany.

231
00:16:27.290 --> 00:16:30.080
And you know how big that is.
That is the fourth country,

232
00:16:30.710 --> 00:16:33.500
fourth biggest country in the world,
um,

233
00:16:33.680 --> 00:16:36.050
as far as a GDP is concerned.

234
00:16:36.740 --> 00:16:40.760
And so that is what I'm talking about when I talk about the cost of healthcare.

235
00:16:41.510 --> 00:16:46.280
This is a consumer price index index of various things that we buy.

236
00:16:46.950 --> 00:16:50.120
Um,
the ones in the bottom are in a clothing,

237
00:16:50.121 --> 00:16:53.390
food and other transportation,
things like that.

238
00:16:53.750 --> 00:16:55.790
The purple one at the top is healthcare.

239
00:16:56.540 --> 00:16:59.360
Healthcare costs keep going up and up and up.

240
00:17:01.060 --> 00:17:03.980
This is another graph talks about quality and uh,

241
00:17:04.460 --> 00:17:07.790
and spending and healthcare is right there.

242
00:17:08.420 --> 00:17:13.280
All the other industrialized countries in developed countries are up,
you know,

243
00:17:14.170 --> 00:17:18.710
have spend less and have a better performance or better quality of care.

244
00:17:19.100 --> 00:17:22.790
We spend more and our quality of care is less

245
00:17:25.580 --> 00:17:30.110
and the knowledge keeps growing.
It's not like we don't know what,
you know,

246
00:17:30.170 --> 00:17:33.560
the science behind these diseases,
you know,

247
00:17:33.710 --> 00:17:36.470
these are the number of studies that uh,

248
00:17:36.530 --> 00:17:39.710
the clinical trials that happen in the u s and that keeps going up.

249
00:17:40.100 --> 00:17:44.600
And the 2017 number is only up till August.

250
00:17:44.840 --> 00:17:49.840
So actually 2018 is probably shooting up out of the,

251
00:17:50.250 --> 00:17:54.510
out of this screen.
Physicians are facing,

252
00:17:54.920 --> 00:17:57.900
had a difficult time.
Um,

253
00:17:58.770 --> 00:18:01.900
mainly because most of the,
the,

254
00:18:02.810 --> 00:18:07.810
the compensation comes from a fee for service world and so they are worried

255
00:18:08.461 --> 00:18:12.450
about their reimbursement.
That means they have to work harder,
see more patients,

256
00:18:12.690 --> 00:18:15.810
do more things to make the same amount of money.

257
00:18:15.811 --> 00:18:20.370
And then of course there's a lot of pressure from the,
uh,

258
00:18:20.400 --> 00:18:22.800
from the payers or the insurance companies.

259
00:18:23.610 --> 00:18:25.770
The regulations are getting more complex.

260
00:18:25.800 --> 00:18:28.920
That means they have to do more and more check more boxes.

261
00:18:28.921 --> 00:18:32.720
And same thing with medical legal risk,
uh,

262
00:18:32.730 --> 00:18:36.000
where they're worried about getting sued.

263
00:18:36.001 --> 00:18:38.780
So they order more tests so that,
you know,

264
00:18:38.781 --> 00:18:42.460
there are covered in case something goes wrong and then,
you know,

265
00:18:42.600 --> 00:18:47.600
that increases their workload and their effort and their stress.

266
00:18:50.250 --> 00:18:54.690
Uh,
which ultimately saves that,
you know,
this is a survey from medscape,

267
00:18:54.780 --> 00:18:56.820
um,
uh,

268
00:18:56.850 --> 00:18:59.910
which shows that 51% of physicians,

269
00:19:00.270 --> 00:19:05.070
they complain about burnout.
That's a sad thing if,

270
00:19:05.071 --> 00:19:09.330
uh,
if half of our physicians are not feeling good about their jobs,

271
00:19:09.360 --> 00:19:12.960
about their workload,
um,

272
00:19:13.140 --> 00:19:17.070
on the technology side as you have no,
uh,
as you know,
um,
you know,

273
00:19:17.071 --> 00:19:18.330
data availability,
you know,

274
00:19:18.331 --> 00:19:22.200
there's a lot of data being collected at thousand fold increase,

275
00:19:22.590 --> 00:19:26.580
hardware speeds are increasing and the algorithms are getting better and better.

276
00:19:28.620 --> 00:19:32.430
So now we come to the current age,

277
00:19:32.431 --> 00:19:36.630
which is the digital era.
Um,
this is where I,

278
00:19:37.110 --> 00:19:42.030
I will talk about some of the solutions are how to tackle these problems that we

279
00:19:42.031 --> 00:19:46.440
have seen that have grown out of the industrial era or the steel era.

280
00:19:48.430 --> 00:19:50.370
Um,
now we have a,
you know,

281
00:19:50.371 --> 00:19:54.590
all 80% of the medical records are digitized,
uh,
are,
you know,

282
00:19:54.610 --> 00:19:58.200
he had charged are pretty,
um,
prevalent.

283
00:19:59.250 --> 00:20:03.510
Uh,
hopefully the efficiency will get better and the quality will get better.

284
00:20:04.050 --> 00:20:08.640
And of course,
artificial intelligence seems to be,
uh,

285
00:20:08.760 --> 00:20:11.870
getting inroads into healthcare and which seems,
which,

286
00:20:11.880 --> 00:20:15.510
which looks like it promising era.
Um,

287
00:20:16.110 --> 00:20:17.460
within my research,
I,
you know,

288
00:20:17.461 --> 00:20:22.410
I do work with several groups here locally and nationally.
Um,

289
00:20:22.440 --> 00:20:25.120
but we know that this is where,
uh,

290
00:20:25.200 --> 00:20:27.780
I think the biggest bang for the buck lice.

291
00:20:30.150 --> 00:20:33.000
Let me share a story with you.
Um,
you know,

292
00:20:33.001 --> 00:20:37.560
that that kind of portrays,
uh,
current healthcare situation.

293
00:20:38.860 --> 00:20:39.693
Um,

294
00:20:40.620 --> 00:20:45.270
there was a town which has a river flowing through a small town and there's a

295
00:20:45.271 --> 00:20:47.410
bridge there.
One day,

296
00:20:47.411 --> 00:20:52.411
one person was walking across the bridge and then saw a child floating in the

297
00:20:53.741 --> 00:20:57.300
water.
Actually the child was trying to swim,

298
00:20:57.320 --> 00:21:00.430
it could not swim and kind of floating away in the river.

299
00:21:00.940 --> 00:21:05.440
And so he tries to put his hand and puts us in a,

300
00:21:05.480 --> 00:21:08.290
throws a rope and then saves the child,
brings him out.

301
00:21:08.620 --> 00:21:13.620
And then soon after another child is coming along the river and then he saves

302
00:21:15.221 --> 00:21:20.040
that child too.
And then more and more people are coming,
you know,

303
00:21:20.041 --> 00:21:25.041
as a trying to survive this flood of water that is going under the bridge and

304
00:21:26.981 --> 00:21:30.520
soon enough he calls his friends and everybody is trying to save these people

305
00:21:30.550 --> 00:21:33.340
out of the water.
Soon they set up some tents,

306
00:21:33.850 --> 00:21:37.840
some fire and some food and everything and then,

307
00:21:38.300 --> 00:21:41.560
and they realize that there's more people coming.
And so this,

308
00:21:41.860 --> 00:21:46.860
they set up a nice building hotels and the build all this infrastructure around

309
00:21:47.560 --> 00:21:52.560
that bridge because they see people floating.
Um,

310
00:21:52.630 --> 00:21:57.630
and some of them are dying and so one wise person comes by and says,

311
00:21:58.780 --> 00:22:01.150
Hey,
do you guys know where are these people are coming from?

312
00:22:01.570 --> 00:22:05.830
Maybe we should stop,
go there and stop that.
That,

313
00:22:07.120 --> 00:22:12.120
that thought of preventing disease and illness and depth is what I'm talking

314
00:22:12.971 --> 00:22:16.990
about when I talk about,
you know,
how technology can help.

315
00:22:18.400 --> 00:22:22.870
If you look at the,
you know,
healthcare industry in general,
it's very complex.

316
00:22:22.871 --> 00:22:25.330
You know that you have the physicians on one side and the patients,

317
00:22:25.570 --> 00:22:30.100
which are the two most important pieces,
uh,
in healthcare.

318
00:22:30.550 --> 00:22:35.020
But then you have the Pharma,
the divide medical devices,
the academia,

319
00:22:36.100 --> 00:22:39.790
regulators,
malpractice in and payers.
Right?

320
00:22:40.090 --> 00:22:45.090
But each of those groups have something else in their minds.

321
00:22:47.200 --> 00:22:50.920
You know,
the farmer,
you know,
they are looking at getting their FDA approval.

322
00:22:50.921 --> 00:22:55.840
They're looking at selling these drugs and eventually making profits,

323
00:22:55.841 --> 00:22:56.740
which is a good thing.

324
00:22:57.230 --> 00:23:02.140
The same thing with devices and in academia you see doctors,
you know,

325
00:23:02.141 --> 00:23:06.300
trying to publish papers because if you don't publish,
you know,
you,
you,

326
00:23:06.301 --> 00:23:08.320
you don't survive.
Um,

327
00:23:08.980 --> 00:23:13.980
but one of the big things that you realize is that well is their primary goal to

328
00:23:15.221 --> 00:23:19.410
help patients.
Uh,
same thing with regulators.
They're trying to,
you know,

329
00:23:20.740 --> 00:23:25.000
control the quality and a malpractice.
Same thing.

330
00:23:25.001 --> 00:23:29.170
The lawyers are trying to control the malpractice so that we can read out the

331
00:23:29.171 --> 00:23:33.850
bad doctors,
um,
insurance companies trying to hold down costs.

332
00:23:34.660 --> 00:23:37.360
But then in the mix,
you know,

333
00:23:37.361 --> 00:23:40.330
the physicians and patients are getting lost.
You know,

334
00:23:40.331 --> 00:23:45.331
they are not the main focus of all these other institutions that have grown

335
00:23:45.470 --> 00:23:47.660
around this health care.

336
00:23:49.970 --> 00:23:54.560
All physicians want is to be able to take care of patients and then hopefully

337
00:23:55.280 --> 00:23:59.180
decrease all these other burdens that come in the way.

338
00:24:01.010 --> 00:24:05.480
So one of the best ways is now I'm talking about solutions and how to,

339
00:24:05.810 --> 00:24:07.510
how to steer this,
uh,

340
00:24:07.720 --> 00:24:12.530
this wave of increased costs and decreased quality.

341
00:24:13.520 --> 00:24:14.353
Number one,

342
00:24:15.320 --> 00:24:19.700
I think the business model has to change and it's already changing in healthcare

343
00:24:20.090 --> 00:24:22.700
from volume based care to value based care.

344
00:24:23.810 --> 00:24:27.350
So when you talk about volume based care is,
you know,

345
00:24:27.351 --> 00:24:31.700
the more procedures you do,
the more money you make,
the more operations,

346
00:24:31.701 --> 00:24:34.250
the more drugs,
the more hospitalizations.

347
00:24:34.670 --> 00:24:39.670
I think it has to be flipped where when you keep the patients healthy,

348
00:24:39.891 --> 00:24:43.340
that's when you make money.
And so,
and that's,

349
00:24:43.341 --> 00:24:46.400
which is already happening and,
and that has to happen.

350
00:24:47.870 --> 00:24:52.790
The second thing is that we have to go upstream.
When I say upstream,

351
00:24:53.350 --> 00:24:54.183
you know we,

352
00:24:54.200 --> 00:24:59.200
we don't want to spend too much effort on people who are really,

353
00:25:01.251 --> 00:25:05.780
really sick but tried to see how we can prevent the sickness even before it

354
00:25:05.781 --> 00:25:09.170
happens.
So you have to look at not just the hospital,

355
00:25:09.410 --> 00:25:13.640
but you have to look at the environment,
you know,
the home,
the,
the,
the,

356
00:25:13.641 --> 00:25:16.340
the city where they live in the zip code,
you know,

357
00:25:16.341 --> 00:25:19.550
they say zip code is more important than your genetic code.

358
00:25:20.390 --> 00:25:25.010
And then you go upstream.
You know,
are there any genetic factors,
you know,

359
00:25:25.110 --> 00:25:27.350
you know,
we look at those things later,

360
00:25:28.220 --> 00:25:33.220
but the idea is to go upstream so that you can attack the problem early on

361
00:25:33.591 --> 00:25:38.010
before it becomes a major problem.
Because the law,
you know,

362
00:25:38.120 --> 00:25:42.650
the more to the right you go,
the more expensive it becomes to,

363
00:25:42.680 --> 00:25:43.880
to deal with the problems

364
00:25:45.900 --> 00:25:50.630
the scientific method has changed or are,
let me put it this way,

365
00:25:50.960 --> 00:25:54.380
the scientific method is there.
You know,
you typically ask a question,

366
00:25:54.560 --> 00:25:57.680
you collect a sample of data and then you get an answer.

367
00:25:58.670 --> 00:26:01.850
But then with Ai,
you know,
you do have the data.

368
00:26:02.000 --> 00:26:04.610
You don't have to collect a sample,
although you could.

369
00:26:05.420 --> 00:26:09.560
The key is to re ask the right questions and be able to get the answers.

370
00:26:11.360 --> 00:26:12.830
Um,
you know,

371
00:26:12.831 --> 00:26:17.780
one thing I keep saying is that healthcare is an information business and the

372
00:26:17.781 --> 00:26:18.920
sooner we realize it,

373
00:26:19.400 --> 00:26:24.400
the more lives we can save because the way you deal with the information,

374
00:26:24.891 --> 00:26:26.690
the way you can analyze the information,

375
00:26:26.691 --> 00:26:31.520
the way you can extract knowledge out of it is it becomes key when you want to

376
00:26:31.521 --> 00:26:34.640
save patients.
I like this.

377
00:26:34.641 --> 00:26:38.930
A paper that came out a long time ago,
uh,
Chris Anderson wrote this,

378
00:26:39.230 --> 00:26:41.880
the end of theory,
uh,

379
00:26:42.470 --> 00:26:46.740
the data delusion makes the scientific method obsolete in a way.

380
00:26:46.741 --> 00:26:51.741
We are seeing that right now because one of the problems with scientific method

381
00:26:52.171 --> 00:26:56.640
is that,
you know,
you take a sample from a big population,

382
00:26:57.420 --> 00:27:02.420
you collect that study sample and then you study it and then you learn things

383
00:27:03.210 --> 00:27:06.150
and apply that knowledge to your target population.

384
00:27:08.940 --> 00:27:12.090
Right now you have the whole target population.
You know,

385
00:27:12.091 --> 00:27:15.990
you go through these steps,
you know,
collecting data,
analyzing and translating,

386
00:27:16.320 --> 00:27:19.290
implementing science.
But then it takes a long time.

387
00:27:19.830 --> 00:27:24.830
It takes about 18 years on average from an idea to studying yet to analyzing it,

388
00:27:28.920 --> 00:27:29.753
implementing it,

389
00:27:30.150 --> 00:27:33.990
and then actually the patient coming to the bedside where the patients are

390
00:27:33.991 --> 00:27:38.130
actually using that.
I want this to speed up.

391
00:27:38.820 --> 00:27:39.331
It shouldn't,

392
00:27:39.331 --> 00:27:44.331
it should take less than a second to be able to get that data in this day and

393
00:27:44.881 --> 00:27:49.530
age,
you know,
we should have access to all that data.
We should have,
uh,

394
00:27:49.590 --> 00:27:53.460
analytics and engines that can work harder.

395
00:27:53.730 --> 00:27:57.990
And this is what I mean by punished the machine is that

396
00:27:59.490 --> 00:28:01.740
the idea that,
you know,
we are,

397
00:28:02.490 --> 00:28:07.490
we have this technology and we just letting it sit there and not be used for the

398
00:28:08.551 --> 00:28:12.430
good of mankind.
A lot of,
uh,

399
00:28:12.480 --> 00:28:15.570
value based systems,
uh,
as,
as,

400
00:28:15.571 --> 00:28:20.400
as you might have noticed or understood from,

401
00:28:21.370 --> 00:28:24.210
you know,
initially they used to be a lot of procedures.
I don't know.

402
00:28:24.480 --> 00:28:25.560
I don't know if you remember,

403
00:28:25.561 --> 00:28:30.480
but we used to do a lot of tonsillectomies and hysterectomy is and all kinds of

404
00:28:30.481 --> 00:28:34.290
procedures.
But we soon realized that,
oh,

405
00:28:34.320 --> 00:28:39.080
we could treat that with medication.
Maybe soon.
We'll realize that a lot of this,

406
00:28:39.330 --> 00:28:42.720
the sickness and illnesses can be treated with diet.

407
00:28:43.260 --> 00:28:46.860
And now we are talking about more of the social

408
00:28:48.420 --> 00:28:50.190
determinants of health,
you know,
house,

409
00:28:50.280 --> 00:28:54.420
how the social situations will contribute to illness and how we can prevent
that.

410
00:28:54.720 --> 00:28:59.260
And so we have to go up that value chain.
Um,

411
00:28:59.590 --> 00:29:02.440
want to talk a little bit about what I do?
You know,

412
00:29:03.330 --> 00:29:08.330
so my main key important people are the physicians and the patients and that's

413
00:29:08.971 --> 00:29:10.230
all I care about.

414
00:29:10.680 --> 00:29:15.680
And how can we design systems that'll help the physicians take better care of

415
00:29:16.141 --> 00:29:20.490
the patients.
Um,
so,

416
00:29:20.520 --> 00:29:21.353
um,

417
00:29:21.930 --> 00:29:26.370
we studied data and we'd go back and look at the retrospective data because

418
00:29:26.371 --> 00:29:28.740
right now we have more than,
you know,
10,

419
00:29:28.741 --> 00:29:33.741
15 years worth of electronic health record data and see what happens to these

420
00:29:34.201 --> 00:29:36.960
disease processes,
how do people get sick?

421
00:29:38.610 --> 00:29:42.910
And then you study how you can predict it,

422
00:29:43.030 --> 00:29:44.560
what will happen,
you know,

423
00:29:44.800 --> 00:29:49.800
if you know that this person has these risk factors and soon they'll end up

424
00:29:49.901 --> 00:29:50.734
here.

425
00:29:51.730 --> 00:29:56.730
Well hopefully you can do something to change those risk factors and prescribed

426
00:29:59.031 --> 00:30:03.550
something that will change the trajectory of that disease process.

427
00:30:04.040 --> 00:30:08.110
And so that's the idea.
So we have descriptive analytics,

428
00:30:08.380 --> 00:30:11.410
predictive analytics and prescriptive analytics.

429
00:30:13.300 --> 00:30:17.710
So what we have done is we've taken data and put it through,

430
00:30:18.220 --> 00:30:20.710
you know,
our engines.
By the way,

431
00:30:20.711 --> 00:30:24.970
this is a whole platform that is now active in Kaiser Permanente in northern

432
00:30:24.971 --> 00:30:28.990
California,
which is a 21 hospitals and,

433
00:30:29.140 --> 00:30:33.070
and 21 emergency departments.
And this is what we did.

434
00:30:33.640 --> 00:30:38.310
We created a platform that can then,
uh,

435
00:30:39.850 --> 00:30:41.050
have the engine,

436
00:30:41.170 --> 00:30:46.170
have the modules for each disease process and then have the clinical decision

437
00:30:47.411 --> 00:30:48.280
support system,

438
00:30:48.880 --> 00:30:53.800
which means that it'll tell the physician what the next step should be.

439
00:30:54.100 --> 00:30:58.780
For example,
if a patient comes into the emergency with chest pain,

440
00:31:00.300 --> 00:31:00.611
you know,

441
00:31:00.611 --> 00:31:04.780
we get a lot of patients in the emergency departments in our 21 emergency

442
00:31:04.781 --> 00:31:05.261
departments.

443
00:31:05.261 --> 00:31:10.120
We have more than a million patients that go through our departments every year.

444
00:31:11.310 --> 00:31:14.290
Um,
chest pain is probably number two or number three,

445
00:31:14.590 --> 00:31:18.790
chief complaint we call it.
So people with chest pain,

446
00:31:18.940 --> 00:31:22.810
maybe 100,000 people come through these departments with chest pain,

447
00:31:23.530 --> 00:31:27.400
only a few,
maybe five,
10,
15,

448
00:31:27.401 --> 00:31:30.400
20% have serious disease,
right?

449
00:31:30.401 --> 00:31:32.650
The other 80% do not have disease.

450
00:31:33.790 --> 00:31:38.300
But there are right now there are no systems.
I mean we,
yeah,
we do look at uh,

451
00:31:39.280 --> 00:31:44.280
or Framingham study but then that's a little bit old because if we can figure

452
00:31:46.001 --> 00:31:51.001
out what their risks are and so predicting the risk and then mitigating those

453
00:31:52.541 --> 00:31:54.140
risks,
uh,

454
00:31:54.840 --> 00:31:57.910
giving that information to the physician so that they can implement those

455
00:31:57.911 --> 00:32:02.560
strategies.
So when,
when a patient comes into the chest with chest pain,
you,

456
00:32:02.960 --> 00:32:07.750
when the,
when the physician clicks a button in the,
in our,
in our medical record,

457
00:32:07.780 --> 00:32:09.640
it goes to our website,

458
00:32:11.320 --> 00:32:16.320
it opens up in the browser within the health record and it walks them through a

459
00:32:16.661 --> 00:32:21.190
few steps and eventually giving the physician the answer.

460
00:32:21.280 --> 00:32:26.280
The answer says this person has a chance of 0.003% of having a heart attack in

461
00:32:29.861 --> 00:32:33.880
the next 60 days.
And so you should do a,
B,
and c.

462
00:32:35.980 --> 00:32:38.920
Now what it does is it'll number one,

463
00:32:39.140 --> 00:32:44.140
assessing the risk exactly for this patient based on your knowledge of 100,000

464
00:32:46.461 --> 00:32:48.320
patients before,
right?

465
00:32:48.500 --> 00:32:53.500
So you can actually guide this physician in the patient's care so that the best

466
00:32:54.411 --> 00:32:58.980
outcome can be had with the least cost and least work.

467
00:33:00.810 --> 00:33:03.910
Um,
and so we are very excited about this.
And,
uh,

468
00:33:04.070 --> 00:33:09.070
right now we are looking at implementing an AI based engine and replacing what

469
00:33:10.761 --> 00:33:15.761
we have right now so that it becomes even more faster and more accurate

470
00:33:15.801 --> 00:33:19.700
hopefully.
And we do clinical trials looking at these,

471
00:33:20.180 --> 00:33:24.800
these,
uh,
these technologies.
So half of the,

472
00:33:24.890 --> 00:33:28.790
um,
medical centers have the technology,
other half do not.

473
00:33:29.060 --> 00:33:34.060
And then we track these patients to see what changes have happened within these,

474
00:33:34.750 --> 00:33:36.170
uh,
within these entities.

475
00:33:37.850 --> 00:33:42.400
And so we are very excited and very,
um,
uh,

476
00:33:43.280 --> 00:33:48.170
happy about this and we got an award for this from our innovation folks.

477
00:33:49.850 --> 00:33:52.460
So all the answers are there.

478
00:33:53.960 --> 00:33:57.200
All the data is there,
the technology is there,

479
00:33:58.070 --> 00:34:02.120
everything is there.
All we need to do is ask the right questions.

480
00:34:02.780 --> 00:34:07.100
And when we asked the right questions,
we get those answers.
Well,

481
00:34:07.160 --> 00:34:12.160
this is my hope and my dream is that when we have an AI enabled care,

482
00:34:15.830 --> 00:34:20.030
the bad outcomes couple go down and the good outcomes will go up.

483
00:34:20.930 --> 00:34:22.100
In the past,
you know,

484
00:34:22.130 --> 00:34:27.130
it used to be very doctor speciphic where while there's this great doctrine in

485
00:34:28.490 --> 00:34:29.420
Houston or,

486
00:34:30.400 --> 00:34:35.400
or Boston or wherever people used to go there right now it is more system

487
00:34:35.661 --> 00:34:40.130
specific.
In other words,
while this is a great system,
oh,
Cleveland Clinic,
oh,

488
00:34:40.131 --> 00:34:42.440
Kaiser Permanente,
they have great quality,

489
00:34:43.490 --> 00:34:48.490
eventually becomes more patient specific where you can actually using the data

490
00:34:49.041 --> 00:34:50.870
of all these patients in the past,

491
00:34:51.170 --> 00:34:55.280
you can really fine tune the care of each individual patient.

492
00:34:56.900 --> 00:35:00.770
In the past it used to be high touch and no tech.
You know,
in the olden days,

493
00:35:02.010 --> 00:35:05.790
now it's kind of low touch and low tech.
You know,
the,

494
00:35:05.791 --> 00:35:09.680
the current Emr are very,
very,
um,

495
00:35:11.500 --> 00:35:14.840
what was the word heart to use?
Um,

496
00:35:15.860 --> 00:35:18.980
in the future it'll be more high tech touch again,

497
00:35:19.610 --> 00:35:24.140
but high tech will be in the background because the physicians won't be directly

498
00:35:25.010 --> 00:35:29.720
interacting,
um,
or,
or entering data because the data is already there.

499
00:35:32.260 --> 00:35:35.480
Um,
so what does the future hold?
Well,

500
00:35:35.840 --> 00:35:40.840
there are some great opportunities in healthcare to be able to make a difference

501
00:35:41.431 --> 00:35:44.040
in people's lives.
This story,

502
00:35:44.041 --> 00:35:48.880
I saw it on the American heart association's website.

503
00:35:49.650 --> 00:35:54.300
Um,
they put the story out about Justin,

504
00:35:54.390 --> 00:35:57.740
um,
because you know,
they,
they,

505
00:35:57.760 --> 00:36:02.520
they see this as the triumph off the healthcare system.
Okay.

506
00:36:03.900 --> 00:36:06.000
Justin had four heart attacks.

507
00:36:07.980 --> 00:36:10.500
He had five bypass surgeries

508
00:36:12.510 --> 00:36:13.560
and believe it or not,

509
00:36:13.561 --> 00:36:18.330
he had 35 sets stents placed in his heart blood vessels.

510
00:36:19.350 --> 00:36:20.730
That's unbelievable.

511
00:36:21.570 --> 00:36:26.570
It's great that Justin survived this and is doing okay.

512
00:36:27.691 --> 00:36:30.570
I hope after all this,

513
00:36:32.940 --> 00:36:37.940
wouldn't it be great if we caught this problem before the heart attacks,

514
00:36:38.880 --> 00:36:43.620
before the bypass surgeries,
before the 35 stents he had to be on.

515
00:36:45.240 --> 00:36:50.240
I think there's definitely potential there to be able to change this.

516
00:36:52.320 --> 00:36:53.520
As you may know,

517
00:36:53.521 --> 00:36:58.521
we are not doing that many bypass surgeries nowadays because we know that we can

518
00:36:58.681 --> 00:37:03.360
do that.
You know,
we can do it better with medications,
even stents.

519
00:37:03.390 --> 00:37:07.260
We are not putting that many because we know that,
you know,

520
00:37:07.261 --> 00:37:11.430
with medication you can actually,
the outcomes are equal or better.

521
00:37:12.990 --> 00:37:13.823
In fact,

522
00:37:14.010 --> 00:37:19.010
there were 800,000 stents placed last year in the United States that were

523
00:37:20.731 --> 00:37:23.430
totally useless that were not needed.

524
00:37:27.120 --> 00:37:30.000
Autism is a big area also.

525
00:37:30.480 --> 00:37:33.800
I feel there is lot of potential there.
It's a,

526
00:37:34.590 --> 00:37:38.460
a neurodevelopmental disorder by the way.

527
00:37:38.461 --> 00:37:43.110
I don't a little bit about it because I have a daughter with autism so I kind of

528
00:37:43.111 --> 00:37:48.060
studied that a little bit.
Uh,
neurodevelopment disorder that is,

529
00:37:48.061 --> 00:37:51.840
you know,
kids are bond with it and they have it for the rest of their lives.

530
00:37:52.230 --> 00:37:56.340
It's a very expensive disorder because it costs more than a few million dollars

531
00:37:56.730 --> 00:38:01.110
to take care of one child or one adult and they,

532
00:38:01.111 --> 00:38:02.400
incidents keeps growing.

533
00:38:03.600 --> 00:38:08.580
A one in 59 was the last one I think.
And it keeps growing.

534
00:38:08.620 --> 00:38:10.110
And that's what CDC says.

535
00:38:11.100 --> 00:38:13.680
And nobody knows why this is happening.

536
00:38:14.370 --> 00:38:19.370
We don't have any clue how many genes are probably in walled,

537
00:38:20.370 --> 00:38:22.710
but that's only a small percentage.

538
00:38:23.760 --> 00:38:26.600
We don't know if there's something in that environment.
Did the,

539
00:38:26.610 --> 00:38:30.680
there's something that we are eating with that we are wearing where we're

540
00:38:31.590 --> 00:38:35.500
inhaling.
But yeah,
this problem keeps growing.

541
00:38:36.760 --> 00:38:41.760
Right now it's so big that it has overshadowed all the developmental disorders,

542
00:38:43.091 --> 00:38:46.600
all the others.
So this is bigger than anything else.

543
00:38:46.601 --> 00:38:50.530
And this is one of the most expensive disorders for the healthcare system.

544
00:38:53.130 --> 00:38:58.000
Um,
another problem,
opioid crisis

545
00:39:00.520 --> 00:39:03.160
or the past,
I think 20 years,

546
00:39:04.930 --> 00:39:08.800
so many people died of opiate abuse and um,
uh,

547
00:39:09.250 --> 00:39:13.810
during this crisis,
and we don't have a solution for this.

548
00:39:15.850 --> 00:39:18.460
Well,
we know that

549
00:39:19.960 --> 00:39:23.710
when we start people on opioid medications,

550
00:39:23.920 --> 00:39:28.270
if there are on opiate medicine for more than three days,

551
00:39:28.870 --> 00:39:33.870
there's a 20% chance that they could get hooked on that medicine for the rest of

552
00:39:35.141 --> 00:39:36.880
their lives.
Imagine that.

553
00:39:37.780 --> 00:39:41.140
And we all must have taken at some point right after surgery or whatever.

554
00:39:41.710 --> 00:39:45.790
And there was a recent study in Jama,
a Andrew Chang and others,

555
00:39:46.540 --> 00:39:51.070
they looked at really,
you know,
opioids.
Are they really that great?

556
00:39:51.730 --> 00:39:56.200
No,
the pain control was the same.

557
00:39:56.470 --> 00:40:00.280
We'd been opioids and non opioids,
you know,
if he give Tylenol and Motrin,

558
00:40:00.580 --> 00:40:02.530
that's the same even if you have a broken ankle.

559
00:40:04.270 --> 00:40:07.390
And so why are we doing this?
How,
how did this,

560
00:40:07.540 --> 00:40:12.540
how did this crisis happened for so many years for so many lives lost.

561
00:40:14.860 --> 00:40:18.460
It's like the whole,
you know,

562
00:40:18.461 --> 00:40:22.600
city of Seattle gone vanished because of this crisis

563
00:40:24.250 --> 00:40:27.730
because we don't know the outcomes of what we are doing.
We don't know.

564
00:40:27.731 --> 00:40:30.370
We haven't studied it.
Maybe we should,

565
00:40:30.790 --> 00:40:35.680
maybe we should look at what are the chances of these people that we're starting

566
00:40:35.890 --> 00:40:40.030
them on opioids that are going to end up dying in the future.

567
00:40:40.630 --> 00:40:42.790
Is it really helping their pain or is it just

568
00:40:44.350 --> 00:40:48.130
building addicts out of it?
So that's a big,

569
00:40:48.160 --> 00:40:51.490
huge problem where data science I think can help.

570
00:40:53.500 --> 00:40:57.670
Okay.
So to conclude my talk,
I'll uh,
tell you this story.

571
00:40:57.820 --> 00:41:02.140
There were about a thousand years ago,
there was a town,

572
00:41:03.250 --> 00:41:07.570
and in that town there was,
it's a nice,
peaceful town,

573
00:41:07.600 --> 00:41:12.600
farmers living their families and there was a king and he was a nice a king.

574
00:41:15.190 --> 00:41:17.140
He took care of those people.

575
00:41:19.030 --> 00:41:24.030
One time what happened was there was a dragon that came to that town and it

576
00:41:25.091 --> 00:41:29.560
started attacking the chicken,
the chicken coops,

577
00:41:29.561 --> 00:41:32.890
and eating their chicken because it was a small baby dragon.

578
00:41:33.800 --> 00:41:37.730
And so it was coming and destroying and breaking things.
And you know,

579
00:41:37.731 --> 00:41:39.620
this is the kind of dragon that spits fire.

580
00:41:39.621 --> 00:41:43.340
So it'll set fire on those things and so destroys the property around.

581
00:41:44.390 --> 00:41:46.480
And so the farmers and,
and,

582
00:41:46.481 --> 00:41:50.180
and they know that it lives in the swamp out right outside the town.

583
00:41:50.850 --> 00:41:55.160
And so the farmers,
uh,
they came together,
the council meeting and said,
you know,

584
00:41:55.161 --> 00:41:58.760
we gotta do something about this.
And so one of the farmers that,
hey,

585
00:41:58.761 --> 00:42:03.200
you know what,
if we give the dragon the chicken every day,

586
00:42:03.230 --> 00:42:04.063
we feed him,

587
00:42:04.190 --> 00:42:09.110
we take him to his place and put it there so that he doesn't come here and burn

588
00:42:09.111 --> 00:42:11.750
these things up.
Yeah,
that's a great idea.

589
00:42:12.380 --> 00:42:17.380
And so they start taking chicken every day and tying him up near the swamp and

590
00:42:18.081 --> 00:42:21.140
the dragon comes out at night,
eats and then goes back.
It goes swamp.

591
00:42:22.340 --> 00:42:26.870
Soon they ran out of chicken,
then they started putting sheep,
goats.

592
00:42:28.070 --> 00:42:33.050
They ran out of those goats and cows and everything.
So now they said,

593
00:42:33.080 --> 00:42:37.760
okay,
well we have our kids,
let's give our children to the dragon.

594
00:42:37.780 --> 00:42:40.250
And so they started tying these kids.

595
00:42:40.590 --> 00:42:44.420
So the dragon keeps eating and the dragon is getting bigger and bigger at wants

596
00:42:44.421 --> 00:42:45.254
more.

597
00:42:47.060 --> 00:42:49.730
And then all the children are gone.

598
00:42:49.740 --> 00:42:53.420
Now it's just the adults except the king's daughter,

599
00:42:54.020 --> 00:42:58.760
the princess.
She was a young teenager and

600
00:43:00.560 --> 00:43:02.780
the farmer said,
well,
it's your turn king.

601
00:43:03.620 --> 00:43:06.770
And the king was where he's sad because that was his only daughter.

602
00:43:06.771 --> 00:43:11.270
He had great hopes for her.
But the first,
you know,

603
00:43:11.271 --> 00:43:15.020
he gave his word that,
you know,
they will feed the children.

604
00:43:15.021 --> 00:43:18.320
So they tied her,
tied her to a,

605
00:43:19.610 --> 00:43:20.930
um,
a tree there.

606
00:43:22.070 --> 00:43:26.000
But then before that happened,
the king announced,

607
00:43:26.420 --> 00:43:30.170
if anybody can slay that dragon,
I will give my daughter

608
00:43:32.120 --> 00:43:36.230
to you.
In a reading and you will have half of my kingdom.

609
00:43:37.670 --> 00:43:42.380
And so one shepherd boy came,
he was another teenager.
He said,
well,

610
00:43:42.381 --> 00:43:47.330
I'll kill the dragon.
Well,
this guy was not a soldier.
He didn't have any,

611
00:43:47.570 --> 00:43:49.970
you know,
um,
weapons.
You know,

612
00:43:49.971 --> 00:43:54.110
all these guys had knives and swords and they couldn't do anything.
But he said,

613
00:43:54.111 --> 00:43:55.320
I will do it.
And so he,

614
00:43:55.321 --> 00:43:59.720
what he did was he went there where the king's daughter was tight.

615
00:44:00.020 --> 00:44:03.650
He took the rope and he hid behind a Bush,

616
00:44:04.250 --> 00:44:05.360
waited for the dragon,

617
00:44:06.320 --> 00:44:09.380
and then he made a noose with the rope,

618
00:44:10.130 --> 00:44:12.650
threw it around its neck,
tightened it,

619
00:44:13.430 --> 00:44:16.910
and so that the dragon cannot breathe,
fire or can't even breathe.

620
00:44:17.480 --> 00:44:21.920
And he dragged the dragon into the town and then eventually kills it.

621
00:44:24.170 --> 00:44:28.700
And that is Saint George.
For those of you,
you know,

622
00:44:28.710 --> 00:44:30.770
you must have seen some of these pictures.

623
00:44:31.590 --> 00:44:36.590
He became Saint George and he married and everybody was happy af after that.

624
00:44:37.650 --> 00:44:42.390
So the moral of the story is that we think about weapons.

625
00:44:42.690 --> 00:44:47.010
We think about things to attack a problem.

626
00:44:49.110 --> 00:44:52.350
This shepherd boy,
he thought in a different way.

627
00:44:52.351 --> 00:44:57.351
He didn't think about guns and knives and swords and spears.

628
00:44:59.400 --> 00:45:00.930
He thought about this rope

629
00:45:02.730 --> 00:45:07.050
and we all have this rope with us.
We just have to use it.

630
00:45:07.530 --> 00:45:12.530
So let's be smart and save some lives and save some doctors from grief.

631
00:45:13.680 --> 00:45:16.860
That's my talk for today.
And,
uh,
I welcome any questions.

632
00:45:16.861 --> 00:45:20.730
Thank you for waiting here patiently.

633
00:45:21.510 --> 00:45:25.670
I invite you to join me in this fight.
Um,
I'll be a,

634
00:45:25.680 --> 00:45:30.460
you can connect with me on Linkedin as the easiest way and,
um,

635
00:45:30.540 --> 00:45:32.160
I'm open to questions right now.

636
00:45:34.210 --> 00:45:35.800
<v 2>Thank you so much.
Um,</v>

637
00:45:35.830 --> 00:45:39.850
so I do have a couple of questions on our online submission for those folks who

638
00:45:39.851 --> 00:45:43.840
are on the live streams since the people on livestream or night able to speak

639
00:45:43.841 --> 00:45:47.650
directly into the room.
So wanting to relay a couple of these.

640
00:45:48.340 --> 00:45:50.760
The first is,
uh,

641
00:45:50.770 --> 00:45:55.770
you said that 50% of what physicians do you see as potentially being

642
00:45:56.501 --> 00:45:58.840
unnecessary?
Uh,
the question is,

643
00:45:58.841 --> 00:46:03.280
do you believe that some part of this comes from living in an overly litigious

644
00:46:03.281 --> 00:46:04.780
culture,
uh,

645
00:46:04.781 --> 00:46:08.950
where physicians become frightened of litigation?

646
00:46:08.951 --> 00:46:11.680
And that is why the unnecessary measures are being taken?

647
00:46:12.280 --> 00:46:17.110
Or do you see it coming as a us from some other source?
They said,

648
00:46:17.111 --> 00:46:20.770
for example,
bloated bureaucracies,
etc.

649
00:46:21.580 --> 00:46:25.810
<v 0>So,
uh,
as I showed you those three dragons,
you know,</v>

650
00:46:27.010 --> 00:46:29.970
litigation is one of those,
um,

651
00:46:30.130 --> 00:46:33.280
documentation saying that,
okay,
I did this,
this,

652
00:46:33.281 --> 00:46:36.880
this because I need to get reimbursed.
Um,

653
00:46:37.210 --> 00:46:41.230
and the regulatory framework know which caused us also to do more work.

654
00:46:41.710 --> 00:46:43.060
But there is a big,

655
00:46:43.840 --> 00:46:46.900
there is a big gap in our knowledge.

656
00:46:48.850 --> 00:46:52.420
And so a lot of it is dogma.
I feel,
Oh,

657
00:46:52.570 --> 00:46:56.570
my professor taught me to do this way and this is what the textbooks say.

658
00:46:56.571 --> 00:47:00.820
So this is what I'm going to do.
Not realizing that

659
00:47:02.410 --> 00:47:07.410
that a lot of it may or may not be true because there may not be scientific

660
00:47:07.961 --> 00:47:11.440
basis for whatever they're doing.
I'll give you a good example.

661
00:47:11.860 --> 00:47:15.490
You don't ankle sprain,
right?
We've been taught,
I dunno,

662
00:47:15.520 --> 00:47:20.520
for 50 years or so that there's the rice protocol or rice treatment,

663
00:47:22.210 --> 00:47:26.620
which is
now I try,
I'm trying to remember what rises.

664
00:47:27.270 --> 00:47:30.700
Um,
so ice,

665
00:47:31.570 --> 00:47:36.520
um,
see for compression e for elevation.
So you have to rest your ankle,

666
00:47:36.910 --> 00:47:41.080
you have to ice it,
you have to put a bandage around it,

667
00:47:41.110 --> 00:47:45.790
compress it and keep it elevated.
I don't know.

668
00:47:45.791 --> 00:47:49.240
Last time I sprained my ankle,
I just walked.
I just did nothing.

669
00:47:49.660 --> 00:47:52.540
And it got better really.

670
00:47:53.200 --> 00:47:57.850
So we don't know his rest better or walking is better.

671
00:47:58.870 --> 00:48:03.370
Is putting a compression a better or you know all those things.

672
00:48:03.371 --> 00:48:08.350
Isis is ice better or heat better?
We have no idea because there's no data.

673
00:48:09.040 --> 00:48:13.600
Our research that supports what we're trying to do.
So like that,

674
00:48:13.720 --> 00:48:18.720
there are several things that happen in health care that physicians don't know

675
00:48:19.151 --> 00:48:20.440
and nobody actually studied.

676
00:48:20.830 --> 00:48:25.830
So most of the research funding doesn't go to those kinds of simple things.

677
00:48:25.960 --> 00:48:26.700
Right.

678
00:48:26.700 --> 00:48:31.700
There has to be a fancy drug or a fancy surgery or a device where you know,

679
00:48:34.630 --> 00:48:39.630
to get to get these researchers interested or the funding agency is interested

680
00:48:40.480 --> 00:48:41.650
so that,
you know,

681
00:48:41.651 --> 00:48:46.300
and so the cost keeps going up because you get these new drugs in there and so

682
00:48:46.330 --> 00:48:48.160
nobody's actually following up.

683
00:48:48.400 --> 00:48:53.400
Even when you have expensive drugs or expensive devices,

684
00:48:54.460 --> 00:48:59.460
we're not following them up to see are they really working or is there something

685
00:48:59.891 --> 00:49:02.020
better?
Um,

686
00:49:02.350 --> 00:49:07.350
and so there's no way to know all these drugs on the market if there are really

687
00:49:07.901 --> 00:49:12.100
effective.
I know they did a lot of research before they got approval,

688
00:49:12.460 --> 00:49:16.360
but once they get approval and once they are in the,
in,

689
00:49:16.361 --> 00:49:20.620
in the market and they're selling those,
we don't know if that works or not.

690
00:49:21.250 --> 00:49:25.930
Um,
the,
the,
the,
the research that happens is on a few thousand patients here,

691
00:49:26.380 --> 00:49:29.560
but then a million patient and millions of patients are using those,

692
00:49:30.010 --> 00:49:32.920
but we are not tracking all the data on those millions of patients.

693
00:49:33.340 --> 00:49:34.630
So that's where the problem comes.

694
00:49:36.660 --> 00:49:41.300
<v 2>That is fascinating.
We have a couple more that are coming in,</v>

695
00:49:41.360 --> 00:49:43.230
popping up here.
Um,

696
00:49:43.370 --> 00:49:48.370
one is from Thomas Kwan in another part of mountain view and his question is

697
00:49:49.371 --> 00:49:52.580
with the Kaiser starting a new medical school next year,

698
00:49:52.820 --> 00:49:57.590
how do you plan to influence the curriculum to train physicians in Ai or other

699
00:49:57.591 --> 00:49:59.280
digital era technologies?

700
00:50:00.430 --> 00:50:04.890
<v 0>I'm sure.
So for those of you who don't know what,
uh,
you know,</v>

701
00:50:05.170 --> 00:50:09.280
the business model of Kaiser Permanente is a Kaiser Permanente is one of the

702
00:50:09.281 --> 00:50:10.690
value based care models,

703
00:50:10.930 --> 00:50:14.830
one of the original value based care models that was started 70 years ago.

704
00:50:15.970 --> 00:50:20.970
It's all happens that now it has become the main thing because people are

705
00:50:21.371 --> 00:50:26.371
looking up to it and value based care model in a value based care model,

706
00:50:27.710 --> 00:50:32.710
the idea is to keep the patients healthy or prevent disease so that you get a

707
00:50:34.131 --> 00:50:37.790
fixed amount of money.
And so if you keep the patients healthy,

708
00:50:37.820 --> 00:50:40.220
that means you make money when the patients are healthy,

709
00:50:41.330 --> 00:50:46.140
which also means that we have to go upstream and look at,
you know,

710
00:50:46.160 --> 00:50:50.360
their vaccinations there.
Um,
you know,
colonoscopies,

711
00:50:50.361 --> 00:50:54.830
the mammograms and all those things in a how do we prevent cancer from happening

712
00:50:54.831 --> 00:50:58.580
there so that we can,
you know,
uh,
tackle it here.

713
00:51:00.290 --> 00:51:04.340
Um,
that in itself,
you know,

714
00:51:04.341 --> 00:51:06.590
now it is becoming more and more,
you know,

715
00:51:06.620 --> 00:51:09.970
the rest of the country is slowly trying to follow that model.
Um,

716
00:51:10.040 --> 00:51:12.910
probably about 30,
40% of the,
of the,

717
00:51:12.911 --> 00:51:17.911
of the companies are healthcare companies are going in that going to that model

718
00:51:18.530 --> 00:51:22.450
and Medicare,
the main,
uh,
um,

719
00:51:22.820 --> 00:51:27.820
government funding agency for health care delivery is requiring more of these

720
00:51:28.701 --> 00:51:32.390
companies to,
to do that.
So,
yes.

721
00:51:32.870 --> 00:51:37.070
And what we found was that,
you know,
one of the big things is that in,
in,
um,

722
00:51:38.000 --> 00:51:41.690
in the traditional medical schools,
they're all geared towards fee for service.

723
00:51:42.860 --> 00:51:47.750
That has been,
you know,
the tradition,
right?
So that continues,
um,

724
00:51:47.810 --> 00:51:50.810
that's one of the reasons why,
you know,
Kaiser is planning to,

725
00:51:50.870 --> 00:51:55.870
to start their own medical school and hopefully all this training will be part

726
00:51:56.151 --> 00:51:58.580
of that.
Uh,
you know,
how do you prevent,

727
00:51:58.581 --> 00:52:01.980
how do you go upstream and prevent problems later on?
Uh,

728
00:52:02.240 --> 00:52:05.720
right now there is enough data that shows that,
um,
you know,

729
00:52:05.721 --> 00:52:08.900
if you are a Kaiser permanent,
by the way,
I'm not plugging in Kaiser Permanente,

730
00:52:08.901 --> 00:52:12.360
but this is just a scientific fact.
Uh,

731
00:52:12.560 --> 00:52:14.450
if you're a Kaiser Permanente member,

732
00:52:14.720 --> 00:52:19.720
you have 50% less chance of having a stroke are 30% less chance of having a

733
00:52:21.111 --> 00:52:23.390
heart attack.
Just Fyi,

734
00:52:23.860 --> 00:52:28.840
<v 2>just FYI.
That's good.
For those of us who are near Kaiser facilities here.</v>

735
00:52:28.870 --> 00:52:31.600
Yes,
we have one question from the audience.
One second.

736
00:52:34.050 --> 00:52:34.690
Um,

737
00:52:34.690 --> 00:52:39.220
I have a question about kind of the decision tool that you're building,

738
00:52:39.280 --> 00:52:43.930
um,
specifically around the fact that it takes a lot of information.

739
00:52:43.931 --> 00:52:44.650
So at both,

740
00:52:44.650 --> 00:52:48.910
it both looks at unsuccessful cases as well as successful cases to help

741
00:52:48.911 --> 00:52:53.710
determine what the right path forward is.
You know,
looking very far out.

742
00:52:53.740 --> 00:52:58.030
If we continue to use that successful case and we continue to follow it,

743
00:52:58.210 --> 00:53:03.040
do we lose some of that training information that's important for that

744
00:53:03.041 --> 00:53:04.720
comparative assessment?

745
00:53:04.721 --> 00:53:09.070
And then kind of thinking beyond that and tying them with the Farmingham study,

746
00:53:09.071 --> 00:53:11.150
like the longitudinal effects.

747
00:53:11.320 --> 00:53:16.320
So maybe we would have been told that opioids is really effective in treating

748
00:53:16.780 --> 00:53:17.590
pain medicine,

749
00:53:17.590 --> 00:53:22.540
but we wouldn't have necessarily seen what happens five years down the line.
Um,

750
00:53:22.690 --> 00:53:26.070
and kind of how some of the longitudinal effects are being incorporated into

751
00:53:26.070 --> 00:53:30.240
<v 0>the decision tool.
That's a,</v>

752
00:53:30.241 --> 00:53:33.990
that's a very good question.
So this actually happened,

753
00:53:35.040 --> 00:53:39.600
uh,
so we were building this tool for a condition called pulmonary embolism.

754
00:53:40.020 --> 00:53:43.170
Pulmonary embolism is where you get a clot in your lung.

755
00:53:43.710 --> 00:53:47.730
It's like a lung attack and you can't breathe,
right.

756
00:53:47.731 --> 00:53:49.360
Some people die because you know,

757
00:53:49.361 --> 00:53:51.630
it cuts off the circulation to the lungs and the brain,

758
00:53:52.980 --> 00:53:54.300
the brain doesn't get oxygen.

759
00:53:54.990 --> 00:53:59.990
So about 95% of these patients get admitted to the hospital as soon as they're

760
00:54:00.331 --> 00:54:01.530
diagnosed in the emergency.

761
00:54:02.310 --> 00:54:07.200
But we know that only a very small percentage of patients will have bad
outcomes.

762
00:54:07.680 --> 00:54:12.150
It's a how do you identify the ones that will have good outcomes and not keep

763
00:54:12.151 --> 00:54:15.360
those patients in the hospital,
the low risk ones,
right.

764
00:54:16.320 --> 00:54:18.690
Because when you stay in the hospital,
you know,
you,

765
00:54:19.070 --> 00:54:20.640
you are at risk for other things,
you know,

766
00:54:20.641 --> 00:54:23.710
hospital acquired infections and stuff.
Also,

767
00:54:23.720 --> 00:54:27.600
it's an inconvenience for the patient and it's a cost for the hospital.

768
00:54:28.440 --> 00:54:30.900
So we did a study and said,
okay,
we,

769
00:54:30.901 --> 00:54:35.070
let's build this tool which can identify the low risk patients.

770
00:54:35.160 --> 00:54:37.700
And we implemented it and um,

771
00:54:38.190 --> 00:54:42.810
10 medical centers or 10 emergency departments and the other 11 emergency

772
00:54:42.811 --> 00:54:47.160
departments were controls.
And we followed these patients for more than a year.

773
00:54:47.610 --> 00:54:50.010
And we found that where this tool was available,

774
00:54:50.160 --> 00:54:55.160
it was very successful in decreasing the admission of unnecessary low risk

775
00:54:56.310 --> 00:55:00.720
patients.
What we also found was that by the way,
this tool,

776
00:55:00.910 --> 00:55:05.910
we pulled it from the literature and this literature comes from Canada,

777
00:55:06.211 --> 00:55:09.150
Australia,
and then what we farmer,

778
00:55:09.151 --> 00:55:13.620
once we studied these patients was that this tool was actually,

779
00:55:13.621 --> 00:55:17.520
or estimating the risk.
Our patients are much healthier,

780
00:55:17.590 --> 00:55:22.230
are at much lower risk.
And so version two,

781
00:55:22.320 --> 00:55:27.120
what we did was we changed the dials on that so that it'll,
it'll,

782
00:55:27.121 --> 00:55:31.350
it'll not,
it'll estimate the risk correctly.
And so,

783
00:55:32.490 --> 00:55:36.720
so you,
to answer your question,
we'll be be losing data maybe,

784
00:55:37.260 --> 00:55:41.490
but maybe if we tried to everybody,
we won't lose any data,
even bad outcomes,

785
00:55:41.491 --> 00:55:44.070
good outcomes,
you know,
so,

786
00:55:44.130 --> 00:55:47.760
and then we have to constantly make the tool better and better.

787
00:55:47.910 --> 00:55:52.050
So each version becomes better as the population gets healthier and,
and,

788
00:55:52.150 --> 00:55:56.100
and so in the long run it'll work great because,
you know,

789
00:55:56.101 --> 00:56:00.330
the population gets better and the tool gets better at estimating the risk.

790
00:56:00.630 --> 00:56:01.411
But yeah,

791
00:56:01.411 --> 00:56:06.411
I understand that he though it does change our knowledge and it does.

792
00:56:06.480 --> 00:56:08.760
It does.
Um,
hopefully for the good,

793
00:56:12.010 --> 00:56:13.920
<v 2>everyone has time to stick around for a couple more questions.</v>

794
00:56:13.921 --> 00:56:16.470
I've got a few more that have popped up here.
Um,

795
00:56:16.530 --> 00:56:21.390
a quick one from Vivian Lee in Cambridge,
Massachusetts.
Uh,

796
00:56:21.391 --> 00:56:26.391
she says thank you for the great of the chest pain AI application in the Kaiser

797
00:56:26.651 --> 00:56:29.650
ers.
Do you have other examples you can share?

798
00:56:30.560 --> 00:56:34.830
<v 0>Sure.
Um,
we did one,
uh,</v>

799
00:56:34.850 --> 00:56:39.680
one study for head injury.
You know,
a lot of,

800
00:56:39.681 --> 00:56:43.910
uh,
uh,
kids that come in with head injury,
they fall and hit their heads,

801
00:56:44.890 --> 00:56:47.540
um,
when they come in.
You know,

802
00:56:47.541 --> 00:56:52.250
one of the tests we order is a CT scan of the head,
right.

803
00:56:52.820 --> 00:56:57.200
And,
and we know,
we know,
again from studies that CT scans,

804
00:56:57.230 --> 00:56:58.880
if you do a CT scan on kids,

805
00:56:58.881 --> 00:57:03.881
it's bad because it is radiation at a young age and it hit that predisposes that

806
00:57:05.001 --> 00:57:07.310
person to be at risk for cancer in the future.

807
00:57:07.730 --> 00:57:12.050
And so how do we decrease the CT scans?
Um,
especially in kids,
you know,

808
00:57:12.051 --> 00:57:15.530
you have to,
you know,
um,
a,
the risk is higher.

809
00:57:17.210 --> 00:57:19.730
And so we designed the study where,
you know,

810
00:57:20.300 --> 00:57:24.350
looking at the kids' features,
how it is,
how they present,

811
00:57:24.860 --> 00:57:28.370
can you identify the low risk kids that don't need a CT scan?

812
00:57:29.150 --> 00:57:31.910
And we did the same thing for belly pain and kids,
you know,

813
00:57:31.911 --> 00:57:36.390
what are the chances of this kid having appendicitis,
which is the most,
um,

814
00:57:36.470 --> 00:57:40.310
you know,
dreaded diagnosis for these kids because you know,

815
00:57:40.311 --> 00:57:45.311
you gotta go through surgery but then radiating them on their bellies.

816
00:57:45.830 --> 00:57:48.290
Probably the highest radiation a kid can get.

817
00:57:49.280 --> 00:57:52.970
Cause this is a large area.
Um,
and in a small child.

818
00:57:53.870 --> 00:57:57.760
And so we were,
we were right down.
That study is going on right now.
And,
uh,

819
00:57:57.860 --> 00:58:01.880
what we are trying to do is we're trying to decrease the number of radiation

820
00:58:01.940 --> 00:58:06.510
without,
you know,
missing appendicitis cases.
Um,

821
00:58:06.770 --> 00:58:11.270
that's a great one.
And then,
so like I said,
you know,
surgery is,
is,

822
00:58:11.280 --> 00:58:15.500
is the treatment for appendicitis.
But in the future,
maybe it's sad to biotics,

823
00:58:16.160 --> 00:58:19.430
maybe it's changing into your diet.
Maybe you know,
all those things.

824
00:58:19.431 --> 00:58:22.340
We don't know because we never thought about it or studied it.

825
00:58:22.880 --> 00:58:25.400
So there's great information coming out of that.

826
00:58:27.480 --> 00:58:29.940
<v 2>Okay.
Final question and then I think that we need to wrap up,</v>

827
00:58:29.941 --> 00:58:32.220
although we do have a couple more here.
Um,

828
00:58:32.250 --> 00:58:37.250
there was one anonymized question that came in asking about single payer systems

829
00:58:37.921 --> 00:58:39.930
like the,
excuse me,

830
00:58:40.230 --> 00:58:43.290
single pair systems like the European economies that you noted,

831
00:58:43.560 --> 00:58:47.460
which spend less but have higher quality of care.
Um,

832
00:58:47.490 --> 00:58:50.010
are these economies are often,
or excuse me,

833
00:58:50.011 --> 00:58:53.520
these systems are often accused of being overly costly for the country.

834
00:58:54.120 --> 00:58:55.510
Do you think that a singer,

835
00:58:55.620 --> 00:58:59.460
single payer model could work in a country like the United States?
A Nice,

836
00:58:59.461 --> 00:59:00.350
easy question for you.

837
00:59:01.690 --> 00:59:05.320
<v 0>That's beyond my scope.
Um,
but you know,</v>

838
00:59:05.500 --> 00:59:07.540
I'm not a great businessperson.

839
00:59:08.770 --> 00:59:11.710
Economists may be able to answer that question better.

840
00:59:13.220 --> 00:59:15.830
<v 2>Well,
doctor chatter,
Polly,
thank you so much for being here.</v>

841
00:59:15.831 --> 00:59:20.240
We're delighted to have you and we very much look forward to both the book

842
00:59:20.241 --> 00:59:23.480
coming out and hearing more about what you're doing with AI and healthcare in

843
00:59:23.481 --> 00:59:26.280
the future.
So thank you so much.
Yes,
thank you.

