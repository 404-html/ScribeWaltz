WEBVTT

1
00:00:06.370 --> 00:00:09.400
We're going to pivot to the panel,
uh,
on,
uh,

2
00:00:09.410 --> 00:00:13.880
whether computers can be racist.
And we had a great,
great group,
uh,
for you.

3
00:00:14.180 --> 00:00:18.230
Uh,
you all know Bradley Horowitz,
uh,
who is,
uh,
obviously,
uh,

4
00:00:18.410 --> 00:00:22.640
[inaudible] at Google.
Uh,
he's,
uh,
running the new,
he can tell you about it,

5
00:00:22.641 --> 00:00:26.170
but the,
the new,
uh,
area,
uh,
the new entrepreneurial,
uh,

6
00:00:26.270 --> 00:00:30.890
incubation hub at Google,
and he is passionate about all of these,
uh,

7
00:00:30.920 --> 00:00:35.680
these issues.
And,
uh,
and,
uh,
what we can do with tech,
uh,

8
00:00:35.690 --> 00:00:39.680
Nancy do [inaudible] is user experience,
uh,
research program man,

9
00:00:39.750 --> 00:00:40.850
a manager at Google,

10
00:00:41.150 --> 00:00:44.150
and she leads a research on the experience for all new launches.

11
00:00:44.151 --> 00:00:46.490
So as she puts it,
she gets to see,
you know,

12
00:00:46.760 --> 00:00:51.070
all the problems with all the products.
Uh,
so she's,
uh,
well,

13
00:00:51.140 --> 00:00:55.400
well suited to talk about these,
these issues.
Megan Rose Dickey,
uh,
is also here.

14
00:00:55.401 --> 00:00:58.260
She's a reporter who's a,
you're probably familiar restroom,

15
00:00:58.270 --> 00:01:02.360
worked at tech crunch,
focusing on diversity inclusion and social justice.

16
00:01:02.600 --> 00:01:05.750
She was at business insider,
uh,
before that.
Uh,

17
00:01:05.751 --> 00:01:10.460
we also have a Neil dot dash who's a quite a thinker on a lot of,
uh,

18
00:01:10.550 --> 00:01:15.180
uh,
tech issues,
especially in terms of how tech can be more humane,

19
00:01:15.360 --> 00:01:18.320
a,
an inclusive,
he's an entrepreneur.
He's a technologist.

20
00:01:18.321 --> 00:01:21.700
I believe he's recently become a CEO of a software company as well.
Uh,

21
00:01:21.770 --> 00:01:25.660
so great group.
Van's going to lead this.
I will exit stage,
right?
Uh,

22
00:01:25.890 --> 00:01:27.070
so how about you sir?

23
00:01:29.480 --> 00:01:31.630
<v 2>Alright,
David Drummond</v>

24
00:01:34.430 --> 00:01:35.620
at the bad brother.

25
00:01:41.770 --> 00:01:44.310
<v 3>If you don't worry about this stuff all the time,</v>

26
00:01:44.311 --> 00:01:47.370
it's just because you don't have to,
but the residents think about all the time.

27
00:01:47.560 --> 00:01:49.020
You know,
there's the famous thing with,

28
00:01:49.021 --> 00:01:53.310
I'm a professor would let Latanya Mcsweeney,
and she's a professor.

29
00:01:53.311 --> 00:01:58.311
She typed her name in and then what comes back or these ads around,

30
00:01:58.470 --> 00:02:01.530
you know,
getting somebody who's criminal records and all this criminal stuff.

31
00:02:02.130 --> 00:02:07.130
And she does the research and it turns out that people who were offering these

32
00:02:08.551 --> 00:02:09.960
services were,

33
00:02:09.961 --> 00:02:13.920
had picked up a bunch of African American names and had the ad search trigger.

34
00:02:15.030 --> 00:02:20.030
Downside is if you're looking for a job and somebody googles you dizzy,

35
00:02:20.150 --> 00:02:22.920
if you're a good employee,
boom,

36
00:02:22.921 --> 00:02:27.750
here's this suggestion that you might be a criminal and so

37
00:02:29.490 --> 00:02:32.720
that would suck.
So,
um,

38
00:02:33.000 --> 00:02:35.220
what can be done to prevent it?
So we've,

39
00:02:35.221 --> 00:02:40.020
we've got some examples of how bad it is.
What can be done to prevent this?
Maybe,

40
00:02:40.021 --> 00:02:42.810
you know,
more inclusive hiring.
But from your point of view,

41
00:02:42.811 --> 00:02:44.580
you think about this stuff a lot.
How do you think about it?

42
00:02:45.310 --> 00:02:48.830
<v 0>Um,
this particular situation,
uh,
with a,
Tanya</v>

43
00:02:50.360 --> 00:02:55.360
had actually some difficulty to do with the ad d designers who were purchasing

44
00:02:55.401 --> 00:02:56.810
names of African Americans,

45
00:02:57.170 --> 00:03:00.670
which means that we need to be very about our guidelines,
right?

46
00:03:00.671 --> 00:03:05.380
We need to start to find a way to capture these use cases and create content

47
00:03:05.381 --> 00:03:08.920
around that.
And the other thing is we need to be able to test those in the right

48
00:03:08.921 --> 00:03:12.680
environments.
Um,
I,
this makes me think of Tay,
the chatbox.

49
00:03:12.790 --> 00:03:15.790
Do you guys know about the artificial intelligence chat bots on Twitter from

50
00:03:15.791 --> 00:03:20.560
Microsoft that became racist and 16 hours and sexist to 16 hours?
Um,

51
00:03:20.561 --> 00:03:25.561
what's interesting about Tay as tay was tested for two years in China where free

52
00:03:28.511 --> 00:03:32.120
speech is not necessarily something that's readily available.
You can,

53
00:03:32.200 --> 00:03:33.610
you can't just say whatever you want,
right?

54
00:03:33.910 --> 00:03:35.860
But when it was introduced in America,

55
00:03:35.861 --> 00:03:39.490
even though they were testing for bias at the time,
within 16 hours,

56
00:03:39.491 --> 00:03:40.330
like Bradley said,

57
00:03:40.331 --> 00:03:44.200
because it didn't have the right environment or the right people to test it on

58
00:03:44.470 --> 00:03:45.550
this occurred.

59
00:03:45.820 --> 00:03:49.540
So how can we make sure that we have not only the right use cases available,

60
00:03:49.750 --> 00:03:53.260
but we also are testing in populations that might be affected.

61
00:03:53.290 --> 00:03:56.080
Are we amplifying the voice of the underrepresented right?

62
00:03:56.110 --> 00:04:00.130
Are we testing and communities of just women we testing in different

63
00:04:00.131 --> 00:04:03.850
socioeconomic backgrounds?
Um,
so one of the things I'd been pressuring,

64
00:04:03.851 --> 00:04:07.690
at least from a research standpoint,
is making sure that from the very beginning,

65
00:04:07.870 --> 00:04:12.340
our research criteria is mixed as far as gender,
socio economic background,

66
00:04:12.341 --> 00:04:14.380
race even left handedness,
right?

67
00:04:14.560 --> 00:04:17.350
To make sure that we're creating inclusive products for,

68
00:04:17.590 --> 00:04:21.820
for better reach in the world and versatility in this world.

69
00:04:22.620 --> 00:04:27.330
<v 3>Uh,
Neil,
I want to get you in here.
First of all,
I love that purple.
Uh,
uh,</v>

70
00:04:27.640 --> 00:04:30.460
it's a tribute to somebody.
We both laughed.
We both look,

71
00:04:30.630 --> 00:04:33.360
this is the first time I've been in public,
not wearing purple since,
uh,

72
00:04:33.420 --> 00:04:37.890
April 21st.
So,
um,
I,
uh,
that's a whole other topic.

73
00:04:38.280 --> 00:04:42.720
Um,
best practices though.
So,
so we,

74
00:04:42.780 --> 00:04:46.100
I think we got a good problem statement.
Good overview.
Uh,
we got,
uh,

75
00:04:46.130 --> 00:04:49.660
a personal testimonial to the need for what?
For wokeness.
Um,

76
00:04:50.880 --> 00:04:55.230
we got a few ideas in terms of how we can begin to improve.
But,
uh,
Neil,

77
00:04:55.231 --> 00:04:57.900
I'd love to hear more from you on the same basic vein.
Well,

78
00:04:58.060 --> 00:05:01.930
<v 4>I look at this now,
you know,
I'm CEO of a company that makes software,
right?</v>

79
00:05:01.990 --> 00:05:06.520
And so we have to take this head on.
And the first thing I look at is,
um,

80
00:05:06.610 --> 00:05:10.710
educating ourselves about the issue,
right?
So as an industry,
what tech companies,

81
00:05:10.780 --> 00:05:15.010
little or big have done is the strategy of pretending we're neutral,

82
00:05:15.730 --> 00:05:20.160
right?
And this has been a very smart thing to do from a regulatory standpoint,

83
00:05:20.170 --> 00:05:24.400
right?
So if we say we're just a neutral platform,
we just reflect society.

84
00:05:24.700 --> 00:05:26.500
Don't regulate us.
Please.
Right?

85
00:05:26.501 --> 00:05:28.720
That's been this tactic that's gone over and over and over.

86
00:05:28.721 --> 00:05:33.280
And I look at that for like out in the world,
something like Uber where,
um,

87
00:05:33.340 --> 00:05:35.320
part of what they've done to present themselves as saying,

88
00:05:35.560 --> 00:05:38.160
we're in neutral platform where anybody can hail a cab and,

89
00:05:38.380 --> 00:05:40.210
and there are good things,
right?
So I,
I mean,

90
00:05:40.240 --> 00:05:42.820
I can't even count how many black friends I've said.
It was like,
okay,

91
00:05:42.821 --> 00:05:46.570
I can hail a cab now because I got to a,
and that's great.
And I said,
well,
yeah,

92
00:05:46.571 --> 00:05:48.550
but for operating outside the world of regulation,

93
00:05:48.551 --> 00:05:50.260
when they start red lining your neighborhood,

94
00:05:50.261 --> 00:05:53.680
what are we going to do to rein them in?
And we don't have an answer there.

95
00:05:53.920 --> 00:05:56.980
And what they've done is very effectively used the tactic of presenting

96
00:05:56.990 --> 00:06:00.770
themselves as neutral or benevolent to avoid the,
you know,

97
00:06:00.820 --> 00:06:03.650
the regulatory infrastructure and social infrastructure that we use.

98
00:06:03.890 --> 00:06:07.970
And we don't have that history,
right?
So if you want to do a startup today,

99
00:06:07.971 --> 00:06:10.400
you come from a CS background and that's great.

100
00:06:10.430 --> 00:06:13.400
I mean you should have those technical skills and that's powerful and valuable,

101
00:06:13.640 --> 00:06:18.140
but you probably had little or no ethics training or when you came up through

102
00:06:18.141 --> 00:06:18.980
that cs program,

103
00:06:18.981 --> 00:06:22.010
you probably had little or no social justice training and we came up through

104
00:06:22.011 --> 00:06:26.030
that program.
So there's no history where you can learn about,
well how did,

105
00:06:26.170 --> 00:06:27.090
how did you know red,

106
00:06:27.290 --> 00:06:30.860
red lining and zoning and all these other exclusionary patterns manifest in the

107
00:06:30.861 --> 00:06:32.960
past that I don't duplicate them in my software.

108
00:06:32.961 --> 00:06:35.540
And how did you know the labor movement to duplicate in the past?

109
00:06:35.541 --> 00:06:38.960
And so bringing that learning into,
when you say red lining,
yeah.

110
00:06:39.110 --> 00:06:42.560
Can you just break down what that means in a practical way for people who live

111
00:06:42.561 --> 00:06:46.640
in this world is all we worry about,
but yeah,
but red lining means the,

112
00:06:46.660 --> 00:06:51.350
the shortest version is using policy and particularly in the case of geography,

113
00:06:51.351 --> 00:06:56.351
like neighborhoods and communities to entrench and enforce a discriminatory

114
00:06:56.631 --> 00:07:00.410
practice around race,
religion,
and other systems of exclusion.
Okay.

115
00:07:00.420 --> 00:07:01.400
How's that look?

116
00:07:02.960 --> 00:07:07.640
<v 3>Red Light in means
if you live in this neighborhood,</v>

117
00:07:07.790 --> 00:07:12.650
you ain't get no loans.
If you can't get alone,
you can't do much.

118
00:07:13.020 --> 00:07:15.710
You have the best little company invests a little business.
Best little idea.

119
00:07:15.711 --> 00:07:18.560
If you can't access capital,
you're stuck.

120
00:07:19.130 --> 00:07:24.130
And this practice is so pervasive that there's a term for it called red lining

121
00:07:24.381 --> 00:07:26.150
that everybody knows and understands.

122
00:07:26.480 --> 00:07:31.220
And the concern now is that rather than the technology companies actually get
it,

123
00:07:31.221 --> 00:07:35.540
getting us to a place where you can equally get access to stuff based on your

124
00:07:35.541 --> 00:07:37.640
ideas and not based on your skin color,
your zip code,

125
00:07:37.750 --> 00:07:39.950
that is actually coming back again.
Again,

126
00:07:40.010 --> 00:07:44.840
we blew up the death star and now here we are back again.
Rebuild game guys with,

127
00:07:44.841 --> 00:07:46.190
with the red light sabers.

128
00:07:46.310 --> 00:07:49.040
<v 4>And that's it.
So we don't have that history.
We don't know that piece.</v>

129
00:07:49.041 --> 00:07:50.270
We don't teach that in a,

130
00:07:50.300 --> 00:07:53.690
there's no intro to coding program that teaches you this is what red lining is,

131
00:07:53.720 --> 00:07:55.360
right?
And this is why the,
you know,

132
00:07:55.400 --> 00:07:58.130
the highway was built through your part of town and not somebody else's part of

133
00:07:58.131 --> 00:08:01.610
town.
Right?
Right.
And so until we know that,
how are we going to say,
well,

134
00:08:01.730 --> 00:08:04.670
I know how to fire up x code in eclipse and I can make an APP.

135
00:08:04.671 --> 00:08:07.550
And that means that I understand the impacts of red lining and what

136
00:08:07.551 --> 00:08:09.350
transportation and transit policy are.

137
00:08:09.500 --> 00:08:11.150
Those are two totally different things in those people,

138
00:08:11.420 --> 00:08:15.170
<v 3>but don't talk,
and I think this is,
I want to come back to you now.
I'm ready.</v>

139
00:08:16.000 --> 00:08:19.990
I think that there is this weird thing for people in tech.

140
00:08:20.740 --> 00:08:25.740
You didn't set out to be a social engineer.

141
00:08:26.170 --> 00:08:29.830
You didn't set out to be the government.
He didn't set out in your mind,

142
00:08:29.860 --> 00:08:33.250
it must be,
isn't this someone else's job to be doing?

143
00:08:33.580 --> 00:08:37.870
I'm just trying to make these gadgets Gizmo,
right?
I mean,

144
00:08:37.871 --> 00:08:42.530
I'm not a tech person,
but that's,
that's the right term that I see.
I,

145
00:08:43.180 --> 00:08:47.320
I studied in the car.
So,

146
00:08:48.280 --> 00:08:53.280
and yet it turns out that you guys go from these little geeky people that nobody

147
00:08:53.381 --> 00:08:56.160
pay attention to.
No offense

148
00:08:59.340 --> 00:09:03.610
kind of being like the digital lords of the universe and the digital age.

149
00:09:04.090 --> 00:09:07.630
So you suddenly,
if he can't now say,
well Geez,
you know,

150
00:09:07.631 --> 00:09:10.420
I don't want to have to go to the diversity class,
leave me alone.
I mean,

151
00:09:10.730 --> 00:09:15.280
doesn't success then with great power doesn't come great responsibility.

152
00:09:15.281 --> 00:09:16.630
I'm just asking.
I'm just asking.

153
00:09:17.380 --> 00:09:21.700
<v 5>So,
um,
I think one thing that I've found,
um,</v>

154
00:09:21.730 --> 00:09:25.690
this concept of woke,
like I think a lot of people at Google,

155
00:09:25.691 --> 00:09:30.691
we've got 65,000 employees who in my experience are pretty well intentioned yes.

156
00:09:31.990 --> 00:09:36.700
And believe that they are doing good for the world and our sort of techno

157
00:09:36.701 --> 00:09:37.690
optimists and all that.

158
00:09:38.260 --> 00:09:43.260
And I think what we're coming to grips with is that as we're repeating here at

159
00:09:44.381 --> 00:09:48.100
the panel,
uh,
good intentions are not the end of the conversation.

160
00:09:48.101 --> 00:09:49.660
They are the start of a journey.

161
00:09:49.990 --> 00:09:54.720
And one thing that I've really been making use of is there are people who've

162
00:09:54.730 --> 00:09:58.540
spent lifetimes thinking about this,
studying it in a scholarly way,

163
00:09:58.720 --> 00:10:02.380
working on these issues,
picking them apart,
understanding them.

164
00:10:02.680 --> 00:10:07.240
And it's not like in an hour training,
we're going to sort of get up to speed on,

165
00:10:07.241 --> 00:10:10.510
you know,
a system of oppression that's been hundreds of years in the making.

166
00:10:10.750 --> 00:10:13.540
We have to turn to these experts and we have to invite them into the

167
00:10:13.541 --> 00:10:18.520
conversation and we have to not sort of think we got this,
but rather,
you know,

168
00:10:18.521 --> 00:10:22.000
come humbly to the,
to the knowledge that's out there already.

169
00:10:22.380 --> 00:10:24.940
<v 2>Yeah.
Woke to me.</v>

170
00:10:29.470 --> 00:10:31.700
<v 3>You started us off.
Um,</v>

171
00:10:32.680 --> 00:10:37.360
are there things that you're seeing now they give you either great encouragement

172
00:10:37.630 --> 00:10:40.270
as far as,
I mean that I thought that comment.
I mean,

173
00:10:40.750 --> 00:10:42.420
that was as good as you can get,

174
00:10:42.440 --> 00:10:45.730
at least from my an intention point of view or the things that give you

175
00:10:45.910 --> 00:10:47.470
encouragement that you're seeing developing.

176
00:10:47.500 --> 00:10:49.390
Are there things that give you great frustration?

177
00:10:50.010 --> 00:10:51.890
<v 6>Um,
yeah,
it's a,
it's a combination.</v>

178
00:10:51.891 --> 00:10:54.770
There's definitely a little bit of both going on.
So in,

179
00:10:55.280 --> 00:10:57.830
in the last couple of years or so,

180
00:10:57.831 --> 00:11:02.700
there's been a lot of talk around unconscious bias trainings and you know,

181
00:11:02.720 --> 00:11:07.640
increasing the number of underrepresented minorities and women and Lgbtq I a

182
00:11:07.641 --> 00:11:11.270
people in these tech companies.
But,
um,

183
00:11:12.380 --> 00:11:16.640
unfortunately it seems like we're kind of getting to this point where there's

184
00:11:16.641 --> 00:11:17.474
not,

185
00:11:17.780 --> 00:11:22.730
we've kind of hit a wall in a way and there's still the problem of actual

186
00:11:22.731 --> 00:11:27.380
inclusion and these companies and,
um,
like I just,

187
00:11:27.410 --> 00:11:30.890
I wonder,
I mean,
I think an event like this is totally great to,
you know,

188
00:11:31.190 --> 00:11:35.390
get employees at these big tech companies talking and thinking about race and

189
00:11:35.391 --> 00:11:39.720
injustice in the tech industry.
Um,
I just want to see a quick show of hands,
um,

190
00:11:40.880 --> 00:11:45.710
who,
um,
let's see.
What was this mandatory where you are you guys here by choice?

191
00:11:46.160 --> 00:11:48.110
Okay.
Okay.
So that's cool.

192
00:11:48.870 --> 00:11:51.080
<v 2>We are here.
Oh,</v>

193
00:11:53.280 --> 00:11:54.160
because we

194
00:11:54.160 --> 00:11:56.080
<v 3>want to be more diverse</v>

195
00:11:58.210 --> 00:12:00.540
now that it's about me.
It's all about you.

196
00:12:00.580 --> 00:12:05.320
<v 6>But I mean,
I,
I,
I struggle,
I go back and forth with the idea of mandates.</v>

197
00:12:05.321 --> 00:12:07.450
But I do think that something like this,

198
00:12:08.530 --> 00:12:11.710
if it were mandatory for all employees to attend that could be,

199
00:12:11.920 --> 00:12:16.460
that could be helpful.
I mean this is what,
maybe an hour of your time and,

200
00:12:16.570 --> 00:12:17.403
<v 3>but let me ask you though,</v>

201
00:12:17.950 --> 00:12:20.200
so these people are all here because they want to be in there.

202
00:12:20.210 --> 00:12:22.210
People who are watching all around the country who wanted to be,

203
00:12:23.140 --> 00:12:25.900
you have a chance to talk to people who might actually be able to make a

204
00:12:25.901 --> 00:12:30.430
difference.
I'm like what is the barrier now?

205
00:12:30.460 --> 00:12:33.570
I mean before there was,
there was almost no conversation about it.

206
00:12:33.580 --> 00:12:36.070
It's like I remember like the digital divide back in the nineties like,

207
00:12:36.080 --> 00:12:37.720
well is every kid going to get a computer?

208
00:12:37.900 --> 00:12:41.830
And now kids have computers in their back pockets,
um,
of some kind.

209
00:12:42.280 --> 00:12:45.340
But then a couple of years ago there was a much bigger conversation at least

210
00:12:45.341 --> 00:12:49.280
talking about the need for more inclusion,
more diversity.
Um,
and you,

211
00:12:49.540 --> 00:12:54.070
you feel some lag between the conversations in the outcomes.
What is in the way,

212
00:12:54.071 --> 00:12:58.090
what are some of the things that you see that may be not obvious during the way?

213
00:13:00.820 --> 00:13:01.290
<v 6>Well,</v>

214
00:13:01.290 --> 00:13:06.290
micro aggressions are huge and that's unfortunately not always visible to

215
00:13:08.291 --> 00:13:12.480
people.
It's very visible to the person experiencing that microaggression,

216
00:13:12.520 --> 00:13:17.030
but not too much to the outsider.
It might be very easily dismissed.
Um,

217
00:13:17.080 --> 00:13:21.130
but I think in general,
just being,
being an advocate for,

218
00:13:21.430 --> 00:13:24.640
for diverse people at your company and you know,

219
00:13:24.641 --> 00:13:28.090
listening to them and hearing them out and then thinking like,
okay,

220
00:13:28.300 --> 00:13:30.220
like that's interesting.
Like how can I,

221
00:13:30.490 --> 00:13:34.000
how can I help make sure that this person feels more supported and actually

222
00:13:34.001 --> 00:13:34.834
wants to stay

223
00:13:34.900 --> 00:13:39.650
<v 3>okay.
But I'm not going to let you go with that.
Okay,
good.
That's nice.
Um,
but</v>

224
00:13:42.040 --> 00:13:46.540
you basically were saying,
just be a good person.
Just be a good person.

225
00:13:46.660 --> 00:13:48.130
Don't be racist,
don't be sad.

226
00:13:48.780 --> 00:13:53.500
But I'm saying that I think most people see themselves as good people and

227
00:13:53.501 --> 00:13:57.160
they're trying,
but something gets in the way.
What,
what's,
what's stopping it?

228
00:13:58.860 --> 00:14:02.530
<v 0>So one of the things I think I talk a lot about designing for empathy and I</v>

229
00:14:02.531 --> 00:14:04.690
think that you can do that with your own life as well.

230
00:14:04.930 --> 00:14:08.500
I really believe that privilege can actually disadvantage to privileged.

231
00:14:08.560 --> 00:14:13.300
And so it takes stepping out of your comfort zone and experiencing other things.

232
00:14:13.301 --> 00:14:14.530
And,
and actually,
you know,

233
00:14:14.590 --> 00:14:17.020
I talk about discomfort and a lot of people aren't comfortable being

234
00:14:17.021 --> 00:14:21.010
uncomfortable,
but for example,
I'm uncomfortable every single day,
right?

235
00:14:21.190 --> 00:14:22.150
Every single day.

236
00:14:22.151 --> 00:14:26.920
And so it's one of those things where to take a moment of your day out to maybe

237
00:14:26.921 --> 00:14:30.280
go into an inner city and tried to help Mike give you an experience and a

238
00:14:30.281 --> 00:14:33.740
perspective that might impact your work differently.
Right?
Or

239
00:14:33.960 --> 00:14:36.340
<v 3>You said designing for empathy,</v>

240
00:14:36.610 --> 00:14:38.530
<v 0>right?
So go ahead.
Oh,
what,</v>

241
00:14:38.531 --> 00:14:41.760
what I mean by that is like I'm in my own work,

242
00:14:41.790 --> 00:14:45.630
I always am trying to figure out a way to necessarily sympathize with people,

243
00:14:45.631 --> 00:14:48.960
but empathize,
really understanding what their needs are.

244
00:14:48.961 --> 00:14:52.220
So I'm designing ethical and social logically

245
00:14:52.220 --> 00:14:56.690
<v 4>impactful solutions for them,
right?
Not just trying to get it done,
you know?</v>

246
00:14:56.810 --> 00:15:01.730
And I think,
um,
in some ways we think about it externally,

247
00:15:01.731 --> 00:15:03.620
but we can also do it within our own lives.

248
00:15:04.450 --> 00:15:06.490
<v 3>So I'm going to bring a New Zealand,</v>

249
00:15:07.120 --> 00:15:12.120
where I like about what you're saying is that it builds on the earlier points,

250
00:15:13.510 --> 00:15:18.510
which is first of all that there are people who know a lot of stuff.

251
00:15:19.300 --> 00:15:22.840
Like if we're,
if you're to try to solve a problem,
you can,
you know,

252
00:15:22.930 --> 00:15:26.320
take the intro course,
you can read a book or you can also deal with the experts.

253
00:15:26.500 --> 00:15:28.360
So you're saying they're experts.
You need to be dealt with.

254
00:15:28.900 --> 00:15:33.900
You're saying also you need to just continue to just want to be good to your,

255
00:15:33.970 --> 00:15:36.880
to your coworkers and you're saying there's a way you can even change your life

256
00:15:37.210 --> 00:15:39.070
where you're bringing in those and,
but you know,

257
00:15:39.071 --> 00:15:40.780
Dave and I were talking about that before.
Like,
you know,

258
00:15:40.781 --> 00:15:45.340
Oakland's not that far from here.
Um,
but I,
I just think that there must be some,

259
00:15:45.370 --> 00:15:48.490
there must be some things that are not obvious that are stopping that from

260
00:15:48.491 --> 00:15:51.070
happening.
If everything you guys are saying just seems perfectly reasonable,

261
00:15:51.130 --> 00:15:53.650
but I guarantee you in four hours people are going to be doing something.

262
00:15:53.651 --> 00:15:55.060
Not that.
So,
so why?

263
00:15:55.150 --> 00:15:58.300
<v 4>But we're talking about making our products more empathetic and more ethical.</v>

264
00:15:58.580 --> 00:15:58.900
You know,

265
00:15:58.900 --> 00:16:03.010
a lot of what we're talking about is we need more black and Latino creators.

266
00:16:03.011 --> 00:16:03.844
For them.

267
00:16:04.210 --> 00:16:08.620
I'd say the single biggest barrier to more black and Latino creators in tech is

268
00:16:08.621 --> 00:16:11.650
Asian American men.
And want to be clear here.

269
00:16:12.070 --> 00:16:15.640
If you look at our potential for advancement,
CEO,

270
00:16:15.641 --> 00:16:19.600
Google and CEO of Microsoft,
CEO of Dolby,
[inaudible],
CEO of my company,

271
00:16:20.140 --> 00:16:22.570
we are Asian American men,
right?
How many,

272
00:16:22.630 --> 00:16:26.920
how many Asian American men are here but trans up.
Okay.
As a percentage.

273
00:16:26.921 --> 00:16:31.480
How does that compare to the percentage of Asian American men at Google?
Right.

274
00:16:32.380 --> 00:16:34.930
And we don't know our history,
right?

275
00:16:34.931 --> 00:16:38.230
Asian Americans in general don't know we are here because we were welcomed by

276
00:16:38.231 --> 00:16:40.300
black and Latino neighborhoods and communities.

277
00:16:40.780 --> 00:16:42.870
And then we turned our back on them.
You know?

278
00:16:42.880 --> 00:16:45.700
And then there's a history of anti-blackness.
We don't know that history.

279
00:16:45.701 --> 00:16:47.500
Now granted,
some of that's because people are new.

280
00:16:47.560 --> 00:16:48.700
They haven't been in this country that long.

281
00:16:48.701 --> 00:16:51.220
They haven't learned about the civil rights movement,
not excusing it.

282
00:16:51.221 --> 00:16:54.970
But I think there's a way to just say there's learning to do and what happens is

283
00:16:54.971 --> 00:16:57.850
if you look at proportional representation is compared to the population of

284
00:16:57.851 --> 00:16:59.890
California versus population,
a tech companies,

285
00:17:00.430 --> 00:17:05.430
percentage of white employees is roughly proportional to overall population

286
00:17:06.970 --> 00:17:11.350
representation of black and Latino workers is extremely low and most of the gap

287
00:17:11.470 --> 00:17:14.830
is taken up by Asian American workers,
primarily Asian American men.

288
00:17:15.370 --> 00:17:19.600
We are able to advance.
There are issues now,
there are,
you know h one B issues,

289
00:17:19.601 --> 00:17:21.580
there's immigration issues,
there's a lot of disparities.

290
00:17:21.730 --> 00:17:24.970
But the biggest thing we have is we take up a lot of space and we don't give a

291
00:17:24.971 --> 00:17:27.370
lot of voice to the people.
We need to hold the door open.

292
00:17:27.810 --> 00:17:32.810
<v 2>[inaudible] and one of the things I'm</v>

293
00:17:33.920 --> 00:17:36.650
<v 4>mindful of is white folks can't say that.
Cause there'll be afraid to say that.</v>

294
00:17:36.920 --> 00:17:39.080
And I,
and I would encourage you,
if you're white,
don't say it,

295
00:17:39.081 --> 00:17:41.250
it's not your place.
But,

296
00:17:41.780 --> 00:17:46.480
but that means that we have to be doubly loud.
If you are an Asian,
you know,

297
00:17:46.520 --> 00:17:50.700
person in tech to talk about this issue of we take up a lot of space and we

298
00:17:50.701 --> 00:17:52.700
don't fight real hard.
Um,

299
00:17:53.280 --> 00:17:57.800
and we build systems that exclude and we don't interrogate them.
So,
you know,

300
00:17:57.810 --> 00:17:59.310
I think that's a thing that,

301
00:17:59.760 --> 00:18:02.130
how many of you have heard an Asian American man and say this issue,

302
00:18:02.131 --> 00:18:06.210
talk about anti-blackness in our community.
Like there's three hands,

303
00:18:06.240 --> 00:18:10.530
four and you follow me on Twitter doesn't count.
Um,
the,
the,

304
00:18:10.740 --> 00:18:14.310
that's the,
that's,
I mean,
I think that is one of the most key issues here.

305
00:18:14.400 --> 00:18:15.830
And we,

306
00:18:15.920 --> 00:18:19.050
we don't have a vocabulary for it and we don't even have a way to have that

307
00:18:19.051 --> 00:18:23.910
conversation without it being mediated by essentially white media.
Right.

308
00:18:23.911 --> 00:18:27.000
And it's like we don't have that dialogue between various Asian American

309
00:18:27.001 --> 00:18:29.030
communities and Black and Latino communities to address.

310
00:18:29.600 --> 00:18:33.390
<v 3>Well,
first of all,
I,
uh,
I'm rarely speechless,</v>

311
00:18:33.391 --> 00:18:37.600
so I'm right.
But we have seven more minutes.
Um,

312
00:18:38.570 --> 00:18:41.770
you know,
this question around solidarity,
mm.
Uh,

313
00:18:41.910 --> 00:18:46.050
is very interesting.
Um,

314
00:18:46.920 --> 00:18:50.760
one of the great achievements within the African American community over the

315
00:18:50.761 --> 00:18:55.710
past 15 to 20 years has been,
you know,
frankly,
because of leaders like,

316
00:18:56.050 --> 00:18:58.830
uh,
Barack and Michelle Obama and others,

317
00:19:00.150 --> 00:19:01.770
African Americans have moved

318
00:19:03.690 --> 00:19:08.690
at least so far to a position of solidarity with the immigrant community.

319
00:19:09.770 --> 00:19:13.830
I including with the Latino community
again,

320
00:19:13.831 --> 00:19:16.650
because we don't talk about this stuff often in publicly business.

321
00:19:16.651 --> 00:19:20.280
You went there,
uh,
in the 1990s here in California,

322
00:19:20.281 --> 00:19:25.170
there was a big fight between black folk and Latinos,

323
00:19:25.830 --> 00:19:27.480
the also on the east coast.

324
00:19:27.481 --> 00:19:30.570
But I was out here for it and it was very bad and it was,

325
00:19:30.840 --> 00:19:35.840
they are taking our jobs and what was happening was seeing more Latinos in the

326
00:19:36.541 --> 00:19:41.070
service sector and in construction and African Americans felt like we were being

327
00:19:41.130 --> 00:19:44.790
displaced
and it was ugly.

328
00:19:45.990 --> 00:19:48.090
And African Americans,

329
00:19:49.230 --> 00:19:52.220
leaders of Barack Obama's,
um,

330
00:19:52.280 --> 00:19:55.530
caliber stepped in and said,
we're not going to do this.

331
00:19:56.220 --> 00:19:59.110
We're not going to be used and trekked to

332
00:20:00.660 --> 00:20:05.660
essentially create Jim crow and Brown face for this whole population of people

333
00:20:07.170 --> 00:20:09.390
who are here and they're working hard,
et cetera.

334
00:20:09.391 --> 00:20:13.230
And yet it was tough in the black community for awhile.
Also,

335
00:20:13.260 --> 00:20:18.110
the question around LGBT
Lgbtq,

336
00:20:18.120 --> 00:20:20.940
I,
Yay,
that was a tough question.

337
00:20:20.941 --> 00:20:25.530
And the black community in the 1990s especially because so much of our struggle

338
00:20:25.531 --> 00:20:28.460
comes out of the church and it was a,

339
00:20:28.490 --> 00:20:33.490
it was an issue and we have gotten to a point in the black community where solid

340
00:20:34.451 --> 00:20:39.330
there obviously you have a leg gay folk as well,

341
00:20:39.331 --> 00:20:43.710
so it's not like a strictly like a racial issue.
But this question around are,

342
00:20:43.950 --> 00:20:45.910
you know,
heterosexual,
cisgendered,

343
00:20:45.960 --> 00:20:50.350
black folk going to be in with our own folk and other,
and that was a big fight.

344
00:20:51.580 --> 00:20:55.300
You're now at a place in the black community where I'm a preacher,

345
00:20:55.301 --> 00:20:59.770
my preach against that issue,
that preacher will not march,

346
00:21:00.250 --> 00:21:03.010
that preacher will not go and actively attack.

347
00:21:03.580 --> 00:21:08.580
So when black folks talk about this heartbreak around a lack of solidarity,

348
00:21:09.880 --> 00:21:14.620
it's not out of no context.
It's out of saying,
here we are,

349
00:21:14.621 --> 00:21:19.060
a community law.
I'm a ninth generation American,
ninth generation American.

350
00:21:19.240 --> 00:21:22.510
I'm the first person in my family that was born with all my rights,

351
00:21:22.540 --> 00:21:25.300
recognized by this government.
In nine generations.

352
00:21:25.301 --> 00:21:30.301
I was born in 1968 my cousin Kenny was born in 64 before they signed the last

353
00:21:30.341 --> 00:21:32.610
civil rights bill.
So I'm the first person in Nigeria.

354
00:21:32.640 --> 00:21:36.010
The of with all my rights in our community and the native American community

355
00:21:36.011 --> 00:21:40.960
continues to be the lowest in every area.
And yet on a moral question,

356
00:21:42.280 --> 00:21:46.930
do we stand with the Latinos when people want to deport them on a moral
question?

357
00:21:47.140 --> 00:21:51.280
Do we think marriage should be for everybody?
Uh,
you look at our voting behavior,

358
00:21:51.310 --> 00:21:55.320
we are the most progressive on those issues.
And so it's all dirty.

359
00:21:55.330 --> 00:21:57.870
Should be a two way street.
Yeah.
And,

360
00:21:58.440 --> 00:22:02.080
and I think that that's a part of the heartbreak for our community to be seen in

361
00:22:02.081 --> 00:22:06.010
such a negative light.
Often you're the criminals.
You're the scary people,

362
00:22:06.040 --> 00:22:08.650
you're the angry people,
you're the,
the threat.

363
00:22:09.130 --> 00:22:13.420
But when you look at our performance and you look at the public role that we

364
00:22:13.421 --> 00:22:17.470
play,
uh,
I think some of that stuff is not merited.
And so I,

365
00:22:17.500 --> 00:22:20.710
I want us to find our way to solidarity.
And frankly,

366
00:22:20.711 --> 00:22:24.220
I want to find our way to solidarity with the best of the Trump voters who felt

367
00:22:24.221 --> 00:22:27.550
left out of this love circle because we have a lot of white men who are feeling

368
00:22:27.551 --> 00:22:30.360
left out of this love circle and we can tell them,
you have,
well,
you know,

369
00:22:30.430 --> 00:22:33.020
check your privilege or we can say,
you know,

370
00:22:33.100 --> 00:22:35.980
check in with us and let's keep talking.
And so anyway,
I just,
I just,

371
00:22:35.981 --> 00:22:39.400
I just want to say that,
uh,
this to me,
this,
this particular thing,

372
00:22:39.401 --> 00:22:43.680
I've never anybody say that when you just said,
um,
and uh,
uh,

373
00:22:43.690 --> 00:22:47.200
this is going to be an age in an era in the next four years at least where

374
00:22:47.201 --> 00:22:51.760
solidarity is gonna be put to the test.
Um,
so that was not in my questions.

375
00:22:52.480 --> 00:22:56.260
Um,
um,
so look,
I want to,

376
00:22:56.261 --> 00:22:59.680
I just want to come back to you.
I mean,
here you are.
What a journey.

377
00:23:01.330 --> 00:23:05.730
I mean,
you're one of the best in your field.
I mean,
without question.
Uh,

378
00:23:06.020 --> 00:23:10.330
you've done so much stuff that people use every day have heard of,

379
00:23:10.690 --> 00:23:14.530
um,
uh,
you're a legend.
Keep going.

380
00:23:16.930 --> 00:23:21.060
Um,
and then you stepped on a rake,
right?

381
00:23:21.750 --> 00:23:25.350
Any southerners?
He stepped,
he stepped on a rake.
You ever stepped on a rake?

382
00:23:28.520 --> 00:23:29.353
<v 2>Okay.</v>

383
00:23:30.390 --> 00:23:34.610
<v 3>And you didn't become
this bitter.</v>

384
00:23:34.670 --> 00:23:38.540
How dare they?
Well,
maybe you did it first,
but you at least do it right now.

385
00:23:38.960 --> 00:23:39.800
You aren't showing up.

386
00:23:39.801 --> 00:23:44.750
Is that guy who's like angry and bitter and defensive and how dare they,

387
00:23:44.751 --> 00:23:48.440
and they're playing the race card and I'm sick of this.
You didn't become bitter.

388
00:23:48.950 --> 00:23:53.950
You became better in a way that people rarely do on any side of these issues.

389
00:23:54.800 --> 00:23:55.633
Why and how?

390
00:23:58.170 --> 00:24:01.430
<v 5>I think we should just close with that.
Um,</v>

391
00:24:02.040 --> 00:24:06.180
I think it gets down to humility sort of,
you know,

392
00:24:06.270 --> 00:24:11.040
I think so many of us are good intention and I think your mandatory is a really

393
00:24:11.041 --> 00:24:15.750
great question because we have those who have the most work to do,

394
00:24:15.780 --> 00:24:18.600
may not know they have any work to do and they're in,

395
00:24:18.601 --> 00:24:20.340
they're not the ones that are showing up.

396
00:24:20.520 --> 00:24:23.520
And I would say I fell into that category.
I felt,
I,
you know,

397
00:24:23.521 --> 00:24:28.521
I grew up in Detroit and lived in Oakland and I felt like most causes,

398
00:24:29.071 --> 00:24:30.270
I was on the right side of,

399
00:24:30.600 --> 00:24:34.920
and I didn't understand that the work started at home until I was humbled.

400
00:24:34.921 --> 00:24:35.700
It took,
you know,

401
00:24:35.700 --> 00:24:39.450
getting hit upside my head with the rake to sort of knock some sense into me or

402
00:24:39.451 --> 00:24:42.150
to wake me up.
And,
um,

403
00:24:42.690 --> 00:24:47.130
I think that's the way to approach this with humility.

404
00:24:47.130 --> 00:24:50.580
It's not a problem that needs to be fixed.
It's not sort of,
you know,

405
00:24:50.581 --> 00:24:54.090
we'll put a patch on the code and you know,
you know,
it won't be racist anymore.

406
00:24:54.300 --> 00:24:58.620
It's a journey we're going to take.
And,
um,
we have to approach it with humility.

407
00:24:58.870 --> 00:25:03.690
Um,
Google tends to like hard problems and once that may take 10 years or not,

408
00:25:03.720 --> 00:25:05.100
you know,
things we shy away from.

409
00:25:05.280 --> 00:25:08.610
So I think we're in this to win it and for the long haul.

410
00:25:08.820 --> 00:25:12.450
And I feel that support from the company that this is something we're going to

411
00:25:12.451 --> 00:25:14.670
do together.
And,
uh,
you know,

412
00:25:14.700 --> 00:25:18.150
I'm grateful to have an opportunity to be at Google in a company that cares as

413
00:25:18.151 --> 00:25:19.140
much as I know we do.

414
00:25:19.680 --> 00:25:20.190
<v 2>Okay.</v>

415
00:25:20.190 --> 00:25:25.190
[inaudible]

416
00:25:26.490 --> 00:25:28.950
<v 3>well,
I see that we were counting down almost 20 seconds,</v>

417
00:25:28.951 --> 00:25:31.740
so I think that means we've got to go into the questions and the answers.
And so,

418
00:25:31.770 --> 00:25:35.360
um,
uh,
just so you know,
you're going to get the last word cause we,

419
00:25:35.440 --> 00:25:38.490
we started with you so everybody get a chance to answer,
but just get ready.
So,

420
00:25:39.240 --> 00:25:41.850
um,
uh,
so,
um,

421
00:25:41.880 --> 00:25:44.580
how do somebody telling me how we do the questions and answers in this,

422
00:25:44.820 --> 00:25:45.653
in this joint?

423
00:25:47.230 --> 00:25:48.840
<v 5>We've got a couple of microphones acts up.</v>

424
00:25:49.480 --> 00:25:52.720
<v 3>Okay,
good.
So we'll start with you on the paper.
Okay.</v>

425
00:25:52.721 --> 00:25:56.410
So folks can line up on the microphone.
Thank you.
Michael Skolnik by the way,

426
00:25:56.470 --> 00:25:57.780
Kevin,
Michael Skolnik another random

427
00:25:58.680 --> 00:26:01.540
<v 2>great leaders.
You know,</v>

428
00:26:03.680 --> 00:26:08.480
<v 3>often when we talk about humility,
I think,
especially guys here,</v>

429
00:26:09.200 --> 00:26:12.510
be weak.
I think guys here,

430
00:26:14.480 --> 00:26:17.060
you know,
you're going to be a less than.

431
00:26:18.980 --> 00:26:22.940
But when I think about some of the strongest people that I know or that I've

432
00:26:22.941 --> 00:26:26.120
studied,
you know,
and Nelson Mandela certainly was not weak,

433
00:26:26.810 --> 00:26:30.800
but there was that deep sense of humility that shines through.

434
00:26:31.270 --> 00:26:34.760
And I think that we need some models where we can still be,
you know,

435
00:26:34.761 --> 00:26:38.270
guys like to be guys.
I mean,
often not all guys,
but a lot of guys,
you know,

436
00:26:38.271 --> 00:26:43.190
I'm talking about and there needs to be some opportunity for us to understand

437
00:26:43.200 --> 00:26:46.560
that the call is for you to be greater.

438
00:26:47.190 --> 00:26:50.700
We're not trying to pull anybody down.
We're not just calling people out.

439
00:26:50.701 --> 00:26:54.540
We're calling people up to an even better,
uh,

440
00:26:54.570 --> 00:26:57.300
expression of who we all can be together.
So look,

441
00:26:57.390 --> 00:27:01.410
that said little sermonette over.
Um,

442
00:27:02.580 --> 00:27:04.800
so I got two questions.
One says,

443
00:27:05.160 --> 00:27:09.330
is our country currently in a dangerous situation where the term racism has

444
00:27:09.331 --> 00:27:10.740
actually been abused?

445
00:27:11.460 --> 00:27:16.460
And how can we determine a person is racist or just a victim of poisoning the

446
00:27:16.861 --> 00:27:19.950
well,
for instance,
half the population voted for Trump this year.

447
00:27:20.160 --> 00:27:24.360
Are they all racist?
Welcome back,
David.
By the way,
David Jones back.

448
00:27:24.420 --> 00:27:29.160
Give him a round applause.
Um,
the jump ball question,

449
00:27:29.170 --> 00:27:33.620
Mrs d,
Somebody asking a deep question is,
has the term racist had been,

450
00:27:33.621 --> 00:27:35.790
you've been abused,
um,

451
00:27:35.930 --> 00:27:38.490
calc you tell somebody who's actually a racist or just a victim of,

452
00:27:38.491 --> 00:27:42.570
of of the circumstance.
Are All the Trump voters racist jump ball,
anybody?

453
00:27:43.600 --> 00:27:45.250
<v 4>Well,
this is that,
that structural issue,</v>

454
00:27:45.251 --> 00:27:50.251
which is like calling a person a racist isn't useful attacking systems of racism

455
00:27:50.680 --> 00:27:54.730
is,
and so,
you know,
if you say to somebody,
are you a racist?
It's like,
yeah,

456
00:27:54.731 --> 00:27:57.760
we're all participatory to different degrees and systems that are racist.

457
00:27:57.910 --> 00:28:00.580
The question is whether you're rooting for their systems are dismantling them.

458
00:28:01.180 --> 00:28:03.130
Right?
And so the,
the,

459
00:28:03.160 --> 00:28:05.320
I think a lot of the rhetoric or like everybody gets called a racist,

460
00:28:05.321 --> 00:28:09.670
I think that comes down to mostly white folks feeling defensive,
right?

461
00:28:09.671 --> 00:28:11.890
And it's the like,
why do you use this term all the time?
So I'm like,

462
00:28:11.891 --> 00:28:15.910
I actually have pulled back on like saying you're a racist just because it's

463
00:28:15.950 --> 00:28:18.490
immediately he's going to put up the defensive wall and it's not useful.

464
00:28:18.491 --> 00:28:19.960
And I never say,
you know,

465
00:28:19.990 --> 00:28:24.760
if I say our criminal justice system is structurally racist,

466
00:28:24.761 --> 00:28:27.060
do you support fixing that?
Uh,

467
00:28:27.120 --> 00:28:29.080
you can have a meaningful conversation with people.

468
00:28:29.081 --> 00:28:30.550
So I think some of this is just about the rhetoric now.

469
00:28:30.551 --> 00:28:32.410
I can't expect everybody to take the time to learn.

470
00:28:32.680 --> 00:28:37.090
Here's the perfect way to in a 140 characters to not make somebody defensive.

471
00:28:37.091 --> 00:28:41.110
But if you have a platform,
I think that's a really important bit of work to do.

472
00:28:42.640 --> 00:28:46.630
<v 1>Can I chime in for a second?
Yeah.
So I think that's right.
And uh,</v>

473
00:28:46.720 --> 00:28:49.570
well one of the interesting things that I had noticed over this,

474
00:28:49.571 --> 00:28:54.571
this campaign season was how a lot of the folks sort of inner around Trump and

475
00:28:55.991 --> 00:28:58.270
sort of supporting him on TV and so forth.

476
00:28:58.660 --> 00:29:03.660
If you started criticizing him on the basis of sort of racial issues and how

477
00:29:04.451 --> 00:29:07.810
he's related to the black community,
Latino community,
the Muslim community,
uh,

478
00:29:08.290 --> 00:29:12.550
they'd come back with your being racist.
Right?
So it's,

479
00:29:12.610 --> 00:29:15.580
I think it's very important to be able to still engage with that.

480
00:29:15.581 --> 00:29:17.120
So a lot of times,
you know,
you,

481
00:29:17.440 --> 00:29:21.150
you sort of are deescalating and sort of,
you know,
it's like,

482
00:29:21.240 --> 00:29:25.630
it feels like unilateral disarmament a little bit amongst know progressive folks

483
00:29:25.631 --> 00:29:28.420
that say,
well,
I'm not going to call anybody racist.
I'm not going to call it out.

484
00:29:28.510 --> 00:29:32.110
But yet other folks or are sort of using this very aggressive tactic of,

485
00:29:32.320 --> 00:29:35.020
you know,
as soon as you make any criticism,
you're being divisive.

486
00:29:35.021 --> 00:29:36.450
You're being a racist.
Yeah.
So

487
00:29:36.920 --> 00:29:39.800
<v 3>they think that you're never happened to me on television.
I don't know.</v>

488
00:29:42.070 --> 00:29:42.903
<v 2>Yeah.</v>

489
00:29:43.170 --> 00:29:44.003
<v 3>Other voices.</v>

490
00:29:44.130 --> 00:29:46.950
<v 6>Yeah.
I was just gonna say that.
Yeah,
that happens to me a lot.</v>

491
00:29:46.951 --> 00:29:49.500
So a lot of what I cover is around diversity and inclusion.

492
00:29:49.530 --> 00:29:51.570
And if ever I bring up,
you know,

493
00:29:51.571 --> 00:29:55.770
issues of race or sexism or homophobia,
I'm,

494
00:29:55.860 --> 00:29:58.470
I get called the racist.
I get called the sexist one.

495
00:29:58.471 --> 00:30:02.520
I get called the homophobic one and it's,
I don't know.
But that's,

496
00:30:02.760 --> 00:30:04.920
that's just the Internet for you.
And

497
00:30:06.870 --> 00:30:07.800
I wonder,

498
00:30:09.010 --> 00:30:10.030
<v 2>is there,</v>

499
00:30:11.260 --> 00:30:15.430
<v 3>is there something that we're,
that we could do better?
Yeah,</v>

500
00:30:15.460 --> 00:30:16.920
that's my question because,

501
00:30:18.110 --> 00:30:18.820
<v 2>yeah.</v>

502
00:30:18.820 --> 00:30:19.930
<v 3>You know,
there's this,</v>

503
00:30:19.990 --> 00:30:24.990
there's this wonderful man named Jeffrey Lord who I got chance to spend a lot of

504
00:30:25.481 --> 00:30:27.880
time with on television.
These actually is a wonderful person,

505
00:30:27.881 --> 00:30:32.881
but on this issue we would just butt heads and um,

506
00:30:34.540 --> 00:30:37.420
yeah,
he would say,
well,
you know,
the Democrats started the clan,

507
00:30:39.730 --> 00:30:43.410
<v 2>like what does that have to do with anything?
But</v>

508
00:30:43.560 --> 00:30:46.590
<v 3>he just,
that was so important for him and we just couldn't figure it out.</v>

509
00:30:46.591 --> 00:30:50.010
So I just wonder,
and I wonder if I might,
you know,

510
00:30:51.500 --> 00:30:54.750
risk your good standing that you've developed your being woke.
Yeah,

511
00:30:55.370 --> 00:30:56.203
<v 2>yeah.</v>

512
00:30:58.790 --> 00:30:59.960
<v 3>Not Speaking for yourself,</v>

513
00:31:00.470 --> 00:31:04.100
but speaking for your old self and speaking for people who might be like your

514
00:31:04.101 --> 00:31:07.520
old self and not to,
you know,

515
00:31:08.470 --> 00:31:10.750
worship white fragility too much,

516
00:31:12.390 --> 00:31:13.250
<v 2>but</v>

517
00:31:13.440 --> 00:31:17.280
<v 3>what's more effective and with less effective as we try to reach out on his</v>

518
00:31:17.281 --> 00:31:19.050
question?
It's not on my paper.
Just honest question.

519
00:31:21.800 --> 00:31:23.720
<v 5>Well,
I liked it.
You know,</v>

520
00:31:23.780 --> 00:31:28.520
I think labeling a is divisive and I think,
you know,

521
00:31:28.521 --> 00:31:32.840
the minute somebody is back on their heels,
they're not listening.
Right?
So,
um,

522
00:31:32.870 --> 00:31:36.420
I think confrontational approaches towards,
um,

523
00:31:36.710 --> 00:31:40.610
this rarely lead to the outcomes that we want.
Um,

524
00:31:40.670 --> 00:31:45.560
and I think your advocacy for an approach of inclusion and love,
like,

525
00:31:45.860 --> 00:31:47.420
you know,
empathy is a great word,

526
00:31:47.421 --> 00:31:52.421
sort of appeal to people's humanness who amongst us hasn't been otherized in

527
00:31:52.641 --> 00:31:55.280
some scenario,
no matter how much privilege you had,

528
00:31:55.390 --> 00:31:57.860
you were the last person picked on the team or whatever.

529
00:31:57.980 --> 00:32:02.980
It's a human experience to experience being ostracized and hurt and left out and

530
00:32:03.141 --> 00:32:06.110
not included.
And,
um,
so I think,
you know,

531
00:32:06.111 --> 00:32:10.940
giving people that benefit of the doubt and understanding that even when you

532
00:32:10.941 --> 00:32:13.850
disagree,
you don't have to otherwise or you know,

533
00:32:13.970 --> 00:32:18.680
turn people into enemies is both going to lead to more pleasant journey but also

534
00:32:18.681 --> 00:32:21.230
lead to better outcomes.
So I think,
you know,

535
00:32:21.231 --> 00:32:25.850
it's very hard to be grounded in that love and approach people you vehemently

536
00:32:25.851 --> 00:32:30.620
disagree with who are looking to take away your rights and,
and still,
you know,

537
00:32:30.621 --> 00:32:34.520
have the patience and determination and steadiness to sort of approach it that

538
00:32:34.521 --> 00:32:37.160
way.
But I think history shown,
it's the only thing that works.

539
00:32:37.700 --> 00:32:40.670
<v 3>Yeah.
I'll say something about about that because it's,
um,</v>

540
00:32:41.700 --> 00:32:46.100
w what you're saying feels both unfair and true.

541
00:32:46.630 --> 00:32:47.650
Hmm.
It's,

542
00:32:47.740 --> 00:32:52.740
it feels unfair that the oppressed person has to be able to develop this

543
00:32:56.031 --> 00:32:59.930
capacity to,
to help the oppressor do better.

544
00:33:00.800 --> 00:33:02.270
And it also feels very true.

545
00:33:03.140 --> 00:33:06.770
And I think that's part of the pain of being one down.

546
00:33:07.640 --> 00:33:11.780
Like when you're one down,
you have all these sore spots.
When you're one up,

547
00:33:11.781 --> 00:33:15.650
you have all these blind spots.
So the person who's one up on race,

548
00:33:15.651 --> 00:33:17.450
or if you're white,
I mean,
I'm like,

549
00:33:17.570 --> 00:33:20.600
I'm the most privileged person in the world has happened to be black,
but I'm,

550
00:33:20.990 --> 00:33:23.870
but I'm one up on gender.
I'm one up on being heterosexual.

551
00:33:23.871 --> 00:33:27.230
I'm one up on education.
I'm one up on knowing David Drum drumming.

552
00:33:27.350 --> 00:33:31.960
I'm one of them.
A lot of things,
man,
except the boats.
But,

553
00:33:32.050 --> 00:33:34.340
and so in those areas in which I'm went up,

554
00:33:34.341 --> 00:33:36.890
I have a lot of blind spots and make a lot of mistakes,

555
00:33:37.280 --> 00:33:41.840
but the area that I'm one down on race,
I have a lot of sore spots,

556
00:33:42.470 --> 00:33:42.861
you know,

557
00:33:42.861 --> 00:33:47.090
and it really hurts and I've had to figure out on national television in real

558
00:33:47.091 --> 00:33:51.170
time how to be hurt and how to be real about it.

559
00:33:51.640 --> 00:33:52.520
And at the same time,

560
00:33:52.521 --> 00:33:56.680
how to try to keep the conversation here and how not to try to call people out

561
00:33:56.681 --> 00:33:59.510
but try to call people up.
And I just,
what?
What I,

562
00:33:59.560 --> 00:34:04.010
the only thing I ask is that when people who are one up ask that of people,

563
00:34:04.570 --> 00:34:05.840
they,
they know what they're asking.

564
00:34:06.660 --> 00:34:10.850
They're asking somebody to do something that I don't know if you can do,

565
00:34:11.510 --> 00:34:16.220
you're asking somebody to do something that I don't know.
You know,

566
00:34:16.250 --> 00:34:18.890
you shouldn't have to be Gandhi at work.
I mean,

567
00:34:18.891 --> 00:34:22.610
you shouldn't have to be Dr. King just to go to work.

568
00:34:22.940 --> 00:34:26.090
And yet I do feel that that's sometimes what's what's being asked.

569
00:34:26.270 --> 00:34:28.640
But I love your response and I'll tell you what,

570
00:34:28.641 --> 00:34:33.260
you guys have an incredible asset here in you because you've gone through the

571
00:34:33.261 --> 00:34:35.720
journey and you know,

572
00:34:35.721 --> 00:34:40.721
and I think that one danger is that we come out of hurt spots and sore spots.

573
00:34:41.360 --> 00:34:44.360
So even when somebody has gone through the journey,
we're still almost ready to,

574
00:34:44.361 --> 00:34:48.490
you know,
where's he going to,
you know,
when it was like,

575
00:34:49.370 --> 00:34:53.290
and I do think that there should be some kind of catch all good badges or

576
00:34:53.291 --> 00:34:55.580
something once people have gotten to a certain level of,
whoa.

577
00:34:58.960 --> 00:35:02.920
So we can control so we can,
I think the convention is,
we give them cookies.

578
00:35:03.040 --> 00:35:05.150
Cookies.
Yeah.
That's good.
You're welcome.

579
00:35:08.230 --> 00:35:10.780
Michael is calling going.
Fire me if I get this next question.
Hold on.
I was like,

580
00:35:11.140 --> 00:35:15.530
um,
how does it proliferate now?
This is deep.
How,

581
00:35:16.780 --> 00:35:19.660
oh,
just go to the audience.
No more questions.
Okay.

582
00:35:20.080 --> 00:35:21.850
Your new beautiful you and the blue.

583
00:35:23.230 --> 00:35:26.110
<v 7>Well,
thanks for coming.
Oh,
Van Jones really enjoyed your talk.</v>

584
00:35:27.100 --> 00:35:31.420
So my question is that it's been said that the only thing that's worse than

585
00:35:31.421 --> 00:35:35.140
people talking about you is people not talking about you.

586
00:35:36.040 --> 00:35:40.410
And for a whole year,
every day,
every week,
every hour,

587
00:35:40.920 --> 00:35:45.210
CNN has been talking only about Donald Trump for better or for worse,

588
00:35:45.211 --> 00:35:48.120
for whatever reason.
I,
I think for all the wrong reasons,
but still,

589
00:35:48.420 --> 00:35:52.260
that's all that I ever heard.
Do you think that CNN,
the networks,

590
00:35:52.261 --> 00:35:56.490
like it unwittingly contributed to Donald Trump's victory by giving it more

591
00:35:56.500 --> 00:35:58.110
publicity than you really deserved?

592
00:36:04.620 --> 00:36:05.850
<v 3>Yes.
Next question.</v>

593
00:36:08.480 --> 00:36:09.080
<v 2>Okay.</v>

594
00:36:09.080 --> 00:36:12.290
<v 7>Thank you for coming.
It's been a great chat.</v>

595
00:36:13.010 --> 00:36:16.400
I wanted to ask the question.
I was hoping you would sort something out for me,

596
00:36:16.790 --> 00:36:21.790
which is that the narrative seems to be now that the election was lost as a

597
00:36:22.071 --> 00:36:26.240
result of economic insecurity by those folks in the belt,

598
00:36:26.241 --> 00:36:29.670
Middle Midwest.
But what was Obamacare?

599
00:36:29.690 --> 00:36:34.130
What was the auto bail out if it wasn't to benefit these people?

600
00:36:34.140 --> 00:36:39.140
And we continue this narrative [inaudible] Bros with the election of Bush,

601
00:36:39.830 --> 00:36:44.690
uh,
against Gore and Kerry.
This whole idea that the Democrats can't and,
uh,

602
00:36:44.780 --> 00:36:48.430
don't speak well to these,
uh,
constituents.
And I,
I,

603
00:36:48.500 --> 00:36:51.920
I want you to sort that out for me because it just doesn't seem accurate.

604
00:36:51.980 --> 00:36:53.480
It seems like we're hiding.

605
00:36:54.260 --> 00:36:55.093
<v 3>Um,</v>

606
00:36:55.180 --> 00:37:00.180
if you look at the counties that were blue for Obama that went red for Trump,

607
00:37:01.900 --> 00:37:06.900
there is a direct correlation with the most recent wave of factory closures for

608
00:37:10.001 --> 00:37:12.720
which we did not.
We did that.
We didn't respond to you.

609
00:37:13.990 --> 00:37:15.670
Now what I will say is simply this,

610
00:37:17.470 --> 00:37:21.970
the fact that we then blame Hillary Clinton is bizarre.

611
00:37:22.840 --> 00:37:23.673
Um,

612
00:37:23.980 --> 00:37:28.210
the reality is a Republican's wouldn't support President Obama on anything.

613
00:37:28.920 --> 00:37:32.230
Uh,
and they,
and they were rewarded for that.
Uh,

614
00:37:32.320 --> 00:37:36.730
President Obama tried to do tax cuts for small businesses in the American jobs

615
00:37:36.731 --> 00:37:40.000
act,
and the Republicans stopped him from giving tax cuts to,

616
00:37:40.001 --> 00:37:43.900
to choose small businesses because they just didn't want him to be successful.

617
00:37:44.440 --> 00:37:48.580
The president wanted to put forward infrastructure,
they called it socialism.

618
00:37:49.000 --> 00:37:52.360
Now Trump wants to do what they call it patriotism.
It's the same bill.

619
00:37:52.450 --> 00:37:56.860
It's lily the same approach.
So you are correct that it's messy.

620
00:37:56.861 --> 00:37:58.540
It's a lot more complicated than it is,

621
00:37:58.990 --> 00:38:02.730
but I think that it is also true if you go to those counties.

622
00:38:02.760 --> 00:38:05.710
I went to the county because I don't do it.
Listen,
I love the media,
but you know,

623
00:38:05.711 --> 00:38:08.020
I worked here.
I want to get my own taste of it.

624
00:38:08.800 --> 00:38:13.210
It is in fact true that you had a large number of people who were willing to

625
00:38:13.211 --> 00:38:15.220
vote for Obama and vote for him twice.

626
00:38:15.580 --> 00:38:20.580
Who in this campaign didn't see themselves reflected and they didn't feel,

627
00:38:21.400 --> 00:38:24.970
listen,
there are a bunch of people who were delighted by that stuff,

628
00:38:25.000 --> 00:38:28.630
even secretly,
but I think a lot of those people were baked into the cake anyway.

629
00:38:29.110 --> 00:38:33.880
It's where was the swing and my concern is that I don't want you to think that

630
00:38:33.881 --> 00:38:38.260
the swing came from racial resentment.
The swing did not come from that.

631
00:38:38.500 --> 00:38:41.530
The swing came from a diff connection with that candidate,

632
00:38:41.920 --> 00:38:44.880
with the democratic narrative of inclusion that felt that didn't,

633
00:38:44.890 --> 00:38:47.080
it didn't let them up with it.
They want it to be.

634
00:38:47.440 --> 00:38:49.240
And the economic anxiety that is true,

635
00:38:49.300 --> 00:38:53.170
which means that Trump is probably worse than you fear,

636
00:38:53.410 --> 00:38:56.230
but Trump's voters are probably better than you fear.

637
00:38:56.500 --> 00:38:57.490
And I want you to know that.

638
00:38:58.290 --> 00:38:59.123
<v 2>Right.</v>

639
00:39:00.200 --> 00:39:02.240
<v 8>Hi.
Hi Mr. Jones.</v>

640
00:39:02.241 --> 00:39:05.540
My family was actually at the Muslim Public Affairs Council dinner yesterday

641
00:39:05.541 --> 00:39:08.870
where you spoke,
so thank you for your presence there.
And also Megan,

642
00:39:08.900 --> 00:39:13.460
good to see you.
She was my colleague.
Do
I?
Uh,

643
00:39:13.880 --> 00:39:17.480
I wanted to actually talk,
I hear a lot about fighting for certain causes,

644
00:39:17.660 --> 00:39:20.600
but I think the fight is often what causes the biggest divide.

645
00:39:20.630 --> 00:39:24.380
How do we actually have those face to face conversations with people that we

646
00:39:24.381 --> 00:39:26.690
probably don't either,

647
00:39:26.750 --> 00:39:30.650
either know that that's their perspective or we don't meet these people.

648
00:39:30.651 --> 00:39:35.651
I think a lot of times having conversations online can be very tough to actually

649
00:39:35.691 --> 00:39:36.800
get anything through.

650
00:39:37.070 --> 00:39:40.220
How do we have those conversations with empathy and really understand someone

651
00:39:40.221 --> 00:39:41.330
else's perspective?

652
00:39:43.290 --> 00:39:47.070
<v 5>Well,
I recently saw some work,
uh,
that people are doing.</v>

653
00:39:47.071 --> 00:39:51.660
And I also saw some research from MIT around this that uses machine learning

654
00:39:51.661 --> 00:39:55.710
when people are online and they type in a comment that may be bullying or

655
00:39:55.711 --> 00:39:57.780
offensive sometimes inadvertently,

656
00:39:57.990 --> 00:40:01.650
it puts a little pop up up there through machine learning and basically says

657
00:40:01.651 --> 00:40:06.651
what you're about to post may actually be hurtful or inflammatory or derogatory.

658
00:40:07.200 --> 00:40:11.040
Are you sure you want to proceed?
You know,
click yes if you want to,

659
00:40:11.041 --> 00:40:15.180
if you're a troll,
you know.
Um,
but a lot of people actually,

660
00:40:15.181 --> 00:40:18.090
when there's that moment of reflection when it's,
you know,

661
00:40:18.091 --> 00:40:20.280
before they actually take the act,

662
00:40:20.580 --> 00:40:24.210
if they just have that little angel on their shoulder,
you know,
um,

663
00:40:24.240 --> 00:40:28.170
they actually back out and they rephrase it and they take it down a bit.

664
00:40:28.171 --> 00:40:32.550
So I think there can be a role for technology to actually foster more useful

665
00:40:32.551 --> 00:40:36.020
conversations.
And I think what we have right now is sort of,
you know,

666
00:40:36.060 --> 00:40:40.150
a platform where anyone can say anything without consequences,
et cetera.

667
00:40:40.380 --> 00:40:44.400
And so you're seeing a lot of,
you know,
the worst of ourselves out there.

668
00:40:44.430 --> 00:40:48.240
But I think there's an opportunity to apply technology to improve that.

669
00:40:48.580 --> 00:40:50.260
<v 1>Let me just,
let me just double down on that comment.</v>

670
00:40:50.261 --> 00:40:53.200
I think we are working on that.
Yeah.
You know,
some of the folks at Jigsaw,

671
00:40:53.210 --> 00:40:56.470
it used to be Google ideas are actually working on this very hard and it's,

672
00:40:56.510 --> 00:41:00.130
it's pretty exciting because if you imagine the notion of sort of an empathy

673
00:41:00.131 --> 00:41:04.600
engine,
uh,
you know,
using machine learning where,
you know,
literally it could be,

674
00:41:04.690 --> 00:41:07.180
yeah,
we can use it to clean up some of the trolling,
you know,

675
00:41:07.181 --> 00:41:11.260
that you see on sites and so forth.
Uh,
you can also use it as,
as Bradley says,

676
00:41:11.261 --> 00:41:11.471
you know,

677
00:41:11.471 --> 00:41:15.790
it's like maybe you get more context and more empathy when you're engaged in one

678
00:41:15.791 --> 00:41:19.960
of these conversations and can adjust your message,
uh,
more productively.
So yeah,

679
00:41:19.961 --> 00:41:23.220
there's a lot of stuff.
There's a lot of research being done on this data's Sida.

680
00:41:23.221 --> 00:41:25.090
Yeah.
Dana boys research group out of New York.

681
00:41:25.091 --> 00:41:26.980
They're doing a lot of smart thinking on this.

682
00:41:26.981 --> 00:41:28.870
I think tech will be part of the answer.

683
00:41:28.871 --> 00:41:32.710
The other part is at a personal level to the point of like how to,

684
00:41:32.860 --> 00:41:34.100
how to be gone to at work.

685
00:41:34.580 --> 00:41:37.730
<v 4>Um,
you almost always have to leave with being vulnerable.</v>

686
00:41:38.210 --> 00:41:41.540
The only way you get somebody to deescalate when they're trying to like,

687
00:41:41.570 --> 00:41:42.980
you know,
you're the real racist,
you know,

688
00:41:42.981 --> 00:41:46.820
is to sort of go back to I'm a person and I'm trying to fight the same injustice

689
00:41:46.821 --> 00:41:48.380
as you are.
That's hard.

690
00:41:48.410 --> 00:41:50.810
Like you gotta be in the right place and they can't be like the phone ringing

691
00:41:50.811 --> 00:41:52.760
and the baby's crying and all that other stuff going on.

692
00:41:52.761 --> 00:41:55.970
But do you even find those people?
Like how do we get them?
They find me?

693
00:41:55.971 --> 00:41:59.400
I don't know.
I don't,
I haven't had to go looking,

694
00:42:00.320 --> 00:42:03.380
<v 3>um,
after we got it as a few more more fucks with want to make sure.</v>

695
00:42:03.381 --> 00:42:05.870
Thank you for the question now.
That's great.
Cool.

696
00:42:06.080 --> 00:42:07.250
One more comment and then we'll move.

697
00:42:07.340 --> 00:42:09.980
<v 0>Oh,
sure.
One of the things I was going to say is,
um,</v>

698
00:42:10.370 --> 00:42:13.700
I talk a lot to people about lending their own privileges to people who are

699
00:42:13.701 --> 00:42:17.450
disadvantaged and that they can actually have a platform to feel comfortable

700
00:42:17.451 --> 00:42:20.870
speaking where we fit.
Again,
lending your privilege if they more about that.

701
00:42:20.990 --> 00:42:23.270
So that's the idea of,
for example,

702
00:42:23.271 --> 00:42:27.710
if I come the Bradley and we're in a meeting and maybe I'm the only black female

703
00:42:27.711 --> 00:42:30.710
there,
maybe he'll say,
you know what,
Nancy,
why don't you speak on this topic?

704
00:42:30.890 --> 00:42:34.760
Or if I'd make a comment,
congratulate me,
you know?
Oh,
that was a great point.

705
00:42:35.000 --> 00:42:39.140
A way of really highlighting someone who may be of a different background,

706
00:42:39.410 --> 00:42:43.490
who may not have that voice to be comfortable to speak up and create an

707
00:42:43.491 --> 00:42:47.050
environment where other people can speak with them too.
So,
um,

708
00:42:47.120 --> 00:42:48.740
for people who are here,

709
00:42:48.741 --> 00:42:52.010
who would like to get people to be a little bit more comfortable speaking to

710
00:42:52.011 --> 00:42:55.670
them by providing that privilege to someone else,

711
00:42:55.671 --> 00:42:59.600
that could also create the dialogue or a space to have an open dialogue to

712
00:42:59.601 --> 00:43:01.490
really talk about some of these issues.

713
00:43:02.050 --> 00:43:03.590
<v 3>That's great.
Thank you.
Yes,
yes.</v>

714
00:43:03.591 --> 00:43:06.610
So actually my question happens to build off of that and Neil mentioned that a

715
00:43:06.611 --> 00:43:11.440
lot of people coming out of computer science backgrounds don't have a lot of

716
00:43:11.441 --> 00:43:13.540
ethics or social justice training,

717
00:43:13.650 --> 00:43:17.380
but there are some of us here who do and a lot of the conversation around

718
00:43:17.381 --> 00:43:20.530
diversity often revolves around hiring more.

719
00:43:20.920 --> 00:43:25.150
But my question was what can we do with the diversity that we already have here?

720
00:43:26.780 --> 00:43:27.613
<v 2>Great.</v>

721
00:43:29.060 --> 00:43:33.070
<v 3>I don't know.
Someone else has to answer</v>

722
00:43:34.580 --> 00:43:38.420
<v 4>just as a user get louder.
I mean I think the biggest thing I'd love to see,</v>

723
00:43:38.720 --> 00:43:42.020
I would feel better about all the Google products I use all day,
every day.

724
00:43:42.290 --> 00:43:45.080
If I saw more Googlers standing up and using their voices.

725
00:43:45.110 --> 00:43:49.460
You know when Steve Bannon talks about undermining your CEO because he is of

726
00:43:49.461 --> 00:43:53.120
south Asian descent,
like you know it's wrong,
you know,

727
00:43:53.121 --> 00:43:57.590
you know that he's going for your leadership for no reason other than the color

728
00:43:57.591 --> 00:44:00.650
of his skin.
That's wrong.
And you all know it and there's no,

729
00:44:00.860 --> 00:44:04.310
I mean this is one of those rare cases were sucking up to the CEO of a publicly

730
00:44:04.311 --> 00:44:07.190
traded company is the right thing to do for social justice.

731
00:44:07.580 --> 00:44:11.720
Like let's celebrate it,
let's enjoy it.
That's great.
Um,
that's,

732
00:44:11.960 --> 00:44:14.780
that's a rare treat,
right?
So I think,
and it like,
it will make your,

733
00:44:14.781 --> 00:44:16.490
your bosses happy.
Why not do that?

734
00:44:16.730 --> 00:44:19.370
I think those are the kinds of things where you can say something that might

735
00:44:19.371 --> 00:44:21.440
seem obvious,
but the power of your voice is,

736
00:44:21.460 --> 00:44:26.450
I think it's easy to underestimate when you're here.
Um,
you know,

737
00:44:26.451 --> 00:44:27.800
this is,
this is one of those,

738
00:44:27.890 --> 00:44:31.860
like if you have somebody who's never been in tech and never to silicon valley,

739
00:44:31.861 --> 00:44:33.630
and they walk in the doors of this place,

740
00:44:33.900 --> 00:44:36.420
when they write their Christmas letter at the end of the year to like what we

741
00:44:36.421 --> 00:44:39.180
did this year,
they're gonna be like,
I went to Google.
Right?

742
00:44:39.181 --> 00:44:40.470
That's one of the highlights of the year.

743
00:44:40.471 --> 00:44:41.830
That's one of the most amazing things that happened.

744
00:44:41.860 --> 00:44:45.270
And then you can do every day that's just going to work.
It's amazing to them.

745
00:44:45.271 --> 00:44:50.271
And so a voice that comes from this place is still amazing and has impact and

746
00:44:51.091 --> 00:44:52.020
you are,
you know,

747
00:44:52.021 --> 00:44:54.750
the rest of the world thinks these are the smartest people in the world and they

748
00:44:54.751 --> 00:44:56.580
have the most power in the world of almost named buddy.

749
00:44:56.970 --> 00:45:00.240
And so every time any of you speak up,
it carries a weight that I don't,

750
00:45:00.390 --> 00:45:03.210
I think it's easy to underestimate when you're all surrounded with each other.

751
00:45:03.240 --> 00:45:04.600
It really,
really matters.
Good,

752
00:45:04.720 --> 00:45:05.553
<v 2>good.</v>

753
00:45:09.710 --> 00:45:14.270
<v 3>So I think that's beautiful,
but may not scratch his itch.
Um,</v>

754
00:45:14.840 --> 00:45:17.630
David,
he's asking about

755
00:45:19.340 --> 00:45:22.310
what more could be done.
It puts you in dog a position,

756
00:45:22.910 --> 00:45:25.010
but what are the kinds of things that,

757
00:45:25.011 --> 00:45:28.250
not necessarily even Google but companies can be doing

758
00:45:28.750 --> 00:45:31.360
<v 1>celebrity?
I think he was Austin talking about,
you know,
what,</v>

759
00:45:31.600 --> 00:45:33.910
what Google is going to do too.
And I,
you know,
I would just,

760
00:45:34.960 --> 00:45:38.810
I guess my answer would be the internal version of what Anil just said.
Uh,

761
00:45:38.811 --> 00:45:42.430
in that,
you know,
it shouldn't just be,
you know,

762
00:45:42.460 --> 00:45:45.190
BGN black Google's was network,
you know,

763
00:45:45.191 --> 00:45:48.750
who has a hoodie march on TJF to bring these issues to the fore.

764
00:45:48.751 --> 00:45:52.850
Like more confrontation,
right,
of,
of,
you know,
at,

765
00:45:52.851 --> 00:45:57.160
at t gif of the,
of our leaders to make sure,
what are we holding it,

766
00:45:57.161 --> 00:46:00.730
holding us to account.
Uh,
you know,
we,
we,
you know,

767
00:46:00.790 --> 00:46:04.510
the most important reason why,
you know,
we released the,
the,

768
00:46:04.511 --> 00:46:08.050
our diversity numbers in my view at least,
maybe,
maybe others disagree,

769
00:46:08.410 --> 00:46:11.950
was for us to be,
to subject ourselves to,
you know,

770
00:46:11.951 --> 00:46:15.580
being brow beaten by people who think we're not doing enough.
Right?

771
00:46:15.581 --> 00:46:18.970
Because that's super important from the outside,
but also,
but more importantly,

772
00:46:19.260 --> 00:46:22.600
uh,
from folks at,
at,
at Google.
So,
uh,
you know,

773
00:46:22.601 --> 00:46:24.030
I think it's another version of winning.

774
00:46:24.060 --> 00:46:26.800
I'll set about making your voices more heard.

775
00:46:26.801 --> 00:46:31.420
We should be hearing about this more,
more often at the OKR meetings.
You know,

776
00:46:31.421 --> 00:46:36.160
we should be demanding that,
you know,
the Okr be up there that,
uh,
that we,

777
00:46:36.161 --> 00:46:39.970
you know,
ask,
you know,
Sundar,
Larry and Sergei,
whoever else is up here,
you know,

778
00:46:39.971 --> 00:46:43.300
what's our progress,
what progress have we made and,
and,
and whatnot.

779
00:46:44.160 --> 00:46:46.980
<v 4>The actions,
the other feature quite late,
sorry,
go ahead.
Sorry.</v>

780
00:46:46.981 --> 00:46:48.890
I just wanted to give a short response.
Um,

781
00:46:49.290 --> 00:46:53.070
the other thing that you guys can do is possibly possibly bring somebody with

782
00:46:53.071 --> 00:46:56.940
you when you come to these race at things.
Right.
I remember I went to um,

783
00:46:57.000 --> 00:46:58.770
or I heard about Tim.
Wise talks gives me,

784
00:46:59.010 --> 00:47:03.780
and he was talking about how we need to embrace our inner prejudice and racism

785
00:47:03.781 --> 00:47:05.940
and the faster we embrace that,
the faster we'll be better.

786
00:47:05.941 --> 00:47:09.480
And I thought to myself,
who would go to a Tim wise talk,
right?

787
00:47:09.750 --> 00:47:12.930
Those are probably people who are pretty woke,
right,
so to speak.

788
00:47:12.931 --> 00:47:16.380
But it would be great if each one of those people took one of their coworkers

789
00:47:16.590 --> 00:47:19.260
and brought them into that top or brought them into this room.

790
00:47:19.440 --> 00:47:23.070
There are plenty of empty seats here.
That's a little surprising to me,
you know?

791
00:47:23.130 --> 00:47:25.730
And so what can we do 10 minutes?

792
00:47:27.800 --> 00:47:28.700
<v 2>We've seen it more power.</v>

793
00:47:30.310 --> 00:47:31.060
<v 0>Yes.</v>

794
00:47:31.060 --> 00:47:35.310
So I mean I feel like we can do our part to make sure for all the future talks

795
00:47:35.311 --> 00:47:37.630
on decoding racy.
We bring someone with us.

796
00:47:38.040 --> 00:47:38.873
<v 2>Beautiful.</v>

797
00:47:43.410 --> 00:47:46.570
<v 8>You first I wanted to say thank you to the panel for opening up and sharing your</v>

798
00:47:46.571 --> 00:47:50.410
ideas.
I know it's kind of hard to maybe share your personal side.

799
00:47:50.440 --> 00:47:53.860
I'm at work and Mr. Jones,
my husband who's the,
um,

800
00:47:54.040 --> 00:47:58.570
he's a blogger on daily coasts.
He's a kid,
Oakland on daily coast.
He says hello.

801
00:47:58.800 --> 00:48:01.700
Uh,
I'm also,
uh,

802
00:48:01.900 --> 00:48:04.130
my question goes back to like what Nancy was saying,
uh,

803
00:48:04.210 --> 00:48:08.440
where you mentioned something and then there's trolls online that Chen come back

804
00:48:08.441 --> 00:48:11.140
and call you though the racist or the bigot or whatever.

805
00:48:11.350 --> 00:48:15.430
What are some last words of infor inspiration for people who really want to

806
00:48:15.431 --> 00:48:20.431
speak up but are maybe too afraid to say something in such a public forum?

807
00:48:24.510 --> 00:48:27.540
<v 0>So I guess I would say that you need to speak up anyway.</v>

808
00:48:27.900 --> 00:48:31.810
Like I don't think you guys know how hard it is for me to be present here,
right.

809
00:48:31.880 --> 00:48:34.890
To talk about these things and constantly think about,
okay,

810
00:48:34.891 --> 00:48:38.760
what might I say wrong that could potentially make me lose my job?
Right.
I think,

811
00:48:39.000 --> 00:48:40.440
right.
I think it's,

812
00:48:40.500 --> 00:48:45.500
if you understood just the amount of discomfort that it feels just to be a brown

813
00:48:45.511 --> 00:48:47.160
person at Google,
for example,

814
00:48:47.340 --> 00:48:52.340
that more of us would be just slightly more open to stepping a little bit more

815
00:48:53.071 --> 00:48:54.870
forward,
if that makes any sense.

816
00:48:55.150 --> 00:49:00.150
I wish somebody could really truly understand daily how it feels and I wish that

817
00:49:00.961 --> 00:49:03.000
was enough encouragement to just say,
you know,

818
00:49:03.420 --> 00:49:08.100
even a moment of you trying and asking,
I had a coworker asked me one day,

819
00:49:08.101 --> 00:49:12.900
for example,
after a shooting,
did it get any better yet?
And My response was,

820
00:49:13.170 --> 00:49:15.300
what?
Why would it be better?

821
00:49:15.510 --> 00:49:18.120
And I remember he ran off after that and he was really scared,

822
00:49:18.270 --> 00:49:21.300
but I was so appreciative.
I'm actually trying to have that conversation,

823
00:49:21.450 --> 00:49:24.630
went back,
found him,
let's talk about this.
I'm not trying to scare you off,

824
00:49:24.810 --> 00:49:25.860
but that is,

825
00:49:25.920 --> 00:49:30.300
that was him trying to connect with me and trying to empathize with me.

826
00:49:30.420 --> 00:49:33.690
And that gave me an opportunity to space,
to speak with them.

827
00:49:33.691 --> 00:49:36.330
So be careful with the minority tax.

828
00:49:36.331 --> 00:49:40.230
It doesn't mean harass your local black person.
Right?
Right.

829
00:49:40.740 --> 00:49:45.600
It means so be conscious of that.
Just maybe ease in slightly and just say,

830
00:49:45.601 --> 00:49:48.810
is there any way I can help or if you ever need a hug,
come get it.
You know,

831
00:49:49.110 --> 00:49:50.160
little things you can do.

832
00:49:51.100 --> 00:49:55.210
<v 6>Yeah.
I just wanted to quickly chime in.
Yeah.
In terms of,
you know,</v>

833
00:49:56.190 --> 00:49:57.023
um,

834
00:49:57.340 --> 00:49:57.690
<v 2>okay.</v>

835
00:49:57.690 --> 00:50:01.950
<v 6>So let's see.
I wrote an article like,
um,</v>

836
00:50:01.980 --> 00:50:03.600
back in last December,

837
00:50:03.601 --> 00:50:08.520
it ended up getting on some like white supremacist reddit thread.

838
00:50:08.521 --> 00:50:13.521
It got like hundreds of thousands of comments on tech crunch and it was a story

839
00:50:13.651 --> 00:50:16.860
that I felt very proud about writing and I felt like it was something that

840
00:50:16.861 --> 00:50:18.260
needed to be said.
It was,
you know,

841
00:50:18.330 --> 00:50:21.530
of course about race and issues of diversity in tech.
Um,

842
00:50:21.630 --> 00:50:25.350
and that was when I kind of had him like my first major dose of then people

843
00:50:25.351 --> 00:50:28.250
firing back and calling me the racist meet a sexist one.

844
00:50:28.251 --> 00:50:31.940
But the bright side to that are the,

845
00:50:32.000 --> 00:50:33.860
the people who came to me and said,

846
00:50:34.070 --> 00:50:37.490
thank you for writing that like this had a really big impact on me.

847
00:50:37.491 --> 00:50:40.700
I've shared it with my coworkers and I were talking about these things in the

848
00:50:40.701 --> 00:50:45.620
office.
And so even though like thousands of people hated me,

849
00:50:45.650 --> 00:50:48.370
like there were,
there were some who actually,
you know,

850
00:50:48.380 --> 00:50:51.890
got something out of it and at the end of the day that makes it all worth it.

851
00:50:52.600 --> 00:50:55.990
<v 3>Good.
So I'm going to let you ask the last question.</v>

852
00:50:56.020 --> 00:51:00.140
You are the three people are just being discriminated against.
I'm sorry.
Um,
and,

853
00:51:00.210 --> 00:51:04.810
and then I'm going to ask that we give our final remarks probably starting with

854
00:51:04.811 --> 00:51:06.720
you David,
and then coming this way.
Um,

855
00:51:06.880 --> 00:51:09.460
partially answering her question and partially probably ignoring it and saying

856
00:51:09.461 --> 00:51:10.420
whatever you're going to say.
Anyway,

857
00:51:10.710 --> 00:51:13.770
<v 8>so go ahead.
So I'll do this quick.</v>

858
00:51:14.370 --> 00:51:17.850
The love army makes sense,
right?
It's something we can all do.

859
00:51:17.851 --> 00:51:20.790
We can all fight for and embrace others.

860
00:51:20.791 --> 00:51:23.730
But how do we fight a two pronged fight when one,

861
00:51:23.760 --> 00:51:28.450
it's in Washington and as you brought up,
they don't,
they block everything.
We're,

862
00:51:28.451 --> 00:51:30.960
we're fighting this fight against people who don't respect the rules of

863
00:51:30.961 --> 00:51:34.890
engagement in which we are playing with him and here at Google and throw out the

864
00:51:34.891 --> 00:51:37.200
tech industry.
They don't respect science as much.

865
00:51:37.230 --> 00:51:40.470
We see this kind of anti elitism,

866
00:51:41.250 --> 00:51:44.340
they don't respect the science or the data or they think that we are doing

867
00:51:44.341 --> 00:51:46.770
something negative or influencing in a negative way.

868
00:51:46.771 --> 00:51:50.250
So how do we both have a love army but also fight a fight in which the rules of

869
00:51:50.251 --> 00:51:52.860
engagement on both tech and,
and DC are not?

870
00:51:53.010 --> 00:51:56.310
There was an engagement that we've been playing with for the past eight years,

871
00:51:56.311 --> 00:51:57.144
at least.

872
00:51:57.200 --> 00:52:01.850
<v 3>Good.
So let me kind of take that,
which is,
it's a tough question.</v>

873
00:52:02.210 --> 00:52:06.620
Um,
and uh,
add a little bit to it so people can answer it as,

874
00:52:06.621 --> 00:52:07.840
as they will.
Um,

875
00:52:07.880 --> 00:52:12.880
we do have this challenge of we want to stand up for our values and we also want

876
00:52:13.821 --> 00:52:17.300
to be inclusive.
Um,
and that's true.
Whether it has to do with Trump,

877
00:52:17.301 --> 00:52:19.970
whether it has to do with race,
whether it has to do with gender,
sexuality,

878
00:52:19.971 --> 00:52:24.650
this is a tough thing.
Um,
and,
um,
as we think about,
uh,

879
00:52:24.651 --> 00:52:27.490
giving folks here some guidance going forward,
what,

880
00:52:27.491 --> 00:52:30.650
what thoughts might you leave people with in light of that challenge?
Um,

881
00:52:30.770 --> 00:52:31.603
Mr Devin,

882
00:52:33.970 --> 00:52:36.280
<v 8>I don't know if I'm the right one.
Then why don't you take it?</v>

883
00:52:37.820 --> 00:52:42.670
Why don't you take it,
Nancy?
I the question.

884
00:52:43.910 --> 00:52:47.040
<v 3>Look at w w we have to do two things that are hard.</v>

885
00:52:47.190 --> 00:52:50.720
We have to stick up for our values and we have to reach out.
Yeah.
Um,

886
00:52:50.820 --> 00:52:53.130
how do you do that and how do you think about that?

887
00:52:53.131 --> 00:52:54.600
And if you don't want to answer it,
say I don't care.

888
00:52:54.601 --> 00:52:56.250
I'm going to say what I want to say cause I only got one,

889
00:52:56.270 --> 00:52:57.560
one last comment we got.
Yeah.

890
00:52:57.850 --> 00:53:01.340
<v 0>I mean from a personal standpoint because I,
it's,
it's really tough.</v>

891
00:53:01.840 --> 00:53:04.840
I joked with a lot of people that I feel like I am eight jobs because I really

892
00:53:04.841 --> 00:53:08.350
just want to focus on my skills and talents,
but I know I can't write.

893
00:53:08.380 --> 00:53:12.250
I know that to just say that I'm good at this without dealing with the fact that

894
00:53:12.251 --> 00:53:17.080
there's all the surrounding stuff that's blocking me from trying to survive in

895
00:53:17.081 --> 00:53:20.260
this space.
It's important to me to make it better for someone else.

896
00:53:20.470 --> 00:53:23.740
It'd be great if everybody did their part to help with that.

897
00:53:24.070 --> 00:53:25.980
But I mean like for personally it's,

898
00:53:26.040 --> 00:53:30.720
<v 4>it definitely feels like a burden that I've taken on and I constantly am</v>

899
00:53:30.721 --> 00:53:32.850
thinking about it from like a self care perspective.

900
00:53:32.851 --> 00:53:34.530
Like how much should I take on?

901
00:53:34.980 --> 00:53:39.540
And sometimes I really wish I didn't have to.
That's my truth.

902
00:53:39.990 --> 00:53:40.340
David.

903
00:53:40.340 --> 00:53:40.521
<v 1>Yeah,</v>

904
00:53:40.521 --> 00:53:45.521
I would just say so I think there still needs to be a period of militancy on

905
00:53:47.930 --> 00:53:52.570
values.
Uh,
in addition to the outreach,
the,
I'm very worried as I,

906
00:53:52.640 --> 00:53:55.910
I my comment earlier about unilateral disarmament,

907
00:53:55.911 --> 00:53:57.360
but I feel like there's a little bit of a,

908
00:53:57.550 --> 00:54:02.450
a symmetrical kind of battle going on where you have these values and you,

909
00:54:02.451 --> 00:54:04.050
you think that there's,
you know,

910
00:54:04.160 --> 00:54:08.240
truth and like things that you can actually verify in the world that are true

911
00:54:08.241 --> 00:54:10.640
and that makes sense.
Or the things that are,
you know,

912
00:54:10.641 --> 00:54:14.120
that our principal a and you want to stick to them and you've got folks on the

913
00:54:14.121 --> 00:54:19.100
other side of the debate sort of,
you know,
disregarding all of that stuff,
right.

914
00:54:19.160 --> 00:54:22.340
And it gives them a leg up and a lot of ways because the,
you know,

915
00:54:22.341 --> 00:54:25.780
they figured out how to master media and they realized that the 24 new hour news

916
00:54:25.781 --> 00:54:28.370
cycle or the two hour new cycle or whatever it is,

917
00:54:28.520 --> 00:54:32.240
people forget when you say something that's wrong,
they just come away thinking,

918
00:54:32.241 --> 00:54:35.470
oh,
they scored a good point.
So I feel,
you know,
Yo,

919
00:54:35.780 --> 00:54:40.520
that ultimately should collapse,
right.
Because at some point,
you know,

920
00:54:40.580 --> 00:54:44.090
disinformation,
maybe you can only sustain that.
I think for so long,

921
00:54:44.091 --> 00:54:45.610
at least I'm hopeful of that and I,

922
00:54:45.630 --> 00:54:50.030
I think that continued militancy in the face of that,
calling it out,

923
00:54:50.720 --> 00:54:51.050
you know,

924
00:54:51.050 --> 00:54:54.740
making sure that your volume values are front and center and so forth still

925
00:54:54.741 --> 00:54:56.960
needs to happen before we go.
You know,

926
00:54:56.961 --> 00:55:01.961
we sort of let everything down and sort of normalize every viewpoint as,

927
00:55:02.181 --> 00:55:05.320
you know,
can't we just get along?
Right,
right.
Uh,
so that,

928
00:55:05.330 --> 00:55:06.500
that would be my response.

929
00:55:06.630 --> 00:55:11.120
<v 4>Yeah.
Very good.
And you just want to add on that,
um,
you are still at Google,</v>

930
00:55:11.121 --> 00:55:12.170
one of the most wealthy,

931
00:55:12.171 --> 00:55:16.970
powerful institutions that's ever existed in the history of the world and a lot

932
00:55:16.971 --> 00:55:20.780
more people love searching on Google and using their android phone and watching

933
00:55:20.781 --> 00:55:24.200
a youtube video and checking their g mail than voted for Donald Trump.

934
00:55:25.010 --> 00:55:30.010
You have a lot of power and you don't have to apologize for using it.

935
00:55:31.040 --> 00:55:35.810
You don't have to be,
you know,
halfassed about using it.
Uh,
I think,

936
00:55:35.811 --> 00:55:39.770
you know,
you have buy in from senior level and talk about being militant,
right?

937
00:55:39.830 --> 00:55:41.840
That is incredibly,
incredibly,

938
00:55:45.690 --> 00:55:48.730
<v 2>you said it,
you said it,
I heard it.
I'm going to tweet it later.</v>

939
00:55:48.940 --> 00:55:49.773
That'll make it true.

940
00:55:51.320 --> 00:55:54.340
<v 4>But,
but I mean it in,
in all seriousness like there,
there,</v>

941
00:55:54.870 --> 00:55:56.540
there's no excuse for not using it.
And I,

942
00:55:56.541 --> 00:55:59.810
and I don't say that lightly because I think there's great responsibility with

943
00:55:59.811 --> 00:56:04.430
great power,
but if you all don't step up,
then who can,

944
00:56:05.090 --> 00:56:07.160
it's not the people who lost their factory jobs.

945
00:56:07.161 --> 00:56:08.750
It is not all these other people.
But this is,

946
00:56:08.751 --> 00:56:12.440
this is the place that has the power and the influence and the daily connection

947
00:56:12.441 --> 00:56:15.230
with billions of people to be able to do it.

948
00:56:15.231 --> 00:56:17.660
And I think you can do it in ways that are thoughtful.

949
00:56:17.840 --> 00:56:20.540
I'm fine if you want to express that is how you compete in the market.

950
00:56:20.541 --> 00:56:24.190
If you want to say we're better than Microsoft and Amazon,
then the Facebook

951
00:56:24.190 --> 00:56:27.700
<v 3>on this feature because we built it to be more empathetic and thoughtful because</v>

952
00:56:27.701 --> 00:56:30.450
we baked our values into it.
I would love to have that beat.

953
00:56:30.451 --> 00:56:35.170
The criteria by which I choose a product.
That's great.
Thank you.
Um,
yeah,
you,

954
00:56:35.171 --> 00:56:36.530
you and me.
Okay.

955
00:56:36.930 --> 00:56:40.480
<v 6>Um,
similar to what Nancy was saying earlier,
I think it's,</v>

956
00:56:40.750 --> 00:56:43.030
it's really important to,
you know,

957
00:56:43.031 --> 00:56:47.180
just stand up and support your,
your coworkers.

958
00:56:47.290 --> 00:56:50.620
You're a diverse coworkers and advocate for them.
And you know,

959
00:56:50.621 --> 00:56:52.870
if you're in a meeting with one of them and they,

960
00:56:53.140 --> 00:56:56.020
they have an idea that you like say,
Oh yeah,
like that was a really great idea.

961
00:56:56.021 --> 00:57:00.700
It's just really little things like that can actually,
they can go a long way.

962
00:57:00.701 --> 00:57:01.900
So just,
yeah,

963
00:57:01.901 --> 00:57:06.040
just try to keep supporting everyone and keep that in mind.

964
00:57:06.240 --> 00:57:09.120
<v 5>Thank you.
Can you,
uh,
like Nancy,</v>

965
00:57:09.121 --> 00:57:12.180
I was really nervous about coming up here is the token white,

966
00:57:12.181 --> 00:57:16.740
middle aged Jewish guy,
you know,
representing,
uh,
and uh,

967
00:57:16.920 --> 00:57:21.540
you were all so inclusive and gracious like way too much in fact.

968
00:57:21.541 --> 00:57:23.640
Cause basically all I did was show up.

969
00:57:23.910 --> 00:57:27.810
But I do love the idea of bring,

970
00:57:27.811 --> 00:57:32.250
bring someone who needs to be here that uh,
isn't inclined to come.

971
00:57:32.251 --> 00:57:36.240
And I think that's a great way just to catch the infectious positive energy that

972
00:57:36.241 --> 00:57:37.170
I'm feeling in this room.

973
00:57:37.520 --> 00:57:41.540
<v 3>Very good.
Well look,
I want to thank everybody here.
Um,
you know,</v>

974
00:57:41.541 --> 00:57:44.830
when we talk about love and we talk about melting,

975
00:57:44.840 --> 00:57:47.690
see those things go together very well.
Honestly,

976
00:57:47.990 --> 00:57:51.680
like a mama bear is militant about her love for her cubs.

977
00:57:51.850 --> 00:57:56.350
Let love is not weak.
Love is the opposite of weak.
Um,
and,

978
00:57:56.400 --> 00:57:58.940
and I think it's about loving harder.

979
00:57:59.360 --> 00:58:03.710
I'm concerned about the people who Trump is targeting Muslims.

980
00:58:03.980 --> 00:58:08.450
Everybody.
I'm also concerned,
I mean,
just took a long list,
a long list,

981
00:58:09.020 --> 00:58:12.260
but I also want to say this,
this is the challenge.

982
00:58:12.710 --> 00:58:15.920
I'm also concerned with the people that he's tricking.

983
00:58:16.550 --> 00:58:21.550
When he tells coal miners he's going to frack everywhere and give them their

984
00:58:22.251 --> 00:58:26.360
jobs back in the coal mines,
he's tricking them.
If you frack everywhere,

985
00:58:26.390 --> 00:58:30.170
you collapse the price of natural gas,
you put every coal miner out of work.

986
00:58:30.650 --> 00:58:34.970
That's my problem too.
I'm PR underdogs and red states and blue states,

987
00:58:35.180 --> 00:58:37.220
and I think that that's the winning formula.

988
00:58:37.460 --> 00:58:42.260
And so I want to be militant about the love that I feel for the dreamers and the

989
00:58:42.261 --> 00:58:44.540
native Americans who are out there fighting for the pipeline.

990
00:58:44.630 --> 00:58:47.960
And those call guys were being tricked,
and if were that way,
we're going to win.

991
00:58:48.110 --> 00:58:48.890
So I'll thank you very much.

