WEBVTT

1
00:00:06.360 --> 00:00:10.270
Good morning.
Thank you all for coming out to meet,
uh,
Rob Califf,

2
00:00:10.730 --> 00:00:14.340
a former FDA commissioner under president Obama and barely science advisor.

3
00:00:14.341 --> 00:00:15.150
I'm Paul Varghese,

4
00:00:15.150 --> 00:00:17.910
the clinical lead for health informatics for verily life sciences.

5
00:00:18.510 --> 00:00:20.880
And today we're going to have a question and answer session.

6
00:00:20.881 --> 00:00:25.010
We had one of the most influential physician scientists in the United States and

7
00:00:25.011 --> 00:00:29.260
health care and internationally.
And I'll be asking Dr Caleb,
uh,

8
00:00:29.290 --> 00:00:31.920
about a range of topics related to tech enabled healthcare and they'll be an

9
00:00:31.921 --> 00:00:36.040
opportunity for the audience to all ask questions as well.
Uh,

10
00:00:36.390 --> 00:00:39.330
so for those of you who may not be familiar with verily,

11
00:00:39.331 --> 00:00:41.550
we are a offshoot of Google X.

12
00:00:41.551 --> 00:00:46.530
We've been around since 2015 and our mission is really closely aligned with what

13
00:00:46.531 --> 00:00:47.131
Google has.

14
00:00:47.131 --> 00:00:51.090
We would like to make the world's healthcare information accessible and useful

15
00:00:51.150 --> 00:00:55.560
for the population at large.
Um,
and we do this in a variety of ways.

16
00:00:55.590 --> 00:00:59.910
One is to collect information,
organize it,
and then make it actionable.

17
00:01:00.630 --> 00:01:04.410
You may have heard of some of our recent efforts in this field,
which is uh,

18
00:01:04.620 --> 00:01:08.610
some of them are prominent public projects.
One is called the baseline project,

19
00:01:08.611 --> 00:01:13.110
which is to enroll 10,000 people to collect evidence in a way that the world has

20
00:01:13.111 --> 00:01:13.891
not seen before.

21
00:01:13.891 --> 00:01:17.250
A broad range of healthcare information which we consider traditional and

22
00:01:17.251 --> 00:01:21.450
nontraditional things like behavioral information,
activity information.

23
00:01:21.870 --> 00:01:25.800
Another more recent,
uh,
initiative is in the disease management space.

24
00:01:25.801 --> 00:01:29.220
We recently have a joint venture that was launched with Santa Fe called undue.

25
00:01:29.460 --> 00:01:31.140
It's for diabetes management,
uh,

26
00:01:31.260 --> 00:01:35.370
initiatives and it's again trying to apply the best of what we know as a Google

27
00:01:35.371 --> 00:01:38.040
and alphabet associated company to the area of health care,

28
00:01:38.041 --> 00:01:41.640
which we think is right for improvement with some of the technologies take for

29
00:01:41.641 --> 00:01:46.470
granted.
Um,
that's a little bit about verily and with me is Robert Taylor,

30
00:01:46.500 --> 00:01:48.300
who's one of our barely science advisors.

31
00:01:48.690 --> 00:01:52.890
There could be a whole separate talk about all the things that Dr Caleb has
done.

32
00:01:53.240 --> 00:01:57.090
Um,
but I will hit the highlights.
Uh,
he is a physician scientist.

33
00:01:57.150 --> 00:01:57.961
He's a cardiologist,

34
00:01:57.961 --> 00:02:02.270
trained primarily at Duke University where he spent the last 30 years becoming

35
00:02:02.400 --> 00:02:05.160
one more,
a prominent medical researchers.
I think.
Uh,

36
00:02:05.200 --> 00:02:09.960
the blurb that I read was a 1200 peer reviewed journal articles,

37
00:02:09.961 --> 00:02:13.420
50,000 citations and uh,
um,

38
00:02:13.500 --> 00:02:17.560
that just shows the kind of work that he's done.
It's a particularly,
uh,

39
00:02:17.670 --> 00:02:21.720
meaningful to me.
I'm trained as a cardiologist and when I did my training,
uh,

40
00:02:21.840 --> 00:02:24.320
we learned Dr Caleb's works and,
uh,

41
00:02:24.330 --> 00:02:28.740
one of them was a one most prominent large scale clinical trials that gave us

42
00:02:28.741 --> 00:02:31.320
true evidence based medicine and was called the Gusto trial.

43
00:02:31.321 --> 00:02:36.120
And when we talk large scale,
I mean large scale 40,000 people.
And uh,
rob can,

44
00:02:36.150 --> 00:02:39.000
we'll have some opportunities to talk about what it was like to sort of do that

45
00:02:39.001 --> 00:02:41.910
kind of evidence generation and data collection.
Uh,

46
00:02:42.010 --> 00:02:45.090
in the era when the most sophisticated technology was a fax machine.

47
00:02:45.360 --> 00:02:47.300
Those are some of the reasons why,
uh,

48
00:02:47.310 --> 00:02:50.610
he was tapped by President Obama to help run the FDA.

49
00:02:50.970 --> 00:02:52.470
For those of you who are not familiar with the FDA,

50
00:02:52.471 --> 00:02:56.550
it is the agency that is tasked with ensuring the safety and efficacy of the

51
00:02:56.551 --> 00:03:00.770
medications and medical devices that we take for in our lives.
I think,
uh,

52
00:03:00.940 --> 00:03:04.960
the blurb from Forbes was that Rob Califf was the most qualified FDA

53
00:03:04.961 --> 00:03:07.060
commissioner in the history of the organization.

54
00:03:07.450 --> 00:03:10.240
Given that sort of wide ranging background that he had.

55
00:03:10.600 --> 00:03:12.520
And for all of those reasons,
uh,

56
00:03:12.530 --> 00:03:15.760
I've asked them to come here and talk a little bit about what it's like to be a

57
00:03:15.761 --> 00:03:20.761
clinician scientist educator and be on the government side of things when we are

58
00:03:21.251 --> 00:03:25.710
trying to find new and innovative ways to apply technology to healthcare.

59
00:03:26.110 --> 00:03:28.960
Um,
so that's the background for Dr Kayla.

60
00:03:30.880 --> 00:03:34.540
Why come to technology?
You have been on the government side,

61
00:03:34.541 --> 00:03:36.780
you've been on the clinical side.
Uh,

62
00:03:37.000 --> 00:03:42.000
what is it about coming over to the technology era or the technology sector that

63
00:03:42.311 --> 00:03:44.420
you found appealing?
Um,

64
00:03:44.500 --> 00:03:48.760
and the related question is something that Eric Schmidt posed to us about a year

65
00:03:48.760 --> 00:03:50.250
ago,
which is,
um,

66
00:03:50.380 --> 00:03:55.180
how does a technology centric company such as alphabet,
such as Google,

67
00:03:55.181 --> 00:03:59.200
such as fairly,
um,
learn to become more healthcare centric?

68
00:03:59.610 --> 00:04:03.340
<v 2>Well.
So first of all,
let me just say I'm a hedging my bets as you know.</v>

69
00:04:03.341 --> 00:04:08.341
So I'm halftime at verily and halftime at back at the old university and North

70
00:04:10.410 --> 00:04:11.010
Carolina.

71
00:04:11.010 --> 00:04:14.610
And I'm also chairing the board of an organization called the people centric

72
00:04:14.611 --> 00:04:16.740
research foundation,
which is,
um,

73
00:04:17.640 --> 00:04:22.640
an effort to generate evidence on behalf of patients and consumers about health

74
00:04:25.651 --> 00:04:30.651
policies and medical products that has been funded through the accountable care

75
00:04:31.501 --> 00:04:32.281
act actually.

76
00:04:32.281 --> 00:04:37.281
And now we have 130 million Americans participating through the ease of their

77
00:04:37.561 --> 00:04:41.510
health records and hopefully in clinical trial.
So,
um,

78
00:04:41.570 --> 00:04:44.130
I see it as a comprehensive effort,
but you know,

79
00:04:44.131 --> 00:04:49.131
why spin at age 66 and two weeks in North Carolina in two weeks in California

80
00:04:50.610 --> 00:04:53.640
seems like an extreme thing to do.

81
00:04:54.510 --> 00:04:57.990
And the answer is because obviously to me,

82
00:04:57.991 --> 00:05:02.991
the technologies sphere as taking off now and has a huge amount of money.

83
00:05:05.070 --> 00:05:07.530
So on one hand we've got tremendous talent,

84
00:05:07.850 --> 00:05:11.310
a revolution in technology,

85
00:05:12.030 --> 00:05:13.410
and then we've got a contrary,

86
00:05:13.590 --> 00:05:17.190
maybe exemplified by the South East and middle America,

87
00:05:17.880 --> 00:05:21.660
where life expectancy has gone the wrong direction three years in a row.
So,

88
00:05:22.110 --> 00:05:25.060
you know,
we should be concerned about the world,
but,
uh,

89
00:05:25.170 --> 00:05:28.560
we'd better be concerned about the United States because our health is

90
00:05:28.561 --> 00:05:33.090
deteriorating measurably,
uh,
under our feet.

91
00:05:33.930 --> 00:05:38.610
So what I would hope to do is to bring these worlds together because,
um,

92
00:05:38.700 --> 00:05:42.930
the traditional healthcare sphere needs better technology and you know,

93
00:05:42.931 --> 00:05:44.550
amazing things are possible,

94
00:05:45.090 --> 00:05:48.410
but at the tech world has failed every single time it's tried to get into this

95
00:05:48.411 --> 00:05:49.244
space.

96
00:05:49.710 --> 00:05:53.400
And there are reasons for that which have to do with the human side of things

97
00:05:53.401 --> 00:05:54.840
that are,
I think,

98
00:05:54.841 --> 00:05:59.150
fundamentally different between software and application,

99
00:05:59.730 --> 00:06:02.900
uh,
to a very human thing like,
uh,

100
00:06:02.930 --> 00:06:07.330
delivering healthcare and people making decisions about their healthcare.
So,

101
00:06:07.750 --> 00:06:09.770
you know,
at this stage of my career,

102
00:06:09.771 --> 00:06:14.771
I want to be helpful and I'm trying to keep a foot in each side of the equation.

103
00:06:16.450 --> 00:06:19.520
<v 1>Uh,
so I'm going to ask you to play the role of physician.
Um,</v>

104
00:06:20.140 --> 00:06:25.140
you're used to giving patients a constructive information that can be tough to

105
00:06:25.901 --> 00:06:30.060
hear.
What kind of constructive,
uh,

106
00:06:30.220 --> 00:06:34.540
tough medicine would you give to an organization like Google,

107
00:06:34.541 --> 00:06:35.650
alphabet and barely,

108
00:06:35.890 --> 00:06:40.810
you mentioned the differences in how clinical medicine has done,
uh,

109
00:06:40.960 --> 00:06:42.040
and software development.

110
00:06:43.110 --> 00:06:43.943
<v 2>So part of this,</v>

111
00:06:44.150 --> 00:06:48.630
the part I'm very confident about giving advice on his pay a lot more attention

112
00:06:48.631 --> 00:06:50.760
to the human side of things.
Uh,

113
00:06:50.920 --> 00:06:54.420
the best software in the world when it comes to people making everyday decisions

114
00:06:54.421 --> 00:06:59.421
about health will not do the job unless it's integrated with people.

115
00:07:00.690 --> 00:07:03.960
Um,
uh,
an individual,
you know,

116
00:07:03.961 --> 00:07:08.280
it seems like a lot of what silicon valley is built on is an individual

117
00:07:08.281 --> 00:07:10.530
interacting with a computer,
which is,
you know,

118
00:07:10.531 --> 00:07:14.940
it's been great for me in my career,
but when I make decisions about my health,

119
00:07:14.941 --> 00:07:19.230
often my wife is involved,
my parents may be involved.

120
00:07:19.800 --> 00:07:22.920
Um,
and for many things,

121
00:07:22.921 --> 00:07:27.921
by law I've got to deal with doctors and nurses and clinics and hospitals.

122
00:07:28.440 --> 00:07:28.891
So it's a,

123
00:07:28.891 --> 00:07:33.891
it's a complicated social human equation that needs to be dealt with.

124
00:07:34.510 --> 00:07:37.380
The part of it I'm a little less confident about,

125
00:07:37.680 --> 00:07:40.470
I'm sure that this is an issue that we need to pay attention now.

126
00:07:41.460 --> 00:07:43.410
It's just looking at what's happened with Facebook.

127
00:07:43.710 --> 00:07:48.710
You cannot assume that using people's information and selling it without a very

128
00:07:51.031 --> 00:07:56.010
strong governance,
probably regulatory eventually,
um,

129
00:07:56.040 --> 00:08:00.340
approach is going to be needed because ultimately this is a,

130
00:08:00.840 --> 00:08:05.840
I think a very powerful double edge sword that if it's not handled through

131
00:08:06.031 --> 00:08:09.660
channels that have governance,
um,

132
00:08:09.870 --> 00:08:13.440
could end up being detrimental is I think,
uh,

133
00:08:13.630 --> 00:08:18.090
this last election was,
it certainly wasn't good for my career.
I,
you know,

134
00:08:19.230 --> 00:08:22.720
I was out of it,
out of a job January 20th.
Okay.

135
00:08:23.880 --> 00:08:25.810
<v 1>So you,
you talked about,
um,</v>

136
00:08:25.830 --> 00:08:30.830
you touched on the role of what we do with people's data and so one of the

137
00:08:31.141 --> 00:08:35.880
promises,
one of the potential promises is that as we do better data collection,

138
00:08:35.881 --> 00:08:40.410
that we have the advantage of technology being fairly ubiquitous.

139
00:08:40.440 --> 00:08:43.170
That we have sensors,
um,

140
00:08:43.310 --> 00:08:46.060
that we're all carrying their smartphones.

141
00:08:46.710 --> 00:08:50.610
We're gathering information and we want to do the right thing by the patient.

142
00:08:50.611 --> 00:08:53.160
We want to do the right thing by the provider.
Um,

143
00:08:53.670 --> 00:08:55.050
can you talk a little bit about where,

144
00:08:55.140 --> 00:08:59.220
see how we empower people to get that information?
Uh,
for example,

145
00:08:59.221 --> 00:09:03.770
you were recently at south by southwest talking on this very subject.
Uh,
um,

146
00:09:04.140 --> 00:09:06.250
can you share where you think,
um,

147
00:09:06.660 --> 00:09:11.660
the obligation is to getting that information back to the patient and do we not

148
00:09:11.941 --> 00:09:14.550
run the risk of overwhelming people?
Um,

149
00:09:14.730 --> 00:09:18.390
more information is not necessarily easier to manage.

150
00:09:18.930 --> 00:09:23.320
<v 2>A lot of wisdom in what you just said.
In my opinion,
this goes way back for me.</v>

151
00:09:23.321 --> 00:09:26.090
I mean,
we were doing studies and the eighties of,
uh,

152
00:09:26.290 --> 00:09:30.620
end of life care and pretty easily demonstrated that uh,

153
00:09:30.730 --> 00:09:34.390
different people will take exactly the same information and make very different

154
00:09:34.391 --> 00:09:36.330
decisions.
Oftentimes kind of shocking.

155
00:09:36.331 --> 00:09:41.020
Like there was a general view that if you had like a 1% chance of living the

156
00:09:41.021 --> 00:09:44.530
next six months,
that people would rationally say,

157
00:09:44.531 --> 00:09:47.620
don't resuscitate me because you know,

158
00:09:47.740 --> 00:09:50.950
it's just going to add a couple of days of misery to,

159
00:09:51.830 --> 00:09:55.960
and I'd rather die peacefully under good social conditions.

160
00:09:56.380 --> 00:09:57.460
But a lot of Americans,

161
00:09:57.461 --> 00:10:02.140
we'll look at 1% say I'm going to be the 1% and often the families would have

162
00:10:02.141 --> 00:10:03.100
opinions about it.

163
00:10:03.101 --> 00:10:08.101
So we've also got to look at the cognitive capabilities of the population of

164
00:10:08.311 --> 00:10:09.730
very high proportion of Americans.

165
00:10:09.731 --> 00:10:12.670
Something like 15% can't read above a second grade level.

166
00:10:13.360 --> 00:10:17.470
So the idea that we're going to empower people by giving them like their Gina

167
00:10:18.280 --> 00:10:22.240
and that circumstances just,
it just doesn't make sense.
On the other hand,

168
00:10:22.690 --> 00:10:27.340
you should get your Gina.
So I think one of the beauties of a,

169
00:10:27.341 --> 00:10:32.341
what could be done with current technologies to tailor the information to the

170
00:10:32.501 --> 00:10:36.070
person according to their capabilities and desires.

171
00:10:36.970 --> 00:10:39.970
Also taking into account that they need to have a relationship,

172
00:10:39.971 --> 00:10:41.440
whether it's someone,

173
00:10:41.441 --> 00:10:44.860
particularly if it has an illness or a serious finding needs to have a

174
00:10:44.861 --> 00:10:49.120
relationship with a clinician of some type and a system.

175
00:10:49.570 --> 00:10:52.200
And like I say,
very often,
um,

176
00:10:52.840 --> 00:10:57.130
with family members,
uh,
I love the British term for this carers.

177
00:10:57.550 --> 00:11:01.000
So carers,
someone is not paid but takes care of you.

178
00:11:01.390 --> 00:11:05.650
A caregiver or someone who is paid like a nurse or social worker.

179
00:11:06.370 --> 00:11:09.100
And those things are different because the carer often comes with a very

180
00:11:09.101 --> 00:11:12.680
emotional component.
You know,
it's,

181
00:11:12.681 --> 00:11:15.910
it's been sad that in a American families,

182
00:11:15.911 --> 00:11:18.580
the women make about 80% of the healthcare decisions.

183
00:11:18.581 --> 00:11:22.870
And I think that is verified by empirical research.
So on average,

184
00:11:22.871 --> 00:11:23.770
if it's a man,

185
00:11:23.800 --> 00:11:28.390
if you're not dealing with a significant other in a heterosexual couple,

186
00:11:29.140 --> 00:11:31.810
a,
you're probably making a mistake.
On average.

187
00:11:32.860 --> 00:11:37.270
<v 1>You talked about,
um,
some of the new demands were placing on,</v>

188
00:11:37.690 --> 00:11:41.440
on patients when they get access to that information and the people surrounding

189
00:11:41.441 --> 00:11:45.210
them,
if that is changing for the,
uh,

190
00:11:45.460 --> 00:11:46.900
patient and the people around them,

191
00:11:47.440 --> 00:11:51.730
where is this going to impact the people we traditionally associate the

192
00:11:51.790 --> 00:11:54.400
caregiver,
the physician,
the nurse,

193
00:11:54.401 --> 00:11:59.170
that there are things that we associate with those roles and responsibilities.

194
00:11:59.560 --> 00:12:04.560
In 2012 there was a lot of attention to what the known Coleslaw said about he

195
00:12:04.931 --> 00:12:07.930
didn't want to see doctor Varghese from doctor Kayla if he wanted to see doctor

196
00:12:07.931 --> 00:12:08.764
algorithm.

197
00:12:08.770 --> 00:12:13.000
And then four years later he kind of backtracked and he said,

198
00:12:13.030 --> 00:12:15.850
well,
I think there's a role for the physician,

199
00:12:15.851 --> 00:12:19.540
but I think he's going to be like 20% physician.
Not like,
you know,
100%.
Uh,

200
00:12:19.610 --> 00:12:24.480
a replacement.
And,
um,
and so,
um,

201
00:12:24.550 --> 00:12:29.470
the question for you is from your vantage point and perspective,

202
00:12:29.740 --> 00:12:31.780
both as a physician and an educator,

203
00:12:31.960 --> 00:12:34.150
where do you see the role of physician evolving?

204
00:12:34.420 --> 00:12:38.710
And I ask in particular because your son is actually doing his medical training

205
00:12:39.100 --> 00:12:41.290
in emergency room medicine.
So what,

206
00:12:41.291 --> 00:12:46.291
what is it that you're going to see happening to your son and other people in

207
00:12:46.361 --> 00:12:48.010
their training with us?

208
00:12:48.410 --> 00:12:50.630
<v 2>You know,
one thing that you learned at the FDA in a big way.</v>

209
00:12:50.631 --> 00:12:51.980
So when people are healthy,

210
00:12:52.250 --> 00:12:56.390
they really do behave like consumers and that is,
uh,

211
00:12:56.810 --> 00:12:59.280
very little risk tolerance.
Um,

212
00:12:59.320 --> 00:13:02.610
a lot of desire for independence particularly and Americans,

213
00:13:02.630 --> 00:13:06.860
it's a little different.
Other cultures.
Um,
but when people get sick,

214
00:13:07.580 --> 00:13:09.590
they become dependent fairly quickly.

215
00:13:09.591 --> 00:13:13.610
They're also willing to take more risk and they're more vulnerable to people

216
00:13:13.611 --> 00:13:17.780
selling them things when they're at risk.
And so,
um,

217
00:13:18.350 --> 00:13:22.460
I don't,
I'm not worried about the role of the physician going away.

218
00:13:22.461 --> 00:13:23.780
I don't think that's going to happen.

219
00:13:23.781 --> 00:13:26.900
The physician becomes an enabler and an information,

220
00:13:27.710 --> 00:13:32.150
a transmitter.
But there's also this other big thing happening,
which is not,

221
00:13:32.151 --> 00:13:35.840
you know,
we use the word physician.
That's really,
I must've met her for,

222
00:13:35.841 --> 00:13:39.890
for a clinician of some tie.
More and more.
Uh,

223
00:13:39.891 --> 00:13:44.150
the clinician may be a nurse or a pharmacist or a physician's assistant,

224
00:13:44.210 --> 00:13:49.210
all kinds of people working in teams to help people take better care of

225
00:13:49.611 --> 00:13:53.060
themselves and when the person desires it,

226
00:13:53.330 --> 00:13:57.320
making decisions for that person.
For example,
as a cardiologist,

227
00:13:57.650 --> 00:14:00.320
pretty rare to have somebody coming in with an acute heart attack,

228
00:14:00.350 --> 00:14:02.450
making an argument about what treatment they want.

229
00:14:02.980 --> 00:14:07.520
You got 10 out of 10 chest pain,
30% chance of dying in the next couple of days.

230
00:14:07.521 --> 00:14:08.480
You're pretty dependent.

231
00:14:09.080 --> 00:14:14.030
And it's critical to have a doctors that can make decisions for people in

232
00:14:14.031 --> 00:14:18.220
healthcare teams that can do that.
On the other hand,
if you're totally healthy,

233
00:14:18.230 --> 00:14:21.740
you're trying to decide which vitamin to take or whether to take a prophylactic

234
00:14:22.160 --> 00:14:24.030
stat.
And you know,

235
00:14:24.050 --> 00:14:29.030
the locus of control is much more in the hands of the individual and the family.

236
00:14:29.031 --> 00:14:31.730
So the role of the doctor will change,
you know,

237
00:14:31.731 --> 00:14:35.270
the most imperil for their current roles would be those that look at images.

238
00:14:35.271 --> 00:14:38.460
So radiologists,
dermatologists,
because,
uh,

239
00:14:38.550 --> 00:14:43.550
it's clear that machine learning can do better for the repetitive parts.

240
00:14:43.791 --> 00:14:47.630
But think about it right now.
Those people make their living for the most part,

241
00:14:47.631 --> 00:14:52.631
looking at images serially and producing reports.

242
00:14:52.850 --> 00:14:53.450
This,

243
00:14:53.450 --> 00:14:58.400
we have a big problem and translating the findings of those reports into useful

244
00:14:58.880 --> 00:15:03.050
actions,
there's a tremendous gap to be filled there,

245
00:15:03.590 --> 00:15:06.990
uh,
for clinicians.
So the jobs will just be different.
Uh,

246
00:15:07.190 --> 00:15:09.800
but I would predict the better information we have,

247
00:15:09.801 --> 00:15:12.950
the more there's going to be a need for healthful people in health care.

248
00:15:13.140 --> 00:15:16.380
<v 1>That's seems to be a natural segue to,
um,</v>

249
00:15:16.470 --> 00:15:21.470
for those areas that are more amenable to these applications of machine learning

250
00:15:22.921 --> 00:15:27.450
and technology.
How reliable should they be and who,
uh,

251
00:15:27.690 --> 00:15:31.620
who is the person,
who are the organizations that should be,

252
00:15:32.600 --> 00:15:35.700
uh,
in charge of those things?
And,
uh,

253
00:15:36.050 --> 00:15:39.810
I asked in particular because the FDA is,

254
00:15:39.840 --> 00:15:44.580
has the purview of evaluating medical software as a device.
Uh,

255
00:15:44.581 --> 00:15:47.040
there are some things that are exempt.
There's some things that are not exempt.

256
00:15:47.100 --> 00:15:48.330
And,
um,

257
00:15:48.870 --> 00:15:53.870
I think I as a physician to have a lot of comfort and a utility when I know that

258
00:15:54.781 --> 00:15:57.690
there's a tool that's reliable.
Um,
I,

259
00:15:57.930 --> 00:16:01.040
I joke with my staff that I can answer,
uh,

260
00:16:01.070 --> 00:16:04.200
nearly every medical question they asked me with one of three answers.

261
00:16:04.380 --> 00:16:09.350
We don't know.
It depends on my new favorite is I don't remember.
And,
um,
uh,

262
00:16:09.540 --> 00:16:14.050
and I'm freely admitting that there are some things that,
uh,
are,
um,

263
00:16:14.550 --> 00:16:19.230
just,
I don't have time for that.
I'm going to need to have a tool that helps me,

264
00:16:19.710 --> 00:16:24.510
but I know how to evaluate the effect safety and effectiveness of a drug or a

265
00:16:24.511 --> 00:16:25.950
medical traditional medical device.

266
00:16:25.951 --> 00:16:30.270
But I think we're really uncharted territory when it comes to the,
the,
uh,

267
00:16:30.300 --> 00:16:35.050
what we do with algorithms.
And having sat at the highest seat in this arena,
uh,

268
00:16:35.410 --> 00:16:38.870
what were your thoughts?
Well,
I mean,
I feel compelled

269
00:16:38.870 --> 00:16:43.640
<v 2>to say my career started in 1975 with one of the first databases in medicine</v>

270
00:16:43.641 --> 00:16:46.280
before the 1970s.

271
00:16:46.310 --> 00:16:49.910
Everything in medicine was on sheets of paper and scribbled handwriting is,

272
00:16:50.570 --> 00:16:51.261
you may know.

273
00:16:51.261 --> 00:16:55.280
And I just was fortunate as a medical student to be in a place that had a

274
00:16:55.281 --> 00:17:00.020
database in cardiology that measured everything that happened to people as part

275
00:17:00.021 --> 00:17:04.520
of clinical care,
as part of the clinical record and then followed them for life.

276
00:17:04.521 --> 00:17:05.580
And so,
um,

277
00:17:05.630 --> 00:17:10.520
we had algorithms that were used in a predictive way back then for one very

278
00:17:10.521 --> 00:17:11.670
specific condition.

279
00:17:11.671 --> 00:17:16.100
And the first paper I wrote that was published in peer review literature

280
00:17:16.101 --> 00:17:20.570
predicted though within five years all diseases would be treated with the

281
00:17:20.571 --> 00:17:25.010
assistance of a computer giving decision support,
uh,

282
00:17:25.130 --> 00:17:28.070
with a doctor and patient looking together at the information.

283
00:17:28.670 --> 00:17:32.540
I was off by 40 years.
We're still not there.
Um,

284
00:17:32.570 --> 00:17:36.440
because it's such a vast enterprise and so much to do.

285
00:17:36.620 --> 00:17:41.540
But if we fast forward to my time at FDA,
um,

286
00:17:41.630 --> 00:17:44.470
you know,
I'd seen this before,
but at FDA you really see it.

287
00:17:45.200 --> 00:17:49.250
If you're really good in your field and you're very principled,

288
00:17:49.380 --> 00:17:50.880
you always do the right thing.

289
00:17:51.300 --> 00:17:54.240
It's hard to imagine that they're nefarious people in the world.

290
00:17:54.930 --> 00:17:58.260
And also that there are a lot of people are well intentioned but just don't get

291
00:17:58.261 --> 00:18:03.180
it right.
And um,
you see that at FDA,

292
00:18:03.181 --> 00:18:04.440
the mixture of those two,

293
00:18:05.010 --> 00:18:07.740
the proportion of the various people is actually quite small,

294
00:18:07.741 --> 00:18:12.240
but they can have very powerful negative effects and the history of the FDA as

295
00:18:12.241 --> 00:18:16.590
catastrophic public health events that lead to regulation.

296
00:18:16.600 --> 00:18:21.600
So it started with a horse named Jem milk wagon horse who was being used to

297
00:18:22.230 --> 00:18:26.580
develop any toxin and the horse got infected and it killed some children and

298
00:18:27.120 --> 00:18:32.070
Saint Louis,
I was 1906 that led to the first biologic control act.

299
00:18:33.210 --> 00:18:34.380
Then it was self an element.

300
00:18:34.390 --> 00:18:38.160
And there was a company that put arsenic in antibiotics.
It was sold to children,

301
00:18:38.161 --> 00:18:41.400
believe it or not,
uh,
killed a number of children.

302
00:18:41.401 --> 00:18:45.730
So that led to safety regulation.
Then there was the letter mind which,
um,

303
00:18:46.320 --> 00:18:50.720
was being,
uh,
given us free samples to pregnant women to control nausea.

304
00:18:50.721 --> 00:18:55.721
Nick cause horrible birth defects that lead to key Farber Harris in 1962.

305
00:18:57.061 --> 00:18:59.880
And it's amazing to think prior to 1962,

306
00:18:59.881 --> 00:19:03.750
you didn't have to show a drug was effective to put it on the market.

307
00:19:03.870 --> 00:19:08.870
And that at that point they reviewed 3000 drugs that were being sold routinely

308
00:19:09.301 --> 00:19:13.050
in the United States that had no evidence that he had any benefit.

309
00:19:13.770 --> 00:19:15.150
They were all pulled off the market.

310
00:19:16.020 --> 00:19:18.750
So if you don't think there are people selling stuff that's bad.

311
00:19:19.350 --> 00:19:23.070
And then for devices there was a doll con shield,
which was uh,
uh,

312
00:19:23.190 --> 00:19:27.660
pregnancy prevention device.
It caused infections and uh,

313
00:19:27.690 --> 00:19:29.850
let's just say the company that,
uh,

314
00:19:29.970 --> 00:19:33.930
made it didn't necessarily come clean with all the adverse event reports in a

315
00:19:33.931 --> 00:19:37.770
timely fashion.
So that led to more device regulation.

316
00:19:38.520 --> 00:19:43.410
So now let's think about decision support and algorithms.
Uh,

317
00:19:43.411 --> 00:19:47.670
it's understandable.
There are a lot of smart people who say,
you know,

318
00:19:47.700 --> 00:19:51.300
regulators are in the way,
um,
we can do this,
right?

319
00:19:51.301 --> 00:19:55.090
It's going to help people we know are good.
And,
um,

320
00:19:56.100 --> 00:19:59.010
let's say that there is some decision support that a lot of people use.

321
00:19:59.250 --> 00:20:00.720
There's a little glitch in the program.

322
00:20:00.721 --> 00:20:04.340
So the wrong dose of the drug is recommended.
You know,
the cover right now,

323
00:20:04.341 --> 00:20:05.011
it would be to say,
well,

324
00:20:05.011 --> 00:20:07.770
it's just a decision support the doctor makes the decision.

325
00:20:08.430 --> 00:20:12.820
But anybody that's been in a busy clinicians office knows that it 10 minutes a

326
00:20:12.821 --> 00:20:16.800
patient,
you're going to use decision support if you think it's good.

327
00:20:17.550 --> 00:20:21.840
And so this is part of my meeting with Obama.
Just a quick story on that.

328
00:20:21.841 --> 00:20:25.740
When I got offered the job,
people know I'm an enormous state basketball fan.

329
00:20:25.750 --> 00:20:27.540
I was captain of my high school team,

330
00:20:28.350 --> 00:20:32.850
I've had season tickets at Cameron indoor stadium for decades.
And um,

331
00:20:33.090 --> 00:20:36.660
he spent the first 10 minutes of our one on one meeting in the Oval Office

332
00:20:37.000 --> 00:20:41.100
telling me how much he hated Duke and he loves you,
loves you and see,

333
00:20:42.030 --> 00:20:46.320
and this went on for a while and,
but then we,
I mean like everybody here.
Yeah,

334
00:20:46.800 --> 00:20:47.410
you got it.

335
00:20:47.410 --> 00:20:51.760
I'd say Duke and the Red Sox have a lot in common or more of the Patriots,

336
00:20:51.761 --> 00:20:55.030
probably terms of national hatred.
But the last,

337
00:20:55.720 --> 00:20:58.270
the last 20 minutes,
we're literally,
um,

338
00:20:59.230 --> 00:21:04.230
about the importance of technology to the American people and how to the need,

339
00:21:04.481 --> 00:21:09.270
the absolute need to regulate it,
but to do so in a way that,
um,

340
00:21:10.360 --> 00:21:15.070
fostered creativity and innovation and allow this industry to thrive.

341
00:21:15.820 --> 00:21:19.600
And you know,
I loved it and I think,
I think there are ways to do that.

342
00:21:19.660 --> 00:21:23.260
It starts with saying that for now the FDA is not going to regulate decision

343
00:21:23.261 --> 00:21:27.390
support heavily.
It's gonna Watch it with something that's called um,

344
00:21:27.430 --> 00:21:28.750
enforcement discretion.

345
00:21:29.530 --> 00:21:33.190
The exception being if that decision support is attached to a real medical

346
00:21:33.191 --> 00:21:36.940
device,
it has consequences like your cardiac defibrillator,

347
00:21:37.330 --> 00:21:40.420
you wouldn't want the algorithm to not quite be right with the cardiac

348
00:21:40.570 --> 00:21:45.520
defibrillator.
So that's going to be regulated as part of a medical device.

349
00:21:45.940 --> 00:21:46.721
But you know,

350
00:21:46.721 --> 00:21:50.710
should you take a stat and decision support what you do with your blood pressure

351
00:21:50.711 --> 00:21:55.180
medicine,
should you get a flu vaccine?
Not going to be heavily regulated,

352
00:21:55.210 --> 00:21:57.850
but I've predicted publicly,
and I'll just say it again,

353
00:21:58.210 --> 00:22:03.210
there will be some catastrophes that will occur and that's how we'll figure out

354
00:22:04.151 --> 00:22:06.940
what should be regulated in the meanwhile,

355
00:22:07.720 --> 00:22:12.370
what are the reasons that I came to verily as part of Google is my believes that

356
00:22:12.371 --> 00:22:16.480
accompany with these resources can afford to do it right?

357
00:22:16.630 --> 00:22:21.630
Because so often what you see both in the academic world and in the industry

358
00:22:21.791 --> 00:22:24.370
world is you've got a company that's just trying to escape by,

359
00:22:24.910 --> 00:22:28.690
minimize expenses and get revenue as quickly as possible.

360
00:22:29.110 --> 00:22:34.110
There's a tendency to cut corners and often you can cut corners and get away

361
00:22:34.811 --> 00:22:39.400
with it and it doesn't affect anything.
But sometimes bad things happen then,

362
00:22:39.850 --> 00:22:44.530
you know,
this company can afford to do it right and I think should not back off.

363
00:22:45.790 --> 00:22:50.080
I do want to say one other thing.
Um,
I uh,

364
00:22:50.110 --> 00:22:55.110
advertising is a big part of this company and the role of advertising and all of

365
00:22:55.931 --> 00:23:00.700
this is a very complex issue that we better pay a lot of attention to.

366
00:23:01.570 --> 00:23:06.460
Um,
because advertising is how you will get products to people both as,

367
00:23:06.461 --> 00:23:10.930
you know,
doctors just like any other human being.
If they don't know about it,

368
00:23:10.931 --> 00:23:13.240
they can't use it.
On the other hand,

369
00:23:13.241 --> 00:23:17.110
most of advertising now is being used to sell things that aren't so good for

370
00:23:17.111 --> 00:23:17.980
people's health.

371
00:23:18.160 --> 00:23:22.510
And I would argue it's one of the reasons that our health statistics are going

372
00:23:22.511 --> 00:23:25.540
backwards now.
We're not eating well when,
you know,

373
00:23:25.541 --> 00:23:30.010
and we're spending a lot of money on things that are detrimental to health.

374
00:23:30.220 --> 00:23:32.110
<v 1>I'm thinking about the flip side of that,
um,</v>

375
00:23:32.140 --> 00:23:37.140
that with those types of interactions that people have with those things that

376
00:23:37.151 --> 00:23:39.490
they're getting prompted to look at,
um,

377
00:23:40.030 --> 00:23:43.900
is this not an opportunity to sort of look at that as information that can help

378
00:23:43.901 --> 00:23:46.010
us make better decisions that,
um,

379
00:23:46.040 --> 00:23:50.280
there is this concept of sort of pharmacol vigilance,
which is,
um,

380
00:23:50.390 --> 00:23:54.740
after a product,
uh,
pharmaceutical or medical device,
um,
uh,

381
00:23:54.840 --> 00:23:58.370
gets released to the market.
It's only then when we realize,
you know,

382
00:23:58.371 --> 00:24:01.520
what is its true effect?
Because when you're doing the trial,

383
00:24:01.521 --> 00:24:06.521
it's hard to anticipate everything and that we get real world evidence now with

384
00:24:06.891 --> 00:24:10.820
people interacting with the things that they're looking for and searching for.

385
00:24:11.150 --> 00:24:14.700
Is this not an opportunity for us to be able to look at that as a,

386
00:24:14.701 --> 00:24:18.890
a valuable data stream to help us,
you know,
maybe counter act,
you know,

387
00:24:18.891 --> 00:24:22.190
some of the,
the things that we're not able to anticipate now.

388
00:24:22.670 --> 00:24:24.710
At the same time though,
I think,
um,

389
00:24:25.100 --> 00:24:29.420
it's nobody has the right answer for like the level of privacy associated with

390
00:24:29.421 --> 00:24:33.400
this.
And I'm curious where you see,
um,
the,

391
00:24:33.430 --> 00:24:35.360
the benefit of that and the challenge of that.

392
00:24:35.890 --> 00:24:36.830
<v 2>Well,
that's a,
it's just,</v>

393
00:24:36.940 --> 00:24:41.590
it is like the world's most important opportunity now to make a difference.

394
00:24:41.820 --> 00:24:42.070
You know,

395
00:24:42.070 --> 00:24:44.800
it's certainly fascinating if you look at the health statistics and we're going

396
00:24:44.801 --> 00:24:49.801
to have an amazing meeting of go parts of Google and the gates foundation and

397
00:24:49.871 --> 00:24:54.820
Chris Murray who produce these beautiful Geo spacial maps of the world.

398
00:24:55.660 --> 00:24:59.050
They've had a focus recently on the u s cause we're looking in particularly bad

399
00:24:59.051 --> 00:25:02.260
compared to other economically developed countries.

400
00:25:02.261 --> 00:25:07.261
And the breakdown of life expectancy and health status roughly looks like the

401
00:25:08.411 --> 00:25:11.080
election map.
It's really fascinating.

402
00:25:11.860 --> 00:25:16.420
But if you go to the red counties where life expectancy is going backwards,

403
00:25:16.421 --> 00:25:20.980
mostly due to opioids,
suicide and cardiometabolic disease,

404
00:25:20.981 --> 00:25:24.310
which is roughly eating too much and not exercising,

405
00:25:24.790 --> 00:25:29.470
everybody has a cell phone,
everybody has access to information.
Um,

406
00:25:29.950 --> 00:25:34.660
it's a question of what do we do with that information then how do we include

407
00:25:34.661 --> 00:25:38.860
people in an active way that leads to better health?

408
00:25:39.760 --> 00:25:41.440
And I,
you know,
maybe people in the room,

409
00:25:41.441 --> 00:25:44.710
there are a lot of smart people here and maybe people here have the answer.

410
00:25:45.940 --> 00:25:50.800
But it seemed to me that a lot of what's going on with the technology now is,

411
00:25:50.830 --> 00:25:54.110
uh,
you know,
I think of them as a dick of circuits is the,
you know,

412
00:25:54.111 --> 00:25:57.550
the neurobiology here is pretty complicated and not so simple,

413
00:25:58.600 --> 00:26:01.210
but most of it's built on immediate delight.

414
00:26:01.450 --> 00:26:04.000
And for almost everything related to your health,

415
00:26:04.450 --> 00:26:06.570
you need to have the opposite of immediate delight.

416
00:26:06.580 --> 00:26:09.940
You need to have executive function and you'd be able to say,
no,

417
00:26:09.941 --> 00:26:13.090
I'm not going to eat that Bojangles biscuit from where I come from.

418
00:26:13.091 --> 00:26:18.070
That's a big deal.
I'm going to hold off and maybe have one a week as a tree.

419
00:26:18.760 --> 00:26:20.230
Um,
but what the,

420
00:26:20.350 --> 00:26:23.230
what the advertising is telling you to do is to eat the biscuit now.

421
00:26:23.230 --> 00:26:27.260
And when you do it feels pretty darn good.
And so,
you know,

422
00:26:27.280 --> 00:26:29.410
how do you engineer,
um,

423
00:26:29.470 --> 00:26:34.410
executive function using the technologies that currently,
um,

424
00:26:34.411 --> 00:26:37.690
lead to repetitive behaviors that are detrimental?

425
00:26:37.780 --> 00:26:41.800
That I think is a huge question.
It's one of the reasons I came,

426
00:26:41.801 --> 00:26:46.020
I was hoping we could figure out because it's not so not so obvious,

427
00:26:47.070 --> 00:26:49.840
but we got to involve people in,
you know,

428
00:26:49.860 --> 00:26:53.190
what I'm telling the university side is I don't know of anything more important

429
00:26:53.230 --> 00:26:58.230
for universities right now then to have serious academic intellectual activity

430
00:27:00.541 --> 00:27:04.770
about,
um,
how societal expectations of technology,

431
00:27:06.330 --> 00:27:09.630
what are the,
what should the rules of the game be that are good for society.

432
00:27:09.960 --> 00:27:14.940
You know,
you need policy schools,
law schools,
medical schools,
um,

433
00:27:15.360 --> 00:27:19.380
English department's history departments to come together and think this
through.

434
00:27:20.430 --> 00:27:24.750
I'll just,
I'll say one other thing cause I,
I sort of make fun of Davos.

435
00:27:24.750 --> 00:27:26.070
I went this year,
I said,
you know,

436
00:27:26.071 --> 00:27:30.630
it's where millionaires still billionaires what the middle class is like,
but um,

437
00:27:31.650 --> 00:27:34.800
what are the themes?
There is this fourth industrial revolution,

438
00:27:34.801 --> 00:27:38.340
which I think is very real.
You know,
the first was water power,

439
00:27:38.341 --> 00:27:43.110
the second was electricity,
the third was an information technology.

440
00:27:43.111 --> 00:27:46.950
We're just recovering from that revolution.
Now we've got the fourth,

441
00:27:46.951 --> 00:27:49.740
which is the,
uh,
merger of biological,

442
00:27:49.741 --> 00:27:54.741
physical and information sciences into a single sphere.

443
00:27:54.960 --> 00:27:59.710
It's dramatic and it has profound consequences for,
um,

444
00:27:59.820 --> 00:28:02.580
our country and for people all over the world.

445
00:28:03.000 --> 00:28:05.520
I think we've learned we can't just leave this to chance.

446
00:28:05.521 --> 00:28:09.810
We've got to have serious people think about how it should be dealt with.

447
00:28:11.820 --> 00:28:14.790
<v 1>Uh,
I'm going to get to a couple of others questions back.</v>

448
00:28:15.150 --> 00:28:19.780
I just want to point out that this was Rob's official FDA portrait and uh,

449
00:28:19.850 --> 00:28:24.650
a couple of weeks after joining barely,
this is his official,
they're like,
uh,

450
00:28:24.660 --> 00:28:28.380
so,
uh,
we,
we,
we got to him real quickly.
I noticed.
Yeah.

451
00:28:28.381 --> 00:28:33.000
Notice a Dr. Harrington sent you a Stanford t-shirt shirt.
Yes,
yes,
yes.

452
00:28:33.360 --> 00:28:37.050
Yeah,
we were,
we were trained for UNC tee shirt,
but it was last minute.

453
00:28:37.920 --> 00:28:39.260
Um,
um,

454
00:28:39.900 --> 00:28:43.500
so you introduced me to this concept.
Um,

455
00:28:43.740 --> 00:28:47.670
and I think it might be related to some of the things that you just touched on

456
00:28:47.671 --> 00:28:50.280
is like how do we sort of view this information differently?

457
00:28:50.281 --> 00:28:54.960
And this is a actually a university of Washington course calling bullshit in the

458
00:28:54.961 --> 00:28:57.870
age of big data and apparently it's incredibly oversubscribed.

459
00:28:58.200 --> 00:29:03.200
So how do we teach people to sort of understand what they're being put?

460
00:29:03.260 --> 00:29:03.970
What's put in front of front

461
00:29:03.970 --> 00:29:06.180
<v 2>end?
Yeah.
Well,
I mean,</v>

462
00:29:06.181 --> 00:29:08.580
I'm going to just talk about the health and medicine part of this.

463
00:29:08.581 --> 00:29:12.180
There's a whole other area in politics that we could spend a long time talking

464
00:29:12.181 --> 00:29:16.460
about.
But I mean you could say the FDA's job basically in society as a called

465
00:29:16.461 --> 00:29:20.670
bullshit,
but to do it with enforcement when it happens.

466
00:29:20.671 --> 00:29:22.540
And the problem we have is that,
um,

467
00:29:23.040 --> 00:29:26.430
if you just say AI or machine learning in much of academia,

468
00:29:26.440 --> 00:29:28.440
now people step back and say it must be great,

469
00:29:28.740 --> 00:29:32.480
but there's a lot of bullshit and much of it,

470
00:29:32.481 --> 00:29:36.150
it comes from technically well done things that just from people who don't

471
00:29:36.151 --> 00:29:41.151
understand things like confounding and lead time bias and context of the

472
00:29:41.531 --> 00:29:45.670
research.
Um,
and it can lead to very tragic,

473
00:29:45.850 --> 00:29:49.180
wrong answers.
And there are many,
many,
many examples of this.

474
00:29:49.660 --> 00:29:53.650
What fascinated me in this course is it's in the context of big data of all

475
00:29:53.651 --> 00:29:56.530
types.
It's a university course,
not a health course,

476
00:29:57.070 --> 00:29:58.850
but essentially the curriculum is,

477
00:29:58.851 --> 00:30:02.440
is what you would learn in what clinical epidemiology one oh one.

478
00:30:02.441 --> 00:30:04.780
If you are in a school of public health or a medical school,

479
00:30:04.781 --> 00:30:07.450
it's how to understand context,

480
00:30:07.480 --> 00:30:12.460
how to know that you need the right teams of people who have a track record of

481
00:30:12.970 --> 00:30:16.870
producing reliable,
uh,
results.
Not just accurate,

482
00:30:16.900 --> 00:30:19.420
but we're liable for the purpose of the analysis,

483
00:30:19.900 --> 00:30:24.900
particularly if it's going to be used to make a decision or derive an action.

484
00:30:25.480 --> 00:30:30.130
And then there's having the courage to call bullshit because,
um,
when you do it,

485
00:30:30.490 --> 00:30:33.220
I mean,
the great thing about the FDA,
you can,
you,
you know,

486
00:30:33.221 --> 00:30:36.580
you get vilified everyday by the press and by Congress,

487
00:30:36.940 --> 00:30:39.970
but you actually have the legal authority to call bullshit and do something

488
00:30:39.971 --> 00:30:41.920
about it.
Um,
you know,

489
00:30:41.921 --> 00:30:44.890
for people who are in everyday work and they see something going on in the

490
00:30:44.891 --> 00:30:49.600
workplace,
this is very important for health research.
That's not right.

491
00:30:50.770 --> 00:30:55.240
People cutting corners,
uh,
it's easier not to speak,
uh,

492
00:30:55.420 --> 00:30:59.440
to not say anything about it and bad things happen when that occurs.

493
00:30:59.660 --> 00:31:02.260
<v 1>So,
um,
maybe you can hold us accountable.
Um,</v>

494
00:31:02.600 --> 00:31:06.130
I think we're all familiar with the phrase dog fooding that,
um,

495
00:31:06.620 --> 00:31:11.620
one of the things that is a tremendous asset for us in this company and the

496
00:31:12.011 --> 00:31:14.270
alphabets fear is that we have a ready,

497
00:31:14.271 --> 00:31:17.000
willing pool of people who are willing to try the newest,

498
00:31:17.001 --> 00:31:20.630
the latest to contribute their information.
Um,

499
00:31:20.900 --> 00:31:22.580
how do we avoid sort of

500
00:31:24.080 --> 00:31:27.230
building in those biases from your,
your description,

501
00:31:27.670 --> 00:31:31.730
you are well versed in sort of looking for these confounding things coming

502
00:31:31.731 --> 00:31:34.550
through old school methods of epidemiology.

503
00:31:34.551 --> 00:31:39.260
But for many of us it's a,
the lure of we have a ready pool,

504
00:31:39.261 --> 00:31:41.510
we can generate data quickly.
Um,

505
00:31:41.570 --> 00:31:45.860
what is it that you think that we should be paying more attention to when it

506
00:31:45.861 --> 00:31:50.030
comes to this especially,
uh,
is this,
uh,
it might be internally valid,

507
00:31:50.031 --> 00:31:53.260
but it's not externally valid.
Well,
uh,
the,

508
00:31:53.600 --> 00:31:54.790
<v 2>anything about the FDA,</v>

509
00:31:54.791 --> 00:31:57.220
at least in the drug and device are right and we didn't get a chance to talk

510
00:31:57.221 --> 00:32:02.020
about food and cosmetics and animal health,
which is,
you know,

511
00:32:02.021 --> 00:32:05.170
the FDA regulates 20% of the economy.
Half of it is further.

512
00:32:05.830 --> 00:32:08.290
We could have a long discussion about that.
I'll avoid that for now,

513
00:32:08.291 --> 00:32:12.270
but still thought that the biscuits,
sorry,
by law,
uh,

514
00:32:12.280 --> 00:32:15.820
companies have to demonstrate that their products are safe and effective in

515
00:32:15.821 --> 00:32:20.140
prospective studies.
And so the,
in health,

516
00:32:20.770 --> 00:32:23.890
um,
it's actually pretty easy,
but you have to have the rig,

517
00:32:23.920 --> 00:32:26.260
you have to have the rigor and the courage to do it right.

518
00:32:26.950 --> 00:32:29.920
And that is you can do all the stuff you want to do as you're developing your

519
00:32:29.921 --> 00:32:31.720
concepts and ideas.
Ultimately,

520
00:32:31.721 --> 00:32:36.130
the test is a prospective study that measures health outcomes.

521
00:32:37.330 --> 00:32:41.960
And if your product doesn't improve health outcomes,
she got to ask the question,

522
00:32:42.470 --> 00:32:47.150
why should anyone pay for it?
And in many cases,

523
00:32:47.151 --> 00:32:51.440
a surprising finding is that there's actually a detrimental effect or something

524
00:32:51.441 --> 00:32:55.910
that you thought was going to be great.
Um,
you know,

525
00:32:55.911 --> 00:32:58.550
and there,
there are many,
many examples I could give,

526
00:32:58.551 --> 00:33:01.640
but I think the same is going to be true of information technology.

527
00:33:01.641 --> 00:33:02.540
It looks logical.

528
00:33:02.541 --> 00:33:06.500
It makes sense that out of work then you deploy it in for reasons that you

529
00:33:06.501 --> 00:33:10.040
didn't understand the wrong things happen.
Now,

530
00:33:10.041 --> 00:33:13.340
if you're selling shoes,
it's probably okay.

531
00:33:13.341 --> 00:33:15.140
If people get shoes that are uncomfortable,

532
00:33:15.141 --> 00:33:18.380
they'll throw them away and buy some new shoes.

533
00:33:19.040 --> 00:33:22.170
If someone's making a life or death decision and they're now dead,

534
00:33:22.370 --> 00:33:27.020
you can't take it back and you cause,
so,
um,

535
00:33:27.140 --> 00:33:28.340
it's a different game.

536
00:33:28.940 --> 00:33:32.270
But the beauty of it is due the outcome studies.

537
00:33:32.271 --> 00:33:37.130
Figure out if what you're doing works and involve,
um,

538
00:33:37.160 --> 00:33:41.240
people or don't have a financial interest in the outcome in the studies,

539
00:33:41.660 --> 00:33:42.860
which is another hard part.

540
00:33:43.270 --> 00:33:45.920
<v 1>It's one of these native tensions that,
uh,</v>

541
00:33:46.810 --> 00:33:49.780
we know that there are things that worked well in the healthcare arena.

542
00:33:50.140 --> 00:33:54.020
At the same time,
we want the advantages of what we know works well in our,

543
00:33:54.080 --> 00:33:56.300
our Rena technology and that,
um,

544
00:33:56.350 --> 00:33:58.840
it is this sort of uncomfortable tension that we,

545
00:33:59.230 --> 00:34:03.930
we sometimes think the adage move fast break things is,
is the,

546
00:34:04.100 --> 00:34:05.260
the thing that makes us successful,

547
00:34:05.261 --> 00:34:09.620
but how does that translate into a more controlled,
uh,

548
00:34:09.700 --> 00:34:11.650
arena where the consequences are much higher?

549
00:34:12.040 --> 00:34:14.050
<v 2>Like I said,
I don't think anyone saw this yet.</v>

550
00:34:14.320 --> 00:34:16.300
There's a reason that silicon valley has failed.

551
00:34:16.301 --> 00:34:19.240
I was 100% of the time in this arena.
Okay.

552
00:34:19.570 --> 00:34:22.210
And part of it is this,
um,

553
00:34:22.360 --> 00:34:26.470
you can't just take human lives and do move fast.
You know,

554
00:34:26.471 --> 00:34:30.460
a brick in the face of a human being is not a trivial issue.

555
00:34:30.910 --> 00:34:32.200
On the other hand,
you know,

556
00:34:32.201 --> 00:34:35.680
taking five years to get an answer for something I technology that can move

557
00:34:35.681 --> 00:34:36.130
faster.

558
00:34:36.130 --> 00:34:40.930
This is what President Obama was all over me about is how to do this.

559
00:34:41.410 --> 00:34:44.380
And I think the key is informing people of what you're doing.

560
00:34:44.410 --> 00:34:49.410
So if you're gonna develop a system that will figure out how to use technology

561
00:34:49.841 --> 00:34:50.261
quickly,

562
00:34:50.261 --> 00:34:54.010
make sure that the people who are getting healthcare in that system understand

563
00:34:54.011 --> 00:34:54.521
what you're doing.

564
00:34:54.521 --> 00:34:57.640
You're not doing it behind their back secretly back to Facebook again.

565
00:34:58.090 --> 00:35:02.740
They need to understand it and participate.
That's why we don't call research.

566
00:35:02.800 --> 00:35:05.260
We used to call them subjects when you did research.

567
00:35:05.261 --> 00:35:08.330
Like they're like inanimate objects that you're doing experiments.

568
00:35:08.360 --> 00:35:11.200
So we now call them participants because they have a right,

569
00:35:11.980 --> 00:35:16.360
a human right to be involved and understand what you're doing and why you're

570
00:35:16.361 --> 00:35:17.110
doing it.

571
00:35:17.110 --> 00:35:20.920
I believe based on what we're seeing in the baseline study that you mentioned,

572
00:35:21.400 --> 00:35:25.260
that if you do inform people,
there'll be excited about it.
Uh,

573
00:35:25.670 --> 00:35:27.640
they'll want to participate in,

574
00:35:27.850 --> 00:35:30.730
particularly if you give them feedback about what you're learning as you go

575
00:35:30.731 --> 00:35:34.330
along.
So I,
I think the transparency,

576
00:35:34.331 --> 00:35:38.610
the feedback and the being in it for the longterm is the other thing if you want

577
00:35:38.611 --> 00:35:41.640
to make about quickly now there's some pretty easy ways to do it.

578
00:35:42.330 --> 00:35:44.490
May Not be good for people in the longterm.

579
00:35:45.070 --> 00:35:49.440
<v 1>You've given us some bracing insights about what,
what we can do better.</v>

580
00:35:50.590 --> 00:35:53.480
I'm going to call up this guy named rob killer.

581
00:35:53.690 --> 00:35:58.230
And when he talked about what we can do with,
uh,
the clinical trial,

582
00:35:58.560 --> 00:36:03.560
this was a formative part of your contribution to the world of medicine,

583
00:36:03.871 --> 00:36:07.200
the large scale clinical trial that produces evidence based medicine that we

584
00:36:07.201 --> 00:36:08.990
rely on.
Yet you,

585
00:36:09.060 --> 00:36:12.600
you're saying some bracing things and critical things about that.

586
00:36:12.601 --> 00:36:15.480
Can you elaborate a little bit about,
uh,
this?
Yeah.
You know,

587
00:36:15.481 --> 00:36:17.860
I just got screwed up early in my career

588
00:36:17.920 --> 00:36:21.390
<v 2>because I came along and someone asked me to run the cardiac care unit.</v>

589
00:36:21.391 --> 00:36:23.580
At the time we didn't know what causes heart attacks.

590
00:36:24.270 --> 00:36:29.270
A 56 year old men are routinely dropping over dead from heart attacks.

591
00:36:29.941 --> 00:36:34.090
Everyone was smoking cigarettes,
very high rates.
And uh,

592
00:36:34.260 --> 00:36:38.220
an enterprising cardiologists did an angiogram and showed it was blood clots in

593
00:36:38.221 --> 00:36:41.600
the coronary arteries and the rice was on the develop treatments that worked.

594
00:36:42.150 --> 00:36:43.560
And for some strange reason,

595
00:36:43.561 --> 00:36:48.480
it became the norm all around the world to enter people into randomized trials.

596
00:36:49.080 --> 00:36:51.780
So we can do these very big randomized trials very quickly,

597
00:36:51.781 --> 00:36:55.680
get the answer and move on.
And the risk of being dead now,

598
00:36:55.681 --> 00:36:56.670
if you have a heart attack,

599
00:36:56.671 --> 00:37:01.671
is less than half of what it was when we started because of this rapid learning

600
00:37:01.741 --> 00:37:04.500
system using randomization to get the right answer.

601
00:37:04.830 --> 00:37:07.020
Because most of what we tried actually didn't work.

602
00:37:07.080 --> 00:37:10.020
People have forgotten about that.
Most of our trials,

603
00:37:10.380 --> 00:37:15.000
the treatment wasn't effective even though we thought it should be.
Um,

604
00:37:15.780 --> 00:37:20.780
but for some reason the whole thing got gummed up in bureaucracy.

605
00:37:20.911 --> 00:37:25.560
And collecting,
uh,
you know,
part of the problem was at the time,
uh,

606
00:37:25.590 --> 00:37:29.730
medical records are all written on paper.
So you had to create a separate,
uh,

607
00:37:30.270 --> 00:37:35.270
enterprise of data systems to collect data separately from the clinical care,

608
00:37:36.000 --> 00:37:36.811
very expensive.

609
00:37:36.811 --> 00:37:40.150
Then you had to fly nurses around airplanes to check to make sure the records

610
00:37:40.151 --> 00:37:44.460
don't match.
It's a,
you know,
amazing amounts of bureaucratic human labor.

611
00:37:45.150 --> 00:37:47.310
The result is that even in cardiology,

612
00:37:47.311 --> 00:37:52.311
85% of our major recommendations are not based on high quality evidence and

613
00:37:52.351 --> 00:37:55.230
that's the best there is.
You know,
God bless you.

614
00:37:55.260 --> 00:37:57.390
My little brother is an orthopedic surgeon.
God bless you.

615
00:37:57.391 --> 00:38:01.800
If you need orthopedic surgery,
it does a lot of good.
But you know,

616
00:38:01.801 --> 00:38:06.150
it's less than 5% of the major recommendations there are based on high quality

617
00:38:06.930 --> 00:38:10.020
evidence.
So we need a different evidence generation system.

618
00:38:10.650 --> 00:38:13.530
I'm very excited that,
um,
you know,

619
00:38:13.531 --> 00:38:18.531
people in this room can come together and produce something that's far superior

620
00:38:19.231 --> 00:38:22.590
with much more ramp and learning,
but we have to,

621
00:38:22.620 --> 00:38:26.000
I call it God's gift of randomization.
Um,
I,

622
00:38:26.240 --> 00:38:30.720
I don't think that just analyzing our way with a bunch of data because of all

623
00:38:30.721 --> 00:38:35.721
the complex biases that are involved is enough about that evidence generation.

624
00:38:37.750 --> 00:38:38.320
Um,

625
00:38:38.320 --> 00:38:43.320
are we trying to have it both ways that we want to make it easily accessible to

626
00:38:45.461 --> 00:38:48.130
produce this information,
to use smartphones,

627
00:38:48.610 --> 00:38:51.870
other types of activity trackers at the same time as the,

628
00:38:51.880 --> 00:38:55.360
these aren't medical grade
units.

629
00:38:55.361 --> 00:38:58.300
So are we trying to get more data,

630
00:38:58.301 --> 00:39:03.301
but at the same time we cannot fully trust that the quality of that data.

631
00:39:03.970 --> 00:39:07.750
I mean,
I think we,
I think we got this ride at the FDA and it's playing out,

632
00:39:07.810 --> 00:39:11.830
you know,
since I've left,
I think in a very pleasing way.
I mean,

633
00:39:11.831 --> 00:39:14.530
basically if you're otherwise healthy and there's a device,

634
00:39:14.531 --> 00:39:19.150
it's meant to remind you to exercise or count your heart rate or stuff,

635
00:39:19.180 --> 00:39:23.980
that's fine.
That's a consumer device.
Doesn't have to be perfect.
You know,

636
00:39:23.981 --> 00:39:28.420
it'll get better and better if you've got a heart failure and you're at risk of

637
00:39:28.421 --> 00:39:30.110
dropping over dead.
Uh,

638
00:39:30.460 --> 00:39:35.170
that device needs to be accurate and it,
um,
I think,

639
00:39:35.370 --> 00:39:35.680
uh,

640
00:39:35.680 --> 00:39:40.680
we need to have the discipline and rigor to perfecting engineering to the point

641
00:39:41.471 --> 00:39:46.300
where it really can be reliable and it's not so easy.

642
00:39:46.360 --> 00:39:51.160
I think we're finding,
but I'm very optimistic.
I think we will be able to do it.

643
00:39:52.030 --> 00:39:56.470
But it also means that we've got to involve like teams of people who really

644
00:39:56.471 --> 00:40:00.670
understand biomedical data and how it's used in the clinical setting,

645
00:40:00.671 --> 00:40:05.050
working with the engineers very closely so that,
uh,

646
00:40:05.110 --> 00:40:09.130
the information can really be used to inform good decisions.

647
00:40:09.940 --> 00:40:10.860
And,
um,

648
00:40:10.990 --> 00:40:14.260
I think the FDA is going to need to look carefully at a lot of the algorithms

649
00:40:14.261 --> 00:40:15.670
when they first come out,

650
00:40:15.671 --> 00:40:20.280
which will be a fascinating exercise.
I would like to,

651
00:40:20.310 --> 00:40:24.630
<v 3>to give the opportunity for the audience to ask questions,
Brian.
Sure.</v>

652
00:40:25.470 --> 00:40:29.670
So first as a pats fan,
speaking to a Duke fan expression,

653
00:40:29.671 --> 00:40:34.610
I'll remind you of they hate us cause they ain't us.
Um,

654
00:40:34.670 --> 00:40:37.320
so this,
this is a little bit off the topic,

655
00:40:37.321 --> 00:40:41.280
but I'm sure you'll have an opinion on it.
Uh,
what do you think are,

656
00:40:41.490 --> 00:40:46.230
and this is more of a business question,
hospitals today,
I believe,
uh,
you know,

657
00:40:46.231 --> 00:40:48.090
health care hospitals in particular,

658
00:40:48.091 --> 00:40:51.630
I headed for a major disruption from a business model perspective.

659
00:40:52.020 --> 00:40:56.190
What are two or three things that you think hospitals need to start doing today

660
00:40:56.191 --> 00:41:01.191
from a business perspective to sustain themselves or to continue to provide the

661
00:41:01.771 --> 00:41:03.720
value that they provide for the population?

662
00:41:03.960 --> 00:41:07.650
<v 4>There was some interesting traffic on Twitter today about this related to an</v>

663
00:41:07.651 --> 00:41:08.431
article came out,

664
00:41:08.431 --> 00:41:12.570
so I would make a big division between urban areas and rural areas.

665
00:41:13.200 --> 00:41:17.520
It turns out that rural hospitals need to become community centers.
Basically,

666
00:41:17.521 --> 00:41:21.210
if you look at rural America where this demise that I talked about is happening

667
00:41:21.600 --> 00:41:26.600
in terms of health statistics is closely related to the economics and a loss of

668
00:41:26.671 --> 00:41:29.040
jobs and all that sort of stuff.

669
00:41:29.830 --> 00:41:33.080
It turns out at Duke we are affiliated with a group called lifepoint,

670
00:41:33.260 --> 00:41:37.970
which is a for profit system that buys rural hospitals and what's happening to

671
00:41:37.971 --> 00:41:40.330
those hospitals when they're well managed as they,

672
00:41:40.390 --> 00:41:42.190
they are a sort of a way station.

673
00:41:42.210 --> 00:41:47.000
They take care of people who need short essays or have a sort of,

674
00:41:47.060 --> 00:41:49.670
not quite nursing home care yet,

675
00:41:50.150 --> 00:41:55.150
but also become community centers because the main employment in these towns is

676
00:41:55.161 --> 00:41:58.190
a education and health care.
Um,

677
00:41:58.820 --> 00:42:00.410
and so that's,

678
00:42:00.470 --> 00:42:04.750
I think the rural hospitals need to be transformed.
My friend Patrick Condom,

679
00:42:04.751 --> 00:42:09.590
I just brought a great Jamma editorial about the pathway for rural hospitals.

680
00:42:10.250 --> 00:42:12.830
Urban Hospitals I think,
um,

681
00:42:13.340 --> 00:42:17.960
can't help themselves right now and there's no amount of efficiency in the

682
00:42:17.961 --> 00:42:20.960
current payment scheme that they can do that will fix the problem.

683
00:42:20.961 --> 00:42:25.030
This is just my opinion.
So it's fair to say they ought to continue to be as a,

684
00:42:25.190 --> 00:42:30.050
to work more and more on getting more efficient,
implementing decision support,

685
00:42:30.051 --> 00:42:30.884
et cetera.

686
00:42:31.010 --> 00:42:34.550
The fundamentally what's needed in a different payment system is to keep people

687
00:42:34.551 --> 00:42:36.830
out of the hospital.
Um,

688
00:42:36.860 --> 00:42:41.720
and then to optimize the use of the hospital using very high grade,

689
00:42:41.750 --> 00:42:44.450
um,
intelligence.
I'm here,

690
00:42:44.451 --> 00:42:48.920
I'll just combined Clayton Christensen's a focus factory thing.

691
00:42:49.350 --> 00:42:52.430
It ought to be that you stay out of a hospital unless you really need one.

692
00:42:53.000 --> 00:42:54.050
Then when you really need one,

693
00:42:54.051 --> 00:42:57.590
y'all to go to a place that's really good at what it does for the problem that

694
00:42:57.591 --> 00:42:59.510
you have,
even if you have to travel,

695
00:43:00.110 --> 00:43:05.110
which is a disruption to the cultural things that we've just been talking about.

696
00:43:05.630 --> 00:43:07.880
But there's no way that in the current payment scheme,

697
00:43:07.881 --> 00:43:10.250
hospitals can afford to make that transition.

698
00:43:10.251 --> 00:43:14.960
So they're going to have to be forced to do it.
And I hope,
uh,

699
00:43:15.150 --> 00:43:19.940
the role of Silicon Valley here with all of its financial prowess as well as

700
00:43:19.941 --> 00:43:24.770
information will be to help push the country to the transition that needs away

701
00:43:24.771 --> 00:43:27.680
from fee for service.
Madison,
which I think is the culprit.

702
00:43:30.610 --> 00:43:34.150
<v 5>Hi.
Um,
my name is Virginia.
I also work with hospitals,
but Brian's,</v>

703
00:43:34.151 --> 00:43:38.710
so I have another hospital's related question.
But um,
from one night,
no,

704
00:43:38.711 --> 00:43:42.070
about machine learning and artificial intelligence is that you get better if you

705
00:43:42.071 --> 00:43:45.040
have bigger data sets and they're more interconnected and bigger,

706
00:43:45.041 --> 00:43:48.520
larger data set.
So there have been great moves to do that.

707
00:43:48.521 --> 00:43:53.521
But what I observed with hospitals is that it's super siloed data that lives in

708
00:43:53.710 --> 00:43:54.940
every different spot.

709
00:43:55.030 --> 00:43:59.920
And while we have made major inroads with digitizing data,

710
00:44:00.130 --> 00:44:03.840
it's not connected to anything.
And,
um,

711
00:44:04.330 --> 00:44:08.950
data about our most vulnerable and needy populations actually live in a lot of

712
00:44:08.951 --> 00:44:10.000
these hospitals,

713
00:44:10.001 --> 00:44:13.810
which arguably are not technology companies are not led by technology people or

714
00:44:13.811 --> 00:44:17.500
people who understand data and again,
have these siloed data structures.

715
00:44:17.501 --> 00:44:22.501
So what can be the next big step to connect data to allow for machine learning

716
00:44:24.551 --> 00:44:28.480
algorithms to get smarter that aren't just kind of like pockets of success that

717
00:44:28.481 --> 00:44:32.460
are really scaled interconnective systems

718
00:44:32.460 --> 00:44:35.610
<v 4>that allow for better evidenced based medicine for your little brother?</v>

719
00:44:36.310 --> 00:44:41.300
A couple of things.
I completely agree with you,
um,
about,
uh,

720
00:44:41.340 --> 00:44:46.320
the nature of the problem,
but I would urge a couple of changes in terminology.

721
00:44:46.321 --> 00:44:49.710
So it's not,
you know,
Americans coalescing into,

722
00:44:49.740 --> 00:44:51.750
it's actually quantifiable.

723
00:44:51.751 --> 00:44:56.751
It's a little over 650 health systems now that own the hospitals.

724
00:44:57.570 --> 00:45:00.510
Uh,
it's been sad and I haven't seen the data yet,

725
00:45:00.511 --> 00:45:02.610
but a pretty reliable person told me,
if you look,

726
00:45:02.611 --> 00:45:07.611
there are about 150 of these health systems that care for about 70% of the

727
00:45:07.711 --> 00:45:11.820
American population now.
And,
uh,
uh,

728
00:45:11.850 --> 00:45:15.060
all of them have a business interest in,
as you know,

729
00:45:15.061 --> 00:45:19.170
in aggregating their information in the data,
whatever the right term is,

730
00:45:19.171 --> 00:45:22.740
warehouses,
legs,
pools,
all that stuff.
Um,

731
00:45:23.160 --> 00:45:27.930
so that they can conduct their business and stay viable financially because of

732
00:45:27.931 --> 00:45:28.980
fee for service.

733
00:45:28.981 --> 00:45:33.120
The data they are aggregating tends to be not quite accurate,

734
00:45:33.121 --> 00:45:34.710
but it's still usable for a lot of things.

735
00:45:34.710 --> 00:45:39.660
I don't want to sound totally cynical about that.
Um,
but the culprit is a,

736
00:45:39.730 --> 00:45:44.040
they think there's great value in that data and so they're not very willing to

737
00:45:44.041 --> 00:45:48.650
share it or give it up often under the guise of HIPAA and,
um,

738
00:45:49.150 --> 00:45:49.490
and,

739
00:45:49.490 --> 00:45:54.490
and privacy issues and concerns which are real but shouldn't pro prohibit this.

740
00:45:56.880 --> 00:46:00.630
Um,
I'd say the second thing is,
uh,
in our,
in our,
uh,

741
00:46:00.720 --> 00:46:05.070
in our center at Duke for a data science,
uh,

742
00:46:05.670 --> 00:46:08.670
I personally think we need to quit just saying machine learning.

743
00:46:08.671 --> 00:46:12.350
We need to say quantitative methods because a lot of what needs to be done

744
00:46:12.370 --> 00:46:15.030
doesn't require machine learning.
It's just if you had the data.

745
00:46:15.790 --> 00:46:19.950
And so what I would do is focus a lot on the curation of the information to
make,

746
00:46:19.951 --> 00:46:21.360
get it in usable form,

747
00:46:21.960 --> 00:46:26.310
and then have teams of people who use the information for the purpose using the

748
00:46:26.311 --> 00:46:30.600
right quantitative method for that purpose.
And then,

749
00:46:30.930 --> 00:46:34.320
um,
what,
what I'd,
what I'd say is we've got to

750
00:46:36.180 --> 00:46:39.600
break down the data silos.
And I think our best weapon,

751
00:46:39.601 --> 00:46:41.700
there's going to be patient advocacy groups.

752
00:46:42.810 --> 00:46:46.020
We really worked on this and the Obama administration as you know,

753
00:46:46.021 --> 00:46:49.800
to try to get over the hurdle,
um,
you know,

754
00:46:49.830 --> 00:46:53.040
if they're just been another four years,
a lot of things would be different.
But,

755
00:46:53.220 --> 00:46:57.990
um,
you know,
we couldn't get there.
And you know,
we are doing things,

756
00:46:57.991 --> 00:47:02.640
as I said,
like the PCRF a sentinel.
Um,
they're,
you know,

757
00:47:02.641 --> 00:47:05.550
in Pcrf we got a third of America's health systems,
uh,

758
00:47:05.580 --> 00:47:09.930
sharing with PCRF is,
it's a people centered research foundation.

759
00:47:10.260 --> 00:47:14.460
It was created through a funding from Pikori,
which was,
um,

760
00:47:15.030 --> 00:47:19.710
it's paid for out of,
um,
uh,
attacks on the Medicare,

761
00:47:19.730 --> 00:47:23.190
um,
fund in insurance companies.

762
00:47:23.450 --> 00:47:28.320
And there's a board that oversees it,
which is a industry,
academia government.

763
00:47:28.650 --> 00:47:30.070
You know,
the suspects,

764
00:47:30.460 --> 00:47:34.270
the goal is to do research using aggregate information,

765
00:47:34.630 --> 00:47:39.630
but what you want is something even much more granular than what we have in

766
00:47:40.331 --> 00:47:42.970
that.
But we're showing that given the right incentives,

767
00:47:43.000 --> 00:47:47.170
people can share data on a massive scale.

768
00:47:48.010 --> 00:47:52.360
But I think actually the place to divert right now for a period of time is this

769
00:47:52.361 --> 00:47:54.250
thing that we were talking about.
Um,

770
00:47:54.580 --> 00:47:58.390
getting the social roles right and having people understand,

771
00:47:59.470 --> 00:48:00.303
uh,

772
00:48:00.310 --> 00:48:05.310
what the rules of the game need to be so it can be shared because technically I

773
00:48:05.351 --> 00:48:08.620
don't think it's a big issue right now.
It's all cultural.

774
00:48:09.380 --> 00:48:14.360
<v 6>One of the dominant ways to get paid in the researching development of,</v>

775
00:48:14.420 --> 00:48:18.800
of drugs and of devices is to charge far based on a monopoly.

776
00:48:19.710 --> 00:48:20.543
Um,

777
00:48:20.600 --> 00:48:24.830
do you see our intellectual property laws as well calibrated to the human health

778
00:48:24.831 --> 00:48:27.020
space today and in the future?

779
00:48:27.680 --> 00:48:30.800
<v 4>I'd say the general principle,
I don't know another way to do it.</v>

780
00:48:30.890 --> 00:48:35.890
Then just say you've got to have some period of exclusivity to recoup the costs

781
00:48:37.071 --> 00:48:39.470
of development.
When you say calibrated,

782
00:48:39.471 --> 00:48:43.940
that's a really important term because it's different for different sarco it

783
00:48:43.941 --> 00:48:47.450
should be different for different circumstances.
Um,

784
00:48:47.840 --> 00:48:50.780
and uh,
I don't know that if the current,

785
00:48:51.020 --> 00:48:55.940
if the current duration of time is right for the circumstances that we're

786
00:48:55.941 --> 00:49:00.380
currently in,
um,
you know,
a simple way.

787
00:49:00.410 --> 00:49:03.040
I think what you're referring to,
um,

788
00:49:03.080 --> 00:49:07.520
obviously the more rigorous we are about what gets on the market,
the more,
um,

789
00:49:08.330 --> 00:49:09.890
actually one argue with myself here,

790
00:49:09.891 --> 00:49:14.360
but in theory the more you'd have to spend on a development to get it right.

791
00:49:15.020 --> 00:49:18.500
But I would argue if we use the technology that you guys are developing and

792
00:49:18.501 --> 00:49:20.780
automate the research systems,

793
00:49:20.990 --> 00:49:24.140
we could radically reduce the cost of development because the biggest costs in

794
00:49:24.141 --> 00:49:27.320
error,
the late phase,
uh,
clinical trials.

795
00:49:27.890 --> 00:49:29.330
But then you just ask a question,

796
00:49:29.331 --> 00:49:33.500
if you have a drug that's life saving and you have a legal monopoly,

797
00:49:33.501 --> 00:49:36.860
which is a deal with society to give you a chance to recoup the costs,

798
00:49:37.550 --> 00:49:41.930
how do you set a price on that when the only way people can survive is if they

799
00:49:41.931 --> 00:49:44.900
get that drug.
And that's where I think we've got it.

800
00:49:45.590 --> 00:49:47.630
Not Not right at this point.

801
00:49:48.020 --> 00:49:51.230
So you can think of that about that either as length of time in which you have

802
00:49:51.231 --> 00:49:56.120
that or some other mechanism of setting prices that's more fair to people who

803
00:49:56.121 --> 00:50:00.610
are vulnerable.
So I think this is and you know,

804
00:50:00.620 --> 00:50:04.820
but let's also now think about the big thing looming as soon as we saw of all

805
00:50:04.821 --> 00:50:06.470
the short term problems we've got,
um,

806
00:50:06.471 --> 00:50:11.471
Alzheimer's and other longterm chronic diseases where you will know that people

807
00:50:11.481 --> 00:50:13.880
are at risk loan before they have the first symptom.

808
00:50:14.510 --> 00:50:19.510
So how do you develop a treatment and price it right when it would be preemptive

809
00:50:20.710 --> 00:50:24.890
and worth a whole lot,
but you got to pay for it upfront.

810
00:50:26.450 --> 00:50:31.370
Uh,
you,
the Hepatitis C drug situation is a good example of that.

811
00:50:32.150 --> 00:50:34.790
Um,
if you actually cost out over a lifetime,

812
00:50:34.791 --> 00:50:36.650
it's a pretty good deal at the price.

813
00:50:36.651 --> 00:50:41.270
But the fact that people have to pay it upfront at one time meant that whole

814
00:50:41.271 --> 00:50:44.840
state medicaid programs would have gone bankrupt,
hadn't been,

815
00:50:44.870 --> 00:50:48.830
what is the price right now?
It's coming down,
but you know,

816
00:50:48.860 --> 00:50:53.860
it's in the tens of thousands per treatment as you know,

817
00:50:53.991 --> 00:50:58.580
for some rare diseases and can't types of cancers,
three or 400,000 bucks a year.

818
00:50:58.940 --> 00:51:02.150
I think.
I think that's what you're getting at.
So I don't think we have it right.

819
00:51:02.151 --> 00:51:04.760
And I think there's a,
it's another area where we need,

820
00:51:05.660 --> 00:51:09.260
once we get adults in the room in Washington and the universities,

821
00:51:09.261 --> 00:51:13.130
we need to have those,
continue those discussions.

822
00:51:13.900 --> 00:51:17.760
<v 5>I have customers in the health care trying to solve some of these problems and</v>

823
00:51:17.860 --> 00:51:22.480
as we work together with them and they have an interest in partnering with

824
00:51:22.481 --> 00:51:25.420
Google,
how do we decide whether it's,
uh,
you know,

825
00:51:25.450 --> 00:51:27.910
the right conversation to have with you guys?
Or are they,

826
00:51:27.911 --> 00:51:32.110
what are the criteria or how do we determine that process?
Ah,

827
00:51:33.630 --> 00:51:35.520
<v 4>so,
um,
it's a little</v>

828
00:51:37.380 --> 00:51:41.960
hard to come into this environment.
I hate to tell you were from a structured,

829
00:51:41.961 --> 00:51:46.460
it's really stark for me because I'll step into my office in North Carolina and

830
00:51:46.461 --> 00:51:48.570
Duke University.
It's very hierarchical.

831
00:51:48.571 --> 00:51:52.520
I've got this big staff that makes everything work and um,

832
00:51:52.710 --> 00:51:56.280
people know who I am.
And there's a funnel of information that comes out to me.

833
00:51:56.281 --> 00:51:57.660
And my job here,

834
00:51:57.661 --> 00:52:01.860
I arrived out at verily I'm in a little glass cage with another guy's dean of a

835
00:52:01.861 --> 00:52:05.430
medical school,
was sharing an office and um,

836
00:52:06.200 --> 00:52:09.010
I'm just another one of the people.
So,
um,

837
00:52:09.090 --> 00:52:11.080
and then you say how the decisions get made.

838
00:52:11.081 --> 00:52:15.120
And this is a place where I think the software culture has a lot to offer in

839
00:52:15.121 --> 00:52:19.320
terms of the flatness,
but it won't work to be,
it's not,

840
00:52:19.321 --> 00:52:23.610
it won't work like a software culture to make it work.
And I,

841
00:52:23.611 --> 00:52:24.720
based on everything I've seen,

842
00:52:24.721 --> 00:52:29.721
I think this is a major topic that the enterprise alphabet needs to spend some

843
00:52:30.931 --> 00:52:33.330
time on.
It's good to have some internal competition.

844
00:52:33.331 --> 00:52:37.890
Harvard is probably one of the best examples of place where you can go a hundred

845
00:52:37.890 --> 00:52:41.920
yards away and see people competing on the same thing.
But,
uh,

846
00:52:41.970 --> 00:52:46.650
to the outside world,
if you don't offer some cohesive approach,

847
00:52:46.710 --> 00:52:50.220
um,
it's hard enough for them just to get by right now as you know.

848
00:52:50.730 --> 00:52:53.670
So I think we need to do some work to figure this out.

849
00:52:53.671 --> 00:52:55.800
And I hope I can be helpful with that.

850
00:52:56.650 --> 00:52:58.280
<v 1>Well,
this has been wonderful.
Uh,</v>

851
00:52:58.330 --> 00:53:02.650
thank you so much for coming down and listening to rob and great questions.
And,

852
00:53:03.040 --> 00:53:06.880
uh,
please come up and visit us on the 12th floor and barely here in Cambridge,

853
00:53:06.910 --> 00:53:10.180
a if you want to continue the conversation and,
uh,

854
00:53:10.240 --> 00:53:13.650
this was a great experience for us and,
uh,

855
00:53:13.930 --> 00:53:15.790
we hope to replicate it in the future.
Thank you.

856
00:53:16.340 --> 00:53:17.670
<v 5>[inaudible].</v>

