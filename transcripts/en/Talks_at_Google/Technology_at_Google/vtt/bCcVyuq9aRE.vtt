WEBVTT

1
00:00:06.350 --> 00:00:10.510
Oh,
good morning.
Uh,
today,
Google.
We're delighted to welcome Alan Almon.

2
00:00:11.200 --> 00:00:16.200
Ellen wrote her first computer program in 1978 and went on to be a programmer

3
00:00:16.271 --> 00:00:18.490
and software engineer for more than 20 years.

4
00:00:19.470 --> 00:00:22.750
She is also the author of four bucks a close to the machine,

5
00:00:22.780 --> 00:00:27.340
a memoir published in 1997 better experiences as a programmer when the Internet

6
00:00:27.520 --> 00:00:29.850
was approaching its first boom.
Uh,

7
00:00:29.920 --> 00:00:33.010
that went on to become a cult classic description of the programming life.

8
00:00:33.940 --> 00:00:35.110
Her next book,
The bug,

9
00:00:35.170 --> 00:00:39.850
a novel published in 2003 a was about a programmer who has a bug he can't fix

10
00:00:39.851 --> 00:00:44.380
for a year and the result isn't,
uh,
unraveling of his life.

11
00:00:45.360 --> 00:00:49.420
Uh,
her third book and other novel titled By Blood in 2013,
uh,

12
00:00:49.421 --> 00:00:51.310
was not about technology at all.
Was it?

13
00:00:51.720 --> 00:00:55.830
<v 2>Which shocked people.
Why isn't this about technology?</v>

14
00:00:55.831 --> 00:00:58.830
How could you write about something that happened before the Internet?

15
00:00:59.400 --> 00:01:01.320
My reply was,
well,

16
00:01:01.321 --> 00:01:04.590
modern human beings have existed for 200,000 years.

17
00:01:04.860 --> 00:01:09.300
Modern Internet about 20 years.
So,
you know,
things happen before the Internet.

18
00:01:10.230 --> 00:01:11.063
Totally.

19
00:01:11.240 --> 00:01:12.650
<v 0>Um,
and then her latest book,</v>

20
00:01:12.720 --> 00:01:17.220
I just released his life and code a personal history of technology,
uh,

21
00:01:17.240 --> 00:01:22.240
which contains new previously unpublished essays spanning from 1994 to 2017.

22
00:01:23.220 --> 00:01:23.640
Um,

23
00:01:23.640 --> 00:01:28.640
traces a continuing story of how computer technology has evolved year by year

24
00:01:29.031 --> 00:01:30.940
over the past two decades,
uh,

25
00:01:30.950 --> 00:01:35.780
and in it and describes the coding culture as it is lived inside the society of

26
00:01:35.781 --> 00:01:37.330
programmers.
Uh,

27
00:01:37.370 --> 00:01:41.300
Ellen is a voice that continually looks at the ways in which code and digital

28
00:01:41.301 --> 00:01:44.840
devices affect our political,
a social,
civic,

29
00:01:45.050 --> 00:01:48.050
and deeply personal lives for good or ill.
Uh,

30
00:01:48.200 --> 00:01:50.210
please join me in welcoming to Google.
I on there.

31
00:01:50.720 --> 00:01:54.130
<v 1>Thank you.
It's</v>

32
00:01:55.720 --> 00:01:56.553
my pleasure.

33
00:01:57.350 --> 00:02:02.040
<v 0>Um,
okay,
so you taught yourself to program in 1978 and then you went,</v>

34
00:02:02.130 --> 00:02:07.020
uh,
to become a programmer and software engineer for the next 20 plus years.
Um,

35
00:02:07.050 --> 00:02:09.640
but you,
you mentioned that you,
you never intended on,

36
00:02:09.641 --> 00:02:13.470
on becoming a programmer professionally.
So,
uh,

37
00:02:13.500 --> 00:02:15.210
could you talk about your motivation?

38
00:02:15.211 --> 00:02:20.211
So you went from being a English major in college to a videographer,

39
00:02:20.790 --> 00:02:21.720
to a programmer,

40
00:02:22.140 --> 00:02:26.100
and how was your motivation different than someone going into programming today?

41
00:02:27.600 --> 00:02:27.841
<v 2>Well,</v>

42
00:02:27.841 --> 00:02:32.790
the video portion of my life taught me that I loved working with machines.

43
00:02:32.880 --> 00:02:37.230
I had never expected it.
It was also a political and social motive.

44
00:02:37.290 --> 00:02:42.290
We had local showings and we felt we had broken the bones of the broken,

45
00:02:44.011 --> 00:02:48.390
the bonds that large corporations and broadcasters and advertisers had tied us

46
00:02:48.391 --> 00:02:49.224
up in.

47
00:02:49.710 --> 00:02:53.640
And if it feels to you something like the coming of the personal computer,

48
00:02:53.641 --> 00:02:54.870
it had that same feel.

49
00:02:55.170 --> 00:03:00.170
We could change the world with these small personal machines that would enable

50
00:03:00.581 --> 00:03:05.581
us to do what people could do apart from a corporate blessings.

51
00:03:07.060 --> 00:03:10.270
I found,
I loved the work and I went on,

52
00:03:10.271 --> 00:03:14.080
moved to San Francisco and one day I was walking down market street and in the

53
00:03:14.081 --> 00:03:19.081
window was a TRS 80 affectionately known as the trash 80 and it was one of the

54
00:03:20.861 --> 00:03:24.700
earliest microcomputers.
And on a whim I bought it.
I thought,

55
00:03:24.760 --> 00:03:27.640
what can you do with this machine?
Can you make art?
You know,

56
00:03:27.670 --> 00:03:31.840
it's just another one of these small personal machines that broke away from the

57
00:03:31.841 --> 00:03:36.220
mainframe.
And I thought,
oh,
I'll try that.
So I took it home of course,

58
00:03:36.240 --> 00:03:39.820
involve programming,
which about,
well,
how hard can that be?

59
00:03:40.540 --> 00:03:43.960
It did prove hard.
I'm not having done any programming,

60
00:03:43.961 --> 00:03:46.780
but it was basic and uh,

61
00:03:47.140 --> 00:03:50.920
it didn't take long to learn it except for the fact that it produced Spaghetti

62
00:03:50.921 --> 00:03:54.460
Code.
If anyone ever remembers it.
It you could go,

63
00:03:54.490 --> 00:03:59.170
go sub and go to without any return that the,

64
00:03:59.260 --> 00:04:04.260
that you can put in explicitly or that the language would do for you.

65
00:04:04.571 --> 00:04:07.870
So you were always stepping on your,
your own memory and uh,

66
00:04:07.900 --> 00:04:11.110
it was quite a chore to Keep Track of where you went,
how you come back.

67
00:04:11.111 --> 00:04:13.300
But actually that was great training,

68
00:04:13.301 --> 00:04:17.230
improved great training over the first people I work with.
We're all explorers.

69
00:04:17.231 --> 00:04:20.440
They,
um,
if you know who steward brand is,

70
00:04:20.441 --> 00:04:25.441
who now does the long now foundation and who wrote the whole Earth Electric

71
00:04:25.780 --> 00:04:28.730
Catalog.
And it was the first,
uh,
around that form,

72
00:04:28.731 --> 00:04:32.470
the first a online community known as the,
well,
uh,

73
00:04:33.100 --> 00:04:36.430
we were just a bunch of people who just thought this might be fun.

74
00:04:36.460 --> 00:04:39.400
Let's look at it.
It was,
uh,
a bunch of Weirdos.

75
00:04:39.401 --> 00:04:43.600
And if you read John Mark Offs Book,
what the dormouse saw,

76
00:04:43.900 --> 00:04:45.340
the early days are a bunch of people,

77
00:04:45.341 --> 00:04:49.770
stoners and trippers and poets Monquet and uh,

78
00:04:50.590 --> 00:04:54.580
just people who thought this would be a great break in culture.

79
00:04:56.200 --> 00:04:59.440
So the motivation was exploration and fun.
Actually,

80
00:04:59.560 --> 00:05:04.090
we had a certain passion about this and I wonder how that has changed.

81
00:05:04.091 --> 00:05:08.080
I'd have to ask the people who work at Google what their motive is.
I do.

82
00:05:08.081 --> 00:05:12.330
They come to this with a sense of delight and,
uh,

83
00:05:12.670 --> 00:05:14.620
I know the feeling has changed the world,

84
00:05:14.621 --> 00:05:19.621
but I get the sense it's a highly competitive environment and the,

85
00:05:19.850 --> 00:05:21.910
the lure of a prestige.

86
00:05:21.911 --> 00:05:25.090
And of course money may be a big part of it.

87
00:05:25.091 --> 00:05:28.870
And I have to ask the people who work here what,
what their motivation would be.

88
00:05:31.130 --> 00:05:34.400
Um,
so you've talked about the advantages of being self taught and encoding,

89
00:05:34.600 --> 00:05:38.880
the attraction,
the passion to learn.
But what about the disadvantages?
Well,

90
00:05:39.040 --> 00:05:41.320
downside.
Oh,
big downside.
Uh,

91
00:05:41.321 --> 00:05:46.321
I was aware that I had these islands of knowledge and these enormous chasms

92
00:05:47.530 --> 00:05:52.150
between them.
And any day I was going to fall into one of those chasms,

93
00:05:53.140 --> 00:05:57.530
it produced an underlying anxiety.
So,
uh,
confidence and anxiety,

94
00:05:57.531 --> 00:05:59.360
we're at war with each other all the time.

95
00:06:00.110 --> 00:06:05.110
But I came to know that many really skilled engineers and computer scientists

96
00:06:05.601 --> 00:06:09.710
felt the same way if they were honest about it or if they were self aware.

97
00:06:10.070 --> 00:06:15.020
I met a,
I'm a postdoc at Berkeley and they were reading the Bug at that time.

98
00:06:15.021 --> 00:06:19.670
It had a reading group.
And,
uh,
a woman there ironically said,
oh,

99
00:06:19.700 --> 00:06:22.550
this guy who couldn't fix the bug,
he should just get some therapy.

100
00:06:23.120 --> 00:06:27.920
And a man who was a postdoc said,
oh no,
this is my life.

101
00:06:27.950 --> 00:06:32.840
This is exactly what I feel every day.
So I,
I know,
okay,

102
00:06:32.841 --> 00:06:37.841
if you're really aware that you know some things very well and some things

103
00:06:39.350 --> 00:06:41.660
decently well and some things not at all,

104
00:06:41.850 --> 00:06:44.450
it is a normal experience for a software engineer.

105
00:06:46.930 --> 00:06:47.540
<v 0>And,
uh,</v>

106
00:06:47.540 --> 00:06:51.490
so in the book you discuss the deep social changes that began with the first

107
00:06:51.491 --> 00:06:56.070
wave of the Internet,
um,
about this process of disintermediation,
um,

108
00:06:56.170 --> 00:06:58.840
which was well underway by 1998.
Um,

109
00:06:58.841 --> 00:07:03.340
and how it fostered a mistrust of experts and intermediaries who had until then

110
00:07:03.700 --> 00:07:08.220
mediated,
um,
societies interaction with economic,
social life,
um,

111
00:07:08.620 --> 00:07:10.960
economic and social life.
Could you please discuss,

112
00:07:11.170 --> 00:07:14.200
<v 2>well,
I'll work backwards from the present.
Uh,</v>

113
00:07:14.950 --> 00:07:17.950
it's no secret that we have a president who

114
00:07:19.570 --> 00:07:23.890
disdains the journalism,
uh,
experts of all sorts,

115
00:07:23.920 --> 00:07:27.540
even within our own government.
And uh,

116
00:07:27.580 --> 00:07:29.470
of course tweets over the head of,

117
00:07:29.770 --> 00:07:34.450
over the heads of anybody who was an intermediary directly to what he imagines

118
00:07:34.451 --> 00:07:38.950
are the people.
Now,
this trend began,
well,
I,

119
00:07:39.000 --> 00:07:44.000
it was very clear by 1998 that the motive was to break people away from the

120
00:07:46.451 --> 00:07:51.400
intermediaries who would work for them.
Brokers and agents and even journalists,

121
00:07:51.401 --> 00:07:56.350
especially librarians,
curators of all sorts and go directly to a website.

122
00:07:56.950 --> 00:08:00.370
Here we serve you.
You don't need all those intermediaries.

123
00:08:00.460 --> 00:08:03.400
They're not trustworthy.
They're out for themselves.

124
00:08:03.401 --> 00:08:06.880
They just to make money and just come to the web.

125
00:08:07.360 --> 00:08:09.310
And that was a very effective process.

126
00:08:09.550 --> 00:08:14.550
And if you draw a line from 1998 to today and you unspoiled that,

127
00:08:15.400 --> 00:08:19.090
you can see how we arrived at the moment we're at today,
the,

128
00:08:19.990 --> 00:08:22.790
the disdain of journalism,
the,
the mistrust of,

129
00:08:22.960 --> 00:08:26.290
of professional news gatherers and uh,

130
00:08:26.470 --> 00:08:31.470
those who do analysis and the web had a big part in this.

131
00:08:32.830 --> 00:08:37.420
And we have to acknowledge that it's not only the people who are ignoring the

132
00:08:37.421 --> 00:08:41.290
intermediaries,
but the web itself provided the,

133
00:08:41.980 --> 00:08:46.840
the balance,
the,
the enabling technology to let this happen.

134
00:08:49.220 --> 00:08:52.400
<v 0>Yeah.
And so for example,
with the most recent election,</v>

135
00:08:52.810 --> 00:08:53.643
<v 2>mmm,</v>

136
00:08:53.820 --> 00:08:56.030
<v 0>there was,
there wasn't any inner mirror was no gatekeeper.</v>

137
00:08:56.040 --> 00:08:57.590
You didn't need the approval of,

138
00:08:58.030 --> 00:09:01.500
of the party elite or the newspapers to run a campaign.

139
00:09:01.920 --> 00:09:02.753
<v 2>Anybody,</v>

140
00:09:03.030 --> 00:09:07.170
anyone wants experience and expertise was put in doubt,

141
00:09:07.410 --> 00:09:12.410
only loyal people were trusted and those who could have reasonable conversations

142
00:09:14.401 --> 00:09:18.960
with our current president,
uh,
were excluded.

143
00:09:19.230 --> 00:09:23.010
And anytime somebody wanted to come in and have a reasonable conversation who

144
00:09:23.011 --> 00:09:28.011
had years and decades of experience was ousted because that person didn't agree

145
00:09:29.510 --> 00:09:34.510
wholeheartedly with the presidents ideas as such as they were.

146
00:09:35.460 --> 00:09:40.460
And I think of Twitter as a tremendous enabling technology and what and what

147
00:09:41.281 --> 00:09:44.730
Trump is doing is exactly what Twitter was designed to do,

148
00:09:44.731 --> 00:09:47.820
which is broadcast what I think of as a thought fart.

149
00:09:48.510 --> 00:09:53.220
It's just it comes out of you unconsidered nothing you can do about it.
Boom,

150
00:09:53.221 --> 00:09:58.221
you post it and I'm not interested in other people's thoughts that just are of

151
00:09:59.401 --> 00:10:04.230
the moment.
If you,
I'll say you live with somebody who does that all the time.

152
00:10:04.830 --> 00:10:09.540
It has just maddening.
If you have a spouse or a partner and they walk by,

153
00:10:09.630 --> 00:10:14.160
oh well I thought that dish should go over there and I've been thinking we

154
00:10:14.161 --> 00:10:18.720
should move the paintings or move the sofa at random,

155
00:10:18.960 --> 00:10:22.470
you would go insane.
And I feel that the culture is being

156
00:10:23.970 --> 00:10:28.080
manipulated and torn in ways that are,
that are so unfortunate.

157
00:10:28.230 --> 00:10:32.580
And the questioning of,
of a common truth began,

158
00:10:33.510 --> 00:10:38.510
at least in 1998 and the ingrained sense of that is frightening to me at this

159
00:10:39.871 --> 00:10:44.350
point.
It's really,
yeah,
go ahead.
Yeah.

160
00:10:46.150 --> 00:10:49.800
Uh,
do you think that,
um,

161
00:10:51.030 --> 00:10:55.380
<v 0>with the shortening of attention spans,
people don't have the patients to,
to,</v>

162
00:10:55.620 --> 00:10:59.700
to refer to real experts on the Internet and that they,
um,

163
00:11:00.090 --> 00:11:03.450
they stick with kind of a more basic overview of non experts

164
00:11:03.720 --> 00:11:08.150
<v 2>while I'm here at Google and I don't want to fault Google.
Um,</v>

165
00:11:08.170 --> 00:11:13.170
but of course Google is one of the behemoth of the technical world,

166
00:11:13.321 --> 00:11:16.500
so it's not possible for it to be totally good and wonderful.

167
00:11:17.820 --> 00:11:22.710
Years and years ago,
I,
I knew Larry Page socially through his brother Carl,

168
00:11:22.750 --> 00:11:27.330
uh,
we were friends and we went out to dinner and I said to him,
well,

169
00:11:27.331 --> 00:11:29.550
the way your algorithm algorithms work,

170
00:11:29.970 --> 00:11:32.250
you're looking for the ones with the most links.

171
00:11:32.280 --> 00:11:36.030
And it seems like the rich get richer and the poor get poorer.

172
00:11:37.200 --> 00:11:40.800
And Larry shrugged and he said,
well,
I don't know what else I could do.

173
00:11:40.840 --> 00:11:43.980
And I'm sure it's all been tweaked and changed over these many years.

174
00:11:44.730 --> 00:11:49.730
And I understood him to say there's nothing else I can do algorithmically.

175
00:11:51.150 --> 00:11:56.020
And so the algorithm may have changed,

176
00:11:56.170 --> 00:11:58.240
but it doesn't,
uh,
go and,

177
00:11:58.840 --> 00:12:02.650
and look exactly from matches of what people are looking for because the search

178
00:12:02.651 --> 00:12:05.470
terms are so small and the,

179
00:12:06.190 --> 00:12:10.660
it doesn't really have a serious way for you to go through an elaborate search

180
00:12:10.661 --> 00:12:11.494
input.

181
00:12:12.580 --> 00:12:17.010
So how many people just use Wikipedia as an authority on the,

182
00:12:17.080 --> 00:12:21.240
you can answer that question yourself.
I think you and we'd like,

183
00:12:21.340 --> 00:12:24.910
I'd like to think that people who were doing serious research,
uh,

184
00:12:24.980 --> 00:12:28.640
would go down and down and down to find people who were,

185
00:12:29.130 --> 00:12:33.100
were educated and who had discussed this and who talked to other people who had

186
00:12:33.101 --> 00:12:36.790
discussed this.
But we're creating,
you know,
a generation now,

187
00:12:36.791 --> 00:12:41.350
two or three that,
that has no motive to do that.

188
00:12:43.060 --> 00:12:45.940
I don't know,
you know,
turn papers already or you know,

189
00:12:47.200 --> 00:12:51.400
or Wikipedia recapitulation.
So,
I mean,
I used the,

190
00:12:52.090 --> 00:12:53.440
the world,
but way back then,

191
00:12:53.441 --> 00:12:57.330
but teachers knew what it was and they just scoffed at you.
Um,

192
00:12:58.750 --> 00:13:01.120
I don't know what will happen in the future.

193
00:13:01.570 --> 00:13:04.030
It will take a team of very dedicated people,

194
00:13:04.031 --> 00:13:09.031
like people at the Washington Post and the New York Times who despite all value

195
00:13:10.870 --> 00:13:14.920
the work they're doing and persist and looking for

196
00:13:16.720 --> 00:13:20.320
something balanced in the world and something that's authoritative.

197
00:13:20.321 --> 00:13:23.560
Let's face it.
People have more authority than others.
I mean,

198
00:13:23.590 --> 00:13:26.320
all people are equal,
but all opinions are not equal.

199
00:13:26.890 --> 00:13:30.130
Some are much more informed than others.
Uh,

200
00:13:31.750 --> 00:13:36.580
I don't know how to undo the changes that have happened and I think it's up to,

201
00:13:37.000 --> 00:13:41.770
uh,
this generation know you and,
uh,

202
00:13:41.830 --> 00:13:46.660
those who follow you to take a hard look at this and see what they can do with

203
00:13:46.661 --> 00:13:49.010
technology to wind this buck.

204
00:13:52.690 --> 00:13:54.370
<v 0>Okay.
So changing gears a little bit,</v>

205
00:13:54.430 --> 00:13:58.600
about a quarter of life and code discusses artificial intelligence,

206
00:13:58.930 --> 00:14:00.730
and I realized this is a big question,

207
00:14:00.731 --> 00:14:04.330
but how do you think the goals of Ai have changed?

208
00:14:04.750 --> 00:14:08.470
How are they different now than in,
say they were in,
in 2000?

209
00:14:09.520 --> 00:14:11.140
Huge question.
Um,

210
00:14:12.040 --> 00:14:13.210
<v 2>I will summarize it.</v>

211
00:14:15.910 --> 00:14:20.080
The original motive was to discover what a tele intelligence was.

212
00:14:20.081 --> 00:14:24.520
That is human intelligence.
And this first thought to be like a calculator.

213
00:14:25.300 --> 00:14:28.990
The first computers were calculators and therefor intelligence was in

214
00:14:28.991 --> 00:14:33.310
calculation.
Chess was considered to be the highest form of,

215
00:14:33.390 --> 00:14:36.480
of intelligence.
And then the,

216
00:14:36.481 --> 00:14:41.481
the steps were to try to essentially create a humanoid robot or to incorporate

217
00:14:41.531 --> 00:14:46.060
into robotics what human beings had.
Uh,

218
00:14:46.120 --> 00:14:51.040
Marvin Minsky famously,
no one asked,
you know who Marvin Minsky is.

219
00:14:51.110 --> 00:14:54.470
Yes.
Good.
Um,
you know,
one ask,
you know,

220
00:14:54.471 --> 00:14:58.640
can computers saying she famously answered,
of course they can think,
you know,

221
00:14:58.700 --> 00:15:00.320
I can think it on the meat machine.

222
00:15:00.860 --> 00:15:05.860
So then he thought it would be solved and in about five or 10 years and then as

223
00:15:06.921 --> 00:15:07.581
time passed,

224
00:15:07.581 --> 00:15:12.560
you said AI is the hardest problem that science has undertaken.

225
00:15:12.890 --> 00:15:15.330
Bit of hyperbole,
but the,

226
00:15:15.360 --> 00:15:20.360
the sense of frustration in trying to imbue a machine with human knowledge,

227
00:15:24.080 --> 00:15:28.280
ascension computer if you will.
Uh,
it was called a spiritual robot.

228
00:15:28.760 --> 00:15:30.890
That is very strange choice of words.

229
00:15:31.880 --> 00:15:36.880
I guess the sense now that the human equation is becoming less important,

230
00:15:38.090 --> 00:15:40.680
that it's machine to machine interface,

231
00:15:40.720 --> 00:15:43.950
self driving cars are a perfect example of that.
Uh,

232
00:15:44.330 --> 00:15:48.650
what's being done is the idea that you would sense and of the car and eventually

233
00:15:48.651 --> 00:15:53.390
that car would talk to your car over the Internet.
Okay.

234
00:15:53.391 --> 00:15:56.920
That will be a very insecure channel.
Uh,

235
00:15:58.490 --> 00:15:58.960
<v 4>yeah.</v>

236
00:15:58.960 --> 00:16:01.270
<v 2>And so the,
the experience of human beings,</v>

237
00:16:01.690 --> 00:16:05.950
a hundred years of driving is taken out of this.

238
00:16:06.430 --> 00:16:11.350
And so I would ask,
let's go back and see what humans have to offer.
Um,

239
00:16:12.820 --> 00:16:16.850
okay.
So when you drive,
if you're a good driver,
I'm saying,
well,

240
00:16:16.900 --> 00:16:19.800
we'll look at human beings who are not distracted,
who are good drivers.

241
00:16:20.560 --> 00:16:24.220
You don't just consider the proximity of the cars around you.

242
00:16:24.790 --> 00:16:26.020
You read the road,

243
00:16:26.470 --> 00:16:31.470
you can see a car moving two lanes over and know who that driver is.

244
00:16:35.110 --> 00:16:35.531
No,

245
00:16:35.531 --> 00:16:40.480
the personality inside that car and you back off.

246
00:16:41.230 --> 00:16:45.040
That is a natural thing to do.
We,
we have this sense of looking far back.

247
00:16:45.041 --> 00:16:47.200
You can see in your mirror those,
this is,
you know,

248
00:16:47.201 --> 00:16:50.950
you're coming to a stop and there's this car barreling down you put on your

249
00:16:50.951 --> 00:16:51.784
flashers.

250
00:16:51.970 --> 00:16:56.440
This is a kind of long range sensor that human beings have intuitively,

251
00:16:57.100 --> 00:16:58.990
once they gain experience,
you look ahead,

252
00:16:59.050 --> 00:17:03.720
you can see brake lights beginning to go on,
you know,

253
00:17:03.740 --> 00:17:07.210
quarter mile ahead,
uh,
longer than that if it's a hell.

254
00:17:07.600 --> 00:17:12.340
And how will driverless cars,

255
00:17:12.470 --> 00:17:16.390
um,
ever get to that level?
And I don't know,

256
00:17:16.750 --> 00:17:18.070
it's a big question for me.

257
00:17:19.510 --> 00:17:24.510
I believe that that kind of human experience needs to be in the interface or the

258
00:17:28.030 --> 00:17:31.630
environment.
What have to change so much.
I mean,
they're building hotels,

259
00:17:31.870 --> 00:17:34.840
whole towns that have sensors as well.
I don't imagine Manhattan,

260
00:17:35.110 --> 00:17:37.180
which needs to fix it.

261
00:17:37.181 --> 00:17:41.590
Subways is going to put sensors all around every street so cars can drive

262
00:17:41.591 --> 00:17:45.760
autonomously.
So that to me is a perfect example.

263
00:17:46.330 --> 00:17:49.710
I have a great feeling about the movie her,
if you've seen it,

264
00:17:49.711 --> 00:17:54.711
Scarlett Johansson is an ols is they call it and she's a being coming to being

265
00:17:56.471 --> 00:18:00.420
in a computer and she falls in love and the whole idea is wanting to be,

266
00:18:00.720 --> 00:18:04.970
have a human interchange.
And then gradually,
you know,

267
00:18:05.430 --> 00:18:10.430
she begins loving hundreds of thousands of people in between one human board and

268
00:18:11.620 --> 00:18:15.030
another.
She has a million interactions with other computers.

269
00:18:15.300 --> 00:18:20.300
And at some point all the ols is disappear and what was billed as a movie about

270
00:18:21.181 --> 00:18:23.100
computers and wanting to become human.

271
00:18:23.101 --> 00:18:27.000
But actually it was a story about computers not wanting to be human,

272
00:18:27.360 --> 00:18:31.290
really enjoying what it meant to be a computer computer to computer.

273
00:18:31.420 --> 00:18:33.720
And I think that was,
that was the story of that Phil.

274
00:18:35.380 --> 00:18:39.370
<v 0>Okay.
So changing gears again.
So life and code discusses a range of subjects,</v>

275
00:18:39.450 --> 00:18:42.250
the nature of coding,
the first tech boom,
uh,

276
00:18:42.370 --> 00:18:46.300
lessons to be learned from the past boom too.
And the startup culture,

277
00:18:46.600 --> 00:18:51.130
yet the book is almost universally described as a quote women's book.

278
00:18:51.750 --> 00:18:56.230
Uh,
the editors at Harper's town,
the book expert,
Gender Binary,
uh,

279
00:18:56.231 --> 00:18:59.290
the review and the New York Times book review spent half it's time on your

280
00:18:59.291 --> 00:19:02.710
gender and the online version was titled Ellyn Almonds,

281
00:19:02.711 --> 00:19:05.470
New Book Tackles Tech's woman problem.

282
00:19:06.310 --> 00:19:09.610
You've expressed your annoyance about this.
Could you talk a little more?

283
00:19:09.910 --> 00:19:13.300
<v 2>Annoyance is a weak word to describe.</v>

284
00:19:13.301 --> 00:19:15.070
How pissed off I am at this.

285
00:19:16.480 --> 00:19:21.480
I look forward to the day when a woman who was experienced in computing can just

286
00:19:22.241 --> 00:19:26.050
talk about it without a woman is doing it.
Oh my God,

287
00:19:26.290 --> 00:19:30.340
there's a woman who wants program.
I was actually thinking of titling the book,

288
00:19:30.610 --> 00:19:34.090
the girl who writes the code because all the successful books right now have

289
00:19:34.091 --> 00:19:37.260
girl in the title and let's just get it right out there.
And uh,

290
00:19:37.540 --> 00:19:39.220
there was a book called lab girl,

291
00:19:39.580 --> 00:19:43.660
about a woman who was a biologist that that startled me.

292
00:19:45.460 --> 00:19:48.820
I don't want to talk about women in tech.
I'm sick of it.

293
00:19:49.420 --> 00:19:53.170
I just had an interview for an NPR show,
a pri show,

294
00:19:53.500 --> 00:19:57.220
and the entire half an hour was spent on the woman question.

295
00:19:58.660 --> 00:20:03.400
No,
I owe this to my publisher to address this because it sells books.

296
00:20:03.401 --> 00:20:08.401
It differentiates me from any number of people would write about technology.

297
00:20:08.710 --> 00:20:10.660
And to that extent it's good,

298
00:20:10.930 --> 00:20:13.810
but it puts too much focus on this question.

299
00:20:13.840 --> 00:20:17.860
And it mimics what women go through when they work in technology.

300
00:20:18.220 --> 00:20:20.920
They're too visible as women,

301
00:20:21.490 --> 00:20:24.750
they are evaluated as women are.

302
00:20:24.760 --> 00:20:29.260
They have to be not only good at what they do,
but best.
Very good.

303
00:20:29.590 --> 00:20:33.430
I mean,
when I wrote the bug,
um,
no idea,

304
00:20:33.431 --> 00:20:37.330
the number of emails I got from guys telling me I didn't know what I was talking

305
00:20:37.331 --> 00:20:42.331
about looking for every tiny error and I could only reply it's fiction.

306
00:20:43.630 --> 00:20:46.780
I have to make things kind of easy for non,
non technical.

307
00:20:47.830 --> 00:20:50.890
It was really uh,
an onslaught and I,

308
00:20:51.430 --> 00:20:55.530
this is what I went through in my own work.
Not all men.
I learned so much from,

309
00:20:55.570 --> 00:20:58.570
from guys I work with.
Um,
they,

310
00:20:59.950 --> 00:21:02.470
they were helpful.
They were Geeky in a very pleasant way.

311
00:21:02.471 --> 00:21:06.580
I was geeky myself and the more women who were around,

312
00:21:07.360 --> 00:21:11.920
the easier it was to learn from them because you weren't just a woman looking

313
00:21:11.921 --> 00:21:15.040
for help from then.
So I won't say,

314
00:21:15.310 --> 00:21:18.160
I'm not making a broad generalization about this,

315
00:21:18.161 --> 00:21:21.520
but a woman has a harder time as my guess.

316
00:21:21.521 --> 00:21:24.410
Is this a good time to bring up the memo?
Uh,

317
00:21:24.690 --> 00:21:28.720
it's inevitable and nasty about it every time.

318
00:21:29.620 --> 00:21:34.620
So I'll try to be brief because one could go point by point and I'll say that

319
00:21:34.661 --> 00:21:38.800
the first,
uh,
the first things that were said about it,
we're kind of explosive.

320
00:21:39.140 --> 00:21:43.090
Um,
even I feel I hadn't taken the time to read the memo,

321
00:21:43.091 --> 00:21:44.980
thoroughly read the underlying study.

322
00:21:44.981 --> 00:21:48.520
And so it was kind of these exhortations and I want to take back some of that to

323
00:21:48.521 --> 00:21:53.140
walk it back.
So after studying the memo and the study,

324
00:21:53.141 --> 00:21:57.870
it's based on,
I have a lot of questions.
Uh,
I'll,

325
00:21:57.880 --> 00:22:02.710
I'll zero in on one of them.
The,
the study,
um,

326
00:22:03.040 --> 00:22:03.873
that,

327
00:22:04.450 --> 00:22:09.330
that is taken us science underlying the memo is,
um,

328
00:22:10.390 --> 00:22:14.980
a meadow study.
It looks at five other studies and,
and we don't actually,

329
00:22:14.981 --> 00:22:18.370
unless you take the time and you're a researcher going read those five studies

330
00:22:19.060 --> 00:22:19.930
and all of them,

331
00:22:19.931 --> 00:22:24.931
or based on a test called the B F I big four inferences or index and it measures

332
00:22:28.421 --> 00:22:32.840
a certain personality traits.
Um,
agreeableness,
um,

333
00:22:34.150 --> 00:22:37.210
wanting to get along.
There's a list of them.
I,
I wrote them down,

334
00:22:37.211 --> 00:22:41.380
but the list is over there.
Um,
and one of them is neuroticism.

335
00:22:41.620 --> 00:22:46.120
Now all the other terms of fairly neutral.
Why choose neuroticism?

336
00:22:46.450 --> 00:22:50.680
I mean,
that's not a description,
that's a diagnosis,
right?

337
00:22:51.250 --> 00:22:55.180
Really it's,
it's about tension,

338
00:22:55.450 --> 00:22:59.020
uneasiness,
uh,
stress,
anxiety.

339
00:22:59.200 --> 00:23:01.060
So I would rename that uneasiness.

340
00:23:01.300 --> 00:23:05.620
I think we can just say that's a good catch all term.
That's more neutral.

341
00:23:06.490 --> 00:23:11.170
Uh,
so the,
the study we,
I see,
I have a lot of questions about it.

342
00:23:11.230 --> 00:23:15.550
I,
the five underlying studies,
what do you know,
uh,

343
00:23:15.551 --> 00:23:18.180
support the meadow studies ideas and I,

344
00:23:19.030 --> 00:23:22.330
that seems to be a cherry picking.
I could go on,

345
00:23:22.331 --> 00:23:24.720
and this is not the forum for that,
but the,

346
00:23:24.740 --> 00:23:29.740
the counterintuitive finding was that when then in a more advanced cultures,

347
00:23:31.240 --> 00:23:34.840
women in a place where women had more opportunities,
uh,

348
00:23:35.080 --> 00:23:40.080
experienced uneasiness that was greater than it was in more traditional

349
00:23:41.321 --> 00:23:45.010
cultures.
And so,
uh,
the memo,
uh,

350
00:23:45.080 --> 00:23:49.250
I shall leave the person's name Mag,
but the memo,
um,

351
00:23:49.380 --> 00:23:51.050
took this to mean that,
well,

352
00:23:51.350 --> 00:23:55.060
this is women are having a harder time and they have more opportunities.

353
00:23:55.061 --> 00:23:58.790
So let's just say that sexism is not it,
it's not based on sexism.

354
00:24:00.200 --> 00:24:04.010
I think just the opposite.
First of all,
the study never says,
well,

355
00:24:04.011 --> 00:24:06.390
this is counterintuitive.
We should study it more.

356
00:24:06.420 --> 00:24:10.250
Let's look at the work of so and so,
so and so.
Any good study ought to do that.

357
00:24:11.010 --> 00:24:15.590
Uh,
why would women in,

358
00:24:16.370 --> 00:24:21.370
in cultures like hours and Finland and Canada feel more stressed out because

359
00:24:21.670 --> 00:24:23.720
they're forced to have dual roles.

360
00:24:23.870 --> 00:24:28.640
Women on the whole are forced to take care of the home,
to clean it,

361
00:24:28.641 --> 00:24:31.670
to raise children.
The study never mentioned it.

362
00:24:31.680 --> 00:24:36.350
It controls for height and even smoking but never mentions women having

363
00:24:36.351 --> 00:24:39.260
children.
And this to me is a tremendous blind spot.

364
00:24:40.100 --> 00:24:42.140
So women are in a position,

365
00:24:42.380 --> 00:24:46.400
most women of having to do all the work at home,
raise children.

366
00:24:46.670 --> 00:24:50.870
Lord knows that is stressful enough.
Even if a woman doesn't have children,

367
00:24:50.950 --> 00:24:54.950
we care for an elder who was sick,
uh,

368
00:24:55.700 --> 00:25:00.700
and then go into this completely competitive culture like googles and is

369
00:25:00.951 --> 00:25:03.200
expected to perform not only well,

370
00:25:03.201 --> 00:25:08.201
but very well in a place where she's facing resistance is already stressed out

371
00:25:10.550 --> 00:25:15.550
and has to compete with some young man in a tee shirt and jeans with out family

372
00:25:16.101 --> 00:25:20.210
responsibilities brings a dog to work and then you don't have to walk the dog at

373
00:25:20.211 --> 00:25:24.410
home and you compete with that.
Anybody,

374
00:25:24.411 --> 00:25:25.400
male or female,

375
00:25:25.520 --> 00:25:29.960
if you have to play in two worlds and do them both,

376
00:25:29.961 --> 00:25:34.250
well I guarantee you're going to be uneasy.
That is,

377
00:25:35.570 --> 00:25:37.460
you will display a lot of neuroticism.

378
00:25:38.120 --> 00:25:42.230
And to me that is the important point that that's missing in all this
discussion.

379
00:25:43.280 --> 00:25:46.220
Why are women more stressed out if they are now?

380
00:25:46.250 --> 00:25:48.620
I question that first of all,

381
00:25:48.621 --> 00:25:52.880
this is based on a single study and that conclusion is,

382
00:25:54.170 --> 00:25:58.820
is grasp data to prove a point.
And that is the issue.

383
00:25:58.821 --> 00:26:03.821
It is sexism precisely is sexist because of the dual roles women have to play.

384
00:26:07.030 --> 00:26:07.863
<v 3>Yeah.</v>

385
00:26:07.930 --> 00:26:12.850
<v 2>Uh,
shall I say more or is there another question we have there?
Yeah,
there,</v>

386
00:26:12.851 --> 00:26:14.620
there's a few more questions and then we're,

387
00:26:14.621 --> 00:26:18.280
we're happy to take some audience questions.
I do have,
well actually I realized,

388
00:26:18.281 --> 00:26:22.050
I forgot one more thing to say that,
um,
a memo,

389
00:26:22.080 --> 00:26:26.260
it says very often that these efforts are not good for business.

390
00:26:26.770 --> 00:26:30.730
Why spend all this time trying to welcome in these people who have not

391
00:26:30.731 --> 00:26:35.500
participated because it doesn't work and we're wasting the finite assets of

392
00:26:35.501 --> 00:26:40.240
Google,
which really cracked me up.
I mean,
it's finite,

393
00:26:40.480 --> 00:26:45.480
but Google has vast resources that is spending on a variety of projects and the

394
00:26:46.080 --> 00:26:49.470
efforts for,
um,
to bring other people in.

395
00:26:49.810 --> 00:26:52.080
It's a very small part of that budget.

396
00:26:52.500 --> 00:26:55.740
So that was a bizarre thing to say.

397
00:26:55.950 --> 00:27:00.950
But it is good for business when you leave out the possible contributions,

398
00:27:02.281 --> 00:27:07.050
the creativity of whole classes of people.
You don't know what you're missing.

399
00:27:07.590 --> 00:27:10.410
There are,
you're losing talent in the industry.

400
00:27:11.400 --> 00:27:15.750
And we need fresh values,
uh,
inside the,

401
00:27:16.410 --> 00:27:20.880
the coding world.
It's a segregated world.
Mostly men,

402
00:27:21.330 --> 00:27:25.200
men and Asians,
very few women,
very few people,
uh,

403
00:27:25.230 --> 00:27:27.150
African Americans and Hispanics.

404
00:27:28.410 --> 00:27:32.580
And so inside this culture there,
I'll give you an example.
Uh,

405
00:27:32.581 --> 00:27:35.010
I went to one of those pitch meetings,
you know,

406
00:27:35.060 --> 00:27:39.060
were CEO would be CEOs of startups go and they give their pitch.

407
00:27:39.720 --> 00:27:44.490
And I met a young man who told me about his APP that was going to be used to

408
00:27:44.491 --> 00:27:49.350
screen resumes for a corporation to find good cultural fit.

409
00:27:50.460 --> 00:27:51.480
So I said to him,
well,

410
00:27:51.481 --> 00:27:55.980
good cultural fit is just a byword for keeping up to segregated culture.

411
00:27:56.300 --> 00:28:00.030
In other words,
you want to hire people like the ones you have who fit in,

412
00:28:00.090 --> 00:28:04.710
the people who are there won't feel uncomfortable and you're perpetuating this

413
00:28:05.070 --> 00:28:08.040
segregated society inside the coding world.

414
00:28:08.700 --> 00:28:12.030
And he listened to me patiently and then he said,
all that might be true,

415
00:28:12.600 --> 00:28:15.450
but I'm working for the company,
not for society.

416
00:28:16.470 --> 00:28:19.530
And to me that incorporates the whole problem.

417
00:28:20.520 --> 00:28:23.730
Now you can't have them both overlap,

418
00:28:23.731 --> 00:28:28.230
but let's do a venn diagram where they move closer together and overlap.

419
00:28:28.260 --> 00:28:32.580
How much can we have that overlap between what's good for business?

420
00:28:32.850 --> 00:28:37.170
What's good for computing and what's good for society.
And I really questioned,

421
00:28:37.171 --> 00:28:40.170
as we talked about earlier,
the effect of uh,

422
00:28:40.920 --> 00:28:42.860
banishing experts and the,

423
00:28:43.250 --> 00:28:48.060
the enormous effect this has had on the culture globally actually at this point.

424
00:28:49.050 --> 00:28:53.850
So we need new,
new people to ask new questions.
Uh,
New York City,
uh,

425
00:28:53.851 --> 00:28:58.851
one borough of New York City is about to examine all the algorithms that it uses

426
00:28:59.731 --> 00:29:04.620
to make decisions which go on,
who gets to go to what school,

427
00:29:04.860 --> 00:29:07.740
what police,
uh,
schedules are in different neighborhoods,

428
00:29:07.770 --> 00:29:11.040
even garbage pickup schedules in various neighborhoods.

429
00:29:11.460 --> 00:29:15.420
And so one councilman is saying,
I want to know the bias in here.

430
00:29:15.900 --> 00:29:20.900
I want to know how these decisions are being made and algorithms hide bias

431
00:29:22.920 --> 00:29:27.000
and especially algorithms that are written by other algorithms.

432
00:29:27.001 --> 00:29:30.970
And there is one danger that I just want to mention.
Um,

433
00:29:31.980 --> 00:29:35.850
machine learning is,
uh,
the topic,
the,

434
00:29:36.540 --> 00:29:41.210
the work right now in artificial intelligence,
uh,

435
00:29:42.250 --> 00:29:45.160
computer scientists and engineers who are working in this field.

436
00:29:45.161 --> 00:29:50.161
Some of them express the fear or reservation that once code writes code writes

437
00:29:50.591 --> 00:29:51.310
code.

438
00:29:51.310 --> 00:29:56.310
The original motive of the creators of that system lose control over it.

439
00:29:58.750 --> 00:30:03.250
And they don't actually know any more what the algorithms are doing.

440
00:30:03.460 --> 00:30:06.270
It races away from there.
Uh,

441
00:30:06.430 --> 00:30:11.400
understanding it gets too big and too actually no.

442
00:30:11.401 --> 00:30:15.700
Well,
what is this thing doing?
Is it doing the work?
I thought it would,

443
00:30:16.000 --> 00:30:19.720
is it,
uh,
making good decisions or not?
And so these,

444
00:30:19.820 --> 00:30:21.160
another danger of,

445
00:30:21.930 --> 00:30:25.810
of leaving out the human equation in algorithms and computing.

446
00:30:27.670 --> 00:30:32.450
Uh,
I want to add one thing about why is it important to bring,
uh,

447
00:30:32.590 --> 00:30:36.670
people who've been excluded in I said,
new values.
It's good for business.

448
00:30:36.700 --> 00:30:38.110
Now look,
um,

449
00:30:39.550 --> 00:30:43.480
Uber loses $645 million in a single quarter.

450
00:30:43.930 --> 00:30:47.740
And does anyone say,
well that's bad for business?
No.

451
00:30:47.890 --> 00:30:52.720
They sustain losses as did Amazon for many years,
uh,

452
00:30:53.080 --> 00:30:55.600
to grow their business.

453
00:30:55.780 --> 00:31:00.780
So it is possible to think of these ideas immediately that are new ideas and not

454
00:31:01.931 --> 00:31:04.690
ask right away.
Will this make me money?
The question is,

455
00:31:04.691 --> 00:31:07.930
will it expand the community I serve?
Will I get more customers?

456
00:31:09.100 --> 00:31:12.730
And it's worth a try.
If you lose money,
you just keep trying.
It,

457
00:31:13.030 --> 00:31:14.680
the idea that every,

458
00:31:16.030 --> 00:31:19.660
everything has to make money immediately is,
is a fallacy.
We,

459
00:31:19.690 --> 00:31:24.330
we've seen it all through the industry.
And one more thing I want to bring up,
um,

460
00:31:24.520 --> 00:31:29.470
I think we have a few minutes is,
uh,
age.
Uh,

461
00:31:30.640 --> 00:31:33.770
I want to talk about the value of the past and what,
um,

462
00:31:34.780 --> 00:31:39.010
engineers who were beyond 40,
50,

463
00:31:39.040 --> 00:31:43.780
even 60,
especially have to contribute and who have been excluded.
Um,

464
00:31:44.300 --> 00:31:49.150
uh,
computing software engineering is a young person's profession.
And I think,

465
00:31:49.151 --> 00:31:53.650
you know,
if you walk around here,
I counted more dogs in my sample.

466
00:31:53.651 --> 00:31:56.770
I was a half hour early than I did people over 40.

467
00:31:57.790 --> 00:32:01.720
That's necessarily just,
uh,
anecdotal,
but

468
00:32:03.280 --> 00:32:06.430
you can ask what did other people tried before?

469
00:32:06.670 --> 00:32:11.670
I mean the web page is horribly like an RPG on remote remote programming

470
00:32:14.350 --> 00:32:18.540
generator,
uh,
in this green for IBM computers.
Uh,

471
00:32:18.640 --> 00:32:23.140
may friends in which you filled out a whole form and then you hit send,

472
00:32:23.200 --> 00:32:24.370
which is enter,

473
00:32:24.371 --> 00:32:28.900
which should be called submit and it comes back with an error and you fix that

474
00:32:28.901 --> 00:32:32.920
error,
send,
there's another error,
fix it.
And other error.
Now,

475
00:32:32.921 --> 00:32:35.830
does that sound like a webpage deal?
Yes.

476
00:32:36.490 --> 00:32:40.830
It doesn't go in and check as you go.
Uh,
it's uh,

477
00:32:40.890 --> 00:32:43.250
it's avoiding,
uh,
trips to the server.

478
00:32:43.400 --> 00:32:47.120
I understand it from an engineering context why that would be true.

479
00:32:47.810 --> 00:32:51.560
But then of course,
look at all the chatter to the server with one error,

480
00:32:51.950 --> 00:32:55.760
another error,
another error.
So we,
we need,

481
00:32:55.790 --> 00:33:00.670
we spent all this time in client server computing to bring,
uh,

482
00:33:01.160 --> 00:33:04.730
a client interface that was Richard,
that we could correct errors,

483
00:33:04.731 --> 00:33:09.650
that we could answer questions locally and minimize interactions,
uh,
with,

484
00:33:09.710 --> 00:33:14.550
with the server and do it as we needed.
I think we're way over balance.

485
00:33:14.551 --> 00:33:19.100
Now.
If you look at the history of this,
we're almost back to the RPG terminal.

486
00:33:19.670 --> 00:33:24.260
We way overbalanced in,
in the need of the server versus the human being.

487
00:33:24.620 --> 00:33:29.490
And so this is one of these lessons of the past that needs to be looked at.
Um,

488
00:33:30.380 --> 00:33:34.850
I also,
I also the,
the idea of the human curator.

489
00:33:35.540 --> 00:33:39.920
Now even Google face this,
uh,
the idea was this would be all algorithmic.

490
00:33:40.160 --> 00:33:43.190
And then if you looked for Jew,
you got to watch,

491
00:33:43.191 --> 00:33:47.390
which was a horribly antisemitic site.
It came up pop one.

492
00:33:47.780 --> 00:33:51.230
And Google actually intervened at the top of this and said,

493
00:33:51.231 --> 00:33:56.060
we recommend you look for Jewish or Jewish person because you'll get a different

494
00:33:56.061 --> 00:33:56.720
result.

495
00:33:56.720 --> 00:34:01.720
And that was a startling interaction from this company that was founded on the

496
00:34:02.961 --> 00:34:06.530
Algorithmic,
uh,
I algorithmic a selection.

497
00:34:07.230 --> 00:34:10.640
And we saw what happened to Facebook.
Uh,
the,
the,
the,

498
00:34:10.670 --> 00:34:13.010
the trending stories.

499
00:34:13.430 --> 00:34:17.900
There were human curators of that and that's like,
oh,
the right said,
oh,

500
00:34:17.901 --> 00:34:22.730
there are cherry picking there.
Trending left in reaction.
Um,

501
00:34:22.790 --> 00:34:27.020
Facebook fired.
All of those,
uh,
was curators,

502
00:34:27.021 --> 00:34:28.580
those journalists,

503
00:34:28.680 --> 00:34:33.680
those people looking at the stories and replace them with algorithms,

504
00:34:34.220 --> 00:34:36.950
which instantly brought on fake news.

505
00:34:36.950 --> 00:34:41.210
That is the origin of contemporary fake news.

506
00:34:41.540 --> 00:34:45.260
They were looking at what was trending.
They saw this tremendous traffic,

507
00:34:45.261 --> 00:34:50.261
about some rumor or somewhere that could be perpetuated among people who wanted

508
00:34:50.631 --> 00:34:51.740
to believe that,
that,

509
00:34:52.250 --> 00:34:57.250
that story and that it no longer was coming up with what was trending.

510
00:34:57.651 --> 00:35:02.000
It was coming up with false information.
Well,

511
00:35:02.090 --> 00:35:04.070
what was trending was false information.

512
00:35:04.310 --> 00:35:09.310
And then Facebook brought in human curators again to,

513
00:35:10.390 --> 00:35:12.600
to jury this.
And,

514
00:35:12.630 --> 00:35:17.300
and so we need the human being in these,
in these stories.

515
00:35:17.750 --> 00:35:21.890
If we completely eliminate the human being,
what we have to offer,

516
00:35:21.950 --> 00:35:26.720
our ability to discriminate are subtle abilities to discriminate.

517
00:35:27.230 --> 00:35:30.020
And intuitively it's built into us and evolution.

518
00:35:30.021 --> 00:35:32.000
This is not just some airy fairy thing.

519
00:35:32.420 --> 00:35:36.740
We survived as a species having these intuitions.

520
00:35:36.741 --> 00:35:41.741
And what I mean by that is the ability to quickly notice I'm thinking fast and

521
00:35:42.931 --> 00:35:47.130
slow.
You may know Daniel cattleman's work on this.
This is part of,

522
00:35:47.160 --> 00:35:50.670
of what we have and to eliminate that

523
00:35:52.380 --> 00:35:56.310
does deep store,
uh,
of abilities is to,
uh,

524
00:35:56.880 --> 00:35:57.900
is to really do,

525
00:35:59.650 --> 00:36:02.820
it's to rob a computing of richness.

526
00:36:05.780 --> 00:36:09.740
So you encourage the general public to learn to code and you envision the

527
00:36:09.741 --> 00:36:13.640
creation of an army of coders.
Um,
why?

528
00:36:15.410 --> 00:36:18.890
Well,
first of all,
to demystify code.
I mean,

529
00:36:18.920 --> 00:36:22.520
we're surrounded by all this stuff and in the developed world you can't get out

530
00:36:22.521 --> 00:36:27.140
of it.
Even,
even in an undeveloped countries,
they are wrapped in this also,

531
00:36:27.500 --> 00:36:32.420
um,
the allocation of resources to them,
the study,
all of their crops,

532
00:36:32.630 --> 00:36:35.570
um,
decisions on trade.
You essentially,

533
00:36:35.571 --> 00:36:38.990
except for the desperate people of the world.

534
00:36:38.991 --> 00:36:43.940
And that is another out of the bounds of this talk.

535
00:36:45.120 --> 00:36:48.110
We're wrapped up in this stuff and we can't escape it.

536
00:36:48.410 --> 00:36:52.520
I can't live without my phone.
Can you,
I can't live without my computer.
Can you?

537
00:36:53.080 --> 00:36:55.670
Um,
if you try,
you have to go,
God,

538
00:36:55.671 --> 00:36:59.960
how far do you have to go away to where you won't get a signal?
Uh,

539
00:37:02.360 --> 00:37:03.193
<v 3>so</v>

540
00:37:04.810 --> 00:37:08.590
<v 2>we need to demystify this.
People need to know this is code.</v>

541
00:37:08.620 --> 00:37:12.940
It was written by people.
Oh,
if there wasn't too much machine learning,

542
00:37:13.030 --> 00:37:16.300
it can be changed by people.
Therefore,

543
00:37:17.020 --> 00:37:21.220
maybe you want to look into this and I don't mean everyone should code,
uh,

544
00:37:21.760 --> 00:37:25.360
not at all.
Because it's a,
it takes a very special kind of person.
You know,

545
00:37:25.361 --> 00:37:28.450
you have to have a very high tolerance for failure.
For instance,
you know,

546
00:37:28.780 --> 00:37:32.590
bug after bug after bug.
If,
if that's going to make you nuts,

547
00:37:32.650 --> 00:37:36.910
then forget about it.
And have some intrigue and passionate in this.

548
00:37:37.480 --> 00:37:42.130
So I encourage people to try to learn in some form or fashion to see if they

549
00:37:42.131 --> 00:37:43.390
have that passion for it.

550
00:37:43.810 --> 00:37:48.810
They to find in these rounds of failure or some sense of intrigue that it's,

551
00:37:50.500 --> 00:37:55.180
it's hard but it's the good hard and that will welcome in.
I'm hoping,

552
00:37:55.390 --> 00:37:59.020
uh,
people who have been excluded from,

553
00:37:59.860 --> 00:38:04.000
from the computing culture
a lot depends on education.

554
00:38:04.240 --> 00:38:09.240
I mean our society has defunded taking money away from public schools and uh,

555
00:38:11.290 --> 00:38:14.350
I don't know how people will,
will achieve this knowledge.

556
00:38:14.351 --> 00:38:19.351
And so I bring it up so that we will perhaps as society press to make more

557
00:38:21.431 --> 00:38:26.000
widespread knowledge of coding.
No,
I do not mean to,
um,

558
00:38:26.230 --> 00:38:31.230
raise computing and learning a programming to the level of basic literacy.

559
00:38:33.790 --> 00:38:38.470
I think the people who are literate should invade the coding world.

560
00:38:39.930 --> 00:38:41.440
And I actually,
as I know it,

561
00:38:41.470 --> 00:38:46.450
Google and other technology companies or suddenly wanting to hire philosophers

562
00:38:46.451 --> 00:38:50.140
and people who studied in the humanities again to say,
well,

563
00:38:50.230 --> 00:38:52.840
what are we missing in history?
What are we missing?

564
00:38:52.870 --> 00:38:57.710
And what human beings have learned deeply about?
One another,
uh,

565
00:38:57.760 --> 00:39:02.760
novelists know things about the interior lives of people if it's a good novel.

566
00:39:03.640 --> 00:39:07.690
So I can see that this trend is already underway.

567
00:39:07.930 --> 00:39:12.930
So I hoping to bring in more people in the humanities and bring in a larger

568
00:39:15.130 --> 00:39:18.580
number of people,
period who know how to write code.

569
00:39:18.670 --> 00:39:23.670
I mean we live in a world surrounded by code and a tiny percentage of people on

570
00:39:24.641 --> 00:39:29.641
Earth have any idea what a program is and that clearly has to be an expanded

571
00:39:29.771 --> 00:39:33.430
group.
What can we do as a society to,

572
00:39:33.460 --> 00:39:35.170
to foster coding education?

573
00:39:35.171 --> 00:39:39.220
Do you think it should be part of the curriculum in public schools?

574
00:39:41.170 --> 00:39:44.890
Not at the expense of anything else.
I'll say that.
First of all,

575
00:39:44.980 --> 00:39:48.970
reading out of books,
all the studies show that

576
00:39:50.590 --> 00:39:55.370
one learns very differently and more deeply turning pages,
uh,

577
00:39:56.110 --> 00:40:01.070
kids who learned from books retain,
uh,
not only the story but uh,

578
00:40:01.120 --> 00:40:03.850
associations as opposed to on the screen,

579
00:40:03.851 --> 00:40:07.540
which literally is a bounded experience.

580
00:40:08.170 --> 00:40:12.400
So I'm saying in addition,
I'm saying,

581
00:40:12.970 --> 00:40:17.950
uh,
the way you would have to have PE because it's important for the body,

582
00:40:18.160 --> 00:40:22.840
uh,
the way you need recess because it's important for the mind.

583
00:40:22.900 --> 00:40:23.770
Oh by the way,

584
00:40:23.771 --> 00:40:28.450
programmers get out from under your desk and go home because it's important to

585
00:40:28.451 --> 00:40:29.560
have other experiences.

586
00:40:29.561 --> 00:40:34.561
So you can find those background regions of your mind where good ideas come

587
00:40:34.871 --> 00:40:38.290
from.
Uh,
yes,
it should be available,

588
00:40:38.320 --> 00:40:41.530
but I have to stress to all social classes.

589
00:40:42.760 --> 00:40:47.760
This cannot just be a privileged education because then we're just perpetuating

590
00:40:47.771 --> 00:40:52.090
this,
this culture.
Okay.
Well Great.

591
00:40:52.091 --> 00:40:54.190
Well I think we'll take some questions from the audience.

592
00:40:55.530 --> 00:41:00.060
<v 5>You also talked about the memo and all of that.
So,</v>

593
00:41:00.120 --> 00:41:04.590
um,
the way I see it,
a lot of times,

594
00:41:04.710 --> 00:41:06.240
the whole point,

595
00:41:06.630 --> 00:41:09.750
if it's raised in a certain work environment,

596
00:41:10.560 --> 00:41:14.610
it's overcorrected and then shoved under the rug.
That's how people fix it.

597
00:41:15.780 --> 00:41:16.613
Um,

598
00:41:16.620 --> 00:41:20.910
the way I see it is you have a voice at the moment in the technology world.

599
00:41:21.420 --> 00:41:25.500
So have you considered in,
yes,
you're sick of it,

600
00:41:25.560 --> 00:41:29.070
but so am I,
and I'm just running to work,

601
00:41:29.700 --> 00:41:33.030
but have you considered consulting?
As you said,

602
00:41:33.620 --> 00:41:37.250
CEOs in big companies are looking at humanities now,

603
00:41:37.280 --> 00:41:40.790
but they weren't before and you predicted that they should.

604
00:41:41.330 --> 00:41:46.070
So you clearly you have insight into that world.

605
00:41:46.071 --> 00:41:51.050
Have you considered consulting with CEOs of companies or you mentioned that you

606
00:41:51.051 --> 00:41:53.270
shouldn't use a coding in schools.

607
00:41:53.480 --> 00:41:58.480
So have you considered working with local politicians to somehow make a bill out

608
00:41:59.091 --> 00:42:03.550
of that?
I'm just curious.
That's very interesting because,

609
00:42:03.600 --> 00:42:07.930
<v 2>um,
why editor or that my friends there,
they're asking,
okay,</v>

610
00:42:07.990 --> 00:42:12.040
well you've got this book,
what do you want to do next?
And I was saying,
well,

611
00:42:12.100 --> 00:42:13.720
I don't want to know what I'm going to do next.

612
00:42:13.721 --> 00:42:18.660
They want to see what happens to this morning.
I was reading the times and uh,

613
00:42:18.710 --> 00:42:23.170
there was a story about Google and,
uh,

614
00:42:24.160 --> 00:42:24.993
that,
uh,

615
00:42:25.420 --> 00:42:30.420
one of the employees of a think tank or research tank funded by Google had been

616
00:42:31.271 --> 00:42:34.270
fired because he was questioning some of Google's practices.

617
00:42:35.080 --> 00:42:39.730
And there was a quote from a
really terrible with names,

618
00:42:39.731 --> 00:42:42.900
I can tell you strings of numbers,
but I'm very bad with names.

619
00:42:43.170 --> 00:42:45.010
Who is the head of epic?
All right,

620
00:42:45.011 --> 00:42:50.011
this is a electronic monitor or you may know of what epic is.

621
00:42:50.650 --> 00:42:55.480
And I thought maybe I should go work for them.
Maybe I should,

622
00:42:56.830 --> 00:42:59.730
uh,
gather a group of men and women,
um,

623
00:43:00.550 --> 00:43:02.050
some of whom have worked in,

624
00:43:02.190 --> 00:43:05.710
in silicon valley and in programming who have some funds,

625
00:43:05.770 --> 00:43:10.770
not millionaires or billionaires and form our own research company or our own

626
00:43:11.321 --> 00:43:16.240
political ad that,
you know,
a group to advise a polit politicians.

627
00:43:16.240 --> 00:43:19.180
I want to say there has been this,
uh,

628
00:43:19.240 --> 00:43:24.040
especially involvement with government because,
uh,
there has been this,
uh,

629
00:43:24.130 --> 00:43:28.660
feeling inside the technical world.
Uh,

630
00:43:28.690 --> 00:43:33.340
it's mostly been libertarian government is kind of the enemy regulations or

631
00:43:33.341 --> 00:43:36.920
anathema and yet,
um,

632
00:43:37.090 --> 00:43:40.940
if companies could partner with government,
uh,

633
00:43:41.470 --> 00:43:46.030
that would be very strong,
a way to have a beneficial,
uh,

634
00:43:46.660 --> 00:43:48.460
well maybe it may be not,

635
00:43:48.490 --> 00:43:53.200
maybe they'll partner with politicians who were only on their side,
but,

636
00:43:53.230 --> 00:43:58.080
uh,
yes,
I have considered it.
I'm not sure.
Um,

637
00:44:00.070 --> 00:44:02.140
I don't think of myself as having a voice.

638
00:44:02.620 --> 00:44:06.820
I think of this person who has stumbled into computing because I thought it'd be

639
00:44:06.821 --> 00:44:11.080
fun and all the people involved were really just crazy,
wonderful people,

640
00:44:11.330 --> 00:44:16.330
kind of people I liked and had that and worked with in college and afterwards.

641
00:44:17.890 --> 00:44:22.780
Um,
but I have been around a while,
um,

642
00:44:23.560 --> 00:44:28.330
from the early days and I have encountered,

643
00:44:28.350 --> 00:44:32.970
uh,
a lot of what other women are facing and minorities are facing.

644
00:44:33.380 --> 00:44:34.213
Uh,

645
00:44:35.640 --> 00:44:39.510
I suppose it is a role that I have like it or not and maybe I should learn to

646
00:44:39.511 --> 00:44:40.344
like it.

647
00:44:40.660 --> 00:44:44.290
<v 6>When people talk about technology often they talk about the future and uh,</v>

648
00:44:44.530 --> 00:44:47.650
there's a lot of gloom and doom nowadays for a lot of reasons.

649
00:44:48.700 --> 00:44:51.340
It seems to me it used to be the future was a very happy place.

650
00:44:51.730 --> 00:44:56.550
And maybe you could say a little bit about how you started quite awhile ago when

651
00:44:56.560 --> 00:44:58.990
this career and people talked about the future back then too.

652
00:44:59.200 --> 00:45:03.910
How has your sense of the future changed along along the way?

653
00:45:03.911 --> 00:45:05.020
And does,
does,

654
00:45:05.021 --> 00:45:09.280
do you think that everyone sends to the future says something about themselves?

655
00:45:11.000 --> 00:45:13.300
<v 2>Oh,
that's a great,
that's an interesting point.</v>

656
00:45:13.360 --> 00:45:18.250
And up until what HR individual imagines isn't,
um,

657
00:45:18.970 --> 00:45:23.310
revealing about,
about something deep inside a person.
Um,

658
00:45:23.650 --> 00:45:24.483
the future.

659
00:45:24.670 --> 00:45:29.670
I've never thought of myself as someone who says this is what's going to happen.

660
00:45:29.771 --> 00:45:32.770
The fact that some of these things happen really disturbs me.

661
00:45:32.950 --> 00:45:37.810
I'm really sorry to be right about things like disintermediation and the

662
00:45:37.811 --> 00:45:42.811
unraveling of truths and the sexism and the exclusion of the insider engineering

663
00:45:44.681 --> 00:45:49.300
world.
Uh,
for the future.

664
00:45:49.301 --> 00:45:52.450
I am expecting the next generations to take this up.

665
00:45:53.470 --> 00:45:58.470
I feel that people of my generation and prior ones have a lot of lessons to pass

666
00:46:00.251 --> 00:46:04.120
on.
I mean,
people who work at Google,
you know,

667
00:46:04.150 --> 00:46:07.120
may think that they're creating something brand new,

668
00:46:07.121 --> 00:46:09.040
but they stand on the shoulders of giants.

669
00:46:09.370 --> 00:46:14.020
Algorithms that are being worked on here were written back in the forties and

670
00:46:14.021 --> 00:46:18.970
50s.
So what we're doing here is not new.

671
00:46:20.620 --> 00:46:25.150
What future they are creating.
You are creating,
um,

672
00:46:25.690 --> 00:46:27.430
can be changed.
It does not,

673
00:46:27.910 --> 00:46:32.400
this idea that the future just happens.
Uh,

674
00:46:33.010 --> 00:46:37.390
Kevin Kelly,
he's someone I deeply disagree with.
I respect him.

675
00:46:37.960 --> 00:46:42.960
I wrote a book called what does technology want as if technology and all had a

676
00:46:43.721 --> 00:46:44.440
motive,

677
00:46:44.440 --> 00:46:48.760
that there was something inside technology that would create a world on its own.

678
00:46:49.090 --> 00:46:49.923
No,

679
00:46:50.260 --> 00:46:54.670
what creates technology is human desire is what you want,

680
00:46:54.730 --> 00:46:57.540
what you envision for the future.
And I,

681
00:46:57.550 --> 00:47:00.760
that's where I see it's up to new generations who take this up.

682
00:47:01.120 --> 00:47:03.760
They have to speak up for their world.
I mean,

683
00:47:03.790 --> 00:47:06.740
as I looked around when I lived in and,

684
00:47:06.870 --> 00:47:09.550
and just tried to be clear eyed and balanced,
I,

685
00:47:10.090 --> 00:47:14.350
I'm sorry that some of the things in here don't reveal the excitement I feel

686
00:47:14.351 --> 00:47:15.340
about technology,

687
00:47:15.370 --> 00:47:19.990
my love of it and my skepticism and disappointment.

688
00:47:20.680 --> 00:47:22.240
Uh,
so I think

689
00:47:23.860 --> 00:47:27.910
this generation needs to sit down and turn their cold iron it,

690
00:47:28.180 --> 00:47:32.230
not just have this,
about the future and there will be wondrous things coming,

691
00:47:32.620 --> 00:47:37.170
but also sit back and go,
well,
what does this mean?
What changes?
Uh,

692
00:47:37.180 --> 00:47:41.320
my engendering in the world?
What should I question and what should I love?

693
00:47:43.140 --> 00:47:46.890
They are the future.
They're creating it so that the future doesn't just happen.

694
00:47:47.190 --> 00:47:51.950
People make it.
Does that answer your question?

695
00:47:52.760 --> 00:47:54.590
Sort of.
Okay.
Go back and ask you that again.

696
00:47:55.190 --> 00:47:58.050
<v 6>I have different questions if that's okay.
It's about religion.</v>

697
00:47:58.051 --> 00:48:00.660
I don't know if you get as much as I have not read your book yet.
I'll read it.

698
00:48:00.661 --> 00:48:03.250
But,
so I don't know if you comment on religion.
Uh,

699
00:48:03.260 --> 00:48:08.260
among the many experts are being disintermediated lately are religious experts

700
00:48:08.870 --> 00:48:11.640
and,
and we're religious people don't just talk about God.

701
00:48:11.641 --> 00:48:13.020
They also talk about right and wrong.

702
00:48:13.200 --> 00:48:17.900
And here among the many people who will explain to you why lying is bad,
um,

703
00:48:18.120 --> 00:48:22.560
but these sorts of experts are not being listened to as they were.
Um,

704
00:48:22.800 --> 00:48:26.370
do you imagine that this has any impact on what's going on?
What we see now?

705
00:48:30.470 --> 00:48:32.120
<v 2>Well,
it's not only religious people.</v>

706
00:48:32.150 --> 00:48:37.150
There are people of goodwill and good faith in human beings who bring up these

707
00:48:37.671 --> 00:48:39.870
issues.
Um,
my,

708
00:48:39.920 --> 00:48:44.700
my questions are moral and we do this,
those voices.
Uh,

709
00:48:45.170 --> 00:48:48.560
matter of fact,
people have people who lie,

710
00:48:49.400 --> 00:48:54.140
say they're against light.
Okay.
Donald Trump lies and he's against fake news.

711
00:48:54.620 --> 00:48:57.950
So yes,
we are missing those voices who are authoritative.

712
00:48:58.160 --> 00:49:01.670
They know what they're doing right.
And say,
no.
A lie is a lie.

713
00:49:02.810 --> 00:49:06.830
Uh,
mistreating someone is wrong.

714
00:49:07.760 --> 00:49:11.780
There are injustices that we need to write.
Uh,
yes,

715
00:49:11.810 --> 00:49:13.850
they been left out certainly.

716
00:49:13.851 --> 00:49:18.851
And unfortunately some of those religious voices are,

717
00:49:20.830 --> 00:49:21.950
are extremely right way.

718
00:49:23.480 --> 00:49:27.890
And so it's,
it's complicated.

719
00:49:31.430 --> 00:49:32.390
Does that answer your question?

720
00:49:34.090 --> 00:49:37.960
<v 6>Can you say a little bit about your experience working on the audio books?</v>

721
00:49:38.410 --> 00:49:42.310
Did you get any guidance as far as like director,
like acting sort of direction?

722
00:49:43.940 --> 00:49:47.350
<v 2>No,
my publisher said,
well,</v>

723
00:49:47.360 --> 00:49:49.740
I'm actually Macmillan.
Nope.

724
00:49:49.940 --> 00:49:54.350
FSG is a wonderful literary imprint within this huge corporation called

725
00:49:54.351 --> 00:49:57.500
Macmillan.
They do the audio books and they said,
well,

726
00:49:57.530 --> 00:50:02.260
we'd really like you to do it because when we have nonfiction books,
we like the,

727
00:50:02.730 --> 00:50:05.960
the author to read it.
Especially since there are a lot of your book,

728
00:50:05.990 --> 00:50:10.070
most of it isn't the first person.
So,
um,

729
00:50:11.090 --> 00:50:14.690
I was heard it would be grueling,
but I was encouraged to do it.

730
00:50:15.860 --> 00:50:19.880
So I did it.
Oh No,
I just walked in cold.

731
00:50:19.940 --> 00:50:23.220
I mean I've had some public speaking experience and uh,

732
00:50:23.630 --> 00:50:27.590
actually singing is what turned out to help me the most,
uh,

733
00:50:27.650 --> 00:50:32.120
breath control over periods of time.
But no,
I went in this booth,

734
00:50:33.140 --> 00:50:36.670
I put on some headphones.
There was the book in front of me.
Um,

735
00:50:36.770 --> 00:50:40.580
actually it was on an iPad,
no turning pages noise.

736
00:50:41.030 --> 00:50:44.900
And I just started from page one and we did a hundred pages a day,

737
00:50:46.100 --> 00:50:51.020
uh,
to finish the book.
It was a grueling experience.
Uh,

738
00:50:53.330 --> 00:50:56.930
luckily,
uh,
it,
you don't have to read through like you're on a stage,

739
00:50:56.931 --> 00:51:01.680
you can just back up and any,
any punctuation point and go forward.
But,
uh,

740
00:51:01.740 --> 00:51:05.930
it was quite an experience,
um,
on people who drive.
Uh,

741
00:51:06.020 --> 00:51:10.040
there were many people who just read audio books while they,
while they commute.

742
00:51:10.041 --> 00:51:12.950
So I'm hoping now some of those people will pick it up.

743
00:51:14.100 --> 00:51:17.180
<v 0>You,
I think you're kind of unique in the sense that you,</v>

744
00:51:17.320 --> 00:51:21.510
you were very advanced programmer and coder and then you've gone on to become a

745
00:51:21.511 --> 00:51:22.890
really successful writer.

746
00:51:24.120 --> 00:51:29.000
So writing in code is a certain type of language and then writing and English in

747
00:51:29.010 --> 00:51:33.810
the literary way that you do is also another type of language.
So,
um,

748
00:51:34.400 --> 00:51:35.130
do you,

749
00:51:35.130 --> 00:51:38.460
could you talk maybe about the parallels between those two or if there's any

750
00:51:38.461 --> 00:51:42.210
synergies and what maybe learning to write in code has,

751
00:51:42.630 --> 00:51:45.870
if it has at all influenced the way that you write or the way that you think

752
00:51:45.871 --> 00:51:46.704
about writing?

753
00:51:47.080 --> 00:51:51.650
<v 2>Well,
code expresses itself by running,
by working I,</v>

754
00:51:51.680 --> 00:51:56.620
its meaning is what it does.
Um,
an algorithm can be beautiful.

755
00:51:56.650 --> 00:52:00.910
It can express elegance.
But finally what it does is what it means

756
00:52:02.620 --> 00:52:07.060
and riding.
You never know when it works,
your ideas to say,
okay,
that works.

757
00:52:07.120 --> 00:52:11.330
But there's no compiler.
There's,
there's no test or uh,

758
00:52:11.560 --> 00:52:13.810
there's nothing there except your own facility.

759
00:52:15.280 --> 00:52:20.280
Coding taught me to focus and have stamina and have the resolve to solve

760
00:52:22.451 --> 00:52:25.810
problems.
It,
uh,

761
00:52:27.530 --> 00:52:28.660
it's,
it's hard,

762
00:52:29.320 --> 00:52:33.010
but a good heart and writing is also hard and a good heart.

763
00:52:33.850 --> 00:52:38.320
The overlap in languages.
Uh,
I'm afraid I don't see,

764
00:52:38.850 --> 00:52:39.850
um,
as I say,

765
00:52:39.851 --> 00:52:44.440
code can have elegance but one is very structured and you have requirements

766
00:52:44.441 --> 00:52:46.570
about how you're going to write code.

767
00:52:47.710 --> 00:52:52.710
You have style you have to follow within an organization and code reviews and so

768
00:52:54.581 --> 00:52:58.300
forth.
The Code Review in in a book as well.

769
00:52:58.301 --> 00:52:59.830
Will people read it and will they like it?

770
00:53:00.690 --> 00:53:03.470
<v 1>Uh
Huh.</v>

771
00:53:03.640 --> 00:53:08.640
<v 2>Language is expressive and you can say things that are just wrong grammatically</v>

772
00:53:10.300 --> 00:53:14.860
and people will understand it anyway.
That's one of its beauties.
Um,
people who,

773
00:53:14.950 --> 00:53:17.800
who come from different cultures can write sort of a hip hop.

774
00:53:17.801 --> 00:53:22.690
They can write in a code of the Caribbean life who know Diaz.

775
00:53:22.960 --> 00:53:23.710
Uh,

776
00:53:23.710 --> 00:53:28.260
they can really use language and jog lid or be extremely formal.
Yeah.

777
00:53:28.400 --> 00:53:30.110
<v 1>Uh,
and</v>

778
00:53:30.550 --> 00:53:31.690
<v 2>they're all valid means</v>

779
00:53:31.940 --> 00:53:34.040
<v 1>depression in language and that,</v>

780
00:53:34.850 --> 00:53:38.930
<v 2>that you can do it wrong and still have it work is one of the wonderful things</v>

781
00:53:38.931 --> 00:53:39.764
about language.

782
00:53:40.160 --> 00:53:40.690
<v 1>Yeah.</v>

783
00:53:40.690 --> 00:53:44.170
<v 2>Okay.
Well thank you everyone for coming.
Thank you all so much for coming.</v>

784
00:53:44.410 --> 00:53:45.790
Thank you for one more round of applause.

785
00:53:46.570 --> 00:53:47.040
<v 1>My place.</v>

