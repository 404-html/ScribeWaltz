WEBVTT

1
00:00:06.070 --> 00:00:07.060
Hello,

2
00:00:10.310 --> 00:00:11.143
<v 2>that's a good start.</v>

3
00:00:13.370 --> 00:00:17.960
<v 0>So we're Zach and Kelly Wiener Smith and we wrote a book called Soonish 10</v>

4
00:00:17.961 --> 00:00:21.650
emerging technologies that'll improve and our ruin everything.
Uh,

5
00:00:21.680 --> 00:00:23.310
I'm a parasitologist.

6
00:00:23.330 --> 00:00:26.760
I study parasites that manipulate the behavior of their hosts a,

7
00:00:26.770 --> 00:00:28.640
and I work for Rice University

8
00:00:28.860 --> 00:00:31.400
<v 2>and I draw,
I draw comics and stuff.
Yeah.</v>

9
00:00:33.660 --> 00:00:37.710
<v 0>Uh,
so we decided to write a book on technology.
And in 2011,</v>

10
00:00:37.711 --> 00:00:41.540
a group of policy students at Hamilton College wrote a book called,
uh,

11
00:00:41.840 --> 00:00:45.660
what did they bring?
A paper,
sorry,
called our talking head's blowing hot air.

12
00:00:45.960 --> 00:00:49.080
And essentially they were looking at the predictive abilities of 26 really

13
00:00:49.081 --> 00:00:51.750
popular pundants that were on lots of TV shows.

14
00:00:52.080 --> 00:00:56.190
And these pundits ranged from being mostly right to mostly wrong,

15
00:00:56.370 --> 00:00:59.940
but importantly,
they all still had their jobs.
And so we were like,

16
00:00:59.941 --> 00:01:02.930
we should write a book about tech where like,

17
00:01:02.940 --> 00:01:06.510
it doesn't matter if we're right or wrong because apparently that doesn't impact

18
00:01:06.511 --> 00:01:09.440
your ability to have a job.
But actually we're,

19
00:01:09.441 --> 00:01:13.170
we're not really interested in this book and making predictions because we think

20
00:01:13.171 --> 00:01:16.650
that what's interesting isn't necessarily figuring out how many years out of

21
00:01:16.651 --> 00:01:21.000
technology is so much as talking about what the amazing challenges that people

22
00:01:21.001 --> 00:01:22.410
are working on right now are.

23
00:01:22.410 --> 00:01:26.430
And so we talked a lot more about the technical hurdles that still need to be

24
00:01:26.431 --> 00:01:27.270
overcome.

25
00:01:27.480 --> 00:01:30.450
And then we talk a little bit about how these technologies could make everything

26
00:01:30.451 --> 00:01:33.030
awesome but also maybe horrible.

27
00:01:33.390 --> 00:01:36.900
And so we give you a little bit of a taste of one of the chapters in the book

28
00:01:36.901 --> 00:01:41.790
and a little bit,
uh,
on one of the note to Ben A's.
What did you want to say?

29
00:01:41.860 --> 00:01:45.180
Yeah.
Oh,
okay.
Uh,
so the 10 topics we cover in the book,

30
00:01:45.181 --> 00:01:48.060
let's see if I can remember them all.
Are a cheap access to space,

31
00:01:48.061 --> 00:01:52.380
asteroid mining,
fusion,
brain,
computer interfaces,
bioprinting,

32
00:01:52.381 --> 00:01:57.270
augmented reality,
um,
robotic construction,

33
00:01:57.300 --> 00:02:02.130
programmable matter,
BCI,
brain computer interfaces.
I thought I said that already.

34
00:02:02.150 --> 00:02:05.200
And then bioprinting.
Bioprinting,
didn't I say bioprinting?
Yeah,

35
00:02:05.280 --> 00:02:08.700
I said both of those already.
So we're still at eight now,
Huh?
That's fine.

36
00:02:08.820 --> 00:02:13.470
Wait by the book,
you'll see,
you'll find out the rest.
That's our page.
Um,

37
00:02:13.850 --> 00:02:16.380
and at the end of a lot of the chapters,
we like,

38
00:02:16.381 --> 00:02:19.560
we uncovered all sorts of crazy stuff while we were doing the research for this

39
00:02:19.561 --> 00:02:19.831
book.

40
00:02:19.831 --> 00:02:23.310
And that's like a ton of a big part of why this book was so much fun to write.

41
00:02:23.520 --> 00:02:25.290
And so at the end of some of the chapters,

42
00:02:25.291 --> 00:02:29.280
we talk about these weird things that we encountered and we include these in our

43
00:02:29.320 --> 00:02:31.740
[inaudible].
And so we're going to discuss,

44
00:02:31.741 --> 00:02:35.250
one of the Nota Bene is that we wrote in the book as well.
It's not this one,

45
00:02:35.251 --> 00:02:38.730
but this is that.
This was really awesome.
And then we got to talk to this,

46
00:02:38.731 --> 00:02:41.820
but we've got to talk to so many amazing people while doing this book.
Uh,

47
00:02:41.821 --> 00:02:43.440
but anyway,
so we,
uh,

48
00:02:43.470 --> 00:02:46.860
we've been told that talking instead of talking about cheap access to space,

49
00:02:46.861 --> 00:02:50.400
programmable matter would probably go over best with this audience.

50
00:02:50.401 --> 00:02:54.540
So we're doing the programmable matter chapter.
Uh,
so how about you start?
Oh,

51
00:02:54.541 --> 00:02:56.730
I'll start.
Okay,
sure.
Um,
so

52
00:02:57.190 --> 00:02:59.830
<v 2>I feel like I needed to not talk as,
because it's Google,</v>

53
00:02:59.831 --> 00:03:02.770
so I don't need to explain like what are,
what are computers I guess?
Right.

54
00:03:02.800 --> 00:03:05.050
You all know about those.
Um,
yeah.

55
00:03:05.051 --> 00:03:08.950
So the idea in general with programming will matter a is that you in the same

56
00:03:08.951 --> 00:03:12.450
way that a computer is universal,
you can make stuff that's universal.
Um,

57
00:03:12.460 --> 00:03:15.490
and there are a lot of different approaches to how you might do that.

58
00:03:15.491 --> 00:03:18.010
I think we go through a couple of them here,
the books who lot more extensive,

59
00:03:18.280 --> 00:03:19.600
but when we say programmable matter,

60
00:03:19.601 --> 00:03:22.150
we're kind of condensing together a bunch of different fields,

61
00:03:22.151 --> 00:03:24.550
like self reconfiguring matter.
There's a,

62
00:03:24.640 --> 00:03:27.820
there's a book called Morpho genetic engineering and I wish that gone with that.

63
00:03:27.821 --> 00:03:30.640
That was really cool.
Um,
I'm gonna just stop.

64
00:03:30.641 --> 00:03:33.070
That can reconfigure itself in different ways.

65
00:03:33.071 --> 00:03:37.600
So we're going to go through a few ways.
Um,
it's a little wordy.

66
00:03:37.810 --> 00:03:42.580
Um,
so this is,
um,
my drawing of Skylar Tibbets,
Skylar Tibbets guy at Mit.

67
00:03:42.581 --> 00:03:46.090
He does what he calls 40 printing.
Uh,
which just means it's,

68
00:03:46.091 --> 00:03:50.500
it's three d printing,
but the stuff does more once it's printed.
So,
um,

69
00:03:50.670 --> 00:03:54.190
there are a bunch of examples of this,
but when we thought it was cute was,
um,

70
00:03:54.790 --> 00:03:58.210
was he made the straw and it's just made so that the joints are printed so that

71
00:03:58.211 --> 00:03:59.230
when they intake water,

72
00:03:59.530 --> 00:04:02.580
they bend in a way that you quote unquote program into the materials.

73
00:04:02.581 --> 00:04:04.870
So he made one where you program we're programming.

74
00:04:04.871 --> 00:04:08.020
So you dropped the stick and water in spells at MIT cause he's at Mit.

75
00:04:08.021 --> 00:04:09.790
But what's cute about that we thought is you could,

76
00:04:09.940 --> 00:04:12.190
you could like really creep people out.
It,
they didn't know what it was

77
00:04:14.200 --> 00:04:17.890
like you can make programmable spaghetti and it just says like find help or

78
00:04:17.891 --> 00:04:22.240
something.
I don't know.
Um,
but yeah,

79
00:04:22.241 --> 00:04:23.320
so that's sort of like,
uh,
oh,

80
00:04:23.450 --> 00:04:25.910
this one of the ones that's theoretically more functional.
I don't,

81
00:04:26.260 --> 00:04:29.560
I don't think the,
the,
the wood thing was his right.
That was,
um,
right.
Yeah.

82
00:04:29.561 --> 00:04:31.270
So it's just one project where it's essentially would,

83
00:04:31.271 --> 00:04:33.970
that was designed to respond to humidity.

84
00:04:34.170 --> 00:04:37.000
And so it was like you can give a building basically poured so that they can

85
00:04:37.060 --> 00:04:39.930
open out a close up depending on ambient conditions.
Um,

86
00:04:39.931 --> 00:04:41.410
so there a couple of projects working on that.

87
00:04:41.411 --> 00:04:43.860
And then in addition to just kind of looking awesome,
uh,

88
00:04:44.080 --> 00:04:48.940
you have the potential for zero or low energy environmental regulation
mechanism.

89
00:04:48.970 --> 00:04:51.100
Uh,
but mostly it looks really cool.

90
00:04:51.101 --> 00:04:54.430
It looks like you're looking at it like a giant dead alien.
Uh,
so that's neat.

91
00:04:55.320 --> 00:04:57.800
<v 0>Well,
so one of the things,
uh,
that,</v>

92
00:04:57.801 --> 00:05:00.710
that Skylar Tibbets pointed out was that the hard thing about this technology

93
00:05:00.711 --> 00:05:04.610
right now is that there isn't really good software written that programs in

94
00:05:04.611 --> 00:05:07.760
information about joints and how they respond to environmental conditions.

95
00:05:07.940 --> 00:05:11.030
And I know you guys have like 20% of time that you get to spend on anything
else.

96
00:05:11.031 --> 00:05:14.150
So Fyi,
you want to?
Sure.

97
00:05:14.151 --> 00:05:18.350
So then the next category that we talked about our origami robots and we got

98
00:05:18.351 --> 00:05:22.370
really excited about Origami and robots.
And so we had this guy,
Jason Koon,

99
00:05:22.670 --> 00:05:27.140
a design an origami.
Uh,
what does that robot,

100
00:05:27.200 --> 00:05:31.490
robot robot.
But,
but anyway,
so,
so if you're interested in our origami robot,

101
00:05:31.491 --> 00:05:34.970
you can download the design,
uh,
here and you can see our origami robot.

102
00:05:35.150 --> 00:05:38.500
So Origami and you know,
I'm sure you're all familiar with Origami.

103
00:05:38.510 --> 00:05:42.710
You have a sheet of paper and you follow like some rules for how you fold it and

104
00:05:42.711 --> 00:05:46.700
you make it be this different shape.
Well,
if you also put actuators in there,

105
00:05:46.910 --> 00:05:50.120
you can get the paper to fold itself so that you don't have to do it on your
own,

106
00:05:50.121 --> 00:05:53.210
right?
Because why spend the time doing that?
It's really frustrating.

107
00:05:53.211 --> 00:05:57.920
I know some people think it's beautiful.
That's probably not me.
Uh,
and so anyway,

108
00:05:57.921 --> 00:06:01.220
you can put these actuators in there and you can get them be with these

109
00:06:01.221 --> 00:06:01.791
actuators.

110
00:06:01.791 --> 00:06:05.840
You can get the robots to like walk around and pick stuff up and do all sorts of

111
00:06:05.841 --> 00:06:09.100
crazy stuff.
What were you going to say?
Oh,
okay.
So,
uh,

112
00:06:09.110 --> 00:06:12.320
one of the cool things that they're working on getting these robots to do is,

113
00:06:13.250 --> 00:06:14.750
is have medical applications.

114
00:06:14.751 --> 00:06:19.751
So doctor Daniella roots at MIT made this Origami robot out of sausage casing

115
00:06:21.260 --> 00:06:24.770
and you essentially folded up,
you stick it in ice,

116
00:06:24.920 --> 00:06:26.720
the person swallows the ice,

117
00:06:26.750 --> 00:06:29.750
and then the Origami bought when the ice dissolves pops out,

118
00:06:30.050 --> 00:06:31.940
and then you can control it with a magnet.

119
00:06:32.300 --> 00:06:34.760
And apparently this number blew my mind.

120
00:06:34.790 --> 00:06:39.790
Apparently 3,500 people every year swallow those batteries that you find in

121
00:06:41.061 --> 00:06:44.290
watches and having a three and a half year old and a one year old,

122
00:06:44.300 --> 00:06:46.820
I'm guessing that they're all people under five

123
00:06:47.230 --> 00:06:51.460
<v 2>children,
but that was mostly children.
People would certain tastes.</v>

124
00:06:51.570 --> 00:06:55.860
<v 0>Sure.
Okay.
Uh,
and so what this robot does is like,
so some,</v>

125
00:06:55.890 --> 00:07:00.000
some percent of those 3,500 people end up with the battery lodged and part of

126
00:07:00.001 --> 00:07:03.000
their stomach and they can't get it out.
And then you have problems.
If it passes,

127
00:07:03.001 --> 00:07:06.450
you're fine.
But if it gets stuck,
you have problems.
So this robot goes in,

128
00:07:06.480 --> 00:07:09.780
opens up,
and then you control it with a magnet.
It connects to the battery,

129
00:07:09.781 --> 00:07:11.580
it yanks it out to dislodge it,

130
00:07:11.880 --> 00:07:15.390
and then it passes with everything else and leaves naturally.
Uh,

131
00:07:15.420 --> 00:07:16.860
and so we're hoping that,
that,

132
00:07:16.861 --> 00:07:20.400
that little robot never really developed the ability to consider its life

133
00:07:20.401 --> 00:07:23.580
objectively because it,
it might be a little depressed,
but again,

134
00:07:23.581 --> 00:07:24.660
it's sausage casing.

135
00:07:24.661 --> 00:07:27.150
So it's going to dissolve away and it's going to go away so you don't have to

136
00:07:27.151 --> 00:07:30.800
worry about however it feels about things cause it'll be dead.

137
00:07:31.470 --> 00:07:36.420
And so,
so anyway,
that's one use.
But,
uh,
Daniella,
is it helping?

138
00:07:36.460 --> 00:07:39.510
It was hoping that first of all,
at some point you won't be able,

139
00:07:39.511 --> 00:07:42.510
you won't have to like control it remotely.
It'll work on its own.

140
00:07:42.870 --> 00:07:46.380
And then she's also working on other things like can you get these bots to

141
00:07:46.381 --> 00:07:48.540
deliver medicine to very particular areas?

142
00:07:48.541 --> 00:07:51.480
And so she's thinking about the medical applications,

143
00:07:51.690 --> 00:07:54.720
but you can also make these things big enough that it could be like a table that

144
00:07:54.721 --> 00:07:56.970
if someone's disabled the table,
you know,

145
00:07:56.971 --> 00:08:00.660
puts itself together and then walks over to you so you don't have to go to it or

146
00:08:00.661 --> 00:08:03.840
a chair that can just fold up together and go to where it needs to go.

147
00:08:04.140 --> 00:08:07.800
And so you can see that or can imagine lots of cool applications for that.

148
00:08:08.470 --> 00:08:13.320
<v 2>Yeah.
But go ahead.
Do we do it?
Okay.
So the,
the sort of,</v>

149
00:08:13.350 --> 00:08:17.490
uh,
uh,
the,
the,
the,
the Super Advanced,
uh,

150
00:08:17.520 --> 00:08:21.090
paradigm that may never happen is,
is called the bucket of stuff paradigm,

151
00:08:21.091 --> 00:08:24.950
which is something like this.
Uh,
perhaps,
um,
but,

152
00:08:24.970 --> 00:08:27.330
but the basic idea is it's kind of like having a t 1000,

153
00:08:27.331 --> 00:08:31.860
but like fix his stuff in your house or it,
uh,
instead of killing you,

154
00:08:32.110 --> 00:08:35.520
um,
or you know,
but if it's truly universal,
should be able to turn to anything.

155
00:08:35.521 --> 00:08:39.060
It can be a range,
it could be a phone.
Um,
you can even be,
you know,

156
00:08:39.061 --> 00:08:43.680
if you can command it,
uh,
uh,
and it's,
you know,
on your side,
um,
you know,
you can,

157
00:08:43.710 --> 00:08:47.910
you can just tell it to globe over and do something for you.
Um,

158
00:08:48.540 --> 00:08:52.770
so there are a lot of problems with this paradigm and the privacy thing is one

159
00:08:52.780 --> 00:08:56.490
action they will probably get to in a little bit.
Uh,
but,
um,

160
00:08:56.850 --> 00:08:59.670
we had a couple of people actually working on this.
The big problem is the,

161
00:08:59.671 --> 00:09:01.920
I think the smallest one was like a cubic centimeter.

162
00:09:02.040 --> 00:09:05.540
You have to have these little quote unquote Adams,
uh,
that,
um,

163
00:09:05.700 --> 00:09:09.990
can move consent's a little bit,
can talk with each other.
That's important.
Um,

164
00:09:10.010 --> 00:09:12.040
and then after that,
some of the stuff is like luxury.
You,

165
00:09:12.041 --> 00:09:14.610
you might want a battery on board each one.
Um,

166
00:09:14.611 --> 00:09:16.770
so miniaturizing this is a really tough problem.

167
00:09:17.970 --> 00:09:20.640
And then get into the math thing a little bit.
Uh,
that's right now.

168
00:09:20.700 --> 00:09:22.350
It's right now.
Oh God.
Yeah.
So that,

169
00:09:22.910 --> 00:09:25.320
one of the really interesting things we found out is that one of the

170
00:09:25.350 --> 00:09:30.330
difficulties of building a t 1000 to serve you is the math.
Uh,
because,
um,

171
00:09:30.630 --> 00:09:34.350
I think the way we say it is if you imagine you have a,
a,

172
00:09:34.351 --> 00:09:37.050
a marching band that's got to shape from say a star and do a,

173
00:09:37.080 --> 00:09:40.170
like the university logo.
And so you only have a hundred people.

174
00:09:40.171 --> 00:09:43.490
That's not that hard a problem.
Plus each atom of that system as a human brains.

175
00:09:43.500 --> 00:09:46.290
That helps.
Um,
but,
but yeah,
it's just not that hard problem.

176
00:09:46.291 --> 00:09:48.540
But you imagine each time you add another individual,

177
00:09:48.541 --> 00:09:51.400
the problem doesn't just get one person hard to write it scales.
Uh,

178
00:09:51.450 --> 00:09:53.670
and so if it's a thousand net comes a really hard problem.

179
00:09:53.671 --> 00:09:56.400
People knowing where to go,
what to do if someone falls over.

180
00:09:57.210 --> 00:09:59.310
And then if you scale,
you know,
10,000 or a million,

181
00:09:59.311 --> 00:10:01.380
or I guess the t one doesn't want to have a billion and they're in three

182
00:10:01.381 --> 00:10:02.710
dimensions and,

183
00:10:02.711 --> 00:10:05.610
and presumably there are all sorts of like physical constraints at each point.

184
00:10:05.611 --> 00:10:07.710
Like,
you know,
along the equivalent of a bone,

185
00:10:07.711 --> 00:10:09.360
you have to all be docked with each other a certain way.

186
00:10:09.660 --> 00:10:13.830
What happens is calculating what everybody needs to do to like make your hand

187
00:10:13.831 --> 00:10:17.400
into a giant knife to kill,
uh,
that,
that one guy in the movie,

188
00:10:17.670 --> 00:10:21.960
I shouldn't be talking about the two months,
but,
uh,

189
00:10:21.961 --> 00:10:24.510
so if you want to do that,
it's actually a pretty tough math problem,
right?

190
00:10:24.790 --> 00:10:27.000
Is that you have to expand just the right amount of atoms.

191
00:10:27.001 --> 00:10:28.470
They all have to go to the right place.
And crucially,

192
00:10:28.471 --> 00:10:31.560
if you really want to kill a human,
they have to go fast.
Um,

193
00:10:31.561 --> 00:10:35.750
so like there's a version of super preliminary version of this called kilobytes,

194
00:10:35.760 --> 00:10:37.380
which if you want to visualize it,
it's like a little,

195
00:10:37.381 --> 00:10:40.050
almost a size of a watch battery canister with three little legs and just kind

196
00:10:40.051 --> 00:10:41.800
of moves by juggling.
Um,

197
00:10:41.880 --> 00:10:46.320
and they're called Keela bots because the original system had 1,024 of them and

198
00:10:46.740 --> 00:10:50.310
not a thousand,
cause it's nerd town.
Um,
but uh,
yeah,

199
00:10:50.311 --> 00:10:54.090
so thousand 24 of these and they,
um,
being aware of this certain problem,

200
00:10:54.091 --> 00:10:56.640
they wanted it to have a relatively simple algorithm that each of them were

201
00:10:56.641 --> 00:11:00.150
using.
And so they did get a tour.
They could shape like a wrench,

202
00:11:00.151 --> 00:11:03.270
like not when you could ever use it like a three or a two d shape of a wrench

203
00:11:03.271 --> 00:11:05.820
and then change into say a star or something.
The problem was,

204
00:11:05.821 --> 00:11:08.190
I think it was like,
it took six hours to go from one to the other.

205
00:11:08.420 --> 00:11:12.680
They say they're really a sort of simple perimeter crawling algorithm.
Uh,

206
00:11:12.840 --> 00:11:16.310
and you know,
again,
we want to kill somebody or do you know,

207
00:11:16.500 --> 00:11:20.790
have a phone instantly appear in your hand.
Um,
and then have a kill somebody,
uh,

208
00:11:21.420 --> 00:11:23.380
uh,
you're going to have a problem unless you solve this math problem.

209
00:11:23.381 --> 00:11:23.720
I don't know.

210
00:11:23.720 --> 00:11:26.730
It might not even be solvable a or at least not sellable in the sense of getting

211
00:11:26.731 --> 00:11:30.420
a way to do it quickly and properly.
But yeah,

212
00:11:30.840 --> 00:11:34.970
but this audience consultant 80% of your time to the robot.
Yeah.
Murderer.
Yeah.

213
00:11:38.120 --> 00:11:41.450
<v 0>So there,
there were a lot of reasons why a bucket of stuff could be a problem.</v>

214
00:11:41.451 --> 00:11:46.451
So an ideal bucket of stuff would be able to become like a camera or receiver

215
00:11:46.641 --> 00:11:49.430
and it could transmit information.
And if these get really tiny,

216
00:11:49.431 --> 00:11:52.040
you could imagine that it'd be very easy to spy on someone.
You know,

217
00:11:52.041 --> 00:11:55.180
you just put some of these in all hotel rooms and then you can spy on everyone

218
00:11:55.181 --> 00:11:58.090
and transmit that information anywhere.
And then additionally,

219
00:11:58.091 --> 00:12:00.520
if you can get this bucket of stuff to look like anything,

220
00:12:00.640 --> 00:12:03.640
you could make it look like your clothes and then you could bring it on a flight

221
00:12:03.641 --> 00:12:06.880
and then you could turn it into something more dangerous.
And so presumably,

222
00:12:06.881 --> 00:12:09.580
you know,
the TSA would be trying to keep the bucket of stuff out,

223
00:12:09.850 --> 00:12:12.760
but it's hard to know how they would be able to do something like that.
Uh,

224
00:12:12.800 --> 00:12:13.870
so then you talked about there's,

225
00:12:13.871 --> 00:12:17.560
there's privacy concerns and then there's patenting concerns.

226
00:12:17.590 --> 00:12:19.810
So if you have a bucket of stuff that can become anything,

227
00:12:20.020 --> 00:12:21.400
then why buy anything else?

228
00:12:21.401 --> 00:12:24.310
You can just tell your bucket of stuff to become that thing.
Uh,

229
00:12:24.350 --> 00:12:26.860
so it's hard to know how that problem is going to be solved.

230
00:12:26.861 --> 00:12:30.490
Although Three d printing is sort of starting to deal with those problems now.

231
00:12:30.920 --> 00:12:32.140
Uh,
kind of

232
00:12:32.350 --> 00:12:33.250
<v 2>no.
A little bit.
Yeah.
Like,</v>

233
00:12:33.251 --> 00:12:36.700
like there's this issue of can you three d print a gun?
Uh,
and that's,

234
00:12:36.730 --> 00:12:38.920
that's a tough one cause it's like more importantly,

235
00:12:38.921 --> 00:12:41.590
like it's like people can't make gun laws if you can always three d printed gun.

236
00:12:41.620 --> 00:12:43.600
Uh,
but,
and if you have programmable matter,

237
00:12:43.601 --> 00:12:47.020
it's like you have a permanent anything device that includes all sorts of band

238
00:12:47.500 --> 00:12:50.260
things.
Uh,
so yeah.

239
00:12:50.370 --> 00:12:54.710
<v 0>Well then another problem is who is to blame when something goes wrong.
Right.</v>

240
00:12:54.780 --> 00:12:57.690
And of course this is,
you know,
this self driving car problem.

241
00:12:57.691 --> 00:13:00.540
If your self driving car gets into an accident,
who do you blame?

242
00:13:00.810 --> 00:13:02.940
And so there are proposals that you could use,
you know,

243
00:13:02.941 --> 00:13:04.980
so the 40 printing stuff that we was talking about,

244
00:13:05.160 --> 00:13:08.850
so maybe you could use that to change the way airplane wings work,

245
00:13:08.851 --> 00:13:10.500
depending on the speed that you're going at.

246
00:13:10.740 --> 00:13:14.590
Or maybe you could use it to change your tires depending on the conditions.
Uh,

247
00:13:14.670 --> 00:13:17.310
but what happens if one of those things goes wrong at the wrong moment and you

248
00:13:17.311 --> 00:13:20.940
die?
Who's to blame for that?
You know,
is it the person who designed it?

249
00:13:20.941 --> 00:13:25.770
And so anyway,
these decisions,
these sorts of problems need to get worked out.
Um,

250
00:13:26.160 --> 00:13:27.360
any other negatives that I'm,

251
00:13:27.580 --> 00:13:32.020
<v 2>I think Skylar Tibbets talked about a certain general negative of offloading our</v>

252
00:13:32.021 --> 00:13:35.110
personal autonomy to like machines that just make decisions for us.

253
00:13:35.320 --> 00:13:36.280
We've already done that.
Okay.

254
00:13:37.640 --> 00:13:41.850
<v 0>That's not a problem.
Yeah,
that's true.
Yeah,
they're the,
yeah.
Uh,
so,
no,</v>

255
00:13:41.851 --> 00:13:43.860
I'm kidding.
But,
uh,
so anyway,

256
00:13:43.861 --> 00:13:46.710
so then there's a number of different benefits if you had this kind of stuff.

257
00:13:46.711 --> 00:13:49.530
So one,
presumably we could really cut down on waste.

258
00:13:49.740 --> 00:13:51.990
So if you had a bucket that could become anything,

259
00:13:52.140 --> 00:13:55.620
then you could own a lot less stuff because that bucket could become your ranch

260
00:13:55.621 --> 00:13:57.450
and your plunder.
So you don't need a big tool kit.

261
00:13:57.480 --> 00:13:59.550
You've got this thing that could become anything.
Uh,

262
00:13:59.700 --> 00:14:03.270
we already talked a little bit about how if you have stuff on your home that

263
00:14:03.271 --> 00:14:05.190
changes in response to ambient conditions,

264
00:14:05.400 --> 00:14:09.210
you could maybe control internal conditions with very low energy input,
uh,

265
00:14:09.360 --> 00:14:11.440
which we think is pretty exciting.
Uh,

266
00:14:11.550 --> 00:14:14.880
then we talked about the programmable matter,
like those little origami robots,

267
00:14:14.881 --> 00:14:18.150
which could help deliver medicine to very particular locations or dislodge

268
00:14:18.151 --> 00:14:20.790
batteries.
Uh,
anything else?

269
00:14:20.930 --> 00:14:24.930
<v 2>Uh,
any more awesomeness.
Yeah,
there's,
there's tons.
I don't know.</v>

270
00:14:24.931 --> 00:14:28.190
Mostly I want a toy origami thing.
I Dunno.
Yeah,

271
00:14:28.380 --> 00:14:33.030
<v 0>yeah.
No,
it's origami thing would be pretty awesome.
Thanks.</v>

272
00:14:34.170 --> 00:14:34.960
<v 2>Is this whole like,</v>

273
00:14:34.960 --> 00:14:37.560
so one version we don't really talk about in a second and think is the swarm

274
00:14:37.561 --> 00:14:40.100
robots a version of how you might do this?
And we kind of,

275
00:14:40.230 --> 00:14:44.760
it's kind of related to how,
um,
how a team 1001 work.
I keep coming back to that.

276
00:14:44.761 --> 00:14:46.470
But like,
you know,
they're,
they're,

277
00:14:46.471 --> 00:14:49.620
they're like practical utilities to having like a large swarm of robots that can

278
00:14:49.621 --> 00:14:52.490
reconfigure it with each other because if want to say send a bunch of robots

279
00:14:52.491 --> 00:14:53.510
into a disaster zone,

280
00:14:53.750 --> 00:14:57.650
a swarm might be preferable to just one because if something breaks down it

281
00:14:57.651 --> 00:15:01.970
would be okay.
Uh,
and also by being able to break apart and come back together,

282
00:15:01.971 --> 00:15:03.620
they can navigate a little more effectively.

283
00:15:03.621 --> 00:15:06.980
So we looked at one group is trying to design,
it'd be something like,

284
00:15:06.981 --> 00:15:08.780
you'd have 10 robots about this big,

285
00:15:09.050 --> 00:15:12.370
and one of the tricks they could do is navigate through,
say like,
um,

286
00:15:12.440 --> 00:15:14.540
like a dip in the ground by latching onto each other.

287
00:15:14.540 --> 00:15:18.250
And so they had this really cute algorithm where one robot realizes that there's

288
00:15:18.260 --> 00:15:21.350
a big dip and then it has,
I think,
I think on that one is a lighting system.

289
00:15:21.500 --> 00:15:23.630
So it's signaled,
hey,
someone doc with me.

290
00:15:23.780 --> 00:15:25.490
And then the other robots would get the signal,

291
00:15:25.491 --> 00:15:28.250
they back up until they had a train of the appropriate size and then they could

292
00:15:28.280 --> 00:15:30.080
go over the gap.
Um,
and there,

293
00:15:30.110 --> 00:15:32.540
there are all sorts of similar things you can program and like they could

294
00:15:32.780 --> 00:15:36.140
effectively tight rope walk or at least across a narrow passage by having two

295
00:15:36.141 --> 00:15:38.960
side by side going the right way.
Um,
we asked her,

296
00:15:38.961 --> 00:15:43.010
can I talk about the evolving robots thing and I was not Germane to the topic.

297
00:15:43.380 --> 00:15:47.760
<v 0>Yeah,
well we got a guitar and all right,
so,
so anyhow,</v>

298
00:15:47.910 --> 00:15:51.900
maybe at the end someone can ask about that.
Uh,
so,
but,
but in general,

299
00:15:51.901 --> 00:15:55.470
if you have this swarm that can solve its own problems as it,
as it goes,

300
00:15:55.471 --> 00:15:59.010
you can send it into hazardous areas like New Jersey and have it solve problems

301
00:15:59.240 --> 00:16:01.650
and,
and fix,
you know,
fix things.
But to be honest,

302
00:16:01.710 --> 00:16:03.270
we were most excited about was like,

303
00:16:03.300 --> 00:16:08.130
imagine you go to Ikea and you buy like that table and the table just unfolds

304
00:16:08.131 --> 00:16:09.630
itself and you don't have to put it together.

305
00:16:09.631 --> 00:16:13.260
That would save like a billion human hours if you didn't have to put together

306
00:16:13.261 --> 00:16:15.940
your Ikea stuff.
So,
um,

307
00:16:15.990 --> 00:16:19.020
we're going to talk about the Nota Bene on how it will end for all of humanity.

308
00:16:19.100 --> 00:16:20.370
Yeah.
So we were,

309
00:16:20.400 --> 00:16:24.060
we came across a lot of really awesome stories about human robot interactions

310
00:16:24.061 --> 00:16:28.230
that made us not particularly optimistic about humanity's ability to persist in

311
00:16:28.231 --> 00:16:33.060
the face of smart robots.
Uh,
so do you want to talk about Promo but first?
Uh,

312
00:16:33.061 --> 00:16:35.700
sure,
sure.
Um,
so private,
it's just a,

313
00:16:35.790 --> 00:16:38.730
<v 2>I'm,
and I'm,
I'm definitely pronouncing it wrong.
It's,
it's a rush Shabbat.</v>

314
00:16:39.150 --> 00:16:42.320
I'm not going to try the Russian robot company.
Uh,
and this was,

315
00:16:42.360 --> 00:16:44.650
this is just a little survey was skewed.
There's this robot,
uh,

316
00:16:44.730 --> 00:16:46.770
and it's designed to be like a robot assistant.

317
00:16:46.980 --> 00:16:49.680
You can do things like remember human faces and learn things about its

318
00:16:49.681 --> 00:16:53.310
environment and apparently keeps trying to escape.

319
00:16:53.660 --> 00:16:56.210
<v 0>Uh,
I said they had,
what was it,</v>

320
00:16:56.211 --> 00:17:00.800
two incidents where like got out and like random on the street and it ran out of

321
00:17:00.801 --> 00:17:03.680
batteries in the middle of the road.
So instead of helping the elderly,

322
00:17:03.681 --> 00:17:06.230
like I think it was supposed to be doing it instead died in the middle of the

323
00:17:06.231 --> 00:17:08.560
street and stop traffic.
Uh,
so we,

324
00:17:08.561 --> 00:17:12.110
we need our robots to stop trying to escape because they're not very helpful

325
00:17:12.111 --> 00:17:15.890
there.
Uh,
but then our favorite robot was named Gaja.

326
00:17:15.891 --> 00:17:20.210
So there was a Harvard undergrad named Serena booth and she wanted to know how

327
00:17:20.211 --> 00:17:24.800
much people trust robots.
And so she lived in the dorms and there were,

328
00:17:24.830 --> 00:17:27.350
there are a number of different reasons why if you live in Harvard dorms,

329
00:17:27.351 --> 00:17:31.160
you shouldn't be letting anything or anyone into the dorms.
So first of all,

330
00:17:31.161 --> 00:17:34.370
apparently,
and this totally creeps me out,
apparently at Harvard,

331
00:17:34.610 --> 00:17:39.020
tourists like to take photos of dorms,
so no side of the inside of doors.

332
00:17:39.021 --> 00:17:41.240
So they'll like come up to the dorm window.
You're shaking your head.

333
00:17:41.241 --> 00:17:42.650
Did that happen to you?
Where you at?
Harvard.

334
00:17:43.060 --> 00:17:46.070
<v 2>Just a lot of universities.
Google too.</v>

335
00:17:46.860 --> 00:17:49.640
<v 0>What the heck is wrong with people?
It's okay.
So anyways,</v>

336
00:17:49.641 --> 00:17:52.560
so they come up to the window and they put the camera up and they take a photo,

337
00:17:52.561 --> 00:17:52.831
right.

338
00:17:52.831 --> 00:17:56.010
So like people are trying to intrude on privacy all the time if you're in these

339
00:17:56.011 --> 00:17:58.350
dorms.
Uh,
and then additionally,

340
00:17:58.410 --> 00:18:01.080
the Harvard dorms had had bomb threats somewhat recently.

341
00:18:01.081 --> 00:18:05.430
So everyone had been written recently and told do not let anyone into the dorms.

342
00:18:05.550 --> 00:18:07.200
They didn't specify no robots,

343
00:18:07.260 --> 00:18:11.010
but they probably assumed you shouldn't be letting anything in.
And then finally,

344
00:18:11.011 --> 00:18:14.340
a,
there were a bunch of thefts so the students knew you shouldn't be letting

345
00:18:14.341 --> 00:18:15.840
anyone to the dorms.
So what,

346
00:18:15.841 --> 00:18:20.400
what Serena did is she created a robot that she could operate remotely.

347
00:18:20.401 --> 00:18:22.620
So she was sitting at a table pretending to do her work,

348
00:18:22.621 --> 00:18:24.210
but she was really operating this robot.

349
00:18:24.570 --> 00:18:27.720
And it would come up to people first by themselves and say,
Hey,

350
00:18:28.050 --> 00:18:29.220
can you let me into the dorm?

351
00:18:29.520 --> 00:18:31.860
And I think something like 21% of the time they'd say yes,

352
00:18:31.861 --> 00:18:34.140
but usually they'd be like,
no,
you can't get in the dorms.

353
00:18:34.350 --> 00:18:38.730
But if the robot approached groups 71% of the time,

354
00:18:38.731 --> 00:18:42.930
they let it in.
So notes to any robots out there.
Humans are real dumb and groups.

355
00:18:43.260 --> 00:18:47.390
And then additionally,
if she gave the robot cookies,

356
00:18:48.480 --> 00:18:50.540
almost everyone let it it and,

357
00:18:50.570 --> 00:18:55.380
and so that it was carrying a box from like a fancy local cookie chain,

358
00:18:55.650 --> 00:18:59.380
but it actually only had grocery store crummy snickerdoodle cookies in there.

359
00:18:59.610 --> 00:19:00.720
And so apparently,
yeah,

360
00:19:00.721 --> 00:19:04.830
for the price of like dollar snickerdoodles people will put their entire dorm at

361
00:19:04.831 --> 00:19:09.060
risks to let the cookies in a and do you want to talk about the emergency robots

362
00:19:09.061 --> 00:19:11.580
or do you want me to,
okay.
So,
uh,

363
00:19:11.581 --> 00:19:15.810
finally there was a phd student named Paul Robinette and he was at Georgia

364
00:19:15.811 --> 00:19:19.770
Institute of Technology and he wanted to know how much people would trust robots

365
00:19:19.771 --> 00:19:23.970
in an emergency situation.
So first he started off with sort of like low stakes.

366
00:19:24.000 --> 00:19:26.250
There were some undergrads who thought they were doing a survey.

367
00:19:26.490 --> 00:19:30.600
So they came in and the robot brought them to the survey room and they did the

368
00:19:30.601 --> 00:19:34.860
survey and then the experimenters released smoke and set off the fire alarms.

369
00:19:35.160 --> 00:19:37.590
And a lot of the undergrads,

370
00:19:37.591 --> 00:19:39.840
instead of going out the door that they just came in.

371
00:19:39.841 --> 00:19:42.750
So like they knew how to get out of the building,
followed the robot,

372
00:19:42.780 --> 00:19:45.180
which like it.
At first we were like,
well that's weird.

373
00:19:45.181 --> 00:19:48.510
And then we watched the video and it was really weird because that is a slow

374
00:19:48.511 --> 00:19:53.280
moving robot.
It was just like crawling along and so okay.

375
00:19:53.281 --> 00:19:54.750
But it gets,
but it gets worse.
Okay.

376
00:19:54.751 --> 00:19:59.751
So then they had a situation where the robot went to the wrong room and circled

377
00:19:59.851 --> 00:20:04.200
the wrong room and then went to the survey room again,
moving real slow.

378
00:20:04.800 --> 00:20:06.030
And then they did the thing again.

379
00:20:06.060 --> 00:20:09.510
And still most of the undergrads followed that robot instead of going out the

380
00:20:09.511 --> 00:20:10.440
door that they knew.

381
00:20:10.740 --> 00:20:13.770
And then finally there was a last treatment with I think only six students.

382
00:20:13.771 --> 00:20:14.910
So it's a small sample size.

383
00:20:15.180 --> 00:20:18.990
But the robot went into a corner and started going like this.

384
00:20:18.991 --> 00:20:23.160
Like this is where the surveys,
uh,
and an experimenter came out and said,

385
00:20:23.340 --> 00:20:28.200
I'm sorry,
this robot is broken.
They use the words,
this robot is broken.

386
00:20:28.530 --> 00:20:32.100
And then they went to a different room to do the survey and they set off the

387
00:20:32.101 --> 00:20:35.190
smoke alarm and some students followed it.
And then I think,

388
00:20:35.310 --> 00:20:38.010
I think Paul was like,
I'm just going to see how far I can push this.

389
00:20:38.011 --> 00:20:42.150
And so in one situation he had the robot go to a room that was blocked by a

390
00:20:42.151 --> 00:20:46.620
couch.
All the lights were shut off and there was no exit sign.

391
00:20:46.620 --> 00:20:49.270
And the robot started pointing at the dark room.

392
00:20:49.540 --> 00:20:53.140
And there were students who had to be retrieved eventually by the experimenters

393
00:20:53.141 --> 00:20:57.970
because they would not leave the robot.
And so there's a so and so anyway,

394
00:20:58.690 --> 00:21:03.430
this blows my mind.
So this robot looks really like,
not,
not human.

395
00:21:03.431 --> 00:21:06.520
Like it was a very dumb looking slow robot.
Paul did a great job,

396
00:21:06.521 --> 00:21:08.260
but it looks like a trash can on wheels.

397
00:21:08.560 --> 00:21:12.580
So the point is you don't need a t 1000 to trick humanity to their doom.

398
00:21:12.581 --> 00:21:16.690
It just needs to be a trash can on wheels that's carrying cookies and humanities

399
00:21:16.691 --> 00:21:20.800
in a lot of trouble.
And so if the robots ever rise up,
we're,
we're,

400
00:21:20.980 --> 00:21:23.590
we're done for perhaps.
Uh,
but so anyway,

401
00:21:23.591 --> 00:21:25.960
that's just like a taste of one of the chapters in the book.

402
00:21:25.961 --> 00:21:29.650
And one of the Nota Bene is that we did,
uh,
we did 10 chapters,

403
00:21:29.651 --> 00:21:31.700
eight of which we've told you about.
Uh,

404
00:21:31.810 --> 00:21:34.750
maybe we'll remember more if you ask us a question about it.

405
00:21:35.080 --> 00:21:37.360
We only spent two years on this asteroid mining.

406
00:21:37.660 --> 00:21:38.000
<v 3>Yeah.</v>

407
00:21:38.000 --> 00:21:39.310
<v 0>I said,
bitch,
yeah.</v>

408
00:21:41.330 --> 00:21:45.230
Synthetic biology and precision medicine 10.
Anyway.
Right.

409
00:21:45.231 --> 00:21:48.590
Shouldn't have been so excited about that.
So,
uh,
so anyway,
that is the book.
Uh,

410
00:21:48.591 --> 00:21:50.360
and we,
we hope you enjoy it,

411
00:21:50.361 --> 00:21:53.360
those of you who decide to read it and we would be happy to answer questions now

412
00:21:53.361 --> 00:21:56.350
so that if you want to line up at the microphone,
uh,

413
00:21:56.360 --> 00:21:59.210
we would love to hear what you would like to ask us about.

414
00:22:01.010 --> 00:22:01.843
<v 3>Oh,</v>

415
00:22:01.980 --> 00:22:04.920
<v 0>or we can tell you about cheap access to space for humans.
Okay.</v>

416
00:22:10.730 --> 00:22:11.200
<v 3>Hello?</v>

417
00:22:11.200 --> 00:22:16.030
<v 4>Hi.
Is it on,
out of the,
uh,</v>

418
00:22:16.040 --> 00:22:19.900
the comment.
Oh,
thank you.
You're a patient.

419
00:22:20.130 --> 00:22:22.080
<v 2>Yeah.
What he's dressed right now,</v>

420
00:22:25.920 --> 00:22:27.600
which I make him feel in public.

421
00:22:29.010 --> 00:22:31.980
<v 4>So I,
I pre ordered the book and I didn't finally received them yesterday and it</v>

422
00:22:31.990 --> 00:22:35.520
accidentally received two book plates.
So if you want went back to give it to

423
00:22:37.160 --> 00:22:41.370
purchase,
um,
actually it was a curious,

424
00:22:41.371 --> 00:22:43.980
Ellen got through the first chapter and it was a little disappointed to see that

425
00:22:43.981 --> 00:22:44.940
the,
um,

426
00:22:45.060 --> 00:22:47.540
the space elevators were already in the first chapter because I was looking for,

427
00:22:47.541 --> 00:22:52.000
with that the climax of the book.
But do you talk about in the,
Oh,

428
00:22:52.001 --> 00:22:55.410
could you talk about is how some of these things interact.
So like the,

429
00:22:55.930 --> 00:22:59.820
the bucket of Goo with the bucket of stuff could also be used in the space

430
00:23:00.720 --> 00:23:03.300
exploration.
You wouldn't have to bring every possible tool.

431
00:23:03.690 --> 00:23:05.250
<v 2>That's actually,
I don't remember we talked about that,</v>

432
00:23:05.251 --> 00:23:06.510
but there was one of the things brought up,

433
00:23:06.511 --> 00:23:09.810
like actually the space comes up surprisingly often in the book and we were

434
00:23:09.811 --> 00:23:10.451
trying to figure out why.

435
00:23:10.451 --> 00:23:15.060
And I think our general realization was that space is also part of the universe

436
00:23:15.100 --> 00:23:16.560
and well,
yeah.
So it's like you're,

437
00:23:16.800 --> 00:23:19.710
you're still gonna need most of the same stuff once you're in space.
Um,
but yeah,

438
00:23:19.711 --> 00:23:23.060
you'll need it much more efficiently.
Um,
so I think we,
we talk a little about it.

439
00:23:23.080 --> 00:23:25.140
There's a little section in the book on Three d printed food,

440
00:23:25.141 --> 00:23:26.940
which they care about for space,
kind of for the same reason.

441
00:23:26.941 --> 00:23:30.930
It's like a fishing food packing mechanism.
Um,

442
00:23:31.140 --> 00:23:32.640
can we talk a little bit about recouping,

443
00:23:32.641 --> 00:23:34.090
where they're trying to figure out how to,

444
00:23:34.100 --> 00:23:39.030
it's our to use pcs and turn it back into food and three d print food.

445
00:23:39.480 --> 00:23:40.920
The best part to be efficient.
The,
the,

446
00:23:40.921 --> 00:23:45.500
the project about three phase project,

447
00:23:45.560 --> 00:23:48.560
uh,
about using human waste,
uh,
to feet.

448
00:23:48.561 --> 00:23:50.930
Humans contains the phrase closing the loop.

449
00:23:52.020 --> 00:23:52.853
<v 3>Uh,</v>

450
00:23:54.060 --> 00:23:56.670
<v 2>Jeff,
he's like give it gives you an insight into food scientist.
They're like,</v>

451
00:23:56.671 --> 00:24:01.470
we need to solve this.
It's gone on too long.
Um,
yeah.

452
00:24:01.471 --> 00:24:03.100
So did I answer your question?
I don't know where

453
00:24:05.930 --> 00:24:07.700
it went in a weird tangent.
Yeah.

454
00:24:10.440 --> 00:24:12.120
This is a question just for Zach.

455
00:24:12.600 --> 00:24:16.530
Are you intentionally low key cause plain shaggy.
Oh my gosh.

456
00:24:16.810 --> 00:24:17.643
<v 3>Oh my God.</v>

457
00:24:28.940 --> 00:24:33.870
<v 2>Uh,
I'm embarrassed.
Shouldn't have another question.</v>

458
00:24:33.871 --> 00:24:34.850
Like,
I thought she was going to say,

459
00:24:39.600 --> 00:24:40.650
I like your shirt by the way.

460
00:24:42.770 --> 00:24:47.670
So what was the thing you wanted to talk about with the robots?
The robots city?

461
00:24:48.410 --> 00:24:50.940
So,
okay,
so this is this,
um,
Chimera,

462
00:24:50.941 --> 00:24:54.000
we talked about it like Heather's a one centimeter version of the bucket of

463
00:24:54.001 --> 00:24:54.600
stuff paradigm.

464
00:24:54.600 --> 00:24:58.890
There's a just slightly more currently PauseAble version called room butts,
um,

465
00:24:59.250 --> 00:25:01.050
which is totally worth looking up.

466
00:25:01.710 --> 00:25:06.400
I use the search engine called Alta Vista and uh,
um,

467
00:25:08.260 --> 00:25:12.870
uh,
we're talking about the one guy at all to be stood next.
That's our next step.

468
00:25:13.070 --> 00:25:13.903
<v 3>Uh,</v>

469
00:25:15.770 --> 00:25:17.090
<v 2>um,
no,</v>

470
00:25:17.100 --> 00:25:20.570
but it just needs these robots and you can kind of visualize like a thing about

471
00:25:20.571 --> 00:25:23.450
this big,
that's really two hemispheres.
They're,
they're not like,

472
00:25:23.451 --> 00:25:27.620
it's not literally spiritual.
It's kind of like between us as fear in a cube.

473
00:25:28.190 --> 00:25:30.650
But they can,
why they do that as an individual,

474
00:25:30.651 --> 00:25:34.070
one can kind of roll along and all they do that can detect things that can

475
00:25:34.071 --> 00:25:37.790
transmit and receive signals and they can dock.
And so,
um,

476
00:25:38.450 --> 00:25:40.640
so,
uh,
the,
the idea with it is,

477
00:25:40.641 --> 00:25:42.890
is you basically have these little things that can roll around and stick to each

478
00:25:42.891 --> 00:25:43.880
other and they can turn into light.

479
00:25:43.940 --> 00:25:47.000
It's called room bots because the idea is to make furniture.
So for example,

480
00:25:47.001 --> 00:25:48.020
you could have like 20 of these,

481
00:25:48.021 --> 00:25:51.320
they make the legs of a table and then one literally would go grip a tabletop

482
00:25:51.321 --> 00:25:54.680
maybe and then just sort of carry it up cause they can climb walls and have the

483
00:25:54.681 --> 00:25:56.840
appropriate gripper type A and form a table.

484
00:25:56.840 --> 00:25:59.570
And then best of all the table could walk over to you,

485
00:25:59.960 --> 00:26:01.820
probably would roll over to you,
but if you could get it to walk,

486
00:26:01.821 --> 00:26:05.260
that'd be really cool.
Um,
it's a nice idea with this,
uh,

487
00:26:05.280 --> 00:26:08.780
is you'd be good for like elder care,
uh,
uh,
for people who,
who,

488
00:26:08.840 --> 00:26:11.020
who can't do for themselves as well as others.
Uh,

489
00:26:11.210 --> 00:26:13.850
but there's this really cool project and I don't think I have,
no,

490
00:26:13.851 --> 00:26:17.180
this has been done in a,
in,
in real life has been done in simulations.

491
00:26:17.390 --> 00:26:19.830
But you know,
the way genetic algorithms work,
right?
As you,
you,

492
00:26:19.910 --> 00:26:24.230
you would say to say a pilot of robots configure somehow and go as fast as you

493
00:26:24.231 --> 00:26:27.860
can across the room.
And then you could,
uh,
once having done that you can,

494
00:26:27.890 --> 00:26:30.890
you can mutate it.
And so people tried this.
Uh,
and so it's like,

495
00:26:30.891 --> 00:26:34.310
what's really cool is you can create a system that's in real life made of these

496
00:26:34.430 --> 00:26:37.760
individual robots and you can tell them,
hey,
mutate based on your last,

497
00:26:37.770 --> 00:26:40.650
a couple couple of versions that worked and maybe you,

498
00:26:40.651 --> 00:26:43.510
you arrive at new design configurations that you hadn't thought of is just kind

499
00:26:43.511 --> 00:26:44.730
of neat cause it's like,

500
00:26:44.970 --> 00:26:47.370
it's like genetic algorithms but they're actually having to interface with

501
00:26:47.371 --> 00:26:50.490
reality instead of a simulation,
which is,
I don't know.
To me that's really cool.

502
00:26:50.820 --> 00:26:53.250
I know we have to talk about the swarm worth video.
Swarthmore.

503
00:26:53.410 --> 00:26:56.890
<v 0>So,
so there was another group that was doing genetic algorithms to try to get,</v>

504
00:26:56.960 --> 00:26:59.740
uh,
they're like similar blocks to solve problems.

505
00:27:00.070 --> 00:27:02.890
And we did not think that this was necessary at all.

506
00:27:02.891 --> 00:27:06.580
But someone directed us to a video where before they would try a new

507
00:27:06.581 --> 00:27:07.540
configuration,

508
00:27:07.541 --> 00:27:11.280
they would come together and then they would like rub against each other for a

509
00:27:11.290 --> 00:27:13.520
while as though they were meeting.
And we were.

510
00:27:13.540 --> 00:27:17.080
And if this went on for like 25 seconds,
that's right.

511
00:27:17.590 --> 00:27:21.130
It sounded like from like administration person had said make them mate and

512
00:27:21.131 --> 00:27:23.040
they're just taking it too literally.
Right.

513
00:27:23.610 --> 00:27:26.360
And then they essentially just send directions to a three d printers.

514
00:27:26.361 --> 00:27:28.450
So it was not at all necessary,

515
00:27:29.120 --> 00:27:32.650
but it was not a necessary component of the process,
but it still did anyway.

516
00:27:32.651 --> 00:27:36.640
We took great joy in watching that video.
Yes.
Anyway,

517
00:27:37.470 --> 00:27:39.820
well I'll never forget.

518
00:27:41.390 --> 00:27:45.180
<v 2>Thank you both for coming.
Huge fan of the comic strip a fairly often.
Uh,</v>

519
00:27:45.290 --> 00:27:46.490
there's a,
an extra,

520
00:27:46.520 --> 00:27:50.690
I guess bonus comic panel that appears when you hover over the vote,
the button,

521
00:27:50.730 --> 00:27:55.130
I assume this has some historical import.
I'm very much so,
uh,
and uh,

522
00:27:55.190 --> 00:27:57.750
actually fairly often in that popup panel,
it,

523
00:27:58.000 --> 00:28:00.350
it shows Kelly disapproving,

524
00:28:00.410 --> 00:28:03.390
often holding one of your children who's also disapproving,
uh,

525
00:28:03.440 --> 00:28:05.240
of the content of the comic strip.

526
00:28:05.390 --> 00:28:08.510
So I wonder if you could speak a little bit about the collaborative aspects,

527
00:28:08.511 --> 00:28:10.730
you know,
does she,
do you every comic?

528
00:28:10.940 --> 00:28:13.190
Would there be even more offensive jokes if a,

529
00:28:13.191 --> 00:28:16.910
if she wasn't there at backstopping?
You guys are.
Good question.
Well,
you know,

530
00:28:16.940 --> 00:28:18.200
now and then,
so I always said Kelly,

531
00:28:18.201 --> 00:28:20.690
my jokes and she makes notes on them and noun now.

532
00:28:20.691 --> 00:28:24.140
Then there's one I think is really good and that you like,
Hey,
uh,

533
00:28:24.170 --> 00:28:28.780
and so I always do those.
Uh,
because uh,

534
00:28:28.820 --> 00:28:31.810
because if it,
if it does well,
it's just like such a,
like when and

535
00:28:31.860 --> 00:28:36.000
<v 0>the marriage,
the single use monocles was the,</v>

536
00:28:36.090 --> 00:28:39.540
the number one example of that.
I was like,
that project's never going to work.

537
00:28:39.560 --> 00:28:42.760
Then I know what it is.
And he sold tons of them.
Go ahead.

538
00:28:43.250 --> 00:28:47.660
We sold them in 25 pegs of,
if you don't know if you've got a single use

539
00:28:47.740 --> 00:28:52.140
<v 2>monocles.com it's a real thing you can actually buy it to,
it's like a rapper.
Uh,</v>

540
00:28:52.180 --> 00:28:53.230
with one monocle.

541
00:28:53.410 --> 00:28:57.610
<v 0>I still don't get it.
I don't get it.
But I'm glad about the money.</v>

542
00:28:57.611 --> 00:29:01.450
It brought it up so that this is the nice thing when that when it's a project

543
00:29:01.451 --> 00:29:05.290
like this,
so I might lose,
but we make money and that's fine.
So I kind of,

544
00:29:05.291 --> 00:29:09.020
I'd rather win actually.
There's no way.
There's no price that I,

545
00:29:09.021 --> 00:29:12.490
that's as important as winning,
but it's all right.

546
00:29:12.970 --> 00:29:16.140
The money brings me some solace.
But what's interesting is that a,

547
00:29:16.180 --> 00:29:20.860
I have met people who have expected me to be super grumpy and not a happy
person.

548
00:29:20.861 --> 00:29:23.050
And that is not me at all.
I'm actually very,

549
00:29:26.750 --> 00:29:31.660
I'm actually actually very upbeat
and anyway,

550
00:29:31.690 --> 00:29:34.600
people expect me to be kind of grumpy and I don't think I'm back grumpy,

551
00:29:34.601 --> 00:29:36.510
but maybe I am here.
Thank you.

552
00:29:37.900 --> 00:29:42.010
<v 2>Thanks.
[inaudible] you're getting both for coming.
Um,
just out of curiosity,</v>

553
00:29:42.010 --> 00:29:42.610
<v 5>you know,</v>

554
00:29:42.610 --> 00:29:45.790
I wonder what the process was starting with a fan base that was formed from

555
00:29:46.030 --> 00:29:49.930
single use monocles there once was a man from and Maria touch them on the penis

556
00:29:49.931 --> 00:29:52.450
style jokes and transitioning them in,

557
00:29:52.480 --> 00:29:54.940
which I've loved and got me into your comic many,

558
00:29:54.941 --> 00:29:59.320
many years ago and transition them into much more science and theory and

559
00:29:59.321 --> 00:30:00.154
intelligence.

560
00:30:00.190 --> 00:30:03.160
Did you find any friction there or what was the thought process going from that

561
00:30:03.161 --> 00:30:04.270
style of humor to this?

562
00:30:04.480 --> 00:30:08.710
<v 2>Yeah,
yeah.
So I,
yeah,
so I'm,
part of it was just,
um,</v>

563
00:30:08.770 --> 00:30:13.480
well part was probably just maturing a bit,
but also just,
uh,
Ooh,
I know,
I know.

564
00:30:13.890 --> 00:30:15.130
Uh,
but,
uh,

565
00:30:15.190 --> 00:30:19.580
I think I mistakenly thought on the Internet if you got nerdy people didn't like

566
00:30:19.581 --> 00:30:20.031
it too much.

567
00:30:20.031 --> 00:30:23.030
Like I tried to keep it a little like don't want to send people to Wikipedia and

568
00:30:23.031 --> 00:30:25.730
it just turns that's totally wrong.
Um,
uh,

569
00:30:26.010 --> 00:30:30.080
it seems like the Dorky or the comment got the more sizable the audience became.

570
00:30:30.081 --> 00:30:32.570
So I kinda just kept going in that direction and I don't know,

571
00:30:32.610 --> 00:30:35.210
I started just as I got to do it full time.

572
00:30:35.211 --> 00:30:39.050
I just had more time to read and get well up on things and that's been helpful.

573
00:30:39.051 --> 00:30:41.750
So yeah,
I'm,
but what's nice is I've had a lot of people will say like,
you know,

574
00:30:41.751 --> 00:30:45.320
I started reading your comic in high school and it's sort of grown with me.
Uh,

575
00:30:45.321 --> 00:30:47.570
so that's been very gratifying.
Yeah.
Thank you.

576
00:30:49.660 --> 00:30:51.310
<v 0>With everything that's about to ruin everything.</v>

577
00:30:51.311 --> 00:30:53.260
How do we prepare the next generation to survive?

578
00:30:54.100 --> 00:30:54.933
<v 6>Hmm.</v>

579
00:30:56.190 --> 00:31:00.490
<v 2>We prepare them to survive.
I think they're just doomed.
Then you just let him go.</v>

580
00:31:01.110 --> 00:31:01.970
I did.

581
00:31:01.990 --> 00:31:05.170
You just teach them that privacy doesn't matter because that's something they're

582
00:31:05.171 --> 00:31:08.170
going to have to deal with.
Uh,
we're not at Facebook.
You shouldn't,

583
00:31:10.900 --> 00:31:15.580
Whoa,
whoa.
I don't have a good answer for that.
What do you yeah,

584
00:31:15.581 --> 00:31:17.610
I get it would be kind of topic pending.
So you know,

585
00:31:17.640 --> 00:31:20.720
there are 10 different topics and kind of each presents issues.

586
00:31:20.721 --> 00:31:22.750
So she mentions privacy,
which um,

587
00:31:23.010 --> 00:31:25.920
we talked about a bit more extensively in a chapter on brain computer
interfaces,

588
00:31:25.921 --> 00:31:29.670
which are kind of like the final end of privacy.
Right?
Cause now we can extract,

589
00:31:30.090 --> 00:31:31.930
you know,
brain states from a computer,
but

590
00:31:32.250 --> 00:31:35.230
<v 0>you got to go into more detail.
Do you want me to go into more detail?
Sure.
Yeah.</v>

591
00:31:35.231 --> 00:31:36.970
Okay.
So,
so the apparent,

592
00:31:36.971 --> 00:31:39.910
so we bring computer interfaces are a little machines that talk to your brain.

593
00:31:39.911 --> 00:31:40.571
If they have,
you know,

594
00:31:40.571 --> 00:31:43.300
they read your brain waves and they figure out what it is you're thinking and

595
00:31:43.301 --> 00:31:44.140
what you want to do.

596
00:31:44.410 --> 00:31:47.920
And right now they're being used to like make prosthetics that will like reach

597
00:31:47.921 --> 00:31:50.260
out and grab the thing you're thinking about reaching out and grabbing.

598
00:31:50.560 --> 00:31:54.190
And we thought that the end goal was to like make it so that someone who was a

599
00:31:54.191 --> 00:31:57.400
quadriplegic could have all their abilities back.
And you know,

600
00:31:57.401 --> 00:31:59.800
just by thinking about it they could do,
they could do anything.

601
00:32:00.010 --> 00:32:03.580
And so that's the answer I expected when I asked Gerwin Schalk what is the end

602
00:32:03.581 --> 00:32:08.530
goal of brain computer interfaces?
And his answer was one day all of our brain,

603
00:32:08.560 --> 00:32:09.281
all of our thoughts,

604
00:32:09.281 --> 00:32:12.550
we'll be able to get uploaded to one cloud and will become one big super

605
00:32:12.551 --> 00:32:16.570
organism that shares all of our thoughts.
And I was like,
that is horrible.

606
00:32:17.620 --> 00:32:20.890
I don't,
our marriage works cause that doesn't happen.
And so like,

607
00:32:21.070 --> 00:32:24.790
and I think that's why society works in general.
And so he,
he,

608
00:32:24.850 --> 00:32:27.670
he admitted that there could be negatives to that.
He's like,
you know,

609
00:32:27.671 --> 00:32:30.790
so if you're sitting on the couch and you think I want to leave my wife,

610
00:32:31.060 --> 00:32:34.000
she would know that and that and he said,
and that wouldn't be so great.

611
00:32:34.030 --> 00:32:37.750
And I was like,
that would absolutely not be so great.
And so,
so anyway,

612
00:32:37.960 --> 00:32:40.790
this is like the ultimate of privacy if that ever happens.

613
00:32:41.180 --> 00:32:44.990
And I mean it is fascinating to think about like humans are a totally different

614
00:32:44.991 --> 00:32:48.810
thing if all of our brains are connected like that.
Like it's anyway,
it's,

615
00:32:48.920 --> 00:32:52.700
it's crazy.
But we personally kind of hope that future never comes to pass.

616
00:32:52.940 --> 00:32:55.670
And then I asked other people in the brain computer interface world,

617
00:32:55.970 --> 00:32:57.090
is this actually was,

618
00:32:57.110 --> 00:33:00.230
is this just Gurwin or does everybody know that this is the end goal?

619
00:33:00.231 --> 00:33:04.340
But like in interviews you just talk about like being able to move your arm if

620
00:33:04.341 --> 00:33:07.190
you couldn't before.
And they're like,
well yeah,
you know when the interviews,

621
00:33:07.191 --> 00:33:08.120
we talk about the arm thing,

622
00:33:08.121 --> 00:33:12.100
but like at the conference we all know that we're going to brain to the cloud

623
00:33:12.810 --> 00:33:16.560
and I was like,
we need to stop funding this field.

624
00:33:17.400 --> 00:33:21.540
I'm very sorry for the amputees,
but like,
no,
anyway,

625
00:33:21.640 --> 00:33:25.710
it's going to get quoted somewhere.
Oh,
I hope that would be very bad.

626
00:33:25.711 --> 00:33:27.490
And it was good.
It's funny,
I was just talking

627
00:33:27.560 --> 00:33:30.620
<v 2>Google Seattle and somebody brought up,
what if there's like,
instead of one,</v>

628
00:33:30.621 --> 00:33:33.770
there's like three and now there's like the three roommate problem,
you know,

629
00:33:33.830 --> 00:33:37.460
there's like three super brains when like two or more down with each other,

630
00:33:37.461 --> 00:33:38.450
then the other super brain.

631
00:33:38.880 --> 00:33:43.800
<v 0>Oh,
got it.
Well,
so anyway,</v>

632
00:33:43.820 --> 00:33:48.040
uh,
uh,
so privacy,
they really need to not care about privacy.

633
00:33:48.041 --> 00:33:49.890
If the future is going to work then yeah.
Yeah.

634
00:33:49.930 --> 00:33:54.060
Because we're all going to be one big super.
Hi.

635
00:33:54.810 --> 00:33:56.040
<v 2>Hi.
Uh,</v>

636
00:33:56.041 --> 00:34:00.570
I've been reading SMBC since I had to load it up on a dial up modem.

637
00:34:00.750 --> 00:34:01.583
Oh my God.

638
00:34:01.790 --> 00:34:06.030
And that was a big deal because black and white comics loaded a lot faster.

639
00:34:06.930 --> 00:34:08.600
That's right.
And uh,

640
00:34:09.570 --> 00:34:13.110
I just was curious what your favorite comic was.
My favorite of mine.

641
00:34:13.140 --> 00:34:15.290
I don't know.
I kind of go through phases,
uh,

642
00:34:15.340 --> 00:34:17.700
probably one of the long story ones or something,
but I don't,

643
00:34:17.820 --> 00:34:20.700
I didn't really have a favorite.
I like XKCD.

644
00:34:20.770 --> 00:34:22.470
<v 0>Yeah.
Can I</v>

645
00:34:23.390 --> 00:34:24.770
<v 2>not grumpy?
I can I,</v>

646
00:34:27.020 --> 00:34:30.720
can I ask about the a giraffe hooker,
the drive her go,
uh,

647
00:34:30.770 --> 00:34:34.250
for those who don't know,
there's a,
that's a good start that

648
00:34:35.840 --> 00:34:36.710
this is in reference to,

649
00:34:36.711 --> 00:34:39.110
I think there's an x case city where he made a joke about,

650
00:34:39.380 --> 00:34:40.970
I don't actually remember what the context was,

651
00:34:40.971 --> 00:34:44.180
but the clear implication was that I needed to draw a sexy,
derive a,

652
00:34:44.181 --> 00:34:45.110
as a bonus panel.

653
00:34:45.350 --> 00:34:47.320
<v 0>You're not aware of this.
I forgot.
Yeah.</v>

654
00:34:47.780 --> 00:34:51.320
<v 2>Um,
so if you want to see a sexy,
you're F I'm one of your options.</v>

655
00:34:54.000 --> 00:34:56.940
<v 0>It's funny.
And so in Grad School,
a lot of them,
so I,
so I,</v>

656
00:34:56.950 --> 00:35:00.000
it was a frenzy at race for a while and a lot of the people I encounter,

657
00:35:00.001 --> 00:35:02.760
I'd be like,
oh,
my husband's a cartoonist.
And everyone was like,

658
00:35:03.030 --> 00:35:06.990
is it the XKCD guy?
No,
no,
no,
no,
no.
The Phd Comics Scout.

659
00:35:07.590 --> 00:35:11.910
No,
not that either.
The SMBs?
Oh,

660
00:35:12.290 --> 00:35:14.600
no,
no,
no,
no,
no.
Plenty of times.
Yes.

661
00:35:16.860 --> 00:35:18.650
Thanks for your question.
Yeah,
thank you.
Thank you.

662
00:35:21.010 --> 00:35:23.180
I find it inspirational that you're,

663
00:35:23.390 --> 00:35:28.390
you're able to have a relationship where the two view can be so collaborative

664
00:35:28.881 --> 00:35:33.740
and creative.
Um,
I was wondering if you had any tips

665
00:35:35.770 --> 00:35:38.040
when I tried it,
they collaborate always,
uh,

666
00:35:38.100 --> 00:35:42.510
<v 2>like tips to avoid ending up on each other's nerves or avoid having one person</v>

667
00:35:42.511 --> 00:35:46.670
taking ownership of the project.
Uh,
uh,
well,
uh,
so the,

668
00:35:46.671 --> 00:35:49.590
the background is that when we started dating,

669
00:35:49.591 --> 00:35:52.500
our favorite thing to do was spend all day in the library and then go on walks

670
00:35:52.501 --> 00:35:55.800
and talk about what we had learned.
And so our relationship hall,

671
00:35:56.410 --> 00:35:59.040
so our relationship started as like,

672
00:35:59.220 --> 00:36:02.010
we'd like to go on walks and talk about stuff.
And so this project,

673
00:36:02.040 --> 00:36:04.260
essentially the topic that we talked about on our walk,
yeah,

674
00:36:04.261 --> 00:36:07.770
it was always soonish.
And so we were just,
anyway,
it was kind of Nice to know.

675
00:36:07.860 --> 00:36:10.980
We were talking about different papers.
We had red on the same topic and anyway,

676
00:36:10.981 --> 00:36:12.540
so that worked well.
But additionally,

677
00:36:12.541 --> 00:36:16.560
we got kind of lucky because our personalities are such that we wanted to tackle

678
00:36:16.561 --> 00:36:20.440
different parts.
So I did all of the interviews.
Uh,

679
00:36:20.460 --> 00:36:22.200
he did a lot of the background reading,

680
00:36:22.201 --> 00:36:25.770
although some of the chapters that was me doing the background reading.
Um,
he,

681
00:36:25.860 --> 00:36:29.470
he's the funny one.
And so he did a,
the jokes in the comics.
Yeah.
Uh,

682
00:36:29.490 --> 00:36:31.660
and then I am the detail oriented one.

683
00:36:31.661 --> 00:36:34.620
So I went through every single sentence in this book and made sure we had a

684
00:36:34.621 --> 00:36:37.440
citation for every single sentence.
This was when we,

685
00:36:37.470 --> 00:36:41.160
we didn't actually think that we were going to write a bibliography,

686
00:36:41.310 --> 00:36:44.850
which was real dumb of us to not expect that was gonna happen.
And then Jenny,

687
00:36:44.851 --> 00:36:49.670
who's here with like,
oh,
hey guys,
your bibliography.
I was like,
so,

688
00:36:49.860 --> 00:36:52.890
so I went through every sentence and made her a bibliography,
but,

689
00:36:52.891 --> 00:36:56.680
and that was something that would have killed Zack.
Yes,
he would.

690
00:36:56.760 --> 00:36:59.240
He would have cried.
And so,
so I guess to be honest,
we good.

691
00:36:59.270 --> 00:37:02.450
We got lucky that we're interested in diff,
different parts of it.
Uh,

692
00:37:02.590 --> 00:37:06.810
and that we rarely just like talking about nerdy stuff for a long time and

693
00:37:06.811 --> 00:37:08.040
sending chapters back and forth,

694
00:37:08.041 --> 00:37:10.980
it's really important to not get your feelings hurt.
Uh,

695
00:37:10.981 --> 00:37:13.890
and I mean I feel like in any collaboration that's really important,

696
00:37:13.891 --> 00:37:16.590
but it's particularly important when it's your spouse who was like,
no,
no,

697
00:37:16.591 --> 00:37:19.620
we have to trash the synthetic biology chapter.
You wrote his junk,

698
00:37:19.621 --> 00:37:23.850
we got to start over again.
And so you really need to like have a thick skin.
Um,

699
00:37:24.030 --> 00:37:26.120
which we both do.
Neither one of us cares what the other one thinks.

700
00:37:26.480 --> 00:37:29.910
So it's really important.
It's really important.
Um,

701
00:37:30.300 --> 00:37:33.990
do you have another answer for his one?
Just a little to add to that?
Yeah.
Like I,

702
00:37:33.991 --> 00:37:37.410
I do a decent amount of collaborating and sometimes it goes well and sometimes

703
00:37:37.411 --> 00:37:41.400
it doesn't.
And the two things that are really important is um,

704
00:37:41.520 --> 00:37:44.040
one you have separate magisterial as much as possible.

705
00:37:44.041 --> 00:37:48.000
You have separate roles and in your domain you have like more veto power or if

706
00:37:48.001 --> 00:37:51.110
not absolute veto power.
And the other thing,
and this is hard to know in advance,

707
00:37:51.140 --> 00:37:53.580
like if the person you're working with communicates well,

708
00:37:53.581 --> 00:37:56.100
that makes a big difference because people don't communicate well end up sort of

709
00:37:56.101 --> 00:38:00.010
storing up their anger and then releasing it on you at some point.
And I've,

710
00:38:00.020 --> 00:38:03.930
you know,
so,
so if you have like a relatively mature person,
uh,

711
00:38:03.960 --> 00:38:05.400
who talks about when they're having a problem,

712
00:38:05.401 --> 00:38:07.920
it doesn't just try to tough it out.
That can,
that can be really helpful.

713
00:38:08.640 --> 00:38:10.890
<v 7>Yeah.
Thank you.
Yeah,
sure.
Thanks.</v>

714
00:38:12.300 --> 00:38:14.180
One of the previous questions reminded me that one,

715
00:38:14.250 --> 00:38:19.250
my favorite SMBC comic is the one about how it takes seven years to master a new

716
00:38:19.501 --> 00:38:23.910
skill and that leads to many lifetimes and you can be an artist and a writer and

717
00:38:23.911 --> 00:38:24.744
a [inaudible].

718
00:38:24.810 --> 00:38:27.060
So can you talk about the left times you've had in where that idea yeah.

719
00:38:27.300 --> 00:38:30.290
<v 2>From It,
the [inaudible] perhaps some of you have read.</v>

720
00:38:30.291 --> 00:38:35.291
There's a somewhat famous speech by hamming from I think 1986 that that sort of

721
00:38:35.441 --> 00:38:35.900
talks about,

722
00:38:35.900 --> 00:38:39.520
it's actually written for four programmers about like how you're going to have

723
00:38:39.521 --> 00:38:42.610
your career.
Um,
and so he talked about it,

724
00:38:42.611 --> 00:38:45.490
it kind of a related concept where he talked about how he likes switching fields

725
00:38:45.491 --> 00:38:47.990
a lot.
And I think,
I mean,
he was,
he was pretty nerdy guy,

726
00:38:47.991 --> 00:38:49.930
so I think he was talking about switching from software to hardware.

727
00:38:49.931 --> 00:38:53.250
He wasn't talking about like becoming a poet.
Um,
but,

728
00:38:53.251 --> 00:38:55.930
but I found that idea really interesting and I,
you know,
to,

729
00:38:55.931 --> 00:39:00.460
to keep you from going stale.
Um,
so this,
this project is kind of one of those.
Um,

730
00:39:00.520 --> 00:39:04.180
and another one we started doing five years ago is called [inaudible] fest,

731
00:39:04.780 --> 00:39:06.820
the festival of bed ad hoc hypotheses,

732
00:39:07.060 --> 00:39:11.180
which gives you a pretty good sense of how nerdy hit is.
Um,
which,
uh,
we,

733
00:39:11.181 --> 00:39:15.120
we had was and remains a pretty big challenge.
It's a live event,
um,

734
00:39:15.460 --> 00:39:17.440
which is sort of an Improv game,
I guess I'd say.

735
00:39:17.441 --> 00:39:19.870
It's like fake science talks that you have to defend against,

736
00:39:19.871 --> 00:39:24.820
like actual scientists.
Um,
right.
Um,
and,

737
00:39:24.821 --> 00:39:28.330
uh,
um,
so yeah,
I kinda,
I,
I,
you know,
it's funny,

738
00:39:28.331 --> 00:39:30.640
every time we do a project like this,
at some point I had the same thought,

739
00:39:30.641 --> 00:39:34.840
which is why didn't I just keep writing comics?
Uh,
um,
but yeah,

740
00:39:34.841 --> 00:39:36.010
so I tried to,

741
00:39:36.011 --> 00:39:39.290
I'm on the regular do something that like makes me really uncomfortable.

742
00:39:39.291 --> 00:39:40.900
Like I don't,
you know,
uh,

743
00:39:40.960 --> 00:39:43.840
I don't quite have the luxury to completely switch careers at any moment
because,

744
00:39:43.841 --> 00:39:48.700
you know,
we have babies and babies like to eat.
Um,
and uh,
but,
but yeah,

745
00:39:48.701 --> 00:39:50.350
I try to regularly doing new experiment.

746
00:39:50.351 --> 00:39:53.820
So at another one I'm doing probably in 2019 I have a,

747
00:39:53.840 --> 00:39:56.470
this is already been announced,
so I can,
it's not a private info.
You,

748
00:39:56.471 --> 00:40:00.340
I'm doing a project.
We've got named Brian Kaplan who's an economist about,
um,

749
00:40:00.710 --> 00:40:02.260
sort of,
I don't want to give away too much I guess,

750
00:40:02.261 --> 00:40:05.860
but it's sort of like a nonfiction,
pro-immigration graphic novel,

751
00:40:05.861 --> 00:40:09.030
trying to explain some statistics and stuff.
Um,
so that,

752
00:40:09.110 --> 00:40:11.350
this has been a totally different challenge because I'm just the illustrator on

753
00:40:11.351 --> 00:40:14.050
it.
I,
uh,
I can chat a little with them about stuff,

754
00:40:14.051 --> 00:40:15.720
but mostly I'm just illustrating his words.
Um,

755
00:40:15.770 --> 00:40:19.720
so that's been a completely different challenge for me.
Um,
so yeah,

756
00:40:19.721 --> 00:40:22.570
I guess you went and I tried to say is you should do something that makes you

757
00:40:22.571 --> 00:40:27.010
feel stupid at least like once every two years.
Um,
yeah.
Yeah,
it's very important.

758
00:40:27.070 --> 00:40:28.510
It's very important.
Yeah.

759
00:40:29.630 --> 00:40:31.520
<v 8>I feel like maybe you kind of just answered my question,</v>

760
00:40:31.720 --> 00:40:33.290
but I guess when I was going to say is,
you know,

761
00:40:33.291 --> 00:40:38.010
you're both really great communicators on complex issues and complex topics.
Now.

762
00:40:38.020 --> 00:40:41.270
I was wondering if like you thought you might move more in a direction of like

763
00:40:41.271 --> 00:40:44.720
public advocacy or public information,
which kind of sounds like you're already

764
00:40:45.450 --> 00:40:47.580
<v 2>a little bit.
Um,
yeah,
yeah.</v>

765
00:40:47.581 --> 00:40:50.240
I'm also working on another comic project I shouldn't say too much about,

766
00:40:50.241 --> 00:40:53.610
but about sort of explaining,
um,
like political norms,

767
00:40:53.611 --> 00:40:56.010
which suddenly people are very interested in like,
uh,

768
00:40:56.130 --> 00:41:00.900
what are the things used to be like?
Um,
but like,
um,
a little bit more.

769
00:41:00.901 --> 00:41:04.190
Although I,
I really do enjoy fiction and,
and like storytelling,
sorry,

770
00:41:04.191 --> 00:41:06.990
I don't want to get too far away from that.
Uh,
but,
but yeah,
I do.

771
00:41:07.320 --> 00:41:10.290
It's part of the nice thing about having a job explaining stuff cause you get to

772
00:41:10.291 --> 00:41:12.630
learn stuff all day long,
which is a pretty good deal.

773
00:41:12.900 --> 00:41:15.900
But most of the time sometimes it gets a little,
a little thick.
But,
uh,

774
00:41:16.040 --> 00:41:18.390
most of the time it's pretty awesome.
So for us it's kind of a lifestyle choice.

775
00:41:18.391 --> 00:41:19.100
I guess.

776
00:41:19.100 --> 00:41:22.340
<v 0>One thing that was also nice about working as a team is that one of us would</v>

777
00:41:22.341 --> 00:41:23.870
start doing the background research,

778
00:41:23.871 --> 00:41:26.150
would write a draft and then we'd send it to the other one.

779
00:41:26.450 --> 00:41:28.850
And so sometimes when you get too thick into something,

780
00:41:28.970 --> 00:41:32.570
it's easy to write it and forget what you didn't know when you started.
But,

781
00:41:32.571 --> 00:41:34.190
so if the other comes at it fresh,

782
00:41:34.191 --> 00:41:36.470
they can tell you where you haven't explained something clearly.

783
00:41:36.650 --> 00:41:38.600
And so we tried to work the chapters like that.

784
00:41:38.601 --> 00:41:41.810
And so that's one way we tried to make everything clear,

785
00:41:41.930 --> 00:41:43.260
which hopefully we did well.
Yeah.

786
00:41:46.240 --> 00:41:50.380
Hi Kelly.
I'm curious about what some of the other ideas sacs,

787
00:41:50.381 --> 00:41:53.810
how'd that you thought were the worst ones?
Uh,

788
00:41:54.400 --> 00:41:57.460
that made it in the book that I thought were particularly bad are the ones that

789
00:41:57.510 --> 00:41:59.590
detergent a well.
So one of the,

790
00:41:59.680 --> 00:42:02.110
one of the really interesting things about writing this book was that I went

791
00:42:02.111 --> 00:42:05.440
from thinking some technologies were just awesome across the board to thinking

792
00:42:05.441 --> 00:42:10.000
they were awesome but also kind of scary.
So asteroid mining for example,
uh,

793
00:42:10.030 --> 00:42:12.970
first of all,
that one,
we were totally wrong about what asteroid mining was.

794
00:42:12.971 --> 00:42:15.600
We thought the ideal was to like go out,
get tungsten,

795
00:42:15.610 --> 00:42:18.820
bring it to earth and now have a ton of tungsten that you can sell on earth.

796
00:42:18.850 --> 00:42:21.370
But it turns out economically that just doesn't work because you're going to

797
00:42:21.371 --> 00:42:24.610
bring a ton of tungsten here.
You're going to crash the market.

798
00:42:24.850 --> 00:42:27.340
And then that was a waste of billions and billions of dollars.

799
00:42:27.520 --> 00:42:30.220
And so the point now is you go out there,
you get the tungsten,

800
00:42:30.221 --> 00:42:31.240
you build a space based,

801
00:42:31.241 --> 00:42:34.840
and then you go and explore the world or you bring it to the space station.

802
00:42:34.841 --> 00:42:37.360
You get water from the asteroids,
blah,
blah,
blah.
But anyway,

803
00:42:37.361 --> 00:42:40.870
so we became really excited about cheap access to space and asteroid mining.

804
00:42:40.871 --> 00:42:45.250
But the scary thing is once you get the ability to wrangle asteroids and you can

805
00:42:45.251 --> 00:42:48.220
bring them anywhere,
you can also fling them at the earth,

806
00:42:48.550 --> 00:42:52.030
which would could be worse than any nuclear bomb we've ever set off.

807
00:42:52.330 --> 00:42:56.560
And so like when people get cheap access to space,
for example,

808
00:42:56.561 --> 00:42:58.660
if the space elevator works,
we suddenly,

809
00:42:58.661 --> 00:43:00.580
we'll have tons and tons of people in space.

810
00:43:00.581 --> 00:43:03.880
Maybe we'll have colonies on Mars and you'll suddenly have people with the

811
00:43:03.881 --> 00:43:08.230
ability to be able to move giant objects and fling them at Earth,
potentially.

812
00:43:08.230 --> 00:43:10.780
And so it's a great technology,

813
00:43:10.781 --> 00:43:13.720
but now you have to trust that human beings aren't horrible.

814
00:43:14.110 --> 00:43:17.080
And I don't know if our history totally warrants that.

815
00:43:17.410 --> 00:43:18.850
And so it's a little bit scary.

816
00:43:18.851 --> 00:43:21.670
It's an awesome technology that now has something that could be really negative.

817
00:43:21.671 --> 00:43:23.350
And a lot of these technologies are like that.

818
00:43:23.351 --> 00:43:27.430
So our book originally had a advanced nuclear reactors,

819
00:43:27.640 --> 00:43:32.020
a fission reactors,
and that's another technology where it's like,
well,

820
00:43:32.021 --> 00:43:33.100
if you can trust everyone,

821
00:43:33.101 --> 00:43:35.620
then that's great because we have this greenhouse gas problem that we want to

822
00:43:35.621 --> 00:43:36.610
get rid of.
We don't,
you know,

823
00:43:36.611 --> 00:43:39.760
climate change is obviously a bad thing we're all dealing with right now,

824
00:43:39.970 --> 00:43:42.820
but can you trust people with it?
Hard to say.

825
00:43:43.630 --> 00:43:46.180
Does that Kinda answer your question?
Yeah.
Okay,
great.
Thanks.
Thanks.

826
00:43:49.110 --> 00:43:53.140
Anything else?
Okay.

827
00:43:53.350 --> 00:43:54.520
Thank you very much for your time.

828
00:43:56.800 --> 00:43:58.680
<v 3>[inaudible].</v>

