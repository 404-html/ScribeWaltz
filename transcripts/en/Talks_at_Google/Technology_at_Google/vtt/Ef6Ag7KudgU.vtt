WEBVTT

1
00:00:06.190 --> 00:00:07.950
I've been at Google nearly 10 years on,

2
00:00:08.020 --> 00:00:10.720
literally for every one of those 10 years,

3
00:00:10.780 --> 00:00:15.090
Andrew has been banging on about the shortcomings of the Internet,
um,

4
00:00:15.330 --> 00:00:19.300
on just know that everybody else is,
has joined the game.

5
00:00:19.600 --> 00:00:23.650
He's moving on ever the contrarian to,
to something else.
So the,
the,
the new book,

6
00:00:23.920 --> 00:00:26.940
it's certainly not without its criticisms,
trenching criticisms of,
uh,

7
00:00:27.370 --> 00:00:29.680
of silicon valley,
but also there's,

8
00:00:29.681 --> 00:00:32.800
there's suggestions on how to,
uh,

9
00:00:32.801 --> 00:00:35.950
to fix the future and resolve some of the issues that he has banged on a boat

10
00:00:35.951 --> 00:00:40.150
for so long.
So,
um,
welcome Andrew.
Thank you peta.

11
00:00:40.210 --> 00:00:43.920
And it's nice to be a wonderful,
uh,
audience,
spend a turnout.

12
00:00:44.230 --> 00:00:47.320
So I suspect you are going to tell us.
Um,
you told us.
So.

13
00:00:49.100 --> 00:00:53.840
<v 0>Yeah,
I,
I firstly I made a lot of mistakes as well and one of my books,</v>

14
00:00:53.841 --> 00:00:55.700
I predicted that Facebook wouldn't survive,

15
00:00:56.960 --> 00:01:00.290
so I am not as precious as I'd like to think I am.

16
00:01:00.320 --> 00:01:05.060
I think I was a little lucky and I think I took a gamble on cal to the amateur

17
00:01:05.061 --> 00:01:09.680
in 2007.
I mean,
it seemed obvious to me,

18
00:01:09.681 --> 00:01:11.860
but it could have easily ended in a different way.
I mean,

19
00:01:11.861 --> 00:01:15.290
I don't mean anything inevitable about it.
So,
uh,

20
00:01:15.300 --> 00:01:19.910
I think I was a little lucky and,
um,
and I've made many errors along the way,

21
00:01:19.911 --> 00:01:21.800
so I don't want to sound too cocky here.

22
00:01:23.020 --> 00:01:26.960
And you talk early on in the book about do you think I was right?

23
00:01:27.590 --> 00:01:30.380
Uh,
we'll,
we'll,
we'll talk about that in due course.

24
00:01:32.060 --> 00:01:35.540
Interesting to talk the normally some important points.

25
00:01:35.750 --> 00:01:39.230
You definitely made some important points and I think that's a diplomatic way of

26
00:01:39.231 --> 00:01:43.460
saying I was right on some of the points that I would disagree with.
But,

27
00:01:44.050 --> 00:01:48.020
uh,
I always joke about my book,
my book's come out,

28
00:01:48.021 --> 00:01:51.320
people always come up to me sort of half apologetically and they say,
we,

29
00:01:51.510 --> 00:01:54.380
we kind of agree with 60% of what you write.

30
00:01:54.680 --> 00:01:57.610
And I always joke and it's not really a joke,
we'll it away.
So

31
00:01:59.210 --> 00:02:01.370
will you talk early on about you?
You said there's,
there,
there's people who are,

32
00:02:01.371 --> 00:02:04.640
yes,
there's prepared no one,
the people who are maybe,
and you say you're a maybe.

33
00:02:04.641 --> 00:02:08.960
So what does that mean?
Well,
maybe means that the revolution,

34
00:02:08.961 --> 00:02:11.840
the digital revolution,
that you guys are engineering more than anyone else,

35
00:02:11.860 --> 00:02:13.190
it has a huge potential.

36
00:02:13.191 --> 00:02:17.060
It's the great event of the early part of the 21st century.
In many ways,

37
00:02:17.061 --> 00:02:19.790
it's the equivalent of the mid 19th century,

38
00:02:20.240 --> 00:02:24.700
early 20th century industrial revolution.
The next chapter,
or you know,

39
00:02:24.740 --> 00:02:29.150
some people call it the second machine age,
the fourth industrial revolution.
Um,

40
00:02:29.360 --> 00:02:33.140
it hasn't,
I think in overall terms,
it hasn't lived up to its billing.

41
00:02:33.500 --> 00:02:38.330
We were promised democracy,
more equality,

42
00:02:38.331 --> 00:02:42.270
more jobs,
a renascence of culture.
I don't think we've got the,

43
00:02:42.271 --> 00:02:45.530
I think we might get it maybe,
but at the moment it's not there

44
00:02:46.940 --> 00:02:48.820
<v 1>terms that you use right the way through the book.</v>

45
00:02:48.821 --> 00:02:52.000
Maybe you should explain what you described as Moore's law,

46
00:02:52.001 --> 00:02:55.620
which is not Myrryrs law,
but as Moore's law.
Uh,
but you,

47
00:02:55.630 --> 00:02:59.770
you talked a lot about that and you also talk about team human and what it to be

48
00:02:59.771 --> 00:03:00.604
human

49
00:03:00.820 --> 00:03:03.670
<v 0>or Mozilla was.
My phrase team human actually is Douglas Rushkoff's.</v>

50
00:03:03.671 --> 00:03:08.620
I stole that one from him.
But,
um,
uh,
you all know of course.

51
00:03:08.740 --> 00:03:12.760
Um,
Gordon Moore's law,
uh,
the law,
I think he,

52
00:03:12.910 --> 00:03:14.680
he came up with it originally.

53
00:03:14.681 --> 00:03:19.390
It was just a throwaway remark in 1965 the computer chips with double in power

54
00:03:19.391 --> 00:03:23.500
every 18 months or two years as the engine of the digital revolution.

55
00:03:23.740 --> 00:03:26.450
And usually when you read these books about,
you know,

56
00:03:26.500 --> 00:03:31.500
industry 3.0 and the second industrial revolution and the second machine age,

57
00:03:32.020 --> 00:03:35.020
there's always an opening chapter on Moore's law as the engine,

58
00:03:35.021 --> 00:03:38.320
as the driver of the change.
It's clearly a,

59
00:03:38.710 --> 00:03:42.520
a law or a scientific law or whether it will continue forever is arguable,

60
00:03:42.760 --> 00:03:44.680
but so far it's driven the revolution.

61
00:03:45.130 --> 00:03:47.820
My argument in the book is that the problem with most,

62
00:03:47.821 --> 00:03:51.100
or it's not a critique of Moore's law and you can't really criticize most or

63
00:03:51.101 --> 00:03:54.940
it's just an observation about the way the world works and the way technology

64
00:03:54.941 --> 00:03:58.950
works.
But my observation about Moore's law is it's,
it's,

65
00:03:58.990 --> 00:04:03.990
the technology has moved so fast that it's actually got beyond humans.

66
00:04:04.540 --> 00:04:09.540
We are being out run by technology and I think that explains why we're feeling

67
00:04:11.680 --> 00:04:15.610
increasingly uncomfortable,
awkward,

68
00:04:15.820 --> 00:04:20.200
disempowered in the face.
Not only have all this amazing new technology,

69
00:04:20.590 --> 00:04:21.250
uh,

70
00:04:21.250 --> 00:04:25.900
from the Internet to AI to a augmented reality,

71
00:04:25.901 --> 00:04:30.220
to a,
um,
smart machines and all the rest of it,
smart cars,

72
00:04:30.580 --> 00:04:34.780
but also in the context of large companies like yours.

73
00:04:34.781 --> 00:04:35.830
And a lot of people feel

74
00:04:37.480 --> 00:04:40.660
intimidated to put it politely by large,

75
00:04:40.661 --> 00:04:43.630
by the large platform players.
There are many of them.

76
00:04:43.631 --> 00:04:48.130
You know who the others are.
Um,
so that's the reality.

77
00:04:48.131 --> 00:04:50.800
So how do we catch up with Moore's law?

78
00:04:50.801 --> 00:04:53.440
How do we as humans catch up with Moore's law?

79
00:04:53.710 --> 00:04:56.560
So I invent the beginning of the book in chapter one,

80
00:04:56.830 --> 00:05:01.480
another kind of Moore's law.
This one is derived from Thomas Law.
Thomas Moore,

81
00:05:01.481 --> 00:05:05.560
the author of a Utopia 16th century Englishman.

82
00:05:05.561 --> 00:05:08.770
Most of you would have read his or certainly familiar with his books from

83
00:05:08.771 --> 00:05:12.090
university or school.
Uh,
he,
of course,
famously road.

84
00:05:13.640 --> 00:05:14.590
Yeah.

85
00:05:18.430 --> 00:05:23.290
Yeah.
Are we good now?
Oh yeah.
So he wrote,
he wrote a youth,

86
00:05:23.740 --> 00:05:27.670
I would argue he wrote a very realistic book,

87
00:05:27.671 --> 00:05:30.760
which is in part a critique of Utopia.
But in the book,

88
00:05:30.761 --> 00:05:35.761
the heart of your topia more's utopia I think is a reminder to people that the

89
00:05:36.491 --> 00:05:41.491
core thing about their responsibility as humans is agency to take responsibility

90
00:05:42.161 --> 00:05:45.070
for society and to build a better world.

91
00:05:45.310 --> 00:05:48.220
What more was reminding people in the 16th century,

92
00:05:48.460 --> 00:05:52.870
particularly in the light of Luther and predicts predestination and all these

93
00:05:53.410 --> 00:05:56.860
incredibly disruptive,
traumatic scientific discoveries,

94
00:05:57.080 --> 00:06:00.620
which suggested that we weren't at the center of the universe and the God was so

95
00:06:00.621 --> 00:06:04.220
infinite.
Didn't matter how we behave.
We were still,
you know,

96
00:06:04.230 --> 00:06:06.920
our faith was determined before we were born.

97
00:06:07.160 --> 00:06:11.990
What more was reminding people is actually human being still count.

98
00:06:12.470 --> 00:06:15.770
That's for me,
what Moore's law is about.
It's all about the agency.

99
00:06:16.130 --> 00:06:21.130
My definition of humans in the 21st century is that we've invented these smart

100
00:06:21.790 --> 00:06:25.250
machines or people like you are inventing these smart machines that can do most

101
00:06:25.251 --> 00:06:28.160
things,
but the one thing they can do is have agency.

102
00:06:28.820 --> 00:06:32.120
So the challenge for us in this age,

103
00:06:32.560 --> 00:06:35.320
not in smashing technology,
not in controlling it,

104
00:06:35.350 --> 00:06:38.480
not necessarily in breaking up Google or Facebook or Amazon,

105
00:06:38.990 --> 00:06:40.640
but in managing the world,

106
00:06:40.641 --> 00:06:45.641
in building a better world that reflects our interests rather than the interests

107
00:06:45.821 --> 00:06:46.910
seemingly of technology.

108
00:06:46.911 --> 00:06:50.540
Not that technology has its own interests or the interests of large platforms.

109
00:06:51.020 --> 00:06:54.290
Moore's law is the guiding principle of my book.

110
00:06:54.620 --> 00:06:59.210
And I reminding people that the dominant theme in the 21st century is human

111
00:06:59.211 --> 00:06:59.840
agencies.

112
00:06:59.840 --> 00:07:04.370
The one thing that distinguishes us from smart machines because the otherwise

113
00:07:04.371 --> 00:07:07.790
smart machines can do everything that we can do.
And I think we,
we,
we,

114
00:07:07.791 --> 00:07:12.470
we obviously agree that the,
the,
the Internet age has thrown up a whole lot of,

115
00:07:12.630 --> 00:07:16.880
of issues and society these days has all kinds of things that are,
uh,

116
00:07:17.150 --> 00:07:21.150
on appealing,
um,
uh,
butter and you kind of come up with,

117
00:07:21.151 --> 00:07:25.220
with five fixes that will help the future.
They're not,
I mean,

118
00:07:25.250 --> 00:07:27.310
they're not very controversial are they know they're not having more

119
00:07:27.320 --> 00:07:30.680
controversial.
Maybe you can rattle them off quickly.
Well,

120
00:07:30.760 --> 00:07:34.310
it's important to remember that my argument is that we've always had five fixes

121
00:07:34.311 --> 00:07:35.150
as human beings,

122
00:07:35.151 --> 00:07:39.650
five broad areas where we've been able to deal with disruption and build a

123
00:07:39.651 --> 00:07:42.290
better world and sort of like articulate more as well.

124
00:07:42.590 --> 00:07:45.050
The first is innovation companies like Google.

125
00:07:45.440 --> 00:07:48.830
The second is regulation of companies like Google,
perhaps.

126
00:07:49.070 --> 00:07:53.510
The third is a consumer choice and worker's choice.

127
00:07:53.511 --> 00:07:55.190
The fourth is citizen engagement,

128
00:07:55.370 --> 00:07:59.480
which is a kind of pure manifestation of Mozilla and the fifth is education and

129
00:07:59.481 --> 00:08:01.520
every time there's a great disruption,

130
00:08:01.521 --> 00:08:04.520
whether it's the industrial age or the renaissance and the reformation,

131
00:08:04.790 --> 00:08:09.110
we've always had these tools to shape a better world.

132
00:08:09.260 --> 00:08:11.780
The key,
in my view is that they work together.

133
00:08:12.150 --> 00:08:16.760
The mistake many people make is to rely on just one of these tools.

134
00:08:16.760 --> 00:08:21.230
So I think the mistake Silicon Valley has made has been to rely on the market,

135
00:08:21.231 --> 00:08:24.810
and I'm not saying certainly Google Uk,
I'm not sure if you,

136
00:08:24.980 --> 00:08:26.360
you fall into that category,

137
00:08:26.690 --> 00:08:30.860
but of course the the Peter teals of the world and the Marc Andreessen's,

138
00:08:31.010 --> 00:08:33.380
these people believe that if you just stay out of it,

139
00:08:33.381 --> 00:08:38.330
the market will eventually resolve all scarcities and create a better world for

140
00:08:38.331 --> 00:08:38.721
everyone.

141
00:08:38.721 --> 00:08:41.450
I think we'd been staying out of it for long enough to know that that's not the

142
00:08:41.451 --> 00:08:43.130
case,
but on the other hand,

143
00:08:43.131 --> 00:08:47.420
of course the Europeans or you will probably tell me the Europeans are too

144
00:08:47.421 --> 00:08:50.630
fixated on regulation,
which maybe there's some truth to that,

145
00:08:50.840 --> 00:08:54.050
so you need a mixture of the,
the the,
the two,

146
00:08:54.260 --> 00:08:57.960
the key argument in the book again is probably fairly self evident or they're

147
00:08:57.961 --> 00:08:58.141
not.

148
00:08:58.141 --> 00:09:02.460
Everyone in silicon valley will agree is that there is no app to fix the future.

149
00:09:02.460 --> 00:09:05.880
There's no simple fix.
Just as there was no simple fix in the industrial age,

150
00:09:06.240 --> 00:09:09.420
it takes a generation.
We need to be patient.
Then of course,

151
00:09:09.690 --> 00:09:11.460
patients in our network day,

152
00:09:11.461 --> 00:09:16.450
just something that doesn't always come to us naturally.
Um,
but,
uh,

153
00:09:17.160 --> 00:09:18.810
I sort of used the metaphor,

154
00:09:18.811 --> 00:09:23.811
the analogy of the tech stack in terms of these five tools and then mixed

155
00:09:24.031 --> 00:09:28.650
together.
And sometimes,
you know,
some reg regulation is often innovative.

156
00:09:28.651 --> 00:09:31.260
The best regulation,
and you,
you,
and I may disagree on this,

157
00:09:31.261 --> 00:09:35.730
but I think for example,
what Margaret Vestager is trying to do in Brussels,

158
00:09:36.720 --> 00:09:39.250
at least in her mind,
I interviewed her in the book.
Um,

159
00:09:39.780 --> 00:09:43.770
she's trying to protect innovator.
She's not,
um,

160
00:09:43.890 --> 00:09:47.760
she's not punishing innovation.
She's not against innovation.

161
00:09:48.090 --> 00:09:52.890
That doesn't mean all regulation is for innovation.
But,

162
00:09:52.920 --> 00:09:56.730
uh,
I think the best kind of regulation only works with innovation.

163
00:09:56.820 --> 00:10:00.730
I thought you were quite won over by Mrs [inaudible].
Have you met,

164
00:10:00.940 --> 00:10:02.490
did you learn that famous office?

165
00:10:02.950 --> 00:10:06.750
She was probably nicer to me than she was to you.
She seemed very nice to you.

166
00:10:06.751 --> 00:10:08.580
Definitely.
Very,
very helpful.
But I actually,

167
00:10:08.581 --> 00:10:13.581
you come out in favor of some things that are perhaps quite surprising that you

168
00:10:14.850 --> 00:10:16.800
seem to be championing or supporting.

169
00:10:16.801 --> 00:10:20.020
I was quite surprised by Singapore for example.
Uh,
and you,

170
00:10:20.390 --> 00:10:24.210
you accept that it's not utopia,
but you're sort of theme is,
uh,

171
00:10:24.430 --> 00:10:29.070
the people who are trying to build digital utopia and you point to Singapore and

172
00:10:29.330 --> 00:10:33.360
Estonia.
Estonia is my best model.
If there is a utopia,

173
00:10:33.361 --> 00:10:38.070
it's Astonia small country,
unique,
obviously post-soviet place,

174
00:10:38.340 --> 00:10:42.180
which benefited because it was in the old Soviet Union.
It was the sort of,
uh,

175
00:10:42.480 --> 00:10:46.560
it was,
uh,
the,
the place where they,
they had good technical universities.

176
00:10:46.950 --> 00:10:51.120
Now is I think the most wired,
the worst network place in Europe with tremendous,

177
00:10:51.360 --> 00:10:54.570
tremendously innovative policies in terms of east citizenship,

178
00:10:54.990 --> 00:10:59.790
in rearchitecting,
uh,
a social contract between consumers,

179
00:11:00.200 --> 00:11:02.820
uh,
or between citizens and government over data.

180
00:11:03.360 --> 00:11:06.660
I think you're right on Singapore.
I'm ambivalent about Singapore.

181
00:11:06.661 --> 00:11:08.550
I think what Singapore is doing on smart,

182
00:11:08.551 --> 00:11:11.430
on their smart nation initiative is interesting.
But of course,

183
00:11:11.431 --> 00:11:14.880
the big question with Singapore is,
uh,

184
00:11:15.330 --> 00:11:19.740
the lack of democracy and the fact that as the nation becomes smarter and

185
00:11:19.741 --> 00:11:24.030
smarter in a,
in a country without d democratic accountability,

186
00:11:24.240 --> 00:11:29.240
one can become quite nervous of the power of government over citizens.

187
00:11:29.850 --> 00:11:33.750
I would say that Estonia is the best case.
The worst case is China.

188
00:11:34.260 --> 00:11:37.910
Singapore is somewhere in between.
Yeah.
I mean you're talking a lot about trust,

189
00:11:37.911 --> 00:11:41.790
trust,
trust.
Trust is,
yeah.
But trust is the,
you know,

190
00:11:41.791 --> 00:11:45.630
the two great scarcities and again,
you guys know this better than I do.

191
00:11:45.631 --> 00:11:50.100
The two great scarcities of are networked age,
our trust and attention.

192
00:11:50.820 --> 00:11:55.090
Um,
and it's no coincidence we have a crisis of trust.

193
00:11:55.091 --> 00:11:59.180
I think in a digital age,
particularly in the sort of the,

194
00:11:59.190 --> 00:12:04.060
the age of user generated content and social networks where everyone is

195
00:12:04.061 --> 00:12:07.630
continually undermining authority.
Now it's not just because of the Internet.

196
00:12:07.900 --> 00:12:11.740
You Know Fox and MSNBC and I'm sure a lot of English TV stations do the same

197
00:12:11.800 --> 00:12:12.400
thing,

198
00:12:12.400 --> 00:12:16.960
but there is a connection between the crisis of trust and the appearance of sort

199
00:12:16.961 --> 00:12:18.490
of network culture.

200
00:12:19.090 --> 00:12:22.780
But do you think people in in this country or indeed more broadly and in Europe,

201
00:12:22.810 --> 00:12:25.830
would it accept government intervention at the,

202
00:12:25.880 --> 00:12:30.880
at the level that the dystonia and Singapore do and would trust him cause level

203
00:12:31.120 --> 00:12:33.200
the level of trust among Singapore citizens?

204
00:12:33.201 --> 00:12:35.080
Citizens and their government is remarkably high.

205
00:12:35.081 --> 00:12:37.900
But I think you could try to do what Singapore does in the UK.

206
00:12:37.901 --> 00:12:41.410
You might get a regiment.com where it's chicken and egg.
I mean the,

207
00:12:41.460 --> 00:12:42.970
the crisis in England,

208
00:12:43.180 --> 00:12:47.440
it's both England and America I think have a crisis of legitimacy of democratic

209
00:12:47.441 --> 00:12:51.600
institutions are sort of redundancy of political parties and ideology.

210
00:12:51.600 --> 00:12:54.250
So where do you start to rebuild that?
Um,

211
00:12:54.430 --> 00:12:59.320
the Estonian model is interesting in that,
um,
I,

212
00:12:59.650 --> 00:13:01.870
I use the Edelman Trust Barometer,

213
00:13:01.871 --> 00:13:06.460
which is the sort of the gold standard for determining who trusts what and it's

214
00:13:06.461 --> 00:13:07.720
always falling every year.

215
00:13:07.721 --> 00:13:11.200
It's amazing how lucky he releases it every year at Davos and every year it's

216
00:13:11.201 --> 00:13:13.180
the same story.
There's less and less trust.

217
00:13:13.470 --> 00:13:16.810
But what do you think he found in Astonia was people trust the government,

218
00:13:16.840 --> 00:13:20.860
but they may not trust the political parties,
which is interesting.
Um,

219
00:13:21.130 --> 00:13:23.770
I'm on the data front.

220
00:13:23.800 --> 00:13:26.830
I think what the Estonians are doing is interesting because they are,

221
00:13:27.850 --> 00:13:32.850
I talked to the former Estonian president who was the real architect of many of

222
00:13:33.101 --> 00:13:36.760
their reforms.
This guy called [inaudible] very charismatic figure.

223
00:13:37.600 --> 00:13:42.580
His argument is that
privacy is history.

224
00:13:43.360 --> 00:13:46.630
His argument is in the age of smart everything,

225
00:13:47.080 --> 00:13:52.080
it's increasingly difficult if not impossible to maintain a kind of 19th century

226
00:13:53.561 --> 00:13:54.790
version of privacy.

227
00:13:55.330 --> 00:13:59.620
The one that was protected in laws by people like Louis Brandeis in the u s and

228
00:13:59.621 --> 00:14:02.410
John Stuart Mill and the UK.
So what,

229
00:14:03.340 --> 00:14:06.400
what these stone have tried to do is say,
okay,

230
00:14:06.401 --> 00:14:09.940
well with all the digital reforms at the Estonians are doing,

231
00:14:10.240 --> 00:14:13.000
the government is going to know a lot about you,
your health records,

232
00:14:13.001 --> 00:14:15.940
your tax records,
your car wreck was,
everything is online.

233
00:14:16.110 --> 00:14:18.610
It's stoner is the first country to really go online.

234
00:14:20.650 --> 00:14:23.890
What the Estonians are trying to do,
and I'm,
I'm sure it's not perfect,

235
00:14:23.891 --> 00:14:28.480
but it's an interesting idea,
is to say,
look,
we do know everything about you,

236
00:14:29.110 --> 00:14:32.740
but if we choose to look at your data,
we will tell you.

237
00:14:33.250 --> 00:14:37.040
So the government has an accountability.
It's a kind of almost,

238
00:14:37.110 --> 00:14:39.100
it's not based on blockchain technology,

239
00:14:39.101 --> 00:14:44.020
but it's a blockchain like thinking which may be one way of rethinking a social

240
00:14:44.021 --> 00:14:46.360
contract.
Um,
you know,

241
00:14:46.361 --> 00:14:50.530
we can cling to our romantic notions of privacy.

242
00:14:50.800 --> 00:14:52.550
I'm just not sure how realistic that is,

243
00:14:52.551 --> 00:14:55.670
particularly since we're on the verge of,
you know,

244
00:14:55.671 --> 00:14:59.390
smart everything from cars to cities to bodies.

245
00:14:59.780 --> 00:15:04.780
I don't quite know in this age how we protect our privacy either from companies

246
00:15:05.301 --> 00:15:07.220
like yours or from governments.

247
00:15:07.490 --> 00:15:11.120
So perhaps rather than trying to do that,

248
00:15:11.121 --> 00:15:13.070
which is a kind of Sisyphean task,

249
00:15:13.370 --> 00:15:17.960
it might be better to force the government under law to be much more

250
00:15:17.961 --> 00:15:20.930
accountable.
So I think what the Estonians are doing is interesting.

251
00:15:20.931 --> 00:15:24.470
I think I'm more,
I,
as you can tell from the Singapore chapter,

252
00:15:24.471 --> 00:15:29.300
I'm a little bit more ambivalent,
but I think it's too easy for,

253
00:15:29.810 --> 00:15:33.680
you know,
British or American people to write off the Singapore experiment.

254
00:15:33.950 --> 00:15:35.510
It's a miracle and economic terms,

255
00:15:35.511 --> 00:15:37.400
they have the best education system in the world.

256
00:15:37.580 --> 00:15:39.680
They have a remarkably innovative economy.

257
00:15:39.860 --> 00:15:44.390
So to write them off as sort of neo authoritarian,
I think it's slightly unfair.

258
00:15:44.440 --> 00:15:48.470
You are your clearly pro government,
you're pro statism.
Is that,

259
00:15:48.590 --> 00:15:51.800
would you agree with that?
No.

260
00:15:53.650 --> 00:15:57.140
Let me explain what you mean.
I mean,
and give me an example of that.

261
00:15:57.200 --> 00:16:01.730
You're in favor of the,
the intervention of,
of governments to regulate.

262
00:16:02.180 --> 00:16:05.390
I am in favor of regulation as one of the five tools.

263
00:16:05.900 --> 00:16:10.040
I believe that the biggest mistake in America was they sort of retreat of

264
00:16:10.041 --> 00:16:11.960
government from the digital terrain.

265
00:16:11.960 --> 00:16:15.820
I think you guys got very lucky with safe harbor.
You guys,
you know,

266
00:16:15.890 --> 00:16:20.330
Eric Schmidt did a very nice job on barrack Obama.
Um,

267
00:16:20.840 --> 00:16:23.300
and uh,
you know,
God knows what Trump's up too,

268
00:16:23.301 --> 00:16:26.780
but I think that the es state,
you call it,

269
00:16:26.781 --> 00:16:30.800
whether it's the government and elected government has a responsibility to

270
00:16:30.801 --> 00:16:32.990
regulate some aspects.

271
00:16:32.991 --> 00:16:36.230
And I used the example of the industrial revolution with other industries.

272
00:16:36.530 --> 00:16:38.510
So we look at food,
uh,

273
00:16:38.511 --> 00:16:41.960
without the regulation of the food industry,

274
00:16:41.961 --> 00:16:45.920
we'd all still be poisoned without the regulation of labor laws.

275
00:16:45.921 --> 00:16:50.600
You'd still have 11 year olds in,
in working,
in factories without regulation,

276
00:16:50.800 --> 00:16:54.740
uh,
unions would still be out load.
So I think we have to be realistic.

277
00:16:54.890 --> 00:16:58.160
Just this is an English audience in America.

278
00:16:58.460 --> 00:17:01.160
I think people hear a little bit more realistic about this.
In America.

279
00:17:01.400 --> 00:17:03.860
Anytime you suggest any kind of regulation,

280
00:17:03.861 --> 00:17:06.320
you're accused of being a stolen this,
which is absurd.

281
00:17:06.590 --> 00:17:10.070
It's just one piece of the puzzle.
One bit of the stack.

282
00:17:10.370 --> 00:17:13.610
But to deny it significance I think is extremely unwise.

283
00:17:13.760 --> 00:17:16.790
I think it's unwise of you as well.
Not You personally,
but your company.

284
00:17:16.791 --> 00:17:21.680
I think you and I think you are acknowledging the government is a reality.

285
00:17:21.740 --> 00:17:26.740
I mean wherein Steve Case's third stage where politics becomes relevant.

286
00:17:27.410 --> 00:17:30.050
I mean if you didn't realize that you wouldn't be spending so much money

287
00:17:30.260 --> 00:17:32.870
lobbying in Washington DC,
it's not a bad thing.

288
00:17:33.860 --> 00:17:35.300
We need accountable government.

289
00:17:35.301 --> 00:17:39.340
I'm very disappointed in many ways with the u s government.
Uh,
but I think the,

290
00:17:39.650 --> 00:17:42.320
the EU government is doing a better job,
not idea.

291
00:17:42.321 --> 00:17:45.590
I think some of the things that some of the European states doing are absurd.

292
00:17:45.591 --> 00:17:46.930
You know,
the,
the,
the,

293
00:17:47.180 --> 00:17:52.180
the French Spanish initiative to force Google to pay newspapers for sending them

294
00:17:52.441 --> 00:17:56.750
traffic from Google news is obviously absurd.
It's an example of,
of,
of,

295
00:17:56.760 --> 00:18:00.870
of shortsighted,
of counterproductive regulation.
Not all regulation is right,

296
00:18:01.110 --> 00:18:06.110
but I think the GDPR is an interesting initiative to make data portable.

297
00:18:06.690 --> 00:18:10.680
I think investigators work in antitrust and taxation is important.

298
00:18:11.160 --> 00:18:13.290
And I thought that,
I thought the comparison you made with,

299
00:18:13.320 --> 00:18:17.010
I mean do you agree I was going to make these avoiding the quest.

300
00:18:18.090 --> 00:18:19.000
I thought the,

301
00:18:19.001 --> 00:18:24.001
the comparison you made with the car industry and Ralph Nader unsafe at any,

302
00:18:24.630 --> 00:18:27.540
at any speed was a very compelling one.

303
00:18:28.150 --> 00:18:30.460
The only difference I would make there is that the,

304
00:18:30.550 --> 00:18:34.800
because of po poorly designed cars,
literally millions of people died.
Yeah,

305
00:18:35.520 --> 00:18:39.950
that's true.
But so,
so my argument about the American car industry,
and again,

306
00:18:39.951 --> 00:18:42.390
I'm speaking to you,
I guess we're in Britain,

307
00:18:42.391 --> 00:18:44.400
but you're still an American company.
My,

308
00:18:44.420 --> 00:18:48.120
my overall argument is I think some companies perhaps including yours,

309
00:18:48.390 --> 00:18:53.040
have lost sight of their customers in,
I don't know who your customers are,

310
00:18:53.041 --> 00:18:54.780
their users interest.

311
00:18:55.230 --> 00:18:58.230
So I think your business model is profoundly flawed in the longterm.

312
00:18:58.290 --> 00:19:02.820
I think it's flawed because it's essentially transforms the user into the

313
00:19:02.821 --> 00:19:07.440
product.
And whilst you,
someone here will argue,
well,
people don't complain,

314
00:19:07.441 --> 00:19:09.120
you're right.
But ultimately they will,

315
00:19:09.150 --> 00:19:11.490
I think people don't want to be watched all the time.

316
00:19:11.880 --> 00:19:13.530
People don't want to be turned into the product.

317
00:19:13.531 --> 00:19:15.930
And I use the example of the American car industry,

318
00:19:16.200 --> 00:19:18.160
which was fat and happy in the 50s.

319
00:19:18.210 --> 00:19:22.170
So fat and happy indeed that they started designing cars that were essentially

320
00:19:22.240 --> 00:19:23.840
deathtraps,
uh,

321
00:19:23.880 --> 00:19:28.880
Neda and 65 wrote is unsafe at any speed that exposed the bad design and lack of

322
00:19:30.841 --> 00:19:35.100
respect for their users in the car industry.
And you know,
pre Tesla,

323
00:19:35.101 --> 00:19:37.170
the American car industry has never recovered.

324
00:19:37.320 --> 00:19:41.790
So I think it's important whether it's Google or any other tech company to think

325
00:19:41.791 --> 00:19:45.690
about the real interests of what they use as well.
Now everyone,
you know,

326
00:19:45.691 --> 00:19:49.830
in terms of search engine,
obviously everyone wants a high quality search engine,

327
00:19:49.831 --> 00:19:52.740
which you have.
But you and I've had this conversation before,

328
00:19:52.741 --> 00:19:56.070
I still think people would like to pay for their search engine if they were

329
00:19:56.071 --> 00:19:59.390
guaranteed complete privacy and that their data would be left alone.

330
00:19:59.430 --> 00:20:02.660
I wish that you would offer that.
I don't see why it's a problem.

331
00:20:02.790 --> 00:20:06.050
And you point to examples of organizations,
other organizations who are,

332
00:20:06.170 --> 00:20:09.030
who are doing that kind of thing.
I mean,
and that's,
that's the market,
isn't it?

333
00:20:09.350 --> 00:20:12.770
At work.
If there is demand for it,
people will,
will come.
Yeah.
But,

334
00:20:12.771 --> 00:20:15.280
but it's the old Steve jobs argument as well.
You know,

335
00:20:15.281 --> 00:20:18.330
if Steve Jobs had waited for the market,
we never would've had the iPhone.

336
00:20:18.750 --> 00:20:23.010
You guys know your market and the mentality of your users better than anyone.

337
00:20:23.340 --> 00:20:27.060
And I think in a sense that you need to become a little bit more responsible and

338
00:20:27.061 --> 00:20:31.740
accountable in pushing your users it towards say,

339
00:20:31.741 --> 00:20:36.390
paying for services.
I,
I don't,
you know,
whether it's in content,
uh,
in Youtube,

340
00:20:36.391 --> 00:20:37.620
whether it's on search,

341
00:20:37.950 --> 00:20:42.950
I think that the biggest tragedy of the history of the web was a fetishization

342
00:20:44.401 --> 00:20:49.090
of free.
I think that has been the most destructive mistake.

343
00:20:49.150 --> 00:20:52.870
I think it's essentially destroyed the media industry or much of the media

344
00:20:52.871 --> 00:20:56.830
industry and a spoil consumers into thinking that they shouldn't have the right

345
00:20:56.831 --> 00:20:57.521
to pave stuff,

346
00:20:57.521 --> 00:21:01.570
which is very misleading and destructive and subscription to something that is

347
00:21:01.571 --> 00:21:05.500
coming both in,
in news,
which is good.
Which I hope,
you know,

348
00:21:05.501 --> 00:21:08.350
larger companies like yours would get behind,
uh,

349
00:21:08.351 --> 00:21:11.640
it offers another business model.
Yeah.
So I mean,

350
00:21:11.740 --> 00:21:15.430
the big disagreement I would have with you about the book is that as far as I

351
00:21:15.431 --> 00:21:19.930
could see that there literally is no acknowledgment of the benefits of

352
00:21:19.931 --> 00:21:24.010
technology in the book.
And,
and the last time we had,
last time you were here,

353
00:21:24.160 --> 00:21:26.560
we did prayer rather cheap shot,
which was to play you the,

354
00:21:26.650 --> 00:21:30.230
the famous can we do month by the Romans ever do for us a clip from Monica,

355
00:21:30.320 --> 00:21:31.930
which I haven't gotten it,
but I'm going to read the quote,

356
00:21:32.260 --> 00:21:36.880
which is where John [inaudible] says,
all right.
But apart from the sanitation,

357
00:21:36.910 --> 00:21:41.800
the medicine,
the education,
the wine,
uh,
public order,
irrigation roads,

358
00:21:41.820 --> 00:21:46.150
freshwater and public health,
what have the Romans ever done for us?

359
00:21:46.640 --> 00:21:50.020
Um,
okay.
But I could,
you know,
we're not,
we're not,
don't Monte pay for now,

360
00:21:50.021 --> 00:21:53.530
but we can pretend to do Monte python.
And I could say,
and I wouldn't,

361
00:21:54.070 --> 00:21:55.000
but I could say

362
00:21:58.720 --> 00:22:02.410
fake news,
technological addiction,

363
00:22:02.890 --> 00:22:06.740
increasing inequality.
So,
but,
but,
but my hip.

364
00:22:07.040 --> 00:22:10.960
But here's the point is that we're pizza would be on that discussion I wrote

365
00:22:10.961 --> 00:22:14.830
whenever,
let me come back on this.
This is not a book about that.

366
00:22:14.860 --> 00:22:17.710
Everyone acknowledges that the Internet's done amazing things.

367
00:22:17.711 --> 00:22:20.410
That's not the issue anymore.
We've moved on from that debate.
It's,

368
00:22:20.411 --> 00:22:25.120
it's not useful anymore to be continually discussing whether or not the Internet

369
00:22:25.121 --> 00:22:29.290
has benefited humanity.
Kelly,
it's done some good and there are lots of problems.

370
00:22:29.410 --> 00:22:30.970
So what I'm doing in the book is saying,

371
00:22:30.971 --> 00:22:33.570
okay [inaudible] what's the point of this?

372
00:22:33.571 --> 00:22:35.230
It's like some sort of political correctness.
The,

373
00:22:35.231 --> 00:22:38.260
every time I read a book I have to explain why the Internet is great,

374
00:22:39.020 --> 00:22:40.630
but the point,
but there should be an acknowledgement.

375
00:22:40.631 --> 00:22:43.710
There should be a balance because the point is when I speak to you,
you're,

376
00:22:43.711 --> 00:22:45.220
you're very reasonable.
Then I read your book.

377
00:22:46.860 --> 00:22:51.190
This is a couple of it.
Generally speaking,
not an unreasonable,
but we'll,

378
00:22:51.191 --> 00:22:52.660
let me give you a fan of quotes from the book.

379
00:22:53.140 --> 00:22:56.800
Much of the digital innovation of big tech companies like Google and Facebook

380
00:22:56.860 --> 00:23:00.730
isn't currently working.
Yeah,
come on.
You can't stand by that.

381
00:23:00.731 --> 00:23:04.500
I mean you can speak to your,
your phone lens in Spanish at lunch.

382
00:23:04.520 --> 00:23:07.420
Let's pick on our friends in Facebook.
Let's leave Google out of this.

383
00:23:07.421 --> 00:23:10.120
Do you think,
I don't know if there's anyone from Facebook.

384
00:23:10.121 --> 00:23:13.570
Do you own Facebook yet?
That's next week,
right?

385
00:23:15.770 --> 00:23:18.790
It's Facebook working.
It's infested with fake news.

386
00:23:18.791 --> 00:23:22.690
Zuckerberg has absolutely no idea of where to go in terms of his business

387
00:23:22.691 --> 00:23:27.660
strategy.
Kids wouldn't be seen dead on Facebook.
Um,
it's,

388
00:23:27.690 --> 00:23:32.540
it's the perfect sort of place.
You know,
Putin spends,
you know,

389
00:23:32.560 --> 00:23:36.640
millions of dollars a year hiring people to post lies on Facebook.

390
00:23:37.050 --> 00:23:39.340
Um,
I think mostly,

391
00:23:39.341 --> 00:23:43.570
and we see more and more research showing that kids are addicted to this thing.

392
00:23:43.780 --> 00:23:46.490
Again,
it doesn't mean that everything,
Facebook is bad.

393
00:23:46.530 --> 00:23:49.100
We don't need to get into that.
But I would say in overall terms,

394
00:23:49.101 --> 00:23:52.940
Facebook is not working.
And I think even Zuckerberg's acknowledging,

395
00:23:52.941 --> 00:23:56.510
I mean he acknowledged the most people now realize that Trump,

396
00:23:56.720 --> 00:24:01.720
one of the main reasons why Trump won the election was because the Russians

397
00:24:02.030 --> 00:24:05.900
gamed.
Facebook
is Facebook working.

398
00:24:05.960 --> 00:24:10.190
I would acknowledge that they're,
there were definitely issues.
Our point,

399
00:24:10.220 --> 00:24:14.300
our point is please hold us to account for what we do and generally speaking,

400
00:24:14.301 --> 00:24:17.600
don't bundle this together and bundles together in the same sense with,

401
00:24:19.080 --> 00:24:20.060
but I look,
here's another,

402
00:24:20.270 --> 00:24:24.080
here's another quote which I wish I fund going a little far.

403
00:24:24.081 --> 00:24:28.670
It seems almost normal for online audiences of millions to watch revenge porn,

404
00:24:28.850 --> 00:24:32.520
live beheadings and suicides.
That's not true,
isn't it?

405
00:24:36.200 --> 00:24:40.190
Almost normal,
which depends on your definition of what

406
00:24:41.800 --> 00:24:42.140
but,

407
00:24:42.140 --> 00:24:45.800
but what do you accept that there has been a profound sort to corrosion at the

408
00:24:45.801 --> 00:24:49.230
culture and the um,
uh,
the,
the,
the,

409
00:24:49.231 --> 00:24:52.460
the kind of content on,
on this media,

410
00:24:52.461 --> 00:24:55.010
which is apps with wet weather is lacking curation.

411
00:24:55.011 --> 00:24:57.760
It's very troubling in many ways.
The,
you know,
the,

412
00:24:57.761 --> 00:25:01.160
the infestation of sexism and racism and,
and,

413
00:25:01.450 --> 00:25:06.200
and sort of cults of violence.
Those are realities.
My point is,
I don't think,

414
00:25:06.740 --> 00:25:10.220
I think that,
you know,
if you want me to,
I'm going to do it again.

415
00:25:10.221 --> 00:25:13.340
I'm going to bundle you and Google and Facebook all together.

416
00:25:13.640 --> 00:25:16.400
I think you have to acknowledge them.
We've had this conversation before.

417
00:25:16.490 --> 00:25:19.040
You have to acknowledge that your media companies and that you have a

418
00:25:19.041 --> 00:25:23.570
responsibility for the content that's published on your network.

419
00:25:23.750 --> 00:25:27.950
You have the same responsibility as a newspaper or a movie studio.

420
00:25:28.280 --> 00:25:32.270
And I think the sooner you,
not you personally,

421
00:25:32.271 --> 00:25:37.070
but certain companies acknowledge that the better for everyone.

422
00:25:37.220 --> 00:25:38.870
Do you do look at,
I don't disagree with that.

423
00:25:39.590 --> 00:25:41.430
I would agree with what you talk about,
which is the,

424
00:25:41.590 --> 00:25:44.510
the combinant tutorial approach,
isn't it?
There's no,

425
00:25:44.511 --> 00:25:47.480
there's no one club to fix these problems.
It's a range of things.

426
00:25:47.481 --> 00:25:50.960
And I think actually if you look at it,
hate speech or violent extremism,

427
00:25:53.100 --> 00:25:56.060
correct oil approach,
what some of the things you're doing right.

428
00:25:56.270 --> 00:26:01.270
What I argue though is that we can't just rely on your generosity to humanity,

429
00:26:04.970 --> 00:26:05.361
uh,

430
00:26:05.361 --> 00:26:10.361
that the only way that large tech companies are coming to the table to fix these

431
00:26:11.271 --> 00:26:14.570
solutions is when they're threatened with major fines.

432
00:26:14.571 --> 00:26:17.920
You only do it with the bottom line threat.
Uh,

433
00:26:17.930 --> 00:26:21.380
it's not enough to just tell these people to take responsibility.

434
00:26:21.381 --> 00:26:23.780
And that's why I think I applaud some of the,
you know,

435
00:26:23.800 --> 00:26:27.950
even what the Germans are doing with youtube and some of the other media.

436
00:26:28.370 --> 00:26:32.540
I think that's the only way it works.
And I think the other mistake I,

437
00:26:32.550 --> 00:26:35.030
since we're on our Facebook,
Google ran,

438
00:26:35.870 --> 00:26:38.300
the other mistake that's happened,

439
00:26:38.750 --> 00:26:40.520
I'm not sure if this is certainly not true of you,

440
00:26:40.521 --> 00:26:45.330
but have some people in these large tech is that they believe their own hype.

441
00:26:45.480 --> 00:26:46.140
They,

442
00:26:46.140 --> 00:26:50.010
they believe their own Koolaid or they've drunk the Koolaid so much that they've

443
00:26:50.011 --> 00:26:52.860
started to believe it.
And one of the delusions of,

444
00:26:52.890 --> 00:26:56.790
I think this was true Google in an early stage,
maybe not now,
but you know,

445
00:26:56.791 --> 00:27:00.270
in the early days and Larry and Sergey and the do no evil stuff,

446
00:27:00.420 --> 00:27:03.960
it was this idea that you could be incredibly successful and do good

447
00:27:03.961 --> 00:27:07.140
simultaneously and that you,
uh,
and Facebook,

448
00:27:07.141 --> 00:27:09.650
you were breaking the mold of capitalist companies.

449
00:27:10.080 --> 00:27:13.290
The typical capitalist company was an oil company or a,

450
00:27:13.590 --> 00:27:16.350
or a big bank and they weren't,
they may not be evil,

451
00:27:16.351 --> 00:27:18.300
but they had no benefit to humanity.

452
00:27:18.630 --> 00:27:23.630
The idea of the Google Ipo and the Facebook IPO and so much of the kind of

453
00:27:23.731 --> 00:27:27.570
ideology that came out of Silicon Valley in the web 2.0 a jap and two maybe

454
00:27:27.571 --> 00:27:30.360
three or four years ago was the we are different.

455
00:27:30.570 --> 00:27:34.920
We are reinventing rearchitecting capitalism and I don't think that's true.

456
00:27:34.921 --> 00:27:39.480
I don't think big tech companies at any worse than you know,

457
00:27:39.481 --> 00:27:44.080
big banks or big Pharma companies,
but they're no better either.
Yeah.

458
00:27:44.160 --> 00:27:47.460
I'm actually one thing that I agree,
one thing up to a point,

459
00:27:47.461 --> 00:27:51.820
one thing that one thing that struck me about the book was not a great deal of

460
00:27:51.821 --> 00:27:56.821
mention of of the user and I think the big difference with with with with Google

461
00:27:57.530 --> 00:28:00.660
and other tech companies is that we are extremely answerable to the user and

462
00:28:00.661 --> 00:28:03.480
when we get things wrong that we hear about it a very,
very quick.

463
00:28:03.510 --> 00:28:07.080
I think that's a fair point.
And I still think,
you know,
the,

464
00:28:08.700 --> 00:28:10.980
since you're being so frank and open,

465
00:28:10.981 --> 00:28:15.981
I think one of the weaknesses of the book is that I still assume that users will

466
00:28:16.441 --> 00:28:18.300
rebel and most,

467
00:28:18.790 --> 00:28:22.060
I'm sure you do a lot of research and how happy your users on,

468
00:28:22.080 --> 00:28:23.520
I assume they're reasonably happy.

469
00:28:23.910 --> 00:28:28.910
My fare and prediction is that we still haven't had a major data event.

470
00:28:30.150 --> 00:28:33.240
We still haven't had an Exxon Valdez or certainly a Chernobyl.

471
00:28:33.750 --> 00:28:38.340
And I still feel something will happen.
It may be a state to state digital event,

472
00:28:38.640 --> 00:28:42.840
which will open people's eyes.
You know,
I did,
you know,
click,
you know,
the,
the,

473
00:28:42.841 --> 00:28:47.160
the burden media search engine is designed to take on Google.

474
00:28:47.760 --> 00:28:52.050
I did,
uh,
when,
when,
uh,
the chief technology officer,
Belgian guy,

475
00:28:52.051 --> 00:28:56.220
I think he's an old friend of Larry and Sergei,
very smart guy.
We sat in a,

476
00:28:56.221 --> 00:28:59.940
in their office in Munich and he showed me how much you can find out about

477
00:28:59.941 --> 00:29:01.680
anyone online if you know what you're doing.

478
00:29:02.550 --> 00:29:05.880
And I think when you show regular people that they would be terrified cause

479
00:29:05.881 --> 00:29:07.730
people still value their privacy.
Yeah.

480
00:29:07.820 --> 00:29:10.140
And I like the line that you use about nothing,
nothing,
nothing.

481
00:29:10.141 --> 00:29:13.590
And then bang and you kind of,
you're kind of suggesting that.
Yeah,

482
00:29:13.591 --> 00:29:17.190
and I think it's always a time where,
you know,
no business model,

483
00:29:17.191 --> 00:29:21.060
no company lasts forever.
I think since I'm being self critical,
the week,

484
00:29:21.120 --> 00:29:26.120
another weakness of the book is on the one hand I worry about monopolies and

485
00:29:28.501 --> 00:29:32.470
huge companies.
And on the other hand I predict the demise.

486
00:29:32.471 --> 00:29:36.480
So I think there is some,
I mean I wouldn't talk about that publicly of course,

487
00:29:36.481 --> 00:29:39.260
but I think there is,
oh yeah.

488
00:29:39.370 --> 00:29:42.970
But Youtube edit the headings.

489
00:29:43.030 --> 00:29:47.380
Are we going to have a ritual bad thing at the end?
So,
um,

490
00:29:48.760 --> 00:29:50.260
towards the,
towards the end of the book you talk,

491
00:29:50.290 --> 00:29:54.190
you talk about the kind of public spiritedness and an education that need for

492
00:29:54.191 --> 00:29:58.930
education.
And actually you talking about the universal basic income as well,

493
00:29:58.931 --> 00:30:03.780
which I think you kind of come out in support of
universal,

494
00:30:04.160 --> 00:30:04.993
you know,

495
00:30:05.140 --> 00:30:09.430
coming out against universal income is basic incomes like coming out against

496
00:30:09.431 --> 00:30:12.250
apple pie or a,
you know,
babies.

497
00:30:13.350 --> 00:30:16.270
I'm a Northern Irish Protestant.
I think it was as fossil fuels,

498
00:30:16.271 --> 00:30:19.240
if you give people money without a job,
they got to spend it on drink.

499
00:30:20.170 --> 00:30:24.310
But that's true in northern,
I uh,
yeah.

500
00:30:24.311 --> 00:30:27.220
But what do you make of the argument?
And again,

501
00:30:27.221 --> 00:30:30.880
it's a popular argument in Silicon Valley.
What do you make of the argument that,

502
00:30:30.881 --> 00:30:31.714
um,

503
00:30:32.970 --> 00:30:36.430
that technology may take away so many jobs that there just aren't going to be a

504
00:30:36.431 --> 00:30:39.670
jobs?
What's everyone going into?
Not everyone can work with Google.
You know,

505
00:30:39.671 --> 00:30:42.130
Google,
what's the market cap of Google?
It's almost a trillion.

506
00:30:42.131 --> 00:30:46.360
How many people work for you?
70.
80,000.
Yeah,
I mean,
exactly.
So,
and,

507
00:30:46.410 --> 00:30:49.690
and we know there'll be less and less as AI becomes more and more central in

508
00:30:49.691 --> 00:30:51.280
your company.
Um,

509
00:30:52.060 --> 00:30:56.620
the thing about what I like about the guaranteed minimum income is it's thinking

510
00:30:56.621 --> 00:31:01.480
big.
Just as in the middle of the 19th century,
there was no social security net,

511
00:31:01.960 --> 00:31:06.070
no way of protecting unlucky workers from the ravages of cap industrial

512
00:31:06.071 --> 00:31:09.730
capitalism.
So Bismarck who certainly was anything but a socialist,

513
00:31:10.780 --> 00:31:15.190
pioneered social security.
Then it was developed in Scandinavia,
in the UK.

514
00:31:15.191 --> 00:31:17.720
And then in,
in the u s um,

515
00:31:17.860 --> 00:31:20.860
I think we need to be thinking in those big terms.

516
00:31:21.070 --> 00:31:26.070
My problem with universal basic income is it seems to satisfy the conscience of

517
00:31:26.891 --> 00:31:30.400
Silicon Valley billionaires like Sam Altman.

518
00:31:31.270 --> 00:31:34.900
But I'm not sure if we have,
you know,
80% of the people in our,

519
00:31:34.901 --> 00:31:38.770
in our society living on $2,000 a month,
uh,

520
00:31:38.771 --> 00:31:41.690
the whole world will look like San Francisco,
but you know,

521
00:31:41.940 --> 00:31:46.300
a few noblemen and barons in their mansions and everybody else living out on the

522
00:31:46.301 --> 00:31:50.770
streets.
So the problem I think with the universal basic income is it can kind of

523
00:31:51.430 --> 00:31:54.010
institutionalize the,

524
00:31:55.180 --> 00:32:00.130
already have the inequalities of our age.
And you're right,
I mean,
look,
we are,
we,

525
00:32:00.131 --> 00:32:01.540
you and I grew up in,
in,

526
00:32:01.541 --> 00:32:04.780
in the age of the doll in the UK where a lot of people did just say on the doll

527
00:32:04.781 --> 00:32:09.370
and do nothing and drink and watch TV.
Um,
so that is an issue,

528
00:32:09.730 --> 00:32:14.080
but I think we need to think creatively in the book I A,
I go to Switzerland,

529
00:32:14.110 --> 00:32:16.390
which is the first country to have a referendum on it.

530
00:32:16.660 --> 00:32:18.940
This is becoming a real issue.
It's a real issue in Finland.

531
00:32:19.390 --> 00:32:23.080
It's an issue in Canada.
It's an issue in Brazil.
We need to think big.

532
00:32:23.081 --> 00:32:27.100
We can't just fall back on the old certainties cause they don't work.
I mean,

533
00:32:27.101 --> 00:32:30.980
what do you think about every economist from,
you know,

534
00:32:31.030 --> 00:32:35.350
even McAfee and bring Johnson who are relatively optimistic,

535
00:32:36.820 --> 00:32:40.760
are concerned about the,
of smart technology on jobs?

536
00:32:40.910 --> 00:32:43.520
Just because technologies in the past have created jobs,

537
00:32:43.521 --> 00:32:46.160
doesn't guarantee it in the future.
Um,

538
00:32:46.190 --> 00:32:50.950
so I do think we have to acknowledge this and if indeed this technology,

539
00:32:50.951 --> 00:32:52.550
as many economists predict,

540
00:32:52.880 --> 00:32:57.440
do put 30 40% of us out of work,
we've got to come up with something.

541
00:32:57.740 --> 00:33:01.280
Otherwise,
you know,
there's revolution on the street.
People are,
you know,

542
00:33:01.281 --> 00:33:05.550
people have nothing to eat.
It's an important issue too to think about.
Yeah,

543
00:33:06.170 --> 00:33:07.270
I would say that,
you know,
the,

544
00:33:07.640 --> 00:33:12.590
the experience of history is that technology has made the world a better place

545
00:33:13.790 --> 00:33:15.800
every single time throughout history.

546
00:33:15.890 --> 00:33:18.580
And there's no reason to suppose that the Lord the last have is going to be any

547
00:33:18.590 --> 00:33:21.500
different.
But it doesn't mean that there's not bumping.
And sort of the pen,

548
00:33:21.501 --> 00:33:24.980
right?
But that's a sort of a linear view of the world.

549
00:33:25.310 --> 00:33:29.570
I think if you lived in Germany in the 1930s technology wasn't making the world

550
00:33:29.571 --> 00:33:30.320
a better place.

551
00:33:30.320 --> 00:33:33.200
Certainly the experience of the Soviet Union didn't make the world a better

552
00:33:33.201 --> 00:33:34.760
place.
So you can,

553
00:33:34.970 --> 00:33:38.270
you can find examples of how the industrial revolution work and you can find

554
00:33:38.271 --> 00:33:41.880
examples of how industrial revolution failed.
And even,
you know,

555
00:33:41.900 --> 00:33:45.430
I talk about environmental regulation and the way in which,
uh,

556
00:33:45.470 --> 00:33:47.990
the industrial revolution in that sense has been tamed.

557
00:33:47.991 --> 00:33:50.430
But we know from people like Naomi Wolf that,
you know,

558
00:33:50.450 --> 00:33:52.460
global warming remains a huge problem,

559
00:33:52.461 --> 00:33:56.810
may indeed be the biggest problem of all or with,
with smart smart machines.
So,

560
00:33:57.260 --> 00:34:00.440
um,
I guess it's bumpiness along the way and that's the purpose of,

561
00:34:00.441 --> 00:34:03.950
but it's not a bad,
but my book is a mat.

562
00:34:03.980 --> 00:34:05.660
I use the metaphor of the map,

563
00:34:05.900 --> 00:34:09.140
but it's not like a Google map where you go from point a to B.

564
00:34:09.141 --> 00:34:13.010
It's a much more complicated math.
The future is not linear.

565
00:34:13.700 --> 00:34:18.230
And just as we move forward.
So there'll be new challenges,
new opportunities.

566
00:34:18.231 --> 00:34:21.350
There are many routes into the future,
many opportunities.

567
00:34:21.351 --> 00:34:25.820
So I think there was also no singular route.
As you know,

568
00:34:25.850 --> 00:34:28.640
the book is,
you know,
it deals with Dystonia in Germany,

569
00:34:28.641 --> 00:34:31.070
in Europe and the u s in India and Singapore.

570
00:34:31.190 --> 00:34:36.190
And I suggest that there are many routes and that the idea that the internet

571
00:34:38.210 --> 00:34:41.180
creates a one world global village,

572
00:34:41.181 --> 00:34:44.750
I think has been proved to be wrong now doesn't mean I glorify what's happening

573
00:34:44.751 --> 00:34:45.830
in China or Russia,

574
00:34:46.100 --> 00:34:49.670
but I think we have to acknowledge the reality of the splinter net that is the

575
00:34:49.671 --> 00:34:52.670
future of the digital world for better or worse.
Okay.

576
00:34:52.671 --> 00:34:55.040
We're going to get some questions from the audience in a second.
So get ready,

577
00:34:55.041 --> 00:34:57.410
but just,
I just,
just before that,
just some quick questions.

578
00:34:57.411 --> 00:35:01.220
What one word answers or actually at a symbols or sums up big sums up our big

579
00:35:01.221 --> 00:35:04.700
thumbs down.
Okay.
Take some APP or pick them down.
Bitcoin

580
00:35:08.730 --> 00:35:13.420
and what says
on balance?

581
00:35:13.421 --> 00:35:16.530
Are you in favor or is it a good thing or a bad thing?
More.

582
00:35:16.560 --> 00:35:20.260
More or less than 50% favorability.
What can I say that?

583
00:35:20.261 --> 00:35:24.250
But then I would say for blockchain,
that okay to do that.
Okay.
Uber,

584
00:35:26.590 --> 00:35:31.120
I use it all the time,
up and down,

585
00:35:32.610 --> 00:35:35.830
up.
Especially since Travis isn't there anymore.

586
00:35:36.220 --> 00:35:40.890
Adblock plus up up.
It's amazing.
What do you think?
But you,

587
00:35:41.130 --> 00:35:45.060
you've appropriated that,
right?
I haven't you built that into chrome while we,
we,

588
00:35:45.061 --> 00:35:46.200
we've acknowledged that.

589
00:35:46.201 --> 00:35:49.380
The reason we ask you this question since we're all family here,

590
00:35:50.660 --> 00:35:55.660
what's on you killing your own business by introducing adblock into,

591
00:35:56.640 --> 00:35:58.250
into crow,
trying to make the,

592
00:35:58.270 --> 00:36:02.070
the improved the overall ecosystem in which we acknowledged the real problems

593
00:36:02.420 --> 00:36:07.280
and that the political next thumbs up or thumbs down.
Karl Marx,

594
00:36:09.090 --> 00:36:13.410
I'm still on Facebook or you know that one.
Google.

595
00:36:14.660 --> 00:36:18.330
I,
since I'm here in New Orleans,
I,

596
00:36:18.630 --> 00:36:22.850
I'm as I'm as Google does anyone,
I probably use it a million times a day.
Right?

597
00:36:23.120 --> 00:36:27.360
What I don't have though is a Google account.
I had to do this interview on,

598
00:36:28.080 --> 00:36:31.960
you still have this thing.
What is it?
Google chat.
Uh,
there was a guy,
you,

599
00:36:31.980 --> 00:36:36.980
what's the thing you screwed up in social media plus year or Google chat room or

600
00:36:37.930 --> 00:36:41.580
something,
but I didn't have a Google account.
I don't use Gmail.

601
00:36:41.880 --> 00:36:46.110
So I sort of have this romantic idea that none of you are watching me,

602
00:36:46.111 --> 00:36:50.130
although I'm sure you are because I just use the search engine without having a

603
00:36:50.131 --> 00:36:55.110
GML or account or a chrome account.
So,
but I,
you know,
Google has,

604
00:36:55.350 --> 00:36:56.940
I couldn't write my books without Google.

605
00:36:56.970 --> 00:37:01.080
I could write my books without Facebook if I'm not on Facebook.
So yeah.

606
00:37:01.920 --> 00:37:04.910
Questions.
Oops.
Sorry.

607
00:37:05.140 --> 00:37:09.250
Microphone and very fair questions by the way.
Peter is a gentleman.

608
00:37:13.540 --> 00:37:17.230
<v 3>Hey Andrew.
Thanks.
Bye.
Thanks for writing another great book about us.
Thank you.</v>

609
00:37:18.250 --> 00:37:21.520
I think it's a great book.
You'll be ready.
I've just skimmed a little bit of it.

610
00:37:21.521 --> 00:37:22.770
It's enough.
Um,

611
00:37:23.180 --> 00:37:26.320
I was actually skimming the bits about to start her and in Brussels.

612
00:37:26.620 --> 00:37:30.880
I think everyone in this room would agree that there is a need for regulation.

613
00:37:30.881 --> 00:37:33.130
And,
uh,
in some of the examples that you cite,

614
00:37:33.131 --> 00:37:37.150
things like the Corvair or meet safety,
food safety regulations,

615
00:37:37.390 --> 00:37:40.810
even antitrust,
going back to standard oil,
it was always based on evidence.

616
00:37:40.900 --> 00:37:44.140
And I think that what I see a little bit in the bits of your book that I just

617
00:37:44.141 --> 00:37:46.600
skimmed and a lot of this discussion around these issues,

618
00:37:46.780 --> 00:37:50.080
there's a lot of rhetoric,
a lot of argument,
not a lot of evidence.
Wow.

619
00:37:50.081 --> 00:37:54.120
That's the guy has a,
again,
I'm not represent,
she would just relying on her,

620
00:37:54.121 --> 00:37:54.800
right.

621
00:37:54.800 --> 00:37:59.370
<v 0>Well,
she's not some schmoe off the street.
I mean she has what,</v>

622
00:37:59.371 --> 00:38:04.180
three as you trust investigations,
have you guys,
um,
and the stuff on,

623
00:38:04.200 --> 00:38:07.380
you know,
you know,
I,
wow.

624
00:38:08.410 --> 00:38:10.920
So I'm not sure whether we can have a,
you know,
uh,

625
00:38:11.490 --> 00:38:14.880
whether we have a compromise here,
but,
um,
you know,

626
00:38:14.881 --> 00:38:19.140
I mean the stuff on travel,
I mean,
that stuff is real.
That's,
let me,

627
00:38:19.260 --> 00:38:20.280
<v 3>let me put it another way.</v>

628
00:38:20.281 --> 00:38:23.610
What we've always had is a press or people like you who write books,

629
00:38:23.611 --> 00:38:28.080
you're actually are independent and look at these issues in an independent way.

630
00:38:28.300 --> 00:38:32.580
Um,
there was evidence that the Corvair was deadly before regulators intervened.

631
00:38:32.700 --> 00:38:36.070
And it wasn't just Ralph Nader standing up saying like,
I don't like this car.
We

632
00:38:36.070 --> 00:38:39.910
<v 0>actually based regulation on a system of rules and a system of evidence.</v>

633
00:38:39.911 --> 00:38:44.260
And I think what's interesting about the internet is that nobody feels obliged

634
00:38:44.261 --> 00:38:47.260
to use those standards anymore.
Maybe the standards should shift,

635
00:38:47.440 --> 00:38:50.930
but I don't think that the discussion,
but let's use the example of Uber.

636
00:38:50.990 --> 00:38:51.410
You know,

637
00:38:51.410 --> 00:38:55.560
recently I saw a report this week that suggested that the average Uber driver

638
00:38:55.561 --> 00:38:59.750
runs three,
three pangea $3 50 an hour.

639
00:39:00.920 --> 00:39:05.190
Um,
and in the book I talk about the way in which again,
I won't pick on it.

640
00:39:05.200 --> 00:39:09.860
Were there other sharing companies do the same thing on not conforming to laws

641
00:39:09.861 --> 00:39:14.180
in terms of protecting the rights of their workers?
I mean,
these are real things.

642
00:39:14.181 --> 00:39:18.140
I mean,
they're not to be ignored.
It doesn't make them necessarily robber barons.

643
00:39:18.141 --> 00:39:22.370
They may not be equivalent of the textile mills where people lose their arms.

644
00:39:22.810 --> 00:39:27.590
Um,
and you know,
11 year olds slave away,
but still it's a problem.

645
00:39:27.620 --> 00:39:30.820
These are issues that need to be resolved.
Um,
you know,

646
00:39:30.830 --> 00:39:35.030
fake news is another huge issue that has implications in our politics and our

647
00:39:35.031 --> 00:39:37.860
culture.
Uh,
the impact of,
you know,

648
00:39:37.861 --> 00:39:41.930
a technology in terms of addiction is also very real.

649
00:39:41.931 --> 00:39:45.740
So I think it's unfair to say that those things,
right,
they have to,

650
00:39:46.010 --> 00:39:48.260
each one has to be this.
No,
I agree.
But you,

651
00:39:48.261 --> 00:39:51.050
so you're particularly talking about antitrust.
I mean,
look,
you know,

652
00:39:51.051 --> 00:39:53.920
a little bit more,
you'd kill me on an attic trust argument,
but I,

653
00:39:53.930 --> 00:39:56.030
I'm not sure you'd kill vaster guy.
I mean,

654
00:39:56.240 --> 00:39:59.630
she's killing it with this work in progress.
Um,
question here.

655
00:40:00.060 --> 00:40:02.680
<v 4>Ah,
hi.
Ah,
I want to ask a question.</v>

656
00:40:02.681 --> 00:40:06.780
So you're kind of what to be bitten to tech companies and responsibility for

657
00:40:06.781 --> 00:40:10.950
fake news,
lack of democracy is spread,
Jim of sexism,
racism and so on and so on.

658
00:40:11.250 --> 00:40:14.880
But,
oh,
you kind of mentioned it in your talk and uh,

659
00:40:14.910 --> 00:40:17.100
but isn't like the tech companies,

660
00:40:17.190 --> 00:40:20.850
they are the light that shone and showed that there are problems because they're

661
00:40:20.851 --> 00:40:25.560
always been there.
And just the smartphones gave access to,
um,

662
00:40:25.650 --> 00:40:27.630
express themselves to Uber drivers,

663
00:40:27.631 --> 00:40:32.270
which was before reserved to Oxbridge educated white,
uh,

664
00:40:32.620 --> 00:40:36.360
uh,
a man who are,
you know,
writing in the times.

665
00:40:36.690 --> 00:40:40.340
Now everyone can express themselves and suddenly we see all this,
you know,

666
00:40:40.640 --> 00:40:44.960
<v 0>five.
Yeah,
I'd be making this change.
I,
I feel,
you know,
I've,</v>

667
00:40:44.980 --> 00:40:48.650
I've been arguing against the idea that somehow it's that the elderly,

668
00:40:48.651 --> 00:40:52.190
I mean you have new elites.
Firstly,
I mean,
you have people with,
you know,

669
00:40:52.191 --> 00:40:55.130
millions of Twitter followers,
huge youtube following.

670
00:40:55.370 --> 00:40:58.190
So we have a new elite that maybe it's not an Oxbridge or leave,

671
00:40:58.220 --> 00:41:01.760
although I'm sure some of these people went to the top universities.
In fact,

672
00:41:01.761 --> 00:41:04.650
if anything,
I think that we have,
uh,
um,

673
00:41:05.360 --> 00:41:09.140
a sort of a narrow elite and the disappearance of,
of,
of a middle in,
in,

674
00:41:09.141 --> 00:41:12.440
in this culture.
And I'm not blaming,
we'll take,
I mean,

675
00:41:12.441 --> 00:41:17.210
you can't blame Facebook or youtube when people,
you know,

676
00:41:17.211 --> 00:41:21.650
when people do really nasty things,
but I do think that I'm the low,

677
00:41:21.651 --> 00:41:24.770
there needs to be more accountability because otherwise,
how'd you clean it up?

678
00:41:25.150 --> 00:41:30.050
Um,
the problem is,
again,
without wishing to acknowledge their immediate company,

679
00:41:30.051 --> 00:41:31.310
they're not hiring curators.

680
00:41:31.311 --> 00:41:35.580
And the only way to fix the problem is not an algorithm but through people.

681
00:41:35.970 --> 00:41:37.740
And it's also a way of creating jobs.

682
00:41:40.170 --> 00:41:43.450
<v 4>Hi.
I wanted to touch on what you said on ubi.
Um,</v>

683
00:41:43.500 --> 00:41:46.320
and I agree with you that we have a certain amount of responsibility for

684
00:41:46.321 --> 00:41:49.550
disruption in the job market and automation,
all of that kind of thing.
Um,

685
00:41:49.590 --> 00:41:52.800
but I would argue that actually we need to think even bigger than ubi and think

686
00:41:52.801 --> 00:41:56.730
about how much actually what we put a monetary value on.
So for instance,

687
00:41:56.790 --> 00:41:59.700
and there's also like a feminist argument here that a lot of care work,

688
00:41:59.760 --> 00:42:00.181
et Cetera,

689
00:42:00.181 --> 00:42:03.810
doesn't have a monetary value or sort of stuff that you could do emotional
labor,

690
00:42:04.340 --> 00:42:08.370
the good,
good in society that we don't put a monetary value on.

691
00:42:08.700 --> 00:42:11.010
And actually society should be valuing that.

692
00:42:11.011 --> 00:42:13.530
So actually that was kind of swayed of things.

693
00:42:13.531 --> 00:42:17.280
I think that it isn't controversial to say we should have a universal basic

694
00:42:17.550 --> 00:42:19.720
income for those people.
And what are your thoughts on that?

695
00:42:20.210 --> 00:42:21.560
<v 0>Well,
I think that,
um,</v>

696
00:42:23.240 --> 00:42:28.240
what the smart machines and doing is making us realize that certain human things

697
00:42:30.531 --> 00:42:33.950
like empathy and sympathy and the ability to communicate,

698
00:42:34.220 --> 00:42:37.670
these are the things that we need to focus on.
And I think we need reforms.

699
00:42:37.671 --> 00:42:38.790
And in my education chapter,

700
00:42:38.791 --> 00:42:43.460
I focus on this with Waldorf education and other humanistic traditions.

701
00:42:43.790 --> 00:42:47.410
So I think you're absolutely right and I think that,
you know,

702
00:42:47.950 --> 00:42:50.750
it's clearly technology is beneficial in many ways.

703
00:42:50.751 --> 00:42:53.300
I mean the invention of the washing machine was one of the most remarkable

704
00:42:53.301 --> 00:42:58.070
inventions in human history.
Liberated women from,
you know,
cleaning clothes,

705
00:42:58.071 --> 00:43:02.420
which is a remarkable step forward.
Obviously no one would ever criticize that.

706
00:43:02.630 --> 00:43:07.610
I think you're absolutely right.
Um,
so the,
the key though is to figure out how,

707
00:43:07.611 --> 00:43:09.410
if you'd like to monetize empathy,

708
00:43:10.040 --> 00:43:13.730
what kinds of new companies are going to exist that build that business models

709
00:43:13.731 --> 00:43:17.690
around empathy,
the services.
And the product.
So you know,
you may,

710
00:43:17.691 --> 00:43:18.730
it's the old dog human.

711
00:43:18.780 --> 00:43:23.270
You may have machines that can detect sickness and illness and disease,

712
00:43:23.600 --> 00:43:26.300
but you won't have machines that can tell someone that they're sick.

713
00:43:27.380 --> 00:43:30.020
That's always going to be a human thing.
I have you want to quit?

714
00:43:30.021 --> 00:43:32.840
You asked the question of what?
What are humans good for?

715
00:43:33.170 --> 00:43:36.560
What are humans good for?
What I think we humans are good for agency.

716
00:43:36.590 --> 00:43:40.340
Humans are good.
Eh,
this might seem a,
an avoidance of the question,

717
00:43:40.341 --> 00:43:43.600
but humans are good for the very things that smart machines can't do.
And,

718
00:43:43.620 --> 00:43:48.500
and the smart machine age.
Now the idea I,
I'm a little wary of defining humanity.

719
00:43:48.501 --> 00:43:52.790
It's such a big word and it's so kind of meaningless and amorphous.

720
00:43:53.240 --> 00:43:57.200
So I argue that every age we have a different definition of what humanity is.

721
00:43:57.201 --> 00:44:02.201
And I suggest in the 21st century what it means to be human is being able to do

722
00:44:02.721 --> 00:44:07.310
things that smart machines can't too.
So it's,
it's the empathy,

723
00:44:07.340 --> 00:44:12.140
it's the,
it's the agency stuff that,
that I bet that licensed the book.

724
00:44:12.320 --> 00:44:13.910
I do have one question for you guys.

725
00:44:14.620 --> 00:44:18.140
Maybe you can answer or maybe there's someone in the audience I'm interested in,

726
00:44:18.141 --> 00:44:19.290
um,
in the,

727
00:44:19.340 --> 00:44:23.420
I do have one section on kind of moral responsibility of the new elite.

728
00:44:23.900 --> 00:44:25.840
What you think the responsibility,

729
00:44:25.860 --> 00:44:29.570
not necessarily of you individually but of the senior executives at this
company,

730
00:44:29.571 --> 00:44:32.520
particularly uh,
the founders of the company,

731
00:44:32.850 --> 00:44:35.850
how they should actually be an,
I want to just pick on them.

732
00:44:35.851 --> 00:44:40.140
I mean you could include Zuckerberg as well and Benioff and be Zos what they

733
00:44:40.141 --> 00:44:45.141
should be giving back to society and whether the analogy of a Carnegie is a

734
00:44:45.961 --> 00:44:48.780
useful one in 19th century rubber bearings.
So,
you know,

735
00:44:48.781 --> 00:44:52.230
I thought that section was was interesting.
I knew you spoke,
uh,

736
00:44:52.680 --> 00:44:54.270
pleasantly about deep mind.

737
00:44:56.190 --> 00:44:58.370
It gets good press on the I actually,

738
00:44:58.630 --> 00:45:02.910
interesting you didn't mention Sundar Pichai at all in the book can again,

739
00:45:02.911 --> 00:45:04.170
I think because I had,
well yeah,

740
00:45:04.171 --> 00:45:06.150
I think you shoot on it and I think actually soon that's probably,

741
00:45:06.170 --> 00:45:10.590
I would probably agree broadly with your,
your five principles.
I mean the way,

742
00:45:10.610 --> 00:45:14.190
what about Larry and Sergei?
I,
I think they would to an Eric,

743
00:45:15.350 --> 00:45:19.060
everyone,
the whole guy.
I make it very clear in the book,

744
00:45:19.080 --> 00:45:21.800
and I'm not against the market and,
and I'm not against capitalism,

745
00:45:22.190 --> 00:45:24.290
but on the other hand that doesn't justify everything.

746
00:45:24.291 --> 00:45:26.420
I just did an interview with Chris Shoes,

747
00:45:26.421 --> 00:45:29.000
the cofounder of Facebook in the beginning of his book.

748
00:45:29.001 --> 00:45:33.380
He said he made half a billion dollars for three years of work as an Undergrad

749
00:45:33.620 --> 00:45:37.580
at Harvard because he happened to share a dorm room with Mark Zuckerberg,

750
00:45:37.820 --> 00:45:40.610
got very lucky,
acknowledged.
He got very lucky.
He says,

751
00:45:40.611 --> 00:45:44.440
that's wrong and I think your point is a fair one.
I might look,

752
00:45:44.460 --> 00:45:46.250
my book can't deal with everything.

753
00:45:46.550 --> 00:45:49.400
Pick a tea has already written and a number of other economists have written

754
00:45:49.401 --> 00:45:51.920
important books about the inequality,

755
00:45:51.921 --> 00:45:56.060
seemingly inherent in contemporary capitalism.
I,

756
00:45:56.070 --> 00:45:59.150
I'm not an economist and I couldn't really address that,
but I,
I agree.

757
00:45:59.151 --> 00:46:04.151
I think the problem is one of contemporary capitalism,

758
00:46:04.191 --> 00:46:09.191
but increasingly it's becoming digital capitalism and the kind of wealth that's

759
00:46:09.411 --> 00:46:13.670
being created for individuals.
I think it's just unhealthy for everyone,

760
00:46:13.671 --> 00:46:15.470
including them.
I mean in the book,
I suggest,

761
00:46:15.471 --> 00:46:18.620
I think the nine wealthiest people in Silicon Valley,

762
00:46:19.310 --> 00:46:21.770
if you add up all their wealth,
it's the same as you know,

763
00:46:21.771 --> 00:46:24.800
2 billion people in the world.
That's just unacceptable.
Now,

764
00:46:24.801 --> 00:46:27.380
you can't blame silicon valley for that,
that playing by the same rules.

765
00:46:27.381 --> 00:46:30.110
They're not,
they're not acquiring that wealth illegally,

766
00:46:30.440 --> 00:46:34.670
but it is a huge problem and ultimately it doesn't benefit the tech community

767
00:46:34.970 --> 00:46:38.240
because you have the vilification of tack.
Tack is,
you know,

768
00:46:38.241 --> 00:46:43.241
when you have Bernie Sanders and Elizabeth Warren and Ted Cruz and Steve Baron

769
00:46:44.090 --> 00:46:47.480
all talking about antitrust,
all saying that there's a problem in silicon valley,

770
00:46:48.020 --> 00:46:50.810
then there's a problem.
Last question here.

771
00:46:53.160 --> 00:46:57.300
<v 5>Um,
you mentioned that one of your assumptions is that at some point consumers or</v>

772
00:46:57.301 --> 00:47:01.680
users will rebel.
Um,
what do you think would be the tipping point?

773
00:47:01.740 --> 00:47:02.573
What would it take?

774
00:47:03.050 --> 00:47:04.910
<v 0>What does I said I,
I think,
uh,</v>

775
00:47:05.030 --> 00:47:08.300
a major when it comes to say your business model,

776
00:47:08.810 --> 00:47:12.980
a major data event,
uh,
David Kirkpatrick,

777
00:47:13.970 --> 00:47:18.330
who was a very well known and responsible respect to tech journalists.

778
00:47:18.331 --> 00:47:21.200
These used to be the tech editor of fortune.

779
00:47:21.201 --> 00:47:26.150
He wrote the book on the history of Facebook.
I bumped into him at ces,
uh,
in,

780
00:47:26.151 --> 00:47:26.990
in January.

781
00:47:27.350 --> 00:47:31.060
He said to me that there's rumors that the Chinese government is actually

782
00:47:31.210 --> 00:47:34.270
acquiring all the data of everyone in America.

783
00:47:34.271 --> 00:47:38.650
And that's a sort of form of economic or data wall.
Now,
I dunno if that's true,

784
00:47:38.651 --> 00:47:42.550
but people like Coke Patrica talking about that it becomes a reality and it's

785
00:47:42.551 --> 00:47:45.670
clear with what the Russians have done in terms of American democracy.

786
00:47:45.671 --> 00:47:48.850
It's clear what the Russians are doing with every election in Europe,

787
00:47:48.851 --> 00:47:51.080
from Italy to check us,

788
00:47:51.270 --> 00:47:54.670
check Czech Republic to the UK and Brexit,
that

789
00:47:56.270 --> 00:48:00.910
that there are a lot of problems with what's going on in the day to work in

790
00:48:00.940 --> 00:48:04.090
politics.
So I think stuff,
you never know what's going to happen,

791
00:48:04.180 --> 00:48:08.470
but stuff will happen.
Stuff so serious that it will wake people up.
You know?

792
00:48:08.471 --> 00:48:11.170
No one could have,
I guess you could have predicted to know well,

793
00:48:11.171 --> 00:48:12.760
but no one did until it happened.

794
00:48:13.120 --> 00:48:16.030
And then it seemed obvious and inevitable and the same will happen.

795
00:48:16.300 --> 00:48:20.620
I think your champion,
this is really my message to Google,
is your dominant.

796
00:48:20.710 --> 00:48:23.860
You've won,
you think you've won or I'm not taking you.

797
00:48:23.861 --> 00:48:28.560
But it seems to me as if you think your,

798
00:48:29.200 --> 00:48:33.880
you're so far ahead that you can't be caught.
I think as always in human history,

799
00:48:33.881 --> 00:48:36.720
the rules of the game change with,
you know,

800
00:48:36.760 --> 00:48:39.880
Harold Macmillan's famous remarks about,
you know,
why,

801
00:48:40.150 --> 00:48:41.710
why did you change your mind?
He said,

802
00:48:41.740 --> 00:48:45.160
events dear boy events and that's what's going to happen here.
Events,

803
00:48:45.190 --> 00:48:49.630
they're people events and then you will see,
I don't know what it's going to be.

804
00:48:49.630 --> 00:48:52.660
I have no idea.
But digital is becoming central and everything,

805
00:48:52.661 --> 00:48:54.230
particularly in state to state relations.

806
00:48:54.231 --> 00:48:57.610
And my guests will be that somehow China will be involved.

807
00:48:57.611 --> 00:48:59.860
China is a much bigger threat,

808
00:48:59.920 --> 00:49:04.270
I think both to Google and to the u s and the West.
And then Russia.

809
00:49:04.271 --> 00:49:07.570
I think we got it wrong with Russia.
I mean they are,
of course,
they're like,
uh,

810
00:49:07.800 --> 00:49:11.710
a fly.
There were annoying flying and,
and,
and,
and China is much larger,

811
00:49:11.711 --> 00:49:12.730
much more destructive,

812
00:49:12.970 --> 00:49:16.540
much more problematic in the long run and much more sophisticated in their use

813
00:49:16.541 --> 00:49:20.730
of data
without [inaudible].
Thank you very much.
Thank.

