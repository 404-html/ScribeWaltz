WEBVTT

1
00:00:06.240 --> 00:00:09.510
On behalf of Google and the many Googlers here and those tuning in around the

2
00:00:09.511 --> 00:00:11.760
world.
Welcome.
It's great to have you here.

3
00:00:12.060 --> 00:00:16.020
We're also fortunate to be here in conversation with Clement Wolf,
Clement leads,

4
00:00:16.021 --> 00:00:19.860
Google's global public policy strategy on a number of search and economic
issues,

5
00:00:20.070 --> 00:00:22.890
notably including the economic impacts of machine learning.

6
00:00:23.580 --> 00:00:26.760
And with that I'll let you guys take it away.
Jerry Kaplan,
everyone.

7
00:00:33.240 --> 00:00:35.070
<v 2>Well,
again,
thank you so much for being here with us today.</v>

8
00:00:35.071 --> 00:00:39.060
I don't think you could be in front of a crowd that cares more about the impacts

9
00:00:39.061 --> 00:00:42.030
of Ai and where it's going.
Uh,
I mean this is not your first Rodeo.

10
00:00:42.031 --> 00:00:46.280
You've been here before,
so I know you know how this goes.
Um,
I think we have a,

11
00:00:46.350 --> 00:00:50.220
an amazing opportunity to chat with you for an hour and delve into some of the

12
00:00:50.221 --> 00:00:52.830
things you discussed your in,
in your book as a start.

13
00:00:52.831 --> 00:00:56.460
It would be great to hear how that specific book came to be.

14
00:00:56.700 --> 00:01:00.450
What drove you to think about it?
Why write about Ai Right now?

15
00:01:01.180 --> 00:01:03.760
<v 3>Well,
the,
um,
the answer is not that interesting.</v>

16
00:01:05.730 --> 00:01:10.210
Um,
uh,
Oxford University press has a series called what everyone needs to know.

17
00:01:10.420 --> 00:01:13.720
Just so you know,
it's,
it's such a pompous title.
I did not make it up myself.

18
00:01:14.090 --> 00:01:18.640
And so they,
they do it on every subject.
You know,
Islam,
the Federal Reserve,
uh,

19
00:01:18.820 --> 00:01:22.870
global warming,
they have,
uh,
it's a book that's written.

20
00:01:22.900 --> 00:01:26.800
These books are short.
They're easy to read,
they're not dumb down,

21
00:01:26.830 --> 00:01:30.940
but a clear and simple explanation of a particular topic.

22
00:01:30.941 --> 00:01:33.070
So you know enough to go to a party and be a know it all.

23
00:01:33.610 --> 00:01:36.010
And so that's really what this about.

24
00:01:36.040 --> 00:01:39.610
So they asked me if I'd write this book and I thought that was a great

25
00:01:39.611 --> 00:01:40.600
opportunity.
But after that,

26
00:01:40.601 --> 00:01:44.770
I realized that there's really a niche for this book that is unfilled.

27
00:01:45.040 --> 00:01:49.860
There really isn't a book that's a foreign intelligence,

28
00:01:50.090 --> 00:01:52.970
uh,
literate audience,
um,

29
00:01:53.350 --> 00:01:56.830
about artificial intelligence that really explains the whole field,

30
00:01:57.070 --> 00:02:01.600
what it means,
how it works,
et Cetera.
We're lucky for the people on camera.

31
00:02:01.650 --> 00:02:03.720
I can't see this,
but Peter Norvig's here,

32
00:02:04.360 --> 00:02:07.680
luminary in the field and uh,
you know,
he,

33
00:02:07.710 --> 00:02:12.190
he's a real superstar and he's written the book that I learned from,
uh,

34
00:02:13.370 --> 00:02:17.350
uh,
a modern approach to artificial intelligence.
If I had more time,

35
00:02:17.351 --> 00:02:21.890
I would've made it shorter.
But
yeah,

36
00:02:21.940 --> 00:02:26.120
I cut out the last thousand pages of my book to the,
uh,

37
00:02:26.620 --> 00:02:31.210
the point is if you read that book,
it's really a,
it's almost a cookbook,

38
00:02:31.211 --> 00:02:33.790
a technical book covering all the different techniques and all that,

39
00:02:34.090 --> 00:02:37.720
but it doesn't really explain it in a way that,
uh,

40
00:02:37.721 --> 00:02:42.570
for a person who is not an engineer or just not trained as a,
uh,

41
00:02:42.900 --> 00:02:46.570
uh,
in the,
in that,
that those kinds of disciplines.

42
00:02:46.960 --> 00:02:49.270
And so there isn't a book that I'm aware of.

43
00:02:49.300 --> 00:02:53.710
They really takes this and explains it in down to Earth.
Simple language,

44
00:02:54.040 --> 00:02:56.560
but without,
without,
as I say,
dumbing it down,
it's,

45
00:02:56.640 --> 00:03:00.970
we hear it gets into what this is,
is really all about.

46
00:03:01.240 --> 00:03:05.410
So that's why I wrote it.
That's what it's for.
And,
uh,
you're welcome to read it.

47
00:03:05.411 --> 00:03:06.370
It's simple and easy.

48
00:03:06.371 --> 00:03:09.560
And then you can go to any cocktail party and be the life of the party yet.

49
00:03:11.100 --> 00:03:11.933
<v 2>That's a good promise.</v>

50
00:03:12.120 --> 00:03:17.120
And one of the things you started with a and which I think was what the,

51
00:03:17.540 --> 00:03:22.380
he is very interesting is the sort of constant ebb and flow of the,

52
00:03:22.381 --> 00:03:25.730
uh,
history of Ai,
uh,
with,
uh,

53
00:03:26.040 --> 00:03:28.830
a trend for ex ante over promising.

54
00:03:28.860 --> 00:03:32.840
And I think one of the examples you mentioned is a,
this quotes,
um,
my,

55
00:03:32.850 --> 00:03:35.080
my colleague Mike McCarthy,
I think in 1956,

56
00:03:35.640 --> 00:03:40.140
a that's a year from that first conference on Ai,
uh,

57
00:03:40.590 --> 00:03:45.590
significant progress would have been made towards a actual machines emulating

58
00:03:45.841 --> 00:03:50.220
human thinking.
Uh,
so over promising quite constantly,

59
00:03:50.490 --> 00:03:54.690
but then again,
um,
at fun to expost trivialization,

60
00:03:55.260 --> 00:03:59.270
uh,
with the idea that once it's sold,
it's not AI anymore.
Uh,

61
00:03:59.271 --> 00:04:01.290
and so a lot of hype cycles.

62
00:04:01.291 --> 00:04:06.180
And I guess the question for us here today is that you think what we're seeing

63
00:04:06.181 --> 00:04:08.550
here,
uh,
over the past few years,

64
00:04:08.551 --> 00:04:12.780
is one more of these hype cycle that's going to come down eventually or are we

65
00:04:13.020 --> 00:04:15.690
now in something that's really new or something that we're different?

66
00:04:15.691 --> 00:04:17.970
Is happening.
Well,
both of those,
those

67
00:04:17.970 --> 00:04:22.380
<v 3>things are true.
Um,
we are definitely in a hype cycle.
I've lived through them.</v>

68
00:04:22.560 --> 00:04:26.000
This is exhausted.
What they're like,
you'll,
you'll,
you'll know it when,
uh,

69
00:04:26.040 --> 00:04:29.240
when the bubble bursts,
but because that's not,

70
00:04:29.241 --> 00:04:32.790
I can tell you that there isn't very valuable technology and a lot of
incredible,

71
00:04:33.000 --> 00:04:35.400
uh,
effects that this is going to have on society.

72
00:04:35.700 --> 00:04:38.940
And those are not really being talked about in the right way.

73
00:04:38.970 --> 00:04:43.970
Because the problem with AI is it's sort of sexy and exciting and it's full of

74
00:04:45.061 --> 00:04:45.990
Gee whiz.

75
00:04:46.560 --> 00:04:50.730
And we need as a group of scientists and engineers,
which is,

76
00:04:50.731 --> 00:04:54.150
I assume mostly what we have here in the room to get the Gee whiz out of this

77
00:04:54.151 --> 00:04:59.151
field because it's distracting us from really understanding the societal impact

78
00:04:59.401 --> 00:05:02.520
that this kind of technology is going to happen have.

79
00:05:02.700 --> 00:05:07.050
So it's sucking the oxygen out of the conversation when we're talking about the

80
00:05:07.051 --> 00:05:11.010
singularity and machines rising up and taking over.
And of course,

81
00:05:11.011 --> 00:05:15.180
the main question I get when I speak is,
you know,
did you see Westworld?
Uh,

82
00:05:15.181 --> 00:05:17.850
you know,
and uh,
we were joking before this,
uh,

83
00:05:17.940 --> 00:05:21.720
which is true that every presentation has to have a picture of the terminator in

84
00:05:21.721 --> 00:05:24.720
it.
So usually I just put up that picture,
right,
to start to say,
here's,

85
00:05:24.810 --> 00:05:26.850
here's the picture of the chairman and around,
let me get on with the talk.

86
00:05:28.260 --> 00:05:29.700
Because artificial intelligence,

87
00:05:30.090 --> 00:05:32.850
there are different ways to look at it in different ways to frame the problem.

88
00:05:33.120 --> 00:05:37.740
And I've come to the sad conclusion that the current framing,

89
00:05:37.741 --> 00:05:41.130
which is that we're building machines which are growing ever more intelligent

90
00:05:41.640 --> 00:05:45.900
and that these are rising up some kind of a linear,
uh,

91
00:05:46.020 --> 00:05:49.770
objective scale toward a human intelligence.
And Oh my God,

92
00:05:49.771 --> 00:05:51.990
what are we going to do when they start to exceed us?

93
00:05:51.991 --> 00:05:56.460
And then they are going to have do bad things to us potentially,

94
00:05:56.461 --> 00:06:00.800
or they'll get of our control.
This is not a helpful framing of the problem.

95
00:06:01.340 --> 00:06:03.560
And that's because there is no,
they,

96
00:06:03.561 --> 00:06:08.561
they're a much better way to think about all of these things.

97
00:06:08.630 --> 00:06:12.710
And the problems and the benefits is to think about artificial intelligence is a

98
00:06:12.711 --> 00:06:17.711
natural continuation of the longstanding historical process of automating tasks

99
00:06:18.081 --> 00:06:21.830
that require human labor,
effort and attention.

100
00:06:22.580 --> 00:06:24.200
And if you think of it that way,

101
00:06:24.350 --> 00:06:29.060
you can eliminate what's going to happen over the next couple of decades by

102
00:06:29.061 --> 00:06:32.060
going back and looking at the long history of automation over the past two or

103
00:06:32.061 --> 00:06:35.600
300 years.
That wasn't your question,
but that's what I wanted to say.

104
00:06:35.990 --> 00:06:40.100
That was a great
answer.
Great answer to know the question question.

105
00:06:40.290 --> 00:06:44.470
<v 2>That's fine.
And when you go into a different question then,
okay.
Um,
you,</v>

106
00:06:44.730 --> 00:06:49.050
you just mentioned how the idea off the scale with the evolution of the eye

107
00:06:49.051 --> 00:06:53.820
towards human intelligence maybe by us floods,
uh,
however,
in the book,

108
00:06:53.910 --> 00:06:54.450
um,

109
00:06:54.450 --> 00:06:59.360
you certainly discuss the idea that even the idea of human intelligence maybe

110
00:06:59.361 --> 00:07:01.110
flowing in some way.
Absolutely.
Uh,

111
00:07:01.111 --> 00:07:04.410
and I think it's an interesting idea for this audience to discuss.
Uh,
could,

112
00:07:04.440 --> 00:07:05.280
would you mind elaborating a bit?

113
00:07:05.480 --> 00:07:06.313
<v 3>Sure.</v>

114
00:07:07.130 --> 00:07:11.690
There is a mythology which has reinforced this whole hype cycle and AI,

115
00:07:12.080 --> 00:07:15.890
that intelligence is some kind of uh,
uh,
objective,

116
00:07:16.130 --> 00:07:18.410
linear,
measurable quantity.

117
00:07:18.411 --> 00:07:23.411
And this person is Iq is 125 and this one's only 120.

118
00:07:23.990 --> 00:07:27.200
And so there's this person's smarter than that person.

119
00:07:27.740 --> 00:07:30.970
But let me just talk about Iq because that,

120
00:07:30.971 --> 00:07:32.990
that's a great illustration of why this is a myth.

121
00:07:33.230 --> 00:07:35.300
If you talk to the people who developed Iq,

122
00:07:35.301 --> 00:07:37.130
the psychologists and the deal with this,

123
00:07:37.131 --> 00:07:41.620
it's an intelligent quotient intelligence quotient they call it.
And uh,

124
00:07:41.690 --> 00:07:44.120
it's really not about intelligence at all.
What they'll tell you,

125
00:07:44.121 --> 00:07:47.120
they'll tell you it's a measure of developmental competence,

126
00:07:47.510 --> 00:07:49.640
which is why you mostly apply to young children.

127
00:07:49.670 --> 00:07:53.660
What they mean is do you have a certain set of skills and at what age did you

128
00:07:53.661 --> 00:07:55.750
gain those skills?
Where you capable of doing that?

129
00:07:56.390 --> 00:07:59.630
And so after you were about 15 or 16 years old,

130
00:08:00.500 --> 00:08:04.760
Iq is functionally meaningless on now cause it's,

131
00:08:04.790 --> 00:08:06.680
it's a quotion.
You guys know what that means.

132
00:08:06.830 --> 00:08:11.570
You scored a bunch of tests and you divided by your age and that's your Iq.

133
00:08:11.930 --> 00:08:15.760
Well,
I must be really dumb because I'm really old.
And that's the development.

134
00:08:16.750 --> 00:08:17.360
Uh,

135
00:08:17.360 --> 00:08:22.360
so the hope concept of intelligence as it's understood by the general public is

136
00:08:23.360 --> 00:08:27.530
mistaken.
Uh,
it's really,
uh,
what we think of as intelligence.

137
00:08:27.650 --> 00:08:30.260
It's not an objective measurable quantity.

138
00:08:30.500 --> 00:08:34.370
It's more like our concept of say,
beauty.
You know,

139
00:08:34.371 --> 00:08:37.520
is this person more beautiful than that person?

140
00:08:37.820 --> 00:08:40.850
Now that's a question one can ask and you couldn't even have a,

141
00:08:40.990 --> 00:08:43.340
a meaningful answer.
Yes,
in general,

142
00:08:43.341 --> 00:08:45.440
this person's regarded as more beautiful net person,

143
00:08:45.920 --> 00:08:49.820
but we don't talk about that as a linear scale where a,
if you've noticed,

144
00:08:49.821 --> 00:08:53.000
if you watch movies,
which there are robots,

145
00:08:53.270 --> 00:08:56.650
they'd been becoming better and better looking.
You know what they say?

146
00:08:57.210 --> 00:09:00.930
They start out,
you know,
rob,
he's a robot and they've come along very far,

147
00:09:01.060 --> 00:09:04.740
but we don't have conferences where we're worrying about robots becoming so

148
00:09:04.741 --> 00:09:08.560
beautiful that we no longer want to meet with human beings because we want to

149
00:09:08.790 --> 00:09:13.410
meet with the robots.
Well,
this is a corollary of the mistake.

150
00:09:13.520 --> 00:09:16.140
I'm staking path that were led down with this notion.

151
00:09:16.141 --> 00:09:19.200
The human intelligence is a,
a sort of a flat linear scale.

152
00:09:19.230 --> 00:09:24.230
It's really a series of complicated capabilities and competencies that we have.

153
00:09:25.050 --> 00:09:30.000
I don't think of a spider is really smart because it knows how to spin a web.

154
00:09:30.750 --> 00:09:34.950
And so I don't think of a person is really smart because they know how to

155
00:09:34.951 --> 00:09:37.020
program in Java.
It's uh,

156
00:09:37.021 --> 00:09:40.260
it's really just a set of competencies that one develops.

157
00:09:40.680 --> 00:09:43.500
So now as we apply that to machines,
think about Iq.

158
00:09:44.010 --> 00:09:47.100
If I can continue for a minute,
think about Iq is applied to a machine.

159
00:09:47.670 --> 00:09:49.380
How do you divide by its age?

160
00:09:49.890 --> 00:09:52.950
What happens if you given intelligence test to a machine?
Well,

161
00:09:52.951 --> 00:09:55.440
it can perform calculations by way of example,

162
00:09:55.441 --> 00:09:57.540
one of the key elements of the Iq test,

163
00:09:58.110 --> 00:10:02.310
I mean millions of times faster than human being.
Oh my God.

164
00:10:02.370 --> 00:10:06.300
Is that mean it's going to take over the earth?
No,
of course not.
And you,

165
00:10:06.390 --> 00:10:09.810
how do you divide by machines?
Age?
Oh,
he built that one 15 minutes ago.

166
00:10:09.811 --> 00:10:14.040
So this is Iq must be,
you know,
100,000,
22.
Um,

167
00:10:14.910 --> 00:10:18.360
clearly we shouldn't be thinking about framing the problem in this particular

168
00:10:18.361 --> 00:10:18.960
way.

169
00:10:18.960 --> 00:10:23.960
We're using these machines precisely because they perform jobs or tasks,

170
00:10:25.230 --> 00:10:29.670
not jobs.
That's another part of the conversation better than humans.
Do.

171
00:10:30.090 --> 00:10:34.610
They do them faster or they do them and less expensively or they do them with a

172
00:10:34.800 --> 00:10:36.450
more accurately or whatever it might be.

173
00:10:37.050 --> 00:10:40.200
We use these machines because they exceed human capabilities.

174
00:10:40.600 --> 00:10:43.200
So with the general public worries about,
Oh my God,

175
00:10:43.201 --> 00:10:46.830
what are we going to do when machines get smarter than us?
They already are.

176
00:10:47.310 --> 00:10:51.940
And that's the whole point.
So I'll stop there.
Oh,

177
00:10:52.020 --> 00:10:53.550
don't stop.
That's continuing.

178
00:10:53.640 --> 00:10:56.530
I go on for an hour on this list of points and it's like,

179
00:10:56.550 --> 00:11:01.410
we'll have you here and she knows all the yen smarter than us in some ways.

180
00:11:01.710 --> 00:11:05.250
Uh,
one other upon your ways in which I think is,
um,
is,
uh,

181
00:11:05.310 --> 00:11:09.750
a relevant level comparison is the idea of a machine freewill and machine

182
00:11:09.751 --> 00:11:13.100
consciousness.
You see that as related in any way to intelligence.

183
00:11:13.140 --> 00:11:17.840
Do you see that as a,
as a possibility?
Um,

184
00:11:18.150 --> 00:11:21.600
the,
the real answer is of course,
I don't know,

185
00:11:21.660 --> 00:11:26.430
but it does raise questions when we build machines with these capabilities.

186
00:11:26.431 --> 00:11:29.490
So whether they might ultimately we might ultimately regard them as having

187
00:11:29.760 --> 00:11:33.600
freewill or being conscious.
And,
uh,

188
00:11:33.601 --> 00:11:37.170
there's a lot of interesting work on this in the philosophical literature.

189
00:11:37.200 --> 00:11:40.380
I happen to be a follower of,
uh,
what's his name?

190
00:11:40.540 --> 00:11:44.270
Sam Harris.
Thank you Sam Harris.
Uh,

191
00:11:44.910 --> 00:11:45.631
terrible with names.

192
00:11:45.631 --> 00:11:48.010
The first thing that goes when you get to my age of the names,

193
00:11:48.650 --> 00:11:52.640
it's a pfeifle Q and a name street.
They just strip them out.
You know,
it's,

194
00:11:53.750 --> 00:11:56.610
you know,
it's a hash function just goes down.
Uh,

195
00:11:57.040 --> 00:12:00.020
I can make jokes like that in front of this audience.
Um,

196
00:12:00.250 --> 00:12:04.330
well basically with Sam Harris pointed out in terms of free will,

197
00:12:04.331 --> 00:12:06.520
and I happen to agree with him after a lot of thought,

198
00:12:06.820 --> 00:12:11.230
is that it's fundamentally our common sense notion of what it means to have free

199
00:12:11.231 --> 00:12:16.231
will is fundamentally in conflict with our science current scientific framework

200
00:12:16.420 --> 00:12:19.570
and worldview.
The two just don't fit.

201
00:12:19.690 --> 00:12:24.690
You have to introduce some kind of magic to the idea that independent of the

202
00:12:25.181 --> 00:12:29.620
world around you and independent of all the history of every event that took

203
00:12:29.621 --> 00:12:33.790
place up to a point in time you decided to turn left instead of right.

204
00:12:33.791 --> 00:12:38.020
And you could have done something differently that simply inconsistent with our

205
00:12:38.021 --> 00:12:40.450
scientific worldview.
And I'm not just,

206
00:12:40.480 --> 00:12:43.030
I'm not suggesting that the world is entirely deterministic.

207
00:12:43.610 --> 00:12:48.610
I'm just saying that what does it mean for this mind to be somehow outside of

208
00:12:49.661 --> 00:12:54.661
that or separate from the physical world in such a way that it gets to stop and

209
00:12:54.701 --> 00:12:59.050
think and make a decision that is not in some sense determined by everything

210
00:12:59.051 --> 00:13:03.220
that has come before.
Uh,
so I don't think,

211
00:13:03.250 --> 00:13:06.400
I think there's a good argument that freewill is actually an illusion.

212
00:13:06.430 --> 00:13:11.000
Either that or we do not yet have the scientific framework or,

213
00:13:11.080 --> 00:13:16.080
or ideas that we can use to explain it in a reasonable way and as uncomfortable

214
00:13:16.871 --> 00:13:21.040
as that might be,
I think that's the,
uh,
the conclusion that one has to reach.

215
00:13:21.400 --> 00:13:23.080
And the same thing's true of consciousness.

216
00:13:23.740 --> 00:13:28.040
But underlying both of these questions,
can a machine ever have freewill?

217
00:13:28.120 --> 00:13:32.080
Can a machine be conscious?
Everybody here has seen,
uh,

218
00:13:32.500 --> 00:13:35.650
how many people have seen Westworld?
Okay,
so I can,

219
00:13:36.370 --> 00:13:40.090
there's dozens of films and that basically around this thing.

220
00:13:40.720 --> 00:13:43.270
When does the machine come alive?

221
00:13:43.570 --> 00:13:48.160
When it does it become independent in its thinking?

222
00:13:49.000 --> 00:13:53.410
The really underlying issue in that is at what point,
if ever,

223
00:13:53.710 --> 00:13:58.710
do we owe the courtesy of our empathy to human created or mechanical machines

224
00:14:00.851 --> 00:14:01.684
and devices?

225
00:14:02.110 --> 00:14:06.280
That's really the underlying question because immediately after you see another

226
00:14:06.281 --> 00:14:10.420
TV show like humans,
how many of you have seen humans,
British TV,
one person,

227
00:14:11.590 --> 00:14:14.570
okay,
well I'm not here to promote the humans,
but

228
00:14:17.330 --> 00:14:19.360
the real question always becomes,

229
00:14:19.960 --> 00:14:24.850
are these simply tools which we can abuse and treat the way we treat any other

230
00:14:24.851 --> 00:14:28.240
tool?
I don't think of myself as abusing a hammer when I use it.
Actually.

231
00:14:28.241 --> 00:14:32.870
I guess I could,
I often break hammers and things was this form of abuse.
Um,

232
00:14:33.040 --> 00:14:36.090
but at what point do they in some sense deserve,
uh,

233
00:14:36.100 --> 00:14:40.790
having some semblance of rights in and of themselves.
And,
uh,

234
00:14:40.990 --> 00:14:43.420
that's the reason we worry about this issue.

235
00:14:43.421 --> 00:14:45.790
Whether a machine can have free will or not,

236
00:14:46.390 --> 00:14:50.360
I'm machine can certainly act as though it has free will.
I know I've got,

237
00:14:50.440 --> 00:14:54.660
I've got a book and,
and if that thing doesn't have freewill,

238
00:14:54.690 --> 00:14:59.650
nothing does.
So,
uh,
but I don't feel like I owe it a,

239
00:14:59.710 --> 00:15:03.620
an opportunity go off and just have this little spinning wheel at anytime it

240
00:15:03.621 --> 00:15:06.400
wants to because it needs a break.
I don't have that,

241
00:15:06.401 --> 00:15:09.100
that same sense of empathy that I do for,
uh,

242
00:15:09.170 --> 00:15:11.540
not just for humans but for many other living things.

243
00:15:13.100 --> 00:15:15.200
<v 2>And that's a really good segue into the second part,
I guess,</v>

244
00:15:15.201 --> 00:15:19.970
of that conversation,
which is we now have brushed up on the fact that,
well,

245
00:15:20.570 --> 00:15:24.620
if maybe in a hive cycle,
but things are changing fast either way.
Uh,

246
00:15:24.690 --> 00:15:28.160
and we don't know if there's a huge difference.

247
00:15:28.161 --> 00:15:31.760
So what the difference is between the way humans are intelligence and the way

248
00:15:31.910 --> 00:15:33.620
meshing their intentions.
We know that it's not the same thing.

249
00:15:33.790 --> 00:15:35.720
It's not even on the same scale.
We don't know how to relate them.

250
00:15:36.050 --> 00:15:38.090
We know that consciousness.

251
00:15:38.180 --> 00:15:42.080
And that's a free maybe flawed notions.

252
00:15:42.081 --> 00:15:46.520
So what that makes a very advanced machine that different from a human and in a,

253
00:15:46.630 --> 00:15:51.050
in a,
in a world where they keep getting well more prevalent,

254
00:15:51.180 --> 00:15:53.360
more pervasive and more sophisticated.

255
00:15:53.840 --> 00:15:57.590
What does that mean in terms of legal and economic impacts?

256
00:15:57.591 --> 00:16:01.880
And I think the first stage of that is a legal impact of course sits if we can

257
00:16:01.881 --> 00:16:03.320
say if a machine is really conscious,

258
00:16:03.620 --> 00:16:05.270
if we can't see if a machine is really intelligent,

259
00:16:05.271 --> 00:16:08.630
if we can't say if it's really,
uh,
if,
if it has free will,

260
00:16:08.900 --> 00:16:12.120
then what can we do about questions like ownership,
uh,

261
00:16:12.260 --> 00:16:16.370
which you raise questions like,
uh,
accountability.
Uh,

262
00:16:16.371 --> 00:16:17.840
how does that play out interview,

263
00:16:18.180 --> 00:16:22.650
<v 3>um,
while even though I have been describing the,
uh,</v>

264
00:16:22.680 --> 00:16:27.680
the Ai World as merely an extension of a previous,

265
00:16:28.291 --> 00:16:31.320
uh,
uh,
types of technology,

266
00:16:31.350 --> 00:16:36.350
I do think it raises a number of new and very challenging questions for society.

267
00:16:36.870 --> 00:16:39.090
And we have different ways of dealing with it.

268
00:16:39.180 --> 00:16:43.200
I don't think we have any obligation to,
uh,
to machines.

269
00:16:44.180 --> 00:16:47.670
I don't my car,
uh,
you know,
I may take care of it,

270
00:16:47.671 --> 00:16:50.160
but not because I think it has some rights.

271
00:16:50.610 --> 00:16:53.880
I think the same thing will be true of any machine that we're likely any of us

272
00:16:53.881 --> 00:16:58.770
are like interact with in our,
in our lifetime.
However,
from a legal standpoint,

273
00:16:59.070 --> 00:17:04.070
there are certain legal fictions which are useful in sorting out the primary

274
00:17:04.291 --> 00:17:08.280
question of who's responsible for what.
So as you may know,

275
00:17:08.400 --> 00:17:12.510
corporations have a concept of personhood that is applied to corporations.

276
00:17:12.660 --> 00:17:13.920
It doesn't mean they're people.

277
00:17:14.280 --> 00:17:18.030
It means that there are certain collections of rights and responsibilities that

278
00:17:18.031 --> 00:17:22.530
go hand in hand.
And that's what we call personhood,

279
00:17:22.800 --> 00:17:26.040
even if it's limited.
And,
uh,
there's also interestingly enough,

280
00:17:26.190 --> 00:17:29.790
not all people have the same rights and responsibilities.
Uh,

281
00:17:29.820 --> 00:17:34.050
you guys don't need a license to do machine learning,

282
00:17:34.800 --> 00:17:37.770
but there are in the law,
for example,

283
00:17:38.280 --> 00:17:42.480
you need a license to practice law and you have certain responsibilities and

284
00:17:42.481 --> 00:17:45.720
then you're given certain rights to be able to work in the,
in that profession.

285
00:17:46.380 --> 00:17:48.780
So,
uh,
this notion of a,

286
00:17:49.320 --> 00:17:52.680
of rights and responsibilities is a very powerful one.
Very useful.

287
00:17:52.700 --> 00:17:53.730
In an illegal context.

288
00:17:53.760 --> 00:17:57.930
And I don't think it's at all unreasonable to the SPEC that courts will find

289
00:17:57.931 --> 00:18:00.440
that certain classes of devices,
uh,

290
00:18:00.540 --> 00:18:05.020
could be given legal personhood in the same,
uh,
limited sense in,

291
00:18:05.021 --> 00:18:07.800
in the future.
So that's something that we need to,

292
00:18:07.820 --> 00:18:11.490
that will have to be worked out.
Um,
but here's the good news.

293
00:18:11.491 --> 00:18:14.880
Everybody's been worried about like,
oh my God,
what happens when,
uh,

294
00:18:15.160 --> 00:18:18.150
self driving cars getting a crash?
Who's responsible?
Well,

295
00:18:18.151 --> 00:18:21.090
assigning responsibility is a very important issue,

296
00:18:21.600 --> 00:18:25.470
but it's not such a great philosophical mystery.

297
00:18:25.500 --> 00:18:30.360
There is a tremendous amount of history and work in the legal profession for

298
00:18:30.361 --> 00:18:35.220
sorting out who's responsible for various kinds of accidents from the

299
00:18:35.221 --> 00:18:40.140
manufacturer to the user,
to the,
uh,
the person who sold you a particular device.

300
00:18:40.141 --> 00:18:44.310
Have they explained properly and told you about what the dangers are,
et Cetera,

301
00:18:44.311 --> 00:18:47.370
et cetera.
And the short answer to this question is,

302
00:18:47.670 --> 00:18:50.790
we don't need to worry about it because the lawyers are going to handle this.

303
00:18:51.000 --> 00:18:55.580
They'll sort it out and it's not going to be a big issue.
Uh,

304
00:18:55.620 --> 00:18:58.170
in particular,
if I can come back to self driving car,

305
00:18:58.500 --> 00:19:00.960
we don't need to worry about it because the insurance companies are going to

306
00:19:00.961 --> 00:19:04.770
drive the adoption of this technology for a variety of economic reasons.

307
00:19:05.160 --> 00:19:07.740
And you won't care.
You got into an accident,

308
00:19:08.100 --> 00:19:09.840
the insurance company will handle it.

309
00:19:10.080 --> 00:19:15.080
And whether they go back and say it was a flaw in the design or a flaw in

310
00:19:15.390 --> 00:19:17.060
something else,
or,
uh,

311
00:19:17.400 --> 00:19:21.000
you are drunk and you instructed your car to go run somebody over,
you know,

312
00:19:21.001 --> 00:19:25.620
that's then your re you're responsible.
So to me this is a,
uh,
uh,

313
00:19:25.621 --> 00:19:27.240
as a practical matter,
it's a nonissue.

314
00:19:27.270 --> 00:19:31.590
We have experts who will take care of these things for us in reasonable ways

315
00:19:31.591 --> 00:19:34.720
through the legal system,
but it will require a lot of precedent and,

316
00:19:34.721 --> 00:19:35.554
and future work.

317
00:19:36.050 --> 00:19:39.470
<v 2>Absolutely.
And I guess as I'm one point,</v>

318
00:19:39.471 --> 00:19:43.080
I will respond to that as these things will be handled ultimately,

319
00:19:43.190 --> 00:19:46.520
but they can be handled in more or less effective and more way,

320
00:19:46.850 --> 00:19:50.270
more or less beneficial ways for society and for individuals.

321
00:19:50.810 --> 00:19:54.180
And that's underneath one key consideration,
especially when it comes to the,
uh,

322
00:19:54.200 --> 00:19:58.390
the question of labor.
Uh,
and the question of the,
uh,
uh,

323
00:19:58.460 --> 00:20:02.090
impacts of automation on employments.
Uh,

324
00:20:02.360 --> 00:20:04.610
there has been a lot of discussion about this of course,

325
00:20:05.360 --> 00:20:09.890
and I don't of considerations that there may not be as mean as many jobs 20

326
00:20:09.891 --> 00:20:13.910
years from now as they are to date.
Uh,
what do you stand on that?
Okay,

327
00:20:14.510 --> 00:20:16.670
<v 3>this is a broad and complex subject,</v>

328
00:20:16.671 --> 00:20:20.810
but let me try to quickly summarize this in the spirit of my short answers in,

329
00:20:21.180 --> 00:20:22.820
in,
in my book,

330
00:20:23.000 --> 00:20:26.780
if you view artificial intelligence not as the emergence of some new form of

331
00:20:26.781 --> 00:20:30.650
life that is heading towards singularity and we're all going to suddenly be

332
00:20:30.651 --> 00:20:35.030
raised up and reincarnated in mechanical form and instead think about it as a

333
00:20:35.031 --> 00:20:38.240
continuation of the longstanding process of automation,

334
00:20:38.570 --> 00:20:41.990
then you can look at a two effects the automation of had in the past.

335
00:20:42.050 --> 00:20:46.910
And what we're seeing if certainly continuation of impossibly in

336
00:20:48.430 --> 00:20:51.790
the first is automation changes the nature of work.

337
00:20:52.330 --> 00:20:56.150
So it's true when you read about my God,
50% of the jobs are,

338
00:20:56.290 --> 00:21:00.490
will be gone in 30 or 40 years.
That's the first half of the sentence.

339
00:21:00.640 --> 00:21:05.130
The other half,
that sentence that you never hear is that's normal it,

340
00:21:05.520 --> 00:21:08.260
but people did 20 or 30 years ago is not what they do today.

341
00:21:08.470 --> 00:21:12.640
And there'll be a whole bunch of new jobs and an expansion of other kinds of

342
00:21:12.641 --> 00:21:17.380
jobs for very important economic reasons,
uh,
that will,

343
00:21:17.440 --> 00:21:21.850
uh,
there'll be plenty of work.
So if these two statements are nodding,

344
00:21:21.940 --> 00:21:23.800
not is inconsistent as they sound,

345
00:21:23.801 --> 00:21:28.801
50% of today's jobs might go away and pick your number of decades and uh,

346
00:21:29.170 --> 00:21:33.070
there'll be still be plenty of work and people will still be employed.
You know,

347
00:21:33.130 --> 00:21:36.940
if you just go back and look at everything from,
uh,

348
00:21:37.140 --> 00:21:41.800
I'm going to give you an idea how old I am here.
I remember barely,

349
00:21:41.980 --> 00:21:45.340
but I remember when I was a kid,
you pick up the telephone,

350
00:21:45.610 --> 00:21:50.370
there was no dial and a person would get on the line,
say,

351
00:21:50.640 --> 00:21:54.370
uh,
what do you want?
What number?
Po Number Police,
I think they used to say,

352
00:21:54.640 --> 00:21:59.110
and you would tell him Bigelow,
eight to 4,200.
I mean,
this actually went on.

353
00:21:59.470 --> 00:22:01.060
So there were people,

354
00:22:01.570 --> 00:22:05.710
I think there were a million people I could be mistaken on that employed as

355
00:22:05.711 --> 00:22:08.650
telephone operators and you would tell them and they would plug things.

356
00:22:08.651 --> 00:22:11.830
Right now,
that switching function,
as you're well aware,

357
00:22:11.831 --> 00:22:15.490
has been taken over by the,
it's called Ess,

358
00:22:15.491 --> 00:22:19.480
the electronic switching systems and all those people are out of work.
Oh my God.

359
00:22:19.990 --> 00:22:21.760
Well,
the problem isn't that they're out of work.

360
00:22:21.790 --> 00:22:25.080
It's the nature of the work changed.
And because of that,

361
00:22:25.130 --> 00:22:27.620
that these improvements make us wealthier as,

362
00:22:27.621 --> 00:22:31.990
or just society that creates a lot of demand,
not just for new kinds of jobs,

363
00:22:32.110 --> 00:22:35.100
but for all kinds of jobs.
Let me give you an example that I,

364
00:22:35.410 --> 00:22:40.240
I just noticed these as I go about my,
my humdrum daily life.
Um,

365
00:22:41.920 --> 00:22:46.150
right now getting a massage from a is a bit of a luxury.

366
00:22:46.660 --> 00:22:49.900
I mean,
all of you of course,
can afford this.
Uh,
in fact,

367
00:22:49.901 --> 00:22:51.740
I think they're free here.
Google.

368
00:22:52.540 --> 00:22:55.870
That's one of the reasons I'd like to work here.
I'll just do that part.
I want,

369
00:22:56.080 --> 00:22:59.500
I want the job of testing the new messsage.
That's my,
yeah,

370
00:22:59.520 --> 00:23:01.680
it's going to be my job.
It is.

371
00:23:05.830 --> 00:23:09.160
Sorry.
It doesn't look good on video.
I've learned when you laugh,

372
00:23:09.390 --> 00:23:11.500
you know about that,
these things.
But that is funny.

373
00:23:14.920 --> 00:23:18.160
Okay.
That totally threw me off.
Okay.

374
00:23:18.310 --> 00:23:23.310
Now what happens when you no longer need to buy your own car or request of

375
00:23:23.631 --> 00:23:27.580
transportation drops by 75%.
You have more money.
Well,

376
00:23:27.581 --> 00:23:28.400
what are you going to do with that?

377
00:23:28.400 --> 00:23:33.400
A lot of people are going to step up and want to do things like get massages or

378
00:23:33.790 --> 00:23:35.200
go to the spa for a day,

379
00:23:35.320 --> 00:23:37.630
and that's gonna generate tremendous demand for that profession.

380
00:23:38.200 --> 00:23:41.680
I've been surprised locally.
That's an incredibly well paid job.
You know,

381
00:23:41.720 --> 00:23:43.870
talking about manual labor,
you know,
it's a,

382
00:23:44.290 --> 00:23:47.300
it's a very well paid job and there's going to be tremendous demand in the

383
00:23:47.301 --> 00:23:50.660
future for a massage therapists.
Well,

384
00:23:50.810 --> 00:23:55.610
that's a side effect.
We often think we're destroying a job.
Uh,

385
00:23:55.611 --> 00:23:57.890
what,
what's going to happen?
Well,

386
00:23:57.891 --> 00:24:02.891
the answer is that money goes to expanding and changing the complexion of the

387
00:24:04.371 --> 00:24:05.204
workforce.

388
00:24:05.240 --> 00:24:10.220
So that existing jobs of certain kinds become a much more prevalent and we need

389
00:24:10.221 --> 00:24:11.660
more and more people to do them,

390
00:24:11.670 --> 00:24:16.460
personal shoppers or people to do flower arranging or whatever it might be.

391
00:24:16.850 --> 00:24:18.890
Uh,
so,
uh,
the way you,

392
00:24:18.900 --> 00:24:23.300
it's the labor markets work as they're very resilient and very,
uh,

393
00:24:24.230 --> 00:24:28.340
dynamic.
And so while it's true that jobs are going to go away,

394
00:24:28.490 --> 00:24:30.620
there will be new jobs and more importantly,

395
00:24:30.621 --> 00:24:33.500
there will be more of certain kinds of existing jobs.

396
00:24:33.860 --> 00:24:37.730
And I'm convinced that this pattern is going to play out in the future.

397
00:24:37.731 --> 00:24:41.840
As I thought about it.
That said,
we're putting people out of work.

398
00:24:41.960 --> 00:24:46.370
The other thing about automation to things that they never want to say when

399
00:24:46.371 --> 00:24:47.320
you're,
uh,

400
00:24:47.650 --> 00:24:51.920
an IBM makes its presentations and other tech companies,

401
00:24:52.160 --> 00:24:56.550
we're not putting people out of work.
Uh,

402
00:24:56.930 --> 00:24:57.710
of course they are.

403
00:24:57.710 --> 00:25:01.250
That's the whole point of automation is to put people out of work.

404
00:25:01.790 --> 00:25:04.460
So you're putting people out of work and the question is,

405
00:25:04.520 --> 00:25:08.660
what do you do with those people?
Do we have the,
uh,
social,

406
00:25:09.170 --> 00:25:10.003
uh,

407
00:25:10.340 --> 00:25:15.320
Po and policy frameworks in order to ensure that we're not,

408
00:25:15.321 --> 00:25:19.550
the costs of the automation is not falling disproportionately on certain groups.

409
00:25:19.551 --> 00:25:22.490
People,
what are we going to do with the millions of drivers?

410
00:25:22.610 --> 00:25:24.830
We're going to be out of jobs in the next 10 to 20 years.

411
00:25:25.190 --> 00:25:26.510
We can just cut them loose.

412
00:25:27.110 --> 00:25:30.380
Maybe that's would be the current administration's point of view on something

413
00:25:30.381 --> 00:25:33.020
like this.
I don't know.
But,
um,
you know,

414
00:25:33.021 --> 00:25:35.630
we need to set up mechanisms and it,

415
00:25:35.660 --> 00:25:39.350
it's incumbent on us as a thinking human beings,

416
00:25:39.740 --> 00:25:41.540
empathetic human beings to,

417
00:25:41.660 --> 00:25:46.040
to figure out better and better ways to reincorporate people into the workforce

418
00:25:46.270 --> 00:25:50.990
in,
in improper ways.
In a sense.
This is what went wrong with globalization.

419
00:25:51.670 --> 00:25:52.070
Uh,

420
00:25:52.070 --> 00:25:56.810
the benefits accrued to a certain small group of people and the costs,

421
00:25:57.230 --> 00:25:58.063
I'm sorry,

422
00:25:58.080 --> 00:26:01.460
the benefits accrued broadly cross much of society and some people benefited

423
00:26:01.461 --> 00:26:02.294
tremendously,

424
00:26:02.540 --> 00:26:06.290
but there was a small group of people maybe like the steel industry or the coal

425
00:26:06.291 --> 00:26:10.940
industry there were devastated and we didn't pay attention to that and now we're

426
00:26:11.120 --> 00:26:13.310
suffering that and the backlash from that.

427
00:26:13.610 --> 00:26:16.940
The same thing is going to be true with the rollout of a lot of the work that's

428
00:26:16.941 --> 00:26:20.780
going on right here at Google.
You guys are going to be fine.

429
00:26:21.050 --> 00:26:22.430
It's not going to be a problem,

430
00:26:23.090 --> 00:26:27.620
but you're going to be replacing people who may not have the opportunities to

431
00:26:27.621 --> 00:26:31.640
retrain or to become,
continue to be productive members of society.

432
00:26:31.760 --> 00:26:33.860
And if we don't address this on a policy level,

433
00:26:34.160 --> 00:26:37.180
we're going to see the same kind of backlash as an [inaudible].

434
00:26:37.330 --> 00:26:41.140
And that's one of the thing we spend enough time thinking about.
So,
uh,
are they,

435
00:26:41.141 --> 00:26:43.980
let's we can do to help people of skill to,

436
00:26:43.980 --> 00:26:48.360
<v 2>uh,
think of new types of social safety nets.
And there's,
of course,</v>

437
00:26:49.440 --> 00:26:54.150
I don't think we've seen a compelling and a full proof on suggest yets.

438
00:26:54.380 --> 00:26:56.970
Uh,
I think a lot of people around the world are thinking about that.

439
00:26:57.270 --> 00:26:58.960
It was interesting to me to see that.
Um,

440
00:26:59.640 --> 00:27:04.200
I mean I'm a French of obviously that's my accent's uh,
the um,

441
00:27:04.380 --> 00:27:09.330
the winner of the left wing primary for the French election urban while I'm on a

442
00:27:09.331 --> 00:27:12.000
one on the program that's based on Ubi,
uh,

443
00:27:12.001 --> 00:27:15.540
in response to the prospects of technological change.

444
00:27:15.780 --> 00:27:19.080
What's interesting to me is that it comes from an optimistic perspective.

445
00:27:19.200 --> 00:27:24.060
His view is that it's a good thing if people work less working less generally.

446
00:27:24.061 --> 00:27:27.210
Okay.
Uh,
on the issue is how do we share the benefits of that?

447
00:27:27.300 --> 00:27:30.690
And so he believes ubi maybe a solution for that.

448
00:27:31.110 --> 00:27:33.750
That's a whole other conversation that I'd love to have with you,

449
00:27:33.751 --> 00:27:36.960
but it's already a 36 and I want to leave time for the audience.

450
00:27:36.961 --> 00:27:41.310
I guess the last question I would selfishly ask before opening it up is we've

451
00:27:41.311 --> 00:27:42.780
talked a lot about some of the states,

452
00:27:42.781 --> 00:27:47.781
some of the considerations you had about what's Ai will be an impact it will

453
00:27:47.971 --> 00:27:48.210
have.

454
00:27:48.210 --> 00:27:51.720
What's one thing that really gets you excited that you're really looking forward

455
00:27:51.721 --> 00:27:54.150
to it you think is going to happen?
A breakthrough is coming soon.

456
00:27:54.960 --> 00:27:55.793
<v 3>Well,</v>

457
00:27:55.980 --> 00:28:00.210
the most obvious things changes to our transportation and shipping

458
00:28:00.211 --> 00:28:03.840
infrastructures.
Talk about preaching to the choir.
How many of you,

459
00:28:03.960 --> 00:28:07.320
how many of you in the room are working on self driving technologies?

460
00:28:10.830 --> 00:28:15.750
We have a alphabet has split now.
Okay.
They're in,
they're all working on it.

461
00:28:15.751 --> 00:28:17.820
And you guys,
you guys

462
00:28:20.350 --> 00:28:25.080
all right.
Well I mean w w when I was a little bit younger,

463
00:28:25.830 --> 00:28:28.020
uh,
the,
uh,
they used to say,
well,

464
00:28:28.021 --> 00:28:30.240
what are we gonna do when machines can program themselves?

465
00:28:30.510 --> 00:28:33.490
And in a way you hear that again,
I'm sorry,
I'm a little bit off of your,

466
00:28:33.491 --> 00:28:37.350
your topic.
Uh,
in the sense that we,

467
00:28:37.351 --> 00:28:42.210
that I programmed machines when I was a a working,
working day engineer.

468
00:28:42.500 --> 00:28:46.630
Um,
we have done that,
you know,
the languages are higher level,
uh,

469
00:28:46.640 --> 00:28:49.230
the machines programs themselves,
uh,
today,
uh,

470
00:28:49.231 --> 00:28:53.260
we don't lay out chips by hand anymore.
Obviously we have great,

471
00:28:53.340 --> 00:28:56.370
great tools to do that.
So the nature of that work continues to change.

472
00:28:56.400 --> 00:28:59.760
And I somehow got completely off your,
uh,
that's the end point,
which was,

473
00:29:00.000 --> 00:29:03.010
which was one thing that really gets you,
it gets me excited.

474
00:29:03.260 --> 00:29:07.530
You want to see happen and you will see on the horizon or the,

475
00:29:07.800 --> 00:29:12.800
I don't think the impact of this revolution where we made a sensory perception

476
00:29:14.670 --> 00:29:18.330
with the ability of,
with machines that can perform functions.

477
00:29:18.331 --> 00:29:20.970
There's almost existing technology on that side.

478
00:29:21.240 --> 00:29:23.400
The stuff that you guys work on a sensory perception.

479
00:29:23.401 --> 00:29:26.760
So we made those two together.
You get a very powerful combination.

480
00:29:26.940 --> 00:29:31.350
Only one example is the self driving car.
And when you do that,

481
00:29:31.710 --> 00:29:35.880
a whole variety of tasks that previously where machines could previously were

482
00:29:35.881 --> 00:29:40.260
confined to factory floors because they did,
couldn't sense their environment.

483
00:29:40.590 --> 00:29:44.230
Now these machines are going to come out of those environments and be in and

484
00:29:44.231 --> 00:29:47.530
around people and that's going to have dramatic impact,

485
00:29:47.531 --> 00:29:51.190
not just on labor but on improving our lives,

486
00:29:51.230 --> 00:29:56.150
making us wealthier and allowing us to do things in,
in,

487
00:29:56.151 --> 00:29:59.720
in greater safety or be better informed,
uh,
at,

488
00:29:59.810 --> 00:30:03.580
at a far lower cost than,
than it ever could be done before.

489
00:30:03.670 --> 00:30:07.760
And I don't think people realize how this revolution is going to,
Eh,
the,

490
00:30:07.770 --> 00:30:11.740
the enormous impact that this particular revolution it's going to have because

491
00:30:11.741 --> 00:30:15.010
it's,
it's going to be quite dramatic.
The problem is,

492
00:30:15.070 --> 00:30:18.550
I want to talk about this for you guys were working on the technologies,

493
00:30:19.090 --> 00:30:23.380
but there are tremendous barriers to the social acceptance of the kinds of

494
00:30:23.381 --> 00:30:25.970
systems and devices that were,
uh,

495
00:30:26.170 --> 00:30:30.490
creating and building machines that,
uh,

496
00:30:30.491 --> 00:30:33.700
abide by our normal social conventions,

497
00:30:33.970 --> 00:30:36.640
human social conventions when they're operating around.

498
00:30:36.641 --> 00:30:41.641
And with us is an area that is just in its infancy and we don't really have a

499
00:30:41.981 --> 00:30:44.890
good theory about this,
you know,
when is it okay,

500
00:30:44.891 --> 00:30:48.480
we'll be okay for a robot to go wait in line for you.
Uh,

501
00:30:48.580 --> 00:30:51.550
is that acceptable or not?
What does it mean?

502
00:30:51.580 --> 00:30:53.920
How are you going to feel the first time your self driving car,

503
00:30:53.921 --> 00:30:55.000
which could park itself,

504
00:30:56.110 --> 00:31:00.630
somebody else's self driving car can zoom into a spot and take a spot and

505
00:31:00.790 --> 00:31:02.860
there's nobody in it and you need to park your car.

506
00:31:03.100 --> 00:31:07.240
Is that okay or is that not okay?
I go on and on with lots and lots of examples.

507
00:31:07.480 --> 00:31:12.480
But where we haven't paid adequate attention to is what kinds of theories of

508
00:31:12.911 --> 00:31:17.911
social behavior and normal conventional norms of society.

509
00:31:18.430 --> 00:31:23.200
How do we take those and codified them and put them into the,
uh,
behavior?

510
00:31:23.201 --> 00:31:27.040
If I could anthropomorphize a little bit on these machines in a way that people

511
00:31:27.041 --> 00:31:28.480
will find acceptable.

512
00:31:33.600 --> 00:31:34.433
<v 1>Okay.</v>

513
00:31:36.470 --> 00:31:38.730
<v 2>Yeah.
So in this age of,
uh,</v>

514
00:31:39.210 --> 00:31:43.160
polarities and people are able to see eye to eye,

515
00:31:43.700 --> 00:31:46.640
is it actually,
uh,
in,
in a full,

516
00:31:46.650 --> 00:31:51.650
put the AI beginning to find a deep similarities between humans rather than the

517
00:31:52.281 --> 00:31:54.560
differences we tend to focus on.
For example,

518
00:31:54.870 --> 00:31:58.400
a Google has found that in spite of language is being so different.

519
00:31:58.760 --> 00:31:59.593
There's a deep,

520
00:32:00.290 --> 00:32:04.120
a commonality at the heart of the language almost at the [inaudible] level.

521
00:32:04.130 --> 00:32:09.130
So if you think of mother and I think of Martha and somebody else thinks of Ma

522
00:32:09.530 --> 00:32:14.090
is same bio rhythm expressed in different ways as it's beginning to expose that

523
00:32:14.091 --> 00:32:18.920
through computation.
So,
um,
and so it's what the expressions are different,

524
00:32:18.921 --> 00:32:21.500
but there's deep commonality between humans,
right?

525
00:32:21.501 --> 00:32:26.501
So is there actually a hope that AI can break us together by finding more of the

526
00:32:26.731 --> 00:32:28.910
similarities versus,
you know,

527
00:32:29.380 --> 00:32:32.570
it's getting us into being more reclusive?

528
00:32:32.960 --> 00:32:33.501
<v 3>Well,
you know,</v>

529
00:32:33.501 --> 00:32:38.420
there's a long history in linguistics of study on this and those who are I

530
00:32:38.450 --> 00:32:42.950
Chomsky's calm skins,
Chompsky heights,
uh,
we'll,
we'll recognize that.
Uh,
his,

531
00:32:42.951 --> 00:32:45.620
his whole theory is,
is that there's a fundamental basis for,
uh,

532
00:32:45.860 --> 00:32:50.150
for language whether or not that's true.
I,
you know,
as a matter of some debate.

533
00:32:50.630 --> 00:32:55.630
But I think that to the extent that we've reflect human behaviors in this,

534
00:32:57.370 --> 00:33:01.820
uh,
I'll call it a mechanical or electronic mirror of these machines that were

535
00:33:01.821 --> 00:33:06.821
building it can obviously help us to inform us about what our own capabilities

536
00:33:08.241 --> 00:33:13.100
are and what the commonalities are,
uh,
among different cultures in different,

537
00:33:13.130 --> 00:33:14.870
different people.
Um,

538
00:33:15.110 --> 00:33:18.260
now that sort of the positive view on this,

539
00:33:18.380 --> 00:33:23.000
the other is that when we hold up this giant technology mirror that we're doing

540
00:33:23.001 --> 00:33:26.870
to society,
for example,
in,
in social media,
uh,

541
00:33:27.480 --> 00:33:31.550
it's going to adopt the same kinds of biases and,
uh,

542
00:33:31.730 --> 00:33:36.650
negative aspects of a human conflict and behavior that,

543
00:33:36.950 --> 00:33:39.560
uh,
that we see in ourselves.

544
00:33:39.560 --> 00:33:43.220
So let's be very careful when we look into this mirror about what we're going to

545
00:33:43.221 --> 00:33:47.870
see.
It may be about what's common and expanding our view of,
uh,

546
00:33:47.871 --> 00:33:50.330
includes a wider and more inclusive,

547
00:33:50.480 --> 00:33:55.480
or it may become a tool for us to be a distinguished ourselves thinking about

548
00:33:56.231 --> 00:34:01.140
their people as other and not wanting to be concerned about,
uh,

549
00:34:01.190 --> 00:34:02.510
like today the,
the,
uh,

550
00:34:02.511 --> 00:34:06.980
the role of immigrants versus a me first kind of attitude that,

551
00:34:07.070 --> 00:34:07.670
that we're,

552
00:34:07.670 --> 00:34:12.170
we're making a turn that's not necessarily good for all of the world.

553
00:34:12.650 --> 00:34:17.450
Uh,
but,
um,
uh,
I think that that,
that's the way that I look at the,

554
00:34:17.540 --> 00:34:19.280
this problem.
It's,
it's informative.

555
00:34:19.730 --> 00:34:22.850
It will be helped to inform us about our own human nature.

556
00:34:23.300 --> 00:34:25.820
Machine nature will help to inform us about our human nature,

557
00:34:27.440 --> 00:34:31.370
<v 4>that there is a more or less equilibrium between demand and supply in the</v>

558
00:34:31.371 --> 00:34:35.150
overall labor markets.
And as automation changes,

559
00:34:35.180 --> 00:34:37.730
demands or reduces demand in one labor market,

560
00:34:37.731 --> 00:34:39.860
the assumptions that technology creates a new one,
right?

561
00:34:39.861 --> 00:34:44.120
If you need less coal miners,
maybe you need more solar panel installers.

562
00:34:44.600 --> 00:34:46.160
Well that assumption always hold true.

563
00:34:46.780 --> 00:34:50.230
<v 3>Well this isn't a law of nature,
but you just have to look at it.</v>

564
00:34:50.260 --> 00:34:52.120
Historically that has been the case.

565
00:34:52.810 --> 00:34:57.810
The concern that we're putting everybody out of work is hundreds of years old.

566
00:34:59.560 --> 00:35:03.640
And this question,
this problem has been brought up repeatedly over time.

567
00:35:04.030 --> 00:35:09.030
Now that's not to diminish the tremendous personal cost that,

568
00:35:09.430 --> 00:35:13.960
uh,
uh,
cruise to people who are displaced by the new technology.

569
00:35:14.260 --> 00:35:18.700
But I just don't see any reason why this,
this is not going to,
uh,

570
00:35:19.030 --> 00:35:22.030
the same balance is just going to come back into it.

571
00:35:22.031 --> 00:35:24.760
We'll come back into balance.
Uh,
even if we just,

572
00:35:26.050 --> 00:35:29.380
even if we displace a large number of people,
of course,

573
00:35:29.381 --> 00:35:33.280
the question is when and how long does that take and what are we going to do in

574
00:35:33.281 --> 00:35:36.990
order to minimize the social and economic impacts of that?
You,

575
00:35:37.080 --> 00:35:38.730
the big picture is pretty simple.

576
00:35:39.210 --> 00:35:43.350
We're doing things that make us wealthier as a society,
significantly wealthier.

577
00:35:43.710 --> 00:35:48.480
Most people don't realize that the average household income in the United States

578
00:35:48.660 --> 00:35:51.130
has doubled reliably,
uh,

579
00:35:51.450 --> 00:35:53.790
every 40 years for over 200 years.

580
00:35:54.090 --> 00:35:57.600
It's incredible how much wealthier we are that our parents were,

581
00:35:57.601 --> 00:36:01.710
even though you might not see it,
uh,
on a day to day basis,
uh,

582
00:36:01.711 --> 00:36:04.350
and our grandparents and going all the way back,

583
00:36:04.410 --> 00:36:09.410
the average household income in the United States 200 years ago was $1,000 a

584
00:36:09.991 --> 00:36:14.940
year.
It's about the same as it is in I think it was Gambia and a couple of,
uh,

585
00:36:15.390 --> 00:36:18.930
uh,
agrarian,
uh,
African countries.

586
00:36:19.290 --> 00:36:24.200
And yet we don't think back on,
uh,
uh,
Ben Franklins time and think,

587
00:36:24.420 --> 00:36:27.280
my God,
those people were dirt poor.
Um,

588
00:36:27.330 --> 00:36:29.970
now what if that pattern is likely to continue?

589
00:36:30.300 --> 00:36:34.840
So we're going to have another hundred percent of wealth,
uh,

590
00:36:34.920 --> 00:36:38.100
compared to today in 40 years.
So we will double the amount of,
well,

591
00:36:38.250 --> 00:36:41.790
the question is how do we,
how do we distribute that,
uh,

592
00:36:42.240 --> 00:36:45.400
properly among the workforce?
It may be,
uh,
you mentioned,
uh,

593
00:36:45.870 --> 00:36:48.990
a universal basic income.
You didn't use the term.

594
00:36:48.991 --> 00:36:52.470
So a lot of the people on the video probably will not know know what that is,

595
00:36:52.590 --> 00:36:55.620
but that's only one approach to this particular problem.

596
00:36:55.860 --> 00:36:58.920
I think it's self correcting,
but we need to pay attention to it.

597
00:36:58.921 --> 00:37:01.170
We shouldn't be worrying about where the machines are going to come alive and

598
00:37:01.171 --> 00:37:02.040
take over the universe.

599
00:37:02.340 --> 00:37:06.870
We should be worried about whether or not the pace of automation is increasing

600
00:37:06.871 --> 00:37:10.920
or we're going through an another wave of increase in automation,

601
00:37:11.130 --> 00:37:15.680
primarily due to a machine learning today.
And,
uh,

602
00:37:15.890 --> 00:37:17.820
how is that going to affect different aspects,

603
00:37:18.000 --> 00:37:22.290
different portions of society and what do we do to mitigate the negative
effects?

604
00:37:22.500 --> 00:37:26.850
<v 2>And if I just may I have one,
a few,
a few points about that actually.
Um,
one is,</v>

605
00:37:26.910 --> 00:37:29.690
uh,
to your coin,
demand changes,
uh,
for employment.

606
00:37:29.760 --> 00:37:32.370
Employment itself itself changes.
Uh,

607
00:37:32.730 --> 00:37:35.400
in 20 years from now what we think of as a job.
Maybe there a difference,

608
00:37:35.550 --> 00:37:37.050
not just in terms of content but they have a structure.

609
00:37:37.051 --> 00:37:39.330
Maybe we'll have way more dig centered economy.

610
00:37:39.660 --> 00:37:42.870
Maybe you will have a much shorter work week of a rural because that's just how

611
00:37:42.871 --> 00:37:46.080
we think all work week as much shorter amount of grandparents.

612
00:37:46.490 --> 00:37:51.340
So it's just not the same thing.
Uh,
so that's one factor.
It's important.
Um,

613
00:37:51.940 --> 00:37:53.160
another one is the,

614
00:37:53.220 --> 00:37:55.740
you were mentioning the speed and the scale automation and how it impacts the

615
00:37:55.741 --> 00:37:56.940
economy.
Uh,

616
00:37:57.210 --> 00:38:00.750
to your point about regulation being a key driver for that,

617
00:38:01.770 --> 00:38:04.490
even if the technology gets there early on,
uh,

618
00:38:04.590 --> 00:38:07.620
if the rules and if society doesn't change at the same pace,

619
00:38:07.920 --> 00:38:09.120
there may be a buffer there.

620
00:38:09.150 --> 00:38:12.720
If we are underestimating now it may be a very bad thing for a lot of reasons

621
00:38:12.750 --> 00:38:14.040
because it stops innovation.

622
00:38:14.490 --> 00:38:18.270
It may be actually an okay thing from the perspective of jobs displacement in

623
00:38:18.271 --> 00:38:21.030
the sense that it gives more time for societies to adapt.

624
00:38:21.330 --> 00:38:23.580
So that's also permitted to have in mind.

625
00:38:24.120 --> 00:38:27.420
<v 3>I think the key issue is it's,
it's a question of the speed of transformation.</v>

626
00:38:27.780 --> 00:38:29.820
If you go back 200 years,
as you may know,

627
00:38:30.180 --> 00:38:33.420
more than 90% of the u s population worked in agriculture.

628
00:38:34.050 --> 00:38:38.140
What it meant to work was to be on a farm and do farming work,

629
00:38:38.360 --> 00:38:43.090
that that's what jobs were only everything else was just 10% of that.
Now,

630
00:38:43.300 --> 00:38:47.410
today,
less than 2% and based on a lot of the work that's going on here,

631
00:38:47.411 --> 00:38:52.180
we're going to be able to make 75% of it,
2% pretty,
pretty quickly.
So,

632
00:38:52.290 --> 00:38:55.310
uh,
basically all do people did,
was grow in,
in,

633
00:38:55.410 --> 00:38:59.830
and consume food and that represented I think 40% of the average person's
budget.

634
00:38:59.831 --> 00:39:02.950
Today,
it's way less than 10%,
which is kind of kind of exciting,

635
00:39:03.370 --> 00:39:06.430
but we're not all out of a job.
You know,

636
00:39:06.431 --> 00:39:10.480
we've got other things to do because our expectations continue to rise,
uh,

637
00:39:10.520 --> 00:39:12.790
et cetera.
But,
uh,
I have,
uh,

638
00:39:12.880 --> 00:39:16.210
maybe a radical statement to make here that I would not make elsewhere.

639
00:39:16.660 --> 00:39:18.220
This is not your problem.

640
00:39:18.970 --> 00:39:22.720
You guys should be out there generating the technology,

641
00:39:22.721 --> 00:39:24.850
which is going to make society wealthier.

642
00:39:25.090 --> 00:39:30.090
And as a slight side effect with mentioning you wealthier and the owners of

643
00:39:30.311 --> 00:39:31.360
Google stock wealthier.

644
00:39:32.020 --> 00:39:35.620
But that is not your problem to be worried about public policy.

645
00:39:35.621 --> 00:39:37.660
Actually it is clips job though.
He,

646
00:39:37.690 --> 00:39:42.450
they let one guy kind of 50,000 and uh,
he,
he could do no,

647
00:39:42.680 --> 00:39:45.130
it's perfectly reasonable thing to be concerned with this,

648
00:39:45.460 --> 00:39:49.420
but the idea that we should not do this is silly.

649
00:39:49.421 --> 00:39:52.990
It's a little bit like saying now we can,
uh,
connect phone calls electronically.

650
00:39:53.560 --> 00:39:57.820
We'll we're gonna suppress that technology so that all of these telephone

651
00:39:57.821 --> 00:40:01.540
operators still have jobs.
It's obviously the wrong approach to take.

652
00:40:02.080 --> 00:40:05.740
Do your jobs,
generate that wealth,
make the future,

653
00:40:06.340 --> 00:40:10.300
give us the opportunity to make the future better and let the people who worry

654
00:40:10.301 --> 00:40:15.301
about policy in a social cohesion figure out how best to address those side

655
00:40:17.591 --> 00:40:19.450
effects of security.

656
00:40:21.220 --> 00:40:25.090
<v 5>So,
uh,
so to follow up on that then,
uh,
one question,</v>

657
00:40:25.091 --> 00:40:29.200
I always wonder from the perspective of anybody who's making a choice about what

658
00:40:29.201 --> 00:40:33.520
kinds of innovations to work on,
an obvious question is do you,

659
00:40:33.550 --> 00:40:38.550
can you give any color around some aspects of automation just to change the jobs

660
00:40:39.191 --> 00:40:42.520
of people and they stay in place and become more powerful,

661
00:40:42.521 --> 00:40:45.970
like your example of the compiler.
Other examples of automation though,

662
00:40:45.971 --> 00:40:49.930
do cause displacement.
Do you have any thoughts about how those sorted out?

663
00:40:50.360 --> 00:40:54.550
<v 3>Um,
well I would argue that they all cause displacement.
Let me explain that.</v>

664
00:40:55.210 --> 00:40:57.940
People worry about machines coming and taking our jobs.

665
00:40:58.240 --> 00:41:02.980
But machines don't take jobs,
don't do jobs.
They perform tasks.

666
00:41:03.370 --> 00:41:06.730
And if you look at what a particular individual does,

667
00:41:06.940 --> 00:41:09.370
usually there is some range of tasks that they are involved in.

668
00:41:09.730 --> 00:41:11.560
Now if you automate 50% of those,

669
00:41:11.890 --> 00:41:14.530
you make the argument correctly that you're making that person much more

670
00:41:14.531 --> 00:41:18.000
productive.
You're freeing them up from routine work typically,
uh,

671
00:41:18.220 --> 00:41:21.640
and give them time to focus on the things where they add the value the most.

672
00:41:21.910 --> 00:41:25.090
So that's great.
We're making programmers more productive.
What does that mean?

673
00:41:25.091 --> 00:41:28.750
We need fewer programs?
Well,
actually we can get a lot today,
but in general,

674
00:41:29.080 --> 00:41:30.120
you know,
that means we would need,

675
00:41:30.150 --> 00:41:32.170
if we were all programming in assembly language,

676
00:41:32.470 --> 00:41:34.690
this place would be the size of a,

677
00:41:34.990 --> 00:41:39.590
a major country in terms of what will be necessary to accomplish,
uh,
the,
the,

678
00:41:39.620 --> 00:41:44.510
the things that you do.
So,
uh,
however,

679
00:41:44.540 --> 00:41:49.190
if your job consists of a single task,
I lay bricks,
I picked or drive a car,

680
00:41:49.370 --> 00:41:53.210
I pick up the brick,
I put it here,
I put the mortar on.
And that's what I do.

681
00:41:53.240 --> 00:41:54.560
You can build a machine that does that.

682
00:41:54.740 --> 00:41:58.220
And if you do only tasks that are automateable,

683
00:41:58.250 --> 00:42:00.050
obviously you're going to be out of a job.

684
00:42:00.410 --> 00:42:03.980
So I think this is a false dichotomy that you hear a lot about.

685
00:42:04.070 --> 00:42:07.790
Are we making people more productive or are we putting them out of work?

686
00:42:07.791 --> 00:42:12.590
And it's somehow this bleeds into our design.
I think that's not really the case.

687
00:42:12.890 --> 00:42:15.770
I think what is the case as we tackle tasks,
you know,

688
00:42:15.771 --> 00:42:18.740
they're very important tasks with tremendous economic benefit.

689
00:42:18.741 --> 00:42:22.520
Like making a machine that plays go,
that's a joke.

690
00:42:23.550 --> 00:42:27.680
It wasn't funny,
but it was supposed to be a joke.
Uh,
you know,
there's just,

691
00:42:27.710 --> 00:42:29.990
there's such a demand for this,
a worldwide,
you know,

692
00:42:29.991 --> 00:42:32.900
to save us from the drudgery of having to play,
go ourselves.

693
00:42:33.890 --> 00:42:37.460
And I'm so glad that you guys have done such a fabulous job on,

694
00:42:37.461 --> 00:42:39.970
on that important societal issue.
Um,

695
00:42:40.730 --> 00:42:44.570
so my point is that you're going to do these particular,

696
00:42:44.660 --> 00:42:48.680
if you automate tasks,
that's what you guys do.
Everybody here,

697
00:42:48.681 --> 00:42:51.740
if you think about what you're trying to do,
it's always task oriented.

698
00:42:51.741 --> 00:42:56.570
It's not job oriented.
And how the jobs fit into this kind of matrix,
uh,

699
00:42:56.571 --> 00:42:58.430
is really the determining factor.

700
00:42:58.431 --> 00:43:02.780
Whether we are making people more productive or whether we are putting them out

701
00:43:02.781 --> 00:43:03.390
for,

702
00:43:03.390 --> 00:43:07.210
<v 2>yeah,
there's an example actually that's pretty,
uh,
uh,
often causes,</v>

703
00:43:07.270 --> 00:43:10.280
which is that of ATM and bank tellers.
I'm sure you guys have made before,

704
00:43:10.690 --> 00:43:13.370
which is that when Dan [inaudible] scheme up,
uh,

705
00:43:13.450 --> 00:43:17.540
there was a notion that's welded,
banked and [inaudible],
uh,

706
00:43:17.680 --> 00:43:21.340
the one needed anyone to hand money at the counselor anymore and so it will be

707
00:43:21.430 --> 00:43:22.600
just less employment for these guys.

708
00:43:22.601 --> 00:43:25.960
And years later studies found that employment has actually remained constant.

709
00:43:26.260 --> 00:43:27.410
And that's because,
uh,

710
00:43:27.430 --> 00:43:32.110
actually of course with Ata you needed less bank tellers in one bank,

711
00:43:32.470 --> 00:43:33.303
but then again,
uh,

712
00:43:33.310 --> 00:43:36.010
you needed to Dennis still because you need the human face and they upscale that

713
00:43:36.011 --> 00:43:37.120
I'd be able to do more things.

714
00:43:37.540 --> 00:43:41.140
And because you saved money and you hyper activity,
they just were more banks.

715
00:43:41.290 --> 00:43:42.123
What makes up things?

716
00:43:42.570 --> 00:43:43.920
<v 3>He was a general principle.
I,</v>

717
00:43:44.100 --> 00:43:47.670
I can put off you automation changes the nature of work.

718
00:43:48.150 --> 00:43:51.270
It's the same title,
different job.
What a,

719
00:43:51.271 --> 00:43:54.690
what a teller did 30 years ago is very different.

720
00:43:54.810 --> 00:43:58.740
I tell her is now a concierge to bank services before they used to sit there and

721
00:43:58.741 --> 00:44:02.400
Dole out money.
So these things constantly change.

722
00:44:03.330 --> 00:44:06.540
My programming skills to describe them as obsolete would,

723
00:44:06.630 --> 00:44:10.650
would be too gracious and polite.
You know,
I mean,

724
00:44:10.651 --> 00:44:14.290
I remember the days before,
uh,
before,
uh,
object oriented pro,

725
00:44:14.310 --> 00:44:17.940
we had structured programming.
Anybody who remembered that stuff.
One guy,

726
00:44:17.941 --> 00:44:22.890
how old are you?
23.
Okay.
What structures?
I love structured private.

727
00:44:22.891 --> 00:44:26.820
It was great advance.
Um,
you know,
the whole,
you look at the way database,

728
00:44:26.870 --> 00:44:28.290
I don't have time for this,

729
00:44:28.291 --> 00:44:32.050
but it's a fascinating thing to look at the history of how databases,
uh,
uh,

730
00:44:32.790 --> 00:44:33.720
were implemented.

731
00:44:33.721 --> 00:44:37.020
And what happened is we moved from networking hierarchical models to the

732
00:44:37.021 --> 00:44:40.320
relational model and how that changed the nature of,
of databases,

733
00:44:40.321 --> 00:44:43.230
but it made it so much more efficient and easier to implement.

734
00:44:44.100 --> 00:44:46.860
Everything is a database today,
uh,
used to be,

735
00:44:46.861 --> 00:44:48.900
if you wanted to store data in a computer,

736
00:44:48.901 --> 00:44:52.260
you had a higher group program is to figure out exactly what that particular

737
00:44:52.261 --> 00:44:55.920
application needed and program and structure at that particular way.

738
00:44:55.950 --> 00:44:59.040
Now we can do that in a much more generally.
So the point is,

739
00:44:59.400 --> 00:45:00.870
even if we call them the same thing,

740
00:45:01.710 --> 00:45:04.830
the jobs change the job of a driver in the future.

741
00:45:05.130 --> 00:45:08.790
Maybe very much of a concierge in a,

742
00:45:09.120 --> 00:45:11.550
imagine a van that comes and picks you up.

743
00:45:11.640 --> 00:45:16.640
There may be somebody sitting there whose job it is to sell you a drink or to

744
00:45:17.280 --> 00:45:21.180
provide you with some other kind of service during the time that you are in this

745
00:45:21.181 --> 00:45:23.730
vehicle is it's picking up and dropping off other people.

746
00:45:23.970 --> 00:45:28.260
So there will be plenty of plenty of jobs.

747
00:45:28.261 --> 00:45:32.540
They'll just be a little bit different.
We had a question from the livestream.

748
00:45:33.200 --> 00:45:36.810
Why would insurance companies be motivated as a lobby for self driving cars?

749
00:45:37.370 --> 00:45:39.420
Industry is born of human mistakes.

750
00:45:39.720 --> 00:45:43.260
Isn't it reasonable to assume that one of the potential losers here would be

751
00:45:43.261 --> 00:45:47.970
insurance companies as accidents go from one per 100,000 miles driven to one per

752
00:45:47.971 --> 00:45:50.160
1 million miles driven?
Well,
the,

753
00:45:50.170 --> 00:45:53.880
the advantages they work like all businesses on the spread,

754
00:45:54.420 --> 00:45:58.800
which is uh,
you know,
what is it costing you and what can you get for it?

755
00:45:59.160 --> 00:46:00.930
So the first thing that's going to happen,

756
00:46:02.400 --> 00:46:05.460
I'm making this up so I shouldn't,
I shouldn't phrase like that.

757
00:46:05.490 --> 00:46:09.750
I expect one of the things that might happen is that when you,

758
00:46:09.810 --> 00:46:13.150
if you have a self driving car and I'm your insurance company,

759
00:46:13.151 --> 00:46:16.050
what I'm going to say in order to reduce my cost,

760
00:46:16.051 --> 00:46:20.260
and pat is if you get into an accident and uh,
you're,

761
00:46:20.510 --> 00:46:22.790
the car was on automatic and it's,

762
00:46:22.800 --> 00:46:26.090
let's assume it can be driven manually or automatically,
uh,

763
00:46:26.250 --> 00:46:27.720
which is controversial as you guys,

764
00:46:27.780 --> 00:46:31.610
you guys know whether that's a good idea or bad idea,
it's a bad idea,
but

765
00:46:33.780 --> 00:46:38.730
they'll say if,
if it was on autopilot so to speak,
then uh,
uh,

766
00:46:38.760 --> 00:46:40.260
we'll waive your deductible.
You know,

767
00:46:40.261 --> 00:46:43.840
you don't have a deductible in the case of an accident.
Now,
uh,

768
00:46:43.860 --> 00:46:47.020
the reason for that is that the amount of money that there's going to save and

769
00:46:47.021 --> 00:46:50.550
the amount they're going to drop your insurance premium by is,

770
00:46:50.551 --> 00:46:54.660
is going to be less than the benefit to them.
That's the spread.
It's really,

771
00:46:54.661 --> 00:46:55.830
really that simple.

772
00:46:55.831 --> 00:47:00.590
So the insurance companies have a very strong incentive to,
uh,

773
00:47:00.660 --> 00:47:03.360
to see a rollout of this kind of technology.

774
00:47:03.630 --> 00:47:06.150
It's true that it will transform their businesses as well,

775
00:47:06.360 --> 00:47:08.490
but people can get into all kinds of trouble.

776
00:47:08.491 --> 00:47:11.100
And I'm sure that we have many other things that,
that uh,

777
00:47:11.400 --> 00:47:13.720
services that they can sell you in,
in,
in terms of,

778
00:47:13.740 --> 00:47:17.190
I'm not worried about the insurance.
That's what I'm saying.
I think there'll be,

779
00:47:17.220 --> 00:47:18.053
there'll be just fun.

780
00:47:18.710 --> 00:47:23.330
<v 6>So the current Ai Paradigm relies very heavily on training data and as we've</v>

781
00:47:23.331 --> 00:47:28.331
seen in domains like bank lending and bro granting historic injustices and

782
00:47:29.500 --> 00:47:33.640
profiling can be perpetuated into the ais that are trained.

783
00:47:33.670 --> 00:47:38.560
We've done some work here on earth moving such biases from the networks.

784
00:47:38.860 --> 00:47:42.760
What do you think are the policy and legal issues that are going to get involved

785
00:47:42.940 --> 00:47:47.530
in inserting such value judgments into the utility functions that build these

786
00:47:47.531 --> 00:47:48.364
ais?

787
00:47:48.540 --> 00:47:51.960
<v 3>Well,
here's an interesting way to look at this problem.</v>

788
00:47:52.080 --> 00:47:55.560
I gave a talk to the mortgage bankers association,

789
00:47:55.830 --> 00:47:58.710
which of course is very concerned about exactly this issue.

790
00:47:58.711 --> 00:48:03.711
There are a lot of laws for very good social reasons that are in place to avoid

791
00:48:04.090 --> 00:48:07.590
red lining and discrimination based on race,
et Cetera,
et cetera.

792
00:48:07.860 --> 00:48:12.260
And they're subject to some very stringent,
a statistical tests,
uh,

793
00:48:12.330 --> 00:48:13.680
in the work that they do.

794
00:48:13.920 --> 00:48:18.600
Their problem is to the extent that human beings are involved in that decision

795
00:48:18.601 --> 00:48:21.030
making process.
They need to train those people.

796
00:48:21.210 --> 00:48:24.900
And often those people do not perform in exactly the way that they want.

797
00:48:25.080 --> 00:48:27.870
So society has this problem today,

798
00:48:27.900 --> 00:48:31.350
but with respect to the people involved in that process,

799
00:48:31.680 --> 00:48:33.930
we are going to transfer that to the problem of these,

800
00:48:34.110 --> 00:48:36.240
these machine learning algorithms.
Okay?

801
00:48:36.241 --> 00:48:40.770
We're going to say you can't discriminate on the basis of race,
but as you know,

802
00:48:40.920 --> 00:48:42.210
it can find a correlated,

803
00:48:42.211 --> 00:48:46.680
a variable of some kind that has the same kind of effect.
You know,

804
00:48:46.710 --> 00:48:50.340
we're not banning Muslims were just stopping the immigration from certain

805
00:48:50.730 --> 00:48:55.110
countries.
They just happened to be muscle.
Okay?
So there are ways to,
uh,

806
00:48:55.500 --> 00:48:56.333
to,
uh,

807
00:48:56.700 --> 00:49:01.700
that you wind up with the same effect of discriminatory and on satisfactory a

808
00:49:01.771 --> 00:49:06.240
social,
uh,
results.
Uh,
even though you've,
you've,
uh,
all you've did,

809
00:49:06.241 --> 00:49:10.980
all I did was the,
you know,
program by my,
a neural net,
you know,
to do this.

810
00:49:11.250 --> 00:49:12.030
So,
um,

811
00:49:12.030 --> 00:49:16.410
it's very important that we put the kinds of hooks and controls entity systems

812
00:49:16.800 --> 00:49:20.160
that will allow us to ensure that they meet our social standards.

813
00:49:20.161 --> 00:49:22.800
I was talking about that in terms of like standing in line,

814
00:49:22.801 --> 00:49:24.090
but this is another example.

815
00:49:24.360 --> 00:49:28.920
Do they perform in ways that we find acceptable as a society that's going to be

816
00:49:28.921 --> 00:49:30.240
very,
very important.

817
00:49:30.690 --> 00:49:35.340
Now if you think about this as automation as opposed to magic,

818
00:49:35.880 --> 00:49:39.420
you get a very different point of view about how to go about doing this.

819
00:49:39.480 --> 00:49:44.180
This is a question of engineering standards and practices and I think we're,

820
00:49:44.190 --> 00:49:45.023
we're going to see,

821
00:49:45.270 --> 00:49:49.830
I hope we will see an emergence in the field of artificial intelligence,
a,

822
00:49:50.070 --> 00:49:54.960
an approach to uh,
the,
uh,
the development,

823
00:49:55.380 --> 00:50:00.380
the testing and the deployment of artificial intelligence systems that is,

824
00:50:01.380 --> 00:50:02.213
uh,

825
00:50:02.250 --> 00:50:07.250
similar to what like the I triple e does for many other areas or civil

826
00:50:08.281 --> 00:50:11.300
engineers,
they have standards for building bridges.
You know,

827
00:50:11.310 --> 00:50:14.670
it has to meet certain kinds of criteria or chip testing.

828
00:50:14.710 --> 00:50:18.450
There's was a standard now that you need to apply and make sure that you chip

829
00:50:18.451 --> 00:50:21.900
meets its specifications.
While we don't have that science yet.

830
00:50:22.170 --> 00:50:24.630
And part of that is this idea that we're,

831
00:50:24.780 --> 00:50:28.010
so I'm sitting in the room here with wizards,
with hats on,
it's all magic.

832
00:50:28.280 --> 00:50:31.490
And so you couldn't possibly,
uh,
tell the machine what to do.

833
00:50:31.500 --> 00:50:34.460
It's going to make up its own mind or of course that's complete nonsense.

834
00:50:34.910 --> 00:50:39.380
So I think what we need is a get the magic out of Ai,

835
00:50:39.830 --> 00:50:42.800
get the Gee whiz at it and start to talk about what kind of engineering

836
00:50:42.801 --> 00:50:46.640
practices and standards in techniques do we want to incorporate into these

837
00:50:46.641 --> 00:50:51.641
devices to ensure that they meet our societal values and that they abide by

838
00:50:51.860 --> 00:50:55.430
normal social conventions,
which people find acceptable.
Otherwise.

839
00:50:55.670 --> 00:51:00.350
My view isn't it,
that the robot's ran a muck.
It's that we built bad tools.

840
00:51:00.610 --> 00:51:01.910
You know,
we don't,
you know,

841
00:51:01.970 --> 00:51:05.210
automated lawnmowers that run children down or something.
Uh,

842
00:51:05.240 --> 00:51:07.280
that's where that and that kind of a prompt comes in.

843
00:51:08.390 --> 00:51:08.580
<v 7>Uh,</v>

844
00:51:08.580 --> 00:51:13.580
you mentioned earlier that there are multiple ways to,

845
00:51:15.180 --> 00:51:18.150
to deal with people whose jobs are getting displaced.

846
00:51:18.630 --> 00:51:22.920
One of them is a universal basic income.
What are some of the others?

847
00:51:23.000 --> 00:51:27.620
<v 3>Well,
the biggest problem that I see is that our education system isn't really</v>

848
00:51:27.621 --> 00:51:32.621
designed to turn out people whose skills are needed by the marketplace.

849
00:51:34.850 --> 00:51:37.280
Uh,
there,
there are many roles of education,

850
00:51:37.281 --> 00:51:40.700
but one of the Mr train people to be productive members of society.

851
00:51:41.030 --> 00:51:45.110
And that means teaching them how to do certain kinds of jobs or,
or,
or tasks.

852
00:51:45.710 --> 00:51:48.740
And the problem we have is that the,

853
00:51:48.890 --> 00:51:53.120
the funder of our education,
the lender of first resort,
if you will,

854
00:51:53.150 --> 00:51:55.970
is the government and the government has no incentive.

855
00:51:55.971 --> 00:51:59.600
There's no incentive built into that structure to ensure that the things that

856
00:51:59.601 --> 00:52:03.920
are being taught are the things that people need to know.
So by way of example,

857
00:52:03.950 --> 00:52:07.010
and this is the worst audience to use this example and Buddha and forks and

858
00:52:07.011 --> 00:52:11.540
other ones.
I,
we teach high school kids calculus.
My view,

859
00:52:11.541 --> 00:52:14.300
that's mostly a waste of time.
They don't need calculus.

860
00:52:14.360 --> 00:52:17.570
I've never needed calculus.
Now,
many of you,
how many people here use calculus?

861
00:52:18.200 --> 00:52:19.820
Okay,
there you go.
So this is why it's,

862
00:52:20.300 --> 00:52:25.160
this is where you need to do to learn calculus by,
um,
you know,
the,

863
00:52:25.161 --> 00:52:25.520
the,

864
00:52:25.520 --> 00:52:29.720
the education that my children got at private schools here in the bay area is

865
00:52:29.721 --> 00:52:30.830
almost identical,

866
00:52:31.190 --> 00:52:34.820
surprisingly not only in content but in the way it was delivered,

867
00:52:34.821 --> 00:52:39.821
which is shocking to the education I had 40 or 50 years ago that hasn't been

868
00:52:40.461 --> 00:52:42.650
responsive to the needs of the marketplace.
Now,

869
00:52:42.651 --> 00:52:47.630
you guys know people like Sebastian Thrun and interlinked,
they're all,
you know,

870
00:52:47.631 --> 00:52:52.430
they're doing things that close this loop by making the economics,

871
00:52:52.820 --> 00:52:56.180
uh,
tied to the teaching,

872
00:52:56.181 --> 00:52:58.700
the stuff that people need to know or you just put it that way.

873
00:52:59.030 --> 00:53:03.890
And that the trick to making this work is to make sure that the investment

874
00:53:03.891 --> 00:53:05.810
that's being made has a likely payback.

875
00:53:05.810 --> 00:53:09.980
And the way you do that is by introducing the discipline of the marketplace and

876
00:53:10.010 --> 00:53:14.660
of,
uh,
financial institutions who,
uh,
will only,

877
00:53:14.930 --> 00:53:18.800
if they need a return on investment in investing in you to take a course,

878
00:53:19.310 --> 00:53:21.590
they're only going to do that if they think there's a likelihood that's going to

879
00:53:21.591 --> 00:53:25.800
pay off.
Like they don't loan you money to build,
buy or build house anywhere.

880
00:53:25.801 --> 00:53:28.230
They do an appraisal on the house and see what it's worth and what's the

881
00:53:28.231 --> 00:53:30.570
neighborhood like.
We need to do the same thing with education.

882
00:53:30.930 --> 00:53:34.770
One of the things that people really need to know in order to get a job and to

883
00:53:34.771 --> 00:53:37.740
be productive.
So places like,
uh,
you,

884
00:53:37.741 --> 00:53:42.360
Udacity and Coursera have begun to focus in on by working with organizations

885
00:53:42.361 --> 00:53:46.920
like Google on,
well,
what do you guys really need?
I talked to a young man,

886
00:53:47.000 --> 00:53:47.760
uh,

887
00:53:47.760 --> 00:53:52.400
just two days ago who took a machine learning course on one of these online.
He,

888
00:53:52.420 --> 00:53:56.280
it was like a three month course or something like that.
He was,
he was amazed.

889
00:53:56.281 --> 00:54:00.810
He took a three month course and it doubled his income and he got a new job and

890
00:54:00.811 --> 00:54:04.470
it was fantastic.
Not that he was great at it,
not like you guys,
but you know,
he,

891
00:54:04.680 --> 00:54:06.120
it was very valuable for him,

892
00:54:06.540 --> 00:54:09.390
whereas he could have gone back to school and gotten a masters degree in

893
00:54:09.391 --> 00:54:12.660
computer science and costs.
I don't know what that costs today,
you know,

894
00:54:12.661 --> 00:54:16.470
$100,000 or something and spent two years and it might not have been as valuable

895
00:54:16.471 --> 00:54:18.840
because it's going to be teaching and things which are not directly relevant to

896
00:54:18.841 --> 00:54:20.490
the work that he was going to be doing.

897
00:54:20.970 --> 00:54:25.470
So performing the educational system so that we close this loop between

898
00:54:25.471 --> 00:54:29.940
investment and payback is one another technique besides just handing out money.

899
00:54:31.470 --> 00:54:36.160
We just saw in the recent us election,
uh,

900
00:54:36.240 --> 00:54:41.240
we have discovered some very negative social side effects of technology.

901
00:54:42.960 --> 00:54:47.850
The Public Commons has moved online in social media companies,

902
00:54:47.851 --> 00:54:49.260
and I'll include Google in this,

903
00:54:49.261 --> 00:54:53.880
although they're not primary among this in such a way that it has fragmented

904
00:54:53.881 --> 00:54:55.740
public conversation and you are,

905
00:54:55.741 --> 00:54:57.930
you've read a lot about the fake news issues and all that.

906
00:54:58.770 --> 00:55:03.210
The problem is that the economic interests of these organizations,

907
00:55:03.420 --> 00:55:06.510
which is to keep you on for just a few seconds longer so they can get one more

908
00:55:06.511 --> 00:55:10.650
ad in front of you and that's worth millions of dollars is uh,

909
00:55:10.980 --> 00:55:15.550
in a tension with the needs of a,

910
00:55:15.880 --> 00:55:19.140
a society,
a democratic society to have informed.

911
00:55:19.620 --> 00:55:21.630
And a vibrant debate.

912
00:55:22.350 --> 00:55:27.030
And there are a lot of the techniques that were used to affect the election by

913
00:55:27.031 --> 00:55:30.690
nefarious parties,
both outside the United States,
inside the United States,

914
00:55:30.840 --> 00:55:35.790
to abuse this new medium communications medium to sway things,

915
00:55:35.840 --> 00:55:39.780
uh,
for it to benefit other people's interests and get people to do things that

916
00:55:39.781 --> 00:55:42.360
fundamentally we're not in there in their own interest.

917
00:55:42.750 --> 00:55:46.500
Here's what I want to say about that.
The were in that,
oh my God moment.

918
00:55:46.501 --> 00:55:50.580
This was the classic Chernobyl moment as far as this thing goes.

919
00:55:50.910 --> 00:55:54.600
But the technology industry created this problem.

920
00:55:55.080 --> 00:55:58.440
And if you look at the history of every communications medium,

921
00:55:58.830 --> 00:56:02.130
a new communications medium throughout history,
they've all had side effects,

922
00:56:02.520 --> 00:56:05.340
some of them like this or other negative side effects.

923
00:56:05.580 --> 00:56:10.380
And we have found ways to mitigate the negative effects of those communications

924
00:56:10.381 --> 00:56:11.214
meetings.

925
00:56:11.280 --> 00:56:15.090
This is a problem created by the technology industry and that's you guys and me.

926
00:56:15.660 --> 00:56:19.800
And we have a primary responsibility for dealing with these side effects in very

927
00:56:19.801 --> 00:56:22.800
much the way that the question over here was about,
you know,

928
00:56:22.801 --> 00:56:26.620
can we build AI systems that,
that meet our,
our normal standards.

929
00:56:26.860 --> 00:56:30.640
So I don't think we should throw up our hands were responsible for it.

930
00:56:30.820 --> 00:56:33.010
We have to fix this and it can be done.

931
00:56:33.790 --> 00:56:36.730
So I can go on at some length about this,
but,

932
00:56:37.050 --> 00:56:39.750
but if you look at what we've done for email,
many,

933
00:56:39.810 --> 00:56:43.510
I'll just give you one example.
Email.
As you may know,

934
00:56:43.570 --> 00:56:48.280
90% of all email,
it's sent as spam today.
And yet we have built systems.

935
00:56:48.460 --> 00:56:51.210
They managed to weed most vet out.
Uh,

936
00:56:51.350 --> 00:56:56.350
there are similar kinds of techniques appropriately applied to our social media

937
00:56:57.041 --> 00:56:59.890
space that can restore balance to the public.

938
00:57:00.360 --> 00:57:03.160
A discussion in the public space,
uh,

939
00:57:03.380 --> 00:57:07.390
that I think is going to be very important over the next five to 10 years.

940
00:57:07.630 --> 00:57:11.100
Otherwise,
we're,
we're heading for some real trouble with,
uh,

941
00:57:11.290 --> 00:57:14.110
the vibrancy and future possibly have our own democracy.

942
00:57:16.100 --> 00:57:21.100
<v 1>[inaudible].</v>

