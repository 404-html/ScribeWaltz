Speaker 1:          00:05          Good afternoon everybody. So thank you very much for coming to this session today where we have the pleasure of having Tomas Sharon to give us a presentation on the death of the research reports. So, um, but uh, as we get started, uh, peace, remember to turn off your cell phones and all buzzing devices. Um, so for many of you guys here who already know Tomer, well, welcome back to Google. For those of you guys who don't know Tomer well, he used to be a senior user experience researcher here at Google. I'm looking on the fourth floor of this building. That's when I got to know him. Um, so toma today is a thought leader and a few of our user experience design and research, uh, working as a vice president and head forward hit off user experience as we work, which is this, a startup that has thousands of locations worldwide, uh, in which, uh, Tomer leads this team that designs work and living spaces, communities and services around the world as we work.

Speaker 1:          01:05          So, uh, there's also a desk at a back of the room where are copies of the most recent book written by Tomo is on sale. So he's the author of two books, validating products, ideas, true lean user research that is on sale. Uh, also the author of a less reasons book, uh, is our research, which I've read and enjoyed a lot. Uh, and both books have a socon actually helped to increase the adoption of user experience research among product teams were white. Um, yeah, so, uh, this is how we're going to do a two day session. So first we would have [inaudible] up here giving a presentation for about 20 minutes and then we would transition to a file chats, a discussion format between me and him. I have some prepared questions for him, I hope will be the same questions that you guys have for his presentation.

Speaker 1:          01:57          And then after that we would open up a session to Q and a from the audience. Is that okay? Okay, great. And then memory gets to audience Q. And. A. I would just request that everybody stepped up to the mics on the both sides of the room. So that's all the questions that you raise can be heard by the rest of the room. It can also be recorded. Uh, yeah. And uh, and we'll leave enough time so that you guys can check out copies of his books at a desk later, which comes with a Google subsidized price. Okay. So with that, uh, well welcome Toma. Thank you Matt. It's great to be back and it's great to see some familiar faces. So when I joined, we work about close to two years ago I asked this question, how do you decide what to work on? And the answer to the question, the answer that I got is pretty much on our walls and our, and our people's t-shirt's do what you love. Um, what we're trying

Speaker 2:          03:00          to do right from the guests from the get go as we work is try and come up with something that helps people understand that passion. Do what you love is not necessarily the only way to decide what to work on. So we added, we created this diagram and we added human needs and ability. So let me explain. Passion is do what you love. You think a product of feature is really cool, you're really passionate about doing that. Uh, so you just do it. That's how many companies decide what to work on. We also added ability. Can you even do that, that thing that you want to do and um, and let's say that passion and ability, sometimes you see a lot of companies decide based on the ability as well. Uh, and then what we wanted to add that was completely missing at we work was considering human needs.

Speaker 2:          03:54          Do people actually need that thing that you're thinking of? And it has to be what you see here. It has to be a combination of all three because if it's only a combination between passion and need, then it's fantasy. You can't, it's a flying car. You can't really, you can't really do it if it's a combination between need and ability that is just work nine to five. There's no passion there. And, uh, forgive me, art lovers, but if it's a combination between the ability and passion than it's arth, uh, it's not really needed, but it's there. We're doing that anyway. So this is, this is how, uh, how we started a couple of words about we work, uh, we work as a company on a mission. This is the company mission to create a world where people work to make your life not just a living.

Speaker 2:          04:41          Um, these are current, we work cities. Um, I think we are in a 50 cities at the moment with the over 160 locations and, uh, and we're growing, we're opening a about 10 between 10 and 20 buildings once, um, once every month. Um, we worked, building we work is, is selling its customers. We call them members, um, space, community and services. So our spaces are known to be really, really beautiful, a really warm environment that invites people to collaborate. We have a lot of events that are going on, uh, throughout the day and over a weekend sometimes. And we also provide our members with a lot of services, a lot of digital products to support meaningful connections between our members and also, um, uh, apps and so on and so forth. Uh, to connect them services include a health insurance if they want. And, and probably hundreds of other, uh, different services.

Speaker 2:          05:50          This is my group's mission design, delightful experiences for our members through data informed human understanding. And here's the challenge with we work. So think about let's say, uh, I don't know, Steve Jobs. Um, he had a relatively easy job. If he wanted to understand the user experience of apple products, he could have said for most of them, bring me all of Apple's products to the table in front of me. I want to touch them. I want to feel them. I want to give you feedback. This guy, this is our cofounder and CEO, Adam Newman. Uh, he can't do that because our buildings are everywhere. Uh, we're opening in more and more far away locations and it's really, really hard to understand what is the user experience, what is our members experience? So from here, I jumped to the research that we do. And here are three problems that we see with research, uh, in companies, uh, happens here too, happens in probably every company that does research.

Speaker 2:          06:56          So first is bad memory. We have bad research memory. Um, let me know if this scenario sounds familiar. Other researchers here, I know there are some. So researchers, you don't have to say yes, but smile if it's true. Um, you want to do a research activity, some kind of study and then you just, before you start planning, you thinking to yourself, hmm, maybe somebody else did it already. So let me ask. So you send an email to all of the researchers in your group, maybe in your product area, may be all researchers at Google. And then, uh, you ask, did somebody do research about x? Whatever it is that you're interested in. And sure enough, by the end of the day you get responses. Um, I remember back in the day, it could go up to 10 responses from people who say, yes, we worked on something exactly like that couple of years ago.

Speaker 2:          07:53          You don't get 10 responses for people who completely forgot they did it maybe five years ago. And then, um, you're a nice person. So you summarize everything you read, you sent back to everybody. I see some smiles. So we know it's true, it's still true. Um, and then you decide to do this study anyway because yours is a little bit different than you want to learn something a little bit different. So you decide to do that anyway. So we have bad research memories. The second problem is silos. Researchers are not the only ones who are doing research in a company. Marketing is doing research. Product managers, whether they call it research or not, they are talking with a lot of users all the time and so on and so forth. If you have a, if you have a call center, they don't do research, but sometimes they issue reports about the top 10 problems that they're facing, uh, that, that people are calling them about. This is also a, a silo where this data is saved. So a lot of departments are doing research. Everybody's happy to share, but there's no one place to make sense out of all of that.

Speaker 3:          09:09          Okay.

Speaker 2:          09:10          And my pet peeve reports, um, I'm pretty sure, although I, I, I remember some exceptions here at goodwill. I'm pretty sure that there's not one person on the face of this earth that's wakes up in the morning saying, I'm going to read a report today. Um, and being a researcher for many years, I don't even like to write them. And uh, I think reports are not what I called the atomic unit of a research insight. Again, researchers would tell you that you go into a research study study and you have a short list of questions you want to answer. Those are the research questions. And then research is life. You look at life and life, uh, looks at you back and say, I have more answers. Then, uh, your question. So you find out more than what you intended to find. And then there's the report.

Speaker 2:          10:05          So you are, as a researcher, you have a dilemma. Should I report answers to the research questions that we were interested in a or should I report that? Plus a lot of other information that I found. Uh, there's no rule about that. So any researcher does whatever they think is right. Uh, in any case reports on not really atomic, there's a report that might include a piece of information that has nothing to do with the reason for running this, a specific research study. And yet it's an important insight that might serve people in the future, but it's buried in the region.

Speaker 3:          10:43          Fourth,

Speaker 2:          10:46          so I'm, I'm aware of this, these problems for many years. And, uh, as soon as I joined we work and was asked to start a group from scratch, there was not a lot of research going on there at the time. Um, I thought this is an opportunity to maybe solve all these problems with this system that we called Polaris. So,

Speaker 3:          11:08          okay.

Speaker 2:          11:10          Polaris is a way for, um, I call it democratize the user experience. Uh, I published a medium post about that and somebody responded saying, you're not really democratizing, why do you call it democratizing user expensive? You just, it's just a research archive. So, yes, it is a research archive, but uh, this, uh, that research archive is the, uh, the output democracy for user experience is the outcome. And that's really important. What's happening with Polaris is very interesting. Um, first of all, Polaris is a system that gathers all of the research that, uh, that is being done at we work. And it gathers that in a different, very different atomic unit than a report. I'll get to that in a second. In any case, Polaris is helping, um, anyone that we work who is designing anything, who's building anything was affecting the member experience to prioritize their work.

Speaker 2:          12:10          And I'll give you an example for that very shortly, um, to educate themselves. I know, um, that teams at, we work as teams here and again, researchers would, would know what I'm talking about are not waiting for anyone. They're not waiting for a researcher or for report or for a study to end to decide what to do or to decide what to work on. That said they are happy to learn and educate themselves about what they decided to do already. So Polaris is a system that helps educate these teams about what they're working on. And lastly, if by any chance there's a team was looking for a project, Polaris can help allocate or assign teams with different projects.

Speaker 2:          13:00          All right, when I talked about the atomic unit of a research insight, this is, this is it a, we call it a research nugget. And uh, we just celebrated a, a little while ago, uh, 5,000 nuggets entered to Polaris in a chicken nugget meal. Um, so what's the nugget and I get is a tagged observations supported by evidence tagged observation, supported by evidence. I'll explain observation. So observation is something that we learned during the study. We saw something, we understood something, we understood why it happened. This is an observation. So we just type it down. This is what we found and this is what explains it. Evidence. There has to be, there's no nugget. Nothing enters Polaris without evidence. It's not about the opinion of anyone. There has to be evidence proof that the observation that true is true. That could be, um, a short video clip. It could be 10 seconds.

Speaker 2:          14:05          It could be a minute, something like that. It could be a photo, it could be a screenshot of an email, it could be a document, it can be whatever it is, whatever media. But proof and evidence that what we said in the observation is a actually what happened. And lastly, and that's what makes Polaris, um, useful in special is a series of tags. The first thing we did when we designed Polaris was created a long taxonomy of tags. How we tag things, how we tag nuggets, uh, within that system. So these could be, um, tags that are pretty, uh, pretty much demographic. Uh, so which city and name of the location, uh, the size of the company and so on that, that the person is representing and so on and so forth. But there are also tags that relate to the experience. So is this observation of positive one neutral or negative?

Speaker 2:          15:06          Is this, um, what is the magnitude of that? What is the frequency and so on and so forth. Um, I call them the three Cs. Three things that people can, uh, can do in Polaris when they go into the system. They can create the nuggets. All right? They can search for different nuggets under different topics and, and um, and create playlists from these nuggets. They can. And this is how it looks like. So you see, uh, an interview here. So we just decide on the clip, uh, what is the, what is the length and the minute, the second that it starts and ends, uh, and that would be the evidence. If that's a, an interview you see a middle left to see, uh, an empty box for typing down the observation, uh, and under right and bottom left you see some of the tags, uh, that we're using.

Speaker 2:          16:03          Um, we tagged by journey, we tanked by emotions, objects and the experience metrics that I just described. And then, uh, you can also cure it. You can add and create a playlist and nuggets to playlists. Um, and you can share these with other people at we work. Uh, this is how I played us a look. Looks like, um, you might suspect they worked here for awhile. Um, so, uh, what you see, uh, obviously the video is, uh, the evidence just below it. You see other members squatting, blah, blah, blah. That's the observation and some of the tags on the right hand side, on the bottom right hand side and on the right is the playlist itself, the different nuggets that compile and create the playlist. You can see that you have nine nuggets there that will take you five, about five minutes to watch and so on.

Speaker 2:          17:04          People can consume, um, consumed these nuggets and playlists. I compared that to a healthy food at first. Uh, they don't really like it, but it's good for them. So they have to, um, and in time they understand that this is useful and they use it more and more. We see very interesting, um, usage scenarios, uh, tracking what's happening behind the scenes. Um, we see people who are, let's say there's a designer, interior designer that designed, um, a conference room at we work. So that person is looking for a feedback about conference room. Uh, we see people in building, in a building. We saw somebody in a, in Chicago watching all 200 nuggets related to that building in Chicago. Uh, and so on and so forth. We've seen teams are making decisions, roadmap decisions about their products based on what they find in Polaris. I want to give you an example for that.

Speaker 2:          18:05          Um, so to become a, we work member, uh, you usually take a tour. If you take a tour, you go through the building you were interested in, uh, they show you different things and then you decide if you want to join or not. Uh, we also serve coffee and we work. Uh, so let's say you are the general manager of we work in Europe and you want to see, um, do you want to get feedback about tours? You want to understand the situation about tours and coffee, what's more important? You search for negative, um, tours in Amsterdam or sorry, in Europe. Um, and then you might get a dish results 72 nuggets in the playlist. And when you search for coffee negative in Europe, you get three nuggets. That alone, without even watching the, the playlists, that alone tells you that there might be a bigger challenge there with tours rather than coffee. So your focus should be on, on, uh, understanding what's happening with tours.

Speaker 3:          19:16          Okay.

Speaker 2:          19:16          I want to show you a quick video of how it looks like and sounds like. So, uh, we also serve be year. So here's some feedback about beer.

Speaker 3:          19:31          Oops. Okay.

Speaker 2:          19:37          So you first, this is how you start the search. You search, these are, we call them props. So beer is a prop. Uh, you search, you select a location. If you're interested in some other filters, you apply them. If not, you move on and you get the results as a, as a playlist, you can save it.

Speaker 3:          20:07          Okay.

Speaker 2:          20:08          I have a very creative, a name for this very creative description

Speaker 3:          20:18          and

Speaker 2:          20:22          let's listen to a couple of, uh, of nuggets

Speaker 4:          20:28          having other than a [inaudible] over here. [inaudible] that to me, I'm no art, but

Speaker 2:          20:42          not everybody likes beer though.

Speaker 3:          20:44          Yeah.

Speaker 2:          20:45          Or the fact of having beer in an office.

Speaker 4:          20:50          It was noisy and they, I think, you know, not everybody's twice. If I'm having people come off and, you know, yeah, it's a cool face everyday, but, you know, having kids drink beer and all that crap is fine, but I think there's a whole nother niche for a little bit more sophisticated. Uh, minorities or people in Ad Tech, right?

Speaker 2:          21:13          So this is how it looks like and sounds like. I obviously pick the audio not to expose the identity of those people. Um, you can also watch a list, um, a playlist as a list, not just as a, as a videos or photos and

Speaker 3:          21:31          of course, no, no, no. Sorry about that. Okay.

Speaker 2:          21:49          All right. Um, and to sum it up, this is division for Polaris because this is not the end. What, what Polaris before I show you that what Belarus is doing is tell you, uh, tell us we work employees, uh, why things aren't happening. Our vision is also to provide a system that tells us, that explains what is happening. So we want to provide a score for everything at we work a quantitative score and then from that score lead to a Polaris playlists. So for example, beer, um, let's say beer is scoring a worldwide, there was a, the score is 44% out of 100. And you want to understand as the person who's responsible for beer, do we work? You want to understand why the score is so low. Then you can get, um, get to Polaris and see a, a playlist of people talking about, um, about beer.

Speaker 2:          22:47          The way we do that is by collecting, for example, satisfaction feedback, uh, through these little devices that we created on the top left. And we put them everywhere in the building, um, through surveys and through an APP. We call this the APP, uh, through an APP that our teams in the buildings, uh, pull out and ask people to rate different things that are happening around them. So this is, uh, how people can understand both what is happening and then understand why it's happening. Uh, I couldn't help it, but, uh, if, if you're interested, this is a, this is another research project that we completed it, we work, we found that a lot of people don't show up to tours because they, uh, can't find the entrance to the building. So this is how we, uh, how we solved it. So you can read about it there and I think

Speaker 1:          23:46          we are done. Yeah. Slide, blah, blah, blah, blah, blah, blah, blah. Bottom this deck, if you're interested. And second to last, if you want to start it on your own, just check it out and see if there's something that you can do. We created a, an air table template, uh, for Polaris for designing your own Polaris. So feel free to, um, to use it. Um, I think that's it. Thank you.

Speaker 3:          24:19          Okay. Hey, great.

Speaker 1:          24:21          Thank you so much for taking the time to explain this. And, uh, I was not expecting to have place that we could use to starts putting together something like this together. So, um, well, so some questions for you. So, uh, if we had to think about, you know, pull that reason and all these processes from the con, the producers and false starts, uh, it seems to me that there's a bit of this assymetry. Where are the people who are trying to put all these nuggets into the system or the guys are putting in all the hard work so that the consumers could actually benefit from this. So, um, so I mean, so in your experience, what have been some of the, you know, the, the socon more successful ways to actually incentivize more people in your organization to create a and archive these nuggets? So at this point I would say we are kind of a control freaks about, uh, what's going on, what goes in a Polaris outside the UX team.

Speaker 1:          25:16          Not a lot of people can, can add nuggets to Polaris. Uh, there are exceptions though. The exceptions are a team that asked us specifically to train them on. It's actually a, it's kind of a hidden way of training teams on how to do their own research. They want to add things to Polaris and we say, you know what, let's train you on how to do research. You'll do research. And then we're also training on how to add nuggets to the system. And then, and then they do that. So we, so far trained to, uh, different teams. Sometimes there's a person that is, you know, an individual that's interested personally in, in doing that, we train them as well. But not everyone is, is, uh, has access to do that. Okay, great. That's interesting. And that actually leads me to the next question. Uh, you know, because you were talking about all these things about observations versus evidence.

Speaker 1:          26:06          Yeah. Well, I mean, what does it actually take to train someone, you know, to, to actually, uh, so called Freeman and does something that is acceptable as a piece of evidence. Uh, we have conversations with the Ux team about many things related to Polaris specifically about that as well. It really depends. If you don't say anything, some people would write down and observation and then choose as evidence the exact same words that the person in the interview is saying, for example. Um, so we asked them and train them to kind of add some more context by extending the time that they, uh, as evidence. Uh, sometimes it's not just a recording, sometimes it's a photo and support that with a photo that better explains or provides evidence to what you were just saying. Um, it's not easy. Um, we have a lot of conversations about the quality, what qualifies as a great nugget.

Speaker 1:          27:02          And so on. We also have a term, a kind of internal Tammy, you don't see it in the system. We call it the golden nugget. A golden nugget is a nugget that is so special that you learned something really important from it. It's really actionable. It's something that we say it's like a headbanger. It's like, Oh man, we have to do something about that. Um, how many percent of your nuggets? Golden Nuggets. Oh, very few. Very, very few. Okay. Yeah. So I was wondering, you know, because yeah, it's that he's a sudden trash whole data piece of evidence needs to meet. So, um, so with those situations when, you know, there might be t employees who actually disagree about the evidence that go into nuggets or sometimes the fact that the evidence might be okay in itself, but it's not relevant to the observation. So what are you guys, what I'm going to say didn't happen yet.

Speaker 1:          27:54          We didn't implement this, but this is something that we're working on. Um, as we train new people and adding a nuggets and evidence to, to Polaris, uh, we want to great different roles of people. And then if you are a kind of a Newbie, a nugget Isare we call it, then, uh, then somebody will have to review and approve your maybe first 100 nuggets. So there's another eye that looks at things. It's not just you. So we hope to somehow compensate for things that don't count as evidence for example. Okay. Yeah. And then, well, as you mentioned, in order to know how good is the most atomic units of research findings, but sometimes our consumers don't really once, uh, two, two cats finding at that level of granularity, they want something like a higher level team that integrates and synthesizes many different nuggets. So do you guys provide any support for that?

Speaker 1:          28:50          Right now? Uh, we create, we also create and curate playlists. So these are UX create a curated playlist and we shared them with teams so we can do their work. We have a way to, it's like, in a way, it's like a report, but it's a playlist with, uh, maybe a few more words about it, some analysis into what is it that you're, you're seeing in the playlist and so on. So we do that. Um, and, and sometimes we train others to do that as well. Yeah. Yeah. Okay. So now switching over to the consumer side of things. Um, so what have you guys found to be the types and characteristics of research nuggets that were more useful to consumers? So I, I touched a little bit earlier. Um, sometimes nuggets, and these are the nuggets is that people who enter them, um, they just in an observation that just describe what happened.

Speaker 1:          29:44          Uh, that's not true. Not really useful. So, uh, the person said that beer is having beer in an office is not a good idea. That's not enough. As, as a nugget, we want to understand why or they held a me maybe describe something more meaningful. Um, the members said that they're taking meetings in a coffee shop rather than in our conference rooms. Interesting. But still, it's not a nugget because we don't understand why. Um, and if they add the why, for example, they do that because we work as too noisy. Then we have something that we can do something about. So, uh, this is meaningful. This is something that hints that there's a problem there. And we know what the problem is and if we search for noise and certainly if it's tagged by noise and conference rooms, and if we search for that, for lifts like that in Polaris and see, I don't know, 120 nuggets, we know we have a problem there that we need to do something about.

Speaker 1:          30:47          Uh, so these nuggets must be, um, kind of a unit and atomic unit and it's not an atomic unit if, if it doesn't have that, why? Okay. Got It. Yeah. So some product questions now because man, uh, understanding an impression about Polaris when watching our presentation is that there's always a pension in computing systems between poo and push mechanisms. Uh, Polaris seems to be something that's more pool driven. Yep. Uh, and you know, if, well, what might be some opportunities to think about supporting more push mechanisms. So currently we have a manual push mechanism. We push lists to people. We heard you've been working on ordering beer for, we work. Here's a playlist of, of people, uh, members from all of the world talking about beer. Uh, that's very manual. Um, one other thing that that's a feature can have an on our, on our roadmap, um, is uh, updating you as somebody who either showed some kind of interest in a playlist.

Speaker 1:          31:51          Either you created it or it was shared with you. Um, each time there's a new nugget that is being added to the list and it's being added automatically because of the tags, you'll get a notification about that. And then we hope that this will encourage people to try it out more. That's a great ideal. You know, as soon as you were talking about tagging, um, I was wondering when you guys were designing Polaris, uh, besides attacking approach, were there any other considerations that you guys considered for other approaches to organizing the nuggets? Uh, no, not really. Um, the only thing we were not sure of, we, it was very clear for us that this was an investment. So we wanted to be sure that this is working before we kind of make that investment. So we created that template that I shared. We created everything, uh, with air table, um, stored everything on the video recordings, on Youtube store, audio recordings on soundcloud.

Speaker 1:          32:53          And that was our system. Um, as soon as we saw the reaction to it, um, we decided, okay, this is worthwhile. And then we invested the time in, um, changing all of that to a system that we can control and okay. That makes total sense to me. Um, well what are some of the challenges to scaling something that Polaris and what are you guys doing about that? Um, so I would say there are two challenges that are kind of related. One is that it needs to be something that, that people like we work are aware of. Cause if they're not aware of it, obviously they're not going to use it. And the other challenge is once they are aware and they search for something, they have to see. I don't know if you noticed the dates, they have to see nuggets that are relatively new.

Speaker 1:          33:42          Otherwise they think it's an old, uh, database of information and they're not going to do anything about it. Although we all know that sometimes, uh, even if you found something a year or two ago, it doesn't mean that it's old. It doesn't, it doesn't mean that it's not true, but that's how people treat it. So we, our challenge is to keep it fresh. Um, and these are the challenges because to keep it fresh, it means that more people need to do research and nuggetize that, um, more and more. And that's something that we don't always have time to do. And awareness is, is again, always a challenge. We need to push and talk about it and mentioned it in every meeting. When somebody asks us a question, we shouldn't just answer, we should say. And that's what we try and do and remember ourselves to do.

Speaker 1:          34:32          Have you checked out Polaris? Do you know what the virus is? Maybe show them and then they start using it on their own? Yeah, I mean, do you guys have, I mean, at a system level, you actually have a way to designate certain of those nuggets as golden nuggets. So that it stuff, it's just a jargon between, yeah. Um, so what are some of the considerations around personally identifiable information? I mean, I saw that you guys had to, uh, you know, just blackout the faces and that's, that's only for the presentation. But yeah, uh, that's a, that's a challenge. I mean, we covered the identity of the person itself. So the member, the company name is not it, it's, it's a kind of a encrypted throats code. And uh, the same for the person's name. We don't even add it to the system. Um, that said, if you are, our people in the buildings are called community teams.

Speaker 1:          35:27          If you're a community manager and you see a video of a member from your building, you would know them. Even though you have a thousand members, you would know them. So are always concerned with people taking it the wrong way, especially if the feedback or the nugget is talking about you, the community manager. So we ask the people we talk with, not to mention names. So just say, so we say, for example, if you have feedback about, uh, your community manager, use that term. My community managers don't use a name. Um, they don't always follow instructions, but we tried. You guys do about evidence if they don't follow instructions. Uh, so it's, it's, it's something we have to cut and, and, and, and not include. Cause that's a, we don't want people to use it as, you know, I saw you saying something bad about me.

Speaker 1:          36:18          Why are you doing that? Um, that's always a challenge. But at least the name and the company name, they're not included in Polaris. They can, they can see that. Okay. Yeah. So going I think to more broader organizational wide issues. I mean, what do you see as some of the necessary conditions that need to be in place for organizations to actually adopt your ideas around Polaris effectively? Um, honestly, uh, I, I, I couldn't care less. I mean, it's really, honestly, I'll tell you that when I described Polaris to important people that we work, I was asked not to do it before we did it. Um, this is something that, uh, I'm sure other researchers here are, um, not foreign to the problems that I described. A, this is, these are problems that are happening for many years. In many companies and it was very clear to us that this is something that we work with benefit from.

Speaker 1:          37:19          So if the company is doing research or is beginning to do research, it's relevant. Other considerations, I think everything is solvable. We have challenges with making people aware of it, but that's something we need to deal with. This is what the company needs. This is the healthy food they needed, although it's not tasty at first. Okay. Well, so do you know of any other organizations that have already adopted your ideas around Polaris? Uh, since we made it public, a lot of companies contacted us. Uh, I don't really know if they implemented it, but um, all of Google's competitors, uh, kept in touch. I have no idea if, if they do anything about it, but there's interest, I don't really know. Wow. Okay. And I think that as a way to encourage us to try to adopt some of these ideas. I'll be honest, I tried here, but, uh, I didn't get a lot of traction. Um, I think what you see here is a, and we talked about it earlier, you see research archives and these research archives that you see here, our archives that you can just upload your report and uh, maybe tagging, I can't remember. Oh, sure.

Speaker 1:          38:30          Um, I think it's not enough. It's not, it's not atomic enough. And, um, and when you're tagging your not and you're thinking about the entire study of thinking about the entire report, the reports are long include many things and the insights as I described earlier and so on. So I would say, uh, no you don't have it and um, maybe it's an opportunity. Okay. Well, uh, so we have 15 minutes left and I just wanted to make sure that members of the audience get to ask any questions that you guys have. So we have mice on the two ends of the room. Free, free to just step out and ask her questions so that the rest of the room can hear your questions.

Speaker 3:          39:08          Okay.

Speaker 5:          39:10          I'm in an organization that loves research reports and I'd love to know a little bit besides just developing a tool, how you weaned people off of their dependency on it.

Speaker 1:          39:20          Why, why do they love the reports? I'm sure they don't have reports. They love the research because it's known. Yeah.

Speaker 5:          39:25          There like familiar so familiar way that they're used to.

Speaker 1:          39:28          You're talking about the researchers or, or uh, the consumers of research,

Speaker 5:          39:32          consumers of research, internal, internal clients of research.

Speaker 1:          39:36          I, I, to me the biggest insult as a researcher is when people read the report and they say, hmm, interesting. Um, I want it to be actionable and um, I think that if you track and see, I'm not gonna, I'm gonna, I'm not gonna you know, really you hear, but are they doing anything with these risk research reports? If the answer is no or not really or not, you know, what you expected, then, uh, I would say don't listen to what they say about reports, look at their behavior as you know, and uh, treat them as users. This is a design challenge and see what you can do about it. Well, Iris is not the only answer, I'm sure. Uh, it's just one possible answer. If reports worked for them and you're happy and they're happy, don't find them off of that.

Speaker 3:          40:25          Okay.

Speaker 5:          40:27          Take some air for your presentation I think is super interesting. And of course as a researcher, I'm always looking for any tool that can help us not only capture feedback but also communicate it with my stakeholders. I noticed one of the images that you had or you have a device, a physical device, one of the phone rooms. Yeah. So two questions. One is, um, how does it work? How is this connected to Polaris and second, why using a physical device instead of sending a survey to your, um, to the consumer?

Speaker 1:          41:04          Very, very, very good question. So, um, the product that is, we work is, has many, many details, many, many things that create it. Um, we think it's hundreds of hundreds of things. It's the beer and it's the, the chairs and it's the, the, the wallpaper and the events and the community team and so many other things. It doesn't make any sense to ask about 500 things. In a survey, uh, nobody would complete it. Um, that's one reason we don't solely use surveys for, uh, for these, uh, quantitative measurements. Um, we also know that especially for physical things and objects and spaces, the best way to collect feedback is there in context. So imagine a, a conference room. So you booked a conference room. Let's say the next time you book a conference room, you have a popup question on your screen that asks you, uh, how has your recent experience with the conference room you just, um, you used maybe five days ago.

Speaker 1:          42:11          Uh, we have that, but we learned from members that in many cases, first they don't remember how was the experience. Uh, and two, they, uh, the person who is booking the room, it's not necessarily the person who is using it. So the feedback is very biased. So in context is the best because we know you're there, we know you just use the room, give us your feedback. And that's it. That said, it's not connected to Polaris. Uh, this is a different system that gathers all this data about a scoring different spaces and other things. And then you get a score for different things and locations and regions. And so on. From there, we take that and we link that with a relevant Polaris or we would that's not connected yet. That's the vision. Connect that to a Polaris playlist, as I said. So if you see the data about beer is flowing to and creating that score in a dashboard somewhere, uh, and then that score, you'll have a link next to it that we'll connect you to a playlist about beer in Polaris. And these devices are based on, we created them. It's uh, these are, uh, Amazon dash buttons and the casing is something we designed.

Speaker 6:          43:29          Okay. Uh, so one of the things that reports sometimes we'll do is give you the background of the research that happens and action items that happen afterwards. How do you see your tool taking into account when significant change or impact has already come from the nuggets that exist?

Speaker 1:          43:48          So that's says democracy. I'm talking about, I'm almost eliminating the researcher from the equation here. I want the person who is watching the, creating the playlist, watching it and deciding what to do next. I want them to create that for themselves. They decide, they know what happened before, they know about the background and they know what they can learn from what they just saw. The researcher is there to help them. I want my, my idea is that everyone's a researcher. I want everyone to do that. Currently we're kind of in a transition mode where, uh, researchers and other Ux people are contributing or adding nuggets to Polaris. But as the consumer of it, let's say a, um, that's equated to what's happening here at product manager or an engineering team or, or whoever is deciding, uh, something about a product. I see them as the ones that almost write these for themselves.

Speaker 1:          44:46          It doesn't have to be in writing, has to be an understanding that they have what they know, what this, uh, what this can help them with, what decisions they can take based on that and so on. There's no, that's, that's exactly what the democracy I talked about. Um, it's not, uh, you know, at least when we use Polaris, the way I'm describing it, uh, those decisions are not in the hands of, um, researchers to write in a report. This is in the hands of consumers, people who are actually designing what to do with the products.

Speaker 6:          45:21          One quick followup I guess. Yeah. You prevent duplicate research from happening around her at the same time. So say if a community manager in Chicago, it goes through some nuggets and on beer and decides to take action based on those nuggets. And then a community manager here does the same thing. Does it duplicate the work or do you think they're just building on top of each other?

Speaker 1:          45:47          So when it comes to what we call consumables, beer, coffee, and so on, it's not that every building is deciding. Not always. Every building is deciding, deciding what to buy. It's being centralized. So it's not really about them. Uh, but if two different people decide to improve the member experience, that's a good thing. That didn't happen before. So that's a good thing. I don't see that as a problem. Consistency is not necessarily a must always.

Speaker 5:          46:15          Hi. I actually had a similar question and just to follow up some curious, are Ken, those community managers actually track that so they're aware of what's happening and what's track what the outcomes are, actions taken based on the nuggets?

Speaker 1:          46:29          Uh, not in Polaris. No.

Speaker 5:          46:32          So you can't necessarily see how someone else, no, you're fine. Do you think that's important or no?

Speaker 1:          46:38          Um, I think we work as a lot of systems in place for, uh, for doing that. We're not trying, I think it's important that we're not trying to solve that with, with Polaris we thought about kind of, we thought about adding a feature which was there for maybe two minutes and then we removed that of kind of allowing people who use a certain playlist that indicates a certain problem to raise their hand digitally in the system as people want to be a part of a team that solves the problem. Um, but then we were afraid that Polaris is going to be become a kind of a, instead of a research tool, it's going to be a project management tool. So we stop there. Okay.

Speaker 3:          47:17          Okay.

Speaker 6:          47:19          Okay. Hi, uh, so any evidence on how this has changed, uh, research reports that we work? Is it dead?

Speaker 1:          47:29          It changed resource

Speaker 5:          47:30          reports because we just don't write them. Yeah. I'm curious how, how has that changed the culture?

Speaker 1:          47:36          It's for, you know, anything that happens in my group, we don't write research reports. It's really, really hard for some people, but we're overcoming that. Um, and instead of writing a research report that if we really have the urge, we just create a playlist and then, uh, if we want to add things that we would have added to a report, we added as a feature request for Polaris and, and, and we make that happen if it's really, really needed. Um, that makes us, you know, we ask ourselves very hard questions sometimes. Uh, what is evidence? For example, do we really need, so there was one request, uh, to add multiple files as evidence. Do we really need three photos that show the same thing from different angles? Not always a, do we need a photo combined with an audio file? Maybe. Um, and so on. But yeah, for a,

Speaker 3:          48:34          yeah.

Speaker 1:          48:34          Anything related to my group, we don't, we don't write research reports. Uh, the hardest thing by the way is by, you know, when we share that with, uh, executives that are used to reading reports, uh, it's sometimes a process and education is involved in, you know, communicating that this is a report, this playlist, this is the report. That's it.

Speaker 3:          48:57          Okay.

Speaker 5:          48:58          Just a followup question. Yep. So, um, in terms of decision making and how do you prevent, um, product manager, sort of decision makers to take what users said at what the community says at face value without like photo there, um, analysis or maybe without checking the golden nuggets and you were imagining.

Speaker 1:          49:26          So, uh, I'm sure you know that what users say that is the solution to their problem is in many cases not the solution to the problem. So we, you exercise know that. So we don't even include that in, in, uh, in the nuggets. So they won't see that. Um, it is a process. This is why we work with these teams. It is a process to teach people and mentor people on how to make sense out of a playlist. And, uh, I should say, I mean, even for a very basic, um, I dunno, usability tests, if you put people in a room or watch videos, whatever, who are not researchers, not your Xers, never seen a usability test. They see videos of people using their product. After an hour of watching these day, we'll have, uh, enough information to come up with decisions on what to change in the product. And these will be very good decisions and in 99% of the cases. So I'm not, I'm not concerned about that at all. Okay. My last question,

Speaker 5:          50:33          you have researchers make or do you have researchers making the editorial decisions about what's worth putting in? So if you have a steady three out of five, four and a five for one and a five run into this, do you pick one that's representative and put that in as a problem and flag it? Do you put all three in combined? What does that look like?

Speaker 1:          50:54          Most of the nuggets that go into Polaris or not of not coming from studies, they're coming with ongoing conversations and interviews we have with members mostly with, um, uh, we call them graduating members with members and decided to leave us. Um, we, we think not because they're pissed at us necessarily. We think this is a good time, uh, for us to talk with them because they understand what's worked well at we work and what works not so well. It's we work. So this is not a study. This is something we do all the time and it's, it's, it's almost never a one at a five, two out of five, three out of five thing that comes to play when you search for, uh, for nuggets and create playlists. So that's the example I gave with the tours and, and, and coffee. Um, you see three nuggets.

Speaker 1:          51:43          That's your three. That's it. That's evidence that the only three nuggets about bad coffee, I don't know, then probably it's not a problem. Um, and, um, it's very rarely that we, I mean, not rarely, but we sometimes do these studies and then we just don't take it as one of the five. They had a fight. We just look at, let's say an interview or observation or whatever it is, and we just derived these nuggets and that's it. And then they go into Polaris, into the ether, and then anyone can search and create whatever they want, but then, hmm, okay. Yeah. I just want to antibiotic treatment. It's been 40 hours. So that does enough time for interested folks who checkouts copies of his book behind. So we bet, uh, please join me in thanking [inaudible] once again for joining us today.

Speaker 3:          52:29          Excellent.