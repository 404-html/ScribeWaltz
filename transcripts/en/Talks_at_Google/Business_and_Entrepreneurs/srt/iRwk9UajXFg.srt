1
00:00:08,180 --> 00:00:10,990
Hi everyone and welcome to
our toxic Google event. Oh,

2
00:00:10,991 --> 00:00:12,830
I'm going to go through
some logistics first.

3
00:00:13,070 --> 00:00:15,440
This is a fire side chat style talks.

4
00:00:15,441 --> 00:00:19,100
So it will take about 40 minutes
to talk through some questions.

5
00:00:19,101 --> 00:00:23,450
And then in the last 20 minutes we'll
open up to the audience to submit

6
00:00:23,451 --> 00:00:24,430
questions.
Um,

7
00:00:24,620 --> 00:00:28,430
both in person here through a Mike in
the back of the room and also through the

8
00:00:28,431 --> 00:00:31,670
dory at go slash ask Dash Sam.

9
00:00:32,240 --> 00:00:36,260
And I'd like to thank everyone who has
helped out to make this talk possible,

10
00:00:36,410 --> 00:00:41,030
including our facilities team and everyone
on the PM Speaker series organizing

11
00:00:41,031 --> 00:00:41,864
team.

12
00:00:43,490 --> 00:00:47,360
So it's great to have Sam Altman
here with us today for toxic Google.

13
00:00:47,870 --> 00:00:50,450
Sam is the president of y Combinator,

14
00:00:50,451 --> 00:00:54,890
which is widely regarded as one of the
top startup incubators in silicon valley.

15
00:00:55,580 --> 00:01:00,580
He went to Stanford and studied computer
science and was the founder and CEO of,

16
00:01:01,041 --> 00:01:05,450
uh, uh, mobile location
based startup called looped,

17
00:01:05,540 --> 00:01:10,540
which was funded by Yc as part of its
first class of startups in 2005 and

18
00:01:10,851 --> 00:01:15,851
acquired by Financial Services Company
green.in 2012 in 2014 he was named

19
00:01:18,351 --> 00:01:19,640
president of y Combinator.

20
00:01:19,641 --> 00:01:23,780
And since then he's worked on a wide
range of initiatives from YC research,

21
00:01:23,781 --> 00:01:28,781
which is a nonprofit branch
of y Combinator that focuses
on doing pure research

22
00:01:29,060 --> 00:01:32,330
around moonshot ideas like
universal basic income.

23
00:01:32,690 --> 00:01:37,490
And he's also worked on open Ai,
which is a nonprofit AI research company.

24
00:01:37,640 --> 00:01:41,840
Looking into finding ways to create safe,
friendly,

25
00:01:41,841 --> 00:01:45,920
artificial intelligence that can
actually help all of humanity.

26
00:01:46,430 --> 00:01:50,900
And it's great to have you here. Thanks
for having me. So just as I mentioned,

27
00:01:50,901 --> 00:01:54,740
you're involved in a wide range of
things from YC to open the eye and most

28
00:01:54,741 --> 00:01:57,470
recently you released the United slate.

29
00:01:57,710 --> 00:02:01,820
So I wanted to ask you how you think
about prioritizing the projects that you

30
00:02:01,821 --> 00:02:02,654
work on.

31
00:02:04,270 --> 00:02:07,090
Yeah,
I think optimal time allocation is,

32
00:02:07,091 --> 00:02:09,450
it's probably like an AI complete problem.
Um,

33
00:02:10,090 --> 00:02:13,570
I think if you can get to spending
like 1% of your time perfectly, that's,

34
00:02:13,571 --> 00:02:14,590
that's really good.
Uh,

35
00:02:14,800 --> 00:02:19,270
and so I think this idea of figuring out
what to focus on and what not to focus

36
00:02:19,271 --> 00:02:23,390
on is both really hard.
Uh,

37
00:02:23,470 --> 00:02:26,760
and sill significantly under invested in,
um,

38
00:02:27,100 --> 00:02:29,140
the frameworks that I have used.

39
00:02:29,141 --> 00:02:33,220
The sort of two big frameworks that I've
used to figure out how to allocate time,

40
00:02:33,310 --> 00:02:33,820
um,

41
00:02:33,820 --> 00:02:38,820
one is impact maximisation
slash regret minimization.

42
00:02:39,200 --> 00:02:39,760
Um,

43
00:02:39,760 --> 00:02:44,230
so I try to look at those two curves
together and I try to think about where I

44
00:02:44,231 --> 00:02:47,710
can have the biggest impact on the world.
Um,

45
00:02:48,620 --> 00:02:52,960
net positive impact on the world,
uh, easy to have, big impact. Um,

46
00:02:53,230 --> 00:02:57,010
and uh, and then also just regret
minimization. You know, you get,

47
00:02:57,670 --> 00:02:58,510
you get to live once.

48
00:02:58,510 --> 00:03:02,350
It's really that you do what you want
to do and that you spend time with the

49
00:03:02,351 --> 00:03:04,820
people you'd like to work with and work
on the things that you find personally

50
00:03:04,821 --> 00:03:05,321
if a filling.

51
00:03:05,321 --> 00:03:09,580
And so if I think I'm going to really
regret doing something or regret not doing

52
00:03:09,581 --> 00:03:10,610
something,
uh,

53
00:03:11,230 --> 00:03:14,840
even if I think it's not the best use
of my time for a pure sort of in the

54
00:03:14,841 --> 00:03:18,190
impact on the world, um, I'm
still willing to be like,

55
00:03:18,610 --> 00:03:19,810
to take that really seriously.

56
00:03:20,140 --> 00:03:22,710
And I think that makes me do
better at the things I do that,

57
00:03:22,730 --> 00:03:25,320
that do help the world. Uh, so, you know,

58
00:03:25,420 --> 00:03:29,920
the broad things that I've learned that
I like to do, um, one is teach people.

59
00:03:30,410 --> 00:03:33,530
Um, another is create economic growth. Uh,

60
00:03:33,570 --> 00:03:37,720
I really do believe that one of the
things that is most fundamentally going

61
00:03:37,721 --> 00:03:41,920
wrong right now in the country is that
we don't have enough economic growth and

62
00:03:41,921 --> 00:03:44,740
little that we do is not
evenly distributed at all. Uh,

63
00:03:44,830 --> 00:03:49,120
and so I think in a democracy,
um, you, you really want,

64
00:03:49,121 --> 00:03:50,620
everyone's lives to get better every year.

65
00:03:50,830 --> 00:03:55,210
We're basically insensitive
to the absolute quality of
their lives and extremely

66
00:03:55,211 --> 00:03:59,360
sensitive to relative differences year
over year to our neighbors. And it's,

67
00:03:59,361 --> 00:04:02,860
it's really important that everyone's
life is getting better constantly. Um,

68
00:04:02,861 --> 00:04:06,310
and I think economic growth supporting
to do that, I think that, um,

69
00:04:06,400 --> 00:04:10,810
I think the AI is going to be the most
important technological trend of our

70
00:04:10,811 --> 00:04:14,620
lifetime. So I'm spending a
lot of my time on that. Um,

71
00:04:15,940 --> 00:04:19,930
and you know, I try to think about
things on those two strategies. Uh,

72
00:04:20,410 --> 00:04:24,280
the other framework besides the sort of
impact maximisation regret minimization

73
00:04:24,730 --> 00:04:29,240
that I found really useful is spend
a little bit of effort, China,

74
00:04:29,260 --> 00:04:34,030
a lot of things and then relentlessly
prune down and focus quickly on the ones

75
00:04:34,031 --> 00:04:37,540
that you like and the ones that seemed
to be working. So in some sense,

76
00:04:37,541 --> 00:04:39,430
this is the y Combinator model,
um,

77
00:04:39,640 --> 00:04:44,440
of fund a lot of startups with a little
bit of money. Um, and then, you know,

78
00:04:44,470 --> 00:04:47,530
most don't work out in some work really
well and you spend more time and more

79
00:04:47,531 --> 00:04:51,220
money on the ones that do.
And this is the, uh, hey guys,

80
00:04:51,970 --> 00:04:56,970
this is the thing that I've tried
to apply to my life more generally,

81
00:04:57,210 --> 00:05:01,240
uh, is this idea that I can try a lot
of things with a little bit of effort.

82
00:05:01,480 --> 00:05:04,900
It's very hard to predict exactly what's
going to work and what hasn't. Um,

83
00:05:04,990 --> 00:05:08,890
but then the hard part about that and the
thing that most people don't do is you

84
00:05:08,891 --> 00:05:13,240
really want to relentlessly focus
down on, on the ones that do work. Um,

85
00:05:15,220 --> 00:05:17,200
and the last thing I'll say
about prioritization is,

86
00:05:17,201 --> 00:05:22,201
the other hack I have learned is if you
can get one really great partner with

87
00:05:23,201 --> 00:05:24,250
you on every project,

88
00:05:24,910 --> 00:05:28,210
that will cover up a lot of the slack
because if you try to do multiple things

89
00:05:28,211 --> 00:05:30,780
at once,
crises come at the same times.

90
00:05:30,780 --> 00:05:32,800
And that's really hard if you
have to do it all yourself.

91
00:05:33,880 --> 00:05:36,280
And focusing more
specifically on productivity,

92
00:05:36,310 --> 00:05:40,000
like what are some life hacks that maybe
you apply to make your everyday day

93
00:05:40,010 --> 00:05:42,340
more productive?
Um,

94
00:05:42,640 --> 00:05:46,090
and I think there's like a lot of crap
written about productivity secrets on the

95
00:05:46,091 --> 00:05:50,170
Internet and people sort of like get into
this thing where they spend more time

96
00:05:50,440 --> 00:05:52,870
like trying to be productive about
their productivity system and actually

97
00:05:52,871 --> 00:05:57,590
getting things done. Um,
I will say I think that,

98
00:05:58,990 --> 00:06:02,800
well, I'll say too, I think pieces of
advice that aren't the obvious. Um,

99
00:06:03,110 --> 00:06:08,110
one is I think far more important than
any particular system is just figuring

100
00:06:08,211 --> 00:06:09,230
out the right things to work on.

101
00:06:09,650 --> 00:06:13,550
And so all of the time that people spend
with like this new productivity app or

102
00:06:13,551 --> 00:06:15,800
that or whatever would be better spent.

103
00:06:16,040 --> 00:06:18,200
Like really trying to
think diligently about,

104
00:06:19,460 --> 00:06:21,980
I have the same number
of hours as anybody else.

105
00:06:22,100 --> 00:06:23,240
What am I going to spend them on?

106
00:06:23,360 --> 00:06:26,960
And getting that right is more important
than exactly like being perfectly

107
00:06:26,961 --> 00:06:28,980
productive with those hours.
Um,

108
00:06:30,620 --> 00:06:33,330
that'd be part of that is not
doing things that waste time. Uh,

109
00:06:33,440 --> 00:06:37,310
I think if you can just like focus on
the things that are important and not do

110
00:06:37,311 --> 00:06:38,950
the things that wastes time,
um,

111
00:06:39,080 --> 00:06:41,330
you can be fairly sloppy
with productivity otherwise,

112
00:06:41,331 --> 00:06:46,210
and you'll still get far more
done than most people. Um, it's,

113
00:06:47,290 --> 00:06:48,380
it's really hard to do that.

114
00:06:48,470 --> 00:06:53,300
The other thing that I think people
don't think about enough is figuring out

115
00:06:53,301 --> 00:06:55,790
your own personal,
like rhythms of productivity.

116
00:06:56,780 --> 00:07:00,500
And there's huge variance I've noticed
between people that figure this out and

117
00:07:00,501 --> 00:07:02,600
don't. Um, so like for me personally,

118
00:07:02,840 --> 00:07:06,980
it turns out that I am most
productive if I go to sleep late,

119
00:07:07,160 --> 00:07:07,910
wake up late,

120
00:07:07,910 --> 00:07:11,120
and then keep the first like three or
four hours of the day and don't schedule

121
00:07:11,121 --> 00:07:12,920
any meetings,
like work from home,

122
00:07:13,100 --> 00:07:16,730
like get through my list of stuff then
and then like pack all my meetings when

123
00:07:16,731 --> 00:07:17,930
I'm kind of less productive.

124
00:07:17,930 --> 00:07:22,010
It just grinding and stuff out or
thinking creatively in the afternoon. Um,

125
00:07:22,400 --> 00:07:26,640
and like it took me some number of years
to figure that out cause it didn't like

126
00:07:26,960 --> 00:07:30,440
fit well with the work schedule I was
naturally in. But then I was like,

127
00:07:30,441 --> 00:07:34,190
all right, if this is like the thing
to that makes me most productive,

128
00:07:34,550 --> 00:07:38,090
then I'm gonna like make my whole
schedule work to support that.

129
00:07:38,450 --> 00:07:40,880
And that was like a really
important change for me. Um,

130
00:07:40,881 --> 00:07:45,881
so I think figuring out your own personal
like optimal times to work on what

131
00:07:45,951 --> 00:07:48,980
kind of different things, uh, people
don't really talk about that much.

132
00:07:48,981 --> 00:07:52,430
And at least for me it had a huge impact.
Shifting gears a little bit,

133
00:07:52,431 --> 00:07:56,450
but still trying to get your
perspective on different things. Uh,

134
00:07:56,570 --> 00:07:57,890
related to day to day,

135
00:07:58,070 --> 00:08:03,070
one of the key issues that has come up
recently is bias in the workplace and

136
00:08:03,470 --> 00:08:05,570
both conscious and unconscious bias.

137
00:08:05,720 --> 00:08:09,230
And I want it to get your perspective
on what are some of the strategies that

138
00:08:09,231 --> 00:08:10,520
you implement personally,

139
00:08:10,790 --> 00:08:14,720
whether in the way you interact with
people that you encounter or in how you

140
00:08:14,721 --> 00:08:19,721
approach the decision making process in
using strategies to minimize your own

141
00:08:19,941 --> 00:08:23,060
bias.
Look,

142
00:08:23,061 --> 00:08:27,110
I think this is an important conversation
happening in silicon valley right now

143
00:08:27,111 --> 00:08:28,730
and there's a lot of
opinions on a lot of sides,

144
00:08:28,731 --> 00:08:31,330
but I would like to say
the following. I think, um,

145
00:08:33,050 --> 00:08:36,230
no matter what you believe about biology,

146
00:08:36,500 --> 00:08:41,500
no one that I respect does not believe
that women and racial minorities and a

147
00:08:41,931 --> 00:08:46,790
number of other groups face an
absolutely unfair playing field. Um,

148
00:08:46,880 --> 00:08:47,691
their entire lives.

149
00:08:47,691 --> 00:08:51,470
I think people start getting told directly
and indirectly from a very young age,

150
00:08:51,700 --> 00:08:55,590
um, this is the kind of thing you
should do or can do or whatever.

151
00:08:55,591 --> 00:08:58,200
And that has an effect,
a company if I can anyone.

152
00:08:58,620 --> 00:09:01,950
And I think trying to counter
that is really important.

153
00:09:02,160 --> 00:09:04,420
And I think um,

154
00:09:04,500 --> 00:09:08,670
again I think there are things people
can reasonable warming and people can

155
00:09:08,671 --> 00:09:09,504
disagree on.

156
00:09:09,700 --> 00:09:13,230
I think unfortunately we spend all our
time talking about the disagreements and

157
00:09:13,231 --> 00:09:17,520
we don't focus enough on the agreements.
And I think almost all smart,

158
00:09:17,521 --> 00:09:22,521
reasonable people will agree that the
society we grew up in has a hugely unfair

159
00:09:25,921 --> 00:09:27,490
paint Plainfield.
Um,

160
00:09:27,500 --> 00:09:31,800
and I think because of that it is not
enough to talk about unconscious bias.

161
00:09:31,950 --> 00:09:33,750
I think that is a real problem.
To be clear.

162
00:09:34,110 --> 00:09:38,670
I think we are all a product
of the society we are up
in and we all have biases

163
00:09:38,671 --> 00:09:42,330
that aren't our fault but still a
responsibility to counteract. Um,

164
00:09:42,690 --> 00:09:47,310
but one thing I don't like about the
discussion in Silicon Valley about

165
00:09:47,370 --> 00:09:51,780
unconscious bias and how
that's the problem that we
need to fix is I think it is

166
00:09:51,781 --> 00:09:54,840
not nearly sufficient and it
ignores the good thing to fix,

167
00:09:54,960 --> 00:09:58,710
but it ignores the fact that,
you know,

168
00:09:59,880 --> 00:10:04,350
decades or centuries of society have
built up a very uneven playing field.

169
00:10:04,770 --> 00:10:09,330
Um, and that is why we
do need programs, um,

170
00:10:09,390 --> 00:10:13,560
to try to proactively kind of that.
So, um, I think it's, you know,

171
00:10:14,460 --> 00:10:17,220
I think it's really important we not
lose sight of that and that unconscious

172
00:10:17,221 --> 00:10:20,610
bias training alone, although currently
very fashionable, will not fix that.

173
00:10:20,880 --> 00:10:24,600
That said, I do believe
unconscious bias is a problem.

174
00:10:24,840 --> 00:10:29,160
We try to counteract it a by talking
about it and doing training in which I

175
00:10:29,161 --> 00:10:33,060
think does help. Um, but B,

176
00:10:33,061 --> 00:10:37,020
I think one of the things that we have
done that unfortunately other investors

177
00:10:37,021 --> 00:10:41,910
have not done as much is just have
a very diverse, um, partnership.

178
00:10:42,450 --> 00:10:47,160
And we have, um, six
female gps on our team. Um,

179
00:10:47,760 --> 00:10:51,640
and, and that's probably
a large part of the, um,

180
00:10:51,690 --> 00:10:54,400
women in top of the best minerals
in silicon valley. Uh, and that's,

181
00:10:54,420 --> 00:10:58,560
that's really that we have, um, you know,
the CEO of our core program is black. Um,

182
00:10:58,620 --> 00:10:59,250
and I think,

183
00:10:59,250 --> 00:11:03,300
I hate to play the kind of like who's
the most discriminated stack game,

184
00:11:03,301 --> 00:11:06,870
but I think black entrepreneurs in
silicon valley have an exceptionally hard

185
00:11:06,871 --> 00:11:11,610
time. Um, and I think by
having a more diverse team,

186
00:11:11,611 --> 00:11:16,611
it helps us have broader networks and
also think about our own unconscious bias

187
00:11:16,831 --> 00:11:17,664
all the time

188
00:11:19,020 --> 00:11:23,850
and the issue of politics or maybe
even bringing these issues up in the

189
00:11:23,851 --> 00:11:26,250
workplace to some,
sometimes seen as a taboo.

190
00:11:26,610 --> 00:11:31,530
And yet you've been pretty vocal about
your views on current political events

191
00:11:31,531 --> 00:11:34,260
and also other issues that are
coming up in silicon valley.

192
00:11:34,261 --> 00:11:38,850
So I wanted to ask you how you walk
that fine line between expressing your

193
00:11:38,851 --> 00:11:39,361
opinions,

194
00:11:39,361 --> 00:11:43,290
but also minimizing any repercussions
that that could have on the day to day

195
00:11:43,291 --> 00:11:44,460
business of Yc.

196
00:11:44,700 --> 00:11:47,490
Well, now it's not even
controversial. Um, you know,

197
00:11:47,550 --> 00:11:51,610
like now all the tech CEOS are
talking about politics. Um,

198
00:11:52,770 --> 00:11:57,260
when, when I started doing it,
um, a couple of years ago, uh,

199
00:11:57,580 --> 00:12:01,650
kind of at the beginning of the rise of
Trump, it was controversial. Um, two,

200
00:12:01,651 --> 00:12:05,830
two things were going on.
One is I think no one took him seriously.

201
00:12:06,570 --> 00:12:07,403
Um,

202
00:12:08,230 --> 00:12:11,230
so most people rolling out of this is a
ridiculous thing that's going to go away

203
00:12:11,440 --> 00:12:13,230
too, is that, uh,

204
00:12:14,230 --> 00:12:19,230
I think in normal times it does make
sense for business leaders of large

205
00:12:19,721 --> 00:12:23,620
organizations to remain
apolitical. Um, you know,

206
00:12:23,650 --> 00:12:26,920
I think it actually does make that
makes a lot of sense. Uh, it's,

207
00:12:27,460 --> 00:12:28,840
it's a huge distraction.

208
00:12:29,500 --> 00:12:33,610
It's hugely time consuming and it has
all of these like weird negative effects.

209
00:12:33,611 --> 00:12:38,440
Like, you know, if you piss
off the president, he can do
bad, bad things to you. Um,

210
00:12:38,620 --> 00:12:43,060
however, however, these
are not normal times. Uh,

211
00:12:43,090 --> 00:12:45,520
and I think the,
you know,

212
00:12:45,521 --> 00:12:50,470
like when the future of the
republic is at risk, um,

213
00:12:50,620 --> 00:12:55,620
the duty to the country and our values
transcends the duty to your particular

214
00:12:56,021 --> 00:12:58,470
company and your stock price.
Uh,

215
00:12:58,780 --> 00:13:02,770
and I think I started that a little
bit earlier than other people,

216
00:13:02,950 --> 00:13:06,160
but at this point I'm in very good
company and it doesn't seem to be that

217
00:13:06,161 --> 00:13:06,994
controversial anymore.

218
00:13:08,530 --> 00:13:12,160
What are some things that you wish people
knew about you that they don't know

219
00:13:12,161 --> 00:13:16,570
about you? Uh, you know,

220
00:13:16,571 --> 00:13:20,800
at this point I would just like to have
like my own little quiet private life

221
00:13:20,801 --> 00:13:25,120
back. I don't feel like, um,
I'll say nothing at all. Okay,

222
00:13:25,660 --> 00:13:26,493
fair answer.

223
00:13:29,740 --> 00:13:31,820
Moving on to talking a
little bit about why,

224
00:13:31,830 --> 00:13:33,640
see and what you look for in companies.

225
00:13:34,270 --> 00:13:39,130
Are there any qualities in founders that
you think are overrated or underrated?

226
00:13:40,600 --> 00:13:42,760
Yeah,
I think the most underrated quality,

227
00:13:45,160 --> 00:13:49,050
the most underrated quality of all
is being really determined. Um,

228
00:13:49,300 --> 00:13:51,160
this is more important than being smart.

229
00:13:51,340 --> 00:13:53,230
This is more important
than having a network.

230
00:13:53,231 --> 00:13:55,130
This is more important than a great idea.
Um,

231
00:13:55,240 --> 00:13:58,660
the hardest thing about
starting a company is

232
00:14:00,670 --> 00:14:03,850
the level and the frequency of
bad stuff that happens to you.

233
00:14:04,240 --> 00:14:08,620
And most people that are good and really
other ways eventually just get killed.

234
00:14:08,650 --> 00:14:11,920
The company gets killed
by, um, stuff going wrong.

235
00:14:12,400 --> 00:14:17,400
And you know so much about being a
successful entrepreneur is just not giving

236
00:14:17,711 --> 00:14:22,360
up. Um, when we have funded
people who have a great idea,

237
00:14:22,870 --> 00:14:26,890
perfect background on
paper and, um, you know,

238
00:14:26,980 --> 00:14:29,250
a great product and still failed,
uh,

239
00:14:29,260 --> 00:14:31,420
it has usually been that they're
insufficiently determined.

240
00:14:31,630 --> 00:14:35,610
So I think this is the most important
non obvious skill of a founder. Um,

241
00:14:35,611 --> 00:14:38,020
of course you need a good product
and a good market and to be smart,

242
00:14:38,260 --> 00:14:43,120
but that's really obvious. Um, the
degree to which being like a, you know,

243
00:14:43,240 --> 00:14:48,240
three or four standard deviation outlier
on determination is a required skill of

244
00:14:48,251 --> 00:14:52,880
a CEO is not something that was
obvious to me when I started.

245
00:14:53,700 --> 00:14:56,630
Uh, that's also bad because
it's really hard to select.

246
00:14:56,660 --> 00:14:58,940
It's really hard to identify that,
you know,

247
00:14:58,941 --> 00:15:02,150
as we have said more publicly
how important that is.

248
00:15:02,780 --> 00:15:04,230
People applying to Yc,

249
00:15:04,231 --> 00:15:07,760
I've gotten better and better at telling
us stories from their past life about

250
00:15:07,761 --> 00:15:12,130
how they overcame these impossible
odds to get through something. Um, and,

251
00:15:12,370 --> 00:15:15,200
and unlike intelligence,

252
00:15:15,201 --> 00:15:19,190
which is very difficult to fake
in a, you know, one hour meeting,

253
00:15:19,220 --> 00:15:22,730
you can definitely fake determination.
In a one hour meeting. Um,

254
00:15:23,210 --> 00:15:25,490
so that's one thing that really matters.
Um,

255
00:15:26,840 --> 00:15:31,700
another thing that really matters that is
non obvious, uh, is independent thought.

256
00:15:32,260 --> 00:15:32,541
And,

257
00:15:32,541 --> 00:15:37,541
and this I think as an even more
unusual skill then determination.

258
00:15:38,650 --> 00:15:41,600
Um, I think both of these you can
make a conscious effort and build up,

259
00:15:41,960 --> 00:15:46,960
but I think independent thought
is one of the hardest skills to,

260
00:15:48,020 --> 00:15:50,340
um,
to,

261
00:15:50,360 --> 00:15:55,270
to build up because like we are all
speaking of social pressures from birth,

262
00:15:55,990 --> 00:15:59,220
we are all pushed to think
like other people. Uh,

263
00:15:59,380 --> 00:16:02,920
and if you think in your own lives about
the number of people you spend time

264
00:16:02,921 --> 00:16:07,921
with that you would say are
true independent thinkers
that consistently have new

265
00:16:08,261 --> 00:16:10,870
ideas you haven't heard from other people
and think about the world in different

266
00:16:10,871 --> 00:16:14,140
ways. Um, it's probably a very short list.

267
00:16:15,800 --> 00:16:18,650
And yet these are the people that start
all the interesting companies that

268
00:16:18,651 --> 00:16:22,730
consensus ideas. Everyone tries.
Google will eat your lunch at. Um,

269
00:16:23,210 --> 00:16:25,860
and they're also not kind of the,

270
00:16:26,170 --> 00:16:29,810
the really big trends of the future.
You want startup ideas.

271
00:16:30,550 --> 00:16:33,640
If you picture a Venn diagram
and here you have, um,

272
00:16:33,830 --> 00:16:38,290
is a good idea and here you
have sounds like a bad idea. Um,

273
00:16:38,750 --> 00:16:41,680
you want that tiny little overlap and,

274
00:16:41,710 --> 00:16:46,710
and those are the kinds of ideas that
are the hardest to identify and the ones

275
00:16:47,301 --> 00:16:51,740
that, um, even if you do manage to notice
them, most people will talk you out of.

276
00:16:52,040 --> 00:16:55,010
So I'd say those are two non
obvious skills that we look for.

277
00:16:56,120 --> 00:17:01,120
And what's one challenge that YC companies
face repeatedly that you've noticed?

278
00:17:02,450 --> 00:17:06,500
Um, hiring engineers, uh,

279
00:17:06,530 --> 00:17:09,050
when Google can just sort of
like throw unlimited cash.

280
00:17:09,090 --> 00:17:13,100
Anyone they want has become very
problematic for startups. Um,

281
00:17:14,710 --> 00:17:17,750
I think the good thing about
that, I always try to, like,

282
00:17:18,170 --> 00:17:20,420
I always try to find the
good in any bad situation.

283
00:17:20,750 --> 00:17:25,750
And the good thing about this is it means
that if a startup does not all of the

284
00:17:27,051 --> 00:17:31,070
startup, all of the really good startups
have really important missions. Um,

285
00:17:31,100 --> 00:17:33,080
eventually they figure it out.
They may not have an on day one,

286
00:17:33,081 --> 00:17:37,190
but they eventually get to
this like missionary mindset.

287
00:17:37,640 --> 00:17:40,910
And it used to be that even
if you didn't have that,

288
00:17:40,940 --> 00:17:44,210
like you could just sort of get a bunch
of mercenaries to come work for you and

289
00:17:44,211 --> 00:17:47,410
now you can't because you
cannot be Google. And so, um,

290
00:17:47,600 --> 00:17:52,140
one positive side effect of this
thing has been that startup,

291
00:17:52,170 --> 00:17:57,170
the importance now of a startup having
a really clear and really important

292
00:17:57,361 --> 00:18:00,480
mission on day three has gone up a lot.

293
00:18:00,481 --> 00:18:02,760
And I think that's what you need it
better startups because otherwise you just

294
00:18:02,761 --> 00:18:05,820
can't recruit. Um, uh,

295
00:18:05,980 --> 00:18:08,010
another common problem that startups have,

296
00:18:08,160 --> 00:18:11,100
and this doesn't sound
like a great insight, ah,

297
00:18:11,520 --> 00:18:15,480
but most startups still don't ever
build a product that people want.

298
00:18:17,130 --> 00:18:21,840
And it doesn't seem to matter
how much we talk about this.

299
00:18:22,050 --> 00:18:24,090
It doesn't seem to matter how
much anyone talks about this.

300
00:18:24,300 --> 00:18:26,700
People still keep trying
to do anything about this.

301
00:18:27,300 --> 00:18:30,510
And if there's like one thing
that a startup has to get right,

302
00:18:31,740 --> 00:18:34,140
it's built a product that people
really want, not a little,

303
00:18:34,170 --> 00:18:36,900
if they want it a little bit, that kind
of, you won't generate enough momentum.

304
00:18:37,080 --> 00:18:41,760
Like you got to build something
that some people really love and,

305
00:18:42,300 --> 00:18:45,870
um, after failing because of
insufficiently determine founders,

306
00:18:46,140 --> 00:18:50,100
this is the number two reason
that startups that seem
really good otherwise fail.

307
00:18:51,120 --> 00:18:51,710
Okay.

308
00:18:51,710 --> 00:18:53,990
Moving on to talking about
different technologies,

309
00:18:54,140 --> 00:18:58,700
what would you say is the
biggest challenge that we're
facing in terms of making

310
00:18:58,701 --> 00:19:00,800
progress on artificial
intelligence right now?

311
00:19:03,440 --> 00:19:05,470
Well, I don't know if any
of you saw this, but, um,

312
00:19:05,840 --> 00:19:10,840
open AI beat the best one being the
best single player Dota players in the

313
00:19:11,571 --> 00:19:16,460
world. Um, last Friday and I,

314
00:19:16,760 --> 00:19:18,620
when we started that project
at the beginning of this year,

315
00:19:18,920 --> 00:19:21,440
I did not think that was gonna happen
this year and I wasn't even sure it was

316
00:19:21,441 --> 00:19:23,300
going to happen next year.
Um,

317
00:19:23,870 --> 00:19:28,870
and it was very wild to watch that
happen because it was almost purely self

318
00:19:31,551 --> 00:19:36,150
training. Um, and it was this
very complex environment. Um,

319
00:19:36,290 --> 00:19:40,490
and the AI was just playing itself and
getting better and better and better. Um,

320
00:19:40,970 --> 00:19:45,890
in fact, the final Bot that we had
that beat all the humans handily,

321
00:19:46,260 --> 00:19:49,700
um,
one day later it lost 60,

322
00:19:49,701 --> 00:19:51,980
42 oh one day more of
all version of itself,

323
00:19:52,760 --> 00:19:57,530
just to give you a sense for the
rate of improvement. Um, and,

324
00:19:59,060 --> 00:20:01,690
and so I think people are kind
of asleep at the wheel here. Um,

325
00:20:01,880 --> 00:20:03,860
there are problems,
but again,

326
00:20:03,861 --> 00:20:08,390
in that same spirit of always trying to
figure out like the good, not the bad, um,

327
00:20:09,350 --> 00:20:14,240
we are making unbelievable progress.
Maybe too much progress. Um, but I think,

328
00:20:17,800 --> 00:20:20,110
I think of all the things
I worry about with Ai,

329
00:20:20,350 --> 00:20:22,630
the technical barriers to progress
or not the top of the list,

330
00:20:24,190 --> 00:20:28,960
there's another camp that says that maybe
we're placing too much emphasis on the

331
00:20:28,961 --> 00:20:31,540
threat of Ai. Uh, what
do you think about that?

332
00:20:32,370 --> 00:20:37,170
Yeah, I think we, we don't
talk enough about the benefits.

333
00:20:37,320 --> 00:20:38,153
Um,

334
00:20:38,430 --> 00:20:43,430
I think the AI has the potential to
eliminate like nearly all human suffering

335
00:20:44,220 --> 00:20:47,190
and the next couple of decades.
I think we can have a world of abundance.

336
00:20:47,260 --> 00:20:51,340
We can eliminate poverty over time. We
can probably cure a whole lot of diseases.

337
00:20:51,670 --> 00:20:55,070
Um, bear all these wonderful
things that technology can do. A,

338
00:20:55,150 --> 00:20:58,300
and I think we're already seeing that and
just how much better a lot of consumer

339
00:20:58,301 --> 00:21:01,120
products we use every day had gotten.
Um,

340
00:21:02,680 --> 00:21:04,960
people like disaster porn.

341
00:21:04,961 --> 00:21:08,740
People are more interested in talking
about the end of the world than they are

342
00:21:08,741 --> 00:21:11,590
about life getting 10% better every
year and having that component,

343
00:21:11,591 --> 00:21:14,920
which gets a lot better. Um, so you know,

344
00:21:15,850 --> 00:21:18,370
you can,
if you're a journalist,

345
00:21:18,460 --> 00:21:21,910
you can write an article about how AI is
going to end the world and get a lot of

346
00:21:21,911 --> 00:21:24,820
clicks and you know,
a page of your bonus or whatever you got.

347
00:21:25,170 --> 00:21:29,080
Or you can like write an article about
how AI is like gradually making all these

348
00:21:29,081 --> 00:21:33,160
problems 10% better. And probably no one
reads it. Certainly no one shares it.

349
00:21:34,180 --> 00:21:38,110
And so I think
for whatever reason,

350
00:21:38,111 --> 00:21:40,870
the way we'll wire it is to talk much
more about the downside of this than the

351
00:21:40,871 --> 00:21:43,900
upside. But the upside I think is
going to be huge. It already is huge.

352
00:21:46,200 --> 00:21:50,670
And what do you think about
cryptocurrencies? Do you see, um,

353
00:21:50,790 --> 00:21:54,960
same growth? Uh, yeah. I
mean, I think, you know,

354
00:21:54,961 --> 00:21:57,840
it's going to go up and down.
I'm like,

355
00:21:57,841 --> 00:22:02,490
I bought my bitcoin a long time ago. I
plan to hold them for a really long time.

356
00:22:02,491 --> 00:22:04,050
I try not to watch the price text,

357
00:22:04,051 --> 00:22:08,340
but it's so addicting that I
can't help myself. Um, I think

358
00:22:12,990 --> 00:22:16,170
this is not investment
advice. Um, I think,

359
00:22:18,450 --> 00:22:23,220
I think the only super compelling use
proven use case we have seen so far is

360
00:22:23,221 --> 00:22:26,220
store value and as a replacement for gold,

361
00:22:26,340 --> 00:22:30,000
I think we are seeing real adoption
there and real collective belief that

362
00:22:30,001 --> 00:22:34,920
actually makes it have some
value. Um, however, if that's
how it's gonna play out,

363
00:22:35,190 --> 00:22:39,840
then I think, um, bitcoin should dominate
biggest network, first biggest brands,

364
00:22:40,260 --> 00:22:42,490
most collective belief, whatever. Uh,

365
00:22:42,810 --> 00:22:47,810
and so I have been surprised
by the continual strength
and all of the alt coins.

366
00:22:48,810 --> 00:22:49,643
Um,

367
00:22:50,640 --> 00:22:55,640
I think there are potential other truly
valuable applications of the blockchain.

368
00:22:57,990 --> 00:23:00,690
Um, you know, file coin has a YC thing,

369
00:23:00,691 --> 00:23:02,820
so I know that one pretty well
and I can see that being big,

370
00:23:03,090 --> 00:23:07,860
but I think most of what's going on
feels like a complete outside of bitcoin.

371
00:23:08,070 --> 00:23:11,490
It feels like a complete speculative
bubble and I feel bad that a lot of people

372
00:23:11,491 --> 00:23:12,324
are going to get burned.

373
00:23:13,110 --> 00:23:17,130
So I think probably the right thing
to do if you believe in it, um,

374
00:23:17,520 --> 00:23:21,300
is to, you know, buy bitcoin and then I'll
think about it for another five years.

375
00:23:23,270 --> 00:23:26,540
Is there any specific product area or
technology that you think people are

376
00:23:26,541 --> 00:23:27,374
sleeping on?

377
00:23:28,840 --> 00:23:29,673
MMM,

378
00:23:32,370 --> 00:23:35,610
sure a lot. Um, AI as we mentioned,

379
00:23:35,611 --> 00:23:38,700
I think people are asleep at the
wheel on a really big way on um,

380
00:23:39,420 --> 00:23:44,420
nuclear fusion I think is within some
single digit number of years of working.

381
00:23:45,650 --> 00:23:46,483
Uh,

382
00:23:46,700 --> 00:23:51,700
and because it's been so bad for so long,

383
00:23:52,010 --> 00:23:56,180
um, people are just kind of burned out
and not really taking a new look at new

384
00:23:56,181 --> 00:23:59,330
materials and stronger magnets and better
computer models that's going to enable

385
00:23:59,331 --> 00:24:02,990
this to work. Um,
synthetic biology, again,

386
00:24:02,991 --> 00:24:05,480
it's like people talked about
it a lot a couple of years ago.

387
00:24:05,510 --> 00:24:08,240
It didn't quite work as
fast as people are hoping.

388
00:24:08,600 --> 00:24:10,910
So we have like this hype cycle
and then it really falls off.

389
00:24:10,940 --> 00:24:14,120
And now we're in the part where I think
the interesting work is happening and

390
00:24:14,121 --> 00:24:16,290
people aren't paying enough attention.
Um,

391
00:24:19,180 --> 00:24:23,410
I guess that's a topic. Hang on for a long
time, but I'll limit it to three. Sure.

392
00:24:24,280 --> 00:24:27,190
Is there any particular problem that
you think technology can't solve?

393
00:24:29,030 --> 00:24:31,310
How to make us nice to each other?
Uh,

394
00:24:31,550 --> 00:24:36,200
technology has clearly been quite bad
at solving. Um, I won't say can't.

395
00:24:36,670 --> 00:24:36,920
Um,

396
00:24:36,920 --> 00:24:40,430
but I haven't seen it yet and I certainly
don't think technology can by itself.

397
00:24:40,880 --> 00:24:43,280
Um,
and I think the,

398
00:24:45,750 --> 00:24:47,830
I think we are kind of in this situation,
um,

399
00:24:47,850 --> 00:24:51,930
at least in the developed world where the
world keeps getting better and we keep

400
00:24:51,931 --> 00:24:55,830
getting unhappier. And you know, there's
like a decent amount of data on this.

401
00:24:55,831 --> 00:24:58,290
And I think,
um,

402
00:25:00,390 --> 00:25:05,340
I think technology is not entirely to
blame, but it's certainly not blameless.

403
00:25:06,270 --> 00:25:10,320
And, um, I think it's 30 minutes left.

404
00:25:10,321 --> 00:25:11,610
I couldn't even start the conversation,

405
00:25:11,611 --> 00:25:16,611
but I think the number of like things
that technology has done to make us,

406
00:25:17,400 --> 00:25:20,530
um, more isolated, um,

407
00:25:20,850 --> 00:25:25,560
feeling relatively worse.
I have a friend of mine,

408
00:25:25,650 --> 00:25:27,270
um,
liberal,

409
00:25:27,300 --> 00:25:32,300
older said she never used to be unhappy
because she had no idea what she was

410
00:25:32,461 --> 00:25:37,020
missing. And now that she has to watch,
like I'm very bad at pop culture,

411
00:25:37,021 --> 00:25:39,150
I'm going to pick a name,
a random Kim Kardashians,

412
00:25:39,300 --> 00:25:41,310
like fly around in private
jets and Instagram all day.

413
00:25:41,311 --> 00:25:45,120
She's jealous and it like makes her very
unhappy and it didn't use to happen.

414
00:25:45,910 --> 00:25:50,640
Um, and I'm in the thinking about that
a lot in the last couple of weeks and I,

415
00:25:51,450 --> 00:25:54,360
I don't have a solution to that,
but I understand why that's a problem.

416
00:25:54,361 --> 00:25:55,790
So I think,
um,

417
00:25:56,430 --> 00:26:00,060
figuring out how to make us like happier
and nicer to each other in particular.

418
00:26:00,240 --> 00:26:05,170
I think one thing that technology
does, it's really bad is, you know,

419
00:26:05,280 --> 00:26:10,280
we have some probably
longstanding evolutionary
pressure that even if I really

420
00:26:10,741 --> 00:26:13,740
don't like you, I'm very unlikely
to come stand next to you and say,

421
00:26:14,040 --> 00:26:16,110
I fucking hate you. You're
a jerk. Like, you know,

422
00:26:16,111 --> 00:26:20,310
whatever else people say on Twitter.
A lot of things because there's this like,

423
00:26:20,370 --> 00:26:20,820
you know,

424
00:26:20,820 --> 00:26:25,290
we're like these pack animals and we have
to like live with each other and help

425
00:26:25,291 --> 00:26:29,850
each other survive. Um, but somehow
like on Twitter, that goes away.

426
00:26:30,780 --> 00:26:35,040
And, um, and I think like I'll
say my whole rant about Twitter,

427
00:26:35,041 --> 00:26:40,041
but I think like that is a platform in
particular that rewards saying the most

428
00:26:41,970 --> 00:26:43,800
aggressive,
snarky things.

429
00:26:43,801 --> 00:26:46,470
That's how you get likes and retweets
and sort of value out of the platform.

430
00:26:46,620 --> 00:26:48,420
But a lot of technology does this,
it,

431
00:26:48,480 --> 00:26:53,130
we don't have whatever kind of like the
human be nice to each other in person.

432
00:26:53,880 --> 00:26:57,390
Instinct is, and we have these
platforms that reward being bad,

433
00:26:58,530 --> 00:27:01,980
reward being a jerk.
Uh, and, and so I think,

434
00:27:04,620 --> 00:27:06,780
I'm not optimistic that technology
is going to solve that problem.

435
00:27:06,781 --> 00:27:08,880
I think that's going to have to
be people saw me not a problem.

436
00:27:10,700 --> 00:27:15,410
In an interview with Vanity Fair in 2015
you mentioned that you were optimistic

437
00:27:15,411 --> 00:27:19,940
about the future and that was right
before all of the election stuff happened

438
00:27:20,390 --> 00:27:23,030
and the situation that
we find ourselves in.

439
00:27:23,031 --> 00:27:26,570
So would you say that you're
still optimistic about the future?

440
00:27:26,960 --> 00:27:30,680
Um, like progress is not,

441
00:27:31,780 --> 00:27:35,360
it's, it's not a perfectly
exponential curve. It's not,

442
00:27:35,361 --> 00:27:39,770
it's not even a straight
line. Um, and you know,

443
00:27:39,771 --> 00:27:43,580
we are clearly in a
challenging period now, um, but

444
00:27:45,960 --> 00:27:49,710
I think if you look back at the last few
hundred and then a few thousand years,

445
00:27:49,780 --> 00:27:50,613
um,

446
00:27:51,450 --> 00:27:56,250
if you zoom out enough that the
squiggles on the curve kind of disappear,

447
00:27:56,580 --> 00:27:57,413
um,

448
00:27:58,140 --> 00:28:00,870
the world's getting so much better and
I think that's going to keep happening.

449
00:28:01,350 --> 00:28:05,520
Um, and I am, you know, even with
everything going on right now,

450
00:28:05,940 --> 00:28:09,150
um, I'm delighted to be alive right now
and not a hundred years ago and certainly

451
00:28:09,151 --> 00:28:12,150
not, you know, 202,000 years ago. Um,

452
00:28:13,230 --> 00:28:16,530
and I think we do have technology to
thank for a lot and we have better

453
00:28:16,560 --> 00:28:20,760
governance. That's like the number
of people living in a democracy. Uh,

454
00:28:21,240 --> 00:28:23,640
you know,
200 years ago was very low.

455
00:28:23,670 --> 00:28:26,640
The percentage and as
horrible as things are,

456
00:28:26,641 --> 00:28:31,250
the fact that we get to stand up and
speak our minds without fear of, you know,

457
00:28:31,300 --> 00:28:34,860
being thrown in jail for
political opposition and
the fact that we get to vote

458
00:28:34,861 --> 00:28:37,290
again three and a half more years.
Um,

459
00:28:37,820 --> 00:28:41,700
I think that's amazing and I think it's
easy to take that for granted. But, uh,

460
00:28:43,050 --> 00:28:46,650
yeah, I think the future is going to be
a lot better. I remain very optimistic.

461
00:28:47,130 --> 00:28:50,850
What do you see as being a big challenge
that we're facing as a society right

462
00:28:50,851 --> 00:28:51,684
now?

463
00:28:52,410 --> 00:28:53,243
Uh,

464
00:28:54,610 --> 00:28:59,610
how we deal with a world where the natural
forces are for wealth to concentrate

465
00:29:01,511 --> 00:29:05,950
into the hands of a smaller and smaller
number of people. As I mentioned earlier,

466
00:29:06,090 --> 00:29:09,640
um, I think people are more sensitive
to relative quality of life,

467
00:29:10,220 --> 00:29:13,960
of an absolute quality of life. And I
think technology is naturally a force.

468
00:29:13,961 --> 00:29:18,590
It's a giant lever that tends
to create way more wealth, um,

469
00:29:18,700 --> 00:29:19,900
but really concentrated.

470
00:29:20,080 --> 00:29:24,460
So I think one of the most
tone deaf things people say
in Silicon Valley is, um,

471
00:29:25,810 --> 00:29:28,420
you know,
like poor people should be happy.

472
00:29:28,421 --> 00:29:30,670
They get this android
phone for not much money,

473
00:29:30,671 --> 00:29:34,960
they can access anything in the world. Um,
and they wouldn't have that without us.

474
00:29:34,961 --> 00:29:39,010
So why are they complaining and, okay.
Like there is some truth to that.

475
00:29:39,011 --> 00:29:44,011
I do think it is cool about the world
that like the richest person and someone

476
00:29:44,441 --> 00:29:47,380
living in absolute poverty, um,
Carrie, you're on the same phone.

477
00:29:48,130 --> 00:29:49,390
That that is something that would not,

478
00:29:49,570 --> 00:29:54,220
there was no analogy to that other
than maybe like if you got a really bad

479
00:29:54,221 --> 00:29:58,900
disease, there was no like a quality
like that, you know, 400, 500 years ago.

480
00:29:59,130 --> 00:30:03,610
Um, however that point, which is always
what silicon valley falls back to,

481
00:30:03,790 --> 00:30:08,740
I think it's like maybe
not the most possible, most
tone deaf possible response.

482
00:30:09,190 --> 00:30:13,030
Um, but up there like people want
to feel like they have agency,

483
00:30:13,150 --> 00:30:14,680
they want to feel like they
have a voice in the future.

484
00:30:14,681 --> 00:30:16,660
They want to feel like
they can participate. Um,

485
00:30:16,960 --> 00:30:20,620
and they want to feel like they're not
just kind of like given this baseline

486
00:30:20,770 --> 00:30:22,750
while,
you know,

487
00:30:22,751 --> 00:30:27,360
like they toil in the
provinces and silicon valley
just gets all the money. Um,

488
00:30:28,330 --> 00:30:32,050
and I think people who don't see that
are just not thinking clearly. Um,

489
00:30:32,260 --> 00:30:34,690
and so I think one of
the greatest threats,

490
00:30:34,720 --> 00:30:38,980
something else I don't think technology
can fix on its own is how we get to a

491
00:30:38,981 --> 00:30:42,730
more just world. I really, really
fundamentally believe that, um,

492
00:30:43,090 --> 00:30:46,750
economic justice is the most important
thing you can do for social justice.

493
00:30:47,050 --> 00:30:50,740
And if you have all of the resources
going to a small set of people, um,

494
00:30:51,130 --> 00:30:54,910
even if everybody else has having their
absolute quality of life raise very

495
00:30:54,911 --> 00:30:55,870
quickly,
that's not enough.

496
00:30:56,790 --> 00:30:58,800
You recently announced a United slate,

497
00:30:58,830 --> 00:31:03,300
which puts forth some policy proposals
as well as an invitation for candidates.

498
00:31:03,570 --> 00:31:06,150
What would you say success
looks at like for that?

499
00:31:06,910 --> 00:31:11,290
You know, if we can run five,
six candidates in the 2018 cycle,

500
00:31:11,470 --> 00:31:13,090
um,
on that election,

501
00:31:13,360 --> 00:31:17,830
on that platform and have two
of them when their elections,

502
00:31:18,250 --> 00:31:21,850
um, I think that'd be an
incredible start. And you know,

503
00:31:21,851 --> 00:31:26,740
maybe it doesn't work at all because
maybe the kind of maybe those issues,

504
00:31:26,741 --> 00:31:30,340
although I believe they're important,
are not yet ready for,

505
00:31:30,880 --> 00:31:33,940
to convince the public at large. Um,
but I think it will be a good start.

506
00:31:34,240 --> 00:31:38,650
And I think over time it would be really
good if people on the progressive side

507
00:31:39,490 --> 00:31:44,490
built up a kind of longterm organization
focusing on winning elections and

508
00:31:45,311 --> 00:31:47,590
shifting public perception at all levels.
Um,

509
00:31:48,010 --> 00:31:49,840
the right has done a fabulous job at this.

510
00:31:49,841 --> 00:31:54,450
Congratulations to any of you who are
on that side, but I would like, uh,

511
00:31:55,050 --> 00:31:57,970
the, the left to do a
better job at that. I don't,

512
00:31:58,600 --> 00:32:00,850
I don't think we're putting
forward our best game there.

513
00:32:01,880 --> 00:32:02,930
In your invitation,

514
00:32:02,931 --> 00:32:05,990
you also mentioned that you'd be willing
to work with Democrats, independents,

515
00:32:05,991 --> 00:32:07,040
and Republicans.

516
00:32:07,340 --> 00:32:12,230
Is there any issue in particular that
you see anyone regardless of political

517
00:32:12,231 --> 00:32:15,980
ideology coming together and agreeing
on more so than other issues?

518
00:32:16,240 --> 00:32:20,110
Um, I think there are, there
are a lot of issues. Again,

519
00:32:20,320 --> 00:32:24,490
I think people agree on way more than they
disagree on. So I think, uh, you know,

520
00:32:24,491 --> 00:32:29,170
it's, we want around the state and talk to
Californians. Um, Republicans, Democrats,

521
00:32:29,200 --> 00:32:34,090
San Francisco, La, Fresno, Shasta
County, wherever you go. Um,

522
00:32:34,750 --> 00:32:38,920
the price of housing is like the biggest
issue for most sort of regular people

523
00:32:38,921 --> 00:32:43,820
that are not Google employees. Um,
maybe even for Google employees. Um,

524
00:32:44,300 --> 00:32:48,620
it's really, really bad. I mean this
is like, this is, if you could fix,

525
00:32:48,621 --> 00:32:53,150
I think one thing that would have
hugely positive secondary effects.

526
00:32:53,570 --> 00:32:57,650
Um, it's like bring the cost of
housing down by a factor of 10 would be

527
00:32:57,651 --> 00:32:59,540
transformative for society.
It's

528
00:33:01,280 --> 00:33:06,260
what eliminate government, right?
Eliminate government regulations. Yeah.

529
00:33:06,290 --> 00:33:11,060
Like I think many regulations
have good elements. I'm not,

530
00:33:11,330 --> 00:33:14,750
I'm not kind of the libertarian and our
kissed all government regulations are

531
00:33:14,751 --> 00:33:18,110
bad, but I do think the, the
sort of no more building,

532
00:33:18,111 --> 00:33:21,470
no building taller has been
disastrous for the state. Uh,

533
00:33:21,560 --> 00:33:26,560
and I think like it is the worst possible
allocation of resources to have people

534
00:33:27,351 --> 00:33:32,200
tie up every free penny they can find
into the place they live. I'm like,

535
00:33:32,210 --> 00:33:33,800
think about anything else
we could spend that on.

536
00:33:33,970 --> 00:33:36,410
I think about what it would be like if
people could like live close to where

537
00:33:36,411 --> 00:33:39,920
they work rather than commuting an hour
and a half to get here each day. Um,

538
00:33:41,270 --> 00:33:45,050
and that is something that
Democrats, Republicans, independents,

539
00:33:45,470 --> 00:33:47,370
almost everybody agrees on a,

540
00:33:47,390 --> 00:33:51,140
or at least there are people in all
of those camps that that do. Um,

541
00:33:53,270 --> 00:33:57,320
so I think like, again, easy to talk
about the divisions, it's really good.

542
00:33:57,500 --> 00:33:59,830
Then when you go talk to regular people,
um,

543
00:34:01,250 --> 00:34:03,800
they all agree on what the most important
issues and they don't want to fix it

544
00:34:04,190 --> 00:34:05,023
in the state.

545
00:34:05,120 --> 00:34:08,870
What do you think the tech industry's
role is in government and politics?

546
00:34:10,670 --> 00:34:11,930
Um,
look,

547
00:34:11,931 --> 00:34:16,160
I think we are all citizens of
this country and we all have like,

548
00:34:17,020 --> 00:34:21,080
you know, there is a, there is
a right to participate in the

549
00:34:22,610 --> 00:34:27,170
electoral process. And I
think there's also not, is a
duty to do that. And I think,

550
00:34:27,580 --> 00:34:30,260
uh, when I think,

551
00:34:30,410 --> 00:34:34,260
I think tech is going through such a
boom right now, it is easy to say, Hey,

552
00:34:34,261 --> 00:34:38,870
I'm just going to focus on like doing my
thing. Um, you know, building products,

553
00:34:38,871 --> 00:34:41,510
making money, whatever, and I'm
going to figure out someone else's,

554
00:34:41,570 --> 00:34:43,730
let someone else take care of the rest.
Um,

555
00:34:44,690 --> 00:34:48,350
one of the kind of like disappointing
things that I have learned as I've gotten

556
00:34:48,351 --> 00:34:52,970
to spend more and more time
with increasingly influential
people and kind of how

557
00:34:52,971 --> 00:34:57,380
the world works is that there is no
plan and there is no group figuring out

558
00:34:57,381 --> 00:34:59,480
everything and it's kind
of like up to all of us.

559
00:34:59,690 --> 00:35:04,340
Everyone hopes like the reason conspiracy
theories are so appealing is that like

560
00:35:04,670 --> 00:35:06,350
everybody wants there to be a conspiracy.

561
00:35:06,650 --> 00:35:11,150
Like you want to think that like
someone has a plan, um, that, you know,

562
00:35:11,151 --> 00:35:11,840
there's like,

563
00:35:11,840 --> 00:35:14,840
there's all this stuff happening but
someone's got a centralized plant and it's

564
00:35:14,841 --> 00:35:17,330
all gonna work out.
And like,

565
00:35:17,360 --> 00:35:21,640
I think one of the sad realizations of
growing up is that there are no grownups,

566
00:35:22,020 --> 00:35:24,800
um, no one has this master plan. And if,

567
00:35:25,730 --> 00:35:28,460
if we don't participate,
the thing can just go off the rails.

568
00:35:29,390 --> 00:35:33,230
What do you think are some of the most
effective ways for employees at a large

569
00:35:33,231 --> 00:35:37,650
tech company like Google to get
involved in government and politics? Um,

570
00:35:38,310 --> 00:35:42,300
everyone wants an answer other than this
one because it would be more convenient.

571
00:35:42,690 --> 00:35:46,260
But I think one of the answers
is to just run for office.

572
00:35:47,880 --> 00:35:48,930
We have this process,

573
00:35:49,320 --> 00:35:53,520
we have this like system instead of
rules that we've all agreed to. Um,

574
00:35:53,850 --> 00:35:57,600
and everyone wants some way to hack
around the edges of that rather than just

575
00:35:57,601 --> 00:35:58,500
participate directly.

576
00:35:59,070 --> 00:35:59,903
MMM.

577
00:36:00,950 --> 00:36:02,890
That's okay.
There are good things to do.

578
00:36:04,930 --> 00:36:08,320
But I think just taking the problem head
on is, uh, not enough people try that.

579
00:36:10,750 --> 00:36:11,560
And

580
00:36:11,560 --> 00:36:14,770
there's rumors going around that Mark
Zuckerberg is gonna run for office like

581
00:36:15,100 --> 00:36:15,970
every other week.

582
00:36:16,150 --> 00:36:20,470
And then there's also been rumors about
you running for governor and we've seen

583
00:36:20,850 --> 00:36:23,200
a tech people run for office in the past.

584
00:36:23,201 --> 00:36:28,201
Meg Whitman ran for governor of California
in 2010 but she lost by a significant

585
00:36:28,541 --> 00:36:30,970
margin to Jerry Brown.
So my question is,

586
00:36:31,180 --> 00:36:35,020
do you think that it's actually feasible
for someone in tech to run for office

587
00:36:35,021 --> 00:36:39,700
and win or is there too much of this
perception of the tech industry as being

588
00:36:39,730 --> 00:36:42,460
elitist such that people
will connect with voters?

589
00:36:43,520 --> 00:36:45,640
I think there is,
I think at least people should try.

590
00:36:45,650 --> 00:36:48,410
I think the tech industry is
the most hated in the bay area.

591
00:36:48,530 --> 00:36:50,930
And if you go out throughout the
rest of the state or the country,

592
00:36:51,080 --> 00:36:53,170
there's a lot of people who think
it's really cool. They aspire to it,

593
00:36:53,171 --> 00:36:55,850
they want it, they want
to participate in it. Um,

594
00:36:56,510 --> 00:37:01,510
and I think it is easy to get too negative
a view of how technology people are

595
00:37:02,241 --> 00:37:06,080
perceived here. I Dunno if a tech
person can win the presidency.

596
00:37:06,280 --> 00:37:08,210
I have a feeling we're going to find out.
Um,

597
00:37:08,330 --> 00:37:10,610
but what I am confident
about is that tech,

598
00:37:10,670 --> 00:37:14,810
not people from the technology industry
could start winning local school board

599
00:37:14,811 --> 00:37:18,500
and city council and you know,
congressional seats.

600
00:37:18,530 --> 00:37:20,900
That'd be a great thing to start with.
Is there any

601
00:37:21,020 --> 00:37:23,930
person currently living that you
would want to run for president?

602
00:37:23,931 --> 00:37:25,100
Who's your ideal candidate?

603
00:37:30,020 --> 00:37:30,853
Okay.

604
00:37:32,060 --> 00:37:35,090
I do. I can't answer that on the
fly. That's, I, I would like,

605
00:37:35,180 --> 00:37:37,940
it's a good question to think about
when you said that. I was like, oh,

606
00:37:37,941 --> 00:37:41,180
I should figure out who that person
is. Go try to convince them to run. Um,

607
00:37:42,110 --> 00:37:44,300
but I don't have an answer like
ready to go all in my head. Okay.

608
00:37:45,380 --> 00:37:49,400
Well with that, I'm going to open
up the floor for audience questions.

609
00:37:49,640 --> 00:37:52,970
So if you have a question, feel free to
go to the mic in the back of the room.

610
00:37:53,120 --> 00:37:55,430
We're also submit your
question to the Dory.

611
00:37:57,520 --> 00:37:58,353
Go for it.

612
00:37:59,140 --> 00:38:03,940
Hi. Um, you mentioned the anecdote
about your friend being unhappy and,

613
00:38:03,950 --> 00:38:07,060
and Kim Kardashians and it reminded
me of a quote and it's set up.

614
00:38:08,170 --> 00:38:08,550
Okay.

615
00:38:08,550 --> 00:38:11,940
People aren't unhappy because
they want to be happy.

616
00:38:12,360 --> 00:38:14,940
They're unhappy because they want
to be happier than other people.

617
00:38:15,720 --> 00:38:19,790
And you mentioned that technology
may or may not be able to solve this.

618
00:38:19,791 --> 00:38:24,791
So I like to hear some of your ideas or
success strategies on how to solve this

619
00:38:25,860 --> 00:38:30,660
on a micro and macro level,
whether it be delete Twitter too.

620
00:38:30,950 --> 00:38:35,800
That's a little thing to do. Um,
that's a good thing to do. I, uh,

621
00:38:35,980 --> 00:38:39,970
yeah, I deleted all the social apps
from my phone. Um, and I, you know,

622
00:38:39,971 --> 00:38:42,520
I can still like check them on
my computer when I want, but, uh,

623
00:38:42,880 --> 00:38:47,020
I don't have that like constant urge to
like push button for dopamine hit here.

624
00:38:47,440 --> 00:38:51,400
Um, and that's, that's been
really good. Um, you know,

625
00:38:52,030 --> 00:38:56,080
I think after talking to a bunch of
these people about the things that make

626
00:38:56,081 --> 00:38:57,670
people happy,
there's like the obvious stuff,

627
00:38:57,671 --> 00:39:01,090
which is like spend time with loved ones,
um,

628
00:39:02,020 --> 00:39:03,790
that everyone knows even if they don't do.

629
00:39:04,060 --> 00:39:09,060
But there's a bunch of things that are
like very not or less obvious to me that

630
00:39:10,001 --> 00:39:13,600
seemed to have huge measured effects.
Um,

631
00:39:13,780 --> 00:39:18,070
like taking time to think about the
things that went well as opposed to things

632
00:39:18,071 --> 00:39:22,600
you're upset about or like going for a
walk outside every day no matter what. Um,

633
00:39:24,160 --> 00:39:29,160
and I think as we get sort of to
abundance and unlimited resources,

634
00:39:30,491 --> 00:39:33,070
this is going to be a
more important topic. Um,

635
00:39:34,030 --> 00:39:36,390
what I would encourage you to do is to,
uh,

636
00:39:36,940 --> 00:39:40,510
just like start reading. There's a
lot of literature on the subject.

637
00:39:40,511 --> 00:39:42,550
No one seems to care about
enough to spend time,

638
00:39:42,880 --> 00:39:44,290
but I would say start reading it.
Um,

639
00:39:44,470 --> 00:39:46,360
and hopefully I'll finish
my blog post on it soon.

640
00:39:48,710 --> 00:39:49,543
Thank you.

641
00:39:51,380 --> 00:39:52,650
Hey Sam, thanks for coming. Uh,

642
00:39:52,670 --> 00:39:56,930
wanted to ask a question about the larger
landscape. As you see in general, uh,

643
00:39:57,140 --> 00:39:59,750
heard a rumor that YC is
looking into a growth fund.

644
00:39:59,790 --> 00:40:01,400
I think at Brooklyn Tech
crunch like a month ago,

645
00:40:01,401 --> 00:40:04,010
like potentially raising
a billion dollar fund. Uh,

646
00:40:04,020 --> 00:40:08,210
wanted to get your thoughts on growth
versus venture and how you see growth

647
00:40:08,211 --> 00:40:11,630
evolving. Uh, for the bay area in general,

648
00:40:11,730 --> 00:40:14,480
you see private companies staying
private longer and longer.

649
00:40:14,890 --> 00:40:19,570
So, um, we already have a growth
fund. It's called a YC continuity. Um,

650
00:40:19,580 --> 00:40:24,160
we raised the first one in 2015.
Um, it's about halfway invested.

651
00:40:24,480 --> 00:40:27,740
Uh, and it, it basically
follows on in capitol,

652
00:40:28,000 --> 00:40:29,710
in YC companies.

653
00:40:29,950 --> 00:40:34,950
One of the things that I wanted to do
after I joined YC was start to fund hard

654
00:40:35,201 --> 00:40:39,550
tech companies. Um, nuclear fusion,
synthetic biology, quantum computing,

655
00:40:40,540 --> 00:40:44,910
self driving cars, long
list. Um, and one of the,

656
00:40:45,580 --> 00:40:49,210
the, one of the good things
that I realized is that
nobody else was funding those

657
00:40:49,211 --> 00:40:51,740
companies. And so we
could have our pick. Um,

658
00:40:52,150 --> 00:40:54,910
one of the bad things that I realized is
nobody else was funding those companies.

659
00:40:55,330 --> 00:40:58,720
Um, and so we could fund them all we
wanted and there was no follow on capital.

660
00:40:59,200 --> 00:41:02,410
And I think,
so that was the action.

661
00:41:02,411 --> 00:41:05,500
The genesis for our growth fund is that
there is this class of companies that I

662
00:41:05,501 --> 00:41:08,860
think are really important and really
valuable and they weren't getting funded

663
00:41:09,550 --> 00:41:13,570
since then. We've expanded it and
we fund lots of other companies. Um,

664
00:41:13,990 --> 00:41:16,400
I will certainly say that,
um,

665
00:41:17,410 --> 00:41:21,400
there is no shortage of growth capital
for software companies in Silicon Valley.

666
00:41:22,000 --> 00:41:25,330
Like, you know, it may have gotten
harder to raise a seed or an a round.

667
00:41:25,331 --> 00:41:29,370
It probably has somewhat, but
if you have things working, uh,

668
00:41:29,860 --> 00:41:31,240
like when you want to go raise a,

669
00:41:31,241 --> 00:41:33,680
B or c round and you have like
this beautiful exponential growth,

670
00:41:34,010 --> 00:41:36,710
like you will not be able to keep up
with the number of terms sheets you're

671
00:41:36,711 --> 00:41:37,370
getting.

672
00:41:37,370 --> 00:41:42,320
That is the stage that I think just has
a huge amount of capital allocated to it.

673
00:41:42,560 --> 00:41:43,160
As,

674
00:41:43,160 --> 00:41:46,700
as companies are staying private longer
as public market investors have a harder

675
00:41:46,701 --> 00:41:50,390
and harder time finding alpha.
Um,

676
00:41:50,540 --> 00:41:54,980
they are more than willing
to do stuff like this. So,
uh, that part of the market,

677
00:41:55,260 --> 00:41:59,330
um, is not suffering, at
least not yet. Thanks. Sure.

678
00:41:59,860 --> 00:42:02,530
Let me do a question from the dory.
Uh,

679
00:42:03,100 --> 00:42:08,100
we have a question about universal basic
income and how we've Google has been

680
00:42:10,510 --> 00:42:15,510
supporting give directly to research
on UBI in Africa and you're a proponent

681
00:42:15,760 --> 00:42:18,640
here in the u s so what
are your thoughts on how,

682
00:42:18,641 --> 00:42:22,870
when and where the universal basic
income approach might be most effective?

683
00:42:23,060 --> 00:42:27,950
Yeah. Um, I don't think universal basic
income is a solution to all problems.

684
00:42:28,020 --> 00:42:28,280
Um,

685
00:42:28,280 --> 00:42:31,490
I think in fact of the bigger problem
that we were talking about earlier of what

686
00:42:31,491 --> 00:42:35,570
makes people happy and fulfilled and feel
needed and valued and have meaning in

687
00:42:35,571 --> 00:42:38,840
their lives. Um, it is not
sufficient to solve that problem.

688
00:42:39,470 --> 00:42:42,470
And I don't think it will replace like
the entire other social safety net.

689
00:42:42,471 --> 00:42:45,780
Like the people who are like eliminate
everything else. You know, no healthcare,

690
00:42:45,860 --> 00:42:49,220
no schools, no minimum wage. I'm put basic
income. I don't believe in that either.

691
00:42:50,090 --> 00:42:54,170
But I do think that if
we do all of our jobs,

692
00:42:54,500 --> 00:42:56,600
if we silicon valley do all of our jobs,

693
00:42:56,960 --> 00:43:01,730
we will create more wealth than the
world has ever seen before and less jobs.

694
00:43:02,270 --> 00:43:05,050
And in that world,
um,

695
00:43:05,360 --> 00:43:08,000
I think there is a moral
obligation to eliminate poverty.

696
00:43:08,570 --> 00:43:11,240
I think poverty is a
obviously very bad thing,

697
00:43:11,241 --> 00:43:14,750
but it is worse even than
people realize. Um, if you,

698
00:43:14,751 --> 00:43:19,751
if you look at the studies on the longterm
psychological damage that living in

699
00:43:20,840 --> 00:43:25,340
Cotsen poverty does to people and how
it means that you never got a chance to

700
00:43:25,341 --> 00:43:28,460
really invest in your own future because
you're always just trying to survive,

701
00:43:28,700 --> 00:43:32,360
I think there's just a huge amount of
wasted potential. And so, you know,

702
00:43:32,361 --> 00:43:35,810
there are a lot of arguments about ubi.
This was another thing,

703
00:43:35,811 --> 00:43:38,000
like when we started this
a couple of years ago,

704
00:43:38,001 --> 00:43:43,001
I did not predict at all that this was
going to ignite into this big national

705
00:43:43,011 --> 00:43:46,820
debate. It was like, we're just
trying to hire a researcher, you know,

706
00:43:46,880 --> 00:43:51,380
like maybe this is a good
idea, can we just do a project?
And it's just been like,

707
00:43:52,760 --> 00:43:53,593
um,

708
00:43:55,280 --> 00:43:59,630
so I don't think I quite understand why
it has become such a national issue so

709
00:43:59,631 --> 00:44:04,240
quickly. But I do think
that a longer term, um,

710
00:44:04,880 --> 00:44:08,300
if we have the ability
to eliminate poverty,

711
00:44:08,540 --> 00:44:10,580
we will get a lot more out
of the world as a whole.

712
00:44:11,230 --> 00:44:14,230
As he started the research
experiment in East Bay.

713
00:44:14,680 --> 00:44:18,490
Have you noticed anything that maybe
you didn't expect or has it changed your

714
00:44:18,491 --> 00:44:19,780
attitude towards it in any way?

715
00:44:20,190 --> 00:44:24,270
You know, we're like seven or eight
months into a five year study, so not yet.

716
00:44:24,470 --> 00:44:25,303
Okay.

717
00:44:26,340 --> 00:44:27,050
Okay.

718
00:44:27,050 --> 00:44:30,890
Hi Sam. I'm thinking more in general a
founders, but this could be general too,

719
00:44:31,200 --> 00:44:32,010
but what

720
00:44:32,010 --> 00:44:34,080
advice do you frequently give,

721
00:44:34,081 --> 00:44:37,530
but you find most people never
actually follow through on?

722
00:44:38,700 --> 00:44:42,200
Uh, I mean, no one ever listens
to any advice, any sort. Um,

723
00:44:43,020 --> 00:44:46,200
so all of it, uh, I think,

724
00:44:48,060 --> 00:44:48,893
okay.

725
00:44:49,110 --> 00:44:51,690
I think the thing that people,
well,

726
00:44:51,960 --> 00:44:56,890
the piece of advice that people later
say, they wish they had listened to. Um,

727
00:44:57,390 --> 00:45:01,560
is that it is very easy to get sucked
into a path in life and you know,

728
00:45:01,561 --> 00:45:05,190
you can do things like say, oh, I want
to start a startup someday, or oh,

729
00:45:05,191 --> 00:45:07,860
I want to be are, you
know, AI researcher Sunday,

730
00:45:08,070 --> 00:45:11,090
but first I'm going to go do
this other job to like, you know,

731
00:45:11,100 --> 00:45:12,900
make money and gain
experience and whatever.

732
00:45:13,650 --> 00:45:17,820
And it's so easy to like get sucked into
a path where you spend your entire life

733
00:45:17,821 --> 00:45:19,680
doing something that is not
really what you want to do.

734
00:45:20,550 --> 00:45:21,383
MMM.

735
00:45:23,450 --> 00:45:26,810
That's probably the piece of advice that
I have given people that they have most

736
00:45:26,811 --> 00:45:29,540
often come to me like
five years later and said,

737
00:45:29,541 --> 00:45:30,770
I really wish I had listened to that.

738
00:45:31,950 --> 00:45:32,783
Thank you.
Sure.

739
00:45:35,050 --> 00:45:38,920
I'm curious to hear your thoughts on
relative rates of progress in Ai between

740
00:45:38,921 --> 00:45:42,340
China and everywhere else.
So you mentioned nobody has a plan,

741
00:45:42,370 --> 00:45:46,660
but China has a plan for state
level centralized investment Nai,

742
00:45:46,720 --> 00:45:50,290
whereas Elon Musk is calling for
regulation that would slow down. Yeah,

743
00:45:50,291 --> 00:45:51,310
definitely everywhere else.

744
00:45:52,310 --> 00:45:57,000
Uh, China does plan. That's
true. Um, they don't,

745
00:45:57,760 --> 00:46:01,480
it doesn't always work. They don't always
have global coordination. Um, you know,

746
00:46:01,520 --> 00:46:04,910
like right now we're in this kind of world
where it seems like you have like the

747
00:46:04,911 --> 00:46:07,540
Internet in China, on the Internet
and the rest of the world. Um,

748
00:46:07,790 --> 00:46:10,810
maybe they have a plan
for their piece of it. Um,

749
00:46:14,790 --> 00:46:19,790
I don't honestly know how far along China
is with AI and I don't think anybody

750
00:46:20,251 --> 00:46:22,020
else honestly really does either.

751
00:46:22,300 --> 00:46:23,133
MMM.

752
00:46:26,380 --> 00:46:29,590
I think it would be bad to get into
an arms race. We'll try and over Ai.

753
00:46:30,340 --> 00:46:33,010
I ain't got something we
should try to avoid. Um,

754
00:46:34,090 --> 00:46:38,290
and I think there is a real opportunity,
although

755
00:46:40,760 --> 00:46:41,450
it might,

756
00:46:41,450 --> 00:46:46,010
we may not have the leaders in place
to do it right now to make this a joint

757
00:46:46,040 --> 00:46:48,680
kind of like worldwide project.
Um,

758
00:46:48,770 --> 00:46:51,410
rather than another space
race or nuclear arms race.

759
00:46:52,560 --> 00:46:53,393
Thanks.

760
00:46:55,260 --> 00:46:58,770
Hi Sam. I was wondering if you could
expand a little bit on funding.

761
00:46:58,771 --> 00:47:00,080
Hard tech cause they ain't,

762
00:47:00,081 --> 00:47:04,680
is was a big problem with a
private capital allocation
and like $70 billion was

763
00:47:04,681 --> 00:47:06,030
put into VC last year,

764
00:47:06,031 --> 00:47:09,090
which is a drop in the bucket in
terms of worldwide investment.

765
00:47:09,360 --> 00:47:14,360
So do you see like YC maybe raising in
$100 billion fund in the future? Namely,

766
00:47:15,260 --> 00:47:16,140
how can we solve this?

767
00:47:16,820 --> 00:47:20,630
I think capital is allocated really badly.
Um,

768
00:47:21,050 --> 00:47:24,350
and unfortunately there are huge efforts.

769
00:47:24,351 --> 00:47:27,230
Like if you have this
really valuable thing,

770
00:47:27,231 --> 00:47:30,940
which is you get access to invest in
great startups and most of the world

771
00:47:30,941 --> 00:47:31,850
doesn't,
um,

772
00:47:31,990 --> 00:47:35,440
then there will obviously be a super
return there and you're going to work

773
00:47:35,441 --> 00:47:38,230
really hard to protect
that. Um, and so I think,

774
00:47:40,150 --> 00:47:44,530
um, I think expanding access
to invest in startups,

775
00:47:44,531 --> 00:47:48,490
which is happening but slowly it's a
really good thing to do. And I think, um,

776
00:47:49,150 --> 00:47:53,740
you know, like crowdfunding
equity crowdfunding, we're
still in the early days of,

777
00:47:53,741 --> 00:47:55,790
but he's an important trend.
Uh,

778
00:47:55,960 --> 00:48:00,190
and I think that will change the capital
allocation. I think you're seeing,

779
00:48:00,490 --> 00:48:04,150
I think you're seeing startups in all
these different ways go to non traditional

780
00:48:04,151 --> 00:48:09,130
funders. Um, and that's been really good.
We first saw that with consumer hardware.

781
00:48:09,190 --> 00:48:12,970
We're now seeing that in a lot
of other places. Um, you know,

782
00:48:12,971 --> 00:48:15,880
I think the other thing that's
happening finally is there's just,

783
00:48:16,000 --> 00:48:17,500
and there's bad to this too,

784
00:48:17,501 --> 00:48:21,940
but there's just a huge amount of more
capital coming in to fund early stage

785
00:48:21,941 --> 00:48:24,130
private companies,
mid stage private companies.

786
00:48:24,580 --> 00:48:29,580
And I expect that in a world
where interest rates stay
even close to as low as

787
00:48:30,371 --> 00:48:33,130
they are. Um, we are in the very
early innings of that trend.

788
00:48:33,310 --> 00:48:35,830
So I think capital
we'll get, you know, my,

789
00:48:35,930 --> 00:48:39,280
my general model is that the world
is very complex financial system,

790
00:48:39,460 --> 00:48:42,860
capital sloshes around looking
for the best return. Um,

791
00:48:43,030 --> 00:48:46,510
there are periods where there's
a really outperforming return,

792
00:48:46,511 --> 00:48:49,420
but then more capital gets allocated
there. And I think we're seeing that now.

793
00:48:49,770 --> 00:48:53,910
But did you think that a nuclear fusion
company could raise $100 million in an

794
00:48:53,911 --> 00:48:55,060
ICO or something like that?

795
00:48:55,540 --> 00:48:59,560
Or maybe I wouldn't suggest
that they do an ICO. Um,

796
00:48:59,590 --> 00:49:02,020
but I do think they can
raise $100 million, uh,

797
00:49:02,110 --> 00:49:04,150
and I don't think they
could have three years ago.

798
00:49:04,540 --> 00:49:08,200
And I think that's a really positive trend
for the world, that this stuff is now,

799
00:49:08,650 --> 00:49:11,110
um,
at least somewhat possible.

800
00:49:11,430 --> 00:49:13,860
Awesome. Thanks. Sure.
Another dory question,

801
00:49:14,100 --> 00:49:17,970
what are the most common reasons that
qualified candidates get rejected from YC?

802
00:49:19,540 --> 00:49:20,373
MMM,

803
00:49:23,090 --> 00:49:23,923
yeah.

804
00:49:24,480 --> 00:49:28,920
Well there is one particular
thing that is, uh,

805
00:49:29,430 --> 00:49:33,120
that happens a lot with super talented
people from large tech companies.

806
00:49:33,121 --> 00:49:35,110
So since I'm here I'll mention that one.
Um,

807
00:49:35,670 --> 00:49:39,720
which is you are really great person,
really talented, really smart.

808
00:49:40,010 --> 00:49:43,680
I'm really driven and you don't have an
a good idea or you don't have any idea

809
00:49:43,681 --> 00:49:46,920
and so you make up a bad one. Um, and

810
00:49:50,030 --> 00:49:53,990
there's this like myth about startups,
the idea doesn't matter at all. Um,

811
00:49:54,200 --> 00:49:58,220
and I think that's just,
there's existential proof
that that's not true. Um,

812
00:49:58,580 --> 00:50:01,580
and I think
there is,

813
00:50:01,610 --> 00:50:04,400
there is this particular failure mode
of people from the big tech companies,

814
00:50:04,401 --> 00:50:07,160
which is like, hey, I'm really
talented and well, I guess you are. Um,

815
00:50:07,190 --> 00:50:09,650
so I'm going to like start a company
and I'll figure out what to do later.

816
00:50:10,490 --> 00:50:13,160
And I think that isn't usually
what produces the great companies.

817
00:50:13,490 --> 00:50:16,280
And so we have learned, um, that

818
00:50:18,670 --> 00:50:23,290
it's much, much better if you're
otherwise a qualified candidate. Um,

819
00:50:24,280 --> 00:50:26,770
to have a, have a good idea.
And I think that's the,

820
00:50:26,830 --> 00:50:29,150
that would be the common failure
case for people coming out of Google.

821
00:50:29,330 --> 00:50:30,163
Hmm.

822
00:50:31,820 --> 00:50:35,680
Hey Sam. Hey. So, um, what are
your thoughts about, you know,

823
00:50:35,681 --> 00:50:38,950
where you're going to go or what you're
going to do with your life after YC or

824
00:50:39,100 --> 00:50:41,680
does that come across? Are you going
to take a political office series?

825
00:50:41,820 --> 00:50:45,900
You know, I don't even like know what
I'm going to do in three weeks. Um,

826
00:50:46,800 --> 00:50:51,300
I uh,
I plan to run YC for a really long time.

827
00:50:51,930 --> 00:50:56,220
Um, I, I do plan to get continually
more involved with politics,

828
00:50:56,221 --> 00:51:00,030
but I'd like to support others.
Um, not run for office myself,

829
00:51:00,060 --> 00:51:04,500
at least not anytime soon. Um, you know,

830
00:51:04,501 --> 00:51:08,130
like I am a big believer in
do the things that you like,

831
00:51:08,131 --> 00:51:10,500
think about in the shower in the morning
when you can think about anything you

832
00:51:10,501 --> 00:51:12,090
want and you're kind of mind is just off.

833
00:51:12,360 --> 00:51:13,770
And the things that
keep coming back to her,

834
00:51:13,771 --> 00:51:16,380
like how can I make YC like
a hundred times bigger?

835
00:51:16,740 --> 00:51:19,980
How can I make sure that their
rival of the AI goes well? Um,

836
00:51:20,010 --> 00:51:21,090
and in the short term,

837
00:51:21,091 --> 00:51:24,360
how can I help our political
system from going off the rails so,

838
00:51:25,190 --> 00:51:29,010
or any more off the rails not
already has. Um, so, you know,

839
00:51:29,011 --> 00:51:32,910
I kind of plan to keep working on
those things for hopefully, you know,

840
00:51:33,680 --> 00:51:34,513
a really long time.

841
00:51:36,550 --> 00:51:37,383
Thanks.

842
00:51:37,600 --> 00:51:42,130
Hi. Thank you for being here. I
have a question about the general,

843
00:51:42,131 --> 00:51:42,964
um,

844
00:51:43,430 --> 00:51:47,950
eh way silicon valley works because it
seems like it's a system which in which

845
00:51:47,951 --> 00:51:49,930
you have like the most talented mines.

846
00:51:50,230 --> 00:51:52,840
Dylan would maybe not the
most important teachers.

847
00:51:52,930 --> 00:51:56,290
Like you have the sharpest mind
walking on social apps or mobile apps,

848
00:51:56,560 --> 00:51:59,620
but we still don't have a cure for
cancer or we still don't have a,

849
00:52:00,100 --> 00:52:05,100
another way of like having energy or like
really important stuff that the world

850
00:52:05,141 --> 00:52:09,060
should handle. Em. So my question
is, do you agree with that?

851
00:52:09,061 --> 00:52:11,620
What is your perspective on that and
how do you think maybe this can be

852
00:52:11,621 --> 00:52:12,454
addressed?

853
00:52:13,060 --> 00:52:13,850
Okay.

854
00:52:13,850 --> 00:52:15,650
I don't agree with that. I think, um,

855
00:52:16,460 --> 00:52:20,630
I used to be my job as a capital allocator
and now I view my job as a cowlick

856
00:52:20,631 --> 00:52:21,410
allocator.

857
00:52:21,410 --> 00:52:26,410
So most of my day is like spent meeting
with really smart people and trying to

858
00:52:26,511 --> 00:52:31,070
convince them to work on
important problems. And you know,

859
00:52:31,071 --> 00:52:31,904
I think,

860
00:52:33,270 --> 00:52:33,630
okay,

861
00:52:33,630 --> 00:52:38,430
in 2007 what everyone said is the best
minds of our generation used to build

862
00:52:38,431 --> 00:52:41,850
spaceships and now they're like
moving numbers around a Wall Street.

863
00:52:41,851 --> 00:52:45,870
And in 2017 what people say is the best
minds of our generation and used to

864
00:52:46,080 --> 00:52:50,790
build space ships and now they get
me to click on ads and you know,

865
00:52:50,791 --> 00:52:54,150
there's always some truth then there's
always someone to pick on there. Um,

866
00:52:54,270 --> 00:52:58,590
but like I think open AI has some
of the smartest minds in our world.

867
00:52:58,650 --> 00:53:02,250
If our generation working on having
an AI go, well, I think helium,

868
00:53:02,251 --> 00:53:03,150
which is that fusion company,

869
00:53:03,151 --> 00:53:06,300
I mentioned some of the smartest minds
of our generation working on nuclear

870
00:53:06,301 --> 00:53:08,790
fusion. Um, why? See, I
think last time I counted,

871
00:53:08,791 --> 00:53:10,950
it's funded eight companies
working on a cure for cancer.

872
00:53:11,100 --> 00:53:15,800
Incredibly talented people are
cures for cancer cancers. Um,

873
00:53:16,980 --> 00:53:20,700
I think you can always say there's all
these people like working on bad stuff or

874
00:53:20,701 --> 00:53:22,830
stuff. It doesn't matter. But, you know,

875
00:53:22,831 --> 00:53:25,680
I think Google is still like a
really great thing for the world. Uh,

876
00:53:25,770 --> 00:53:28,430
this is why we're here. Yeah.
And I think it does matter.

877
00:53:28,460 --> 00:53:30,630
I think it's always easy
to pick on people and say,

878
00:53:30,840 --> 00:53:34,290
you're not working on a cure for cancer.
You know, you're wasting your time,

879
00:53:34,620 --> 00:53:39,540
but you're making this thing better that
people use every day and their lives

880
00:53:39,541 --> 00:53:43,290
would be a lot worse if it went away
and you're making it better every day.

881
00:53:43,320 --> 00:53:48,090
And I think it's, it's, it's really
easy to pick on people, um, and say,

882
00:53:48,150 --> 00:53:50,370
you know,
that person's not spending her time right.

883
00:53:51,890 --> 00:53:54,960
And I think it always says more about
the person that says that then the person

884
00:53:54,961 --> 00:53:57,930
they're pointing to. And if you're
doing something useful for the world,

885
00:53:58,470 --> 00:54:00,570
if you're doing something you enjoy,
if you're sort of like,

886
00:54:01,020 --> 00:54:02,340
even if you're making a small impact,

887
00:54:02,341 --> 00:54:05,150
but on a product that a lot of
people really use and love, um,

888
00:54:05,250 --> 00:54:06,360
I think that's really valuable.

889
00:54:06,390 --> 00:54:09,630
And I think the people who
say this generally have a
lot of insecurity about how

890
00:54:09,631 --> 00:54:10,464
they're spending their time.

891
00:54:11,070 --> 00:54:15,720
So you don't think like silicon valley
is like avoiding the tough problems.

892
00:54:17,250 --> 00:54:18,180
I invite you to like,

893
00:54:18,181 --> 00:54:21,690
come sit in my office for a day and like
listen to the people coming through and

894
00:54:21,691 --> 00:54:25,170
what they're working on. A, I really don't
think that, no. Okay, cool. Thank you.

895
00:54:27,490 --> 00:54:32,110
Hey, Sam. Uh, seeing that you're
largely influential public figure, um,

896
00:54:32,180 --> 00:54:35,160
could you speak to the privileges
that might've led to that, um,

897
00:54:35,170 --> 00:54:38,330
and helped you with your success and,
um,

898
00:54:38,540 --> 00:54:41,540
how you may be a good or bad ally.

899
00:54:41,930 --> 00:54:46,730
And then also seeing that a lot of our
consumer products are largely bias,

900
00:54:47,130 --> 00:54:50,150
um, specifically to the
people who create them,

901
00:54:50,151 --> 00:54:54,680
which was largely silicone
valley engineers like myself
and others in this room.

902
00:54:55,270 --> 00:54:59,600
Uh, how do you keep your ideas
and thoughts diverse and, um,

903
00:54:59,601 --> 00:55:01,190
addressing intersectional needs?

904
00:55:02,380 --> 00:55:03,660
Yeah, great question. Um,

905
00:55:03,880 --> 00:55:07,930
I basically had like nearly
all of the possible privileges.

906
00:55:07,931 --> 00:55:12,820
I had wonderful loving parents. Um, I
grew up in a safe house. I'm a white guy.

907
00:55:13,270 --> 00:55:14,103
Um,

908
00:55:14,290 --> 00:55:17,710
we had enough money that I was able to
pursue the things that I was interested

909
00:55:17,711 --> 00:55:22,390
in and go to a great college.
Um, and it's never lost on me.

910
00:55:22,391 --> 00:55:23,920
How if I had been born

911
00:55:25,420 --> 00:55:30,010
a mile in a different place to a different
family, um, different skin color,

912
00:55:30,011 --> 00:55:31,720
different gender,
I wouldn't be where I am now.

913
00:55:32,080 --> 00:55:37,080
I view that as an obligation to try to
make the world more just going forward.

914
00:55:37,570 --> 00:55:42,370
Um, I think anyone who is really
successful and doesn't like,

915
00:55:43,600 --> 00:55:46,210
I think anyone should try to do the
best they can with whatever hand they're

916
00:55:46,211 --> 00:55:50,110
dealt. But if you're dealt, you know,
like four aces then and you win,

917
00:55:50,500 --> 00:55:54,370
then I think you have an extra obligation
to try to sort of make the world a

918
00:55:54,371 --> 00:55:55,150
little better.

919
00:55:55,150 --> 00:56:00,150
So I try to be really thankful of what
everyone's done that has allowed me to do

920
00:56:00,401 --> 00:56:01,630
this.
Um,

921
00:56:02,830 --> 00:56:06,310
I also try to figure out
how to pay that forward. Um,

922
00:56:07,000 --> 00:56:07,870
and I think,

923
00:56:09,380 --> 00:56:12,160
I think anyone who is really successful
or almost anyone who is really

924
00:56:12,161 --> 00:56:16,510
successful has, um, like
privilege luck's going hard work.

925
00:56:16,900 --> 00:56:20,860
And I think people who try to say it's
just one or just the other all tend to be

926
00:56:20,861 --> 00:56:21,970
wrong.
Um,

927
00:56:22,750 --> 00:56:27,130
so I'm thankful for that and
I try to pay that forward. Um,

928
00:56:27,280 --> 00:56:32,110
I, in terms of biases
in the product, I think,

929
00:56:32,580 --> 00:56:35,830
uh, this is one of the reasons that
diverse teams are most important.

930
00:56:36,100 --> 00:56:40,720
The moral question aside, uh,
that consumer products, the teams,

931
00:56:40,721 --> 00:56:45,721
the companies that I think have done the
best job addressing this head on have

932
00:56:45,771 --> 00:56:48,160
had a very diverse set of
voices around the table.

933
00:56:48,640 --> 00:56:52,120
And I think that is always the strategy
I recommend cause that's the only one

934
00:56:52,121 --> 00:56:56,290
I've seen consistently work. Cool.
Thank you. Sure. We're on time now,

935
00:56:56,291 --> 00:56:59,680
but if you can do these last
three questions quickly,
we can get through them.

936
00:56:59,830 --> 00:57:02,140
Sure. Thank you for coming. Sam. Uh,

937
00:57:02,290 --> 00:57:06,880
my question is on machine learning and
artificial intelligence companies. Uh,

938
00:57:06,881 --> 00:57:11,560
it seems to me that the scarce or valuable
thing is data in those areas and less

939
00:57:11,561 --> 00:57:13,540
so the algorithm because
it mostly open source.

940
00:57:13,630 --> 00:57:16,870
So I was wondering how you think about
that when getting pitches from company,

941
00:57:16,930 --> 00:57:20,950
a company. I used to believe that I now
believe that it's going to be compute,

942
00:57:20,951 --> 00:57:25,570
not data. I think data is important but
there will be a lot of it available.

943
00:57:25,870 --> 00:57:29,990
Uh, and just my own
experience with open AI, um,

944
00:57:30,820 --> 00:57:34,240
to really be at the forefront here you
just need massive amounts of compute.

945
00:57:34,840 --> 00:57:38,360
And so I used to ask companies like how
they're going to get a lot of data and I

946
00:57:38,370 --> 00:57:41,530
ask them how they're going to go out
of compute. It's okay. Thank you. Sure.

947
00:57:42,610 --> 00:57:45,190
Hey Sam, thanks for coming. Um, I know
you were talking about like, you know,

948
00:57:45,191 --> 00:57:47,740
people are already becoming
unhappier and the world food problem,

949
00:57:47,741 --> 00:57:50,260
all this kind of things that
are kind of like quantifiable,

950
00:57:50,261 --> 00:57:51,580
like physical needs of people.

951
00:57:51,910 --> 00:57:54,670
Have you ever had companies that come
to you kind of like try to solve the

952
00:57:54,671 --> 00:57:57,880
spiritual needs of people because
we can identify the physical needs,

953
00:57:58,090 --> 00:58:01,510
but what about like spiritual needs
or research or companies? We have had

954
00:58:01,560 --> 00:58:05,200
a few, um, none of them have really
worked yet. Uh, but you know,

955
00:58:05,201 --> 00:58:08,620
one that like stuck out of memory was
a company came to us and said like,

956
00:58:10,540 --> 00:58:13,280
churches, like, you know,

957
00:58:13,310 --> 00:58:18,310
organized religion had this really
important effect that was totally separate

958
00:58:18,311 --> 00:58:21,000
from the religion itself,
which was this,

959
00:58:21,330 --> 00:58:26,020
this tight knit community and how do you
build that in a world where most people

960
00:58:26,170 --> 00:58:29,560
are declining, number of people believe
in religion and go to church. Um,

961
00:58:29,561 --> 00:58:31,930
and so I think there are people
thinking about things like that,

962
00:58:31,931 --> 00:58:35,170
which are sort of these
non obvious attacks on the
problem that are interesting,

963
00:58:35,470 --> 00:58:38,930
but none that I could yet 0.2 is, here's
this thing that's really worked well.

964
00:58:41,910 --> 00:58:46,650
He say, I'm a, so you partnered
with the ACL Aclu earlier this year,

965
00:58:46,651 --> 00:58:48,330
which you got mixed reactions to you.

966
00:58:48,780 --> 00:58:52,520
I'm wondering what you've learned
from that whole experience, uh,

967
00:58:52,560 --> 00:58:55,830
what successes you've had
and what partnerships who
are looking forward to with

968
00:58:55,831 --> 00:58:57,060
other nonprofits in the future?

969
00:58:57,780 --> 00:59:02,680
I, I've been, I was really excited
about how that went. That was a,

970
00:59:02,810 --> 00:59:04,870
we had never done anything
like that before. Um,

971
00:59:04,930 --> 00:59:07,750
we often try new things usually
don't work. Sometimes they do.

972
00:59:07,990 --> 00:59:11,830
That's when we would do again. Um,
we would do something more like that.

973
00:59:11,831 --> 00:59:16,831
I think there are these really important
organizations in the world that can use

974
00:59:18,311 --> 00:59:20,530
our help to build better technology teams.
Um,

975
00:59:20,900 --> 00:59:24,230
and that was an experiment that went well
and, but we'd love to try it again. Um,

976
00:59:24,440 --> 00:59:27,440
one of our software engineers,
Cajun went there for,

977
00:59:28,400 --> 00:59:29,960
I think she went for
like eight weeks almost.

978
00:59:29,961 --> 00:59:32,000
The whole program sat in
their office has helped them.

979
00:59:32,001 --> 00:59:34,760
We got a call from him later
about how well it went,

980
00:59:34,840 --> 00:59:39,170
been able to help them put it, you know,
expand and supplement their team. Um,

981
00:59:39,290 --> 00:59:42,050
and that, you know, I think that's
something we'd like to try again.

982
00:59:43,020 --> 00:59:44,760
And one last question
because this has gotten,

983
00:59:44,761 --> 00:59:49,761
so Elon Musk recently call the floor
preemptive AI regulation at the National

984
00:59:51,000 --> 00:59:53,580
Governors Association. As a chair
of open AI and friend of mosques,

985
00:59:53,581 --> 00:59:57,240
what is your opinion on this issue and
what specific actions can we take to

986
00:59:57,241 --> 00:59:58,530
minimize future risk?

987
00:59:58,710 --> 01:00:02,490
The specific thing I would support
today is just, um, insight. Like,

988
01:00:02,491 --> 01:00:07,350
I think the government should understand
where the edge of capabilities are and

989
01:00:07,351 --> 01:00:10,800
how it's evolving. Um,
because I think no one,

990
01:00:11,130 --> 01:00:15,330
certainly not the government knows what
the regulation for AI should look like

991
01:00:15,331 --> 01:00:19,950
today, but I'd be in favor of starting
that education process. Awesome. So yeah,

992
01:00:19,951 --> 01:00:21,650
that was our last question.
Thank you all everyone.

993
01:00:22,070 --> 01:00:27,070
[inaudible].

