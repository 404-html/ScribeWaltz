Speaker 1:          00:01          Hello Greek

Speaker 2:          00:02          bitches. Do you think in our lifetimes and or in our children's lifetime, it's feasible that we figure out a way in some way to, I'm not endorsing like taking people's money and giving it to other people, but in some sort of a way to eliminate poverty. Is that even possible? Is it ever going to be possible to completely eliminate poverty worldwide and within like a lifetime? Well, I think we talked about this the last time when we spoke about ai, but I mean this is the implication of much of what we talked about here. If you, if you imagine building the perfect labor saving technology, right? What do you imagine? Imagine just having a machine that can build any machine that can do any human labor. You powered by sunlight more or less for the cost of raw materials, right? So you're talking about the ultimate wealth generation device and now we're not just talking about blue collar labor, we're talking about the kind of labor you and I do, right?

Speaker 2:          01:03          So like artistic labor and scientific labor and, um, you know, just a machine that comes up with good ideas. Right? And He, what we're talking about, general artificial intelligence, um, this if in the right political and economic system, this would just cancel any need for people to have to work to survive, right? It just would be, there'd be enough of everything to go around. And then the question would be, do we have the right political and economic system where we, where we actually could spread that wealth or would we just, we just find ourselves in some kind of horrendous arms race and, and, uh, uh, situation of, of wealth inequality, uh, unlike any we've ever seen. Um, it's a, um, we don't, we don't have the, it's not in place now and if someone just handed us this device, you know, if, if, um, and it were, you know, all of my concerns about Ai, we're gone.

Speaker 2:          02:03          I mean, there's no question about this thing, uh, doing things we didn't want it would do exactly what we want when we want it. And there's no, there's just no danger of it. Uh, it's interest becoming misaligned with our own. It's just like a perfect oracle and a perfect designer of new technology. Um, if, if it was handed to us now, you know, I would expect just complete chaos, right? I would explain it if, if, if facebook built this thing tomorrow and announced it or rumor spread that they had built it, right. What are the implications for Russia and China? Well, in so far as they are as adversarial as they are now, it would be rational for them to just new California, right? Because, because the happiness device is, there's a winner take all scenario. I mean you, you, when the world, if you have this device, you can turn the lights off in China.

Speaker 2:          02:56          You know, the moment you have this device, you can just, it's the ultimate. Um, cause literally you were talking about and you know, many people are made doubt whether such a thing as possible. But again, we're just talking about the implications of intelligence that can make refinements to itself in over time. Course that is bears no relationship to what we experience as apes. Right? So you're talking about a system that can make changes to his own source code, um, and become better and better at learning. And more and more knowledgeable has instantaneous, if we give it access to the Internet, it has instantaneous access to all human and machine knowledge. And uh, it does, you know, thousands of years of work every, every day of our lives, right? They just thousands of years of equivalent human level intellectual work. Um, it's just a, it's on, I mean, our intuitions completely falter at t to, to capture just how immensely powerful such a thing would be.

Speaker 2:          04:02          And there's no reason to think this isn't possible. I mean, the, the, the most skeptical thing you can honestly say about this is that this isn't coming soon, right? It's like this is not, but to say that this is not possible makes no scientific sense at this point. There's no reason to think that a sufficiently advanced digital computer can't pee, can't instantiate general intelligence of a sort that we have. There's no reason to think that the intelligence has to be at bottom some form of information processing. And if we get the algorithm right with enough hardware resources and the, and the limit is definitely not the hardware at this point. It's, it's the, the algorithms, um, there's just no reason to think this could, can't take off and, and scale. And then we would be in the presence of something that is, that is, uh, like having an, an alternate human civilization in a box that is making thousands of years of progress every day.

Speaker 2:          05:05          Right? So just imagine that if you had in a box, you know, the 10 smartest people who've ever lived, and you know, every time, every week, they make 20,000 years of progress, right? Because that is the actual, we're talking about electronic circuits being a million times faster than, than biological circuits. So even if it was just, and I, I believe I said this the last time we talked about ai, but this is, you know, this is what brings it home for me. Even if it's just a matter of faster, right? It's not, it's not anything especially spooky is chest. This can do human level, intellectual work, but just a million times faster. And again, this is totally under sells. The prospects of superintelligence, I think human level intellectual work is, is um, uh, it's going to seem pretty paltry in the end. But if you just imagine just speeding it up, if you imagine if a full, if we were doing this podcast, imagine how smart I would seem if between every sentence.

Speaker 2:          06:04          I actually had a year to figure out what I was going to say next. Right? And so I say this one sentence and you say, you asked me a question and then in my world, I just have a year, I'm going to go spend the next year getting, getting ready for, for Joe, and it's going to be perfect. And this is just compounding upon itself. Like not only can I not only, uh, I, um, am I working faster? Ultimately I can change my, my ability to work faster. I mean, like we were talking about software that can change itself. You're talking about something that that becomes self improving. So there's a compounding function there. But, um, it's the point is, is unimaginable, uh, in terms of how, uh, how much change this could effect. And if you imagine the best case scenario where this is under our control, right, where there's no alignment problem, where it's just, it doesn't, this thing doesn't do anything that surprises us, this thing we'll always take direction from us.

Speaker 2:          07:04          It will never, it will never develop interests of its own. Right. Which is again the fear. But let's, let's just say this is totally obedient, is just an oracle and a genie route, you know, in, in one. And um, you know, we say, you know, cure Alzheimer's and it cures Alzheimer's. You know, you solve the protein folding problem and, and it just is, it's just and running and to develop a perfect nanotechnology and it does that. This is all, again, going back to David Deutsch, there's no reason to think this isn't possible because anything that's compatible with the laws of physics can be done given the requisite knowledge, right? So you just, you get enough intelligence, as long as you're not violating the laws of physics, you can do something in that space. Um, so, but the problem is this is a winner take all scenario.

Speaker 2:          07:54          So facebook does it tomorrow and China and Russia find out about it. They can't afford to wait around to see whether the US decides to do something not entirely selfish with this, right? Because they're, the, their worst fears could be realized if Donald Trump is president. What's Donald Trump going to do with a perfect ai when he has already told the world that he hates Islam? Right? Um, it's a, it's a, um, we would have to have a political and economic system that allowed us to absorb this ultimate wealth saved. We'll wealth producing technology. Um, and, and again, so this may all sound like pure Psi Phi craziness to people. Um, I don't think there is any reason to believe that it is, but walk way back from that edge of craziness and just look at, um, dumb ai, you know, narrow ai, just self driving cars and automation and, um, intelligent algorithms that can do human level work that is already poised to change our world massively and create massive wealth inequality, which we have.

Speaker 2:          09:06          We would have to figure out how to spread this wealth. You know, what do you do when you can automate, uh, 50% of, of human labor? Were you paying attention to the, uh, artificial intelligence go match? Yeah. Yeah. Uh, no, I don't actually play goes, so I wasn't paying that kind of attention to it, but I'm aware of what happened there and you know, the rules of go, um, not, not so that I know, actually I don't, I don't, I don't play it a, no, I don't, I don't even know if I know vaguely how you, how you, um, how it looks when a game is played, but I don't supposed to be very complicated. The more complicated and more possibilities than chess. Yeah. And that's why it took 20 years longer for a computer to be the best player in the world. Um, it's, it is.

Speaker 2:          09:52          Um, did you see how the computer did it too? Well, I didn't, I know, I mean, this is to the company that did it is, um, deep mind, which was acquired by Google and they're at the cutting edge of ai research and yeah. Well, it's, the cartoons are unfortunately not so far from what is possible. But, um, the, uh, yeah, I mean there's, again, this is no, this is not general intelligence. Like we're talking about something like these are not machines that can even play tic Tac toe right now. There's some, there there've been some moves away from this or like deep mind has trained an algorithm to play all of the Atari Games like, oh, from 1980 or whenever. Um, and it is very quickly became superhuman on most of them. I think. I, I don't think it's superhuman and all of them yet, but it could play in a space invaders and all these and breakout and all these games that are, are, um, uh, uh, to highly unlike one another.

Speaker 2:          10:58          And it's the same algorithm becoming expert in superhuman and all of them. And that's, that's a new paradigm and it's using a technique called deep learning for that. Um, and that's, and that's been very exciting and I will be incredibly useful. You know, this is, I mean, the other, the flip side of all of this, I know that everything I tend to say on this sound scary, but this is all like, I mean, the next scariest thing is not to do any of this stuff. It's like we, we want intelligence, we want automation, we want to figure out how to solve problems that we can't get soft. So like intelligence is the best thing we've got. So we want more of it. Uh, but we have to have a system where, I mean, it's scary that we have a system where if you gave the best possible version of it to one research lab or to one government, it's not obvious that that wouldn't destroy humanity. Right. That wouldn't lead massive dislocations where you'd have, you know, some trillionaire who's trumpeting his new device and, and just, you know, 50% unemployment in the u s you know, oh, in a month. Right? I mean, like, he, like, it's not obvious how we would absorb this level of, of progress. Um, and we, we, we definitely have to, to figure out how to do it. I know, of course we can't assume the best case scenario. Right? That's the best case scenario.

Speaker 3:          12:18          I think there's a few people that put it the way you put it, that terrify the shit out of people. Right. And everyone else seems to have this rosy vision of increased longevity and automated everything and everything fixed and easy to get to work and medical procedures would be easier. Or they're going to know how to do it, but everybody looks at it like, we are always going to be here, but are we obsolete? I mean, is this idea of a living thing that's creative and wrapped up in emotions and lust and desires and jealousy and all the pettiness that we celebrated all the time. We still see it. It's not getting any better. Right. If, if we obsolete, I mean, what if this thing comes along and says, listen, there's a way to do it. You're going to abandon all that stupid shit and you can abandon all that makes you all the stuff that makes you fun to be around. Yeah. And also fucks with, you could live three times as long without that stuff.

Speaker 2:          13:12          Oh, well, I think it, it would in the best case, would usher in Ei. Ei,

Speaker 4:          13:22          yeah.

Speaker 2:          13:23          The possibility of, of kind of fundamentally creative life where on the order of something like the Matrix, whether it's in the matrix or it's just in the world that has been made as beautiful as, as possible, um, based on

Speaker 2:          13:44          what would functionally be an unlimited resource of intelligence. Let me just say, it's just like for there to be a, an ability to solve problems of a sword that we can't currently imagine. I mean, it's just, it really is like a place on the map that you can't, you can't, and you can indicate it's over there. You know, it was like a blank spot on the map. This is why it's called the singularity. Right? It's like, this is it. This is a, uh, it was, it was John von Neumann, the, um, the, the inventor of game theory who a mathematician who, um, uh, is one along with Alan Turing and a couple of other people's was really responsible for the computer revolution. He was the first person to use this term, singularity, uh, to describe just this, that, that there's a speeding up of, um, information processing technology and April a cultural reliance upon it, uh, beyond which we can't actually foresee the level of change that can come over our society.

Speaker 2:          14:48          It's like, you know, an event horizon pass, which we can't. Um, and uh, this certainly becomes true when you talk about these intelligence systems being able to make changes to themselves. And again, we're talking mostly software is not, I'm not imagining, um, I mean the, the most important breakthroughs certainly at the level of, of better software. I mean the, is we have, in terms of the computing power that if the physical hardware on earth, it's not, that's not what's limiting our ai at the moment. It's not like we need more, more, uh, hardware. Um, but we will get more hardware to up to the limits of physics and it'll, it get smaller and smaller as it has a, and you know, if quantum computing becomes possible, um, or practical, um, that will, uh, actually David Deutsch is, is, um, the physicist I mentioned is one of the fathers of the concept of quantum computing.

Speaker 2:          15:48          Um, that will open up a whole nother area, you know, extreme of computing power. That is, I'm not at all analogous to the kinds of, of, uh, machines we have now. But, um, it's just when you imagine people don't, people seem to always want to, I just had this conversation with Neil degrasse Tyson on my podcast. He named dropper. Yeah. I know. It was just, I'm just keeping pookie people. I'm just, I'm just attributing these ideas to him. Uh, he's not a, he doesn't take this line at all. He's not at all, he thinks it's all bullshit. Right. He's not at all worried about Ai. Right. What does he think? He thinks that, you know, we just, we just use, he's drawing an analogy from how we, you currently use computers that they just, they just keep helping us do what we want to do.

Speaker 2:          16:44          Like we decide what we want to do with computers and we just add them to our process and that process becomes automated and then we'll find new jobs somewhere else. Like you didn't, you don't need a stenographer once you have voice recognition technology and um, uh, that's not a problem. A stenographer we'll find something else to do. And so the economic dislocations isn't that bad and um, computers will just get better than they are and you know, eventually Siri will actually work, you know, and you'll, she'll answer your questions. Well and you're not, it's not going to be a laugh line what Siri said to you today. And, um, then all of this, we'll just proceed to make life better right now. Um, none of that is imagining what it will be like to make it, because it would be a certain point where you'll have systems that are, you know, it's like the Cha, the best chess player on earth is now always going to be a computer, right?

Speaker 2:          17:42          It's never, there's no, there's not going to be a human born tomorrow that's going to be better than the best computer. I mean, it's like, it's already like we have superhuman chess players on earth. Now imagine having computers that are superhuman at every, uh, every task that is relevant, every intellectual task, right? So the best physicist is a computer. You know, the best medical diagnostician is a computer. The best, um, prover of math theorems has a computer. The best engineer is a computer, right? Then there's no, there's no reason why we're not headed there. I mean, it would be the only reason I could see we're not headed there is it's something massively dislocating happens that prevents us from continuing to improve our intelligent machines. But if you just, the moment you admit that intelligence is just a matter of information processing and you admit that we will continue to improve our machines unless something heinous happens because this is intelligence and automation are the most valuable things we have.

Speaker 2:          18:43          Um, at a certain point, whether you think it's in five years or 500 years, we're going to find ourselves in the presence of super intelligent machines. And then at that point, the best source of innovation for the next generation of software or hardware or both will be the machines themselves. Right? So then, so then you just have, then that's where you get what, what was, what the mathematician Ij good described as as the intelligence explosion, which is just the process can take off on its own. Um, and this is where the singularity people, um, either either are hopeful or worried. Um, but you know, cause there's nothing, there's no guarantee that this process will be, remain aligned with our interests. And, and every person who I meet, even, you know, very smart people like Neil, um, who says they're not worried about this. When you actually drill down on why they're not worried, you find that they're actually not imagining machines making changes to their own source code.

Speaker 2:          19:52          Um, and they're not an or they're, they, they simply believe that this is so far away that we don't have to worry about it now. Right. And that's actually a non sequitur. I mean, to say that this is far away is not actually grappling with, it's not an argument this isn't going to happen. And, um, and it's based on what to, and it's, and it's based on, first of all, there's no, there's no reason to believe Jamie want to find out where it is. Um, there's no, I mean we don't know how long it will take us to prepare for this. Right? So like, like if, if you were, if you knew this, it was going to take 50 years for this to happen, right. Is 50 years enough for us to prepare politically and economically to deal with the ramifications of this and, and to do it and to add to say nothing of actually building the ai safely in a way that's aligned with our interests.

Speaker 2:          20:53          I don't know. I mean 50 so 50 years is, it's like we've had the iphone for what, 10 years? Nine years? I'm it, it's like 50 years, not a lot of time, right. To, to deal, to deal with this. And um, this has no reason to think it's, it's that far away. If we keep making progress, I mean it's, it's not, it will be amazing if it were 500 years away. I mean, that, that seems like it's, it's, it's more likely from what I am in the sense I get from the people who are doing this work, it's far more likely to be 50 years than 500 years. Like, you know, um, uh, I mean the p at the people who think this is a long, long way off or, I mean they're saying, you know, 50 to 100 years, no one says 500 years.

Speaker 2:          21:50          No, no. As far as I know, no one who was actually close to this work and some people think it could be in five years. Right. I mean the people who are, you know, like the deep mind people who are very close to this or are the sorts of people who say, cause the people, the people who are close to his work are astonished by what's happened in the last 10 years. We went from a place of, you know, very little progress too, you know, wow, this is all of a sudden really, really interesting and powerful. And um, and again, progress is compounding in a way that's counter intuitive. People systematically overestimate how much change can happen in a year and underestimate how much change can happen in 10 years. And I, you know, as far as estimating how much change can happen in 50 or a hundred years, I don't know that anyone is good at that.

Speaker 3:          22:43          How could you be with giant leaps come giant exponential leaps off those leaps. And it's, it's almost impossible for us to really predict what we're going to be looking at 50 years from now. But I don't, I don't know what they're going to think about us. That's what's most bizarre about it as well. We really might be obsolete if we look at how ridiculous we are. Look at this political campaign, look at what we pay attention to in the news. Look at the things we really focus on where our strange, ridiculous animal and w y if we look back on, you know, some strange dinosaur that at a weird neck, why should that fucking thing make it, you know, why should we make it? We, we might be here to make that thing. And that thing takes over from here with no emotions, no lust, no greed, and just purely existing electronically and for what reason?

Speaker 2:          23:34          Well, that, that's a little scary there. There are computer scientists who when you talk about why they're not worried or talk to them about why they're not worried, they just swallow this pill without any qualm that we're going to make. The thing that is far more powerful and beautiful and important than we are. And it doesn't matter what happens to us. I mean, that was our role. Our role was to build these mechanical gods and, and it's fine if they squash us. Um, and I've literally heard a, a, people say, I've heard someone give a talk. I mean, that's what woke me up to, oh, how interesting in this area is. I went to this conference in San Juan about a year ago. Um, and there were, uh, you know, like the people from deep mind were there and there were the people who were very close to this work were there.

Speaker 2:          24:30          And, um, I mean to hear some of the reasons why you shouldn't be worried from people who were interested in, in calming the fear is so they could get on with doing their very important work. Um, it was amazing because they were highly uncompelling reasons not to be worried. It's just, um, so, so they had a, they had a desire to be compelled. They're not, they're not well, not well known that people, people want to do this. There's a deep assumption in many of these people that we can figure it out as we go along. Right? It's like, you know, it's just like, we're going to, we're just going to get, going to get closer. We're foot. We're far enough away now. Even five year, even if it's five years, five years, we'll, we'll, we'll get there. Once we get closer, once we get something a little scary, then we'll pull the brakes and talk about it.

Speaker 2:          25:26          But the problem is they are a sent, everyone is essentially in a race condition by default. And you have, you know, Google is racing against facebook and the u s is racing against China and every, every group is racing against every other group. Um, however you want to conceive of groups. This, this is a, to be the first one to be the first one with incredibly powerful, narrow ai is to be the next, you know, multibillion dollar company, right? So everyone's trying to get there and uh, if they suddenly get there and sort of overshoot a little bit and now they've got something like, you know, general intelligence or something close, um, what we're relying on every, and they know everyone else's attempting to do this, right? Um, W we don't have a system set up where everyone can pull the breaks together and say, listen, we've got to stop racing here.

Speaker 2:          26:22          We have to share everything. We have to share the wealth, we have to share the information, we have to um, this truly has to be open source in every conceivable way. And um, we have to diffuse this winner take all dynamic. Um, you know, I think we need something like a Manhattan project to figure out how to do that. You know, not if not to figure out how to build the AI, but to figure out how to build it in a way that does not create an arms race that does not create, um, an incentive to build unsafe ai, which is almost certainly going to be easier than building safe ai and just to work out all of these issues because it's, it's not because what I think we are, we're going to build this by default. We're just going to keep building more and more intelligent machines and this is going to be done in by everyone who can, can do it with each generation.

Speaker 2:          27:16          If we were even talking about generations, it's going to be, it will have the tools made by the prior generation that are more powerful than anyone imagined a hundred years ago. And it just, it gets going to keep going like that. Did anybody actually make that quote about giving birth to the mechanical gods? I, no, that was just made. Yeah, but it was, there was a scientist that actually was thinking and saying that, but that was, that was the content of what he was saying. He's like, what are we going to build the next species that is far more important than we are? And that's a good thing. And it, well, and actually I can go there with him. I mean, it actually, the only, the only caveat here is that unless they're not conscious, right? Like if you, the true horror for me is that we can build things more intelligent than we are, more powerful than we are.

Speaker 2:          28:09          Uh, and that can squash us and they might not, they might be unconscious, right? There might be nothing like the universe could go dark if they squash us, right? Or, or at least our corner of the universe could go dark. Um, and yet these things will be immensely powerful. Um, so if, and this is just, you know, the jury's out on this, but if there's nothing about intelligence scaling that demands that consciousness come along for the ride, um, then it's possible that, I mean, nobody thinks our machines are, you know, very few people would think are machines that are intelligent or conscious. Right? So at what point does consciousness come online? Um, maybe it's possible to build super intelligence that's unconscious, you know, super powerful, does everything better than we do. You know, it'll recognize your emotion better than, than another person can, but then the lights aren't on that.

Speaker 2:          29:02          That's, that's also I think possible. But maybe it's not possible, but that's, that's the worst case scenario because in the ethical silver lining and speaking, you know, outside of our self interest now, but just from a bird's eye view, um, the ethical silver lining to building these mechanical gods that are conscious is that, yes, okay. We've, in fact if we have built something that is far wiser and has far more beautiful experiences and deeper experiences of the universe and we could ever imagine and there there's something that it's like to be that thing that's just, you know, it is a, has a kind of a God like uh, experience. Um, well that would be a very good thing then we will have built you, we will have built something that was, you know, if you stand outside of our narrow self interest, I can understand why he would say that he, he was just assuming what was scary about that particular talk cause he was assuming that consciousness comes along for the ride here. And I don't know that that is a safe assumption. Well and the really terrifying thing is who,

Speaker 3:          30:10          if, if this is constantly improving itself and it's under the Beck and call of a person then so it's either conscious or unconscious where it acts as itself, right? It acts as an individual thinking unit. Right? Or as a a a thing outside of it's aware, right? Either it is or it isn't. And if it isn't aware and some person can manipulate it, like imagine if it's getting 10,000, how many, how many thousands of years in a week did you say? I'll go, well, if it was just improvement, it was just a million times faster than we are. It's 20,000 years, 20,000 years in a week and a weekend in a week. So with every week, this thing constantly gets better at even doing that. Right? So it's reprogramming itself. So it's all exponential.

Speaker 2:          30:55          Presumably it just, it just imagine again that you could keep it in the most restricted case. You could just keep it at our level, but

Speaker 3:          31:03          just, just faster, just a million times faster. But if it did, all of these things, if it kept going and kept every week was thousands of years, right? We're going to control it. A person. No, think that's even more insane.

Speaker 2:          31:15          Imagine being in dialogue with something that had that, that lived the 20,000 years of human progress in a week. And you come back, you know, on Monday and say, listen, um, I, that thing I told you to do last Monday, I want to change that up and this thing has made 20,000 years of progress. Um, and if it's in a condition where it has access, I mean, so we're imagining this thing, you know, in a box, you know, air gapped from the Internet and it's got nothing. He's got no way to get out. Right. Uh, even that is an unstable situation. But just imagine this emerging in some way online, right? Already being out in the wild. Right? So let's say it's in a financial market, right? Um, that's again, this is what worries me most about this and what is also interesting is that our intuitions here, I think the primary intuition that people have is no, no, no, that's just, that's just not possible.

Speaker 2:          32:13          Or not at all likely. But if, if you're going to fund, if you, you're going to think it's impossible or even unlikely, you have to find something wrong with the claim. That intelligence is just a matter of information processing. Um, I don't know any scientific reason to doubt that claim at the moment. Um, and uh, very good reasons to believe that it's just undoubtable, uh, and the, and you have to doubt that we will continue to make progress in the design of intelligent machines. And, but once you then is then that all this left is just time, right? If, if, if intelligence is just information processing and we were going to continue to build better and better information processors, at a certain point we're going to build something that is superhuman. Um, and so whether it's in five years or 50, it's, it's a huge, I mean it's, it's the biggest change in human history I think we can imagine.

Speaker 2:          33:22          Right? Um, so, uh, and then people, I what I felt fine. I keep finding myself in the presence of people who seem at least to my eye to be refusing to imagine it like late, they're treating it like the y two k virus or whatever where it's just the y two k bug where it just may or may not be an issue. Right? Like it, like it's a hypothetical, like May this is just, we're going to get there and it's, it's just going to be, it's either not going to happen or it's, it's, it's going to be trivial. But how you don't, if you don't have an argument for why this isn't going to happen, uh, then you have to have then, then you're left with, okay, what's it gonna be like to have, uh, systems that are better than we are at everything in the intellectual space. Um, and you know, what will happen if that suddenly happens in one country and not in another, right? It's, um, it's, uh, it's, I mean, it has enormous implications, but it just sounds like science fiction. Yup.

Speaker 3:          34:27          I don't know what's scarier. The idea that an artificial intelligence can emerge. It's conscious, it's aware of itself and that acts to present per, per protect itself or the idea that a person, a regular person like of today could be in control of essentially a god. Right? Because if this thing continues to get smarter and smarter with every week and more and more power and more and more potential, more and more understanding, thousands of years, I mean it's just, yeah, this one person, uh, per regular person controlling that is almost more terrifying than creating a new life or, or any group of people who don't have the total welfare of humanity as their central concern. And so just imagine, I mean, what would, what would China do with it now? Right? What would we, what would we do if we thought China, you know, Baidu or whatever,

Speaker 2:          35:16          or some Chinese company was on the verge of this thing. Um, what would it be rational for us to do? You know? I mean, if North Korea had it, it would be, it'd be rational to nuke them given what they say about, you know, their relationship with the rest of the world. So it's, um,

Speaker 3:          35:34          well that kind of power. Would you say rational that of power is, it's so life changing. It's so what paradigm shifting,

Speaker 2:          35:42          right? But if you to, to wind this back to what someone like Neil degrasse Tyson would say, is that the only basis for fear is, yeah, don't give your superintelligent ai to the next Hitler, right? That's, that's obviously bad. But if we don't, if we're not idiots and we just use it, well, we're fine. And that I think is an intuition that is just, that's just a failure to, to unpack what is entailed by, again, something like an intelligence explosion, a process that once, once you're talking about something that is able to change itself and you have to get it. So what would it be like to guarantee level? Let's say we decide, okay, we're just not going to build anything that can make changes to his own source code. You know, any change to it, to software at a certain point is going to have to be run through a human brain.

Speaker 2:          36:37          Um, and we're going to have veto power. Well, is every person working on ai going to abide by that rule? It's like we, we've agreed not to clone humans, right? But you know, we're going to stand by that agreement for the, in the rest of human history. And is, is a, is our agreement binding on China or Singapore or you know, any other country that might think otherwise. It's just we have a, it's a free for all and at a certain point we're going to be, you know, close enough. Everyone's going to be close enough to making the final breakthrough that, um, unless we have some, uh, agreement about how to proceed if someone is going to get there first, that is a terrifying scenario of the future.

Speaker 3:          37:22          You know, you cemented this last time you were here, but not as extreme as this time. You seem to be accelerating the rhetoric. Exactly. Yes. You're going deep. Yeah. Boy, I hope you're wrong. I'm on team Neil degrasse Tyson, right? This one,

Speaker 2:          37:41          Neil. Um, and also in defensive of the other side too, I should say that, you know, like, so David Deutsch also thinks I'm wrong, but he thinks I'm wrong because we will integrate ourselves with these machines. I mean, so that we, this will be, there'll be extensions of ourselves and they can't help but be aligned with us because we will, we will be connected to them. That seems to be the only way we could all get along. We have to merge become one. But I just think there's no, there's no deep reason why, even if we decided to do that, right? Like in the u s or, or, or in half the world, um, one, there's, I think there are reasons to worry that even that could go haywire, but there's no guarantee that someone else couldn't just build ai in a box. I mean, if we, if we can build ai such that we can merge our brains with it, um, someone can also just build ai in a box.

Speaker 2:          38:30          Right. And, and that's, uh, um, and then then inherit all the other problems that people are saying. We don't have to worry about. If it was a good coen brothers movie, it would be invented in the middle of the presidency of Donald Trump. And so then that's when ai would go live. And then ai would have to challenge Donald Trump and they would have like an insult context, but that that's when this thing becomes so comically, uh, terrifying where it's just, just imagine Donald Trump being in a position to make the final decisions on topics like this for the country. That is, acne is going to do this almost certainly in the near term. It's like he should, we have a Manhattan project on this point, Mr President. Um, you know, the idea that anything of a value could be happening between his ears on this topic. We're a hundred others.

Speaker 2:          39:27          Like it I think is now really inconceivable. And so what w what, what price could we, might we pay for that kind of inattention and, and you know, self satisfied in attention to these kinds of issues? Well, this, this issue, if this is real and if this could go live in 50 years, this is the issue. Yeah. Unless we fuck ourselves up beyond repair before then and shut the power off if it keeps going. Yeah. No, I think it is. I think it is the issue, but unfortunately it's the, it's the issue that doesn't, it sounds like a goof. Yeah, it does just sound, you sound like a crack pot even worrying about this issue. It sounds completely ridiculous, but that might be what's how it's sneaking in. Yeah. Yeah. I mean it's just, it, just imagine that the tiny increment that would make suddenly make it compelling.

Speaker 2:          40:20          I mean, just imagine, I mean chest doesn't do it because chess is so far from any central human concern. But just imagine if your, if your phone recognized your emotional state better than any than your best friend or your wife or anyone in your life. And it did it reliably. Right. So as your body liked that movie with Joaquin Phoenix, her, he falls in love with his phone. Right? I mean, that's just not, you know, that is not far off that far off as not, it's a very discreet ability. I mean, you could do that. You could do that without any, any other ability in the phone. Really. It's like, it doesn't, it doesn't have to to, uh, stand on the shoulders of any other kind of intelligence. It could just, you know, he just, you have, I mean, this could be, you could do this with just brute force in the same way that you have a great chess player that doesn't necessarily understand that is playing chess. You could have it, you know, it's kind of facial recognition, facial recognition of emotion and the, and the tone of voice, recognition of emotion and um, the idea that it's going to, it's going to be a very long time for computers that get better than people at that I think is very farfetched. I was thinking, yeah, think you're right. I was

Speaker 3:          41:33          just thinking how strange would it be if you had like headphones on and your phone was in your pocket and you have rational conversations with your phone? Like your phone knew you better than you know, you like, I mean, I don't know what to do. I mean, I don't think I was out of line. She yelled at me. I mean, what should I say? And it would have listened to every one of your conversations with your friends. Exactly. Train up on that and just talk to you about it and go, listen man, this is what you got gotta do. I was talking to political, you were sounding angry. You got defensive, you got defensive. Why were you so to hold that Joe? Yeah. Apologize, relax. Let's all move on. If you could accelerate it or, okay. You right man. Right man, and like you're talking to this little artificial.

Speaker 3:          42:07          Maybe that's the first version of artificial intelligence that we suggest. We'd say, all right, let's give it a shot and like self help guys in your phone, you have like a personal trainer in your phone. How to talk to girls at first. Breakthrough yet slow down, dude. Slow down. You're talking too fast. Got To cool. Yeah, I mean literally like giving you information. That would be like step one. That'd be like the Sony Walkman. Remember when you had a Walkman, like a cassette player that was like, that was like a vcr when we were on our way to what we have today where you have fucking 30,000 songs in your phone or something. I think I remember the first Walkman. The first thing I just want to it back when I skied, there was something called, it was called astral tunes or something. It was like, it was like a car radio that, that you could just put on in a pack on your chest.

Speaker 3:          42:53          Um, yeah, it was, they kept coming out with those sounds. They would get smaller and smaller. So then that little, the little dude would start telling you, Yo man, dude, listen to keep it. Place me over a year. Just let him stick me in. Your brain will be together all the time. Yeah, yeah. Giving you good advice for years, Bro. Let me and your brain. And so you and this little artificial intelligence, do you have a relationship over time? And eventually it talks you into getting your head drilled and the screw it in there and your artificial intelligence is always powered by your central nervous system. Did you, have you seen most of these movies? Like did you see her and, and no, I didn't. Okay. No. Did you see Alex? Mark and I, that was one of my, that was good. Top 10 all time favorite movies.

Speaker 3:          43:35          Yeah, I loved that movie actually. I like it. I saw it twice. I said, I, I, I was slow to realize how well, uh, they, they did it. I mean, it was just the first time I saw it, I thought, um, I wasn't as impressed and I watched it again and they really, I mean, first of all, the performance of, of um, I forgot the actress's name. Uh, the candor, at least you have a candor today. Um, there was a woman who plays the robot in an x, Monica. It was just fantastic. But, um, scary. Good tuck you in. Anything. We're getting a little full on time. Yeah. What do we like five hours in a half hours in. But I just got to know how to spot to fill up. Wait a minute. How many hours? Four and a half hour. No computers about to fill up. Yeah we did. We just did a four and a half hour pocket. We ready to keep going to Jesus. Jamie didn't cock block. You know what man, once you opened up that box up Pandora's box of artificial, I haven't heard you guys discussed

Speaker 5:          44:32          it and I've looked up, is there any sort of concept of autism in ai? Like a spectrum of ai? Like oh, there are dumb ai and there's going to be smart Ai.

Speaker 2:          44:42          I know. So, I mean the scary thing so that yeah, it's like super autism. There's no, um, across the board there's, I think that super intelligence and motivation and goals are totally separable. So you could have a super intelligent machine that is purposed toward a goal that just seems completely absurd and harmful and non common sensical. And they said they, the example that that Nick Bostrom uses in his, in his book superintelligence which was a great book, um, and did more to inform my thinking on this topic than any other source. Um, he talks about it a paperclip maximizer. You could, you could build a super intelligent paperclip maximizer. Now, not that anyone would do this, but the point is you could build a machine that was, that was smarter than we are in every conceivable way, but all it wants to do is produce paper clips.

Speaker 2:          45:34          Right. Now that seems counterintuitive, but there's no, there's no reason when you kind of dig deeply into this, there's no reason why you couldn't build a superhuman paperclip maximizer. I just wants to turn everything, you know, just literally the atoms in your body would be better used as paperclips. Um, and so this is just the point he's making is that superintelligence could be very counterintuitive. It's not necessarily going to inherit everything we find as, you know, common sensical or, or emotionally appropriate or wise or desirable. Uh, it could be totally foreign, totally trivial in some way, you know, focused on something that means nothing to us, but means everything to it because of some quirk and how it's motivation system is structured and yet it can build the perfect nanotechnology that will allow it to build more paperclips. Right? So, um, and Lisa, at least, I don't think anyone can see why that's ruled out in advance.

Speaker 2:          46:35          I mean, there's no reason why we would intentionally build that, but the fear is we might build something that either is not perfectly aligned with our goals and our common sense and our, and our, um, aspirations and that it could form some kind of separate in instrumental goals to get what his wants, wants that are totally incompatible with life as we know it. And that's, you know, I mean, again, the, the, the examples of this are always cartoonish. Like, you know how I mean, Elon Musk said, you know, if you built up super intelligent machine and he told it to reduce spam, well then it could just kill all people as a great way to reduce spam. Right. Um, but see the reason why that's La, that's laughable, but there's, you can't assume the common sense won't be there unless we've built it. Right? Like, you have to have anticipated all of this.

Speaker 2:          47:27          You can't, if you say, take me to the airport as fast as you can. Again, this is Bostrom, you know, and you have a super intelligent, automatic a car. Um, you know, a self driving car, you'll just, you'll get to the airport covered in vomit because he'll just, it's, it's just going to go as fast as it can go. Um, so it's a, it's our intuitions about what it would mean to be super intelligent necessarily, or are, um, I mean there's no, we have to correct for them, cause I think our intuitions are, are bad.

Speaker 6:          48:03          [inaudible].