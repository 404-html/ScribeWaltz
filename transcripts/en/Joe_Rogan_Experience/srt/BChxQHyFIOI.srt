1
00:00:01,030 --> 00:00:02,120
Hello Greek

2
00:00:02,120 --> 00:00:06,110
bitches.
Do you think in our lifetimes and or in

3
00:00:06,111 --> 00:00:11,111
our children's lifetime,
it's feasible that we figure out a way

4
00:00:12,080 --> 00:00:15,980
in some way to,
I'm not endorsing like taking people's

5
00:00:15,981 --> 00:00:19,280
money and giving it to other people,
but in some sort of a way to eliminate

6
00:00:19,281 --> 00:00:21,440
poverty.
Is that even possible?

7
00:00:21,530 --> 00:00:24,890
Is it ever going to be possible to
completely eliminate poverty worldwide

8
00:00:25,100 --> 00:00:28,430
and within like a lifetime?
Well,

9
00:00:28,760 --> 00:00:32,330
I think we talked about this the last
time when we spoke about ai,

10
00:00:32,331 --> 00:00:36,530
but I mean this is the implication of
much of what we talked about here.

11
00:00:36,531 --> 00:00:40,280
If you,
if you imagine building the perfect

12
00:00:40,281 --> 00:00:42,590
labor saving technology,
right?

13
00:00:42,650 --> 00:00:46,430
What do you imagine?
Imagine just having a machine that can

14
00:00:46,431 --> 00:00:49,700
build any machine that can do any human
labor.

15
00:00:50,610 --> 00:00:53,660
You powered by sunlight more or less for
the cost of raw materials,

16
00:00:53,661 --> 00:00:56,660
right?
So you're talking about the ultimate

17
00:00:56,661 --> 00:00:59,930
wealth generation device and now we're
not just talking about blue collar

18
00:00:59,931 --> 00:01:02,090
labor,
we're talking about the kind of labor

19
00:01:02,710 --> 00:01:03,770
you and I do,
right?

20
00:01:03,770 --> 00:01:06,980
So like artistic labor and scientific
labor and,

21
00:01:07,460 --> 00:01:08,630
um,
you know,

22
00:01:08,870 --> 00:01:11,480
just a machine that comes up with good
ideas.

23
00:01:11,481 --> 00:01:11,890
Right?
And He,

24
00:01:11,890 --> 00:01:13,910
what we're talking about,
general artificial intelligence,

25
00:01:14,480 --> 00:01:18,380
um,
this if in the right political and

26
00:01:18,381 --> 00:01:22,400
economic system,
this would just cancel any need for

27
00:01:22,401 --> 00:01:25,460
people to have to work to survive,
right?

28
00:01:25,490 --> 00:01:28,070
It just would be,
there'd be enough of everything to go

29
00:01:28,071 --> 00:01:30,530
around.
And then the question would be,

30
00:01:31,010 --> 00:01:33,920
do we have the right political and
economic system where we,

31
00:01:33,980 --> 00:01:37,550
where we actually could spread that
wealth or would we just,

32
00:01:37,650 --> 00:01:41,900
we just find ourselves in some kind of
horrendous arms race and,

33
00:01:41,901 --> 00:01:43,040
and,
uh,

34
00:01:43,310 --> 00:01:44,143
uh,
situation of,

35
00:01:44,150 --> 00:01:46,430
of wealth inequality,
uh,

36
00:01:46,660 --> 00:01:49,320
unlike any we've ever seen.
Um,

37
00:01:50,360 --> 00:01:51,320
it's a,
um,

38
00:01:51,510 --> 00:01:52,550
we don't,
we don't have the,

39
00:01:52,900 --> 00:01:56,390
it's not in place now and if someone
just handed us this device,

40
00:01:56,870 --> 00:01:57,260
you know,
if,

41
00:01:57,260 --> 00:01:59,030
if,
um,

42
00:02:00,260 --> 00:02:01,040
and it were,
you know,

43
00:02:01,040 --> 00:02:03,320
all of my concerns about Ai,
we're gone.

44
00:02:03,320 --> 00:02:05,000
I mean,
there's no question about this thing,

45
00:02:05,200 --> 00:02:07,460
uh,
doing things we didn't want it would do

46
00:02:07,461 --> 00:02:10,460
exactly what we want when we want it.
And there's no,

47
00:02:10,461 --> 00:02:12,680
there's just no danger of it.
Uh,

48
00:02:12,810 --> 00:02:14,870
it's interest becoming misaligned with
our own.

49
00:02:14,871 --> 00:02:18,530
It's just like a perfect oracle and a
perfect designer of new technology.

50
00:02:19,100 --> 00:02:20,580
Um,
if,

51
00:02:20,740 --> 00:02:23,660
if it was handed to us now,
you know,

52
00:02:23,810 --> 00:02:27,920
I would expect just complete chaos,
right?

53
00:02:27,921 --> 00:02:28,670
I would explain it if,
if,

54
00:02:28,670 --> 00:02:33,670
if facebook built this thing tomorrow
and announced it or rumor spread that

55
00:02:34,041 --> 00:02:34,970
they had built it,
right.

56
00:02:35,690 --> 00:02:37,970
What are the implications for Russia and
China?

57
00:02:38,120 --> 00:02:42,500
Well,
in so far as they are as adversarial as

58
00:02:42,501 --> 00:02:44,810
they are now,
it would be rational for them to just

59
00:02:44,811 --> 00:02:46,520
new California,
right?

60
00:02:46,521 --> 00:02:50,200
Because,
because the happiness device is,

61
00:02:50,240 --> 00:02:52,550
there's a winner take all scenario.
I mean you,

62
00:02:52,551 --> 00:02:53,720
you,
when the world,

63
00:02:53,960 --> 00:02:56,450
if you have this device,
you can turn the lights off in China.

64
00:02:56,500 --> 00:02:58,070
You know,
the moment you have this device,

65
00:02:58,071 --> 00:02:59,740
you can just,
it's the ultimate.

66
00:03:00,320 --> 00:03:03,940
Um,
cause literally you were talking about

67
00:03:04,210 --> 00:03:06,370
and you know,
many people are made doubt whether such

68
00:03:06,371 --> 00:03:08,410
a thing as possible.
But again,

69
00:03:08,411 --> 00:03:13,330
we're just talking about the
implications of intelligence that can

70
00:03:13,331 --> 00:03:18,331
make refinements to itself in over time.
Course that is bears no relationship to

71
00:03:22,121 --> 00:03:24,490
what we experience as apes.
Right?

72
00:03:24,491 --> 00:03:28,390
So you're talking about a system that
can make changes to his own source code,

73
00:03:28,900 --> 00:03:31,390
um,
and become better and better at

74
00:03:31,420 --> 00:03:33,910
learning.
And more and more knowledgeable has

75
00:03:33,911 --> 00:03:36,490
instantaneous,
if we give it access to the Internet,

76
00:03:37,150 --> 00:03:40,330
it has instantaneous access to all human
and machine knowledge.

77
00:03:40,750 --> 00:03:43,360
And uh,
it does,

78
00:03:43,480 --> 00:03:46,570
you know,
thousands of years of work every,

79
00:03:46,870 --> 00:03:49,930
every day of our lives,
right?

80
00:03:49,931 --> 00:03:53,110
They just thousands of years of
equivalent human level intellectual

81
00:03:53,111 --> 00:03:54,450
work.
Um,

82
00:03:55,270 --> 00:03:56,370
it's just a,
it's on,

83
00:03:56,680 --> 00:03:58,990
I mean,
our intuitions completely falter at t

84
00:03:59,020 --> 00:04:01,630
to,
to capture just how immensely powerful

85
00:04:01,631 --> 00:04:02,440
such a thing would be.

86
00:04:02,440 --> 00:04:05,560
And there's no reason to think this
isn't possible.

87
00:04:05,860 --> 00:04:06,170
I mean,
the,

88
00:04:06,170 --> 00:04:07,840
the,
the most skeptical thing you can

89
00:04:07,900 --> 00:04:11,680
honestly say about this is that this
isn't coming soon,

90
00:04:12,070 --> 00:04:13,270
right?
It's like this is not,

91
00:04:13,271 --> 00:04:17,710
but to say that this is not possible
makes no scientific sense at this point.

92
00:04:17,960 --> 00:04:22,960
There's no reason to think that a
sufficiently advanced digital computer

93
00:04:23,770 --> 00:04:28,090
can't pee,
can't instantiate general intelligence

94
00:04:28,091 --> 00:04:30,720
of a sort that we have.
There's no reason to think that the

95
00:04:30,790 --> 00:04:34,930
intelligence has to be at bottom some
form of information processing.

96
00:04:34,960 --> 00:04:39,960
And if we get the algorithm right with
enough hardware resources and the,

97
00:04:41,060 --> 00:04:43,390
and the limit is definitely not the
hardware at this point.

98
00:04:43,391 --> 00:04:44,224
It's,
it's the,

99
00:04:44,290 --> 00:04:46,600
the algorithms,
um,

100
00:04:47,980 --> 00:04:49,380
there's just no reason to think this
could,

101
00:04:49,381 --> 00:04:52,180
can't take off and,
and scale.

102
00:04:52,181 --> 00:04:54,280
And then we would be in the presence of
something that is,

103
00:04:54,820 --> 00:04:55,653
that is,
uh,

104
00:04:56,050 --> 00:05:01,050
like having an,
an alternate human civilization in a box

105
00:05:01,420 --> 00:05:04,720
that is making thousands of years of
progress every day.

106
00:05:05,110 --> 00:05:06,700
Right?
So just imagine that if you had in a

107
00:05:06,701 --> 00:05:08,200
box,
you know,

108
00:05:08,201 --> 00:05:10,210
the 10 smartest people who've ever
lived,

109
00:05:10,810 --> 00:05:13,060
and you know,
every time,

110
00:05:13,061 --> 00:05:15,520
every week,
they make 20,000 years of progress,

111
00:05:15,710 --> 00:05:18,190
right?
Because that is the actual,

112
00:05:18,810 --> 00:05:22,300
we're talking about electronic circuits
being a million times faster than,

113
00:05:22,360 --> 00:05:25,690
than biological circuits.
So even if it was just,

114
00:05:25,840 --> 00:05:27,700
and I,
I believe I said this the last time we

115
00:05:27,701 --> 00:05:28,990
talked about ai,
but this is,

116
00:05:29,020 --> 00:05:30,610
you know,
this is what brings it home for me.

117
00:05:31,240 --> 00:05:33,760
Even if it's just a matter of faster,
right?

118
00:05:33,790 --> 00:05:36,160
It's not,
it's not anything especially spooky is

119
00:05:36,161 --> 00:05:38,440
chest.
This can do human level,

120
00:05:38,441 --> 00:05:41,620
intellectual work,
but just a million times faster.

121
00:05:42,130 --> 00:05:45,310
And again,
this is totally under sells.

122
00:05:45,320 --> 00:05:50,050
The prospects of superintelligence,
I think human level intellectual work

123
00:05:50,051 --> 00:05:51,580
is,
is um,

124
00:05:52,360 --> 00:05:54,580
uh,
it's going to seem pretty paltry in the

125
00:05:54,581 --> 00:05:56,650
end.
But if you just imagine just speeding it

126
00:05:56,651 --> 00:05:57,750
up,
if you imagine if a full,

127
00:05:57,840 --> 00:06:02,630
if we were doing this podcast,
imagine how smart I would seem if

128
00:06:02,690 --> 00:06:04,370
between every sentence.

129
00:06:04,880 --> 00:06:09,440
I actually had a year to figure out what
I was going to say next.

130
00:06:09,441 --> 00:06:11,000
Right?
And so I say this one sentence and you

131
00:06:11,001 --> 00:06:12,980
say,
you asked me a question and then in my

132
00:06:12,981 --> 00:06:14,030
world,
I just have a year,

133
00:06:14,090 --> 00:06:16,340
I'm going to go spend the next year
getting,

134
00:06:16,341 --> 00:06:18,230
getting ready for,
for Joe,

135
00:06:18,560 --> 00:06:22,940
and it's going to be perfect.
And this is just compounding upon

136
00:06:22,941 --> 00:06:25,400
itself.
Like not only can I not only,

137
00:06:25,610 --> 00:06:26,660
uh,
I,

138
00:06:26,661 --> 00:06:28,490
um,
am I working faster?

139
00:06:29,030 --> 00:06:32,900
Ultimately I can change my,
my ability to work faster.

140
00:06:32,901 --> 00:06:34,430
I mean,
like we were talking about software that

141
00:06:34,431 --> 00:06:36,190
can change itself.
You're talking about something that that

142
00:06:36,410 --> 00:06:40,100
becomes self improving.
So there's a compounding function there.

143
00:06:40,101 --> 00:06:41,420
But,
um,

144
00:06:42,320 --> 00:06:45,590
it's the point is,
is unimaginable,

145
00:06:45,890 --> 00:06:47,870
uh,
in terms of how,

146
00:06:48,260 --> 00:06:52,490
uh,
how much change this could effect.

147
00:06:52,910 --> 00:06:55,970
And if you imagine the best case
scenario where this is under our

148
00:06:55,971 --> 00:06:56,900
control,
right,

149
00:06:56,901 --> 00:06:58,700
where there's no alignment problem,
where it's just,

150
00:06:58,701 --> 00:07:00,050
it doesn't,
this thing doesn't do anything that

151
00:07:00,051 --> 00:07:03,050
surprises us,
this thing we'll always take direction

152
00:07:03,051 --> 00:07:03,884
from us.

153
00:07:04,040 --> 00:07:06,260
It will never,
it will never develop interests of its

154
00:07:06,261 --> 00:07:07,430
own.
Right.

155
00:07:07,850 --> 00:07:09,740
Which is again the fear.
But let's,

156
00:07:09,741 --> 00:07:14,340
let's just say this is totally obedient,
is just an oracle and a genie route,

157
00:07:14,470 --> 00:07:15,100
you know,
in,

158
00:07:15,100 --> 00:07:17,270
in one.
And um,

159
00:07:17,720 --> 00:07:18,381
you know,
we say,

160
00:07:18,381 --> 00:07:19,790
you know,
cure Alzheimer's and it cures

161
00:07:19,791 --> 00:07:21,050
Alzheimer's.
You know,

162
00:07:21,051 --> 00:07:23,520
you solve the protein folding problem
and,

163
00:07:23,521 --> 00:07:26,390
and it just is,
it's just and running and to develop a

164
00:07:26,391 --> 00:07:30,230
perfect nanotechnology and it does that.
This is all,

165
00:07:30,231 --> 00:07:31,850
again,
going back to David Deutsch,

166
00:07:32,420 --> 00:07:36,650
there's no reason to think this isn't
possible because anything that's

167
00:07:36,651 --> 00:07:41,651
compatible with the laws of physics can
be done given the requisite knowledge,

168
00:07:42,080 --> 00:07:43,430
right?
So you just,

169
00:07:43,490 --> 00:07:47,240
you get enough intelligence,
as long as you're not violating the laws

170
00:07:47,241 --> 00:07:49,940
of physics,
you can do something in that space.

171
00:07:50,480 --> 00:07:52,220
Um,
so,

172
00:07:52,221 --> 00:07:54,350
but the problem is this is a winner take
all scenario.

173
00:07:54,350 --> 00:07:59,180
So facebook does it tomorrow and China
and Russia find out about it.

174
00:07:59,810 --> 00:08:04,700
They can't afford to wait around to see
whether the US decides to do something

175
00:08:04,730 --> 00:08:07,190
not entirely selfish with this,
right?

176
00:08:07,191 --> 00:08:09,590
Because they're,
the,

177
00:08:09,650 --> 00:08:12,680
their worst fears could be realized if
Donald Trump is president.

178
00:08:12,710 --> 00:08:16,130
What's Donald Trump going to do with a
perfect ai when he has already told the

179
00:08:16,131 --> 00:08:18,560
world that he hates Islam?
Right?

180
00:08:19,040 --> 00:08:20,780
Um,
it's a,

181
00:08:21,800 --> 00:08:22,820
it's a,
um,

182
00:08:23,270 --> 00:08:27,890
we would have to have a political and
economic system that allowed us to

183
00:08:27,891 --> 00:08:32,420
absorb this ultimate wealth saved.
We'll wealth producing technology.

184
00:08:32,510 --> 00:08:33,740
Um,
and,

185
00:08:34,250 --> 00:08:37,020
and again,
so this may all sound like pure Psi Phi

186
00:08:37,021 --> 00:08:39,050
craziness to people.
Um,

187
00:08:39,470 --> 00:08:41,420
I don't think there is any reason to
believe that it is,

188
00:08:41,421 --> 00:08:45,500
but walk way back from that edge of
craziness and just look at,

189
00:08:46,490 --> 00:08:47,810
um,
dumb ai,

190
00:08:47,811 --> 00:08:48,860
you know,
narrow ai,

191
00:08:48,861 --> 00:08:51,980
just self driving cars and automation
and,

192
00:08:52,440 --> 00:08:57,440
um,
intelligent algorithms that can do human

193
00:08:57,631 --> 00:09:02,460
level work
that is already poised to change our

194
00:09:02,461 --> 00:09:06,040
world massively and create massive
wealth inequality,

195
00:09:06,200 --> 00:09:06,640
which we have.

196
00:09:06,640 --> 00:09:08,760
We would have to figure out how to
spread this wealth.

197
00:09:09,050 --> 00:09:10,620
You know,
what do you do when you can automate,

198
00:09:11,560 --> 00:09:13,650
uh,
50% of,

199
00:09:13,850 --> 00:09:17,130
of human labor?
Were you paying attention to the,

200
00:09:17,160 --> 00:09:19,620
uh,
artificial intelligence go match?

201
00:09:19,621 --> 00:09:20,520
Yeah.
Yeah.

202
00:09:20,960 --> 00:09:21,980
Uh,
no,

203
00:09:22,120 --> 00:09:24,450
I don't actually play goes,
so I wasn't paying that kind of

204
00:09:24,451 --> 00:09:27,210
attention to it,
but I'm aware of what happened there and

205
00:09:27,211 --> 00:09:28,260
you know,
the rules of go,

206
00:09:29,190 --> 00:09:30,450
um,
not,

207
00:09:30,451 --> 00:09:31,670
not so that I know,
actually I don't,

208
00:09:31,980 --> 00:09:32,860
I don't,
I don't play it a,

209
00:09:32,861 --> 00:09:33,450
no,
I don't,

210
00:09:33,450 --> 00:09:35,310
I don't even know if I know vaguely how
you,

211
00:09:35,340 --> 00:09:36,220
how you,
um,

212
00:09:36,480 --> 00:09:39,270
how it looks when a game is played,
but I don't supposed to be very

213
00:09:39,271 --> 00:09:41,700
complicated.
The more complicated and more

214
00:09:41,701 --> 00:09:43,800
possibilities than chess.
Yeah.

215
00:09:44,190 --> 00:09:49,190
And that's why it took 20 years longer
for a computer to be the best player in

216
00:09:49,861 --> 00:09:51,300
the world.
Um,

217
00:09:51,330 --> 00:09:52,650
it's,
it is.

218
00:09:52,710 --> 00:09:55,860
Um,
did you see how the computer did it too?

219
00:09:56,930 --> 00:09:57,480
Well,
I didn't,

220
00:09:57,480 --> 00:09:58,940
I know,
I mean,

221
00:09:58,960 --> 00:10:01,670
this is to the company that did it is,
um,

222
00:10:02,220 --> 00:10:04,920
deep mind,
which was acquired by Google and they're

223
00:10:05,040 --> 00:10:09,000
at the cutting edge of ai research and
yeah.

224
00:10:09,001 --> 00:10:09,834
Well,
it's,

225
00:10:10,710 --> 00:10:14,940
the cartoons are unfortunately not so
far from what is possible.

226
00:10:14,941 --> 00:10:16,410
But,
um,

227
00:10:17,820 --> 00:10:18,653
the,
uh,

228
00:10:18,960 --> 00:10:20,040
yeah,
I mean there's,

229
00:10:21,120 --> 00:10:21,953
again,
this is no,

230
00:10:21,960 --> 00:10:25,200
this is not general intelligence.
Like we're talking about something like

231
00:10:25,410 --> 00:10:28,290
these are not machines that can even
play tic Tac toe right now.

232
00:10:28,291 --> 00:10:31,320
There's some,
there there've been some moves away from

233
00:10:31,321 --> 00:10:36,321
this or like deep mind has trained an
algorithm to play all of the Atari Games

234
00:10:37,681 --> 00:10:38,100
like,
oh,

235
00:10:38,100 --> 00:10:40,440
from 1980 or whenever.
Um,

236
00:10:40,740 --> 00:10:44,490
and it is very quickly became superhuman
on most of them.

237
00:10:44,491 --> 00:10:44,881
I think.
I,

238
00:10:44,881 --> 00:10:46,650
I don't think it's superhuman and all of
them yet,

239
00:10:46,651 --> 00:10:50,820
but it could play in a space invaders
and all these and breakout and all these

240
00:10:50,821 --> 00:10:51,960
games that are,
are,

241
00:10:51,961 --> 00:10:53,450
um,
uh,

242
00:10:54,370 --> 00:10:57,720
uh,
to highly unlike one another.

243
00:10:58,140 --> 00:11:03,060
And it's the same algorithm becoming
expert in superhuman and all of them.

244
00:11:03,061 --> 00:11:05,700
And that's,
that's a new paradigm and it's using a

245
00:11:05,701 --> 00:11:08,910
technique called deep learning for that.
Um,

246
00:11:09,690 --> 00:11:12,450
and that's,
and that's been very exciting and I will

247
00:11:12,451 --> 00:11:14,330
be incredibly useful.
You know,

248
00:11:14,340 --> 00:11:15,031
this is,
I mean,

249
00:11:15,031 --> 00:11:16,230
the other,
the flip side of all of this,

250
00:11:16,231 --> 00:11:19,020
I know that everything I tend to say on
this sound scary,

251
00:11:19,021 --> 00:11:21,210
but this is all like,
I mean,

252
00:11:21,670 --> 00:11:25,080
the next scariest thing is not to do any
of this stuff.

253
00:11:25,110 --> 00:11:26,850
It's like we,
we want intelligence,

254
00:11:26,851 --> 00:11:30,390
we want automation,
we want to figure out how to solve

255
00:11:30,391 --> 00:11:32,910
problems that we can't get soft.
So like intelligence is the best thing

256
00:11:32,911 --> 00:11:34,440
we've got.
So we want more of it.

257
00:11:34,990 --> 00:11:37,140
Uh,
but we have to have a system where,

258
00:11:37,510 --> 00:11:39,570
I mean,
it's scary that we have a system where

259
00:11:39,571 --> 00:11:44,571
if you gave the best possible version of
it to one research lab or to one

260
00:11:45,421 --> 00:11:49,800
government,
it's not obvious that that wouldn't

261
00:11:49,801 --> 00:11:51,900
destroy humanity.
Right.

262
00:11:51,990 --> 00:11:55,390
That wouldn't lead massive dislocations
where you'd have,

263
00:11:55,391 --> 00:11:57,520
you know,
some trillionaire who's trumpeting his

264
00:11:57,521 --> 00:11:59,890
new device and,
and just,

265
00:11:59,891 --> 00:12:02,380
you know,
50% unemployment in the u s you know,

266
00:12:02,381 --> 00:12:03,550
oh,
in a month.

267
00:12:03,551 --> 00:12:03,881
Right?
I mean,

268
00:12:03,881 --> 00:12:04,190
like,
he,

269
00:12:04,190 --> 00:12:06,520
like,
it's not obvious how we would absorb

270
00:12:07,090 --> 00:12:09,100
this level of,
of progress.

271
00:12:09,610 --> 00:12:10,550
Um,
and we,

272
00:12:10,560 --> 00:12:11,830
we,
we definitely have to,

273
00:12:12,340 --> 00:12:14,410
to figure out how to do it.
I know,

274
00:12:14,411 --> 00:12:16,450
of course we can't assume the best case
scenario.

275
00:12:16,451 --> 00:12:17,830
Right?
That's the best case scenario.

276
00:12:18,600 --> 00:12:21,210
I think there's a few people that put it
the way you put it,

277
00:12:21,600 --> 00:12:24,480
that terrify the shit out of people.
Right.

278
00:12:24,510 --> 00:12:29,510
And everyone else seems to have this
rosy vision of increased longevity and

279
00:12:29,701 --> 00:12:34,701
automated everything and everything
fixed and easy to get to work and

280
00:12:35,370 --> 00:12:38,310
medical procedures would be easier.
Or they're going to know how to do it,

281
00:12:38,311 --> 00:12:41,700
but everybody looks at it like,
we are always going to be here,

282
00:12:41,790 --> 00:12:43,470
but are we obsolete?
I mean,

283
00:12:43,471 --> 00:12:48,471
is this idea of a living thing that's
creative and wrapped up in emotions and

284
00:12:48,631 --> 00:12:53,631
lust and desires and jealousy and all
the pettiness that we celebrated all the

285
00:12:53,971 --> 00:12:55,110
time.
We still see it.

286
00:12:55,500 --> 00:12:57,090
It's not getting any better.
Right.

287
00:12:57,360 --> 00:12:59,430
If,
if we obsolete,

288
00:12:59,670 --> 00:13:01,740
I mean,
what if this thing comes along and says,

289
00:13:01,741 --> 00:13:02,990
listen,
there's a way to do it.

290
00:13:02,991 --> 00:13:06,450
You're going to abandon all that stupid
shit and you can abandon all that makes

291
00:13:06,451 --> 00:13:08,760
you all the stuff that makes you fun to
be around.

292
00:13:08,790 --> 00:13:10,170
Yeah.
And also fucks with,

293
00:13:10,171 --> 00:13:12,570
you could live three times as long
without that stuff.

294
00:13:12,970 --> 00:13:13,550
Oh,
well,

295
00:13:13,550 --> 00:13:17,520
I think it,
it would in the best case,

296
00:13:17,521 --> 00:13:20,970
would usher in Ei.
Ei,

297
00:13:22,960 --> 00:13:23,660
yeah.

298
00:13:23,660 --> 00:13:27,800
The possibility of,
of kind of fundamentally creative life

299
00:13:27,950 --> 00:13:31,160
where on the order of something like the
Matrix,

300
00:13:31,161 --> 00:13:36,161
whether it's in the matrix or it's just
in the world that has been made as

301
00:13:38,060 --> 00:13:40,040
beautiful as,
as possible,

302
00:13:40,670 --> 00:13:42,590
um,
based on

303
00:13:44,250 --> 00:13:47,700
what would functionally be an unlimited
resource of intelligence.

304
00:13:47,850 --> 00:13:52,850
Let me just say,
it's just like for there to be a,

305
00:13:53,970 --> 00:13:58,970
an ability to solve problems of a sword
that we can't currently imagine.

306
00:13:59,101 --> 00:13:59,491
I mean,
it's just,

307
00:13:59,491 --> 00:14:01,950
it really is like a place on the map
that you can't,

308
00:14:02,520 --> 00:14:04,680
you can't,
and you can indicate it's over there.

309
00:14:04,681 --> 00:14:06,420
You know,
it was like a blank spot on the map.

310
00:14:06,690 --> 00:14:08,460
This is why it's called the singularity.
Right?

311
00:14:08,461 --> 00:14:09,420
It's like,
this is it.

312
00:14:09,421 --> 00:14:10,980
This is a,
uh,

313
00:14:10,981 --> 00:14:12,120
it was,
it was John von Neumann,

314
00:14:12,121 --> 00:14:12,954
the,
um,

315
00:14:13,400 --> 00:14:15,860
the,
the inventor of game theory who a

316
00:14:16,200 --> 00:14:18,400
mathematician who,
um,

317
00:14:19,080 --> 00:14:22,050
uh,
is one along with Alan Turing and a

318
00:14:22,051 --> 00:14:25,410
couple of other people's was really
responsible for the computer revolution.

319
00:14:25,770 --> 00:14:27,450
He was the first person to use this
term,

320
00:14:27,510 --> 00:14:28,890
singularity,
uh,

321
00:14:28,990 --> 00:14:30,450
to describe just this,
that,

322
00:14:30,451 --> 00:14:34,860
that there's a speeding up of,
um,

323
00:14:36,090 --> 00:14:41,090
information processing technology
and April a cultural reliance upon it,

324
00:14:42,410 --> 00:14:45,120
uh,
beyond which we can't actually foresee

325
00:14:45,121 --> 00:14:48,020
the level of change that can come over
our society.

326
00:14:48,020 --> 00:14:48,853
It's like,
you know,

327
00:14:48,900 --> 00:14:50,970
an event horizon pass,
which we can't.

328
00:14:51,740 --> 00:14:54,170
Um,
and uh,

329
00:14:54,410 --> 00:14:58,610
this certainly becomes true when you
talk about these intelligence systems

330
00:14:58,640 --> 00:15:01,100
being able to make changes to
themselves.

331
00:15:01,400 --> 00:15:04,010
And again,
we're talking mostly software is not,

332
00:15:04,011 --> 00:15:06,240
I'm not imagining,
um,

333
00:15:06,770 --> 00:15:08,390
I mean the,
the most important breakthroughs

334
00:15:08,740 --> 00:15:11,450
certainly at the level of,
of better software.

335
00:15:11,451 --> 00:15:12,740
I mean the,
is we have,

336
00:15:13,280 --> 00:15:18,140
in terms of the computing power that if
the physical hardware on earth,

337
00:15:18,620 --> 00:15:21,440
it's not,
that's not what's limiting our ai at the

338
00:15:21,441 --> 00:15:22,790
moment.
It's not like we need more,

339
00:15:23,570 --> 00:15:24,403
more,
uh,

340
00:15:25,160 --> 00:15:26,780
hardware.
Um,

341
00:15:26,900 --> 00:15:30,810
but we will get more hardware to up to
the limits of physics and it'll,

342
00:15:30,811 --> 00:15:33,650
it get smaller and smaller as it has a,
and you know,

343
00:15:33,651 --> 00:15:37,160
if quantum computing becomes possible,
um,

344
00:15:37,640 --> 00:15:39,410
or practical,
um,

345
00:15:39,470 --> 00:15:40,880
that will,
uh,

346
00:15:41,630 --> 00:15:43,310
actually David Deutsch is,
is,

347
00:15:43,311 --> 00:15:45,380
um,
the physicist I mentioned is one of the

348
00:15:45,381 --> 00:15:48,440
fathers of the concept of quantum
computing.

349
00:15:48,940 --> 00:15:53,580
Um,
that will open up a whole nother area,

350
00:15:53,630 --> 00:15:56,330
you know,
extreme of computing power.

351
00:15:56,810 --> 00:15:59,750
That is,
I'm not at all analogous to the kinds

352
00:15:59,751 --> 00:16:00,430
of,
of,

353
00:16:00,430 --> 00:16:02,240
uh,
machines we have now.

354
00:16:02,690 --> 00:16:04,470
But,
um,

355
00:16:05,810 --> 00:16:10,810
it's just when you imagine
people don't,

356
00:16:11,360 --> 00:16:15,230
people seem to always want to,
I just had this conversation with Neil

357
00:16:15,231 --> 00:16:18,830
degrasse Tyson on my podcast.
He named dropper.

358
00:16:19,070 --> 00:16:19,700
Yeah.
I know.

359
00:16:19,700 --> 00:16:21,440
It was just,
I'm just keeping pookie people.

360
00:16:21,441 --> 00:16:23,560
I'm just,
I'm just attributing these ideas to him.

361
00:16:24,920 --> 00:16:26,240
Uh,
he's not a,

362
00:16:26,241 --> 00:16:27,680
he doesn't take this line at all.
He's not at all,

363
00:16:27,681 --> 00:16:28,910
he thinks it's all bullshit.
Right.

364
00:16:29,120 --> 00:16:31,460
He's not at all worried about Ai.
Right.

365
00:16:31,461 --> 00:16:33,020
What does he think?
He thinks that,

366
00:16:33,220 --> 00:16:34,010
you know,
we just,

367
00:16:34,010 --> 00:16:38,630
we just use,
he's drawing an analogy from how we,

368
00:16:39,100 --> 00:16:41,660
you currently use computers that they
just,

369
00:16:41,661 --> 00:16:44,510
they just keep helping us do what we
want to do.

370
00:16:44,510 --> 00:16:48,260
Like we decide what we want to do with
computers and we just add them to our

371
00:16:48,261 --> 00:16:52,600
process and that process becomes
automated and then we'll find new jobs

372
00:16:52,601 --> 00:16:53,540
somewhere else.
Like you didn't,

373
00:16:53,541 --> 00:16:57,410
you don't need a stenographer once you
have voice recognition technology and

374
00:16:57,910 --> 00:16:59,230
um,
uh,

375
00:16:59,240 --> 00:17:01,250
that's not a problem.
A stenographer we'll find something else

376
00:17:01,251 --> 00:17:03,320
to do.
And so the economic dislocations isn't

377
00:17:03,321 --> 00:17:08,321
that bad and um,
computers will just get better than they

378
00:17:08,391 --> 00:17:11,450
are and you know,
eventually Siri will actually work,

379
00:17:11,451 --> 00:17:12,284
you know,
and you'll,

380
00:17:12,750 --> 00:17:14,630
she'll answer your questions.
Well and you're not,

381
00:17:14,660 --> 00:17:17,690
it's not going to be a laugh line what
Siri said to you today.

382
00:17:18,080 --> 00:17:19,220
And,
um,

383
00:17:19,820 --> 00:17:24,320
then all of this,
we'll just proceed to make life better

384
00:17:24,650 --> 00:17:26,280
right now.
Um,

385
00:17:26,990 --> 00:17:30,480
none of that is imagining what it will
be like to make it,

386
00:17:30,490 --> 00:17:34,670
because it would be a certain point
where you'll have systems that are,

387
00:17:35,330 --> 00:17:36,163
you know,
it's like

388
00:17:37,850 --> 00:17:40,220
the Cha,
the best chess player on earth is now

389
00:17:40,340 --> 00:17:42,030
always going to be a computer,
right?

390
00:17:42,070 --> 00:17:42,980
It's never,
there's no,

391
00:17:43,010 --> 00:17:46,430
there's not going to be a human born
tomorrow that's going to be better than

392
00:17:46,431 --> 00:17:47,550
the best computer.
I mean,

393
00:17:47,551 --> 00:17:51,150
it's like,
it's already like we have superhuman

394
00:17:51,210 --> 00:17:55,410
chess players on earth.
Now imagine having computers that are

395
00:17:55,411 --> 00:17:58,060
superhuman at every,
uh,

396
00:17:58,200 --> 00:18:01,230
every task that is relevant,
every intellectual task,

397
00:18:01,231 --> 00:18:03,900
right?
So the best physicist is a computer.

398
00:18:04,050 --> 00:18:06,840
You know,
the best medical diagnostician is a

399
00:18:06,841 --> 00:18:07,800
computer.
The best,

400
00:18:08,650 --> 00:18:11,460
um,
prover of math theorems has a computer.

401
00:18:11,461 --> 00:18:13,290
The best engineer is a computer,
right?

402
00:18:13,291 --> 00:18:15,930
Then there's no,
there's no reason why we're not headed

403
00:18:15,931 --> 00:18:16,301
there.
I mean,

404
00:18:16,301 --> 00:18:19,650
it would be the only reason I could see
we're not headed there is it's something

405
00:18:19,651 --> 00:18:24,651
massively dislocating happens that
prevents us from continuing to improve

406
00:18:25,471 --> 00:18:27,800
our intelligent machines.
But if you just,

407
00:18:27,870 --> 00:18:31,260
the moment you admit that intelligence
is just a matter of information

408
00:18:31,261 --> 00:18:36,261
processing and you admit that we will
continue to improve our machines unless

409
00:18:36,721 --> 00:18:40,350
something heinous happens because this
is intelligence and automation are the

410
00:18:40,351 --> 00:18:42,480
most valuable things we have.

411
00:18:43,020 --> 00:18:45,180
Um,
at a certain point,

412
00:18:45,181 --> 00:18:47,790
whether you think it's in five years or
500 years,

413
00:18:48,330 --> 00:18:52,770
we're going to find ourselves in the
presence of super intelligent machines.

414
00:18:52,771 --> 00:18:57,771
And then at that point,
the best source of innovation for the

415
00:18:58,951 --> 00:19:03,951
next generation of software or hardware
or both will be the machines themselves.

416
00:19:04,860 --> 00:19:05,693
Right?
So then,

417
00:19:06,090 --> 00:19:08,400
so then you just have,
then that's where you get what,

418
00:19:08,670 --> 00:19:12,360
what was,
what the mathematician Ij good described

419
00:19:12,361 --> 00:19:16,080
as as the intelligence explosion,
which is just the process can take off

420
00:19:16,081 --> 00:19:17,690
on its own.
Um,

421
00:19:17,730 --> 00:19:20,010
and this is where the singularity
people,

422
00:19:20,770 --> 00:19:24,420
um,
either either are hopeful or worried.

423
00:19:24,830 --> 00:19:26,430
Um,
but you know,

424
00:19:26,431 --> 00:19:29,070
cause there's nothing,
there's no guarantee that this process

425
00:19:29,610 --> 00:19:33,270
will be,
remain aligned with our interests.

426
00:19:33,300 --> 00:19:35,530
And,
and every person who I meet,

427
00:19:35,580 --> 00:19:36,001
even,
you know,

428
00:19:36,001 --> 00:19:38,390
very smart people like Neil,
um,

429
00:19:39,060 --> 00:19:43,650
who says they're not worried about this.
When you actually drill down on why

430
00:19:43,651 --> 00:19:46,740
they're not worried,
you find that they're actually not

431
00:19:46,741 --> 00:19:51,741
imagining machines making changes to
their own source code.

432
00:19:52,840 --> 00:19:57,500
Um,
and they're not an or they're,

433
00:19:57,501 --> 00:20:01,470
they,
they simply believe that this is so far

434
00:20:01,471 --> 00:20:04,200
away that we don't have to worry about
it now.

435
00:20:04,590 --> 00:20:06,690
Right.
And that's actually a non sequitur.

436
00:20:06,691 --> 00:20:09,690
I mean,
to say that this is far away is not

437
00:20:09,691 --> 00:20:13,400
actually grappling with,
it's not an argument this isn't going to

438
00:20:13,410 --> 00:20:14,370
happen.
And,

439
00:20:15,000 --> 00:20:17,010
um,
and it's based on what to,

440
00:20:17,430 --> 00:20:18,570
and it's,
and it's based on,

441
00:20:18,960 --> 00:20:20,400
first of all,
there's no,

442
00:20:21,060 --> 00:20:24,830
there's no reason to believe Jamie want
to find out where it is.

443
00:20:26,340 --> 00:20:29,070
Um,
there's no,

444
00:20:29,190 --> 00:20:31,950
I mean we don't know how long it will
take us to prepare for this.

445
00:20:32,010 --> 00:20:32,670
Right?
So like,

446
00:20:32,670 --> 00:20:33,570
like if,
if you were,

447
00:20:33,571 --> 00:20:38,130
if you knew this,
it was going to take 50 years for this

448
00:20:38,131 --> 00:20:39,150
to happen,
right.

449
00:20:40,050 --> 00:20:44,430
Is 50 years enough for us to prepare
politically and economically to deal

450
00:20:44,431 --> 00:20:49,000
with the ramifications of this and,
and to do it and to add to say nothing

451
00:20:49,001 --> 00:20:53,200
of actually building the ai safely in a
way that's aligned with our interests.

452
00:20:53,500 --> 00:20:56,440
I don't know.
I mean 50 so 50 years is,

453
00:20:56,610 --> 00:20:59,710
it's like we've had the iphone for what,
10 years?

454
00:21:00,580 --> 00:21:01,500
Nine years?
I'm it,

455
00:21:01,620 --> 00:21:03,880
it's like 50 years,
not a lot of time,

456
00:21:04,230 --> 00:21:04,661
right.
To,

457
00:21:04,661 --> 00:21:05,890
to deal,
to deal with this.

458
00:21:05,920 --> 00:21:10,920
And um,
this has no reason to think it's,

459
00:21:11,230 --> 00:21:14,890
it's that far away.
If we keep making progress,

460
00:21:14,891 --> 00:21:15,850
I mean it's,
it's not,

461
00:21:15,940 --> 00:21:17,890
it will be amazing if it were 500 years
away.

462
00:21:18,250 --> 00:21:18,641
I mean,
that,

463
00:21:18,641 --> 00:21:19,970
that seems like it's,
it's,

464
00:21:20,590 --> 00:21:25,590
it's more likely from what I am in the
sense I get from the people who are

465
00:21:26,200 --> 00:21:31,200
doing this work,
it's far more likely to be 50 years than

466
00:21:31,871 --> 00:21:33,400
500 years.
Like,

467
00:21:33,940 --> 00:21:34,990
you know,
um,

468
00:21:36,100 --> 00:21:39,640
uh,
I mean the p at the people who think

469
00:21:39,641 --> 00:21:41,780
this is a long,
long way off or,

470
00:21:41,781 --> 00:21:45,610
I mean they're saying,
you know,

471
00:21:45,700 --> 00:21:49,540
50 to 100 years,
no one says 500 years.

472
00:21:50,200 --> 00:21:51,000
No,
no.

473
00:21:51,000 --> 00:21:53,050
As far as I know,
no one who was actually close to this

474
00:21:53,051 --> 00:21:57,100
work and some people think it could be
in five years.

475
00:21:57,190 --> 00:21:58,690
Right.
I mean the people who are,

476
00:21:58,710 --> 00:22:01,090
you know,
like the deep mind people who are very

477
00:22:01,091 --> 00:22:03,340
close to this or are the sorts of people
who say,

478
00:22:03,730 --> 00:22:06,370
cause the people,
the people who are close to his work are

479
00:22:06,371 --> 00:22:09,550
astonished by what's happened in the
last 10 years.

480
00:22:10,030 --> 00:22:13,350
We went from a place of,
you know,

481
00:22:13,360 --> 00:22:15,550
very little progress too,
you know,

482
00:22:15,700 --> 00:22:19,240
wow,
this is all of a sudden really,

483
00:22:19,241 --> 00:22:22,210
really interesting and powerful.
And um,

484
00:22:23,880 --> 00:22:26,230
and again,
progress is compounding in a way that's

485
00:22:26,231 --> 00:22:30,250
counter intuitive.
People systematically overestimate how

486
00:22:30,251 --> 00:22:34,030
much change can happen in a year and
underestimate how much change can happen

487
00:22:34,031 --> 00:22:35,150
in 10 years.
And I,

488
00:22:35,170 --> 00:22:37,300
you know,
as far as estimating how much change can

489
00:22:37,301 --> 00:22:40,780
happen in 50 or a hundred years,
I don't know that anyone is good at

490
00:22:40,781 --> 00:22:41,614
that.

491
00:22:43,540 --> 00:22:48,540
How could you be with giant leaps come
giant exponential leaps off those leaps.

492
00:22:48,851 --> 00:22:51,670
And it's,
it's almost impossible for us to really

493
00:22:51,671 --> 00:22:54,310
predict what we're going to be looking
at 50 years from now.

494
00:22:54,700 --> 00:22:57,880
But I don't,
I don't know what they're going to think

495
00:22:57,881 --> 00:23:00,910
about us.
That's what's most bizarre about it as

496
00:23:00,911 --> 00:23:03,310
well.
We really might be obsolete if we look

497
00:23:03,311 --> 00:23:07,270
at how ridiculous we are.
Look at this political campaign,

498
00:23:07,271 --> 00:23:09,160
look at what we pay attention to in the
news.

499
00:23:09,161 --> 00:23:12,190
Look at the things we really focus on
where our strange,

500
00:23:12,191 --> 00:23:16,300
ridiculous animal and w y if we look
back on,

501
00:23:16,301 --> 00:23:18,550
you know,
some strange dinosaur that at a weird

502
00:23:18,551 --> 00:23:20,560
neck,
why should that fucking thing make it,

503
00:23:20,970 --> 00:23:22,240
you know,
why should we make it?

504
00:23:22,380 --> 00:23:24,940
We,
we might be here to make that thing.

505
00:23:25,360 --> 00:23:28,690
And that thing takes over from here with
no emotions,

506
00:23:28,691 --> 00:23:30,040
no lust,
no greed,

507
00:23:30,041 --> 00:23:34,450
and just purely existing electronically
and for what reason?

508
00:23:34,650 --> 00:23:34,991
Well,
that,

509
00:23:34,991 --> 00:23:38,940
that's a little scary there.
There are computer scientists who when

510
00:23:38,941 --> 00:23:42,630
you talk about why they're not worried
or talk to them about why they're not

511
00:23:42,631 --> 00:23:47,631
worried,
they just swallow this pill without any

512
00:23:48,141 --> 00:23:52,520
qualm that we're going to make.
The thing that is far more powerful and

513
00:23:52,521 --> 00:23:57,500
beautiful and important than we are.
And it doesn't matter what happens to

514
00:23:57,501 --> 00:23:58,220
us.
I mean,

515
00:23:58,220 --> 00:24:02,270
that was our role.
Our role was to build these mechanical

516
00:24:02,271 --> 00:24:07,130
gods and,
and it's fine if they squash us.

517
00:24:07,550 --> 00:24:10,550
Um,
and I've literally heard a,

518
00:24:10,610 --> 00:24:11,630
a,
people say,

519
00:24:11,631 --> 00:24:13,320
I've heard someone give a talk.
I mean,

520
00:24:13,520 --> 00:24:15,780
that's what woke me up to,
oh,

521
00:24:16,670 --> 00:24:20,600
how interesting in this area is.
I went to this conference in San Juan

522
00:24:20,601 --> 00:24:22,250
about a year ago.
Um,

523
00:24:22,940 --> 00:24:24,260
and there were,
uh,

524
00:24:24,760 --> 00:24:26,510
you know,
like the people from deep mind were

525
00:24:26,520 --> 00:24:29,930
there and there were the people who were
very close to this work were there.

526
00:24:30,380 --> 00:24:31,890
And,
um,

527
00:24:32,260 --> 00:24:37,260
I mean to hear some of the reasons why
you shouldn't be worried from people who

528
00:24:38,330 --> 00:24:40,970
were interested in,
in calming the fear is so they could get

529
00:24:40,971 --> 00:24:43,330
on with doing their very important work.
Um,

530
00:24:43,820 --> 00:24:48,820
it was amazing because they were highly
uncompelling reasons not to be worried.

531
00:24:51,180 --> 00:24:53,300
It's just,
um,

532
00:24:54,470 --> 00:24:56,060
so,
so they had a,

533
00:24:56,420 --> 00:24:58,670
they had a desire to be compelled.
They're not,

534
00:24:58,700 --> 00:25:01,940
they're not well,
not well known that people,

535
00:25:01,970 --> 00:25:05,990
people want to do this.
There's a deep assumption in many of

536
00:25:05,991 --> 00:25:09,920
these people that we can figure it out
as we go along.

537
00:25:10,130 --> 00:25:10,970
Right?
It's like,

538
00:25:11,380 --> 00:25:11,991
you know,
it's just like,

539
00:25:11,991 --> 00:25:13,670
we're going to,
we're just going to get,

540
00:25:13,920 --> 00:25:14,900
going to get closer.
We're foot.

541
00:25:14,901 --> 00:25:17,540
We're far enough away now.
Even five year,

542
00:25:17,541 --> 00:25:18,890
even if it's five years,
five years,

543
00:25:18,920 --> 00:25:19,310
we'll,
we'll,

544
00:25:19,310 --> 00:25:20,570
we'll get there.
Once we get closer,

545
00:25:20,571 --> 00:25:25,340
once we get something a little scary,
then we'll pull the brakes and talk

546
00:25:25,341 --> 00:25:26,174
about it.

547
00:25:26,240 --> 00:25:30,200
But the problem is they are a sent,
everyone is essentially in a race

548
00:25:30,201 --> 00:25:31,790
condition by default.
And you have,

549
00:25:31,820 --> 00:25:34,250
you know,
Google is racing against facebook and

550
00:25:34,251 --> 00:25:37,100
the u s is racing against China and
every,

551
00:25:37,640 --> 00:25:39,740
every group is racing against every
other group.

552
00:25:40,250 --> 00:25:43,010
Um,
however you want to conceive of groups.

553
00:25:43,040 --> 00:25:44,150
This,
this is a,

554
00:25:44,480 --> 00:25:48,530
to be the first one to be the first one
with

555
00:25:50,690 --> 00:25:55,040
incredibly powerful,
narrow ai is to be the next,

556
00:25:55,220 --> 00:25:58,250
you know,
multibillion dollar company,

557
00:25:58,251 --> 00:26:01,070
right?
So everyone's trying to get there and

558
00:26:01,540 --> 00:26:03,230
uh,
if they suddenly get there and sort of

559
00:26:03,231 --> 00:26:05,390
overshoot a little bit and now they've
got something like,

560
00:26:05,391 --> 00:26:08,000
you know,
general intelligence or something close,

561
00:26:08,540 --> 00:26:10,660
um,
what we're relying on every,

562
00:26:10,800 --> 00:26:13,340
and they know everyone else's attempting
to do this,

563
00:26:13,370 --> 00:26:14,690
right?
Um,

564
00:26:15,430 --> 00:26:20,330
W we don't have a system set up where
everyone can pull the breaks together

565
00:26:20,331 --> 00:26:20,960
and say,
listen,

566
00:26:20,960 --> 00:26:22,700
we've got to stop racing here.

567
00:26:22,700 --> 00:26:25,490
We have to share everything.
We have to share the wealth,

568
00:26:25,491 --> 00:26:28,100
we have to share the information,
we have to um,

569
00:26:28,730 --> 00:26:31,790
this truly has to be open source in
every conceivable way.

570
00:26:31,791 --> 00:26:35,480
And um,
we have to diffuse this winner take all

571
00:26:36,320 --> 00:26:38,150
dynamic.
Um,

572
00:26:39,080 --> 00:26:40,460
you know,
I think we need something like a

573
00:26:40,461 --> 00:26:43,230
Manhattan project to figure out how to
do that.

574
00:26:43,320 --> 00:26:45,060
You know,
not if not to figure out how to build

575
00:26:45,061 --> 00:26:48,030
the AI,
but to figure out how to build it in a

576
00:26:48,031 --> 00:26:51,450
way that does not create an arms race
that does not create,

577
00:26:51,980 --> 00:26:55,080
um,
an incentive to build unsafe ai,

578
00:26:55,081 --> 00:26:59,160
which is almost certainly going to be
easier than building safe ai and just to

579
00:26:59,161 --> 00:27:00,980
work out all of these issues because
it's,

580
00:27:00,981 --> 00:27:04,860
it's not because what I think we are,
we're going to build this by default.

581
00:27:04,861 --> 00:27:09,300
We're just going to keep building more
and more intelligent machines and this

582
00:27:09,301 --> 00:27:14,100
is going to be done in by everyone who
can,

583
00:27:14,280 --> 00:27:16,890
can do it with each generation.

584
00:27:16,890 --> 00:27:18,300
If we were even talking about
generations,

585
00:27:18,301 --> 00:27:21,930
it's going to be,
it will have the tools made by the prior

586
00:27:21,931 --> 00:27:26,070
generation that are more powerful than
anyone imagined a hundred years ago.

587
00:27:26,071 --> 00:27:28,350
And it just,
it gets going to keep going like that.

588
00:27:28,980 --> 00:27:33,780
Did anybody actually make that quote
about giving birth to the mechanical

589
00:27:33,781 --> 00:27:34,611
gods?
I,

590
00:27:34,611 --> 00:27:35,444
no,
that was just made.

591
00:27:36,260 --> 00:27:37,110
Yeah,
but it was,

592
00:27:37,111 --> 00:27:39,930
there was a scientist that actually was
thinking and saying that,

593
00:27:39,960 --> 00:27:42,990
but that was,
that was the content of what he was

594
00:27:43,020 --> 00:27:43,771
saying.
He's like,

595
00:27:43,771 --> 00:27:48,600
what are we going to build the next
species that is far more important than

596
00:27:48,601 --> 00:27:51,240
we are?
And that's a good thing.

597
00:27:51,290 --> 00:27:51,720
And it,
well,

598
00:27:51,720 --> 00:27:54,210
and actually I can go there with him.
I mean,

599
00:27:54,260 --> 00:27:55,620
it actually,
the only,

600
00:27:55,621 --> 00:28:00,621
the only
caveat here is that unless they're not

601
00:28:00,631 --> 00:28:01,530
conscious,
right?

602
00:28:01,531 --> 00:28:04,920
Like if you,
the true horror for me is that we can

603
00:28:04,921 --> 00:28:07,170
build things more intelligent than we
are,

604
00:28:07,620 --> 00:28:08,880
more powerful than we are.

605
00:28:09,640 --> 00:28:13,980
Uh,
and that can squash us and they might

606
00:28:13,981 --> 00:28:15,630
not,
they might be unconscious,

607
00:28:15,680 --> 00:28:17,460
right?
There might be nothing like the universe

608
00:28:17,461 --> 00:28:19,170
could go dark if they squash us,
right?

609
00:28:19,171 --> 00:28:21,060
Or,
or at least our corner of the universe

610
00:28:21,061 --> 00:28:22,110
could go dark.
Um,

611
00:28:22,500 --> 00:28:24,360
and yet these things will be immensely
powerful.

612
00:28:24,840 --> 00:28:27,150
Um,
so if,

613
00:28:27,300 --> 00:28:28,111
and this is just,
you know,

614
00:28:28,111 --> 00:28:30,210
the jury's out on this,
but if there's nothing about

615
00:28:30,211 --> 00:28:34,860
intelligence scaling that demands that
consciousness come along for the ride,

616
00:28:35,580 --> 00:28:38,160
um,
then it's possible that,

617
00:28:38,161 --> 00:28:39,690
I mean,
nobody thinks our machines are,

618
00:28:39,691 --> 00:28:41,640
you know,
very few people would think are machines

619
00:28:41,790 --> 00:28:43,860
that are intelligent or conscious.
Right?

620
00:28:44,310 --> 00:28:46,710
So at what point does consciousness come
online?

621
00:28:47,240 --> 00:28:49,720
Um,
maybe it's possible to build super

622
00:28:49,721 --> 00:28:51,870
intelligence that's unconscious,
you know,

623
00:28:51,900 --> 00:28:54,900
super powerful,
does everything better than we do.

624
00:28:54,930 --> 00:28:57,090
You know,
it'll recognize your emotion better

625
00:28:57,091 --> 00:28:59,250
than,
than another person can,

626
00:28:59,640 --> 00:29:02,270
but then the lights aren't on that.

627
00:29:02,460 --> 00:29:04,830
That's,
that's also I think possible.

628
00:29:05,330 --> 00:29:07,420
But maybe it's not possible,
but that's,

629
00:29:07,421 --> 00:29:12,421
that's the worst case scenario because
in the ethical silver lining and

630
00:29:12,930 --> 00:29:13,780
speaking,
you know,

631
00:29:13,800 --> 00:29:17,850
outside of our self interest now,
but just from a bird's eye view,

632
00:29:18,380 --> 00:29:21,780
um,
the ethical silver lining to building

633
00:29:22,080 --> 00:29:24,330
these mechanical gods that are conscious
is that,

634
00:29:24,331 --> 00:29:24,841
yes,
okay.

635
00:29:24,841 --> 00:29:27,990
We've,
in fact if we have built something that

636
00:29:28,020 --> 00:29:33,020
is far wiser and has far more beautiful
experiences and deeper experiences of

637
00:29:33,511 --> 00:29:37,320
the universe and we could ever imagine
and there there's something that it's

638
00:29:37,321 --> 00:29:39,450
like to be that thing that's just,
you know,

639
00:29:39,480 --> 00:29:42,370
it is a,
has a kind of a God like uh,

640
00:29:42,400 --> 00:29:44,110
experience.
Um,

641
00:29:44,290 --> 00:29:47,020
well that would be a very good thing
then we will have built you,

642
00:29:47,080 --> 00:29:48,910
we will have built something that was,
you know,

643
00:29:48,940 --> 00:29:51,340
if you stand outside of our narrow self
interest,

644
00:29:52,120 --> 00:29:54,590
I can understand why he would say that
he,

645
00:29:54,591 --> 00:29:58,870
he was just assuming what was scary
about that particular talk cause he was

646
00:29:58,871 --> 00:30:03,871
assuming that consciousness comes along
for the ride here.

647
00:30:04,001 --> 00:30:05,890
And I don't know that that is a safe
assumption.

648
00:30:06,070 --> 00:30:09,790
Well and the really terrifying thing is
who,

649
00:30:10,610 --> 00:30:13,390
if,
if this is constantly improving itself

650
00:30:13,840 --> 00:30:18,840
and it's under the Beck and call of a
person then so it's either conscious or

651
00:30:19,360 --> 00:30:21,460
unconscious where it acts as itself,
right?

652
00:30:21,490 --> 00:30:24,160
It acts as an individual thinking unit.
Right?

653
00:30:24,161 --> 00:30:27,580
Or as a a a thing outside of it's aware,
right?

654
00:30:27,610 --> 00:30:31,600
Either it is or it isn't.
And if it isn't aware and some person

655
00:30:31,601 --> 00:30:34,930
can manipulate it,
like imagine if it's getting 10,000,

656
00:30:34,931 --> 00:30:37,000
how many,
how many thousands of years in a week

657
00:30:37,001 --> 00:30:38,180
did you say?
I'll go,

658
00:30:38,240 --> 00:30:39,900
well,
if it was just improvement,

659
00:30:39,910 --> 00:30:42,370
it was just a million times faster than
we are.

660
00:30:42,371 --> 00:30:45,850
It's 20,000 years,
20,000 years in a week and a weekend in

661
00:30:45,851 --> 00:30:47,710
a week.
So with every week,

662
00:30:47,711 --> 00:30:50,950
this thing constantly gets better at
even doing that.

663
00:30:50,980 --> 00:30:52,660
Right?
So it's reprogramming itself.

664
00:30:52,661 --> 00:30:54,370
So it's all exponential.

665
00:30:55,310 --> 00:30:58,250
Presumably it just,
it just imagine again that you could

666
00:30:58,251 --> 00:31:03,110
keep it in the most restricted case.
You could just keep it at our level,

667
00:31:03,630 --> 00:31:03,860
but

668
00:31:03,860 --> 00:31:05,240
just,
just faster,

669
00:31:05,330 --> 00:31:07,070
just a million times faster.
But if it did,

670
00:31:07,100 --> 00:31:09,710
all of these things,
if it kept going and kept every week was

671
00:31:09,770 --> 00:31:10,980
thousands of years,
right?

672
00:31:11,420 --> 00:31:13,070
We're going to control it.
A person.

673
00:31:13,170 --> 00:31:15,050
No,
think that's even more insane.

674
00:31:15,320 --> 00:31:18,440
Imagine being in dialogue with something
that had that,

675
00:31:18,630 --> 00:31:23,630
that lived the 20,000 years of human
progress in a week.

676
00:31:24,140 --> 00:31:25,500
And you come back,
you know,

677
00:31:25,580 --> 00:31:27,050
on Monday and say,
listen,

678
00:31:27,080 --> 00:31:27,920
um,
I,

679
00:31:27,921 --> 00:31:31,670
that thing I told you to do last Monday,
I want to change that up and this thing

680
00:31:31,671 --> 00:31:34,950
has made 20,000 years of progress.
Um,

681
00:31:35,090 --> 00:31:37,700
and if it's in a condition where it has
access,

682
00:31:37,701 --> 00:31:39,650
I mean,
so we're imagining this thing,

683
00:31:39,651 --> 00:31:40,910
you know,
in a box,

684
00:31:40,930 --> 00:31:44,180
you know,
air gapped from the Internet and it's

685
00:31:44,181 --> 00:31:45,800
got nothing.
He's got no way to get out.

686
00:31:45,830 --> 00:31:46,663
Right.
Uh,

687
00:31:47,090 --> 00:31:52,090
even that is an unstable situation.
But just imagine this emerging in some

688
00:31:52,161 --> 00:31:53,230
way online,
right?

689
00:31:53,270 --> 00:31:55,130
Already being out in the wild.
Right?

690
00:31:55,131 --> 00:31:57,020
So let's say it's in a financial market,
right?

691
00:31:57,590 --> 00:32:00,710
Um,
that's again,

692
00:32:00,711 --> 00:32:05,030
this is what worries me most about this
and what is also interesting is that our

693
00:32:05,060 --> 00:32:08,450
intuitions here,
I think the primary intuition that

694
00:32:08,451 --> 00:32:10,180
people have is no,
no,

695
00:32:10,181 --> 00:32:10,790
no,
that's just,

696
00:32:10,790 --> 00:32:13,070
that's just not possible.

697
00:32:13,100 --> 00:32:15,270
Or not at all likely.
But if,

698
00:32:15,280 --> 00:32:16,350
if you're going to fund,
if you,

699
00:32:16,351 --> 00:32:19,040
you're going to think it's impossible or
even unlikely,

700
00:32:19,700 --> 00:32:22,880
you have to find something wrong with
the claim.

701
00:32:23,120 --> 00:32:27,650
That intelligence is just a matter of
information processing.

702
00:32:28,330 --> 00:32:32,030
Um,
I don't know any scientific reason to

703
00:32:32,031 --> 00:32:34,490
doubt that claim at the moment.
Um,

704
00:32:34,940 --> 00:32:38,300
and uh,
very good reasons to believe that it's

705
00:32:38,301 --> 00:32:40,250
just undoubtable,
uh,

706
00:32:40,460 --> 00:32:45,460
and the,
and you have to doubt that we will

707
00:32:46,221 --> 00:32:50,090
continue to make progress in the design
of intelligent machines.

708
00:32:50,150 --> 00:32:54,500
And,
but once you then is then that all this

709
00:32:54,501 --> 00:32:56,000
left is just time,
right?

710
00:32:56,001 --> 00:32:56,540
If,
if,

711
00:32:56,540 --> 00:33:01,490
if intelligence is just information
processing and we were going to continue

712
00:33:01,491 --> 00:33:05,990
to build better and better information
processors,

713
00:33:06,530 --> 00:33:11,060
at a certain point we're going to build
something that is superhuman.

714
00:33:11,970 --> 00:33:16,970
Um,
and so whether it's in five years or 50,

715
00:33:17,310 --> 00:33:18,460
it's,
it's a huge,

716
00:33:18,550 --> 00:33:21,400
I mean it's,
it's the biggest change in human history

717
00:33:21,401 --> 00:33:22,390
I think we can imagine.

718
00:33:22,480 --> 00:33:24,050
Right?
Um,

719
00:33:24,520 --> 00:33:25,960
so,
uh,

720
00:33:26,500 --> 00:33:28,210
and then people,
I what I felt fine.

721
00:33:28,211 --> 00:33:32,860
I keep finding myself in the presence of
people who seem at least to my eye to be

722
00:33:32,861 --> 00:33:36,460
refusing to imagine it like late,
they're treating it like the y two k

723
00:33:36,550 --> 00:33:41,110
virus or whatever where it's just the y
two k bug where it just may or may not

724
00:33:41,111 --> 00:33:42,370
be an issue.
Right?

725
00:33:42,371 --> 00:33:43,900
Like it,
like it's a hypothetical,

726
00:33:44,080 --> 00:33:45,740
like May this is just,
we're going to get there and it's,

727
00:33:45,741 --> 00:33:48,550
it's just going to be,
it's either not going to happen or it's,

728
00:33:48,551 --> 00:33:49,810
it's,
it's going to be trivial.

729
00:33:49,811 --> 00:33:53,050
But how you don't,
if you don't have an argument for why

730
00:33:53,051 --> 00:33:55,970
this isn't going to happen,
uh,

731
00:33:56,410 --> 00:33:58,780
then you have to have then,
then you're left with,

732
00:33:58,810 --> 00:34:01,390
okay,
what's it gonna be like to have,

733
00:34:02,550 --> 00:34:07,550
uh,
systems that are better than we are at

734
00:34:07,930 --> 00:34:12,190
everything in the intellectual space.
Um,

735
00:34:14,050 --> 00:34:16,410
and
you know,

736
00:34:16,411 --> 00:34:18,990
what will happen if that suddenly
happens in one country and not in

737
00:34:18,991 --> 00:34:19,824
another,
right?

738
00:34:20,010 --> 00:34:20,843
It's,
um,

739
00:34:22,110 --> 00:34:22,943
it's,
uh,

740
00:34:23,610 --> 00:34:24,061
it's,
I mean,

741
00:34:24,061 --> 00:34:26,670
it has enormous implications,
but it just sounds like science fiction.

742
00:34:27,030 --> 00:34:27,280
Yup.

743
00:34:27,280 --> 00:34:31,600
I don't know what's scarier.
The idea that an artificial intelligence

744
00:34:31,601 --> 00:34:32,980
can emerge.
It's conscious,

745
00:34:32,981 --> 00:34:36,090
it's aware of itself and that acts to
present per,

746
00:34:36,091 --> 00:34:39,430
per protect itself or the idea that a
person,

747
00:34:40,390 --> 00:34:44,980
a regular person like of today could be
in control of essentially a god.

748
00:34:45,310 --> 00:34:47,500
Right?
Because if this thing continues to get

749
00:34:47,501 --> 00:34:50,890
smarter and smarter with every week and
more and more power and more and more

750
00:34:50,891 --> 00:34:53,290
potential,
more and more understanding,

751
00:34:53,291 --> 00:34:54,730
thousands of years,
I mean it's just,

752
00:34:55,720 --> 00:34:57,400
yeah,
this one person,

753
00:34:57,430 --> 00:34:59,980
uh,
per regular person controlling that is

754
00:34:59,981 --> 00:35:03,180
almost more terrifying than creating a
new life or,

755
00:35:03,210 --> 00:35:08,210
or any group of people who don't have
the total welfare of humanity as their

756
00:35:08,411 --> 00:35:10,270
central concern.
And so just imagine,

757
00:35:10,271 --> 00:35:10,721
I mean,
what would,

758
00:35:10,721 --> 00:35:12,730
what would China do with it now?
Right?

759
00:35:12,760 --> 00:35:14,950
What would we,
what would we do if we thought China,

760
00:35:15,400 --> 00:35:16,540
you know,
Baidu or whatever,

761
00:35:16,680 --> 00:35:19,800
or some Chinese company was on the verge
of this thing.

762
00:35:20,210 --> 00:35:22,650
Um,
what would it be rational for us to do?

763
00:35:22,800 --> 00:35:23,430
You know?
I mean,

764
00:35:23,430 --> 00:35:25,150
if North Korea had it,
it would be,

765
00:35:25,151 --> 00:35:28,230
it'd be rational to nuke them given what
they say about,

766
00:35:28,850 --> 00:35:30,670
you know,
their relationship with the rest of the

767
00:35:30,671 --> 00:35:32,520
world.
So it's,

768
00:35:32,521 --> 00:35:33,354
um,

769
00:35:34,000 --> 00:35:37,950
well that kind of power.
Would you say rational that of power is,

770
00:35:37,951 --> 00:35:41,670
it's so life changing.
It's so what paradigm shifting,

771
00:35:42,620 --> 00:35:43,650
right?
But if you to,

772
00:35:43,651 --> 00:35:47,060
to wind this back to what someone like
Neil degrasse Tyson would say,

773
00:35:47,061 --> 00:35:51,380
is that the only basis for fear is,
yeah,

774
00:35:51,410 --> 00:35:54,440
don't give your superintelligent ai to
the next Hitler,

775
00:35:54,620 --> 00:35:55,400
right?
That's,

776
00:35:55,400 --> 00:35:57,770
that's obviously bad.
But if we don't,

777
00:35:57,800 --> 00:36:01,970
if we're not idiots and we just use it,
well,

778
00:36:02,750 --> 00:36:05,240
we're fine.
And that I think is an intuition that is

779
00:36:05,241 --> 00:36:07,400
just,
that's just a failure to,

780
00:36:07,970 --> 00:36:12,970
to unpack what is entailed by,
again,

781
00:36:13,400 --> 00:36:15,230
something like an intelligence
explosion,

782
00:36:15,231 --> 00:36:19,850
a process that once,
once you're talking about something that

783
00:36:19,851 --> 00:36:24,740
is able to change itself and you have to
get it.

784
00:36:24,770 --> 00:36:27,090
So what would it be like to guarantee
level?

785
00:36:27,100 --> 00:36:28,100
Let's say we decide,
okay,

786
00:36:28,101 --> 00:36:30,920
we're just not going to build anything
that can make changes to his own source

787
00:36:30,921 --> 00:36:31,341
code.
You know,

788
00:36:31,341 --> 00:36:35,560
any change to it,
to software at a certain point is going

789
00:36:35,570 --> 00:36:37,310
to have to be run through a human brain.

790
00:36:37,770 --> 00:36:39,260
Um,
and we're going to have veto power.

791
00:36:39,261 --> 00:36:42,800
Well,
is every person working on ai going to

792
00:36:42,801 --> 00:36:44,400
abide by that rule?
It's like we,

793
00:36:44,420 --> 00:36:46,370
we've agreed not to clone humans,
right?

794
00:36:46,371 --> 00:36:48,650
But you know,
we're going to stand by that agreement

795
00:36:48,651 --> 00:36:50,180
for the,
in the rest of human history.

796
00:36:50,181 --> 00:36:52,210
And is,
is a,

797
00:36:52,280 --> 00:36:55,930
is our agreement binding on China or
Singapore or you know,

798
00:36:55,940 --> 00:36:58,130
any other country that might think
otherwise.

799
00:36:58,700 --> 00:37:00,950
It's just we have a,
it's a free for all and at a certain

800
00:37:00,951 --> 00:37:03,410
point we're going to be,
you know,

801
00:37:03,470 --> 00:37:06,170
close enough.
Everyone's going to be close enough to

802
00:37:06,171 --> 00:37:09,110
making the final breakthrough that,
um,

803
00:37:09,410 --> 00:37:12,010
unless we have some,
uh,

804
00:37:12,440 --> 00:37:16,670
agreement about how to proceed if
someone is going to get there first,

805
00:37:18,260 --> 00:37:21,590
that is a terrifying scenario of the
future.

806
00:37:22,540 --> 00:37:24,430
You know,
you cemented this last time you were

807
00:37:24,431 --> 00:37:27,940
here,
but not as extreme as this time.

808
00:37:27,941 --> 00:37:29,620
You seem to be accelerating the
rhetoric.

809
00:37:30,220 --> 00:37:31,053
Exactly.
Yes.

810
00:37:32,950 --> 00:37:34,710
You're going deep.
Yeah.

811
00:37:35,530 --> 00:37:36,430
Boy,
I hope you're wrong.

812
00:37:36,700 --> 00:37:39,140
I'm on team Neil degrasse Tyson,
right?

813
00:37:39,150 --> 00:37:39,983
This one,

814
00:37:41,040 --> 00:37:42,340
Neil.
Um,

815
00:37:42,480 --> 00:37:46,120
and also in defensive of the other side
too,

816
00:37:46,121 --> 00:37:46,831
I should say that,
you know,

817
00:37:46,831 --> 00:37:48,730
like,
so David Deutsch also thinks I'm wrong,

818
00:37:48,760 --> 00:37:53,760
but he thinks I'm wrong because we will
integrate ourselves with these machines.

819
00:37:54,401 --> 00:37:54,960
I mean,
so that we,

820
00:37:54,960 --> 00:37:57,460
this will be,
there'll be extensions of ourselves and

821
00:37:57,461 --> 00:37:59,620
they can't help but be aligned with us
because we will,

822
00:37:59,830 --> 00:38:02,620
we will be connected to them.
That seems to be the only way we could

823
00:38:02,621 --> 00:38:04,480
all get along.
We have to merge become one.

824
00:38:04,810 --> 00:38:08,230
But I just think there's no,
there's no deep reason why,

825
00:38:08,380 --> 00:38:10,330
even if we decided to do that,
right?

826
00:38:10,331 --> 00:38:11,920
Like in the u s or,
or,

827
00:38:11,980 --> 00:38:14,160
or in half the world,
um,

828
00:38:14,560 --> 00:38:15,311
one,
there's,

829
00:38:15,311 --> 00:38:18,040
I think there are reasons to worry that
even that could go haywire,

830
00:38:18,041 --> 00:38:23,041
but there's no guarantee that someone
else couldn't just build ai in a box.

831
00:38:23,291 --> 00:38:23,800
I mean,
if we,

832
00:38:23,800 --> 00:38:27,220
if we can build ai such that we can
merge our brains with it,

833
00:38:27,900 --> 00:38:30,670
um,
someone can also just build ai in a box.

834
00:38:30,910 --> 00:38:31,743
Right.
And,

835
00:38:31,900 --> 00:38:32,790
and that's,
uh,

836
00:38:33,850 --> 00:38:35,800
um,
and then then inherit all the other

837
00:38:35,801 --> 00:38:37,840
problems that people are saying.
We don't have to worry about.

838
00:38:37,960 --> 00:38:41,680
If it was a good coen brothers movie,
it would be invented in the middle of

839
00:38:41,681 --> 00:38:45,400
the presidency of Donald Trump.
And so then that's when ai would go

840
00:38:45,401 --> 00:38:47,890
live.
And then ai would have to challenge

841
00:38:47,891 --> 00:38:50,170
Donald Trump and they would have like an
insult context,

842
00:38:51,570 --> 00:38:56,570
but that that's when this thing becomes
so comically,

843
00:38:57,020 --> 00:38:58,660
uh,
terrifying where it's just,

844
00:38:59,200 --> 00:39:04,200
just imagine Donald Trump being in a
position to make the final decisions on

845
00:39:05,891 --> 00:39:10,540
topics like this for the country.
That is,

846
00:39:10,541 --> 00:39:13,270
acne is going to do this almost
certainly in the near term.

847
00:39:13,300 --> 00:39:16,390
It's like he should,
we have a Manhattan project on this

848
00:39:16,960 --> 00:39:18,130
point,
Mr President.

849
00:39:18,760 --> 00:39:20,920
Um,
you know,

850
00:39:21,640 --> 00:39:26,020
the idea that anything of a value could
be happening between his ears on this

851
00:39:26,021 --> 00:39:27,280
topic.
We're a hundred others.

852
00:39:27,280 --> 00:39:31,390
Like it I think is now really
inconceivable.

853
00:39:32,380 --> 00:39:36,040
And so what w what,
what price could we,

854
00:39:36,041 --> 00:39:38,830
might we pay for that kind of
inattention and,

855
00:39:39,690 --> 00:39:43,900
and you know,
self satisfied in attention to these

856
00:39:43,901 --> 00:39:45,220
kinds of issues?
Well,

857
00:39:45,221 --> 00:39:46,510
this,
this issue,

858
00:39:46,690 --> 00:39:50,200
if this is real and if this could go
live in 50 years,

859
00:39:50,260 --> 00:39:51,910
this is the issue.
Yeah.

860
00:39:52,150 --> 00:39:56,020
Unless we fuck ourselves up beyond
repair before then and shut the power

861
00:39:56,021 --> 00:39:58,480
off if it keeps going.
Yeah.

862
00:39:58,481 --> 00:39:59,440
No,
I think it is.

863
00:39:59,470 --> 00:40:01,590
I think it is the issue,
but unfortunately it's the,

864
00:40:01,640 --> 00:40:04,230
it's the issue that doesn't,
it sounds like a goof.

865
00:40:04,240 --> 00:40:05,380
Yeah,
it does just sound,

866
00:40:05,800 --> 00:40:08,140
you sound like a crack pot even worrying
about this issue.

867
00:40:08,260 --> 00:40:11,310
It sounds completely ridiculous,
but that might be what's how it's

868
00:40:11,311 --> 00:40:12,730
sneaking in.
Yeah.

869
00:40:13,300 --> 00:40:14,290
Yeah.
I mean it's just,

870
00:40:14,320 --> 00:40:17,110
it,
just imagine that the tiny increment

871
00:40:17,111 --> 00:40:19,870
that would make suddenly make it
compelling.

872
00:40:20,020 --> 00:40:21,220
I mean,
just imagine,

873
00:40:22,030 --> 00:40:26,050
I mean chest doesn't do it because chess
is so far from any central human

874
00:40:26,051 --> 00:40:27,550
concern.
But just imagine if your,

875
00:40:27,580 --> 00:40:32,580
if your phone recognized your emotional
state better than any than your best

876
00:40:33,731 --> 00:40:36,130
friend or your wife or anyone in your
life.

877
00:40:36,550 --> 00:40:38,490
And it did it reliably.
Right.

878
00:40:38,580 --> 00:40:41,890
So as your body liked that movie with
Joaquin Phoenix,

879
00:40:42,160 --> 00:40:44,230
her,
he falls in love with his phone.

880
00:40:44,231 --> 00:40:45,370
Right?
I mean,

881
00:40:45,371 --> 00:40:46,360
that's just not,
you know,

882
00:40:47,380 --> 00:40:51,820
that is not far off that far off as not,
it's a very discreet ability.

883
00:40:51,821 --> 00:40:52,451
I mean,
you could do that.

884
00:40:52,451 --> 00:40:56,130
You could do that without any,
any other ability in the phone.

885
00:40:56,140 --> 00:40:56,561
Really.
It's like,

886
00:40:56,561 --> 00:40:58,930
it doesn't,
it doesn't have to to,

887
00:40:59,560 --> 00:41:02,590
uh,
stand on the shoulders of any other kind

888
00:41:02,591 --> 00:41:04,420
of intelligence.
It could just,

889
00:41:04,421 --> 00:41:05,201
you know,
he just,

890
00:41:05,201 --> 00:41:06,250
you have,
I mean,

891
00:41:06,280 --> 00:41:08,770
this could be,
you could do this with just brute force

892
00:41:08,771 --> 00:41:11,950
in the same way that you have a great
chess player that doesn't necessarily

893
00:41:11,951 --> 00:41:15,120
understand that is playing chess.
You could have it,

894
00:41:15,130 --> 00:41:16,630
you know,
it's kind of facial recognition,

895
00:41:16,631 --> 00:41:20,530
facial recognition of emotion and the,
and the tone of voice,

896
00:41:20,531 --> 00:41:25,531
recognition of emotion and um,
the idea that it's going to,

897
00:41:26,301 --> 00:41:29,500
it's going to be a very long time for
computers that get better than people at

898
00:41:29,501 --> 00:41:32,680
that I think is very farfetched.
I was thinking,

899
00:41:32,710 --> 00:41:33,680
yeah,
think you're right.

900
00:41:33,710 --> 00:41:33,980
I was

901
00:41:33,980 --> 00:41:37,910
just thinking how strange would it be if
you had like headphones on and your

902
00:41:37,911 --> 00:41:41,030
phone was in your pocket and you have
rational conversations with your phone?

903
00:41:41,180 --> 00:41:42,860
Like your phone knew you better than you
know,

904
00:41:42,861 --> 00:41:44,090
you like,
I mean,

905
00:41:44,091 --> 00:41:45,110
I don't know what to do.
I mean,

906
00:41:45,111 --> 00:41:47,210
I don't think I was out of line.
She yelled at me.

907
00:41:47,211 --> 00:41:48,080
I mean,
what should I say?

908
00:41:48,230 --> 00:41:50,600
And it would have listened to every one
of your conversations with your friends.

909
00:41:50,601 --> 00:41:52,610
Exactly.
Train up on that and just talk to you

910
00:41:52,611 --> 00:41:53,630
about it and go,
listen man,

911
00:41:53,660 --> 00:41:55,620
this is what you got gotta do.
I was talking to political,

912
00:41:55,660 --> 00:41:58,390
you were sounding angry.
You got defensive,

913
00:41:58,440 --> 00:42:00,530
you got defensive.
Why were you so to hold that Joe?

914
00:42:00,531 --> 00:42:01,520
Yeah.
Apologize,

915
00:42:01,521 --> 00:42:03,110
relax.
Let's all move on.

916
00:42:03,260 --> 00:42:04,700
If you could accelerate it or,
okay.

917
00:42:04,701 --> 00:42:05,660
You right man.
Right man,

918
00:42:05,870 --> 00:42:07,370
and like you're talking to this little
artificial.

919
00:42:07,370 --> 00:42:10,670
Maybe that's the first version of
artificial intelligence that we suggest.

920
00:42:11,080 --> 00:42:11,691
We'd say,
all right,

921
00:42:11,691 --> 00:42:14,090
let's give it a shot and like self help
guys in your phone,

922
00:42:14,370 --> 00:42:16,250
you have like a personal trainer in your
phone.

923
00:42:16,280 --> 00:42:19,640
How to talk to girls at first.
Breakthrough yet slow down,

924
00:42:19,641 --> 00:42:20,780
dude.
Slow down.

925
00:42:21,040 --> 00:42:23,330
You're talking too fast.
Got To cool.

926
00:42:23,570 --> 00:42:26,330
Yeah,
I mean literally like giving you

927
00:42:26,331 --> 00:42:27,830
information.
That would be like step one.

928
00:42:27,831 --> 00:42:30,590
That'd be like the Sony Walkman.
Remember when you had a Walkman,

929
00:42:30,591 --> 00:42:35,591
like a cassette player that was like,
that was like a vcr when we were on our

930
00:42:35,781 --> 00:42:40,190
way to what we have today where you have
fucking 30,000 songs in your phone or

931
00:42:40,191 --> 00:42:42,410
something.
I think I remember the first Walkman.

932
00:42:42,411 --> 00:42:45,260
The first thing I just want to it back
when I skied,

933
00:42:45,261 --> 00:42:47,780
there was something called,
it was called astral tunes or something.

934
00:42:47,781 --> 00:42:50,210
It was like,
it was like a car radio that,

935
00:42:50,260 --> 00:42:52,490
that you could just put on in a pack on
your chest.

936
00:42:53,090 --> 00:42:54,860
Um,
yeah,

937
00:42:54,861 --> 00:42:56,720
it was,
they kept coming out with those sounds.

938
00:42:56,930 --> 00:42:59,000
They would get smaller and smaller.
So then that little,

939
00:42:59,060 --> 00:43:00,830
the little dude would start telling you,
Yo man,

940
00:43:00,831 --> 00:43:02,240
dude,
listen to keep it.

941
00:43:02,241 --> 00:43:04,310
Place me over a year.
Just let him stick me in.

942
00:43:04,311 --> 00:43:06,170
Your brain will be together all the
time.

943
00:43:06,350 --> 00:43:08,140
Yeah,
yeah.

944
00:43:08,540 --> 00:43:10,520
Giving you good advice for years,
Bro.

945
00:43:10,910 --> 00:43:14,360
Let me and your brain.
And so you and this little artificial

946
00:43:14,361 --> 00:43:16,760
intelligence,
do you have a relationship over time?

947
00:43:17,020 --> 00:43:20,600
And eventually it talks you into getting
your head drilled and the screw it in

948
00:43:20,601 --> 00:43:24,260
there and your artificial intelligence
is always powered by your central

949
00:43:24,261 --> 00:43:25,370
nervous system.
Did you,

950
00:43:25,430 --> 00:43:28,370
have you seen most of these movies?
Like did you see her and,

951
00:43:28,520 --> 00:43:29,450
and no,
I didn't.

952
00:43:29,510 --> 00:43:30,360
Okay.
No.

953
00:43:31,700 --> 00:43:33,030
Did you see Alex?
Mark and I,

954
00:43:33,260 --> 00:43:34,460
that was one of my,
that was good.

955
00:43:34,580 --> 00:43:35,960
Top 10 all time favorite movies.

956
00:43:35,960 --> 00:43:37,760
Yeah,
I loved that movie actually.

957
00:43:37,761 --> 00:43:39,290
I like it.
I saw it twice.

958
00:43:39,291 --> 00:43:39,591
I said,
I,

959
00:43:39,591 --> 00:43:42,860
I,
I was slow to realize how well,

960
00:43:43,310 --> 00:43:44,420
uh,
they,

961
00:43:44,930 --> 00:43:45,770
they did it.
I mean,

962
00:43:45,771 --> 00:43:48,080
it was just the first time I saw it,
I thought,

963
00:43:48,700 --> 00:43:51,410
um,
I wasn't as impressed and I watched it

964
00:43:51,411 --> 00:43:52,940
again and they really,
I mean,

965
00:43:53,000 --> 00:43:54,050
first of all,
the performance of,

966
00:43:54,080 --> 00:43:57,590
of um,
I forgot the actress's name.

967
00:43:57,860 --> 00:43:59,590
Uh,
the candor,

968
00:43:59,610 --> 00:44:01,660
at least you have a candor today.
Um,

969
00:44:01,790 --> 00:44:03,650
there was a woman who plays the robot in
an x,

970
00:44:03,670 --> 00:44:05,450
Monica.
It was just fantastic.

971
00:44:05,510 --> 00:44:06,570
But,
um,

972
00:44:06,620 --> 00:44:08,450
scary.
Good tuck you in.

973
00:44:08,451 --> 00:44:10,700
Anything.
We're getting a little full on time.

974
00:44:10,870 --> 00:44:12,890
Yeah.
What do we like five hours in a half

975
00:44:12,890 --> 00:44:14,630
hours in.
But I just got to know how to spot to

976
00:44:14,631 --> 00:44:15,464
fill up.
Wait a minute.

977
00:44:15,470 --> 00:44:17,080
How many hours?
Four and a half hour.

978
00:44:17,100 --> 00:44:19,370
No computers about to fill up.
Yeah we did.

979
00:44:19,550 --> 00:44:21,020
We just did a four and a half hour
pocket.

980
00:44:21,260 --> 00:44:24,400
We ready to keep going to Jesus.
Jamie didn't cock block.

981
00:44:26,360 --> 00:44:29,060
You know what man,
once you opened up that box up Pandora's

982
00:44:29,061 --> 00:44:32,070
box of artificial,
I haven't heard you guys discussed

983
00:44:32,070 --> 00:44:36,060
it and I've looked up,
is there any sort of concept of autism

984
00:44:36,061 --> 00:44:38,010
in ai?
Like a spectrum of ai?

985
00:44:38,370 --> 00:44:40,320
Like oh,
there are dumb ai and there's going to

986
00:44:40,321 --> 00:44:41,154
be smart Ai.

987
00:44:42,060 --> 00:44:42,893
I know.
So,

988
00:44:42,940 --> 00:44:46,190
I mean the scary thing so that yeah,
it's like super autism.

989
00:44:46,700 --> 00:44:48,260
There's no,
um,

990
00:44:49,490 --> 00:44:53,900
across the board there's,
I think that super intelligence and

991
00:44:54,320 --> 00:44:57,230
motivation and goals are totally
separable.

992
00:44:57,231 --> 00:45:02,231
So you could have a super intelligent
machine that is purposed toward a goal

993
00:45:02,841 --> 00:45:07,370
that just seems completely absurd and
harmful and non common sensical.

994
00:45:07,371 --> 00:45:09,860
And they said they,
the example that that Nick Bostrom uses

995
00:45:09,861 --> 00:45:12,020
in his,
in his book superintelligence which was

996
00:45:12,021 --> 00:45:13,820
a great book,
um,

997
00:45:13,940 --> 00:45:17,270
and did more to inform my thinking on
this topic than any other source.

998
00:45:17,690 --> 00:45:20,540
Um,
he talks about it a paperclip maximizer.

999
00:45:20,570 --> 00:45:22,400
You could,
you could build a super intelligent

1000
00:45:22,401 --> 00:45:23,630
paperclip maximizer.
Now,

1001
00:45:23,631 --> 00:45:27,470
not that anyone would do this,
but the point is you could build a

1002
00:45:27,471 --> 00:45:31,040
machine that was,
that was smarter than we are in every

1003
00:45:31,041 --> 00:45:33,380
conceivable way,
but all it wants to do is produce paper

1004
00:45:33,381 --> 00:45:34,040
clips.

1005
00:45:34,040 --> 00:45:35,710
Right.
Now that seems counterintuitive,

1006
00:45:35,720 --> 00:45:38,360
but there's no,
there's no reason when you kind of dig

1007
00:45:38,420 --> 00:45:42,600
deeply into this,
there's no reason why you couldn't build

1008
00:45:42,700 --> 00:45:46,670
a superhuman paperclip maximizer.
I just wants to turn everything,

1009
00:45:46,671 --> 00:45:48,680
you know,
just literally the atoms in your body

1010
00:45:48,681 --> 00:45:51,090
would be better used as paperclips.
Um,

1011
00:45:51,740 --> 00:45:56,600
and so this is just the point he's
making is that superintelligence could

1012
00:45:56,601 --> 00:46:00,260
be very counterintuitive.
It's not necessarily going to inherit

1013
00:46:00,950 --> 00:46:02,810
everything we find as,
you know,

1014
00:46:02,811 --> 00:46:06,980
common sensical or,
or emotionally appropriate or wise or

1015
00:46:06,981 --> 00:46:08,300
desirable.
Uh,

1016
00:46:08,301 --> 00:46:13,301
it could be totally foreign,
totally trivial in some way,

1017
00:46:14,091 --> 00:46:16,610
you know,
focused on something that means nothing

1018
00:46:16,611 --> 00:46:18,470
to us,
but means everything to it because of

1019
00:46:18,471 --> 00:46:23,471
some quirk and how it's motivation
system is structured and yet it can

1020
00:46:24,140 --> 00:46:28,460
build the perfect nanotechnology that
will allow it to build more paperclips.

1021
00:46:28,550 --> 00:46:29,383
Right?
So,

1022
00:46:29,870 --> 00:46:31,850
um,
and Lisa,

1023
00:46:31,851 --> 00:46:34,070
at least,
I don't think anyone can see why that's

1024
00:46:34,071 --> 00:46:35,180
ruled out in advance.

1025
00:46:35,180 --> 00:46:36,800
I mean,
there's no reason why we would

1026
00:46:36,980 --> 00:46:41,450
intentionally build that,
but the fear is we might build something

1027
00:46:42,230 --> 00:46:47,230
that either is not
perfectly aligned with our goals and our

1028
00:46:49,221 --> 00:46:50,930
common sense and our,
and our,

1029
00:46:50,931 --> 00:46:54,560
um,
aspirations and that it could form some

1030
00:46:55,100 --> 00:46:58,490
kind of separate in instrumental goals
to get what his wants,

1031
00:46:58,520 --> 00:47:02,630
wants that are totally incompatible with
life as we know it.

1032
00:47:02,631 --> 00:47:04,400
And that's,
you know,

1033
00:47:04,430 --> 00:47:05,170
I mean,
again,

1034
00:47:05,170 --> 00:47:05,661
the,
the,

1035
00:47:05,661 --> 00:47:07,490
the examples of this are always
cartoonish.

1036
00:47:07,491 --> 00:47:08,271
Like,
you know how I mean,

1037
00:47:08,271 --> 00:47:09,380
Elon Musk said,
you know,

1038
00:47:09,390 --> 00:47:12,500
if you built up super intelligent
machine and he told it to reduce spam,

1039
00:47:12,950 --> 00:47:16,040
well then it could just kill all people
as a great way to reduce spam.

1040
00:47:16,041 --> 00:47:16,874
Right.
Um,

1041
00:47:17,270 --> 00:47:20,210
but see the reason why that's La,
that's laughable,

1042
00:47:20,211 --> 00:47:23,420
but there's,
you can't assume the common sense won't

1043
00:47:23,421 --> 00:47:25,280
be there unless we've built it.
Right?

1044
00:47:25,281 --> 00:47:26,810
Like,
you have to have anticipated all of

1045
00:47:26,811 --> 00:47:27,140
this.

1046
00:47:27,140 --> 00:47:27,980
You can't,
if you say,

1047
00:47:27,981 --> 00:47:29,890
take me to the airport as fast as you
can.

1048
00:47:30,340 --> 00:47:31,340
Again,
this is Bostrom,

1049
00:47:31,620 --> 00:47:33,730
you know,
and you have a super intelligent,

1050
00:47:34,090 --> 00:47:36,700
automatic a car.
Um,

1051
00:47:36,910 --> 00:47:38,200
you know,
a self driving car,

1052
00:47:38,210 --> 00:47:39,970
you'll just,
you'll get to the airport covered in

1053
00:47:39,971 --> 00:47:41,220
vomit because he'll just,
it's,

1054
00:47:41,221 --> 00:47:43,660
it's just going to go as fast as it can
go.

1055
00:47:44,290 --> 00:47:46,510
Um,
so it's a,

1056
00:47:47,410 --> 00:47:51,130
it's our intuitions about what it would
mean to be super intelligent

1057
00:47:51,520 --> 00:47:52,840
necessarily,
or are,

1058
00:47:52,841 --> 00:47:54,880
um,
I mean there's no,

1059
00:47:54,970 --> 00:47:57,680
we have to correct for them,
cause I think our intuitions are,

1060
00:47:57,790 --> 00:47:58,623
are bad.

1061
00:48:03,070 --> 00:48:06,030
[inaudible].

