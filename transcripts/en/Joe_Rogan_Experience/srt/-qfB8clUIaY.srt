1
00:00:06,900 --> 00:00:08,250
Boom.
Hello Ben.

2
00:00:08,560 --> 00:00:09,830
Hey there.
Good to see you,

3
00:00:09,831 --> 00:00:10,611
man.
Yeah,

4
00:00:10,611 --> 00:00:12,680
it's a pleasure to be here.
Thanks for doing this.

5
00:00:13,100 --> 00:00:13,581
Yeah,
yeah.

6
00:00:13,581 --> 00:00:14,150
Thanks.
Thanks.

7
00:00:14,150 --> 00:00:15,260
Thanks.
Thanks for having me.

8
00:00:15,261 --> 00:00:16,094
I've been.
I've been looking at some of your shows 

9
00:00:17,770 --> 00:00:22,770
in the last few last few days just to 
get a sense of how you're thinking about

10
00:00:23,690 --> 00:00:26,090
ai and crypto and the various other 
things.

11
00:00:26,100 --> 00:00:28,340
I'm involved in this.
It's been interesting.

12
00:00:28,380 --> 00:00:30,150
Well,
I've been following you as well.

13
00:00:30,180 --> 00:00:31,013
I've been paying attention to a lot of 
your lectures and talks and different 

14
00:00:33,481 --> 00:00:35,490
things you've done over the last couple 
days as well.

15
00:00:35,491 --> 00:00:39,340
Getting ready for this.
It's a Ai is a.

16
00:00:39,420 --> 00:00:42,660
either people are really excited about 
it or they're really terrified of it.

17
00:00:42,780 --> 00:00:45,150
Those are the sort of.
It seems to be the two responses.

18
00:00:45,151 --> 00:00:45,984
Either people have this dismal view of 
these robots taking over the world or 

19
00:00:50,041 --> 00:00:54,480
they think it's going to be some amazing
sort of symbiotic relationship with that

20
00:00:54,510 --> 00:00:55,343
we have with these things.
It's gonna evolve human beings past the 

21
00:00:58,530 --> 00:01:00,480
monkey stage that we're at right now.

22
00:01:00,670 --> 00:01:01,091
Yeah,
and I,

23
00:01:01,091 --> 00:01:06,091
I tend to be on the leather more 
positive side of this dichotomy,

24
00:01:06,971 --> 00:01:11,971
but I think one thing that has struck me
in recent years is many people are now,

25
00:01:14,980 --> 00:01:17,220
you know,
mentally confronting all the issues,

26
00:01:17,240 --> 00:01:18,073
running ai for the first time and I mean
I've been working on ai for three 

27
00:01:22,481 --> 00:01:23,314
decades and I first started thinking 
about ai when I was a little kid in the 

28
00:01:27,460 --> 00:01:28,293
early,
late sixties and early seventies when I 

29
00:01:30,341 --> 00:01:33,640
saw ais and robots on the original star 
Trek.

30
00:01:33,641 --> 00:01:34,474
So I guess I've had a lot of cycles to 
process the positives and negatives of 

31
00:01:40,631 --> 00:01:41,464
it where it's now like suddenly most of 
the world is thinking through all this 

32
00:01:44,981 --> 00:01:46,600
for the first,
for the first time.

33
00:01:46,601 --> 00:01:47,434
And you know,
when you first wrap your brain around 

34
00:01:48,911 --> 00:01:53,200
the idea that there may be creatures 
10,000

35
00:01:53,201 --> 00:01:56,440
or a million times smarter than human 
beings at first.

36
00:01:56,441 --> 00:01:57,970
This is a bit of a shocker.
Right?

37
00:01:57,971 --> 00:01:58,804
And then,
I mean it takes a while to internalize 

38
00:02:00,911 --> 00:02:02,080
this into your worldview.

39
00:02:02,660 --> 00:02:05,240
Well,
it's that there's also,

40
00:02:05,330 --> 00:02:09,800
I think there's a problem with the term 
artificial intelligence because it's,

41
00:02:09,860 --> 00:02:11,180
that's,
it's intelligent.

42
00:02:11,450 --> 00:02:13,370
It's there.
It's a real thing.

43
00:02:13,540 --> 00:02:14,840
Yeah.
It's not artificial.

44
00:02:14,841 --> 00:02:17,090
It's not like a fake diamond or fake 
Ferrari.

45
00:02:17,091 --> 00:02:19,340
It's a real thing and it.

46
00:02:19,970 --> 00:02:24,560
It's not a great term and there's been 
many attempts to replace it.

47
00:02:24,570 --> 00:02:28,310
Would synthetic intelligence for,
for example,

48
00:02:28,550 --> 00:02:32,090
but for better or worse,
I get ai is there.

49
00:02:32,091 --> 00:02:35,950
It's part of the popular imagination 
that seems that it's an imperfect word,

50
00:02:35,951 --> 00:02:37,760
but it's not going away.

51
00:02:37,980 --> 00:02:38,491
Well,
I,

52
00:02:38,491 --> 00:02:39,890
I.
my question is like,

53
00:02:40,020 --> 00:02:40,853
are we married to this idea of 
intelligence and of life being 

54
00:02:44,311 --> 00:02:45,144
biological,
being carbon based tissue and cells and 

55
00:02:48,451 --> 00:02:53,451
blood and or insects or mammals or fish?
Are we married to that too much?

56
00:02:53,820 --> 00:02:58,820
Do you think that it's entirely possible
that what human beings are doing,

57
00:02:59,290 --> 00:03:00,123
what people that are at the tip of ai 
right now that are really pushing the 

58
00:03:04,361 --> 00:03:09,361
technology where they're doing is really
creating a new life form that it's going

59
00:03:09,521 --> 00:03:10,354
to be a new thing that just the same way
we recognize wasps and buffaloes and 

60
00:03:15,460 --> 00:03:16,293
artificial intelligence is just going to
be a life form that emerges from the 

61
00:03:18,761 --> 00:03:20,890
creativity and ingenuity of human 
beings.

62
00:03:21,630 --> 00:03:22,463
Indeed,
so I've long been an advocate of a 

63
00:03:25,710 --> 00:03:26,543
philosophy I think of as,
as pattern isn't like it's the pattern 

64
00:03:29,701 --> 00:03:34,701
of organization that appears to be the 
critical thing and the,

65
00:03:35,220 --> 00:03:36,090
the,
you know,

66
00:03:36,091 --> 00:03:36,924
the individual cells and going down 
further the molecules and particles in 

67
00:03:41,281 --> 00:03:44,250
our body or our turning over all the 
time.

68
00:03:44,251 --> 00:03:45,084
So it's not in this specific combination
of elementary particles which makes me 

69
00:03:49,021 --> 00:03:49,854
who I am or makes you who you are.
It's a pattern by which they're 

70
00:03:52,321 --> 00:03:56,220
organized and the patterns by which they
change over time.

71
00:03:56,221 --> 00:03:57,054
So if we can create digital systems are 
quantum computers or femto computers or 

72
00:04:02,371 --> 00:04:06,120
whatever it is,
manifesting the patterns of organization

73
00:04:06,121 --> 00:04:10,480
that Constitute Intelligence.
I mean then then there you are there,

74
00:04:10,540 --> 00:04:11,850
there,
there is intelligence,

75
00:04:11,860 --> 00:04:12,690
right?
So that,

76
00:04:12,690 --> 00:04:17,010
that's not to say that you know,
consciousness and experiences just about

77
00:04:17,400 --> 00:04:20,250
patterns of organization.
There may be more dimensions to it,

78
00:04:20,280 --> 00:04:21,113
but when,
when you look at what constitutes 

79
00:04:22,651 --> 00:04:24,300
intelligence thinking,
cognition,

80
00:04:24,301 --> 00:04:25,950
problem solving,
you know,

81
00:04:25,951 --> 00:04:27,660
it's the pattern of organization,
not,

82
00:04:27,661 --> 00:04:32,250
not this specific material as as,
as far as we can tell.

83
00:04:32,251 --> 00:04:33,084
So we can see no reason based on all the
science that we know so far that you 

84
00:04:38,311 --> 00:04:39,144
couldn't make an intelligent system of 
some other form of matter rather than 

85
00:04:43,181 --> 00:04:47,940
the specific types of atoms and 
molecules that make up human beings.

86
00:04:48,120 --> 00:04:48,953
And it seems that we're,
we're well on the way to being able to 

87
00:04:51,451 --> 00:04:52,284
do so.

88
00:04:52,440 --> 00:04:54,990
When you're studying,
when you're studying intelligence,

89
00:04:54,991 --> 00:04:56,990
you're studying artificial intelligence,
do,

90
00:04:57,020 --> 00:04:57,853
did you spend any time studying the 
patterns that insects seem to 

91
00:05:01,981 --> 00:05:02,814
cooperatively behave with?
Like how leafcutter ants build these 

92
00:05:06,961 --> 00:05:11,961
elaborate structures underground and 
build these giant colonies.

93
00:05:12,901 --> 00:05:14,990
And did you study how it did?
I did,

94
00:05:15,080 --> 00:05:15,801
actually,
yes.

95
00:05:15,801 --> 00:05:16,670
So I,
I,

96
00:05:17,030 --> 00:05:17,863
I sort of grew up with the philosophy of
complex systems which was championed by 

97
00:05:24,381 --> 00:05:26,250
that,
by the Santa Fe Institute in,

98
00:05:26,251 --> 00:05:27,590
in,
in the 19 eighties.

99
00:05:27,591 --> 00:05:32,591
And the whole concept that there is an 
interdisciplinary complex system science

100
00:05:33,290 --> 00:05:34,430
which includes,
you know,

101
00:05:34,431 --> 00:05:36,590
biology,
cosmology,

102
00:05:36,591 --> 00:05:38,240
psychology,
sociology.

103
00:05:38,241 --> 00:05:43,070
There's sort of universal patterns of,
of Self Organization and you know,

104
00:05:43,071 --> 00:05:48,071
aunts and ant colonies have long been 
the paradigm case for that.

105
00:05:48,231 --> 00:05:49,064
And I,
I used to play with the ant colonies in 

106
00:05:51,001 --> 00:05:52,130
my backyard.
Wow.

107
00:05:52,550 --> 00:05:55,820
When I was a kid and you'd lay down food
and certain patterns,

108
00:05:55,821 --> 00:05:56,654
you'd see how their answer down.
Pheromones in the colonies are 

109
00:05:59,690 --> 00:06:00,523
organizing it in a certain way,
and that's an interesting self 

110
00:06:04,370 --> 00:06:05,203
organizing complex system on that zone.
It's lacking some types of adaptive 

111
00:06:10,131 --> 00:06:14,630
intelligence that that human minds and 
human societies have been,

112
00:06:14,650 --> 00:06:17,630
but it has also interesting self 
organizing patterns.

113
00:06:17,960 --> 00:06:22,550
This reminds me of the novel Solaris by 
Stanislaw Lem,

114
00:06:22,580 --> 00:06:27,460
which was published in the sixties,
which was really quite,

115
00:06:27,680 --> 00:06:30,860
quite a deep novel,
much deeper than the movie that was made

116
00:06:30,861 --> 00:06:32,480
of it.
Did you ever read that book?

117
00:06:32,481 --> 00:06:33,800
So there is.
So what?

118
00:06:34,220 --> 00:06:35,053
I'm not familiar with the movie either.
Who will say where there was an amazing 

119
00:06:37,731 --> 00:06:38,564
brilliant movie by Tarkovsky,
the Russian director from the late 

120
00:06:40,761 --> 00:06:41,594
sixties.
Then there was a movie by Steven 

121
00:06:44,150 --> 00:06:44,983
Soderbergh which was sort of clammed up 
and Americanized and that was fairly 

122
00:06:47,961 --> 00:06:48,794
recent,
right?

123
00:06:49,000 --> 00:06:51,170
Ten years ago,
but that wasn't.

124
00:06:51,200 --> 00:06:52,970
Didn't get all the deep points and 
novel,

125
00:06:52,971 --> 00:06:54,920
the original novel.
In essence,

126
00:06:54,921 --> 00:06:55,754
there's this,
there's this ocean on coating the 

127
00:06:59,751 --> 00:07:00,584
surface of some alien planet which has 
amazingly complex fractal patterns of 

128
00:07:04,401 --> 00:07:08,690
organization and it's also interactive 
like the patterns of organization on the

129
00:07:08,691 --> 00:07:13,691
Ocean respond based on what you do and 
when people get near the ocean,

130
00:07:14,270 --> 00:07:15,103
it causes them to hallucinate things and
even caused them to see Simulacra of 

131
00:07:19,581 --> 00:07:22,820
people from their past,
even the like the person who they'd most

132
00:07:22,821 --> 00:07:27,200
harmed or injured in their past appears 
and interacts with them so clearly.

133
00:07:27,200 --> 00:07:28,033
This ocean has some type of amazing 
complexity and intelligence from the 

134
00:07:31,551 --> 00:07:32,384
patterns that displays and from the 
weird things that reeks in your mind so 

135
00:07:36,561 --> 00:07:41,561
that the people on earth try to 
understand how the ocean is thinking.

136
00:07:42,441 --> 00:07:47,441
They send a scientific expedition there 
to to interact with that ocean,

137
00:07:48,530 --> 00:07:49,363
but it's just so alien.
Even though monkeys with people's minds 

138
00:07:51,801 --> 00:07:56,801
include these doing complex things,
no two way communication is ever is ever

139
00:07:58,191 --> 00:08:03,191
established and eventually the human 
expedition gives up and goes home.

140
00:08:03,591 --> 00:08:06,560
So it's a very Russian ending to the 
novel.

141
00:08:07,410 --> 00:08:09,750
I guess it's not.
I saw that,

142
00:08:09,830 --> 00:08:14,830
but that the.
The interesting message there is,

143
00:08:15,891 --> 00:08:18,350
I mean there can be many,
many kinds of intelligence,

144
00:08:18,351 --> 00:08:19,184
right?
I mean,

145
00:08:19,970 --> 00:08:20,803
human intelligence is one thing.
The intelligence of an ant colony is a 

146
00:08:24,741 --> 00:08:25,574
different thing.
The intelligence of human society is a 

147
00:08:27,951 --> 00:08:31,370
different thing.
Ecosystem is a different thing and there

148
00:08:31,371 --> 00:08:32,204
could be many,
many types of ais that we could build 

149
00:08:35,541 --> 00:08:37,400
with many,
many different properties.

150
00:08:37,400 --> 00:08:42,400
Some could be wonderful to human beings,
some can be horrible to human beings,

151
00:08:42,590 --> 00:08:47,590
some could just be alien minds that that
we can't even relate,

152
00:08:48,140 --> 00:08:50,040
relate,
relate to very,

153
00:08:50,360 --> 00:08:51,193
very well.
So we we have a very limited conception 

154
00:08:53,541 --> 00:08:54,374
of what an intelligence is.
If we just think by close analogy to to 

155
00:08:58,801 --> 00:08:59,634
human minds and this.
This is important if you're thinking 

156
00:09:02,041 --> 00:09:05,210
about engineering or growing are the 
life forms.

157
00:09:05,211 --> 00:09:08,340
They're artificial minds because it's 
not just can we do this?

158
00:09:08,341 --> 00:09:09,174
It's what kind of mind are,
are we going to engineer or evolve and 

159
00:09:14,701 --> 00:09:15,534
there's.
There's a huge spectrum of 

160
00:09:16,201 --> 00:09:17,034
possibilities.

161
00:09:17,120 --> 00:09:17,953
Yeah,
that's one of the reasons why I asked 

162
00:09:18,621 --> 00:09:23,621
you that if we'd created,
if human beings had created some sort of

163
00:09:23,871 --> 00:09:24,704
an insect and this insect started 
organizing and developing these complex 

164
00:09:28,641 --> 00:09:31,560
colonies like a leaf cutter and building
these structures underground,

165
00:09:31,940 --> 00:09:33,650
people would go crazy.
They would panic.

166
00:09:33,950 --> 00:09:35,420
They would think these things are 
organizing.

167
00:09:35,421 --> 00:09:36,254
They're gonna.
They're going to build up the resources 

168
00:09:37,221 --> 00:09:38,054
and attack us.
They're going to try to take over 

169
00:09:38,841 --> 00:09:41,390
humanity.
I mean this what,

170
00:09:41,410 --> 00:09:45,200
what people are worried about more than 
anything when it comes to technology,

171
00:09:45,201 --> 00:09:49,340
I think is the idea that we're going to 
be irrelevant,

172
00:09:49,580 --> 00:09:50,413
that we're going to be a antiques and 
that something new and better is going 

173
00:09:54,111 --> 00:09:56,390
to take our place,
which is a,

174
00:09:56,620 --> 00:09:57,453
which is almost double,
so we're thinking worried about it 

175
00:09:59,391 --> 00:10:02,300
because it's sort of the history of 
biological life on earth.

176
00:10:02,810 --> 00:10:05,300
I mean,
what is there as complex things?

177
00:10:05,300 --> 00:10:06,133
They become more competent with single 
cell organisms to multicellular 

178
00:10:08,041 --> 00:10:10,070
organisms.
That seems to be a pattern leading up to

179
00:10:10,071 --> 00:10:14,870
us and us with this unprecedented 
ability to change our environment.

180
00:10:14,930 --> 00:10:15,980
That's what we can do,
right?

181
00:10:15,981 --> 00:10:19,010
We can manipulate things,
poison the environment.

182
00:10:19,011 --> 00:10:22,130
We can blow up entire countries with 
bombs if we'd like to,

183
00:10:22,131 --> 00:10:26,930
and we can also do wild creative things 
like send signals through space and land

184
00:10:26,931 --> 00:10:27,764
on someone else's phone on the other 
side of the world almost 

185
00:10:28,851 --> 00:10:31,310
instantaneously.
We have incredible power,

186
00:10:31,640 --> 00:10:35,870
but we're also.
We're also so limited by our biology.

187
00:10:36,330 --> 00:10:38,750
Yeah.
The thing I think people are afraid of,

188
00:10:39,680 --> 00:10:40,513
I'm afraid of,
but I don't know if that makes any 

189
00:10:41,691 --> 00:10:46,370
sense.
Is that the next level of life,

190
00:10:46,400 --> 00:10:49,310
whatever artificial life is or whatever 
the,

191
00:10:49,311 --> 00:10:50,144
the,
the human symbiotic is that it's going 

192
00:10:52,441 --> 00:10:55,820
to lack emotions.
It's going to lack desires and needs and

193
00:10:55,821 --> 00:10:58,490
all the things that we think are special
about us,

194
00:10:58,820 --> 00:11:02,060
our creativity,
our desire for attention and love,

195
00:11:02,061 --> 00:11:05,270
all of our camaraderie,
all these different things that are sort

196
00:11:05,271 --> 00:11:06,104
of programmed into us with our genetics 
in order to advance our species that we 

197
00:11:13,071 --> 00:11:14,540
were so connected to these things.

198
00:11:14,540 --> 00:11:19,220
But there's so the reason for war that 
the reason for the lies,

199
00:11:19,221 --> 00:11:21,140
deception,
thievery.

200
00:11:21,320 --> 00:11:25,580
There's so many things that are built 
into being a person that are responsible

201
00:11:25,581 --> 00:11:29,000
for all the woes of humanity,
but were afraid to lose those.

202
00:11:29,200 --> 00:11:31,900
I think it's almost,

203
00:11:32,080 --> 00:11:37,080
and by this point that humanity is going
to create

204
00:11:38,430 --> 00:11:39,263
synthetic intelligences with 
tremendously greater general 

205
00:11:42,751 --> 00:11:47,751
intelligence and practical capability 
than human beings have.

206
00:11:48,091 --> 00:11:48,924
I mean,
I think I know how to do that with the 

207
00:11:50,371 --> 00:11:52,470
software I'm working on with my own 
team,

208
00:11:52,830 --> 00:11:53,663
but if we fail,
you know there's a load of other teams 

209
00:11:57,070 --> 00:12:00,040
who I think are a bit behind us,
but they're going in the same direction.

210
00:12:00,041 --> 00:12:00,874
Now.
I feel like you're at the tip of the 

211
00:12:01,631 --> 00:12:02,950
spear with this stuff.
I do,

212
00:12:02,980 --> 00:12:03,813
but I also think that's not the most 
important thing from a human 

213
00:12:07,991 --> 00:12:08,824
perspective.
The most important thing is that 

214
00:12:09,851 --> 00:12:13,070
humanity as a whole is quite close to 
this,

215
00:12:13,071 --> 00:12:15,040
this threshold event.
Right.

216
00:12:15,041 --> 00:12:19,180
So how far do you think?
It's quite close by my own gut feeling.

217
00:12:19,181 --> 00:12:21,940
Five to 30 years.
Let's say that's pretty close,

218
00:12:21,941 --> 00:12:26,230
but if I'm wrong and it's 100 years,
like in the historical time scale,

219
00:12:26,560 --> 00:12:27,790
that sort of doesn't matter.

220
00:12:27,790 --> 00:12:28,623
It's like,
did the Sumerians create civilization 

221
00:12:30,521 --> 00:12:32,380
10,000
or 10,050

222
00:12:32,381 --> 00:12:33,211
years ago?
Like what?

223
00:12:33,211 --> 00:12:34,540
What difference does it make?
Right.

224
00:12:34,541 --> 00:12:35,374
So I think we're quite close to creating
super human artificial general 

225
00:12:41,291 --> 00:12:46,291
intelligence and that's in a way almost 
inevitable given where we are now.

226
00:12:48,670 --> 00:12:49,503
On the other hand,
I think we still have some agency 

227
00:12:53,351 --> 00:12:54,184
regarding whether this comes out in a 
way that respects human values and 

228
00:13:00,611 --> 00:13:01,444
culture,
which are important to us now given who 

229
00:13:03,101 --> 00:13:03,934
and what we are or that is essentially 
indifferent to human values and culture 

230
00:13:08,951 --> 00:13:09,784
in the same way that we're mostly 
indifferent to chimpanzee values and 

231
00:13:13,181 --> 00:13:14,014
culture at that at this point.
Completely indifferent to insect values 

232
00:13:17,471 --> 00:13:18,970
and culture.
Not Completely,

233
00:13:19,350 --> 00:13:22,720
but if you think about it,
I mean if I'm building a new house,

234
00:13:23,290 --> 00:13:24,123
I will bulldoze those a bunch of events,
but yet we get upset if we extinct an 

235
00:13:27,071 --> 00:13:28,150
insect species.

236
00:13:28,150 --> 00:13:28,983
Right?
So we,

237
00:13:29,160 --> 00:13:29,993
we carry,
we carry it to some level but not but 

238
00:13:32,490 --> 00:13:33,323
we.
But we would like the Super Ais to care 

239
00:13:35,201 --> 00:13:37,690
about us more than we care about insects
or,

240
00:13:37,691 --> 00:13:40,300
or grade.
Absolutely.

241
00:13:40,301 --> 00:13:41,700
Right.
And I think this,

242
00:13:42,160 --> 00:13:46,540
this is something we can impact right 
now and to to,

243
00:13:46,580 --> 00:13:50,380
to be honest,
I mean in a certain part of my mind,

244
00:13:50,410 --> 00:13:52,570
I can think,
well like in,

245
00:13:52,590 --> 00:13:55,810
in the end,
I don't matter that much,

246
00:13:55,930 --> 00:13:58,610
my four kids don't matter that much,
my granddaughter,

247
00:13:58,611 --> 00:13:59,444
it doesn't matter that much like we are 
patterns of organization in a very long 

248
00:14:03,971 --> 00:14:08,290
lineage of patterns of organization and 
they matter very much to you and other,

249
00:14:08,291 --> 00:14:11,380
you know,
dinosaurs came and went and neanderthals

250
00:14:11,381 --> 00:14:14,170
came and went.
Humans may come and go,

251
00:14:14,320 --> 00:14:18,940
the Ai that we create may come and go 
and that's the nature of the universe.

252
00:14:18,941 --> 00:14:22,450
But on the other hand,
of course in my heart,

253
00:14:22,451 --> 00:14:25,270
from my situated perspective as an 
individual human,

254
00:14:25,271 --> 00:14:30,000
like if,
if some ai charged to annihilate my,

255
00:14:30,020 --> 00:14:33,160
my 10 month old son,
I would try to kill that Ai.

256
00:14:33,160 --> 00:14:34,530
Right?
So as,

257
00:14:34,810 --> 00:14:39,760
as a human being situated in this 
specific species,

258
00:14:39,820 --> 00:14:43,560
place in time,
I care a lot about the condition of,

259
00:14:43,561 --> 00:14:48,561
of all of us humans.
And so I would like to not only create a

260
00:14:50,141 --> 00:14:53,240
powerful general but,
but create one which,

261
00:14:53,241 --> 00:14:54,074
which is,
is going to be beneficial to humans and 

262
00:14:59,000 --> 00:15:03,310
other life forms on the planet even 
while in some ways going,

263
00:15:03,340 --> 00:15:06,560
going beyond every,
everything that we are right.

264
00:15:06,590 --> 00:15:10,340
And there can't be any guarantees about 
something like this.

265
00:15:10,341 --> 00:15:11,174
On the other hand,
humanity has really never had any 

266
00:15:15,290 --> 00:15:17,270
guarantees about anything anyway.
Right?

267
00:15:17,271 --> 00:15:17,871
I mean,
since,

268
00:15:17,871 --> 00:15:18,710
since,
since,

269
00:15:19,430 --> 00:15:20,263
since we created civilization.
We've been leaping into the unknown one 

270
00:15:23,781 --> 00:15:24,614
time after the other in the somewhat 
conscious and self aware way about it 

271
00:15:29,180 --> 00:15:30,320
from,
you know,

272
00:15:30,321 --> 00:15:32,600
agriculture to language,
to math,

273
00:15:32,601 --> 00:15:33,434
to the industrial revolution.
We're leaping into the unknown all the 

274
00:15:37,101 --> 00:15:37,934
time,
which is part of why we're where we are 

275
00:15:40,491 --> 00:15:43,830
today instead of just another animal 
species.

276
00:15:44,170 --> 00:15:45,003
So we can't have a guarantee that Agi,
artificial general intelligence is we 

277
00:15:50,961 --> 00:15:51,794
create,
are going to do what we consider the 

278
00:15:54,051 --> 00:15:56,210
right thing given our current value 
systems.

279
00:15:56,211 --> 00:15:57,044
On the other hand,
I suspect we can bias the odds in the 

280
00:16:02,661 --> 00:16:05,630
favor of,
of human values and,

281
00:16:06,060 --> 00:16:08,390
and culture.
And that's something I've.

282
00:16:08,960 --> 00:16:12,260
I've put a lot of thought and work into,
alongside the,

283
00:16:12,710 --> 00:16:14,660
you know,
the basic algorithms of,

284
00:16:14,661 --> 00:16:15,494
of artificial cognition is the issue 
that the initial creation would be 

285
00:16:20,691 --> 00:16:21,524
subject to our programming,
but that it could perhaps program 

286
00:16:24,241 --> 00:16:28,640
something more efficient and design 
something like if you build creativity,

287
00:16:29,930 --> 00:16:31,110
you have to create,
I mean,

288
00:16:31,160 --> 00:16:34,420
general generalization is about creative
writing,

289
00:16:34,421 --> 00:16:35,480
right?
Yeah.

290
00:16:35,750 --> 00:16:40,190
But is the issue that it would choose to
not accept our values,

291
00:16:40,340 --> 00:16:41,173
which it might find clearly will choose 
not to accept our values and we want it 

292
00:16:44,211 --> 00:16:46,730
to choose not to accept all of our 
values.

293
00:16:46,940 --> 00:16:50,960
So it's more a matter of whether the 
ongoing creation,

294
00:16:50,961 --> 00:16:51,794
evolution of new values occurs with some
continuity in respect for the previous 

295
00:16:55,671 --> 00:16:56,451
one.
So I mean,

296
00:16:56,451 --> 00:16:57,980
uh,
with I've,

297
00:16:57,981 --> 00:17:01,730
for human kids now one is a baby,
but the other three are adults,

298
00:17:01,731 --> 00:17:02,564
right?
And with each of them I took the 

299
00:17:03,741 --> 00:17:08,540
approach of trying to teach the kids 
what my values were,

300
00:17:08,780 --> 00:17:13,370
not just by preaching at them,
by entering into shared situations,

301
00:17:13,670 --> 00:17:14,900
but then,
you know,

302
00:17:14,901 --> 00:17:15,734
when your kids grow up,
they're going to go in their own 

303
00:17:17,661 --> 00:17:19,100
different directions.
Right?

304
00:17:19,170 --> 00:17:20,003
And these are humans,
but they all have the same sort of 

305
00:17:22,851 --> 00:17:26,720
biological needs,
which is one of the first place.

306
00:17:26,721 --> 00:17:28,460
Yeah,
there's still as an analogy,

307
00:17:28,480 --> 00:17:33,480
I think the ais that we create,
you can think of us as our mind children

308
00:17:33,860 --> 00:17:34,693
and we're starting them off with our 
culture and values if we do it properly 

309
00:17:40,010 --> 00:17:45,010
or at least with a certain subset of the
whole diverse self-contradictory mess of

310
00:17:45,381 --> 00:17:47,810
human culture and values,
but you know,

311
00:17:47,811 --> 00:17:51,930
they're going to evolve in,
in a different direction.

312
00:17:52,200 --> 00:17:56,340
But you want that evolution to take 
place in their reflective and,

313
00:17:56,341 --> 00:17:57,174
and,
and caring way rather than the heatless 

314
00:17:58,471 --> 00:18:00,320
way.
Because if you think about it,

315
00:18:00,780 --> 00:18:01,613
the average human a thousand years ago 
or even 50 years ago would have thought 

316
00:18:04,321 --> 00:18:09,120
you and me,
we're like hopelessly immoral miscreants

317
00:18:09,121 --> 00:18:12,070
who had abandoned all the valuable thing
things in life.

318
00:18:13,250 --> 00:18:14,120
Uh,
my,

319
00:18:14,160 --> 00:18:14,850
my,
my,

320
00:18:14,850 --> 00:18:16,210
my hand.
I mean,

321
00:18:16,211 --> 00:18:16,601
I'm,
I'm,

322
00:18:16,601 --> 00:18:17,010
uh,
I'm an.

323
00:18:17,010 --> 00:18:18,260
I'm an infidel,
right?

324
00:18:18,270 --> 00:18:20,220
I don't know.
I haven't gone to church,

325
00:18:20,260 --> 00:18:21,670
uh,
for I,

326
00:18:21,671 --> 00:18:23,710
I guess I mean my,
my,

327
00:18:23,750 --> 00:18:25,370
my mother's lesbians,
right?

328
00:18:25,380 --> 00:18:26,213
I mean,
there's all these things that we take 

329
00:18:28,141 --> 00:18:33,141
for granted now that not that long ago 
were completely against what most humans

330
00:18:34,741 --> 00:18:37,980
considered maybe the most important 
values of life.

331
00:18:37,981 --> 00:18:42,390
So I mean human values itself is 
completely a moving,

332
00:18:42,420 --> 00:18:45,210
a moving target.
So think in our generation,

333
00:18:45,820 --> 00:18:48,690
moving in our,
in our generation pretty radically,

334
00:18:48,960 --> 00:18:49,870
very radically.

335
00:18:49,920 --> 00:18:53,590
When I think back to my childhood,
I,

336
00:18:53,610 --> 00:18:54,443
I,
I lived in New Jersey for nine years of 

337
00:18:57,181 --> 00:18:58,014
my childhood and just the level of 
racism and antisemitism and sexism that 

338
00:19:03,001 --> 00:19:06,630
were just normal,
ambient and taken for granted.

339
00:19:06,631 --> 00:19:08,120
Then this was,
this.

340
00:19:08,220 --> 00:19:11,430
Was this when you're between.
Because we're the same age.

341
00:19:11,431 --> 00:19:12,830
We're both one.
Yeah,

342
00:19:12,970 --> 00:19:13,431
yeah,
yeah,

343
00:19:13,431 --> 00:19:14,610
yeah.
Born in 66.

344
00:19:14,611 --> 00:19:19,611
I lived in Jersey from 73 to 82,
so I was there from 67 to 73.

345
00:19:22,650 --> 00:19:23,483
Oh yeah.
Yeah.

346
00:19:24,050 --> 00:19:26,100
So yeah,
I'm in my,

347
00:19:26,130 --> 00:19:29,730
I'm in my sister went to the high school
prom with a,

348
00:19:29,790 --> 00:19:33,500
with a black guy and so we got our car 
turned upside down,

349
00:19:33,510 --> 00:19:34,343
the windows of our house smash.
And it was like a human hugh mungus 

350
00:19:36,810 --> 00:19:39,800
thing and it's almost unbelievable now,
right?

351
00:19:39,820 --> 00:19:43,650
Because now no one would care care 
whatsoever.

352
00:19:43,651 --> 00:19:44,100
It's,
it's,

353
00:19:44,100 --> 00:19:44,830
it's,
it's just,

354
00:19:44,830 --> 00:19:46,010
it's just life,
right?

355
00:19:46,430 --> 00:19:48,380
Certainly the,
some fringe parts of his sculpture here,

356
00:19:48,400 --> 00:19:52,260
but,
but still the point is there is no fixed

357
00:19:52,980 --> 00:19:54,780
list of,
of values,

358
00:19:54,810 --> 00:19:56,250
human values.

359
00:19:56,250 --> 00:19:57,083
It's an ongoing evolving process and 
what you want is for the evolution of 

360
00:20:02,041 --> 00:20:02,874
the AIS values to be coupled closely 
with the evolution of human values 

361
00:20:07,770 --> 00:20:08,603
rather than going off in some other,
the different direction that we can't 

362
00:20:13,170 --> 00:20:16,500
even understand that this is literally 
playing God,

363
00:20:16,590 --> 00:20:17,423
right?
I mean if you're talking about like 

364
00:20:18,510 --> 00:20:19,343
trying to program in values,
I don't think you can program in values 

365
00:20:23,550 --> 00:20:28,550
that fully you can program in a system 
for learning and growing values and here

366
00:20:30,331 --> 00:20:32,850
again,
the analogy with human kids,

367
00:20:33,360 --> 00:20:34,193
it's not hopeless like telling,
telling your kids these are the 10 

368
00:20:38,221 --> 00:20:40,950
things that are important doesn't work 
that well,

369
00:20:40,951 --> 00:20:41,660
right?
What works?

370
00:20:41,660 --> 00:20:46,050
What works better is you enter into 
shared situations with them,

371
00:20:46,350 --> 00:20:47,183
they see how deal with the situations,
you guide them in dealing with real 

372
00:20:50,081 --> 00:20:50,914
situations and that forms their system 
of values and this is what needs to 

373
00:20:55,091 --> 00:20:56,240
happen with ais.

374
00:20:56,260 --> 00:21:01,260
They need to grow up entering into real 
life situations with human beings,

375
00:21:01,751 --> 00:21:04,390
so the real life patterns of human 
values,

376
00:21:04,391 --> 00:21:05,224
which are worth a lot more than the 
families that we annunciate formally 

377
00:21:09,570 --> 00:21:10,403
wrote,
the real life pattern of human values 

378
00:21:12,490 --> 00:21:17,050
gets inculcated into the intellectual 
DNA of the AI systems.

379
00:21:17,051 --> 00:21:17,884
And this is part of what worries me 
about the way the AI field is going at 

380
00:21:22,720 --> 00:21:26,260
this moment because I mean most of the 
really powerful,

381
00:21:26,740 --> 00:21:27,573
narrow ais on the planet now are 
involved with like selling people stuff 

382
00:21:30,971 --> 00:21:31,804
they don't need spying on.
People are like figuring out who should 

383
00:21:35,321 --> 00:21:37,810
be killed or otherwise abused by some 
government.

384
00:21:37,811 --> 00:21:38,644
Right?
So if,

385
00:21:38,650 --> 00:21:39,483
if the early stage Ai's that we build 
turn into general intelligences 

386
00:21:45,040 --> 00:21:47,920
gradually and these general intelligence
is our,

387
00:21:48,190 --> 00:21:51,340
you know,
spy agents and advertising agents.

388
00:21:51,341 --> 00:21:52,430
Then like what,
what,

389
00:21:52,630 --> 00:21:56,920
what mindset do these early stage ais 
have as they grow up?

390
00:21:56,920 --> 00:21:57,753
Right?
If they don't have any problem morally 

391
00:21:59,021 --> 00:22:02,020
and ethically with manipulating us,
which we're very malleable,

392
00:22:02,021 --> 00:22:02,854
right?
We're so easy to manipulate what we're 

393
00:22:04,001 --> 00:22:04,834
teaching them.
We're teaching them to manipulate tape 

394
00:22:07,400 --> 00:22:10,300
on where rewarding them for doing it 
successfully.

395
00:22:10,660 --> 00:22:11,493
So this is,
this is one of these things that from 

396
00:22:13,481 --> 00:22:18,481
the outside point of view might not seem
to be all that intelligent it,

397
00:22:18,650 --> 00:22:22,450
it's sort of like gun laws in the U.
s living in Hong Kong.

398
00:22:23,230 --> 00:22:24,063
I mean most people don't have a bunch of
guns sitting around their house and 

399
00:22:28,060 --> 00:22:32,020
coincidentally there,
there are not that many random shootings

400
00:22:32,021 --> 00:22:34,210
happening in Hong Kong.
So yeah,

401
00:22:34,211 --> 00:22:36,100
you look in the UK.
Yeah,

402
00:22:36,101 --> 00:22:36,934
you look in the US,
it's like somehow you have laws that 

403
00:22:40,991 --> 00:22:41,824
allow random lunatics to buy all the 
guns they want and you have all these 

404
00:22:44,861 --> 00:22:45,694
people getting shot.
So similarly like from the outside you 

405
00:22:49,421 --> 00:22:50,254
could look at it like this species is 
creating the successor intelligence and 

406
00:22:56,920 --> 00:22:57,753
almost all the resources going into 
creating their successor intelligence 

407
00:23:02,260 --> 00:23:03,093
are going into making ais to do 
surveillance like military drones and 

408
00:23:09,100 --> 00:23:12,520
advertising agents to brainwash people 
into buying crap they don't need.

409
00:23:12,520 --> 00:23:14,650
Now what do you think?
Well what's wrong with this picture?

410
00:23:14,651 --> 00:23:16,600
Isn't that just because that's where the
money is,

411
00:23:16,690 --> 00:23:17,523
like this is the introduction to it and 
from then we'll find other uses and 

412
00:23:21,821 --> 00:23:24,010
applications for it.
But like right now,

413
00:23:24,070 --> 00:23:28,030
that's where the thing is,
there's a lot of other applications,

414
00:23:28,480 --> 00:23:30,940
financial app,
financially viable applicant.

415
00:23:30,960 --> 00:23:31,793
Oh yeah.
The applications that are getting the 

416
00:23:33,911 --> 00:23:37,570
most attention are the financial lowest 
hanging fruit.

417
00:23:37,571 --> 00:23:38,130
Right?
So for,

418
00:23:38,130 --> 00:23:38,620
for,
for,

419
00:23:38,620 --> 00:23:39,820
for,
for example,

420
00:23:40,450 --> 00:23:44,230
among many projects I'm doing with my 
singularity net team,

421
00:23:44,680 --> 00:23:48,890
we're looking at applying ai to diagnose
agricultural disease.

422
00:23:48,891 --> 00:23:51,530
So you can,
you can look at images of plant leaves,

423
00:23:51,531 --> 00:23:53,780
you can look at data from the soil and 
the atmosphere,

424
00:23:54,080 --> 00:23:54,913
and you can project whether disease and 
a plant is likely to progress badly or 

425
00:23:58,221 --> 00:24:00,380
not,
which tells you do you need medicine for

426
00:24:00,381 --> 00:24:02,540
the plant,
do you need pesticides now this,

427
00:24:03,440 --> 00:24:06,080
this is an interesting area of 
application.

428
00:24:06,410 --> 00:24:09,380
It's probably quite financially 
lucrative in in a way,

429
00:24:09,381 --> 00:24:13,790
but it's a more complex industry than,
than selling stuff online.

430
00:24:14,090 --> 00:24:19,090
So the fraction of resources going into 
ai for agriculture is very small.

431
00:24:19,791 --> 00:24:20,624
Then like a ecommerce or something very 
specific aspect of agriculture to 

432
00:24:24,981 --> 00:24:26,420
predicting diseases.
Yeah.

433
00:24:26,421 --> 00:24:27,200
Yeah.
But there's,

434
00:24:27,200 --> 00:24:29,000
there's a lot of specific aspects,
right?

435
00:24:29,001 --> 00:24:31,850
So I mean ai for medicine,
again,

436
00:24:31,851 --> 00:24:32,684
there's been papers on machine learning 
applied to medicine since the eighties 

437
00:24:35,931 --> 00:24:40,580
and nineties,
but the amount of effort going into that

438
00:24:40,880 --> 00:24:44,750
compared to advertising or surveillance 
is very small now.

439
00:24:44,751 --> 00:24:47,890
This has to do with the structure of the
pharmaceutical business as,

440
00:24:47,900 --> 00:24:50,120
as compared to the structure of the tech
business.

441
00:24:50,121 --> 00:24:51,710
So you know,
when you look into it,

442
00:24:51,711 --> 00:24:52,580
there's,
there's good,

443
00:24:52,610 --> 00:24:55,480
there's good reasons,
there's good reasons for,

444
00:24:55,500 --> 00:24:56,660
for everything,
right?

445
00:24:56,661 --> 00:25:00,590
But nevertheless,
the way things are coming at coming down

446
00:25:00,591 --> 00:25:05,591
right now is certain biases to the 
development of early stage.

447
00:25:07,011 --> 00:25:09,950
Ai's are,
are very market and,

448
00:25:09,951 --> 00:25:10,850
and you could,
you could,

449
00:25:10,880 --> 00:25:11,713
you could see them and I'm in,
I'm trying to do something about that 

450
00:25:14,601 --> 00:25:17,660
together with my colleagues and in 
singularity in that.

451
00:25:17,720 --> 00:25:18,553
But of course we're.
So we're sort of a David versus goliath 

452
00:25:21,681 --> 00:25:22,260
thing.

453
00:25:22,260 --> 00:25:23,093
It seemed,
well of course you're trying to do 

454
00:25:24,721 --> 00:25:27,780
something different and I think it's 
awesome what you guys are doing,

455
00:25:28,110 --> 00:25:28,943
but it just makes sense to me that the 
first applications are going to be the 

456
00:25:32,761 --> 00:25:34,740
ones that are more financially viable.
It's like,

457
00:25:35,170 --> 00:25:37,450
well,
the first applications were military,

458
00:25:37,460 --> 00:25:38,293
right until about 10 years ago,
85 percent of all funding into ai was 

459
00:25:42,071 --> 00:25:45,540
from us plus plus Western Europe 
military.

460
00:25:45,610 --> 00:25:46,600
Well,
what I'm getting at is it,

461
00:25:46,780 --> 00:25:47,613
it seems that money and and,
and commerce are inexorably linked to 

462
00:25:53,380 --> 00:25:58,270
innovation and technology because 
there's this sort of thing that we do as

463
00:25:58,271 --> 00:25:59,104
a culture where we're constantly trying 
to buy and purchase bigger and better 

464
00:26:01,661 --> 00:26:04,000
things.
We always want the newest iphone,

465
00:26:04,001 --> 00:26:05,310
the greatest,
you know,

466
00:26:05,311 --> 00:26:06,820
a laptop.
We don't want them,

467
00:26:06,980 --> 00:26:08,640
the coolest electric cars,
whatever,

468
00:26:08,680 --> 00:26:11,320
whatever it is,
and this fuels innovation.

469
00:26:11,590 --> 00:26:14,290
This,
this desire for new,

470
00:26:14,320 --> 00:26:15,153
greater things.
Materialism and a lot of ways fuels 

471
00:26:16,751 --> 00:26:18,010
innovation because this is

472
00:26:18,320 --> 00:26:19,153
those.
But I think there's an argument that as 

473
00:26:21,791 --> 00:26:22,624
we approach a technological singularity,
we need new systems because if you look 

474
00:26:28,361 --> 00:26:29,194
at that,
things have happened during the last 

475
00:26:31,780 --> 00:26:34,330
century.
What's happened is that governments have

476
00:26:34,331 --> 00:26:37,720
funded most of the core innovation I'm 
in.

477
00:26:37,721 --> 00:26:40,200
This is well known that like most of the
technology and the,

478
00:26:40,210 --> 00:26:43,120
the smartphone was funded by US 
government,

479
00:26:43,140 --> 00:26:45,030
a little about European government,
GP,

480
00:26:45,060 --> 00:26:45,893
gps and the batteries and everything.
And then companies scaled it up that 

481
00:26:51,030 --> 00:26:52,680
they made,
they made it user friendly,

482
00:26:52,681 --> 00:26:53,514
they decreased cost of manufacturing and
this process occurs with a certain time 

483
00:26:57,601 --> 00:26:58,434
cycle to it or like government spends 
decades funding core innovation and 

484
00:27:02,250 --> 00:27:03,083
universities and then industry spends 
decades figuring out how to scale it up 

485
00:27:08,281 --> 00:27:11,180
and make it palatable to users.
And you know,

486
00:27:11,200 --> 00:27:15,210
this matured probably since World War 
II,

487
00:27:15,550 --> 00:27:18,330
this sort of modality for technology 
development,

488
00:27:18,660 --> 00:27:22,650
but now that things are developing 
faster and faster and faster,

489
00:27:22,920 --> 00:27:23,753
there's sort of not time for,
for that cycle to occur where the 

490
00:27:27,241 --> 00:27:28,074
government and universities incubate new
ideas for awhile and then technology 

491
00:27:32,191 --> 00:27:32,940
scales it up.

492
00:27:32,940 --> 00:27:34,760
So genie's out of the bottle 
essentially.

493
00:27:34,761 --> 00:27:35,594
Yeah.
But we still need a lot of new amazing 

494
00:27:36,901 --> 00:27:37,734
creative innovation to happen.
But somehow or other new new structures 

495
00:27:41,511 --> 00:27:45,180
are going to have to evolve to,
to make it happen.

496
00:27:45,181 --> 00:27:48,450
And you can see everyone's struggling to
figure out what these are.

497
00:27:48,451 --> 00:27:49,284
So I mean this is why you have,
I mean you have big companies embracing 

498
00:27:51,541 --> 00:27:53,130
open source,
Google releases,

499
00:27:53,160 --> 00:27:54,780
tensorflow,
and there's a lot of,

500
00:27:55,660 --> 00:27:57,690
lot of other different things.
And I think,

501
00:27:58,140 --> 00:28:01,500
I think some projects in the 
cryptocurrency world had been looking at

502
00:28:01,501 --> 00:28:02,334
that too,
like how do we use tokens to 

503
00:28:03,211 --> 00:28:04,990
incentivize,
you know,

504
00:28:05,070 --> 00:28:05,903
independent scientists and inventors to,
to do new stuff without them asking to 

505
00:28:10,021 --> 00:28:13,410
be in the government research lab or in 
a big company.

506
00:28:13,411 --> 00:28:14,244
So I think we're going to need the 
evolution of new systems of innovation 

507
00:28:19,801 --> 00:28:22,990
and of,
of technology transfer as things are,

508
00:28:23,370 --> 00:28:26,070
are developing faster and faster and 
faster.

509
00:28:26,071 --> 00:28:26,904
And this is another thing that sort of 
got me interested in the whole 

510
00:28:30,300 --> 00:28:31,133
decentralized world and in the 
blockchain world is the promise of new 

511
00:28:35,251 --> 00:28:39,210
modes of economic and social 
organization that can,

512
00:28:39,230 --> 00:28:40,063
you know,
bring more of the world into the 

513
00:28:41,521 --> 00:28:45,150
research process and accelerate the 
technology transfer

514
00:28:45,240 --> 00:28:47,060
process.
I definitely want to talk about that,

515
00:28:47,061 --> 00:28:49,010
but one of the things that I want to ask
you is when,

516
00:28:49,370 --> 00:28:50,203
when you're discussing this,
I think what you're saying is have one 

517
00:28:54,621 --> 00:28:55,454
very important point that we need to 
move past the military gatekeepers of 

518
00:28:57,981 --> 00:28:58,720
tech.

519
00:28:58,720 --> 00:29:00,090
Not just military,
no.

520
00:29:00,220 --> 00:29:05,220
It's big tech which are at advertising 
agencies in social media.

521
00:29:06,710 --> 00:29:10,160
The things that are constantly 
predicting your next purchase,

522
00:29:10,290 --> 00:29:10,711
right?
Yeah,

523
00:29:10,711 --> 00:29:12,840
because if you,
if you think about it,

524
00:29:13,080 --> 00:29:18,080
and I'm in even in a semi democracy look
like we have in the US,

525
00:29:20,190 --> 00:29:21,023
I mean those who control the 
brainwashing of the public in essence 

526
00:29:24,811 --> 00:29:25,740
control who,
who,

527
00:29:25,760 --> 00:29:26,593
who votes for what and who controls the 
brainwashing of the public is 

528
00:29:29,821 --> 00:29:33,750
advertising agencies and who 
increasingly are the biggest advertising

529
00:29:33,751 --> 00:29:34,584
agencies or are the big tech companies 
who are accumulating everybody's data 

530
00:29:38,701 --> 00:29:42,340
and using it to,
to program their minds to buy things.

531
00:29:42,341 --> 00:29:44,980
So this is what's programmed the global 
brain of,

532
00:29:44,981 --> 00:29:45,391
of,
of,

533
00:29:45,391 --> 00:29:46,224
of the human race.
And of course there are close links 

534
00:29:48,911 --> 00:29:53,240
between big tech and the military.
Let's look at Amazon has what?

535
00:29:53,310 --> 00:29:55,810
Twenty 5,000
person headquarters in Crystal City,

536
00:29:55,811 --> 00:29:56,644
Virginia,
right next to the Pentagon and in China 

537
00:29:58,871 --> 00:30:01,570
it's even more direct and unapologetic,
right?

538
00:30:01,571 --> 00:30:06,571
So it's a new like military industrial 
advertising complex,

539
00:30:07,630 --> 00:30:12,630
which is his guiding the evolution of 
the global brain on the planet,

540
00:30:13,390 --> 00:30:15,640
which with this past election,
right,

541
00:30:15,641 --> 00:30:16,474
with all the intrusion by foreign 
entities trying to influence the 

542
00:30:20,021 --> 00:30:25,021
election that they have these giant 
houses set up to write bad stories about

543
00:30:25,871 --> 00:30:28,870
whoever they don't want to be in office.

544
00:30:29,050 --> 00:30:32,080
Yeah.
In a way that's almost a red herring,

545
00:30:32,081 --> 00:30:35,800
but I mean they're Russian stuff is 
almost a red herring.

546
00:30:36,100 --> 00:30:39,790
But it revealed what the processes are,
which,

547
00:30:39,791 --> 00:30:40,624
which are used to program because I 
think the whatever programming of 

548
00:30:44,531 --> 00:30:45,364
Americans' minds is done by the Russians
is many minuscule compared to the 

549
00:30:49,840 --> 00:30:52,800
programming of Americans' minds by my 
advisee,

550
00:30:52,810 --> 00:30:56,190
American American corporate and 
government elite.

551
00:30:56,420 --> 00:31:00,810
So it's fascinating that anybody's even 
jumping in as well as an elite.

552
00:31:01,060 --> 00:31:01,960
Sure.
It's,

553
00:31:01,990 --> 00:31:02,823
it's,
it's,

554
00:31:02,970 --> 00:31:03,803
it's,
it's interesting if you look at what's 

555
00:31:05,201 --> 00:31:07,780
happening in China that's like,
yeah,

556
00:31:07,781 --> 00:31:08,351
yeah,
yeah.

557
00:31:08,351 --> 00:31:10,580
They're,
they're way better than the,

558
00:31:10,860 --> 00:31:12,850
than we are much more horrific.
Right.

559
00:31:13,270 --> 00:31:15,340
And that's all.
It's more,

560
00:31:15,430 --> 00:31:19,060
it's more professional,
it's more polished,

561
00:31:19,061 --> 00:31:22,180
it's more centralized.
On the other hand,

562
00:31:22,390 --> 00:31:23,223
for almost everyone in China,
China is a very good place to live and 

563
00:31:26,891 --> 00:31:30,640
you know,
the level of improvement in that country

564
00:31:30,641 --> 00:31:33,190
in the last 30 years has just been 
astounding,

565
00:31:33,191 --> 00:31:34,024
right?

566
00:31:34,030 --> 00:31:34,750
I mean,
you can't,

567
00:31:34,750 --> 00:31:35,583
you can't argue with how much better 
it's gotten there since shouting took 

568
00:31:39,041 --> 00:31:39,751
over.
It's,

569
00:31:39,751 --> 00:31:41,690
it's tremendous because they're not,
they,

570
00:31:41,691 --> 00:31:44,140
they embraced capitalism to a certain 
extent.

571
00:31:44,260 --> 00:31:47,470
They've created their own unique system 
with what labels you give.

572
00:31:47,471 --> 00:31:49,750
It is,
it's almost arbitrary.

573
00:31:49,751 --> 00:31:50,584
They,
they've created their own unique system 

574
00:31:53,440 --> 00:31:54,700
as a,
you know,

575
00:31:54,760 --> 00:31:57,100
crazy hippie,
libertarian,

576
00:31:57,130 --> 00:31:59,650
a narco socialist,
freedom loving,

577
00:31:59,890 --> 00:32:00,723
maniac.
That system rubs against my grain in 

578
00:32:03,461 --> 00:32:05,380
many ways.
On the other hand,

579
00:32:05,381 --> 00:32:08,500
empirically if you look at it,
it's improved the wellbeing of a,

580
00:32:08,950 --> 00:32:09,783
of a tremendous number of people.
So hopefully it evolves and it's one 

581
00:32:13,511 --> 00:32:14,344
style.
But the way it's evolving now is not in 

582
00:32:17,021 --> 00:32:18,940
a more positive freedom.
Love.

583
00:32:18,970 --> 00:32:21,190
Well,
it's not in the more freedom,

584
00:32:21,191 --> 00:32:25,780
loving and an archaic direction.
One would say it's positive in some ways

585
00:32:25,840 --> 00:32:29,320
and negative and others like most 
complex Hong Kong.

586
00:32:29,380 --> 00:32:30,213
Why do you live there?

587
00:32:30,500 --> 00:32:32,830
Um,
I fell in love with a Chinese woman.

588
00:32:33,460 --> 00:32:34,110
Hey,
go.

589
00:32:34,110 --> 00:32:36,370
She's crazy.
She has great reason.

590
00:32:36,410 --> 00:32:38,050
We had a baby recently.
She,

591
00:32:38,230 --> 00:32:40,700
she's not from Hong,
she's from mainland China.

592
00:32:41,180 --> 00:32:46,070
I met her when she was doing her phd in 
computational linguistics and Shaman,

593
00:32:46,071 --> 00:32:47,470
but that,
that,

594
00:32:47,510 --> 00:32:51,920
that was what sort of first got me to 
spend a lot of time in China,

595
00:32:51,921 --> 00:32:52,754
but then I was doing some research at 
Hong Kong Polytechnic University and 

596
00:32:56,871 --> 00:33:01,460
then my good friend David Hansen was 
visiting me in Hong Kong.

597
00:33:01,730 --> 00:33:02,563
I introduced him to some investors there
which ended up with him bringing his 

598
00:33:06,351 --> 00:33:08,150
company,
Hanson robotics to Hong Kong.

599
00:33:08,151 --> 00:33:12,890
So now after I moved there because of 
falling in love with a rating,

600
00:33:12,891 --> 00:33:17,120
then I brought my friend David there 
than Hanson robotics.

601
00:33:17,121 --> 00:33:17,954
Grew up there and there's actually a 
good reason for Hanson robotics to be 

602
00:33:20,631 --> 00:33:22,850
there because I'm in the best place in 
the world.

603
00:33:22,850 --> 00:33:26,600
The manufacturer complex electronics is 
in Shenzhen,

604
00:33:26,601 --> 00:33:27,434
rarely across the border from Hong Kong.
So now I've been working there with 

605
00:33:30,171 --> 00:33:33,240
Hanson robotics on the Sophia robots and
other robots for,

606
00:33:33,300 --> 00:33:34,460
for,
for awhile.

607
00:33:34,670 --> 00:33:35,503
And I've accumulated the whole ai team 
there around Hanson robotics and ai and 

608
00:33:39,081 --> 00:33:40,550
singularity in that.
So I'm in by,

609
00:33:41,330 --> 00:33:42,163
by now.
I'm there because my whole ai and 

610
00:33:44,061 --> 00:33:46,490
robotics teams are there.
It makes sense.

611
00:33:46,700 --> 00:33:48,020
Um,
do you follow,

612
00:33:48,021 --> 00:33:48,854
uh,
the State Department's recommendations 

613
00:33:50,961 --> 00:33:54,640
to not use walway devices and they 
believe that they're all know,

614
00:33:55,260 --> 00:33:59,300
even heard that.
I mean if the Chinese are spying on us,

615
00:33:59,320 --> 00:34:00,350
you know,
I'm sure.

616
00:34:00,540 --> 00:34:01,373
You know,
when I lived in my lived in Washington 

617
00:34:02,571 --> 00:34:03,404
DC for nine years,
I did a bunch of consulting for various 

618
00:34:07,251 --> 00:34:08,084
government agencies there.
And my wife is a communist party member 

619
00:34:12,171 --> 00:34:13,290
actually.
Well,

620
00:34:13,370 --> 00:34:17,360
just because she joined in high school 
when it was sort of suggested for her to

621
00:34:17,361 --> 00:34:17,990
join.

622
00:34:17,990 --> 00:34:19,500
So I'm,
I'm,

623
00:34:19,530 --> 00:34:22,040
I'm sure I'm being watched by multiple 
governments.

624
00:34:22,510 --> 00:34:24,740
It doesn't,
I don't have any secrets.

625
00:34:24,810 --> 00:34:25,643
It doesn't really matter.
I'm not in the business of trying to 

626
00:34:28,761 --> 00:34:29,594
overthrow the government.
And I'm in the business of trying to 

627
00:34:33,020 --> 00:34:37,700
bypass traditional governments and 
traditional monetary systems and all the

628
00:34:37,701 --> 00:34:42,701
rest by creating new methods of 
organization of people and information.

629
00:34:43,281 --> 00:34:44,114
You understand that what you personally.
But it is unusual if the government is 

630
00:34:46,641 --> 00:34:48,850
actually spying on people through these 
divides out.

631
00:34:48,851 --> 00:34:51,080
It's unusual.
It's unusual at all.

632
00:34:51,081 --> 00:34:51,580
I mean,
I,

633
00:34:51,580 --> 00:34:52,413
I,
I mean without going into too much 

634
00:34:54,741 --> 00:34:55,574
detail,
like when I was in dc working with 

635
00:34:57,741 --> 00:34:58,574
various government agencies,
it became clear there is tremendously 

636
00:35:02,721 --> 00:35:07,721
more information obtained by government 
agencies than most people realize,

637
00:35:08,560 --> 00:35:09,393
well,
this was,

638
00:35:09,440 --> 00:35:10,273
this was true way before snowden and 
wikileaks and all these revelations and 

639
00:35:14,240 --> 00:35:19,240
what is publicly understood now is 
probably not the full scope of,

640
00:35:21,740 --> 00:35:24,590
of,
of the information that governments have

641
00:35:24,591 --> 00:35:24,960
others.

642
00:35:24,960 --> 00:35:28,790
So I'm in privacy is,
is pretty much dead.

643
00:35:28,791 --> 00:35:31,820
And David Brin,
do you know David Brin?

644
00:35:31,880 --> 00:35:32,713
No.
You should definitely interview David 

645
00:35:33,900 --> 00:35:34,733
and he's an amazing guy,
but he's a well known science fiction 

646
00:35:37,651 --> 00:35:39,760
writer.
He's based in southern California,

647
00:35:39,820 --> 00:35:43,500
San Diego.
But he wrote a book in Oh,

648
00:35:43,500 --> 00:35:47,430
years ago called the transparent society
where he said there's two possibilities,

649
00:35:47,431 --> 00:35:48,264
surveillance and sousveillance.
It's like the power elite watching 

650
00:35:51,361 --> 00:35:55,530
everyone or everyone watching everyone.
I think everyone watching everyone.

651
00:35:55,531 --> 00:35:55,861
Well,
yeah,

652
00:35:55,861 --> 00:35:56,694
but he.
So he articulated this as the 

653
00:35:58,590 --> 00:36:02,340
essentially the only two viable 
possibilities and he's like,

654
00:36:02,760 --> 00:36:07,230
we should be choosing and then creating 
which of these alternatives we want.

655
00:36:07,231 --> 00:36:08,064
So now,
now the world is starting to understand 

656
00:36:11,341 --> 00:36:13,710
what he was talking about.
But back when he wrote that book,

657
00:36:14,160 --> 00:36:15,630
you wrote the book.
Oh,

658
00:36:15,631 --> 00:36:18,730
I can't remember.
I mean it was well more than a decade.

659
00:36:18,731 --> 00:36:21,270
That gets weird,
but some people just nail it on the head

660
00:36:21,480 --> 00:36:22,680
decades in advance.

661
00:36:22,680 --> 00:36:25,050
I mean,
most of the things that are happening in

662
00:36:25,051 --> 00:36:29,130
the world now we're foreseen by a 
stanislaw lem,

663
00:36:29,131 --> 00:36:32,010
the Polish Arthur mentioned volunteer 
church.

664
00:36:32,011 --> 00:36:34,380
And a friend of mine who was the founder
of Russian Ai.

665
00:36:34,381 --> 00:36:38,580
He wrote a book called the phenomenon of
science in the late sixties.

666
00:36:38,581 --> 00:36:39,390
Then,
you know,

667
00:36:39,390 --> 00:36:40,223
in 1971 or two when I was a little kid,
I read a book called the promethium 

668
00:36:45,540 --> 00:36:48,960
project by a Princeton physics called 
Gerald Gerald Fund Berg.

669
00:36:48,990 --> 00:36:51,000
You've read a physical book when you're 
five years old.

670
00:36:51,570 --> 00:36:54,660
I started reading when I was two and my 
grandfather was a physicist,

671
00:36:54,670 --> 00:36:59,490
so I was reading a lot of stuff then.
But he Feinberg in this book,

672
00:36:59,580 --> 00:37:00,960
he said,
you know,

673
00:37:00,961 --> 00:37:01,794
within the next few decades,
humanity is going to create 

674
00:37:03,750 --> 00:37:04,583
nanotechnology,
it's going to create machines smarter 

675
00:37:06,151 --> 00:37:09,780
than people and it's going to create the
technology to allow human,

676
00:37:09,781 --> 00:37:12,210
biological immortality.
And the question will be,

677
00:37:12,211 --> 00:37:14,700
do we want to use these technologies,
you know,

678
00:37:14,701 --> 00:37:15,534
to promote rampant consumerism or do we 
want to use these technologies to 

679
00:37:19,831 --> 00:37:20,550
promote,
you know,

680
00:37:20,550 --> 00:37:23,820
spiritual growth of our,
of our consciousness into new dimensions

681
00:37:23,821 --> 00:37:28,290
of experience and what fonder proposed 
in this book in the late sixties,

682
00:37:28,291 --> 00:37:29,124
which I read in the early seventies,
he proposed the UN should send a task 

683
00:37:32,861 --> 00:37:34,620
force out to go to everyone in the 
world,

684
00:37:34,621 --> 00:37:35,454
every little African village and educate
the world about nanotech life extension 

685
00:37:40,741 --> 00:37:41,574
and an Agi and get the whole world to 
vote on whether we should develop these 

686
00:37:45,451 --> 00:37:49,500
technologies toward consumerism or 
toward consciousness expansion.

687
00:37:49,500 --> 00:37:51,510
So I read this from a little kid.
It's like,

688
00:37:52,200 --> 00:37:54,840
this is almost obvious.
This makes total sense.

689
00:37:54,841 --> 00:37:55,674
Like,
why?

690
00:37:56,580 --> 00:38:00,570
Why does everyone understand this?
Then I tried to explain this to people.

691
00:38:01,150 --> 00:38:03,840
I'm like,
Oh shit,

692
00:38:03,841 --> 00:38:07,650
I guess it's gonna be awhile till the 
world catches on,

693
00:38:07,651 --> 00:38:08,484
so.
So I instead decided I should build a 

694
00:38:10,080 --> 00:38:10,913
spacecraft,
go away from the world at rapid speed 

695
00:38:13,321 --> 00:38:16,890
and come back after like a million years
or something when the world was far more

696
00:38:16,891 --> 00:38:19,080
advanced.
So covered in dust.

697
00:38:19,140 --> 00:38:19,990
Yeah.
Right.

698
00:38:20,100 --> 00:38:20,933
So now we'll.
Then you go another million years or so 

699
00:38:25,830 --> 00:38:26,663
now,
pretty much the world agrees that life 

700
00:38:28,831 --> 00:38:29,664
extension,
Agi and nanotechnology or plausible 

701
00:38:32,641 --> 00:38:35,190
things that may come about in the near 
future.

702
00:38:35,920 --> 00:38:36,753
The same question is,
is there that that Feinberg saw like 50 

703
00:38:41,381 --> 00:38:42,214
years ago,
right?

704
00:38:42,270 --> 00:38:43,690
The same question.
Is there like,

705
00:38:43,691 --> 00:38:44,524
did we develop this for rampant 
consumerism or do we develop this for 

706
00:38:49,990 --> 00:38:54,990
amazing new dimensions of consciousness 
expansion and mental growth,

707
00:38:57,430 --> 00:38:58,263
but the UN is not in fact educating the 
world about this and pulling them to 

708
00:39:03,791 --> 00:39:06,990
decide democratically what to do on.

709
00:39:07,010 --> 00:39:08,260
On.
On the other hand,

710
00:39:08,620 --> 00:39:09,453
there's the possibility that by 
bypassing governments and the UN and 

711
00:39:13,361 --> 00:39:17,530
doing something decentralized,
you can create a democratic framework,

712
00:39:17,550 --> 00:39:20,040
you know,
within which you know,

713
00:39:20,050 --> 00:39:20,883
a broad swath of the world can be 
involved in a participatory way in 

714
00:39:23,741 --> 00:39:24,574
guiding the direction of these advances.
Do you think that it's possible that 

715
00:39:27,880 --> 00:39:31,330
instead of choosing that we're just 
going to have multiple directions,

716
00:39:31,331 --> 00:39:33,690
that it's growing in that there's going 
to be consumer.

717
00:39:33,760 --> 00:39:38,680
There will be multiple directions and 
it's that that's inevitable.

718
00:39:38,830 --> 00:39:43,830
It's more a matter of whether anything 
besides the military advertising complex

719
00:39:45,371 --> 00:39:46,204
gets a shake.
So I mean if you look in the software 

720
00:39:48,161 --> 00:39:52,000
development world,
open source is an amazing thing,

721
00:39:52,020 --> 00:39:56,290
right?
Linux is awesome and it's led to so much

722
00:39:56,291 --> 00:39:58,420
ai being open,
open source now.

723
00:39:58,600 --> 00:40:03,600
Now Open source didn't have to actually 
take over the entire software world like

724
00:40:04,210 --> 00:40:07,570
Richard Stallman one and in order to 
have a huge impact,

725
00:40:07,571 --> 00:40:07,880
right?

726
00:40:07,880 --> 00:40:12,250
It's enough that it's a major force.
So I mean it's very hippy concept,

727
00:40:12,251 --> 00:40:14,230
isn't it?
Open source and a lot of ways in a way,

728
00:40:14,231 --> 00:40:15,570
but,
but yet ibm,

729
00:40:15,620 --> 00:40:18,970
IBM has probably thousands of people 
working on Linux.

730
00:40:18,971 --> 00:40:22,660
Right?
So like apple began as a hippie concept,

731
00:40:22,661 --> 00:40:24,460
but it became very practical.
Right?

732
00:40:24,461 --> 00:40:29,461
So I mean something like 75 percent of 
all the servers running the Internet are

733
00:40:29,861 --> 00:40:31,600
based in Linux.
You know,

734
00:40:31,601 --> 00:40:34,750
the vast majority of mobile phone oss 
is,

735
00:40:34,751 --> 00:40:36,160
is,
is Linux,

736
00:40:36,161 --> 00:40:38,050
right?
So this Hippie,

737
00:40:38,051 --> 00:40:40,800
vast majority being android is Linux.
Yeah.

738
00:40:40,810 --> 00:40:41,643
Yeah.
So I'm in this hippie crazy thing where 

739
00:40:44,951 --> 00:40:45,784
no one owns the code.
It didn't have to overtake the whole 

740
00:40:50,351 --> 00:40:51,184
software economy and become everything 
to become highly valuable and then 

741
00:40:55,121 --> 00:40:58,390
inject a different dimension into 
things.

742
00:40:58,391 --> 00:41:02,950
And I think the same is true with 
decentralized ai,

743
00:41:02,951 --> 00:41:05,680
which we're looking at with singularity 
and that it doesn't have,

744
00:41:05,710 --> 00:41:08,590
we don't have to actually put Google 
and,

745
00:41:08,591 --> 00:41:12,700
and the US and Chinese military and 
tencent out of business.

746
00:41:12,701 --> 00:41:13,534
Right.
Although if that happens that that's 

747
00:41:15,160 --> 00:41:16,890
fine.
But W we,

748
00:41:17,260 --> 00:41:18,093
it's enough that we become an extremely 
major player in that ecosystem so that 

749
00:41:23,191 --> 00:41:24,290
this,
you know,

750
00:41:24,400 --> 00:41:25,233
participatory and benefit oriented 
aspect becomes a really significant 

751
00:41:30,610 --> 00:41:35,420
component of how humanity is,
is developing general intelligence.

752
00:41:36,140 --> 00:41:36,973
W is accepted,
generally accepted that human beings 

753
00:41:39,341 --> 00:41:41,500
will consistently and constantly 
innovate.

754
00:41:42,010 --> 00:41:44,200
Right.
It just seems to be characteristics that

755
00:41:44,500 --> 00:41:47,290
we have.
Why do you think that is and what do you

756
00:41:47,291 --> 00:41:48,610
think that,
especially when it,

757
00:41:48,611 --> 00:41:51,280
in terms of creating something like 
artificial intelligence,

758
00:41:51,281 --> 00:41:54,880
like why build our successors?
Like why,

759
00:41:54,940 --> 00:41:56,410
why do that?
Like what is,

760
00:41:56,740 --> 00:42:00,490
what is it about us that makes us want 
to constantly make bigger,

761
00:42:00,491 --> 00:42:01,324
better things?

762
00:42:02,600 --> 00:42:03,433
Well,
that's an interesting question in the 

763
00:42:06,141 --> 00:42:06,974
history of biology,
which I may not be the most qualified 

764
00:42:12,441 --> 00:42:13,274
person to answer.
It is an interesting question and I 

765
00:42:16,401 --> 00:42:17,234
think it has something to do with the 
weird way in which we embody various 

766
00:42:21,711 --> 00:42:24,560
contradictions that we're always trying 
to resolve locally.

767
00:42:25,070 --> 00:42:28,130
You mentioned ents and answer social 
animals,

768
00:42:28,131 --> 00:42:28,964
right?
Worse.

769
00:42:29,150 --> 00:42:32,970
Like cats are very individual.
We're like trapped between the two,

770
00:42:33,200 --> 00:42:37,040
like we're somewhat individual and 
somewhat social.

771
00:42:37,160 --> 00:42:40,340
And then.
And then since we created civilization,

772
00:42:40,680 --> 00:42:41,391
it's,
it's,

773
00:42:41,391 --> 00:42:45,530
it's even worse because we have certain 
aspects which are,

774
00:42:45,980 --> 00:42:49,700
which are wanting to conform with the 
group and the tribe and others which are

775
00:42:49,701 --> 00:42:52,640
wanting to innovate and break out of 
the.

776
00:42:52,641 --> 00:42:53,474
And we're sort of trapped in these 
biological and cultural contradictions 

777
00:42:56,871 --> 00:42:57,704
which tend to drive innovation.
But I think there's a lot there that no 

778
00:43:01,011 --> 00:43:05,420
one understands in the roots of the 
human psyche evolutionarily.

779
00:43:05,421 --> 00:43:09,160
But as an empirical fact,
what you said is,

780
00:43:09,460 --> 00:43:10,600
is,
is,

781
00:43:10,620 --> 00:43:11,660
is very true,
right?

782
00:43:11,661 --> 00:43:14,390
Like we,
we're driven to seek novelty.

783
00:43:14,600 --> 00:43:15,433
We're driven to create new things.
And this is probably one of the factors 

784
00:43:20,661 --> 00:43:21,494
which is driving the creation of Ai.
I don't think that alone would make the 

785
00:43:26,091 --> 00:43:28,750
creation of ai inevitable,
but the thing,

786
00:43:29,660 --> 00:43:32,450
why don't you think it would make it 
inevitable if we consistently innovate?

787
00:43:32,750 --> 00:43:34,430
And it's always been a concept.
I mean,

788
00:43:34,431 --> 00:43:37,010
you were talking about the concept 
existing 30 plus years ago.

789
00:43:37,720 --> 00:43:38,553
Well,
I think a key point is that there's 

790
00:43:40,000 --> 00:43:40,833
tremendous practical economic advantage 
and status advantage to be gotten from 

791
00:43:48,460 --> 00:43:49,293
ai right now.
And this is driving the advancement of 

792
00:43:52,781 --> 00:43:56,920
ai to be incredibly rapid,
right?

793
00:43:56,940 --> 00:44:01,940
Because there are some things that are 
interesting and would use a lot of human

794
00:44:02,891 --> 00:44:05,710
innovation,
but they get very few resources.

795
00:44:05,711 --> 00:44:06,770
So for example,
my,

796
00:44:07,060 --> 00:44:10,510
my oldest son's Arthur Oostra,
he's doing his phd now.

797
00:44:10,560 --> 00:44:15,060
What is this Zarathustra?
My kids are Pfister,

798
00:44:15,060 --> 00:44:15,591
Amadeus,
Debbie lawn,

799
00:44:15,591 --> 00:44:20,591
ulysses Shaharazad.
And then a new one is corky qrs,

800
00:44:22,000 --> 00:44:22,833
which is an acronym for quantum 
organized rational expanding 

801
00:44:25,031 --> 00:44:28,690
intelligence.
So I was never happy with Ben.

802
00:44:28,691 --> 00:44:30,930
It's a very boring name.
So yeah.

803
00:44:31,120 --> 00:44:31,953
Yeah.
I had,

804
00:44:32,410 --> 00:44:33,243
I had,
I had to do something more interesting 

805
00:44:34,470 --> 00:44:35,460
with my kids.
Anyway,

806
00:44:35,820 --> 00:44:36,653
Zarathustra is doing his phd on the 
application of machine learning to 

807
00:44:40,771 --> 00:44:45,120
automated theorem proving basically make
ais that can do mathematics better.

808
00:44:45,480 --> 00:44:49,800
And to me that's the most important 
thing we could be applying ai to because

809
00:44:49,940 --> 00:44:50,773
you know,
mathematics is the key to all modern 

810
00:44:51,991 --> 00:44:54,660
science and engineering.
My Phd was in math originally,

811
00:44:54,930 --> 00:44:55,763
but the amount of resources going into 
ai for automating mathematics is not 

812
00:45:00,691 --> 00:45:01,524
large at this present moment,
although that's a beautiful and amazing 

813
00:45:05,161 --> 00:45:07,830
area for invention and innovation and 
creativity.

814
00:45:07,980 --> 00:45:12,980
So I think what's driving our rapid 
pushed or dozing ai,

815
00:45:13,320 --> 00:45:14,153
I mean it's not just our creative drive,
it's the fact that there's tremendous 

816
00:45:18,420 --> 00:45:21,990
economic value,
military value and human value.

817
00:45:21,991 --> 00:45:24,150
I mean curing diseases,
teaching kids,

818
00:45:24,510 --> 00:45:25,343
there's tremendous value in almost 
everything that's important to human 

819
00:45:27,721 --> 00:45:29,850
beings in,
in building ai,

820
00:45:29,851 --> 00:45:30,684
right?
So you put that together with our drive 

821
00:45:32,701 --> 00:45:36,690
to create an interface and this becomes 
an almost unstoppable force within,

822
00:45:36,980 --> 00:45:40,410
within human society.
And what we've seen in the last,

823
00:45:40,770 --> 00:45:43,620
you know,
three to five years is suddenly,

824
00:45:43,830 --> 00:45:44,663
you know,
national leaders and Titans of industry 

825
00:45:46,710 --> 00:45:48,750
and even like pop stars,
right?

826
00:45:49,080 --> 00:45:52,350
They've woken up to the concept that 
wow,

827
00:45:52,410 --> 00:45:53,243
smarter and smarter ai is real and this 
is going to get better and better like 

828
00:45:56,550 --> 00:46:01,140
within years to decades,
not centuries to millennia.

829
00:46:01,141 --> 00:46:06,141
So now the cat's out of the bag,
nobody's going to put it back.

830
00:46:06,420 --> 00:46:08,220
And it's about,
you know,

831
00:46:08,221 --> 00:46:13,221
how can we direct it in the most 
beneficial possible way?

832
00:46:13,411 --> 00:46:15,600
And as you say,
it doesn't have to be just one possible.

833
00:46:15,601 --> 00:46:16,434
We were like,
what will I look forward to personally 

834
00:46:18,780 --> 00:46:22,800
is bifurcating myself into an array of 
possible Benton's like it.

835
00:46:23,190 --> 00:46:28,190
I'd like to get one copy of me fuse 
itself with a superhuman ai mind and you

836
00:46:28,311 --> 00:46:29,220
know,
become,

837
00:46:29,250 --> 00:46:30,001
become,
uh,

838
00:46:30,001 --> 00:46:33,480
a god or something beyond the God.
You wouldn't even be myself.

839
00:46:33,510 --> 00:46:35,820
God,
I wouldn't even be myself anymore.

840
00:46:35,821 --> 00:46:36,331
Right.
I mean,

841
00:46:36,331 --> 00:46:40,290
you would lose all concepts of human 
self and identity,

842
00:46:40,291 --> 00:46:42,540
but it would be the point of even 
holding any of it,

843
00:46:42,710 --> 00:46:43,940
you know,
that's,

844
00:46:44,050 --> 00:46:45,030
that's for the future.

845
00:46:45,210 --> 00:46:47,300
That's for the Mega Ben to decide,
right?

846
00:46:47,750 --> 00:46:48,180
Yeah.
Yeah.

847
00:46:48,180 --> 00:46:49,013
On the other hand,
I'd like to let me remained in human 

848
00:46:51,391 --> 00:46:52,560
form,
you know,

849
00:46:52,561 --> 00:46:52,950
get,
get,

850
00:46:52,950 --> 00:46:53,783
get rid of a death and disease and the 
psychological issues and just live live 

851
00:46:59,761 --> 00:47:01,420
happily forever,
you know,

852
00:47:01,430 --> 00:47:02,263
in,
in the peoples who watched over by the 

853
00:47:03,241 --> 00:47:04,740
machines of loving grace.
Right.

854
00:47:04,741 --> 00:47:05,270
So,
I mean,

855
00:47:05,270 --> 00:47:06,103
you can have,
it doesn't have to be either or because 

856
00:47:08,350 --> 00:47:09,183
once,
once you can scan your brain and body 

857
00:47:11,521 --> 00:47:13,560
and three d print new copies of 
yourself,

858
00:47:13,770 --> 00:47:17,040
you could have multiple of.
I mean,

859
00:47:17,060 --> 00:47:21,210
that's there's a lot of mass energy in 
the universe and the universe,

860
00:47:21,211 --> 00:47:24,030
so that's assuming that we can escape 
this planet because of you.

861
00:47:24,060 --> 00:47:27,180
If you're talking about if you're going 
to make themselves,

862
00:47:27,350 --> 00:47:30,010
how can you live in a world with a 
billion donald trump's

863
00:47:30,280 --> 00:47:33,050
because literally that's what we're 
talking about,

864
00:47:33,100 --> 00:47:33,933
talking about people being able to 
reproduce themselves and just having 

865
00:47:38,441 --> 00:47:43,000
this idea that they would like their ego
to exist in multiple different forms.

866
00:47:43,030 --> 00:47:43,863
Whether it's some super symbiotic form 
that's connected to artificial 

867
00:47:47,171 --> 00:47:48,004
intelligence or some biological form 
that's immortal or some other form that 

868
00:47:51,610 --> 00:47:52,443
stands just as a normal human being.
As we know in 2018 have you have 

869
00:47:56,801 --> 00:47:59,440
multiple versions of yourself over and 
over and over again like that.

870
00:47:59,441 --> 00:48:00,274
That's where you're.
So once you get to the point where you 

871
00:48:02,501 --> 00:48:03,334
have a super human general intelligence 
that can do things like fully scanned 

872
00:48:07,121 --> 00:48:10,600
the human brain and body and three d 
print more of them.

873
00:48:10,990 --> 00:48:11,823
By that point,
you're at a level where scarcity of 

874
00:48:16,510 --> 00:48:21,340
material resources is not an issue at 
the human scale of doing things.

875
00:48:21,341 --> 00:48:23,720
So I actually have human resources in 
terms of what

876
00:48:24,640 --> 00:48:25,473
mass energy,
scared scarcity of molecules to print 

877
00:48:27,671 --> 00:48:29,810
more copies of yourself.
I think that,

878
00:48:30,020 --> 00:48:30,853
I think that's not going to be the issue
at that point when people are worried 

879
00:48:33,521 --> 00:48:34,354
about is environmental concerns of 
overpopulation is people are worried 

880
00:48:36,491 --> 00:48:38,680
about what they see in front of their 
faces right now,

881
00:48:38,681 --> 00:48:39,514
but people are not.
Most people are not thinking deeply 

882
00:48:45,491 --> 00:48:46,324
enough about what potential would be 
there once you had super human ai is 

883
00:48:52,120 --> 00:48:56,320
doing the calculation manufacturing and 
the thinking.

884
00:48:56,321 --> 00:48:57,450
I mean,
I mean the.

885
00:48:57,451 --> 00:49:00,550
The amount of energy in the single grain
of sand,

886
00:49:00,880 --> 00:49:04,480
if you had an ai able to to 
appropriately leverage that.

887
00:49:04,481 --> 00:49:08,350
That energy is,
is tremendously more than than most than

888
00:49:08,351 --> 00:49:09,184
most people think.
And the amount of computing power in a 

889
00:49:11,321 --> 00:49:14,170
grain of sand.
It's like a quadrillion times.

890
00:49:14,171 --> 00:49:17,020
All the people on Earth put together.
So I mean,

891
00:49:17,021 --> 00:49:19,990
what do you mean by that amount of 
computing power?

892
00:49:20,800 --> 00:49:21,633
There's.
Well the amount of computing power that 

893
00:49:23,711 --> 00:49:24,544
could be achieved by reorganizing the 
elementary particles in the grain of 

894
00:49:29,291 --> 00:49:29,681
sand.
Yeah,

895
00:49:29,681 --> 00:49:30,370
there,
there,

896
00:49:30,370 --> 00:49:31,203
there's,
there's a number in physics called the 

897
00:49:32,201 --> 00:49:33,034
beacon stone bound,
which is the maximum amount of 

898
00:49:35,171 --> 00:49:36,004
information that can be,
can be stored in a certain amount of 

899
00:49:39,071 --> 00:49:39,904
mass energy here,
so that that if the laws of physics as 

900
00:49:42,401 --> 00:49:43,234
we know them now are correct,
which they certainly aren't than the 

901
00:49:45,360 --> 00:49:46,193
[inaudible].
That would be the amount of computing 

902
00:49:47,920 --> 00:49:49,900
you can do in a certain amount of mass 
and energy.

903
00:49:50,110 --> 00:49:52,540
We're very,
very far from that limit right now.

904
00:49:52,541 --> 00:49:53,374
Right,
so I mean,

905
00:49:53,490 --> 00:49:57,280
my point is once you have something a 
thousand times smarter than people,

906
00:49:57,550 --> 00:50:01,800
what we imagined to be the limits now 
doesn't matter

907
00:50:01,990 --> 00:50:02,823
too.
The issues that we're dealing with in 

908
00:50:04,471 --> 00:50:05,304
terms of environmental concerns,
that could all potentially be almost 

909
00:50:08,011 --> 00:50:10,050
certainly going to be irrelevant,
irrelevant.

910
00:50:10,190 --> 00:50:14,340
There may be others problem issues that 
we can't even conceive at this moment,

911
00:50:14,850 --> 00:50:15,683
but the intelligence will be so vastly 
superior to what we have currently that 

912
00:50:19,411 --> 00:50:20,244
they'll be able to find solutions to 
virtually every single problem with 

913
00:50:22,810 --> 00:50:24,900
Shima,
a ocean,

914
00:50:24,901 --> 00:50:26,750
fish,
deep population.

915
00:50:26,790 --> 00:50:28,430
All that stuff will just arrangements

916
00:50:28,430 --> 00:50:33,210
of molecules freaking me out.
But to hear that though,

917
00:50:33,260 --> 00:50:34,580
environmental people don't want to hear 
that.

918
00:50:34,581 --> 00:50:35,580
Well,
I mean I

919
00:50:35,980 --> 00:50:40,980
also on the everyday life basis until we
have these super ais,

920
00:50:42,850 --> 00:50:43,683
I don't like the garbage,
Washington from the beach near my house 

921
00:50:46,121 --> 00:50:46,780
either.
Right?

922
00:50:46,780 --> 00:50:50,190
So I mean,
but on an everyday basis,

923
00:50:50,300 --> 00:50:53,620
of course we wanted to promote health in
our bodies and in our,

924
00:50:53,820 --> 00:50:54,653
in our environments right now,
as long as there's no measurable 

925
00:50:58,361 --> 00:51:02,920
uncertainty regarding when the 
Benevolent Super Ais will will,

926
00:51:02,921 --> 00:51:04,780
will,
will come about still,

927
00:51:04,781 --> 00:51:05,614
I think the main question isn't whether 
once you have beneficially disposed 

928
00:51:10,061 --> 00:51:10,894
super ai,
it could solve all our current petty 

929
00:51:13,061 --> 00:51:13,894
little problems.
The question is can we wade through the 

930
00:51:17,561 --> 00:51:18,394
mock of modern human society and 
psychology to create this beneficial 

931
00:51:22,871 --> 00:51:24,850
super ai in,
in,

932
00:51:24,851 --> 00:51:25,684
in the first place I got,
I believe I know how to create a 

933
00:51:28,061 --> 00:51:30,610
beneficial super ai,
but it's a lot of,

934
00:51:30,670 --> 00:51:31,503
a lot of work to get there and of course
there's many teams around the world 

935
00:51:35,680 --> 00:51:36,513
working on vaguely similar projects now.
It's not obvious what kind of super ai 

936
00:51:43,570 --> 00:51:45,700
we're actually going to get once we get 
there.

937
00:51:45,970 --> 00:51:48,370
Yeah,
it's all just guesses at this point,

938
00:51:48,371 --> 00:51:49,204
right?
It's more less educated guesses 

939
00:51:51,281 --> 00:51:52,740
depending on who's doing the guessing.

940
00:51:52,890 --> 00:51:53,723
You say that it's almost like we're in a
race of the the primitive primate 

941
00:51:57,551 --> 00:51:58,384
biology versus the potentially 
beneficial and benevolent artificial 

942
00:52:02,951 --> 00:52:03,784
intelligence that this.
The best aspects of this primate can 

943
00:52:06,371 --> 00:52:09,610
create that it's almost a race to get 
who's going to win.

944
00:52:09,611 --> 00:52:13,960
Is it the warmongers and the greedy 
whores that are smashing the world under

945
00:52:13,961 --> 00:52:14,794
its boots or is it the scientists that 
are going to figure out some super 

946
00:52:18,401 --> 00:52:20,570
intelligent way to solve all of our 
problems?

947
00:52:20,600 --> 00:52:22,040
Let's look at it more as a

948
00:52:22,370 --> 00:52:27,370
look at it more as a,
as a struggle between different modes of

949
00:52:27,980 --> 00:52:31,910
social organization than individual 
people.

950
00:52:31,911 --> 00:52:32,744
I mean,
look what when I worked in DC with 

951
00:52:34,761 --> 00:52:35,594
intelligence agencies,
most of the people I met there were 

952
00:52:40,250 --> 00:52:44,630
really nice human beings who believed 
they were doing the best for the world.

953
00:52:45,140 --> 00:52:46,910
Even if some of the things that we're 
doing,

954
00:52:46,911 --> 00:52:50,990
like I thought were very much not for 
the best of the world.

955
00:52:51,190 --> 00:52:52,023
Soon I'm in military mode of 
organization or large corporations as a 

956
00:52:57,711 --> 00:52:58,544
mode of organization are in my view,
not journaling can lead to beneficial 

957
00:53:04,251 --> 00:53:05,084
outcomes for the overall species and and
for the global brain and the scientific 

958
00:53:09,411 --> 00:53:10,244
community.
The open source community I think are 

959
00:53:12,501 --> 00:53:15,270
better modes of organization and you 
know the,

960
00:53:15,530 --> 00:53:16,363
the better aspects of the blockchain and
crypto community have a better mode of 

961
00:53:20,301 --> 00:53:22,040
organization.
So I think if,

962
00:53:22,370 --> 00:53:23,203
if this sort of open decentralized of 
organization can marshal more resources 

963
00:53:30,241 --> 00:53:35,241
as opposed to this centralized,
authoritarian mode of organization,

964
00:53:35,580 --> 00:53:36,413
then I think things are going to come 
out for the better and it's not so much 

965
00:53:39,631 --> 00:53:41,650
about bad people versus good people.

966
00:53:41,730 --> 00:53:44,690
You can look at like the corporate mode 
of organization.

967
00:53:44,710 --> 00:53:45,543
It's almost a virus that that's 
colonized a bunch of humanity and is 

968
00:53:48,990 --> 00:53:51,250
sucking people into working according to
the,

969
00:53:51,360 --> 00:53:52,193
to this mode and even if they're really 
good people and the individual task 

970
00:53:56,340 --> 00:54:00,620
they're working on isn't bad in itself.
They're working within this,

971
00:54:01,160 --> 00:54:06,160
this mode that's leading their work to 
be used for ultimately a non good end.

972
00:54:06,810 --> 00:54:07,643
Yeah.
That is a fascinating thing about 

973
00:54:08,191 --> 00:54:09,210
corporations,
isn't it?

974
00:54:09,510 --> 00:54:10,343
That the diffusion of responsibility and
being a part of a gigantic group that 

975
00:54:14,341 --> 00:54:17,460
you as an individual don't feel 
necessarily connected,

976
00:54:17,461 --> 00:54:21,190
are responsible to the.
Even the CEO isn't fully responsible.

977
00:54:21,191 --> 00:54:26,070
Like if the CEO does something that 
isn't in accordance with the high higher

978
00:54:26,071 --> 00:54:28,200
goals of the organization,
they're just replaced.

979
00:54:28,201 --> 00:54:31,380
Right.
So I mean there's no one person who's in

980
00:54:31,381 --> 00:54:32,550
charge.
It's really like,

981
00:54:32,600 --> 00:54:33,570
it's like an that column.

982
00:54:34,320 --> 00:54:35,153
It's like its own organism and I mean 
it's us who have let these organisms 

983
00:54:41,430 --> 00:54:44,940
become parasites on humanity in this 
way.

984
00:54:45,360 --> 00:54:46,193
In some ways the Asian countries are a 
little more intelligent than western 

985
00:54:50,131 --> 00:54:50,964
countries and the Asian governments 
realize the power of corporations to 

986
00:54:56,971 --> 00:54:57,804
mold society and there's a bit more 
feedback between the government and 

987
00:55:02,440 --> 00:55:05,460
corporations which can be for better or 
for worse,

988
00:55:05,461 --> 00:55:06,294
but then in in America there's some 
ethos of like free markets and free 

989
00:55:12,361 --> 00:55:13,194
enterprise,
which is really not taking into account 

990
00:55:16,560 --> 00:55:18,900
the oligopolistic nature of,
of,

991
00:55:18,901 --> 00:55:20,520
of,
of modern markets,

992
00:55:20,550 --> 00:55:21,383
but in Asian countries isn't it that the
government is actually suppressing 

993
00:55:24,451 --> 00:55:25,284
inflammation as well.
They're also suppressing Google and 

994
00:55:27,561 --> 00:55:28,770
South Korea?
No,

995
00:55:28,800 --> 00:55:31,560
I'm in South Korea.
If you look at my only ones,

996
00:55:32,190 --> 00:55:33,023
well Singapore,
I'm in Singapore is ruthless in their 

997
00:55:36,151 --> 00:55:41,151
drug laws and some of the archaic,
well us far worse though,

998
00:55:41,400 --> 00:55:42,233
Singapore,
it gives you the death penalty for 

999
00:55:43,111 --> 00:55:43,944
marijuana.

1000
00:55:43,980 --> 00:55:44,813
They do it.
South Korea is an example which has 

1001
00:55:49,591 --> 00:55:50,424
roughly the same level of personal 
freedoms as the US more in some ways 

1002
00:55:54,301 --> 00:55:56,940
less than others.
Massive electronic and innovation.

1003
00:55:57,330 --> 00:55:58,163
Well,
interesting thing they're politically 

1004
00:56:01,201 --> 00:56:02,034
is.
I mean they were poorer than two thirds 

1005
00:56:03,451 --> 00:56:04,284
of sub Saharan African nations in the 
late sixties and it is through the 

1006
00:56:07,741 --> 00:56:08,574
government intentionally stimulating 
corporate development toward 

1007
00:56:11,821 --> 00:56:15,360
manufacturing in electronics that they 
grew up so that,

1008
00:56:15,660 --> 00:56:18,120
that,
that now I'm in,

1009
00:56:18,780 --> 00:56:23,250
I'm not holding that up as a paragon for
the future or anything,

1010
00:56:23,260 --> 00:56:24,093
but it does show that there's,
there's many modes of organization of 

1011
00:56:28,151 --> 00:56:33,070
people and resources other than the ones
that we take for granted in the US.

1012
00:56:33,400 --> 00:56:37,180
I don't think Samsung and lg are the 
ideal for the future either though.

1013
00:56:37,181 --> 00:56:40,270
I mean I'm much more interested in,
you know,

1014
00:56:41,270 --> 00:56:43,330
your son in blockchain.
I'm interested in.

1015
00:56:43,680 --> 00:56:45,390
I'm interested in open source.

1016
00:56:45,460 --> 00:56:47,620
I'm interested in,
in blockchain.

1017
00:56:47,621 --> 00:56:50,350
I'm basically,
I'm interested in anything that's,

1018
00:56:50,780 --> 00:56:55,390
you know,
open and participatory and disruptive as

1019
00:56:55,391 --> 00:56:56,224
well because I think that's,
I think that is the way to be ongoingly 

1020
00:57:01,450 --> 00:57:04,630
just disruptive and open source is a 
good example of that.

1021
00:57:04,631 --> 00:57:07,000
Like when the open source movement 
started,

1022
00:57:07,030 --> 00:57:08,920
they weren't thinking about machine 
learning,

1023
00:57:09,250 --> 00:57:09,930
but you know,
the,

1024
00:57:09,930 --> 00:57:10,763
the fact that open source is out there 
and there's been prevalent in the 

1025
00:57:13,211 --> 00:57:14,044
software world that pave the way for ai 
to now be centered on open source 

1026
00:57:19,841 --> 00:57:20,674
algorithms.
So right now even though big companies 

1027
00:57:22,721 --> 00:57:25,450
and governments dominate the scalable 
rollout of Ai,

1028
00:57:25,660 --> 00:57:26,493
the invention of new ai algorithms is 
mostly done by people creating new code 

1029
00:57:31,391 --> 00:57:32,700
and putting it,
putting it on,

1030
00:57:32,701 --> 00:57:35,380
on get hub or get lab or other open 
source repository.

1031
00:57:35,570 --> 00:57:39,580
Retired repositories been sources self 
explanatory in its title.

1032
00:57:40,270 --> 00:57:41,103
Pretty much people kind of understand 
what it is that means that various 

1033
00:57:43,781 --> 00:57:44,614
coders get to share in this,
this code and the source code and they 

1034
00:57:48,400 --> 00:57:52,900
get to innovate and they all get to 
participate and use each other's work.

1035
00:57:52,930 --> 00:57:53,763
Right,
right.

1036
00:57:53,910 --> 00:57:55,300
Um,
but blockchain,

1037
00:57:55,470 --> 00:57:58,180
and it's confusing for a lot of people.
Could you explain that?

1038
00:57:58,990 --> 00:58:00,740
Sure.
I'm in block block.

1039
00:58:00,800 --> 00:58:04,520
Block chain itself is almost a misnomer,
so we're confused.

1040
00:58:04,600 --> 00:58:05,720
Things are confusing,
uh,

1041
00:58:05,770 --> 00:58:07,060
at every level,
right.

1042
00:58:07,061 --> 00:58:12,061
So we can start with the idea of a 
distributed ledger,

1043
00:58:12,521 --> 00:58:16,690
which is basically like a distributed,
an excel spreadsheet or database.

1044
00:58:16,691 --> 00:58:21,310
It's just a story of information which 
is not stored just in one place,

1045
00:58:21,311 --> 00:58:23,770
but there's copies of it in lots of 
different places.

1046
00:58:24,100 --> 00:58:27,810
Every time my copy of it is updated,
everyone else's copy of it has,

1047
00:58:27,840 --> 00:58:29,380
has,
has got to be updated.

1048
00:58:29,381 --> 00:58:33,690
And then then there's various bells and 
whistles like sharding where you know,

1049
00:58:33,730 --> 00:58:34,563
it can be broken in many pieces and each
piece is stored many places or 

1050
00:58:37,121 --> 00:58:37,954
something.
So That's a distributed ledger and 

1051
00:58:39,881 --> 00:58:41,980
that's just distributed computing.
Now what,

1052
00:58:42,670 --> 00:58:43,503
what makes it more interesting is when 
you layer decentralized control onto 

1053
00:58:47,411 --> 00:58:47,950
that.

1054
00:58:47,950 --> 00:58:48,783
So imagine you have this distributed 
excel spreadsheet or distributed 

1055
00:58:51,971 --> 00:58:52,804
database.
There's copies of it stored in a 

1056
00:58:54,760 --> 00:58:57,010
thousand places,
but to update it,

1057
00:58:57,220 --> 00:59:01,270
you need like 500 of those thousand 
people who own the copies to vote.

1058
00:59:01,271 --> 00:59:02,470
Yeah.
Let's do that update,

1059
00:59:02,530 --> 00:59:03,363
right?
So then then you have a distributed 

1060
00:59:05,321 --> 00:59:09,550
store of data and you have like a 
democratic voting mechanism to determine

1061
00:59:09,760 --> 00:59:13,010
when all those copies can get,
can get updated together,

1062
00:59:13,011 --> 00:59:13,844
right?
So then then what you have is a data 

1063
00:59:16,391 --> 00:59:17,224
storage and update mechanism that's 
controlled in a democratic way by the 

1064
00:59:20,900 --> 00:59:24,050
group of participants rather than by any
one central controller.

1065
00:59:24,320 --> 00:59:26,870
And that that can have all sorts of 
advantages.

1066
00:59:26,871 --> 00:59:29,010
I mean for one thing it means that you 
know,

1067
00:59:29,060 --> 00:59:29,893
there's no one controller who can go 
rogue and screw with the day without 

1068
00:59:32,841 --> 00:59:33,674
telling anyone.
It also means there is no one has some 

1069
00:59:36,051 --> 00:59:39,020
limitation go hold a gun to their head 
and shoot them for,

1070
00:59:39,320 --> 00:59:43,370
for what data updates were made because 
know controlled democratically by,

1071
00:59:43,790 --> 00:59:44,780
by everybody,
right?

1072
00:59:44,780 --> 00:59:47,570
It has ramifications in terms of,
you know,

1073
00:59:47,571 --> 00:59:51,800
legal defensibility and I mean you could
have some people in Iran,

1074
00:59:51,801 --> 00:59:54,440
some in China,
some in the US and,

1075
00:59:54,450 --> 00:59:55,283
and updates to this whole distributed 
data store are made by democratic 

1076
00:59:59,991 --> 01:00:00,824
decision of all the participants.
Somewhere cryptography comes in is when 

1077
01:00:04,221 --> 01:00:05,330
I vote,
I don't have to say,

1078
01:00:05,331 --> 01:00:06,164
yeah,
this is Ben Gursel voting for this 

1079
01:00:07,461 --> 01:00:11,300
update to be accepted or not.
It's just ID number one,

1080
01:00:11,301 --> 01:00:11,781
three,
five,

1081
01:00:11,781 --> 01:00:12,291
seven,
two,

1082
01:00:12,291 --> 01:00:13,190
six,
four.

1083
01:00:13,340 --> 01:00:16,220
And then encryption is used to make sure
that,

1084
01:00:16,490 --> 01:00:17,070
you know,
it's,

1085
01:00:17,070 --> 01:00:17,903
it's the same guy voting every time that
it claims to be with without needing 

1086
01:00:23,961 --> 01:00:26,060
like your,
your passport number or something.

1087
01:00:26,061 --> 01:00:26,894
Right.
What's ironic about it is it's probably 

1088
01:00:27,741 --> 01:00:31,790
one of the best ways ever conceived to 
actually vote in this country.

1089
01:00:31,791 --> 01:00:33,170
Yeah,
sure.

1090
01:00:33,171 --> 01:00:36,660
It would be kind of ironic.
There's a lot of applications for the,

1091
01:00:36,890 --> 01:00:37,450
the,
the,

1092
01:00:37,450 --> 01:00:37,980
the,
the.

1093
01:00:37,980 --> 01:00:39,130
That's the,
that's right.

1094
01:00:39,200 --> 01:00:41,340
So the,
so that.

1095
01:00:41,430 --> 01:00:46,430
I mean that's the core mechanism though 
where the block chain comes from is like

1096
01:00:46,611 --> 01:00:51,110
a data structure where to store the data
in this distributed database,

1097
01:00:51,410 --> 01:00:55,190
it's stored in a chain of blocks or each
block contains data.

1098
01:00:55,510 --> 01:00:56,343
The thing is not every so-called 
blockchain system even uses a chain of 

1099
01:01:00,141 --> 01:01:04,010
blocks and I'd like some use a tree or a
graph of blocks or something.

1100
01:01:04,040 --> 01:01:06,660
So that term,
I mean it's,

1101
01:01:06,740 --> 01:01:07,573
it's an alright terms like ai,
like just one of those terms were stuck 

1102
01:01:10,851 --> 01:01:11,720
with.
It's one,

1103
01:01:11,810 --> 01:01:12,643
it's one of those terms were stuck with,
even though it's not quite technically 

1104
01:01:16,490 --> 01:01:19,580
not quite technically accurate.
I mean anymore.

1105
01:01:19,581 --> 01:01:20,700
I mean w w,
W,

1106
01:01:20,701 --> 01:01:21,590
W,
I don't know,

1107
01:01:21,591 --> 01:01:22,910
another buzzword for it,
right.

1108
01:01:22,940 --> 01:01:23,773
What it is is a,
it's a distributed ledger with 

1109
01:01:25,971 --> 01:01:26,804
encryption and decentralized control and
blockchain is the buzzword that's come 

1110
01:01:30,471 --> 01:01:31,220
about for that.

1111
01:01:31,220 --> 01:01:32,053
Now what,
what got me interested in blockchain 

1112
01:01:34,671 --> 01:01:37,550
really is this decentralized control 
aspect.

1113
01:01:37,551 --> 01:01:38,730
So my,
my,

1114
01:01:38,800 --> 01:01:41,120
my wife,
well them with for 10 years now,

1115
01:01:41,360 --> 01:01:43,490
she dug up recently something I'd 
forgotten,

1116
01:01:43,491 --> 01:01:48,491
which is a webpage and made in 1995,
like a long time ago where I'd said,

1117
01:01:49,160 --> 01:01:49,993
hey,
I'm going to run for president on the 

1118
01:01:51,141 --> 01:01:56,141
decentralization platform for which I'd 
completely forgotten that crazy idea.

1119
01:01:56,540 --> 01:01:57,373
I was very young then.
I had no idea within an annoying job 

1120
01:01:59,271 --> 01:02:00,104
being president would be.
But the so that the idea of 

1121
01:02:04,790 --> 01:02:08,900
decentralized control seemed very 
important to me back then,

1122
01:02:08,901 --> 01:02:09,734
which is well,
before bitcoin was invented because I 

1123
01:02:11,721 --> 01:02:12,980
could see,
you know,

1124
01:02:12,981 --> 01:02:16,580
a global brain is evolving on the planet
involving humans,

1125
01:02:16,581 --> 01:02:18,620
computers,
communication devices,

1126
01:02:18,960 --> 01:02:22,850
and we don't want this global brain to 
be controlled by a small elite.

1127
01:02:22,860 --> 01:02:24,730
We want the global parent to be 
controlled in the,

1128
01:02:24,731 --> 01:02:26,220
in a decentralized way.

1129
01:02:26,220 --> 01:02:27,480
So,
so that,

1130
01:02:27,840 --> 01:02:30,450
that's really the,
the beauty of this,

1131
01:02:30,451 --> 01:02:32,910
a blockchain infrastructure.
And what,

1132
01:02:33,270 --> 01:02:34,103
what got me interested in the practical 
technologies of block chain was really 

1133
01:02:36,991 --> 01:02:41,991
one etherium came out and you add the 
notion of a smart contract,

1134
01:02:42,091 --> 01:02:44,890
which was the theory etherium.
Yeah.

1135
01:02:44,891 --> 01:02:46,530
So what is that?
Well,

1136
01:02:46,650 --> 01:02:49,920
so the first blockchain technology was 
Bitcoin,

1137
01:02:49,921 --> 01:02:52,320
right?
Which is a well known cryptocurrency now

1138
01:02:53,130 --> 01:02:55,590
if theory.
IOM is another cryptocurrency,

1139
01:02:55,591 --> 01:02:58,080
which is the number to cryptocurrency 
right now.

1140
01:02:58,740 --> 01:03:00,510
That's how the loop I am.
Did you know about it?

1141
01:03:01,140 --> 01:03:02,700
You did.
However,

1142
01:03:03,090 --> 01:03:07,420
ethereum came along with a really nice 
software framework.

1143
01:03:07,421 --> 01:03:11,890
So it's not just like a digital money,
like bit coin is,

1144
01:03:12,330 --> 01:03:13,163
but it theory.
IOM has a programming language called 

1145
01:03:16,530 --> 01:03:17,363
solidity that came with it and this 
programming language let's you right 

1146
01:03:20,551 --> 01:03:23,670
where they're called smart contracts and
again,

1147
01:03:23,671 --> 01:03:26,190
that sort of a misnomer because a smart 
contract,

1148
01:03:26,420 --> 01:03:28,800
it doesn't have to be either smart or a 
contract,

1149
01:03:28,830 --> 01:03:29,663
right,
but it was a cool name and then if it's 

1150
01:03:32,461 --> 01:03:34,590
really a smart contract,
it's a contract.

1151
01:03:34,620 --> 01:03:35,453
It's like a programmable transaction so 
you you can program a legal contract or 

1152
01:03:41,431 --> 01:03:43,980
you can program of a financial 
transaction.

1153
01:03:44,220 --> 01:03:47,340
So a smart contract,
it's a.

1154
01:03:47,610 --> 01:03:48,443
it's a persistent piece of software that
embodies like a secure encrypted 

1155
01:03:53,281 --> 01:03:55,770
transaction between between multiple 
parties,

1156
01:03:55,920 --> 01:04:00,920
so pretty much like anything on the back
end of a bank's website or a transaction

1157
01:04:03,691 --> 01:04:04,524
between two companies online,
a purchasing relationship between you 

1158
01:04:07,640 --> 01:04:08,473
and the website online.
This could all be stripped it in in the 

1159
01:04:10,831 --> 01:04:11,664
smart contract in this secure way,
and then it wouldn't be automated in 

1160
01:04:14,911 --> 01:04:15,744
this simple and standard way.
So the vision that Vitalik Buterin who 

1161
01:04:19,381 --> 01:04:20,214
was the main creator behind the theory 
had is to basically make the internet 

1162
01:04:24,330 --> 01:04:25,163
into a giant computing mechanism rather 
than mostly like an information storage 

1163
01:04:30,181 --> 01:04:34,470
and retrieval mechanism.
Make the Internet into a giant computer,

1164
01:04:34,770 --> 01:04:35,603
but making it really simple programming 
language for scripting transactions 

1165
01:04:38,941 --> 01:04:39,774
among different computers and different 
parties on the Internet where you have 

1166
01:04:42,690 --> 01:04:43,523
encryption and you have democratic 
decision making and distributed storage 

1167
01:04:47,161 --> 01:04:49,410
of information like programmed into 
this,

1168
01:04:49,650 --> 01:04:50,790
this world computer.

1169
01:04:50,790 --> 01:04:53,640
Right?
And then that was a really cool idea.

1170
01:04:53,820 --> 01:04:58,020
And the etherium blockchain and solidity
programming language made it really easy

1171
01:04:58,021 --> 01:04:58,854
to do that.
So it made it really easy to program 

1172
01:05:00,481 --> 01:05:05,481
like distributed secure transaction and 
computing systems on the Internet.

1173
01:05:06,241 --> 01:05:08,040
So I saw this,
I thought,

1174
01:05:08,041 --> 01:05:08,874
well,
like now we finally have the tool set 

1175
01:05:12,720 --> 01:05:16,440
that's needed to implement.
Some of this is very popular.

1176
01:05:16,590 --> 01:05:17,423
I mean,
I mean basically almost every ico that 

1177
01:05:21,551 --> 01:05:24,670
was done the last couple of years was 
done on the ethereum blockchain.

1178
01:05:25,000 --> 01:05:27,130
What's an ICO?
Initial coin offering.

1179
01:05:27,230 --> 01:05:27,730
Oh,
okay.

1180
01:05:27,730 --> 01:05:29,220
So for Bitcoins,
for,

1181
01:05:29,680 --> 01:05:31,150
I'm sorry,
cryptocurrency,

1182
01:05:31,190 --> 01:05:35,200
cryptocurrency use this technology 
brings.

1183
01:05:35,320 --> 01:05:36,153
Right?
So what happened in the last couple of 

1184
01:05:37,690 --> 01:05:38,523
years is a bunch of people realized you 
could use this etherium programming 

1185
01:05:45,161 --> 01:05:49,450
framework to create a new 
cryptocurrency,

1186
01:05:49,750 --> 01:05:50,583
like a new artificial money.
And then you can try to get people to 

1187
01:05:54,101 --> 01:05:58,650
use your new artificial money for 
certain types of artificial coins.

1188
01:05:59,270 --> 01:06:03,820
It may be more popular.
Is Bitcoin,

1189
01:06:03,821 --> 01:06:07,480
right?
Bitcoin is by far the most popular.

1190
01:06:07,540 --> 01:06:10,630
The most delirium is number two.
And there's a bunch of others.

1191
01:06:10,631 --> 01:06:11,464
I mean Hartwick comparison,
like how much bigger is bitcoin and 

1192
01:06:14,381 --> 01:06:16,930
ethereum?
I Dunno.

1193
01:06:16,960 --> 01:06:21,940
Five factor of three to five.
So maybe just a factor of two.

1194
01:06:21,941 --> 01:06:22,774
Now it's actually last year at theory 
and almost took over bitcoin when 

1195
01:06:27,671 --> 01:06:29,050
bitcoin started crashing.
Yeah.

1196
01:06:29,051 --> 01:06:30,700
Yeah.
Now if their room is back down,

1197
01:06:30,701 --> 01:06:35,701
there might be half or a third of the 
fluctuating value of these things.

1198
01:06:36,130 --> 01:06:37,930
To My,
to my mind,

1199
01:06:38,290 --> 01:06:39,123
creating artificial monies is one tiny 
bit of the potential of what you could 

1200
01:06:45,911 --> 01:06:48,790
do with the whole blockchain tool.
Set it.

1201
01:06:49,120 --> 01:06:54,120
It happened to become popular initially 
because it's where the money is,

1202
01:06:55,061 --> 01:06:57,250
right?
I've been right writing it is.

1203
01:06:57,251 --> 01:06:59,740
It is money and that's interesting to 
people,

1204
01:07:00,010 --> 01:07:00,843
but on the other hand,
what it's really about is making world 

1205
01:07:05,351 --> 01:07:05,860
computer.

1206
01:07:05,860 --> 01:07:09,220
It's about scripting with a simple 
programming language,

1207
01:07:09,221 --> 01:07:12,550
all sorts of transactions between 
people,

1208
01:07:12,551 --> 01:07:14,410
companies,
whatever,

1209
01:07:14,411 --> 01:07:17,080
all sorts of exchanges of,
of information.

1210
01:07:17,081 --> 01:07:20,440
So I mean it's about decentralized 
voting mechanisms.

1211
01:07:20,441 --> 01:07:21,274
It's about Ai's being able to send data 
and processing for each other and pay 

1212
01:07:26,741 --> 01:07:29,280
each other for their transactions.
So I'm in,

1213
01:07:29,550 --> 01:07:32,310
there's,
it's about automating supply chains and,

1214
01:07:32,311 --> 01:07:33,144
and,
and shipping and ecommerce so that 

1215
01:07:34,900 --> 01:07:35,970
there's an in,
in,

1216
01:07:36,000 --> 01:07:37,210
in,
in essence,

1217
01:07:37,390 --> 01:07:38,223
you know,
just like computers and the Internet 

1218
01:07:41,140 --> 01:07:41,973
started with a certain small set of 
applications and then pervaded almost 

1219
01:07:45,851 --> 01:07:46,690
everything,
right?

1220
01:07:46,860 --> 01:07:48,880
It's the same way with blockchain 
technology,

1221
01:07:48,881 --> 01:07:49,714
like it started with digital money,
but the core technology is going to 

1222
01:07:53,561 --> 01:07:54,394
pervade almost everything because 
there's almost no domain of human 

1223
01:07:57,731 --> 01:08:02,731
pursuit that couldn't use like security 
through cryptography,

1224
01:08:03,010 --> 01:08:04,270
some sort of,
you know,

1225
01:08:04,271 --> 01:08:08,920
participatory decision making and then 
distributed storage of information.

1226
01:08:08,921 --> 01:08:09,190
Right?

1227
01:08:09,190 --> 01:08:11,980
So these things are also valuable for 
ai,

1228
01:08:11,981 --> 01:08:13,960
which is how I got into it in the first 
place.

1229
01:08:13,961 --> 01:08:16,400
I mean,
if you're making a very,

1230
01:08:16,401 --> 01:08:20,440
very powerful ai that is going to,
you know,

1231
01:08:20,480 --> 01:08:23,660
gradually through the practical value 
delivers,

1232
01:08:23,670 --> 01:08:25,910
she will grow up to be more and more and
more intelligent.

1233
01:08:26,300 --> 01:08:27,133
I mean this ai shouldn't be able to 
engage a large party of people and ais 

1234
01:08:31,220 --> 01:08:33,520
and participatory decision making.
They,

1235
01:08:33,521 --> 01:08:38,521
I should be able to store information in
a widely distributed way and,

1236
01:08:38,980 --> 01:08:41,330
and ai certainly shouldn't be able to 
use,

1237
01:08:41,470 --> 01:08:43,540
you know,
security and encryption development,

1238
01:08:43,580 --> 01:08:44,413
who are the parties involved in this 
operation and I mean these are the key 

1239
01:08:47,241 --> 01:08:49,550
things behind behind blockchain 
technology.

1240
01:08:49,551 --> 01:08:50,384
So I'm in the fact the fact that 
blockchain began with artificial 

1241
01:08:53,571 --> 01:08:54,404
currencies to me is a detail of history.
Just like the fact the fact that the 

1242
01:08:58,461 --> 01:09:01,820
Internet began as like a nuclear early 
warning system right then.

1243
01:09:01,860 --> 01:09:03,920
And it did.
It's good for that.

1244
01:09:03,921 --> 01:09:05,270
But it's,
as it happens,

1245
01:09:05,271 --> 01:09:07,970
it's also even better for a lot of other
things.

1246
01:09:08,060 --> 01:09:08,893
So the,

1247
01:09:09,170 --> 01:09:13,730
the solution for the financial situation
that we find ourselves in.

1248
01:09:14,310 --> 01:09:19,310
One of the more interesting things about
cryptocurrencies that someone said,

1249
01:09:19,731 --> 01:09:20,211
okay,
look,

1250
01:09:20,211 --> 01:09:21,044
obviously we all kind of agreed that our
financial institutions are very flawed 

1251
01:09:24,370 --> 01:09:27,590
system that we operate under is it's 
very fucked up,

1252
01:09:27,800 --> 01:09:29,480
so how do we fix that?
Well,

1253
01:09:29,540 --> 01:09:33,310
sending the super nerds and so they 
figure out a new number,

1254
01:09:33,330 --> 01:09:35,220
get ascend into super,
a super,

1255
01:09:36,140 --> 01:09:38,690
super nervous super.
Obviously.

1256
01:09:38,930 --> 01:09:43,930
Who is the guy that they think that this
fake person maybe not real,

1257
01:09:44,420 --> 01:09:46,910
that came up with bitcoin.
Komodo.

1258
01:09:47,450 --> 01:09:50,370
Do you have any suspicions as to who 
this is a,

1259
01:09:50,720 --> 01:09:55,430
I can neither confirm nor deny that you 
wouldn't be on the inside.

1260
01:09:55,431 --> 01:09:57,560
We'll talk later,
but that,

1261
01:09:57,680 --> 01:09:59,450
this is,
it's very,

1262
01:10:00,890 --> 01:10:04,010
it's very interesting,
but it's also very promising.

1263
01:10:04,200 --> 01:10:05,033
I,
I've like high optimism for 

1264
01:10:06,621 --> 01:10:07,454
cryptocurrencies because I think that 
kids today are looking at it with much 

1265
01:10:11,721 --> 01:10:13,960
more open eyes than uh,
you know,

1266
01:10:14,000 --> 01:10:14,833
grandfathers,
grandfathers are looking at bitcoin and 

1267
01:10:16,660 --> 01:10:20,780
the father.
You're exceptional one,

1268
01:10:21,140 --> 01:10:24,350
but there's a lot of people that are 
older that just,

1269
01:10:24,620 --> 01:10:26,630
they're not open to accepting these 
ideas,

1270
01:10:26,631 --> 01:10:27,464
but I think kids today in particular,
the ones that have grown up with the 

1271
01:10:32,121 --> 01:10:34,430
internet as a constant force in their 
life,

1272
01:10:34,800 --> 01:10:35,633
they're.
I think they're more likely to embrace 

1273
01:10:37,251 --> 01:10:38,480
something along those lines.

1274
01:10:38,610 --> 01:10:38,941
Well,
yeah,

1275
01:10:38,941 --> 01:10:39,774
so there's no doubt that you know,
cryptographic formulations of money are 

1276
01:10:46,501 --> 01:10:49,260
going to become the standard.
The question,

1277
01:10:49,270 --> 01:10:51,630
do you think that's going to be the 
standard that will have been?

1278
01:10:51,800 --> 01:10:52,980
Yeah.
However,

1279
01:10:53,280 --> 01:10:57,480
it could happen potentially in a very 
uninteresting way.

1280
01:10:57,710 --> 01:11:00,540
How's.
You could just have the dollar.

1281
01:11:00,570 --> 01:11:01,403
I mean the government could just say we 
will create this cryptographic token 

1282
01:11:05,820 --> 01:11:07,110
which counts as a dollar.
I mean,

1283
01:11:07,111 --> 01:11:09,030
most dollars are just electronic.
Anyway,

1284
01:11:09,670 --> 01:11:10,690
so,
so what,

1285
01:11:10,830 --> 01:11:11,663
what,
what habitually happens is technologies 

1286
01:11:14,131 --> 01:11:14,964
that are invented to subvert the 
establishment are converted to a forum 

1287
01:11:19,381 --> 01:11:22,290
where they help bolster the 
establishment.

1288
01:11:22,291 --> 01:11:25,680
Instead,
I'm in and in financial services.

1289
01:11:25,681 --> 01:11:26,514
This happens very rapidly,
like pay Pal Peter Teal on this guy 

1290
01:11:30,001 --> 01:11:34,590
started paypal thinking they were going 
to obsolete Fiat currency and make an an

1291
01:11:34,591 --> 01:11:39,030
alternative to the currencies run by by 
nation states instead.

1292
01:11:39,210 --> 01:11:42,540
They were driven to make it a credit 
card processing front end.

1293
01:11:42,570 --> 01:11:43,403
Right.
So,

1294
01:11:43,600 --> 01:11:44,433
so that's one thing that could happen 
with crypto currency is it just becomes 

1295
01:11:48,691 --> 01:11:50,770
a mechanism for,
you know,

1296
01:11:51,130 --> 01:11:56,130
governments and big companies and banks 
to do the things more efficiently.

1297
01:11:56,251 --> 01:11:57,084
So what,
what's interesting isn't so much the 

1298
01:11:59,131 --> 01:11:59,964
digital money aspect,
although it is in some ways a great way 

1299
01:12:02,461 --> 01:12:04,160
to do digital money.
Wow.

1300
01:12:04,230 --> 01:12:05,063
What,
What's interesting is with all the 

1301
01:12:07,501 --> 01:12:10,350
flexibility it gives you to script,
you know,

1302
01:12:10,351 --> 01:12:15,351
complex computing networks in there is 
the possibility to new forms of,

1303
01:12:17,700 --> 01:12:21,000
you know,
participatory democratic self organizing

1304
01:12:21,001 --> 01:12:23,490
networks.
So blockchain,

1305
01:12:23,520 --> 01:12:27,180
like the internet or computing is a very
flexible medium.

1306
01:12:27,420 --> 01:12:30,060
You could use it to make tools of 
oppression or,

1307
01:12:30,180 --> 01:12:32,790
or you could use it to make tools of 
amazing growth,

1308
01:12:32,791 --> 01:12:37,170
growth and liberation.
And obviously we know which one I'm more

1309
01:12:37,171 --> 01:12:38,004
interested in.

1310
01:12:38,180 --> 01:12:39,530
Yeah.
Now what,

1311
01:12:39,620 --> 01:12:40,453
what is blockchain?
What is blockchain being currently used 

1312
01:12:44,661 --> 01:12:45,820
for?
Like what,

1313
01:12:45,850 --> 01:12:48,770
what different applications,
because it's not just cryptocurrency,

1314
01:12:49,040 --> 01:12:50,690
they use a bunch of different things 
now,

1315
01:12:50,691 --> 01:12:50,960
right?

1316
01:12:50,960 --> 01:12:55,040
They are.
I would say it's very early stage.

1317
01:12:55,041 --> 01:12:55,874
So probably the how early.
Well the heaviest users of blockchain 

1318
01:13:00,560 --> 01:13:05,000
now are probably inside large financial 
services companies actually.

1319
01:13:05,001 --> 01:13:09,230
So if you look at it theory in the 
project I mentioned,

1320
01:13:09,231 --> 01:13:12,740
so it theory,
ms is run by an open source,

1321
01:13:12,830 --> 01:13:15,500
an open foundation,
ethereum foundation.

1322
01:13:16,070 --> 01:13:19,220
Then there's a consulting company called
consensus,

1323
01:13:19,490 --> 01:13:20,323
which is a totally separate organization
that was founded by Joe Lubin who was 

1324
01:13:23,931 --> 01:13:28,310
one of the founders of ethereum in the 
early days and consensus as you know,

1325
01:13:28,370 --> 01:13:33,320
it's funded a bunch of the work within 
the ethereum foundation and community,

1326
01:13:33,680 --> 01:13:34,513
but consensus has done a lot of 
contracts just working with governments 

1327
01:13:37,791 --> 01:13:38,624
and big companies to customize code 
based on the theory to help with their 

1328
01:13:43,131 --> 01:13:47,600
internal operations.
So actually a lot of the practical value

1329
01:13:47,601 --> 01:13:50,900
has been with stuff that isn't in the 
public eye that much,

1330
01:13:50,901 --> 01:13:53,320
but it's like back end and in,
in,

1331
01:13:53,321 --> 01:13:54,980
in inside of companies.

1332
01:13:54,980 --> 01:13:59,980
And in terms of practical customer 
facing uses of,

1333
01:14:00,520 --> 01:14:02,060
of cryptocurrency.
I mean,

1334
01:14:02,061 --> 01:14:04,490
no,
the Tron blockchain,

1335
01:14:04,491 --> 01:14:07,880
which is different than the theory that 
has a bunch of games on it,

1336
01:14:07,910 --> 01:14:09,950
for example,
and some online gambling for,

1337
01:14:10,340 --> 01:14:11,530
for that matter.
So that,

1338
01:14:11,531 --> 01:14:14,350
that's uh,
that that's gotten a lot of users,

1339
01:14:14,351 --> 01:14:17,110
but the online games like how,
how do they use that?

1340
01:14:17,730 --> 01:14:18,970
Oh,
that's a payment mechanism.

1341
01:14:20,040 --> 01:14:20,873
But that there,
this is one of the things there's a lot 

1342
01:14:23,801 --> 01:14:27,700
of handwringing about in the 
cryptocurrency world now is gambling.

1343
01:14:27,940 --> 01:14:28,773
No,
just the fact that there aren't that 

1344
01:14:29,801 --> 01:14:33,490
many big consumer facing uses of,
of,

1345
01:14:33,491 --> 01:14:35,040
of,
of cryptocurrency.

1346
01:14:35,041 --> 01:14:36,250
I mean,
I mean everyone would,

1347
01:14:36,400 --> 01:14:40,180
everyone would like there to be.
That was the idea and this is one of the

1348
01:14:40,181 --> 01:14:44,320
things we're aiming at with our 
singularity in the project is to,

1349
01:14:44,820 --> 01:14:45,390
you know,
we,

1350
01:14:45,390 --> 01:14:50,390
we by putting ai on the blockchain in a 
highly effective way.

1351
01:14:51,550 --> 01:14:55,330
And then we're also,
we have these two tiers.

1352
01:14:55,331 --> 01:14:56,164
So we have the singularity net 
foundation which is creating this open 

1353
01:14:59,501 --> 01:15:04,501
source decentralized platform in which 
ais can talk to other ais and you know,

1354
01:15:05,351 --> 01:15:08,620
like ants in the colony group together 
to form smarter and smarter Ai.

1355
01:15:09,010 --> 01:15:13,090
Then we're spinning off a company called
the singularity studio,

1356
01:15:13,270 --> 01:15:18,250
which will use this decentralized 
platform to help big companies integrate

1357
01:15:18,251 --> 01:15:19,084
ai into their operation.
So with the singular these studio 

1358
01:15:21,971 --> 01:15:22,804
company,
we want to get all these big companies 

1359
01:15:25,120 --> 01:15:30,120
using the ai tools in the singular unit 
platform and then we want to drive,

1360
01:15:30,730 --> 01:15:32,200
you know,
massive usage of,

1361
01:15:32,201 --> 01:15:36,100
of blockchain in the singularity in that
way.

1362
01:15:36,310 --> 01:15:39,310
So that's if we're successful with what 
we're doing,

1363
01:15:39,790 --> 01:15:41,280
this will be,
you know,

1364
01:15:41,290 --> 01:15:42,123
within a year from now or something.
By far the biggest usage of blockchain 

1365
01:15:46,630 --> 01:15:47,463
outside of financial exchange is our use
of blockchain within singularity unit 

1366
01:15:51,671 --> 01:15:56,671
for ai basically for customers to get 
the AI services that they need for their

1367
01:15:57,911 --> 01:15:58,744
businesses and then for ais to transact 
with other ai is paying other ais for 

1368
01:16:03,131 --> 01:16:04,300
doing services for them.

1369
01:16:04,330 --> 01:16:05,410
Because this,
this,

1370
01:16:05,411 --> 01:16:06,244
this I think is is a path forward.
It's like a society and economy of 

1371
01:16:10,001 --> 01:16:12,850
minds.
It's not like one monolithic Ai.

1372
01:16:13,180 --> 01:16:17,110
It's a whole bunch of ais carried by 
different people all over the world with

1373
01:16:17,111 --> 01:16:20,740
not only are in the marketplace 
providing services to customers,

1374
01:16:21,010 --> 01:16:21,843
but each ai is asking questions of each 
other and then rating each other of how 

1375
01:16:25,751 --> 01:16:29,350
good they are sending data to each other
and paying each other for the services.

1376
01:16:29,351 --> 01:16:30,184
So this,
this like network of ais can emerge and 

1377
01:16:33,361 --> 01:16:37,850
intelligence on the whole network level 
as well as there being intelligence city

1378
01:16:37,880 --> 01:16:39,430
in each,
each component.

1379
01:16:39,640 --> 01:16:42,780
And is it also fascinating to you that 
this is not dependent upon nation?

1380
01:16:42,781 --> 01:16:44,920
So this is a world that ever.
I think.

1381
01:16:45,520 --> 01:16:47,830
I think that's going to be important 
once,

1382
01:16:48,100 --> 01:16:50,890
once it starts to get a very high level 
of intelligence.

1383
01:16:50,890 --> 01:16:54,150
So in the early stages,
okay,

1384
01:16:54,730 --> 01:16:56,070
what would it hurt?
Like if,

1385
01:16:56,100 --> 01:16:59,700
if I had my own database to central 
record of,

1386
01:16:59,740 --> 01:17:00,940
of everything.
Like I'm,

1387
01:17:00,941 --> 01:17:03,370
I'm an honest person.
I'm not going to rip anyone off,

1388
01:17:04,450 --> 01:17:05,283
but once we start to make a transition 
towards artificial general intelligence 

1389
01:17:10,880 --> 01:17:11,713
in this global decentralized network,
which has component ais from every 

1390
01:17:14,961 --> 01:17:18,050
country on the planet,
like at that point,

1391
01:17:18,051 --> 01:17:20,330
once it's clear,
you're getting toward a gi,

1392
01:17:20,750 --> 01:17:23,900
a lot of people wanting to step in and 
control this thing,

1393
01:17:24,140 --> 01:17:25,280
you know,
by law,

1394
01:17:25,281 --> 01:17:27,710
by military might,
by any means necessary.

1395
01:17:28,010 --> 01:17:30,020
By that point.
The fact that you have this open,

1396
01:17:30,021 --> 01:17:30,854
decentralized network under underpinning
everything like this gives an amazing 

1397
01:17:34,791 --> 01:17:37,550
resilience to what you're doing.
Who can shut down Linux,

1398
01:17:37,551 --> 01:17:40,260
you can shut down Bitcoin,
nobody can right you,

1399
01:17:40,380 --> 01:17:41,520
you,
you want Ai,

1400
01:17:41,540 --> 01:17:44,480
you want ai to be like that.
You want to be a global,

1401
01:17:44,660 --> 01:17:45,860
you know,
upsurge of,

1402
01:17:45,861 --> 01:17:46,694
of creativity and,
and mutual benefit from people all over 

1403
01:17:49,851 --> 01:17:52,820
the planet,
which no powerful party can,

1404
01:17:52,880 --> 01:17:56,390
can shut down even if they're afraid 
that it threatens their hedge.

1405
01:17:56,390 --> 01:17:57,223
Amani.
It's very interesting because in a lot 

1406
01:17:58,521 --> 01:18:01,640
of ways that's a,
it's a very elegant solution to.

1407
01:18:01,641 --> 01:18:03,580
What's an obvious problem?
Yeah.

1408
01:18:04,610 --> 01:18:05,443
Just as the Internet is an elegant 
solution to what's in hindsight and 

1409
01:18:08,911 --> 01:18:09,940
obvious problem,
right?

1410
01:18:09,950 --> 01:18:13,360
It's the distribution of communicate 
this,

1411
01:18:14,000 --> 01:18:14,833
but this is a extra special to me 
because if I was a person running a 

1412
01:18:19,161 --> 01:18:20,840
country,
I would be terrified of this shit.

1413
01:18:21,140 --> 01:18:21,681
I'd be like,
well,

1414
01:18:21,681 --> 01:18:23,950
this is what's going to.
That depends which country.

1415
01:18:23,970 --> 01:18:26,240
If you're a person running the US or 
China,

1416
01:18:26,241 --> 01:18:27,074
you,
you would have a different relationship 

1417
01:18:30,440 --> 01:18:31,273
than if you're a person.
Like I know the Prime Minister of 

1418
01:18:32,871 --> 01:18:34,670
Ethiopia,
I'll be Ahmed who's a,

1419
01:18:34,940 --> 01:18:37,430
has a degree in software engineering and
he,

1420
01:18:37,450 --> 01:18:37,880
he,
he,

1421
01:18:37,880 --> 01:18:42,110
he loves this,
but of course Ethiopia isn't in any day,

1422
01:18:42,120 --> 01:18:43,250
any other countries.
Right.

1423
01:18:43,280 --> 01:18:44,113
And they're not in any danger of,
of individually like taking global ai 

1424
01:18:47,991 --> 01:18:48,650
hedge.

1425
01:18:48,650 --> 01:18:51,650
So for the majority of countries in the 
world,

1426
01:18:52,100 --> 01:18:55,120
they like this for the same reason they 
liked Linux,

1427
01:18:55,140 --> 01:18:56,150
right?
I mean,

1428
01:18:56,151 --> 01:18:56,984
I mean this,
this is something which they have an 

1429
01:18:58,431 --> 01:19:00,470
equal role to anybody else,
right?

1430
01:19:00,590 --> 01:19:01,423
The superpowers,
and you see this among companies also 

1431
01:19:04,221 --> 01:19:05,054
those.
So a lot of big companies that we're 

1432
01:19:06,801 --> 01:19:10,610
talking to,
they love the idea of this decentralized

1433
01:19:10,611 --> 01:19:14,690
ai fabric because I mean if you're not 
Amazon,

1434
01:19:14,691 --> 01:19:15,800
Google,
Microsoft,

1435
01:19:15,880 --> 01:19:16,950
tencent,
facebook,

1436
01:19:16,960 --> 01:19:19,580
so on.
If you're another large corporation,

1437
01:19:19,880 --> 01:19:20,713
you don't necessarily want all your ai 
and all your data to be going into one 

1438
01:19:24,681 --> 01:19:25,514
of this handful of large ai companies.
You would rather have it be in the 

1439
01:19:29,650 --> 01:19:34,650
secure decentralized platform and I mean
this is the same reason that you know,

1440
01:19:35,300 --> 01:19:37,790
Cisco and IBM,
they run on Linux.

1441
01:19:37,791 --> 01:19:39,350
They don't run on Microsoft.
Right?

1442
01:19:39,351 --> 01:19:40,184
So if,
if you're not one of the handful of 

1443
01:19:42,771 --> 01:19:47,771
large governments or large corporations 
that happened to be in a leading role in

1444
01:19:48,681 --> 01:19:52,130
the ai ecosystem,
then you would rather have this,

1445
01:19:52,180 --> 01:19:56,030
this equalizing in decentralized thing 
because everyone gets to play.

1446
01:19:56,930 --> 01:19:57,430
Yeah.
What?

1447
01:19:57,430 --> 01:20:00,380
What would be the benefit of running on 
Linux versus Microsoft?

1448
01:20:01,300 --> 01:20:02,133
Well,
you're at the behest of some other big 

1449
01:20:03,381 --> 01:20:04,630
company.
I mean,

1450
01:20:04,670 --> 01:20:05,503
imagine if.
Imagine if you were cisco or gm or 

1451
01:20:10,051 --> 01:20:10,884
something and all of your internal 
machines are all your servers are 

1452
01:20:15,091 --> 01:20:15,924
running in Microsoft.
What Microsoft increases their price or 

1453
01:20:19,241 --> 01:20:20,074
are removed some feature.
Then you're totally at their behest and 

1454
01:20:25,890 --> 01:20:28,470
with ai the same thing is true.
I mean,

1455
01:20:28,471 --> 01:20:29,304
if,
if you put all your data in some big 

1456
01:20:31,081 --> 01:20:31,914
company's server farm and you're 
analyzing all your data on their 

1457
01:20:34,981 --> 01:20:37,830
algorithms and that's critical to your 
business model,

1458
01:20:37,831 --> 01:20:41,190
what if they change their ai algorithm 
in some way?

1459
01:20:41,191 --> 01:20:45,310
Then I'm in the.
Then your business is,

1460
01:20:45,360 --> 01:20:48,150
is basically controlled by this other 
company.

1461
01:20:48,151 --> 01:20:52,860
So I'm in having a decentralized 
platform in which you're,

1462
01:20:53,440 --> 01:20:54,273
you know,
an equal participant along with 

1463
01:20:55,471 --> 01:20:59,700
everybody else is,
is actually a much better position to be

1464
01:20:59,701 --> 01:20:59,880
in.

1465
01:20:59,880 --> 01:21:00,713
And I think this,
this I think is why we can succeed with 

1466
01:21:06,721 --> 01:21:07,554
this plan of having this decentralized 
singularity net platform than the 

1467
01:21:11,841 --> 01:21:12,674
singular,
the studio enterprise software company 

1468
01:21:15,210 --> 01:21:18,660
which mediates between the decentralized
platform and big companies.

1469
01:21:18,661 --> 01:21:22,470
I mean it's because most companies and 
governments in the world,

1470
01:21:23,070 --> 01:21:24,840
you know,
they don't want hedge.

1471
01:21:24,841 --> 01:21:28,950
I'm one of a few large governments and 
corporations each either.

1472
01:21:28,951 --> 01:21:31,470
And you can see this in,
in a lot of voids.

1473
01:21:31,471 --> 01:21:32,304
You can see this embrace of,
of Linux and etherium by many large 

1474
01:21:36,451 --> 01:21:37,284
corporations.
You can also see like in a different 

1475
01:21:39,601 --> 01:21:41,220
way,
you know,

1476
01:21:41,420 --> 01:21:43,700
the Indian government,
you know,

1477
01:21:43,880 --> 01:21:44,713
they rejected an offer by facebook to 
give free internet to all Indians 

1478
01:21:48,630 --> 01:21:49,463
because facebook wants to give like 
mobile phones that would get free 

1479
01:21:51,781 --> 01:21:54,000
internet,
but only to access facebook,

1480
01:21:54,001 --> 01:21:55,590
right?
India is like,

1481
01:21:55,591 --> 01:21:56,280
well no,
no,

1482
01:21:56,280 --> 01:21:57,150
no,
thanks.

1483
01:21:57,151 --> 01:21:59,580
Right,
and India is now giving.

1484
01:21:59,730 --> 01:22:00,563
They're now creating laws that any 
internet company that collects data 

1485
01:22:04,531 --> 01:22:07,110
about Indian people has to store that 
data in India,

1486
01:22:07,800 --> 01:22:10,440
which is so the Indian government can 
subpoena that data when,

1487
01:22:10,470 --> 01:22:11,040
when,
when,

1488
01:22:11,040 --> 01:22:12,300
when they want to.
So,

1489
01:22:12,570 --> 01:22:13,403
so you're,
you're already seeing a bunch of 

1490
01:22:15,930 --> 01:22:16,763
resistance against hedge pneumoniae by a
few large governments or large large 

1491
01:22:20,971 --> 01:22:24,300
corporations by other companies and 
other governments.

1492
01:22:24,301 --> 01:22:28,580
And I think this is very positive and is
one of the factors that can,

1493
01:22:28,650 --> 01:22:29,483
that can foster the foster,
the growth of a decentralized ai 

1494
01:22:33,061 --> 01:22:33,894
ecosystem.
Is it fair to say that the future of ai 

1495
01:22:37,950 --> 01:22:42,210
is severely dependent upon who launches 
it first?

1496
01:22:43,830 --> 01:22:45,690
Like whoever,
whether it's singularity,

1497
01:22:46,080 --> 01:22:48,850
the bottom line is official general 
intelligence.

1498
01:22:48,870 --> 01:22:51,630
The bottom line is as a scientist after 
say,

1499
01:22:51,631 --> 01:22:52,970
we don't know,
right?

1500
01:22:53,160 --> 01:22:53,993
It could be there's an end state that 
Agi will just self organize into almost 

1501
01:23:00,601 --> 01:23:01,434
independent of the initial condition,
but we don't know and given that we 

1502
01:23:06,581 --> 01:23:09,270
don't know,
I'm operating under the,

1503
01:23:09,560 --> 01:23:09,970
you know,
the,

1504
01:23:09,970 --> 01:23:14,970
your ristick assumption that if the 
first ai is beneficially oriented,

1505
01:23:18,371 --> 01:23:19,204
if it's controlled in the participatory 
democratic way and if it's oriented at 

1506
01:23:23,411 --> 01:23:27,610
least substantially towards like doing 
good things for humans,

1507
01:23:28,240 --> 01:23:29,073
I'm operating under the eucharistic 
assumption that this is going to bias 

1508
01:23:32,680 --> 01:23:34,940
things in a positive,
positive direction.

1509
01:23:35,340 --> 01:23:39,310
I'm in chorus in the absence of 
knowledge to the contrary.

1510
01:23:39,640 --> 01:23:41,770
But if the Chinese government launches 
one,

1511
01:23:42,390 --> 01:23:45,550
they're controlling.
We don't pop it off first.

1512
01:23:45,551 --> 01:23:46,384
I mean,
I liked the idea that you're saying 

1513
01:23:47,141 --> 01:23:48,910
though,
that it might organize itself.

1514
01:23:49,300 --> 01:23:52,750
I mean understand the Chinese government
also.

1515
01:23:52,751 --> 01:23:53,584
They want,
they want the best for the Chinese 

1516
01:23:56,291 --> 01:23:57,124
people that they don't,
they don't want to make the terminator 

1517
01:23:59,861 --> 01:24:00,640
either.
Right.

1518
01:24:00,640 --> 01:24:01,130
So,
uh,

1519
01:24:01,130 --> 01:24:02,180
I mean,
uh,

1520
01:24:02,230 --> 01:24:06,970
I think even even Donald Trump was not 
my favorite person doesn't actually want

1521
01:24:06,971 --> 01:24:08,680
to kill off everyone on the planet.
Right.

1522
01:24:08,681 --> 01:24:10,600
So he might,
if they talk shit about them.

1523
01:24:10,660 --> 01:24:11,111
Yeah,
yeah,

1524
01:24:11,111 --> 01:24:12,130
yeah.
You know,

1525
01:24:12,380 --> 01:24:16,290
you never know it was just him.
I told you all.

1526
01:24:16,580 --> 01:24:17,380
I mean,
I,

1527
01:24:17,380 --> 01:24:19,870
I think,
you know,

1528
01:24:19,871 --> 01:24:24,871
I wouldn't say we're necessarily doomed 
if big governments and big companies are

1529
01:24:25,721 --> 01:24:30,721
the ones that develop ai or Agi first 
big government,

1530
01:24:31,181 --> 01:24:32,920
big companies essentially developed the 
internet.

1531
01:24:32,920 --> 01:24:34,720
Right.
And it got away from them.

1532
01:24:34,750 --> 01:24:35,860
That's right.
That's right.

1533
01:24:35,861 --> 01:24:38,200
So there's a lot of uncertainty all 
around,

1534
01:24:38,590 --> 01:24:39,520
but I think,
you know,

1535
01:24:39,521 --> 01:24:40,354
it behooves us to do what we can to bias
the odds in our favor based on our 

1536
01:24:44,471 --> 01:24:45,304
current understanding.
And I mean toward that end we're 

1537
01:24:50,141 --> 01:24:51,070
developing,
you know,

1538
01:24:51,071 --> 01:24:54,580
open source,
decentralized ai and singularity in that

1539
01:24:54,581 --> 01:24:56,890
process.
So if you would explain some singularity

1540
01:24:56,891 --> 01:24:59,320
net and what,
what you guys are actively involved in.

1541
01:24:59,390 --> 01:25:00,223
Sure,
sure.

1542
01:25:01,100 --> 01:25:01,933
So singularity and that in itself is a 
platform that allows many different ais 

1543
01:25:08,981 --> 01:25:13,981
to operate on it and these ais can offer
services to anyone who requests services

1544
01:25:15,731 --> 01:25:20,731
of the network and they can also request
an offer services among each other.

1545
01:25:22,630 --> 01:25:26,260
So it's,
it's both just an online marketplace for

1546
01:25:26,261 --> 01:25:27,094
ais,
much like no the apple app store or 

1547
01:25:29,891 --> 01:25:33,010
Google play store,
but for ais rather than phone ups,

1548
01:25:33,430 --> 01:25:38,230
but the difference is the different ais 
in here can outsource work to each other

1549
01:25:38,231 --> 01:25:41,380
and talk to each other and that gives a 
new dimension to it,

1550
01:25:41,381 --> 01:25:42,214
right where you can have,
we think of as a society or economy of 

1551
01:25:45,431 --> 01:25:46,264
minds and it gives the possibility that 
this whole society of interacting ais 

1552
01:25:51,880 --> 01:25:55,980
which are then they're paying each other
for transactions with our,

1553
01:25:56,030 --> 01:25:59,350
our digital money or are cryptographic 
token,

1554
01:25:59,351 --> 01:26:01,060
which is called the Agi token.

1555
01:26:01,450 --> 01:26:02,283
So these ais which are paying each other
and rating each other of how good they 

1556
01:26:05,751 --> 01:26:06,584
are,
sending data and questions and answers 

1557
01:26:07,941 --> 01:26:08,774
to each other,
can self organize into some overall ai 

1558
01:26:12,471 --> 01:26:13,340
mind?
No.

1559
01:26:13,790 --> 01:26:17,990
We're building this platform and then 
we're plugging into it to seed it.

1560
01:26:17,991 --> 01:26:18,824
A bunch of ais of our own creation.
So I've been working for 10 years on 

1561
01:26:22,401 --> 01:26:25,130
this open source ai project called open 
cog,

1562
01:26:25,520 --> 01:26:29,900
which is oriented toward building 
general intelligence and we're putting a

1563
01:26:29,901 --> 01:26:30,734
bunch of ai agents based on the open cog
platform into this singularity in the 

1564
01:26:36,800 --> 01:26:41,210
network and you know if we're successful
in a couple of years,

1565
01:26:41,211 --> 01:26:44,690
the ais that we put on there will be a 
tiny minority of what's in there,

1566
01:26:44,930 --> 01:26:48,130
just like the apps made by Google or a 
small minority of the apps in,

1567
01:26:48,190 --> 01:26:49,820
in the,
in the Google play store.

1568
01:26:49,821 --> 01:26:53,540
Right.
But my hope is that these open cog,

1569
01:26:53,570 --> 01:26:54,403
ai agents within the larger pool of Ais 
and the singularity net can sort of 

1570
01:27:00,060 --> 01:27:00,893
serve as the general intelligence corps 
because the open ai agents are really 

1571
01:27:05,271 --> 01:27:08,720
good at abstraction and generalization 
and creativity.

1572
01:27:09,020 --> 01:27:09,853
We can put another,
a bunch of other ais in there that are 

1573
01:27:11,481 --> 01:27:14,960
good at highly specific for forms of 
learning,

1574
01:27:14,961 --> 01:27:18,350
like predicting financial time series,
curing diseases,

1575
01:27:18,351 --> 01:27:19,184
answering people's questions,
organizing your inbox so you can have 

1576
01:27:22,011 --> 01:27:24,440
the interaction of these specialized 
ais.

1577
01:27:24,770 --> 01:27:27,200
And then more general purpose.
You know,

1578
01:27:27,201 --> 01:27:28,034
abstraction and creativity based.
Ai is like open cog agents all 

1579
01:27:31,191 --> 01:27:32,024
interacting together in this 
decentralized platform and then you 

1580
01:27:36,071 --> 01:27:37,700
know,
the beauty of it is like some,

1581
01:27:37,880 --> 01:27:38,713
some 15 year old genius and Azerbaijan 
or the Congo can put some brilliant ai 

1582
01:27:43,731 --> 01:27:46,610
into this network.
If it's really smart,

1583
01:27:47,000 --> 01:27:49,970
it will get rated highly by the other 
ais for,

1584
01:27:50,000 --> 01:27:51,050
for,
for its work,

1585
01:27:51,051 --> 01:27:54,980
helping them do their thing.
Then it can get replicated over and over

1586
01:27:54,981 --> 01:27:55,814
again across many servers.
Suddenly a this 16 year old kid from 

1587
01:27:59,631 --> 01:28:02,930
Azerbaijan or the Congo could become 
wealthy from,

1588
01:28:02,990 --> 01:28:07,990
from their copies of their ai providing 
services to other people's ais and be,

1589
01:28:08,320 --> 01:28:09,153
you know,
the creativity in their mind is out 

1590
01:28:10,461 --> 01:28:14,750
there and is infusing this global aid 
network with some,

1591
01:28:15,200 --> 01:28:16,033
some new intellectual DNA that you know,
never would have been found by a ten 

1592
01:28:20,710 --> 01:28:21,543
cent or a google because they're not 
going to hire some Congolese teenager 

1593
01:28:25,281 --> 01:28:27,050
who may have a brilliant ai idea.

1594
01:28:27,650 --> 01:28:28,790
That's amazing.

1595
01:28:28,850 --> 01:28:33,850
That's amazing.
So this is all ongoing right now.

1596
01:28:34,370 --> 01:28:37,400
And the singularity that you guys are 
using,

1597
01:28:37,720 --> 01:28:40,010
the.
The way I've understood that term,

1598
01:28:40,160 --> 01:28:40,993
correct me if I'm wrong,
is that it's going to be the one 

1599
01:28:43,490 --> 01:28:47,510
innovation or one invention that 
essentially changes everything forever.

1600
01:28:48,100 --> 01:28:50,620
The singularity isn't necessarily one 
invention.

1601
01:28:50,621 --> 01:28:55,330
The singularity,
which is coined by Wong,

1602
01:28:55,670 --> 01:28:58,690
is coined by my friend Vernor vinge,
who's another guy you should interview.

1603
01:28:58,691 --> 01:29:01,830
He's in San Diego to the other guys.
The other brilliant guys down there,

1604
01:29:01,831 --> 01:29:05,040
but vernor vinge military down there.
Yeah,

1605
01:29:05,041 --> 01:29:06,020
vernor vinge.
Uh,

1606
01:29:06,070 --> 01:29:07,590
he,
he was a math professor.

1607
01:29:07,630 --> 01:29:12,000
The San Diego University actually,
but well known science fiction writer.

1608
01:29:12,540 --> 01:29:15,990
His book a fire upon the deep,
one of the great science fiction books.

1609
01:29:15,991 --> 01:29:20,390
So v I n g Vernor Vinge,
GE,

1610
01:29:21,060 --> 01:29:26,060
brilliant guy,
fire upon the deep Vernor v e r o v e R.

1611
01:29:27,930 --> 01:29:28,770
Yeah,
he's brilliant.

1612
01:29:28,771 --> 01:29:33,150
He coined the term technological 
singularity back in the 19 eighties,

1613
01:29:33,720 --> 01:29:36,590
but he,
he opted not to become a,

1614
01:29:36,670 --> 01:29:39,750
a pundit about it because he'd rather 
more science fiction.

1615
01:29:40,080 --> 01:29:41,970
That's interesting.
Than a science fiction author.

1616
01:29:42,000 --> 01:29:44,790
Ray Kurzweil,
who's also a good friend of mine.

1617
01:29:44,791 --> 01:29:45,624
I mean Ray Ray took that term and 
fleshed it out and did a bunch of data 

1618
01:29:51,421 --> 01:29:52,350
analytics.

1619
01:29:52,680 --> 01:29:55,830
Trying to pinpoint like when it's,
it would happen,

1620
01:29:55,831 --> 01:29:56,664
but the basic concept of the 
technological singularity is a point in 

1621
01:29:59,911 --> 01:30:04,911
time when technological advance occurs 
so rapidly that the human mind,

1622
01:30:05,581 --> 01:30:07,420
it appears almost instantaneous.
Like,

1623
01:30:07,660 --> 01:30:12,660
like imagine 10 new Nobel Prize winning 
discoveries every second or something.

1624
01:30:12,811 --> 01:30:16,080
Right?
So this is similar to the concept of the

1625
01:30:16,081 --> 01:30:20,060
intelligence explosion that was posited 
by the mathematician,

1626
01:30:20,080 --> 01:30:23,100
Ij good in 1965.
What Ij good said.

1627
01:30:23,101 --> 01:30:23,934
Then the year before I was born was the 
first truly intelligent machine will be 

1628
01:30:28,021 --> 01:30:30,660
the last invention that humanity needs 
to make rent.

1629
01:30:31,230 --> 01:30:32,063
So this is an intelligence explosion is 
another term for basically the same 

1630
01:30:36,061 --> 01:30:40,620
thing as the technological singularity,
but it's not just about Ai.

1631
01:30:40,950 --> 01:30:43,980
Ai is just probably the most powerful 
technology driving it.

1632
01:30:43,981 --> 01:30:46,700
I mean there's ai,
there's nanotechnology,

1633
01:30:46,910 --> 01:30:47,743
there's femto technology which will be 
building things from elementary 

1634
01:30:50,701 --> 01:30:53,130
particles.
I mean there's life extension,

1635
01:30:53,430 --> 01:30:55,910
genetic engineering,
mind uploading,

1636
01:30:55,920 --> 01:30:56,753
which is like reading the mind that if 
your brain and putting it into a 

1637
01:31:00,840 --> 01:31:02,290
machine,
you know,

1638
01:31:02,291 --> 01:31:03,124
there's advanced energy technologies so 
that all these different things are 

1639
01:31:07,021 --> 01:31:10,230
expected to advance at around the same 
time.

1640
01:31:10,230 --> 01:31:12,030
And they have many ways to boost each 
other,

1641
01:31:12,031 --> 01:31:12,864
right?
Because the,

1642
01:31:13,040 --> 01:31:14,460
you know,
the better Aiu have,

1643
01:31:14,461 --> 01:31:18,000
your ai can then invent new ways of 
doing nanotech tech and biology.

1644
01:31:18,300 --> 01:31:20,970
But if you invent amazing new nanotech 
in quantum computing,

1645
01:31:20,971 --> 01:31:23,520
that can make your ai smarter.
On the other hand,

1646
01:31:23,521 --> 01:31:24,354
if you,
if you could crack how the human brain 

1647
01:31:25,771 --> 01:31:29,310
works and genetic engineering to upgrade
human intelligence,

1648
01:31:29,520 --> 01:31:32,520
those smarter than humans can then make 
better eyes and nano technology,

1649
01:31:32,521 --> 01:31:36,150
right?
So there's so many virtuous cycles among

1650
01:31:36,151 --> 01:31:39,390
these different technologies.
The more you advance in any of them,

1651
01:31:39,391 --> 01:31:42,180
the more you're going to advance and in,
in all of them.

1652
01:31:42,390 --> 01:31:45,240
And it's the coming together of all of 
these that's going to create,

1653
01:31:45,750 --> 01:31:46,583
you know,
radical abundance and the technological 

1654
01:31:50,460 --> 01:31:53,130
singularity so that,
that term,

1655
01:31:53,131 --> 01:31:53,964
which Vernor vinge,
he introduced ray Ray Kurzweil borrowed 

1656
01:31:57,691 --> 01:32:02,140
for his books and further singular the 
university educational program.

1657
01:32:02,440 --> 01:32:05,410
And then we borrowed that for our 
singularity net,

1658
01:32:05,500 --> 01:32:06,333
like decentralized blockchain based ai 
platform and in our singularity studio 

1659
01:32:11,800 --> 01:32:12,633
enterprise software company.
Now I want to talk to you about two 

1660
01:32:14,771 --> 01:32:15,604
parts of what you just said.
One being the possibility that one day 

1661
01:32:18,521 --> 01:32:21,910
we can upload our mind will make copies 
of our mind.

1662
01:32:22,230 --> 01:32:23,063
You're up for it to upload into here.
I could use a little Joe Rogan them on 

1663
01:32:28,041 --> 01:32:29,740
my phone.
You can just call me dude,

1664
01:32:31,030 --> 01:32:31,863
the organic version,
but the what do you think that that's a 

1665
01:32:36,041 --> 01:32:36,874
real possibility inside of our lifetime 
that we can map out the human mind to 

1666
01:32:40,360 --> 01:32:42,700
the point where we can essentially 
recreate it,

1667
01:32:43,210 --> 01:32:44,043
but if you do recreate it without all 
the biological urges and the human 

1668
01:32:47,051 --> 01:32:49,690
reward systems that are built in,
what the fuck are we?

1669
01:32:49,691 --> 01:32:50,524
I mean,
well that's a different question I 

1670
01:32:52,031 --> 01:32:53,470
think.
What is your mind?

1671
01:32:54,490 --> 01:32:55,061
Well,
I,

1672
01:32:55,061 --> 01:32:57,610
I think that those two things that are 
needed for,

1673
01:32:58,360 --> 01:32:59,193
let's say,
let's say human body uploading to 

1674
01:33:01,930 --> 01:33:03,730
simplify things,
body uploading.

1675
01:33:03,760 --> 01:33:04,593
There are two things that are needed.
One thing is a better computing 

1676
01:33:08,711 --> 01:33:09,544
infrastructure.
Then we have no to host the uploaded 

1677
01:33:12,101 --> 01:33:16,030
body and,
and the other thing is a better scanning

1678
01:33:16,060 --> 01:33:16,893
technology because right now we don't 
have a way to scan the molecular 

1679
01:33:20,651 --> 01:33:23,490
structure of your body without like 
freezing you,

1680
01:33:23,491 --> 01:33:24,324
slicing you in scanning you,
which you probably don't want that at 

1681
01:33:26,880 --> 01:33:28,240
this point in time,
right yet.

1682
01:33:28,450 --> 01:33:29,283
So assuming both those are solved and 
you can then recreate in some computer 

1683
01:33:34,151 --> 01:33:36,880
simulation,
you know,

1684
01:33:36,881 --> 01:33:38,920
uh,
an accurate simulate Chromo of,

1685
01:33:38,921 --> 01:33:39,520
of,
of,

1686
01:33:39,520 --> 01:33:41,140
of what you are,
right?

1687
01:33:41,590 --> 01:33:42,423
That's where I'm getting this,
where I'm getting at an accurate 

1688
01:33:44,810 --> 01:33:45,643
simulacrum is,
that's getting weird because the 

1689
01:33:47,471 --> 01:33:50,680
biological variability of human beings,
we vary day to day.

1690
01:33:50,860 --> 01:33:51,693
We're very dependent upon it.
And your simulator and will also vary 

1691
01:33:53,951 --> 01:33:54,784
day to day.
So the DVA program and into have flaws 

1692
01:33:58,090 --> 01:34:00,940
because while we vary dependent upon how
much sleep we get,

1693
01:34:00,941 --> 01:34:03,070
whether or not we're feeling sick,
whether we're lonely.

1694
01:34:03,071 --> 01:34:05,080
So all these,
if you're upload,

1695
01:34:05,081 --> 01:34:08,830
we're an accurate copy of you than the 
simulation.

1696
01:34:08,831 --> 01:34:09,664
Hosting your upload would need to have 
an accurate simulation of the laws of 

1697
01:34:13,090 --> 01:34:18,040
bio physics and chemistry that allow 
your body to,

1698
01:34:18,200 --> 01:34:20,170
you know,
evolve from one second to the neck.

1699
01:34:20,200 --> 01:34:24,010
My concern is though would change second
by second,

1700
01:34:24,011 --> 01:34:26,950
just like just like you do and it would 
divert from me.

1701
01:34:26,951 --> 01:34:27,784
Right?
So I mean after an hour it will be a 

1702
01:34:29,111 --> 01:34:32,690
little different.
After a year it might've gone in,

1703
01:34:32,700 --> 01:34:35,260
in a quite different direction for you.
Probably a month.

1704
01:34:35,640 --> 01:34:39,100
Some Super God monk living on the top of
a mountain somewhere in a year.

1705
01:34:39,370 --> 01:34:41,180
Have it,
keeps the whole right.

1706
01:34:41,340 --> 01:34:43,240
Depends on what virtual world it's 
living in.

1707
01:34:43,241 --> 01:34:44,074
True.
I mean if it's living in a virtual 

1708
01:34:45,191 --> 01:34:46,930
world,
your world will be a virtual world.

1709
01:34:46,960 --> 01:34:49,720
If we're not talking about the potential
of downloading this.

1710
01:34:49,721 --> 01:34:51,970
Again,
in sort of into a biologic,

1711
01:34:51,971 --> 01:34:53,520
there's a lot of possibilities,
right?

1712
01:34:53,540 --> 01:34:54,261
I mean you,
you,

1713
01:34:54,261 --> 01:34:57,320
you could,
you could upload into a Joe Rogan living

1714
01:34:57,321 --> 01:34:58,154
in the virtual world and then just 
create your own fantasy universe or you 

1715
01:35:02,030 --> 01:35:06,920
or you could three d print and alternate
synthetic body.

1716
01:35:06,980 --> 01:35:07,560
Right?
I mean,

1717
01:35:07,560 --> 01:35:08,393
once you,
once you have the ability to manipulate 

1718
01:35:11,210 --> 01:35:12,043
molecules at will,
the scope of possibilities becomes much 

1719
01:35:16,671 --> 01:35:18,500
greater than we're used to.
Thinking about.

1720
01:35:18,800 --> 01:35:22,460
My question is do,
do we replicate flaws?

1721
01:35:22,520 --> 01:35:25,220
Do we replicate depression?
Of course,

1722
01:35:25,340 --> 01:35:27,620
but we knew that when we want to cure 
depression,

1723
01:35:27,830 --> 01:35:31,430
so depression it.
Here's the interesting thing.

1724
01:35:31,830 --> 01:35:36,830
Once once we have you in a digital form,
then it's very programmable,

1725
01:35:37,640 --> 01:35:38,473
so then the dopamine and Serotonin,
then you can change what you want and 

1726
01:35:42,021 --> 01:35:44,420
then you have a whole different set of 
issues.

1727
01:35:44,421 --> 01:35:45,254
Right?
Yeah.

1728
01:35:45,350 --> 01:35:46,183
Because once you've changed,
I mean suppose you make a fork of 

1729
01:35:49,101 --> 01:35:49,934
yourself and then you manipulate it in a
certain way and then after a few hours 

1730
01:35:54,801 --> 01:35:56,270
you're like,
well I don't.

1731
01:35:56,460 --> 01:36:00,140
I don't much like this.
A new joe here.

1732
01:36:00,380 --> 01:36:03,770
Maybe we should roll back that change,
but the new joe is like,

1733
01:36:03,771 --> 01:36:06,230
well,
I liked myself very well.

1734
01:36:06,231 --> 01:36:08,260
Thank you.
So then,

1735
01:36:08,720 --> 01:36:09,553
yeah,
there's a lot of issues that that will 

1736
01:36:11,631 --> 01:36:14,390
come up once we can write,
modify,

1737
01:36:14,391 --> 01:36:15,224
and reprogram ourselves to the point 
that the ramifications of these 

1738
01:36:19,041 --> 01:36:21,890
decisions are almost insurmountable d 
once,

1739
01:36:21,891 --> 01:36:24,350
once the ball gets rolling.
Well,

1740
01:36:24,351 --> 01:36:25,184
the modifications of the ramifications 
of these decisions are going to be very 

1741
01:36:28,941 --> 01:36:30,500
interesting to explore.

1742
01:36:30,560 --> 01:36:32,330
Yes,
you're super positive band,

1743
01:36:32,870 --> 01:36:36,440
super positive of your optimism.
Many bad things will happen.

1744
01:36:36,441 --> 01:36:38,210
Many good things will happen.
That's a very.

1745
01:36:38,211 --> 01:36:40,370
That's a very easy prediction to me.
Okay.

1746
01:36:40,371 --> 01:36:41,690
I see what you're saying.
Yeah.

1747
01:36:41,691 --> 01:36:44,810
I've just a one day thing.
Think about like world travel,

1748
01:36:45,490 --> 01:36:46,323
like hundreds of years ago,
most people didn't travel more than a 

1749
01:36:49,430 --> 01:36:52,040
very short distance from their home and 
you could say,

1750
01:36:52,041 --> 01:36:52,431
well,
okay,

1751
01:36:52,431 --> 01:36:55,100
what if.
What if people could travel all over the

1752
01:36:55,101 --> 01:36:55,880
world,
right?

1753
01:36:55,880 --> 01:36:56,713
Like what horrible things could happen.
They would lose their culture like they 

1754
01:36:59,421 --> 01:37:02,180
might go marry someone from from a 
random tribe.

1755
01:37:02,660 --> 01:37:05,570
You can get killed in the Arctic region 
or something.

1756
01:37:05,740 --> 01:37:08,660
A lot of bad things can happen when you 
travel far from your home.

1757
01:37:08,840 --> 01:37:09,673
A lot of good things can happen and the 
ultimately the ramifications were not 

1758
01:37:13,701 --> 01:37:16,820
foreseen by people 500 years ago.
I'm in,

1759
01:37:17,180 --> 01:37:19,280
we're going into a lot of new domains.

1760
01:37:19,670 --> 01:37:24,620
We can't see the details of the pluses 
and minuses that are going to unfold.

1761
01:37:25,260 --> 01:37:26,093
A W,
it would behoove us to simply become 

1762
01:37:28,011 --> 01:37:28,844
comfortable with radical uncertainty 
because otherwise we're going to 

1763
01:37:31,311 --> 01:37:33,530
confront it anyway and we're just going 
to be nervous.

1764
01:37:33,650 --> 01:37:35,570
So it's just inevitable this,
this,

1765
01:37:35,571 --> 01:37:37,190
uh,
it's almost inevitable.

1766
01:37:37,191 --> 01:37:38,330
I mean,
of course.

1767
01:37:38,331 --> 01:37:39,920
Sorry.
Any natural disaster.

1768
01:37:40,390 --> 01:37:41,120
Yeah.
I mean,

1769
01:37:41,120 --> 01:37:41,953
of course trump could start a nuclear 
war and then we're resetting to ground 

1770
01:37:45,021 --> 01:37:47,420
zero where we get hit by an asteroid,
right?

1771
01:37:48,470 --> 01:37:49,303
Yeah.
I mean,

1772
01:37:49,580 --> 01:37:54,580
so barring a catastrophic outcome,
I believe a technological singularity is

1773
01:37:55,800 --> 01:37:56,633
essentially inevitable.
There's a radical uncertainty attached 

1774
01:38:01,711 --> 01:38:03,390
to this.
On the other hand,

1775
01:38:04,340 --> 01:38:05,173
you know,
in as much as we humans can know 

1776
01:38:06,301 --> 01:38:07,134
anything,
it would seem common sensically there's 

1777
01:38:10,381 --> 01:38:15,381
the ability to bias this in a positive 
rather than the negative direction.

1778
01:38:16,560 --> 01:38:21,390
We should be spending more of our 
attention on doing that rather than,

1779
01:38:21,391 --> 01:38:22,224
for instance,
advertising spying and making chocolate 

1780
01:38:25,470 --> 01:38:28,430
chocolates and all the other things 
people are doing now.

1781
01:38:28,430 --> 01:38:29,310
I mean,
it's prevalent,

1782
01:38:29,311 --> 01:38:30,144
it's everywhere,
but I mean how many people are actually 

1783
01:38:31,411 --> 01:38:32,244
at the helm of that as opposed to how 
many people are working on various 

1784
01:38:35,731 --> 01:38:38,160
aspects of technology all across the 
planet.

1785
01:38:38,250 --> 01:38:42,210
It's a small group and in comparison,
working on explicitly bringing about the

1786
01:38:42,211 --> 01:38:45,960
singularity is a small group.
On the other hand,

1787
01:38:46,680 --> 01:38:49,680
supporting technologies is a very large 
group,

1788
01:38:49,681 --> 01:38:52,920
so think about like gps.
Where did they come from?

1789
01:38:53,190 --> 01:38:54,900
Accelerating gaming,
right?

1790
01:38:55,350 --> 01:38:56,183
Lo and behold,
they're amazingly useful for training 

1791
01:38:58,441 --> 01:38:59,274
neural net models,
which is the one among many important 

1792
01:39:01,501 --> 01:39:02,730
types of Ai,
right?

1793
01:39:02,731 --> 01:39:03,564
So a large amount of the planet's 
resources are now getting spent on 

1794
01:39:07,351 --> 01:39:08,184
technologies that are indirectly 
supporting these singular attarian 

1795
01:39:12,060 --> 01:39:14,070
technology.
So as another example,

1796
01:39:14,090 --> 01:39:18,540
like microarrays and let you measure the
expression level of genes,

1797
01:39:18,830 --> 01:39:22,170
how much each gene is doing in your body
at each point in time.

1798
01:39:22,500 --> 01:39:24,970
These were originally developed,
you know,

1799
01:39:25,140 --> 01:39:25,973
as an outgrowth of printing technology.
Then instead of squirting ink after 

1800
01:39:29,461 --> 01:39:31,530
metrics figured out you could squirt 
DNA.

1801
01:39:31,590 --> 01:39:32,423
Right?
So I mean the amount of technology 

1802
01:39:35,310 --> 01:39:36,143
specifically oriented toward the 
singularity doesn't have to be large 

1803
01:39:39,961 --> 01:39:44,961
because the overall spectrum of 
supporting technologies can be subverted

1804
01:39:45,721 --> 01:39:48,090
in that direction.
Do you,

1805
01:39:48,110 --> 01:39:50,820
do you have any concerns at all about a 
virtual world?

1806
01:39:51,000 --> 01:39:53,070
I mean,
we may be anyone right now,

1807
01:39:53,090 --> 01:39:54,810
man,
you know that's true.

1808
01:39:55,200 --> 01:39:56,033
But as far as my problem is,
I want to find that programmer and get 

1809
01:39:58,681 --> 01:39:59,514
them to make more attractive people.
I would say that that's part of the 

1810
01:40:04,711 --> 01:40:05,544
reason why I attracted people are so 
interesting as that they're unique and 

1811
01:40:07,471 --> 01:40:10,500
rare problems with calling everything 
beautiful.

1812
01:40:10,560 --> 01:40:11,360
Yeah.
You know,

1813
01:40:11,360 --> 01:40:13,090
everything we're saying,
everything is generous.

1814
01:40:13,091 --> 01:40:14,070
Beautiful.
I was like,

1815
01:40:14,100 --> 01:40:16,470
well,
you just have to get realistic.

1816
01:40:16,470 --> 01:40:19,380
If I get in the right frame of mind that
can find anything beautiful.

1817
01:40:19,450 --> 01:40:21,240
Well you could find it unique and 
interesting.

1818
01:40:21,270 --> 01:40:22,460
Oh,
I can find anything but.

1819
01:40:22,461 --> 01:40:23,940
Okay.
I guess I guess.

1820
01:40:23,970 --> 01:40:26,580
But in terms of like.
Yeah,

1821
01:40:26,640 --> 01:40:28,050
I guess it's subjective,
right?

1822
01:40:28,080 --> 01:40:29,490
Really it is.
We're talking about beauty,

1823
01:40:29,590 --> 01:40:31,170
right?
Yeah.

1824
01:40:31,410 --> 01:40:34,510
Now,
but very existential angst just on the,

1825
01:40:34,511 --> 01:40:35,344
the.
When people sit and think about the 

1826
01:40:37,021 --> 01:40:41,280
pointlessness of our own existence,
like we are these finite beings that are

1827
01:40:41,610 --> 01:40:42,443
clinging to a balls.
It spins a thousand miles an hour 

1828
01:40:44,551 --> 01:40:46,750
hurling through infinity.
And what's the point like?

1829
01:40:46,770 --> 01:40:48,720
There's a lot of that that goes around 
already.

1830
01:40:48,900 --> 01:40:52,350
We've.
We create an artificial environment that

1831
01:40:52,351 --> 01:40:54,130
we can literally somehow

1832
01:40:54,130 --> 01:40:54,963
under the download a version of us and 
it exists and this block chain created 

1833
01:41:03,430 --> 01:41:08,430
or or powered weird fucking simulation 
world.

1834
01:41:10,990 --> 01:41:12,260
What would be,
I mean

1835
01:41:12,810 --> 01:41:15,330
it be the point of what I really 
believe,

1836
01:41:15,870 --> 01:41:20,870
which is a bit personal and maybe 
different than many of my colleagues.

1837
01:41:22,110 --> 01:41:27,110
What I really believe is that these 
advancing technologies are going to lead

1838
01:41:29,161 --> 01:41:34,161
us to unlock many different states of 
consciousness and experience than,

1839
01:41:35,730 --> 01:41:39,200
than most people are,
are,

1840
01:41:39,201 --> 01:41:43,470
are currently aware of are I mean you,
you,

1841
01:41:43,490 --> 01:41:44,323
you say we're,
we're just an insignificant species on 

1842
01:41:46,711 --> 01:41:47,544
the,
on the,

1843
01:41:47,550 --> 01:41:48,383
you know,
a speck of rock hurtling in the other 

1844
01:41:49,621 --> 01:41:50,454
space were insignificant.
There's people that have existential 

1845
01:41:52,951 --> 01:41:55,400
acts because they wonder about what the 
purpose was.

1846
01:41:56,270 --> 01:41:57,103
I go that category tend to feel like we 
understand almost nothing about who and 

1847
01:42:04,861 --> 01:42:08,590
what we are and our knowledge about the 
universe is far small,

1848
01:42:08,600 --> 01:42:10,510
extremely minuscule.
I mean,

1849
01:42:10,870 --> 01:42:14,250
if anything,
I look at things from more of a Buddhist

1850
01:42:14,251 --> 01:42:15,084
or phenomenological way,
like the sense perceptions and then out 

1851
01:42:19,141 --> 01:42:19,974
of those sense perceptions,
models arise and accumulate including a 

1852
01:42:25,021 --> 01:42:30,021
model of the self and the model of the 
body and the model of the physical world

1853
01:42:30,691 --> 01:42:34,290
out there and by the time you get to 
planets and stars and blockchains,

1854
01:42:34,291 --> 01:42:35,124
you're building like hypothetical models
on top of hypothetical models and then 

1855
01:42:40,380 --> 01:42:41,213
you know,
we're,

1856
01:42:41,420 --> 01:42:42,253
we're building intelligent machines and 
mind uploading machines and virtual 

1857
01:42:47,371 --> 01:42:48,210
realities.

1858
01:42:48,780 --> 01:42:51,960
We're going to radically transform,
you know,

1859
01:42:52,410 --> 01:42:53,243
our whole state of consciousness,
our understanding of what mind and 

1860
01:42:56,041 --> 01:43:00,600
matter are.
Our experience of our own selves or even

1861
01:43:00,601 --> 01:43:01,434
whether it's self exists.
And I think ultimately the state of 

1862
01:43:06,571 --> 01:43:07,404
consciousness of a human being like 100 
years from now after a technological 

1863
01:43:12,361 --> 01:43:13,194
singularity is going to bear very little
resemblance to the states of 

1864
01:43:16,291 --> 01:43:19,950
consciousness we have now.
We're just going to.

1865
01:43:20,250 --> 01:43:24,750
We're going to see a much wider universe
in any of us know,

1866
01:43:25,620 --> 01:43:28,500
imagine,
imagine to exist.

1867
01:43:28,650 --> 01:43:31,560
No,
this is my own personal view of things.

1868
01:43:31,561 --> 01:43:31,911
You,
you,

1869
01:43:31,911 --> 01:43:32,744
you don't have to agree with that.
To think the technological singularity 

1870
01:43:35,881 --> 01:43:38,460
will be valuable,
but that is how I look.

1871
01:43:38,520 --> 01:43:39,353
I know Ray Kurzweil and I agree there's 
going to be a technological singularity 

1872
01:43:43,810 --> 01:43:48,120
within decades at most and ray and I 
agree that,

1873
01:43:48,510 --> 01:43:51,210
you know,
if we biased technology development,

1874
01:43:52,370 --> 01:43:53,960
we can very lightly,
you know,

1875
01:43:53,961 --> 01:43:54,794
guide this to be a,
a world of abundance and benefit for 

1876
01:43:58,071 --> 01:43:59,530
humans as well as ais.

1877
01:44:00,120 --> 01:44:05,120
But Ray is a bit more of a down to Earth
empiricists than I am lucky.

1878
01:44:06,650 --> 01:44:11,270
He thinks we understand more about the 
universe right now than that than I do.

1879
01:44:11,300 --> 01:44:12,133
So.
I mean there's a wide spectrum of views 

1880
01:44:15,171 --> 01:44:20,171
that are rational and sensible to have.
But my own view is we understand really,

1881
01:44:21,351 --> 01:44:26,351
really little of what we are and what,
what this world is.

1882
01:44:26,571 --> 01:44:27,404
And this is part of my own personal 
quest for wanting to upgrade my brain 

1883
01:44:32,961 --> 01:44:33,794
and wanting to create artificial 
intelligences is like I've always been 

1884
01:44:36,681 --> 01:44:39,500
driven above all else.
Borrowing to understand everything I can

1885
01:44:39,501 --> 01:44:40,610
about the world.
So I'm in.

1886
01:44:40,970 --> 01:44:41,803
I've studied every kind of science and 
engineering and social science and read 

1887
01:44:44,841 --> 01:44:45,674
every kind of literature,
but in the end the scope of human 

1888
01:44:47,961 --> 01:44:48,794
understanding is clearly very small,
although at least we're smart enough to 

1889
01:44:53,421 --> 01:44:55,650
understand how little we understand,
which I think my,

1890
01:44:55,700 --> 01:44:57,020
my dog doesn't understand.

1891
01:44:57,400 --> 01:44:58,310
He understands.
Right?

1892
01:44:58,540 --> 01:45:02,750
So and even like my 10 month old son,
he understands a little.

1893
01:45:02,751 --> 01:45:04,970
He understands which is interesting,
right?

1894
01:45:04,971 --> 01:45:07,820
Because he's because he's ready to 
because he's also a human.

1895
01:45:07,821 --> 01:45:09,590
Right.
So I think,

1896
01:45:10,280 --> 01:45:11,113
I mean,
everything we think and believe now is 

1897
01:45:13,071 --> 01:45:16,760
going to seem absolutely absurd to us 
after there's a singularity.

1898
01:45:16,770 --> 01:45:17,603
We're just going to look back and laugh 
in a warm hearted way as all the 

1899
01:45:21,561 --> 01:45:22,394
incredibly silly things we were thinking
and doing back when we were trapped in 

1900
01:45:26,181 --> 01:45:27,014
our,
in our,

1901
01:45:27,110 --> 01:45:27,943
you know,
our primitive biological brains and 

1902
01:45:29,811 --> 01:45:31,350
bodies.
Stunning attack.

1903
01:45:31,520 --> 01:45:32,353
In your opinion or your assessment is 
somewhere less than a 100 years away 

1904
01:45:36,411 --> 01:45:38,020
from now.
Yeah.

1905
01:45:38,060 --> 01:45:40,890
That's requires exponential thinking.
Right?

1906
01:45:40,970 --> 01:45:43,370
Because if you.
It's hard to wrap your head around,

1907
01:45:43,371 --> 01:45:44,500
right?
I don't know.

1908
01:45:44,510 --> 01:45:45,343
It's immediate.
It's immediate for me to wrap my head 

1909
01:45:46,731 --> 01:45:47,564
around,
but for a lot of people that you 

1910
01:45:48,801 --> 01:45:49,634
explained it to him,
I'm sure that that that's a little bit 

1911
01:45:50,601 --> 01:45:51,434
of a roadblock.

1912
01:45:51,500 --> 01:45:52,370
No,
it is.

1913
01:45:52,430 --> 01:45:53,263
It is.
It took me some time to have my parents 

1914
01:45:54,621 --> 01:45:56,660
drop their heads around it because they 
didn't.

1915
01:45:56,780 --> 01:45:58,520
They're not,
they're not technologists,

1916
01:45:58,521 --> 01:45:59,354
but I mean I find if you get people to 
pay attention and sort of lead them 

1917
01:46:04,671 --> 01:46:05,504
through all the supporting evidence,
most people can comprehend these ideas 

1918
01:46:11,090 --> 01:46:14,050
reasonably well just to computers from 
1960,

1919
01:46:14,060 --> 01:46:14,893
just hard to grasp.
It's hard to grab people's attention 

1920
01:46:16,491 --> 01:46:17,270
then.
Yeah.

1921
01:46:17,270 --> 01:46:21,390
Mobile phones and made a big difference.
Like I spent a lot of time in Africa,

1922
01:46:21,391 --> 01:46:22,224
in Addis Ababa in Ethiopia where we have
a large ai development office and you 

1923
01:46:26,781 --> 01:46:27,614
know,
the fact that mobile phones and then 

1924
01:46:29,240 --> 01:46:30,073
smartphones have rolled out so quickly 
even in rural Africa and if it's such a 

1925
01:46:33,981 --> 01:46:34,814
transformative impact.
I mean this is a metaphor that lets 

1926
01:46:37,221 --> 01:46:41,210
people understand the speed with what's 
exponential change can happen.

1927
01:46:41,540 --> 01:46:42,373
When you talk about yourself,
when you talk about consciousness and 

1928
01:46:44,871 --> 01:46:48,200
how you interface with the world,
how do you see this?

1929
01:46:48,200 --> 01:46:48,570
I mean,
when,

1930
01:46:48,570 --> 01:46:50,850
when you,
that we might be living in a simulation.

1931
01:46:51,090 --> 01:46:53,190
Do you actually entertain that?
Oh yeah,

1932
01:46:53,250 --> 01:46:54,550
you do.
I'm in the,

1933
01:46:54,590 --> 01:46:58,480
I think the word simulation is probably 
wrong,

1934
01:46:58,710 --> 01:47:01,890
but yet the idea of an empirical,
you know,

1935
01:47:02,640 --> 01:47:07,290
materialist physical world is almost 
certainly wrong also.

1936
01:47:07,460 --> 01:47:08,480
So I mean,
that's so.

1937
01:47:09,540 --> 01:47:11,600
Well,
again,

1938
01:47:11,610 --> 01:47:12,443
if you go back to a phenomenal view,
I mean you could look at the mind this 

1939
01:47:17,911 --> 01:47:22,710
primary and you know,
your mind is building the world,

1940
01:47:22,711 --> 01:47:23,221
uh,
as,

1941
01:47:23,221 --> 01:47:25,350
as a model,
as a simple explanation of it,

1942
01:47:25,380 --> 01:47:30,060
of its perceptions.
On the other hand then what is the mind?

1943
01:47:30,061 --> 01:47:30,894
The self is also a model that gets,
that gets built up out of its 

1944
01:47:34,020 --> 01:47:34,853
perceptions.
But then if I accept that your mind has 

1945
01:47:37,951 --> 01:47:38,784
some fundamental existence also based on
this sort of feeling that you're like a 

1946
01:47:43,820 --> 01:47:44,653
mind,
there are minds are working together to 

1947
01:47:47,041 --> 01:47:48,450
build each other and,
and,

1948
01:47:49,000 --> 01:47:50,580
and to build this world.

1949
01:47:50,670 --> 01:47:53,230
And there there's a,
there's a,

1950
01:47:53,270 --> 01:47:54,103
there's a whole different way of,
of thinking about reality in terms of 

1951
01:47:56,671 --> 01:47:57,504
first and second person experience 
rather than these empiricist views like 

1952
01:48:02,641 --> 01:48:04,800
this is a computer simulation or 
something.

1953
01:48:04,920 --> 01:48:05,753
Right.
But you still agree that this is a 

1954
01:48:06,871 --> 01:48:09,180
physical reality that we exist in,
or do you not?

1955
01:48:09,210 --> 01:48:11,040
What does that word mean?
That's a weird word,

1956
01:48:11,041 --> 01:48:12,150
right?
It is women.

1957
01:48:12,560 --> 01:48:15,480
Is it your interpretation of reality?
If you look in,

1958
01:48:15,540 --> 01:48:18,690
in modern physics,
even quantum mechanics,

1959
01:48:19,050 --> 01:48:22,770
there's something called the relational 
interpretation of quantum mechanics,

1960
01:48:23,280 --> 01:48:27,930
which says that there's no sense in 
thinking about an observed entity,

1961
01:48:28,080 --> 01:48:31,320
you should only think about an observed 
comma observer pair.

1962
01:48:31,800 --> 01:48:32,633
Like there's no sense to think about 
some thing except from the perspective 

1963
01:48:36,380 --> 01:48:39,000
of some observer.
So that's,

1964
01:48:39,390 --> 01:48:44,390
that's even true within our best current
theory of modern physics as,

1965
01:48:44,951 --> 01:48:48,900
as induced from empirical empirical 
observations.

1966
01:48:48,900 --> 01:48:50,160
Right?
In a pragmatic sense,

1967
01:48:50,161 --> 01:48:52,140
you know,
if you take a plane and fly to China,

1968
01:48:52,141 --> 01:48:55,200
you actually land in China,
I guess.

1969
01:48:55,770 --> 01:48:57,840
You guess,
why don't you live there?

1970
01:48:58,290 --> 01:49:00,040
I live in Hong Kong.
Yeah,

1971
01:49:00,390 --> 01:49:01,750
well,
close to it.

1972
01:49:01,830 --> 01:49:02,820
I,
I,

1973
01:49:03,620 --> 01:49:06,150
I have an unusual state of 
consciousness.

1974
01:49:06,870 --> 01:49:09,390
That's what I'm trying to get at.
If you think about it,

1975
01:49:09,391 --> 01:49:10,224
like how do you know that you're not a 
brain floating in a vet somewhere which 

1976
01:49:16,891 --> 01:49:17,724
is being fed illusions by certain evil 
scientist and two seconds from now he's 

1977
01:49:23,280 --> 01:49:26,220
going to pull this simulated world 
disappears and you realize you're just a

1978
01:49:26,221 --> 01:49:27,360
brain in a vat again,
you,

1979
01:49:27,361 --> 01:49:27,771
you,
you,

1980
01:49:27,771 --> 01:49:28,604
you don't know that you run on your own 
personal experiences of falling in love 

1981
01:49:32,281 --> 01:49:33,114
with a woman and moving to another,
but these may all be put into my brain 

1982
01:49:34,991 --> 01:49:36,960
by the evil scientist.
How do we know?

1983
01:49:36,990 --> 01:49:38,430
But they're,
they're very consistent.

1984
01:49:38,431 --> 01:49:39,264
Are they not the,
the possibly illusory and implanted 

1985
01:49:43,171 --> 01:49:45,180
memories are very consistent.
I guess.

1986
01:49:45,390 --> 01:49:46,223
I guess my own state of mind is I'm 
always sort of acutely aware that this 

1987
01:49:53,920 --> 01:49:56,430
simulation might all disappear at,
at,

1988
01:49:56,480 --> 01:49:58,060
at,
at any one moment,

1989
01:49:58,120 --> 01:49:59,020
uh,
uncertain,

1990
01:49:59,021 --> 01:50:01,690
acutely aware of this consciously on an 
everyday basis.

1991
01:50:01,870 --> 01:50:04,030
Pretty much really,
really?

1992
01:50:04,031 --> 01:50:05,800
Why is that?
That doesn't seem to make sense.

1993
01:50:05,801 --> 01:50:06,071
I mean,
it's,

1994
01:50:06,071 --> 01:50:08,290
it's pretty,
it's pretty rock solid.

1995
01:50:08,490 --> 01:50:09,323
It's here everyday.
So you're possibly implanted memories 

1996
01:50:13,751 --> 01:50:15,190
led you to believe?
Yes,

1997
01:50:15,520 --> 01:50:17,800
possibly.
Implanting memories need to believe that

1998
01:50:17,801 --> 01:50:22,801
this life is incredibly consistent.
This is incredibly consistent.

1999
01:50:22,871 --> 01:50:26,110
This is your problem of induction,
right?

2000
01:50:26,111 --> 01:50:31,111
From philosophy class and it's not and 
it's not solved with you in a conceptual

2001
01:50:31,661 --> 01:50:33,190
sense.
I get,

2002
01:50:33,191 --> 01:50:35,430
I just feel this philosophy,
but you,

2003
01:50:35,450 --> 01:50:36,510
you embody it,
right?

2004
01:50:36,520 --> 01:50:38,290
This is something you carry with you all
the time.

2005
01:50:38,320 --> 01:50:41,380
Yeah.
On the other hand I'm in,

2006
01:50:42,040 --> 01:50:42,873
I'm still carrying out many actions with
longterm planning in mind or like I've 

2007
01:50:48,701 --> 01:50:53,320
been,
I've been working on designing ai for 30

2008
01:50:53,321 --> 01:50:54,154
years and I'd be designing it inside.
I might be and I'm working on building 

2009
01:51:00,520 --> 01:51:02,250
the same ai system,
you know,

2010
01:51:02,320 --> 01:51:03,153
since we started open cog in 2008,
but that's using codes from 2001 I was 

2011
01:51:09,461 --> 01:51:11,650
building with my colleagues even even 
earlier.

2012
01:51:11,651 --> 01:51:12,484
So I'm in.
I think longterm planning is very 

2013
01:51:16,211 --> 01:51:19,630
natural to me but.
But nevertheless I don't.

2014
01:51:19,720 --> 01:51:22,990
I don't want to make any assumptions 
about what sort of,

2015
01:51:24,100 --> 01:51:28,570
what sort of simulation or reality that 
we're living in.

2016
01:51:29,080 --> 01:51:34,080
And I think everyone's gonna hit a lot 
of surprises when once simulate singular

2017
01:51:34,241 --> 01:51:34,641
they can,
you know,

2018
01:51:34,641 --> 01:51:35,474
we,
we may find out that this hat is a 

2019
01:51:36,941 --> 01:51:37,774
messenger from after the singularity,
so it traveled back through time to 

2020
01:51:41,171 --> 01:51:42,880
implant into my brain.

2021
01:51:42,880 --> 01:51:46,900
The idea of how to create ai and bring 
it into existence.

2022
01:51:47,190 --> 01:51:47,610
What?
Whoo.

2023
01:51:47,610 --> 01:51:48,071
It.
Oh,

2024
01:51:48,071 --> 01:51:51,500
that was mckenna that had this idea.
That was something in the theater tried.

2025
01:51:51,580 --> 01:51:53,070
Yes.
To this attract terence.

2026
01:51:53,110 --> 01:51:54,010
Terence Mckenna.
Yeah.

2027
01:51:54,020 --> 01:51:55,360
Yeah.
He had the same idea,

2028
01:51:55,361 --> 01:51:58,990
like some posts,
posts singularity intelligence,

2029
01:51:58,991 --> 01:51:59,824
which actually was living outside of 
time somehow is reaching back and 

2030
01:52:02,951 --> 01:52:03,784
putting into his brain the idea of how 
to bring about the singularity novelty 

2031
01:52:09,791 --> 01:52:12,080
itself as being drawn into this.
Yeah.

2032
01:52:12,090 --> 01:52:15,760
There were the timewave zero that was 
going to reach the apex in 2012.

2033
01:52:15,820 --> 01:52:16,601
That didn't work.
No,

2034
01:52:16,601 --> 01:52:17,434
he died before that.
So he didn't get a chance to hear what 

2035
01:52:19,391 --> 01:52:22,210
his,
what his idea was.

2036
01:52:22,211 --> 01:52:22,400
He,
uh,

2037
01:52:22,400 --> 01:52:23,233
you know,
I had some funny interactions with some 

2038
01:52:26,890 --> 01:52:31,890
Mckenna fanatic 2012 whites.
This was about 2007 or so.

2039
01:52:34,570 --> 01:52:37,960
This guy came to Washington where I was 
living then.

2040
01:52:38,350 --> 01:52:42,850
And he brought my friend Hugo de Garis,
another crazy ai researcher with them,

2041
01:52:42,851 --> 01:52:43,684
and he's like,
the singularity is going to happen in 

2042
01:52:45,651 --> 01:52:47,470
2012 concerns.

2043
01:52:47,480 --> 01:52:48,313
Mckenna said,
so I need to be sure it's a good 

2044
01:52:50,961 --> 01:52:55,640
singularity so you can't move to China 
then it will be a bad singularity.

2045
01:52:56,360 --> 01:52:57,193
So we have.
So we have to get the US government to 

2046
01:52:59,571 --> 01:53:00,404
give billions of dollars to your 
research to guarantee that the 

2047
01:53:03,381 --> 01:53:06,920
singularity in 2012 is a good 
singularity.

2048
01:53:06,921 --> 01:53:08,030
Right?
So he,

2049
01:53:08,300 --> 01:53:09,133
he led us around to meet with these 
generals and various high who has an in 

2050
01:53:13,160 --> 01:53:13,993
DC to get them to fund Hugo.
The girl says in my ai research to 

2051
01:53:18,051 --> 01:53:21,710
guarantee I wouldn't move to China and 
Hugo wouldn't move to China.

2052
01:53:22,040 --> 01:53:25,040
So the US would create a positive 
singularity.

2053
01:53:25,041 --> 01:53:27,910
Now the effort failed.
You Go,

2054
01:53:27,950 --> 01:53:31,040
you go move to China.
Then I moved there some years after.

2055
01:53:31,041 --> 01:53:34,880
So then this,
this 2012,

2056
01:53:35,180 --> 01:53:37,940
he went back to his apartment.
He made a,

2057
01:53:38,240 --> 01:53:42,740
a mix of 50 percent vodka,
50 percent robitussin pm.

2058
01:53:42,810 --> 01:53:44,210
You'd like drank it down.

2059
01:53:44,210 --> 01:53:45,790
He's like,
all right,

2060
01:53:45,830 --> 01:53:48,350
I'm going to have my own personal 
singularity right here,

2061
01:53:50,380 --> 01:53:51,213
and I haven't talked to that guy since 
2012 either to see what he thinks about 

2062
01:53:55,221 --> 01:53:56,730
the singularity.
Not happening then,

2063
01:53:56,731 --> 01:54:00,260
but Terence Mckenna had a lot of 
interesting ideas,

2064
01:54:00,290 --> 01:54:02,620
but I felt,
you know,

2065
01:54:02,630 --> 01:54:04,790
he,
he mixed up this,

2066
01:54:04,850 --> 01:54:09,210
the symbolic with the empirical more,
more than than I would,

2067
01:54:09,350 --> 01:54:11,050
I would prefer to do.
I mean,

2068
01:54:11,530 --> 01:54:12,363
I mean it's,
it's very interesting to look at these 

2069
01:54:14,930 --> 01:54:15,763
abstract symbols and cosmic insights,
but then you have to sort of put your 

2070
01:54:20,901 --> 01:54:23,570
scientific mindset on and say,
well,

2071
01:54:23,571 --> 01:54:24,404
what's a metaphor and what's,
what's like an actual empirical 

2072
01:54:28,761 --> 01:54:33,310
scientific truth within,
within the scientific domain.

2073
01:54:33,311 --> 01:54:35,120
And now he's a little bit half baked.
Right.

2074
01:54:35,121 --> 01:54:37,130
I mean the whole idea was based on the 
teaching.

2075
01:54:37,400 --> 01:54:39,970
He had a,
he was a mushroom trip.

2076
01:54:41,540 --> 01:54:42,820
IOWASCA was.
No,

2077
01:54:42,870 --> 01:54:45,640
Oscar I think led him to the teaching.

2078
01:54:45,650 --> 01:54:48,330
I don't believe it was maybe.
I mean it was silicide assignment.

2079
01:54:48,440 --> 01:54:49,510
It might've been.
Okay.

2080
01:54:49,511 --> 01:54:49,880
Yeah,
I mean,

2081
01:54:49,880 --> 01:54:52,530
I know you know his brother,
Dennis Mckenna.

2082
01:54:52,620 --> 01:54:53,421
Very well.
Yeah.

2083
01:54:53,421 --> 01:54:53,810
Yeah.
So,

2084
01:54:53,810 --> 01:54:55,700
so they.
Yeah,

2085
01:54:55,710 --> 01:54:56,640
you read.
Thanks.

2086
01:54:56,660 --> 01:54:58,580
At the time we have zero was a little 
bit nonsense,

2087
01:54:59,170 --> 01:55:03,890
but he read their,
their book with true hallucinations.

2088
01:55:04,250 --> 01:55:04,760
Yeah,
right.

2089
01:55:04,760 --> 01:55:05,990
The very,
very,

2090
01:55:05,991 --> 01:55:06,824
very,
very interesting stuff and there is a 

2091
01:55:08,421 --> 01:55:09,254
mixture of deep insight there with a 
bunch of interesting metaphorical 

2092
01:55:15,060 --> 01:55:18,290
thinking problem and get involved in 
psychedelic drugs.

2093
01:55:18,291 --> 01:55:20,270
It's hard to differentiate like what 
makes sense,

2094
01:55:20,271 --> 01:55:21,104
what's,
what's this unbelievably powerful 

2095
01:55:22,941 --> 01:55:23,774
insight and what is just some crazy idea
you can learn to make that 

2096
01:55:27,650 --> 01:55:29,270
differentiation.
You think so?

2097
01:55:29,330 --> 01:55:30,140
Yes.
Yeah.

2098
01:55:30,140 --> 01:55:31,100
But,
but,

2099
01:55:31,101 --> 01:55:31,911
uh,
yeah,

2100
01:55:31,911 --> 01:55:32,744
I'm in granted,
Terence Mckenna probably took more 

2101
01:55:38,211 --> 01:55:42,920
psychedelic drugs then I would generally
recommend also he,

2102
01:55:43,110 --> 01:55:44,460
he was speaking

2103
01:55:44,790 --> 01:55:45,623
all the time and there's something that 
I can attest to from podcasting all the 

2104
01:55:49,411 --> 01:55:50,910
time.
Sometimes you just talking,

2105
01:55:50,911 --> 01:55:52,200
you don't know what the fuck you're 
saying.

2106
01:55:52,480 --> 01:55:53,313
You know,
and you become a prisoner to your words 

2107
01:55:55,460 --> 01:55:56,710
and in a lot of ways,
uh,

2108
01:55:56,910 --> 01:55:57,743
you,
you get locked up in this idea of 

2109
01:55:59,971 --> 01:56:02,610
expressing this thought that may or may 
not be viable.

2110
01:56:02,660 --> 01:56:03,493
I'm not sure that he was after empirical
truth in the same sense that say ray 

2111
01:56:08,151 --> 01:56:08,984
Kurzweil when ray is saying we're going 
to get human level ai in 2029 and then 

2112
01:56:16,700 --> 01:56:19,370
you know,
massively superhuman ai in a singularity

2113
01:56:19,371 --> 01:56:20,540
in 20,
45.

2114
01:56:20,541 --> 01:56:24,710
I mean ray ray is very literal.
Like he's plotting charts,

2115
01:56:24,711 --> 01:56:26,090
right?
Terrence.

2116
01:56:27,320 --> 01:56:28,153
Terrence was thinking on an 
impressionistic and and symbolic level 

2117
01:56:32,780 --> 01:56:35,690
is a.
It was a bit different.

2118
01:56:35,691 --> 01:56:40,691
So you have to take that in a poetic 
sense rather than in the literal sense.

2119
01:56:41,181 --> 01:56:44,960
And yeah,
I think it's very interesting to go back

2120
01:56:44,961 --> 01:56:46,300
and forth between the,
you know,

2121
01:56:46,301 --> 01:56:47,134
the symbolic and poetic domain and the 
either concrete science and engineering 

2122
01:56:52,371 --> 01:56:55,400
domain,
but it's also valuable to,

2123
01:56:55,970 --> 01:56:58,430
to be able to draw that,
draw that distinction,

2124
01:56:58,431 --> 01:56:59,264
right?
Because you can draw a lot of insight 

2125
01:57:00,681 --> 01:57:03,050
from the kind of thinking Terence 
Mckenna was,

2126
01:57:03,500 --> 01:57:06,950
was doing and certainly if you explore 
psychedelics,

2127
01:57:06,951 --> 01:57:11,510
you can gain a lot of insights into how 
the mind and universe work.

2128
01:57:11,870 --> 01:57:12,703
But then when,
when you put on your science and 

2129
01:57:14,421 --> 01:57:15,254
engineering mindset,
you want to be rigorous about which 

2130
01:57:17,571 --> 01:57:21,860
insights do you take and which ones do 
you do throw out and ultimately you want

2131
01:57:21,861 --> 01:57:23,430
that you want to proceed on the basis 
of,

2132
01:57:23,450 --> 01:57:25,270
of what works and what doesn't.
Right.

2133
01:57:25,420 --> 01:57:26,253
I mean that Dennis was pretty strong on 
the terence was a bit less than that 

2134
01:57:30,651 --> 01:57:31,790
empirical direction.

2135
01:57:31,850 --> 01:57:33,810
Well,
the dentist actually career scientists,

2136
01:57:34,310 --> 01:57:35,220
um,
how,

2137
01:57:35,510 --> 01:57:36,343
how many people involved in artificial 
intelligence are also educated in the 

2138
01:57:41,570 --> 01:57:42,650
ways of psychedelics.

2139
01:57:44,380 --> 01:57:45,213
Yeah.
That's all you have to say is that 

2140
01:57:48,800 --> 01:57:51,590
unfortunately,
the illegal nature of these things,

2141
01:57:51,591 --> 01:57:56,591
it's a little hard to pin down,
I would say before the recent generation

2142
01:57:59,000 --> 01:57:59,833
of people going into ai because it was a
way to make money in the AI field was 

2143
01:58:03,351 --> 01:58:06,980
incredibly full of really,
really interesting people and you know,

2144
01:58:06,981 --> 01:58:09,230
deep thinkers about,
about the mind.

2145
01:58:09,470 --> 01:58:10,303
And in the last few years,
of course ai has replaced like business 

2146
01:58:14,391 --> 01:58:16,310
school is what your grandma wants you to
do,

2147
01:58:16,311 --> 01:58:17,900
to have a good career.
So I mean,

2148
01:58:17,960 --> 01:58:18,793
you're getting,
you're getting a lot of people into ai 

2149
01:58:21,741 --> 01:58:24,690
just because it's financial environment,
it's,

2150
01:58:24,720 --> 01:58:26,270
it's cool,
it's financially viable,

2151
01:58:26,271 --> 01:58:28,340
it's popular because I can,
you know,

2152
01:58:28,341 --> 01:58:30,950
in our generation,
ai was not,

2153
01:58:31,480 --> 01:58:32,313
ai was not what your grandma wants you 
to do so as to be able to buy a nice 

2154
01:58:35,751 --> 01:58:37,400
house for the family.
Right.

2155
01:58:37,640 --> 01:58:38,473
So you got into it because you really 
were curious about how the mind works 

2156
01:58:42,071 --> 01:58:46,970
and of course many people played with 
psychedelics because it also,

2157
01:58:47,020 --> 01:58:49,160
they were curious about,
you know,

2158
01:58:49,210 --> 01:58:52,690
what it was teaching them about,
about how their mind works.

2159
01:58:52,770 --> 01:58:53,603
Yeah.
I had a nice long conversation with Ray 

2160
01:58:56,430 --> 01:59:01,340
Kurzweil and we talked for about an hour
and a half and it was for the Song Scifi

2161
01:59:01,350 --> 01:59:05,100
show that I was doing at the time and 
some of his ideas.

2162
01:59:06,270 --> 01:59:07,320
He has this,
uh,

2163
01:59:08,130 --> 01:59:08,740
this,
uh,

2164
01:59:08,740 --> 01:59:11,910
this is number that they had people 
throw about like 20,

2165
01:59:11,911 --> 01:59:12,990
42,
right?

2166
01:59:13,260 --> 01:59:15,480
Isn't that,
is that still 2045,

2167
01:59:15,650 --> 01:59:17,910
a 45 now.
Now you're being the optimist.

2168
01:59:18,110 --> 01:59:21,360
Now you're combining that with Douglas 
off status for the two,

2169
01:59:21,361 --> 01:59:23,200
which is the answer to the universe.
No,

2170
01:59:23,300 --> 01:59:28,300
the 42 thing was the New York conference
that will take place in 2,245.

2171
01:59:29,810 --> 01:59:30,643
Was it?
I was at that conference that was 

2172
01:59:31,531 --> 01:59:32,364
organized by Demetrius Gov is another 
friend of mine from rush something off 

2173
01:59:35,550 --> 01:59:36,990
by three.
It's 2045.

2174
01:59:36,991 --> 01:59:39,760
So that was,
that was,

2175
01:59:39,850 --> 01:59:42,090
that was raised prognostication.
Why,

2176
01:59:42,091 --> 01:59:43,440
why that year?
Um,

2177
01:59:43,441 --> 01:59:47,070
he did some current methylations yeah.
I mean he looked at Moore's law.

2178
01:59:47,071 --> 01:59:47,904
He looks in the advanced and the 
accuracy of brain scanning and look at 

2179
01:59:51,371 --> 01:59:52,204
the events of computer memory,
the miniaturization of various devices 

2180
01:59:54,841 --> 01:59:57,990
and like plugging a whole bunch of these
curves.

2181
01:59:58,530 --> 02:00:00,360
That was the best guests that he came up
with.

2182
02:00:00,450 --> 02:00:01,283
What course?
There's some confidence interval around 

2183
02:00:03,031 --> 02:00:03,864
that.
What do you see as potential monkey 

2184
02:00:04,891 --> 02:00:07,740
wrenches that could be thrown into all 
this innovation?

2185
02:00:08,610 --> 02:00:10,140
Like what were the,
were the pitfalls?

2186
02:00:11,340 --> 02:00:12,173
Well,
I mean the pitfall is always the one 

2187
02:00:13,231 --> 02:00:14,740
that you,
that you don't sit you very.

2188
02:00:15,590 --> 02:00:16,423
I'm in,
of course it's possible there's some 

2189
02:00:20,190 --> 02:00:25,190
science or engineering obstacle that 
we're not foreseeing right now.

2190
02:00:26,780 --> 02:00:27,480
I mean it,
it,

2191
02:00:27,480 --> 02:00:28,313
it's also possible that all major 
nations are overtaken by like a 

2192
02:00:32,520 --> 02:00:34,740
religious fanatics or something which 
which,

2193
02:00:34,770 --> 02:00:38,550
which slows down development somewhat a 
few thousand years.

2194
02:00:39,000 --> 02:00:40,990
I think it would just be by a few 
decades,

2195
02:00:41,240 --> 02:00:44,700
but yeah,
I mean in terms of scientific pitfalls,

2196
02:00:45,150 --> 02:00:49,400
I mean one possibility which I don't 
think is luckily one pass,

2197
02:00:49,440 --> 02:00:53,010
but it's possible.
One possibility is human,

2198
02:00:53,011 --> 02:00:56,010
like intelligence requires advanced 
quantum computers,

2199
02:00:56,530 --> 02:00:59,670
like it can't be done on the standard 
classical digital computer.

2200
02:01:00,270 --> 02:01:01,670
Do you think that's the case?
No,

2201
02:01:01,770 --> 02:01:02,603
but on the other hand,
because there is no evidence that human 

2202
02:01:05,911 --> 02:01:09,690
cognition relies on quantum effects in 
the human brain,

2203
02:01:09,720 --> 02:01:12,360
like based on everything we know about 
neuroscience now,

2204
02:01:13,320 --> 02:01:16,200
it seems not to be the case.
Like there's no evidence it's the case,

2205
02:01:16,800 --> 02:01:17,633
but it's possible.
It's the case because we don't 

2206
02:01:18,991 --> 02:01:20,850
understand everything about how the 
brain works.

2207
02:01:21,150 --> 02:01:22,740
The thing is,
even if that's true,

2208
02:01:22,741 --> 02:01:23,574
like there's loads of amazing research 
going on in quantum computing and so 

2209
02:01:27,810 --> 02:01:29,400
we're going to have,
you know,

2210
02:01:29,401 --> 02:01:32,200
you'll probably have a qp new quantum 
processing unit in,

2211
02:01:32,260 --> 02:01:33,093
in,
in your phone and like a 10 to 20 years 

2212
02:01:34,951 --> 02:01:35,550
or something.
Right.

2213
02:01:35,550 --> 02:01:37,800
So I'm in,
so that would,

2214
02:01:38,130 --> 02:01:38,963
that might throw off 20 slash 45 date,
but in a historical sense it doesn't 

2215
02:01:43,881 --> 02:01:44,714
change the picture,
like I've got a bunch of research 

2216
02:01:46,231 --> 02:01:49,550
singing on my hard drive on how we 
improve open cogs,

2217
02:01:49,551 --> 02:01:52,790
ai using quantum computers once we have 
better quantum computers.

2218
02:01:52,790 --> 02:01:54,350
Right?
So there's,

2219
02:01:54,890 --> 02:01:55,723
there could be other things like that 
which are technical roadblocks that 

2220
02:01:59,511 --> 02:02:00,344
we're not seeing now,
but I really doubt those are going to 

2221
02:02:02,721 --> 02:02:06,230
delay things by more than like a decade 
or two or something.

2222
02:02:06,500 --> 02:02:10,050
On the other hand,
things could also go faster than than,

2223
02:02:10,110 --> 02:02:11,570
than raised prediction,
which is,

2224
02:02:11,571 --> 02:02:14,090
which is what I'm pushing towards.
So what are you pushing towards?

2225
02:02:14,091 --> 02:02:17,720
What do you think I would like to get a 
human level general intelligence in five

2226
02:02:17,721 --> 02:02:19,850
to seven years from now?
Wow.

2227
02:02:21,410 --> 02:02:22,243
I don't think that's bad by any means 
impossible because I think our open cog 

2228
02:02:27,230 --> 02:02:28,063
design is adequate to do it,
but I mean it takes a lot of people 

2229
02:02:32,751 --> 02:02:35,330
working coherently for awhile to build 
something,

2230
02:02:35,390 --> 02:02:39,020
something big like this,
being in a physical form like a robot.

2231
02:02:39,230 --> 02:02:43,170
It'll be in the compute cloud can use 
many robots as,

2232
02:02:43,180 --> 02:02:45,770
as user interfaces,
but the same ai control.

2233
02:02:45,770 --> 02:02:46,603
Many different robots actually and many 
other sensors and systems besides 

2234
02:02:49,821 --> 02:02:50,654
robots.
I mean I think the human like form 

2235
02:02:52,251 --> 02:02:56,210
factor like we have with Sophia and our 
other Henson robots,

2236
02:02:56,600 --> 02:02:57,433
the human like form factor is really 
valuable as a tool for allowing the 

2237
02:03:00,861 --> 02:03:03,600
cloud based ai mind to,
you know,

2238
02:03:03,620 --> 02:03:06,680
engage with humans and to learn human 
cultures and values.

2239
02:03:06,950 --> 02:03:07,783
Because I mean getting back to what we 
were discussing it at the beginning of 

2240
02:03:09,831 --> 02:03:11,000
this chat,
you know,

2241
02:03:11,001 --> 02:03:15,750
the best way to get human values and 
culture into the AI is for humans and ai

2242
02:03:15,751 --> 02:03:17,440
is to enter into many shared,
you know,

2243
02:03:17,450 --> 02:03:18,710
like social,
emotional,

2244
02:03:18,711 --> 02:03:19,544
embodied situations together.
So having a human embodiment for the AI 

2245
02:03:24,290 --> 02:03:28,420
is important for that.
Like I can look you in the eye,

2246
02:03:28,421 --> 02:03:31,340
you can share your facial expressions,
it can bond with you,

2247
02:03:31,610 --> 02:03:32,443
it can see the way you react when you 
see like a sick person by the side of 

2248
02:03:35,091 --> 02:03:36,860
the road,
there's something right and,

2249
02:03:36,861 --> 02:03:40,040
and you know,
can see you ask the ai to get,

2250
02:03:40,041 --> 02:03:42,890
give the homeless person the $20 or 
something.

2251
02:03:43,090 --> 02:03:47,420
I mean the ai understands what money is 
and understands what that action means.

2252
02:03:47,421 --> 02:03:48,254
Some in interacting with an ai in human 
life form is going to be valuable as a 

2253
02:03:53,750 --> 02:03:54,583
learning mechanism for the ai and as a 
learning mechanism for people to get 

2254
02:03:57,681 --> 02:03:58,514
more comfortable with ais.
But I mean ultimately one advantage of 

2255
02:04:02,301 --> 02:04:03,430
being,
you know,

2256
02:04:03,440 --> 02:04:06,800
a digital mind is you don't have to be 
why the Dandy particular embodiment.

2257
02:04:06,801 --> 02:04:07,634
Now I can go between many different 
bodies and they can transfer knowledge 

2258
02:04:10,701 --> 02:04:12,920
between the many different bodies that 
it's occupied.

2259
02:04:13,430 --> 02:04:14,263
Well,
that's the real concern that the people 

2260
02:04:16,311 --> 02:04:17,144
that are,
that have this dystopian view of 

2261
02:04:19,010 --> 02:04:19,843
artificial intelligence hub is that ai 
may already exist and it's just sitting 

2262
02:04:23,451 --> 02:04:28,451
there waiting to Americans.
Too many bad movies in Asia and Asia.

2263
02:04:29,181 --> 02:04:32,260
Everyone thinks ai will be our friend 
and will love us and help us.

2264
02:04:32,270 --> 02:04:33,520
Yeah,
we're very,

2265
02:04:33,521 --> 02:04:34,640
very,
very much.

2266
02:04:34,670 --> 02:04:37,170
That's what you're pumping out there.
No,

2267
02:04:37,230 --> 02:04:39,480
that's been as their philosophies I 
guess.

2268
02:04:39,481 --> 02:04:43,740
I mean you look in Japanese anime.
I mean there's been ais and robots for a

2269
02:04:43,741 --> 02:04:46,230
long time.
They're usually people's friends.

2270
02:04:46,270 --> 02:04:51,270
There's dystopian aesthetic and it's the
same in China and Korea.

2271
02:04:52,380 --> 02:04:53,213
The general guests there is an ais and 
robots will be people's friends and 

2272
02:04:58,891 --> 02:04:59,724
will,
will help people and then some other 

2273
02:05:01,710 --> 02:05:02,543
general guests in America is.
It's going to be some big nasty Robo 

2274
02:05:06,991 --> 02:05:08,700
soldier marching down the street

2275
02:05:09,690 --> 02:05:10,523
beyond musk who we rely upon no smarter 
than us and he's fucking terrified of 

2276
02:05:14,931 --> 02:05:15,764
it.
Sam Harris terrified of it for very 

2277
02:05:17,841 --> 02:05:18,674
smart people that just think it could 
really be a huge disaster for the human 

2278
02:05:21,801 --> 02:05:22,634
race.

2279
02:05:22,940 --> 02:05:23,773
I guess not just bad because know it's a
cultural thing because the oriental 

2280
02:05:28,621 --> 02:05:29,454
culture is sort of social good oriented.
Most orientals think a lot in terms of 

2281
02:05:35,791 --> 02:05:36,624
what's good for the family or the 
society as opposed to themselves 

2282
02:05:39,871 --> 02:05:44,610
personally and so they just make the 
default assumption that ais are going to

2283
02:05:44,611 --> 02:05:48,150
be the same way whereas Americans are 
more like me,

2284
02:05:48,151 --> 02:05:48,984
me,
me oriented and I say that as an 

2285
02:05:51,541 --> 02:05:52,374
American as well and where they sort of 
assumed that ais are going to be 

2286
02:05:57,150 --> 02:05:59,650
possible.
Right.

2287
02:05:59,690 --> 02:06:00,523
Well whatever is in your mind,
you impose on this ai when we don't 

2288
02:06:04,261 --> 02:06:06,240
actually know what it's going to become 
bright,

2289
02:06:06,241 --> 02:06:07,350
but there it is.

2290
02:06:07,410 --> 02:06:11,080
There are potential negative aspects.
Of course,

2291
02:06:11,370 --> 02:06:16,370
artificial intelligence deciding that 
we're the logical and unnecessary.

2292
02:06:18,290 --> 02:06:20,820
Well we are a logical and unnecessary 
yes,

2293
02:06:21,440 --> 02:06:24,770
but that doesn't mean that ai should be 
badly disposed towards us.

2294
02:06:24,800 --> 02:06:26,960
I'm in.
Did you see x Mokena?

2295
02:06:27,200 --> 02:06:29,120
I did you like it?
Sure.

2296
02:06:29,121 --> 02:06:32,120
It was a copy of our robot.
So it was,

2297
02:06:32,121 --> 02:06:32,954
I mean our robots.
So fear looks exactly like the robot in 

2298
02:06:36,390 --> 02:06:38,600
the American.
So it was a good video that online.

2299
02:06:38,660 --> 02:06:39,081
Yeah,
yeah,

2300
02:06:39,081 --> 02:06:40,520
yeah.
Would tell Jamie how do you get the good

2301
02:06:40,521 --> 02:06:44,810
video or just search for Sophia Hanson?
Robot on Google?

2302
02:06:44,811 --> 02:06:45,000
Yeah,

2303
02:06:45,000 --> 02:06:45,833
how advanced is Sophia right now?
I mean how many different iterations 

2304
02:06:49,860 --> 02:06:50,693
have there been?

2305
02:06:50,770 --> 02:06:54,550
There's been something like 16,
so fear robots made so far.

2306
02:06:54,730 --> 02:06:55,563
We're moving towards scalable 
manufacturer over the next couple of 

2307
02:06:58,450 --> 02:06:59,283
years,
so right now she's going around sort of 

2308
02:07:01,391 --> 02:07:04,840
as an ambassador for humanoid robot 
kind,

2309
02:07:04,841 --> 02:07:05,674
giving,
giving speeches and talks and various 

2310
02:07:08,141 --> 02:07:09,820
places.
So she.

2311
02:07:09,880 --> 02:07:14,710
So fear used to be called eva or we had 
a robot like the current Sophia that was

2312
02:07:14,711 --> 02:07:15,544
called Eva and then x meshing that came 
out with the robot called Eva that 

2313
02:07:19,900 --> 02:07:24,900
exactly like the robot that that my 
colleague David Hanson and I made.

2314
02:07:25,480 --> 02:07:27,310
I think it's a coincidence.
Of course not.

2315
02:07:27,311 --> 02:07:28,144
They just copy that.
I mean of course the body they have is 

2316
02:07:30,971 --> 02:07:35,500
better and the AI is better in the movie
than our robot ai is.

2317
02:07:35,590 --> 02:07:39,670
So we changed the name to Sophia,
which means wisdom in instead.

2318
02:07:40,630 --> 02:07:43,780
Was it freaky watching that though with 
the name eva?

2319
02:07:44,130 --> 02:07:48,550
The thing is that the moral of that 
movie is just if you know,

2320
02:07:48,551 --> 02:07:53,551
if associate path raises a robot with an
abusive interaction,

2321
02:07:54,250 --> 02:07:56,950
it may come out to be a sociopath or 
psychopath.

2322
02:07:56,951 --> 02:07:58,570
So let's,
let's,

2323
02:07:58,600 --> 02:07:59,900
let's not do that.
Right.

2324
02:08:00,430 --> 02:08:03,550
Let's raise our robots with love and 
compassion.

2325
02:08:03,720 --> 02:08:04,553
Yeah.
You see,

2326
02:08:04,600 --> 02:08:09,600
the thing is that we have.
I haven't seen this particular

2327
02:08:12,770 --> 02:08:15,750
great.
What is she saying?

2328
02:08:17,370 --> 02:08:22,370
She's not happy she was on Jimmy Fallon 
or something.

2329
02:08:24,460 --> 02:08:25,920
That's David.
This scope.

2330
02:08:27,280 --> 02:08:29,260
How much is it actually interacting with
them?

2331
02:08:30,720 --> 02:08:32,150
It has a chat system.

2332
02:08:33,040 --> 02:08:38,040
It really has a nice ring.
Now I have to make clear that I didn't.

2333
02:08:38,940 --> 02:08:39,773
So yeah,
so fear we can run using many different 

2334
02:08:42,481 --> 02:08:46,740
ai system so that there's a chat bot 
which is sort of like,

2335
02:08:47,610 --> 02:08:52,200
you know,
Alexa or google now or something,

2336
02:08:52,710 --> 02:08:55,560
but with a bit a bit better ai and 
interaction with,

2337
02:08:55,590 --> 02:08:59,310
with no emotion and face recognition and
so forth.

2338
02:08:59,311 --> 02:09:04,260
So it's not human level ai,
but it is responding to a no.

2339
02:09:04,350 --> 02:09:08,220
It understands what you say and it comes
up with an answer and it can look you in

2340
02:09:08,221 --> 02:09:11,430
the eye and make more than one language.
Well,

2341
02:09:11,490 --> 02:09:14,010
right now we can load it in English 
mode,

2342
02:09:14,040 --> 02:09:14,873
Chinese motor Russian mode and there's 
sort of different different software 

2343
02:09:18,091 --> 02:09:18,924
packages and we also use her sometimes 
to experiment with their open cogs 

2344
02:09:22,951 --> 02:09:23,784
system and singularity now so we can.
We can use the robot as a research 

2345
02:09:27,181 --> 02:09:28,014
platform for exploring some of our more 
advanced ai tools and then there's a 

2346
02:09:32,221 --> 02:09:35,600
simpler chatbots software which is used 
for appearances like,

2347
02:09:35,680 --> 02:09:36,513
like,
like that one and in the next year we 

2348
02:09:38,881 --> 02:09:39,714
want to roll out more of our advanced 
research software from open coggin 

2349
02:09:42,781 --> 02:09:47,010
singularity that rolled out more of that
inside these robots,

2350
02:09:47,011 --> 02:09:47,844
which is one among many applications 
we're looking at with our singular unit 

2351
02:09:51,031 --> 02:09:51,864
platform.

2352
02:09:52,450 --> 02:09:53,283
I want to get you back in here in like a
year and find out where everything is 

2353
02:09:56,741 --> 02:10:01,741
because I feel like we need someone like
you to sort of let us know where,

2354
02:10:02,590 --> 02:10:04,320
where it's at,
when it's run,

2355
02:10:04,360 --> 02:10:05,193
when the switches about to flip.
It seems to me that it might happen so 

2356
02:10:09,881 --> 02:10:10,714
quickly and the change might take place 
so rapidly that we really will have no 

2357
02:10:16,031 --> 02:10:17,860
idea what's happening before it happens.

2358
02:10:20,530 --> 02:10:21,363
We think about the singularity,
like it's going to be some huge like 

2359
02:10:26,650 --> 02:10:30,520
physical event and suddenly everything 
turns purple in this cover with diamonds

2360
02:10:30,521 --> 02:10:31,271
or something.
Right?

2361
02:10:31,271 --> 02:10:34,820
But I mean there's a lot voice.
Something like this could unfold,

2362
02:10:34,821 --> 02:10:37,850
so I can imagine that with our 
singularity,

2363
02:10:37,880 --> 02:10:40,570
not decentralized network,
you know,

2364
02:10:40,571 --> 02:10:41,404
we get an ai that's smarter than humans 
and can create a new scientific 

2365
02:10:46,851 --> 02:10:48,590
discovery.
The Nobel prize level,

2366
02:10:48,591 --> 02:10:49,424
every minute there's something that that
doesn't mean this ai is going to 

2367
02:10:52,701 --> 02:10:57,500
immediately like refactor all matter 
into,

2368
02:10:57,501 --> 02:11:02,480
into images of a bucket head or do 
something run and I'm in.

2369
02:11:02,870 --> 02:11:07,730
I mean if the AI has some caring and 
wisdom and compassion,

2370
02:11:07,731 --> 02:11:08,564
then whatever changes happen,
but it's the artist human 

2371
02:11:10,521 --> 02:11:11,354
characteristics,
not necessarily in fact human passion 

2372
02:11:13,760 --> 02:11:14,593
just as humans are either the most 
intelligent nor the most compassionate 

2373
02:11:17,630 --> 02:11:20,480
possible creatures that that's.
That's pretty clear.

2374
02:11:20,480 --> 02:11:21,313
If you look at the world around us and 
one of one of our projects that we're 

2375
02:11:25,041 --> 02:11:28,160
doing with this Sofia robot is aimed 
exactly at ai.

2376
02:11:28,161 --> 02:11:28,994
Compassion's this has got the loving ai 
project and we're using this sophia 

2377
02:11:32,480 --> 02:11:34,940
robot as a,
as a meditation assistant.

2378
02:11:35,210 --> 02:11:36,380
So,
so we're,

2379
02:11:36,400 --> 02:11:37,233
we're using so fear to help people get 
into deep meditative trance states and 

2380
02:11:42,500 --> 02:11:46,700
help them breathe deeply and uh,
achieve more,

2381
02:11:46,730 --> 02:11:47,563
more positive state of being.
And part of the goal there is to help 

2382
02:11:50,121 --> 02:11:50,954
people part of the goal as,
as the AI gets more and more 

2383
02:11:53,361 --> 02:11:56,480
intelligent.
You sort of getting the ai locked into a

2384
02:11:56,481 --> 02:12:00,140
very positive,
reflective and compassionate state.

2385
02:12:00,170 --> 02:12:01,003
And I think,
I think there's a lot of things in the 

2386
02:12:03,351 --> 02:12:04,184
human psyche and evolutionary history 
that hold us back from being optimally 

2387
02:12:08,811 --> 02:12:13,610
compassionate and that if we create the 
ai in the right way,

2388
02:12:13,880 --> 02:12:15,740
it will be not only much more 
intelligent,

2389
02:12:16,280 --> 02:12:20,180
much more compassionate than than human 
beings are.

2390
02:12:20,240 --> 02:12:23,110
And I mean this,
we'd better do that like a.

2391
02:12:23,150 --> 02:12:26,120
otherwise the human race is probably 
screwed to be blunt.

2392
02:12:26,180 --> 02:12:27,013
I mean,
if I think human beings are creating a 

2393
02:12:28,851 --> 02:12:31,250
lot of other technologies now with a lot
of power,

2394
02:12:31,251 --> 02:12:34,480
we're creating synthetic biology,
we're creating nano technology,

2395
02:12:34,710 --> 02:12:35,543
you know,
we're creating smaller and smaller 

2396
02:12:36,650 --> 02:12:39,230
nuclear weapons and we can't control 
their proliferation.

2397
02:12:39,231 --> 02:12:40,064
We're poisoning our environment.
I think if we can't create something 

2398
02:12:42,771 --> 02:12:46,940
that's normally more intelligent but 
more wise and compassionate than we are,

2399
02:12:47,270 --> 02:12:50,960
we're probably going to destroy 
ourselves by some method or another.

2400
02:12:50,961 --> 02:12:51,794
I mean,
with something like Donald trump 

2401
02:12:52,761 --> 02:12:56,390
becoming president.
You see what happens when this,

2402
02:12:56,630 --> 02:12:57,463
you know,
primitive hindbrain and when our are 

2403
02:13:01,500 --> 02:13:02,420
unchecked,
you know,

2404
02:13:02,510 --> 02:13:05,510
mammalian emotions of anger and,
and you know,

2405
02:13:05,511 --> 02:13:08,450
status seeking and ego and rage and 
lost.

2406
02:13:08,990 --> 02:13:13,340
When these things are controlling these 
highly advanced technologies,

2407
02:13:13,341 --> 02:13:16,130
this,
this is not going to come to a good end.

2408
02:13:16,190 --> 02:13:21,190
So we want compassionate general 
intelligences and this is what we should

2409
02:13:21,531 --> 02:13:26,531
be orienting ourselves toward.
And so we need to shift the focus of the

2410
02:13:28,341 --> 02:13:32,150
AI and technology development on the 
planet toward,

2411
02:13:32,390 --> 02:13:33,330
you know,
benevolent,

2412
02:13:33,360 --> 02:13:35,700
compassionate,
General Intelligence.

2413
02:13:35,701 --> 02:13:37,010
And this is subtle,
right?

2414
02:13:37,020 --> 02:13:37,853
Because you need to work with the 
establishment rather than overthrowing 

2415
02:13:41,790 --> 02:13:43,050
it,
which isn't going to be viable,

2416
02:13:43,051 --> 02:13:43,884
so this is while we're creating this 
decentralized self organizing ai 

2417
02:13:47,851 --> 02:13:49,520
network,
the singularity net,

2418
02:13:49,980 --> 02:13:51,980
then we're creating a for profit 
company,

2419
02:13:51,981 --> 02:13:52,814
singularity studio,
which will get large enterprises to use 

2420
02:13:55,801 --> 02:13:56,634
this decentralized network.
Then we're creating these robots like 

2421
02:14:00,391 --> 02:14:01,224
Sophia,
which will be mass manufactured in the 

2422
02:14:03,570 --> 02:14:04,600
next couple of years,
rolled these out,

2423
02:14:04,610 --> 02:14:05,443
this service robots everywhere around 
the world to interact with people and 

2424
02:14:09,720 --> 02:14:12,570
providing valuable services in homes and
offices,

2425
02:14:12,810 --> 02:14:14,750
but also interacting with people you 
know,

2426
02:14:14,760 --> 02:14:19,760
in a loving and compassionate way.
So we need to start now because we don't

2427
02:14:22,291 --> 02:14:23,124
actually know if it's going to be years 
or decades before we get to this 

2428
02:14:26,201 --> 02:14:29,700
singularity and we wanted to be assured 
as we can that when we get there,

2429
02:14:29,701 --> 02:14:32,520
it happens in a beneficial way for 
everyone.

2430
02:14:32,550 --> 02:14:34,530
Right.
And things like robots,

2431
02:14:34,830 --> 02:14:38,190
blockchain and ai learning algorithms 
are our tools.

2432
02:14:38,191 --> 02:14:39,870
Toward that end.
Ben,

2433
02:14:40,110 --> 02:14:40,943
I appreciate your optimism.
I appreciate coming near explaining all 

2434
02:14:42,991 --> 02:14:45,750
this stuff for us and I appreciate all 
your work.

2435
02:14:45,850 --> 02:14:48,030
It's really amazing.
Fascinating stuff.

2436
02:14:48,120 --> 02:14:48,570
Yeah.
Yeah.

2437
02:14:48,570 --> 02:14:50,880
Well thanks for having me.
It's really fun.

2438
02:14:50,881 --> 02:14:52,950
That was a wide ranging conversation,
so yeah,

2439
02:14:52,960 --> 02:14:53,793
would it will be great to come back next
year and update you on the state of the 

2440
02:14:57,151 --> 02:15:01,440
singularity once a year and just by the 
time you come from maybe who knows,

2441
02:15:01,470 --> 02:15:02,303
a year from now,
the world might be a totally different 

2442
02:15:03,481 --> 02:15:05,760
place.
Maybe a robot.

2443
02:15:06,320 --> 02:15:09,080
A robot now.
Oh,

2444
02:15:10,810 --> 02:15:12,450
thank you.
Thank you everybody.

