WEBVTT

1
00:00:06.900 --> 00:00:08.250
<v Speaker 1>Boom.</v>
<v Speaker 1>Hello Ben.</v>

2
00:00:08.560 --> 00:00:09.830
<v Speaker 2>Hey there.</v>
<v Speaker 2>Good to see you,</v>

3
00:00:09.831 --> 00:00:10.611
<v Speaker 2>man.</v>
<v Speaker 2>Yeah,</v>

4
00:00:10.611 --> 00:00:12.680
<v Speaker 2>it's a pleasure to be here.</v>
<v Speaker 2>Thanks for doing this.</v>

5
00:00:13.100 --> 00:00:13.581
<v Speaker 2>Yeah,</v>
<v Speaker 2>yeah.</v>

6
00:00:13.581 --> 00:00:14.150
<v Speaker 2>Thanks.</v>
<v Speaker 2>Thanks.</v>

7
00:00:14.150 --> 00:00:15.260
<v Speaker 2>Thanks.</v>
<v Speaker 2>Thanks for having me.</v>

8
00:00:15.261 --> 00:00:16.094
<v Speaker 2>I've been.</v>
<v Speaker 2>I've been looking at some of your shows </v>

9
00:00:17.770 --> 00:00:22.770
<v Speaker 2>in the last few last few days just to </v>
<v Speaker 2>get a sense of how you're thinking about</v>

10
00:00:23.690 --> 00:00:26.090
<v Speaker 2>ai and crypto and the various other </v>
<v Speaker 2>things.</v>

11
00:00:26.100 --> 00:00:28.340
<v Speaker 2>I'm involved in this.</v>
<v Speaker 2>It's been interesting.</v>

12
00:00:28.380 --> 00:00:30.150
<v Speaker 1>Well,</v>
<v Speaker 1>I've been following you as well.</v>

13
00:00:30.180 --> 00:00:31.013
<v Speaker 1>I've been paying attention to a lot of </v>
<v Speaker 1>your lectures and talks and different </v>

14
00:00:33.481 --> 00:00:35.490
<v Speaker 1>things you've done over the last couple </v>
<v Speaker 1>days as well.</v>

15
00:00:35.491 --> 00:00:39.340
<v Speaker 1>Getting ready for this.</v>
<v Speaker 1>It's a Ai is a.</v>

16
00:00:39.420 --> 00:00:42.660
<v Speaker 1>either people are really excited about </v>
<v Speaker 1>it or they're really terrified of it.</v>

17
00:00:42.780 --> 00:00:45.150
<v Speaker 1>Those are the sort of.</v>
<v Speaker 1>It seems to be the two responses.</v>

18
00:00:45.151 --> 00:00:45.984
<v Speaker 1>Either people have this dismal view of </v>
<v Speaker 1>these robots taking over the world or </v>

19
00:00:50.041 --> 00:00:54.480
<v Speaker 1>they think it's going to be some amazing</v>
<v Speaker 1>sort of symbiotic relationship with that</v>

20
00:00:54.510 --> 00:00:55.343
<v Speaker 1>we have with these things.</v>
<v Speaker 1>It's gonna evolve human beings past the </v>

21
00:00:58.530 --> 00:01:00.480
<v Speaker 1>monkey stage that we're at right now.</v>

22
00:01:00.670 --> 00:01:01.091
<v Speaker 2>Yeah,</v>
<v Speaker 2>and I,</v>

23
00:01:01.091 --> 00:01:06.091
<v Speaker 2>I tend to be on the leather more </v>
<v Speaker 2>positive side of this dichotomy,</v>

24
00:01:06.971 --> 00:01:11.971
<v Speaker 2>but I think one thing that has struck me</v>
<v Speaker 2>in recent years is many people are now,</v>

25
00:01:14.980 --> 00:01:17.220
<v Speaker 2>you know,</v>
<v Speaker 2>mentally confronting all the issues,</v>

26
00:01:17.240 --> 00:01:18.073
<v Speaker 2>running ai for the first time and I mean</v>
<v Speaker 2>I've been working on ai for three </v>

27
00:01:22.481 --> 00:01:23.314
<v Speaker 2>decades and I first started thinking </v>
<v Speaker 2>about ai when I was a little kid in the </v>

28
00:01:27.460 --> 00:01:28.293
<v Speaker 2>early,</v>
<v Speaker 2>late sixties and early seventies when I </v>

29
00:01:30.341 --> 00:01:33.640
<v Speaker 2>saw ais and robots on the original star </v>
<v Speaker 2>Trek.</v>

30
00:01:33.641 --> 00:01:34.474
<v Speaker 2>So I guess I've had a lot of cycles to </v>
<v Speaker 2>process the positives and negatives of </v>

31
00:01:40.631 --> 00:01:41.464
<v Speaker 2>it where it's now like suddenly most of </v>
<v Speaker 2>the world is thinking through all this </v>

32
00:01:44.981 --> 00:01:46.600
<v Speaker 2>for the first,</v>
<v Speaker 2>for the first time.</v>

33
00:01:46.601 --> 00:01:47.434
<v Speaker 2>And you know,</v>
<v Speaker 2>when you first wrap your brain around </v>

34
00:01:48.911 --> 00:01:53.200
<v Speaker 2>the idea that there may be creatures </v>
<v Speaker 2>10,000</v>

35
00:01:53.201 --> 00:01:56.440
<v Speaker 2>or a million times smarter than human </v>
<v Speaker 2>beings at first.</v>

36
00:01:56.441 --> 00:01:57.970
<v Speaker 2>This is a bit of a shocker.</v>
<v Speaker 2>Right?</v>

37
00:01:57.971 --> 00:01:58.804
<v Speaker 2>And then,</v>
<v Speaker 2>I mean it takes a while to internalize </v>

38
00:02:00.911 --> 00:02:02.080
<v Speaker 2>this into your worldview.</v>

39
00:02:02.660 --> 00:02:05.240
<v Speaker 1>Well,</v>
<v Speaker 1>it's that there's also,</v>

40
00:02:05.330 --> 00:02:09.800
<v Speaker 1>I think there's a problem with the term </v>
<v Speaker 1>artificial intelligence because it's,</v>

41
00:02:09.860 --> 00:02:11.180
<v Speaker 1>that's,</v>
<v Speaker 1>it's intelligent.</v>

42
00:02:11.450 --> 00:02:13.370
<v Speaker 1>It's there.</v>
<v Speaker 1>It's a real thing.</v>

43
00:02:13.540 --> 00:02:14.840
<v Speaker 1>Yeah.</v>
<v Speaker 1>It's not artificial.</v>

44
00:02:14.841 --> 00:02:17.090
<v Speaker 1>It's not like a fake diamond or fake </v>
<v Speaker 1>Ferrari.</v>

45
00:02:17.091 --> 00:02:19.340
<v Speaker 1>It's a real thing and it.</v>

46
00:02:19.970 --> 00:02:24.560
<v Speaker 2>It's not a great term and there's been </v>
<v Speaker 2>many attempts to replace it.</v>

47
00:02:24.570 --> 00:02:28.310
<v Speaker 2>Would synthetic intelligence for,</v>
<v Speaker 2>for example,</v>

48
00:02:28.550 --> 00:02:32.090
<v Speaker 2>but for better or worse,</v>
<v Speaker 2>I get ai is there.</v>

49
00:02:32.091 --> 00:02:35.950
<v Speaker 2>It's part of the popular imagination </v>
<v Speaker 2>that seems that it's an imperfect word,</v>

50
00:02:35.951 --> 00:02:37.760
<v Speaker 2>but it's not going away.</v>

51
00:02:37.980 --> 00:02:38.491
<v Speaker 1>Well,</v>
<v Speaker 1>I,</v>

52
00:02:38.491 --> 00:02:39.890
<v Speaker 1>I.</v>
<v Speaker 1>my question is like,</v>

53
00:02:40.020 --> 00:02:40.853
<v Speaker 1>are we married to this idea of </v>
<v Speaker 1>intelligence and of life being </v>

54
00:02:44.311 --> 00:02:45.144
<v Speaker 1>biological,</v>
<v Speaker 1>being carbon based tissue and cells and </v>

55
00:02:48.451 --> 00:02:53.451
<v Speaker 1>blood and or insects or mammals or fish?</v>
<v Speaker 1>Are we married to that too much?</v>

56
00:02:53.820 --> 00:02:58.820
<v Speaker 1>Do you think that it's entirely possible</v>
<v Speaker 1>that what human beings are doing,</v>

57
00:02:59.290 --> 00:03:00.123
<v Speaker 1>what people that are at the tip of ai </v>
<v Speaker 1>right now that are really pushing the </v>

58
00:03:04.361 --> 00:03:09.361
<v Speaker 1>technology where they're doing is really</v>
<v Speaker 1>creating a new life form that it's going</v>

59
00:03:09.521 --> 00:03:10.354
<v Speaker 1>to be a new thing that just the same way</v>
<v Speaker 1>we recognize wasps and buffaloes and </v>

60
00:03:15.460 --> 00:03:16.293
<v Speaker 1>artificial intelligence is just going to</v>
<v Speaker 1>be a life form that emerges from the </v>

61
00:03:18.761 --> 00:03:20.890
<v Speaker 1>creativity and ingenuity of human </v>
<v Speaker 1>beings.</v>

62
00:03:21.630 --> 00:03:22.463
<v Speaker 2>Indeed,</v>
<v Speaker 2>so I've long been an advocate of a </v>

63
00:03:25.710 --> 00:03:26.543
<v Speaker 2>philosophy I think of as,</v>
<v Speaker 2>as pattern isn't like it's the pattern </v>

64
00:03:29.701 --> 00:03:34.701
<v Speaker 2>of organization that appears to be the </v>
<v Speaker 2>critical thing and the,</v>

65
00:03:35.220 --> 00:03:36.090
<v Speaker 2>the,</v>
<v Speaker 2>you know,</v>

66
00:03:36.091 --> 00:03:36.924
<v Speaker 2>the individual cells and going down </v>
<v Speaker 2>further the molecules and particles in </v>

67
00:03:41.281 --> 00:03:44.250
<v Speaker 2>our body or our turning over all the </v>
<v Speaker 2>time.</v>

68
00:03:44.251 --> 00:03:45.084
<v Speaker 2>So it's not in this specific combination</v>
<v Speaker 2>of elementary particles which makes me </v>

69
00:03:49.021 --> 00:03:49.854
<v Speaker 2>who I am or makes you who you are.</v>
<v Speaker 2>It's a pattern by which they're </v>

70
00:03:52.321 --> 00:03:56.220
<v Speaker 2>organized and the patterns by which they</v>
<v Speaker 2>change over time.</v>

71
00:03:56.221 --> 00:03:57.054
<v Speaker 2>So if we can create digital systems are </v>
<v Speaker 2>quantum computers or femto computers or </v>

72
00:04:02.371 --> 00:04:06.120
<v Speaker 2>whatever it is,</v>
<v Speaker 2>manifesting the patterns of organization</v>

73
00:04:06.121 --> 00:04:10.480
<v Speaker 2>that Constitute Intelligence.</v>
<v Speaker 2>I mean then then there you are there,</v>

74
00:04:10.540 --> 00:04:11.850
<v Speaker 2>there,</v>
<v Speaker 2>there is intelligence,</v>

75
00:04:11.860 --> 00:04:12.690
<v Speaker 2>right?</v>
<v Speaker 2>So that,</v>

76
00:04:12.690 --> 00:04:17.010
<v Speaker 2>that's not to say that you know,</v>
<v Speaker 2>consciousness and experiences just about</v>

77
00:04:17.400 --> 00:04:20.250
<v Speaker 2>patterns of organization.</v>
<v Speaker 2>There may be more dimensions to it,</v>

78
00:04:20.280 --> 00:04:21.113
<v Speaker 2>but when,</v>
<v Speaker 2>when you look at what constitutes </v>

79
00:04:22.651 --> 00:04:24.300
<v Speaker 2>intelligence thinking,</v>
<v Speaker 2>cognition,</v>

80
00:04:24.301 --> 00:04:25.950
<v Speaker 2>problem solving,</v>
<v Speaker 2>you know,</v>

81
00:04:25.951 --> 00:04:27.660
<v Speaker 2>it's the pattern of organization,</v>
<v Speaker 2>not,</v>

82
00:04:27.661 --> 00:04:32.250
<v Speaker 2>not this specific material as as,</v>
<v Speaker 2>as far as we can tell.</v>

83
00:04:32.251 --> 00:04:33.084
<v Speaker 2>So we can see no reason based on all the</v>
<v Speaker 2>science that we know so far that you </v>

84
00:04:38.311 --> 00:04:39.144
<v Speaker 2>couldn't make an intelligent system of </v>
<v Speaker 2>some other form of matter rather than </v>

85
00:04:43.181 --> 00:04:47.940
<v Speaker 2>the specific types of atoms and </v>
<v Speaker 2>molecules that make up human beings.</v>

86
00:04:48.120 --> 00:04:48.953
<v Speaker 2>And it seems that we're,</v>
<v Speaker 2>we're well on the way to being able to </v>

87
00:04:51.451 --> 00:04:52.284
<v Speaker 2>do so.</v>

88
00:04:52.440 --> 00:04:54.990
<v Speaker 1>When you're studying,</v>
<v Speaker 1>when you're studying intelligence,</v>

89
00:04:54.991 --> 00:04:56.990
<v Speaker 1>you're studying artificial intelligence,</v>
<v Speaker 1>do,</v>

90
00:04:57.020 --> 00:04:57.853
<v Speaker 1>did you spend any time studying the </v>
<v Speaker 1>patterns that insects seem to </v>

91
00:05:01.981 --> 00:05:02.814
<v Speaker 1>cooperatively behave with?</v>
<v Speaker 1>Like how leafcutter ants build these </v>

92
00:05:06.961 --> 00:05:11.961
<v Speaker 1>elaborate structures underground and </v>
<v Speaker 1>build these giant colonies.</v>

93
00:05:12.901 --> 00:05:14.990
<v Speaker 1>And did you study how it did?</v>
<v Speaker 1>I did,</v>

94
00:05:15.080 --> 00:05:15.801
<v Speaker 2>actually,</v>
<v Speaker 2>yes.</v>

95
00:05:15.801 --> 00:05:16.670
<v Speaker 2>So I,</v>
<v Speaker 2>I,</v>

96
00:05:17.030 --> 00:05:17.863
<v Speaker 2>I sort of grew up with the philosophy of</v>
<v Speaker 2>complex systems which was championed by </v>

97
00:05:24.381 --> 00:05:26.250
<v Speaker 2>that,</v>
<v Speaker 2>by the Santa Fe Institute in,</v>

98
00:05:26.251 --> 00:05:27.590
<v Speaker 2>in,</v>
<v Speaker 2>in the 19 eighties.</v>

99
00:05:27.591 --> 00:05:32.591
<v Speaker 2>And the whole concept that there is an </v>
<v Speaker 2>interdisciplinary complex system science</v>

100
00:05:33.290 --> 00:05:34.430
<v Speaker 2>which includes,</v>
<v Speaker 2>you know,</v>

101
00:05:34.431 --> 00:05:36.590
<v Speaker 2>biology,</v>
<v Speaker 2>cosmology,</v>

102
00:05:36.591 --> 00:05:38.240
<v Speaker 2>psychology,</v>
<v Speaker 2>sociology.</v>

103
00:05:38.241 --> 00:05:43.070
<v Speaker 2>There's sort of universal patterns of,</v>
<v Speaker 2>of Self Organization and you know,</v>

104
00:05:43.071 --> 00:05:48.071
<v Speaker 2>aunts and ant colonies have long been </v>
<v Speaker 2>the paradigm case for that.</v>

105
00:05:48.231 --> 00:05:49.064
<v Speaker 2>And I,</v>
<v Speaker 2>I used to play with the ant colonies in </v>

106
00:05:51.001 --> 00:05:52.130
<v Speaker 2>my backyard.</v>
<v Speaker 2>Wow.</v>

107
00:05:52.550 --> 00:05:55.820
<v Speaker 2>When I was a kid and you'd lay down food</v>
<v Speaker 2>and certain patterns,</v>

108
00:05:55.821 --> 00:05:56.654
<v Speaker 2>you'd see how their answer down.</v>
<v Speaker 2>Pheromones in the colonies are </v>

109
00:05:59.690 --> 00:06:00.523
<v Speaker 2>organizing it in a certain way,</v>
<v Speaker 2>and that's an interesting self </v>

110
00:06:04.370 --> 00:06:05.203
<v Speaker 2>organizing complex system on that zone.</v>
<v Speaker 2>It's lacking some types of adaptive </v>

111
00:06:10.131 --> 00:06:14.630
<v Speaker 2>intelligence that that human minds and </v>
<v Speaker 2>human societies have been,</v>

112
00:06:14.650 --> 00:06:17.630
<v Speaker 2>but it has also interesting self </v>
<v Speaker 2>organizing patterns.</v>

113
00:06:17.960 --> 00:06:22.550
<v Speaker 2>This reminds me of the novel Solaris by </v>
<v Speaker 2>Stanislaw Lem,</v>

114
00:06:22.580 --> 00:06:27.460
<v Speaker 2>which was published in the sixties,</v>
<v Speaker 2>which was really quite,</v>

115
00:06:27.680 --> 00:06:30.860
<v Speaker 2>quite a deep novel,</v>
<v Speaker 2>much deeper than the movie that was made</v>

116
00:06:30.861 --> 00:06:32.480
<v Speaker 2>of it.</v>
<v Speaker 2>Did you ever read that book?</v>

117
00:06:32.481 --> 00:06:33.800
<v Speaker 2>So there is.</v>
<v Speaker 2>So what?</v>

118
00:06:34.220 --> 00:06:35.053
<v Speaker 2>I'm not familiar with the movie either.</v>
<v Speaker 2>Who will say where there was an amazing </v>

119
00:06:37.731 --> 00:06:38.564
<v Speaker 2>brilliant movie by Tarkovsky,</v>
<v Speaker 2>the Russian director from the late </v>

120
00:06:40.761 --> 00:06:41.594
<v Speaker 2>sixties.</v>
<v Speaker 2>Then there was a movie by Steven </v>

121
00:06:44.150 --> 00:06:44.983
<v Speaker 2>Soderbergh which was sort of clammed up </v>
<v Speaker 2>and Americanized and that was fairly </v>

122
00:06:47.961 --> 00:06:48.794
<v Speaker 2>recent,</v>
<v Speaker 2>right?</v>

123
00:06:49.000 --> 00:06:51.170
<v Speaker 2>Ten years ago,</v>
<v Speaker 2>but that wasn't.</v>

124
00:06:51.200 --> 00:06:52.970
<v Speaker 2>Didn't get all the deep points and </v>
<v Speaker 2>novel,</v>

125
00:06:52.971 --> 00:06:54.920
<v Speaker 2>the original novel.</v>
<v Speaker 2>In essence,</v>

126
00:06:54.921 --> 00:06:55.754
<v Speaker 2>there's this,</v>
<v Speaker 2>there's this ocean on coating the </v>

127
00:06:59.751 --> 00:07:00.584
<v Speaker 2>surface of some alien planet which has </v>
<v Speaker 2>amazingly complex fractal patterns of </v>

128
00:07:04.401 --> 00:07:08.690
<v Speaker 2>organization and it's also interactive </v>
<v Speaker 2>like the patterns of organization on the</v>

129
00:07:08.691 --> 00:07:13.691
<v Speaker 2>Ocean respond based on what you do and </v>
<v Speaker 2>when people get near the ocean,</v>

130
00:07:14.270 --> 00:07:15.103
<v Speaker 2>it causes them to hallucinate things and</v>
<v Speaker 2>even caused them to see Simulacra of </v>

131
00:07:19.581 --> 00:07:22.820
<v Speaker 2>people from their past,</v>
<v Speaker 2>even the like the person who they'd most</v>

132
00:07:22.821 --> 00:07:27.200
<v Speaker 2>harmed or injured in their past appears </v>
<v Speaker 2>and interacts with them so clearly.</v>

133
00:07:27.200 --> 00:07:28.033
<v Speaker 2>This ocean has some type of amazing </v>
<v Speaker 2>complexity and intelligence from the </v>

134
00:07:31.551 --> 00:07:32.384
<v Speaker 2>patterns that displays and from the </v>
<v Speaker 2>weird things that reeks in your mind so </v>

135
00:07:36.561 --> 00:07:41.561
<v Speaker 2>that the people on earth try to </v>
<v Speaker 2>understand how the ocean is thinking.</v>

136
00:07:42.441 --> 00:07:47.441
<v Speaker 2>They send a scientific expedition there </v>
<v Speaker 2>to to interact with that ocean,</v>

137
00:07:48.530 --> 00:07:49.363
<v Speaker 2>but it's just so alien.</v>
<v Speaker 2>Even though monkeys with people's minds </v>

138
00:07:51.801 --> 00:07:56.801
<v Speaker 2>include these doing complex things,</v>
<v Speaker 2>no two way communication is ever is ever</v>

139
00:07:58.191 --> 00:08:03.191
<v Speaker 2>established and eventually the human </v>
<v Speaker 2>expedition gives up and goes home.</v>

140
00:08:03.591 --> 00:08:06.560
<v Speaker 2>So it's a very Russian ending to the </v>
<v Speaker 2>novel.</v>

141
00:08:07.410 --> 00:08:09.750
<v Speaker 2>I guess it's not.</v>
<v Speaker 2>I saw that,</v>

142
00:08:09.830 --> 00:08:14.830
<v Speaker 2>but that the.</v>
<v Speaker 2>The interesting message there is,</v>

143
00:08:15.891 --> 00:08:18.350
<v Speaker 2>I mean there can be many,</v>
<v Speaker 2>many kinds of intelligence,</v>

144
00:08:18.351 --> 00:08:19.184
<v Speaker 2>right?</v>
<v Speaker 2>I mean,</v>

145
00:08:19.970 --> 00:08:20.803
<v Speaker 2>human intelligence is one thing.</v>
<v Speaker 2>The intelligence of an ant colony is a </v>

146
00:08:24.741 --> 00:08:25.574
<v Speaker 2>different thing.</v>
<v Speaker 2>The intelligence of human society is a </v>

147
00:08:27.951 --> 00:08:31.370
<v Speaker 2>different thing.</v>
<v Speaker 2>Ecosystem is a different thing and there</v>

148
00:08:31.371 --> 00:08:32.204
<v Speaker 2>could be many,</v>
<v Speaker 2>many types of ais that we could build </v>

149
00:08:35.541 --> 00:08:37.400
<v Speaker 2>with many,</v>
<v Speaker 2>many different properties.</v>

150
00:08:37.400 --> 00:08:42.400
<v Speaker 2>Some could be wonderful to human beings,</v>
<v Speaker 2>some can be horrible to human beings,</v>

151
00:08:42.590 --> 00:08:47.590
<v Speaker 2>some could just be alien minds that that</v>
<v Speaker 2>we can't even relate,</v>

152
00:08:48.140 --> 00:08:50.040
<v Speaker 2>relate,</v>
<v Speaker 2>relate to very,</v>

153
00:08:50.360 --> 00:08:51.193
<v Speaker 2>very well.</v>
<v Speaker 2>So we we have a very limited conception </v>

154
00:08:53.541 --> 00:08:54.374
<v Speaker 2>of what an intelligence is.</v>
<v Speaker 2>If we just think by close analogy to to </v>

155
00:08:58.801 --> 00:08:59.634
<v Speaker 2>human minds and this.</v>
<v Speaker 2>This is important if you're thinking </v>

156
00:09:02.041 --> 00:09:05.210
<v Speaker 2>about engineering or growing are the </v>
<v Speaker 2>life forms.</v>

157
00:09:05.211 --> 00:09:08.340
<v Speaker 2>They're artificial minds because it's </v>
<v Speaker 2>not just can we do this?</v>

158
00:09:08.341 --> 00:09:09.174
<v Speaker 2>It's what kind of mind are,</v>
<v Speaker 2>are we going to engineer or evolve and </v>

159
00:09:14.701 --> 00:09:15.534
<v Speaker 2>there's.</v>
<v Speaker 2>There's a huge spectrum of </v>

160
00:09:16.201 --> 00:09:17.034
<v Speaker 2>possibilities.</v>

161
00:09:17.120 --> 00:09:17.953
<v Speaker 1>Yeah,</v>
<v Speaker 1>that's one of the reasons why I asked </v>

162
00:09:18.621 --> 00:09:23.621
<v Speaker 1>you that if we'd created,</v>
<v Speaker 1>if human beings had created some sort of</v>

163
00:09:23.871 --> 00:09:24.704
<v Speaker 1>an insect and this insect started </v>
<v Speaker 1>organizing and developing these complex </v>

164
00:09:28.641 --> 00:09:31.560
<v Speaker 1>colonies like a leaf cutter and building</v>
<v Speaker 1>these structures underground,</v>

165
00:09:31.940 --> 00:09:33.650
<v Speaker 1>people would go crazy.</v>
<v Speaker 1>They would panic.</v>

166
00:09:33.950 --> 00:09:35.420
<v Speaker 1>They would think these things are </v>
<v Speaker 1>organizing.</v>

167
00:09:35.421 --> 00:09:36.254
<v Speaker 1>They're gonna.</v>
<v Speaker 1>They're going to build up the resources </v>

168
00:09:37.221 --> 00:09:38.054
<v Speaker 1>and attack us.</v>
<v Speaker 1>They're going to try to take over </v>

169
00:09:38.841 --> 00:09:41.390
<v Speaker 1>humanity.</v>
<v Speaker 1>I mean this what,</v>

170
00:09:41.410 --> 00:09:45.200
<v Speaker 1>what people are worried about more than </v>
<v Speaker 1>anything when it comes to technology,</v>

171
00:09:45.201 --> 00:09:49.340
<v Speaker 1>I think is the idea that we're going to </v>
<v Speaker 1>be irrelevant,</v>

172
00:09:49.580 --> 00:09:50.413
<v Speaker 1>that we're going to be a antiques and </v>
<v Speaker 1>that something new and better is going </v>

173
00:09:54.111 --> 00:09:56.390
<v Speaker 1>to take our place,</v>
<v Speaker 1>which is a,</v>

174
00:09:56.620 --> 00:09:57.453
<v Speaker 1>which is almost double,</v>
<v Speaker 1>so we're thinking worried about it </v>

175
00:09:59.391 --> 00:10:02.300
<v Speaker 1>because it's sort of the history of </v>
<v Speaker 1>biological life on earth.</v>

176
00:10:02.810 --> 00:10:05.300
<v Speaker 1>I mean,</v>
<v Speaker 1>what is there as complex things?</v>

177
00:10:05.300 --> 00:10:06.133
<v Speaker 1>They become more competent with single </v>
<v Speaker 1>cell organisms to multicellular </v>

178
00:10:08.041 --> 00:10:10.070
<v Speaker 1>organisms.</v>
<v Speaker 1>That seems to be a pattern leading up to</v>

179
00:10:10.071 --> 00:10:14.870
<v Speaker 1>us and us with this unprecedented </v>
<v Speaker 1>ability to change our environment.</v>

180
00:10:14.930 --> 00:10:15.980
<v Speaker 1>That's what we can do,</v>
<v Speaker 1>right?</v>

181
00:10:15.981 --> 00:10:19.010
<v Speaker 1>We can manipulate things,</v>
<v Speaker 1>poison the environment.</v>

182
00:10:19.011 --> 00:10:22.130
<v Speaker 1>We can blow up entire countries with </v>
<v Speaker 1>bombs if we'd like to,</v>

183
00:10:22.131 --> 00:10:26.930
<v Speaker 1>and we can also do wild creative things </v>
<v Speaker 1>like send signals through space and land</v>

184
00:10:26.931 --> 00:10:27.764
<v Speaker 1>on someone else's phone on the other </v>
<v Speaker 1>side of the world almost </v>

185
00:10:28.851 --> 00:10:31.310
<v Speaker 1>instantaneously.</v>
<v Speaker 1>We have incredible power,</v>

186
00:10:31.640 --> 00:10:35.870
<v Speaker 1>but we're also.</v>
<v Speaker 1>We're also so limited by our biology.</v>

187
00:10:36.330 --> 00:10:38.750
<v Speaker 1>Yeah.</v>
<v Speaker 1>The thing I think people are afraid of,</v>

188
00:10:39.680 --> 00:10:40.513
<v Speaker 1>I'm afraid of,</v>
<v Speaker 1>but I don't know if that makes any </v>

189
00:10:41.691 --> 00:10:46.370
<v Speaker 1>sense.</v>
<v Speaker 1>Is that the next level of life,</v>

190
00:10:46.400 --> 00:10:49.310
<v Speaker 1>whatever artificial life is or whatever </v>
<v Speaker 1>the,</v>

191
00:10:49.311 --> 00:10:50.144
<v Speaker 1>the,</v>
<v Speaker 1>the human symbiotic is that it's going </v>

192
00:10:52.441 --> 00:10:55.820
<v Speaker 1>to lack emotions.</v>
<v Speaker 1>It's going to lack desires and needs and</v>

193
00:10:55.821 --> 00:10:58.490
<v Speaker 1>all the things that we think are special</v>
<v Speaker 1>about us,</v>

194
00:10:58.820 --> 00:11:02.060
<v Speaker 1>our creativity,</v>
<v Speaker 1>our desire for attention and love,</v>

195
00:11:02.061 --> 00:11:05.270
<v Speaker 1>all of our camaraderie,</v>
<v Speaker 1>all these different things that are sort</v>

196
00:11:05.271 --> 00:11:06.104
<v Speaker 1>of programmed into us with our genetics </v>
<v Speaker 1>in order to advance our species that we </v>

197
00:11:13.071 --> 00:11:14.540
<v Speaker 1>were so connected to these things.</v>

198
00:11:14.540 --> 00:11:19.220
<v Speaker 1>But there's so the reason for war that </v>
<v Speaker 1>the reason for the lies,</v>

199
00:11:19.221 --> 00:11:21.140
<v Speaker 1>deception,</v>
<v Speaker 1>thievery.</v>

200
00:11:21.320 --> 00:11:25.580
<v Speaker 1>There's so many things that are built </v>
<v Speaker 1>into being a person that are responsible</v>

201
00:11:25.581 --> 00:11:29.000
<v Speaker 1>for all the woes of humanity,</v>
<v Speaker 1>but were afraid to lose those.</v>

202
00:11:29.200 --> 00:11:31.900
<v Speaker 1>I think it's almost,</v>

203
00:11:32.080 --> 00:11:37.080
<v Speaker 2>and by this point that humanity is going</v>
<v Speaker 2>to create</v>

204
00:11:38.430 --> 00:11:39.263
<v Speaker 2>synthetic intelligences with </v>
<v Speaker 2>tremendously greater general </v>

205
00:11:42.751 --> 00:11:47.751
<v Speaker 2>intelligence and practical capability </v>
<v Speaker 2>than human beings have.</v>

206
00:11:48.091 --> 00:11:48.924
<v Speaker 2>I mean,</v>
<v Speaker 2>I think I know how to do that with the </v>

207
00:11:50.371 --> 00:11:52.470
<v Speaker 2>software I'm working on with my own </v>
<v Speaker 2>team,</v>

208
00:11:52.830 --> 00:11:53.663
<v Speaker 2>but if we fail,</v>
<v Speaker 2>you know there's a load of other teams </v>

209
00:11:57.070 --> 00:12:00.040
<v Speaker 2>who I think are a bit behind us,</v>
<v Speaker 2>but they're going in the same direction.</v>

210
00:12:00.041 --> 00:12:00.874
<v Speaker 2>Now.</v>
<v Speaker 2>I feel like you're at the tip of the </v>

211
00:12:01.631 --> 00:12:02.950
<v Speaker 2>spear with this stuff.</v>
<v Speaker 2>I do,</v>

212
00:12:02.980 --> 00:12:03.813
<v Speaker 2>but I also think that's not the most </v>
<v Speaker 2>important thing from a human </v>

213
00:12:07.991 --> 00:12:08.824
<v Speaker 2>perspective.</v>
<v Speaker 2>The most important thing is that </v>

214
00:12:09.851 --> 00:12:13.070
<v Speaker 2>humanity as a whole is quite close to </v>
<v Speaker 2>this,</v>

215
00:12:13.071 --> 00:12:15.040
<v Speaker 2>this threshold event.</v>
<v Speaker 2>Right.</v>

216
00:12:15.041 --> 00:12:19.180
<v Speaker 2>So how far do you think?</v>
<v Speaker 2>It's quite close by my own gut feeling.</v>

217
00:12:19.181 --> 00:12:21.940
<v Speaker 2>Five to 30 years.</v>
<v Speaker 2>Let's say that's pretty close,</v>

218
00:12:21.941 --> 00:12:26.230
<v Speaker 2>but if I'm wrong and it's 100 years,</v>
<v Speaker 2>like in the historical time scale,</v>

219
00:12:26.560 --> 00:12:27.790
<v Speaker 2>that sort of doesn't matter.</v>

220
00:12:27.790 --> 00:12:28.623
<v Speaker 2>It's like,</v>
<v Speaker 2>did the Sumerians create civilization </v>

221
00:12:30.521 --> 00:12:32.380
<v Speaker 2>10,000</v>
<v Speaker 2>or 10,050</v>

222
00:12:32.381 --> 00:12:33.211
<v Speaker 2>years ago?</v>
<v Speaker 2>Like what?</v>

223
00:12:33.211 --> 00:12:34.540
<v Speaker 2>What difference does it make?</v>
<v Speaker 2>Right.</v>

224
00:12:34.541 --> 00:12:35.374
<v Speaker 2>So I think we're quite close to creating</v>
<v Speaker 2>super human artificial general </v>

225
00:12:41.291 --> 00:12:46.291
<v Speaker 2>intelligence and that's in a way almost </v>
<v Speaker 2>inevitable given where we are now.</v>

226
00:12:48.670 --> 00:12:49.503
<v Speaker 2>On the other hand,</v>
<v Speaker 2>I think we still have some agency </v>

227
00:12:53.351 --> 00:12:54.184
<v Speaker 2>regarding whether this comes out in a </v>
<v Speaker 2>way that respects human values and </v>

228
00:13:00.611 --> 00:13:01.444
<v Speaker 2>culture,</v>
<v Speaker 2>which are important to us now given who </v>

229
00:13:03.101 --> 00:13:03.934
<v Speaker 2>and what we are or that is essentially </v>
<v Speaker 2>indifferent to human values and culture </v>

230
00:13:08.951 --> 00:13:09.784
<v Speaker 2>in the same way that we're mostly </v>
<v Speaker 2>indifferent to chimpanzee values and </v>

231
00:13:13.181 --> 00:13:14.014
<v Speaker 2>culture at that at this point.</v>
<v Speaker 2>Completely indifferent to insect values </v>

232
00:13:17.471 --> 00:13:18.970
<v Speaker 2>and culture.</v>
<v Speaker 2>Not Completely,</v>

233
00:13:19.350 --> 00:13:22.720
<v Speaker 2>but if you think about it,</v>
<v Speaker 2>I mean if I'm building a new house,</v>

234
00:13:23.290 --> 00:13:24.123
<v Speaker 2>I will bulldoze those a bunch of events,</v>
<v Speaker 2>but yet we get upset if we extinct an </v>

235
00:13:27.071 --> 00:13:28.150
<v Speaker 2>insect species.</v>

236
00:13:28.150 --> 00:13:28.983
<v Speaker 2>Right?</v>
<v Speaker 2>So we,</v>

237
00:13:29.160 --> 00:13:29.993
<v Speaker 2>we carry,</v>
<v Speaker 2>we carry it to some level but not but </v>

238
00:13:32.490 --> 00:13:33.323
<v Speaker 2>we.</v>
<v Speaker 2>But we would like the Super Ais to care </v>

239
00:13:35.201 --> 00:13:37.690
<v Speaker 2>about us more than we care about insects</v>
<v Speaker 2>or,</v>

240
00:13:37.691 --> 00:13:40.300
<v Speaker 2>or grade.</v>
<v Speaker 2>Absolutely.</v>

241
00:13:40.301 --> 00:13:41.700
<v Speaker 2>Right.</v>
<v Speaker 2>And I think this,</v>

242
00:13:42.160 --> 00:13:46.540
<v Speaker 2>this is something we can impact right </v>
<v Speaker 2>now and to to,</v>

243
00:13:46.580 --> 00:13:50.380
<v Speaker 2>to be honest,</v>
<v Speaker 2>I mean in a certain part of my mind,</v>

244
00:13:50.410 --> 00:13:52.570
<v Speaker 2>I can think,</v>
<v Speaker 2>well like in,</v>

245
00:13:52.590 --> 00:13:55.810
<v Speaker 2>in the end,</v>
<v Speaker 2>I don't matter that much,</v>

246
00:13:55.930 --> 00:13:58.610
<v Speaker 2>my four kids don't matter that much,</v>
<v Speaker 2>my granddaughter,</v>

247
00:13:58.611 --> 00:13:59.444
<v Speaker 2>it doesn't matter that much like we are </v>
<v Speaker 2>patterns of organization in a very long </v>

248
00:14:03.971 --> 00:14:08.290
<v Speaker 2>lineage of patterns of organization and </v>
<v Speaker 2>they matter very much to you and other,</v>

249
00:14:08.291 --> 00:14:11.380
<v Speaker 2>you know,</v>
<v Speaker 2>dinosaurs came and went and neanderthals</v>

250
00:14:11.381 --> 00:14:14.170
<v Speaker 2>came and went.</v>
<v Speaker 2>Humans may come and go,</v>

251
00:14:14.320 --> 00:14:18.940
<v Speaker 2>the Ai that we create may come and go </v>
<v Speaker 2>and that's the nature of the universe.</v>

252
00:14:18.941 --> 00:14:22.450
<v Speaker 2>But on the other hand,</v>
<v Speaker 2>of course in my heart,</v>

253
00:14:22.451 --> 00:14:25.270
<v Speaker 2>from my situated perspective as an </v>
<v Speaker 2>individual human,</v>

254
00:14:25.271 --> 00:14:30.000
<v Speaker 2>like if,</v>
<v Speaker 2>if some ai charged to annihilate my,</v>

255
00:14:30.020 --> 00:14:33.160
<v Speaker 2>my 10 month old son,</v>
<v Speaker 2>I would try to kill that Ai.</v>

256
00:14:33.160 --> 00:14:34.530
<v Speaker 2>Right?</v>
<v Speaker 2>So as,</v>

257
00:14:34.810 --> 00:14:39.760
<v Speaker 2>as a human being situated in this </v>
<v Speaker 2>specific species,</v>

258
00:14:39.820 --> 00:14:43.560
<v Speaker 2>place in time,</v>
<v Speaker 2>I care a lot about the condition of,</v>

259
00:14:43.561 --> 00:14:48.561
<v Speaker 2>of all of us humans.</v>
<v Speaker 2>And so I would like to not only create a</v>

260
00:14:50.141 --> 00:14:53.240
<v Speaker 2>powerful general but,</v>
<v Speaker 2>but create one which,</v>

261
00:14:53.241 --> 00:14:54.074
<v Speaker 2>which is,</v>
<v Speaker 2>is going to be beneficial to humans and </v>

262
00:14:59.000 --> 00:15:03.310
<v Speaker 2>other life forms on the planet even </v>
<v Speaker 2>while in some ways going,</v>

263
00:15:03.340 --> 00:15:06.560
<v Speaker 2>going beyond every,</v>
<v Speaker 2>everything that we are right.</v>

264
00:15:06.590 --> 00:15:10.340
<v Speaker 2>And there can't be any guarantees about </v>
<v Speaker 2>something like this.</v>

265
00:15:10.341 --> 00:15:11.174
<v Speaker 2>On the other hand,</v>
<v Speaker 2>humanity has really never had any </v>

266
00:15:15.290 --> 00:15:17.270
<v Speaker 2>guarantees about anything anyway.</v>
<v Speaker 2>Right?</v>

267
00:15:17.271 --> 00:15:17.871
<v Speaker 2>I mean,</v>
<v Speaker 2>since,</v>

268
00:15:17.871 --> 00:15:18.710
<v Speaker 2>since,</v>
<v Speaker 2>since,</v>

269
00:15:19.430 --> 00:15:20.263
<v Speaker 2>since we created civilization.</v>
<v Speaker 2>We've been leaping into the unknown one </v>

270
00:15:23.781 --> 00:15:24.614
<v Speaker 2>time after the other in the somewhat </v>
<v Speaker 2>conscious and self aware way about it </v>

271
00:15:29.180 --> 00:15:30.320
<v Speaker 2>from,</v>
<v Speaker 2>you know,</v>

272
00:15:30.321 --> 00:15:32.600
<v Speaker 2>agriculture to language,</v>
<v Speaker 2>to math,</v>

273
00:15:32.601 --> 00:15:33.434
<v Speaker 2>to the industrial revolution.</v>
<v Speaker 2>We're leaping into the unknown all the </v>

274
00:15:37.101 --> 00:15:37.934
<v Speaker 2>time,</v>
<v Speaker 2>which is part of why we're where we are </v>

275
00:15:40.491 --> 00:15:43.830
<v Speaker 2>today instead of just another animal </v>
<v Speaker 2>species.</v>

276
00:15:44.170 --> 00:15:45.003
<v Speaker 2>So we can't have a guarantee that Agi,</v>
<v Speaker 2>artificial general intelligence is we </v>

277
00:15:50.961 --> 00:15:51.794
<v Speaker 2>create,</v>
<v Speaker 2>are going to do what we consider the </v>

278
00:15:54.051 --> 00:15:56.210
<v Speaker 2>right thing given our current value </v>
<v Speaker 2>systems.</v>

279
00:15:56.211 --> 00:15:57.044
<v Speaker 2>On the other hand,</v>
<v Speaker 2>I suspect we can bias the odds in the </v>

280
00:16:02.661 --> 00:16:05.630
<v Speaker 2>favor of,</v>
<v Speaker 2>of human values and,</v>

281
00:16:06.060 --> 00:16:08.390
<v Speaker 2>and culture.</v>
<v Speaker 2>And that's something I've.</v>

282
00:16:08.960 --> 00:16:12.260
<v Speaker 2>I've put a lot of thought and work into,</v>
<v Speaker 2>alongside the,</v>

283
00:16:12.710 --> 00:16:14.660
<v Speaker 2>you know,</v>
<v Speaker 2>the basic algorithms of,</v>

284
00:16:14.661 --> 00:16:15.494
<v Speaker 2>of artificial cognition is the issue </v>
<v Speaker 2>that the initial creation would be </v>

285
00:16:20.691 --> 00:16:21.524
<v Speaker 2>subject to our programming,</v>
<v Speaker 2>but that it could perhaps program </v>

286
00:16:24.241 --> 00:16:28.640
<v Speaker 2>something more efficient and design </v>
<v Speaker 2>something like if you build creativity,</v>

287
00:16:29.930 --> 00:16:31.110
<v Speaker 2>you have to create,</v>
<v Speaker 2>I mean,</v>

288
00:16:31.160 --> 00:16:34.420
<v Speaker 2>general generalization is about creative</v>
<v Speaker 2>writing,</v>

289
00:16:34.421 --> 00:16:35.480
<v Speaker 2>right?</v>
<v Speaker 2>Yeah.</v>

290
00:16:35.750 --> 00:16:40.190
<v Speaker 2>But is the issue that it would choose to</v>
<v Speaker 2>not accept our values,</v>

291
00:16:40.340 --> 00:16:41.173
<v Speaker 2>which it might find clearly will choose </v>
<v Speaker 2>not to accept our values and we want it </v>

292
00:16:44.211 --> 00:16:46.730
<v Speaker 2>to choose not to accept all of our </v>
<v Speaker 2>values.</v>

293
00:16:46.940 --> 00:16:50.960
<v Speaker 2>So it's more a matter of whether the </v>
<v Speaker 2>ongoing creation,</v>

294
00:16:50.961 --> 00:16:51.794
<v Speaker 2>evolution of new values occurs with some</v>
<v Speaker 2>continuity in respect for the previous </v>

295
00:16:55.671 --> 00:16:56.451
<v Speaker 2>one.</v>
<v Speaker 2>So I mean,</v>

296
00:16:56.451 --> 00:16:57.980
<v Speaker 2>uh,</v>
<v Speaker 2>with I've,</v>

297
00:16:57.981 --> 00:17:01.730
<v Speaker 2>for human kids now one is a baby,</v>
<v Speaker 2>but the other three are adults,</v>

298
00:17:01.731 --> 00:17:02.564
<v Speaker 2>right?</v>
<v Speaker 2>And with each of them I took the </v>

299
00:17:03.741 --> 00:17:08.540
<v Speaker 2>approach of trying to teach the kids </v>
<v Speaker 2>what my values were,</v>

300
00:17:08.780 --> 00:17:13.370
<v Speaker 2>not just by preaching at them,</v>
<v Speaker 2>by entering into shared situations,</v>

301
00:17:13.670 --> 00:17:14.900
<v Speaker 2>but then,</v>
<v Speaker 2>you know,</v>

302
00:17:14.901 --> 00:17:15.734
<v Speaker 2>when your kids grow up,</v>
<v Speaker 2>they're going to go in their own </v>

303
00:17:17.661 --> 00:17:19.100
<v Speaker 2>different directions.</v>
<v Speaker 2>Right?</v>

304
00:17:19.170 --> 00:17:20.003
<v Speaker 2>And these are humans,</v>
<v Speaker 2>but they all have the same sort of </v>

305
00:17:22.851 --> 00:17:26.720
<v Speaker 2>biological needs,</v>
<v Speaker 2>which is one of the first place.</v>

306
00:17:26.721 --> 00:17:28.460
<v Speaker 2>Yeah,</v>
<v Speaker 2>there's still as an analogy,</v>

307
00:17:28.480 --> 00:17:33.480
<v Speaker 2>I think the ais that we create,</v>
<v Speaker 2>you can think of us as our mind children</v>

308
00:17:33.860 --> 00:17:34.693
<v Speaker 2>and we're starting them off with our </v>
<v Speaker 2>culture and values if we do it properly </v>

309
00:17:40.010 --> 00:17:45.010
<v Speaker 2>or at least with a certain subset of the</v>
<v Speaker 2>whole diverse self-contradictory mess of</v>

310
00:17:45.381 --> 00:17:47.810
<v Speaker 2>human culture and values,</v>
<v Speaker 2>but you know,</v>

311
00:17:47.811 --> 00:17:51.930
<v Speaker 2>they're going to evolve in,</v>
<v Speaker 2>in a different direction.</v>

312
00:17:52.200 --> 00:17:56.340
<v Speaker 2>But you want that evolution to take </v>
<v Speaker 2>place in their reflective and,</v>

313
00:17:56.341 --> 00:17:57.174
<v Speaker 2>and,</v>
<v Speaker 2>and caring way rather than the heatless </v>

314
00:17:58.471 --> 00:18:00.320
<v Speaker 2>way.</v>
<v Speaker 2>Because if you think about it,</v>

315
00:18:00.780 --> 00:18:01.613
<v Speaker 2>the average human a thousand years ago </v>
<v Speaker 2>or even 50 years ago would have thought </v>

316
00:18:04.321 --> 00:18:09.120
<v Speaker 2>you and me,</v>
<v Speaker 2>we're like hopelessly immoral miscreants</v>

317
00:18:09.121 --> 00:18:12.070
<v Speaker 2>who had abandoned all the valuable thing</v>
<v Speaker 2>things in life.</v>

318
00:18:13.250 --> 00:18:14.120
<v Speaker 2>Uh,</v>
<v Speaker 2>my,</v>

319
00:18:14.160 --> 00:18:14.850
<v Speaker 2>my,</v>
<v Speaker 2>my,</v>

320
00:18:14.850 --> 00:18:16.210
<v Speaker 2>my hand.</v>
<v Speaker 2>I mean,</v>

321
00:18:16.211 --> 00:18:16.601
<v Speaker 2>I'm,</v>
<v Speaker 2>I'm,</v>

322
00:18:16.601 --> 00:18:17.010
<v Speaker 2>uh,</v>
<v Speaker 2>I'm an.</v>

323
00:18:17.010 --> 00:18:18.260
<v Speaker 2>I'm an infidel,</v>
<v Speaker 2>right?</v>

324
00:18:18.270 --> 00:18:20.220
<v Speaker 2>I don't know.</v>
<v Speaker 2>I haven't gone to church,</v>

325
00:18:20.260 --> 00:18:21.670
<v Speaker 2>uh,</v>
<v Speaker 2>for I,</v>

326
00:18:21.671 --> 00:18:23.710
<v Speaker 2>I guess I mean my,</v>
<v Speaker 2>my,</v>

327
00:18:23.750 --> 00:18:25.370
<v Speaker 2>my mother's lesbians,</v>
<v Speaker 2>right?</v>

328
00:18:25.380 --> 00:18:26.213
<v Speaker 2>I mean,</v>
<v Speaker 2>there's all these things that we take </v>

329
00:18:28.141 --> 00:18:33.141
<v Speaker 2>for granted now that not that long ago </v>
<v Speaker 2>were completely against what most humans</v>

330
00:18:34.741 --> 00:18:37.980
<v Speaker 2>considered maybe the most important </v>
<v Speaker 2>values of life.</v>

331
00:18:37.981 --> 00:18:42.390
<v Speaker 2>So I mean human values itself is </v>
<v Speaker 2>completely a moving,</v>

332
00:18:42.420 --> 00:18:45.210
<v Speaker 2>a moving target.</v>
<v Speaker 2>So think in our generation,</v>

333
00:18:45.820 --> 00:18:48.690
<v Speaker 2>moving in our,</v>
<v Speaker 2>in our generation pretty radically,</v>

334
00:18:48.960 --> 00:18:49.870
<v Speaker 2>very radically.</v>

335
00:18:49.920 --> 00:18:53.590
<v Speaker 2>When I think back to my childhood,</v>
<v Speaker 2>I,</v>

336
00:18:53.610 --> 00:18:54.443
<v Speaker 2>I,</v>
<v Speaker 2>I lived in New Jersey for nine years of </v>

337
00:18:57.181 --> 00:18:58.014
<v Speaker 2>my childhood and just the level of </v>
<v Speaker 2>racism and antisemitism and sexism that </v>

338
00:19:03.001 --> 00:19:06.630
<v Speaker 2>were just normal,</v>
<v Speaker 2>ambient and taken for granted.</v>

339
00:19:06.631 --> 00:19:08.120
<v Speaker 2>Then this was,</v>
<v Speaker 2>this.</v>

340
00:19:08.220 --> 00:19:11.430
<v Speaker 2>Was this when you're between.</v>
<v Speaker 2>Because we're the same age.</v>

341
00:19:11.431 --> 00:19:12.830
<v Speaker 2>We're both one.</v>
<v Speaker 2>Yeah,</v>

342
00:19:12.970 --> 00:19:13.431
<v Speaker 2>yeah,</v>
<v Speaker 2>yeah,</v>

343
00:19:13.431 --> 00:19:14.610
<v Speaker 2>yeah.</v>
<v Speaker 2>Born in 66.</v>

344
00:19:14.611 --> 00:19:19.611
<v Speaker 2>I lived in Jersey from 73 to 82,</v>
<v Speaker 2>so I was there from 67 to 73.</v>

345
00:19:22.650 --> 00:19:23.483
<v Speaker 2>Oh yeah.</v>
<v Speaker 2>Yeah.</v>

346
00:19:24.050 --> 00:19:26.100
<v Speaker 2>So yeah,</v>
<v Speaker 2>I'm in my,</v>

347
00:19:26.130 --> 00:19:29.730
<v Speaker 2>I'm in my sister went to the high school</v>
<v Speaker 2>prom with a,</v>

348
00:19:29.790 --> 00:19:33.500
<v Speaker 2>with a black guy and so we got our car </v>
<v Speaker 2>turned upside down,</v>

349
00:19:33.510 --> 00:19:34.343
<v Speaker 2>the windows of our house smash.</v>
<v Speaker 2>And it was like a human hugh mungus </v>

350
00:19:36.810 --> 00:19:39.800
<v Speaker 2>thing and it's almost unbelievable now,</v>
<v Speaker 2>right?</v>

351
00:19:39.820 --> 00:19:43.650
<v Speaker 2>Because now no one would care care </v>
<v Speaker 2>whatsoever.</v>

352
00:19:43.651 --> 00:19:44.100
<v Speaker 2>It's,</v>
<v Speaker 2>it's,</v>

353
00:19:44.100 --> 00:19:44.830
<v Speaker 2>it's,</v>
<v Speaker 2>it's just,</v>

354
00:19:44.830 --> 00:19:46.010
<v Speaker 2>it's just life,</v>
<v Speaker 2>right?</v>

355
00:19:46.430 --> 00:19:48.380
<v Speaker 2>Certainly the,</v>
<v Speaker 2>some fringe parts of his sculpture here,</v>

356
00:19:48.400 --> 00:19:52.260
<v Speaker 2>but,</v>
<v Speaker 2>but still the point is there is no fixed</v>

357
00:19:52.980 --> 00:19:54.780
<v Speaker 2>list of,</v>
<v Speaker 2>of values,</v>

358
00:19:54.810 --> 00:19:56.250
<v Speaker 2>human values.</v>

359
00:19:56.250 --> 00:19:57.083
<v Speaker 2>It's an ongoing evolving process and </v>
<v Speaker 2>what you want is for the evolution of </v>

360
00:20:02.041 --> 00:20:02.874
<v Speaker 2>the AIS values to be coupled closely </v>
<v Speaker 2>with the evolution of human values </v>

361
00:20:07.770 --> 00:20:08.603
<v Speaker 2>rather than going off in some other,</v>
<v Speaker 2>the different direction that we can't </v>

362
00:20:13.170 --> 00:20:16.500
<v Speaker 2>even understand that this is literally </v>
<v Speaker 2>playing God,</v>

363
00:20:16.590 --> 00:20:17.423
<v Speaker 2>right?</v>
<v Speaker 2>I mean if you're talking about like </v>

364
00:20:18.510 --> 00:20:19.343
<v Speaker 2>trying to program in values,</v>
<v Speaker 2>I don't think you can program in values </v>

365
00:20:23.550 --> 00:20:28.550
<v Speaker 2>that fully you can program in a system </v>
<v Speaker 2>for learning and growing values and here</v>

366
00:20:30.331 --> 00:20:32.850
<v Speaker 2>again,</v>
<v Speaker 2>the analogy with human kids,</v>

367
00:20:33.360 --> 00:20:34.193
<v Speaker 2>it's not hopeless like telling,</v>
<v Speaker 2>telling your kids these are the 10 </v>

368
00:20:38.221 --> 00:20:40.950
<v Speaker 2>things that are important doesn't work </v>
<v Speaker 2>that well,</v>

369
00:20:40.951 --> 00:20:41.660
<v Speaker 2>right?</v>
<v Speaker 2>What works?</v>

370
00:20:41.660 --> 00:20:46.050
<v Speaker 2>What works better is you enter into </v>
<v Speaker 2>shared situations with them,</v>

371
00:20:46.350 --> 00:20:47.183
<v Speaker 2>they see how deal with the situations,</v>
<v Speaker 2>you guide them in dealing with real </v>

372
00:20:50.081 --> 00:20:50.914
<v Speaker 2>situations and that forms their system </v>
<v Speaker 2>of values and this is what needs to </v>

373
00:20:55.091 --> 00:20:56.240
<v Speaker 2>happen with ais.</v>

374
00:20:56.260 --> 00:21:01.260
<v Speaker 2>They need to grow up entering into real </v>
<v Speaker 2>life situations with human beings,</v>

375
00:21:01.751 --> 00:21:04.390
<v Speaker 2>so the real life patterns of human </v>
<v Speaker 2>values,</v>

376
00:21:04.391 --> 00:21:05.224
<v Speaker 2>which are worth a lot more than the </v>
<v Speaker 2>families that we annunciate formally </v>

377
00:21:09.570 --> 00:21:10.403
<v Speaker 2>wrote,</v>
<v Speaker 2>the real life pattern of human values </v>

378
00:21:12.490 --> 00:21:17.050
<v Speaker 2>gets inculcated into the intellectual </v>
<v Speaker 2>DNA of the AI systems.</v>

379
00:21:17.051 --> 00:21:17.884
<v Speaker 2>And this is part of what worries me </v>
<v Speaker 2>about the way the AI field is going at </v>

380
00:21:22.720 --> 00:21:26.260
<v Speaker 2>this moment because I mean most of the </v>
<v Speaker 2>really powerful,</v>

381
00:21:26.740 --> 00:21:27.573
<v Speaker 2>narrow ais on the planet now are </v>
<v Speaker 2>involved with like selling people stuff </v>

382
00:21:30.971 --> 00:21:31.804
<v Speaker 2>they don't need spying on.</v>
<v Speaker 2>People are like figuring out who should </v>

383
00:21:35.321 --> 00:21:37.810
<v Speaker 2>be killed or otherwise abused by some </v>
<v Speaker 2>government.</v>

384
00:21:37.811 --> 00:21:38.644
<v Speaker 2>Right?</v>
<v Speaker 2>So if,</v>

385
00:21:38.650 --> 00:21:39.483
<v Speaker 2>if the early stage Ai's that we build </v>
<v Speaker 2>turn into general intelligences </v>

386
00:21:45.040 --> 00:21:47.920
<v Speaker 2>gradually and these general intelligence</v>
<v Speaker 2>is our,</v>

387
00:21:48.190 --> 00:21:51.340
<v Speaker 2>you know,</v>
<v Speaker 2>spy agents and advertising agents.</v>

388
00:21:51.341 --> 00:21:52.430
<v Speaker 2>Then like what,</v>
<v Speaker 2>what,</v>

389
00:21:52.630 --> 00:21:56.920
<v Speaker 2>what mindset do these early stage ais </v>
<v Speaker 2>have as they grow up?</v>

390
00:21:56.920 --> 00:21:57.753
<v Speaker 2>Right?</v>
<v Speaker 2>If they don't have any problem morally </v>

391
00:21:59.021 --> 00:22:02.020
<v Speaker 2>and ethically with manipulating us,</v>
<v Speaker 2>which we're very malleable,</v>

392
00:22:02.021 --> 00:22:02.854
<v Speaker 2>right?</v>
<v Speaker 2>We're so easy to manipulate what we're </v>

393
00:22:04.001 --> 00:22:04.834
<v Speaker 2>teaching them.</v>
<v Speaker 2>We're teaching them to manipulate tape </v>

394
00:22:07.400 --> 00:22:10.300
<v Speaker 2>on where rewarding them for doing it </v>
<v Speaker 2>successfully.</v>

395
00:22:10.660 --> 00:22:11.493
<v Speaker 2>So this is,</v>
<v Speaker 2>this is one of these things that from </v>

396
00:22:13.481 --> 00:22:18.481
<v Speaker 2>the outside point of view might not seem</v>
<v Speaker 2>to be all that intelligent it,</v>

397
00:22:18.650 --> 00:22:22.450
<v Speaker 2>it's sort of like gun laws in the U.</v>
<v Speaker 2>s living in Hong Kong.</v>

398
00:22:23.230 --> 00:22:24.063
<v Speaker 2>I mean most people don't have a bunch of</v>
<v Speaker 2>guns sitting around their house and </v>

399
00:22:28.060 --> 00:22:32.020
<v Speaker 2>coincidentally there,</v>
<v Speaker 2>there are not that many random shootings</v>

400
00:22:32.021 --> 00:22:34.210
<v Speaker 2>happening in Hong Kong.</v>
<v Speaker 2>So yeah,</v>

401
00:22:34.211 --> 00:22:36.100
<v Speaker 2>you look in the UK.</v>
<v Speaker 2>Yeah,</v>

402
00:22:36.101 --> 00:22:36.934
<v Speaker 2>you look in the US,</v>
<v Speaker 2>it's like somehow you have laws that </v>

403
00:22:40.991 --> 00:22:41.824
<v Speaker 2>allow random lunatics to buy all the </v>
<v Speaker 2>guns they want and you have all these </v>

404
00:22:44.861 --> 00:22:45.694
<v Speaker 2>people getting shot.</v>
<v Speaker 2>So similarly like from the outside you </v>

405
00:22:49.421 --> 00:22:50.254
<v Speaker 2>could look at it like this species is </v>
<v Speaker 2>creating the successor intelligence and </v>

406
00:22:56.920 --> 00:22:57.753
<v Speaker 2>almost all the resources going into </v>
<v Speaker 2>creating their successor intelligence </v>

407
00:23:02.260 --> 00:23:03.093
<v Speaker 2>are going into making ais to do </v>
<v Speaker 2>surveillance like military drones and </v>

408
00:23:09.100 --> 00:23:12.520
<v Speaker 2>advertising agents to brainwash people </v>
<v Speaker 2>into buying crap they don't need.</v>

409
00:23:12.520 --> 00:23:14.650
<v Speaker 2>Now what do you think?</v>
<v Speaker 2>Well what's wrong with this picture?</v>

410
00:23:14.651 --> 00:23:16.600
<v Speaker 2>Isn't that just because that's where the</v>
<v Speaker 2>money is,</v>

411
00:23:16.690 --> 00:23:17.523
<v Speaker 2>like this is the introduction to it and </v>
<v Speaker 2>from then we'll find other uses and </v>

412
00:23:21.821 --> 00:23:24.010
<v Speaker 2>applications for it.</v>
<v Speaker 2>But like right now,</v>

413
00:23:24.070 --> 00:23:28.030
<v Speaker 2>that's where the thing is,</v>
<v Speaker 2>there's a lot of other applications,</v>

414
00:23:28.480 --> 00:23:30.940
<v Speaker 2>financial app,</v>
<v Speaker 2>financially viable applicant.</v>

415
00:23:30.960 --> 00:23:31.793
<v Speaker 2>Oh yeah.</v>
<v Speaker 2>The applications that are getting the </v>

416
00:23:33.911 --> 00:23:37.570
<v Speaker 2>most attention are the financial lowest </v>
<v Speaker 2>hanging fruit.</v>

417
00:23:37.571 --> 00:23:38.130
<v Speaker 2>Right?</v>
<v Speaker 2>So for,</v>

418
00:23:38.130 --> 00:23:38.620
<v Speaker 2>for,</v>
<v Speaker 2>for,</v>

419
00:23:38.620 --> 00:23:39.820
<v Speaker 2>for,</v>
<v Speaker 2>for example,</v>

420
00:23:40.450 --> 00:23:44.230
<v Speaker 2>among many projects I'm doing with my </v>
<v Speaker 2>singularity net team,</v>

421
00:23:44.680 --> 00:23:48.890
<v Speaker 2>we're looking at applying ai to diagnose</v>
<v Speaker 2>agricultural disease.</v>

422
00:23:48.891 --> 00:23:51.530
<v Speaker 2>So you can,</v>
<v Speaker 2>you can look at images of plant leaves,</v>

423
00:23:51.531 --> 00:23:53.780
<v Speaker 2>you can look at data from the soil and </v>
<v Speaker 2>the atmosphere,</v>

424
00:23:54.080 --> 00:23:54.913
<v Speaker 2>and you can project whether disease and </v>
<v Speaker 2>a plant is likely to progress badly or </v>

425
00:23:58.221 --> 00:24:00.380
<v Speaker 2>not,</v>
<v Speaker 2>which tells you do you need medicine for</v>

426
00:24:00.381 --> 00:24:02.540
<v Speaker 2>the plant,</v>
<v Speaker 2>do you need pesticides now this,</v>

427
00:24:03.440 --> 00:24:06.080
<v Speaker 2>this is an interesting area of </v>
<v Speaker 2>application.</v>

428
00:24:06.410 --> 00:24:09.380
<v Speaker 2>It's probably quite financially </v>
<v Speaker 2>lucrative in in a way,</v>

429
00:24:09.381 --> 00:24:13.790
<v Speaker 2>but it's a more complex industry than,</v>
<v Speaker 2>than selling stuff online.</v>

430
00:24:14.090 --> 00:24:19.090
<v Speaker 2>So the fraction of resources going into </v>
<v Speaker 2>ai for agriculture is very small.</v>

431
00:24:19.791 --> 00:24:20.624
<v Speaker 2>Then like a ecommerce or something very </v>
<v Speaker 2>specific aspect of agriculture to </v>

432
00:24:24.981 --> 00:24:26.420
<v Speaker 2>predicting diseases.</v>
<v Speaker 2>Yeah.</v>

433
00:24:26.421 --> 00:24:27.200
<v Speaker 2>Yeah.</v>
<v Speaker 2>But there's,</v>

434
00:24:27.200 --> 00:24:29.000
<v Speaker 2>there's a lot of specific aspects,</v>
<v Speaker 2>right?</v>

435
00:24:29.001 --> 00:24:31.850
<v Speaker 2>So I mean ai for medicine,</v>
<v Speaker 2>again,</v>

436
00:24:31.851 --> 00:24:32.684
<v Speaker 2>there's been papers on machine learning </v>
<v Speaker 2>applied to medicine since the eighties </v>

437
00:24:35.931 --> 00:24:40.580
<v Speaker 2>and nineties,</v>
<v Speaker 2>but the amount of effort going into that</v>

438
00:24:40.880 --> 00:24:44.750
<v Speaker 2>compared to advertising or surveillance </v>
<v Speaker 2>is very small now.</v>

439
00:24:44.751 --> 00:24:47.890
<v Speaker 2>This has to do with the structure of the</v>
<v Speaker 2>pharmaceutical business as,</v>

440
00:24:47.900 --> 00:24:50.120
<v Speaker 2>as compared to the structure of the tech</v>
<v Speaker 2>business.</v>

441
00:24:50.121 --> 00:24:51.710
<v Speaker 2>So you know,</v>
<v Speaker 2>when you look into it,</v>

442
00:24:51.711 --> 00:24:52.580
<v Speaker 2>there's,</v>
<v Speaker 2>there's good,</v>

443
00:24:52.610 --> 00:24:55.480
<v Speaker 2>there's good reasons,</v>
<v Speaker 2>there's good reasons for,</v>

444
00:24:55.500 --> 00:24:56.660
<v Speaker 2>for everything,</v>
<v Speaker 2>right?</v>

445
00:24:56.661 --> 00:25:00.590
<v Speaker 2>But nevertheless,</v>
<v Speaker 2>the way things are coming at coming down</v>

446
00:25:00.591 --> 00:25:05.591
<v Speaker 2>right now is certain biases to the </v>
<v Speaker 2>development of early stage.</v>

447
00:25:07.011 --> 00:25:09.950
<v Speaker 2>Ai's are,</v>
<v Speaker 2>are very market and,</v>

448
00:25:09.951 --> 00:25:10.850
<v Speaker 2>and you could,</v>
<v Speaker 2>you could,</v>

449
00:25:10.880 --> 00:25:11.713
<v Speaker 2>you could see them and I'm in,</v>
<v Speaker 2>I'm trying to do something about that </v>

450
00:25:14.601 --> 00:25:17.660
<v Speaker 2>together with my colleagues and in </v>
<v Speaker 2>singularity in that.</v>

451
00:25:17.720 --> 00:25:18.553
<v Speaker 2>But of course we're.</v>
<v Speaker 2>So we're sort of a David versus goliath </v>

452
00:25:21.681 --> 00:25:22.260
<v Speaker 2>thing.</v>

453
00:25:22.260 --> 00:25:23.093
<v Speaker 3>It seemed,</v>
<v Speaker 3>well of course you're trying to do </v>

454
00:25:24.721 --> 00:25:27.780
<v Speaker 3>something different and I think it's </v>
<v Speaker 3>awesome what you guys are doing,</v>

455
00:25:28.110 --> 00:25:28.943
<v Speaker 3>but it just makes sense to me that the </v>
<v Speaker 3>first applications are going to be the </v>

456
00:25:32.761 --> 00:25:34.740
<v Speaker 3>ones that are more financially viable.</v>
<v Speaker 3>It's like,</v>

457
00:25:35.170 --> 00:25:37.450
<v Speaker 2>well,</v>
<v Speaker 2>the first applications were military,</v>

458
00:25:37.460 --> 00:25:38.293
<v Speaker 2>right until about 10 years ago,</v>
<v Speaker 2>85 percent of all funding into ai was </v>

459
00:25:42.071 --> 00:25:45.540
<v Speaker 2>from us plus plus Western Europe </v>
<v Speaker 2>military.</v>

460
00:25:45.610 --> 00:25:46.600
<v Speaker 3>Well,</v>
<v Speaker 3>what I'm getting at is it,</v>

461
00:25:46.780 --> 00:25:47.613
<v Speaker 3>it seems that money and and,</v>
<v Speaker 3>and commerce are inexorably linked to </v>

462
00:25:53.380 --> 00:25:58.270
<v Speaker 3>innovation and technology because </v>
<v Speaker 3>there's this sort of thing that we do as</v>

463
00:25:58.271 --> 00:25:59.104
<v Speaker 3>a culture where we're constantly trying </v>
<v Speaker 3>to buy and purchase bigger and better </v>

464
00:26:01.661 --> 00:26:04.000
<v Speaker 3>things.</v>
<v Speaker 3>We always want the newest iphone,</v>

465
00:26:04.001 --> 00:26:05.310
<v Speaker 3>the greatest,</v>
<v Speaker 3>you know,</v>

466
00:26:05.311 --> 00:26:06.820
<v Speaker 3>a laptop.</v>
<v Speaker 3>We don't want them,</v>

467
00:26:06.980 --> 00:26:08.640
<v Speaker 3>the coolest electric cars,</v>
<v Speaker 3>whatever,</v>

468
00:26:08.680 --> 00:26:11.320
<v Speaker 3>whatever it is,</v>
<v Speaker 3>and this fuels innovation.</v>

469
00:26:11.590 --> 00:26:14.290
<v Speaker 3>This,</v>
<v Speaker 3>this desire for new,</v>

470
00:26:14.320 --> 00:26:15.153
<v Speaker 3>greater things.</v>
<v Speaker 3>Materialism and a lot of ways fuels </v>

471
00:26:16.751 --> 00:26:18.010
<v Speaker 3>innovation because this is</v>

472
00:26:18.320 --> 00:26:19.153
<v Speaker 2>those.</v>
<v Speaker 2>But I think there's an argument that as </v>

473
00:26:21.791 --> 00:26:22.624
<v Speaker 2>we approach a technological singularity,</v>
<v Speaker 2>we need new systems because if you look </v>

474
00:26:28.361 --> 00:26:29.194
<v Speaker 2>at that,</v>
<v Speaker 2>things have happened during the last </v>

475
00:26:31.780 --> 00:26:34.330
<v Speaker 2>century.</v>
<v Speaker 2>What's happened is that governments have</v>

476
00:26:34.331 --> 00:26:37.720
<v Speaker 2>funded most of the core innovation I'm </v>
<v Speaker 2>in.</v>

477
00:26:37.721 --> 00:26:40.200
<v Speaker 2>This is well known that like most of the</v>
<v Speaker 2>technology and the,</v>

478
00:26:40.210 --> 00:26:43.120
<v Speaker 2>the smartphone was funded by US </v>
<v Speaker 2>government,</v>

479
00:26:43.140 --> 00:26:45.030
<v Speaker 2>a little about European government,</v>
<v Speaker 2>GP,</v>

480
00:26:45.060 --> 00:26:45.893
<v Speaker 2>gps and the batteries and everything.</v>
<v Speaker 2>And then companies scaled it up that </v>

481
00:26:51.030 --> 00:26:52.680
<v Speaker 2>they made,</v>
<v Speaker 2>they made it user friendly,</v>

482
00:26:52.681 --> 00:26:53.514
<v Speaker 2>they decreased cost of manufacturing and</v>
<v Speaker 2>this process occurs with a certain time </v>

483
00:26:57.601 --> 00:26:58.434
<v Speaker 2>cycle to it or like government spends </v>
<v Speaker 2>decades funding core innovation and </v>

484
00:27:02.250 --> 00:27:03.083
<v Speaker 2>universities and then industry spends </v>
<v Speaker 2>decades figuring out how to scale it up </v>

485
00:27:08.281 --> 00:27:11.180
<v Speaker 2>and make it palatable to users.</v>
<v Speaker 2>And you know,</v>

486
00:27:11.200 --> 00:27:15.210
<v Speaker 2>this matured probably since World War </v>
<v Speaker 2>II,</v>

487
00:27:15.550 --> 00:27:18.330
<v Speaker 2>this sort of modality for technology </v>
<v Speaker 2>development,</v>

488
00:27:18.660 --> 00:27:22.650
<v Speaker 2>but now that things are developing </v>
<v Speaker 2>faster and faster and faster,</v>

489
00:27:22.920 --> 00:27:23.753
<v Speaker 2>there's sort of not time for,</v>
<v Speaker 2>for that cycle to occur where the </v>

490
00:27:27.241 --> 00:27:28.074
<v Speaker 2>government and universities incubate new</v>
<v Speaker 2>ideas for awhile and then technology </v>

491
00:27:32.191 --> 00:27:32.940
<v Speaker 2>scales it up.</v>

492
00:27:32.940 --> 00:27:34.760
<v Speaker 2>So genie's out of the bottle </v>
<v Speaker 2>essentially.</v>

493
00:27:34.761 --> 00:27:35.594
<v Speaker 2>Yeah.</v>
<v Speaker 2>But we still need a lot of new amazing </v>

494
00:27:36.901 --> 00:27:37.734
<v Speaker 2>creative innovation to happen.</v>
<v Speaker 2>But somehow or other new new structures </v>

495
00:27:41.511 --> 00:27:45.180
<v Speaker 2>are going to have to evolve to,</v>
<v Speaker 2>to make it happen.</v>

496
00:27:45.181 --> 00:27:48.450
<v Speaker 2>And you can see everyone's struggling to</v>
<v Speaker 2>figure out what these are.</v>

497
00:27:48.451 --> 00:27:49.284
<v Speaker 2>So I mean this is why you have,</v>
<v Speaker 2>I mean you have big companies embracing </v>

498
00:27:51.541 --> 00:27:53.130
<v Speaker 2>open source,</v>
<v Speaker 2>Google releases,</v>

499
00:27:53.160 --> 00:27:54.780
<v Speaker 2>tensorflow,</v>
<v Speaker 2>and there's a lot of,</v>

500
00:27:55.660 --> 00:27:57.690
<v Speaker 2>lot of other different things.</v>
<v Speaker 2>And I think,</v>

501
00:27:58.140 --> 00:28:01.500
<v Speaker 2>I think some projects in the </v>
<v Speaker 2>cryptocurrency world had been looking at</v>

502
00:28:01.501 --> 00:28:02.334
<v Speaker 2>that too,</v>
<v Speaker 2>like how do we use tokens to </v>

503
00:28:03.211 --> 00:28:04.990
<v Speaker 2>incentivize,</v>
<v Speaker 2>you know,</v>

504
00:28:05.070 --> 00:28:05.903
<v Speaker 2>independent scientists and inventors to,</v>
<v Speaker 2>to do new stuff without them asking to </v>

505
00:28:10.021 --> 00:28:13.410
<v Speaker 2>be in the government research lab or in </v>
<v Speaker 2>a big company.</v>

506
00:28:13.411 --> 00:28:14.244
<v Speaker 2>So I think we're going to need the </v>
<v Speaker 2>evolution of new systems of innovation </v>

507
00:28:19.801 --> 00:28:22.990
<v Speaker 2>and of,</v>
<v Speaker 2>of technology transfer as things are,</v>

508
00:28:23.370 --> 00:28:26.070
<v Speaker 2>are developing faster and faster and </v>
<v Speaker 2>faster.</v>

509
00:28:26.071 --> 00:28:26.904
<v Speaker 2>And this is another thing that sort of </v>
<v Speaker 2>got me interested in the whole </v>

510
00:28:30.300 --> 00:28:31.133
<v Speaker 2>decentralized world and in the </v>
<v Speaker 2>blockchain world is the promise of new </v>

511
00:28:35.251 --> 00:28:39.210
<v Speaker 2>modes of economic and social </v>
<v Speaker 2>organization that can,</v>

512
00:28:39.230 --> 00:28:40.063
<v Speaker 2>you know,</v>
<v Speaker 2>bring more of the world into the </v>

513
00:28:41.521 --> 00:28:45.150
<v Speaker 2>research process and accelerate the </v>
<v Speaker 2>technology transfer</v>

514
00:28:45.240 --> 00:28:47.060
<v Speaker 3>process.</v>
<v Speaker 3>I definitely want to talk about that,</v>

515
00:28:47.061 --> 00:28:49.010
<v Speaker 3>but one of the things that I want to ask</v>
<v Speaker 3>you is when,</v>

516
00:28:49.370 --> 00:28:50.203
<v Speaker 3>when you're discussing this,</v>
<v Speaker 3>I think what you're saying is have one </v>

517
00:28:54.621 --> 00:28:55.454
<v Speaker 3>very important point that we need to </v>
<v Speaker 3>move past the military gatekeepers of </v>

518
00:28:57.981 --> 00:28:58.720
<v Speaker 3>tech.</v>

519
00:28:58.720 --> 00:29:00.090
<v Speaker 2>Not just military,</v>
<v Speaker 2>no.</v>

520
00:29:00.220 --> 00:29:05.220
<v Speaker 2>It's big tech which are at advertising </v>
<v Speaker 2>agencies in social media.</v>

521
00:29:06.710 --> 00:29:10.160
<v Speaker 3>The things that are constantly </v>
<v Speaker 3>predicting your next purchase,</v>

522
00:29:10.290 --> 00:29:10.711
<v Speaker 2>right?</v>
<v Speaker 2>Yeah,</v>

523
00:29:10.711 --> 00:29:12.840
<v Speaker 2>because if you,</v>
<v Speaker 2>if you think about it,</v>

524
00:29:13.080 --> 00:29:18.080
<v Speaker 2>and I'm in even in a semi democracy look</v>
<v Speaker 2>like we have in the US,</v>

525
00:29:20.190 --> 00:29:21.023
<v Speaker 2>I mean those who control the </v>
<v Speaker 2>brainwashing of the public in essence </v>

526
00:29:24.811 --> 00:29:25.740
<v Speaker 2>control who,</v>
<v Speaker 2>who,</v>

527
00:29:25.760 --> 00:29:26.593
<v Speaker 2>who votes for what and who controls the </v>
<v Speaker 2>brainwashing of the public is </v>

528
00:29:29.821 --> 00:29:33.750
<v Speaker 2>advertising agencies and who </v>
<v Speaker 2>increasingly are the biggest advertising</v>

529
00:29:33.751 --> 00:29:34.584
<v Speaker 2>agencies or are the big tech companies </v>
<v Speaker 2>who are accumulating everybody's data </v>

530
00:29:38.701 --> 00:29:42.340
<v Speaker 2>and using it to,</v>
<v Speaker 2>to program their minds to buy things.</v>

531
00:29:42.341 --> 00:29:44.980
<v Speaker 2>So this is what's programmed the global </v>
<v Speaker 2>brain of,</v>

532
00:29:44.981 --> 00:29:45.391
<v Speaker 2>of,</v>
<v Speaker 2>of,</v>

533
00:29:45.391 --> 00:29:46.224
<v Speaker 2>of the human race.</v>
<v Speaker 2>And of course there are close links </v>

534
00:29:48.911 --> 00:29:53.240
<v Speaker 2>between big tech and the military.</v>
<v Speaker 2>Let's look at Amazon has what?</v>

535
00:29:53.310 --> 00:29:55.810
<v Speaker 2>Twenty 5,000</v>
<v Speaker 2>person headquarters in Crystal City,</v>

536
00:29:55.811 --> 00:29:56.644
<v Speaker 2>Virginia,</v>
<v Speaker 2>right next to the Pentagon and in China </v>

537
00:29:58.871 --> 00:30:01.570
<v Speaker 2>it's even more direct and unapologetic,</v>
<v Speaker 2>right?</v>

538
00:30:01.571 --> 00:30:06.571
<v Speaker 2>So it's a new like military industrial </v>
<v Speaker 2>advertising complex,</v>

539
00:30:07.630 --> 00:30:12.630
<v Speaker 2>which is his guiding the evolution of </v>
<v Speaker 2>the global brain on the planet,</v>

540
00:30:13.390 --> 00:30:15.640
<v Speaker 2>which with this past election,</v>
<v Speaker 2>right,</v>

541
00:30:15.641 --> 00:30:16.474
<v Speaker 2>with all the intrusion by foreign </v>
<v Speaker 2>entities trying to influence the </v>

542
00:30:20.021 --> 00:30:25.021
<v Speaker 2>election that they have these giant </v>
<v Speaker 2>houses set up to write bad stories about</v>

543
00:30:25.871 --> 00:30:28.870
<v Speaker 2>whoever they don't want to be in office.</v>

544
00:30:29.050 --> 00:30:32.080
<v Speaker 2>Yeah.</v>
<v Speaker 2>In a way that's almost a red herring,</v>

545
00:30:32.081 --> 00:30:35.800
<v Speaker 2>but I mean they're Russian stuff is </v>
<v Speaker 2>almost a red herring.</v>

546
00:30:36.100 --> 00:30:39.790
<v Speaker 2>But it revealed what the processes are,</v>
<v Speaker 2>which,</v>

547
00:30:39.791 --> 00:30:40.624
<v Speaker 2>which are used to program because I </v>
<v Speaker 2>think the whatever programming of </v>

548
00:30:44.531 --> 00:30:45.364
<v Speaker 2>Americans' minds is done by the Russians</v>
<v Speaker 2>is many minuscule compared to the </v>

549
00:30:49.840 --> 00:30:52.800
<v Speaker 2>programming of Americans' minds by my </v>
<v Speaker 2>advisee,</v>

550
00:30:52.810 --> 00:30:56.190
<v Speaker 2>American American corporate and </v>
<v Speaker 2>government elite.</v>

551
00:30:56.420 --> 00:31:00.810
<v Speaker 2>So it's fascinating that anybody's even </v>
<v Speaker 2>jumping in as well as an elite.</v>

552
00:31:01.060 --> 00:31:01.960
<v Speaker 2>Sure.</v>
<v Speaker 2>It's,</v>

553
00:31:01.990 --> 00:31:02.823
<v Speaker 2>it's,</v>
<v Speaker 2>it's,</v>

554
00:31:02.970 --> 00:31:03.803
<v Speaker 2>it's,</v>
<v Speaker 2>it's interesting if you look at what's </v>

555
00:31:05.201 --> 00:31:07.780
<v Speaker 2>happening in China that's like,</v>
<v Speaker 2>yeah,</v>

556
00:31:07.781 --> 00:31:08.351
<v Speaker 2>yeah,</v>
<v Speaker 2>yeah.</v>

557
00:31:08.351 --> 00:31:10.580
<v Speaker 2>They're,</v>
<v Speaker 2>they're way better than the,</v>

558
00:31:10.860 --> 00:31:12.850
<v Speaker 2>than we are much more horrific.</v>
<v Speaker 2>Right.</v>

559
00:31:13.270 --> 00:31:15.340
<v Speaker 2>And that's all.</v>
<v Speaker 2>It's more,</v>

560
00:31:15.430 --> 00:31:19.060
<v Speaker 2>it's more professional,</v>
<v Speaker 2>it's more polished,</v>

561
00:31:19.061 --> 00:31:22.180
<v Speaker 2>it's more centralized.</v>
<v Speaker 2>On the other hand,</v>

562
00:31:22.390 --> 00:31:23.223
<v Speaker 2>for almost everyone in China,</v>
<v Speaker 2>China is a very good place to live and </v>

563
00:31:26.891 --> 00:31:30.640
<v Speaker 2>you know,</v>
<v Speaker 2>the level of improvement in that country</v>

564
00:31:30.641 --> 00:31:33.190
<v Speaker 2>in the last 30 years has just been </v>
<v Speaker 2>astounding,</v>

565
00:31:33.191 --> 00:31:34.024
<v Speaker 2>right?</v>

566
00:31:34.030 --> 00:31:34.750
<v Speaker 2>I mean,</v>
<v Speaker 2>you can't,</v>

567
00:31:34.750 --> 00:31:35.583
<v Speaker 2>you can't argue with how much better </v>
<v Speaker 2>it's gotten there since shouting took </v>

568
00:31:39.041 --> 00:31:39.751
<v Speaker 2>over.</v>
<v Speaker 2>It's,</v>

569
00:31:39.751 --> 00:31:41.690
<v Speaker 2>it's tremendous because they're not,</v>
<v Speaker 2>they,</v>

570
00:31:41.691 --> 00:31:44.140
<v Speaker 2>they embraced capitalism to a certain </v>
<v Speaker 2>extent.</v>

571
00:31:44.260 --> 00:31:47.470
<v Speaker 2>They've created their own unique system </v>
<v Speaker 2>with what labels you give.</v>

572
00:31:47.471 --> 00:31:49.750
<v Speaker 2>It is,</v>
<v Speaker 2>it's almost arbitrary.</v>

573
00:31:49.751 --> 00:31:50.584
<v Speaker 2>They,</v>
<v Speaker 2>they've created their own unique system </v>

574
00:31:53.440 --> 00:31:54.700
<v Speaker 2>as a,</v>
<v Speaker 2>you know,</v>

575
00:31:54.760 --> 00:31:57.100
<v Speaker 2>crazy hippie,</v>
<v Speaker 2>libertarian,</v>

576
00:31:57.130 --> 00:31:59.650
<v Speaker 2>a narco socialist,</v>
<v Speaker 2>freedom loving,</v>

577
00:31:59.890 --> 00:32:00.723
<v Speaker 2>maniac.</v>
<v Speaker 2>That system rubs against my grain in </v>

578
00:32:03.461 --> 00:32:05.380
<v Speaker 2>many ways.</v>
<v Speaker 2>On the other hand,</v>

579
00:32:05.381 --> 00:32:08.500
<v Speaker 2>empirically if you look at it,</v>
<v Speaker 2>it's improved the wellbeing of a,</v>

580
00:32:08.950 --> 00:32:09.783
<v Speaker 2>of a tremendous number of people.</v>
<v Speaker 2>So hopefully it evolves and it's one </v>

581
00:32:13.511 --> 00:32:14.344
<v Speaker 2>style.</v>
<v Speaker 2>But the way it's evolving now is not in </v>

582
00:32:17.021 --> 00:32:18.940
<v Speaker 2>a more positive freedom.</v>
<v Speaker 2>Love.</v>

583
00:32:18.970 --> 00:32:21.190
<v Speaker 2>Well,</v>
<v Speaker 2>it's not in the more freedom,</v>

584
00:32:21.191 --> 00:32:25.780
<v Speaker 2>loving and an archaic direction.</v>
<v Speaker 2>One would say it's positive in some ways</v>

585
00:32:25.840 --> 00:32:29.320
<v Speaker 2>and negative and others like most </v>
<v Speaker 2>complex Hong Kong.</v>

586
00:32:29.380 --> 00:32:30.213
<v Speaker 2>Why do you live there?</v>

587
00:32:30.500 --> 00:32:32.830
<v Speaker 2>Um,</v>
<v Speaker 2>I fell in love with a Chinese woman.</v>

588
00:32:33.460 --> 00:32:34.110
<v Speaker 2>Hey,</v>
<v Speaker 2>go.</v>

589
00:32:34.110 --> 00:32:36.370
<v Speaker 2>She's crazy.</v>
<v Speaker 2>She has great reason.</v>

590
00:32:36.410 --> 00:32:38.050
<v Speaker 2>We had a baby recently.</v>
<v Speaker 2>She,</v>

591
00:32:38.230 --> 00:32:40.700
<v Speaker 2>she's not from Hong,</v>
<v Speaker 2>she's from mainland China.</v>

592
00:32:41.180 --> 00:32:46.070
<v Speaker 2>I met her when she was doing her phd in </v>
<v Speaker 2>computational linguistics and Shaman,</v>

593
00:32:46.071 --> 00:32:47.470
<v Speaker 2>but that,</v>
<v Speaker 2>that,</v>

594
00:32:47.510 --> 00:32:51.920
<v Speaker 2>that was what sort of first got me to </v>
<v Speaker 2>spend a lot of time in China,</v>

595
00:32:51.921 --> 00:32:52.754
<v Speaker 2>but then I was doing some research at </v>
<v Speaker 2>Hong Kong Polytechnic University and </v>

596
00:32:56.871 --> 00:33:01.460
<v Speaker 2>then my good friend David Hansen was </v>
<v Speaker 2>visiting me in Hong Kong.</v>

597
00:33:01.730 --> 00:33:02.563
<v Speaker 2>I introduced him to some investors there</v>
<v Speaker 2>which ended up with him bringing his </v>

598
00:33:06.351 --> 00:33:08.150
<v Speaker 2>company,</v>
<v Speaker 2>Hanson robotics to Hong Kong.</v>

599
00:33:08.151 --> 00:33:12.890
<v Speaker 2>So now after I moved there because of </v>
<v Speaker 2>falling in love with a rating,</v>

600
00:33:12.891 --> 00:33:17.120
<v Speaker 2>then I brought my friend David there </v>
<v Speaker 2>than Hanson robotics.</v>

601
00:33:17.121 --> 00:33:17.954
<v Speaker 2>Grew up there and there's actually a </v>
<v Speaker 2>good reason for Hanson robotics to be </v>

602
00:33:20.631 --> 00:33:22.850
<v Speaker 2>there because I'm in the best place in </v>
<v Speaker 2>the world.</v>

603
00:33:22.850 --> 00:33:26.600
<v Speaker 2>The manufacturer complex electronics is </v>
<v Speaker 2>in Shenzhen,</v>

604
00:33:26.601 --> 00:33:27.434
<v Speaker 2>rarely across the border from Hong Kong.</v>
<v Speaker 2>So now I've been working there with </v>

605
00:33:30.171 --> 00:33:33.240
<v Speaker 2>Hanson robotics on the Sophia robots and</v>
<v Speaker 2>other robots for,</v>

606
00:33:33.300 --> 00:33:34.460
<v Speaker 2>for,</v>
<v Speaker 2>for awhile.</v>

607
00:33:34.670 --> 00:33:35.503
<v Speaker 2>And I've accumulated the whole ai team </v>
<v Speaker 2>there around Hanson robotics and ai and </v>

608
00:33:39.081 --> 00:33:40.550
<v Speaker 2>singularity in that.</v>
<v Speaker 2>So I'm in by,</v>

609
00:33:41.330 --> 00:33:42.163
<v Speaker 2>by now.</v>
<v Speaker 2>I'm there because my whole ai and </v>

610
00:33:44.061 --> 00:33:46.490
<v Speaker 2>robotics teams are there.</v>
<v Speaker 2>It makes sense.</v>

611
00:33:46.700 --> 00:33:48.020
<v Speaker 2>Um,</v>
<v Speaker 2>do you follow,</v>

612
00:33:48.021 --> 00:33:48.854
<v Speaker 2>uh,</v>
<v Speaker 2>the State Department's recommendations </v>

613
00:33:50.961 --> 00:33:54.640
<v Speaker 2>to not use walway devices and they </v>
<v Speaker 2>believe that they're all know,</v>

614
00:33:55.260 --> 00:33:59.300
<v Speaker 2>even heard that.</v>
<v Speaker 2>I mean if the Chinese are spying on us,</v>

615
00:33:59.320 --> 00:34:00.350
<v Speaker 2>you know,</v>
<v Speaker 2>I'm sure.</v>

616
00:34:00.540 --> 00:34:01.373
<v Speaker 2>You know,</v>
<v Speaker 2>when I lived in my lived in Washington </v>

617
00:34:02.571 --> 00:34:03.404
<v Speaker 2>DC for nine years,</v>
<v Speaker 2>I did a bunch of consulting for various </v>

618
00:34:07.251 --> 00:34:08.084
<v Speaker 2>government agencies there.</v>
<v Speaker 2>And my wife is a communist party member </v>

619
00:34:12.171 --> 00:34:13.290
<v Speaker 2>actually.</v>
<v Speaker 2>Well,</v>

620
00:34:13.370 --> 00:34:17.360
<v Speaker 2>just because she joined in high school </v>
<v Speaker 2>when it was sort of suggested for her to</v>

621
00:34:17.361 --> 00:34:17.990
<v Speaker 2>join.</v>

622
00:34:17.990 --> 00:34:19.500
<v Speaker 2>So I'm,</v>
<v Speaker 2>I'm,</v>

623
00:34:19.530 --> 00:34:22.040
<v Speaker 2>I'm sure I'm being watched by multiple </v>
<v Speaker 2>governments.</v>

624
00:34:22.510 --> 00:34:24.740
<v Speaker 2>It doesn't,</v>
<v Speaker 2>I don't have any secrets.</v>

625
00:34:24.810 --> 00:34:25.643
<v Speaker 2>It doesn't really matter.</v>
<v Speaker 2>I'm not in the business of trying to </v>

626
00:34:28.761 --> 00:34:29.594
<v Speaker 2>overthrow the government.</v>
<v Speaker 2>And I'm in the business of trying to </v>

627
00:34:33.020 --> 00:34:37.700
<v Speaker 2>bypass traditional governments and </v>
<v Speaker 2>traditional monetary systems and all the</v>

628
00:34:37.701 --> 00:34:42.701
<v Speaker 2>rest by creating new methods of </v>
<v Speaker 2>organization of people and information.</v>

629
00:34:43.281 --> 00:34:44.114
<v Speaker 2>You understand that what you personally.</v>
<v Speaker 2>But it is unusual if the government is </v>

630
00:34:46.641 --> 00:34:48.850
<v Speaker 2>actually spying on people through these </v>
<v Speaker 2>divides out.</v>

631
00:34:48.851 --> 00:34:51.080
<v Speaker 2>It's unusual.</v>
<v Speaker 2>It's unusual at all.</v>

632
00:34:51.081 --> 00:34:51.580
<v Speaker 2>I mean,</v>
<v Speaker 2>I,</v>

633
00:34:51.580 --> 00:34:52.413
<v Speaker 2>I,</v>
<v Speaker 2>I mean without going into too much </v>

634
00:34:54.741 --> 00:34:55.574
<v Speaker 2>detail,</v>
<v Speaker 2>like when I was in dc working with </v>

635
00:34:57.741 --> 00:34:58.574
<v Speaker 2>various government agencies,</v>
<v Speaker 2>it became clear there is tremendously </v>

636
00:35:02.721 --> 00:35:07.721
<v Speaker 2>more information obtained by government </v>
<v Speaker 2>agencies than most people realize,</v>

637
00:35:08.560 --> 00:35:09.393
<v Speaker 2>well,</v>
<v Speaker 2>this was,</v>

638
00:35:09.440 --> 00:35:10.273
<v Speaker 2>this was true way before snowden and </v>
<v Speaker 2>wikileaks and all these revelations and </v>

639
00:35:14.240 --> 00:35:19.240
<v Speaker 2>what is publicly understood now is </v>
<v Speaker 2>probably not the full scope of,</v>

640
00:35:21.740 --> 00:35:24.590
<v Speaker 2>of,</v>
<v Speaker 2>of the information that governments have</v>

641
00:35:24.591 --> 00:35:24.960
<v Speaker 2>others.</v>

642
00:35:24.960 --> 00:35:28.790
<v Speaker 2>So I'm in privacy is,</v>
<v Speaker 2>is pretty much dead.</v>

643
00:35:28.791 --> 00:35:31.820
<v Speaker 2>And David Brin,</v>
<v Speaker 2>do you know David Brin?</v>

644
00:35:31.880 --> 00:35:32.713
<v Speaker 2>No.</v>
<v Speaker 2>You should definitely interview David </v>

645
00:35:33.900 --> 00:35:34.733
<v Speaker 2>and he's an amazing guy,</v>
<v Speaker 2>but he's a well known science fiction </v>

646
00:35:37.651 --> 00:35:39.760
<v Speaker 2>writer.</v>
<v Speaker 2>He's based in southern California,</v>

647
00:35:39.820 --> 00:35:43.500
<v Speaker 2>San Diego.</v>
<v Speaker 2>But he wrote a book in Oh,</v>

648
00:35:43.500 --> 00:35:47.430
<v Speaker 2>years ago called the transparent society</v>
<v Speaker 2>where he said there's two possibilities,</v>

649
00:35:47.431 --> 00:35:48.264
<v Speaker 2>surveillance and sousveillance.</v>
<v Speaker 2>It's like the power elite watching </v>

650
00:35:51.361 --> 00:35:55.530
<v Speaker 2>everyone or everyone watching everyone.</v>
<v Speaker 2>I think everyone watching everyone.</v>

651
00:35:55.531 --> 00:35:55.861
<v Speaker 2>Well,</v>
<v Speaker 2>yeah,</v>

652
00:35:55.861 --> 00:35:56.694
<v Speaker 2>but he.</v>
<v Speaker 2>So he articulated this as the </v>

653
00:35:58.590 --> 00:36:02.340
<v Speaker 2>essentially the only two viable </v>
<v Speaker 2>possibilities and he's like,</v>

654
00:36:02.760 --> 00:36:07.230
<v Speaker 2>we should be choosing and then creating </v>
<v Speaker 2>which of these alternatives we want.</v>

655
00:36:07.231 --> 00:36:08.064
<v Speaker 2>So now,</v>
<v Speaker 2>now the world is starting to understand </v>

656
00:36:11.341 --> 00:36:13.710
<v Speaker 2>what he was talking about.</v>
<v Speaker 2>But back when he wrote that book,</v>

657
00:36:14.160 --> 00:36:15.630
<v Speaker 2>you wrote the book.</v>
<v Speaker 2>Oh,</v>

658
00:36:15.631 --> 00:36:18.730
<v Speaker 2>I can't remember.</v>
<v Speaker 2>I mean it was well more than a decade.</v>

659
00:36:18.731 --> 00:36:21.270
<v Speaker 2>That gets weird,</v>
<v Speaker 2>but some people just nail it on the head</v>

660
00:36:21.480 --> 00:36:22.680
<v Speaker 2>decades in advance.</v>

661
00:36:22.680 --> 00:36:25.050
<v Speaker 2>I mean,</v>
<v Speaker 2>most of the things that are happening in</v>

662
00:36:25.051 --> 00:36:29.130
<v Speaker 2>the world now we're foreseen by a </v>
<v Speaker 2>stanislaw lem,</v>

663
00:36:29.131 --> 00:36:32.010
<v Speaker 2>the Polish Arthur mentioned volunteer </v>
<v Speaker 2>church.</v>

664
00:36:32.011 --> 00:36:34.380
<v Speaker 2>And a friend of mine who was the founder</v>
<v Speaker 2>of Russian Ai.</v>

665
00:36:34.381 --> 00:36:38.580
<v Speaker 2>He wrote a book called the phenomenon of</v>
<v Speaker 2>science in the late sixties.</v>

666
00:36:38.581 --> 00:36:39.390
<v Speaker 2>Then,</v>
<v Speaker 2>you know,</v>

667
00:36:39.390 --> 00:36:40.223
<v Speaker 2>in 1971 or two when I was a little kid,</v>
<v Speaker 2>I read a book called the promethium </v>

668
00:36:45.540 --> 00:36:48.960
<v Speaker 2>project by a Princeton physics called </v>
<v Speaker 2>Gerald Gerald Fund Berg.</v>

669
00:36:48.990 --> 00:36:51.000
<v Speaker 2>You've read a physical book when you're </v>
<v Speaker 2>five years old.</v>

670
00:36:51.570 --> 00:36:54.660
<v Speaker 2>I started reading when I was two and my </v>
<v Speaker 2>grandfather was a physicist,</v>

671
00:36:54.670 --> 00:36:59.490
<v Speaker 2>so I was reading a lot of stuff then.</v>
<v Speaker 2>But he Feinberg in this book,</v>

672
00:36:59.580 --> 00:37:00.960
<v Speaker 2>he said,</v>
<v Speaker 2>you know,</v>

673
00:37:00.961 --> 00:37:01.794
<v Speaker 2>within the next few decades,</v>
<v Speaker 2>humanity is going to create </v>

674
00:37:03.750 --> 00:37:04.583
<v Speaker 2>nanotechnology,</v>
<v Speaker 2>it's going to create machines smarter </v>

675
00:37:06.151 --> 00:37:09.780
<v Speaker 2>than people and it's going to create the</v>
<v Speaker 2>technology to allow human,</v>

676
00:37:09.781 --> 00:37:12.210
<v Speaker 2>biological immortality.</v>
<v Speaker 2>And the question will be,</v>

677
00:37:12.211 --> 00:37:14.700
<v Speaker 2>do we want to use these technologies,</v>
<v Speaker 2>you know,</v>

678
00:37:14.701 --> 00:37:15.534
<v Speaker 2>to promote rampant consumerism or do we </v>
<v Speaker 2>want to use these technologies to </v>

679
00:37:19.831 --> 00:37:20.550
<v Speaker 2>promote,</v>
<v Speaker 2>you know,</v>

680
00:37:20.550 --> 00:37:23.820
<v Speaker 2>spiritual growth of our,</v>
<v Speaker 2>of our consciousness into new dimensions</v>

681
00:37:23.821 --> 00:37:28.290
<v Speaker 2>of experience and what fonder proposed </v>
<v Speaker 2>in this book in the late sixties,</v>

682
00:37:28.291 --> 00:37:29.124
<v Speaker 2>which I read in the early seventies,</v>
<v Speaker 2>he proposed the UN should send a task </v>

683
00:37:32.861 --> 00:37:34.620
<v Speaker 2>force out to go to everyone in the </v>
<v Speaker 2>world,</v>

684
00:37:34.621 --> 00:37:35.454
<v Speaker 2>every little African village and educate</v>
<v Speaker 2>the world about nanotech life extension </v>

685
00:37:40.741 --> 00:37:41.574
<v Speaker 2>and an Agi and get the whole world to </v>
<v Speaker 2>vote on whether we should develop these </v>

686
00:37:45.451 --> 00:37:49.500
<v Speaker 2>technologies toward consumerism or </v>
<v Speaker 2>toward consciousness expansion.</v>

687
00:37:49.500 --> 00:37:51.510
<v Speaker 2>So I read this from a little kid.</v>
<v Speaker 2>It's like,</v>

688
00:37:52.200 --> 00:37:54.840
<v Speaker 2>this is almost obvious.</v>
<v Speaker 2>This makes total sense.</v>

689
00:37:54.841 --> 00:37:55.674
<v Speaker 2>Like,</v>
<v Speaker 2>why?</v>

690
00:37:56.580 --> 00:38:00.570
<v Speaker 2>Why does everyone understand this?</v>
<v Speaker 2>Then I tried to explain this to people.</v>

691
00:38:01.150 --> 00:38:03.840
<v Speaker 2>I'm like,</v>
<v Speaker 2>Oh shit,</v>

692
00:38:03.841 --> 00:38:07.650
<v Speaker 2>I guess it's gonna be awhile till the </v>
<v Speaker 2>world catches on,</v>

693
00:38:07.651 --> 00:38:08.484
<v Speaker 2>so.</v>
<v Speaker 2>So I instead decided I should build a </v>

694
00:38:10.080 --> 00:38:10.913
<v Speaker 2>spacecraft,</v>
<v Speaker 2>go away from the world at rapid speed </v>

695
00:38:13.321 --> 00:38:16.890
<v Speaker 2>and come back after like a million years</v>
<v Speaker 2>or something when the world was far more</v>

696
00:38:16.891 --> 00:38:19.080
<v Speaker 2>advanced.</v>
<v Speaker 2>So covered in dust.</v>

697
00:38:19.140 --> 00:38:19.990
<v Speaker 2>Yeah.</v>
<v Speaker 2>Right.</v>

698
00:38:20.100 --> 00:38:20.933
<v Speaker 2>So now we'll.</v>
<v Speaker 2>Then you go another million years or so </v>

699
00:38:25.830 --> 00:38:26.663
<v Speaker 2>now,</v>
<v Speaker 2>pretty much the world agrees that life </v>

700
00:38:28.831 --> 00:38:29.664
<v Speaker 2>extension,</v>
<v Speaker 2>Agi and nanotechnology or plausible </v>

701
00:38:32.641 --> 00:38:35.190
<v Speaker 2>things that may come about in the near </v>
<v Speaker 2>future.</v>

702
00:38:35.920 --> 00:38:36.753
<v Speaker 2>The same question is,</v>
<v Speaker 2>is there that that Feinberg saw like 50 </v>

703
00:38:41.381 --> 00:38:42.214
<v Speaker 2>years ago,</v>
<v Speaker 2>right?</v>

704
00:38:42.270 --> 00:38:43.690
<v Speaker 2>The same question.</v>
<v Speaker 2>Is there like,</v>

705
00:38:43.691 --> 00:38:44.524
<v Speaker 2>did we develop this for rampant </v>
<v Speaker 2>consumerism or do we develop this for </v>

706
00:38:49.990 --> 00:38:54.990
<v Speaker 2>amazing new dimensions of consciousness </v>
<v Speaker 2>expansion and mental growth,</v>

707
00:38:57.430 --> 00:38:58.263
<v Speaker 2>but the UN is not in fact educating the </v>
<v Speaker 2>world about this and pulling them to </v>

708
00:39:03.791 --> 00:39:06.990
<v Speaker 2>decide democratically what to do on.</v>

709
00:39:07.010 --> 00:39:08.260
<v Speaker 2>On.</v>
<v Speaker 2>On the other hand,</v>

710
00:39:08.620 --> 00:39:09.453
<v Speaker 2>there's the possibility that by </v>
<v Speaker 2>bypassing governments and the UN and </v>

711
00:39:13.361 --> 00:39:17.530
<v Speaker 2>doing something decentralized,</v>
<v Speaker 2>you can create a democratic framework,</v>

712
00:39:17.550 --> 00:39:20.040
<v Speaker 2>you know,</v>
<v Speaker 2>within which you know,</v>

713
00:39:20.050 --> 00:39:20.883
<v Speaker 2>a broad swath of the world can be </v>
<v Speaker 2>involved in a participatory way in </v>

714
00:39:23.741 --> 00:39:24.574
<v Speaker 2>guiding the direction of these advances.</v>
<v Speaker 2>Do you think that it's possible that </v>

715
00:39:27.880 --> 00:39:31.330
<v Speaker 2>instead of choosing that we're just </v>
<v Speaker 2>going to have multiple directions,</v>

716
00:39:31.331 --> 00:39:33.690
<v Speaker 2>that it's growing in that there's going </v>
<v Speaker 2>to be consumer.</v>

717
00:39:33.760 --> 00:39:38.680
<v Speaker 2>There will be multiple directions and </v>
<v Speaker 2>it's that that's inevitable.</v>

718
00:39:38.830 --> 00:39:43.830
<v Speaker 2>It's more a matter of whether anything </v>
<v Speaker 2>besides the military advertising complex</v>

719
00:39:45.371 --> 00:39:46.204
<v Speaker 2>gets a shake.</v>
<v Speaker 2>So I mean if you look in the software </v>

720
00:39:48.161 --> 00:39:52.000
<v Speaker 2>development world,</v>
<v Speaker 2>open source is an amazing thing,</v>

721
00:39:52.020 --> 00:39:56.290
<v Speaker 2>right?</v>
<v Speaker 2>Linux is awesome and it's led to so much</v>

722
00:39:56.291 --> 00:39:58.420
<v Speaker 2>ai being open,</v>
<v Speaker 2>open source now.</v>

723
00:39:58.600 --> 00:40:03.600
<v Speaker 2>Now Open source didn't have to actually </v>
<v Speaker 2>take over the entire software world like</v>

724
00:40:04.210 --> 00:40:07.570
<v Speaker 2>Richard Stallman one and in order to </v>
<v Speaker 2>have a huge impact,</v>

725
00:40:07.571 --> 00:40:07.880
<v Speaker 2>right?</v>

726
00:40:07.880 --> 00:40:12.250
<v Speaker 2>It's enough that it's a major force.</v>
<v Speaker 2>So I mean it's very hippy concept,</v>

727
00:40:12.251 --> 00:40:14.230
<v Speaker 2>isn't it?</v>
<v Speaker 2>Open source and a lot of ways in a way,</v>

728
00:40:14.231 --> 00:40:15.570
<v Speaker 2>but,</v>
<v Speaker 2>but yet ibm,</v>

729
00:40:15.620 --> 00:40:18.970
<v Speaker 2>IBM has probably thousands of people </v>
<v Speaker 2>working on Linux.</v>

730
00:40:18.971 --> 00:40:22.660
<v Speaker 2>Right?</v>
<v Speaker 2>So like apple began as a hippie concept,</v>

731
00:40:22.661 --> 00:40:24.460
<v Speaker 2>but it became very practical.</v>
<v Speaker 2>Right?</v>

732
00:40:24.461 --> 00:40:29.461
<v Speaker 2>So I mean something like 75 percent of </v>
<v Speaker 2>all the servers running the Internet are</v>

733
00:40:29.861 --> 00:40:31.600
<v Speaker 2>based in Linux.</v>
<v Speaker 2>You know,</v>

734
00:40:31.601 --> 00:40:34.750
<v Speaker 2>the vast majority of mobile phone oss </v>
<v Speaker 2>is,</v>

735
00:40:34.751 --> 00:40:36.160
<v Speaker 2>is,</v>
<v Speaker 2>is Linux,</v>

736
00:40:36.161 --> 00:40:38.050
<v Speaker 2>right?</v>
<v Speaker 2>So this Hippie,</v>

737
00:40:38.051 --> 00:40:40.800
<v Speaker 2>vast majority being android is Linux.</v>
<v Speaker 2>Yeah.</v>

738
00:40:40.810 --> 00:40:41.643
<v Speaker 2>Yeah.</v>
<v Speaker 2>So I'm in this hippie crazy thing where </v>

739
00:40:44.951 --> 00:40:45.784
<v Speaker 2>no one owns the code.</v>
<v Speaker 2>It didn't have to overtake the whole </v>

740
00:40:50.351 --> 00:40:51.184
<v Speaker 2>software economy and become everything </v>
<v Speaker 2>to become highly valuable and then </v>

741
00:40:55.121 --> 00:40:58.390
<v Speaker 2>inject a different dimension into </v>
<v Speaker 2>things.</v>

742
00:40:58.391 --> 00:41:02.950
<v Speaker 2>And I think the same is true with </v>
<v Speaker 2>decentralized ai,</v>

743
00:41:02.951 --> 00:41:05.680
<v Speaker 2>which we're looking at with singularity </v>
<v Speaker 2>and that it doesn't have,</v>

744
00:41:05.710 --> 00:41:08.590
<v Speaker 2>we don't have to actually put Google </v>
<v Speaker 2>and,</v>

745
00:41:08.591 --> 00:41:12.700
<v Speaker 2>and the US and Chinese military and </v>
<v Speaker 2>tencent out of business.</v>

746
00:41:12.701 --> 00:41:13.534
<v Speaker 2>Right.</v>
<v Speaker 2>Although if that happens that that's </v>

747
00:41:15.160 --> 00:41:16.890
<v Speaker 2>fine.</v>
<v Speaker 2>But W we,</v>

748
00:41:17.260 --> 00:41:18.093
<v Speaker 2>it's enough that we become an extremely </v>
<v Speaker 2>major player in that ecosystem so that </v>

749
00:41:23.191 --> 00:41:24.290
<v Speaker 2>this,</v>
<v Speaker 2>you know,</v>

750
00:41:24.400 --> 00:41:25.233
<v Speaker 2>participatory and benefit oriented </v>
<v Speaker 2>aspect becomes a really significant </v>

751
00:41:30.610 --> 00:41:35.420
<v Speaker 2>component of how humanity is,</v>
<v Speaker 2>is developing general intelligence.</v>

752
00:41:36.140 --> 00:41:36.973
<v Speaker 3>W is accepted,</v>
<v Speaker 3>generally accepted that human beings </v>

753
00:41:39.341 --> 00:41:41.500
<v Speaker 3>will consistently and constantly </v>
<v Speaker 3>innovate.</v>

754
00:41:42.010 --> 00:41:44.200
<v Speaker 3>Right.</v>
<v Speaker 3>It just seems to be characteristics that</v>

755
00:41:44.500 --> 00:41:47.290
<v Speaker 3>we have.</v>
<v Speaker 3>Why do you think that is and what do you</v>

756
00:41:47.291 --> 00:41:48.610
<v Speaker 3>think that,</v>
<v Speaker 3>especially when it,</v>

757
00:41:48.611 --> 00:41:51.280
<v Speaker 3>in terms of creating something like </v>
<v Speaker 3>artificial intelligence,</v>

758
00:41:51.281 --> 00:41:54.880
<v Speaker 3>like why build our successors?</v>
<v Speaker 3>Like why,</v>

759
00:41:54.940 --> 00:41:56.410
<v Speaker 3>why do that?</v>
<v Speaker 3>Like what is,</v>

760
00:41:56.740 --> 00:42:00.490
<v Speaker 3>what is it about us that makes us want </v>
<v Speaker 3>to constantly make bigger,</v>

761
00:42:00.491 --> 00:42:01.324
<v Speaker 3>better things?</v>

762
00:42:02.600 --> 00:42:03.433
<v Speaker 2>Well,</v>
<v Speaker 2>that's an interesting question in the </v>

763
00:42:06.141 --> 00:42:06.974
<v Speaker 2>history of biology,</v>
<v Speaker 2>which I may not be the most qualified </v>

764
00:42:12.441 --> 00:42:13.274
<v Speaker 2>person to answer.</v>
<v Speaker 2>It is an interesting question and I </v>

765
00:42:16.401 --> 00:42:17.234
<v Speaker 2>think it has something to do with the </v>
<v Speaker 2>weird way in which we embody various </v>

766
00:42:21.711 --> 00:42:24.560
<v Speaker 2>contradictions that we're always trying </v>
<v Speaker 2>to resolve locally.</v>

767
00:42:25.070 --> 00:42:28.130
<v Speaker 2>You mentioned ents and answer social </v>
<v Speaker 2>animals,</v>

768
00:42:28.131 --> 00:42:28.964
<v Speaker 2>right?</v>
<v Speaker 2>Worse.</v>

769
00:42:29.150 --> 00:42:32.970
<v Speaker 2>Like cats are very individual.</v>
<v Speaker 2>We're like trapped between the two,</v>

770
00:42:33.200 --> 00:42:37.040
<v Speaker 2>like we're somewhat individual and </v>
<v Speaker 2>somewhat social.</v>

771
00:42:37.160 --> 00:42:40.340
<v Speaker 2>And then.</v>
<v Speaker 2>And then since we created civilization,</v>

772
00:42:40.680 --> 00:42:41.391
<v Speaker 2>it's,</v>
<v Speaker 2>it's,</v>

773
00:42:41.391 --> 00:42:45.530
<v Speaker 2>it's even worse because we have certain </v>
<v Speaker 2>aspects which are,</v>

774
00:42:45.980 --> 00:42:49.700
<v Speaker 2>which are wanting to conform with the </v>
<v Speaker 2>group and the tribe and others which are</v>

775
00:42:49.701 --> 00:42:52.640
<v Speaker 2>wanting to innovate and break out of </v>
<v Speaker 2>the.</v>

776
00:42:52.641 --> 00:42:53.474
<v Speaker 2>And we're sort of trapped in these </v>
<v Speaker 2>biological and cultural contradictions </v>

777
00:42:56.871 --> 00:42:57.704
<v Speaker 2>which tend to drive innovation.</v>
<v Speaker 2>But I think there's a lot there that no </v>

778
00:43:01.011 --> 00:43:05.420
<v Speaker 2>one understands in the roots of the </v>
<v Speaker 2>human psyche evolutionarily.</v>

779
00:43:05.421 --> 00:43:09.160
<v Speaker 2>But as an empirical fact,</v>
<v Speaker 2>what you said is,</v>

780
00:43:09.460 --> 00:43:10.600
<v Speaker 2>is,</v>
<v Speaker 2>is,</v>

781
00:43:10.620 --> 00:43:11.660
<v Speaker 2>is very true,</v>
<v Speaker 2>right?</v>

782
00:43:11.661 --> 00:43:14.390
<v Speaker 2>Like we,</v>
<v Speaker 2>we're driven to seek novelty.</v>

783
00:43:14.600 --> 00:43:15.433
<v Speaker 2>We're driven to create new things.</v>
<v Speaker 2>And this is probably one of the factors </v>

784
00:43:20.661 --> 00:43:21.494
<v Speaker 2>which is driving the creation of Ai.</v>
<v Speaker 2>I don't think that alone would make the </v>

785
00:43:26.091 --> 00:43:28.750
<v Speaker 2>creation of ai inevitable,</v>
<v Speaker 2>but the thing,</v>

786
00:43:29.660 --> 00:43:32.450
<v Speaker 3>why don't you think it would make it </v>
<v Speaker 3>inevitable if we consistently innovate?</v>

787
00:43:32.750 --> 00:43:34.430
<v Speaker 3>And it's always been a concept.</v>
<v Speaker 3>I mean,</v>

788
00:43:34.431 --> 00:43:37.010
<v Speaker 3>you were talking about the concept </v>
<v Speaker 3>existing 30 plus years ago.</v>

789
00:43:37.720 --> 00:43:38.553
<v Speaker 2>Well,</v>
<v Speaker 2>I think a key point is that there's </v>

790
00:43:40.000 --> 00:43:40.833
<v Speaker 2>tremendous practical economic advantage </v>
<v Speaker 2>and status advantage to be gotten from </v>

791
00:43:48.460 --> 00:43:49.293
<v Speaker 2>ai right now.</v>
<v Speaker 2>And this is driving the advancement of </v>

792
00:43:52.781 --> 00:43:56.920
<v Speaker 2>ai to be incredibly rapid,</v>
<v Speaker 2>right?</v>

793
00:43:56.940 --> 00:44:01.940
<v Speaker 2>Because there are some things that are </v>
<v Speaker 2>interesting and would use a lot of human</v>

794
00:44:02.891 --> 00:44:05.710
<v Speaker 2>innovation,</v>
<v Speaker 2>but they get very few resources.</v>

795
00:44:05.711 --> 00:44:06.770
<v Speaker 2>So for example,</v>
<v Speaker 2>my,</v>

796
00:44:07.060 --> 00:44:10.510
<v Speaker 2>my oldest son's Arthur Oostra,</v>
<v Speaker 2>he's doing his phd now.</v>

797
00:44:10.560 --> 00:44:15.060
<v Speaker 2>What is this Zarathustra?</v>
<v Speaker 2>My kids are Pfister,</v>

798
00:44:15.060 --> 00:44:15.591
<v Speaker 2>Amadeus,</v>
<v Speaker 2>Debbie lawn,</v>

799
00:44:15.591 --> 00:44:20.591
<v Speaker 2>ulysses Shaharazad.</v>
<v Speaker 2>And then a new one is corky qrs,</v>

800
00:44:22.000 --> 00:44:22.833
<v Speaker 2>which is an acronym for quantum </v>
<v Speaker 2>organized rational expanding </v>

801
00:44:25.031 --> 00:44:28.690
<v Speaker 2>intelligence.</v>
<v Speaker 2>So I was never happy with Ben.</v>

802
00:44:28.691 --> 00:44:30.930
<v Speaker 2>It's a very boring name.</v>
<v Speaker 2>So yeah.</v>

803
00:44:31.120 --> 00:44:31.953
<v Speaker 2>Yeah.</v>
<v Speaker 2>I had,</v>

804
00:44:32.410 --> 00:44:33.243
<v Speaker 2>I had,</v>
<v Speaker 2>I had to do something more interesting </v>

805
00:44:34.470 --> 00:44:35.460
<v Speaker 2>with my kids.</v>
<v Speaker 2>Anyway,</v>

806
00:44:35.820 --> 00:44:36.653
<v Speaker 2>Zarathustra is doing his phd on the </v>
<v Speaker 2>application of machine learning to </v>

807
00:44:40.771 --> 00:44:45.120
<v Speaker 2>automated theorem proving basically make</v>
<v Speaker 2>ais that can do mathematics better.</v>

808
00:44:45.480 --> 00:44:49.800
<v Speaker 2>And to me that's the most important </v>
<v Speaker 2>thing we could be applying ai to because</v>

809
00:44:49.940 --> 00:44:50.773
<v Speaker 2>you know,</v>
<v Speaker 2>mathematics is the key to all modern </v>

810
00:44:51.991 --> 00:44:54.660
<v Speaker 2>science and engineering.</v>
<v Speaker 2>My Phd was in math originally,</v>

811
00:44:54.930 --> 00:44:55.763
<v Speaker 2>but the amount of resources going into </v>
<v Speaker 2>ai for automating mathematics is not </v>

812
00:45:00.691 --> 00:45:01.524
<v Speaker 2>large at this present moment,</v>
<v Speaker 2>although that's a beautiful and amazing </v>

813
00:45:05.161 --> 00:45:07.830
<v Speaker 2>area for invention and innovation and </v>
<v Speaker 2>creativity.</v>

814
00:45:07.980 --> 00:45:12.980
<v Speaker 2>So I think what's driving our rapid </v>
<v Speaker 2>pushed or dozing ai,</v>

815
00:45:13.320 --> 00:45:14.153
<v Speaker 2>I mean it's not just our creative drive,</v>
<v Speaker 2>it's the fact that there's tremendous </v>

816
00:45:18.420 --> 00:45:21.990
<v Speaker 2>economic value,</v>
<v Speaker 2>military value and human value.</v>

817
00:45:21.991 --> 00:45:24.150
<v Speaker 2>I mean curing diseases,</v>
<v Speaker 2>teaching kids,</v>

818
00:45:24.510 --> 00:45:25.343
<v Speaker 2>there's tremendous value in almost </v>
<v Speaker 2>everything that's important to human </v>

819
00:45:27.721 --> 00:45:29.850
<v Speaker 2>beings in,</v>
<v Speaker 2>in building ai,</v>

820
00:45:29.851 --> 00:45:30.684
<v Speaker 2>right?</v>
<v Speaker 2>So you put that together with our drive </v>

821
00:45:32.701 --> 00:45:36.690
<v Speaker 2>to create an interface and this becomes </v>
<v Speaker 2>an almost unstoppable force within,</v>

822
00:45:36.980 --> 00:45:40.410
<v Speaker 2>within human society.</v>
<v Speaker 2>And what we've seen in the last,</v>

823
00:45:40.770 --> 00:45:43.620
<v Speaker 2>you know,</v>
<v Speaker 2>three to five years is suddenly,</v>

824
00:45:43.830 --> 00:45:44.663
<v Speaker 2>you know,</v>
<v Speaker 2>national leaders and Titans of industry </v>

825
00:45:46.710 --> 00:45:48.750
<v Speaker 2>and even like pop stars,</v>
<v Speaker 2>right?</v>

826
00:45:49.080 --> 00:45:52.350
<v Speaker 2>They've woken up to the concept that </v>
<v Speaker 2>wow,</v>

827
00:45:52.410 --> 00:45:53.243
<v Speaker 2>smarter and smarter ai is real and this </v>
<v Speaker 2>is going to get better and better like </v>

828
00:45:56.550 --> 00:46:01.140
<v Speaker 2>within years to decades,</v>
<v Speaker 2>not centuries to millennia.</v>

829
00:46:01.141 --> 00:46:06.141
<v Speaker 2>So now the cat's out of the bag,</v>
<v Speaker 2>nobody's going to put it back.</v>

830
00:46:06.420 --> 00:46:08.220
<v Speaker 2>And it's about,</v>
<v Speaker 2>you know,</v>

831
00:46:08.221 --> 00:46:13.221
<v Speaker 2>how can we direct it in the most </v>
<v Speaker 2>beneficial possible way?</v>

832
00:46:13.411 --> 00:46:15.600
<v Speaker 2>And as you say,</v>
<v Speaker 2>it doesn't have to be just one possible.</v>

833
00:46:15.601 --> 00:46:16.434
<v Speaker 2>We were like,</v>
<v Speaker 2>what will I look forward to personally </v>

834
00:46:18.780 --> 00:46:22.800
<v Speaker 2>is bifurcating myself into an array of </v>
<v Speaker 2>possible Benton's like it.</v>

835
00:46:23.190 --> 00:46:28.190
<v Speaker 2>I'd like to get one copy of me fuse </v>
<v Speaker 2>itself with a superhuman ai mind and you</v>

836
00:46:28.311 --> 00:46:29.220
<v Speaker 2>know,</v>
<v Speaker 2>become,</v>

837
00:46:29.250 --> 00:46:30.001
<v Speaker 2>become,</v>
<v Speaker 2>uh,</v>

838
00:46:30.001 --> 00:46:33.480
<v Speaker 2>a god or something beyond the God.</v>
<v Speaker 2>You wouldn't even be myself.</v>

839
00:46:33.510 --> 00:46:35.820
<v Speaker 2>God,</v>
<v Speaker 2>I wouldn't even be myself anymore.</v>

840
00:46:35.821 --> 00:46:36.331
<v Speaker 2>Right.</v>
<v Speaker 2>I mean,</v>

841
00:46:36.331 --> 00:46:40.290
<v Speaker 2>you would lose all concepts of human </v>
<v Speaker 2>self and identity,</v>

842
00:46:40.291 --> 00:46:42.540
<v Speaker 2>but it would be the point of even </v>
<v Speaker 2>holding any of it,</v>

843
00:46:42.710 --> 00:46:43.940
<v Speaker 2>you know,</v>
<v Speaker 2>that's,</v>

844
00:46:44.050 --> 00:46:45.030
<v Speaker 2>that's for the future.</v>

845
00:46:45.210 --> 00:46:47.300
<v Speaker 2>That's for the Mega Ben to decide,</v>
<v Speaker 2>right?</v>

846
00:46:47.750 --> 00:46:48.180
<v Speaker 2>Yeah.</v>
<v Speaker 2>Yeah.</v>

847
00:46:48.180 --> 00:46:49.013
<v Speaker 2>On the other hand,</v>
<v Speaker 2>I'd like to let me remained in human </v>

848
00:46:51.391 --> 00:46:52.560
<v Speaker 2>form,</v>
<v Speaker 2>you know,</v>

849
00:46:52.561 --> 00:46:52.950
<v Speaker 2>get,</v>
<v Speaker 2>get,</v>

850
00:46:52.950 --> 00:46:53.783
<v Speaker 2>get rid of a death and disease and the </v>
<v Speaker 2>psychological issues and just live live </v>

851
00:46:59.761 --> 00:47:01.420
<v Speaker 2>happily forever,</v>
<v Speaker 2>you know,</v>

852
00:47:01.430 --> 00:47:02.263
<v Speaker 2>in,</v>
<v Speaker 2>in the peoples who watched over by the </v>

853
00:47:03.241 --> 00:47:04.740
<v Speaker 2>machines of loving grace.</v>
<v Speaker 2>Right.</v>

854
00:47:04.741 --> 00:47:05.270
<v Speaker 2>So,</v>
<v Speaker 2>I mean,</v>

855
00:47:05.270 --> 00:47:06.103
<v Speaker 2>you can have,</v>
<v Speaker 2>it doesn't have to be either or because </v>

856
00:47:08.350 --> 00:47:09.183
<v Speaker 2>once,</v>
<v Speaker 2>once you can scan your brain and body </v>

857
00:47:11.521 --> 00:47:13.560
<v Speaker 2>and three d print new copies of </v>
<v Speaker 2>yourself,</v>

858
00:47:13.770 --> 00:47:17.040
<v Speaker 2>you could have multiple of.</v>
<v Speaker 2>I mean,</v>

859
00:47:17.060 --> 00:47:21.210
<v Speaker 2>that's there's a lot of mass energy in </v>
<v Speaker 2>the universe and the universe,</v>

860
00:47:21.211 --> 00:47:24.030
<v Speaker 2>so that's assuming that we can escape </v>
<v Speaker 2>this planet because of you.</v>

861
00:47:24.060 --> 00:47:27.180
<v Speaker 2>If you're talking about if you're going </v>
<v Speaker 2>to make themselves,</v>

862
00:47:27.350 --> 00:47:30.010
<v Speaker 2>how can you live in a world with a </v>
<v Speaker 2>billion donald trump's</v>

863
00:47:30.280 --> 00:47:33.050
<v Speaker 1>because literally that's what we're </v>
<v Speaker 1>talking about,</v>

864
00:47:33.100 --> 00:47:33.933
<v Speaker 1>talking about people being able to </v>
<v Speaker 1>reproduce themselves and just having </v>

865
00:47:38.441 --> 00:47:43.000
<v Speaker 1>this idea that they would like their ego</v>
<v Speaker 1>to exist in multiple different forms.</v>

866
00:47:43.030 --> 00:47:43.863
<v Speaker 1>Whether it's some super symbiotic form </v>
<v Speaker 1>that's connected to artificial </v>

867
00:47:47.171 --> 00:47:48.004
<v Speaker 1>intelligence or some biological form </v>
<v Speaker 1>that's immortal or some other form that </v>

868
00:47:51.610 --> 00:47:52.443
<v Speaker 1>stands just as a normal human being.</v>
<v Speaker 1>As we know in 2018 have you have </v>

869
00:47:56.801 --> 00:47:59.440
<v Speaker 1>multiple versions of yourself over and </v>
<v Speaker 1>over and over again like that.</v>

870
00:47:59.441 --> 00:48:00.274
<v Speaker 1>That's where you're.</v>
<v Speaker 1>So once you get to the point where you </v>

871
00:48:02.501 --> 00:48:03.334
<v Speaker 1>have a super human general intelligence </v>
<v Speaker 1>that can do things like fully scanned </v>

872
00:48:07.121 --> 00:48:10.600
<v Speaker 1>the human brain and body and three d </v>
<v Speaker 1>print more of them.</v>

873
00:48:10.990 --> 00:48:11.823
<v Speaker 1>By that point,</v>
<v Speaker 1>you're at a level where scarcity of </v>

874
00:48:16.510 --> 00:48:21.340
<v Speaker 1>material resources is not an issue at </v>
<v Speaker 1>the human scale of doing things.</v>

875
00:48:21.341 --> 00:48:23.720
<v Speaker 1>So I actually have human resources in </v>
<v Speaker 1>terms of what</v>

876
00:48:24.640 --> 00:48:25.473
<v Speaker 2>mass energy,</v>
<v Speaker 2>scared scarcity of molecules to print </v>

877
00:48:27.671 --> 00:48:29.810
<v Speaker 2>more copies of yourself.</v>
<v Speaker 2>I think that,</v>

878
00:48:30.020 --> 00:48:30.853
<v Speaker 2>I think that's not going to be the issue</v>
<v Speaker 2>at that point when people are worried </v>

879
00:48:33.521 --> 00:48:34.354
<v Speaker 2>about is environmental concerns of </v>
<v Speaker 2>overpopulation is people are worried </v>

880
00:48:36.491 --> 00:48:38.680
<v Speaker 2>about what they see in front of their </v>
<v Speaker 2>faces right now,</v>

881
00:48:38.681 --> 00:48:39.514
<v Speaker 2>but people are not.</v>
<v Speaker 2>Most people are not thinking deeply </v>

882
00:48:45.491 --> 00:48:46.324
<v Speaker 2>enough about what potential would be </v>
<v Speaker 2>there once you had super human ai is </v>

883
00:48:52.120 --> 00:48:56.320
<v Speaker 2>doing the calculation manufacturing and </v>
<v Speaker 2>the thinking.</v>

884
00:48:56.321 --> 00:48:57.450
<v Speaker 2>I mean,</v>
<v Speaker 2>I mean the.</v>

885
00:48:57.451 --> 00:49:00.550
<v Speaker 2>The amount of energy in the single grain</v>
<v Speaker 2>of sand,</v>

886
00:49:00.880 --> 00:49:04.480
<v Speaker 2>if you had an ai able to to </v>
<v Speaker 2>appropriately leverage that.</v>

887
00:49:04.481 --> 00:49:08.350
<v Speaker 2>That energy is,</v>
<v Speaker 2>is tremendously more than than most than</v>

888
00:49:08.351 --> 00:49:09.184
<v Speaker 2>most people think.</v>
<v Speaker 2>And the amount of computing power in a </v>

889
00:49:11.321 --> 00:49:14.170
<v Speaker 2>grain of sand.</v>
<v Speaker 2>It's like a quadrillion times.</v>

890
00:49:14.171 --> 00:49:17.020
<v Speaker 2>All the people on Earth put together.</v>
<v Speaker 2>So I mean,</v>

891
00:49:17.021 --> 00:49:19.990
<v Speaker 2>what do you mean by that amount of </v>
<v Speaker 2>computing power?</v>

892
00:49:20.800 --> 00:49:21.633
<v Speaker 2>There's.</v>
<v Speaker 2>Well the amount of computing power that </v>

893
00:49:23.711 --> 00:49:24.544
<v Speaker 2>could be achieved by reorganizing the </v>
<v Speaker 2>elementary particles in the grain of </v>

894
00:49:29.291 --> 00:49:29.681
<v Speaker 2>sand.</v>
<v Speaker 2>Yeah,</v>

895
00:49:29.681 --> 00:49:30.370
<v Speaker 2>there,</v>
<v Speaker 2>there,</v>

896
00:49:30.370 --> 00:49:31.203
<v Speaker 2>there's,</v>
<v Speaker 2>there's a number in physics called the </v>

897
00:49:32.201 --> 00:49:33.034
<v Speaker 2>beacon stone bound,</v>
<v Speaker 2>which is the maximum amount of </v>

898
00:49:35.171 --> 00:49:36.004
<v Speaker 2>information that can be,</v>
<v Speaker 2>can be stored in a certain amount of </v>

899
00:49:39.071 --> 00:49:39.904
<v Speaker 2>mass energy here,</v>
<v Speaker 2>so that that if the laws of physics as </v>

900
00:49:42.401 --> 00:49:43.234
<v Speaker 2>we know them now are correct,</v>
<v Speaker 2>which they certainly aren't than the </v>

901
00:49:45.360 --> 00:49:46.193
<v Speaker 2>[inaudible].</v>
<v Speaker 2>That would be the amount of computing </v>

902
00:49:47.920 --> 00:49:49.900
<v Speaker 2>you can do in a certain amount of mass </v>
<v Speaker 2>and energy.</v>

903
00:49:50.110 --> 00:49:52.540
<v Speaker 2>We're very,</v>
<v Speaker 2>very far from that limit right now.</v>

904
00:49:52.541 --> 00:49:53.374
<v Speaker 2>Right,</v>
<v Speaker 2>so I mean,</v>

905
00:49:53.490 --> 00:49:57.280
<v Speaker 2>my point is once you have something a </v>
<v Speaker 2>thousand times smarter than people,</v>

906
00:49:57.550 --> 00:50:01.800
<v Speaker 2>what we imagined to be the limits now </v>
<v Speaker 2>doesn't matter</v>

907
00:50:01.990 --> 00:50:02.823
<v Speaker 1>too.</v>
<v Speaker 1>The issues that we're dealing with in </v>

908
00:50:04.471 --> 00:50:05.304
<v Speaker 1>terms of environmental concerns,</v>
<v Speaker 1>that could all potentially be almost </v>

909
00:50:08.011 --> 00:50:10.050
<v Speaker 1>certainly going to be irrelevant,</v>
<v Speaker 1>irrelevant.</v>

910
00:50:10.190 --> 00:50:14.340
<v Speaker 1>There may be others problem issues that </v>
<v Speaker 1>we can't even conceive at this moment,</v>

911
00:50:14.850 --> 00:50:15.683
<v Speaker 1>but the intelligence will be so vastly </v>
<v Speaker 1>superior to what we have currently that </v>

912
00:50:19.411 --> 00:50:20.244
<v Speaker 1>they'll be able to find solutions to </v>
<v Speaker 1>virtually every single problem with </v>

913
00:50:22.810 --> 00:50:24.900
<v Speaker 1>Shima,</v>
<v Speaker 1>a ocean,</v>

914
00:50:24.901 --> 00:50:26.750
<v Speaker 1>fish,</v>
<v Speaker 1>deep population.</v>

915
00:50:26.790 --> 00:50:28.430
<v Speaker 1>All that stuff will just arrangements</v>

916
00:50:28.430 --> 00:50:33.210
<v Speaker 3>of molecules freaking me out.</v>
<v Speaker 3>But to hear that though,</v>

917
00:50:33.260 --> 00:50:34.580
<v Speaker 3>environmental people don't want to hear </v>
<v Speaker 3>that.</v>

918
00:50:34.581 --> 00:50:35.580
<v Speaker 3>Well,</v>
<v Speaker 3>I mean I</v>

919
00:50:35.980 --> 00:50:40.980
<v Speaker 2>also on the everyday life basis until we</v>
<v Speaker 2>have these super ais,</v>

920
00:50:42.850 --> 00:50:43.683
<v Speaker 2>I don't like the garbage,</v>
<v Speaker 2>Washington from the beach near my house </v>

921
00:50:46.121 --> 00:50:46.780
<v Speaker 2>either.</v>
<v Speaker 2>Right?</v>

922
00:50:46.780 --> 00:50:50.190
<v Speaker 2>So I mean,</v>
<v Speaker 2>but on an everyday basis,</v>

923
00:50:50.300 --> 00:50:53.620
<v Speaker 2>of course we wanted to promote health in</v>
<v Speaker 2>our bodies and in our,</v>

924
00:50:53.820 --> 00:50:54.653
<v Speaker 2>in our environments right now,</v>
<v Speaker 2>as long as there's no measurable </v>

925
00:50:58.361 --> 00:51:02.920
<v Speaker 2>uncertainty regarding when the </v>
<v Speaker 2>Benevolent Super Ais will will,</v>

926
00:51:02.921 --> 00:51:04.780
<v Speaker 2>will,</v>
<v Speaker 2>will come about still,</v>

927
00:51:04.781 --> 00:51:05.614
<v Speaker 2>I think the main question isn't whether </v>
<v Speaker 2>once you have beneficially disposed </v>

928
00:51:10.061 --> 00:51:10.894
<v Speaker 2>super ai,</v>
<v Speaker 2>it could solve all our current petty </v>

929
00:51:13.061 --> 00:51:13.894
<v Speaker 2>little problems.</v>
<v Speaker 2>The question is can we wade through the </v>

930
00:51:17.561 --> 00:51:18.394
<v Speaker 2>mock of modern human society and </v>
<v Speaker 2>psychology to create this beneficial </v>

931
00:51:22.871 --> 00:51:24.850
<v Speaker 2>super ai in,</v>
<v Speaker 2>in,</v>

932
00:51:24.851 --> 00:51:25.684
<v Speaker 2>in the first place I got,</v>
<v Speaker 2>I believe I know how to create a </v>

933
00:51:28.061 --> 00:51:30.610
<v Speaker 2>beneficial super ai,</v>
<v Speaker 2>but it's a lot of,</v>

934
00:51:30.670 --> 00:51:31.503
<v Speaker 2>a lot of work to get there and of course</v>
<v Speaker 2>there's many teams around the world </v>

935
00:51:35.680 --> 00:51:36.513
<v Speaker 2>working on vaguely similar projects now.</v>
<v Speaker 2>It's not obvious what kind of super ai </v>

936
00:51:43.570 --> 00:51:45.700
<v Speaker 2>we're actually going to get once we get </v>
<v Speaker 2>there.</v>

937
00:51:45.970 --> 00:51:48.370
<v Speaker 2>Yeah,</v>
<v Speaker 2>it's all just guesses at this point,</v>

938
00:51:48.371 --> 00:51:49.204
<v Speaker 2>right?</v>
<v Speaker 2>It's more less educated guesses </v>

939
00:51:51.281 --> 00:51:52.740
<v Speaker 2>depending on who's doing the guessing.</v>

940
00:51:52.890 --> 00:51:53.723
<v Speaker 3>You say that it's almost like we're in a</v>
<v Speaker 3>race of the the primitive primate </v>

941
00:51:57.551 --> 00:51:58.384
<v Speaker 3>biology versus the potentially </v>
<v Speaker 3>beneficial and benevolent artificial </v>

942
00:52:02.951 --> 00:52:03.784
<v Speaker 3>intelligence that this.</v>
<v Speaker 3>The best aspects of this primate can </v>

943
00:52:06.371 --> 00:52:09.610
<v Speaker 3>create that it's almost a race to get </v>
<v Speaker 3>who's going to win.</v>

944
00:52:09.611 --> 00:52:13.960
<v Speaker 3>Is it the warmongers and the greedy </v>
<v Speaker 3>whores that are smashing the world under</v>

945
00:52:13.961 --> 00:52:14.794
<v Speaker 3>its boots or is it the scientists that </v>
<v Speaker 3>are going to figure out some super </v>

946
00:52:18.401 --> 00:52:20.570
<v Speaker 3>intelligent way to solve all of our </v>
<v Speaker 3>problems?</v>

947
00:52:20.600 --> 00:52:22.040
<v Speaker 3>Let's look at it more as a</v>

948
00:52:22.370 --> 00:52:27.370
<v Speaker 2>look at it more as a,</v>
<v Speaker 2>as a struggle between different modes of</v>

949
00:52:27.980 --> 00:52:31.910
<v Speaker 2>social organization than individual </v>
<v Speaker 2>people.</v>

950
00:52:31.911 --> 00:52:32.744
<v Speaker 2>I mean,</v>
<v Speaker 2>look what when I worked in DC with </v>

951
00:52:34.761 --> 00:52:35.594
<v Speaker 2>intelligence agencies,</v>
<v Speaker 2>most of the people I met there were </v>

952
00:52:40.250 --> 00:52:44.630
<v Speaker 2>really nice human beings who believed </v>
<v Speaker 2>they were doing the best for the world.</v>

953
00:52:45.140 --> 00:52:46.910
<v Speaker 2>Even if some of the things that we're </v>
<v Speaker 2>doing,</v>

954
00:52:46.911 --> 00:52:50.990
<v Speaker 2>like I thought were very much not for </v>
<v Speaker 2>the best of the world.</v>

955
00:52:51.190 --> 00:52:52.023
<v Speaker 2>Soon I'm in military mode of </v>
<v Speaker 2>organization or large corporations as a </v>

956
00:52:57.711 --> 00:52:58.544
<v Speaker 2>mode of organization are in my view,</v>
<v Speaker 2>not journaling can lead to beneficial </v>

957
00:53:04.251 --> 00:53:05.084
<v Speaker 2>outcomes for the overall species and and</v>
<v Speaker 2>for the global brain and the scientific </v>

958
00:53:09.411 --> 00:53:10.244
<v Speaker 2>community.</v>
<v Speaker 2>The open source community I think are </v>

959
00:53:12.501 --> 00:53:15.270
<v Speaker 2>better modes of organization and you </v>
<v Speaker 2>know the,</v>

960
00:53:15.530 --> 00:53:16.363
<v Speaker 2>the better aspects of the blockchain and</v>
<v Speaker 2>crypto community have a better mode of </v>

961
00:53:20.301 --> 00:53:22.040
<v Speaker 2>organization.</v>
<v Speaker 2>So I think if,</v>

962
00:53:22.370 --> 00:53:23.203
<v Speaker 2>if this sort of open decentralized of </v>
<v Speaker 2>organization can marshal more resources </v>

963
00:53:30.241 --> 00:53:35.241
<v Speaker 2>as opposed to this centralized,</v>
<v Speaker 2>authoritarian mode of organization,</v>

964
00:53:35.580 --> 00:53:36.413
<v Speaker 2>then I think things are going to come </v>
<v Speaker 2>out for the better and it's not so much </v>

965
00:53:39.631 --> 00:53:41.650
<v Speaker 2>about bad people versus good people.</v>

966
00:53:41.730 --> 00:53:44.690
<v Speaker 2>You can look at like the corporate mode </v>
<v Speaker 2>of organization.</v>

967
00:53:44.710 --> 00:53:45.543
<v Speaker 2>It's almost a virus that that's </v>
<v Speaker 2>colonized a bunch of humanity and is </v>

968
00:53:48.990 --> 00:53:51.250
<v Speaker 2>sucking people into working according to</v>
<v Speaker 2>the,</v>

969
00:53:51.360 --> 00:53:52.193
<v Speaker 2>to this mode and even if they're really </v>
<v Speaker 2>good people and the individual task </v>

970
00:53:56.340 --> 00:54:00.620
<v Speaker 2>they're working on isn't bad in itself.</v>
<v Speaker 2>They're working within this,</v>

971
00:54:01.160 --> 00:54:06.160
<v Speaker 2>this mode that's leading their work to </v>
<v Speaker 2>be used for ultimately a non good end.</v>

972
00:54:06.810 --> 00:54:07.643
<v Speaker 2>Yeah.</v>
<v Speaker 2>That is a fascinating thing about </v>

973
00:54:08.191 --> 00:54:09.210
<v Speaker 2>corporations,</v>
<v Speaker 2>isn't it?</v>

974
00:54:09.510 --> 00:54:10.343
<v Speaker 2>That the diffusion of responsibility and</v>
<v Speaker 2>being a part of a gigantic group that </v>

975
00:54:14.341 --> 00:54:17.460
<v Speaker 2>you as an individual don't feel </v>
<v Speaker 2>necessarily connected,</v>

976
00:54:17.461 --> 00:54:21.190
<v Speaker 2>are responsible to the.</v>
<v Speaker 2>Even the CEO isn't fully responsible.</v>

977
00:54:21.191 --> 00:54:26.070
<v Speaker 2>Like if the CEO does something that </v>
<v Speaker 2>isn't in accordance with the high higher</v>

978
00:54:26.071 --> 00:54:28.200
<v Speaker 2>goals of the organization,</v>
<v Speaker 2>they're just replaced.</v>

979
00:54:28.201 --> 00:54:31.380
<v Speaker 2>Right.</v>
<v Speaker 2>So I mean there's no one person who's in</v>

980
00:54:31.381 --> 00:54:32.550
<v Speaker 2>charge.</v>
<v Speaker 2>It's really like,</v>

981
00:54:32.600 --> 00:54:33.570
<v Speaker 2>it's like an that column.</v>

982
00:54:34.320 --> 00:54:35.153
<v Speaker 2>It's like its own organism and I mean </v>
<v Speaker 2>it's us who have let these organisms </v>

983
00:54:41.430 --> 00:54:44.940
<v Speaker 2>become parasites on humanity in this </v>
<v Speaker 2>way.</v>

984
00:54:45.360 --> 00:54:46.193
<v Speaker 2>In some ways the Asian countries are a </v>
<v Speaker 2>little more intelligent than western </v>

985
00:54:50.131 --> 00:54:50.964
<v Speaker 2>countries and the Asian governments </v>
<v Speaker 2>realize the power of corporations to </v>

986
00:54:56.971 --> 00:54:57.804
<v Speaker 2>mold society and there's a bit more </v>
<v Speaker 2>feedback between the government and </v>

987
00:55:02.440 --> 00:55:05.460
<v Speaker 2>corporations which can be for better or </v>
<v Speaker 2>for worse,</v>

988
00:55:05.461 --> 00:55:06.294
<v Speaker 2>but then in in America there's some </v>
<v Speaker 2>ethos of like free markets and free </v>

989
00:55:12.361 --> 00:55:13.194
<v Speaker 2>enterprise,</v>
<v Speaker 2>which is really not taking into account </v>

990
00:55:16.560 --> 00:55:18.900
<v Speaker 2>the oligopolistic nature of,</v>
<v Speaker 2>of,</v>

991
00:55:18.901 --> 00:55:20.520
<v Speaker 2>of,</v>
<v Speaker 2>of modern markets,</v>

992
00:55:20.550 --> 00:55:21.383
<v Speaker 2>but in Asian countries isn't it that the</v>
<v Speaker 2>government is actually suppressing </v>

993
00:55:24.451 --> 00:55:25.284
<v Speaker 2>inflammation as well.</v>
<v Speaker 2>They're also suppressing Google and </v>

994
00:55:27.561 --> 00:55:28.770
<v Speaker 2>South Korea?</v>
<v Speaker 2>No,</v>

995
00:55:28.800 --> 00:55:31.560
<v Speaker 2>I'm in South Korea.</v>
<v Speaker 2>If you look at my only ones,</v>

996
00:55:32.190 --> 00:55:33.023
<v Speaker 2>well Singapore,</v>
<v Speaker 2>I'm in Singapore is ruthless in their </v>

997
00:55:36.151 --> 00:55:41.151
<v Speaker 2>drug laws and some of the archaic,</v>
<v Speaker 2>well us far worse though,</v>

998
00:55:41.400 --> 00:55:42.233
<v Speaker 2>Singapore,</v>
<v Speaker 2>it gives you the death penalty for </v>

999
00:55:43.111 --> 00:55:43.944
<v Speaker 2>marijuana.</v>

1000
00:55:43.980 --> 00:55:44.813
<v Speaker 2>They do it.</v>
<v Speaker 2>South Korea is an example which has </v>

1001
00:55:49.591 --> 00:55:50.424
<v Speaker 2>roughly the same level of personal </v>
<v Speaker 2>freedoms as the US more in some ways </v>

1002
00:55:54.301 --> 00:55:56.940
<v Speaker 2>less than others.</v>
<v Speaker 2>Massive electronic and innovation.</v>

1003
00:55:57.330 --> 00:55:58.163
<v Speaker 2>Well,</v>
<v Speaker 2>interesting thing they're politically </v>

1004
00:56:01.201 --> 00:56:02.034
<v Speaker 2>is.</v>
<v Speaker 2>I mean they were poorer than two thirds </v>

1005
00:56:03.451 --> 00:56:04.284
<v Speaker 2>of sub Saharan African nations in the </v>
<v Speaker 2>late sixties and it is through the </v>

1006
00:56:07.741 --> 00:56:08.574
<v Speaker 2>government intentionally stimulating </v>
<v Speaker 2>corporate development toward </v>

1007
00:56:11.821 --> 00:56:15.360
<v Speaker 2>manufacturing in electronics that they </v>
<v Speaker 2>grew up so that,</v>

1008
00:56:15.660 --> 00:56:18.120
<v Speaker 2>that,</v>
<v Speaker 2>that now I'm in,</v>

1009
00:56:18.780 --> 00:56:23.250
<v Speaker 2>I'm not holding that up as a paragon for</v>
<v Speaker 2>the future or anything,</v>

1010
00:56:23.260 --> 00:56:24.093
<v Speaker 2>but it does show that there's,</v>
<v Speaker 2>there's many modes of organization of </v>

1011
00:56:28.151 --> 00:56:33.070
<v Speaker 2>people and resources other than the ones</v>
<v Speaker 2>that we take for granted in the US.</v>

1012
00:56:33.400 --> 00:56:37.180
<v Speaker 2>I don't think Samsung and lg are the </v>
<v Speaker 2>ideal for the future either though.</v>

1013
00:56:37.181 --> 00:56:40.270
<v Speaker 2>I mean I'm much more interested in,</v>
<v Speaker 2>you know,</v>

1014
00:56:41.270 --> 00:56:43.330
<v Speaker 2>your son in blockchain.</v>
<v Speaker 2>I'm interested in.</v>

1015
00:56:43.680 --> 00:56:45.390
<v Speaker 2>I'm interested in open source.</v>

1016
00:56:45.460 --> 00:56:47.620
<v Speaker 2>I'm interested in,</v>
<v Speaker 2>in blockchain.</v>

1017
00:56:47.621 --> 00:56:50.350
<v Speaker 2>I'm basically,</v>
<v Speaker 2>I'm interested in anything that's,</v>

1018
00:56:50.780 --> 00:56:55.390
<v Speaker 2>you know,</v>
<v Speaker 2>open and participatory and disruptive as</v>

1019
00:56:55.391 --> 00:56:56.224
<v Speaker 2>well because I think that's,</v>
<v Speaker 2>I think that is the way to be ongoingly </v>

1020
00:57:01.450 --> 00:57:04.630
<v Speaker 2>just disruptive and open source is a </v>
<v Speaker 2>good example of that.</v>

1021
00:57:04.631 --> 00:57:07.000
<v Speaker 2>Like when the open source movement </v>
<v Speaker 2>started,</v>

1022
00:57:07.030 --> 00:57:08.920
<v Speaker 2>they weren't thinking about machine </v>
<v Speaker 2>learning,</v>

1023
00:57:09.250 --> 00:57:09.930
<v Speaker 2>but you know,</v>
<v Speaker 2>the,</v>

1024
00:57:09.930 --> 00:57:10.763
<v Speaker 2>the fact that open source is out there </v>
<v Speaker 2>and there's been prevalent in the </v>

1025
00:57:13.211 --> 00:57:14.044
<v Speaker 2>software world that pave the way for ai </v>
<v Speaker 2>to now be centered on open source </v>

1026
00:57:19.841 --> 00:57:20.674
<v Speaker 2>algorithms.</v>
<v Speaker 2>So right now even though big companies </v>

1027
00:57:22.721 --> 00:57:25.450
<v Speaker 2>and governments dominate the scalable </v>
<v Speaker 2>rollout of Ai,</v>

1028
00:57:25.660 --> 00:57:26.493
<v Speaker 2>the invention of new ai algorithms is </v>
<v Speaker 2>mostly done by people creating new code </v>

1029
00:57:31.391 --> 00:57:32.700
<v Speaker 2>and putting it,</v>
<v Speaker 2>putting it on,</v>

1030
00:57:32.701 --> 00:57:35.380
<v Speaker 2>on get hub or get lab or other open </v>
<v Speaker 2>source repository.</v>

1031
00:57:35.570 --> 00:57:39.580
<v Speaker 2>Retired repositories been sources self </v>
<v Speaker 2>explanatory in its title.</v>

1032
00:57:40.270 --> 00:57:41.103
<v Speaker 2>Pretty much people kind of understand </v>
<v Speaker 2>what it is that means that various </v>

1033
00:57:43.781 --> 00:57:44.614
<v Speaker 2>coders get to share in this,</v>
<v Speaker 2>this code and the source code and they </v>

1034
00:57:48.400 --> 00:57:52.900
<v Speaker 2>get to innovate and they all get to </v>
<v Speaker 2>participate and use each other's work.</v>

1035
00:57:52.930 --> 00:57:53.763
<v Speaker 2>Right,</v>
<v Speaker 2>right.</v>

1036
00:57:53.910 --> 00:57:55.300
<v Speaker 2>Um,</v>
<v Speaker 2>but blockchain,</v>

1037
00:57:55.470 --> 00:57:58.180
<v Speaker 2>and it's confusing for a lot of people.</v>
<v Speaker 2>Could you explain that?</v>

1038
00:57:58.990 --> 00:58:00.740
<v Speaker 2>Sure.</v>
<v Speaker 2>I'm in block block.</v>

1039
00:58:00.800 --> 00:58:04.520
<v Speaker 2>Block chain itself is almost a misnomer,</v>
<v Speaker 2>so we're confused.</v>

1040
00:58:04.600 --> 00:58:05.720
<v Speaker 2>Things are confusing,</v>
<v Speaker 2>uh,</v>

1041
00:58:05.770 --> 00:58:07.060
<v Speaker 2>at every level,</v>
<v Speaker 2>right.</v>

1042
00:58:07.061 --> 00:58:12.061
<v Speaker 2>So we can start with the idea of a </v>
<v Speaker 2>distributed ledger,</v>

1043
00:58:12.521 --> 00:58:16.690
<v Speaker 2>which is basically like a distributed,</v>
<v Speaker 2>an excel spreadsheet or database.</v>

1044
00:58:16.691 --> 00:58:21.310
<v Speaker 2>It's just a story of information which </v>
<v Speaker 2>is not stored just in one place,</v>

1045
00:58:21.311 --> 00:58:23.770
<v Speaker 2>but there's copies of it in lots of </v>
<v Speaker 2>different places.</v>

1046
00:58:24.100 --> 00:58:27.810
<v Speaker 2>Every time my copy of it is updated,</v>
<v Speaker 2>everyone else's copy of it has,</v>

1047
00:58:27.840 --> 00:58:29.380
<v Speaker 2>has,</v>
<v Speaker 2>has got to be updated.</v>

1048
00:58:29.381 --> 00:58:33.690
<v Speaker 2>And then then there's various bells and </v>
<v Speaker 2>whistles like sharding where you know,</v>

1049
00:58:33.730 --> 00:58:34.563
<v Speaker 2>it can be broken in many pieces and each</v>
<v Speaker 2>piece is stored many places or </v>

1050
00:58:37.121 --> 00:58:37.954
<v Speaker 2>something.</v>
<v Speaker 2>So That's a distributed ledger and </v>

1051
00:58:39.881 --> 00:58:41.980
<v Speaker 2>that's just distributed computing.</v>
<v Speaker 2>Now what,</v>

1052
00:58:42.670 --> 00:58:43.503
<v Speaker 2>what makes it more interesting is when </v>
<v Speaker 2>you layer decentralized control onto </v>

1053
00:58:47.411 --> 00:58:47.950
<v Speaker 2>that.</v>

1054
00:58:47.950 --> 00:58:48.783
<v Speaker 2>So imagine you have this distributed </v>
<v Speaker 2>excel spreadsheet or distributed </v>

1055
00:58:51.971 --> 00:58:52.804
<v Speaker 2>database.</v>
<v Speaker 2>There's copies of it stored in a </v>

1056
00:58:54.760 --> 00:58:57.010
<v Speaker 2>thousand places,</v>
<v Speaker 2>but to update it,</v>

1057
00:58:57.220 --> 00:59:01.270
<v Speaker 2>you need like 500 of those thousand </v>
<v Speaker 2>people who own the copies to vote.</v>

1058
00:59:01.271 --> 00:59:02.470
<v Speaker 2>Yeah.</v>
<v Speaker 2>Let's do that update,</v>

1059
00:59:02.530 --> 00:59:03.363
<v Speaker 2>right?</v>
<v Speaker 2>So then then you have a distributed </v>

1060
00:59:05.321 --> 00:59:09.550
<v Speaker 2>store of data and you have like a </v>
<v Speaker 2>democratic voting mechanism to determine</v>

1061
00:59:09.760 --> 00:59:13.010
<v Speaker 2>when all those copies can get,</v>
<v Speaker 2>can get updated together,</v>

1062
00:59:13.011 --> 00:59:13.844
<v Speaker 2>right?</v>
<v Speaker 2>So then then what you have is a data </v>

1063
00:59:16.391 --> 00:59:17.224
<v Speaker 2>storage and update mechanism that's </v>
<v Speaker 2>controlled in a democratic way by the </v>

1064
00:59:20.900 --> 00:59:24.050
<v Speaker 2>group of participants rather than by any</v>
<v Speaker 2>one central controller.</v>

1065
00:59:24.320 --> 00:59:26.870
<v Speaker 2>And that that can have all sorts of </v>
<v Speaker 2>advantages.</v>

1066
00:59:26.871 --> 00:59:29.010
<v Speaker 2>I mean for one thing it means that you </v>
<v Speaker 2>know,</v>

1067
00:59:29.060 --> 00:59:29.893
<v Speaker 2>there's no one controller who can go </v>
<v Speaker 2>rogue and screw with the day without </v>

1068
00:59:32.841 --> 00:59:33.674
<v Speaker 2>telling anyone.</v>
<v Speaker 2>It also means there is no one has some </v>

1069
00:59:36.051 --> 00:59:39.020
<v Speaker 2>limitation go hold a gun to their head </v>
<v Speaker 2>and shoot them for,</v>

1070
00:59:39.320 --> 00:59:43.370
<v Speaker 2>for what data updates were made because </v>
<v Speaker 2>know controlled democratically by,</v>

1071
00:59:43.790 --> 00:59:44.780
<v Speaker 2>by everybody,</v>
<v Speaker 2>right?</v>

1072
00:59:44.780 --> 00:59:47.570
<v Speaker 2>It has ramifications in terms of,</v>
<v Speaker 2>you know,</v>

1073
00:59:47.571 --> 00:59:51.800
<v Speaker 2>legal defensibility and I mean you could</v>
<v Speaker 2>have some people in Iran,</v>

1074
00:59:51.801 --> 00:59:54.440
<v Speaker 2>some in China,</v>
<v Speaker 2>some in the US and,</v>

1075
00:59:54.450 --> 00:59:55.283
<v Speaker 2>and updates to this whole distributed </v>
<v Speaker 2>data store are made by democratic </v>

1076
00:59:59.991 --> 01:00:00.824
<v Speaker 2>decision of all the participants.</v>
<v Speaker 2>Somewhere cryptography comes in is when </v>

1077
01:00:04.221 --> 01:00:05.330
<v Speaker 2>I vote,</v>
<v Speaker 2>I don't have to say,</v>

1078
01:00:05.331 --> 01:00:06.164
<v Speaker 2>yeah,</v>
<v Speaker 2>this is Ben Gursel voting for this </v>

1079
01:00:07.461 --> 01:00:11.300
<v Speaker 2>update to be accepted or not.</v>
<v Speaker 2>It's just ID number one,</v>

1080
01:00:11.301 --> 01:00:11.781
<v Speaker 2>three,</v>
<v Speaker 2>five,</v>

1081
01:00:11.781 --> 01:00:12.291
<v Speaker 2>seven,</v>
<v Speaker 2>two,</v>

1082
01:00:12.291 --> 01:00:13.190
<v Speaker 2>six,</v>
<v Speaker 2>four.</v>

1083
01:00:13.340 --> 01:00:16.220
<v Speaker 2>And then encryption is used to make sure</v>
<v Speaker 2>that,</v>

1084
01:00:16.490 --> 01:00:17.070
<v Speaker 2>you know,</v>
<v Speaker 2>it's,</v>

1085
01:00:17.070 --> 01:00:17.903
<v Speaker 2>it's the same guy voting every time that</v>
<v Speaker 2>it claims to be with without needing </v>

1086
01:00:23.961 --> 01:00:26.060
<v Speaker 2>like your,</v>
<v Speaker 2>your passport number or something.</v>

1087
01:00:26.061 --> 01:00:26.894
<v Speaker 2>Right.</v>
<v Speaker 2>What's ironic about it is it's probably </v>

1088
01:00:27.741 --> 01:00:31.790
<v Speaker 2>one of the best ways ever conceived to </v>
<v Speaker 2>actually vote in this country.</v>

1089
01:00:31.791 --> 01:00:33.170
<v Speaker 2>Yeah,</v>
<v Speaker 2>sure.</v>

1090
01:00:33.171 --> 01:00:36.660
<v Speaker 2>It would be kind of ironic.</v>
<v Speaker 2>There's a lot of applications for the,</v>

1091
01:00:36.890 --> 01:00:37.450
<v Speaker 2>the,</v>
<v Speaker 2>the,</v>

1092
01:00:37.450 --> 01:00:37.980
<v Speaker 2>the,</v>
<v Speaker 2>the.</v>

1093
01:00:37.980 --> 01:00:39.130
<v Speaker 2>That's the,</v>
<v Speaker 2>that's right.</v>

1094
01:00:39.200 --> 01:00:41.340
<v Speaker 2>So the,</v>
<v Speaker 2>so that.</v>

1095
01:00:41.430 --> 01:00:46.430
<v Speaker 2>I mean that's the core mechanism though </v>
<v Speaker 2>where the block chain comes from is like</v>

1096
01:00:46.611 --> 01:00:51.110
<v Speaker 2>a data structure where to store the data</v>
<v Speaker 2>in this distributed database,</v>

1097
01:00:51.410 --> 01:00:55.190
<v Speaker 2>it's stored in a chain of blocks or each</v>
<v Speaker 2>block contains data.</v>

1098
01:00:55.510 --> 01:00:56.343
<v Speaker 2>The thing is not every so-called </v>
<v Speaker 2>blockchain system even uses a chain of </v>

1099
01:01:00.141 --> 01:01:04.010
<v Speaker 2>blocks and I'd like some use a tree or a</v>
<v Speaker 2>graph of blocks or something.</v>

1100
01:01:04.040 --> 01:01:06.660
<v Speaker 2>So that term,</v>
<v Speaker 2>I mean it's,</v>

1101
01:01:06.740 --> 01:01:07.573
<v Speaker 2>it's an alright terms like ai,</v>
<v Speaker 2>like just one of those terms were stuck </v>

1102
01:01:10.851 --> 01:01:11.720
<v Speaker 2>with.</v>
<v Speaker 2>It's one,</v>

1103
01:01:11.810 --> 01:01:12.643
<v Speaker 2>it's one of those terms were stuck with,</v>
<v Speaker 2>even though it's not quite technically </v>

1104
01:01:16.490 --> 01:01:19.580
<v Speaker 2>not quite technically accurate.</v>
<v Speaker 2>I mean anymore.</v>

1105
01:01:19.581 --> 01:01:20.700
<v Speaker 2>I mean w w,</v>
<v Speaker 2>W,</v>

1106
01:01:20.701 --> 01:01:21.590
<v Speaker 2>W,</v>
<v Speaker 2>I don't know,</v>

1107
01:01:21.591 --> 01:01:22.910
<v Speaker 2>another buzzword for it,</v>
<v Speaker 2>right.</v>

1108
01:01:22.940 --> 01:01:23.773
<v Speaker 2>What it is is a,</v>
<v Speaker 2>it's a distributed ledger with </v>

1109
01:01:25.971 --> 01:01:26.804
<v Speaker 2>encryption and decentralized control and</v>
<v Speaker 2>blockchain is the buzzword that's come </v>

1110
01:01:30.471 --> 01:01:31.220
<v Speaker 2>about for that.</v>

1111
01:01:31.220 --> 01:01:32.053
<v Speaker 2>Now what,</v>
<v Speaker 2>what got me interested in blockchain </v>

1112
01:01:34.671 --> 01:01:37.550
<v Speaker 2>really is this decentralized control </v>
<v Speaker 2>aspect.</v>

1113
01:01:37.551 --> 01:01:38.730
<v Speaker 2>So my,</v>
<v Speaker 2>my,</v>

1114
01:01:38.800 --> 01:01:41.120
<v Speaker 2>my wife,</v>
<v Speaker 2>well them with for 10 years now,</v>

1115
01:01:41.360 --> 01:01:43.490
<v Speaker 2>she dug up recently something I'd </v>
<v Speaker 2>forgotten,</v>

1116
01:01:43.491 --> 01:01:48.491
<v Speaker 2>which is a webpage and made in 1995,</v>
<v Speaker 2>like a long time ago where I'd said,</v>

1117
01:01:49.160 --> 01:01:49.993
<v Speaker 2>hey,</v>
<v Speaker 2>I'm going to run for president on the </v>

1118
01:01:51.141 --> 01:01:56.141
<v Speaker 2>decentralization platform for which I'd </v>
<v Speaker 2>completely forgotten that crazy idea.</v>

1119
01:01:56.540 --> 01:01:57.373
<v Speaker 2>I was very young then.</v>
<v Speaker 2>I had no idea within an annoying job </v>

1120
01:01:59.271 --> 01:02:00.104
<v Speaker 2>being president would be.</v>
<v Speaker 2>But the so that the idea of </v>

1121
01:02:04.790 --> 01:02:08.900
<v Speaker 2>decentralized control seemed very </v>
<v Speaker 2>important to me back then,</v>

1122
01:02:08.901 --> 01:02:09.734
<v Speaker 2>which is well,</v>
<v Speaker 2>before bitcoin was invented because I </v>

1123
01:02:11.721 --> 01:02:12.980
<v Speaker 2>could see,</v>
<v Speaker 2>you know,</v>

1124
01:02:12.981 --> 01:02:16.580
<v Speaker 2>a global brain is evolving on the planet</v>
<v Speaker 2>involving humans,</v>

1125
01:02:16.581 --> 01:02:18.620
<v Speaker 2>computers,</v>
<v Speaker 2>communication devices,</v>

1126
01:02:18.960 --> 01:02:22.850
<v Speaker 2>and we don't want this global brain to </v>
<v Speaker 2>be controlled by a small elite.</v>

1127
01:02:22.860 --> 01:02:24.730
<v Speaker 2>We want the global parent to be </v>
<v Speaker 2>controlled in the,</v>

1128
01:02:24.731 --> 01:02:26.220
<v Speaker 2>in a decentralized way.</v>

1129
01:02:26.220 --> 01:02:27.480
<v Speaker 2>So,</v>
<v Speaker 2>so that,</v>

1130
01:02:27.840 --> 01:02:30.450
<v Speaker 2>that's really the,</v>
<v Speaker 2>the beauty of this,</v>

1131
01:02:30.451 --> 01:02:32.910
<v Speaker 2>a blockchain infrastructure.</v>
<v Speaker 2>And what,</v>

1132
01:02:33.270 --> 01:02:34.103
<v Speaker 2>what got me interested in the practical </v>
<v Speaker 2>technologies of block chain was really </v>

1133
01:02:36.991 --> 01:02:41.991
<v Speaker 2>one etherium came out and you add the </v>
<v Speaker 2>notion of a smart contract,</v>

1134
01:02:42.091 --> 01:02:44.890
<v Speaker 2>which was the theory etherium.</v>
<v Speaker 2>Yeah.</v>

1135
01:02:44.891 --> 01:02:46.530
<v Speaker 2>So what is that?</v>
<v Speaker 2>Well,</v>

1136
01:02:46.650 --> 01:02:49.920
<v Speaker 2>so the first blockchain technology was </v>
<v Speaker 2>Bitcoin,</v>

1137
01:02:49.921 --> 01:02:52.320
<v Speaker 2>right?</v>
<v Speaker 2>Which is a well known cryptocurrency now</v>

1138
01:02:53.130 --> 01:02:55.590
<v Speaker 2>if theory.</v>
<v Speaker 2>IOM is another cryptocurrency,</v>

1139
01:02:55.591 --> 01:02:58.080
<v Speaker 2>which is the number to cryptocurrency </v>
<v Speaker 2>right now.</v>

1140
01:02:58.740 --> 01:03:00.510
<v Speaker 2>That's how the loop I am.</v>
<v Speaker 2>Did you know about it?</v>

1141
01:03:01.140 --> 01:03:02.700
<v Speaker 2>You did.</v>
<v Speaker 2>However,</v>

1142
01:03:03.090 --> 01:03:07.420
<v Speaker 2>ethereum came along with a really nice </v>
<v Speaker 2>software framework.</v>

1143
01:03:07.421 --> 01:03:11.890
<v Speaker 2>So it's not just like a digital money,</v>
<v Speaker 2>like bit coin is,</v>

1144
01:03:12.330 --> 01:03:13.163
<v Speaker 2>but it theory.</v>
<v Speaker 2>IOM has a programming language called </v>

1145
01:03:16.530 --> 01:03:17.363
<v Speaker 2>solidity that came with it and this </v>
<v Speaker 2>programming language let's you right </v>

1146
01:03:20.551 --> 01:03:23.670
<v Speaker 2>where they're called smart contracts and</v>
<v Speaker 2>again,</v>

1147
01:03:23.671 --> 01:03:26.190
<v Speaker 2>that sort of a misnomer because a smart </v>
<v Speaker 2>contract,</v>

1148
01:03:26.420 --> 01:03:28.800
<v Speaker 2>it doesn't have to be either smart or a </v>
<v Speaker 2>contract,</v>

1149
01:03:28.830 --> 01:03:29.663
<v Speaker 2>right,</v>
<v Speaker 2>but it was a cool name and then if it's </v>

1150
01:03:32.461 --> 01:03:34.590
<v Speaker 2>really a smart contract,</v>
<v Speaker 2>it's a contract.</v>

1151
01:03:34.620 --> 01:03:35.453
<v Speaker 2>It's like a programmable transaction so </v>
<v Speaker 2>you you can program a legal contract or </v>

1152
01:03:41.431 --> 01:03:43.980
<v Speaker 2>you can program of a financial </v>
<v Speaker 2>transaction.</v>

1153
01:03:44.220 --> 01:03:47.340
<v Speaker 2>So a smart contract,</v>
<v Speaker 2>it's a.</v>

1154
01:03:47.610 --> 01:03:48.443
<v Speaker 2>it's a persistent piece of software that</v>
<v Speaker 2>embodies like a secure encrypted </v>

1155
01:03:53.281 --> 01:03:55.770
<v Speaker 2>transaction between between multiple </v>
<v Speaker 2>parties,</v>

1156
01:03:55.920 --> 01:04:00.920
<v Speaker 2>so pretty much like anything on the back</v>
<v Speaker 2>end of a bank's website or a transaction</v>

1157
01:04:03.691 --> 01:04:04.524
<v Speaker 2>between two companies online,</v>
<v Speaker 2>a purchasing relationship between you </v>

1158
01:04:07.640 --> 01:04:08.473
<v Speaker 2>and the website online.</v>
<v Speaker 2>This could all be stripped it in in the </v>

1159
01:04:10.831 --> 01:04:11.664
<v Speaker 2>smart contract in this secure way,</v>
<v Speaker 2>and then it wouldn't be automated in </v>

1160
01:04:14.911 --> 01:04:15.744
<v Speaker 2>this simple and standard way.</v>
<v Speaker 2>So the vision that Vitalik Buterin who </v>

1161
01:04:19.381 --> 01:04:20.214
<v Speaker 2>was the main creator behind the theory </v>
<v Speaker 2>had is to basically make the internet </v>

1162
01:04:24.330 --> 01:04:25.163
<v Speaker 2>into a giant computing mechanism rather </v>
<v Speaker 2>than mostly like an information storage </v>

1163
01:04:30.181 --> 01:04:34.470
<v Speaker 2>and retrieval mechanism.</v>
<v Speaker 2>Make the Internet into a giant computer,</v>

1164
01:04:34.770 --> 01:04:35.603
<v Speaker 2>but making it really simple programming </v>
<v Speaker 2>language for scripting transactions </v>

1165
01:04:38.941 --> 01:04:39.774
<v Speaker 2>among different computers and different </v>
<v Speaker 2>parties on the Internet where you have </v>

1166
01:04:42.690 --> 01:04:43.523
<v Speaker 2>encryption and you have democratic </v>
<v Speaker 2>decision making and distributed storage </v>

1167
01:04:47.161 --> 01:04:49.410
<v Speaker 2>of information like programmed into </v>
<v Speaker 2>this,</v>

1168
01:04:49.650 --> 01:04:50.790
<v Speaker 2>this world computer.</v>

1169
01:04:50.790 --> 01:04:53.640
<v Speaker 2>Right?</v>
<v Speaker 2>And then that was a really cool idea.</v>

1170
01:04:53.820 --> 01:04:58.020
<v Speaker 2>And the etherium blockchain and solidity</v>
<v Speaker 2>programming language made it really easy</v>

1171
01:04:58.021 --> 01:04:58.854
<v Speaker 2>to do that.</v>
<v Speaker 2>So it made it really easy to program </v>

1172
01:05:00.481 --> 01:05:05.481
<v Speaker 2>like distributed secure transaction and </v>
<v Speaker 2>computing systems on the Internet.</v>

1173
01:05:06.241 --> 01:05:08.040
<v Speaker 2>So I saw this,</v>
<v Speaker 2>I thought,</v>

1174
01:05:08.041 --> 01:05:08.874
<v Speaker 2>well,</v>
<v Speaker 2>like now we finally have the tool set </v>

1175
01:05:12.720 --> 01:05:16.440
<v Speaker 2>that's needed to implement.</v>
<v Speaker 2>Some of this is very popular.</v>

1176
01:05:16.590 --> 01:05:17.423
<v Speaker 2>I mean,</v>
<v Speaker 2>I mean basically almost every ico that </v>

1177
01:05:21.551 --> 01:05:24.670
<v Speaker 2>was done the last couple of years was </v>
<v Speaker 2>done on the ethereum blockchain.</v>

1178
01:05:25.000 --> 01:05:27.130
<v Speaker 2>What's an ICO?</v>
<v Speaker 2>Initial coin offering.</v>

1179
01:05:27.230 --> 01:05:27.730
<v Speaker 2>Oh,</v>
<v Speaker 2>okay.</v>

1180
01:05:27.730 --> 01:05:29.220
<v Speaker 2>So for Bitcoins,</v>
<v Speaker 2>for,</v>

1181
01:05:29.680 --> 01:05:31.150
<v Speaker 2>I'm sorry,</v>
<v Speaker 2>cryptocurrency,</v>

1182
01:05:31.190 --> 01:05:35.200
<v Speaker 2>cryptocurrency use this technology </v>
<v Speaker 2>brings.</v>

1183
01:05:35.320 --> 01:05:36.153
<v Speaker 2>Right?</v>
<v Speaker 2>So what happened in the last couple of </v>

1184
01:05:37.690 --> 01:05:38.523
<v Speaker 2>years is a bunch of people realized you </v>
<v Speaker 2>could use this etherium programming </v>

1185
01:05:45.161 --> 01:05:49.450
<v Speaker 2>framework to create a new </v>
<v Speaker 2>cryptocurrency,</v>

1186
01:05:49.750 --> 01:05:50.583
<v Speaker 2>like a new artificial money.</v>
<v Speaker 2>And then you can try to get people to </v>

1187
01:05:54.101 --> 01:05:58.650
<v Speaker 2>use your new artificial money for </v>
<v Speaker 2>certain types of artificial coins.</v>

1188
01:05:59.270 --> 01:06:03.820
<v Speaker 2>It may be more popular.</v>
<v Speaker 2>Is Bitcoin,</v>

1189
01:06:03.821 --> 01:06:07.480
<v Speaker 2>right?</v>
<v Speaker 2>Bitcoin is by far the most popular.</v>

1190
01:06:07.540 --> 01:06:10.630
<v Speaker 2>The most delirium is number two.</v>
<v Speaker 2>And there's a bunch of others.</v>

1191
01:06:10.631 --> 01:06:11.464
<v Speaker 2>I mean Hartwick comparison,</v>
<v Speaker 2>like how much bigger is bitcoin and </v>

1192
01:06:14.381 --> 01:06:16.930
<v Speaker 2>ethereum?</v>
<v Speaker 2>I Dunno.</v>

1193
01:06:16.960 --> 01:06:21.940
<v Speaker 2>Five factor of three to five.</v>
<v Speaker 2>So maybe just a factor of two.</v>

1194
01:06:21.941 --> 01:06:22.774
<v Speaker 2>Now it's actually last year at theory </v>
<v Speaker 2>and almost took over bitcoin when </v>

1195
01:06:27.671 --> 01:06:29.050
<v Speaker 2>bitcoin started crashing.</v>
<v Speaker 2>Yeah.</v>

1196
01:06:29.051 --> 01:06:30.700
<v Speaker 2>Yeah.</v>
<v Speaker 2>Now if their room is back down,</v>

1197
01:06:30.701 --> 01:06:35.701
<v Speaker 2>there might be half or a third of the </v>
<v Speaker 2>fluctuating value of these things.</v>

1198
01:06:36.130 --> 01:06:37.930
<v Speaker 2>To My,</v>
<v Speaker 2>to my mind,</v>

1199
01:06:38.290 --> 01:06:39.123
<v Speaker 2>creating artificial monies is one tiny </v>
<v Speaker 2>bit of the potential of what you could </v>

1200
01:06:45.911 --> 01:06:48.790
<v Speaker 2>do with the whole blockchain tool.</v>
<v Speaker 2>Set it.</v>

1201
01:06:49.120 --> 01:06:54.120
<v Speaker 2>It happened to become popular initially </v>
<v Speaker 2>because it's where the money is,</v>

1202
01:06:55.061 --> 01:06:57.250
<v Speaker 2>right?</v>
<v Speaker 2>I've been right writing it is.</v>

1203
01:06:57.251 --> 01:06:59.740
<v Speaker 2>It is money and that's interesting to </v>
<v Speaker 2>people,</v>

1204
01:07:00.010 --> 01:07:00.843
<v Speaker 2>but on the other hand,</v>
<v Speaker 2>what it's really about is making world </v>

1205
01:07:05.351 --> 01:07:05.860
<v Speaker 2>computer.</v>

1206
01:07:05.860 --> 01:07:09.220
<v Speaker 2>It's about scripting with a simple </v>
<v Speaker 2>programming language,</v>

1207
01:07:09.221 --> 01:07:12.550
<v Speaker 2>all sorts of transactions between </v>
<v Speaker 2>people,</v>

1208
01:07:12.551 --> 01:07:14.410
<v Speaker 2>companies,</v>
<v Speaker 2>whatever,</v>

1209
01:07:14.411 --> 01:07:17.080
<v Speaker 2>all sorts of exchanges of,</v>
<v Speaker 2>of information.</v>

1210
01:07:17.081 --> 01:07:20.440
<v Speaker 2>So I mean it's about decentralized </v>
<v Speaker 2>voting mechanisms.</v>

1211
01:07:20.441 --> 01:07:21.274
<v Speaker 2>It's about Ai's being able to send data </v>
<v Speaker 2>and processing for each other and pay </v>

1212
01:07:26.741 --> 01:07:29.280
<v Speaker 2>each other for their transactions.</v>
<v Speaker 2>So I'm in,</v>

1213
01:07:29.550 --> 01:07:32.310
<v Speaker 2>there's,</v>
<v Speaker 2>it's about automating supply chains and,</v>

1214
01:07:32.311 --> 01:07:33.144
<v Speaker 2>and,</v>
<v Speaker 2>and shipping and ecommerce so that </v>

1215
01:07:34.900 --> 01:07:35.970
<v Speaker 2>there's an in,</v>
<v Speaker 2>in,</v>

1216
01:07:36.000 --> 01:07:37.210
<v Speaker 2>in,</v>
<v Speaker 2>in essence,</v>

1217
01:07:37.390 --> 01:07:38.223
<v Speaker 2>you know,</v>
<v Speaker 2>just like computers and the Internet </v>

1218
01:07:41.140 --> 01:07:41.973
<v Speaker 2>started with a certain small set of </v>
<v Speaker 2>applications and then pervaded almost </v>

1219
01:07:45.851 --> 01:07:46.690
<v Speaker 2>everything,</v>
<v Speaker 2>right?</v>

1220
01:07:46.860 --> 01:07:48.880
<v Speaker 2>It's the same way with blockchain </v>
<v Speaker 2>technology,</v>

1221
01:07:48.881 --> 01:07:49.714
<v Speaker 2>like it started with digital money,</v>
<v Speaker 2>but the core technology is going to </v>

1222
01:07:53.561 --> 01:07:54.394
<v Speaker 2>pervade almost everything because </v>
<v Speaker 2>there's almost no domain of human </v>

1223
01:07:57.731 --> 01:08:02.731
<v Speaker 2>pursuit that couldn't use like security </v>
<v Speaker 2>through cryptography,</v>

1224
01:08:03.010 --> 01:08:04.270
<v Speaker 2>some sort of,</v>
<v Speaker 2>you know,</v>

1225
01:08:04.271 --> 01:08:08.920
<v Speaker 2>participatory decision making and then </v>
<v Speaker 2>distributed storage of information.</v>

1226
01:08:08.921 --> 01:08:09.190
<v Speaker 2>Right?</v>

1227
01:08:09.190 --> 01:08:11.980
<v Speaker 2>So these things are also valuable for </v>
<v Speaker 2>ai,</v>

1228
01:08:11.981 --> 01:08:13.960
<v Speaker 2>which is how I got into it in the first </v>
<v Speaker 2>place.</v>

1229
01:08:13.961 --> 01:08:16.400
<v Speaker 2>I mean,</v>
<v Speaker 2>if you're making a very,</v>

1230
01:08:16.401 --> 01:08:20.440
<v Speaker 2>very powerful ai that is going to,</v>
<v Speaker 2>you know,</v>

1231
01:08:20.480 --> 01:08:23.660
<v Speaker 2>gradually through the practical value </v>
<v Speaker 2>delivers,</v>

1232
01:08:23.670 --> 01:08:25.910
<v Speaker 2>she will grow up to be more and more and</v>
<v Speaker 2>more intelligent.</v>

1233
01:08:26.300 --> 01:08:27.133
<v Speaker 2>I mean this ai shouldn't be able to </v>
<v Speaker 2>engage a large party of people and ais </v>

1234
01:08:31.220 --> 01:08:33.520
<v Speaker 2>and participatory decision making.</v>
<v Speaker 2>They,</v>

1235
01:08:33.521 --> 01:08:38.521
<v Speaker 2>I should be able to store information in</v>
<v Speaker 2>a widely distributed way and,</v>

1236
01:08:38.980 --> 01:08:41.330
<v Speaker 2>and ai certainly shouldn't be able to </v>
<v Speaker 2>use,</v>

1237
01:08:41.470 --> 01:08:43.540
<v Speaker 2>you know,</v>
<v Speaker 2>security and encryption development,</v>

1238
01:08:43.580 --> 01:08:44.413
<v Speaker 2>who are the parties involved in this </v>
<v Speaker 2>operation and I mean these are the key </v>

1239
01:08:47.241 --> 01:08:49.550
<v Speaker 2>things behind behind blockchain </v>
<v Speaker 2>technology.</v>

1240
01:08:49.551 --> 01:08:50.384
<v Speaker 2>So I'm in the fact the fact that </v>
<v Speaker 2>blockchain began with artificial </v>

1241
01:08:53.571 --> 01:08:54.404
<v Speaker 2>currencies to me is a detail of history.</v>
<v Speaker 2>Just like the fact the fact that the </v>

1242
01:08:58.461 --> 01:09:01.820
<v Speaker 2>Internet began as like a nuclear early </v>
<v Speaker 2>warning system right then.</v>

1243
01:09:01.860 --> 01:09:03.920
<v Speaker 2>And it did.</v>
<v Speaker 2>It's good for that.</v>

1244
01:09:03.921 --> 01:09:05.270
<v Speaker 2>But it's,</v>
<v Speaker 2>as it happens,</v>

1245
01:09:05.271 --> 01:09:07.970
<v Speaker 2>it's also even better for a lot of other</v>
<v Speaker 2>things.</v>

1246
01:09:08.060 --> 01:09:08.893
<v Speaker 2>So the,</v>

1247
01:09:09.170 --> 01:09:13.730
<v Speaker 1>the solution for the financial situation</v>
<v Speaker 1>that we find ourselves in.</v>

1248
01:09:14.310 --> 01:09:19.310
<v Speaker 1>One of the more interesting things about</v>
<v Speaker 1>cryptocurrencies that someone said,</v>

1249
01:09:19.731 --> 01:09:20.211
<v Speaker 1>okay,</v>
<v Speaker 1>look,</v>

1250
01:09:20.211 --> 01:09:21.044
<v Speaker 1>obviously we all kind of agreed that our</v>
<v Speaker 1>financial institutions are very flawed </v>

1251
01:09:24.370 --> 01:09:27.590
<v Speaker 1>system that we operate under is it's </v>
<v Speaker 1>very fucked up,</v>

1252
01:09:27.800 --> 01:09:29.480
<v Speaker 1>so how do we fix that?</v>
<v Speaker 1>Well,</v>

1253
01:09:29.540 --> 01:09:33.310
<v Speaker 1>sending the super nerds and so they </v>
<v Speaker 1>figure out a new number,</v>

1254
01:09:33.330 --> 01:09:35.220
<v Speaker 1>get ascend into super,</v>
<v Speaker 1>a super,</v>

1255
01:09:36.140 --> 01:09:38.690
<v Speaker 1>super nervous super.</v>
<v Speaker 1>Obviously.</v>

1256
01:09:38.930 --> 01:09:43.930
<v Speaker 1>Who is the guy that they think that this</v>
<v Speaker 1>fake person maybe not real,</v>

1257
01:09:44.420 --> 01:09:46.910
<v Speaker 1>that came up with bitcoin.</v>
<v Speaker 1>Komodo.</v>

1258
01:09:47.450 --> 01:09:50.370
<v Speaker 1>Do you have any suspicions as to who </v>
<v Speaker 1>this is a,</v>

1259
01:09:50.720 --> 01:09:55.430
<v Speaker 1>I can neither confirm nor deny that you </v>
<v Speaker 1>wouldn't be on the inside.</v>

1260
01:09:55.431 --> 01:09:57.560
<v Speaker 1>We'll talk later,</v>
<v Speaker 1>but that,</v>

1261
01:09:57.680 --> 01:09:59.450
<v Speaker 1>this is,</v>
<v Speaker 1>it's very,</v>

1262
01:10:00.890 --> 01:10:04.010
<v Speaker 1>it's very interesting,</v>
<v Speaker 1>but it's also very promising.</v>

1263
01:10:04.200 --> 01:10:05.033
<v Speaker 1>I,</v>
<v Speaker 1>I've like high optimism for </v>

1264
01:10:06.621 --> 01:10:07.454
<v Speaker 1>cryptocurrencies because I think that </v>
<v Speaker 1>kids today are looking at it with much </v>

1265
01:10:11.721 --> 01:10:13.960
<v Speaker 1>more open eyes than uh,</v>
<v Speaker 1>you know,</v>

1266
01:10:14.000 --> 01:10:14.833
<v Speaker 1>grandfathers,</v>
<v Speaker 1>grandfathers are looking at bitcoin and </v>

1267
01:10:16.660 --> 01:10:20.780
<v Speaker 1>the father.</v>
<v Speaker 1>You're exceptional one,</v>

1268
01:10:21.140 --> 01:10:24.350
<v Speaker 1>but there's a lot of people that are </v>
<v Speaker 1>older that just,</v>

1269
01:10:24.620 --> 01:10:26.630
<v Speaker 1>they're not open to accepting these </v>
<v Speaker 1>ideas,</v>

1270
01:10:26.631 --> 01:10:27.464
<v Speaker 1>but I think kids today in particular,</v>
<v Speaker 1>the ones that have grown up with the </v>

1271
01:10:32.121 --> 01:10:34.430
<v Speaker 1>internet as a constant force in their </v>
<v Speaker 1>life,</v>

1272
01:10:34.800 --> 01:10:35.633
<v Speaker 1>they're.</v>
<v Speaker 1>I think they're more likely to embrace </v>

1273
01:10:37.251 --> 01:10:38.480
<v Speaker 1>something along those lines.</v>

1274
01:10:38.610 --> 01:10:38.941
<v Speaker 2>Well,</v>
<v Speaker 2>yeah,</v>

1275
01:10:38.941 --> 01:10:39.774
<v Speaker 2>so there's no doubt that you know,</v>
<v Speaker 2>cryptographic formulations of money are </v>

1276
01:10:46.501 --> 01:10:49.260
<v Speaker 2>going to become the standard.</v>
<v Speaker 2>The question,</v>

1277
01:10:49.270 --> 01:10:51.630
<v Speaker 2>do you think that's going to be the </v>
<v Speaker 2>standard that will have been?</v>

1278
01:10:51.800 --> 01:10:52.980
<v Speaker 2>Yeah.</v>
<v Speaker 2>However,</v>

1279
01:10:53.280 --> 01:10:57.480
<v Speaker 2>it could happen potentially in a very </v>
<v Speaker 2>uninteresting way.</v>

1280
01:10:57.710 --> 01:11:00.540
<v Speaker 2>How's.</v>
<v Speaker 2>You could just have the dollar.</v>

1281
01:11:00.570 --> 01:11:01.403
<v Speaker 2>I mean the government could just say we </v>
<v Speaker 2>will create this cryptographic token </v>

1282
01:11:05.820 --> 01:11:07.110
<v Speaker 2>which counts as a dollar.</v>
<v Speaker 2>I mean,</v>

1283
01:11:07.111 --> 01:11:09.030
<v Speaker 2>most dollars are just electronic.</v>
<v Speaker 2>Anyway,</v>

1284
01:11:09.670 --> 01:11:10.690
<v Speaker 2>so,</v>
<v Speaker 2>so what,</v>

1285
01:11:10.830 --> 01:11:11.663
<v Speaker 2>what,</v>
<v Speaker 2>what habitually happens is technologies </v>

1286
01:11:14.131 --> 01:11:14.964
<v Speaker 2>that are invented to subvert the </v>
<v Speaker 2>establishment are converted to a forum </v>

1287
01:11:19.381 --> 01:11:22.290
<v Speaker 2>where they help bolster the </v>
<v Speaker 2>establishment.</v>

1288
01:11:22.291 --> 01:11:25.680
<v Speaker 2>Instead,</v>
<v Speaker 2>I'm in and in financial services.</v>

1289
01:11:25.681 --> 01:11:26.514
<v Speaker 2>This happens very rapidly,</v>
<v Speaker 2>like pay Pal Peter Teal on this guy </v>

1290
01:11:30.001 --> 01:11:34.590
<v Speaker 2>started paypal thinking they were going </v>
<v Speaker 2>to obsolete Fiat currency and make an an</v>

1291
01:11:34.591 --> 01:11:39.030
<v Speaker 2>alternative to the currencies run by by </v>
<v Speaker 2>nation states instead.</v>

1292
01:11:39.210 --> 01:11:42.540
<v Speaker 2>They were driven to make it a credit </v>
<v Speaker 2>card processing front end.</v>

1293
01:11:42.570 --> 01:11:43.403
<v Speaker 2>Right.</v>
<v Speaker 2>So,</v>

1294
01:11:43.600 --> 01:11:44.433
<v Speaker 2>so that's one thing that could happen </v>
<v Speaker 2>with crypto currency is it just becomes </v>

1295
01:11:48.691 --> 01:11:50.770
<v Speaker 2>a mechanism for,</v>
<v Speaker 2>you know,</v>

1296
01:11:51.130 --> 01:11:56.130
<v Speaker 2>governments and big companies and banks </v>
<v Speaker 2>to do the things more efficiently.</v>

1297
01:11:56.251 --> 01:11:57.084
<v Speaker 2>So what,</v>
<v Speaker 2>what's interesting isn't so much the </v>

1298
01:11:59.131 --> 01:11:59.964
<v Speaker 2>digital money aspect,</v>
<v Speaker 2>although it is in some ways a great way </v>

1299
01:12:02.461 --> 01:12:04.160
<v Speaker 2>to do digital money.</v>
<v Speaker 2>Wow.</v>

1300
01:12:04.230 --> 01:12:05.063
<v Speaker 2>What,</v>
<v Speaker 2>What's interesting is with all the </v>

1301
01:12:07.501 --> 01:12:10.350
<v Speaker 2>flexibility it gives you to script,</v>
<v Speaker 2>you know,</v>

1302
01:12:10.351 --> 01:12:15.351
<v Speaker 2>complex computing networks in there is </v>
<v Speaker 2>the possibility to new forms of,</v>

1303
01:12:17.700 --> 01:12:21.000
<v Speaker 2>you know,</v>
<v Speaker 2>participatory democratic self organizing</v>

1304
01:12:21.001 --> 01:12:23.490
<v Speaker 2>networks.</v>
<v Speaker 2>So blockchain,</v>

1305
01:12:23.520 --> 01:12:27.180
<v Speaker 2>like the internet or computing is a very</v>
<v Speaker 2>flexible medium.</v>

1306
01:12:27.420 --> 01:12:30.060
<v Speaker 2>You could use it to make tools of </v>
<v Speaker 2>oppression or,</v>

1307
01:12:30.180 --> 01:12:32.790
<v Speaker 2>or you could use it to make tools of </v>
<v Speaker 2>amazing growth,</v>

1308
01:12:32.791 --> 01:12:37.170
<v Speaker 2>growth and liberation.</v>
<v Speaker 2>And obviously we know which one I'm more</v>

1309
01:12:37.171 --> 01:12:38.004
<v Speaker 2>interested in.</v>

1310
01:12:38.180 --> 01:12:39.530
<v Speaker 3>Yeah.</v>
<v Speaker 3>Now what,</v>

1311
01:12:39.620 --> 01:12:40.453
<v Speaker 3>what is blockchain?</v>
<v Speaker 3>What is blockchain being currently used </v>

1312
01:12:44.661 --> 01:12:45.820
<v Speaker 3>for?</v>
<v Speaker 3>Like what,</v>

1313
01:12:45.850 --> 01:12:48.770
<v Speaker 3>what different applications,</v>
<v Speaker 3>because it's not just cryptocurrency,</v>

1314
01:12:49.040 --> 01:12:50.690
<v Speaker 3>they use a bunch of different things </v>
<v Speaker 3>now,</v>

1315
01:12:50.691 --> 01:12:50.960
<v Speaker 3>right?</v>

1316
01:12:50.960 --> 01:12:55.040
<v Speaker 2>They are.</v>
<v Speaker 2>I would say it's very early stage.</v>

1317
01:12:55.041 --> 01:12:55.874
<v Speaker 2>So probably the how early.</v>
<v Speaker 2>Well the heaviest users of blockchain </v>

1318
01:13:00.560 --> 01:13:05.000
<v Speaker 2>now are probably inside large financial </v>
<v Speaker 2>services companies actually.</v>

1319
01:13:05.001 --> 01:13:09.230
<v Speaker 2>So if you look at it theory in the </v>
<v Speaker 2>project I mentioned,</v>

1320
01:13:09.231 --> 01:13:12.740
<v Speaker 2>so it theory,</v>
<v Speaker 2>ms is run by an open source,</v>

1321
01:13:12.830 --> 01:13:15.500
<v Speaker 2>an open foundation,</v>
<v Speaker 2>ethereum foundation.</v>

1322
01:13:16.070 --> 01:13:19.220
<v Speaker 2>Then there's a consulting company called</v>
<v Speaker 2>consensus,</v>

1323
01:13:19.490 --> 01:13:20.323
<v Speaker 2>which is a totally separate organization</v>
<v Speaker 2>that was founded by Joe Lubin who was </v>

1324
01:13:23.931 --> 01:13:28.310
<v Speaker 2>one of the founders of ethereum in the </v>
<v Speaker 2>early days and consensus as you know,</v>

1325
01:13:28.370 --> 01:13:33.320
<v Speaker 2>it's funded a bunch of the work within </v>
<v Speaker 2>the ethereum foundation and community,</v>

1326
01:13:33.680 --> 01:13:34.513
<v Speaker 2>but consensus has done a lot of </v>
<v Speaker 2>contracts just working with governments </v>

1327
01:13:37.791 --> 01:13:38.624
<v Speaker 2>and big companies to customize code </v>
<v Speaker 2>based on the theory to help with their </v>

1328
01:13:43.131 --> 01:13:47.600
<v Speaker 2>internal operations.</v>
<v Speaker 2>So actually a lot of the practical value</v>

1329
01:13:47.601 --> 01:13:50.900
<v Speaker 2>has been with stuff that isn't in the </v>
<v Speaker 2>public eye that much,</v>

1330
01:13:50.901 --> 01:13:53.320
<v Speaker 2>but it's like back end and in,</v>
<v Speaker 2>in,</v>

1331
01:13:53.321 --> 01:13:54.980
<v Speaker 2>in inside of companies.</v>

1332
01:13:54.980 --> 01:13:59.980
<v Speaker 2>And in terms of practical customer </v>
<v Speaker 2>facing uses of,</v>

1333
01:14:00.520 --> 01:14:02.060
<v Speaker 2>of cryptocurrency.</v>
<v Speaker 2>I mean,</v>

1334
01:14:02.061 --> 01:14:04.490
<v Speaker 2>no,</v>
<v Speaker 2>the Tron blockchain,</v>

1335
01:14:04.491 --> 01:14:07.880
<v Speaker 2>which is different than the theory that </v>
<v Speaker 2>has a bunch of games on it,</v>

1336
01:14:07.910 --> 01:14:09.950
<v Speaker 2>for example,</v>
<v Speaker 2>and some online gambling for,</v>

1337
01:14:10.340 --> 01:14:11.530
<v Speaker 2>for that matter.</v>
<v Speaker 2>So that,</v>

1338
01:14:11.531 --> 01:14:14.350
<v Speaker 2>that's uh,</v>
<v Speaker 2>that that's gotten a lot of users,</v>

1339
01:14:14.351 --> 01:14:17.110
<v Speaker 2>but the online games like how,</v>
<v Speaker 2>how do they use that?</v>

1340
01:14:17.730 --> 01:14:18.970
<v Speaker 2>Oh,</v>
<v Speaker 2>that's a payment mechanism.</v>

1341
01:14:20.040 --> 01:14:20.873
<v Speaker 2>But that there,</v>
<v Speaker 2>this is one of the things there's a lot </v>

1342
01:14:23.801 --> 01:14:27.700
<v Speaker 2>of handwringing about in the </v>
<v Speaker 2>cryptocurrency world now is gambling.</v>

1343
01:14:27.940 --> 01:14:28.773
<v Speaker 2>No,</v>
<v Speaker 2>just the fact that there aren't that </v>

1344
01:14:29.801 --> 01:14:33.490
<v Speaker 2>many big consumer facing uses of,</v>
<v Speaker 2>of,</v>

1345
01:14:33.491 --> 01:14:35.040
<v Speaker 2>of,</v>
<v Speaker 2>of cryptocurrency.</v>

1346
01:14:35.041 --> 01:14:36.250
<v Speaker 2>I mean,</v>
<v Speaker 2>I mean everyone would,</v>

1347
01:14:36.400 --> 01:14:40.180
<v Speaker 2>everyone would like there to be.</v>
<v Speaker 2>That was the idea and this is one of the</v>

1348
01:14:40.181 --> 01:14:44.320
<v Speaker 2>things we're aiming at with our </v>
<v Speaker 2>singularity in the project is to,</v>

1349
01:14:44.820 --> 01:14:45.390
<v Speaker 2>you know,</v>
<v Speaker 2>we,</v>

1350
01:14:45.390 --> 01:14:50.390
<v Speaker 2>we by putting ai on the blockchain in a </v>
<v Speaker 2>highly effective way.</v>

1351
01:14:51.550 --> 01:14:55.330
<v Speaker 2>And then we're also,</v>
<v Speaker 2>we have these two tiers.</v>

1352
01:14:55.331 --> 01:14:56.164
<v Speaker 2>So we have the singularity net </v>
<v Speaker 2>foundation which is creating this open </v>

1353
01:14:59.501 --> 01:15:04.501
<v Speaker 2>source decentralized platform in which </v>
<v Speaker 2>ais can talk to other ais and you know,</v>

1354
01:15:05.351 --> 01:15:08.620
<v Speaker 2>like ants in the colony group together </v>
<v Speaker 2>to form smarter and smarter Ai.</v>

1355
01:15:09.010 --> 01:15:13.090
<v Speaker 2>Then we're spinning off a company called</v>
<v Speaker 2>the singularity studio,</v>

1356
01:15:13.270 --> 01:15:18.250
<v Speaker 2>which will use this decentralized </v>
<v Speaker 2>platform to help big companies integrate</v>

1357
01:15:18.251 --> 01:15:19.084
<v Speaker 2>ai into their operation.</v>
<v Speaker 2>So with the singular these studio </v>

1358
01:15:21.971 --> 01:15:22.804
<v Speaker 2>company,</v>
<v Speaker 2>we want to get all these big companies </v>

1359
01:15:25.120 --> 01:15:30.120
<v Speaker 2>using the ai tools in the singular unit </v>
<v Speaker 2>platform and then we want to drive,</v>

1360
01:15:30.730 --> 01:15:32.200
<v Speaker 2>you know,</v>
<v Speaker 2>massive usage of,</v>

1361
01:15:32.201 --> 01:15:36.100
<v Speaker 2>of blockchain in the singularity in that</v>
<v Speaker 2>way.</v>

1362
01:15:36.310 --> 01:15:39.310
<v Speaker 2>So that's if we're successful with what </v>
<v Speaker 2>we're doing,</v>

1363
01:15:39.790 --> 01:15:41.280
<v Speaker 2>this will be,</v>
<v Speaker 2>you know,</v>

1364
01:15:41.290 --> 01:15:42.123
<v Speaker 2>within a year from now or something.</v>
<v Speaker 2>By far the biggest usage of blockchain </v>

1365
01:15:46.630 --> 01:15:47.463
<v Speaker 2>outside of financial exchange is our use</v>
<v Speaker 2>of blockchain within singularity unit </v>

1366
01:15:51.671 --> 01:15:56.671
<v Speaker 2>for ai basically for customers to get </v>
<v Speaker 2>the AI services that they need for their</v>

1367
01:15:57.911 --> 01:15:58.744
<v Speaker 2>businesses and then for ais to transact </v>
<v Speaker 2>with other ai is paying other ais for </v>

1368
01:16:03.131 --> 01:16:04.300
<v Speaker 2>doing services for them.</v>

1369
01:16:04.330 --> 01:16:05.410
<v Speaker 2>Because this,</v>
<v Speaker 2>this,</v>

1370
01:16:05.411 --> 01:16:06.244
<v Speaker 2>this I think is is a path forward.</v>
<v Speaker 2>It's like a society and economy of </v>

1371
01:16:10.001 --> 01:16:12.850
<v Speaker 2>minds.</v>
<v Speaker 2>It's not like one monolithic Ai.</v>

1372
01:16:13.180 --> 01:16:17.110
<v Speaker 2>It's a whole bunch of ais carried by </v>
<v Speaker 2>different people all over the world with</v>

1373
01:16:17.111 --> 01:16:20.740
<v Speaker 2>not only are in the marketplace </v>
<v Speaker 2>providing services to customers,</v>

1374
01:16:21.010 --> 01:16:21.843
<v Speaker 2>but each ai is asking questions of each </v>
<v Speaker 2>other and then rating each other of how </v>

1375
01:16:25.751 --> 01:16:29.350
<v Speaker 2>good they are sending data to each other</v>
<v Speaker 2>and paying each other for the services.</v>

1376
01:16:29.351 --> 01:16:30.184
<v Speaker 2>So this,</v>
<v Speaker 2>this like network of ais can emerge and </v>

1377
01:16:33.361 --> 01:16:37.850
<v Speaker 2>intelligence on the whole network level </v>
<v Speaker 2>as well as there being intelligence city</v>

1378
01:16:37.880 --> 01:16:39.430
<v Speaker 2>in each,</v>
<v Speaker 2>each component.</v>

1379
01:16:39.640 --> 01:16:42.780
<v Speaker 2>And is it also fascinating to you that </v>
<v Speaker 2>this is not dependent upon nation?</v>

1380
01:16:42.781 --> 01:16:44.920
<v Speaker 2>So this is a world that ever.</v>
<v Speaker 2>I think.</v>

1381
01:16:45.520 --> 01:16:47.830
<v Speaker 2>I think that's going to be important </v>
<v Speaker 2>once,</v>

1382
01:16:48.100 --> 01:16:50.890
<v Speaker 2>once it starts to get a very high level </v>
<v Speaker 2>of intelligence.</v>

1383
01:16:50.890 --> 01:16:54.150
<v Speaker 2>So in the early stages,</v>
<v Speaker 2>okay,</v>

1384
01:16:54.730 --> 01:16:56.070
<v Speaker 2>what would it hurt?</v>
<v Speaker 2>Like if,</v>

1385
01:16:56.100 --> 01:16:59.700
<v Speaker 2>if I had my own database to central </v>
<v Speaker 2>record of,</v>

1386
01:16:59.740 --> 01:17:00.940
<v Speaker 2>of everything.</v>
<v Speaker 2>Like I'm,</v>

1387
01:17:00.941 --> 01:17:03.370
<v Speaker 2>I'm an honest person.</v>
<v Speaker 2>I'm not going to rip anyone off,</v>

1388
01:17:04.450 --> 01:17:05.283
<v Speaker 2>but once we start to make a transition </v>
<v Speaker 2>towards artificial general intelligence </v>

1389
01:17:10.880 --> 01:17:11.713
<v Speaker 2>in this global decentralized network,</v>
<v Speaker 2>which has component ais from every </v>

1390
01:17:14.961 --> 01:17:18.050
<v Speaker 2>country on the planet,</v>
<v Speaker 2>like at that point,</v>

1391
01:17:18.051 --> 01:17:20.330
<v Speaker 2>once it's clear,</v>
<v Speaker 2>you're getting toward a gi,</v>

1392
01:17:20.750 --> 01:17:23.900
<v Speaker 2>a lot of people wanting to step in and </v>
<v Speaker 2>control this thing,</v>

1393
01:17:24.140 --> 01:17:25.280
<v Speaker 2>you know,</v>
<v Speaker 2>by law,</v>

1394
01:17:25.281 --> 01:17:27.710
<v Speaker 2>by military might,</v>
<v Speaker 2>by any means necessary.</v>

1395
01:17:28.010 --> 01:17:30.020
<v Speaker 2>By that point.</v>
<v Speaker 2>The fact that you have this open,</v>

1396
01:17:30.021 --> 01:17:30.854
<v Speaker 2>decentralized network under underpinning</v>
<v Speaker 2>everything like this gives an amazing </v>

1397
01:17:34.791 --> 01:17:37.550
<v Speaker 2>resilience to what you're doing.</v>
<v Speaker 2>Who can shut down Linux,</v>

1398
01:17:37.551 --> 01:17:40.260
<v Speaker 2>you can shut down Bitcoin,</v>
<v Speaker 2>nobody can right you,</v>

1399
01:17:40.380 --> 01:17:41.520
<v Speaker 2>you,</v>
<v Speaker 2>you want Ai,</v>

1400
01:17:41.540 --> 01:17:44.480
<v Speaker 2>you want ai to be like that.</v>
<v Speaker 2>You want to be a global,</v>

1401
01:17:44.660 --> 01:17:45.860
<v Speaker 2>you know,</v>
<v Speaker 2>upsurge of,</v>

1402
01:17:45.861 --> 01:17:46.694
<v Speaker 2>of creativity and,</v>
<v Speaker 2>and mutual benefit from people all over </v>

1403
01:17:49.851 --> 01:17:52.820
<v Speaker 2>the planet,</v>
<v Speaker 2>which no powerful party can,</v>

1404
01:17:52.880 --> 01:17:56.390
<v Speaker 2>can shut down even if they're afraid </v>
<v Speaker 2>that it threatens their hedge.</v>

1405
01:17:56.390 --> 01:17:57.223
<v Speaker 2>Amani.</v>
<v Speaker 2>It's very interesting because in a lot </v>

1406
01:17:58.521 --> 01:18:01.640
<v Speaker 2>of ways that's a,</v>
<v Speaker 2>it's a very elegant solution to.</v>

1407
01:18:01.641 --> 01:18:03.580
<v Speaker 2>What's an obvious problem?</v>
<v Speaker 2>Yeah.</v>

1408
01:18:04.610 --> 01:18:05.443
<v Speaker 2>Just as the Internet is an elegant </v>
<v Speaker 2>solution to what's in hindsight and </v>

1409
01:18:08.911 --> 01:18:09.940
<v Speaker 2>obvious problem,</v>
<v Speaker 2>right?</v>

1410
01:18:09.950 --> 01:18:13.360
<v Speaker 2>It's the distribution of communicate </v>
<v Speaker 2>this,</v>

1411
01:18:14.000 --> 01:18:14.833
<v Speaker 2>but this is a extra special to me </v>
<v Speaker 2>because if I was a person running a </v>

1412
01:18:19.161 --> 01:18:20.840
<v Speaker 2>country,</v>
<v Speaker 2>I would be terrified of this shit.</v>

1413
01:18:21.140 --> 01:18:21.681
<v Speaker 2>I'd be like,</v>
<v Speaker 2>well,</v>

1414
01:18:21.681 --> 01:18:23.950
<v Speaker 2>this is what's going to.</v>
<v Speaker 2>That depends which country.</v>

1415
01:18:23.970 --> 01:18:26.240
<v Speaker 2>If you're a person running the US or </v>
<v Speaker 2>China,</v>

1416
01:18:26.241 --> 01:18:27.074
<v Speaker 2>you,</v>
<v Speaker 2>you would have a different relationship </v>

1417
01:18:30.440 --> 01:18:31.273
<v Speaker 2>than if you're a person.</v>
<v Speaker 2>Like I know the Prime Minister of </v>

1418
01:18:32.871 --> 01:18:34.670
<v Speaker 2>Ethiopia,</v>
<v Speaker 2>I'll be Ahmed who's a,</v>

1419
01:18:34.940 --> 01:18:37.430
<v Speaker 2>has a degree in software engineering and</v>
<v Speaker 2>he,</v>

1420
01:18:37.450 --> 01:18:37.880
<v Speaker 2>he,</v>
<v Speaker 2>he,</v>

1421
01:18:37.880 --> 01:18:42.110
<v Speaker 2>he loves this,</v>
<v Speaker 2>but of course Ethiopia isn't in any day,</v>

1422
01:18:42.120 --> 01:18:43.250
<v Speaker 2>any other countries.</v>
<v Speaker 2>Right.</v>

1423
01:18:43.280 --> 01:18:44.113
<v Speaker 2>And they're not in any danger of,</v>
<v Speaker 2>of individually like taking global ai </v>

1424
01:18:47.991 --> 01:18:48.650
<v Speaker 2>hedge.</v>

1425
01:18:48.650 --> 01:18:51.650
<v Speaker 2>So for the majority of countries in the </v>
<v Speaker 2>world,</v>

1426
01:18:52.100 --> 01:18:55.120
<v Speaker 2>they like this for the same reason they </v>
<v Speaker 2>liked Linux,</v>

1427
01:18:55.140 --> 01:18:56.150
<v Speaker 2>right?</v>
<v Speaker 2>I mean,</v>

1428
01:18:56.151 --> 01:18:56.984
<v Speaker 2>I mean this,</v>
<v Speaker 2>this is something which they have an </v>

1429
01:18:58.431 --> 01:19:00.470
<v Speaker 2>equal role to anybody else,</v>
<v Speaker 2>right?</v>

1430
01:19:00.590 --> 01:19:01.423
<v Speaker 2>The superpowers,</v>
<v Speaker 2>and you see this among companies also </v>

1431
01:19:04.221 --> 01:19:05.054
<v Speaker 2>those.</v>
<v Speaker 2>So a lot of big companies that we're </v>

1432
01:19:06.801 --> 01:19:10.610
<v Speaker 2>talking to,</v>
<v Speaker 2>they love the idea of this decentralized</v>

1433
01:19:10.611 --> 01:19:14.690
<v Speaker 2>ai fabric because I mean if you're not </v>
<v Speaker 2>Amazon,</v>

1434
01:19:14.691 --> 01:19:15.800
<v Speaker 2>Google,</v>
<v Speaker 2>Microsoft,</v>

1435
01:19:15.880 --> 01:19:16.950
<v Speaker 2>tencent,</v>
<v Speaker 2>facebook,</v>

1436
01:19:16.960 --> 01:19:19.580
<v Speaker 2>so on.</v>
<v Speaker 2>If you're another large corporation,</v>

1437
01:19:19.880 --> 01:19:20.713
<v Speaker 2>you don't necessarily want all your ai </v>
<v Speaker 2>and all your data to be going into one </v>

1438
01:19:24.681 --> 01:19:25.514
<v Speaker 2>of this handful of large ai companies.</v>
<v Speaker 2>You would rather have it be in the </v>

1439
01:19:29.650 --> 01:19:34.650
<v Speaker 2>secure decentralized platform and I mean</v>
<v Speaker 2>this is the same reason that you know,</v>

1440
01:19:35.300 --> 01:19:37.790
<v Speaker 2>Cisco and IBM,</v>
<v Speaker 2>they run on Linux.</v>

1441
01:19:37.791 --> 01:19:39.350
<v Speaker 2>They don't run on Microsoft.</v>
<v Speaker 2>Right?</v>

1442
01:19:39.351 --> 01:19:40.184
<v Speaker 2>So if,</v>
<v Speaker 2>if you're not one of the handful of </v>

1443
01:19:42.771 --> 01:19:47.771
<v Speaker 2>large governments or large corporations </v>
<v Speaker 2>that happened to be in a leading role in</v>

1444
01:19:48.681 --> 01:19:52.130
<v Speaker 2>the ai ecosystem,</v>
<v Speaker 2>then you would rather have this,</v>

1445
01:19:52.180 --> 01:19:56.030
<v Speaker 2>this equalizing in decentralized thing </v>
<v Speaker 2>because everyone gets to play.</v>

1446
01:19:56.930 --> 01:19:57.430
<v Speaker 2>Yeah.</v>
<v Speaker 2>What?</v>

1447
01:19:57.430 --> 01:20:00.380
<v Speaker 2>What would be the benefit of running on </v>
<v Speaker 2>Linux versus Microsoft?</v>

1448
01:20:01.300 --> 01:20:02.133
<v Speaker 2>Well,</v>
<v Speaker 2>you're at the behest of some other big </v>

1449
01:20:03.381 --> 01:20:04.630
<v Speaker 2>company.</v>
<v Speaker 2>I mean,</v>

1450
01:20:04.670 --> 01:20:05.503
<v Speaker 2>imagine if.</v>
<v Speaker 2>Imagine if you were cisco or gm or </v>

1451
01:20:10.051 --> 01:20:10.884
<v Speaker 2>something and all of your internal </v>
<v Speaker 2>machines are all your servers are </v>

1452
01:20:15.091 --> 01:20:15.924
<v Speaker 2>running in Microsoft.</v>
<v Speaker 2>What Microsoft increases their price or </v>

1453
01:20:19.241 --> 01:20:20.074
<v Speaker 2>are removed some feature.</v>
<v Speaker 2>Then you're totally at their behest and </v>

1454
01:20:25.890 --> 01:20:28.470
<v Speaker 2>with ai the same thing is true.</v>
<v Speaker 2>I mean,</v>

1455
01:20:28.471 --> 01:20:29.304
<v Speaker 2>if,</v>
<v Speaker 2>if you put all your data in some big </v>

1456
01:20:31.081 --> 01:20:31.914
<v Speaker 2>company's server farm and you're </v>
<v Speaker 2>analyzing all your data on their </v>

1457
01:20:34.981 --> 01:20:37.830
<v Speaker 2>algorithms and that's critical to your </v>
<v Speaker 2>business model,</v>

1458
01:20:37.831 --> 01:20:41.190
<v Speaker 2>what if they change their ai algorithm </v>
<v Speaker 2>in some way?</v>

1459
01:20:41.191 --> 01:20:45.310
<v Speaker 2>Then I'm in the.</v>
<v Speaker 2>Then your business is,</v>

1460
01:20:45.360 --> 01:20:48.150
<v Speaker 2>is basically controlled by this other </v>
<v Speaker 2>company.</v>

1461
01:20:48.151 --> 01:20:52.860
<v Speaker 2>So I'm in having a decentralized </v>
<v Speaker 2>platform in which you're,</v>

1462
01:20:53.440 --> 01:20:54.273
<v Speaker 2>you know,</v>
<v Speaker 2>an equal participant along with </v>

1463
01:20:55.471 --> 01:20:59.700
<v Speaker 2>everybody else is,</v>
<v Speaker 2>is actually a much better position to be</v>

1464
01:20:59.701 --> 01:20:59.880
<v Speaker 2>in.</v>

1465
01:20:59.880 --> 01:21:00.713
<v Speaker 2>And I think this,</v>
<v Speaker 2>this I think is why we can succeed with </v>

1466
01:21:06.721 --> 01:21:07.554
<v Speaker 2>this plan of having this decentralized </v>
<v Speaker 2>singularity net platform than the </v>

1467
01:21:11.841 --> 01:21:12.674
<v Speaker 2>singular,</v>
<v Speaker 2>the studio enterprise software company </v>

1468
01:21:15.210 --> 01:21:18.660
<v Speaker 2>which mediates between the decentralized</v>
<v Speaker 2>platform and big companies.</v>

1469
01:21:18.661 --> 01:21:22.470
<v Speaker 2>I mean it's because most companies and </v>
<v Speaker 2>governments in the world,</v>

1470
01:21:23.070 --> 01:21:24.840
<v Speaker 2>you know,</v>
<v Speaker 2>they don't want hedge.</v>

1471
01:21:24.841 --> 01:21:28.950
<v Speaker 2>I'm one of a few large governments and </v>
<v Speaker 2>corporations each either.</v>

1472
01:21:28.951 --> 01:21:31.470
<v Speaker 2>And you can see this in,</v>
<v Speaker 2>in a lot of voids.</v>

1473
01:21:31.471 --> 01:21:32.304
<v Speaker 2>You can see this embrace of,</v>
<v Speaker 2>of Linux and etherium by many large </v>

1474
01:21:36.451 --> 01:21:37.284
<v Speaker 2>corporations.</v>
<v Speaker 2>You can also see like in a different </v>

1475
01:21:39.601 --> 01:21:41.220
<v Speaker 2>way,</v>
<v Speaker 2>you know,</v>

1476
01:21:41.420 --> 01:21:43.700
<v Speaker 2>the Indian government,</v>
<v Speaker 2>you know,</v>

1477
01:21:43.880 --> 01:21:44.713
<v Speaker 2>they rejected an offer by facebook to </v>
<v Speaker 2>give free internet to all Indians </v>

1478
01:21:48.630 --> 01:21:49.463
<v Speaker 2>because facebook wants to give like </v>
<v Speaker 2>mobile phones that would get free </v>

1479
01:21:51.781 --> 01:21:54.000
<v Speaker 2>internet,</v>
<v Speaker 2>but only to access facebook,</v>

1480
01:21:54.001 --> 01:21:55.590
<v Speaker 2>right?</v>
<v Speaker 2>India is like,</v>

1481
01:21:55.591 --> 01:21:56.280
<v Speaker 2>well no,</v>
<v Speaker 2>no,</v>

1482
01:21:56.280 --> 01:21:57.150
<v Speaker 2>no,</v>
<v Speaker 2>thanks.</v>

1483
01:21:57.151 --> 01:21:59.580
<v Speaker 2>Right,</v>
<v Speaker 2>and India is now giving.</v>

1484
01:21:59.730 --> 01:22:00.563
<v Speaker 2>They're now creating laws that any </v>
<v Speaker 2>internet company that collects data </v>

1485
01:22:04.531 --> 01:22:07.110
<v Speaker 2>about Indian people has to store that </v>
<v Speaker 2>data in India,</v>

1486
01:22:07.800 --> 01:22:10.440
<v Speaker 2>which is so the Indian government can </v>
<v Speaker 2>subpoena that data when,</v>

1487
01:22:10.470 --> 01:22:11.040
<v Speaker 2>when,</v>
<v Speaker 2>when,</v>

1488
01:22:11.040 --> 01:22:12.300
<v Speaker 2>when they want to.</v>
<v Speaker 2>So,</v>

1489
01:22:12.570 --> 01:22:13.403
<v Speaker 2>so you're,</v>
<v Speaker 2>you're already seeing a bunch of </v>

1490
01:22:15.930 --> 01:22:16.763
<v Speaker 2>resistance against hedge pneumoniae by a</v>
<v Speaker 2>few large governments or large large </v>

1491
01:22:20.971 --> 01:22:24.300
<v Speaker 2>corporations by other companies and </v>
<v Speaker 2>other governments.</v>

1492
01:22:24.301 --> 01:22:28.580
<v Speaker 2>And I think this is very positive and is</v>
<v Speaker 2>one of the factors that can,</v>

1493
01:22:28.650 --> 01:22:29.483
<v Speaker 2>that can foster the foster,</v>
<v Speaker 2>the growth of a decentralized ai </v>

1494
01:22:33.061 --> 01:22:33.894
<v Speaker 2>ecosystem.</v>
<v Speaker 2>Is it fair to say that the future of ai </v>

1495
01:22:37.950 --> 01:22:42.210
<v Speaker 2>is severely dependent upon who launches </v>
<v Speaker 2>it first?</v>

1496
01:22:43.830 --> 01:22:45.690
<v Speaker 2>Like whoever,</v>
<v Speaker 2>whether it's singularity,</v>

1497
01:22:46.080 --> 01:22:48.850
<v Speaker 2>the bottom line is official general </v>
<v Speaker 2>intelligence.</v>

1498
01:22:48.870 --> 01:22:51.630
<v Speaker 2>The bottom line is as a scientist after </v>
<v Speaker 2>say,</v>

1499
01:22:51.631 --> 01:22:52.970
<v Speaker 2>we don't know,</v>
<v Speaker 2>right?</v>

1500
01:22:53.160 --> 01:22:53.993
<v Speaker 2>It could be there's an end state that </v>
<v Speaker 2>Agi will just self organize into almost </v>

1501
01:23:00.601 --> 01:23:01.434
<v Speaker 2>independent of the initial condition,</v>
<v Speaker 2>but we don't know and given that we </v>

1502
01:23:06.581 --> 01:23:09.270
<v Speaker 2>don't know,</v>
<v Speaker 2>I'm operating under the,</v>

1503
01:23:09.560 --> 01:23:09.970
<v Speaker 2>you know,</v>
<v Speaker 2>the,</v>

1504
01:23:09.970 --> 01:23:14.970
<v Speaker 2>your ristick assumption that if the </v>
<v Speaker 2>first ai is beneficially oriented,</v>

1505
01:23:18.371 --> 01:23:19.204
<v Speaker 2>if it's controlled in the participatory </v>
<v Speaker 2>democratic way and if it's oriented at </v>

1506
01:23:23.411 --> 01:23:27.610
<v Speaker 2>least substantially towards like doing </v>
<v Speaker 2>good things for humans,</v>

1507
01:23:28.240 --> 01:23:29.073
<v Speaker 2>I'm operating under the eucharistic </v>
<v Speaker 2>assumption that this is going to bias </v>

1508
01:23:32.680 --> 01:23:34.940
<v Speaker 2>things in a positive,</v>
<v Speaker 2>positive direction.</v>

1509
01:23:35.340 --> 01:23:39.310
<v Speaker 2>I'm in chorus in the absence of </v>
<v Speaker 2>knowledge to the contrary.</v>

1510
01:23:39.640 --> 01:23:41.770
<v Speaker 2>But if the Chinese government launches </v>
<v Speaker 2>one,</v>

1511
01:23:42.390 --> 01:23:45.550
<v Speaker 2>they're controlling.</v>
<v Speaker 2>We don't pop it off first.</v>

1512
01:23:45.551 --> 01:23:46.384
<v Speaker 2>I mean,</v>
<v Speaker 2>I liked the idea that you're saying </v>

1513
01:23:47.141 --> 01:23:48.910
<v Speaker 2>though,</v>
<v Speaker 2>that it might organize itself.</v>

1514
01:23:49.300 --> 01:23:52.750
<v Speaker 2>I mean understand the Chinese government</v>
<v Speaker 2>also.</v>

1515
01:23:52.751 --> 01:23:53.584
<v Speaker 2>They want,</v>
<v Speaker 2>they want the best for the Chinese </v>

1516
01:23:56.291 --> 01:23:57.124
<v Speaker 2>people that they don't,</v>
<v Speaker 2>they don't want to make the terminator </v>

1517
01:23:59.861 --> 01:24:00.640
<v Speaker 2>either.</v>
<v Speaker 2>Right.</v>

1518
01:24:00.640 --> 01:24:01.130
<v Speaker 2>So,</v>
<v Speaker 2>uh,</v>

1519
01:24:01.130 --> 01:24:02.180
<v Speaker 2>I mean,</v>
<v Speaker 2>uh,</v>

1520
01:24:02.230 --> 01:24:06.970
<v Speaker 2>I think even even Donald Trump was not </v>
<v Speaker 2>my favorite person doesn't actually want</v>

1521
01:24:06.971 --> 01:24:08.680
<v Speaker 2>to kill off everyone on the planet.</v>
<v Speaker 2>Right.</v>

1522
01:24:08.681 --> 01:24:10.600
<v Speaker 2>So he might,</v>
<v Speaker 2>if they talk shit about them.</v>

1523
01:24:10.660 --> 01:24:11.111
<v Speaker 2>Yeah,</v>
<v Speaker 2>yeah,</v>

1524
01:24:11.111 --> 01:24:12.130
<v Speaker 2>yeah.</v>
<v Speaker 2>You know,</v>

1525
01:24:12.380 --> 01:24:16.290
<v Speaker 2>you never know it was just him.</v>
<v Speaker 2>I told you all.</v>

1526
01:24:16.580 --> 01:24:17.380
<v Speaker 2>I mean,</v>
<v Speaker 2>I,</v>

1527
01:24:17.380 --> 01:24:19.870
<v Speaker 2>I think,</v>
<v Speaker 2>you know,</v>

1528
01:24:19.871 --> 01:24:24.871
<v Speaker 2>I wouldn't say we're necessarily doomed </v>
<v Speaker 2>if big governments and big companies are</v>

1529
01:24:25.721 --> 01:24:30.721
<v Speaker 2>the ones that develop ai or Agi first </v>
<v Speaker 2>big government,</v>

1530
01:24:31.181 --> 01:24:32.920
<v Speaker 2>big companies essentially developed the </v>
<v Speaker 2>internet.</v>

1531
01:24:32.920 --> 01:24:34.720
<v Speaker 2>Right.</v>
<v Speaker 2>And it got away from them.</v>

1532
01:24:34.750 --> 01:24:35.860
<v Speaker 2>That's right.</v>
<v Speaker 2>That's right.</v>

1533
01:24:35.861 --> 01:24:38.200
<v Speaker 2>So there's a lot of uncertainty all </v>
<v Speaker 2>around,</v>

1534
01:24:38.590 --> 01:24:39.520
<v Speaker 2>but I think,</v>
<v Speaker 2>you know,</v>

1535
01:24:39.521 --> 01:24:40.354
<v Speaker 2>it behooves us to do what we can to bias</v>
<v Speaker 2>the odds in our favor based on our </v>

1536
01:24:44.471 --> 01:24:45.304
<v Speaker 2>current understanding.</v>
<v Speaker 2>And I mean toward that end we're </v>

1537
01:24:50.141 --> 01:24:51.070
<v Speaker 2>developing,</v>
<v Speaker 2>you know,</v>

1538
01:24:51.071 --> 01:24:54.580
<v Speaker 2>open source,</v>
<v Speaker 2>decentralized ai and singularity in that</v>

1539
01:24:54.581 --> 01:24:56.890
<v Speaker 2>process.</v>
<v Speaker 2>So if you would explain some singularity</v>

1540
01:24:56.891 --> 01:24:59.320
<v Speaker 2>net and what,</v>
<v Speaker 2>what you guys are actively involved in.</v>

1541
01:24:59.390 --> 01:25:00.223
<v Speaker 2>Sure,</v>
<v Speaker 2>sure.</v>

1542
01:25:01.100 --> 01:25:01.933
<v Speaker 2>So singularity and that in itself is a </v>
<v Speaker 2>platform that allows many different ais </v>

1543
01:25:08.981 --> 01:25:13.981
<v Speaker 2>to operate on it and these ais can offer</v>
<v Speaker 2>services to anyone who requests services</v>

1544
01:25:15.731 --> 01:25:20.731
<v Speaker 2>of the network and they can also request</v>
<v Speaker 2>an offer services among each other.</v>

1545
01:25:22.630 --> 01:25:26.260
<v Speaker 2>So it's,</v>
<v Speaker 2>it's both just an online marketplace for</v>

1546
01:25:26.261 --> 01:25:27.094
<v Speaker 2>ais,</v>
<v Speaker 2>much like no the apple app store or </v>

1547
01:25:29.891 --> 01:25:33.010
<v Speaker 2>Google play store,</v>
<v Speaker 2>but for ais rather than phone ups,</v>

1548
01:25:33.430 --> 01:25:38.230
<v Speaker 2>but the difference is the different ais </v>
<v Speaker 2>in here can outsource work to each other</v>

1549
01:25:38.231 --> 01:25:41.380
<v Speaker 2>and talk to each other and that gives a </v>
<v Speaker 2>new dimension to it,</v>

1550
01:25:41.381 --> 01:25:42.214
<v Speaker 2>right where you can have,</v>
<v Speaker 2>we think of as a society or economy of </v>

1551
01:25:45.431 --> 01:25:46.264
<v Speaker 2>minds and it gives the possibility that </v>
<v Speaker 2>this whole society of interacting ais </v>

1552
01:25:51.880 --> 01:25:55.980
<v Speaker 2>which are then they're paying each other</v>
<v Speaker 2>for transactions with our,</v>

1553
01:25:56.030 --> 01:25:59.350
<v Speaker 2>our digital money or are cryptographic </v>
<v Speaker 2>token,</v>

1554
01:25:59.351 --> 01:26:01.060
<v Speaker 2>which is called the Agi token.</v>

1555
01:26:01.450 --> 01:26:02.283
<v Speaker 2>So these ais which are paying each other</v>
<v Speaker 2>and rating each other of how good they </v>

1556
01:26:05.751 --> 01:26:06.584
<v Speaker 2>are,</v>
<v Speaker 2>sending data and questions and answers </v>

1557
01:26:07.941 --> 01:26:08.774
<v Speaker 2>to each other,</v>
<v Speaker 2>can self organize into some overall ai </v>

1558
01:26:12.471 --> 01:26:13.340
<v Speaker 2>mind?</v>
<v Speaker 2>No.</v>

1559
01:26:13.790 --> 01:26:17.990
<v Speaker 2>We're building this platform and then </v>
<v Speaker 2>we're plugging into it to seed it.</v>

1560
01:26:17.991 --> 01:26:18.824
<v Speaker 2>A bunch of ais of our own creation.</v>
<v Speaker 2>So I've been working for 10 years on </v>

1561
01:26:22.401 --> 01:26:25.130
<v Speaker 2>this open source ai project called open </v>
<v Speaker 2>cog,</v>

1562
01:26:25.520 --> 01:26:29.900
<v Speaker 2>which is oriented toward building </v>
<v Speaker 2>general intelligence and we're putting a</v>

1563
01:26:29.901 --> 01:26:30.734
<v Speaker 2>bunch of ai agents based on the open cog</v>
<v Speaker 2>platform into this singularity in the </v>

1564
01:26:36.800 --> 01:26:41.210
<v Speaker 2>network and you know if we're successful</v>
<v Speaker 2>in a couple of years,</v>

1565
01:26:41.211 --> 01:26:44.690
<v Speaker 2>the ais that we put on there will be a </v>
<v Speaker 2>tiny minority of what's in there,</v>

1566
01:26:44.930 --> 01:26:48.130
<v Speaker 2>just like the apps made by Google or a </v>
<v Speaker 2>small minority of the apps in,</v>

1567
01:26:48.190 --> 01:26:49.820
<v Speaker 2>in the,</v>
<v Speaker 2>in the Google play store.</v>

1568
01:26:49.821 --> 01:26:53.540
<v Speaker 2>Right.</v>
<v Speaker 2>But my hope is that these open cog,</v>

1569
01:26:53.570 --> 01:26:54.403
<v Speaker 2>ai agents within the larger pool of Ais </v>
<v Speaker 2>and the singularity net can sort of </v>

1570
01:27:00.060 --> 01:27:00.893
<v Speaker 2>serve as the general intelligence corps </v>
<v Speaker 2>because the open ai agents are really </v>

1571
01:27:05.271 --> 01:27:08.720
<v Speaker 2>good at abstraction and generalization </v>
<v Speaker 2>and creativity.</v>

1572
01:27:09.020 --> 01:27:09.853
<v Speaker 2>We can put another,</v>
<v Speaker 2>a bunch of other ais in there that are </v>

1573
01:27:11.481 --> 01:27:14.960
<v Speaker 2>good at highly specific for forms of </v>
<v Speaker 2>learning,</v>

1574
01:27:14.961 --> 01:27:18.350
<v Speaker 2>like predicting financial time series,</v>
<v Speaker 2>curing diseases,</v>

1575
01:27:18.351 --> 01:27:19.184
<v Speaker 2>answering people's questions,</v>
<v Speaker 2>organizing your inbox so you can have </v>

1576
01:27:22.011 --> 01:27:24.440
<v Speaker 2>the interaction of these specialized </v>
<v Speaker 2>ais.</v>

1577
01:27:24.770 --> 01:27:27.200
<v Speaker 2>And then more general purpose.</v>
<v Speaker 2>You know,</v>

1578
01:27:27.201 --> 01:27:28.034
<v Speaker 2>abstraction and creativity based.</v>
<v Speaker 2>Ai is like open cog agents all </v>

1579
01:27:31.191 --> 01:27:32.024
<v Speaker 2>interacting together in this </v>
<v Speaker 2>decentralized platform and then you </v>

1580
01:27:36.071 --> 01:27:37.700
<v Speaker 2>know,</v>
<v Speaker 2>the beauty of it is like some,</v>

1581
01:27:37.880 --> 01:27:38.713
<v Speaker 2>some 15 year old genius and Azerbaijan </v>
<v Speaker 2>or the Congo can put some brilliant ai </v>

1582
01:27:43.731 --> 01:27:46.610
<v Speaker 2>into this network.</v>
<v Speaker 2>If it's really smart,</v>

1583
01:27:47.000 --> 01:27:49.970
<v Speaker 2>it will get rated highly by the other </v>
<v Speaker 2>ais for,</v>

1584
01:27:50.000 --> 01:27:51.050
<v Speaker 2>for,</v>
<v Speaker 2>for its work,</v>

1585
01:27:51.051 --> 01:27:54.980
<v Speaker 2>helping them do their thing.</v>
<v Speaker 2>Then it can get replicated over and over</v>

1586
01:27:54.981 --> 01:27:55.814
<v Speaker 2>again across many servers.</v>
<v Speaker 2>Suddenly a this 16 year old kid from </v>

1587
01:27:59.631 --> 01:28:02.930
<v Speaker 2>Azerbaijan or the Congo could become </v>
<v Speaker 2>wealthy from,</v>

1588
01:28:02.990 --> 01:28:07.990
<v Speaker 2>from their copies of their ai providing </v>
<v Speaker 2>services to other people's ais and be,</v>

1589
01:28:08.320 --> 01:28:09.153
<v Speaker 2>you know,</v>
<v Speaker 2>the creativity in their mind is out </v>

1590
01:28:10.461 --> 01:28:14.750
<v Speaker 2>there and is infusing this global aid </v>
<v Speaker 2>network with some,</v>

1591
01:28:15.200 --> 01:28:16.033
<v Speaker 2>some new intellectual DNA that you know,</v>
<v Speaker 2>never would have been found by a ten </v>

1592
01:28:20.710 --> 01:28:21.543
<v Speaker 2>cent or a google because they're not </v>
<v Speaker 2>going to hire some Congolese teenager </v>

1593
01:28:25.281 --> 01:28:27.050
<v Speaker 2>who may have a brilliant ai idea.</v>

1594
01:28:27.650 --> 01:28:28.790
<v Speaker 2>That's amazing.</v>

1595
01:28:28.850 --> 01:28:33.850
<v Speaker 1>That's amazing.</v>
<v Speaker 1>So this is all ongoing right now.</v>

1596
01:28:34.370 --> 01:28:37.400
<v Speaker 1>And the singularity that you guys are </v>
<v Speaker 1>using,</v>

1597
01:28:37.720 --> 01:28:40.010
<v Speaker 1>the.</v>
<v Speaker 1>The way I've understood that term,</v>

1598
01:28:40.160 --> 01:28:40.993
<v Speaker 1>correct me if I'm wrong,</v>
<v Speaker 1>is that it's going to be the one </v>

1599
01:28:43.490 --> 01:28:47.510
<v Speaker 1>innovation or one invention that </v>
<v Speaker 1>essentially changes everything forever.</v>

1600
01:28:48.100 --> 01:28:50.620
<v Speaker 2>The singularity isn't necessarily one </v>
<v Speaker 2>invention.</v>

1601
01:28:50.621 --> 01:28:55.330
<v Speaker 2>The singularity,</v>
<v Speaker 2>which is coined by Wong,</v>

1602
01:28:55.670 --> 01:28:58.690
<v Speaker 2>is coined by my friend Vernor vinge,</v>
<v Speaker 2>who's another guy you should interview.</v>

1603
01:28:58.691 --> 01:29:01.830
<v Speaker 2>He's in San Diego to the other guys.</v>
<v Speaker 2>The other brilliant guys down there,</v>

1604
01:29:01.831 --> 01:29:05.040
<v Speaker 2>but vernor vinge military down there.</v>
<v Speaker 2>Yeah,</v>

1605
01:29:05.041 --> 01:29:06.020
<v Speaker 2>vernor vinge.</v>
<v Speaker 2>Uh,</v>

1606
01:29:06.070 --> 01:29:07.590
<v Speaker 2>he,</v>
<v Speaker 2>he was a math professor.</v>

1607
01:29:07.630 --> 01:29:12.000
<v Speaker 2>The San Diego University actually,</v>
<v Speaker 2>but well known science fiction writer.</v>

1608
01:29:12.540 --> 01:29:15.990
<v Speaker 2>His book a fire upon the deep,</v>
<v Speaker 2>one of the great science fiction books.</v>

1609
01:29:15.991 --> 01:29:20.390
<v Speaker 2>So v I n g Vernor Vinge,</v>
<v Speaker 2>GE,</v>

1610
01:29:21.060 --> 01:29:26.060
<v Speaker 2>brilliant guy,</v>
<v Speaker 2>fire upon the deep Vernor v e r o v e R.</v>

1611
01:29:27.930 --> 01:29:28.770
<v Speaker 2>Yeah,</v>
<v Speaker 2>he's brilliant.</v>

1612
01:29:28.771 --> 01:29:33.150
<v Speaker 2>He coined the term technological </v>
<v Speaker 2>singularity back in the 19 eighties,</v>

1613
01:29:33.720 --> 01:29:36.590
<v Speaker 2>but he,</v>
<v Speaker 2>he opted not to become a,</v>

1614
01:29:36.670 --> 01:29:39.750
<v Speaker 2>a pundit about it because he'd rather </v>
<v Speaker 2>more science fiction.</v>

1615
01:29:40.080 --> 01:29:41.970
<v Speaker 2>That's interesting.</v>
<v Speaker 2>Than a science fiction author.</v>

1616
01:29:42.000 --> 01:29:44.790
<v Speaker 2>Ray Kurzweil,</v>
<v Speaker 2>who's also a good friend of mine.</v>

1617
01:29:44.791 --> 01:29:45.624
<v Speaker 2>I mean Ray Ray took that term and </v>
<v Speaker 2>fleshed it out and did a bunch of data </v>

1618
01:29:51.421 --> 01:29:52.350
<v Speaker 2>analytics.</v>

1619
01:29:52.680 --> 01:29:55.830
<v Speaker 2>Trying to pinpoint like when it's,</v>
<v Speaker 2>it would happen,</v>

1620
01:29:55.831 --> 01:29:56.664
<v Speaker 2>but the basic concept of the </v>
<v Speaker 2>technological singularity is a point in </v>

1621
01:29:59.911 --> 01:30:04.911
<v Speaker 2>time when technological advance occurs </v>
<v Speaker 2>so rapidly that the human mind,</v>

1622
01:30:05.581 --> 01:30:07.420
<v Speaker 2>it appears almost instantaneous.</v>
<v Speaker 2>Like,</v>

1623
01:30:07.660 --> 01:30:12.660
<v Speaker 2>like imagine 10 new Nobel Prize winning </v>
<v Speaker 2>discoveries every second or something.</v>

1624
01:30:12.811 --> 01:30:16.080
<v Speaker 2>Right?</v>
<v Speaker 2>So this is similar to the concept of the</v>

1625
01:30:16.081 --> 01:30:20.060
<v Speaker 2>intelligence explosion that was posited </v>
<v Speaker 2>by the mathematician,</v>

1626
01:30:20.080 --> 01:30:23.100
<v Speaker 2>Ij good in 1965.</v>
<v Speaker 2>What Ij good said.</v>

1627
01:30:23.101 --> 01:30:23.934
<v Speaker 2>Then the year before I was born was the </v>
<v Speaker 2>first truly intelligent machine will be </v>

1628
01:30:28.021 --> 01:30:30.660
<v Speaker 2>the last invention that humanity needs </v>
<v Speaker 2>to make rent.</v>

1629
01:30:31.230 --> 01:30:32.063
<v Speaker 2>So this is an intelligence explosion is </v>
<v Speaker 2>another term for basically the same </v>

1630
01:30:36.061 --> 01:30:40.620
<v Speaker 2>thing as the technological singularity,</v>
<v Speaker 2>but it's not just about Ai.</v>

1631
01:30:40.950 --> 01:30:43.980
<v Speaker 2>Ai is just probably the most powerful </v>
<v Speaker 2>technology driving it.</v>

1632
01:30:43.981 --> 01:30:46.700
<v Speaker 2>I mean there's ai,</v>
<v Speaker 2>there's nanotechnology,</v>

1633
01:30:46.910 --> 01:30:47.743
<v Speaker 2>there's femto technology which will be </v>
<v Speaker 2>building things from elementary </v>

1634
01:30:50.701 --> 01:30:53.130
<v Speaker 2>particles.</v>
<v Speaker 2>I mean there's life extension,</v>

1635
01:30:53.430 --> 01:30:55.910
<v Speaker 2>genetic engineering,</v>
<v Speaker 2>mind uploading,</v>

1636
01:30:55.920 --> 01:30:56.753
<v Speaker 2>which is like reading the mind that if </v>
<v Speaker 2>your brain and putting it into a </v>

1637
01:31:00.840 --> 01:31:02.290
<v Speaker 2>machine,</v>
<v Speaker 2>you know,</v>

1638
01:31:02.291 --> 01:31:03.124
<v Speaker 2>there's advanced energy technologies so </v>
<v Speaker 2>that all these different things are </v>

1639
01:31:07.021 --> 01:31:10.230
<v Speaker 2>expected to advance at around the same </v>
<v Speaker 2>time.</v>

1640
01:31:10.230 --> 01:31:12.030
<v Speaker 2>And they have many ways to boost each </v>
<v Speaker 2>other,</v>

1641
01:31:12.031 --> 01:31:12.864
<v Speaker 2>right?</v>
<v Speaker 2>Because the,</v>

1642
01:31:13.040 --> 01:31:14.460
<v Speaker 2>you know,</v>
<v Speaker 2>the better Aiu have,</v>

1643
01:31:14.461 --> 01:31:18.000
<v Speaker 2>your ai can then invent new ways of </v>
<v Speaker 2>doing nanotech tech and biology.</v>

1644
01:31:18.300 --> 01:31:20.970
<v Speaker 2>But if you invent amazing new nanotech </v>
<v Speaker 2>in quantum computing,</v>

1645
01:31:20.971 --> 01:31:23.520
<v Speaker 2>that can make your ai smarter.</v>
<v Speaker 2>On the other hand,</v>

1646
01:31:23.521 --> 01:31:24.354
<v Speaker 2>if you,</v>
<v Speaker 2>if you could crack how the human brain </v>

1647
01:31:25.771 --> 01:31:29.310
<v Speaker 2>works and genetic engineering to upgrade</v>
<v Speaker 2>human intelligence,</v>

1648
01:31:29.520 --> 01:31:32.520
<v Speaker 2>those smarter than humans can then make </v>
<v Speaker 2>better eyes and nano technology,</v>

1649
01:31:32.521 --> 01:31:36.150
<v Speaker 2>right?</v>
<v Speaker 2>So there's so many virtuous cycles among</v>

1650
01:31:36.151 --> 01:31:39.390
<v Speaker 2>these different technologies.</v>
<v Speaker 2>The more you advance in any of them,</v>

1651
01:31:39.391 --> 01:31:42.180
<v Speaker 2>the more you're going to advance and in,</v>
<v Speaker 2>in all of them.</v>

1652
01:31:42.390 --> 01:31:45.240
<v Speaker 2>And it's the coming together of all of </v>
<v Speaker 2>these that's going to create,</v>

1653
01:31:45.750 --> 01:31:46.583
<v Speaker 2>you know,</v>
<v Speaker 2>radical abundance and the technological </v>

1654
01:31:50.460 --> 01:31:53.130
<v Speaker 2>singularity so that,</v>
<v Speaker 2>that term,</v>

1655
01:31:53.131 --> 01:31:53.964
<v Speaker 2>which Vernor vinge,</v>
<v Speaker 2>he introduced ray Ray Kurzweil borrowed </v>

1656
01:31:57.691 --> 01:32:02.140
<v Speaker 2>for his books and further singular the </v>
<v Speaker 2>university educational program.</v>

1657
01:32:02.440 --> 01:32:05.410
<v Speaker 2>And then we borrowed that for our </v>
<v Speaker 2>singularity net,</v>

1658
01:32:05.500 --> 01:32:06.333
<v Speaker 2>like decentralized blockchain based ai </v>
<v Speaker 2>platform and in our singularity studio </v>

1659
01:32:11.800 --> 01:32:12.633
<v Speaker 2>enterprise software company.</v>
<v Speaker 2>Now I want to talk to you about two </v>

1660
01:32:14.771 --> 01:32:15.604
<v Speaker 2>parts of what you just said.</v>
<v Speaker 2>One being the possibility that one day </v>

1661
01:32:18.521 --> 01:32:21.910
<v Speaker 2>we can upload our mind will make copies </v>
<v Speaker 2>of our mind.</v>

1662
01:32:22.230 --> 01:32:23.063
<v Speaker 2>You're up for it to upload into here.</v>
<v Speaker 2>I could use a little Joe Rogan them on </v>

1663
01:32:28.041 --> 01:32:29.740
<v Speaker 2>my phone.</v>
<v Speaker 2>You can just call me dude,</v>

1664
01:32:31.030 --> 01:32:31.863
<v Speaker 2>the organic version,</v>
<v Speaker 2>but the what do you think that that's a </v>

1665
01:32:36.041 --> 01:32:36.874
<v Speaker 2>real possibility inside of our lifetime </v>
<v Speaker 2>that we can map out the human mind to </v>

1666
01:32:40.360 --> 01:32:42.700
<v Speaker 2>the point where we can essentially </v>
<v Speaker 2>recreate it,</v>

1667
01:32:43.210 --> 01:32:44.043
<v Speaker 2>but if you do recreate it without all </v>
<v Speaker 2>the biological urges and the human </v>

1668
01:32:47.051 --> 01:32:49.690
<v Speaker 2>reward systems that are built in,</v>
<v Speaker 2>what the fuck are we?</v>

1669
01:32:49.691 --> 01:32:50.524
<v Speaker 2>I mean,</v>
<v Speaker 2>well that's a different question I </v>

1670
01:32:52.031 --> 01:32:53.470
<v Speaker 2>think.</v>
<v Speaker 2>What is your mind?</v>

1671
01:32:54.490 --> 01:32:55.061
<v Speaker 2>Well,</v>
<v Speaker 2>I,</v>

1672
01:32:55.061 --> 01:32:57.610
<v Speaker 2>I think that those two things that are </v>
<v Speaker 2>needed for,</v>

1673
01:32:58.360 --> 01:32:59.193
<v Speaker 2>let's say,</v>
<v Speaker 2>let's say human body uploading to </v>

1674
01:33:01.930 --> 01:33:03.730
<v Speaker 2>simplify things,</v>
<v Speaker 2>body uploading.</v>

1675
01:33:03.760 --> 01:33:04.593
<v Speaker 2>There are two things that are needed.</v>
<v Speaker 2>One thing is a better computing </v>

1676
01:33:08.711 --> 01:33:09.544
<v Speaker 2>infrastructure.</v>
<v Speaker 2>Then we have no to host the uploaded </v>

1677
01:33:12.101 --> 01:33:16.030
<v Speaker 2>body and,</v>
<v Speaker 2>and the other thing is a better scanning</v>

1678
01:33:16.060 --> 01:33:16.893
<v Speaker 2>technology because right now we don't </v>
<v Speaker 2>have a way to scan the molecular </v>

1679
01:33:20.651 --> 01:33:23.490
<v Speaker 2>structure of your body without like </v>
<v Speaker 2>freezing you,</v>

1680
01:33:23.491 --> 01:33:24.324
<v Speaker 2>slicing you in scanning you,</v>
<v Speaker 2>which you probably don't want that at </v>

1681
01:33:26.880 --> 01:33:28.240
<v Speaker 2>this point in time,</v>
<v Speaker 2>right yet.</v>

1682
01:33:28.450 --> 01:33:29.283
<v Speaker 2>So assuming both those are solved and </v>
<v Speaker 2>you can then recreate in some computer </v>

1683
01:33:34.151 --> 01:33:36.880
<v Speaker 2>simulation,</v>
<v Speaker 2>you know,</v>

1684
01:33:36.881 --> 01:33:38.920
<v Speaker 2>uh,</v>
<v Speaker 2>an accurate simulate Chromo of,</v>

1685
01:33:38.921 --> 01:33:39.520
<v Speaker 2>of,</v>
<v Speaker 2>of,</v>

1686
01:33:39.520 --> 01:33:41.140
<v Speaker 2>of what you are,</v>
<v Speaker 2>right?</v>

1687
01:33:41.590 --> 01:33:42.423
<v Speaker 2>That's where I'm getting this,</v>
<v Speaker 2>where I'm getting at an accurate </v>

1688
01:33:44.810 --> 01:33:45.643
<v Speaker 2>simulacrum is,</v>
<v Speaker 2>that's getting weird because the </v>

1689
01:33:47.471 --> 01:33:50.680
<v Speaker 2>biological variability of human beings,</v>
<v Speaker 2>we vary day to day.</v>

1690
01:33:50.860 --> 01:33:51.693
<v Speaker 2>We're very dependent upon it.</v>
<v Speaker 2>And your simulator and will also vary </v>

1691
01:33:53.951 --> 01:33:54.784
<v Speaker 2>day to day.</v>
<v Speaker 2>So the DVA program and into have flaws </v>

1692
01:33:58.090 --> 01:34:00.940
<v Speaker 2>because while we vary dependent upon how</v>
<v Speaker 2>much sleep we get,</v>

1693
01:34:00.941 --> 01:34:03.070
<v Speaker 2>whether or not we're feeling sick,</v>
<v Speaker 2>whether we're lonely.</v>

1694
01:34:03.071 --> 01:34:05.080
<v Speaker 2>So all these,</v>
<v Speaker 2>if you're upload,</v>

1695
01:34:05.081 --> 01:34:08.830
<v Speaker 2>we're an accurate copy of you than the </v>
<v Speaker 2>simulation.</v>

1696
01:34:08.831 --> 01:34:09.664
<v Speaker 2>Hosting your upload would need to have </v>
<v Speaker 2>an accurate simulation of the laws of </v>

1697
01:34:13.090 --> 01:34:18.040
<v Speaker 2>bio physics and chemistry that allow </v>
<v Speaker 2>your body to,</v>

1698
01:34:18.200 --> 01:34:20.170
<v Speaker 2>you know,</v>
<v Speaker 2>evolve from one second to the neck.</v>

1699
01:34:20.200 --> 01:34:24.010
<v Speaker 2>My concern is though would change second</v>
<v Speaker 2>by second,</v>

1700
01:34:24.011 --> 01:34:26.950
<v Speaker 2>just like just like you do and it would </v>
<v Speaker 2>divert from me.</v>

1701
01:34:26.951 --> 01:34:27.784
<v Speaker 2>Right?</v>
<v Speaker 2>So I mean after an hour it will be a </v>

1702
01:34:29.111 --> 01:34:32.690
<v Speaker 2>little different.</v>
<v Speaker 2>After a year it might've gone in,</v>

1703
01:34:32.700 --> 01:34:35.260
<v Speaker 2>in a quite different direction for you.</v>
<v Speaker 2>Probably a month.</v>

1704
01:34:35.640 --> 01:34:39.100
<v Speaker 2>Some Super God monk living on the top of</v>
<v Speaker 2>a mountain somewhere in a year.</v>

1705
01:34:39.370 --> 01:34:41.180
<v Speaker 2>Have it,</v>
<v Speaker 2>keeps the whole right.</v>

1706
01:34:41.340 --> 01:34:43.240
<v Speaker 2>Depends on what virtual world it's </v>
<v Speaker 2>living in.</v>

1707
01:34:43.241 --> 01:34:44.074
<v Speaker 2>True.</v>
<v Speaker 2>I mean if it's living in a virtual </v>

1708
01:34:45.191 --> 01:34:46.930
<v Speaker 2>world,</v>
<v Speaker 2>your world will be a virtual world.</v>

1709
01:34:46.960 --> 01:34:49.720
<v Speaker 2>If we're not talking about the potential</v>
<v Speaker 2>of downloading this.</v>

1710
01:34:49.721 --> 01:34:51.970
<v Speaker 2>Again,</v>
<v Speaker 2>in sort of into a biologic,</v>

1711
01:34:51.971 --> 01:34:53.520
<v Speaker 2>there's a lot of possibilities,</v>
<v Speaker 2>right?</v>

1712
01:34:53.540 --> 01:34:54.261
<v Speaker 2>I mean you,</v>
<v Speaker 2>you,</v>

1713
01:34:54.261 --> 01:34:57.320
<v Speaker 2>you could,</v>
<v Speaker 2>you could upload into a Joe Rogan living</v>

1714
01:34:57.321 --> 01:34:58.154
<v Speaker 2>in the virtual world and then just </v>
<v Speaker 2>create your own fantasy universe or you </v>

1715
01:35:02.030 --> 01:35:06.920
<v Speaker 2>or you could three d print and alternate</v>
<v Speaker 2>synthetic body.</v>

1716
01:35:06.980 --> 01:35:07.560
<v Speaker 2>Right?</v>
<v Speaker 2>I mean,</v>

1717
01:35:07.560 --> 01:35:08.393
<v Speaker 2>once you,</v>
<v Speaker 2>once you have the ability to manipulate </v>

1718
01:35:11.210 --> 01:35:12.043
<v Speaker 2>molecules at will,</v>
<v Speaker 2>the scope of possibilities becomes much </v>

1719
01:35:16.671 --> 01:35:18.500
<v Speaker 2>greater than we're used to.</v>
<v Speaker 2>Thinking about.</v>

1720
01:35:18.800 --> 01:35:22.460
<v Speaker 2>My question is do,</v>
<v Speaker 2>do we replicate flaws?</v>

1721
01:35:22.520 --> 01:35:25.220
<v Speaker 2>Do we replicate depression?</v>
<v Speaker 2>Of course,</v>

1722
01:35:25.340 --> 01:35:27.620
<v Speaker 2>but we knew that when we want to cure </v>
<v Speaker 2>depression,</v>

1723
01:35:27.830 --> 01:35:31.430
<v Speaker 2>so depression it.</v>
<v Speaker 2>Here's the interesting thing.</v>

1724
01:35:31.830 --> 01:35:36.830
<v Speaker 2>Once once we have you in a digital form,</v>
<v Speaker 2>then it's very programmable,</v>

1725
01:35:37.640 --> 01:35:38.473
<v Speaker 2>so then the dopamine and Serotonin,</v>
<v Speaker 2>then you can change what you want and </v>

1726
01:35:42.021 --> 01:35:44.420
<v Speaker 2>then you have a whole different set of </v>
<v Speaker 2>issues.</v>

1727
01:35:44.421 --> 01:35:45.254
<v Speaker 2>Right?</v>
<v Speaker 2>Yeah.</v>

1728
01:35:45.350 --> 01:35:46.183
<v Speaker 2>Because once you've changed,</v>
<v Speaker 2>I mean suppose you make a fork of </v>

1729
01:35:49.101 --> 01:35:49.934
<v Speaker 2>yourself and then you manipulate it in a</v>
<v Speaker 2>certain way and then after a few hours </v>

1730
01:35:54.801 --> 01:35:56.270
<v Speaker 2>you're like,</v>
<v Speaker 2>well I don't.</v>

1731
01:35:56.460 --> 01:36:00.140
<v Speaker 2>I don't much like this.</v>
<v Speaker 2>A new joe here.</v>

1732
01:36:00.380 --> 01:36:03.770
<v Speaker 2>Maybe we should roll back that change,</v>
<v Speaker 2>but the new joe is like,</v>

1733
01:36:03.771 --> 01:36:06.230
<v Speaker 2>well,</v>
<v Speaker 2>I liked myself very well.</v>

1734
01:36:06.231 --> 01:36:08.260
<v Speaker 2>Thank you.</v>
<v Speaker 2>So then,</v>

1735
01:36:08.720 --> 01:36:09.553
<v Speaker 2>yeah,</v>
<v Speaker 2>there's a lot of issues that that will </v>

1736
01:36:11.631 --> 01:36:14.390
<v Speaker 2>come up once we can write,</v>
<v Speaker 2>modify,</v>

1737
01:36:14.391 --> 01:36:15.224
<v Speaker 2>and reprogram ourselves to the point </v>
<v Speaker 2>that the ramifications of these </v>

1738
01:36:19.041 --> 01:36:21.890
<v Speaker 2>decisions are almost insurmountable d </v>
<v Speaker 2>once,</v>

1739
01:36:21.891 --> 01:36:24.350
<v Speaker 2>once the ball gets rolling.</v>
<v Speaker 2>Well,</v>

1740
01:36:24.351 --> 01:36:25.184
<v Speaker 2>the modifications of the ramifications </v>
<v Speaker 2>of these decisions are going to be very </v>

1741
01:36:28.941 --> 01:36:30.500
<v Speaker 2>interesting to explore.</v>

1742
01:36:30.560 --> 01:36:32.330
<v Speaker 2>Yes,</v>
<v Speaker 2>you're super positive band,</v>

1743
01:36:32.870 --> 01:36:36.440
<v Speaker 2>super positive of your optimism.</v>
<v Speaker 2>Many bad things will happen.</v>

1744
01:36:36.441 --> 01:36:38.210
<v Speaker 2>Many good things will happen.</v>
<v Speaker 2>That's a very.</v>

1745
01:36:38.211 --> 01:36:40.370
<v Speaker 2>That's a very easy prediction to me.</v>
<v Speaker 2>Okay.</v>

1746
01:36:40.371 --> 01:36:41.690
<v Speaker 2>I see what you're saying.</v>
<v Speaker 2>Yeah.</v>

1747
01:36:41.691 --> 01:36:44.810
<v Speaker 2>I've just a one day thing.</v>
<v Speaker 2>Think about like world travel,</v>

1748
01:36:45.490 --> 01:36:46.323
<v Speaker 2>like hundreds of years ago,</v>
<v Speaker 2>most people didn't travel more than a </v>

1749
01:36:49.430 --> 01:36:52.040
<v Speaker 2>very short distance from their home and </v>
<v Speaker 2>you could say,</v>

1750
01:36:52.041 --> 01:36:52.431
<v Speaker 2>well,</v>
<v Speaker 2>okay,</v>

1751
01:36:52.431 --> 01:36:55.100
<v Speaker 2>what if.</v>
<v Speaker 2>What if people could travel all over the</v>

1752
01:36:55.101 --> 01:36:55.880
<v Speaker 2>world,</v>
<v Speaker 2>right?</v>

1753
01:36:55.880 --> 01:36:56.713
<v Speaker 2>Like what horrible things could happen.</v>
<v Speaker 2>They would lose their culture like they </v>

1754
01:36:59.421 --> 01:37:02.180
<v Speaker 2>might go marry someone from from a </v>
<v Speaker 2>random tribe.</v>

1755
01:37:02.660 --> 01:37:05.570
<v Speaker 2>You can get killed in the Arctic region </v>
<v Speaker 2>or something.</v>

1756
01:37:05.740 --> 01:37:08.660
<v Speaker 2>A lot of bad things can happen when you </v>
<v Speaker 2>travel far from your home.</v>

1757
01:37:08.840 --> 01:37:09.673
<v Speaker 2>A lot of good things can happen and the </v>
<v Speaker 2>ultimately the ramifications were not </v>

1758
01:37:13.701 --> 01:37:16.820
<v Speaker 2>foreseen by people 500 years ago.</v>
<v Speaker 2>I'm in,</v>

1759
01:37:17.180 --> 01:37:19.280
<v Speaker 2>we're going into a lot of new domains.</v>

1760
01:37:19.670 --> 01:37:24.620
<v Speaker 2>We can't see the details of the pluses </v>
<v Speaker 2>and minuses that are going to unfold.</v>

1761
01:37:25.260 --> 01:37:26.093
<v Speaker 2>A W,</v>
<v Speaker 2>it would behoove us to simply become </v>

1762
01:37:28.011 --> 01:37:28.844
<v Speaker 2>comfortable with radical uncertainty </v>
<v Speaker 2>because otherwise we're going to </v>

1763
01:37:31.311 --> 01:37:33.530
<v Speaker 2>confront it anyway and we're just going </v>
<v Speaker 2>to be nervous.</v>

1764
01:37:33.650 --> 01:37:35.570
<v Speaker 2>So it's just inevitable this,</v>
<v Speaker 2>this,</v>

1765
01:37:35.571 --> 01:37:37.190
<v Speaker 2>uh,</v>
<v Speaker 2>it's almost inevitable.</v>

1766
01:37:37.191 --> 01:37:38.330
<v Speaker 2>I mean,</v>
<v Speaker 2>of course.</v>

1767
01:37:38.331 --> 01:37:39.920
<v Speaker 2>Sorry.</v>
<v Speaker 2>Any natural disaster.</v>

1768
01:37:40.390 --> 01:37:41.120
<v Speaker 2>Yeah.</v>
<v Speaker 2>I mean,</v>

1769
01:37:41.120 --> 01:37:41.953
<v Speaker 2>of course trump could start a nuclear </v>
<v Speaker 2>war and then we're resetting to ground </v>

1770
01:37:45.021 --> 01:37:47.420
<v Speaker 2>zero where we get hit by an asteroid,</v>
<v Speaker 2>right?</v>

1771
01:37:48.470 --> 01:37:49.303
<v Speaker 2>Yeah.</v>
<v Speaker 2>I mean,</v>

1772
01:37:49.580 --> 01:37:54.580
<v Speaker 2>so barring a catastrophic outcome,</v>
<v Speaker 2>I believe a technological singularity is</v>

1773
01:37:55.800 --> 01:37:56.633
<v Speaker 2>essentially inevitable.</v>
<v Speaker 2>There's a radical uncertainty attached </v>

1774
01:38:01.711 --> 01:38:03.390
<v Speaker 2>to this.</v>
<v Speaker 2>On the other hand,</v>

1775
01:38:04.340 --> 01:38:05.173
<v Speaker 2>you know,</v>
<v Speaker 2>in as much as we humans can know </v>

1776
01:38:06.301 --> 01:38:07.134
<v Speaker 2>anything,</v>
<v Speaker 2>it would seem common sensically there's </v>

1777
01:38:10.381 --> 01:38:15.381
<v Speaker 2>the ability to bias this in a positive </v>
<v Speaker 2>rather than the negative direction.</v>

1778
01:38:16.560 --> 01:38:21.390
<v Speaker 2>We should be spending more of our </v>
<v Speaker 2>attention on doing that rather than,</v>

1779
01:38:21.391 --> 01:38:22.224
<v Speaker 2>for instance,</v>
<v Speaker 2>advertising spying and making chocolate </v>

1780
01:38:25.470 --> 01:38:28.430
<v Speaker 2>chocolates and all the other things </v>
<v Speaker 2>people are doing now.</v>

1781
01:38:28.430 --> 01:38:29.310
<v Speaker 2>I mean,</v>
<v Speaker 2>it's prevalent,</v>

1782
01:38:29.311 --> 01:38:30.144
<v Speaker 2>it's everywhere,</v>
<v Speaker 2>but I mean how many people are actually </v>

1783
01:38:31.411 --> 01:38:32.244
<v Speaker 2>at the helm of that as opposed to how </v>
<v Speaker 2>many people are working on various </v>

1784
01:38:35.731 --> 01:38:38.160
<v Speaker 2>aspects of technology all across the </v>
<v Speaker 2>planet.</v>

1785
01:38:38.250 --> 01:38:42.210
<v Speaker 2>It's a small group and in comparison,</v>
<v Speaker 2>working on explicitly bringing about the</v>

1786
01:38:42.211 --> 01:38:45.960
<v Speaker 2>singularity is a small group.</v>
<v Speaker 2>On the other hand,</v>

1787
01:38:46.680 --> 01:38:49.680
<v Speaker 2>supporting technologies is a very large </v>
<v Speaker 2>group,</v>

1788
01:38:49.681 --> 01:38:52.920
<v Speaker 2>so think about like gps.</v>
<v Speaker 2>Where did they come from?</v>

1789
01:38:53.190 --> 01:38:54.900
<v Speaker 2>Accelerating gaming,</v>
<v Speaker 2>right?</v>

1790
01:38:55.350 --> 01:38:56.183
<v Speaker 2>Lo and behold,</v>
<v Speaker 2>they're amazingly useful for training </v>

1791
01:38:58.441 --> 01:38:59.274
<v Speaker 2>neural net models,</v>
<v Speaker 2>which is the one among many important </v>

1792
01:39:01.501 --> 01:39:02.730
<v Speaker 2>types of Ai,</v>
<v Speaker 2>right?</v>

1793
01:39:02.731 --> 01:39:03.564
<v Speaker 2>So a large amount of the planet's </v>
<v Speaker 2>resources are now getting spent on </v>

1794
01:39:07.351 --> 01:39:08.184
<v Speaker 2>technologies that are indirectly </v>
<v Speaker 2>supporting these singular attarian </v>

1795
01:39:12.060 --> 01:39:14.070
<v Speaker 2>technology.</v>
<v Speaker 2>So as another example,</v>

1796
01:39:14.090 --> 01:39:18.540
<v Speaker 2>like microarrays and let you measure the</v>
<v Speaker 2>expression level of genes,</v>

1797
01:39:18.830 --> 01:39:22.170
<v Speaker 2>how much each gene is doing in your body</v>
<v Speaker 2>at each point in time.</v>

1798
01:39:22.500 --> 01:39:24.970
<v Speaker 2>These were originally developed,</v>
<v Speaker 2>you know,</v>

1799
01:39:25.140 --> 01:39:25.973
<v Speaker 2>as an outgrowth of printing technology.</v>
<v Speaker 2>Then instead of squirting ink after </v>

1800
01:39:29.461 --> 01:39:31.530
<v Speaker 2>metrics figured out you could squirt </v>
<v Speaker 2>DNA.</v>

1801
01:39:31.590 --> 01:39:32.423
<v Speaker 2>Right?</v>
<v Speaker 2>So I mean the amount of technology </v>

1802
01:39:35.310 --> 01:39:36.143
<v Speaker 2>specifically oriented toward the </v>
<v Speaker 2>singularity doesn't have to be large </v>

1803
01:39:39.961 --> 01:39:44.961
<v Speaker 2>because the overall spectrum of </v>
<v Speaker 2>supporting technologies can be subverted</v>

1804
01:39:45.721 --> 01:39:48.090
<v Speaker 2>in that direction.</v>
<v Speaker 2>Do you,</v>

1805
01:39:48.110 --> 01:39:50.820
<v Speaker 2>do you have any concerns at all about a </v>
<v Speaker 2>virtual world?</v>

1806
01:39:51.000 --> 01:39:53.070
<v Speaker 2>I mean,</v>
<v Speaker 2>we may be anyone right now,</v>

1807
01:39:53.090 --> 01:39:54.810
<v Speaker 2>man,</v>
<v Speaker 2>you know that's true.</v>

1808
01:39:55.200 --> 01:39:56.033
<v Speaker 2>But as far as my problem is,</v>
<v Speaker 2>I want to find that programmer and get </v>

1809
01:39:58.681 --> 01:39:59.514
<v Speaker 2>them to make more attractive people.</v>
<v Speaker 2>I would say that that's part of the </v>

1810
01:40:04.711 --> 01:40:05.544
<v Speaker 2>reason why I attracted people are so </v>
<v Speaker 2>interesting as that they're unique and </v>

1811
01:40:07.471 --> 01:40:10.500
<v Speaker 2>rare problems with calling everything </v>
<v Speaker 2>beautiful.</v>

1812
01:40:10.560 --> 01:40:11.360
<v Speaker 2>Yeah.</v>
<v Speaker 2>You know,</v>

1813
01:40:11.360 --> 01:40:13.090
<v Speaker 2>everything we're saying,</v>
<v Speaker 2>everything is generous.</v>

1814
01:40:13.091 --> 01:40:14.070
<v Speaker 2>Beautiful.</v>
<v Speaker 2>I was like,</v>

1815
01:40:14.100 --> 01:40:16.470
<v Speaker 2>well,</v>
<v Speaker 2>you just have to get realistic.</v>

1816
01:40:16.470 --> 01:40:19.380
<v Speaker 2>If I get in the right frame of mind that</v>
<v Speaker 2>can find anything beautiful.</v>

1817
01:40:19.450 --> 01:40:21.240
<v Speaker 2>Well you could find it unique and </v>
<v Speaker 2>interesting.</v>

1818
01:40:21.270 --> 01:40:22.460
<v Speaker 2>Oh,</v>
<v Speaker 2>I can find anything but.</v>

1819
01:40:22.461 --> 01:40:23.940
<v Speaker 2>Okay.</v>
<v Speaker 2>I guess I guess.</v>

1820
01:40:23.970 --> 01:40:26.580
<v Speaker 2>But in terms of like.</v>
<v Speaker 2>Yeah,</v>

1821
01:40:26.640 --> 01:40:28.050
<v Speaker 2>I guess it's subjective,</v>
<v Speaker 2>right?</v>

1822
01:40:28.080 --> 01:40:29.490
<v Speaker 2>Really it is.</v>
<v Speaker 2>We're talking about beauty,</v>

1823
01:40:29.590 --> 01:40:31.170
<v Speaker 2>right?</v>
<v Speaker 2>Yeah.</v>

1824
01:40:31.410 --> 01:40:34.510
<v Speaker 2>Now,</v>
<v Speaker 2>but very existential angst just on the,</v>

1825
01:40:34.511 --> 01:40:35.344
<v Speaker 2>the.</v>
<v Speaker 2>When people sit and think about the </v>

1826
01:40:37.021 --> 01:40:41.280
<v Speaker 2>pointlessness of our own existence,</v>
<v Speaker 2>like we are these finite beings that are</v>

1827
01:40:41.610 --> 01:40:42.443
<v Speaker 2>clinging to a balls.</v>
<v Speaker 2>It spins a thousand miles an hour </v>

1828
01:40:44.551 --> 01:40:46.750
<v Speaker 2>hurling through infinity.</v>
<v Speaker 2>And what's the point like?</v>

1829
01:40:46.770 --> 01:40:48.720
<v Speaker 2>There's a lot of that that goes around </v>
<v Speaker 2>already.</v>

1830
01:40:48.900 --> 01:40:52.350
<v Speaker 2>We've.</v>
<v Speaker 2>We create an artificial environment that</v>

1831
01:40:52.351 --> 01:40:54.130
<v Speaker 2>we can literally somehow</v>

1832
01:40:54.130 --> 01:40:54.963
<v Speaker 1>under the download a version of us and </v>
<v Speaker 1>it exists and this block chain created </v>

1833
01:41:03.430 --> 01:41:08.430
<v Speaker 1>or or powered weird fucking simulation </v>
<v Speaker 1>world.</v>

1834
01:41:10.990 --> 01:41:12.260
<v Speaker 1>What would be,</v>
<v Speaker 1>I mean</v>

1835
01:41:12.810 --> 01:41:15.330
<v Speaker 2>it be the point of what I really </v>
<v Speaker 2>believe,</v>

1836
01:41:15.870 --> 01:41:20.870
<v Speaker 2>which is a bit personal and maybe </v>
<v Speaker 2>different than many of my colleagues.</v>

1837
01:41:22.110 --> 01:41:27.110
<v Speaker 2>What I really believe is that these </v>
<v Speaker 2>advancing technologies are going to lead</v>

1838
01:41:29.161 --> 01:41:34.161
<v Speaker 2>us to unlock many different states of </v>
<v Speaker 2>consciousness and experience than,</v>

1839
01:41:35.730 --> 01:41:39.200
<v Speaker 2>than most people are,</v>
<v Speaker 2>are,</v>

1840
01:41:39.201 --> 01:41:43.470
<v Speaker 2>are currently aware of are I mean you,</v>
<v Speaker 2>you,</v>

1841
01:41:43.490 --> 01:41:44.323
<v Speaker 2>you say we're,</v>
<v Speaker 2>we're just an insignificant species on </v>

1842
01:41:46.711 --> 01:41:47.544
<v Speaker 2>the,</v>
<v Speaker 2>on the,</v>

1843
01:41:47.550 --> 01:41:48.383
<v Speaker 2>you know,</v>
<v Speaker 2>a speck of rock hurtling in the other </v>

1844
01:41:49.621 --> 01:41:50.454
<v Speaker 2>space were insignificant.</v>
<v Speaker 2>There's people that have existential </v>

1845
01:41:52.951 --> 01:41:55.400
<v Speaker 2>acts because they wonder about what the </v>
<v Speaker 2>purpose was.</v>

1846
01:41:56.270 --> 01:41:57.103
<v Speaker 2>I go that category tend to feel like we </v>
<v Speaker 2>understand almost nothing about who and </v>

1847
01:42:04.861 --> 01:42:08.590
<v Speaker 2>what we are and our knowledge about the </v>
<v Speaker 2>universe is far small,</v>

1848
01:42:08.600 --> 01:42:10.510
<v Speaker 2>extremely minuscule.</v>
<v Speaker 2>I mean,</v>

1849
01:42:10.870 --> 01:42:14.250
<v Speaker 2>if anything,</v>
<v Speaker 2>I look at things from more of a Buddhist</v>

1850
01:42:14.251 --> 01:42:15.084
<v Speaker 2>or phenomenological way,</v>
<v Speaker 2>like the sense perceptions and then out </v>

1851
01:42:19.141 --> 01:42:19.974
<v Speaker 2>of those sense perceptions,</v>
<v Speaker 2>models arise and accumulate including a </v>

1852
01:42:25.021 --> 01:42:30.021
<v Speaker 2>model of the self and the model of the </v>
<v Speaker 2>body and the model of the physical world</v>

1853
01:42:30.691 --> 01:42:34.290
<v Speaker 2>out there and by the time you get to </v>
<v Speaker 2>planets and stars and blockchains,</v>

1854
01:42:34.291 --> 01:42:35.124
<v Speaker 2>you're building like hypothetical models</v>
<v Speaker 2>on top of hypothetical models and then </v>

1855
01:42:40.380 --> 01:42:41.213
<v Speaker 2>you know,</v>
<v Speaker 2>we're,</v>

1856
01:42:41.420 --> 01:42:42.253
<v Speaker 2>we're building intelligent machines and </v>
<v Speaker 2>mind uploading machines and virtual </v>

1857
01:42:47.371 --> 01:42:48.210
<v Speaker 2>realities.</v>

1858
01:42:48.780 --> 01:42:51.960
<v Speaker 2>We're going to radically transform,</v>
<v Speaker 2>you know,</v>

1859
01:42:52.410 --> 01:42:53.243
<v Speaker 2>our whole state of consciousness,</v>
<v Speaker 2>our understanding of what mind and </v>

1860
01:42:56.041 --> 01:43:00.600
<v Speaker 2>matter are.</v>
<v Speaker 2>Our experience of our own selves or even</v>

1861
01:43:00.601 --> 01:43:01.434
<v Speaker 2>whether it's self exists.</v>
<v Speaker 2>And I think ultimately the state of </v>

1862
01:43:06.571 --> 01:43:07.404
<v Speaker 2>consciousness of a human being like 100 </v>
<v Speaker 2>years from now after a technological </v>

1863
01:43:12.361 --> 01:43:13.194
<v Speaker 2>singularity is going to bear very little</v>
<v Speaker 2>resemblance to the states of </v>

1864
01:43:16.291 --> 01:43:19.950
<v Speaker 2>consciousness we have now.</v>
<v Speaker 2>We're just going to.</v>

1865
01:43:20.250 --> 01:43:24.750
<v Speaker 2>We're going to see a much wider universe</v>
<v Speaker 2>in any of us know,</v>

1866
01:43:25.620 --> 01:43:28.500
<v Speaker 2>imagine,</v>
<v Speaker 2>imagine to exist.</v>

1867
01:43:28.650 --> 01:43:31.560
<v Speaker 2>No,</v>
<v Speaker 2>this is my own personal view of things.</v>

1868
01:43:31.561 --> 01:43:31.911
<v Speaker 2>You,</v>
<v Speaker 2>you,</v>

1869
01:43:31.911 --> 01:43:32.744
<v Speaker 2>you don't have to agree with that.</v>
<v Speaker 2>To think the technological singularity </v>

1870
01:43:35.881 --> 01:43:38.460
<v Speaker 2>will be valuable,</v>
<v Speaker 2>but that is how I look.</v>

1871
01:43:38.520 --> 01:43:39.353
<v Speaker 2>I know Ray Kurzweil and I agree there's </v>
<v Speaker 2>going to be a technological singularity </v>

1872
01:43:43.810 --> 01:43:48.120
<v Speaker 2>within decades at most and ray and I </v>
<v Speaker 2>agree that,</v>

1873
01:43:48.510 --> 01:43:51.210
<v Speaker 2>you know,</v>
<v Speaker 2>if we biased technology development,</v>

1874
01:43:52.370 --> 01:43:53.960
<v Speaker 2>we can very lightly,</v>
<v Speaker 2>you know,</v>

1875
01:43:53.961 --> 01:43:54.794
<v Speaker 2>guide this to be a,</v>
<v Speaker 2>a world of abundance and benefit for </v>

1876
01:43:58.071 --> 01:43:59.530
<v Speaker 2>humans as well as ais.</v>

1877
01:44:00.120 --> 01:44:05.120
<v Speaker 2>But Ray is a bit more of a down to Earth</v>
<v Speaker 2>empiricists than I am lucky.</v>

1878
01:44:06.650 --> 01:44:11.270
<v Speaker 2>He thinks we understand more about the </v>
<v Speaker 2>universe right now than that than I do.</v>

1879
01:44:11.300 --> 01:44:12.133
<v Speaker 2>So.</v>
<v Speaker 2>I mean there's a wide spectrum of views </v>

1880
01:44:15.171 --> 01:44:20.171
<v Speaker 2>that are rational and sensible to have.</v>
<v Speaker 2>But my own view is we understand really,</v>

1881
01:44:21.351 --> 01:44:26.351
<v Speaker 2>really little of what we are and what,</v>
<v Speaker 2>what this world is.</v>

1882
01:44:26.571 --> 01:44:27.404
<v Speaker 2>And this is part of my own personal </v>
<v Speaker 2>quest for wanting to upgrade my brain </v>

1883
01:44:32.961 --> 01:44:33.794
<v Speaker 2>and wanting to create artificial </v>
<v Speaker 2>intelligences is like I've always been </v>

1884
01:44:36.681 --> 01:44:39.500
<v Speaker 2>driven above all else.</v>
<v Speaker 2>Borrowing to understand everything I can</v>

1885
01:44:39.501 --> 01:44:40.610
<v Speaker 2>about the world.</v>
<v Speaker 2>So I'm in.</v>

1886
01:44:40.970 --> 01:44:41.803
<v Speaker 2>I've studied every kind of science and </v>
<v Speaker 2>engineering and social science and read </v>

1887
01:44:44.841 --> 01:44:45.674
<v Speaker 2>every kind of literature,</v>
<v Speaker 2>but in the end the scope of human </v>

1888
01:44:47.961 --> 01:44:48.794
<v Speaker 2>understanding is clearly very small,</v>
<v Speaker 2>although at least we're smart enough to </v>

1889
01:44:53.421 --> 01:44:55.650
<v Speaker 2>understand how little we understand,</v>
<v Speaker 2>which I think my,</v>

1890
01:44:55.700 --> 01:44:57.020
<v Speaker 2>my dog doesn't understand.</v>

1891
01:44:57.400 --> 01:44:58.310
<v Speaker 2>He understands.</v>
<v Speaker 2>Right?</v>

1892
01:44:58.540 --> 01:45:02.750
<v Speaker 2>So and even like my 10 month old son,</v>
<v Speaker 2>he understands a little.</v>

1893
01:45:02.751 --> 01:45:04.970
<v Speaker 2>He understands which is interesting,</v>
<v Speaker 2>right?</v>

1894
01:45:04.971 --> 01:45:07.820
<v Speaker 2>Because he's because he's ready to </v>
<v Speaker 2>because he's also a human.</v>

1895
01:45:07.821 --> 01:45:09.590
<v Speaker 2>Right.</v>
<v Speaker 2>So I think,</v>

1896
01:45:10.280 --> 01:45:11.113
<v Speaker 2>I mean,</v>
<v Speaker 2>everything we think and believe now is </v>

1897
01:45:13.071 --> 01:45:16.760
<v Speaker 2>going to seem absolutely absurd to us </v>
<v Speaker 2>after there's a singularity.</v>

1898
01:45:16.770 --> 01:45:17.603
<v Speaker 2>We're just going to look back and laugh </v>
<v Speaker 2>in a warm hearted way as all the </v>

1899
01:45:21.561 --> 01:45:22.394
<v Speaker 2>incredibly silly things we were thinking</v>
<v Speaker 2>and doing back when we were trapped in </v>

1900
01:45:26.181 --> 01:45:27.014
<v Speaker 2>our,</v>
<v Speaker 2>in our,</v>

1901
01:45:27.110 --> 01:45:27.943
<v Speaker 2>you know,</v>
<v Speaker 2>our primitive biological brains and </v>

1902
01:45:29.811 --> 01:45:31.350
<v Speaker 2>bodies.</v>
<v Speaker 2>Stunning attack.</v>

1903
01:45:31.520 --> 01:45:32.353
<v Speaker 2>In your opinion or your assessment is </v>
<v Speaker 2>somewhere less than a 100 years away </v>

1904
01:45:36.411 --> 01:45:38.020
<v Speaker 2>from now.</v>
<v Speaker 2>Yeah.</v>

1905
01:45:38.060 --> 01:45:40.890
<v Speaker 2>That's requires exponential thinking.</v>
<v Speaker 2>Right?</v>

1906
01:45:40.970 --> 01:45:43.370
<v Speaker 2>Because if you.</v>
<v Speaker 2>It's hard to wrap your head around,</v>

1907
01:45:43.371 --> 01:45:44.500
<v Speaker 2>right?</v>
<v Speaker 2>I don't know.</v>

1908
01:45:44.510 --> 01:45:45.343
<v Speaker 2>It's immediate.</v>
<v Speaker 2>It's immediate for me to wrap my head </v>

1909
01:45:46.731 --> 01:45:47.564
<v Speaker 2>around,</v>
<v Speaker 2>but for a lot of people that you </v>

1910
01:45:48.801 --> 01:45:49.634
<v Speaker 2>explained it to him,</v>
<v Speaker 2>I'm sure that that that's a little bit </v>

1911
01:45:50.601 --> 01:45:51.434
<v Speaker 2>of a roadblock.</v>

1912
01:45:51.500 --> 01:45:52.370
<v Speaker 2>No,</v>
<v Speaker 2>it is.</v>

1913
01:45:52.430 --> 01:45:53.263
<v Speaker 2>It is.</v>
<v Speaker 2>It took me some time to have my parents </v>

1914
01:45:54.621 --> 01:45:56.660
<v Speaker 2>drop their heads around it because they </v>
<v Speaker 2>didn't.</v>

1915
01:45:56.780 --> 01:45:58.520
<v Speaker 2>They're not,</v>
<v Speaker 2>they're not technologists,</v>

1916
01:45:58.521 --> 01:45:59.354
<v Speaker 2>but I mean I find if you get people to </v>
<v Speaker 2>pay attention and sort of lead them </v>

1917
01:46:04.671 --> 01:46:05.504
<v Speaker 2>through all the supporting evidence,</v>
<v Speaker 2>most people can comprehend these ideas </v>

1918
01:46:11.090 --> 01:46:14.050
<v Speaker 2>reasonably well just to computers from </v>
<v Speaker 2>1960,</v>

1919
01:46:14.060 --> 01:46:14.893
<v Speaker 2>just hard to grasp.</v>
<v Speaker 2>It's hard to grab people's attention </v>

1920
01:46:16.491 --> 01:46:17.270
<v Speaker 2>then.</v>
<v Speaker 2>Yeah.</v>

1921
01:46:17.270 --> 01:46:21.390
<v Speaker 2>Mobile phones and made a big difference.</v>
<v Speaker 2>Like I spent a lot of time in Africa,</v>

1922
01:46:21.391 --> 01:46:22.224
<v Speaker 2>in Addis Ababa in Ethiopia where we have</v>
<v Speaker 2>a large ai development office and you </v>

1923
01:46:26.781 --> 01:46:27.614
<v Speaker 2>know,</v>
<v Speaker 2>the fact that mobile phones and then </v>

1924
01:46:29.240 --> 01:46:30.073
<v Speaker 2>smartphones have rolled out so quickly </v>
<v Speaker 2>even in rural Africa and if it's such a </v>

1925
01:46:33.981 --> 01:46:34.814
<v Speaker 2>transformative impact.</v>
<v Speaker 2>I mean this is a metaphor that lets </v>

1926
01:46:37.221 --> 01:46:41.210
<v Speaker 2>people understand the speed with what's </v>
<v Speaker 2>exponential change can happen.</v>

1927
01:46:41.540 --> 01:46:42.373
<v Speaker 2>When you talk about yourself,</v>
<v Speaker 2>when you talk about consciousness and </v>

1928
01:46:44.871 --> 01:46:48.200
<v Speaker 2>how you interface with the world,</v>
<v Speaker 2>how do you see this?</v>

1929
01:46:48.200 --> 01:46:48.570
<v Speaker 2>I mean,</v>
<v Speaker 2>when,</v>

1930
01:46:48.570 --> 01:46:50.850
<v Speaker 2>when you,</v>
<v Speaker 2>that we might be living in a simulation.</v>

1931
01:46:51.090 --> 01:46:53.190
<v Speaker 2>Do you actually entertain that?</v>
<v Speaker 2>Oh yeah,</v>

1932
01:46:53.250 --> 01:46:54.550
<v Speaker 2>you do.</v>
<v Speaker 2>I'm in the,</v>

1933
01:46:54.590 --> 01:46:58.480
<v Speaker 2>I think the word simulation is probably </v>
<v Speaker 2>wrong,</v>

1934
01:46:58.710 --> 01:47:01.890
<v Speaker 2>but yet the idea of an empirical,</v>
<v Speaker 2>you know,</v>

1935
01:47:02.640 --> 01:47:07.290
<v Speaker 2>materialist physical world is almost </v>
<v Speaker 2>certainly wrong also.</v>

1936
01:47:07.460 --> 01:47:08.480
<v Speaker 2>So I mean,</v>
<v Speaker 2>that's so.</v>

1937
01:47:09.540 --> 01:47:11.600
<v Speaker 2>Well,</v>
<v Speaker 2>again,</v>

1938
01:47:11.610 --> 01:47:12.443
<v Speaker 2>if you go back to a phenomenal view,</v>
<v Speaker 2>I mean you could look at the mind this </v>

1939
01:47:17.911 --> 01:47:22.710
<v Speaker 2>primary and you know,</v>
<v Speaker 2>your mind is building the world,</v>

1940
01:47:22.711 --> 01:47:23.221
<v Speaker 2>uh,</v>
<v Speaker 2>as,</v>

1941
01:47:23.221 --> 01:47:25.350
<v Speaker 2>as a model,</v>
<v Speaker 2>as a simple explanation of it,</v>

1942
01:47:25.380 --> 01:47:30.060
<v Speaker 2>of its perceptions.</v>
<v Speaker 2>On the other hand then what is the mind?</v>

1943
01:47:30.061 --> 01:47:30.894
<v Speaker 2>The self is also a model that gets,</v>
<v Speaker 2>that gets built up out of its </v>

1944
01:47:34.020 --> 01:47:34.853
<v Speaker 2>perceptions.</v>
<v Speaker 2>But then if I accept that your mind has </v>

1945
01:47:37.951 --> 01:47:38.784
<v Speaker 2>some fundamental existence also based on</v>
<v Speaker 2>this sort of feeling that you're like a </v>

1946
01:47:43.820 --> 01:47:44.653
<v Speaker 2>mind,</v>
<v Speaker 2>there are minds are working together to </v>

1947
01:47:47.041 --> 01:47:48.450
<v Speaker 2>build each other and,</v>
<v Speaker 2>and,</v>

1948
01:47:49.000 --> 01:47:50.580
<v Speaker 2>and to build this world.</v>

1949
01:47:50.670 --> 01:47:53.230
<v Speaker 2>And there there's a,</v>
<v Speaker 2>there's a,</v>

1950
01:47:53.270 --> 01:47:54.103
<v Speaker 2>there's a whole different way of,</v>
<v Speaker 2>of thinking about reality in terms of </v>

1951
01:47:56.671 --> 01:47:57.504
<v Speaker 2>first and second person experience </v>
<v Speaker 2>rather than these empiricist views like </v>

1952
01:48:02.641 --> 01:48:04.800
<v Speaker 2>this is a computer simulation or </v>
<v Speaker 2>something.</v>

1953
01:48:04.920 --> 01:48:05.753
<v Speaker 2>Right.</v>
<v Speaker 2>But you still agree that this is a </v>

1954
01:48:06.871 --> 01:48:09.180
<v Speaker 2>physical reality that we exist in,</v>
<v Speaker 2>or do you not?</v>

1955
01:48:09.210 --> 01:48:11.040
<v Speaker 2>What does that word mean?</v>
<v Speaker 2>That's a weird word,</v>

1956
01:48:11.041 --> 01:48:12.150
<v Speaker 2>right?</v>
<v Speaker 2>It is women.</v>

1957
01:48:12.560 --> 01:48:15.480
<v Speaker 2>Is it your interpretation of reality?</v>
<v Speaker 2>If you look in,</v>

1958
01:48:15.540 --> 01:48:18.690
<v Speaker 2>in modern physics,</v>
<v Speaker 2>even quantum mechanics,</v>

1959
01:48:19.050 --> 01:48:22.770
<v Speaker 2>there's something called the relational </v>
<v Speaker 2>interpretation of quantum mechanics,</v>

1960
01:48:23.280 --> 01:48:27.930
<v Speaker 2>which says that there's no sense in </v>
<v Speaker 2>thinking about an observed entity,</v>

1961
01:48:28.080 --> 01:48:31.320
<v Speaker 2>you should only think about an observed </v>
<v Speaker 2>comma observer pair.</v>

1962
01:48:31.800 --> 01:48:32.633
<v Speaker 2>Like there's no sense to think about </v>
<v Speaker 2>some thing except from the perspective </v>

1963
01:48:36.380 --> 01:48:39.000
<v Speaker 2>of some observer.</v>
<v Speaker 2>So that's,</v>

1964
01:48:39.390 --> 01:48:44.390
<v Speaker 2>that's even true within our best current</v>
<v Speaker 2>theory of modern physics as,</v>

1965
01:48:44.951 --> 01:48:48.900
<v Speaker 2>as induced from empirical empirical </v>
<v Speaker 2>observations.</v>

1966
01:48:48.900 --> 01:48:50.160
<v Speaker 2>Right?</v>
<v Speaker 2>In a pragmatic sense,</v>

1967
01:48:50.161 --> 01:48:52.140
<v Speaker 2>you know,</v>
<v Speaker 2>if you take a plane and fly to China,</v>

1968
01:48:52.141 --> 01:48:55.200
<v Speaker 2>you actually land in China,</v>
<v Speaker 2>I guess.</v>

1969
01:48:55.770 --> 01:48:57.840
<v Speaker 2>You guess,</v>
<v Speaker 2>why don't you live there?</v>

1970
01:48:58.290 --> 01:49:00.040
<v Speaker 2>I live in Hong Kong.</v>
<v Speaker 2>Yeah,</v>

1971
01:49:00.390 --> 01:49:01.750
<v Speaker 2>well,</v>
<v Speaker 2>close to it.</v>

1972
01:49:01.830 --> 01:49:02.820
<v Speaker 2>I,</v>
<v Speaker 2>I,</v>

1973
01:49:03.620 --> 01:49:06.150
<v Speaker 2>I have an unusual state of </v>
<v Speaker 2>consciousness.</v>

1974
01:49:06.870 --> 01:49:09.390
<v Speaker 2>That's what I'm trying to get at.</v>
<v Speaker 2>If you think about it,</v>

1975
01:49:09.391 --> 01:49:10.224
<v Speaker 2>like how do you know that you're not a </v>
<v Speaker 2>brain floating in a vet somewhere which </v>

1976
01:49:16.891 --> 01:49:17.724
<v Speaker 2>is being fed illusions by certain evil </v>
<v Speaker 2>scientist and two seconds from now he's </v>

1977
01:49:23.280 --> 01:49:26.220
<v Speaker 2>going to pull this simulated world </v>
<v Speaker 2>disappears and you realize you're just a</v>

1978
01:49:26.221 --> 01:49:27.360
<v Speaker 2>brain in a vat again,</v>
<v Speaker 2>you,</v>

1979
01:49:27.361 --> 01:49:27.771
<v Speaker 2>you,</v>
<v Speaker 2>you,</v>

1980
01:49:27.771 --> 01:49:28.604
<v Speaker 2>you don't know that you run on your own </v>
<v Speaker 2>personal experiences of falling in love </v>

1981
01:49:32.281 --> 01:49:33.114
<v Speaker 2>with a woman and moving to another,</v>
<v Speaker 2>but these may all be put into my brain </v>

1982
01:49:34.991 --> 01:49:36.960
<v Speaker 2>by the evil scientist.</v>
<v Speaker 2>How do we know?</v>

1983
01:49:36.990 --> 01:49:38.430
<v Speaker 2>But they're,</v>
<v Speaker 2>they're very consistent.</v>

1984
01:49:38.431 --> 01:49:39.264
<v Speaker 2>Are they not the,</v>
<v Speaker 2>the possibly illusory and implanted </v>

1985
01:49:43.171 --> 01:49:45.180
<v Speaker 2>memories are very consistent.</v>
<v Speaker 2>I guess.</v>

1986
01:49:45.390 --> 01:49:46.223
<v Speaker 2>I guess my own state of mind is I'm </v>
<v Speaker 2>always sort of acutely aware that this </v>

1987
01:49:53.920 --> 01:49:56.430
<v Speaker 2>simulation might all disappear at,</v>
<v Speaker 2>at,</v>

1988
01:49:56.480 --> 01:49:58.060
<v Speaker 2>at,</v>
<v Speaker 2>at any one moment,</v>

1989
01:49:58.120 --> 01:49:59.020
<v Speaker 2>uh,</v>
<v Speaker 2>uncertain,</v>

1990
01:49:59.021 --> 01:50:01.690
<v Speaker 2>acutely aware of this consciously on an </v>
<v Speaker 2>everyday basis.</v>

1991
01:50:01.870 --> 01:50:04.030
<v Speaker 2>Pretty much really,</v>
<v Speaker 2>really?</v>

1992
01:50:04.031 --> 01:50:05.800
<v Speaker 2>Why is that?</v>
<v Speaker 2>That doesn't seem to make sense.</v>

1993
01:50:05.801 --> 01:50:06.071
<v Speaker 2>I mean,</v>
<v Speaker 2>it's,</v>

1994
01:50:06.071 --> 01:50:08.290
<v Speaker 2>it's pretty,</v>
<v Speaker 2>it's pretty rock solid.</v>

1995
01:50:08.490 --> 01:50:09.323
<v Speaker 2>It's here everyday.</v>
<v Speaker 2>So you're possibly implanted memories </v>

1996
01:50:13.751 --> 01:50:15.190
<v Speaker 2>led you to believe?</v>
<v Speaker 2>Yes,</v>

1997
01:50:15.520 --> 01:50:17.800
<v Speaker 2>possibly.</v>
<v Speaker 2>Implanting memories need to believe that</v>

1998
01:50:17.801 --> 01:50:22.801
<v Speaker 2>this life is incredibly consistent.</v>
<v Speaker 2>This is incredibly consistent.</v>

1999
01:50:22.871 --> 01:50:26.110
<v Speaker 2>This is your problem of induction,</v>
<v Speaker 2>right?</v>

2000
01:50:26.111 --> 01:50:31.111
<v Speaker 2>From philosophy class and it's not and </v>
<v Speaker 2>it's not solved with you in a conceptual</v>

2001
01:50:31.661 --> 01:50:33.190
<v Speaker 2>sense.</v>
<v Speaker 2>I get,</v>

2002
01:50:33.191 --> 01:50:35.430
<v Speaker 2>I just feel this philosophy,</v>
<v Speaker 2>but you,</v>

2003
01:50:35.450 --> 01:50:36.510
<v Speaker 2>you embody it,</v>
<v Speaker 2>right?</v>

2004
01:50:36.520 --> 01:50:38.290
<v Speaker 2>This is something you carry with you all</v>
<v Speaker 2>the time.</v>

2005
01:50:38.320 --> 01:50:41.380
<v Speaker 2>Yeah.</v>
<v Speaker 2>On the other hand I'm in,</v>

2006
01:50:42.040 --> 01:50:42.873
<v Speaker 2>I'm still carrying out many actions with</v>
<v Speaker 2>longterm planning in mind or like I've </v>

2007
01:50:48.701 --> 01:50:53.320
<v Speaker 2>been,</v>
<v Speaker 2>I've been working on designing ai for 30</v>

2008
01:50:53.321 --> 01:50:54.154
<v Speaker 2>years and I'd be designing it inside.</v>
<v Speaker 2>I might be and I'm working on building </v>

2009
01:51:00.520 --> 01:51:02.250
<v Speaker 2>the same ai system,</v>
<v Speaker 2>you know,</v>

2010
01:51:02.320 --> 01:51:03.153
<v Speaker 2>since we started open cog in 2008,</v>
<v Speaker 2>but that's using codes from 2001 I was </v>

2011
01:51:09.461 --> 01:51:11.650
<v Speaker 2>building with my colleagues even even </v>
<v Speaker 2>earlier.</v>

2012
01:51:11.651 --> 01:51:12.484
<v Speaker 2>So I'm in.</v>
<v Speaker 2>I think longterm planning is very </v>

2013
01:51:16.211 --> 01:51:19.630
<v Speaker 2>natural to me but.</v>
<v Speaker 2>But nevertheless I don't.</v>

2014
01:51:19.720 --> 01:51:22.990
<v Speaker 2>I don't want to make any assumptions </v>
<v Speaker 2>about what sort of,</v>

2015
01:51:24.100 --> 01:51:28.570
<v Speaker 2>what sort of simulation or reality that </v>
<v Speaker 2>we're living in.</v>

2016
01:51:29.080 --> 01:51:34.080
<v Speaker 2>And I think everyone's gonna hit a lot </v>
<v Speaker 2>of surprises when once simulate singular</v>

2017
01:51:34.241 --> 01:51:34.641
<v Speaker 2>they can,</v>
<v Speaker 2>you know,</v>

2018
01:51:34.641 --> 01:51:35.474
<v Speaker 2>we,</v>
<v Speaker 2>we may find out that this hat is a </v>

2019
01:51:36.941 --> 01:51:37.774
<v Speaker 2>messenger from after the singularity,</v>
<v Speaker 2>so it traveled back through time to </v>

2020
01:51:41.171 --> 01:51:42.880
<v Speaker 2>implant into my brain.</v>

2021
01:51:42.880 --> 01:51:46.900
<v Speaker 2>The idea of how to create ai and bring </v>
<v Speaker 2>it into existence.</v>

2022
01:51:47.190 --> 01:51:47.610
<v Speaker 2>What?</v>
<v Speaker 2>Whoo.</v>

2023
01:51:47.610 --> 01:51:48.071
<v Speaker 2>It.</v>
<v Speaker 2>Oh,</v>

2024
01:51:48.071 --> 01:51:51.500
<v Speaker 2>that was mckenna that had this idea.</v>
<v Speaker 2>That was something in the theater tried.</v>

2025
01:51:51.580 --> 01:51:53.070
<v Speaker 2>Yes.</v>
<v Speaker 2>To this attract terence.</v>

2026
01:51:53.110 --> 01:51:54.010
<v Speaker 2>Terence Mckenna.</v>
<v Speaker 2>Yeah.</v>

2027
01:51:54.020 --> 01:51:55.360
<v Speaker 2>Yeah.</v>
<v Speaker 2>He had the same idea,</v>

2028
01:51:55.361 --> 01:51:58.990
<v Speaker 2>like some posts,</v>
<v Speaker 2>posts singularity intelligence,</v>

2029
01:51:58.991 --> 01:51:59.824
<v Speaker 2>which actually was living outside of </v>
<v Speaker 2>time somehow is reaching back and </v>

2030
01:52:02.951 --> 01:52:03.784
<v Speaker 2>putting into his brain the idea of how </v>
<v Speaker 2>to bring about the singularity novelty </v>

2031
01:52:09.791 --> 01:52:12.080
<v Speaker 2>itself as being drawn into this.</v>
<v Speaker 2>Yeah.</v>

2032
01:52:12.090 --> 01:52:15.760
<v Speaker 2>There were the timewave zero that was </v>
<v Speaker 2>going to reach the apex in 2012.</v>

2033
01:52:15.820 --> 01:52:16.601
<v Speaker 2>That didn't work.</v>
<v Speaker 2>No,</v>

2034
01:52:16.601 --> 01:52:17.434
<v Speaker 2>he died before that.</v>
<v Speaker 2>So he didn't get a chance to hear what </v>

2035
01:52:19.391 --> 01:52:22.210
<v Speaker 2>his,</v>
<v Speaker 2>what his idea was.</v>

2036
01:52:22.211 --> 01:52:22.400
<v Speaker 2>He,</v>
<v Speaker 2>uh,</v>

2037
01:52:22.400 --> 01:52:23.233
<v Speaker 2>you know,</v>
<v Speaker 2>I had some funny interactions with some </v>

2038
01:52:26.890 --> 01:52:31.890
<v Speaker 2>Mckenna fanatic 2012 whites.</v>
<v Speaker 2>This was about 2007 or so.</v>

2039
01:52:34.570 --> 01:52:37.960
<v Speaker 2>This guy came to Washington where I was </v>
<v Speaker 2>living then.</v>

2040
01:52:38.350 --> 01:52:42.850
<v Speaker 2>And he brought my friend Hugo de Garis,</v>
<v Speaker 2>another crazy ai researcher with them,</v>

2041
01:52:42.851 --> 01:52:43.684
<v Speaker 2>and he's like,</v>
<v Speaker 2>the singularity is going to happen in </v>

2042
01:52:45.651 --> 01:52:47.470
<v Speaker 2>2012 concerns.</v>

2043
01:52:47.480 --> 01:52:48.313
<v Speaker 2>Mckenna said,</v>
<v Speaker 2>so I need to be sure it's a good </v>

2044
01:52:50.961 --> 01:52:55.640
<v Speaker 2>singularity so you can't move to China </v>
<v Speaker 2>then it will be a bad singularity.</v>

2045
01:52:56.360 --> 01:52:57.193
<v Speaker 2>So we have.</v>
<v Speaker 2>So we have to get the US government to </v>

2046
01:52:59.571 --> 01:53:00.404
<v Speaker 2>give billions of dollars to your </v>
<v Speaker 2>research to guarantee that the </v>

2047
01:53:03.381 --> 01:53:06.920
<v Speaker 2>singularity in 2012 is a good </v>
<v Speaker 2>singularity.</v>

2048
01:53:06.921 --> 01:53:08.030
<v Speaker 2>Right?</v>
<v Speaker 2>So he,</v>

2049
01:53:08.300 --> 01:53:09.133
<v Speaker 2>he led us around to meet with these </v>
<v Speaker 2>generals and various high who has an in </v>

2050
01:53:13.160 --> 01:53:13.993
<v Speaker 2>DC to get them to fund Hugo.</v>
<v Speaker 2>The girl says in my ai research to </v>

2051
01:53:18.051 --> 01:53:21.710
<v Speaker 2>guarantee I wouldn't move to China and </v>
<v Speaker 2>Hugo wouldn't move to China.</v>

2052
01:53:22.040 --> 01:53:25.040
<v Speaker 2>So the US would create a positive </v>
<v Speaker 2>singularity.</v>

2053
01:53:25.041 --> 01:53:27.910
<v Speaker 2>Now the effort failed.</v>
<v Speaker 2>You Go,</v>

2054
01:53:27.950 --> 01:53:31.040
<v Speaker 2>you go move to China.</v>
<v Speaker 2>Then I moved there some years after.</v>

2055
01:53:31.041 --> 01:53:34.880
<v Speaker 2>So then this,</v>
<v Speaker 2>this 2012,</v>

2056
01:53:35.180 --> 01:53:37.940
<v Speaker 2>he went back to his apartment.</v>
<v Speaker 2>He made a,</v>

2057
01:53:38.240 --> 01:53:42.740
<v Speaker 2>a mix of 50 percent vodka,</v>
<v Speaker 2>50 percent robitussin pm.</v>

2058
01:53:42.810 --> 01:53:44.210
<v Speaker 2>You'd like drank it down.</v>

2059
01:53:44.210 --> 01:53:45.790
<v Speaker 2>He's like,</v>
<v Speaker 2>all right,</v>

2060
01:53:45.830 --> 01:53:48.350
<v Speaker 2>I'm going to have my own personal </v>
<v Speaker 2>singularity right here,</v>

2061
01:53:50.380 --> 01:53:51.213
<v Speaker 2>and I haven't talked to that guy since </v>
<v Speaker 2>2012 either to see what he thinks about </v>

2062
01:53:55.221 --> 01:53:56.730
<v Speaker 2>the singularity.</v>
<v Speaker 2>Not happening then,</v>

2063
01:53:56.731 --> 01:54:00.260
<v Speaker 2>but Terence Mckenna had a lot of </v>
<v Speaker 2>interesting ideas,</v>

2064
01:54:00.290 --> 01:54:02.620
<v Speaker 2>but I felt,</v>
<v Speaker 2>you know,</v>

2065
01:54:02.630 --> 01:54:04.790
<v Speaker 2>he,</v>
<v Speaker 2>he mixed up this,</v>

2066
01:54:04.850 --> 01:54:09.210
<v Speaker 2>the symbolic with the empirical more,</v>
<v Speaker 2>more than than I would,</v>

2067
01:54:09.350 --> 01:54:11.050
<v Speaker 2>I would prefer to do.</v>
<v Speaker 2>I mean,</v>

2068
01:54:11.530 --> 01:54:12.363
<v Speaker 2>I mean it's,</v>
<v Speaker 2>it's very interesting to look at these </v>

2069
01:54:14.930 --> 01:54:15.763
<v Speaker 2>abstract symbols and cosmic insights,</v>
<v Speaker 2>but then you have to sort of put your </v>

2070
01:54:20.901 --> 01:54:23.570
<v Speaker 2>scientific mindset on and say,</v>
<v Speaker 2>well,</v>

2071
01:54:23.571 --> 01:54:24.404
<v Speaker 2>what's a metaphor and what's,</v>
<v Speaker 2>what's like an actual empirical </v>

2072
01:54:28.761 --> 01:54:33.310
<v Speaker 2>scientific truth within,</v>
<v Speaker 2>within the scientific domain.</v>

2073
01:54:33.311 --> 01:54:35.120
<v Speaker 2>And now he's a little bit half baked.</v>
<v Speaker 2>Right.</v>

2074
01:54:35.121 --> 01:54:37.130
<v Speaker 2>I mean the whole idea was based on the </v>
<v Speaker 2>teaching.</v>

2075
01:54:37.400 --> 01:54:39.970
<v Speaker 2>He had a,</v>
<v Speaker 2>he was a mushroom trip.</v>

2076
01:54:41.540 --> 01:54:42.820
<v Speaker 2>IOWASCA was.</v>
<v Speaker 2>No,</v>

2077
01:54:42.870 --> 01:54:45.640
<v Speaker 2>Oscar I think led him to the teaching.</v>

2078
01:54:45.650 --> 01:54:48.330
<v Speaker 2>I don't believe it was maybe.</v>
<v Speaker 2>I mean it was silicide assignment.</v>

2079
01:54:48.440 --> 01:54:49.510
<v Speaker 2>It might've been.</v>
<v Speaker 2>Okay.</v>

2080
01:54:49.511 --> 01:54:49.880
<v Speaker 2>Yeah,</v>
<v Speaker 2>I mean,</v>

2081
01:54:49.880 --> 01:54:52.530
<v Speaker 2>I know you know his brother,</v>
<v Speaker 2>Dennis Mckenna.</v>

2082
01:54:52.620 --> 01:54:53.421
<v Speaker 2>Very well.</v>
<v Speaker 2>Yeah.</v>

2083
01:54:53.421 --> 01:54:53.810
<v Speaker 2>Yeah.</v>
<v Speaker 2>So,</v>

2084
01:54:53.810 --> 01:54:55.700
<v Speaker 2>so they.</v>
<v Speaker 2>Yeah,</v>

2085
01:54:55.710 --> 01:54:56.640
<v Speaker 2>you read.</v>
<v Speaker 2>Thanks.</v>

2086
01:54:56.660 --> 01:54:58.580
<v Speaker 2>At the time we have zero was a little </v>
<v Speaker 2>bit nonsense,</v>

2087
01:54:59.170 --> 01:55:03.890
<v Speaker 2>but he read their,</v>
<v Speaker 2>their book with true hallucinations.</v>

2088
01:55:04.250 --> 01:55:04.760
<v Speaker 2>Yeah,</v>
<v Speaker 2>right.</v>

2089
01:55:04.760 --> 01:55:05.990
<v Speaker 2>The very,</v>
<v Speaker 2>very,</v>

2090
01:55:05.991 --> 01:55:06.824
<v Speaker 2>very,</v>
<v Speaker 2>very interesting stuff and there is a </v>

2091
01:55:08.421 --> 01:55:09.254
<v Speaker 2>mixture of deep insight there with a </v>
<v Speaker 2>bunch of interesting metaphorical </v>

2092
01:55:15.060 --> 01:55:18.290
<v Speaker 2>thinking problem and get involved in </v>
<v Speaker 2>psychedelic drugs.</v>

2093
01:55:18.291 --> 01:55:20.270
<v Speaker 2>It's hard to differentiate like what </v>
<v Speaker 2>makes sense,</v>

2094
01:55:20.271 --> 01:55:21.104
<v Speaker 2>what's,</v>
<v Speaker 2>what's this unbelievably powerful </v>

2095
01:55:22.941 --> 01:55:23.774
<v Speaker 2>insight and what is just some crazy idea</v>
<v Speaker 2>you can learn to make that </v>

2096
01:55:27.650 --> 01:55:29.270
<v Speaker 2>differentiation.</v>
<v Speaker 2>You think so?</v>

2097
01:55:29.330 --> 01:55:30.140
<v Speaker 2>Yes.</v>
<v Speaker 2>Yeah.</v>

2098
01:55:30.140 --> 01:55:31.100
<v Speaker 2>But,</v>
<v Speaker 2>but,</v>

2099
01:55:31.101 --> 01:55:31.911
<v Speaker 2>uh,</v>
<v Speaker 2>yeah,</v>

2100
01:55:31.911 --> 01:55:32.744
<v Speaker 2>I'm in granted,</v>
<v Speaker 2>Terence Mckenna probably took more </v>

2101
01:55:38.211 --> 01:55:42.920
<v Speaker 2>psychedelic drugs then I would generally</v>
<v Speaker 2>recommend also he,</v>

2102
01:55:43.110 --> 01:55:44.460
<v Speaker 2>he was speaking</v>

2103
01:55:44.790 --> 01:55:45.623
<v Speaker 3>all the time and there's something that </v>
<v Speaker 3>I can attest to from podcasting all the </v>

2104
01:55:49.411 --> 01:55:50.910
<v Speaker 3>time.</v>
<v Speaker 3>Sometimes you just talking,</v>

2105
01:55:50.911 --> 01:55:52.200
<v Speaker 3>you don't know what the fuck you're </v>
<v Speaker 3>saying.</v>

2106
01:55:52.480 --> 01:55:53.313
<v Speaker 3>You know,</v>
<v Speaker 3>and you become a prisoner to your words </v>

2107
01:55:55.460 --> 01:55:56.710
<v Speaker 3>and in a lot of ways,</v>
<v Speaker 3>uh,</v>

2108
01:55:56.910 --> 01:55:57.743
<v Speaker 3>you,</v>
<v Speaker 3>you get locked up in this idea of </v>

2109
01:55:59.971 --> 01:56:02.610
<v Speaker 3>expressing this thought that may or may </v>
<v Speaker 3>not be viable.</v>

2110
01:56:02.660 --> 01:56:03.493
<v Speaker 2>I'm not sure that he was after empirical</v>
<v Speaker 2>truth in the same sense that say ray </v>

2111
01:56:08.151 --> 01:56:08.984
<v Speaker 2>Kurzweil when ray is saying we're going </v>
<v Speaker 2>to get human level ai in 2029 and then </v>

2112
01:56:16.700 --> 01:56:19.370
<v Speaker 2>you know,</v>
<v Speaker 2>massively superhuman ai in a singularity</v>

2113
01:56:19.371 --> 01:56:20.540
<v Speaker 2>in 20,</v>
<v Speaker 2>45.</v>

2114
01:56:20.541 --> 01:56:24.710
<v Speaker 2>I mean ray ray is very literal.</v>
<v Speaker 2>Like he's plotting charts,</v>

2115
01:56:24.711 --> 01:56:26.090
<v Speaker 2>right?</v>
<v Speaker 2>Terrence.</v>

2116
01:56:27.320 --> 01:56:28.153
<v Speaker 2>Terrence was thinking on an </v>
<v Speaker 2>impressionistic and and symbolic level </v>

2117
01:56:32.780 --> 01:56:35.690
<v Speaker 2>is a.</v>
<v Speaker 2>It was a bit different.</v>

2118
01:56:35.691 --> 01:56:40.691
<v Speaker 2>So you have to take that in a poetic </v>
<v Speaker 2>sense rather than in the literal sense.</v>

2119
01:56:41.181 --> 01:56:44.960
<v Speaker 2>And yeah,</v>
<v Speaker 2>I think it's very interesting to go back</v>

2120
01:56:44.961 --> 01:56:46.300
<v Speaker 2>and forth between the,</v>
<v Speaker 2>you know,</v>

2121
01:56:46.301 --> 01:56:47.134
<v Speaker 2>the symbolic and poetic domain and the </v>
<v Speaker 2>either concrete science and engineering </v>

2122
01:56:52.371 --> 01:56:55.400
<v Speaker 2>domain,</v>
<v Speaker 2>but it's also valuable to,</v>

2123
01:56:55.970 --> 01:56:58.430
<v Speaker 2>to be able to draw that,</v>
<v Speaker 2>draw that distinction,</v>

2124
01:56:58.431 --> 01:56:59.264
<v Speaker 2>right?</v>
<v Speaker 2>Because you can draw a lot of insight </v>

2125
01:57:00.681 --> 01:57:03.050
<v Speaker 2>from the kind of thinking Terence </v>
<v Speaker 2>Mckenna was,</v>

2126
01:57:03.500 --> 01:57:06.950
<v Speaker 2>was doing and certainly if you explore </v>
<v Speaker 2>psychedelics,</v>

2127
01:57:06.951 --> 01:57:11.510
<v Speaker 2>you can gain a lot of insights into how </v>
<v Speaker 2>the mind and universe work.</v>

2128
01:57:11.870 --> 01:57:12.703
<v Speaker 2>But then when,</v>
<v Speaker 2>when you put on your science and </v>

2129
01:57:14.421 --> 01:57:15.254
<v Speaker 2>engineering mindset,</v>
<v Speaker 2>you want to be rigorous about which </v>

2130
01:57:17.571 --> 01:57:21.860
<v Speaker 2>insights do you take and which ones do </v>
<v Speaker 2>you do throw out and ultimately you want</v>

2131
01:57:21.861 --> 01:57:23.430
<v Speaker 2>that you want to proceed on the basis </v>
<v Speaker 2>of,</v>

2132
01:57:23.450 --> 01:57:25.270
<v Speaker 2>of what works and what doesn't.</v>
<v Speaker 2>Right.</v>

2133
01:57:25.420 --> 01:57:26.253
<v Speaker 2>I mean that Dennis was pretty strong on </v>
<v Speaker 2>the terence was a bit less than that </v>

2134
01:57:30.651 --> 01:57:31.790
<v Speaker 2>empirical direction.</v>

2135
01:57:31.850 --> 01:57:33.810
<v Speaker 3>Well,</v>
<v Speaker 3>the dentist actually career scientists,</v>

2136
01:57:34.310 --> 01:57:35.220
<v Speaker 3>um,</v>
<v Speaker 3>how,</v>

2137
01:57:35.510 --> 01:57:36.343
<v Speaker 3>how many people involved in artificial </v>
<v Speaker 3>intelligence are also educated in the </v>

2138
01:57:41.570 --> 01:57:42.650
<v Speaker 3>ways of psychedelics.</v>

2139
01:57:44.380 --> 01:57:45.213
<v Speaker 2>Yeah.</v>
<v Speaker 2>That's all you have to say is that </v>

2140
01:57:48.800 --> 01:57:51.590
<v Speaker 2>unfortunately,</v>
<v Speaker 2>the illegal nature of these things,</v>

2141
01:57:51.591 --> 01:57:56.591
<v Speaker 2>it's a little hard to pin down,</v>
<v Speaker 2>I would say before the recent generation</v>

2142
01:57:59.000 --> 01:57:59.833
<v Speaker 2>of people going into ai because it was a</v>
<v Speaker 2>way to make money in the AI field was </v>

2143
01:58:03.351 --> 01:58:06.980
<v Speaker 2>incredibly full of really,</v>
<v Speaker 2>really interesting people and you know,</v>

2144
01:58:06.981 --> 01:58:09.230
<v Speaker 2>deep thinkers about,</v>
<v Speaker 2>about the mind.</v>

2145
01:58:09.470 --> 01:58:10.303
<v Speaker 2>And in the last few years,</v>
<v Speaker 2>of course ai has replaced like business </v>

2146
01:58:14.391 --> 01:58:16.310
<v Speaker 2>school is what your grandma wants you to</v>
<v Speaker 2>do,</v>

2147
01:58:16.311 --> 01:58:17.900
<v Speaker 2>to have a good career.</v>
<v Speaker 2>So I mean,</v>

2148
01:58:17.960 --> 01:58:18.793
<v Speaker 2>you're getting,</v>
<v Speaker 2>you're getting a lot of people into ai </v>

2149
01:58:21.741 --> 01:58:24.690
<v Speaker 2>just because it's financial environment,</v>
<v Speaker 2>it's,</v>

2150
01:58:24.720 --> 01:58:26.270
<v Speaker 2>it's cool,</v>
<v Speaker 2>it's financially viable,</v>

2151
01:58:26.271 --> 01:58:28.340
<v Speaker 2>it's popular because I can,</v>
<v Speaker 2>you know,</v>

2152
01:58:28.341 --> 01:58:30.950
<v Speaker 2>in our generation,</v>
<v Speaker 2>ai was not,</v>

2153
01:58:31.480 --> 01:58:32.313
<v Speaker 2>ai was not what your grandma wants you </v>
<v Speaker 2>to do so as to be able to buy a nice </v>

2154
01:58:35.751 --> 01:58:37.400
<v Speaker 2>house for the family.</v>
<v Speaker 2>Right.</v>

2155
01:58:37.640 --> 01:58:38.473
<v Speaker 2>So you got into it because you really </v>
<v Speaker 2>were curious about how the mind works </v>

2156
01:58:42.071 --> 01:58:46.970
<v Speaker 2>and of course many people played with </v>
<v Speaker 2>psychedelics because it also,</v>

2157
01:58:47.020 --> 01:58:49.160
<v Speaker 2>they were curious about,</v>
<v Speaker 2>you know,</v>

2158
01:58:49.210 --> 01:58:52.690
<v Speaker 2>what it was teaching them about,</v>
<v Speaker 2>about how their mind works.</v>

2159
01:58:52.770 --> 01:58:53.603
<v Speaker 2>Yeah.</v>
<v Speaker 2>I had a nice long conversation with Ray </v>

2160
01:58:56.430 --> 01:59:01.340
<v Speaker 2>Kurzweil and we talked for about an hour</v>
<v Speaker 2>and a half and it was for the Song Scifi</v>

2161
01:59:01.350 --> 01:59:05.100
<v Speaker 2>show that I was doing at the time and </v>
<v Speaker 2>some of his ideas.</v>

2162
01:59:06.270 --> 01:59:07.320
<v Speaker 2>He has this,</v>
<v Speaker 2>uh,</v>

2163
01:59:08.130 --> 01:59:08.740
<v Speaker 2>this,</v>
<v Speaker 2>uh,</v>

2164
01:59:08.740 --> 01:59:11.910
<v Speaker 2>this is number that they had people </v>
<v Speaker 2>throw about like 20,</v>

2165
01:59:11.911 --> 01:59:12.990
<v Speaker 2>42,</v>
<v Speaker 2>right?</v>

2166
01:59:13.260 --> 01:59:15.480
<v Speaker 2>Isn't that,</v>
<v Speaker 2>is that still 2045,</v>

2167
01:59:15.650 --> 01:59:17.910
<v Speaker 2>a 45 now.</v>
<v Speaker 2>Now you're being the optimist.</v>

2168
01:59:18.110 --> 01:59:21.360
<v Speaker 2>Now you're combining that with Douglas </v>
<v Speaker 2>off status for the two,</v>

2169
01:59:21.361 --> 01:59:23.200
<v Speaker 2>which is the answer to the universe.</v>
<v Speaker 2>No,</v>

2170
01:59:23.300 --> 01:59:28.300
<v Speaker 2>the 42 thing was the New York conference</v>
<v Speaker 2>that will take place in 2,245.</v>

2171
01:59:29.810 --> 01:59:30.643
<v Speaker 2>Was it?</v>
<v Speaker 2>I was at that conference that was </v>

2172
01:59:31.531 --> 01:59:32.364
<v Speaker 2>organized by Demetrius Gov is another </v>
<v Speaker 2>friend of mine from rush something off </v>

2173
01:59:35.550 --> 01:59:36.990
<v Speaker 2>by three.</v>
<v Speaker 2>It's 2045.</v>

2174
01:59:36.991 --> 01:59:39.760
<v Speaker 2>So that was,</v>
<v Speaker 2>that was,</v>

2175
01:59:39.850 --> 01:59:42.090
<v Speaker 2>that was raised prognostication.</v>
<v Speaker 2>Why,</v>

2176
01:59:42.091 --> 01:59:43.440
<v Speaker 2>why that year?</v>
<v Speaker 2>Um,</v>

2177
01:59:43.441 --> 01:59:47.070
<v Speaker 2>he did some current methylations yeah.</v>
<v Speaker 2>I mean he looked at Moore's law.</v>

2178
01:59:47.071 --> 01:59:47.904
<v Speaker 2>He looks in the advanced and the </v>
<v Speaker 2>accuracy of brain scanning and look at </v>

2179
01:59:51.371 --> 01:59:52.204
<v Speaker 2>the events of computer memory,</v>
<v Speaker 2>the miniaturization of various devices </v>

2180
01:59:54.841 --> 01:59:57.990
<v Speaker 2>and like plugging a whole bunch of these</v>
<v Speaker 2>curves.</v>

2181
01:59:58.530 --> 02:00:00.360
<v Speaker 2>That was the best guests that he came up</v>
<v Speaker 2>with.</v>

2182
02:00:00.450 --> 02:00:01.283
<v Speaker 2>What course?</v>
<v Speaker 2>There's some confidence interval around </v>

2183
02:00:03.031 --> 02:00:03.864
<v Speaker 2>that.</v>
<v Speaker 2>What do you see as potential monkey </v>

2184
02:00:04.891 --> 02:00:07.740
<v Speaker 2>wrenches that could be thrown into all </v>
<v Speaker 2>this innovation?</v>

2185
02:00:08.610 --> 02:00:10.140
<v Speaker 2>Like what were the,</v>
<v Speaker 2>were the pitfalls?</v>

2186
02:00:11.340 --> 02:00:12.173
<v Speaker 2>Well,</v>
<v Speaker 2>I mean the pitfall is always the one </v>

2187
02:00:13.231 --> 02:00:14.740
<v Speaker 2>that you,</v>
<v Speaker 2>that you don't sit you very.</v>

2188
02:00:15.590 --> 02:00:16.423
<v Speaker 2>I'm in,</v>
<v Speaker 2>of course it's possible there's some </v>

2189
02:00:20.190 --> 02:00:25.190
<v Speaker 2>science or engineering obstacle that </v>
<v Speaker 2>we're not foreseeing right now.</v>

2190
02:00:26.780 --> 02:00:27.480
<v Speaker 2>I mean it,</v>
<v Speaker 2>it,</v>

2191
02:00:27.480 --> 02:00:28.313
<v Speaker 2>it's also possible that all major </v>
<v Speaker 2>nations are overtaken by like a </v>

2192
02:00:32.520 --> 02:00:34.740
<v Speaker 2>religious fanatics or something which </v>
<v Speaker 2>which,</v>

2193
02:00:34.770 --> 02:00:38.550
<v Speaker 2>which slows down development somewhat a </v>
<v Speaker 2>few thousand years.</v>

2194
02:00:39.000 --> 02:00:40.990
<v Speaker 2>I think it would just be by a few </v>
<v Speaker 2>decades,</v>

2195
02:00:41.240 --> 02:00:44.700
<v Speaker 2>but yeah,</v>
<v Speaker 2>I mean in terms of scientific pitfalls,</v>

2196
02:00:45.150 --> 02:00:49.400
<v Speaker 2>I mean one possibility which I don't </v>
<v Speaker 2>think is luckily one pass,</v>

2197
02:00:49.440 --> 02:00:53.010
<v Speaker 2>but it's possible.</v>
<v Speaker 2>One possibility is human,</v>

2198
02:00:53.011 --> 02:00:56.010
<v Speaker 2>like intelligence requires advanced </v>
<v Speaker 2>quantum computers,</v>

2199
02:00:56.530 --> 02:00:59.670
<v Speaker 2>like it can't be done on the standard </v>
<v Speaker 2>classical digital computer.</v>

2200
02:01:00.270 --> 02:01:01.670
<v Speaker 2>Do you think that's the case?</v>
<v Speaker 2>No,</v>

2201
02:01:01.770 --> 02:01:02.603
<v Speaker 2>but on the other hand,</v>
<v Speaker 2>because there is no evidence that human </v>

2202
02:01:05.911 --> 02:01:09.690
<v Speaker 2>cognition relies on quantum effects in </v>
<v Speaker 2>the human brain,</v>

2203
02:01:09.720 --> 02:01:12.360
<v Speaker 2>like based on everything we know about </v>
<v Speaker 2>neuroscience now,</v>

2204
02:01:13.320 --> 02:01:16.200
<v Speaker 2>it seems not to be the case.</v>
<v Speaker 2>Like there's no evidence it's the case,</v>

2205
02:01:16.800 --> 02:01:17.633
<v Speaker 2>but it's possible.</v>
<v Speaker 2>It's the case because we don't </v>

2206
02:01:18.991 --> 02:01:20.850
<v Speaker 2>understand everything about how the </v>
<v Speaker 2>brain works.</v>

2207
02:01:21.150 --> 02:01:22.740
<v Speaker 2>The thing is,</v>
<v Speaker 2>even if that's true,</v>

2208
02:01:22.741 --> 02:01:23.574
<v Speaker 2>like there's loads of amazing research </v>
<v Speaker 2>going on in quantum computing and so </v>

2209
02:01:27.810 --> 02:01:29.400
<v Speaker 2>we're going to have,</v>
<v Speaker 2>you know,</v>

2210
02:01:29.401 --> 02:01:32.200
<v Speaker 2>you'll probably have a qp new quantum </v>
<v Speaker 2>processing unit in,</v>

2211
02:01:32.260 --> 02:01:33.093
<v Speaker 2>in,</v>
<v Speaker 2>in your phone and like a 10 to 20 years </v>

2212
02:01:34.951 --> 02:01:35.550
<v Speaker 2>or something.</v>
<v Speaker 2>Right.</v>

2213
02:01:35.550 --> 02:01:37.800
<v Speaker 2>So I'm in,</v>
<v Speaker 2>so that would,</v>

2214
02:01:38.130 --> 02:01:38.963
<v Speaker 2>that might throw off 20 slash 45 date,</v>
<v Speaker 2>but in a historical sense it doesn't </v>

2215
02:01:43.881 --> 02:01:44.714
<v Speaker 2>change the picture,</v>
<v Speaker 2>like I've got a bunch of research </v>

2216
02:01:46.231 --> 02:01:49.550
<v Speaker 2>singing on my hard drive on how we </v>
<v Speaker 2>improve open cogs,</v>

2217
02:01:49.551 --> 02:01:52.790
<v Speaker 2>ai using quantum computers once we have </v>
<v Speaker 2>better quantum computers.</v>

2218
02:01:52.790 --> 02:01:54.350
<v Speaker 2>Right?</v>
<v Speaker 2>So there's,</v>

2219
02:01:54.890 --> 02:01:55.723
<v Speaker 2>there could be other things like that </v>
<v Speaker 2>which are technical roadblocks that </v>

2220
02:01:59.511 --> 02:02:00.344
<v Speaker 2>we're not seeing now,</v>
<v Speaker 2>but I really doubt those are going to </v>

2221
02:02:02.721 --> 02:02:06.230
<v Speaker 2>delay things by more than like a decade </v>
<v Speaker 2>or two or something.</v>

2222
02:02:06.500 --> 02:02:10.050
<v Speaker 2>On the other hand,</v>
<v Speaker 2>things could also go faster than than,</v>

2223
02:02:10.110 --> 02:02:11.570
<v Speaker 2>than raised prediction,</v>
<v Speaker 2>which is,</v>

2224
02:02:11.571 --> 02:02:14.090
<v Speaker 2>which is what I'm pushing towards.</v>
<v Speaker 2>So what are you pushing towards?</v>

2225
02:02:14.091 --> 02:02:17.720
<v Speaker 2>What do you think I would like to get a </v>
<v Speaker 2>human level general intelligence in five</v>

2226
02:02:17.721 --> 02:02:19.850
<v Speaker 2>to seven years from now?</v>
<v Speaker 2>Wow.</v>

2227
02:02:21.410 --> 02:02:22.243
<v Speaker 2>I don't think that's bad by any means </v>
<v Speaker 2>impossible because I think our open cog </v>

2228
02:02:27.230 --> 02:02:28.063
<v Speaker 2>design is adequate to do it,</v>
<v Speaker 2>but I mean it takes a lot of people </v>

2229
02:02:32.751 --> 02:02:35.330
<v Speaker 2>working coherently for awhile to build </v>
<v Speaker 2>something,</v>

2230
02:02:35.390 --> 02:02:39.020
<v Speaker 2>something big like this,</v>
<v Speaker 2>being in a physical form like a robot.</v>

2231
02:02:39.230 --> 02:02:43.170
<v Speaker 2>It'll be in the compute cloud can use </v>
<v Speaker 2>many robots as,</v>

2232
02:02:43.180 --> 02:02:45.770
<v Speaker 2>as user interfaces,</v>
<v Speaker 2>but the same ai control.</v>

2233
02:02:45.770 --> 02:02:46.603
<v Speaker 2>Many different robots actually and many </v>
<v Speaker 2>other sensors and systems besides </v>

2234
02:02:49.821 --> 02:02:50.654
<v Speaker 2>robots.</v>
<v Speaker 2>I mean I think the human like form </v>

2235
02:02:52.251 --> 02:02:56.210
<v Speaker 2>factor like we have with Sophia and our </v>
<v Speaker 2>other Henson robots,</v>

2236
02:02:56.600 --> 02:02:57.433
<v Speaker 2>the human like form factor is really </v>
<v Speaker 2>valuable as a tool for allowing the </v>

2237
02:03:00.861 --> 02:03:03.600
<v Speaker 2>cloud based ai mind to,</v>
<v Speaker 2>you know,</v>

2238
02:03:03.620 --> 02:03:06.680
<v Speaker 2>engage with humans and to learn human </v>
<v Speaker 2>cultures and values.</v>

2239
02:03:06.950 --> 02:03:07.783
<v Speaker 2>Because I mean getting back to what we </v>
<v Speaker 2>were discussing it at the beginning of </v>

2240
02:03:09.831 --> 02:03:11.000
<v Speaker 2>this chat,</v>
<v Speaker 2>you know,</v>

2241
02:03:11.001 --> 02:03:15.750
<v Speaker 2>the best way to get human values and </v>
<v Speaker 2>culture into the AI is for humans and ai</v>

2242
02:03:15.751 --> 02:03:17.440
<v Speaker 2>is to enter into many shared,</v>
<v Speaker 2>you know,</v>

2243
02:03:17.450 --> 02:03:18.710
<v Speaker 2>like social,</v>
<v Speaker 2>emotional,</v>

2244
02:03:18.711 --> 02:03:19.544
<v Speaker 2>embodied situations together.</v>
<v Speaker 2>So having a human embodiment for the AI </v>

2245
02:03:24.290 --> 02:03:28.420
<v Speaker 2>is important for that.</v>
<v Speaker 2>Like I can look you in the eye,</v>

2246
02:03:28.421 --> 02:03:31.340
<v Speaker 2>you can share your facial expressions,</v>
<v Speaker 2>it can bond with you,</v>

2247
02:03:31.610 --> 02:03:32.443
<v Speaker 2>it can see the way you react when you </v>
<v Speaker 2>see like a sick person by the side of </v>

2248
02:03:35.091 --> 02:03:36.860
<v Speaker 2>the road,</v>
<v Speaker 2>there's something right and,</v>

2249
02:03:36.861 --> 02:03:40.040
<v Speaker 2>and you know,</v>
<v Speaker 2>can see you ask the ai to get,</v>

2250
02:03:40.041 --> 02:03:42.890
<v Speaker 2>give the homeless person the $20 or </v>
<v Speaker 2>something.</v>

2251
02:03:43.090 --> 02:03:47.420
<v Speaker 2>I mean the ai understands what money is </v>
<v Speaker 2>and understands what that action means.</v>

2252
02:03:47.421 --> 02:03:48.254
<v Speaker 2>Some in interacting with an ai in human </v>
<v Speaker 2>life form is going to be valuable as a </v>

2253
02:03:53.750 --> 02:03:54.583
<v Speaker 2>learning mechanism for the ai and as a </v>
<v Speaker 2>learning mechanism for people to get </v>

2254
02:03:57.681 --> 02:03:58.514
<v Speaker 2>more comfortable with ais.</v>
<v Speaker 2>But I mean ultimately one advantage of </v>

2255
02:04:02.301 --> 02:04:03.430
<v Speaker 2>being,</v>
<v Speaker 2>you know,</v>

2256
02:04:03.440 --> 02:04:06.800
<v Speaker 2>a digital mind is you don't have to be </v>
<v Speaker 2>why the Dandy particular embodiment.</v>

2257
02:04:06.801 --> 02:04:07.634
<v Speaker 2>Now I can go between many different </v>
<v Speaker 2>bodies and they can transfer knowledge </v>

2258
02:04:10.701 --> 02:04:12.920
<v Speaker 2>between the many different bodies that </v>
<v Speaker 2>it's occupied.</v>

2259
02:04:13.430 --> 02:04:14.263
<v Speaker 2>Well,</v>
<v Speaker 2>that's the real concern that the people </v>

2260
02:04:16.311 --> 02:04:17.144
<v Speaker 2>that are,</v>
<v Speaker 2>that have this dystopian view of </v>

2261
02:04:19.010 --> 02:04:19.843
<v Speaker 2>artificial intelligence hub is that ai </v>
<v Speaker 2>may already exist and it's just sitting </v>

2262
02:04:23.451 --> 02:04:28.451
<v Speaker 2>there waiting to Americans.</v>
<v Speaker 2>Too many bad movies in Asia and Asia.</v>

2263
02:04:29.181 --> 02:04:32.260
<v Speaker 2>Everyone thinks ai will be our friend </v>
<v Speaker 2>and will love us and help us.</v>

2264
02:04:32.270 --> 02:04:33.520
<v Speaker 2>Yeah,</v>
<v Speaker 2>we're very,</v>

2265
02:04:33.521 --> 02:04:34.640
<v Speaker 2>very,</v>
<v Speaker 2>very much.</v>

2266
02:04:34.670 --> 02:04:37.170
<v Speaker 2>That's what you're pumping out there.</v>
<v Speaker 2>No,</v>

2267
02:04:37.230 --> 02:04:39.480
<v Speaker 2>that's been as their philosophies I </v>
<v Speaker 2>guess.</v>

2268
02:04:39.481 --> 02:04:43.740
<v Speaker 2>I mean you look in Japanese anime.</v>
<v Speaker 2>I mean there's been ais and robots for a</v>

2269
02:04:43.741 --> 02:04:46.230
<v Speaker 2>long time.</v>
<v Speaker 2>They're usually people's friends.</v>

2270
02:04:46.270 --> 02:04:51.270
<v Speaker 2>There's dystopian aesthetic and it's the</v>
<v Speaker 2>same in China and Korea.</v>

2271
02:04:52.380 --> 02:04:53.213
<v Speaker 2>The general guests there is an ais and </v>
<v Speaker 2>robots will be people's friends and </v>

2272
02:04:58.891 --> 02:04:59.724
<v Speaker 2>will,</v>
<v Speaker 2>will help people and then some other </v>

2273
02:05:01.710 --> 02:05:02.543
<v Speaker 2>general guests in America is.</v>
<v Speaker 2>It's going to be some big nasty Robo </v>

2274
02:05:06.991 --> 02:05:08.700
<v Speaker 2>soldier marching down the street</v>

2275
02:05:09.690 --> 02:05:10.523
<v Speaker 3>beyond musk who we rely upon no smarter </v>
<v Speaker 3>than us and he's fucking terrified of </v>

2276
02:05:14.931 --> 02:05:15.764
<v Speaker 3>it.</v>
<v Speaker 3>Sam Harris terrified of it for very </v>

2277
02:05:17.841 --> 02:05:18.674
<v Speaker 3>smart people that just think it could </v>
<v Speaker 3>really be a huge disaster for the human </v>

2278
02:05:21.801 --> 02:05:22.634
<v Speaker 3>race.</v>

2279
02:05:22.940 --> 02:05:23.773
<v Speaker 2>I guess not just bad because know it's a</v>
<v Speaker 2>cultural thing because the oriental </v>

2280
02:05:28.621 --> 02:05:29.454
<v Speaker 2>culture is sort of social good oriented.</v>
<v Speaker 2>Most orientals think a lot in terms of </v>

2281
02:05:35.791 --> 02:05:36.624
<v Speaker 2>what's good for the family or the </v>
<v Speaker 2>society as opposed to themselves </v>

2282
02:05:39.871 --> 02:05:44.610
<v Speaker 2>personally and so they just make the </v>
<v Speaker 2>default assumption that ais are going to</v>

2283
02:05:44.611 --> 02:05:48.150
<v Speaker 2>be the same way whereas Americans are </v>
<v Speaker 2>more like me,</v>

2284
02:05:48.151 --> 02:05:48.984
<v Speaker 2>me,</v>
<v Speaker 2>me oriented and I say that as an </v>

2285
02:05:51.541 --> 02:05:52.374
<v Speaker 2>American as well and where they sort of </v>
<v Speaker 2>assumed that ais are going to be </v>

2286
02:05:57.150 --> 02:05:59.650
<v Speaker 2>possible.</v>
<v Speaker 2>Right.</v>

2287
02:05:59.690 --> 02:06:00.523
<v Speaker 2>Well whatever is in your mind,</v>
<v Speaker 2>you impose on this ai when we don't </v>

2288
02:06:04.261 --> 02:06:06.240
<v Speaker 2>actually know what it's going to become </v>
<v Speaker 2>bright,</v>

2289
02:06:06.241 --> 02:06:07.350
<v Speaker 2>but there it is.</v>

2290
02:06:07.410 --> 02:06:11.080
<v Speaker 3>There are potential negative aspects.</v>
<v Speaker 3>Of course,</v>

2291
02:06:11.370 --> 02:06:16.370
<v Speaker 3>artificial intelligence deciding that </v>
<v Speaker 3>we're the logical and unnecessary.</v>

2292
02:06:18.290 --> 02:06:20.820
<v Speaker 2>Well we are a logical and unnecessary </v>
<v Speaker 2>yes,</v>

2293
02:06:21.440 --> 02:06:24.770
<v Speaker 2>but that doesn't mean that ai should be </v>
<v Speaker 2>badly disposed towards us.</v>

2294
02:06:24.800 --> 02:06:26.960
<v Speaker 2>I'm in.</v>
<v Speaker 2>Did you see x Mokena?</v>

2295
02:06:27.200 --> 02:06:29.120
<v Speaker 2>I did you like it?</v>
<v Speaker 2>Sure.</v>

2296
02:06:29.121 --> 02:06:32.120
<v Speaker 2>It was a copy of our robot.</v>
<v Speaker 2>So it was,</v>

2297
02:06:32.121 --> 02:06:32.954
<v Speaker 2>I mean our robots.</v>
<v Speaker 2>So fear looks exactly like the robot in </v>

2298
02:06:36.390 --> 02:06:38.600
<v Speaker 2>the American.</v>
<v Speaker 2>So it was a good video that online.</v>

2299
02:06:38.660 --> 02:06:39.081
<v Speaker 2>Yeah,</v>
<v Speaker 2>yeah,</v>

2300
02:06:39.081 --> 02:06:40.520
<v Speaker 2>yeah.</v>
<v Speaker 2>Would tell Jamie how do you get the good</v>

2301
02:06:40.521 --> 02:06:44.810
<v Speaker 2>video or just search for Sophia Hanson?</v>
<v Speaker 2>Robot on Google?</v>

2302
02:06:44.811 --> 02:06:45.000
<v Speaker 2>Yeah,</v>

2303
02:06:45.000 --> 02:06:45.833
<v Speaker 3>how advanced is Sophia right now?</v>
<v Speaker 3>I mean how many different iterations </v>

2304
02:06:49.860 --> 02:06:50.693
<v Speaker 3>have there been?</v>

2305
02:06:50.770 --> 02:06:54.550
<v Speaker 2>There's been something like 16,</v>
<v Speaker 2>so fear robots made so far.</v>

2306
02:06:54.730 --> 02:06:55.563
<v Speaker 2>We're moving towards scalable </v>
<v Speaker 2>manufacturer over the next couple of </v>

2307
02:06:58.450 --> 02:06:59.283
<v Speaker 2>years,</v>
<v Speaker 2>so right now she's going around sort of </v>

2308
02:07:01.391 --> 02:07:04.840
<v Speaker 2>as an ambassador for humanoid robot </v>
<v Speaker 2>kind,</v>

2309
02:07:04.841 --> 02:07:05.674
<v Speaker 2>giving,</v>
<v Speaker 2>giving speeches and talks and various </v>

2310
02:07:08.141 --> 02:07:09.820
<v Speaker 2>places.</v>
<v Speaker 2>So she.</v>

2311
02:07:09.880 --> 02:07:14.710
<v Speaker 2>So fear used to be called eva or we had </v>
<v Speaker 2>a robot like the current Sophia that was</v>

2312
02:07:14.711 --> 02:07:15.544
<v Speaker 2>called Eva and then x meshing that came </v>
<v Speaker 2>out with the robot called Eva that </v>

2313
02:07:19.900 --> 02:07:24.900
<v Speaker 2>exactly like the robot that that my </v>
<v Speaker 2>colleague David Hanson and I made.</v>

2314
02:07:25.480 --> 02:07:27.310
<v Speaker 2>I think it's a coincidence.</v>
<v Speaker 2>Of course not.</v>

2315
02:07:27.311 --> 02:07:28.144
<v Speaker 2>They just copy that.</v>
<v Speaker 2>I mean of course the body they have is </v>

2316
02:07:30.971 --> 02:07:35.500
<v Speaker 2>better and the AI is better in the movie</v>
<v Speaker 2>than our robot ai is.</v>

2317
02:07:35.590 --> 02:07:39.670
<v Speaker 2>So we changed the name to Sophia,</v>
<v Speaker 2>which means wisdom in instead.</v>

2318
02:07:40.630 --> 02:07:43.780
<v Speaker 3>Was it freaky watching that though with </v>
<v Speaker 3>the name eva?</v>

2319
02:07:44.130 --> 02:07:48.550
<v Speaker 2>The thing is that the moral of that </v>
<v Speaker 2>movie is just if you know,</v>

2320
02:07:48.551 --> 02:07:53.551
<v Speaker 2>if associate path raises a robot with an</v>
<v Speaker 2>abusive interaction,</v>

2321
02:07:54.250 --> 02:07:56.950
<v Speaker 2>it may come out to be a sociopath or </v>
<v Speaker 2>psychopath.</v>

2322
02:07:56.951 --> 02:07:58.570
<v Speaker 2>So let's,</v>
<v Speaker 2>let's,</v>

2323
02:07:58.600 --> 02:07:59.900
<v Speaker 2>let's not do that.</v>
<v Speaker 2>Right.</v>

2324
02:08:00.430 --> 02:08:03.550
<v Speaker 2>Let's raise our robots with love and </v>
<v Speaker 2>compassion.</v>

2325
02:08:03.720 --> 02:08:04.553
<v Speaker 2>Yeah.</v>
<v Speaker 2>You see,</v>

2326
02:08:04.600 --> 02:08:09.600
<v Speaker 2>the thing is that we have.</v>
<v Speaker 2>I haven't seen this particular</v>

2327
02:08:12.770 --> 02:08:15.750
<v Speaker 4>great.</v>
<v Speaker 4>What is she saying?</v>

2328
02:08:17.370 --> 02:08:22.370
<v Speaker 4>She's not happy she was on Jimmy Fallon </v>
<v Speaker 4>or something.</v>

2329
02:08:24.460 --> 02:08:25.920
<v Speaker 4>That's David.</v>
<v Speaker 4>This scope.</v>

2330
02:08:27.280 --> 02:08:29.260
<v Speaker 4>How much is it actually interacting with</v>
<v Speaker 4>them?</v>

2331
02:08:30.720 --> 02:08:32.150
<v Speaker 2>It has a chat system.</v>

2332
02:08:33.040 --> 02:08:38.040
<v Speaker 4>It really has a nice ring.</v>
<v Speaker 4>Now I have to make clear that I didn't.</v>

2333
02:08:38.940 --> 02:08:39.773
<v Speaker 2>So yeah,</v>
<v Speaker 2>so fear we can run using many different </v>

2334
02:08:42.481 --> 02:08:46.740
<v Speaker 2>ai system so that there's a chat bot </v>
<v Speaker 2>which is sort of like,</v>

2335
02:08:47.610 --> 02:08:52.200
<v Speaker 2>you know,</v>
<v Speaker 2>Alexa or google now or something,</v>

2336
02:08:52.710 --> 02:08:55.560
<v Speaker 2>but with a bit a bit better ai and </v>
<v Speaker 2>interaction with,</v>

2337
02:08:55.590 --> 02:08:59.310
<v Speaker 2>with no emotion and face recognition and</v>
<v Speaker 2>so forth.</v>

2338
02:08:59.311 --> 02:09:04.260
<v Speaker 2>So it's not human level ai,</v>
<v Speaker 2>but it is responding to a no.</v>

2339
02:09:04.350 --> 02:09:08.220
<v Speaker 2>It understands what you say and it comes</v>
<v Speaker 2>up with an answer and it can look you in</v>

2340
02:09:08.221 --> 02:09:11.430
<v Speaker 2>the eye and make more than one language.</v>
<v Speaker 2>Well,</v>

2341
02:09:11.490 --> 02:09:14.010
<v Speaker 2>right now we can load it in English </v>
<v Speaker 2>mode,</v>

2342
02:09:14.040 --> 02:09:14.873
<v Speaker 2>Chinese motor Russian mode and there's </v>
<v Speaker 2>sort of different different software </v>

2343
02:09:18.091 --> 02:09:18.924
<v Speaker 2>packages and we also use her sometimes </v>
<v Speaker 2>to experiment with their open cogs </v>

2344
02:09:22.951 --> 02:09:23.784
<v Speaker 2>system and singularity now so we can.</v>
<v Speaker 2>We can use the robot as a research </v>

2345
02:09:27.181 --> 02:09:28.014
<v Speaker 2>platform for exploring some of our more </v>
<v Speaker 2>advanced ai tools and then there's a </v>

2346
02:09:32.221 --> 02:09:35.600
<v Speaker 2>simpler chatbots software which is used </v>
<v Speaker 2>for appearances like,</v>

2347
02:09:35.680 --> 02:09:36.513
<v Speaker 2>like,</v>
<v Speaker 2>like that one and in the next year we </v>

2348
02:09:38.881 --> 02:09:39.714
<v Speaker 2>want to roll out more of our advanced </v>
<v Speaker 2>research software from open coggin </v>

2349
02:09:42.781 --> 02:09:47.010
<v Speaker 2>singularity that rolled out more of that</v>
<v Speaker 2>inside these robots,</v>

2350
02:09:47.011 --> 02:09:47.844
<v Speaker 2>which is one among many applications </v>
<v Speaker 2>we're looking at with our singular unit </v>

2351
02:09:51.031 --> 02:09:51.864
<v Speaker 2>platform.</v>

2352
02:09:52.450 --> 02:09:53.283
<v Speaker 3>I want to get you back in here in like a</v>
<v Speaker 3>year and find out where everything is </v>

2353
02:09:56.741 --> 02:10:01.741
<v Speaker 3>because I feel like we need someone like</v>
<v Speaker 3>you to sort of let us know where,</v>

2354
02:10:02.590 --> 02:10:04.320
<v Speaker 3>where it's at,</v>
<v Speaker 3>when it's run,</v>

2355
02:10:04.360 --> 02:10:05.193
<v Speaker 3>when the switches about to flip.</v>
<v Speaker 3>It seems to me that it might happen so </v>

2356
02:10:09.881 --> 02:10:10.714
<v Speaker 3>quickly and the change might take place </v>
<v Speaker 3>so rapidly that we really will have no </v>

2357
02:10:16.031 --> 02:10:17.860
<v Speaker 3>idea what's happening before it happens.</v>

2358
02:10:20.530 --> 02:10:21.363
<v Speaker 2>We think about the singularity,</v>
<v Speaker 2>like it's going to be some huge like </v>

2359
02:10:26.650 --> 02:10:30.520
<v Speaker 2>physical event and suddenly everything </v>
<v Speaker 2>turns purple in this cover with diamonds</v>

2360
02:10:30.521 --> 02:10:31.271
<v Speaker 2>or something.</v>
<v Speaker 2>Right?</v>

2361
02:10:31.271 --> 02:10:34.820
<v Speaker 2>But I mean there's a lot voice.</v>
<v Speaker 2>Something like this could unfold,</v>

2362
02:10:34.821 --> 02:10:37.850
<v Speaker 2>so I can imagine that with our </v>
<v Speaker 2>singularity,</v>

2363
02:10:37.880 --> 02:10:40.570
<v Speaker 2>not decentralized network,</v>
<v Speaker 2>you know,</v>

2364
02:10:40.571 --> 02:10:41.404
<v Speaker 2>we get an ai that's smarter than humans </v>
<v Speaker 2>and can create a new scientific </v>

2365
02:10:46.851 --> 02:10:48.590
<v Speaker 2>discovery.</v>
<v Speaker 2>The Nobel prize level,</v>

2366
02:10:48.591 --> 02:10:49.424
<v Speaker 2>every minute there's something that that</v>
<v Speaker 2>doesn't mean this ai is going to </v>

2367
02:10:52.701 --> 02:10:57.500
<v Speaker 2>immediately like refactor all matter </v>
<v Speaker 2>into,</v>

2368
02:10:57.501 --> 02:11:02.480
<v Speaker 2>into images of a bucket head or do </v>
<v Speaker 2>something run and I'm in.</v>

2369
02:11:02.870 --> 02:11:07.730
<v Speaker 2>I mean if the AI has some caring and </v>
<v Speaker 2>wisdom and compassion,</v>

2370
02:11:07.731 --> 02:11:08.564
<v Speaker 2>then whatever changes happen,</v>
<v Speaker 2>but it's the artist human </v>

2371
02:11:10.521 --> 02:11:11.354
<v Speaker 2>characteristics,</v>
<v Speaker 2>not necessarily in fact human passion </v>

2372
02:11:13.760 --> 02:11:14.593
<v Speaker 2>just as humans are either the most </v>
<v Speaker 2>intelligent nor the most compassionate </v>

2373
02:11:17.630 --> 02:11:20.480
<v Speaker 2>possible creatures that that's.</v>
<v Speaker 2>That's pretty clear.</v>

2374
02:11:20.480 --> 02:11:21.313
<v Speaker 2>If you look at the world around us and </v>
<v Speaker 2>one of one of our projects that we're </v>

2375
02:11:25.041 --> 02:11:28.160
<v Speaker 2>doing with this Sofia robot is aimed </v>
<v Speaker 2>exactly at ai.</v>

2376
02:11:28.161 --> 02:11:28.994
<v Speaker 2>Compassion's this has got the loving ai </v>
<v Speaker 2>project and we're using this sophia </v>

2377
02:11:32.480 --> 02:11:34.940
<v Speaker 2>robot as a,</v>
<v Speaker 2>as a meditation assistant.</v>

2378
02:11:35.210 --> 02:11:36.380
<v Speaker 2>So,</v>
<v Speaker 2>so we're,</v>

2379
02:11:36.400 --> 02:11:37.233
<v Speaker 2>we're using so fear to help people get </v>
<v Speaker 2>into deep meditative trance states and </v>

2380
02:11:42.500 --> 02:11:46.700
<v Speaker 2>help them breathe deeply and uh,</v>
<v Speaker 2>achieve more,</v>

2381
02:11:46.730 --> 02:11:47.563
<v Speaker 2>more positive state of being.</v>
<v Speaker 2>And part of the goal there is to help </v>

2382
02:11:50.121 --> 02:11:50.954
<v Speaker 2>people part of the goal as,</v>
<v Speaker 2>as the AI gets more and more </v>

2383
02:11:53.361 --> 02:11:56.480
<v Speaker 2>intelligent.</v>
<v Speaker 2>You sort of getting the ai locked into a</v>

2384
02:11:56.481 --> 02:12:00.140
<v Speaker 2>very positive,</v>
<v Speaker 2>reflective and compassionate state.</v>

2385
02:12:00.170 --> 02:12:01.003
<v Speaker 2>And I think,</v>
<v Speaker 2>I think there's a lot of things in the </v>

2386
02:12:03.351 --> 02:12:04.184
<v Speaker 2>human psyche and evolutionary history </v>
<v Speaker 2>that hold us back from being optimally </v>

2387
02:12:08.811 --> 02:12:13.610
<v Speaker 2>compassionate and that if we create the </v>
<v Speaker 2>ai in the right way,</v>

2388
02:12:13.880 --> 02:12:15.740
<v Speaker 2>it will be not only much more </v>
<v Speaker 2>intelligent,</v>

2389
02:12:16.280 --> 02:12:20.180
<v Speaker 2>much more compassionate than than human </v>
<v Speaker 2>beings are.</v>

2390
02:12:20.240 --> 02:12:23.110
<v Speaker 2>And I mean this,</v>
<v Speaker 2>we'd better do that like a.</v>

2391
02:12:23.150 --> 02:12:26.120
<v Speaker 2>otherwise the human race is probably </v>
<v Speaker 2>screwed to be blunt.</v>

2392
02:12:26.180 --> 02:12:27.013
<v Speaker 2>I mean,</v>
<v Speaker 2>if I think human beings are creating a </v>

2393
02:12:28.851 --> 02:12:31.250
<v Speaker 2>lot of other technologies now with a lot</v>
<v Speaker 2>of power,</v>

2394
02:12:31.251 --> 02:12:34.480
<v Speaker 2>we're creating synthetic biology,</v>
<v Speaker 2>we're creating nano technology,</v>

2395
02:12:34.710 --> 02:12:35.543
<v Speaker 2>you know,</v>
<v Speaker 2>we're creating smaller and smaller </v>

2396
02:12:36.650 --> 02:12:39.230
<v Speaker 2>nuclear weapons and we can't control </v>
<v Speaker 2>their proliferation.</v>

2397
02:12:39.231 --> 02:12:40.064
<v Speaker 2>We're poisoning our environment.</v>
<v Speaker 2>I think if we can't create something </v>

2398
02:12:42.771 --> 02:12:46.940
<v Speaker 2>that's normally more intelligent but </v>
<v Speaker 2>more wise and compassionate than we are,</v>

2399
02:12:47.270 --> 02:12:50.960
<v Speaker 2>we're probably going to destroy </v>
<v Speaker 2>ourselves by some method or another.</v>

2400
02:12:50.961 --> 02:12:51.794
<v Speaker 2>I mean,</v>
<v Speaker 2>with something like Donald trump </v>

2401
02:12:52.761 --> 02:12:56.390
<v Speaker 2>becoming president.</v>
<v Speaker 2>You see what happens when this,</v>

2402
02:12:56.630 --> 02:12:57.463
<v Speaker 2>you know,</v>
<v Speaker 2>primitive hindbrain and when our are </v>

2403
02:13:01.500 --> 02:13:02.420
<v Speaker 2>unchecked,</v>
<v Speaker 2>you know,</v>

2404
02:13:02.510 --> 02:13:05.510
<v Speaker 2>mammalian emotions of anger and,</v>
<v Speaker 2>and you know,</v>

2405
02:13:05.511 --> 02:13:08.450
<v Speaker 2>status seeking and ego and rage and </v>
<v Speaker 2>lost.</v>

2406
02:13:08.990 --> 02:13:13.340
<v Speaker 2>When these things are controlling these </v>
<v Speaker 2>highly advanced technologies,</v>

2407
02:13:13.341 --> 02:13:16.130
<v Speaker 2>this,</v>
<v Speaker 2>this is not going to come to a good end.</v>

2408
02:13:16.190 --> 02:13:21.190
<v Speaker 2>So we want compassionate general </v>
<v Speaker 2>intelligences and this is what we should</v>

2409
02:13:21.531 --> 02:13:26.531
<v Speaker 2>be orienting ourselves toward.</v>
<v Speaker 2>And so we need to shift the focus of the</v>

2410
02:13:28.341 --> 02:13:32.150
<v Speaker 2>AI and technology development on the </v>
<v Speaker 2>planet toward,</v>

2411
02:13:32.390 --> 02:13:33.330
<v Speaker 2>you know,</v>
<v Speaker 2>benevolent,</v>

2412
02:13:33.360 --> 02:13:35.700
<v Speaker 2>compassionate,</v>
<v Speaker 2>General Intelligence.</v>

2413
02:13:35.701 --> 02:13:37.010
<v Speaker 2>And this is subtle,</v>
<v Speaker 2>right?</v>

2414
02:13:37.020 --> 02:13:37.853
<v Speaker 2>Because you need to work with the </v>
<v Speaker 2>establishment rather than overthrowing </v>

2415
02:13:41.790 --> 02:13:43.050
<v Speaker 2>it,</v>
<v Speaker 2>which isn't going to be viable,</v>

2416
02:13:43.051 --> 02:13:43.884
<v Speaker 2>so this is while we're creating this </v>
<v Speaker 2>decentralized self organizing ai </v>

2417
02:13:47.851 --> 02:13:49.520
<v Speaker 2>network,</v>
<v Speaker 2>the singularity net,</v>

2418
02:13:49.980 --> 02:13:51.980
<v Speaker 2>then we're creating a for profit </v>
<v Speaker 2>company,</v>

2419
02:13:51.981 --> 02:13:52.814
<v Speaker 2>singularity studio,</v>
<v Speaker 2>which will get large enterprises to use </v>

2420
02:13:55.801 --> 02:13:56.634
<v Speaker 2>this decentralized network.</v>
<v Speaker 2>Then we're creating these robots like </v>

2421
02:14:00.391 --> 02:14:01.224
<v Speaker 2>Sophia,</v>
<v Speaker 2>which will be mass manufactured in the </v>

2422
02:14:03.570 --> 02:14:04.600
<v Speaker 2>next couple of years,</v>
<v Speaker 2>rolled these out,</v>

2423
02:14:04.610 --> 02:14:05.443
<v Speaker 2>this service robots everywhere around </v>
<v Speaker 2>the world to interact with people and </v>

2424
02:14:09.720 --> 02:14:12.570
<v Speaker 2>providing valuable services in homes and</v>
<v Speaker 2>offices,</v>

2425
02:14:12.810 --> 02:14:14.750
<v Speaker 2>but also interacting with people you </v>
<v Speaker 2>know,</v>

2426
02:14:14.760 --> 02:14:19.760
<v Speaker 2>in a loving and compassionate way.</v>
<v Speaker 2>So we need to start now because we don't</v>

2427
02:14:22.291 --> 02:14:23.124
<v Speaker 2>actually know if it's going to be years </v>
<v Speaker 2>or decades before we get to this </v>

2428
02:14:26.201 --> 02:14:29.700
<v Speaker 2>singularity and we wanted to be assured </v>
<v Speaker 2>as we can that when we get there,</v>

2429
02:14:29.701 --> 02:14:32.520
<v Speaker 2>it happens in a beneficial way for </v>
<v Speaker 2>everyone.</v>

2430
02:14:32.550 --> 02:14:34.530
<v Speaker 2>Right.</v>
<v Speaker 2>And things like robots,</v>

2431
02:14:34.830 --> 02:14:38.190
<v Speaker 2>blockchain and ai learning algorithms </v>
<v Speaker 2>are our tools.</v>

2432
02:14:38.191 --> 02:14:39.870
<v Speaker 2>Toward that end.</v>
<v Speaker 2>Ben,</v>

2433
02:14:40.110 --> 02:14:40.943
<v Speaker 2>I appreciate your optimism.</v>
<v Speaker 2>I appreciate coming near explaining all </v>

2434
02:14:42.991 --> 02:14:45.750
<v Speaker 2>this stuff for us and I appreciate all </v>
<v Speaker 2>your work.</v>

2435
02:14:45.850 --> 02:14:48.030
<v Speaker 2>It's really amazing.</v>
<v Speaker 2>Fascinating stuff.</v>

2436
02:14:48.120 --> 02:14:48.570
<v Speaker 2>Yeah.</v>
<v Speaker 2>Yeah.</v>

2437
02:14:48.570 --> 02:14:50.880
<v Speaker 2>Well thanks for having me.</v>
<v Speaker 2>It's really fun.</v>

2438
02:14:50.881 --> 02:14:52.950
<v Speaker 2>That was a wide ranging conversation,</v>
<v Speaker 2>so yeah,</v>

2439
02:14:52.960 --> 02:14:53.793
<v Speaker 2>would it will be great to come back next</v>
<v Speaker 2>year and update you on the state of the </v>

2440
02:14:57.151 --> 02:15:01.440
<v Speaker 2>singularity once a year and just by the </v>
<v Speaker 2>time you come from maybe who knows,</v>

2441
02:15:01.470 --> 02:15:02.303
<v Speaker 2>a year from now,</v>
<v Speaker 2>the world might be a totally different </v>

2442
02:15:03.481 --> 02:15:05.760
<v Speaker 2>place.</v>
<v Speaker 2>Maybe a robot.</v>

2443
02:15:06.320 --> 02:15:09.080
<v Speaker 2>A robot now.</v>
<v Speaker 2>Oh,</v>

2444
02:15:10.810 --> 02:15:12.450
<v Speaker 2>thank you.</v>
<v Speaker 2>Thank you everybody.</v>

