WEBVTT

1
00:00:01.030 --> 00:00:02.120
<v Speaker 1>Hello Greek</v>

2
00:00:02.120 --> 00:00:06.110
<v Speaker 2>bitches.</v>
<v Speaker 2>Do you think in our lifetimes and or in</v>

3
00:00:06.111 --> 00:00:11.111
<v Speaker 2>our children's lifetime,</v>
<v Speaker 2>it's feasible that we figure out a way</v>

4
00:00:12.080 --> 00:00:15.980
<v Speaker 2>in some way to,</v>
<v Speaker 2>I'm not endorsing like taking people's</v>

5
00:00:15.981 --> 00:00:19.280
<v Speaker 2>money and giving it to other people,</v>
<v Speaker 2>but in some sort of a way to eliminate</v>

6
00:00:19.281 --> 00:00:21.440
<v Speaker 2>poverty.</v>
<v Speaker 2>Is that even possible?</v>

7
00:00:21.530 --> 00:00:24.890
<v Speaker 2>Is it ever going to be possible to</v>
<v Speaker 2>completely eliminate poverty worldwide</v>

8
00:00:25.100 --> 00:00:28.430
<v Speaker 2>and within like a lifetime?</v>
<v Speaker 2>Well,</v>

9
00:00:28.760 --> 00:00:32.330
<v Speaker 2>I think we talked about this the last</v>
<v Speaker 2>time when we spoke about ai,</v>

10
00:00:32.331 --> 00:00:36.530
<v Speaker 2>but I mean this is the implication of</v>
<v Speaker 2>much of what we talked about here.</v>

11
00:00:36.531 --> 00:00:40.280
<v Speaker 2>If you,</v>
<v Speaker 2>if you imagine building the perfect</v>

12
00:00:40.281 --> 00:00:42.590
<v Speaker 2>labor saving technology,</v>
<v Speaker 2>right?</v>

13
00:00:42.650 --> 00:00:46.430
<v Speaker 2>What do you imagine?</v>
<v Speaker 2>Imagine just having a machine that can</v>

14
00:00:46.431 --> 00:00:49.700
<v Speaker 2>build any machine that can do any human</v>
<v Speaker 2>labor.</v>

15
00:00:50.610 --> 00:00:53.660
<v Speaker 2>You powered by sunlight more or less for</v>
<v Speaker 2>the cost of raw materials,</v>

16
00:00:53.661 --> 00:00:56.660
<v Speaker 2>right?</v>
<v Speaker 2>So you're talking about the ultimate</v>

17
00:00:56.661 --> 00:00:59.930
<v Speaker 2>wealth generation device and now we're</v>
<v Speaker 2>not just talking about blue collar</v>

18
00:00:59.931 --> 00:01:02.090
<v Speaker 2>labor,</v>
<v Speaker 2>we're talking about the kind of labor</v>

19
00:01:02.710 --> 00:01:03.770
<v Speaker 2>you and I do,</v>
<v Speaker 2>right?</v>

20
00:01:03.770 --> 00:01:06.980
<v Speaker 2>So like artistic labor and scientific</v>
<v Speaker 2>labor and,</v>

21
00:01:07.460 --> 00:01:08.630
<v Speaker 2>um,</v>
<v Speaker 2>you know,</v>

22
00:01:08.870 --> 00:01:11.480
<v Speaker 2>just a machine that comes up with good</v>
<v Speaker 2>ideas.</v>

23
00:01:11.481 --> 00:01:11.890
<v Speaker 2>Right?</v>
<v Speaker 2>And He,</v>

24
00:01:11.890 --> 00:01:13.910
<v Speaker 2>what we're talking about,</v>
<v Speaker 2>general artificial intelligence,</v>

25
00:01:14.480 --> 00:01:18.380
<v Speaker 2>um,</v>
<v Speaker 2>this if in the right political and</v>

26
00:01:18.381 --> 00:01:22.400
<v Speaker 2>economic system,</v>
<v Speaker 2>this would just cancel any need for</v>

27
00:01:22.401 --> 00:01:25.460
<v Speaker 2>people to have to work to survive,</v>
<v Speaker 2>right?</v>

28
00:01:25.490 --> 00:01:28.070
<v Speaker 2>It just would be,</v>
<v Speaker 2>there'd be enough of everything to go</v>

29
00:01:28.071 --> 00:01:30.530
<v Speaker 2>around.</v>
<v Speaker 2>And then the question would be,</v>

30
00:01:31.010 --> 00:01:33.920
<v Speaker 2>do we have the right political and</v>
<v Speaker 2>economic system where we,</v>

31
00:01:33.980 --> 00:01:37.550
<v Speaker 2>where we actually could spread that</v>
<v Speaker 2>wealth or would we just,</v>

32
00:01:37.650 --> 00:01:41.900
<v Speaker 2>we just find ourselves in some kind of</v>
<v Speaker 2>horrendous arms race and,</v>

33
00:01:41.901 --> 00:01:43.040
<v Speaker 2>and,</v>
<v Speaker 2>uh,</v>

34
00:01:43.310 --> 00:01:44.143
<v Speaker 2>uh,</v>
<v Speaker 2>situation of,</v>

35
00:01:44.150 --> 00:01:46.430
<v Speaker 2>of wealth inequality,</v>
<v Speaker 2>uh,</v>

36
00:01:46.660 --> 00:01:49.320
<v Speaker 2>unlike any we've ever seen.</v>
<v Speaker 2>Um,</v>

37
00:01:50.360 --> 00:01:51.320
<v Speaker 2>it's a,</v>
<v Speaker 2>um,</v>

38
00:01:51.510 --> 00:01:52.550
<v Speaker 2>we don't,</v>
<v Speaker 2>we don't have the,</v>

39
00:01:52.900 --> 00:01:56.390
<v Speaker 2>it's not in place now and if someone</v>
<v Speaker 2>just handed us this device,</v>

40
00:01:56.870 --> 00:01:57.260
<v Speaker 2>you know,</v>
<v Speaker 2>if,</v>

41
00:01:57.260 --> 00:01:59.030
<v Speaker 2>if,</v>
<v Speaker 2>um,</v>

42
00:02:00.260 --> 00:02:01.040
<v Speaker 2>and it were,</v>
<v Speaker 2>you know,</v>

43
00:02:01.040 --> 00:02:03.320
<v Speaker 2>all of my concerns about Ai,</v>
<v Speaker 2>we're gone.</v>

44
00:02:03.320 --> 00:02:05.000
<v Speaker 2>I mean,</v>
<v Speaker 2>there's no question about this thing,</v>

45
00:02:05.200 --> 00:02:07.460
<v Speaker 2>uh,</v>
<v Speaker 2>doing things we didn't want it would do</v>

46
00:02:07.461 --> 00:02:10.460
<v Speaker 2>exactly what we want when we want it.</v>
<v Speaker 2>And there's no,</v>

47
00:02:10.461 --> 00:02:12.680
<v Speaker 2>there's just no danger of it.</v>
<v Speaker 2>Uh,</v>

48
00:02:12.810 --> 00:02:14.870
<v Speaker 2>it's interest becoming misaligned with</v>
<v Speaker 2>our own.</v>

49
00:02:14.871 --> 00:02:18.530
<v Speaker 2>It's just like a perfect oracle and a</v>
<v Speaker 2>perfect designer of new technology.</v>

50
00:02:19.100 --> 00:02:20.580
<v Speaker 2>Um,</v>
<v Speaker 2>if,</v>

51
00:02:20.740 --> 00:02:23.660
<v Speaker 2>if it was handed to us now,</v>
<v Speaker 2>you know,</v>

52
00:02:23.810 --> 00:02:27.920
<v Speaker 2>I would expect just complete chaos,</v>
<v Speaker 2>right?</v>

53
00:02:27.921 --> 00:02:28.670
<v Speaker 2>I would explain it if,</v>
<v Speaker 2>if,</v>

54
00:02:28.670 --> 00:02:33.670
<v Speaker 2>if facebook built this thing tomorrow</v>
<v Speaker 2>and announced it or rumor spread that</v>

55
00:02:34.041 --> 00:02:34.970
<v Speaker 2>they had built it,</v>
<v Speaker 2>right.</v>

56
00:02:35.690 --> 00:02:37.970
<v Speaker 2>What are the implications for Russia and</v>
<v Speaker 2>China?</v>

57
00:02:38.120 --> 00:02:42.500
<v Speaker 2>Well,</v>
<v Speaker 2>in so far as they are as adversarial as</v>

58
00:02:42.501 --> 00:02:44.810
<v Speaker 2>they are now,</v>
<v Speaker 2>it would be rational for them to just</v>

59
00:02:44.811 --> 00:02:46.520
<v Speaker 2>new California,</v>
<v Speaker 2>right?</v>

60
00:02:46.521 --> 00:02:50.200
<v Speaker 2>Because,</v>
<v Speaker 2>because the happiness device is,</v>

61
00:02:50.240 --> 00:02:52.550
<v Speaker 2>there's a winner take all scenario.</v>
<v Speaker 2>I mean you,</v>

62
00:02:52.551 --> 00:02:53.720
<v Speaker 2>you,</v>
<v Speaker 2>when the world,</v>

63
00:02:53.960 --> 00:02:56.450
<v Speaker 2>if you have this device,</v>
<v Speaker 2>you can turn the lights off in China.</v>

64
00:02:56.500 --> 00:02:58.070
<v Speaker 2>You know,</v>
<v Speaker 2>the moment you have this device,</v>

65
00:02:58.071 --> 00:02:59.740
<v Speaker 2>you can just,</v>
<v Speaker 2>it's the ultimate.</v>

66
00:03:00.320 --> 00:03:03.940
<v Speaker 2>Um,</v>
<v Speaker 2>cause literally you were talking about</v>

67
00:03:04.210 --> 00:03:06.370
<v Speaker 2>and you know,</v>
<v Speaker 2>many people are made doubt whether such</v>

68
00:03:06.371 --> 00:03:08.410
<v Speaker 2>a thing as possible.</v>
<v Speaker 2>But again,</v>

69
00:03:08.411 --> 00:03:13.330
<v Speaker 2>we're just talking about the</v>
<v Speaker 2>implications of intelligence that can</v>

70
00:03:13.331 --> 00:03:18.331
<v Speaker 2>make refinements to itself in over time.</v>
<v Speaker 2>Course that is bears no relationship to</v>

71
00:03:22.121 --> 00:03:24.490
<v Speaker 2>what we experience as apes.</v>
<v Speaker 2>Right?</v>

72
00:03:24.491 --> 00:03:28.390
<v Speaker 2>So you're talking about a system that</v>
<v Speaker 2>can make changes to his own source code,</v>

73
00:03:28.900 --> 00:03:31.390
<v Speaker 2>um,</v>
<v Speaker 2>and become better and better at</v>

74
00:03:31.420 --> 00:03:33.910
<v Speaker 2>learning.</v>
<v Speaker 2>And more and more knowledgeable has</v>

75
00:03:33.911 --> 00:03:36.490
<v Speaker 2>instantaneous,</v>
<v Speaker 2>if we give it access to the Internet,</v>

76
00:03:37.150 --> 00:03:40.330
<v Speaker 2>it has instantaneous access to all human</v>
<v Speaker 2>and machine knowledge.</v>

77
00:03:40.750 --> 00:03:43.360
<v Speaker 2>And uh,</v>
<v Speaker 2>it does,</v>

78
00:03:43.480 --> 00:03:46.570
<v Speaker 2>you know,</v>
<v Speaker 2>thousands of years of work every,</v>

79
00:03:46.870 --> 00:03:49.930
<v Speaker 2>every day of our lives,</v>
<v Speaker 2>right?</v>

80
00:03:49.931 --> 00:03:53.110
<v Speaker 2>They just thousands of years of</v>
<v Speaker 2>equivalent human level intellectual</v>

81
00:03:53.111 --> 00:03:54.450
<v Speaker 2>work.</v>
<v Speaker 2>Um,</v>

82
00:03:55.270 --> 00:03:56.370
<v Speaker 2>it's just a,</v>
<v Speaker 2>it's on,</v>

83
00:03:56.680 --> 00:03:58.990
<v Speaker 2>I mean,</v>
<v Speaker 2>our intuitions completely falter at t</v>

84
00:03:59.020 --> 00:04:01.630
<v Speaker 2>to,</v>
<v Speaker 2>to capture just how immensely powerful</v>

85
00:04:01.631 --> 00:04:02.440
<v Speaker 2>such a thing would be.</v>

86
00:04:02.440 --> 00:04:05.560
<v Speaker 2>And there's no reason to think this</v>
<v Speaker 2>isn't possible.</v>

87
00:04:05.860 --> 00:04:06.170
<v Speaker 2>I mean,</v>
<v Speaker 2>the,</v>

88
00:04:06.170 --> 00:04:07.840
<v Speaker 2>the,</v>
<v Speaker 2>the most skeptical thing you can</v>

89
00:04:07.900 --> 00:04:11.680
<v Speaker 2>honestly say about this is that this</v>
<v Speaker 2>isn't coming soon,</v>

90
00:04:12.070 --> 00:04:13.270
<v Speaker 2>right?</v>
<v Speaker 2>It's like this is not,</v>

91
00:04:13.271 --> 00:04:17.710
<v Speaker 2>but to say that this is not possible</v>
<v Speaker 2>makes no scientific sense at this point.</v>

92
00:04:17.960 --> 00:04:22.960
<v Speaker 2>There's no reason to think that a</v>
<v Speaker 2>sufficiently advanced digital computer</v>

93
00:04:23.770 --> 00:04:28.090
<v Speaker 2>can't pee,</v>
<v Speaker 2>can't instantiate general intelligence</v>

94
00:04:28.091 --> 00:04:30.720
<v Speaker 2>of a sort that we have.</v>
<v Speaker 2>There's no reason to think that the</v>

95
00:04:30.790 --> 00:04:34.930
<v Speaker 2>intelligence has to be at bottom some</v>
<v Speaker 2>form of information processing.</v>

96
00:04:34.960 --> 00:04:39.960
<v Speaker 2>And if we get the algorithm right with</v>
<v Speaker 2>enough hardware resources and the,</v>

97
00:04:41.060 --> 00:04:43.390
<v Speaker 2>and the limit is definitely not the</v>
<v Speaker 2>hardware at this point.</v>

98
00:04:43.391 --> 00:04:44.224
<v Speaker 2>It's,</v>
<v Speaker 2>it's the,</v>

99
00:04:44.290 --> 00:04:46.600
<v Speaker 2>the algorithms,</v>
<v Speaker 2>um,</v>

100
00:04:47.980 --> 00:04:49.380
<v Speaker 2>there's just no reason to think this</v>
<v Speaker 2>could,</v>

101
00:04:49.381 --> 00:04:52.180
<v Speaker 2>can't take off and,</v>
<v Speaker 2>and scale.</v>

102
00:04:52.181 --> 00:04:54.280
<v Speaker 2>And then we would be in the presence of</v>
<v Speaker 2>something that is,</v>

103
00:04:54.820 --> 00:04:55.653
<v Speaker 2>that is,</v>
<v Speaker 2>uh,</v>

104
00:04:56.050 --> 00:05:01.050
<v Speaker 2>like having an,</v>
<v Speaker 2>an alternate human civilization in a box</v>

105
00:05:01.420 --> 00:05:04.720
<v Speaker 2>that is making thousands of years of</v>
<v Speaker 2>progress every day.</v>

106
00:05:05.110 --> 00:05:06.700
<v Speaker 2>Right?</v>
<v Speaker 2>So just imagine that if you had in a</v>

107
00:05:06.701 --> 00:05:08.200
<v Speaker 2>box,</v>
<v Speaker 2>you know,</v>

108
00:05:08.201 --> 00:05:10.210
<v Speaker 2>the 10 smartest people who've ever</v>
<v Speaker 2>lived,</v>

109
00:05:10.810 --> 00:05:13.060
<v Speaker 2>and you know,</v>
<v Speaker 2>every time,</v>

110
00:05:13.061 --> 00:05:15.520
<v Speaker 2>every week,</v>
<v Speaker 2>they make 20,000 years of progress,</v>

111
00:05:15.710 --> 00:05:18.190
<v Speaker 2>right?</v>
<v Speaker 2>Because that is the actual,</v>

112
00:05:18.810 --> 00:05:22.300
<v Speaker 2>we're talking about electronic circuits</v>
<v Speaker 2>being a million times faster than,</v>

113
00:05:22.360 --> 00:05:25.690
<v Speaker 2>than biological circuits.</v>
<v Speaker 2>So even if it was just,</v>

114
00:05:25.840 --> 00:05:27.700
<v Speaker 2>and I,</v>
<v Speaker 2>I believe I said this the last time we</v>

115
00:05:27.701 --> 00:05:28.990
<v Speaker 2>talked about ai,</v>
<v Speaker 2>but this is,</v>

116
00:05:29.020 --> 00:05:30.610
<v Speaker 2>you know,</v>
<v Speaker 2>this is what brings it home for me.</v>

117
00:05:31.240 --> 00:05:33.760
<v Speaker 2>Even if it's just a matter of faster,</v>
<v Speaker 2>right?</v>

118
00:05:33.790 --> 00:05:36.160
<v Speaker 2>It's not,</v>
<v Speaker 2>it's not anything especially spooky is</v>

119
00:05:36.161 --> 00:05:38.440
<v Speaker 2>chest.</v>
<v Speaker 2>This can do human level,</v>

120
00:05:38.441 --> 00:05:41.620
<v Speaker 2>intellectual work,</v>
<v Speaker 2>but just a million times faster.</v>

121
00:05:42.130 --> 00:05:45.310
<v Speaker 2>And again,</v>
<v Speaker 2>this is totally under sells.</v>

122
00:05:45.320 --> 00:05:50.050
<v Speaker 2>The prospects of superintelligence,</v>
<v Speaker 2>I think human level intellectual work</v>

123
00:05:50.051 --> 00:05:51.580
<v Speaker 2>is,</v>
<v Speaker 2>is um,</v>

124
00:05:52.360 --> 00:05:54.580
<v Speaker 2>uh,</v>
<v Speaker 2>it's going to seem pretty paltry in the</v>

125
00:05:54.581 --> 00:05:56.650
<v Speaker 2>end.</v>
<v Speaker 2>But if you just imagine just speeding it</v>

126
00:05:56.651 --> 00:05:57.750
<v Speaker 2>up,</v>
<v Speaker 2>if you imagine if a full,</v>

127
00:05:57.840 --> 00:06:02.630
<v Speaker 2>if we were doing this podcast,</v>
<v Speaker 2>imagine how smart I would seem if</v>

128
00:06:02.690 --> 00:06:04.370
<v Speaker 2>between every sentence.</v>

129
00:06:04.880 --> 00:06:09.440
<v Speaker 2>I actually had a year to figure out what</v>
<v Speaker 2>I was going to say next.</v>

130
00:06:09.441 --> 00:06:11.000
<v Speaker 2>Right?</v>
<v Speaker 2>And so I say this one sentence and you</v>

131
00:06:11.001 --> 00:06:12.980
<v Speaker 2>say,</v>
<v Speaker 2>you asked me a question and then in my</v>

132
00:06:12.981 --> 00:06:14.030
<v Speaker 2>world,</v>
<v Speaker 2>I just have a year,</v>

133
00:06:14.090 --> 00:06:16.340
<v Speaker 2>I'm going to go spend the next year</v>
<v Speaker 2>getting,</v>

134
00:06:16.341 --> 00:06:18.230
<v Speaker 2>getting ready for,</v>
<v Speaker 2>for Joe,</v>

135
00:06:18.560 --> 00:06:22.940
<v Speaker 2>and it's going to be perfect.</v>
<v Speaker 2>And this is just compounding upon</v>

136
00:06:22.941 --> 00:06:25.400
<v Speaker 2>itself.</v>
<v Speaker 2>Like not only can I not only,</v>

137
00:06:25.610 --> 00:06:26.660
<v Speaker 2>uh,</v>
<v Speaker 2>I,</v>

138
00:06:26.661 --> 00:06:28.490
<v Speaker 2>um,</v>
<v Speaker 2>am I working faster?</v>

139
00:06:29.030 --> 00:06:32.900
<v Speaker 2>Ultimately I can change my,</v>
<v Speaker 2>my ability to work faster.</v>

140
00:06:32.901 --> 00:06:34.430
<v Speaker 2>I mean,</v>
<v Speaker 2>like we were talking about software that</v>

141
00:06:34.431 --> 00:06:36.190
<v Speaker 2>can change itself.</v>
<v Speaker 2>You're talking about something that that</v>

142
00:06:36.410 --> 00:06:40.100
<v Speaker 2>becomes self improving.</v>
<v Speaker 2>So there's a compounding function there.</v>

143
00:06:40.101 --> 00:06:41.420
<v Speaker 2>But,</v>
<v Speaker 2>um,</v>

144
00:06:42.320 --> 00:06:45.590
<v Speaker 2>it's the point is,</v>
<v Speaker 2>is unimaginable,</v>

145
00:06:45.890 --> 00:06:47.870
<v Speaker 2>uh,</v>
<v Speaker 2>in terms of how,</v>

146
00:06:48.260 --> 00:06:52.490
<v Speaker 2>uh,</v>
<v Speaker 2>how much change this could effect.</v>

147
00:06:52.910 --> 00:06:55.970
<v Speaker 2>And if you imagine the best case</v>
<v Speaker 2>scenario where this is under our</v>

148
00:06:55.971 --> 00:06:56.900
<v Speaker 2>control,</v>
<v Speaker 2>right,</v>

149
00:06:56.901 --> 00:06:58.700
<v Speaker 2>where there's no alignment problem,</v>
<v Speaker 2>where it's just,</v>

150
00:06:58.701 --> 00:07:00.050
<v Speaker 2>it doesn't,</v>
<v Speaker 2>this thing doesn't do anything that</v>

151
00:07:00.051 --> 00:07:03.050
<v Speaker 2>surprises us,</v>
<v Speaker 2>this thing we'll always take direction</v>

152
00:07:03.051 --> 00:07:03.884
<v Speaker 2>from us.</v>

153
00:07:04.040 --> 00:07:06.260
<v Speaker 2>It will never,</v>
<v Speaker 2>it will never develop interests of its</v>

154
00:07:06.261 --> 00:07:07.430
<v Speaker 2>own.</v>
<v Speaker 2>Right.</v>

155
00:07:07.850 --> 00:07:09.740
<v Speaker 2>Which is again the fear.</v>
<v Speaker 2>But let's,</v>

156
00:07:09.741 --> 00:07:14.340
<v Speaker 2>let's just say this is totally obedient,</v>
<v Speaker 2>is just an oracle and a genie route,</v>

157
00:07:14.470 --> 00:07:15.100
<v Speaker 2>you know,</v>
<v Speaker 2>in,</v>

158
00:07:15.100 --> 00:07:17.270
<v Speaker 2>in one.</v>
<v Speaker 2>And um,</v>

159
00:07:17.720 --> 00:07:18.381
<v Speaker 2>you know,</v>
<v Speaker 2>we say,</v>

160
00:07:18.381 --> 00:07:19.790
<v Speaker 2>you know,</v>
<v Speaker 2>cure Alzheimer's and it cures</v>

161
00:07:19.791 --> 00:07:21.050
<v Speaker 2>Alzheimer's.</v>
<v Speaker 2>You know,</v>

162
00:07:21.051 --> 00:07:23.520
<v Speaker 2>you solve the protein folding problem</v>
<v Speaker 2>and,</v>

163
00:07:23.521 --> 00:07:26.390
<v Speaker 2>and it just is,</v>
<v Speaker 2>it's just and running and to develop a</v>

164
00:07:26.391 --> 00:07:30.230
<v Speaker 2>perfect nanotechnology and it does that.</v>
<v Speaker 2>This is all,</v>

165
00:07:30.231 --> 00:07:31.850
<v Speaker 2>again,</v>
<v Speaker 2>going back to David Deutsch,</v>

166
00:07:32.420 --> 00:07:36.650
<v Speaker 2>there's no reason to think this isn't</v>
<v Speaker 2>possible because anything that's</v>

167
00:07:36.651 --> 00:07:41.651
<v Speaker 2>compatible with the laws of physics can</v>
<v Speaker 2>be done given the requisite knowledge,</v>

168
00:07:42.080 --> 00:07:43.430
<v Speaker 2>right?</v>
<v Speaker 2>So you just,</v>

169
00:07:43.490 --> 00:07:47.240
<v Speaker 2>you get enough intelligence,</v>
<v Speaker 2>as long as you're not violating the laws</v>

170
00:07:47.241 --> 00:07:49.940
<v Speaker 2>of physics,</v>
<v Speaker 2>you can do something in that space.</v>

171
00:07:50.480 --> 00:07:52.220
<v Speaker 2>Um,</v>
<v Speaker 2>so,</v>

172
00:07:52.221 --> 00:07:54.350
<v Speaker 2>but the problem is this is a winner take</v>
<v Speaker 2>all scenario.</v>

173
00:07:54.350 --> 00:07:59.180
<v Speaker 2>So facebook does it tomorrow and China</v>
<v Speaker 2>and Russia find out about it.</v>

174
00:07:59.810 --> 00:08:04.700
<v Speaker 2>They can't afford to wait around to see</v>
<v Speaker 2>whether the US decides to do something</v>

175
00:08:04.730 --> 00:08:07.190
<v Speaker 2>not entirely selfish with this,</v>
<v Speaker 2>right?</v>

176
00:08:07.191 --> 00:08:09.590
<v Speaker 2>Because they're,</v>
<v Speaker 2>the,</v>

177
00:08:09.650 --> 00:08:12.680
<v Speaker 2>their worst fears could be realized if</v>
<v Speaker 2>Donald Trump is president.</v>

178
00:08:12.710 --> 00:08:16.130
<v Speaker 2>What's Donald Trump going to do with a</v>
<v Speaker 2>perfect ai when he has already told the</v>

179
00:08:16.131 --> 00:08:18.560
<v Speaker 2>world that he hates Islam?</v>
<v Speaker 2>Right?</v>

180
00:08:19.040 --> 00:08:20.780
<v Speaker 2>Um,</v>
<v Speaker 2>it's a,</v>

181
00:08:21.800 --> 00:08:22.820
<v Speaker 2>it's a,</v>
<v Speaker 2>um,</v>

182
00:08:23.270 --> 00:08:27.890
<v Speaker 2>we would have to have a political and</v>
<v Speaker 2>economic system that allowed us to</v>

183
00:08:27.891 --> 00:08:32.420
<v Speaker 2>absorb this ultimate wealth saved.</v>
<v Speaker 2>We'll wealth producing technology.</v>

184
00:08:32.510 --> 00:08:33.740
<v Speaker 2>Um,</v>
<v Speaker 2>and,</v>

185
00:08:34.250 --> 00:08:37.020
<v Speaker 2>and again,</v>
<v Speaker 2>so this may all sound like pure Psi Phi</v>

186
00:08:37.021 --> 00:08:39.050
<v Speaker 2>craziness to people.</v>
<v Speaker 2>Um,</v>

187
00:08:39.470 --> 00:08:41.420
<v Speaker 2>I don't think there is any reason to</v>
<v Speaker 2>believe that it is,</v>

188
00:08:41.421 --> 00:08:45.500
<v Speaker 2>but walk way back from that edge of</v>
<v Speaker 2>craziness and just look at,</v>

189
00:08:46.490 --> 00:08:47.810
<v Speaker 2>um,</v>
<v Speaker 2>dumb ai,</v>

190
00:08:47.811 --> 00:08:48.860
<v Speaker 2>you know,</v>
<v Speaker 2>narrow ai,</v>

191
00:08:48.861 --> 00:08:51.980
<v Speaker 2>just self driving cars and automation</v>
<v Speaker 2>and,</v>

192
00:08:52.440 --> 00:08:57.440
<v Speaker 2>um,</v>
<v Speaker 2>intelligent algorithms that can do human</v>

193
00:08:57.631 --> 00:09:02.460
<v Speaker 2>level work</v>
<v Speaker 2>that is already poised to change our</v>

194
00:09:02.461 --> 00:09:06.040
<v Speaker 2>world massively and create massive</v>
<v Speaker 2>wealth inequality,</v>

195
00:09:06.200 --> 00:09:06.640
<v Speaker 2>which we have.</v>

196
00:09:06.640 --> 00:09:08.760
<v Speaker 2>We would have to figure out how to</v>
<v Speaker 2>spread this wealth.</v>

197
00:09:09.050 --> 00:09:10.620
<v Speaker 2>You know,</v>
<v Speaker 2>what do you do when you can automate,</v>

198
00:09:11.560 --> 00:09:13.650
<v Speaker 2>uh,</v>
<v Speaker 2>50% of,</v>

199
00:09:13.850 --> 00:09:17.130
<v Speaker 2>of human labor?</v>
<v Speaker 2>Were you paying attention to the,</v>

200
00:09:17.160 --> 00:09:19.620
<v Speaker 2>uh,</v>
<v Speaker 2>artificial intelligence go match?</v>

201
00:09:19.621 --> 00:09:20.520
<v Speaker 2>Yeah.</v>
<v Speaker 2>Yeah.</v>

202
00:09:20.960 --> 00:09:21.980
<v Speaker 2>Uh,</v>
<v Speaker 2>no,</v>

203
00:09:22.120 --> 00:09:24.450
<v Speaker 2>I don't actually play goes,</v>
<v Speaker 2>so I wasn't paying that kind of</v>

204
00:09:24.451 --> 00:09:27.210
<v Speaker 2>attention to it,</v>
<v Speaker 2>but I'm aware of what happened there and</v>

205
00:09:27.211 --> 00:09:28.260
<v Speaker 2>you know,</v>
<v Speaker 2>the rules of go,</v>

206
00:09:29.190 --> 00:09:30.450
<v Speaker 2>um,</v>
<v Speaker 2>not,</v>

207
00:09:30.451 --> 00:09:31.670
<v Speaker 2>not so that I know,</v>
<v Speaker 2>actually I don't,</v>

208
00:09:31.980 --> 00:09:32.860
<v Speaker 2>I don't,</v>
<v Speaker 2>I don't play it a,</v>

209
00:09:32.861 --> 00:09:33.450
<v Speaker 2>no,</v>
<v Speaker 2>I don't,</v>

210
00:09:33.450 --> 00:09:35.310
<v Speaker 2>I don't even know if I know vaguely how</v>
<v Speaker 2>you,</v>

211
00:09:35.340 --> 00:09:36.220
<v Speaker 2>how you,</v>
<v Speaker 2>um,</v>

212
00:09:36.480 --> 00:09:39.270
<v Speaker 2>how it looks when a game is played,</v>
<v Speaker 2>but I don't supposed to be very</v>

213
00:09:39.271 --> 00:09:41.700
<v Speaker 2>complicated.</v>
<v Speaker 2>The more complicated and more</v>

214
00:09:41.701 --> 00:09:43.800
<v Speaker 2>possibilities than chess.</v>
<v Speaker 2>Yeah.</v>

215
00:09:44.190 --> 00:09:49.190
<v Speaker 2>And that's why it took 20 years longer</v>
<v Speaker 2>for a computer to be the best player in</v>

216
00:09:49.861 --> 00:09:51.300
<v Speaker 2>the world.</v>
<v Speaker 2>Um,</v>

217
00:09:51.330 --> 00:09:52.650
<v Speaker 2>it's,</v>
<v Speaker 2>it is.</v>

218
00:09:52.710 --> 00:09:55.860
<v Speaker 2>Um,</v>
<v Speaker 2>did you see how the computer did it too?</v>

219
00:09:56.930 --> 00:09:57.480
<v Speaker 2>Well,</v>
<v Speaker 2>I didn't,</v>

220
00:09:57.480 --> 00:09:58.940
<v Speaker 2>I know,</v>
<v Speaker 2>I mean,</v>

221
00:09:58.960 --> 00:10:01.670
<v Speaker 2>this is to the company that did it is,</v>
<v Speaker 2>um,</v>

222
00:10:02.220 --> 00:10:04.920
<v Speaker 2>deep mind,</v>
<v Speaker 2>which was acquired by Google and they're</v>

223
00:10:05.040 --> 00:10:09.000
<v Speaker 2>at the cutting edge of ai research and</v>
<v Speaker 2>yeah.</v>

224
00:10:09.001 --> 00:10:09.834
<v Speaker 2>Well,</v>
<v Speaker 2>it's,</v>

225
00:10:10.710 --> 00:10:14.940
<v Speaker 2>the cartoons are unfortunately not so</v>
<v Speaker 2>far from what is possible.</v>

226
00:10:14.941 --> 00:10:16.410
<v Speaker 2>But,</v>
<v Speaker 2>um,</v>

227
00:10:17.820 --> 00:10:18.653
<v Speaker 2>the,</v>
<v Speaker 2>uh,</v>

228
00:10:18.960 --> 00:10:20.040
<v Speaker 2>yeah,</v>
<v Speaker 2>I mean there's,</v>

229
00:10:21.120 --> 00:10:21.953
<v Speaker 2>again,</v>
<v Speaker 2>this is no,</v>

230
00:10:21.960 --> 00:10:25.200
<v Speaker 2>this is not general intelligence.</v>
<v Speaker 2>Like we're talking about something like</v>

231
00:10:25.410 --> 00:10:28.290
<v Speaker 2>these are not machines that can even</v>
<v Speaker 2>play tic Tac toe right now.</v>

232
00:10:28.291 --> 00:10:31.320
<v Speaker 2>There's some,</v>
<v Speaker 2>there there've been some moves away from</v>

233
00:10:31.321 --> 00:10:36.321
<v Speaker 2>this or like deep mind has trained an</v>
<v Speaker 2>algorithm to play all of the Atari Games</v>

234
00:10:37.681 --> 00:10:38.100
<v Speaker 2>like,</v>
<v Speaker 2>oh,</v>

235
00:10:38.100 --> 00:10:40.440
<v Speaker 2>from 1980 or whenever.</v>
<v Speaker 2>Um,</v>

236
00:10:40.740 --> 00:10:44.490
<v Speaker 2>and it is very quickly became superhuman</v>
<v Speaker 2>on most of them.</v>

237
00:10:44.491 --> 00:10:44.881
<v Speaker 2>I think.</v>
<v Speaker 2>I,</v>

238
00:10:44.881 --> 00:10:46.650
<v Speaker 2>I don't think it's superhuman and all of</v>
<v Speaker 2>them yet,</v>

239
00:10:46.651 --> 00:10:50.820
<v Speaker 2>but it could play in a space invaders</v>
<v Speaker 2>and all these and breakout and all these</v>

240
00:10:50.821 --> 00:10:51.960
<v Speaker 2>games that are,</v>
<v Speaker 2>are,</v>

241
00:10:51.961 --> 00:10:53.450
<v Speaker 2>um,</v>
<v Speaker 2>uh,</v>

242
00:10:54.370 --> 00:10:57.720
<v Speaker 2>uh,</v>
<v Speaker 2>to highly unlike one another.</v>

243
00:10:58.140 --> 00:11:03.060
<v Speaker 2>And it's the same algorithm becoming</v>
<v Speaker 2>expert in superhuman and all of them.</v>

244
00:11:03.061 --> 00:11:05.700
<v Speaker 2>And that's,</v>
<v Speaker 2>that's a new paradigm and it's using a</v>

245
00:11:05.701 --> 00:11:08.910
<v Speaker 2>technique called deep learning for that.</v>
<v Speaker 2>Um,</v>

246
00:11:09.690 --> 00:11:12.450
<v Speaker 2>and that's,</v>
<v Speaker 2>and that's been very exciting and I will</v>

247
00:11:12.451 --> 00:11:14.330
<v Speaker 2>be incredibly useful.</v>
<v Speaker 2>You know,</v>

248
00:11:14.340 --> 00:11:15.031
<v Speaker 2>this is,</v>
<v Speaker 2>I mean,</v>

249
00:11:15.031 --> 00:11:16.230
<v Speaker 2>the other,</v>
<v Speaker 2>the flip side of all of this,</v>

250
00:11:16.231 --> 00:11:19.020
<v Speaker 2>I know that everything I tend to say on</v>
<v Speaker 2>this sound scary,</v>

251
00:11:19.021 --> 00:11:21.210
<v Speaker 2>but this is all like,</v>
<v Speaker 2>I mean,</v>

252
00:11:21.670 --> 00:11:25.080
<v Speaker 2>the next scariest thing is not to do any</v>
<v Speaker 2>of this stuff.</v>

253
00:11:25.110 --> 00:11:26.850
<v Speaker 2>It's like we,</v>
<v Speaker 2>we want intelligence,</v>

254
00:11:26.851 --> 00:11:30.390
<v Speaker 2>we want automation,</v>
<v Speaker 2>we want to figure out how to solve</v>

255
00:11:30.391 --> 00:11:32.910
<v Speaker 2>problems that we can't get soft.</v>
<v Speaker 2>So like intelligence is the best thing</v>

256
00:11:32.911 --> 00:11:34.440
<v Speaker 2>we've got.</v>
<v Speaker 2>So we want more of it.</v>

257
00:11:34.990 --> 00:11:37.140
<v Speaker 2>Uh,</v>
<v Speaker 2>but we have to have a system where,</v>

258
00:11:37.510 --> 00:11:39.570
<v Speaker 2>I mean,</v>
<v Speaker 2>it's scary that we have a system where</v>

259
00:11:39.571 --> 00:11:44.571
<v Speaker 2>if you gave the best possible version of</v>
<v Speaker 2>it to one research lab or to one</v>

260
00:11:45.421 --> 00:11:49.800
<v Speaker 2>government,</v>
<v Speaker 2>it's not obvious that that wouldn't</v>

261
00:11:49.801 --> 00:11:51.900
<v Speaker 2>destroy humanity.</v>
<v Speaker 2>Right.</v>

262
00:11:51.990 --> 00:11:55.390
<v Speaker 2>That wouldn't lead massive dislocations</v>
<v Speaker 2>where you'd have,</v>

263
00:11:55.391 --> 00:11:57.520
<v Speaker 2>you know,</v>
<v Speaker 2>some trillionaire who's trumpeting his</v>

264
00:11:57.521 --> 00:11:59.890
<v Speaker 2>new device and,</v>
<v Speaker 2>and just,</v>

265
00:11:59.891 --> 00:12:02.380
<v Speaker 2>you know,</v>
<v Speaker 2>50% unemployment in the u s you know,</v>

266
00:12:02.381 --> 00:12:03.550
<v Speaker 2>oh,</v>
<v Speaker 2>in a month.</v>

267
00:12:03.551 --> 00:12:03.881
<v Speaker 2>Right?</v>
<v Speaker 2>I mean,</v>

268
00:12:03.881 --> 00:12:04.190
<v Speaker 2>like,</v>
<v Speaker 2>he,</v>

269
00:12:04.190 --> 00:12:06.520
<v Speaker 2>like,</v>
<v Speaker 2>it's not obvious how we would absorb</v>

270
00:12:07.090 --> 00:12:09.100
<v Speaker 2>this level of,</v>
<v Speaker 2>of progress.</v>

271
00:12:09.610 --> 00:12:10.550
<v Speaker 2>Um,</v>
<v Speaker 2>and we,</v>

272
00:12:10.560 --> 00:12:11.830
<v Speaker 2>we,</v>
<v Speaker 2>we definitely have to,</v>

273
00:12:12.340 --> 00:12:14.410
<v Speaker 2>to figure out how to do it.</v>
<v Speaker 2>I know,</v>

274
00:12:14.411 --> 00:12:16.450
<v Speaker 2>of course we can't assume the best case</v>
<v Speaker 2>scenario.</v>

275
00:12:16.451 --> 00:12:17.830
<v Speaker 2>Right?</v>
<v Speaker 2>That's the best case scenario.</v>

276
00:12:18.600 --> 00:12:21.210
<v Speaker 3>I think there's a few people that put it</v>
<v Speaker 3>the way you put it,</v>

277
00:12:21.600 --> 00:12:24.480
<v Speaker 3>that terrify the shit out of people.</v>
<v Speaker 3>Right.</v>

278
00:12:24.510 --> 00:12:29.510
<v Speaker 3>And everyone else seems to have this</v>
<v Speaker 3>rosy vision of increased longevity and</v>

279
00:12:29.701 --> 00:12:34.701
<v Speaker 3>automated everything and everything</v>
<v Speaker 3>fixed and easy to get to work and</v>

280
00:12:35.370 --> 00:12:38.310
<v Speaker 3>medical procedures would be easier.</v>
<v Speaker 3>Or they're going to know how to do it,</v>

281
00:12:38.311 --> 00:12:41.700
<v Speaker 3>but everybody looks at it like,</v>
<v Speaker 3>we are always going to be here,</v>

282
00:12:41.790 --> 00:12:43.470
<v Speaker 3>but are we obsolete?</v>
<v Speaker 3>I mean,</v>

283
00:12:43.471 --> 00:12:48.471
<v Speaker 3>is this idea of a living thing that's</v>
<v Speaker 3>creative and wrapped up in emotions and</v>

284
00:12:48.631 --> 00:12:53.631
<v Speaker 3>lust and desires and jealousy and all</v>
<v Speaker 3>the pettiness that we celebrated all the</v>

285
00:12:53.971 --> 00:12:55.110
<v Speaker 3>time.</v>
<v Speaker 3>We still see it.</v>

286
00:12:55.500 --> 00:12:57.090
<v Speaker 3>It's not getting any better.</v>
<v Speaker 3>Right.</v>

287
00:12:57.360 --> 00:12:59.430
<v Speaker 3>If,</v>
<v Speaker 3>if we obsolete,</v>

288
00:12:59.670 --> 00:13:01.740
<v Speaker 3>I mean,</v>
<v Speaker 3>what if this thing comes along and says,</v>

289
00:13:01.741 --> 00:13:02.990
<v Speaker 3>listen,</v>
<v Speaker 3>there's a way to do it.</v>

290
00:13:02.991 --> 00:13:06.450
<v Speaker 3>You're going to abandon all that stupid</v>
<v Speaker 3>shit and you can abandon all that makes</v>

291
00:13:06.451 --> 00:13:08.760
<v Speaker 3>you all the stuff that makes you fun to</v>
<v Speaker 3>be around.</v>

292
00:13:08.790 --> 00:13:10.170
<v Speaker 3>Yeah.</v>
<v Speaker 3>And also fucks with,</v>

293
00:13:10.171 --> 00:13:12.570
<v Speaker 3>you could live three times as long</v>
<v Speaker 3>without that stuff.</v>

294
00:13:12.970 --> 00:13:13.550
<v Speaker 2>Oh,</v>
<v Speaker 2>well,</v>

295
00:13:13.550 --> 00:13:17.520
<v Speaker 2>I think it,</v>
<v Speaker 2>it would in the best case,</v>

296
00:13:17.521 --> 00:13:20.970
<v Speaker 2>would usher in Ei.</v>
<v Speaker 2>Ei,</v>

297
00:13:22.960 --> 00:13:23.660
<v Speaker 4>yeah.</v>

298
00:13:23.660 --> 00:13:27.800
<v Speaker 2>The possibility of,</v>
<v Speaker 2>of kind of fundamentally creative life</v>

299
00:13:27.950 --> 00:13:31.160
<v Speaker 2>where on the order of something like the</v>
<v Speaker 2>Matrix,</v>

300
00:13:31.161 --> 00:13:36.161
<v Speaker 2>whether it's in the matrix or it's just</v>
<v Speaker 2>in the world that has been made as</v>

301
00:13:38.060 --> 00:13:40.040
<v Speaker 2>beautiful as,</v>
<v Speaker 2>as possible,</v>

302
00:13:40.670 --> 00:13:42.590
<v Speaker 2>um,</v>
<v Speaker 2>based on</v>

303
00:13:44.250 --> 00:13:47.700
<v Speaker 2>what would functionally be an unlimited</v>
<v Speaker 2>resource of intelligence.</v>

304
00:13:47.850 --> 00:13:52.850
<v Speaker 2>Let me just say,</v>
<v Speaker 2>it's just like for there to be a,</v>

305
00:13:53.970 --> 00:13:58.970
<v Speaker 2>an ability to solve problems of a sword</v>
<v Speaker 2>that we can't currently imagine.</v>

306
00:13:59.101 --> 00:13:59.491
<v Speaker 2>I mean,</v>
<v Speaker 2>it's just,</v>

307
00:13:59.491 --> 00:14:01.950
<v Speaker 2>it really is like a place on the map</v>
<v Speaker 2>that you can't,</v>

308
00:14:02.520 --> 00:14:04.680
<v Speaker 2>you can't,</v>
<v Speaker 2>and you can indicate it's over there.</v>

309
00:14:04.681 --> 00:14:06.420
<v Speaker 2>You know,</v>
<v Speaker 2>it was like a blank spot on the map.</v>

310
00:14:06.690 --> 00:14:08.460
<v Speaker 2>This is why it's called the singularity.</v>
<v Speaker 2>Right?</v>

311
00:14:08.461 --> 00:14:09.420
<v Speaker 2>It's like,</v>
<v Speaker 2>this is it.</v>

312
00:14:09.421 --> 00:14:10.980
<v Speaker 2>This is a,</v>
<v Speaker 2>uh,</v>

313
00:14:10.981 --> 00:14:12.120
<v Speaker 2>it was,</v>
<v Speaker 2>it was John von Neumann,</v>

314
00:14:12.121 --> 00:14:12.954
<v Speaker 2>the,</v>
<v Speaker 2>um,</v>

315
00:14:13.400 --> 00:14:15.860
<v Speaker 2>the,</v>
<v Speaker 2>the inventor of game theory who a</v>

316
00:14:16.200 --> 00:14:18.400
<v Speaker 2>mathematician who,</v>
<v Speaker 2>um,</v>

317
00:14:19.080 --> 00:14:22.050
<v Speaker 2>uh,</v>
<v Speaker 2>is one along with Alan Turing and a</v>

318
00:14:22.051 --> 00:14:25.410
<v Speaker 2>couple of other people's was really</v>
<v Speaker 2>responsible for the computer revolution.</v>

319
00:14:25.770 --> 00:14:27.450
<v Speaker 2>He was the first person to use this</v>
<v Speaker 2>term,</v>

320
00:14:27.510 --> 00:14:28.890
<v Speaker 2>singularity,</v>
<v Speaker 2>uh,</v>

321
00:14:28.990 --> 00:14:30.450
<v Speaker 2>to describe just this,</v>
<v Speaker 2>that,</v>

322
00:14:30.451 --> 00:14:34.860
<v Speaker 2>that there's a speeding up of,</v>
<v Speaker 2>um,</v>

323
00:14:36.090 --> 00:14:41.090
<v Speaker 2>information processing technology</v>
<v Speaker 2>and April a cultural reliance upon it,</v>

324
00:14:42.410 --> 00:14:45.120
<v Speaker 2>uh,</v>
<v Speaker 2>beyond which we can't actually foresee</v>

325
00:14:45.121 --> 00:14:48.020
<v Speaker 2>the level of change that can come over</v>
<v Speaker 2>our society.</v>

326
00:14:48.020 --> 00:14:48.853
<v Speaker 2>It's like,</v>
<v Speaker 2>you know,</v>

327
00:14:48.900 --> 00:14:50.970
<v Speaker 2>an event horizon pass,</v>
<v Speaker 2>which we can't.</v>

328
00:14:51.740 --> 00:14:54.170
<v Speaker 2>Um,</v>
<v Speaker 2>and uh,</v>

329
00:14:54.410 --> 00:14:58.610
<v Speaker 2>this certainly becomes true when you</v>
<v Speaker 2>talk about these intelligence systems</v>

330
00:14:58.640 --> 00:15:01.100
<v Speaker 2>being able to make changes to</v>
<v Speaker 2>themselves.</v>

331
00:15:01.400 --> 00:15:04.010
<v Speaker 2>And again,</v>
<v Speaker 2>we're talking mostly software is not,</v>

332
00:15:04.011 --> 00:15:06.240
<v Speaker 2>I'm not imagining,</v>
<v Speaker 2>um,</v>

333
00:15:06.770 --> 00:15:08.390
<v Speaker 2>I mean the,</v>
<v Speaker 2>the most important breakthroughs</v>

334
00:15:08.740 --> 00:15:11.450
<v Speaker 2>certainly at the level of,</v>
<v Speaker 2>of better software.</v>

335
00:15:11.451 --> 00:15:12.740
<v Speaker 2>I mean the,</v>
<v Speaker 2>is we have,</v>

336
00:15:13.280 --> 00:15:18.140
<v Speaker 2>in terms of the computing power that if</v>
<v Speaker 2>the physical hardware on earth,</v>

337
00:15:18.620 --> 00:15:21.440
<v Speaker 2>it's not,</v>
<v Speaker 2>that's not what's limiting our ai at the</v>

338
00:15:21.441 --> 00:15:22.790
<v Speaker 2>moment.</v>
<v Speaker 2>It's not like we need more,</v>

339
00:15:23.570 --> 00:15:24.403
<v Speaker 2>more,</v>
<v Speaker 2>uh,</v>

340
00:15:25.160 --> 00:15:26.780
<v Speaker 2>hardware.</v>
<v Speaker 2>Um,</v>

341
00:15:26.900 --> 00:15:30.810
<v Speaker 2>but we will get more hardware to up to</v>
<v Speaker 2>the limits of physics and it'll,</v>

342
00:15:30.811 --> 00:15:33.650
<v Speaker 2>it get smaller and smaller as it has a,</v>
<v Speaker 2>and you know,</v>

343
00:15:33.651 --> 00:15:37.160
<v Speaker 2>if quantum computing becomes possible,</v>
<v Speaker 2>um,</v>

344
00:15:37.640 --> 00:15:39.410
<v Speaker 2>or practical,</v>
<v Speaker 2>um,</v>

345
00:15:39.470 --> 00:15:40.880
<v Speaker 2>that will,</v>
<v Speaker 2>uh,</v>

346
00:15:41.630 --> 00:15:43.310
<v Speaker 2>actually David Deutsch is,</v>
<v Speaker 2>is,</v>

347
00:15:43.311 --> 00:15:45.380
<v Speaker 2>um,</v>
<v Speaker 2>the physicist I mentioned is one of the</v>

348
00:15:45.381 --> 00:15:48.440
<v Speaker 2>fathers of the concept of quantum</v>
<v Speaker 2>computing.</v>

349
00:15:48.940 --> 00:15:53.580
<v Speaker 2>Um,</v>
<v Speaker 2>that will open up a whole nother area,</v>

350
00:15:53.630 --> 00:15:56.330
<v Speaker 2>you know,</v>
<v Speaker 2>extreme of computing power.</v>

351
00:15:56.810 --> 00:15:59.750
<v Speaker 2>That is,</v>
<v Speaker 2>I'm not at all analogous to the kinds</v>

352
00:15:59.751 --> 00:16:00.430
<v Speaker 2>of,</v>
<v Speaker 2>of,</v>

353
00:16:00.430 --> 00:16:02.240
<v Speaker 2>uh,</v>
<v Speaker 2>machines we have now.</v>

354
00:16:02.690 --> 00:16:04.470
<v Speaker 2>But,</v>
<v Speaker 2>um,</v>

355
00:16:05.810 --> 00:16:10.810
<v Speaker 2>it's just when you imagine</v>
<v Speaker 2>people don't,</v>

356
00:16:11.360 --> 00:16:15.230
<v Speaker 2>people seem to always want to,</v>
<v Speaker 2>I just had this conversation with Neil</v>

357
00:16:15.231 --> 00:16:18.830
<v Speaker 2>degrasse Tyson on my podcast.</v>
<v Speaker 2>He named dropper.</v>

358
00:16:19.070 --> 00:16:19.700
<v Speaker 2>Yeah.</v>
<v Speaker 2>I know.</v>

359
00:16:19.700 --> 00:16:21.440
<v Speaker 2>It was just,</v>
<v Speaker 2>I'm just keeping pookie people.</v>

360
00:16:21.441 --> 00:16:23.560
<v Speaker 2>I'm just,</v>
<v Speaker 2>I'm just attributing these ideas to him.</v>

361
00:16:24.920 --> 00:16:26.240
<v Speaker 2>Uh,</v>
<v Speaker 2>he's not a,</v>

362
00:16:26.241 --> 00:16:27.680
<v Speaker 2>he doesn't take this line at all.</v>
<v Speaker 2>He's not at all,</v>

363
00:16:27.681 --> 00:16:28.910
<v Speaker 2>he thinks it's all bullshit.</v>
<v Speaker 2>Right.</v>

364
00:16:29.120 --> 00:16:31.460
<v Speaker 2>He's not at all worried about Ai.</v>
<v Speaker 2>Right.</v>

365
00:16:31.461 --> 00:16:33.020
<v Speaker 2>What does he think?</v>
<v Speaker 2>He thinks that,</v>

366
00:16:33.220 --> 00:16:34.010
<v Speaker 2>you know,</v>
<v Speaker 2>we just,</v>

367
00:16:34.010 --> 00:16:38.630
<v Speaker 2>we just use,</v>
<v Speaker 2>he's drawing an analogy from how we,</v>

368
00:16:39.100 --> 00:16:41.660
<v Speaker 2>you currently use computers that they</v>
<v Speaker 2>just,</v>

369
00:16:41.661 --> 00:16:44.510
<v Speaker 2>they just keep helping us do what we</v>
<v Speaker 2>want to do.</v>

370
00:16:44.510 --> 00:16:48.260
<v Speaker 2>Like we decide what we want to do with</v>
<v Speaker 2>computers and we just add them to our</v>

371
00:16:48.261 --> 00:16:52.600
<v Speaker 2>process and that process becomes</v>
<v Speaker 2>automated and then we'll find new jobs</v>

372
00:16:52.601 --> 00:16:53.540
<v Speaker 2>somewhere else.</v>
<v Speaker 2>Like you didn't,</v>

373
00:16:53.541 --> 00:16:57.410
<v Speaker 2>you don't need a stenographer once you</v>
<v Speaker 2>have voice recognition technology and</v>

374
00:16:57.910 --> 00:16:59.230
<v Speaker 2>um,</v>
<v Speaker 2>uh,</v>

375
00:16:59.240 --> 00:17:01.250
<v Speaker 2>that's not a problem.</v>
<v Speaker 2>A stenographer we'll find something else</v>

376
00:17:01.251 --> 00:17:03.320
<v Speaker 2>to do.</v>
<v Speaker 2>And so the economic dislocations isn't</v>

377
00:17:03.321 --> 00:17:08.321
<v Speaker 2>that bad and um,</v>
<v Speaker 2>computers will just get better than they</v>

378
00:17:08.391 --> 00:17:11.450
<v Speaker 2>are and you know,</v>
<v Speaker 2>eventually Siri will actually work,</v>

379
00:17:11.451 --> 00:17:12.284
<v Speaker 2>you know,</v>
<v Speaker 2>and you'll,</v>

380
00:17:12.750 --> 00:17:14.630
<v Speaker 2>she'll answer your questions.</v>
<v Speaker 2>Well and you're not,</v>

381
00:17:14.660 --> 00:17:17.690
<v Speaker 2>it's not going to be a laugh line what</v>
<v Speaker 2>Siri said to you today.</v>

382
00:17:18.080 --> 00:17:19.220
<v Speaker 2>And,</v>
<v Speaker 2>um,</v>

383
00:17:19.820 --> 00:17:24.320
<v Speaker 2>then all of this,</v>
<v Speaker 2>we'll just proceed to make life better</v>

384
00:17:24.650 --> 00:17:26.280
<v Speaker 2>right now.</v>
<v Speaker 2>Um,</v>

385
00:17:26.990 --> 00:17:30.480
<v Speaker 2>none of that is imagining what it will</v>
<v Speaker 2>be like to make it,</v>

386
00:17:30.490 --> 00:17:34.670
<v Speaker 2>because it would be a certain point</v>
<v Speaker 2>where you'll have systems that are,</v>

387
00:17:35.330 --> 00:17:36.163
<v Speaker 2>you know,</v>
<v Speaker 2>it's like</v>

388
00:17:37.850 --> 00:17:40.220
<v Speaker 2>the Cha,</v>
<v Speaker 2>the best chess player on earth is now</v>

389
00:17:40.340 --> 00:17:42.030
<v Speaker 2>always going to be a computer,</v>
<v Speaker 2>right?</v>

390
00:17:42.070 --> 00:17:42.980
<v Speaker 2>It's never,</v>
<v Speaker 2>there's no,</v>

391
00:17:43.010 --> 00:17:46.430
<v Speaker 2>there's not going to be a human born</v>
<v Speaker 2>tomorrow that's going to be better than</v>

392
00:17:46.431 --> 00:17:47.550
<v Speaker 2>the best computer.</v>
<v Speaker 2>I mean,</v>

393
00:17:47.551 --> 00:17:51.150
<v Speaker 2>it's like,</v>
<v Speaker 2>it's already like we have superhuman</v>

394
00:17:51.210 --> 00:17:55.410
<v Speaker 2>chess players on earth.</v>
<v Speaker 2>Now imagine having computers that are</v>

395
00:17:55.411 --> 00:17:58.060
<v Speaker 2>superhuman at every,</v>
<v Speaker 2>uh,</v>

396
00:17:58.200 --> 00:18:01.230
<v Speaker 2>every task that is relevant,</v>
<v Speaker 2>every intellectual task,</v>

397
00:18:01.231 --> 00:18:03.900
<v Speaker 2>right?</v>
<v Speaker 2>So the best physicist is a computer.</v>

398
00:18:04.050 --> 00:18:06.840
<v Speaker 2>You know,</v>
<v Speaker 2>the best medical diagnostician is a</v>

399
00:18:06.841 --> 00:18:07.800
<v Speaker 2>computer.</v>
<v Speaker 2>The best,</v>

400
00:18:08.650 --> 00:18:11.460
<v Speaker 2>um,</v>
<v Speaker 2>prover of math theorems has a computer.</v>

401
00:18:11.461 --> 00:18:13.290
<v Speaker 2>The best engineer is a computer,</v>
<v Speaker 2>right?</v>

402
00:18:13.291 --> 00:18:15.930
<v Speaker 2>Then there's no,</v>
<v Speaker 2>there's no reason why we're not headed</v>

403
00:18:15.931 --> 00:18:16.301
<v Speaker 2>there.</v>
<v Speaker 2>I mean,</v>

404
00:18:16.301 --> 00:18:19.650
<v Speaker 2>it would be the only reason I could see</v>
<v Speaker 2>we're not headed there is it's something</v>

405
00:18:19.651 --> 00:18:24.651
<v Speaker 2>massively dislocating happens that</v>
<v Speaker 2>prevents us from continuing to improve</v>

406
00:18:25.471 --> 00:18:27.800
<v Speaker 2>our intelligent machines.</v>
<v Speaker 2>But if you just,</v>

407
00:18:27.870 --> 00:18:31.260
<v Speaker 2>the moment you admit that intelligence</v>
<v Speaker 2>is just a matter of information</v>

408
00:18:31.261 --> 00:18:36.261
<v Speaker 2>processing and you admit that we will</v>
<v Speaker 2>continue to improve our machines unless</v>

409
00:18:36.721 --> 00:18:40.350
<v Speaker 2>something heinous happens because this</v>
<v Speaker 2>is intelligence and automation are the</v>

410
00:18:40.351 --> 00:18:42.480
<v Speaker 2>most valuable things we have.</v>

411
00:18:43.020 --> 00:18:45.180
<v Speaker 2>Um,</v>
<v Speaker 2>at a certain point,</v>

412
00:18:45.181 --> 00:18:47.790
<v Speaker 2>whether you think it's in five years or</v>
<v Speaker 2>500 years,</v>

413
00:18:48.330 --> 00:18:52.770
<v Speaker 2>we're going to find ourselves in the</v>
<v Speaker 2>presence of super intelligent machines.</v>

414
00:18:52.771 --> 00:18:57.771
<v Speaker 2>And then at that point,</v>
<v Speaker 2>the best source of innovation for the</v>

415
00:18:58.951 --> 00:19:03.951
<v Speaker 2>next generation of software or hardware</v>
<v Speaker 2>or both will be the machines themselves.</v>

416
00:19:04.860 --> 00:19:05.693
<v Speaker 2>Right?</v>
<v Speaker 2>So then,</v>

417
00:19:06.090 --> 00:19:08.400
<v Speaker 2>so then you just have,</v>
<v Speaker 2>then that's where you get what,</v>

418
00:19:08.670 --> 00:19:12.360
<v Speaker 2>what was,</v>
<v Speaker 2>what the mathematician Ij good described</v>

419
00:19:12.361 --> 00:19:16.080
<v Speaker 2>as as the intelligence explosion,</v>
<v Speaker 2>which is just the process can take off</v>

420
00:19:16.081 --> 00:19:17.690
<v Speaker 2>on its own.</v>
<v Speaker 2>Um,</v>

421
00:19:17.730 --> 00:19:20.010
<v Speaker 2>and this is where the singularity</v>
<v Speaker 2>people,</v>

422
00:19:20.770 --> 00:19:24.420
<v Speaker 2>um,</v>
<v Speaker 2>either either are hopeful or worried.</v>

423
00:19:24.830 --> 00:19:26.430
<v Speaker 2>Um,</v>
<v Speaker 2>but you know,</v>

424
00:19:26.431 --> 00:19:29.070
<v Speaker 2>cause there's nothing,</v>
<v Speaker 2>there's no guarantee that this process</v>

425
00:19:29.610 --> 00:19:33.270
<v Speaker 2>will be,</v>
<v Speaker 2>remain aligned with our interests.</v>

426
00:19:33.300 --> 00:19:35.530
<v Speaker 2>And,</v>
<v Speaker 2>and every person who I meet,</v>

427
00:19:35.580 --> 00:19:36.001
<v Speaker 2>even,</v>
<v Speaker 2>you know,</v>

428
00:19:36.001 --> 00:19:38.390
<v Speaker 2>very smart people like Neil,</v>
<v Speaker 2>um,</v>

429
00:19:39.060 --> 00:19:43.650
<v Speaker 2>who says they're not worried about this.</v>
<v Speaker 2>When you actually drill down on why</v>

430
00:19:43.651 --> 00:19:46.740
<v Speaker 2>they're not worried,</v>
<v Speaker 2>you find that they're actually not</v>

431
00:19:46.741 --> 00:19:51.741
<v Speaker 2>imagining machines making changes to</v>
<v Speaker 2>their own source code.</v>

432
00:19:52.840 --> 00:19:57.500
<v Speaker 2>Um,</v>
<v Speaker 2>and they're not an or they're,</v>

433
00:19:57.501 --> 00:20:01.470
<v Speaker 2>they,</v>
<v Speaker 2>they simply believe that this is so far</v>

434
00:20:01.471 --> 00:20:04.200
<v Speaker 2>away that we don't have to worry about</v>
<v Speaker 2>it now.</v>

435
00:20:04.590 --> 00:20:06.690
<v Speaker 2>Right.</v>
<v Speaker 2>And that's actually a non sequitur.</v>

436
00:20:06.691 --> 00:20:09.690
<v Speaker 2>I mean,</v>
<v Speaker 2>to say that this is far away is not</v>

437
00:20:09.691 --> 00:20:13.400
<v Speaker 2>actually grappling with,</v>
<v Speaker 2>it's not an argument this isn't going to</v>

438
00:20:13.410 --> 00:20:14.370
<v Speaker 2>happen.</v>
<v Speaker 2>And,</v>

439
00:20:15.000 --> 00:20:17.010
<v Speaker 2>um,</v>
<v Speaker 2>and it's based on what to,</v>

440
00:20:17.430 --> 00:20:18.570
<v Speaker 2>and it's,</v>
<v Speaker 2>and it's based on,</v>

441
00:20:18.960 --> 00:20:20.400
<v Speaker 2>first of all,</v>
<v Speaker 2>there's no,</v>

442
00:20:21.060 --> 00:20:24.830
<v Speaker 2>there's no reason to believe Jamie want</v>
<v Speaker 2>to find out where it is.</v>

443
00:20:26.340 --> 00:20:29.070
<v Speaker 2>Um,</v>
<v Speaker 2>there's no,</v>

444
00:20:29.190 --> 00:20:31.950
<v Speaker 2>I mean we don't know how long it will</v>
<v Speaker 2>take us to prepare for this.</v>

445
00:20:32.010 --> 00:20:32.670
<v Speaker 2>Right?</v>
<v Speaker 2>So like,</v>

446
00:20:32.670 --> 00:20:33.570
<v Speaker 2>like if,</v>
<v Speaker 2>if you were,</v>

447
00:20:33.571 --> 00:20:38.130
<v Speaker 2>if you knew this,</v>
<v Speaker 2>it was going to take 50 years for this</v>

448
00:20:38.131 --> 00:20:39.150
<v Speaker 2>to happen,</v>
<v Speaker 2>right.</v>

449
00:20:40.050 --> 00:20:44.430
<v Speaker 2>Is 50 years enough for us to prepare</v>
<v Speaker 2>politically and economically to deal</v>

450
00:20:44.431 --> 00:20:49.000
<v Speaker 2>with the ramifications of this and,</v>
<v Speaker 2>and to do it and to add to say nothing</v>

451
00:20:49.001 --> 00:20:53.200
<v Speaker 2>of actually building the ai safely in a</v>
<v Speaker 2>way that's aligned with our interests.</v>

452
00:20:53.500 --> 00:20:56.440
<v Speaker 2>I don't know.</v>
<v Speaker 2>I mean 50 so 50 years is,</v>

453
00:20:56.610 --> 00:20:59.710
<v Speaker 2>it's like we've had the iphone for what,</v>
<v Speaker 2>10 years?</v>

454
00:21:00.580 --> 00:21:01.500
<v Speaker 2>Nine years?</v>
<v Speaker 2>I'm it,</v>

455
00:21:01.620 --> 00:21:03.880
<v Speaker 2>it's like 50 years,</v>
<v Speaker 2>not a lot of time,</v>

456
00:21:04.230 --> 00:21:04.661
<v Speaker 2>right.</v>
<v Speaker 2>To,</v>

457
00:21:04.661 --> 00:21:05.890
<v Speaker 2>to deal,</v>
<v Speaker 2>to deal with this.</v>

458
00:21:05.920 --> 00:21:10.920
<v Speaker 2>And um,</v>
<v Speaker 2>this has no reason to think it's,</v>

459
00:21:11.230 --> 00:21:14.890
<v Speaker 2>it's that far away.</v>
<v Speaker 2>If we keep making progress,</v>

460
00:21:14.891 --> 00:21:15.850
<v Speaker 2>I mean it's,</v>
<v Speaker 2>it's not,</v>

461
00:21:15.940 --> 00:21:17.890
<v Speaker 2>it will be amazing if it were 500 years</v>
<v Speaker 2>away.</v>

462
00:21:18.250 --> 00:21:18.641
<v Speaker 2>I mean,</v>
<v Speaker 2>that,</v>

463
00:21:18.641 --> 00:21:19.970
<v Speaker 2>that seems like it's,</v>
<v Speaker 2>it's,</v>

464
00:21:20.590 --> 00:21:25.590
<v Speaker 2>it's more likely from what I am in the</v>
<v Speaker 2>sense I get from the people who are</v>

465
00:21:26.200 --> 00:21:31.200
<v Speaker 2>doing this work,</v>
<v Speaker 2>it's far more likely to be 50 years than</v>

466
00:21:31.871 --> 00:21:33.400
<v Speaker 2>500 years.</v>
<v Speaker 2>Like,</v>

467
00:21:33.940 --> 00:21:34.990
<v Speaker 2>you know,</v>
<v Speaker 2>um,</v>

468
00:21:36.100 --> 00:21:39.640
<v Speaker 2>uh,</v>
<v Speaker 2>I mean the p at the people who think</v>

469
00:21:39.641 --> 00:21:41.780
<v Speaker 2>this is a long,</v>
<v Speaker 2>long way off or,</v>

470
00:21:41.781 --> 00:21:45.610
<v Speaker 2>I mean they're saying,</v>
<v Speaker 2>you know,</v>

471
00:21:45.700 --> 00:21:49.540
<v Speaker 2>50 to 100 years,</v>
<v Speaker 2>no one says 500 years.</v>

472
00:21:50.200 --> 00:21:51.000
<v Speaker 2>No,</v>
<v Speaker 2>no.</v>

473
00:21:51.000 --> 00:21:53.050
<v Speaker 2>As far as I know,</v>
<v Speaker 2>no one who was actually close to this</v>

474
00:21:53.051 --> 00:21:57.100
<v Speaker 2>work and some people think it could be</v>
<v Speaker 2>in five years.</v>

475
00:21:57.190 --> 00:21:58.690
<v Speaker 2>Right.</v>
<v Speaker 2>I mean the people who are,</v>

476
00:21:58.710 --> 00:22:01.090
<v Speaker 2>you know,</v>
<v Speaker 2>like the deep mind people who are very</v>

477
00:22:01.091 --> 00:22:03.340
<v Speaker 2>close to this or are the sorts of people</v>
<v Speaker 2>who say,</v>

478
00:22:03.730 --> 00:22:06.370
<v Speaker 2>cause the people,</v>
<v Speaker 2>the people who are close to his work are</v>

479
00:22:06.371 --> 00:22:09.550
<v Speaker 2>astonished by what's happened in the</v>
<v Speaker 2>last 10 years.</v>

480
00:22:10.030 --> 00:22:13.350
<v Speaker 2>We went from a place of,</v>
<v Speaker 2>you know,</v>

481
00:22:13.360 --> 00:22:15.550
<v Speaker 2>very little progress too,</v>
<v Speaker 2>you know,</v>

482
00:22:15.700 --> 00:22:19.240
<v Speaker 2>wow,</v>
<v Speaker 2>this is all of a sudden really,</v>

483
00:22:19.241 --> 00:22:22.210
<v Speaker 2>really interesting and powerful.</v>
<v Speaker 2>And um,</v>

484
00:22:23.880 --> 00:22:26.230
<v Speaker 2>and again,</v>
<v Speaker 2>progress is compounding in a way that's</v>

485
00:22:26.231 --> 00:22:30.250
<v Speaker 2>counter intuitive.</v>
<v Speaker 2>People systematically overestimate how</v>

486
00:22:30.251 --> 00:22:34.030
<v Speaker 2>much change can happen in a year and</v>
<v Speaker 2>underestimate how much change can happen</v>

487
00:22:34.031 --> 00:22:35.150
<v Speaker 2>in 10 years.</v>
<v Speaker 2>And I,</v>

488
00:22:35.170 --> 00:22:37.300
<v Speaker 2>you know,</v>
<v Speaker 2>as far as estimating how much change can</v>

489
00:22:37.301 --> 00:22:40.780
<v Speaker 2>happen in 50 or a hundred years,</v>
<v Speaker 2>I don't know that anyone is good at</v>

490
00:22:40.781 --> 00:22:41.614
<v Speaker 2>that.</v>

491
00:22:43.540 --> 00:22:48.540
<v Speaker 3>How could you be with giant leaps come</v>
<v Speaker 3>giant exponential leaps off those leaps.</v>

492
00:22:48.851 --> 00:22:51.670
<v Speaker 3>And it's,</v>
<v Speaker 3>it's almost impossible for us to really</v>

493
00:22:51.671 --> 00:22:54.310
<v Speaker 3>predict what we're going to be looking</v>
<v Speaker 3>at 50 years from now.</v>

494
00:22:54.700 --> 00:22:57.880
<v Speaker 3>But I don't,</v>
<v Speaker 3>I don't know what they're going to think</v>

495
00:22:57.881 --> 00:23:00.910
<v Speaker 3>about us.</v>
<v Speaker 3>That's what's most bizarre about it as</v>

496
00:23:00.911 --> 00:23:03.310
<v Speaker 3>well.</v>
<v Speaker 3>We really might be obsolete if we look</v>

497
00:23:03.311 --> 00:23:07.270
<v Speaker 3>at how ridiculous we are.</v>
<v Speaker 3>Look at this political campaign,</v>

498
00:23:07.271 --> 00:23:09.160
<v Speaker 3>look at what we pay attention to in the</v>
<v Speaker 3>news.</v>

499
00:23:09.161 --> 00:23:12.190
<v Speaker 3>Look at the things we really focus on</v>
<v Speaker 3>where our strange,</v>

500
00:23:12.191 --> 00:23:16.300
<v Speaker 3>ridiculous animal and w y if we look</v>
<v Speaker 3>back on,</v>

501
00:23:16.301 --> 00:23:18.550
<v Speaker 3>you know,</v>
<v Speaker 3>some strange dinosaur that at a weird</v>

502
00:23:18.551 --> 00:23:20.560
<v Speaker 3>neck,</v>
<v Speaker 3>why should that fucking thing make it,</v>

503
00:23:20.970 --> 00:23:22.240
<v Speaker 3>you know,</v>
<v Speaker 3>why should we make it?</v>

504
00:23:22.380 --> 00:23:24.940
<v Speaker 3>We,</v>
<v Speaker 3>we might be here to make that thing.</v>

505
00:23:25.360 --> 00:23:28.690
<v Speaker 3>And that thing takes over from here with</v>
<v Speaker 3>no emotions,</v>

506
00:23:28.691 --> 00:23:30.040
<v Speaker 3>no lust,</v>
<v Speaker 3>no greed,</v>

507
00:23:30.041 --> 00:23:34.450
<v Speaker 3>and just purely existing electronically</v>
<v Speaker 3>and for what reason?</v>

508
00:23:34.650 --> 00:23:34.991
<v Speaker 2>Well,</v>
<v Speaker 2>that,</v>

509
00:23:34.991 --> 00:23:38.940
<v Speaker 2>that's a little scary there.</v>
<v Speaker 2>There are computer scientists who when</v>

510
00:23:38.941 --> 00:23:42.630
<v Speaker 2>you talk about why they're not worried</v>
<v Speaker 2>or talk to them about why they're not</v>

511
00:23:42.631 --> 00:23:47.631
<v Speaker 2>worried,</v>
<v Speaker 2>they just swallow this pill without any</v>

512
00:23:48.141 --> 00:23:52.520
<v Speaker 2>qualm that we're going to make.</v>
<v Speaker 2>The thing that is far more powerful and</v>

513
00:23:52.521 --> 00:23:57.500
<v Speaker 2>beautiful and important than we are.</v>
<v Speaker 2>And it doesn't matter what happens to</v>

514
00:23:57.501 --> 00:23:58.220
<v Speaker 2>us.</v>
<v Speaker 2>I mean,</v>

515
00:23:58.220 --> 00:24:02.270
<v Speaker 2>that was our role.</v>
<v Speaker 2>Our role was to build these mechanical</v>

516
00:24:02.271 --> 00:24:07.130
<v Speaker 2>gods and,</v>
<v Speaker 2>and it's fine if they squash us.</v>

517
00:24:07.550 --> 00:24:10.550
<v Speaker 2>Um,</v>
<v Speaker 2>and I've literally heard a,</v>

518
00:24:10.610 --> 00:24:11.630
<v Speaker 2>a,</v>
<v Speaker 2>people say,</v>

519
00:24:11.631 --> 00:24:13.320
<v Speaker 2>I've heard someone give a talk.</v>
<v Speaker 2>I mean,</v>

520
00:24:13.520 --> 00:24:15.780
<v Speaker 2>that's what woke me up to,</v>
<v Speaker 2>oh,</v>

521
00:24:16.670 --> 00:24:20.600
<v Speaker 2>how interesting in this area is.</v>
<v Speaker 2>I went to this conference in San Juan</v>

522
00:24:20.601 --> 00:24:22.250
<v Speaker 2>about a year ago.</v>
<v Speaker 2>Um,</v>

523
00:24:22.940 --> 00:24:24.260
<v Speaker 2>and there were,</v>
<v Speaker 2>uh,</v>

524
00:24:24.760 --> 00:24:26.510
<v Speaker 2>you know,</v>
<v Speaker 2>like the people from deep mind were</v>

525
00:24:26.520 --> 00:24:29.930
<v Speaker 2>there and there were the people who were</v>
<v Speaker 2>very close to this work were there.</v>

526
00:24:30.380 --> 00:24:31.890
<v Speaker 2>And,</v>
<v Speaker 2>um,</v>

527
00:24:32.260 --> 00:24:37.260
<v Speaker 2>I mean to hear some of the reasons why</v>
<v Speaker 2>you shouldn't be worried from people who</v>

528
00:24:38.330 --> 00:24:40.970
<v Speaker 2>were interested in,</v>
<v Speaker 2>in calming the fear is so they could get</v>

529
00:24:40.971 --> 00:24:43.330
<v Speaker 2>on with doing their very important work.</v>
<v Speaker 2>Um,</v>

530
00:24:43.820 --> 00:24:48.820
<v Speaker 2>it was amazing because they were highly</v>
<v Speaker 2>uncompelling reasons not to be worried.</v>

531
00:24:51.180 --> 00:24:53.300
<v Speaker 2>It's just,</v>
<v Speaker 2>um,</v>

532
00:24:54.470 --> 00:24:56.060
<v Speaker 2>so,</v>
<v Speaker 2>so they had a,</v>

533
00:24:56.420 --> 00:24:58.670
<v Speaker 2>they had a desire to be compelled.</v>
<v Speaker 2>They're not,</v>

534
00:24:58.700 --> 00:25:01.940
<v Speaker 2>they're not well,</v>
<v Speaker 2>not well known that people,</v>

535
00:25:01.970 --> 00:25:05.990
<v Speaker 2>people want to do this.</v>
<v Speaker 2>There's a deep assumption in many of</v>

536
00:25:05.991 --> 00:25:09.920
<v Speaker 2>these people that we can figure it out</v>
<v Speaker 2>as we go along.</v>

537
00:25:10.130 --> 00:25:10.970
<v Speaker 2>Right?</v>
<v Speaker 2>It's like,</v>

538
00:25:11.380 --> 00:25:11.991
<v Speaker 2>you know,</v>
<v Speaker 2>it's just like,</v>

539
00:25:11.991 --> 00:25:13.670
<v Speaker 2>we're going to,</v>
<v Speaker 2>we're just going to get,</v>

540
00:25:13.920 --> 00:25:14.900
<v Speaker 2>going to get closer.</v>
<v Speaker 2>We're foot.</v>

541
00:25:14.901 --> 00:25:17.540
<v Speaker 2>We're far enough away now.</v>
<v Speaker 2>Even five year,</v>

542
00:25:17.541 --> 00:25:18.890
<v Speaker 2>even if it's five years,</v>
<v Speaker 2>five years,</v>

543
00:25:18.920 --> 00:25:19.310
<v Speaker 2>we'll,</v>
<v Speaker 2>we'll,</v>

544
00:25:19.310 --> 00:25:20.570
<v Speaker 2>we'll get there.</v>
<v Speaker 2>Once we get closer,</v>

545
00:25:20.571 --> 00:25:25.340
<v Speaker 2>once we get something a little scary,</v>
<v Speaker 2>then we'll pull the brakes and talk</v>

546
00:25:25.341 --> 00:25:26.174
<v Speaker 2>about it.</v>

547
00:25:26.240 --> 00:25:30.200
<v Speaker 2>But the problem is they are a sent,</v>
<v Speaker 2>everyone is essentially in a race</v>

548
00:25:30.201 --> 00:25:31.790
<v Speaker 2>condition by default.</v>
<v Speaker 2>And you have,</v>

549
00:25:31.820 --> 00:25:34.250
<v Speaker 2>you know,</v>
<v Speaker 2>Google is racing against facebook and</v>

550
00:25:34.251 --> 00:25:37.100
<v Speaker 2>the u s is racing against China and</v>
<v Speaker 2>every,</v>

551
00:25:37.640 --> 00:25:39.740
<v Speaker 2>every group is racing against every</v>
<v Speaker 2>other group.</v>

552
00:25:40.250 --> 00:25:43.010
<v Speaker 2>Um,</v>
<v Speaker 2>however you want to conceive of groups.</v>

553
00:25:43.040 --> 00:25:44.150
<v Speaker 2>This,</v>
<v Speaker 2>this is a,</v>

554
00:25:44.480 --> 00:25:48.530
<v Speaker 2>to be the first one to be the first one</v>
<v Speaker 2>with</v>

555
00:25:50.690 --> 00:25:55.040
<v Speaker 2>incredibly powerful,</v>
<v Speaker 2>narrow ai is to be the next,</v>

556
00:25:55.220 --> 00:25:58.250
<v Speaker 2>you know,</v>
<v Speaker 2>multibillion dollar company,</v>

557
00:25:58.251 --> 00:26:01.070
<v Speaker 2>right?</v>
<v Speaker 2>So everyone's trying to get there and</v>

558
00:26:01.540 --> 00:26:03.230
<v Speaker 2>uh,</v>
<v Speaker 2>if they suddenly get there and sort of</v>

559
00:26:03.231 --> 00:26:05.390
<v Speaker 2>overshoot a little bit and now they've</v>
<v Speaker 2>got something like,</v>

560
00:26:05.391 --> 00:26:08.000
<v Speaker 2>you know,</v>
<v Speaker 2>general intelligence or something close,</v>

561
00:26:08.540 --> 00:26:10.660
<v Speaker 2>um,</v>
<v Speaker 2>what we're relying on every,</v>

562
00:26:10.800 --> 00:26:13.340
<v Speaker 2>and they know everyone else's attempting</v>
<v Speaker 2>to do this,</v>

563
00:26:13.370 --> 00:26:14.690
<v Speaker 2>right?</v>
<v Speaker 2>Um,</v>

564
00:26:15.430 --> 00:26:20.330
<v Speaker 2>W we don't have a system set up where</v>
<v Speaker 2>everyone can pull the breaks together</v>

565
00:26:20.331 --> 00:26:20.960
<v Speaker 2>and say,</v>
<v Speaker 2>listen,</v>

566
00:26:20.960 --> 00:26:22.700
<v Speaker 2>we've got to stop racing here.</v>

567
00:26:22.700 --> 00:26:25.490
<v Speaker 2>We have to share everything.</v>
<v Speaker 2>We have to share the wealth,</v>

568
00:26:25.491 --> 00:26:28.100
<v Speaker 2>we have to share the information,</v>
<v Speaker 2>we have to um,</v>

569
00:26:28.730 --> 00:26:31.790
<v Speaker 2>this truly has to be open source in</v>
<v Speaker 2>every conceivable way.</v>

570
00:26:31.791 --> 00:26:35.480
<v Speaker 2>And um,</v>
<v Speaker 2>we have to diffuse this winner take all</v>

571
00:26:36.320 --> 00:26:38.150
<v Speaker 2>dynamic.</v>
<v Speaker 2>Um,</v>

572
00:26:39.080 --> 00:26:40.460
<v Speaker 2>you know,</v>
<v Speaker 2>I think we need something like a</v>

573
00:26:40.461 --> 00:26:43.230
<v Speaker 2>Manhattan project to figure out how to</v>
<v Speaker 2>do that.</v>

574
00:26:43.320 --> 00:26:45.060
<v Speaker 2>You know,</v>
<v Speaker 2>not if not to figure out how to build</v>

575
00:26:45.061 --> 00:26:48.030
<v Speaker 2>the AI,</v>
<v Speaker 2>but to figure out how to build it in a</v>

576
00:26:48.031 --> 00:26:51.450
<v Speaker 2>way that does not create an arms race</v>
<v Speaker 2>that does not create,</v>

577
00:26:51.980 --> 00:26:55.080
<v Speaker 2>um,</v>
<v Speaker 2>an incentive to build unsafe ai,</v>

578
00:26:55.081 --> 00:26:59.160
<v Speaker 2>which is almost certainly going to be</v>
<v Speaker 2>easier than building safe ai and just to</v>

579
00:26:59.161 --> 00:27:00.980
<v Speaker 2>work out all of these issues because</v>
<v Speaker 2>it's,</v>

580
00:27:00.981 --> 00:27:04.860
<v Speaker 2>it's not because what I think we are,</v>
<v Speaker 2>we're going to build this by default.</v>

581
00:27:04.861 --> 00:27:09.300
<v Speaker 2>We're just going to keep building more</v>
<v Speaker 2>and more intelligent machines and this</v>

582
00:27:09.301 --> 00:27:14.100
<v Speaker 2>is going to be done in by everyone who</v>
<v Speaker 2>can,</v>

583
00:27:14.280 --> 00:27:16.890
<v Speaker 2>can do it with each generation.</v>

584
00:27:16.890 --> 00:27:18.300
<v Speaker 2>If we were even talking about</v>
<v Speaker 2>generations,</v>

585
00:27:18.301 --> 00:27:21.930
<v Speaker 2>it's going to be,</v>
<v Speaker 2>it will have the tools made by the prior</v>

586
00:27:21.931 --> 00:27:26.070
<v Speaker 2>generation that are more powerful than</v>
<v Speaker 2>anyone imagined a hundred years ago.</v>

587
00:27:26.071 --> 00:27:28.350
<v Speaker 2>And it just,</v>
<v Speaker 2>it gets going to keep going like that.</v>

588
00:27:28.980 --> 00:27:33.780
<v Speaker 2>Did anybody actually make that quote</v>
<v Speaker 2>about giving birth to the mechanical</v>

589
00:27:33.781 --> 00:27:34.611
<v Speaker 2>gods?</v>
<v Speaker 2>I,</v>

590
00:27:34.611 --> 00:27:35.444
<v Speaker 2>no,</v>
<v Speaker 2>that was just made.</v>

591
00:27:36.260 --> 00:27:37.110
<v Speaker 2>Yeah,</v>
<v Speaker 2>but it was,</v>

592
00:27:37.111 --> 00:27:39.930
<v Speaker 2>there was a scientist that actually was</v>
<v Speaker 2>thinking and saying that,</v>

593
00:27:39.960 --> 00:27:42.990
<v Speaker 2>but that was,</v>
<v Speaker 2>that was the content of what he was</v>

594
00:27:43.020 --> 00:27:43.771
<v Speaker 2>saying.</v>
<v Speaker 2>He's like,</v>

595
00:27:43.771 --> 00:27:48.600
<v Speaker 2>what are we going to build the next</v>
<v Speaker 2>species that is far more important than</v>

596
00:27:48.601 --> 00:27:51.240
<v Speaker 2>we are?</v>
<v Speaker 2>And that's a good thing.</v>

597
00:27:51.290 --> 00:27:51.720
<v Speaker 2>And it,</v>
<v Speaker 2>well,</v>

598
00:27:51.720 --> 00:27:54.210
<v Speaker 2>and actually I can go there with him.</v>
<v Speaker 2>I mean,</v>

599
00:27:54.260 --> 00:27:55.620
<v Speaker 2>it actually,</v>
<v Speaker 2>the only,</v>

600
00:27:55.621 --> 00:28:00.621
<v Speaker 2>the only</v>
<v Speaker 2>caveat here is that unless they're not</v>

601
00:28:00.631 --> 00:28:01.530
<v Speaker 2>conscious,</v>
<v Speaker 2>right?</v>

602
00:28:01.531 --> 00:28:04.920
<v Speaker 2>Like if you,</v>
<v Speaker 2>the true horror for me is that we can</v>

603
00:28:04.921 --> 00:28:07.170
<v Speaker 2>build things more intelligent than we</v>
<v Speaker 2>are,</v>

604
00:28:07.620 --> 00:28:08.880
<v Speaker 2>more powerful than we are.</v>

605
00:28:09.640 --> 00:28:13.980
<v Speaker 2>Uh,</v>
<v Speaker 2>and that can squash us and they might</v>

606
00:28:13.981 --> 00:28:15.630
<v Speaker 2>not,</v>
<v Speaker 2>they might be unconscious,</v>

607
00:28:15.680 --> 00:28:17.460
<v Speaker 2>right?</v>
<v Speaker 2>There might be nothing like the universe</v>

608
00:28:17.461 --> 00:28:19.170
<v Speaker 2>could go dark if they squash us,</v>
<v Speaker 2>right?</v>

609
00:28:19.171 --> 00:28:21.060
<v Speaker 2>Or,</v>
<v Speaker 2>or at least our corner of the universe</v>

610
00:28:21.061 --> 00:28:22.110
<v Speaker 2>could go dark.</v>
<v Speaker 2>Um,</v>

611
00:28:22.500 --> 00:28:24.360
<v Speaker 2>and yet these things will be immensely</v>
<v Speaker 2>powerful.</v>

612
00:28:24.840 --> 00:28:27.150
<v Speaker 2>Um,</v>
<v Speaker 2>so if,</v>

613
00:28:27.300 --> 00:28:28.111
<v Speaker 2>and this is just,</v>
<v Speaker 2>you know,</v>

614
00:28:28.111 --> 00:28:30.210
<v Speaker 2>the jury's out on this,</v>
<v Speaker 2>but if there's nothing about</v>

615
00:28:30.211 --> 00:28:34.860
<v Speaker 2>intelligence scaling that demands that</v>
<v Speaker 2>consciousness come along for the ride,</v>

616
00:28:35.580 --> 00:28:38.160
<v Speaker 2>um,</v>
<v Speaker 2>then it's possible that,</v>

617
00:28:38.161 --> 00:28:39.690
<v Speaker 2>I mean,</v>
<v Speaker 2>nobody thinks our machines are,</v>

618
00:28:39.691 --> 00:28:41.640
<v Speaker 2>you know,</v>
<v Speaker 2>very few people would think are machines</v>

619
00:28:41.790 --> 00:28:43.860
<v Speaker 2>that are intelligent or conscious.</v>
<v Speaker 2>Right?</v>

620
00:28:44.310 --> 00:28:46.710
<v Speaker 2>So at what point does consciousness come</v>
<v Speaker 2>online?</v>

621
00:28:47.240 --> 00:28:49.720
<v Speaker 2>Um,</v>
<v Speaker 2>maybe it's possible to build super</v>

622
00:28:49.721 --> 00:28:51.870
<v Speaker 2>intelligence that's unconscious,</v>
<v Speaker 2>you know,</v>

623
00:28:51.900 --> 00:28:54.900
<v Speaker 2>super powerful,</v>
<v Speaker 2>does everything better than we do.</v>

624
00:28:54.930 --> 00:28:57.090
<v Speaker 2>You know,</v>
<v Speaker 2>it'll recognize your emotion better</v>

625
00:28:57.091 --> 00:28:59.250
<v Speaker 2>than,</v>
<v Speaker 2>than another person can,</v>

626
00:28:59.640 --> 00:29:02.270
<v Speaker 2>but then the lights aren't on that.</v>

627
00:29:02.460 --> 00:29:04.830
<v Speaker 2>That's,</v>
<v Speaker 2>that's also I think possible.</v>

628
00:29:05.330 --> 00:29:07.420
<v Speaker 2>But maybe it's not possible,</v>
<v Speaker 2>but that's,</v>

629
00:29:07.421 --> 00:29:12.421
<v Speaker 2>that's the worst case scenario because</v>
<v Speaker 2>in the ethical silver lining and</v>

630
00:29:12.930 --> 00:29:13.780
<v Speaker 2>speaking,</v>
<v Speaker 2>you know,</v>

631
00:29:13.800 --> 00:29:17.850
<v Speaker 2>outside of our self interest now,</v>
<v Speaker 2>but just from a bird's eye view,</v>

632
00:29:18.380 --> 00:29:21.780
<v Speaker 2>um,</v>
<v Speaker 2>the ethical silver lining to building</v>

633
00:29:22.080 --> 00:29:24.330
<v Speaker 2>these mechanical gods that are conscious</v>
<v Speaker 2>is that,</v>

634
00:29:24.331 --> 00:29:24.841
<v Speaker 2>yes,</v>
<v Speaker 2>okay.</v>

635
00:29:24.841 --> 00:29:27.990
<v Speaker 2>We've,</v>
<v Speaker 2>in fact if we have built something that</v>

636
00:29:28.020 --> 00:29:33.020
<v Speaker 2>is far wiser and has far more beautiful</v>
<v Speaker 2>experiences and deeper experiences of</v>

637
00:29:33.511 --> 00:29:37.320
<v Speaker 2>the universe and we could ever imagine</v>
<v Speaker 2>and there there's something that it's</v>

638
00:29:37.321 --> 00:29:39.450
<v Speaker 2>like to be that thing that's just,</v>
<v Speaker 2>you know,</v>

639
00:29:39.480 --> 00:29:42.370
<v Speaker 2>it is a,</v>
<v Speaker 2>has a kind of a God like uh,</v>

640
00:29:42.400 --> 00:29:44.110
<v Speaker 2>experience.</v>
<v Speaker 2>Um,</v>

641
00:29:44.290 --> 00:29:47.020
<v Speaker 2>well that would be a very good thing</v>
<v Speaker 2>then we will have built you,</v>

642
00:29:47.080 --> 00:29:48.910
<v Speaker 2>we will have built something that was,</v>
<v Speaker 2>you know,</v>

643
00:29:48.940 --> 00:29:51.340
<v Speaker 2>if you stand outside of our narrow self</v>
<v Speaker 2>interest,</v>

644
00:29:52.120 --> 00:29:54.590
<v Speaker 2>I can understand why he would say that</v>
<v Speaker 2>he,</v>

645
00:29:54.591 --> 00:29:58.870
<v Speaker 2>he was just assuming what was scary</v>
<v Speaker 2>about that particular talk cause he was</v>

646
00:29:58.871 --> 00:30:03.871
<v Speaker 2>assuming that consciousness comes along</v>
<v Speaker 2>for the ride here.</v>

647
00:30:04.001 --> 00:30:05.890
<v Speaker 2>And I don't know that that is a safe</v>
<v Speaker 2>assumption.</v>

648
00:30:06.070 --> 00:30:09.790
<v Speaker 2>Well and the really terrifying thing is</v>
<v Speaker 2>who,</v>

649
00:30:10.610 --> 00:30:13.390
<v Speaker 3>if,</v>
<v Speaker 3>if this is constantly improving itself</v>

650
00:30:13.840 --> 00:30:18.840
<v Speaker 3>and it's under the Beck and call of a</v>
<v Speaker 3>person then so it's either conscious or</v>

651
00:30:19.360 --> 00:30:21.460
<v Speaker 3>unconscious where it acts as itself,</v>
<v Speaker 3>right?</v>

652
00:30:21.490 --> 00:30:24.160
<v Speaker 3>It acts as an individual thinking unit.</v>
<v Speaker 3>Right?</v>

653
00:30:24.161 --> 00:30:27.580
<v Speaker 3>Or as a a a thing outside of it's aware,</v>
<v Speaker 3>right?</v>

654
00:30:27.610 --> 00:30:31.600
<v Speaker 3>Either it is or it isn't.</v>
<v Speaker 3>And if it isn't aware and some person</v>

655
00:30:31.601 --> 00:30:34.930
<v Speaker 3>can manipulate it,</v>
<v Speaker 3>like imagine if it's getting 10,000,</v>

656
00:30:34.931 --> 00:30:37.000
<v Speaker 3>how many,</v>
<v Speaker 3>how many thousands of years in a week</v>

657
00:30:37.001 --> 00:30:38.180
<v Speaker 3>did you say?</v>
<v Speaker 3>I'll go,</v>

658
00:30:38.240 --> 00:30:39.900
<v Speaker 3>well,</v>
<v Speaker 3>if it was just improvement,</v>

659
00:30:39.910 --> 00:30:42.370
<v Speaker 3>it was just a million times faster than</v>
<v Speaker 3>we are.</v>

660
00:30:42.371 --> 00:30:45.850
<v Speaker 3>It's 20,000 years,</v>
<v Speaker 3>20,000 years in a week and a weekend in</v>

661
00:30:45.851 --> 00:30:47.710
<v Speaker 3>a week.</v>
<v Speaker 3>So with every week,</v>

662
00:30:47.711 --> 00:30:50.950
<v Speaker 3>this thing constantly gets better at</v>
<v Speaker 3>even doing that.</v>

663
00:30:50.980 --> 00:30:52.660
<v Speaker 3>Right?</v>
<v Speaker 3>So it's reprogramming itself.</v>

664
00:30:52.661 --> 00:30:54.370
<v Speaker 3>So it's all exponential.</v>

665
00:30:55.310 --> 00:30:58.250
<v Speaker 2>Presumably it just,</v>
<v Speaker 2>it just imagine again that you could</v>

666
00:30:58.251 --> 00:31:03.110
<v Speaker 2>keep it in the most restricted case.</v>
<v Speaker 2>You could just keep it at our level,</v>

667
00:31:03.630 --> 00:31:03.860
<v Speaker 2>but</v>

668
00:31:03.860 --> 00:31:05.240
<v Speaker 3>just,</v>
<v Speaker 3>just faster,</v>

669
00:31:05.330 --> 00:31:07.070
<v Speaker 3>just a million times faster.</v>
<v Speaker 3>But if it did,</v>

670
00:31:07.100 --> 00:31:09.710
<v Speaker 3>all of these things,</v>
<v Speaker 3>if it kept going and kept every week was</v>

671
00:31:09.770 --> 00:31:10.980
<v Speaker 3>thousands of years,</v>
<v Speaker 3>right?</v>

672
00:31:11.420 --> 00:31:13.070
<v Speaker 3>We're going to control it.</v>
<v Speaker 3>A person.</v>

673
00:31:13.170 --> 00:31:15.050
<v Speaker 3>No,</v>
<v Speaker 3>think that's even more insane.</v>

674
00:31:15.320 --> 00:31:18.440
<v Speaker 2>Imagine being in dialogue with something</v>
<v Speaker 2>that had that,</v>

675
00:31:18.630 --> 00:31:23.630
<v Speaker 2>that lived the 20,000 years of human</v>
<v Speaker 2>progress in a week.</v>

676
00:31:24.140 --> 00:31:25.500
<v Speaker 2>And you come back,</v>
<v Speaker 2>you know,</v>

677
00:31:25.580 --> 00:31:27.050
<v Speaker 2>on Monday and say,</v>
<v Speaker 2>listen,</v>

678
00:31:27.080 --> 00:31:27.920
<v Speaker 2>um,</v>
<v Speaker 2>I,</v>

679
00:31:27.921 --> 00:31:31.670
<v Speaker 2>that thing I told you to do last Monday,</v>
<v Speaker 2>I want to change that up and this thing</v>

680
00:31:31.671 --> 00:31:34.950
<v Speaker 2>has made 20,000 years of progress.</v>
<v Speaker 2>Um,</v>

681
00:31:35.090 --> 00:31:37.700
<v Speaker 2>and if it's in a condition where it has</v>
<v Speaker 2>access,</v>

682
00:31:37.701 --> 00:31:39.650
<v Speaker 2>I mean,</v>
<v Speaker 2>so we're imagining this thing,</v>

683
00:31:39.651 --> 00:31:40.910
<v Speaker 2>you know,</v>
<v Speaker 2>in a box,</v>

684
00:31:40.930 --> 00:31:44.180
<v Speaker 2>you know,</v>
<v Speaker 2>air gapped from the Internet and it's</v>

685
00:31:44.181 --> 00:31:45.800
<v Speaker 2>got nothing.</v>
<v Speaker 2>He's got no way to get out.</v>

686
00:31:45.830 --> 00:31:46.663
<v Speaker 2>Right.</v>
<v Speaker 2>Uh,</v>

687
00:31:47.090 --> 00:31:52.090
<v Speaker 2>even that is an unstable situation.</v>
<v Speaker 2>But just imagine this emerging in some</v>

688
00:31:52.161 --> 00:31:53.230
<v Speaker 2>way online,</v>
<v Speaker 2>right?</v>

689
00:31:53.270 --> 00:31:55.130
<v Speaker 2>Already being out in the wild.</v>
<v Speaker 2>Right?</v>

690
00:31:55.131 --> 00:31:57.020
<v Speaker 2>So let's say it's in a financial market,</v>
<v Speaker 2>right?</v>

691
00:31:57.590 --> 00:32:00.710
<v Speaker 2>Um,</v>
<v Speaker 2>that's again,</v>

692
00:32:00.711 --> 00:32:05.030
<v Speaker 2>this is what worries me most about this</v>
<v Speaker 2>and what is also interesting is that our</v>

693
00:32:05.060 --> 00:32:08.450
<v Speaker 2>intuitions here,</v>
<v Speaker 2>I think the primary intuition that</v>

694
00:32:08.451 --> 00:32:10.180
<v Speaker 2>people have is no,</v>
<v Speaker 2>no,</v>

695
00:32:10.181 --> 00:32:10.790
<v Speaker 2>no,</v>
<v Speaker 2>that's just,</v>

696
00:32:10.790 --> 00:32:13.070
<v Speaker 2>that's just not possible.</v>

697
00:32:13.100 --> 00:32:15.270
<v Speaker 2>Or not at all likely.</v>
<v Speaker 2>But if,</v>

698
00:32:15.280 --> 00:32:16.350
<v Speaker 2>if you're going to fund,</v>
<v Speaker 2>if you,</v>

699
00:32:16.351 --> 00:32:19.040
<v Speaker 2>you're going to think it's impossible or</v>
<v Speaker 2>even unlikely,</v>

700
00:32:19.700 --> 00:32:22.880
<v Speaker 2>you have to find something wrong with</v>
<v Speaker 2>the claim.</v>

701
00:32:23.120 --> 00:32:27.650
<v Speaker 2>That intelligence is just a matter of</v>
<v Speaker 2>information processing.</v>

702
00:32:28.330 --> 00:32:32.030
<v Speaker 2>Um,</v>
<v Speaker 2>I don't know any scientific reason to</v>

703
00:32:32.031 --> 00:32:34.490
<v Speaker 2>doubt that claim at the moment.</v>
<v Speaker 2>Um,</v>

704
00:32:34.940 --> 00:32:38.300
<v Speaker 2>and uh,</v>
<v Speaker 2>very good reasons to believe that it's</v>

705
00:32:38.301 --> 00:32:40.250
<v Speaker 2>just undoubtable,</v>
<v Speaker 2>uh,</v>

706
00:32:40.460 --> 00:32:45.460
<v Speaker 2>and the,</v>
<v Speaker 2>and you have to doubt that we will</v>

707
00:32:46.221 --> 00:32:50.090
<v Speaker 2>continue to make progress in the design</v>
<v Speaker 2>of intelligent machines.</v>

708
00:32:50.150 --> 00:32:54.500
<v Speaker 2>And,</v>
<v Speaker 2>but once you then is then that all this</v>

709
00:32:54.501 --> 00:32:56.000
<v Speaker 2>left is just time,</v>
<v Speaker 2>right?</v>

710
00:32:56.001 --> 00:32:56.540
<v Speaker 2>If,</v>
<v Speaker 2>if,</v>

711
00:32:56.540 --> 00:33:01.490
<v Speaker 2>if intelligence is just information</v>
<v Speaker 2>processing and we were going to continue</v>

712
00:33:01.491 --> 00:33:05.990
<v Speaker 2>to build better and better information</v>
<v Speaker 2>processors,</v>

713
00:33:06.530 --> 00:33:11.060
<v Speaker 2>at a certain point we're going to build</v>
<v Speaker 2>something that is superhuman.</v>

714
00:33:11.970 --> 00:33:16.970
<v Speaker 2>Um,</v>
<v Speaker 2>and so whether it's in five years or 50,</v>

715
00:33:17.310 --> 00:33:18.460
<v Speaker 2>it's,</v>
<v Speaker 2>it's a huge,</v>

716
00:33:18.550 --> 00:33:21.400
<v Speaker 2>I mean it's,</v>
<v Speaker 2>it's the biggest change in human history</v>

717
00:33:21.401 --> 00:33:22.390
<v Speaker 2>I think we can imagine.</v>

718
00:33:22.480 --> 00:33:24.050
<v Speaker 2>Right?</v>
<v Speaker 2>Um,</v>

719
00:33:24.520 --> 00:33:25.960
<v Speaker 2>so,</v>
<v Speaker 2>uh,</v>

720
00:33:26.500 --> 00:33:28.210
<v Speaker 2>and then people,</v>
<v Speaker 2>I what I felt fine.</v>

721
00:33:28.211 --> 00:33:32.860
<v Speaker 2>I keep finding myself in the presence of</v>
<v Speaker 2>people who seem at least to my eye to be</v>

722
00:33:32.861 --> 00:33:36.460
<v Speaker 2>refusing to imagine it like late,</v>
<v Speaker 2>they're treating it like the y two k</v>

723
00:33:36.550 --> 00:33:41.110
<v Speaker 2>virus or whatever where it's just the y</v>
<v Speaker 2>two k bug where it just may or may not</v>

724
00:33:41.111 --> 00:33:42.370
<v Speaker 2>be an issue.</v>
<v Speaker 2>Right?</v>

725
00:33:42.371 --> 00:33:43.900
<v Speaker 2>Like it,</v>
<v Speaker 2>like it's a hypothetical,</v>

726
00:33:44.080 --> 00:33:45.740
<v Speaker 2>like May this is just,</v>
<v Speaker 2>we're going to get there and it's,</v>

727
00:33:45.741 --> 00:33:48.550
<v Speaker 2>it's just going to be,</v>
<v Speaker 2>it's either not going to happen or it's,</v>

728
00:33:48.551 --> 00:33:49.810
<v Speaker 2>it's,</v>
<v Speaker 2>it's going to be trivial.</v>

729
00:33:49.811 --> 00:33:53.050
<v Speaker 2>But how you don't,</v>
<v Speaker 2>if you don't have an argument for why</v>

730
00:33:53.051 --> 00:33:55.970
<v Speaker 2>this isn't going to happen,</v>
<v Speaker 2>uh,</v>

731
00:33:56.410 --> 00:33:58.780
<v Speaker 2>then you have to have then,</v>
<v Speaker 2>then you're left with,</v>

732
00:33:58.810 --> 00:34:01.390
<v Speaker 2>okay,</v>
<v Speaker 2>what's it gonna be like to have,</v>

733
00:34:02.550 --> 00:34:07.550
<v Speaker 2>uh,</v>
<v Speaker 2>systems that are better than we are at</v>

734
00:34:07.930 --> 00:34:12.190
<v Speaker 2>everything in the intellectual space.</v>
<v Speaker 2>Um,</v>

735
00:34:14.050 --> 00:34:16.410
<v Speaker 2>and</v>
<v Speaker 2>you know,</v>

736
00:34:16.411 --> 00:34:18.990
<v Speaker 2>what will happen if that suddenly</v>
<v Speaker 2>happens in one country and not in</v>

737
00:34:18.991 --> 00:34:19.824
<v Speaker 2>another,</v>
<v Speaker 2>right?</v>

738
00:34:20.010 --> 00:34:20.843
<v Speaker 2>It's,</v>
<v Speaker 2>um,</v>

739
00:34:22.110 --> 00:34:22.943
<v Speaker 2>it's,</v>
<v Speaker 2>uh,</v>

740
00:34:23.610 --> 00:34:24.061
<v Speaker 2>it's,</v>
<v Speaker 2>I mean,</v>

741
00:34:24.061 --> 00:34:26.670
<v Speaker 2>it has enormous implications,</v>
<v Speaker 2>but it just sounds like science fiction.</v>

742
00:34:27.030 --> 00:34:27.280
<v Speaker 2>Yup.</v>

743
00:34:27.280 --> 00:34:31.600
<v Speaker 3>I don't know what's scarier.</v>
<v Speaker 3>The idea that an artificial intelligence</v>

744
00:34:31.601 --> 00:34:32.980
<v Speaker 3>can emerge.</v>
<v Speaker 3>It's conscious,</v>

745
00:34:32.981 --> 00:34:36.090
<v Speaker 3>it's aware of itself and that acts to</v>
<v Speaker 3>present per,</v>

746
00:34:36.091 --> 00:34:39.430
<v Speaker 3>per protect itself or the idea that a</v>
<v Speaker 3>person,</v>

747
00:34:40.390 --> 00:34:44.980
<v Speaker 3>a regular person like of today could be</v>
<v Speaker 3>in control of essentially a god.</v>

748
00:34:45.310 --> 00:34:47.500
<v Speaker 3>Right?</v>
<v Speaker 3>Because if this thing continues to get</v>

749
00:34:47.501 --> 00:34:50.890
<v Speaker 3>smarter and smarter with every week and</v>
<v Speaker 3>more and more power and more and more</v>

750
00:34:50.891 --> 00:34:53.290
<v Speaker 3>potential,</v>
<v Speaker 3>more and more understanding,</v>

751
00:34:53.291 --> 00:34:54.730
<v Speaker 3>thousands of years,</v>
<v Speaker 3>I mean it's just,</v>

752
00:34:55.720 --> 00:34:57.400
<v Speaker 3>yeah,</v>
<v Speaker 3>this one person,</v>

753
00:34:57.430 --> 00:34:59.980
<v Speaker 3>uh,</v>
<v Speaker 3>per regular person controlling that is</v>

754
00:34:59.981 --> 00:35:03.180
<v Speaker 3>almost more terrifying than creating a</v>
<v Speaker 3>new life or,</v>

755
00:35:03.210 --> 00:35:08.210
<v Speaker 3>or any group of people who don't have</v>
<v Speaker 3>the total welfare of humanity as their</v>

756
00:35:08.411 --> 00:35:10.270
<v Speaker 3>central concern.</v>
<v Speaker 3>And so just imagine,</v>

757
00:35:10.271 --> 00:35:10.721
<v Speaker 3>I mean,</v>
<v Speaker 3>what would,</v>

758
00:35:10.721 --> 00:35:12.730
<v Speaker 3>what would China do with it now?</v>
<v Speaker 3>Right?</v>

759
00:35:12.760 --> 00:35:14.950
<v Speaker 3>What would we,</v>
<v Speaker 3>what would we do if we thought China,</v>

760
00:35:15.400 --> 00:35:16.540
<v Speaker 3>you know,</v>
<v Speaker 3>Baidu or whatever,</v>

761
00:35:16.680 --> 00:35:19.800
<v Speaker 2>or some Chinese company was on the verge</v>
<v Speaker 2>of this thing.</v>

762
00:35:20.210 --> 00:35:22.650
<v Speaker 2>Um,</v>
<v Speaker 2>what would it be rational for us to do?</v>

763
00:35:22.800 --> 00:35:23.430
<v Speaker 2>You know?</v>
<v Speaker 2>I mean,</v>

764
00:35:23.430 --> 00:35:25.150
<v Speaker 2>if North Korea had it,</v>
<v Speaker 2>it would be,</v>

765
00:35:25.151 --> 00:35:28.230
<v Speaker 2>it'd be rational to nuke them given what</v>
<v Speaker 2>they say about,</v>

766
00:35:28.850 --> 00:35:30.670
<v Speaker 2>you know,</v>
<v Speaker 2>their relationship with the rest of the</v>

767
00:35:30.671 --> 00:35:32.520
<v Speaker 2>world.</v>
<v Speaker 2>So it's,</v>

768
00:35:32.521 --> 00:35:33.354
<v Speaker 2>um,</v>

769
00:35:34.000 --> 00:35:37.950
<v Speaker 3>well that kind of power.</v>
<v Speaker 3>Would you say rational that of power is,</v>

770
00:35:37.951 --> 00:35:41.670
<v Speaker 3>it's so life changing.</v>
<v Speaker 3>It's so what paradigm shifting,</v>

771
00:35:42.620 --> 00:35:43.650
<v Speaker 2>right?</v>
<v Speaker 2>But if you to,</v>

772
00:35:43.651 --> 00:35:47.060
<v Speaker 2>to wind this back to what someone like</v>
<v Speaker 2>Neil degrasse Tyson would say,</v>

773
00:35:47.061 --> 00:35:51.380
<v Speaker 2>is that the only basis for fear is,</v>
<v Speaker 2>yeah,</v>

774
00:35:51.410 --> 00:35:54.440
<v Speaker 2>don't give your superintelligent ai to</v>
<v Speaker 2>the next Hitler,</v>

775
00:35:54.620 --> 00:35:55.400
<v Speaker 2>right?</v>
<v Speaker 2>That's,</v>

776
00:35:55.400 --> 00:35:57.770
<v Speaker 2>that's obviously bad.</v>
<v Speaker 2>But if we don't,</v>

777
00:35:57.800 --> 00:36:01.970
<v Speaker 2>if we're not idiots and we just use it,</v>
<v Speaker 2>well,</v>

778
00:36:02.750 --> 00:36:05.240
<v Speaker 2>we're fine.</v>
<v Speaker 2>And that I think is an intuition that is</v>

779
00:36:05.241 --> 00:36:07.400
<v Speaker 2>just,</v>
<v Speaker 2>that's just a failure to,</v>

780
00:36:07.970 --> 00:36:12.970
<v Speaker 2>to unpack what is entailed by,</v>
<v Speaker 2>again,</v>

781
00:36:13.400 --> 00:36:15.230
<v Speaker 2>something like an intelligence</v>
<v Speaker 2>explosion,</v>

782
00:36:15.231 --> 00:36:19.850
<v Speaker 2>a process that once,</v>
<v Speaker 2>once you're talking about something that</v>

783
00:36:19.851 --> 00:36:24.740
<v Speaker 2>is able to change itself and you have to</v>
<v Speaker 2>get it.</v>

784
00:36:24.770 --> 00:36:27.090
<v Speaker 2>So what would it be like to guarantee</v>
<v Speaker 2>level?</v>

785
00:36:27.100 --> 00:36:28.100
<v Speaker 2>Let's say we decide,</v>
<v Speaker 2>okay,</v>

786
00:36:28.101 --> 00:36:30.920
<v Speaker 2>we're just not going to build anything</v>
<v Speaker 2>that can make changes to his own source</v>

787
00:36:30.921 --> 00:36:31.341
<v Speaker 2>code.</v>
<v Speaker 2>You know,</v>

788
00:36:31.341 --> 00:36:35.560
<v Speaker 2>any change to it,</v>
<v Speaker 2>to software at a certain point is going</v>

789
00:36:35.570 --> 00:36:37.310
<v Speaker 2>to have to be run through a human brain.</v>

790
00:36:37.770 --> 00:36:39.260
<v Speaker 2>Um,</v>
<v Speaker 2>and we're going to have veto power.</v>

791
00:36:39.261 --> 00:36:42.800
<v Speaker 2>Well,</v>
<v Speaker 2>is every person working on ai going to</v>

792
00:36:42.801 --> 00:36:44.400
<v Speaker 2>abide by that rule?</v>
<v Speaker 2>It's like we,</v>

793
00:36:44.420 --> 00:36:46.370
<v Speaker 2>we've agreed not to clone humans,</v>
<v Speaker 2>right?</v>

794
00:36:46.371 --> 00:36:48.650
<v Speaker 2>But you know,</v>
<v Speaker 2>we're going to stand by that agreement</v>

795
00:36:48.651 --> 00:36:50.180
<v Speaker 2>for the,</v>
<v Speaker 2>in the rest of human history.</v>

796
00:36:50.181 --> 00:36:52.210
<v Speaker 2>And is,</v>
<v Speaker 2>is a,</v>

797
00:36:52.280 --> 00:36:55.930
<v Speaker 2>is our agreement binding on China or</v>
<v Speaker 2>Singapore or you know,</v>

798
00:36:55.940 --> 00:36:58.130
<v Speaker 2>any other country that might think</v>
<v Speaker 2>otherwise.</v>

799
00:36:58.700 --> 00:37:00.950
<v Speaker 2>It's just we have a,</v>
<v Speaker 2>it's a free for all and at a certain</v>

800
00:37:00.951 --> 00:37:03.410
<v Speaker 2>point we're going to be,</v>
<v Speaker 2>you know,</v>

801
00:37:03.470 --> 00:37:06.170
<v Speaker 2>close enough.</v>
<v Speaker 2>Everyone's going to be close enough to</v>

802
00:37:06.171 --> 00:37:09.110
<v Speaker 2>making the final breakthrough that,</v>
<v Speaker 2>um,</v>

803
00:37:09.410 --> 00:37:12.010
<v Speaker 2>unless we have some,</v>
<v Speaker 2>uh,</v>

804
00:37:12.440 --> 00:37:16.670
<v Speaker 2>agreement about how to proceed if</v>
<v Speaker 2>someone is going to get there first,</v>

805
00:37:18.260 --> 00:37:21.590
<v Speaker 2>that is a terrifying scenario of the</v>
<v Speaker 2>future.</v>

806
00:37:22.540 --> 00:37:24.430
<v Speaker 3>You know,</v>
<v Speaker 3>you cemented this last time you were</v>

807
00:37:24.431 --> 00:37:27.940
<v Speaker 3>here,</v>
<v Speaker 3>but not as extreme as this time.</v>

808
00:37:27.941 --> 00:37:29.620
<v Speaker 3>You seem to be accelerating the</v>
<v Speaker 3>rhetoric.</v>

809
00:37:30.220 --> 00:37:31.053
<v Speaker 3>Exactly.</v>
<v Speaker 3>Yes.</v>

810
00:37:32.950 --> 00:37:34.710
<v Speaker 3>You're going deep.</v>
<v Speaker 3>Yeah.</v>

811
00:37:35.530 --> 00:37:36.430
<v Speaker 3>Boy,</v>
<v Speaker 3>I hope you're wrong.</v>

812
00:37:36.700 --> 00:37:39.140
<v Speaker 3>I'm on team Neil degrasse Tyson,</v>
<v Speaker 3>right?</v>

813
00:37:39.150 --> 00:37:39.983
<v Speaker 3>This one,</v>

814
00:37:41.040 --> 00:37:42.340
<v Speaker 2>Neil.</v>
<v Speaker 2>Um,</v>

815
00:37:42.480 --> 00:37:46.120
<v Speaker 2>and also in defensive of the other side</v>
<v Speaker 2>too,</v>

816
00:37:46.121 --> 00:37:46.831
<v Speaker 2>I should say that,</v>
<v Speaker 2>you know,</v>

817
00:37:46.831 --> 00:37:48.730
<v Speaker 2>like,</v>
<v Speaker 2>so David Deutsch also thinks I'm wrong,</v>

818
00:37:48.760 --> 00:37:53.760
<v Speaker 2>but he thinks I'm wrong because we will</v>
<v Speaker 2>integrate ourselves with these machines.</v>

819
00:37:54.401 --> 00:37:54.960
<v Speaker 2>I mean,</v>
<v Speaker 2>so that we,</v>

820
00:37:54.960 --> 00:37:57.460
<v Speaker 2>this will be,</v>
<v Speaker 2>there'll be extensions of ourselves and</v>

821
00:37:57.461 --> 00:37:59.620
<v Speaker 2>they can't help but be aligned with us</v>
<v Speaker 2>because we will,</v>

822
00:37:59.830 --> 00:38:02.620
<v Speaker 2>we will be connected to them.</v>
<v Speaker 2>That seems to be the only way we could</v>

823
00:38:02.621 --> 00:38:04.480
<v Speaker 2>all get along.</v>
<v Speaker 2>We have to merge become one.</v>

824
00:38:04.810 --> 00:38:08.230
<v Speaker 2>But I just think there's no,</v>
<v Speaker 2>there's no deep reason why,</v>

825
00:38:08.380 --> 00:38:10.330
<v Speaker 2>even if we decided to do that,</v>
<v Speaker 2>right?</v>

826
00:38:10.331 --> 00:38:11.920
<v Speaker 2>Like in the u s or,</v>
<v Speaker 2>or,</v>

827
00:38:11.980 --> 00:38:14.160
<v Speaker 2>or in half the world,</v>
<v Speaker 2>um,</v>

828
00:38:14.560 --> 00:38:15.311
<v Speaker 2>one,</v>
<v Speaker 2>there's,</v>

829
00:38:15.311 --> 00:38:18.040
<v Speaker 2>I think there are reasons to worry that</v>
<v Speaker 2>even that could go haywire,</v>

830
00:38:18.041 --> 00:38:23.041
<v Speaker 2>but there's no guarantee that someone</v>
<v Speaker 2>else couldn't just build ai in a box.</v>

831
00:38:23.291 --> 00:38:23.800
<v Speaker 2>I mean,</v>
<v Speaker 2>if we,</v>

832
00:38:23.800 --> 00:38:27.220
<v Speaker 2>if we can build ai such that we can</v>
<v Speaker 2>merge our brains with it,</v>

833
00:38:27.900 --> 00:38:30.670
<v Speaker 2>um,</v>
<v Speaker 2>someone can also just build ai in a box.</v>

834
00:38:30.910 --> 00:38:31.743
<v Speaker 2>Right.</v>
<v Speaker 2>And,</v>

835
00:38:31.900 --> 00:38:32.790
<v Speaker 2>and that's,</v>
<v Speaker 2>uh,</v>

836
00:38:33.850 --> 00:38:35.800
<v Speaker 2>um,</v>
<v Speaker 2>and then then inherit all the other</v>

837
00:38:35.801 --> 00:38:37.840
<v Speaker 2>problems that people are saying.</v>
<v Speaker 2>We don't have to worry about.</v>

838
00:38:37.960 --> 00:38:41.680
<v Speaker 2>If it was a good coen brothers movie,</v>
<v Speaker 2>it would be invented in the middle of</v>

839
00:38:41.681 --> 00:38:45.400
<v Speaker 2>the presidency of Donald Trump.</v>
<v Speaker 2>And so then that's when ai would go</v>

840
00:38:45.401 --> 00:38:47.890
<v Speaker 2>live.</v>
<v Speaker 2>And then ai would have to challenge</v>

841
00:38:47.891 --> 00:38:50.170
<v Speaker 2>Donald Trump and they would have like an</v>
<v Speaker 2>insult context,</v>

842
00:38:51.570 --> 00:38:56.570
<v Speaker 2>but that that's when this thing becomes</v>
<v Speaker 2>so comically,</v>

843
00:38:57.020 --> 00:38:58.660
<v Speaker 2>uh,</v>
<v Speaker 2>terrifying where it's just,</v>

844
00:38:59.200 --> 00:39:04.200
<v Speaker 2>just imagine Donald Trump being in a</v>
<v Speaker 2>position to make the final decisions on</v>

845
00:39:05.891 --> 00:39:10.540
<v Speaker 2>topics like this for the country.</v>
<v Speaker 2>That is,</v>

846
00:39:10.541 --> 00:39:13.270
<v Speaker 2>acne is going to do this almost</v>
<v Speaker 2>certainly in the near term.</v>

847
00:39:13.300 --> 00:39:16.390
<v Speaker 2>It's like he should,</v>
<v Speaker 2>we have a Manhattan project on this</v>

848
00:39:16.960 --> 00:39:18.130
<v Speaker 2>point,</v>
<v Speaker 2>Mr President.</v>

849
00:39:18.760 --> 00:39:20.920
<v Speaker 2>Um,</v>
<v Speaker 2>you know,</v>

850
00:39:21.640 --> 00:39:26.020
<v Speaker 2>the idea that anything of a value could</v>
<v Speaker 2>be happening between his ears on this</v>

851
00:39:26.021 --> 00:39:27.280
<v Speaker 2>topic.</v>
<v Speaker 2>We're a hundred others.</v>

852
00:39:27.280 --> 00:39:31.390
<v Speaker 2>Like it I think is now really</v>
<v Speaker 2>inconceivable.</v>

853
00:39:32.380 --> 00:39:36.040
<v Speaker 2>And so what w what,</v>
<v Speaker 2>what price could we,</v>

854
00:39:36.041 --> 00:39:38.830
<v Speaker 2>might we pay for that kind of</v>
<v Speaker 2>inattention and,</v>

855
00:39:39.690 --> 00:39:43.900
<v Speaker 2>and you know,</v>
<v Speaker 2>self satisfied in attention to these</v>

856
00:39:43.901 --> 00:39:45.220
<v Speaker 2>kinds of issues?</v>
<v Speaker 2>Well,</v>

857
00:39:45.221 --> 00:39:46.510
<v Speaker 2>this,</v>
<v Speaker 2>this issue,</v>

858
00:39:46.690 --> 00:39:50.200
<v Speaker 2>if this is real and if this could go</v>
<v Speaker 2>live in 50 years,</v>

859
00:39:50.260 --> 00:39:51.910
<v Speaker 2>this is the issue.</v>
<v Speaker 2>Yeah.</v>

860
00:39:52.150 --> 00:39:56.020
<v Speaker 2>Unless we fuck ourselves up beyond</v>
<v Speaker 2>repair before then and shut the power</v>

861
00:39:56.021 --> 00:39:58.480
<v Speaker 2>off if it keeps going.</v>
<v Speaker 2>Yeah.</v>

862
00:39:58.481 --> 00:39:59.440
<v Speaker 2>No,</v>
<v Speaker 2>I think it is.</v>

863
00:39:59.470 --> 00:40:01.590
<v Speaker 2>I think it is the issue,</v>
<v Speaker 2>but unfortunately it's the,</v>

864
00:40:01.640 --> 00:40:04.230
<v Speaker 2>it's the issue that doesn't,</v>
<v Speaker 2>it sounds like a goof.</v>

865
00:40:04.240 --> 00:40:05.380
<v Speaker 2>Yeah,</v>
<v Speaker 2>it does just sound,</v>

866
00:40:05.800 --> 00:40:08.140
<v Speaker 2>you sound like a crack pot even worrying</v>
<v Speaker 2>about this issue.</v>

867
00:40:08.260 --> 00:40:11.310
<v Speaker 2>It sounds completely ridiculous,</v>
<v Speaker 2>but that might be what's how it's</v>

868
00:40:11.311 --> 00:40:12.730
<v Speaker 2>sneaking in.</v>
<v Speaker 2>Yeah.</v>

869
00:40:13.300 --> 00:40:14.290
<v Speaker 2>Yeah.</v>
<v Speaker 2>I mean it's just,</v>

870
00:40:14.320 --> 00:40:17.110
<v Speaker 2>it,</v>
<v Speaker 2>just imagine that the tiny increment</v>

871
00:40:17.111 --> 00:40:19.870
<v Speaker 2>that would make suddenly make it</v>
<v Speaker 2>compelling.</v>

872
00:40:20.020 --> 00:40:21.220
<v Speaker 2>I mean,</v>
<v Speaker 2>just imagine,</v>

873
00:40:22.030 --> 00:40:26.050
<v Speaker 2>I mean chest doesn't do it because chess</v>
<v Speaker 2>is so far from any central human</v>

874
00:40:26.051 --> 00:40:27.550
<v Speaker 2>concern.</v>
<v Speaker 2>But just imagine if your,</v>

875
00:40:27.580 --> 00:40:32.580
<v Speaker 2>if your phone recognized your emotional</v>
<v Speaker 2>state better than any than your best</v>

876
00:40:33.731 --> 00:40:36.130
<v Speaker 2>friend or your wife or anyone in your</v>
<v Speaker 2>life.</v>

877
00:40:36.550 --> 00:40:38.490
<v Speaker 2>And it did it reliably.</v>
<v Speaker 2>Right.</v>

878
00:40:38.580 --> 00:40:41.890
<v Speaker 2>So as your body liked that movie with</v>
<v Speaker 2>Joaquin Phoenix,</v>

879
00:40:42.160 --> 00:40:44.230
<v Speaker 2>her,</v>
<v Speaker 2>he falls in love with his phone.</v>

880
00:40:44.231 --> 00:40:45.370
<v Speaker 2>Right?</v>
<v Speaker 2>I mean,</v>

881
00:40:45.371 --> 00:40:46.360
<v Speaker 2>that's just not,</v>
<v Speaker 2>you know,</v>

882
00:40:47.380 --> 00:40:51.820
<v Speaker 2>that is not far off that far off as not,</v>
<v Speaker 2>it's a very discreet ability.</v>

883
00:40:51.821 --> 00:40:52.451
<v Speaker 2>I mean,</v>
<v Speaker 2>you could do that.</v>

884
00:40:52.451 --> 00:40:56.130
<v Speaker 2>You could do that without any,</v>
<v Speaker 2>any other ability in the phone.</v>

885
00:40:56.140 --> 00:40:56.561
<v Speaker 2>Really.</v>
<v Speaker 2>It's like,</v>

886
00:40:56.561 --> 00:40:58.930
<v Speaker 2>it doesn't,</v>
<v Speaker 2>it doesn't have to to,</v>

887
00:40:59.560 --> 00:41:02.590
<v Speaker 2>uh,</v>
<v Speaker 2>stand on the shoulders of any other kind</v>

888
00:41:02.591 --> 00:41:04.420
<v Speaker 2>of intelligence.</v>
<v Speaker 2>It could just,</v>

889
00:41:04.421 --> 00:41:05.201
<v Speaker 2>you know,</v>
<v Speaker 2>he just,</v>

890
00:41:05.201 --> 00:41:06.250
<v Speaker 2>you have,</v>
<v Speaker 2>I mean,</v>

891
00:41:06.280 --> 00:41:08.770
<v Speaker 2>this could be,</v>
<v Speaker 2>you could do this with just brute force</v>

892
00:41:08.771 --> 00:41:11.950
<v Speaker 2>in the same way that you have a great</v>
<v Speaker 2>chess player that doesn't necessarily</v>

893
00:41:11.951 --> 00:41:15.120
<v Speaker 2>understand that is playing chess.</v>
<v Speaker 2>You could have it,</v>

894
00:41:15.130 --> 00:41:16.630
<v Speaker 2>you know,</v>
<v Speaker 2>it's kind of facial recognition,</v>

895
00:41:16.631 --> 00:41:20.530
<v Speaker 2>facial recognition of emotion and the,</v>
<v Speaker 2>and the tone of voice,</v>

896
00:41:20.531 --> 00:41:25.531
<v Speaker 2>recognition of emotion and um,</v>
<v Speaker 2>the idea that it's going to,</v>

897
00:41:26.301 --> 00:41:29.500
<v Speaker 2>it's going to be a very long time for</v>
<v Speaker 2>computers that get better than people at</v>

898
00:41:29.501 --> 00:41:32.680
<v Speaker 2>that I think is very farfetched.</v>
<v Speaker 2>I was thinking,</v>

899
00:41:32.710 --> 00:41:33.680
<v Speaker 2>yeah,</v>
<v Speaker 2>think you're right.</v>

900
00:41:33.710 --> 00:41:33.980
<v Speaker 2>I was</v>

901
00:41:33.980 --> 00:41:37.910
<v Speaker 3>just thinking how strange would it be if</v>
<v Speaker 3>you had like headphones on and your</v>

902
00:41:37.911 --> 00:41:41.030
<v Speaker 3>phone was in your pocket and you have</v>
<v Speaker 3>rational conversations with your phone?</v>

903
00:41:41.180 --> 00:41:42.860
<v Speaker 3>Like your phone knew you better than you</v>
<v Speaker 3>know,</v>

904
00:41:42.861 --> 00:41:44.090
<v Speaker 3>you like,</v>
<v Speaker 3>I mean,</v>

905
00:41:44.091 --> 00:41:45.110
<v Speaker 3>I don't know what to do.</v>
<v Speaker 3>I mean,</v>

906
00:41:45.111 --> 00:41:47.210
<v Speaker 3>I don't think I was out of line.</v>
<v Speaker 3>She yelled at me.</v>

907
00:41:47.211 --> 00:41:48.080
<v Speaker 3>I mean,</v>
<v Speaker 3>what should I say?</v>

908
00:41:48.230 --> 00:41:50.600
<v Speaker 3>And it would have listened to every one</v>
<v Speaker 3>of your conversations with your friends.</v>

909
00:41:50.601 --> 00:41:52.610
<v Speaker 3>Exactly.</v>
<v Speaker 3>Train up on that and just talk to you</v>

910
00:41:52.611 --> 00:41:53.630
<v Speaker 3>about it and go,</v>
<v Speaker 3>listen man,</v>

911
00:41:53.660 --> 00:41:55.620
<v Speaker 3>this is what you got gotta do.</v>
<v Speaker 3>I was talking to political,</v>

912
00:41:55.660 --> 00:41:58.390
<v Speaker 3>you were sounding angry.</v>
<v Speaker 3>You got defensive,</v>

913
00:41:58.440 --> 00:42:00.530
<v Speaker 3>you got defensive.</v>
<v Speaker 3>Why were you so to hold that Joe?</v>

914
00:42:00.531 --> 00:42:01.520
<v Speaker 3>Yeah.</v>
<v Speaker 3>Apologize,</v>

915
00:42:01.521 --> 00:42:03.110
<v Speaker 3>relax.</v>
<v Speaker 3>Let's all move on.</v>

916
00:42:03.260 --> 00:42:04.700
<v Speaker 3>If you could accelerate it or,</v>
<v Speaker 3>okay.</v>

917
00:42:04.701 --> 00:42:05.660
<v Speaker 3>You right man.</v>
<v Speaker 3>Right man,</v>

918
00:42:05.870 --> 00:42:07.370
<v Speaker 3>and like you're talking to this little</v>
<v Speaker 3>artificial.</v>

919
00:42:07.370 --> 00:42:10.670
<v Speaker 3>Maybe that's the first version of</v>
<v Speaker 3>artificial intelligence that we suggest.</v>

920
00:42:11.080 --> 00:42:11.691
<v Speaker 3>We'd say,</v>
<v Speaker 3>all right,</v>

921
00:42:11.691 --> 00:42:14.090
<v Speaker 3>let's give it a shot and like self help</v>
<v Speaker 3>guys in your phone,</v>

922
00:42:14.370 --> 00:42:16.250
<v Speaker 3>you have like a personal trainer in your</v>
<v Speaker 3>phone.</v>

923
00:42:16.280 --> 00:42:19.640
<v Speaker 3>How to talk to girls at first.</v>
<v Speaker 3>Breakthrough yet slow down,</v>

924
00:42:19.641 --> 00:42:20.780
<v Speaker 3>dude.</v>
<v Speaker 3>Slow down.</v>

925
00:42:21.040 --> 00:42:23.330
<v Speaker 3>You're talking too fast.</v>
<v Speaker 3>Got To cool.</v>

926
00:42:23.570 --> 00:42:26.330
<v Speaker 3>Yeah,</v>
<v Speaker 3>I mean literally like giving you</v>

927
00:42:26.331 --> 00:42:27.830
<v Speaker 3>information.</v>
<v Speaker 3>That would be like step one.</v>

928
00:42:27.831 --> 00:42:30.590
<v Speaker 3>That'd be like the Sony Walkman.</v>
<v Speaker 3>Remember when you had a Walkman,</v>

929
00:42:30.591 --> 00:42:35.591
<v Speaker 3>like a cassette player that was like,</v>
<v Speaker 3>that was like a vcr when we were on our</v>

930
00:42:35.781 --> 00:42:40.190
<v Speaker 3>way to what we have today where you have</v>
<v Speaker 3>fucking 30,000 songs in your phone or</v>

931
00:42:40.191 --> 00:42:42.410
<v Speaker 3>something.</v>
<v Speaker 3>I think I remember the first Walkman.</v>

932
00:42:42.411 --> 00:42:45.260
<v Speaker 3>The first thing I just want to it back</v>
<v Speaker 3>when I skied,</v>

933
00:42:45.261 --> 00:42:47.780
<v Speaker 3>there was something called,</v>
<v Speaker 3>it was called astral tunes or something.</v>

934
00:42:47.781 --> 00:42:50.210
<v Speaker 3>It was like,</v>
<v Speaker 3>it was like a car radio that,</v>

935
00:42:50.260 --> 00:42:52.490
<v Speaker 3>that you could just put on in a pack on</v>
<v Speaker 3>your chest.</v>

936
00:42:53.090 --> 00:42:54.860
<v Speaker 3>Um,</v>
<v Speaker 3>yeah,</v>

937
00:42:54.861 --> 00:42:56.720
<v Speaker 3>it was,</v>
<v Speaker 3>they kept coming out with those sounds.</v>

938
00:42:56.930 --> 00:42:59.000
<v Speaker 3>They would get smaller and smaller.</v>
<v Speaker 3>So then that little,</v>

939
00:42:59.060 --> 00:43:00.830
<v Speaker 3>the little dude would start telling you,</v>
<v Speaker 3>Yo man,</v>

940
00:43:00.831 --> 00:43:02.240
<v Speaker 3>dude,</v>
<v Speaker 3>listen to keep it.</v>

941
00:43:02.241 --> 00:43:04.310
<v Speaker 3>Place me over a year.</v>
<v Speaker 3>Just let him stick me in.</v>

942
00:43:04.311 --> 00:43:06.170
<v Speaker 3>Your brain will be together all the</v>
<v Speaker 3>time.</v>

943
00:43:06.350 --> 00:43:08.140
<v Speaker 3>Yeah,</v>
<v Speaker 3>yeah.</v>

944
00:43:08.540 --> 00:43:10.520
<v Speaker 3>Giving you good advice for years,</v>
<v Speaker 3>Bro.</v>

945
00:43:10.910 --> 00:43:14.360
<v Speaker 3>Let me and your brain.</v>
<v Speaker 3>And so you and this little artificial</v>

946
00:43:14.361 --> 00:43:16.760
<v Speaker 3>intelligence,</v>
<v Speaker 3>do you have a relationship over time?</v>

947
00:43:17.020 --> 00:43:20.600
<v Speaker 3>And eventually it talks you into getting</v>
<v Speaker 3>your head drilled and the screw it in</v>

948
00:43:20.601 --> 00:43:24.260
<v Speaker 3>there and your artificial intelligence</v>
<v Speaker 3>is always powered by your central</v>

949
00:43:24.261 --> 00:43:25.370
<v Speaker 3>nervous system.</v>
<v Speaker 3>Did you,</v>

950
00:43:25.430 --> 00:43:28.370
<v Speaker 3>have you seen most of these movies?</v>
<v Speaker 3>Like did you see her and,</v>

951
00:43:28.520 --> 00:43:29.450
<v Speaker 3>and no,</v>
<v Speaker 3>I didn't.</v>

952
00:43:29.510 --> 00:43:30.360
<v Speaker 3>Okay.</v>
<v Speaker 3>No.</v>

953
00:43:31.700 --> 00:43:33.030
<v Speaker 3>Did you see Alex?</v>
<v Speaker 3>Mark and I,</v>

954
00:43:33.260 --> 00:43:34.460
<v Speaker 3>that was one of my,</v>
<v Speaker 3>that was good.</v>

955
00:43:34.580 --> 00:43:35.960
<v Speaker 3>Top 10 all time favorite movies.</v>

956
00:43:35.960 --> 00:43:37.760
<v Speaker 3>Yeah,</v>
<v Speaker 3>I loved that movie actually.</v>

957
00:43:37.761 --> 00:43:39.290
<v Speaker 3>I like it.</v>
<v Speaker 3>I saw it twice.</v>

958
00:43:39.291 --> 00:43:39.591
<v Speaker 3>I said,</v>
<v Speaker 3>I,</v>

959
00:43:39.591 --> 00:43:42.860
<v Speaker 3>I,</v>
<v Speaker 3>I was slow to realize how well,</v>

960
00:43:43.310 --> 00:43:44.420
<v Speaker 3>uh,</v>
<v Speaker 3>they,</v>

961
00:43:44.930 --> 00:43:45.770
<v Speaker 3>they did it.</v>
<v Speaker 3>I mean,</v>

962
00:43:45.771 --> 00:43:48.080
<v Speaker 3>it was just the first time I saw it,</v>
<v Speaker 3>I thought,</v>

963
00:43:48.700 --> 00:43:51.410
<v Speaker 3>um,</v>
<v Speaker 3>I wasn't as impressed and I watched it</v>

964
00:43:51.411 --> 00:43:52.940
<v Speaker 3>again and they really,</v>
<v Speaker 3>I mean,</v>

965
00:43:53.000 --> 00:43:54.050
<v Speaker 3>first of all,</v>
<v Speaker 3>the performance of,</v>

966
00:43:54.080 --> 00:43:57.590
<v Speaker 3>of um,</v>
<v Speaker 3>I forgot the actress's name.</v>

967
00:43:57.860 --> 00:43:59.590
<v Speaker 3>Uh,</v>
<v Speaker 3>the candor,</v>

968
00:43:59.610 --> 00:44:01.660
<v Speaker 3>at least you have a candor today.</v>
<v Speaker 3>Um,</v>

969
00:44:01.790 --> 00:44:03.650
<v Speaker 3>there was a woman who plays the robot in</v>
<v Speaker 3>an x,</v>

970
00:44:03.670 --> 00:44:05.450
<v Speaker 3>Monica.</v>
<v Speaker 3>It was just fantastic.</v>

971
00:44:05.510 --> 00:44:06.570
<v Speaker 3>But,</v>
<v Speaker 3>um,</v>

972
00:44:06.620 --> 00:44:08.450
<v Speaker 3>scary.</v>
<v Speaker 3>Good tuck you in.</v>

973
00:44:08.451 --> 00:44:10.700
<v Speaker 3>Anything.</v>
<v Speaker 3>We're getting a little full on time.</v>

974
00:44:10.870 --> 00:44:12.890
<v Speaker 3>Yeah.</v>
<v Speaker 3>What do we like five hours in a half</v>

975
00:44:12.890 --> 00:44:14.630
<v Speaker 3>hours in.</v>
<v Speaker 3>But I just got to know how to spot to</v>

976
00:44:14.631 --> 00:44:15.464
<v Speaker 3>fill up.</v>
<v Speaker 3>Wait a minute.</v>

977
00:44:15.470 --> 00:44:17.080
<v Speaker 3>How many hours?</v>
<v Speaker 3>Four and a half hour.</v>

978
00:44:17.100 --> 00:44:19.370
<v Speaker 3>No computers about to fill up.</v>
<v Speaker 3>Yeah we did.</v>

979
00:44:19.550 --> 00:44:21.020
<v Speaker 3>We just did a four and a half hour</v>
<v Speaker 3>pocket.</v>

980
00:44:21.260 --> 00:44:24.400
<v Speaker 3>We ready to keep going to Jesus.</v>
<v Speaker 3>Jamie didn't cock block.</v>

981
00:44:26.360 --> 00:44:29.060
<v Speaker 3>You know what man,</v>
<v Speaker 3>once you opened up that box up Pandora's</v>

982
00:44:29.061 --> 00:44:32.070
<v Speaker 3>box of artificial,</v>
<v Speaker 3>I haven't heard you guys discussed</v>

983
00:44:32.070 --> 00:44:36.060
<v Speaker 5>it and I've looked up,</v>
<v Speaker 5>is there any sort of concept of autism</v>

984
00:44:36.061 --> 00:44:38.010
<v Speaker 5>in ai?</v>
<v Speaker 5>Like a spectrum of ai?</v>

985
00:44:38.370 --> 00:44:40.320
<v Speaker 5>Like oh,</v>
<v Speaker 5>there are dumb ai and there's going to</v>

986
00:44:40.321 --> 00:44:41.154
<v Speaker 5>be smart Ai.</v>

987
00:44:42.060 --> 00:44:42.893
<v Speaker 2>I know.</v>
<v Speaker 2>So,</v>

988
00:44:42.940 --> 00:44:46.190
<v Speaker 2>I mean the scary thing so that yeah,</v>
<v Speaker 2>it's like super autism.</v>

989
00:44:46.700 --> 00:44:48.260
<v Speaker 2>There's no,</v>
<v Speaker 2>um,</v>

990
00:44:49.490 --> 00:44:53.900
<v Speaker 2>across the board there's,</v>
<v Speaker 2>I think that super intelligence and</v>

991
00:44:54.320 --> 00:44:57.230
<v Speaker 2>motivation and goals are totally</v>
<v Speaker 2>separable.</v>

992
00:44:57.231 --> 00:45:02.231
<v Speaker 2>So you could have a super intelligent</v>
<v Speaker 2>machine that is purposed toward a goal</v>

993
00:45:02.841 --> 00:45:07.370
<v Speaker 2>that just seems completely absurd and</v>
<v Speaker 2>harmful and non common sensical.</v>

994
00:45:07.371 --> 00:45:09.860
<v Speaker 2>And they said they,</v>
<v Speaker 2>the example that that Nick Bostrom uses</v>

995
00:45:09.861 --> 00:45:12.020
<v Speaker 2>in his,</v>
<v Speaker 2>in his book superintelligence which was</v>

996
00:45:12.021 --> 00:45:13.820
<v Speaker 2>a great book,</v>
<v Speaker 2>um,</v>

997
00:45:13.940 --> 00:45:17.270
<v Speaker 2>and did more to inform my thinking on</v>
<v Speaker 2>this topic than any other source.</v>

998
00:45:17.690 --> 00:45:20.540
<v Speaker 2>Um,</v>
<v Speaker 2>he talks about it a paperclip maximizer.</v>

999
00:45:20.570 --> 00:45:22.400
<v Speaker 2>You could,</v>
<v Speaker 2>you could build a super intelligent</v>

1000
00:45:22.401 --> 00:45:23.630
<v Speaker 2>paperclip maximizer.</v>
<v Speaker 2>Now,</v>

1001
00:45:23.631 --> 00:45:27.470
<v Speaker 2>not that anyone would do this,</v>
<v Speaker 2>but the point is you could build a</v>

1002
00:45:27.471 --> 00:45:31.040
<v Speaker 2>machine that was,</v>
<v Speaker 2>that was smarter than we are in every</v>

1003
00:45:31.041 --> 00:45:33.380
<v Speaker 2>conceivable way,</v>
<v Speaker 2>but all it wants to do is produce paper</v>

1004
00:45:33.381 --> 00:45:34.040
<v Speaker 2>clips.</v>

1005
00:45:34.040 --> 00:45:35.710
<v Speaker 2>Right.</v>
<v Speaker 2>Now that seems counterintuitive,</v>

1006
00:45:35.720 --> 00:45:38.360
<v Speaker 2>but there's no,</v>
<v Speaker 2>there's no reason when you kind of dig</v>

1007
00:45:38.420 --> 00:45:42.600
<v Speaker 2>deeply into this,</v>
<v Speaker 2>there's no reason why you couldn't build</v>

1008
00:45:42.700 --> 00:45:46.670
<v Speaker 2>a superhuman paperclip maximizer.</v>
<v Speaker 2>I just wants to turn everything,</v>

1009
00:45:46.671 --> 00:45:48.680
<v Speaker 2>you know,</v>
<v Speaker 2>just literally the atoms in your body</v>

1010
00:45:48.681 --> 00:45:51.090
<v Speaker 2>would be better used as paperclips.</v>
<v Speaker 2>Um,</v>

1011
00:45:51.740 --> 00:45:56.600
<v Speaker 2>and so this is just the point he's</v>
<v Speaker 2>making is that superintelligence could</v>

1012
00:45:56.601 --> 00:46:00.260
<v Speaker 2>be very counterintuitive.</v>
<v Speaker 2>It's not necessarily going to inherit</v>

1013
00:46:00.950 --> 00:46:02.810
<v Speaker 2>everything we find as,</v>
<v Speaker 2>you know,</v>

1014
00:46:02.811 --> 00:46:06.980
<v Speaker 2>common sensical or,</v>
<v Speaker 2>or emotionally appropriate or wise or</v>

1015
00:46:06.981 --> 00:46:08.300
<v Speaker 2>desirable.</v>
<v Speaker 2>Uh,</v>

1016
00:46:08.301 --> 00:46:13.301
<v Speaker 2>it could be totally foreign,</v>
<v Speaker 2>totally trivial in some way,</v>

1017
00:46:14.091 --> 00:46:16.610
<v Speaker 2>you know,</v>
<v Speaker 2>focused on something that means nothing</v>

1018
00:46:16.611 --> 00:46:18.470
<v Speaker 2>to us,</v>
<v Speaker 2>but means everything to it because of</v>

1019
00:46:18.471 --> 00:46:23.471
<v Speaker 2>some quirk and how it's motivation</v>
<v Speaker 2>system is structured and yet it can</v>

1020
00:46:24.140 --> 00:46:28.460
<v Speaker 2>build the perfect nanotechnology that</v>
<v Speaker 2>will allow it to build more paperclips.</v>

1021
00:46:28.550 --> 00:46:29.383
<v Speaker 2>Right?</v>
<v Speaker 2>So,</v>

1022
00:46:29.870 --> 00:46:31.850
<v Speaker 2>um,</v>
<v Speaker 2>and Lisa,</v>

1023
00:46:31.851 --> 00:46:34.070
<v Speaker 2>at least,</v>
<v Speaker 2>I don't think anyone can see why that's</v>

1024
00:46:34.071 --> 00:46:35.180
<v Speaker 2>ruled out in advance.</v>

1025
00:46:35.180 --> 00:46:36.800
<v Speaker 2>I mean,</v>
<v Speaker 2>there's no reason why we would</v>

1026
00:46:36.980 --> 00:46:41.450
<v Speaker 2>intentionally build that,</v>
<v Speaker 2>but the fear is we might build something</v>

1027
00:46:42.230 --> 00:46:47.230
<v Speaker 2>that either is not</v>
<v Speaker 2>perfectly aligned with our goals and our</v>

1028
00:46:49.221 --> 00:46:50.930
<v Speaker 2>common sense and our,</v>
<v Speaker 2>and our,</v>

1029
00:46:50.931 --> 00:46:54.560
<v Speaker 2>um,</v>
<v Speaker 2>aspirations and that it could form some</v>

1030
00:46:55.100 --> 00:46:58.490
<v Speaker 2>kind of separate in instrumental goals</v>
<v Speaker 2>to get what his wants,</v>

1031
00:46:58.520 --> 00:47:02.630
<v Speaker 2>wants that are totally incompatible with</v>
<v Speaker 2>life as we know it.</v>

1032
00:47:02.631 --> 00:47:04.400
<v Speaker 2>And that's,</v>
<v Speaker 2>you know,</v>

1033
00:47:04.430 --> 00:47:05.170
<v Speaker 2>I mean,</v>
<v Speaker 2>again,</v>

1034
00:47:05.170 --> 00:47:05.661
<v Speaker 2>the,</v>
<v Speaker 2>the,</v>

1035
00:47:05.661 --> 00:47:07.490
<v Speaker 2>the examples of this are always</v>
<v Speaker 2>cartoonish.</v>

1036
00:47:07.491 --> 00:47:08.271
<v Speaker 2>Like,</v>
<v Speaker 2>you know how I mean,</v>

1037
00:47:08.271 --> 00:47:09.380
<v Speaker 2>Elon Musk said,</v>
<v Speaker 2>you know,</v>

1038
00:47:09.390 --> 00:47:12.500
<v Speaker 2>if you built up super intelligent</v>
<v Speaker 2>machine and he told it to reduce spam,</v>

1039
00:47:12.950 --> 00:47:16.040
<v Speaker 2>well then it could just kill all people</v>
<v Speaker 2>as a great way to reduce spam.</v>

1040
00:47:16.041 --> 00:47:16.874
<v Speaker 2>Right.</v>
<v Speaker 2>Um,</v>

1041
00:47:17.270 --> 00:47:20.210
<v Speaker 2>but see the reason why that's La,</v>
<v Speaker 2>that's laughable,</v>

1042
00:47:20.211 --> 00:47:23.420
<v Speaker 2>but there's,</v>
<v Speaker 2>you can't assume the common sense won't</v>

1043
00:47:23.421 --> 00:47:25.280
<v Speaker 2>be there unless we've built it.</v>
<v Speaker 2>Right?</v>

1044
00:47:25.281 --> 00:47:26.810
<v Speaker 2>Like,</v>
<v Speaker 2>you have to have anticipated all of</v>

1045
00:47:26.811 --> 00:47:27.140
<v Speaker 2>this.</v>

1046
00:47:27.140 --> 00:47:27.980
<v Speaker 2>You can't,</v>
<v Speaker 2>if you say,</v>

1047
00:47:27.981 --> 00:47:29.890
<v Speaker 2>take me to the airport as fast as you</v>
<v Speaker 2>can.</v>

1048
00:47:30.340 --> 00:47:31.340
<v Speaker 2>Again,</v>
<v Speaker 2>this is Bostrom,</v>

1049
00:47:31.620 --> 00:47:33.730
<v Speaker 2>you know,</v>
<v Speaker 2>and you have a super intelligent,</v>

1050
00:47:34.090 --> 00:47:36.700
<v Speaker 2>automatic a car.</v>
<v Speaker 2>Um,</v>

1051
00:47:36.910 --> 00:47:38.200
<v Speaker 2>you know,</v>
<v Speaker 2>a self driving car,</v>

1052
00:47:38.210 --> 00:47:39.970
<v Speaker 2>you'll just,</v>
<v Speaker 2>you'll get to the airport covered in</v>

1053
00:47:39.971 --> 00:47:41.220
<v Speaker 2>vomit because he'll just,</v>
<v Speaker 2>it's,</v>

1054
00:47:41.221 --> 00:47:43.660
<v Speaker 2>it's just going to go as fast as it can</v>
<v Speaker 2>go.</v>

1055
00:47:44.290 --> 00:47:46.510
<v Speaker 2>Um,</v>
<v Speaker 2>so it's a,</v>

1056
00:47:47.410 --> 00:47:51.130
<v Speaker 2>it's our intuitions about what it would</v>
<v Speaker 2>mean to be super intelligent</v>

1057
00:47:51.520 --> 00:47:52.840
<v Speaker 2>necessarily,</v>
<v Speaker 2>or are,</v>

1058
00:47:52.841 --> 00:47:54.880
<v Speaker 2>um,</v>
<v Speaker 2>I mean there's no,</v>

1059
00:47:54.970 --> 00:47:57.680
<v Speaker 2>we have to correct for them,</v>
<v Speaker 2>cause I think our intuitions are,</v>

1060
00:47:57.790 --> 00:47:58.623
<v Speaker 2>are bad.</v>

1061
00:48:03.070 --> 00:48:06.030
<v Speaker 6>[inaudible].</v>

