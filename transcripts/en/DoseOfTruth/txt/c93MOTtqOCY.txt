Speaker 1:          00:02          This is an interesting thing to know. It's called the binomial effect size. Display effect size is the magnitude of an effect, right? And, and it's not an easy thing to get a handle on all that you really need to if you're going to be a psychologist because in any study there's an effect size indicator correlation off often r and r squared, which is the correlation squared or a Cohen's d which has uh, the effect size expressed in standard deviation. So something like that, but you kind of have to understand that at a basic level to understand what statistics actually do. And there's this phenomenon called the binomial effect size display that can help you understand what like an embodied sense, what the magnitude of a correlation means. So here's how it works. Imagine that you have a predictor of 0.2. So the correlation is r equals 0.2 between um, phenomena one we'll say conscientiousness and phenomena two workplace performance 0.2 correlation.

Speaker 1:          00:58          The question might be, well, how much, how much would you improve your predictive capacity over chance levels if you applied that predictor? And the answer is that the ar is the difference between the odds ratio. So let me explain that. So 0.5 0.5 if you subtract one from the another, you get zero so the predictive validity of selection by chance is 0.5 minus 0.5 equals zero that's the predictive validity of chance. If you have a predictor of 0.2 which is approximately, that's sort of the low end estimate for conscientiousness, then that would change your odds ratio from 0.5 0.5 right random 2.6 0.4 because 0.6 minus 0.4 is 0.2 and so the correlation coefficient turns out to be the the difference between the odds between the odds so and so it gives you a quick rule of thumb. So for example, so if you have a 0.2 predictor that gives you a 60 40 you have a 0.3 predictor that gives you 65 35 because 0.65 minus 0.35 is 0.3 and if you have a 0.6 predictor, which is really up on the high end, right, you're really starting to push your the limits of statistical prediction, prediction validity at that point.

Speaker 1:          02:20          That gives you a 0.8 minus 0.2 and so what you've done, if you use a predictive, a predictor that has a correlation Coefficient of 0.6 which you could get for example, if you took conscientiousness and combine that with a good test of Iq for predicting complex jobs, you might be able to get up to 0.6 that moves your odds ratio of selecting an above average person for the position from 0.5 0.5 to 0.8 0.2 so it cuts here your failure rate by more than half, right? It brings it down from 0.5 0.2 because 0.8 minus 0.2 0.6 so that's a really good thing to know. That's called the binomial effect size display. It's good thing to have in your mind. It's very simple. It's just a, it's just subtraction and it gives you some sense of the power of, of, of statistical prediction. Now the question might be, well, let's say you had a predictor of 0.2 the conscientiousness, you might say, well if you square, they are, that gives you 4% of the variance.

Speaker 1:          03:18          Like who the hell cares? 4% of the variance. You've left 95% of the variability between people in terms of their performance unexplained. You might say, well, why even bother? Well, the answer to that question is how much difference in productive output is there between people? Because if there's a tremendous degree of productive, of difference in productive output between people, then increasing your ability to predict someone's performance even by some relatively small increment might have massive economic utility. You know, if, if let's say the top 10% of your people are 50 times as productive as the bottom 10% of your people, then shifting your ability to predict up so that you have more of those extremely high performing people or less of the extremely low performing PR people might more than pay off, might more than pay for itself from an economic perspective. Even though your prediction, your predictor isn't, it doesn't have that massive amount of power when that actually happens to be the case.

Speaker 1:          04:17          So back in 1968 there was a guy named Walter Michelle, he had reviewed, he's a social psychologist. He reviewed the personality literature up to that point and concluded that the typical personality measure only predicted the typical performance measure at about 0.2 and that's actually remained relatively stable. I would say it's a little higher than that. It's probably 0.25 especially if you do things like correct for measurement error and so forth. And what Michelle said was, because it's only 0.25 let's say you square that, that's 5% of the variants. You leave 95% of the phenomena unexplained, you might as well not even bother measuring personality. And so that actually killed the field of personality from a psychometric perspective for about 25 years really until about the early 1990s when people woke up and thought, wait a minute, what are the typical effect sizes in other domains of prediction?

Speaker 1:          05:08          And then they found out that, well, the 0.2 correlation that was typical of personality prediction was actually pretty damn good by social sciences or health sciences standards. Like it doesn't sound good when you just think about it as an absolute measure because it leaves 95% of the phenomena unexplained. But when you compare it to other things that people consider of reasonable magnitude than it turns out that personality psychologists are doing just fine. And then also in the 1990s and I'll show you some of this there, economic calculations done and the, so one of the calculations would be, well imagine that you have, maybe it took 20 companies and you did it a distribution of the productivity of their employees. It's hard thing to do because you have to measure their productivity. It's like how the hell do you do that? Right? With salespeople, you can measure sales.

Speaker 1:          05:55          That's pretty straight forward with you can measure hours billed. Like there are some, there are some occupations where the performance measure is sort of built into the job. But if you're a manager in the mid level of a large corporation, how the hell can you tell how productive you are? So there's a measurement problem on the productivity measurement and as well as the performance prediction and, and it's, it's a very intractable problem. And the way that people often do that is by saying, well let's say you're a manager in the mid level of a corporation. How do we determine how productive we are? You are, well, we might ask you to compare your work productivity with your peers, maybe construct up a questionnaire asking about your efficient use of time and so forth. And then we might get your peers to do the same thing to you.

Speaker 1:          06:38          We might get your supervisors to do the same thing to you and we might get your, your subordinates to do the same thing to you and then aggregate across all those measures and infer that that aggregate opinion actually constitutes a valid measure of productivity. You actually don't know, right, because that assumes that what you're doing that your peers and your supervisors and so forth, our rating is actually related in some positive manner to the bottom line of the company and you actually don't, you actually can't figure that out. This is actually, I think why why large companies start to become unstable is because if there's enough layers between the operations of the people in the tears of the corporation and the real outcome measure, which is basically profit because that's what we've got. Then the, the relationship between your activity as a manager and the productivity of the company starts to become increasingly blurred and that might mean that you're working as hard as you can on something that's actually going to cost the company money.

Speaker 1:          07:36          So you would actually be much more productive from a profit perspective if he just didn't go to work at all and that that happens a lot in large corporations because you'd never know, especially if there's a lot of steps that have to be undertaken in a process before you can test the product in the market. You have no idea if you're wasting time and resources you just can't tell. So the performance measurement issue is a very, very complicated one. We haven't talked about it that much, but I give you a kind of a brief overview of it. Now what you really want to do is have multiple sources of information about performance and aggregate across them and if you can use real world measures that are tied to two income generation, so much the better because you have to use something as your gold standard, right?

Speaker 1:          08:18          You have to say at some point, well, we're going to define this as reality when it comes to performance and in a free market economy, roughly what you do there is you say that what profit is the proxy for productivity, and that isn't the same thing saying is that it isn't the same thing as saying that profit is productivity. That's not the same thing. It's saying that at some point you have to decide what you're going to accept as a measure of productivity because otherwise there's no point in even talking about it and you can't just not talk about productivity if you're running an organization because the organization doesn't exist unless it produces something that we'll keep it going. And generally that happens to be money.