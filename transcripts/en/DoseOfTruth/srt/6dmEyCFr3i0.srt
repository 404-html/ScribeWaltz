1
00:00:06,440 --> 00:00:09,360
So let's,
let's look at that user interface idea.

2
00:00:10,560 --> 00:00:13,840
Obviously what happens when you're
looking at a computer screen is that the

3
00:00:13,841 --> 00:00:18,460
complexity of the screen is reduced to a
set of icons that can serve as tools.

4
00:00:19,620 --> 00:00:22,390
And I think that that's a reasonable way
of thinking about how we look at the

5
00:00:22,391 --> 00:00:23,110
world

6
00:00:23,110 --> 00:00:25,480
flex in it,
in that Hoffman's theory is that natural

7
00:00:25,481 --> 00:00:29,920
selection didn't select our brains to,
to record an accurate representation of

8
00:00:29,921 --> 00:00:32,770
reality.
Like the scientific model we'll attempts

9
00:00:32,771 --> 00:00:34,840
to get ever closer to what reality is
really light.

10
00:00:34,841 --> 00:00:37,630
No natural selection just wants us to
escape predators.

11
00:00:37,631 --> 00:00:41,020
It doesn't matter what they look like,
what the icon is in your brain of

12
00:00:41,021 --> 00:00:42,610
breaths,
that's brain or whatever,

13
00:00:42,910 --> 00:00:44,560
as long as you survive,
that's all that matters.

14
00:00:44,561 --> 00:00:45,500
So,
uh,

15
00:00:45,580 --> 00:00:49,270
this is why we're so easily deceived by
illusions and magic tricks and things

16
00:00:49,271 --> 00:00:50,104
like that,
that,

17
00:00:50,310 --> 00:00:52,270
you know,
our brains aren't really wired to,

18
00:00:52,400 --> 00:00:55,660
to represent reality as it really is.
Whatever that means.

19
00:00:55,661 --> 00:00:56,860
And,
um,

20
00:00:57,400 --> 00:00:59,710
yes,
I can say that without human

21
00:00:59,711 --> 00:01:03,310
consciousness,
the iconic reality that we inhabit would

22
00:01:03,311 --> 00:01:04,150
not exist.

23
00:01:04,480 --> 00:01:05,590
Right.
Okay.

24
00:01:06,810 --> 00:01:08,510
Did that slightly is that,
um,

25
00:01:08,980 --> 00:01:10,870
is I told them so let's say,
uh,

26
00:01:10,871 --> 00:01:12,610
you know,
what's it like to be a dolphin?

27
00:01:13,060 --> 00:01:13,960
I don't know.
Okay.

28
00:01:13,961 --> 00:01:16,200
So some kind of echolocation system.
So,

29
00:01:16,510 --> 00:01:17,740
and his point is,
well there,

30
00:01:17,830 --> 00:01:19,180
you know,
the sharks are dangerous,

31
00:01:19,181 --> 00:01:20,120
so,
uh,

32
00:01:20,170 --> 00:01:21,880
his,
they avoid charts.

33
00:01:21,881 --> 00:01:25,960
But the question is what does a shark
look like in Dalton's brain versus what

34
00:01:26,140 --> 00:01:29,620
it looks like in our brain?
It's probably quite different and I

35
00:01:29,621 --> 00:01:32,110
really have no idea what a shark looks
like two adults,

36
00:01:32,590 --> 00:01:34,720
but I do know this,
they're really our shirts and they

37
00:01:34,721 --> 00:01:38,200
really have sharp things on one end and
a tail on the other and they're eating

38
00:01:38,201 --> 00:01:41,360
machines and you should avoid them.
So yeah,

39
00:01:42,180 --> 00:01:46,020
that's actually a weakness of the icon
claim I would say,

40
00:01:46,021 --> 00:01:49,680
because it looks to me like here,
here's the twist on it.

41
00:01:51,150 --> 00:01:56,150
What we see in our conscious experience
are functional icons,

42
00:01:57,510 --> 00:02:01,410
but they're also low resolution
representations of the things that are

43
00:02:01,411 --> 00:02:03,260
actually there.
And,

44
00:02:03,261 --> 00:02:07,920
and I don't think that computer icons
are low resolution representations of

45
00:02:07,921 --> 00:02:10,890
the things that are there.
They're just functional icons.

46
00:02:11,220 --> 00:02:12,840
Now.
I might be wrong about that because,

47
00:02:14,090 --> 00:02:17,850
because it's hard to to conjure up that
analysis on the fly.

48
00:02:17,880 --> 00:02:19,440
But the Trashcan,
for example,

49
00:02:19,441 --> 00:02:23,520
on your desktop is actually,
it's actually a low resolution

50
00:02:23,521 --> 00:02:27,120
representations of an actual trashcan,
not a computer trashcan,

51
00:02:27,150 --> 00:02:29,290
even though it functions the same way.
Right.

52
00:02:29,760 --> 00:02:31,980
So,
so I think I liked the icon idea,

53
00:02:31,981 --> 00:02:35,940
but I think it misses some element of
the actual relationship between the

54
00:02:35,941 --> 00:02:39,420
perception and the reality.
We definitely see in low resolution,

55
00:02:39,720 --> 00:02:43,560
which is why we can stand animated
pictures like say the Simpsons,

56
00:02:43,830 --> 00:02:44,610
you know,
where,

57
00:02:44,610 --> 00:02:47,130
where everything is one extremely low
resolution,

58
00:02:47,131 --> 00:02:49,530
but that makes no functional difference
to us whatsoever.

59
00:02:50,190 --> 00:02:52,830
And we definitely see and hear in low
resolution,

60
00:02:52,890 --> 00:02:57,300
but the resolution like I think what we
see or something like instead of icons,

61
00:02:57,320 --> 00:02:59,620
they're more like thumbnails that are
functional.

62
00:03:00,460 --> 00:03:02,030
That's a good analogy.
Well,

63
00:03:02,050 --> 00:03:07,050
I like it because the thumbnail,
the thumbnail actually is an unbiased

64
00:03:07,391 --> 00:03:11,420
sampling of the actual object,
right?

65
00:03:11,770 --> 00:03:15,280
Because a photograph is relatively
unbiased sample of an object and you can

66
00:03:15,281 --> 00:03:16,960
compress it.
You can,

67
00:03:16,990 --> 00:03:19,180
you can,
you can until it,

68
00:03:20,200 --> 00:03:23,830
and what you're doing is blurring out
distinctions between,

69
00:03:24,940 --> 00:03:28,060
you're blurring out distinctions between
different aspects of it without,

70
00:03:28,630 --> 00:03:31,300
without losing the relationship between
the parts.

71
00:03:31,310 --> 00:03:32,260
It's something like that.

72
00:03:32,360 --> 00:03:33,193
Yeah,
that's right.

73
00:03:33,380 --> 00:03:35,270
Yeah.
In a way,

74
00:03:35,271 --> 00:03:38,120
much of science operates at a
metaphorical level,

75
00:03:38,150 --> 00:03:39,200
you know,
that string,

76
00:03:39,201 --> 00:03:40,970
you know,
have you ever seen a string theory

77
00:03:40,971 --> 00:03:44,240
documentary that didn't have violins
featured in it?

78
00:03:44,241 --> 00:03:45,074
You know,
that,

79
00:03:45,080 --> 00:03:47,240
that the computer,
the brain is like a computer.

80
00:03:47,241 --> 00:03:52,241
It's like a dual processor.
Yeah.

81
00:03:53,240 --> 00:03:56,740
Because we have to talk and we have to
transfer the literal meaning of

82
00:03:56,741 --> 00:03:59,740
metaphors to move something from here to
there in the original Greek.

83
00:04:00,070 --> 00:04:00,880
So,
you know,

84
00:04:00,880 --> 00:04:05,880
we tried to capture some idea that's
really hard to get that buy something

85
00:04:06,041 --> 00:04:07,450
we're very familiar with.
So.

