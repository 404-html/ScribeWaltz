1
00:00:11,030 --> 00:00:11,863
Thank you for coming.

2
00:00:12,460 --> 00:00:14,030
I appreciate it a lot.

3
00:00:14,290 --> 00:00:14,530
Uh,

4
00:00:14,530 --> 00:00:19,220
I wanted to get your opinion on
censorship they were seeing on the web.

5
00:00:19,480 --> 00:00:19,670
Uh,

6
00:00:19,670 --> 00:00:20,690
it's accelerating.

7
00:00:20,691 --> 00:00:22,160
You are a very notable example.

8
00:00:22,161 --> 00:00:25,220
You were locked out of your
Gmail and Indian Trump account.

9
00:00:25,310 --> 00:00:26,143
Pardon me?

10
00:00:26,890 --> 00:00:27,440
Oh yeah.

11
00:00:27,440 --> 00:00:30,170
Trump just got a deleted
by an air in person,

12
00:00:30,171 --> 00:00:30,381
you know,

13
00:00:30,381 --> 00:00:34,550
now they're saying that perhaps this
was just a contractor and you know,

14
00:00:34,551 --> 00:00:35,384
maybe,

15
00:00:35,450 --> 00:00:35,780
uh,

16
00:00:35,780 --> 00:00:40,370
someone from Twitter who's gone
in a very far left direction,

17
00:00:40,430 --> 00:00:40,521
uh,

18
00:00:40,521 --> 00:00:42,830
youtube has gone in a
very far left I erection.

19
00:00:43,180 --> 00:00:43,520
Uh,

20
00:00:43,520 --> 00:00:44,450
I'm just wondering,

21
00:00:44,451 --> 00:00:45,410
I've started a,

22
00:00:45,420 --> 00:00:47,720
an alternative to youtube called Pew Tube.

23
00:00:47,980 --> 00:00:48,320
Uh,

24
00:00:48,320 --> 00:00:49,460
what kind of,

25
00:00:49,550 --> 00:00:50,383
um,

26
00:00:50,580 --> 00:00:50,890
what,

27
00:00:50,890 --> 00:00:54,840
what do you see for possible solutions
and just your thoughts in general?

28
00:00:56,090 --> 00:00:56,800
Here's an,

29
00:00:56,800 --> 00:00:57,800
it's a crazy thought,

30
00:00:58,460 --> 00:01:00,050
but I'm going to tell it to you anyways.

31
00:01:00,330 --> 00:01:03,140
So I was just reading one
of Ray Kurzweil's books,

32
00:01:03,141 --> 00:01:03,891
I think it was called,

33
00:01:03,891 --> 00:01:04,790
how to make a mind.

34
00:01:04,850 --> 00:01:06,080
I really liked it actually.

35
00:01:07,940 --> 00:01:11,350
It helped me understand how the brain
compresses information because the world

36
00:01:11,351 --> 00:01:12,430
is really complicated day,

37
00:01:12,431 --> 00:01:16,070
so you have to make a low resolution
representation of it to live in it.

38
00:01:16,520 --> 00:01:19,880
And he actually explained to me in a way
that I hadn't really understood how the

39
00:01:19,881 --> 00:01:21,620
brain might do that neurologically.

40
00:01:21,621 --> 00:01:22,454
So that was cool.

41
00:01:22,580 --> 00:01:22,911
But you know,

42
00:01:22,911 --> 00:01:25,940
Kurt swells this guy who
thinks that he's a smart guy,

43
00:01:26,370 --> 00:01:28,560
a very smart guy and uh,

44
00:01:29,480 --> 00:01:34,480
he's invented a fair bit of high end
technical technological software and

45
00:01:34,941 --> 00:01:38,090
hardware and he's the guy that
thinks that we're heading towards the

46
00:01:38,091 --> 00:01:38,811
singularity.

47
00:01:38,811 --> 00:01:40,160
And so the singularity is,

48
00:01:40,390 --> 00:01:44,960
you know how processing speed doubles
every 18 months and like hard disk

49
00:01:44,961 --> 00:01:48,890
capacity every year and there's
a bunch of doublings going on,

50
00:01:49,190 --> 00:01:53,210
a huge number of them and
they accelerate exponentially.

51
00:01:53,210 --> 00:01:55,670
And so it's probably,

52
00:01:55,700 --> 00:01:57,260
we're probably three years away,

53
00:01:57,290 --> 00:02:01,780
maybe even less then from
building a computer that has,

54
00:02:01,781 --> 00:02:06,781
is the capacity to make
as many calculations as
reasonable estimates of the

55
00:02:07,191 --> 00:02:12,191
calculating capacity of the human brain
are currently set up 18 months away,

56
00:02:13,370 --> 00:02:14,061
two years away,

57
00:02:14,061 --> 00:02:14,960
something like that.

58
00:02:16,850 --> 00:02:18,980
And then we're 18 months
away from having one.

59
00:02:18,981 --> 00:02:21,680
That's twice that fast and then
18 months away from having one.

60
00:02:21,681 --> 00:02:22,850
That's twice as fast as that.

61
00:02:22,851 --> 00:02:27,851
So that's like say six years and then
we've got something that's eight times as

62
00:02:28,281 --> 00:02:29,540
smart as a human being.

63
00:02:30,080 --> 00:02:31,700
But there's a twist on that.

64
00:02:31,701 --> 00:02:33,140
And this is Kurt swells twist,

65
00:02:33,141 --> 00:02:36,080
which is as soon as you make a machine
smart enough to make the next machine

66
00:02:36,081 --> 00:02:37,160
that's smarter than it,

67
00:02:37,520 --> 00:02:42,520
which is sort of what we're doing because
computers are so fast that that will

68
00:02:42,621 --> 00:02:45,590
scale up to near infinite computing
power computing power almost

69
00:02:45,591 --> 00:02:46,640
instantaneously.

70
00:02:47,870 --> 00:02:48,703
Now,

71
00:02:49,020 --> 00:02:49,970
let me think.

72
00:02:50,060 --> 00:02:50,421
No,

73
00:02:50,421 --> 00:02:51,111
probably not.

74
00:02:51,111 --> 00:02:56,111
And Alan Gates partner has written
critiques of Kurtzweil and you know,

75
00:02:57,470 --> 00:02:59,860
you might think if something's
impossible it won't happen.

76
00:02:59,861 --> 00:03:00,910
Even if you don't know why.

77
00:03:00,911 --> 00:03:02,500
And there's reasons to,

78
00:03:02,610 --> 00:03:04,180
to not think that that will happen.

79
00:03:04,210 --> 00:03:09,210
But Kurzweil's trace back the doubling
of computing power way before the

80
00:03:09,581 --> 00:03:13,270
existence of the transistor and
it's been ridiculously stable,

81
00:03:13,330 --> 00:03:14,470
crazily stable.

82
00:03:14,680 --> 00:03:17,980
So God only knows what
we're coming up with here.

83
00:03:18,670 --> 00:03:19,120
You know,

84
00:03:19,120 --> 00:03:22,540
and you don't know what something of
infinite computing power might be like,

85
00:03:23,240 --> 00:03:25,070
like you seriously don't know.

86
00:03:25,990 --> 00:03:28,060
And there are serious people who are very,

87
00:03:28,061 --> 00:03:28,481
very,

88
00:03:28,481 --> 00:03:29,800
very worried about that.

89
00:03:29,801 --> 00:03:30,521
They're very worried,

90
00:03:30,521 --> 00:03:31,091
for example,

91
00:03:31,091 --> 00:03:34,930
that companies like Facebook and
Google will manage that first.

92
00:03:35,440 --> 00:03:35,861
And you know,

93
00:03:35,861 --> 00:03:40,861
those companies are already
making censorship AI bots
and that's not that smart.

94
00:03:41,260 --> 00:03:43,990
It's sort of like making really
fast robots that can shoot people.

95
00:03:43,990 --> 00:03:45,190
It's not that smart.

96
00:03:45,490 --> 00:03:47,290
And we're doing that to very rapidly.

97
00:03:47,291 --> 00:03:47,861
And you know,

98
00:03:47,861 --> 00:03:51,460
I know some guys who work in advanced
AI and you know how you look,

99
00:03:51,490 --> 00:03:55,030
you watch the term terminator movies and
you see the robots that miss when they

100
00:03:55,031 --> 00:03:55,864
shoot at you.

101
00:03:56,110 --> 00:04:00,100
Like they're not very bright because the
bright ones not only shoot at where you

102
00:04:00,101 --> 00:04:00,371
are,

103
00:04:00,371 --> 00:04:03,160
but they estimate where you're going to
be when you make your escape moves and

104
00:04:03,161 --> 00:04:08,161
they shoot their simultaneously and their
death rate is 100% and so there's no

105
00:04:10,090 --> 00:04:11,560
war against the robots.

106
00:04:11,561 --> 00:04:11,831
I mean,

107
00:04:11,831 --> 00:04:12,820
when those things get going,

108
00:04:12,821 --> 00:04:15,640
they're going to be so much faster than
us that will look like we're moving

109
00:04:15,641 --> 00:04:16,900
through molasses to them.

110
00:04:17,490 --> 00:04:18,323
So

111
00:04:22,160 --> 00:04:22,401
you know,

112
00:04:22,401 --> 00:04:25,790
so maybe what we're deciding now with
all of our individual decisions about

113
00:04:25,791 --> 00:04:28,550
censorship and the way that we're going
to construct the world and all that is

114
00:04:28,551 --> 00:04:31,910
exactly what kind of super intelligence
we're going to bring into being.

115
00:04:32,810 --> 00:04:35,960
And I would suggest that we try to bring
one in that's good and moral rather

116
00:04:35,961 --> 00:04:38,000
than one that's evil and demonic,

117
00:04:39,710 --> 00:04:40,543
right?

118
00:04:41,000 --> 00:04:44,540
So what can we do about that?

119
00:04:48,560 --> 00:04:49,310
The,

120
00:04:49,310 --> 00:04:50,450
there's only one answer to that.

121
00:04:50,451 --> 00:04:54,170
As far as I know that that
works is gay rock together.

122
00:04:55,640 --> 00:04:57,710
You're going to be the
person who's working in Ai,

123
00:04:58,010 --> 00:04:58,843
right?

124
00:04:59,180 --> 00:05:00,470
I know some of these people,

125
00:05:01,070 --> 00:05:04,250
they better be good people because they're
going to build whatever they're like

126
00:05:04,251 --> 00:05:05,210
into their machines.

127
00:05:06,110 --> 00:05:09,080
They better have their head screwed on
straight because they're going to get

128
00:05:09,081 --> 00:05:10,520
amplified like mad.

129
00:05:10,910 --> 00:05:14,570
And I don't like what's happening
with Google and Facebook and Youtube,

130
00:05:14,780 --> 00:05:18,410
they're building censorship bots
predicated on a certain kind of ideology,

131
00:05:18,430 --> 00:05:20,550
the kind of ideology
that we outlined today.

132
00:05:20,880 --> 00:05:22,590
It's a very bad idea.

133
00:05:27,810 --> 00:05:29,730
Hopefully good people will stop that.

134
00:05:30,870 --> 00:05:31,531
So then that,

135
00:05:31,531 --> 00:05:34,830
what that means is that your
moral obligation is to be good.

136
00:05:35,040 --> 00:05:38,940
And the way you do that is
first by stopping being bad.

137
00:05:39,270 --> 00:05:41,370
And everyone can do that a little bit.

138
00:05:41,910 --> 00:05:46,910
So I hope that's what
everyone does because the
consequences of not doing it are

139
00:05:47,521 --> 00:05:48,810
not going to be pleasant.

140
00:05:48,811 --> 00:05:49,644
They never are.

141
00:05:51,230 --> 00:05:51,560
Thank you.

