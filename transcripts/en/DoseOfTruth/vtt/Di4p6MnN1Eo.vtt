WEBVTT

1
00:00:11.030 --> 00:00:11.863
<v Speaker 1>Thank you for coming.</v>

2
00:00:12.460 --> 00:00:14.030
<v Speaker 1>I appreciate it a lot.</v>

3
00:00:14.290 --> 00:00:14.530
<v Speaker 1>Uh,</v>

4
00:00:14.530 --> 00:00:19.220
<v Speaker 1>I wanted to get your opinion on censorship they were seeing on the web.</v>

5
00:00:19.480 --> 00:00:19.670
<v Speaker 1>Uh,</v>

6
00:00:19.670 --> 00:00:20.690
<v Speaker 1>it's accelerating.</v>

7
00:00:20.691 --> 00:00:22.160
<v Speaker 1>You are a very notable example.</v>

8
00:00:22.161 --> 00:00:25.220
<v Speaker 1>You were locked out of your Gmail and Indian Trump account.</v>

9
00:00:25.310 --> 00:00:26.143
<v Speaker 1>Pardon me?</v>

10
00:00:26.890 --> 00:00:27.440
<v Speaker 1>Oh yeah.</v>

11
00:00:27.440 --> 00:00:30.170
<v Speaker 1>Trump just got a deleted by an air in person,</v>

12
00:00:30.171 --> 00:00:30.381
<v Speaker 1>you know,</v>

13
00:00:30.381 --> 00:00:34.550
<v Speaker 1>now they're saying that perhaps this was just a contractor and you know,</v>

14
00:00:34.551 --> 00:00:35.384
<v Speaker 1>maybe,</v>

15
00:00:35.450 --> 00:00:35.780
<v Speaker 1>uh,</v>

16
00:00:35.780 --> 00:00:40.370
<v Speaker 1>someone from Twitter who's gone in a very far left direction,</v>

17
00:00:40.430 --> 00:00:40.521
<v Speaker 1>uh,</v>

18
00:00:40.521 --> 00:00:42.830
<v Speaker 1>youtube has gone in a very far left I erection.</v>

19
00:00:43.180 --> 00:00:43.520
<v Speaker 1>Uh,</v>

20
00:00:43.520 --> 00:00:44.450
<v Speaker 1>I'm just wondering,</v>

21
00:00:44.451 --> 00:00:45.410
<v Speaker 1>I've started a,</v>

22
00:00:45.420 --> 00:00:47.720
<v Speaker 1>an alternative to youtube called Pew Tube.</v>

23
00:00:47.980 --> 00:00:48.320
<v Speaker 1>Uh,</v>

24
00:00:48.320 --> 00:00:49.460
<v Speaker 1>what kind of,</v>

25
00:00:49.550 --> 00:00:50.383
<v Speaker 1>um,</v>

26
00:00:50.580 --> 00:00:50.890
<v Speaker 1>what,</v>

27
00:00:50.890 --> 00:00:54.840
<v Speaker 1>what do you see for possible solutions and just your thoughts in general?</v>

28
00:00:56.090 --> 00:00:56.800
<v Speaker 2>Here's an,</v>

29
00:00:56.800 --> 00:00:57.800
<v Speaker 2>it's a crazy thought,</v>

30
00:00:58.460 --> 00:01:00.050
<v Speaker 2>but I'm going to tell it to you anyways.</v>

31
00:01:00.330 --> 00:01:03.140
<v Speaker 2>So I was just reading one of Ray Kurzweil's books,</v>

32
00:01:03.141 --> 00:01:03.891
<v Speaker 2>I think it was called,</v>

33
00:01:03.891 --> 00:01:04.790
<v Speaker 2>how to make a mind.</v>

34
00:01:04.850 --> 00:01:06.080
<v Speaker 2>I really liked it actually.</v>

35
00:01:07.940 --> 00:01:11.350
<v Speaker 2>It helped me understand how the brain compresses information because the world</v>

36
00:01:11.351 --> 00:01:12.430
<v Speaker 2>is really complicated day,</v>

37
00:01:12.431 --> 00:01:16.070
<v Speaker 2>so you have to make a low resolution representation of it to live in it.</v>

38
00:01:16.520 --> 00:01:19.880
<v Speaker 2>And he actually explained to me in a way that I hadn't really understood how the</v>

39
00:01:19.881 --> 00:01:21.620
<v Speaker 2>brain might do that neurologically.</v>

40
00:01:21.621 --> 00:01:22.454
<v Speaker 2>So that was cool.</v>

41
00:01:22.580 --> 00:01:22.911
<v Speaker 2>But you know,</v>

42
00:01:22.911 --> 00:01:25.940
<v Speaker 2>Kurt swells this guy who thinks that he's a smart guy,</v>

43
00:01:26.370 --> 00:01:28.560
<v Speaker 2>a very smart guy and uh,</v>

44
00:01:29.480 --> 00:01:34.480
<v Speaker 2>he's invented a fair bit of high end technical technological software and</v>

45
00:01:34.941 --> 00:01:38.090
<v Speaker 2>hardware and he's the guy that thinks that we're heading towards the</v>

46
00:01:38.091 --> 00:01:38.811
<v Speaker 2>singularity.</v>

47
00:01:38.811 --> 00:01:40.160
<v Speaker 2>And so the singularity is,</v>

48
00:01:40.390 --> 00:01:44.960
<v Speaker 2>you know how processing speed doubles every 18 months and like hard disk</v>

49
00:01:44.961 --> 00:01:48.890
<v Speaker 2>capacity every year and there's a bunch of doublings going on,</v>

50
00:01:49.190 --> 00:01:53.210
<v Speaker 2>a huge number of them and they accelerate exponentially.</v>

51
00:01:53.210 --> 00:01:55.670
<v Speaker 2>And so it's probably,</v>

52
00:01:55.700 --> 00:01:57.260
<v Speaker 2>we're probably three years away,</v>

53
00:01:57.290 --> 00:02:01.780
<v Speaker 2>maybe even less then from building a computer that has,</v>

54
00:02:01.781 --> 00:02:06.781
<v Speaker 2>is the capacity to make as many calculations as reasonable estimates of the</v>

55
00:02:07.191 --> 00:02:12.191
<v Speaker 2>calculating capacity of the human brain are currently set up 18 months away,</v>

56
00:02:13.370 --> 00:02:14.061
<v Speaker 2>two years away,</v>

57
00:02:14.061 --> 00:02:14.960
<v Speaker 2>something like that.</v>

58
00:02:16.850 --> 00:02:18.980
<v Speaker 2>And then we're 18 months away from having one.</v>

59
00:02:18.981 --> 00:02:21.680
<v Speaker 2>That's twice that fast and then 18 months away from having one.</v>

60
00:02:21.681 --> 00:02:22.850
<v Speaker 2>That's twice as fast as that.</v>

61
00:02:22.851 --> 00:02:27.851
<v Speaker 2>So that's like say six years and then we've got something that's eight times as</v>

62
00:02:28.281 --> 00:02:29.540
<v Speaker 2>smart as a human being.</v>

63
00:02:30.080 --> 00:02:31.700
<v Speaker 2>But there's a twist on that.</v>

64
00:02:31.701 --> 00:02:33.140
<v Speaker 2>And this is Kurt swells twist,</v>

65
00:02:33.141 --> 00:02:36.080
<v Speaker 2>which is as soon as you make a machine smart enough to make the next machine</v>

66
00:02:36.081 --> 00:02:37.160
<v Speaker 2>that's smarter than it,</v>

67
00:02:37.520 --> 00:02:42.520
<v Speaker 2>which is sort of what we're doing because computers are so fast that that will</v>

68
00:02:42.621 --> 00:02:45.590
<v Speaker 2>scale up to near infinite computing power computing power almost</v>

69
00:02:45.591 --> 00:02:46.640
<v Speaker 2>instantaneously.</v>

70
00:02:47.870 --> 00:02:48.703
<v Speaker 2>Now,</v>

71
00:02:49.020 --> 00:02:49.970
<v Speaker 2>let me think.</v>

72
00:02:50.060 --> 00:02:50.421
<v Speaker 2>No,</v>

73
00:02:50.421 --> 00:02:51.111
<v Speaker 2>probably not.</v>

74
00:02:51.111 --> 00:02:56.111
<v Speaker 2>And Alan Gates partner has written critiques of Kurtzweil and you know,</v>

75
00:02:57.470 --> 00:02:59.860
<v Speaker 2>you might think if something's impossible it won't happen.</v>

76
00:02:59.861 --> 00:03:00.910
<v Speaker 2>Even if you don't know why.</v>

77
00:03:00.911 --> 00:03:02.500
<v Speaker 2>And there's reasons to,</v>

78
00:03:02.610 --> 00:03:04.180
<v Speaker 2>to not think that that will happen.</v>

79
00:03:04.210 --> 00:03:09.210
<v Speaker 2>But Kurzweil's trace back the doubling of computing power way before the</v>

80
00:03:09.581 --> 00:03:13.270
<v Speaker 2>existence of the transistor and it's been ridiculously stable,</v>

81
00:03:13.330 --> 00:03:14.470
<v Speaker 2>crazily stable.</v>

82
00:03:14.680 --> 00:03:17.980
<v Speaker 2>So God only knows what we're coming up with here.</v>

83
00:03:18.670 --> 00:03:19.120
<v Speaker 2>You know,</v>

84
00:03:19.120 --> 00:03:22.540
<v Speaker 2>and you don't know what something of infinite computing power might be like,</v>

85
00:03:23.240 --> 00:03:25.070
<v Speaker 2>like you seriously don't know.</v>

86
00:03:25.990 --> 00:03:28.060
<v Speaker 2>And there are serious people who are very,</v>

87
00:03:28.061 --> 00:03:28.481
<v Speaker 2>very,</v>

88
00:03:28.481 --> 00:03:29.800
<v Speaker 2>very worried about that.</v>

89
00:03:29.801 --> 00:03:30.521
<v Speaker 2>They're very worried,</v>

90
00:03:30.521 --> 00:03:31.091
<v Speaker 2>for example,</v>

91
00:03:31.091 --> 00:03:34.930
<v Speaker 2>that companies like Facebook and Google will manage that first.</v>

92
00:03:35.440 --> 00:03:35.861
<v Speaker 2>And you know,</v>

93
00:03:35.861 --> 00:03:40.861
<v Speaker 2>those companies are already making censorship AI bots and that's not that smart.</v>

94
00:03:41.260 --> 00:03:43.990
<v Speaker 2>It's sort of like making really fast robots that can shoot people.</v>

95
00:03:43.990 --> 00:03:45.190
<v Speaker 2>It's not that smart.</v>

96
00:03:45.490 --> 00:03:47.290
<v Speaker 2>And we're doing that to very rapidly.</v>

97
00:03:47.291 --> 00:03:47.861
<v Speaker 2>And you know,</v>

98
00:03:47.861 --> 00:03:51.460
<v Speaker 2>I know some guys who work in advanced AI and you know how you look,</v>

99
00:03:51.490 --> 00:03:55.030
<v Speaker 2>you watch the term terminator movies and you see the robots that miss when they</v>

100
00:03:55.031 --> 00:03:55.864
<v Speaker 2>shoot at you.</v>

101
00:03:56.110 --> 00:04:00.100
<v Speaker 2>Like they're not very bright because the bright ones not only shoot at where you</v>

102
00:04:00.101 --> 00:04:00.371
<v Speaker 2>are,</v>

103
00:04:00.371 --> 00:04:03.160
<v Speaker 2>but they estimate where you're going to be when you make your escape moves and</v>

104
00:04:03.161 --> 00:04:08.161
<v Speaker 2>they shoot their simultaneously and their death rate is 100% and so there's no</v>

105
00:04:10.090 --> 00:04:11.560
<v Speaker 2>war against the robots.</v>

106
00:04:11.561 --> 00:04:11.831
<v Speaker 2>I mean,</v>

107
00:04:11.831 --> 00:04:12.820
<v Speaker 2>when those things get going,</v>

108
00:04:12.821 --> 00:04:15.640
<v Speaker 2>they're going to be so much faster than us that will look like we're moving</v>

109
00:04:15.641 --> 00:04:16.900
<v Speaker 2>through molasses to them.</v>

110
00:04:17.490 --> 00:04:18.323
<v Speaker 3>So</v>

111
00:04:22.160 --> 00:04:22.401
<v Speaker 2>you know,</v>

112
00:04:22.401 --> 00:04:25.790
<v Speaker 2>so maybe what we're deciding now with all of our individual decisions about</v>

113
00:04:25.791 --> 00:04:28.550
<v Speaker 2>censorship and the way that we're going to construct the world and all that is</v>

114
00:04:28.551 --> 00:04:31.910
<v Speaker 2>exactly what kind of super intelligence we're going to bring into being.</v>

115
00:04:32.810 --> 00:04:35.960
<v Speaker 2>And I would suggest that we try to bring one in that's good and moral rather</v>

116
00:04:35.961 --> 00:04:38.000
<v Speaker 2>than one that's evil and demonic,</v>

117
00:04:39.710 --> 00:04:40.543
<v Speaker 2>right?</v>

118
00:04:41.000 --> 00:04:44.540
<v Speaker 2>So what can we do about that?</v>

119
00:04:48.560 --> 00:04:49.310
<v Speaker 3>The,</v>

120
00:04:49.310 --> 00:04:50.450
<v Speaker 2>there's only one answer to that.</v>

121
00:04:50.451 --> 00:04:54.170
<v Speaker 2>As far as I know that that works is gay rock together.</v>

122
00:04:55.640 --> 00:04:57.710
<v Speaker 2>You're going to be the person who's working in Ai,</v>

123
00:04:58.010 --> 00:04:58.843
<v Speaker 2>right?</v>

124
00:04:59.180 --> 00:05:00.470
<v Speaker 2>I know some of these people,</v>

125
00:05:01.070 --> 00:05:04.250
<v Speaker 2>they better be good people because they're going to build whatever they're like</v>

126
00:05:04.251 --> 00:05:05.210
<v Speaker 2>into their machines.</v>

127
00:05:06.110 --> 00:05:09.080
<v Speaker 2>They better have their head screwed on straight because they're going to get</v>

128
00:05:09.081 --> 00:05:10.520
<v Speaker 2>amplified like mad.</v>

129
00:05:10.910 --> 00:05:14.570
<v Speaker 2>And I don't like what's happening with Google and Facebook and Youtube,</v>

130
00:05:14.780 --> 00:05:18.410
<v Speaker 2>they're building censorship bots predicated on a certain kind of ideology,</v>

131
00:05:18.430 --> 00:05:20.550
<v Speaker 2>the kind of ideology that we outlined today.</v>

132
00:05:20.880 --> 00:05:22.590
<v Speaker 2>It's a very bad idea.</v>

133
00:05:27.810 --> 00:05:29.730
<v Speaker 2>Hopefully good people will stop that.</v>

134
00:05:30.870 --> 00:05:31.531
<v Speaker 2>So then that,</v>

135
00:05:31.531 --> 00:05:34.830
<v Speaker 2>what that means is that your moral obligation is to be good.</v>

136
00:05:35.040 --> 00:05:38.940
<v Speaker 2>And the way you do that is first by stopping being bad.</v>

137
00:05:39.270 --> 00:05:41.370
<v Speaker 2>And everyone can do that a little bit.</v>

138
00:05:41.910 --> 00:05:46.910
<v Speaker 2>So I hope that's what everyone does because the consequences of not doing it are</v>

139
00:05:47.521 --> 00:05:48.810
<v Speaker 2>not going to be pleasant.</v>

140
00:05:48.811 --> 00:05:49.644
<v Speaker 2>They never are.</v>

141
00:05:51.230 --> 00:05:51.560
<v Speaker 3>Thank you.</v>

