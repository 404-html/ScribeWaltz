WEBVTT

1
00:00:06.780 --> 00:00:07.261
And you know,

2
00:00:07.261 --> 00:00:11.280
one of the things that Carl Young pointed out was that he had this idea that

3
00:00:12.120 --> 00:00:12.953
when science,

4
00:00:13.580 --> 00:00:14.600
that that elk,

5
00:00:14.640 --> 00:00:14.870
you know,

6
00:00:14.870 --> 00:00:17.040
alchemy is the root of science in some sense,

7
00:00:17.041 --> 00:00:20.340
it's the dream like sub straight out of which science emerged.

8
00:00:20.341 --> 00:00:20.880
And,

9
00:00:20.880 --> 00:00:24.930
but alchemy was kind of a weird ad mixture of religious thinking and scientific

10
00:00:24.931 --> 00:00:28.620
thinking because those two things hadn't been differentiated back when there

11
00:00:28.621 --> 00:00:29.371
were alchemists.

12
00:00:29.371 --> 00:00:31.320
And you don't believe that what had happened in,

13
00:00:31.380 --> 00:00:31.921
in Europe,

14
00:00:31.921 --> 00:00:36.921
at least first was that the scientific and developing me blew up and expanded at

15
00:00:36.961 --> 00:00:38.040
an exponential rate.

16
00:00:38.041 --> 00:00:41.640
And that led to this advanced technological civilization that we have.

17
00:00:41.641 --> 00:00:44.910
But that the moral dimension that was embedded in the religious symbolism,

18
00:00:45.020 --> 00:00:46.140
it didn't develop at all.

19
00:00:46.141 --> 00:00:51.030
And so we're in this unstable situation where we're far more technologically

20
00:00:51.031 --> 00:00:52.860
proficient than we are wise.

21
00:00:53.100 --> 00:00:57.090
And that that's actually a big problem because obviously the more powerful the

22
00:00:57.091 --> 00:00:58.170
tools you generate,

23
00:00:58.500 --> 00:01:03.500
the more intelligent ethically you better be or things are going to really,

24
00:01:03.570 --> 00:01:04.190
uh,

25
00:01:04.190 --> 00:01:04.550
are,

26
00:01:04.550 --> 00:01:06.600
are going to go to hell in a hand basket very,

27
00:01:06.900 --> 00:01:07.710
very rapidly.

28
00:01:07.710 --> 00:01:08.310
You know,

29
00:01:08.310 --> 00:01:09.360
and I had this thought,

30
00:01:09.361 --> 00:01:11.100
I think I shared it a little bit last night,

31
00:01:11.101 --> 00:01:11.660
that,

32
00:01:11.660 --> 00:01:11.880
you know,

33
00:01:11.880 --> 00:01:13.080
in the next five years,

34
00:01:13.081 --> 00:01:13.651
six years,

35
00:01:13.651 --> 00:01:16.560
we're going to develop pretty viciously intelligent AI systems.

36
00:01:16.561 --> 00:01:17.700
And that's already happening.

37
00:01:17.701 --> 00:01:17.941
You know,

38
00:01:17.941 --> 00:01:18.450
I mean,

39
00:01:18.450 --> 00:01:20.880
they're monitoring youtube and they're monitoring Facebook and they're

40
00:01:20.881 --> 00:01:23.310
monitoring Google and they're trying to make ethical decisions,

41
00:01:23.311 --> 00:01:24.330
these AI systems.

42
00:01:24.331 --> 00:01:25.560
And the problem is,

43
00:01:25.561 --> 00:01:29.430
is that the ethical presuppositions of the programmers are being embedded into

44
00:01:29.431 --> 00:01:30.990
the infrastructure of the net.

45
00:01:31.350 --> 00:01:34.740
And that's a hell of a thing to think because it means that for better or worse,

46
00:01:34.950 --> 00:01:38.490
we're building automated intelligence is that reflect our own morality.

47
00:01:39.030 --> 00:01:41.820
And we better be very careful about what our morality is if we're going to

48
00:01:41.821 --> 00:01:42.654
automate it.

49
00:01:42.840 --> 00:01:45.240
Because automated systems are incredibly powerful.

50
00:01:45.241 --> 00:01:46.074
So,

51
00:01:46.170 --> 00:01:46.710
so that's,

52
00:01:46.710 --> 00:01:47.400
that's kind of,

53
00:01:47.400 --> 00:01:48.350
that's where we're at,

54
00:01:48.360 --> 00:01:53.070
at least to some degree in terms of the new technological transformations with

55
00:01:53.470 --> 00:01:55.590
in communication technology,

56
00:01:55.620 --> 00:01:55.831
you know,

57
00:01:55.831 --> 00:01:57.060
it puts each of us at the,

58
00:01:57.390 --> 00:02:01.350
at the center of a wide web of connections and makes the consequences of our

59
00:02:01.351 --> 00:02:04.440
moral decisions much more immediately manifest to each of us.

