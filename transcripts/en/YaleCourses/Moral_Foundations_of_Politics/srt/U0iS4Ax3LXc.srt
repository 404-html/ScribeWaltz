1
00:00:01,380 --> 00:00:06,380
So today we're going to start talking
about classical utilitarianism and we're

2
00:00:07,861 --> 00:00:11,670
going to use as our,
as our point of departure,

3
00:00:12,060 --> 00:00:14,060
Jeremy Bentham,
uh,

4
00:00:14,280 --> 00:00:19,280
who lived between 1748 and 1832 and came
up with the canonical statement of the

5
00:00:23,761 --> 00:00:27,000
doctrine of utilitarianism.
It's a doc train,

6
00:00:27,001 --> 00:00:32,001
which is still very much alive and kicking
and the contemporary west despite all

7
00:00:33,001 --> 00:00:37,020
of its problems. And we'll have
things to say about why that is.

8
00:00:38,280 --> 00:00:43,280
But I wanted to make a couple of prefatory
remarks first about Bentham himself

9
00:00:44,940 --> 00:00:49,740
that there is some think is in,
in a western tradition,

10
00:00:49,870 --> 00:00:54,870
I guess in any tradition who have a
particular characteristic that um,

11
00:00:56,280 --> 00:00:59,880
Bentham certainly has. And I think
of the folks we're going to read,

12
00:00:59,881 --> 00:01:04,420
Karl Marx had and Robert Nozick had.
And the,

13
00:01:04,421 --> 00:01:09,421
the thing I'm thinking of here is they
are the kind of person who takes one idea

14
00:01:10,801 --> 00:01:14,010
to the most extreme possible formulation.

15
00:01:14,640 --> 00:01:17,460
They asked themselves the question,

16
00:01:17,820 --> 00:01:22,820
how would the world be if this idea that
I have is that is the only important

17
00:01:22,981 --> 00:01:23,814
idea?

18
00:01:24,060 --> 00:01:28,860
And they take it to its logical extreme
to an excessive kind of formulation.

19
00:01:29,280 --> 00:01:34,280
And they will go places with their
idea that nobody else will go.

20
00:01:35,460 --> 00:01:40,380
Um, and so that makes them
a little bit crazy. Um,

21
00:01:40,440 --> 00:01:44,900
you know, their motto I
call obsessive leak, uh,

22
00:01:44,940 --> 00:01:48,030
consumed with their idea.
In the case of Bentham,

23
00:01:48,031 --> 00:01:52,820
it's the idea of utility, which we're
going to unpack a little bit, uh,

24
00:01:53,160 --> 00:01:55,020
in the moment.
Um,

25
00:01:55,410 --> 00:02:00,410
but what's always interesting about
people like this is that they play out an

26
00:02:03,631 --> 00:02:08,631
idea to its logical extreme and that
exhibits both its strengths and its

27
00:02:08,671 --> 00:02:13,671
limitations because it's just because
they're willing to go when others will not

28
00:02:14,881 --> 00:02:16,650
go think the unthinkable,

29
00:02:16,770 --> 00:02:21,770
think politically very incorrect
things for their time in pursuit of,

30
00:02:22,380 --> 00:02:25,770
of really pushing this idea
to the absolute health.

31
00:02:26,220 --> 00:02:29,700
And so Bentham is a kind of thinker who,
um,

32
00:02:30,100 --> 00:02:34,440
I suspect at the end of the day,
nobody will be fully convinced by,

33
00:02:34,680 --> 00:02:38,310
but he's very useful.
He's a very useful, um,

34
00:02:38,340 --> 00:02:40,530
diagnostician of what,

35
00:02:40,531 --> 00:02:45,330
what it is about utilitarianism that's
going to be appealing to you and where

36
00:02:45,390 --> 00:02:49,140
eventually you're going to want to put
some limits on it just because he goes

37
00:02:49,141 --> 00:02:53,220
beyond the limits. And so
you can see what happens. Um,

38
00:02:53,490 --> 00:02:58,410
if you push it all the way to the hilt.
Secondly,

39
00:02:58,411 --> 00:03:03,330
I wanted to just say that
Bentham is important, um, as a,

40
00:03:03,520 --> 00:03:08,520
as a fountain of more than Utilitarian
Nissam but also of modern conceptions of

41
00:03:10,811 --> 00:03:15,520
value more generally considered.
Um,

42
00:03:15,700 --> 00:03:20,210
you'll see that he is one of the,
there were the,

43
00:03:20,211 --> 00:03:25,090
there were rumblings of the kinds of
things he had to say about value in the

44
00:03:25,091 --> 00:03:27,940
17th century.
Hobbes for example,

45
00:03:27,941 --> 00:03:32,941
who I mentioned last time criticized
Aristotle for not seeing that what is good

46
00:03:33,401 --> 00:03:37,630
for some people may not be
good for other people. Um,

47
00:03:38,080 --> 00:03:42,640
and Bentham builds on that idea.
Uh,

48
00:03:43,330 --> 00:03:44,290
you'll see Bentham,

49
00:03:44,291 --> 00:03:48,910
we'll start to link the good to
what it is that people desire.

50
00:03:49,480 --> 00:03:54,160
Um, it, there were also rumblings
of Bentham's methods in,

51
00:03:54,161 --> 00:03:59,161
in particularly his aspirations to found
politics on scientific principles in

52
00:04:00,041 --> 00:04:03,800
the 17th century.
We already saw last time the hubs,

53
00:04:03,801 --> 00:04:08,020
Yana and Lockian creationist
theories of science,

54
00:04:08,230 --> 00:04:10,510
but they were really transitional figures.

55
00:04:10,690 --> 00:04:14,800
They also gave a theological
justifications for their arguments.

56
00:04:14,801 --> 00:04:19,360
As I explained at some length in
lock, uh, in the context of lock.

57
00:04:19,361 --> 00:04:21,700
I didn't have time to do it with Hobbs,

58
00:04:22,060 --> 00:04:27,060
but many of you will know that if you
read the the second two thirds of Hobbs'

59
00:04:29,291 --> 00:04:30,041
Leviathan,

60
00:04:30,041 --> 00:04:34,690
it's almost all about interpretation
of the scriptures showing that uh,

61
00:04:34,750 --> 00:04:39,700
his scientifically derived principles
are also consistent with the Bible.

62
00:04:40,630 --> 00:04:44,700
Bentham sheds all of this for Bentham.
Um,

63
00:04:44,890 --> 00:04:48,040
he's not interested in
appeals to tradition.

64
00:04:48,040 --> 00:04:50,560
He's not interested in
appeals to religion.

65
00:04:50,561 --> 00:04:54,370
He's not interested in
appeals to natural law.

66
00:04:54,610 --> 00:04:59,610
He dismisses the natural law tradition
as dangerous nonsense nonsense on stilts.

67
00:05:01,480 --> 00:05:06,480
He's only interested in a scientific set
of principles for organizing politics.

68
00:05:12,310 --> 00:05:14,440
And one of the nice things about Bentham,

69
00:05:14,441 --> 00:05:18,070
at least from your point of view is um,

70
00:05:18,100 --> 00:05:23,100
and we'll see that utilitarianism
values efficiency in in many ways.

71
00:05:24,010 --> 00:05:28,930
But one of the interesting things or
the helpful things about Bell Bentham is

72
00:05:28,931 --> 00:05:33,931
that he reduces his whole doctrine to
a single paragraph and he puts that

73
00:05:34,181 --> 00:05:39,181
paragraph right at the front
of his introduction to the
principles of morals and

74
00:05:39,251 --> 00:05:41,050
legislation.
So yeah,

75
00:05:41,051 --> 00:05:45,820
you have the kind of cliff notes
formulation of Bentham's argument.

76
00:05:46,270 --> 00:05:50,020
He says that nature has placed mankind

77
00:05:52,720 --> 00:05:57,680
under the governance of two
sovereign masters pain and pleasure.

78
00:05:58,460 --> 00:06:02,690
It is for them alone to point out what
we ought to do as well as to determine

79
00:06:02,691 --> 00:06:03,830
what we shall do.

80
00:06:03,830 --> 00:06:08,830
So this is going to be about describing
human behavior and about what ought to

81
00:06:09,561 --> 00:06:11,900
be the case,
right?

82
00:06:12,800 --> 00:06:17,800
What we shall do to determine what we
ought to do is to point out why we ought

83
00:06:17,931 --> 00:06:21,950
to do as well as to determine
what we shall do. On the one hand,

84
00:06:21,951 --> 00:06:24,140
the standard of right
and wrong on the other,

85
00:06:24,141 --> 00:06:28,370
that chain of causes and effects
are fastened to their throne.

86
00:06:28,371 --> 00:06:30,740
That's the throne of pain and pleasure.

87
00:06:31,400 --> 00:06:34,070
They govern us in all
we do and all we say,

88
00:06:34,071 --> 00:06:39,020
and in all we think every effort we
can make to throw off our subjection.

89
00:06:39,050 --> 00:06:44,050
That's how subjection to pleasure
seeking and pain avoiding will serve,

90
00:06:44,181 --> 00:06:46,310
but to demonstrate and confirm it,

91
00:06:46,311 --> 00:06:49,310
to confirm that objection in words,

92
00:06:49,311 --> 00:06:52,190
a man may pretend to
have juror their empire.

93
00:06:52,191 --> 00:06:55,880
That's the empire of pain and pleasure,
but in reality,

94
00:06:55,881 --> 00:06:58,760
he will remain subject to it.
All the while.

95
00:06:59,210 --> 00:07:04,210
The principle of utility recognizes
this subjection and assumes it for the

96
00:07:04,851 --> 00:07:06,710
foundation of that system,

97
00:07:06,920 --> 00:07:11,920
the object of which is to repair the
fabric of Felicity by the hands of reason

98
00:07:12,921 --> 00:07:15,050
and law system,

99
00:07:15,051 --> 00:07:20,051
which it systems which attempt to question
it deal in sounds instead of senses

100
00:07:20,840 --> 00:07:25,840
in caprice instead of reason and
in darkness instead of light.

101
00:07:27,170 --> 00:07:31,790
That is in a Nacelle Bentham's theory.

102
00:07:35,860 --> 00:07:36,693
Okay,

103
00:07:37,230 --> 00:07:40,950
very bold,
unequivocal statement.

104
00:07:40,980 --> 00:07:45,780
He's saying if you want to understand
human beings in a causal explanatory

105
00:07:45,781 --> 00:07:46,614
sense,

106
00:07:46,920 --> 00:07:51,920
all you have to know about them is that
they're going to seek pleasure and avoid

107
00:07:52,711 --> 00:07:53,544
pain.

108
00:07:53,970 --> 00:07:58,290
And if you want to think about what
ought to happen in the design of

109
00:07:58,291 --> 00:08:00,630
institutions they need,

110
00:08:00,631 --> 00:08:05,631
they should be designed around
that fact to accommodate that fact.

111
00:08:06,060 --> 00:08:09,930
And he is going to develop a system
of laws, a system of government,

112
00:08:10,380 --> 00:08:11,213
uh,

113
00:08:11,580 --> 00:08:16,580
that that takes into account in his belt
upon this assumption about human nature

114
00:08:18,751 --> 00:08:23,520
as he would have called it human
psychology, as we would call it today.

115
00:08:24,930 --> 00:08:29,930
Now I'm going to make five points about
Bentham system to give you some sense of

116
00:08:32,491 --> 00:08:34,260
the full dimensions of it.

117
00:08:34,290 --> 00:08:39,290
Before we start this disecting it and
subjecting it to critical scrutiny.

118
00:08:39,790 --> 00:08:44,790
Want to make sure that we understand
exactly what his system is.

119
00:08:48,590 --> 00:08:53,590
I want to first of all notice that it is
what I'm going to call a comprehensive

120
00:08:54,570 --> 00:08:56,640
and account.

121
00:08:59,340 --> 00:09:04,200
I call it a comprehensive
and deterministic account
in that it's an account of

122
00:09:04,350 --> 00:09:06,300
all human behavior.

123
00:09:07,670 --> 00:09:12,670
He wants to say everything you do is
ultimately determined by pleasure seeking

124
00:09:13,490 --> 00:09:15,050
and pain,
avoiding

125
00:09:17,840 --> 00:09:20,320
how plausible.
Who thinks that's plausible?

126
00:09:23,570 --> 00:09:27,750
Hands up plausible
implausible.

127
00:09:28,710 --> 00:09:30,390
Okay.
Give us an example.

128
00:09:30,391 --> 00:09:34,200
Somebody of something that is
not pleasure seeking or pain,

129
00:09:34,201 --> 00:09:36,520
avoiding anybody.

130
00:09:39,190 --> 00:09:42,950
Something that is not the result
of pleasure seeking pain. Avoiding

131
00:09:46,000 --> 00:09:50,890
while you put your hands up, you must've
had some thoughts. Yeah. Oh, okay. Yeah.

132
00:09:50,891 --> 00:09:52,600
You, yeah. You take the

133
00:09:54,420 --> 00:09:56,960
fire to rescue people.
Pardon?

134
00:09:57,040 --> 00:10:02,040
Running into a fire to rescue people
running into a fire to rescue people.

135
00:10:03,280 --> 00:10:05,710
Okay.
You run into a fire to rescue people.

136
00:10:06,010 --> 00:10:10,630
What do you think Bentham would say
about that example? Yeah. Over here, sir,

137
00:10:12,310 --> 00:10:14,440
the pleasure is actually
saving the people,

138
00:10:14,441 --> 00:10:16,990
so there is like this
benefit that you get from it.

139
00:10:17,770 --> 00:10:19,480
Pleasure you get from having saved,

140
00:10:19,481 --> 00:10:23,260
the people must outweigh the pain
of the file or you wouldn't do it.

141
00:10:24,490 --> 00:10:25,780
Any other example?

142
00:10:29,310 --> 00:10:33,620
Nobody's got an exam son. Nobody
can think of an example. No.

143
00:10:34,610 --> 00:10:35,600
Yes,
yes sir.

144
00:10:38,630 --> 00:10:42,320
Um, will there may be some, for
example, saving one's child,

145
00:10:42,380 --> 00:10:45,590
maybe purely instinctual
rather than driven by pain

146
00:10:46,010 --> 00:10:49,400
or say sacrificing your
life to save your child.

147
00:10:49,490 --> 00:10:53,150
Let's say they put in an extreme case. Why
would rent them say about that? I mean,

148
00:10:53,151 --> 00:10:57,510
this seems like a genuinely altruistic,
um,

149
00:10:58,490 --> 00:11:01,700
action. Somebody lays down
their life for their own child.

150
00:11:01,701 --> 00:11:06,560
How can that be pleasure seeking and pain
avoiding? What would Bentham say? Yeah,

151
00:11:08,910 --> 00:11:09,950
that way we need to,

152
00:11:11,660 --> 00:11:15,240
could relieve the pain of having lost a
child like outweighs whatever pleasures.

153
00:11:16,310 --> 00:11:19,400
Yeah, I think that is what he would
say. Think of the counterfactual.

154
00:11:19,970 --> 00:11:24,350
How could I live with myself for the
rest of my life? If I didn't do it?

155
00:11:24,650 --> 00:11:29,630
The pain would be too great.
And Bentham considers cases like that.

156
00:11:29,690 --> 00:11:33,140
There's this sort of thing.
Apparently altruistic acts,

157
00:11:33,320 --> 00:11:38,320
same ultimately always reduceable to
the pleasure cane calculus one example.

158
00:11:39,321 --> 00:11:43,940
He considers his people acting from
religious motivations. And he says,

159
00:11:43,941 --> 00:11:48,740
Ha, just read the Bible. You know,

160
00:11:48,741 --> 00:11:51,440
look at the descriptions
of Heaven and hell.

161
00:11:51,770 --> 00:11:54,490
Isn't that a made to order?

162
00:11:54,491 --> 00:11:59,020
Pleasure seeking and pain avoidance.
Hell is described as, you know,

163
00:11:59,350 --> 00:12:01,420
the fires of Hell,
perpetual pain.

164
00:12:01,870 --> 00:12:06,870
So the people who constructed religious
doctrines clearly had an understanding

165
00:12:07,781 --> 00:12:11,200
of human nature or they wouldn't have
described hell in a way that they

166
00:12:11,201 --> 00:12:15,640
described it and heaven in
the way that they describe it.

167
00:12:17,170 --> 00:12:19,090
So the f,

168
00:12:19,130 --> 00:12:23,680
the first thing he wants to say is
that this is a completely comprehensive

169
00:12:23,770 --> 00:12:26,560
explanation of human behavior.

170
00:12:28,300 --> 00:12:28,960
Okay.

171
00:12:28,960 --> 00:12:29,720
Can,

172
00:12:29,720 --> 00:12:34,430
can anybody think of any example that
couldn't be read described as fitting this

173
00:12:34,431 --> 00:12:36,620
pleasure? Pain Calculus. Yeah.

174
00:12:42,140 --> 00:12:45,420
Thinks that if life is all about pain
and pleasure and we will be willing to

175
00:12:45,421 --> 00:12:49,290
replace our load with one with own,
I'm in one only with pleasure, right?

176
00:12:49,740 --> 00:12:52,260
But we wouldn't,
we've all your life by yourself,

177
00:12:52,410 --> 00:12:55,500
there's something indescribable
quality to it. I mean,

178
00:12:55,770 --> 00:12:59,790
love Pfister some of old
experiences rather than just
playing pain and pleasure,

179
00:12:59,791 --> 00:13:00,624
sir.

180
00:13:00,830 --> 00:13:05,120
So you think that there is more
complexity to human motivation?

181
00:13:05,620 --> 00:13:06,453
Um,

182
00:13:06,680 --> 00:13:11,630
that's just not expressible as a
reduceable to pain and pleasure.

183
00:13:11,870 --> 00:13:13,460
I think, you know, that was a,

184
00:13:13,510 --> 00:13:18,510
that's a very sophisticated and common
critique that's been made of Bentham.

185
00:13:19,491 --> 00:13:24,491
If you go and read and indeed if you read
the obituary of him that was written,

186
00:13:26,000 --> 00:13:26,833
um,

187
00:13:28,190 --> 00:13:28,510
okay.

188
00:13:28,510 --> 00:13:30,490
By Coleridge,
I think it was,

189
00:13:30,620 --> 00:13:35,200
it makes exactly this point that
there was this sophistication to human

190
00:13:35,201 --> 00:13:40,000
motivation that isn't captured in
this idea. I think that the truth has,

191
00:13:40,001 --> 00:13:43,600
Bentham would have acknowledged some of
that, but he would have said, you know,

192
00:13:43,601 --> 00:13:46,210
at the end of the day it's
unimportant because the pleasure,

193
00:13:46,211 --> 00:13:49,690
pain calculus overrides
when the chips are down.

194
00:13:49,960 --> 00:13:54,490
If we're going to think about what it is
that's going to motivate people, it's,

195
00:13:54,580 --> 00:13:58,690
it's, um, pleasure seeking
and pain avoidance.

196
00:13:59,290 --> 00:14:00,123
Okay.

197
00:14:00,410 --> 00:14:05,410
A second thing that you should
notice about this doctrine is that,

198
00:14:05,561 --> 00:14:10,561
I'm going to call it a
naturalistic doctrine in some ways.

199
00:14:11,530 --> 00:14:15,910
It's astounding that writing almost
half a century before Darwin,

200
00:14:16,210 --> 00:14:20,110
Darwin was born in 1809
and lift till 1882,

201
00:14:20,440 --> 00:14:24,610
so writing almost half a
century before Darwin. Um,

202
00:14:25,330 --> 00:14:29,380
Bentham grounds his
principle in the apparent,

203
00:14:29,381 --> 00:14:31,900
in the imperatives for human survival.

204
00:14:33,030 --> 00:14:33,450
Okay.

205
00:14:33,450 --> 00:14:38,450
He thinks that the pleasure pain
principle has a natural biological basis,

206
00:14:39,600 --> 00:14:40,980
although there are religious,

207
00:14:40,981 --> 00:14:44,580
moral and political sources and
sanctions of pain and pleasure.

208
00:14:44,760 --> 00:14:48,780
These are all secondary to the
physical sources for Bentham.

209
00:14:49,980 --> 00:14:53,780
The physical, he refers at one point
as the groundwork of the political,

210
00:14:53,781 --> 00:14:57,260
moral and religious.
It is included in each of them.

211
00:14:58,990 --> 00:15:03,920
At another point he says, we are bound
by the principle of utility as, uh,

212
00:15:04,030 --> 00:15:07,750
the natural constitution
of the human frame. Often,

213
00:15:07,751 --> 00:15:10,560
unconsciously and office
when often went out,

214
00:15:10,561 --> 00:15:15,561
conscious explanations for our actions
are inconsistent with the principle of

215
00:15:15,731 --> 00:15:17,590
utility.
I'll come back to that point.

216
00:15:19,270 --> 00:15:21,940
If we didn't abide by
the principle of utility,

217
00:15:21,941 --> 00:15:26,860
he says in his little essay on the
economic psychology of man, he says,

218
00:15:27,070 --> 00:15:27,521
quote,

219
00:15:27,521 --> 00:15:32,380
the human species could not continue
in existence and that in a few months,

220
00:15:32,381 --> 00:15:33,310
not to say week,

221
00:15:33,311 --> 00:15:37,780
so days we would would be all that
would be needed for its annihilation.

222
00:15:38,530 --> 00:15:39,400
In other words,

223
00:15:39,580 --> 00:15:44,580
the principle of utility expresses our
objective interests as living creatures.

224
00:15:49,060 --> 00:15:49,520
Okay.

225
00:15:49,520 --> 00:15:54,520
A third point that I'm going to make
about Bentham's doctrine is that it's what

226
00:15:54,741 --> 00:15:58,430
I will call egoistic,
but not subjectivist.

227
00:15:58,760 --> 00:16:03,760
Now that's a lot of babble terminology
and let me explain what it means.

228
00:16:04,550 --> 00:16:09,550
The reason I'm using those two words
together is that they don't normally go

229
00:16:09,741 --> 00:16:10,574
together.

230
00:16:10,730 --> 00:16:15,290
That is to say egoistic views
are usually subjectivists.

231
00:16:15,320 --> 00:16:19,310
So I'm, I'm pointing out that
they're not. And by Egoistic,

232
00:16:19,370 --> 00:16:21,740
I mean it is,
uh,

233
00:16:22,670 --> 00:16:26,690
just like in wall economics assumptions,
assumption of self interest,

234
00:16:26,900 --> 00:16:28,850
people are self interested,

235
00:16:28,870 --> 00:16:32,300
seek as off to pleasure
and self interested.

236
00:16:32,540 --> 00:16:37,540
Avoid is a pain in exactly the way you
learn about them in an economics 101 a

237
00:16:38,350 --> 00:16:41,270
textbook. Um, and we'll,

238
00:16:41,271 --> 00:16:46,040
we'll have occasion to examine
that self interested premise,

239
00:16:46,270 --> 00:16:48,950
uh,
in some depth later,

240
00:16:50,060 --> 00:16:55,060
but it's not a subjectivist doctrine in
that Bentham wants to say this is true.

241
00:16:55,400 --> 00:16:59,690
Regardless of what we ourselves
say about our preferences,

242
00:17:00,650 --> 00:17:03,480
it's not dependent upon a,

243
00:17:03,500 --> 00:17:07,340
your acknowledging it's
truth for it's being true.

244
00:17:08,860 --> 00:17:09,693
Okay.

245
00:17:09,790 --> 00:17:14,790
So you might think you're motivated by
altruism or love of your child or your

246
00:17:16,691 --> 00:17:21,280
religious faith. Bentham says
you're just muddled and diluted.

247
00:17:21,310 --> 00:17:23,830
You don't understand,
uh,

248
00:17:23,831 --> 00:17:28,690
your subjective understanding is not in
accord with the science of the matter.

249
00:17:29,980 --> 00:17:34,820
Um,
at one of,

250
00:17:34,850 --> 00:17:39,850
at one point he says it
is with the anatomy of the
human mind as it is with the

251
00:17:41,091 --> 00:17:44,060
anatomy and physiology of the human body.

252
00:17:44,600 --> 00:17:49,600
The rare case is not of a man's being
unconverted hunt but of his being

253
00:17:50,820 --> 00:17:53,940
conversant with it.
So just as you know,

254
00:17:53,941 --> 00:17:57,720
if you have a pain in your side and you
don't know if it's your liver or your

255
00:17:57,721 --> 00:18:02,460
spleen or your long, uh, you know,

256
00:18:02,461 --> 00:18:06,060
the rare case as you get it right,
um,

257
00:18:06,090 --> 00:18:09,330
he wants to do is exactly the
same with your motivation.

258
00:18:09,750 --> 00:18:13,710
The fact that you don't understand or
wouldn't agree with or don't acknowledge

259
00:18:14,220 --> 00:18:17,340
what's motivating you so
much the worst for you.

260
00:18:17,341 --> 00:18:22,341
You just have an inaccurate or incomplete
understanding of your motivation.

261
00:18:25,770 --> 00:18:30,720
You just wrong. Okay. So it's in
that sense picks up on the idea.

262
00:18:30,721 --> 00:18:35,640
This is an objectivist account. People
are, it's, it is objectively the case.

263
00:18:35,641 --> 00:18:38,610
Whatever people think about it,
whatever people say about it,

264
00:18:38,760 --> 00:18:42,570
it is objective may be the case
that they behave self interest.

265
00:18:42,571 --> 00:18:47,400
The Lea and the pursuit of
pleasure and the avoidance of pain.

266
00:18:49,140 --> 00:18:54,140
Fourth Bentham's is a radically
consequentialist doctrine.

267
00:18:56,100 --> 00:19:00,840
Anyone know what that might mean?
Anyone want to tell us what is our,

268
00:19:01,060 --> 00:19:06,060
what do you think I might mean by
calling it a consequentialist doctrine?

269
00:19:10,940 --> 00:19:11,930
Yeah.
Yes ma'am.

270
00:19:20,300 --> 00:19:20,720
Um,

271
00:19:20,720 --> 00:19:25,720
that in being motivated by pleasure
and pain were concerned with the

272
00:19:26,001 --> 00:19:30,290
consequences of our actions. If we
know something's going to be painful,

273
00:19:30,291 --> 00:19:33,800
we'll avoid it. And if it's going to
be pleasurable, will move towards it.

274
00:19:34,220 --> 00:19:36,920
Yeah. But as I said at the
beginning of the electric,

275
00:19:37,460 --> 00:19:39,650
Bentham takes everything to the extreme.

276
00:19:39,680 --> 00:19:42,560
So if we are concerned with
the consequences of our,

277
00:19:42,561 --> 00:19:45,290
of the action and nothing else,

278
00:19:45,560 --> 00:19:48,930
it's an extreme
consequentialist doctorate.

279
00:19:49,430 --> 00:19:54,350
He's not interested in our intentions,
right?

280
00:19:54,530 --> 00:19:58,190
The road to hell is paved with
good intentions for Bentham. No,

281
00:19:58,360 --> 00:20:03,120
doesn't matter what people intend.
It matters what happens, right?

282
00:20:03,460 --> 00:20:06,650
It's a radically
consequentialist doctrine.

283
00:20:06,890 --> 00:20:11,890
We will see that there's an alternative
tradition of thinking about ethics and

284
00:20:13,401 --> 00:20:16,970
politics that is deeply
rooted in human intentions.

285
00:20:17,420 --> 00:20:19,880
When we come to,
um,

286
00:20:20,840 --> 00:20:25,160
to read Robert Nozick and John
Rawls and people who draw on cons.

287
00:20:25,240 --> 00:20:27,680
Emmanuel [inaudible] ethics.
But,

288
00:20:27,710 --> 00:20:30,780
and that's what gets to
you to give you all of the,

289
00:20:31,090 --> 00:20:35,870
the jargon a upfront that is what
will be called de ontological.

290
00:20:36,410 --> 00:20:37,243
Um,

291
00:20:37,640 --> 00:20:42,410
sometimes contrast that with teleological
consequentialist is a kind of

292
00:20:42,500 --> 00:20:47,260
teleological doctrine.
What is teleological main?

293
00:20:47,261 --> 00:20:48,094
Anybody?

294
00:20:53,910 --> 00:20:54,743
Yeah,
at the back

295
00:20:58,260 --> 00:20:59,670
teleological system.

296
00:21:07,250 --> 00:21:08,083
Well,

297
00:21:08,180 --> 00:21:12,830
given that a telos in Greek is the end,

298
00:21:13,010 --> 00:21:13,670
the end,

299
00:21:13,670 --> 00:21:16,340
that's purpose. The
consequence. Exactly right.

300
00:21:16,730 --> 00:21:20,960
So consequential as doctrines I
like are teleological doctrines.

301
00:21:21,230 --> 00:21:25,670
They're all about the consequences, the
purpose, the end, the goals, the results.

302
00:21:26,060 --> 00:21:31,060
Whereas what we will talk about later
when we get to day ontological systems are

303
00:21:31,071 --> 00:21:35,300
the antithesis of that.
They are focus on intentions,

304
00:21:35,301 --> 00:21:39,170
on processes, on procedures,
on high, on how you do things,

305
00:21:39,171 --> 00:21:41,480
not on where you get to.
Okay.

306
00:21:41,481 --> 00:21:45,590
So Bentham is a radical
consequentialist and

307
00:21:47,240 --> 00:21:51,250
you judge a doctrine simply
by looking you judge a,

308
00:21:51,251 --> 00:21:53,360
a possible policy and action.

309
00:21:53,361 --> 00:21:58,361
Anything you thinking of doing or not
doing simply by virtue of what effect that

310
00:21:58,671 --> 00:22:02,450
is likely to have and nothing else,
nothing else matters.

311
00:22:07,240 --> 00:22:08,073
Finally,

312
00:22:10,410 --> 00:22:13,980
Bentham. Thanks everything
he's doing this quantifiable.

313
00:22:15,010 --> 00:22:15,460
Yeah,

314
00:22:15,460 --> 00:22:15,670
no,

315
00:22:15,670 --> 00:22:20,670
I gave you just a sliver to read from
his introduction to the principles of

316
00:22:21,131 --> 00:22:22,630
morals and legislation,

317
00:22:22,870 --> 00:22:27,070
just so that you could get a sense
of how this guy's mind actually work.

318
00:22:27,400 --> 00:22:32,400
He really thought it was the case that
he could develop a kind of science of

319
00:22:33,461 --> 00:22:38,461
utilitarianism where he would figure
out exactly how many you tells you.

320
00:22:38,681 --> 00:22:43,160
We might call them, we might call
them standard international utiles.

321
00:22:43,190 --> 00:22:48,130
The Siu would attached
to a of pleasure or pain,

322
00:22:48,131 --> 00:22:49,690
any policy or action,

323
00:22:50,230 --> 00:22:54,700
and that eventually you could figure
out exactly what all of the optimal

324
00:22:54,701 --> 00:22:58,780
policies were for the
Organization of society.

325
00:22:59,170 --> 00:23:03,130
He thought about utility.
He thought it had really four dimensions.

326
00:23:03,490 --> 00:23:07,870
How intense is it? Duration,
how long does it last?

327
00:23:08,290 --> 00:23:10,600
It's certainty or uncertainty.

328
00:23:10,601 --> 00:23:14,380
That is probability that
the result will occur.

329
00:23:14,920 --> 00:23:19,510
And it's what we, what he called
propinquity or remoteness, which we would,

330
00:23:19,680 --> 00:23:24,580
modern economist would say, we discount
pleasure into their future. Um,

331
00:23:24,790 --> 00:23:27,100
so our dollar is more that if you'll say,

332
00:23:27,220 --> 00:23:30,280
I'll give you a dollar today or
I'll give you a dollar tomorrow,

333
00:23:30,370 --> 00:23:34,420
you'll get more utility from the
dollar that you get today. Okay.

334
00:23:34,930 --> 00:23:39,850
So he, he thought that these were all
quantifiable dimensions of utilities,

335
00:23:39,880 --> 00:23:43,360
a little unsure about the intensity,
um,

336
00:23:43,390 --> 00:23:46,310
but he's sure that everything
else can be quantified.

337
00:23:46,550 --> 00:23:49,280
And he sat about quantifying,
uh,

338
00:23:49,281 --> 00:23:53,360
he set about trying to figure
out a system of legislation,

339
00:23:53,600 --> 00:23:55,970
not only for his society,
by the way,

340
00:23:56,270 --> 00:24:00,710
he started writing constitutions for
other countries and when he ran off to

341
00:24:00,711 --> 00:24:04,040
Poland and various places and,
and said, hey, look, here's my,

342
00:24:04,250 --> 00:24:07,040
my utilitarian constitution
for your country.

343
00:24:07,400 --> 00:24:12,400
And he was very disappointed when people
didn't rush off and implemented a right

344
00:24:13,491 --> 00:24:15,360
away. So, you know, he,

345
00:24:15,361 --> 00:24:18,930
he truly believed that you
could come up with a, uh,

346
00:24:18,950 --> 00:24:23,950
scientifically demonstrably demonstrable
system of organizing society based on

347
00:24:26,090 --> 00:24:30,710
the quantify tie ball
character of utilitarianism.

348
00:24:39,050 --> 00:24:44,050
So one further feature of
this quantifiable character
of utilitarianism is that

349
00:24:46,311 --> 00:24:50,540
he thought we could make
comparisons across people.

350
00:24:50,630 --> 00:24:53,390
We could do the math across people.

351
00:24:54,560 --> 00:24:56,150
We could add up

352
00:24:58,910 --> 00:25:03,910
how much utility one person gets from a
possible action and how much you tell or

353
00:25:04,151 --> 00:25:09,151
the or this utility another person gets
and redistribute in order to do what he

354
00:25:11,151 --> 00:25:12,380
thought we should do,

355
00:25:12,590 --> 00:25:17,590
which was to maximize the greatest
happiness of the greatest number.

356
00:25:22,240 --> 00:25:24,100
He's a complete consequentialist.

357
00:25:24,101 --> 00:25:29,101
So we would do whatever we have to do to
maximize the greatest happiness of the

358
00:25:30,491 --> 00:25:34,150
greatest number.
So for example,

359
00:25:35,690 --> 00:25:37,220
I happen to know that Denise,

360
00:25:37,221 --> 00:25:41,300
who's sitting over there is it has
got a great capacity for utility.

361
00:25:41,301 --> 00:25:45,920
She's easily pleased. She,
uh, you know, if you give her,

362
00:25:45,950 --> 00:25:50,860
if you give her a book, she'll be just
the lighting. But Antonio over the eyes,

363
00:25:50,870 --> 00:25:54,470
kind of grumpy guy, you know, if
you give him a book is say, well,

364
00:25:54,650 --> 00:25:59,300
why didn't you give me two bucks?
You know, one measly buck. You know,

365
00:25:59,660 --> 00:26:04,340
so if I have a choice between giving
this book to Denise or giving the book to

366
00:26:04,520 --> 00:26:05,240
Antony,

367
00:26:05,240 --> 00:26:09,020
I'm going to give the book to Denise
because she's going to get more utility

368
00:26:09,410 --> 00:26:13,310
than Anthony's going to get a,
from having this buck.

369
00:26:13,610 --> 00:26:18,020
And we don't really care who has the
utility from a social perspective,

370
00:26:18,260 --> 00:26:18,980
we want a great,

371
00:26:18,980 --> 00:26:23,980
maximize the greatest happiness
of the greatest number.

372
00:26:24,920 --> 00:26:25,753
Okay.

373
00:26:25,970 --> 00:26:30,020
But then what we might discover is
that layer Neda over there has an even

374
00:26:30,021 --> 00:26:34,430
greater capacity for utility.
He is just a utility monster.

375
00:26:34,940 --> 00:26:35,773
Um,

376
00:26:36,230 --> 00:26:41,230
he's got such a capacity for happiness
that any little thing that most of us

377
00:26:41,991 --> 00:26:45,960
would think, it's neither here nor there.
I was really going to make him happy.

378
00:26:46,230 --> 00:26:50,980
Well then we should give everything
to him, right? So it's, it's,

379
00:26:51,300 --> 00:26:56,300
it's a doctrine that's completely
uninterested in the distributor side of

380
00:26:56,791 --> 00:26:59,580
utilitarianism except
in an instrumental way.

381
00:27:00,000 --> 00:27:03,680
We'll come back to that on a next Monday,
uh,

382
00:27:03,720 --> 00:27:08,720
or you want to do is maximize the greatest
happiness of the greatest number in

383
00:27:11,341 --> 00:27:14,100
society,
the total amount of happiness.

384
00:27:15,430 --> 00:27:16,263
Okay.

385
00:27:16,320 --> 00:27:21,320
Now here's a further feature of Bentham's
doctrine and I think it follows from

386
00:27:22,201 --> 00:27:25,680
the consequentialism that
you should at least notice,

387
00:27:26,040 --> 00:27:30,780
cause I think it will bears
on our thinking about,

388
00:27:30,781 --> 00:27:34,870
for instance, the
Eichmann problem. Um, and

389
00:27:36,760 --> 00:27:40,360
this, this is actually taken
from Robert Nozick book,

390
00:27:40,361 --> 00:27:44,260
who you're going to read later in
the semester in his critique of

391
00:27:44,290 --> 00:27:47,020
utilitarianism.
He says,

392
00:27:47,021 --> 00:27:51,880
let's consider the following
thought experiment. Suppose,

393
00:27:52,450 --> 00:27:56,510
um,
you were cut,

394
00:27:56,660 --> 00:28:01,660
your brain was connected to electrodes
to buy electrodes to a computer and the

395
00:28:04,221 --> 00:28:08,930
computer was programmed to make
you have whatever experiences,

396
00:28:09,860 --> 00:28:13,430
give you pleasure and not to have
any experiences that give you pain.

397
00:28:14,330 --> 00:28:17,540
And you were to him. So you would,
you would in fact be unconscious.

398
00:28:17,541 --> 00:28:21,350
I think actually in nozicks example,
floating in a Vat,

399
00:28:22,220 --> 00:28:23,090
I'm conscious,

400
00:28:23,120 --> 00:28:27,440
but you would believe you were doing
whatever it is that gives you the greatest

401
00:28:27,560 --> 00:28:30,410
pleasure.
And the question now is a cas is,

402
00:28:30,570 --> 00:28:32,900
would you want to be
connected to the machine?

403
00:28:34,340 --> 00:28:38,500
Who would want to be connected
to the machine? Okay.

404
00:28:38,501 --> 00:28:42,640
We only have one, two, three, four,
five, six, seven, eight, nine, 10,

405
00:28:42,641 --> 00:28:43,600
15.

406
00:28:43,660 --> 00:28:48,660
I see about 15 candidates for Nozick
splasher machine who would not want to be

407
00:28:49,031 --> 00:28:51,850
connected to this machine.
Okay.

408
00:28:51,851 --> 00:28:56,800
We Ha we have a probably two
thirds of it of you who's not sure.

409
00:28:57,670 --> 00:29:02,440
Okay. Some are not sure who, those
who wouldn't want to be connected.

410
00:29:02,441 --> 00:29:06,250
Why not? I mean, this is great,
isn't it? You have to work anymore.

411
00:29:06,251 --> 00:29:09,940
You don't have to do assignments. You
don't have to show up to class. You just,

412
00:29:10,030 --> 00:29:13,840
you know, for the rest of your
life. Maybe your program too,

413
00:29:14,650 --> 00:29:19,330
to have the experiences that give
you the most pleasure in life.

414
00:29:19,480 --> 00:29:24,340
What could be better than that? Why
don't you want to do it? Yeah. Over here.

415
00:29:25,170 --> 00:29:26,003
Yeah.

416
00:29:30,610 --> 00:29:34,780
I think the point of life is to have
like a complexity of experiences and

417
00:29:34,781 --> 00:29:39,220
without experiencing pain at some point,
pleasure wouldn't be a sweet.

418
00:29:39,590 --> 00:29:42,910
Okay.
The point of life is to hold onto the mic.

419
00:29:42,911 --> 00:29:46,900
I just want to put the follow this alone,
the, the point of life is to have some,

420
00:29:47,680 --> 00:29:50,770
some contrast effects. You
know, Richard Nixon said,

421
00:29:50,771 --> 00:29:53,440
only if you've been in the deepest value,

422
00:29:53,441 --> 00:29:58,441
can you appreciate the joy of being on
the highest mountain as he was being run

423
00:29:58,481 --> 00:30:03,350
out of the White House in 1974.
Uh,

424
00:30:03,490 --> 00:30:05,110
well you could say,
okay,

425
00:30:05,140 --> 00:30:09,230
well in that case we'll
program the machine accordingly
so you'll have, you know,

426
00:30:09,340 --> 00:30:14,340
you'll have certain painful experiences
in order to maximize the neck of

427
00:30:15,521 --> 00:30:19,720
pleasure and pain. So we, you know,
uh, you know, every, I don't know,

428
00:30:20,410 --> 00:30:20,681
you know,

429
00:30:20,681 --> 00:30:24,580
every fifth minute you'll have
some unpleasant experience
just so that you don't

430
00:30:24,581 --> 00:30:28,990
forget how pleasant the
pleasant experiences, you
know, we can, we can do that.

431
00:30:29,180 --> 00:30:29,610
All right,

432
00:30:29,610 --> 00:30:33,810
well then you wouldn't be like having
freewill and experiencing the various,

433
00:30:34,250 --> 00:30:37,220
okay, so that's different, right?
It's not the contrast. It's,

434
00:30:37,221 --> 00:30:40,770
it's not the banality of
pleasure. A, if you like, it's,

435
00:30:40,850 --> 00:30:44,860
it's that the lack of freewill
or autonomy, but coding,

436
00:30:44,870 --> 00:30:49,790
we program it to make you thank, you were
acting freely even though you weren't,

437
00:30:50,150 --> 00:30:54,060
I think. Um, what about
that you think, you know,

438
00:30:54,110 --> 00:30:57,320
maybe some people say that's
true of us all, you know,

439
00:30:57,510 --> 00:31:00,530
the parent is this idea we have free,
well,

440
00:31:00,531 --> 00:31:04,880
it's a lot of bunk were all really
basically just acting out certain impulses

441
00:31:04,881 --> 00:31:08,180
and instincts. But we
believe we have free. Well,

442
00:31:08,181 --> 00:31:12,890
so you could be made to believe that
you're making choices even if in fact you

443
00:31:12,891 --> 00:31:13,680
aren't,

444
00:31:13,680 --> 00:31:18,390
well wouldn't the like free will be as
much a component of like the natural

445
00:31:18,391 --> 00:31:23,220
physical nature of man? Well
now you will his plan. Okay,

446
00:31:23,221 --> 00:31:24,420
I'm adding more to,

447
00:31:24,810 --> 00:31:28,350
okay, so that would be a different
theory then Bentham's theory,

448
00:31:28,680 --> 00:31:32,640
but you can see where you can
see where this is going. Right.

449
00:31:32,641 --> 00:31:35,940
That I think there were some
people in the room, if we,

450
00:31:35,941 --> 00:31:38,610
if we had time to pursue
this conversation,

451
00:31:38,820 --> 00:31:43,290
there are some people in the room who no
matter what you did to the programming

452
00:31:43,291 --> 00:31:44,940
and the experience machine,

453
00:31:45,180 --> 00:31:49,740
they wouldn't like it and they wouldn't
like it for two principle reasons.

454
00:31:49,741 --> 00:31:52,440
I think one has just been articulated,

455
00:31:52,710 --> 00:31:57,710
which is that somehow this seems like
an abdication of your own autonomy.

456
00:31:59,190 --> 00:32:01,860
Um,
and you know,

457
00:32:01,890 --> 00:32:04,440
as when you think back
to the Eifman problem,

458
00:32:04,680 --> 00:32:09,680
one of the things that troubled people
was his abdication of his autonomy is

459
00:32:10,771 --> 00:32:14,310
given his giving up.
If he's free well to say yes or no,

460
00:32:14,311 --> 00:32:15,630
I think this is right or wrong.

461
00:32:15,631 --> 00:32:19,590
I'm going to do it on the basis
of my own autonomous judgment.

462
00:32:20,190 --> 00:32:25,190
The second thing I think that people
would worry about is who's operating the

463
00:32:25,291 --> 00:32:26,124
machine,

464
00:32:27,930 --> 00:32:31,170
who's operating a machine is a kind of uh,

465
00:32:31,440 --> 00:32:34,920
how do you know that once they've
got you floating in that Vat,

466
00:32:36,060 --> 00:32:39,000
what you wanted to have
done well in fact happen.

467
00:32:39,020 --> 00:32:41,720
And so there's a basic problem of um,

468
00:32:41,750 --> 00:32:46,100
agency and accountability
that makes people nervous.

469
00:32:46,460 --> 00:32:51,460
But let's just put those things
to one side for the moment and

470
00:32:54,130 --> 00:32:54,390
okay,

471
00:32:54,390 --> 00:32:58,830
focus on the rest of the
exposition of Bentham's doctrine.

472
00:32:58,831 --> 00:33:01,530
We're going to come back to all
of these issues. I promise you,

473
00:33:01,531 --> 00:33:04,470
I just want to get them
everything out on the table.

474
00:33:05,370 --> 00:33:10,050
What he says is that the
role of government is

475
00:33:12,670 --> 00:33:14,710
a measure of government.
It's a kind of,

476
00:33:15,790 --> 00:33:19,510
which is about a particular kind of
action performed by a particular person or

477
00:33:19,511 --> 00:33:24,250
persons may be said to be comfortable
too or dictated by the principle of

478
00:33:24,251 --> 00:33:28,960
utility in like Ma when in life man.
And the tendency,

479
00:33:28,961 --> 00:33:33,961
which it has to augment the happiness of
the community is greater than any which

480
00:33:34,241 --> 00:33:37,540
it has to diminish it.
So again,

481
00:33:37,541 --> 00:33:42,541
the bumper sticker version of that
for Bentham is maximize the greatest

482
00:33:42,971 --> 00:33:45,550
happiness of the greatest number.

483
00:33:45,970 --> 00:33:50,970
And for those of you who like thinking
diagrammatically and as I noted at the,

484
00:33:51,850 --> 00:33:54,400
in my opening lecture,
not everybody does,

485
00:33:54,730 --> 00:33:57,400
but if we imagine a two person society,

486
00:33:57,640 --> 00:34:02,380
so Aa has this much utility,
that's the status quo, right? Hey,

487
00:34:02,381 --> 00:34:07,381
has this much utility B has that much
utility and let's say there's some outer

488
00:34:08,411 --> 00:34:10,870
limit of possible utility,
which,

489
00:34:10,871 --> 00:34:15,460
which we will call the possibility
frontier. Bentham would say any,

490
00:34:15,461 --> 00:34:17,290
if you draw that line there,

491
00:34:17,291 --> 00:34:21,870
anything that puts us in this Sha,

492
00:34:22,660 --> 00:34:24,970
whatever it is, cloudy zone, yeah.

493
00:34:25,630 --> 00:34:30,630
Would be a net increase in the total
amount of utility in the society.

494
00:34:31,420 --> 00:34:33,700
Pretty straight forward plan.
Right?

495
00:34:33,701 --> 00:34:37,690
So we went from there to that
both would have more utility,

496
00:34:37,691 --> 00:34:40,180
but if we went from there,
say to that,

497
00:34:40,670 --> 00:34:44,740
as utility would have gone up and bees
would have gone down, but we don't care.

498
00:34:45,100 --> 00:34:47,050
Right? Because, um,

499
00:34:47,170 --> 00:34:50,800
the total amount of utility
in this society has gone up.

500
00:34:50,980 --> 00:34:55,980
What we wouldn't want to do is come
anywhere into this area because then

501
00:34:56,380 --> 00:34:57,213
utility,

502
00:34:57,220 --> 00:35:02,220
the total amount of utility in
the society would have decreased.

503
00:35:07,280 --> 00:35:09,800
Okay?
So that's basically the story.

504
00:35:11,740 --> 00:35:13,210
Now you might say,
well,

505
00:35:15,860 --> 00:35:18,410
why do you need government at all?

506
00:35:19,280 --> 00:35:19,820
Okay,

507
00:35:19,820 --> 00:35:20,990
if this is the story,

508
00:35:21,740 --> 00:35:25,570
everybody is whatever they think,

509
00:35:25,571 --> 00:35:28,480
whatever they say,
whatever they understand,

510
00:35:28,690 --> 00:35:33,690
everybody is a mindless pleasure seeker
and pain avoider or perhaps mindful

511
00:35:34,931 --> 00:35:38,820
pleasure, secret pain and avoid it,
but they have no control over that.

512
00:35:39,000 --> 00:35:43,710
They're going to just do
what they have to do. Um,

513
00:35:44,030 --> 00:35:49,030
why create a government with the principle
that it should maximize utility in

514
00:35:50,011 --> 00:35:54,690
the society? It seems like an odd
thing to do. Why would you do that?

515
00:35:57,110 --> 00:35:58,130
Anyone?
Yeah,

516
00:36:05,850 --> 00:36:07,980
so just looking at that last graph,
right?

517
00:36:08,010 --> 00:36:10,590
If each person tried to
maximize their utility,

518
00:36:10,950 --> 00:36:15,300
then deed both want to be on the
opposite, you know, corners of each other.

519
00:36:15,690 --> 00:36:19,650
So then you would get chaos. When you
extrapolate that to a larger group,

520
00:36:19,651 --> 00:36:22,830
you need something to kind of
manage everybody's pleasure.

521
00:36:22,980 --> 00:36:27,980
So people won't voluntarily do
things that maximize one another's,

522
00:36:28,280 --> 00:36:31,520
maximize the total social utility,
right?

523
00:36:31,521 --> 00:36:36,521
If taking something from a and giving
it to be would increase speeds utility

524
00:36:36,770 --> 00:36:39,470
more than it would diminish A's utility,

525
00:36:39,680 --> 00:36:43,270
why is not going to go
for that voluntarily be,

526
00:36:43,271 --> 00:36:47,990
might go and take it. Um, but he may
or may not be strong enough to take it.

527
00:36:48,230 --> 00:36:49,130
We don't know.

528
00:36:49,580 --> 00:36:54,580
So that's a very shrewd observation
in response to that diagram.

529
00:36:55,220 --> 00:37:00,220
And it's actually gets to
more sophisticated questions
about redistribution and

530
00:37:00,801 --> 00:37:04,730
utilitarianism that I'm going to
take up on Monday. But there's,

531
00:37:04,731 --> 00:37:07,790
I think before we get to those questions,

532
00:37:09,680 --> 00:37:14,030
there's a more fundamental level at which

533
00:37:15,730 --> 00:37:16,660
Bentham,
thanks.

534
00:37:16,661 --> 00:37:21,661
Utilitarianism creates the need for
government and that is that there's a

535
00:37:22,961 --> 00:37:27,961
disconnect between what's individually
optimal and what's socially optimal.

536
00:37:29,470 --> 00:37:32,980
Even before we get to the
redistributive questions,

537
00:37:33,220 --> 00:37:38,220
we might call it the market
failure theory of government where,

538
00:37:42,110 --> 00:37:42,750
okay,

539
00:37:42,750 --> 00:37:47,750
other 18th century thinkers had taken
the view that when you know Adam Smith's

540
00:37:50,011 --> 00:37:51,750
famous invisible hand,

541
00:37:52,200 --> 00:37:57,030
everybody acting selfishly leads
to a collectively optimal results.

542
00:37:57,960 --> 00:38:02,340
Bentham, we'll see. Thanks. That's
true a lot of the time, but not always.

543
00:38:02,610 --> 00:38:05,730
There are certain circumstances in which,

544
00:38:07,450 --> 00:38:07,820
okay,

545
00:38:07,820 --> 00:38:12,820
people are likely not to act in a
way that produces a common result.

546
00:38:17,090 --> 00:38:17,923
The great,

547
00:38:18,140 --> 00:38:23,140
the great enemy enemies of public piece
or the selfish and this social passions

548
00:38:23,600 --> 00:38:25,130
necessary as they are.

549
00:38:25,160 --> 00:38:30,160
Society has held together only by the
sacrifices that men can be induced to May

550
00:38:31,310 --> 00:38:32,630
of the gratifications.

551
00:38:32,631 --> 00:38:36,880
They demand to obtain these
sacrifices is the great difficulty,

552
00:38:37,240 --> 00:38:42,240
the great task of government
and he's thinking really I've,

553
00:38:43,150 --> 00:38:46,600
and it's maybe the first
formulation of it that we find

554
00:38:48,490 --> 00:38:51,360
what we today call free writing.
Free loading.

555
00:38:55,840 --> 00:38:56,340
Okay.

556
00:38:56,340 --> 00:39:00,600
He thinks about the provision of
something like national defense,

557
00:39:03,250 --> 00:39:07,690
what we call economists
call a public good. You,

558
00:39:07,900 --> 00:39:12,200
you can't be excluded from the
benefits of it, right? But,

559
00:39:13,180 --> 00:39:17,260
and it must be jointly supplied.
So it says if for example,

560
00:39:17,261 --> 00:39:21,490
the commencement or continuing of a
war being the question upon the carpet,

561
00:39:21,520 --> 00:39:26,520
if upon his calculation a hundred a year
during the continuance of the war or

562
00:39:26,591 --> 00:39:29,530
forever will be the amount
of the contribution,

563
00:39:29,800 --> 00:39:33,340
which according to kiss calculation,
he will have to pay.

564
00:39:33,580 --> 00:39:37,360
You have to pay $100 a year
in taxes to finance this war.

565
00:39:37,870 --> 00:39:42,870
If his expected profit by the war
will be zero and no particular Gusta

566
00:39:43,780 --> 00:39:48,070
passionate into beam to drive him from
the pursuit of what appears to be his

567
00:39:48,071 --> 00:39:51,670
lasting interest upon the whole,
he will be against the war.

568
00:39:51,940 --> 00:39:56,940
And what influence it may happen to him
to possess will be exerted on the other

569
00:39:57,220 --> 00:40:00,940
side.
Now why would his benefit be zero?

570
00:40:02,770 --> 00:40:03,500
Okay.

571
00:40:03,500 --> 00:40:07,220
What's,
I mean this is rather convoluted prose,

572
00:40:07,240 --> 00:40:12,230
but what Bentham saying is if the
war's going to be four to anyway,

573
00:40:13,700 --> 00:40:16,280
I get no marginal benefit
from supporting it,

574
00:40:17,180 --> 00:40:19,060
I might as well oppose it.

575
00:40:19,070 --> 00:40:23,660
Well I might as well refuse to pay
taxes in supportive. Right, right.

576
00:40:25,040 --> 00:40:30,040
And that is the nature of public goods
that people can free ride on that

577
00:40:30,531 --> 00:40:35,531
provision because an economist says the
two features of a public good or they

578
00:40:35,751 --> 00:40:39,440
must be jointly surprised.
Everybody has to contribute to them.

579
00:40:39,980 --> 00:40:44,980
And you can't exclude anybody from
the benefits of them like clean areas.

580
00:40:45,470 --> 00:40:47,330
We keep clean air for some people,

581
00:40:47,331 --> 00:40:51,200
we're going to create clean
air for all people. Okay.

582
00:40:51,290 --> 00:40:56,090
So people are going to have to be
coerced in the provision of public goods.

583
00:40:56,330 --> 00:41:00,740
People are going to have to be
cohort coerced to pay for the war.

584
00:41:01,400 --> 00:41:03,260
So that's one example.

585
00:41:03,470 --> 00:41:07,880
Another one that comes up is the so
called tragedy of the Commons problem.

586
00:41:09,050 --> 00:41:10,970
Suppose you have some common land,

587
00:41:10,971 --> 00:41:15,971
and we'll come back to talking about
this in connection with walk a social

588
00:41:16,371 --> 00:41:18,470
contract theory.
You know,

589
00:41:18,471 --> 00:41:22,820
God gave the world to mankind in
common on lock story, so long as,

590
00:41:23,150 --> 00:41:26,960
as much in as good as available
to have this in common. So if,

591
00:41:27,050 --> 00:41:29,660
if you have common land,
he has the problem.

592
00:41:30,830 --> 00:41:33,020
You thinking about
grazing with sheep on the,

593
00:41:35,000 --> 00:41:37,700
if I put my sheep onto that common land,

594
00:41:38,360 --> 00:41:43,020
it doesn't do any lasting damage.
But if everybody grazers,

595
00:41:43,021 --> 00:41:46,850
they're sheep on the land and and
none of it's allowed to lie fallow,

596
00:41:47,060 --> 00:41:49,940
then it destroys the common.
Okay,

597
00:41:50,120 --> 00:41:53,930
so there are too many sheep for
everybody to graze the sheep on the land,

598
00:41:54,050 --> 00:41:58,640
but any individual person doesn't
have a reason not to graze his sheep.

599
00:41:59,360 --> 00:42:04,360
This was finally formulated in a rigorous
way by a man called Garrett Hardin.

600
00:42:05,000 --> 00:42:07,520
The tragedy of the Commons,
that if you have Commons,

601
00:42:07,700 --> 00:42:11,720
there'll be destroyed because each
person will do something that makes

602
00:42:11,810 --> 00:42:16,460
individual rational sense but
not collective rational sense.

603
00:42:16,760 --> 00:42:21,740
So this is, again, it's not the exactly
the same as the free riding problem,

604
00:42:21,920 --> 00:42:24,350
but it's related to the
free riding problem.

605
00:42:24,740 --> 00:42:28,720
I won't see any reason in the world why,
um,

606
00:42:29,090 --> 00:42:32,870
I shouldn't grace my cow,
my sheep on the common.

607
00:42:33,050 --> 00:42:37,640
But when everybody does that, we
destroy the common, it's a bit like,

608
00:42:37,820 --> 00:42:41,060
you know, walking down the street
with a soda can and you think,

609
00:42:41,360 --> 00:42:45,740
should I take the trouble to cross the
street to put it in a recycling bin?

610
00:42:45,920 --> 00:42:50,120
Or just throw it in the trash,
one coke bottle, you know?

611
00:42:50,330 --> 00:42:53,080
I mean, what difference one
coke was not going to make it,

612
00:42:53,081 --> 00:42:57,230
but if everybody doesn't cross the
street, the same problem. Okay.

613
00:42:57,380 --> 00:43:01,100
So these are the areas
where there's a disconnect.

614
00:43:02,270 --> 00:43:06,230
There's a disconnect between
individual utility and social utility.

615
00:43:06,440 --> 00:43:11,150
And that is what creates
the need for government.

616
00:43:12,080 --> 00:43:14,360
We will pursue these questions,
uh,

617
00:43:14,390 --> 00:43:18,380
and my child's about classical
utilitarianism next Monday.

