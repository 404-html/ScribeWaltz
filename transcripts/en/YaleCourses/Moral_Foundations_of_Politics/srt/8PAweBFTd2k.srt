1
00:00:01,500 --> 00:00:06,210
This morning we're going to begin
making the transition from the classical

2
00:00:06,211 --> 00:00:11,211
utilitarian doctrine of Jeremy Bentham's
to the neoclassical doctrine that was

3
00:00:12,841 --> 00:00:17,400
championed by a number of different
figures in the late 19th and early 20th

4
00:00:17,401 --> 00:00:22,350
century. And we're going to end up
by focusing on John Stuart Mill,

5
00:00:22,680 --> 00:00:23,041
um,

6
00:00:23,041 --> 00:00:28,041
as the principal expository of
neoclassical utilitarianism.

7
00:00:28,950 --> 00:00:33,900
And where are we headed is for a
doctrine that I'm going to call the right

8
00:00:33,930 --> 00:00:35,790
utility synthesis,

9
00:00:36,210 --> 00:00:41,210
the reits utilities synthesis signals
that we're looking for an attempt to put

10
00:00:43,081 --> 00:00:46,260
together both a commitment too,

11
00:00:46,350 --> 00:00:51,350
utilitarian efficiency that's grounded
in science and on the one hand and

12
00:00:52,800 --> 00:00:57,300
respect for individual rights that's
grounded in the workmanship ideal.

13
00:00:57,510 --> 00:00:58,560
On the other hand,

14
00:00:59,070 --> 00:01:03,990
and we're not going to actually get to
the reits utilities synthesis as it's

15
00:01:03,991 --> 00:01:08,991
expressed in politics by John
Stuart Mill until next Monday.

16
00:01:09,870 --> 00:01:14,690
What instead I'm going to
do today is explain how the,

17
00:01:14,800 --> 00:01:19,740
the transition from classical
to neoclassical utilitarianism,

18
00:01:20,490 --> 00:01:21,323
um,

19
00:01:21,360 --> 00:01:26,360
really went on in all fields of thinking
about the human sciences at more or

20
00:01:26,941 --> 00:01:28,530
less the same time.

21
00:01:29,010 --> 00:01:34,010
There were developments in Pol political
theory that we're going to talk about

22
00:01:34,011 --> 00:01:37,760
at considerable length,
but there will also, um,

23
00:01:37,870 --> 00:01:42,870
undergirding that developments
in economics and in
philosophy that are going to

24
00:01:43,471 --> 00:01:46,380
be my principal focus in today's lecture.

25
00:01:47,280 --> 00:01:51,960
What you're also going to get as a
byproduct of today's lecture is everything

26
00:01:51,961 --> 00:01:56,370
you ever needed to know about
neoclassical economics in 45 minutes.

27
00:01:57,270 --> 00:02:02,270
That is to say a neoclassical economics
is a brilliant intellectual creation to

28
00:02:04,111 --> 00:02:07,620
a large extent the creation
of bill freight out Pareto,

29
00:02:08,340 --> 00:02:12,570
an Italian economist that I'm
going to talk about today. Um,

30
00:02:12,600 --> 00:02:17,370
but with the important contributions from
other figures in the modern history of

31
00:02:17,371 --> 00:02:21,790
economics such as Marshall
a and, w I'm going to, uh,

32
00:02:21,870 --> 00:02:26,870
it's mentioned in some
detail and economist called
Edgeworth and they developed

33
00:02:27,391 --> 00:02:28,710
a system of,
of,

34
00:02:28,920 --> 00:02:33,690
of thinking about economics and the
theory of value that was going to be

35
00:02:33,691 --> 00:02:37,080
tremendously influential and important.
Um,

36
00:02:37,081 --> 00:02:40,170
not only in the way
utilitarianism involved,

37
00:02:40,380 --> 00:02:45,030
but in the way of our
thinking about markets.

38
00:02:45,031 --> 00:02:50,031
Legitimacy and distributive justice
would evolve at the same time,

39
00:02:51,330 --> 00:02:52,500
more or less.

40
00:02:52,800 --> 00:02:57,690
There were very important developments
in moral philosophy that I just want to

41
00:02:57,691 --> 00:02:58,890
alert you to,

42
00:02:59,140 --> 00:03:03,930
that we're going to return to later when
we come to consider Alistair McEntire's

43
00:03:03,940 --> 00:03:08,120
book after virtue. And this, um,

44
00:03:08,230 --> 00:03:10,300
movement in philosophy that I'm,

45
00:03:10,720 --> 00:03:15,720
I'm mentioning here is the doctrine that
would come to be called emotive Islam.

46
00:03:18,250 --> 00:03:23,000
It was associated with a man
by the name of Stephenson, uh,

47
00:03:23,140 --> 00:03:27,970
who, who wrote several books
advocating the emotive as doctrine,

48
00:03:28,420 --> 00:03:33,280
uh, when he was an untenured professor
in the Yale philosophy department as a

49
00:03:33,281 --> 00:03:38,281
byproduct of which he never became a
tenured professor in the Yale Philosophy

50
00:03:39,130 --> 00:03:42,970
Department because his doctrine
was thought to be so repugnant.

51
00:03:43,720 --> 00:03:48,720
His doctrine was that when we
make claims like murder is wrong,

52
00:03:49,810 --> 00:03:50,643
um,

53
00:03:50,660 --> 00:03:55,660
we are making claims that express
our emotions are emotive reactions to

54
00:03:56,501 --> 00:04:00,190
propositions.
Just as when we say,

55
00:04:00,220 --> 00:04:05,220
I like ice cream or I prefer chocolate
ice cream to strawberry ice cream,

56
00:04:06,310 --> 00:04:11,140
all we're doing is expressing our tastes,
our emotional reactions,

57
00:04:11,440 --> 00:04:16,000
and that there is nothing more
to say about ethics than that.

58
00:04:17,110 --> 00:04:21,550
Now you could say this,
this emotive as doctrine is a,

59
00:04:22,240 --> 00:04:27,240
an end point in an philosophical
evolution that really begins in the 17th

60
00:04:27,641 --> 00:04:28,474
century.

61
00:04:28,570 --> 00:04:33,570
Hobbs who I mentioned to you in a very
first lecture criticized Aristotle for

62
00:04:34,811 --> 00:04:39,811
not seeing as Hobbes thought that what
is desirable for some people is not

63
00:04:40,601 --> 00:04:43,000
desirable for others.
Um,

64
00:04:43,300 --> 00:04:45,610
but Hobbs didn't truth be told,

65
00:04:45,611 --> 00:04:50,110
take that view that seriously
because he thought for the most part,

66
00:04:50,290 --> 00:04:52,450
we're all pretty much the same.

67
00:04:53,650 --> 00:04:57,970
And if you look at the other
classical utilitarians like, um,

68
00:04:58,430 --> 00:05:01,900
um, David Hume who were
not reading in this course,

69
00:05:01,901 --> 00:05:06,250
but we could read in this
course or Sidgwick who were
not reading in this course,

70
00:05:06,251 --> 00:05:08,050
but we could have read in this course.

71
00:05:08,590 --> 00:05:13,590
They also basically thought human beings
were more or less all alike in their

72
00:05:14,141 --> 00:05:18,160
psychological structure,
in their basic human needs.

73
00:05:18,940 --> 00:05:23,830
Human has a famous line somewhere to
the effect that if all factual questions

74
00:05:23,831 --> 00:05:26,950
were resolved,
no moral questions would remain.

75
00:05:27,400 --> 00:05:31,930
If all factual questions were resolved,
no moral questions would remain.

76
00:05:32,170 --> 00:05:33,670
And it's not that human thought.

77
00:05:33,671 --> 00:05:38,671
We could derive an ort statement from
an is Hume's famous for the idea that an

78
00:05:39,220 --> 00:05:43,300
ort cannot be derived from an is
that there's a fact value problem.

79
00:05:43,840 --> 00:05:46,900
Um, but he, he thought nonetheless,

80
00:05:47,410 --> 00:05:49,450
people are pretty much the same.

81
00:05:49,810 --> 00:05:52,720
And so if you can figure out
what makes one of them check,

82
00:05:52,870 --> 00:05:55,600
you can figure out what
makes all of them tick.

83
00:05:56,380 --> 00:06:01,090
And that was most emphatically
Jeremy Bentham's view. Uh,

84
00:06:01,260 --> 00:06:04,760
it's opposed in everything
we discussed last time.

85
00:06:05,180 --> 00:06:10,180
If you think about the idea of doing
interpersonal comparisons of utility,

86
00:06:11,320 --> 00:06:12,153
um,

87
00:06:12,260 --> 00:06:17,260
and making the judgment that taking that
dollar from Donald Trump and giving it

88
00:06:17,541 --> 00:06:22,370
to the bag lady increases her
utility more than it decreases his,

89
00:06:22,550 --> 00:06:27,550
you're assuming that they basically
all have the same kinds of utility

90
00:06:27,921 --> 00:06:32,570
functions.
Stevenson questioned that idea radically.

91
00:06:33,140 --> 00:06:35,390
He said,
we don't actually know.

92
00:06:35,420 --> 00:06:40,420
We should take obs much more seriously
in his critique of Aristotle then he was

93
00:06:40,491 --> 00:06:41,960
willing to take himself.

94
00:06:42,290 --> 00:06:47,290
We don't actually know if people have
the same kinds of utility functions.

95
00:06:48,291 --> 00:06:53,291
We don't know whether or not what makes
some people happy will make others happy

96
00:06:55,040 --> 00:06:55,873
as well.

97
00:06:56,090 --> 00:07:01,090
And so Stevenson was thought to be a
proponent of a kind of moral relativism

98
00:07:01,671 --> 00:07:05,060
because he linked ethics to
our desires and preferences,

99
00:07:05,061 --> 00:07:08,480
any emotions and nothing else.
And then he said,

100
00:07:08,690 --> 00:07:10,610
it's actually an open question.

101
00:07:10,700 --> 00:07:13,970
Stevenson was criticizing
Hume on this point,

102
00:07:13,971 --> 00:07:16,870
but he might as well have been
criticizing Hobbs or Bentham.

103
00:07:17,240 --> 00:07:22,240
Stephen's had said it's an open question
whether people are like in their basic

104
00:07:23,931 --> 00:07:25,790
psychological structures in there,

105
00:07:25,850 --> 00:07:30,170
in there the physiology
of their human needs.

106
00:07:30,500 --> 00:07:35,500
And so that was thought to be a
radically relativist doctrine because it,

107
00:07:36,380 --> 00:07:40,850
it seemed to undermine
the possibility of making,

108
00:07:41,170 --> 00:07:46,100
um, ethical judgements of
any sword across people.

109
00:07:46,340 --> 00:07:50,030
That is a doctrine to which
we will return. As I said,

110
00:07:50,120 --> 00:07:51,760
when we get to,
uh,

111
00:07:51,870 --> 00:07:56,870
the Anti Enlightenment and in particular
Alistair McEntire's book after virtue.

112
00:07:59,120 --> 00:08:04,120
But today we're going to focus for the
rest of our time on the economics of the

113
00:08:04,251 --> 00:08:09,110
transition from classical to
neoclassical utilitarianism.

114
00:08:10,010 --> 00:08:15,010
And I'm going to ask you to suspend
disbelief for the rest of today's lecture

115
00:08:16,100 --> 00:08:17,600
and just trust me.

116
00:08:18,260 --> 00:08:21,980
Because what I'm going to do is I'm
going to go into this backwards.

117
00:08:22,010 --> 00:08:27,010
I'm going to come into the transition
from classical to neoclassical economics.

118
00:08:29,480 --> 00:08:30,313
Um,

119
00:08:30,650 --> 00:08:35,000
by looking at a very different problem
that the neoclassical economists were

120
00:08:35,001 --> 00:08:39,740
concerned with that had nothing to
do with utilitarianism or rights will

121
00:08:39,741 --> 00:08:42,830
anything that we've been
talking about in this lecture.

122
00:08:43,280 --> 00:08:47,870
And it's not until you get to the end
of this narrative that you will start to

123
00:08:47,871 --> 00:08:52,871
see why the transition from classical
to neoclassical utilitarianism in

124
00:08:53,631 --> 00:08:58,631
economics was essential for the transition
in political theory and indeed for

125
00:09:00,091 --> 00:09:05,010
the transition in moral
philosophy. So, uh, as I said,

126
00:09:05,011 --> 00:09:09,570
you're going to have to suspend your
disbelief and just follow me through the

127
00:09:09,571 --> 00:09:14,190
ABC of neoclassical price theory,
which is what we're going to do now.

128
00:09:14,400 --> 00:09:16,470
And as I said, the bow, the bonus here,

129
00:09:16,471 --> 00:09:21,471
the byproduct is you're going to get
the whole of econ one o one reduced to a

130
00:09:21,751 --> 00:09:26,751
single lecture because indeed it is true
that enormously complex and subtle and

131
00:09:29,971 --> 00:09:34,971
sophisticated as the neo classical
theory of micro economics is,

132
00:09:36,840 --> 00:09:39,600
it's all belt out of three ideas.

133
00:09:40,800 --> 00:09:45,660
It's all built out of the three ideas
that I'm going to spell out for you,

134
00:09:46,050 --> 00:09:50,790
uh, in what some of you might initially
go regardless, laborious and detail.

135
00:09:51,060 --> 00:09:53,760
Um, but I'm gonna do it anyway. Uh,

136
00:09:53,761 --> 00:09:57,030
and I think you'll see
what I'm getting at. Um,

137
00:09:57,120 --> 00:10:01,530
once we get towards the
end of today's discussion,

138
00:10:02,670 --> 00:10:03,503
so

139
00:10:06,770 --> 00:10:09,890
imagine a single person,

140
00:10:09,980 --> 00:10:14,620
we're going to call them a as testimony
to my lack of imaginative invest,

141
00:10:14,690 --> 00:10:18,180
but you could call them anything you like.
Um,

142
00:10:18,620 --> 00:10:23,480
and let's imagine a world in which
they're just two commodities in this guy's

143
00:10:23,481 --> 00:10:24,860
wine and bread.

144
00:10:27,710 --> 00:10:32,710
The system of neoclassical utilitarianism
invented by Paredo says that other

145
00:10:36,441 --> 00:10:41,060
things being equal, you want more rather
than less. Um, any source of utility,

146
00:10:41,240 --> 00:10:45,500
right? We know that from
classical utilitarianism. Um,

147
00:10:46,070 --> 00:10:47,210
but if you have,

148
00:10:48,170 --> 00:10:53,170
if you have six bottles of wine
and only one loaf of bread bread is

149
00:10:54,351 --> 00:10:57,380
comparatively more
valuable to you than wine,

150
00:10:58,340 --> 00:11:02,600
so that you would exchange a lot of
wine to get a second loaf of bread.

151
00:11:02,660 --> 00:11:05,120
If on the other hand,
uh,

152
00:11:05,390 --> 00:11:10,010
you would choking on your six loaves
of bread and dying of thirst, uh, the,

153
00:11:10,040 --> 00:11:11,900
the reverse would be
true and you would be,

154
00:11:11,990 --> 00:11:16,990
you would give up a lot of bread in
order to get a small amount of wine and

155
00:11:18,261 --> 00:11:18,890
these,

156
00:11:18,890 --> 00:11:23,890
these are what are called indifference
curves and neoclassical economics and

157
00:11:24,771 --> 00:11:29,771
indifference curves basically
imply exactly as the name
suggests that you would

158
00:11:31,851 --> 00:11:36,851
be indifferent among the mixes of
bread and wine anywhere on this curve.

159
00:11:39,290 --> 00:11:43,850
And this curve is always shaped that way.
Concave toward the origin.

160
00:11:44,270 --> 00:11:48,800
Anybody wanting to tell us why
somebody who, what is it reflecting,

161
00:11:50,030 --> 00:11:51,790
why is it,
why is it that shape?

162
00:11:54,070 --> 00:11:58,870
Someone who's done an econ class or
somebody who remembers Monday's lecture?

163
00:11:59,950 --> 00:12:04,750
Yeah.
Yes.

164
00:12:04,780 --> 00:12:05,171
Okay.

165
00:12:05,171 --> 00:12:10,171
It reflects the idea of diminishing
marginal utility reflects the idea of

166
00:12:11,411 --> 00:12:16,180
diminishing marginal utility in exactly
the sense that I just said that if you

167
00:12:16,181 --> 00:12:18,220
have a huge amount of bread,

168
00:12:18,490 --> 00:12:22,360
the next loaf of bread is less
valuable to you at the margin.

169
00:12:22,510 --> 00:12:26,200
Then the previous loaf of bread was,
so if you've got a lot of bread,

170
00:12:26,201 --> 00:12:30,190
you'll give up a lot of brand in all
the order to get a small amount of wine.

171
00:12:30,700 --> 00:12:35,580
Okay? So that is the idea of
diminishing marginal utility. And when,

172
00:12:35,610 --> 00:12:38,020
when we call this what I've labeled here,

173
00:12:38,021 --> 00:12:41,050
I won and indifference curve,

174
00:12:41,410 --> 00:12:46,410
we were saying literally you would get
the same amount of utility no matter

175
00:12:47,261 --> 00:12:52,210
where you were on that curve. So
if you have one of, we have here,

176
00:12:52,660 --> 00:12:55,030
this isn't very well drawn,
but uh,

177
00:12:55,090 --> 00:12:58,870
say four bottles of wine
and two loaves of bread,

178
00:12:58,871 --> 00:13:03,420
you would be equally happy as if
you had, um, what does it look like?

179
00:13:03,420 --> 00:13:07,510
Have four loaves of bread and,
and one and a quarter bottle of wine.

180
00:13:07,511 --> 00:13:12,070
You would be equally happy between
those two distributions. Okay.

181
00:13:12,460 --> 00:13:14,380
What would increase your happiness?

182
00:13:15,400 --> 00:13:18,970
What would increase your happiness would
be to get onto a higher indifference

183
00:13:18,971 --> 00:13:23,920
curve. If you could have more bread and
more wine, of course you'd be happier.

184
00:13:24,340 --> 00:13:24,970
Right?

185
00:13:24,970 --> 00:13:29,970
And so the idea of of indifference
curves is that you want to go this way,

186
00:13:31,570 --> 00:13:35,260
you want and go from p toward cue.
You want to get onto,

187
00:13:35,470 --> 00:13:38,860
as they put it in the jargon
of neoclassical theory,

188
00:13:39,100 --> 00:13:43,510
you want to get onto as
high an indifference curve
as you can possibly get.

189
00:13:43,900 --> 00:13:48,250
So you're this, and this is for
those of you who like the jargon,

190
00:13:48,490 --> 00:13:53,170
this would be a utility function. You want
to get up, go up your utility function,

191
00:13:53,620 --> 00:13:58,180
all the way to Q. If you could, we
don't know where Q is out in the, uh,

192
00:13:58,630 --> 00:14:02,160
in the stratosphere,
but any,

193
00:14:02,190 --> 00:14:04,900
wherever you were on
your utility function,

194
00:14:04,901 --> 00:14:09,100
you could draw one of these curves
through it. In principle, you could,

195
00:14:09,160 --> 00:14:12,310
you could find, so if you
know, if you were here,

196
00:14:12,311 --> 00:14:17,311
you could find the mix of bread and
wine in each instance among which you're

197
00:14:18,341 --> 00:14:23,200
going to be indifferent. So that is
the notion of an indifference curve.

198
00:14:24,040 --> 00:14:24,760
Now,

199
00:14:24,760 --> 00:14:29,760
an important consideration in the theory
of indifference curves was to say that

200
00:14:33,520 --> 00:14:36,440
we don't know,
I've,

201
00:14:36,460 --> 00:14:38,620
I've put these equally apart,

202
00:14:38,950 --> 00:14:42,640
but in a way I shouldn't
have because it's misleading.

203
00:14:43,120 --> 00:14:47,620
If you get from one to two and
then you get from two to three

204
00:14:49,240 --> 00:14:53,240
you haven't increased your utility
necessarily by the same amount.

205
00:14:53,360 --> 00:14:58,340
These distances don't mean anything.
Okay. I could have put three right here

206
00:15:00,240 --> 00:15:04,410
because the system of
neoclassical utilitarianism,

207
00:15:04,770 --> 00:15:07,620
unlike the system of
classical utilitarianism,

208
00:15:08,100 --> 00:15:10,350
works with ordinal scales,

209
00:15:10,460 --> 00:15:14,460
ordered all mathematical
styles and as the word implies,

210
00:15:14,940 --> 00:15:17,610
it means all we do is rank order.

211
00:15:18,240 --> 00:15:21,450
We rank order our preferences,
but the we,

212
00:15:21,490 --> 00:15:23,460
we don't say anything more.

213
00:15:23,910 --> 00:15:28,910
So this individual a prefers 43 three
to two and two to one but we can't say

214
00:15:31,080 --> 00:15:36,080
that he prefers was she prefers four
to three more than she prefers three to

215
00:15:36,181 --> 00:15:38,640
two.
We don't know that.

216
00:15:38,641 --> 00:15:41,250
We don't have a cardinal scale.

217
00:15:41,251 --> 00:15:44,790
Remember that in Bentham
system we had cardinal scales.

218
00:15:44,990 --> 00:15:49,380
We were thinking of sort of lumps of
utility that could be picked up and moved

219
00:15:49,381 --> 00:15:52,320
around and we it to people,
right?

220
00:15:52,590 --> 00:15:57,360
The neoclassical economists didn't want
to do that and they didn't want to do

221
00:15:57,361 --> 00:15:58,770
that for a different,

222
00:15:58,860 --> 00:16:02,640
a different reason than anything
I've talked about in these lectures.

223
00:16:03,510 --> 00:16:06,840
They didn't want to do that because they
were actually concerned with a quiet

224
00:16:06,841 --> 00:16:07,800
another problem.

225
00:16:09,390 --> 00:16:14,390
The problem they wanted to solve was
to understand the behavior of markets.

226
00:16:16,960 --> 00:16:21,960
They want it to be able to more precisely
to predict what prices we're going to

227
00:16:23,291 --> 00:16:27,100
be in markets and they wanted
to do that for reasons.

228
00:16:27,101 --> 00:16:31,900
I'm going to elaborate too much later
on when we come and talk about Marx and

229
00:16:31,901 --> 00:16:36,260
the Labor theory of value
and its its limitations. Um,

230
00:16:36,610 --> 00:16:40,330
but that that's for a future lecture.
For today's lecture,

231
00:16:40,360 --> 00:16:44,740
all you need to concern yourself with
is the fact that they want it to be able

232
00:16:44,741 --> 00:16:49,030
to understand the nature of
markets of how market prices move,

233
00:16:49,840 --> 00:16:51,910
but they want it to be able to do this

234
00:16:53,640 --> 00:16:56,460
with as little information as possible.

235
00:16:57,300 --> 00:17:01,650
They realized that for Bentham
system to work, for example,

236
00:17:02,430 --> 00:17:06,870
the government would have to have a kind
of utility tolmatter and run around and

237
00:17:06,871 --> 00:17:11,520
sticking it out into people's tongues
to measure their utility, right?

238
00:17:11,790 --> 00:17:16,620
Very intrusive ad you need a lot of
information to do Bentham system.

239
00:17:17,400 --> 00:17:18,390
They want it to say,

240
00:17:18,480 --> 00:17:23,480
how can we develop a well articulated
theory of market prices based on as little

241
00:17:26,341 --> 00:17:31,341
information about people and their
preferences as possible and Paredo and

242
00:17:31,621 --> 00:17:36,621
Marshall and Edgeworth and others who
are in their circle thought you could do

243
00:17:37,981 --> 00:17:40,140
this just with ordinal utility.

244
00:17:40,440 --> 00:17:45,440
So moving from cardinal to ordinal
utility is going to turn out to have huge

245
00:17:46,680 --> 00:17:50,730
ideological consequences which
I'm going to for you, uh,

246
00:17:50,820 --> 00:17:54,690
towards the end of today's lecture.
But as an analytic matter,

247
00:17:55,260 --> 00:17:58,740
looking at this from the inside,
it had the,

248
00:17:58,950 --> 00:18:03,950
the great virtue of the providing the
building blocks for a theory of price

249
00:18:06,451 --> 00:18:11,451
behavior in market systems that required
almost no information about people.

250
00:18:13,200 --> 00:18:16,470
All we would know about
this person a as I said,

251
00:18:16,740 --> 00:18:21,380
is that they prefer four to three, three
to two to two, one, one, two, zero.

252
00:18:21,450 --> 00:18:26,450
But we can't say anything about how much
they prefer those things because these

253
00:18:27,031 --> 00:18:29,280
distances don't actually mean anything.

254
00:18:29,580 --> 00:18:33,000
All we get is an ordered ranking.

255
00:18:34,500 --> 00:18:37,170
Now there is one other thing we can say.

256
00:18:39,120 --> 00:18:40,860
One other thing we can say,

257
00:18:45,550 --> 00:18:46,020
okay

258
00:18:46,020 --> 00:18:49,530
is that this is a no,
no.

259
00:18:52,380 --> 00:18:55,200
These indifference curves cannot cross.

260
00:18:56,830 --> 00:19:00,370
Can anybody tell us why?
Why can't they cross?

261
00:19:05,590 --> 00:19:06,423
Wait for the mic?

262
00:19:08,620 --> 00:19:13,420
Because at the intersection they should
have the same utility even though

263
00:19:13,421 --> 00:19:16,090
they're different in
different indifference curves.

264
00:19:16,310 --> 00:19:20,630
So what would happen if, what
would be what? What, what's,
what's the problem with,

265
00:19:20,720 --> 00:19:23,540
I mean you're on the right track and
what's the problem with that crossing?

266
00:19:24,220 --> 00:19:29,220
Because you say I too as utility of
two I 2.5 as utility of two point by,

267
00:19:30,941 --> 00:19:33,880
but at that point where they intersect,

268
00:19:33,881 --> 00:19:36,430
they both have to have the same utility.

269
00:19:37,620 --> 00:19:42,420
So you've got a kind of contradiction
on your hands, is that right? Yeah.

270
00:19:43,260 --> 00:19:44,093
Okay.

271
00:19:44,100 --> 00:19:48,420
And just just to spell out the
contradiction more emphatically,

272
00:19:48,421 --> 00:19:53,421
I think you basically made the point if
we're saying that we're in different,

273
00:19:53,730 --> 00:19:57,660
among all of the things on this
curve and we're in different,

274
00:19:57,661 --> 00:20:00,390
among all the things on this curve,
we can't,

275
00:20:01,350 --> 00:20:05,460
we can't have a cross because
then we're saying here, right?

276
00:20:06,210 --> 00:20:10,590
2.5 is preferred to two,
but here we're saying the opposite.

277
00:20:10,591 --> 00:20:15,180
We're saying two is preferred to 2.5 okay.
The jargon.

278
00:20:15,181 --> 00:20:18,330
Anybody just happen to know, yell
it out. We don't need the mic.

279
00:20:18,331 --> 00:20:20,400
Does anybody know that jargon for this

280
00:20:22,920 --> 00:20:27,490
France activity? The preferences,
I was assumed to be transferred in.

281
00:20:27,910 --> 00:20:32,740
So if you prefer a two B and B to c,
you must prefer a to z.

282
00:20:33,160 --> 00:20:35,260
That's all that transit. That means, okay,

283
00:20:36,190 --> 00:20:39,070
if you prefer a two B and B two c,

284
00:20:39,100 --> 00:20:42,160
it must be the case that
you also prefer a to see.

285
00:20:42,340 --> 00:20:46,620
Otherwise you're contradicting
the principle of transitivity.

286
00:20:47,860 --> 00:20:48,490
Okay?

287
00:20:48,490 --> 00:20:53,260
So we cannot have these indifference
curves crossing one another.

288
00:20:58,920 --> 00:20:59,753
Now

289
00:21:00,990 --> 00:21:05,250
what we're going to do here instead of

290
00:21:06,930 --> 00:21:11,730
one person and two commodities,
we're going to think about two people,

291
00:21:12,270 --> 00:21:16,170
okay? We're putting, we're creating
a diagram with two people on it.

292
00:21:17,310 --> 00:21:22,310
And this is I say as I promised
you earlier in the semester,

293
00:21:22,740 --> 00:21:26,130
anything I do with a diagram
I will also do verbally.

294
00:21:26,131 --> 00:21:29,720
So if you find this in any way confusing,
um,

295
00:21:29,970 --> 00:21:34,560
just listen to their narrative and then
we'll see whether you get it that way.

296
00:21:35,120 --> 00:21:39,780
Um, but so now we have a diagram
with two people on it. Okay?

297
00:21:39,990 --> 00:21:44,990
So this is person a and this
is person B and these axes,

298
00:21:45,571 --> 00:21:49,020
the x axis here is A's utility function.

299
00:21:49,030 --> 00:21:54,030
Remember Aa and the previous slide
was trying to get from Pete towards Q,

300
00:21:54,570 --> 00:21:59,130
right?
Hey was trying to go up here.

301
00:21:59,280 --> 00:22:04,280
So this on this slide is the
same thing as this on this slide.

302
00:22:05,010 --> 00:22:09,060
So A's trying to go this way
and B is trying to go this way,

303
00:22:10,080 --> 00:22:10,913
okay?

304
00:22:11,280 --> 00:22:16,280
And what we imagine is some
distribution of utility between them.

305
00:22:16,980 --> 00:22:21,980
So a has this much utility f
if this is the status quo x,

306
00:22:22,500 --> 00:22:26,700
okay, hey has this master utility
and B has this much, you tell them

307
00:22:28,980 --> 00:22:33,860
ace, happy than be right. Wrong.

308
00:22:35,040 --> 00:22:37,630
He's not,
we don't know that A's happy than before.

309
00:22:37,631 --> 00:22:42,230
What I'm from what I just
said, right? These distances
don't mean anything, right?

310
00:22:42,680 --> 00:22:47,590
So, uh, it looks like A's happier
than be, but that's misleading. If,

311
00:22:47,660 --> 00:22:52,660
if the different distances I've taken to
imply in your mind that A's happy than

312
00:22:53,031 --> 00:22:57,650
be uh, this abuse yourself
of that thought right away.

313
00:22:58,010 --> 00:23:01,760
Okay? So we have a
distribution here, okay?

314
00:23:02,210 --> 00:23:05,070
Now what Paredo said,
he said,

315
00:23:05,280 --> 00:23:09,350
let's draw a line north,
south through the status quo,

316
00:23:09,590 --> 00:23:14,180
and let's draw a line east, west
through the status quo. Okay?

317
00:23:15,830 --> 00:23:20,040
And we'll imagine that there's a finite
source of utility that's called the Po.

318
00:23:20,670 --> 00:23:25,640
It's gets called in the ECON textbox
the Predo possibility frontier.

319
00:23:26,540 --> 00:23:31,280
So there's not an infinite
source of utility. Now

320
00:23:33,190 --> 00:23:37,220
paredo said, well, if we draw the
north, south and the east west,

321
00:23:37,221 --> 00:23:40,850
we get four quadrants, we get this
one, we get this one, we get this one,

322
00:23:40,851 --> 00:23:45,650
and then we get this one right.
And what parade said is,

323
00:23:45,651 --> 00:23:49,190
well that's interesting because we
can say different things about them.

324
00:23:50,240 --> 00:23:54,230
One thing we can say is if you go
anywhere into the northeast quadrant,

325
00:23:56,420 --> 00:24:01,250
both of them are better off,
right? So if we go from x to Y,

326
00:24:02,810 --> 00:24:07,220
we know a, as you tell it,
he's gone up and we know bees
you, Tony's gone up, right?

327
00:24:08,390 --> 00:24:11,900
We don't know by how much,
but we know it's gone up.

328
00:24:12,920 --> 00:24:17,690
So that both better off. On the other
hand, if we went in anywhere in here,

329
00:24:18,590 --> 00:24:21,620
this quadrant south west,
as it were,

330
00:24:24,660 --> 00:24:27,390
obviously that both better at worse off.
Okay.

331
00:24:28,620 --> 00:24:32,400
Because if we,
if I had put a point here,

332
00:24:32,401 --> 00:24:36,120
you know Q, well no,
that's not used to Jay.

333
00:24:36,810 --> 00:24:41,400
Um, that's not you. Yeah. And
let's use J by a point here, Jay.

334
00:24:41,401 --> 00:24:44,670
We would say that A's gone down.
Ed Bee's gone down.

335
00:24:45,990 --> 00:24:50,970
Now to make this a bit more real,
imagine

336
00:24:52,590 --> 00:24:56,970
in here,
this is the sphere of market transactions.

337
00:24:56,971 --> 00:25:01,860
This is where a and B will
go voluntarily, right? So A,

338
00:25:01,861 --> 00:25:05,820
we'll say to be, well I have all this
wine and you have all that bread.

339
00:25:05,821 --> 00:25:09,390
How about I swap you a bottle of wine
for a loaf of bread? And you're saying,

340
00:25:09,391 --> 00:25:13,320
okay, you give it to them. Both
people are better off. Okay.

341
00:25:13,500 --> 00:25:17,310
And we know that both better off because
they did it voluntarily and we know

342
00:25:17,311 --> 00:25:20,490
they're both trying to get onto is high
and in different score as possible.

343
00:25:20,520 --> 00:25:23,490
Right? So they met, they swapped the wine,

344
00:25:23,491 --> 00:25:28,410
they swapped the bread and both of
them are happier, a little more tipsy.

345
00:25:28,411 --> 00:25:33,150
But also a, a little, uh,
a little better fat. Okay.

346
00:25:34,890 --> 00:25:36,240
Uh,
move into here

347
00:25:38,040 --> 00:25:40,030
would be a ZF.

348
00:25:40,770 --> 00:25:45,770
The government taxes them both
and uses the money to spend on,

349
00:25:47,180 --> 00:25:50,400
um, foreign aid to a
country they both despise,

350
00:25:51,900 --> 00:25:52,733
let's say.

351
00:25:53,430 --> 00:25:57,960
So they both paid attacks and the money
has gone to something they don't support.

352
00:25:59,670 --> 00:26:03,840
We can say that's what,
that, that's Paredo inferior,

353
00:26:05,260 --> 00:26:08,460
righto. Superior or
righto inferior. Right.

354
00:26:08,850 --> 00:26:11,340
It's Baredo inferior because
they both don't want it.

355
00:26:12,030 --> 00:26:15,930
And both of them would resist it
if the government tried to do it.

356
00:26:16,320 --> 00:26:21,320
Obviously they wouldn't go there to a
market transaction because it reduce,

357
00:26:21,511 --> 00:26:25,590
it puts both of them on a
lower indifference curve. Okay.

358
00:26:26,730 --> 00:26:31,290
So that's all well and good.
Well that leaves these two other quadrants

359
00:26:35,740 --> 00:26:39,130
and about those two quadrants,
Pareto's says,

360
00:26:39,710 --> 00:26:43,210
we can say nothing at all.

361
00:26:45,300 --> 00:26:47,370
We can say nothing at all,

362
00:26:48,910 --> 00:26:50,650
at least nothing scientific.

363
00:26:53,330 --> 00:26:58,330
And then he says in his famous 700 page
book called the manual of political

364
00:26:58,971 --> 00:27:03,620
economy, he says, people are
going to misinterpret me.

365
00:27:03,980 --> 00:27:08,150
People are going to interpret me as
saying we should never move into either of

366
00:27:08,151 --> 00:27:11,510
these quadrants.
I'm not saying that,

367
00:27:12,050 --> 00:27:17,050
all I'm saying is we will never have a
scientific reason for moving into either

368
00:27:18,651 --> 00:27:23,651
of these quadrants because if we were to
move from x to g so that we took some,

369
00:27:26,230 --> 00:27:29,980
we tax a by that amount
and we give it to be,

370
00:27:30,820 --> 00:27:32,170
we cannot say,

371
00:27:33,780 --> 00:27:38,780
we cannot say that bees gain is greater
than A's loss because these distances

372
00:27:39,301 --> 00:27:42,900
don't mean anything.
Despite where I put the G,

373
00:27:43,710 --> 00:27:48,710
we just have no way of knowing because
we don't allow interpersonal comparisons

374
00:27:50,730 --> 00:27:52,050
of utility.

375
00:27:52,650 --> 00:27:57,650
And that's the link to the Stevenson
in philosophy that I was talking to you

376
00:27:57,690 --> 00:28:00,930
about. Oh yeah. There's no way of knowing

377
00:28:03,860 --> 00:28:06,040
weather,
um,

378
00:28:08,780 --> 00:28:13,780
B's loss is as big as a gain or the
reverse because we can't make comparisons

379
00:28:15,201 --> 00:28:19,190
across individuals.
There's no scientific way to do it.

380
00:28:19,880 --> 00:28:24,320
We can't assume with Bentham
and with whom and with Sedgwick,

381
00:28:24,860 --> 00:28:29,750
we can't assume that everybody's
basically the same. Perhaps they are.

382
00:28:30,290 --> 00:28:35,030
Perhaps they aren't, but
we just don't know. Okay.

383
00:28:35,031 --> 00:28:37,520
So that is the Pareto principle.

384
00:28:38,120 --> 00:28:42,620
The paredo principle out of which the
hall of neoclassical economic theory was

385
00:28:42,621 --> 00:28:47,210
constructed depends on this idea of
indifference curves. People are trying to,

386
00:28:47,420 --> 00:28:49,690
a is trying to get up
on those investments.

387
00:28:49,700 --> 00:28:53,990
Curb B is trying to get along
on these difference curves. Um,

388
00:28:55,040 --> 00:28:58,280
and we have paredo superior,

389
00:28:58,580 --> 00:29:00,800
righto in Farrier and then these two,

390
00:29:00,801 --> 00:29:04,910
which he call Paredo undecidable
because he can't decide,

391
00:29:06,050 --> 00:29:08,060
not because you're a [inaudible],

392
00:29:08,090 --> 00:29:13,090
but because there's no scientific way
to tell whether in this case B's loss

393
00:29:16,040 --> 00:29:20,900
exceeds A's game. And if we went up
here would be an analogous problem.

394
00:29:22,490 --> 00:29:27,230
Now if,
let's just suppose we,

395
00:29:27,360 --> 00:29:32,360
we go back that a proposes to be swapping
a loaf of bread for a bottle of wine

396
00:29:32,881 --> 00:29:37,500
and they be grazed. They go
to y. And then I says, well,

397
00:29:37,501 --> 00:29:42,501
I'll give you another love red for
another bottle of wine and basis forget it

398
00:29:43,230 --> 00:29:47,050
says, well come on, how about a half
a bottle of wine? He says, okay,

399
00:29:47,950 --> 00:29:52,660
then they go to z. Okay. And if,

400
00:29:52,780 --> 00:29:53,613
if,

401
00:29:53,680 --> 00:29:58,680
if they then get to a point at which
no matter what swap a is willing to

402
00:29:58,991 --> 00:30:03,040
propose,
B says no and no matter what,

403
00:30:03,041 --> 00:30:07,270
swap bean is willing to propose a says no.
Then you know,

404
00:30:07,271 --> 00:30:11,290
they've hit that front here. What's
called the Paredo possibility frontier,

405
00:30:11,680 --> 00:30:13,570
because now there's no way,

406
00:30:14,910 --> 00:30:19,140
there's no way to make be better
off without making a worse off.

407
00:30:19,860 --> 00:30:23,400
Okay, so there won't this.
They will. They will.

408
00:30:23,940 --> 00:30:24,960
What about half of Bob?

409
00:30:24,961 --> 00:30:28,200
What about a quarter of a bottle and
then I want two thirds of alive. No,

410
00:30:28,201 --> 00:30:32,400
that's too much. Blah, blah, blah.
Back and forth. But again, and I'm off.

411
00:30:32,880 --> 00:30:36,990
Okay. No transaction occurs. You
know, they've hit that front yet.

412
00:30:38,340 --> 00:30:42,960
So the Paredo principle says
that in a market system, they'll,

413
00:30:42,990 --> 00:30:46,320
they'll move toward the front tier
and when they get there, they'll stop.

414
00:30:48,840 --> 00:30:52,410
Now of course, they may have gotten
this somewhere else. They might've gone.

415
00:30:52,470 --> 00:30:54,210
They might say,

416
00:30:54,390 --> 00:30:58,050
they might have gone from
x to two g over here,

417
00:30:58,051 --> 00:31:02,190
and then they would have done a new one
and then they might've gone to here and

418
00:31:02,191 --> 00:31:02,581
so on.

419
00:31:02,581 --> 00:31:07,280
And that would just reflect shrewdness
in bargaining or how much people can

420
00:31:07,290 --> 00:31:10,680
lower down there in different scrubs.
Well are the idiosyncrasies,

421
00:31:10,681 --> 00:31:14,610
but once they wind up anywhere
on this indifference curve,

422
00:31:15,090 --> 00:31:16,770
they're not going to move off of it

423
00:31:18,300 --> 00:31:23,300
because now there's no way of improving
one person's utility without diminishing

424
00:31:24,960 --> 00:31:29,850
the next person's utility.
And that is what is called Paredo optimal.

425
00:31:30,060 --> 00:31:31,050
Okay.
So

426
00:31:32,670 --> 00:31:37,670
why is Pareto superior to
x and Z is Pareto optimal,

427
00:31:38,250 --> 00:31:41,490
can be improved upon.
It's an optimum in that sense.

428
00:31:43,070 --> 00:31:43,460
Okay.

429
00:31:43,460 --> 00:31:44,293
That's it.

430
00:31:44,960 --> 00:31:49,550
That is neoclassical economic
theory in a nutshell.

431
00:31:51,680 --> 00:31:55,280
Now I'm going to show you
one more diagram that, well,

432
00:31:55,310 --> 00:31:58,190
possibly look intimidating
when I first put it up,

433
00:31:58,430 --> 00:32:03,430
but all it does is put the preceding
diagrams that we've just looked at

434
00:32:04,970 --> 00:32:05,803
together.

435
00:32:07,560 --> 00:32:08,393
Okay.

436
00:32:08,530 --> 00:32:13,280
And this is, this is what's
in the econ literature, uh,

437
00:32:13,390 --> 00:32:18,390
was invented by an economist called
Edgeworth and it's called an Edgeworth box

438
00:32:18,820 --> 00:32:23,020
diagram.
And let me explain it for you.

439
00:32:23,110 --> 00:32:25,660
Diagrammatically and then
if anybody doesn't get it,

440
00:32:25,661 --> 00:32:28,870
we'll wait up and I'll go
through it more slowly.

441
00:32:29,560 --> 00:32:34,180
But think about the, the, uh,

442
00:32:34,210 --> 00:32:38,390
diagram we just did. Okay. Think about a,

443
00:32:38,420 --> 00:32:42,080
a is in the corner. Okay. But,

444
00:32:42,200 --> 00:32:46,100
but basically now we're putting
the two previous diagrams together.

445
00:32:46,610 --> 00:32:50,870
Maybe I'll just go back.
So was everybody's clear what we're doing.

446
00:32:54,510 --> 00:32:54,890
We're,

447
00:32:54,890 --> 00:32:59,890
we're putting this diagram where we have
to commodities and one person and this

448
00:33:03,071 --> 00:33:07,630
diagram where we have two people on just
utility, we're putting it all together

449
00:33:09,250 --> 00:33:13,630
into one big picture.
And so this is the,

450
00:33:13,660 --> 00:33:17,320
you'll see why this is helpful.
Uh, once we get to the end of it.

451
00:33:17,770 --> 00:33:22,720
So here a,
once an ae has indifference curves,

452
00:33:23,080 --> 00:33:27,490
this is our first picture, right?
So a is trying to go northeast,

453
00:33:27,730 --> 00:33:32,710
we have a, we have wine, we have
bread. So if a is on this difference,

454
00:33:32,711 --> 00:33:37,210
curb the A's difference, indifference
curves are the dotted lines. Okay.

455
00:33:37,850 --> 00:33:40,060
A is trying to get that way,
that way,

456
00:33:40,061 --> 00:33:44,290
that way and keep going all that that way.
Right?

457
00:33:44,740 --> 00:33:48,090
And what Edgeworth did,
and this is I,

458
00:33:48,130 --> 00:33:50,050
it wasn't that I was a
late night or anything,

459
00:33:50,051 --> 00:33:54,790
that I put this writing
upside down. Um, he said,

460
00:33:54,791 --> 00:33:57,610
just imagine a mirror going down here.

461
00:34:03,080 --> 00:34:07,250
Be as coming at it from the other corner.
Okay.

462
00:34:07,400 --> 00:34:11,990
So B
is looking at bottles of wine,

463
00:34:12,020 --> 00:34:16,700
loves of bread, and B is now got
this solid in different scarves.

464
00:34:16,701 --> 00:34:20,750
And be as trying to go this way,
right? B is trying to go this way.

465
00:34:21,590 --> 00:34:26,590
So a would improve if he went this way
and B would improve if she went this way.

466
00:34:28,620 --> 00:34:31,490
Okay, so B, this is an,

467
00:34:31,560 --> 00:34:34,200
this is an indifference curve for B.

468
00:34:34,590 --> 00:34:37,530
Now it's going south west
instead of north east.

469
00:34:37,531 --> 00:34:41,580
Just because it's looking in the mirror
being as, hey, look in the mirror.

470
00:34:42,270 --> 00:34:46,920
Get it. Okay. So B wants
to go this way. As I said,

471
00:34:46,921 --> 00:34:49,920
southwest a wants to go northeast.

472
00:34:50,820 --> 00:34:53,130
So if you imagine,
um,

473
00:34:55,330 --> 00:35:00,330
if you imagine that this were the status
quo from right where a has almost all

474
00:35:03,821 --> 00:35:08,320
of the bread, right? And B
has almost all of the wine.

475
00:35:08,650 --> 00:35:10,930
Then this shaded area here,

476
00:35:10,931 --> 00:35:13,870
this big football is the parade.
Oh,

477
00:35:13,871 --> 00:35:18,550
superior set on the previous diagram.
Because if you think about it,

478
00:35:18,580 --> 00:35:20,560
it's the set that,

479
00:35:20,890 --> 00:35:21,723
okay,

480
00:35:22,010 --> 00:35:26,500
wherever, if you move anywhere in the
biggest football, I've shaded here,

481
00:35:28,130 --> 00:35:32,270
like say we started, let's say we
started here at ECS, wear a hat, all the,

482
00:35:32,330 --> 00:35:36,200
almost all the wine it'd be,
had almost a, all of the bread.

483
00:35:38,520 --> 00:35:42,660
Anything that like if
they went from x to y,

484
00:35:43,470 --> 00:35:47,640
a would be on a higher indifference
curve cause a would've gone from here to

485
00:35:47,641 --> 00:35:52,020
here and be coming the other way would
also be on a hiring difference curve.

486
00:35:52,380 --> 00:35:55,200
And you could draw a new football,
right?

487
00:35:55,201 --> 00:35:59,130
It would be a smaller football
within the bigger football.

488
00:36:00,690 --> 00:36:03,360
And then they, they
haggle again. And I says,

489
00:36:03,361 --> 00:36:05,220
well what about half a bottle of B says,

490
00:36:05,221 --> 00:36:07,110
well then I won three quarters of a love,
blah,

491
00:36:07,111 --> 00:36:10,020
blah back and forth and they wind up at Z.

492
00:36:11,370 --> 00:36:16,370
And you know,Z is on the Pos
Parade Possibility Front Kia,

493
00:36:17,370 --> 00:36:22,230
because every proposal that
one makes the other rejects,

494
00:36:22,980 --> 00:36:23,760
right?

495
00:36:23,760 --> 00:36:28,760
And so you captured in an Edgeworth box
by having their indifference curves be

496
00:36:30,001 --> 00:36:31,380
points of Tangency.

497
00:36:31,650 --> 00:36:36,650
So at z a wants to go this way,

498
00:36:37,500 --> 00:36:42,500
but the only way that a can go this
way would move be off his or her

499
00:36:43,470 --> 00:36:47,670
indifference curve, which is
this one coming this way. Okay.

500
00:36:49,020 --> 00:36:52,530
So all the Edgeworth box diagram

501
00:36:54,600 --> 00:36:59,010
just puts, puts all of the
pictures together. There's no,

502
00:36:59,040 --> 00:37:01,590
nothing conceptually new in it at all.

503
00:37:02,160 --> 00:37:05,700
And it only becomes
relevant to our purposes,

504
00:37:07,230 --> 00:37:08,940
um, because in, in a,

505
00:37:08,941 --> 00:37:12,330
in will enable us to start thinking
about distributed questions,

506
00:37:12,630 --> 00:37:15,810
which I will get to in a minute.
Okay.

507
00:37:21,040 --> 00:37:21,821
Let me pause.

508
00:37:21,821 --> 00:37:26,350
Can I want to be sure if I'd
gone through this too quickly?

509
00:37:26,380 --> 00:37:28,660
Would anyone like me to
walk through it again?

510
00:37:29,860 --> 00:37:32,080
If you think you want me
to walk through it again?

511
00:37:32,081 --> 00:37:35,140
Probably half the people in
the room do so I don't feel

512
00:37:37,370 --> 00:37:38,203
awkward.

513
00:37:38,410 --> 00:37:42,880
It's actually a lot simpler
than it looks, right? I mean,

514
00:37:42,881 --> 00:37:47,560
all you have to do is take the,
the, the previous, the parade. Oh,

515
00:37:47,561 --> 00:37:51,070
principal. Um, the diagramming,

516
00:37:51,100 --> 00:37:55,180
imagine it with a mirror and think
of B is upside down coming toward a,

517
00:37:55,181 --> 00:37:58,900
and then it just all fits together.
Very clever.

518
00:38:03,270 --> 00:38:07,710
Okay. And the one thing I
would say this line here,

519
00:38:09,570 --> 00:38:10,050
okay.

520
00:38:10,050 --> 00:38:14,550
Is One on the previous diagram was
the Predo possibility frontier. Right.

521
00:38:14,551 --> 00:38:18,540
This line is all the
points of tangency between

522
00:38:19,680 --> 00:38:20,513
mmm.

523
00:38:21,900 --> 00:38:25,890
Like there, there is one,
you know, there is one.

524
00:38:26,640 --> 00:38:27,473
MMM.

525
00:38:27,820 --> 00:38:31,960
There is one. So you, you
join up all the points,

526
00:38:32,020 --> 00:38:36,790
points at which the indifference
curves are tangents. To one another.

527
00:38:37,210 --> 00:38:40,180
That is the line where if,
if they get onto this line,

528
00:38:40,181 --> 00:38:44,290
they won't move off of it
voluntarily. Right. By definition.

529
00:38:44,950 --> 00:38:49,090
So they got,
they went from x to y to Z,

530
00:38:49,091 --> 00:38:52,030
but perhaps if they
had gone, you know, if,

531
00:38:52,090 --> 00:38:56,980
if I'm a had driven a
harder bargain early on, um,

532
00:38:57,160 --> 00:38:58,840
they might've gone,
uh,

533
00:38:59,200 --> 00:39:03,390
by a different path and would've
wound up at a different point. Um,

534
00:39:05,610 --> 00:39:06,443
on the,

535
00:39:08,870 --> 00:39:11,570
this, this is the parade.
Oh, possibility frontier,

536
00:39:11,571 --> 00:39:16,250
which Edgeworth called the contract
curve. It's the same thing. It's the,

537
00:39:16,280 --> 00:39:21,200
it's where they will make a contract
to get to, they will agree to,

538
00:39:21,850 --> 00:39:26,450
um, they'll agree to transactions
that get them onto that curve,

539
00:39:26,990 --> 00:39:31,940
but once they're on it, they stay
there. Okay. That's the basic intuition.

540
00:39:32,720 --> 00:39:37,720
So market transactions into the Predo
superior zone and eventually they stop

541
00:39:39,050 --> 00:39:41,390
when things are paredo optimal.

542
00:39:42,830 --> 00:39:47,830
Now let's think about comparing classical
and neoclassical utilitarianism.

543
00:39:49,791 --> 00:39:54,791
If you think back to Monday's lecture
we said with Bentham's utility.

544
00:39:59,620 --> 00:40:00,453
Yeah.

545
00:40:00,490 --> 00:40:03,850
Anything.
This isn't a very good 45 degrees is it?

546
00:40:04,660 --> 00:40:09,250
I guess it is just depends where
you stand with Bentham's utility.

547
00:40:09,520 --> 00:40:11,050
We said that anything,

548
00:40:12,430 --> 00:40:14,830
anything in this whole area,

549
00:40:15,760 --> 00:40:20,560
the first shaded area maximizes the
greatest happiness of the greatest number,

550
00:40:20,620 --> 00:40:23,330
right? That's was his, um,

551
00:40:24,740 --> 00:40:25,573
imperative.

552
00:40:26,980 --> 00:40:28,150
The parado principle.

553
00:40:28,151 --> 00:40:33,151
We now know singles out this parade of
superior area as unambiguously better

554
00:40:36,221 --> 00:40:39,820
because people will go there voluntarily.
So,

555
00:40:39,850 --> 00:40:44,830
so everything that is parade, oh,
superior is Bentham superior, right?

556
00:40:47,820 --> 00:40:52,680
Everything that's Paredo inferior
is Bentham inferior, right?

557
00:40:52,681 --> 00:40:54,900
This is,
this is uninteresting.

558
00:40:54,901 --> 00:40:59,901
It's unambiguously worse whether
you're a classical or a neoclassical

559
00:41:00,330 --> 00:41:02,090
utilitarian,
right?

560
00:41:02,490 --> 00:41:06,150
So everything that's Pereda
superior is bent them superior,

561
00:41:06,900 --> 00:41:11,070
everything that's Predo inferior,
it's bent them inferior.

562
00:41:11,640 --> 00:41:14,160
But now the interesting stuff,

563
00:41:14,190 --> 00:41:17,520
which is where all of
redistributive politics goes on,

564
00:41:17,521 --> 00:41:22,521
and all the battles in politics go
on are in the two Paredo undecidable

565
00:41:23,520 --> 00:41:24,930
quadrants,
right?

566
00:41:24,960 --> 00:41:29,960
This one and this one
about which Paredo says,

567
00:41:30,241 --> 00:41:33,710
as a matter of science,
we can say nothing.

568
00:41:35,800 --> 00:41:37,420
And Bentham disagrees,

569
00:41:38,470 --> 00:41:39,303
right?

570
00:41:39,550 --> 00:41:42,160
Ben femmes principle by sex,

571
00:41:43,260 --> 00:41:48,260
these Paredo undecidable quadrants
because Bentham makes his interpersonal

572
00:41:48,361 --> 00:41:51,270
judgments of utility.
And so Bentham says,

573
00:41:51,870 --> 00:41:53,610
if you go into this area,

574
00:41:54,230 --> 00:41:54,760
yeah,

575
00:41:54,760 --> 00:41:58,160
it's a Bentham improvement though.
It's Paredo undecidable.

576
00:41:58,450 --> 00:42:01,870
Whereas if you go into this area,
it's not a Bantham improvement,

577
00:42:01,900 --> 00:42:03,910
even though it's Prorato undecidable.

578
00:42:04,300 --> 00:42:09,300
So Ben femmes and Sedgwick and you and
the classical utilitarians think we can

579
00:42:10,931 --> 00:42:12,820
make interpersonal judgments,

580
00:42:12,821 --> 00:42:17,821
which allow us to say when it will
make sense for the government to tax a

581
00:42:21,280 --> 00:42:22,540
and benefit B.

582
00:42:22,780 --> 00:42:26,410
Whereas Paredo says
there's no way to tell.

583
00:42:27,990 --> 00:42:28,441
Again,

584
00:42:28,441 --> 00:42:33,441
he says people are going to interpret
me as saying the government should never

585
00:42:34,261 --> 00:42:35,150
redistribute.

586
00:42:35,250 --> 00:42:39,810
I'm not saying that and it's a
misuse of my doctrine to say that.

587
00:42:41,010 --> 00:42:46,010
All I'm saying is if the
government chooses to redistribute,

588
00:42:46,350 --> 00:42:49,590
there's not going to be a scientific
principle to tell them how.

589
00:42:53,610 --> 00:42:56,850
So just to make the point dramatic,

590
00:43:00,180 --> 00:43:03,240
let's suppose we've gone
back to the address diagram.

591
00:43:03,780 --> 00:43:06,720
Let's suppose this is the status quo

592
00:43:11,040 --> 00:43:11,611
that is,

593
00:43:11,611 --> 00:43:16,250
let's suppose B has
everything and a has nothing.

594
00:43:16,370 --> 00:43:18,530
He has all the wine and all the Bragg,

595
00:43:27,020 --> 00:43:32,020
we know they're on the contract curve
because B has nothing that a once,

596
00:43:32,540 --> 00:43:33,373
right.

597
00:43:37,160 --> 00:43:40,790
So I have to thank again
of Trump and the bag lady.

598
00:43:41,870 --> 00:43:43,580
She has nothing that he wants.

599
00:43:43,581 --> 00:43:48,581
So the parade efficient outcome
is for the bag lady to start.

600
00:43:51,980 --> 00:43:54,800
It might not be morally defensible,

601
00:43:54,801 --> 00:43:58,310
but it's the parade efficient
outcome there on the contract curve.

602
00:43:58,760 --> 00:44:03,760
There is nothing you can do to improve
the bag ladies utility that will not

603
00:44:04,461 --> 00:44:06,080
diminish Trump's utility

604
00:44:07,280 --> 00:44:08,113
right

605
00:44:08,310 --> 00:44:11,520
now you could say, but of course
she's on the verge of starvation.

606
00:44:11,521 --> 00:44:13,470
How can that possibly be the case?

607
00:44:13,890 --> 00:44:18,890
But notice we've ruled out interpersonal
comparisons of utility now and so

608
00:44:21,810 --> 00:44:26,810
proponents of neoclassical
utilitarianism have no way

609
00:44:29,400 --> 00:44:33,150
Jamaica interpersonal
judgments of utility.

610
00:44:37,720 --> 00:44:39,400
So the PARETO efficient,

611
00:44:40,420 --> 00:44:41,100
okay.

612
00:44:41,100 --> 00:44:46,100
Transaction is no transaction and
the bag lady starves to death.

613
00:44:47,560 --> 00:44:48,393
Yeah.

614
00:44:53,020 --> 00:44:57,100
You'll see when we come to read
John Rawls later in the semester,

615
00:44:58,420 --> 00:44:58,901
he says,

616
00:44:58,901 --> 00:45:03,670
the trouble with utilitarianism is
that it doesn't take seriously the

617
00:45:03,671 --> 00:45:06,010
differences among persons.

618
00:45:06,670 --> 00:45:11,290
The trouble of utilitarianism says John
Rawls and his theory of justice is that

619
00:45:11,291 --> 00:45:15,010
it doesn't take seriously the
differences among persons.

620
00:45:16,390 --> 00:45:19,360
You now know enough to
see that actually rolls.

621
00:45:19,720 --> 00:45:24,040
His argument is half right?
Because the truth is,

622
00:45:24,460 --> 00:45:27,850
that's the problem with Neal
with classical utilitarianism.

623
00:45:28,480 --> 00:45:33,070
Classical utilitarianism
says, well, if, if,

624
00:45:33,160 --> 00:45:35,440
uh,
taking all of your utility and gives,

625
00:45:35,470 --> 00:45:38,920
giving it to me increases
overall neck utility,

626
00:45:39,520 --> 00:45:44,110
then we should do that because improve
the greatest happiness of the greatest

627
00:45:44,111 --> 00:45:47,290
number. We don't care who
has the utility, right?

628
00:45:47,740 --> 00:45:52,360
So classical utilitarianism is indeed
vulnerable two roles as critique.

629
00:45:52,361 --> 00:45:57,280
It doesn't take seriously the
differences between persons. Right?

630
00:45:57,281 --> 00:45:59,420
And we saw that,
that when we,

631
00:45:59,421 --> 00:46:04,421
you allow interpersonal judgments of
utility and interpersonal comparisons,

632
00:46:06,610 --> 00:46:11,050
you get the radically redistributive
doctrine that Bentham then tries to fend

633
00:46:11,051 --> 00:46:15,430
off with his distinction between absolute
and practical equality that we talked

634
00:46:15,431 --> 00:46:16,300
about last time.

635
00:46:17,770 --> 00:46:21,070
But rolls this point
doesn't actually apply.

636
00:46:21,250 --> 00:46:26,250
The problem with neoclassical
utilitarianism is that
it takes the differences

637
00:46:26,801 --> 00:46:29,920
between individuals,
hypos seriously,

638
00:46:30,580 --> 00:46:31,900
so seriously,

639
00:46:32,320 --> 00:46:37,320
that you would say the bag lady should
starve in this example rather than have

640
00:46:38,680 --> 00:46:42,820
something redistributed
to her by the state, um,

641
00:46:42,850 --> 00:46:45,550
forcibly taken from trial.

642
00:46:46,660 --> 00:46:51,660
So classical utilitarianism ignores
the differences among individuals.

643
00:46:53,650 --> 00:46:58,650
Neoclassical utilitarianism fetishize
is the differences among people to an

644
00:46:59,681 --> 00:47:04,390
incredible extreme. So that proponents
of neoclassical utilitarianism,

645
00:47:04,391 --> 00:47:06,730
like Richard Posner and his book,

646
00:47:06,731 --> 00:47:11,731
the economics of Justice concedes that
it's a problem with you neoclassical

647
00:47:11,771 --> 00:47:12,820
utilitarianism,

648
00:47:12,821 --> 00:47:17,530
that if you have a disabled person
who's not capable of working for anybody

649
00:47:17,531 --> 00:47:20,260
else,
there's no reason or can,

650
00:47:20,261 --> 00:47:23,890
as he puts it contributing to anybody
else's, you tell any function.

651
00:47:24,770 --> 00:47:28,000
There's no reason that that person
shouldn't be allowed to die.

652
00:47:28,001 --> 00:47:32,020
And that he posed to say says, well,
that's a problem with utilitarianism.

653
00:47:32,440 --> 00:47:35,810
And he throws up his hands
and says, I don't really know
what to do about it. And,

654
00:47:35,890 --> 00:47:40,810
and he moves on. It's a deep problem
with neoclassical utilitarianism.

655
00:47:41,350 --> 00:47:45,520
But notice from the point of view
of the history of ideologies,

656
00:47:45,760 --> 00:47:50,760
what has happened in this transition
from classical to neoclassical

657
00:47:51,160 --> 00:47:52,300
utilitarianism.

658
00:47:52,690 --> 00:47:57,690
We've gone from a world in which the
doctrine of classical utilitarianism was a

659
00:47:58,421 --> 00:48:03,421
very radical idea that will legitimate
huge redistribution by the state into a

660
00:48:05,501 --> 00:48:10,501
world in which the radical fangs of
classical utilitarianism have been ripped

661
00:48:13,361 --> 00:48:14,194
out.

662
00:48:14,230 --> 00:48:19,230
And it is now a doctrine that is very
friendly to whatever status quo happens to

663
00:48:20,621 --> 00:48:23,020
be generated in a market system.

664
00:48:23,350 --> 00:48:28,350
So it ceases to be this radically
redistributive doctrine and in the process

665
00:48:30,130 --> 00:48:34,870
imports into utilitarianism,
a very robust,

666
00:48:34,990 --> 00:48:39,820
some would say hyper robust
doctrine of individual rights,

667
00:48:40,180 --> 00:48:45,180
and we'll see how that paid
out in political theory
when we come to look at John

668
00:48:45,280 --> 00:48:48,310
Stuart Mill's harm principle next Monday.
See you then.

