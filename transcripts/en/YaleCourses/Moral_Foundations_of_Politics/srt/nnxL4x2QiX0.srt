1
00:00:01,290 --> 00:00:05,820
Okay. So our task today
is to finish up teaching,

2
00:00:06,110 --> 00:00:07,830
uh,
about roles.

3
00:00:07,890 --> 00:00:12,690
And then I'm going to take
a step back and look at the,

4
00:00:13,230 --> 00:00:16,320
where we've gotten to
so far in today's cores,

5
00:00:16,321 --> 00:00:20,580
because we're really entering
a transition moment, uh,

6
00:00:21,030 --> 00:00:23,460
from the enlightenment to
the Anti Enlightenment,

7
00:00:23,461 --> 00:00:27,360
which with which is what
we'll begin with on Wednesday.

8
00:00:28,050 --> 00:00:32,880
So we had just, just to recap briefly, um,

9
00:00:33,540 --> 00:00:36,960
we had been talking about roles
as two principles of justice,

10
00:00:36,961 --> 00:00:39,690
which were really three
principles of justice.

11
00:00:39,870 --> 00:00:41,940
One for the distribution of liberties,

12
00:00:41,941 --> 00:00:45,630
which was his most expansive system,
consistent,

13
00:00:45,690 --> 00:00:50,690
a compatible with the like liberty for
one specific principle of equality,

14
00:00:51,031 --> 00:00:55,830
of opportunity,
which is to be in his lexical ranking,

15
00:00:55,831 --> 00:01:00,831
which was some reason known only to John
Rawls comes before two a and the third

16
00:01:01,441 --> 00:01:03,090
one was to a,
this,

17
00:01:03,240 --> 00:01:07,740
this so called difference principle
or what used to be called in welfare

18
00:01:07,770 --> 00:01:11,130
economics, the Maxine men
principle, which says,

19
00:01:11,131 --> 00:01:12,810
maximize the minimum.

20
00:01:12,840 --> 00:01:17,840
Cher and I had explained to you why
Rawls thinks the standpoint of justice is

21
00:01:20,521 --> 00:01:24,240
the standpoint of the
most adversely affected,

22
00:01:24,510 --> 00:01:29,280
which is not a bleeding heart idea,
but rather this universal izable idea,

23
00:01:29,550 --> 00:01:33,300
this notion that if you
can affirm a principal,

24
00:01:33,330 --> 00:01:38,330
even from the standpoint of that those
people who are most adversely affected by

25
00:01:38,611 --> 00:01:43,320
it, you'll affirm it from every
other conceivable standpoint as well.

26
00:01:43,321 --> 00:01:48,321
And so this translated into these
L-shaped indifference curves her,

27
00:01:49,080 --> 00:01:52,670
if we start with a distribution like that,
um,

28
00:01:53,280 --> 00:01:58,280
anywhere in this area would be preferred
because all roles is interested in is

29
00:01:59,581 --> 00:02:03,580
it is maximizing that distance.
Um,

30
00:02:03,930 --> 00:02:07,470
that is the, the, the size
of the share at the bottom.

31
00:02:07,500 --> 00:02:11,130
He seem different to who has that share.
Um,

32
00:02:11,160 --> 00:02:16,160
so that say an x to move from x to f of
here would be an improvement for roles

33
00:02:18,840 --> 00:02:20,790
even though, um, it's,

34
00:02:20,850 --> 00:02:25,850
it's a clearly a massive loss
to a and a huge gain to be a,

35
00:02:26,220 --> 00:02:30,840
he's not interested in that. The point
is that the distance from the, the,

36
00:02:30,841 --> 00:02:31,674
um,

37
00:02:31,920 --> 00:02:36,920
access to f is greater here than the
distance from the axis to x is there.

38
00:02:37,501 --> 00:02:40,320
And that's the only key.

39
00:02:40,350 --> 00:02:43,320
That's the only relevant consideration.

40
00:02:44,850 --> 00:02:45,960
Um,
now,

41
00:02:45,961 --> 00:02:50,961
sometimes roles is called an egalitarian
and as I began to point out to you at

42
00:02:51,661 --> 00:02:55,260
the end of Wednesday's lecture,
that's really misleading.

43
00:02:55,680 --> 00:02:58,830
His principal is rather
very undead determined.

44
00:02:59,620 --> 00:03:01,920
That is to say,
um,

45
00:03:02,890 --> 00:03:05,790
if we compare it with
the parade principle,

46
00:03:06,490 --> 00:03:10,780
it's completely contains within
it the parade oh principle.

47
00:03:11,170 --> 00:03:13,780
So that if somebody came along and said,
well,

48
00:03:13,781 --> 00:03:18,640
the best way to benefit the people
at the bottom is to have markets,

49
00:03:19,180 --> 00:03:20,860
um,
trickled down,

50
00:03:20,861 --> 00:03:25,210
let's say that that the pie will grow the
most and the people at the bottom will

51
00:03:25,211 --> 00:03:28,450
benefit the most from
a pure market system.

52
00:03:28,810 --> 00:03:31,420
Roles would say fine.
Um,

53
00:03:31,600 --> 00:03:36,600
because everything that is Paredo
superior is also rolls preferred.

54
00:03:37,210 --> 00:03:38,290
On the other hand,

55
00:03:38,650 --> 00:03:43,650
if somebody came and made the case that
heavy state intervention to redistribute

56
00:03:45,400 --> 00:03:50,320
would in fact work to the greatest
benefit of the least advantaged,

57
00:03:50,650 --> 00:03:53,290
then he would agree with that as well.

58
00:03:53,470 --> 00:03:56,960
And so you can conjure up,
um,

59
00:03:57,100 --> 00:04:01,030
egalitarian results that a compatible,
uh,

60
00:04:01,060 --> 00:04:06,060
with the roles he and scheme or
Anti-gang attarian results of further

61
00:04:07,390 --> 00:04:09,310
consideration comes in here.

62
00:04:09,311 --> 00:04:14,311
When we stopped just talking about
the worst off individual because,

63
00:04:14,740 --> 00:04:17,380
um, you can start to think about, well,

64
00:04:17,381 --> 00:04:22,060
what happens if you get a small marginal
increment for the person at the bottom,

65
00:04:22,480 --> 00:04:23,313
um,

66
00:04:23,410 --> 00:04:28,410
paid for by massive cuts on the middle
class and perhaps big benefits to the

67
00:04:29,141 --> 00:04:30,010
very wealthy.

68
00:04:30,011 --> 00:04:34,780
And I mentioned the example of
the Reagan tax cuts in the 1980s,

69
00:04:34,781 --> 00:04:39,781
which had that structure roles would
have no objection to that either,

70
00:04:40,210 --> 00:04:45,040
even though from an
egalitarian perspective that
would look like I regress of

71
00:04:45,041 --> 00:04:49,000
redistribution, uh, from
the status quo in 1980.

72
00:04:49,560 --> 00:04:53,140
Um,
so it's a very under determined principle.

73
00:04:53,141 --> 00:04:58,141
It's not necessarily egalitarian or
necessarily anti-gang attarian all it says

74
00:04:59,681 --> 00:05:04,681
is arrange things to the greatest
benefit of the person at the bottom.

75
00:05:05,440 --> 00:05:08,260
And I,
as I think I mentioned to you,

76
00:05:08,261 --> 00:05:13,261
roles took a lot of criticism in the
1970s in 1980s from people who would plow

77
00:05:14,201 --> 00:05:18,790
all the way through to page 300
and whatever it is in his book, uh,

78
00:05:18,791 --> 00:05:23,791
to learn that role says he's agnostic
between capitalism and socialism.

79
00:05:24,251 --> 00:05:26,440
And people say,
well,

80
00:05:26,620 --> 00:05:31,620
to read 300 pages of a book about justice
and discover the author is agnostic

81
00:05:31,661 --> 00:05:36,661
between the two main political ICAN
economic systems of the 20th century is an

82
00:05:37,511 --> 00:05:41,410
exactly, um, satisfying. Um,

83
00:05:41,650 --> 00:05:46,650
but roles as answer as I said is which
type of political economic system

84
00:05:47,981 --> 00:05:49,360
actually operates to.

85
00:05:49,361 --> 00:05:53,710
The benefit of the person at the
bottom is not a question for political

86
00:05:53,711 --> 00:05:54,230
theorists.

87
00:05:54,230 --> 00:05:59,230
That's a question maybe for a political
maybe for trial and error through policy

88
00:06:01,150 --> 00:06:02,930
innovation.
Um,

89
00:06:03,260 --> 00:06:07,820
it's going to have to get hammered out
in the real world of practical political

90
00:06:07,880 --> 00:06:08,601
economy.

91
00:06:08,601 --> 00:06:13,601
And so it's not a critique of
their roles in standard that,

92
00:06:14,030 --> 00:06:14,863
um,

93
00:06:16,100 --> 00:06:21,100
it's undetermined with respect to the
choice of actual political economic

94
00:06:22,191 --> 00:06:23,024
systems.

95
00:06:23,300 --> 00:06:27,380
And I think that Rawls is on
relatively firm ground there.

96
00:06:27,381 --> 00:06:30,050
We wouldn't want to think that what,

97
00:06:30,320 --> 00:06:35,270
what political economy is
the most efficient from the
standpoint of benefiting

98
00:06:35,271 --> 00:06:40,271
people at the bottom is really
a philosophical question
when clearly it is not.

99
00:06:41,660 --> 00:06:44,720
So that's the roles in story.

100
00:06:44,750 --> 00:06:48,410
I gave you the big picture
at the beginning, uh, his,

101
00:06:48,680 --> 00:06:51,380
his general conception
of distributive justice.

102
00:06:51,381 --> 00:06:56,381
And then these more specific principles
that get added in the course of trying

103
00:06:57,141 --> 00:06:59,900
to figure out, um, how the,

104
00:06:59,930 --> 00:07:04,670
the general conception of justice can,
can actually be applied.

105
00:07:05,630 --> 00:07:08,300
Now I want to take a step back.

106
00:07:08,301 --> 00:07:10,940
I'm going to come back to
roles a little bit later,

107
00:07:10,941 --> 00:07:15,860
but I want to take a step back first and,
and think about where we've come from.

108
00:07:16,600 --> 00:07:19,550
Uh, and then we'll see how roles, uh,

109
00:07:19,640 --> 00:07:22,130
leads us to think about
where we've gotten to.

110
00:07:22,850 --> 00:07:26,960
We started out this course by
talking about enlightenment,

111
00:07:26,990 --> 00:07:28,220
political theory,

112
00:07:28,700 --> 00:07:33,700
the enlightenment being a philosophical
movement that really starts in the 17th

113
00:07:33,831 --> 00:07:38,360
century but gathers steam
in the 18th century.

114
00:07:38,900 --> 00:07:43,760
And I said that fall from the point
of view of political arrangements,

115
00:07:44,000 --> 00:07:47,120
there were really two core
values of the enlightenment.

116
00:07:47,840 --> 00:07:52,840
One was to a commitment to the idea of
individual freedom as realized through a

117
00:07:53,121 --> 00:07:58,121
doctrine of individual rights as the
most important value in politics.

118
00:08:00,410 --> 00:08:05,410
And the second was a commitment to
science reason and science as the basis of

119
00:08:08,031 --> 00:08:13,031
politics rather than things that had
prevailed hit a to such as natural law or

120
00:08:14,571 --> 00:08:19,571
tradition or natural rights or religious
argument that rare that the move,

121
00:08:20,811 --> 00:08:24,110
which we source so dramatically
with Bentham but has been present,

122
00:08:24,111 --> 00:08:28,880
he won our way or another with everybody
we've looked at. The move is to say,

123
00:08:28,940 --> 00:08:33,380
no, we're not going to appeal
to tradition. We're not
going to appeal to religion.

124
00:08:33,381 --> 00:08:36,200
We're not going to appeal to
natural law and natural rights.

125
00:08:36,350 --> 00:08:39,710
We're going to appeal
to the idea of science.

126
00:08:40,280 --> 00:08:43,760
Science understood through reason.

127
00:08:44,750 --> 00:08:47,570
And so those are the two twin ideas,

128
00:08:47,571 --> 00:08:52,571
which one way or another have shaped every
single theorist that we've looked at.

129
00:08:55,830 --> 00:08:58,980
So starting with the
commitment to science,

130
00:08:59,100 --> 00:09:04,020
remember those early enlightenment
theorists were very different from modern

131
00:09:04,021 --> 00:09:09,021
enlightenment thinkers in that they
identified science with certainty.

132
00:09:10,740 --> 00:09:12,480
Remember the,
the I,

133
00:09:12,481 --> 00:09:17,481
the Cartesian idea that you have certainty
about the contents of your own mind

134
00:09:17,850 --> 00:09:20,580
or locks, uh, locks point that,

135
00:09:20,670 --> 00:09:25,670
that only we have that kind of privileged
access into the contents of our own

136
00:09:26,341 --> 00:09:30,930
soul. Remember his famous
line to the effect that,

137
00:09:30,980 --> 00:09:31,813
um,

138
00:09:32,490 --> 00:09:37,490
true and Inwood a true and
lasting conviction requires
inward persuasion of the

139
00:09:38,281 --> 00:09:41,780
mind. And that can't be
forced on anybody, uh,

140
00:09:42,060 --> 00:09:44,760
by the magistrate or,
or anybody else.

141
00:09:44,761 --> 00:09:48,750
That this is internal
certainty is so important.

142
00:09:49,230 --> 00:09:51,810
And for Hobbs, we saw that, um,

143
00:09:52,020 --> 00:09:56,190
it's this w knowledge has
its basis in willing thing.

144
00:09:56,700 --> 00:10:00,010
So, um, we got this raw,

145
00:10:00,011 --> 00:10:03,900
the curious result that he
said, you know, uh, the,

146
00:10:03,930 --> 00:10:08,930
the laws of geometry have the force
of laws because they're the product of

147
00:10:08,971 --> 00:10:11,760
wills. We make the triangle, if you like.

148
00:10:12,360 --> 00:10:16,230
And the laws of politics were like the
laws of geometry because we make the

149
00:10:16,231 --> 00:10:21,231
Commonwealth in the same way that we
make the triangle a and that therefore,

150
00:10:21,481 --> 00:10:23,520
in the early enlightenment,
there was this,

151
00:10:23,580 --> 00:10:28,580
this enormous emphasis on certainty
as the hallmark of science.

152
00:10:30,270 --> 00:10:35,250
But then we saw as we moved into the
mature enlightenment with people like John

153
00:10:35,250 --> 00:10:39,690
Stuart Mill, that actually they had
a very different view of science,

154
00:10:39,960 --> 00:10:42,420
that the hallmark of science was fallible.

155
00:10:42,430 --> 00:10:45,180
Isn't that everything we think,

156
00:10:45,181 --> 00:10:48,180
we know a is subject to doubt.

157
00:10:48,690 --> 00:10:51,240
There isn't anything in, in, uh,

158
00:10:51,241 --> 00:10:56,241
reasoning about the real world that
meets the Cartesian a criterion of being

159
00:10:57,510 --> 00:11:02,510
impossible to doubt what we think we
know is always revisable in the light of

160
00:11:02,761 --> 00:11:04,680
more evidence, uh, and,

161
00:11:05,070 --> 00:11:10,070
and bed scientific investigation and
mills defense of the marketplace,

162
00:11:10,111 --> 00:11:12,580
of ideas and of competition.
Uh,

163
00:11:12,581 --> 00:11:17,100
and argument was precisely
to encourage that. And so,

164
00:11:17,250 --> 00:11:22,120
um, the longs chapter on freedom
of thought and action in,

165
00:11:22,140 --> 00:11:25,440
uh,
in on liberty is really about,

166
00:11:25,710 --> 00:11:29,550
remember how we get from, from, uh,

167
00:11:29,580 --> 00:11:34,500
freedom to utility via a system
that allows the truth to come out,

168
00:11:34,800 --> 00:11:39,800
name me a system in which ideas have
to confront contrarian ideas in the

169
00:11:41,460 --> 00:11:46,280
marketplace, uh, of public
speech. And that, that, that, uh,

170
00:11:46,290 --> 00:11:50,190
science is, is it really
a process, uh, which,

171
00:11:50,370 --> 00:11:55,370
which makes it possible or more possible
than any other process for us to

172
00:11:55,811 --> 00:11:57,250
approximate the truth.

173
00:11:57,250 --> 00:12:01,990
But that's something very different
from having this notion of certainty.

174
00:12:02,560 --> 00:12:07,050
So the commitment to science becomes
the commitment to Falabella awesome.

175
00:12:07,660 --> 00:12:10,000
Uh, and the commitment
to assist them, which,

176
00:12:10,180 --> 00:12:15,070
which has room in it
for, um, experimental,

177
00:12:15,150 --> 00:12:15,640
uh,

178
00:12:15,640 --> 00:12:20,640
searching after the truth as an as
a perpetual feature of human social

179
00:12:22,510 --> 00:12:23,470
association.

180
00:12:24,760 --> 00:12:29,740
Now when we look at Rawls and Nozick
and the social contract theorists,

181
00:12:30,100 --> 00:12:35,080
um,
we find in at least in some respects,

182
00:12:35,260 --> 00:12:39,370
it's sort of throw back to the
early enlightenment at least in the,

183
00:12:39,371 --> 00:12:43,360
in the Nozick and the roles that you've,
we've talked about today.

184
00:12:44,080 --> 00:12:46,160
That is to say,
um,

185
00:12:46,600 --> 00:12:51,580
they are looking for a unique
answer to the question,

186
00:12:51,610 --> 00:12:56,610
what principles would people agree upon
if they would designing society of fresh

187
00:12:59,830 --> 00:13:01,120
and the,

188
00:13:01,121 --> 00:13:06,121
this hypothetical social contract a so
called depends on the idea that there's a

189
00:13:06,911 --> 00:13:09,970
unique answer to that question,
right?

190
00:13:09,971 --> 00:13:13,720
So they can see that there never
was a social contract, but they say,

191
00:13:13,721 --> 00:13:16,900
suppose we were designing
the rules from scratch,

192
00:13:17,200 --> 00:13:21,880
what rules would we design even though
no society was having created in that

193
00:13:21,881 --> 00:13:26,110
way? And the the, the thought was, well,

194
00:13:27,220 --> 00:13:29,200
if there's an answer to that question,

195
00:13:29,230 --> 00:13:33,520
if there's a definitive answer to that
question, then we have a standard,

196
00:13:33,521 --> 00:13:38,200
a yardstick for measuring actual
institutional arrangements.

197
00:13:38,470 --> 00:13:43,470
That is to say societies that come closer
to that standard can be judged better

198
00:13:44,680 --> 00:13:46,840
than societies that are further from it.

199
00:13:47,320 --> 00:13:51,430
And as societies evolve over time,
if they evolve toward it,

200
00:13:51,431 --> 00:13:54,700
there'll be improving and if
they evolved away from it,

201
00:13:54,730 --> 00:13:56,530
they would be getting worse.

202
00:13:56,740 --> 00:14:01,740
So this was the Cancerian or I called neo
con t and aspiration to come up with a

203
00:14:03,281 --> 00:14:08,200
standard which any rational
person must on reflection, affirm.

204
00:14:09,040 --> 00:14:11,230
And if you can say what that is,

205
00:14:11,980 --> 00:14:16,980
then you have your yardstick by
reference to which you can look at actual

206
00:14:17,740 --> 00:14:22,740
political and social arrangements and
it might be worth pausing just for a

207
00:14:24,041 --> 00:14:27,490
second to notice that
Emmanuel concave himself,

208
00:14:27,520 --> 00:14:28,930
who we did and read it in this course.

209
00:14:28,930 --> 00:14:31,630
So you'll have to just
take it on faith from me.

210
00:14:32,140 --> 00:14:37,140
Conn himself was deeply skeptical that
that could be done because he thought

211
00:14:37,511 --> 00:14:41,650
that social and political arrangements
are inevitably dependent on empirical

212
00:14:41,651 --> 00:14:42,700
considerations.

213
00:14:42,701 --> 00:14:47,701
And you're not going to get you an
the universal laws about those things.

214
00:14:48,280 --> 00:14:53,280
So conn himself would have been skeptical
of the roles in a project you to say,

215
00:14:54,740 --> 00:14:59,740
you're not going to be able
to get empirical propositions
about the organization

216
00:15:00,561 --> 00:15:05,561
of society that are going to rise to
this level of a categorical imperative.

217
00:15:06,320 --> 00:15:11,320
And I think that that one implication
about discussion of roles is that con was

218
00:15:12,201 --> 00:15:15,080
in fact right,
that um,

219
00:15:16,190 --> 00:15:18,860
for example, um, roles,

220
00:15:18,861 --> 00:15:23,861
a story about protecting the position
of the person at the bottom as tsums an

221
00:15:26,721 --> 00:15:29,420
enormous degree of risk aversion.

222
00:15:30,290 --> 00:15:33,290
Now he had an answer for why he said,
well,

223
00:15:33,320 --> 00:15:38,320
there's no necessary relationship between
the level of economic development and

224
00:15:38,481 --> 00:15:41,300
what the,
the fortunes of the person at the bottom.

225
00:15:41,690 --> 00:15:44,240
So you had that graves risk assumption,

226
00:15:44,900 --> 00:15:49,900
but we saw that even that
becomes problematic if you
think about very marginal

227
00:15:51,351 --> 00:15:55,490
improvements in the condition that the
person at the bottom coming at enormous

228
00:15:55,491 --> 00:15:59,540
costs for the person a
little bit higher up.

229
00:15:59,810 --> 00:16:04,810
It's not at all clear that the rational
thing to do would be always to add,

230
00:16:05,871 --> 00:16:10,850
no matter what cost to preserve the
condition of the person at the bottom.

231
00:16:11,360 --> 00:16:16,360
Important a article about this
published by John Harsanyi in 1975,

232
00:16:18,200 --> 00:16:21,380
I'll use that. If you
really think that, um,

233
00:16:21,410 --> 00:16:23,750
people have a rational view of risk,

234
00:16:24,020 --> 00:16:28,880
it would make more sense to choose
utilitarian Nissam behind the veil of

235
00:16:28,910 --> 00:16:33,050
ignorance. Then it would
make to choose, uh,

236
00:16:33,110 --> 00:16:36,150
the Rawlsian commitment to,
uh,

237
00:16:36,200 --> 00:16:39,650
maximizing the position of
the person at the bottom.

238
00:16:40,160 --> 00:16:43,280
Now it's not obvious
that Harsanyi is right,

239
00:16:43,880 --> 00:16:47,900
but nor is it obvious that rolls is right.
And once you,

240
00:16:47,930 --> 00:16:51,560
once you make that, um, admission,

241
00:16:51,920 --> 00:16:55,180
then you no longer have a
unique answer. You say, well,

242
00:16:55,220 --> 00:17:00,220
we plugged one set of
psychological assumptions in
about risk and we get roles,

243
00:17:00,530 --> 00:17:05,030
we play a different set of assumptions
of in about risk and we get Harsanyi.

244
00:17:05,420 --> 00:17:10,420
And so it's all being driven by the
assumptions about human psychology that we

245
00:17:10,761 --> 00:17:15,050
plug into the model.
And of course, once you,

246
00:17:15,350 --> 00:17:19,400
you recognize that,
then you don't have a unique answer.

247
00:17:19,950 --> 00:17:20,783
Um,

248
00:17:20,870 --> 00:17:25,870
another way of putting it is that the
young roles was rather naive about what is

249
00:17:26,361 --> 00:17:29,870
uncontroversial in economics,

250
00:17:29,900 --> 00:17:34,900
psychology and sociology that is rolls
made a distinction between the laws of

251
00:17:36,081 --> 00:17:39,410
psychology and economics as he put them,
uh,

252
00:17:39,440 --> 00:17:43,370
which we do have knowledge of behind
the veil of ignorance in which he was

253
00:17:43,371 --> 00:17:45,200
treating as uncontroversial.

254
00:17:45,440 --> 00:17:50,440
And then the specific knowledge that we
have about our particular life plans,

255
00:17:50,641 --> 00:17:54,150
goals and so on,
which he kept hidden from us.

256
00:17:54,390 --> 00:17:59,390
But it turns out that there are very few
uncontroversial assumptions about human

257
00:18:00,451 --> 00:18:05,451
psychology or economics and so
you can't get unique answers.

258
00:18:06,690 --> 00:18:11,580
And indeed, uh, if we own a
restrict ourselves to economics,

259
00:18:11,970 --> 00:18:16,970
one of the most interesting developments
in economics of the past decade is

260
00:18:17,161 --> 00:18:22,161
precisely the turn away from standard
economistic assumptions into the field of

261
00:18:24,421 --> 00:18:27,660
psychology to see how
preferences are formed,

262
00:18:27,870 --> 00:18:31,650
why people have the risk
profiles that they do and so on.

263
00:18:31,651 --> 00:18:36,651
And much of modern behavioral economics
regards as subjects with study rather

264
00:18:38,311 --> 00:18:41,940
than the thing,
then a axiomatic assumptions,

265
00:18:42,150 --> 00:18:45,360
the sorts of things roles
wanted to work with.

266
00:18:45,870 --> 00:18:49,950
So there isn't a unique answer.
Now,

267
00:18:50,610 --> 00:18:52,110
as it turns out,

268
00:18:52,950 --> 00:18:55,890
as roles became a older,

269
00:18:56,070 --> 00:19:01,070
he realized that this neo con tn venture
was belt upon a hell of sand and that

270
00:19:04,680 --> 00:19:07,710
he wasn't going to be able to make cons,
ethics,

271
00:19:08,190 --> 00:19:13,190
do the work and social contract theory
that traditionally had been done by

272
00:19:14,130 --> 00:19:18,270
natural law that for the
reason that I just gave you,

273
00:19:18,271 --> 00:19:21,960
you're not going to actually be able
to get unique results out of it.

274
00:19:22,710 --> 00:19:27,270
And so the mature rolls made
a different kind of move,

275
00:19:27,570 --> 00:19:32,040
which in some ways is an even bigger
retreat from the early enlightenment then

276
00:19:32,041 --> 00:19:34,260
was males retreat to Falabella.
Awesome.

277
00:19:34,770 --> 00:19:39,270
And that is the move that comes
under the heading of political,

278
00:19:39,330 --> 00:19:41,040
not metaphysical.

279
00:19:41,820 --> 00:19:46,820
And that's the name of the sub title
of the article I had you reading about,

280
00:19:48,030 --> 00:19:49,770
uh,
for today.

281
00:19:50,340 --> 00:19:53,550
And so he is the intuition.

282
00:19:55,050 --> 00:19:58,700
It's counterintuitive and Ted
Hughes thinking it through. Um,

283
00:19:58,770 --> 00:20:03,570
but then I think it's actually
quite a powerful intuition. Um,

284
00:20:04,410 --> 00:20:06,570
and I'm going to,
I'm going to explain it to you actually,

285
00:20:06,571 --> 00:20:08,490
not by reference to roles,

286
00:20:08,760 --> 00:20:13,760
but by another person called Cass Sunstein
who's a lawyer at the University of

287
00:20:15,210 --> 00:20:16,590
Chicago who might want,

288
00:20:16,620 --> 00:20:20,430
you might hear a lot about him cause
he's on the long short list, uh,

289
00:20:20,610 --> 00:20:23,700
for Obama picks for the Supreme Court.
Uh,

290
00:20:23,701 --> 00:20:26,940
so cass Sunstein might be
in the news stay tuned,

291
00:20:27,780 --> 00:20:31,830
but Sunstein has had a different slogan,
not political,

292
00:20:31,831 --> 00:20:33,090
not metaphysical,

293
00:20:33,480 --> 00:20:38,480
but rather what he called a theory of
in incompletely theorized agreement.

294
00:20:40,590 --> 00:20:45,240
Now let these, this is not an elegant
term, but let me give you the, the,

295
00:20:45,300 --> 00:20:47,710
um,
the intuition here.

296
00:20:48,490 --> 00:20:53,470
We often think when we think about
political disagreement, we often think,

297
00:20:53,471 --> 00:20:53,621
well,

298
00:20:53,621 --> 00:20:57,940
people are Kinda agree on very general
things and the devil is in the details.

299
00:20:58,270 --> 00:21:01,210
People can agree, you
know, that read is good,

300
00:21:01,930 --> 00:21:05,500
but they can't agree on what is
actually required for freedom.

301
00:21:05,740 --> 00:21:10,720
When you get down to brass
tacks of arguing about policies,

302
00:21:10,721 --> 00:21:11,554
you know,

303
00:21:11,630 --> 00:21:12,463
mmm.

304
00:21:13,770 --> 00:21:18,770
Does having universal is this freedom
require people have universal health care?

305
00:21:18,811 --> 00:21:22,380
Some people say yes, some
people say no. All right. So,

306
00:21:22,500 --> 00:21:27,500
so one view of political disagreement
is we can agree at a very high altitude,

307
00:21:27,840 --> 00:21:30,780
but then when you start to get into,
uh,

308
00:21:30,810 --> 00:21:35,430
the dirty particulars of everyday
life, uh, then we can't agree.

309
00:21:36,400 --> 00:21:36,820
Okay.

310
00:21:36,820 --> 00:21:41,820
Sunstein and roles in his political
not metaphysical mode have the almost

311
00:21:43,150 --> 00:21:47,670
opposite tuition to that.
And so, so here the, the,

312
00:21:47,700 --> 00:21:50,760
the sort of example would be,
well,

313
00:21:54,070 --> 00:21:54,580
okay,

314
00:21:54,580 --> 00:21:55,840
think about a,

315
00:21:56,290 --> 00:22:01,290
a faculty in the university trying to
decide whether or not a junior person

316
00:22:04,271 --> 00:22:05,290
should get tenure.

317
00:22:06,730 --> 00:22:11,730
They might be able to agree
that the person should get
tenure without being able

318
00:22:12,701 --> 00:22:17,320
to agree in a million years about
why the person should get tenure.

319
00:22:18,430 --> 00:22:22,450
Or when we think of Congress
passing a piece of legislation,

320
00:22:23,230 --> 00:22:26,930
when we think of Congress
passing the healthcare bill, uh,

321
00:22:27,320 --> 00:22:32,320
that just went through a warts
and all through the house.

322
00:22:33,350 --> 00:22:34,183
MMM.

323
00:22:35,730 --> 00:22:37,500
You know,
you get the votes,

324
00:22:38,370 --> 00:22:43,370
but if those people have to agree
upon why they were voting for,

325
00:22:44,730 --> 00:22:46,530
they couldn't, they couldn't
agree in the mail. Yeah.

326
00:22:46,531 --> 00:22:51,530
If they all have different reasons for
why they're voting for it or why they're

327
00:22:51,540 --> 00:22:54,300
voting against it for that matter.
Right.

328
00:22:54,750 --> 00:22:58,920
So the notion of incompletely
theorized agreement is, you know,

329
00:22:58,921 --> 00:23:00,360
why we don't care,

330
00:23:02,620 --> 00:23:07,450
we don't care or rolls this idea
of political, not metaphysical,

331
00:23:07,480 --> 00:23:08,313
it's,

332
00:23:08,620 --> 00:23:13,620
it's the question is what political
arrangements would people with very

333
00:23:14,351 --> 00:23:19,180
different values, commitments,
worldviews, metaphysical systems,

334
00:23:19,480 --> 00:23:20,950
what would they agree on?

335
00:23:21,460 --> 00:23:24,700
What would be to use
another one of Rose's terms?

336
00:23:24,940 --> 00:23:27,790
What would be the overlapping consensus?

337
00:23:30,020 --> 00:23:31,610
Thank you. Sort of, you know,

338
00:23:32,090 --> 00:23:36,020
a big Venn diagram was where
you've gotten a lot of circles.

339
00:23:36,350 --> 00:23:38,150
Mostly not overlapping,

340
00:23:38,151 --> 00:23:41,750
but they all overlap at
[inaudible] in one area.

341
00:23:43,040 --> 00:23:43,310
Okay.

342
00:23:43,310 --> 00:23:46,490
That's the overlapping.
Okay.

343
00:23:47,300 --> 00:23:50,330
And we don't care about the
parts that don't overlap.

344
00:23:51,920 --> 00:23:53,720
So we,
it's,

345
00:23:53,740 --> 00:23:58,220
it's much less rationalistic.
Lee ambitious.

346
00:23:58,700 --> 00:24:01,190
Cause now if you think back
to the first principle,

347
00:24:01,430 --> 00:24:06,430
we don't have to say that
the fundamentalist would
agree that she or he has as

348
00:24:07,431 --> 00:24:08,360
much free,

349
00:24:08,420 --> 00:24:13,420
has more freedom in a disestablished
church regime than the non fundamentalist

350
00:24:14,301 --> 00:24:18,830
would have in the fundamentalist regime.
All we have to say is that the,

351
00:24:18,831 --> 00:24:22,610
the fundamentalist accepts this.
We don't know why.

352
00:24:23,240 --> 00:24:24,410
We don't care why.

353
00:24:25,460 --> 00:24:29,450
So that's the notion of incompletely
theorized agreement or political,

354
00:24:29,451 --> 00:24:34,030
not metaphysical. We're just
going to look for water where it,

355
00:24:34,110 --> 00:24:39,110
what is the overlapping consensus for
people with very different worldviews,

356
00:24:40,010 --> 00:24:44,390
metaphysical systems,
beliefs, et cetera. Okay?

357
00:24:44,570 --> 00:24:47,840
And so, so another way you
can think about this is,

358
00:24:50,680 --> 00:24:51,400
okay,

359
00:24:51,400 --> 00:24:52,840
instead of saying,

360
00:24:53,980 --> 00:24:58,360
first I'm going to convince you of my
metaphysics and epistemology and my theory

361
00:24:58,361 --> 00:25:02,410
of science, and then when I've
persuaded you about all of those things,

362
00:25:02,411 --> 00:25:05,620
I'm going to show you how
my political theory follows

363
00:25:08,920 --> 00:25:12,850
on this. Sunstein or mature
role's view. That's a Mug's game.

364
00:25:12,851 --> 00:25:16,000
You never going to do it because people
are never going to agree about all of

365
00:25:16,001 --> 00:25:18,670
those things.
And more important for politics.

366
00:25:18,700 --> 00:25:21,730
We know need them to agree
about all of those things.

367
00:25:22,360 --> 00:25:26,560
All we need to do is find
the overlapping consensus,

368
00:25:27,990 --> 00:25:28,823
uh,

369
00:25:30,030 --> 00:25:32,940
uh, that they will affirm. So they might,

370
00:25:32,970 --> 00:25:36,900
they might affirm a series of,
of political arrangements,

371
00:25:36,901 --> 00:25:41,850
institutions for very different reasons
from one another. And it doesn't matter.

372
00:25:42,420 --> 00:25:46,350
We don't need it to be
any more robust than that.

373
00:25:46,620 --> 00:25:50,100
And so that's where the
mature roles winds up.

374
00:25:50,670 --> 00:25:54,180
And as we'll see in the final
lectures of this course,

375
00:25:54,600 --> 00:25:58,440
there is a certain
democratic element to this,

376
00:25:59,130 --> 00:26:03,480
but it's under theorized in roles.
And I, and the, the analogy I will,

377
00:26:03,510 --> 00:26:08,510
I'll just mention and I'll come back to
it later in the course is the analogy is

378
00:26:08,941 --> 00:26:10,080
the secret ballot.

379
00:26:11,670 --> 00:26:11,990
Okay.

380
00:26:11,990 --> 00:26:16,990
We don't require people to give
reasons for the way in which they vote.

381
00:26:18,830 --> 00:26:23,830
They can have reasons for choosing the
same candidate as we choose that we would

382
00:26:24,741 --> 00:26:27,950
regard as completely idiotic.
We don't care.

383
00:26:29,090 --> 00:26:29,700
Okay.

384
00:26:29,700 --> 00:26:32,970
Right. So there's this,
the, this political,

385
00:26:32,971 --> 00:26:37,680
not metaphysical move or the
incompletely theorized agreement move is,

386
00:26:37,730 --> 00:26:42,730
is analogous to the idea of the secret
ballot in that we become much less of,

387
00:26:43,621 --> 00:26:48,621
people don't have to have good
reasons for voting the way they do

388
00:26:50,910 --> 00:26:54,690
their reasons are there private business.
Okay.

389
00:26:54,691 --> 00:26:56,220
And so the political,

390
00:26:56,221 --> 00:26:59,940
not metaphysical move bills
on that kind of intuition.

391
00:27:00,390 --> 00:27:05,390
And obviously it's a huge retreat from
the original enlightenment motivation to

392
00:27:08,191 --> 00:27:12,000
get principles that must
follow scientifically.

393
00:27:15,250 --> 00:27:16,083
MMM.

394
00:27:16,800 --> 00:27:19,590
For any clear headed thinking person.

395
00:27:20,130 --> 00:27:22,290
So the mature roles is a,

396
00:27:22,410 --> 00:27:27,410
is a kind of extreme
retreat you might say,

397
00:27:27,720 --> 00:27:30,810
even though the young
roles is, is uh, you know,

398
00:27:30,811 --> 00:27:34,290
an enlightenment thinker with
all the zeal of a Jeremy Bentham,

399
00:27:34,291 --> 00:27:38,970
the mature roles really gives up
on the enlightenment project. Um,

400
00:27:39,150 --> 00:27:41,400
and of course you then
get into the question,

401
00:27:42,960 --> 00:27:47,960
he still thinks his is three
principles would be affirmed.

402
00:27:48,660 --> 00:27:51,730
He thinks these three principles
are part of the, of, of,

403
00:27:51,740 --> 00:27:55,080
or are part of this overlapping consensus.

404
00:27:55,590 --> 00:28:00,330
But he has no way of knowing that that in
fact is true. Uh, it just an assertion,

405
00:28:00,800 --> 00:28:05,160
uh, that it's true and we,
it's not necessarily the case.

406
00:28:05,640 --> 00:28:06,980
So,
um,

407
00:28:07,290 --> 00:28:11,850
what I'm going to suggest to you in later
lectures that is that rolls actually

408
00:28:11,851 --> 00:28:16,650
retreats too far from the enlightenment
project and that there's a way of

409
00:28:16,651 --> 00:28:21,600
thinking about the mature enlightenment
that's consistent with, um,

410
00:28:22,440 --> 00:28:22,501
uh,

411
00:28:22,501 --> 00:28:27,501
democratic political outlook that
doesn't give up so completely as the

412
00:28:28,080 --> 00:28:28,591
political,

413
00:28:28,591 --> 00:28:33,120
not metaphysical view dies
on the alignment project.

414
00:28:33,150 --> 00:28:34,860
But that's for the future.

415
00:28:35,190 --> 00:28:40,190
Let's first focus on the other
element of the enlightenment.

416
00:28:40,261 --> 00:28:44,040
I said the, the, the one was
this commitment to science.

417
00:28:44,041 --> 00:28:49,041
And we've seen how that played out now
from the 17th century to the late 20th

418
00:28:49,801 --> 00:28:53,070
century in this gradual
re this affirmation.

419
00:28:53,071 --> 00:28:55,620
But this gradual retreat from certainty

420
00:28:57,820 --> 00:29:02,820
that mark the march from the
early enlightenment thinkers
through mail two roles

421
00:29:03,850 --> 00:29:07,180
Sunstein and others.
But now let's focus on the idea,

422
00:29:07,390 --> 00:29:10,300
the normative idea of individual rights.

423
00:29:10,301 --> 00:29:14,590
Remember we said that,
um,

424
00:29:14,950 --> 00:29:16,330
the most input,

425
00:29:16,360 --> 00:29:20,710
the sooner bonem the most
important value of the,

426
00:29:20,890 --> 00:29:22,410
of the enlightenment.
This,

427
00:29:22,420 --> 00:29:26,350
this idea of individual
freedom recognized or,

428
00:29:26,590 --> 00:29:30,760
or institutionalized by
doctrine of individual rights,

429
00:29:30,761 --> 00:29:34,660
the rights of the individual
are somehow sacrosanct.

430
00:29:35,470 --> 00:29:38,050
And we saw that in the
early enlightenment.

431
00:29:38,980 --> 00:29:43,960
This had a theological basis.
Remember I said to you that,

432
00:29:43,990 --> 00:29:44,823
uh,

433
00:29:45,190 --> 00:29:50,190
lock was tormented by the theological
controversy between the two sides.

434
00:29:51,130 --> 00:29:55,810
Uh, some of them said, uh, uh,

435
00:29:56,050 --> 00:30:00,850
God is omnipotent. But if you said
God is omnipotent, that seemed to,

436
00:30:00,940 --> 00:30:01,300
uh,

437
00:30:01,300 --> 00:30:05,650
undermine the idea that the laws of
nature could be timeless because if God is

438
00:30:05,651 --> 00:30:10,060
omnipotent, he could chai it, decided
to change them tomorrow. So, uh,

439
00:30:10,150 --> 00:30:14,680
either God is omnipotent or the laws
of nature of time has been not both.

440
00:30:14,680 --> 00:30:15,970
And he wrestled with that.

441
00:30:15,971 --> 00:30:20,360
If you've gone and read his essays on
the law of nature, published in 1660,

442
00:30:20,361 --> 00:30:23,680
you see him really tormenting himself.
But at the end of his life,

443
00:30:23,681 --> 00:30:28,420
he comes down firmly on the,
on what we call the will based theory,

444
00:30:28,630 --> 00:30:33,370
the idea that something can be a law
unless it's a product of a well. And so,

445
00:30:33,420 --> 00:30:34,030
um,

446
00:30:34,030 --> 00:30:39,030
we'll go with the omnipotence and let
the omniscience fall by and let the

447
00:30:39,821 --> 00:30:44,440
universalism fall by the wayside.
And so that was the idea,

448
00:30:44,650 --> 00:30:46,060
uh, of, uh,

449
00:30:46,390 --> 00:30:51,390
God owns his creation because he made
it and God knows his creation because he

450
00:30:51,941 --> 00:30:52,774
made it.

451
00:30:53,200 --> 00:30:58,200
And then the move lock makes is that God
gave us the capacity to make things for

452
00:30:59,861 --> 00:31:02,270
ourselves. We become,
as he called, you know,

453
00:31:02,390 --> 00:31:07,390
miniature gods that so long as we act
within the constraints of the law of

454
00:31:07,721 --> 00:31:10,220
nature, we can behave, uh,

455
00:31:10,330 --> 00:31:15,330
in our realm in a way that's an analogy
of the way God behaves in his realm.

456
00:31:15,820 --> 00:31:17,690
We can create,
uh,

457
00:31:17,890 --> 00:31:22,890
things over which we have make
us knowledge just as God created,

458
00:31:23,460 --> 00:31:25,240
make us knowledge,
uh,

459
00:31:26,320 --> 00:31:29,440
and rights of proprietorship
over his creation.

460
00:31:29,770 --> 00:31:32,060
This idea that we are miniature God.

461
00:31:33,040 --> 00:31:36,160
And then we saw what happened
to that idea, that work.

462
00:31:36,161 --> 00:31:41,020
We called it the workmanship model. We saw
what had happened, what happened to it.

463
00:31:41,460 --> 00:31:46,330
Um, over the course of the next
several centuries in particular,

464
00:31:46,331 --> 00:31:48,610
we saw that beginning with marks.

465
00:31:48,940 --> 00:31:53,940
What you get is an attempt to secularize
the workmanship idea that is to say,

466
00:31:55,481 --> 00:32:00,481
to cut it loose from its theological
moorings but still affirm the basic

467
00:32:01,360 --> 00:32:05,590
structure of the idea that
making confers ownership.

468
00:32:07,510 --> 00:32:11,160
And um,
we saw that Mark's,

469
00:32:11,161 --> 00:32:16,161
his version of that ran into trouble
because he wanted to say only the worker

470
00:32:17,530 --> 00:32:22,530
makes things when in fact
we saw the capitalist also
contributes to the value of

471
00:32:23,051 --> 00:32:27,730
things. And then we looked at the feminist
critique of marks, which was, well,

472
00:32:27,731 --> 00:32:28,090
yes.

473
00:32:28,090 --> 00:32:31,990
And the stay at home spouse contributes
to the value of what the work can makes.

474
00:32:32,260 --> 00:32:32,951
And indeed,

475
00:32:32,951 --> 00:32:37,090
even perhaps the Sunday school teacher
who drum the work ethic into the worker

476
00:32:37,250 --> 00:32:41,090
can contributes to the value of
what the word come makes and so on.

477
00:32:41,091 --> 00:32:45,680
So that if you have this idea
that making confers ownership,

478
00:32:46,130 --> 00:32:50,270
you're going to get a complicated
web of overlapping and indecipherable

479
00:32:50,271 --> 00:32:51,320
entitlements,

480
00:32:51,440 --> 00:32:56,440
not any clean argument of the sort that
Marx's theory of exploitation aspired to

481
00:32:57,621 --> 00:33:01,370
be. And so, um, we,

482
00:33:01,400 --> 00:33:05,090
we saw that if you look
at the Marxist tradition,

483
00:33:05,300 --> 00:33:07,910
they eventually give up on all of that.

484
00:33:07,970 --> 00:33:11,930
People like John Roemer and
Yon Elster and Jerry Cohen.

485
00:33:12,200 --> 00:33:14,090
And instead just turn,

486
00:33:14,150 --> 00:33:18,950
they give up on the idea of workmanship
and turn to an argument arguments about

487
00:33:18,951 --> 00:33:20,900
power.
Um,

488
00:33:21,380 --> 00:33:26,380
but most people wouldn't find that
entirely satisfying because most people do

489
00:33:28,671 --> 00:33:32,510
want at some level to link
what we get to what we do.

490
00:33:33,230 --> 00:33:35,620
People, I'm not, even if the,

491
00:33:35,710 --> 00:33:40,070
the notion of workmanship is problematic.

492
00:33:40,310 --> 00:33:42,980
Most people don't want
to give it up entirely.

493
00:33:43,460 --> 00:33:48,460
And I think you see this very
dramatically with roles because roles

494
00:33:50,420 --> 00:33:55,420
takes the idea of workmanship apart in
a hardheaded way that nobody before him

495
00:33:58,101 --> 00:34:01,420
ever did. He, and this is, this is the,

496
00:34:01,530 --> 00:34:06,530
the debate Irie I was referring to last
Wednesday about nature and nurture and

497
00:34:07,641 --> 00:34:10,280
moral arbitrariness.
Just to remind you,

498
00:34:10,850 --> 00:34:15,850
there's this huge debate it's gone on
for 150 years are the differences between

499
00:34:16,011 --> 00:34:18,230
us.
The result of nature will,

500
00:34:18,231 --> 00:34:22,620
are they the result of nurture and
roles makes the point, you know what,

501
00:34:22,710 --> 00:34:27,110
it doesn't matter. It doesn't make
any difference because in either case,

502
00:34:28,190 --> 00:34:33,080
these differences are morally
arbitrary. Right? Remember this? Yeah.

503
00:34:33,800 --> 00:34:35,900
So,
um,

504
00:34:36,470 --> 00:34:41,240
whether I'm a good athlete because of
my genes or whether I'm a good athlete

505
00:34:41,241 --> 00:34:43,850
because of the way I was
raised is immaterial.

506
00:34:44,150 --> 00:34:49,150
I did nothing to have certain genes
or to be raised in a certain way.

507
00:34:49,640 --> 00:34:54,590
Uh, and I didn't even make choices
that lead to those results.

508
00:34:54,591 --> 00:34:59,300
So any benefits I'd get a morally
arbitrary. That was the notion,

509
00:34:59,660 --> 00:35:04,310
uh, that that roles, uh, brought to bear.

510
00:35:04,700 --> 00:35:05,960
But then the question is,
well,

511
00:35:05,961 --> 00:35:10,460
why should anyone be
entitled to what they make?

512
00:35:11,710 --> 00:35:13,040
And,
and you know,

513
00:35:13,041 --> 00:35:17,540
roles is really pushed in the
direction of a kind of socialization of

514
00:35:17,541 --> 00:35:22,010
capacities, strategy as I call
it. It, that piece that I had,

515
00:35:22,011 --> 00:35:23,000
you read it.

516
00:35:24,260 --> 00:35:27,950
But it's very unsatisfying
because, you know, if,

517
00:35:28,010 --> 00:35:31,850
if I spend five years writing a book,
uh,

518
00:35:32,000 --> 00:35:35,210
and you come along and say, well, you're
not entitled to that book. You didn't,

519
00:35:35,211 --> 00:35:39,170
you didn't, you don't have any
special claim on the capacities.

520
00:35:39,180 --> 00:35:42,530
You I want to get really bad. You know,
I worked really hard on that. I wanted,

521
00:35:42,540 --> 00:35:46,860
it's mine. You know, who do you
take it away, right? So the,

522
00:35:46,861 --> 00:35:51,600
so even if you can't give a good
philosophical defense of this workmanship

523
00:35:51,601 --> 00:35:56,340
ideal, people are deeply
unwilling to let go of it.

524
00:35:57,100 --> 00:36:01,190
Okay. Indeed. So it's raw,
steeply, unwilling to let go of it.

525
00:36:01,760 --> 00:36:05,720
So roles and we walked through this on
Wednesday, but I'll just remind you,

526
00:36:05,721 --> 00:36:10,330
I'm it again.
Rawls makes a distinction between,

527
00:36:11,330 --> 00:36:14,890
um,
the capacities that we have and they use,

528
00:36:14,891 --> 00:36:19,540
we choose to make or those capacities,
but that's not very,

529
00:36:19,660 --> 00:36:22,840
uh, you know, the, the,
the notion was so, um,

530
00:36:22,870 --> 00:36:27,870
if we both have the same Iq but one of
us chooses to work and the artist chooses

531
00:36:27,911 --> 00:36:30,250
to sit on the couch watching ESPN,

532
00:36:30,640 --> 00:36:34,600
the one who works should get
more because they chose to work.

533
00:36:35,230 --> 00:36:39,610
And then the person who sits on the couch
should get less because they chose not

534
00:36:39,611 --> 00:36:40,710
to work.
Um,

535
00:36:41,230 --> 00:36:45,430
but that doesn't really work for roles
because once you've made the move into

536
00:36:45,431 --> 00:36:50,080
this a land of moral arbitrary ness,

537
00:36:50,800 --> 00:36:55,800
the difference differences in weakness
of the will are themselves distributed in

538
00:36:57,521 --> 00:37:02,290
morally arbitrary ways.
So perhaps, uh, you know,

539
00:37:02,291 --> 00:37:02,570
the,

540
00:37:02,570 --> 00:37:07,390
the person who works all day had the
work ethic drummed into them a mile a

541
00:37:07,390 --> 00:37:11,500
minute by some hot Sunday school
teacher or very involved parent.

542
00:37:11,770 --> 00:37:14,770
Whereas the one who winds up
sitting on the couch all day,

543
00:37:14,950 --> 00:37:18,400
didn't their parent, their
father was, you know,

544
00:37:18,401 --> 00:37:23,150
off stoned all day or something when
they were supposed to be being, uh,

545
00:37:23,890 --> 00:37:24,850
taught the work ethic.

546
00:37:25,360 --> 00:37:29,740
Well if the capacities
themselves are morally arbitrary,

547
00:37:29,741 --> 00:37:34,510
then the differences in the capacity to
use the capacities is so you're going to

548
00:37:34,511 --> 00:37:37,000
get obviously an infinite regress.

549
00:37:37,300 --> 00:37:42,300
So roles tries to kind of build a moat
around this implication of his argument,

550
00:37:42,881 --> 00:37:45,100
but it doesn't work.
And if you,

551
00:37:45,101 --> 00:37:49,840
if we had time to talk about
other theorists in this tradition,

552
00:37:50,050 --> 00:37:51,460
you'd find the same thing.

553
00:37:51,460 --> 00:37:56,460
I'll just mention the example of Ronald
Dworkin whom who also liked roles has a

554
00:37:56,891 --> 00:38:01,810
resources to view and also like rolls
sees that the differences between us,

555
00:38:02,020 --> 00:38:05,200
a morally arbitrary, which they are. Um,

556
00:38:05,830 --> 00:38:09,160
and he says,
well,

557
00:38:09,610 --> 00:38:13,690
we should make a distinction between
what he calls material resources people

558
00:38:13,691 --> 00:38:14,051
have,

559
00:38:14,051 --> 00:38:19,051
which is sort of like Rawlsian primary
goods and physical and mental powers.

560
00:38:22,030 --> 00:38:26,650
And we should treat the differences in
material resources as morally arbitrary

561
00:38:26,651 --> 00:38:31,630
but not the differences in physical
and mental coward. But again,

562
00:38:31,631 --> 00:38:34,900
you have to come back and say,
why not he,

563
00:38:35,140 --> 00:38:39,460
while we couldn't redistribute them, but
actually that's not true. For instance,

564
00:38:39,461 --> 00:38:43,720
think about blind people. You could
have a system which said, well,

565
00:38:43,721 --> 00:38:44,950
if some people are blind,

566
00:38:46,090 --> 00:38:51,090
we have to compensate them for their
blindness because they have a and morally

567
00:38:51,341 --> 00:38:55,660
arbitrary disadvantage, right? Or indeed,

568
00:38:55,661 --> 00:38:57,970
if you really want it
to be brutal about it,

569
00:38:58,000 --> 00:39:01,660
but it's nothing in the logic of what do,

570
00:39:01,670 --> 00:39:06,570
what can sang rules are that we should
just blind all the sighted people, right?

571
00:39:06,780 --> 00:39:11,320
Well, maybe the technology, we get
forcible, I transplant it's not a path.

572
00:39:11,321 --> 00:39:14,590
People are gonna want to go
down, right? It's not a path.

573
00:39:14,591 --> 00:39:17,440
People are gonna want to go down,
but it's hard to see

574
00:39:19,060 --> 00:39:19,893
why not.

575
00:39:22,150 --> 00:39:26,740
Right? It's hard to see why not once you
take this idea of moral arbitrariness.

576
00:39:26,770 --> 00:39:29,770
Seriously.
So two points to make about that.

577
00:39:32,910 --> 00:39:37,500
We got back to the very beginning.
This was never a problem for lock,

578
00:39:38,400 --> 00:39:42,860
right? And we should remember that
because after all of this comes from Loc,

579
00:39:43,580 --> 00:39:47,850
uh, this workmanship model comes
from Loc. Why is it not a problem?

580
00:39:48,090 --> 00:39:50,250
A lock?
Because for lock,

581
00:39:50,670 --> 00:39:54,840
if we have differences in capacities,

582
00:39:56,220 --> 00:40:00,570
it must've been God's
plan, right? Remember,

583
00:40:00,571 --> 00:40:05,130
lock story is human
beings are God's creation.

584
00:40:05,250 --> 00:40:08,040
Human beings do not
create other human beings.

585
00:40:08,470 --> 00:40:11,640
He's very clear about this in his
discussion of parental rights.

586
00:40:11,641 --> 00:40:16,641
We don't own our children in the way
that we own our property because we don't

587
00:40:16,771 --> 00:40:20,880
create children. God creates
children and he, he, you know,

588
00:40:20,881 --> 00:40:25,500
he implants in,
in human beings the urge to reproduce.

589
00:40:25,770 --> 00:40:28,650
But we can't, we don't
fashion the child. We can,

590
00:40:29,070 --> 00:40:33,660
we can create a child in, in an
architectural sense. And most importantly,

591
00:40:33,661 --> 00:40:38,040
of course for lock, we don't put the salt
and the child. So God does all of that.

592
00:40:38,041 --> 00:40:41,770
So children are God's property
and parents are fight usury.

593
00:40:41,790 --> 00:40:43,260
So we don't own our children.

594
00:40:43,620 --> 00:40:46,770
So if it turns out that some
of us are smarter than others,

595
00:40:46,771 --> 00:40:50,100
or some of us are more hard working
than others and some of us are better

596
00:40:50,101 --> 00:40:53,820
athletes than others,
we don't have to,

597
00:40:54,180 --> 00:40:55,410
there's no moral

598
00:40:57,060 --> 00:41:02,010
imperative for us to have some account
of why those differences exist.

599
00:41:02,310 --> 00:41:06,850
Um, because they're not, they're
not products of human action,

600
00:41:06,870 --> 00:41:09,750
that products of the divine joy.
So,

601
00:41:10,400 --> 00:41:13,410
so in the Lockian story,
this isn't a problem,

602
00:41:13,440 --> 00:41:17,520
but the minute you secularize
the workmanship ideal,

603
00:41:18,000 --> 00:41:22,280
this issue arises a quiescent

604
00:41:24,120 --> 00:41:29,120
that some people should get more than
others just because of morally arbitrary

605
00:41:30,450 --> 00:41:34,480
characteristics and the,
um,

606
00:41:39,200 --> 00:41:39,680
the,

607
00:41:39,680 --> 00:41:44,680
the ways in which people who have gone
down this path try to not get to the end

608
00:41:46,011 --> 00:41:50,960
of it, aren't very Roosevelt. Right.

609
00:41:51,140 --> 00:41:53,420
I mentioned roles.
I mentioned Dworkin.

610
00:41:53,900 --> 00:41:58,150
We could have looked at some others I talk
about in that piece. Jerry Cohen Beer,

611
00:41:58,550 --> 00:42:00,650
one of them.
But it doesn't work.

612
00:42:01,280 --> 00:42:06,280
So you're left with the fact that if you
embrace the socialization of capacity

613
00:42:06,961 --> 00:42:07,820
strategy,

614
00:42:09,810 --> 00:42:14,810
you're going to get to a place
that very few of us want to go to.

615
00:42:16,010 --> 00:42:19,400
Now it's actually even worse than that.

616
00:42:19,940 --> 00:42:23,990
It's even more problematic
than that because

617
00:42:25,760 --> 00:42:29,090
everything I've said
to you so far presumes

618
00:42:32,000 --> 00:42:37,000
that in the absence of a
justification for inequality,

619
00:42:38,600 --> 00:42:41,210
we should presume equality off parole.
I said,

620
00:42:41,480 --> 00:42:45,800
why should somebody get more
just because they work hard?

621
00:42:46,220 --> 00:42:50,600
If the capacity to work, is it
south, morally arbitrary, right?

622
00:42:50,601 --> 00:42:52,810
That presumes that,
uh,

623
00:42:53,780 --> 00:42:57,830
there's some assumption that
other things being equal,

624
00:42:57,860 --> 00:42:59,000
we should all get the same.

625
00:43:02,320 --> 00:43:06,430
But why assume that?
Why assume that

626
00:43:08,050 --> 00:43:09,460
we could just say,
wow,

627
00:43:12,150 --> 00:43:15,390
it's not a divine plan. So, you know,

628
00:43:15,391 --> 00:43:20,070
some people are lucky and some
people are lucky. There it is. Um,

629
00:43:20,100 --> 00:43:23,610
losses miss lie where they fall
as a famous American judge,

630
00:43:23,640 --> 00:43:28,640
one set and gains should lie where
they fall and so you could get nature.

631
00:43:30,090 --> 00:43:34,380
We'll hear a little bit more about nature
when we read Alistair Mcintyre. Uh,

632
00:43:34,381 --> 00:43:37,140
next week you could get
the view that you know,

633
00:43:37,830 --> 00:43:42,140
this strong when you know,
and it just the way it is, uh,

634
00:43:42,180 --> 00:43:45,960
any asking moral questions
about it is simply irrelevant.

635
00:43:47,010 --> 00:43:49,440
Now,
Rawls thinks he has an answer to that,

636
00:43:50,280 --> 00:43:55,230
which is what I started with at the
very beginning of the roles lectures,

637
00:43:56,670 --> 00:43:59,070
which was essentially,

638
00:43:59,071 --> 00:44:02,580
remember when we had the discussion
of watch the fairway to cut a cake.

639
00:44:03,210 --> 00:44:04,540
And one of you said,
wow,

640
00:44:04,541 --> 00:44:08,430
the fair way to cut a cake is to give the
knife to the person who takes the last

641
00:44:08,431 --> 00:44:10,410
slice and um,

642
00:44:11,280 --> 00:44:14,280
then he will divided equally
or she will divide it equally.

643
00:44:14,281 --> 00:44:19,281
Big preside machine being the person
wants to maximize the slice that they get,

644
00:44:20,730 --> 00:44:25,560
the residual slice. So that, and the
way you do that is to divide it equally.

645
00:44:28,520 --> 00:44:32,010
Bye.
I made

646
00:44:32,010 --> 00:44:36,630
the point that that's not
really an argument for
equality, right? That's not a,

647
00:44:36,631 --> 00:44:38,510
a moral argument for equality.

648
00:44:38,990 --> 00:44:43,990
It's not a moral argument for equality
because it assumes what we want to get is

649
00:44:44,731 --> 00:44:47,190
equality and then you create the,
the,

650
00:44:47,970 --> 00:44:52,890
the mechanism to generate that result.
So if for example,

651
00:44:53,340 --> 00:44:58,190
we added more information and
we said well we know that this,

652
00:44:58,400 --> 00:45:01,560
the six people here waiting
for the slice of cake,

653
00:45:02,520 --> 00:45:06,360
one of them has three cakes at home

654
00:45:08,010 --> 00:45:13,010
and one of them has nothing and one of
them is a diabetic and one as you know,

655
00:45:13,641 --> 00:45:18,570
you could, as soon as you
introduce information of that sort,

656
00:45:20,100 --> 00:45:24,150
then it's not obvious where you
want to wind up is equality.

657
00:45:24,240 --> 00:45:28,080
And so giving them get st
to the person with a knife,

658
00:45:29,130 --> 00:45:32,940
you get the last slice
becomes problematic, right?

659
00:45:32,970 --> 00:45:37,740
So all of this is only to make the point
that the cake cutting example does not

660
00:45:37,741 --> 00:45:42,300
establish the moral desirability
of equality. On the contrary,

661
00:45:42,301 --> 00:45:46,410
it assumes you've decided equality's
where you want to end up and then you

662
00:45:46,411 --> 00:45:48,810
create a mechanism that generates it.

663
00:45:49,170 --> 00:45:52,470
And that's the structure of what
roles does in his theory of justice.

664
00:45:52,740 --> 00:45:57,450
He assumes that his principles of where
we want to end up and then he structures

665
00:45:57,451 --> 00:45:59,910
the choice situation to generate them.

666
00:46:00,540 --> 00:46:05,540
But it means if you favorite quality or
if you favor efficiency or if you favor

667
00:46:06,241 --> 00:46:08,580
some other basis for distribution,

668
00:46:09,900 --> 00:46:11,580
you have to have an argument for it.

669
00:46:12,030 --> 00:46:17,030
Some other argument for it other than
just that it gets generated in this way.

670
00:46:19,350 --> 00:46:24,120
And to the extent roles has any argument
at all. It's this kind of prudence,

671
00:46:24,150 --> 00:46:28,320
this there, but for fortune
go I the grave risks, um,

672
00:46:28,530 --> 00:46:32,340
we better take care of the person at the
bottom cause it might turn out to be me.

673
00:46:32,800 --> 00:46:33,120
Um,

674
00:46:33,120 --> 00:46:38,120
but as we saw that assumes a view of
risk that some people find irrationally

675
00:46:39,991 --> 00:46:42,120
conservative.
Um,

676
00:46:42,420 --> 00:46:46,110
and so we don't really have
a clear clean cut answer.

677
00:46:46,920 --> 00:46:51,920
And so we don't really have
a very satisfying evolution
of the workmanship ideal

678
00:46:55,411 --> 00:46:57,240
from locked down through the present.

679
00:46:58,230 --> 00:47:03,120
It's a theological argument in locks
formulation. It's got this very nice,

680
00:47:03,450 --> 00:47:06,060
coherent all fits together.

681
00:47:06,390 --> 00:47:08,910
But the moment you try to secularize it,

682
00:47:09,180 --> 00:47:14,020
you left with this problem that it leads
into directions people are not going to

683
00:47:14,021 --> 00:47:17,580
want to go in on the one hand.
On the other hand,

684
00:47:17,700 --> 00:47:20,610
they're not going to want to
get rid of it entirely either.

685
00:47:21,000 --> 00:47:26,000
And so the ways in which people try
and hedge in the parade of horribles,

686
00:47:27,540 --> 00:47:30,610
um,
doesn't entirely work.

687
00:47:30,790 --> 00:47:35,790
And we're left with this kind of nagging
feeling that there's gotta be some way

688
00:47:36,041 --> 00:47:38,380
to salvage this workmanship idea,

689
00:47:38,920 --> 00:47:41,170
but we haven't managed to do it.

690
00:47:42,190 --> 00:47:47,190
And so just as the evolution of science
has taken us with the evolution of the

691
00:47:47,921 --> 00:47:49,210
commitment to science,

692
00:47:49,510 --> 00:47:54,510
it's taken us to a rather uncomfortable
end point when we get to the world of

693
00:47:55,901 --> 00:47:57,640
political,
not metaphysical.

694
00:47:57,910 --> 00:48:02,910
So the evolution of the workmanship ideal
has taken us to a rather uncomfortable

695
00:48:03,930 --> 00:48:08,930
and point once we get to roles as move
and his attempt to limit the radical

696
00:48:11,021 --> 00:48:13,150
implications of it that he doesn't like.

697
00:48:14,230 --> 00:48:19,230
And so we will pick up that story when
we come to talk about the democratic

698
00:48:19,691 --> 00:48:22,480
tradition in the last few
lectures of this course.

699
00:48:23,110 --> 00:48:24,730
But before we get there,

700
00:48:25,630 --> 00:48:30,630
we're going to couldn't fit as
seriously the idea that may be the whole

701
00:48:32,351 --> 00:48:34,630
enlightenment project was a mistake.

702
00:48:35,500 --> 00:48:39,760
Maybe we should reject the
whole enlightenment venture.

703
00:48:40,450 --> 00:48:41,500
This was of course,

704
00:48:41,501 --> 00:48:46,501
the view of Edmund Burke Irish political
thinker that we'll talk about on

705
00:48:47,321 --> 00:48:49,420
Wednesday. And it's, it's a,

706
00:48:49,480 --> 00:48:52,840
it's a long tradition from
Burke down through the present.

707
00:48:52,841 --> 00:48:56,650
We're going to look at Alisdair
Macintyre as a contemporary, uh,

708
00:48:56,950 --> 00:48:59,170
a defender of this view.

709
00:48:59,560 --> 00:49:04,560
But this is the idea that we shouldn't
be surprised that the enlightenment

710
00:49:04,601 --> 00:49:09,601
project turns out to be untenable
because it was a profound and indeed

711
00:49:11,860 --> 00:49:15,760
politically dangerous mistake.
We'll start with that on Wednesday.

