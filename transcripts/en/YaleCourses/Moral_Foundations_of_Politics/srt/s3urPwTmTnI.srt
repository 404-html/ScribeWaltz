1
00:00:01,980 --> 00:00:05,580
I want to pick up where
I left off on Monday.

2
00:00:06,090 --> 00:00:09,870
Speaking about roles is
two principles of justice.

3
00:00:10,320 --> 00:00:12,060
And as you will recall,

4
00:00:12,540 --> 00:00:17,540
I mentioned that roles really changed
the subject with respect to what the

5
00:00:21,120 --> 00:00:25,980
metric of justice is that
rather than focus on utility,

6
00:00:25,981 --> 00:00:30,540
somehow measured or welfare
as it's sometimes called. Um,

7
00:00:31,800 --> 00:00:34,550
instead roles embraces resources.

8
00:00:34,551 --> 00:00:37,860
The idea of focusing on
certain basic resources,

9
00:00:37,890 --> 00:00:42,890
the assumption being that no matter what
your particular goals in life turn out

10
00:00:43,441 --> 00:00:48,090
to be, no matter what your
particular life plan turns out to be.

11
00:00:48,091 --> 00:00:52,950
And those are not things we know because
we're behind the veil of ignorance.

12
00:00:53,220 --> 00:00:57,600
You're going to want more rather
than less in the way of liberties,

13
00:00:57,780 --> 00:01:02,780
more rather than less in the way of
opportunities and more rather than less in

14
00:01:03,121 --> 00:01:05,790
the way of income and wealth.

15
00:01:06,620 --> 00:01:11,620
So I'm thinking I didn't mention that'll
come into play in today's lecture is

16
00:01:11,821 --> 00:01:14,520
that rolls has to of course,

17
00:01:14,940 --> 00:01:19,830
deal with the fact that the moment you
have a theory that affirms more than one

18
00:01:19,831 --> 00:01:24,350
value, you have to think about, well
what happens when the values conflict?

19
00:01:25,210 --> 00:01:25,740
Um,

20
00:01:25,740 --> 00:01:30,740
what if maximizing liberties can only
come at the expense of opportunities or if

21
00:01:33,781 --> 00:01:34,890
you like,
uh,

22
00:01:35,880 --> 00:01:40,710
distributing income and wealth and if
in a way that you regard as fair or just

23
00:01:41,100 --> 00:01:45,150
conflicts with what you say about
the distribution of liberties, uh,

24
00:01:45,151 --> 00:01:46,890
anytime you have more than one value,

25
00:01:46,891 --> 00:01:50,160
you have to deal with the possibility
of the conflict among them.

26
00:01:50,610 --> 00:01:53,910
And he does deal with that. He has, um,

27
00:01:54,870 --> 00:01:58,230
an appeal to what he
calls a lexical ran king,

28
00:01:58,231 --> 00:02:02,820
which is short for the more cumbersome
term lexicographical ranking.

29
00:02:02,821 --> 00:02:07,821
And what that means is that anytime you
want more rather than fewer in the way

30
00:02:09,301 --> 00:02:09,961
of liberties,

31
00:02:09,961 --> 00:02:13,950
you want more in fewer in
the way of opportunities and
you want more than rather

32
00:02:13,951 --> 00:02:18,450
than less than the way of
income and wealth. But in
any time there's a conflict,

33
00:02:18,510 --> 00:02:20,820
something higher in
the electrical ranking,

34
00:02:20,821 --> 00:02:25,821
Trump's what's lower so that if the only
way you could get more rather than less

35
00:02:27,691 --> 00:02:31,560
income and wealth was to compromise
people's liberties, you wouldn't do it.

36
00:02:32,160 --> 00:02:35,340
Okay.
So that's the notion of a lexical ranking.

37
00:02:35,370 --> 00:02:37,240
You want to maximize each,

38
00:02:37,590 --> 00:02:42,590
each item in the lexical ranking subject
to the constraint that it does not come

39
00:02:44,340 --> 00:02:49,340
at the expense of maximizing something
that's higher in that lexical ranking.

40
00:02:51,030 --> 00:02:51,331
Okay.

41
00:02:51,331 --> 00:02:56,331
And then we talked about
his first principle and I
gave you the illustration of

42
00:02:57,511 --> 00:03:02,511
religious freedom as the of thing he's
thinking about when he talks about

43
00:03:02,650 --> 00:03:07,650
distributing all liberties in a way
that gives people the most extensive

44
00:03:08,141 --> 00:03:12,700
possible freedom,
compatible with a like freedom for all.

45
00:03:12,880 --> 00:03:16,690
And this is not to be confused
with the idea of neutrality, right?

46
00:03:16,691 --> 00:03:18,790
We went through that.
Um,

47
00:03:19,120 --> 00:03:23,830
and so we gave the example of whether
to have, if you comparing say,

48
00:03:23,831 --> 00:03:28,630
a fundamentalist regime with a regime
that has a disestablished church.

49
00:03:28,960 --> 00:03:33,960
The reason for preferring the regime with
the disestablished church is that the

50
00:03:34,480 --> 00:03:38,650
most disadvantaged person in
that regime is less, namely,

51
00:03:38,651 --> 00:03:43,651
that say the fundamentalists is less
disadvantaged than the person who does not

52
00:03:45,461 --> 00:03:50,461
affirm the established fundamentalist
regime beliefs of a fundamentalist regime.

53
00:03:51,280 --> 00:03:55,150
So you, you always
compare the least, then.

54
00:03:55,151 --> 00:03:59,590
I know it's a cumbersome way
of putting it, but I, there's
not a, there's not a, uh,

55
00:04:01,120 --> 00:04:01,953
mmm

56
00:04:02,320 --> 00:04:06,780
elegant way of putting it.
But what you want to do is compare that,

57
00:04:06,810 --> 00:04:11,810
the condition of the most adversely
affected person in each situation and say

58
00:04:14,290 --> 00:04:19,290
which, which would you
rather be basically. And
you're going to always pack,

59
00:04:19,800 --> 00:04:20,590
uh,

60
00:04:20,590 --> 00:04:25,210
the one that minimizes the harm
to the least advantage person.

61
00:04:25,720 --> 00:04:26,710
So the justice,

62
00:04:26,711 --> 00:04:30,550
the standpoint of justice is the
standpoint of the least advantaged person.

63
00:04:30,820 --> 00:04:35,770
But this isn't a bleeding hardpoint
but rather a self interested point,

64
00:04:36,030 --> 00:04:40,630
um, because you don't know who you
are behind the veil of ignorance.

65
00:04:41,440 --> 00:04:43,240
Okay.
Um,

66
00:04:46,160 --> 00:04:46,993
no,

67
00:04:48,250 --> 00:04:51,610
let's talk about his second principle,

68
00:04:51,611 --> 00:04:54,910
which is in fact divided
into two principals.

69
00:04:54,911 --> 00:04:59,320
So he really has three principles.
The, the first part of it is for,

70
00:04:59,321 --> 00:05:04,321
he says social and economic inequalities
are to be arranged so that they are

71
00:05:05,651 --> 00:05:06,484
both

72
00:05:08,470 --> 00:05:09,370
attached

73
00:05:10,850 --> 00:05:15,800
two offices and positions open to all
under conditions of fair equality of

74
00:05:15,801 --> 00:05:20,240
opportunity. That is, you'll
see to be, that's not a typo.

75
00:05:20,420 --> 00:05:21,470
I'll come to,
to a,

76
00:05:21,471 --> 00:05:25,460
in a minute for some reason
known only to John Rawls.

77
00:05:25,930 --> 00:05:29,450
Um, he, he put to a before to be,

78
00:05:29,451 --> 00:05:34,451
but he meant to put to be before two a
in the sense that to be his lexicon prior

79
00:05:35,091 --> 00:05:36,380
to two a.
Okay.

80
00:05:36,381 --> 00:05:41,381
So that's what I'm doing to be first
and that to be is what governs the

81
00:05:41,691 --> 00:05:44,930
distribution of opportunities.
And he's essentially saying,

82
00:05:46,720 --> 00:05:47,110
okay,

83
00:05:47,110 --> 00:05:49,090
fair equality of opportunity.

84
00:05:49,850 --> 00:05:50,290
Okay,

85
00:05:50,290 --> 00:05:54,790
what does that mean? It means
no apartheid. It means if you,

86
00:05:54,791 --> 00:05:55,451
for instance,

87
00:05:55,451 --> 00:06:00,451
we ha we still today have a system in
America where occupation by occupation

88
00:06:01,251 --> 00:06:06,251
women earn about 86% of what men
earn in exactly the same occupation.

89
00:06:07,101 --> 00:06:07,934
So there's,

90
00:06:08,180 --> 00:06:13,180
there's gender discrimination
in renumeration for employment.

91
00:06:13,701 --> 00:06:16,130
So we would say,
um,

92
00:06:17,120 --> 00:06:20,750
those systems are illegitimate systems,

93
00:06:20,751 --> 00:06:24,710
which reward women less than
men on a systematic basis are,

94
00:06:25,310 --> 00:06:26,990
wouldn't be chosen.
It's not,

95
00:06:26,991 --> 00:06:31,370
you wouldn't never choose a system that
privileges one gender because you don't

96
00:06:31,371 --> 00:06:34,970
know where they're going to turn
out to be the women or the men.

97
00:06:35,180 --> 00:06:40,180
You would never accept the system of job
reservation such as a park aid because

98
00:06:40,371 --> 00:06:44,960
you don't know whether you're going
to be black or white and not knowing.

99
00:06:45,140 --> 00:06:45,973
Um,

100
00:06:46,040 --> 00:06:50,660
you always look from the standpoint
of the most adversely affected person.

101
00:06:50,930 --> 00:06:54,470
And so you would say,
uh, no to, uh, partake.

102
00:06:54,470 --> 00:06:59,470
You'd say no to a system which
privileges one gender over the other.

103
00:07:00,500 --> 00:07:02,150
Okay.
And then you can see,

104
00:07:02,151 --> 00:07:07,151
I think how the lexical ranking would
come into play because let's suppose you

105
00:07:07,161 --> 00:07:09,170
have,
um,

106
00:07:09,350 --> 00:07:12,320
let's suppose you have a
status quo in which as I said,

107
00:07:12,321 --> 00:07:17,321
women on average earn 86% of what
men earn in the same professions.

108
00:07:18,110 --> 00:07:20,870
And somebody comes along and says,
well,

109
00:07:20,900 --> 00:07:24,560
so we need an affirmative
action program to remedy that.

110
00:07:25,250 --> 00:07:29,900
Then the question would be, okay,

111
00:07:29,901 --> 00:07:34,901
but does the affirmative action program
conflict with any thing protected by the

112
00:07:35,091 --> 00:07:38,210
first principle?
And those opposed to it would say yes.

113
00:07:38,211 --> 00:07:40,400
And those in favor of it would say no.

114
00:07:40,401 --> 00:07:43,580
And that's what you would
be arguing about. Okay.

115
00:07:43,581 --> 00:07:48,581
Because it might be the case that if I'm
the only way in which you could achieve

116
00:07:52,251 --> 00:07:56,720
affirmative action actually interfered
with the liberties protected by the first

117
00:07:56,810 --> 00:07:59,570
principle,
then you would say,

118
00:07:59,600 --> 00:08:03,530
even though it's necessary from the
standpoint of the second principle,

119
00:08:03,531 --> 00:08:08,510
we won't do it. And if you, if
you, uh, if we had more time,

120
00:08:08,511 --> 00:08:13,511
we could have gone into the new haven
firefighters case and maybe we can do some

121
00:08:13,761 --> 00:08:18,260
of this in section that the
Supreme Court dealt with loss, um,

122
00:08:18,710 --> 00:08:21,050
summer,
uh,

123
00:08:21,080 --> 00:08:26,080
where essentially they said some version
of the fact that the affirmative action

124
00:08:26,781 --> 00:08:31,781
program to achieve promotions in the new
haven fire department interfered with

125
00:08:32,031 --> 00:08:34,440
basic freedoms.
Um,

126
00:08:34,490 --> 00:08:39,490
that rolls would of put under the first
principle and of course the other side

127
00:08:39,830 --> 00:08:42,050
made the opposite plane,

128
00:08:42,340 --> 00:08:45,890
but that that is essentially
how it would be argued about.

129
00:08:46,790 --> 00:08:49,050
Um,
so I think that the,

130
00:08:49,051 --> 00:08:53,360
the principle of fair
equality of opportunity is
relatively straight forward and

131
00:08:53,361 --> 00:08:57,230
its own terms. You would, the, the, uh,

132
00:08:57,290 --> 00:09:02,290
animating thought is that not
knowing who you're going to be,

133
00:09:03,361 --> 00:09:08,100
you would never agree to assist them
that systematically this privileges some

134
00:09:08,101 --> 00:09:12,750
group for fear that you're going to
turn out to be in that group. Okay.

135
00:09:13,500 --> 00:09:18,210
Um, so it's relatively straight
forward. But now I want to come to,

136
00:09:18,211 --> 00:09:19,620
to a,

137
00:09:24,050 --> 00:09:29,050
which is probably the argument in
roles as book that's attracted the most

138
00:09:29,270 --> 00:09:33,480
attention and that is that as it,

139
00:09:33,560 --> 00:09:38,440
it's actually third in his lexical
ranking. Um, and that is the,

140
00:09:38,660 --> 00:09:43,660
the claim that income and wealth is to
be distributed to the greatest benefit of

141
00:09:46,131 --> 00:09:51,131
the least advantaged individual to the
greatest benefit of the least advantaged

142
00:09:53,930 --> 00:09:54,763
individual.

143
00:09:56,260 --> 00:09:59,520
This is not a principle
that rolls invented.

144
00:09:59,530 --> 00:10:03,400
It's an old principle of welfare
economics, which used to go under the,

145
00:10:04,120 --> 00:10:07,530
under the label Maxi men,

146
00:10:07,810 --> 00:10:10,120
not Maxine Milam Maxi men,

147
00:10:10,121 --> 00:10:15,121
Max I m I n short for maximize the minimum
share Maxi men maximize the minimum

148
00:10:18,940 --> 00:10:23,740
share. Um, Rawls calls it
the difference principle,

149
00:10:24,520 --> 00:10:29,230
but it's the same idea as maximizing
the minimum share. And th the,

150
00:10:29,240 --> 00:10:34,240
the intuition behind the
difference principle is
exactly the same intuition that

151
00:10:34,781 --> 00:10:37,900
we've been talking about
by reference to the,

152
00:10:38,080 --> 00:10:41,960
the general conception of
distributive justice, um,

153
00:10:42,340 --> 00:10:47,340
which is remember distributed all
goods equally and less than and equal

154
00:10:48,280 --> 00:10:50,950
distribution works to
everybody's advantage.

155
00:10:51,280 --> 00:10:56,230
And you get from everybody's advantage
to focusing on the condition of the worst

156
00:10:56,231 --> 00:10:57,970
off.
Why?

157
00:10:58,150 --> 00:11:02,440
Because of this argument that,
well,

158
00:11:02,860 --> 00:11:06,820
if you're the worst off person
and you can affirm something,

159
00:11:07,150 --> 00:11:09,400
then everybody else
will affirm it as well.

160
00:11:09,430 --> 00:11:12,610
If you'll choose it when you're
the most adversely affected,

161
00:11:12,880 --> 00:11:17,170
you'll also choose it if you're a
second or third or fourth or fifth most

162
00:11:17,320 --> 00:11:22,300
adversely affected person.
Now,

163
00:11:22,360 --> 00:11:23,193
um,

164
00:11:26,640 --> 00:11:30,870
there's actually a complexity when you
start to think about the distribution of

165
00:11:30,871 --> 00:11:35,100
income and wealth that has not come up
in the consideration of the other two

166
00:11:35,101 --> 00:11:35,934
principals,

167
00:11:36,150 --> 00:11:40,620
which I'll just mention and it's then say
a couple of things about and then move

168
00:11:40,621 --> 00:11:45,300
on and we'll come back to it later.
And that is well, but what if,

169
00:11:46,920 --> 00:11:51,750
what if there was a principal that gave
a very small benefit to the person at

170
00:11:51,751 --> 00:11:56,751
the bottom but at a huge
cost to the middle class?

171
00:12:00,540 --> 00:12:01,373
Okay.

172
00:12:02,390 --> 00:12:03,800
Would you,
would you chose it?

173
00:12:03,801 --> 00:12:06,980
Because what are the odds that you're
going to turn out to be the person at the

174
00:12:06,981 --> 00:12:08,030
very bottom?

175
00:12:10,010 --> 00:12:10,843
Okay.

176
00:12:11,490 --> 00:12:14,400
And you would think about
this for a variety of reasons.

177
00:12:14,401 --> 00:12:16,890
It might be a trickle
down argument or you know,

178
00:12:16,891 --> 00:12:20,970
Bentham's claim that the ritual burn
that crops before giving them to the poor

179
00:12:21,120 --> 00:12:22,350
or some other argument.

180
00:12:22,650 --> 00:12:26,310
But if you could achieve a very
minor increments to the condition,

181
00:12:26,311 --> 00:12:31,090
the person at the bottom, at a huge cost
to the middle class would, you would,

182
00:12:31,110 --> 00:12:32,730
you wouldn't necessarily

183
00:12:35,160 --> 00:12:36,150
want to do that.

184
00:12:37,080 --> 00:12:41,040
And so roles has two points
to make about that. Um,

185
00:12:41,041 --> 00:12:44,910
neither of which is entirely satisfying.

186
00:12:46,110 --> 00:12:50,460
The one is his argument
about grave risks and it,

187
00:12:50,490 --> 00:12:55,020
it works like this.
It's the claim that,

188
00:12:55,500 --> 00:12:59,100
well,
one of the things we know,

189
00:12:59,101 --> 00:13:03,120
and this is a perfectly um,
uncontroversial claim,

190
00:13:03,360 --> 00:13:07,140
one of the things we know is that
even when there's moderate scarcity,

191
00:13:08,400 --> 00:13:12,120
that doesn't mean there won't be
some people who are in grave danger.

192
00:13:13,200 --> 00:13:17,190
That is to say there's no necessary
relationship between the level of economic

193
00:13:17,191 --> 00:13:21,750
development in a country and the
distribution of income and well,

194
00:13:21,960 --> 00:13:24,120
so you can have a wealthy country,

195
00:13:24,121 --> 00:13:29,040
but there still can be extremely
poor people in it. Okay. Um,

196
00:13:29,190 --> 00:13:30,023
that's true.

197
00:13:30,360 --> 00:13:34,950
We can have bag ladies living out
of lockers in grand central station,

198
00:13:34,951 --> 00:13:38,190
at least when they used to have
lockers in grand central station,

199
00:13:38,191 --> 00:13:40,470
which they don't anymore. But, uh,

200
00:13:41,880 --> 00:13:44,130
let's not deal with that particular piece.

201
00:13:44,880 --> 00:13:49,590
So there's no necessary relationship
between the level of economic development

202
00:13:49,591 --> 00:13:53,940
and the distribution of income and
wealth in a society. Therefore,

203
00:13:54,060 --> 00:13:58,350
you have to assume, even if, even if

204
00:14:02,130 --> 00:14:03,960
there's relative scarcity,

205
00:14:04,320 --> 00:14:07,380
you might turn out to be
the person who's starving.

206
00:14:08,070 --> 00:14:12,120
You might turn out to be that bag lady.
Okay.

207
00:14:12,510 --> 00:14:13,380
So even if,

208
00:14:13,600 --> 00:14:18,300
and even if the probability
of being that person is low,

209
00:14:18,990 --> 00:14:23,590
the costs of being that person,
that's our high. So, uh,

210
00:14:23,820 --> 00:14:28,440
even if the probability, it's like,
I don't know if you remember the,

211
00:14:28,680 --> 00:14:32,640
the argument, uh, Rumsfeld made, uh,

212
00:14:32,641 --> 00:14:37,350
in his counter terrorism strategy and the,
the so called 1% solution,

213
00:14:37,910 --> 00:14:38,640
uh,
the,

214
00:14:38,640 --> 00:14:43,640
and this was that even if there's a 1%
probability that we're going to be hit by

215
00:14:44,161 --> 00:14:45,900
a certain kind of terrorist attack,

216
00:14:45,960 --> 00:14:50,960
we should treat it as a 100% probability
because the costs of being hated so

217
00:14:51,291 --> 00:14:55,980
high. Um, so the,

218
00:14:56,030 --> 00:15:00,450
the, the probability of the
event may be low, but the,

219
00:15:00,630 --> 00:15:04,140
if you turn out to be that person,
you're going to starve to death.

220
00:15:04,380 --> 00:15:06,000
So we,

221
00:15:06,060 --> 00:15:10,440
it's this as roles as assumption
about grave risks. Okay.

222
00:15:10,650 --> 00:15:12,330
So all of that's plausible enough.

223
00:15:12,331 --> 00:15:17,070
The reason I say it's not entirely
satisfying is if you really took the grave

224
00:15:17,071 --> 00:15:18,840
risks,
ideas seriously,

225
00:15:19,290 --> 00:15:22,950
why in the world would you make
this third in your lexicon ranking?

226
00:15:22,951 --> 00:15:23,671
Because after all,

227
00:15:23,671 --> 00:15:27,780
what good is freedom of speech or freedom
of religion to somebody who's on the

228
00:15:27,781 --> 00:15:30,600
verge of starvation.
So I,

229
00:15:30,720 --> 00:15:33,350
it's not entirely satisfying
in that sense that,

230
00:15:33,450 --> 00:15:36,330
that if a justifies saying,
well,

231
00:15:36,331 --> 00:15:38,760
we will protect the person at the bottom,

232
00:15:38,790 --> 00:15:43,080
even though even though the probability
of that person turns out to be low

233
00:15:43,081 --> 00:15:46,690
because of the grave risks
assumption, why then, uh,

234
00:15:46,780 --> 00:15:48,300
this is very annoying.

235
00:15:50,350 --> 00:15:51,183
Yeah,

236
00:15:54,570 --> 00:15:57,510
why then would we we make it third,

237
00:15:57,540 --> 00:16:02,100
but it's not a really a deep criticism of
roles in that you could just say, okay,

238
00:16:02,101 --> 00:16:06,960
well we should have reordered his lexical
ranking and put this Iraq payment.

239
00:16:07,050 --> 00:16:10,650
But anyway,
that that's not entirely satisfying.

240
00:16:11,160 --> 00:16:16,160
And then the second thing he says that's
not entirely satisfying is he says he,

241
00:16:16,421 --> 00:16:18,210
he's sort of sensitive to this,

242
00:16:18,690 --> 00:16:23,160
this problem that you might get
absurdities out of this because

243
00:16:24,270 --> 00:16:25,103
mmm.

244
00:16:26,380 --> 00:16:27,010
If it's,

245
00:16:27,010 --> 00:16:31,810
if it's very costly to help the person
at the bottom in terms of what other

246
00:16:31,811 --> 00:16:35,110
people have to give up,
um,

247
00:16:36,310 --> 00:16:37,320
maybe,
you know,

248
00:16:37,330 --> 00:16:41,590
maybe people wouldn't be that impressed
by the grave risks assumption.

249
00:16:41,860 --> 00:16:46,810
So he throws in this idea
of chain connection and he
says, well, as a matter of,

250
00:16:46,840 --> 00:16:47,950
even though,

251
00:16:49,090 --> 00:16:53,920
even though my argument doesn't
depend on this, I'm gonna, I'm,

252
00:16:54,040 --> 00:16:57,700
I think it's true. It's you. When
somebody does that, you know,

253
00:16:57,701 --> 00:17:01,900
there's some slight of hand going
on and he basically says, well,

254
00:17:02,050 --> 00:17:05,470
if you help the person at
the bottom that will help,

255
00:17:05,530 --> 00:17:09,250
that will have some kind
of a chain we action,

256
00:17:09,251 --> 00:17:12,400
it will help the person at the next
level and that'll help the person,

257
00:17:12,401 --> 00:17:14,830
that next level and that'll
help the person the next level.

258
00:17:15,160 --> 00:17:20,160
So it's a kind of Keynesian idea that
if you stimulate demand at the bottom,

259
00:17:20,441 --> 00:17:23,710
they'll be multiplier effects
throughout the whole system.

260
00:17:23,740 --> 00:17:27,040
That'll make everybody
else better off too. Well,

261
00:17:27,041 --> 00:17:30,430
that may or may not be true.
And it also, by the way,

262
00:17:30,431 --> 00:17:35,431
I think it makes the disagreement between
roles in ism and utilitarianism much

263
00:17:35,501 --> 00:17:40,501
less interesting because then anything
that rolls would choose a utilitarian

264
00:17:41,441 --> 00:17:42,610
would choose as well.

265
00:17:43,120 --> 00:17:46,390
And we really want to look at when
they pull in opposite directions, if,

266
00:17:46,420 --> 00:17:50,660
if we want to see what's
at stake between them. But,

267
00:17:50,661 --> 00:17:54,320
so there's this chain connection idea.
I think it's just sort of,

268
00:17:54,350 --> 00:17:58,910
he throws it in there to make
his art, his, his argument,

269
00:17:58,911 --> 00:18:03,530
not more, uh, appealing on,
on consequentialist grounds,

270
00:18:03,800 --> 00:18:08,720
but actually, um, it's not a no
reason to believe it's true. And B,

271
00:18:08,721 --> 00:18:09,800
if it were,

272
00:18:10,010 --> 00:18:14,540
then what's really at stake between roles
and utilitarianism becomes much less

273
00:18:14,720 --> 00:18:17,690
interesting because by satisfying roles,

274
00:18:17,840 --> 00:18:22,280
we're also going to be satisfying
utilitarianism. So I think,

275
00:18:22,700 --> 00:18:27,170
uh, the best thing about chain connection
is to ignore it. So I'm not gonna,

276
00:18:27,620 --> 00:18:31,820
I'm not gonna say anything more,
but I will say this in, in, um,

277
00:18:33,660 --> 00:18:35,910
enrolls his defense on this point,

278
00:18:35,940 --> 00:18:40,800
which is a lot of people
have criticized roles create.

279
00:18:40,801 --> 00:18:45,270
And I even did some of this myself
when I think I'm fair in retrospect,

280
00:18:45,840 --> 00:18:47,250
people create

281
00:18:47,420 --> 00:18:48,253
mmm.

282
00:18:50,710 --> 00:18:52,240
Examples.
We're helping.

283
00:18:52,241 --> 00:18:56,140
The person at the bottom comes at a
huge cost to others and it looks rather

284
00:18:56,420 --> 00:18:57,490
implausible.

285
00:18:58,540 --> 00:19:03,540
But one thing we should say about roles
is he's not trying to give policy advice

286
00:19:06,551 --> 00:19:10,090
for every marginal choice.
At one point he says in the book,

287
00:19:10,450 --> 00:19:15,370
I'm thinking about the basic structure
of society, the basic institutions.

288
00:19:15,760 --> 00:19:19,390
So he's not saying, you
know, I mean, the example,

289
00:19:19,480 --> 00:19:23,560
the example people sometimes
gave is the Reagan tax cuts.

290
00:19:24,160 --> 00:19:29,070
Um, in the 1980. So actually
the, the Bush tax cut, uh,

291
00:19:29,170 --> 00:19:31,000
in the,
in the two thousands,

292
00:19:31,450 --> 00:19:36,430
but the Reagan one had this structure
more explicitly where there was a very big

293
00:19:36,431 --> 00:19:37,930
tax cut for the wealthy,

294
00:19:38,260 --> 00:19:41,500
the tiny tax cut for the
people at the very bottom,

295
00:19:41,950 --> 00:19:45,940
and a huge increase in middle class taxes,
basically,

296
00:19:45,941 --> 00:19:50,830
that this was the structure of it. And
people said, so roles would prefer this.

297
00:19:52,110 --> 00:19:52,943
MMM.

298
00:19:54,160 --> 00:19:56,510
And you know,
the answer is he,

299
00:19:56,570 --> 00:19:59,340
he's not trying to make a
recommendation at the neck,

300
00:19:59,380 --> 00:20:02,740
at the level of the next
incremental policy choice.

301
00:20:02,950 --> 00:20:06,970
He's trying to say what the underlying
institutions should be structured as.

302
00:20:07,360 --> 00:20:11,980
And so he, he would resist saying,
well, this shows my theory.

303
00:20:12,280 --> 00:20:15,080
It's silly, or my theory
doesn't generate our, uh,

304
00:20:15,430 --> 00:20:20,180
conclusions that I want it to
a, he, it's not a, it's not,

305
00:20:20,260 --> 00:20:24,220
he's not a policy wonk.
Okay? He's saying this is,

306
00:20:24,221 --> 00:20:27,640
he's thinking about constitutional
principles, basic principles,

307
00:20:27,641 --> 00:20:32,530
the basic structure of
society. Um, and indeed,

308
00:20:32,531 --> 00:20:35,590
I'll just make one footnote
to that footnote, which is,

309
00:20:37,250 --> 00:20:38,083
mmm.

310
00:20:39,060 --> 00:20:39,421
If you,

311
00:20:39,421 --> 00:20:43,590
if you start at the front of a theory of
justice and you really plow through all

312
00:20:43,591 --> 00:20:47,740
of it, uh, you get to about
page 300 and something,

313
00:20:48,340 --> 00:20:52,560
and he says words to the effect that,
um,

314
00:20:53,680 --> 00:20:57,190
his theory is agnostic between
capitalism and socialism.

315
00:20:58,240 --> 00:20:58,520
Okay.

316
00:20:58,520 --> 00:21:01,480
And he,
he took a lot of abuse for that in,

317
00:21:01,490 --> 00:21:06,320
in the 1970s and 1980s
people's, you know, I say, wow,

318
00:21:06,350 --> 00:21:09,740
you may not plow through 300
pages of a book about justice.

319
00:21:09,741 --> 00:21:13,130
I don't need to be told it's agnostic
between capitalism and socialism.

320
00:21:13,460 --> 00:21:15,820
Gimme a break.
Uh,

321
00:21:16,250 --> 00:21:19,900
but in defense of rolls on
that point, he would say, look,

322
00:21:20,970 --> 00:21:21,440
okay,

323
00:21:21,440 --> 00:21:26,440
what economic system actually operates
in the interest of the least advantaged?

324
00:21:29,090 --> 00:21:33,890
That's an empirical question of political
economy, trial and error and so on.

325
00:21:34,070 --> 00:21:38,300
That is not a question for
political philosophy to settle.

326
00:21:38,600 --> 00:21:41,960
I don't know whether it's capitalism,
socialism,

327
00:21:41,961 --> 00:21:44,610
some version of a mixed economy,
um,

328
00:21:45,140 --> 00:21:49,820
that works to the greatest benefit of the
least advantaged person that's for the

329
00:21:49,821 --> 00:21:54,821
policymakers in political economist to
figure out what I'm telling you is what

330
00:21:55,161 --> 00:21:58,460
the standards should be.
And so I think that that is a,

331
00:21:58,461 --> 00:22:01,220
and that is a good argument
on roles as part of this.

332
00:22:01,460 --> 00:22:04,850
And he's saying this is
what the standard should be.

333
00:22:04,940 --> 00:22:09,940
The standard should be that whatever
system you have works to the greatest

334
00:22:10,011 --> 00:22:14,630
benefit of, of the least advantage play
at when compared with other systems.

335
00:22:14,960 --> 00:22:16,430
And yes,
you know,

336
00:22:17,720 --> 00:22:18,010
yeah.

337
00:22:18,010 --> 00:22:22,540
Before the experience of centrally planned
economies, people may have thought,

338
00:22:22,860 --> 00:22:23,650
uh,

339
00:22:23,650 --> 00:22:28,650
some version of state socialism would do
that after half a century of experience

340
00:22:29,771 --> 00:22:31,930
with it doesn't look so good.

341
00:22:31,960 --> 00:22:36,280
So we'll go back to some
kind of market system, uh,

342
00:22:36,281 --> 00:22:37,930
and after,
you know,

343
00:22:37,931 --> 00:22:42,931
decades of experience with unregulated
markets or minimally regulated markets,

344
00:22:43,001 --> 00:22:46,240
and we discovered the costs of
those for the people at the bottom,

345
00:22:46,390 --> 00:22:48,010
maybe we'll end up with something else.

346
00:22:48,011 --> 00:22:51,910
So it's not a failing of my John Rolls,

347
00:22:51,911 --> 00:22:56,710
his theory that I don't tell you what
kind of, uh, economic system to have.

348
00:22:56,920 --> 00:23:01,920
My aspiration is to tell you what the
normative criterion is that it should

349
00:23:02,231 --> 00:23:06,560
meet. Okay. So that's the,
that I think that's the,

350
00:23:06,561 --> 00:23:11,560
the most important takeaway point.
Now

351
00:23:21,360 --> 00:23:22,350
let's give you a picture

352
00:23:25,060 --> 00:23:28,930
for those who like pictures. And
for those who don't, we will,

353
00:23:29,380 --> 00:23:33,610
we will explain it in words.
This is,

354
00:23:33,640 --> 00:23:37,390
this has gone back to our
paredo style of diagram.

355
00:23:37,780 --> 00:23:40,240
Let's suppose that the status quo,
okay.

356
00:23:40,241 --> 00:23:45,241
And now we've got primary goods in this
case and Commonwealth for two people,

357
00:23:45,500 --> 00:23:50,030
a IPN be along here. And that's
the, that is the district.

358
00:23:50,031 --> 00:23:52,730
That is the status quo.
So Aa has more than B.

359
00:23:53,680 --> 00:23:54,513
Okay.

360
00:23:55,610 --> 00:24:00,610
Roles is difference principle or the
so called maximin principle of welfare

361
00:24:00,951 --> 00:24:05,951
economics says rob up line down to that.

362
00:24:06,441 --> 00:24:08,510
That's point is perfect quality.
Right?

363
00:24:11,640 --> 00:24:13,020
And then go east

364
00:24:15,360 --> 00:24:17,700
and everything in this shaded area.

365
00:24:21,350 --> 00:24:22,183
Yeah.

366
00:24:23,140 --> 00:24:27,160
Is What we might call roll
superior to the status quo.

367
00:24:27,910 --> 00:24:31,780
So is it kind of l shaped
indifference curve, right.

368
00:24:32,260 --> 00:24:37,260
Because down through the status quo
to equality and then it turns right.

369
00:24:42,030 --> 00:24:42,960
Anyone want to

370
00:24:44,620 --> 00:24:47,140
take a stab at telling us why?

371
00:24:49,710 --> 00:24:52,380
Why would you have these l shaped him?
Difference curves.

372
00:24:56,460 --> 00:25:00,030
Yeah. Why didn't you get the
mine? Uh, cool. Come to them.

373
00:25:05,850 --> 00:25:06,683
Okay.

374
00:25:08,000 --> 00:25:11,930
You can move right as far as possible.

375
00:25:12,020 --> 00:25:17,020
And that will be increasing the goods
for B and then as you move down you have

376
00:25:17,661 --> 00:25:20,840
to stop at the quantity.

377
00:25:20,940 --> 00:25:25,710
Okay. But why I'm, why say a has
this, my tried when we start out,

378
00:25:25,950 --> 00:25:26,820
why would it,
why,

379
00:25:26,821 --> 00:25:30,780
why isn't this points here that
I'm lighting up say roles preferred

380
00:25:30,940 --> 00:25:35,470
because then a becomes the least
advantaged person and they have less than

381
00:25:36,130 --> 00:25:40,130
exactly right. You got
it. So the reason we turn,

382
00:25:40,160 --> 00:25:45,160
we'd go head east or turn right at the
point of equality is what are we trying

383
00:25:45,591 --> 00:25:46,250
to do in Matt?

384
00:25:46,250 --> 00:25:51,250
We maximizing the minimum share with
saying the person will only want to say is

385
00:25:52,671 --> 00:25:57,650
that whoever turns out to be at the
bottom has the highest possible share.

386
00:25:57,651 --> 00:26:00,740
So if we went from x to down here,

387
00:26:01,490 --> 00:26:05,540
then then we would have changed who at
the bottom. And that's not important.

388
00:26:05,930 --> 00:26:07,640
What would be important is that the,

389
00:26:07,720 --> 00:26:11,480
this bottom share would be
smaller and we wouldn't want that.

390
00:26:11,510 --> 00:26:16,510
So we don't care who gets it because
we don't know whether we're a or B.

391
00:26:17,421 --> 00:26:21,680
That's not material,
right? We had done our way,

392
00:26:21,681 --> 00:26:24,680
the one that when the veil of
ignorance turns out to be lifted,

393
00:26:24,710 --> 00:26:26,720
we don't know where that
we're going to be a or B.

394
00:26:27,230 --> 00:26:31,300
So we just gonna assume we're going to
be whoever turns out to be the worst off.

395
00:26:32,300 --> 00:26:34,690
Okay,
so this is,

396
00:26:34,730 --> 00:26:39,730
this distance here represents the
minimum and you wouldn't want it to get

397
00:26:40,101 --> 00:26:43,470
smaller basically.
Okay.

398
00:26:43,560 --> 00:26:47,700
So if we moved anywhere in this area here,

399
00:26:47,701 --> 00:26:52,410
the minimum share would get
bigger. So if we went to y, right,

400
00:26:52,650 --> 00:26:56,040
then we could do a new l
shaped indifference curve.

401
00:26:56,610 --> 00:27:00,660
Why is it doing this
much later?

402
00:27:01,140 --> 00:27:03,240
Why isn't there a restart
much later button?

403
00:27:06,060 --> 00:27:10,920
Okay, so, so that's the basic
idea. You, you just get the,

404
00:27:10,921 --> 00:27:15,450
keep getting these al shape
indifference curves. Okay.

405
00:27:15,750 --> 00:27:16,583
Now

406
00:27:18,950 --> 00:27:23,120
I want to say something about what a
radical idea this is in a philosophical

407
00:27:23,121 --> 00:27:23,954
sense.

408
00:27:25,720 --> 00:27:30,720
It's not necessarily that radically in
the distributed of sense for the reason

409
00:27:30,791 --> 00:27:33,760
I've already indicated to you.

410
00:27:34,540 --> 00:27:37,080
It could be compatible with,
um,

411
00:27:37,240 --> 00:27:41,410
trickled down if we took the view that
trickle down works better than any other

412
00:27:41,411 --> 00:27:46,270
system from the point of view of the
least demand cottage. So we might say,

413
00:27:47,880 --> 00:27:52,630
if you think about, let's put the,

414
00:27:53,110 --> 00:27:57,700
the,
this role's Bentham and Paredo compared,

415
00:27:57,820 --> 00:28:01,660
this is a lobe taking something of a
liberty because we've got different things

416
00:28:01,661 --> 00:28:06,190
on the axes. So it's sort of a
little, ultimately not coherent,

417
00:28:06,250 --> 00:28:11,200
but I think you can still get an
insight out of it if we, if we, uh,

418
00:28:11,230 --> 00:28:15,880
start with that status quo. We
know watch Paredo preferred, right?

419
00:28:16,390 --> 00:28:21,010
So everything that's Pereda preferred
is also rolls preferred, right?

420
00:28:22,150 --> 00:28:24,430
So if it turned out to be true,

421
00:28:25,030 --> 00:28:29,680
that the best way to help the
person at the bottom is to have,

422
00:28:29,820 --> 00:28:34,660
um, only the market trend, you know, uh,

423
00:28:34,930 --> 00:28:38,210
market transactions,
then we would do it. Um,

424
00:28:38,290 --> 00:28:43,290
but if it turned out that there was other
ways that we're parade out undecidable

425
00:28:43,750 --> 00:28:48,280
like these, um, to help
the person that bottom,

426
00:28:48,281 --> 00:28:49,960
we would do that.
Okay.

427
00:28:50,200 --> 00:28:53,620
So it's not necessarily radical
in a distributive sense.

428
00:28:53,800 --> 00:28:58,180
You could get very egalitarian,
radical redistribution,

429
00:28:58,181 --> 00:29:01,460
but you could also get, uh, a diff, you,

430
00:29:01,470 --> 00:29:05,200
you could get no redistribution and
could get the parade, oh, system.

431
00:29:05,380 --> 00:29:10,380
If that turned out to be the way in which
the most disadvantaged person is help

432
00:29:13,000 --> 00:29:14,960
the most.
Um,

433
00:29:15,460 --> 00:29:20,460
but it's radical in a philosophical sense
that I think is captured by the fact

434
00:29:20,891 --> 00:29:25,000
that it, by the observation
that we don't care whether it,

435
00:29:25,090 --> 00:29:29,860
whether we turn out to be gay or
whether we turn out to B, two, B, B,

436
00:29:30,310 --> 00:29:34,240
two, B, B. Um, and that is

437
00:29:35,880 --> 00:29:36,713
the following.

438
00:29:40,710 --> 00:29:41,543
Okay.

439
00:29:46,060 --> 00:29:51,060
There's been a huge debate
in our lifetimes over whether
the differences between

440
00:29:53,411 --> 00:29:58,340
us are the result of nature
or nurture right now.

441
00:29:58,510 --> 00:30:01,490
Enormous debate. You read, you know,

442
00:30:01,510 --> 00:30:05,890
you read a book like calm
Charles Murray losing ground.

443
00:30:05,920 --> 00:30:07,150
How many people I've heard of that club?

444
00:30:08,220 --> 00:30:11,580
Nobody. Wow. Uh,

445
00:30:13,210 --> 00:30:15,000
quickly, things change. Well, it was a,

446
00:30:15,010 --> 00:30:19,060
it was a book that came out
about probably 20 years ago.

447
00:30:19,061 --> 00:30:21,490
That's probably why you haven't read it.
Uh,

448
00:30:21,520 --> 00:30:26,520
basically saying that the differences
are between us are genetically determined

449
00:30:28,060 --> 00:30:32,080
that genetic differences in Iq,
uh,

450
00:30:32,350 --> 00:30:36,740
that show up in, uh, various
ways includes including racially.

451
00:30:36,741 --> 00:30:39,340
And it was a huge storm of criticism.

452
00:30:39,610 --> 00:30:42,590
He was accused of being a racist,
uh,

453
00:30:42,610 --> 00:30:47,590
and there was charges and counter
charges. Um, and people said, no,

454
00:30:47,591 --> 00:30:50,170
it's not genetics,
it's environment and so on.

455
00:30:50,740 --> 00:30:52,780
So one of the most important things,

456
00:30:52,781 --> 00:30:55,390
and I'm going to focus on
this much more next Monday,

457
00:30:55,391 --> 00:30:58,660
I just want to mention it now
so you can think about it.

458
00:30:59,950 --> 00:31:03,190
One of the,
one of the points roles makes his look

459
00:31:05,230 --> 00:31:08,050
possibly differences between us,
our genetic,

460
00:31:09,190 --> 00:31:09,870
okay.

461
00:31:09,870 --> 00:31:13,800
If the differences between us
such genetic, it just moral luck,

462
00:31:14,820 --> 00:31:15,121
right?

463
00:31:15,121 --> 00:31:20,121
Because you didn't choose to have the
genes that you have and not only didn't

464
00:31:22,231 --> 00:31:24,780
you choose it,
you didn't do anything to get the jeans.

465
00:31:25,000 --> 00:31:27,280
God,
it's moral luck.

466
00:31:28,910 --> 00:31:32,510
On the other hand, suppose differences
between us, our environmental,

467
00:31:34,140 --> 00:31:35,130
well,

468
00:31:37,030 --> 00:31:38,140
it's moral luck.

469
00:31:38,620 --> 00:31:42,460
You didn't choose to be born in the
country and the family you were born into.

470
00:31:43,090 --> 00:31:46,510
You didn't make any choices
in that regard. Furthermore,

471
00:31:46,511 --> 00:31:48,310
you didn't do any work to be

472
00:31:50,210 --> 00:31:51,043
in the

473
00:31:51,860 --> 00:31:56,180
family or the country, country
or the family you happened
to have been raised in.

474
00:31:56,181 --> 00:32:00,740
Again, it's just luck from your point
of view. It's a completely random thing.

475
00:32:00,741 --> 00:32:04,010
You could have been born
somewhere else to somebody else.

476
00:32:04,011 --> 00:32:08,750
So you could have been born to parents
who didn't have the resources that your

477
00:32:08,751 --> 00:32:10,130
parents have.

478
00:32:10,490 --> 00:32:15,490
So this whole debate about nature and
nurture says John Rawls is beside the

479
00:32:16,461 --> 00:32:20,090
point from the standpoint of justice.
We don't care.

480
00:32:23,300 --> 00:32:28,100
And that is his argument. I
think for what it's worth,

481
00:32:28,130 --> 00:32:33,130
the most important argument enrolls a
spot that the differences between us are

482
00:32:34,171 --> 00:32:39,080
morally arbitrary, whether it's
nature or nurture, it doesn't matter.

483
00:32:42,390 --> 00:32:46,290
They're not the result of choice and
they're not really result of work.

484
00:32:47,660 --> 00:32:52,660
They had just fell out of the sky
as far as were actually concerned.

485
00:32:57,460 --> 00:33:00,760
That being the case,
I'm going to,

486
00:33:00,820 --> 00:33:04,300
I'm going to go into the assumptions
behind that in more detail on Monday,

487
00:33:04,301 --> 00:33:08,950
but from the point of view
of this discussion, so we
don't really care if a or B

488
00:33:10,930 --> 00:33:12,190
is the worst off person.

489
00:33:12,191 --> 00:33:16,090
We're just going to say from
the standpoint of justice,

490
00:33:16,091 --> 00:33:18,730
we want to improve the lot
of the worst off person.

491
00:33:18,731 --> 00:33:22,120
And even if the worst off person changes,
you know,

492
00:33:22,121 --> 00:33:25,690
so we go from x, you know, two, two g,

493
00:33:27,770 --> 00:33:28,290
okay.

494
00:33:28,290 --> 00:33:29,580
It's morally irrelevant.

495
00:33:30,180 --> 00:33:35,180
All we want to do is maximize the
share of the person at the bottom.

496
00:33:43,740 --> 00:33:46,920
So that's the roles in
difference principle.

497
00:33:50,910 --> 00:33:51,780
And

498
00:33:59,420 --> 00:34:01,280
as I said,
you can see it,

499
00:34:01,400 --> 00:34:06,400
it overlaps and contained with
and contains the predo principle.

500
00:34:07,010 --> 00:34:10,580
And this has some overlap with Bentham,
uh,

501
00:34:10,610 --> 00:34:14,300
in that it would sanction
moving into the parade. Oh,

502
00:34:14,301 --> 00:34:16,190
undecidable zone here.

503
00:34:16,490 --> 00:34:21,490
That might would be Bentham preferred
if it works for the greatest benefit of

504
00:34:21,831 --> 00:34:25,820
the least advantaged person
and rolls his claim to you.

505
00:34:25,821 --> 00:34:30,740
The reader is,
this is the principle you would choose.

506
00:34:31,340 --> 00:34:36,340
You would want the economic system that
works to the benefit of the person at

507
00:34:37,010 --> 00:34:37,843
the bottom.

508
00:34:40,930 --> 00:34:41,763
What do you think?

509
00:34:50,680 --> 00:34:55,670
Who likes this idea? Who
doesn't like it? What's,

510
00:34:56,150 --> 00:34:57,230
what don't you like about

511
00:35:06,620 --> 00:35:07,670
who was it was, yeah. Yeah.

512
00:35:11,160 --> 00:35:14,440
Uh, it, it assumes that,
uh, once you're born,

513
00:35:14,441 --> 00:35:16,750
you're going to stay in that
position for the rest of your life.

514
00:35:16,751 --> 00:35:18,100
There's nothing you can do about it.

515
00:35:20,050 --> 00:35:22,540
Ah, okay. That, well,
that's a good observation.

516
00:35:24,610 --> 00:35:26,300
I'm not entirely sure what you saying.

517
00:35:26,350 --> 00:35:29,220
Just just explain a little bit more and
I'll see if he was saying what I think,

518
00:35:29,470 --> 00:35:32,650
what about effort that people put
into changing their social position.

519
00:35:34,390 --> 00:35:36,910
Okay. What about effort? I
thought you were making another,

520
00:35:37,200 --> 00:35:40,950
so let me just respond to the
point I thought you were making,

521
00:35:40,951 --> 00:35:44,730
which you weren't making, but we
should nonetheless address, uh,

522
00:35:44,760 --> 00:35:47,700
since people do sometimes make it,
but then I'll come to your point,

523
00:35:47,701 --> 00:35:50,140
which is in any way much more interesting.
The,

524
00:35:50,141 --> 00:35:54,570
the point I thought you were making
is this has no dynamic side to it.

525
00:35:55,380 --> 00:35:58,080
That is to say, um, it's, it's,

526
00:35:58,081 --> 00:36:01,560
it's static and exactly the way
the predo principle is static.

527
00:36:01,561 --> 00:36:06,561
But in any economist would want a theory
that has a dynamic dimension to what

528
00:36:06,781 --> 00:36:08,490
you would want to know over time.

529
00:36:08,820 --> 00:36:13,490
What's the effect of a certain
redistributive change. Uh,

530
00:36:13,491 --> 00:36:16,290
and that, so we would, we
would want to say, you know,

531
00:36:16,630 --> 00:36:17,463
mmm,

532
00:36:18,840 --> 00:36:21,300
we would want to say,
well,

533
00:36:22,590 --> 00:36:27,590
if benefiting the person at
the bottom slightly improves,

534
00:36:27,870 --> 00:36:30,170
improves their welfare,
uh,

535
00:36:31,830 --> 00:36:34,290
in the next three months.

536
00:36:34,560 --> 00:36:39,040
But it comes at the cost of lower
economic growth over time. Would we want,

537
00:36:39,150 --> 00:36:41,100
would we want to do that?
Okay.

538
00:36:41,640 --> 00:36:46,640
And it's fair to say roles doesn't
have an answer to that question.

539
00:36:47,210 --> 00:36:50,970
He doesn't have a dynamic theory. On
the other hand, I think his defense,

540
00:36:51,090 --> 00:36:54,000
this is why it's ultimately not
a very interesting criticism.

541
00:36:54,240 --> 00:36:58,470
I think his defense would kick in that
while I'm telling you what the criteria

542
00:36:58,471 --> 00:37:01,560
and should be,
not how to run the economy.

543
00:37:01,920 --> 00:37:03,930
But let's come to the point about fn

544
00:37:06,210 --> 00:37:11,210
and this has got to to some degree Greek
get us into a next Monday's lecture.

545
00:37:11,851 --> 00:37:16,851
But it's good to make it a start at it
cause it's a very deep point actually.

546
00:37:20,280 --> 00:37:24,240
What about effort?
So yes,

547
00:37:24,241 --> 00:37:29,241
the capacities we have might be
distributed in morally arbitrary ways,

548
00:37:32,240 --> 00:37:33,073
but

549
00:37:34,390 --> 00:37:39,390
some people choose to work really hard
and some people choose to sit on the

550
00:37:39,671 --> 00:37:41,800
couch and watch ESPN all day.

551
00:37:42,930 --> 00:37:43,390
Yeah.

552
00:37:43,390 --> 00:37:48,130
Right. And let's suppose you have
two people with exactly the same Iq,

553
00:37:49,850 --> 00:37:53,960
but one watches ESPN all
day and one studies hard.

554
00:37:53,961 --> 00:37:58,961
So the one who studies hard gets the a
and the one who watches ESPN all day gets

555
00:38:00,750 --> 00:38:03,470
to see and

556
00:38:09,990 --> 00:38:10,823
okay.

557
00:38:12,190 --> 00:38:16,130
I take the input of what you saying. Well
there's some legitimate dessert there.

558
00:38:16,490 --> 00:38:21,160
The person who works should
get the AA. Yeah. Okay.

559
00:38:21,310 --> 00:38:25,630
Now roles sort of with you,

560
00:38:25,660 --> 00:38:30,250
but in a way that I don't think works
for him because if you read Rawls

561
00:38:30,251 --> 00:38:35,020
carefully,
what he says is exactly what you'd said.

562
00:38:35,260 --> 00:38:39,970
He says this, yes, the differences
between us, I'm morally arbitrary,

563
00:38:40,720 --> 00:38:41,553
but

564
00:38:42,610 --> 00:38:46,690
the use we choose to make
of our capacities is not,

565
00:38:49,770 --> 00:38:50,910
why doesn't it work for him?

566
00:38:52,970 --> 00:38:57,650
This was sorta like bent them being
scared of the egalitarian implications of

567
00:38:57,651 --> 00:38:58,191
his theory.

568
00:38:58,191 --> 00:39:02,240
And so he wheels out the difference
between absolute and practical equality of

569
00:39:02,241 --> 00:39:04,420
it hasn't really worked for him
either. What reasons? He said,

570
00:39:04,610 --> 00:39:06,350
why doesn't this really worked for roles?

571
00:39:08,550 --> 00:39:09,383
Yeah,

572
00:39:14,660 --> 00:39:16,260
me,
sorry,

573
00:39:16,520 --> 00:39:20,240
could you say that it's someone's
like naturally, you know,

574
00:39:20,241 --> 00:39:23,510
just by luck given a capacity
or a predilection to work hard.

575
00:39:25,460 --> 00:39:30,350
So that's exactly that. The where
I, I was hoping you would go that

576
00:39:31,970 --> 00:39:35,240
well.
Some people have a

577
00:39:37,070 --> 00:39:39,590
supercharged work ethic
and some people don't.

578
00:39:40,730 --> 00:39:45,650
And why do some people have a supercharged
work ethic because of the way they

579
00:39:45,651 --> 00:39:49,820
were raised? Perhaps. Maybe
some of it's genetic paps,

580
00:39:50,420 --> 00:39:54,530
but why isn't that
morally arbitrary as well?

581
00:39:55,190 --> 00:39:58,010
If the differences in Iq
are morally arbitrary?

582
00:39:58,280 --> 00:40:03,160
So weakness of the well
weakness of the,

583
00:40:03,161 --> 00:40:06,940
well no morally arbitrary too

584
00:40:08,770 --> 00:40:13,300
or strengthened the, well, it's
morally arbitrary. So you know,

585
00:40:13,620 --> 00:40:13,950
the,

586
00:40:13,950 --> 00:40:18,950
the person who sits on the couch watching
ESPN all day just doesn't have the

587
00:40:19,961 --> 00:40:20,794
same,

588
00:40:21,310 --> 00:40:26,310
he doesn't have the moral luck to
have a lot of Protestant work ethic.

589
00:40:28,070 --> 00:40:31,700
So he should,
shouldn't be penalized for that.

590
00:40:33,980 --> 00:40:38,510
So now you can see why roles
doesn't want to go there.

591
00:40:39,440 --> 00:40:40,010
Right?

592
00:40:40,010 --> 00:40:45,010
Because it has the effect of completely
obliterating the concept of any personal

593
00:40:47,001 --> 00:40:47,930
responsibility.

594
00:40:48,750 --> 00:40:49,320
Okay.

595
00:40:49,320 --> 00:40:53,310
Right. Ultimately, because
once you make that move,

596
00:40:57,210 --> 00:40:59,830
why should you differentiate between the,

597
00:40:59,980 --> 00:41:04,860
the weakness of the well or the strength
of the well and say that's not morally

598
00:41:04,861 --> 00:41:05,581
arbitrary,

599
00:41:05,581 --> 00:41:10,581
but differences in Iq are more
morally arbitrary or athletic ability.

600
00:41:10,810 --> 00:41:15,510
RMR morally arbitrary. It
doesn't seem to work. Right.

601
00:41:15,690 --> 00:41:16,370
So it's,

602
00:41:16,370 --> 00:41:20,790
it's not as satisfying way out for roles.

603
00:41:21,030 --> 00:41:25,830
And he does it because he's afraid of
the radical implications of this view.

604
00:41:27,390 --> 00:41:29,940
But what's,
what's interesting about this,

605
00:41:35,290 --> 00:41:37,390
you know, Rawls is fixed, doesn't work,

606
00:41:38,590 --> 00:41:43,180
but he's underlying arguments are
very powerful argument. I mean,

607
00:41:43,181 --> 00:41:46,990
isn't it right? It, isn't it just true

608
00:41:48,970 --> 00:41:49,990
that you know,

609
00:41:50,020 --> 00:41:54,940
the differences between us nature
or nurture are morally arbitrary.

610
00:41:55,830 --> 00:42:00,340
They all, it is moral luck whether
it's genetics, so upbringing,

611
00:42:00,730 --> 00:42:04,030
nothing. You did nothing.
You chose nothing.

612
00:42:04,031 --> 00:42:08,170
You have therefore any
particular right to.

613
00:42:10,890 --> 00:42:11,610
So you know,

614
00:42:11,610 --> 00:42:15,360
you guys think you all worked so hard
to get into Yale and all this and you

615
00:42:15,361 --> 00:42:19,530
deserve to be here.
It's allowed of bug not have.

616
00:42:19,531 --> 00:42:21,960
You deserve to be here
more than anybody else.

617
00:42:24,000 --> 00:42:27,280
That's what he's saying.
You know,

618
00:42:27,281 --> 00:42:31,810
it might be a nice fiction you're telling
yourself, but you know, and his uh,

619
00:42:31,840 --> 00:42:32,171
you know,

620
00:42:32,171 --> 00:42:37,171
as this little exchange showed his
attempt to put some limits on his ideas,

621
00:42:38,440 --> 00:42:42,070
just pathetic. It doesn't
work. But the basic,

622
00:42:42,760 --> 00:42:45,640
the basic argument about
moral arbitrariness,

623
00:42:46,480 --> 00:42:50,640
it's totally compelling.
Anyone here think it's,

624
00:42:50,980 --> 00:42:53,800
it's not compelling and I don't
think it's a good argument.

625
00:42:54,190 --> 00:42:56,950
I don't think it's compelling
because it has implications.

626
00:42:56,951 --> 00:42:58,210
I don't want to live with.

627
00:42:58,960 --> 00:42:59,793
MMM.

628
00:43:02,180 --> 00:43:05,950
B You have to have some other reason.
I mean, it may, well, I think it does.

629
00:43:06,340 --> 00:43:07,240
Let me say this.

630
00:43:07,420 --> 00:43:12,420
I think it has implications that
if you really drill down into them,

631
00:43:13,060 --> 00:43:17,290
probably nobody in this room wants to
live with just like John Rawls didn't want

632
00:43:17,291 --> 00:43:20,740
to live with them,
but what's a good way out?

633
00:43:26,470 --> 00:43:27,303
Okay.

634
00:43:30,130 --> 00:43:32,670
Who thinks I'm wrong?
Who thinks this is a bad argument?

635
00:43:32,680 --> 00:43:33,700
It just not a good argument.

636
00:43:39,940 --> 00:43:40,773
Nobody.

637
00:43:47,890 --> 00:43:48,723
Yeah.

638
00:43:54,280 --> 00:43:59,000
Yeah. Well not that I disagree,
but it seems to strip down
human freewill in that,

639
00:43:59,020 --> 00:44:03,100
um, if the only condition which manages
the circumstances of your birth,

640
00:44:03,340 --> 00:44:07,660
then you don't really have any
choice as to the course of your life.

641
00:44:08,260 --> 00:44:12,970
Um, so it seems, it seems
completely deterministic,

642
00:44:13,000 --> 00:44:13,833
which might not.

643
00:44:15,250 --> 00:44:19,900
Yes and no. It's not it. I think it's
agnostic on the question of freewill.

644
00:44:20,170 --> 00:44:24,910
He's not saying we don't have free,
well, maybe we do. Maybe we done.

645
00:44:24,970 --> 00:44:26,410
I think what he's saying is

646
00:44:28,270 --> 00:44:33,270
if some of us have a greater
say to work hard or to,

647
00:44:34,770 --> 00:44:38,370
you know, engage in delayed
gratification than others,

648
00:44:40,470 --> 00:44:44,550
that is a difference between us. You
know, just as an empirical matter.

649
00:44:44,551 --> 00:44:46,140
That is a difference between us.

650
00:44:46,880 --> 00:44:47,713
MMM.

651
00:44:48,400 --> 00:44:53,400
But the person who has what he's saying
is that the person who has the greater

652
00:44:53,731 --> 00:44:58,731
capacity for deferred gratification or
the greater capacity to work hard isn't

653
00:44:59,291 --> 00:45:03,850
entitled to more benefits than the person
who doesn't happen just in virtue of

654
00:45:03,851 --> 00:45:08,650
that strong strength of the
will. So He's, he's not saying,

655
00:45:08,651 --> 00:45:13,390
I mean, it might also be true
that we don't have free will,

656
00:45:13,420 --> 00:45:14,380
that's another matter.

657
00:45:14,381 --> 00:45:19,210
But I think he's just not taking
a position on that question. So,

658
00:45:19,270 --> 00:45:22,290
um, in that, you know, I,

659
00:45:22,370 --> 00:45:26,450
I think in that sense,
I mean, he, he, he, he,

660
00:45:27,490 --> 00:45:31,690
you could, you could do a two by two in
Philly and all the boxes. He's, he's not,

661
00:45:31,720 --> 00:45:36,220
he's not saying we don't have,
well, we, we, we can't make choices.

662
00:45:36,221 --> 00:45:37,150
He's just saying

663
00:45:38,650 --> 00:45:41,620
the choices that we
make don't give us any,

664
00:45:42,370 --> 00:45:46,870
any particular rights. Now,
I mean, I, I think one is,

665
00:45:47,750 --> 00:45:50,440
and maybe what you're getting at that is,

666
00:45:51,910 --> 00:45:56,860
does pin the tail on the donkey. Is
he what he, what he's saying is, um,

667
00:45:58,150 --> 00:46:01,990
ultimately subversive of the idea
of individual responsibility.

668
00:46:02,650 --> 00:46:05,700
But that's not the same thing
as determinism. You know, when,

669
00:46:05,701 --> 00:46:09,730
when did they come together
and other settings.

670
00:46:09,731 --> 00:46:14,170
So if somebody says, uh,
while I committed the murder,

671
00:46:14,171 --> 00:46:16,350
but I was in the grip of,
of uh,

672
00:46:16,990 --> 00:46:19,900
schizophrenia disorder and
so I didn't have free, well,

673
00:46:20,050 --> 00:46:24,310
so I'm not responsible because
I did, you know, that's when,

674
00:46:25,660 --> 00:46:26,493
uh,

675
00:46:27,680 --> 00:46:30,860
determinism and the issue
of the welcome together,

676
00:46:30,890 --> 00:46:32,660
but he's not making that kind of argument.

677
00:46:32,870 --> 00:46:37,870
He's conceding I think for the purposes
of discussion that the risk free.

678
00:46:38,001 --> 00:46:41,150
Well,
but just saying,

679
00:46:41,640 --> 00:46:44,180
I'm s I'm saying when
you take away his fix,

680
00:46:44,181 --> 00:46:46,400
which really I don't think it does work.

681
00:46:46,880 --> 00:46:51,880
You're saying that differences that flow
from our strengths of well should an

682
00:46:52,121 --> 00:46:53,900
entitle us to anything in particular.

683
00:46:54,440 --> 00:46:59,440
So none of you deserve all the good
things you've gotten in life just because

684
00:47:01,131 --> 00:47:04,310
you work hard. So why, if you work hard,

685
00:47:04,700 --> 00:47:07,070
you have the capacity to work hard.
I let people did it.

686
00:47:08,630 --> 00:47:09,463
So

687
00:47:13,810 --> 00:47:15,760
anyone think it's just
not a good argument?

688
00:47:20,450 --> 00:47:24,800
Anyone think? Seems like a good
argument, but you really don't like it.

689
00:47:27,950 --> 00:47:30,640
At least some who really likes it.

690
00:47:32,310 --> 00:47:34,820
The people I want to go
and watch ESPN all day.

691
00:47:37,120 --> 00:47:37,953
MMM.

692
00:47:39,690 --> 00:47:43,320
You know, and they're,
they're philosophers who
followed the same tuition.

693
00:47:43,321 --> 00:47:47,670
There's a guy called Phillip Perez,
a Belgian political thinker. Yeah.

694
00:47:47,671 --> 00:47:48,540
What were you going to say?

695
00:47:54,330 --> 00:47:55,163
Yeah.

696
00:47:56,610 --> 00:47:57,490
Um, is this, uh,

697
00:47:57,550 --> 00:48:01,690
is this an argument in favor of
complete equality then in terms of

698
00:48:01,691 --> 00:48:04,210
redistribution? Okay, so
that's a good question.

699
00:48:05,600 --> 00:48:08,600
I'll leave Philly out of it.
Cause your question's more important,

700
00:48:08,601 --> 00:48:12,660
not when he has to say, um, is this, uh,

701
00:48:12,710 --> 00:48:15,560
an argument for equality roles?

702
00:48:15,561 --> 00:48:19,040
His answer is a qualified yes.

703
00:48:19,041 --> 00:48:21,440
He's saying it's not an
argument for equality.

704
00:48:21,441 --> 00:48:24,740
It's an argument for the
difference principle.

705
00:48:24,890 --> 00:48:29,840
He's saying it's an argument
for distributing things
in such a way that they

706
00:48:29,841 --> 00:48:32,090
benefit the part,
the person,

707
00:48:32,320 --> 00:48:34,840
the bottom.
Now

708
00:48:37,140 --> 00:48:40,620
you have to have a whole theory of
how the political economy works to say

709
00:48:40,621 --> 00:48:45,540
whether redistribution toward, to absolute
equality would do that. Because if,

710
00:48:45,600 --> 00:48:49,680
if redistribution to equality would
destroy incentives, let's say.

711
00:48:50,220 --> 00:48:55,200
And so that over time, you
know, this would go this way,

712
00:48:55,960 --> 00:49:00,900
um, then it wouldn't be
right. Um, but he's so,

713
00:49:00,930 --> 00:49:04,560
so it's, it's an, he would say we,

714
00:49:04,561 --> 00:49:06,480
what it's an argument for is,

715
00:49:06,570 --> 00:49:10,570
is detaching what we get from
any theory that it's ours as,

716
00:49:10,650 --> 00:49:13,050
as some kind of moral,
right?

717
00:49:14,310 --> 00:49:19,120
And connecting it to what
some theory that the,

718
00:49:19,130 --> 00:49:24,130
the best going theory of the day about
how you organize an economy to benefit

719
00:49:24,871 --> 00:49:27,740
the person at the bottom.
That's what you should do.

720
00:49:28,830 --> 00:49:33,390
If [inaudible] quality does that, you
have a qual. If the market does that,

721
00:49:33,391 --> 00:49:36,930
you have the market,
but it's not anything else.

722
00:49:36,931 --> 00:49:41,931
It's just as pure consequentialist plane
do it in order to help the person at

723
00:49:42,421 --> 00:49:43,254
the bottom.

724
00:49:45,610 --> 00:49:49,660
So Fun paresis point, Phillipe,
fond paresis boy, he says,

725
00:49:50,080 --> 00:49:55,000
while he wrote a book called Real
Freedom for all, and he said, yes,

726
00:49:55,001 --> 00:49:58,480
everybody should get
a minimum basic income

727
00:50:00,540 --> 00:50:05,190
and it shouldn't have anything to do
with their work, their capacity to work.

728
00:50:06,060 --> 00:50:10,070
So, and the famous one liner, he
says, even surface should get pet.

729
00:50:11,430 --> 00:50:14,900
Even surfers should get
paid. We should, uh,

730
00:50:14,940 --> 00:50:17,420
h s s e s fond parades puts it.

731
00:50:17,750 --> 00:50:22,550
There should be the highest sustainable
universal basic income, whatever that is.

732
00:50:24,660 --> 00:50:27,620
And it shouldn't be connected
to work because you know,

733
00:50:27,920 --> 00:50:30,680
capacity to work is morally arbitrary.

734
00:50:31,610 --> 00:50:34,400
So what I'm going to talk about
on Mondays, I'm going to come,

735
00:50:34,401 --> 00:50:39,080
we're going to really dig into this
question because you can now see, I mean,

736
00:50:39,081 --> 00:50:43,160
one of the ironies I want you to
mull over between now and then,

737
00:50:43,370 --> 00:50:48,370
one of the ironies is that this puts
rolls white to the left of marks in a

738
00:50:49,251 --> 00:50:54,251
certain sense because as we
saw marks was as straight up,

739
00:50:57,130 --> 00:50:57,963
uh,

740
00:50:58,100 --> 00:51:03,020
enlightenment theorist wedded to the
whole workmanship idea. Remember,

741
00:51:03,021 --> 00:51:07,400
lock workmanship, labor theory
of value, all that stuff, right?

742
00:51:07,980 --> 00:51:12,290
The Marxist critique of
capitalism was the work.

743
00:51:12,291 --> 00:51:15,530
I doesn't get what he produces.
Rawls is saying,

744
00:51:15,710 --> 00:51:19,670
we don't care in any moral sense.

745
00:51:19,730 --> 00:51:24,730
We don't care who did the work
because the capacity for work isn't,

746
00:51:29,880 --> 00:51:30,661
uh,
uh,

747
00:51:30,661 --> 00:51:35,661
capacity that brings with it any
particular moral valence because

748
00:51:39,600 --> 00:51:43,980
of this moral arbitrariness argument.
Okay?

749
00:51:44,040 --> 00:51:46,680
We will pick up from there next week.

