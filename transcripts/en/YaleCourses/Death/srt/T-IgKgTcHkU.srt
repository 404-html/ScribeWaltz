1
00:00:00,910 --> 00:00:04,640
Well, last time we turn to questions
about the morality of suicide,

2
00:00:05,270 --> 00:00:08,810
and I started with two arguments that
are called quicken dirty arguments.

3
00:00:08,811 --> 00:00:11,570
I suppose it would have been fair to
say that they were really theological

4
00:00:11,571 --> 00:00:12,091
arguments,

5
00:00:12,091 --> 00:00:17,091
so or they were moral arguments that
used in pars theological premises.

6
00:00:20,350 --> 00:00:22,180
I suggested that at least,

7
00:00:22,970 --> 00:00:23,390
okay,

8
00:00:23,390 --> 00:00:25,820
if we look at them at the end there,
quick and dirty versions,

9
00:00:25,821 --> 00:00:26,840
they were inadequate,

10
00:00:27,110 --> 00:00:31,040
and if we'll go ahead and make a more
careful argument about the morality of

11
00:00:31,041 --> 00:00:31,731
suicide,

12
00:00:31,731 --> 00:00:36,731
we need to turn to a more systematic
view about the contents of,

13
00:00:36,881 --> 00:00:37,730
of morality.

14
00:00:37,910 --> 00:00:42,320
We need to look at suicide in terms
of the basic moral principles. Now,

15
00:00:42,560 --> 00:00:45,830
that's not something we've got
the chance to do in detail,

16
00:00:46,310 --> 00:00:50,780
but I think we can at least say enough
about a couple of basic approaches to the

17
00:00:50,781 --> 00:00:55,130
contents of morality or the basic
moral rules to get the beginnings of an

18
00:00:55,131 --> 00:00:58,310
understanding of what might emerge
about the morality of suicide.

19
00:00:58,311 --> 00:01:03,140
If we were to do that more carefully.
So hold off on suicide for the moment.

20
00:01:03,290 --> 00:01:04,430
Let's ask ourselves,

21
00:01:05,150 --> 00:01:09,710
what is it that makes an action morally
acceptable or morally forbidden?

22
00:01:09,980 --> 00:01:14,180
This is unsurprisingly something that
different moral theories disagree about,

23
00:01:14,210 --> 00:01:19,210
but there's at least one factor or one
feature that all or almost all moral

24
00:01:19,791 --> 00:01:24,791
theories agree about and that is that
the consequences of your action matter.

25
00:01:26,150 --> 00:01:30,410
That is we might or might not think that
consequences are the only thing that

26
00:01:30,411 --> 00:01:34,070
are morally relevant. When we think
about the morality of your action,

27
00:01:34,310 --> 00:01:37,550
but surely it is one thing
that's morally relevant.

28
00:01:38,330 --> 00:01:40,400
What are the consequences
of your action going to be?

29
00:01:41,760 --> 00:01:46,760
So let's think about the morality of
suicide with an eye towards consequences.

30
00:01:48,180 --> 00:01:51,060
Bearing in mind that it, since we're
talking about a moral point of view,

31
00:01:51,240 --> 00:01:56,160
we need to take into account
the consequences as they
affect everybody. Now,

32
00:01:56,430 --> 00:02:01,430
the person who of course is most affected
by suicide is of course the person

33
00:02:02,401 --> 00:02:03,450
who's killing themselves.

34
00:02:03,720 --> 00:02:07,650
And at first glance it might seem pretty
clear that the consequences of suicide

35
00:02:07,651 --> 00:02:09,750
are bad for that person.

36
00:02:09,751 --> 00:02:14,751
After all the person was alive and now
they're dead and we normally would take

37
00:02:15,091 --> 00:02:19,090
death to be a bad result. You know,
if I were to tell you, you know,

38
00:02:19,620 --> 00:02:22,590
here's a switch on the wall.
If you were to flip the switch,

39
00:02:23,010 --> 00:02:26,460
a thousand people who would
otherwise be alive would end up dead.

40
00:02:26,490 --> 00:02:30,720
You would normally take that
to be a pretty compelling
argument against flipping

41
00:02:30,721 --> 00:02:34,410
the switch. Why? Because the
results would be bad. Why?

42
00:02:34,440 --> 00:02:37,800
Because a thousand people
would end up dead. Well,

43
00:02:37,860 --> 00:02:42,300
one person ending up dead isn't as bad
as a thousand people ended up dead.

44
00:02:42,301 --> 00:02:43,740
But for all that,

45
00:02:43,830 --> 00:02:48,120
shouldn't we still say it's a bad
consequence and it's a result of that?

46
00:02:48,121 --> 00:02:49,080
Shouldn't we say that?

47
00:02:49,350 --> 00:02:54,350
However far appeal to consequences goes
in terms of giving us our moral theory?

48
00:02:55,780 --> 00:02:55,950
Yeah.

49
00:02:55,950 --> 00:03:00,820
Don't we have to say in terms of
consequences are with regard to suicide is

50
00:03:00,821 --> 00:03:03,430
immoral but not so quick.

51
00:03:04,030 --> 00:03:08,620
Even though it's true that
normally death is a bad thing,

52
00:03:08,770 --> 00:03:10,600
it's not always a bad thing.

53
00:03:10,601 --> 00:03:14,170
This is the sort of thing that we've
learned by thinking about what does the

54
00:03:14,171 --> 00:03:16,660
badness of death consistent,

55
00:03:18,370 --> 00:03:23,370
typical cases are ones in
which the person's dying
robs them of a chunk of life

56
00:03:25,151 --> 00:03:26,320
that would have been good for them.

57
00:03:26,321 --> 00:03:30,400
Overall and because of that
dying then is bad for them.

58
00:03:30,670 --> 00:03:35,200
But in the kinds of cases
that we're thinking about
cases where suicide would be

59
00:03:35,201 --> 00:03:39,670
rationally acceptable and we're now
asking whether or not it's morally

60
00:03:39,671 --> 00:03:42,130
acceptable in those sorts of cases,

61
00:03:42,131 --> 00:03:45,250
at least the kind of paradigm
examples that we've been focused on,

62
00:03:45,610 --> 00:03:48,730
the person is better off dead,
they're better off dead,

63
00:03:48,760 --> 00:03:52,030
meaning that what life
now holds out for them,

64
00:03:52,031 --> 00:03:56,350
although perhaps not negative through
and through, is negative on balance.

65
00:03:56,920 --> 00:03:58,450
It's negative on balance.

66
00:03:58,720 --> 00:04:03,640
They're not better off continuing
to live the better off dying,

67
00:04:04,300 --> 00:04:08,410
and that means of course the dying isn't
bad for them, but rather good for them.

68
00:04:08,560 --> 00:04:11,350
And so their death is
not a bad consequence,

69
00:04:11,530 --> 00:04:16,530
but rather a good consequence provided
that you're prepared to accept the

70
00:04:17,201 --> 00:04:22,201
possibility of cases in which somebody
will be better off if their life ended

71
00:04:22,211 --> 00:04:23,800
sooner rather than later.

72
00:04:24,970 --> 00:04:29,970
We're led to the conclusion that from the
moral point of view as far as focusing

73
00:04:30,071 --> 00:04:35,071
on consequences goes to consequences
might actually be good rather than bad.

74
00:04:36,130 --> 00:04:40,510
If the person were to kill themselves,
they will free themself.

75
00:04:40,570 --> 00:04:44,950
Let's suppose of the suffering they
would otherwise have to undergo.

76
00:04:46,960 --> 00:04:51,070
Well, that's first glance said
consequences says suicide's wrong.

77
00:04:51,071 --> 00:04:55,090
Second glance says consequences.
It says at least in certain circumstances,

78
00:04:55,091 --> 00:04:56,890
suicides, right? Of course,

79
00:04:56,950 --> 00:05:01,950
third glance suggests we can't just focus
on consequences for the person who's

80
00:05:03,820 --> 00:05:07,300
contemplating suicide because from
the point of view of morality,

81
00:05:07,301 --> 00:05:10,480
we have to look at the
consequences for everybody.

82
00:05:10,600 --> 00:05:15,600
Who else might get affected by the
death or suicide of the of the person?

83
00:05:18,040 --> 00:05:18,221
Well,

84
00:05:18,221 --> 00:05:22,840
the most obvious people for us to think
about it that point then are the family

85
00:05:22,841 --> 00:05:23,830
and loved ones.

86
00:05:24,250 --> 00:05:28,960
The people who most directly know
about and care about the person who is

87
00:05:28,961 --> 00:05:32,890
contemplating suicide, and again,
I'm running out of glances,

88
00:05:32,891 --> 00:05:36,520
but at first glance you
might say, well, they're,

89
00:05:36,521 --> 00:05:40,600
the consequences are clearly bad when
when the person kills themselves,

90
00:05:40,601 --> 00:05:45,601
that causes typically a great deal of
distress for the family and friends of the

91
00:05:46,031 --> 00:05:47,740
person who's killed themself.

92
00:05:49,690 --> 00:05:51,040
Even if that's true,

93
00:05:51,041 --> 00:05:56,041
we now have to ask how do the
consequences way out after all we,

94
00:05:57,440 --> 00:06:02,440
we live in a world in which no single
app typically has only good consequences

95
00:06:02,841 --> 00:06:06,110
or no single acts has bad consequences
and only bad consequences.

96
00:06:06,320 --> 00:06:10,460
Often our choices are mixed packages
where we have to ask whether the good that

97
00:06:10,461 --> 00:06:14,780
we can do is greater than the bad that
we'd be doing with this act or that act

98
00:06:14,810 --> 00:06:15,860
or some third act.

99
00:06:17,830 --> 00:06:22,030
Even if there are then
negative consequences in
terms of distressed and the

100
00:06:22,031 --> 00:06:25,300
family, friends and loved ones
of the person who kills himself,

101
00:06:25,420 --> 00:06:30,420
that might still be outweighed by the
benefit to the person himself or herself.

102
00:06:31,000 --> 00:06:32,770
If it was really the case
that she'd be better,

103
00:06:32,800 --> 00:06:35,290
he or she will be better off dying,

104
00:06:38,800 --> 00:06:43,800
but it's also worth bearing in mind
that in so far as we're thinking about

105
00:06:44,051 --> 00:06:49,030
people who will love and care about
the person who's considering dying,

106
00:06:50,320 --> 00:06:55,320
then they may actually overall on balance
be relieved that this suffering of

107
00:06:59,081 --> 00:07:01,330
their loved one has come to an end.

108
00:07:01,950 --> 00:07:02,783
Look,

109
00:07:03,840 --> 00:07:08,840
we will of course all be
horribly distressed that
that nature or the fates or

110
00:07:10,471 --> 00:07:11,041
what have you,

111
00:07:11,041 --> 00:07:16,041
has brought it about that this person's
choices are now reduced to killing

112
00:07:16,111 --> 00:07:21,111
themselves on the one hand or continuing
this terminal stages of some illness

113
00:07:22,020 --> 00:07:25,470
where they're incapacitated and in pain.
We will of course,

114
00:07:25,471 --> 00:07:29,580
wish there was a serious
prospect of a cure,

115
00:07:29,760 --> 00:07:34,230
some chance of recovery. Wish they'd
never gotten ill in the first place,

116
00:07:35,400 --> 00:07:38,220
but given the limited choices,

117
00:07:38,610 --> 00:07:41,130
continued suffering and
pain on the one hand,

118
00:07:41,310 --> 00:07:44,070
or having an end to
that suffering and pain,

119
00:07:45,270 --> 00:07:50,270
if the person can rationally assess
their prospects and reasonably come to

120
00:07:51,331 --> 00:07:56,331
believe they're better off
dead than that's a judgment
their loved ones can come

121
00:07:56,881 --> 00:07:58,590
to share as well.

122
00:07:58,890 --> 00:08:02,580
They may well regret the
fact more than regret.

123
00:08:02,640 --> 00:08:07,110
Curse the fact that these are
the all new choices they've got,

124
00:08:07,140 --> 00:08:10,770
but still given the limited choices,
they may agree,

125
00:08:10,800 --> 00:08:14,850
they may come to agree better
to put an end to the suffering.

126
00:08:15,360 --> 00:08:20,040
And so when the person kills themselves,
they May 2nd that choice.

127
00:08:20,041 --> 00:08:23,970
They may say at least they're
not in pain and agony anymore.

128
00:08:25,890 --> 00:08:29,490
So if we look at it from the point
of view of consequences, in fact,

129
00:08:29,550 --> 00:08:34,550
suppose we had a moral view that said
consequences are on just one thing that

130
00:08:35,611 --> 00:08:38,640
was morally relevant.
In thinking about the right,

131
00:08:38,700 --> 00:08:40,290
what makes it an action right or wrong?

132
00:08:40,390 --> 00:08:44,760
Suppose we took the bold claim that
consequences are the only thing that's

133
00:08:44,761 --> 00:08:48,920
morally relevant. There are moral
views that that that take his position.

134
00:08:48,940 --> 00:08:53,610
I suppose the best known example of this
kind of consequence only approached him

135
00:08:53,611 --> 00:08:54,630
morality is

136
00:08:56,370 --> 00:09:00,270
utilitarianism is the moral
doctrine that says right and wrong.

137
00:09:00,271 --> 00:09:05,271
As a matter of producing as much
happiness for everybody as possible,

138
00:09:05,700 --> 00:09:10,700
counting everybody's happiness equally
and when you can't produce happiness than

139
00:09:10,711 --> 00:09:13,830
at least trying to minimize
the misery and suffering,

140
00:09:13,950 --> 00:09:17,070
counting everybody's misery
and suffering equally.

141
00:09:17,310 --> 00:09:20,970
So suppose we accept this
utilitarian position.

142
00:09:22,080 --> 00:09:27,080
What conclusions would we come to
then about the morality of suicide?

143
00:09:27,810 --> 00:09:32,810
I suppose the conclusion would be a kind
of moderate one and the one hand we'd

144
00:09:33,681 --> 00:09:38,681
be rejecting the extreme that says suicide
is never morally acceptable because

145
00:09:40,771 --> 00:09:45,771
to say that you'd have to be claiming
suicide always has bad consequences

146
00:09:46,141 --> 00:09:49,770
overall, and that strikes me,
although it's an empirical claim,

147
00:09:49,771 --> 00:09:53,130
it strikes me as a rather
implausible empirical claim.

148
00:09:53,490 --> 00:09:55,110
It's sadly enough,

149
00:09:55,170 --> 00:10:00,170
not too difficult to describe cases in
which the results may actually be better

150
00:10:02,191 --> 00:10:06,480
if the person kills themselves rather
than having their suffering continue.

151
00:10:07,700 --> 00:10:10,440
It may be better for them
and better for their family.

152
00:10:11,880 --> 00:10:15,690
On the other hand, we certainly
wouldn't want if we were utilitarians,

153
00:10:15,691 --> 00:10:20,280
we also wouldn't want to go to the
other extreme and say suicide is always

154
00:10:20,310 --> 00:10:24,690
morally acceptable because of course to
say that it's always morally acceptable

155
00:10:24,750 --> 00:10:29,750
is to say that the consequences are
never bad when you kill yourself,

156
00:10:30,180 --> 00:10:34,020
and that's also pretty obviously
an implausible thing to claim.

157
00:10:35,150 --> 00:10:39,840
You guys are young, you're healthy,
you've got a great future in front of you.

158
00:10:40,290 --> 00:10:42,930
If you were to kill yourself,
the results wouldn't be good.

159
00:10:42,960 --> 00:10:47,730
The results would be worse overall
than if you had refrained from killing

160
00:10:47,731 --> 00:10:52,500
yourself.
So utilitarian position is in the middle.

161
00:10:52,501 --> 00:10:54,810
It doesn't say suicides,
never acceptable.

162
00:10:54,840 --> 00:10:57,150
It doesn't say suicide
is always acceptable.

163
00:10:57,210 --> 00:11:02,210
It says perhaps unsurprisingly
it's sometimes acceptable.

164
00:11:02,520 --> 00:11:05,610
It depends on the facts.
It depends on the results.

165
00:11:05,760 --> 00:11:09,450
It depends on comparing
the results of this action,

166
00:11:09,480 --> 00:11:13,050
killing yourself to the
alternatives open to you.

167
00:11:15,930 --> 00:11:17,490
We have to ask,

168
00:11:18,270 --> 00:11:21,000
is your life worse than nothing?

169
00:11:21,390 --> 00:11:25,470
Is there some medical procedure
available to you that would cure you?

170
00:11:26,130 --> 00:11:28,410
If there is, and even if your
life is worse than nothing,

171
00:11:28,530 --> 00:11:31,380
that still doesn't make it the best
choice in terms of the consequences.

172
00:11:31,560 --> 00:11:35,580
Getting medical help is a preferable
choice in terms of the consequences.

173
00:11:36,630 --> 00:11:40,710
We can even think of cases where
your life is worse than nothing.

174
00:11:40,711 --> 00:11:45,711
You'd be better off dead and there is no
medical alternative of a cure available

175
00:11:45,811 --> 00:11:47,280
to you.
But for all of that,

176
00:11:47,310 --> 00:11:52,050
it still isn't morally legitimate to
kill yourself in terms of the utilitarian

177
00:11:52,051 --> 00:11:57,051
outlook because as always we have to
think about the consequences for others.

178
00:11:57,760 --> 00:12:02,760
And there may be others who'd be so
adversely affected by your death that the

179
00:12:03,761 --> 00:12:08,761
harm to them outweighs the costs
to you of keeping yourself alive.

180
00:12:09,340 --> 00:12:14,290
Suppose for example, that you're
the single parent of young children.

181
00:12:14,980 --> 00:12:19,980
You've got a kind of moral obligation
to look after them if you were to die,

182
00:12:21,750 --> 00:12:24,580
you know,
they really have it horribly.

183
00:12:26,220 --> 00:12:28,770
It's conceivable that in cases like that,

184
00:12:28,771 --> 00:12:33,771
the suffering of your children wore
you to kill yourself without way the

185
00:12:35,341 --> 00:12:40,050
suffering that you'd have to undergo war
you to keep yourself alive for the sake

186
00:12:40,051 --> 00:12:43,980
of your children.
So it all depends on the facts.

187
00:12:44,250 --> 00:12:45,120
Still,

188
00:12:46,290 --> 00:12:49,260
if we accept the utilitarian position,

189
00:12:49,410 --> 00:12:53,100
we do end up with a moderate conclusion.

190
00:12:53,430 --> 00:12:55,410
In certain circumstances,

191
00:12:55,440 --> 00:12:58,920
suicide will be morally justified.

192
00:12:59,160 --> 00:13:04,160
Roughly speaking in those cases where
you're better off dead and the effects on

193
00:13:07,321 --> 00:13:10,830
others aren't so great
as to outweigh that,

194
00:13:11,220 --> 00:13:15,570
those will be that sort of paradigm
cases in which suicide makes sense or as

195
00:13:15,571 --> 00:13:19,260
legitimate morally speaking from
the utilitarian perspective.

196
00:13:20,920 --> 00:13:21,753
Okay?

197
00:13:21,800 --> 00:13:22,370
But of course,

198
00:13:22,370 --> 00:13:27,170
that doesn't mean that suicide is indeed
ever morally legitimate because we

199
00:13:27,171 --> 00:13:31,820
don't necessarily want to embrace
the utilitarian theory of morality.

200
00:13:31,970 --> 00:13:34,970
Utilitarianism is what you get.
Roughly speaking.

201
00:13:35,150 --> 00:13:39,320
When we say consequences matter
and there are all that matters,

202
00:13:40,960 --> 00:13:41,793
but

203
00:13:42,820 --> 00:13:47,820
most of us are inclined to think
that there's more to morality than

204
00:13:48,370 --> 00:13:49,480
consequences.

205
00:13:49,750 --> 00:13:54,220
Most of us are inclined to think that
there are cases in which actions can have

206
00:13:54,221 --> 00:13:55,660
bad results,

207
00:13:55,720 --> 00:14:00,720
whether actions can have good results
and yet for all that be morally forbidden

208
00:14:02,500 --> 00:14:07,030
or actions could have bad results
and yet for all that still be morally

209
00:14:07,031 --> 00:14:11,890
required. That's not to say that
consequences don't matter morally,

210
00:14:12,310 --> 00:14:17,310
it's to claim rather that consequences
aren't the only thing that matter.

211
00:14:17,470 --> 00:14:22,450
Morally. Consequences can be outweighed
by other morally relevant factors.

212
00:14:22,630 --> 00:14:23,170
Well,

213
00:14:23,170 --> 00:14:28,170
that's the position that's held by
the branch of moral theory known as

214
00:14:31,001 --> 00:14:32,200
Deontology.

215
00:14:32,650 --> 00:14:37,650
So deontologists say other things
matter morally besides consequences and

216
00:14:39,521 --> 00:14:41,830
deciding whether or not your
action is right or wrong.

217
00:14:42,010 --> 00:14:43,570
You have to pay attention
to the consequences,

218
00:14:43,690 --> 00:14:45,910
but you have to pay attention
to the other things as well.

219
00:14:47,750 --> 00:14:49,790
What other things?
Well and surprisingly,

220
00:14:49,910 --> 00:14:54,910
this is a area in which
different deontologists we'll
disagree one to the next

221
00:14:56,630 --> 00:15:01,630
in terms of what else they want to add
to the list of morally relevant factors.

222
00:15:02,780 --> 00:15:07,780
But there's one kind of additional factor
that most of Austin are deontological

223
00:15:09,981 --> 00:15:13,010
moods would want to add to the
list and that to this. So one,

224
00:15:13,011 --> 00:15:14,360
at any rate that relevant,

225
00:15:14,361 --> 00:15:17,600
I think most directly relevant
for thinking about suicide,

226
00:15:19,130 --> 00:15:24,130
that factor is the factor of not just
what was the upshot of your action,

227
00:15:25,850 --> 00:15:28,940
but how you produced that upshot,

228
00:15:29,180 --> 00:15:31,790
not just what the results were,

229
00:15:31,970 --> 00:15:36,970
but what was your means of getting those
results and more particularly still did

230
00:15:39,021 --> 00:15:43,190
you have to harm anybody
to produce the results?

231
00:15:44,210 --> 00:15:49,210
Most of us are inclined to think it's
wrong to harm people or at least innocent

232
00:15:52,251 --> 00:15:54,560
people.
It's wrong to harm innocent people.

233
00:15:54,860 --> 00:15:57,830
Even if the results of
doing that might be good.

234
00:15:58,310 --> 00:16:02,960
Now I threw in the qualification about
innocent people because of course it's

235
00:16:02,961 --> 00:16:06,250
also true that most of us are
inclined to think that sue,

236
00:16:06,300 --> 00:16:09,230
a self defense might be justified,
uh,

237
00:16:09,410 --> 00:16:13,310
harming people who are attacking you or
your friends or your fellow countrymen

238
00:16:13,490 --> 00:16:15,110
that may be legitimate.

239
00:16:15,290 --> 00:16:19,460
And so it's not as though we want to say
it's never legitimate to harm somebody,

240
00:16:21,290 --> 00:16:23,870
but those people are guilty.
There are aggressors.

241
00:16:24,500 --> 00:16:29,270
What most of us in our deontological
moods are inclined to think is it's never

242
00:16:29,300 --> 00:16:32,600
legitimate to harm an innocent person.

243
00:16:33,170 --> 00:16:35,660
And the crucial point is that's true.

244
00:16:35,690 --> 00:16:40,490
Even if the results would be better.
Look,

245
00:16:41,360 --> 00:16:46,190
there's no debate between deontologists
and utilitarians about harming innocent

246
00:16:46,191 --> 00:16:49,100
people in the normal case.
Because normally of course, yeah,

247
00:16:49,101 --> 00:16:52,970
I suppose that to make an example, to end
the class with a nice big bang, right,

248
00:16:53,220 --> 00:16:55,730
I brought my Uzi submachine gun,

249
00:16:55,731 --> 00:16:59,960
I now take it and go ratatat
tat killing 15 of you. Well,

250
00:17:00,020 --> 00:17:03,980
that would not be something
that would have good results.

251
00:17:04,550 --> 00:17:09,550
And so clearly the utilitarians
going to reject that as well as the

252
00:17:11,271 --> 00:17:14,750
deontologists through an agreement
about that. And the typical case,

253
00:17:14,870 --> 00:17:19,070
killing an innocent person
has bad results, harms
them. It's wrong. Full Stop.

254
00:17:19,071 --> 00:17:19,904
We're done.

255
00:17:19,910 --> 00:17:24,910
But what should we say about cases where
killing an innocent person has better

256
00:17:26,870 --> 00:17:27,703
results?

257
00:17:29,010 --> 00:17:29,540
Okay.

258
00:17:29,540 --> 00:17:31,940
In real life,
it's hard to think of cases like that,

259
00:17:31,970 --> 00:17:36,860
but we can at least go science
fictiony and tell an example.

260
00:17:37,190 --> 00:17:41,270
So here's one of my favorite
examples. Uh, in moral philosophy,

261
00:17:42,590 --> 00:17:47,590
suppose that we have five patients in a
hospital who are going to die because of

262
00:17:50,191 --> 00:17:53,850
Oregon failures of one sort or another.
One of them needs a heart transplant.

263
00:17:54,030 --> 00:17:55,860
One of them needs a kidney transplant,

264
00:17:55,980 --> 00:17:59,340
one of them needs a liver
transplant and so forth and so on.

265
00:17:59,670 --> 00:18:03,750
Unfortunately,
because of tissue incompatibilities,

266
00:18:03,900 --> 00:18:05,250
even as they begin to die,

267
00:18:05,251 --> 00:18:09,180
we can't use the organs from the ones
that have died to save the others.

268
00:18:10,380 --> 00:18:11,520
Meanwhile,

269
00:18:12,660 --> 00:18:17,660
here in the hospital for a routine
checkup is John John's perfectly healthy.

270
00:18:19,260 --> 00:18:21,540
And as you're doing your exams on him,

271
00:18:21,541 --> 00:18:26,541
you discover that he's exactly suitable
to be an organ donor for all five of the

272
00:18:28,651 --> 00:18:29,484
patients.

273
00:18:30,300 --> 00:18:35,300
And it occurs to you that if you were to
find some way to kill him but cover up

274
00:18:36,390 --> 00:18:39,540
the cause of death so
it look like he died.

275
00:18:39,540 --> 00:18:42,450
If some unexpected freak seizure,

276
00:18:43,740 --> 00:18:47,520
you can then use his
organs to save the five.

277
00:18:47,550 --> 00:18:51,060
This one gets the kidney, that one gets
the other kidney, that one gets the heart,

278
00:18:51,061 --> 00:18:53,160
that one gets deliver,
that one gets the lungs.

279
00:18:54,600 --> 00:18:56,850
So your choice roughly is this,

280
00:18:57,570 --> 00:19:01,080
just give John his routine medical exam,

281
00:19:01,440 --> 00:19:06,440
in which case the five other patients die
or chop up John Killam and chop him up

282
00:19:08,610 --> 00:19:11,640
using his organs to
save the five patients.

283
00:19:13,230 --> 00:19:17,820
Well, what should we say is the right
thing to do in the organ transplant case?

284
00:19:19,650 --> 00:19:24,270
In terms of consequences, it looks
as though if we tell the story right,

285
00:19:24,271 --> 00:19:29,271
at least their results would be
better if we chop up John after all,

286
00:19:29,551 --> 00:19:34,290
it's one versus five and although the
death of John's a horrible bad result,

287
00:19:34,410 --> 00:19:38,040
the death of the five is a horrible
bad result and saw the results would be

288
00:19:38,041 --> 00:19:41,460
better if we were to kill innocent John.

289
00:19:42,840 --> 00:19:45,810
Well, if we had more time we could argue
about are the results really going to be

290
00:19:45,811 --> 00:19:48,180
better? Is that a realistic
story? What have you,

291
00:19:48,181 --> 00:19:51,600
either other long term effects on the
health care profession that we haven't

292
00:19:51,601 --> 00:19:52,440
taken into account,

293
00:19:52,590 --> 00:19:56,610
but we don't have time to really
pursue this story in detail.

294
00:19:56,970 --> 00:20:01,320
Let's just suppose we could
eventually get the details right.

295
00:20:01,860 --> 00:20:05,490
The results really would be
better if we chopped up John,

296
00:20:06,630 --> 00:20:08,190
is that the right thing to do?
Well,

297
00:20:08,220 --> 00:20:11,010
maybe utilitarianism says
it's the right thing to do,

298
00:20:11,100 --> 00:20:14,820
but it's precisely for that reason that
most of us would then say, you know,

299
00:20:14,821 --> 00:20:17,790
there's more to morality than
what utilitarianism says.

300
00:20:18,450 --> 00:20:22,320
Now whether that objections are good ones,
a very,

301
00:20:22,350 --> 00:20:26,820
very complicated question and if you,
if you'd like to pursue it,

302
00:20:26,821 --> 00:20:27,720
if you want to pursue it,

303
00:20:27,840 --> 00:20:32,310
then I invite you to take an
introductory class in moral philosophy.

304
00:20:32,550 --> 00:20:33,480
For our purposes,

305
00:20:33,540 --> 00:20:38,540
let's just suppose that most of us are
on board with the deontologists when they

306
00:20:39,511 --> 00:20:44,511
say there's more to morality than what
the utilitarian has and this example

307
00:20:45,151 --> 00:20:46,140
brings it out.

308
00:20:46,230 --> 00:20:50,470
It's wrong to kill
somebody who's innocent.

309
00:20:51,160 --> 00:20:55,000
Even though by hypothesis the results
would be better. It's five to one.

310
00:20:55,360 --> 00:20:59,230
People have a right to life,
a right not to,

311
00:20:59,260 --> 00:21:04,260
he killed and that right ways in when
we're deciding what to do morally so that

312
00:21:05,471 --> 00:21:07,750
it's wrong to kill an innocent person,

313
00:21:07,780 --> 00:21:10,210
even if the results really
would be better. All right,

314
00:21:10,270 --> 00:21:14,920
let's suppose we agree with
that. Agree. Except that again,

315
00:21:15,850 --> 00:21:18,760
in a fuller class on more a philosophy
we'd have to ask ourselves what's the

316
00:21:18,761 --> 00:21:21,610
basis of that, right? What
other deontological rights too?

317
00:21:21,611 --> 00:21:25,060
People have what exactly are
the contours of that? Right?

318
00:21:25,090 --> 00:21:28,510
But here we can just ask.
Suppose we accept a right like that.

319
00:21:28,570 --> 00:21:33,570
What are the implications of that for
the morality of suicide and now it seems

320
00:21:34,151 --> 00:21:38,110
what we have to say is suicide is wrong.

321
00:21:38,260 --> 00:21:43,150
Suicide is morally unacceptable.
Because when I kill myself,

322
00:21:43,570 --> 00:21:45,370
well I'm killing somebody.

323
00:21:45,850 --> 00:21:50,410
And didn't we just say as deontologists
that killing an innocent person and I'm

324
00:21:50,411 --> 00:21:55,210
an innocent person, killing an
innocent person is morally wrong. Well,

325
00:21:55,240 --> 00:21:58,600
I'm a person,
so killing me is morally wrong.

326
00:22:00,220 --> 00:22:03,610
And it's not really any help
to come back and say, but look,

327
00:22:03,640 --> 00:22:08,640
we stipulated that this is a case
where the person's better off dead.

328
00:22:10,000 --> 00:22:14,650
The results will really be better
overall if he kills himself. Said,

329
00:22:14,770 --> 00:22:18,340
yeah, that's right. Maybe that
is right. It doesn't matter.

330
00:22:18,341 --> 00:22:23,341
Because as deontologists we said
the right to life is so powerful,

331
00:22:23,530 --> 00:22:28,270
it outweighs consequences.
Just as it was wrong to chop up John,

332
00:22:28,300 --> 00:22:31,030
even though the results would be better,

333
00:22:31,060 --> 00:22:34,930
five versus one drawn to kill yourself,

334
00:22:34,960 --> 00:22:37,300
even if the results would be better,

335
00:22:37,630 --> 00:22:40,510
even if that's the only way to put
yourself out of pain and those are good

336
00:22:40,511 --> 00:22:41,860
results,
it doesn't matter.

337
00:22:41,890 --> 00:22:46,120
The right to life outweighs
the appeal to consequences.

338
00:22:46,210 --> 00:22:51,210
So as deontologists it seems we
have to say suicide is forbidden.

339
00:22:52,890 --> 00:22:57,010
Stop.
Well as usual and philosophy,

340
00:22:57,011 --> 00:23:01,510
it's not quite as simple as that one
possible response. Somebody might make us,

341
00:23:01,600 --> 00:23:06,600
but look morality's only
about how I treat others.

342
00:23:08,200 --> 00:23:13,200
It's not about how I treat myself
and if we were to accept that claim,

343
00:23:15,310 --> 00:23:20,310
then we could say the right to life only
covers how I treat others in particular

344
00:23:21,551 --> 00:23:24,940
rules up my killing. Other people,
even when the results would be good,

345
00:23:25,420 --> 00:23:30,420
but it doesn't have any implications
for how I treat myself and in particular

346
00:23:30,461 --> 00:23:34,990
then if the right to life
doesn't exclude self killing,

347
00:23:34,991 --> 00:23:36,910
well then suicide is acceptable.

348
00:23:39,130 --> 00:23:41,260
That's a possible moral view.

349
00:23:41,470 --> 00:23:46,470
But I find it rather implausible if we
want to start to explain what it is about

350
00:23:49,940 --> 00:23:53,750
you that explains why it's
wrong for me to kill you,

351
00:23:53,960 --> 00:23:57,890
we'd start saying things about how well
you're a person and as such you've got

352
00:23:57,891 --> 00:23:59,870
all these plans and so forth and so on.

353
00:23:59,871 --> 00:24:03,680
And as a person you've got certain rights,

354
00:24:03,770 --> 00:24:05,360
certain things that
shouldn't be done to you.

355
00:24:06,310 --> 00:24:06,800
Yeah,

356
00:24:06,800 --> 00:24:08,030
you're not just,

357
00:24:08,390 --> 00:24:11,930
this is the thought that lies behind
much deontological thinking, right?

358
00:24:12,140 --> 00:24:16,700
People aren't objects. We
can't just destroy them for
the sake of better results.

359
00:24:17,840 --> 00:24:19,880
Well, that's right. People aren't objects,

360
00:24:20,530 --> 00:24:25,530
but of course I'm a person too and
so when I contemplate killing myself,

361
00:24:26,690 --> 00:24:29,750
I'm contemplating destroying a person.

362
00:24:31,420 --> 00:24:36,420
So it's at least difficult to see why
we would accept the claim that morality

363
00:24:38,051 --> 00:24:41,590
only governs how I treat other people,

364
00:24:42,340 --> 00:24:45,610
it seems.
Although the issue is a complicated one,

365
00:24:45,611 --> 00:24:47,560
which we don't have time
to pursue further today.

366
00:24:49,320 --> 00:24:54,320
It seems to me more plausible to say
morality includes rules not only governing

367
00:24:54,661 --> 00:24:57,720
how I treat others,
but also how I treat myself.

368
00:24:59,930 --> 00:25:04,130
Yeah, if that's right and if among
the moral rules are a right to life,

369
00:25:04,131 --> 00:25:06,410
a prohibition against harming people,

370
00:25:09,440 --> 00:25:13,720
then don't we have to say, look, it's,

371
00:25:13,760 --> 00:25:18,500
it's a, it's wrong from the deontological
perspective to to kill yourself.

372
00:25:21,590 --> 00:25:25,190
Well, of course the natural response to
this line of thought is to say, but look,

373
00:25:25,400 --> 00:25:30,080
when I kill myself, unlike the case of
chopping up John to say five others,

374
00:25:30,170 --> 00:25:34,160
when I kill myself,
I'm doing it for my own sake.

375
00:25:35,180 --> 00:25:38,480
I'm harming myself for my own sake.

376
00:25:38,540 --> 00:25:43,540
That seems highly relevant in
thinking about the morality of suicide

377
00:25:46,880 --> 00:25:47,840
does seem relevant though.

378
00:25:47,841 --> 00:25:50,810
It's not a hundred percent clear
what to do with that thought.

379
00:25:50,870 --> 00:25:55,100
Here's two possible interpretations
of that thought. First of all,

380
00:25:55,250 --> 00:25:56,770
you might think that the,
the,

381
00:25:56,900 --> 00:26:01,280
the relevance of saying that I'm
harming myself for my own sake is this.

382
00:26:01,970 --> 00:26:05,660
If I'm harming myself for my own sake,
what I'm saying is,

383
00:26:05,661 --> 00:26:09,560
despite the fact that I'm harming
myself, I'm better off. After all,

384
00:26:09,680 --> 00:26:14,030
we stipulated that we were focusing on
cases in which suicide was rational,

385
00:26:14,510 --> 00:26:19,160
so the person's better off dead if they're
better off dead, that'll cost then.

386
00:26:19,250 --> 00:26:23,460
Although it's certainly true that there's
a sense in which they're harming you,

387
00:26:23,461 --> 00:26:26,000
saw them killing yourself
as doing harm to yourself.

388
00:26:26,120 --> 00:26:29,480
Still it's not harm overall.

389
00:26:29,540 --> 00:26:34,250
The bottom line we were imagining
is positive when you kill yourself.

390
00:26:35,330 --> 00:26:36,410
And so,

391
00:26:36,500 --> 00:26:41,500
although unlike the case of John where
you've harmed him and benefited others,

392
00:26:42,500 --> 00:26:45,330
so you have harmed him overall,

393
00:26:47,150 --> 00:26:48,380
in the case of suicide,

394
00:26:48,381 --> 00:26:52,940
when I harm myself to avoid the
suffering I would otherwise go through,

395
00:26:53,270 --> 00:26:55,970
I'm not really as we might say,

396
00:26:56,090 --> 00:26:59,030
harming myself overall.

397
00:27:00,320 --> 00:27:05,210
So perhaps the deontological prohibition
against harm is really a prohibition

398
00:27:05,211 --> 00:27:08,660
against harming people overall.

399
00:27:09,420 --> 00:27:10,253
Look,

400
00:27:11,420 --> 00:27:15,690
you, you got some sort of a
disease, uh, in your infection,

401
00:27:15,691 --> 00:27:20,691
in your leg that has now spread and it's
going to kill you unless we amputate

402
00:27:20,911 --> 00:27:24,960
your leg. So you go into surgery
and the surgeon shops off your leg.

403
00:27:25,080 --> 00:27:28,980
Has He done something immoral?
It doesn't seem as though he has,

404
00:27:29,430 --> 00:27:33,570
but after all, he chopped off your
leg. He harmed, you used to have a leg.

405
00:27:33,571 --> 00:27:36,150
Now you don't have one.
Well,

406
00:27:36,180 --> 00:27:40,560
what we want to say is he
didn't harm you overall.

407
00:27:40,860 --> 00:27:45,390
He harmed you in such a way that it was
the only way to leave you better off

408
00:27:45,391 --> 00:27:49,530
bottom line. And that's not a
violation of the rule against harming.

409
00:27:49,840 --> 00:27:52,470
At least that's a possible thing to say.

410
00:27:53,520 --> 00:27:56,730
And if that's the right thing to say that
maybe that's what we should say about

411
00:27:56,731 --> 00:27:59,580
the suicide case yet again.
Yeah,

412
00:27:59,581 --> 00:28:03,210
there's a deontological prohibition
against harming innocent people,

413
00:28:03,300 --> 00:28:08,280
but what it's really a prohibition
against is leaving them worse off overall.

414
00:28:08,700 --> 00:28:12,600
And when I kill myself,
I'm not leaving myself worse off overall.

415
00:28:13,620 --> 00:28:17,400
And if that's right, and even from
the deontological perspective,

416
00:28:17,520 --> 00:28:20,610
suicide may be morally legitimate.
Well,

417
00:28:20,880 --> 00:28:25,880
that's at least one possible way to
carry out the deontological stand.

418
00:28:27,000 --> 00:28:31,140
One passive way of interpreting the
remark, but look, when I kill myself,

419
00:28:31,141 --> 00:28:32,640
I'm doing it for my own benefit.

420
00:28:33,420 --> 00:28:36,150
Here's another possible way
of interpreting that thought

421
00:28:38,280 --> 00:28:42,600
when I kill myself. Given that
I'm doing it for my own benefit,

422
00:28:42,750 --> 00:28:45,000
I've obviously got my own agreement.

423
00:28:46,190 --> 00:28:48,990
I can't kill myself against my will.

424
00:28:49,020 --> 00:28:54,020
Suicide is something you do to yourself
and so I have my own consent to what I'm

425
00:28:55,621 --> 00:28:59,580
doing.
That seems pretty important.

426
00:28:59,940 --> 00:29:03,330
Notice how different it is from the
case of John. When I chopped up John,

427
00:29:03,560 --> 00:29:06,870
I imagine I don't have John's approval.

428
00:29:08,010 --> 00:29:13,010
Consent seems to be present
in the case of suicide,

429
00:29:13,200 --> 00:29:15,270
but not in the case of chopping up John.

430
00:29:15,480 --> 00:29:18,210
Maybe that's morally relevant as well.

431
00:29:19,770 --> 00:29:24,770
Now to accept that view is of course to
say we need to add yet another factor

432
00:29:25,170 --> 00:29:28,980
into our deontological theory. We
have consequences, we have harm doing,

433
00:29:29,070 --> 00:29:34,070
but we also have the factor of consent
and so we need to think about the moral

434
00:29:34,260 --> 00:29:39,260
relevance of having the consent of the
victim and once we thinking about that,

435
00:29:43,090 --> 00:29:48,090
I think most of us would be inclined to
the accept the conclusion that consent

436
00:29:48,760 --> 00:29:53,760
can make it acceptable to do to someone
what would normally be wrong in the

437
00:29:56,591 --> 00:30:00,880
absence of their consent. By
the, by, you'll notice that,

438
00:30:00,881 --> 00:30:02,990
that that seems to be one of
the things that's relevant.

439
00:30:02,991 --> 00:30:06,490
And thinking about the surgery case,
not the organ transplant case, but uh,

440
00:30:06,740 --> 00:30:10,220
performing the amputation
of the leg to save the, uh,

441
00:30:10,290 --> 00:30:12,800
save the person who would otherwise die.
Yeah.

442
00:30:13,040 --> 00:30:16,280
Surely it seems relevant that the
person that the patient has given you

443
00:30:16,370 --> 00:30:18,410
permission to operate on them.

444
00:30:21,030 --> 00:30:23,580
Here's another example that shows
you the relevance of consent.

445
00:30:23,700 --> 00:30:25,260
It would not be okay.

446
00:30:25,290 --> 00:30:30,150
It would not be morally acceptable for
me to go up and hit you in the nose

447
00:30:31,530 --> 00:30:36,480
just like it wouldn't be okay for you
to go up and hit me in the face or the

448
00:30:36,481 --> 00:30:37,314
gut.

449
00:30:38,340 --> 00:30:43,050
And yet boxing matches are,
I suppose,

450
00:30:43,051 --> 00:30:46,560
morally acceptable.
Why is that?

451
00:30:46,890 --> 00:30:49,620
Because from a deontological perspective,

452
00:30:49,621 --> 00:30:52,230
the answer is when people are boxing,

453
00:30:52,980 --> 00:30:55,140
they've agreed to it.

454
00:30:55,710 --> 00:31:00,270
I give you permission to hit me or at
least to try to hit me in exchange for

455
00:31:00,271 --> 00:31:03,930
your giving me permission to hit
you or at least to try to hit you.

456
00:31:04,590 --> 00:31:09,590
And it's the presence of that consent
that makes it permissible for you to harm

457
00:31:09,961 --> 00:31:12,930
me. Assuming that you're
a better boxer than I am,

458
00:31:13,230 --> 00:31:15,540
which I'm confident would
have to be the case.

459
00:31:17,460 --> 00:31:22,460
So consent makes it legitimate to harm
people even though in the absence of

460
00:31:24,301 --> 00:31:28,500
consent it wouldn't be legitimate.
Alright, if that's right,

461
00:31:28,860 --> 00:31:32,430
then bring that thought home to
thinking about the case of suicide.

462
00:31:32,730 --> 00:31:35,730
Suicide might be wrong because after all,

463
00:31:35,731 --> 00:31:37,980
I'm a person at first glance,

464
00:31:38,910 --> 00:31:42,360
but since I'm killing myself,

465
00:31:42,720 --> 00:31:44,610
I've given myself permission.

466
00:31:44,670 --> 00:31:47,850
I've given myself consent to harm myself,

467
00:31:48,330 --> 00:31:53,330
and if consent makes it permissible to
do what would normally be forbidden,

468
00:31:54,810 --> 00:31:58,800
Ben Consent makes it permissible
for me to kill myself.

469
00:31:59,730 --> 00:32:03,750
And so now we're led again
to the conclusion that
from a more fully developed

470
00:32:03,751 --> 00:32:08,310
deontological perspective,
we ought to say suicide is permissible.

471
00:32:09,890 --> 00:32:14,330
At least if we're prepared to throw in
this kind of factor of consent and think

472
00:32:14,331 --> 00:32:18,470
that it can just wipe out the protections
that would otherwise normally be in

473
00:32:18,471 --> 00:32:21,370
place.
Indeed,

474
00:32:21,460 --> 00:32:26,380
if we think that we're going to be led
to a rather bold and extreme conclusion

475
00:32:26,381 --> 00:32:30,040
about the morality of suicide,
the persons killed himself.

476
00:32:30,041 --> 00:32:33,130
So he's clearly consented.
And so in every case,

477
00:32:33,340 --> 00:32:38,110
what he's done is acceptable.
Now maybe that's right,

478
00:32:38,870 --> 00:32:41,900
if we're prepared to go that far
with the principle of consent,

479
00:32:42,560 --> 00:32:45,440
but maybe we shouldn't go that
far with the principle of consent.

480
00:32:46,460 --> 00:32:51,320
Suppose no, we're talking
after class and you say to me,

481
00:32:51,350 --> 00:32:55,100
Shelly,
you've got my permission to kill me

482
00:32:56,770 --> 00:32:59,300
and so I get out my gun
and I shoot you to death

483
00:33:00,800 --> 00:33:04,760
doesn't seem morally acceptable.

484
00:33:05,330 --> 00:33:07,700
Even though you gave me your permission.

485
00:33:10,470 --> 00:33:13,530
Especially, you know, think
of even weirder cases.

486
00:33:13,560 --> 00:33:18,510
Suppose that you are feeling like you
want to be killed because you're overcome

487
00:33:18,511 --> 00:33:22,590
with guilt because you
believe you killed John Smith,

488
00:33:23,650 --> 00:33:28,480
but you're crazy. You didn't
kill John Smith. John Smith,

489
00:33:28,500 --> 00:33:32,640
not even dead, but in your
insanity, you think you did do it.

490
00:33:32,670 --> 00:33:34,050
And so you say,
Shelley,

491
00:33:34,260 --> 00:33:38,610
please kill me and I know that
you're insane, but hey, you know,

492
00:33:38,611 --> 00:33:41,940
consent is consent and so I kill you.
Oh,

493
00:33:42,120 --> 00:33:43,950
that clearly isn't acceptable.

494
00:33:45,600 --> 00:33:50,340
Or suppose you know you're playing
with your three year old nephew.

495
00:33:51,090 --> 00:33:53,280
He said, Oh yeah, I don't
really like being like kill me.

496
00:33:54,000 --> 00:33:59,000
I clearly doesn't make it acceptable
to kill him or her nephew it to him.

497
00:34:02,420 --> 00:34:06,860
So if we start accepting
this consent principle,

498
00:34:06,861 --> 00:34:10,160
we're led to some pretty
implausible conclusions.

499
00:34:10,770 --> 00:34:11,990
So maybe we should throw it out.

500
00:34:12,710 --> 00:34:17,180
Maybe we should say no consent really
doesn't have the kind of power that a

501
00:34:17,180 --> 00:34:18,350
minute ago it looked like it did.

502
00:34:21,760 --> 00:34:26,530
But I'm inclined to think we shouldn't
go that far and throw away the consent

503
00:34:26,531 --> 00:34:31,531
principle altogether because if we
do throw out the consent principle,

504
00:34:32,530 --> 00:34:36,370
we're going to find ourselves on able
to say some things that I think it's

505
00:34:36,371 --> 00:34:38,530
pretty important to us to say.

506
00:34:40,600 --> 00:34:41,920
Consider the following an example.

507
00:34:43,090 --> 00:34:47,650
Suppose that we're in war and uh,

508
00:34:47,680 --> 00:34:52,680
we're in the foxhole and a a hand
grenades been thrown into the foxhole and

509
00:34:54,370 --> 00:34:56,440
unless something happens quick,

510
00:34:56,770 --> 00:35:01,770
the Hagmann it's going to blow up and it
will kill my five buddies who are near

511
00:35:03,341 --> 00:35:06,460
the hand grenade, unfortunately because
they're playing cards, whatever,

512
00:35:06,461 --> 00:35:11,230
they don't see it, but I see it.
But I don't have time to warn them.

513
00:35:11,350 --> 00:35:16,060
By the time I tell them what's going on,
they won't have time to react.

514
00:35:16,660 --> 00:35:21,130
Really. It's do nothing. Let
them get killed, but I will.

515
00:35:21,131 --> 00:35:26,131
I'll probably won't be hurt very much
or throw myself on the hand grenade.

516
00:35:26,530 --> 00:35:31,000
My body absorbs the blow,
saves my buddies, kills me.

517
00:35:31,600 --> 00:35:34,510
Imagine what happens then is I
throw myself on the hand grenade.

518
00:35:34,960 --> 00:35:37,830
I've sacrificed myself,
them,

519
00:35:38,040 --> 00:35:40,830
I've done something amazing.

520
00:35:41,370 --> 00:35:45,090
Few of us would have it
within ourselves to do this,

521
00:35:45,270 --> 00:35:46,410
but amazingly enough,

522
00:35:46,440 --> 00:35:50,850
some people do and we admire
and praise these people.

523
00:35:51,420 --> 00:35:52,920
They've committed,

524
00:35:53,850 --> 00:35:57,960
they've undertaken an incredible
act of heroic self sacrifice,

525
00:35:58,260 --> 00:36:01,440
morally commendable above
and beyond the call of duty.

526
00:36:01,441 --> 00:36:04,590
We want to say praise worthy,
bow in it.

527
00:36:05,370 --> 00:36:06,600
How could it be praiseworthy?

528
00:36:07,470 --> 00:36:12,360
The person threw himself on a hand grenade
knowing the result of this was that

529
00:36:12,361 --> 00:36:17,361
he was going to die and so he killed a
person thereby apparently violating the

530
00:36:19,171 --> 00:36:22,110
deontological right not to
have innocent people be killed.

531
00:36:24,960 --> 00:36:28,530
Don't talk about the results
are better. Yeah, of course.

532
00:36:28,590 --> 00:36:30,750
Five buddies saved results are better,

533
00:36:30,751 --> 00:36:35,070
but that doesn't seem to enough to us
in our deontological moods after all.

534
00:36:35,190 --> 00:36:37,710
Suppose that I see the hand grenade.

535
00:36:37,770 --> 00:36:42,250
And so what I do is I take Jones
and throw him on the grenade. Oh,

536
00:36:42,251 --> 00:36:45,360
that's not okay.
Even though the results are the same,

537
00:36:46,350 --> 00:36:47,820
what makes the difference?

538
00:36:47,910 --> 00:36:52,770
Why is it morally legitimate for
Jones to throw himself on the grenade?

539
00:36:53,640 --> 00:36:57,210
The only answer that I can see his,
because he agrees to it.

540
00:36:57,600 --> 00:37:01,620
He did it to himself, he
volunteered. It has his consent.

541
00:37:03,090 --> 00:37:05,430
We throw away the consent principle.

542
00:37:06,300 --> 00:37:10,230
We're forced to say what Jones
did isn't morally admirable.

543
00:37:10,260 --> 00:37:14,130
It's morally appalling.
It's morally forbidden.

544
00:37:14,520 --> 00:37:18,840
I can't believe that. So we need a
consent principle. But on the other hand,

545
00:37:18,990 --> 00:37:23,640
we don't want to go with such a strong
Concentra and supple that we say, oh,

546
00:37:23,641 --> 00:37:26,910
it's okay to kill crazy people
are killed children, you know,

547
00:37:26,911 --> 00:37:30,870
just because they say, Oh
kill me. So we need something,

548
00:37:31,230 --> 00:37:34,230
a more moderate form of
the consent principle.

549
00:37:34,740 --> 00:37:37,830
We need to say consent can do its thing,

550
00:37:38,700 --> 00:37:40,860
but only under certain conditions.

551
00:37:41,940 --> 00:37:45,660
What exactly are the relevant
conditions? Well, this is of course,

552
00:37:45,661 --> 00:37:50,340
one more topic open for debate.
We might insist that look,

553
00:37:50,640 --> 00:37:53,580
uh, the, the, the permissions
got any given freely.

554
00:37:54,090 --> 00:37:57,930
It's gotta be given knowing what
the upshots are going to be.

555
00:37:58,410 --> 00:38:02,790
It's got to be given by somebody
who's saying, who's rational,

556
00:38:02,820 --> 00:38:07,200
who's competent, who's, and that may
deal with the child case as well,

557
00:38:07,260 --> 00:38:10,470
who's not yet competent to
make this sort of decision.

558
00:38:11,970 --> 00:38:16,920
There's room for disagreement about what
exactly are the relevant conditions to

559
00:38:16,921 --> 00:38:20,400
put in to a proper version
of the consent principle.

560
00:38:21,420 --> 00:38:25,500
We might also want to throw
in some requirement that
the person had good reasons

561
00:38:25,860 --> 00:38:28,110
for his giving you permission.

562
00:38:28,560 --> 00:38:31,920
That my deal with the case where you
just come up to me after class and say,

563
00:38:31,950 --> 00:38:36,730
kill me. I mean, you're not insane
or at least might not be insane. Uh,

564
00:38:37,810 --> 00:38:38,561
your,
you know,

565
00:38:38,561 --> 00:38:43,150
what's going to happen in
some sense you've got the
Mitch the age of competence,

566
00:38:43,270 --> 00:38:45,180
but you don't have any
good reasons for it.

567
00:38:45,230 --> 00:38:48,480
Maybe that's enough to undermine
the force of consent. Well,

568
00:38:48,490 --> 00:38:52,270
suppose we've got some kind
of modified consent principle.

569
00:38:52,780 --> 00:38:56,290
What should we say about suicide then?
Well,

570
00:38:56,350 --> 00:39:01,350
it seems to me what we're led to is
once again a modest view about suicide.

571
00:39:03,670 --> 00:39:08,530
The mere fact that the person killed
themselves won't show that it was morally

572
00:39:08,531 --> 00:39:12,310
legitimate because of course, even though
they've given themselves permission,

573
00:39:12,311 --> 00:39:16,690
they may not have had, for example,
good reason or they might be insane.

574
00:39:17,770 --> 00:39:21,490
But for all that,
if we can have cases and I take it,

575
00:39:21,491 --> 00:39:26,491
we can have cases where somebody's
rationally assesses their situation,

576
00:39:27,790 --> 00:39:32,560
sees that they're better off dead thought,
thinks the case through,

577
00:39:32,630 --> 00:39:33,890
doesn't rush into it,

578
00:39:33,970 --> 00:39:38,770
makes an informed and voluntary
decision with good reason behind it.

579
00:39:39,160 --> 00:39:40,360
In a situation like that,

580
00:39:40,361 --> 00:39:45,160
it seems to me that consent
principle might well come into play,

581
00:39:45,820 --> 00:39:50,820
in which case consent will Trump or
nullify the force of the deontological

582
00:39:52,121 --> 00:39:54,310
prohibition against
harming innocent people.

583
00:39:54,970 --> 00:39:59,290
So suicide will again be
acceptable in some cases though,

584
00:39:59,350 --> 00:40:04,030
not in all. And that's the conclusion
that seems to me to be the right one.

585
00:40:04,031 --> 00:40:07,720
Whether we accept the utilitarian
position or one of these deontological

586
00:40:07,721 --> 00:40:11,410
positions.
Suicide isn't always legitimate,

587
00:40:11,650 --> 00:40:13,780
but it's sometimes legitimate

588
00:40:15,340 --> 00:40:19,330
still leaves the question,
what should we do when we see,

589
00:40:19,331 --> 00:40:24,331
when we come across somebody trying to
kill themselves and there I think there's

590
00:40:25,390 --> 00:40:26,950
good reason to ask yourself,

591
00:40:26,980 --> 00:40:31,980
are you confident that the person has
satisfied the conditions on the consent

592
00:40:32,801 --> 00:40:33,634
principle?

593
00:40:34,460 --> 00:40:34,920
Okay.

594
00:40:34,920 --> 00:40:39,900
Perhaps we should err on the side of
caution and assume that the person may be

595
00:40:39,901 --> 00:40:43,650
acting under distress, not
thinking clearly, not informed,

596
00:40:43,651 --> 00:40:46,470
not altogether competent,
not acting for good reasons,

597
00:40:50,810 --> 00:40:55,580
but to accept that is not to
accept the stronger conclusion.

598
00:40:55,880 --> 00:41:00,880
That we must never permit somebody to
kill themselves if we become convinced

599
00:41:01,580 --> 00:41:05,300
that they have thought it through.
If they do have good reason,

600
00:41:05,390 --> 00:41:09,110
that they are informed that
they are acting voluntarily.

601
00:41:09,410 --> 00:41:14,410
In some such cases it may be legitimate
for them to kill himself and for us to

602
00:41:14,571 --> 00:41:18,070
let them.
Alright,

603
00:41:19,130 --> 00:41:20,150
almost out of time.

604
00:41:20,750 --> 00:41:25,750
So let me shift gears for the very last
time and take a quick look at where

605
00:41:27,291 --> 00:41:31,340
we've been
starting this semester.

606
00:41:31,341 --> 00:41:36,341
I invited you to think hard about the
nature of death or the facts about life

607
00:41:38,541 --> 00:41:39,320
and death.

608
00:41:39,320 --> 00:41:44,320
Most of us try very hard to
not think hard about death.

609
00:41:46,610 --> 00:41:51,110
It seems to be an unpleasant topic
and we put it out of our mind.

610
00:41:51,380 --> 00:41:52,550
We don't think about it.

611
00:41:52,580 --> 00:41:57,580
Even when there's a sense in which it's
staring us in the face every single

612
00:41:58,251 --> 00:42:01,850
class of this semester,
every single day of this semester.

613
00:42:01,970 --> 00:42:06,970
You've come into this building and have
walked past a cemetery right across the

614
00:42:08,691 --> 00:42:12,230
street.
How many times did you notice it?

615
00:42:12,770 --> 00:42:17,770
How many times did you stop to think
about the complete visual reminder that we

616
00:42:21,171 --> 00:42:25,880
are on this earth for awhile
and then we're not anymore.

617
00:42:27,170 --> 00:42:29,210
Most of us just don't think about it.

618
00:42:29,211 --> 00:42:31,850
Well of course you are in
some sense the exceptions.

619
00:42:31,851 --> 00:42:36,851
You spent a semester thinking about it
and I will be largely content if you've

620
00:42:38,901 --> 00:42:43,580
taken the opportunity this semester
to take a hard look at the things you

621
00:42:43,581 --> 00:42:44,414
believe,

622
00:42:44,930 --> 00:42:49,930
whether or not you ended up agreeing with
me about the various claims that I've

623
00:42:51,621 --> 00:42:54,950
put forward is less important than that.

624
00:42:54,951 --> 00:42:59,951
You've taken the chance to take a hard
look at your beliefs and ask yourself not

625
00:43:00,291 --> 00:43:04,370
just what you're sort of hoped or
wished or kind of believed was true,

626
00:43:04,760 --> 00:43:09,140
but what you could actually defend.
Still.

627
00:43:09,170 --> 00:43:10,520
Having said that,

628
00:43:12,050 --> 00:43:16,820
it would be disingenuous of me to pretend
that I don't also hope that you've

629
00:43:16,850 --> 00:43:19,910
come around if you didn't start
out believing what I believe.

630
00:43:20,060 --> 00:43:23,170
You've come around to
believing what I believe as a,

631
00:43:23,190 --> 00:43:24,740
as I pointed out in the first day,

632
00:43:25,190 --> 00:43:30,190
most people accept a great deal of this
package of beliefs about the nature of

633
00:43:31,701 --> 00:43:35,030
life and death,
that they believe we have a soul,

634
00:43:35,031 --> 00:43:40,031
that there's something more to us than
our bodies and they believe that because

635
00:43:41,240 --> 00:43:46,240
they think given the existence of a
soul will have the possibility of living

636
00:43:46,461 --> 00:43:51,461
forever in mortality is a possibility
and we all hope for and crave the

637
00:43:53,631 --> 00:43:58,631
possibility that we will live forever
because death is and must be horrible.

638
00:44:02,960 --> 00:44:05,330
It's so horrible that we
try not to think about it.

639
00:44:05,390 --> 00:44:09,740
It's so horrible that when we do
think about it were filled with dread,

640
00:44:09,830 --> 00:44:11,330
terror and fear,

641
00:44:11,331 --> 00:44:16,331
and it's just obvious that that's the
only sensible reaction to the facts about

642
00:44:17,541 --> 00:44:18,380
life and death.

643
00:44:19,820 --> 00:44:24,820
Life is so incredible that
under no circumstances could
it ever make sense to be

644
00:44:25,731 --> 00:44:30,350
glad that it had come to an end
in mortality would be desirable.

645
00:44:30,410 --> 00:44:33,450
Suicide could never be
a reasonable response.

646
00:44:35,670 --> 00:44:36,990
Over the course of this semester,

647
00:44:36,991 --> 00:44:41,991
I've argued that that package of
beliefs common as it may be is mistaken

648
00:44:43,350 --> 00:44:46,200
virtually from start to finish.

649
00:44:46,920 --> 00:44:51,330
There is no soul.
We are just machines.

650
00:44:51,420 --> 00:44:55,740
We're not just any old machine.
We are amazing machines.

651
00:44:55,741 --> 00:45:00,720
We are machines capable of
loving, capable, of dreaming,

652
00:45:00,810 --> 00:45:05,580
capable of being creative, capable of
making plans and sharing them with others.

653
00:45:05,670 --> 00:45:10,170
We are people,
but we're just machines anyway,

654
00:45:10,950 --> 00:45:15,450
and when the machine breaks,
that's the end.

655
00:45:15,900 --> 00:45:18,990
Death is not some big mystery,

656
00:45:19,110 --> 00:45:22,860
which we can't get our heads around.

657
00:45:23,550 --> 00:45:24,930
Death is in some sense,

658
00:45:25,290 --> 00:45:30,290
no more mysterious than the fact that
your lamp can break or your computer can

659
00:45:30,871 --> 00:45:34,950
break or any other machine can,
will eventually fail.

660
00:45:36,750 --> 00:45:37,210
Okay.

661
00:45:37,210 --> 00:45:42,210
I never meant to claim that it's not
regrettable that we die the way we do.

662
00:45:45,310 --> 00:45:47,860
As I argued when talking
about in mortality,

663
00:45:47,861 --> 00:45:52,861
better still would be if only we had
the prospect of living as long as life

664
00:45:54,791 --> 00:45:59,791
still had something left to offer
us as long as life would be good.

665
00:45:59,921 --> 00:46:03,040
Overall death is bad and
I think for most of us,

666
00:46:03,070 --> 00:46:07,660
death comes too soon,
but having said that,

667
00:46:07,690 --> 00:46:11,200
it doesn't follow that in
mortality would be a good thing.

668
00:46:11,350 --> 00:46:15,640
On the contrary in mortality
would be a bad thing.

669
00:46:16,770 --> 00:46:17,200
Yeah.

670
00:46:17,200 --> 00:46:22,200
The reaction that makes sense in thinking
about the facts of death are not just

671
00:46:23,541 --> 00:46:28,240
find it as some great mystery, too
dreadful to think about too overwhelming,

672
00:46:28,630 --> 00:46:32,980
but rather fear far from being the
rationally appropriate response,

673
00:46:32,981 --> 00:46:35,050
I think is an inappropriate response.

674
00:46:35,500 --> 00:46:38,350
Although it can be sad
that we die too soon.

675
00:46:38,770 --> 00:46:43,770
That perhaps should be balanced by the
fact of the recognition of just how

676
00:46:44,651 --> 00:46:49,651
incredibly lucky we all are
to have been alive at all.

677
00:46:50,860 --> 00:46:52,060
Yet.
At the same time,

678
00:46:52,061 --> 00:46:57,061
recognizing that sense of luck and
being fortunate doesn't mean that we're

679
00:46:58,151 --> 00:47:02,920
always lucky to be remaining alive.
For some of us,

680
00:47:02,921 --> 00:47:06,550
the time will come in which
that's no longer true,

681
00:47:06,790 --> 00:47:11,040
and when that happens,
life is not something to be held on,

682
00:47:11,041 --> 00:47:14,890
to come what may under
any and all circumstances,

683
00:47:15,130 --> 00:47:19,270
the time could come for some of
us in which it's time to let go.

684
00:47:21,380 --> 00:47:26,380
What I then invited you to do over the
course of the semester is not only to

685
00:47:27,501 --> 00:47:31,450
think for yourself about
the facts of life and death,

686
00:47:31,900 --> 00:47:36,900
but I invite you all to come to face
death without fear and without illusion.

687
00:47:40,480 --> 00:47:41,320
Thanks very much.

