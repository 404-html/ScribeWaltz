1
00:00:00,950 --> 00:00:05,930
Welcome back to human emotion.
So when we think about emotions,

2
00:00:06,020 --> 00:00:07,850
we know that they're inherently social,

3
00:00:08,120 --> 00:00:10,610
they're a vital part of
our social relationships,

4
00:00:10,850 --> 00:00:12,920
and they help bind us to other people.

5
00:00:13,310 --> 00:00:15,560
So to what extent when
we think about emotions,

6
00:00:15,620 --> 00:00:20,300
are we really thinking about emotions in
a social world and what is that social

7
00:00:20,301 --> 00:00:21,020
world like?

8
00:00:21,020 --> 00:00:25,850
And to what extent do our emotions play
a pivotal role in shaping the social

9
00:00:25,851 --> 00:00:26,900
world that we live in?

10
00:00:27,260 --> 00:00:31,730
So these are some of the questions we'll
be returning to today and our roadmap.

11
00:00:31,820 --> 00:00:35,330
We'll be looking at the question
of emotions in a social world.

12
00:00:35,331 --> 00:00:39,350
This is our second part of this broad
or a question of looking at emotions and

13
00:00:39,351 --> 00:00:40,400
social contexts.

14
00:00:40,760 --> 00:00:44,780
And we'll be having a very special guest
lecture today with a renowned expert on

15
00:00:44,781 --> 00:00:45,920
this very topic.

16
00:00:46,940 --> 00:00:51,710
What we'll be doing today is first
starting with a special guest lecture on

17
00:00:51,711 --> 00:00:56,390
social emotions. I'm very pleased
today to have Dr. David Desteno,

18
00:00:56,391 --> 00:01:00,500
a professor of psychology at northeastern
university who'll be speaking with us

19
00:01:00,501 --> 00:01:05,210
today about this very puzzling but
incredibly exciting question of social

20
00:01:05,211 --> 00:01:06,050
emotions.

21
00:01:06,410 --> 00:01:10,640
So I'd like to welcome Dr. David Desteno
today as our special guest lecture.

22
00:01:12,050 --> 00:01:15,650
So when I talk about social emotions,
I usually like to start talking a little,

23
00:01:15,740 --> 00:01:18,500
a little,
a bit about the idea of character.

24
00:01:19,100 --> 00:01:23,270
Character after all refers to
our morality, our reputation,

25
00:01:23,271 --> 00:01:25,460
basically how we navigate
the social world.

26
00:01:25,910 --> 00:01:30,710
And what I would like to suggest is that
our idea of character and primarily how

27
00:01:30,711 --> 00:01:33,890
emotions fit into it needs
a little bit of updating.

28
00:01:34,580 --> 00:01:36,410
Now the term character,

29
00:01:36,411 --> 00:01:41,240
the origins of the term comes from a
word that referred to the marks that were

30
00:01:41,241 --> 00:01:45,590
indelibly stamped in ancient Greek coins.
Once they were stamped, they were fixed.

31
00:01:45,591 --> 00:01:48,980
Once you had your
character, that was it. Um,

32
00:01:49,640 --> 00:01:54,230
the motif that we usually are familiar
with is one of an angel on one shoulder

33
00:01:54,320 --> 00:01:56,870
and a devil on the other.
And as you grow up,

34
00:01:56,930 --> 00:02:00,980
if you learn to listen to the good voice,
well everything will be wonderful.

35
00:02:01,220 --> 00:02:02,180
Your life will be great,

36
00:02:02,181 --> 00:02:05,390
you will be a virtuous person
and all be right with the world.

37
00:02:06,170 --> 00:02:08,000
There's just one problem with that though.

38
00:02:08,510 --> 00:02:12,710
And that's the data from everything
that we know from the past 15 years of

39
00:02:12,711 --> 00:02:15,440
psychological science.
It's that human morality,

40
00:02:15,441 --> 00:02:18,290
human social behavior just
doesn't work that way.

41
00:02:18,770 --> 00:02:23,770
And so what I want to do is to
suggest to you that character,

42
00:02:24,080 --> 00:02:28,160
morality, social behavior itself
is a much more dynamic process.

43
00:02:28,460 --> 00:02:32,360
And a better metaphor really comes
from thinking of it as a scale,

44
00:02:32,750 --> 00:02:34,550
a scale that's always in motion.

45
00:02:34,880 --> 00:02:39,230
And where that scale is at any one moment
is going to determine what your social

46
00:02:39,231 --> 00:02:41,240
behavior is going to be.

47
00:02:42,770 --> 00:02:47,770
Now the correction I want to make is it
doesn't really make sense to think about

48
00:02:48,380 --> 00:02:52,070
social behavior as the mind
having good and evil mechanisms.

49
00:02:52,071 --> 00:02:53,870
Unless we're talking
about psychopathology,

50
00:02:54,170 --> 00:02:58,700
it makes little sense to assume that the
mind would trave truly evil mechanisms.

51
00:02:58,701 --> 00:03:01,570
What would they serve?
Rather,

52
00:03:01,571 --> 00:03:05,830
what I want to argue is
that the size of this scale,

53
00:03:05,860 --> 00:03:09,190
what determines our social behavior
are two classes of mechanisms that are

54
00:03:09,191 --> 00:03:11,590
captured best buy a subs fable of the,

55
00:03:11,591 --> 00:03:14,590
of the grant and the grant of
the ant and the grasshopper. Um,

56
00:03:15,340 --> 00:03:18,720
and for those of you who don't remember
it, let me tell you quickly, um,

57
00:03:19,060 --> 00:03:23,350
the story goes that the ads worked hard
all summer and toilets so that for the

58
00:03:23,351 --> 00:03:25,780
long term it would have food
while the grasshopper wiled away.

59
00:03:25,781 --> 00:03:28,360
It's days having a lot of fun and
enjoying itself in the moment.

60
00:03:28,810 --> 00:03:32,200
And what I want to suggest to you is that
a lot of human social life is really a

61
00:03:32,201 --> 00:03:36,100
battle between these two types
of mechanisms in our mind.

62
00:03:36,820 --> 00:03:40,990
Now, one of the classic examples of
this comes from work by Walter Michelle,

63
00:03:40,991 --> 00:03:43,450
what's in what's colloquially
known as the marshmallow studies.

64
00:03:43,451 --> 00:03:46,270
And in many of you probably know
this, but for those who don't, let me,

65
00:03:46,300 --> 00:03:50,210
let me tell you, I'm, Michelle
did a, did a wonderful, uh,

66
00:03:50,230 --> 00:03:53,530
experiment where he would bring young
children in the lab and we'd put a

67
00:03:53,531 --> 00:03:58,180
marshmallow down in front of them and
he would say, I have to go do something.

68
00:03:59,080 --> 00:04:03,130
If you don't eat that when I come
back, you can have two, right?

69
00:04:03,220 --> 00:04:05,860
Classic Longterm, short
term, short term trade off.

70
00:04:06,310 --> 00:04:08,980
And what you would see if you ever
get a chance to watch these tapes I,

71
00:04:08,981 --> 00:04:12,190
these videos, I encourage you to do it
because it's great. You'll see the kids,

72
00:04:12,520 --> 00:04:15,610
they'll peek through their
hands at the marshmallow,

73
00:04:15,611 --> 00:04:20,530
they'll lick around the table and you can
see them exerting a lot of effort. Um,

74
00:04:20,560 --> 00:04:23,190
some of them to not eat them. And some
of them of course, gobbled them up.

75
00:04:23,191 --> 00:04:28,191
And what Michelle found is that the
individuals who were able to wait to

76
00:04:30,011 --> 00:04:34,540
forestall immediate cravings to engage
in self regulation longterm had wonderful

77
00:04:34,541 --> 00:04:37,750
outcomes. They had better social outcomes,
they have better academic outcomes,

78
00:04:37,751 --> 00:04:41,470
they are viewed as more loyal, more
intelligent, more trustworthy, et cetera.

79
00:04:41,980 --> 00:04:46,330
And this idea has given rise to the
fact that we have two mines, right?

80
00:04:46,360 --> 00:04:50,060
We have this older emotional
mind that cares about, uh,

81
00:04:50,110 --> 00:04:55,060
donuts and sweets and sleep.
And we have this newer rational mind,

82
00:04:55,061 --> 00:04:58,270
right? That cares about, uh, what's
good and what's right in the world.

83
00:04:58,630 --> 00:05:03,630
And the way we come to think of that
is that the way to be a good person

84
00:05:03,761 --> 00:05:08,440
socially is that if we can just tamp
down those crave and emotional responses,

85
00:05:08,830 --> 00:05:12,940
that's where virtue comes from.
That's where being a good,

86
00:05:12,941 --> 00:05:17,570
reliable partner in honest friend,
someone who perseveres for the longterm,

87
00:05:17,590 --> 00:05:20,440
that's where it comes from.
And sure it can come from that.

88
00:05:20,441 --> 00:05:25,441
But what I want to suggest to you is that
it's wrong to assume that all emotions

89
00:05:25,600 --> 00:05:30,490
are craving and that all emotions are
there to focus on immediate results and

90
00:05:30,491 --> 00:05:32,200
immediate desires in the short term.

91
00:05:32,410 --> 00:05:36,250
I want to suggest to you that I'm both
the rational level and the emotional

92
00:05:36,251 --> 00:05:36,490
level.

93
00:05:36,490 --> 00:05:41,490
This battle between short term and Mecca
and longterm is being fought and social

94
00:05:41,591 --> 00:05:45,580
emotions are a special class of emotions
that I think exist to push people

95
00:05:45,581 --> 00:05:50,260
toward favoring longterm outcomes.
So the next few minutes,

96
00:05:50,261 --> 00:05:54,340
what I want to do is give
you some examples of some
central findings of of this

97
00:05:54,341 --> 00:05:58,460
and how we go about actually studying it.
So let me start with a basic question,

98
00:05:58,461 --> 00:06:02,000
which is why do we cooperate?
Right?

99
00:06:02,450 --> 00:06:05,810
Cooperation is fundamental
to lots of human endeavor.

100
00:06:06,110 --> 00:06:09,140
But the problem with cooperation,
if there was always a risk, right?

101
00:06:09,141 --> 00:06:13,550
If one person gets aid and then
doesn't help the other person. So if,

102
00:06:13,850 --> 00:06:18,320
um, if, uh, June, uh, gives
me money that I can borrow,

103
00:06:18,680 --> 00:06:21,440
if I don't pay June back,
I've done really well for myself,

104
00:06:21,560 --> 00:06:25,010
but the problem is longterm
June's probably not going
to want to cooperate with

105
00:06:25,011 --> 00:06:25,844
me anymore.

106
00:06:26,120 --> 00:06:30,260
And so what makes me remember
that I have to do my end,

107
00:06:30,290 --> 00:06:31,880
that I have to take costs.

108
00:06:31,890 --> 00:06:36,650
It's also support the relationship
because true social bonds only work when

109
00:06:36,651 --> 00:06:40,890
individuals can suppress short term
desires and engage in behaviors that long

110
00:06:40,891 --> 00:06:43,640
term foster better social bonds.

111
00:06:44,030 --> 00:06:47,960
And so what I want to suggest to you is
that one way we do this is a function of

112
00:06:47,961 --> 00:06:49,100
the emotion gratitude.

113
00:06:49,460 --> 00:06:53,060
So George has Emelda the sociologist is
known for saying gratitude is the moral

114
00:06:53,061 --> 00:06:57,860
memory of mankind. It's what doesn't.
Let me forget that I owe you something,

115
00:06:57,861 --> 00:07:02,450
even though in the moment I might want
to get away with not paying you back. Um,

116
00:07:03,290 --> 00:07:07,060
Robert trevors who came up with the
idea for reciprocal altruism, um,

117
00:07:07,160 --> 00:07:10,790
where we help other people because
longterm they're going to help us back.

118
00:07:11,090 --> 00:07:12,230
That view's pretty well known.

119
00:07:12,231 --> 00:07:16,760
What's less well known is that Trevor's
himself actually postulated that there

120
00:07:16,761 --> 00:07:21,140
would be discrete emotional states that
would mediate these social behaviors.

121
00:07:22,070 --> 00:07:26,060
And so what I want to give you an example
of is that gratitude can function as a

122
00:07:26,061 --> 00:07:29,930
distinct emotion that mediates
social or economic reciprocity.

123
00:07:32,210 --> 00:07:36,920
And I want to show you how it functions
to push people to favor longterm gains

124
00:07:37,250 --> 00:07:40,310
over short ones and thereby
build social capital.

125
00:07:41,060 --> 00:07:44,420
The problem of course is how do you
study these things? If I ask somebody,

126
00:07:44,421 --> 00:07:48,820
are you going to be an honest person, a
fair person, are you going to cooperate?

127
00:07:48,850 --> 00:07:53,300
Most people will say yes sometimes because
they know they won't in they're lying.

128
00:07:53,301 --> 00:07:57,650
But more often because people are very
poor predictors of knowing what they're

129
00:07:57,651 --> 00:08:01,430
going to do when hypothetical situation,
w two hypothetical situations.

130
00:08:01,431 --> 00:08:04,640
So in real situations arise
and when push comes to shove,

131
00:08:04,641 --> 00:08:07,520
that's what we can really see,
what role emotions are going to have.

132
00:08:07,910 --> 00:08:12,230
And so in my experiment,
what we typically do our stage events.

133
00:08:12,260 --> 00:08:15,690
So uh, in real time we can
evoke these emotional states.

134
00:08:15,691 --> 00:08:20,470
So let me give you a quick example of
how we do that. Here we have always a,

135
00:08:20,471 --> 00:08:24,080
a real subject and a person who is a
confederate that is a person who works for

136
00:08:24,081 --> 00:08:26,960
us, but who the other subjects
believe is also a subject.

137
00:08:27,530 --> 00:08:31,490
We have them come in and we have
them work on a task together.

138
00:08:31,491 --> 00:08:34,880
The only purpose of this is just so that
they can get some experience working

139
00:08:34,881 --> 00:08:37,190
together and when we ask them
later on the real subjects,

140
00:08:37,191 --> 00:08:39,740
what did you think of the other person?
There's a context for doing it.

141
00:08:40,820 --> 00:08:43,700
They didn't engage in along an onerous
task and the only thing you need to know

142
00:08:43,701 --> 00:08:47,030
about that is they really don't
like doing it. And that's by design.

143
00:08:48,050 --> 00:08:49,550
At the end of this task,

144
00:08:50,120 --> 00:08:55,120
the computer is rigged so that it dies
and the poor subject is sitting knowing,

145
00:08:56,190 --> 00:08:56,940
oh my goodness,

146
00:08:56,940 --> 00:09:00,090
all my work has now gone away and
I'm going to have to do it again.

147
00:09:00,900 --> 00:09:05,610
At that moment, the confederate comes
over and says, oh, what happened? Oh,

148
00:09:05,670 --> 00:09:07,080
let me see if I can help you with that.

149
00:09:07,590 --> 00:09:11,640
And through a set of
prearranged activities, she'll
go about fixing the computer,

150
00:09:11,641 --> 00:09:13,620
at which point the computer comes back on.

151
00:09:14,120 --> 00:09:16,560
And the subjects are always very grateful
for this because they don't want to

152
00:09:16,830 --> 00:09:19,380
Redo that very tedious task.

153
00:09:19,740 --> 00:09:22,560
And then we have them play
this economic game together.

154
00:09:23,160 --> 00:09:24,360
And that's the economic game.

155
00:09:24,361 --> 00:09:28,170
The point of that is it
really compares self-interest,

156
00:09:28,230 --> 00:09:31,170
short term self interest versus
longterm communal interests.

157
00:09:31,171 --> 00:09:36,171
And our sense is that would gratitude
should do is enhance your decisions toward

158
00:09:37,561 --> 00:09:39,630
longterm gains towards communal outcomes.

159
00:09:40,060 --> 00:09:43,560
And so the way the game works is
each person is given four tokens.

160
00:09:43,830 --> 00:09:48,830
Each of my tokens is worth a dollar to
me and worth $2 if I give it to the other

161
00:09:49,591 --> 00:09:50,424
person.

162
00:09:50,730 --> 00:09:55,730
And so the best outcomes that you
can have realistically in terms of,

163
00:09:56,250 --> 00:09:58,320
um,
uh,

164
00:09:58,350 --> 00:10:02,340
selfish gain is not to give
anybody anything and to
get them to give you all of

165
00:10:02,341 --> 00:10:05,310
theirs. And then you'll have
$12 and they'll have nothing.

166
00:10:07,230 --> 00:10:12,000
The best communal outcome is for each
person to completely exchange everything

167
00:10:12,300 --> 00:10:15,330
and then they'll have $8 each.
And so what you can see as long term,

168
00:10:15,331 --> 00:10:19,170
it's better for me to not cheat June and
to exchange everything I have with her,

169
00:10:19,171 --> 00:10:22,800
but short term I can gain on my
own if I don't give anything.

170
00:10:23,550 --> 00:10:25,140
And so what happens?
Well,

171
00:10:25,141 --> 00:10:29,420
in these experiments what we
have is a situation where um,

172
00:10:29,880 --> 00:10:30,750
individuals,

173
00:10:30,840 --> 00:10:35,840
their computer either breaks or it
doesn't and they feel gratitude where they

174
00:10:36,091 --> 00:10:40,860
don't. And what you see here is
that the amount of tokens they give,

175
00:10:40,861 --> 00:10:43,470
the more tokens you give,
the more communal that you're being,

176
00:10:43,471 --> 00:10:46,920
the more fair you're being.
When you feel gratitude,

177
00:10:47,370 --> 00:10:50,610
you actually give significantly
more tokens. Then when you don't,

178
00:10:51,030 --> 00:10:54,000
now you might say, well
that's interesting, but
this person just helped me.

179
00:10:54,000 --> 00:10:58,170
Maybe I'm doing it because I feel like I
owe this person and that would be fair.

180
00:10:58,740 --> 00:11:02,700
So what we do is we also
run the experiment where
they now play the game with

181
00:11:02,701 --> 00:11:06,630
someone they've never met before. So
yes, someone has just helped them,

182
00:11:06,690 --> 00:11:09,270
but now they're going to play the game
with someone they've never met and to

183
00:11:09,271 --> 00:11:11,910
whom they don't know anything.
And here again,

184
00:11:12,480 --> 00:11:17,480
simply feeling grateful encourages
people to be more prosocial,

185
00:11:18,750 --> 00:11:23,370
to suppress their own desires for self
interest and self profit in favor of

186
00:11:23,371 --> 00:11:24,600
larger communal gain.

187
00:11:25,110 --> 00:11:28,860
And if you look at actually how this
plays out in terms of people's emotions,

188
00:11:29,220 --> 00:11:31,410
it's a pretty
straightforward relationship.

189
00:11:31,800 --> 00:11:34,260
The more gratitude you feel,

190
00:11:35,190 --> 00:11:38,790
the more you actually give
in a fairly linear fashion.

191
00:11:40,560 --> 00:11:43,500
Now we've done this again with other
things that aren't monetarily based.

192
00:11:43,501 --> 00:11:48,030
We've done it with requests for
help. And so in this case, uh,

193
00:11:48,210 --> 00:11:52,680
we have a situation where individuals
feel gratitude and they are given the

194
00:11:52,681 --> 00:11:56,530
option to help a complete stranger.

195
00:11:56,770 --> 00:11:59,830
So they leave our experiment after they're
feeling grateful and they're walking

196
00:11:59,831 --> 00:12:02,140
down the hall and someone
asks them for help.

197
00:12:02,470 --> 00:12:04,600
And you can see here this
is the time spent helping.

198
00:12:04,960 --> 00:12:08,260
They will help an individual significantly
more if they're feeling grateful.

199
00:12:08,890 --> 00:12:13,420
But the reason we know it's gratitude
as if before they leave the lab,

200
00:12:13,450 --> 00:12:16,630
we say to them, hey, the
other person in there,

201
00:12:16,631 --> 00:12:19,510
they helped you in your computer
broke, right? And they'll say, yes,

202
00:12:19,511 --> 00:12:21,730
that person did help me.
A,

203
00:12:21,731 --> 00:12:25,360
what we're doing is we're binding the
feeling of gratitude toward that person.

204
00:12:25,361 --> 00:12:27,220
They can't miss a tribute to someone else.

205
00:12:27,580 --> 00:12:29,950
And then when someone
else asks them for help,

206
00:12:30,220 --> 00:12:33,700
they don't help any more than they
would if in fact they weren't feeling

207
00:12:33,701 --> 00:12:37,450
grateful at all. And that's how you know
it's not a pay it forward. You know,

208
00:12:37,451 --> 00:12:39,460
it's not that I am,

209
00:12:39,940 --> 00:12:42,460
I'm helping this person
because someone just helped me.

210
00:12:42,460 --> 00:12:46,030
When we remind you that someone helped
you and remind you who you should feel

211
00:12:46,031 --> 00:12:48,730
grateful toward,
you don't help anybody else.

212
00:12:48,760 --> 00:12:50,410
But when you're just feeling grateful,

213
00:12:50,620 --> 00:12:55,030
you could miss a tribute to that to a
new person and thereby help them as well.

214
00:12:55,150 --> 00:12:59,530
And so we know it's an emotion based
effect. Let me give you one more example.

215
00:12:59,560 --> 00:13:00,860
This is compassion.
Now,

216
00:13:00,910 --> 00:13:05,910
compassion is a really interesting thing
to study because the world is full of

217
00:13:07,001 --> 00:13:11,830
more people who need help than
we can possibly help, right?

218
00:13:11,831 --> 00:13:13,780
If we tried to feel
compassion for everyone,

219
00:13:14,110 --> 00:13:15,760
it would be impossible and overwhelming.

220
00:13:16,480 --> 00:13:19,390
And so the question is out of all the
people in the world who need help,

221
00:13:19,660 --> 00:13:24,040
how do we decide who it is
most beneficial to help?

222
00:13:24,070 --> 00:13:25,570
Who is worthy of compassion?

223
00:13:26,270 --> 00:13:30,290
And what I want to suggest is the way
we do this again is based on this trade

224
00:13:30,291 --> 00:13:32,470
off between long term and
short term mechanisms.

225
00:13:32,470 --> 00:13:35,140
And I'm going to show you how our
emotions help us solve this problem.

226
00:13:37,030 --> 00:13:41,230
Short term, it makes no sense for me to
help anybody. It's going to cost me money,

227
00:13:41,680 --> 00:13:45,310
time, effort, some type of
resources to help someone.

228
00:13:46,120 --> 00:13:51,120
But long term if I help them then when
I need help they're more likely to help

229
00:13:52,181 --> 00:13:53,014
me.

230
00:13:53,170 --> 00:13:57,640
And so the question is which
way is the scale gonna go?

231
00:13:58,240 --> 00:14:02,200
Who among the people who need help are
going to be the ones that I choose to

232
00:14:02,201 --> 00:14:05,740
help and what are the longterm
is going to push me to do?

233
00:14:06,280 --> 00:14:09,910
How are they going to determine what
compassion I feel and what I want to

234
00:14:09,911 --> 00:14:10,421
suggest.

235
00:14:10,421 --> 00:14:14,260
He was one way that we go about deciding
whether or not we should help someone

236
00:14:14,261 --> 00:14:18,790
or whether or not we should feel
compassionate toward them is by a simple

237
00:14:18,880 --> 00:14:22,900
analysis.
And that is do we see ourselves in them?

238
00:14:24,160 --> 00:14:28,270
And so I want to suggest that one way
compassion works is it's based on a simple

239
00:14:28,271 --> 00:14:30,820
metric and that metric is similarity.

240
00:14:31,300 --> 00:14:33,970
The idea is the more
similar someone is to me,

241
00:14:34,480 --> 00:14:37,960
the more compassion I will feel for them
even though they're suffering the same

242
00:14:37,961 --> 00:14:42,190
objective. Uh, tragedy
as is another individual.

243
00:14:42,940 --> 00:14:47,650
And what this suggests is that distress
is really in the eye of the beholder.

244
00:14:48,040 --> 00:14:52,130
How much compassion I feel for someone
isn't a function of befallen them.

245
00:14:52,670 --> 00:14:57,110
It's an a, it's a function
of their links to me. Now,

246
00:14:57,111 --> 00:15:02,000
if I said to you on the battlefield in
American soldier comes upon a wounded

247
00:15:02,001 --> 00:15:03,120
member of the Taliban and,

248
00:15:03,121 --> 00:15:06,440
and wounded American soldiers and they
feel more compassion for the American

249
00:15:06,441 --> 00:15:08,840
soldier,
that might not be surprising to you.

250
00:15:09,230 --> 00:15:12,290
Those groups were in a
conflict for a long time,

251
00:15:12,560 --> 00:15:16,940
but what I want to suggest to you is that
this bias is so deeply embedded in the

252
00:15:16,941 --> 00:15:21,010
mind that we can see it with
the subtlest of cues and said,

253
00:15:21,011 --> 00:15:25,730
the cue I really want to look at stripping
it down to bare bones is simple motor

254
00:15:25,731 --> 00:15:28,490
synchrony, right? Moving in time together.

255
00:15:28,970 --> 00:15:33,320
If you move your body in time
together, it's a marker of that. Now,

256
00:15:34,020 --> 00:15:38,750
the, for this moment to individuals
or one they're purposes are joined,

257
00:15:38,870 --> 00:15:43,040
their goals are joined and those are
the individuals who longterm are most

258
00:15:43,041 --> 00:15:46,700
likely going to help me.
So how do we do this?

259
00:15:46,701 --> 00:15:50,810
We bring individuals into lab and we sit
them down at a table across from each

260
00:15:50,811 --> 00:15:52,010
other and they put on earphones.

261
00:15:52,070 --> 00:15:55,070
They think they're in a music perception
study and their goal is simple.

262
00:15:55,790 --> 00:15:58,520
Tap your hands to the tones you hear.

263
00:15:59,750 --> 00:16:04,750
The only difference is sometimes
they tap their hands in unison.

264
00:16:05,420 --> 00:16:07,160
Sometimes the tones are random,

265
00:16:07,161 --> 00:16:11,630
so they're tapping and it completely
asynchronous way. They don't talk,

266
00:16:11,810 --> 00:16:13,100
they don't do anything else.

267
00:16:14,030 --> 00:16:17,990
What happens next is you see the
partner who you were tapping with,

268
00:16:18,980 --> 00:16:22,430
engage in another experiment
that you're observing where uh,

269
00:16:22,520 --> 00:16:27,520
he or she is cheated by another subject
and get stuck doing this onerous,

270
00:16:30,470 --> 00:16:31,550
tedious task.

271
00:16:32,660 --> 00:16:37,560
And then simply what we
do is we ask people, um,

272
00:16:38,510 --> 00:16:40,970
if they want to help this person or not.

273
00:16:41,380 --> 00:16:45,140
Now we don't ask them as experimenters
because there might be pressure there at

274
00:16:45,141 --> 00:16:47,360
the end of the experiment that
computer simply says to them,

275
00:16:48,260 --> 00:16:49,520
there's a lot of work to be done.

276
00:16:49,550 --> 00:16:51,770
If for some reason that you'd
like to help somebody else,

277
00:16:52,160 --> 00:16:56,960
please find the experimenters and
let them know. And what we found,

278
00:16:56,961 --> 00:17:00,350
I have to admit to you is
rather astounding to me.

279
00:17:01,760 --> 00:17:04,580
The simple act of tapping
your hands in time,

280
00:17:05,030 --> 00:17:08,210
make people feel more
similar to each other.

281
00:17:08,211 --> 00:17:10,580
Now they couldn't tell us
why they were more similar.

282
00:17:10,730 --> 00:17:13,250
They would create stories about
why they were more similar.

283
00:17:13,430 --> 00:17:16,370
They didn't even talk to the person,
but yet they felt more similar.

284
00:17:16,640 --> 00:17:20,900
And what that similarity did is it gave
the longterm mechanisms of the mind

285
00:17:20,901 --> 00:17:24,080
greater power to increase the
compassionate Rio in the field.

286
00:17:24,080 --> 00:17:26,480
To make us help these individuals.
Because out of the two,

287
00:17:26,481 --> 00:17:29,660
those individuals were more likely the
ones who would benefit us in the longterm.

288
00:17:30,230 --> 00:17:34,400
And so here you can see the amount of
compassion they felt was also influenced

289
00:17:35,090 --> 00:17:38,930
by whether they are not, they tap their
hands and time. Remember in each case,

290
00:17:39,200 --> 00:17:44,200
the person is victimized in the same way
and is cheated in exactly the same way,

291
00:17:44,600 --> 00:17:48,890
but how much compassion we feel for them
is a function of how similar we feel to

292
00:17:48,900 --> 00:17:52,710
them. Moreover, if you look
at the decisions to help,

293
00:17:52,800 --> 00:17:55,050
there's a really large difference,
right?

294
00:17:55,380 --> 00:18:00,120
17 out of 35 people decided to help an
individual with whom they tap their hands

295
00:18:00,121 --> 00:18:00,954
in time.

296
00:18:01,560 --> 00:18:06,560
Only six out of 34 decided to do that in
cases where there was less similarity.

297
00:18:07,560 --> 00:18:11,220
And if you look at the time they spent
helping, it's even more dramatic, right?

298
00:18:11,820 --> 00:18:16,530
If I felt similar to you,
I helped you for much,

299
00:18:16,531 --> 00:18:21,450
much longer than I did. If I felt
that you and I, we're not similar.

300
00:18:21,930 --> 00:18:25,370
Now, the interesting thing about
this is if you look at how the,

301
00:18:25,420 --> 00:18:30,090
the variables are linked, um, yes,

302
00:18:30,091 --> 00:18:32,610
if I tap,
if you tapped your hands in time with me,

303
00:18:33,960 --> 00:18:37,170
I felt more similar to
you and I liked you more.

304
00:18:37,380 --> 00:18:41,640
But how much I liked you didn't predict
how much compassion I felt for you.

305
00:18:42,100 --> 00:18:46,170
It didn't predict how much I helped you.
Similarity did.

306
00:18:46,980 --> 00:18:50,400
And there's where you can see the trade
off between short term and long term.

307
00:18:50,401 --> 00:18:54,420
And again, the more compassion I felt,
the more effort I engaged in to help you,

308
00:18:54,421 --> 00:18:57,750
which longterm with
cement those social bonds.

309
00:18:58,470 --> 00:19:03,210
And so what these data and findings
like them suggest is that yes,

310
00:19:03,240 --> 00:19:05,760
human character,
human morality is flexible,

311
00:19:06,450 --> 00:19:10,690
but it's not only the case that the
way we solve these problems of longterm

312
00:19:10,740 --> 00:19:13,950
versus short term trade offs
is to suppress our emotions.

313
00:19:14,490 --> 00:19:17,880
Rather as if we cultivate these emotions,
these social emotions,

314
00:19:17,881 --> 00:19:19,950
things like gratitude,
things like compassion.

315
00:19:20,400 --> 00:19:24,330
They will push us to act a
virtuous to cement social bonds,

316
00:19:24,331 --> 00:19:28,850
to act in ways that raise all boats
as opposed to giving one person to

317
00:19:28,851 --> 00:19:30,000
asymmetric profit.

318
00:19:30,390 --> 00:19:34,710
And they will do so in a way that doesn't
require willpower because willpower

319
00:19:35,100 --> 00:19:35,940
can fail.

320
00:19:36,480 --> 00:19:41,480
And what it suggests is that the way
to navigate our social landscape most

321
00:19:41,611 --> 00:19:43,890
successfully is really,

322
00:19:43,891 --> 00:19:46,650
at least in terms of a
psychological mechanisms,

323
00:19:46,651 --> 00:19:50,130
is to find the right balance
between short term and longterm.

324
00:19:50,580 --> 00:19:55,020
It was Aristotle who said that virtue
is to be found between the two vices,

325
00:19:55,170 --> 00:19:57,960
the vice, that's selfishness
and the vice of selflessness.

326
00:19:58,380 --> 00:20:01,920
One can be too selfless,
at least biologically speaking.

327
00:20:02,400 --> 00:20:07,400
And what our social emotions do are they
help push us as a countervailing wait

328
00:20:07,681 --> 00:20:12,480
to selfishness to act in ways that builds
social capital and cement our social

329
00:20:12,481 --> 00:20:15,390
bonds.
And thank you for your attention.

330
00:20:16,070 --> 00:20:20,090
Thank you Dr. David Desteno for
just stay fabulous guest lecturer.

331
00:20:20,091 --> 00:20:24,500
That made us all think a bit more about
what our social emotions and what role

332
00:20:24,501 --> 00:20:29,501
do they play in shaping us to be not
too selfish but not too selfless either.

333
00:20:31,010 --> 00:20:34,340
So what I want to turn to now,
our takeaway questions,

334
00:20:34,341 --> 00:20:38,030
and these are going to be some of the
major take home points for you to ponder.

335
00:20:38,031 --> 00:20:42,080
After you heard our very special
guest lecture today. So for today,

336
00:20:42,440 --> 00:20:44,690
our take home questions are as follows.

337
00:20:45,500 --> 00:20:50,050
So the first question says, what is
the pro social function of gratitude?

338
00:20:50,530 --> 00:20:55,270
How can gratitude be experimentally
induced and what are the associated

339
00:20:55,271 --> 00:20:58,360
behavioral tendencies?
Second,

340
00:20:58,390 --> 00:21:03,390
what is the social function of
compassion and how can compassion affect

341
00:21:03,521 --> 00:21:05,350
perceived similarity and helping?

342
00:21:05,500 --> 00:21:10,060
So some of the experimental data that
Dr Desteno spoke about just recently.

343
00:21:10,960 --> 00:21:11,980
And then finally,

344
00:21:12,010 --> 00:21:15,250
this was sort of a interesting take home
point that was talked about it both the

345
00:21:15,251 --> 00:21:17,650
beginning and end of his guest lecture.

346
00:21:18,280 --> 00:21:23,110
To what extent do social emotions tell
us something about our moral character

347
00:21:23,320 --> 00:21:27,280
and about the different vices that
he talked about of trying to be both,

348
00:21:27,340 --> 00:21:28,173
you know,

349
00:21:28,180 --> 00:21:32,470
selfish but not too selfish and
selfless but not too selfless.

350
00:21:33,460 --> 00:21:36,520
And so with that, I want to turn
now to our expert interview.

351
00:21:37,150 --> 00:21:42,150
This is part of our experts in emotion
interview series today for our experts

352
00:21:42,731 --> 00:21:46,200
and emotion interview. We
have Dr. David Desteno. Um,

353
00:21:46,210 --> 00:21:49,180
he'll be speaking with us on
emotions and social interaction.

354
00:21:49,480 --> 00:21:53,950
So Dr Desteno received his phd in
psychology from Yale University and is

355
00:21:53,951 --> 00:21:58,030
currently a professor of psychology at
northeastern university where he directs

356
00:21:58,031 --> 00:22:00,970
the social emotions lab
at the broadest level.

357
00:22:00,971 --> 00:22:05,320
His lab examines the mechanisms of the
mind that shape social behavior from

358
00:22:05,321 --> 00:22:10,120
hypocrisy to compassion,
from pride to prejudiced, and
from honesty to punishment.

359
00:22:10,390 --> 00:22:15,310
His work continuously reveals that human
moral behavior is much more variable

360
00:22:15,340 --> 00:22:16,750
than most people would predict.

361
00:22:17,380 --> 00:22:21,610
Dr distended was a fellow of the
association for Psychological Science and

362
00:22:21,611 --> 00:22:25,300
editor of the American Psychological
Association sternal emotion.

363
00:22:25,930 --> 00:22:29,890
His work has been repeatedly funded
by the National Science Foundation,

364
00:22:30,100 --> 00:22:33,790
and he has been regularly featured in
the media, including the New York Times,

365
00:22:34,000 --> 00:22:38,320
Newsweek, CBS, Sunday morning and
PR, and the Wall Street Journal.

366
00:22:38,680 --> 00:22:42,790
He's also the coauthor of the Wall Street
Journal psychology bestseller out of

367
00:22:42,791 --> 00:22:43,624
character,

368
00:22:43,750 --> 00:22:47,200
and as written about his research for
the New York Times and Boston Globe.

369
00:22:47,470 --> 00:22:51,430
So I now turn with excitement to our
experts and emotion interview with Dr.

370
00:22:51,430 --> 00:22:51,940
David Desteno.

