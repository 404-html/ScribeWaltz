1
00:00:00,100 --> 00:00:03,060
Our experts in emotion interview
will be with our Touro.

2
00:00:03,070 --> 00:00:07,120
They Har the director of engineering at
Facebook and he'll be speaking with us

3
00:00:07,121 --> 00:00:09,010
on emotion and social media.

4
00:00:09,410 --> 00:00:12,520
Arturo as director of
engineering at Facebook,

5
00:00:12,521 --> 00:00:16,960
has worked on social tools to help
people be more mindful of each other.

6
00:00:17,260 --> 00:00:20,980
This includes such areas as
identity and abuse prevention,

7
00:00:21,160 --> 00:00:25,990
ways to develop better online citizens
and tools designed to help resolve

8
00:00:25,991 --> 00:00:28,660
conflicts between
individuals such as bullying.

9
00:00:29,170 --> 00:00:33,460
Previously he worked at Yahoo where he
worked with a global team dedicated to

10
00:00:33,461 --> 00:00:35,710
providing a secure online experience.

11
00:00:35,980 --> 00:00:40,540
So I now turn to our last experts and
emotion interview together with our tour

12
00:00:40,541 --> 00:00:43,240
obey Har on emotion and social media.

13
00:00:44,670 --> 00:00:47,550
So welcome Arturo.
Thank you for speaking with us today.

14
00:00:48,510 --> 00:00:49,950
Thank you very much for hosting me.

15
00:00:50,230 --> 00:00:50,950
Great.

16
00:00:50,950 --> 00:00:55,210
So I wanted to start just by asking you
a little bit about what first got you

17
00:00:55,211 --> 00:00:58,270
interested in studying emotion
in the context of Facebook,

18
00:00:58,271 --> 00:01:00,190
in the broader world of social media.

19
00:01:01,820 --> 00:01:06,650
So we're looking at the reports that we're
getting from people about photographs

20
00:01:06,651 --> 00:01:10,310
that they have upload on Facebook.
And usually it's most sites too.

21
00:01:10,311 --> 00:01:13,220
You have these reports that are
about the rules of the site.

22
00:01:13,280 --> 00:01:16,820
This has some violence, discuss some
drug use. And somebody says, Hey,

23
00:01:16,821 --> 00:01:19,880
this photo has Greg Hughes,
can you please make it go away?

24
00:01:19,881 --> 00:01:22,780
And we would look at the phone and
there was nothing in the photo,

25
00:01:22,820 --> 00:01:25,520
even remotely looked like that.
It was just somebody like smiling,

26
00:01:25,521 --> 00:01:28,430
waving at the camera and like,
what's going on here?

27
00:01:28,431 --> 00:01:33,020
And we realized that the person
that we're submitting the report,

28
00:01:33,620 --> 00:01:34,460
in many cases,

29
00:01:34,461 --> 00:01:39,080
most cases was in the photo and
the person had uploaded the photo,

30
00:01:39,260 --> 00:01:43,850
was one of their friends. Um, and so
we're like, how can we help with this?

31
00:01:43,940 --> 00:01:46,850
And we figured out, okay, so you want
people to be talking to each other.

32
00:01:47,210 --> 00:01:49,490
So we told them, hey, can you
send a message to your friends,

33
00:01:49,491 --> 00:01:51,980
letting them know that you would
like them to remove the picture?

34
00:01:52,570 --> 00:01:57,200
And we put on an empty message box
and only 20% of people would send the

35
00:01:57,201 --> 00:02:00,560
message. And, um, and then in most cases,

36
00:02:00,561 --> 00:02:04,130
even the messages didn't convey really
what, what it's like, hey, you know,

37
00:02:04,490 --> 00:02:04,911
this photo,

38
00:02:04,911 --> 00:02:07,730
there's something about it that makes
me want to ask you to take it down.

39
00:02:09,200 --> 00:02:11,920
At that time, we began meeting people, um,

40
00:02:12,110 --> 00:02:16,600
that are doing research scientist
from Stanford and Berkeley. And yeah,

41
00:02:17,110 --> 00:02:20,450
we're doing all of this research about
how people communicate and relate to each

42
00:02:20,451 --> 00:02:21,284
other.

43
00:02:21,290 --> 00:02:24,680
And one of the things that we learned
is if you pick up that somebody else is

44
00:02:24,681 --> 00:02:28,520
experiencing an emotion, you're more
likely to have a compassionate response.

45
00:02:28,521 --> 00:02:30,110
You're more likely to want to help them.

46
00:02:31,310 --> 00:02:34,970
So we did this experiment
where we provided some
default messaging that people

47
00:02:34,971 --> 00:02:38,900
can edit and send saying, hey,
you know, I don't like this photo.

48
00:02:38,901 --> 00:02:40,760
Could you please to gate down.
It makes me sad,

49
00:02:40,790 --> 00:02:45,050
but experimented with emotion and we saw
a really meaningful results from that.

50
00:02:45,440 --> 00:02:49,580
Over 50% of people would send a message
and then most of the people who receive

51
00:02:49,581 --> 00:02:52,150
the message would remove
the photo. Um, I mean,

52
00:02:52,151 --> 00:02:55,070
we knew we were onto something
and this was where it began.

53
00:02:55,170 --> 00:02:57,770
A lawyers of collaborations
we're doing right now.

54
00:02:58,130 --> 00:03:01,960
We're exploring what are ways that people
can communicate emotion to each other

55
00:03:01,961 --> 00:03:02,794
online.

56
00:03:03,090 --> 00:03:05,850
That's fascinating. And so I
wanted to ask you then, you know,

57
00:03:05,851 --> 00:03:09,530
the work that you're doing right now
at Facebook related to emotion. Um,

58
00:03:09,600 --> 00:03:12,690
what role have you found
in this work so far?

59
00:03:12,990 --> 00:03:17,280
I'm suggesting that emotion does play
a role in these online interactions and

60
00:03:17,281 --> 00:03:18,760
Facebook.
Okay.

61
00:03:18,960 --> 00:03:22,010
Oh, it is a central, um, uh, because it's,

62
00:03:22,011 --> 00:03:25,560
it's central to be how people
relate with each other. Um, we're,

63
00:03:25,590 --> 00:03:30,240
we have this flow now that that'd
be put into place where we say, Hey,

64
00:03:30,241 --> 00:03:32,960
I'm in the photo. I don't want
others to see me in the story.

65
00:03:32,961 --> 00:03:36,360
If you're known as a framing on
that is, it's very emotional. Um,

66
00:03:36,510 --> 00:03:40,490
and then we asked you what is it about
this photo that you don't like and we'll

67
00:03:40,491 --> 00:03:44,550
give you options that our emotions, it's
embarrassing. It's about photo of me,

68
00:03:44,610 --> 00:03:49,080
not as much emotion, but an
important thing to convey.
It makes me angry or upset.

69
00:03:49,081 --> 00:03:50,760
It makes me sad.
Um,

70
00:03:50,820 --> 00:03:55,680
and we found that people would
select an emotion and then,

71
00:03:55,770 --> 00:03:56,720
um,
uh,

72
00:03:56,790 --> 00:04:00,240
take the message where we provided that
incorporated what we have learned about

73
00:04:00,241 --> 00:04:03,480
politeness and how you
communicate. So, um, Hey Alan,

74
00:04:03,481 --> 00:04:06,820
is there something about this photo
that's a little embarrassing to me? Um,

75
00:04:07,020 --> 00:04:11,070
could you please take it down?
And we found that over 60,

76
00:04:11,071 --> 00:04:15,840
I think 67% of the people will actually
send the message, which is huge.

77
00:04:15,870 --> 00:04:18,060
Wow. Um, and, uh,

78
00:04:18,150 --> 00:04:22,810
depending on the message between 80 and
90% of people will use the texts that we

79
00:04:22,811 --> 00:04:27,580
provided, um, which means that
we've captured their experience. Um,

80
00:04:27,660 --> 00:04:28,880
and even,
um,

81
00:04:28,980 --> 00:04:32,310
I think more exciting than that is
we asked the people who had created,

82
00:04:32,370 --> 00:04:37,230
uploaded the photo, the content
creators, how they felt about the person,

83
00:04:37,560 --> 00:04:39,260
what sent them the message.
Um,

84
00:04:39,360 --> 00:04:42,120
and remember this is somebody who is
just share something with all of their

85
00:04:42,121 --> 00:04:45,390
friends because they think
it's really cool. And one of
the friends told him, Hey,

86
00:04:45,391 --> 00:04:48,300
you know, that actually, you
know, it's upsetting to me. Well,

87
00:04:48,301 --> 00:04:53,301
it turns out that the person who receives
the message feels at 60% positive,

88
00:04:53,640 --> 00:04:58,380
like 6% of them feel positive about the
person who sent them the message. Wow. Um,

89
00:04:58,410 --> 00:05:03,360
and another 20%, they feel neutral.
So it's like 80% of people,

90
00:05:03,420 --> 00:05:07,410
they kind of want to know if they did
something that was upsetting to you,

91
00:05:07,590 --> 00:05:11,670
which is very interesting to contrast
against the only 20% of people willing to

92
00:05:11,671 --> 00:05:15,930
communicate that emotion to their
friends to help resolve the situation.

93
00:05:16,690 --> 00:05:17,650
It's so interesting.

94
00:05:17,651 --> 00:05:20,560
So in what way do you think is
worth thinking about these emotional

95
00:05:20,561 --> 00:05:25,030
relationships between people? You know,
I'm trying to do things that will foster,

96
00:05:25,300 --> 00:05:28,270
you know, more positive
interactions online. Um,

97
00:05:28,300 --> 00:05:32,320
I want to ask a little bit about your
work suggesting that Facebook may actually

98
00:05:32,321 --> 00:05:36,400
be one route to sort of cultivate,
you know, compassion and encourage it.

99
00:05:37,790 --> 00:05:38,180
Well,

100
00:05:38,180 --> 00:05:43,180
we think based on the research that
we've seen about how if you detect that

101
00:05:43,671 --> 00:05:45,500
somebody else is experiencing emotion,

102
00:05:45,830 --> 00:05:47,870
you're more likely to want
to ease their suffering.

103
00:05:47,900 --> 00:05:50,780
And so hence the engage in a kind
of question and response. Um,

104
00:05:51,590 --> 00:05:54,040
so I think we all know that,
uh,

105
00:05:54,050 --> 00:05:58,670
emails and messages might not be the best
way to convey. As at certain emotions,

106
00:05:59,270 --> 00:06:02,720
happy emotions are actually fairly
easy to convey like smiley faces,

107
00:06:02,721 --> 00:06:03,551
like buttons.

108
00:06:03,551 --> 00:06:08,551
These things are very easy to convey
that our positive feelings but conveying

109
00:06:08,750 --> 00:06:12,080
more complicated emotional
stake, something gets
embarrassing or something sad.

110
00:06:12,320 --> 00:06:16,460
Uh, we have looked at all
of these mechanisms that we
have when we talk to each

111
00:06:16,461 --> 00:06:19,120
other where we can, we
didn't, somebody faces,

112
00:06:19,130 --> 00:06:20,780
you can hear it and
that's one of their boys.

113
00:06:21,110 --> 00:06:26,110
And there's an absence of that
today in many online communications.

114
00:06:26,630 --> 00:06:29,150
And so when we look at
the fabric of Facebook,

115
00:06:29,151 --> 00:06:31,700
which is all about connecting people
to each other and having great

116
00:06:31,701 --> 00:06:35,760
relationships and cultivating
that community over time, um,

117
00:06:35,780 --> 00:06:40,450
having a mechanism that takes something
that could be a source of distancing. Um,

118
00:06:40,510 --> 00:06:43,730
if, if you uploaded a photo and it made
me upset and I didn't tell you anything

119
00:06:43,731 --> 00:06:47,700
about it, that that pushes
people further apart. Yeah. It,

120
00:06:47,780 --> 00:06:51,500
instead of having that you could have
something where you can communicate how

121
00:06:51,501 --> 00:06:54,590
you feel and you can feel hurt.
The other person says, Oh,

122
00:06:54,591 --> 00:06:58,310
I'm sorry it made you feel this way too.
It's not my intent at all.

123
00:06:58,640 --> 00:07:01,460
And you begin a dialogue,
even if the father doesn't come down,

124
00:07:01,490 --> 00:07:03,290
which is a perfectly legitimate outcome,

125
00:07:03,560 --> 00:07:06,650
that relationship gets closer
and that bonds Franklin's.

126
00:07:07,100 --> 00:07:11,270
And so we hope if we have interactions
that have positive results for people,

127
00:07:11,600 --> 00:07:14,720
that people will go like, and then what
happened there? What can I learn from,

128
00:07:14,721 --> 00:07:18,560
from that communication and take this
into other aspects of their lives.

129
00:07:19,180 --> 00:07:22,180
So when you think about trying to
promote a community that brings people

130
00:07:22,181 --> 00:07:24,460
together as opposed to
distancing them apart,

131
00:07:24,970 --> 00:07:26,830
is this part of the reason underlying,
you know,

132
00:07:26,831 --> 00:07:30,850
why there might not be the dislike button
that a lot of people are often asking

133
00:07:30,860 --> 00:07:31,693
about?

134
00:07:32,710 --> 00:07:36,700
I think so. I mean, I think
that, um, uh, this like, uh,

135
00:07:37,000 --> 00:07:42,000
could be very easily misinterpreted and
the ideal form of a dislike is one that

136
00:07:42,101 --> 00:07:46,210
begins a conversation. Um, so you
want to be able to say, hey, you know,

137
00:07:46,211 --> 00:07:50,650
that thing that you, ah, shit, I disagree
with that. Um, or that thing that,

138
00:07:50,710 --> 00:07:54,790
um, that, that you posted and Maytag and
make me sad. It was upsetting to me. Um,

139
00:07:54,840 --> 00:07:58,090
and you need really good ways of
communicating emotion for that to be a

140
00:07:58,091 --> 00:08:00,230
successful conversation.
Um,

141
00:08:00,231 --> 00:08:04,720
and so we're starting in this realm
of like photos and feedback. Um, and,

142
00:08:04,780 --> 00:08:05,321
and uh,

143
00:08:05,321 --> 00:08:09,250
and also bullying is another area that
we're exploring to build tools that can

144
00:08:09,280 --> 00:08:12,040
successfully communicate
emotion to and between people,

145
00:08:12,070 --> 00:08:16,240
these more complicated emotions. Um, but
we hope that if we do a good job on this,

146
00:08:16,241 --> 00:08:20,200
that what we can learn here can be
applied in many other areas of online

147
00:08:20,201 --> 00:08:21,034
communication.

148
00:08:21,730 --> 00:08:24,550
So in embarking in this really,
you know,

149
00:08:24,551 --> 00:08:29,140
still mysterious but rapidly growing
world, you know, of online interaction,

150
00:08:29,170 --> 00:08:33,010
has there been anything
that's really surprised you
during your time at Facebook?

151
00:08:34,120 --> 00:08:38,830
Oh, man. Every step along
the way, there's game.

152
00:08:38,860 --> 00:08:42,070
So crisis, um, [inaudible] talks about,

153
00:08:42,071 --> 00:08:46,810
which is only 20% of the people willing
to send a message telling their friends,

154
00:08:47,110 --> 00:08:50,170
but most of their friends wanting to
hear about and feeling good about the

155
00:08:50,171 --> 00:08:54,930
contact. That was a really big
surprise. Um, another one is, uh,

156
00:08:54,960 --> 00:08:58,710
when people come into the report
flow for most of them over 80%,

157
00:08:58,711 --> 00:09:02,340
it's extremely important, um,
for the photo to go to come down.

158
00:09:02,850 --> 00:09:06,450
But we believe that the action of um,

159
00:09:06,600 --> 00:09:08,310
acknowledging this is what's happening,

160
00:09:08,340 --> 00:09:13,050
naming the emotion that
you're experiencing and then
communicating that to the

161
00:09:13,051 --> 00:09:17,270
other person in a way where you feel
like you've successfully, um, eh, eh,

162
00:09:17,340 --> 00:09:19,050
are heard.
Um,

163
00:09:19,290 --> 00:09:22,440
the combination of those things make it
so that at the end of that experience we

164
00:09:22,441 --> 00:09:25,800
asked them how they feel about the person
that uploaded the photo and the number

165
00:09:25,801 --> 00:09:28,980
said or 50% positive. Um, which is again,

166
00:09:29,010 --> 00:09:32,310
it's surprising considering you're
going into this going like, oh wait,

167
00:09:32,311 --> 00:09:34,100
did you share this at the same metric?
Me,

168
00:09:35,100 --> 00:09:37,590
another one that I think has
been a very important insight,

169
00:09:37,591 --> 00:09:41,850
which goes against anything that you've
seen in terms of computer design.

170
00:09:41,880 --> 00:09:46,880
And experience is that you get taught
in all computer design classes.

171
00:09:47,340 --> 00:09:52,130
That language has to be very clear
and very universal. So we been there,

172
00:09:52,140 --> 00:09:52,500
this,

173
00:09:52,500 --> 00:09:57,090
this slide where we can ask for a motion
photo makes you feel our first version

174
00:09:57,091 --> 00:10:01,710
of it had adjectives. I'm like
embarrassing. Just the word embarrassing.

175
00:10:01,740 --> 00:10:06,000
I just the word saddening. And we just
had that the simplest, cleanest Ui.

176
00:10:06,750 --> 00:10:11,750
And then we had an option to in other
where people could type in and only at 50%

177
00:10:12,871 --> 00:10:15,870
of people would select
an emotion and then, uh,

178
00:10:16,040 --> 00:10:19,430
the other people and other 30%
would put something in other.

179
00:10:20,130 --> 00:10:25,050
And what would they select in other, it
would type in, it's embarrassing. Hmm.

180
00:10:25,530 --> 00:10:30,070
We're going like, wait, there was like
an option for him. What's going on? Um,

181
00:10:30,210 --> 00:10:34,680
well it turns out that it's important
when you design things online that you use

182
00:10:34,690 --> 00:10:37,140
language that matches the experience.
Can they,

183
00:10:37,141 --> 00:10:39,720
that people use when they talk
about things with each other.

184
00:10:40,500 --> 00:10:44,880
So we change the options to sentence
fragments. It's embarrassing.

185
00:10:45,420 --> 00:10:47,730
It makes, uh, um, and,

186
00:10:47,740 --> 00:10:51,090
and when we changed the emotions
to a sentence fragments,

187
00:10:51,420 --> 00:10:55,230
we went from a 50% selection to 78%,

188
00:10:55,820 --> 00:10:59,580
which is in the world of online systems
that that 30 point swings are just like

189
00:10:59,581 --> 00:11:01,730
completely nuts and wonderful.
Um,

190
00:11:01,980 --> 00:11:05,850
and we've applied that principle in other
areas of the work where the language

191
00:11:05,851 --> 00:11:09,930
is much longer and much more descriptive
than what you would expect from any

192
00:11:09,931 --> 00:11:11,610
normal product that you have.

193
00:11:11,940 --> 00:11:16,260
But all of the measures of the language
say having language that matches people

194
00:11:16,261 --> 00:11:17,640
experience at captured,

195
00:11:17,641 --> 00:11:21,270
whether it is that they're feeling that
they're trying to say steak really works

196
00:11:21,271 --> 00:11:23,740
and, and, and to the tune of 20%,

197
00:11:23,970 --> 00:11:27,690
30% improvements in the areas where
we've applied these principles.

198
00:11:28,010 --> 00:11:28,843
Wow.

199
00:11:29,000 --> 00:11:32,450
So in what way have you felt that your
collaborations with people at Berkeley

200
00:11:32,451 --> 00:11:35,030
like Dacher Keltner, Paul Piff, you know,

201
00:11:35,031 --> 00:11:38,290
people at Yale like Marc
Brackett among many others, um,

202
00:11:38,720 --> 00:11:42,110
how has this work from people who
study aspect of science, you know,

203
00:11:42,111 --> 00:11:43,070
in psychology.

204
00:11:43,340 --> 00:11:46,610
How has that informed the kind of work
that you're doing now at Facebook?

205
00:11:47,310 --> 00:11:50,280
Oh, it's been insanely wonderful. Good.

206
00:11:52,200 --> 00:11:54,240
Well because, uh, it, it, it,

207
00:11:54,370 --> 00:11:58,450
everything I'm talking about has been
born out of conversations with them.

208
00:11:58,990 --> 00:12:01,670
So in the work we've been
doing with, with mark, um,

209
00:12:01,800 --> 00:12:04,280
at bracket from Yale on bullying,
um,

210
00:12:04,440 --> 00:12:06,660
we're asking these really
big questions about like,

211
00:12:06,670 --> 00:12:10,180
what kind of bullying is happening
for you? How does it make you feel?

212
00:12:10,240 --> 00:12:13,870
How intense is the emotion? I mean, it's,
this is, it's pretty big to say, hey,

213
00:12:14,230 --> 00:12:17,900
are you feeling afraid
halfway? Do you feel, um, and,

214
00:12:17,950 --> 00:12:21,280
but it's central for being able to get
people kind of the tools that they need

215
00:12:22,150 --> 00:12:22,983
and the department,

216
00:12:23,010 --> 00:12:26,230
and these are very close partnerships
where we meet up with them once a week and

217
00:12:26,231 --> 00:12:30,700
we talked through the different ideas.
We share the numbers, um, and, and uh,

218
00:12:30,701 --> 00:12:33,240
and the same has been proved
the relationship with Paul,

219
00:12:33,241 --> 00:12:35,230
I piss and attackers and the,

220
00:12:35,410 --> 00:12:40,060
I'm on Thomas from Berkeley where we sit
down and we'll look at the experiences

221
00:12:40,061 --> 00:12:42,600
people are having and we're
asking the big question of like,

222
00:12:43,300 --> 00:12:45,310
what's really going on here?
What punches?

223
00:12:45,311 --> 00:12:48,280
And when we first began
getting exposed to the word,

224
00:12:48,281 --> 00:12:51,710
like that emotional intelligence
work. And, and the, um,

225
00:12:52,170 --> 00:12:56,080
as a self regulation and emotional
literacy for kids with mark, uh,

226
00:12:56,320 --> 00:12:58,570
and bracket and the work that,
uh,

227
00:12:58,630 --> 00:13:02,020
they were good is doing with Paul's
work and that work and Eliana sport,

228
00:13:02,230 --> 00:13:06,400
but in some ways that people communicate
in motion with each other. Um, it was,

229
00:13:06,401 --> 00:13:08,680
it was very exciting for us
because we're like, okay, this,

230
00:13:08,681 --> 00:13:12,880
this is a total application for
everything we're doing. And,

231
00:13:12,910 --> 00:13:13,900
and it was the basis of,

232
00:13:13,901 --> 00:13:17,020
of the compassionate research stay that
we have or which we have another one

233
00:13:17,290 --> 00:13:18,900
tomorrow.
They were very excited about.

234
00:13:19,230 --> 00:13:23,790
That's so exciting to hear that Facebook
is taking the science of emotion and

235
00:13:23,791 --> 00:13:27,870
using it to change the way that we
interact with this world of Facebook.

236
00:13:27,871 --> 00:13:30,240
That's so many people participate in.

237
00:13:31,400 --> 00:13:35,900
Yes. And it's, it's our hope by talking
about it like talking with you today,

238
00:13:35,901 --> 00:13:39,080
but having a confessional research to
any sort of other outlets that we're

239
00:13:39,081 --> 00:13:44,081
pursuing to help other people understand
just how much we've gotten out of our

240
00:13:44,451 --> 00:13:49,270
partnerships with, uh, our
researcher friends. Um,

241
00:13:49,310 --> 00:13:53,480
and, uh, and because I think anybody who's
building any kind of tools to enable to

242
00:13:53,481 --> 00:13:58,481
medication or to be learning from what
we were learning out to be working with

243
00:13:58,731 --> 00:14:02,520
their local scientists to figure out
how they can explore these ideas. Um,

244
00:14:02,780 --> 00:14:05,660
because by every measure
of the work we've done,

245
00:14:05,900 --> 00:14:10,310
it's been a positive thing for the
people who use Facebook. Um, and um,

246
00:14:10,340 --> 00:14:11,930
and we hope that by the data that we get,

247
00:14:11,931 --> 00:14:16,400
and then we sorta put sharings that
illuminates some pathways in the fields

248
00:14:16,610 --> 00:14:19,550
because one of the things you talk
about when I'm getting started is most

249
00:14:19,551 --> 00:14:24,070
studies in the field and what was 200
people and everything we've done here

250
00:14:24,100 --> 00:14:26,960
spends hundreds of thousands if not
millions of people that have gone through

251
00:14:26,961 --> 00:14:30,500
this. And they're telling us that's
making the feel and it's like the results.

252
00:14:30,501 --> 00:14:34,250
Cause we help open up pathways for
other areas of research and inquiry.

253
00:14:34,610 --> 00:14:38,660
I mean absolutely those, those kinds of
sample sizes or things that, you know,

254
00:14:38,661 --> 00:14:41,000
psychologists like myself and
the ones you collaborate with,

255
00:14:41,001 --> 00:14:45,440
we could never dream of.
Right. You know, you be there.

256
00:14:45,890 --> 00:14:47,090
Yeah. Facebook, you know,

257
00:14:47,360 --> 00:14:52,200
not so bad when it comes to just
being able to access so many people.

258
00:14:52,230 --> 00:14:52,591
Right.

259
00:14:52,591 --> 00:14:57,330
And really get truly rich emotions that
are things that are meaningful to them,

260
00:14:57,331 --> 00:15:00,300
you know, pictures of themselves,
relationships with their friends.

261
00:15:00,301 --> 00:15:03,930
I mean this is where emotion,
the meat of it is.

262
00:15:04,850 --> 00:15:07,470
Yes. And there was a real, um,

263
00:15:08,380 --> 00:15:13,380
I think responsibility on us to honor the
trust that people have in sharing this

264
00:15:13,701 --> 00:15:16,880
information through us and providing
them the best tools that we can.

265
00:15:17,330 --> 00:15:21,560
And we'll do the genesis of all of this
work is going at the issues that people

266
00:15:21,561 --> 00:15:25,250
are having on the side and understanding
them and understand what's going on,

267
00:15:25,251 --> 00:15:27,880
what happens when there's a breakup and,

268
00:15:27,940 --> 00:15:30,530
and somebody posts pictures
of it after a couple of,

269
00:15:30,531 --> 00:15:31,790
when they were there together.

270
00:15:32,060 --> 00:15:35,430
And have you resolved that
communication between people? Um,

271
00:15:35,600 --> 00:15:38,660
what happens when uh, uh, like a low too,

272
00:15:38,661 --> 00:15:41,030
so that a little bit
bullying is not intentional.

273
00:15:41,031 --> 00:15:43,940
It's somebody coming in and say,
Hey, you look really beautiful today.

274
00:15:44,420 --> 00:15:46,380
And then the person looking at that,

275
00:15:46,490 --> 00:15:51,140
the message feeling bad about their self
image and then navigating that and up

276
00:15:51,141 --> 00:15:53,430
to the point where we began
this partnership setting.

277
00:15:53,450 --> 00:15:57,830
The standard for the industry is like
you look at a report, the report,

278
00:15:57,831 --> 00:16:00,540
Newson criteria,
a hand comes down from the stand,

279
00:16:00,650 --> 00:16:05,540
makes it the piece of content go away.
But who learns from that? How do you grow?

280
00:16:05,541 --> 00:16:09,200
How do you help the community heal
and improve from that? Instead,

281
00:16:09,201 --> 00:16:11,660
what we're trying to
do with, um, with, uh,

282
00:16:11,670 --> 00:16:15,620
the work that we're doing for sample on
bullying is can you talk to the person

283
00:16:15,621 --> 00:16:19,100
if you can feel comfortable doing so
whole uploaded the content? If not,

284
00:16:19,101 --> 00:16:22,340
can you reach out to somebody whom you
trust that can support you navigating

285
00:16:22,341 --> 00:16:22,551
this?

286
00:16:22,551 --> 00:16:25,790
Because the best thing you can do for
somebody who is the recipient or feels

287
00:16:26,090 --> 00:16:29,630
bullied or targeted is to get somebody
in their life can sit down with them and

288
00:16:29,631 --> 00:16:33,440
help them navigate the
situation and hopefully give
them tools to navigate it as

289
00:16:33,441 --> 00:16:36,590
well. And so this is all, this
is the first year of the field,

290
00:16:36,630 --> 00:16:41,090
literally began working on this a year
ago in truly Ernest partnership with our

291
00:16:41,091 --> 00:16:42,290
scientists. But, um,

292
00:16:42,590 --> 00:16:45,200
but I think there's a lot of good work
that remains to be done in the field.

293
00:16:45,620 --> 00:16:49,790
So what do you see in store for the
future of emotion and social media in

294
00:16:49,791 --> 00:16:51,530
Facebook and,
and more generally?

295
00:16:53,220 --> 00:16:56,560
So think we need to be exploring
different ways for people can communicate

296
00:16:56,561 --> 00:16:58,190
emotion to each other.
Um,

297
00:16:58,210 --> 00:17:02,180
there's work around the voice that's
really interesting in terms of sealing

298
00:17:02,190 --> 00:17:07,190
sales work about facial expressions and
how could you actually communicate those

299
00:17:07,600 --> 00:17:09,670
more nuanced feeling sort
of facial expressions.

300
00:17:09,671 --> 00:17:13,900
Is there a better smiley to be had
then the ones that we have today. Um,

301
00:17:13,930 --> 00:17:17,560
and I think there's also a lot of work
to be done in supporting different kinds

302
00:17:17,561 --> 00:17:22,330
of communities we're looking
at, at, at what we can do,
for example, with veterans.

303
00:17:22,610 --> 00:17:25,360
Uh, we're as a community that in
order for them to get support,

304
00:17:25,510 --> 00:17:28,270
it has to come from
somebody who they trust and,

305
00:17:28,280 --> 00:17:30,580
and have a sense of
identification with them.

306
00:17:31,090 --> 00:17:36,040
And so being able to give people
the tools that make them feel seen,

307
00:17:36,100 --> 00:17:39,840
heard, and met on language or spoke to
each other, it seems like it's, it's,

308
00:17:39,841 --> 00:17:43,750
it's something that we're just beginning
to understand that of the work from the

309
00:17:43,750 --> 00:17:46,510
last year and that we will be exploring
for the foreseeable future. Sure.

310
00:17:47,480 --> 00:17:51,390
So what kind of advice do you have for
students or people who are interested in

311
00:17:51,391 --> 00:17:55,860
learning more about emotion, you
know, and how it sort of, um,

312
00:17:56,340 --> 00:17:58,890
I don't know,
sort of manifests in this world,

313
00:17:58,891 --> 00:18:01,080
this social media world and in Facebook.

314
00:18:01,350 --> 00:18:03,600
What would you tell them if they
want to learn more about it?

315
00:18:06,500 --> 00:18:10,020
Two things. One is we do have
this compassion research videos,

316
00:18:10,460 --> 00:18:14,630
articles written, happy to see what
people have to see for themselves there.

317
00:18:14,631 --> 00:18:18,500
And so we're going to be live
streaming the day tomorrow. Um, and uh,

318
00:18:18,501 --> 00:18:21,800
and there should be, there's going to
be lots of really wonderful stuff there.

319
00:18:22,490 --> 00:18:26,330
But the other one is I think the biggest
lesson that I've gotten out of my work

320
00:18:26,331 --> 00:18:29,020
in this space, which is that, um,

321
00:18:29,150 --> 00:18:34,150
a lot of people seem to think that
because it's online it is different and,

322
00:18:34,700 --> 00:18:37,990
and as you start from the thesis,
it has to be different online.

323
00:18:38,000 --> 00:18:39,730
The mechanisms to resolve
just these things.

324
00:18:39,740 --> 00:18:44,740
It has to be different online and they
start carve out areas and it turns out

325
00:18:45,771 --> 00:18:46,320
the listen,

326
00:18:46,320 --> 00:18:50,450
like people are people and if you
want to make a big difference on line,

327
00:18:50,451 --> 00:18:54,650
the best thing that you could possibly
do is do a great job of understanding how

328
00:18:54,651 --> 00:18:59,360
people relate and communicate and resolve
issues with each other in different

329
00:18:59,361 --> 00:19:03,750
kinds of communities across the world.
Turns into debt. You can't, you, you,

330
00:19:03,770 --> 00:19:06,860
you have to be in communal living and
have to see the person that they have to.

331
00:19:06,861 --> 00:19:11,120
So you don't have the luxury of like
being really mean to them and then or

332
00:19:11,121 --> 00:19:14,030
disrespectful to them because
no matter whatever happened,

333
00:19:14,031 --> 00:19:16,670
I mean you're going to have to live with
them the next thing for the cost per

334
00:19:16,671 --> 00:19:19,190
serving. But it had like
people, it has more respectful,

335
00:19:20,030 --> 00:19:22,400
something's funny have to be,
um,

336
00:19:22,460 --> 00:19:27,110
but you could go into a high school
and study what are the emotions that,

337
00:19:27,200 --> 00:19:28,040
uh, like, uh,

338
00:19:28,120 --> 00:19:33,080
teens communicates to each other that
helped them resolve and navigate conflicts

339
00:19:33,470 --> 00:19:36,440
and take those lessons and then go
look at a chapter on one line and say,

340
00:19:36,441 --> 00:19:39,300
what can they, what happened from what
I learned in the high school? What,

341
00:19:39,320 --> 00:19:41,620
how could I apply this to a chat room?
Um,

342
00:19:41,630 --> 00:19:44,330
what I have learned from the wisdom of
a community that seems to be functioning

343
00:19:44,331 --> 00:19:48,380
really well. How can I apply
that in this other context?

344
00:19:48,400 --> 00:19:50,570
And that's the way the
information needs to flow.

345
00:19:51,150 --> 00:19:52,930
So I think the best thing that comes to a,

346
00:19:52,931 --> 00:19:57,830
or a student that's getting started on
the field is to pick a problem that that

347
00:19:57,831 --> 00:20:01,880
gets addressed by successful
communication of emotion in real life,

348
00:20:01,910 --> 00:20:04,610
like within their circle of friends
from in a school or something like that.

349
00:20:04,611 --> 00:20:08,990
And study that and then see how those
results and lessons map to an online

350
00:20:08,991 --> 00:20:12,590
environment because there's a lot of
that that needs to be happening in the

351
00:20:12,590 --> 00:20:14,540
coming years. And again, as I
said, we're just getting started.

352
00:20:15,240 --> 00:20:18,740
Fantastic. Well, thank you so much
for speaking with us today, Arturo.

353
00:20:18,930 --> 00:20:22,990
It was fantastic to hear your thoughts
and thank you very much for having timing.

354
00:20:23,040 --> 00:20:23,873
Thank you.

355
00:20:23,970 --> 00:20:28,050
So this concludes our experts in emotion
interview with Facebook engineering

356
00:20:28,051 --> 00:20:31,260
director, our Touro. They
had, thank you so much again.

