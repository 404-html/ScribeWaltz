1
00:00:00,650 --> 00:00:05,650
Dr Rand earned his phd from Harvard
University and systems biology and his Ba

2
00:00:06,021 --> 00:00:09,020
from Cornell University
in computational biology.

3
00:00:09,800 --> 00:00:12,650
His work focuses on human
cooperative behavior.

4
00:00:12,920 --> 00:00:17,240
His approach combines
empirical observations from
behavioral experiments with

5
00:00:17,241 --> 00:00:22,130
predictions generated by evolutionary
game theoretic math models and computer

6
00:00:22,131 --> 00:00:25,130
simulations.
By doing this doctor ran ass.

7
00:00:25,160 --> 00:00:30,160
What prosocial and antisocial decisions
people make in particular situations and

8
00:00:30,471 --> 00:00:31,520
social environments.

9
00:00:31,760 --> 00:00:35,450
The cognitive mechanisms that determine
how these decisions are actually made

10
00:00:35,720 --> 00:00:40,520
and the ultimate explanations for why our
decision making processes have come to

11
00:00:40,521 --> 00:00:41,690
function as they do.

12
00:00:42,230 --> 00:00:46,250
He draws on methodologies from psychology
as well as economics and evolutionary

13
00:00:46,251 --> 00:00:50,810
biology and is interested in applications
including law management and public

14
00:00:50,811 --> 00:00:51,644
policy.

15
00:00:51,860 --> 00:00:55,460
Dr Ran has published in high impact
journals including nature and science,

16
00:00:55,850 --> 00:00:59,330
was named wired magazine's smart list,
uh,

17
00:00:59,360 --> 00:01:02,750
in 2012 as one of 50 young
people who will change the world.

18
00:01:03,110 --> 00:01:06,350
And his work has widely appeared
in the media, including the BBC,

19
00:01:06,620 --> 00:01:10,940
NPR morning edition marketplace and all
things considered scientific American

20
00:01:10,941 --> 00:01:14,990
Huffington posts, a New York
Times, La Times, the Atlantic,

21
00:01:14,991 --> 00:01:19,990
The Washington Post, and Msnbc
among many others. Today,

22
00:01:19,991 --> 00:01:24,790
I'll be speaking with Dr. David
Rand from Yale University. Hi, Dave.

23
00:01:24,850 --> 00:01:26,170
Thanks for speaking with us today.

24
00:01:26,860 --> 00:01:28,170
Uh,
my greatest pleasure.

25
00:01:28,720 --> 00:01:29,850
So, Dave, um,

26
00:01:30,070 --> 00:01:33,820
I think what I'd love to hear from you
is just a little bit about what first got

27
00:01:33,821 --> 00:01:36,460
you interested in the topic of emotion.

28
00:01:36,670 --> 00:01:40,850
So maybe you could tell us a little bit
about what took you to that path. Um,

29
00:01:40,900 --> 00:01:44,230
and you know, what, what do you
think really sparked your interest?

30
00:01:45,040 --> 00:01:49,450
So the way that I came to all of this
is maybe a little bit of traditional,

31
00:01:49,690 --> 00:01:54,340
which is that I, um, I kind of
came out of experimental economics,

32
00:01:54,640 --> 00:01:55,750
uh,
and an econ.

33
00:01:55,900 --> 00:02:00,900
There's this huge focus on rationality
as like the key and center of everything.

34
00:02:01,200 --> 00:02:04,900
Um, and I'm particularly interested
in pro social behavior, cooperation,

35
00:02:04,901 --> 00:02:09,730
altruism kind of stuff. And it
was pretty clear to me that, uh,

36
00:02:10,570 --> 00:02:14,200
rationality with not the only thing
or even perhaps the main thing,

37
00:02:14,201 --> 00:02:16,570
I'm motivating a lot of
decisions in that domain.

38
00:02:16,571 --> 00:02:20,500
So as I try and understand more and more
how people actually make decisions in

39
00:02:20,501 --> 00:02:22,480
the context of how they each other,
uh,

40
00:02:22,481 --> 00:02:27,481
it became clear that sort of intuitive
automatic processes were really important

41
00:02:27,701 --> 00:02:29,170
of which emotions are a big company.

42
00:02:29,950 --> 00:02:31,960
Excellent.
So what's really neat about your work,

43
00:02:31,961 --> 00:02:35,770
and I would love to ask you some questions
now about your research is that it

44
00:02:35,771 --> 00:02:36,650
really,
um,

45
00:02:36,670 --> 00:02:40,420
working on cooperation takes this really
awesome interdisciplinary approach.

46
00:02:40,421 --> 00:02:43,300
I mean, you draw and
methodologies from psychology,

47
00:02:43,570 --> 00:02:47,530
economics and evolutionary biology,
which is a really unique perspective.

48
00:02:47,531 --> 00:02:50,680
I don't think that anyone to date has
really taken to look at the role of

49
00:02:50,681 --> 00:02:55,090
cooperation and, and how it relates
to emotion. So in humans, you know,

50
00:02:55,091 --> 00:03:00,091
you found that are more quick moving and
intuitive instincts are to be where are

51
00:03:00,401 --> 00:03:04,630
more slow moving or reflective responses
are actually to be more self focused or

52
00:03:04,631 --> 00:03:05,464
selfish.

53
00:03:05,680 --> 00:03:09,100
And I wondered if you could tell us all
just a little bit more about the studies

54
00:03:09,101 --> 00:03:10,570
that you've been doing in this vein.

55
00:03:11,430 --> 00:03:13,230
Yeah, so, uh,

56
00:03:13,950 --> 00:03:17,700
we did a bunch of experiments using um,
economic games.

57
00:03:17,701 --> 00:03:21,660
So the sort of thing where
you create a [inaudible],

58
00:03:21,790 --> 00:03:25,920
the key of sort of cooperation is
situations where there's a tension between

59
00:03:25,921 --> 00:03:26,640
what's individual,

60
00:03:26,640 --> 00:03:30,030
what's best for the individual and
what's best for the group as a whole.

61
00:03:30,390 --> 00:03:34,470
So using these economic games you do
things like everybody gets some money,

62
00:03:34,560 --> 00:03:37,830
they choose how much to keep for
themselves and how it to contribute to the

63
00:03:37,831 --> 00:03:38,730
group.
We're,

64
00:03:38,731 --> 00:03:42,090
all of the contributions get doubled
and then split equally by everyone.

65
00:03:42,150 --> 00:03:45,540
So if everyone contributes,
everyone is better off. But uh,

66
00:03:45,541 --> 00:03:47,400
you personally would lose
money on contributing.

67
00:03:47,401 --> 00:03:50,370
So the best thing for you
would be everyone else
contributes and you just free

68
00:03:50,371 --> 00:03:55,020
ride off of that as soon as they create
this tension. And then we look at, uh,

69
00:03:55,021 --> 00:03:57,810
you know, what happens when people have
to make actually make their decisions.

70
00:03:58,080 --> 00:04:02,550
And what we find is what you were saying
that it seems like the automatic sort

71
00:04:02,551 --> 00:04:07,540
of first response, intuitive decision
is to contribute a lot and then, uh,

72
00:04:07,560 --> 00:04:12,240
the longer you think about it,
the more selfish you get. And so,

73
00:04:12,750 --> 00:04:17,160
um, the correlational evidence,
we have manipulation studies.

74
00:04:17,161 --> 00:04:19,810
You force people to respond quickly.
They contribute more,

75
00:04:19,830 --> 00:04:23,880
you prime them to be more
intuitive, they contribute more. Um,

76
00:04:24,720 --> 00:04:29,550
and I think the other thing that I want
to say about it is that it's a little

77
00:04:29,551 --> 00:04:32,550
bit of a simplification to say that,
uh,

78
00:04:32,880 --> 00:04:35,920
we are intuitively cooperative,
um,

79
00:04:36,060 --> 00:04:39,870
in the following sense is that what we
find is basically the people's intuitions

80
00:04:39,990 --> 00:04:44,330
reflect the social world that they
come from. So people that, uh,

81
00:04:44,340 --> 00:04:48,000
experience like the world is a
trustworthy place where their interaction

82
00:04:48,001 --> 00:04:49,680
partners are nice to
them and stuff like that.

83
00:04:49,860 --> 00:04:52,800
Their automatic reaction
is to be cooperative.

84
00:04:53,040 --> 00:04:57,390
People that come from a nasty world,
their automatic response is to be selfish,

85
00:04:57,990 --> 00:05:01,650
but then no matter where you start,
the more you think about it,

86
00:05:01,651 --> 00:05:02,820
the more selfish you get.

87
00:05:03,210 --> 00:05:08,010
So it's like deliberation undermines
whatever your automatic sort of heuristic

88
00:05:08,011 --> 00:05:08,844
responses.

89
00:05:09,500 --> 00:05:11,420
So almost in some ways pay,

90
00:05:11,421 --> 00:05:14,900
perhaps he should deliberate less when
we think about cooperating with other

91
00:05:14,901 --> 00:05:15,734
people.

92
00:05:15,800 --> 00:05:18,380
Right. Which is a weird thing to be
saying because then, you know, in general,

93
00:05:18,381 --> 00:05:22,230
I'm a big fan of deliberation.
Uh, you know, uh,

94
00:05:22,470 --> 00:05:26,010
and I don't really think that in general
the world would be better off and

95
00:05:26,011 --> 00:05:29,780
everybody deliberated less. Uh,
but that is the sort of, um,

96
00:05:31,400 --> 00:05:35,720
basically the result of what we find is
that in these games you get better sort

97
00:05:35,721 --> 00:05:37,280
of social.

98
00:05:37,350 --> 00:05:40,460
So we're more socially beneficial
behavior when people deliberate less.

99
00:05:40,760 --> 00:05:43,850
It's fascinating stuff. I mean,
you've also done some work,

100
00:05:43,880 --> 00:05:48,110
I'm discovering that rewarding
interactions are actually as effective in

101
00:05:48,111 --> 00:05:52,310
promoting group cooperation as punishment
and that these positive or rewarding

102
00:05:52,311 --> 00:05:54,980
interactions may even
lead to better outcomes.

103
00:05:55,280 --> 00:05:58,370
And so could you just tell us a little
bit about this because it's really

104
00:05:58,371 --> 00:06:03,050
fascinating and also one of these results
that might be not what people exactly

105
00:06:03,051 --> 00:06:03,884
expect.

106
00:06:04,450 --> 00:06:08,590
Yeah. So the backdrop for that
work was that there's been,

107
00:06:08,660 --> 00:06:10,780
in terms of the literature on cooperation,

108
00:06:10,781 --> 00:06:14,920
on the evolution of cooperation and stuff
like that in the last maybe 10 or 15

109
00:06:14,921 --> 00:06:15,251
years,

110
00:06:15,251 --> 00:06:19,450
there's been this huge focus on basically
the reason people cooperate with each

111
00:06:19,451 --> 00:06:23,680
other is that if they don't,
people will pay costs to punish others.

112
00:06:23,920 --> 00:06:27,910
And so you should be a nice person or
like a well behaved person because if not,

113
00:06:27,970 --> 00:06:30,640
other people will like really
make you suffer for it.

114
00:06:31,180 --> 00:06:34,900
And like when I look around
the world that I live in,

115
00:06:35,140 --> 00:06:39,460
I don't really see like a lot of like
spiteful or costly punishment where people

116
00:06:39,461 --> 00:06:44,050
are going out of their way to impose
costs on other people as like a main

117
00:06:44,051 --> 00:06:47,770
motivating force of most of
my social interaction. Uh,

118
00:06:51,410 --> 00:06:54,400
um, but, uh,

119
00:06:54,490 --> 00:06:58,690
and so what we were thinking
is that, um, you know,

120
00:06:58,691 --> 00:07:02,050
I want to think about what do
I actually experienced in life.

121
00:07:02,140 --> 00:07:03,820
It's not to say that people are just,

122
00:07:03,821 --> 00:07:05,920
you can do whatever you want
and there's no consequences.

123
00:07:06,160 --> 00:07:10,070
But it's that the main like vehicle for,
uh,

124
00:07:10,090 --> 00:07:13,540
incentivizing good behavior,
I feel like is denial,

125
00:07:13,541 --> 00:07:17,980
a future reward? And it's, we're
not talking about monetary rewards,

126
00:07:18,220 --> 00:07:22,470
but if you think about things like,
um, okay, so the, the sort of the,

127
00:07:22,570 --> 00:07:26,750
the little joke anecdote or did I tell
about this is, so if you think about, uh,

128
00:07:26,930 --> 00:07:30,610
the sort of the cooperation problem being
a bunch of housemates living together

129
00:07:30,611 --> 00:07:35,250
and like doing their dishes or not doing
their dishes as the case may be, uh,

130
00:07:35,620 --> 00:07:39,070
that if it's just you're doing
dishes or you're not doing dishes,

131
00:07:39,071 --> 00:07:41,800
as soon as one person stops doing their
dishes, everybody else is like, well,

132
00:07:41,801 --> 00:07:44,530
screw it. I don't want to be the
person that's still doing my dishes.

133
00:07:45,730 --> 00:07:50,730
And so like what the main body of work
in this field says is your solution

134
00:07:51,641 --> 00:07:55,540
should be, is like when your
housemate doesn't do her dishes,

135
00:07:55,660 --> 00:07:59,500
you should go into a room and smash a
laptop with a baseball bat and then she'll

136
00:07:59,501 --> 00:08:02,530
do her dishes the next
time. And we're like, well,

137
00:08:02,860 --> 00:08:07,300
maybe not a great idea,
but like what you can do instead.

138
00:08:07,301 --> 00:08:08,890
That's that.
Is this denial,

139
00:08:08,891 --> 00:08:12,010
a future reward kind of thing is like
next time say she comes and is like, hey,

140
00:08:12,011 --> 00:08:14,380
you know, like I need help
with my of this math problem.

141
00:08:14,381 --> 00:08:17,740
Can you help me with that? Can I borrow
your computer? Or something like that.

142
00:08:17,741 --> 00:08:20,770
You're like, sorry, why don't you do
your dishes and then we'll talk about it.

143
00:08:21,700 --> 00:08:23,560
So there's still,
it's still incentives,

144
00:08:23,561 --> 00:08:27,220
but it's about rather than going out of
your way to do something destructive to

145
00:08:27,221 --> 00:08:28,420
the person saying,
look,

146
00:08:28,421 --> 00:08:32,200
I'm only willing to help you in the
context of our pairwise relationship if

147
00:08:32,201 --> 00:08:33,970
you're being a good citizen at the group.

148
00:08:34,020 --> 00:08:34,853
Well,

149
00:08:34,920 --> 00:08:38,700
so that's nice in a way that it sort of
motivates people to be better citizens.

150
00:08:38,701 --> 00:08:41,550
That they're more likely to get
cooperation from others if they behave

151
00:08:41,551 --> 00:08:43,770
positively in a social context.

152
00:08:44,590 --> 00:08:45,370
Exactly.

153
00:08:45,370 --> 00:08:49,540
And so the way that you can explain a
lot of group level cooperation is by

154
00:08:49,541 --> 00:08:52,660
remembering that the group interaction
doesn't happen in a vacuum,

155
00:08:52,900 --> 00:08:57,150
but it happens super imposed on this
network. Personal relationships.

156
00:08:57,390 --> 00:09:00,810
Excellent. So I want to ask
you a little bit, I mean,

157
00:09:01,020 --> 00:09:04,220
what role do you think
emotion can play here? Um,

158
00:09:04,330 --> 00:09:06,450
and thinking about some of the
stuff you've just talked about.

159
00:09:07,460 --> 00:09:08,650
I think that,
uh,

160
00:09:09,010 --> 00:09:13,480
a lot of what's going on with
the sort of automatic intuitive,

161
00:09:13,481 --> 00:09:18,010
cooperative response is presumably,
uh, driven by emotion and affect.

162
00:09:18,340 --> 00:09:20,930
Um, and so I think that that, uh,

163
00:09:21,580 --> 00:09:24,850
emotions are probably a good,
um,

164
00:09:24,880 --> 00:09:28,130
sort of proxy or tool for, for, uh,

165
00:09:28,510 --> 00:09:31,630
communicating sort of general features
of the situation you're in and sort of

166
00:09:31,631 --> 00:09:36,490
guiding your, uh, a little
bit more generalized responses
rather than, you know,

167
00:09:36,491 --> 00:09:40,300
so if you think about this
in like a dual process, um,

168
00:09:40,360 --> 00:09:43,690
respective of like these
automatic sort of generalized,

169
00:09:43,691 --> 00:09:46,750
not very specific processes and then,
uh,

170
00:09:46,810 --> 00:09:50,440
that are fast and then you wouldn't have
your slow controlled reason. Thank you.

171
00:09:50,441 --> 00:09:53,830
Say, Oh, well, in this exact situation,
like what exactly should I be doing?

172
00:09:54,220 --> 00:09:58,080
So I would see probably
a lot of emotions as a,

173
00:09:58,090 --> 00:10:01,840
some of the somewhat more generalized
agents. I mean, it's not, that's not,

174
00:10:01,900 --> 00:10:03,670
it's not to say that
they're completely general,

175
00:10:03,820 --> 00:10:05,680
but that they respond to cues that aren't,

176
00:10:05,681 --> 00:10:08,200
things that are like very
deliberation oriented.

177
00:10:08,650 --> 00:10:09,281
Yeah,
that'll be fun.

178
00:10:09,281 --> 00:10:12,210
I know you and I've had conversations
thinking about the role that maybe

179
00:10:12,220 --> 00:10:14,890
positive emotions play
versus negative emotions.

180
00:10:14,891 --> 00:10:19,630
And it seems like this is a wide open
territory to try to look into. Definitely.

181
00:10:19,950 --> 00:10:21,970
So, you know, I'm thinking about this,

182
00:10:21,971 --> 00:10:26,260
this is one of the other questions
about your research is that, you know,

183
00:10:26,290 --> 00:10:29,080
this field of emotion and
cooperation is really exciting.

184
00:10:29,110 --> 00:10:31,870
It's new and it's emerging.
And as part of that,

185
00:10:31,871 --> 00:10:36,700
there's just a lot of unanswered
questions. Right. Um, and so,

186
00:10:36,701 --> 00:10:38,560
you know, as an expert in cooperation, I,

187
00:10:38,770 --> 00:10:43,750
I wonder what you think are
the most promising candidates
for emotion or related

188
00:10:43,751 --> 00:10:48,751
processes to emotion that we could try
to really leverage to better understand

189
00:10:48,911 --> 00:10:49,840
cooperation.

190
00:10:50,900 --> 00:10:53,600
Yeah. So as you said, I think
there's a ton of candidates.

191
00:10:53,690 --> 00:10:58,010
So the first thing that comes most
obviously to mind is empathy. Um,

192
00:10:58,011 --> 00:10:59,780
that you know, that, Eh, if you,

193
00:10:59,781 --> 00:11:01,610
the more empathic you feel
towards the other person,

194
00:11:01,611 --> 00:11:04,070
the more inclined you are going to be,
uh,

195
00:11:04,160 --> 00:11:07,610
to be cooperative from just like a
sort of, I don't know the right word,

196
00:11:07,611 --> 00:11:10,760
but like a very direct kind of basic
thing. Okay. I unempathic I'll do it.

197
00:11:11,210 --> 00:11:13,070
Then things that seem like,
uh,

198
00:11:13,240 --> 00:11:18,110
it's maybe important for a little bit
more second order thing is our emotions.

199
00:11:18,111 --> 00:11:20,840
Like pride. Like, you know,
you want to be a good person,

200
00:11:20,841 --> 00:11:22,160
you feel sort of proud when,
you know,

201
00:11:22,161 --> 00:11:24,740
when you help a stranger then you feel
actually, yeah, I really did write thing.

202
00:11:24,741 --> 00:11:28,760
Like, that's good. You know, I feel good
about that. Or shame, you know, as the,

203
00:11:28,761 --> 00:11:30,760
as the converse of that.
Um,

204
00:11:30,770 --> 00:11:34,980
and then also when you get
into the domain of, um,

205
00:11:35,840 --> 00:11:38,230
the reward and punishment kind
of stuff. So like, you know,

206
00:11:38,240 --> 00:11:39,470
provisioning incentives,

207
00:11:39,471 --> 00:11:42,280
it's trying to incentivize other
people to be cooperative. Uh,

208
00:11:42,380 --> 00:11:46,760
presumably anger will be an important
motivation there. And also discussed.

209
00:11:46,910 --> 00:11:50,090
I imagine it's something that
could be quite important.

210
00:11:50,410 --> 00:11:52,900
Well, it will be really exciting to
see what some of these discoveries are.

211
00:11:52,901 --> 00:11:56,650
I'm that you'll V sphere heading
in the years to come. Um,

212
00:11:57,280 --> 00:12:01,060
so when you think about some of these
potential discoveries with the motion and

213
00:12:01,061 --> 00:12:04,210
cooperation, you know, I know
that some people might wonder,

214
00:12:04,211 --> 00:12:09,211
well is the role of emotion in cooperation
something that is a human universal

215
00:12:09,371 --> 00:12:14,230
kind of cross culturally same everywhere
or might there be some important

216
00:12:14,231 --> 00:12:16,720
sources of cross cultural variation here?

217
00:12:17,610 --> 00:12:18,371
Yeah,
so I think that's,

218
00:12:18,371 --> 00:12:22,620
it's such a fascinating question and it's
a direction that I'm really interested

219
00:12:22,621 --> 00:12:25,260
in exploring. Um, cause
I think that there's,

220
00:12:25,290 --> 00:12:29,910
there's good documentation
of cross cultural differences
in behavior in terms of

221
00:12:29,911 --> 00:12:32,580
cooperation. And so I, what I think is,

222
00:12:33,060 --> 00:12:38,060
is really interesting to me is to what
extent are those differences differences

223
00:12:38,491 --> 00:12:40,170
because um,

224
00:12:40,680 --> 00:12:45,680
this sort of mechanisms for getting
people to follow norms are the same.

225
00:12:46,441 --> 00:12:50,060
And what varies across cultures are
the norms. So it could be, you know,

226
00:12:50,070 --> 00:12:53,790
people feel proud when they follow the
norm. They feel shame when they don't,

227
00:12:53,820 --> 00:12:55,590
they get angered when other people don't.

228
00:12:55,770 --> 00:12:58,960
And it's basically when people don't
behave appropriately or do behave

229
00:12:58,961 --> 00:12:59,550
appropriately.

230
00:12:59,550 --> 00:13:03,330
And then what varies across cultures is
what's the definition of appropriate or

231
00:13:03,331 --> 00:13:05,750
could it be that there are
actually deeper level, uh,

232
00:13:05,910 --> 00:13:09,870
differences and that like doing
appropriate behavior triggers,

233
00:13:09,871 --> 00:13:12,190
different emotional responses
in different places.

234
00:13:12,191 --> 00:13:15,180
And I don't think anybody knows and I
think it would be very interesting to find

235
00:13:15,181 --> 00:13:16,014
out.

236
00:13:16,150 --> 00:13:20,140
So when you think about the face of
the future and think about emotion or

237
00:13:20,860 --> 00:13:23,800
emotion in cooperation,
what do you see in store for the future?

238
00:13:24,860 --> 00:13:27,560
Well, I think, like you were
saying a little bit earlier,

239
00:13:27,561 --> 00:13:30,170
it's really like the wild
west that they're, you know,

240
00:13:30,171 --> 00:13:34,520
there's like 10 million different awesome
things to do. Uh, so what, I mean,

241
00:13:34,521 --> 00:13:37,760
the main thing that I see is
lots of cool papers coming out,

242
00:13:38,240 --> 00:13:43,130
trying to actually pin down specific
relationships between particular emotions

243
00:13:43,131 --> 00:13:48,080
or particular combinations of emotions
and behavior outputs in, in this context.

244
00:13:48,290 --> 00:13:51,200
Yeah. And so what advice
would you have for students?

245
00:13:51,201 --> 00:13:54,800
A lot of them will be spearheading
this wild west in the future.

246
00:13:55,100 --> 00:13:57,220
What advice would you have
for these students? You know,

247
00:13:57,290 --> 00:14:00,800
as they try to embark and studying
some of these questions that relate to

248
00:14:00,810 --> 00:14:01,643
emotion?

249
00:14:03,720 --> 00:14:04,553
MMM.

250
00:14:05,050 --> 00:14:07,720
Yeah. Okay. What advice?
It's a good question. Uh,

251
00:14:08,670 --> 00:14:09,503
okay.

252
00:14:09,580 --> 00:14:10,413
I think that

253
00:14:12,150 --> 00:14:15,390
one thing that I would say
is particularly important,

254
00:14:15,660 --> 00:14:17,040
at least from my perspective.

255
00:14:17,041 --> 00:14:21,000
So I really like using economic games
and I think that like economics offers a

256
00:14:21,001 --> 00:14:24,690
lot of really good methodologies for
studying these kinds of questions.

257
00:14:24,960 --> 00:14:25,890
And in particular,

258
00:14:25,891 --> 00:14:29,310
I think it could be really fruitful for
people coming from psychology and from a

259
00:14:29,311 --> 00:14:33,970
sort of an ocean perspective, uh,
that you can use these approaches, uh,

260
00:14:33,980 --> 00:14:38,280
like a methodologies from economics in
combination with ways of thinking about

261
00:14:38,281 --> 00:14:42,270
things and approaches from psychology
to get these really sort of productive,

262
00:14:42,271 --> 00:14:46,440
synergistic interactions in a way that
people that are just doing econ would

263
00:14:46,441 --> 00:14:49,050
never think to ask about
emotions in these kinds of way.

264
00:14:49,290 --> 00:14:52,670
People that were just doing
emotion research using the
econ wouldn't have these

265
00:14:52,671 --> 00:14:53,510
cool tools.

266
00:14:53,660 --> 00:14:58,280
So I think that this is really a fruitful
place for integrating approaches.

267
00:14:58,550 --> 00:15:01,880
And what I would say in pursuit of
that is if you're going to do it,

268
00:15:02,030 --> 00:15:06,830
people should really try and understand
some behavioral economics. Um, and like,

269
00:15:06,950 --> 00:15:08,070
I think there's a lot of people using it,

270
00:15:08,071 --> 00:15:11,420
economic games without
really understanding the way
economists think about them.

271
00:15:11,720 --> 00:15:14,810
And it would be worth understanding that
because there's some interesting stuff

272
00:15:14,811 --> 00:15:15,400
there.

273
00:15:15,400 --> 00:15:17,530
Well,
thanks so much for speaking with us today.

274
00:15:17,560 --> 00:15:22,560
This concludes our expert in emotion
series with Dr. David ran from Yale

275
00:15:22,630 --> 00:15:22,630
University.

