WEBVTT

1
00:00:00.090 --> 00:00:01.740
Welcome back to human emotion.

2
00:00:01.990 --> 00:00:05.790
Today we're going to be talking about the way in which emotions may actually

3
00:00:05.791 --> 00:00:09.150
influence our sense of what is right from what is wrong.

4
00:00:09.360 --> 00:00:12.510
So this is the field referred to as emotion and morality.

5
00:00:12.900 --> 00:00:17.190
Now in what way could emotions really shape our sense of what's morally right

6
00:00:17.191 --> 00:00:21.720
and morally wrong?
Well,
if we think about emotions and morality,

7
00:00:21.900 --> 00:00:25.680
some recent work that's actually emerged here at Yale University by researchers,

8
00:00:25.681 --> 00:00:27.780
Kylie Hamlin,
Paul Bloom,

9
00:00:27.781 --> 00:00:31.410
and Karen when has showed that young babies as young as the one you can see

10
00:00:31.411 --> 00:00:36.411
right here in the photo have this almost innate sense of right from wrong and a

11
00:00:36.421 --> 00:00:41.421
preference for morally right and good characters over morally wrong characters.

12
00:00:41.730 --> 00:00:46.050
So it seems that were almost born with this tendency to discriminate between

13
00:00:46.080 --> 00:00:51.030
right and wrong and actually prefer,
you know,
the morally right or good person.

14
00:00:52.590 --> 00:00:57.120
Moreover,
moral judgments are a pervasive part of humanity.

15
00:00:57.360 --> 00:01:01.880
We can see a variety of different religious doctrines that really outline,
um,

16
00:01:02.100 --> 00:01:03.420
the ways in which,
you know,

17
00:01:03.421 --> 00:01:07.680
emotions and feelings essentially play a role in our moral judgements.

18
00:01:07.681 --> 00:01:12.420
So in Buddhism,
you know,
there's these doctrines that sit suggest to hurt,

19
00:01:12.421 --> 00:01:16.650
not others with with that which pains yourself.
Um,

20
00:01:16.651 --> 00:01:20.970
were Christian based traditions say all things whatsoever he would,

21
00:01:20.971 --> 00:01:24.030
that men should do to you.
Do you,
do you even?

22
00:01:24.031 --> 00:01:26.700
So to them for this is the law and the prophets.

23
00:01:27.090 --> 00:01:30.630
And finally on the right you can see here within Muslim traditions,

24
00:01:30.840 --> 00:01:35.700
no one of you is a believer until he desires for his brother that which he

25
00:01:35.701 --> 00:01:37.260
desires for himself.

26
00:01:37.500 --> 00:01:41.400
And so what you want to see here is that across different religious traditions,

27
00:01:41.580 --> 00:01:43.890
not only our moral judgments pervasive,

28
00:01:44.040 --> 00:01:48.450
but that part of what underlies these moral scruples are emotions themselves.

29
00:01:48.690 --> 00:01:52.770
Seeing emotions like pain or seeing emotions here,
like desire.

30
00:01:52.920 --> 00:01:55.410
So it seems that what my actually underlie,

31
00:01:55.411 --> 00:02:00.411
our very sense of right and wrong are our feelings or emotions towards actions

32
00:02:00.481 --> 00:02:03.180
of other people as well as actions of ourselves.

33
00:02:03.780 --> 00:02:06.840
So this leads us to think that,
you know,

34
00:02:07.290 --> 00:02:12.290
emotions seem to play this orchestra role in guiding our moral beliefs in our

35
00:02:12.391 --> 00:02:15.690
moral judgments.
But in another words,
you know,

36
00:02:15.691 --> 00:02:18.750
can we be more without emotions?
In other words,

37
00:02:18.751 --> 00:02:23.751
how much do emotions essentially influence our sense of what is right from what

38
00:02:24.121 --> 00:02:29.121
is wrong and how much can emotions also influence morally relevant behavior?

39
00:02:29.400 --> 00:02:33.230
So both for the good as well as the bad.
Um,

40
00:02:33.300 --> 00:02:35.910
so these are some of the questions we're going to be looking at today where

41
00:02:35.911 --> 00:02:39.510
we're going to be really looking at,
as you can see here,
the good from the bad.

42
00:02:39.690 --> 00:02:43.800
And to what extent emotions play a role in shaping who we are.

43
00:02:43.800 --> 00:02:46.680
Can they actually shape our moral character?

44
00:02:48.630 --> 00:02:50.460
Some people may say then,
well,

45
00:02:50.461 --> 00:02:55.440
if emotions play such a central role in shaping the goodness and badness of who

46
00:02:55.441 --> 00:02:58.020
we are and the goodness and badness of our behaviors,

47
00:02:58.350 --> 00:03:03.350
then our really just elaborated feelings that we have some feeling of desire and

48
00:03:04.961 --> 00:03:09.460
we put these kind of cognitive and linguistic layers on top of it and turn that

49
00:03:09.461 --> 00:03:14.110
into a moral judgment.
Can there be more to morality than just emotions?

50
00:03:14.470 --> 00:03:19.090
So today we're going to be looking at things such as the sense in which we feel

51
00:03:19.091 --> 00:03:22.420
bad for people who suffer.
We think that that's morally wrong.

52
00:03:22.840 --> 00:03:27.040
We also feel a sense of loyalty to friends and family,
right?

53
00:03:27.220 --> 00:03:31.150
We think there's a sense of being morally good if we're loyal to those close

54
00:03:31.151 --> 00:03:31.990
around us.

55
00:03:33.280 --> 00:03:38.280
We also feel guilty about doing things that we consider morally wrong or bad.

56
00:03:38.830 --> 00:03:42.370
So what we're seeing here is emotions are feeling compassion for those who

57
00:03:42.371 --> 00:03:47.371
suffer loyalty or love towards family and friends and guilt about doing what we

58
00:03:47.951 --> 00:03:50.470
consider to be morally wrong.

59
00:03:50.471 --> 00:03:54.960
Things that go against our own values like pigging out at a kitchen,
you know,

60
00:03:55.060 --> 00:03:58.930
fridge late at night.
So then a question comes up,

61
00:03:59.080 --> 00:04:03.190
do emotions,
can they actually causally make us good people?

62
00:04:03.670 --> 00:04:07.090
And if so,
what kinds of emotions make us good people?

63
00:04:07.870 --> 00:04:11.470
By contrast,
can emotions also make us bad?

64
00:04:11.740 --> 00:04:15.850
And to what extent can they even help explain some of the most heinous and evil

65
00:04:15.851 --> 00:04:18.430
behavior known to mankind?
Can,

66
00:04:18.431 --> 00:04:22.450
did emotions really play a role in shaping some of the altruistic behavior,

67
00:04:22.451 --> 00:04:24.760
you know of mother Theresa and some of the,
you know,

68
00:04:24.761 --> 00:04:27.370
just morally repugnant behavior of Hitler.

69
00:04:28.600 --> 00:04:31.150
So here's where I want to guide us today.

70
00:04:31.151 --> 00:04:34.300
So this is our segment on emotions and morality.

71
00:04:34.480 --> 00:04:38.230
And this is going to be our introduction to what are called moral emotions.

72
00:04:38.470 --> 00:04:42.550
And this is our part one of a three part series on moral emotions.

73
00:04:43.060 --> 00:04:47.470
So what we're gonna do today is I'm first going to introduce you to what our

74
00:04:47.471 --> 00:04:49.750
moral emotions.
Second,

75
00:04:49.751 --> 00:04:53.020
we're going to look at the ways in which researchers have tried to organize

76
00:04:53.021 --> 00:04:56.680
different families or classes of moral emotions.

77
00:04:57.010 --> 00:05:00.310
Then we'll actually talk about the extent to which,
you know,

78
00:05:00.311 --> 00:05:04.810
contemporary neuroimaging research actually suggests that our brain may actually

79
00:05:04.811 --> 00:05:08.860
play a critical role in determining our more reactions to different kinds of

80
00:05:08.861 --> 00:05:13.180
behaviors.
Finally,
we'll conclude with our normal tea takeaway questions.

81
00:05:14.020 --> 00:05:18.970
And we'll have a couple expert interviews today with two renowned experts on

82
00:05:18.971 --> 00:05:20.470
morality and emotion.

83
00:05:21.520 --> 00:05:25.900
So let's start today by first introducing this concept of moral emotions.

84
00:05:25.901 --> 00:05:28.540
Sort of what do we mean by this term?

85
00:05:28.541 --> 00:05:32.110
Are these a specific class or type of emotions?

86
00:05:32.890 --> 00:05:37.480
So one of the sort of fathers of moral emotions,
um,
is Jonathan Height.

87
00:05:37.720 --> 00:05:42.610
So he seen as the authoritative figure here and has wrote several comprehensive

88
00:05:42.611 --> 00:05:46.660
pieces about what exactly moral emotions are.
This includes,

89
00:05:46.661 --> 00:05:47.740
as you can see here,

90
00:05:47.920 --> 00:05:52.240
his writings on the moral emotions as well as what he called the new synthesis

91
00:05:52.241 --> 00:05:56.500
in moral psychology.
And John Hype said,

92
00:05:56.810 --> 00:06:01.220
morality is therefore like the temple on the hill of human nature.

93
00:06:01.670 --> 00:06:04.040
It is our more sacred attributes,

94
00:06:04.280 --> 00:06:09.280
a trait that is often said to separate us from other animals and bring us closer

95
00:06:09.381 --> 00:06:12.380
to God.
So when describing moral emotions,

96
00:06:12.381 --> 00:06:17.381
he highlights the fundamental importance to all humans of morality and moral

97
00:06:18.231 --> 00:06:22.610
virtues.
So what is a moral emotion then?

98
00:06:22.640 --> 00:06:26.600
So recent theorists have suggested that emotions,
as we've seen earlier,

99
00:06:26.601 --> 00:06:30.410
play a huge role in morality.
They shape what's wrong and what's right.

100
00:06:30.890 --> 00:06:35.600
And in fact,
they may even be in charge of this temple of morality,

101
00:06:35.780 --> 00:06:38.570
as alluded to in the quote earlier by Jonathan Height.

102
00:06:38.930 --> 00:06:42.170
And if we look at what a definition of moral emotions might be,

103
00:06:42.380 --> 00:06:47.300
we see here that the moral emotions are those emotions that are linked to the

104
00:06:47.301 --> 00:06:52.301
interest or welfare of either society as a whole or people other than the

105
00:06:52.791 --> 00:06:53.624
agents.

106
00:06:53.780 --> 00:06:58.460
And in particular height identified what he thought of his to prototypical

107
00:06:58.461 --> 00:07:03.260
features of a moral emotion.
The first,
he called disinterested,

108
00:07:03.261 --> 00:07:04.100
illicit hers.

109
00:07:04.610 --> 00:07:09.290
So some emotions occur when good or bad things happen to oneself,

110
00:07:09.470 --> 00:07:11.150
like fear or happiness,

111
00:07:11.390 --> 00:07:15.830
and they require the self to be related to something outside of oneself.

112
00:07:15.920 --> 00:07:20.090
Maybe you're happy that your friend got engaged or you're fearful.

113
00:07:20.091 --> 00:07:24.410
You know that someone that you don't like,
um,
maybe after you,
right?

114
00:07:24.590 --> 00:07:28.790
But other emotions are triggered when your own self has nothing at stake,
right?

115
00:07:28.791 --> 00:07:29.750
And the triggering event,

116
00:07:30.020 --> 00:07:34.580
like seeing photos here of a starving child or an abused animal,
right?

117
00:07:34.870 --> 00:07:35.190
Um,

118
00:07:35.190 --> 00:07:39.260
and the more and emotion can be triggered by what's called these disinterested

119
00:07:39.360 --> 00:07:44.000
Alyssa sitters.
So things in which the self has nothing at personal stake,

120
00:07:44.240 --> 00:07:48.590
the more it can be considered a prototypical emotion response.

121
00:07:48.860 --> 00:07:53.860
So that's the first feature of a moral emotion as proposed by Jonathan Height.

122
00:07:54.560 --> 00:07:56.810
The second is what he referred to

123
00:07:58.310 --> 00:08:01.160
as a pro social action tendency.

124
00:08:01.400 --> 00:08:05.960
So what's really unique about moral emotions is that they motivate a very

125
00:08:05.961 --> 00:08:09.550
specific kind of behavior or action tendency.
Um,

126
00:08:09.610 --> 00:08:13.840
but it's not to fight or it's not to flee or it's not to rest.
Instead,

127
00:08:13.860 --> 00:08:18.710
moral emotions are associated with action tendencies that are really geared at

128
00:08:18.711 --> 00:08:23.270
benefiting other people or trying to uphold the general social order.

129
00:08:23.450 --> 00:08:28.130
So this is what's referred to as these pro social action tendencies that are

130
00:08:28.131 --> 00:08:31.340
part and parcel of what a moral emotion is.

131
00:08:32.570 --> 00:08:37.570
Now here what you see is a figure that has been illustrated by John Height that

132
00:08:37.791 --> 00:08:42.080
really tries to depict two of these criteria we just talked about in two

133
00:08:42.081 --> 00:08:45.140
dimensional space.
You can see the prosocial,
um,

134
00:08:45.230 --> 00:08:48.770
nature of the action tendency on the left and on the bottom right,

135
00:08:48.771 --> 00:08:51.500
the disinterestedness of the solicitors,
right?

136
00:08:51.770 --> 00:08:55.680
So the prototypical moral emotions are what you're going to see in the top right

137
00:08:55.681 --> 00:08:59.880
corner.
So here we see anger,
elevation,
guilt,

138
00:08:59.881 --> 00:09:00.930
and compassion.

139
00:09:01.110 --> 00:09:05.040
So what it's trying to do here is really distinguished between moral emotions,

140
00:09:05.041 --> 00:09:08.250
which are high on both of these domains,
pro sociality,

141
00:09:08.251 --> 00:09:09.510
as well as disinterestedness,

142
00:09:09.600 --> 00:09:14.220
no solicitors from other kinds of emotions like sadness and pride and fear.

143
00:09:15.990 --> 00:09:19.680
So we've now briefly talked about what a moral emotion is.

144
00:09:19.681 --> 00:09:23.700
And we've talked about sort of the two prototypical features of a moral emotion.

145
00:09:24.060 --> 00:09:28.350
So now I want us to delve in more deeply to what's thought of as the family or

146
00:09:28.351 --> 00:09:33.330
classification system of the variety of different kinds of moral emotions out

147
00:09:33.331 --> 00:09:33.960
there.

148
00:09:33.960 --> 00:09:38.820
So now we're going to turn to this idea of moral emotion families as you can see

149
00:09:38.821 --> 00:09:39.654
right here.

150
00:09:39.720 --> 00:09:44.720
So what are these emotions and how can we categorize them according to different

151
00:09:44.941 --> 00:09:49.941
categories of moral emotions or different ways in which they influence how we

152
00:09:50.131 --> 00:09:54.210
think of what is right versus how we distinguish what is wrong in the world

153
00:09:54.211 --> 00:09:55.044
around us.

154
00:09:55.740 --> 00:10:00.060
So this is what's considered the moral emotion families.

155
00:10:00.061 --> 00:10:03.930
And as you can see here,
it's broken up into two larger families.

156
00:10:04.260 --> 00:10:07.920
The other condemning emotions,
including contempt,
anger,

157
00:10:07.921 --> 00:10:11.240
and discussed as well as self conscious emotions,
uh,

158
00:10:11.310 --> 00:10:14.700
that we talked about earlier in class,
including shame,
embarrassment and guilt.

159
00:10:15.210 --> 00:10:18.240
And this is then also thought of as two smaller families,

160
00:10:18.241 --> 00:10:22.500
which include other suffering emotions like compassion and other praising

161
00:10:22.501 --> 00:10:24.810
emotions like gratitude and elevation.

162
00:10:25.020 --> 00:10:28.290
So we're going to start today with these two large families here.

163
00:10:28.560 --> 00:10:32.520
And in particular we're going to start here with the other condemning emotions.

164
00:10:32.760 --> 00:10:35.850
So this is content anger and discussed.

165
00:10:36.450 --> 00:10:40.440
So this is what's called,
in other words,
the cad triad,
right?

166
00:10:40.650 --> 00:10:45.650
So here John Height argued that we live in this rich world of reputations of

167
00:10:45.781 --> 00:10:50.340
third party concerns.
He said,
we care that what other people do to each other,

168
00:10:50.400 --> 00:10:50.970
right?

169
00:10:50.970 --> 00:10:55.290
And we readily develop negative feelings towards people we may have never even

170
00:10:55.320 --> 00:10:56.880
actually met,
right?

171
00:10:57.420 --> 00:11:01.770
And it's these negative feelings towards characters that we may not even know

172
00:11:02.010 --> 00:11:06.630
that unites this family of what's called the aether condemning moral emotions.

173
00:11:06.840 --> 00:11:11.580
So this includes again,
contempt,
anger and disgust,
or the CA d triad.

174
00:11:11.760 --> 00:11:14.880
And I'll walk you through each of these three according to the solicitor and

175
00:11:14.881 --> 00:11:18.990
their associated action tendency.
So if we start with anger,

176
00:11:18.991 --> 00:11:23.160
what you can see here is that anger,
which we talked about earlier in class,

177
00:11:23.161 --> 00:11:27.210
is elicited in response to goal blockage or frustration.

178
00:11:27.211 --> 00:11:30.210
So you're trying to pursue a goal and something gets in the way.

179
00:11:31.290 --> 00:11:34.860
It's also associated in response to unjustified insults,
betrayal,

180
00:11:34.861 --> 00:11:36.360
trail and unfair treatment.

181
00:11:36.690 --> 00:11:41.070
The action tendency associated with anger as to be motivated to attack back,

182
00:11:41.071 --> 00:11:42.960
right?
To humiliate,

183
00:11:42.961 --> 00:11:47.961
to get revenge towards the person or thing that seems to be acting immorally.

184
00:11:48.420 --> 00:11:51.000
So we see that anger,
for example,
um,

185
00:11:51.001 --> 00:11:56.001
is a moral emotion in so far as it motivates us to readdress injustices and can

186
00:11:56.681 --> 00:12:01.390
be applied actually to the benefit of society in cases that include things like

187
00:12:01.391 --> 00:12:04.270
racism,
oppression or exploitation,

188
00:12:04.271 --> 00:12:07.630
that anger may actually motivate us to act against.

189
00:12:07.631 --> 00:12:09.370
The societal injustices

190
00:12:11.260 --> 00:12:15.460
discussed is really interesting too cause we see discussed expanding out into

191
00:12:15.461 --> 00:12:19.360
the social moral domain to repel things that are disgusted.

192
00:12:19.630 --> 00:12:20.770
So this makes sense.

193
00:12:20.771 --> 00:12:23.920
And as adaptive when we think of contaminated food for example,

194
00:12:24.280 --> 00:12:29.280
but fascinatingly discussed seems to also sort of permeate our moral judgments

195
00:12:29.321 --> 00:12:33.580
that have nothing whatsoever to do with like toxic food substances.

196
00:12:33.850 --> 00:12:34.631
So for example,

197
00:12:34.631 --> 00:12:38.230
we want little to do with things associated with Hitler for example.

198
00:12:38.470 --> 00:12:43.180
And little can be done,
you know,
to cleanse a sweater of,
of a hated person,
right?

199
00:12:43.480 --> 00:12:43.750
Um,

200
00:12:43.750 --> 00:12:48.750
so discuss is also prosocial as a moral emotion and that it motivates or

201
00:12:49.511 --> 00:12:53.950
triggers people to set up reward and punishment structures that really act as

202
00:12:53.951 --> 00:12:57.220
these strong deterrents to culturally inappropriate behaviors,

203
00:12:57.430 --> 00:13:01.390
especially those that involve the body.
Um,
so if we look here,

204
00:13:01.391 --> 00:13:04.100
we look at the illicit or have discussed,
we see,
um,

205
00:13:04.300 --> 00:13:07.120
two different kinds of discuss.
We see core discussed,

206
00:13:07.121 --> 00:13:11.170
which is this sort of basic emotion.
It's a rejection system,

207
00:13:11.410 --> 00:13:16.180
usually was food related and is really in response to food and trying to expel

208
00:13:16.181 --> 00:13:19.510
toxic food from the body.
But when we start to expand,

209
00:13:19.511 --> 00:13:22.840
discuss and sort of bring it into the domain of morality,

210
00:13:23.080 --> 00:13:26.890
we see that it's thought of as this guardian of the temple of the body.

211
00:13:27.130 --> 00:13:32.130
And it gets triggered by violation of cultural rules for how to use bodies.

212
00:13:32.201 --> 00:13:35.440
So when relating to drug use,
sexual behavior,
et cetera.

213
00:13:35.770 --> 00:13:40.180
And there's also the socio moral disgust for different kinds of social

214
00:13:40.181 --> 00:13:44.640
transgressions such as cruelty and behavior.
Um,

215
00:13:45.130 --> 00:13:47.920
and the action tendency as a motivation to avoid,

216
00:13:47.921 --> 00:13:52.921
expel or break off contact with offending entities and can include motivations

217
00:13:53.250 --> 00:13:56.020
even wash or try to purify oneself,

218
00:13:56.050 --> 00:13:59.230
remove contact from this entity.
Um,

219
00:13:59.260 --> 00:14:03.310
but what's sort of the dark side here of disgust and the way it relates to

220
00:14:03.311 --> 00:14:07.870
morality is that it can actually be really disturbing at times is it carries a

221
00:14:07.871 --> 00:14:09.250
sense of condemnation.

222
00:14:09.490 --> 00:14:13.600
So recent research by David Pizarro and colleagues has actually showed that

223
00:14:13.601 --> 00:14:18.601
discussed is a major underlying feature for the moral condemnation for example,

224
00:14:19.511 --> 00:14:22.360
of homosexuals.
So you can sort of take,

225
00:14:22.361 --> 00:14:27.361
discuss too far and have it even pervade moral judgements about other people's

226
00:14:27.490 --> 00:14:32.380
sexual lives and behaviors.
Um,
so disgust as a moral emotion.

227
00:14:32.381 --> 00:14:34.090
Let's talk about this a bit more.

228
00:14:34.420 --> 00:14:39.420
So there's different ways in which we can think of disgust as a moral emotion.

229
00:14:39.670 --> 00:14:42.970
So,
and Pizarro and colleagues outlined three claims.

230
00:14:42.971 --> 00:14:47.890
You see the first year that attempt to describe really how discussed and moral

231
00:14:47.891 --> 00:14:50.110
judgments are causally connected.
Um,

232
00:14:50.111 --> 00:14:55.111
so each of these makes a separate argument for how disgust and more morality are

233
00:14:55.341 --> 00:14:58.250
related.
So I'll walk you through each of them.
So the first is,

234
00:14:58.251 --> 00:15:03.251
you can see here is that disgust is a consequence of moral violations.

235
00:15:03.530 --> 00:15:08.290
So the claim here is that discussed is experienced as a result of appraising a

236
00:15:08.300 --> 00:15:10.340
mall that a moral violation has occurred.

237
00:15:10.670 --> 00:15:15.020
And in response to what are referred to as moral purity or taboo violations,

238
00:15:15.260 --> 00:15:17.330
some of this supportive evidence here is that,

239
00:15:17.550 --> 00:15:20.660
and this is a really interesting scenario that Paul Rosin and discussed,

240
00:15:20.900 --> 00:15:22.870
is that people actually experienced,

241
00:15:22.871 --> 00:15:27.170
discussed in response to what are thought of as very harmless moral violations.

242
00:15:27.440 --> 00:15:31.790
So he'll to pick these scenarios where he asks people to report how they feel if

243
00:15:31.791 --> 00:15:35.810
a family's dog were run over by a car and they pick up the,
you know,

244
00:15:35.811 --> 00:15:39.380
the pet dog and they decided to prepare and eat it for dinner.

245
00:15:39.620 --> 00:15:42.740
And most people will suggest that this is morally wrong,
right?

246
00:15:43.010 --> 00:15:45.610
And that may be driven by a sense of disgust.
Um,

247
00:15:45.720 --> 00:15:47.690
and then there's also another example of,
you know,

248
00:15:47.691 --> 00:15:52.250
having sex with a dead chicken,
right?
This is somewhat a harmless moral offense,

249
00:15:52.370 --> 00:15:54.770
but many people report this being morally wrong.

250
00:15:54.980 --> 00:15:58.190
And it's almost a sense of disgust that drives these judgments.

251
00:15:59.180 --> 00:16:04.180
The second is this claim that discussed is an amplifier of moral judgments.

252
00:16:06.230 --> 00:16:11.230
So here what we see is that discuss seems to make things even more wrong than

253
00:16:11.691 --> 00:16:15.260
they are,
right?
So you may think something's wrong and discuss,

254
00:16:15.261 --> 00:16:19.220
just ratchets up the ante and makes you feel like it's even more wrong than it

255
00:16:19.221 --> 00:16:20.090
already was.

256
00:16:20.360 --> 00:16:23.720
So some of the supportive evidence here is that when you induce feelings of

257
00:16:23.721 --> 00:16:24.890
disgust and people,

258
00:16:25.310 --> 00:16:29.270
they actually make harsher judgments of moral violations,
right?

259
00:16:29.480 --> 00:16:32.660
So you definitely don't want,
if you're,
you know,
going to court,
um,

260
00:16:32.930 --> 00:16:34.430
and they're determining your sentence,

261
00:16:34.431 --> 00:16:37.940
you don't want your judge to already be feeling disgusted when he comes in

262
00:16:38.150 --> 00:16:42.740
because he's more likely to make a more harsh moral judgment and likely to

263
00:16:42.770 --> 00:16:44.540
perhaps even punish you further.

264
00:16:44.690 --> 00:16:49.690
So discuss just seems to make us more harsh and condemning the moral behaviors

265
00:16:49.851 --> 00:16:54.530
of others.
And then finally,
as you can see,
or here at the bottom discussed,

266
00:16:54.531 --> 00:16:57.530
is also thought of as a moralizing emotion by itself.

267
00:16:57.740 --> 00:17:02.740
So the claim here is that morally neutral acts that are not inherently right or

268
00:17:02.811 --> 00:17:07.280
wrong can become moral in virtue of being perceived as disgusting.

269
00:17:07.700 --> 00:17:08.690
So in other words,

270
00:17:08.691 --> 00:17:13.100
the feeling of disgust itself is evidence that the act is morally wrong.

271
00:17:13.370 --> 00:17:17.450
And this leads to what we talked about earlier where people who feel disgust

272
00:17:17.451 --> 00:17:22.400
towards homosexual behavior that that discussed itself is actually what may

273
00:17:22.401 --> 00:17:27.401
explain their antigay moral attitudes such as being gay must be wrong.

274
00:17:29.330 --> 00:17:33.650
This also applies to incest and people even feel this way watching neutral
films.

275
00:17:33.830 --> 00:17:36.620
So the fact is just by feeling disgusted about something,

276
00:17:36.621 --> 00:17:40.660
it may lead you to then decide that it must be morally wrong.
Um,

277
00:17:40.670 --> 00:17:44.270
so these are three ways in which you know,
discussed,

278
00:17:44.300 --> 00:17:46.820
may be influencing our moral judgments.

279
00:17:48.020 --> 00:17:49.770
If you'd like to learn more about this,

280
00:17:49.771 --> 00:17:52.680
I highly recommend a Ted talk by David Pizarro,

281
00:17:52.681 --> 00:17:57.150
who's also one of our expert interviews later today where he talks about the way

282
00:17:57.151 --> 00:18:02.010
in which disgust influences many important and even,
uh,
you know,

283
00:18:02.310 --> 00:18:05.520
everyday political decisions we make in society,

284
00:18:05.790 --> 00:18:09.900
including our attitudes towards homosexuality,
for example.
So you,

285
00:18:09.940 --> 00:18:11.970
I highly recommend checking out his talk,

286
00:18:12.120 --> 00:18:16.560
which is entitled the strange politics of discussed and how it influences our

287
00:18:16.561 --> 00:18:20.370
moral life.
So finally,
um,

288
00:18:20.371 --> 00:18:22.980
what I want to turn to now is contempt in this triad.

289
00:18:23.220 --> 00:18:28.080
So contempt is elicited when you're looking down on someone or feeling morally

290
00:18:28.081 --> 00:18:32.400
superior to them and you're perceiving that that other person just doesn't seem

291
00:18:32.401 --> 00:18:36.270
to measure up to the right sort of moral standards,
right?

292
00:18:36.450 --> 00:18:40.200
So contempt is interesting in that it's almost this middle sibling of anger and

293
00:18:40.201 --> 00:18:43.140
disgust.
It seems to kind of fall right in the middle.

294
00:18:43.560 --> 00:18:47.310
And the tendency that's associated with contempt is to promote this what's

295
00:18:47.311 --> 00:18:51.390
called social cognitive change or you treat others with less,

296
00:18:51.391 --> 00:18:54.450
more respect and less consideration.

297
00:18:55.860 --> 00:19:00.860
So if we think about this cad triad hypothesis,
what really,
um,

298
00:19:00.900 --> 00:19:05.550
it states is that there's moral emotions,
um,
that are experienced,

299
00:19:05.600 --> 00:19:06.240
um,

300
00:19:06.240 --> 00:19:10.650
at the core in response to violations of three moral codes as follows.

301
00:19:10.651 --> 00:19:13.780
So in other words,
this,
the cad,
um,

302
00:19:13.950 --> 00:19:17.580
triad acts as guardians of different parts of the moral order.

303
00:19:17.850 --> 00:19:20.910
So the first you can see here is see for community,

304
00:19:21.060 --> 00:19:24.750
which includes disrespect and violations of duty or here Archi.

305
00:19:24.990 --> 00:19:29.990
And this is where the experience of contempt plays in second is autonomy.

306
00:19:30.870 --> 00:19:34.680
So when you perceive moral violations of rights and fairness,

307
00:19:35.150 --> 00:19:36.840
it's emotion of anger,
you know,

308
00:19:36.841 --> 00:19:40.350
sort of picks up and begins to play a role in moral judgments.

309
00:19:40.650 --> 00:19:42.840
And then finally the D is divinity.

310
00:19:43.020 --> 00:19:48.020
So here if there's potentially moral violations of physical purity such as food

311
00:19:48.421 --> 00:19:52.740
and sex taboos such in the case of homosexuality,
incest,
you know,

312
00:19:52.860 --> 00:19:55.590
having sex with the dead chicken.
As we talked about earlier,

313
00:19:55.860 --> 00:20:00.860
these emotions of disgust really seemed to play a role in shaping our assumption

314
00:20:01.201 --> 00:20:03.420
that there's been these important violations.

315
00:20:03.540 --> 00:20:06.540
So it's community autonomy and divinity.

316
00:20:07.680 --> 00:20:12.680
So now what I want to turn to is the second domain of these two large families

317
00:20:13.411 --> 00:20:16.080
of moral emotions.
And these are the self conscious emotions.

318
00:20:16.320 --> 00:20:18.390
So we've talked about shame earlier in class,

319
00:20:18.391 --> 00:20:22.530
which is elicited by a negative evaluation of oneself.
Um,

320
00:20:22.560 --> 00:20:24.930
the behavior includes a hunched over posture,

321
00:20:24.931 --> 00:20:29.040
retreating or hiding from a social group and a consequence in which are actually

322
00:20:29.041 --> 00:20:32.910
less likely to take corrective action.
You kind of step back in a way.

323
00:20:34.050 --> 00:20:36.780
There's also embarrassment,
which we've talked about earlier as well,

324
00:20:37.140 --> 00:20:41.340
associated with violating social conventions or norms and really motivates you

325
00:20:41.341 --> 00:20:43.170
to try to acknowledge mistake the mistake,

326
00:20:43.440 --> 00:20:46.440
remedy the social transgression and repair the relationship.

327
00:20:46.830 --> 00:20:50.500
And consequence actually promotes more sort of pro social actions like

328
00:20:50.501 --> 00:20:55.420
forgiveness,
laughter,
liking and trust.
And then finally,
guilt,

329
00:20:55.421 --> 00:20:58.300
which,
um,
we also talked about earlier.
But as a refresher,

330
00:20:58.510 --> 00:21:01.420
this involves a negative evaluation of an action.
One,

331
00:21:01.421 --> 00:21:06.421
did you feel negative emotion towards the target and you have a motivation to

332
00:21:07.361 --> 00:21:11.290
try to address the problem or remedy the behavior?
Okay.

333
00:21:11.710 --> 00:21:16.570
So how are these emotions,
shame,
embarrassment,
and guilt?
Actually moral emotions.

334
00:21:16.900 --> 00:21:21.190
So here we can see with shame,
there's this distinction mooching prodo shame,

335
00:21:21.370 --> 00:21:25.750
which is nonmoral how one should act and what's called moral shame.

336
00:21:25.990 --> 00:21:30.190
So when you violate a norm and you know that someone else knows about the

337
00:21:30.191 --> 00:21:35.050
violation,
and thus there's must be a defect in your one's core self.

338
00:21:35.260 --> 00:21:39.850
So you appraise oneself as morally bad.
And the case of embarrassment,

339
00:21:39.851 --> 00:21:44.080
it can be a moral emotion when it's elicited by appraisals of one social

340
00:21:44.081 --> 00:21:47.530
identity or persona as either damaged or threatened.

341
00:21:48.040 --> 00:21:50.350
And finally with guilt,
um,

342
00:21:50.390 --> 00:21:53.560
when you see a violation of more rules are imperatives,

343
00:21:53.830 --> 00:21:56.680
especially if they're caused,
um,

344
00:21:56.740 --> 00:21:58.870
if they've caused harm or suffering to others,

345
00:21:59.050 --> 00:22:02.350
you then appraise your own actions as morally bad.

346
00:22:02.590 --> 00:22:04.720
So although these are self conscious emotions,

347
00:22:04.721 --> 00:22:09.610
they also play an important role in our own moral wellbeing and the way in which

348
00:22:09.611 --> 00:22:13.140
we evaluate to what extent the actions we've done are morally good,

349
00:22:13.720 --> 00:22:17.980
good or bad.
So these two smaller families,

350
00:22:17.981 --> 00:22:22.600
we'll touch in our next a second module and moral emotions where we turn to

351
00:22:22.601 --> 00:22:26.470
what's considered the lighter side of moral emotions where we talk about

352
00:22:26.471 --> 00:22:30.720
compassion and gratitude and elevation.
Um,

353
00:22:31.060 --> 00:22:31.841
for now though,

354
00:22:31.841 --> 00:22:36.460
the last domain I want to cover today is this idea that can we actually have a

355
00:22:36.461 --> 00:22:37.450
moral brain?

356
00:22:37.870 --> 00:22:42.870
And hear this work was really spearheaded by Dr. Joshua Green at Harvard

357
00:22:43.180 --> 00:22:46.810
University who conducted one of the first studies using functional neuro imaging

358
00:22:47.080 --> 00:22:51.220
to investigate the role of emotional engagement in moral judgment.
Right.

359
00:22:51.490 --> 00:22:53.230
So although up to this point,

360
00:22:53.231 --> 00:22:56.650
we had spoken a lot about how emotion plays an important role in moral

361
00:22:56.651 --> 00:22:58.960
judgements of what's right versus what's wrong.

362
00:22:59.620 --> 00:23:03.100
We have known very little about the neural correlates,

363
00:23:03.101 --> 00:23:05.200
that underlying emotional judgements.

364
00:23:05.500 --> 00:23:10.060
So Dr Josh Green and his colleagues tried to really systematically investigate

365
00:23:10.061 --> 00:23:14.080
these questions by having participants presented with different kinds of moral

366
00:23:14.081 --> 00:23:18.730
dilemmas while they were undergoing a functional magnetic resonance imaging

367
00:23:18.731 --> 00:23:19.564
scan.

368
00:23:19.600 --> 00:23:22.240
So I'm gonna walk you through some of the moral dilemmas that participants in

369
00:23:22.241 --> 00:23:26.440
his study actually saw.
And I'd like you to think too,
while we go through these,

370
00:23:26.790 --> 00:23:30.550
um,
what decisions you might have made when presented with these dilemmas.

371
00:23:30.970 --> 00:23:33.040
So the first dilemma,

372
00:23:33.070 --> 00:23:35.980
as you can see here is referred to as the trolley dilemma.

373
00:23:36.460 --> 00:23:39.010
So on the trolley dilemma and when something is follows.

374
00:23:39.460 --> 00:23:43.210
So there was a runaway trolley headed for five people.

375
00:23:43.660 --> 00:23:47.240
As you can see on the bottom track here,
it's actually four people there,

376
00:23:47.241 --> 00:23:51.170
but it was headed for a group of people who would be killed if it proceeded on

377
00:23:51.171 --> 00:23:55.160
its course alone.
And the only way to save them was to hit a switch.

378
00:23:55.400 --> 00:23:59.060
You're here standing with the switch and if you hit that switch,

379
00:23:59.061 --> 00:24:02.690
it would turn the trolley onto a different set of tracks at the top where it

380
00:24:02.691 --> 00:24:06.740
would kill one person instead of a larger group of people at the bottom.

381
00:24:07.010 --> 00:24:10.700
So the question that I have for you is,
should you,

382
00:24:10.730 --> 00:24:15.200
the person with the switch turn the trolley in order to save,
you know,

383
00:24:15.201 --> 00:24:19.010
four or five people at the expense of killing one person.

384
00:24:19.490 --> 00:24:23.810
So what would you say in this scenario,
what should you do?
Well,

385
00:24:24.080 --> 00:24:26.780
most people in the scenario would say yes,

386
00:24:26.960 --> 00:24:31.040
they would switch the trolley to kill one person at the expense of four or five

387
00:24:31.041 --> 00:24:32.300
people down here at the bottom.

388
00:24:34.250 --> 00:24:36.950
The second dilemma is called the footbridge dilemma.

389
00:24:37.070 --> 00:24:41.670
And this is a little bit different,
so consider a similar problem,
um,

390
00:24:41.690 --> 00:24:44.840
but it's now called the footbridge dilemma.
Instead of the trolley dilemma.

391
00:24:45.110 --> 00:24:46.100
So as before,

392
00:24:46.101 --> 00:24:50.300
you've got this crazy runaway runaway trolley that is threatening to kill a

393
00:24:50.301 --> 00:24:53.150
large group of people on the other side of the track,
right?

394
00:24:53.151 --> 00:24:57.650
And you're now standing on a footbridge next to a very large,
uh,

395
00:24:57.651 --> 00:24:59.990
overweight stranger.
And here,

396
00:24:59.991 --> 00:25:04.991
the only way to save this group of people is to just push this big guy off the

397
00:25:05.661 --> 00:25:06.470
bridge.

398
00:25:06.470 --> 00:25:11.360
So he lands and plops on the tracks below where his body is so large that he's

399
00:25:11.361 --> 00:25:15.980
actually going to stop the trolley mid action.
Now he's going to die,

400
00:25:16.220 --> 00:25:18.350
but the other people will be saved.

401
00:25:18.620 --> 00:25:21.830
So should you say these other people normally refer to as,
you know,

402
00:25:21.831 --> 00:25:26.600
a group of five people by pushing the stranger to his death,
what would you say?

403
00:25:26.660 --> 00:25:29.630
Would you say,
yes,
you should do it or no,
you shouldn't.

404
00:25:31.220 --> 00:25:34.160
What's really interesting here is that most people say,
no,

405
00:25:34.340 --> 00:25:39.050
you should not do that.
Even though they said yes and the earlier dilemma.

406
00:25:39.770 --> 00:25:42.840
So what we're seeing here,
um,

407
00:25:42.860 --> 00:25:47.690
is that the top with our,
um,
trolley dilemma,

408
00:25:49.070 --> 00:25:54.070
people will say yes to pushing the lever to save a group of people at the

409
00:25:54.831 --> 00:25:56.330
expense of killing one person.

410
00:25:56.870 --> 00:26:01.870
But they will say no to pushing someone off the bridge themselves to save the

411
00:26:02.991 --> 00:26:05.600
Group of people at the expense of the life of one person.

412
00:26:05.900 --> 00:26:08.930
So this has really created a puzzle for more researchers.

413
00:26:09.080 --> 00:26:13.340
Why is one scenario acceptable?
But the others not?
So Josh Green,

414
00:26:13.341 --> 00:26:16.340
his colleagues argue that the critical differences,
how a Mo,

415
00:26:16.370 --> 00:26:19.640
how we emotionally process these two scenarios.

416
00:26:20.030 --> 00:26:21.980
So the footbridge scenario,

417
00:26:21.981 --> 00:26:26.510
the second one at the bottom seems to be more emotionally salient because you

418
00:26:26.511 --> 00:26:30.560
have to actually physically touch and push a man off the bridge.

419
00:26:30.800 --> 00:26:34.100
Whereas in the first scenario at the top,
the trolley dilemma,
Oh,

420
00:26:34.101 --> 00:26:35.510
you have to do is hit a switch.

421
00:26:35.840 --> 00:26:40.840
So it may be that some moral dilemmas actually engage a deeper and richer kind

422
00:26:41.001 --> 00:26:42.980
of emotional processing than others,

423
00:26:43.200 --> 00:26:47.100
which affects people's judgments of what is right and what is wrong.

424
00:26:47.520 --> 00:26:52.520
And he actually also tried to then understand what might be some of the um,

425
00:26:52.800 --> 00:26:53.101
you know,

426
00:26:53.101 --> 00:26:57.600
neural systems that are involved in differentiating these two scenarios.

427
00:26:58.020 --> 00:27:02.610
So the first is our trolley dilemma and he talks about this as the moral

428
00:27:02.611 --> 00:27:04.170
impersonal scenario.

429
00:27:04.410 --> 00:27:09.240
So here what you saw in the research they did putting people into the scanner is

430
00:27:09.241 --> 00:27:14.241
more activation and cognitive processing and working memory systems such as the

431
00:27:14.701 --> 00:27:19.470
middle frontal gyrus and the right and left parietal lobes.
By contrast,

432
00:27:19.471 --> 00:27:21.300
when we looked at the footbridge dilemma,

433
00:27:21.690 --> 00:27:24.120
this was called the moral personal condition.

434
00:27:24.330 --> 00:27:28.080
You actually saw greater activation in the posterior cingulate gyrus,

435
00:27:28.290 --> 00:27:31.590
the right and left angular gyrus,
and the medial frontal gyrus,

436
00:27:31.770 --> 00:27:36.770
which are more heavily associated with emotional processes as opposed to more

437
00:27:36.991 --> 00:27:40.890
cold cognitive processes in the trolley scenario bub.

438
00:27:41.310 --> 00:27:46.140
So Green and colleagues concluded that what might be right versus wrong is

439
00:27:46.141 --> 00:27:51.141
actually driven by the extent to which emotional salience is processed in the

440
00:27:51.511 --> 00:27:56.190
brain as opposed to more kind of cool cognitive processing.

441
00:27:56.460 --> 00:28:01.460
So this is why we feel that one scenario up here is right and okay to do.

442
00:28:01.860 --> 00:28:05.730
And the scenario down here is morally unacceptable or wrong to do

443
00:28:07.500 --> 00:28:09.720
so.
Interesting.

444
00:28:09.721 --> 00:28:12.720
Then in conclusion was we think about morality in emotion.

445
00:28:12.721 --> 00:28:16.380
Let's ask ourselves a question.
So Hume said,

446
00:28:16.650 --> 00:28:21.650
reason is an odd only to be the slave of the passions and can never pretend to

447
00:28:22.261 --> 00:28:26.280
be any other office than to say serve and obey them.

448
00:28:26.760 --> 00:28:31.410
So does emotion or morality predominate the other?

449
00:28:32.040 --> 00:28:36.270
So whether you take one perspective or the other that emotions completely

450
00:28:36.271 --> 00:28:39.210
override or morality or that our morality can exist.

451
00:28:39.211 --> 00:28:41.400
Independent arm of our emotions.

452
00:28:41.790 --> 00:28:45.750
I don't think that any of us can argue that emotions and morality are not

453
00:28:45.751 --> 00:28:48.480
centrally related to one another.
And in fact,

454
00:28:48.481 --> 00:28:53.481
we know that emotions play a huge role in helping create and maintain our

455
00:28:53.610 --> 00:28:57.480
everyday sense of morality.
And our sense of what is right is wrong,

456
00:28:57.840 --> 00:28:58.830
a right versus wrong,

457
00:28:58.831 --> 00:29:02.280
and a sense of what we feel is good from what we feel is bad.

458
00:29:02.760 --> 00:29:05.730
And I think important.
Wherever you stand in this debate,

459
00:29:06.000 --> 00:29:11.000
knowing more about moral psychology will help you understand why some people

460
00:29:11.461 --> 00:29:12.750
have different moral views.

461
00:29:12.990 --> 00:29:17.100
So why do some people view homosexuality is absolutely wrong?

462
00:29:17.310 --> 00:29:21.810
And others you'd is completely acceptable without any reservations whatsoever.

463
00:29:22.050 --> 00:29:25.980
So you might think about to what extent they're different emotional
dispositions,

464
00:29:26.160 --> 00:29:30.330
maybe shaping beliefs as important as whether or not someone should or should

465
00:29:30.331 --> 00:29:34.890
not be able to be in a relationship with someone of the same sex,
for example.

466
00:29:35.460 --> 00:29:35.850
Um,

467
00:29:35.850 --> 00:29:40.850
so think about this wherever you stand and knowing that emotions fundamentally

468
00:29:41.681 --> 00:29:46.681
shaped morality and that maybe one per chapter dominates over the other,

469
00:29:46.870 --> 00:29:51.100
but regardless of that,
they both are intimately related to one another.

470
00:29:52.000 --> 00:29:56.200
So with that,
I'm going to now conclude with our takeaway questions,
um,

471
00:29:56.250 --> 00:30:00.010
and our introduction to moral emotions today.
So

472
00:30:01.630 --> 00:30:03.260
the first question,
um,

473
00:30:03.490 --> 00:30:08.490
covers what were the core features of what makes up a moral emotion in the first

474
00:30:08.861 --> 00:30:12.430
place?
How is it different from other classes of emotions?

475
00:30:13.300 --> 00:30:17.680
Second,
can you describe the different moral families of an emotion?

476
00:30:18.190 --> 00:30:18.791
In other words,

477
00:30:18.791 --> 00:30:23.320
what is the cad triad hypothesis within this broader framework?

478
00:30:24.490 --> 00:30:25.270
Finally,

479
00:30:25.270 --> 00:30:30.270
what is the supportive neural evidence from studies using Fmri tools for the

480
00:30:30.821 --> 00:30:33.570
critical role of emotion in,

481
00:30:33.680 --> 00:30:38.680
in motivating and explaining moral judgments of right versus wrong.

482
00:30:40.480 --> 00:30:44.530
We'll now turn to our expert interviews today where we have two fascinating

483
00:30:44.531 --> 00:30:45.340
experts,

484
00:30:45.340 --> 00:30:50.230
leaders in the field of emotion and morality and this is part of our experts and

485
00:30:50.231 --> 00:30:51.790
emotion interview series

486
00:30:53.650 --> 00:30:55.600
for our expert in emotion interview.

487
00:30:55.660 --> 00:30:58.660
We have the honor of speaking with Dr Jonathan Heit.

488
00:30:59.020 --> 00:31:04.020
Jonathan Height is the Thomas Cooley professor of ethical leadership at the Nyu

489
00:31:04.600 --> 00:31:09.280
Stern School of business and received his Ba from Yale University and phd from

490
00:31:09.281 --> 00:31:10.810
the University of Pennsylvania.

491
00:31:11.260 --> 00:31:15.760
He then did his post doctoral research at the University of Chicago as well as

492
00:31:15.761 --> 00:31:19.960
in Orissa,
Indiana or India.
Sorry,
I'm just going to start that over.
Okay.

493
00:31:19.990 --> 00:31:23.890
Cause I messed it up for our experts in emotion interview.

494
00:31:23.891 --> 00:31:26.320
We have the honor of speaking with Dr Jonathan Heit.

495
00:31:26.560 --> 00:31:31.560
Jonathan Heit is the Thomas Cooley professor of ethical leadership at the Nyu

496
00:31:31.570 --> 00:31:32.403
Stern School of business.

497
00:31:32.500 --> 00:31:36.010
He received his Ba from Yale University and phd from the University of

498
00:31:36.011 --> 00:31:40.840
Pennsylvania and also completed postdoctoral research at the University of

499
00:31:40.840 --> 00:31:41.380
Chicago.

500
00:31:41.380 --> 00:31:46.240
He was a professor at the University of Virginia from 1995 until 2011 when he

501
00:31:46.241 --> 00:31:50.860
joined the Stern School of business at Nyu.
His research focuses on morality,

502
00:31:50.980 --> 00:31:55.450
it's emotional foundations,
cultural variations and developmental course.

503
00:31:55.930 --> 00:31:59.650
He began his career first studying negative moral emotions,

504
00:31:59.800 --> 00:32:02.350
such as discussed shame and vengeance,

505
00:32:02.650 --> 00:32:07.650
but then later moved on to the understudied positive moral emotions such as

506
00:32:07.751 --> 00:32:10.810
admiration.
Ah,
an elevation.

507
00:32:11.380 --> 00:32:15.580
This work got him involved with the field of positive psychology in which he's

508
00:32:15.581 --> 00:32:16.960
been a leading researcher.

509
00:32:17.500 --> 00:32:21.790
He's the co developer of the moral foundations theory and of the research site,

510
00:32:21.791 --> 00:32:26.791
your morals.org he uses his research to help people understand and respect the

511
00:32:27.341 --> 00:32:29.140
moral motives of their enemies.

512
00:32:29.650 --> 00:32:33.280
He's won three teaching awards from the University of Virginia and one from the

513
00:32:33.281 --> 00:32:36.790
governor of Virginia.
He spoken twice at the Ted Conference,

514
00:32:37.000 --> 00:32:39.310
one on politics and one on religion.

515
00:32:39.560 --> 00:32:44.560
He's been named as a top 100 global thinker by in 2012 by foreign policy

516
00:32:45.021 --> 00:32:49.490
magazine and as the author of more than 90 academic articles and two books,

517
00:32:49.491 --> 00:32:51.740
including the happiness hypothesis,

518
00:32:51.741 --> 00:32:56.450
finding modern truth and ancient wisdom and the righteous mind why good people

519
00:32:56.451 --> 00:32:58.880
are divided by politics and religion.

520
00:32:59.120 --> 00:33:03.470
So I now turn to a very special experts and emotion interview with Doctor

521
00:33:03.471 --> 00:33:06.110
Jonathan Heit on morality and emotion.

