Speaker 1:          00:00:06       Thanks for being here for lecture five of Cs two 30. Um, today we have the chance to, to host a guest speaker of Pronto Rashford Park, who's a Phd, students in computer science, advised by a professor Andrew Ng and prophecy or Percy Liang. So Pranav is, uh, is working on, um, AI in high impact projects, uh, specifically related to health care and natural language processing. And today he's going to two presents an overview of AI for healthcare and he's willing to dig into some projects. He has led a true case studies. So, uh, don't hesitate to interrupt. I think we have a lot to learn from [inaudible] and he's really an industry expert for AI for healthcare. Um, and I let you the Mike print off. Thanks for being here.

Speaker 2:          00:00:58       Thanks Cam. Thanks for inviting me. Uh, can you hear me at the back? Is the mic on? All right, fantastic. Well, really glad to be here. Um, so I want to cover three things today. The first is give you a sort of broad overview of what AI applications in healthcare look like. The second is bring you three case studies from the lab that I'm in, um, as demonstrations of AI in healthcare research and then finally, uh, some ways that you can get involved if you're interested in applying AI to high impact problems in healthcare or if you're from a healthcare background as well. Let's start with the first.

Speaker 3:          00:01:44       Okay,

Speaker 2:          00:01:45       so one way we can decompose the kinds of things, yeah, I can do in healthcare is by trying to formulate levels of questions that we can ask from data at the lowest level are what are descriptive questions here we're really trying to get at what happened.

Speaker 2:          00:02:05       Then there are diagnostic questions where we're asking why did it happen if a patient had chest pains, I took their uh, x-ray, what does that chest x ray show? If they have palpitations, what is their ECG show? Then they're predictive problems here. I care about asking about the future, what's going to happen in the next six months and then at the highest level are prescriptive problems. Sure. I'm really trying to ask, okay, I know this is the patient. This is the symptoms they're coming in with. This is how their trajectory will look like in terms of, um, in terms of things that may happen that their risk off. What should I do? And this is the real action point and that's, I would say the, the gold mine. But, uh, to get there requires a lot of data and a lot of steps and we'll a little bit more about that.

Speaker 2:          00:03:03       So in cs two 30, you're all well aware of the paradigm shift of deep learning. And if we look at machine learning in healthcare literature, we see that has a very similar pattern is that we had this, uh, feature extraction engineer who was responsible for, um, getting from the input to a set of features that a classifier can understand. And the deep learning paradigm is to combine feature extraction and the classification and to one step a by automatically extracting features, which is cool. Here's what I think will be the next paradigm shift for AI in healthcare, but also more generally is a, we still have a deep learning engineer up here. Ah, that's you, that's me, uh, that are designing the networks that are making decisions like a convolutional neural network is the best architecture for this problem. The specific type of architecture. There's an Rnn, CNN and whatever, and then you can throw on there.

Speaker 2:          00:04:10       But what if we could just replace out the Mol engineer as well? Uh, and I find this quite funny because everyone, you know, in Ai for healthcare question that I get asked a lot is, are we going to replace doctors with all these AI solutions? And nobody actually realizes that we might replace machine learning engineers faster than me might replace doctors of this is to be the case. And a lot of research is, uh, developing algorithms that can automatically learn architecture, some of which you might go through in this class. Great. So that's the general overview. Now I want to talk about three case studies in the lab of Ai being applied to different problems. And because healthcare is so broad, I thought I'd focus in on one narrow vertical and let us go deep on that. And that's medical imaging. So I've chosen three problems and one of them's a one B problem.

Speaker 2:          00:05:11       The second is a two d problem is, and the third is, uh, is he three d problems. So I thought we could, we could walk through all the different kinds of data here. Uh, so this is some work that was done early last year in the lab where we showed that we were able to detect arrhythmias of the level of cardiologists. Um, so arrhythmias are an important problem. In fact, millions of people, uh, this is a specialty come to light recently with uh, uh, devices like the Apple Watch, which now have a ECG monitoring. Uh, and uh, the thing about this is that sometimes you might have symptoms and know that you have arrhythmias, but other times you may not have and still have arrhythmias that can be addressed with, uh, with, uh, if, if you were to do an ECG. And the ECDS test is basically showing the heart's electrical activity over time.

Speaker 2:          00:06:10       The electrodes are attached the skin safe tests and it takes over a few minutes. And this is what it looks like when you're hooked up to all the different electrodes. Uh, so this test is often done for a few minutes in the hospital. And the finding is basically that, uh, in a few minutes you can't really capture a person's abnormal heart rhythms. So let's send them home for 24 to 48 hours with a holter monitor and let's see what we can find. Um, they're more recent devices such as the Zio Patch, which let, um, let patients be monitored for up to two weeks. And it's, it's quite convenient. You can use it in the shower or while you're sleeping so you really can capture a lot of what what's happening in the hearts, uh, ECG activity.

Speaker 3:          00:07:02       Okay.

Speaker 2:          00:07:02       But if we look at the amount of data that's generated in two weeks, it's 1.6 million heartbeats. That's a lot. And there are very few doctors who would be willing to go through two weeks if ECG reading for each of their patients. And this really motivates why we need automated interpretation here. But automated detection comes with its challenges. One of them is, you know, you have in the hospital several electrodes. And in more recent devices we have just one. Uh, and the way one can think of several electrodes is sort of the, the electrical activity of the heart is tweedy and um, each one of the electrodes is giving a different two d perspective into the Three d perspective. Um, but now that we have only one lead, we only have one of these perspectives available. Uh, and the second one is that the differences between the heart rhythms are very subtle. This is what a cardiac cycle looks like. And when we're looking at, um, arrhythmias or abnormal heart rhythms, a one's going to look at the substructures within the cycle. And then, uh, the s the structure between cycles as well and the differences are, are quite subtle.

Speaker 2:          00:08:25       So when we started working on this problem, Oh, maybe I should share this story. So, uh, we started working on this problem and then it was, uh, me, my, my collaborator on and, uh, and Professor Ang and one of the things that he, that he mentioned we should do, you said, let's just go out and read ECG books and let's do the exercises. And if you're in med school, they're, these books were where you can, where you can learn about ECG interpretation and then there are several exercises that you can do to test yourself. Uh, so I went to the med school library, uh, you know, they have those, uh, a hand crank, a sheltered the bottom, so you have to move them and then grabbed my book. And then we went for two weeks and did, uh, did go through two books and learn ECG interpretation.

Speaker 2:          00:09:13       And it was pretty challenging. And if we looked at previous literature to this, I think they were sort of drawing upon some domain knowledge share in that here we're looking at waves, how can we extract specific features from waves that doctors are also looking at? So there was a lot of feature engineering going on and if you're familiar with wavelet transforms, they were the sort of um, there were the most common approach, uh, with a lot of sort of like different mother wave let's et Cetera, et Cetera, preprocessing band, pass filters. So everything you can imagine doing with signals was done and then you fed it into your SVM and you called it a day. I would deep learning, we can change things up a bit. So on the left we have an ECG signal and on the right is just uh, three heart rhythms. We're going to call them a, B, and c, and we're going to learn a mapping to go straight from the input to the APP. What,

Speaker 3:          00:10:13       okay.

Speaker 2:          00:10:14       And here's how we're going to break it out. We're going to say that every label labels the same amount of the signal. So if we had four labels and the ECG would be split into these four, sort of, this rhythm is labeling this part and then we're going to use a deep neural network. So we built a one the convolutional neural network, which runs over the time dimension of the input. Cause remember we're getting one scalar over over time and then this architecture's 34 layers deep. Um, so I thought I'd talk a little bit about the architecture. We've seen resonance before.

Speaker 2:          00:11:00       Okay. Uh, so should I go into this? Okay, cool. Here's my one minute spiel of resonant then, is that you're going deeper in terms of the number of players that you're having in a network. You should be able to re represent a larger set of functions. But when we look at the training error for these very deep networks, what we find is that it's worse than a smaller network. Now this is not the validation error. This is the training air. That means even with the ability to represent a more complex function, we aren't able to represent the training data. So the motivating idea of residual networks is to say, hey, let's add shortcuts within the network so as to minimize the from the error signal to each of my layers. Uh, this is just math to say the same thing. So further work on rest net showed that, okay, we have the shortcut connection, how should we make information flow through it the best? And the finding was basically that anything you, you add the shortcut to the highway, think of these as stop signs or um, or rs signals on a highway. And it's basically saying the fastest way on the highways to not have anything but addition on it.

Speaker 2:          00:12:35       And then there were a few advancements on top of that, um, like adding dropout and increasing the number of filters in the convolutional neural network, um, that we also added to this network. Okay. So that's the convolutional neural network. Let's talk a little bit about data. So one thing that was cool about this project was that we got to partner up with a, um, with a startup that manufacturers these hardware patches and we got data off of patients who are wearing these patches for up to two weeks. And this was from around 30,000 patients. Um, and this is 600 times bigger than the largest dataset that that was out there before. And for each of these ECG signals, what happens is that each of them is amputated by a clinical ECG expert who says, here's where we're the May starts and fierce where s so let's mark the whole ECG that way.

Speaker 2:          00:13:40       Obviously very time intensive, but a good data source. And then we had a test set as well. And here we used, um, could we use a committee of cardiologists so they'd get together, sit in a room and decide, okay, we disagree on this specific point, let's try to, let's try to discuss which one of us is right or what this rhythm actually is. So they arrive at a ground truth after discussion. And then we can of course test cardiologists as well. And the way we do this, we have them do it individually. So this is not the same set that did the ground truth. There's a different set of cardiologists coming in one at a time. You tell me what's going on here and we're going to test you. So when we compared the performance of our algorithm to cardiologists, uh, we found that we were able to surpass them on the f one metrics. So this is precision and recall.

Speaker 3:          00:14:35       Okay.

Speaker 2:          00:14:36       And when we looked at where the mistakes were made, uh, we can see that sort of the, the biggest mistake was between distinguishing two rhythms, which look very, very similar, um, but actually don't have a difference in, um, in, in treatment. Here is another case where the model is not making a mistake which the experts are making. Um, and turns out this is a costly mistake. This is saying a benign heart rhythm, uh, or what experts thought was a benign heart rhythm was actually a pretty serious one. Um, so that's, that's one beauty of automation is that we are able to catch these, um, catch these misdiagnosis. Uh, here are three heart blocks, uh, which are clinically relevant to catch on which the model outperformed the experts and on atrial fibrillation, which is probably the most common, serious or Rhythmia, the same pulse.

Speaker 2:          00:15:39       So one of the things that's neat about this application, a lot of applications in healthcare is what automation, uh, with deep learning, machine learning enables, is for us to be able to continuously monitor patients. And this is not something we've been able to do before. So a lot of even science of understanding how patients, uh, risks factors, uh, what they are or how they change hasn't been done before. And this is an exciting opportunity to be able to advance science as well. And the Apple Watch has recently, uh, release their, their ECG monitoring. Um, and it'll be exciting to see what new things we can find out about the health of our hearts from, uh, from these inventions. Okay. So that was our first gay. Yep. Question.

Speaker 4:          00:16:39       Awesome. Yeah. So repeat the question. How couple was it to,

Speaker 2:          00:16:46       um, to, uh, sort of, um, deal with data privacy and sort of keep patient information private? So in, in this case, we did not have, we had completely de identified data, so it was just, um, someone's ECG signal without any extra information about their, um, their clinical records or anything like that. So it's, it's very, it's very de identified.

Speaker 4:          00:17:10       All right guys, I guess went to ask that. Um, how did you get over that and were there problems in getting, um, because um, you know, there's a lot of concerns so you have to like get it signed off. Let's cut real authority. You are like what are the obstacles and getting that to be there? Oh, sure. And I think we can, we can take this question offline as well, but one of the

Speaker 2:          00:17:32       beauties of working at Stanford is that there's a lot of, uh, industry research collaborations and uh, we have great infrastructure to be able to work with that. Uh, so, which brings me onto my second case study. Sorry. Yeah, go for it.

Speaker 5:          00:17:50       Tender four patients that has the mosque, I feels like you, the expert comparing with their us. So how can exploit the clinicals there? That's a good question.

Speaker 2:          00:18:08       Repeat the question. How did we define the gold standard when we have experts setting the gold standard? Uh, so here's how we did it. So one, one way to come up with a gold standard is to say, okay, if we looked at what a consensus would say, what would they say? And so we got three cardiologists in a room to set the gold standard, and then to compare the performance of experts. These were individuals who are separate from those groups of cardiologists who sat in another room and said what they taught of the, um, of the ECG signals. So that way there's, there's some disagreement where the gold standards set by the committee.

Speaker 2:          00:18:54       Great. So here we looked at how we can detect pneumonia off of chest x rays. Uh, so pneumonia is an infection that affects millions in the, uh, us. Uh, it's big global burden is actually in, um, in kits. So that's where it's really useful to be able to, uh, detect that automatically and well. So to detect pneumonia, there's a chest x ray exam, uh, and chest x rays are the most common, uh, imaging procedure, uh, with 2 billion chest x rays done per year. And the way abnormalities are detected. And chest x rays is, they present as areas of increased density. Uh, so where things should appear dark fifth year brighter or vice versa. And here's what a characteristically pneumonia looks like, where it's like a fluffy cloud. Uh, but this is an oversimplification of course because uh, pneumonia is one of the Alveoli fill up with puss, but the Alveoli can fill up with a lot of other things as well, which lead to very different interpretations and diagnoses for the patients and treatment for the patient.

Speaker 2:          00:20:13       So it's quite confusing, which is why radiologists trained for years to be able to do this. Um, the set up is we'll take an input image of someone's chest, x ray and output the binary label zero one, which indicates the presence or the absence of pneumonia. And here we use a d convolutional neural network, which is pre, pre trained on image net. Okay. So we looked at shortcut connections earlier and uh, Dan snippets had this um, idea to take short good connections to the extreme. It says, what happens if we connect every layer to every other layer instead of just connecting sort of one, instead of having just one shortcut, which is what resonate hat and a dense net beat the previous state of the art, um, and has generally lower error and fewer parameters on the image net challenge. So that's what we used, uh, for the Dataset.

Speaker 2:          00:21:21       When we started working on this project, uh, which was around October of last year, uh, there was this large data set that was released by the NIH of hundred thousand chest x rays. And this was the largest public dataset at the time. And hear each extra is annotated with up to 14 different pathologies. And the way the sanitation works is there's an NLP system which reads a report and then outputs for each of several, put apologies whether there is a mention, whether there is a negation, like not pneumonia for instance. Um, and then annotates accordingly. And then for our test set, uh, we had four radiologists here at Stanford independently annotate and tell us what they thought was going on in those extras. So one of the questions that comes up often in medical imaging is we have, we have, um, a model, we have several experts, but we don't really have a ground truth and we don't have a ground truth for several reasons.

Speaker 2:          00:22:31       Sometimes one of them is just that it's difficult to tell whether someone has pneumonia or not without additional information like their clinical record or even once you gave them antibiotics, antibiotics, did they get treated? Uh, so really one way to evaluate whether a model is better than a radiologists are as well as doing as well as the radiologist is by saying do they agree with other experts similarly. So that's what we use here. That's the idea. We say, okay, let's have one of the radiologists be the, the, the prediction model where evaluating and let's set another radiologist to be ground troop and now we're going to compute the f one score once, uh, change the ground truth, do it the second time, change it again third and then also use the model as the ground truth and do it again. And we can use a very symmetric evaluation scheme.

Speaker 2:          00:23:37       But this time having the model we evaluated against each of the four experts. So we do that and we get a score for both of them. Well for all of the experts and for the model. And we showed in our work that we were able to do better than the average radiologist at this task, um, to weigh six done this in the future is to be able to look at patient history as well and look at a lateral radiographs and be able to improve upon this diagnosis. Um, at the time at which we released our work, um, on all 14 pathologies, we were able to outperform the previous state of the art. Okay. So model interpretation, model interpretation. Yes. There's question.

Speaker 4:          00:24:35       So if you have pneumonia, you present it to the soccer golf like fever, Jakafi and your rib. Certain cops too much can't sleep. That's not included in the model. If you go to a set and you're trying to determine does this person happened and what you are not, that's one thing. But you don't, not that you just don't have that data, but you're not looking at other images. Let's say disaster stuff, cancer, other, because being Neil Cole as Testis, it's all, those are images that you're not we look at, so let's say, and there's going to tough situation. So the obvious situation is really doing a bunch of it right? But in a tough situation, if you get a patient that has a fever, it's coffee [inaudible] don't know this cancer or pneumonia or black lung disease, then how do you, how do you get your outcomes of the work in that condition? And also if you're not including all those other cases, then it's not just that. What's the use of it with like, you don't want to say something like, well, so I'm trying to keep it at this technical does a technical class, what's the, is there a neural network architecture that you would use to be able to solve problem number one? It's a multitask learning. Is it like, sure, sure. Okay. So let me try to boil those sort of sets of, so

Speaker 2:          00:25:52       questions down. So one is patients are coming in, we're not getting access to their clinical history. So how are we able to make this determination? At all. So one thing is that when we're training the algorithm, we're training the algorithm on, on, uh, pathologies extracted from radiology reports. And these radiology reports are written with understanding of full clinical history and understanding of, uh, sort of what the patient presented with in terms of symptoms as well. So we're training the model on, on these radiology reports, which had access to more information. And the second is that the utility of this is not as much in being able to compare a patient's X-ray's day to day as much as here is a new patient, uh, with a set of symptoms. And can we identify things from their chest x rays.

Speaker 3:          00:26:53       Okay.

Speaker 2:          00:26:54       Which brings us to model interpretation. So if you were a end user for model, um, oh, I, I, so when I was back in Undergrad, um, and I was in the lab, we were working on autonomous cars. Um, and I thought about this a lot. How many of you have been in an autonomous car? How many of you would trust being in an autonomous car?

Speaker 2:          00:27:25       All right, cool. Yeah, I thought about this as well. Would I trust being in an Thomas Cart? And I thought it'd be pretty sweet if the algorithm that was that was in the car would tell me whatever decision it was going to make an advance. I know that's not possible at high speeds so that, you know, just in case I disagreed with a particular decision, uh, I could say no a board and uh, and a half the model sort of, you know, a remake its decision. And I think the same holds true in healthcare as well. The one advantage that happens in healthcare is rather than having to make decisions within seconds, like in the case of the, of his car, there is often a larger timeframe like minutes or hours that we have. And, and here it's, it's useful to be able to inform the clinician that's treating the patient to say, Hey, here's what my model thought and why. Um,

Speaker 2:          00:28:28       so here's the technique we use for that class activation maps, which you may cover in another lecture. Uh, so I'll just, I'll just leave it out saying that there are ways of being able to look at what parts of the image are most evident of a particular pathology, uh, to generate these, these heat maps. So here's a heat map that's generated for a pneumonia. So this extra has pneumonia and I can and, and, um, uh, and the algorithm and red is able to highlight the areas where it taught was most problematic for that. Here's one in which a, it's able to do a collapsed right lung. Here's one in which able is able to find a small cancer.

Speaker 3:          00:29:18       Okay.

Speaker 2:          00:29:19       And here are the goal is to be able to improve healthcare delivery where, um, in the develop world, one of the things that's useful for is to be able to prioritize the workflow, make sure the radiologists are getting to the patients most in need of care before one's who's x look more normal. Uh, but the part which I'm quite excited about is to increase the access of medical imaging expertise globally. Uh, where right now the World Health Organization estimates that about two thirds of the world's population does not have access to diagnostics. Um, and so we thought, hey, wouldn't it be cool if we just made an APP that was able to, uh, allow users to upload images of their, um, off rays and be able to give its diagnosis? Uh, so this is still in the works, so I'll show you what we've got running locally. And, uh, so here I'm presented with a screen that asks me to upload and X-ray.

Speaker 3:          00:30:29       Okay.

Speaker 2:          00:30:30       And so I have, I have several x-ray's here. Uh, and I'm gonna pick the one that says, uh, cardiomegaly. So cardiomegaly refers to the enlargement of the heart.

Speaker 3:          00:30:43       Okay.

Speaker 2:          00:30:44       So I uploaded it. Now it's running the models running in the backend and within a couple of seconds it's output it, it's diagnoses on the right. So you'll see the 14 pathologies that the model is trained on being listed and then next to them a bar. Um, and at the top of this list is cardiomegaly, which is what this patient has the, the hardest sort of extending out. And if I hover on cardiomegaly, I can see that the probability is displayed on there. And now we talked about interpretation. How do I believe that this model is actually looking at the heart rather than looking at something else. And so if I click on it, um, I get the class activation map for this, which shows that indeed it is focused on the heart, uh, to be able to, um, and, and is looking at the right thing. So I guess you can say the Algorithms Heart's in the right place. Cool. Uh, but I thought, so this is an image that I got from the, the data set that we were using NIH, but it's pretty cool if an algorithm is able to generalize to populations beyond. And so I thought what we'd do is we could just look up, look up an image of cardiomegaly, uh, and

Speaker 3:          00:32:12       yeah,

Speaker 2:          00:32:12       download it and just see if our model is able to, this one looks pretty large,

Speaker 3:          00:32:20       so does this,

Speaker 2:          00:32:25       I don't want an annotated one. All right, that's good. So we can

Speaker 3:          00:32:33       do that, save it desktop

Speaker 2:          00:32:39       and now we can upload it here. And it's already read on this thing and all the top is cardiomegaly once again. So it's able to generalize too. And there's the highlight. So it's able to generalize to populations beyond just the ones it was trained on. So I'm very excited by that. And what I got even more excited by is, uh, we're thinking of deploying this out in, um, out in different parts of the world. And when we got an image, uh, that showed how x rays are red in, uh, this hospital that we're working with in Africa, uh, this is what we saw. And so the idea that one could snap a picture and upload it seems and get a diagnosis seems very powerful. Um, so the third case study I want to take you through is, um, being able to look at Mri. So we've talked about one d one d setup where we had an ECG signal. We've talked about a two d setup with an x ray. How many of you are thinking of working on a three d problem for your project? Okay, phew. That's good. Cool.

Speaker 3:          00:34:06       Yeah.

Speaker 2:          00:34:06       See here we looked at Nia, Mr. So Mris of the knee is the standard of care to valuate nitas orders and more MRI examinations are performed on the knee than any other part of the body. Um, and the question that we sought out to answer was, can we identify knee of normalities? Um, two of the most common ones include an ACL tear and a municipal tear at the level of radiologists.

Speaker 3:          00:34:39       Okay.

Speaker 2:          00:34:40       Now with the three d problem, one thing that we have that we don't have in a two d setting is the ability to look at the same, same thing from different angles. And so when radiologists do this diagnosis, they look at three views, the sagittal coronal and the axial, which are, which are three ways of, uh, looking through the three d structure of the knee. And in an Mr you get different types of series, uh, based on the magnetic fields. And so this year, uh, three different, uh, series that are, that are used.

Speaker 3:          00:35:18       Okay.

Speaker 2:          00:35:19       And what we're going to do is output for a particular need, Mr Examination, the probability that it's abnormal, the probability of an ACL tear and the probability of a meniscal tear. Important thing to recognize here is this is not a multi-class in that I could have both types of tears. It's a multi labeled problem. So we're going to train a, a convolutional neural network for every view pathology pair. So that's nine convolutional networks. And then combine them together, uh, using a logistic regression. So here's what each convolutional neural network looks like. I have a bunch of slices within a view. I'm going to pass each of them to a feature extractor and I'm going to get an output probability.

Speaker 2:          00:36:19       So we had 1,400 and Emr exams from, uh, the Stanford Medical Center. And uh, we tested on one 20 of them where the majority vote of three a subspecialty radiologists establish the, the ground troop. And we found that we did pretty well on, on the three tasks and had the model be able to pick up the different abnormalities pretty well. And one can extend these, these methods of interpreter interpretability, uh, to, uh, to three d inputs as well. So that's what we did here. Okay. So I saw this, I saw this cartoon a few a few weeks ago and I thought it was pretty funny. Uh, which is a lot of machine learning engineers think, uh, that they don't need to externally validate, which is find out how my model works on, uh, works on data. That's not my, where my original data set came from. So there's a, there's a difference in, in distributions, uh, but it's really quite exciting when a model does generalize to two data sets that it's not seen before.

Speaker 2:          00:37:45       And so we got this dataset that's, that's public from a hospital in Croatia and here's how it was different. So it was a different, it was a different kind of series of different magnetic properties, uh, is a different scanner and it was a different institution in a different country. And we asked, okay, what happens when we run this model off the shelf that was trained on Stanford data, but tested on that kind of data? And we found that it did relatively well without any training at all. But then when we trained on it, we found that we were able to outperform the previous lead best reported result on the Dataset. So there's still some work to be done in being able to generalize, um, sort of my here that was trained on my data to be able to work on data sets from different institutions, different countries, swell. But we're making some steps along that way remains a very open problem for taking.

Speaker 6:          00:38:54       Yeah. So we did the best we could in terms of processing. So we had, so one of the preprocessing steps as important is being able to, uh, get the mean of the, of the input data to be as close to the mean of the input data that you're trained on. Uh,

Speaker 2:          00:39:14       so that was one preprocessing step we tried, we were trying to minimize that to say, out of the box, how would this work if we had never seen this data before? How would it work on that population? So one big topic in across a lot of applied fields is asking question, okay, we're talking about models working automatically autonomously. How would these models work in when working together with experts in different fields? And here we ask that questions about radiologists and about imaging models. Would it be possible to be able to boost the performance if the model and the radiologist's work together? And so that's really the setup. A radiologist with model does that better than the radiologist by themselves. And here's how we set it up. We said let's have experts read the same case twice separated by a certain set of weeks. Um, and then see how they would perform on the same set of cases.

Speaker 3:          00:40:33       Yeah.

Speaker 2:          00:40:34       And what we found that we were able to increase the performance generally with a significant significant increase in specificity for ACL tears. That means if someone, if a patient came in, uh, without a, without an ACL tear, I'd be able to, uh, find it better. So in the future, yes. Question,

Speaker 6:          00:41:05       radiology, sergeants that the intended you want to kind of biases being informed actually looked at the commissions out. Yeah. So that's a good question. And I,

Speaker 2:          00:41:14       and I think how so sort of automation bias captures a lot of this and that once we have sort of models working with um, experts together, can we expect that the experts will sort of take it less seriously? Cause that's, that's a big concern and start relying on what the model says and says, I won't even look at this exam. I'm just going to trust with the model says blindly. Um, that's absolutely possible in a very open area of research. Some of the ways that people have tried to address it is to say, you know what I'm going to do from time to time, I'm going to pass in an exam to the radiologist for which I'm going to flip the answer and I'll know the right one. And if they get that wrong, I'll alert them that you're relying too much on the model, uh, stop. Uh, but there are a lot of more sophisticated ways to go about addressing automated bias. As far as I know, it's a very open field of research, especially as we're getting into deep learning assistants.

Speaker 3:          00:42:21       Okay.

Speaker 2:          00:42:22       And one utility of this is to say basically that the set of patients don't need follow up, let's not send them for unnecessary surgery. Great. So I shared a three case studies from the lab. The final thing I want to do is to talk a little bit about how you can get involved if you're interested in applications of AI to help care. Uh, so the first is, uh, the ability for you to just get your hands dirty with, uh, data sets and, and be able to try out your own model. So we have a, from our lab released, uh, the Mora Dataset, which is a large Dataset of uh, uh, bone x rays. And the task is to be able to tell if it's um, if the x rays are normal or not. And they come from different, uh, parts of the, of the upper body. Um, and that's, that's what the data set x rays look like.

Speaker 2:          00:43:25       And this is a pretty interesting setup because you have more than one view us more than one angle for the same body part for the same study, for the same patient. And the goal is to be able to combine this well, uh, into convolutional neural network and, and be able to output the probability of, of normality. And one of the interesting things here for transfer learning as well is do you want to train the models differently per body part or do you want to train them? Train the same model for body parts are combined certain models. Uh, so a lot of design decisions there, and this is what train some train models look like. This is a model baseline that we released that's able to identify a fracture here and a piece of hardware on the right. Um, and you can download the Dataset, offer a website. So if you Google Maura Dataset or go on our website, Stanford Mol Group Dot. Get hub.io, you should be able to find it.

Speaker 2:          00:44:29       Uh, the second way to get involved is through the Ai for healthcare bootcamp, which is a two quarter long program that our lab runs, uh, which provides, uh, students coming out of, uh, classes like two 30 and opportunity to get involved in research. And here's, uh, students receive training from a PSE students in the lab and medical school faculty, uh, to work on structured projects over two quarters. Um, and if you have a background in Ai, which you do, then you're encouraged to apply. And we're working on a wide set of problems across radiology, uh, Ehr, public health and pathology right now.

Speaker 3:          00:45:14       Um, this is what the lab looks like. We have a lot of fun.

Speaker 2:          00:45:22       Um, and the applications for the bootcamp starting in the winter are now open. So the early application deadline is November 23rd and you can go on this link and um, and, and apply. Uh, so that's my time. Thank you so much for having me and thanks for having me. Kevin.

Speaker 3:          00:45:42       [inaudible] did you want to take one or two questions? So I'll take a couple of questions.

Speaker 4:          00:45:57       Oh, you asked a question about privacy concerns for the leather. I think his concerns. What about compensation for the medical experts that you're potentially putting out of business with a free tool like the one that you're, you're developing or you know, and just in general because their, their knowledge is being used to train these models. It's not free. Yeah. So the question,

Speaker 2:          00:46:21       shit it was, we're having these, uh, automated AI models trained with the knowledge of medical experts. Um, what are ways in which we're thinking of compensating these medical experts, uh, right now or in the future when we have, uh, possibly automated models? Uh, I think a lot of people are thinking about these problems and working on them right now. There are a variety of approaches that people are thinking about in terms of economic incentives. And there's a lot of fear about sort of, well, AI actually work with or augment experts in whatever field they're working on. I don't have a great a silver bullet for this. Uh, but I know there's, there's a lot of work going on in there.

Speaker 3:          00:47:14       No.

Speaker 4:          00:47:15       Um, when you're looking through Mris, we show looking at four or five gap issues, like one of them is the most lazy. Uh, it's possible that the human looking at it quite of something that was

Speaker 6:          00:47:30       not be looked at by the EA model at that time. Yes. So how do you address that? Yeah, that's a great question. So the, just to repeat the question, it's, uh, we have, we're looking at MRI exams and we're saying for these three pathologies, Yep.

Speaker 2:          00:47:45       We're able to put the probabilities, what happens if there's another pathology that we have it looked at. Uh, so I have a couple of answers for that. The first is that one, if the one of the categories here was simply to tell whether it was normal or abnormal. So the idea here is that the abnormality class will capture a lot of different pathologies there, at least the ones seen at Stanford. Uh, but it's often the case that we're building for one particular pathology. And then there's obviously a, um, a burden on the model and the model developers to be able to convey, hey, look, our algorithm model only does this. And you really need to watch out for everything else that the model doesn't cover. Maybe that's the, unless there's one more question. No. All right, that's the last question. We'll take them. Thank you once again.

Speaker 5:          00:48:42       Okay,

Speaker 1:          00:48:47       well now you've got, you've got the perspective. Is the microphone working? Yeah. Now you've got the perspective of a knee, I researcher working in healthcare. Now you are going to be the AI research, a researcher working in healthcare. We're going to go over a case study and that is targeted at skin disease. So you know, uh, in order to detect skin disease, sometimes you take pictures, microscopic pictures of cells on your skin and then in the lies those pictures. So that's what we're going to talk about today. So let me talk about the problem statement. You're a deep yearning engineer and you've been chosen by your group of healthcare practitioners, uh, to determine which parts of a microscopic image corresponds to a cell. Okay? So here's how it looks like, um, on did the black and white, it's not black and white image. It's a color image, but it looks black and white.

Speaker 1:          00:49:40       The input image is the, the one that is closer to me. Um, and the yellow one is the ground truth that has been labeled by a doctor. Let's see. So what you're trying to do is two segments. The sales on this image, and we didn't talk about segmentation yet or a little bit segmentation is uh, is about producing a value, a class for each of the pixels on our image. So in this case, each pixel would correspond to either no cell or cell zero or one. And once we output a matrix of Zeros and ones telling us which peaks those corresponded to a cell, we should get hopefully a mask like the yellow mask that I overlapped with the input image. Does that make sense? Yeah. Third category. That's the boundary. The colored image, the yellow one. You don't have the boundaries part of yourself.

Speaker 1:          00:50:38       Yeah, we'll talk about the boundary later, but right now I assume it's a binary segmentation. So zero in one, no selling seller. Okay. So, uh, is going to be very interactive and I think we're going to use mentee for several question and groups. You guys into groups of three. So here are other examples of images that were segmented with a mask. Now doctors have collected 100,000 images coming from microscopes but d images come from treaty, from microscopes. There is a type A, type B and type C microscope and the data is split in between these tree as 50% for type a 25% for type B, 25% for type C. Um, the first question I have for you is given that the doctors want to be able to use your algorithm on images from the microscope of type C, these microscopies, the latest one, it's the one that is going to be used widely in the field and they want your network to work on this one. How would you split your data set into train Dev and test set as a question and please group in teams of two or three and discuss it for a minute on how you would split this Dataset.

Speaker 5:          00:52:03       Yeah, thanks.

Speaker 1:          00:53:03       You can start going on. Mentee and write down your answers is what,

Speaker 5:          00:53:27       okay,

Speaker 1:          00:53:29       take 30 seconds to input your, your insights on, on mentee. You can do one per team.

Speaker 5:          00:53:38       Okay.

Speaker 1:          00:53:39       And we'll start going over some of the answers here.

Speaker 5:          00:53:44       Okay.

Speaker 1:          00:53:46       Dev tests, sleeves split. See Train on a plus B 20 chain train 2.5 in Dev and test

Speaker 1:          00:53:57       training. 80 all a all be five KC DEF 10 Casey test 10 Casey 95 five where test and Dev is from population we care about. I think these are good answers. I think there is no perfect answer to that. But two things to take into consideration. You have a lot of data so you probably want to split it into 95, five closer to that than to 60 20, 20 and most importantly, you want to have see images in the test. They haven't test set to have the same distribution among these two. That's what you've seen in the third course. Uh, and we would prefer to have actually seen ages in the train set. You own your algorithm to a half since the images. So I would say a very good answer is, is this one 95 five where the five and five are exclusively from c and you also have seen images in the 90% of train images. Any other insights on that? But he agrees. Yep.

Speaker 4:          00:55:01       Hidden features that will mess up. Yeah.

Speaker 1:          00:55:05       So there is much more thing with you to talk about here. One is how do we know what's the distribution of microscope a images and microscopy images versus microscope. See, do they look like each other? If they do all good, if they don't, how can we, how can we make sure the model doesn't get bad hints from these two distributions? Uh, another thing is data, data augmentation. We could augment this data set as well and try to get as much as see distribution images as possible. We're going to talk about that.

Speaker 5:          00:55:37       Okay.

Speaker 1:          00:55:39       Split has to roughly be 95, 560, 2020 distribution of Dev and test sets has to be the same continuations from CN there. There should also be seeing much in the training set. Now Talk to you about data augmentation. Uh, you think you can augment these data and if yes, give only three these things method you would use. If no xsplit explicate. Explain why you cannot. You want to take 30 seconds to talk about it with your neighbors.

Speaker 5:          00:56:09       Yep. [inaudible]

Speaker 1:          00:57:36       let's go over some of the answers. So rotation, zoom, Blur. I think looking at the images that we have from the sales, this might work very well. Uh, rotation, zoom, Blur, translation, a combination of those stretch symmetry. Like probably a lot of those work at one follow up question that I'll have is can you, can someone give an example of a task where data augmentation might hurt the model rather than helping it?

Speaker 7:          00:58:12       Yeah. If you want to have over feet on the test set, can you be more precise? You don't want to generalize. Oh, you don't want your model to generalize it too much. Okay. Yeah, that there's some cases where you don't want the model to generalize too much, especially though doing quarter. But any other ideas like face detection would be like upside down or like either side. I see. So if you do face detection, you probably don't want the defense to be upside down. Although we never know, depending on the youth. But, uh, it's, it's not going to help much use. The camera is always like that and it's building humans that are not upside down, but I don't think it's going to hurt the model.

Speaker 1:          00:59:00       It's probably going to not help the model I guess. Yeah.

Speaker 7:          00:59:08       And if you stretch the image, so they're their algorithms, like maybe you know, flow nets. It's an algorithm that's used for on videos to detect the speed of a car, let's say, uh, is you stretch you need is probably, you cannot detect the speed of the car anymore.

Speaker 1:          00:59:26       Any other examples?

Speaker 3:          00:59:30       Yeah,

Speaker 7:          00:59:35       character called mission I think is a good example. So let's say you're trying to detect is, and you do

Speaker 1:          00:59:42       three flip and you get that, you know, like you're, you're labeling is be everything that was d and as the, everything that was B four, nine and six. It's the same story. So these data augmentations are actually hurting the model because you don't really label when you data when you're mature data. Right. Okay. Okay. So yeah, many augmentation methods are possible cropping, adding random noise, um, changing contrasts. I think they told my Chin is super important. I remember a story of um, of a company that was working on a self driving cars and, and also uh, virtual assistance in cars. You know what like these type of interaction you have with someone in your car or a virtual assistant and they notice that the speech recognition system was actually not working well when the car was going backwards. Like no idea why, like why is this doesn't seem related to the speech recognition system of the car.

Speaker 1:          01:00:43       And they test it out and they looked and they figured out that people were putting their hands in the passenger seat looking back and talking to the virtual assistants. And because of microphone was in the front, the voice was very different when you were talking to, to, to do back of the car rather than the front of the car. And so they use data augmentation in order to augment their current data. They didn't have data on that type of, of people talking to the back of the car. So by augmenting smarter, you can change the voices so that they look like they were used by someone who was talking to the back of the car and then solve the problem. Okay. A small question. Oh, we can do it quickly. What is the mathematical relationship between annex and then why? So remember, we have an RGB image and we can, we can flatten it into a vector of size annex. And the output is a mask of size. And why, what's the relationship between Nx Nna? Why someone wants to go for it?

Speaker 3:          01:01:45       Hmm.

Speaker 7:          01:01:48       Very cool. Very cool with thinks they're not equal. And why and why? It would be three annex annex would be three. And why? Because you have RGB images and for each RGB pixel you would have one hour to zero one. Okay. That was a question on one of the midterms. It was a complicated question.

Speaker 1:          01:02:20       Uh, what's the last activation of your network sigmoid you on? Probably a with zero and one, uh, Nsu had several classes. So later on when we see we can also segment per disease, then you would have the softmax, uh, what loss functions should we use?

Speaker 1:          01:02:39       Want to give it to you to go quickly because we don't have too much time. You're going to use a binary cross entropy loss over all the outputs and the entries of, of the outputs of your network. Does that make sense? So always think the thinking through the loss function is interesting. Okay. So you have a first try in and you have coded your own neural network that you've left, you've named model and one and one and you've trained it for 1000 bucks. It doesn't end up performing well. So it looks like that you give it the input image to the model and get an output that is expected to be the following one, but it's not. So one of your friends tells you about transfer learning and they, they, they tell you about another label data set of 1 million microscope images that have been labeled for skin disease classification, which are very similar to those you want to work with from microscopes seat.

Speaker 1:          01:03:43       So a model m two has already been trained by another research lab on this new data sets on a 10 class dcs classification. And so here is an example of input output of the model. You have an input to manage that probably looks very similar to the ones you're working on. The network has a certain number of layers and a soft Max last vacation at the end that gives you the probably distribution over the dcs that seems to correspond to this image. So they're not doing segmentation anymore, right? They're doing classification. Okay. So the question here is going to be you want to perform transfer learning from [inaudible]. What are the hyper parameters that you will have to choose? It's more difficult than it looks like. So think about it. Discuss with your neighbors for a minutes trying to figure out what are the hyper parameters involved in this transfer learning process.

Speaker 5:          01:04:58       Yeah, yeah.

Speaker 1:          01:06:00       Okay, thanks. 15 more seconds to wrap it up.

Speaker 5:          01:06:12       Okay.

Speaker 1:          01:06:14       Then see what you guys have learning rates Edis a hyper parameter. I don't know if it's specific to the, to the trust for learning weights of the last layers. So I don't think that's a hyper parameter. Weights are parameters. New cost function for additional output layers. I think that's the hyper, the choice of the loss. You might count it as high parameter. I don't think it's specifically related to transfer learning. You will have to train with the loss you've used on your model and one.

Speaker 5:          01:06:45       Okay.

Speaker 1:          01:06:46       Number of new layers yet weights of the new, not a hyper parameter. Okay. Last one or two either layers of and too. So do we train, what do we find? Tune. So a lot about layers actually. Size of added layers.

Speaker 1:          01:07:07       Not sure. Okay, let's, let's, let's go over it together cause it seems that there's a lot of different answers here. Um, let me try to write it down here. So let's say we have, we have the model m two is it big enough for the back? We have the model m too. And so we give it to an input image. Okay. Input and the model and to gives us a probability distribution. Softmax so we have this soft Max here. You, you, you will agree that we probably don't need the softmax layer. We don't want it. We want to do some segmentation. So one thing we have to choose is how much of these pretrade network, because it's a pretrained networking. How much of these network do we keep? Let's say we keep these layers because they probably know the inheritance salient features of the Dataset, like the edges of the cells that were very interested in. So we take it so we have it here and you agreed at here we have the first hyper parameter that is l the number of layers from him too that we take. Now, what other high performance do we have to choose?

Speaker 1:          01:08:25       This is l we probably have to add a certain number of layers here in order to produce our segmentation. So there's totally another hyper parameter which is n zero how many layers do I stuck on top of this one? And remember these layers are pretrained but these ones are randomly initialized. That makes sense. So to hyper parameters, anyone sees a third one? The third one comes when you decide to train this new network. You have the input image,

Speaker 1:          01:09:14       give it to the network. Did the outputs, segmentation, mask, segmentation, masculinity, seg mask. And what's your have to decide is how many of these layers will I freeze? How many of the pretrained layers I freeze? Probably you have a small data set you'd prefer keeping the seizures that are here freezing them and focusing on retraining the last few layers. So there is another high profile theater, which is how much of this will I freeze? LF? What does it mean to freeze? It means during training. I don't train these layers. I assume that they've been seeing a lot of data already. They understand very well the edge is and a less complex features of the data. I'm going to use my knee, my small data set to train the last layers. So three hyper-parameters, l l zero and l f. Does that make sense? Okay. So this is for transfer learning. So it looks more complicated than the question. The question was more compact dictated and it looked like, okay, let's wear on me.

Speaker 5:          01:10:22       Okay.

Speaker 1:          01:10:23       Okay, let's go over another question.

Speaker 1:          01:10:30       Okay. So this, we did it. Now it's interesting because, uh, here we have an input image and in the middle we have the output that the doctor would light. But under rights you have the output of your algorithm. So you see that there is a difference between what they want and what we're producing. And it goes back to, someone mentioned it earlier, there's a problem here. How would you think you can correct the model and or data set to satisfy the doctor's request? So these should we with this image is that they want to be able to separate the cells among them and they cannot do it based on your algorithm. It's still a little hard. There is, there's something to add. So can someone come up with the answer or do you want to explain it to the, you mentioned one of the, so that we, we can finish this like yeah, you want to add boundaries because now it looks like you could have like three cells on the bottom left blurring in together.

Speaker 1:          01:11:22       And so if you asked her attic boundaries, it makes the selling super well defined. Good answer. So one way is when you label your datasets origin nearly you labeled with Zeros and ones for every pixel. Now instead you will label with three classes, zero one or boundary, let's say zero, one two, four boundary. Or even the best method I would say is that for each pixel, for each input pixel, the output will be the corresponding, okay, this one is not good. The corresponding label like this is a cell picture. He off cell p of boundary and Pete have no cell. What you will do is that instead of having a sigmoid activation, you will use a soft max activation. Okay, and the soft Max will be for a pixel. One other way to do that, if it still doesn't work, doesn't work. Even if you label the boundaries. What is another way to do that? You relabel your datasets by taking into account the boundaries. The model still doesn't perform well.

Speaker 1:          01:12:39       I think it's all about the weighting of the loss function. It's likely that the number of pixels that are boundaries are going to be fewer than the number of pixels that ourselves or no cells. So the network will be biased towards predicting sale or no sale. Instead, what you can do is when you computer lost function, you're not functioning should have three terms. One Binary Cross entropy, let's say for no sell, one for cell and one for boundary. Okay? And this is going to be summed over by equals one to n I their whole output pixel values. What you can do is to attribute acquisitions to each of those Alpha or Beta or one and by tweaking this coefficient, if you put a very high or very low number here and there, it means you're telling your model to focus on the boundary. You're telling them on model that if you miss the boundary, it's a huge penalty. We want you to train by figuring out all the boundaries. That's another trick that you could use.

Speaker 1:          01:13:44       One question on that. Yeah, we'd question really will your Dataset, this, this last Friday section, you'd be labeling bounding boxes, you know, for the Yolo Algorithm. So the same tools are available for segmentation where you have an image and you would draw the different lines in practice. If the Ma is the tool that you were using the line used, we'll just count as a sale. Everything including the wound with everything inside what you drove. Plus the boundary would count a cell and the rest has no cell is just a line of code to make a difference. The line you drew will count as boundary. Everything inside will count as cell and everything outside would count as no cell. So it's the way you use your labeling tool. That's all.

Speaker 7:          01:14:40       I think it's not learnable. Parametric, it's more hyper parameters to choose. So you know the same way you to lambda for your regularization, you would choose Alpha and Beta. When do you make a distinction like that? Attention back. If you combine those two. So this is not an attention mechanics and because he's just a training trick, I would say you cannot know how much attention we tell you for each image, how much the model is looking at this part versus that part. This is not going to tell you that it's just a training thing. What's the advantage to doing it this way as opposed to like object detection? Like they're texting each. So, so the question is what's the advantage of doing segmentation rather than detection? Yeah. So detection means means

Speaker 1:          01:15:24       you want to output a bounding box. If y'all put a bounding box, what you could do is I'll put the bounding box, crop it out, and then analyze the cell and try to find a contour of the set. But if you want to separate the cells, if you want to be very precise, segmentation is going to work well. If you want to be very fast, bounding boxes would work better. I think that's the general way. Segmentation is not working as fast as the Ula algorithm works for object detection. Yeah, I would say that, but it's more much more precise. Okay. So when you find the data sets in order to label the boundaries on top of that, you can change a lost function to give more weight to boundaries or penalize false positives. Okay. Uh, we have one more slide I think. Uh, so let's go over it.

Speaker 1:          01:16:12       So now the doctors, they give you a new dataset that contains images similar to the previous ones. Uh, the differences that each image now is labeled with zero in one zero meaning there are no cancer cells on that image. And one means there is at least a cancer cell on this thing. So we're not doing segmentation anymore. It's a binary classification, image, cancer or no cancer. Okay? So you'll easily be the state of the art model because you're, you're a very strong person in classification and you achieve 99% accuracy. The doctors are super happy and they asked you to explain the network's prediction. So given an image classified as one, how can you figure out based on which sell the model predicts one? So print, I've talked a little bit about that. There are other methods that you should be able to figure out right now, even if you don't know class activation maps. So to sum it up, we have a new image input image. Put it in your new network. That is a binary classifier. And the network says one, you want to figure out why the network stays one based on which pixels. What'd you do?

Speaker 1:          01:17:38       Visualize the weights. Uh, what'd you visualize in the woods? I think visualizing the waste, uh, is not related to the input. The weights are not going to change based on the inputs. So here you want to know why this inputs lead to one. So it's not about the weights, each pixel. Good idea. So you know, after you get the one here, this is why hats basically, it's not exactly one, let's say it's 0.7 probability. What you going to remember is that this number derivative of y hat we'd be Spec two x is what? It's a matrix of shapes. Same as x. You know, it's a matrix and each entry of the Matrix is telling you how much moving these big sale influences white hat. You're great. So the top left number here is telling you, um, how much x one is impacting y hat. Is it or not?

Speaker 1:          01:18:45       Maybe it's nuts. If you have a cad detector and the cat is here, you can change the pixel. It's never going to change anything. So the value here is going to be very small, closer to zero. Let's assume the cancer cell is here, you will see high number in this part of the matrix because these these pieces, these are the peaks. So let's, if we move them, it would change my hat. Does it make sense? So quick way to interpret your network. It doesn't, it's not too, too good. Like you're not going to have tremendous results, but you should see these pixels have higher derivative values than the others. Okay? That's one way. And then we will see in two weeks how to interpret neural networks, visualizing the weights included and all the other methods. Okay, so gradients with respect to your model, the techs cancer cells from the test set images with 99% accuracy. Why a doctor would on average perform 97% on the same task. Is this possible or not? Who thinks it's possible to have a network that I choose more accuracy on the test set then the doctor. Okay. Can someone, can someone say why

Speaker 3:          01:20:03       you have an explanation? Can you look at two

Speaker 1:          01:20:08       things? Possibly it's going to get it. Okay. The next one probably looks at complex things that doctor didn't say. They didn't see it. That's what you're saying. Possibly. I think there is a, um, a more rigorous explanation, human errors and approximation for Basier, but we don't actually know where it is. So theoretically you can get better. So here we're talking about base, they're human level performance and all that stuff. That's when you should see it. So one thing is that there are many concepts that you will see in course street at are actually implemented in industry, but it's not because you know them that you're, you're going to understand that it's time to use them. And that's what we want you to get to. Like now when I ask you these questions, you have to talk, think about baser human level accuracy and so on.

Speaker 1:          01:20:51       So the question that you should ask here is what was the data set labeled? What were the labels coming from if the data set was labeled by individual doctors? I think that looks weird. Like if it was labeled by individual doctors, I think it's very weird that the model performs better on the test set. Then what doctors have labor because simply because the labels are wrong 2% of the time on average the labels are wrong. So you're teaching wrong things to your model 3% of the time. So it's surprising that it gets better. It could happen but surprising. But if every single image of the data set has been labeled by a group of doctors as print, I've talked about it, then the average accuracy of these group of doctor is probably higher than one doctor. Maybe it's 99% in which case it makes sense that the model can be to one doctor. Does it make sense? So you have baser you're trying to approximate with, we'd like the best area you can achieve. So regrouping, grouping a cluster of doctors probably better than one doctor. This is your human level performance and then you should be able to beat one doctor. Fine.

Speaker 1:          01:22:01       Okay, so you want to build the pipeline that goes from image taken by you, the front of your car to steering direction for autonomous driving. What you could do is that you could send this image to a cord detector that detects all the cars, a pedestrian detector that detects all the pedestrians and then you can give it to a path planner. Let's say that plans the path and outputs the steering direction let's say. So it's not end to end. And to end would be I have an input image and I give it, I want this output. So a few other disadvantages of this is is something can go wrong anywhere in the model, you know, how do you know which part of the model went wrong? Can you tell me which parts I give you an image? The same direction is wrong. Why?

Speaker 3:          01:23:18       Yep,

Speaker 1:          01:23:22       good idea. Looking at the different components. So what you can do is look what happens here in there. Look, look what's happening here and there. You think based on this image, the car detector worked well or not, you can check it out. You think the pedestrian detector works well? Not you can check it out. If there is something wrong here, it's probably one of these two items. It doesn't mean this one is good, it just means that these two items are wrong. How do you check that? This one is good. You can label ground truth images and give them here as input to this one and figure out is it's figuring out the steering direction or not. If it is, it seems that the past planner is working well. If it is not, it means there's a problem here. Now what if every single components seem to work properly? Like let's say these two are properly but there is still a problem. It might be because what you selected as a human was wrong.

Speaker 1:          01:24:20       The past Tanner cannot detect, cannot get this team direction correct based on only the pedestrians and the car detection and the car's probably need the stop signs and stuff like that as well. You know it so because you made hand engineering choices here, your model might go wrong. That's another thing. And another advantage of, of, um, of this type of pipeline is that data is probably easier to find out at m for every algorithm rather than the four hole, the whole end to end pipeline. If you want to collect data for the entire pipeline, you would need to take a car, put a camera in the front, uh, like, like build a kind of steering wheel angle detector that will measure your ceiling will at every step while you're driving. So you need to drive everywhere basically with a car that has this feature, it's pretty hard. You need a lot of data, a lot of roads. While this one you can collect data of images anywhere and labeled, it's labeled the Palestinians on it. You can detect cars by the same process. Okay, so these choices also, depending on what data can you access easily or what data is harder to acquire. Any questions on that?

Speaker 1:          01:25:34       Uh, you are going to learn about convolutional neural networks. Now we're going to get fun with a lot of imaging. Uh, you'll have a quiz into programming assignment for the first module, second module, same midterm next Friday, not this one. Everything up to c, four and two will be included in the midterm. So up to the videos you're watching this week, uh, includes ta sections and the next one and every in class lecture, including next Wednesday's and destroy you. You have a ta section.

Speaker 7:          01:26:04       Any questions on that? Okay. See you next week guys.