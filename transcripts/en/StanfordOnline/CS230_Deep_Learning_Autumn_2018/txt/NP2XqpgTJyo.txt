Speaker 1:          00:00:06       Hi Everyone, and welcome to lecture nine of for [inaudible]. Uh, today we're going to discuss an advanced topic, uh, that will be kind of the, the marriage between deep learning and another field of AI, which is reinforcement learning. And we will see a practical application and how deep learning methods can be plugged in another family of algorithm. So it's interesting because deep learning methods and deep neural networks have been shown to be very good a function approximators essentially that's what they are. We're giving them data so that they can approximate a function. There are a lot of different thieves which require this function. Approximators and deep learning methods can be plugged in. All these methods. This is one of these examples. So we'll first motivate, uh, the, the setting of reinforcement learning. Why do we need to enforcement learning? Why cannot wait? Why can't we use deep learning methods to solve everything?

Speaker 1:          00:01:10       There are some set of methods that we cannot solve with deep learning and reinforcement learning. Reinforcement learning applications are examples of that. We will see, uh, an example, uh, to introduce an algorithm of a reinforcement learning algorithm called Q learning. And we will add deep learning to this algorithm and make it deep learning. Uh, as we've seen with a generative adversarial networks and also deep neural networks, most models are hard to train. We've had, we had to come up with Xavier initialization with dropout, with batch norm and a myriads of myriads of methods to make these deep neural networks train in gangs. We had to use methods as well in order to train gans and tricks and hacks. So here we will see some of the tips and tricks to train deep cue learning, which is an enforcement learning algorithm. And at the end we will have a guest speaker coming to talk about advanced topics, which are mostly research which combine deep learning and reinforcement learning.

Speaker 1:          00:02:20       Sounds good. Okay, let's go. So real deeper enforcement. Louie is a very recent field, I would say. Although both fields are, are enforced, wondering how has existed for a long time. Only recently it's been shown that using deep learning as a way to approximate the functions that play a big role in reinforcement learning algorithms has worked a lot. So one example is Alphago. And, uh, you probably all have heard of it. It's Google deepmind's Alphago has a beaten world champions in a game called the game of go, which is a very, a very strategy old game and the one on the right here, um, or on your rights, human living controlled three d print. Forcement learning is also a deep mind Google deep mine paper that came out and hit the headlines on the front page of nature, which is, uh, one of the leading, uh, multidisciplinary peer review journals in the world. And they've shown that,

Speaker 2:          00:03:25       okay,

Speaker 1:          00:03:25       we deep learnings plugged in a reinforcement learning setting. They can train an agent that beats human level in of RUTF gapes. And in fact, these are our three gates. So they've shown actually that their algorithm the same, I agree them reproduce for a large number of games can

Speaker 1:          00:03:45       beats humans on all of these games. Most of these games, not all of these camp. So these are two examples or do they use different sub techniques of reinforcement learning? They both include some deep learning aspect in it. And today we, we mostly talk about the human level controls through deeper enforcement learning, also called deep to network presented in this paper. So let's, let's start with we'd motivating reinforcement learning and using the alpha go setting. Um, this is a board of Google and the picture comes from deep mines blog. Uh, so go, you can think of it as a strategy game where you have agreed that he's up to 19 by 19 and you have two players. One player has white stones and one player is Blackstone's. And that's every step in the game. You can position a stone on the, on the board, on one of the grid cross.

Speaker 2:          00:04:38       Yeah.

Speaker 1:          00:04:38       The goal is to surround your openings, so to maximize your territory by surrounding your openings. And it's a very complex game for different reasons. Uh, one reason is that you have to be you. You cannot be shortsighted in this game. You have to have a longterm strategy. And other reason is that the board is so big, it's much bigger than a chess board, right? Chessboard is eight by eight. So let me ask you a question. If you had to solve, we'll build an agency that solves this game and beats humans or plays very well, at least with deep learning methods that you've seen so far. How would you do that? Someone wants to try.

Speaker 2:          00:05:39       Yeah,

Speaker 1:          00:05:39       so let's say you have a, you have to collect the data set because in classic supervise learning, we need a data set with x and y. What'd you think would be your x and y?

Speaker 2:          00:05:52       Yep.

Speaker 1:          00:05:53       Where does the book then? The output is the probability of victory from that.

Speaker 3:          00:05:58       Okay. Game boards and output is probability of victory in that position. So that's, that's a good one. I think input output. What's the issue with that one? So yeah, it's hard. It's super hard to represent what the probability of winning is from the sport. Even like nobody can tell you, even if I ask an expert human to come and tell us what's the problem of black winning in this or white twinning in this setting, they wouldn't be able to tell you that this is a little more complete. Any other ideas of datasets? Yup. Okay. Good point. So we could have a

Speaker 1:          00:06:45       the grid like this one and then this is the inputs and the outputs would be the move, the next action taken by probably a professional player. So we would just watch professional players playing and we would record their moves and we would build the data sets of what is a professional move and we hope that's our network. Using these input outputs will at some point learn how the professional players play and given an input, a states of the board will be able to decide of the next move. What's the issue with that?

Speaker 2:          00:07:25       Yep.

Speaker 3:          00:07:32       You need a whole lot of data. Why? And you said it's you, you said, uh, because, uh, we need basically to represent all types of positions of the board, all states. So if you were actually, it's, let's do that. If we were to compute the number of possible states of this board, what would it be? It's a 19 by 19 board. Remember what we did with adversarial examples. We did four pixels. Right now we're doing youth leader board. So what's the question? First is you want to try, yeah. Three to do power. 19 Times 19 or 19 squared. Yeah. So why is it that, what, why is it, is it this last thoughts? Stone? Yeah, each spot and there are 19 times 19 spot can have three state basically no stone, white stone or Blackstone. So this is the old possible state. This is about 10 to the 170 so it's super, super.

Speaker 1:          00:08:56       So we can probably not get even close to that by observing professional players for us because we don't have enough pro Sheltie years and because we're humans and we don't have infinite life. So the professional for years, you're not play forever. They might get tired as well. Uh, but so one issue is the state space is too big. Another one is that the ground truth probably would be wrong. It's not because you're a professional player that you will play the best move every time. Right? Every player has their own strategy. So the ground truth where we're having here is not necessarily true and our network might, might not be able to beat these human players. What we're looking into here is an algorithm that beats humans. Okay? Second one. Too many states in the game as you mentioned. And third one we will likely not generalize.

Speaker 1:          00:09:45       The reason we will not generalize is because it's classic supervise learning. We're looking for patterns. If I ask you to build an algorithm to detect cats versus dogs, it will look for what the pattern of the cat is versus what the pattern of the dog is. And in the convolutional theaters we learned that in this case it's about a strategy. It's not about a putter, so you have to understand the process of winning this game in order to make the next move. You cannot generalize if you don't understand this process of longterm strategy. So we have to incorporate that and that's where our l comes into place. Rl is reinforcement learning or method that would could be described with one sentence as automatically learning to make good sequences of decision. So it's about the long term. It's not about the shorter and we would use it.

Speaker 1:          00:10:32       Generally when we have delayed labels, like in this game, the label that you mentioned that the beginning was probably to of victory. This is a longterm label. We cannot get this labeled now, but over time the closer we get to the end, the better we ha we are at seeing the victory or not and it's to make sequences of decision so we make a move, then the it makes a move, then we make another move and all the decisions of these move or correlated with each other, like you'd have to plan it in advance. When you're human, you do that when you play chess, when you play golf. So examples of our applications can be robotics and it's still a research topic, how deep our El can change robotics. But thinking about having a robot walking from here and you want to send it there, you want to send the robot there.

Speaker 1:          00:11:18       What you're teaching the robot is if you get there, it's good, right? It's good you achieve the task, but I cannot give you the probability of getting there at every point. I can help you out by giving you a reward when you arrived there and let you trial and error. So the robot will try and randomly initialized the robot. We just fall down at the first at first get a negative reward, then repeats. This time the robot knows that it shouldn't fall down and she wouldn't go down. It should probably go this way. So to trial and error and reward on the longterm, the robot is supposed to learn these pattern. No do one, he's games and that's the one we would see today. Uh, games can be represented as, as, as a set of reward for reinforcement learning algorithm. So this is where you win.

Speaker 1:          00:12:03       This is where you lose led the algorithm play and figure out what winning means and what losing means until it learns. Okay. The problem with using deep learning is that the algorithm will not learn cause this reward is to long term. So we're using reinforcement learning and finally advertisements. So a lot of advertisement, um, our real time bidding. So you want to know, given a budget, when you want to invest this budget, and this is a longterm strategy planning as well that reinforcement learning can help with. Okay, so this was the motivation of reinforcement learning. We're going to jump to a concrete example that is a super vanilla example to understand cue learning. So let's start with this game or environment. So we call that an environment generally, and it has several states in this case, five states. So we have these states and we can define rewards, which are the following.

Speaker 1:          00:12:57       So let's see what is our goal in this game? We defined it as maximize the return or the rewards on the long term. And what is the reward is the numbers that you have here that were defined by a human. So this is where the human defines the rework. Now what's the game? The game has five states. State. One is a trashcan and has the reward of plus two state too is a starting state's initial states. And we assumed that we would start in the initial state with a plastic bottle in our hand. The goal would be to throw these plastic bottle in a can. If it's the Trashcan we get plus two. If we get to state five we get to the recycle bin and we can get plus 10 super important application state for has a chocolate. So what happens is if you go to state for, you get a reward of one because you can eat the chocolate and you can also through the chocolate in the, in the, in the, in the recycle bin. Hopefully those, the setting make sense. So the states are of three types. One is the starting state initial, which is Brown, the normal state, which is not a starting, neither, neither starting Nora unending state. And it's great. And the blue states are terminal states. So if we get to the terminal state, we end up a game or an episode, let's say, does this setting makes sense? Okay. And your two possible actions you have to move. Either you go on the left or you go on the right.

Speaker 1:          00:14:33       An additional rule will will add is that the garbage collector will come in three minutes and every step takes you one minute. So you cannot spend more than three minutes in this gate. In other words, you cannot stay at the chocolate and it chocolate forever. You have to move at some point. Okay, so one question I have is how would you define the longterm return? Because we said we want a longterm return, we don't want, we don't care about short term returns.

Speaker 2:          00:15:03       Okay.

Speaker 3:          00:15:04       What do you think is a good way to define the long term return here? Just some of the terminals states, the sum of how many points you have when you reach the terminal states. So let's say, I mean stay too. I have zero reward right now. It's like I used to determine terminal state under right on, on the, on your left, the Plus Two plus two reward. And I finished the game. If I go under rights instead and I reached a plus 10 you're saying that the longterm return can be all the, some of the rewards I got to get there. So plus 11 so this is one way to define the longterm return. Any other ideas? Reduce the copper, the word, we probably want to incorporate the time steps and reducing the reward as pie, as time passes. And in fact, this would be called a discounted return versus what you said would be called a return. Here we use a discounted returning and it has several advantages. Some are mathematical because the return you described, which is not discounted, my not converge, it might go up to plus infinity. Yeah.

Speaker 1:          00:16:29       Uh, this discounted return will converge with the appropriate discount. Um, so intuitively also, why is the discounted return intuitive? Is it because time is always an important factor in our decision making? People would prefer cash now then cash in 10 years, right? Or similarly, you can consider that the robot has a unique to life expectancy. Like it has a battery and loses battery every time it moves. So you want to take into account these discount of, if I can eat chocolates close, I go for it because I know that is a chocolate is too far. I might not get there because I'm losing some battery, some energy, for example. So this is the discounted return. Now if we take gamma equals one which means we have no discounts, the best strategy to follow in this setting seems to be to go to the, to the left or to go to the right, starting in the initial state to right. And the reason is it's a simple computation. On one side I get plus two on the other side, I guess plus 11 what if my discount was 0.1?

Speaker 3:          00:17:39       Which one would it be better? Yeah, you're left with veteran directly to Gloucester. And the reason is because we compute in your, we

Speaker 1:          00:17:49       just do 0.1 times a month one which gives us 0.1 plus 0.1 squared times 10 and it's less than two. We know it. Okay, so now we're going to assume that the discount is 0.9 and it's a very common discount. Two, two, two, two, two years. You mean enforcement authority and we use a discounted return. So the general question here, and it's the core of reinforcement learning in this case of [inaudible] learning is what do we want to learn? And this is really, really think of it as a human. What would you like to learn? What are the numbers do you need to have in order to be able to make decisions really quickly? Assuming you had a lot more states than that and actions. Any ideas of what we want to learn? What would help our decision making?

Speaker 2:          00:18:54       Passion.

Speaker 1:          00:18:55       Optimal action at each state. Yep.

Speaker 2:          00:18:58       Yeah,

Speaker 1:          00:18:58       that's exactly what you want to do. For even a state's telling me the action that I can take and for that I need to have a score for all the actions in every state in order to store the scores. We need the matrix, right? So this is our matrix. We will call it a cute table. It's going to be of shape, number of states, times, number of actions. If I have this matrix of scores and the scores are correct, I'm in Stage three. I can look on the third row of this matrix and look what's the maximum value I have? Is it the first one or the second one? If he's the first one, I go to the left. If he's the second one that is more maximum, I go to the right. This is what we would like to have. Does that make sense?

Speaker 1:          00:19:41       It's cure table. So now let's try to build a cute table for this example. If you had to build it, you would first think of it as a treat. Oh, and by the way, every entry of this Q table tells you how good it is to take this action in that state state corresponding to the rural action corresponding to the column. So now how do we get there? We didn't build the tree and that's, that's similar to what we were doing. Our mind, we starting this to in two we have two options. Either we go to s one we get to or we go to a street and we get zero from [inaudible]. Two weeks from this one, we cannot go anywhere. It's a terminal state, but from s three we can go to as to and get zero by going back. Or we can go to s four and get one.

Speaker 1:          00:20:30       That makes sense from us for saying we can get zero by going back to a street or we can go to s five and yet plus 10 now here I just have my immediate reward for every state. What I would like to come to you, it is the discounted return for all the states because ultimately what should lead my decision making in the state is if I take this action, I get you a new states, what's the maximum reward I can get from there in the future? Not just the reward I get in that state. If I take the other action I get to another state, what's the maximum reward I could get from that state? Not just the immediate reward that I get from going to that state. So what we would do, we can do it together. Let's say we want to compute the value of of the actions from s three from a going right and left from his shirt.

Speaker 1:          00:21:18       I can either go to s four or s two going to s four I know that the immediate reward was one and I know that from s four I can get plus 10 this is the maximum I can get so I can discount this 10 multiplied by 0.9 10 times 0.9 Jesus nine plus one which was the immediate reward is just nine and he's just, he's us 10 so 10 is the score that we give to the action. Go right from state as tree. Now what if we do it from one step before [inaudible] from s to I know that I can go to s three and two is three I get zero reward. So the immediate reward is zero but I know that from history I can get 10 reward. Ultimately on the long term I need to discount this reward from one step. So I multiply this 10 by 0.9 and I get zero plus 0.9 times 10 which gives me nine so now in state two going right will give us a longterm reward of nine make sense?

Speaker 1:          00:22:19       And you do the same thing. You can copy back that going from s four two s three will give you zero plus the maximum you can get from my street, which was 10 discounted by point night. Or you can do it from s two from s to I can go left and get lost too. Or I can go right and get nine and the immediate reward would be nine would be zero. And I will discount the nine by 0.9 and get 8.1 so that's the process we would do to come to that. And you see that it's an iterative algorithm are you will just copy back all these values in my matrix. And now if I'm in state too, I can clearly say that the best action seems to go, seems to say, uh, go to the left because the longterm discounted reward is nine.

Speaker 1:          00:23:05       While the longterm discounted reward for going to the right is to, and I'm done. That's true learning. I solved the problem I had. I had, uh, a state a problem statement. I found a matrix that tells me in every state what action I should take. I'm blind. So why do we need deep learning? So question we will try to answer. So the best strategy to follow with 0.9 is still right, right, right. And the way I see it is I just look at my matrix at every step and I follow always the maximum of my role. So from state to nine is the maximum. So I go right from say 10 is the maximum. So I still go right and from state for 10 years, the maximum. So I go right again. So I take the maximum of where all the action's in a specific state. Okay. Now, one interesting thing to follow is that when you do these iterative algorithm, at some point it should converge and ours converged to some values that represent the discounts and rewards. For every state and action.

Speaker 1:          00:24:10       There is an equation that's this cue function follows. And we know that the optimal Q function follow this equation. The one we have here follows this equation. This equation is called the bellman equation. Any ties, uh, two terms. One is art and one is these counts times the maximum of the Q scores over all the actions. So how does that make sense given that you're in a state as you want to know the score of going, of taking action a in this state, the score should be the reward that you get by going there plus the discount times the maximum you can get in the future. That's actually what we used in the iteration. Does these bellmen equation makes sense?

Speaker 1:          00:24:57       Okay, so remember this is going to be very important in jewelry is bellmen equation. It's the equation that is satisfied by the optimal Q table or Q function. And if you try out all these entries, you will see that it follows this equation. So when Q didn't is not optimal, it's not following this equation yet, we would like you to follow this equation. Another point of vocabulary, we were forced where learning is a policy policies, they noted piece sometimes or new and uh, sorry, pie pie of s is equal to Arg Max over the actions of the optimal queue. That's yours. What it means. It means it's exactly our decision process. It's even that were in state as we look at all the columns of the state as in our table, we take the maximum and this is what [inaudible] is telling us. It's telling us this is the action you should take. So Pi or policy is our decision making.

Speaker 4:          00:25:53       Okay.

Speaker 1:          00:25:54       It tells us what's the best strategy to follow in a given state. Any questions so far? Okay. And so I have a question for you. Um, why is deep yearning had foot? Yes.

Speaker 3:          00:26:19       Yeah. That's the reason number of Stacy's way too large to store at a table like that. So like if you have a small number of states, a number of actions, then easy, you can use it to your table. You can at every stage look into the key table. It's super quick and find out what you should do. But ultimately this Q table would get bigger and bigger depending on the application. Right? And the number of states

Speaker 1:          00:26:44       for go is 10 to the power 117 approximately. Which means that this matrix should have a number of rows equal to 10 we'd want 170 zeros after. I mean you, you know what I mean? It's very big and number of actions is also going to be bigger and go, you can place your action everywhere on the board that is available of course. Okay. So many way too many states and actions. So we would need to come up with maybe a function approximator that's can give us the action based on the state instead of having to store these matrix. That's where deep learning would come. So just to reach out to these first 30 minutes, in terms of vocabulary, we learn what an environment is. It's that it's the general game definition or an agent is the thing. We're trying to train the decision maker, a state, an action reward, total return, a discount factor. The Q table, which is the matrix of entries were presenting hi is is to take action a in state s a policy, which is our decision making function, telling us what's the best strategy to apply in the state. And by many equation, which he starts his fight by d up similar to table and that way we'll tweak this cute table into ACU function and that's where we shift from to learning to deep cue learning. So final Q function to replace the shoe table.

Speaker 2:          00:28:11       Right.

Speaker 1:          00:28:12       Okay. So this is the setting. We have our problem statements. We have our crew table and we want to change it into a function approximator that we'd be on neural network.

Speaker 1:          00:28:23       Does that make sense? How deep yearning comes into reinforcement learning here? So now we take a state does inputs for propagated in the deep network and get an output, which is an action, an action score for all the actions it makes sense to have announced foods layer. That is the size of the number of actions because we don't want to, we don't want to give an action as input and the state has inputs and get the score for this action taken in the states. Instead we can be much quicker. You can just give the state has inputs, get all the distribution of scores over the output and we just select the maximum of this vector which will tell us which action is best. So we felt we were in states to, let's say here we're in state too and we Ford Probe I gates say too, we get to values which are the scores off going left and right from state to we can select the maximum of dose and it will give us our action. The question is how to train this network. We know how to trainings. We've been learning it for nine weeks. Computer loss, back propagate. Can you guys think of some issues that that make this setting different from a classic supervise learning setting?

Speaker 1:          00:29:51       The reward changes dynamically so the reward doesn't change. The reward is sets. You define it at the beginning. It doesn't change on empty, but I think what you meant is that the Q scores changed dynamically. Yup, that's true. The Q scores change dynamically, but that's probably okay because our network change that our network is now the Q score. So when we update department chairs of the network, it updates the Q scores. What's, what's another issue that we might have no labels. Remember in supervised learning, you need labels to train the network. What are the labels here and don't say compute the cute table. Use them as labels. So I've got to work. Okay, so that's the main issue that makes this problem very different from classic supervised learning. So let's see, uh, how, how deep learning can be tweaked a little. And we want you to see these techniques because they, they're helpful. When you read the variety of research papers we have our network, even a state gives us two scores that represent actions for going left and right from the states. The last function that will define is it a classification problem or a regression problem?

Speaker 1:          00:31:12       Regression problem. Because their Q score, it doesn't have to be a probably be the zero and one is just a score that you want to give in that she'd look, let you meet, make the longterm discounted, rewarding. So in fact the lost function we can use is

Speaker 1:          00:31:28       he's the l two loss function y minus the Q score squared. So let's say we do it for the queue going to the right, the question is what is why, what is the target for this Q? And remember what I could feed on the top of the slide is development equation. We know that the optimal cue should follow this equation. We know it. The problem is that these equations depends on its own queue. You know like you have to on both sides of the equation, it means if you said the label to be our plus gamma times Max of two stars, then when you, we'll back propagates. It will also have a derivative here. It's neat. Let me go into the dictates. Let's they find a target value. Let's assume that going, uh, left is better than going right at this point in time. So we initialize the network randomly. We four propagate stayed too in the network and the Q score four left is more than the Q score for rights. So that's the action that we will take at this point is going left. Let's define our targets. Why as the reward you get when you go left immediates plus gamma times do maximum

Speaker 1:          00:32:44       of all the Q values you get from the next step. So let me spend a little more time on that because it's a little complicated. I'm in s I moved to s next using a movie on the left I get immediate reward art and I also get to new state as prime as next I can Ford probe, I get this state in the network and they're just to, what is the maximum I can get from this state? Take the maximum value and plug it in here.

Speaker 1:          00:33:18       So this is hopefully what's the optimal [inaudible]? It's a proxy to a would label. It means we know that's development equation tells us the best shoe satisfies this equation. When you fuck this equation is not true yet because Detroit question, we have Q star here, not cue to start, which is the optimal cheap. What we hope is that if we use this proxy as our label and we learn the difference between where we are now and this proxy, we can then update the proxy, get closer to the optimality train again of the proxy, get closer to optimality, train again and so on. Our only hope is that this will converge. So does it make sense how this is different from deep printing? The labels are moving, they're not static labels.

Speaker 1:          00:34:12       We define a label to be a best guess of what would be the best to function we have. Then we come to the loss of where the to function is right now compared to cds, we backed by gates so that dual Q function gets closer to our best guess. Then now that we have a better cue function, we can have a better yes. So we make a better guess and we seek these guests and now we come to you. The difference between these two functions that we have and our best guess, we backed up, propagates up, we get to our best guess. We can update our best guests again and we hope that doing that it's relatively, we'll end with a convergence and a two function that will be very close to satisfied development equation. The optimal been winning equation. Does it make sense? This is the most complicated part of to learning. Yeah.

Speaker 3:          00:35:04       For the function we generate the output of the network, we get to kill function. We compare it to the queue. The best to function that we think is it is what is the best to function that the one that satisfies development equation, but we'd never actually think you did them both. Valid equation we don't want but we guess it's based on the queue we have. Okay, so basically when you have Q you can come to these bad men equation and it will give you some values. These values are probably closer to where you want to get to, where from where you are now, where you are now is farther from this optimality

Speaker 1:          00:35:38       and you want to reduce this gap by bye. Like to close the gap. You back propagate. Yes. Is it possible you need that is the ability to collaborate. So the question is, is there a possibility for these to diversity? So this is a broader discussion that would take a full lecture to prove. So I put a paper here from [inaudible] called mellow, which proves the convergence of this algorithm. So it converges and in fact it converges because we're using a lot of tips and tricks that we will see later. But if you want to see the math behind it and it's a, it's a full lecture of proof, I invite you to look at this simple, uh, proof for convergence of development equation. Okay? Okay. So this is the case where a left score is hired in rice score and we have two terms in our targets. You may get reward for taking action left and also discounting maximum future reward when you are in state. It's s next.

Speaker 1:          00:36:37       Okay. Did the tricky part is that, let's say we, we come to that, we can do it. We have everything, we have everything to complete our targets. We have art which is defined by the, by the human at the beginning. And we can also get this number because we know that if we take action lists we can then get s next and we forward propagates aesthetics into network. We take the maximum output and it's this. So we have everything in this in this equation. No problem. Now is if I plug this end, my shoe score in my loss function and I asked you to back propagate backpropagation is what w equals w minus alpha times the derivative of the loss function. When respect to w department is of the network, which term we're having non zero value, obviously the second term of s go to the left, we'll have the non zero value because it depends on the parameters of the network w but why we'll also have a non zero value because you have Q here.

Speaker 1:          00:37:42       So how would you handle that? You actually get a feedback loop in this backpropagation that makes the network unstable. What we do is that we consider this fixed. We will consider this Q fixed. The issue that is our target is going to be six for many duration, let's say a million or a hundred thousand nutrition until we get close to there and our gradient is small, then we'll update it and we'll fix it. So we actually have two networks in pilot. One that is fixed and one that is not fixed. Okay. And the second case is similar. If the Q score to go on the right was more than the Q score to go on the left, we would define our targets as immediate reward of going to the right plus gamma times. The maximum Q score we get if we're in the states that we in the next states and take the best action. Does this make

Speaker 1:          00:38:36       sense? Is the most complicated part of tutoring. This is the hard part to understand. So he made your tree walk to go to the rights and discounted maximum feature. We're one year in state s next going to dry. So this is hold six, four backdrop. So no derivative. If we do that, then no problem. Why is just a number? We come back to our origin of supervised learning setting. Why is a number and we come to the loss and we backed, propagate, no difference. Okay, so compute dear DL over DW and update w using stochastic gradient descent methods. RMS brought Adam whatever you guys want.

Speaker 1:          00:39:17       So let's go over this. These full you deep to network implementation. And this slide is a pseudo code to help you understand how this entire algorithm work. We will actually plug in many methods in this, in this pseudo code. So please focus right now and, and you should understand this, you understand the entire rest of the lecture. We initialize our tonight's network by amateurs. Just as we initialize the network in deep yearning, we loop over episodes. So let's define an episode to be one game. Like going from start to end to a terminal. State is one piece. Um, we can also define a of sometimes to be many states like breakouts, which is the game with the puddle usually is 20 points. The first day or two get 20 points, finishes the game. So at peace with it would be 20 points. Once you're looping over episode starts from an initial state s in our case it's only one initial states, which is state too. And Lupe overtime steps for propagate s state to India to network.

Speaker 2:          00:40:22       Wow.

Speaker 1:          00:40:22       Execute action a which has the maximum Q score, observe or immediate reward are. And the next step is prime.

Speaker 2:          00:40:32       Okay.

Speaker 1:          00:40:33       Compute target. Why? And to compute why we know that we need to take as prime for propagates it in the network again and then continue the loss function of the department. There's with gradient descent, there was this loop. Makes Sense. It's very close to what we do in general. The only difference would be the sports. Like we come to target, why? Using a double for publication. So we formed propagation with four propagate two times in each loop. You guys have any questions on on this pseudo code? Okay, so we will now see a concrete application of Egyptian networks. So this was the theoretical partner. We're going to, the practical part, she's going to be to be more fun. So let's look at this game. It's called breakouts. The goal when you played breakouts is to destroy all the bricks without having bolt pass the line on the bottom. So we have a pedal and our decisions can be idle, stay, stay where you are, moved to pedal to the right or move the paddle to the left, right?

Speaker 1:          00:41:49       And these demo and you have the credits on the bottom of the slide. A shows that after training breakouts, using Cuellar and ink, they get a super intelligent agents, which figures out the trick to finish the game very quickly. So actually even like the good players didn't, don't know district professional failures, no district. But uh, in breakout you can actually try to dig a tunnel to get on the other side of the bricks and then you will destroy all the brick super quickly from top to bottom instead of bottom up. What's super interesting is that the network, figure it out this on its own without human supervision. And this is the kind of thing we want to remember. If we were to use inputs, the goal boards and output professional players, we will not figure out that type of stuff most of the time. So my question is what's the input of the acute network in this setting or goal is to destroy all the bricks. So play breakouts.

Speaker 2:          00:42:56       Okay.

Speaker 1:          00:42:58       What should be the inputs?

Speaker 2:          00:43:11       Try something. The board at the bottom, it should have breaks.

Speaker 1:          00:43:19       Position of the puddle of the bricks. What else? Ball position.

Speaker 2:          00:43:23       Okay.

Speaker 1:          00:43:24       Yeah, I agree. So this is what we would call a future representation. It means when you're in an environment you can extract some features, right? And these are examples of features give you the position of the ball is one feature. Give you the position of the bricks and other feature. Give me the position of the paddle and other feature, which are good features for this game. But if you want to get the entire information, you'd better do something else.

Speaker 1:          00:43:54       Yeah, the big cells, you don't want any human supervision. You don't want to put seizures, you just okay, take the pixels. They do game, you can control the paddle, take the pixel. So yeah, this is a good input to the queue network. What's the outputs? I said it earlier, probably the output of the network will be tricks you values representing the action going left, going right and staying idle in a specific states. That is the input of the network. So give it a pixel image. We want to predict Q scores for the tree. Possible actions. Now what's the issue with that? Do you think that would work or no?

Speaker 2:          00:44:40       Okay.

Speaker 1:          00:44:42       10 someone think of something going wrong here. You can get the inputs.

Speaker 2:          00:44:59       Yep.

Speaker 1:          00:45:01       Okay. I'm going to have to uh, if I need you. Yeah, you're on the train. Oh yeah. Good points. Based on this image, you cannot know if the ball is going up or down. It's like Jane, it's super hard because the action you take highly depends on who the girl is going up or down. Right? Is that ball is going down and even he, what he's going down and you don't even know which dereg direction he's going. Yeah. So there is a problem here. Definitely there is not enough information to make a decision on the actions to take. And if it's hard for us, he's going to be hard for her to network. So what's the hot too to prevent that, it's to take successive frames. So instead of one frame we can take four frames. Success, he freaks and here the same setting as we had before, but we see that the ball is going up, we see which direction is going up and we know what action we should tell you because we know the slope of the bowl and also uh, also if it's going up or down.

Speaker 2:          00:46:02       Yeah,

Speaker 1:          00:46:02       that makes sense.

Speaker 2:          00:46:03       Okay.

Speaker 1:          00:46:05       Okay, so this is called the preprocessing. You've under state computer function, fight of s that gives you the history of this state, which is the four sequence of four last frames. What other preprocessing can we do? And this is something I want you to be quick, like we learned it together in deep learning, input, preprocessing. You remember the second lecture where the question was what's resolution should we use? Remember you as a catcher got mission, what's resolution would you want to use? Here's same thing. If we can reduce the size of the inputs, let's do it. If we don't need all that inflammation, let's do it. For example, do you think the colors are important?

Speaker 2:          00:46:57       Okay,

Speaker 1:          00:46:58       very minor. I don't think they're important. So maybe we can gray scale everything that removes three chat, that's converse three channels into one channel, which is amazing in terms of computation.

Speaker 1:          00:47:10       What else? I think we can crop a lot of this. Like maybe there's a line here. We don't need to make any decision. We don't need the scores. Maybe. So actually there's some games where the score is important for decision making. An example is football or soccer. Uh, when, when you're winning one zero you, you'd better if you were playing against a strong theme, defend, like get back and defend to keep these one zero. So the score is actually important in the decision making process. And in fact, uh, there are famous coach in football which have this technique called park the bus where you just put all your team in front of the goal once you have scored a goal. So this is an example. So here there's no park, the bus, but we can definitely get rid of the score which removes some pixels and reduces the number of computations

Speaker 1:          00:48:06       and we can reduce to gray scale. One important thing to be careful about when you reduce your regret scale is that gray scale is a dimension illiterate Dixon technique. It means you lose information, but you know if you have three channels and you reduce everything in one channel, sometimes you would have different color pixels which will end up with the same gray scale value depending on the gray scale that you use and it's been seen that you lose some information sometimes. So let's say the ball and some bricks have the same gray scale value, then you would not differentiate them or let's say the paddle and the background have the same gray scale value, then you would not differentiate them. So you have to be careful of that type of stuff. And there's other methods that do gray scale and not other ways like illuminates. So we have our five s which is this, which is this input to the queue network. And the deep Q network architecture is going to be a convolutional neural network because we're working with the images. So we propagate dots. This is the architecture from mean government. So glue silver at all from the point [inaudible] Relu to fully connected layers and you get your Q scores

Speaker 1:          00:49:13       and we get back to our training loop. So what do we need to change in our training loop here is we said that one frame is not enough. So we preprocess all the frames. So the initial state is converted to five of us. The four propagation did state is five of us and so on. So everywhere we had s or s prime, we convert two five s or five s prime, which gives us the story. Now there are a lot more techniques that we can plug in here and we will see three more. One is keeping track of the terminal states in the slope. We should keep track of the terminal state because we said if we reach a terminal state, we want to end the loop, break the loop and not the reason is because the wife function. So basically we have to add, create a Boolean to detect the terminal states before looping through the time steps and inside the loop we want to check if the new s prime we're going to is a terminal states.

Speaker 1:          00:50:10       If it's a terminal state, then I can stop the sloop and go back, play another APP visit. So play another start. That's another starting states and continuing my game now, this wide target that we compute is different. If we're in a terminal state are nuts because if we were at 10 real estate, there is no reason to have a discounted longterm reward. There's nothing behind that term real estate. So if we're in turmoil sake, we just said it to the immediate reward and we break. If we're not in a terminal state, then we would add these discounted future reward. Any questions on that?

Speaker 1:          00:50:51       Yup. Another issue that we're seeing these and which makes, uh, eastern enforcement learning setting super different from the classic supervised learning setting is that we only train on what we explore. It means I'm starting, you know, state s I come to [inaudible] forward [inaudible] this fire of us in my network. I get my vector of two values. I select the best Q value, they're our largest, I get the new states because I can move now from state sts, Brian. So I have a transition from s take action, a get as prime or fire of us. They actually ate it. Five of us, Brian. Now this is what I will use to train my network. I can forward propagate five s prime again in the network and get my why targets come from my why to my queue and then back propagate. The issue is I may never explore the state transition again.

Speaker 1:          00:51:52       Maybe I will never get there anymore. It's super different from what we do in supervised learning where you have a datasets and your data set can be used. Many times we batch grading dissent or with any gradient, decent algorithm. One epoch, you see all the data points. So you should do to eight bucks. You see everyday two points, two times. If you do 10 bucks, you see everyday to put straight three times or 10 times. So it means that everyday two points can be used several times to train your algorithm in classic deep yearning that we've seen together. In this case it doesn't seem possible because we only train when we explore and we might never get back there, especially because the training will be influenced by where we go. So maybe there's some places where we will never go because why we train and why we learn it will, it will kind of direct or decision process and we will never train on some parts of the game.

Speaker 1:          00:52:40       So this is why we have other techniques to keep these training stable. One is called experience replay. So as I said, here's what we are currently doing. We have five of us, four propagates get a from taking action. A we observe an immediate reward are and a new states firewise prime. Then from Firebase prime which can take a new action, a prime observer, a new reward or prime and the new state fire of us prime, prime and so on and each of these is called a state transition and can be used to train. This is one experience leads to one iteration of gradient descent,

Speaker 1:          00:53:23       e one e two e three experience, one experience to experienced tree and the training will be trained on experience one then train on experience to then try not experience street. What we're doing with experienced replay is the following. We will observe experienced one because we started to say we take an action, we see another state and really reward and this is called experience one. We'll create a replay memory. You can think of it as a data structure in computer science and you will place this experience one tapo in this replay memory. Then from there we will experience experience too. We'll put the experience to indirect play memory. Same with experienced tree, put it in the replay number and so on. Now during training, what we will do is we will first train on experience one because it's just only experience we have. So so far.

Speaker 1:          00:54:13       Next step instead of training on e two we will train on a sample from [inaudible]. It means we will take one out of the replay memory and use this one for training, but we will still continue to experiment something else and we'll sample from there. And at every step the replay memory will become bigger and bigger. And why we train, we will not necessarily train on the step we explore, we will train on a sample which is the replay memory plus the new state we we we explore why is it good is because he won as you see can be useful many times in the training and maybe one was a critical state, like it was a very important data point to learn or Q function and so on. And so does the repayment where it makes sense. So several advantages. One is data efficiency. We can use data many times don't have to use one data point only one time.

Speaker 1:          00:55:08       Another very important advantage of experience replay is that if you don't use experience replay, you have a lot of correlation between the successive data points. So let's say the ball is on the bottom right here and the ball is going to the top left for the next 10 data points. The bull is always going to go to the top left and it means the action you can take is always the same. It actually doesn't matter a lot because the ball is going up, but most likely you want to follow the where the ball is going. So the action will be to go towards the ball for 10 actions in a row and then the ro the ball would bounce on the wall and on the top and go back down here down to the bottom left bottom right. What will happen if you're a pilot is here, is that for 10 steps in euro, you will send your paddle on the right.

Speaker 1:          00:56:04       Remember what we said when, which, when we ask the question, if you had to train a chat vs. Dot. Classifier with batches of images of cats, batches of images of dog trained first on the cats, then trains on the dogs than trends on the cats. Then trends on the dogs. We will not converge because your network will be super biased towards predicting cats after seeing 10 images of cats. Super bias, we predicting dogs when sees 10 images of dogs. That's what's happening here. So you want to correlate all these experiences. You want to be able to take one experience, take another one that has nothing to do with it and so on. This is what experience purely platos. And the third one is that the third one is that, uh, you're basically trading computation and memory against exploration. Exploration is super costly. The state space might be super big, but you know, you have enough computation probably you can have a lot of competition and you have memory space. Let's use an experienced replay.

Speaker 4:          00:57:02       Okay,

Speaker 1:          00:57:03       so let's address experience replay to our code here. The transition resulting from this part is added to the experience to the replay memory d and will not necessarily be using nutrition space. So what's happening is we're four propagate, five of us, we get, we observe a reward and an action, and this action leads to a state five s prime. This is an experience instead of training on this experience, I'm just going to take it, put it in the replay memory, add experience to replay memory. And what I we train on is not, this experience is a sample random minibatch of transition from the replay memory. So you see we're exploring, but we're not training on what we explore. We're training on the replay memory, but the replacement where he's dynamic, it changes

Speaker 1:          00:57:54       and updates using the sample transitions. So the sample transition from the replacement, we will be used to do the updates. That's the huck. Now another hot, we want the last hack. We want to talk about these exploration versus exploitation. So as a human, let's say you're commuting to Stanford every day and you know the road you were commuting at, you know it, you always take the same road and your bias towards taking this road. Why? Because the first time you took it to 2012 and the more you take it no more you learn about it. Not that it's good to know the tricks of how to drive fast, but there's like you know the tricks, you know that this, this, these slides is going to be green at that moment and so on. So you, you, you build a very good expertise in this road, super expert, but maybe there's another road that you don't want to try it.

Speaker 1:          00:58:44       That is better. You just don't try it because you're focused on that road. You're doing exploitation, you exploit what you already know. Exploration would be, okay, let's do it. I'm going to try another road today. I might get late to the course but maybe I will have a good discovery and I would like this road and then we'll take it later on. There's a trade off between these two because the aural algorithm is going to figure out some strategies that are super good and we'll try to do local search in this to get better and better. But you might have another minimum that is better than this one and you don't explore it using the algorithm we currently have. There is no tradeoff between exploitation and exploration. We are almost doing only exploitation. So how to incentivize these exploration. You guys have an idea.

Speaker 3:          00:59:45       So right now when we were in a state s were for propagating the state's purposes states in the network and we take the action that is the best action always. So we exploiting, we're exploiting what we already know. We take the best action. Instead of teaching these best action, what can we do? Yup. Monte Carlo sampling, we points another one. You wanted to try something else. It's the ratio of times you take the best action versus exploring another action. Okay. Take a pipe or parameter that tells you when you can explore, when you can exploit that what you mean. Yeah, that's a good point. So I think that that's

Speaker 1:          01:00:29       the solution. You can take an eyebrow primary two, that is a probability telling you with this probability explore. Otherwise we'd one minus it this probably t exploit. That's what, that's what we're going to do. So let's look why exploration versus explanation doesn't work. We're in initial state one s one and we have three options. Either we go using action a one two s two and we get rewarded of zero or we go to action. Use Action to get to as three and get reward of one or use action tree and go to s four and get the reward of 1000 so this is obviously where we want to go. We want to go to s four because it has the maximum reward and we don't need to do much computation in our heads. It's simple. There is no discount. It's direct. Just after initializing the Q networks, you get the following key values for propagates as one in the queue network and get 0.54 taking action 1.44 take action. 2.3 for texting action three. So this is obviously not good, but our network, he was randomly initialized. What it's telling us is that 0.5 is the maximum. So we should take action one. So let's go take action one observe as to your observer. Reward of zero. Our targets, because it's a terminal state is only equal to the reward. There's no additional term. So we want our target too much. Our Queue, our target is zero so Q should match zero. So we train and we get the cue. That should be zero. Does that make sense?

Speaker 1:          01:02:06       Now we do another round of iteration. We look, we're in s one we get back to the beginning of the episode. We see that or chew function tells us that action two is the best because 0.4 is the maximum. It means go to a street. I go to a street, I observed reward of one. What does it mean? It's a terminal state. So my targets is one. Why you called [inaudible]? I wanted to acute too much, my wife, so my cue should be one. Now I continue. Third step up to function. Ses, go to Aa to I go to eight to nothing happens. I already matched the dirty work for step, go to [inaudible]. You see what happens? We never go there. We'll never get there because we're not exploring. So instead of doing that, what we were saying is that 5% of the time take a random action to explore and 95% of the time follow your exploitation.

Speaker 1:          01:03:04       Okay? So that's where we added, we've probably to epsilon the hyper parameter, take random action eight, otherwise do what we were doing before exploit. Does that make sense? Okay, cool. So now we plugged in all these tricks in our pseudo code and this is our new sudo code. So we have to initialize a replay memory, which we didn't have to do earlier in blue. And you can find the replay memory. I did lines in orange, you can find the ID lines for checking the terminal state. And in purple you can find the, I did lines related to epsilon greedy, uh, exploration versus exploitation and find it in bold, the preprocessing. Any questions on that? So that, that's, that's we want you to see a volume into of how deep learning can be used in the setting that is not necessarily classic supervisor and exotic. And you see that domain advantage vintage of deep learning in this case is it's a good function approximator you'll convolutional neural network can extract a lot of information from the pixels that we were not able to get with other networks. Okay, so let's, let's see what we have here. We have our Super Atari butts that's going to dig a tunnel and he's going to destroy all the brick super quick.

Speaker 1:          01:04:33       It's cool to see that after building it like so this is work from deep mines team and that you can find these video on youtube. Okay. Another thing I wanted to say quickly is what's the difference between with and without human knowledge? You will see a lot of people, a lot of papers mentioning that this algorithm was trained with human learn knowledge or this algorithm was trained without any human in the loop. Why is human knowledge very important? Like think about it. Just playing one game as a human in teaching that to the algorithm will help the algorithm a lot. When the algorithm sees this gay, what it sees, it's pixels. What do we see when we see that game? We see that there is a key here. We know the key is usually a good thing. So we have a lot of context, right?

Speaker 1:          01:05:21       As a human, we know I'm probably gonna go for the key. I'm not going to go for this, this thing. No. Uh, same leather. What is the ladder? We directly identified that the ladder is something we can go up and down. We identified that this rope is probably something I can use to jump from one site to the other. So as a human, there's a lot more background information that we have even without knowing it, without realizing it. So there's a huge difference between algorithms trained with human in the loop and without human in the loop. This game is actually Montezuma revenge, the DQA and algorithm. When the paper came out on, on the nature or nature in nature, then the second, the second version of the paper, they showed that they beat human on 49 games that are the same type of games. I was very careful, but this one was the hardest one.

Speaker 1:          01:06:07       So they couldn't beat human on this one. And the reason was because there's a lot of information and also the game has is very long. So in order it's called Montezuma revenge. And I think rounding Carol Marty is going to talk about Italy till later, but in order to get to win this game, you have to go through a lot of different stages and it's super long. So it's super hard for the, the, the algorithm to explore all the state space. Okay. So that said, I will show you a few more games that, that uh, the deepmind team has solved. Pong is one sequence is another one and space invaders that you might know which, which is probably the most famous of the tree. What Gino. Okay, so that said, I'm going to hand in the microphone too. We're lucky to have a neural expert. So Rom team, Tara Martiza, first year PhD students, uh, in Rl, uh, working with prophecy brunskill at Stanford. And uh, he will tell us a little bit about his experience and he will show us some advanced applications of deep learning and RL and how these plugin

Speaker 5:          01:07:17       to get her. Thank you. Thanks for that intro. Lecture. Oh yeah. Uh, can everyone hear me now? All right, good. Cool. I get can first I have like eight, nine minutes. Uh, have more. Okay, great. Uh, okay. First question after seeing that lecture so far, like how many are, you're thinking that Earl is actually cool? Like honestly, that's like, oh, that's a lot. Oh yeah, that's a lot. Okay. My hope is after showing you some other advanced topics years, then the percentage go even increase. So let's, uh, let's see. Uh, it's almost impossible to talk about advancement oral look recently without mentioning Alphago. I think somewhere right now who wrote that on the table that it's almost tend to the, uh, public hundred and 70 different configuration of the board. And that's roughly mode that, I mean, that's more than the estimated number of atoms in the universe.

Speaker 5:          01:08:20       So avant traditional algorithm, but before the planning and stuff like that was like two searching RL, which is basically go exhaustively search all the other possible action that you can take and then take the best one in that situation. All forgot. That's all almost impossible. So what they do that's also a paper from deep mind is that they train, uh, and we were on this before that they kind of marriage the two research with deep, deep neural network that they have. They have two kinds of networks. One is called the value network value network is basically consuming this image, a image of a book and telling you what's the probability that if you can win in this situation, so if the value is higher than the probability of winning is higher, how does it help you? He'll help you in the case that if you want to search for the action, you don't have to go until the end of the game because the end of the game is a lot of steps and it's almost impossible to go to the end of the gaming all the simulations.

Speaker 5:          01:09:21       So that just helps you to understand what's the value of each game like beforehand, like after it for the 50th state, if you're going to win that game of, or if you're going to lose that game, there is another. And let's rock of the Policy Network, which helps us to take action. But I think the most interesting thing of the Alphago is that it's trained from scratch. So trends from nothing and they have, it's really called self play that I did is to AI playing with each other. The best one refugee, the West, the best one, I keep it fixed. And I have another one that is trying to copy it, the Treevis version of itself and after it's complete, the previous version of itself, like reliably, many times that I replace this again for the previous one and that I just said. So this is a training care of Africa itself. Self play off the Alphago as we say, and it takes a lot of compute. So that's kind of crazy. But finally they beat the human.

Speaker 5:          01:10:19       Okay. And other type of algorithm that this is like the whole different class of algorithm called policy gradients. Uh, algorithm called trust region. Policia. Oh, can I stop the massive resistance, locomotion controller. Right. Can you renewed the sound please? Okay, great. So policy Garrick an algorithm, well I can do is that we stop this from here. Uh, no, there's not a piece of work. Okay. So here, like in the Dick Shoe and that you've seen, uh, you, you came and the compute, the Q value of feature state and then what you have done is that you take the Arg Max of this with respect to action and then you choose the action that you want choose, right? But what chair at the end of the day is the action, which is the mapping from state to action, which if we call it a good policy, right? So did you care at the end of the day actually the policy, like what actions should they take is not really Q value itself, right?

Speaker 5:          01:11:22       So this class, a class of methods that call it policy gradients is trying to directly optimize for the policy. So rather than update the two function, I can't put a gradient of my policy or updating the policy network again and again and again. So let's see these videos. So this is like this guy that is trying to reach the pink bottle over there and sometimes it gets hit by the some external forces and this is called hover off the algorithm called the PPO policy gradient. I try to reach to that book. So I think that you've heard of uh, open AI like five, like the, but that is playing Dota. So this is like completely like Tpu Allard and they have like a lot of the compute to show in that. And I guess I have the numbers here. There's 180 years of play in one day.

Speaker 5:          01:12:21       This is how much copy would be. Uh, so that's fine. Uh, this another even funnier Vido Paul competencies again, the same idea, but it's fun. It's the gate yet is that you put inside of that and do it. I did slide boots on it and if they did each other, they get to be one that the most interesting cloud is up there was applicant that day. The purpose is just to, to pull the other one out. Right. But they understand some emergent behavior. This is for us. Makes Sense. But for them to learn out of nothing. It's kind of cool. So there's like one risk here that when they're playing they say this guy's trying to kick the ball inside the vine vine risk areas to overfit.

Speaker 2:          01:13:27       Yeah,

Speaker 5:          01:13:28       that's also cool. Fun Technical Point. Before I move on, I think the whole point here, is that here or is that not the next slide. Okay. He had that two to age playing with each other and we are updating the person with the best other agent previously if you're doing self play is that you overfeed to the actual agent that you're in front of you. So the agent in front of you is powerful, but you might over fit to this. And if I put the agent that is not that powerful, but it's using this simple trick that the polar from agent that can never users, then you might just lose it again. Right? So once a week here to make it more is that rather than playing against only one agent, you alternate between different version of the agent itself. So it all lands all the skills together.

Speaker 5:          01:14:21       It doesn't overfit to this stuff. So there's another uh, thing, conduct Meta learning, math or learning is a whole different algorithms again and uh, I, the purpose is that a lot of tasks are look similar to each other, right? Because apple watching two left, I'm working to write and like working in different direction. They're like same test essentially. So the point is rather than training on a single task, which is like go left or go right, you train a distribution of tests that are similar to each other and then the idea is that for each specific task I should learn with like a very few gradient of steps. So very few updates should be enough for me. So if I learn play this year is like at the beginning of decision that has been trained with metal learning before, it doesn't know how to move, but just look at the number of gradient of steps like after two or three games and stuffs, it totally knows how to move. That's, that's normally it takes a lot of steps to train. But that's only because of the amateur learning approach that we've used here. Metal learning is also cool. I mean, the algorithm is from Berkeley, Josephine, she's now also coming to Stanford is called model agnostic matter learning.

Speaker 5:          01:15:34       So all right, and other point, this a very interesting game. Montezuma's revenge that talk how much time they have. Uh, so you've seen uh, exploration, exploitation dilemma, right? So it's, it's, it's the bad if you don't exploit, you're gonna fail many times. So if you do the exploration with the scheme that you just saw that this is a map of the game and you got to see it off that game. If you like expiration randomly, and my thing has like 21 or 20 something different, there's hard to reach. So there's this recent paper I think from Google brain, for Mark [inaudible] team, it's called a unifying the canvas methods for exploration. Exploration is essentially very hard challenge, mostly in the situation that the reward is a sparse for exactly in this game. The first reward that you get is when you reach the key right from soft to here. It's almost like 200 steps. And getting the number of actions after 200 steps exactly right by like random exploration is almost impossible. So you never going to do that. What a very interesting trick here is that you kind of keep a count on how many times you've visited the states and then if you visit a state that is uh, that has the fewer accounts, then you give you a reward to the agent. So we call it the intrinsic reward. So it's kind of the,

Speaker 2:          01:17:11       okay, so he changes it

Speaker 5:          01:17:31       because he treats the cows of the statement and has never seen before. So this gets the agent I should explore. Oh. So he just goes down visit like different rooms and stuff like that. So I think this, this game is interested in, you just started a lot of people that need to solve the game has a huge research book. I get the highest amount of this game and it's just fun also to see the agent play.

Speaker 2:          01:17:59       Any questions on that?

Speaker 5:          01:18:14       There is also another interesting

Speaker 5:          01:18:21       point, bad would be just fun to know about is called imitation learning. Imitation learning is the case that, well, I mean oral agent saw, sometimes you don't know the river. Like for example in Atari Games they'll devolve is the cook very well defined, right? If I get the key, I get the reward that is obvious. But sometimes like defining the Revard, it's hard. For example, when the car like the blue one one a drive in, uh, in some high value, what is the definition of the river? Right? So we don't have a clear definition of that. But on the other hand, we had a person, like you have human experts that can drive for us. And then we see, oh, this is the right way of driving, right? So in this situation we have something called imitation learning that we tried to mimic the behavior of experts. So not exactly copying this because if we copy this and then you show us at completely different states that we don't know what to do. But from now we learn, and this is like my example, and there's a paper that called journey to the sordid limitation learning, which was like from Stefanos group here at Stanford. And that was also interesting.

Speaker 2:          01:19:25       Well, I think that's advanced topic. If you have any questions, I'm here. Can question. No

Speaker 3:          01:19:40       for next week. So there's no assignments you guys have to finish by models. Now we're wanting to start the beat partners. As you know, there is going to be a project teenager reading research papers. Can we go over in the object detection? It'll be two papers from Redlands.