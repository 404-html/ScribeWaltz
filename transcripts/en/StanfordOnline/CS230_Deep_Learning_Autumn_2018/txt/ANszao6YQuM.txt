Speaker 1:          00:00:05       Okay, let's get started guys. So welcome to lecture number four. Um, today we will go over to topics that are not discussed in the Coursera videos. Uh, you've been learning c two m one and c two M two, if I'm not mistaking. So you've learned about, uh, what, uh, an initialization is, how to tune your own networks, what tests, validation and train sets are today we're going to go a little further or you should have the background to understand 80% of this lecture. There's going to be 20% that I want you to look back after you've seen the batch norm videos for those of you who haven't seen them. So we feed the lecture in two parts and I put back the attendance code at the, at the very end of the lecture. So don't worry. Uh, one topic is attacking a neural networks, a weed adversarial examples.

Speaker 1:          00:00:59       Uh, the second one is generative adversarial networks. And although these two topics have a common word which is adversarial, there are two separate topics. You will understand why it's called adversarial in both cases. So let's get started with adversarial examples. And in 2013, uh, Christians negative and his team have a published a paper called intriguing properties of neural networks. What they noticed is that neural networks, neural networks have kind of a blind spot, a spot for which several machine learning, including the state of the art ones that you will learn about. Uh, VGG 1619 inception, uh, networks and raise residual networks are vulnerable to something called adversarial examples. These adversarial examples, you're going to learn what it is in three parts first, by explaining how these examples in the context of images can attack your network in their blind spots and, and make the network classify these images as something totally wrong, how to defend against these type of examples and why our networks vulnerable to these type of examples.

Speaker 1:          00:02:13       This is a little bit more theoretical and we're going to go over it on the board. The, the papers that are listed on the bottom are the two big papers that, that started this field of research. So I would advise you to go and read them because we have only one hour and a half to go over two big topics. Um, in, in deep learning and, uh, we will not have the time to go into details of everything. Okay. So let's set up the goal. The goal is like, is that given a pretrained network? So a network trained on image net on a thousand classes, millions of images, uh, find an input image that is not only Guana so it doesn't look like the Animal Iguana. Uh, but we be classified by the network as an Iguanas. We call this an adversarial example if we manage to find it. Okay. Yeah. One question

Speaker 2:          00:03:05       I love this came, so 28, 48, 89 opinion writing down on the board. Thank you.

Speaker 3:          00:03:17       Yes.

Speaker 1:          00:03:19       Can you guys see, okay, that's more so we have a network pretrained on your major. It's a very good network. Uh, what I want is too full it by giving it to an image that doesn't look like anyone. Nobody's classified as anyone. So if they give it to cattle image to start with, the network is obviously going to give me a vector of probabilities that has the maximum probability for cats because it's a good network. And you can guess what's the output layer of this network is probably a soft Max. It's a classification network. Now what I want is to find then you make x that is going to be classified as an, you'd go on up. I do network. Okay. Does the, the setting make sense to everyone? Okay. Now as usual, uh, this, this, this might remind you of what we've seen together about neural style transfer.

Speaker 1:          00:04:16       You remember our generation thing where we want it to generate an image based on the content of the first image and the style of another image. And in that problem, the main difference with classic supervised learning was that we fixed department chairs of the network, which was also pretrained and we backed, propagate the error of the loss all the way back to the input image to update the pixels so that it looks like the content of your content image and the style of the style image. The first thing we did is that we rephrased the problem. We tried to to, to phrase what exactly we want. So what, what would you say is a sentence that defines or loss function? Let's see.

Speaker 3:          00:05:07       Yes. Yep.

Speaker 2:          00:05:17       And you mentioned that provides minimum cost and he made that provides minimum cost. Okay. What's the cost? You were talking about Costco did the difference between the expected decline and not expect to be expected?

Speaker 1:          00:05:29       Gwen, I know unexpected you go out. What? What do you mean exactly by that?

Speaker 2:          00:05:32       So we're sort of going back to the training stage and we were trying to train it on it. We really want him to think that this is the tap water. Yeah.

Speaker 1:          00:05:42       Okay. So you want a decent age to minimize a certain loss function and the loss function would be the distance metric between the output you're looking for and yeah, always put you want. Okay. Yeah. So I would say we want to find x, the image that that why hat of x, which is the result of the forward propagation of x into network is equal to y e Guana, which is a one hodge vector. We do one I did position of ego on. Does that make sense? So now based on that we defined our loss function, which is can be an l two loss, can be an l one loss can be across entropy in practice, uh, this one works better. So you see that minimizing these lost function. We lead our image x to be outfitted as any Guana by the network. That makes sense. And then the process is very similar to neuro side transfer where we will optimize the image iteratively. So we will start with x when we forward propagates it. The loss function that we just defined. And remember we're not training the network, right? We just take the derivative of the loss function all the way back to the inputs and update the input using a graduate decent algorithm until we get something that's east classified as anyone. Yeah. Any question on that? But this doesn't necessarily mean that the,

Speaker 3:          00:07:05       okay,

Speaker 1:          00:07:06       so you mentioned that it doesn't guarantee that x is Lou going to look like something. The only thing is guaranteeing is that this x will be classified as any Guana if we train property where we, we've talked about that. Now another question in the back I thought yeah,

Speaker 3:          00:07:23       or logistic regression.

Speaker 1:          00:07:26       Oh yeah. It could be binary croissant. It couldn't decrease entre B. Yeah. So in this case, not binary cross entropy because we have a uh, a vector of, of any classes. I mean it could have been Chris Entrepreneur. Okay. So yeah, that's true. We are, we going to need that, the forge image x. This one is going to look like anyone who thinks it's going to look like an Iguana. Who thinks it's not going to look like anyone. Okay. Mine's retail fuel. So can someone tell me why it's not going to look like anyone else?

Speaker 3:          00:08:11       Right.

Speaker 1:          00:08:14       Okay. So you're saying the last function is unconstrained is very unconstrained. So we didn't put any constraints on what the image should look like. That's true. Actually, the answer to this question is, it depends. We don't know. Maybe it looks like any one or maybe does it, but in terms of property, it is, it's high chance that it doesn't look like anyone. So there isn't, is here, let's say this is our space of input images. And the interesting thing is that even if as human on a daily basis, we deal with images of the real world. So like, I mean if you look at the TV, uh, that is totally buggy. You see pixels, random pixels. But in other contexts we usually see real word distribution images. My networking deterministic, it means it takes any image, any input image that fits the, the first layer would, would, would produce an output, right? So this is the whole space of input images. That's the network can see, um, this is the space of real images. It's a lot smaller. Can someone tell me what's the size of the, the, the space of possible input images for network?

Speaker 2:          00:09:19       Sorry, infinite. It's not influenced a lot, but uh, yeah, there is an idea here. Someone said the same thing. We just number of possible permutations. Yeah, that's true. So more precisely, you would start with

Speaker 1:          00:09:39       how many pixels values are there. There are 255, 256 pixel values. And then what's the size of an image? Let's say 64 by 64 by three. And your results would give you 256 so you fixed the first pixel, 206 is 56 possible value. Then the second one can be anything else. Then the third one can be anything else and you end up with a very big number. So this is a huge number and the space of real images is here. Now, if we had to plot to the space of off images classified as a need Guana it would be something like that. Right? And you see that there's a small overlap between the space of real images and the space of images. Classic for advice as a new Guana by doing network. And this is where we probably are nuts. We're probably in the green part that is not overlapping with the red part because we didn't constrain our optimization problems. Does that make sense? Okay. Now we're going to constrain it a little bit more because in practice, these type of attacks or not too dangerous because as a, as a human, we would see that the pictures look like garbage. The dangerous attack is if the picture looks like a cat, but the network sees it as an Iguanas and humans seat as a cat. Can someone think of uh, of like malicious applications of that

Speaker 2:          00:11:02       face recognition? You could show a face of you. You could show your picture of your face with push the networks. I think it's a face of someone else. What else? Yeah, breaking captures and breaking. Breaking as if you know what the output, what output you want. You can force the networks.

Speaker 1:          00:11:27       I ain't got this capture a DDS in bootcamp shot is the output it's speaking for or in general? I would say like social medias. If someone is managed and wants to put a violent content online, there is all these companies have algorithms to check for these violent content. If people can use adversarial examples that look still violence but are not detected as violence by the algorithms using methodology. Nico still publish their violence pictures. Think about self driving cars. A stop sign that looks like a stop sign for everyone. But when the self driving car sees it, it's not a stop sign. So these are my issues, applications of adversarial examples and there are a lot more. Okay. And in fact the picture we generated previously would look like that. It's nothing special. So now let's constraint or problem a little bit more. We're going to say we want a picture to look like a cat, but the classified as a need one. Okay, so now same. We have our neural network. If we give you the cat is going to predict that it's a cat. What we want is still give you two cats but predict that it's anyone.

Speaker 3:          00:12:38       Okay.

Speaker 1:          00:12:40       I go quickly over that because it's very similar to what we did before. So I just plot. I just put back what we had on the previous slide. Okay. Exactly the same thing. No. The way we rephrase or problem, we'd be a little different. Instead of saying we want only y hat of x equals y you wanna we have another constraint. The other constraint,

Speaker 3:          00:13:04       yes.

Speaker 2:          00:13:08       Did feature x should be closer to the teacher of the cats. So we want x equal or very close to x.

Speaker 1:          00:13:16       And in terms of loss function, what it does is that it adds another term which is going to decide how x should be close to x [inaudible] if we minimize this loss, now we should have an image that looks like a cat because of the second term and that is predicted as any Guana because of the first term. Does that make sense? So we're just building up our last functions and I guess you guys are very familiar with these type of thought process now. Okay. And same process we optimize until we hopefully get, okay now the question is what should be the initial image we start with? Well you didn't talk about that in your previous example. Yeah. White noise.

Speaker 2:          00:14:06       Yeah. Possibly white noise. Any other cat? Which cats? I Dunno. Probably the cat that we put in the loss function. Right? Cause he's the closest one to what we want to get. So if we want to have a fast process, we'd better start with

Speaker 1:          00:14:29       Xochi Scott, which is the one we put you in our last function here. Right? If we put an under cat is going to be a little longer because we have to change the pixel of the other cat to look like this cat. That's what we told our loss function. If we start with white noise, it will take even longer because we have to change the pixels all the way so that it looks real and then it looks like a cat that we defined here. So yeah, the best thing would be probably need to start with the picture of the cat. Does that make sense? And then move the Pixel. So that's, this term is also minimized. Yup. So when you're ready,

Speaker 4:          00:15:04       it seems like you have facility saying what does human sees as a cat will just be like minimizing the IMSE air to the actual cat picture. Right. Is that, I thought the RMSC era was actually a really bad way to gauge whether or not as unit like sauce with images is similar. Yeah. Did you see

Speaker 1:          00:15:23       empirical, the fact that we use that type of of loss function, but in practice it could have been any distance between x and x cat and any distance between who I heard Tim, why cats. Yeah, and why you're going elsewhere. Yes.

Speaker 4:          00:15:36       So what do you see? One specific care, exactly. Take conscious loss function, but it's a bunch of cats then it's like something that could meet him on get right. Can we, is that is why he recommended a book is a big run on cat. Um, sure

Speaker 1:          00:16:09       about the second method. But just to repeat the point you mentioned is that here we had to choose a cat. It means the x Scott is actually an image of a cat. So what if we don't know what the catch you look like? We just want random cats to come out and be classified as a, your corner. We're going to see a January generating networks after which can be used to do that type of stuff. But um, for the second part of the question, I'm not sure what the optimization process will look like. Okay, let's move on. So yeah, it's probably a good idea to start with the image that we specified in the loss function.

Speaker 1:          00:16:48       Okay. And so then we have an image of a cat. That's origin origins was classified as 92% cuts and we modified a few pixels. So you can see that this image looks a little blurry. So by doing these modifications, the network, we think it's a need Guana. Okay. And sometimes these quantification can be very slight and we can even not be able to notice it. Sounds good. Now let's add something else to this, uh, to this, uh, to this draft. We add the third set, which is a space of images that look real to human. So that's interesting because the space of images that look real to human is actually bigger. The space then the space of real images. And an example is this one. This is probably an image that looks real to human, but it's not an image that we could see in the daily life because of the slight excel changes. Okay? So these are the space of dangerous adversarial examples. They look real to human when they're not actually real. They might be used to fully model. Okay, now let's see a video by cracking it all. A real word example of a adversarial examples. So for those who cannot read, they're taking, uh, a camera which we just classify, which has a classifier and the classifier classifies the first part. That's the library. And the second image that is the same as a prison.

Speaker 1:          00:18:22       So the second image has slightly different pixels, but it's hard to see for human. Same here. So the classifier on the phone classifies the first image as a washer with 52% accuracy continence and the second one as a doormat.

Speaker 3:          00:18:43       Yeah,

Speaker 1:          00:18:46       this is a, a small example of what can, what can be done. Okay, now let's go. We've seen how to generate these adversarial examples. It's an optimization process. We will see, uh, what are the type of attacks that we can lead and what our defenses against this adversarial examples. So we would usually, uh, speed the t the attacks into two parts, non targeted attacks and targeted at ducks. So non targeted attacks means that's we just want outputs. We just want to find an adversarial example. Then he's going to fool the model. While targeted attack is we want to force this you an example to be output to output a specific class that we chose. These are two different type of attacks that are widely discussed in, in the research knowledge of the attacker. Is something very important for those of you who did some Crypto, you know that we talk about white box and tax, blackbox and tax.

Speaker 1:          00:19:43       So one interesting thing is that black in touch, a white box and Tuck is when you have access to a network. So we have our image and pretend and free trade network, we have fully access to, to all the parameters and and uh, the gradients. So it's probably an easier attack, right? We can, we can back propagate all the way back to the image and update the image like we did a blackbox attack is when the model is probably encrypted or something like that so that we don't have access to inspire matters, activations and architecture. So new question is how do we attach in blackbox attack if we cannot back propagates because we don't have access to the layers. Any ideas? Yeah. Numerical, numerical grade. Yeah. So you know, you will tweak the image a little bit and you will see how we change changes the loss. Looking at these you can, you can do have an estimate of the new miracle ingredients. Even if the model is a black box model, this assumes that you can query the model, right? You can query it. What if you cannot even query to model or you can query it one time will leads to send the adversarial example. How would you do that? So this becomes more complicated.

Speaker 1:          00:21:08       So there is a very complex property of this adversarial examples is is that they're highly transferable. It means I haven't model here that is a 90 Mil classifier. Okay. I don't have access to it. I cannot even query it. I still want to fill it. What I'm going to do is that I'm going to build my own animal classifier, forge and adversarial example on it. It's highly likely that it's going to be an adversarial example for the other one as well. So this is called transferability and it's still a research topic. Okay. We're trying to understand why this happens and uh, also, uh, how to defend against that. You know, maybe a defense against that is to, is to, uh, we're going to see it after, I'm not going to say no, sorry. Uh, does that make sense? Or No, this transferability probably it's because two animal classifier is look at the same features in images, right?

Speaker 1:          00:22:05       And maybe these pixels that are playing, we're playing with our changing also the output of the other network. Let's go over some kind of defenses. So one solution to defend against these adversarial networks is to create a safe safety net. The safety net is what is, uh, a net that like a firewall. You would put it before your network. Every image that comes in will be classified as fake, like forged or real by the network. And you only take those which are real and are not adversarial. Does that make sense? So you could, you could, you could say that's okay, but we can also build an adversarial network that foods this network, right? Just black box or white box. We can just create an ad. An example for this network. It's true, but the issue is that now we have two constraints. We have to fill the first one and the second one at the same time. You know, maybe if you fill the first one, there is a chance that the second one is going to be food. We don't know. It just makes it more complex. There is no good defense at this point. Two to two, all type of adversarial examples. This is an option that people are researching for it. So the paper is here. If you wanna check it out. Can you guys think of another solution?

Speaker 3:          00:23:29       Yeah. Train on multiple loss function. So different networks. So you're talking about an

Speaker 2:          00:23:46       maybe we can, maybe we can create five minutes works to do our tasks and it's highly unlikely that the address, for example, is going to who? The fire, uh, networks the same way. Right? And other ideas just get very adversarial examples. Exactly generates adversarial examples and train on them. Okay, so

Speaker 1:          00:24:11       you regenerate [inaudible] Jerry's adversarial. So some pixels have been changed to full network. You will label it as the human sees it. So as a cat, because you want the network to still see that as a cat and you will train on those. The downside of that is that it's very costly. We've seen that generating adversarial examples. It's super costly. And also we don't know if he can generalize to other adversarial examples. Maybe we going to over feet to the ones we have. So it's another optimization problem. Now another solution is to train on adversarial examples at the same time as we train on on normal examples. So look at these loss function, this loss function. The last new is a son of two last functions. One is the classic loss function we would use. So let's say Krista entropy in the case of her of her classification. And the second one is the same loss function, but we give it the adversarial version of x. So what does the complexity of that at every gradient descent step.

Speaker 3:          00:25:16       Okay.

Speaker 2:          00:25:23       For every iteration of our gradient descent, we're going to have to iterate enough to forge an adversarial example at every step, right? Because we have x, what we want to do is for propagate x to the network to compute the first term, generate x adversarial with the optimization process and for propagated to calculate the second term and then back propagate over the weights of the network. These super costly as well. And it's very similar to what you said is just online and just all the time. Okay. So

Speaker 1:          00:25:54       what is interesting is, uh, we're going to delve a little more, there's another technical logic pairing. I just put it here. We're not going to talk about it. There was a paper here, if you want to check it, it's another way to do adversarial training. Uh, but what I would like to talk about is more from a theoretical perspective. Why are neural network vulnerable too? Adversarial examples. So let's, let's do some, some work on the board. Yeah. One question.

Speaker 2:          00:26:42       So the thing is that just like in Crypto, every time you come up with a defense, someone we come up with an attack and it's a race between human, you know, so this is the same type of problems. Security problems are, okay. So let's go over some things

Speaker 1:          00:26:57       interesting that he's more on the, on the intuition side of adversarial examples. So let me, let me write down something. So one question we ask ourselves is, why do adverse example exists? What's the reason? And, uh, young Goodfellow and, and, and his team have came up with explaining, uh, with, uh, the, one of the seminal papers of adversity examples where they argued that's old though many people in the past have, have I trained UTVs existence of adverse in examples, too high nonlinear re nonlinearities of neural networks and over fitting. So because we over feet to a specified data set, we actually don't understand what cats are. We just understanding what, what we've been trained on, uh, they argued at is actually the linear parts of networks that is, Nicole was off the existence of adversarial examples. So let's see why. Uh, and the example I'm going to, I'm going to look at is linear regression. So together we've seen logistic regression. Linear regression is basically the same thing. We all just seem way so before to seem like we have white hats equals w x plus B. So in the forward propagation of our network is going to be y hats equals w x plus me,

Speaker 1:          00:28:18       okay? And our first example is going to be a six dimensional inputs.

Speaker 3:          00:28:24       Okay?

Speaker 1:          00:28:32       Okay. We have in your own here, but then your own doesn't have any activation. And because we are in linear regression, so here what happens is simply was plus B, okay? And then we get white huts and we probably use an l one or l two loss because it's a regression problem to, uh, to train these networks. Now let's look into the first example. The first example where, uh, where x, where are we framed our network? So network has been trained, sorry, natural. Rick has been trained and converged.

Speaker 3:          00:29:24       Okay?

Speaker 1:          00:29:25       Two W equals one three minus one, two, three. This is w. And you know like because we defined x to be the vector of sizes, a column vector w has to be a row vector of size six so the natural converse to this value of w and B equals zero. So now we're going to look at these inputs. We're giving a new input to the network and the input is going to be one minus one two zero three minus two. Okay? So I'm going to four propagate this to get white hats equals w x plus B. And this value is going to be one times one minus three minus two plus zero plus six minus six if I didn't make a mistake up up to minus three. Okay? Okay. And so we, we basically get

Speaker 5:          00:30:51       minus four. Okay, so this is the, the, the first, the first example that it was propagated. Now the question is how to change x into x star search, why hats changes radically. But agstar is close to x.

Speaker 1:          00:31:45       So these basically your prominent with adversarial examples. Can we find an example that is very close to x but radically, radically changes the output of our network? And we're trying to build intuition on, on adversarial neural networks. So the interesting part is to, is to identify how we should modify x and a d intuition comes from the derivative. If you take the derivative of white hats with respect to x, you know that the definition of this term is, is like correlated to the impact on why hat of small changes

Speaker 5:          00:32:33       of X. Okay.

Speaker 1:          00:32:35       How, what's the impact of small changes of x two on the outputs? And if you computed, when do you get w everybody agrees? What's, what's the shape of this thing?

Speaker 3:          00:32:59       Yeah.

Speaker 1:          00:33:01       Shape of that is the same as shape of x. So should be w transpose, remember nerve itself a scaler. And with respect to a vector is the shape of the vector. Okay. Now it's interesting too to see these because if we come to Texstar to me, let's see, x plus a small purchase nation like I, we call it perturbation value. Yeah, sorry. And can you see the top one you said yes or no? Okay. So what if x star equals x plus epsilon time w transpose, you know, and this epsilon I will call it value of the situation. No. If we four probe, I get x star, it means we do. Why hot star equals x star Plus B with the zero. At this point we're going to get w x plus epsilon w times that I've been new transports and w times w transports, he's a dog product, right? So this is the same as w square.

Speaker 1:          00:34:44       So what is interesting, it's interesting because d does smart parts was that this term is always going to be positive. It means we we moved a little bit x because we can make this change little by changing epsilon to a small value, but it's going to push why hat to a larger volume for sure. You know, and if I had a minus here instead of a plus, it would push my heart to a smaller value. And the, the interesting thing is now if we compute x star to be x plus epsilon times w transpose and we take epsilon to be a small value like let's say 0.2 you can make the calculation. What we get is, is this, so one minus one two zero three minus two plus 0.2 times one 0.2 times three mine 0.2 just 0.4 plus 0.4 and plus 0.6

Speaker 5:          00:36:02       okay.

Speaker 1:          00:36:05       So as you look at that, all the positive values have been pushed on the rights you agree and all the negative values. Oh sorry, sorry. No, that's my bad. No, no, no, that's okay. So let, let's finish their calculation and I can give the insight after 1.2 mine is 0.4 1.8 0.4 3.4 and minus 1.4 so this is our x star that we hope to be adversarial. Okay, let's come. Cute. Why hide star to see what happens. It's w x star Plus d which is zero. So what we get when we multiply w by x star is 1.2

Speaker 5:          00:37:06       1.2 minus 1.2 minus 1.8 plus 0.8 plus 6.8 and minus 4.2 which I believe is going to give us 0.5

Speaker 1:          00:37:40       okay. So we see a very slight change in x star has pushed white hats from minus four 2.5 and so a few things we want to notice here. So insights on this, on this small example. The first one is that um, if w is large, then x star is not similar to x, right? The larger w the less egg stories is likely to be like X. And specifically if one entry of w is very large, XII, the pixel to the century is going to be very different from X. I star, um, if WWE is large, exterior is going to be different in x. So what we're going to do is that we're going to take sign, sign off w instead of taking w what's the reason why we do that? Because the interesting part is the sign of, of the WWE. It means if we play correctly with the sign of w, we will always push the x, this term w x star in the positive side because every entry here, these multiplication is going to give us a positive number. Right? And the second insights is that as x grows in dimension, the impact off plus epsilon sign of w

Speaker 3:          00:39:47       increases. Does that make sense?

Speaker 1:          00:40:01       So the impact of sign of w and y hats

Speaker 3:          00:40:06       increases.

Speaker 1:          00:40:09       And so what's interesting to notice is that we can keep epsilon as small as possible. It means x and x star will be very similar. But as we grow in dimension, we're going to get more term in these a lot more term. And the change in white hats, he's going to grow in growing, growing, growing. And so the one reason why Andrew Sewell examples exist for images is because the diamond chain is very high, 64 by 64 by three. So we can make epsilon very small and take the sign of w we, we still gets why hats to be far from the original value that it had. Does it make sense? Yeah. You guys have any questions on that? So epsilon doesn't grow with the dimension, but it's impact of this term increases. We did dementia.

Speaker 3:          00:41:01       Okay. I assume that when you really need to sense of just how they lose fingers. Way Cooler and I think was working towards was that, I know to include her and I think was what into what, how'd the input

Speaker 2:          00:41:34       image cat. Yup. Do you that gives you a sparser mapping to another again. Okay. So you like you try to find adversarily decap yeah, yeah. I don't know if that has been done. I don't think that has been done. You, you're talking about thinking one or two a quarter and it's takes the adverse in example, conversation to a normal image of a cat and then the cats. Sure. Maybe. Yeah.

Speaker 1:          00:42:00       I don't know. Sorry. It's a topic of research. Uh, okay. Let's move on because we don't have too much time. So just to conclude what we're going to count as a general way to generate adverse suit examples is this formula.

Speaker 5:          00:42:21       Okay.

Speaker 1:          00:42:24       This is going to be a fast way to generate that verse at example. So this method is called the phase fast gradients sign method.

Speaker 5:          00:42:38       Okay?

Speaker 1:          00:42:39       So basically what we're doing is that we can, we're linear rising the cost function in, in the proximity of the parameters and we're saying that's what's applied to linear networks here is going to also apply for these general formula for deeper networks. So we're pushing the pixel images in one direction that is going to impact highly the output. Okay? So that's the intuition behind it. Now you might say that's okay, we did this example on the linear network, but neural networks are not linear and they're highly nonlinear. Uh, in fact, if you look where the research has been going for the past few years, we're trying to linearize all the behaviors of this neural networks. We'd read for example, or we'd does here initialization, all that type of methods, even a sigmoid. When we train on Sim, we do all we can to put sigmoid interlinear regime because we want fast training. Okay. And one last thing that I mentioned for adversarial examples is if I have a network like this, so fully connected with three dimensionally inputs up. Yeah. And then one here and then the output. What's interesting is computing the chain rule on, on, on, on this neuron.

Speaker 5:          00:44:10       Yeah.

Speaker 1:          00:44:10       We give you that derivative of the loss function with respect to let's say x is equal to the derivative of the loss function with respect to z one one

Speaker 1:          00:44:27       here times their votes evolve. Z one one with respect to x, let's say where we're going. We're going, there's like trivia summation here. But anyway, uh, just let me illustrate the point once we're, what we saying is that what we're, what we tried to do with your own networks is to have these gradients be high because if these gradients is not high, we're not able to train the parameters of this neuron. We need these gradient to be high. Because if you want to do the same thing with the we w one one which is department, there's related to this neuron you we'd need to go to this training. Correct. So we need this gradient to be high end. If these gradient is high, the gradient with respect to the input is also going to be high because you use the same gradient in the chain rule. So networks that are, that have high gradients and that are operating in their linear regime or even more vulnerable to adversarial examples because of this observation.

Speaker 1:          00:45:35       So any question on, on adversarial examples before we move on? I think we don't have time and I would like to to go over the, the, the gangs with you guys. So let's move on to guns. I stick around to answer your questions on that point. So the general question we're asking now is a, do neural networks understand the data? Uh, because we've seen that some, some data points look like there would be real, uh, but the neural networks don't understand it. So more generally, uh, can we build generated in networks that can mimic the real world distribution of images, let's say. And this is what we will call generative adversarial networks. We'll start by motivating it. And then we look at something called the minimax game between two networks, a generator and a discriminator that are going to help each other improve. And finally we'll see that gans are hard to train. Um, we'll see some tips to train them and finally go over some nice results and, uh, methods to evaluate. Gans. Okay.

Speaker 1:          00:46:40       So the motivation behind generative adversarial networks is to endo computers with an understanding of our world. Okay. So by that we mean that we want to collect a lot of data, use it to train a model that can generate images that look like they're real, even if they're not. So a dog that has never existed can be generated by this network. Um, and finally, uh, the number of parameters of the model is smaller than the amount of data. We already talked about that. And this is the intuition behind why are generating network can exist is because there is too much data in the world. Any majors counters a data for generative network and there are not enough parameters to mimic this data. You know, you have the network needs to understand the salient features of data set because he doesn't have enough perimeter to overfit everything.

Speaker 1:          00:47:34       So let's talk about probability distributions. So these are samples from real images that have been taken. And if you plot these real data distribution in a two d map, uh, it would look like something like that. I made it up. But this is the image space similar to what we talked about in adversarial networks. And this green shape is the space of real world images. Now, if you train a generator and generate some images that look like this, and these images come from stack Gan, uh, from Jungler, uh, this distribution, if the generator is not good, it's not going to match the real word distribution. So our goal here is to do something so that the red distribution matches the real world distribution went to train the network so that it, it realizes what we want.

Speaker 1:          00:48:27       So this is our generator and it's what counts. It's what, what we want to train. Ultimately we want to give it, let's say a random number or a random latent code of 400 diamond Shin, a scalar numbers and we want it to output an image. But of course because it's not trained initially is going to output a random image. It looks like something like that. Random pixels. Now this image doesn't look very good. What we want is these images to look like generated images that are very similar to the real world. So how are we going to help this generator train? It's not like what we did in classic supervised learning because we don't have, uh, we don't really have inputs and labels. You know, there is no label. We could maybe give it an image of a cat and ask it to output and other cats. But we want the network to be able to output things that don't exist. Things that we've never seen, right? So we want the network to understand what a cat is, but not over feet to the cat. We give it.

Speaker 1:          00:49:33       So the way we're going to do it is through a small game between this network called the generator g. And another network called the discriminator d. Let's, let's look at how it works. We have a database of real images and we're going to start with this distribution on the bottom, which is the real world data distribution is a distribution of the images in this database. Now our generator has this distribution initiative. It means the pixels that you see here probably follow a distribution that doesn't match. The real world will define the discriminator d and the goal of the discriminator will be to detect if an image is real or not. So we're going to give several images to this Christian matters. Sometimes we will give it generated images and sometimes we will give it real world images. What we want is that this discriminator is a binary classifier that outputs one if the image is real and zero if the image was generated. Okay, so let's say we give it x coming from the generated image is going to give us zero because we want the discriminator to detect that x was actually g of z.

Speaker 1:          00:50:55       If Dmh came from our database of real images, we want the discriminator to say one. So it seems like the discriminator would be easy to train, right? It's just a binary classification. We can define a loss function that is the binary cross entropy and the good thing is we can have as many label as we want. Like it's, it's unsupervised but a little bit supervised. You know we have this database and we label it all as one. It's just these image exists, let's label them as one for this community and everything that comes out of the generator, let's label it as zero four discriminator. So we basically, data is not costly at all in this point.

Speaker 1:          00:51:33       The way we will train is that we will back propagate the gradient to the discriminator to train the discriminator using a binary cross entropy. But what we ultimately want is to train the generator. That's what we want. At the end, we were not going to use the discriminator, we just want to generate images. So we're going to direct the gradients to go back to the generator. And why does this gradient can go back to the generator? The reason is that x is g of Z. It means we can back propagate the gradients all the way back to the input of the discriminator. But this input depends on the input of the generator. If the image was generated. So we can also back propagate and direct degree into the generator. Does it make sense? There is a direct relation between z and the loss function. In the case where the image was generated, if the image was real, then the generator couldn't get the gradient because x doesn't depend on z or on the features and parameters of the generator. Okay. So we would run an algorithm such as Adam, um, simultaneously on too many batches, one for the true data and from farms generated data.

Speaker 1:          00:52:43       Does this scheme makes sense to everyone? Yeah. One question.

Speaker 2:          00:52:54       So there's many methods of doing your, your question is about mixing them in in batches. Usually we would use, we would use one mini batch for the real data and one mini batch for the fake data. But if in practice you can try other things. So there are many methods that are being tried to train gans properly. We're going to delve a little more into the details of that when we will see the loss functions. So we hope that the property distributions will match at the end and if it matches, we're going to just take the generator and generates

Speaker 1:          00:53:25       images normally should be able to generate images that look real that looked like they came from his distribution. Okay, sounds good. So now let's talk more about the training procedure and try to figure out what the loss functions should be. In this case, what should be the cost of the discriminator? Assuming assuming we'd give too many batches. One for real data. So real images and one for generated data that come from Ge.

Speaker 3:          00:54:06       Yes,

Speaker 4:          00:54:06       the same basic last one. Should we use for every finite classes,

Speaker 1:          00:54:10       the same basic loss function we use from binary class for buying the class fees? It's true. We're going to tweak it a tiny bit, but it's the same idea. So this is what it can look like. We're going to call it j d cost function of the discriminator. It has two terms. What does the first term say? What does the second term say?

Speaker 1:          00:54:30       And you can recognize the binary cross entropy here. The only difference is that we have labeled. That is why Rio and a label, that is why generated in practice, why real and why generated or always going to be set to values. We know that why generated is zero and we know that why really is one so we can just remove these two terms because they're both equals to one. The first term is telling us the should correctly labeled real data as one. It's across entre beater, the first term of a binary cross entropy. The second term is going to tell us these should correctly labeled generated data zero. So the difference with classic or Centropy we've seen is that this summation is the summation over the real minibatch and the summation on the second cross entropy is a summation on generated minibatch. Does it make sense? So we both want the D to correctly identify it with real data and also correctly identified fake data. That's why we have two terms. Now what about the generator? What do you think should be the cost function of the generator?

Speaker 4:          00:55:40       Yes, I will run the first half because I don't have a uh, a wide range of inputs coming into the generator. Yes, exactly. But in your batch you will have like a certain number of real examples,

Speaker 1:          00:55:58       generate examples. The generated examples have no impact on the first cross entropy and same for the real examples on the second question. Trouble. Any other questions? Okay. So coming back to the cross to the, to the cost of the generator, what should it be? Just tiny bit complicated.

Speaker 2:          00:56:26       Let's move. Let's move on cause we don't have too much time. Your cost of the generator. Basically it should say that Ge should try to fool. The goal is to

Speaker 1:          00:56:36       to generate real samples and in order to generate real samples we want to fool the, if g managed to full d and d is very good, it means Ge is very good. Right? The problem is that it's a game because if these bad and g foods d it doesn't mean that Ge is good cause g cause these bad, it doesn't detect very well the real versus fake examples. We want d to go up to, to be very good, Angie, to go up at the same time and TV equilibrium is reached at a certain point where Di will always output one half like random. Probably it is because it cannot distinguish the samples coming from GE versus the real samples. So this cost function is basically saying uh, for generated images we want Di to classify them as one. That's what it's saying. We want to fool the, okay. Yeah. One question,

Speaker 2:          00:57:43       try to see how you would implement would be, but how would you implement, is there, has there been a module trend this cause if you're using, so how to implement that. If you're using a deep learning framework, you've been building a graph, right? And at the end of your graph you've been building your costs function that is very close to a binary core, central big.

Speaker 1:          00:58:13       Uh, what you were going to just do is to define a note that is going to be minus your costs function of the is going every time you're going to call the function j of Ge is going to run the graph that you defined for Jfd and run a an opposition operation and opposite of operation. Yeah.

Speaker 2:          00:58:36       Oh, the gradients back the same way. Propagate gradients back the same way. We're not going to propagate the same way. We're going to turn into nine to sign four degree for the generator. So you know, you, you, you back propagate on the, on the on on d and when you back propagates on g you would flip, you would feed the sign. That's all we do is the same thing we designed flicks in terms of implementation is just another operation. Okay. Now let's look at something interesting. Is that this log garrison, let's look at a graph of the logo.

Speaker 1:          00:59:22       So I'm going to plot against the [inaudible] g sorry, d of GLC. So what does this mean? This access is the output of the, when given a generated example, GLC is going to be between zero and one because it's a probability. These are binary classifier. We just see more eda output probably. Uh, if we plot logarithm of x, so like these type of thing, this would be log of the of g of Z. Does it make sense? He's a logarithm function. If I plot minus that minus, that's so let me, let me plot minus LogRhythm of g of doc or, or let me, let me do something else. Let me plot logarithm of minus D of g of Z. Okay. This is it. Do you guys agree? Now what I'm going to do is that I'm going to plot another function that is this one that is luxury them of one minus D of g on Z. Okay, so the question is right now what we're doing is that we're saying the cost function of the generator is Laogai rhythm of one minus D of g of Z. So it looks like this, right? It looks like this one. What's the issue with this one? What'd you think is the issue with discuss function? Looking at it like that?

Speaker 2:          01:01:27       Sorry, can you say it louder? Goes to goes to negative infinity in one. That's what you mean? Yeah. Yeah, and so do the consequence of that is that the gradient here is going to be very large. The closer we go to one, but the closer we are to zero, the lower is the gradient and it's the reverse phenomenon for this life.

Speaker 1:          01:01:54       Garrison, the gradient is very high and very high. I'm in absolute value and very high when we're close to zero, but it's very low when we go close to one. Okay, so which loss function you think would be better? A loss function that looks like this one or loss function that looks like this one.

Speaker 2:          01:02:17       To train our generator.

Speaker 1:          01:02:24       The broader question is where are we early in the training? Are we close to here or all we chose to there? What does it mean to be closed there? Two. One, you're fooling the network. It means that d thinks that generated samples are real. You were here. This place is the country. D thinks that generated samples are fake. It means correctly finds out that they're fake early on. We're generally here because the discriminator is better than the generator. Generates your outputs garbage at the beginning. And it's very easy for this community to figure out that it's fake because this garbage, it looks very different from real world data. So early on we're here. So which function is the best one to two to two to be our costs?

Speaker 1:          01:03:16       Yeah, so probably this one is better. So we have to use a mathematical trick to change this into that. Right? And the mathematical trick is pretty standard. Right now we're minimizing something that is in log of one minus x. We can say that doing so is the same as maximizing something that is in log of x. You agree? Simple flip. I mean Max flip and we can also say that it's the same as minimizing something in minus log of x. Does it make sense? So we're going to use these mathematical trick to convert our function that is a saturating costs. We would say into a non saturating costs. There is going to look more like this.

Speaker 1:          01:04:09       Let's see what it looks like. So to sum up our cost function currently looks like that it's a saturating costs because early on the gradients or small we cannot train. Gee, we're going to do a flip that I just talked about on the board and convert this into another function that is a non saturating cost. Okay. Yup. Well actually yeah, so the reason it's the blue one is like that is because I did a minus sign here so I'm flipping this. Okay. And it's the same thing. It's just the sign of the gradient that is willing to be different. Like that's, the gradients is high at the beginning and low at the end. That makes sense. So we're, we're, we're going to do the youth this flip and so we have a new training procedure and now we're a g of Dee. Dee didn't change but Jay g changed.

Speaker 1:          01:05:03       We have a minus sign here and instead of the log of one minus d of GMC, we have the log of g o d of Jersey. Does that make sense to everyone? Cool. And actually, so this is a fun thing. If she checked the speaker, which is really cool or against creating equality to large study of many, many different gangs. It shows what people have tried. And you can see that people have tried all types of loss to make gans trainable. So it looks, it looks complicated here, but actually the MM gun is the first one we saw together is the mini Max loss function. The second one is the non saturating one that we just see. So you see between the first two, the only difference is that on the generator we get the log of one minus d of x hat becoming love La minus log of the of excellent.

Speaker 1:          01:05:54       Okay. Now another trick to train Ganz is to use the fact that uh, a non saturating, uh, to use the fact that the is usually easier to train than Ge. But as the improvs, g can improve if he doesn't improve, G cannot improve. So you can see the, the, the, the performance of the, as an upper bound to what GE can achieve. Because of that, we will usually train the more time than we will train g. So we will basically train for Nami Titration, K Times d one time G K Times d one time g and so on so that the discriminator becomes better than the, the generator can catch up better than can catch up. And so that makes sense. There also methods to use like different learning race for the Ng to take this into account to try and foster the discriminator. Okay. Uh, because we don't have too much time. I'm going to skip the bathroom with guns. We were going to sit probably next week together after you guys have seen the bathroom videos.

Speaker 1:          01:07:02       Okay. It's cool. So just to sum up some, some tips to train Ganz is to modify the cost function. We've seen one modification, there are many more, uh, keeping the up to date with respect to g. So updating d more than you have the g using virtual batch norm, which is a dairy rate of bachelor on. So it's a different type of batch normally is used here and something called one sided labeled smoothing that I'm not going to talk about it today because we don't have time. So let's see some nice results now. And that's the funniest part. Um, so some of you I've worked with word embeddings and you, you might know that's a word. Embeddings are vectors that can encode the meaning of the word. And you can compute operations sometimes on this, on this words. So if you take, uh, if you take king minus queen, it should be equal to mine minus woman operations like that.

Speaker 1:          01:07:56       That's happened in the space of encoding. So here's the thing, you can use a generator to generate faces and the paper is listed on the bottom here. So you give a quote that is a random code and it will give you an image of a face. You can give it a second code, it's going to give you a second image that is different from the first one because the code was different. You can give it, the third one is going to give you a third pick surface. The fun part is if you take code one minus Code Two plus code three. So basically image of a man with glasses minus image of a man plus image of a women. We give you an image of a woman with glasses.

Speaker 1:          01:08:37       So this is interesting because it means that linear operation in the late and space of codes have impact directly on the image stays. Okay, let's look at something even better. So you can use guns for image generation. Of course, these are very nice samples you see that sometimes gans have problem with with uh, I dunno, I don't think that's the dog but, but the, but these are Stagen plus versus a is a very impressive Gannette has generated. It has been state of the art for a long time. Okay. So let's see, something fun. Something called image to image translation. So, uh, actually the, the, the project winners last quarter in spring was a project dealing with exactly that, generating satellite images based on the map image. So given the map you mentioned generate the satellite image using again. So you see that instead of giving you later and code that was 100 dimensional, you could give a very detailed code.

Speaker 1:          01:09:39       The code can be this image, right? And you have to find a way to constrain your network in a certain, in a certain way, to push it to output exactly the satellite image that corresponded to this mark image. There are many other restaurants that are fun converting zebras, two horses to zebras and zebras, two horses, uh, an apples to oranges and oranges to apple. So let's do a case study together. Let's say our goal is to convert horses to zebras on images and vice versa. Can you tell me what data we need? Let's go quickly so that we have some time. Yeah. Horses and zebras. Do you need per images? You know, like, do you need to have the same image of a horse as a zebra? Yeah, so the problem is, okay, we could have labeled images, you know, like, uh, a horse and it's a zebra doppelganger in the same position. Uh, and we could train a network to take one and out with the other. Unfortunately, we don't, not every horse has a doppelganger that is a zebra. So we cannot do that. Uh, so instead we're going to do unpaired, unpaired, generative adversarial networks. It means we have a database of horses and a database of zebras, but these are different horses and zebras. You're not one to one. There was no one to one mapping between them. There's no mapping at all. What architecture do you want to use?

Speaker 1:          01:11:07       Nice. Again. Okay, so let's see about the architecture and new cost. So I'm going to go very quickly because it's a, it's a very fun game. It's called cycle again. So the way we're going to work it out is we have a horse called capital h. We want to generate the Zebra version of this horse, right? So we give it to a generator that we called Gyn. You can call it h two Z, like horse to zebra. It should give us a this horse h as a zebra. Right? And in fact, if we were training again, we need a discriminator. So we will add the discriminator. That is going to be a binary classifier to tell us if this image out polluted by generator one is real or not. So these discriminator is going to take in some images of zebras probably or yeah, zebras or horses and he's going to also take the generated images and going to see which one is fake, which one Israel.

Speaker 1:          01:12:12       On the other hand we're going to do and the vice versa is very important. We need to enforce the fact that this horse g one of age should be the same horse as age. In order to do that, we're going to create another generator which is going to take the generated image and generate back the input image and this is where we will be able to enforce the constraints that g two of g one of h should be equal to age. You see why this loop is super important? Because if we don't have this loop, we don't have the constraints on the fact that the horse should be the, the zebra should be the horse as a zebra. The same horse has h so we'll do that and we had a second discriminator to decide if this image is real. This is one step h to z and understate my dizzy to age where we start with the zebra, give it to generate or to generate the horse version of the zebra discriminate, generate back the zebra version of the Zebra and this community. Does that make sense? So this is the general pattern using cycle gans and what I'd like to go over is what laws should we minimize in order to enforce the fact that we want the horse to be converted to a zebra that is the same as the horse and someone gives me the terms that we need.

Speaker 1:          01:13:43       Someone wants to give you the tray,

Speaker 3:          01:13:50       go for it. Two minutes. Yes.

Speaker 4:          01:13:55       You want to make sure that the picture that is zebra that you started off with matches the zebra that you started with or the horse to start off with the supports that you had originally. Okay. But at the same time you also need to have discriminator to identifying that the image is a real zebra or a real, yeah, because you

Speaker 2:          01:14:12       don't want it to just sort of input in the sample image and then output back to you at the same way. So I think you'd want to add the output of the pos function from discriminary of two to be a cost that you get comparing the starting it, which is great. So you're saying we need the classic cross functions that we've seen previously plus another one that is the matching between h and g two of g, one of H and z n g one or g two ops? Yes. Correct.

Speaker 1:          01:14:40       So we'll have all these terms. One term to train d one which is the classic term we've seen differentiate real images from generating images. G One as well. Same. We were using the non saturating costs on generating images. Same 42 st four g two these are classics. The one we need to add to all of this is the cycle costs, which is the distance between this term g, two of g, one of h and h and the same thing for zebras. Does that make sense? So you have the intuition to build that type of loss. We just some everything and gives us the cost function we're looking for. Yeah.

Speaker 2:          01:15:21       Do you want to see to recognize or the same cost function for d one and d two so the, the you could but it's not going to work that well. I think so. I think there's a, there's a tiny mistake here is that uh, the Z

Speaker 1:          01:15:38       ballsy, I should be small Ahi and the small hii on top, she just Mosey the eye because the discriminator one is going to receive generated samples that look like zebras because it came out of g. So you want the real database that you give it to, to be zebras as well to force to force the generator, want to output things that look like zebras and vice versa for the second one. Okay. And this is my favorite, so you can convert a Roman to a face and back to a ramen is the most fun application I've found in some narrow Tomita and Taqueria Taco. So it's Japanese research lab. We're working hard to do face around. Yeah. And actually in two, in two to three weeks you will learn object detection, you know, to detect faces. And if you learned that maybe you can start a project to like detect the face and then replace it by a ramen and on and also funny, funny work by Mary told me. Okay. Oh this is a super cool application as well. So let's look at that. Okay. So we have, so this model is a conditional gun that was conditioned on learning, uh, learning edges and generating cuts based on the edges. So I'm gonna, I'm to try to draw a cat.

Speaker 5:          01:17:12       Okay. Sorry. I cannot see you again. I'm not a good dryer. Okay. He's going to be dominant. The model. I hope he's going to work. You okay?

Speaker 1:          01:17:44       Okay. Now. Yeah, I don't think it works, but it's supposed to work. So you can generate Todd speeds on the edges and a, you can do eight for different things. You can do it for a shoe. So all of these models have been trained for that.

Speaker 5:          01:18:00       Okay. Yes, go for it. What is most, would you have this, this one, January, next up buildings cat, sorry. Can you repeat, is it generalizable or would you have to train it specifically for the jump up to training specifically for the domain. So like these models are different that been trained for my prison and I missed it.

Speaker 1:          01:18:36       Presentation disappeared. Okay. Um, another application is super resolution. You can give a lower resolution image in January, the super resolution version of it using gans. And this is pretty cool because you can get a high resolution image down, sample it and use this as a demeaning Max game. You know, like you have the high resolution version of the lower version, a very low resolution image. Um, are there applications can be privacy preserving. So some people have been working on, you know, in medical, uh, in the medical space. Privacy is a huge issue. Uh, you cannot share data set among hospitals among medical teams is coming. So people have been looking at generating a data set that looks like a medical data set. If you train the model on this data set is going to give you the same type of fire meters then the other one.

Speaker 1:          01:19:28       But this data set is anonymized so they can share the anonymized data with each other and train their model without being able to access an inflammation of the patient. And who is a manufacturing is important as well. So gangs can generate a very specific, uh, objects that can replace bones for humans, personal lives to, to, to the human body. So same for dental. If you lose a teeth, the technician can take a picture and decide what's the, the crown should look like. The gang can generate it as uh, another topic is how to evaluate against, you know, um, you might say, we can just look at the images and see if they look real and it will give us an idea if your gut is working well in practice it's hard because maybe the images you're looking at or overfitting images from the real samples you gave to the, to the, to the discriminator.

Speaker 1:          01:20:28       Uh, so how do you check that? It's very complicated. So human annotation is a big one where you would, uh, you would build a software, push it on the cloud and people all around the world are going to select which images look generated, which images look not generated to see if a human can can, can compare your, again to real world data and how you're getting performance. So it would look like that. Or Web APP indicates which image is fake, which image is real. You can, you can do different experiments. Like you can show very quickly an image for a fraction of a second and ask them was it real or not? Or you can give them a limited time. Different experiments can be led. Uh, there was another one that is more scalable because human annotation is very painful. You know, every time you train, again, you want to do that to verify if the gun is working well, it takes a lot of time.

Speaker 1:          01:21:17       So instead of using humans, why don't we use a very good network that is good at classification. In fact, in fact, the inception network is a tremendous network that does classification. We're going to give our image samples to this inception network and see what the network thinks of this image. Does he think that it's a dog or not? Does it look like a dog for the network or not? And we can scale it and make it very quick. And there's an inception score that, that we can talk next week too about when we'll have time. Uh, it measures the quality of the samples, uh, and also it measures the diversity of the sample. I go over it next week, hopefully. Uh, there's another distance that is very popular. Uh, this has been growingly popular recently called the fresh air inception distance. And I advise you to check some of these papers that you're more interested in it for four year projects.

Speaker 1:          01:22:08       So just to end, uh, for next Wednesday we'll have a c, two and three, and also the whole see tree modules. You'll have three quizzes. Uh, be careful these two quiz [inaudible] or longer than the normal quizzes. They're like wide case studies. So take your time and go over it. Um, and you'll have one programming assignments. Uh, make sure you understand the bachelor on videos so that we can go over the virtual batch norm hopefully next week together. Um, and hands on section this Friday, uh, you will receive your project proposal as soon as possible and meet with your project Tas to go over the proposal and to make decisions regarding the next steps for your projects. I'll stick around in case you have any questions. Okay. Thanks guys.