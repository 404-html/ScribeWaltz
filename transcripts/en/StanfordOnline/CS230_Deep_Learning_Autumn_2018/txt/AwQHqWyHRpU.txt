Speaker 1:          00:00:06       Hello everyone. Welcome to the second lecture for CST 30. So as I, as I said earlier, uh, you can go on menti.com from your smartphones or computers and entered these code 84 57 zero nine. We will use these tool for interactive questions during the lecture and we will also use it to track attendance. Uh, I, I did at the end of the lecture, but uh, if you have time to do it now let's start the lecture or you guys are doing that. Okay. So today's lecture is going to be about deep yearning, intuition. And the goal is to give you a systematic way to think about projects. Everything related to deep learning. It includes how to collect your data, how to label your data, how to choose an architecture, but also how to design a proper loss function to optimize. So all of these decisions are decisions you are going to have to do during your projects.

Speaker 1:          00:01:06       And we try to give you here an overview of this systematic way of ticking for different projects is going to be high level more than other lectures. But we hope it gives you a good start for your project. We start with the 10 minutes recap on what you've seen in the two first two in the first week, uh, about neural networks. So as you know, you can think of a machine learning, deep learning in general as modeling a function that takes an input that can be an image, a speech and natural language, or a CSV file. Give it to a box and get an output that can be classification easy to cats. Zero. Is there a cat on the CMH output one or is there no tat on this image? Outputs zero. And I think a good way to remember what is a model is to define it as architecture plus piraters architecture is the design that you choose.

Speaker 1:          00:02:06       So logistic regression is the first one you've seen. You will see shallow neural networks, deep neural networks. Then you will see convolutional neural networks and retro neural networks. So these are all types of architectures and you can choose to make them deeper or shallower parameters or the core parts. They're the numbers that makes your function. Take these cats as inputs. And converted to announce boots. So these are millions of numbers. And the goal of machine learning, deep learning is to find all these numbers. So we're all trying hard to find numbers. Basically millions of numbers in matrices.

Speaker 1:          00:02:41       If you give these cats and you forward propagated. So we propagated through the model to get an output. You will have to compare this output to the ground truth. Uh, the function used to do so it's called the lost function. You've seen an example of a loss function this week that is the logistic loss function. Uh, we will see more or less functions, uh, later on. Uh, computing. The gradient of these loss function is going to tell you how much should I move my perimeters in order to update in order in order to make the loss go down. So in order to make these function, recognize cats better than before. You do that many, many times until you find the right parameters to plug in your architecture, you can then give your cats and get an output. What is very interesting and deep learning is that many things can change.

Speaker 1:          00:03:28       You can change the inputs. We've talked about natural language, speech, structured, unstructured data in general, you can change the outputs. It can be a classification algorithm, it can be a multi-class algorithm. I can ask you, give me the breed of the cat. Instead of asking you, give me just the cats, which makes the problem more complicated. We can also be a regression problem. I give you the cat, I ask you, give me the age of the cat. She's much more complicated again. Does that make sense? Okay. Another thing that can change the architecture. We talked about it earlier and finally the last function. I think that its function is something that, that people struggle with to understand what plus function to, to choose, uh, for a specific project. And we're going to put a huge emphasis on that today. Okay. And of course in the architecture you can change the activation functions in this optimization look, you can choose a specific optimizers we're going to see in about three weeks, all the optimizers that can be Adam, stochastic gradient descent, batch gradient descent, rms, prop and momentum. And finally, all the hyper parameters. What is the learning rate of this loop? What is the batch that I'm using for my optimization? We're going to see all that together, but there's a bunch of things that can change in this scheme. Any questions on that in general?

Speaker 1:          00:04:48       So far, so good. Okay, so let's take the first architecture that we've seen together. Logistic Regression. As you know, an image in computer science can be represented by a three d matrix. Each matrix, we present a certain color, RGB, red, green, blue. We can take all these numbers from these three d metrics and put it in a vector. We flatten it in order to give it to our logistic regression. We for propagated, we multiply it byW , which is our parameter and B, which is our bias. Give it to a sigmoid function and get an output. If the network is trained properly, we should get a number that is more than 0.5 here to tell us that there is a cat in this summit. So this is the basic scheme. Now, my question for you is if I want to do the same thing, but uh, I want to have a classifier that 10 classify several animals. So on the image there could be a giraffe, there could be an elephant or there could be a cat. How would you modify this architecture? Yes,

Speaker 2:          00:06:04       yes.

Speaker 1:          00:06:05       So that's a good point. We could add several units. So several neurons, one for each animal and we will call it multi logistic regression. So it could be something like that. So we have a fully connection here. Before we were all, all the inputs were connected to this neuron and now we added two neurons and each neuron is going to be responsible for one animal. How do we know which neuron is responsible for which animal? Is the network going to figure it out on its own or do we have

Speaker 3:          00:06:35       to help it?

Speaker 2:          00:06:43       Exactly.

Speaker 1:          00:06:44       The label is important. So what is going to tell your model? This neuron should focus on cat dispersions, refocused on elephant. Decent flips on Giraffe is the way you label your data. So how should we label these data? Now if we were to do this specific task.

Speaker 3:          00:07:06       Any ideas? Yeah.

Speaker 2:          00:07:11       Okay. So one HUD

Speaker 1:          00:07:12       term means a vector with all Zeros and one one. Any other ideas? Hmm,

Speaker 2:          00:07:21       one, two, three. So I assume you, you say that each integer we correspond to a circle anymore. Okay. Any other ideas?

Speaker 1:          00:07:35       Hmm,

Speaker 2:          00:07:36       no loss of function. What do you find? The last hundred you want to put more weight on one animal. So you modified the loss function or what? Exactly. It was more, like I said, we didn't want her to come into it. So quick one,

Speaker 1:          00:07:52       Hudson coding. I think there's a downside to do one hot encoding. What is the downside of the one cuts and Courtney?

Speaker 2:          00:08:04       Yeah. So you're saying that the data, we have a lot of animals, the detox, the labels, all new content. Zero in one. One. So there's a huge imbalance. I don't think that's

Speaker 1:          00:08:13       it's, you should because these neurons are independent from each other right now. So yeah, it could run into an issue of a really a lot of animals this, but there is another problem with it. The problem is that, do you think if you want, if you won hearts and codes, uh, your labels, you would be able to detect an image with us, giraffe and an elephant on the image, you will not be able to do so. You need the multi huts and coding. So in this case, if there is a cat on the image, I will use a one hot, I would say zero one zero as my label. But if I have a dog and a cat on the image, I would say one one zero.

Speaker 3:          00:08:51       Okay?

Speaker 1:          00:08:51       Okay. The one hot encoding works very well when you have the constraint of having only one animal per image. And in this case you would not use an activation function called sigmoid. You would use another one, which is soft Max. The softmax function we're going to see together. And for those of you tonight, you probably heard of it. Okay? So what I wanted to explain here is the way you choose your labeling is very important and it's a decision you should make prior to start the project. Okay? In terms of notation in the, in this class we're going to use the following a square brackets one with the note all the activations of the first layer. So the square brackets would, would they know the layer and the lower squeak we they know the index of the neuron in delay. Okay. And of course you can stack these neurone on top of each other to make the network more complex depending on the task you're solving.

Speaker 1:          00:09:48       Okay. Now the concept I wanted to introduce in this recap was the concept of encoding. Um, you probably, some of you have probably seen this image before. If you have a network that is not too shallow, you would not, is that what the first year owns? See are very um, precise representation of the data. So there are pixel level representation of the data x three. I is probably one of the three channels of the treaty Matrix. Just one number. So what these neurons sees is going to be a pixel level representation of the image. Okay. What these neurons is the second layer, the one in the hidden layer is going to see the representation output. It by all the neurons in the first layer. These are going to be more high level, more complex because the first year owns, we'll see pixels. They're going to outputs a little more detailed information. Like I found an edge here, I found an edge there. And so on. Give it to the second layer. The secretary is going to see more complex inflammation is going to give it to the third layer, which is going to assemble some high level complex features. That could be eyes, nose, mouth, depending on what network you've been training. So this is an extraction of what's happening in each layer, uh, when the network was trained on a face recognition. Yes.

Speaker 1:          00:11:16       Yeah. So I see you are fully connected network, but that's true. These type of visuals are more a observed in convolutional neural networks because these are filters. But this happens also in this type of network. It's just harder to visualize. Okay. So this is what we call an encoding. It means if I extract the information from this layer, so all the numbers that are coming out of these edges, I extract them. I will have a complex representation of my input data. If I extract the numbers that are at the end of the first layer, I will have a lower representation of my data. That might be edges. Okay. We're going to use these including a two, five this lecture. Any questions on that?

Speaker 1:          00:12:06       Okay, so let's build intuition on concrete applications. We're going to start, uh, we, the short warm up we the day and night classification and then quickly moved to face verification and face recognition. And after that we'll do some art generation and finished with a trigger word detection. If we have time, we will talk about how to ship a model, which is shipping architecture plus parameters. Okay. We didn't, I'm, he's, as I said on the architecture of the lost the training strategy to help you make decisions during your project. So let's start with the first game. Uh, were given an image and we have to build a network that tells us if the image is taken during the day labeled zero or was taken at night label one. So first question is what data said do we need to collect,

Speaker 2:          00:13:06       look at the images captured during the day and during the nights? I agree. So probably, oh yeah. Then he asked the question how many images that was wrong,

Speaker 1:          00:13:20       how many images? Like how do you get this number?

Speaker 2:          00:13:27       Can someone give me an estimate of how many images you need in order to solve this problem and explain how you get around those. So you're saying a number of similar to a number of parameters you have in the network. So I think it's better to think of it the other way around the network

Speaker 1:          00:13:46       after. So right now you don't know what network you will use. So you cannot decide the number of data points based on your parameters later on based on how your network is flexible. You can add more data and that's where I'll be what, two minutes. But at first you want to get, you want to get to number? Yeah,

Speaker 2:          00:14:05       just the pixels within an image, more images than big soles within an image. Uh, I, I don't think that that's that that has anything to do with the pixels. You can have a very simple task. Like you have only images that are red and green and you want to classify reading green. Doing Mitch can be giant. You can have a lot of it.

Speaker 1:          00:14:24       Pixels isn't going to change the number of data points in it.

Speaker 2:          00:14:32       Okay. So you're talking about computation resource sources.

Speaker 1:          00:14:35       So the more images we have, probably the more computation resources we will need. So to me, yeah, there's something like that. I think in general, uh, you want to try to engage the complexity of the task. So let's say we did a problem that was Catterick on mission detective. There was a cut on any major nuts in this problem. We remember that we 10,000 images, we managed to train a pretty good classifier. How do you compare this problem to the cats problem? You think it's easier or harder? Easier. Yeah, I agree. That's probably easier. So in terms of complexity, these tasks looks less complex than the Catholic commission task. So you would probably need less data. That's a rule of thumb. The second rule of thumb and why I get to this image is what do we exactly want to do? Do we want to classify pictures that were taken outside, which seems even easier? Or do we want also the network to classify complicated pictures? What do I mean by complicated pictures?

Speaker 3:          00:15:36       Yeah,

Speaker 1:          00:15:39       inside your house. So like let's say on a picture you have a window on the right side of the human will be able to say it's the day because they see the window, but for the network he's got to take a much longer to learn that much longer than four pictures taken upside. What else? What are other complicated? Dawn? Dawn, twilight. Sunrise. Sunset in general. It's complicated because you have to define it and you have to teach your network. What does that mean? Is it night or day? Okay, so depending on what tasks you want to solve, it's going to tell you is you need more data or less data. I think for this task, if you take outside teachers, 10,000 images is going to be enough, but if you want the network to detect indoor as well, you probably need 100,000 images or something and this is based on comparing with projects you did in the past so he's going to come with experience. Now as you know when you have a later set you need to split it between train validation and test sets. Some of you I've heard that we're going to see together even more. You need to train your network on a specific sets and test it on another one. How do you think you should split these 10,000 images?

Speaker 1:          00:16:51       50 50 between training tests, 80 20 I think we would go towards 80 20 because the test sets is made for an Eliza to analyze if your network is doing well on real world data or not. I think 2000 images is enough to get that sense probably. And you want to put complicated examples in this data set as well. So I would go towards 80 20 and the bigger the data set, the more I would put in the train set. So if I have 1 million images, I would put even more like 98% maybe in the train set and 2% to test my model. Okay. Now I wrote bias here. What do I mean by bias? Yes, you need to correct balance between classes. You don't want to give 9,000 dark images in 1000 day images. You want to balance between these two two teacher networks to recognize both classes. Okay. What should be the input? Your network?

Speaker 3:          00:17:58       Yeah,

Speaker 1:          00:17:58       so this is an example of a pixel image. It's the Louvre Museum during the day. Harder question, what should be the resolution of this image and why do we care? Yeah,

Speaker 3:          00:18:20       that's great

Speaker 1:          00:18:22       for a civilian students as well as low as you can in order to achieve good results. Why do we want low resolution is because in terms of computation is going to be better. You remember if I have a 32 by 32 image, how many pixels there are? If it's color, I have 32 times 32 times three. If I have 400 by 400 I have 400 by 400 by three it's a lot more. So I want to minimize the resolution in order to still be able to achieve good performance. So what does it mean to still achieve good performance? How do I get this number?

Speaker 2:          00:19:06       Okay. Similar resolution as you expect the algorithm in real life to work on. Yeah, probably.

Speaker 1:          00:19:11       I agree. What else? What other rule of thumb you use in order to choose this resolution?

Speaker 2:          00:19:23       Great idea. Compared to human performance. So what they do, so there's one way to do it, which is the brute force way. I would say we will train models on different resolutions and then compare their results. Or you can be smart and use human performance as a comparison. So I would print this image or several images like this in different resolutions on paper and that would go see humans and say classify those, classify those and classify those.

Speaker 1:          00:19:47       And I would compare a human performance on all these three types of resolution in order to decide what's the minimum resolution that I can use in order to get perfect human performance. So by doing that, I got that 64 by 64 by three was enough resolution for a human to detect if an image is taken during the day or during the night. And this is a pretty small resolution imaging, but it seems like a small, like an easy task. If you have to find a breed of a chats, you probably need more because some cats are very look very alike and you need a high resolution to distinguish them. And maybe training for the human as well. I normally three breeds of cats so I wouldn't be able to do it anyway. Um, what should be the output of the model labels? The y equals zero four day. Why he called one four nights. I agree. What should be the last activation of the network?

Speaker 3:          00:20:45       Okay.

Speaker 1:          00:20:46       The last

Speaker 3:          00:20:48       sigmoid, we saw that Seymour, it takes a number between plus and minus in Fijian bloods. 50 puts it between zero and one. So that we can interpret it as a problem. What architecture would you use? Yeah,

Speaker 1:          00:21:06       fully connected or convolutional. I think later this quarter you will see that convolutional to perform well in imaging, so we would directly use a convolutional, when I think of shallow network, fully connected to a convolution of would do the job pretty well. You don't need a deep network because you gauge the complexity of this task and what should be the loss function finally. Yeah.

Speaker 2:          00:21:37       Yeah. It's also called the logistic loss. That's the one you're talking about. The way you get this number and you'd prove it in in cs two to nine we're, we're not doing it to prove it here, but basically you interpret your data in a probabilistic way

Speaker 1:          00:21:52       and you take the maximum likelihood estimation of the data, which gives you this formula. For those of you who did the math behind it, you can ask any office hours, teas are going to help you understand it more properly. Okay, and of course this means that if y equals zero, we want why had the prediction to be close to zero, if why you called one. We want white hide the prediction to be close to one. Okay, so this was the warmer, now we're going to delve into face verification. Any question on day night classification? Yes,

Speaker 3:          00:22:39       that's the,

Speaker 1:          00:22:48       so you're, the question is about how you choose the size of the test set versus the train sets. In general you would first say how many images do I need or data points in order to gable to understand what my model do in the real world. These can depend on the task. Like if I talk about, if I, if I tell you about speech recognition, you want to figure out if your model is doing well for all accents in the world. So your test set might be very big and very distributed. In this case you might have a few examples that are doing the day few during the nights and a few, I don't a sunset, sunrise and also indoor two of those is going to give you a number. So there was no good number. There is like you have to gauge it. Okay. One more question.

Speaker 2:          00:23:34       Yeah, that's a good question. So how do you choose the loss function? We're going to see,

Speaker 1:          00:23:39       uh, uh, in the next slides how to choose loss functions. But for these ones specifically, you chose this one because it's, it's, it's a, it's a convex function for classification of plot problem, it's easier to optimize then other loss functions. So there is approved but, but I will not go over it here. If you know the l one loss that copper's why too. I had this one is harder to optimize for a classification problem, we would use it for regression problems. Okay. So our new game is, uh, the school wants to use face verification to validate student Ids in facilities like the gym. So they know when you entered the gym you swiped your Id and then, uh, I guess the person sees your face on the screen based on these ID and looks at your face in real and compares. It's like, so now we want to put the camera and have you swiped and the camera is going to compare this image to the imaging, the database. Does that make sense to let you in or not? So what's what data said, do we need to solve this problem? What should we collect?

Speaker 3:          00:24:52       Okay.

Speaker 1:          00:24:53       Making between the ID and the image. Yeah, so probably schools have databases because when you enter the school, you submit your image and your, sorry, given a card, an ID. So you'll have this mapping. Okay. What else do we need? So pictures of every student labeled with their names. That's what you said. So this is a picture of where car is a picture when he was younger and that's the one he gave to the school when he arrived. What should be the input of our model? Is it this picture?

Speaker 3:          00:25:29       Hmm.

Speaker 1:          00:25:31       More photos of him. I'm asking just like the output of the model. Like we probably need more photos of him as well. But what's, what's going to be the image we give to the model? Exactly. The person standing in front of the camera when entering the gym. So this is the entrance of the gym and a veterans trying to enter the gym. So it's him. Okay. What should be the resolution? Those of you who have done projects in imaging, what'd you think should be the resolution? Two 56 by two 56 and the other idea for precise?

Speaker 3:          00:26:14       Yeah,

Speaker 1:          00:26:16       I think in general you will go over 400. The 400 by 400. What's the reason? Why do we need 64 four for the night and 404 phase verification? Yeah. Yeah. There's more details to the tech. So like distance between the eyes probably size of the nose, mouth, uh, general, general features of the, these are harder to

Speaker 2:          00:26:44       detect for 64 by 60, 40 minutes and you can test it. You can go outside and show two pictures of people that look like each other and ask people can you differentiate those two person or not? And you'll see that with less than that, sometimes it's people are struggling. Is color important? That's a good question. We should have talked about it in day and night. Actually. Is color important because if you remove the

Speaker 1:          00:27:06       color, you basically divide by three the number of Pixels, right? So if we could do it without color, we would do it without color. In this case, color is going to be important because, uh, probably you want your camera to work in different settings a day, night as well. So the luminosity is different, the brightness and also we all have different colors and we need to all be detected compared teacher. Yeah, I might go summer in, in an island and come back, uh, you know, full of color but uh, but I see on to be able to access the gym, uh, outputs. What should be the output? I think if you have unlimited computational power, you would take more resolution. But that's a trade off between computation and results. So output is going to be one if it's you and zero if it's not you, in which case they would not let you in. Okay? Now the question is what architecture should we use to solve this problem? Now that we collected the data set of mapping between student ideas and images,

Speaker 4:          00:28:21       how do you know how many images?

Speaker 2:          00:28:27       The question is, how do you know how many images we need to train the network? You don't know.

Speaker 1:          00:28:34       You can find an estimate. It's going to depend on your architecture, but in general, the more complex the task, the more data you would need. And we will see something called error analysis in about four weeks, which is once your network works, you're going to give it a lot of examples. Detect which examples are misclassified by your network and you're going to add more of these in the training set. So you're going to boost your dataset. Okay. Talk to you about the architecture. If I ask you what's the easiest way to compare two images, what would you like these three images, the database image and the input image,

Speaker 2:          00:29:11       sort of Hash Hash. What do you mean? Put runs, standardized function on it and then they're okay taking him. Take this, run it into a specific function, take this, run it into a specific function and compared the two largest that's correct. That's a good idea. And the more basic one is just computer distance between the pixels. Just compute the distance between the pixels and you get if it's the same person or not. And unfortunately it doesn't work and a few reasons are the background. Lighting can be different. And so if I do this,

Speaker 1:          00:29:42       mine is this, this pixel, which is let's say dark is going to have a value of zero. This pixel, which is why it is going to have a value of two 55. The distance is gigantic, but it's still the same person. He's a problem per can wear makeup. I can grow a beard, can be younger on a picture, the ID can be outdated. So it doesn't work. To just compare these two pictures together, we need to find a function that we will apply this, this, these two images too, and will give us a more, a better representation of the image. So that's what we're going to do. Now what we're going to do is that will encode information, use the encoding that we talked about of the picture in the vector. So we want a vector that would represent teachers like distance between eyes, nose, mouth, color, or all these type of stuff, hair, uh, in a vector.

Speaker 1:          00:30:35       So this is a picture of [inaudible] home from the ID. We would run it to a network and we hopefully can find the good encoding of this network. Then we will run the picture of Beth Hall at the facility, run it in the deep network, get another vector, and hopefully if we train the network properly, these two vectors should be close to each other. Let's say we have a threshold that is 0.5 0.4 is the distance between these two. It's less than the threshold. So I would say [inaudible] is the right person. It's you. Does this scheme exchange may make sense.

Speaker 1:          00:31:12       What does the one 28 minutes? So the question is, can I say that the third entry corresponds to something specific? It's complicated to say, but depending on what networks you choose and the training process you choose, it will use your different network, a different vector. So that's what we're going to talk about. Now the question is how do I know that these vector is good? Like right now, if I take a random network, I give my image to it is going to output around the vector. This vector is not going to contain any useful information. I want to make sure that this information is useful and that's how I will design my loss function. Okay, so just to recap, we gather all student faces and coding in a database. Once we have this and given a new picture, we come to the distance between, between the new picture and all the vectors in the database. If we find a match, oh sorry. We compare this vector of the input image with the vector corresponding to the ID image. If it's small, we consider that this the same person. Okay. Now talking about the loss and the training to figure out is this vector corresponds to something meaningful.

Speaker 1:          00:32:22       First, we need more data because we need our model to understand in general the features of the face and a university that has a thousand students. It's probably not going to be enough to have a thousand image in order to push them all to understand all the features of the face. Instead we will go online, find open data sets with millions of teachers of faces and help the model learn from these faces to then use it inside the facility. There was a question in the back,

Speaker 3:          00:32:49       what do you like? We did with the like the cat elephant giraffe. But every student is a wine. That's another option. So

Speaker 1:          00:32:56       the question is why can you tell? Can't you use the one hot encoding? We couldn't be Lou classifier that's has an output neurons and corresponding to the number of students in the school and you take an image, you, Ronnie to the network is going to tell you which student it is. What's the issue with that? Every year student center to school, you will have to modify your network every year because you have more students and you need to hire output vector, larger output vector. And we don't want to retrain all the time our networks. Okay, so what's what, what's we really want, if we want to put it in words, is that uh oh, there's a mistake here. What we really want is if I give you two pictures of the same person, I want a similar in quitting, I want the vector to be similar.

Speaker 1:          00:33:46       If I give you two pictures of different persons, I want different encodings. I want the victor to be very different and we're going to rely on these two assumptions and these two thoughts in order to generate, uh, our loss function by giving you triplets. Triplets means three pictures, one that we call anchor, that is the person a person, one did, we call it positive. That is the same person as the anchor, but a different picture of that person. And the third one that we call negative, that is a picture of someone else. And now what we want to do is to minimize the encoding distance between the anchor and the positive and maximize including the sins between gang corals. In the end, the negative, does these two thoughts make sense? So now my question for you is what should be the loss function?

Speaker 3:          00:34:36       Yeah,

Speaker 1:          00:34:38       what should be the loss function? So please go and mentee and enter the code and their tree off. Since here a, B, and c, choose which of these you think should be the right loss function to use for these problems? Oh, you have it on your phone as well, like issue. It's more on the screen, but you can see it on on, it's got us,

Speaker 3:          00:35:10       it's better here in the back. That's too small. Eight four five seven zero nine can you see it on your phone? Yeah. Yes.

Speaker 1:          00:36:05       So by adjunct of a, I mean and codings vector off the anchor, my anchor fee, I mean the encoding vector of the positive image after you run them through the network.

Speaker 3:          00:36:41       Okay. 30 more seconds.

Speaker 1:          00:36:52       Okay. All right. 20 more seconds.

Speaker 3:          00:37:03       Yeah.

Speaker 1:          00:37:07       Okay. Let's see what we have. Okay. So two thirds of the people think that that it's the first uncertain a, so I read it for everyone. The last is equal to the l two distance between the encoding of a engine cording of t minus the Ltd stands between the encoding of a and the encoding of n. So someone who has uncertainties, do you want to give a an explanation? Yes,

Speaker 2:          00:37:42       we are trying to minimize the first difference between the positive and you're trying to maximize the of and the negative. And then you subtract. So let's stop for a second. Fuck. And blessed because a lot in cinema and the first one would be minimized. Yes, that's correct.

Speaker 1:          00:37:59       So what you said, I repeated it. Just be the students. We want to maximize the distance between the encoding of a and encoding of the negative. That's why we have a minus sign here because we wanted the last to go down and to go down, we put a minus sign in, we maximize Easter and on your hand we want to minimize the other term because it's deployed. Positive Story. Okay. So I agree. We don't say, okay, that was the first time you use these tool. It's going to be quitter next time. Okay. So we have uh, we have, uh, figured out what's the loss function should be. And now think about it. Now that we designed loss function, we're able to use an optimization algorithm, run an image in the network, sorry, run, run three images into networks like that gets three outputs and coding of a encoding of tea and coding event. Come to the loss, take the gradients of the loss and update department chairs in order to minimize the loss. Hopefully after doing that many times we would get an encoding. That's the presents features of the face cause him network. We'll have to figure out who are the same people who are different people. Does that make sense? These called the treatment of loss and I cheated a little bit in the, in the quiz. I didn't write this alpha. The true loss function contains a small alpha. You know why?

Speaker 3:          00:39:25       Okay.

Speaker 1:          00:39:27       Yes.

Speaker 2:          00:39:30       So do you don't have negative loss. Yeah, dead. That's not exactly

Speaker 1:          00:39:34       the role of the outside. In order to not have negative lost with what you can do is to use a maximum of the loss and zero and train on the maximum of the loss. N Zero but there is another reason why we have this alpha.

Speaker 2:          00:39:45       Yes. Essentially it's you have like different stitching, false negatives and false positives like which one, which one do you prefer based on false negative and false negatives? No, it's not about that. So sometimes you have announced weight loss function too.

Speaker 1:          00:40:00       Puts a weight on some classes, but this is an additional al fights, not a multiplicative alpha. So it has nothing to do with that. Yeah.

Speaker 2:          00:40:10       We painted the guards. Wait, so you're talking about characterization? Yes.

Speaker 1:          00:40:14       We had weights in this formula next to the Alpha Alpha times the norm of the weights. This would dare regularization but hear this term doesn't penalize weights, Constance.

Speaker 2:          00:40:28       It's not going to affect the ingredients. It's not going to effect, it's not going to affect the weights. But the reason we hung it here is because let's say the encoding function is, uh, let's say you and Coleen function is just a function zero.

Speaker 1:          00:40:42       What we're going to have is that we're going to have encoding of a equals zero minus zero, zero minus zero. And so we will have basically a perfect loss of zero. Uh, and we still didn't train or network. We just learned the no. So these are five school, the margin and it pushes your network to learn something meaningful in order to, to, to stay stabilize itself on, on Zeros. Okay.

Speaker 2:          00:41:14       But there's nothing changes. Yeah. So it also has to do with the initializations. But because we didn't talk about any civilization yet, we only saw zero initialization I think in consultation to get her another way to, to avoid, uh, the networks to stabilize or to, to become stable and zero is to change the initialization scheme. And in two weeks we're going to see different industrialization schemes together. Is it guaranteed?

Speaker 3:          00:41:42       Wait, just wait. It's going to do the robot patients other like scaling.

Speaker 2:          00:41:51       So the question is how do we know that this network is going to be robust to rotations of the image or scaling of the image or translation of the image? We know it's because in the data set we're going to give, let's say your picture and your picture scale and are we going to tell the network this is the same person. So their network will have to learn that the scale doesn't mean it's not the same person. You have to learn this feature. Okay. One more question and then we move on. I'm fine. Yeah. So why isn't it starting at zero problem? Can you just wait and Megan loss values. Yeah. So a good question. Why is he to call them to to to stay at to establish at zero is because it's common to keep them. The loss function is positive and in the paper that you can find its face. That paper they don't train exactly this last they trend the maximum of this loss. N Zero.

Speaker 3:          00:42:38       Yeah.

Speaker 2:          00:42:40       Okay, so you train and you get the right function. Now let's make the problem a little more complicated. What we did so far was face [inaudible]. We're going to do face recognition. What's the difference? The difference is there is no more ID. So now you just have the camera in the facility, you enter the camera, looks at you and finds you. How would you design this new network?

Speaker 2:          00:43:14       Yes, in Dubai you got it in an element now of recognition as well because now for you sort of stand in front of it and you that every picture had a base. Now it needs to detect the face. Okay, so you're saying maybe we need to add an element to the pipeline that is a detection limit. That's true in general. For face recognition, let's say you have a picture that is quite big. You want to use the first network that identifies the face, like finds it on the picture, detects it, and then crop the face and give it to another network. That's true. That could also be using verification as well.

Speaker 3:          00:43:50       Well, it's in the old space.

Speaker 2:          00:43:54       Wait, so the difference, maybe what you're saying is maybe we can use it or verification algorithm to be trained, but instead of looking at one to one comparison, we look at one, two n comparison, so we have the picture as a whole. The students in the database, what we can do is run all these database pictures in the model gets a vector that represents them, right? We'll get the vectors. Now you entered the facility, we get your picture. We run it through the model. We get your vector and we can compare these vector to all the vectors in the database to identify you. What's the of this?

Speaker 1:          00:44:35       It's at the number of students you have for every prediction to go over the whole day database days. And a common network like model that you can use to do that is 10 years neighbors. So of course, if you have only one picture per students, it's not going to be very precise. But if you collect three pictures per student and you run a two nearest neighbors algorithm, you would decide that if the two pictures are the same, it's likely that this person is the same as the two person on the picture.

Speaker 3:          00:45:04       Okay.

Speaker 1:          00:45:07       Now let's make it a little more complicated. You probably saw that on your, on your phones. Uh, sometimes you take a picture and it recognizes that it's your grandmother or your grandfather or your mother and father. Uh, what's happening behind these that there is some clustering happening. It means we have a bunch of images and we want to cluster them together. So this is also another algorithm that you've seen [inaudible] nine eight, which is Kamins algorithm. And this is a clustering algorithm by taking all the vectors that we have in the database, we can find, uh, let, let's say, sorry you haven't, you have a phone, you have thousands of pictures of let's say 20 different people. What you want is to cluster all the teachers have the same person separately. What you will do is that you will encode all the pictures in vectors and then you will run a cloud clustering algorithm like Kamins in order to cluster those into groups. These are the vectors that look like each other. These are the vectors that you've liked each other. Okay. And then you can simply give folders to the users with all the pictures of your mom, all the pictures of your dad. And so how,

Speaker 5:          00:46:23       yeah,

Speaker 1:          00:46:23       good question. How would you define the cake? So, uh, someone has an idea actually.

Speaker 5:          00:46:37       Yeah. So one way is to,

Speaker 1:          00:46:43       as you said, to try different values, train or clustering algorithm and look at a certain last few to find how small it is. There's actually an algorithm called x means that is used x meals. You might search for that. If you want to find, uh, to find the k there was also a method called the elbow method or that you want to search for his way to figure out the cake.

Speaker 3:          00:47:06       Okay.

Speaker 1:          00:47:07       And as you said, maybe we need to detect the face first and then crop and give it to the algorithm. One more question on face verification.

Speaker 5:          00:47:15       Do you also use the louder, you also need to, okay.

Speaker 1:          00:47:32       You trained for classification? Um, sorry, I do. I didn't understand. So you mean could,

Speaker 5:          00:47:50       oh, so where is the encoding coming from? That's what you mean in in the network? Yeah. Okay. Good question.

Speaker 1:          00:47:56       So you have a deep network and you want to decide where should you take the encoding from. In this case, the more complex the task, the deeper you would go. But for face verification and what you want and you know it as a human, you want to know features like a distance between the eyes, nose and stuff. And so you have to go deeper. You need, the first layer is to figure out the edges, give the edges to the second layer. The second layer to figure out the nose, the eyes, give it to the third layer, the third layer to figure out the distances between the eyes, the distance in between the ears. So you would go deeper and get the encoding deeper because you know that you want high level features.

Speaker 3:          00:48:32       Okay?

Speaker 1:          00:48:34       Our generation, even a picture and making it look beautiful as usual data. What do we need?

Speaker 5:          00:48:49       It's a little complicated because we have to define what you did for this data. Some beautiful pictures. I don't know. Maybe my concept of beautiful is different. In Europe they turned a certain stylist,

Speaker 1:          00:49:08       so we might say that beautiful means paintings, like paintings are usually beautiful. So you want to have a set kind of a state yet it's true.

Speaker 3:          00:49:16       Okay,

Speaker 1:          00:49:16       so let's say we have any data that we want. What we're going to do and the way we define this problem is let's take an image that we call the content image. And here again you have the loof museum and let's take an image that we call the style image. And this is a painting that we find beautiful. What we want is to generate an image that looks like it's the content of the content image but painted by the painter of the style image. So this style image is occluded Monet and here we have the Louver painted by Claude Monet. Even if he was dead when this pyramid was created. So that's our goal and this is what we would call our generation. There are other methods but this is one. So how do we do that? What architecture is do we need and please try to use what you've seen in the past two applications together, what training scheme, what applications, what, what architecture.

Speaker 5:          00:50:28       No one wants to try this. We pick some images to a network

Speaker 1:          00:51:02       and then at work outputs, yes or no, one or zero

Speaker 5:          00:51:09       generate. We want to generate to any events yet this side, okay

Speaker 1:          00:51:24       you're proposing is we get an image that is the content image and we have a network that is the style style network, which was style. This image and we will get the content but stag version of the content,

Speaker 5:          00:51:45       certain feature of his and change your site according to what the network isn't it. You see it actually done. And this is one method that's not the one who would see today the should with this method.

Speaker 1:          00:51:56       Well, the issue is that you have to train your network to learn. One style network learns one style. You give the content, it gives you the constant with the specific style of the model. What we want to do is to have no model that is restricted to a specific style. I want to be able to give a painting of Picasso and Guess This picture painted by Picasso. So the difference here is that we're not, we're not going to learn perimeters of a network like we did for phase verification or four a day and night classification. We're going to learn an image. So you remember when we talked about backpropagation of the gradient of the parameters. We're not going to do that. We're going to back propagate all the way back to the image. Let's see how it works. So first we have to understand what content means and what stylists to do that we're going to use encoding.

Speaker 1:          00:52:47       We're going to to to to use the ideas that we talked about later. Giving the content image to a network that is very good will allow us to extract some information about the content of this image we specifically so together that earlier layers with the tech, the edges, the edges are usually a good representation of the content of the image so I might have a very good network. Give my contents image, extract the information from the first layer. This information is going to be the content of the image. Now the question is how do I get the style?

Speaker 1:          00:53:23       I want to give my style image and find a way to extract this die. That's what we're going to learn later in this course. It's a technical Graham Matrix and the important thing to remember is that the style is non localized inflammation. If I show you the the pictures in the previous slide, oh sorry. Here you see that CG generated picture old on this tiny image, there was a tree on the left side. There is no tree on the generated image. It means when I extracted the style, I just extracted known localized inflammation. What's the technique that code when it has used to paint? I didn't want to extract these tree that was on the stock image. Don't want the content. Okay, so we're going to take the network that understands images very well and they're coming online. You can find image nets, classified classification networks online.

Speaker 1:          00:54:18       That's where trains to recognize more than thousand thousands of objects. This networks is going to understand basically anything you give it. If I give you the Louvre Museum, it's going to find all the edges very easily. It's going to figure out that there is, it's during the day, it's going to figure out their buildings on the sides and all the features of the DME because it was trained for months on thousands of classes, but I'd say we have this network, we give our contents image to it and we extract information from the first few layers. This information, we call it content c content of the contents image. Does that make sense? Now I give the style image and I will use another method that is called the Grain Matrix to extract style style of the style image. Okay. And now the question is what should be the loss function? So let's go and mentee. So same code as usual. Just open it. Do you want me to repeat? I can repeat. Do Code if you want. Eight four five seven zero nine and these are the three proposals for the loss function. So reminder content c means content of their contents. Image style, Essman style of the style. Image Style, g means style of the generated image. Content g means content of the generated image.

Speaker 1:          00:56:07       Like a minute. Small.

Speaker 3:          00:56:19       Yeah.

Speaker 1:          00:56:21       Oh, new code eight four five seven zero nine

Speaker 1:          00:56:58       what? So just repeating the question of why do we need to use image net? Because we, we don't treat it. You need to classify an image and he's going to waste time. The reason we need Gmh Nancy's because image net understands our pictures. So you can give the contents image to a network that doesn't understand teachers very well. You're not going to get the edge is very well. So you want a network that you don't care about the classification outputs. You just cut the network in the middle, extract the layers in the middle. Okay. Let's see what the answers are according to you guys

Speaker 3:          00:57:35       giving style, style of training.

Speaker 1:          00:57:40       So yeah, I repeat, we are not training anything here. We're getting a model that exists and we use this model. We're going to talk about your training yester. Okay. Someone who has uncertain the second question and I will read it out loud. The loss is the l two difference between the style of the style image in the generated style plus the ultra distance between the generator, the generators content and the content is content. Yup. We want to maximize,

Speaker 3:          00:58:11       oh okay.

Speaker 1:          00:58:17       So yeah, we want to nice both terms here. So we want the content of their contents image to look like the content of the generated image. So we want to minimize the l to the says of this too. And the reason we use a plus is because we also want to minimize the difference of styles between degenerated in the style image. So you see we don't have any terms that say's style of the content, image minus style of the generated image is many ways. This is the last one.

Speaker 3:          00:58:44       Okay.

Speaker 1:          00:58:51       Okay, so just going over the architecture again. So the loss function we're going to use will be the one we sold. And so one thing that I want to emphasize here is we're not training the network. There's no parameter that we train. The parameters are indie may Jeanette's classification network, we use them, we don't train them. What we will train is the image. So you get an image and you start with white noise. You run this image through the classification network, but you don't care about the classification of this image. Image net is going to give a random class to this image. Totally random. Instead you will extract content g and Sig. Okay? So from this image you run it and you extract information from this network using the same techniques that you've used to extract contents c and stylists. So contents stay and stylist, you'll have it, you'll have it. You able to compute the loss function because now you have the four terms of the class function. You computed derivatives instead of stopping in the network, you go all the way back to the pixels of the image and you decide how much should I move the big soles in order to make this last go down. And you do that many times. You had many times and the more you do that, the more this is going to look like the content of the content image and the style of the stylish. Yeah. One question,

Speaker 2:          01:00:17       style of images, you need to do a new training like this. Yeah, so the downside of this effort is although it has the flexibility to work with any style, any content, every time you want to generate an image, you have to do this training loop. While the other network that you talked about doesn't need that because the model is trained to to convert to content to a style and just give it and goes trained. And that broke on many kinds of look which network you talk about this network. So do we need to train these network on morning images? Usually not. This network is trained on millions of images. It's basically seen everything you can imagine. Yeah.

Speaker 2:          01:01:04       What do you mean back propagate properly here? You're not training the network, you're giving this image competing the backpropagation and going back to the image. Only updating the image. You don't update the network. So then where does the art, it comes from content, CN style as it comes from the stylus. So the loss function, you bait. The baseline is you have content, CN stylists, because you've chosen a content picture in a sight picture and now every, at every step you will find the new content g and strategy back, propagate updates, give it again, get the new content giants dig, update again and so on.

Speaker 2:          01:01:44       No did the art never touches the just one time. The art image just touches one time. Then your electric, you can you extract stylists and then that's all you don't use it again. Okay. Let's do one more question yet here. The content or the style? Good question. Why do you start with white nose instead of the content or the style? Actually, do you think it's better to start with the content or this time? Probably the style. I think probably the contents because uh, the, the edges at least look like the content is to help the network converged quicker. Yeah, that's true. You don't have to start with white noise in generally the baseline you started with white noise so that anything can happen is he'll give you the content to start with. He's going to have a bias towards the content but gives you train longer.

Speaker 2:          01:02:32       Okay. One more question and then we can run these style of his content copies. Like know what is style, price, image and it doesn't understand what content and style, but you mentioned it finds the edges on the image and so you can give the contents image and extract the shoe first layers to get information about them. Because when it was trained on classification, it needed to find the ages to find that a dog is a dog. You first need to find the edges of the dog sweets. It's trained to do so. And for the style, it's complicated to understand the style, but the network science, all the features on the image. And then we use the post processing techniques and is called the grant matrix. In order to extract what we called style, it's basically a, a cross correlation of all the features of the network. We will learn it together later. Okay,

Speaker 1:          01:03:22       let's move on to the next application because we don't have too much time. So this is the one I prefer, uh, given a ten second node, your speech, detect the word activate. So you know, we talked about trigger word detection and there are many companies that have this wake word thing where you have a device at home and when you say you're searching a word you'd activates itself. So here's the same thing for the word activate. What data do we need?

Speaker 3:          01:03:45       Okay,

Speaker 1:          01:03:47       do we need a lot or no?

Speaker 1:          01:03:51       Probably a lot because there are many accents. And one thing that is counter intuitive is that if two humans, like let's say, let's say to two women speak as a human, you would say this voices are are pretty similar, right? You can detect the word. What did network sees is a list of numbers that are totally different from one person to another because the frequencies we're using, our voices are totally different from each other. So the numbers are very different. Although as a human we feel that it's very similar. So we need a lot of ten second video clips. Let's see what should be the distribution. It should contain as many accidents as you can, as many female male voices, uh, kid, adults, uh, and so on. What should be the input of the network? It should be a ten second video clip that we can represent like that. The ten second video clip is going to contain some positive words in green. Positive word is activate and it's also going to contain negative words in pink, like kitchen lion, whatever words that are not activate. And we want only to detect the positive word. What should be the sample rates? Again. Same question you would test on humans, uh, you would, you would, you would also talk to an expert in spatial cognition to know what's

Speaker 6:          01:05:18       the best sample rate to use for speech processing. What should be the output?

Speaker 3:          01:05:24       Any ideas? Okay, ma'am?

Speaker 6:          01:05:38       Classification. Yes. No, so zero one actually, let's make your test. Let's, let's do a test. So we have three [inaudible] speech here. Speech, one, speech to speech. The three. I don't know if we have the sound here. Do we have the sound?

Speaker 3:          01:05:58       Maybe we'll have it now. Okay, let's try it. So this is labeled one.

Speaker 6:          01:06:15       Nobody speaks Italian in the, in the, in the room. No. Second one. Okay. What's the wake word? Has Anybody found? What was the trigger word? We need more? So, you know what's funny is this is the right scheme to label. Like it's definitely possible, but it seems that even for humans, this labeling scheme is super hard. We're not able to find what's, what's happening. Like I don't know, even if I did this slide, I don't even remember. No kidding. Now let's try something else. Okay. So now we have a different labeling scheme that tells us also where the wake word is happening. Let's hear it again

Speaker 7:          01:07:27       because it's an inclusive community within the shopping.

Speaker 6:          01:07:32       Okay. What's the trigger word for Angel? For Mary Jo means afternoon in Italian. Okay. So you see what I'm trying to illustrate is a comfort a human to the computer and you will get what's the right labeling scheme to use. And of course the labeling scheme here is going to be better for the model rather than the first one. And we just proved it. Uh, the, the, the important thing is to know that the first one would also work. We just need a ton of data. We need a lot more data to make the first labeling scheme work, then we need for the second one. Does that make sense? So, yeah, we will use something like that. You get,

Speaker 2:          01:08:16       it's the acquisition where it starts or when you have one. Good question. Actually, this is not the best labeling scheme, as you said. Should the one come before or after

Speaker 6:          01:08:27       stir the word was said? What'd you guys think before?

Speaker 2:          01:08:33       Yeah, you will see,

Speaker 6:          01:08:34       uh, record your general networks are going basically to look at the data just as human dude, like temporarily from the beginning to the end. And in this case you need to hear the word in order to detect it. So we're going to put the one right after the word was said. And another issue that we have with this is that there are too many zeros. It's highly unbalanced. So the network is pushed to always predict Zeros. So what we do as a hack, and there's a lot of hacks like that happening in papers, if you read them, we're going to add several ones after the word was say I would add 20 ones basically. Okay. So this is our labeling scheme. Now what should be the last activation of our network?

Speaker 2:          01:09:23       Sigmoid function? Yeah, sigmoid. But sequential

Speaker 6:          01:09:26       for every time step you would use a sigmoid throughout. Put Zero or one basic. No worries. You don't understand specifically what networks were using. You're going to learn it in a few weeks. So the architecture should, should be like a recurrent neural network. Probably a convolutional neural networks might work as well. We'll see it later on in the course. And the last function should be the same as before, but we should make it sequential for every time step. We should use the loss function like that and we should some them over all the times. That sounds good. So another insights on this project, I'll take it after, uh, is what it was critical to the success of this project. I think there are two things that are really critical when you, when you build such a project. The first one is to have a straight strategic data acquisition pipeline. So let's talk more about that. We said that our data should be ten second video clips that content positive and negative words from many different access. How would you collect this data?

Speaker 3:          01:10:43       Yes.

Speaker 2:          01:10:48       You said you paid the host gives you 10 seconds of their voice comments then maybe, but yeah, I think you, you, you can take your phone, go around

Speaker 6:          01:10:58       this and that's actually how we did it. We took our phones, we went around campus and we got some audio recordings. So one way to do it is that to go and get ten second or the recordings from different people with a large distribution of accents. And then what did you do? You label you labeled by hand. That's one method. Is it long or short? Is it, is it quick or no, it's super slow yet. Oh, subtitles in movies. Oh that's a good idea. Actually. You could like based on the licensing of the movie, you could like take a Nogio from a movie and you did the subtitles and you were looking for activate and every time the subject say activate, you could label your data. That's Super Fun. That's super good actually. You could label automatically using that.

Speaker 6:          01:11:51       Yeah, so that's a good idea. I think there's another way to do it that is closer to that, which is we're going to collect three databases. The first one is going to be the positive word database. The second one is going to be the negative word database. The third one is going to be the background though database. So I think the ground 10 seconds, I insert randomly from one, two, three negative words. And I insert randomly from one, two, three positive words, making sure it doesn't overlap with a negative words. Okay. What's the main advantage of this method?

Speaker 6:          01:12:32       Programmatic generation of samples? Yeah, program at some generation of samples and automated labeling. I tend label, I know where I inserted my positive words. Can I just add ones where I inserted it? I can generate millions of data examples like that just because I found the right strategy to to create data. You see the difference between the two methods, the one where you have to go out and collect data and the one where you just go out, collect positive words, negative words, and then find background noise on youtube or wherever you have the right license to use. It's, it's a big difference and this can make to make a company succeed compared to another company, it's very common. So I would go on campus, take one second audio clips of positive words, put it into the database in green, take one second. Audio clips of negative words of the same people as well. Put it in the pink database and get background noise from anywhere. I can find it. It's very cheap. And then create these synthetic data, label it automatic and you know, we'd like five positive words, five negative words, five backgrounds. You can create a lot of data points.

Speaker 8:          01:13:43       Okay.

Speaker 6:          01:13:44       Okay. So this is an important technique that you might want to think about in your projects. The second thing that is important for the success of such a project is the architecture of search and hyper parameter to uni. So all of you, you will have complicated projects where you would be lost, uh, regarding Jean cur architecture to use. At first it's a complicated process to find the architecture, but you, you should not give up. And the first I would say is talk to the experts. So let me tell you the story of this project. Uh, first I, I started like looking at the literature and figuring out what network I could use for this project. And I ended up using that for, for, for the beginning part, I use a four year transform to extract features from the speech who's familiar with spectrum rams or four year transforms.

Speaker 6:          01:14:35       So for the others tinker about audio speech as a one d signal, but everyone does signal can be decomposed in a sum of signs and Co signs with a specific frequency and amplitude for each of these and so I can convert a one d signal into a matrix for with with with basically with basically one axis that is the frequency one axis. That is the time going from going from zero to 10 seconds and I will get the value of all the the amplitude of this frequency, so maybe this one is a strong frequency is one is a strong frequency. This one is a low one and so on for every time step. This is a spectrogram of an audio speech. You're going to learn a little bit more about that. So after I got the Spectrogram, which is better than the one the signal for the network, I would use an LSTM which is a restaurant general network and add a sigmoid layer after it to get probabilities between zero and one I will threshold them everything.

Speaker 6:          01:15:42       We more than 0.5, I would consider that it's a one, everything less. It's a zero. I tried for a long time fitting this network on the data. It didn't work, but one day I was working on campus and I, I, I, I found a friend that was an expert in spatial cognition. He's worked a lot on all these problems and he exactly knew that this was not going to work. He could told me, he could have told me. So he told me there are several issues with this network. The first one is you're hyper parameters in the fourier transform their wrong. Go on my guitar. You will find what hyper parameters are used for this four year transform. You will find specifically what sampling rate, what's window size, what frequencies are used. So that was better. Then he said one issue is that your record neural network is too big.

Speaker 6:          01:16:32       It's super hard to train. Instead, you should reduce it. So I've used, so he told me to use the convolution to reduce the number of time steps of my audio clip. You will learn about all these layers later. Uh, and also use batch norm, which is a specific type of layer that that makes the training easier. And finally you get your sigmoid layer and you output zeros and ones, but because the outputs time steps is smaller than the inputs, you have to expand it. So you need an expansion algorithm. Just a script that expands every zero in two Zeros, let's say in two ones and so on. And now I get to another architecture that I managed to train within a day and this was all because I was lucky enough to find the experts and gets advice from this person. So I think you will run into the same problems as I run into during your projects.

Speaker 6:          01:17:26       The important thing is spend more time figuring out who is the expert and who can tell you the answer rather than trying. Yeah. Trends and things. I think this is uh, an important thing to think about. Okay. So don't give up and also use their analysis, which we're going to see later. Uh, we have two more minutes so I'm not going to go over this one. I'm just going to talk about it quickly there. Another way to solve wake word detection and the other way is to use the triplet loss algorithm. Instead of using anchor positive and negative faces, you can use audio speech of one second. Anchor is the word activate. Positive is the word activates, said differently and negative is another word. You will train your network to encode, activates in a certain vector and then compare the distance between vectors to figure out this activate is present or not. Okay. We have about two more minutes, so I'm going to, my butt is on me. Uh, so just to finish with two more slides, now that you've seen some loss function, I want to show you another one and I want you to tell me what application does this beauty food loss correspond to these one of the most beautiful loss I've seen him

Speaker 2:          01:18:55       so someone can tell me what's the application, what problem are we trying to solve if we use this loss function?

Speaker 3:          01:19:05       Okay.

Speaker 2:          01:19:07       Speech recognition. No, it's not the good, good try. Regression. That's true. It's a regression problem, but it's a specific regression problem. Bounding box. Good bounding boxes. Object detection. This is object detection, so I put the paper here. You can check it out, but how do you know that it's objective

Speaker 6:          01:19:29       detection?

Speaker 2:          01:19:33       Oh, you've done it before. Okay, so this is the last one. She knows her network are called

Speaker 6:          01:19:40       Yolo and the reason you can find out these bounding boxes is because if you look at the first year you would see that it's comparing x two two x predicted x two through x predicted y to true why? This is the center of a bounding box, x, y second term is w and h, w and h stands for width and height of the bounding box and it's trying to minimize the distance between the bounding box and the predicted bounding box. Basically the third term has an ideal indicator function with objects. It's saying if there is an object you should have a high probability of objectness. The fourth therm is saying that if there is no object, you should have a lower probability of objectness. And finally the final term is telling you you have to find a class that is in this box. Is it a chat? Is the dog, is it an elephant? Is whatever. So this is an object detection, uh, loss function. Actually. Do you know why? Why you? We have a square root here.

Speaker 6:          01:20:45       Hmm. What was the TVT no products. The reason we have the square roots is because you want to penalize more errors on small bounding boxes rather than big bounding boxes. So if I give you an image of a human like that, indoor cats like this, you can have, so this box, the one insight is the ground truth is a very tight box. This one, same and the box that are predicted or the predictions. So these are the predictions and the other ones are the ground truth. What is interesting is that a two pixel error on these cats is much more important than a two pixel air on this human, because the box is smaller. So that's why you use a square root to penalize more the errors on small boxes then on big boxes. Okay. And finally, the final slide. Okay, let's go over the, so just recalling what we have for next week, you have two modules to complete for next Wednesday.

Speaker 6:          01:21:52       Uh, which are [inaudible] with the following quiz and the following programming assignments, c one m four, with one quiz and true programming assignments. You're going to build your first deep neural network. This is all going to be on the web. It's already on the website and we'll publish just slides. Now, uh, you have ta project mentorship that is mandatory, this switch. So ta Project mentorships are mandatory this week to start the week before the project proposal. The week before the project? No, after the price proposal, after the project milestone and before the final project submission. Okay. And Fry ATS sections, you're going to do some neural style transfer and our generation, uh, fill in the AWS forum. I don't know if it's been done yet. We were going to try to give you some credits for your projects with gps. Okay. Thanks guys.