1
00:00:05,720 --> 00:00:09,870
All right. Hey everyone, welcome
back. This is people hear me okay.

2
00:00:11,340 --> 00:00:15,540
All right. So if as usually you can
take a second to enter your, uh,

3
00:00:15,750 --> 00:00:19,110
id so we know who's here.
Um,

4
00:00:19,380 --> 00:00:24,380
so today's lecture will be a
choose your own adventure lecture.

5
00:00:25,470 --> 00:00:27,090
Um,
so I think you know,

6
00:00:27,091 --> 00:00:32,091
by now you've learned a lot about the
technical aspects of building learning

7
00:00:32,101 --> 00:00:36,960
algorithms. And then in the third
course, in the third set of modules,

8
00:00:36,961 --> 00:00:40,590
you saw some of the principles
for debugging, learning
algorithms and how they,

9
00:00:40,600 --> 00:00:45,600
as you use these tools in order to be
efficient in how you build a machine

10
00:00:45,781 --> 00:00:50,670
learning application. What I want to
do today is a step through with you,

11
00:00:50,730 --> 00:00:54,900
a moderately complicated
machine learning application.

12
00:00:55,260 --> 00:00:56,170
And,
um,

13
00:00:56,430 --> 00:01:01,430
throughout all of today's lecture I'm
going to step you through a scenario and

14
00:01:01,771 --> 00:01:04,590
then as you to kind of
choose your own adventure.

15
00:01:04,680 --> 00:01:08,010
Because if you are working on this
project, what are you going to do, right?

16
00:01:08,400 --> 00:01:11,610
And to give you more of that
practice in the next, um,

17
00:01:11,820 --> 00:01:16,820
what are in a bit that we
have a on thinking through
machine learning strategy.

18
00:01:18,230 --> 00:01:22,700
Um, and you know, I've, I've
seen in so many projects, uh,

19
00:01:23,010 --> 00:01:28,010
they're there sometimes things that a
less strategically sophisticated team will

20
00:01:28,621 --> 00:01:29,670
take a year to do.

21
00:01:30,780 --> 00:01:34,770
But if you're actually very strategic
and very sophisticated in deciding what

22
00:01:34,771 --> 00:01:37,440
you will do next,
very how the drive the project forward.

23
00:01:37,830 --> 00:01:41,550
I've seen many times that what a
different team will take to do.

24
00:01:41,730 --> 00:01:45,870
Maybe you could do it in a
month or two and you know,

25
00:01:45,900 --> 00:01:50,550
if you are trying to read a
research paper or build a business,

26
00:01:50,551 --> 00:01:51,384
I've got the product,

27
00:01:51,410 --> 00:01:56,280
the ability to drive a machine learning
project quickly gives you a huge

28
00:01:56,281 --> 00:01:57,390
advantage and just,
you know,

29
00:01:57,391 --> 00:02:00,750
you're making much more efficient
use of your life as well. Right. Um,

30
00:02:00,900 --> 00:02:04,310
so for today I'd like to, uh, uh,

31
00:02:04,500 --> 00:02:08,310
I'm going to pose a scenario, post a
machine learning application and say,

32
00:02:08,700 --> 00:02:12,270
all right, I mean, you are the CEO of this
project. What are you going to do next?

33
00:02:12,271 --> 00:02:16,290
So, but I'd like to have today's
meeting be quite interactive as well.

34
00:02:16,291 --> 00:02:21,090
So can they get people to sit in
groups of two and ideally three or so,

35
00:02:21,120 --> 00:02:23,110
maybe plus minus one. And, um,

36
00:02:23,490 --> 00:02:26,820
tried to sit next to someone that you
don't work with all the time. Uh, so,

37
00:02:26,821 --> 00:02:29,340
so if a Sydney sitting next year,
best friend,

38
00:02:29,370 --> 00:02:31,500
I'm glad your best friend
is in the class review,

39
00:02:31,740 --> 00:02:34,190
but go sit through someone
else because I think, um,

40
00:02:35,040 --> 00:02:37,670
I've done this multiple times and
the discussions that he, Richard,

41
00:02:37,680 --> 00:02:40,230
you talk to someone that
you don't know super well.

42
00:02:41,550 --> 00:02:46,530
So as you take a second and introduce
yourself and just your neighbor,

43
00:02:46,531 --> 00:02:47,364
I guess.

44
00:02:47,790 --> 00:02:52,790
So the example I want to go through
today is actually a continuation of the

45
00:02:52,951 --> 00:02:55,480
example I described briefly, uh, uh, in,

46
00:02:55,490 --> 00:02:59,800
in the last lecture I us a building
a speech recognition system, right?

47
00:02:59,801 --> 00:03:02,350
So remember I briefly,

48
00:03:02,351 --> 00:03:05,230
I'm motivated this a trigger word,

49
00:03:05,290 --> 00:03:08,800
wake word or trigger word
detection system last time where,

50
00:03:09,850 --> 00:03:12,280
you know, uh, right, I,

51
00:03:12,290 --> 00:03:16,780
I actually had both an Amazon
echo and Google home, uh,

52
00:03:16,870 --> 00:03:17,740
but you know,

53
00:03:17,780 --> 00:03:21,160
is this a lot of work to configure these
things to turn on and off your light

54
00:03:21,161 --> 00:03:25,550
bulbs. Um, and so if you
can build a chip, uh,

55
00:03:25,630 --> 00:03:29,520
to sell, to say, a lamp
maker to recognize, uh,

56
00:03:29,740 --> 00:03:33,350
phrases like, you know, let's say
we call the Lamp Robin, right?

57
00:03:34,960 --> 00:03:38,890
Then you can recognize phases like
Robin's her in the [inaudible], right?

58
00:03:39,430 --> 00:03:43,330
Robin turned off and he kept that little
switch to give this thing different

59
00:03:43,331 --> 00:03:47,200
names were called robbers or
leaner or allies or somethings.

60
00:03:47,230 --> 00:03:51,280
You can also lean air turned on the,
turned off his gift to give you a lamp,

61
00:03:51,370 --> 00:03:53,590
a name, and just say, Hey,
Robert turned on right.

62
00:03:53,950 --> 00:03:57,520
So rather than detecting different
names and turn on and turn the off,

63
00:03:57,550 --> 00:04:01,060
I'm just going to focus on just
for the technical discussion,

64
00:04:01,270 --> 00:04:05,320
I'm just going to focus on the
phrase Robin turned on. Uh,

65
00:04:05,350 --> 00:04:08,620
but it's kind of the same problem and
we need to solve like four times to give

66
00:04:08,621 --> 00:04:12,430
it to names or to turn on and turn off.
So I'm going to abbreviate Robert,

67
00:04:12,431 --> 00:04:16,390
turning on this Rto rights.
If you want to call your name Roberts,

68
00:04:16,450 --> 00:04:21,400
and I'm a tell you lamp to turn
on. Um, I think it was inspired by,

69
00:04:21,430 --> 00:04:23,280
well,
Isaac Asimov wrote these alums,

70
00:04:23,650 --> 00:04:27,250
robotic novel C's and all this
robots name started with r.

71
00:04:27,520 --> 00:04:32,200
So maybe our robot turn on. Um, and so,

72
00:04:32,440 --> 00:04:36,230
uh, let's see. So let's see that, um,

73
00:04:36,400 --> 00:04:41,400
you are the new CEO of a small startup
with fie persons are and you go is the

74
00:04:44,651 --> 00:04:47,200
building is to build a circuit.
Oh,

75
00:04:47,210 --> 00:04:52,000
actually your goal is to build a learning
algorithm that can recognize this race.

76
00:04:52,030 --> 00:04:53,450
Robert turned on,
uh,

77
00:04:53,610 --> 00:04:58,610
so that when someone buys this lamp and
they say Robert's in the on the lamp can

78
00:04:58,930 --> 00:05:03,920
turn on, right? And just focusing on the
task of building and then you know, to,

79
00:05:03,930 --> 00:05:06,190
to, to be CEO of this thought. They've
used to do a lot of things, right?

80
00:05:06,191 --> 00:05:08,090
I need to figure out how to
do the embedded circuitry.

81
00:05:08,091 --> 00:05:10,020
You figure out who the land
Meeker's the sales journey.

82
00:05:10,330 --> 00:05:12,130
So there's all that stuff.
But for today,

83
00:05:12,131 --> 00:05:15,760
let's just focus on the machine
learning aspect of it. Um,

84
00:05:15,790 --> 00:05:19,950
and so my first question to
you is very open ended is, but,

85
00:05:19,960 --> 00:05:21,610
and this is the life of the CEO,
right?

86
00:05:21,610 --> 00:05:25,210
You wake up one day and you just
got to decide what to do. Um, but,

87
00:05:25,211 --> 00:05:29,110
so my first question to you is, an open
ended question is, uh, you're the CEO,

88
00:05:30,340 --> 00:05:32,440
you're going to show up
at work, uh, you know,

89
00:05:32,441 --> 00:05:37,420
tomorrow in your startup office and you
want to build a learning algorithm to

90
00:05:37,421 --> 00:05:42,250
detect the face Robert turned on
for this application, right? So, um,

91
00:05:42,280 --> 00:05:45,100
so my question is what are
you going to do? Right?

92
00:05:45,101 --> 00:05:48,910
So take a take a minute to answer
that by yourself first. No, don't,

93
00:05:48,911 --> 00:05:50,520
don't discuss your neighbor yet,
but you know,

94
00:05:50,521 --> 00:05:51,850
you're going to show
up in your office and,

95
00:05:52,100 --> 00:05:55,120
and you're going to start working on this
engineering problem to build a neural

96
00:05:55,121 --> 00:05:59,510
network to do this. So, uh, and, and do
this as yourself, right? Don't, don't,

97
00:05:59,511 --> 00:06:03,290
don't pretend that Ya'll,
this hypothetical, whatever,

98
00:06:03,740 --> 00:06:08,360
a startup CEO of 10 minute, $10 billion
to spend on whether it was just do this.

99
00:06:08,600 --> 00:06:12,890
See, yeah, I, but I, I don't think
this is a terrible style of idea. I, I,

100
00:06:12,920 --> 00:06:15,170
this is not the best idea,
but I think this could work.

101
00:06:15,171 --> 00:06:16,610
So you actually welcome to do this,

102
00:06:17,090 --> 00:06:20,120
but let's say you decide to do this
and you go into your office tomorrow,

103
00:06:20,530 --> 00:06:22,640
what do you do? Right? Once you take, um,

104
00:06:23,000 --> 00:06:27,090
why don't you take let's say two minutes
to enter an answer, then we can do,

105
00:06:27,100 --> 00:06:31,910
you can discuss in fact,
I think of, yeah. Yes.

106
00:06:32,300 --> 00:06:36,470
One thing I really like about answer
sheet the readers because the Singlish

107
00:06:36,471 --> 00:06:41,030
usher upon, right? Um, in fact, when
you're suddenly a new project, um,

108
00:06:41,270 --> 00:06:44,740
uh, uh, and I think, um, uh,

109
00:06:44,890 --> 00:06:47,660
it's not the only project
like that assuming you've
not worked in trigger words,

110
00:06:47,661 --> 00:06:49,220
it's action before,
you know,

111
00:06:49,221 --> 00:06:52,790
reading research papers or reading code
and get hub or reading blog posts and

112
00:06:52,791 --> 00:06:56,760
this problems as you're very good way
to quickly level of fuel knowledge. Um,

113
00:06:57,020 --> 00:07:01,190
and I think that, you know,
it, it turns out that, um, uh,

114
00:07:01,730 --> 00:07:06,430
in terms of your exploration
strategy, right? Um,

115
00:07:06,560 --> 00:07:11,240
I want to describe to you how
I read research papers. Um, uh,

116
00:07:11,270 --> 00:07:12,103
which is,

117
00:07:12,710 --> 00:07:17,270
so this is not a good way
to review the literature,

118
00:07:17,300 --> 00:07:22,300
which is if the x axis is time and
the vertical axis is research papers,

119
00:07:22,880 --> 00:07:27,740
where some people will do is find the
first research paper and read that until

120
00:07:27,741 --> 00:07:32,130
it's done and then go in and find
a second research paper and V.

121
00:07:32,130 --> 00:07:32,990
Dot until it's done.

122
00:07:33,020 --> 00:07:35,990
And then you can find the third
research paper printers for this is very

123
00:07:35,991 --> 00:07:39,470
sequential way of um,
meeting research papers.

124
00:07:39,471 --> 00:07:42,040
And I find it the more strategic way to,

125
00:07:42,041 --> 00:07:46,130
to go through these reasonables is
everything ranging from blog posts, um,

126
00:07:46,280 --> 00:07:48,920
lots of good medium articles
and they explain things very,

127
00:07:48,950 --> 00:07:53,020
it's our research papers. Um, right.

128
00:07:53,300 --> 00:07:58,300
Good hog is if you use a
parallel exploration process
where this does actually

129
00:07:59,901 --> 00:08:02,710
what it feels like when I'm doing research
on where I'm trying to learn about a

130
00:08:02,720 --> 00:08:04,370
new few of them, not that expanded, right?

131
00:08:04,371 --> 00:08:06,710
So after she's done a lot of
work on trigger words detection,

132
00:08:06,920 --> 00:08:10,780
but if it hadn't worked on this before
then I would probably be fine, you know,

133
00:08:10,880 --> 00:08:12,140
three papers.
So again,

134
00:08:12,170 --> 00:08:16,850
it's classes as time and virtually acids
as different papers and um, you know,

135
00:08:16,851 --> 00:08:21,770
read a few papers kind of in parallel
at a surface level and skimmed them.

136
00:08:21,890 --> 00:08:26,890
And based on that you might decide to
read that one in greater detail and then

137
00:08:27,111 --> 00:08:31,490
to add other papers that you started
screaming and maybe finding another one.

138
00:08:31,491 --> 00:08:36,491
Do you want to read in great detail and
into gradually add new papers to your

139
00:08:36,501 --> 00:08:41,190
reading lists? A and B, some to
confusion and some not to completion. Um,

140
00:08:41,720 --> 00:08:45,560
you know, I was actually chatting
with um, uh, uh, one of my friends,

141
00:08:45,561 --> 00:08:50,120
a PDRP former students at Berkeley who
mentioned that he was wanting to learn

142
00:08:50,121 --> 00:08:52,580
about a new topic and he, he was, uh,

143
00:08:52,581 --> 00:08:56,730
he told me he was compiling a reading
list of 200 research papers they want to

144
00:08:56,731 --> 00:08:59,400
read. That sounded like a lot.
You rarely read 200 papers,

145
00:08:59,401 --> 00:09:04,260
but I think if you read 10
papers, you have a basic
understanding. If you read 50,

146
00:09:04,261 --> 00:09:07,590
you have a pretty decent
understanding of read like a hundred.

147
00:09:07,591 --> 00:09:09,760
I think you were very
good at understanding, uh,

148
00:09:10,200 --> 00:09:14,460
of of a few but often does
this time well spent I guess.

149
00:09:14,930 --> 00:09:18,830
Um, and, uh, some other
tips, again, this is,

150
00:09:19,230 --> 00:09:22,800
I'm really thinking if you really are
CEO of this startup and this is what you

151
00:09:22,801 --> 00:09:27,150
want to do, what advice
would I give you? Um, uh,

152
00:09:27,210 --> 00:09:31,670
when you're reading papers are other
things to realize? Uh, one is that, uh,

153
00:09:31,680 --> 00:09:35,400
some papers don't make sense, right?
And this fine, uh, uh, you know,

154
00:09:35,401 --> 00:09:39,060
even ivy some papers I just go, nope, I
don't think that makes sense. Uh, and,

155
00:09:39,330 --> 00:09:44,330
and it's not uncommon for us to
find papers from a decade ago that,

156
00:09:45,001 --> 00:09:48,450
and we learned that half of it was free
and the other half of it, you know,

157
00:09:48,540 --> 00:09:52,910
was really talking about things that were
not that important. So it was okay. Uh,

158
00:09:52,980 --> 00:09:55,950
authors, you know, usually
papers of technically accurate,

159
00:09:55,951 --> 00:09:58,260
but often what they thought was important.

160
00:09:58,261 --> 00:10:01,560
Like maybe an author thought that using
bash them was really important for this

161
00:10:01,561 --> 00:10:04,560
problem, but it just turns out not to
be the case that that happens a lot.

162
00:10:04,760 --> 00:10:05,940
But that happens sometimes.

163
00:10:06,690 --> 00:10:10,860
And I think the other tactic that I see
Stanford students sometimes not using

164
00:10:10,880 --> 00:10:14,910
now is talking to experts,
including contacting the authors.

165
00:10:14,940 --> 00:10:18,990
So when I read the paper, um, uh, I don't,

166
00:10:19,170 --> 00:10:22,530
I don't bother the authors unless I've
actually like tried to figure it out

167
00:10:22,531 --> 00:10:23,341
myself.
Right?

168
00:10:23,341 --> 00:10:27,030
But if you actually spend some time
trying to understand the paper and if it

169
00:10:27,031 --> 00:10:30,380
really doesn't make sense
to you, uh, uh, uh, is, is,

170
00:10:30,390 --> 00:10:34,680
is okay to email the authors and see if
they respond and, and people are busy.

171
00:10:34,681 --> 00:10:36,490
Maybe there's a 50% chance of responding.

172
00:10:36,510 --> 00:10:40,050
And that's okay because it takes you five
minutes to write an email and there's

173
00:10:40,051 --> 00:10:44,880
a 50% chance to get back to
you. That could be time pretty
well spent. Uh, uh, but,

174
00:10:44,881 --> 00:10:48,180
but don't, don't, don't bother people
on this. You tried to do your own work.

175
00:10:48,240 --> 00:10:50,460
I actually got a lot of emails from,
you know,

176
00:10:50,461 --> 00:10:53,430
high school students that do not feel
like they've done their own work.

177
00:10:53,450 --> 00:10:58,260
And I just write and then, Eh, so, so
just don't, don't, don't bother people.

178
00:10:58,260 --> 00:11:03,030
And as you've actually tried to,
um,

179
00:11:04,320 --> 00:11:07,280
cool. So after, um,

180
00:11:08,070 --> 00:11:11,490
looking at the literature
and having a base,

181
00:11:11,520 --> 00:11:15,600
maybe downloading a open
source implementation or
getting a sense of an avenue

182
00:11:15,601 --> 00:11:16,740
where they try,
oh,

183
00:11:16,780 --> 00:11:20,490
it turns out they trigger words detection
literature is actually one literature

184
00:11:20,491 --> 00:11:23,880
where there isn't consensus on this is
a good algorithm does is a bad algorithm

185
00:11:23,930 --> 00:11:27,750
where despite all the trigger words
or wake word detection systems that,

186
00:11:27,780 --> 00:11:30,660
you know, some of you may use
already, uh, there, there,

187
00:11:30,661 --> 00:11:33,700
there isn't actually a
consensus in the, in, in,

188
00:11:33,710 --> 00:11:37,380
in their research me today on like,
this is the best average try. Um,

189
00:11:38,670 --> 00:11:41,370
but so let's say that, um,
you've read some papers,

190
00:11:41,371 --> 00:11:43,290
downloads some open
source and plantations,

191
00:11:43,291 --> 00:11:46,920
and now you want to start
training your first system, right?

192
00:11:46,930 --> 00:11:48,150
The last time we talked about this,

193
00:11:48,151 --> 00:11:51,870
we talked a little bit about how much
time you would spend to collect data.

194
00:11:51,900 --> 00:11:54,490
And you know,
we said we spend a small amount of time,

195
00:11:54,510 --> 00:11:56,500
spend like a day or
maybe two days at most.

196
00:11:56,860 --> 00:12:00,420
So collect your first data set
to start training up a motto. Um,

197
00:12:00,730 --> 00:12:05,350
but my next question to you is a,
what data would you collect?

198
00:12:06,000 --> 00:12:08,650
Right? Um, in particular,

199
00:12:08,920 --> 00:12:12,910
what train def test data

200
00:12:19,930 --> 00:12:20,651
would you collect?

201
00:12:20,651 --> 00:12:25,651
So you've decided on an initial
new inotrope architecture
and you want to train

202
00:12:25,901 --> 00:12:28,480
something to recognize this face.
Robert turned on.

203
00:12:28,590 --> 00:12:33,550
I think there's a pervy I don't think
as possible to download the data set.

204
00:12:33,551 --> 00:12:36,400
I don't think anyone has collected
a data set with the West.

205
00:12:36,401 --> 00:12:38,260
Robert turned on and
posted on the Internet.

206
00:12:38,261 --> 00:12:41,110
So you have to collect your own data for
this particular trigger phrase that you

207
00:12:41,111 --> 00:12:43,360
want to use. But it's um, you know,

208
00:12:43,390 --> 00:12:47,830
as CEO of this thoughts of trying to
build a new and yet to detect the phrase

209
00:12:47,831 --> 00:12:52,540
Robert turned on.
What data do you collect?

210
00:12:52,720 --> 00:12:54,790
Right?
So why don't you take once you again take,

211
00:12:55,240 --> 00:13:00,100
Oh know let's say three minutes
to write an answer to this.

212
00:13:00,790 --> 00:13:03,880
Yeah, I think this is
an interesting one. Um,

213
00:13:04,540 --> 00:13:09,070
Robert turned on over and over.
And then data augmentation,

214
00:13:09,910 --> 00:13:13,980
data augmentation is one of
those techniques that, um, uh,

215
00:13:14,020 --> 00:13:16,040
is a way to reduce,
uh,

216
00:13:16,150 --> 00:13:20,310
in senior learning our room because
you're generating more data and, uh,

217
00:13:20,440 --> 00:13:24,910
having worked on this problem, I happen
to know data augmentation works, you know,

218
00:13:25,120 --> 00:13:28,960
is very useful for this problem.
But if you didn't already know that fact,

219
00:13:28,961 --> 00:13:32,830
this is one of the things I would
probably not do right away because I would

220
00:13:32,860 --> 00:13:34,390
train a quick and dirty system,

221
00:13:34,780 --> 00:13:38,680
validate that you really have a high
clearance problem before investing in the

222
00:13:38,681 --> 00:13:41,830
effort and do data augmentation. So they
only as well as those techniques that,

223
00:13:42,160 --> 00:13:45,670
so, you know, it never hurts
her barely hurts, usually hopes,

224
00:13:45,671 --> 00:13:50,320
but I don't bother to make that
investment unless you have collected the

225
00:13:50,321 --> 00:13:53,770
evidence that you actually have a high
his problem and that this is actually a

226
00:13:53,771 --> 00:13:55,680
good use of your time.
Right?

227
00:14:07,720 --> 00:14:08,553
Yeah.

228
00:14:10,660 --> 00:14:15,550
I think this one that she does
actually nice. So, um, uh,

229
00:14:15,610 --> 00:14:18,730
required everyone to start to Saint
Robert turned a hundred times that really

230
00:14:18,731 --> 00:14:23,080
nice thing about that. You can
get that really quickly. Um, uh,

231
00:14:23,110 --> 00:14:25,070
when I'm working with teams,
um,

232
00:14:25,150 --> 00:14:29,580
I actually think in terms
of hours in terms of hollow
it take us to do something.

233
00:14:29,590 --> 00:14:33,820
So this one you could probably do and
like 30 minutes, right? So get you,

234
00:14:33,880 --> 00:14:38,230
the collected 30 minutes ain't get going
or, or have you run around Stanford,

235
00:14:38,231 --> 00:14:43,050
they just ask, you know, friends or
strangers to speak into your, uh,

236
00:14:43,090 --> 00:14:45,530
laptop microphone.
You didn't spend a few hours together,

237
00:14:45,560 --> 00:14:48,760
much bigger data set then pass with
the startups. I apparently do that.

238
00:14:48,780 --> 00:14:52,850
Probably actually go to collect data
in several hours routing only spend 30

239
00:14:52,851 --> 00:14:53,151
minutes.

240
00:14:53,151 --> 00:14:55,520
But this is actually pretty interesting
as well because unless you get it done

241
00:14:55,521 --> 00:14:59,660
pretty quickly, it makes
sense. Right? So, um,

242
00:15:04,240 --> 00:15:09,150
yeah, so let me actually the uh, uh,
share some more concrete advice. Right.

243
00:15:09,230 --> 00:15:13,510
And I think actually some point sometime
back to prepare a homework problem that

244
00:15:13,511 --> 00:15:17,370
you see later in this course, Ken and
Eunice and I were actually, you know,

245
00:15:17,410 --> 00:15:20,830
building this system partially to, to to
create the homework, right? That, that,

246
00:15:20,831 --> 00:15:23,920
that you see later in
this call. So this is, uh,

247
00:15:24,100 --> 00:15:25,630
this trigger where thing is a nice run.

248
00:15:25,631 --> 00:15:29,390
The example that we're using the few
points throughout this course. Um,

249
00:15:29,890 --> 00:15:34,870
so here's one thing. You can do it.
This does actually what, um, uh,

250
00:15:34,900 --> 00:15:37,690
what we did right,
which is a collect,

251
00:15:38,910 --> 00:15:42,220
I'm simplifying a little bit.
Um,

252
00:15:44,890 --> 00:15:49,780
collect a hundred examples
of uh,

253
00:15:50,110 --> 00:15:52,240
uh,
ten second audio clips.

254
00:15:55,940 --> 00:15:57,680
Great. And so, uh,

255
00:15:57,730 --> 00:16:02,320
it turns out once you grab a hold of
someone and ask them to speak into your

256
00:16:02,321 --> 00:16:06,910
microphone, you know, you can
keep them for three seconds,

257
00:16:06,911 --> 00:16:08,650
which is how long it takes
to say Robert turned on.

258
00:16:08,860 --> 00:16:10,480
Or you can keep them for 10 seconds,

259
00:16:10,481 --> 00:16:13,480
which they're actually very willing to
spend an extra seven seconds of you.

260
00:16:13,540 --> 00:16:18,070
Right? Um, but so if this is 10
seconds of audio data, you know,

261
00:16:18,071 --> 00:16:23,071
so this is 10 seconds of audio and audio
is just patterns of a little changes in

262
00:16:23,351 --> 00:16:26,810
air pressure, right? So if you plot audio,
the reason it looks like this way form,

263
00:16:26,860 --> 00:16:31,860
it's just the way you're hearing my voice
is my voice or the speakers I creating

264
00:16:32,381 --> 00:16:35,320
very rapid changes in air
pressure and your ear measures,

265
00:16:35,321 --> 00:16:37,900
those very rapid changes in the
air pressure interprets as a sound.

266
00:16:37,930 --> 00:16:40,390
And so a microphone, uh, is a,

267
00:16:40,391 --> 00:16:43,000
is a sensitive device
for recording these very,

268
00:16:43,001 --> 00:16:44,980
very high frequency
changes in air pressure.

269
00:16:44,980 --> 00:16:48,340
And this plots there you see an audio
is just what is the air pressure at

270
00:16:48,341 --> 00:16:52,030
different moments in time,
right? But so given a, um,

271
00:16:52,770 --> 00:16:55,420
a ten second clip like this,

272
00:16:55,810 --> 00:17:00,810
if this is the fee second section
where they said Robert turned on,

273
00:17:02,980 --> 00:17:06,580
then what you would like to
do is to build a desk lamp.

274
00:17:06,581 --> 00:17:11,560
Say they can sit here and the lamp
is turned off, turned off, turn off,

275
00:17:11,740 --> 00:17:16,360
turn off, turn off, turn off. And
at the moment they finish saying,

276
00:17:16,361 --> 00:17:20,620
Robert turned on, you know, you
turn it on. So this is a open label.

277
00:17:20,621 --> 00:17:25,360
Why Rooney? Right. And then, and then
it's not detecting the free three.

278
00:17:25,390 --> 00:17:30,040
So. So, so what you want to do for the
trick word system is, um, at, you know,

279
00:17:30,041 --> 00:17:33,980
pretty much the moment they finish
saying Robert turned on, uh,

280
00:17:34,270 --> 00:17:38,410
you want your learning how room to
output a one that's your target label.

281
00:17:38,410 --> 00:17:41,260
Why saying Yep, I just
heard this trigger word. Uh,

282
00:17:41,270 --> 00:17:44,730
and for all other times you
want it to operate zero, right?

283
00:17:44,780 --> 00:17:49,090
Cause cause the one is when
you decide to turn on the lamp,

284
00:17:49,260 --> 00:17:54,110
that moment in time, right?
So to collected data sets, um,

285
00:17:54,200 --> 00:17:59,200
she has something you can do
which is collect 100 audio clips

286
00:18:03,670 --> 00:18:07,290
of 10 seconds each.
And you know,

287
00:18:07,470 --> 00:18:10,230
when I'm prioritizing my work whole,
my teams who are,

288
00:18:10,290 --> 00:18:14,810
I would really look at these numbers and
think, okay, let's say let's say she,

289
00:18:14,870 --> 00:18:15,860
if y'all doing it,
let's see,

290
00:18:15,861 --> 00:18:20,700
you are running around Stanford and you
want to collect a hundred audio clips.

291
00:18:20,760 --> 00:18:21,710
Uh,
uh,

292
00:18:21,780 --> 00:18:26,220
maybe 10 people tend to cliffs for person
or maybe a hundred different people.

293
00:18:26,510 --> 00:18:31,380
Um, I would actually estimate, you know,
if you go to Stanford cafeteria, uh,

294
00:18:31,410 --> 00:18:33,920
how long does it take to get one person?
Right?

295
00:18:33,921 --> 00:18:37,050
And you could probably get one person
every minutes or two if you go to a busy

296
00:18:37,051 --> 00:18:39,210
place out on like a staff,
a cafeteria,

297
00:18:39,570 --> 00:18:43,350
so you could probably get this done in
like a 100 to 200 minutes I two or three

298
00:18:43,351 --> 00:18:47,340
hours. Right? It's not that bad. So
you get this done quite quickly. Um,

299
00:18:48,570 --> 00:18:53,150
and so and and less. He collect hundred
audio clips and Ashley for the, for the,

300
00:18:53,160 --> 00:18:54,990
for the purposes of today,

301
00:18:54,991 --> 00:18:59,670
let's say you collect a hundred
audio clips to use for training

302
00:19:02,820 --> 00:19:05,490
25 for your deaf set

303
00:19:08,380 --> 00:19:12,120
at zero for the test set, right?
It's actually not that uncommon.

304
00:19:12,150 --> 00:19:13,420
If you're building a new product,

305
00:19:13,440 --> 00:19:17,220
they just not have a test set because
you go is to build something that y'all

306
00:19:17,221 --> 00:19:20,950
convinces, you know, just early
prototyping phases of a project.

307
00:19:21,000 --> 00:19:24,960
Sometimes I don't bother with a test
that if it goes to publish a paper,

308
00:19:24,961 --> 00:19:27,180
then of course you need a rigorous,
he collected test that.

309
00:19:27,450 --> 00:19:30,610
But if you're just building a product
and you don't need a rigorous evaluation,

310
00:19:30,611 --> 00:19:34,500
sometimes you can just get started
without dealing with a test set. Right?

311
00:19:34,501 --> 00:19:39,470
So it was pretty well get
started. Um, and then,

312
00:19:53,470 --> 00:19:56,160
all
right,

313
00:19:56,550 --> 00:19:59,640
so taking that audio clip from above,

314
00:20:00,320 --> 00:20:04,200
um, one thing you can do to
turn this into a supervisor,

315
00:20:04,220 --> 00:20:06,810
any problem is to take,

316
00:20:07,130 --> 00:20:11,970
so the phrase Roberts hair can be said
in less than three seconds. So let's see,

317
00:20:11,971 --> 00:20:14,880
you take three seconds
as the duration of audio.

318
00:20:15,240 --> 00:20:17,650
So we can do is a clip out.

319
00:20:18,150 --> 00:20:21,060
So let's say if here was when
Robin turn on them, I said,

320
00:20:21,510 --> 00:20:24,950
so what you can do is I'm
the type of paper that says

321
00:20:28,980 --> 00:20:33,240
why you can do is then it clip out
different audio clips of three seconds.

322
00:20:33,660 --> 00:20:37,470
So here's one audio clip.
You can take that audio clip,

323
00:20:38,460 --> 00:20:43,460
this is x and the target label is zero
because Robert turned on was not sad.

324
00:20:46,590 --> 00:20:51,550
And you can take on all this audio clip
at different band of nature clipped

325
00:20:51,940 --> 00:20:56,480
three second clip and that
clip also has its of a bugs.

326
00:20:57,880 --> 00:21:02,110
Um, and you know, for this one, right,

327
00:21:02,111 --> 00:21:05,980
which is a fee second
clip that, that, that,

328
00:21:05,981 --> 00:21:09,940
that ends at the real,
on the last part of the on sound,

329
00:21:09,970 --> 00:21:14,870
you would have a toggle label of one,
right? So, and, and, uh, when, when,

330
00:21:14,871 --> 00:21:16,960
when you learned about sequence models,
the rns,

331
00:21:16,961 --> 00:21:20,170
you learn a better method than this
explicit clipping. But for now,

332
00:21:20,171 --> 00:21:24,490
let's say you take these, um,
audio clips and turn it into,

333
00:21:25,120 --> 00:21:29,530
so take a ten second clip.
And by clipping out ran different windows,

334
00:21:29,531 --> 00:21:34,531
you can take your let's
say hundred uh clips.

335
00:21:35,860 --> 00:21:39,160
And because for each ten second clip,
you can take different windows.

336
00:21:39,400 --> 00:21:42,580
You could turn this into let's say 3000

337
00:21:45,820 --> 00:21:49,890
training examples, right? So here
I took a ten second clip and, and,

338
00:21:49,950 --> 00:21:54,520
and show you three different
three seconds windows,

339
00:21:54,521 --> 00:21:56,560
but you would take 33 seconds windows.

340
00:21:56,980 --> 00:22:00,160
Then each ten second audio cook
becomes through the examples.

341
00:22:00,190 --> 00:22:04,240
And now you've turned the problem into a
binary classification problem where you

342
00:22:04,241 --> 00:22:07,810
need to train a neural network,
the inputs, a three second clip,

343
00:22:08,170 --> 00:22:12,610
and they balls it as either zero or one.
It doesn't make sense.

344
00:22:12,670 --> 00:22:16,290
And so this is an example
of uh, uh, the, the,

345
00:22:16,291 --> 00:22:20,770
the more complex pipelines you might
have if you're building a learning

346
00:22:20,771 --> 00:22:25,771
algorithm to take a continuous Yale audio
detection problem and turn it into a

347
00:22:25,910 --> 00:22:28,740
binary classification problem,
which you've learned how to bill.

348
00:22:28,741 --> 00:22:32,050
Baer is neural networks for great.
And again, we learn about our nose,

349
00:22:32,290 --> 00:22:36,040
you learn about other ways to process
sequence data or temporal data. Okay.

350
00:22:37,690 --> 00:22:39,910
So, um, oh, go ahead.

351
00:22:45,880 --> 00:22:49,680
Oh, uh, is this Mandy
leaving? Yes, I would. Yeah.

352
00:22:49,750 --> 00:22:51,960
Actually if you have a hundred examples,
um,

353
00:22:52,030 --> 00:22:56,590
is not that hard to just listen to you
on your laptop or some audio playing

354
00:22:57,010 --> 00:22:59,230
software to figure out when,

355
00:22:59,590 --> 00:23:04,590
when they finish saying Robert turned on
and then at that moment to put a one in

356
00:23:04,900 --> 00:23:08,020
to target navel. Right. Cause this is
really when you want the lab to turn on.

357
00:23:08,770 --> 00:23:13,690
Right. Let me see. Cool. So,

358
00:23:14,290 --> 00:23:18,670
um, any other questions? Actually a few
few that ask clarifying questions. Yeah,

359
00:23:18,700 --> 00:23:19,533
go ahead.

360
00:23:24,940 --> 00:23:29,280
Oh sure. Let me get back to
that. Anything else? All right.

361
00:23:40,900 --> 00:23:44,890
Why do we do three seconds
or five seconds? This is
cause another hyper pounds.

362
00:23:44,891 --> 00:23:48,980
You can test. Oh No. Oh yeah.

363
00:23:50,090 --> 00:23:55,010
You had to say it really slowly
to take on l three seconds.

364
00:23:55,011 --> 00:23:59,250
Is this right?
Fizzy Robert Turn

365
00:23:59,970 --> 00:24:03,160
on right? Does it sound okay?
Is it is a design choice? Yeah.

366
00:24:05,130 --> 00:24:09,220
Yeah. Um, all right. So, so, um,

367
00:24:10,050 --> 00:24:14,610
let's say you do this, feed
it to supervisor in the
algorithm, train the new,

368
00:24:14,611 --> 00:24:19,410
and that's where, um, and let's
say that when you classify this,

369
00:24:19,530 --> 00:24:21,180
uh, when, when you run this algorithm,

370
00:24:21,600 --> 00:24:26,040
you ends up with a 99.5% accuracy,

371
00:24:28,420 --> 00:24:30,980
right? Um, uh,

372
00:24:31,540 --> 00:24:35,740
but you find that the
algorithm has zero detections.

373
00:24:44,120 --> 00:24:44,953
Great.

374
00:24:45,220 --> 00:24:49,570
Um, and I what that mean is that
whether the audio you give it,

375
00:24:50,050 --> 00:24:53,680
it just opens zero all the time.
So the algorithm just says, nope,

376
00:24:53,710 --> 00:24:58,290
I never heard the phrase Robert, her
and not, you know, so, so, so, um,

377
00:24:59,830 --> 00:25:04,570
so, uh, and so my question to
you is, you know, and by the way,

378
00:25:05,020 --> 00:25:09,440
the reason I'm going through these
scenarios is, um, I found that, uh,

379
00:25:09,610 --> 00:25:12,810
a good way to gain good intuitions and,

380
00:25:12,811 --> 00:25:16,020
and to become good at making these
decisions is these are the decisions that

381
00:25:16,021 --> 00:25:19,120
project leader write a tech leader,
a CEO needs to make.

382
00:25:19,121 --> 00:25:22,000
These are actually like pretty much
exactly the design she needs to make.

383
00:25:22,510 --> 00:25:26,370
And I find that I'm one of the ways
that this type of experience, if you,

384
00:25:26,510 --> 00:25:31,090
you know, find a job with a good AI team
and work with them for five years, right?

385
00:25:31,120 --> 00:25:33,580
And then you said she lived through
this and you see what they do,

386
00:25:34,030 --> 00:25:38,680
but instead of needing you to go and spend
five years to see 10 examples of this,

387
00:25:38,681 --> 00:25:43,630
I'm trying to step you through
maybe one example in one hour. So,

388
00:25:43,631 --> 00:25:46,330
so instead of, uh, you know, gaining,

389
00:25:46,331 --> 00:25:50,280
does experience through work experience,
which is great, but texts many,

390
00:25:50,290 --> 00:25:54,480
many years, many, many
months hoping to, you know,

391
00:25:54,490 --> 00:25:56,590
let's just put you in that position
and making these decisions,

392
00:25:56,591 --> 00:26:00,510
he comes there and from that
much faster. Right. Um, so,

393
00:26:02,620 --> 00:26:05,610
uh, and, and all the examples I'm giving
are actually completely realistic.

394
00:26:05,640 --> 00:26:10,360
Right there. You either exactly or
very similar to things I have seen in,

395
00:26:10,361 --> 00:26:14,250
in actual, you know, very real
project. So question is, uh,

396
00:26:14,680 --> 00:26:18,820
you're learning our room gives this
result out. 95% of assay zero detections.

397
00:26:18,910 --> 00:26:23,530
Whether you do, let me mention some of
some of the answers I really liked. Um,

398
00:26:24,070 --> 00:26:28,690
I think that, uh, um, you know, I,

399
00:26:28,720 --> 00:26:31,300
when I think of building
learning algorithms, uh,

400
00:26:31,630 --> 00:26:36,630
the process is often specify a depth set
and our test set that measure what you

401
00:26:37,001 --> 00:26:41,500
care about us and then, um,
you don't always have to do it,

402
00:26:41,530 --> 00:26:45,450
but it's good hygiene.
It, it's, it is, um, uh,

403
00:26:45,570 --> 00:26:47,790
Shopkins clarity of your thinking,
right?

404
00:26:47,820 --> 00:26:49,860
If you have a very clear
specification of the problem.

405
00:26:49,890 --> 00:26:53,130
And I think when inside all of
this is that if your desk set,

406
00:26:53,131 --> 00:26:56,630
there's really all the
whack, right? Because it's so
unbalanced. That actress here,

407
00:26:56,670 --> 00:27:01,080
Jeff set doesn't translate to what you
actually care about. Uh, because you know,

408
00:27:01,140 --> 00:27:04,590
presume it is 99.5% accurate
on the deaf set as well.

409
00:27:05,010 --> 00:27:07,950
But this performance is terrible.
So it's doing great on the dead set,

410
00:27:07,951 --> 00:27:10,320
on your accuracy. Mashaikh we're
giving terrible performance.

411
00:27:10,650 --> 00:27:15,480
So I think of it as good hygiene. You
know, this is kind of good sound practice,

412
00:27:15,840 --> 00:27:17,910
uh, to, to just specify,

413
00:27:17,911 --> 00:27:20,670
make sure you at least have a just
said and the validation metric that

414
00:27:20,671 --> 00:27:22,680
corresponds more closely
to what you care about.

415
00:27:23,100 --> 00:27:25,580
So making the depth set more balance,
uh,

416
00:27:25,740 --> 00:27:29,520
equal numbers of positive and negative
with would be a good step to have that.

417
00:27:29,850 --> 00:27:34,830
Um, uh, and then I think,
um, uh, you could also,

418
00:27:35,220 --> 00:27:37,890
uh, there are a few people
that talked about, um,

419
00:27:38,370 --> 00:27:42,920
give the higher weights to the positive
examples, right? So, you know, uh,

420
00:27:43,040 --> 00:27:48,030
uh, one way to do this is the reassemble
your training and your Jeff says to make

421
00:27:48,031 --> 00:27:49,650
them more proportionate.

422
00:27:49,770 --> 00:27:53,340
In terms of maybe closer to a balanced
ratio of positive negative examples.

423
00:27:53,341 --> 00:27:55,900
That'd be okay.
The other way to not do re sampling,

424
00:27:55,901 --> 00:27:59,280
we were just give the positive
examples a great to await, right? Um,

425
00:27:59,670 --> 00:28:04,600
I would probably be re sample. Um,
another thing you could do, uh, uh,

426
00:28:04,840 --> 00:28:08,970
uh, you know, in the
inches of, um, uh, speed,

427
00:28:09,300 --> 00:28:10,390
even if it's not the math,

428
00:28:10,391 --> 00:28:15,391
math most most sound thing to do is to
change the target labels to be a bunch of

429
00:28:15,601 --> 00:28:20,160
ones after that. Um,
uh, and this is a hack.

430
00:28:20,161 --> 00:28:21,780
This is not formally rigorous,

431
00:28:22,140 --> 00:28:25,050
but if you've implemented the
rest of this code already,

432
00:28:25,290 --> 00:28:28,230
this might be a reasonable, you
know, a little bit hacky thing to do.

433
00:28:28,231 --> 00:28:33,210
But this is this, this, this
might work well enough, right?
I, I would, I might not,

434
00:28:33,480 --> 00:28:35,850
I don't know if I would want to try to,
you know,

435
00:28:35,851 --> 00:28:39,270
write an academic research paper with
this method. Maybe I can get away with it.

436
00:28:39,271 --> 00:28:42,030
But this little thing that I think if
you tried to publish the paper with this,

437
00:28:42,690 --> 00:28:46,290
academic reviewers might raise their
eyebrows and say, maybe, you know,

438
00:28:46,320 --> 00:28:49,740
maybe it is okay, but I think of
your one something quick and dirty.

439
00:28:49,740 --> 00:28:51,810
They just works. I think, uh, uh,

440
00:28:51,940 --> 00:28:56,220
they bring the ones changing a
bunch of labels to be one. So that,

441
00:28:56,280 --> 00:29:00,080
say a clip here, right? Uh,

442
00:29:00,570 --> 00:29:04,240
that ends just a little bit after
Robert turned on is still labor one.

443
00:29:04,260 --> 00:29:07,740
There'll be pretty reasonable, but
it still just be saying that, um, uh,

444
00:29:08,490 --> 00:29:12,980
for anywhere within maybe a
0.5 second period off there,

445
00:29:12,981 --> 00:29:14,190
Robert turned on finish.

446
00:29:14,191 --> 00:29:17,520
It's okay to turn on the light
anytime within that period.

447
00:29:17,521 --> 00:29:21,810
Then you kind of want to be turning on
the light, turning on the lamp, you know,

448
00:29:21,811 --> 00:29:26,430
say within half a second right off there.
Robert turned on this has been said,

449
00:29:27,810 --> 00:29:29,130
and this would be a not,

450
00:29:29,320 --> 00:29:33,420
does it be a way to just get
more labels of ones in there?

451
00:29:34,260 --> 00:29:35,850
Great.
Let me say this.

452
00:29:41,370 --> 00:29:42,203
Uh,

453
00:29:42,210 --> 00:29:44,980
how does that translate to like,
when you deploy this,

454
00:29:44,981 --> 00:29:48,040
you're not going to see
Robert Turn Os much, right?

455
00:29:48,430 --> 00:29:51,920
Like one out of 1000 might be
reflective of what you expect to see.

456
00:29:52,900 --> 00:29:57,010
Yeah, this is good. Yeah. Right.
So, um, I think that, uh, put it,

457
00:29:57,720 --> 00:30:01,750
um, so if you actually,
yes, so what I did,

458
00:30:01,810 --> 00:30:06,630
this is how bad deaf set and
evaluation the I show kind
of question, right? So, uh,

459
00:30:06,640 --> 00:30:10,360
one of the couple of the metrics that
people often use a when actually working

460
00:30:10,361 --> 00:30:15,250
on this is when someone says Robert
turned on whether it's the chance that she

461
00:30:15,251 --> 00:30:17,170
wakes up or the turns on.

462
00:30:17,290 --> 00:30:21,370
And then the second is if no one is
saying anything to the lamp, you know,

463
00:30:21,371 --> 00:30:25,480
how often does it randomly turn on by
itself without you having said anything?

464
00:30:25,481 --> 00:30:29,420
So those are the two metrics
people actually use. And, and, uh,

465
00:30:29,440 --> 00:30:32,860
sometimes you could also try the combined
in a single now be evaluation metrical

466
00:30:32,861 --> 00:30:36,250
something. Uh, uh, but
I think that's, um, uh,

467
00:30:36,251 --> 00:30:39,010
you could tend to find a data set
to measure both of these things.

468
00:30:39,010 --> 00:30:41,680
And then and then and then
hopefully finding a way to
combine them into a single

469
00:30:41,681 --> 00:30:45,640
rule number, which I think, yeah, I think
one of the ways we talked about in the,

470
00:30:45,641 --> 00:30:49,210
in the videos as well. Great. So it
makes sense. Oh yeah. But I think,

471
00:30:49,211 --> 00:30:53,610
I think so question is really,
um, uh, what is it? The size,

472
00:30:53,611 --> 00:30:56,640
size is using the right.
Yeah. And Oh and just one,

473
00:30:56,641 --> 00:31:01,641
one thing about the straight forward way
of rebalancing is that if you don't do

474
00:31:01,781 --> 00:31:06,640
this then your whole data set just
as very few positive examples. Right.

475
00:31:06,900 --> 00:31:07,570
Um,

476
00:31:07,570 --> 00:31:12,550
and so if you throw away all the negative
examples so that you cut down the

477
00:31:12,551 --> 00:31:15,850
number of negative examples until you
have exactly equal numbers was a positive

478
00:31:15,851 --> 00:31:19,270
and negatives, you've actually thrown
away a lot of negative examples.

479
00:31:19,390 --> 00:31:21,190
Does it make sense? And so one, one,

480
00:31:21,191 --> 00:31:24,970
one problem with the straight forward
way of rebalancing is that, you know,

481
00:31:25,650 --> 00:31:29,350
and your audio clip and your test ten
second clip they were collected by running

482
00:31:29,351 --> 00:31:34,120
around Stanford.
You have one example of Robert turned on.

483
00:31:35,020 --> 00:31:39,220
And so if you want exactly
PR perfectly balance,

484
00:31:39,370 --> 00:31:43,260
positive and negative, it means that
you're allowed to own the clip out.

485
00:31:43,261 --> 00:31:45,190
One negative example,
all of this,

486
00:31:45,191 --> 00:31:49,810
you can say that's a negative and that's
a positive and Yukon clip out more

487
00:31:49,811 --> 00:31:52,600
negative examples from
this, right? So, so,

488
00:31:52,601 --> 00:31:57,520
so of you use a he of the perfect
rebalance Yara is throwing away a lot of

489
00:31:57,521 --> 00:32:01,960
negative examples that that could be
helpful for them running off. Great.

490
00:32:03,310 --> 00:32:06,430
Um,
so

491
00:32:09,070 --> 00:32:13,050
all right,
so,

492
00:32:14,000 --> 00:32:15,470
um,

493
00:32:16,200 --> 00:32:20,840
you know, a lot of the workflow of
building learning algorithms is um,

494
00:32:21,390 --> 00:32:24,930
uh, building learning Auburn's
feels more light did buggy, right?

495
00:32:24,931 --> 00:32:28,380
Because what happens in a typical
machine or in your work though is you

496
00:32:28,381 --> 00:32:30,030
implement something and it doesn't work.

497
00:32:30,031 --> 00:32:33,690
We'll see you figure out whether is
a problem. So he fixed that. Ah, AH,

498
00:32:33,750 --> 00:32:34,770
like rebalancing.

499
00:32:34,790 --> 00:32:38,730
Are we waiting or adding more once
and so that fixes the current problem.

500
00:32:39,020 --> 00:32:42,960
And then after fixing the current problem,
which, which is the one we just solve,

501
00:32:42,961 --> 00:32:46,850
say you then come across a new problem
and you have to solve that and you to fix

502
00:32:46,851 --> 00:32:49,770
that problem somewhere else. And now
the new problems, I find that, uh,

503
00:32:50,150 --> 00:32:53,300
the workflow of, um, when I'm working
in a machine learning project,

504
00:32:53,660 --> 00:32:58,160
it often feels more like
software debugging, then
software development, right?

505
00:32:58,161 --> 00:33:01,190
Because you're often trying to figure
out what doesn't work and then trying to

506
00:33:01,191 --> 00:33:03,080
fix that.
And after you fix that problem,

507
00:33:03,350 --> 00:33:06,560
then another bug surfaces and the squash
that and you do that and then another,

508
00:33:06,600 --> 00:33:08,610
and you kind of keep doing
that until the offer of work.

509
00:33:08,660 --> 00:33:13,130
So if I keep talking about, you know, your
algorithm doesn't work, what do you do,

510
00:33:13,131 --> 00:33:16,970
nick? Right? That's kind of the theme
of today's presentation, but that,

511
00:33:16,971 --> 00:33:18,890
that is what the workflow,

512
00:33:18,920 --> 00:33:22,100
that is what your day to day work
of developing a learning algorithm.

513
00:33:22,280 --> 00:33:24,860
It's usually like, because it's
like it doesn't work and you fix it,

514
00:33:24,890 --> 00:33:27,560
it still doesn't work. You fix that
and he still doesn't work. You fix it.

515
00:33:27,830 --> 00:33:30,150
And He'd do that enough times
until it works, right? That, that,

516
00:33:30,151 --> 00:33:33,270
that is actually what often working
on the learning algorithm works look,

517
00:33:33,350 --> 00:33:35,200
looks like.
Um,

518
00:33:37,570 --> 00:33:42,030
all right. So let's say
you fix that problem, um,

519
00:33:42,650 --> 00:33:47,360
and you conclude, uh, through
doing error analysis that you're,

520
00:33:47,361 --> 00:33:51,980
our room is over fitting, right? So,

521
00:33:52,240 --> 00:33:54,230
you know,
you've added a lot more ones,

522
00:33:54,231 --> 00:33:56,300
so the Dataset is a
little bit more balanced.

523
00:33:56,301 --> 00:33:59,600
So let's just add a bunch of ones
like I did on that previous board.

524
00:33:59,900 --> 00:34:01,910
Let's just add a lot of ones here.

525
00:34:02,180 --> 00:34:05,840
So the INSEAD isn't as unbalanced and,
um,

526
00:34:09,660 --> 00:34:10,493
let's see.

527
00:34:18,450 --> 00:34:19,283
MMM,

528
00:34:23,740 --> 00:34:25,660
right. Okay, good. Um,

529
00:34:27,310 --> 00:34:28,900
let's say that,

530
00:34:31,520 --> 00:34:32,353
sorry,

531
00:34:37,420 --> 00:34:38,800
two pages of notes here.

532
00:34:41,200 --> 00:34:45,340
Okay, good. So let's say that, um,
you find that are the cheese now,

533
00:34:45,700 --> 00:34:50,700
98% accuracy on training and
50% accuracy on the Dev side,

534
00:34:53,020 --> 00:34:57,670
right? So very lush gap
boutine your training and your,

535
00:34:57,890 --> 00:35:01,690
um, deaf set performance. And
so a clear sign of over fitting.

536
00:35:01,720 --> 00:35:03,580
And so I think one of
the earlier questions,

537
00:35:03,581 --> 00:35:05,980
someone talked about data augmentation.
Uh,

538
00:35:05,981 --> 00:35:09,030
and so we have this clear
sign of overfitting, um,

539
00:35:09,130 --> 00:35:13,810
this is a good time to consider
data augmentation, right then.

540
00:35:13,980 --> 00:35:17,590
So let's say you go ahead and do
data augmentation. So for audio,

541
00:35:17,610 --> 00:35:20,450
this is how you could do data
augmentation, which is, um,

542
00:35:20,830 --> 00:35:22,660
collect a bunch of background,
all audio.

543
00:35:24,660 --> 00:35:28,800
So I guess if you're trying to build a
lab that might go into people's homes,

544
00:35:29,100 --> 00:35:31,820
then you could go into your
friend's homes and, uh, you know,

545
00:35:32,160 --> 00:35:35,580
with their permission record,
right, what the background sounded,

546
00:35:35,620 --> 00:35:38,280
their home looks like.
You maybe people talking to background,

547
00:35:38,281 --> 00:35:42,000
maybe the TV on in the
background, whatever it goes
on people in people's homes.

548
00:35:42,360 --> 00:35:46,480
Um, and then it turns out
that if you take a, um,

549
00:35:46,740 --> 00:35:48,210
say a one second clip

550
00:35:49,940 --> 00:35:54,940
of Robin turned on an Rto and
you add that to background clip,

551
00:35:57,240 --> 00:36:01,170
then you can synthesize an audio clip
of what it sounds like in your friend's

552
00:36:01,171 --> 00:36:04,140
house. If someone were to
suddenly pop up and say,

553
00:36:04,141 --> 00:36:09,030
Robert turned on against the background
sound of your friend's house. Right? Um,

554
00:36:09,330 --> 00:36:13,400
and, and it turns out that, um, uh, uh,

555
00:36:13,800 --> 00:36:17,670
if you want to make the system robust,
so actually phrase apple, they have a,

556
00:36:17,970 --> 00:36:18,803
Oh no,

557
00:36:19,410 --> 00:36:23,040
I actually know someone that lives
unfortunately closely to a train station.

558
00:36:23,040 --> 00:36:27,190
And so their holes as she has a lot of
translation noise from the Caltrain. Uh,

559
00:36:27,191 --> 00:36:32,191
and so when you can do to make your system
more robust is also a take a clip of

560
00:36:33,061 --> 00:36:36,210
safe train noise, right?
Like cow train noise.

561
00:36:36,690 --> 00:36:40,950
And if you take that noise and take, uh,
in, in this case, let's say one second,

562
00:36:40,980 --> 00:36:44,280
one second or three second clip of
someone saying Robert's her and on,

563
00:36:44,580 --> 00:36:47,970
and you synthesize that on top
of the train in the background.

564
00:36:48,330 --> 00:36:52,320
Then once you end up with is a
ten second clip of someone saying,

565
00:36:52,350 --> 00:36:55,210
Robert turned on against the noisy,
you know,

566
00:36:55,290 --> 00:36:58,130
trained in the background
type of debit noise, right?

567
00:36:59,040 --> 00:37:03,680
And so in order to do data
augmentation or data synthesis,

568
00:37:03,990 --> 00:37:07,800
you can take some one second clips that
people saying Robert turned on and the

569
00:37:07,801 --> 00:37:10,690
quiet background.
And then take some one second clip.

570
00:37:10,710 --> 00:37:15,440
But people saying random words, right?
And let's say, you know, Cardinal, right,

571
00:37:15,690 --> 00:37:19,770
since the staffing and synthesize this
against the train noise background.

572
00:37:20,070 --> 00:37:21,900
And then you would have,
in this case,

573
00:37:21,901 --> 00:37:25,500
you would have what sounds like crazy
noise train, train on street noise.

574
00:37:25,650 --> 00:37:30,500
Robert turned [inaudible],
right. And then, uh,

575
00:37:30,580 --> 00:37:34,440
you could generate the
labels now as zeros there,

576
00:37:34,920 --> 00:37:38,570
one's there, and then Zeros there. Right?

577
00:37:38,730 --> 00:37:41,790
Because if this is what it actually
sounded like in a, in a user's home,

578
00:37:41,791 --> 00:37:46,200
then you want the lab to turn on off.
Robert turned on.

579
00:37:46,201 --> 00:37:50,340
But not after these random words, he
can take different random words. Great.

580
00:37:52,170 --> 00:37:56,790
Um, so let's see.

581
00:37:59,100 --> 00:38:01,980
Great.
So,

582
00:38:09,260 --> 00:38:10,093
um,

583
00:38:11,040 --> 00:38:15,540
what I'd like you to do is evaluate,
um,

584
00:38:16,110 --> 00:38:16,890
uh,

585
00:38:16,890 --> 00:38:21,890
three different possible
ways to collect noisy data,

586
00:38:23,160 --> 00:38:27,720
right. Uh, to, to, to collect this
type of background data. Right?

587
00:38:28,230 --> 00:38:31,650
Um, and so, um,

588
00:38:31,830 --> 00:38:35,410
when I like they do for the next
question is let's say you and your team,

589
00:38:35,740 --> 00:38:40,060
you know, have, uh, uh,
uh, brainstormed, um, uh,

590
00:38:40,510 --> 00:38:43,090
uh, brainstormed a few different ways, uh,

591
00:38:43,190 --> 00:38:47,300
to collect this type of
background noise data. Um,

592
00:38:47,590 --> 00:38:52,590
and let's say you've decided that
you would like to collect 10 hours of

593
00:38:52,901 --> 00:38:56,080
background noise data, right? Okay.

594
00:38:56,710 --> 00:39:00,790
So I'm going to present to
you three options. One is, um,

595
00:39:05,840 --> 00:39:06,560
you know,

596
00:39:06,560 --> 00:39:10,460
running around Stanford and
taste microphones around
Stanford or your friend's

597
00:39:10,461 --> 00:39:14,360
house. Do this with consent and don't,
don't, you know, California or shoes,

598
00:39:14,420 --> 00:39:17,180
you're not supposed to. I don't recall
people about their knowledge and consent.

599
00:39:17,860 --> 00:39:20,500
Uh,
second is a

600
00:39:23,880 --> 00:39:26,220
downloads clips online.

601
00:39:28,720 --> 00:39:30,540
Uh, it, it turns out
you can go to youtube.

602
00:39:30,541 --> 00:39:35,340
There are these like 10 hour
long clips of, you know,

603
00:39:35,760 --> 00:39:40,350
rain noise or cars driving around.
And so you actually,

604
00:39:40,800 --> 00:39:42,260
uh,
and again if you do that,

605
00:39:42,270 --> 00:39:45,390
find something that's creative comments
and sort of appropriate with the license.

606
00:39:45,930 --> 00:39:50,430
Right? Um, another thing you
could do is use a mechanical Turk,

607
00:39:57,190 --> 00:39:58,480
Amazon Mechanical Turk.

608
00:40:01,040 --> 00:40:04,850
We can have people over or around
the world, um, be paid, you know,

609
00:40:04,910 --> 00:40:08,420
modest amounts of money to
submit audio clips. Right?

610
00:40:08,840 --> 00:40:12,070
So for the next exercise, what
I want you to do, because I'm,

611
00:40:12,080 --> 00:40:14,930
and I want you that this exercise
of, of, of this discipline,

612
00:40:14,960 --> 00:40:18,830
which is why the why should they
do is, um, I went to the estimate,

613
00:40:19,160 --> 00:40:22,910
let's see what time is it now?
Okay.

614
00:40:23,060 --> 00:40:28,060
It's 12:30 PM very now whether wants
you to do is write down three numbers in

615
00:40:28,581 --> 00:40:33,260
the next exercise to estimate.
If you went to do this,

616
00:40:34,220 --> 00:40:39,080
you know, let's say you were to go do
this right now, right? By what time,

617
00:40:39,081 --> 00:40:43,490
where you have finished. If you were to
do option one, what time would you finish?

618
00:40:43,491 --> 00:40:45,980
And we had to do option two.
What time would you finish?

619
00:40:45,981 --> 00:40:50,540
You were to do option three if you go is
a collect 10 hours of data through one

620
00:40:50,541 --> 00:40:54,140
of these mechanisms. Does that make
sense? So it's tough though. The PM now.

621
00:40:54,620 --> 00:40:57,530
So one of the lecture they do is
just write down three numbers.

622
00:40:58,850 --> 00:41:02,390
First number is what time is it,
what time would it be?

623
00:41:03,410 --> 00:41:07,910
By the time you collected 10 hours of
data, you know, from around Stanford,

624
00:41:07,940 --> 00:41:11,800
what time would it be? Right?
And if you could do this in a so,

625
00:41:11,820 --> 00:41:14,030
so if you think you
would do it by tonight,

626
00:41:14,031 --> 00:41:15,980
then write 9:00 PM if you think it'll do,

627
00:41:15,981 --> 00:41:17,300
if you think it will
take you one the week,

628
00:41:17,660 --> 00:41:20,660
then write the date one week from
now, right? Whatever it is. Uh,

629
00:41:21,080 --> 00:41:24,080
but just write down three
numbers. Obesity activity. Okay,

630
00:41:24,400 --> 00:41:28,140
let's do this one relatively quickly.
You can people do this and like, uh,

631
00:41:28,220 --> 00:41:32,510
maybe a minute and a half. All
right, cool. This is interesting.

632
00:41:33,250 --> 00:41:34,083
Um,

633
00:41:40,090 --> 00:41:43,670
what did people think? Actually this
is a surprising large variability.

634
00:41:44,000 --> 00:41:48,220
I'll mention one thing that surprised me.
Um,

635
00:41:48,650 --> 00:41:51,300
I'll give you my own
assessment. I think that, uh,

636
00:41:53,570 --> 00:41:57,200
you know, w when I'm leading startup
teams, we tend to be very scrappy, right?

637
00:41:57,230 --> 00:42:00,890
And so I think that, um, if it
goes to collect 10 hours of data,

638
00:42:01,250 --> 00:42:03,170
if you have three friends who have laptop,

639
00:42:03,290 --> 00:42:06,860
you can collect three hours of data per
hour cause he goes see recording's going

640
00:42:06,861 --> 00:42:10,880
in parallel. So if I were doing this
with say two other friends, you know,

641
00:42:11,240 --> 00:42:14,400
I bet I bet we could get
this done by tonight, right?

642
00:42:15,160 --> 00:42:18,710
Cause if you need nine hours of data
that each person needs to collect three

643
00:42:18,711 --> 00:42:22,370
hours of data and you run around Stanford,
the Keith and microphones running,

644
00:42:22,640 --> 00:42:26,870
I bet. I Bet I could get this done by
6:00 PM. Right? Maybe, maybe even earlier.

645
00:42:26,900 --> 00:42:31,070
I don't know. Download clips online, uh,

646
00:42:31,670 --> 00:42:33,710
is actually earlier on.
It's actually an interesting one.

647
00:42:33,770 --> 00:42:35,180
Maybe be about the same time.

648
00:42:35,870 --> 00:42:40,590
It turns out one tricky thing about
downloading clips online is that um,

649
00:42:41,210 --> 00:42:43,580
uh, I think a lot of the, you know, there,

650
00:42:43,590 --> 00:42:47,450
there are people that have
trouble sleeping at night
so they listen to highway

651
00:42:47,451 --> 00:42:50,510
noise or whatever. And so
there are these, you know,

652
00:42:50,511 --> 00:42:54,740
20 hours of highway clips,
highway noise on youtube you can find,

653
00:42:55,160 --> 00:42:58,190
but I don't know how
those clips are generated.

654
00:42:58,460 --> 00:43:02,720
And I suspect a lot of them loop right
meaning is the same one hour play over

655
00:43:02,721 --> 00:43:05,720
and over.
So actually think it's harder than than,

656
00:43:05,721 --> 00:43:10,721
than one might guess they get
10 hours of non repetitive data.

657
00:43:11,120 --> 00:43:12,470
And as one of those things,
you know,

658
00:43:12,620 --> 00:43:17,090
if I take an hour of how high sound and
Loopit you can't tell the difference

659
00:43:17,091 --> 00:43:20,600
cause all high res sounds sounds of
saying I just can't tell one minute of

660
00:43:20,601 --> 00:43:21,890
highway sound from another one.

661
00:43:21,890 --> 00:43:26,840
But if you have one hour of highway
sound loop 10 times the learning our room

662
00:43:26,841 --> 00:43:31,010
where she performed much less well than
if you have 10 hours of fresh highway

663
00:43:31,011 --> 00:43:34,700
sounds. So this I will actually have a
harder time doing I think our priority.

664
00:43:35,120 --> 00:43:38,990
I would probably, if I were doing
this I, because of these problems,

665
00:43:38,991 --> 00:43:43,820
I were carried by June until
sometime tomorrow. Right. Maybe,

666
00:43:43,821 --> 00:43:47,630
maybe 9:00 PM or something. Maybe
that's doable. I'm not sure. I'm,

667
00:43:47,810 --> 00:43:51,580
the one surprise to me was some people
thought they could do this by tonight. I,

668
00:43:51,590 --> 00:43:52,040
again,

669
00:43:52,040 --> 00:43:55,910
I've used Amazon Mechanical Turk is
actually a huge process to set up Amazon

670
00:43:55,911 --> 00:44:00,570
where Carl's her get people on board and
especially they get them microphone. Uh,

671
00:44:00,830 --> 00:44:03,500
I don't have your implants something
on flash so we can speak in their web

672
00:44:03,501 --> 00:44:06,920
browser or an and be supportive is,
it's actually,

673
00:44:07,580 --> 00:44:11,330
it's actually not that easy to get a
lot of the Turkers to do this and the

674
00:44:11,630 --> 00:44:15,830
global supply of [inaudible]
salsa unlimited. So I
would, if I were doing this,

675
00:44:15,831 --> 00:44:19,800
I would probably, I know maybe
a week or something, right.

676
00:44:19,820 --> 00:44:21,810
How to say natural.
Um,

677
00:44:22,220 --> 00:44:25,400
but so does specific opinion
isn't that important?

678
00:44:25,430 --> 00:44:29,560
But I want you to go through this
exercise because this is how, um,

679
00:44:29,810 --> 00:44:34,290
efficient startup teams either brainstorm
a list of things and then you all

680
00:44:34,291 --> 00:44:38,460
figure out how long you think it'll take
to do these things. And I think, uh,

681
00:44:38,880 --> 00:44:41,460
you can have a debate about how
high quality the day duress,

682
00:44:41,490 --> 00:44:45,860
I think you can get very high quality
data from this. And from this, uh, I,

683
00:44:46,330 --> 00:44:49,140
I just didn't cuss of those online,
all your sources.

684
00:44:49,840 --> 00:44:53,130
But if this is really fast and you
can get pretty high quality data,

685
00:44:53,131 --> 00:44:57,060
I will probably do this, a collective
backgrounds how to get going. Great.

686
00:44:57,180 --> 00:45:00,630
But I think that part of their
work, so I see of, you know,

687
00:45:00,870 --> 00:45:05,760
fast moving teams is pretty much exactly
what she did, which is why, why is that,

688
00:45:05,761 --> 00:45:09,990
that exercise of brainstorming the list
of options and then really estimating,

689
00:45:10,020 --> 00:45:14,240
oh, what time can we get this done and
then use that to pick an option. Right.

690
00:45:14,850 --> 00:45:19,190
Um, and then I want to just
mention one last thing, um,

691
00:45:20,020 --> 00:45:20,950
which is that

692
00:45:24,000 --> 00:45:28,040
these differences matter, right? Um,

693
00:45:28,940 --> 00:45:29,850
you know,
I've actually put,

694
00:45:30,340 --> 00:45:34,190
I put the last piece of simple all of
machine learning systems, but, um, oh and,

695
00:45:34,191 --> 00:45:35,060
and I think by the way,

696
00:45:35,270 --> 00:45:39,440
if you do everything we just described
and you see this later in a problem set,

697
00:45:39,500 --> 00:45:42,200
uh, you can actually,
with this set of ideas,

698
00:45:42,201 --> 00:45:44,450
pretty much the set of ideas
that we just went through today.

699
00:45:44,750 --> 00:45:47,570
You can actually print the bill, the
bill, a pretty decent trigger words.

700
00:45:47,571 --> 00:45:49,940
It's action system.
I'll wake word detection system.

701
00:45:49,941 --> 00:45:53,570
And in fact we'll ask you to do pretty
much this and the later homework exercise.

702
00:45:53,571 --> 00:45:57,710
But now you know, when you get to that
whore exercise, when you do RNN stuff,

703
00:45:58,010 --> 00:46:01,070
you know how you could come out with this.
So the process yourself,

704
00:46:01,310 --> 00:46:04,610
if you didn't already know how
to make these types of choices.

705
00:46:11,150 --> 00:46:15,850
Oh my, my, my results at the beginning,

706
00:46:15,851 --> 00:46:18,090
I could see like it's not too frozen.

707
00:46:18,270 --> 00:46:22,800
Like my micro lights is the
same as the one that's used. Oh,

708
00:46:22,820 --> 00:46:27,420
when I run the run,
let's mess that up.

709
00:46:27,960 --> 00:46:30,670
That's what we're asked to think about it.

710
00:46:31,220 --> 00:46:34,760
Yeah. So my advice. So what
does your microphone affect
your results? Right? Yeah.

711
00:46:34,761 --> 00:46:36,440
So my advice is put pizza,

712
00:46:37,060 --> 00:46:41,780
get something going quick and
dirty and then develop a depth set,

713
00:46:42,170 --> 00:46:44,220
right with the actual types
of days that you think.

714
00:46:44,221 --> 00:46:47,360
If you get on your role microphone
and then see if it is a problem.

715
00:46:47,780 --> 00:46:50,720
And it may be different microphones,
do you have different characteristics?

716
00:46:50,721 --> 00:46:54,500
And if it is a problem, then go back
and think about how you'd collect data.

717
00:46:54,501 --> 00:46:57,000
There's more representative
of how you test.

718
00:46:57,560 --> 00:46:59,960
I want to mention one more quick thing.
Do I had a heart surveys.

719
00:46:59,961 --> 00:47:01,880
I want us to do something
real quick, which is, um,

720
00:47:02,270 --> 00:47:04,920
I want to tell you why these
things really matter, which is, um,

721
00:47:05,570 --> 00:47:08,810
if this is a performance, right? Oh, oh,

722
00:47:08,840 --> 00:47:12,770
let's say Ashley era and this is the time,

723
00:47:13,690 --> 00:47:18,350
right? And this is today and you had to
see all this thoughts. I remember that.

724
00:47:18,360 --> 00:47:19,880
That's what we're doing.
Just lesson.

725
00:47:20,330 --> 00:47:24,780
And this is six months from now and
this is 12 months from now. Right?

726
00:47:25,700 --> 00:47:29,440
Um, you know, maybe have a
competitor actually, maybe, maybe,

727
00:47:29,980 --> 00:47:33,790
I don't know. Maybe it's because we
talked about this so much in this class.

728
00:47:34,120 --> 00:47:36,460
Maybe two of you in this classical,
I thought the sauce,

729
00:47:36,550 --> 00:47:41,230
but it's certainly a competitor.
But over time,

730
00:47:41,410 --> 00:47:43,150
most of the machine learning teams,

731
00:47:45,580 --> 00:47:48,460
the Arrow actually goes down over time
as you work on problems right there.

732
00:47:48,480 --> 00:47:51,460
That means this is what I see in
tons of practical projects. You know,

733
00:47:51,461 --> 00:47:52,330
you work on the project,

734
00:47:52,870 --> 00:47:55,750
improve the system and the era
as she goes down over time,

735
00:47:55,751 --> 00:47:57,600
as you work on this over the next,
next

736
00:47:57,600 --> 00:48:00,420
12 months, say right, if you're
really see over started doing this.

737
00:48:00,930 --> 00:48:04,950
And it turns out that it's a startups
after discipline to constantly be the most

738
00:48:04,951 --> 00:48:08,640
efficient. Um, don't do something
that takes you two days.

739
00:48:08,641 --> 00:48:10,530
If you can get a similar
result in one day,

740
00:48:10,980 --> 00:48:13,140
the difference is not
that your one day slower,

741
00:48:13,410 --> 00:48:16,180
the differences that your two
x faster, right? And then,

742
00:48:16,181 --> 00:48:20,310
then having that mindset it when take
this whole chart and compress it on the

743
00:48:20,311 --> 00:48:23,850
horizontal axis. Um, then you

744
00:48:26,220 --> 00:48:28,980
want to be the startup
that makes the same amount,

745
00:48:28,981 --> 00:48:31,130
the pros in six months
instead of 12 months. Right?

746
00:48:31,860 --> 00:48:34,170
Because if you're able to do this,

747
00:48:34,171 --> 00:48:38,180
then your startup will actually perform
much better in the marketplace. Uh,

748
00:48:38,190 --> 00:48:41,460
assuming, you know, accuracy's important,
which it seems to be for wake word.

749
00:48:41,850 --> 00:48:44,910
And so don't think of this as
saving you a day here and there.

750
00:48:44,940 --> 00:48:47,130
Think of this as making
your teeth twice as fast.

751
00:48:47,430 --> 00:48:50,510
And that's the difference between this
level, the form. Isn't that lovely?

752
00:48:50,970 --> 00:48:54,990
So that's why when I'm building
teams that execute these projects,

753
00:48:54,991 --> 00:48:59,310
I tend to be pretty obsessive about
making sure we're very efficient in

754
00:48:59,311 --> 00:49:03,690
exploring the options and don't wait
until tomorrow to collect data of dubious

755
00:49:03,691 --> 00:49:06,990
quality when you have a better
idea of collecting data by today.

756
00:49:07,060 --> 00:49:10,410
Because the difference is not that you
wasted 12 hours a different as you had

757
00:49:10,411 --> 00:49:13,470
twice as slow as a company.
Right? So I think, uh,

758
00:49:13,800 --> 00:49:17,750
so hopefully through this example on
your ongoing experiences throughout this

759
00:49:17,750 --> 00:49:22,160
quarter can help you continue to
get better at this. Right. Um,

760
00:49:22,320 --> 00:49:25,760
last thing we wanted to do was, uh, we're
about halfway through the course. Oh,

761
00:49:25,761 --> 00:49:26,594
go ahead.
Um,

762
00:49:26,820 --> 00:49:31,820
we want to handle the survey and anonymous
survey to get some feedback from you

763
00:49:32,341 --> 00:49:37,340
about this costs. And whenever we get
these surveys, uh, we ended up, uh,

764
00:49:37,430 --> 00:49:39,990
uh, thanks to the previous
generations is his feedback.

765
00:49:39,991 --> 00:49:41,940
We've already been gradually
we can cause better.

766
00:49:42,210 --> 00:49:45,480
So I think Ken and I actually
read all of these questions,

767
00:49:45,481 --> 00:49:49,730
I'll cells and try to find ways to
take your feedback to improve the cost.

768
00:49:49,731 --> 00:49:52,460
So I can take, you know, five minutes, uh,

769
00:49:53,250 --> 00:49:56,950
felt the survey and you can hand it in
and just drop it off and on them see up

770
00:49:56,951 --> 00:50:00,990
here in front of me. Very grateful
for your suggestions. Okay.

771
00:50:01,380 --> 00:50:02,490
So,
um,

772
00:50:05,670 --> 00:50:09,550
I think if you haven't
entered your id yet, uh,

773
00:50:09,730 --> 00:50:12,070
you could still do so. But,
uh, that's it for today.

774
00:50:12,120 --> 00:50:16,200
So please start the survey
in the anonymous. You just
drop that back in front,

775
00:50:16,201 --> 00:50:17,490
then we'll wrap up. Okay. Thank you.

