1
00:00:06,030 --> 00:00:09,000
All right here. He [inaudible].
Okay. I guess what life, uh,

2
00:00:10,730 --> 00:00:14,310
as as Arthur was saying, please
enter your son. It's ID. Uh,

3
00:00:14,530 --> 00:00:17,800
we can bring this up again
at the end of class today.

4
00:00:18,610 --> 00:00:21,250
We're just take another like what,
20 seconds and then we'll,

5
00:00:21,310 --> 00:00:23,110
we'll go into the main discussion.

6
00:00:30,520 --> 00:00:33,450
All
right.

7
00:00:34,860 --> 00:00:39,780
So, um, what I want to
discuss with you today is,

8
00:00:39,830 --> 00:00:44,640
um, uh, may, why were they called full
cycle deep learning applications, right?

9
00:00:44,870 --> 00:00:45,703
Um,

10
00:00:53,600 --> 00:00:57,190
and so, um, I think this Sunday, uh,

11
00:00:57,200 --> 00:01:02,200
you'll be submitting your proposals for
the class projects you do this quarter.

12
00:01:02,660 --> 00:01:06,160
And, um, in most of the, uh, in,

13
00:01:06,161 --> 00:01:08,410
in a lot of the West you learn
about the machine learning projects.

14
00:01:08,430 --> 00:01:10,930
You learn how to build
machine learning models. Um,

15
00:01:11,180 --> 00:01:16,180
one of the one to do today is share with
you the bigger context of how a machine

16
00:01:16,371 --> 00:01:19,880
learning model, how a
neural network by train, uh,

17
00:01:20,120 --> 00:01:24,710
fits in the context of a bigger project.
Uh, so what are all the steps, right?

18
00:01:24,740 --> 00:01:27,580
Just as if you're writing
a software product. Yeah.

19
00:01:27,590 --> 00:01:29,630
You take other classes and learn you,
oh,

20
00:01:29,800 --> 00:01:33,170
what happened there that teach you
how to build a website, for example?

21
00:01:34,670 --> 00:01:36,470
What does that,
um,

22
00:01:37,160 --> 00:01:41,780
but to build a product requires more
than just building a website. Right?

23
00:01:41,781 --> 00:01:42,501
So what are the,

24
00:01:42,501 --> 00:01:45,680
what are the other things you need to
do to actually do a successful software

25
00:01:45,681 --> 00:01:50,660
project? And in this case
to do a successful machine
learning application? Um,

26
00:01:51,320 --> 00:01:55,100
and so, uh, let's see.

27
00:01:55,580 --> 00:01:59,570
So to Oh, Yep. Oh, test,

28
00:01:59,571 --> 00:02:02,750
test is the audio on test.
Could you turn off the audio?

29
00:02:04,100 --> 00:02:09,080
How's this? Yup. Can't hear me. Hello.
Hi. Oh, I think I'm broadcasting.

30
00:02:09,620 --> 00:02:14,270
I hear myself great.
Okay. You can hear me now.

31
00:02:14,810 --> 00:02:18,080
Great. Thank you. Alright,
thank you. Alright, so one,

32
00:02:18,081 --> 00:02:20,990
the one that did his share of you,
um, full cycle machine learning,

33
00:02:21,020 --> 00:02:25,580
not just how to, uh, you learn a lot
about how the built deep learning models,

34
00:02:25,581 --> 00:02:28,400
but how does that fit in a bigger project,
right?

35
00:02:28,401 --> 00:02:31,460
Just as if you're taking the clause
on building a website, then great.

36
00:02:31,461 --> 00:02:33,560
You didn't know how the code,
they have a website that really valuable,

37
00:02:33,860 --> 00:02:37,240
but whether all the things you need to
do to make us successful website to build

38
00:02:37,241 --> 00:02:37,370
their,

39
00:02:37,370 --> 00:02:40,400
build a project that involves launching
a website or a mobile app or whatever.

40
00:02:41,120 --> 00:02:43,320
Um,
so as,

41
00:02:43,330 --> 00:02:48,330
as you plan for your class
project proposals due to Sunday,

42
00:02:48,900 --> 00:02:53,450
uh, if you're doing an application project
that fits in the context of a bigger

43
00:02:53,451 --> 00:02:58,340
application also keeps on with
these steps in mind. Right? So, um,

44
00:02:59,070 --> 00:03:01,240
you know,
these are what I think of as the steps

45
00:03:03,370 --> 00:03:07,120
of an ml project or really
maybe maybe not fast project,

46
00:03:07,121 --> 00:03:11,890
but maybe are serious machine
learning application. Right?

47
00:03:11,920 --> 00:03:12,760
And I think,

48
00:03:12,790 --> 00:03:16,450
I'll know I've built a lot of machine
learning products over several years.

49
00:03:16,451 --> 00:03:19,480
So some of these are also things that
I wish I had known like, you know,

50
00:03:19,481 --> 00:03:24,070
many years ago. Um, one,

51
00:03:25,510 --> 00:03:28,870
this was kind of maybe kind of obvious,
but you know,

52
00:03:29,780 --> 00:03:30,640
a problem.

53
00:03:30,700 --> 00:03:34,960
And let's say for the
sake of simplicity data,

54
00:03:35,380 --> 00:03:39,640
you use supervised learning,
right? It turns out for the
CSU 30 costs, Rajai eggs,

55
00:03:39,910 --> 00:03:44,080
I think more than 50% of the class
projects tend to use supervisor.

56
00:03:44,081 --> 00:03:45,970
And then there are also
other projects they use.

57
00:03:46,300 --> 00:03:49,390
And I've using gans we should talk about
later this quarter or other things.

58
00:03:49,391 --> 00:03:50,500
But I think,
you know,

59
00:03:50,530 --> 00:03:52,930
let's say you supervise there anything
to take build that you're seeing

60
00:03:52,960 --> 00:03:55,520
application. Um, and,

61
00:03:56,040 --> 00:03:59,770
and I think for today I'm going
to use as a running example,

62
00:04:00,330 --> 00:04:05,200
building a, um, uh, building a
voice activated device. Right? So,

63
00:04:05,440 --> 00:04:06,340
you know,
uh,

64
00:04:06,390 --> 00:04:09,970
I don't know actually how many of you
have like a smart speaker in your home,

65
00:04:09,971 --> 00:04:14,430
like a voice activated device in
your home, you know, in the US? Well,

66
00:04:14,440 --> 00:04:18,670
not that many of you are
interesting. Okay, cool. Yeah,
so I think, uh, uh, you know,

67
00:04:18,671 --> 00:04:23,440
the Amazon Echos Google homes, that apple
series or the, the in, in, in China,

68
00:04:23,620 --> 00:04:27,850
my one of my former team
was built by two zero s uh,

69
00:04:27,880 --> 00:04:32,880
but let's say for the sake of argument
that you want to build a voice activated

70
00:04:33,100 --> 00:04:36,280
device and I'm going to use
it as a running example. Um,

71
00:04:36,340 --> 00:04:40,360
and so in order to build a voice
activated device, and again,

72
00:04:40,390 --> 00:04:43,930
I'm not going to use any of the commercial
brands like Alexa or okay Google or

73
00:04:43,931 --> 00:04:48,090
hey Siri or I guess in China was a hello
sal duty while the south do anyhow,

74
00:04:48,120 --> 00:04:50,850
which means kind of roughly,
hello little do. Um,

75
00:04:51,280 --> 00:04:53,200
but let's use a more neutral word,

76
00:04:53,230 --> 00:04:56,920
which is he wanted to build a device that
your response there where it activates

77
00:04:57,310 --> 00:05:00,730
and you're actually going to implement
it as a problem set later this quarter.

78
00:05:01,300 --> 00:05:04,180
Um,
but so you want to build a yeah,

79
00:05:07,900 --> 00:05:12,300
nope. Volume. Uh, uh,

80
00:05:12,340 --> 00:05:17,290
let's see how that, okay. Is this
better now? Yes. No, this is better.

81
00:05:17,291 --> 00:05:18,550
Okay, cool. Thank you. Logan.

82
00:05:18,551 --> 00:05:22,420
Ironic talk about speech recognition
and the volume is higher. Okay.

83
00:05:23,080 --> 00:05:25,870
Um, so let's say you
want, wow, let's do that.

84
00:05:26,860 --> 00:05:30,250
Let me know if they've come software
gave it. Thank you. Um, so let's,

85
00:05:30,251 --> 00:05:33,550
you want to build a voice activated
device. So the key components,

86
00:05:33,551 --> 00:05:37,690
the key machine learning and deep learning
components is going to be a learning

87
00:05:37,691 --> 00:05:42,691
algorithm that takes us input
and audio clip and outputs.

88
00:05:44,830 --> 00:05:46,730
Um,
did it detect

89
00:05:49,240 --> 00:05:50,920
what's sometimes called a trigger word.

90
00:05:53,920 --> 00:05:57,190
Now did I go software game? Okay,
this is okay, great. All right. And,

91
00:05:57,200 --> 00:06:02,200
and I'll plus y zero one digit to check
the trigger words such as Alexa or okay

92
00:06:03,471 --> 00:06:07,630
Google or hey Siri or a
hello little do or, um, uh,

93
00:06:07,730 --> 00:06:12,620
or activate or whatever wake where
they'll trigger word, right? Um,

94
00:06:13,340 --> 00:06:14,173
and so

95
00:06:17,180 --> 00:06:21,320
step one is a select a problem.
Um,

96
00:06:21,350 --> 00:06:24,590
and then in order the trainer
there learning algorithm,

97
00:06:25,340 --> 00:06:28,910
you need to get they both data.
If you apply supervised learning

98
00:06:32,360 --> 00:06:35,870
and then you design a model,

99
00:06:39,640 --> 00:06:43,760
use back prof or some of the other albums
you've learned about momentum, Adam,

100
00:06:43,920 --> 00:06:48,020
you know, various optimization algorithms,
gradient descent to train the model.

101
00:06:50,610 --> 00:06:52,790
And then maybe you
testing all your test that

102
00:06:56,900 --> 00:06:58,280
and then you deploy it.

103
00:06:58,310 --> 00:07:00,930
Meaning just start selling these
smart speakers and you know,

104
00:07:00,950 --> 00:07:04,880
putting them into hopefully
until your uses homes. Um,

105
00:07:06,230 --> 00:07:07,063
and then

106
00:07:10,400 --> 00:07:14,340
you have to maintain the system. They'll
talk about this later as well. Uh, and,

107
00:07:14,341 --> 00:07:16,770
and this is not chronological beds.
Uh,

108
00:07:16,970 --> 00:07:20,330
one thing that is often done but I want
to talk about today at the end is dead.

109
00:07:20,360 --> 00:07:24,710
It's not really, step eight is a
Qa, which is a quality assurance,

110
00:07:24,711 --> 00:07:29,690
which is an ongoing process.
Right? And so, um, one,

111
00:07:29,930 --> 00:07:33,620
uh, let's see. So as you said,
if you want to build a product,

112
00:07:33,720 --> 00:07:35,180
everyone selling machine
there any product,

113
00:07:35,181 --> 00:07:39,220
these are maybe some of the key
steps you need to work on. Um,

114
00:07:39,800 --> 00:07:43,730
some observations when you train them
all though training them all the is often

115
00:07:43,731 --> 00:07:46,750
a very iterative process. So every time
we train the machine learning model,

116
00:07:47,000 --> 00:07:51,290
you find that, you know, I can
almost guarantee whatever you do,

117
00:07:51,291 --> 00:07:54,320
it will not work, at least
not the first time. Right?

118
00:07:54,380 --> 00:07:57,560
And so you find that even though I've
written it as a sequence of steps,

119
00:07:57,800 --> 00:08:01,650
when you train them all though young ago,
know that neural network architecture,

120
00:08:01,670 --> 00:08:02,271
it didn't work.

121
00:08:02,271 --> 00:08:06,410
I need to increase the number of hidden
units or change the realization or such.

122
00:08:06,411 --> 00:08:09,440
They're RNN or switch to a
totally different architecture.

123
00:08:09,441 --> 00:08:13,050
And sometimes you train them all day
and go, nope, that didn't work. Um,

124
00:08:13,430 --> 00:08:15,710
I need to get more data.
Right?

125
00:08:15,740 --> 00:08:20,450
And so this is often a very iterative
process where you're cycling through, um,

126
00:08:20,960 --> 00:08:25,410
oh to several different steps
here. Um, and then I think, uh,

127
00:08:25,580 --> 00:08:28,970
one distinction that you have not yet
during the ball in the Coursera and the

128
00:08:28,971 --> 00:08:33,440
deep learning.ai Coursera videos is how
to split up the data into train death

129
00:08:33,441 --> 00:08:36,410
and tests. So I'm going to
simplify those details for now,

130
00:08:36,620 --> 00:08:39,290
but just as a foreshadowing,
I guess

131
00:08:41,670 --> 00:08:44,570
you learn later in the, in
the, uh, deeply into AI,

132
00:08:44,630 --> 00:08:48,680
Coursera videos is how to take a data set.
You have training to excuse me,

133
00:08:48,690 --> 00:08:51,170
the entire training set, um, uh,

134
00:08:51,480 --> 00:08:53,960
into a set that you actually test,

135
00:08:54,080 --> 00:08:57,970
cross validate using during development
called the deaf center development sets,

136
00:08:57,971 --> 00:09:00,360
or hold our cross validation set
as soon as a separate test set.

137
00:09:00,361 --> 00:09:04,800
So you learn about this later, but I'm
just simplifying a little bit for today.

138
00:09:05,520 --> 00:09:08,940
Okay. So, um, uh,

139
00:09:10,170 --> 00:09:11,920
so I think,
um,

140
00:09:12,600 --> 00:09:17,250
the first thing I want to do
is ask you a question, right?

141
00:09:17,251 --> 00:09:20,390
So we're going to talk through
many of these steps. Oh.
And it turns out that, um,

142
00:09:20,640 --> 00:09:24,510
what a lot of machine learning classes
do and do a good job to change is

143
00:09:24,690 --> 00:09:28,200
focusing on maybe these three steps.

144
00:09:28,200 --> 00:09:30,390
So maybe these four steps,
right.

145
00:09:30,480 --> 00:09:32,850
And one of the ones that do
today is spend more time.

146
00:09:33,180 --> 00:09:36,810
So this is the hearts and machine
learning. How do you build a great model?

147
00:09:37,170 --> 00:09:41,580
A and one other one that do today is
spend more time talking about step one and

148
00:09:41,581 --> 00:09:45,330
six and seven and then just a little
bit of time talking about the call this

149
00:09:45,331 --> 00:09:47,820
because you kind of need to do the
steps was well you want to go with deep

150
00:09:47,821 --> 00:09:52,000
learning product. I'll go on the
machine learning application. Okay. Um,

151
00:09:52,470 --> 00:09:54,820
so let's start the discussion question.
Um,

152
00:09:55,410 --> 00:09:58,180
I'm actually curious,
uh,

153
00:09:58,530 --> 00:10:00,540
if you are selecting a

154
00:10:02,820 --> 00:10:07,110
project to work on, uh,
uh, what are the actually,

155
00:10:07,111 --> 00:10:09,090
so I don't,
don't answer this yet.

156
00:10:09,180 --> 00:10:12,960
I'll tell you what the question
was going to ask is, which is,

157
00:10:16,360 --> 00:10:20,520
all right, uh, what properties make for
a good candidate deep learning project.

158
00:10:20,540 --> 00:10:22,100
But don't answer yet right though.

159
00:10:22,110 --> 00:10:26,340
I want to say a few more things before I
invite you to answer, which is that, um,

160
00:10:26,430 --> 00:10:29,340
all of you for the last few days I hope
have been thinking about what pressure

161
00:10:29,350 --> 00:10:30,480
you want to do for this cause.

162
00:10:30,510 --> 00:10:34,500
And what I want to do is just discuss
some properties of what are good projects

163
00:10:34,501 --> 00:10:37,920
to work on and what are maybe not
good practice and work on. Okay. And,

164
00:10:38,250 --> 00:10:41,550
and think of this as your chance to
give your classmates advice, right.

165
00:10:41,600 --> 00:10:43,160
One of the things you're
consummation should think about,

166
00:10:43,170 --> 00:10:46,410
the shiny decides this is a good
price, the work on. Okay. Um,

167
00:10:46,950 --> 00:10:50,800
and so what I want to do
for today is, uh, use, um,

168
00:10:51,090 --> 00:10:54,930
this voice activated thing as,
as a, as a motivating example.

169
00:10:55,290 --> 00:10:59,970
And you know, there's actually
one project I was working on.

170
00:11:00,180 --> 00:11:02,460
Uh, the, there's actually a
thought that we're actually,

171
00:11:02,461 --> 00:11:06,020
there was one project I
thought that working on with
decided not to work on, uh,

172
00:11:06,120 --> 00:11:10,500
and that, that there's a voice activated
device. So it turns out that, um,

173
00:11:10,650 --> 00:11:13,830
these voice activated devices
and Echo Google homes and so on,

174
00:11:13,831 --> 00:11:17,900
they are taking off quite rapidly.
In the U s and around the world. Um,

175
00:11:18,030 --> 00:11:19,750
it turns out that one of the,
you know,

176
00:11:19,800 --> 00:11:24,750
significant pain points of these devices
is the need to configure it, right.

177
00:11:24,751 --> 00:11:29,160
To set that up for Wifi. So I've done
a lot of work on speech recognition,

178
00:11:29,190 --> 00:11:32,430
you know, a hotel dialogue
work and Google speech system.

179
00:11:32,431 --> 00:11:33,780
I let the buy to speech system.

180
00:11:33,960 --> 00:11:37,080
So I've been published papers and
speech recognition and I have a,

181
00:11:37,090 --> 00:11:40,680
I have one of these devices in my
home, right? Uh, actually I was,

182
00:11:40,710 --> 00:11:43,030
I have the Amazon Echo in my living room.
Um,

183
00:11:43,410 --> 00:11:48,410
but even to this day I have configured
exactly one light bulb to be hooked up to

184
00:11:48,601 --> 00:11:53,230
be controlled by my echo because the
setup process, not blaming any country,

185
00:11:53,260 --> 00:11:55,330
it's just difficult to hook up,
you know,

186
00:11:55,331 --> 00:12:00,331
it Wifi enabled light bulb and then to
set it up so that your smart speaker or

187
00:12:01,271 --> 00:12:04,450
whatever, I was going to say, you know,
smart device turned off to the lab.

188
00:12:04,480 --> 00:12:09,130
So I have one light bulb in my living
room where I can turn on and off and

189
00:12:09,131 --> 00:12:13,060
that's it, right. Even as
a speech researcher. So

190
00:12:16,200 --> 00:12:19,660
I'm going to say a bad
example. Um, so one, one,

191
00:12:19,780 --> 00:12:24,040
one application that I think dead I was
actually seriously cause were working on

192
00:12:24,070 --> 00:12:29,070
is to build a embedded device that
you can sell to lamp makers so that,

193
00:12:30,550 --> 00:12:33,250
I don't know where you buy your downstroke
and you'll have a few labs or my

194
00:12:33,251 --> 00:12:34,930
career or a few labs or wherever,

195
00:12:35,200 --> 00:12:39,250
but you can buy a desk lamp so
that when you buy the desk lamp,

196
00:12:39,280 --> 00:12:43,780
there's already a built in microphone
so that without needing to connect this

197
00:12:43,781 --> 00:12:47,810
thing to Wifi, you know as a,
Hey, here's a $20 a desk lamp. Um,

198
00:12:48,070 --> 00:12:51,170
but then on your desk and you
can go home and say desk lamp,

199
00:12:51,190 --> 00:12:53,940
turn on or turn off a.

200
00:12:53,950 --> 00:12:58,950
Then I think that will help a lot more
users get voice activated devices into

201
00:12:59,051 --> 00:13:00,910
their home and it's
actually not clear to me.

202
00:13:01,270 --> 00:13:04,930
If you want to turn on a desk lamp is
actually not clear to me that you want to

203
00:13:04,931 --> 00:13:07,780
turn to a smart speaker and say,
Hey smart speaker,

204
00:13:07,960 --> 00:13:11,680
please turn on that lab over there.
It may be a fuse one,

205
00:13:11,681 --> 00:13:16,030
natural it just talk directly to a desk
lamp and tell it to turn on a term. Um,

206
00:13:16,420 --> 00:13:20,710
and so, uh, also for wellness
where if someone were friends,
now we evaluated this,

207
00:13:20,711 --> 00:13:24,190
we actually thought that this could be
a reasonable business to build embedded

208
00:13:24,191 --> 00:13:28,870
devices to sell to lamp makers or other
device makers so that they can sell

209
00:13:29,020 --> 00:13:32,650
their own voice activated devices without
needing this complicated wifi setup

210
00:13:32,680 --> 00:13:35,650
process. Um, and so to do this,

211
00:13:35,651 --> 00:13:38,470
you would need to build a learning
algorithm and have it run in an embedded

212
00:13:38,471 --> 00:13:42,250
device. They just impose an audio
clip and I'll quiz, you know,

213
00:13:42,280 --> 00:13:43,690
whenever it detects the,

214
00:13:43,740 --> 00:13:47,680
the wake word and instead of a wake
where being activated the week where it

215
00:13:47,681 --> 00:13:50,050
would be a lamp turn on or turn off,

216
00:13:50,051 --> 00:13:54,160
you need to wake words or trigger words
one to turn it on, when to turn it off.

217
00:13:54,310 --> 00:13:57,370
Right. Oh. And, and, and, and I
think just the other thing that, um,

218
00:13:57,520 --> 00:14:02,110
I think would make this work,
uh, is, um, uh, uh, to, uh,

219
00:14:02,140 --> 00:14:06,040
to give these devices names.
So if you have five lamps or two lambs,

220
00:14:06,041 --> 00:14:10,620
you you need an way to index into these
different desk lamps. So, um, let's see,

221
00:14:10,630 --> 00:14:13,480
you decide for your project, you
know, to have a little switch here.

222
00:14:13,720 --> 00:14:18,720
So this lab could be called
John or Mary or Bob or hours,

223
00:14:19,441 --> 00:14:20,560
like a four way switch.

224
00:14:20,770 --> 00:14:24,040
So that's depending on where you said
this before we switch, you can see,

225
00:14:24,190 --> 00:14:24,551
you know,

226
00:14:24,551 --> 00:14:29,551
John very term on or is it if you
decide to call this lamb John,

227
00:14:29,861 --> 00:14:30,460
like it was a goodness,

228
00:14:30,460 --> 00:14:34,540
some of the names so you don't have
every land by the same name. Okay. Um,

229
00:14:34,600 --> 00:14:39,100
so what I'm going to do is use as a,
a motivating example,

230
00:14:39,310 --> 00:14:43,360
um, this as a possible project.
Oh. And I'm not working on this.

231
00:14:43,361 --> 00:14:46,750
If any of you want to build a start up
doing this, go for it. I, I, this is not,

232
00:14:48,080 --> 00:14:51,230
I felt my teams and I had better ideas
that we wanted to do other things in this,

233
00:14:51,231 --> 00:14:52,790
but I should don't see
anything wrong with this.

234
00:14:52,820 --> 00:14:55,510
I think this actually could be a
reasonable thing to come see us for help.

235
00:14:55,511 --> 00:14:58,860
And I'm not doing it. So you are very
welcome to if you want. Okay. Um,

236
00:14:59,180 --> 00:15:02,530
so now the question I
want to post you is a,

237
00:15:02,810 --> 00:15:06,770
when you're brainstorming project ideas,
you know, like this idea, some other idea,

238
00:15:07,060 --> 00:15:10,590
um, what are the things you would
want to watch out for? What,

239
00:15:10,591 --> 00:15:14,600
what are the properties that you would
want to be true in order for you to few

240
00:15:14,601 --> 00:15:18,560
good proposing this as a,
as a CSU 30 project, right?

241
00:15:18,561 --> 00:15:23,000
So why should take a minute and write
this down? I think, uh, uh, uh, yeah.

242
00:15:23,070 --> 00:15:25,900
What if you're asking a friend,
if a friend is asking you,

243
00:15:26,010 --> 00:15:29,450
what are the things I should look at
to see if something has a big project,

244
00:15:29,570 --> 00:15:32,810
what would you, what is your
recommended them? So feel free,

245
00:15:32,811 --> 00:15:37,540
just write down a few keywords and then
we'll see what people say. And then,

246
00:15:37,550 --> 00:15:41,000
and then I'll tell you what I tend to
look out for when I'm selecting projects

247
00:15:41,660 --> 00:15:43,970
and they have lists of a five points.

248
00:15:50,150 --> 00:15:50,983
Okay.

249
00:15:52,290 --> 00:15:57,040
To take late, I don't know,
like two minutes to, oh,

250
00:15:57,100 --> 00:15:58,690
sorry,
this is not activate it.

251
00:16:00,760 --> 00:16:01,593
MMM.

252
00:16:02,640 --> 00:16:05,490
I do not ever, the answer is
up and to answer this. Okay.

253
00:16:06,570 --> 00:16:09,510
Let me test the Internet says

254
00:16:13,840 --> 00:16:18,010
just checking it on. Yup. I am connected
to the Internet. Uh, it, any ideas?

255
00:16:21,160 --> 00:16:24,280
Oh, I see. Okay. All
right. Let me try that.

256
00:16:29,310 --> 00:16:30,143
Okay.
Thank you.

257
00:16:34,840 --> 00:16:35,790
Oh yes,
thank you.

258
00:16:41,770 --> 00:16:42,603
Thanks.

259
00:17:11,410 --> 00:17:14,320
So can you take like two minutes to enter?
I think,

260
00:17:14,350 --> 00:17:17,140
I think I can think of this and
let you enter multiple answers.

261
00:17:17,180 --> 00:17:18,130
Let me stick tunes.

262
00:18:08,130 --> 00:18:09,370
All right,
another one minute.

263
00:18:18,550 --> 00:18:19,383
Okay.

264
00:18:25,640 --> 00:18:26,473
Yeah.

265
00:18:36,230 --> 00:18:37,063
Hi,

266
00:18:37,260 --> 00:18:38,570
Miss 30 seconds.

267
00:18:47,750 --> 00:18:52,430
Okay.
Yeah.

268
00:19:01,120 --> 00:19:05,750
Okay.
Okay.

269
00:19:06,230 --> 00:19:07,970
Sweet to one.

270
00:19:10,860 --> 00:19:11,693
Well,

271
00:19:12,740 --> 00:19:15,050
maybe in hindsight that
wasn't the best visualization.

272
00:19:17,600 --> 00:19:18,620
Can people see this?

273
00:19:32,260 --> 00:19:33,093
MMM,

274
00:19:39,750 --> 00:19:43,720
well the line, trying to see if,
all right, so, uh, data novels,

275
00:19:43,721 --> 00:19:45,910
he lost the data.
Some of these really small,

276
00:19:46,260 --> 00:19:49,480
he wouldn't doable number of examples.
Do you have one, two months? No.

277
00:19:49,481 --> 00:19:54,250
Office salvo. You industry a fields
a clear objective. Practical.

278
00:19:54,251 --> 00:19:55,084
Useful.

279
00:19:57,000 --> 00:19:57,833
Yeah.

280
00:19:57,970 --> 00:20:01,100
Huh. Oh, can we finish in time? Uh,

281
00:20:05,070 --> 00:20:09,000
host real life problem. Useful. Hasn't
been done computationally tractable.

282
00:20:10,250 --> 00:20:14,850
Yeah. Generalization.
I see. Cool. Great. Um,

283
00:20:16,410 --> 00:20:20,670
let me make some comments on fees. I
think I, this is, this is pretty good.

284
00:20:21,150 --> 00:20:24,120
I had a list of five bullet points
that maybe I just share with you my,

285
00:20:24,121 --> 00:20:28,770
this a five. Ah, um, which is I think
just some things I encourage you to

286
00:20:30,330 --> 00:20:32,760
pay attention to. Um, you know, this,

287
00:20:32,761 --> 00:20:37,120
this may or may not be the best criteria
but interests, interests, just, oh no,

288
00:20:37,140 --> 00:20:37,470
it doesn't.

289
00:20:37,470 --> 00:20:40,060
Hopefully you can work on something
that you're actually interested in. Um,

290
00:20:40,950 --> 00:20:43,100
and then I think, uh, uh,

291
00:20:45,690 --> 00:20:46,830
data availability,

292
00:20:46,860 --> 00:20:51,350
which many of you cited is a good
criteria or one of the ways that, um,

293
00:20:51,610 --> 00:20:55,660
Stanford cost projects sometimes do not
go well is if students spend a month to

294
00:20:55,661 --> 00:20:59,500
try to collect data and after a monk
have not yet found the Datatel and then,

295
00:20:59,530 --> 00:21:04,520
and then, you know, and then this and
then there's a lot of waste of time. Um,

296
00:21:05,590 --> 00:21:08,230
one thing that I would encourage you

297
00:21:13,120 --> 00:21:16,100
to consider as well is a domain knowledge.
Um,

298
00:21:16,510 --> 00:21:21,510
and I think that if you are a biologist
and have unique knowledge into some

299
00:21:21,761 --> 00:21:24,550
aspect of biology to which you
want to apply machine learning,

300
00:21:24,910 --> 00:21:27,880
that will actually let you use
very interesting project, right?

301
00:21:27,940 --> 00:21:31,610
That is actually difficult
for others to do. Um,

302
00:21:31,990 --> 00:21:36,700
and I think more generally as, as advice
for navigating your careers. Right?

303
00:21:36,701 --> 00:21:40,330
So you a shame because AI, machine
learning, deep learning, there's so much,

304
00:21:40,360 --> 00:21:43,990
there's so many people wanting to jump
into machine learning and deep learning.

305
00:21:44,360 --> 00:21:45,970
Um,
actually give an example.

306
00:21:46,180 --> 00:21:50,190
So I sometimes talk to doctors
near Radiology students, uh,

307
00:21:50,320 --> 00:21:55,150
including Stanford and other universities
realize you students that want to

308
00:21:55,151 --> 00:21:57,760
learn about machine learning, right?
Because they hear about, you know,

309
00:21:57,820 --> 00:22:00,610
deep learning, maybe someday
affecting radiologists as jobs.

310
00:22:00,611 --> 00:22:02,220
And so they wanted to be politic learning.

311
00:22:02,590 --> 00:22:07,590
And so my career advice to them is usually
to not forget everything they learned

312
00:22:08,531 --> 00:22:10,510
as a doctor and try to,
you know,

313
00:22:10,511 --> 00:22:13,990
do machine learning one on one from
scratch and just forget everything they

314
00:22:13,991 --> 00:22:17,230
learned as a doctor and just become
a CS major. I think that that,

315
00:22:17,231 --> 00:22:22,060
that path can work, but I
think we're radiologists
could do the most unique work,

316
00:22:22,460 --> 00:22:23,010
uh,
that,

317
00:22:23,010 --> 00:22:25,990
that allows them to make the most union
contribution is that they use their

318
00:22:25,991 --> 00:22:27,860
domain knowledge of healthcare,

319
00:22:27,880 --> 00:22:31,690
radiology and do something in machine
learning applied to radiology rate. Uh,

320
00:22:31,790 --> 00:22:36,780
and so, all right. How, how, how many,

321
00:22:37,840 --> 00:22:39,710
how many millennials
are there in this class?

322
00:22:42,310 --> 00:22:43,980
What does that mean? Me, me, me, me thing.

323
00:22:46,980 --> 00:22:48,130
All right.

324
00:22:51,340 --> 00:22:55,960
This is really wrong.
I think it's because of the word cloud.

325
00:22:55,990 --> 00:23:00,070
So very council where it frequency,
right? The money thing. I don't know.

326
00:23:00,071 --> 00:23:03,400
I have very mixed feelings about that.
All

327
00:23:05,060 --> 00:23:07,070
right. Um, but then as you for,

328
00:23:07,150 --> 00:23:10,850
I actually know that some of you are
taking your deep learning because you work

329
00:23:10,851 --> 00:23:14,350
in a different discipline and you
want to do something in this hot, new,

330
00:23:14,351 --> 00:23:17,410
exciting thing of machine learning and
they think whatever this summit is you're

331
00:23:17,411 --> 00:23:20,940
in. If your domain knowledge about
some of the area, you know, education,

332
00:23:20,960 --> 00:23:23,350
civil engineering, biology, law, um,

333
00:23:23,770 --> 00:23:26,650
taking deep learning allows
you to do very unique work,

334
00:23:26,651 --> 00:23:30,550
applying machine learning
to your domain. Right? Oh,

335
00:23:31,540 --> 00:23:34,380
let's see. Um, I think that, uh, uh,

336
00:23:36,660 --> 00:23:40,570
um,
I think what I call the utility,

337
00:23:40,571 --> 00:23:41,740
but several of you mentioned as well,

338
00:23:41,750 --> 00:23:45,790
something that has a positive impact that
I should helps other people, uh, uh, uh,

339
00:23:46,350 --> 00:23:49,490
I don't have money.
It could be an aspect of utility,

340
00:23:49,491 --> 00:23:53,760
but maybe not the most inspiring
one. Uh, and then I think, um,

341
00:23:57,990 --> 00:24:01,520
I think one of the biggest challenges
we faced in the industry today is,

342
00:24:01,521 --> 00:24:05,030
so frankly there's actually a
good judgment on feasibility. Um,

343
00:24:05,330 --> 00:24:10,330
so today I still see too many leaders are
sometimes CEOs of large companies that

344
00:24:12,291 --> 00:24:15,230
stand on stage and announced
to the whole world, you know,

345
00:24:15,231 --> 00:24:18,530
we're going to do this machine learning
project to do this by this deadline.

346
00:24:18,590 --> 00:24:23,030
And then 20 minutes later I talked to
their engineers and the engineers say,

347
00:24:23,060 --> 00:24:27,020
nope, no way not happening. Where
the CEO just farming this age,

348
00:24:27,420 --> 00:24:30,140
Ho Ng modelization, there's not
doing it. And those is impossible.

349
00:24:30,141 --> 00:24:34,010
So I think one of the biggest challenges
is actually feasibility, um, in, in fact,

350
00:24:34,160 --> 00:24:38,840
I actually know that a, you know, that's a
chatter of rt, about, uh, the, the um, uh,

351
00:24:38,900 --> 00:24:42,670
ta office hours. And I know that,
uh, uh, have been a lot of, you know,

352
00:24:42,770 --> 00:24:46,820
long of you have been thinking
about applying a end to
end deep learning. Right.

353
00:24:46,850 --> 00:24:50,590
You know, can you input any acts and
output any why and, and do the Acura the,

354
00:24:50,600 --> 00:24:52,820
and sometimes it's possible
and sometimes it's not.

355
00:24:52,880 --> 00:24:57,350
And it still takes a relatively deep
judgment about what new nether us can and

356
00:24:57,351 --> 00:25:01,670
cannot do with a certain amount of data
that you may or may not be or April to

357
00:25:01,671 --> 00:25:05,900
acquire in order to do some of
these things. Right. Um, so,

358
00:25:06,110 --> 00:25:10,220
so I think throughout this quarter you've
gained much deeper judgment as well on

359
00:25:11,180 --> 00:25:16,070
what is feasible. I guess
it was pretty interesting.

360
00:25:16,071 --> 00:25:17,140
I once, no, uh, I,

361
00:25:17,141 --> 00:25:21,770
I knew a CEO of a very large
company that once told his team,

362
00:25:21,920 --> 00:25:25,350
um, uh, he actually gave us seem
decent structure. It's, he said,

363
00:25:25,880 --> 00:25:30,540
I wish the assume that AI can do
anything. Uh, and, and, and it, it,

364
00:25:30,750 --> 00:25:35,580
I think that had an interesting
effect, I guess. Uh, uh, uh,

365
00:25:36,050 --> 00:25:38,120
yeah. Cool. All right.

366
00:25:38,390 --> 00:25:42,200
So I think step one was selected project.

367
00:25:42,201 --> 00:25:45,320
I hope there's this thing projects chose.
Keep some of those things in mind. Um,

368
00:25:45,740 --> 00:25:47,330
step two is get data.

369
00:25:51,630 --> 00:25:56,390
Um, and so, uh, what I want you to do,

370
00:25:56,730 --> 00:25:59,660
and I'm going to pose a second question
and then have some to discuss this.

371
00:25:59,930 --> 00:26:03,690
Let's say that you're actually
working on this, you know, smarts,

372
00:26:03,850 --> 00:26:07,100
a voice activated embedded device thing,
right?

373
00:26:07,101 --> 00:26:09,410
So let's say that you and your
friends want to build a startup.

374
00:26:09,800 --> 00:26:12,910
So train the deep burning our
river to detect you there.

375
00:26:12,930 --> 00:26:15,000
Phrases like John turned
on will marry you,

376
00:26:15,001 --> 00:26:19,790
turnoff or Bob turned off or whatever
to sell to a device makers so that they

377
00:26:19,791 --> 00:26:24,290
can a low voice embedded voice detection
chip that doesn't require a complicated

378
00:26:24,380 --> 00:26:27,200
wifi set up process, right? So
let's see when, uh, let's see,

379
00:26:27,201 --> 00:26:28,040
I actually want to do this.

380
00:26:28,490 --> 00:26:32,480
So you need to collect some data in
order to site training or learning our

381
00:26:32,481 --> 00:26:36,710
revenue. So, um, the second
question that I pose to you is, uh,

382
00:26:37,550 --> 00:26:40,130
uh, uh, to, uh, question in two parts,

383
00:26:40,400 --> 00:26:44,760
but I'll have you answer it all at the
same time, which is I'm in how many,

384
00:26:44,790 --> 00:26:45,810
how many days?

385
00:26:45,840 --> 00:26:49,890
Let's say you actually propose this
for your CSU 30 project this Sunday,

386
00:26:49,891 --> 00:26:52,290
and then you start work on
it, you know, like on Monday,

387
00:26:52,291 --> 00:26:55,410
are you going to start work on it
today before the proposal, but, uh,

388
00:26:55,530 --> 00:26:58,930
how many days would you
spend collecting data? Uh,

389
00:26:58,960 --> 00:27:02,830
and how would you collected data?
Okay. And I think, um, I actually,

390
00:27:02,870 --> 00:27:06,420
how many of you have participated
in engineering scrum?

391
00:27:06,480 --> 00:27:10,050
If you know what that means. Oh, okay. A
few of you. Those who have an industry.

392
00:27:10,051 --> 00:27:12,390
Okay. All right. So
engineering estimation,

393
00:27:12,420 --> 00:27:14,250
when you estimate how
long a project takes,

394
00:27:14,251 --> 00:27:18,750
one of the common practices is use a
Fibonacci sequence to estimate how long a

395
00:27:18,751 --> 00:27:22,710
project will take. Right? And so
people now she sequence one, one, two,

396
00:27:23,160 --> 00:27:25,500
three, five, eight,

397
00:27:26,130 --> 00:27:29,700
13 and so on.
And there's roughly powers of two,

398
00:27:29,701 --> 00:27:34,140
but doesn't grow as fast as pilots too.
And FIBONACCI numbers are cool. Right? So,

399
00:27:34,410 --> 00:27:38,240
uh, so, so whether one should they do
actually the me just finish over the

400
00:27:38,241 --> 00:27:39,330
configuration.
Right.

401
00:27:45,920 --> 00:27:49,740
Okay.
When I would,

402
00:27:51,800 --> 00:27:54,060
um, speech bubbles.
Okay. Yeah, that's good.

403
00:27:56,470 --> 00:27:59,180
All right. So what I'd like you
to do is in the text answer,

404
00:27:59,240 --> 00:28:01,880
I really write two things.
One is write a number.

405
00:28:01,910 --> 00:28:05,450
How many days do you think you spend
on collecting data union teammates if

406
00:28:05,451 --> 00:28:07,930
you're actually doing this
project? Uh, and then how,

407
00:28:07,931 --> 00:28:09,530
how would you go about
collecting the data?

408
00:28:10,170 --> 00:28:14,340
Okay. So why don't you take
like another two minutes

409
00:28:17,940 --> 00:28:19,110
to write in an answer.

410
00:28:25,490 --> 00:28:29,550
Oh, I'm sorry. Let me say reload now.

411
00:28:29,670 --> 00:28:30,660
Still not activated

412
00:28:41,190 --> 00:28:43,800
Sarah down.
I'm trying to hit what we're just saying.

413
00:28:45,320 --> 00:28:46,460
That is not helpful.

414
00:28:52,470 --> 00:28:56,660
Okay. All right, Dennis.
Definitely not helpful.

415
00:28:59,750 --> 00:29:01,670
Um hmm.

416
00:29:04,220 --> 00:29:05,590
All right,
let's do this.

417
00:29:05,680 --> 00:29:10,540
Write down your answer on a piece of paper
first and take two. And this is, yeah,

418
00:29:10,541 --> 00:29:13,800
so the two questions off or how many days,
um,

419
00:29:14,170 --> 00:29:17,680
pick a number for the people
Nachi sequence and uh,

420
00:29:17,730 --> 00:29:19,600
or are you going to use
that all time? Oh, okay.

421
00:29:20,080 --> 00:29:23,420
Let's swap out my computer for
rts. Oh, actually, yeah. Oh,

422
00:29:23,530 --> 00:29:25,060
it hotties computers working.

423
00:29:25,420 --> 00:29:26,253
Yeah, sure. Go ahead.

424
00:29:28,990 --> 00:29:32,900
Oh yeah, I can just present. Let's plug
in your laptop. Just use your laptop.

425
00:29:37,640 --> 00:29:41,050
Sure. Yeah, yeah.

426
00:29:43,120 --> 00:29:45,730
Doesn't it say, I wonder if it's a death
where problem a web browser problem.

427
00:29:46,500 --> 00:29:47,333
MMM.

428
00:29:47,440 --> 00:29:51,940
I started that using a Firefox recently
in addition to chrome and Safari and

429
00:29:51,941 --> 00:29:54,490
Dallas fire falls. I tried
with other web browsers. Liter.

430
00:29:59,100 --> 00:30:02,890
Great. All right, cool. Thanks. Awesome.

431
00:30:08,470 --> 00:30:13,270
All right. Maybe we have maybe people
that take another minute from now.

432
00:30:13,300 --> 00:30:14,410
Just extend the time a bit.

433
00:31:05,140 --> 00:31:06,980
All right.
10 seconds.

434
00:31:13,400 --> 00:31:17,430
Yeah. All right, cool.

435
00:31:18,630 --> 00:31:23,220
Let's see answers. Okay. All

436
00:31:24,810 --> 00:31:25,610
right.

437
00:31:25,610 --> 00:31:26,900
Well,
365

438
00:31:32,810 --> 00:31:36,180
does, uh, does uh, does a lot of
variance in the answer is right.

439
00:31:37,930 --> 00:31:38,763
MMM.

440
00:31:43,220 --> 00:31:47,120
I know. Download from online depends on
what data you want. It turns out well.

441
00:31:47,121 --> 00:31:51,050
So if you're trying to find data or
phrases like John turned on that that data

442
00:31:51,051 --> 00:31:52,510
doesn't exist online.
Uh,

443
00:31:52,850 --> 00:31:56,060
it turns out that you're trying to
find audio clips of the wet activate.

444
00:31:56,090 --> 00:31:59,540
There are some websites with a
single worst pronouns, but those,

445
00:31:59,810 --> 00:32:02,150
but I'm not a lot of audio clips actually.

446
00:32:02,151 --> 00:32:05,450
You said they tried to word or the wake
word is the word activate a derail.

447
00:32:05,451 --> 00:32:10,280
Some websites we can download like maybe
10 audio clips of a few people saying

448
00:32:10,310 --> 00:32:10,911
activate,

449
00:32:10,911 --> 00:32:14,720
but it's quite hard to find hundreds of
examples of different people saying that

450
00:32:14,721 --> 00:32:15,554
we're activate.

451
00:32:16,340 --> 00:32:17,173
MMM.

452
00:32:24,810 --> 00:32:26,340
Five days,
it falls from the sky.

453
00:32:31,760 --> 00:32:36,640
All right. So let me suggest, um,

454
00:32:36,940 --> 00:32:40,040
uh,
let me suggest that you guys discuss

455
00:32:40,040 --> 00:32:44,180
with each other in small groups, uh, what
you think would be the best strategy.

456
00:32:44,181 --> 00:32:46,660
How many days would you find collecting
the data and how was your cycling?

457
00:32:46,670 --> 00:32:50,490
Because anytime you try to convince
people next to you on that, um, and,

458
00:32:50,491 --> 00:32:53,930
and before I asked you to
start discussing, I want to
leave you with one thought,

459
00:32:53,960 --> 00:32:58,040
which is, um, how long do you think
it'll take you to train your first model?

460
00:32:58,490 --> 00:33:03,020
Right? And so it will take you a day
to train your first Waldo or two days.

461
00:33:03,400 --> 00:33:07,730
Uh, do you want to spend x time
collecting data and then spend,

462
00:33:08,180 --> 00:33:09,260
let's say, you know, oh no,

463
00:33:09,261 --> 00:33:12,170
it's a seventh deep learning thing and
train them all though it might take a

464
00:33:12,170 --> 00:33:14,390
couple of days, right? Especially if
you download open source packages. So,

465
00:33:14,840 --> 00:33:18,890
so if the amount of time needed to
collect data as x followed by two days to

466
00:33:18,891 --> 00:33:22,670
train your first model, what do you
think? Extra beat the amount of time.

467
00:33:22,700 --> 00:33:25,760
Why don't you guys spend like two minutes
to discuss with each other and see if

468
00:33:25,761 --> 00:33:29,630
you can cut their, their, their answers
are, there's a very large areas, right?

469
00:33:29,631 --> 00:33:30,950
Why don't you guys to
discuss if you actually,

470
00:33:31,220 --> 00:33:33,330
if the people sitting next to
you or your project partners,

471
00:33:33,350 --> 00:33:36,320
why should discuss with them how many
days do you think you should spank

472
00:33:36,321 --> 00:33:39,110
collecting data and how
you collected data? Okay,

473
00:33:39,380 --> 00:33:41,140
let's take two minutes to discuss an yeah.

474
00:33:43,850 --> 00:33:44,683
Okay.

475
00:34:26,150 --> 00:34:26,983
Yes.

476
00:35:48,280 --> 00:35:49,113
All

477
00:35:53,480 --> 00:35:54,810
right guys.
So,

478
00:35:58,640 --> 00:36:03,440
all right guys. Hey guys. So

479
00:36:05,000 --> 00:36:05,833
all right,

480
00:36:06,100 --> 00:36:09,370
mother of exciting discussion. Um, uh,

481
00:36:09,430 --> 00:36:13,470
so actually how many of you helped me up?
The groove swells up on the,

482
00:36:13,480 --> 00:36:16,090
on the low end. How many of you, you know,

483
00:36:16,210 --> 00:36:20,110
convince each other that maybe it
should be like three days or less?

484
00:36:22,330 --> 00:36:27,070
Just a few of you. How come some assault
someone, someone to say, why, why is it,

485
00:36:27,090 --> 00:36:27,923
why wife?

486
00:36:29,380 --> 00:36:31,980
Because you're welcome to just to
see if that algorithm works first.

487
00:36:32,330 --> 00:36:35,390
So you need some data to
just test to see a job were,

488
00:36:35,430 --> 00:36:40,180
was even reaching sort of some sort of
good bench mark before you then go and

489
00:36:40,181 --> 00:36:41,590
collect a massive dataset.

490
00:36:42,200 --> 00:36:42,750
Cool.
Yeah,

491
00:36:42,750 --> 00:36:45,440
I guess a little bit of the data to test
how the average horse before you even

492
00:36:45,441 --> 00:36:47,850
go into like a massive
data set. Okay, cool. Yeah.

493
00:36:48,330 --> 00:36:52,010
And did anyone has had a high end,
like a 13 days or more?

494
00:36:53,510 --> 00:36:55,970
Very few. How come? Anyone edgy. Anyone,

495
00:36:55,971 --> 00:37:00,320
anyone with insights you want to share
with the whole cloth as you, what,

496
00:37:00,321 --> 00:37:02,120
what were you all
discussing? So excited. Yeah.

497
00:37:06,870 --> 00:37:10,900
Okay. So, um,

498
00:37:11,030 --> 00:37:13,560
depending on your domain
knowledge, um, data collection,

499
00:37:13,610 --> 00:37:15,470
you can't take a long time,
especially with this problem,

500
00:37:15,990 --> 00:37:19,510
like based on the one I get to the
system last year thinking like,

501
00:37:19,870 --> 00:37:22,780
could we use like movie clips and
like sometimes it was too late,

502
00:37:23,320 --> 00:37:26,980
generates on August minutes as
far as he was since you won.

503
00:37:28,750 --> 00:37:32,800
And that would take like a thing
to like mine data from movies.

504
00:37:34,430 --> 00:37:39,340
Good.
Yeah.

505
00:37:39,510 --> 00:37:40,120
Yeah.

506
00:37:40,120 --> 00:37:44,350
Right. Yeah. And so the
complicated systems and look
at it, a subtitle, videos,

507
00:37:44,800 --> 00:37:47,770
right? Uh, like a youtube videos
have captions or something.

508
00:37:47,810 --> 00:37:51,610
And if there's a the right creative
Commons day to day, I think it use. Yeah.

509
00:37:52,120 --> 00:37:54,190
So let me,
let me tell you my bias.

510
00:37:54,210 --> 00:37:58,080
[inaudible] I just tell you what I would
do if I was working on this project. Uh,

511
00:37:58,380 --> 00:37:59,550
what, well, one caveat,

512
00:37:59,590 --> 00:38:02,710
if I haven't done so much work in speech
recognition previously fragments was my

513
00:38:02,711 --> 00:38:04,170
first project.
Um,

514
00:38:04,360 --> 00:38:09,360
I would probably spend one to two days
collecting days Hara kind of on the short

515
00:38:09,711 --> 00:38:14,420
end. Right? And I think that's,
um, you know, one of the, and, and,

516
00:38:14,430 --> 00:38:19,000
and one of the reasons is that machine
learning kind of that circle I drew up

517
00:38:19,001 --> 00:38:23,410
there is actually a very
iterative process where, um,

518
00:38:24,010 --> 00:38:25,420
until you try it,
you,

519
00:38:25,600 --> 00:38:30,310
you almost never know what's actually
going to be hard about the problem. Right?

520
00:38:30,430 --> 00:38:33,850
And so, um, so if I was seeing this
project just so you all to see what,

521
00:38:33,851 --> 00:38:36,430
what I would do, they like
it. I've actually thought
about this project a bunch,

522
00:38:36,431 --> 00:38:39,820
right? Including, you know,
trying to validate market
acceptance and so on. But um,

523
00:38:40,510 --> 00:38:43,900
which is that, um, I would
get a cheap microphone,

524
00:38:43,940 --> 00:38:48,310
a user or use a built in
that top microphone or buy
a microphone off, you know,

525
00:38:48,440 --> 00:38:51,700
buy a microphone off Amazon
or something and go around,

526
00:38:51,850 --> 00:38:55,390
say go around Stanford campus and go to
your friends and have them just say, Hey,

527
00:38:55,391 --> 00:38:59,530
do you mind saying to this microphone
there word activate or John Turner or

528
00:38:59,531 --> 00:39:03,490
whatever, and collected a
bunch of data that way. Um,

529
00:39:03,880 --> 00:39:08,680
and then, uh, uh, and
with one or two days,

530
00:39:08,820 --> 00:39:13,210
um, you should be able to collect
at least hundreds of examples. Uh,

531
00:39:13,330 --> 00:39:17,920
and that might be enough of a Dataset
to start training a rudimentary learning

532
00:39:17,921 --> 00:39:19,180
algorithm to get going.

533
00:39:19,630 --> 00:39:22,720
Because if you have not yet
worked on this problem before,

534
00:39:23,020 --> 00:39:26,770
it turns out to be very difficult to
know what's going to be hard about the

535
00:39:26,771 --> 00:39:31,500
problem. So is what's going to be hard,
um, highly accent of speakers, right? Uh,

536
00:39:31,580 --> 00:39:35,500
always what's going to be hard background
noise or it's what's going to be hard,

537
00:39:35,770 --> 00:39:39,440
you know, confusing turned
on. We've turned off, he's
year. John turned. And then,

538
00:39:40,610 --> 00:39:43,060
uh,
but when you build a new machine,

539
00:39:43,061 --> 00:39:47,770
learning any system is very difficult to
know what's hard and what's easy about

540
00:39:47,771 --> 00:39:52,390
the problem or it was what's going to be
difficult that a far few, which is um,

541
00:39:52,510 --> 00:39:55,630
the technical term for if the
microphone is very far away. Right?

542
00:39:55,631 --> 00:40:00,490
So it turns out that if we turn on the
microphone on my laptop now, for example,

543
00:40:00,800 --> 00:40:04,960
um, the, the laptop, which is what,
like three meters away from me, uh,

544
00:40:04,961 --> 00:40:09,961
we'll be hearing voice directly from my
mouth as well as voice bouncing off the

545
00:40:09,971 --> 00:40:12,130
walls. So there's lot of
reverberation in this room.

546
00:40:12,131 --> 00:40:13,900
And so that makes me
free condition harder.

547
00:40:14,230 --> 00:40:17,980
Humie are so good at processing our
reverberant sounds reverberations that you

548
00:40:17,981 --> 00:40:20,620
almost don't notice it,
but it actually,

549
00:40:20,621 --> 00:40:25,060
the learning room we'll have sometimes
has problems with reverberations,

550
00:40:25,420 --> 00:40:28,260
echoes bouncing off the
hard walls of this room. Um,

551
00:40:28,720 --> 00:40:33,310
and so depending on what you're
learning, Arbonne has trouble with. Um,

552
00:40:33,430 --> 00:40:37,210
you will then want to go back to collect
very different types of data or explore

553
00:40:37,211 --> 00:40:38,420
very different types of algorithms.

554
00:40:38,440 --> 00:40:39,950
What was the problem that
sometimes the speaker,

555
00:40:39,951 --> 00:40:43,070
that's just the volume is just
too soft in which case, you know,

556
00:40:43,090 --> 00:40:45,830
maybe we need to do something else and
normalize all of your volumes of buying

557
00:40:45,831 --> 00:40:46,920
more sensitive microphone or something.

558
00:40:47,380 --> 00:40:50,710
So it turns out they weren't building
most machine learning applications unless

559
00:40:50,711 --> 00:40:52,420
you've experienced working on it.

560
00:40:52,800 --> 00:40:55,380
And so I've actually worked on this
problem before so I have a sense of what's

561
00:40:55,381 --> 00:40:57,880
hot and it was easy, but they work on
the new project for the first time.

562
00:40:58,240 --> 00:41:00,130
It's very difficult to know.
It was hard, it was easy.

563
00:41:00,131 --> 00:41:05,131
And so my advice to most teams is rather
than spending say 20 days to collect

564
00:41:08,231 --> 00:41:13,231
data and then two days to collect model
to train the model and is often by

565
00:41:14,531 --> 00:41:18,820
training a model and then seeing
whether the examples it gets wrong.

566
00:41:18,850 --> 00:41:23,850
When did the average fail that that
that's you feedback to either collect more

567
00:41:24,011 --> 00:41:27,700
data or redesigned the model,
right?

568
00:41:27,970 --> 00:41:29,830
Or try something else.
Um,

569
00:41:30,310 --> 00:41:34,820
and if you can string the data collection
period down to be more comparable to

570
00:41:34,821 --> 00:41:37,220
how long have you ended up
taking to train your model,

571
00:41:37,250 --> 00:41:41,000
then you can start iterating
much more rapidly on,

572
00:41:41,001 --> 00:41:44,450
on actually improving your
model. Right. Oh, and um,

573
00:41:45,040 --> 00:41:49,140
so maybe one roof I'm actually tend to
recommend for most cost projects is oh no.

574
00:41:49,141 --> 00:41:52,730
If, if maybe if he needs to spend a
week up to a week to collect data, yeah,

575
00:41:52,770 --> 00:41:56,810
maybe that's okay. But it didn't
get going even will quickly. Uh,

576
00:41:56,870 --> 00:42:00,590
I would even maybe more
strongly recommend that. Um,

577
00:42:01,110 --> 00:42:05,960
and they've been so few examples in my
life where the first time I trained the

578
00:42:05,961 --> 00:42:09,380
learning algorithm room, it works. Right?
It's like pretty much never happens.

579
00:42:10,670 --> 00:42:13,460
It happened once about a year
ago and I was so surprised.

580
00:42:13,461 --> 00:42:18,170
I still remember that one
time. Ah, and so what, so,

581
00:42:18,171 --> 00:42:21,740
so, uh, machine learning development
is often a very iterative process.

582
00:42:21,770 --> 00:42:25,850
And by quickly can I have days that
and often data sets a collected through

583
00:42:25,851 --> 00:42:27,440
sweat and hardware,
right?

584
00:42:27,470 --> 00:42:31,960
And so I wouldn't literally Yo and as w
I bet she thought long world speech and

585
00:42:32,030 --> 00:42:35,370
it going quickly, I would
probably just um, uh,

586
00:42:35,420 --> 00:42:39,620
have myself or my team members run
around and find people and asked them to

587
00:42:39,621 --> 00:42:42,840
speak into a microphone and record
all your clips that way. Um,

588
00:42:42,860 --> 00:42:46,160
and then only when you validate that you
need a bigger dataset. What'd you go to?

589
00:42:46,161 --> 00:42:49,850
More complicated things. I set up an
Amazon Mechanical Turk thing, right?

590
00:42:49,851 --> 00:42:52,290
To Krauss was, which I've
also done Ashley, right?

591
00:42:52,300 --> 00:42:55,790
Also had very large DSF collect
off Amazon Mechanical Turk,

592
00:42:55,791 --> 00:42:59,630
but only in a later stage the project
you understand what you really need.

593
00:42:59,950 --> 00:43:04,850
All right. Um, so as you,

594
00:43:04,851 --> 00:43:08,090
as you start working on your class
projects, maybe, maybe keep that,

595
00:43:08,150 --> 00:43:11,960
keep that in their minds now. Um,

596
00:43:19,540 --> 00:43:20,373
oh,

597
00:43:28,140 --> 00:43:33,120
so one other tip that uh,
machine learning researchers on average,

598
00:43:33,121 --> 00:43:35,030
we tend to be terrible at this.
Um,

599
00:43:35,310 --> 00:43:38,730
but like if this advice anyway is when
you're going through this process,

600
00:43:38,760 --> 00:43:42,780
get some data or design a model,
a literature search would be very helpful.

601
00:43:42,800 --> 00:43:45,840
You also see what others see what our
rooms others are using. For this problem.

602
00:43:45,870 --> 00:43:47,730
It turns out that literature
actually quite immature.

603
00:43:47,970 --> 00:43:50,190
There isn't a convergence of,
uh,

604
00:43:50,280 --> 00:43:52,620
like a well standard said
that standed out for him.

605
00:43:52,621 --> 00:43:54,750
As for trigger words detection,
then lift your shirt.

606
00:43:54,751 --> 00:43:57,220
Right now I should be boss still
making up algorithms. So if you,

607
00:43:57,270 --> 00:43:59,670
if you do the survey,
you find that to be case,

608
00:43:59,671 --> 00:44:01,460
but he didn't train the initial model.
Um,

609
00:44:01,890 --> 00:44:05,100
and in most machine learning applications
who go through this process multiple

610
00:44:05,101 --> 00:44:05,461
times.

611
00:44:05,461 --> 00:44:10,461
So one tip that I would recommend
you do is keep clear notes.

612
00:44:12,570 --> 00:44:13,403
Okay.

613
00:44:13,680 --> 00:44:15,820
Um,
on the experiments you've run,

614
00:44:18,380 --> 00:44:19,330
right?
Because

615
00:44:19,330 --> 00:44:22,460
so often be, uh, as he trade
them all though, you see, oh,

616
00:44:22,461 --> 00:44:25,940
this model where it's a great on the
American accent of speakers but not on

617
00:44:25,941 --> 00:44:29,030
British accent to speakers,
right. I was born in the UK,

618
00:44:29,031 --> 00:44:31,190
so I'm just gonna use British
accents, right? Example, if a,

619
00:44:31,191 --> 00:44:32,040
for a different part of the world,

620
00:44:32,400 --> 00:44:35,040
you have to think of different global
access descends down from the UK.

621
00:44:35,080 --> 00:44:37,950
I'm just gonna pick on
British accents, I guess. Um,

622
00:44:38,250 --> 00:44:39,930
keep clearing those on
the experiments run.

623
00:44:39,931 --> 00:44:44,100
Because what happens in every machine
learning project is after a while you have

624
00:44:44,101 --> 00:44:47,250
trades 30 models and then you and
your team members a good, oh yeah,

625
00:44:47,251 --> 00:44:49,380
we tried that idea two weeks ago,
did it work?

626
00:44:49,381 --> 00:44:52,950
And if you have clear notes from when
you actually did that work two weeks ago,

627
00:44:52,951 --> 00:44:56,150
then you can refer back route. Didn't
have to be run experiments. Oh,

628
00:44:56,640 --> 00:44:58,520
the other thing that some groups do is uh,

629
00:44:59,250 --> 00:45:03,300
have a spreadsheet that keeps track
of what's the learning rate you use,

630
00:45:03,301 --> 00:45:06,730
what's the number of head and unions
was this was this was this or, or, or,

631
00:45:06,731 --> 00:45:09,390
or cheaper than a text documents.
So that,

632
00:45:09,420 --> 00:45:13,370
which will make it easier to refer back
to it, to know something you try earlier.

633
00:45:14,270 --> 00:45:17,250
Um,
this is one piece of comedy given advice.

634
00:45:17,300 --> 00:45:20,430
And this is one of those things that
every machine learning person knows we

635
00:45:20,431 --> 00:45:24,470
should do this, but on average we're
very bad at doing this. But that,

636
00:45:24,530 --> 00:45:29,280
that you could, I don't know. Uh, but at
the Times I managed to keep good notes.

637
00:45:29,281 --> 00:45:32,790
It actually saved them all the time.
Right. To try. Remember what exact view.

638
00:45:32,791 --> 00:45:37,160
Tried two weeks ago. Okay. So, um,

639
00:45:37,250 --> 00:45:40,740
a lot of this class will be on
this process of how they get data,

640
00:45:40,800 --> 00:45:43,740
develop the chain Dev, test, the
design, the model to train the model,

641
00:45:44,470 --> 00:45:47,610
eventually test them all the innovate.
So a lot of this causes on this,

642
00:45:47,880 --> 00:45:51,560
so when they jump ahead to when you have
a good enough model and you wanted to

643
00:45:51,561 --> 00:45:56,070
deploy it. Okay. So step
six, I guess since deployment

644
00:45:58,170 --> 00:46:01,770
now, um, uh, uh,

645
00:46:02,400 --> 00:46:04,230
this is uh, oh, um,

646
00:46:04,890 --> 00:46:08,550
one of the reasons I want to step through
this example going through a concrete

647
00:46:08,551 --> 00:46:12,120
example is I find that when
you're learning about machine
learning for the first

648
00:46:12,121 --> 00:46:16,020
time is often seeing, you know, what,
what my team sends it called war stories,

649
00:46:16,060 --> 00:46:17,820
kind of stories of projects that,
that,

650
00:46:17,821 --> 00:46:21,990
that others have built before that often
provides the best learning experience.

651
00:46:21,991 --> 00:46:24,780
So I think like,
I have built speech recognition systems,

652
00:46:24,781 --> 00:46:28,350
it tells me like a year or two years and
so we need to do it. So I'm trying to,

653
00:46:28,620 --> 00:46:32,610
so rather than, you know, having you spend
two years of your life building speech,

654
00:46:32,620 --> 00:46:35,220
the sims if can summarize a war story,
right?

655
00:46:35,220 --> 00:46:36,750
To tell you what the process is like.

656
00:46:36,760 --> 00:46:39,780
I'm hoping that these concrete examples
of what building these systems,

657
00:46:39,781 --> 00:46:44,450
our life in large corporations that
that can help you a salary of learnings

658
00:46:44,460 --> 00:46:47,040
without needing to get two
years of on the job experience.

659
00:46:47,041 --> 00:46:50,460
You can just hear the
salient points. Okay. Now,

660
00:46:50,940 --> 00:46:53,580
if you're deploying a system like this,
one of the things,

661
00:46:54,180 --> 00:46:55,050
and this is actually true,

662
00:46:55,051 --> 00:46:59,100
there's actually a real phenomenon for
deploys speech systems is you have the

663
00:46:59,101 --> 00:47:04,070
audio clip, you have a neuro
network, and then, you know,

664
00:47:04,080 --> 00:47:07,190
this will all put zero one. And, um,

665
00:47:07,500 --> 00:47:11,340
the neural networks that work well,
we'll tend to be relatively large,

666
00:47:11,680 --> 00:47:15,210
relatively large model, the Laotian or
hidden Eunice, relatively high complexity.

667
00:47:15,750 --> 00:47:20,050
And, um, if you have some of the
smart speakers in your home, um,

668
00:47:20,850 --> 00:47:25,850
you recognize that a lot of them are
h devices as opposed to a purely cloud

669
00:47:26,941 --> 00:47:30,520
computation, right? So we all
know what the cloud is. Um, uh,

670
00:47:30,760 --> 00:47:33,390
and what an edge devices,
edge devices,

671
00:47:33,500 --> 00:47:36,820
the smart speaker that's in your home
or the cell phone and your wallet.

672
00:47:36,850 --> 00:47:41,350
So a edge devices are, you know, the
things that are close to the data,

673
00:47:41,351 --> 00:47:44,230
I suppose in the cloud, which is a giant
surface we have in our data centers,

674
00:47:44,620 --> 00:47:49,030
right? So, um, because of
network latency, uh, uh,

675
00:47:49,060 --> 00:47:52,270
and uh, uh, and, and,
and because of privacy,

676
00:47:52,340 --> 00:47:56,350
a lot of these computations are done
on edge devices like a smart speaker in

677
00:47:56,351 --> 00:48:00,340
your home or, uh, like a,
I guess, Hey Siri or, okay,

678
00:48:00,341 --> 00:48:03,780
Google can wake up your cell phone,
right?

679
00:48:03,910 --> 00:48:08,110
And so edge devices have much lower
computational budgets and much lower power

680
00:48:08,111 --> 00:48:09,670
budgets are limited battery life,

681
00:48:09,880 --> 00:48:13,060
much less powerful processes than
we have in our cloud data centers.

682
00:48:13,600 --> 00:48:15,940
And so it turns all that salt,

683
00:48:15,941 --> 00:48:20,380
salt serving up a very large neural
network is, it's quite difficult.

684
00:48:20,680 --> 00:48:23,740
It's very difficult for,
you know, a low power,

685
00:48:24,300 --> 00:48:29,140
inexpensive microprocessor sitting in a
smart speaker in your living room to run

686
00:48:29,141 --> 00:48:31,820
a very large neural network with a
lot of hidden units and with all the

687
00:48:31,821 --> 00:48:35,290
parameters.
And so what is often done

688
00:48:38,840 --> 00:48:42,800
is to actually
do this,

689
00:48:48,080 --> 00:48:52,370
which is, um, to input an audio clip

690
00:48:53,700 --> 00:48:56,970
and then have a much simpler algorithm,
uh,

691
00:48:57,060 --> 00:49:01,360
figure out if you know anyone who has
even talking, right? Because, uh, oh,

692
00:49:01,370 --> 00:49:05,520
so the smart speaker in my living room,
he was silence most of the day, right?

693
00:49:05,521 --> 00:49:08,340
Because usually just no one at home,
right? There's no, no, no voice.

694
00:49:08,790 --> 00:49:10,800
And then only if it hears,
you know,

695
00:49:10,801 --> 00:49:15,801
someone's talking then feed it to the big
neural network that you've trained and

696
00:49:17,101 --> 00:49:20,220
GRANDPA use a larger part budget. Um, uh,

697
00:49:20,250 --> 00:49:24,990
in order to classify zero one. Okay. Um,

698
00:49:25,020 --> 00:49:28,500
just component goes by many
different names, uh, um, in,

699
00:49:28,501 --> 00:49:30,420
in reasonably standard terminology,

700
00:49:30,421 --> 00:49:33,990
but not totally standard
terminology in the literature.

701
00:49:33,991 --> 00:49:38,520
I'm going to call this VA, um,
for voice activity detection,

702
00:49:41,310 --> 00:49:42,143
right?

703
00:49:42,210 --> 00:49:44,460
Oh, it turns out that voice
activity, it's actually,

704
00:49:44,470 --> 00:49:47,880
there's a standard component is in, in
many different speech recognition systems.

705
00:49:48,420 --> 00:49:51,210
If you are using a cell phone,
for example,

706
00:49:51,410 --> 00:49:54,750
a VAT is a component that tries
to figure that hands even talking.

707
00:49:54,751 --> 00:49:56,490
Because if it thinks no one is talking,

708
00:49:56,491 --> 00:50:00,270
then there's no need to encode the audio
and tried to transmit the audio right

709
00:50:00,820 --> 00:50:03,090
onto cause,
you know?

710
00:50:05,610 --> 00:50:08,160
Um, and so, uh,

711
00:50:09,210 --> 00:50:12,360
so the next question I want to ask you,

712
00:50:12,380 --> 00:50:17,030
and I thought this is
timely because, um, uh, uh,

713
00:50:17,160 --> 00:50:17,993
wow

714
00:50:20,400 --> 00:50:23,290
is, um, a couple of options, right?

715
00:50:23,310 --> 00:50:28,100
Option one is to build a non
machine learning VAT system,

716
00:50:28,101 --> 00:50:31,820
voice activity detection system,
which is just, you know, see,

717
00:50:32,680 --> 00:50:33,513
okay,

718
00:50:36,180 --> 00:50:38,310
if the volume of the audio,

719
00:50:38,670 --> 00:50:41,270
your spa speakers recording
has greater than epsilon.

720
00:50:41,370 --> 00:50:43,380
So the side is just to get it.

721
00:50:45,210 --> 00:50:48,070
And option two is a train
a small neuronetwork,

722
00:50:51,900 --> 00:50:55,270
um, uh, to recommend
on, on, on human speech.

723
00:50:59,480 --> 00:51:00,313
Right.

724
00:51:00,380 --> 00:51:03,110
Um, and so, uh,

725
00:51:04,880 --> 00:51:08,660
my next question to you is if
you're working on this project,

726
00:51:09,320 --> 00:51:14,180
which you pick option one or what'd you
pick options? Who? Great. Uh, as you,

727
00:51:14,181 --> 00:51:18,620
as you, as you move to what, oh,
sorry. And I think, um, a small,

728
00:51:18,630 --> 00:51:19,463
newer network,

729
00:51:20,080 --> 00:51:24,020
a small network or in some cases I've
seen people use this small support vector

730
00:51:24,021 --> 00:51:25,850
machine as well. For those.
He didn't know what that is.

731
00:51:26,210 --> 00:51:28,700
A small wall that it can be wrong
with a low computational budget.

732
00:51:28,701 --> 00:51:31,550
This is a much simpler problem to
detect that someone is talking then to

733
00:51:31,551 --> 00:51:33,770
recognize the word they said.
So you could actually do this.

734
00:51:34,220 --> 00:51:37,180
You don't know what reasonable
accuracy was. Small, newer network.

735
00:51:37,210 --> 00:51:40,970
But if you actually work on
this project, uh, for CSU 30,

736
00:51:41,840 --> 00:51:46,250
which would you try for us? So Michelle,
can we go onto the next question? Yes.

737
00:51:46,610 --> 00:51:49,980
Okay, cool. Figure out how
to do that. Sure. Yeah.

738
00:51:50,280 --> 00:51:51,950
We can let them start answering I guess.

739
00:51:51,951 --> 00:51:53,510
And then why you figured
out the projection.

740
00:51:56,880 --> 00:52:00,310
Cool.
I'm just keep unlocking it periodically.

741
00:52:03,720 --> 00:52:07,190
Are People able to vote?
No. The no votes yet.

742
00:52:14,600 --> 00:52:17,590
Well,
I see.

743
00:52:19,010 --> 00:52:20,200
I guess you write so much code,

744
00:52:20,201 --> 00:52:22,670
you have a shortcut to go to your
coding environment or your laptop

745
00:52:23,970 --> 00:52:26,300
on the whole.
Great.

746
00:52:32,100 --> 00:52:34,860
All right, cool. Great. Thank you.

747
00:52:48,810 --> 00:52:51,200
Hello. All right. Yvonne,

748
00:52:51,270 --> 00:52:55,010
him quickly and now
they're like 20 seconds,

749
00:52:55,011 --> 00:52:56,710
if that's enough time to give you answers.

750
00:53:08,910 --> 00:53:12,140
Oh,
cool.

751
00:53:21,550 --> 00:53:23,830
All right, cool. Um,

752
00:53:28,000 --> 00:53:28,833
okay,

753
00:53:29,390 --> 00:53:34,350
that's fascinating. There's a lot
of disagreement in this Hos, um, uh,

754
00:53:34,550 --> 00:53:37,040
people want to say why,
why would you choose option one?

755
00:53:37,041 --> 00:53:38,980
Why would you choose often to, and, and I,

756
00:53:38,981 --> 00:53:42,970
I have a very strong point of view
on what I would do. Right. Uh, but,

757
00:53:42,980 --> 00:53:46,490
but I'm curious why, uh,
why option one and why two

758
00:53:46,750 --> 00:53:47,583
go ahead.

759
00:53:51,910 --> 00:53:55,860
Easy to debug issues
either.

760
00:53:55,861 --> 00:53:57,450
I was option one officer,

761
00:54:00,310 --> 00:54:01,143
actually simple

762
00:54:06,370 --> 00:54:09,030
options. Who's not the harder

763
00:54:11,660 --> 00:54:15,660
dog in your house and carts and pros,
they're activated.

764
00:54:17,760 --> 00:54:22,760
You can probably out already like simplify
the problem that knowing if a noise,

765
00:54:25,771 --> 00:54:30,060
I mean activates the machine.
But that's why I picked up

766
00:54:40,110 --> 00:54:44,920
there may be people have like

767
00:54:47,670 --> 00:54:48,503
three

768
00:54:56,130 --> 00:55:00,680
options,
noisy place

769
00:55:04,770 --> 00:55:05,603
and

770
00:55:10,110 --> 00:55:12,180
I think it will pick up the background

771
00:55:15,600 --> 00:55:20,590
noise. You have a friend
who's a train station,

772
00:55:20,830 --> 00:55:23,110
right? So option one, we'll
pick up a lot. The pain

773
00:55:25,790 --> 00:55:29,260
has to be running constantly,
very cheap.

774
00:55:29,280 --> 00:55:31,690
So it seems like option
one is better because

775
00:55:35,700 --> 00:55:38,210
no power,
no country.

776
00:55:39,250 --> 00:55:43,940
So let me show you some
of the pros and cons. Uh,

777
00:55:44,440 --> 00:55:48,840
so I think there are pros
and cons of option one.

778
00:55:48,841 --> 00:55:49,674
Option two is why

779
00:55:50,280 --> 00:55:51,770
so many volts,

780
00:55:51,800 --> 00:55:56,550
but both options I proceed with
choose option one. Um, but,

781
00:55:56,570 --> 00:56:00,800
but let me just, let's just discuss the
pros and cons. Right? I think that um, uh,

782
00:56:00,980 --> 00:56:05,240
option one first is just a
few lines of code is yes,

783
00:56:05,241 --> 00:56:09,140
maybe option two isn't that complicated,
but option one is even simpler.

784
00:56:09,620 --> 00:56:11,920
And I think that, um, uh,

785
00:56:12,620 --> 00:56:15,020
maybe I would say if I hadn't
worked on this problem before,

786
00:56:15,021 --> 00:56:16,820
I would choose option one.
Uh,

787
00:56:16,970 --> 00:56:20,110
but since I have experienced as feet
recognition eventually I know you need

788
00:56:20,120 --> 00:56:23,590
option too. But that's because I, because
I've worked on this problem before,

789
00:56:23,920 --> 00:56:26,170
but there's your first time working
on the speech application problem.

790
00:56:26,500 --> 00:56:30,490
I would encourage you on
average to try to really simple,

791
00:56:30,550 --> 00:56:34,840
quick and dirty solutions and
go ahead. And, um, so let's see,

792
00:56:34,870 --> 00:56:36,910
how long would it take to implement this?
Right?

793
00:56:36,940 --> 00:56:40,880
I would say like 10 minutes or
five minutes. I Dunno. Right?

794
00:56:41,410 --> 00:56:42,790
Hollywood thinking to implement that.

795
00:56:43,870 --> 00:56:47,410
Oh hell for all of us one
day or I don't really know.

796
00:56:47,411 --> 00:56:52,300
Actually right now, let me just write one
day and then I'm not quite sure. Great.

797
00:56:52,930 --> 00:56:56,080
But if I'm often went
to BMC in 10 minutes,

798
00:56:56,110 --> 00:57:01,110
then I will encourage you to do that and
go ahead and put the smart speaker in

799
00:57:01,840 --> 00:57:04,300
your home or in your
potential users homes.

800
00:57:04,660 --> 00:57:09,660
And only when you find out that the dog
barking is a problem or the train on

801
00:57:09,701 --> 00:57:11,800
their airways, seizure,
no, whatever is a problem,

802
00:57:12,040 --> 00:57:16,360
then go back and invest more in the
fixing it. Right. And in fact, um,

803
00:57:16,630 --> 00:57:20,110
it's true that maybe it's annoying if the
dog barking and keeps on waking up the

804
00:57:20,111 --> 00:57:23,800
system. But maybe that's okay
because if the launch neural network,

805
00:57:23,830 --> 00:57:27,550
then screens are all the dog barking then
the overall performance systems as you

806
00:57:27,551 --> 00:57:29,750
just fine and, uh, and,

807
00:57:29,850 --> 00:57:34,430
and you now have a much simpler
system. Right? But, uh, but,

808
00:57:34,431 --> 00:57:39,431
but it turns out that the reason you might
need to go to option to eventually is

809
00:57:39,461 --> 00:57:43,180
because they're awesome homes in
noisy environments. Uh, you know,

810
00:57:43,181 --> 00:57:44,680
this constant background noise.

811
00:57:44,681 --> 00:57:47,860
And so that will keep the large neural
nets were running a little bit too

812
00:57:47,861 --> 00:57:51,610
frequently. So, so if you have a
large engineering budget, you know,

813
00:57:51,611 --> 00:57:54,670
so some of the smart speaker teams are
like hundreds of engineers working on it

814
00:57:55,020 --> 00:57:57,460
so they have hundreds of
injuries and work on it. Totally.

815
00:57:58,180 --> 00:57:59,770
Option two will perform better.

816
00:58:00,220 --> 00:58:04,030
But if you're strapped startup team is
scrappy startup team lift fee of you

817
00:58:04,031 --> 00:58:08,290
working on a class project, you
know, the evidence that you need,

818
00:58:08,291 --> 00:58:10,930
that level of complexity is not that high.

819
00:58:11,350 --> 00:58:15,850
And I would really do that first and use
that to gather evidence that you really

820
00:58:16,060 --> 00:58:19,180
should make the investment to build a
more complex system before actually making

821
00:58:19,181 --> 00:58:22,660
the investments of days off.
And eventually,

822
00:58:22,870 --> 00:58:25,240
I think this is one day to pull
your first photo. All right.

823
00:58:25,241 --> 00:58:28,800
And then eventually you'll be,
I'll be more complicated. Um,

824
00:58:29,350 --> 00:58:33,420
it turns out that, um, did other reason,

825
00:58:34,480 --> 00:58:39,180
um, the other huge advantage of
the simple method is the following.

826
00:58:39,670 --> 00:58:44,020
Um,
and this is one of the faculty.

827
00:58:44,021 --> 00:58:44,854
This is one of the,

828
00:58:45,220 --> 00:58:48,420
there's actually one of the big problems
and big weaknesses of machine learning

829
00:58:48,430 --> 00:58:53,260
algorithms and deep learning algorithms,
which is what happens is, uh, um,

830
00:58:53,800 --> 00:58:57,430
when you build a system and you should
ship a product, the data will change,

831
00:58:57,580 --> 00:59:01,510
right? And so, um, I'm going to simplify
the example a little bit, but you know,

832
00:59:01,530 --> 00:59:05,770
I know Stanford is very cosmopolitan.
There's Powells is very hospital. See,

833
00:59:05,800 --> 00:59:09,130
we collect data in this region. You get
access from people all over the world,

834
00:59:09,160 --> 00:59:12,530
right? Because, because that's
Stanford, that's Palo Alto. But,

835
00:59:12,540 --> 00:59:16,630
but just to simplify these apple
orbit, um, let's say that you train

836
00:59:18,880 --> 00:59:23,570
on a u s accents, right? Uh,

837
00:59:23,600 --> 00:59:27,980
but you know, for some reason, uh,
uh, when you're ship a product,

838
00:59:28,280 --> 00:59:32,240
maybe it sells really well in
the UK and you start getting data

839
00:59:35,360 --> 00:59:36,193
okay.

840
00:59:37,970 --> 00:59:41,990
With a UK or with British accents.
All right,

841
00:59:42,680 --> 00:59:47,680
so one of the biggest problems you
face in practical deployment of machine

842
00:59:47,751 --> 00:59:52,580
learning systems is that the day to train
on is not going to be the day you need

843
00:59:52,581 --> 00:59:54,930
to perform well on. Um, and,

844
00:59:55,000 --> 00:59:58,340
and I'm gonna share with you some
practical ideas for how to solve this,

845
00:59:58,341 --> 01:00:02,450
but this is one of those
practical realities and
practical weaknesses is machine

846
01:00:02,451 --> 01:00:07,130
learning that is actually not talked
about much in academia. Uh, uh,

847
01:00:07,160 --> 01:00:11,690
because it turns out that the data says
we have in academia are not set up well

848
01:00:12,050 --> 01:00:14,470
for researchers to study and
published papers on this.

849
01:00:14,480 --> 01:00:16,640
I think we could sell a new machine there,
any benchmarks in the future,

850
01:00:17,030 --> 01:00:20,090
but this is one of those problems as
it's actually kind of underappreciated in

851
01:00:20,091 --> 01:00:24,290
academic literature.
But that is a problem facing many,

852
01:00:24,291 --> 01:00:28,190
many practical deployments are
machine learning algorithms. Um,

853
01:00:28,820 --> 01:00:31,940
and uh,
and so more general,

854
01:00:31,980 --> 01:00:36,450
the problem is one of data
changing, right? And, uh,

855
01:00:36,451 --> 01:00:38,450
you might have new houses of users

856
01:00:40,010 --> 01:00:44,140
with new accidents or you
might train a lot on, uh,

857
01:00:44,360 --> 01:00:48,460
the maybe get data from even
Stanford users and maybe Stanford.

858
01:00:48,461 --> 01:00:49,460
It's not too noisy.

859
01:00:49,490 --> 01:00:52,880
All staff are there certain types of
cars in the states where you ship it to

860
01:00:52,881 --> 01:00:56,420
another city, another country.
There's much more noisy, uh, you know,

861
01:00:56,421 --> 01:00:57,710
different background noise.

862
01:01:01,990 --> 01:01:02,823
Okay.

863
01:01:02,940 --> 01:01:03,650
Right?

864
01:01:03,650 --> 01:01:08,630
Or you start manufacturing the smart
speaker and to lower the costs of the

865
01:01:08,631 --> 01:01:10,520
speaker,
they swap it out.

866
01:01:10,550 --> 01:01:14,090
They swap out the high end microphones
that you use from your laptop to collect

867
01:01:14,091 --> 01:01:16,760
the data for lower and microphone.

868
01:01:19,050 --> 01:01:22,010
It's a very common thing done in, yeah,
well done in manufacturing, right?

869
01:01:22,011 --> 01:01:26,150
If you could use a cheaper microphone,
why not? Right. And often to a human eras,

870
01:01:26,180 --> 01:01:28,910
the sound sounds just fine
on the cheaper microphone.

871
01:01:28,940 --> 01:01:33,440
But if you change your learning algorithm
using your, you know, I guess, yeah,

872
01:01:33,441 --> 01:01:36,190
well I use a Mac,
but the Mac is a pretty decent microphone.

873
01:01:36,191 --> 01:01:39,590
So if you train the data using all
collateral Mac and then eventually as a

874
01:01:39,591 --> 01:01:42,410
different microphone,
it may not generalize well.

875
01:01:42,890 --> 01:01:45,890
So one of the challenges of,
um,

876
01:01:46,460 --> 01:01:51,410
machine learning is that you often develop
a system on one dataset and then when

877
01:01:51,411 --> 01:01:54,980
you ship a proud out, something about
the world changes, uh, and, and,

878
01:01:54,990 --> 01:01:59,660
and your system needs to perform on a
very different type of data than what

879
01:01:59,661 --> 01:02:03,620
you've had training. Um, and so,

880
01:02:10,010 --> 01:02:10,843
um,

881
01:02:10,850 --> 01:02:15,260
and so what will happened is
after you deployed a model,

882
01:02:16,000 --> 01:02:20,430
uh, the world may change and you're
often end up going back to get more data.

883
01:02:20,690 --> 01:02:25,080
We designed the model, right?
And, and I guess, sorry, and
this is, this is, uh, uh,

884
01:02:25,140 --> 01:02:27,610
the maintenance of the
machine learning model. Um,

885
01:02:27,670 --> 01:02:32,400
I want to give some of those
apples a web search, right?

886
01:02:32,970 --> 01:02:36,180
This happens all the time and multiple
of us search engines, which is, uh,

887
01:02:36,200 --> 01:02:40,470
you train a neural network or you train
the system to give relevant web search

888
01:02:40,471 --> 01:02:43,860
results, but then something about the
world changes. You are, for example,

889
01:02:43,890 --> 01:02:44,730
there's a major political,

890
01:02:44,770 --> 01:02:48,780
then some new person is elected president
of some foreign country or does a

891
01:02:48,781 --> 01:02:52,860
major scandal or just the Internet
changes, right? Or there's a,

892
01:02:53,210 --> 01:02:57,280
actually what happens in China is a new
word getting invented all the time in,

893
01:02:57,281 --> 01:03:00,650
in China, the China. So is
that by of I Google and Baidu,

894
01:03:00,660 --> 01:03:04,110
but the Chinese languages a more
fluid than the English language.

895
01:03:04,111 --> 01:03:07,920
And so new worst he invented all the time.
And so the language changes.

896
01:03:07,921 --> 01:03:12,670
And so whether it be a trade just isn't
working as well as it used to. Right.

897
01:03:12,900 --> 01:03:17,670
Um, or, or maybe a different company,
southern shuts off, you know,

898
01:03:17,760 --> 01:03:21,270
their entire website to your search
index because we don't want you, uh,

899
01:03:21,480 --> 01:03:22,590
indexing their website.

900
01:03:22,591 --> 01:03:27,450
And so internet changes and what have
you had done doesn't work anymore. Um,

901
01:03:27,750 --> 01:03:30,850
or um, uh, self driving.

902
01:03:32,790 --> 01:03:33,360
Uh,

903
01:03:33,360 --> 01:03:36,700
it turns out that you build a self driving
car in California and then you tried

904
01:03:36,701 --> 01:03:40,620
to deploy these vehicles
in Texas. Um, you know,

905
01:03:40,621 --> 01:03:44,170
it turns out traffic lights in Texas look
very different than traffic lights in,

906
01:03:44,171 --> 01:03:48,770
in, in California. So, um, although
I trained on California, Texas on,

907
01:03:49,000 --> 01:03:53,760
so a newer network trained to recognize
a California traffic ones actually

908
01:03:53,761 --> 01:03:56,610
doesn't work very well on
Texas traffic leads. Right? Oh,

909
01:03:56,940 --> 01:03:58,560
I'm trying to remember
which way round than us.

910
01:03:58,561 --> 01:04:02,640
But I think California and Texas has
a different distribution of horizontal

911
01:04:02,641 --> 01:04:05,990
versus vertical traffic lights, for
example. It's actually a human's,

912
01:04:05,991 --> 01:04:07,550
they'll notice if you
go, oh yeah, red, yellow,

913
01:04:07,560 --> 01:04:10,140
green for the learning algorithm
doesn't actually generalize that well.

914
01:04:10,141 --> 01:04:13,110
If you go to a different location
and go to a foreign country, again,

915
01:04:13,380 --> 01:04:17,120
traffic lights, signage, the
lean markers or change, um,

916
01:04:17,670 --> 01:04:22,520
or I guess, well one example I was
working on earlier this week, right? Uh,

917
01:04:22,710 --> 01:04:24,750
manufacturing, right? Learning Ai,

918
01:04:25,080 --> 01:04:28,740
working on inspection of
parts in factories. Um,

919
01:04:29,160 --> 01:04:34,160
and so if you are doing visual inspection
in the factory and the factory starts

920
01:04:35,101 --> 01:04:38,280
making a new component, you know, they
were making this model cell phone,

921
01:04:38,310 --> 01:04:41,580
but cell phones turned over quickly.
And so, but in a few months later,

922
01:04:41,581 --> 01:04:44,610
they're making a different type of cell
phone or something weird happens in

923
01:04:44,611 --> 01:04:47,850
manufacturing process. So the lighting
changes as new type of defect.

924
01:04:48,180 --> 01:04:52,020
So the world changes.
And um,

925
01:04:53,040 --> 01:04:55,430
so I'm knocking again,

926
01:04:58,100 --> 01:05:03,100
what I like to do is I'm actually
revisit the previous question in light of

927
01:05:05,131 --> 01:05:10,080
this, uh, the world changes
phenomenon, right? Which is,

928
01:05:10,081 --> 01:05:13,570
let's say you've collected a lot of data.
We an American accent of speakers. Um,

929
01:05:13,850 --> 01:05:18,310
and then, you know, we shut the
product in the UK and then, um,

930
01:05:21,610 --> 01:05:25,030
and then for some reason you find that
you've all these British accents speakers,

931
01:05:25,420 --> 01:05:27,340
right? Trying to use your spot. Speaker.

932
01:05:28,030 --> 01:05:30,940
So between these two algorithms
and our machine learning approach,

933
01:05:30,941 --> 01:05:33,160
which is a set the threshold
versus train the neural network,

934
01:05:33,670 --> 01:05:37,660
which system do you think will be more
robust for Vat? Boys activity detection.

935
01:05:37,740 --> 01:05:38,573
Okay.

936
01:05:56,370 --> 01:05:59,300
All right. I'm particularly
another, I'll know 40 seconds.

937
01:06:28,340 --> 01:06:29,173
All

938
01:06:37,550 --> 01:06:38,383
right.

939
01:06:40,470 --> 01:06:44,740
Yeah. Interesting to people
when they comment, well,

940
01:06:44,910 --> 01:06:48,000
more people voted for this one.
Let me explain why.

941
01:06:49,380 --> 01:06:50,560
No,
go ahead.

942
01:07:02,520 --> 01:07:03,990
If we want to detect

943
01:07:08,060 --> 01:07:11,410
that and instead,

944
01:07:13,100 --> 01:07:14,710
right.
So say for the VAT,

945
01:07:17,020 --> 01:07:21,070
if you just measure the volume that it
doesn't really depend on on the accent,

946
01:07:21,410 --> 01:07:24,220
so non ml might be more robust.
Anyone else?

947
01:07:28,990 --> 01:07:29,823
All right.
So,

948
01:07:30,050 --> 01:07:31,700
okay. Again, you show you, I thought,

949
01:07:31,701 --> 01:07:36,380
so it turns out that if you
train a small neuronetwork, uh,

950
01:07:36,740 --> 01:07:40,190
to um, uh, you know, American
accent, his speech, uh,

951
01:07:40,520 --> 01:07:45,350
there's a bigger chance that your neural
network, because it's so clever, right?

952
01:07:45,380 --> 01:07:49,820
They'll learn to recognize American
speech and have a harder time generalizing

953
01:07:49,821 --> 01:07:53,540
to British accented speech.
Does that make sense?

954
01:07:53,630 --> 01:07:58,540
And so one of the things that I've
seen a lot of teams are, whereas,

955
01:07:58,600 --> 01:08:03,410
so, so one way the non GMO thing could
fail to generalize would be a British

956
01:08:03,411 --> 01:08:05,180
speakers are systematically,
you know,

957
01:08:05,300 --> 01:08:08,000
allow there are softer than the
American speakers. Right? So, you know,

958
01:08:08,001 --> 01:08:11,010
I don't know, I don't have
Americans stereotypically allowed,

959
01:08:11,050 --> 01:08:13,030
there are less loud than
British, but, but you know,

960
01:08:13,480 --> 01:08:14,990
but the American and British speakers,

961
01:08:15,310 --> 01:08:18,620
one country just as loud a
voice as a softer voices,

962
01:08:18,621 --> 01:08:23,240
then maybe the threshold you set won't
generalize. Well, but that seems unlikely.

963
01:08:23,310 --> 01:08:26,030
I don't see that being realistic thing.
But um,

964
01:08:26,270 --> 01:08:30,210
but they have a training on your own
network, a lot of parameters then, uh,

965
01:08:30,230 --> 01:08:32,430
is more likely that the neuro network,

966
01:08:32,431 --> 01:08:37,400
we're pickup on some idiosyncrasy of
American accents to decide the 70s even

967
01:08:37,401 --> 01:08:39,680
speaking. Um, and does,

968
01:08:39,920 --> 01:08:43,850
maybe that's robust who generalizing
to a British accent it speech. Right.

969
01:08:44,130 --> 01:08:47,480
And another way to think about the, or
imagine to take an even further example,

970
01:08:47,720 --> 01:08:51,800
imagine that you're using Va d for a
totally different language than they then

971
01:08:52,100 --> 01:08:55,820
English, right? Where um, take
a different language, you know,

972
01:08:56,150 --> 01:09:00,560
Chinese or Hindi or Spanish or something
where the sounds of really different.

973
01:09:00,590 --> 01:09:04,100
If you create a VAT system
to detect, you know, English,

974
01:09:04,280 --> 01:09:08,750
it may not at all work for detecting
Spanish or Chinese or French or,

975
01:09:09,080 --> 01:09:13,880
or some other language. Um, and so
if you think of British accents as a,

976
01:09:13,881 --> 01:09:16,790
as somewhere on the spectrum,
not the foreign language by any means,

977
01:09:16,791 --> 01:09:21,791
but just more different than I think the
[inaudible] system is more likely to be

978
01:09:23,210 --> 01:09:26,730
robust. Right. And so one of
the lessons that too many that,

979
01:09:26,731 --> 01:09:30,440
that a lot of machine learning teams
during the hard way it is, uh, um,

980
01:09:30,680 --> 01:09:33,110
if you don't need to use a
learning algorithm or something,

981
01:09:33,140 --> 01:09:34,880
if you can hand called the simple rule,

982
01:09:34,881 --> 01:09:39,730
like if volume greater than
0.01 do this or that, uh,

983
01:09:39,780 --> 01:09:42,080
those rules are,
can be more robust.

984
01:09:42,980 --> 01:09:45,390
And the one of the reasons
we use only our rooms,

985
01:09:45,391 --> 01:09:47,230
it's where we can't hang caught something,
right?

986
01:09:47,231 --> 01:09:50,720
I don't know how the handcuffs something
to detect the cat or to the car on the

987
01:09:50,721 --> 01:09:54,260
road or detective the person.
So use learning Alvin's with those.

988
01:09:54,290 --> 01:09:57,920
But if there was actually a hand called
the rule that actually does pretty well,

989
01:09:58,250 --> 01:10:03,250
you find that it is more robust to shifts
into data and what often generalized

990
01:10:03,701 --> 01:10:07,580
better. Oh, and uh, if any
of you take, uh, uh, we,

991
01:10:07,590 --> 01:10:11,870
we talk a bit about this a little bit in
cs two, three, nine a I think CSC here,

992
01:10:11,871 --> 01:10:14,210
90 talks about this as well.
But this book,

993
01:10:14,211 --> 01:10:17,870
the observation is backed up by very
rigorous learning theory and the learning

994
01:10:17,871 --> 01:10:21,810
theory is basically that the
fewer parameters you have, uh,

995
01:10:21,860 --> 01:10:23,530
if you still do well in your training set,

996
01:10:23,540 --> 01:10:26,720
if you kind of model with
very few parameters that
does well and your training

997
01:10:26,721 --> 01:10:28,910
set your generalized
better, right? So there's,

998
01:10:28,911 --> 01:10:32,240
there's very rigorous machine learning
theory that basically says that.

999
01:10:32,570 --> 01:10:34,640
And in the case of the non
machine learning approach,

1000
01:10:34,641 --> 01:10:38,510
there's maybe one parameter
which is what's the threshold
for epsilon and that's

1001
01:10:38,511 --> 01:10:40,010
worthwhile enough for your training set.

1002
01:10:40,040 --> 01:10:44,870
Then your odds of a generalizing even
when the data changes is much higher

1003
01:10:45,780 --> 01:10:49,010
right.
Now,

1004
01:10:51,650 --> 01:10:54,110
the last question,
um,

1005
01:10:55,790 --> 01:10:59,360
I want to post with
discussion today is, um, when,

1006
01:10:59,361 --> 01:11:03,080
when discussing deployments, uh, oh
and so one of the lessons deployments,

1007
01:11:03,350 --> 01:11:04,930
that's just the way the world works,
you know,

1008
01:11:04,990 --> 01:11:06,440
the machine learning system and deploy it,

1009
01:11:06,500 --> 01:11:10,380
the world will usually change and you
often end up collecting data and having an

1010
01:11:10,381 --> 01:11:12,230
elevator and maybe improve the model,
right?

1011
01:11:12,231 --> 01:11:15,330
And then they'll fix them off
a previous speakers happen. Um,

1012
01:11:16,020 --> 01:11:20,550
so we talked about edge deployments
as well as cloud deployments,

1013
01:11:21,060 --> 01:11:22,850
right? And so, um,

1014
01:11:22,920 --> 01:11:27,750
ignoring issues of user privacy and
latency, which is super important.

1015
01:11:27,751 --> 01:11:29,460
But for the purposes, the
question, let's, let's,

1016
01:11:29,640 --> 01:11:33,630
let's put aside issues of user
privacy and network latency. Um,

1017
01:11:33,690 --> 01:11:36,900
if you need to maintain them all though,
sorry. Maintenance means updating them.

1018
01:11:36,901 --> 01:11:41,700
Haldol, right? Even as
the world changes, um, oh,

1019
01:11:41,701 --> 01:11:43,290
sorry. I miss uh, miss tabby should be,

1020
01:11:43,300 --> 01:11:47,040
does he cow or h deployment make
maintenance easier if not earth. Right?

1021
01:11:47,730 --> 01:11:51,630
Why don't you, why don't you just enter
a one word answer and, and, and why,

1022
01:11:52,830 --> 01:11:55,950
right? And so managing that is this, when
the world changes, something changes.

1023
01:11:55,951 --> 01:11:58,230
So you need to update the
learning model to take advantage,

1024
01:11:58,560 --> 01:12:02,790
take care of this British accent. So
which type of deployment makes it easier?

1025
01:12:08,700 --> 01:12:11,300
Mystic Lake. Yeah. Another two
minutes. Send to your answers.

1026
01:12:51,750 --> 01:12:52,583
Yeah.

1027
01:13:14,440 --> 01:13:16,580
All right.
Another like 50 seconds.

1028
01:13:58,170 --> 01:14:00,980
All right, cool. See
what people are routes.

1029
01:14:07,960 --> 01:14:10,840
Wow. Cool, great. All right.
Almost everyone in the St Cloud,

1030
01:14:16,600 --> 01:14:21,500
most people are saying
cloud. All right, cool.

1031
01:14:21,590 --> 01:14:23,450
Great.
And then just to summarize,

1032
01:14:23,451 --> 01:14:26,360
I think they're actually two
reasons why most people send,

1033
01:14:26,370 --> 01:14:28,580
it's easier to push updates.
That's part of it.

1034
01:14:28,880 --> 01:14:31,790
I think the other positive is that
if all the data that is at the edge,

1035
01:14:31,840 --> 01:14:35,120
if all the data is processed yo and it
uses home and none of it comes to cloud,

1036
01:14:35,420 --> 01:14:38,460
then even if you have all these
unhappy British accents to users,

1037
01:14:38,490 --> 01:14:41,180
you may not give him the fine Dell. Right?
You're saying the company headquarters,

1038
01:14:41,510 --> 01:14:43,610
you have all these users and
this theory is these, you know,

1039
01:14:43,611 --> 01:14:46,760
seem to be not using your device maybe
cause they're unsatisfied with it.

1040
01:14:47,150 --> 01:14:50,660
But if the data isn't coming to your
servers in the cloud that you may not even

1041
01:14:50,661 --> 01:14:54,390
find out about it. Now there are
serious issues about user privacy, uh,

1042
01:14:54,590 --> 01:14:57,770
as we as security, right? So, so, so
please, if you have a brother product,

1043
01:14:57,771 --> 01:15:01,070
please be respectful of that.
Uh, and then take, take,

1044
01:15:01,130 --> 01:15:05,330
take care of that in very thoughtful
and respectful way of users. But, um,

1045
01:15:06,690 --> 01:15:10,710
uh, for us, um, so this
is the cloud, right?

1046
01:15:11,360 --> 01:15:15,440
If you have a lot of edge devices
and all the data is processed there,

1047
01:15:16,130 --> 01:15:20,180
you won't even know what your users
are doing and they're happy, unhappy,

1048
01:15:20,240 --> 01:15:21,080
you just don't know.

1049
01:15:21,470 --> 01:15:25,820
But if some of the data in the stream to
your servers of the cow and if the user

1050
01:15:25,821 --> 01:15:29,200
privacy with really easy
as good user consent,

1051
01:15:29,240 --> 01:15:33,320
tell people what you're doing with the
data. But if you take care of that in a,

1052
01:15:33,321 --> 01:15:36,140
in a, in a, in a, in
these moe and sound way,

1053
01:15:36,500 --> 01:15:40,160
if you're able to examine some of the
data that you can at least figure out that

1054
01:15:40,400 --> 01:15:44,090
gee, looks like analyzing the data.
There are these people, if this accident,

1055
01:15:44,091 --> 01:15:47,940
this background noise that um,
uh, is, uh, giving it back.

1056
01:15:48,000 --> 01:15:52,160
Other days be rinse and you can also
maybe have the data so you can gather the

1057
01:15:52,161 --> 01:15:56,510
data from the edge to feed
back to your model, right? So,

1058
01:15:56,720 --> 01:15:59,690
so that's you detect if
something's gone wrong. Um,

1059
01:16:00,060 --> 01:16:04,690
unless you have the data to retrain
the model to solve the British accent

1060
01:16:04,700 --> 01:16:08,430
problems are going to retrain the model
a lot British accent to speech. Um,

1061
01:16:08,690 --> 01:16:12,050
and if find it, he lets you push them
all back home, right? So of course,

1062
01:16:12,320 --> 01:16:16,820
and that's, you detect what's going
on too. It gives you data for training

1063
01:16:20,060 --> 01:16:23,790
and see, unless you're more easily
push the model back up, pushing,

1064
01:16:24,000 --> 01:16:27,500
pushing the new model to
production to deployments. Okay.

1065
01:16:28,230 --> 01:16:33,110
And does this also why, uh, even if your
compensation needs to run on the edge,

1066
01:16:33,140 --> 01:16:37,460
if you could in a way respectful of user
privacy in this transparent about how

1067
01:16:37,461 --> 01:16:38,294
you use data.

1068
01:16:38,570 --> 01:16:42,850
If you can get even a small sample
of data or have a few volunteer uses,

1069
01:16:42,851 --> 01:16:44,600
sent you some dates back to the cloud,

1070
01:16:44,930 --> 01:16:48,350
that will greatly increase your ability
to detect if something Kahn wrong as

1071
01:16:48,351 --> 01:16:51,080
well as maybe give you some
data to retrain the model. Uh,

1072
01:16:51,140 --> 01:16:54,260
so even if you can hold you too.
So the push updates, right. This,

1073
01:16:54,261 --> 01:16:59,060
this will just will help.
Creepy. Okay. Um, all right.

1074
01:16:59,120 --> 01:17:02,950
So finally the one last
comment, I think, uh, one, one,

1075
01:17:02,960 --> 01:17:06,110
one last challenge is a lot of
the machine learning systems,

1076
01:17:06,111 --> 01:17:07,670
you're not done at deployment.

1077
01:17:07,671 --> 01:17:12,290
There's a constant ongoing maintenance
process and I think one of the processes,

1078
01:17:12,380 --> 01:17:14,830
uh, you know, AI teams are
getting better at as well.

1079
01:17:14,831 --> 01:17:18,830
I set up Qa to make sure that we update
the model. You don't break something.

1080
01:17:18,831 --> 01:17:21,770
So then Qa and large company
is quality assurance process.

1081
01:17:21,770 --> 01:17:25,340
It's kind of testing this and I think
the way you test machine or the arb rooms

1082
01:17:25,341 --> 01:17:27,140
is different than the way you test,
which are,

1083
01:17:27,141 --> 01:17:29,870
there is no software because
the performance of machine
learning algorithms is

1084
01:17:29,871 --> 01:17:33,920
often measured the status call way, right?
So it doesn't work. And it doesn't work.

1085
01:17:34,130 --> 01:17:37,090
It either works or doesn't work.
Instead that works, you know,

1086
01:17:37,130 --> 01:17:38,750
95% of the time or something.

1087
01:17:38,780 --> 01:17:42,320
And so a lot of companies are
evolving the Qa processes,

1088
01:17:42,350 --> 01:17:44,720
they have this type of statistical
testing to make sure that you don't,

1089
01:17:44,721 --> 01:17:48,080
you changed them all, then you do a
push update. It's the works, you know,

1090
01:17:48,110 --> 01:17:51,200
95 or 99% of the time or
something rather than so, so,

1091
01:17:51,530 --> 01:17:55,580
so putting in place new Qa
test processes as well. Okay.

1092
01:17:56,630 --> 01:17:58,490
All right.
I hope that was helpful.

1093
01:17:58,491 --> 01:18:01,760
Stepping through what the full arc of a
machine learning project will look like,

1094
01:18:01,790 --> 01:18:02,860
uh, uh, will,

1095
01:18:02,861 --> 01:18:05,810
will they did this course or any course
suite as well as in a later lecture

1096
01:18:05,811 --> 01:18:06,351
there are present,

1097
01:18:06,351 --> 01:18:09,650
we'll keep talking about machine learning
strategy and how they make decisions.

1098
01:18:10,400 --> 01:18:11,780
So let's break for today.

