1
00:00:05,340 --> 00:00:09,480
Okay, let's get started guys. So
welcome to lecture number four.

2
00:00:09,710 --> 00:00:10,543
Um,

3
00:00:11,220 --> 00:00:16,220
today we will go over to topics that are
not discussed in the Coursera videos.

4
00:00:17,370 --> 00:00:21,720
Uh, you've been learning c
two m one and c two M two,

5
00:00:21,870 --> 00:00:26,400
if I'm not mistaking. So you've
learned about, uh, what, uh,

6
00:00:26,401 --> 00:00:30,480
an initialization is, how to tune
your own networks, what tests,

7
00:00:30,481 --> 00:00:35,481
validation and train sets are today
we're going to go a little further or you

8
00:00:35,971 --> 00:00:39,900
should have the background to
understand 80% of this lecture.

9
00:00:39,901 --> 00:00:43,670
There's going to be 20% that I want you
to look back after you've seen the batch

10
00:00:43,671 --> 00:00:45,720
norm videos for those of
you who haven't seen them.

11
00:00:46,290 --> 00:00:51,270
So we feed the lecture in two parts and
I put back the attendance code at the,

12
00:00:51,271 --> 00:00:53,780
at the very end of the
lecture. So don't worry. Uh,

13
00:00:53,820 --> 00:00:56,850
one topic is attacking a neural networks,

14
00:00:57,190 --> 00:01:00,270
a weed adversarial examples.
Uh,

15
00:01:00,300 --> 00:01:03,300
the second one is generative
adversarial networks.

16
00:01:03,301 --> 00:01:07,860
And although these two topics have
a common word which is adversarial,

17
00:01:08,160 --> 00:01:09,600
there are two separate topics.

18
00:01:09,780 --> 00:01:12,930
You will understand why it's
called adversarial in both cases.

19
00:01:13,440 --> 00:01:16,530
So let's get started with
adversarial examples.

20
00:01:16,560 --> 00:01:19,790
And in 2013,
uh,

21
00:01:19,900 --> 00:01:24,210
Christians negative and his team have
a published a paper called intriguing

22
00:01:24,211 --> 00:01:28,800
properties of neural networks.
What they noticed is that neural networks,

23
00:01:28,890 --> 00:01:31,230
neural networks have kind of a blind spot,

24
00:01:31,720 --> 00:01:34,290
a spot for which several machine learning,

25
00:01:34,680 --> 00:01:38,190
including the state of the art
ones that you will learn about. Uh,

26
00:01:38,220 --> 00:01:41,540
VGG 1619 inception,
uh,

27
00:01:41,550 --> 00:01:45,960
networks and raise residual networks
are vulnerable to something called

28
00:01:45,961 --> 00:01:50,250
adversarial examples. These adversarial
examples, you're going to learn what it is

29
00:01:52,050 --> 00:01:53,310
in three parts first,

30
00:01:53,340 --> 00:01:58,020
by explaining how these examples in
the context of images can attack your

31
00:01:58,021 --> 00:02:00,360
network in their blind spots and,

32
00:02:00,800 --> 00:02:04,620
and make the network classify these
images as something totally wrong,

33
00:02:06,030 --> 00:02:11,030
how to defend against these
type of examples and why
our networks vulnerable to

34
00:02:12,301 --> 00:02:13,260
these type of examples.

35
00:02:13,260 --> 00:02:16,470
This is a little bit more theoretical and
we're going to go over it on the board.

36
00:02:17,430 --> 00:02:21,570
The, the papers that are listed on the
bottom are the two big papers that,

37
00:02:21,571 --> 00:02:23,760
that started this field of research.

38
00:02:23,970 --> 00:02:28,320
So I would advise you to go and read
them because we have only one hour and a

39
00:02:28,321 --> 00:02:33,080
half to go over two big topics.
Um, in, in deep learning and, uh,

40
00:02:33,120 --> 00:02:37,290
we will not have the time to go
into details of everything. Okay.

41
00:02:37,320 --> 00:02:41,610
So let's set up the goal. The goal is
like, is that given a pretrained network?

42
00:02:41,730 --> 00:02:46,410
So a network trained on image net on a
thousand classes, millions of images,

43
00:02:46,910 --> 00:02:47,490
uh,

44
00:02:47,490 --> 00:02:52,490
find an input image that is not only
Guana so it doesn't look like the Animal

45
00:02:53,370 --> 00:02:56,790
Iguana. Uh, but we be classified
by the network as an Iguanas.

46
00:02:57,630 --> 00:03:02,230
We call this an adversarial
example if we manage to find it.

47
00:03:03,100 --> 00:03:04,300
Okay. Yeah. One question

48
00:03:05,960 --> 00:03:07,000
I love this came,

49
00:03:09,720 --> 00:03:14,060
so 28, 48, 89 opinion
writing down on the board.

50
00:03:15,140 --> 00:03:15,973
Thank you.

51
00:03:17,520 --> 00:03:18,353
Yes.

52
00:03:19,870 --> 00:03:20,703
Can you guys see,

53
00:03:25,350 --> 00:03:26,183
okay,
that's more

54
00:03:27,670 --> 00:03:31,900
so we have a network pretrained on
your major. It's a very good network.

55
00:03:32,710 --> 00:03:37,150
Uh, what I want is too full it by giving
it to an image that doesn't look like

56
00:03:37,151 --> 00:03:38,760
anyone.
Nobody's classified as anyone.

57
00:03:39,310 --> 00:03:41,230
So if they give it to
cattle image to start with,

58
00:03:41,890 --> 00:03:46,240
the network is obviously going to give
me a vector of probabilities that has the

59
00:03:46,241 --> 00:03:49,210
maximum probability for cats
because it's a good network.

60
00:03:49,840 --> 00:03:53,560
And you can guess what's the output layer
of this network is probably a soft Max.

61
00:03:53,950 --> 00:03:55,450
It's a classification network.

62
00:03:55,780 --> 00:04:00,780
Now what I want is to find then you make
x that is going to be classified as an,

63
00:04:01,310 --> 00:04:04,540
you'd go on up. I do network. Okay.

64
00:04:05,740 --> 00:04:10,600
Does the, the setting make sense
to everyone? Okay. Now as usual,

65
00:04:10,700 --> 00:04:12,560
uh, this, this,

66
00:04:12,680 --> 00:04:16,150
this might remind you of what we've seen
together about neural style transfer.

67
00:04:16,210 --> 00:04:21,210
You remember our generation thing where
we want it to generate an image based on

68
00:04:21,251 --> 00:04:24,400
the content of the first image
and the style of another image.

69
00:04:24,780 --> 00:04:26,170
And in that problem,

70
00:04:26,171 --> 00:04:30,810
the main difference with classic
supervised learning was that we fixed

71
00:04:30,820 --> 00:04:34,290
department chairs of the network,
which was also pretrained and we backed,

72
00:04:34,291 --> 00:04:38,350
propagate the error of the loss all the
way back to the input image to update

73
00:04:38,351 --> 00:04:43,351
the pixels so that it looks
like the content of your
content image and the style

74
00:04:44,711 --> 00:04:48,190
of the style image. The first thing we
did is that we rephrased the problem.

75
00:04:48,750 --> 00:04:52,430
We tried to to, to phrase
what exactly we want. So what,

76
00:04:52,431 --> 00:04:56,980
what would you say is a sentence that
defines or loss function? Let's see.

77
00:05:07,050 --> 00:05:07,883
Yes.

78
00:05:15,830 --> 00:05:16,663
Yep.

79
00:05:17,460 --> 00:05:21,750
And you mentioned that provides minimum
cost and he made that provides minimum

80
00:05:21,751 --> 00:05:22,900
cost. Okay. What's the cost?

81
00:05:22,910 --> 00:05:27,210
You were talking about Costco did the
difference between the expected decline

82
00:05:27,211 --> 00:05:29,700
and not expect to be expected?

83
00:05:29,820 --> 00:05:32,570
Gwen, I know unexpected you go out.
What? What do you mean exactly by that?

84
00:05:32,840 --> 00:05:36,710
So we're sort of going back
to the training stage and
we were trying to train it

85
00:05:36,800 --> 00:05:40,850
on it. We really want him to think
that this is the tap water. Yeah.

86
00:05:42,090 --> 00:05:46,680
Okay. So you want a decent age to minimize
a certain loss function and the loss

87
00:05:46,681 --> 00:05:51,210
function would be the distance metric
between the output you're looking for and

88
00:05:51,211 --> 00:05:54,150
yeah, always put you want. Okay. Yeah.

89
00:05:54,180 --> 00:05:58,340
So I would say we want to find x,
the image that that why hat of x,

90
00:05:58,370 --> 00:06:02,810
which is the result of the
forward propagation of x
into network is equal to y e

91
00:06:02,811 --> 00:06:07,460
Guana, which is a one hodge vector.
We do one I did position of ego on.

92
00:06:08,330 --> 00:06:11,450
Does that make sense? So now based
on that we defined our loss function,

93
00:06:12,200 --> 00:06:14,300
which is can be an l two loss,

94
00:06:14,301 --> 00:06:18,380
can be an l one loss can be
across entropy in practice, uh,

95
00:06:18,440 --> 00:06:20,450
this one works better.

96
00:06:20,960 --> 00:06:24,080
So you see that minimizing
these lost function.

97
00:06:24,110 --> 00:06:29,110
We lead our image x to be outfitted
as any Guana by the network.

98
00:06:30,230 --> 00:06:31,063
That makes sense.

99
00:06:31,130 --> 00:06:34,460
And then the process is very similar
to neuro side transfer where we will

100
00:06:34,461 --> 00:06:36,110
optimize the image iteratively.

101
00:06:36,620 --> 00:06:40,470
So we will start with x when
we forward propagates it.

102
00:06:41,150 --> 00:06:43,040
The loss function that we just defined.

103
00:06:43,940 --> 00:06:45,890
And remember we're not
training the network, right?

104
00:06:46,220 --> 00:06:50,030
We just take the derivative of the loss
function all the way back to the inputs

105
00:06:50,330 --> 00:06:55,330
and update the input using a graduate
decent algorithm until we get something

106
00:06:56,391 --> 00:07:01,250
that's east classified as anyone.
Yeah. Any question on that?

107
00:07:01,460 --> 00:07:03,190
But this doesn't
necessarily mean that the,

108
00:07:05,140 --> 00:07:05,973
okay,

109
00:07:06,220 --> 00:07:10,210
so you mentioned that it doesn't
guarantee that x is Lou going to look like

110
00:07:10,211 --> 00:07:11,044
something.

111
00:07:11,050 --> 00:07:15,430
The only thing is guaranteeing is that
this x will be classified as any Guana if

112
00:07:15,431 --> 00:07:18,280
we train property where we,
we've talked about that.

113
00:07:18,281 --> 00:07:20,470
Now another question in
the back I thought yeah,

114
00:07:23,120 --> 00:07:25,150
or logistic regression.

115
00:07:26,200 --> 00:07:29,920
Oh yeah. It could be binary croissant.
It couldn't decrease entre B. Yeah.

116
00:07:29,950 --> 00:07:34,170
So in this case, not binary cross entropy
because we have a uh, a vector of,

117
00:07:34,430 --> 00:07:39,280
of any classes. I mean it could
have been Chris Entrepreneur. Okay.

118
00:07:39,640 --> 00:07:43,990
So yeah, that's true. We are, we
going to need that, the forge image x.

119
00:07:44,050 --> 00:07:46,660
This one is going to look like anyone

120
00:07:48,280 --> 00:07:51,040
who thinks it's going
to look like an Iguana.

121
00:07:54,340 --> 00:07:58,330
Who thinks it's not going to look
like anyone. Okay. Mine's retail fuel.

122
00:07:58,750 --> 00:08:01,660
So can someone tell me why it's
not going to look like anyone else?

123
00:08:11,120 --> 00:08:11,953
Right.

124
00:08:14,470 --> 00:08:17,980
Okay. So you're saying the last function
is unconstrained is very unconstrained.

125
00:08:18,250 --> 00:08:21,370
So we didn't put any constraints on what
the image should look like. That's true.

126
00:08:21,670 --> 00:08:24,250
Actually, the answer to this question
is, it depends. We don't know.

127
00:08:24,251 --> 00:08:27,580
Maybe it looks like any one or maybe
does it, but in terms of property, it is,

128
00:08:27,610 --> 00:08:31,270
it's high chance that it doesn't look
like anyone. So there isn't, is here,

129
00:08:31,630 --> 00:08:34,180
let's say this is our
space of input images.

130
00:08:34,750 --> 00:08:38,290
And the interesting thing is that
even if as human on a daily basis,

131
00:08:38,291 --> 00:08:41,500
we deal with images of the real world.
So like,

132
00:08:42,070 --> 00:08:46,390
I mean if you look at the TV, uh,
that is totally buggy. You see pixels,

133
00:08:46,391 --> 00:08:47,171
random pixels.

134
00:08:47,171 --> 00:08:50,740
But in other contexts we usually
see real word distribution images.

135
00:08:51,490 --> 00:08:54,730
My networking deterministic,
it means it takes any image,

136
00:08:55,050 --> 00:08:58,410
any input image that fits the,
the first layer would, would,

137
00:08:58,790 --> 00:09:00,700
would produce an output,
right?

138
00:09:00,960 --> 00:09:05,790
So this is the whole space of input
images. That's the network can see,

139
00:09:06,790 --> 00:09:11,700
um, this is the space of real
images. It's a lot smaller.

140
00:09:12,270 --> 00:09:14,790
Can someone tell me what's
the size of the, the,

141
00:09:14,890 --> 00:09:17,370
the space of possible
input images for network?

142
00:09:19,300 --> 00:09:24,220
Sorry, infinite. It's
not influenced a lot,

143
00:09:24,240 --> 00:09:28,440
but
uh,

144
00:09:29,400 --> 00:09:32,420
yeah, there is an idea here.
Someone said the same thing.

145
00:09:32,421 --> 00:09:36,980
We just number of possible
permutations. Yeah, that's true.

146
00:09:37,820 --> 00:09:39,710
So more precisely,
you would start with

147
00:09:39,840 --> 00:09:44,820
how many pixels values are there.
There are 255,

148
00:09:45,150 --> 00:09:48,780
256 pixel values.
And then what's the size of an image?

149
00:09:49,050 --> 00:09:50,940
Let's say 64 by 64 by three.

150
00:09:51,420 --> 00:09:55,740
And your results would give you
256 so you fixed the first pixel,

151
00:09:56,160 --> 00:10:00,390
206 is 56 possible value.
Then the second one can be anything else.

152
00:10:00,391 --> 00:10:03,270
Then the third one can be anything else
and you end up with a very big number.

153
00:10:03,450 --> 00:10:07,800
So this is a huge number and the
space of real images is here. Now,

154
00:10:07,950 --> 00:10:12,630
if we had to plot to the space of off
images classified as a need Guana it would

155
00:10:12,631 --> 00:10:14,610
be something like that.
Right?

156
00:10:15,060 --> 00:10:19,440
And you see that there's a small overlap
between the space of real images and

157
00:10:19,441 --> 00:10:22,350
the space of images. Classic for
advice as a new Guana by doing network.

158
00:10:22,920 --> 00:10:26,940
And this is where we probably are nuts.

159
00:10:27,060 --> 00:10:30,180
We're probably in the green part that
is not overlapping with the red part

160
00:10:30,450 --> 00:10:34,080
because we didn't constrain
our optimization problems.
Does that make sense?

161
00:10:35,100 --> 00:10:39,210
Okay. Now we're going to constrain it
a little bit more because in practice,

162
00:10:39,211 --> 00:10:43,920
these type of attacks or not
too dangerous because as a,

163
00:10:43,921 --> 00:10:46,830
as a human, we would see that
the pictures look like garbage.

164
00:10:47,630 --> 00:10:51,090
The dangerous attack is if
the picture looks like a cat,

165
00:10:51,300 --> 00:10:54,840
but the network sees it as an
Iguanas and humans seat as a cat.

166
00:10:55,500 --> 00:10:59,190
Can someone think of uh,
of like malicious applications of that

167
00:11:02,760 --> 00:11:06,390
face recognition?
You could show a face of you.

168
00:11:06,420 --> 00:11:09,800
You could show your picture of
your face with push the networks.

169
00:11:10,150 --> 00:11:13,410
I think it's a face of someone else.
What else?

170
00:11:17,440 --> 00:11:20,940
Yeah,
breaking captures and breaking.

171
00:11:23,330 --> 00:11:26,850
Breaking as if you know what the output,
what output you want.

172
00:11:26,851 --> 00:11:27,710
You can force the networks.

173
00:11:27,880 --> 00:11:32,410
I ain't got this capture a DDS in bootcamp
shot is the output it's speaking for

174
00:11:33,430 --> 00:11:35,870
or in general?
I would say like social medias.

175
00:11:37,180 --> 00:11:41,980
If someone is managed and wants
to put a violent content online,

176
00:11:42,130 --> 00:11:45,640
there is all these companies
have algorithms to check
for these violent content.

177
00:11:46,360 --> 00:11:50,590
If people can use adversarial examples
that look still violence but are not

178
00:11:50,591 --> 00:11:53,950
detected as violence by the
algorithms using methodology.

179
00:11:54,130 --> 00:11:58,450
Nico still publish their violence
pictures. Think about self driving cars.

180
00:11:58,510 --> 00:12:00,790
A stop sign that looks like
a stop sign for everyone.

181
00:12:00,820 --> 00:12:03,820
But when the self driving car sees it,
it's not a stop sign.

182
00:12:05,140 --> 00:12:06,330
So these are my issues,

183
00:12:06,340 --> 00:12:10,570
applications of adversarial examples
and there are a lot more. Okay.

184
00:12:10,600 --> 00:12:13,630
And in fact the picture we generated
previously would look like that.

185
00:12:14,170 --> 00:12:18,850
It's nothing special. So now let's
constraint or problem a little bit more.

186
00:12:18,970 --> 00:12:22,620
We're going to say we want a
picture to look like a cat,

187
00:12:22,810 --> 00:12:26,850
but the classified as a need one.
Okay,

188
00:12:27,640 --> 00:12:29,800
so now same.
We have our neural network.

189
00:12:29,830 --> 00:12:32,560
If we give you the cat is going
to predict that it's a cat.

190
00:12:32,770 --> 00:12:36,300
What we want is still give you two
cats but predict that it's anyone.

191
00:12:38,220 --> 00:12:39,053
Okay.

192
00:12:40,450 --> 00:12:43,720
I go quickly over that because it's
very similar to what we did before.

193
00:12:43,721 --> 00:12:48,700
So I just plot. I just put back what
we had on the previous slide. Okay.

194
00:12:48,730 --> 00:12:52,360
Exactly the same thing. No.
The way we rephrase or problem,

195
00:12:52,361 --> 00:12:53,470
we'd be a little different.

196
00:12:54,190 --> 00:12:58,390
Instead of saying we want only y hat
of x equals y you wanna we have another

197
00:12:58,391 --> 00:12:59,980
constraint.
The other constraint,

198
00:13:04,590 --> 00:13:05,423
yes.

199
00:13:08,680 --> 00:13:12,540
Did feature x should be closer
to the teacher of the cats.

200
00:13:12,820 --> 00:13:15,680
So we want x equal or very close to x.

201
00:13:16,850 --> 00:13:20,840
And in terms of loss function,
what it does is that it adds another term

202
00:13:22,940 --> 00:13:26,480
which is going to decide how x should
be close to x [inaudible] if we minimize

203
00:13:26,481 --> 00:13:27,170
this loss,

204
00:13:27,170 --> 00:13:32,170
now we should have an image that looks
like a cat because of the second term and

205
00:13:32,451 --> 00:13:36,500
that is predicted as any Guana because
of the first term. Does that make sense?

206
00:13:37,250 --> 00:13:40,100
So we're just building up our last
functions and I guess you guys are very

207
00:13:40,101 --> 00:13:43,580
familiar with these type of
thought process now. Okay.

208
00:13:43,581 --> 00:13:47,060
And same process we optimize
until we hopefully get,

209
00:13:47,360 --> 00:13:52,360
okay now the question is what should
be the initial image we start with?

210
00:13:56,910 --> 00:13:59,380
Well you didn't talk about
that in your previous example.

211
00:14:03,950 --> 00:14:06,150
Yeah.
White noise.

212
00:14:06,970 --> 00:14:11,030
Yeah. Possibly white noise. Any other cat?

213
00:14:12,760 --> 00:14:13,593
Which cats?

214
00:14:20,250 --> 00:14:23,410
I Dunno. Probably the cat that we
put in the loss function. Right?

215
00:14:24,200 --> 00:14:26,380
Cause he's the closest one
to what we want to get.

216
00:14:26,680 --> 00:14:29,800
So if we want to have a fast process,
we'd better start with

217
00:14:29,900 --> 00:14:33,460
Xochi Scott, which is the one we
put you in our last function here.

218
00:14:34,390 --> 00:14:35,223
Right?

219
00:14:35,440 --> 00:14:39,040
If we put an under cat is going to be a
little longer because we have to change

220
00:14:39,041 --> 00:14:41,050
the pixel of the other
cat to look like this cat.

221
00:14:41,590 --> 00:14:44,500
That's what we told our loss function.
If we start with white noise,

222
00:14:44,530 --> 00:14:47,830
it will take even longer because we
have to change the pixels all the way so

223
00:14:47,831 --> 00:14:51,470
that it looks real and then it looks
like a cat that we defined here. So yeah,

224
00:14:51,471 --> 00:14:54,710
the best thing would be probably need
to start with the picture of the cat.

225
00:14:55,850 --> 00:14:58,370
Does that make sense? And then
move the Pixel. So that's,

226
00:14:58,550 --> 00:15:03,260
this term is also minimized.
Yup. So when you're ready,

227
00:15:04,170 --> 00:15:08,790
it seems like you have facility saying
what does human sees as a cat will just

228
00:15:08,791 --> 00:15:13,110
be like minimizing the IMSE air
to the actual cat picture. Right.

229
00:15:14,130 --> 00:15:14,963
Is that,

230
00:15:15,310 --> 00:15:18,960
I thought the RMSC era was actually a
really bad way to gauge whether or not as

231
00:15:18,980 --> 00:15:22,130
unit like sauce with images
is similar. Yeah. Did you see

232
00:15:23,090 --> 00:15:26,520
empirical, the fact that we use
that type of of loss function,

233
00:15:26,880 --> 00:15:30,780
but in practice it could have been any
distance between x and x cat and any

234
00:15:30,781 --> 00:15:34,010
distance between who I heard
Tim, why cats. Yeah, and
why you're going elsewhere.

235
00:15:35,220 --> 00:15:36,053
Yes.

236
00:15:36,740 --> 00:15:37,620
So what do you see?

237
00:15:42,300 --> 00:15:46,490
One specific care,
exactly.

238
00:15:48,190 --> 00:15:51,080
Take conscious loss function,

239
00:15:51,081 --> 00:15:54,970
but it's a bunch of cats then it's like
something that could meet him on get

240
00:15:55,010 --> 00:15:57,110
right.
Can we,

241
00:15:57,450 --> 00:16:00,170
is that is why he recommended

242
00:16:02,980 --> 00:16:07,640
a book is a big run on cat.
Um,

243
00:16:09,330 --> 00:16:09,490
sure

244
00:16:09,490 --> 00:16:10,321
about the second method.

245
00:16:10,321 --> 00:16:14,460
But just to repeat the point you mentioned
is that here we had to choose a cat.

246
00:16:15,330 --> 00:16:18,030
It means the x Scott is
actually an image of a cat.

247
00:16:18,600 --> 00:16:21,780
So what if we don't know
what the catch you look like?

248
00:16:21,781 --> 00:16:26,140
We just want random cats to come out
and be classified as a, your corner.

249
00:16:26,460 --> 00:16:30,000
We're going to see a January generating
networks after which can be used to do

250
00:16:30,001 --> 00:16:34,320
that type of stuff. But um, for
the second part of the question,

251
00:16:34,530 --> 00:16:39,300
I'm not sure what the optimization
process will look like. Okay,

252
00:16:39,450 --> 00:16:42,390
let's move on.
So yeah,

253
00:16:42,420 --> 00:16:46,140
it's probably a good idea to start with
the image that we specified in the loss

254
00:16:46,141 --> 00:16:49,440
function.
Okay.

255
00:16:49,500 --> 00:16:51,450
And so then we have an image of a cat.

256
00:16:51,451 --> 00:16:56,430
That's origin origins was classified as
92% cuts and we modified a few pixels.

257
00:16:56,460 --> 00:16:59,220
So you can see that this
image looks a little blurry.

258
00:17:00,480 --> 00:17:05,460
So by doing these modifications, the
network, we think it's a need Guana. Okay.

259
00:17:06,060 --> 00:17:09,720
And sometimes these quantification can
be very slight and we can even not be

260
00:17:09,721 --> 00:17:12,480
able to notice it.
Sounds good.

261
00:17:13,740 --> 00:17:18,540
Now let's add something
else to this, uh, to this,

262
00:17:18,541 --> 00:17:21,990
uh, to this draft. We add the third set,

263
00:17:22,020 --> 00:17:24,930
which is a space of images
that look real to human.

264
00:17:25,950 --> 00:17:29,730
So that's interesting because the space
of images that look real to human is

265
00:17:29,731 --> 00:17:32,610
actually bigger.
The space then the space of real images.

266
00:17:32,970 --> 00:17:37,530
And an example is this one.
This is probably an image
that looks real to human,

267
00:17:37,920 --> 00:17:41,440
but it's not an image that we could see
in the daily life because of the slight

268
00:17:41,441 --> 00:17:43,290
excel changes.
Okay?

269
00:17:43,530 --> 00:17:47,220
So these are the space of
dangerous adversarial examples.

270
00:17:47,550 --> 00:17:50,220
They look real to human when
they're not actually real.

271
00:17:50,730 --> 00:17:54,240
They might be used to fully model.
Okay,

272
00:17:57,340 --> 00:18:00,700
now let's see a video by cracking it all.

273
00:18:00,920 --> 00:18:04,780
A real word example of
a adversarial examples.

274
00:18:05,980 --> 00:18:10,230
So for those who cannot
read, they're taking, uh,

275
00:18:10,640 --> 00:18:12,520
a camera which we just classify,

276
00:18:12,550 --> 00:18:16,420
which has a classifier and the
classifier classifies the first part.

277
00:18:16,421 --> 00:18:19,900
That's the library. And the second
image that is the same as a prison.

278
00:18:22,580 --> 00:18:24,800
So the second image has
slightly different pixels,

279
00:18:24,801 --> 00:18:27,440
but it's hard to see for human.
Same here.

280
00:18:27,830 --> 00:18:32,830
So the classifier on the phone
classifies the first image

281
00:18:35,670 --> 00:18:40,670
as a washer with 52% accuracy continence
and the second one as a doormat.

282
00:18:43,450 --> 00:18:44,283
Yeah,

283
00:18:46,560 --> 00:18:49,800
this is a, a small example of
what can, what can be done.

284
00:18:51,900 --> 00:18:55,590
Okay, now let's go. We've seen how to
generate these adversarial examples.

285
00:18:55,591 --> 00:18:59,090
It's an optimization
process. We will see, uh,

286
00:18:59,490 --> 00:19:03,360
what are the type of attacks that we can
lead and what our defenses against this

287
00:19:03,361 --> 00:19:07,050
adversarial examples.
So we would usually, uh,

288
00:19:07,110 --> 00:19:09,360
speed the t the attacks into two parts,

289
00:19:09,570 --> 00:19:13,500
non targeted attacks
and targeted at ducks.

290
00:19:14,430 --> 00:19:18,390
So non targeted attacks means
that's we just want outputs.

291
00:19:18,420 --> 00:19:21,630
We just want to find an
adversarial example. Then
he's going to fool the model.

292
00:19:22,050 --> 00:19:26,910
While targeted attack is we want to
force this you an example to be output to

293
00:19:26,911 --> 00:19:28,740
output a specific class that we chose.

294
00:19:29,610 --> 00:19:34,140
These are two different type of
attacks that are widely discussed in,

295
00:19:34,141 --> 00:19:36,810
in the research knowledge of the attacker.

296
00:19:36,811 --> 00:19:39,390
Is something very important for
those of you who did some Crypto,

297
00:19:39,600 --> 00:19:42,780
you know that we talk about white
box and tax, blackbox and tax.

298
00:19:43,230 --> 00:19:47,840
So one interesting thing
is that black in touch,

299
00:19:47,930 --> 00:19:50,280
a white box and Tuck is when
you have access to a network.

300
00:19:50,281 --> 00:19:54,690
So we have our image and pretend and free
trade network, we have fully access to,

301
00:19:54,930 --> 00:19:57,300
to all the parameters and and uh,
the gradients.

302
00:19:57,780 --> 00:20:01,470
So it's probably an easier
attack, right? We can,

303
00:20:01,620 --> 00:20:06,030
we can back propagate all the way back
to the image and update the image like we

304
00:20:06,031 --> 00:20:10,710
did a blackbox attack is when the model
is probably encrypted or something like

305
00:20:10,711 --> 00:20:13,050
that so that we don't have
access to inspire matters,

306
00:20:13,080 --> 00:20:15,720
activations and architecture.

307
00:20:16,170 --> 00:20:21,170
So new question is how do we attach
in blackbox attack if we cannot back

308
00:20:21,530 --> 00:20:26,040
propagates because we don't have
access to the layers. Any ideas?

309
00:20:27,090 --> 00:20:31,080
Yeah. Numerical, numerical
grade. Yeah. So you know,

310
00:20:31,350 --> 00:20:34,380
you will tweak the image a little bit and
you will see how we change changes the

311
00:20:34,381 --> 00:20:37,090
loss.
Looking at these you can,

312
00:20:37,130 --> 00:20:40,230
you can do have an estimate of
the new miracle ingredients.

313
00:20:40,920 --> 00:20:42,600
Even if the model is a black box model,

314
00:20:43,020 --> 00:20:46,930
this assumes that you can query
the model, right? You can query it.

315
00:20:47,560 --> 00:20:51,100
What if you cannot even query to model
or you can query it one time will leads

316
00:20:51,101 --> 00:20:53,920
to send the adversarial example.
How would you do that?

317
00:20:55,400 --> 00:20:56,920
So this becomes more complicated.

318
00:21:08,720 --> 00:21:13,550
So there is a very complex property of
this adversarial examples is is that

319
00:21:13,551 --> 00:21:14,930
they're highly transferable.

320
00:21:16,370 --> 00:21:21,080
It means I haven't model here
that is a 90 Mil classifier.

321
00:21:21,860 --> 00:21:25,610
Okay. I don't have access to
it. I cannot even query it.

322
00:21:26,570 --> 00:21:27,710
I still want to fill it.

323
00:21:28,280 --> 00:21:31,940
What I'm going to do is that I'm going
to build my own animal classifier,

324
00:21:32,300 --> 00:21:34,430
forge and adversarial example on it.

325
00:21:34,730 --> 00:21:38,090
It's highly likely that it's going to
be an adversarial example for the other

326
00:21:38,091 --> 00:21:42,500
one as well. So this is
called transferability and
it's still a research topic.

327
00:21:42,920 --> 00:21:47,600
Okay. We're trying to understand
why this happens and uh, also,

328
00:21:47,660 --> 00:21:52,640
uh, how to defend against that. You
know, maybe a defense against that is to,

329
00:21:52,940 --> 00:21:56,760
is to, uh, we're going to see it after,
I'm not going to say no, sorry. Uh,

330
00:21:57,020 --> 00:21:57,980
does that make sense?
Or No,

331
00:21:58,040 --> 00:22:02,420
this transferability probably it's because
two animal classifier is look at the

332
00:22:02,421 --> 00:22:07,070
same features in images, right? And
maybe these pixels that are playing,

333
00:22:07,100 --> 00:22:10,460
we're playing with our changing also
the output of the other network.

334
00:22:12,050 --> 00:22:13,880
Let's go over some kind of defenses.

335
00:22:15,260 --> 00:22:20,260
So one solution to defend against these
adversarial networks is to create a safe

336
00:22:20,540 --> 00:22:25,430
safety net. The safety net is what
is, uh, a net that like a firewall.

337
00:22:25,460 --> 00:22:27,110
You would put it before your network.

338
00:22:27,800 --> 00:22:31,280
Every image that comes in
will be classified as fake,

339
00:22:31,310 --> 00:22:34,790
like forged or real by the network.

340
00:22:35,210 --> 00:22:40,130
And you only take those which
are real and are not adversarial.

341
00:22:40,550 --> 00:22:45,260
Does that make sense? So you could,
you could, you could say that's okay,

342
00:22:45,261 --> 00:22:49,460
but we can also build an adversarial
network that foods this network, right?

343
00:22:50,360 --> 00:22:53,710
Just black box or white box.
We can just create an ad.

344
00:22:54,200 --> 00:22:55,940
An example for this network.
It's true,

345
00:22:56,270 --> 00:22:58,580
but the issue is that now
we have two constraints.

346
00:22:58,581 --> 00:23:02,330
We have to fill the first one and the
second one at the same time. You know,

347
00:23:02,780 --> 00:23:04,190
maybe if you fill the first one,

348
00:23:04,700 --> 00:23:07,850
there is a chance that the second one
is going to be food. We don't know.

349
00:23:09,200 --> 00:23:13,530
It just makes it more complex. There is
no good defense at this point. Two to two,

350
00:23:13,550 --> 00:23:15,170
all type of adversarial examples.

351
00:23:15,200 --> 00:23:18,830
This is an option that people are
researching for it. So the paper is here.

352
00:23:18,831 --> 00:23:22,940
If you wanna check it out.
Can you guys think of another solution?

353
00:23:29,790 --> 00:23:30,623
Yeah.

354
00:23:39,720 --> 00:23:42,360
Train on multiple loss function.
So different networks.

355
00:23:43,670 --> 00:23:44,910
So you're talking about an

356
00:23:46,190 --> 00:23:47,000
maybe we can,

357
00:23:47,000 --> 00:23:51,230
maybe we can create five minutes works
to do our tasks and it's highly unlikely

358
00:23:51,231 --> 00:23:55,660
that the address, for example,
is going to who? The fire, uh,

359
00:23:55,670 --> 00:24:00,120
networks the same way.
Right? And other ideas

360
00:24:01,630 --> 00:24:04,940
just get very adversarial examples.

361
00:24:05,840 --> 00:24:10,190
Exactly generates adversarial
examples and train on them. Okay,

362
00:24:10,910 --> 00:24:11,743
so

363
00:24:11,880 --> 00:24:14,340
you regenerate [inaudible]
Jerry's adversarial.

364
00:24:14,341 --> 00:24:16,770
So some pixels have been
changed to full network.

365
00:24:16,950 --> 00:24:20,670
You will label it as the human sees it.
So as a cat,

366
00:24:20,970 --> 00:24:24,270
because you want the network to still
see that as a cat and you will train on

367
00:24:24,271 --> 00:24:27,330
those. The downside of that
is that it's very costly.

368
00:24:27,570 --> 00:24:30,720
We've seen that generating adversarial
examples. It's super costly.

369
00:24:31,740 --> 00:24:35,700
And also we don't know if he can
generalize to other adversarial examples.

370
00:24:35,701 --> 00:24:38,040
Maybe we going to over
feet to the ones we have.

371
00:24:38,220 --> 00:24:39,900
So it's another optimization problem.

372
00:24:40,710 --> 00:24:45,710
Now another solution is to
train on adversarial examples
at the same time as we

373
00:24:47,201 --> 00:24:50,830
train on on normal examples.
So look at these loss function,

374
00:24:51,520 --> 00:24:55,150
this loss function. The last new
is a son of two last functions.

375
00:24:55,210 --> 00:24:57,220
One is the classic loss
function we would use.

376
00:24:57,250 --> 00:25:01,330
So let's say Krista entropy in the
case of her of her classification.

377
00:25:02,050 --> 00:25:04,480
And the second one is
the same loss function,

378
00:25:04,540 --> 00:25:06,730
but we give it the
adversarial version of x.

379
00:25:08,660 --> 00:25:11,750
So what does the complexity of that
at every gradient descent step.

380
00:25:16,100 --> 00:25:16,933
Okay.

381
00:25:23,080 --> 00:25:25,280
For every iteration of
our gradient descent,

382
00:25:25,440 --> 00:25:29,850
we're going to have to iterate enough
to forge an adversarial example at every

383
00:25:29,851 --> 00:25:32,250
step, right? Because we have x,

384
00:25:32,880 --> 00:25:36,630
what we want to do is for propagate x to
the network to compute the first term,

385
00:25:37,650 --> 00:25:42,120
generate x adversarial with the
optimization process and for propagated to

386
00:25:42,121 --> 00:25:45,720
calculate the second term and then
back propagate over the weights of the

387
00:25:45,721 --> 00:25:47,880
network.
These super costly as well.

388
00:25:47,881 --> 00:25:50,910
And it's very similar to what you said
is just online and just all the time.

389
00:25:52,320 --> 00:25:53,850
Okay.
So

390
00:25:54,370 --> 00:25:58,150
what is interesting is, uh, we're
going to delve a little more,

391
00:25:58,180 --> 00:26:00,850
there's another technical logic pairing.
I just put it here.

392
00:26:00,851 --> 00:26:03,550
We're not going to talk about it. There
was a paper here, if you want to check it,

393
00:26:03,700 --> 00:26:06,260
it's another way to do
adversarial training. Uh,

394
00:26:06,261 --> 00:26:09,760
but what I would like to talk about is
more from a theoretical perspective.

395
00:26:09,761 --> 00:26:14,080
Why are neural network vulnerable too?
Adversarial examples.

396
00:26:14,590 --> 00:26:16,810
So let's, let's do some,
some work on the board.

397
00:26:19,710 --> 00:26:20,590
Yeah.
One question.

398
00:26:42,120 --> 00:26:45,840
So the thing is that just like in Crypto,
every time you come up with a defense,

399
00:26:45,841 --> 00:26:49,170
someone we come up with an attack and
it's a race between human, you know,

400
00:26:50,030 --> 00:26:53,040
so this is the same type of problems.
Security problems are,

401
00:26:54,900 --> 00:26:57,470
okay.
So let's go over some things

402
00:26:57,580 --> 00:27:02,500
interesting that he's more on the, on the
intuition side of adversarial examples.

403
00:27:03,250 --> 00:27:05,260
So let me,
let me write down something.

404
00:27:06,700 --> 00:27:11,200
So one question we ask ourselves is,
why do adverse example exists?

405
00:27:11,590 --> 00:27:15,870
What's the reason? And, uh,
young Goodfellow and, and,

406
00:27:15,940 --> 00:27:19,070
and his team have came up with
explaining, uh, with, uh, the,

407
00:27:19,180 --> 00:27:23,670
one of the seminal papers of adversity
examples where they argued that's old

408
00:27:23,671 --> 00:27:26,360
though many people in the past have,

409
00:27:26,770 --> 00:27:30,100
have I trained UTVs existence
of adverse in examples,

410
00:27:30,101 --> 00:27:34,930
too high nonlinear re nonlinearities
of neural networks and over fitting.

411
00:27:35,080 --> 00:27:37,180
So because we over feet
to a specified data set,

412
00:27:37,390 --> 00:27:41,380
we actually don't understand what
cats are. We just understanding what,

413
00:27:41,740 --> 00:27:44,030
what we've been trained on,
uh,

414
00:27:44,140 --> 00:27:47,560
they argued at is actually the
linear parts of networks that is,

415
00:27:47,780 --> 00:27:52,030
Nicole was off the existence
of adversarial examples.
So let's see why. Uh,

416
00:27:52,120 --> 00:27:55,270
and the example I'm going to,
I'm going to look at is linear regression.

417
00:27:57,610 --> 00:28:00,100
So together we've seen
logistic regression.

418
00:28:00,370 --> 00:28:02,200
Linear regression is
basically the same thing.

419
00:28:02,201 --> 00:28:06,580
We all just seem way so before to seem
like we have white hats equals w x plus B.

420
00:28:07,120 --> 00:28:08,650
So in the forward propagation

421
00:28:11,690 --> 00:28:16,640
of our network is going to
be y hats equals w x plus me,

422
00:28:18,760 --> 00:28:19,593
okay?

423
00:28:19,690 --> 00:28:24,590
And our first example is going
to be a six dimensional inputs.

424
00:28:24,970 --> 00:28:25,803
Okay?

425
00:28:32,720 --> 00:28:37,700
Okay. We have in your own here, but then
your own doesn't have any activation.

426
00:28:37,810 --> 00:28:39,110
And because we are in linear regression,

427
00:28:39,560 --> 00:28:43,760
so here what happens is simply was plus B,
okay?

428
00:28:45,110 --> 00:28:50,110
And then we get white huts and we probably
use an l one or l two loss because

429
00:28:51,261 --> 00:28:55,060
it's a regression problem to,
uh, to train these networks.

430
00:28:58,250 --> 00:29:00,770
Now let's look into the first example.

431
00:29:01,850 --> 00:29:04,820
The first example where, uh, where x,

432
00:29:05,540 --> 00:29:08,150
where are we framed our network?
So

433
00:29:09,660 --> 00:29:13,970
network has been trained, sorry,
natural. Rick has been trained

434
00:29:19,230 --> 00:29:20,580
and converged.

435
00:29:24,070 --> 00:29:24,903
Okay?

436
00:29:25,250 --> 00:29:28,760
Two W equals

437
00:29:30,350 --> 00:29:34,220
one three
minus one,

438
00:29:35,540 --> 00:29:38,690
two, three. This is w.

439
00:29:38,990 --> 00:29:42,490
And you know like because
we defined x to be

440
00:29:43,990 --> 00:29:45,190
the vector of sizes,

441
00:29:45,191 --> 00:29:49,420
a column vector w has to
be a row vector of size six

442
00:29:51,040 --> 00:29:54,550
so the natural converse to this
value of w and B equals zero.

443
00:29:56,860 --> 00:30:01,510
So now we're going to look at these
inputs. We're giving a new input

444
00:30:03,280 --> 00:30:08,280
to the network and the input is going
to be one minus one two zero three minus

445
00:30:11,051 --> 00:30:14,020
two.
Okay?

446
00:30:14,380 --> 00:30:19,380
So I'm going to four propagate this
to get white hats equals w x plus B.

447
00:30:24,760 --> 00:30:29,760
And this value is going to be one times
one minus three minus two plus zero plus

448
00:30:34,241 --> 00:30:35,920
six minus six

449
00:30:38,980 --> 00:30:43,980
if I didn't make a mistake
up up to minus three.

450
00:30:45,160 --> 00:30:49,840
Okay? Okay. And so we, we basically get

451
00:30:51,440 --> 00:30:55,360
minus four.
Okay,

452
00:30:55,430 --> 00:30:57,410
so this is the, the, the first,

453
00:30:58,700 --> 00:31:00,590
the first example that it was propagated.

454
00:31:04,850 --> 00:31:06,710
Now the question is

455
00:31:11,470 --> 00:31:14,240
how to change

456
00:31:16,760 --> 00:31:20,960
x
into x star

457
00:31:23,390 --> 00:31:24,223
search,

458
00:31:27,250 --> 00:31:30,050
why hats changes

459
00:31:31,870 --> 00:31:36,470
radically.
But

460
00:31:39,100 --> 00:31:41,160
agstar is close to x.

461
00:31:45,190 --> 00:31:47,110
So these basically your prominent
with adversarial examples.

462
00:31:47,111 --> 00:31:51,820
Can we find an example that is
very close to x but radically,

463
00:31:51,850 --> 00:31:53,980
radically changes the
output of our network?

464
00:31:55,930 --> 00:31:58,510
And we're trying to build intuition on,
on adversarial neural networks.

465
00:31:59,410 --> 00:32:02,590
So the interesting part is to,

466
00:32:02,650 --> 00:32:07,650
is to identify how we should modify
x and a d intuition comes from the

467
00:32:09,161 --> 00:32:09,994
derivative.

468
00:32:10,480 --> 00:32:15,480
If you take the derivative of
white hats with respect to x,

469
00:32:16,630 --> 00:32:19,510
you know that the
definition of this term is,

470
00:32:21,870 --> 00:32:24,190
is like correlated to the impact

471
00:32:26,050 --> 00:32:29,410
on why hat of small changes

472
00:32:33,220 --> 00:32:34,330
of X.
Okay.

473
00:32:35,390 --> 00:32:40,100
How, what's the impact of small
changes of x two on the outputs?

474
00:32:40,850 --> 00:32:41,950
And if you computed,

475
00:32:46,010 --> 00:32:49,710
when do you get
w

476
00:32:53,090 --> 00:32:56,900
everybody agrees? What's,
what's the shape of this thing?

477
00:32:59,680 --> 00:33:00,513
Yeah.

478
00:33:01,600 --> 00:33:03,640
Shape of that is the same as shape of x.

479
00:33:06,160 --> 00:33:07,780
So should be w transpose,

480
00:33:10,290 --> 00:33:12,020
remember nerve itself a scaler.

481
00:33:12,021 --> 00:33:15,580
And with respect to a vector
is the shape of the vector.

482
00:33:17,860 --> 00:33:22,460
Okay. Now it's interesting too to see
these because if we come to Texstar

483
00:33:24,140 --> 00:33:25,780
to me,
let's see,

484
00:33:25,840 --> 00:33:30,720
x plus a small purchase nation like I,

485
00:33:30,740 --> 00:33:32,560
we call it perturbation value.

486
00:33:36,290 --> 00:33:37,300
Yeah,
sorry.

487
00:33:40,270 --> 00:33:44,530
And can you see the top one
you said yes or no?

488
00:33:45,240 --> 00:33:46,073
Okay.

489
00:33:49,420 --> 00:33:54,190
So what if x star equals x
plus epsilon time w transpose,

490
00:33:54,850 --> 00:33:58,750
you know, and this epsilon I will
call it value of the situation.

491
00:34:02,400 --> 00:34:06,880
No. If we four probe, I
get x star, it means we do.

492
00:34:06,940 --> 00:34:10,360
Why hot star equals

493
00:34:11,980 --> 00:34:15,040
x star Plus B with the zero.

494
00:34:15,041 --> 00:34:20,041
At this point we're going
to get w x plus epsilon w

495
00:34:24,220 --> 00:34:26,350
times that I've been new transports

496
00:34:30,210 --> 00:34:34,950
and w times w transports,
he's a dog product, right?

497
00:34:36,150 --> 00:34:39,510
So this is the same as w square.

498
00:34:44,760 --> 00:34:45,840
So what is interesting,

499
00:34:46,680 --> 00:34:51,680
it's interesting because d does smart
parts was that this term is always going

500
00:34:52,351 --> 00:34:53,184
to be positive.

501
00:34:54,030 --> 00:34:58,980
It means we we moved a little bit x
because we can make this change little by

502
00:34:58,981 --> 00:35:00,690
changing epsilon to a small value,

503
00:35:01,590 --> 00:35:06,300
but it's going to push why hat
to a larger volume for sure.

504
00:35:06,630 --> 00:35:09,630
You know, and if I had a
minus here instead of a plus,

505
00:35:10,050 --> 00:35:13,310
it would push my heart to a smaller value.
And the,

506
00:35:13,311 --> 00:35:16,770
the interesting thing is
now if we compute x star

507
00:35:18,960 --> 00:35:23,960
to be x plus epsilon times w transpose
and we take epsilon to be a small value

508
00:35:27,121 --> 00:35:29,010
like let's say 0.2

509
00:35:31,920 --> 00:35:36,180
you can make the calculation.
What we get is,

510
00:35:36,990 --> 00:35:37,823
is this,

511
00:35:38,190 --> 00:35:43,190
so one minus one two zero three
minus two plus 0.2 times one

512
00:35:49,120 --> 00:35:50,490
0.2 times three

513
00:35:52,890 --> 00:35:57,890
mine 0.2 just 0.4 plus 0.4 and plus 0.6

514
00:36:02,230 --> 00:36:03,063
okay.

515
00:36:05,830 --> 00:36:09,970
So as you look at that, all the positive
values have been pushed on the rights

516
00:36:11,810 --> 00:36:16,780
you agree
and all the negative values.

517
00:36:17,350 --> 00:36:20,700
Oh sorry, sorry. No, that's my bad.
No, no, no, that's okay. So let,

518
00:36:20,701 --> 00:36:25,701
let's finish their calculation and I can
give the insight after 1.2 mine is 0.4

519
00:36:29,440 --> 00:36:34,440
1.8 0.4 3.4 and minus 1.4 so this is our
x star that we hope to be adversarial.

520
00:36:40,000 --> 00:36:43,900
Okay, let's come. Cute. Why
hide star to see what happens.

521
00:36:44,440 --> 00:36:48,670
It's w x star Plus d which is zero.

522
00:36:49,660 --> 00:36:54,660
So what we get when we
multiply w by x star is 1.2

523
00:37:06,830 --> 00:37:11,830
1.2 minus 1.2 minus 1.8

524
00:37:15,290 --> 00:37:20,290
plus 0.8 plus 6.8 and minus 4.2

525
00:37:28,940 --> 00:37:29,900
which I believe

526
00:37:31,640 --> 00:37:33,980
is going to give us 0.5

527
00:37:40,160 --> 00:37:40,341
okay.

528
00:37:40,341 --> 00:37:45,341
So we see a very slight change in x star
has pushed white hats from minus four

529
00:37:47,480 --> 00:37:52,250
2.5 and so a few things
we want to notice here.

530
00:37:59,060 --> 00:38:02,600
So insights on this,
on this small example.

531
00:38:02,990 --> 00:38:06,110
The first one is that um,

532
00:38:08,420 --> 00:38:11,210
if w is large,

533
00:38:16,850 --> 00:38:21,830
then x star is not similar to x,

534
00:38:23,000 --> 00:38:23,833
right?

535
00:38:24,050 --> 00:38:29,030
The larger w the less egg
stories is likely to be like X.

536
00:38:29,300 --> 00:38:33,410
And specifically if one
entry of w is very large,

537
00:38:34,010 --> 00:38:38,470
XII, the pixel to the century is going
to be very different from X. I star,

538
00:38:40,070 --> 00:38:43,030
um,
if WWE is large,

539
00:38:43,060 --> 00:38:45,130
exterior is going to be different in x.

540
00:38:45,850 --> 00:38:50,850
So what we're going to do is
that we're going to take sign,

541
00:38:53,230 --> 00:38:58,230
sign off w instead of taking w
what's the reason why we do that?

542
00:38:58,751 --> 00:39:01,930
Because the interesting part
is the sign of, of the WWE.

543
00:39:02,830 --> 00:39:06,760
It means if we play
correctly with the sign of w,

544
00:39:07,030 --> 00:39:10,270
we will always push the x,

545
00:39:11,190 --> 00:39:16,190
this term w x star in the positive
side because every entry here,

546
00:39:17,020 --> 00:39:20,980
these multiplication is going to
give us a positive number. Right?

547
00:39:23,800 --> 00:39:26,230
And the second insights is that

548
00:39:27,760 --> 00:39:32,020
as x grows in dimension,

549
00:39:38,890 --> 00:39:39,790
the impact

550
00:39:42,760 --> 00:39:45,910
off plus epsilon sign of w

551
00:39:47,760 --> 00:39:48,593
increases.

552
00:39:53,070 --> 00:39:53,903
Does that make sense?

553
00:40:01,280 --> 00:40:04,160
So the impact of sign of w and y hats

554
00:40:06,430 --> 00:40:07,263
increases.

555
00:40:09,750 --> 00:40:13,990
And so what's interesting to notice is
that we can keep epsilon as small as

556
00:40:13,991 --> 00:40:18,760
possible. It means x and x star will be
very similar. But as we grow in dimension,

557
00:40:19,210 --> 00:40:22,240
we're going to get more term
in these a lot more term.

558
00:40:22,960 --> 00:40:25,780
And the change in white hats, he's going
to grow in growing, growing, growing.

559
00:40:26,440 --> 00:40:31,440
And so the one reason why Andrew Sewell
examples exist for images is because the

560
00:40:31,631 --> 00:40:35,530
diamond chain is very high,
64 by 64 by three.

561
00:40:35,860 --> 00:40:40,860
So we can make epsilon very
small and take the sign of w we,

562
00:40:40,990 --> 00:40:45,730
we still gets why hats to be far
from the original value that it had.

563
00:40:46,350 --> 00:40:49,720
Does it make sense? Yeah. You
guys have any questions on that?

564
00:40:51,870 --> 00:40:53,980
So epsilon doesn't grow
with the dimension,

565
00:40:54,550 --> 00:40:58,120
but it's impact of this term increases.
We did dementia.

566
00:41:01,200 --> 00:41:02,033
Okay.

567
00:41:11,950 --> 00:41:14,110
I assume that

568
00:41:17,100 --> 00:41:20,950
when you really need to
sense of just how they

569
00:41:25,990 --> 00:41:30,120
lose fingers. Way Cooler and I
think was working towards was that,

570
00:41:30,180 --> 00:41:34,160
I know to include her and I think
was what into what, how'd the input

571
00:41:34,160 --> 00:41:35,390
image cat.
Yup.

572
00:41:36,090 --> 00:41:39,500
Do you that gives you a sparser

573
00:41:41,240 --> 00:41:45,650
mapping to another again.
Okay.

574
00:41:45,651 --> 00:41:49,940
So you like you try to find
adversarily decap yeah, yeah.

575
00:41:50,190 --> 00:41:52,920
I don't know if that has been done. I
don't think that has been done. You,

576
00:41:53,020 --> 00:41:55,370
you're talking about thinking one or two
a quarter and it's takes the adverse in

577
00:41:55,371 --> 00:42:00,290
example, conversation to a normal image
of a cat and then the cats. Sure. Maybe.

578
00:42:00,291 --> 00:42:00,790
Yeah.

579
00:42:00,790 --> 00:42:04,610
I don't know. Sorry. It's a
topic of research. Uh, okay.

580
00:42:04,611 --> 00:42:06,230
Let's move on because we
don't have too much time.

581
00:42:06,560 --> 00:42:11,560
So just to conclude what we're going
to count as a general way to generate

582
00:42:11,721 --> 00:42:13,610
adverse suit examples is this formula.

583
00:42:21,050 --> 00:42:21,883
Okay.

584
00:42:24,250 --> 00:42:28,180
This is going to be a fast way to
generate that verse at example.

585
00:42:28,720 --> 00:42:32,650
So this method is called
the phase fast gradients

586
00:42:35,380 --> 00:42:37,480
sign method.

587
00:42:38,990 --> 00:42:39,823
Okay?

588
00:42:39,840 --> 00:42:41,610
So basically what we're
doing is that we can,

589
00:42:41,700 --> 00:42:44,720
we're linear rising the cost function in,

590
00:42:45,080 --> 00:42:50,080
in the proximity of the parameters and
we're saying that's what's applied to

591
00:42:50,941 --> 00:42:55,620
linear networks here is going to also
apply for these general formula for deeper

592
00:42:55,621 --> 00:42:56,454
networks.

593
00:42:57,000 --> 00:43:01,710
So we're pushing the pixel images in
one direction that is going to impact

594
00:43:01,711 --> 00:43:05,820
highly the output. Okay? So
that's the intuition behind it.

595
00:43:06,150 --> 00:43:09,960
Now you might say that's okay,
we did this example on the linear network,

596
00:43:09,961 --> 00:43:14,040
but neural networks are not linear and
they're highly nonlinear. Uh, in fact,

597
00:43:14,160 --> 00:43:17,190
if you look where the research has
been going for the past few years,

598
00:43:17,340 --> 00:43:21,060
we're trying to linearize all the
behaviors of this neural networks.

599
00:43:21,390 --> 00:43:26,390
We'd read for example, or we'd does here
initialization, all that type of methods,

600
00:43:26,430 --> 00:43:28,490
even a sigmoid.
When we train on Sim,

601
00:43:28,590 --> 00:43:33,570
we do all we can to put
sigmoid interlinear regime
because we want fast training.

602
00:43:34,800 --> 00:43:39,120
Okay. And one last thing that I
mentioned for adversarial examples

603
00:43:41,990 --> 00:43:44,220
is if I have a network like this,

604
00:43:53,000 --> 00:43:58,000
so fully connected with three
dimensionally inputs up.

605
00:43:58,550 --> 00:43:59,383
Yeah.

606
00:44:00,350 --> 00:44:04,030
And then one here and then the output.

607
00:44:05,210 --> 00:44:09,200
What's interesting is computing the
chain rule on, on, on, on this neuron.

608
00:44:10,180 --> 00:44:10,660
Yeah.

609
00:44:10,660 --> 00:44:15,660
We give you that derivative of the loss
function with respect to let's say x is

610
00:44:18,611 --> 00:44:23,611
equal to the derivative of the loss
function with respect to z one one

611
00:44:27,540 --> 00:44:29,510
here times their votes evolve.

612
00:44:29,520 --> 00:44:34,520
Z one one with respect to x,

613
00:44:34,620 --> 00:44:38,220
let's say where we're going. We're going,
there's like trivia summation here.

614
00:44:39,090 --> 00:44:43,700
But anyway, uh, just let me
illustrate the point once we're,

615
00:44:43,710 --> 00:44:45,770
what we saying is that what we're,

616
00:44:45,800 --> 00:44:50,550
what we tried to do with your own networks
is to have these gradients be high

617
00:44:52,710 --> 00:44:54,690
because if these gradients is not high,

618
00:44:54,750 --> 00:44:57,060
we're not able to train the
parameters of this neuron.

619
00:44:57,870 --> 00:44:59,040
We need these gradient to be high.

620
00:44:59,550 --> 00:45:04,550
Because if you want to do the same
thing with the we w one one which is

621
00:45:05,240 --> 00:45:09,060
department, there's related to this neuron
you we'd need to go to this training.

622
00:45:10,170 --> 00:45:14,220
Correct. So we need this gradient to
be high end. If these gradient is high,

623
00:45:14,820 --> 00:45:19,110
the gradient with respect to the input
is also going to be high because you use

624
00:45:19,111 --> 00:45:22,860
the same gradient in the chain rule.
So networks that are,

625
00:45:23,490 --> 00:45:27,450
that have high gradients and that are
operating in their linear regime or even

626
00:45:27,451 --> 00:45:31,980
more vulnerable to adversarial
examples because of this observation.

627
00:45:35,460 --> 00:45:39,780
So any question on,
on adversarial examples before we move on?

628
00:45:39,781 --> 00:45:42,830
I think we don't have time and I
would like to to go over the, the,

629
00:45:42,850 --> 00:45:46,470
the gangs with you guys.
So let's move on to guns.

630
00:45:47,610 --> 00:45:49,350
I stick around to answer
your questions on that point.

631
00:45:50,250 --> 00:45:52,770
So the general question
we're asking now is a,

632
00:45:52,780 --> 00:45:57,600
do neural networks understand the
data? Uh, because we've seen that some,

633
00:45:57,990 --> 00:46:01,800
some data points look like
there would be real, uh,

634
00:46:01,920 --> 00:46:05,850
but the neural networks don't
understand it. So more generally, uh,

635
00:46:05,880 --> 00:46:10,880
can we build generated in networks that
can mimic the real world distribution of

636
00:46:11,011 --> 00:46:15,270
images, let's say. And this is what we
will call generative adversarial networks.

637
00:46:15,271 --> 00:46:16,650
We'll start by motivating it.

638
00:46:16,980 --> 00:46:20,370
And then we look at something called
the minimax game between two networks,

639
00:46:20,371 --> 00:46:24,150
a generator and a discriminator that
are going to help each other improve.

640
00:46:24,570 --> 00:46:29,410
And finally we'll see that
gans are hard to train. Um,

641
00:46:29,970 --> 00:46:34,560
we'll see some tips to train them and
finally go over some nice results and,

642
00:46:34,750 --> 00:46:38,160
uh, methods to evaluate. Gans. Okay.

643
00:46:40,230 --> 00:46:44,880
So the motivation behind generative
adversarial networks is to endo computers

644
00:46:44,881 --> 00:46:49,470
with an understanding of our world.
Okay.

645
00:46:49,800 --> 00:46:53,490
So by that we mean that we
want to collect a lot of data,

646
00:46:53,700 --> 00:46:57,360
use it to train a model that can generate
images that look like they're real,

647
00:46:57,540 --> 00:46:58,380
even if they're not.

648
00:46:58,710 --> 00:47:02,670
So a dog that has never existed
can be generated by this network.

649
00:47:03,240 --> 00:47:05,130
Um, and finally, uh,

650
00:47:05,160 --> 00:47:09,630
the number of parameters of the model
is smaller than the amount of data.

651
00:47:09,631 --> 00:47:10,830
We already talked about that.

652
00:47:11,010 --> 00:47:15,600
And this is the intuition behind why are
generating network can exist is because

653
00:47:15,960 --> 00:47:18,060
there is too much data in the world.

654
00:47:18,061 --> 00:47:22,080
Any majors counters a data for generative
network and there are not enough

655
00:47:22,110 --> 00:47:24,940
parameters to mimic this data.
You know,

656
00:47:24,960 --> 00:47:29,940
you have the network needs to understand
the salient features of data set

657
00:47:30,290 --> 00:47:33,040
because he doesn't have enough
perimeter to overfit everything.

658
00:47:34,510 --> 00:47:36,490
So let's talk about
probability distributions.

659
00:47:36,790 --> 00:47:40,450
So these are samples from real
images that have been taken.

660
00:47:40,810 --> 00:47:45,550
And if you plot these real data
distribution in a two d map, uh,

661
00:47:45,790 --> 00:47:48,400
it would look like something like that.
I made it up.

662
00:47:48,430 --> 00:47:52,430
But this is the image space similar
to what we talked about in adversarial

663
00:47:52,450 --> 00:47:53,110
networks.

664
00:47:53,110 --> 00:47:57,580
And this green shape is the
space of real world images.

665
00:47:58,390 --> 00:47:59,223
Now,

666
00:47:59,890 --> 00:48:04,390
if you train a generator and generate
some images that look like this,

667
00:48:04,510 --> 00:48:09,310
and these images come from
stack Gan, uh, from Jungler, uh,

668
00:48:09,850 --> 00:48:12,760
this distribution,
if the generator is not good,

669
00:48:12,790 --> 00:48:14,860
it's not going to match
the real word distribution.

670
00:48:15,460 --> 00:48:20,460
So our goal here is to do something so
that the red distribution matches the

671
00:48:20,921 --> 00:48:24,070
real world distribution went to
train the network so that it,

672
00:48:24,130 --> 00:48:25,660
it realizes what we want.

673
00:48:27,910 --> 00:48:32,320
So this is our generator and it's what
counts. It's what, what we want to train.

674
00:48:32,350 --> 00:48:34,210
Ultimately we want to give it,

675
00:48:34,300 --> 00:48:39,300
let's say a random number or a random
latent code of 400 diamond Shin,

676
00:48:39,700 --> 00:48:43,420
a scalar numbers and we
want it to output an image.

677
00:48:43,450 --> 00:48:47,850
But of course because it's not trained
initially is going to output a random

678
00:48:47,851 --> 00:48:50,860
image. It looks like something
like that. Random pixels.

679
00:48:51,790 --> 00:48:55,720
Now this image doesn't look very good.

680
00:48:56,230 --> 00:49:00,640
What we want is these images to look like
generated images that are very similar

681
00:49:00,641 --> 00:49:04,480
to the real world. So how are we
going to help this generator train?

682
00:49:05,080 --> 00:49:09,730
It's not like what we did in classic
supervised learning because we don't have,

683
00:49:09,860 --> 00:49:14,680
uh, we don't really have inputs and
labels. You know, there is no label.

684
00:49:14,860 --> 00:49:19,860
We could maybe give it an image of a
cat and ask it to output and other cats.

685
00:49:21,850 --> 00:49:24,940
But we want the network to be able
to output things that don't exist.

686
00:49:24,941 --> 00:49:27,180
Things that we've never seen,
right?

687
00:49:27,400 --> 00:49:31,180
So we want the network to understand what
a cat is, but not over feet to the cat.

688
00:49:31,181 --> 00:49:32,014
We give it.

689
00:49:33,730 --> 00:49:38,470
So the way we're going to do it is
through a small game between this network

690
00:49:38,471 --> 00:49:42,700
called the generator g. And another
network called the discriminator d.

691
00:49:43,990 --> 00:49:46,210
Let's,
let's look at how it works.

692
00:49:46,630 --> 00:49:49,060
We have a database of real images

693
00:49:50,770 --> 00:49:55,030
and we're going to start with
this distribution on the bottom,

694
00:49:55,031 --> 00:49:58,240
which is the real world data distribution
is a distribution of the images in

695
00:49:58,241 --> 00:50:02,920
this database. Now our generator
has this distribution initiative.

696
00:50:02,950 --> 00:50:06,910
It means the pixels that you see here
probably follow a distribution that

697
00:50:06,911 --> 00:50:07,451
doesn't match.

698
00:50:07,451 --> 00:50:12,451
The real world will define
the discriminator d and the
goal of the discriminator

699
00:50:13,390 --> 00:50:18,100
will be to detect if an
image is real or not.

700
00:50:18,760 --> 00:50:21,280
So we're going to give several
images to this Christian matters.

701
00:50:21,310 --> 00:50:25,900
Sometimes we will give it generated
images and sometimes we will give it real

702
00:50:25,901 --> 00:50:26,734
world images.

703
00:50:26,990 --> 00:50:31,990
What we want is that this discriminator
is a binary classifier that outputs one

704
00:50:34,340 --> 00:50:39,340
if the image is real and zero
if the image was generated.

705
00:50:40,430 --> 00:50:40,940
Okay,

706
00:50:40,940 --> 00:50:45,940
so let's say we give it x coming from
the generated image is going to give us

707
00:50:46,520 --> 00:50:51,520
zero because we want the discriminator
to detect that x was actually g of z.

708
00:50:55,330 --> 00:50:58,180
If Dmh came from our
database of real images,

709
00:50:58,210 --> 00:51:00,100
we want the discriminator to say one.

710
00:51:01,510 --> 00:51:04,120
So it seems like the discriminator
would be easy to train, right?

711
00:51:04,150 --> 00:51:05,740
It's just a binary classification.

712
00:51:05,741 --> 00:51:10,300
We can define a loss function that is
the binary cross entropy and the good

713
00:51:10,301 --> 00:51:13,540
thing is we can have as many
label as we want. Like it's,

714
00:51:13,541 --> 00:51:15,660
it's unsupervised but a
little bit supervised.

715
00:51:15,661 --> 00:51:19,750
You know we have this database
and we label it all as one.

716
00:51:19,990 --> 00:51:21,430
It's just these image exists,

717
00:51:21,580 --> 00:51:25,000
let's label them as one for this community
and everything that comes out of the

718
00:51:25,001 --> 00:51:28,690
generator, let's label it as zero
four discriminator. So we basically,

719
00:51:28,691 --> 00:51:30,760
data is not costly at all in this point.

720
00:51:33,100 --> 00:51:36,070
The way we will train is that we will
back propagate the gradient to the

721
00:51:36,071 --> 00:51:39,940
discriminator to train the discriminator
using a binary cross entropy.

722
00:51:40,390 --> 00:51:44,350
But what we ultimately want is to train
the generator. That's what we want.

723
00:51:44,351 --> 00:51:46,720
At the end, we were not going
to use the discriminator,

724
00:51:46,900 --> 00:51:48,130
we just want to generate images.

725
00:51:48,430 --> 00:51:51,250
So we're going to direct the
gradients to go back to the generator.

726
00:51:52,030 --> 00:51:55,150
And why does this gradient
can go back to the generator?

727
00:51:55,450 --> 00:51:59,570
The reason is that x is g of Z.

728
00:52:00,430 --> 00:52:04,420
It means we can back propagate the
gradients all the way back to the input of

729
00:52:04,421 --> 00:52:08,950
the discriminator. But this input
depends on the input of the generator.

730
00:52:08,951 --> 00:52:10,180
If the image was generated.

731
00:52:10,450 --> 00:52:13,750
So we can also back propagate and
direct degree into the generator.

732
00:52:14,290 --> 00:52:18,670
Does it make sense? There is a direct
relation between z and the loss function.

733
00:52:19,150 --> 00:52:23,200
In the case where the image was generated,
if the image was real,

734
00:52:23,680 --> 00:52:28,680
then the generator couldn't
get the gradient because x
doesn't depend on z or on

735
00:52:29,081 --> 00:52:32,680
the features and parameters
of the generator. Okay.

736
00:52:33,160 --> 00:52:37,060
So we would run an algorithm such as Adam,
um,

737
00:52:37,660 --> 00:52:39,340
simultaneously on too many batches,

738
00:52:39,341 --> 00:52:41,770
one for the true data and
from farms generated data.

739
00:52:43,960 --> 00:52:48,520
Does this scheme makes sense to
everyone? Yeah. One question.

740
00:52:54,960 --> 00:52:56,280
So there's many methods of doing your,

741
00:52:56,310 --> 00:52:58,990
your question is about mixing them
in in batches. Usually we would use,

742
00:52:59,590 --> 00:53:03,940
we would use one mini batch for the real
data and one mini batch for the fake

743
00:53:03,941 --> 00:53:07,660
data. But if in practice
you can try other things.

744
00:53:08,860 --> 00:53:13,060
So there are many methods that are
being tried to train gans properly.

745
00:53:13,360 --> 00:53:16,090
We're going to delve a little more into
the details of that when we will see the

746
00:53:16,091 --> 00:53:16,924
loss functions.

747
00:53:17,710 --> 00:53:22,710
So we hope that the property distributions
will match at the end and if it

748
00:53:23,291 --> 00:53:25,980
matches, we're going to just
take the generator and generates

749
00:53:25,980 --> 00:53:30,420
images normally should be able to
generate images that look real that looked

750
00:53:30,421 --> 00:53:34,980
like they came from his
distribution. Okay, sounds good.

751
00:53:35,730 --> 00:53:39,630
So now let's talk more about the training
procedure and try to figure out what

752
00:53:39,631 --> 00:53:42,000
the loss functions should be.
In this case,

753
00:53:44,980 --> 00:53:46,870
what should be the cost
of the discriminator?

754
00:53:51,940 --> 00:53:56,920
Assuming assuming we'd give too
many batches. One for real data.

755
00:53:56,950 --> 00:54:00,070
So real images and one for
generated data that come from Ge.

756
00:54:06,200 --> 00:54:06,830
Yes,

757
00:54:06,830 --> 00:54:09,990
the same basic last one.
Should we use for every finite classes,

758
00:54:10,260 --> 00:54:13,990
the same basic loss function we use from
binary class for buying the class fees?

759
00:54:14,490 --> 00:54:17,580
It's true. We're going to tweak it
a tiny bit, but it's the same idea.

760
00:54:18,090 --> 00:54:19,320
So this is what it can look like.

761
00:54:19,860 --> 00:54:24,180
We're going to call it j d cost function
of the discriminator. It has two terms.

762
00:54:24,960 --> 00:54:27,600
What does the first term say?
What does the second term say?

763
00:54:30,140 --> 00:54:32,600
And you can recognize the
binary cross entropy here.

764
00:54:33,780 --> 00:54:37,020
The only difference is
that we have labeled.

765
00:54:37,021 --> 00:54:41,520
That is why Rio and a label,
that is why generated in practice,

766
00:54:41,640 --> 00:54:44,730
why real and why generated or
always going to be set to values.

767
00:54:44,731 --> 00:54:49,530
We know that why generated is zero and
we know that why really is one so we can

768
00:54:49,531 --> 00:54:51,990
just remove these two terms
because they're both equals to one.

769
00:54:53,280 --> 00:54:57,450
The first term is telling us the should
correctly labeled real data as one.

770
00:54:58,280 --> 00:55:01,890
It's across entre beater,
the first term of a binary cross entropy.

771
00:55:03,150 --> 00:55:06,810
The second term is going to tell us
these should correctly labeled generated

772
00:55:06,811 --> 00:55:07,644
data zero.

773
00:55:08,280 --> 00:55:12,180
So the difference with classic or Centropy
we've seen is that this summation is

774
00:55:12,181 --> 00:55:16,680
the summation over the real minibatch
and the summation on the second cross

775
00:55:16,681 --> 00:55:20,250
entropy is a summation on generated
minibatch. Does it make sense?

776
00:55:23,040 --> 00:55:28,040
So we both want the D to
correctly identify it with
real data and also correctly

777
00:55:29,581 --> 00:55:31,920
identified fake data.
That's why we have two terms.

778
00:55:34,560 --> 00:55:35,970
Now what about the generator?

779
00:55:36,210 --> 00:55:38,610
What do you think should be the
cost function of the generator?

780
00:55:40,720 --> 00:55:41,553
Yes,

781
00:55:45,520 --> 00:55:49,380
I will run the first half
because I don't have a uh,

782
00:55:49,410 --> 00:55:53,620
a wide range of inputs coming
into the generator. Yes, exactly.

783
00:55:54,190 --> 00:55:57,940
But in your batch you will have like
a certain number of real examples,

784
00:55:58,630 --> 00:55:59,463
generate examples.

785
00:55:59,650 --> 00:56:03,710
The generated examples have no impact
on the first cross entropy and same for

786
00:56:03,711 --> 00:56:05,420
the real examples on the second question.
Trouble.

787
00:56:08,120 --> 00:56:09,050
Any other questions?

788
00:56:14,620 --> 00:56:18,790
Okay. So coming back to the cross to
the, to the cost of the generator,

789
00:56:19,750 --> 00:56:24,130
what should it be?
Just tiny bit complicated.

790
00:56:26,300 --> 00:56:28,570
Let's move. Let's move on cause
we don't have too much time.

791
00:56:29,170 --> 00:56:34,060
Your cost of the generator. Basically it
should say that Ge should try to fool.

792
00:56:34,810 --> 00:56:35,890
The goal is to

793
00:56:36,470 --> 00:56:41,000
to generate real samples and in order
to generate real samples we want to fool

794
00:56:41,001 --> 00:56:45,200
the,
if g managed to full d and d is very good,

795
00:56:45,201 --> 00:56:47,920
it means Ge is very good.
Right?

796
00:56:48,770 --> 00:56:53,770
The problem is that it's a game because
if these bad and g foods d it doesn't

797
00:56:55,281 --> 00:56:58,100
mean that Ge is good
cause g cause these bad,

798
00:56:58,130 --> 00:57:02,900
it doesn't detect very well
the real versus fake examples.
We want d to go up to,

799
00:57:02,901 --> 00:57:04,170
to be very good,
Angie,

800
00:57:04,220 --> 00:57:08,780
to go up at the same time
and TV equilibrium is reached
at a certain point where

801
00:57:09,110 --> 00:57:12,290
Di will always output
one half like random.

802
00:57:12,291 --> 00:57:16,580
Probably it is because
it cannot distinguish the
samples coming from GE versus

803
00:57:16,581 --> 00:57:21,550
the real samples. So this cost
function is basically saying uh,

804
00:57:22,120 --> 00:57:27,110
for generated images we want
Di to classify them as one.

805
00:57:28,580 --> 00:57:31,560
That's what it's saying.
We want to fool the,

806
00:57:33,650 --> 00:57:36,710
okay. Yeah. One question,

807
00:57:43,260 --> 00:57:48,250
try to see how you would implement
would be, but how would you implement,

808
00:57:51,350 --> 00:57:55,070
is there,
has there been a module trend this cause

809
00:57:58,940 --> 00:58:00,930
if you're using,
so how to implement that.

810
00:58:00,931 --> 00:58:05,320
If you're using a deep learning framework,
you've been building a graph, right?

811
00:58:05,970 --> 00:58:10,560
And at the end of your graph you've been
building your costs function that is

812
00:58:10,620 --> 00:58:12,390
very close to a binary core,
central big.

813
00:58:13,080 --> 00:58:17,090
Uh, what you were going to just do is to
define a note that is going to be minus

814
00:58:17,780 --> 00:58:22,780
your costs function of the is going every
time you're going to call the function

815
00:58:23,750 --> 00:58:28,750
j of Ge is going to run the graph
that you defined for Jfd and run a an

816
00:58:30,680 --> 00:58:33,740
opposition operation and
opposite of operation. Yeah.

817
00:58:36,930 --> 00:58:39,090
Oh,
the gradients back the same way.

818
00:58:42,360 --> 00:58:46,740
Propagate gradients back the same way.
We're not going to propagate the same way.

819
00:58:46,741 --> 00:58:50,880
We're going to turn into nine to
sign four degree for the generator.

820
00:58:51,960 --> 00:58:53,790
So you know, you, you,
you back propagate on the,

821
00:58:53,791 --> 00:58:57,690
on the on on d and when you back
propagates on g you would flip,

822
00:58:58,180 --> 00:58:59,013
you would feed the sign.

823
00:59:00,660 --> 00:59:04,200
That's all we do is the same thing we
designed flicks in terms of implementation

824
00:59:04,201 --> 00:59:07,620
is just another operation.
Okay.

825
00:59:07,621 --> 00:59:09,270
Now let's look at something interesting.

826
00:59:09,271 --> 00:59:13,530
Is that this log garrison,

827
00:59:13,710 --> 00:59:17,520
let's look at
a graph of the logo.

828
00:59:22,130 --> 00:59:23,390
So I'm going to plot

829
00:59:27,140 --> 00:59:31,160
against the [inaudible] g sorry,
d of GLC.

830
00:59:32,300 --> 00:59:33,800
So what does this mean?

831
00:59:34,070 --> 00:59:37,400
This access is the output of the,

832
00:59:37,401 --> 00:59:39,170
when given a generated example,

833
00:59:39,720 --> 00:59:42,530
GLC is going to be between zero and one

834
00:59:44,780 --> 00:59:47,630
because it's a probability.
These are binary classifier.

835
00:59:47,780 --> 00:59:50,890
We just see more eda output probably.
Uh,

836
00:59:50,960 --> 00:59:53,990
if we plot logarithm of x,

837
00:59:55,100 --> 00:59:57,560
so like these type of thing,

838
00:59:58,190 --> 01:00:03,050
this would be log of the of g of Z.

839
01:00:04,370 --> 01:00:06,780
Does it make sense?
He's a logarithm function.

840
01:00:08,390 --> 01:00:12,410
If I plot minus that minus,
that's

841
01:00:14,200 --> 01:00:18,710
so let me, let me plot minus
LogRhythm of g of doc or,

842
01:00:19,250 --> 01:00:21,200
or let me, let me do
something else. Let me plot

843
01:00:22,910 --> 01:00:27,910
logarithm of minus D of g of Z.

844
01:00:31,030 --> 01:00:35,110
Okay. This is it. Do you guys agree?

845
01:00:35,950 --> 01:00:40,950
Now what I'm going to do is that I'm going
to plot another function that is this

846
01:00:42,191 --> 01:00:47,191
one that is luxury them
of one minus D of g on Z.

847
01:00:54,390 --> 01:00:55,223
Okay,

848
01:00:55,770 --> 01:01:00,770
so the question is right now what we're
doing is that we're saying the cost

849
01:01:02,341 --> 01:01:07,341
function of the generator is Laogai
rhythm of one minus D of g of Z.

850
01:01:08,610 --> 01:01:12,720
So it looks like this, right?
It looks like this one.

851
01:01:14,430 --> 01:01:15,870
What's the issue with this one?

852
01:01:17,580 --> 01:01:21,840
What'd you think is the issue with
discuss function? Looking at it like that?

853
01:01:27,110 --> 01:01:31,060
Sorry,
can you say it louder?

854
01:01:32,290 --> 01:01:36,580
Goes to
goes to negative infinity in one.

855
01:01:37,640 --> 01:01:39,680
That's what you mean? Yeah. Yeah,

856
01:01:39,770 --> 01:01:44,720
and so do the consequence of that is that
the gradient here is going to be very

857
01:01:44,721 --> 01:01:49,520
large. The closer we go to one,
but the closer we are to zero,

858
01:01:49,521 --> 01:01:54,260
the lower is the gradient and it's
the reverse phenomenon for this life.

859
01:01:54,550 --> 01:01:58,030
Garrison,
the gradient is very high and very high.

860
01:01:58,031 --> 01:02:01,420
I'm in absolute value and very
high when we're close to zero,

861
01:02:01,930 --> 01:02:05,680
but it's very low when we go close to one.
Okay,

862
01:02:06,490 --> 01:02:09,280
so which loss function
you think would be better?

863
01:02:09,500 --> 01:02:13,060
A loss function that looks like this one
or loss function that looks like this

864
01:02:13,061 --> 01:02:13,894
one.

865
01:02:17,020 --> 01:02:18,130
To train our generator.

866
01:02:24,470 --> 01:02:27,470
The broader question is where
are we early in the training?

867
01:02:27,570 --> 01:02:29,790
Are we close to here or
all we chose to there?

868
01:02:32,180 --> 01:02:34,690
What does it mean to be
closed there? Two. One,

869
01:02:37,720 --> 01:02:38,660
you're fooling the network.

870
01:02:38,690 --> 01:02:43,580
It means that d thinks that
generated samples are real.

871
01:02:44,050 --> 01:02:47,390
You were here.
This place is the country.

872
01:02:47,950 --> 01:02:51,410
D thinks that generated samples are fake.

873
01:02:51,650 --> 01:02:56,210
It means correctly finds out
that they're fake early on.

874
01:02:56,211 --> 01:03:00,710
We're generally here
because the discriminator is
better than the generator.

875
01:03:01,070 --> 01:03:03,050
Generates your outputs
garbage at the beginning.

876
01:03:03,200 --> 01:03:06,020
And it's very easy for this community to
figure out that it's fake because this

877
01:03:06,021 --> 01:03:10,130
garbage, it looks very different from
real world data. So early on we're here.

878
01:03:10,400 --> 01:03:14,390
So which function is the best one
to two to two to be our costs?

879
01:03:16,860 --> 01:03:19,100
Yeah,
so probably this one is better.

880
01:03:20,030 --> 01:03:24,350
So we have to use a mathematical
trick to change this into that. Right?

881
01:03:25,250 --> 01:03:27,560
And the mathematical
trick is pretty standard.

882
01:03:27,920 --> 01:03:32,210
Right now we're minimizing something
that is in log of one minus x.

883
01:03:33,650 --> 01:03:38,650
We can say that doing so is the same as
maximizing something that is in log of

884
01:03:42,051 --> 01:03:45,350
x. You agree? Simple flip.

885
01:03:45,860 --> 01:03:50,860
I mean Max flip and we can also say that
it's the same as minimizing something

886
01:03:51,680 --> 01:03:56,300
in minus log of x.
Does it make sense?

887
01:03:56,810 --> 01:04:01,370
So we're going to use these mathematical
trick to convert our function that is a

888
01:04:01,371 --> 01:04:04,970
saturating costs.
We would say into a non saturating costs.

889
01:04:05,060 --> 01:04:06,380
There is going to look more like this.

890
01:04:09,200 --> 01:04:10,190
Let's see what it looks like.

891
01:04:11,030 --> 01:04:16,030
So to sum up our cost function currently
looks like that it's a saturating costs

892
01:04:17,000 --> 01:04:20,780
because early on the gradients
or small we cannot train. Gee,

893
01:04:21,890 --> 01:04:26,870
we're going to do a flip that I just
talked about on the board and convert this

894
01:04:26,900 --> 01:04:31,460
into another function that is
a non saturating cost. Okay.

895
01:04:32,250 --> 01:04:33,980
Yup.
Well actually yeah,

896
01:04:34,280 --> 01:04:39,020
so the reason it's the blue one is like
that is because I did a minus sign here

897
01:04:39,260 --> 01:04:44,000
so I'm flipping this. Okay.
And it's the same thing.

898
01:04:44,001 --> 01:04:47,910
It's just the sign of the gradient that
is willing to be different. Like that's,

899
01:04:48,500 --> 01:04:52,190
the gradients is high at the
beginning and low at the end.

900
01:04:53,210 --> 01:04:56,080
That makes sense. So we're, we're,

901
01:04:56,081 --> 01:04:59,660
we're going to do the youth this flip
and so we have a new training procedure

902
01:04:59,661 --> 01:05:03,770
and now we're a g of Dee.
Dee didn't change but Jay g changed.

903
01:05:03,890 --> 01:05:07,880
We have a minus sign here and instead
of the log of one minus d of GMC,

904
01:05:08,090 --> 01:05:12,770
we have the log of g o d of Jersey.
Does that make sense to everyone?

905
01:05:14,320 --> 01:05:18,280
Cool. And actually, so this is a fun
thing. If she checked the speaker,

906
01:05:18,281 --> 01:05:23,020
which is really cool or against creating
equality to large study of many,

907
01:05:23,021 --> 01:05:26,170
many different gangs.
It shows what people have tried.

908
01:05:26,171 --> 01:05:30,790
And you can see that people have tried
all types of loss to make gans trainable.

909
01:05:31,540 --> 01:05:33,730
So it looks,
it looks complicated here,

910
01:05:33,731 --> 01:05:38,731
but actually the MM gun is the first
one we saw together is the mini Max loss

911
01:05:38,961 --> 01:05:42,280
function. The second one is the non
saturating one that we just see.

912
01:05:42,281 --> 01:05:43,750
So you see between the first two,

913
01:05:43,990 --> 01:05:48,990
the only difference is that
on the generator we get
the log of one minus d of x

914
01:05:49,151 --> 01:05:52,490
hat becoming love La minus
log of the of excellent.

915
01:05:54,940 --> 01:05:55,773
Okay.

916
01:05:56,110 --> 01:05:59,950
Now another trick to train Ganz
is to use the fact that uh,

917
01:06:01,090 --> 01:06:02,540
a non saturating,
uh,

918
01:06:02,560 --> 01:06:07,540
to use the fact that the is
usually easier to train than Ge.

919
01:06:08,170 --> 01:06:12,910
But as the improvs,
g can improve if he doesn't improve,

920
01:06:13,030 --> 01:06:17,170
G cannot improve. So you
can see the, the, the,

921
01:06:17,380 --> 01:06:20,380
the performance of the,
as an upper bound to what GE can achieve.

922
01:06:22,450 --> 01:06:25,900
Because of that, we will usually train
the more time than we will train g.

923
01:06:27,010 --> 01:06:30,790
So we will basically
train for Nami Titration,

924
01:06:31,120 --> 01:06:36,120
K Times d one time G K Times d one time
g and so on so that the discriminator

925
01:06:36,611 --> 01:06:40,870
becomes better than the, the generator
can catch up better than can catch up.

926
01:06:41,260 --> 01:06:42,340
And so that makes sense.

927
01:06:42,610 --> 01:06:47,610
There also methods to use like different
learning race for the Ng to take this

928
01:06:47,921 --> 01:06:52,270
into account to try and foster
the discriminator. Okay.

929
01:06:52,800 --> 01:06:55,800
Uh, because we don't have too much time.
I'm going to skip the bathroom with guns.

930
01:06:55,801 --> 01:07:00,070
We were going to sit probably next week
together after you guys have seen the

931
01:07:00,130 --> 01:07:04,630
bathroom videos. Okay. It's cool.

932
01:07:05,500 --> 01:07:09,970
So just to sum up some, some tips to
train Ganz is to modify the cost function.

933
01:07:10,000 --> 01:07:12,940
We've seen one modification,
there are many more, uh,

934
01:07:12,970 --> 01:07:14,950
keeping the up to date with respect to g.

935
01:07:14,951 --> 01:07:18,880
So updating d more than you have
the g using virtual batch norm,

936
01:07:18,881 --> 01:07:20,800
which is a dairy rate of bachelor on.

937
01:07:20,890 --> 01:07:24,910
So it's a different type of batch normally
is used here and something called one

938
01:07:24,911 --> 01:07:29,911
sided labeled smoothing that I'm not
going to talk about it today because we

939
01:07:30,251 --> 01:07:34,450
don't have time. So let's see some nice
results now. And that's the funniest part.

940
01:07:35,310 --> 01:07:40,100
Um, so some of you I've worked
with word embeddings and you,

941
01:07:40,101 --> 01:07:41,560
you might know that's a word.

942
01:07:41,561 --> 01:07:44,080
Embeddings are vectors that can
encode the meaning of the word.

943
01:07:44,590 --> 01:07:48,760
And you can compute operations
sometimes on this, on this words.

944
01:07:48,970 --> 01:07:52,180
So if you take, uh, if
you take king minus queen,

945
01:07:52,181 --> 01:07:55,990
it should be equal to mine minus
woman operations like that.

946
01:07:56,080 --> 01:07:59,830
That's happened in the space of encoding.
So here's the thing,

947
01:08:00,310 --> 01:08:04,870
you can use a generator to generate faces
and the paper is listed on the bottom

948
01:08:04,871 --> 01:08:05,590
here.

949
01:08:05,590 --> 01:08:09,610
So you give a quote that is a random
code and it will give you an image of a

950
01:08:09,611 --> 01:08:12,220
face.
You can give it a second code,

951
01:08:12,430 --> 01:08:15,290
it's going to give you a second image
that is different from the first one

952
01:08:15,291 --> 01:08:17,290
because the code was different.
You can give it,

953
01:08:17,291 --> 01:08:20,240
the third one is going to
give you a third pick surface.

954
01:08:20,780 --> 01:08:25,100
The fun part is if you take code
one minus Code Two plus code three.

955
01:08:26,300 --> 01:08:31,300
So basically image of a man with glasses
minus image of a man plus image of a

956
01:08:32,031 --> 01:08:35,390
women. We give you an image
of a woman with glasses.

957
01:08:37,440 --> 01:08:42,260
So this is interesting because it means
that linear operation in the late and

958
01:08:42,261 --> 01:08:46,100
space of codes have impact
directly on the image stays.

959
01:08:48,410 --> 01:08:50,870
Okay,
let's look at something even better.

960
01:08:52,610 --> 01:08:54,740
So you can use guns for image generation.
Of course,

961
01:08:54,741 --> 01:08:59,741
these are very nice samples you see that
sometimes gans have problem with with

962
01:08:59,751 --> 01:09:04,540
uh, I dunno, I don't
think that's the dog but,

963
01:09:04,541 --> 01:09:05,374
but the,

964
01:09:06,470 --> 01:09:10,550
but these are Stagen plus versus a is a
very impressive Gannette has generated.

965
01:09:10,630 --> 01:09:14,490
It has been state of the
art for a long time. Okay.

966
01:09:15,350 --> 01:09:20,240
So let's see, something fun. Something
called image to image translation. So, uh,

967
01:09:21,080 --> 01:09:21,811
actually the,
the,

968
01:09:21,811 --> 01:09:26,210
the project winners last quarter in
spring was a project dealing with exactly

969
01:09:26,211 --> 01:09:30,020
that, generating satellite
images based on the map image.

970
01:09:30,860 --> 01:09:33,710
So given the map you mentioned generate
the satellite image using again.

971
01:09:34,100 --> 01:09:37,180
So you see that instead of giving you
later and code that was 100 dimensional,

972
01:09:37,220 --> 01:09:41,990
you could give a very detailed code.
The code can be this image, right?

973
01:09:42,320 --> 01:09:45,110
And you have to find a way to
constrain your network in a certain,

974
01:09:45,950 --> 01:09:46,731
in a certain way,

975
01:09:46,731 --> 01:09:51,540
to push it to output exactly the satellite
image that corresponded to this mark

976
01:09:51,541 --> 01:09:55,910
image. There are many other restaurants
that are fun converting zebras,

977
01:09:55,911 --> 01:09:59,450
two horses to zebras and
zebras, two horses, uh,

978
01:09:59,510 --> 01:10:01,940
an apples to oranges and oranges to apple.

979
01:10:02,690 --> 01:10:04,910
So let's do a case study together.

980
01:10:06,320 --> 01:10:10,400
Let's say our goal is to convert horses
to zebras on images and vice versa.

981
01:10:11,840 --> 01:10:15,380
Can you tell me what data we need? Let's
go quickly so that we have some time.

982
01:10:17,300 --> 01:10:21,560
Yeah. Horses and zebras. Do you
need per images? You know, like,

983
01:10:21,580 --> 01:10:26,450
do you need to have the same
image of a horse as a zebra? Yeah,

984
01:10:26,451 --> 01:10:30,770
so the problem is, okay, we could
have labeled images, you know, like,

985
01:10:31,030 --> 01:10:35,780
uh, a horse and it's a zebra
doppelganger in the same position.

986
01:10:36,380 --> 01:10:39,590
Uh, and we could train a network
to take one and out with the other.

987
01:10:39,650 --> 01:10:43,280
Unfortunately, we don't, not every horse
has a doppelganger that is a zebra.

988
01:10:43,610 --> 01:10:47,300
So we cannot do that. Uh, so
instead we're going to do unpaired,

989
01:10:48,500 --> 01:10:50,690
unpaired,
generative adversarial networks.

990
01:10:50,870 --> 01:10:54,350
It means we have a database of
horses and a database of zebras,

991
01:10:54,680 --> 01:10:58,160
but these are different horses and zebras.
You're not one to one.

992
01:10:58,220 --> 01:11:00,740
There was no one to one mapping between
them. There's no mapping at all.

993
01:11:01,820 --> 01:11:03,590
What architecture do you want to use?

994
01:11:07,720 --> 01:11:10,470
Nice.
Again.

995
01:11:15,060 --> 01:11:17,910
Okay, so let's see about the
architecture and new cost.

996
01:11:18,030 --> 01:11:21,980
So I'm going to go very quickly
because it's a, it's a very fun game.

997
01:11:23,020 --> 01:11:24,150
It's called cycle again.

998
01:11:24,390 --> 01:11:29,100
So the way we're going to work it out
is we have a horse called capital h.

999
01:11:29,700 --> 01:11:32,520
We want to generate the Zebra
version of this horse, right?

1000
01:11:32,521 --> 01:11:36,420
So we give it to a generator that we
called Gyn. You can call it h two Z,

1001
01:11:36,450 --> 01:11:41,160
like horse to zebra. It should
give us a this horse h as a zebra.

1002
01:11:41,970 --> 01:11:46,200
Right? And in fact, if we were training
again, we need a discriminator.

1003
01:11:46,380 --> 01:11:47,700
So we will add the discriminator.

1004
01:11:47,701 --> 01:11:52,701
That is going to be a binary classifier
to tell us if this image out polluted by

1005
01:11:53,161 --> 01:11:54,930
generator one is real or not.

1006
01:11:55,710 --> 01:12:00,710
So these discriminator is going to take
in some images of zebras probably or

1007
01:12:03,500 --> 01:12:03,750
yeah,

1008
01:12:03,750 --> 01:12:08,750
zebras or horses and he's going to also
take the generated images and going to

1009
01:12:09,840 --> 01:12:11,670
see which one is fake,
which one Israel.

1010
01:12:12,870 --> 01:12:16,350
On the other hand we're going to do
and the vice versa is very important.

1011
01:12:16,680 --> 01:12:21,680
We need to enforce the fact that this
horse g one of age should be the same

1012
01:12:22,141 --> 01:12:26,130
horse as age.
In order to do that,

1013
01:12:26,220 --> 01:12:31,080
we're going to create another generator
which is going to take the generated

1014
01:12:31,081 --> 01:12:35,970
image and generate back the input image
and this is where we will be able to

1015
01:12:35,971 --> 01:12:40,950
enforce the constraints that g two
of g one of h should be equal to age.

1016
01:12:42,510 --> 01:12:46,590
You see why this loop is super important?
Because if we don't have this loop,

1017
01:12:46,591 --> 01:12:50,870
we don't have the constraints on the
fact that the horse should be the,

1018
01:12:51,170 --> 01:12:53,640
the zebra should be the horse as a zebra.

1019
01:12:53,700 --> 01:12:57,930
The same horse has h so we'll do that
and we had a second discriminator to

1020
01:12:57,931 --> 01:12:59,340
decide if this image is real.

1021
01:13:00,510 --> 01:13:05,510
This is one step h to z and understate
my dizzy to age where we start with the

1022
01:13:05,971 --> 01:13:06,450
zebra,

1023
01:13:06,450 --> 01:13:10,920
give it to generate or to generate the
horse version of the zebra discriminate,

1024
01:13:12,030 --> 01:13:17,030
generate back the zebra version
of the Zebra and this community.

1025
01:13:18,450 --> 01:13:19,283
Does that make sense?

1026
01:13:20,400 --> 01:13:25,400
So this is the general pattern using
cycle gans and what I'd like to go over is

1027
01:13:27,750 --> 01:13:32,750
what laws should we minimize in order to
enforce the fact that we want the horse

1028
01:13:33,691 --> 01:13:36,330
to be converted to a zebra
that is the same as the horse

1029
01:13:38,720 --> 01:13:40,560
and someone gives me
the terms that we need.

1030
01:13:43,240 --> 01:13:44,410
Someone wants to give you the tray,

1031
01:13:50,190 --> 01:13:54,920
go for it. Two minutes. Yes.

1032
01:13:55,440 --> 01:13:59,600
You want to make sure that the picture
that is zebra that you started off with

1033
01:13:59,610 --> 01:14:02,730
matches the zebra that you started
with or the horse to start off with the

1034
01:14:02,760 --> 01:14:04,310
supports that you had originally.
Okay.

1035
01:14:04,610 --> 01:14:08,390
But at the same time you also need to
have discriminator to identifying that the

1036
01:14:08,391 --> 01:14:12,040
image is a real zebra or
a real, yeah, because you

1037
01:14:12,040 --> 01:14:15,580
don't want it to just sort of input in
the sample image and then output back to

1038
01:14:15,581 --> 01:14:16,414
you at the same way.

1039
01:14:17,440 --> 01:14:21,720
So I think you'd want to add the output
of the pos function from discriminary of

1040
01:14:21,730 --> 01:14:26,240
two to be a cost that you get
comparing the starting it,

1041
01:14:26,250 --> 01:14:27,450
which is great.

1042
01:14:27,730 --> 01:14:30,970
So you're saying we need the classic
cross functions that we've seen previously

1043
01:14:31,540 --> 01:14:36,340
plus another one that is the
matching between h and g two of g,

1044
01:14:36,341 --> 01:14:40,210
one of H and z n g one or
g two ops? Yes. Correct.

1045
01:14:40,560 --> 01:14:41,700
So we'll have all these terms.

1046
01:14:41,790 --> 01:14:46,790
One term to train d one which is the
classic term we've seen differentiate real

1047
01:14:47,161 --> 01:14:51,720
images from generating images.
G One as well.

1048
01:14:52,230 --> 01:14:56,250
Same. We were using the non
saturating costs on generating images.

1049
01:14:56,290 --> 01:14:58,890
Same 42 st four g two these are classics.

1050
01:14:59,250 --> 01:15:02,490
The one we need to add to all
of this is the cycle costs,

1051
01:15:03,960 --> 01:15:07,880
which is the distance between this term g,
two of g,

1052
01:15:07,890 --> 01:15:12,600
one of h and h and the same thing
for zebras. Does that make sense?

1053
01:15:13,680 --> 01:15:15,780
So you have the intuition
to build that type of loss.

1054
01:15:15,781 --> 01:15:19,380
We just some everything and gives us the
cost function we're looking for. Yeah.

1055
01:15:21,170 --> 01:15:22,340
Do you want to see to

1056
01:15:24,860 --> 01:15:29,860
recognize or the same cost function
for d one and d two so the,

1057
01:15:30,400 --> 01:15:34,420
the you could but it's not going to work
that well. I think so. I think there's a,

1058
01:15:34,550 --> 01:15:38,090
there's a tiny mistake here is that uh,
the Z

1059
01:15:38,800 --> 01:15:40,530
ballsy,
I should be small Ahi

1060
01:15:42,340 --> 01:15:44,020
and the small hii on top,

1061
01:15:44,021 --> 01:15:47,710
she just Mosey the eye because the
discriminator one is going to receive

1062
01:15:47,711 --> 01:15:51,510
generated samples that look like
zebras because it came out of g.

1063
01:15:52,600 --> 01:15:55,720
So you want the real
database that you give it to,

1064
01:15:55,721 --> 01:15:59,500
to be zebras as well to
force to force the generator,

1065
01:15:59,501 --> 01:16:04,120
want to output things that look like
zebras and vice versa for the second one.

1066
01:16:04,870 --> 01:16:07,780
Okay.
And this is my favorite,

1067
01:16:08,590 --> 01:16:12,640
so you can convert a Roman
to a face and back to a ramen

1068
01:16:14,970 --> 01:16:19,970
is the most fun application I've found
in some narrow Tomita and Taqueria Taco.

1069
01:16:22,210 --> 01:16:26,750
So it's Japanese research lab.
We're working hard to do face around.

1070
01:16:28,550 --> 01:16:33,070
Yeah. And actually in two, in two to three
weeks you will learn object detection,

1071
01:16:33,071 --> 01:16:34,510
you know,
to detect faces.

1072
01:16:34,810 --> 01:16:37,780
And if you learned that maybe you can
start a project to like detect the face

1073
01:16:38,050 --> 01:16:42,370
and then replace it by a
ramen and on and also funny,

1074
01:16:42,371 --> 01:16:46,560
funny work by Mary told me.
Okay.

1075
01:16:46,660 --> 01:16:51,010
Oh this is a super cool application
as well. So let's look at that.

1076
01:16:54,130 --> 01:16:55,960
Okay.
So we have,

1077
01:16:58,380 --> 01:17:03,380
so this model is a conditional gun
that was conditioned on learning,

1078
01:17:03,490 --> 01:17:08,350
uh, learning edges and generating
cuts based on the edges.

1079
01:17:08,410 --> 01:17:10,310
So I'm gonna,
I'm to try to draw a cat.

1080
01:17:12,220 --> 01:17:14,010
Okay. Sorry. I cannot see you

1081
01:17:17,490 --> 01:17:18,860
again.
I'm not a good dryer.

1082
01:17:27,840 --> 01:17:29,710
Okay. He's going to be
dominant. The model.

1083
01:17:33,070 --> 01:17:34,190
I hope he's going to work.
You

1084
01:17:43,400 --> 01:17:44,233
okay?

1085
01:17:44,380 --> 01:17:48,880
Okay. Now. Yeah, I don't think it
works, but it's supposed to work.

1086
01:17:49,840 --> 01:17:53,280
So you can generate Todd
speeds on the edges and a,

1087
01:17:53,290 --> 01:17:56,470
you can do eight for different things.
You can do it for a shoe.

1088
01:17:56,650 --> 01:17:58,420
So all of these models
have been trained for that.

1089
01:18:00,350 --> 01:18:04,310
Okay. Yes, go for it.

1090
01:18:06,130 --> 01:18:09,970
What is most, would you
have this, this one,

1091
01:18:11,480 --> 01:18:16,410
January, next up buildings cat, sorry.

1092
01:18:16,411 --> 01:18:17,244
Can you repeat,

1093
01:18:17,550 --> 01:18:21,280
is it generalizable or would you have
to train it specifically for the jump up

1094
01:18:21,300 --> 01:18:22,950
to training specifically for the domain.

1095
01:18:23,250 --> 01:18:25,700
So like these models are
different that been trained

1096
01:18:28,680 --> 01:18:30,570
for my prison and I missed it.

1097
01:18:36,330 --> 01:18:39,920
Presentation disappeared. Okay. Um,

1098
01:18:40,180 --> 01:18:41,940
another application is super resolution.

1099
01:18:41,941 --> 01:18:43,980
You can give a lower
resolution image in January,

1100
01:18:43,981 --> 01:18:47,130
the super resolution
version of it using gans.

1101
01:18:47,490 --> 01:18:51,520
And this is pretty cool because you
can get a high resolution image down,

1102
01:18:51,570 --> 01:18:56,100
sample it and use this as a
demeaning Max game. You know,

1103
01:18:56,580 --> 01:18:59,890
like you have the high resolution
version of the lower version,

1104
01:18:59,940 --> 01:19:02,950
a very low resolution image.
Um,

1105
01:19:03,200 --> 01:19:05,850
are there applications
can be privacy preserving.

1106
01:19:05,851 --> 01:19:09,990
So some people have been working
on, you know, in medical, uh,

1107
01:19:10,080 --> 01:19:12,890
in the medical space.
Privacy is a huge issue. Uh,

1108
01:19:12,900 --> 01:19:16,890
you cannot share data set among
hospitals among medical teams is coming.

1109
01:19:17,430 --> 01:19:22,350
So people have been looking at generating
a data set that looks like a medical

1110
01:19:22,351 --> 01:19:23,184
data set.

1111
01:19:23,460 --> 01:19:27,000
If you train the model on this data set
is going to give you the same type of

1112
01:19:27,001 --> 01:19:28,290
fire meters then the other one.

1113
01:19:28,470 --> 01:19:33,450
But this data set is anonymized so they
can share the anonymized data with each

1114
01:19:33,451 --> 01:19:38,451
other and train their model without being
able to access an inflammation of the

1115
01:19:38,821 --> 01:19:41,460
patient.
And who is a

1116
01:19:43,080 --> 01:19:45,150
manufacturing is important as well.

1117
01:19:45,151 --> 01:19:49,470
So gangs can generate a very specific,

1118
01:19:50,050 --> 01:19:53,820
uh,
objects that can replace bones for humans,

1119
01:19:54,000 --> 01:19:58,470
personal lives to, to, to the
human body. So same for dental.

1120
01:19:58,500 --> 01:19:59,520
If you lose a teeth,

1121
01:20:00,420 --> 01:20:03,780
the technician can take a
picture and decide what's the,

1122
01:20:03,870 --> 01:20:05,100
the crown should look like.

1123
01:20:05,520 --> 01:20:09,420
The gang can generate it as uh,

1124
01:20:09,510 --> 01:20:12,840
another topic is how to
evaluate against, you know, um,

1125
01:20:14,160 --> 01:20:14,761
you might say,

1126
01:20:14,761 --> 01:20:18,420
we can just look at the images and see
if they look real and it will give us an

1127
01:20:18,421 --> 01:20:21,840
idea if your gut is working
well in practice it's hard
because maybe the images

1128
01:20:21,841 --> 01:20:26,070
you're looking at or overfitting images
from the real samples you gave to the,

1129
01:20:26,130 --> 01:20:30,810
to the, to the discriminator.
Uh, so how do you check that?

1130
01:20:30,840 --> 01:20:35,460
It's very complicated. So human
annotation is a big one where you would,

1131
01:20:35,550 --> 01:20:38,430
uh,
you would build a software,

1132
01:20:38,460 --> 01:20:41,550
push it on the cloud and people all
around the world are going to select which

1133
01:20:41,551 --> 01:20:46,380
images look generated, which images look
not generated to see if a human can can,

1134
01:20:46,381 --> 01:20:50,730
can compare your, again to real world
data and how you're getting performance.

1135
01:20:51,440 --> 01:20:54,990
So it would look like that.
Or Web APP indicates which image is fake,

1136
01:20:54,991 --> 01:20:57,960
which image is real. You can,
you can do different experiments.

1137
01:20:57,961 --> 01:21:02,790
Like you can show very quickly an image
for a fraction of a second and ask them

1138
01:21:02,850 --> 01:21:05,310
was it real or not?
Or you can give them a limited time.

1139
01:21:05,790 --> 01:21:08,230
Different experiments can be led.
Uh,

1140
01:21:08,300 --> 01:21:11,430
there was another one that is more
scalable because human annotation is very

1141
01:21:11,431 --> 01:21:13,470
painful. You know, every
time you train, again,

1142
01:21:13,471 --> 01:21:16,710
you want to do that to verify if the gun
is working well, it takes a lot of time.

1143
01:21:17,100 --> 01:21:19,110
So instead of using humans,

1144
01:21:19,170 --> 01:21:22,980
why don't we use a very good network
that is good at classification. In fact,

1145
01:21:22,981 --> 01:21:27,030
in fact, the inception network
is a tremendous network
that does classification.

1146
01:21:27,300 --> 01:21:32,300
We're going to give our image samples to
this inception network and see what the

1147
01:21:32,551 --> 01:21:35,940
network thinks of this image.
Does he think that it's a dog or not?

1148
01:21:36,120 --> 01:21:37,830
Does it look like a dog
for the network or not?

1149
01:21:37,831 --> 01:21:41,670
And we can scale it and make
it very quick. And there's
an inception score that,

1150
01:21:41,730 --> 01:21:44,790
that we can talk next week too
about when we'll have time. Uh,

1151
01:21:44,791 --> 01:21:47,790
it measures the quality of the samples,
uh,

1152
01:21:48,060 --> 01:21:52,500
and also it measures the diversity of
the sample. I go over it next week,

1153
01:21:52,530 --> 01:21:57,130
hopefully. Uh, there's another
distance that is very popular. Uh,

1154
01:21:57,210 --> 01:22:01,590
this has been growingly popular recently
called the fresh air inception distance.

1155
01:22:02,070 --> 01:22:06,270
And I advise you to check some of these
papers that you're more interested in it

1156
01:22:06,271 --> 01:22:09,840
for four year projects.
So just to end, uh,

1157
01:22:09,960 --> 01:22:13,590
for next Wednesday we'll have a c,
two and three,

1158
01:22:13,650 --> 01:22:17,750
and also the whole see tree modules.
You'll have three quizzes. Uh,

1159
01:22:17,790 --> 01:22:22,790
be careful these two quiz [inaudible]
or longer than the normal quizzes.

1160
01:22:23,040 --> 01:22:27,960
They're like wide case studies. So
take your time and go over it. Um,

1161
01:22:28,290 --> 01:22:30,580
and you'll have one
programming assignments. Uh,

1162
01:22:30,600 --> 01:22:33,750
make sure you understand the bachelor
on videos so that we can go over the

1163
01:22:33,751 --> 01:22:36,960
virtual batch norm hopefully
next week together. Um,

1164
01:22:37,230 --> 01:22:39,810
and hands on section this Friday,
uh,

1165
01:22:39,811 --> 01:22:44,160
you will receive your project proposal
as soon as possible and meet with your

1166
01:22:44,161 --> 01:22:48,570
project Tas to go over the proposal and
to make decisions regarding the next

1167
01:22:48,571 --> 01:22:53,160
steps for your projects. I'll stick around
in case you have any questions. Okay.

1168
01:22:53,190 --> 01:22:53,700
Thanks guys.

