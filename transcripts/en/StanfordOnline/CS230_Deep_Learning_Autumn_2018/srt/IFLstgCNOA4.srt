1
00:00:05,690 --> 00:00:10,690
So hello everyone and welcome for the
last lecture of cs two 30 deep learning.

2
00:00:12,330 --> 00:00:14,130
So He's been 10 weeks and you've been,

3
00:00:14,310 --> 00:00:17,070
you've been studying
deep learning all around,

4
00:00:17,071 --> 00:00:20,670
starting with fully connected networks,
uh,

5
00:00:20,760 --> 00:00:25,710
understanding how to boost these networks
and make them better. And then, uh,

6
00:00:25,711 --> 00:00:29,160
using record neural networks in the last
part then is and convolutional neural

7
00:00:29,161 --> 00:00:34,161
networks in the fourth part to build
models for imaging and texts and other

8
00:00:35,581 --> 00:00:40,020
applications.
So today's the costs wrap up and uh,

9
00:00:40,060 --> 00:00:44,850
the lecture might be a
slightly shorter than usual,

10
00:00:45,060 --> 00:00:50,060
which we're going to go over a small case
study on conversational assistance to

11
00:00:50,881 --> 00:00:54,240
start with, which is a,
an industrial topic. Um,

12
00:00:54,450 --> 00:00:59,450
we will do a small quiz competition with
mentee and the fastest Uiux person who

13
00:01:00,661 --> 00:01:05,661
has the best answer would win a
400 hours of GPU credits on Amazon.

14
00:01:08,940 --> 00:01:12,540
So you guys can, can start,
can start working on it.

15
00:01:13,590 --> 00:01:14,423
Um,

16
00:01:14,730 --> 00:01:19,380
we will see some class projects advice
because you guys have about two weeks,

17
00:01:20,190 --> 00:01:21,450
less than two weeks before the,

18
00:01:21,470 --> 00:01:26,190
the poster presentation and
to find no a project due date.

19
00:01:27,570 --> 00:01:31,110
We'd also go over some of the
next steps after cs two 30.

20
00:01:31,111 --> 00:01:36,111
What have our students done over the
past year and what we think are good.

21
00:01:36,151 --> 00:01:39,210
Next steps and closing remarks to finish.

22
00:01:40,220 --> 00:01:43,860
I'll buy you one if you have a
clicker, uh, with, with battery please.

23
00:01:43,920 --> 00:01:47,700
Can you bring it to me? Yep. Um, okay.

24
00:01:47,701 --> 00:01:52,701
So let's get started with how to build
a chat bot to help students find ore and

25
00:01:53,790 --> 00:01:56,160
enroll in the right course.

26
00:01:56,730 --> 00:02:01,730
So he's going to be a pretty simple
case of a chat bot because chatbots and

27
00:02:01,951 --> 00:02:06,290
conversational conversational assistance
in general have been very hard to be

28
00:02:06,340 --> 00:02:11,050
then having a neutral topic. There is
some places where academia has helped,

29
00:02:11,190 --> 00:02:14,430
uh,
the chatbots improvements.

30
00:02:14,790 --> 00:02:18,800
And here we're going to see how
we can take all our algorithms,

31
00:02:18,820 --> 00:02:22,560
what we've learned in discuss and plug
it in in a conversational setting.

32
00:02:23,410 --> 00:02:25,530
That sounds good.
So let me give you an example.

33
00:02:26,600 --> 00:02:31,150
The students might write to the Chat Bot
high. I went to enroll in cs one six,

34
00:02:31,160 --> 00:02:34,110
eight four winter 2019 to learn coding.

35
00:02:35,820 --> 00:02:39,000
Your Chat Bot cannot answer for sure.
I just enrolled you.

36
00:02:39,390 --> 00:02:41,730
So that would be one goal of the Chat Bot.

37
00:02:43,260 --> 00:02:48,120
A second example might be finding
information about classes.

38
00:02:48,750 --> 00:02:49,111
Hi,

39
00:02:49,111 --> 00:02:54,111
what are the undergraduate level history
classes offered in spring 2019 then the

40
00:02:55,051 --> 00:02:57,330
chat Bot can get back to
the students and said,

41
00:02:57,331 --> 00:03:01,480
here's the least of history
classes offered in spring 2008.

42
00:03:03,040 --> 00:03:05,080
So we're making a smarter assumption here.

43
00:03:05,081 --> 00:03:09,700
We're building a chat bot for a
very restricted area in general.

44
00:03:09,760 --> 00:03:13,510
And a lot of time chatbots,
which work very well,

45
00:03:13,540 --> 00:03:16,660
are super goal oriented or transactional.

46
00:03:16,960 --> 00:03:20,790
And the state of possible,
uh,

47
00:03:20,850 --> 00:03:23,920
archer and or requests
from users is small,

48
00:03:24,100 --> 00:03:27,340
smaller than what you could expect
in other industrial settings.

49
00:03:27,790 --> 00:03:31,600
So here we're making the assumption
that the students will only try to find

50
00:03:31,601 --> 00:03:36,370
information about a course or
we try to enroll in the course.

51
00:03:37,300 --> 00:03:41,530
So I want you guys to,
to pair in groups of two or three and uh,

52
00:03:41,560 --> 00:03:46,560
tried to come up with ideas of what
methods that we've seen together.

53
00:03:47,711 --> 00:03:51,880
It can be used in order to
implement such a chatbox. Okay.

54
00:03:52,270 --> 00:03:55,810
So take a minute,
introduce yourself to your mates and,

55
00:03:55,860 --> 00:03:59,500
and tried to figure out which methods
can be leveraged in this case.

56
00:04:01,630 --> 00:04:03,790
Okay.
Let's see what we have here.

57
00:04:05,010 --> 00:04:08,620
RNS for natural language
processing yet transfer learning

58
00:04:10,230 --> 00:04:14,200
and it seemed to pick out important
words from inputs based on those input

59
00:04:14,201 --> 00:04:18,160
triggers. Output, some predefined
inflammation from storage. Yeah.

60
00:04:18,610 --> 00:04:23,610
So this seems to to say that there's
going to be one learning parts where we

61
00:04:26,411 --> 00:04:31,411
need to have probably recurrent neural
networks helping out and one other

62
00:04:31,480 --> 00:04:35,050
knowledge base or storage part where
we can retrieve some information.

63
00:04:35,950 --> 00:04:38,950
We're going to see that
some of the tension models,

64
00:04:38,951 --> 00:04:43,951
it's true that today a lot of natural
language processing models are built with

65
00:04:44,441 --> 00:04:45,640
attention models.

66
00:04:48,570 --> 00:04:49,403
Yeah.

67
00:04:49,640 --> 00:04:52,120
RNN forced speech recognition
and speech generation four.

68
00:04:52,170 --> 00:04:56,380
So we didn't talk about the speech
parts so far. We assume that uh,

69
00:04:56,460 --> 00:04:58,740
the conversational
assistant is tech space. Uh,

70
00:04:58,790 --> 00:05:02,630
but later on we will see what happens
if we want to add speech to it.

71
00:05:06,360 --> 00:05:07,193
Okay.

72
00:05:08,180 --> 00:05:11,980
Fancy methods.
Oh,

73
00:05:11,981 --> 00:05:15,250
reinforcement learning for making
this students about responses.

74
00:05:15,730 --> 00:05:16,563
That's interesting.

75
00:05:17,440 --> 00:05:20,620
So why do you guys think we would
need reinforcement learning?

76
00:05:23,300 --> 00:05:24,133
Yes.

77
00:05:25,640 --> 00:05:28,930
Contexts. You have different states
and you also have like our value.

78
00:05:28,931 --> 00:05:32,100
And so she wouldn't be said it was very
goal oriented and so you could sort of

79
00:05:32,101 --> 00:05:34,800
have a progressive that
fashion. Yeah, that's good.

80
00:05:35,240 --> 00:05:36,650
Just to repeat,
uh,

81
00:05:36,740 --> 00:05:41,260
it's important to keep a notion of context
and also we have a sequence off our

82
00:05:41,350 --> 00:05:43,240
truancies from the user and uh,

83
00:05:43,650 --> 00:05:48,230
the commercial assistance and probably
the outcome of the conversation would

84
00:05:48,231 --> 00:05:52,720
come far along the way and not that
every step. So that's true. Uh,

85
00:05:53,840 --> 00:05:57,100
reinforcement learning has
been, uh, uh, research, Xapi,

86
00:05:57,590 --> 00:05:59,120
commercial assistance as well.

87
00:05:59,480 --> 00:06:02,480
And oftentimes we will try to
learn a policy for the Chatbot,

88
00:06:02,510 --> 00:06:05,750
which given the state will tell
us what action to take next.

89
00:06:06,200 --> 00:06:09,470
This can be done using Cuellar and ink,
which is the method we've seen together.

90
00:06:09,710 --> 00:06:11,330
Or sometime we'd policy grievance.

91
00:06:13,290 --> 00:06:14,123
Okay.

92
00:06:15,340 --> 00:06:18,520
Word and coding.
So word embedding probably.

93
00:06:21,640 --> 00:06:22,473
Okay,

94
00:06:22,770 --> 00:06:25,750
cool. So it would be,
there's many ways to,

95
00:06:25,820 --> 00:06:30,090
to plug in a deep learning algorithm in,
in this box setting.

96
00:06:30,091 --> 00:06:31,440
We're going to see a few of them.

97
00:06:32,550 --> 00:06:33,383
Oh,

98
00:06:37,980 --> 00:06:41,010
uh, first I'd like to
introduce some vocabulary, uh,

99
00:06:41,260 --> 00:06:44,590
which is commonly use when talking
about commercial assistance,

100
00:06:44,870 --> 00:06:47,710
conversational assistance.
Uh, not Terrance is,

101
00:06:47,860 --> 00:06:51,650
you can think of it as a user inputs.
So if I say the student utterance,

102
00:06:51,730 --> 00:06:56,630
it's the sentence that was written
by the students for the Chat Bot. Uh,

103
00:06:56,980 --> 00:07:00,180
the assistant utterance is the
one coming from the chatbots,

104
00:07:00,190 --> 00:07:03,700
like the intended notes,
the intention of the user.

105
00:07:03,940 --> 00:07:07,270
So in our case we will have two intense,
which is very limited.

106
00:07:07,840 --> 00:07:12,840
The user either wants to find information
from for a course or the user wants

107
00:07:13,241 --> 00:07:15,850
to, uh, enroll in a class.

108
00:07:15,880 --> 00:07:20,500
These are two different intentions that
are probably to be detected early on the

109
00:07:20,501 --> 00:07:25,430
conversation. And then you have
something called slots. Slots, uh,

110
00:07:26,110 --> 00:07:30,850
are used together multiple information
from the user on a specific intense that

111
00:07:30,851 --> 00:07:35,440
you use your hands. So let's say the
students wants to enroll in your class.

112
00:07:35,500 --> 00:07:39,040
In order to enroll the students in your
class, you need to fill in several slots.

113
00:07:39,550 --> 00:07:44,140
You need to understand probably which
class the student is talking about,

114
00:07:44,770 --> 00:07:48,040
which quarter the student
wants to enroll in the class,

115
00:07:48,340 --> 00:07:50,380
which year is the student talking about?

116
00:07:50,680 --> 00:07:54,670
And eventually you want to know
this shoot suid of the students.

117
00:07:55,210 --> 00:07:58,180
But probably we can assume that the
MSU ID is already included in the

118
00:07:58,181 --> 00:08:00,700
conversation on the environment we're in.

119
00:08:01,840 --> 00:08:06,840
So these are three vocabulary and we're
also going to talk about turns for

120
00:08:06,971 --> 00:08:08,740
commercialization assistance.

121
00:08:09,730 --> 00:08:14,730
So so single turn a conversation is
when there is just an user returns.

122
00:08:16,150 --> 00:08:21,150
Any response and multi turn is when
there's several user authoring Cs and a

123
00:08:23,260 --> 00:08:28,260
conversational assistant utterances and
you understand that Malta multirotor

124
00:08:29,120 --> 00:08:33,670
utterance conversations are
harder to understand because
we need to track context

125
00:08:35,180 --> 00:08:38,950
or assumption today will be that we work
in an environment with remitted intense

126
00:08:38,980 --> 00:08:42,580
and slots. It means we can define too
intense and for each of these two intense,

127
00:08:42,581 --> 00:08:46,000
there are several slots that we want to
fill in is going to make our life easier.

128
00:08:47,050 --> 00:08:51,970
Of course, in practice you
can have multi myriads off,

129
00:08:51,971 --> 00:08:54,010
intense in slots and you did.

130
00:08:54,011 --> 00:08:56,940
The task becomes more complicated
when you have more of those.

131
00:08:58,260 --> 00:09:03,260
So my first question would be how
to detect the intense based on,

132
00:09:04,910 --> 00:09:07,110
uh,
the user utterance.

133
00:09:08,490 --> 00:09:12,090
Can you talk about what kind of data set
you need to build in order to train a

134
00:09:12,091 --> 00:09:13,340
model to detect the intense?

135
00:09:24,180 --> 00:09:25,013
Okay.

136
00:09:26,060 --> 00:09:27,910
Or what type of network
do you want to use?

137
00:09:37,450 --> 00:09:40,770
There's not a single good answer,
so go for it. Him brainstorm.

138
00:09:42,500 --> 00:09:46,600
So I think there's, there's going to be
two options obviously because we have a,

139
00:09:47,020 --> 00:09:49,630
we have a sequence coming in,
which is the user inputs.

140
00:09:49,930 --> 00:09:54,340
We might want to use a recurrent neural
network to encode long term dependencies,

141
00:09:54,341 --> 00:09:58,150
or we might want to use a
convolutional network. Actually,

142
00:09:58,750 --> 00:10:03,010
a convolutional networks have some
benefits that's recurrent neural networks

143
00:10:03,011 --> 00:10:05,950
don't have, and they, they
might work better. For example,

144
00:10:05,951 --> 00:10:10,240
if the intent we're looking for is always
included in a small number of words

145
00:10:10,540 --> 00:10:12,610
somewhere,
Indian put sequence,

146
00:10:13,450 --> 00:10:16,010
because you will have a filter
scanning that's in the future.

147
00:10:16,011 --> 00:10:16,990
It can detect the entities.

148
00:10:17,350 --> 00:10:21,580
So each other filter that was trained
in order to detect the intense inform,

149
00:10:22,210 --> 00:10:26,560
another feature trained to detect the
intense enroll. Then these two filter,

150
00:10:26,561 --> 00:10:28,840
we detect the word enroll,
Lord,

151
00:10:28,841 --> 00:10:32,440
the word I'm looking for and so
on in order to take the intent.

152
00:10:33,490 --> 00:10:35,500
Okay.
In terms of data,

153
00:10:36,490 --> 00:10:41,380
what you probably need is pairs of user
utterances along with the intent of the

154
00:10:41,381 --> 00:10:42,214
user.

155
00:10:42,430 --> 00:10:46,390
So you would need to label the data
sets like this one with eggs and input.

156
00:10:46,391 --> 00:10:47,950
I want to,
so it's padded.

157
00:10:47,951 --> 00:10:52,951
I want to enroll in [inaudible] 64 winter
2019 to learn coding and this you will

158
00:10:53,171 --> 00:10:57,910
label it as enroll and notice
that enroll here is a function.

159
00:10:58,450 --> 00:11:03,220
So did the label is actually noted as
a function and the reason is because we

160
00:11:03,221 --> 00:11:05,890
can call this function in
order to retrieve information.

161
00:11:06,260 --> 00:11:07,660
And another example is high.

162
00:11:07,661 --> 00:11:11,470
What are the undergraduate level history
classes offered in spring 2018 and this

163
00:11:11,471 --> 00:11:12,750
would be labeled as in four.

164
00:11:14,020 --> 00:11:17,830
So he's probably a two class
classification or three classes.

165
00:11:17,860 --> 00:11:22,860
If you want to add a third class that
corresponds to other intense user might

166
00:11:23,531 --> 00:11:28,090
want to use these chatbots for another
intense that the child wasn't built for.

167
00:11:29,770 --> 00:11:32,020
So these are new classes
enrolling in form.

168
00:11:32,110 --> 00:11:36,160
And what's interesting is that if we
identify that the intent of the user is

169
00:11:36,161 --> 00:11:36,994
enroll,

170
00:11:37,420 --> 00:11:41,740
we probably want to call an API or to
request information from another server.

171
00:11:41,920 --> 00:11:46,150
And in this case it might be access
because the platform we use to enroll in

172
00:11:46,151 --> 00:11:51,151
classes is access and same to retrieve
information in order to help the user

173
00:11:51,431 --> 00:11:52,330
about their classes.

174
00:11:52,360 --> 00:11:57,360
We can probably explore courses assuming
that these these services have aps,

175
00:12:01,210 --> 00:12:03,820
the surfaces of aps.
Does that make sense?

176
00:12:05,140 --> 00:12:10,140
And now the interesting part is that the
enrollment function might request some

177
00:12:10,210 --> 00:12:14,890
inputs that you have to identify. Those
will be the slots, same 14 form function.

178
00:12:16,420 --> 00:12:20,440
Okay.
So we could train a sequence classifier,

179
00:12:20,650 --> 00:12:24,920
either convolutional or records. And these
were not going to go into the details.

180
00:12:24,940 --> 00:12:29,920
You've learned it in the sequence models
class. How did they take the slots now?

181
00:12:32,040 --> 00:12:36,150
So in terms of data, it's going to
look very similar to the previous one,

182
00:12:36,151 --> 00:12:40,990
but we will have a sequence to sequence
problem now where the user assurance

183
00:12:41,010 --> 00:12:45,510
will be a sequence of words and the
slot stag will also be a sequence.

184
00:12:45,930 --> 00:12:46,980
So for example,

185
00:12:47,130 --> 00:12:52,130
show me the Tuesday 5th of December
flights from Paris to Kuala Lumpur.

186
00:12:52,560 --> 00:12:57,560
If you were to build a conversational
assistance for flights and booking,

187
00:12:59,010 --> 00:13:02,780
then uh, the label you want to have
is probably something like that.

188
00:13:02,800 --> 00:13:04,260
Doesn't have to be exactly this,

189
00:13:04,740 --> 00:13:08,640
but why they know zero
for some of the words,

190
00:13:08,830 --> 00:13:13,830
the sequence is B day I day or be Dep,

191
00:13:14,670 --> 00:13:19,260
B r. R what do you think these
correspond to and why do we need that?

192
00:13:21,810 --> 00:13:25,710
We've probably, you've probably seen that
in, in the sections a few weeks back.

193
00:13:31,210 --> 00:13:35,430
So why do we do know these labels?
Certain

194
00:13:37,130 --> 00:13:37,963
format.

195
00:13:42,130 --> 00:13:42,890
Huh?

196
00:13:42,890 --> 00:13:46,820
It helps you out and find a slot facts
and like departure or I won't ramble.

197
00:13:47,240 --> 00:13:49,940
And then the other one for
day two possible things.

198
00:13:51,730 --> 00:13:54,460
Yep. Correct. So, uh,

199
00:13:55,370 --> 00:13:57,980
I agree with what you said for day day.

200
00:13:58,410 --> 00:14:02,850
Sure. Arrival arrivals. So these words
are encoding day, departure and arrival.

201
00:14:02,970 --> 00:14:05,020
How about the B and the I and the o?

202
00:14:08,060 --> 00:14:11,340
Yeah.
Someone has an idea beginning,

203
00:14:11,341 --> 00:14:14,550
beginning because sometimes these things
are more than one word. Yeah, exactly.

204
00:14:14,620 --> 00:14:17,100
It's me be the notes beginning.

205
00:14:18,200 --> 00:14:23,150
I they know it's in or inside
an o out or output in general.

206
00:14:24,230 --> 00:14:28,310
So what happens here is that sometimes
you would have a slot which might be

207
00:14:28,311 --> 00:14:32,420
filled by several words and not a single
work and you want to be able to detect

208
00:14:32,421 --> 00:14:36,390
this entire chunk called chunking.
Um,

209
00:14:36,500 --> 00:14:41,500
so you would use a special encoding in
order to identify if this word is the

210
00:14:41,571 --> 00:14:46,571
beginning of a word that you want to feel
in the slot or is the end or inside or

211
00:14:47,151 --> 00:14:52,151
out of the word you want to fill in the
slot and then they departure arrival or

212
00:14:52,221 --> 00:14:55,850
three possible slots that we want to
seal in in order to be able to book the

213
00:14:55,851 --> 00:14:58,670
flight.
If you don't receive these slots,

214
00:14:58,671 --> 00:15:02,150
you might want to have your
chatbots request these slots. Itero

215
00:15:03,750 --> 00:15:04,200
okay.

216
00:15:04,200 --> 00:15:04,620
Okay.

217
00:15:04,620 --> 00:15:09,620
So another example in classes here can
be daily departure or arrival class.

218
00:15:10,591 --> 00:15:13,080
Like,
do you want to travel in echo or business,

219
00:15:13,420 --> 00:15:16,770
a number of passenger that you
want to have on your flight? Uh,

220
00:15:17,430 --> 00:15:20,460
if we were for our Chad,
huge would be high.

221
00:15:20,461 --> 00:15:24,600
I want to enrolled in cs one to six today,
a four winter, 2019 to learn coding.

222
00:15:24,601 --> 00:15:28,470
And we will include it by the
beginning of the code of the class,

223
00:15:28,800 --> 00:15:31,020
beginning of the quarter
and beginning of the year.

224
00:15:31,830 --> 00:15:35,790
That would be a possibility in
coding and then you will train, uh,

225
00:15:36,600 --> 00:15:37,201
using a,

226
00:15:37,201 --> 00:15:41,700
probably a recurrent neural network
and algorithm to predict all these tax.

227
00:15:42,690 --> 00:15:43,523
Does that make sense?

228
00:15:45,300 --> 00:15:49,390
So now we have already two models.
That's all running on our chat bots.

229
00:15:49,620 --> 00:15:53,700
One Daddy's 40 intense and
one that is for the tax.

230
00:15:57,740 --> 00:16:01,340
What'd you think about joint training?
Do you think it's something we could do

231
00:16:01,800 --> 00:16:02,633
here

232
00:16:04,370 --> 00:16:06,050
and what do I mean by joint training?

233
00:16:16,340 --> 00:16:16,790
Yup.

234
00:16:16,790 --> 00:16:21,790
Pruning on all the different codes like
trading order here and class rather than

235
00:16:22,401 --> 00:16:23,720
training and separate
network for each of them.

236
00:16:23,740 --> 00:16:25,370
Settled like the drag
element of the training.

237
00:16:27,740 --> 00:16:30,020
Not Training for different codes.
No,

238
00:16:30,650 --> 00:16:33,320
I was talking more about
training for different tasks.

239
00:16:33,321 --> 00:16:38,321
So intense and intense for enrolling in
10 from the intention and a and slots

240
00:16:39,680 --> 00:16:43,910
tagging is here.
We have one intense classifier,

241
00:16:43,940 --> 00:16:47,300
which takes any input,
sequence and outputs a single class.

242
00:16:47,690 --> 00:16:50,960
And we have started tiger,

243
00:16:51,380 --> 00:16:53,180
which takes the same inputs,

244
00:16:53,840 --> 00:16:57,770
exactly the same inputs and tags
every single word in the sequence.

245
00:16:58,220 --> 00:17:01,760
So probably we can use join trainee in
order to train one network that might be

246
00:17:01,761 --> 00:17:03,530
able to do both.

247
00:17:03,830 --> 00:17:07,400
And these would be jointly trained
with two different loss functions, one,

248
00:17:07,401 --> 00:17:09,390
four d intense.
And in one photo as loss,

249
00:17:10,520 --> 00:17:14,390
it's usually had fallen to
join key train two networks,

250
00:17:14,420 --> 00:17:17,900
especially in the earlier layers because
you end up learning the same type of

251
00:17:17,901 --> 00:17:21,890
features. That's, that's interesting.
For natural language processing.

252
00:17:23,410 --> 00:17:26,330
There's a yes. Have you do the
trek loss function for them?

253
00:17:26,620 --> 00:17:29,570
Is it calculate both losses and had
been deleted and some of them together,

254
00:17:29,720 --> 00:17:33,800
or is there a trade off between
findings versus finding spots?

255
00:17:34,520 --> 00:17:35,630
So the question is,

256
00:17:35,660 --> 00:17:38,690
how would you describe the loss
function in this joint training?

257
00:17:39,020 --> 00:17:41,990
He was actually some to loss functions.
The toolers functions you were using,

258
00:17:41,991 --> 00:17:42,060
using

259
00:17:42,060 --> 00:17:46,140
it with just some them and hope that's
the backpropagation we train actually

260
00:17:46,440 --> 00:17:51,360
both networks and the
networks will probably have a
common base and then we'd be

261
00:17:51,361 --> 00:17:52,350
separated after.

262
00:17:52,620 --> 00:17:57,620
So let's say you have a first LSTM layer
that encode some information about your

263
00:17:58,050 --> 00:18:01,440
user utterance.
Then this will give a,

264
00:18:01,890 --> 00:18:04,380
we give its output to
two different networks,

265
00:18:04,381 --> 00:18:08,550
which bill will be trained separately.
Okay.

266
00:18:08,580 --> 00:18:11,250
And class says here our
codes for the class quarter,

267
00:18:11,251 --> 00:18:14,580
you're and suid assuming suid
is already in the environment,

268
00:18:14,581 --> 00:18:16,320
we will not need to request it.

269
00:18:17,040 --> 00:18:21,240
So can you tell me how to acquire
this data now that we've seen it?

270
00:18:22,870 --> 00:18:23,641
So take,

271
00:18:23,641 --> 00:18:28,050
take about two minutes to discuss with
your mates how to acquire that type of

272
00:18:28,051 --> 00:18:31,950
data. And then on Sir on mentee. Okay.

273
00:18:33,300 --> 00:18:34,090
So let's go over

274
00:18:34,090 --> 00:18:38,440
some of the answers.
Mechanical Turk,

275
00:18:38,441 --> 00:18:41,320
have people manually collect,
annotate the data? That's true.

276
00:18:41,950 --> 00:18:44,200
So as we discussed earlier in the quarter,

277
00:18:44,201 --> 00:18:47,920
this would be the method which is
probably the more rigorous, uh,

278
00:18:48,160 --> 00:18:53,160
when it's applied with a specific labeling
process and data collection process.

279
00:18:54,550 --> 00:18:56,050
Uh,
it will take more time.

280
00:18:56,590 --> 00:18:59,800
So you would have to build a Ui,

281
00:19:00,090 --> 00:19:03,730
a user interface for them to be
able to label all these data,

282
00:19:03,820 --> 00:19:05,440
which is not trivial in general.

283
00:19:06,900 --> 00:19:10,680
Amazon Mechanical Turk pay large
number of Stanford students at works.

284
00:19:11,440 --> 00:19:12,273
Uh,

285
00:19:12,810 --> 00:19:17,700
have a human chat assistant service user
and entered the data in hand labeled

286
00:19:17,701 --> 00:19:20,370
data. Yeah, it, you can
start with a hand labeling,

287
00:19:20,371 --> 00:19:25,200
probably can auto generate some data by
substituting dead courses quarter and

288
00:19:25,201 --> 00:19:30,090
other types of Oh, that's a good idea. So
who wrote that? Someone wants to comment.

289
00:19:30,130 --> 00:19:30,963
Yeah.

290
00:19:41,300 --> 00:19:44,690
Yeah,
that's a good idea.

291
00:19:44,691 --> 00:19:47,270
So I repeat for the CBD students,

292
00:19:47,670 --> 00:19:50,860
um,
we already have a bunch of possible dates.

293
00:19:50,861 --> 00:19:55,600
We can easily find a list of dates. You've
done it in one assignment rights, uh,

294
00:19:55,990 --> 00:19:59,650
where you were using the neural machine
translation to transfer for human

295
00:19:59,651 --> 00:20:01,570
readable dates to machine readable dates.

296
00:20:01,990 --> 00:20:05,350
So we have data sets of
dates so we could use that.

297
00:20:05,780 --> 00:20:10,780
We also have a list of course is that
we can probably find on explore courses.

298
00:20:11,680 --> 00:20:15,970
Uh, we know that there are
not too many quarters and,

299
00:20:16,270 --> 00:20:17,080
and we are,

300
00:20:17,080 --> 00:20:21,350
we have probably databases for any other
tag like least of possible Su ids or

301
00:20:21,370 --> 00:20:24,280
like seven figures, something like
that. So all numbers of selling,

302
00:20:24,290 --> 00:20:27,940
two years hopefully. Um, and
then we can have sentences.

303
00:20:27,941 --> 00:20:32,941
We'd like blank spots where we insert
these and we can generate a lot of data

304
00:20:33,970 --> 00:20:38,110
using these insertions scheme automated.
And every time we insert we can label.

305
00:20:38,140 --> 00:20:40,990
We're going to see that.
Um,

306
00:20:43,000 --> 00:20:44,200
I like these ideas.
Well,

307
00:20:44,230 --> 00:20:48,160
user part of speech Tagger I did
three commission to identify examples,

308
00:20:48,161 --> 00:20:51,280
requests that are found elsewhere. So, uh,

309
00:20:51,760 --> 00:20:56,530
one thing we discussed in section is
that you have available models to do part

310
00:20:56,531 --> 00:20:59,980
of speech tagging, right?
So why don't we use them.

311
00:20:59,981 --> 00:21:04,981
These are trained really well and we
could give our user Altran sees that we

312
00:21:05,081 --> 00:21:07,070
collected online,
uh,

313
00:21:07,300 --> 00:21:10,450
and tag them automatically
using these good models.

314
00:21:11,020 --> 00:21:12,580
Of course he's not going to be perfect,

315
00:21:12,640 --> 00:21:17,170
but we can at least get started with dots
and leverage a model that someone else

316
00:21:17,171 --> 00:21:20,230
has built to tag and label our data set.

317
00:21:22,300 --> 00:21:23,680
Okay.
Good ideas here.

318
00:21:29,370 --> 00:21:32,340
So let's see the data generation process,

319
00:21:32,341 --> 00:21:36,360
which is the most strategy to
start with. I would say, uh,

320
00:21:37,860 --> 00:21:42,750
we would have talking about the, the
flight booking, a virtual assistance,

321
00:21:43,080 --> 00:21:46,170
we would have a database of
all the departure locations,

322
00:21:46,650 --> 00:21:50,830
so whatever, Paris, London, uh,

323
00:21:51,000 --> 00:21:54,210
Kuala Lumpur and a lot
of our eyeballs as well.

324
00:21:54,390 --> 00:21:57,660
So these are list of cities that
have airports probably in the world.

325
00:21:59,130 --> 00:22:04,130
And we will have a list of way to right
days and also class business Eiko Eiko

326
00:22:05,281 --> 00:22:09,110
plus premium, I don't know,
whatever you want, uh,

327
00:22:09,210 --> 00:22:13,020
and user utterances. And then what we
will do is that we will pool a user,

328
00:22:13,021 --> 00:22:15,840
a trans from the database
such as this one.

329
00:22:15,870 --> 00:22:20,070
I would like to book a flight
from depth to our Ivo for,

330
00:22:21,160 --> 00:22:24,810
uh, in, in, in business class,
let's say in class for this date.

331
00:22:26,430 --> 00:22:29,850
And then we can plug in
from the to set randomly

332
00:22:31,970 --> 00:22:36,870
the slots. Does that make sense? We can
generate a lot of data using this process.

333
00:22:36,871 --> 00:22:41,871
So this user attributes can be augmented
in virtually tens or hundreds of

334
00:22:44,431 --> 00:22:46,470
different combinations.

335
00:22:49,290 --> 00:22:52,710
So that's one way to augment your
data set automatically. Label it,

336
00:22:52,770 --> 00:22:57,770
but you also need hand labeled data
because you don't want your model to over

337
00:22:58,951 --> 00:23:03,680
feet to this specific type
of user utterances. Okay.

338
00:23:04,960 --> 00:23:05,793
And so,

339
00:23:07,150 --> 00:23:11,280
so same for our virtual assistant for the,
for,

340
00:23:11,300 --> 00:23:12,960
for the university.
Hi.

341
00:23:12,970 --> 00:23:17,340
I want to enroll in code for quarter
year and then we can insert from the

342
00:23:17,341 --> 00:23:19,230
database the quarter,
the year,

343
00:23:19,350 --> 00:23:23,670
the code of different classes so that
we can train our network on that.

344
00:23:24,900 --> 00:23:26,760
Those, the state
augmentation. Makes Sense.

345
00:23:28,980 --> 00:23:32,160
So these are common tricks you
would see in invoice papers.

346
00:23:32,161 --> 00:23:36,510
And this is an example of one of them.
Okay.

347
00:23:36,570 --> 00:23:40,440
So we can label automatically when
inserting and we can train a sequence to

348
00:23:40,441 --> 00:23:43,080
sequence model in order
to fill in the slots.

349
00:23:45,650 --> 00:23:50,540
Okay. So let's go on mentee
and start the competition,

350
00:23:50,750 --> 00:23:54,620
which is the the most fun.
Okay.

351
00:23:54,980 --> 00:23:57,530
So let's get back to to to where we were.

352
00:23:57,590 --> 00:24:01,280
We have a chat bot that he's able to
answer for sure. I just enrolled you.

353
00:24:01,281 --> 00:24:03,980
The way he does that is that
it receives the user a trends.

354
00:24:03,981 --> 00:24:07,970
I want to enroll in cs one six
eight winter 2019 to learn coding.

355
00:24:08,000 --> 00:24:13,000
It identifies the intent of the
user using a sequence classifier,

356
00:24:13,100 --> 00:24:16,460
same type of network as you've
built for the Mog fi assignment.

357
00:24:16,970 --> 00:24:21,970
And then it also runs another
algorithm which will fill in the slots.

358
00:24:22,550 --> 00:24:26,390
And here we have all the slots need.
We have the code for the class,

359
00:24:26,540 --> 00:24:30,470
we have the quarter, and we have the year.
Those tonight ID is implicitly given.

360
00:24:30,680 --> 00:24:32,150
So we able to enroll two,

361
00:24:32,151 --> 00:24:36,020
enroll the students by calling
access with all these slots. Dot.

362
00:24:36,650 --> 00:24:40,430
Now let's make it a little
more complicated. Let's
say the students say, hi,

363
00:24:40,431 --> 00:24:44,420
I want to enroll in cs one oh
six eight two to learn coding.

364
00:24:45,650 --> 00:24:49,460
So the difference between these assurance
and the previous one example one is

365
00:24:49,461 --> 00:24:53,420
that you don't have all
the slots you identify, uh,

366
00:24:53,450 --> 00:24:57,470
with your slot stagger.
That's [inaudible] is Dakota of a class,

367
00:24:57,650 --> 00:24:59,510
but you don't know the quarter,
you don't know the year.

368
00:24:59,960 --> 00:25:02,590
So you probably want your
chat Bot to get back to the,

369
00:25:02,620 --> 00:25:06,530
to the student and say for which
quarter would you like to enroll? Right?

370
00:25:07,730 --> 00:25:11,600
And then the student would hopefully
say winter 2000, 19 or winter.

371
00:25:11,600 --> 00:25:16,130
And then you'd have to ask for the year
2019 and finally you can say for sure I

372
00:25:16,131 --> 00:25:16,964
just enrolled you.

373
00:25:17,240 --> 00:25:21,110
So we're not making any assumption
here on natural language generation.

374
00:25:21,111 --> 00:25:24,500
You've worked on a Shakespeare assignment
where you generate Shakespeare like

375
00:25:24,800 --> 00:25:25,633
sentences.

376
00:25:25,880 --> 00:25:29,780
In fact a good chat Bot would have
this feature of generating language,

377
00:25:30,020 --> 00:25:31,000
but for our purposes,

378
00:25:31,001 --> 00:25:34,190
which can just hard code that when you're
able to enroll the students you just

379
00:25:34,191 --> 00:25:35,660
say,
I just enrolled you.

380
00:25:35,960 --> 00:25:38,630
When you were able to retrieve
information from the students,

381
00:25:38,631 --> 00:25:39,464
you would just write,

382
00:25:39,470 --> 00:25:43,820
here is some information and you would
plug in whatever the explore courses API

383
00:25:44,360 --> 00:25:47,060
sent back in a Jason.
Okay,

384
00:25:47,540 --> 00:25:52,430
so here the idea is these students
archer and cannot be understood without

385
00:25:52,431 --> 00:25:53,264
context.

386
00:25:53,560 --> 00:25:58,560
There is no way to understand winter 2019
if you don't have a context management

387
00:25:59,001 --> 00:26:01,040
system.
Does it make sense?

388
00:26:01,970 --> 00:26:04,280
So we want to build that
context management system

389
00:26:07,220 --> 00:26:10,910
and then the question is how to handle
context. So there is a, there is many,

390
00:26:10,940 --> 00:26:13,970
there are many ways to do that and people
are still searching for the best ways.

391
00:26:14,390 --> 00:26:18,620
One way is to handle it with reinforcement
learning. As we mentioned earlier,

392
00:26:18,980 --> 00:26:21,110
another way,
which is quite intuitive and,

393
00:26:21,140 --> 00:26:25,450
and closer to what we've seen
together in sequence model. Uh,

394
00:26:25,550 --> 00:26:30,200
in the module in the module five is, uh,
these type of architectures, which is,

395
00:26:30,350 --> 00:26:34,730
which is taken from channels all
entwined memory networks with knowledge

396
00:26:34,790 --> 00:26:37,820
carryover for multiterm
spoken language understanding.

397
00:26:37,821 --> 00:26:41,780
So now you're able to understand what
multi-term means and twin memory networks.

398
00:26:41,781 --> 00:26:46,781
So what happens here just to it is we
will save all the history of trances.

399
00:26:47,940 --> 00:26:49,680
It means from the beginning
of the conversation,

400
00:26:49,681 --> 00:26:54,681
we will record all the utterances and
messages exchanged between the user and

401
00:26:55,070 --> 00:26:59,910
the, the assistant will keep eating
the storage that we recall. History,

402
00:26:59,970 --> 00:27:03,900
our transits c is the current insurance.

403
00:27:03,960 --> 00:27:08,560
So let's say the students says winter
2019, this is the utterance of the,

404
00:27:08,600 --> 00:27:13,260
the student.
At this point we will run this Si,

405
00:27:13,500 --> 00:27:15,670
uh, and of course like it's, it's,

406
00:27:16,230 --> 00:27:20,520
this entrance would be around into an RNN
and we will get back to an encoding of

407
00:27:20,521 --> 00:27:21,354
the sentence.

408
00:27:21,630 --> 00:27:24,900
So there is all the like word embedding
stuff that I don't describe what you

409
00:27:24,901 --> 00:27:29,080
guys are used to it. So we use word
embeddings, we were running to uh,

410
00:27:29,160 --> 00:27:34,160
we run into an RNN and we get back the
encoding of the user attributes and these

411
00:27:34,621 --> 00:27:38,610
and CUNY will then be compared to what
we have in memory. So all of the user,

412
00:27:38,611 --> 00:27:42,660
our truancy is that we had in memory
are also going to be running an RNN that

413
00:27:42,661 --> 00:27:44,940
will encode their information in vectors.

414
00:27:46,680 --> 00:27:51,680
These vectors are going to be put
in a memory representation and are

415
00:27:54,000 --> 00:27:57,090
you will be directly inner product.

416
00:27:57,120 --> 00:28:01,980
We will have an inner product from are
you with all the memories and this pooled

417
00:28:01,981 --> 00:28:06,900
into a softmax will give us a vector of
attention that you guys should be used

418
00:28:06,901 --> 00:28:09,390
to. Now, knowledge,
attention, distribution,

419
00:28:09,900 --> 00:28:12,450
telling us what the relation,

420
00:28:12,451 --> 00:28:16,410
where should we put our attention
in the memory for this specific,

421
00:28:17,680 --> 00:28:21,660
does that make sense?
So simple in our product.

422
00:28:21,700 --> 00:28:25,290
Softmax gives us a series of weights here.

423
00:28:28,050 --> 00:28:28,860
Okay.

424
00:28:28,860 --> 00:28:33,120
Then we get a weighted sum of all these
attention weights multiplied by your

425
00:28:33,121 --> 00:28:37,290
memory and it gives us a vector that
encodes the relevance of the memory

426
00:28:37,470 --> 00:28:40,260
regarding our current utterance.

427
00:28:41,660 --> 00:28:46,660
This is then psalms and run into a simple
matrix multiplication to get an output

428
00:28:48,751 --> 00:28:51,090
vector, which would be around
in a slot stag in sequence.

429
00:28:51,120 --> 00:28:56,120
And usually it's experimental but they
pass also the current utterance to the

430
00:28:56,581 --> 00:29:01,290
RNN tiger and the RNN Togare comes up
with a slot tagging. So using that,

431
00:29:01,291 --> 00:29:06,291
you can understand that winter 2019 is
actually the tag for the slots quarter

432
00:29:08,340 --> 00:29:12,840
and year because you have this
memory network. Does it make sense?

433
00:29:14,910 --> 00:29:19,560
So this is another type of attention
models you want to use and these memory

434
00:29:19,561 --> 00:29:23,190
networks, it can be used to manage
some context for the slot Steiger.

435
00:29:26,070 --> 00:29:30,420
Okay, so just to recap,
we have our example. Hi,

436
00:29:30,421 --> 00:29:34,470
I want to enroll in a class and we
detect the intense which is enrolled.

437
00:29:34,950 --> 00:29:38,880
We also detect that there is some
slots missing because we know,

438
00:29:38,910 --> 00:29:42,190
we know that the enrollment function
needs the quarter, the year end,

439
00:29:42,191 --> 00:29:45,220
the class in order to
be able to be called.

440
00:29:45,880 --> 00:29:47,350
So we have to ask for those.

441
00:29:47,650 --> 00:29:50,740
So we probably hard good in the fact
that if you don't have the quarter,

442
00:29:50,920 --> 00:29:51,481
the year end,

443
00:29:51,481 --> 00:29:56,410
the class you probably want to first
ask for the class or the quarter or the

444
00:29:56,411 --> 00:29:57,730
year,
then you can,

445
00:29:57,790 --> 00:30:01,540
you can get back to the person by asking
which class do you want to enroll in?

446
00:30:03,010 --> 00:30:04,090
The person would get back to you.

447
00:30:04,091 --> 00:30:09,091
You will use your memory
network to understand that
cs two 30 is a slots for the

448
00:30:10,751 --> 00:30:13,210
enroll intense.
You would fill it in.

449
00:30:13,211 --> 00:30:15,420
So now we have our intent
with the class equals yes,

450
00:30:15,430 --> 00:30:19,510
two 30 and we have our slots quarter
and year, which are to be filled.

451
00:30:20,500 --> 00:30:23,980
The chatbot get bags for each quarter
and hopefully the student gives you the

452
00:30:23,980 --> 00:30:26,890
year at the same time and
you can fill in the slots.

453
00:30:28,060 --> 00:30:31,660
And then you are enrolled
in cs two 34 winter 2019

454
00:30:33,580 --> 00:30:37,990
yeah, she'd be spring. Yeah, he's shy,

455
00:30:37,991 --> 00:30:42,970
but he's not trained very way.
Okay.

456
00:30:43,330 --> 00:30:46,630
Any questions on that?
So these are very simple case of a,

457
00:30:46,690 --> 00:30:50,200
of a conversational assistant.
Just to give you some ideas.

458
00:30:50,350 --> 00:30:54,280
There's some paper listed
in the presentation that you
can go to in order to get

459
00:30:54,281 --> 00:30:58,910
more advanced, uh, research insights. Uh,

460
00:30:58,960 --> 00:31:03,570
but the idea here is that we're limited
to a specific intense to two specific

461
00:31:03,590 --> 00:31:04,840
intense and a few slots.

462
00:31:05,350 --> 00:31:09,190
What do you think we would need if we
didn't restrict ourselves to specific

463
00:31:09,191 --> 00:31:10,270
intense and slots?

464
00:31:25,640 --> 00:31:26,570
It's a very complicated,

465
00:31:27,870 --> 00:31:32,120
one industrial way to do it
is to use a knowledge graph.

466
00:31:34,190 --> 00:31:36,620
What do you mean?
Let's say you were an ecommerce platform.

467
00:31:37,190 --> 00:31:42,190
You probably have from your platform or
knowledge draft oil of all the items on

468
00:31:42,981 --> 00:31:47,840
the platform. We've connections
among them. Like let's say color of,

469
00:31:48,200 --> 00:31:52,310
let's say you have a shoe,
a shoe is lost.

470
00:31:52,311 --> 00:31:56,180
That might be the objective for the
intense, I want to buy something,

471
00:31:57,080 --> 00:31:57,913
right?

472
00:31:58,010 --> 00:32:02,990
The shoe can have several attributes
like color or size or men or women like

473
00:32:02,991 --> 00:32:07,240
gender and all these are connected
together in a gen, in a, in a, in a,

474
00:32:07,340 --> 00:32:09,650
in a gigantic knowledge graph.

475
00:32:10,460 --> 00:32:13,460
And you will follow the path of
this knowledge graph following some

476
00:32:13,461 --> 00:32:18,440
probabilities, probabilities. So when
we detect the intent of the user,

477
00:32:18,441 --> 00:32:20,360
which is buy something,

478
00:32:21,110 --> 00:32:24,740
we could identify the object,
I want to buy a shoe.

479
00:32:25,070 --> 00:32:26,690
And then based on our knowledge graph,

480
00:32:26,720 --> 00:32:29,270
it says that the next
question that we should ask,

481
00:32:29,271 --> 00:32:34,271
or the next slots that we need to feel
is a which brand you want your shoe to

482
00:32:34,341 --> 00:32:35,174
be.

483
00:32:35,270 --> 00:32:40,070
And so the knowledge graph is going to
tell you we'd 60% probability go to brand

484
00:32:40,071 --> 00:32:42,380
and ask about the brand once you're there,

485
00:32:42,381 --> 00:32:47,150
what other information you need in order
to be able to retrieve five results for

486
00:32:47,151 --> 00:32:49,880
the user to review and so on.

487
00:32:50,480 --> 00:32:52,260
So the knowledge graph
is something industrial.

488
00:32:52,320 --> 00:32:56,990
It can be used in order to have multiple
intense multiple slots for every

489
00:32:56,991 --> 00:33:00,350
intense.
Okay.

490
00:33:00,380 --> 00:33:05,380
And at the end we can make an APA call
hair we see as two 30 quarter winter 2019

491
00:33:05,480 --> 00:33:08,420
quarter winter year 2019.
And the suid.

492
00:33:10,340 --> 00:33:14,670
Okay. Another question I had for you, I,

493
00:33:14,671 --> 00:33:15,740
I've had for you,

494
00:33:15,770 --> 00:33:19,490
I have for you is how to evaluate
the performance of the Chat Bot.

495
00:33:21,050 --> 00:33:21,950
What do you think of that?

496
00:33:33,700 --> 00:33:38,320
So there are common ways to, to, to
evaluate several parts of your pipeline.

497
00:33:38,350 --> 00:33:43,330
Like how is your slot tiger doing?
How is your intent classifier?

498
00:33:43,340 --> 00:33:43,720
Duke,

499
00:33:43,720 --> 00:33:48,720
you can use metrics such as precision and
recall f one score does a mix of both,

500
00:33:50,080 --> 00:33:50,500
uh,

501
00:33:50,500 --> 00:33:55,500
and report those in order to compare how
this module is doing for the Chat Bot.

502
00:33:56,890 --> 00:34:00,430
But ultimately you want to understand
how good is your chat bot overall.

503
00:34:01,060 --> 00:34:05,290
So some experiments are done and this
is a paper of a deeper enforcement

504
00:34:05,291 --> 00:34:09,940
learning chatbots built in 2017
by the Miller, uh, uh, on, at all.

505
00:34:10,300 --> 00:34:14,920
And what they did is that they used
mechanical Turk in order to evaluate their

506
00:34:14,921 --> 00:34:18,500
chat Bot and also build a scoring system
for their reinforcement learning chat

507
00:34:18,501 --> 00:34:21,430
Bot. So I'm reading for
you the instructions. Uh,

508
00:34:21,820 --> 00:34:25,840
you will be presented with a conversation
between two speakers, speaker a and B.

509
00:34:26,170 --> 00:34:29,470
You will also be presented with four
potential responses from one of the

510
00:34:29,471 --> 00:34:30,730
speakers for this dialogue.

511
00:34:30,970 --> 00:34:35,970
And the task is for you to reach
each of the responses between one in

512
00:34:36,641 --> 00:34:37,450
appropriate.

513
00:34:37,450 --> 00:34:41,830
It doesn't make sense to five highly
appropriate and interesting based on how

514
00:34:41,831 --> 00:34:46,180
appropriate the response is to continue
the conversation. Three is neutral.

515
00:34:46,900 --> 00:34:50,740
And uh,
if two responses are equally appropriate,

516
00:34:50,770 --> 00:34:52,660
you should give them the same score.

517
00:34:52,960 --> 00:34:56,440
And if you see response that is not
in English, please give a one score.

518
00:34:57,430 --> 00:35:02,110
So here's what happens from
a user perspective, you
would have a conversation,

519
00:35:02,140 --> 00:35:06,290
you need to work on your English.
Why do you say that?

520
00:35:06,320 --> 00:35:09,550
That's about me. Well,
your English is very poor.

521
00:35:10,720 --> 00:35:14,470
So this is a conversation.
And then the response one is,

522
00:35:14,500 --> 00:35:17,800
but English is my native language.
Response to is.

523
00:35:17,801 --> 00:35:20,350
What other reasons come to mind responds?

524
00:35:20,410 --> 00:35:23,830
Three is here is a funny facts.

525
00:35:24,280 --> 00:35:28,480
Go is the shortest complete
sentence in the English language.

526
00:35:28,900 --> 00:35:32,760
And then the fourth, the
responses by doggy. Yeah.

527
00:35:34,000 --> 00:35:38,730
So if you see you have to, you have
to score, uh, you have to score these,

528
00:35:38,731 --> 00:35:42,510
uh, these responses according to what
you think, how relevant they are.

529
00:35:43,080 --> 00:35:44,420
And then,
uh,

530
00:35:45,960 --> 00:35:49,500
and then these scores will be used either
for the scoring system of the deeper

531
00:35:49,501 --> 00:35:53,220
enforcement learning chatbots or it can
be used to evaluate how good is your

532
00:35:53,221 --> 00:35:54,820
chat Bot compared to other chatbots.

533
00:35:54,870 --> 00:35:59,010
So maybe each of these responses
come from a different model.

534
00:36:00,050 --> 00:36:05,040
Does that make sense?
So these are a few ways.

535
00:36:05,690 --> 00:36:06,810
They are not under way,

536
00:36:07,020 --> 00:36:11,890
which is asking for the opinion
of the user on a different uh,

537
00:36:12,020 --> 00:36:16,250
responses.
So let's say you are a user and um,

538
00:36:16,590 --> 00:36:20,190
you are,
you are comparing to chatbots.

539
00:36:20,580 --> 00:36:23,430
You can give your opinion on which
one you think is more natural.

540
00:36:23,670 --> 00:36:25,290
And you would ask a lot
of users to do that,

541
00:36:25,291 --> 00:36:29,670
to compare two or three chatbots
together and also compare them to natural

542
00:36:29,671 --> 00:36:33,620
language from human.
And then by doing a lot of uh,

543
00:36:34,020 --> 00:36:36,900
mean opinion score, uh, experiments,

544
00:36:36,901 --> 00:36:39,510
you can evaluate which chatbots
are better than the others.

545
00:36:39,960 --> 00:36:43,470
Just comparing them one on one.
Okay.

546
00:36:44,640 --> 00:36:49,640
Now getting back to one thing that a
student mentioned earlier is what if we

547
00:36:50,071 --> 00:36:54,300
want to have a vocal assistance? So
right now, or as you said is not vocal,

548
00:36:54,330 --> 00:36:55,163
it's just text.

549
00:36:55,650 --> 00:36:58,890
What other things do we need to build
in order to make it a vocal assistance?

550
00:37:04,470 --> 00:37:08,350
We're not going to go into in the details,
but roughly you would need a uh,

551
00:37:08,380 --> 00:37:10,560
a speech to text system,

552
00:37:11,850 --> 00:37:14,730
which will take the voice of
the user converted into a text.

553
00:37:15,240 --> 00:37:19,410
And these ads you've seen in the sequence
model class has different step in the

554
00:37:19,411 --> 00:37:20,900
pipeline,
uh,

555
00:37:20,910 --> 00:37:25,320
and the speech to text and the text
to speech that takes the text from the

556
00:37:25,330 --> 00:37:27,570
chatbots and converted into a voice.

557
00:37:28,050 --> 00:37:31,860
So that's how you have like virtual
assistants talking to us is because they

558
00:37:31,861 --> 00:37:35,760
have a text to speech system running.
And these are three papers.

559
00:37:35,761 --> 00:37:39,300
The first one is the speech
to from Baidu seem, uh,

560
00:37:39,430 --> 00:37:42,300
which built an end to end speech
recognition in English and Mandarin.

561
00:37:42,660 --> 00:37:45,750
And the two others are
text to speech synthesis.

562
00:37:46,290 --> 00:37:50,680
So one came up in February, 2018,
which is the TACO truck two.

563
00:37:51,150 --> 00:37:54,390
And the second one is wavenet,
which is a very popular generative models.

564
00:37:54,780 --> 00:37:59,720
And these are, these are far
beyond the scope of the class. Uh,

565
00:37:59,760 --> 00:38:02,910
but uh, you can study them
in other classes at Stanford,

566
00:38:02,911 --> 00:38:07,350
which are more specific to speech.
Okay. Cut Class project advice.

567
00:38:07,351 --> 00:38:10,630
So this Friday we're going to go over,
uh,

568
00:38:10,860 --> 00:38:15,090
again the rubrics of
what we look at when we,

569
00:38:15,150 --> 00:38:19,320
when we grade projects. And here is
the list of things we would look at.

570
00:38:19,770 --> 00:38:24,060
Uh, so make sure you have a very good
problem description. When you read papers,

571
00:38:24,061 --> 00:38:25,710
you see that there is
a very good abstract.

572
00:38:25,711 --> 00:38:28,560
We expect you to give us a very good
abstract so that when we read it,

573
00:38:28,890 --> 00:38:32,550
we get a good understanding of the paper,
a hyper parameter tuning,

574
00:38:32,580 --> 00:38:36,520
always report what you do.
You don't need to to be very exhaustive,

575
00:38:36,521 --> 00:38:41,140
but what you can just tell us what hyper
parameters you've been choosing and

576
00:38:41,141 --> 00:38:46,070
which ones you've been testing and why
they didn't work. Um, the writing, uh,

577
00:38:46,240 --> 00:38:50,230
we look for typos. This is common
in integrating scheme typos,

578
00:38:50,380 --> 00:38:55,300
a clear language. Uh, so review
it, uh, peer review your paper,

579
00:38:55,870 --> 00:38:58,690
uh, explanation of choice in this year.
And this is a very important part.

580
00:38:59,020 --> 00:39:02,470
We expect you to explain, uh,
the decisions you're making.

581
00:39:03,130 --> 00:39:05,980
So we don't want you to,
to tell us I've taken,

582
00:39:06,010 --> 00:39:08,680
I've made that decision
just without explaining,

583
00:39:08,890 --> 00:39:13,060
but rather tell us there is this paper
that mentioned that this architecture

584
00:39:13,061 --> 00:39:17,290
worked well on that specific task.
Uh, I've tried three architectures.

585
00:39:17,291 --> 00:39:20,530
Here are my hyper parameters and results.
That's why I'm going to,

586
00:39:20,890 --> 00:39:22,480
I'm going to dig more into that one.

587
00:39:22,720 --> 00:39:27,190
And so on a data cleaning and
preprocessing if applicable
to your pro project,

588
00:39:27,220 --> 00:39:30,340
explain it, uh, how much
code you wrote on your own.

589
00:39:30,370 --> 00:39:35,370
It's important to us and please submit
your guitar or privately to the Tas when

590
00:39:36,371 --> 00:39:40,570
you submit your is going to make it
easier for us to review the code. Um,

591
00:39:41,350 --> 00:39:44,170
insights and discussions
include the next steps.

592
00:39:44,200 --> 00:39:46,990
What would you have done
if you had more time? Uh,

593
00:39:46,991 --> 00:39:51,220
and also interpret your results. Don't
just give results without explanation,

594
00:39:51,221 --> 00:39:54,430
but rather tried to extract
information from these results.

595
00:39:55,540 --> 00:40:00,280
And you can also drive your next
steps. Explanation, uh, residents.

596
00:40:00,290 --> 00:40:04,240
So important. But if you don't have
the results you expected, it's fine.

597
00:40:04,241 --> 00:40:07,660
We will look at how much work you've
done and some tasks are very complicated.

598
00:40:07,870 --> 00:40:11,050
We don't expect you to beat state
of the art on every single task.

599
00:40:11,350 --> 00:40:14,610
Some of you are going to meet
state of the art, hopefully, uh,

600
00:40:14,620 --> 00:40:16,450
but those of you who didn't steal,

601
00:40:16,451 --> 00:40:20,620
report all your results and
explain why it didn't work. Uh,

602
00:40:20,800 --> 00:40:24,280
give references and also penalty
for more than five pages.

603
00:40:24,460 --> 00:40:28,180
So if you're working on a,
on a theoretical project,

604
00:40:28,181 --> 00:40:32,410
you can add additional
pages as appendix a.

605
00:40:32,410 --> 00:40:36,760
You can also add an appendix
for your project, but the
core has to be five pages.

606
00:40:37,580 --> 00:40:41,950
Uh, and for the final poster presentation,
which will happen not this Friday.

607
00:40:41,951 --> 00:40:46,480
Next one, uh, we will ask you to
pitch your project in three minutes.

608
00:40:47,050 --> 00:40:50,380
So not everyone in the group has to talk,
but at least one person has to talk.

609
00:40:50,381 --> 00:40:54,100
And, and we prefer if several
of you talk in the project,

610
00:40:54,430 --> 00:40:56,210
but you have three minutes
to pitch your project.

611
00:40:56,211 --> 00:41:00,790
So prepared the pitch in advance and you
will have two minutes of questions from

612
00:41:00,791 --> 00:41:03,850
the ta, which are also
part of the grading. Okay.

613
00:41:05,770 --> 00:41:10,030
Finally, what's next after cs two 30.
So there's a ton of class at Stanford.

614
00:41:10,031 --> 00:41:13,970
We're in a good learning
environment, which is just super, uh,

615
00:41:14,290 --> 00:41:18,820
next steps can be in the university
classes you can take in natural language

616
00:41:18,821 --> 00:41:22,360
processing and computer vision,

617
00:41:22,390 --> 00:41:27,370
but also classes from
different departments. Uh,
deep generative models is a,

618
00:41:27,371 --> 00:41:32,371
a good way to learn about text
to speech for example or Ganz.

619
00:41:32,840 --> 00:41:36,150
Uh, probably see graphical models is
also a very important class in the,

620
00:41:36,160 --> 00:41:39,470
in the CS department.
Of course if you haven't taken it yet,

621
00:41:39,500 --> 00:41:41,900
cs two to nine machine
learning or cs two tonight,

622
00:41:42,530 --> 00:41:46,130
applied machine learning or uh, do
go to, to learn machine learning.

623
00:41:46,430 --> 00:41:48,470
Reinforcement learning
is a class where you can,

624
00:41:48,830 --> 00:41:53,720
you can delve more into a cue learning
policy gradients and all these methods

625
00:41:54,740 --> 00:41:56,210
that sometime use deep learning.

626
00:41:57,530 --> 00:42:00,530
So we're going to publish that
list in case you want to check it.

627
00:42:00,800 --> 00:42:03,650
But these are examples of classes you
can take an of course there are other

628
00:42:03,651 --> 00:42:08,400
classes that tournament not mentioned
here that might be relevant to pursue. Uh,

629
00:42:08,450 --> 00:42:13,220
you're learning in, in deep deep
learning and machine learning. Okay.

630
00:42:13,250 --> 00:42:14,960
That said, uh, I'm willing to,

631
00:42:15,070 --> 00:42:19,460
to give the microphone to Andrew
for closing remarks and a,

632
00:42:19,580 --> 00:42:22,130
and yeah, good luck on your projects. Uh,

633
00:42:22,160 --> 00:42:26,660
so we'll see you on Friday for the
discussion sections and next week for the

634
00:42:26,720 --> 00:42:29,710
final project. Do you have
your microphone? Sure.

635
00:42:37,200 --> 00:42:42,140
Yeah. So all right, here we
are at the end of this class.

636
00:42:42,260 --> 00:42:46,520
Oh yeah. The, at the end of
this class, um, you know,

637
00:42:46,530 --> 00:42:50,330
dia new reps conference, um,
uh, is taking place right now.

638
00:42:50,331 --> 00:42:52,010
Formerly the nips conference up,

639
00:42:52,011 --> 00:42:56,570
I renamed to new ropes and I
remember it was a 10 years ago that,

640
00:42:56,630 --> 00:43:01,280
um, at that time a phd student in the
Diet or ratio Rainer presents the paper

641
00:43:01,310 --> 00:43:03,860
workshop paper in Nips,
uh, telling people, hey,

642
00:43:04,010 --> 00:43:06,470
cause they're using gps and crew there,

643
00:43:06,471 --> 00:43:10,610
which is a new thing that Nvidia just
publish, um, to train your networks.

644
00:43:10,670 --> 00:43:15,610
And we've done that work on a
GPU server that Ian Goodfellow,

645
00:43:15,680 --> 00:43:18,860
the creative gans how
built in his dorm room, um,

646
00:43:19,160 --> 00:43:20,750
when he was an Undergrad at Stanford.

647
00:43:20,810 --> 00:43:25,130
So our first few few serve was built
into Stanford undergrads dorm room. Um,

648
00:43:25,790 --> 00:43:27,770
and, um, uh,

649
00:43:28,190 --> 00:43:31,430
I remember sitting down with Jeff
Hinton and a food court and saying, hey,

650
00:43:31,431 --> 00:43:35,030
check out this crew the thing. And Jeff
said, ah, but GPU program is really hot.

651
00:43:35,050 --> 00:43:39,250
But then, but then, but, but oh, maybe
this glue, the thing looks promising. Um,

652
00:43:39,590 --> 00:43:44,590
and I tell this story because I want you
to know as Stanford students that your

653
00:43:45,651 --> 00:43:49,520
work can Matta, right.
When Ian Goodfellow, um,

654
00:43:49,580 --> 00:43:52,450
build that GPU server in his dorm room.
Um,

655
00:43:53,120 --> 00:43:56,810
I had no idea if you realize
that a decade later, you know,

656
00:43:56,811 --> 00:44:00,670
someone would be winning several
hundred hours of AWS credits, uh, uh,

657
00:44:01,040 --> 00:44:04,030
to try and pick it up. Deep
Learning Algorithms. But, um,

658
00:44:04,580 --> 00:44:08,000
I think as Stanford here
at Stanford University,

659
00:44:08,001 --> 00:44:11,660
but very much at the heart
of the technology world,

660
00:44:11,960 --> 00:44:16,400
I think Silicon Valley is here to a
large pot because Stanford University is

661
00:44:16,401 --> 00:44:18,160
here. And, um,

662
00:44:18,200 --> 00:44:22,550
we live in a world where with the
superpowers that you now have,

663
00:44:22,910 --> 00:44:26,800
um, you have a lot of opportunities
to do new and exciting world,

664
00:44:26,870 --> 00:44:31,190
which may or may not seem like a match in
the short run, uh, maybe even seen, um,

665
00:44:31,260 --> 00:44:34,380
consequential in the short run be concerns
that you have a huge impact in the

666
00:44:34,381 --> 00:44:39,060
long run. Um, actually a
couple of weekends ago, so, um,

667
00:44:39,120 --> 00:44:42,120
my wife, uh, we roast
coffee beans at home, right?

668
00:44:42,121 --> 00:44:46,650
My wife buys raw coffee beans and
then we actually roast them and Carol,

669
00:44:46,651 --> 00:44:50,790
my wife tends to roast them and this is
really cheap popcorn popper that we have.

670
00:44:50,820 --> 00:44:53,910
Right. You know, so I don't have, I don't
know how much coffee you guys drink.

671
00:44:53,911 --> 00:44:58,590
I drink a lot of coffee and so, um, you
know, so carer bases being coffee beans,

672
00:44:58,770 --> 00:45:02,720
she puts them in this like cheap popcorn
popper, which me for popping popcorn,

673
00:45:02,730 --> 00:45:06,600
not me for roasting coffee beans is
one of the standard cheap ways to roast

674
00:45:06,601 --> 00:45:09,990
coffee regions and, and alumni way.
I drink the coffee, she makes her,

675
00:45:09,991 --> 00:45:12,870
sometimes she burns the
coffee views. Um, so, uh,

676
00:45:12,900 --> 00:45:17,900
I found this article on the Internet
from a former student that had written an

677
00:45:18,391 --> 00:45:21,400
article and how they use
machine learning to rose to, to,

678
00:45:21,410 --> 00:45:25,830
to optimize the roasting our coffee
beans. Um, and say Ford the did the Carol,

679
00:45:26,730 --> 00:45:31,600
she wasn't very happy about that,
but that I raise,

680
00:45:31,601 --> 00:45:36,300
this is another example of
how, um, uh, all of you,

681
00:45:36,330 --> 00:45:37,380
um,
you know,

682
00:45:37,410 --> 00:45:40,020
I would never have thought of applying
machine learning of the roasting coffee

683
00:45:40,021 --> 00:45:42,360
views. Uh, it's just, I mean, you know,

684
00:45:42,540 --> 00:45:45,570
I like my coffee but that it had
never occurred to me to do that.

685
00:45:46,140 --> 00:45:50,190
That someone taking a machine
learning class, um, like you guys are,

686
00:45:50,580 --> 00:45:52,530
go ahead and come up with a better,

687
00:45:52,600 --> 00:45:56,970
we're roasting coffee beans using
learning our ribbons. Um, and again,

688
00:45:57,000 --> 00:45:57,480
I think you would,

689
00:45:57,480 --> 00:46:00,330
I don't know if for compressing the
road is or was thinking of building a

690
00:46:00,331 --> 00:46:02,790
business, all of it, I don't know.
There might be a business there.

691
00:46:02,791 --> 00:46:06,400
They might not or it might be just a fun
personal hobby actually. Don't know. Um,

692
00:46:06,690 --> 00:46:11,070
but all of you with these skills
have that opportunity. And then,

693
00:46:11,480 --> 00:46:16,360
um, again, earlier this week,
was it Monday night, um,

694
00:46:16,860 --> 00:46:20,940
a group of us, uh, we were
actually in the gates building, um,

695
00:46:21,300 --> 00:46:23,260
where a bunch of students
actually from there, yeah.

696
00:46:23,261 --> 00:46:26,920
For Healthcare bootcamp
that Ken alluded to. Yeah.

697
00:46:26,940 --> 00:46:29,100
We're going over some of
the final projects for the,

698
00:46:29,130 --> 00:46:32,930
for the students and that you have to
healthcare bootcamp. Um, we're, we're,

699
00:46:32,931 --> 00:46:34,320
we're working on and I think,

700
00:46:34,321 --> 00:46:38,370
and I think I actually met several people
including our ti right when she first

701
00:46:38,371 --> 00:46:41,960
participated in much earlier version
of that Ui. Healthcare. We can see it,

702
00:46:41,961 --> 00:46:46,440
you can ask rt of others lives
interested. But there, um, one of the, uh,

703
00:46:46,560 --> 00:46:49,740
masters students who's working
with pcs in print of Raj broker,

704
00:46:49,790 --> 00:46:51,120
they think you guys been in this class.

705
00:46:51,540 --> 00:46:56,540
He was demoing an app where you
could pull up an x ray film,

706
00:46:57,510 --> 00:47:01,250
uh, and took a picture
with your cell phone. Um,

707
00:47:01,380 --> 00:47:05,940
upload the picture to a website
and have a website, you know,

708
00:47:05,941 --> 00:47:10,830
read the x ray and suggest a
diagnosis for, uh, for our patients.

709
00:47:11,140 --> 00:47:15,030
Uh, most of the planets today has
insufficient access to radiology services.

710
00:47:15,270 --> 00:47:19,770
There are many countries where it
costs you three months of salary, um,

711
00:47:20,070 --> 00:47:24,780
to go and get an x ray taken and then
maybe try to find the radiologist to read

712
00:47:24,781 --> 00:47:26,700
it. But most of the planet, um,

713
00:47:26,940 --> 00:47:30,850
billions of people on this planet do not
have sufficient services or radiology

714
00:47:30,851 --> 00:47:32,710
services. And, um,

715
00:47:32,980 --> 00:47:37,210
while the statuses and the AFA healthcare
bootcamp is still a research project,

716
00:47:37,211 --> 00:47:40,090
actually you recall Arthur on the checks
and that people weren't sure it yeah,

717
00:47:40,091 --> 00:47:43,720
right. Yes, I'll do, is share a coauthor
on, on, on, on when these papers, um,

718
00:47:44,260 --> 00:47:47,890
it is a game, maybe work done
here at Stanford that, you know,

719
00:47:47,891 --> 00:47:52,891
is taking the first steps to what maybe
if we can improve the deep learning

720
00:47:53,261 --> 00:47:56,920
algorithms, past rectory
hurdles, you know, proof safety,

721
00:47:58,030 --> 00:48:01,630
maybe that type of work.
Tapping, you're a Stanford,

722
00:48:01,830 --> 00:48:03,660
I'm doing half the healthcare.

723
00:48:03,670 --> 00:48:08,270
Maybe that will have a transformative
effect on how healthcare is run, um,

724
00:48:08,480 --> 00:48:11,460
around the world. So, um,

725
00:48:12,600 --> 00:48:15,820
the skills that you guys now have, uh, uh,

726
00:48:16,600 --> 00:48:18,190
are very unique set of skills.

727
00:48:18,191 --> 00:48:22,210
They're not that many people on the
planet today that can apply learning

728
00:48:22,211 --> 00:48:25,240
algorithms and deep learning
algorithms the way that you can.

729
00:48:25,600 --> 00:48:28,760
And you can tell a lot of the ideas you
learned in this course where you know,

730
00:48:28,780 --> 00:48:30,760
invented in the last year or two.

731
00:48:30,761 --> 00:48:33,940
So this is just not yet been time
for these ideas even come widespread.

732
00:48:34,510 --> 00:48:39,510
And if I look at a lot of
the most pressing problems
facing society be a lack of

733
00:48:39,521 --> 00:48:41,790
access to health care or um,

734
00:48:41,910 --> 00:48:45,830
scientists spent a lot of times think
about climate change. Um, uh, uh,

735
00:48:45,880 --> 00:48:50,080
and I think if we look at the, the,
can we improve access to education?

736
00:48:50,380 --> 00:48:53,700
Can we just make whole society
run more efficiently? Um,

737
00:48:54,070 --> 00:48:58,810
I think that all of you have the
skills to do very unique projects. Um,

738
00:48:58,870 --> 00:49:02,330
and I hope that as you graduate from
this class, I'm sure some of you are,

739
00:49:02,331 --> 00:49:04,720
we'll great businesses mean
all of the money. That's great.

740
00:49:04,760 --> 00:49:09,520
And I hope that all of you will also take
the unique skills you have to work on

741
00:49:09,521 --> 00:49:13,230
projects that, not to the most
to other people that, that,

742
00:49:13,231 --> 00:49:15,520
that hope other people.
Um,

743
00:49:15,670 --> 00:49:20,260
because if one of you does not take
your skills to do something meaningful,

744
00:49:20,261 --> 00:49:23,320
then there's probably some very meaningful
project that just no one is working

745
00:49:23,321 --> 00:49:26,800
on because I think the number
of meaningful projects, um,

746
00:49:27,280 --> 00:49:30,040
I think actually greatly exceeds the
number of people in the world today.

747
00:49:30,070 --> 00:49:31,390
That a skill,
that deep learning,

748
00:49:31,420 --> 00:49:36,300
which is why all of you have a unique
opportunity to take these algorithms and

749
00:49:36,301 --> 00:49:40,940
you now know about to apply to anything
from developing novel chatbots,

750
00:49:41,070 --> 00:49:45,910
um, uh, to improving health care
too. I guess my team are landing ais,

751
00:49:45,940 --> 00:49:49,640
improving manufacturing, agriculture,
also some health care, um,

752
00:49:49,930 --> 00:49:52,720
to maybe helping with climate change,
um,

753
00:49:52,870 --> 00:49:57,310
to helping with global education. Uh,
and, and, and the other problems that,

754
00:49:57,910 --> 00:50:02,740
that really matter. So I hope, I hope,
I hope that all of you go on, um,

755
00:50:03,280 --> 00:50:06,460
to, to do work that matters. Um,

756
00:50:07,000 --> 00:50:10,420
and then one last story.
Um, you know, a few,

757
00:50:11,140 --> 00:50:15,520
few months ago now, um, I got to drive
a tractor, right? It was very vague.

758
00:50:15,880 --> 00:50:20,140
Lovin scary. If he was like
a big a machine, then I
should be qualified to drive.

759
00:50:20,650 --> 00:50:24,670
Um, is, is a huge tractor and it turns
out that when you drive a tractor,

760
00:50:24,940 --> 00:50:27,500
so it turns out when you
drive a normal car, you know,

761
00:50:27,501 --> 00:50:30,260
it's really clear which way is up
by a steering wheel, right? Yeah.

762
00:50:30,261 --> 00:50:34,240
You point us to ring up and heal
your car drives forward, uh,

763
00:50:34,490 --> 00:50:37,160
for the tractor that I got
to drive this huge tractor.

764
00:50:37,340 --> 00:50:41,990
It turns out there is this giant
steering wheel. And to drive straight,

765
00:50:41,991 --> 00:50:45,140
the giant steering wheel was just
oriented. That's a weird angle.

766
00:50:45,410 --> 00:50:48,880
And to turn right, you turn it
clockwise to turn it up your turn.

767
00:50:48,900 --> 00:50:52,760
Anticlockwise and Dell, is that
right? So there's a lot of fun. Um,

768
00:50:53,390 --> 00:50:58,200
and maybe in addition to, uh, uh,
and, and it was just fun. You know,

769
00:50:58,490 --> 00:51:02,190
I drove a tractor, made a uterine,
drove back to where I started,

770
00:51:02,210 --> 00:51:06,710
did not hit anyone, no accident. And then
I climbed down off this giant tractor.

771
00:51:07,170 --> 00:51:10,820
Um, and maybe I tell that
story because, uh, uh,

772
00:51:10,850 --> 00:51:15,770
I hope that even while you are
doing this important, uh, uh,

773
00:51:15,800 --> 00:51:18,590
maybe beneficial to other people
sense of work, um, I hope,

774
00:51:18,591 --> 00:51:19,910
I hope you also have fun.

775
00:51:20,000 --> 00:51:24,990
I think that I feel really privileged
that as a machine learning engineer, um,

776
00:51:25,250 --> 00:51:29,650
I, some days I get to go drive
a tractor. Right. Uh, and,

777
00:51:29,660 --> 00:51:34,580
and I hope that, um, and one of the
most exciting things, um, you know,

778
00:51:35,730 --> 00:51:39,860
I feel like, um, uh, a lot of the best,

779
00:51:39,900 --> 00:51:43,520
a lot of the biggest untapped
opportunities for AI like old side,

780
00:51:43,530 --> 00:51:47,030
the software industry. Um, I'm very
proud of the work that help do, you know,

781
00:51:47,031 --> 00:51:49,130
leading the group rain team Vdi.
I do.

782
00:51:49,131 --> 00:51:53,550
And I think more people should do that
type of work. Um, and I think that, um,

783
00:51:53,900 --> 00:51:54,920
here in silicon valley,

784
00:51:54,921 --> 00:51:58,490
many of you will get jobs in the
tech sector and that's great.

785
00:51:58,491 --> 00:51:59,690
We need more people to do that.

786
00:52:00,170 --> 00:52:04,220
And I also think that if you
look at all of human activity,

787
00:52:04,250 --> 00:52:07,430
the majority of human activity is
actually outside the software industry.

788
00:52:07,431 --> 00:52:12,431
The majority of global GDP
growth or global GDP is
actually outside the software

789
00:52:13,011 --> 00:52:13,844
industry.

790
00:52:13,880 --> 00:52:18,200
And I would just urge you
as you're considering what
is the most meaningful work,

791
00:52:18,500 --> 00:52:22,310
so consider the software industry, but
also look outside the software industry.

792
00:52:22,310 --> 00:52:27,050
Because I think really the
biggest untapped opportunities
for AI, like outside,

793
00:52:27,410 --> 00:52:30,340
I think light outside the
software industry and um,

794
00:52:30,710 --> 00:52:33,620
and we can't have everyone
doing the same thing, right?

795
00:52:33,621 --> 00:52:36,440
It's actually not a healthy planet.
If everyone you know,

796
00:52:36,441 --> 00:52:40,040
works on the improve web
search or improve or, or, or,

797
00:52:40,060 --> 00:52:41,510
or even improved healthcare.

798
00:52:41,910 --> 00:52:45,800
I think we need a world where all of you
have these skills, share these skills,

799
00:52:45,801 --> 00:52:50,120
teach other people what you've learned
and go out to do this work that hopefully

800
00:52:50,121 --> 00:52:53,820
affects the software industry, affects
other industries, affects profit,

801
00:52:53,990 --> 00:52:56,220
nonprofit affects governments,
um,

802
00:52:56,360 --> 00:53:00,830
but uses these AI capabilities to
lift up the whole human race. Okay.

803
00:53:01,340 --> 00:53:03,790
Um, and then finally, um, uh,

804
00:53:04,070 --> 00:53:08,840
the last thing to say on behalf of Ken
and me and the whole teaching team is, um,

805
00:53:09,020 --> 00:53:13,070
I wanted to thank you for your hard work
in this class. I know that, you know,

806
00:53:13,071 --> 00:53:17,350
watching the videos are, uh, doing
the homeworks and the website, uh,

807
00:53:17,460 --> 00:53:21,260
and the TA's going to the
section. Um, uh, you know,

808
00:53:21,261 --> 00:53:25,770
that many of you have put a lot of work
in this class and it wasn't so long ago.

809
00:53:25,771 --> 00:53:30,060
I guess when I was a student,
uh, you know, staying at
home, doing this homework,

810
00:53:30,090 --> 00:53:34,230
like trying to derive that math thing,
I also take some online courses myself.

811
00:53:34,231 --> 00:53:36,450
So it's actually not so long ago that,
you know,

812
00:53:36,480 --> 00:53:40,590
I was sitting in a computer mice that
you kind of watch some cool Sarah videos

813
00:53:40,591 --> 00:53:44,990
and they click on this click on dad and
also things online. Uh, and, and, and I,

814
00:53:45,010 --> 00:53:47,040
I appreciate, uh, Jen and I,

815
00:53:47,041 --> 00:53:50,770
the whole teaching team appreciate all
the hard work you've put into this. Um,

816
00:53:51,060 --> 00:53:52,970
and I hope also that,
um,

817
00:53:53,220 --> 00:53:57,390
you called the law all of your hard work
and that you will take these rare and

818
00:53:57,391 --> 00:54:00,130
unique skills you now have to go on and,

819
00:54:00,170 --> 00:54:04,170
and when you graduate from Stanford's a
offer, the Oh, oh, for the home viewers,

820
00:54:04,171 --> 00:54:09,171
I guess a whole viewers as it was in
costume Vus that you take these risks,

821
00:54:09,651 --> 00:54:11,820
those do not happen.
And he'd go on to do work,

822
00:54:11,821 --> 00:54:16,620
then maths isn't going to do record
calls on the people. So if that's, um,

823
00:54:16,860 --> 00:54:21,730
I look forward to seeing all of your
projects at the poster session, uh, uh,

824
00:54:21,750 --> 00:54:23,630
and apologize in advance.
We won't be there.

825
00:54:23,820 --> 00:54:26,280
Really get a deep understanding
of three minutes, but don't worry,

826
00:54:26,281 --> 00:54:30,170
we do reach your pressure roof once. Uh,
uh, but I look forward to seeing, uh,

827
00:54:30,270 --> 00:54:33,600
hope you are looking for also to seeing
everyone else's work or the poster

828
00:54:33,601 --> 00:54:36,690
session that,
let me just say on behalf of the,

829
00:54:36,860 --> 00:54:40,170
and me and the whole teaching your team.
Thank you all very much. Thank you.

830
00:54:41,110 --> 00:54:45,110
[inaudible].

