WEBVTT

1
00:00:05.370 --> 00:00:08.040
Hi everyone.
Welcome to lecture number seven.

2
00:00:08.920 --> 00:00:13.170
So up to now,
I believe,
can you hear me in the back?

3
00:00:14.100 --> 00:00:18.990
Is it easy?
Okay.
So in the last set of modules that you've seen,

4
00:00:18.991 --> 00:00:22.410
you've learned about convolutional neural networks and how they can be applied

5
00:00:22.560 --> 00:00:25.110
to imaging,
no Tivoli,
uh,

6
00:00:25.111 --> 00:00:29.610
you've played with different types of layers including pooling,
Max pooling,

7
00:00:29.611 --> 00:00:32.610
average pooling and convolutional layers.

8
00:00:32.880 --> 00:00:37.880
You've also seen some classification with the most classic Algorithms,

9
00:00:38.680 --> 00:00:41.430
uh,
all the way up to inception and resonance.

10
00:00:42.450 --> 00:00:46.560
And then you jumped into advanced application like object detection with Yolo,

11
00:00:46.780 --> 00:00:51.240
uh,
and the fast RCN and foster or CNN series a with an optional video.

12
00:00:51.780 --> 00:00:54.730
And finally a face rekognition and neural psych transferred.

13
00:00:54.740 --> 00:00:56.610
If we talked a little bit about in the past lectures.

14
00:00:57.030 --> 00:01:00.960
So today we're going to build on top of everything you've seen in these set of

15
00:01:00.961 --> 00:01:05.380
modules to try to delve into the neural networks and interpret them because you,

16
00:01:05.390 --> 00:01:09.810
you,
you're Nazis.
After seeing,
uh,
the set of modules up to now,

17
00:01:09.811 --> 00:01:14.811
that's a lot of improvements of the neural networks are based on trial and

18
00:01:15.631 --> 00:01:19.290
error.
So we try something,
we do hyper parameter search.

19
00:01:19.291 --> 00:01:21.960
Sometimes the model improves,
sometimes it doesn't.

20
00:01:22.170 --> 00:01:26.430
We use a validation set to find the right set of methods that would make our

21
00:01:26.431 --> 00:01:30.510
model improve.
It's not satisfactory from a scientific standpoint.

22
00:01:30.810 --> 00:01:35.810
So people are also searching how can we find a new effective way to improve our

23
00:01:36.211 --> 00:01:38.850
neural networks?
Not only we try it in there,

24
00:01:38.910 --> 00:01:42.090
but we'd theory that goes into the network and visualizations.

25
00:01:42.750 --> 00:01:47.250
So today we focused on that.
We first,
uh,
we'll see three methods,

26
00:01:47.700 --> 00:01:51.450
saliency maps or occlusion sensitivity and class activation maps,

27
00:01:51.900 --> 00:01:56.900
which are used to kind of understand what was the decision process of the

28
00:01:57.031 --> 00:01:58.950
network given this output,

29
00:02:00.150 --> 00:02:05.150
how can we map back the output decision on the input space to see which part of

30
00:02:05.401 --> 00:02:07.580
the inputs were discriminated for this output.

31
00:02:08.460 --> 00:02:12.570
And later on we will delve even more in details into the network by looking at

32
00:02:12.571 --> 00:02:15.880
intermediate layers.
What happens at an activation level,
uh,

33
00:02:15.890 --> 00:02:19.800
to lay your level and at a network level with another set of methods,

34
00:02:19.801 --> 00:02:23.190
gradient essence,
class model,
visualization,
Dataset set,

35
00:02:23.191 --> 00:02:24.870
search and deconvolution.

36
00:02:25.470 --> 00:02:29.460
We will spend some times under the convolution because it's so cool.

37
00:02:29.490 --> 00:02:34.200
It's a cool type of mathematical operation to know and it will give you more

38
00:02:34.201 --> 00:02:38.970
intuition on how the convolution works from a mathematical perspective.
Uh,

39
00:02:39.030 --> 00:02:41.970
if we have time,
we go over a phone application called deep dream,

40
00:02:42.840 --> 00:02:47.100
which is super cool visuals for some of you who know it.
Okay,
let's go.

41
00:02:48.300 --> 00:02:52.810
Mentee code is on the board if you guys need to to sign up.
So,
uh,

42
00:02:53.160 --> 00:02:58.020
as usual we go over some context,
the information and and small case studies.

43
00:02:58.021 --> 00:02:59.530
So don't hesitate to participate.

44
00:02:59.860 --> 00:03:04.860
So you've built an animal classifier for a pet shop and you gave it to them.

45
00:03:04.941 --> 00:03:09.670
It's,
it's super good.
It's been trained on image net plus some other data.

46
00:03:10.030 --> 00:03:10.870
And what,

47
00:03:10.930 --> 00:03:15.340
what is a little worrying is that the pet shop is a little reluctant to use your

48
00:03:15.341 --> 00:03:19.060
network because they don't understand the decision process of the model.

49
00:03:19.930 --> 00:03:24.930
So how can you quickly show that the model is actually looking at a specific

50
00:03:25.451 --> 00:03:28.630
animal?
Let's say your cat,
if I give it an input that is a cat,

51
00:03:30.850 --> 00:03:35.230
we've seen that together one time.
Everybody remembers.
So I go quickly.

52
00:03:36.250 --> 00:03:40.780
You have a network.
Here's a dog given as an input to a CNN,
the CNN,

53
00:03:40.870 --> 00:03:44.510
assuming the constraint is that there is one animal page was trained with a soft

54
00:03:44.511 --> 00:03:49.310
Max output layer and we get a probability distribution over all animals,
Iguanas,

55
00:03:49.410 --> 00:03:52.510
dog car cats and in crap.

56
00:03:53.290 --> 00:03:57.220
And what we want is to take the derivative of the score of dog and bark

57
00:03:57.221 --> 00:04:01.660
propagated to the inputs to know which parts of the inputs where discriminative

58
00:04:01.930 --> 00:04:06.040
for this score of dog.
Does that make sense?
Everybody remembers this

59
00:04:07.540 --> 00:04:11.800
and so the interesting part is that this value is the same shape as x.

60
00:04:11.860 --> 00:04:14.620
So it's the size of the input.
It's a matrix of numbers.

61
00:04:14.860 --> 00:04:16.750
If the numbers are large in absolute value,

62
00:04:16.751 --> 00:04:20.710
it means the pixels corresponding to these locations had an impact on the score

63
00:04:21.100 --> 00:04:25.790
of duck.
Okay.
What do you think the score of dog is?
Easy.

64
00:04:25.791 --> 00:04:29.580
The output probability or no?
What?
What?
What?

65
00:04:29.581 --> 00:04:31.270
What do I mean by [inaudible] dog?

66
00:04:41.460 --> 00:04:42.293
<v 1>Yep.</v>

67
00:04:43.030 --> 00:04:47.770
<v 2>It's a score of the dog yet,
but it's a point 85 that's what I need.</v>

68
00:04:48.480 --> 00:04:51.940
Actual formula is used to compute the funding going through the soft Max factor

69
00:04:52.300 --> 00:04:56.390
that when you're supposed to be until yes,
it's the,
it's the scored Eddie's

70
00:04:56.460 --> 00:05:01.050
<v 0>pre soft Max is a score that comes before the softmax.
So as a reminder,</v>

71
00:05:01.051 --> 00:05:05.010
here's a softmax layer and this is how it could be presented.

72
00:05:05.310 --> 00:05:08.370
So you get us a vector that is a set of scores that are not necessarily

73
00:05:08.371 --> 00:05:11.970
probabilities.
They're just scores between minus infinity to plus infinity.

74
00:05:12.600 --> 00:05:16.200
You give them to the soft Max and the softmax when he's going to do is that he's

75
00:05:16.201 --> 00:05:21.030
going to output a vector where the sum of all the properties in this vector or

76
00:05:21.031 --> 00:05:23.370
going to sum up to one.
Okay.

77
00:05:24.460 --> 00:05:28.850
And so the issue is if instead of using the derivative of what we called white

78
00:05:28.851 --> 00:05:29.730
hat last time,

79
00:05:30.510 --> 00:05:33.840
we use the score of Doug where we get a better representation here.

80
00:05:34.110 --> 00:05:39.110
The reason is in order to maximize this number score of dog divided by the sum

81
00:05:40.200 --> 00:05:42.300
of the score of Al all animals,

82
00:05:42.630 --> 00:05:47.540
or like maybe I should write exponential of score of dog divided by some of the

83
00:05:47.541 --> 00:05:49.500
exponential of the score of all animals.

84
00:05:50.460 --> 00:05:55.460
One way is to minimize does so the scores of all the other animals rather than

85
00:05:56.461 --> 00:05:59.300
maximizing score of dog.
So you see,

86
00:05:59.301 --> 00:06:02.360
so maybe moving a certain peaks so that we minimize the score a fish.

87
00:06:02.840 --> 00:06:06.950
And so this speaks that we'll have a high influence on white hats,

88
00:06:07.070 --> 00:06:08.480
the general output of the network,

89
00:06:08.780 --> 00:06:12.440
but you actually doesn't have an influence on the score of dug one layer before.

90
00:06:14.000 --> 00:06:17.270
Does it make sense?
So that's why we would use,
uh,

91
00:06:17.630 --> 00:06:22.030
the scores pre softmax instead of using the scores plus soft Max Authority

92
00:06:22.050 --> 00:06:26.360
properties.
Okay.
And what's fun is it here,

93
00:06:26.361 --> 00:06:28.610
you cannot see there's this slides online if you want to,

94
00:06:28.670 --> 00:06:30.590
if you want to look at it on your computers,

95
00:06:30.890 --> 00:06:35.120
but you have some of the pixels that are roughly the same positions as the dog

96
00:06:35.150 --> 00:06:37.700
is on the input image that are stronger.

97
00:06:38.510 --> 00:06:43.510
So we see some white pixels here and these can be used to segment the dog

98
00:06:44.150 --> 00:06:44.983
probably.

99
00:06:44.990 --> 00:06:48.980
So you could use a simple trash shoulding to find where the dog was based on

100
00:06:48.981 --> 00:06:49.970
this pixel.

101
00:06:50.310 --> 00:06:54.650
A Pixel there with the big score map

102
00:06:56.420 --> 00:07:00.020
doesn't work too well in practice.
So we have better methods to do segmentation,

103
00:07:00.021 --> 00:07:01.100
but this can be done as well.

104
00:07:02.900 --> 00:07:07.850
So this is what it's called saliency maps and it's a common technique to quickly

105
00:07:08.210 --> 00:07:11.660
visualize what the network is looking at.
In practice,

106
00:07:11.661 --> 00:07:13.430
we will use other methods.

107
00:07:14.390 --> 00:07:17.420
So here's another contextual story.

108
00:07:18.000 --> 00:07:20.870
Now you've built the animal classifier.
They're still a little scared,

109
00:07:20.871 --> 00:07:24.320
but you want to prove that the model is actually looking at the inputs.

110
00:07:24.321 --> 00:07:27.410
He major the right position.
You don't need to be quick,

111
00:07:27.440 --> 00:07:30.650
but you have to be very precise.
Yeah,

112
00:07:36.110 --> 00:07:38.720
no,
the saliency map is literally distinct.

113
00:07:39.130 --> 00:07:42.420
Here is the values of the derivation.

114
00:07:46.370 --> 00:07:48.800
So you,
you,
you take the score of dog,

115
00:07:48.801 --> 00:07:50.710
you back propagate the grade in all the way to the inputs.

116
00:07:51.140 --> 00:07:55.160
It gives you a matrix that is exactly the same size as the x and you use,

117
00:07:55.161 --> 00:07:58.120
you use like a specific color scheme to see which speaks.

118
00:07:58.121 --> 00:08:02.420
Those are the strongest.
Okay,

119
00:08:02.510 --> 00:08:04.340
so here we have our CNN,

120
00:08:04.460 --> 00:08:09.460
the dog is for propagated and you get a score of a probability score for the

121
00:08:10.251 --> 00:08:14.450
dog.
Now you want a method that is more precise than the previous one but not

122
00:08:14.451 --> 00:08:17.900
necessarily too fast.
And this one,
we've talked about it a little bit,

123
00:08:17.930 --> 00:08:19.280
it's occlusion sensitivity.

124
00:08:19.610 --> 00:08:24.610
So the idea here is to put a gray square on the dog here and we propagate this

125
00:08:26.150 --> 00:08:29.810
image with the gray square at this position through the CNN.

126
00:08:30.320 --> 00:08:34.040
What we get is another property distribution that is probably similar to the one

127
00:08:34.041 --> 00:08:37.320
we had before because the grade score doesn't seem to impact too much.
The image,

128
00:08:38.580 --> 00:08:41.870
at least from a human perspective,
we still see a dog,
right?

129
00:08:42.230 --> 00:08:43.670
So the score of dot might be high,

130
00:08:43.760 --> 00:08:48.760
83% probably what we can say is that we can build a probability map

131
00:08:49.380 --> 00:08:54.380
corresponding to the class dog and ha and we will write down on this map how

132
00:08:54.930 --> 00:08:56.430
confident is the network.

133
00:08:57.010 --> 00:09:01.440
If the gray square is that as best finger location so far our first location,

134
00:09:01.441 --> 00:09:04.800
it seems that the network is very confident.
So let's put the Red Square here.

135
00:09:05.820 --> 00:09:09.330
Now I'm going to move the gray square a little bit and shifting it just as we do

136
00:09:09.331 --> 00:09:11.790
for convolution.
And I'm going to send a gain.

137
00:09:11.820 --> 00:09:16.820
This new image in the network is going to give me a new probably t distribution

138
00:09:17.911 --> 00:09:22.050
output and the score of dot might change.
So looking at the score of dog,

139
00:09:22.051 --> 00:09:23.160
I'm going to say,
okay,

140
00:09:23.161 --> 00:09:27.480
the network is still very confident that there is a dog here and I continue,

141
00:09:27.481 --> 00:09:31.050
I shifted to gain here.
Same networks,
still very confident that there is a dog.

142
00:09:31.051 --> 00:09:34.190
Now I shifted the square,
uh,

143
00:09:34.350 --> 00:09:37.220
vertically down and I see that part.

144
00:09:37.221 --> 00:09:40.680
Sure that the face of the dog is partially occluded.

145
00:09:41.280 --> 00:09:46.080
Probably tee off dog will probably go down because the network cannot see one I

146
00:09:46.081 --> 00:09:51.060
have the dog is not confident that there's a dog anymore.
So probably the,

147
00:09:51.250 --> 00:09:52.950
the confidence of the network went down.

148
00:09:52.951 --> 00:09:57.300
I'm going to put a square that is tending to be blue and I continue,

149
00:09:58.140 --> 00:10:00.990
I shifted to gain and here we don't see the dog face anymore.

150
00:10:01.290 --> 00:10:06.290
So probably the network Mites might classify this as a chair rights because the

151
00:10:06.661 --> 00:10:09.450
chair is more obvious than the dog now.
And so they're probably,

152
00:10:09.451 --> 00:10:10.920
the score of dog might go down.

153
00:10:12.000 --> 00:10:16.920
So I'm going to put the Blue Square here and we're going to continue here.

154
00:10:16.950 --> 00:10:19.110
We don't see the tail of the dog.
It's still fine.

155
00:10:19.111 --> 00:10:21.300
The network is pretty confident and so on.

156
00:10:23.280 --> 00:10:26.250
And what that we look at now is this probably team up,

157
00:10:26.490 --> 00:10:28.350
which tells me roughly where the dog is.

158
00:10:28.950 --> 00:10:32.010
So here we used a pretty big filter compared to the size of the image.

159
00:10:32.310 --> 00:10:36.720
The smaller the,
sorry,
the pretty big gray square.
The smaller the gray square,

160
00:10:38.520 --> 00:10:42.510
the more precise this probably team up is going to be.
Does that make sense?

161
00:10:43.380 --> 00:10:45.840
So this is if you have time,
if you can,

162
00:10:45.841 --> 00:10:49.260
you can take your time with the pet chart to explain them what's happening.

163
00:10:49.500 --> 00:10:51.820
You will do that.
Yeah,

164
00:10:58.700 --> 00:11:00.810
we'll,
we'll see that in the next light.
That's correct.

165
00:11:02.100 --> 00:11:06.210
So let's see more examples here.
We have three classes in these.
These,

166
00:11:06.211 --> 00:11:10.860
these images has been have been generated by my dealer and Rob Fergus.

167
00:11:11.130 --> 00:11:15.500
These paper visualizing and understanding convolutional networks is uh,

168
00:11:15.630 --> 00:11:18.330
one of the seminal paper that has led the research in,

169
00:11:18.331 --> 00:11:20.370
in visualizing and interpreting your own networks.

170
00:11:20.700 --> 00:11:24.300
So I advise you to take a look at it and we will refer to it a lot of time in

171
00:11:24.301 --> 00:11:27.960
this lecture.
So now we have three examples.
One is a Pomeranian,

172
00:11:27.990 --> 00:11:31.200
which is this type of cute dog,
a car wheel,

173
00:11:31.650 --> 00:11:35.160
which is a true class of the second image.
And,
uh,
an Afghan hound,

174
00:11:35.220 --> 00:11:38.520
which is this type of dog here,
uh,
on the last image.

175
00:11:39.150 --> 00:11:42.930
So if you do the same thing as we did before,
that's what you would see.

176
00:11:44.400 --> 00:11:48.540
So just to clarify,
here we see a blue color.

177
00:11:48.570 --> 00:11:52.740
It means when the gray square was positioned here or centered at this,

178
00:11:54.070 --> 00:11:57.840
the network was less confidence.
That's the true class was Pomeranian.

179
00:11:58.900 --> 00:12:01.630
And in fact,
if you look at the paper,

180
00:12:01.631 --> 00:12:03.670
they explained that when the gray square was here,

181
00:12:04.840 --> 00:12:08.140
their confidence of Pomeranian went down because the conference,

182
00:12:08.200 --> 00:12:10.420
because the confidence of tennis ball went up.

183
00:12:11.200 --> 00:12:14.200
And in fact the Pomeranian dog has a tennis ball in the mouth.

184
00:12:15.190 --> 00:12:18.640
And another interesting thing to notice is on the last picture here,

185
00:12:19.060 --> 00:12:24.060
you see that there's a red color on the top left of the image.

186
00:12:25.720 --> 00:12:29.590
And this is you exactly at what as what you mentioned at him is that when the

187
00:12:29.591 --> 00:12:31.270
square was on the face of the human,

188
00:12:31.810 --> 00:12:35.410
the network was much more confidence than the true that the true class was the

189
00:12:35.411 --> 00:12:40.030
dog because you removed a lot of meaningful information for the network,

190
00:12:40.031 --> 00:12:44.950
which was the face of the human.
And similarly,
if you put the square on the dog,

191
00:12:45.600 --> 00:12:49.840
do a true class that the network was out putting was human problem.

192
00:12:50.980 --> 00:12:54.100
Does that make sense?
Okay.

193
00:12:54.130 --> 00:12:59.130
So DC's called occlusion sensitivity and it's the second method that's uh,

194
00:12:59.290 --> 00:13:04.290
you now have seen for interpreting where the network Luke sites on an input.

195
00:13:07.510 --> 00:13:10.510
So let's move to class activation maps.
So I know if you remember,

196
00:13:10.511 --> 00:13:13.940
but two weeks ago,
uh,
print off when he discussed,
uh,

197
00:13:13.941 --> 00:13:18.760
the techniques that he has used in healthcare,
he explained that you get,

198
00:13:18.761 --> 00:13:19.090
uh,

199
00:13:19.090 --> 00:13:24.090
he get a chest x ray and he manages to to tell the doctor where the network is

200
00:13:26.681 --> 00:13:31.300
looking at when predicting a certain disease based on these checks.
X Ray,
right?

201
00:13:31.330 --> 00:13:32.163
You remember that?

202
00:13:32.380 --> 00:13:35.650
So this was done through class activation maps and that's what we're going to

203
00:13:35.651 --> 00:13:36.484
see now.

204
00:13:37.390 --> 00:13:42.390
So one important thing to notice is that we discussed that classification

205
00:13:43.271 --> 00:13:48.271
networks seem to have a very good localization ability and we can see it with

206
00:13:49.061 --> 00:13:51.610
the two methods that we previously discussed.

207
00:13:52.180 --> 00:13:56.140
Same thing for those of you who have read the Yellow paper that you've studied

208
00:13:56.141 --> 00:13:57.190
in these set of modules,

209
00:13:57.610 --> 00:14:02.440
you low V two algorithm has first been trained on classification because

210
00:14:02.441 --> 00:14:04.000
classification has a lot of data,

211
00:14:04.090 --> 00:14:08.170
a lot more than object detection has been trained on classification.

212
00:14:08.440 --> 00:14:12.220
Built a very good localization ability and then has been fine tuned and

213
00:14:12.221 --> 00:14:15.900
retrained on object detection data sets.
Okay.

214
00:14:16.660 --> 00:14:20.140
And so the core idea of cloth activation map is to show that uh,

215
00:14:20.710 --> 00:14:25.710
CNN have a very good localization ability even if they were trained only on

216
00:14:25.811 --> 00:14:26.950
image level labels.

217
00:14:27.910 --> 00:14:32.910
So we have this network that is a very classic network use for classification.

218
00:14:34.060 --> 00:14:36.830
We give it a kid and a dog,
uh,

219
00:14:37.380 --> 00:14:40.000
discuss activation map is coming from MIT,

220
00:14:40.001 --> 00:14:45.001
the MIT lab with bullied you at all in 2016 and you for propagate this image of

221
00:14:45.731 --> 00:14:48.370
a kid with a dog through the network,
which has some calm,

222
00:14:48.371 --> 00:14:52.670
really Max pool classic series of layers,
several of them.

223
00:14:52.700 --> 00:14:57.700
And at the end usually Slaton the last output volume of the comp and Ronnie to

224
00:14:58.580 --> 00:15:00.230
several fully connected layer,

225
00:15:00.231 --> 00:15:05.231
which are going to play the role of the classifier and send it to a softmax and

226
00:15:05.241 --> 00:15:06.230
get the property to output.

227
00:15:06.950 --> 00:15:11.060
Now what we're going to do is that we're going to prove that this CNN is

228
00:15:11.061 --> 00:15:12.680
generalizing to localization.

229
00:15:13.700 --> 00:15:18.050
So we're going to convert this same network in another network and the part

230
00:15:18.051 --> 00:15:19.760
which is going to change,
it's only the last part.

231
00:15:21.230 --> 00:15:26.230
The downside of using flatten plus fully connected is that you lose all spacial

232
00:15:27.081 --> 00:15:31.880
information,
right?
You have a volume that has spatial information,

233
00:15:32.030 --> 00:15:35.500
although it's been gone through some Max pooling,
so it's been down sample.

234
00:15:35.501 --> 00:15:39.650
Then you lost some part of the special localization,
flattening kills it,

235
00:15:39.680 --> 00:15:40.370
you flatten it,

236
00:15:40.370 --> 00:15:43.250
you run it through a fully connected layer and then it's over you.

237
00:15:43.251 --> 00:15:48.251
It's super hard to find out where the activation was corresponds to on the boot

238
00:15:48.831 --> 00:15:52.040
space.
So instead of using flatten plus fully connected,

239
00:15:52.340 --> 00:15:56.390
we're going to use global average pooling.
We're going to explain what it is,

240
00:15:57.020 --> 00:16:00.650
a fully connected softmax layer and get the property to output and we're going

241
00:16:00.651 --> 00:16:05.390
to show that now this network can be trained very quickly because we just need

242
00:16:05.391 --> 00:16:06.410
to train one layer,

243
00:16:06.440 --> 00:16:11.440
the fully connected here and can show where the network looks at the same as the

244
00:16:11.661 --> 00:16:14.830
previous network.
So let's,
let's talk about it more in detail.

245
00:16:15.780 --> 00:16:20.510
I assume this was the last comp layer of our network and it outputs a volume,

246
00:16:20.770 --> 00:16:24.380
a volume that is size to simplify four by four by six.

247
00:16:24.620 --> 00:16:28.160
So six filters were used in the last comp.

248
00:16:28.220 --> 00:16:31.580
And so we have six feature maps now that makes sense.

249
00:16:32.840 --> 00:16:36.200
I'm going to convert this using your glove global average pooling to just a

250
00:16:36.201 --> 00:16:37.430
vector of six values.

251
00:16:37.730 --> 00:16:40.970
What is global average pooling is just taking this feature maps,

252
00:16:41.390 --> 00:16:43.520
each of them averaging them into one number.

253
00:16:44.360 --> 00:16:47.840
So now instead of having a four by four by six volume,

254
00:16:48.200 --> 00:16:52.160
I have a one by one by six volume,
but we can call it a vector.

255
00:16:53.390 --> 00:16:54.223
Doesn't make sense.

256
00:16:55.260 --> 00:16:59.900
So what's interesting is that this number actually holds the formation of the

257
00:16:59.901 --> 00:17:04.730
whole feature map that came before in one number being average over it.

258
00:17:05.750 --> 00:17:10.430
I'm going to put these in a vector and I'm going to call them activations as

259
00:17:10.431 --> 00:17:14.150
usual.
A one,
a two,
a three,
a four,
a five,
a six.

260
00:17:15.260 --> 00:17:15.831
As I said,

261
00:17:15.831 --> 00:17:20.180
I'm willing to train a fully connected layer here with a soft max activation.

262
00:17:20.690 --> 00:17:22.550
And the outputs are going to be the probabilities.

263
00:17:24.110 --> 00:17:28.640
So what is interesting about that is that the feature maps here,

264
00:17:28.700 --> 00:17:30.920
as you know,
we contain some visual patterns.

265
00:17:31.280 --> 00:17:35.090
So if I look at the first feature map,
I can plot it here.

266
00:17:35.091 --> 00:17:36.140
So these are the values.

267
00:17:36.141 --> 00:17:41.141
And of course this one is much more granular than four by four it's not a four

268
00:17:41.211 --> 00:17:43.430
by four it's much more numbers.
But this,

269
00:17:43.700 --> 00:17:47.390
you can say that this is the feature map and it seems that the activations have

270
00:17:47.391 --> 00:17:48.470
found something here.

271
00:17:49.080 --> 00:17:53.130
There was a visual pattern in the inputs that activated the the seizure mop and

272
00:17:53.131 --> 00:17:56.730
to seem tourism,
which generated this feature map here in this location.

273
00:17:57.450 --> 00:17:58.470
Same for the second one.

274
00:17:58.560 --> 00:18:03.560
There's probably two objects or two patterns that's activated,

275
00:18:03.750 --> 00:18:06.840
the filters that generated these feature map and so on.

276
00:18:07.530 --> 00:18:12.530
So we have six of those and after I've trained my fully connected layers here,

277
00:18:13.440 --> 00:18:14.610
my fully connected layer,

278
00:18:15.120 --> 00:18:20.120
I look at the score of dog score of dog is 91% what I can do is to know this 91%

279
00:18:24.090 --> 00:18:29.090
how much did it come from these feature maps and how can I know it is because

280
00:18:29.251 --> 00:18:31.410
now I have a direct mapping using the weights.

281
00:18:32.190 --> 00:18:34.740
I knew that the weight number one here,

282
00:18:34.770 --> 00:18:39.770
this edge you see it is how much the score was dependent on the orange feature

283
00:18:41.371 --> 00:18:45.160
map.
Does it make sense?

284
00:18:46.090 --> 00:18:47.080
The second weight,

285
00:18:47.620 --> 00:18:52.620
if you look at the green edge is the weights that has multiplied this feature

286
00:18:53.831 --> 00:18:57.280
map to give birth to the outputs of a dog.

287
00:18:57.970 --> 00:19:01.930
So this way it is telling me how much this feature map the green one has

288
00:19:01.931 --> 00:19:05.380
influence on the output.
Does that make sense?

289
00:19:06.730 --> 00:19:08.260
So now what I can do is do some,

290
00:19:08.261 --> 00:19:12.190
all of these are weighted sum of all these feature maps.

291
00:19:12.850 --> 00:19:15.610
And if I just do this way,
did some I,
we get to another feature map,

292
00:19:16.450 --> 00:19:17.290
something like that.

293
00:19:19.480 --> 00:19:22.960
And you notice that this one seems to be highly influenced by the green one,

294
00:19:24.040 --> 00:19:25.530
the green feature,
ma?
Yeah,

295
00:19:26.080 --> 00:19:29.350
it means probably the weight here was higher.

296
00:19:31.330 --> 00:19:36.330
It probably means that the second filter of the last comp was the one that was

297
00:19:37.091 --> 00:19:40.930
looking at the dog.
That makes sense.
Yeah.

298
00:19:43.540 --> 00:19:45.620
Okay.
And then once I get these feature mop,

299
00:19:45.670 --> 00:19:48.070
this feature map is not the size of the input image,
right?

300
00:19:48.610 --> 00:19:53.570
It's the size of the height and width of the output of the last [inaudible].

301
00:19:54.010 --> 00:19:55.420
So the only thing I'm going to do is like,

302
00:19:55.421 --> 00:20:00.421
I'm going to sample it back simply so that it fits the size of the input image.

303
00:20:00.520 --> 00:20:04.970
And I'm going to overlay it on the input image to get my class activation map.

304
00:20:05.800 --> 00:20:09.880
The reason it's called class activation map is because this feature map is

305
00:20:09.881 --> 00:20:14.320
dependent on the class you were talking about.
If I was using,
uh,

306
00:20:14.680 --> 00:20:19.680
they'd say I was using car here that was using car,

307
00:20:19.810 --> 00:20:22.520
the weights would have been different,
right?

308
00:20:22.630 --> 00:20:26.740
Look at the edge that connect the first activation to the activation of the

309
00:20:26.741 --> 00:20:28.600
previous layer.
These weights are different.

310
00:20:28.990 --> 00:20:32.170
So if I sum all of these feature maps,
I'm going to get something else.

311
00:20:33.990 --> 00:20:37.020
Does that make sense?
So this is class activation maps

312
00:20:39.940 --> 00:20:43.090
and in fact there is a dog here and there's a human there.

313
00:20:43.540 --> 00:20:45.220
And what you can notice is probably,

314
00:20:45.221 --> 00:20:50.221
if I look at the class of human do weights number one might be very high because

315
00:20:50.921 --> 00:20:55.921
it seems that this visual pattern that activated the first teacher map was the

316
00:20:56.051 --> 00:20:56.884
face of the kid.

317
00:20:59.050 --> 00:20:59.770
<v 3>Yeah.</v>

318
00:20:59.770 --> 00:21:00.490
<v 0>Okay.</v>

319
00:21:00.490 --> 00:21:03.760
So what are you super cool is that you can get your network and just change the

320
00:21:03.761 --> 00:21:07.210
last few layers into global average schooling plus a softmax fully connected

321
00:21:07.211 --> 00:21:09.670
layer and you can do that and visualize very well.

322
00:21:10.210 --> 00:21:13.990
It requires a small fine tuning.
Yeah.

323
00:21:14.290 --> 00:21:15.880
So were these like saliency map?

324
00:21:16.000 --> 00:21:20.120
<v 4>Oh for the activation though.
So it's a different vocabulary.</v>

325
00:21:20.121 --> 00:21:24.910
I would use saliency maps for the backpropagation up to the pixels and class

326
00:21:24.920 --> 00:21:27.470
activation maps related to one class.

327
00:21:29.240 --> 00:21:30.740
It's not the back propagation at all,

328
00:21:31.100 --> 00:21:35.000
it's just a nut sampling through those to the input space based on the feature

329
00:21:35.001 --> 00:21:36.080
maps of the last couple of days.

330
00:21:37.270 --> 00:21:42.170
So it's mostly just examining the weights and sort of doing like a max operation

331
00:21:42.950 --> 00:21:46.250
on the,
not so much if about backpropagation.
Yes.

332
00:21:48.380 --> 00:21:52.070
Any other questions on costs?
Activation?
Maps kill the,

333
00:21:54.200 --> 00:21:56.150
yeah,
that's a good question.
So taking the average,

334
00:21:56.151 --> 00:22:00.020
does it kill the spatial information?
So let me,
let me write down a formula here.

335
00:22:00.680 --> 00:22:02.780
This is the score that's we're interested in.

336
00:22:02.990 --> 00:22:07.990
Let's say dog Class C once you could say is that this court is the sum of key

337
00:22:09.770 --> 00:22:13.130
equal one to six of Dama UK,

338
00:22:13.340 --> 00:22:16.400
which is the weight that that connects.

339
00:22:16.480 --> 00:22:21.050
You have to put activation to the previous layer times.
What's times a

340
00:22:21.850 --> 00:22:25.460
<v 0>of the previous layer?
Um,
let's say we,
we,</v>

341
00:22:25.461 --> 00:22:27.020
we use a notation that he's like,

342
00:22:27.040 --> 00:22:31.020
Kay is the chase feature map and I,

343
00:22:31.480 --> 00:22:34.810
Jay is the location and I summed that over the locations.

344
00:22:36.910 --> 00:22:39.130
Can you seem to bag coffee?

345
00:22:40.090 --> 00:22:43.960
So what I'm saying is that here I have my global average pooling that happened

346
00:22:43.961 --> 00:22:46.480
here and I can divided by a certain number,

347
00:22:46.630 --> 00:22:50.740
so divided by 16,
four by four.

348
00:22:51.460 --> 00:22:55.300
Okay.
I can switch the two songs.

349
00:22:55.330 --> 00:23:00.280
So I can say that this thing is a sum over Ij.
The locations

350
00:23:02.590 --> 00:23:06.850
times some over a t equals one two,

351
00:23:06.851 --> 00:23:09.850
six of what?
W K

352
00:23:11.530 --> 00:23:13.600
Times H j.

353
00:23:13.601 --> 00:23:18.601
So the activations of the case teacher map in position a I J and Times the

354
00:23:19.480 --> 00:23:22.930
normalization one 16 does it make sense?

355
00:23:27.440 --> 00:23:32.210
Does this make sense?
So I,
I still have the Duluth,
the location,
I still moved,

356
00:23:32.240 --> 00:23:33.620
I see moved to some around.

357
00:23:34.280 --> 00:23:39.280
And what I could do is to say that these things is the score in location

358
00:23:45.270 --> 00:23:48.830
Ij
of your class activation map?

359
00:23:48.870 --> 00:23:51.260
Is it a class score for dislocation?

360
00:23:51.290 --> 00:23:54.770
Ij and I'm somebody needs overall locations.

361
00:23:55.880 --> 00:23:59.510
So just by flipping what the average pooling was doing over the locations,

362
00:23:59.840 --> 00:24:03.440
I can say that my waiting,
using my weights,

363
00:24:03.620 --> 00:24:08.620
all the activation in a specific location for all the feature maps,

364
00:24:08.900 --> 00:24:13.820
I can get the score of this position in regards to the final output.

365
00:24:15.570 --> 00:24:16.403
Does that make sense?

366
00:24:18.720 --> 00:24:21.090
So we were not losing the this patient information.

367
00:24:24.190 --> 00:24:24.930
<v 2>The reason we're not</v>

368
00:24:24.930 --> 00:24:29.930
<v 0>losing it is because we know we know what the feature maps are,</v>

369
00:24:30.450 --> 00:24:33.870
right?
We know what they are and we know that day's been averaged.
Exactly.

370
00:24:33.871 --> 00:24:36.200
So we exactly can map it back.

371
00:24:42.030 --> 00:24:45.170
<v 2>Yeah.
Because we assume that each tilter</v>

372
00:24:45.260 --> 00:24:48.740
<v 0>that generated these feature maps,
the TX one one specific thing.</v>

373
00:24:50.060 --> 00:24:50.780
<v 3>Okay.</v>

374
00:24:50.780 --> 00:24:53.480
<v 0>So like if,
if this is the feature map,</v>

375
00:24:53.510 --> 00:24:56.210
it means assuming the filter was detecting dog,

376
00:24:56.690 --> 00:25:00.950
that's we're going to see just,
just something here.

377
00:25:00.951 --> 00:25:04.820
Meaning that there's a dog here and if there was a dog on the lower part of the

378
00:25:04.821 --> 00:25:08.420
image,
we would also have strong activations in these parts.

379
00:25:13.210 --> 00:25:17.830
<v 2>I would say if you want to see more of the math behind it,
check the papers,</v>

380
00:25:18.460 --> 00:25:19.790
but,
uh,
[inaudible]

381
00:25:20.020 --> 00:25:20.801
<v 0>we shouldn't behind it.</v>

382
00:25:20.801 --> 00:25:24.910
You can flip the summations using the global average pooling and showed that you

383
00:25:24.911 --> 00:25:26.500
keep the,
the spatial information.

384
00:25:28.720 --> 00:25:30.520
The thing is you do the global average pooling,

385
00:25:30.521 --> 00:25:34.240
but you don't lose the feature maps because you know where they were from the

386
00:25:34.241 --> 00:25:37.930
output of their comp.
Right.
So you're not,
you're not deleting this inflammation.

387
00:25:38.320 --> 00:25:41.800
That makes sense.
Yep.

388
00:25:43.810 --> 00:25:48.070
<v 2>The occupations case,
right.
It by 16 is it started with taking the average.
Great.</v>

389
00:25:50.580 --> 00:25:55.570
Okay,
let's move on and watch a Pulido on how acs class activation not work.

390
00:25:56.410 --> 00:25:58.240
If you do.
It was from Kelly MacDonald.

391
00:26:02.170 --> 00:26:05.010
<v 0>They,
it's a,
it's live.
So it's very quick.</v>

392
00:26:10.690 --> 00:26:13.270
So you can see that the network is looking at this speed boat.

393
00:26:21.390 --> 00:26:22.223
Okay.

394
00:26:29.430 --> 00:26:34.430
So now the three methods we've seen or methods that are roughly mapping back the

395
00:26:37.231 --> 00:26:40.130
output to the input space and how of visualize,

396
00:26:40.140 --> 00:26:44.730
which parts of the inputs were the most discriminative to lead to these outputs

397
00:26:44.731 --> 00:26:45.870
and the district of the network.

398
00:26:46.410 --> 00:26:50.010
Now we're going to try to delve more into details in the,
in the,

399
00:26:50.011 --> 00:26:54.360
in the intermediate layers of the network and try to interpret how does the

400
00:26:54.361 --> 00:26:55.860
network see our world.

401
00:26:56.070 --> 00:27:00.630
Not necessarily related to a specific input but in general.
Okay,

402
00:27:02.760 --> 00:27:06.900
so the pet shop now trust your model because you've used the occlusion

403
00:27:06.901 --> 00:27:07.651
sensitivity,

404
00:27:07.651 --> 00:27:10.770
saliency map and cass activation maps to show that the model is looking at the

405
00:27:10.771 --> 00:27:12.230
right place.
Uh,

406
00:27:12.420 --> 00:27:17.220
but they got a little scared when you did that and they ask you to explain what

407
00:27:17.221 --> 00:27:19.860
the model thinks a dog is.

408
00:27:21.030 --> 00:27:25.410
So you have these trained convolutional neural network and you have an output

409
00:27:25.411 --> 00:27:29.700
probably t
oh yeah.
Let me take one in the back.
Yeah.

410
00:27:31.310 --> 00:27:35.680
<v 4>Good ways to visualize like not image data,
image data.
That's a,</v>

411
00:27:35.690 --> 00:27:38.580
that's a good question.
It's actually sort the reason we were seeing images.

412
00:27:38.600 --> 00:27:41.960
What's most of the research has been focusing on images.
Um,

413
00:27:42.400 --> 00:27:45.100
if you look at let's say time series data.

414
00:27:45.490 --> 00:27:50.110
So either speech or not your language domain way to visualize those.

415
00:27:50.140 --> 00:27:54.400
Is we the attention method,
are you familiar with that?

416
00:27:54.820 --> 00:27:58.060
So in the next set of modules that you're going to start this week and you're

417
00:27:58.061 --> 00:27:59.320
going to study in the next two weeks,

418
00:27:59.321 --> 00:28:02.500
you will see a visualization method called attention models,

419
00:28:03.400 --> 00:28:06.220
which will tell you which part of a sentence was important.

420
00:28:06.221 --> 00:28:08.620
Let's say cloud put a number,

421
00:28:09.820 --> 00:28:12.970
like assuming you're doing machine translation,
you know,

422
00:28:12.971 --> 00:28:16.240
some languages they don't have the direct one to one mapping.
It means,

423
00:28:16.241 --> 00:28:18.880
I might say,
uh,
I love cats,

424
00:28:19.180 --> 00:28:24.070
but in another language may be the same sentence would be chats I log or

425
00:28:24.071 --> 00:28:25.180
something like that.
It's flipped.

426
00:28:25.690 --> 00:28:28.930
And you want an attention while though to see to show you that the cat was

427
00:28:28.931 --> 00:28:33.670
referring to the second.
I think it's,
it's,
it's,
it's,
okay.

428
00:28:33.730 --> 00:28:34.563
Sorry guys.

429
00:28:38.950 --> 00:28:43.950
So going back to the presentation now we're going to delve into inside the

430
00:28:44.231 --> 00:28:46.030
network.
And so the new thing is

431
00:28:46.840 --> 00:28:50.560
<v 0>the pet shop is little scared and ask you to explain what the network thing</v>

432
00:28:50.561 --> 00:28:53.890
could Doug is,
what's a representation of doc for the network?

433
00:28:54.310 --> 00:28:57.610
So here we're going to use a method that we've already seen together called

434
00:28:57.611 --> 00:28:58.510
gradient essence,

435
00:28:58.990 --> 00:29:03.990
which is the finding an objective that is technically the score of the dog minus

436
00:29:06.550 --> 00:29:07.690
a regularization term.

437
00:29:08.170 --> 00:29:11.830
What the regularization term is doing is it's saying that x should look natural,

438
00:29:12.400 --> 00:29:16.330
it's not necessarily held to regularization can be something else and we will

439
00:29:16.331 --> 00:29:19.600
discuss it in the next slide.
But don't think about it right now.

440
00:29:20.050 --> 00:29:23.410
What we will do is we will compute the backpropagation of the subjective

441
00:29:23.411 --> 00:29:28.411
function all the way back to the inputs and perform gradient assent to find the

442
00:29:28.450 --> 00:29:32.710
image that maximizes the score of the dog.
So it's an iterative process.

443
00:29:32.730 --> 00:29:34.600
It takes longer than the class activation map.

444
00:29:36.040 --> 00:29:38.560
And we repeat the process for what propagate x,

445
00:29:38.561 --> 00:29:41.590
compute the objective back propagates and update pixels.

446
00:29:41.650 --> 00:29:45.080
And so you guys are familiar with that.
So let's see what,
what,

447
00:29:45.081 --> 00:29:46.630
what we can visualize doing that.

448
00:29:47.770 --> 00:29:51.610
So actually if you take an image nets classification network and you perform

449
00:29:51.611 --> 00:29:55.060
this underclasses of goose or ostrich or Kit Fox,

450
00:29:55.061 --> 00:29:59.920
Husky dot machines and can see what the network is looking at or what the

451
00:29:59.921 --> 00:30:01.370
network thinks [inaudible] is.

452
00:30:01.770 --> 00:30:06.770
So [inaudible] you can see some some black dots on a white background somehow,

453
00:30:07.960 --> 00:30:12.340
but these are are still quite hard to interpret.
It's not super easy to see.

454
00:30:12.341 --> 00:30:15.790
And even worse here on the screen,
better on your computers.

455
00:30:16.180 --> 00:30:19.960
But you can see a fox,
some here,

456
00:30:19.961 --> 00:30:21.730
you can see orange color for the Fox.

457
00:30:21.731 --> 00:30:25.600
It means that pushing the pixels to an orange color would actually lead to a

458
00:30:25.601 --> 00:30:28.450
higher score of the Kit Fox in the output.

459
00:30:29.500 --> 00:30:34.300
If you use a better regularization than l two you might get better pictures.

460
00:30:34.600 --> 00:30:37.650
So this is for Flamingo,
this is for Pelican,
and this is for heartbeat.

461
00:30:38.350 --> 00:30:41.770
So a few things that are interesting to see is that in order to maximize the

462
00:30:41.771 --> 00:30:42.790
score of Flamingo,

463
00:30:43.270 --> 00:30:47.050
what the network visualized is many flamingos,

464
00:30:47.650 --> 00:30:51.890
it means that 10 flamingo leads to a higher score of the class [inaudible].

465
00:30:51.960 --> 00:30:54.070
Then one flamingo for the network.

466
00:30:55.950 --> 00:30:59.260
Talking about regularization.
What does Altura musicians say?

467
00:30:59.800 --> 00:31:04.420
It says that for visualizing we don't want to have extreme values of pixel.

468
00:31:04.510 --> 00:31:07.720
It doesn't help much to have one pixel with an extreme value,

469
00:31:07.870 --> 00:31:09.850
one pixel with the low value and so on,

470
00:31:09.851 --> 00:31:13.360
so we're going to regularize all the pixels so that all the values are around

471
00:31:13.361 --> 00:31:17.710
each other and then we can rescale it between zero and 2,255 if you want.

472
00:31:18.400 --> 00:31:23.400
One thing to notice is that the gradient ascent process doesn't constrain the

473
00:31:24.431 --> 00:31:29.431
inputs to be between zero and 255 you can go to plus infinity potentially while

474
00:31:30.881 --> 00:31:34.990
an image is stored with numbers between zero and two 55 so we might want to keep

475
00:31:34.991 --> 00:31:37.690
that as well.
This is another type of regularization.

476
00:31:38.530 --> 00:31:42.820
One thing that led to beautiful pictures was what Jason,

477
00:31:42.821 --> 00:31:47.821
you're Sinskey and his team did is they for propagates it an image computed the

478
00:31:48.431 --> 00:31:52.450
score competent,
the objective function back propagated,

479
00:31:52.540 --> 00:31:55.780
updated the pixels and blurred them,
blurred the picture.

480
00:31:56.350 --> 00:32:00.910
Cause what what is not useful for visualizing is if you have high frequency

481
00:32:00.911 --> 00:32:02.140
variation between pixels,

482
00:32:02.500 --> 00:32:06.400
it doesn't help to visualize if you have many pixels close to each other that

483
00:32:06.401 --> 00:32:07.630
have many different values.

484
00:32:08.050 --> 00:32:11.140
Instead you want to have a smooth transition among pixels.

485
00:32:11.440 --> 00:32:16.270
And this is another type of regularization called Goshen blurring.
Okay.

486
00:32:17.710 --> 00:32:20.620
So this method actually makes a lot of sense in,

487
00:32:20.780 --> 00:32:22.720
in in scientific terms you're,

488
00:32:22.770 --> 00:32:27.220
you're maximizing an objective function that gives you what the network sees as

489
00:32:27.221 --> 00:32:29.500
Flamingo,
which would maximize the score a flamingo.

490
00:32:30.520 --> 00:32:35.500
So we call it also class model visualization.
Yes.

491
00:32:39.050 --> 00:32:39.883
To a more accurate,

492
00:32:40.880 --> 00:32:44.070
<v 3>um,
but more relaxed.</v>

493
00:32:44.090 --> 00:32:48.350
<v 0>The classic model digitalization correspond to a more accurate,</v>

494
00:32:48.351 --> 00:32:52.700
so it's hard to map the accuracy of the model based on these visualization.

495
00:32:52.950 --> 00:32:57.090
It's a good way to validate that the network is looking at the right thing.
Yeah.

496
00:32:57.680 --> 00:32:59.390
We're going to see more of these later.

497
00:33:01.370 --> 00:33:05.050
I think the most interesting part is actually on this slide is we're,

498
00:33:05.070 --> 00:33:08.510
we did it for the class score,
but we could have done it with any activation.

499
00:33:09.080 --> 00:33:12.830
So let's say I stopped in the middle of the network and I defined my objective

500
00:33:12.831 --> 00:33:14.990
function to be this activation,

501
00:33:16.070 --> 00:33:20.750
I'm going to back propagate and find the input that we maximize this activation.

502
00:33:21.050 --> 00:33:25.760
It will tell me what is this activation?
What does this activation fire for?

503
00:33:26.510 --> 00:33:29.840
So that's even more interesting.
I think they're looking at the inputs and then,

504
00:33:29.850 --> 00:33:33.950
yeah.
Does that make sense that we could do it on any activation?

505
00:33:36.370 --> 00:33:41.320
<v 3>Yep.
Any questions on that?</v>

506
00:33:45.550 --> 00:33:48.560
Okay.
So now are

507
00:33:48.560 --> 00:33:51.110
<v 0>going to do another shake,
which is debt to set search.</v>

508
00:33:51.111 --> 00:33:55.700
It's actually one of the most useful,
I think,
uh,
not fast but very useful.

509
00:33:56.090 --> 00:33:59.720
So the patch off loved the previous technique and asks if there are other

510
00:33:59.780 --> 00:34:02.300
alternatives to,
to show what,

511
00:34:02.320 --> 00:34:07.120
what an activation in the middle of a network is thinking.
Uh,

512
00:34:07.460 --> 00:34:10.550
you take an image for propagated through the network,
gets your output.

513
00:34:11.360 --> 00:34:16.130
Now what you're going to do is select a feature map.
Let's say this one,

514
00:34:16.520 --> 00:34:21.520
we're at this layer and the feature map is offsites five by five by 256 it means

515
00:34:22.251 --> 00:34:26.630
that the comp layer here had 256 filters,

516
00:34:27.110 --> 00:34:27.943
right?

517
00:34:28.440 --> 00:34:33.440
You are going to look at these feature maps and select probably uh yeah.

518
00:34:36.501 --> 00:34:39.410
What you're going to do,
select one of the feature maps.
Okay.

519
00:34:39.860 --> 00:34:44.860
We select one out of 250 60 feature map and we're going to learn a lot of data

520
00:34:45.950 --> 00:34:50.950
for propagated to the network and look which data points have had the maximum

521
00:34:52.191 --> 00:34:53.570
activation of this feature map.

522
00:34:55.250 --> 00:34:57.710
So let's say we do it with the first feature map.

523
00:34:57.740 --> 00:35:02.740
We notice that these are the top five images that really fired this feature map,

524
00:35:04.790 --> 00:35:06.410
like high activations on the feature.
Ma'Am,

525
00:35:07.010 --> 00:35:12.010
what it tells us is there's probably this feature map is detecting shirts could

526
00:35:12.621 --> 00:35:13.221
do the same thing.

527
00:35:13.221 --> 00:35:18.221
Let's say we take the second feature map and we look which data points have

528
00:35:19.790 --> 00:35:24.230
maximized the activations of this feature map out of a lot of data and we see

529
00:35:24.231 --> 00:35:25.880
that this is what we got.

530
00:35:25.910 --> 00:35:30.910
The top five images probably means that the other feature map seems to be

531
00:35:31.041 --> 00:35:34.130
activated when seeing edges.

532
00:35:36.230 --> 00:35:39.450
So the second one is more likely to appear earlier in their network,

533
00:35:39.451 --> 00:35:40.590
obviously then later on.

534
00:35:43.420 --> 00:35:46.840
So one thing that you may ask is these images seem crop,

535
00:35:46.900 --> 00:35:51.040
like I don't think that this was an image in the data set is probably a sub part

536
00:35:51.100 --> 00:35:55.360
of the image.
What do you think this crop corresponds to?

537
00:36:06.470 --> 00:36:11.330
Any idea how we crops the image
and why these are crops?

538
00:36:17.740 --> 00:36:20.200
Like what?
Why didn't I show you the full images?

539
00:36:20.410 --> 00:36:22.480
How was I able to show you the crop?

540
00:36:29.840 --> 00:36:31.520
<v 5>Anything outside?
It's not important.</v>

541
00:36:33.130 --> 00:36:35.890
<v 0>That's correct.
So let's say we pick an activation,</v>

542
00:36:37.470 --> 00:36:38.670
an activation into network.

543
00:36:39.240 --> 00:36:43.740
This activation for a combination or your network oftentimes doesn't see the

544
00:36:43.741 --> 00:36:47.430
entire input image,
right?
Doesn't see it.

545
00:36:48.120 --> 00:36:52.320
What it sees is a subspace of the inputs image.

546
00:36:53.400 --> 00:36:54.233
<v 3>Okay,</v>

547
00:36:54.240 --> 00:36:57.380
<v 0>that makes sense.
So let's look at another slide here.</v>

548
00:36:57.381 --> 00:37:01.520
We have a picture of Eunice 64 by 64 by three it's our inputs.

549
00:37:01.521 --> 00:37:03.560
We run it through a five layer continence

550
00:37:05.060 --> 00:37:09.230
and now we get an encoding volume that is much smaller in height and width but

551
00:37:09.260 --> 00:37:10.640
bigger in depth.

552
00:37:12.380 --> 00:37:17.030
If I tell you what this activation is seeing,
if you might be back,

553
00:37:17.031 --> 00:37:19.310
you look at the stride and the filter size you've used,

554
00:37:19.340 --> 00:37:24.020
you could say that this is the part that this feature is sync the this,
this,
uh,

555
00:37:24.080 --> 00:37:25.330
this activation is sick.

556
00:37:25.790 --> 00:37:30.650
It means the pixel that was up there had no influence on this activation and it

557
00:37:30.651 --> 00:37:32.960
makes sense when you think of it,
you're,
you're,

558
00:37:33.230 --> 00:37:36.980
the easiest way to think about it is looking at the,
the top picks,

559
00:37:37.010 --> 00:37:40.400
the topic entry on the encoding volume,
top left entry.

560
00:37:41.720 --> 00:37:44.810
You have the input image,
you put a filter here,
these filter,

561
00:37:44.811 --> 00:37:48.020
it gives you one number,
right?
This number,

562
00:37:48.021 --> 00:37:51.170
this activation only depends on this part of the image,

563
00:37:52.220 --> 00:37:54.860
but then if you add a convolution after it,

564
00:37:54.950 --> 00:37:59.270
it will take more filters.
And so the deeper you go,

565
00:37:59.840 --> 00:38:02.720
the more part of the image,
the activation we'll see.

566
00:38:03.500 --> 00:38:08.500
So if you look at an activation in layer 10 it will see much a much larger part

567
00:38:08.691 --> 00:38:12.350
of the inputs.
Then an activation in layer one.
That makes sense.

568
00:38:13.760 --> 00:38:18.500
So that's why,
that's why probably the pictures that I showed here,

569
00:38:18.590 --> 00:38:23.390
these ones are very small part truck crops,
small crops of the image,

570
00:38:23.391 --> 00:38:27.640
which means the activation I was talking about here is probably earlier in their

571
00:38:27.641 --> 00:38:29.840
network.
It sees a much smaller part of the input.

572
00:38:37.690 --> 00:38:41.150
<v 2>It's going to respond to one the image.</v>

573
00:38:42.850 --> 00:38:46.330
Yeah.
Yeah.
So what you look at is which activation was maximum.

574
00:38:47.320 --> 00:38:52.090
You look at this one and then you map this one back to crop.
It makes sense.

575
00:38:55.180 --> 00:38:56.013
Okay.

576
00:38:56.050 --> 00:39:00.250
<v 0>So here's UNESCO gain up and saying this one would correspond more in the center</v>

577
00:39:00.251 --> 00:39:04.210
of the image
is intuition.
Make sense?

578
00:39:06.160 --> 00:39:06.993
<v 1>Okay.</v>

579
00:39:07.750 --> 00:39:11.470
<v 0>Okay,
cool.
So let's talk about deconvolution.</v>

580
00:39:11.471 --> 00:39:13.810
Now there's going to be the hardest part of the lecture,

581
00:39:13.811 --> 00:39:18.250
but probably helping with with more intuition on deconvolution.

582
00:39:18.550 --> 00:39:23.550
You remember that that was the generative adversarial networks scheme and we

583
00:39:24.851 --> 00:39:27.070
said that giving a code to the generator,

584
00:39:27.100 --> 00:39:28.990
the generator is able to output an image.

585
00:39:29.980 --> 00:39:33.280
So there's something happening here that we didn't talk about is how can we

586
00:39:33.281 --> 00:39:38.281
start with a 100 dimensional vector and outputs a 64 by 64 by three image?

587
00:39:41.170 --> 00:39:43.450
That seems weird.
We could use,

588
00:39:43.451 --> 00:39:48.451
you might say a fully connected layer with a lot of neurons right to up sample

589
00:39:49.270 --> 00:39:52.690
in practice.
This is one method and other one is to use a d convolution network.

590
00:39:53.290 --> 00:39:58.290
So convolutions will encode the information in a smaller volume in heightened

591
00:39:58.801 --> 00:40:02.840
with deeper in in depth while the deconvolution.

592
00:40:02.880 --> 00:40:04.240
We'll do the reverse.

593
00:40:04.660 --> 00:40:08.170
It will up sample the height and width of an image,

594
00:40:08.950 --> 00:40:10.540
so that would be useful in this case,

595
00:40:12.190 --> 00:40:14.680
another chase where it would be usefully segmentation.

596
00:40:14.710 --> 00:40:17.870
You remember our case studies for segmentation,
life,
lifestyle,

597
00:40:18.490 --> 00:40:23.380
microscopic images of sales.
Give it to a convolution network.

598
00:40:23.470 --> 00:40:26.200
It's going to encode it so it's going to lower the heightened with.

599
00:40:27.670 --> 00:40:30.970
The interesting thing about this encoding in the middle is that it holds a lot

600
00:40:30.971 --> 00:40:34.450
of meaningful information,
but what we want ultimately,

601
00:40:34.451 --> 00:40:39.130
it's to get a segmentation mask and the segmentation mosque in height and width

602
00:40:39.131 --> 00:40:42.110
has to be the same size as the big sell limit.

603
00:40:42.700 --> 00:40:45.550
So we need a deconvolution network to add sample it.

604
00:40:47.200 --> 00:40:48.033
<v 1>Yeah,</v>

605
00:40:48.110 --> 00:40:50.060
<v 0>so the competition are used in these cases.</v>

606
00:40:50.480 --> 00:40:53.750
Today the case we're going to talk about is visualization.

607
00:40:55.130 --> 00:40:57.590
Remember the gradient assence method we talked about.

608
00:40:58.100 --> 00:41:02.030
We did find an objective function by choosing an activation in the middle of the

609
00:41:02.031 --> 00:41:02.361
network,

610
00:41:02.361 --> 00:41:06.050
and we want the objective to be called to this activation to find the input

611
00:41:06.051 --> 00:41:10.310
image that maximizes his activation through an iterative process.
Now,

612
00:41:10.311 --> 00:41:12.110
we don't want to use any strategy process.

613
00:41:12.350 --> 00:41:16.850
We want to use a reconstruction of this activation directly in the input space

614
00:41:17.000 --> 00:41:18.920
by one backward pass.

615
00:41:19.870 --> 00:41:24.870
So let's say I select this feature map out of the 40 to 55 sorry,

616
00:41:27.291 --> 00:41:32.291
five by five by two 56 what I'm going to do is I'm going to identify the Max of

617
00:41:33.591 --> 00:41:35.990
these feature map.
Here it is.
It's this one,

618
00:41:38.790 --> 00:41:43.110
third column.
Second row,
I'm going to set all the others to zero.

619
00:41:43.890 --> 00:41:48.840
Just this one.
I keep it because it seems that this one has detected something.

620
00:41:49.560 --> 00:41:50.700
Don't want to talk about the others.

621
00:41:51.660 --> 00:41:55.680
I'm going to try to reconstruct in the input space what this activation has

622
00:41:55.710 --> 00:41:56.543
fired for.

623
00:41:57.120 --> 00:42:02.120
So I'm willing to come to the reverse mathematical operation of pooling,

624
00:42:02.580 --> 00:42:06.380
Relo and convolution.
I will pool,

625
00:42:06.520 --> 00:42:10.290
I will unrelevant.
Let's say it doesn't like these word doesn't exist.

626
00:42:10.291 --> 00:42:13.470
We don't use it with unreal and ECON.

627
00:42:13.920 --> 00:42:17.790
And I will do it several times because these activation went through several of

628
00:42:17.791 --> 00:42:22.080
them.
So I do it again and again until I see,
oh,

629
00:42:23.130 --> 00:42:28.130
this specific activation that I selected in the feature map fired because it's

630
00:42:29.371 --> 00:42:34.200
so the ears of the dog and as you see,
this image is cropped.
Again,

631
00:42:34.230 --> 00:42:37.170
it's not the entire image,
it's just the part that the activation has seen.

632
00:42:37.620 --> 00:42:40.380
And if you look at where the activation is located on the feature map,

633
00:42:40.740 --> 00:42:43.350
it makes sense that this is the part that corresponds to it.

634
00:42:45.480 --> 00:42:48.450
So now the higher level intuition is this.

635
00:42:48.510 --> 00:42:52.520
We're going to delve into it and see what do we mean by an pull?

636
00:42:52.530 --> 00:42:56.790
What do we mean by unreliable and what do we mean by decomp?
Okay,

637
00:42:57.690 --> 00:42:58.523
yes,

638
00:42:59.650 --> 00:43:03.230
<v 4>you're at the restaurant.
What do we,</v>

639
00:43:03.790 --> 00:43:07.630
a reconstruction of the whole image.
So the new phrase,

640
00:43:08.860 --> 00:43:10.270
zero out all the activations,

641
00:43:10.650 --> 00:43:13.620
he said that these three construction would be messier.
Okay.

642
00:43:14.130 --> 00:43:16.210
It's going to be more messy.
Shit doesn't,

643
00:43:16.270 --> 00:43:21.190
doesn't necessarily mean you will not get the full image because probably the

644
00:43:21.191 --> 00:43:24.800
other activations probably didn't even fire it means they didn't detect it.

645
00:43:24.820 --> 00:43:26.890
Anything else?
It's just that it's gonna.

646
00:43:26.920 --> 00:43:31.120
It's gonna add some noise to this reconstruction.
Okay,

647
00:43:31.150 --> 00:43:34.360
so let's talk about the convolution and he'll beat on the board.

648
00:43:37.890 --> 00:43:40.090
So to starts with deconvolution.

649
00:43:41.930 --> 00:43:42.763
<v 1>Yeah.</v>

650
00:43:42.870 --> 00:43:44.420
<v 0>And you,
you guys can take notes if she wanted.</v>

651
00:43:44.421 --> 00:43:47.880
We were going to spend about 20 minutes on the board now to discuss the

652
00:43:47.900 --> 00:43:48.900
convolution.
Okay.

653
00:43:55.490 --> 00:43:58.880
To understand the convolution.
We first need to understand the convolution.

654
00:43:59.480 --> 00:44:03.170
We've seen it from a computer science perspective,

655
00:44:03.530 --> 00:44:07.670
but actually what we're going to do here is we're going to frame the convolution

656
00:44:07.700 --> 00:44:10.910
as a simple matrix vector mathematical operation.

657
00:44:11.870 --> 00:44:13.520
You're going to see that it's actually possible.

658
00:44:13.970 --> 00:44:15.620
So let's start with the one decomp

659
00:44:26.150 --> 00:44:29.480
41 deconvolution.
I will take an input x,

660
00:44:29.690 --> 00:44:34.690
which is of size 12 x one x two x three x four x five x six x seven x eight so

661
00:44:41.581 --> 00:44:45.540
eight plus two padding,
which gives me the 12th that I mentioned.

662
00:44:47.280 --> 00:44:51.630
So the inputs is a one dimensional vector,

663
00:44:52.410 --> 00:44:55.380
which has hardening off to on both sides.

664
00:44:57.000 --> 00:44:59.920
I will give it to a layer that will be a one d calm.

665
00:45:01.860 --> 00:45:04.230
And this layer we'd have only one filter

666
00:45:05.940 --> 00:45:07.140
and the filter size

667
00:45:10.740 --> 00:45:11.573
we before

668
00:45:14.970 --> 00:45:18.240
we will also use a stride equal to two.

669
00:45:22.380 --> 00:45:26.880
So my first question is what's the size of the outputs?

670
00:45:27.240 --> 00:45:29.130
Can you guys come to teach on your,

671
00:45:30.380 --> 00:45:33.490
on your notepads and,

672
00:45:33.680 --> 00:45:35.570
and tell me what's the size of the outputs?

673
00:45:44.490 --> 00:45:49.490
<v 4>Employee size 12 intero size for straight off to hiding off to</v>

674
00:45:52.130 --> 00:45:53.350
fight.
Yeah,
I heard it.
Yeah.

675
00:45:54.510 --> 00:45:57.320
<v 0>So you remember you use n x,
sorry.</v>

676
00:45:58.050 --> 00:46:03.050
And Y equals x minus f Plus Two p divided by stride and you will get five.

677
00:46:08.070 --> 00:46:13.070
So what I'm going to get is why one y two y three y four white fife.

678
00:46:18.200 --> 00:46:19.033
<v 1>Yeah.</v>

679
00:46:22.670 --> 00:46:26.300
<v 0>So I'm going to focus on this specific convolution for now and I'm going to show</v>

680
00:46:26.301 --> 00:46:28.700
now that we can define it as,

681
00:46:30.540 --> 00:46:32.970
as a mathematical operation between the Matrix and a vector.

682
00:46:33.600 --> 00:46:35.280
So the way to do it is,

683
00:46:35.520 --> 00:46:40.410
I guess the easiest way is to write the system of equation that is underlying

684
00:46:40.411 --> 00:46:44.370
here.
What is white wine?
Why one is,

685
00:46:45.370 --> 00:46:45.900
<v 1>okay,</v>

686
00:46:45.900 --> 00:46:50.520
<v 0>the filter applied to the four first values here.
Does it make sense?</v>

687
00:46:50.910 --> 00:46:52.530
So he find the find my center

688
00:46:54.540 --> 00:46:59.540
as being t y w one w two w three NW four.

689
00:47:01.800 --> 00:47:06.800
What I'm going to get is that y one equals w one times zero plus w two times

690
00:47:08.431 --> 00:47:11.520
zero plus w three times x one.

691
00:47:12.040 --> 00:47:16.920
So it's got the u four times x to you just make sense.

692
00:47:18.180 --> 00:47:22.590
Just a convolution element twice operation and then some all of it.

693
00:47:26.450 --> 00:47:29.890
Why two is going to be same thing,

694
00:47:29.920 --> 00:47:32.140
but we just tried have to going to down.

695
00:47:33.240 --> 00:47:35.770
So he's going to give me w one times x,

696
00:47:35.771 --> 00:47:39.940
one plus w two times x to close that to you.

697
00:47:39.941 --> 00:47:41.290
Three times x,

698
00:47:41.291 --> 00:47:45.040
three plus w four times x floor.

699
00:47:46.360 --> 00:47:50.200
Correct.
Everybody's following no.

700
00:47:51.370 --> 00:47:52.203
Same thing.

701
00:47:52.390 --> 00:47:57.100
We will do it for all the wise until y five and we know that's why five is

702
00:47:57.101 --> 00:48:00.970
element twice operation between the filter and the four last number here,

703
00:48:01.450 --> 00:48:06.450
summing them since we give me w one times x seven plus w two times x eight plus

704
00:48:10.841 --> 00:48:15.841
zero plus w three times zero plus w four time zero.

705
00:48:27.250 --> 00:48:30.940
Okay.
Now what we're going to do is to try to write down why

706
00:48:32.980 --> 00:48:36.490
as a matrix vector operation between w and x.

707
00:48:37.360 --> 00:48:39.670
We need to find what these w matrix is

708
00:48:41.350 --> 00:48:45.070
and looking at this system of equation,
it seems that it's not impossible.

709
00:48:46.240 --> 00:48:49.660
So let's try to do it.
I will write my wife Vector here,

710
00:48:49.870 --> 00:48:54.700
y one y two y three y 4.5

711
00:48:56.720 --> 00:48:59.050
and I will write my matrix here

712
00:49:02.710 --> 00:49:04.030
and my vector x here.

713
00:49:05.890 --> 00:49:10.720
So first question is what do you think will be the shape of this w matrix?

714
00:49:18.140 --> 00:49:23.050
<v 2>Five bites way correct.
We know that this is five by one this</v>

715
00:49:23.270 --> 00:49:28.270
<v 0>is 12 by one so of course WWE is going to be five by 12th right?</v>

716
00:49:30.590 --> 00:49:35.590
So now let's try to fill it in zero zero x one x two x treat,

717
00:49:36.290 --> 00:49:39.290
blah blah blah,
x eight zero zero

718
00:49:41.660 --> 00:49:46.430
can you guys seem to back or no?
Yeah.
Okay,
cool.
Uh,

719
00:49:46.460 --> 00:49:49.970
so I'm willing to fill in this matrix regarding this system of equation.

720
00:49:50.690 --> 00:49:55.610
I know that the why one would be w one times zero w two times zero w three

721
00:49:55.611 --> 00:49:56.950
dynamics one w four times x two.

722
00:49:57.320 --> 00:50:02.120
So this vector is willing to multiply the first role here.

723
00:50:02.360 --> 00:50:06.050
So I just have to place my W's here.
That'd be one.

724
00:50:06.080 --> 00:50:09.710
We've come here at Monte plays zero w two we'll come here.

725
00:50:09.740 --> 00:50:13.730
W three would come here and w four we'd come here and all the rest would be

726
00:50:13.731 --> 00:50:17.420
filled in with Zeros.
Right?
I don't want any more multiplications.

727
00:50:18.740 --> 00:50:20.990
How about the second row of this matrix?

728
00:50:21.320 --> 00:50:25.970
I know that why two has to be called to this dot product with this role.

729
00:50:26.240 --> 00:50:29.850
And I know that it's to give me w one x one plus w two x two plus there'll be

730
00:50:29.870 --> 00:50:34.870
three x three x one is the third input on these vector to turn entry.

731
00:50:36.590 --> 00:50:41.590
So I would need to shift what I had in the previs row we just trade off to until

732
00:50:42.560 --> 00:50:43.393
it gives me that,

733
00:50:43.970 --> 00:50:44.803
<v 1>oh</v>

734
00:50:48.030 --> 00:50:52.320
<v 0>it doesn't make sense.
So if I use the dot product of this row with that,</v>

735
00:50:52.350 --> 00:50:56.940
I should get the second equation up there
and so on.

736
00:50:56.970 --> 00:51:01.320
And you understand what happens,
right?
This pattern we just shift.

737
00:51:01.590 --> 00:51:03.000
We just tried off to on the side.

738
00:51:03.510 --> 00:51:08.510
So I would get zeroes here and I would get my w one w two w three w four and

739
00:51:10.261 --> 00:51:15.030
then Zeros and all the way down here and all the way down here.

740
00:51:15.031 --> 00:51:15.721
What did we get?

741
00:51:15.721 --> 00:51:20.721
These w four w three w two w one and Zeros.

742
00:51:23.980 --> 00:51:27.730
So the only thing I want to mention here is that the competition operation as

743
00:51:27.731 --> 00:51:31.240
you see can be framed as a simple matrix times vector.

744
00:51:33.370 --> 00:51:34.203
<v 4>Yes.</v>

745
00:51:37.700 --> 00:51:41.280
Cause that's going to
pour the top row.

746
00:51:41.430 --> 00:51:43.570
Why are on the right side?
Yes.

747
00:51:44.080 --> 00:51:46.630
Because I don't want white hat

748
00:51:46.950 --> 00:51:51.950
<v 0>one to be dependent on x three two x eight so I want this to be zero</v>

749
00:51:53.071 --> 00:51:54.120
multiplicate pliers.

750
00:52:01.590 --> 00:52:02.660
<v 1>Okay.
So</v>

751
00:52:03.850 --> 00:52:07.510
<v 4>why is this important for the inclusion behind Judy convolution in the existence</v>

752
00:52:07.511 --> 00:52:12.511
of the convolution is because if we manage to write down why you called w x,

753
00:52:13.510 --> 00:52:17.680
we probably can write down x equal

754
00:52:18.150 --> 00:52:20.100
<v 0>w minus one.
Why</v>

755
00:52:22.080 --> 00:52:24.480
is w is an invertible matrix?

756
00:52:27.250 --> 00:52:31.990
And this is going to to be our the convolution.
And in fact,
what's the,
the,

757
00:52:32.050 --> 00:52:33.970
what's the shape of this new matrix?

758
00:52:43.950 --> 00:52:44.783
<v 1>Hmm.</v>

759
00:52:45.940 --> 00:52:47.110
<v 4>Yes.
Twice by five.</v>

760
00:52:50.290 --> 00:52:54.070
<v 0>We have 12 by one on one site,
five by one on the other.
It has to be 12 by five.</v>

761
00:52:54.130 --> 00:52:55.750
So it's flipped competency w

762
00:52:59.030 --> 00:53:03.310
so one thing we going to do here is we're going to make an assumption.

763
00:53:03.970 --> 00:53:07.780
First assumption is that w is an invertible matrix.

764
00:53:08.530 --> 00:53:12.010
And on top of that,
we were going to make us stronger assumption,
which is

765
00:53:14.020 --> 00:53:18.130
that w is an autobahn all matrix.

766
00:53:29.490 --> 00:53:33.710
Without going into the details here,
same as when we proved,
uh,

767
00:53:33.860 --> 00:53:36.360
exactly [inaudible] initialization in sections.

768
00:53:36.361 --> 00:53:38.640
We made some assumptions that are not always true.

769
00:53:38.970 --> 00:53:42.630
This assumption is not going to be always true.
One,

770
00:53:42.810 --> 00:53:46.410
one intuition that you can have is if I'm using a filter that is,

771
00:53:48.460 --> 00:53:50.820
assume the filter is an edge detector,

772
00:53:51.030 --> 00:53:55.050
so like a plus one zero zero minus one

773
00:53:58.080 --> 00:54:00.790
in this case,
the matrix would be orthogonal.

774
00:54:01.620 --> 00:54:06.620
Why a matrix that he's auto knoll means that if I take two of the columns here,

775
00:54:08.700 --> 00:54:11.280
I dot product them together,
it should give me zero.

776
00:54:12.720 --> 00:54:14.280
Same with the rows.
You can see it.

777
00:54:14.520 --> 00:54:19.520
So what's interesting is that if the stride was for,

778
00:54:21.480 --> 00:54:23.730
there will be no overlap between these two rocks.

779
00:54:24.160 --> 00:54:27.810
It would give me a nautical matrix here.
Let's try this too.

780
00:54:28.290 --> 00:54:33.290
But if I replace this w one by minus one zero zero plus one plus one zero zero

781
00:54:33.301 --> 00:54:37.230
minus one and minus plus one zero zero minus one you can see that the dot

782
00:54:37.231 --> 00:54:38.250
product would be zero.

783
00:54:39.140 --> 00:54:42.940
The zero is when we multiply the ones and the ones who were multiplied.

784
00:54:42.950 --> 00:54:45.510
The Zeros give me a zero dot.
Prague.

785
00:54:45.511 --> 00:54:49.020
So this is a case where it works in practice.
It doesn't always work.

786
00:54:49.680 --> 00:54:52.590
The reason we're making this assumption is because we want to make a

787
00:54:52.591 --> 00:54:54.390
reconstruction,
right?

788
00:54:54.660 --> 00:54:59.310
So we want to be able to have these w minus one,
this,
this,

789
00:54:59.940 --> 00:55:03.300
this invert and the reconstruction is not going to be exact,

790
00:55:03.690 --> 00:55:08.690
but at at a first order approximation we can assume that the reconstruction will

791
00:55:09.271 --> 00:55:12.630
still be useful to us even if this assumption is not always true.

792
00:55:13.650 --> 00:55:15.480
In the case where Wu is auto go,
no,

793
00:55:16.500 --> 00:55:21.500
I know that's the universe of w w transpose or another way to write it is that

794
00:55:23.071 --> 00:55:24.510
for Autobahn or matrices,

795
00:55:25.140 --> 00:55:28.230
w transports time w is the identity matrix.

796
00:55:30.030 --> 00:55:35.030
So what's he tells me is that x is going to be done the You transpose Tame y

797
00:55:36.210 --> 00:55:39.810
times y.
So let's see,

798
00:55:40.350 --> 00:55:41.520
what do we get from that?

799
00:55:51.170 --> 00:55:52.730
<v 1>Me write down the MNC code.</v>

800
00:56:01.670 --> 00:56:05.480
So

801
00:56:06.240 --> 00:56:10.800
<v 0>let's say now we have our x and we want to regenerate our,</v>

802
00:56:11.370 --> 00:56:14.880
oh we will have our why and we want to generate our x using this method.

803
00:56:15.510 --> 00:56:18.750
So I would once I read rights is

804
00:56:20.610 --> 00:56:22.540
to understand the one D D Con.

805
00:56:23.650 --> 00:56:28.240
We can use the following illustrations where we have x here,

806
00:56:29.170 --> 00:56:34.170
which is zero zero x one x two x three all the way down to x eight

807
00:56:37.570 --> 00:56:38.403
okay,

808
00:56:39.880 --> 00:56:42.880
and I will have my w matrix here,

809
00:56:44.950 --> 00:56:49.950
w transpose and my why you vector y one y two y three y four and why five here

810
00:56:55.510 --> 00:56:59.770
and so I know that this matrix will be the transpose of the one I have here.

811
00:56:59.980 --> 00:57:02.220
Right?
So I can just write down the transport.

812
00:57:02.840 --> 00:57:07.540
The transport is will the w one w two w three none that you four.

813
00:57:08.280 --> 00:57:12.920
Okay.
I we shifted down.
We just tried off too

814
00:57:18.560 --> 00:57:19.393
<v 1>and so on</v>

815
00:57:32.110 --> 00:57:34.330
<v 0>and this whole thing will be w transpose.</v>

816
00:57:38.770 --> 00:57:39.603
<v 1>Okay,</v>

817
00:57:41.540 --> 00:57:46.400
<v 0>so do the small issue here is that this in practice is not,</v>

818
00:57:46.980 --> 00:57:49.160
it's going to be very similar to a convolution,

819
00:57:49.790 --> 00:57:54.710
but because it's going to meet a tiny little different in terms of

820
00:57:54.711 --> 00:57:55.544
implementation.

821
00:57:56.650 --> 00:58:01.220
Another question I might ask is how can we do the same thing with the same

822
00:58:01.221 --> 00:58:02.600
pattern as we have here?

823
00:58:02.990 --> 00:58:06.260
It means the stride is going from left to right.

824
00:58:06.320 --> 00:58:07.730
Instead of going from up to down.

825
00:58:11.030 --> 00:58:15.340
I'm going to introduce that with a technique called sub peak.
So

826
00:58:17.300 --> 00:58:18.133
convolution

827
00:58:20.570 --> 00:58:23.570
and for those of you who read papers in Segmentation,

828
00:58:23.571 --> 00:58:26.660
in visualization oftentime this is the type of convolution that is used for

829
00:58:26.680 --> 00:58:28.850
reconstruction.
So let's see how it works.

830
00:58:33.200 --> 00:58:35.990
I just wanted to do the same operation,
but instead of doing it,

831
00:58:36.080 --> 00:58:38.330
we just try going from up to down.

832
00:58:38.360 --> 00:58:40.220
I want to do it from a strike going from left to right.

833
00:58:44.580 --> 00:58:45.413
<v 1>Okay.</v>

834
00:58:46.700 --> 00:58:48.570
<v 0>Oh well one thing you want to,</v>

835
00:58:48.870 --> 00:58:52.670
you want to notice here is that uh,

836
00:58:52.980 --> 00:58:57.480
the two lines that I wrote here are cropped.

837
00:58:59.520 --> 00:59:03.810
And the reason is because we're using a potted inputs here.

838
00:59:03.840 --> 00:59:08.040
We would just crop the two top lines and same for the two last lines.

839
00:59:09.980 --> 00:59:10.950
Then we'd be corrupt.

840
00:59:15.180 --> 00:59:17.780
Okay.
That's w one.
We might simply,

841
00:59:17.781 --> 00:59:21.500
why one and this one we want to buy y two so on.

842
00:59:21.501 --> 00:59:26.360
So this dot product we give me w one times y one but I don't want that to happen

843
00:59:26.390 --> 00:59:28.310
because I want to get to [inaudible] zero here.

844
00:59:28.760 --> 00:59:33.760
So we just crop that's in this matrix is actually going to be smaller than it

845
00:59:34.281 --> 00:59:35.114
seems.

846
00:59:35.240 --> 00:59:39.630
It's going to generate my x one three x way eight and then I will pad the diff

847
00:59:39.631 --> 00:59:42.530
top values in the button values.
Okay,

848
00:59:47.340 --> 00:59:51.390
so let's look at the sub Pixel convolution.
I have my inputs

849
00:59:57.640 --> 00:59:59.650
and now we do something quite fun.

850
01:00:03.260 --> 01:00:07.640
I would perform a sub pixel operation on why,
what does it mean?

851
01:00:07.730 --> 01:00:10.820
I will insert Zeros almost everywhere.

852
01:00:11.460 --> 01:00:16.460
I would insert them and now we get zero zero y one zero two zero three zero y

853
01:00:19.411 --> 01:00:22.790
four zero y five and zero zero

854
01:00:25.300 --> 01:00:28.970
zero here.
So these victories,
just

855
01:00:31.210 --> 01:00:35.830
the vector why with some zeros inserted around it and also in the middle between

856
01:00:35.950 --> 01:00:40.150
the elements of why.
Now,
why is that interesting?

857
01:00:40.690 --> 01:00:45.690
It's interesting because I can now write down my convolution by flipping my

858
01:00:47.141 --> 01:00:47.974
weight.

859
01:01:07.950 --> 01:01:10.770
So let me explain a little bit what happened here.

860
01:01:13.880 --> 01:01:17.750
What we wanted is in order to be able to efficiency come to,

861
01:01:17.760 --> 01:01:22.310
to do deconvolution the same way as we've learned to compute the convolution.

862
01:01:23.090 --> 01:01:28.090
We wanted to have the weights scattered from left to right with a stride moving

863
01:01:28.611 --> 01:01:29.444
from left to right.

864
01:01:30.380 --> 01:01:35.380
What we did is that we use a sub pixel version of y by inserting Zeros in the

865
01:01:35.481 --> 01:01:37.670
middle.
And we divided the stride by two.

866
01:01:38.240 --> 01:01:42.050
So instead of having a straight off to as we had in our convolution,

867
01:01:42.410 --> 01:01:45.210
we have a stride of one in order the convolution.

868
01:01:45.560 --> 01:01:50.560
So Nazis that I shift my weights from one at every step when I moved from one

869
01:01:51.141 --> 01:01:55.160
role to another.
Second thing is I flipped my weights,

870
01:01:56.650 --> 01:02:00.940
I flipped my ways.
So instead of having WWE and WCW,
CW four,

871
01:02:01.120 --> 01:02:03.520
now I have w four W's to WWF.

872
01:02:04.840 --> 01:02:09.840
And what you could see is looking at that first look at this row.

873
01:02:11.680 --> 01:02:16.680
The first road that is not cropped the result of the dot product of this row

874
01:02:18.990 --> 01:02:23.990
with this vector is going to be y one times w three plus.

875
01:02:26.560 --> 01:02:30.280
Why two times w one?
Yeah.

876
01:02:31.000 --> 01:02:35.230
Now let's look what happened here.
I look at my first role here,

877
01:02:36.100 --> 01:02:40.180
the dot product of these first room with my why.

878
01:02:40.181 --> 01:02:44.590
Here is going to be uh,
sorry.
Sorry,

879
01:02:44.770 --> 01:02:46.280
we these two are crops is one

880
01:02:49.860 --> 01:02:50.693
and same here.

881
01:02:53.260 --> 01:02:58.260
So looking at my first non cropped role here as a dot product with this vector,

882
01:03:00.430 --> 01:03:04.000
what I get is w three times y one

883
01:03:05.530 --> 01:03:10.390
plus w two uh,
sorry,
plus w one times white too.

884
01:03:11.200 --> 01:03:12.880
So exactly the same thing as I got there.

885
01:03:13.750 --> 01:03:18.400
So these two operations are exactly the same operations.
They're just same thing.

886
01:03:19.060 --> 01:03:21.400
You get the same results to different way of doing it.

887
01:03:21.820 --> 01:03:26.820
One is using a weird operation with strides going from top to bottom.

888
01:03:27.430 --> 01:03:30.580
And the second one is exactly a convolution.
These are convolution,

889
01:03:31.240 --> 01:03:33.250
convolution plus flipped weights,

890
01:03:33.850 --> 01:03:38.230
insertion of Zeros for the sub Pixel version of why

891
01:03:41.870 --> 01:03:44.990
and on top of that padding here and there.

892
01:03:47.600 --> 01:03:50.900
So these ones the hardest parts.
Okay.

893
01:03:51.800 --> 01:03:54.020
Does it give you more intuition on the convolution here?

894
01:03:55.370 --> 01:03:58.880
You know now how it convolution can be framed as a mathematical operation

895
01:03:58.881 --> 01:04:03.020
between a matrix and a vector and you know also that under these assumptions,

896
01:04:03.740 --> 01:04:08.270
the way we will de convolve is just by flipping our weights,

897
01:04:09.920 --> 01:04:12.800
dividing this tried by two and inserting zeroes.

898
01:04:13.700 --> 01:04:15.490
If we just do that to where deconvolute deconvoluting

899
01:04:17.540 --> 01:04:20.810
for propagates into convolution the following way,

900
01:04:20.840 --> 01:04:25.520
you want to deconvolute just flip all the weights,
insert zero's,

901
01:04:26.000 --> 01:04:29.000
sub Pixel and finally divide the stride.

902
01:04:29.960 --> 01:04:34.280
And that's the deconvolution.
So super complex thing to understand.

903
01:04:34.281 --> 01:04:35.870
But this is the intuition behind it.

904
01:04:36.860 --> 01:04:40.670
Now let's try to have an intuition of how it would work in two dimension.

905
01:04:44.550 --> 01:04:46.490
Uh,
led to me writing down

906
01:04:54.320 --> 01:04:57.410
why'd you we use that because in terms of implementation,

907
01:04:57.411 --> 01:05:01.970
this is the same as what we've been using here
is very similar.

908
01:05:02.230 --> 01:05:06.620
What this one is another implementation.
So you could do both the same.

909
01:05:06.830 --> 01:05:07.850
It's the same operation,

910
01:05:08.120 --> 01:05:12.080
but in practice this one is easier to understand because it's exactly the same

911
01:05:12.081 --> 01:05:14.780
operation of the convolution with flipped weights,

912
01:05:14.840 --> 01:05:19.460
insertion of Zeros and divided strike.
That's why I wanted to shoot it.

913
01:05:20.820 --> 01:05:25.370
<v 4>What happens with the assumption assumption doesn't hold.</v>

914
01:05:26.060 --> 01:05:27.830
So oftentimes you assumption doesn't hold.

915
01:05:28.100 --> 01:05:32.000
But what we want is to be able to see our construction and if we use this
method,

916
01:05:32.001 --> 01:05:34.910
we will still see your construction practice.

917
01:05:35.030 --> 01:05:39.770
If we had really w minus one,
the reconstruction would be much better.

918
01:05:40.910 --> 01:05:45.830
But we don't.
So,
uh,
let me go over to,
to the,
uh,
the two,

919
01:05:45.831 --> 01:05:46.401
the example.

920
01:05:46.401 --> 01:05:49.190
We're going to go a little overtime because we have two hours technically for

921
01:05:49.260 --> 01:05:51.320
one hour and 50 minutes.
And,
uh,

922
01:05:52.190 --> 01:05:57.190
and let me go over the two d example and then we will answer this question on

923
01:05:57.741 --> 01:05:59.600
why we need to make this assumption

924
01:06:06.370 --> 01:06:10.700
here is the interpretation of the two D de convolution and me writing down here.

925
01:06:10.701 --> 01:06:10.900
Right?

926
01:06:10.900 --> 01:06:11.733
<v 3>Okay.</v>

927
01:06:18.700 --> 01:06:22.510
<v 0>The intuition behind the two D D Con is I get my input,</v>

928
01:06:23.380 --> 01:06:27.940
which is five by five and this I call it X.

929
01:06:29.740 --> 01:06:30.880
I for propagates.

930
01:06:30.881 --> 01:06:35.881
It's using a filter of size two by two in a comp layer and astride off too.

931
01:06:37.810 --> 01:06:41.380
This is my convolution,
what I get.

932
01:06:42.280 --> 01:06:47.260
So if you do five minus two plus the padding,

933
01:06:47.261 --> 01:06:52.261
which is zero divided by two plus one plus one plus one,

934
01:06:54.220 --> 01:06:56.200
and you ignore it.
So,

935
01:06:56.530 --> 01:07:01.530
so five minus two divided by two gives you a three divided by two plus one.

936
01:07:05.590 --> 01:07:09.900
No,
actually we'll give you three by three.
Yeah,
three by three.
Oh.

937
01:07:09.980 --> 01:07:13.900
Why have two by three?
That's what you get.
And now,

938
01:07:14.500 --> 01:07:16.660
um,
these,
you call it,
why,

939
01:07:17.950 --> 01:07:22.030
what's you're going to do here is you're going to de convolve why in order to

940
01:07:22.031 --> 01:07:23.010
decompose,
why

941
01:07:25.750 --> 01:07:30.610
in order to de convolve it,
you're going to use a stride one.

942
01:07:31.060 --> 01:07:34.240
And what we said is that we need to divide the stride by two,
right?

943
01:07:34.810 --> 01:07:36.100
So we need to strike of one

944
01:07:37.690 --> 01:07:40.420
and the filter will be the same two by two.

945
01:07:40.720 --> 01:07:43.660
And you remember that's what we've seen is that the future is the same.

946
01:07:44.590 --> 01:07:46.060
It's just that he's going to be flipped.

947
01:07:46.900 --> 01:07:50.170
So you will use a filter of Dubai to but flipped

948
01:07:54.550 --> 01:07:55.900
and now what do we get?

949
01:07:56.590 --> 01:07:59.710
We hope to get a five by five inputs,

950
01:08:00.970 --> 01:08:04.780
which is going to be our reconstructed x five by five inputs.

951
01:08:05.110 --> 01:08:08.740
And the way we're going to do it is this is the intuition behind it.
Yeah.

952
01:08:15.140 --> 01:08:18.000
<v 4>Yeah.
It's too late to look it up too late too.</v>

953
01:08:19.190 --> 01:08:23.870
Thanks to like to um,
five by fights here.

954
01:08:24.170 --> 01:08:25.640
That's what we hope to reconstruct.

955
01:08:26.150 --> 01:08:30.170
The way we will do it is we will take this into s is two by two.

956
01:08:30.890 --> 01:08:32.180
We will put it here

957
01:08:34.880 --> 01:08:39.880
and we will multiply all the weights of this filter by y one one all the weights

958
01:08:42.621 --> 01:08:43.940
would be multiplied by one point.

959
01:08:45.910 --> 01:08:47.380
<v 0>So we get four values here,</v>

960
01:08:48.160 --> 01:08:53.160
which are going to be w four y one one w three one one and so on.

961
01:08:54.490 --> 01:08:59.490
Now I will shift this with a strike of one and I would put my filter again here

962
01:09:01.030 --> 01:09:04.390
and I will multiply all the entries by y one,

963
01:09:04.391 --> 01:09:06.280
two and so on.

964
01:09:08.110 --> 01:09:12.430
And you'll see that this entry has an overlap.
So tweet it will,

965
01:09:12.431 --> 01:09:15.100
it will be updated at every step of the convolution.

966
01:09:15.580 --> 01:09:18.070
It's not like what happened in the FordPass.

967
01:09:18.940 --> 01:09:23.940
So this is the intuition behind the two deconvolution three the same thing.

968
01:09:24.610 --> 01:09:29.470
You have uh,
a volume here.
So your filter is going to be a volume.

969
01:09:30.070 --> 01:09:34.960
What you're going to do is you're going to put the volume here multiplied by
one,

970
01:09:34.961 --> 01:09:39.040
one,
one and so on.
And then if you have a second filter,

971
01:09:39.100 --> 01:09:42.460
you would put it again on top of it and multiply by one,
one,

972
01:09:42.461 --> 01:09:46.260
one or the weights of the filter and so on.
So these little complicated,

973
01:09:46.270 --> 01:09:49.690
but this is the intuition behind the convolution.
Okay,

974
01:09:49.780 --> 01:09:53.050
let's get back to the lecture.
I'm going to take one question here.

975
01:09:53.051 --> 01:09:54.760
If you guys need clarification,

976
01:10:00.070 --> 01:10:00.550
<v 4>no worries.</v>

977
01:10:00.550 --> 01:10:03.670
You don't understand the capabilities and fully the important part is that you

978
01:10:03.671 --> 01:10:07.980
get the intuition here and you understand how we do.
So let me make a comment.

979
01:10:09.190 --> 01:10:11.890
Why do we need to make this assumption and do we need to make,

980
01:10:12.820 --> 01:10:17.390
when we want to reconstruct like we're doing here in the visualization,

981
01:10:17.650 --> 01:10:21.610
we need to make this assumption because we don't want to retrain waits for the

982
01:10:21.611 --> 01:10:22.720
de convolution neural network.

983
01:10:23.350 --> 01:10:28.350
What we know is that the activation we selected here on the feature map is has

984
01:10:29.111 --> 01:10:33.610
gone through the entire pipeline of the confidence.
So to reconstruct,

985
01:10:33.611 --> 01:10:36.280
we need to use the weights that we already have in the confidence.

986
01:10:36.760 --> 01:10:39.490
We need to pass them to the deconvolution and reconstructs.

987
01:10:40.030 --> 01:10:44.260
If we're doing those segmentation like we talked about for the license excel,

988
01:10:45.300 --> 01:10:46.860
<v 0>we don't need to do this assumption.</v>

989
01:10:46.980 --> 01:10:51.630
We're just saying that this is a procedure that is a deconvolution and we will

990
01:10:51.631 --> 01:10:53.250
train the weights of the deconvolution.

991
01:10:54.720 --> 01:10:56.640
So there is no need to make this assumption.
He's just,

992
01:10:56.641 --> 01:11:01.260
we have a technique that is dividing those tried by one and a inserting zeroes

993
01:11:01.320 --> 01:11:02.160
and then B,

994
01:11:02.190 --> 01:11:06.510
we were trained to weights and we get an output that is an up sample version of

995
01:11:06.511 --> 01:11:09.840
the input that was given to it.
So there was two use case,

996
01:11:09.900 --> 01:11:13.440
one where you use the weights and one way you don't,
in this case,

997
01:11:13.441 --> 01:11:16.680
we don't want to retrain.
We want to use the weights.
So let's see,

998
01:11:17.130 --> 01:11:20.340
let's see a version more visual of the upsampling.

999
01:11:21.270 --> 01:11:24.150
So we did a [inaudible] image.
This is my image,

1000
01:11:24.360 --> 01:11:28.710
four by four I insert Zeros and IPAD.
I get to nine by 90 Mitch.

1001
01:11:29.520 --> 01:11:33.450
I have my filter like that and these filter will come volv.

1002
01:11:34.320 --> 01:11:36.580
I will weed controls over the input.

1003
01:11:36.581 --> 01:11:40.620
So I would place it on my inputs and at every step I would perform a convolution

1004
01:11:40.800 --> 01:11:44.340
up.
I will get a value here.
The value is blue because as you can see,

1005
01:11:44.341 --> 01:11:48.450
the waste that effected the output.
We're only the blue weights.

1006
01:11:49.020 --> 01:11:49.620
I would use this,

1007
01:11:49.620 --> 01:11:54.620
try to have one beam now the weights that affect my input are the green ones and

1008
01:11:55.441 --> 01:11:59.100
so on.
And I would just come volve as I do usually

1009
01:12:02.260 --> 01:12:04.870
and so on.
And now one step down,

1010
01:12:05.170 --> 01:12:08.740
I see that the ways that are impacting my input or the purple ones.

1011
01:12:09.010 --> 01:12:11.680
So I will put the Purple Square here and so on.

1012
01:12:11.800 --> 01:12:15.970
So I just do the convolution like that.
And so,

1013
01:12:16.870 --> 01:12:21.520
so one thing that is interesting here is that divide use auto are blue.

1014
01:12:21.521 --> 01:12:26.521
In my out of six by six outputs were generated only using the blue values of the

1015
01:12:27.461 --> 01:12:32.440
filter.
The blue weights in the filter,
the ones that are green,

1016
01:12:32.460 --> 01:12:36.850
we're only used,
you were only generated using the green values of my seater.

1017
01:12:37.150 --> 01:12:39.400
So actually this subsample sub beak.

1018
01:12:39.420 --> 01:12:44.420
So convolution or deconvolution could have been done with four convolutions with

1019
01:12:45.971 --> 01:12:48.790
the Blue [inaudible],
green weights,
uh,

1020
01:12:49.120 --> 01:12:54.120
purple weights and yellow weights and then just just replaced such that the

1021
01:12:55.571 --> 01:12:57.010
adjustment would be the output.

1022
01:12:57.700 --> 01:13:02.700
Just put the output of each of these comp and mix them to give out a six by six

1023
01:13:03.341 --> 01:13:05.320
output.
Only thing you need to know.

1024
01:13:05.321 --> 01:13:08.020
We have an input four by four and we get to an output six by six.

1025
01:13:08.021 --> 01:13:10.240
That's what we wanted.
We wanted to have sample the image.

1026
01:13:10.540 --> 01:13:13.240
We can retrain the weights or use the transport version of them.

1027
01:13:14.110 --> 01:13:17.220
So let's see what happens.
Now we understood what,
uh,

1028
01:13:17.280 --> 01:13:18.490
what did you come voice doing?

1029
01:13:18.580 --> 01:13:23.170
So we're able to decomp what we need to do is also too,
I'm pool and to unreal.

1030
01:13:24.460 --> 01:13:28.030
Fortunately it's easier than the decomp so we're not going to do board work

1031
01:13:28.031 --> 01:13:32.960
anymore.
So let's see how uncool it works.
If I give you this,
uh,

1032
01:13:33.010 --> 01:13:35.740
inputs to the pooling to a max pooling layer,

1033
01:13:36.550 --> 01:13:41.230
the output is obviously going to be this one 42 is the maximum of these four

1034
01:13:41.231 --> 01:13:43.390
numbers.
Assuming we're using a two by two center,

1035
01:13:43.391 --> 01:13:46.180
which right off to vertically and horizontally,

1036
01:13:47.620 --> 01:13:49.540
12 is the maximum of the green numbers.

1037
01:13:49.870 --> 01:13:54.010
Six is the maximum of the red numbers and seven to your engines.
No question.

1038
01:13:54.520 --> 01:13:59.470
I give you back the outputs and I tell you he's made the input.

1039
01:14:00.940 --> 01:14:05.820
Can you give me the input or no?
No.
Why?

1040
01:14:05.821 --> 01:14:08.870
Why
do you need,
you need,

1041
01:14:08.940 --> 01:14:12.160
you only achieved the maximum so you,
you,
you lost all the other

1042
01:14:12.160 --> 01:14:15.700
<v 4>numbers.
I don't know anymore the zero one and minus one.</v>

1043
01:14:15.850 --> 01:14:19.780
That's where the red numbers,
because they get passed to the maximum,

1044
01:14:20.860 --> 01:14:25.360
so max pool is not invertible from a mathematical perspective.

1045
01:14:26.080 --> 01:14:29.560
What we can do is approximate.
It's invert.
How can we do that?

1046
01:14:33.780 --> 01:14:35.410
Spread it out.
That's a good point.

1047
01:14:35.560 --> 01:14:39.010
We could spread out the six among the four values.

1048
01:14:39.550 --> 01:14:40.900
That would be an approximation.

1049
01:14:42.700 --> 01:14:46.330
A better way if we manage to cash from that I use is to catch something.

1050
01:14:46.331 --> 01:14:47.170
We called the switches.

1051
01:14:48.280 --> 01:14:53.280
We cashed the values of the maximum using a matrix that is very easy to store of

1052
01:14:53.711 --> 01:14:58.711
Zeros and ones and we pass it to the and pooling and now we can approximate the

1053
01:14:59.711 --> 01:15:01.870
inverts because we know where six was.

1054
01:15:02.110 --> 01:15:04.930
We know we're 12 was we know where 42 and seven it was,

1055
01:15:06.100 --> 01:15:09.850
but it's still not invertible because we,
we lost all the other numbers.

1056
01:15:11.640 --> 01:15:15.270
Think about Max boot backpropagation it's exactly the same thing.

1057
01:15:16.290 --> 01:15:20.610
These numbers zero one minus one the had no impact in the loss function at the

1058
01:15:20.611 --> 01:15:23.250
end because they didn't pass the two for propagation.

1059
01:15:23.940 --> 01:15:27.870
So actually with the switches,
you can have the exact backpropagation.

1060
01:15:27.900 --> 01:15:31.650
We know that the other values are going to be Zeros because they did an affected

1061
01:15:31.651 --> 01:15:35.460
the loss during the for propagation that that makes sense.

1062
01:15:36.660 --> 01:15:40.020
Okay,
so this is Max pooling,
pooling,
earn Max pooling,

1063
01:15:40.500 --> 01:15:44.360
and we can use it with the switches.
You can approximate cash.

1064
01:15:44.850 --> 01:15:46.770
The whole original matrix.
Yeah.

1065
01:15:46.830 --> 01:15:49.100
Why don't we just catch the whole Oregon electric could,

1066
01:15:49.560 --> 01:15:50.490
could catch the entire thing.

1067
01:15:50.790 --> 01:15:54.120
But in terms of back for backpropagation in terms of efficiency,

1068
01:15:54.121 --> 01:15:58.170
we would just use the switches because it's enough.
Yeah,
yeah,
yeah.

1069
01:15:58.200 --> 01:16:00.270
For on pulling your right,
we could catch everything,

1070
01:16:00.660 --> 01:16:03.840
but then it's cheating like you,
you kept it.
So just give it back.

1071
01:16:05.580 --> 01:16:09.340
Okay,
so now we know how I'm pulling works.
Let's look at the relative.

1072
01:16:10.890 --> 01:16:14.790
So what we need to do in fact is to pass the switches and the filters back to

1073
01:16:14.791 --> 01:16:18.660
the ampoule in the account in order to reconstruct switches are the matrix of

1074
01:16:18.661 --> 01:16:20.730
Zeros and ones indicating where the maximum yeah

1075
01:16:20.740 --> 01:16:25.740
<v 0>is where and filters are the filters that we transpose under this assumption</v>

1076
01:16:27.190 --> 01:16:32.020
under board.
Okay,
and so on and so on,
and I get my reconstruction.

1077
01:16:32.380 --> 01:16:33.850
I just need to explain the relevant.
Now

1078
01:16:35.560 --> 01:16:39.700
I give you this input to Relo and I forward propagated.
What do we get?

1079
01:16:39.760 --> 01:16:40.491
All the negatives.

1080
01:16:40.491 --> 01:16:45.430
I'm PR numbers are going to be equalized to zero and the others are going to be

1081
01:16:45.431 --> 01:16:49.840
kept.
Now let's say I'm doing a backpropagation to Relo.

1082
01:16:50.470 --> 01:16:52.000
What do I get if I give you that?

1083
01:16:52.030 --> 01:16:56.320
This is the gradients that are coming back and I'm asking you what are the

1084
01:16:56.321 --> 01:16:58.930
gradients after the Relo,
during the backpropagation,

1085
01:17:00.520 --> 01:17:02.530
how does the Relu behave in backdrop

1086
01:17:08.560 --> 01:17:09.393
zeros?

1087
01:17:09.790 --> 01:17:14.640
<v 4>Which ones or zeroes
there negatives are zeroes.</v>

1088
01:17:15.420 --> 01:17:17.410
You agree the negative

1089
01:17:17.430 --> 01:17:21.430
<v 0>she's in this yellow matrix are going to be Zeros during the backdrop.</v>

1090
01:17:22.700 --> 01:17:23.250
<v 3>Okay.</v>

1091
01:17:23.250 --> 01:17:24.083
<v 0>I guess.
Sure.</v>

1092
01:17:28.540 --> 01:17:29.100
<v 3>Okay.</v>

1093
01:17:29.100 --> 01:17:32.970
<v 0>Think always about what was the influence of the input on the loss function.</v>

1094
01:17:34.020 --> 01:17:34.440
<v 3>Okay?</v>

1095
01:17:34.440 --> 01:17:38.700
<v 0>And you will find out what was the backpropagation.
Look at this number,</v>

1096
01:17:41.130 --> 01:17:46.130
this number here minus two did this number have the fact that it was minus two

1097
01:17:46.470 --> 01:17:50.020
did it have any influence on the loss function?
No,

1098
01:17:50.021 --> 01:17:53.340
we could have been minus 10 it could have been minus 20 it's not going to impact

1099
01:17:53.341 --> 01:17:58.280
the loss function.
So what do you think should be the number here?
Zero.

1100
01:17:59.520 --> 01:18:02.610
Even if the number that is coming back,
the gradient is 10

1101
01:18:04.260 --> 01:18:08.100
so what do you think should be the revenue?
Backward output?

1102
01:18:17.150 --> 01:18:18.260
Same idea as Max.

1103
01:18:21.140 --> 01:18:23.360
What we need to do is to remember the switches.

1104
01:18:23.710 --> 01:18:26.990
You remember which of these values had an impact on the loss.

1105
01:18:27.610 --> 01:18:27.940
<v 3>Okay?</v>

1106
01:18:27.940 --> 01:18:32.470
<v 0>We pass the switches.
All of these values here that are kind of the why.</v>

1107
01:18:32.500 --> 01:18:37.150
You know this is a why all of these ones had no impact on the loss function.

1108
01:18:37.600 --> 01:18:40.330
So when you back propagate,
their gradients should be set to zero.

1109
01:18:40.510 --> 01:18:43.330
It doesn't matter to update them,
it's not going to make the loss go down.

1110
01:18:44.500 --> 01:18:47.410
So these are all Zeros and the rest they just pass.

1111
01:18:48.130 --> 01:18:49.600
Why do they pass with the same value?

1112
01:18:49.601 --> 01:18:52.030
Because rare relo for positive numbers was one.

1113
01:18:52.750 --> 01:18:56.350
So this number one here that passed the revenue during the fort propagation,

1114
01:18:56.351 --> 01:18:57.370
it was not modified.

1115
01:18:57.580 --> 01:19:01.360
It's gradient is going to be one that make sense.

1116
01:19:01.840 --> 01:19:06.430
So this is really backward.
Now in this reconstruction method,

1117
01:19:06.431 --> 01:19:08.020
we're not going to use rail backward.

1118
01:19:08.380 --> 01:19:11.000
We're going to use something we call a Relo de confident.

1119
01:19:11.060 --> 01:19:15.460
Let's say the reason we're not dean tuition between why we're not using rarely

1120
01:19:15.461 --> 01:19:19.870
backward is because what we're interested in is to know which pixels of the

1121
01:19:19.871 --> 01:19:24.810
impact positively affected the activation that we are talking up.

1122
01:19:25.330 --> 01:19:27.610
So what we're going to do is that we're just going to do a relo.

1123
01:19:28.300 --> 01:19:29.770
We're just going to do a really backward.

1124
01:19:30.390 --> 01:19:32.830
And another reason is when we reconstruct,

1125
01:19:32.860 --> 01:19:37.240
we want to have the minimum influence from the for propagation because we don't

1126
01:19:37.241 --> 01:19:40.270
really want our reconstruction to depend on the,
for propagation.

1127
01:19:40.690 --> 01:19:44.320
We would like our reconstruction to be unbiased and just look at this
activation,

1128
01:19:44.321 --> 01:19:48.450
reconstruct what happened.
So that's what you're going to use.

1129
01:19:49.230 --> 01:19:49.710
Again,

1130
01:19:49.710 --> 01:19:54.710
this is a hatch that has been found through trial and error and it's not going

1131
01:19:55.261 --> 01:19:59.910
to be scientifically viable all the time.
Okay,

1132
01:19:59.911 --> 01:20:04.380
so now we can do everything and we can reconstruct and find out what was this

1133
01:20:04.381 --> 01:20:08.190
activation corresponds to.
It took time to understand,
but it's super fast to do.

1134
01:20:08.191 --> 01:20:12.840
Now is just one path,
not iterative.
We could do it with every layer.

1135
01:20:12.870 --> 01:20:17.190
So let's say we do it with the first block of [inaudible] Max Spoon.

1136
01:20:17.580 --> 01:20:21.930
I go here,
I choose an activation.
I,
I,
I find the maximum activation.

1137
01:20:21.931 --> 01:20:25.080
I said all the others,
two zero I and pull relative the convent.

1138
01:20:25.081 --> 01:20:26.610
I find out the reconstruction,

1139
01:20:26.910 --> 01:20:29.460
these specifi activation was looking at edges like that.

1140
01:20:31.860 --> 01:20:36.860
So let's delve into the phone and see how we can visualize insight,

1141
01:20:37.441 --> 01:20:38.700
what's happening inside the network.

1142
01:20:38.701 --> 01:20:42.600
So all the visualization we're going to see now can be found in much use dealers

1143
01:20:42.601 --> 01:20:46.500
in rob Ferris uses paper,
visualizing,
understanding convulsion networks.

1144
01:20:46.770 --> 01:20:49.260
I'm going to explain what they correspond to,
but check,

1145
01:20:49.290 --> 01:20:52.440
check out their papers if you want to understand more and to be dictates.

1146
01:20:52.500 --> 01:20:57.500
So what's happens here is that on the top left you have nine pictures.

1147
01:20:59.580 --> 01:21:04.500
These are the crop pictures of the Dataset that activated the first filter of

1148
01:21:04.501 --> 01:21:05.970
the first layer maximum.

1149
01:21:07.500 --> 01:21:12.480
So we have a first feature on the first layer and we run all the data sets and

1150
01:21:12.481 --> 01:21:17.070
we recorded what are the main pictures that activate these filter,
these words,

1151
01:21:17.080 --> 01:21:17.913
the main ones,

1152
01:21:17.970 --> 01:21:22.590
and we did the same thing for all of the filters of the first layer and there

1153
01:21:22.591 --> 01:21:25.500
are nine times nine of them.
There are a lot of them.
I think

1154
01:21:27.720 --> 01:21:29.880
in the bottom here you have the filters,

1155
01:21:30.540 --> 01:21:35.490
which are the weights that were plotted.
Just take the filter,
plot the whites.

1156
01:21:35.940 --> 01:21:38.880
This is,
this is important only for the first layer.

1157
01:21:39.360 --> 01:21:42.540
When you go deeper into the network,
the filter itself cannot be interpreted.

1158
01:21:42.780 --> 01:21:46.770
It's super hard to understand it here because the weights are directly

1159
01:21:46.771 --> 01:21:48.000
multiplying the pixels.

1160
01:21:48.150 --> 01:21:53.150
The first lay your weights can be interpretable and in fact you see that the,

1161
01:21:54.180 --> 01:21:57.570
let's look at the third one,
the third filter here on the first row.

1162
01:21:58.020 --> 01:22:00.870
The third filter has weights that are kind of diagonal.

1163
01:22:01.380 --> 01:22:06.380
Like one of the diagonals and in fact if you look at the Datas that maximize

1164
01:22:07.290 --> 01:22:11.370
these filters,
activation in feature map corresponding to this filter,

1165
01:22:11.610 --> 01:22:14.640
they're all like cropped images that correspond to diagonals.

1166
01:22:15.240 --> 01:22:19.440
That's what happens now.
The deeper we go,
the more fun we have.

1167
01:22:19.470 --> 01:22:23.130
So let's go results on a vibration set of 50,000 images.

1168
01:22:23.700 --> 01:22:27.780
What's happened here is they took 50,000 images,

1169
01:22:27.840 --> 01:22:32.840
therefore propagated to the network they recorded which image is the maximum,

1170
01:22:33.390 --> 01:22:37.770
the one that maximized the activation of the feature map corresponding to the

1171
01:22:37.771 --> 01:22:42.570
first filter of layer two,
second filter and so on for all the filters.

1172
01:22:43.050 --> 01:22:47.940
Let's look at one of them.
We can see that.
Okay,
we have a circle on this one.

1173
01:22:47.941 --> 01:22:49.290
It means that this,

1174
01:22:49.620 --> 01:22:54.620
the filter gender which generated the feature map corresponding to this has been

1175
01:22:55.741 --> 01:22:59.630
activated through probably a wheel or something like that,
so the,

1176
01:22:59.631 --> 01:23:03.210
the image of the wheel was the one that maximizes the activation of this one and

1177
01:23:03.211 --> 01:23:05.590
then we use the deacons method to reconstruct it.

1178
01:23:07.380 --> 01:23:10.170
Any questions on that?
Yeah,

1179
01:23:11.800 --> 01:23:15.390
<v 2>face not give you the lord the question.</v>

1180
01:23:15.820 --> 01:23:17.950
What if do activation function is not really

1181
01:23:18.810 --> 01:23:19.411
<v 0>in practice,</v>

1182
01:23:19.411 --> 01:23:23.910
you would just use a backward to reconstruct if it's Dinesh would use the same,

1183
01:23:23.940 --> 01:23:27.330
the same type of method and you will try to approximate the reconstruction.

1184
01:23:30.190 --> 01:23:32.200
Okay,
let's go a little deeper.

1185
01:23:32.740 --> 01:23:36.690
So now same layer two four propagates all the images of the Dataset.

1186
01:23:36.730 --> 01:23:40.690
Find the nine images that are the maximum activity that lead to the maximum

1187
01:23:40.691 --> 01:23:44.650
activation of the first filter.
These are plotted on top here.

1188
01:23:44.860 --> 01:23:49.860
What you can see is like for this filter that is the sixth row first filter

1189
01:23:50.500 --> 01:23:52.450
features are more inviting to small changes.

1190
01:23:52.451 --> 01:23:56.830
So this filter actually was activated to many different types of circles,

1191
01:23:56.831 --> 01:24:00.760
spirels wheels,
and so it's,
it's still activated,

1192
01:24:00.761 --> 01:24:03.160
although the circles where different sites,

1193
01:24:04.710 --> 01:24:05.180
<v 3>okay,</v>

1194
01:24:05.180 --> 01:24:09.920
<v 0>can go even deeper up third layer.
What's interesting is that the deeper you go,</v>

1195
01:24:10.070 --> 01:24:11.330
the more complexity you see.

1196
01:24:11.510 --> 01:24:15.830
So at the beginning we were seeing on the edges and now we see much more complex

1197
01:24:15.890 --> 01:24:20.060
figures.
You can see a face here in this,
in this entry.

1198
01:24:20.480 --> 01:24:24.050
It means that this filter activated for when it's cds,

1199
01:24:24.100 --> 01:24:27.490
when it has seen a data point that had this face that we reconstructed,

1200
01:24:27.600 --> 01:24:30.740
it cropped it on the face.
The face is kind of red.

1201
01:24:30.770 --> 01:24:35.210
It means that the more red it was doing more activation,
it led to

1202
01:24:37.130 --> 01:24:37.660
<v 3>okay</v>

1203
01:24:37.660 --> 01:24:39.370
<v 0>and same top nine four layer tree.</v>

1204
01:24:39.371 --> 01:24:42.100
So these are the nine images that actually led to the face.

1205
01:24:42.520 --> 01:24:44.560
These are the nine images that maximize the act,

1206
01:24:44.830 --> 01:24:49.830
the activation of the feature map corresponding to that filter and so on.

1207
01:24:51.740 --> 01:24:52.573
So here's a,

1208
01:25:06.230 --> 01:25:07.063
<v 3>okay.</v>

1209
01:25:28.560 --> 01:25:29.393
Okay.

1210
01:25:59.680 --> 01:26:03.350
<v 2>We can switch back and forth between showing the actual activations and showing</v>

1211
01:26:03.380 --> 01:26:04.340
images,
synthesize

1212
01:26:04.340 --> 01:26:05.230
<v 6>to produce high activity.</v>

1213
01:26:05.630 --> 01:26:07.790
<v 5>He's giving his own.
You mentioned the network right now</v>

1214
01:26:09.280 --> 01:26:11.290
<v 6>at the time we get to the fifth convolutional layer.</v>

1215
01:26:11.530 --> 01:26:14.470
The features being computed represent abstract concepts.

1216
01:26:14.680 --> 01:26:18.970
So these are the greatest as for example,
this neuron seems to respond to faces.

1217
01:26:19.450 --> 01:26:22.030
We can further investigate this neuron by showing a few different types of

1218
01:26:22.031 --> 01:26:23.860
information.
First,

1219
01:26:23.890 --> 01:26:27.370
we can artificially create optimized images using new regularization techniques.

1220
01:26:28.480 --> 01:26:32.560
The one we got this new thing they're on fire is in response to her face and

1221
01:26:32.561 --> 01:26:33.030
she'll just,

1222
01:26:33.030 --> 01:26:36.250
one is that they also thought the image search training set the activate this

1223
01:26:36.251 --> 01:26:39.040
neuron the most as well as pixels from those images.

1224
01:26:39.041 --> 01:26:42.670
Most responsible for the high activations computer via the de convolution

1225
01:26:43.320 --> 01:26:43.841
convolution.

1226
01:26:43.841 --> 01:26:47.950
Rick's feature response to multiple faces in different locations and by looking

1227
01:26:47.951 --> 01:26:48.820
at the [inaudible].

1228
01:26:49.810 --> 01:26:50.430
<v 3>Okay,</v>

1229
01:26:50.430 --> 01:26:53.790
<v 6>we can see that it would respond more strongly if we had even darker eyes and</v>

1230
01:26:53.791 --> 01:26:54.630
Rosier Lips.

1231
01:26:55.080 --> 01:26:58.590
We can also confirm that it cares about the head and shoulders that ignores the

1232
01:26:58.591 --> 01:26:59.424
arms and torso.

1233
01:27:00.450 --> 01:27:00.810
<v 3>Okay.</v>

1234
01:27:00.810 --> 01:27:05.810
<v 6>We can even see that it fires to some extent for cat faces using backdrop or di</v>

1235
01:27:06.451 --> 01:27:07.170
come.

1236
01:27:07.170 --> 01:27:10.200
We can see that this unit depends most strongly on a couple of units and the

1237
01:27:10.201 --> 01:27:14.130
previous layer kind of form and not about a dozen or so.
In contrast,

1238
01:27:15.370 --> 01:27:17.660
let's look at another nurse on your own lead.

1239
01:27:17.900 --> 01:27:20.910
So what is this unit doing from the top nine images?

1240
01:27:21.180 --> 01:27:23.490
We may conclude that at fires for different types of clothing,

1241
01:27:24.240 --> 01:27:27.240
but examining this synthetic images.
So it's did,
it may be detecting,

1242
01:27:27.241 --> 01:27:30.780
not clothing per se,
but wrinkles in the lifetime.

1243
01:27:30.900 --> 01:27:35.370
We can see that it's activated by my shirt and smoothing out half of my shirt

1244
01:27:35.670 --> 01:27:39.900
causes that hack of the activations to decrease.
Finally,

1245
01:27:40.020 --> 01:27:41.120
here's another interesting though.

1246
01:27:42.810 --> 01:27:46.080
This one has learned to look for printed text in a variety of sizes,

1247
01:27:46.260 --> 01:27:47.640
colors and fonts.

1248
01:27:48.810 --> 01:27:52.050
This is pretty cool because we never asked the network to look for wrinkles or

1249
01:27:52.051 --> 01:27:55.860
text or faces.
The only labels we provided the were at the very last leg,

1250
01:27:56.190 --> 01:27:59.100
so the only reason that network learned features like texts and faces in the

1251
01:27:59.101 --> 01:28:03.750
middle was to support final decisions at that last layer.
For example,

1252
01:28:03.840 --> 01:28:08.130
the text detector may provide good evidence that a rectangle is in fact it

1253
01:28:08.131 --> 01:28:08.391
booked,

1254
01:28:08.391 --> 01:28:12.390
seen on edge and detecting many books next to each other might be a good way of

1255
01:28:12.391 --> 01:28:13.440
detecting a bookcase,

1256
01:28:13.590 --> 01:28:16.350
which was one of the categories we trained the net to recognize.

1257
01:28:17.860 --> 01:28:18.170
<v 3>Yeah.</v>

1258
01:28:18.170 --> 01:28:21.680
<v 6>In this video we've shown some of the features of the deep this toolbox and a</v>

1259
01:28:21.681 --> 01:28:24.470
few of the things we've learned by using it.
You can download it.

1260
01:28:24.720 --> 01:28:28.170
<v 5>Yup.
So they have a toolbox,
which is exactly what you need.</v>

1261
01:28:28.171 --> 01:28:32.490
Your lights here and you could test the two bucks on your model.

1262
01:28:32.510 --> 01:28:34.800
It takes time to,
to get,
get it to run.
But,

1263
01:28:35.250 --> 01:28:39.630
but if you want to visualize all the neurons,
it's very helpful.
Okay.

1264
01:28:39.810 --> 01:28:44.490
So,
uh,
let's go quickly.
We spend about three minutes on the optional,

1265
01:28:44.520 --> 01:28:48.150
the dream 100 fun and uh,
yeah,
feel free.

1266
01:28:48.420 --> 01:28:49.890
Feel free to jump in and ask questions.

1267
01:28:50.760 --> 01:28:53.550
So did the dream one is

1268
01:28:55.050 --> 01:28:58.410
like Google and uh,
the p the,

1269
01:28:58.560 --> 01:29:01.800
the blog post is by Alexandre more sef.

1270
01:29:01.860 --> 01:29:06.450
The idea here is to generate parts using these knowledge of digitalization and

1271
01:29:06.451 --> 01:29:08.850
how would they do that is quite interesting.

1272
01:29:09.360 --> 01:29:14.360
Now we'll take any input for propagated to the network and at a specific layer

1273
01:29:15.660 --> 01:29:18.000
that we call the dream layer.

1274
01:29:18.390 --> 01:29:22.410
Then we'll take the activation and sets the gradient to be called to this

1275
01:29:22.411 --> 01:29:24.440
activation,
the gradient,

1276
01:29:24.441 --> 01:29:27.200
that layer and they would back propagate integrations to Naples.

1277
01:29:28.000 --> 01:29:32.430
So earlier what we need is that we defined the new objective function that was

1278
01:29:32.431 --> 01:29:36.270
equal to an activation and we tried to maximize this objective function.

1279
01:29:36.490 --> 01:29:38.310
Who would be there doing it even stronger.

1280
01:29:38.610 --> 01:29:41.610
They take the activations and they said the gradients to be called to the

1281
01:29:41.611 --> 01:29:44.280
activations and so the stronger the activation,

1282
01:29:44.640 --> 01:29:48.630
the stronger it's going to become later on and so on and so on and so on.

1283
01:29:49.020 --> 01:29:54.020
So they're trying to see what the network is activating for and increase even

1284
01:29:54.241 --> 01:29:55.074
this activation.

1285
01:29:56.690 --> 01:30:00.080
So for probably get DMH said the gradient of the dreaming layer to be called to

1286
01:30:00.081 --> 01:30:00.860
exaggeration,

1287
01:30:00.860 --> 01:30:04.150
but light propagates all the way back to the inputs and update data,

1288
01:30:04.160 --> 01:30:07.820
Pixel of the image.
Do that several times and every time the activation,

1289
01:30:07.830 --> 01:30:12.290
good change.
So you have to set again the new activations to be the the,

1290
01:30:12.300 --> 01:30:15.860
the gradients of the green layer and by propagating and also makes it,

1291
01:30:15.861 --> 01:30:19.340
you would see things happening,
so it's hard to see here on the screen,

1292
01:30:19.341 --> 01:30:22.010
but you would have a pig appearing here.

1293
01:30:22.220 --> 01:30:26.150
You'd have like a tree somewhere there and some animals and a lot of animals are

1294
01:30:26.151 --> 01:30:30.560
going to start appearing in this cloud.
It's interesting because it means,

1295
01:30:30.620 --> 01:30:32.420
let's say you see this cloud here.

1296
01:30:32.930 --> 01:30:37.040
If the network thoughts that this cloud looked a little bit like a dog.

1297
01:30:37.370 --> 01:30:39.330
So one of the,
the,
the,

1298
01:30:39.331 --> 01:30:43.730
the feature maps was which would be generated by the feature that detects dog

1299
01:30:43.731 --> 01:30:48.260
would activate itself a little bit because we said the gradient to equal to the

1300
01:30:48.261 --> 01:30:53.261
activation is going to increase the appearance of the dog in the image and so

1301
01:30:54.651 --> 01:30:57.410
on.
And then you would see a dog appearing after a few generations.

1302
01:30:58.550 --> 01:31:01.130
It's quite fine and issues zoom,
you'll see that type of thing.

1303
01:31:01.131 --> 01:31:05.700
So you see a peak snail,
it's kind of a big,
uh,
with this snail,
uh,

1304
01:31:05.840 --> 01:31:09.230
CarePass Cameron bird,
dog,
dog,
fish.

1305
01:31:09.750 --> 01:31:13.340
I advise you to like look at these on the slides or rather than on the screen,

1306
01:31:13.341 --> 01:31:17.990
but it's quite fine.
And uh,
same like if you give that type of image,

1307
01:31:17.991 --> 01:31:22.170
you would see that because the network thought there was like a tower earlier

1308
01:31:22.280 --> 01:31:22.910
bits,

1309
01:31:22.910 --> 01:31:26.270
you will increase the network's confidence in the fact that there is a tower by

1310
01:31:26.271 --> 01:31:29.970
changing the image and the tower would come out and so on.

1311
01:31:30.680 --> 01:31:34.220
It's quite a cool.
Uh,
yeah.

1312
01:31:34.280 --> 01:31:35.900
And if you dream in lower layers,

1313
01:31:36.230 --> 01:31:41.230
obviously you will see edges happening or patterns coming because the,

1314
01:31:42.231 --> 01:31:46.220
the lower layers seem to detect an edge and then you will increase his

1315
01:31:46.221 --> 01:31:50.030
confidence and his edge.
So equally it would create an edge on the image.

1316
01:31:51.320 --> 01:31:52.153
These are fun.

1317
01:31:53.960 --> 01:31:55.820
<v 7>Deep dream on a video</v>

1318
01:32:33.520 --> 01:32:34.760
<v 5>gets to the tricky,
I'm going to stop.</v>

1319
01:32:38.190 --> 01:32:43.010
So what one insight that he's fun about it is if the network,

1320
01:32:43.100 --> 01:32:46.980
and this is not only for deep dream,
it's also for,
it's mostly for graded assets.

1321
01:32:47.390 --> 01:32:52.390
Let's say we have an awkward score offered Dumbbell and we define your objective

1322
01:32:52.941 --> 01:32:54.740
function can be a dumbbell score.

1323
01:32:54.770 --> 01:32:59.030
And we tried to find images that maximizes the dumbbell when we'd see something

1324
01:32:59.031 --> 01:32:59.864
like that.

1325
01:33:00.130 --> 01:33:04.580
It's interesting is that the network thinks that the dumbbell is a hand with a

1326
01:33:04.581 --> 01:33:09.010
Dumbo,
not only to Dumbo.
And you could see it too.

1327
01:33:09.011 --> 01:33:12.290
You see the hands and the reason he has never seen a gun bill alone.

1328
01:33:12.320 --> 01:33:14.640
So probably image editor who's,
they'll beat you up,

1329
01:33:14.650 --> 01:33:19.580
you don't feel alone in a corner and labeled as number.
But he said,

1330
01:33:19.640 --> 01:33:22.740
uh,
it's usually a human trying to,
we've hired,

1331
01:33:24.620 --> 01:33:28.190
okay.
So just to summarize what we've learned today,

1332
01:33:28.670 --> 01:33:32.180
we are now able to answer all the following questions.

1333
01:33:32.240 --> 01:33:34.920
What part of the employee's responsible for 40 hours goods beam,

1334
01:33:35.000 --> 01:33:38.270
occlusion sensitivity,
class activation map seem to be the best way to go.

1335
01:33:38.630 --> 01:33:41.660
What is there all of a,
given your own feature layer he called involve,

1336
01:33:41.661 --> 01:33:43.820
reconstruct,
searching the data set.

1337
01:33:43.821 --> 01:33:47.420
What are the top images and who radiant sense checked?

1338
01:33:47.750 --> 01:33:51.080
Can we check what the number of focuses on [inaudible] intensity saliency,

1339
01:33:51.081 --> 01:33:54.770
map class activation maps,
how does the network cr world,

1340
01:33:54.800 --> 01:33:57.920
I would say great in the sense maybe deep dreams of cool stuff.

1341
01:33:58.340 --> 01:34:00.000
And then what are the,

1342
01:34:00.770 --> 01:34:04.340
the implication and use cases of these visualizations.

1343
01:34:05.720 --> 01:34:07.520
You can use Saint Lynsey map to segments.

1344
01:34:07.550 --> 01:34:09.890
It's not very useful given the new methods we have,

1345
01:34:10.010 --> 01:34:13.760
but the deconvolution that would sing together is widely used for segmentation

1346
01:34:13.761 --> 01:34:15.020
and reconstruction.
Uh,

1347
01:34:15.300 --> 01:34:19.820
also for generative adversarial networks to generate images and art.

1348
01:34:19.850 --> 01:34:24.850
Sometimes these visualizations or also helpful to detect if some of the neurons

1349
01:34:25.641 --> 01:34:26.780
in your network are dead.

1350
01:34:27.050 --> 01:34:30.380
So let's say you have a network and you use the toolbox and you see that

1351
01:34:30.980 --> 01:34:31.910
whatever the input,

1352
01:34:31.911 --> 01:34:36.911
you may give some feature maps or always like it means that the feature that

1353
01:34:37.581 --> 01:34:41.120
generated these feature map icon building over the inputs probably never

1354
01:34:41.121 --> 01:34:43.260
detected anything.
So it's not even trained.

1355
01:34:43.910 --> 01:34:48.830
That's a type of each site you can get.
Okay.
Thanks guys.
Sorry we went over time.

