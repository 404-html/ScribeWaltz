WEBVTT

1
00:00:06.180 --> 00:00:10.840
Hi Everyone,
and welcome to lecture nine of for [inaudible].

2
00:00:11.230 --> 00:00:15.630
Uh,
today we're going to discuss an advanced topic,
uh,

3
00:00:15.660 --> 00:00:17.370
that will be kind of the,

4
00:00:18.090 --> 00:00:22.740
the marriage between deep learning and another field of AI,

5
00:00:22.741 --> 00:00:24.060
which is reinforcement learning.

6
00:00:24.330 --> 00:00:29.330
And we will see a practical application and how deep learning methods can be

7
00:00:31.231 --> 00:00:33.780
plugged in another family of algorithm.

8
00:00:34.260 --> 00:00:37.830
So it's interesting because deep learning methods and deep neural networks have

9
00:00:37.831 --> 00:00:42.480
been shown to be very good a function approximators essentially that's what they

10
00:00:42.481 --> 00:00:45.330
are.
We're giving them data so that they can approximate a function.

11
00:00:45.720 --> 00:00:49.950
There are a lot of different thieves which require this function.

12
00:00:50.010 --> 00:00:54.330
Approximators and deep learning methods can be plugged in.
All these methods.

13
00:00:54.600 --> 00:00:59.250
This is one of these examples.
So we'll first motivate,
uh,

14
00:00:59.260 --> 00:01:02.160
the,
the setting of reinforcement learning.

15
00:01:02.490 --> 00:01:05.940
Why do we need to enforcement learning?
Why cannot wait?

16
00:01:05.941 --> 00:01:09.990
Why can't we use deep learning methods to solve everything?

17
00:01:10.430 --> 00:01:14.610
There are some set of methods that we cannot solve with deep learning and

18
00:01:14.611 --> 00:01:18.750
reinforcement learning.
Reinforcement learning applications are examples of that.

19
00:01:19.570 --> 00:01:22.330
We will see,
uh,
an example,
uh,

20
00:01:22.350 --> 00:01:27.350
to introduce an algorithm of a reinforcement learning algorithm called Q

21
00:01:27.481 --> 00:01:28.314
learning.

22
00:01:28.680 --> 00:01:33.210
And we will add deep learning to this algorithm and make it deep learning.

23
00:01:34.520 --> 00:01:38.910
Uh,
as we've seen with a generative adversarial networks and also deep neural

24
00:01:38.911 --> 00:01:43.750
networks,
most models are hard to train.
We've had,

25
00:01:43.780 --> 00:01:47.340
we had to come up with Xavier initialization with dropout,

26
00:01:47.430 --> 00:01:52.430
with batch norm and a myriads of myriads of methods to make these deep neural

27
00:01:52.921 --> 00:01:54.660
networks train in gangs.

28
00:01:54.661 --> 00:01:59.661
We had to use methods as well in order to train gans and tricks and hacks.

29
00:02:00.600 --> 00:02:05.600
So here we will see some of the tips and tricks to train deep cue learning,

30
00:02:06.990 --> 00:02:09.450
which is an enforcement learning algorithm.

31
00:02:10.860 --> 00:02:15.510
And at the end we will have a guest speaker coming to talk about advanced

32
00:02:15.511 --> 00:02:20.040
topics,
which are mostly research which combine deep learning and reinforcement

33
00:02:20.041 --> 00:02:22.980
learning.
Sounds good.
Okay,
let's go.

34
00:02:23.880 --> 00:02:28.740
So real deeper enforcement.
Louie is a very recent field,
I would say.

35
00:02:28.741 --> 00:02:32.850
Although both fields are,
are enforced,
wondering how has existed for a long time.

36
00:02:34.260 --> 00:02:39.260
Only recently it's been shown that using deep learning as a way to approximate

37
00:02:39.781 --> 00:02:43.950
the functions that play a big role in reinforcement learning algorithms has

38
00:02:43.951 --> 00:02:47.760
worked a lot.
So one example is Alphago.
And,
uh,

39
00:02:47.761 --> 00:02:49.560
you probably all have heard of it.

40
00:02:49.590 --> 00:02:54.590
It's Google deepmind's Alphago has a beaten world champions in a game called the

41
00:02:55.741 --> 00:02:58.050
game of go,
which is a very,

42
00:02:58.610 --> 00:03:03.560
a very strategy old game and the one on the right here,

43
00:03:03.640 --> 00:03:08.030
um,
or on your rights,
human living controlled three d print.

44
00:03:08.031 --> 00:03:13.031
Forcement learning is also a deep mind Google deep mine paper that came out and

45
00:03:13.970 --> 00:03:18.560
hit the headlines on the front page of nature,
which is,
uh,
one of the leading,

46
00:03:19.070 --> 00:03:22.820
uh,
multidisciplinary peer review journals in the world.

47
00:03:23.270 --> 00:03:24.590
And they've shown that,

48
00:03:25.130 --> 00:03:25.280
<v 1>okay,</v>

49
00:03:25.280 --> 00:03:29.210
<v 0>we deep learnings plugged in a reinforcement learning setting.</v>

50
00:03:29.540 --> 00:03:34.540
They can train an agent that beats human level in of RUTF gapes.

51
00:03:36.120 --> 00:03:37.680
And in fact,
these are our three gates.

52
00:03:38.250 --> 00:03:41.190
So they've shown actually that their algorithm the same,

53
00:03:41.191 --> 00:03:44.850
I agree them reproduce for a large number of games can

54
00:03:45.350 --> 00:03:48.950
beats humans on all of these games.
Most of these games,
not all of these camp.

55
00:03:50.750 --> 00:03:54.740
So these are two examples or do they use different sub techniques of

56
00:03:55.040 --> 00:03:58.730
reinforcement learning?
They both include some deep learning aspect in it.

57
00:03:59.090 --> 00:03:59.621
And today we,

58
00:03:59.621 --> 00:04:02.690
we mostly talk about the human level controls through deeper enforcement

59
00:04:02.691 --> 00:04:06.470
learning,
also called deep to network presented in this paper.

60
00:04:07.940 --> 00:04:08.840
So let's,

61
00:04:08.841 --> 00:04:12.830
let's start with we'd motivating reinforcement learning and using the alpha go

62
00:04:12.831 --> 00:04:15.200
setting.
Um,

63
00:04:15.860 --> 00:04:20.810
this is a board of Google and the picture comes from deep mines blog.
Uh,
so go,

64
00:04:20.811 --> 00:04:25.811
you can think of it as a strategy game where you have agreed that he's up to 19

65
00:04:25.881 --> 00:04:28.310
by 19 and you have two players.

66
00:04:28.640 --> 00:04:31.640
One player has white stones and one player is Blackstone's.

67
00:04:31.700 --> 00:04:34.970
And that's every step in the game.
You can position a stone on the,
on the board,

68
00:04:35.000 --> 00:04:36.990
on one of the grid cross.

69
00:04:38.090 --> 00:04:38.300
<v 1>Yeah.</v>

70
00:04:38.300 --> 00:04:40.580
<v 0>The goal is to surround your openings,</v>

71
00:04:40.730 --> 00:04:44.420
so to maximize your territory by surrounding your openings.

72
00:04:44.840 --> 00:04:47.930
And it's a very complex game for different reasons.
Uh,

73
00:04:48.200 --> 00:04:52.850
one reason is that you have to be you.
You cannot be shortsighted in this game.

74
00:04:52.851 --> 00:04:54.470
You have to have a longterm strategy.

75
00:04:54.890 --> 00:04:56.960
And other reason is that the board is so big,

76
00:04:57.350 --> 00:05:01.130
it's much bigger than a chess board,
right?
Chessboard is eight by eight.

77
00:05:02.350 --> 00:05:05.560
So let me ask you a question.
If you had to solve,

78
00:05:06.320 --> 00:05:11.320
we'll build an agency that solves this game and beats humans or plays very well,

79
00:05:11.691 --> 00:05:15.260
at least with deep learning methods that you've seen so far.

80
00:05:15.290 --> 00:05:16.160
How would you do that?

81
00:05:33.750 --> 00:05:34.670
Someone wants to try.

82
00:05:39.180 --> 00:05:39.900
<v 1>Yeah,</v>

83
00:05:39.900 --> 00:05:40.733
<v 0>so let's say you have a,</v>

84
00:05:40.800 --> 00:05:43.980
you have to collect the data set because in classic supervise learning,

85
00:05:44.340 --> 00:05:48.630
we need a data set with x and y.
What'd you think would be your x and y?

86
00:05:52.710 --> 00:05:53.543
<v 1>Yep.</v>

87
00:05:53.810 --> 00:05:56.990
<v 0>Where does the book then?
The output is the probability of victory from that.</v>

88
00:05:58.280 --> 00:05:59.113
<v 2>Okay.</v>

89
00:05:59.190 --> 00:06:03.530
Game boards and output is probability of victory in that position.

90
00:06:03.980 --> 00:06:08.270
So that's,
that's a good one.
I think input output.
What's the issue with that one?

91
00:06:14.980 --> 00:06:17.090
So yeah,
it's hard.

92
00:06:19.720 --> 00:06:22.960
It's super hard to represent what the probability of winning is from the sport.

93
00:06:23.350 --> 00:06:25.600
Even like nobody can tell you,

94
00:06:25.810 --> 00:06:29.710
even if I ask an expert human to come and tell us what's the problem of black

95
00:06:29.711 --> 00:06:32.440
winning in this or white twinning in this setting,

96
00:06:32.470 --> 00:06:36.160
they wouldn't be able to tell you that this is a little more complete.

97
00:06:36.190 --> 00:06:40.990
Any other ideas of datasets?
Yup.

98
00:06:43.120 --> 00:06:45.470
Okay.
Good point.
So we could have a

99
00:06:45.680 --> 00:06:50.570
<v 0>the grid like this one and then this is the inputs and the outputs would be the</v>

100
00:06:50.571 --> 00:06:54.230
move,
the next action taken by probably a professional player.

101
00:06:54.710 --> 00:06:58.220
So we would just watch professional players playing and we would record their

102
00:06:58.221 --> 00:07:03.221
moves and we would build the data sets of what is a professional move and we

103
00:07:03.591 --> 00:07:04.970
hope that's our network.

104
00:07:05.300 --> 00:07:09.500
Using these input outputs will at some point learn how the professional players

105
00:07:09.501 --> 00:07:11.090
play and given an input,

106
00:07:11.150 --> 00:07:14.720
a states of the board will be able to decide of the next move.

107
00:07:16.100 --> 00:07:17.180
What's the issue with that?

108
00:07:25.040 --> 00:07:25.873
<v 1>Yep.</v>

109
00:07:32.340 --> 00:07:36.800
<v 2>You need a whole lot of data.
Why?
And you said it's you,
you said,
uh,</v>

110
00:07:36.801 --> 00:07:41.740
because,
uh,
we need basically to represent all types of positions of the board,

111
00:07:42.100 --> 00:07:45.070
all states.
So if you were actually,
it's,
let's do that.

112
00:07:45.400 --> 00:07:50.400
If we were to compute the number of possible states of this board,

113
00:07:50.501 --> 00:07:51.334
what would it be?

114
00:07:59.420 --> 00:08:00.780
It's a 19 by 19 board.

115
00:08:07.230 --> 00:08:10.540
Remember what we did with adversarial examples.
We did four pixels.

116
00:08:10.550 --> 00:08:12.750
Right now we're doing youth leader board.

117
00:08:14.380 --> 00:08:17.630
So what's the question?
First is you want to try,

118
00:08:20.610 --> 00:08:23.590
yeah.
Three to do power.

119
00:08:23.650 --> 00:08:28.360
19 Times 19 or 19 squared.
Yeah.
So why is it that,

120
00:08:29.380 --> 00:08:30.640
what,
why is it,
is it this

121
00:08:33.770 --> 00:08:36.140
last thoughts?
Stone?
Yeah,

122
00:08:36.880 --> 00:08:41.880
each spot and there are 19 times 19 spot can have three state basically no

123
00:08:42.791 --> 00:08:46.390
stone,
white stone or Blackstone.
So this is the old possible state.

124
00:08:48.760 --> 00:08:52.060
This is about 10 to the 170

125
00:08:53.850 --> 00:08:55.030
so it's super,
super.

126
00:08:56.490 --> 00:09:01.400
<v 0>So we can probably not get even close to that by observing professional players</v>

127
00:09:01.730 --> 00:09:05.790
for us because we don't have enough pro Sheltie years and because we're humans

128
00:09:05.791 --> 00:09:07.970
and we don't have infinite life.
So the professional for years,

129
00:09:08.230 --> 00:09:11.980
you're not play forever.
They might get tired as well.
Uh,

130
00:09:12.060 --> 00:09:14.970
but so one issue is the state space is too big.

131
00:09:14.971 --> 00:09:17.700
Another one is that the ground truth probably would be wrong.

132
00:09:18.450 --> 00:09:21.260
It's not because you're a professional player that you will play the best move

133
00:09:21.270 --> 00:09:24.060
every time.
Right?
Every player has their own strategy.

134
00:09:24.870 --> 00:09:29.870
So the ground truth where we're having here is not necessarily true and our

135
00:09:30.601 --> 00:09:34.380
network might,
might not be able to beat these human players.

136
00:09:34.620 --> 00:09:39.030
What we're looking into here is an algorithm that beats humans.
Okay?

137
00:09:40.050 --> 00:09:42.570
Second one.
Too many states in the game as you mentioned.

138
00:09:42.870 --> 00:09:44.760
And third one we will likely not generalize.

139
00:09:45.690 --> 00:09:48.960
The reason we will not generalize is because it's classic supervise learning.

140
00:09:48.961 --> 00:09:50.100
We're looking for patterns.

141
00:09:50.220 --> 00:09:53.430
If I ask you to build an algorithm to detect cats versus dogs,

142
00:09:53.670 --> 00:09:56.760
it will look for what the pattern of the cat is versus what the pattern of the

143
00:09:56.761 --> 00:09:57.241
dog is.

144
00:09:57.241 --> 00:10:01.110
And in the convolutional theaters we learned that in this case it's about a

145
00:10:01.111 --> 00:10:02.820
strategy.
It's not about a putter,

146
00:10:03.540 --> 00:10:07.680
so you have to understand the process of winning this game in order to make the

147
00:10:07.681 --> 00:10:08.430
next move.

148
00:10:08.430 --> 00:10:12.840
You cannot generalize if you don't understand this process of longterm strategy.

149
00:10:13.470 --> 00:10:17.910
So we have to incorporate that and that's where our l comes into place.

150
00:10:18.690 --> 00:10:23.690
Rl is reinforcement learning or method that would could be described with one

151
00:10:24.001 --> 00:10:27.780
sentence as automatically learning to make good sequences of decision.

152
00:10:28.140 --> 00:10:32.280
So it's about the long term.
It's not about the shorter and we would use it.

153
00:10:32.310 --> 00:10:36.240
Generally when we have delayed labels,
like in this game,

154
00:10:36.300 --> 00:10:39.840
the label that you mentioned that the beginning was probably to of victory.

155
00:10:40.020 --> 00:10:42.930
This is a longterm label.
We cannot get this labeled now,

156
00:10:43.110 --> 00:10:45.450
but over time the closer we get to the end,

157
00:10:45.750 --> 00:10:48.870
the better we ha we are at seeing the victory or not

158
00:10:50.370 --> 00:10:53.130
and it's to make sequences of decision so we make a move,

159
00:10:53.190 --> 00:10:54.330
then the it makes a move,

160
00:10:54.331 --> 00:10:58.060
then we make another move and all the decisions of these move or correlated with

161
00:10:58.061 --> 00:11:01.980
each other,
like you'd have to plan it in advance.
When you're human,

162
00:11:01.981 --> 00:11:03.840
you do that when you play chess,
when you play golf.

163
00:11:04.830 --> 00:11:09.660
So examples of our applications can be robotics and it's still a research topic,

164
00:11:09.661 --> 00:11:11.700
how deep our El can change robotics.

165
00:11:11.940 --> 00:11:16.170
But thinking about having a robot walking from here and you want to send it

166
00:11:16.200 --> 00:11:17.970
there,
you want to send the robot there.

167
00:11:18.870 --> 00:11:22.230
What you're teaching the robot is if you get there,
it's good,
right?

168
00:11:22.710 --> 00:11:24.030
It's good you achieve the task,

169
00:11:24.390 --> 00:11:27.690
but I cannot give you the probability of getting there at every point.

170
00:11:28.050 --> 00:11:32.400
I can help you out by giving you a reward when you arrived there and let you

171
00:11:32.401 --> 00:11:36.090
trial and error.
So the robot will try and randomly initialized the robot.

172
00:11:36.091 --> 00:11:39.510
We just fall down at the first at first get a negative reward,

173
00:11:40.110 --> 00:11:41.220
then repeats.

174
00:11:41.580 --> 00:11:45.090
This time the robot knows that it shouldn't fall down and she wouldn't go down.

175
00:11:45.270 --> 00:11:49.610
It should probably go this way.
So to trial and error and reward on the longterm,

176
00:11:49.810 --> 00:11:53.790
the robot is supposed to learn these pattern.
No do one,

177
00:11:53.791 --> 00:11:58.480
he's games and that's the one we would see today.
Uh,
games can be represented as,

178
00:11:58.540 --> 00:12:01.790
as,
as a set of reward for reinforcement learning algorithm.

179
00:12:01.791 --> 00:12:03.370
So this is where you win.

180
00:12:03.400 --> 00:12:08.200
This is where you lose led the algorithm play and figure out what winning means

181
00:12:08.201 --> 00:12:11.730
and what losing means until it learns.
Okay.

182
00:12:11.770 --> 00:12:14.830
The problem with using deep learning is that the algorithm will not learn cause

183
00:12:14.831 --> 00:12:16.520
this reward is to long term.

184
00:12:16.930 --> 00:12:20.220
So we're using reinforcement learning and finally advertisements.

185
00:12:20.230 --> 00:12:25.120
So a lot of advertisement,
um,
our real time bidding.
So you want to know,

186
00:12:25.150 --> 00:12:27.610
given a budget,
when you want to invest this budget,

187
00:12:28.090 --> 00:12:32.290
and this is a longterm strategy planning as well that reinforcement learning can

188
00:12:32.291 --> 00:12:34.720
help with.
Okay,

189
00:12:35.380 --> 00:12:37.840
so this was the motivation of reinforcement learning.

190
00:12:37.870 --> 00:12:42.040
We're going to jump to a concrete example that is a super vanilla example to

191
00:12:42.041 --> 00:12:43.210
understand cue learning.

192
00:12:43.990 --> 00:12:47.170
So let's start with this game or environment.

193
00:12:47.171 --> 00:12:51.430
So we call that an environment generally,
and it has several states in this case,

194
00:12:51.431 --> 00:12:55.600
five states.
So we have these states and we can define rewards,

195
00:12:56.050 --> 00:12:59.110
which are the following.
So let's see what is our goal in this game?

196
00:12:59.890 --> 00:13:04.150
We defined it as maximize the return or the rewards on the long term.

197
00:13:04.240 --> 00:13:08.440
And what is the reward is the numbers that you have here that were defined by a

198
00:13:08.441 --> 00:13:11.760
human.
So this is where the human defines the rework.

199
00:13:13.000 --> 00:13:15.720
Now what's the game?
The game has five states.
State.

200
00:13:15.740 --> 00:13:20.740
One is a trashcan and has the reward of plus two state too is a starting state's

201
00:13:22.990 --> 00:13:23.830
initial states.

202
00:13:23.860 --> 00:13:27.220
And we assumed that we would start in the initial state with a plastic bottle in

203
00:13:27.221 --> 00:13:31.450
our hand.
The goal would be to throw these plastic bottle in a can.

204
00:13:32.080 --> 00:13:34.030
If it's the Trashcan we get plus two.

205
00:13:34.420 --> 00:13:39.420
If we get to state five we get to the recycle bin and we can get plus 10 super

206
00:13:40.361 --> 00:13:41.300
important application

207
00:13:42.930 --> 00:13:45.760
state for has a chocolate.

208
00:13:46.600 --> 00:13:48.760
So what happens is if you go to state for,

209
00:13:48.850 --> 00:13:52.000
you get a reward of one because you can eat the chocolate and you can also

210
00:13:52.001 --> 00:13:56.260
through the chocolate in the,
in the,
in the,
in the recycle bin.

211
00:13:56.261 --> 00:13:58.690
Hopefully those,
the setting make sense.

212
00:13:59.830 --> 00:14:04.330
So the states are of three types.
One is the starting state initial,

213
00:14:04.540 --> 00:14:08.410
which is Brown,
the normal state,

214
00:14:08.470 --> 00:14:12.970
which is not a starting,
neither,
neither starting Nora unending state.

215
00:14:13.510 --> 00:14:17.710
And it's great.
And the blue states are terminal states.

216
00:14:17.770 --> 00:14:22.690
So if we get to the terminal state,
we end up a game or an episode,
let's say,

217
00:14:23.890 --> 00:14:26.920
does this setting makes sense?
Okay.

218
00:14:26.950 --> 00:14:29.350
And your two possible actions you have to move.

219
00:14:29.380 --> 00:14:31.300
Either you go on the left or you go on the right.

220
00:14:33.560 --> 00:14:36.710
An additional rule will will add is that the garbage collector will come in

221
00:14:36.711 --> 00:14:40.070
three minutes and every step takes you one minute.

222
00:14:40.610 --> 00:14:43.310
So you cannot spend more than three minutes in this gate.
In other words,

223
00:14:43.311 --> 00:14:45.890
you cannot stay at the chocolate and it chocolate forever.

224
00:14:46.190 --> 00:14:50.050
You have to move at some point.
Okay,

225
00:14:50.900 --> 00:14:54.680
so one question I have is how would you define the longterm return?

226
00:14:55.820 --> 00:14:58.490
Because we said we want a longterm return,
we don't want,

227
00:14:58.491 --> 00:15:00.230
we don't care about short term returns.

228
00:15:03.110 --> 00:15:03.943
<v 1>Okay.</v>

229
00:15:04.910 --> 00:15:09.000
<v 2>What do you think is a good way to define the long term return here?</v>

230
00:15:11.790 --> 00:15:13.470
Just some of the terminals states,

231
00:15:17.980 --> 00:15:21.600
the sum of how many points you have when you reach the terminal states.

232
00:15:22.080 --> 00:15:26.460
So let's say,
I mean stay too.
I have zero reward right now.

233
00:15:26.870 --> 00:15:31.170
It's like I used to determine terminal state under right on,
on the,
on your left,

234
00:15:31.470 --> 00:15:34.920
the Plus Two plus two reward.
And I finished the game.

235
00:15:35.280 --> 00:15:40.280
If I go under rights instead and I reached a plus 10 you're saying that the

236
00:15:40.770 --> 00:15:44.310
longterm return can be all the,
some of the rewards I got to get there.

237
00:15:44.400 --> 00:15:47.860
So plus 11 so this is one way to define the longterm return.

238
00:15:48.930 --> 00:15:49.800
Any other ideas?

239
00:15:59.980 --> 00:16:02.500
Reduce the copper,
the word,

240
00:16:03.720 --> 00:16:08.720
we probably want to incorporate the time steps and reducing the reward as pie,

241
00:16:09.060 --> 00:16:10.920
as time passes.
And in fact,

242
00:16:10.921 --> 00:16:15.120
this would be called a discounted return versus what you said would be called a

243
00:16:15.121 --> 00:16:15.954
return.

244
00:16:17.580 --> 00:16:21.510
Here we use a discounted returning and it has several advantages.

245
00:16:21.511 --> 00:16:25.590
Some are mathematical because the return you described,
which is not discounted,

246
00:16:25.591 --> 00:16:29.740
my not converge,
it might go up to plus infinity.
Yeah.

247
00:16:29.800 --> 00:16:34.730
<v 0>Uh,
this discounted return will converge with the appropriate discount.
Um,</v>

248
00:16:35.380 --> 00:16:40.030
so intuitively also,
why is the discounted return intuitive?

249
00:16:40.031 --> 00:16:45.031
Is it because time is always an important factor in our decision making?

250
00:16:45.790 --> 00:16:49.960
People would prefer cash now then cash in 10 years,
right?
Or similarly,

251
00:16:49.961 --> 00:16:53.980
you can consider that the robot has a unique to life expectancy.

252
00:16:53.981 --> 00:16:56.680
Like it has a battery and loses battery every time it moves.

253
00:16:57.070 --> 00:16:59.830
So you want to take into account these discount of,

254
00:17:00.100 --> 00:17:02.290
if I can eat chocolates close,

255
00:17:02.800 --> 00:17:06.160
I go for it because I know that is a chocolate is too far.

256
00:17:06.161 --> 00:17:10.690
I might not get there because I'm losing some battery,
some energy,
for example.

257
00:17:10.960 --> 00:17:12.370
So this is the discounted return.

258
00:17:12.940 --> 00:17:17.940
Now if we take gamma equals one which means we have no discounts,

259
00:17:18.430 --> 00:17:22.840
the best strategy to follow in this setting seems to be to go to the,

260
00:17:22.900 --> 00:17:24.670
to the left or to go to the right,

261
00:17:24.671 --> 00:17:27.910
starting in the initial state to right.

262
00:17:28.330 --> 00:17:30.310
And the reason is it's a simple computation.

263
00:17:30.760 --> 00:17:32.410
On one side I get plus two on the other side,

264
00:17:32.411 --> 00:17:36.160
I guess plus 11 what if my discount was 0.1?

265
00:17:39.430 --> 00:17:40.450
<v 2>Which one would it be better?</v>

266
00:17:43.970 --> 00:17:46.860
Yeah,
you're left with veteran directly to Gloucester.

267
00:17:47.260 --> 00:17:49.980
And the reason is because we compute in your,
we

268
00:17:49.980 --> 00:17:54.980
<v 0>just do 0.1 times a month one which gives us 0.1 plus 0.1 squared times 10 and</v>

269
00:17:59.311 --> 00:18:02.100
it's less than two.
We know it.
Okay,

270
00:18:03.690 --> 00:18:08.160
so now we're going to assume that the discount is 0.9 and it's a very common

271
00:18:08.161 --> 00:18:10.040
discount.
Two,
two,
two,
two,
two years.

272
00:18:10.730 --> 00:18:13.110
You mean enforcement authority and we use a discounted return.

273
00:18:14.460 --> 00:18:16.110
So the general question here,

274
00:18:16.140 --> 00:18:19.890
and it's the core of reinforcement learning in this case of [inaudible] learning

275
00:18:19.891 --> 00:18:24.050
is what do we want to learn?
And this is really,

276
00:18:24.140 --> 00:18:26.810
really think of it as a human.
What would you like to learn?

277
00:18:26.900 --> 00:18:30.920
What are the numbers do you need to have in order to be able to make decisions

278
00:18:30.921 --> 00:18:35.090
really quickly?
Assuming you had a lot more states than that and actions.

279
00:18:39.500 --> 00:18:41.450
Any ideas of what we want to learn?

280
00:18:46.220 --> 00:18:47.960
What would help our decision making?

281
00:18:54.300 --> 00:18:55.133
<v 1>Passion.</v>

282
00:18:55.980 --> 00:18:57.660
<v 0>Optimal action at each state.
Yep.</v>

283
00:18:58.380 --> 00:18:58.690
<v 1>Yeah,</v>

284
00:18:58.690 --> 00:18:59.950
<v 0>that's exactly what you want to do.</v>

285
00:19:00.120 --> 00:19:04.420
For even a state's telling me the action that I can take and for that I need to

286
00:19:04.421 --> 00:19:08.380
have a score for all the actions in every state in order to store the scores.

287
00:19:08.381 --> 00:19:12.910
We need the matrix,
right?
So this is our matrix.
We will call it a cute table.

288
00:19:13.150 --> 00:19:17.680
It's going to be of shape,
number of states,
times,
number of actions.

289
00:19:18.340 --> 00:19:22.090
If I have this matrix of scores and the scores are correct,

290
00:19:22.450 --> 00:19:23.680
I'm in Stage three.

291
00:19:23.770 --> 00:19:28.770
I can look on the third row of this matrix and look what's the maximum value I

292
00:19:29.471 --> 00:19:32.950
have?
Is it the first one or the second one?
If he's the first one,

293
00:19:33.310 --> 00:19:37.180
I go to the left.
If he's the second one that is more maximum,
I go to the right.

294
00:19:37.750 --> 00:19:41.980
This is what we would like to have.
Does that make sense?
It's cure table.

295
00:19:43.810 --> 00:19:48.370
So now let's try to build a cute table for this example.
If you had to build it,

296
00:19:48.400 --> 00:19:51.790
you would first think of it as a treat.
Oh,
and by the way,

297
00:19:51.820 --> 00:19:56.820
every entry of this Q table tells you how good it is to take this action in that

298
00:19:57.791 --> 00:20:01.540
state state corresponding to the rural action corresponding to the column.

299
00:20:03.080 --> 00:20:06.100
So now how do we get there?
We didn't build the tree and that's,

300
00:20:06.340 --> 00:20:08.110
that's similar to what we were doing.
Our mind,

301
00:20:08.680 --> 00:20:10.840
we starting this to in two we have two options.

302
00:20:10.990 --> 00:20:15.990
Either we go to s one we get to or we go to a street and we get zero from

303
00:20:17.401 --> 00:20:20.170
[inaudible].
Two weeks from this one,
we cannot go anywhere.
It's a terminal state,

304
00:20:20.230 --> 00:20:25.230
but from s three we can go to as to and get zero by going back.

305
00:20:26.470 --> 00:20:28.900
Or we can go to s four and get one.

306
00:20:30.320 --> 00:20:34.850
That makes sense from us for saying we can get zero by going back to a street or

307
00:20:34.851 --> 00:20:39.851
we can go to s five and yet plus 10 now here I just have my immediate reward for

308
00:20:40.161 --> 00:20:42.350
every state.
What I would like to come to you,

309
00:20:42.351 --> 00:20:46.730
it is the discounted return for all the states because ultimately what should

310
00:20:46.731 --> 00:20:50.440
lead my decision making in the state is if I take this action,

311
00:20:51.040 --> 00:20:52.240
I get you a new states,

312
00:20:52.540 --> 00:20:55.810
what's the maximum reward I can get from there in the future?

313
00:20:56.500 --> 00:20:58.420
Not just the reward I get in that state.

314
00:20:59.140 --> 00:21:01.630
If I take the other action I get to another state,

315
00:21:02.080 --> 00:21:05.050
what's the maximum reward I could get from that state?

316
00:21:05.200 --> 00:21:08.230
Not just the immediate reward that I get from going to that state.

317
00:21:09.040 --> 00:21:10.840
So what we would do,
we can do it together.

318
00:21:11.170 --> 00:21:16.170
Let's say we want to compute the value of of the actions from s three from a

319
00:21:16.860 --> 00:21:18.820
going right and left from his shirt.

320
00:21:18.820 --> 00:21:23.590
I can either go to s four or s two going to s four I know that the immediate

321
00:21:23.591 --> 00:21:28.591
reward was one and I know that from s four I can get plus 10 this is the maximum

322
00:21:28.601 --> 00:21:33.601
I can get so I can discount this 10 multiplied by 0.9 10 times 0.9 Jesus nine

323
00:21:35.590 --> 00:21:39.820
plus one which was the immediate reward is just nine and he's just,
he's us 10

324
00:21:41.350 --> 00:21:44.830
so 10 is the score that we give to the action.

325
00:21:44.831 --> 00:21:46.660
Go right from state as tree.

326
00:21:48.340 --> 00:21:51.480
Now what if we do it from one step before [inaudible]

327
00:21:53.020 --> 00:21:57.670
from s to I know that I can go to s three and two is three I get zero reward.

328
00:21:57.700 --> 00:22:02.350
So the immediate reward is zero but I know that from history I can get 10
reward.

329
00:22:02.470 --> 00:22:06.040
Ultimately on the long term I need to discount this reward from one step.

330
00:22:06.220 --> 00:22:11.050
So I multiply this 10 by 0.9 and I get zero plus 0.9 times 10 which gives me

331
00:22:11.051 --> 00:22:16.051
nine so now in state two going right will give us a longterm reward of nine make

332
00:22:17.961 --> 00:22:20.920
sense?
And you do the same thing.

333
00:22:21.580 --> 00:22:26.580
You can copy back that going from s four two s three will give you zero plus the

334
00:22:26.861 --> 00:22:30.010
maximum you can get from my street,
which was 10 discounted by point night.

335
00:22:30.640 --> 00:22:35.440
Or you can do it from s two from s to I can go left and get lost too.

336
00:22:35.770 --> 00:22:40.770
Or I can go right and get nine and the immediate reward would be nine would be

337
00:22:40.871 --> 00:22:41.291
zero.

338
00:22:41.291 --> 00:22:46.000
And I will discount the nine by 0.9 and get 8.1 so that's the process we would

339
00:22:46.001 --> 00:22:48.760
do to come to that.
And you see that it's an iterative algorithm

340
00:22:50.560 --> 00:22:53.260
are you will just copy back all these values in my matrix.

341
00:22:53.590 --> 00:22:55.270
And now if I'm in state too,

342
00:22:56.110 --> 00:23:00.230
I can clearly say that the best action seems to go,
seems to say,
uh,

343
00:23:00.250 --> 00:23:05.250
go to the left because the longterm discounted reward is nine.

344
00:23:05.770 --> 00:23:09.250
While the longterm discounted reward for going to the right is to,

345
00:23:10.690 --> 00:23:15.640
and I'm done.
That's true learning.
I solved the problem I had.
I had,
uh,

346
00:23:15.690 --> 00:23:17.470
a state a problem statement.

347
00:23:17.740 --> 00:23:21.370
I found a matrix that tells me in every state what action I should take.

348
00:23:21.460 --> 00:23:24.850
I'm blind.
So why do we need deep learning?

349
00:23:26.690 --> 00:23:27.940
So question we will try to answer.

350
00:23:29.370 --> 00:23:33.760
So the best strategy to follow with 0.9 is still right,
right,
right.

351
00:23:33.790 --> 00:23:38.410
And the way I see it is I just look at my matrix at every step and I follow

352
00:23:39.160 --> 00:23:43.660
always the maximum of my role.
So from state to nine is the maximum.

353
00:23:43.661 --> 00:23:46.460
So I go right from say 10 is the maximum.

354
00:23:46.461 --> 00:23:50.090
So I still go right and from state for 10 years,
the maximum.
So I go right again.

355
00:23:50.750 --> 00:23:54.800
So I take the maximum of where all the action's in a specific state.
Okay.

356
00:23:55.580 --> 00:23:56.413
Now,

357
00:23:57.320 --> 00:24:00.620
one interesting thing to follow is that when you do these iterative algorithm,

358
00:24:00.621 --> 00:24:04.640
at some point it should converge and ours converged to some values that

359
00:24:04.641 --> 00:24:07.880
represent the discounts and rewards.
For every state and action.

360
00:24:10.020 --> 00:24:15.020
There is an equation that's this cue function follows.

361
00:24:15.460 --> 00:24:18.100
And we know that the optimal Q function follow this equation.

362
00:24:18.550 --> 00:24:20.530
The one we have here follows this equation.

363
00:24:21.280 --> 00:24:25.750
This equation is called the bellman equation.
Any ties,
uh,
two terms.

364
00:24:25.751 --> 00:24:30.751
One is art and one is these counts times the maximum of the Q scores over all

365
00:24:32.951 --> 00:24:33.784
the actions.

366
00:24:34.360 --> 00:24:39.360
So how does that make sense given that you're in a state as you want to know the

367
00:24:39.581 --> 00:24:42.700
score of going,
of taking action a in this state,

368
00:24:42.940 --> 00:24:47.940
the score should be the reward that you get by going there plus the discount

369
00:24:48.071 --> 00:24:49.990
times the maximum you can get in the future.

370
00:24:50.020 --> 00:24:51.760
That's actually what we used in the iteration.

371
00:24:52.690 --> 00:24:57.400
Does these bellmen equation makes sense?
Okay,

372
00:24:57.430 --> 00:25:01.330
so remember this is going to be very important in jewelry is bellmen equation.

373
00:25:01.630 --> 00:25:05.980
It's the equation that is satisfied by the optimal Q table or Q function.

374
00:25:06.850 --> 00:25:09.730
And if you try out all these entries,
you will see that it follows this equation.

375
00:25:11.620 --> 00:25:16.090
So when Q didn't is not optimal,
it's not following this equation yet,

376
00:25:16.900 --> 00:25:21.390
we would like you to follow this equation.
Another point of vocabulary,

377
00:25:21.391 --> 00:25:23.950
we were forced where learning is a policy policies,

378
00:25:23.980 --> 00:25:27.810
they noted piece sometimes or new and uh,
sorry,

379
00:25:27.820 --> 00:25:32.820
pie pie of s is equal to Arg Max over the actions of the optimal queue.

380
00:25:33.970 --> 00:25:37.300
That's yours.
What it means.
It means it's exactly our decision process.

381
00:25:37.330 --> 00:25:42.190
It's even that were in state as we look at all the columns of the state as in

382
00:25:42.191 --> 00:25:45.880
our table,
we take the maximum and this is what [inaudible] is telling us.

383
00:25:45.910 --> 00:25:47.440
It's telling us this is the action you should take.

384
00:25:47.980 --> 00:25:50.620
So Pi or policy is our decision making.

385
00:25:53.210 --> 00:25:54.043
<v 3>Okay.</v>

386
00:25:54.070 --> 00:25:56.650
<v 0>It tells us what's the best strategy to follow in a given state.</v>

387
00:25:57.430 --> 00:25:58.570
Any questions so far?

388
00:26:06.510 --> 00:26:09.900
Okay.
And so I have a question for you.
Um,

389
00:26:11.550 --> 00:26:15.180
why is deep yearning had foot?
Yes.

390
00:26:19.130 --> 00:26:19.700
<v 2>Yeah.</v>

391
00:26:19.700 --> 00:26:24.110
That's the reason number of Stacy's way too large to store at a table like that.

392
00:26:25.020 --> 00:26:29.750
So like if you have a small number of states,
a number of actions,
then easy,

393
00:26:29.751 --> 00:26:33.170
you can use it to your table.
You can at every stage look into the key table.

394
00:26:33.171 --> 00:26:35.780
It's super quick and find out what you should do.

395
00:26:36.320 --> 00:26:39.800
But ultimately this Q table would get bigger and bigger depending on the

396
00:26:39.801 --> 00:26:43.320
application.
Right?
And the number of states

397
00:26:44.130 --> 00:26:48.330
<v 0>for go is 10 to the power 117 approximately.</v>

398
00:26:49.050 --> 00:26:54.050
Which means that this matrix should have a number of rows equal to 10 we'd want

399
00:26:55.791 --> 00:26:58.440
170 zeros after.
I mean you,
you know what I mean?

400
00:26:59.040 --> 00:27:04.040
It's very big and number of actions is also going to be bigger and go,

401
00:27:04.051 --> 00:27:07.470
you can place your action everywhere on the board that is available of course.

402
00:27:08.700 --> 00:27:12.360
Okay.
So many way too many states and actions.

403
00:27:12.390 --> 00:27:17.390
So we would need to come up with maybe a function approximator that's can give

404
00:27:18.331 --> 00:27:22.110
us the action based on the state instead of having to store these matrix.

405
00:27:22.830 --> 00:27:24.070
That's where deep learning would come.

406
00:27:24.750 --> 00:27:29.100
So just to reach out to these first 30 minutes,
in terms of vocabulary,

407
00:27:29.101 --> 00:27:30.630
we learn what an environment is.

408
00:27:30.910 --> 00:27:35.910
It's that it's the general game definition or an agent is the thing.

409
00:27:36.391 --> 00:27:39.420
We're trying to train the decision maker,
a state,

410
00:27:40.340 --> 00:27:44.640
an action reward,
total return,
a discount factor.
The Q table,

411
00:27:44.641 --> 00:27:48.410
which is the matrix of entries were presenting hi is is to take action a in

412
00:27:48.460 --> 00:27:52.350
state s a policy,
which is our decision making function,

413
00:27:52.380 --> 00:27:55.950
telling us what's the best strategy to apply in the state.
And by many equation,

414
00:27:55.951 --> 00:28:00.510
which he starts his fight by d up similar to table and that way we'll tweak this

415
00:28:00.511 --> 00:28:05.511
cute table into ACU function and that's where we shift from to learning to deep

416
00:28:06.451 --> 00:28:10.860
cue learning.
So final Q function to replace the shoe table.

417
00:28:11.350 --> 00:28:12.183
<v 1>Right.</v>

418
00:28:12.540 --> 00:28:15.510
<v 0>Okay.
So this is the setting.
We have our problem statements.</v>

419
00:28:15.511 --> 00:28:19.920
We have our crew table and we want to change it into a function approximator

420
00:28:20.520 --> 00:28:24.580
that we'd be on neural network.
Does that make sense?

421
00:28:24.581 --> 00:28:27.610
How deep yearning comes into reinforcement learning here?

422
00:28:28.960 --> 00:28:33.960
So now we take a state does inputs for propagated in the deep network and get an

423
00:28:35.171 --> 00:28:37.630
output,
which is an action,

424
00:28:37.840 --> 00:28:40.660
an action score for all the actions

425
00:28:42.250 --> 00:28:44.830
it makes sense to have announced foods layer.

426
00:28:44.890 --> 00:28:49.120
That is the size of the number of actions because we don't want to,

427
00:28:49.210 --> 00:28:53.260
we don't want to give an action as input and the state has inputs and get the

428
00:28:53.261 --> 00:28:58.120
score for this action taken in the states.
Instead we can be much quicker.

429
00:28:58.230 --> 00:28:59.860
You can just give the state has inputs,

430
00:28:59.920 --> 00:29:04.920
get all the distribution of scores over the output and we just select the

431
00:29:05.501 --> 00:29:08.620
maximum of this vector which will tell us which action is best.

432
00:29:09.640 --> 00:29:11.950
So we felt we were in states to,

433
00:29:11.951 --> 00:29:16.951
let's say here we're in state too and we Ford Probe I gates say too,

434
00:29:18.100 --> 00:29:23.100
we get to values which are the scores off going left and right from state to we

435
00:29:23.891 --> 00:29:26.260
can select the maximum of dose and it will give us our action.

436
00:29:28.350 --> 00:29:33.240
The question is how to train this network.
We know how to trainings.

437
00:29:33.241 --> 00:29:37.090
We've been learning it for nine weeks.
Computer loss,
back propagate.

438
00:29:37.770 --> 00:29:42.770
Can you guys think of some issues that that make this setting different from a

439
00:29:42.791 --> 00:29:44.230
classic supervise learning setting?

440
00:29:51.870 --> 00:29:55.560
The reward changes dynamically so the reward doesn't change.
The reward is sets.

441
00:29:56.070 --> 00:29:58.240
You define it at the beginning.
It doesn't change on empty,

442
00:29:58.260 --> 00:30:02.140
but I think what you meant is that the Q scores changed dynamically.
Yup,

443
00:30:02.790 --> 00:30:04.530
that's true.
The Q scores change dynamically,

444
00:30:04.860 --> 00:30:09.720
but that's probably okay because our network change that our network is now the

445
00:30:09.721 --> 00:30:12.330
Q score.
So when we update department chairs of the network,

446
00:30:12.331 --> 00:30:16.620
it updates the Q scores.
What's,
what's another issue that we might have

447
00:30:19.950 --> 00:30:24.880
no labels.
Remember in supervised learning,
you need labels to train the network.

448
00:30:24.910 --> 00:30:26.380
What are the labels here

449
00:30:33.480 --> 00:30:37.470
and don't say compute the cute table.
Use them as labels.
So I've got to work.

450
00:30:41.240 --> 00:30:41.540
Okay,

451
00:30:41.540 --> 00:30:44.900
so that's the main issue that makes this problem very different from classic

452
00:30:44.901 --> 00:30:46.760
supervised learning.
So let's see,
uh,
how,

453
00:30:47.060 --> 00:30:49.070
how deep learning can be tweaked a little.

454
00:30:49.550 --> 00:30:52.610
And we want you to see these techniques because they,
they're helpful.

455
00:30:52.611 --> 00:30:56.990
When you read the variety of research papers we have our network,

456
00:30:57.050 --> 00:31:01.820
even a state gives us two scores that represent actions for going left and right

457
00:31:01.821 --> 00:31:02.654
from the states.

458
00:31:02.930 --> 00:31:06.560
The last function that will define is it a classification problem or a

459
00:31:06.561 --> 00:31:07.394
regression problem?

460
00:31:12.200 --> 00:31:14.960
Regression problem.
Because their Q score,

461
00:31:14.961 --> 00:31:19.490
it doesn't have to be a probably be the zero and one is just a score that you

462
00:31:19.491 --> 00:31:23.330
want to give in that she'd look,
let you meet,
make the longterm discounted,

463
00:31:23.331 --> 00:31:26.360
rewarding.
So in fact the lost function we can use is

464
00:31:28.610 --> 00:31:32.420
he's the l two loss function y minus the Q score squared.

465
00:31:33.290 --> 00:31:36.350
So let's say we do it for the queue going to the right,

466
00:31:37.910 --> 00:31:41.690
the question is what is why,
what is the target for this Q?

467
00:31:42.590 --> 00:31:46.310
And remember what I could feed on the top of the slide is development equation.

468
00:31:46.880 --> 00:31:50.750
We know that the optimal cue should follow this equation.

469
00:31:51.380 --> 00:31:56.150
We know it.
The problem is that these equations depends on its own queue.

470
00:31:56.990 --> 00:31:59.000
You know like you have to on both sides of the equation,

471
00:31:59.600 --> 00:32:04.600
it means if you said the label to be our plus gamma times Max of two stars,

472
00:32:05.510 --> 00:32:09.350
then when you,
we'll back propagates.
It will also have a derivative here.

473
00:32:10.490 --> 00:32:14.180
It's neat.
Let me go into the dictates.
Let's they find a target value.

474
00:32:14.240 --> 00:32:16.300
Let's assume that going,
uh,

475
00:32:16.400 --> 00:32:19.610
left is better than going right at this point in time.

476
00:32:20.120 --> 00:32:21.920
So we initialize the network randomly.

477
00:32:22.400 --> 00:32:26.510
We four propagate stayed too in the network and the Q score four left is more

478
00:32:26.511 --> 00:32:27.920
than the Q score for rights.

479
00:32:28.640 --> 00:32:31.340
So that's the action that we will take at this point is going left.

480
00:32:32.900 --> 00:32:34.460
Let's define our targets.

481
00:32:34.850 --> 00:32:39.850
Why as the reward you get when you go left immediates plus gamma times do

482
00:32:42.231 --> 00:32:43.130
maximum

483
00:32:44.960 --> 00:32:49.160
of all the Q values you get from the next step.

484
00:32:51.850 --> 00:32:54.700
So let me spend a little more time on that because it's a little complicated.

485
00:32:55.270 --> 00:33:00.270
I'm in s I moved to s next using a movie on the left I get immediate reward art

486
00:33:03.160 --> 00:33:07.550
and I also get to new state as prime as next I can Ford probe,

487
00:33:07.551 --> 00:33:10.720
I get this state in the network and they're just to,

488
00:33:10.730 --> 00:33:12.970
what is the maximum I can get from this state?

489
00:33:13.780 --> 00:33:15.910
Take the maximum value and plug it in here.

490
00:33:18.260 --> 00:33:22.850
So this is hopefully what's the optimal [inaudible]?

491
00:33:23.900 --> 00:33:25.880
It's a proxy to a would label.

492
00:33:26.780 --> 00:33:31.780
It means we know that's development equation tells us the best shoe satisfies

493
00:33:31.851 --> 00:33:32.684
this equation.

494
00:33:33.470 --> 00:33:36.740
When you fuck this equation is not true yet because Detroit question,

495
00:33:36.741 --> 00:33:41.450
we have Q star here,
not cue to start,
which is the optimal cheap.

496
00:33:41.960 --> 00:33:46.960
What we hope is that if we use this proxy as our label and we learn the

497
00:33:47.331 --> 00:33:51.980
difference between where we are now and this proxy,
we can then update the proxy,

498
00:33:52.370 --> 00:33:56.540
get closer to the optimality train again of the proxy,

499
00:33:56.541 --> 00:33:58.970
get closer to optimality,
train again and so on.

500
00:33:59.650 --> 00:34:01.310
Our only hope is that this will converge.

501
00:34:02.960 --> 00:34:05.750
So does it make sense how this is different from deep printing?

502
00:34:06.380 --> 00:34:10.490
The labels are moving,
they're not static labels.

503
00:34:12.440 --> 00:34:16.820
We define a label to be a best guess of what would be the best to function we

504
00:34:16.821 --> 00:34:17.654
have.

505
00:34:17.930 --> 00:34:22.490
Then we come to the loss of where the to function is right now compared to cds,

506
00:34:22.700 --> 00:34:26.120
we backed by gates so that dual Q function gets closer to our best guess.

507
00:34:26.690 --> 00:34:29.900
Then now that we have a better cue function,
we can have a better yes.

508
00:34:30.560 --> 00:34:35.450
So we make a better guess and we seek these guests and now we come to you.

509
00:34:35.451 --> 00:34:38.960
The difference between these two functions that we have and our best guess,

510
00:34:39.590 --> 00:34:43.100
we backed up,
propagates up,
we get to our best guess.

511
00:34:43.130 --> 00:34:46.670
We can update our best guests again and we hope that doing that it's relatively,

512
00:34:46.940 --> 00:34:51.940
we'll end with a convergence and a two function that will be very close to

513
00:34:52.281 --> 00:34:55.100
satisfied development equation.
The optimal been winning equation.

514
00:34:55.960 --> 00:35:00.140
Does it make sense?
This is the most complicated part of to learning.
Yeah.

515
00:35:04.140 --> 00:35:07.930
<v 2>For the function we generate the output of the network,</v>

516
00:35:08.350 --> 00:35:11.290
we get to kill function.
We compare it to the queue.

517
00:35:11.680 --> 00:35:15.880
The best to function that we think is it is what is the best to function that

518
00:35:15.910 --> 00:35:17.830
the one that satisfies development equation,

519
00:35:19.260 --> 00:35:20.620
but we'd never actually think you did them both.

520
00:35:20.621 --> 00:35:24.520
Valid equation we don't want but we guess it's based on the queue we have.

521
00:35:25.460 --> 00:35:26.020
Okay,

522
00:35:26.020 --> 00:35:29.620
so basically when you have Q you can come to these bad men equation and it will

523
00:35:29.621 --> 00:35:30.730
give you some values.

524
00:35:31.240 --> 00:35:34.180
These values are probably closer to where you want to get to,

525
00:35:34.181 --> 00:35:38.130
where from where you are now,
where you are now is farther from this optimality

526
00:35:38.820 --> 00:35:43.350
<v 0>and you want to reduce this gap by bye.
Like to close the gap.
You back propagate.</v>

527
00:35:44.910 --> 00:35:47.460
Yes.
Is it possible you need that is the ability to collaborate.

528
00:35:48.120 --> 00:35:51.240
So the question is,
is there a possibility for these to diversity?

529
00:35:51.450 --> 00:35:55.320
So this is a broader discussion that would take a full lecture to prove.

530
00:35:55.620 --> 00:35:58.230
So I put a paper here from [inaudible] called mellow,

531
00:35:58.410 --> 00:36:00.420
which proves the convergence of this algorithm.

532
00:36:00.421 --> 00:36:05.421
So it converges and in fact it converges because we're using a lot of tips and

533
00:36:05.491 --> 00:36:06.540
tricks that we will see later.

534
00:36:07.380 --> 00:36:09.960
But if you want to see the math behind it and it's a,

535
00:36:10.500 --> 00:36:15.410
it's a full lecture of proof,
I invite you to look at this simple,
uh,

536
00:36:15.510 --> 00:36:18.660
proof for convergence of development equation.
Okay?

537
00:36:20.250 --> 00:36:20.550
Okay.

538
00:36:20.550 --> 00:36:25.550
So this is the case where a left score is hired in rice score and we have two

539
00:36:26.161 --> 00:36:27.220
terms in our targets.

540
00:36:27.270 --> 00:36:31.590
You may get reward for taking action left and also discounting maximum future

541
00:36:31.591 --> 00:36:34.470
reward when you are in state.
It's s next.

542
00:36:37.160 --> 00:36:41.660
Okay.
Did the tricky part is that,

543
00:36:42.230 --> 00:36:45.590
let's say we,
we come to that,
we can do it.
We have everything,

544
00:36:45.650 --> 00:36:49.880
we have everything to complete our targets.
We have art which is defined by the,

545
00:36:49.940 --> 00:36:51.870
by the human at the beginning.

546
00:36:52.400 --> 00:36:57.170
And we can also get this number because we know that if we take action lists we

547
00:36:57.171 --> 00:37:01.670
can then get s next and we forward propagates aesthetics into network.

548
00:37:02.120 --> 00:37:04.250
We take the maximum output and it's this.

549
00:37:04.520 --> 00:37:08.270
So we have everything in this in this equation.
No problem.

550
00:37:08.271 --> 00:37:10.190
Now is if I plug this end,

551
00:37:10.191 --> 00:37:13.820
my shoe score in my loss function and I asked you to back propagate

552
00:37:14.600 --> 00:37:19.520
backpropagation is what w equals w minus alpha times the derivative of the loss

553
00:37:19.521 --> 00:37:22.190
function.
When respect to w department is of the network,

554
00:37:23.660 --> 00:37:25.550
which term we're having non zero value,

555
00:37:26.540 --> 00:37:29.720
obviously the second term of s go to the left,

556
00:37:29.721 --> 00:37:32.990
we'll have the non zero value because it depends on the parameters of the

557
00:37:32.991 --> 00:37:37.991
network w but why we'll also have a non zero value because you have Q here.

558
00:37:42.200 --> 00:37:43.550
So how would you handle that?

559
00:37:43.940 --> 00:37:48.940
You actually get a feedback loop in this backpropagation that makes the network

560
00:37:49.670 --> 00:37:53.750
unstable.
What we do is that we consider this fixed.

561
00:37:54.140 --> 00:37:55.910
We will consider this Q fixed.

562
00:37:56.330 --> 00:37:59.300
The issue that is our target is going to be six for many duration,

563
00:37:59.960 --> 00:38:04.220
let's say a million or a hundred thousand nutrition until we get close to there

564
00:38:04.520 --> 00:38:08.210
and our gradient is small,
then we'll update it and we'll fix it.

565
00:38:08.450 --> 00:38:10.250
So we actually have two networks in pilot.

566
00:38:10.310 --> 00:38:14.300
One that is fixed and one that is not fixed.
Okay.

567
00:38:14.510 --> 00:38:16.280
And the second case is similar.

568
00:38:16.670 --> 00:38:20.420
If the Q score to go on the right was more than the Q score to go on the left,

569
00:38:20.690 --> 00:38:25.670
we would define our targets as immediate reward of going to the right plus gamma

570
00:38:25.730 --> 00:38:26.121
times.

571
00:38:26.121 --> 00:38:31.121
The maximum Q score we get if we're in the states that we in the next states and

572
00:38:32.481 --> 00:38:36.250
take the best action.
Does this make
sense?

573
00:38:36.251 --> 00:38:40.150
Is the most complicated part of tutoring.
This is the hard part to understand.

574
00:38:40.780 --> 00:38:43.660
So he made your tree walk to go to the rights and discounted maximum feature.

575
00:38:43.661 --> 00:38:46.840
We're one year in state s next going to dry.

576
00:38:48.040 --> 00:38:51.070
So this is hold six,
four backdrop.

577
00:38:52.240 --> 00:38:56.470
So no derivative.
If we do that,
then no problem.
Why is just a number?

578
00:38:56.920 --> 00:38:59.280
We come back to our origin of supervised learning setting.

579
00:38:59.350 --> 00:39:04.060
Why is a number and we come to the loss and we backed,
propagate,
no difference.

580
00:39:06.520 --> 00:39:07.300
Okay,

581
00:39:07.300 --> 00:39:11.710
so compute dear DL over DW and update w using stochastic gradient descent

582
00:39:11.711 --> 00:39:15.700
methods.
RMS brought Adam whatever you guys want.

583
00:39:17.710 --> 00:39:20.050
So let's go over this.

584
00:39:20.580 --> 00:39:24.400
These full you deep to network implementation.

585
00:39:24.970 --> 00:39:29.350
And this slide is a pseudo code to help you understand how this entire algorithm

586
00:39:29.351 --> 00:39:33.550
work.
We will actually plug in many methods in this,
in this pseudo code.

587
00:39:33.551 --> 00:39:36.370
So please focus right now and,
and you should understand this,

588
00:39:36.371 --> 00:39:38.110
you understand the entire rest of the lecture.

589
00:39:39.010 --> 00:39:41.170
We initialize our tonight's network by amateurs.

590
00:39:41.200 --> 00:39:45.040
Just as we initialize the network in deep yearning,
we loop over episodes.

591
00:39:45.130 --> 00:39:47.260
So let's define an episode to be one game.

592
00:39:47.410 --> 00:39:52.160
Like going from start to end to a terminal.
State is one piece.
Um,

593
00:39:52.360 --> 00:39:57.360
we can also define a of sometimes to be many states like breakouts,

594
00:39:57.490 --> 00:40:00.670
which is the game with the puddle usually is 20 points.

595
00:40:00.730 --> 00:40:02.980
The first day or two get 20 points,
finishes the game.

596
00:40:03.610 --> 00:40:05.830
So at peace with it would be 20 points.

597
00:40:07.570 --> 00:40:12.570
Once you're looping over episode starts from an initial state s in our case it's

598
00:40:12.701 --> 00:40:14.770
only one initial states,
which is state too.

599
00:40:15.370 --> 00:40:20.370
And Lupe overtime steps for propagate s state to India to network.

600
00:40:22.380 --> 00:40:22.670
<v 1>Wow.</v>

601
00:40:22.670 --> 00:40:25.520
<v 0>Execute action a which has the maximum Q score,</v>

602
00:40:26.870 --> 00:40:30.650
observe or immediate reward are.
And the next step is prime.

603
00:40:32.520 --> 00:40:33.210
<v 1>Okay.</v>

604
00:40:33.210 --> 00:40:34.410
<v 0>Compute target.
Why?</v>

605
00:40:34.800 --> 00:40:39.150
And to compute why we know that we need to take as prime for propagates it in

606
00:40:39.300 --> 00:40:43.830
the network again
and then continue the loss function of the department.

607
00:40:43.831 --> 00:40:47.250
There's with gradient descent,
there was this loop.
Makes Sense.

608
00:40:47.730 --> 00:40:49.590
It's very close to what we do in general.

609
00:40:49.591 --> 00:40:54.420
The only difference would be the sports.
Like we come to target,
why?

610
00:40:54.480 --> 00:40:56.190
Using a double for publication.

611
00:40:56.191 --> 00:40:59.520
So we formed propagation with four propagate two times in each loop.

612
00:41:02.440 --> 00:41:06.500
You guys have any questions on on this pseudo code?

613
00:41:13.490 --> 00:41:14.323
Okay,

614
00:41:15.530 --> 00:41:19.460
so we will now see a concrete application of Egyptian networks.

615
00:41:19.550 --> 00:41:22.820
So this was the theoretical partner.
We're going to,
the practical part,

616
00:41:23.180 --> 00:41:25.790
she's going to be to be more fun.
So let's look at this game.

617
00:41:25.850 --> 00:41:26.840
It's called breakouts.

618
00:41:28.660 --> 00:41:32.990
The goal when you played breakouts is to destroy all the bricks without having

619
00:41:33.110 --> 00:41:35.930
bolt pass the line on the bottom.

620
00:41:36.410 --> 00:41:40.460
So we have a pedal and our decisions can be idle,
stay,

621
00:41:40.461 --> 00:41:43.910
stay where you are,
moved to pedal to the right or move the paddle to the left,

622
00:41:44.870 --> 00:41:45.703
right?

623
00:41:49.430 --> 00:41:51.740
And these demo

624
00:41:53.240 --> 00:41:55.460
and you have the credits on the bottom of the slide.

625
00:41:55.890 --> 00:41:58.700
A shows that after training breakouts,

626
00:41:59.900 --> 00:42:04.190
using Cuellar and ink,
they get a super intelligent agents,

627
00:42:04.310 --> 00:42:08.240
which figures out the trick to finish the game very quickly.

628
00:42:09.410 --> 00:42:13.160
So actually even like the good players didn't,

629
00:42:13.161 --> 00:42:17.180
don't know district professional failures,
no district.
But uh,

630
00:42:17.240 --> 00:42:21.590
in breakout you can actually try to dig a tunnel to get on the other side of the

631
00:42:21.591 --> 00:42:26.060
bricks and then you will destroy all the brick super quickly from top to bottom

632
00:42:26.061 --> 00:42:28.800
instead of bottom up.
What's super interesting is that the network,

633
00:42:28.801 --> 00:42:32.210
figure it out this on its own without human supervision.

634
00:42:34.100 --> 00:42:37.730
And this is the kind of thing we want to remember.
If we were to use inputs,

635
00:42:38.330 --> 00:42:40.970
the goal boards and output professional players,

636
00:42:40.971 --> 00:42:44.660
we will not figure out that type of stuff most of the time.

637
00:42:46.700 --> 00:42:50.840
So my question is what's the input of the acute network in this setting or goal

638
00:42:50.841 --> 00:42:53.270
is to destroy all the bricks.
So play breakouts.

639
00:42:56.720 --> 00:42:57.553
<v 1>Okay.</v>

640
00:42:58.290 --> 00:42:59.310
<v 0>What should be the inputs?</v>

641
00:43:11.450 --> 00:43:12.283
<v 1>Try something.</v>

642
00:43:16.550 --> 00:43:18.960
The board at the bottom,
it should have breaks.

643
00:43:19.970 --> 00:43:23.240
<v 0>Position of the puddle of the bricks.
What else?
Ball position.</v>

644
00:43:23.940 --> 00:43:24.320
<v 1>Okay.</v>

645
00:43:24.320 --> 00:43:28.760
<v 0>Yeah,
I agree.
So this is what we would call a future representation.</v>

646
00:43:29.120 --> 00:43:33.380
It means when you're in an environment you can extract some features,
right?

647
00:43:33.560 --> 00:43:36.290
And these are examples of features give you the position of the ball is one

648
00:43:36.291 --> 00:43:39.050
feature.
Give you the position of the bricks and other feature.

649
00:43:39.170 --> 00:43:41.150
Give me the position of the paddle and other feature,

650
00:43:41.360 --> 00:43:42.890
which are good features for this game.

651
00:43:43.490 --> 00:43:47.000
But if you want to get the entire information,
you'd better do something else.

652
00:43:54.680 --> 00:43:58.520
Yeah,
the big cells,
you don't want any human supervision.

653
00:43:58.730 --> 00:44:03.080
You don't want to put seizures,
you just okay,
take the pixels.
They do game,

654
00:44:03.081 --> 00:44:06.380
you can control the paddle,
take the pixel.
So yeah,

655
00:44:06.440 --> 00:44:10.520
this is a good input to the queue network.
What's the outputs?
I said it earlier,

656
00:44:11.150 --> 00:44:16.150
probably the output of the network will be tricks you values representing the

657
00:44:16.701 --> 00:44:20.570
action going left,
going right and staying idle in a specific states.

658
00:44:20.690 --> 00:44:24.860
That is the input of the network.
So give it a pixel image.

659
00:44:24.920 --> 00:44:29.090
We want to predict Q scores for the tree.
Possible actions.

660
00:44:30.530 --> 00:44:33.690
Now what's the issue with that?
Do you think that would work or no?

661
00:44:40.550 --> 00:44:41.383
<v 1>Okay.</v>

662
00:44:42.100 --> 00:44:44.590
<v 0>10 someone think of something going wrong here.</v>

663
00:44:48.150 --> 00:44:49.200
You can get the inputs.

664
00:44:59.760 --> 00:45:00.593
<v 1>Yep.</v>

665
00:45:01.170 --> 00:45:06.060
<v 0>Okay.
I'm going to have to uh,
if I need you.
Yeah,
you're on the train.</v>

666
00:45:09.090 --> 00:45:11.460
Oh yeah.
Good points.
Based on this image,

667
00:45:11.461 --> 00:45:14.480
you cannot know if the ball is going up or down.
It's like Jane,

668
00:45:14.540 --> 00:45:18.120
it's super hard because the action you take highly depends on who the girl is

669
00:45:18.121 --> 00:45:19.230
going up or down.
Right?

670
00:45:19.950 --> 00:45:23.500
Is that ball is going down and even he,

671
00:45:23.540 --> 00:45:26.250
what he's going down and you don't even know which dereg direction he's going.

672
00:45:27.240 --> 00:45:28.770
Yeah.
So there is a problem here.

673
00:45:28.771 --> 00:45:31.620
Definitely there is not enough information to make a decision on the actions to

674
00:45:31.621 --> 00:45:35.130
take.
And if it's hard for us,
he's going to be hard for her to network.

675
00:45:35.880 --> 00:45:39.850
So what's the hot too to prevent that,

676
00:45:40.060 --> 00:45:42.100
it's to take successive frames.

677
00:45:42.700 --> 00:45:45.810
So instead of one frame we can take four frames.
Success,

678
00:45:46.040 --> 00:45:49.180
he freaks and here the same setting as we had before,

679
00:45:49.181 --> 00:45:51.130
but we see that the ball is going up,

680
00:45:52.250 --> 00:45:55.400
we see which direction is going up and we know what action we should tell you

681
00:45:55.401 --> 00:45:59.720
because we know the slope of the bowl and also uh,

682
00:45:59.930 --> 00:46:01.160
also if it's going up or down.

683
00:46:02.180 --> 00:46:02.790
<v 1>Yeah,</v>

684
00:46:02.790 --> 00:46:03.623
<v 0>that makes sense.</v>

685
00:46:03.740 --> 00:46:04.573
<v 1>Okay.</v>

686
00:46:05.000 --> 00:46:09.110
<v 0>Okay,
so this is called the preprocessing.
You've under state computer function,</v>

687
00:46:09.530 --> 00:46:14.150
fight of s that gives you the history of this state,

688
00:46:14.270 --> 00:46:16.880
which is the four sequence of four last frames.

689
00:46:17.720 --> 00:46:19.430
What other preprocessing can we do?

690
00:46:23.030 --> 00:46:24.950
And this is something I want you to be quick,

691
00:46:24.951 --> 00:46:28.240
like we learned it together in deep learning,
input,
preprocessing.

692
00:46:31.730 --> 00:46:36.730
You remember the second lecture where the question was what's resolution should

693
00:46:37.161 --> 00:46:41.870
we use?
Remember you as a catcher got mission,

694
00:46:41.871 --> 00:46:45.890
what's resolution would you want to use?
Here's same thing.

695
00:46:47.920 --> 00:46:50.800
If we can reduce the size of the inputs,
let's do it.

696
00:46:51.370 --> 00:46:55.150
If we don't need all that inflammation,
let's do it.
For example,

697
00:46:55.180 --> 00:46:56.620
do you think the colors are important?

698
00:46:57.750 --> 00:46:58.180
<v 1>Okay,</v>

699
00:46:58.180 --> 00:47:00.400
<v 0>very minor.
I don't think they're important.</v>

700
00:47:00.970 --> 00:47:04.750
So maybe we can gray scale everything that removes three chat,

701
00:47:05.080 --> 00:47:07.120
that's converse three channels into one channel,

702
00:47:07.420 --> 00:47:11.060
which is amazing in terms of computation.
What else?

703
00:47:11.240 --> 00:47:15.590
I think we can crop a lot of this.
Like maybe there's a line here.

704
00:47:15.591 --> 00:47:20.480
We don't need to make any decision.
We don't need the scores.
Maybe.

705
00:47:20.840 --> 00:47:24.260
So actually there's some games where the score is important for decision making.

706
00:47:24.920 --> 00:47:28.850
An example is football or soccer.
Uh,
when,

707
00:47:29.560 --> 00:47:31.950
when you're winning one zero you,

708
00:47:31.960 --> 00:47:34.330
you'd better if you were playing against a strong theme,
defend,

709
00:47:34.331 --> 00:47:37.000
like get back and defend to keep these one zero.

710
00:47:37.270 --> 00:47:41.680
So the score is actually important in the decision making process.
And in fact,

711
00:47:42.050 --> 00:47:42.580
uh,

712
00:47:42.580 --> 00:47:47.580
there are famous coach in football which have this technique called park the bus

713
00:47:48.010 --> 00:47:51.340
where you just put all your team in front of the goal once you have scored a

714
00:47:51.341 --> 00:47:55.570
goal.
So this is an example.
So here there's no park,
the bus,

715
00:47:56.020 --> 00:48:00.790
but we can definitely get rid of the score which removes some pixels and reduces

716
00:48:00.791 --> 00:48:01.900
the number of computations

717
00:48:06.100 --> 00:48:07.250
and we can reduce to gray scale.

718
00:48:07.270 --> 00:48:10.170
One important thing to be careful about when you reduce your regret scale is

719
00:48:10.180 --> 00:48:13.120
that gray scale is a dimension illiterate Dixon technique.

720
00:48:13.390 --> 00:48:14.920
It means you lose information,

721
00:48:15.550 --> 00:48:19.540
but you know if you have three channels and you reduce everything in one
channel,

722
00:48:19.570 --> 00:48:22.780
sometimes you would have different color pixels which will end up with the same

723
00:48:22.781 --> 00:48:26.950
gray scale value depending on the gray scale that you use and it's been seen

724
00:48:26.951 --> 00:48:29.050
that you lose some information sometimes.

725
00:48:29.051 --> 00:48:33.220
So let's say the ball and some bricks have the same gray scale value,

726
00:48:34.090 --> 00:48:38.380
then you would not differentiate them or let's say the paddle and the background

727
00:48:38.381 --> 00:48:41.260
have the same gray scale value,
then you would not differentiate them.

728
00:48:41.290 --> 00:48:43.060
So you have to be careful of that type of stuff.

729
00:48:43.360 --> 00:48:47.230
And there's other methods that do gray scale and not other ways like
illuminates.

730
00:48:48.610 --> 00:48:51.070
So we have our five s which is this,

731
00:48:51.820 --> 00:48:54.970
which is this input to the queue network.

732
00:48:55.030 --> 00:48:57.760
And the deep Q network architecture is going to be a convolutional neural

733
00:48:57.761 --> 00:49:01.270
network because we're working with the images.
So we propagate dots.

734
00:49:01.271 --> 00:49:03.670
This is the architecture from mean government.

735
00:49:03.671 --> 00:49:08.671
So glue silver at all from the point [inaudible] Relu to fully connected layers

736
00:49:10.601 --> 00:49:11.860
and you get your Q scores

737
00:49:13.920 --> 00:49:17.940
and we get back to our training loop.

738
00:49:18.570 --> 00:49:23.570
So what do we need to change in our training loop here is we said that one frame

739
00:49:23.641 --> 00:49:25.620
is not enough.
So we preprocess all the frames.

740
00:49:25.920 --> 00:49:28.670
So the initial state is converted to five of us.

741
00:49:29.500 --> 00:49:32.490
The four propagation did state is five of us and so on.

742
00:49:32.730 --> 00:49:37.260
So everywhere we had s or s prime,
we convert two five s or five s prime,

743
00:49:37.261 --> 00:49:38.250
which gives us the story.

744
00:49:39.180 --> 00:49:43.470
Now there are a lot more techniques that we can plug in here and we will see

745
00:49:43.500 --> 00:49:47.490
three more.
One is keeping track of the terminal states in the slope.

746
00:49:47.491 --> 00:49:50.310
We should keep track of the terminal state because we said if we reach a

747
00:49:50.311 --> 00:49:51.810
terminal state,
we want to end the loop,

748
00:49:51.860 --> 00:49:55.950
break the loop and not the reason is because the wife function.

749
00:49:56.610 --> 00:49:58.320
So basically we have to add,

750
00:49:58.380 --> 00:50:02.060
create a Boolean to detect the terminal states before looping through the time

751
00:50:02.061 --> 00:50:07.061
steps and inside the loop we want to check if the new s prime we're going to is

752
00:50:09.061 --> 00:50:11.400
a terminal states.
If it's a terminal state,

753
00:50:11.670 --> 00:50:15.240
then I can stop the sloop and go back,
play another APP visit.

754
00:50:15.870 --> 00:50:19.680
So play another start.
That's another starting states and continuing my game

755
00:50:21.360 --> 00:50:25.380
now,
this wide target that we compute is different.

756
00:50:25.381 --> 00:50:26.910
If we're in a terminal state are nuts

757
00:50:29.390 --> 00:50:30.740
because if we were at 10 real estate,

758
00:50:30.770 --> 00:50:33.740
there is no reason to have a discounted longterm reward.

759
00:50:33.980 --> 00:50:37.270
There's nothing behind that term real estate.
So if we're in turmoil sake,

760
00:50:37.280 --> 00:50:39.740
we just said it to the immediate reward and we break.

761
00:50:40.070 --> 00:50:41.480
If we're not in a terminal state,

762
00:50:41.510 --> 00:50:43.640
then we would add these discounted future reward.

763
00:50:45.870 --> 00:50:46.860
Any questions on that?

764
00:50:51.680 --> 00:50:52.513
Yup.

765
00:50:53.210 --> 00:50:57.530
Another issue that we're seeing these and which makes,
uh,

766
00:50:57.680 --> 00:51:00.800
eastern enforcement learning setting super different from the classic supervised

767
00:51:00.801 --> 00:51:04.550
learning setting is that we only train on what we explore.

768
00:51:05.750 --> 00:51:08.060
It means I'm starting,
you know,
state s

769
00:51:09.650 --> 00:51:13.520
I come to [inaudible] forward [inaudible] this fire of us in my network.

770
00:51:14.150 --> 00:51:18.770
I get my vector of two values.
I select the best Q value,

771
00:51:18.830 --> 00:51:23.230
they're our largest,
I get the new states because I can move now from state sts,

772
00:51:23.240 --> 00:51:26.750
Brian.
So I have a transition from s take action,

773
00:51:26.780 --> 00:51:31.280
a get as prime or fire of us.
They actually ate it.
Five of us,
Brian.

774
00:51:32.600 --> 00:51:36.200
Now this is what I will use to train my network.

775
00:51:36.950 --> 00:51:41.950
I can forward propagate five s prime again in the network and get my why targets

776
00:51:43.610 --> 00:51:47.420
come from my why to my queue and then back propagate.

777
00:51:48.620 --> 00:51:51.860
The issue is I may never explore the state transition again.

778
00:51:52.160 --> 00:51:54.470
Maybe I will never get there anymore.

779
00:51:55.010 --> 00:51:57.800
It's super different from what we do in supervised learning where you have a

780
00:51:57.810 --> 00:52:00.080
datasets and your data set can be used.

781
00:52:00.081 --> 00:52:04.580
Many times we batch grading dissent or with any gradient,
decent algorithm.

782
00:52:04.610 --> 00:52:09.320
One epoch,
you see all the data points.
So you should do to eight bucks.

783
00:52:09.321 --> 00:52:11.760
You see everyday two points,
two times.
If you do 10 bucks,

784
00:52:11.780 --> 00:52:14.330
you see everyday to put straight three times or 10 times.

785
00:52:14.780 --> 00:52:17.750
So it means that everyday two points can be used several times to train your

786
00:52:17.751 --> 00:52:20.510
algorithm in classic deep yearning that we've seen together.

787
00:52:20.720 --> 00:52:25.640
In this case it doesn't seem possible because we only train when we explore and

788
00:52:25.641 --> 00:52:26.780
we might never get back there,

789
00:52:27.740 --> 00:52:30.710
especially because the training will be influenced by where we go.

790
00:52:31.100 --> 00:52:34.850
So maybe there's some places where we will never go because why we train and why

791
00:52:34.851 --> 00:52:36.050
we learn it will,

792
00:52:36.320 --> 00:52:40.040
it will kind of direct or decision process and we will never train on some parts

793
00:52:40.041 --> 00:52:40.874
of the game.

794
00:52:40.880 --> 00:52:43.700
So this is why we have other techniques to keep these training stable.

795
00:52:43.910 --> 00:52:48.230
One is called experience replay.
So as I said,
here's what we are currently doing.

796
00:52:49.010 --> 00:52:50.090
We have five of us,

797
00:52:50.690 --> 00:52:54.290
four propagates get a from taking action.

798
00:52:54.320 --> 00:52:58.130
A we observe an immediate reward are and a new states firewise prime.

799
00:52:58.940 --> 00:53:03.830
Then from Firebase prime which can take a new action,
a prime observer,

800
00:53:03.840 --> 00:53:08.330
a new reward or prime and the new state fire of us prime,

801
00:53:08.331 --> 00:53:10.280
prime and so on

802
00:53:12.020 --> 00:53:16.700
and each of these is called a state transition and can be used to train.

803
00:53:16.880 --> 00:53:20.480
This is one experience leads to one iteration of gradient descent,

804
00:53:23.480 --> 00:53:25.500
e one e two e three experience,

805
00:53:25.501 --> 00:53:29.100
one experience to experienced tree and the training will be trained on

806
00:53:29.101 --> 00:53:32.010
experience one then train on experience to then try not experience street.

807
00:53:32.610 --> 00:53:35.580
What we're doing with experienced replay is the following.

808
00:53:36.600 --> 00:53:41.280
We will observe experienced one because we started to say we take an action,

809
00:53:41.340 --> 00:53:44.550
we see another state and really reward and this is called experience one.

810
00:53:45.120 --> 00:53:47.190
We'll create a replay memory.

811
00:53:48.180 --> 00:53:51.930
You can think of it as a data structure in computer science and you will place

812
00:53:51.931 --> 00:53:54.690
this experience one tapo in this replay memory.

813
00:53:55.860 --> 00:53:58.050
Then from there we will experience experience too.

814
00:53:58.800 --> 00:54:02.070
We'll put the experience to indirect play memory.
Same with experienced tree,

815
00:54:02.100 --> 00:54:06.780
put it in the replay number and so on.
Now during training,

816
00:54:06.781 --> 00:54:10.500
what we will do is we will first train on experience one because it's just only

817
00:54:10.501 --> 00:54:12.930
experience we have.
So so far.

818
00:54:13.980 --> 00:54:17.770
Next step instead of training on e two we will train on a sample from

819
00:54:17.860 --> 00:54:18.693
[inaudible].

820
00:54:18.720 --> 00:54:22.470
It means we will take one out of the replay memory and use this one for
training,

821
00:54:24.570 --> 00:54:29.010
but we will still continue to experiment something else and we'll sample from

822
00:54:29.011 --> 00:54:33.120
there.
And at every step the replay memory will become bigger and bigger.

823
00:54:33.270 --> 00:54:36.960
And why we train,
we will not necessarily train on the step we explore,

824
00:54:36.990 --> 00:54:41.990
we will train on a sample which is the replay memory plus the new state we we we

825
00:54:42.331 --> 00:54:43.164
explore

826
00:54:44.790 --> 00:54:48.360
why is it good is because he won as you see can be useful many times in the

827
00:54:48.361 --> 00:54:50.580
training and maybe one was a critical state,

828
00:54:50.581 --> 00:54:55.581
like it was a very important data point to learn or Q function and so on.

829
00:54:56.431 --> 00:54:59.340
And so does the repayment where it makes sense.

830
00:55:00.640 --> 00:55:03.180
So several advantages.
One is data efficiency.

831
00:55:03.600 --> 00:55:07.350
We can use data many times don't have to use one data point only one time.

832
00:55:08.790 --> 00:55:13.790
Another very important advantage of experience replay is that if you don't use

833
00:55:15.121 --> 00:55:15.900
experience replay,

834
00:55:15.900 --> 00:55:19.350
you have a lot of correlation between the successive data points.

835
00:55:19.920 --> 00:55:24.180
So let's say the ball is on the bottom right here and the ball is going to the

836
00:55:24.181 --> 00:55:29.181
top left for the next 10 data points.

837
00:55:29.700 --> 00:55:34.700
The bull is always going to go to the top left and it means the action you can

838
00:55:35.851 --> 00:55:37.770
take is always the same.

839
00:55:37.800 --> 00:55:40.290
It actually doesn't matter a lot because the ball is going up,

840
00:55:41.190 --> 00:55:43.800
but most likely you want to follow the where the ball is going.

841
00:55:43.920 --> 00:55:48.920
So the action will be to go towards the ball for 10 actions in a row and then

842
00:55:49.351 --> 00:55:54.351
the ro the ball would bounce on the wall and on the top and go back down here

843
00:55:54.840 --> 00:55:59.670
down to the bottom left bottom right.
What will happen if you're a pilot is here,

844
00:55:59.671 --> 00:56:03.180
is that for 10 steps in euro,
you will send your paddle on the right.

845
00:56:04.740 --> 00:56:07.650
Remember what we said when,
which,
when we ask the question,

846
00:56:07.680 --> 00:56:12.030
if you had to train a chat vs.
Dot.
Classifier with batches of images of cats,

847
00:56:12.570 --> 00:56:15.180
batches of images of dog trained first on the cats,

848
00:56:15.181 --> 00:56:18.150
then trains on the dogs than trends on the cats.
Then trends on the dogs.

849
00:56:18.570 --> 00:56:22.290
We will not converge because your network will be super biased towards

850
00:56:22.291 --> 00:56:26.160
predicting cats after seeing 10 images of cats.
Super bias,

851
00:56:26.620 --> 00:56:29.890
we predicting dogs when sees 10 images of dogs.
That's what's happening here.

852
00:56:31.230 --> 00:56:33.970
So you want to correlate all these experiences.

853
00:56:34.270 --> 00:56:35.890
You want to be able to take one experience,

854
00:56:35.891 --> 00:56:38.230
take another one that has nothing to do with it and so on.

855
00:56:39.310 --> 00:56:40.860
This is what experience purely platos.

856
00:56:41.350 --> 00:56:44.890
And the third one is that the third one is that,
uh,

857
00:56:45.030 --> 00:56:49.690
you're basically trading computation and memory against exploration.

858
00:56:50.560 --> 00:56:54.700
Exploration is super costly.
The state space might be super big,
but you know,

859
00:56:54.701 --> 00:56:57.640
you have enough computation probably you can have a lot of competition and you

860
00:56:57.641 --> 00:56:59.950
have memory space.
Let's use an experienced replay.

861
00:57:02.400 --> 00:57:03.233
<v 3>Okay,</v>

862
00:57:03.800 --> 00:57:06.320
<v 0>so let's address experience replay to our code here.</v>

863
00:57:08.210 --> 00:57:12.740
The transition resulting from this part is added to the experience to the replay

864
00:57:12.741 --> 00:57:15.770
memory d and will not necessarily be using nutrition space.

865
00:57:15.980 --> 00:57:20.330
So what's happening is we're four propagate,
five of us,
we get,

866
00:57:20.840 --> 00:57:23.420
we observe a reward and an action,

867
00:57:24.410 --> 00:57:26.810
and this action leads to a state five s prime.

868
00:57:27.410 --> 00:57:31.010
This is an experience instead of training on this experience,

869
00:57:31.460 --> 00:57:33.530
I'm just going to take it,
put it in the replay memory,

870
00:57:34.130 --> 00:57:38.390
add experience to replay memory.
And what I we train on is not,

871
00:57:38.391 --> 00:57:42.380
this experience is a sample random minibatch of transition from the replay

872
00:57:42.381 --> 00:57:46.400
memory.
So you see we're exploring,
but we're not training on what we explore.

873
00:57:46.940 --> 00:57:50.000
We're training on the replay memory,
but the replacement where he's dynamic,

874
00:57:51.230 --> 00:57:52.063
it changes

875
00:57:54.460 --> 00:57:56.320
and updates using the sample transitions.

876
00:57:56.470 --> 00:57:59.920
So the sample transition from the replacement,
we will be used to do the updates.

877
00:58:00.460 --> 00:58:04.730
That's the huck.
Now another hot,
we want the last hack.

878
00:58:04.740 --> 00:58:08.980
We want to talk about these exploration versus exploitation.
So as a human,

879
00:58:09.200 --> 00:58:13.240
let's say you're commuting to Stanford every day and you know the road you were

880
00:58:13.241 --> 00:58:15.160
commuting at,
you know it,

881
00:58:15.190 --> 00:58:19.570
you always take the same road and your bias towards taking this road.
Why?

882
00:58:19.630 --> 00:58:24.550
Because the first time you took it to 2012 and the more you take it no more you

883
00:58:24.551 --> 00:58:28.330
learn about it.
Not that it's good to know the tricks of how to drive fast,

884
00:58:28.331 --> 00:58:31.720
but there's like you know the tricks,
you know that this,
this,

885
00:58:32.500 --> 00:58:36.220
these slides is going to be green at that moment and so on.
So you,
you,

886
00:58:36.221 --> 00:58:40.210
you build a very good expertise in this road,
super expert,

887
00:58:41.410 --> 00:58:44.740
but maybe there's another road that you don't want to try it.
That is better.

888
00:58:44.980 --> 00:58:48.040
You just don't try it because you're focused on that road.

889
00:58:48.070 --> 00:58:51.280
You're doing exploitation,
you exploit what you already know.

890
00:58:52.330 --> 00:58:56.050
Exploration would be,
okay,
let's do it.
I'm going to try another road today.

891
00:58:56.080 --> 00:58:59.230
I might get late to the course but maybe I will have a good discovery and I

892
00:58:59.231 --> 00:59:00.830
would like this road and then we'll take it later on.

893
00:59:01.510 --> 00:59:04.870
There's a trade off between these two because the aural algorithm is going to

894
00:59:04.871 --> 00:59:09.871
figure out some strategies that are super good and we'll try to do local search

895
00:59:11.350 --> 00:59:12.790
in this to get better and better.

896
00:59:13.270 --> 00:59:17.770
But you might have another minimum that is better than this one and you don't

897
00:59:17.771 --> 00:59:20.710
explore it using the algorithm we currently have.

898
00:59:20.711 --> 00:59:23.450
There is no tradeoff between exploitation and exploration.

899
00:59:23.451 --> 00:59:27.980
We are almost doing only exploitation.
So how to incentivize these exploration.

900
00:59:29.240 --> 00:59:30.110
You guys have an idea.

901
00:59:45.710 --> 00:59:50.710
<v 2>So right now when we were in a state s were for propagating the state's purposes</v>

902
00:59:51.130 --> 00:59:54.560
states in the network and we take the action that is the best action always.

903
00:59:55.670 --> 00:59:58.730
So we exploiting,
we're exploiting what we already know.
We take the best action.

904
00:59:59.720 --> 01:00:02.510
Instead of teaching these best action,
what can we do?

905
01:00:06.250 --> 01:00:10.840
Yup.
Monte Carlo sampling,
we points another one.

906
01:00:10.870 --> 01:00:11.830
You wanted to try something else.

907
01:00:14.060 --> 01:00:18.460
It's the ratio of times you take the best action versus exploring another
action.

908
01:00:19.000 --> 01:00:23.140
Okay.
Take a pipe or parameter that tells you when you can explore,

909
01:00:23.141 --> 01:00:27.940
when you can exploit that what you mean.
Yeah,
that's a good point.

910
01:00:27.941 --> 01:00:29.100
So I think that that's

911
01:00:29.400 --> 01:00:31.320
<v 0>the solution.
You can take an eyebrow primary two,</v>

912
01:00:31.321 --> 01:00:36.240
that is a probability telling you with this probability explore.

913
01:00:36.960 --> 01:00:41.040
Otherwise we'd one minus it this probably t exploit.
That's what,

914
01:00:41.070 --> 01:00:41.940
that's what we're going to do.

915
01:00:42.630 --> 01:00:45.450
So let's look why exploration versus explanation doesn't work.

916
01:00:46.470 --> 01:00:49.980
We're in initial state one s one and we have three options.

917
01:00:50.190 --> 01:00:54.840
Either we go using action a one two s two and we get rewarded of zero or we go

918
01:00:54.841 --> 01:00:55.590
to action.

919
01:00:55.590 --> 01:01:00.590
Use Action to get to as three and get reward of one or use action tree and go to

920
01:01:02.041 --> 01:01:07.041
s four and get the reward of 1000 so this is obviously where we want to go.

921
01:01:07.980 --> 01:01:11.820
We want to go to s four because it has the maximum reward and we don't need to

922
01:01:11.821 --> 01:01:15.660
do much computation in our heads.
It's simple.
There is no discount.
It's direct.

923
01:01:17.070 --> 01:01:18.930
Just after initializing the Q networks,

924
01:01:18.931 --> 01:01:23.931
you get the following key values for propagates as one in the queue network and

925
01:01:24.901 --> 01:01:29.010
get 0.54 taking action 1.44 take action.

926
01:01:29.011 --> 01:01:31.870
2.3 for texting action three.

927
01:01:33.000 --> 01:01:36.420
So this is obviously not good,
but our network,
he was randomly initialized.

928
01:01:37.110 --> 01:01:41.250
What it's telling us is that
0.5 is the maximum.

929
01:01:42.150 --> 01:01:43.530
So we should take action one.

930
01:01:44.160 --> 01:01:47.820
So let's go take action one observe as to your observer.
Reward of zero.

931
01:01:48.240 --> 01:01:52.320
Our targets,
because it's a terminal state is only equal to the reward.

932
01:01:52.350 --> 01:01:56.670
There's no additional term.
So we want our target too much.
Our Queue,

933
01:01:56.700 --> 01:02:01.360
our target is zero so Q should match zero.
So we train and we get the cue.

934
01:02:01.400 --> 01:02:04.170
That should be zero.
Does that make sense?

935
01:02:06.210 --> 01:02:09.840
Now we do another round of iteration.
We look,

936
01:02:09.900 --> 01:02:13.080
we're in s one we get back to the beginning of the episode.

937
01:02:13.230 --> 01:02:18.230
We see that or chew function tells us that action two is the best because 0.4 is

938
01:02:18.451 --> 01:02:21.720
the maximum.
It means go to a street.

939
01:02:22.800 --> 01:02:26.910
I go to a street,
I observed reward of one.
What does it mean?

940
01:02:26.940 --> 01:02:30.390
It's a terminal state.
So my targets is one.
Why you called [inaudible]?

941
01:02:31.030 --> 01:02:34.260
I wanted to acute too much,
my wife,
so my cue should be one.

942
01:02:36.090 --> 01:02:39.990
Now I continue.
Third step up to function.
Ses,

943
01:02:40.020 --> 01:02:43.230
go to Aa to I go to eight to nothing happens.

944
01:02:43.260 --> 01:02:46.830
I already matched the dirty work for step,
go to [inaudible].

945
01:02:47.930 --> 01:02:50.160
You see what happens?
We never go there.

946
01:02:50.700 --> 01:02:55.150
We'll never get there because we're not exploring.
So instead of doing that,

947
01:02:55.160 --> 01:03:00.160
what we were saying is that 5% of the time take a random action to explore and

948
01:03:01.171 --> 01:03:05.340
95% of the time follow your exploitation.
Okay?

949
01:03:07.290 --> 01:03:10.560
So that's where we added,
we've probably to epsilon the hyper parameter,

950
01:03:10.650 --> 01:03:12.090
take random action eight,

951
01:03:12.720 --> 01:03:15.870
otherwise do what we were doing before exploit.

952
01:03:17.040 --> 01:03:19.890
Does that make sense?
Okay,
cool.

953
01:03:20.310 --> 01:03:24.810
So now we plugged in all these tricks in our pseudo code and this is our new

954
01:03:24.811 --> 01:03:27.990
sudo code.
So we have to initialize a replay memory,

955
01:03:28.050 --> 01:03:32.250
which we didn't have to do earlier in blue.
And you can find the replay memory.

956
01:03:32.400 --> 01:03:37.290
I did lines in orange,
you can find the ID lines for checking the terminal state.

957
01:03:37.380 --> 01:03:41.850
And in purple you can find the,
I did lines related to epsilon greedy,

958
01:03:42.530 --> 01:03:45.180
uh,
exploration versus exploitation

959
01:03:46.860 --> 01:03:49.110
and find it in bold,
the preprocessing.

960
01:03:51.030 --> 01:03:54.990
Any questions on that?
So that,
that's,

961
01:03:54.991 --> 01:03:59.490
that's we want you to see a volume into of how deep learning can be used in the

962
01:03:59.550 --> 01:04:02.820
setting that is not necessarily classic supervisor and exotic.

963
01:04:07.300 --> 01:04:10.330
And you see that domain advantage vintage of deep learning in this case is it's

964
01:04:10.331 --> 01:04:14.050
a good function approximator you'll convolutional neural network can extract a

965
01:04:14.051 --> 01:04:18.130
lot of information from the pixels that we were not able to get with other

966
01:04:18.131 --> 01:04:21.820
networks.
Okay,
so let's,

967
01:04:21.821 --> 01:04:23.170
let's see what we have here.

968
01:04:23.350 --> 01:04:28.350
We have our Super Atari butts that's going to dig a tunnel and he's going to

969
01:04:28.871 --> 01:04:30.250
destroy all the brick super quick.

970
01:04:33.120 --> 01:04:37.400
It's cool to see that after building it like so this is work from deep mines

971
01:04:37.401 --> 01:04:41.880
team and that you can find these video on youtube.
Okay.

972
01:04:42.060 --> 01:04:45.600
Another thing I wanted to say quickly is what's the difference between with and

973
01:04:45.601 --> 01:04:48.210
without human knowledge?
You will see a lot of people,

974
01:04:48.211 --> 01:04:52.890
a lot of papers mentioning that this algorithm was trained with human learn

975
01:04:52.891 --> 01:04:56.310
knowledge or this algorithm was trained without any human in the loop.

976
01:04:57.540 --> 01:05:02.160
Why is human knowledge very important?
Like think about it.

977
01:05:02.220 --> 01:05:06.870
Just playing one game as a human in teaching that to the algorithm will help the

978
01:05:06.871 --> 01:05:10.050
algorithm a lot.
When the algorithm sees this gay,

979
01:05:11.550 --> 01:05:16.050
what it sees,
it's pixels.
What do we see when we see that game?

980
01:05:16.500 --> 01:05:20.020
We see that there is a key here.
We know the key is usually a good thing.

981
01:05:20.380 --> 01:05:22.390
So we have a lot of context,
right?
As a human,

982
01:05:22.570 --> 01:05:25.890
we know I'm probably gonna go for the key.
I'm not going to go for this,

983
01:05:25.891 --> 01:05:29.410
this thing.
No.
Uh,
same leather.
What is the ladder?

984
01:05:29.420 --> 01:05:33.070
We directly identified that the ladder is something we can go up and down.

985
01:05:33.730 --> 01:05:37.300
We identified that this rope is probably something I can use to jump from one

986
01:05:37.301 --> 01:05:38.950
site to the other.
So as a human,

987
01:05:38.951 --> 01:05:42.430
there's a lot more background information that we have even without knowing it,

988
01:05:42.670 --> 01:05:43.750
without realizing it.

989
01:05:44.350 --> 01:05:48.220
So there's a huge difference between algorithms trained with human in the loop

990
01:05:48.250 --> 01:05:51.910
and without human in the loop.
This game is actually Montezuma revenge,

991
01:05:52.390 --> 01:05:55.260
the DQA and algorithm.
When the paper came out on,

992
01:05:55.360 --> 01:05:57.790
on the nature or nature in nature,
then the second,

993
01:05:57.820 --> 01:05:59.110
the second version of the paper,

994
01:05:59.500 --> 01:06:04.210
they showed that they beat human on 49 games that are the same type of games.

995
01:06:04.211 --> 01:06:07.060
I was very careful,
but this one was the hardest one.

996
01:06:07.720 --> 01:06:09.700
So they couldn't beat human on this one.

997
01:06:10.270 --> 01:06:15.270
And the reason was because there's a lot of information and also the game has is

998
01:06:16.181 --> 01:06:19.240
very long.
So in order it's called Montezuma revenge.

999
01:06:19.270 --> 01:06:22.750
And I think rounding Carol Marty is going to talk about Italy till later,

1000
01:06:22.751 --> 01:06:25.210
but in order to get to win this game,

1001
01:06:25.570 --> 01:06:29.290
you have to go through a lot of different stages and it's super long.

1002
01:06:30.160 --> 01:06:34.810
So it's super hard for the,
the,
the algorithm to explore all the state space.

1003
01:06:36.400 --> 01:06:40.860
Okay.
So that said,
I will show you a few more games that,
that uh,

1004
01:06:40.900 --> 01:06:42.430
the deepmind team has solved.

1005
01:06:42.431 --> 01:06:47.431
Pong is one sequence is another one and space invaders that you might know

1006
01:06:47.561 --> 01:06:51.580
which,
which is probably the most famous of the tree.
What Gino.

1007
01:06:53.200 --> 01:06:55.600
Okay,
so that said,

1008
01:06:55.690 --> 01:06:58.510
I'm going to hand in the microphone too.

1009
01:06:58.540 --> 01:07:02.410
We're lucky to have a neural expert.
So Rom team,
Tara Martiza,

1010
01:07:02.650 --> 01:07:06.150
first year PhD students,
uh,
in Rl,
uh,

1011
01:07:06.190 --> 01:07:09.670
working with prophecy brunskill at Stanford.
And uh,

1012
01:07:09.730 --> 01:07:12.970
he will tell us a little bit about his experience and he will show us some

1013
01:07:13.000 --> 01:07:16.660
advanced applications of deep learning and RL and how these plugin

1014
01:07:17.270 --> 01:07:22.070
<v 4>to get her.
Thank you.
Thanks for that intro.
Lecture.
Oh yeah.</v>

1015
01:07:22.560 --> 01:07:27.260
Uh,
can everyone hear me now?
All right,
good.
Cool.

1016
01:07:27.890 --> 01:07:32.830
I get can first I have like eight,
nine minutes.
Uh,
have more.

1017
01:07:33.170 --> 01:07:34.820
Okay,
great.
Uh,
okay.

1018
01:07:34.821 --> 01:07:39.020
First question after seeing that lecture so far,

1019
01:07:39.140 --> 01:07:43.040
like how many are,
you're thinking that Earl is actually cool?
Like honestly,

1020
01:07:44.420 --> 01:07:48.890
that's like,
oh,
that's a lot.
Oh yeah,
that's a lot.
Okay.

1021
01:07:49.070 --> 01:07:53.050
My hope is after showing you some other advanced topics years,

1022
01:07:53.070 --> 01:07:58.040
then the percentage go even increase.
So let's,
uh,
let's see.

1023
01:07:58.570 --> 01:08:03.560
Uh,
it's almost impossible to talk about advancement oral look recently without

1024
01:08:03.770 --> 01:08:05.020
mentioning Alphago.

1025
01:08:05.240 --> 01:08:09.940
I think somewhere right now who wrote that on the table that it's almost tend to

1026
01:08:09.990 --> 01:08:14.060
the,
uh,
public hundred and 70 different configuration of the board.

1027
01:08:14.810 --> 01:08:18.080
And that's roughly mode that,
I mean,

1028
01:08:18.110 --> 01:08:20.540
that's more than the estimated number of atoms in the universe.

1029
01:08:20.930 --> 01:08:23.370
So avant traditional algorithm,

1030
01:08:23.371 --> 01:08:27.200
but before the planning and stuff like that was like two searching RL,

1031
01:08:27.230 --> 01:08:32.230
which is basically go exhaustively search all the other possible action that you

1032
01:08:32.271 --> 01:08:35.860
can take and then take the best one in that situation.
All forgot.

1033
01:08:35.861 --> 01:08:37.430
That's all almost impossible.

1034
01:08:38.030 --> 01:08:42.080
So what they do that's also a paper from deep mind is that they train,

1035
01:08:42.680 --> 01:08:47.360
uh,
and we were on this before that they kind of marriage the two research with

1036
01:08:47.361 --> 01:08:51.170
deep,
deep neural network that they have.

1037
01:08:51.560 --> 01:08:53.030
They have two kinds of networks.

1038
01:08:53.360 --> 01:08:58.360
One is called the value network value network is basically consuming this image,

1039
01:08:58.460 --> 01:09:03.460
a image of a book and telling you what's the probability that if you can win in

1040
01:09:03.471 --> 01:09:04.304
this situation,

1041
01:09:05.060 --> 01:09:09.100
so if the value is higher than the probability of winning is higher,

1042
01:09:10.250 --> 01:09:11.061
how does it help you?

1043
01:09:11.061 --> 01:09:13.820
He'll help you in the case that if you want to search for the action,

1044
01:09:13.821 --> 01:09:16.400
you don't have to go until the end of the game because the end of the game is a

1045
01:09:16.401 --> 01:09:19.790
lot of steps and it's almost impossible to go to the end of the gaming all the

1046
01:09:19.791 --> 01:09:20.624
simulations.

1047
01:09:21.170 --> 01:09:24.650
So that just helps you to understand what's the value of each game like

1048
01:09:24.651 --> 01:09:28.250
beforehand,
like after it for the 50th state,
if you're going to win that game of,

1049
01:09:28.251 --> 01:09:30.440
or if you're going to lose that game,
there is another.

1050
01:09:30.470 --> 01:09:33.920
And let's rock of the Policy Network,
which helps us to take action.

1051
01:09:34.310 --> 01:09:39.310
But I think the most interesting thing of the Alphago is that it's trained from

1052
01:09:40.011 --> 01:09:43.970
scratch.
So trends from nothing and they have,

1053
01:09:43.971 --> 01:09:48.971
it's really called self play that I did is to AI playing with each other.

1054
01:09:49.820 --> 01:09:53.960
The best one refugee,
the West,
the best one,
I keep it fixed.

1055
01:09:53.990 --> 01:09:56.510
And I have another one that is trying to copy it,

1056
01:09:56.780 --> 01:09:59.480
the Treevis version of itself and after it's complete,

1057
01:09:59.481 --> 01:10:02.090
the previous version of itself,
like reliably,

1058
01:10:02.091 --> 01:10:05.870
many times that I replace this again for the previous one and that I just said.

1059
01:10:06.170 --> 01:10:11.000
So this is a training care of Africa itself.
Self play off the Alphago as we say,

1060
01:10:11.600 --> 01:10:14.870
and it takes a lot of compute.
So that's kind of crazy.

1061
01:10:15.050 --> 01:10:16.820
But finally they beat the human.

1062
01:10:19.700 --> 01:10:23.450
Okay.
And other type of algorithm that this is like the whole different class of

1063
01:10:23.451 --> 01:10:27.400
algorithm called policy gradients.
Uh,

1064
01:10:28.130 --> 01:10:30.440
algorithm called trust region.
Policia.
Oh,

1065
01:10:30.680 --> 01:10:35.460
can I stop the massive resistance,
locomotion controller.
Right.

1066
01:10:35.480 --> 01:10:39.320
Can you renewed the sound please?
Okay,
great.

1067
01:10:39.530 --> 01:10:41.360
So policy Garrick an algorithm,

1068
01:10:45.930 --> 01:10:50.280
well I can do is that we stop this from here.
Uh,
no,

1069
01:10:50.300 --> 01:10:54.200
there's not a piece of work.
Okay.
So here,

1070
01:10:54.201 --> 01:10:58.100
like in the Dick Shoe and that you've seen,
uh,
you,

1071
01:10:58.850 --> 01:11:00.220
you came and the compute,

1072
01:11:00.230 --> 01:11:04.760
the Q value of feature state and then what you have done is that you take the

1073
01:11:04.760 --> 01:11:07.640
Arg Max of this with respect to action and then you choose the action that you

1074
01:11:07.641 --> 01:11:11.570
want choose,
right?
But what chair at the end of the day is the action,

1075
01:11:11.630 --> 01:11:15.600
which is the mapping from state to action,
which if we call it a good policy,

1076
01:11:16.080 --> 01:11:18.840
right?
So did you care at the end of the day actually the policy,

1077
01:11:18.841 --> 01:11:21.730
like what actions should they take is not really Q value itself,
right?

1078
01:11:22.050 --> 01:11:23.010
So this class,

1079
01:11:23.350 --> 01:11:28.350
a class of methods that call it policy gradients is trying to directly optimize

1080
01:11:28.531 --> 01:11:31.740
for the policy.
So rather than update the two function,

1081
01:11:32.190 --> 01:11:36.670
I can't put a gradient of my policy or updating the policy network again and

1082
01:11:36.671 --> 01:11:40.170
again and again.
So let's see these videos.

1083
01:11:41.220 --> 01:11:46.220
So this is like this guy that is trying to reach the pink bottle over there and

1084
01:11:47.340 --> 01:11:52.340
sometimes it gets hit by the some external forces and this is called hover off

1085
01:11:53.521 --> 01:11:58.220
the algorithm called the PPO policy gradient.
I try to reach to that book.

1086
01:11:59.160 --> 01:12:03.600
So I think that you've heard of uh,
open AI like five,

1087
01:12:03.601 --> 01:12:06.240
like the,
but that is playing Dota.

1088
01:12:06.810 --> 01:12:11.810
So this is like completely like Tpu Allard and they have like a lot of the

1089
01:12:12.990 --> 01:12:17.850
compute to show in that.
And I guess I have the numbers here.

1090
01:12:18.480 --> 01:12:23.060
There's 180 years of play in one day.
This is how much copy would be.

1091
01:12:24.360 --> 01:12:26.970
Uh,
so that's fine.
Uh,

1092
01:12:27.190 --> 01:12:29.930
this another even funnier Vido Paul

1093
01:12:31.950 --> 01:12:35.420
competencies
again,
the same idea,
but it's fun.

1094
01:12:35.421 --> 01:12:39.760
It's the gate yet is that you put inside of that and do it.

1095
01:12:40.030 --> 01:12:42.620
I did slide boots on it and if they did each other,

1096
01:12:42.621 --> 01:12:47.621
they get to be one that the most interesting cloud is up there was applicant

1097
01:12:48.260 --> 01:12:51.980
that day.
The purpose is just to,
to pull the other one out.
Right.

1098
01:12:52.450 --> 01:12:56.130
But they understand some emergent behavior.

1099
01:12:56.270 --> 01:12:59.990
This is for us.
Makes Sense.

1100
01:12:59.991 --> 01:13:03.260
But for them to learn out of nothing.
It's kind of cool.

1101
01:13:15.090 --> 01:13:18.680
So there's like one risk here that when they're playing they say this guy's

1102
01:13:18.690 --> 01:13:23.460
trying to kick the ball inside the vine vine risk areas to overfit.

1103
01:13:27.040 --> 01:13:27.873
<v 1>Yeah,</v>

1104
01:13:28.690 --> 01:13:29.523
<v 4>that's also cool.</v>

1105
01:13:34.530 --> 01:13:38.380
Fun Technical Point.
Before I move on,
I think the whole point here,

1106
01:13:38.381 --> 01:13:42.280
is that here or is that not the next slide.
Okay.

1107
01:13:42.281 --> 01:13:47.281
He had that two to age playing with each other and we are updating the person

1108
01:13:48.221 --> 01:13:53.221
with the best other agent previously if you're doing self play is that you

1109
01:13:53.351 --> 01:13:55.840
overfeed to the actual agent that you're in front of you.

1110
01:13:56.050 --> 01:13:59.470
So the agent in front of you is powerful,
but you might over fit to this.

1111
01:13:59.471 --> 01:14:02.530
And if I put the agent that is not that powerful,

1112
01:14:02.531 --> 01:14:06.250
but it's using this simple trick that the polar from agent that can never users,

1113
01:14:06.400 --> 01:14:08.500
then you might just lose it again.
Right?

1114
01:14:08.830 --> 01:14:13.750
So once a week here to make it more is that rather than playing against only one

1115
01:14:13.751 --> 01:14:18.700
agent,
you alternate between different version of the agent itself.

1116
01:14:18.820 --> 01:14:23.050
So it all lands all the skills together.
It doesn't overfit to this stuff.

1117
01:14:24.320 --> 01:14:29.120
So there's another uh,
thing,
conduct Meta learning,

1118
01:14:29.580 --> 01:14:33.590
math or learning is a whole different algorithms again and uh,
I,

1119
01:14:33.591 --> 01:14:36.980
the purpose is that a lot of tasks are look similar to each other,
right?

1120
01:14:36.981 --> 01:14:38.610
Because apple watching two left,

1121
01:14:38.611 --> 01:14:40.920
I'm working to write and like working in different direction.

1122
01:14:40.921 --> 01:14:42.770
They're like same test essentially.

1123
01:14:43.220 --> 01:14:46.550
So the point is rather than training on a single task,

1124
01:14:46.551 --> 01:14:49.010
which is like go left or go right,

1125
01:14:49.190 --> 01:14:53.240
you train a distribution of tests that are similar to each other and then the

1126
01:14:53.241 --> 01:14:58.241
idea is that for each specific task I should learn with like a very few gradient

1127
01:14:59.001 --> 01:15:01.790
of steps.
So very few updates should be enough for me.

1128
01:15:02.240 --> 01:15:07.240
So if I learn play this year is like at the beginning of decision that has been

1129
01:15:07.760 --> 01:15:10.820
trained with metal learning before,
it doesn't know how to move,

1130
01:15:11.180 --> 01:15:14.630
but just look at the number of gradient of steps like after two or three games

1131
01:15:14.631 --> 01:15:17.210
and stuffs,
it totally knows how to move.
That's,

1132
01:15:17.330 --> 01:15:19.520
that's normally it takes a lot of steps to train.

1133
01:15:19.850 --> 01:15:22.880
But that's only because of the amateur learning approach that we've used here.

1134
01:15:23.330 --> 01:15:27.840
Metal learning is also cool.
I mean,
the algorithm is from Berkeley,
Josephine,

1135
01:15:27.870 --> 01:15:31.970
she's now also coming to Stanford is called model agnostic matter learning.

1136
01:15:34.280 --> 01:15:38.210
So all right,
and other point,
this a very interesting game.

1137
01:15:38.211 --> 01:15:43.100
Montezuma's revenge that
talk how much time they have.

1138
01:15:44.150 --> 01:15:48.480
Uh,
so you've seen uh,
exploration,

1139
01:15:48.481 --> 01:15:51.450
exploitation dilemma,
right?
So it's,
it's,

1140
01:15:51.690 --> 01:15:55.440
it's the bad if you don't exploit,
you're gonna fail many times.

1141
01:15:55.800 --> 01:15:59.760
So if you do the exploration with the scheme that you just saw that

1142
01:16:01.620 --> 01:16:06.480
this is a map of the game and you got to see it off that game.

1143
01:16:06.900 --> 01:16:08.790
If you like expiration randomly,

1144
01:16:09.270 --> 01:16:13.470
and my thing has like 21 or 20 something different,

1145
01:16:14.510 --> 01:16:18.680
there's hard to reach.
So there's this recent paper I think from Google brain,

1146
01:16:18.681 --> 01:16:20.180
for Mark [inaudible] team,

1147
01:16:20.600 --> 01:16:23.920
it's called a unifying the canvas methods for exploration.

1148
01:16:24.070 --> 01:16:26.330
Exploration is essentially very hard challenge,

1149
01:16:26.690 --> 01:16:31.400
mostly in the situation that the reward is a sparse for exactly in this game.

1150
01:16:31.480 --> 01:16:36.480
The first reward that you get is when you reach the key right from soft to here.

1151
01:16:37.431 --> 01:16:39.260
It's almost like 200 steps.

1152
01:16:39.590 --> 01:16:44.510
And getting the number of actions after 200 steps exactly right by like random

1153
01:16:44.511 --> 01:16:47.600
exploration is almost impossible.
So you never going to do that.

1154
01:16:48.380 --> 01:16:53.380
What a very interesting trick here is that you kind of keep a count on how many

1155
01:16:55.161 --> 01:17:00.161
times you've visited the states and then if you visit a state that is uh,

1156
01:17:02.150 --> 01:17:05.730
that has the fewer accounts,
then you give you a reward to the agent.

1157
01:17:05.731 --> 01:17:09.970
So we call it the intrinsic reward.
So it's kind of the,

1158
01:17:11.910 --> 01:17:12.743
<v 1>okay,</v>

1159
01:17:17.500 --> 01:17:20.580
so he changes it

1160
01:17:31.320 --> 01:17:35.850
<v 4>because he treats the cows of the statement and has never seen before.</v>

1161
01:17:36.150 --> 01:17:39.380
So this gets the agent I should explore.
Oh.

1162
01:17:39.381 --> 01:17:43.250
So he just goes down visit like different rooms and stuff like that.

1163
01:17:44.030 --> 01:17:46.220
So I think this,
this game is interested in,

1164
01:17:46.230 --> 01:17:51.230
you just started a lot of people that need to solve the game has a huge research

1165
01:17:52.650 --> 01:17:52.980
book.

1166
01:17:52.980 --> 01:17:57.320
I get the highest amount of this game and it's just fun also to see the agent

1167
01:17:57.321 --> 01:17:58.154
play.

1168
01:17:59.670 --> 01:18:02.270
<v 1>Any questions on that?</v>

1169
01:18:14.600 --> 01:18:18.060
<v 4>There is also another interesting</v>

1170
01:18:21.070 --> 01:18:26.050
point,
bad would be just fun to know about is called imitation learning.

1171
01:18:26.410 --> 01:18:30.070
Imitation learning is the case that,
well,
I mean oral agent saw,

1172
01:18:30.160 --> 01:18:32.150
sometimes you don't know the river.

1173
01:18:32.180 --> 01:18:35.350
Like for example in Atari Games they'll devolve is the cook very well defined,

1174
01:18:35.351 --> 01:18:38.830
right?
If I get the key,
I get the reward that is obvious.

1175
01:18:39.220 --> 01:18:42.370
But sometimes like defining the Revard,
it's hard.
For example,

1176
01:18:42.371 --> 01:18:47.030
when the car like the blue one one a drive in,
uh,
in some high value,

1177
01:18:47.230 --> 01:18:48.940
what is the definition of the river?
Right?

1178
01:18:49.450 --> 01:18:52.070
So we don't have a clear definition of that.
But on the other hand,

1179
01:18:52.071 --> 01:18:55.600
we had a person,
like you have human experts that can drive for us.

1180
01:18:55.900 --> 01:18:58.540
And then we see,
oh,
this is the right way of driving,
right?

1181
01:18:58.750 --> 01:19:02.330
So in this situation we have something called imitation learning that we tried

1182
01:19:02.331 --> 01:19:04.800
to mimic the behavior of experts.

1183
01:19:05.530 --> 01:19:10.020
So not exactly copying this because if we copy this and then you show us at

1184
01:19:10.120 --> 01:19:14.450
completely different states that we don't know what to do.
But from now we learn,

1185
01:19:14.470 --> 01:19:15.730
and this is like my example,

1186
01:19:15.731 --> 01:19:20.170
and there's a paper that called journey to the sordid limitation learning,

1187
01:19:20.171 --> 01:19:22.930
which was like from Stefanos group here at Stanford.

1188
01:19:22.990 --> 01:19:24.160
And that was also interesting.

1189
01:19:25.010 --> 01:19:29.860
<v 1>Well,
I think that's advanced topic.
If you have any questions,
I'm here.</v>

1190
01:19:29.890 --> 01:19:32.120
Can question.

1191
01:19:35.090 --> 01:19:35.923
No

1192
01:19:40.060 --> 01:19:41.850
<v 2>for next week.</v>

1193
01:19:41.851 --> 01:19:46.851
So there's no assignments you guys have to finish by models.

1194
01:19:47.470 --> 01:19:51.990
Now we're wanting to start the beat partners.

1195
01:19:52.620 --> 01:19:53.453
As you know,

1196
01:19:55.530 --> 01:19:59.390
there is going to be a project teenager

1197
01:20:02.640 --> 01:20:06.920
reading research papers.
Can we go over in the object detection?

1198
01:20:07.580 --> 01:20:10.230
It'll be two papers from Redlands.

