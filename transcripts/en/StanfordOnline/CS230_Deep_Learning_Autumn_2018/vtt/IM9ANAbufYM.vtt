WEBVTT

1
00:00:06.320 --> 00:00:10.970
Thanks for being here for lecture five of Cs two 30.
Um,

2
00:00:11.060 --> 00:00:13.680
today we have the chance to,

3
00:00:14.120 --> 00:00:18.380
to host a guest speaker of Pronto Rashford Park,
who's a Phd,

4
00:00:18.381 --> 00:00:20.690
students in computer science,

5
00:00:20.691 --> 00:00:24.350
advised by a professor Andrew Ng and prophecy or Percy Liang.

6
00:00:25.070 --> 00:00:29.150
So Pranav is,
uh,
is working on,
um,

7
00:00:29.740 --> 00:00:32.180
AI in high impact projects,
uh,

8
00:00:32.330 --> 00:00:35.930
specifically related to health care and natural language processing.

9
00:00:36.530 --> 00:00:41.450
And today he's going to two presents an overview of AI for healthcare and he's

10
00:00:41.451 --> 00:00:46.370
willing to dig into some projects.
He has led a true case studies.
So,

11
00:00:46.430 --> 00:00:47.990
uh,
don't hesitate to interrupt.

12
00:00:47.991 --> 00:00:52.610
I think we have a lot to learn from [inaudible] and he's really an industry

13
00:00:52.611 --> 00:00:57.590
expert for AI for healthcare.
Um,
and I let you the Mike print off.

14
00:00:57.800 --> 00:00:58.633
Thanks for being here.

15
00:00:58.820 --> 00:01:03.530
<v 1>Thanks Cam.
Thanks for inviting me.
Uh,
can you hear me at the back?
Is the mic on?</v>

16
00:01:04.040 --> 00:01:08.200
All right,
fantastic.
Well,
really glad to be here.
Um,

17
00:01:09.020 --> 00:01:11.720
so I want to cover three things today.

18
00:01:11.960 --> 00:01:16.700
The first is give you a sort of broad overview of what AI applications in

19
00:01:16.701 --> 00:01:17.780
healthcare look like.

20
00:01:18.470 --> 00:01:23.470
The second is bring you three case studies from the lab that I'm in,

21
00:01:24.710 --> 00:01:25.130
um,

22
00:01:25.130 --> 00:01:29.810
as demonstrations of AI in healthcare research and then finally,

23
00:01:30.170 --> 00:01:30.680
uh,

24
00:01:30.680 --> 00:01:35.540
some ways that you can get involved if you're interested in applying AI to high

25
00:01:35.541 --> 00:01:38.810
impact problems in healthcare or if you're from a healthcare background as well.

26
00:01:41.340 --> 00:01:42.510
Let's start with the first.

27
00:01:44.540 --> 00:01:45.080
<v 2>Okay,</v>

28
00:01:45.080 --> 00:01:48.580
<v 1>so one way we can decompose the kinds of things,
yeah,</v>

29
00:01:48.590 --> 00:01:53.590
I can do in healthcare is by trying to formulate levels of questions that we can

30
00:01:53.841 --> 00:01:58.841
ask from data at the lowest level are what are descriptive questions here we're

31
00:02:01.011 --> 00:02:02.780
really trying to get at what happened.

32
00:02:05.190 --> 00:02:09.090
Then there are diagnostic questions where we're asking why did it happen if a

33
00:02:09.091 --> 00:02:13.470
patient had chest pains,
I took their uh,
x-ray,

34
00:02:13.710 --> 00:02:17.670
what does that chest x ray show?
If they have palpitations,

35
00:02:17.850 --> 00:02:19.230
what is their ECG show?

36
00:02:20.640 --> 00:02:25.530
Then they're predictive problems here.
I care about asking about the future,

37
00:02:25.560 --> 00:02:30.560
what's going to happen in the next six months and then at the highest level are

38
00:02:30.601 --> 00:02:34.260
prescriptive problems.
Sure.
I'm really trying to ask,
okay,

39
00:02:34.470 --> 00:02:37.860
I know this is the patient.
This is the symptoms they're coming in with.

40
00:02:38.190 --> 00:02:42.150
This is how their trajectory will look like in terms of,

41
00:02:42.960 --> 00:02:47.340
um,
in terms of things that may happen that their risk off.

42
00:02:47.341 --> 00:02:51.930
What should I do?
And this is the real action point and that's,

43
00:02:52.020 --> 00:02:55.050
I would say the,
the gold mine.
But,
uh,

44
00:02:55.110 --> 00:02:59.710
to get there requires a lot of data and a lot of steps and we'll a little bit

45
00:02:59.711 --> 00:03:00.550
more about that.

46
00:03:03.610 --> 00:03:07.780
So in cs two 30,
you're all well aware

47
00:03:09.580 --> 00:03:12.430
of the paradigm shift of deep learning.

48
00:03:12.820 --> 00:03:17.350
And if we look at machine learning in healthcare literature,

49
00:03:17.680 --> 00:03:22.540
we see that has a very similar pattern is that we had this,

50
00:03:22.541 --> 00:03:26.830
uh,
feature extraction engineer who was responsible for,

51
00:03:27.340 --> 00:03:28.173
um,

52
00:03:28.630 --> 00:03:32.830
getting from the input to a set of features that a classifier can understand.

53
00:03:33.160 --> 00:03:37.570
And the deep learning paradigm is to combine feature extraction and the

54
00:03:37.571 --> 00:03:41.980
classification and to one step a by automatically extracting features,

55
00:03:42.070 --> 00:03:42.903
which is cool.

56
00:03:43.060 --> 00:03:48.060
Here's what I think will be the next paradigm shift for AI in healthcare,

57
00:03:48.850 --> 00:03:52.150
but also more generally is a,

58
00:03:52.360 --> 00:03:57.350
we still have a deep learning engineer up here.
Ah,
that's you,
that's me,
uh,

59
00:03:57.430 --> 00:04:01.120
that are designing the networks that are making decisions like a convolutional

60
00:04:01.121 --> 00:04:03.700
neural network is the best architecture for this problem.

61
00:04:03.880 --> 00:04:08.560
The specific type of architecture.
There's an Rnn,
CNN and whatever,

62
00:04:08.561 --> 00:04:09.910
and then you can throw on there.

63
00:04:10.390 --> 00:04:15.390
But what if we could just replace out the Mol engineer as well?

64
00:04:16.900 --> 00:04:20.170
Uh,
and I find this quite funny because everyone,
you know,

65
00:04:20.171 --> 00:04:22.930
in Ai for healthcare question that I get asked a lot is,

66
00:04:23.050 --> 00:04:26.050
are we going to replace doctors with all these AI solutions?

67
00:04:26.410 --> 00:04:31.410
And nobody actually realizes that we might replace machine learning engineers

68
00:04:31.781 --> 00:04:35.500
faster than me might replace doctors of this is to be the case.

69
00:04:35.830 --> 00:04:38.260
And a lot of research is,
uh,

70
00:04:38.290 --> 00:04:41.540
developing algorithms that can automatically learn architecture,

71
00:04:41.541 --> 00:04:45.700
some of which you might go through in this class.
Great.

72
00:04:46.120 --> 00:04:47.470
So that's the general overview.

73
00:04:47.770 --> 00:04:52.770
Now I want to talk about three case studies in the lab of Ai being applied to

74
00:04:53.770 --> 00:04:56.560
different problems.
And because healthcare is so broad,

75
00:04:56.710 --> 00:05:01.710
I thought I'd focus in on one narrow vertical and let us go deep on that.

76
00:05:02.890 --> 00:05:04.450
And that's medical imaging.

77
00:05:05.380 --> 00:05:10.380
So I've chosen three problems and one of them's a one B problem.

78
00:05:11.890 --> 00:05:16.090
The second is a two d problem is,
and the third is,
uh,
is he three d problems.

79
00:05:16.091 --> 00:05:16.930
So I thought we could,

80
00:05:17.020 --> 00:05:21.700
we could walk through all the different kinds of data here.
Uh,

81
00:05:22.000 --> 00:05:27.000
so this is some work that was done early last year in the lab where we showed

82
00:05:28.121 --> 00:05:31.450
that we were able to detect arrhythmias of the level of cardiologists.

83
00:05:32.890 --> 00:05:37.870
Um,
so arrhythmias are an important problem.
In fact,
millions of people,
uh,

84
00:05:37.960 --> 00:05:41.340
this is a specialty come to light recently with uh,
uh,

85
00:05:41.510 --> 00:05:45.880
devices like the Apple Watch,
which now have a ECG monitoring.

86
00:05:46.390 --> 00:05:48.670
Uh,
and uh,

87
00:05:48.940 --> 00:05:53.380
the thing about this is that sometimes you might have symptoms and know that you

88
00:05:53.381 --> 00:05:54.280
have arrhythmias,

89
00:05:54.281 --> 00:05:59.281
but other times you may not have and still have arrhythmias that can be

90
00:05:59.901 --> 00:06:03.770
addressed with,
uh,
with,
uh,
if,

91
00:06:03.800 --> 00:06:05.420
if you were to do an ECG.

92
00:06:06.140 --> 00:06:10.190
And the ECDS test is basically showing the heart's electrical activity over
time.

93
00:06:10.760 --> 00:06:15.760
The electrodes are attached the skin safe tests and it takes over a few minutes.

94
00:06:15.891 --> 00:06:18.590
And this is what it looks like when you're hooked up to all the different

95
00:06:18.591 --> 00:06:20.990
electrodes.
Uh,

96
00:06:21.170 --> 00:06:25.280
so this test is often done for a few minutes in the hospital.

97
00:06:26.030 --> 00:06:28.340
And the finding is basically that,
uh,

98
00:06:28.341 --> 00:06:33.080
in a few minutes you can't really capture a person's abnormal heart rhythms.

99
00:06:33.140 --> 00:06:38.120
So let's send them home for 24 to 48 hours with a holter monitor and let's see

100
00:06:38.121 --> 00:06:42.890
what we can find.
Um,
they're more recent devices such as the Zio Patch,

101
00:06:42.891 --> 00:06:47.630
which let,
um,
let patients be monitored for up to two weeks.

102
00:06:47.631 --> 00:06:49.910
And it's,
it's quite convenient.

103
00:06:49.911 --> 00:06:53.840
You can use it in the shower or while you're sleeping so you really can capture

104
00:06:54.200 --> 00:06:59.030
a lot of what what's happening in the hearts,
uh,

105
00:06:59.090 --> 00:07:00.110
ECG activity.

106
00:07:02.010 --> 00:07:02.570
<v 2>Okay.</v>

107
00:07:02.570 --> 00:07:05.600
<v 1>But if we look at the amount of data that's generated in two weeks,</v>

108
00:07:05.601 --> 00:07:08.810
it's 1.6 million heartbeats.
That's a lot.

109
00:07:08.900 --> 00:07:13.760
And there are very few doctors who would be willing to go through two weeks if

110
00:07:13.780 --> 00:07:16.490
ECG reading for each of their patients.

111
00:07:17.030 --> 00:07:20.720
And this really motivates why we need automated interpretation here.

112
00:07:22.400 --> 00:07:26.930
But automated detection comes with its challenges.
One of them is,

113
00:07:27.680 --> 00:07:31.070
you know,
you have in the hospital several electrodes.

114
00:07:31.220 --> 00:07:35.480
And in more recent devices we have just one.
Uh,

115
00:07:35.510 --> 00:07:39.070
and the way one can think of several electrodes is sort of the,

116
00:07:39.440 --> 00:07:43.530
the electrical activity of the heart is tweedy and um,

117
00:07:43.910 --> 00:07:47.420
each one of the electrodes is giving a different two d perspective into the

118
00:07:47.421 --> 00:07:52.220
Three d perspective.
Um,
but now that we have only one lead,

119
00:07:52.250 --> 00:07:56.150
we only have one of these perspectives available.
Uh,

120
00:07:56.151 --> 00:07:59.870
and the second one is that the differences between the heart rhythms are very

121
00:07:59.930 --> 00:08:03.110
subtle.
This is what a cardiac cycle looks like.

122
00:08:03.440 --> 00:08:05.720
And when we're looking at,
um,

123
00:08:06.410 --> 00:08:08.870
arrhythmias or abnormal heart rhythms,

124
00:08:09.200 --> 00:08:13.760
a one's going to look at the substructures within the cycle.

125
00:08:13.761 --> 00:08:15.230
And then,
uh,

126
00:08:15.290 --> 00:08:19.960
the s the structure between cycles as well and the differences are,

127
00:08:19.961 --> 00:08:21.170
are quite subtle.

128
00:08:25.250 --> 00:08:29.720
So when we started working on this problem,
Oh,
maybe I should share this story.

129
00:08:29.750 --> 00:08:34.120
So,
uh,
we started working on this problem and then it was,
uh,
me,
my,

130
00:08:34.140 --> 00:08:36.230
my collaborator on and,
uh,

131
00:08:36.290 --> 00:08:41.060
and Professor Ang and one of the things that he,
that he mentioned we should do,

132
00:08:41.210 --> 00:08:45.650
you said,
let's just go out and read ECG books and let's do the exercises.

133
00:08:45.651 --> 00:08:50.240
And if you're in med school,
they're,
these books were where you can,

134
00:08:50.590 --> 00:08:54.020
where you can learn about ECG interpretation and then there are several

135
00:08:54.021 --> 00:08:56.570
exercises that you can do to test yourself.
Uh,

136
00:08:56.600 --> 00:09:00.760
so I went to the med school library,
uh,
you know,
they have those,
uh,

137
00:09:01.260 --> 00:09:03.810
a hand crank,
a sheltered the bottom,

138
00:09:03.811 --> 00:09:05.940
so you have to move them and then grabbed my book.

139
00:09:06.270 --> 00:09:09.190
And then we went for two weeks and did,
uh,

140
00:09:09.960 --> 00:09:13.080
did go through two books and learn ECG interpretation.

141
00:09:13.080 --> 00:09:14.280
And it was pretty challenging.

142
00:09:15.840 --> 00:09:19.560
And if we looked at previous literature to this,

143
00:09:19.650 --> 00:09:24.650
I think they were sort of drawing upon some domain knowledge share in that here

144
00:09:25.021 --> 00:09:26.310
we're looking at waves,

145
00:09:26.580 --> 00:09:31.560
how can we extract specific features from waves that doctors are also looking

146
00:09:31.561 --> 00:09:32.190
at?

147
00:09:32.190 --> 00:09:35.580
So there was a lot of feature engineering going on and if you're familiar with

148
00:09:35.581 --> 00:09:38.490
wavelet transforms,
they were the sort of um,

149
00:09:39.120 --> 00:09:42.280
there were the most common approach,
uh,

150
00:09:42.330 --> 00:09:45.840
with a lot of sort of like different mother wave let's et Cetera,
et Cetera,

151
00:09:46.260 --> 00:09:48.420
preprocessing band,
pass filters.

152
00:09:48.600 --> 00:09:52.560
So everything you can imagine doing with signals was done and then you fed it

153
00:09:52.561 --> 00:09:57.480
into your SVM and you called it a day.
I would deep learning,

154
00:09:57.481 --> 00:09:58.740
we can change things up a bit.

155
00:09:59.090 --> 00:10:04.090
So on the left we have an ECG signal and on the right is just uh,

156
00:10:04.430 --> 00:10:07.500
three heart rhythms.
We're going to call them a,
B,
and c,

157
00:10:07.770 --> 00:10:11.140
and we're going to learn a mapping to go straight from the input to the APP.

158
00:10:11.141 --> 00:10:11.974
What,

159
00:10:13.720 --> 00:10:14.140
<v 2>okay.</v>

160
00:10:14.140 --> 00:10:15.700
<v 1>And here's how we're going to break it out.</v>

161
00:10:15.910 --> 00:10:20.910
We're going to say that every label labels the same amount of the signal.

162
00:10:22.570 --> 00:10:27.130
So if we had four labels and the ECG would be split into these four,

163
00:10:27.310 --> 00:10:30.130
sort of,
this rhythm is labeling this part

164
00:10:32.050 --> 00:10:34.510
and then we're going to use a deep neural network.

165
00:10:36.700 --> 00:10:39.970
So we built a one the convolutional neural network,

166
00:10:40.690 --> 00:10:44.140
which runs over the time dimension of the input.

167
00:10:44.141 --> 00:10:49.141
Cause remember we're getting one scalar over over time and then this

168
00:10:49.601 --> 00:10:52.620
architecture's 34 layers deep.
Um,

169
00:10:52.690 --> 00:10:55.660
so I thought I'd talk a little bit about the architecture.

170
00:10:57.100 --> 00:11:01.870
We've seen resonance before.
Okay.
Uh,
so

171
00:11:03.580 --> 00:11:08.250
should I go into this?
Okay,

172
00:11:08.560 --> 00:11:11.380
cool.
Here's my one minute spiel of resonant then,

173
00:11:11.860 --> 00:11:16.840
is that you're going deeper in terms of the number of players that you're having

174
00:11:16.841 --> 00:11:18.310
in a network.

175
00:11:18.760 --> 00:11:22.180
You should be able to re represent a larger set of functions.

176
00:11:22.990 --> 00:11:26.620
But when we look at the training error for these very deep networks,

177
00:11:26.770 --> 00:11:31.000
what we find is that it's worse than a smaller network.

178
00:11:31.090 --> 00:11:33.940
Now this is not the validation error.
This is the training air.

179
00:11:34.450 --> 00:11:38.830
That means even with the ability to represent a more complex function,

180
00:11:39.040 --> 00:11:42.160
we aren't able to represent the training data.

181
00:11:43.930 --> 00:11:48.760
So the motivating idea of residual networks is to say,
hey,

182
00:11:48.761 --> 00:11:53.761
let's add shortcuts within the network so as to minimize the from the error

183
00:11:54.281 --> 00:11:55.900
signal to each of my layers.

184
00:11:59.750 --> 00:12:01.730
Uh,
this is just math to say the same thing.

185
00:12:03.410 --> 00:12:06.920
So further work on rest net showed that,
okay,

186
00:12:06.921 --> 00:12:08.540
we have the shortcut connection,

187
00:12:08.660 --> 00:12:12.410
how should we make information flow through it the best?

188
00:12:13.250 --> 00:12:17.390
And the finding was basically that anything you,

189
00:12:17.660 --> 00:12:19.670
you add the shortcut to the highway,

190
00:12:19.770 --> 00:12:23.420
think of these as stop signs or um,

191
00:12:24.200 --> 00:12:26.450
or rs signals on a highway.

192
00:12:26.750 --> 00:12:29.960
And it's basically saying the fastest way on the highways to not have anything

193
00:12:29.961 --> 00:12:30.950
but addition on it.

194
00:12:35.020 --> 00:12:39.630
And then there were a few advancements on top of that,
um,

195
00:12:39.760 --> 00:12:44.590
like adding dropout and increasing the number of filters in the convolutional

196
00:12:44.591 --> 00:12:49.030
neural network,
um,
that we also added to this network.

197
00:12:49.990 --> 00:12:52.930
Okay.
So that's the convolutional neural network.

198
00:12:53.650 --> 00:12:54.910
Let's talk a little bit about data.

199
00:12:56.350 --> 00:13:01.350
So one thing that was cool about this project was that we got to partner up with

200
00:13:01.450 --> 00:13:03.050
a,
um,

201
00:13:03.790 --> 00:13:08.790
with a startup that manufacturers these hardware patches and we got data off of

202
00:13:09.611 --> 00:13:12.610
patients who are wearing these patches for up to two weeks.

203
00:13:14.020 --> 00:13:18.940
And this was from around 30,000 patients.
Um,

204
00:13:19.000 --> 00:13:23.890
and this is 600 times bigger than the largest dataset that that was out there

205
00:13:23.891 --> 00:13:28.180
before.
And for each of these ECG signals,

206
00:13:28.450 --> 00:13:33.450
what happens is that each of them is amputated by a clinical ECG expert who

207
00:13:34.541 --> 00:13:35.230
says,

208
00:13:35.230 --> 00:13:39.340
here's where we're the May starts and fierce where s so let's mark the whole ECG

209
00:13:39.341 --> 00:13:43.090
that way.
Obviously very time intensive,
but a good data source.

210
00:13:44.440 --> 00:13:48.050
And then we had a test set as well.
And here we used,
um,

211
00:13:48.150 --> 00:13:52.300
could we use a committee of cardiologists so they'd get together,

212
00:13:52.301 --> 00:13:56.560
sit in a room and decide,
okay,
we disagree on this specific point,

213
00:13:56.710 --> 00:13:57.550
let's try to,

214
00:13:57.580 --> 00:14:01.780
let's try to discuss which one of us is right or what this rhythm actually is.

215
00:14:01.781 --> 00:14:04.450
So they arrive at a ground truth after discussion.

216
00:14:05.890 --> 00:14:09.320
And then we can of course test cardiologists as well.
And the way we do this,

217
00:14:09.400 --> 00:14:11.290
we have them do it individually.

218
00:14:11.291 --> 00:14:13.540
So this is not the same set that did the ground truth.

219
00:14:13.750 --> 00:14:16.480
There's a different set of cardiologists coming in one at a time.

220
00:14:16.510 --> 00:14:19.030
You tell me what's going on here and we're going to test you.

221
00:14:21.670 --> 00:14:26.500
So when we compared the performance of our algorithm to cardiologists,

222
00:14:26.890 --> 00:14:27.210
uh,

223
00:14:27.210 --> 00:14:32.210
we found that we were able to surpass them on the f one metrics.

224
00:14:32.531 --> 00:14:34.120
So this is precision and recall.

225
00:14:35.720 --> 00:14:36.250
<v 2>Okay.</v>

226
00:14:36.250 --> 00:14:40.300
<v 1>And when we looked at where the mistakes were made,
uh,</v>

227
00:14:40.360 --> 00:14:42.870
we can see that sort of the,

228
00:14:42.940 --> 00:14:47.620
the biggest mistake was between distinguishing two rhythms,
which look very,

229
00:14:47.621 --> 00:14:52.580
very similar,
um,
but actually don't have a difference in,
um,
in,

230
00:14:53.000 --> 00:14:53.900
in treatment.

231
00:14:54.130 --> 00:14:58.670
Here is another case where the model is not making a mistake which the experts

232
00:14:58.671 --> 00:15:01.700
are making.
Um,
and turns out this is a costly mistake.

233
00:15:01.850 --> 00:15:04.690
This is saying a benign heart rhythm,
uh,

234
00:15:05.120 --> 00:15:09.020
or what experts thought was a benign heart rhythm was actually a pretty serious

235
00:15:09.021 --> 00:15:12.320
one.
Um,
so that's,

236
00:15:12.440 --> 00:15:17.420
that's one beauty of automation is that we are able to catch these,
um,

237
00:15:18.050 --> 00:15:21.480
catch these misdiagnosis.
Uh,

238
00:15:21.560 --> 00:15:24.310
here are three heart blocks,
uh,

239
00:15:24.680 --> 00:15:28.550
which are clinically relevant to catch on which the model outperformed the

240
00:15:28.551 --> 00:15:32.990
experts and on atrial fibrillation,
which is probably the most common,

241
00:15:32.991 --> 00:15:35.060
serious or Rhythmia,
the same pulse.

242
00:15:39.640 --> 00:15:43.490
So one of the things that's neat about this application,

243
00:15:43.520 --> 00:15:48.410
a lot of applications in healthcare is what automation,
uh,
with deep learning,

244
00:15:48.411 --> 00:15:53.120
machine learning enables,
is for us to be able to continuously monitor patients.

245
00:15:53.450 --> 00:15:56.120
And this is not something we've been able to do before.

246
00:15:56.150 --> 00:16:01.150
So a lot of even science of understanding how patients,
uh,

247
00:16:01.220 --> 00:16:02.950
risks factors,
uh,

248
00:16:03.140 --> 00:16:06.560
what they are or how they change hasn't been done before.

249
00:16:06.561 --> 00:16:11.450
And this is an exciting opportunity to be able to advance science as well.

250
00:16:13.310 --> 00:16:18.140
And the Apple Watch has recently,
uh,
release their,

251
00:16:18.200 --> 00:16:20.450
their ECG monitoring.
Um,

252
00:16:20.510 --> 00:16:25.510
and it'll be exciting to see what new things we can find out about the health of

253
00:16:25.641 --> 00:16:30.620
our hearts from,
uh,
from these inventions.
Okay.

254
00:16:30.650 --> 00:16:33.560
So that was our first gay.
Yep.
Question.

255
00:16:39.950 --> 00:16:43.060
<v 3>Awesome.
Yeah.
So repeat the question.</v>

256
00:16:43.210 --> 00:16:45.730
How couple was it to,

257
00:16:46.100 --> 00:16:49.440
<v 1>um,
to,
uh,
sort of,
um,</v>

258
00:16:50.010 --> 00:16:54.390
deal with data privacy and sort of keep patient information private?

259
00:16:54.570 --> 00:16:59.010
So in,
in this case,
we did not have,
we had completely de identified data,

260
00:16:59.190 --> 00:17:00.890
so it was just,
um,

261
00:17:01.200 --> 00:17:05.610
someone's ECG signal without any extra information about their,
um,

262
00:17:05.640 --> 00:17:08.550
their clinical records or anything like that.
So it's,
it's very,

263
00:17:08.551 --> 00:17:09.690
it's very de identified.

264
00:17:10.970 --> 00:17:12.540
<v 3>All right guys,
I guess went to ask that.
Um,</v>

265
00:17:13.130 --> 00:17:16.880
how did you get over that and were there problems in getting,

266
00:17:18.030 --> 00:17:20.150
um,
because um,
you know,

267
00:17:20.151 --> 00:17:24.420
there's a lot of concerns so you have to like get it signed off.

268
00:17:24.421 --> 00:17:25.430
Let's cut real authority.

269
00:17:25.431 --> 00:17:28.640
You are like what are the obstacles and getting that to be there?
Oh,
sure.

270
00:17:28.910 --> 00:17:32.120
And I think we can,
we can take this question offline as well,
but one of the

271
00:17:32.370 --> 00:17:35.920
<v 1>beauties of working at Stanford is that there's a lot of,
uh,</v>

272
00:17:36.130 --> 00:17:38.800
industry research collaborations and uh,

273
00:17:39.090 --> 00:17:43.680
we have great infrastructure to be able to work with that.
Uh,
so,

274
00:17:43.681 --> 00:17:48.120
which brings me onto my second case study.
Sorry.
Yeah,
go for it.

275
00:17:50.600 --> 00:17:55.590
<v 4>Tender four patients that has the mosque,</v>

276
00:17:56.960 --> 00:17:59.350
I feels like you,
the expert

277
00:18:01.740 --> 00:18:06.030
comparing with their us.
So how can exploit the clinicals there?

278
00:18:06.420 --> 00:18:07.253
That's a good question.

279
00:18:08.360 --> 00:18:09.050
<v 1>Repeat the question.</v>

280
00:18:09.050 --> 00:18:13.940
How did we define the gold standard when we have experts setting the gold

281
00:18:13.941 --> 00:18:16.880
standard?
Uh,
so here's how we did it.
So one,

282
00:18:16.910 --> 00:18:20.480
one way to come up with a gold standard is to say,
okay,

283
00:18:20.481 --> 00:18:24.740
if we looked at what a consensus would say,
what would they say?

284
00:18:24.741 --> 00:18:29.741
And so we got three cardiologists in a room to set the gold standard,

285
00:18:30.080 --> 00:18:32.720
and then to compare the performance of experts.

286
00:18:33.320 --> 00:18:37.380
These were individuals who are separate from those groups of cardiologists who

287
00:18:37.381 --> 00:18:42.200
sat in another room and said what they taught of the,
um,

288
00:18:42.320 --> 00:18:44.330
of the ECG signals.
So that way there's,

289
00:18:44.360 --> 00:18:48.620
there's some disagreement where the gold standards set by the committee.

290
00:18:54.080 --> 00:18:54.890
Great.

291
00:18:54.890 --> 00:18:59.890
So here we looked at how we can detect pneumonia off of chest x rays.

292
00:19:01.320 --> 00:19:05.870
Uh,
so pneumonia is an infection that affects millions in the,

293
00:19:06.080 --> 00:19:08.070
uh,
us.
Uh,

294
00:19:08.150 --> 00:19:12.760
it's big global burden is actually in,
um,
in kits.

295
00:19:13.420 --> 00:19:16.990
So that's where it's really useful to be able to,
uh,

296
00:19:17.060 --> 00:19:19.220
detect that automatically and well.

297
00:19:20.240 --> 00:19:24.590
So to detect pneumonia,
there's a chest x ray exam,
uh,

298
00:19:25.580 --> 00:19:29.780
and chest x rays are the most common,
uh,

299
00:19:29.840 --> 00:19:33.890
imaging procedure,
uh,
with 2 billion chest x rays done per year.

300
00:19:35.030 --> 00:19:38.960
And the way abnormalities are detected.
And chest x rays is,

301
00:19:38.961 --> 00:19:43.280
they present as areas of increased density.

302
00:19:43.840 --> 00:19:47.930
Uh,
so where things should appear dark fifth year brighter or vice versa.

303
00:19:49.460 --> 00:19:53.960
And here's what a characteristically pneumonia looks like,

304
00:19:53.961 --> 00:19:56.870
where it's like a fluffy cloud.
Uh,

305
00:19:56.930 --> 00:20:00.740
but this is an oversimplification of course because uh,

306
00:20:00.770 --> 00:20:02.930
pneumonia is one of the Alveoli fill up with puss,

307
00:20:02.931 --> 00:20:06.260
but the Alveoli can fill up with a lot of other things as well,

308
00:20:06.410 --> 00:20:10.910
which lead to very different interpretations and diagnoses for the patients and

309
00:20:10.911 --> 00:20:14.060
treatment for the patient.
So it's quite confusing,

310
00:20:14.090 --> 00:20:17.300
which is why radiologists trained for years to be able to do this.

311
00:20:18.810 --> 00:20:19.270
Um,

312
00:20:19.270 --> 00:20:24.260
the set up is we'll take an input image of someone's chest,

313
00:20:24.261 --> 00:20:27.830
x ray and output the binary label zero one,

314
00:20:28.010 --> 00:20:31.940
which indicates the presence or the absence of pneumonia.

315
00:20:33.080 --> 00:20:37.070
And here we use a d convolutional neural network,
which is pre,

316
00:20:37.090 --> 00:20:40.560
pre trained on image net.
Okay.

317
00:20:40.820 --> 00:20:45.320
So we looked at shortcut connections earlier and uh,

318
00:20:45.390 --> 00:20:47.840
Dan snippets had this um,

319
00:20:49.540 --> 00:20:53.290
idea to take short good connections to the extreme.
It says,

320
00:20:53.320 --> 00:20:58.270
what happens if we connect every layer to every other layer instead of just

321
00:20:58.271 --> 00:21:02.110
connecting sort of one,
instead of having just one shortcut,

322
00:21:02.140 --> 00:21:03.400
which is what resonate hat

323
00:21:05.680 --> 00:21:10.100
and a dense net beat the previous state of the art,
um,

324
00:21:10.420 --> 00:21:15.420
and has generally lower error and fewer parameters on the image net challenge.

325
00:21:17.530 --> 00:21:20.620
So that's what we used,
uh,
for the Dataset.

326
00:21:21.400 --> 00:21:23.580
When we started working on this project,
uh,

327
00:21:23.710 --> 00:21:27.700
which was around October of last year,

328
00:21:28.510 --> 00:21:29.020
uh,

329
00:21:29.020 --> 00:21:34.020
there was this large data set that was released by the NIH of hundred thousand

330
00:21:34.211 --> 00:21:35.110
chest x rays.

331
00:21:35.440 --> 00:21:39.730
And this was the largest public dataset at the time.

332
00:21:41.380 --> 00:21:46.030
And hear each extra is annotated with up to 14 different pathologies.

333
00:21:46.210 --> 00:21:51.210
And the way the sanitation works is there's an NLP system which reads a report

334
00:21:51.370 --> 00:21:56.170
and then outputs for each of several,
put apologies whether there is a mention,

335
00:21:56.350 --> 00:22:00.580
whether there is a negation,
like not pneumonia for instance.

336
00:22:01.080 --> 00:22:05.750
Um,
and then annotates accordingly.
And then for our test set,

337
00:22:06.030 --> 00:22:06.340
uh,

338
00:22:06.340 --> 00:22:11.080
we had four radiologists here at Stanford independently annotate and tell us

339
00:22:11.081 --> 00:22:13.960
what they thought was going on in those extras.

340
00:22:15.580 --> 00:22:20.580
So one of the questions that comes up often in medical imaging is we have,

341
00:22:22.720 --> 00:22:27.040
we have,
um,
a model,
we have several experts,

342
00:22:27.160 --> 00:22:30.730
but we don't really have a ground truth and we don't have a ground truth for

343
00:22:30.731 --> 00:22:31.600
several reasons.

344
00:22:31.600 --> 00:22:36.600
Sometimes one of them is just that it's difficult to tell whether someone has

345
00:22:36.671 --> 00:22:41.671
pneumonia or not without additional information like their clinical record or

346
00:22:43.720 --> 00:22:48.430
even once you gave them antibiotics,
antibiotics,
did they get treated?

347
00:22:49.390 --> 00:22:49.851
Uh,

348
00:22:49.851 --> 00:22:54.851
so really one way to evaluate whether a model is better than a radiologists are

349
00:22:57.281 --> 00:23:02.281
as well as doing as well as the radiologist is by saying do they agree with

350
00:23:02.470 --> 00:23:04.450
other experts similarly.

351
00:23:06.130 --> 00:23:09.460
So that's what we use here.
That's the idea.
We say,
okay,

352
00:23:09.490 --> 00:23:13.470
let's have one of the radiologists be the,

353
00:23:14.620 --> 00:23:15.260
the,

354
00:23:15.260 --> 00:23:20.260
the prediction model where evaluating and let's set another radiologist to be

355
00:23:20.981 --> 00:23:24.520
ground troop and now we're going to compute the f one score once,

356
00:23:25.570 --> 00:23:28.510
uh,
change the ground truth,
do it the second time,

357
00:23:28.600 --> 00:23:33.600
change it again third and then also use the model as the ground truth and do it

358
00:23:33.761 --> 00:23:37.090
again.
And we can use a very symmetric evaluation scheme.

359
00:23:37.840 --> 00:23:42.840
But this time having the model we evaluated against each of the four experts.

360
00:23:44.080 --> 00:23:46.700
So we do that and we get a score for both of them.

361
00:23:47.660 --> 00:23:49.910
Well for all of the experts and for the model.

362
00:23:50.450 --> 00:23:55.450
And we showed in our work that we were able to do better than the average

363
00:23:55.671 --> 00:23:57.470
radiologist at this task,

364
00:24:00.340 --> 00:24:01.160
um,

365
00:24:01.160 --> 00:24:05.900
to weigh six done this in the future is to be able to look at patient history as

366
00:24:05.901 --> 00:24:10.901
well and look at a lateral radiographs and be able to improve upon this

367
00:24:11.690 --> 00:24:14.210
diagnosis.
Um,

368
00:24:14.450 --> 00:24:17.430
at the time at which we released our work,
um,

369
00:24:17.750 --> 00:24:20.240
on all 14 pathologies,

370
00:24:20.360 --> 00:24:23.690
we were able to outperform the previous state of the art.

371
00:24:25.970 --> 00:24:30.870
Okay.
So model interpretation,
model interpretation.
Yes.
There's question.

372
00:24:35.240 --> 00:24:37.510
<v 3>So if you have pneumonia,</v>

373
00:24:37.570 --> 00:24:42.420
you present it to the soccer golf like fever,
Jakafi and your rib.

374
00:24:42.450 --> 00:24:46.270
Certain cops too much can't sleep.
That's not included in the model.

375
00:24:47.730 --> 00:24:51.700
If you go to a set and you're trying to determine does this person happened and

376
00:24:51.701 --> 00:24:54.730
what you are not,
that's one thing.
But you don't,

377
00:24:55.630 --> 00:24:58.990
not that you just don't have that data,
but you're not looking at other images.

378
00:24:58.991 --> 00:25:02.810
Let's say disaster stuff,
cancer,
other,

379
00:25:03.480 --> 00:25:07.500
because being Neil Cole as Testis,
it's all,

380
00:25:07.520 --> 00:25:12.200
those are images that you're not we look at,
so let's say,

381
00:25:12.201 --> 00:25:13.540
and there's going to tough situation.

382
00:25:13.660 --> 00:25:15.820
So the obvious situation is really doing a bunch of it right?

383
00:25:16.060 --> 00:25:19.810
But in a tough situation,
if you get a patient that has a fever,

384
00:25:19.870 --> 00:25:24.160
it's coffee [inaudible] don't know this cancer or pneumonia or black lung

385
00:25:24.161 --> 00:25:25.600
disease,
then how do you,

386
00:25:25.720 --> 00:25:28.780
how do you get your outcomes of the work in that condition?

387
00:25:28.781 --> 00:25:32.780
And also if you're not including all those other cases,
then it's not just that.

388
00:25:32.810 --> 00:25:37.670
What's the use of it with like,
you don't want to say
something like,
well,

389
00:25:37.720 --> 00:25:41.080
so I'm trying to keep it at this technical does a technical class,
what's the,

390
00:25:41.230 --> 00:25:45.120
is there a neural network architecture that you would use to be able to solve

391
00:25:45.130 --> 00:25:49.660
problem number one?
It's a multitask learning.
Is it like,
sure,
sure.

392
00:25:49.850 --> 00:25:52.370
Okay.
So let me try to boil those sort of sets of,
so

393
00:25:52.430 --> 00:25:56.200
<v 1>questions down.
So one is patients are coming in,</v>

394
00:25:56.201 --> 00:25:59.590
we're not getting access to their clinical history.

395
00:25:59.591 --> 00:26:02.110
So how are we able to make this determination?
At all.

396
00:26:02.440 --> 00:26:05.020
So one thing is that when we're training the algorithm,

397
00:26:05.140 --> 00:26:09.150
we're training the algorithm on,
on,
uh,

398
00:26:09.220 --> 00:26:12.790
pathologies extracted from radiology reports.

399
00:26:13.300 --> 00:26:18.250
And these radiology reports are written with understanding of full clinical

400
00:26:18.251 --> 00:26:21.970
history and understanding of,
uh,

401
00:26:22.060 --> 00:26:26.110
sort of what the patient presented with in terms of symptoms as well.

402
00:26:26.380 --> 00:26:29.500
So we're training the model on,

403
00:26:29.740 --> 00:26:33.610
on these radiology reports,
which had access to more information.

404
00:26:34.540 --> 00:26:39.540
And the second is that the utility of this is not as much in being able to

405
00:26:40.301 --> 00:26:45.301
compare a patient's X-ray's day to day as much as here is a new patient,

406
00:26:45.950 --> 00:26:50.910
uh,
with a set of symptoms.
And can we identify things from their chest x rays.

407
00:26:53.670 --> 00:26:54.503
<v 2>Okay.</v>

408
00:26:54.670 --> 00:26:56.470
<v 1>Which brings us to model interpretation.</v>

409
00:26:56.740 --> 00:27:00.910
So if you were a end user for model,

410
00:27:01.840 --> 00:27:03.940
um,
oh,
I,
I,

411
00:27:03.970 --> 00:27:08.710
so when I was back in Undergrad,
um,
and I was in the lab,

412
00:27:08.711 --> 00:27:13.360
we were working on autonomous cars.
Um,
and I thought about this a lot.

413
00:27:13.361 --> 00:27:15.970
How many of you have been in an autonomous car?

414
00:27:17.660 --> 00:27:21.680
How many of you would trust being in an autonomous car?

415
00:27:25.600 --> 00:27:29.290
All right,
cool.
Yeah,
I thought about this as well.

416
00:27:29.320 --> 00:27:31.300
Would I trust being in an Thomas Cart?

417
00:27:31.690 --> 00:27:36.690
And I thought it'd be pretty sweet if the algorithm that was that was in the car

418
00:27:37.690 --> 00:27:41.020
would tell me whatever decision it was going to make an advance.

419
00:27:41.740 --> 00:27:44.800
I know that's not possible at high speeds so that,
you know,

420
00:27:44.801 --> 00:27:48.250
just in case I disagreed with a particular decision,
uh,

421
00:27:48.251 --> 00:27:51.430
I could say no a board and uh,

422
00:27:51.730 --> 00:27:56.470
and a half the model sort of,
you know,
a remake its decision.

423
00:27:56.860 --> 00:27:59.650
And I think the same holds true in healthcare as well.

424
00:28:00.040 --> 00:28:03.520
The one advantage that happens in healthcare is rather than having to make

425
00:28:03.521 --> 00:28:06.850
decisions within seconds,
like in the case of the,
of his car,

426
00:28:07.090 --> 00:28:12.090
there is often a larger timeframe like minutes or hours that we have.

427
00:28:12.870 --> 00:28:15.190
And,
and here it's,

428
00:28:16.360 --> 00:28:20.380
it's useful to be able to inform the clinician that's treating the patient to

429
00:28:20.381 --> 00:28:24.980
say,
Hey,
here's what my model thought and why.
Um,

430
00:28:28.090 --> 00:28:31.030
so here's the technique we use for that class activation maps,

431
00:28:31.060 --> 00:28:35.350
which you may cover in another lecture.
Uh,
so I'll just,

432
00:28:35.470 --> 00:28:40.470
I'll just leave it out saying that there are ways of being able to look at what

433
00:28:40.871 --> 00:28:45.400
parts of the image are most evident of a particular pathology,

434
00:28:46.130 --> 00:28:48.730
uh,
to generate these,
these heat maps.

435
00:28:49.570 --> 00:28:54.280
So here's a heat map that's generated for a pneumonia.

436
00:28:54.281 --> 00:28:58.810
So this extra has pneumonia and I can and,
and,
um,
uh,

437
00:28:58.811 --> 00:29:03.190
and the algorithm and red is able to highlight the areas where it taught was

438
00:29:03.191 --> 00:29:08.170
most problematic for that.
Here's one in which a,

439
00:29:08.260 --> 00:29:11.800
it's able to do a collapsed right lung.

440
00:29:12.700 --> 00:29:17.080
Here's one in which able is able to find a small cancer.

441
00:29:18.450 --> 00:29:19.190
<v 2>Okay.</v>

442
00:29:19.190 --> 00:29:24.190
<v 1>And here are the goal is to be able to improve healthcare delivery where,</v>

443
00:29:25.320 --> 00:29:27.020
um,
in the develop world,

444
00:29:27.021 --> 00:29:31.640
one of the things that's useful for is to be able to prioritize the workflow,

445
00:29:31.670 --> 00:29:35.960
make sure the radiologists are getting to the patients most in need of care

446
00:29:36.170 --> 00:29:40.580
before one's who's x look more normal.
Uh,

447
00:29:40.581 --> 00:29:45.581
but the part which I'm quite excited about is to increase the access of medical

448
00:29:46.331 --> 00:29:48.570
imaging expertise globally.
Uh,

449
00:29:48.670 --> 00:29:52.930
where right now the World Health Organization estimates that about two thirds of

450
00:29:52.931 --> 00:29:55.900
the world's population does not have access to diagnostics.

451
00:29:57.560 --> 00:30:00.430
Um,
and so we thought,
hey,

452
00:30:00.431 --> 00:30:05.431
wouldn't it be cool if we just made an APP that was able to,

453
00:30:05.890 --> 00:30:10.460
uh,
allow users to upload images of their,
um,

454
00:30:11.440 --> 00:30:16.360
off rays and be able to give its diagnosis?
Uh,

455
00:30:16.420 --> 00:30:18.010
so this is still in the works,

456
00:30:18.550 --> 00:30:23.120
so I'll show you what we've got running locally.
And,
uh,

457
00:30:23.470 --> 00:30:28.470
so here I'm presented with a screen that asks me to upload and X-ray.

458
00:30:29.260 --> 00:30:30.093
<v 2>Okay.</v>

459
00:30:30.190 --> 00:30:34.300
<v 1>And so I have,
I have several x-ray's here.
Uh,</v>

460
00:30:34.360 --> 00:30:38.110
and I'm gonna pick the one that says,
uh,
cardiomegaly.

461
00:30:38.140 --> 00:30:41.050
So cardiomegaly refers to the enlargement of the heart.

462
00:30:43.160 --> 00:30:43.993
<v 2>Okay.</v>

463
00:30:44.060 --> 00:30:45.230
<v 1>So I uploaded it.</v>

464
00:30:45.260 --> 00:30:49.700
Now it's running the models running in the backend and within a couple of

465
00:30:49.700 --> 00:30:52.910
seconds it's output it,
it's diagnoses on the right.

466
00:30:53.630 --> 00:30:58.630
So you'll see the 14 pathologies that the model is trained on being listed and

467
00:30:59.781 --> 00:31:02.450
then next to them a bar.
Um,

468
00:31:02.660 --> 00:31:05.180
and at the top of this list is cardiomegaly,

469
00:31:05.420 --> 00:31:09.380
which is what this patient has the,

470
00:31:09.381 --> 00:31:11.210
the hardest sort of extending out.

471
00:31:12.470 --> 00:31:15.260
And if I hover on cardiomegaly,

472
00:31:15.290 --> 00:31:19.040
I can see that the probability is displayed on there.

473
00:31:20.210 --> 00:31:21.920
And now we talked about interpretation.

474
00:31:21.921 --> 00:31:25.280
How do I believe that this model is actually looking at the heart rather than

475
00:31:25.281 --> 00:31:29.510
looking at something else.
And so if I click on it,
um,

476
00:31:29.570 --> 00:31:31.760
I get the class activation map for this,

477
00:31:31.880 --> 00:31:36.180
which shows that indeed it is focused on the heart,
uh,

478
00:31:36.290 --> 00:31:40.460
to be able to,
um,
and,
and is looking at the right thing.

479
00:31:40.580 --> 00:31:44.180
So I guess you can say the Algorithms Heart's in the right place.

480
00:31:45.950 --> 00:31:48.740
Cool.
Uh,
but I thought,

481
00:31:49.130 --> 00:31:52.490
so this is an image that I got from the,

482
00:31:52.910 --> 00:31:55.250
the data set that we were using NIH,

483
00:31:55.580 --> 00:32:00.500
but it's pretty cool if an algorithm is able to generalize to populations
beyond.

484
00:32:00.800 --> 00:32:03.800
And so I thought what we'd do is we could just look up,

485
00:32:06.680 --> 00:32:10.730
look up an image of cardiomegaly,
uh,
and

486
00:32:12.230 --> 00:32:12.680
<v 2>yeah,</v>

487
00:32:12.680 --> 00:32:15.530
<v 1>download it and just see if our model is able to,</v>

488
00:32:18.890 --> 00:32:20.210
this one looks pretty large,

489
00:32:20.720 --> 00:32:21.590
<v 2>so does this,</v>

490
00:32:25.490 --> 00:32:29.960
<v 1>I don't want an annotated one.
All right,
that's good.
So we can</v>

491
00:32:33.800 --> 00:32:37.640
<v 2>do that,
save it desktop</v>

492
00:32:39.690 --> 00:32:41.460
<v 1>and now we can upload it here.</v>

493
00:32:48.550 --> 00:32:53.550
And it's already read on this thing and all the top is cardiomegaly once again.

494
00:32:55.150 --> 00:32:59.380
So it's able to generalize too.
And there's the highlight.

495
00:32:59.500 --> 00:33:04.210
So it's able to generalize to populations beyond just the ones it was trained
on.

496
00:33:05.380 --> 00:33:07.060
So I'm very excited by that.

497
00:33:08.930 --> 00:33:12.830
And what I got even more excited by is,
uh,

498
00:33:12.860 --> 00:33:16.370
we're thinking of deploying this out in,
um,

499
00:33:16.430 --> 00:33:21.070
out in different parts of the world.
And when we got an image,
uh,

500
00:33:21.170 --> 00:33:25.260
that showed how x rays are red in,
uh,

501
00:33:25.340 --> 00:33:29.320
this hospital that we're working with in Africa,
uh,

502
00:33:29.540 --> 00:33:30.620
this is what we saw.

503
00:33:31.580 --> 00:33:36.580
And so the idea that one could snap a picture and upload it seems and get a

504
00:33:37.191 --> 00:33:41.750
diagnosis seems very powerful.
Um,

505
00:33:41.910 --> 00:33:45.060
so the third case study I want to take you through is,
um,

506
00:33:45.480 --> 00:33:46.650
being able to look at Mri.

507
00:33:46.651 --> 00:33:51.570
So we've talked about one d one d setup where we had an ECG signal.

508
00:33:51.571 --> 00:33:53.880
We've talked about a two d setup with an x ray.

509
00:33:55.110 --> 00:34:00.110
How many of you are thinking of working on a three d problem for your project?

510
00:34:00.640 --> 00:34:03.900
Okay,
phew.
That's good.
Cool.

511
00:34:06.150 --> 00:34:06.800
<v 2>Yeah.</v>

512
00:34:06.800 --> 00:34:08.110
<v 1>See here we looked at Nia,</v>

513
00:34:08.111 --> 00:34:13.111
Mr. So Mris of the knee is the standard of care to valuate nitas orders and more

514
00:34:16.331 --> 00:34:20.560
MRI examinations are performed on the knee than any other part of the body.

515
00:34:21.190 --> 00:34:22.023
Um,

516
00:34:23.510 --> 00:34:27.050
and the question that we sought out to answer was,

517
00:34:27.051 --> 00:34:31.280
can we identify knee of normalities?
Um,

518
00:34:31.670 --> 00:34:36.200
two of the most common ones include an ACL tear and a municipal tear at the

519
00:34:36.201 --> 00:34:38.330
level of radiologists.

520
00:34:39.760 --> 00:34:40.570
<v 2>Okay.</v>

521
00:34:40.570 --> 00:34:42.040
<v 1>Now with the three d problem,</v>

522
00:34:42.310 --> 00:34:46.910
one thing that we have that we don't have in a two d setting is the ability to

523
00:34:47.020 --> 00:34:50.680
look at the same,
same thing from different angles.

524
00:34:51.010 --> 00:34:54.820
And so when radiologists do this diagnosis,
they look at three views,

525
00:34:55.090 --> 00:34:59.170
the sagittal coronal and the axial,
which are,

526
00:34:59.950 --> 00:35:02.240
which are three ways of,
uh,

527
00:35:02.350 --> 00:35:06.040
looking through the three d structure of the knee.

528
00:35:06.490 --> 00:35:11.240
And in an Mr you get different types of series,
uh,

529
00:35:11.290 --> 00:35:15.650
based on the magnetic fields.
And so this year,
uh,
three different,
uh,

530
00:35:15.700 --> 00:35:17.500
series that are,
that are used.

531
00:35:18.980 --> 00:35:19.340
<v 2>Okay.</v>

532
00:35:19.340 --> 00:35:24.340
<v 1>And what we're going to do is output for a particular need,</v>

533
00:35:24.801 --> 00:35:28.910
Mr Examination,
the probability that it's abnormal,

534
00:35:29.330 --> 00:35:33.800
the probability of an ACL tear and the probability of a meniscal tear.

535
00:35:34.160 --> 00:35:39.160
Important thing to recognize here is this is not a multi-class in that I could

536
00:35:39.271 --> 00:35:43.260
have both types of tears.
It's a multi labeled problem.

537
00:35:46.050 --> 00:35:47.840
So we're going to train a,

538
00:35:48.020 --> 00:35:53.020
a convolutional neural network for every view pathology pair.

539
00:35:54.750 --> 00:35:58.410
So that's nine convolutional networks.

540
00:35:58.530 --> 00:36:03.240
And then combine them together,
uh,
using a logistic regression.

541
00:36:05.250 --> 00:36:08.970
So here's what each convolutional neural network looks like.

542
00:36:09.390 --> 00:36:12.210
I have a bunch of slices within a view.

543
00:36:12.390 --> 00:36:16.320
I'm going to pass each of them to a feature extractor and I'm going to get an

544
00:36:16.321 --> 00:36:17.490
output probability.

545
00:36:19.500 --> 00:36:24.500
So we had 1,400 and Emr exams from,

546
00:36:25.110 --> 00:36:29.580
uh,
the Stanford Medical Center.
And uh,

547
00:36:29.610 --> 00:36:34.610
we tested on one 20 of them where the majority vote of three a subspecialty

548
00:36:36.361 --> 00:36:40.180
radiologists establish the,
the ground troop.

549
00:36:41.880 --> 00:36:45.600
And we found that we did pretty well on,

550
00:36:46.770 --> 00:36:51.770
on the three tasks and had the model be able to pick up the different

551
00:36:52.400 --> 00:36:55.590
abnormalities pretty well.
And one can extend these,

552
00:36:55.591 --> 00:36:59.760
these methods of interpreter interpretability,
uh,
to,

553
00:37:00.630 --> 00:37:04.770
uh,
to three d inputs as well.
So that's what we did here.

554
00:37:05.790 --> 00:37:09.360
Okay.
So I saw this,

555
00:37:09.690 --> 00:37:14.690
I saw this cartoon a few a few weeks ago and I thought it was pretty funny.

556
00:37:15.390 --> 00:37:18.970
Uh,
which is a lot of machine learning engineers think,
uh,

557
00:37:19.050 --> 00:37:21.390
that they don't need to externally validate,

558
00:37:21.391 --> 00:37:25.630
which is find out how my model works on,
uh,

559
00:37:25.920 --> 00:37:30.270
works on data.
That's not my,
where my original data set came from.

560
00:37:30.271 --> 00:37:34.460
So there's a,
there's a difference in,
in distributions,
uh,

561
00:37:34.500 --> 00:37:39.500
but it's really quite exciting when a model does generalize to two data sets

562
00:37:40.981 --> 00:37:42.330
that it's not seen before.

563
00:37:45.210 --> 00:37:47.490
And so we got this dataset that's,

564
00:37:47.491 --> 00:37:52.380
that's public from a hospital in Croatia and here's how it was different.

565
00:37:52.381 --> 00:37:53.640
So it was a different,

566
00:37:54.090 --> 00:37:58.890
it was a different kind of series of different magnetic properties,
uh,

567
00:37:58.950 --> 00:38:02.520
is a different scanner and it was a different institution in a different
country.

568
00:38:03.420 --> 00:38:04.620
And we asked,
okay,

569
00:38:04.650 --> 00:38:08.190
what happens when we run this model off the shelf that was trained on Stanford

570
00:38:08.191 --> 00:38:11.010
data,
but tested on that kind of data?

571
00:38:11.700 --> 00:38:16.500
And we found that it did relatively well without any training at all.

572
00:38:17.820 --> 00:38:20.070
But then when we trained on it,

573
00:38:20.490 --> 00:38:25.490
we found that we were able to outperform the previous lead best reported result

574
00:38:26.640 --> 00:38:27.480
on the Dataset.

575
00:38:28.710 --> 00:38:33.150
So there's still some work to be done in being able to generalize,

576
00:38:33.510 --> 00:38:34.020
um,

577
00:38:34.020 --> 00:38:39.020
sort of my here that was trained on my data to be able to work on data sets from

578
00:38:40.271 --> 00:38:42.820
different institutions,
different countries,
swell.

579
00:38:43.060 --> 00:38:47.350
But we're making some steps along that way remains a very open problem for

580
00:38:47.351 --> 00:38:48.184
taking.

581
00:38:54.810 --> 00:38:59.170
<v 5>Yeah.
So we did the best we could in terms of processing.
So we had,</v>

582
00:38:59.380 --> 00:39:04.250
so one of the preprocessing steps as important is being able to,
uh,

583
00:39:04.330 --> 00:39:06.250
get the mean of the,

584
00:39:06.251 --> 00:39:11.251
of the input data to be as close to the mean of the input data that you're

585
00:39:12.191 --> 00:39:14.020
trained on.
Uh,

586
00:39:14.220 --> 00:39:16.440
<v 1>so that was one preprocessing step we tried,</v>

587
00:39:16.680 --> 00:39:19.920
we were trying to minimize that to say,
out of the box,

588
00:39:19.950 --> 00:39:22.440
how would this work if we had never seen this data before?

589
00:39:22.620 --> 00:39:24.330
How would it work on that population?

590
00:39:27.600 --> 00:39:32.600
So one big topic in across a lot of applied fields is asking question,

591
00:39:35.910 --> 00:39:36.540
okay,

592
00:39:36.540 --> 00:39:41.540
we're talking about models working automatically autonomously.

593
00:39:42.060 --> 00:39:47.060
How would these models work in when working together with experts in different

594
00:39:50.221 --> 00:39:51.054
fields?

595
00:39:51.210 --> 00:39:56.210
And here we ask that questions about radiologists and about imaging models.

596
00:39:56.610 --> 00:40:01.610
Would it be possible to be able to boost the performance if the model and the

597
00:40:02.611 --> 00:40:03.900
radiologist's work together?

598
00:40:06.860 --> 00:40:08.390
And so that's really the setup.

599
00:40:08.800 --> 00:40:13.520
A radiologist with model does that better than the radiologist by themselves.

600
00:40:16.060 --> 00:40:17.200
And here's how we set it up.

601
00:40:17.230 --> 00:40:22.230
We said let's have experts read the same case twice separated by a certain set

602
00:40:25.241 --> 00:40:27.010
of weeks.
Um,

603
00:40:27.880 --> 00:40:31.780
and then see how they would perform on the same set of cases.

604
00:40:33.640 --> 00:40:34.270
<v 2>Yeah.</v>

605
00:40:34.270 --> 00:40:39.270
<v 1>And what we found that we were able to increase the performance generally with a</v>

606
00:40:39.611 --> 00:40:44.611
significant significant increase in specificity for ACL tears.

607
00:40:46.420 --> 00:40:50.450
That means if someone,
if a patient came in,
uh,

608
00:40:50.680 --> 00:40:55.300
without a,
without an ACL tear,

609
00:40:55.360 --> 00:40:58.240
I'd be able to,
uh,
find it better.

610
00:41:00.530 --> 00:41:02.420
So in the future,
yes.
Question,

611
00:41:05.270 --> 00:41:05.790
<v 5>radiology,</v>

612
00:41:05.790 --> 00:41:10.790
sergeants that the intended you want to kind of biases being informed actually

613
00:41:10.860 --> 00:41:14.360
looked at the commissions out.
Yeah.
So that's a good question.
And I,

614
00:41:14.480 --> 00:41:19.480
<v 1>and I think how so sort of automation bias captures a lot of this and that once</v>

615
00:41:22.611 --> 00:41:26.470
we have sort of models working with um,

616
00:41:27.290 --> 00:41:28.550
experts together,

617
00:41:28.790 --> 00:41:33.200
can we expect that the experts will sort of take it less seriously?

618
00:41:33.520 --> 00:41:34.010
Cause that's,

619
00:41:34.010 --> 00:41:37.850
that's a big concern and start relying on what the model says and says,

620
00:41:37.851 --> 00:41:40.160
I won't even look at this exam.

621
00:41:40.220 --> 00:41:43.350
I'm just going to trust with the model says blindly.
Um,

622
00:41:44.180 --> 00:41:47.240
that's absolutely possible in a very open area of research.

623
00:41:47.480 --> 00:41:51.080
Some of the ways that people have tried to address it is to say,

624
00:41:51.590 --> 00:41:53.420
you know what I'm going to do from time to time,

625
00:41:53.450 --> 00:41:58.450
I'm going to pass in an exam to the radiologist for which I'm going to flip the

626
00:41:58.461 --> 00:42:03.440
answer
and I'll know the right one.
And if they get that wrong,

627
00:42:03.470 --> 00:42:07.850
I'll alert them that you're relying too much on the model,
uh,
stop.

628
00:42:08.510 --> 00:42:08.631
Uh,

629
00:42:08.631 --> 00:42:12.800
but there are a lot of more sophisticated ways to go about addressing automated

630
00:42:12.801 --> 00:42:16.580
bias.
As far as I know,
it's a very open field of research,

631
00:42:16.581 --> 00:42:19.340
especially as we're getting into deep learning assistants.

632
00:42:21.620 --> 00:42:22.453
<v 2>Okay.</v>

633
00:42:22.470 --> 00:42:27.470
<v 1>And one utility of this is to say basically that the set of patients don't need</v>

634
00:42:28.261 --> 00:42:31.770
follow up,
let's not send them for unnecessary surgery.

635
00:42:33.120 --> 00:42:36.600
Great.
So I shared a three case studies from the lab.

636
00:42:37.170 --> 00:42:41.670
The final thing I want to do is to talk a little bit about how you can get

637
00:42:41.671 --> 00:42:45.840
involved if you're interested in applications of AI to help care.

638
00:42:47.540 --> 00:42:50.540
Uh,
so the first is,
uh,

639
00:42:51.960 --> 00:42:56.050
the ability for you to just get your hands dirty with,
uh,

640
00:42:56.090 --> 00:42:59.610
data sets and,
and be able to try out your own model.

641
00:42:59.880 --> 00:43:04.650
So we have a,
from our lab released,
uh,
the Mora Dataset,

642
00:43:04.680 --> 00:43:09.600
which is a large Dataset of uh,
uh,
bone x rays.

643
00:43:09.690 --> 00:43:12.540
And the task is to be able to tell if it's um,

644
00:43:12.840 --> 00:43:17.080
if the x rays are normal or not.
And they come from different,
uh,

645
00:43:17.760 --> 00:43:21.750
parts of the,
of the upper body.
Um,
and that's,

646
00:43:21.751 --> 00:43:24.810
that's what the data set x rays look like.

647
00:43:25.650 --> 00:43:30.650
And this is a pretty interesting setup because you have more than one view us

648
00:43:30.930 --> 00:43:34.410
more than one angle for the same body part for the same study,

649
00:43:34.411 --> 00:43:38.530
for the same patient.
And the goal is to be able to combine this well,
uh,

650
00:43:38.550 --> 00:43:43.020
into convolutional neural network and,
and be able to output the probability of,

651
00:43:43.021 --> 00:43:43.860
of normality.

652
00:43:44.220 --> 00:43:48.750
And one of the interesting things here for transfer learning as well is do you

653
00:43:48.751 --> 00:43:53.430
want to train the models differently per body part or do you want to train them?

654
00:43:53.960 --> 00:43:58.020
Train the same model for body parts are combined certain models.

655
00:43:58.860 --> 00:44:00.990
Uh,
so a lot of design decisions there,

656
00:44:01.500 --> 00:44:04.200
and this is what train some train models look like.

657
00:44:04.201 --> 00:44:09.201
This is a model baseline that we released that's able to identify a fracture

658
00:44:09.451 --> 00:44:12.570
here and a piece of hardware on the right.

659
00:44:15.370 --> 00:44:19.420
Um,
and you can download the Dataset,
offer a website.

660
00:44:19.430 --> 00:44:23.410
So if you Google Maura Dataset or go on our website,

661
00:44:23.411 --> 00:44:27.010
Stanford Mol Group Dot.
Get hub.io,
you should be able to find it.

662
00:44:29.050 --> 00:44:33.510
Uh,
the second way to get involved is through the Ai for healthcare bootcamp,

663
00:44:33.840 --> 00:44:38.250
which is a two quarter long program that our lab runs,

664
00:44:38.550 --> 00:44:42.270
uh,
which provides,
uh,
students coming out of,
uh,

665
00:44:42.360 --> 00:44:46.860
classes like two 30 and opportunity to get involved in research.

666
00:44:47.190 --> 00:44:48.890
And here's,
uh,

667
00:44:49.350 --> 00:44:54.000
students receive training from a PSE students in the lab and medical school

668
00:44:54.001 --> 00:44:58.770
faculty,
uh,
to work on structured projects over two quarters.

669
00:44:59.200 --> 00:45:03.330
Um,
and if you have a background in Ai,
which you do,

670
00:45:03.990 --> 00:45:05.430
then you're encouraged to apply.

671
00:45:06.300 --> 00:45:11.100
And we're working on a wide set of problems across radiology,
uh,
Ehr,

672
00:45:11.101 --> 00:45:13.020
public health and pathology right now.

673
00:45:14.840 --> 00:45:19.210
<v 2>Um,
this is what the lab looks like.
We have a lot of fun.</v>

674
00:45:22.190 --> 00:45:22.640
<v 1>Um,</v>

675
00:45:22.640 --> 00:45:27.500
and the applications for the bootcamp starting in the winter are now open.

676
00:45:27.501 --> 00:45:32.501
So the early application deadline is November 23rd and you can go on this link

677
00:45:33.380 --> 00:45:37.850
and um,
and,
and apply.
Uh,

678
00:45:37.970 --> 00:45:41.090
so that's my time.
Thank you so much for having me and thanks for having me.

679
00:45:41.100 --> 00:45:41.933
Kevin.

680
00:45:42.810 --> 00:45:47.810
<v 2>[inaudible]</v>

681
00:45:53.700 --> 00:45:56.650
did you want to take one or two questions?
So I'll take a couple of questions.

682
00:45:57.900 --> 00:46:01.950
<v 3>Oh,
you asked a question about privacy concerns for the leather.</v>

683
00:46:01.951 --> 00:46:02.970
I think his concerns.

684
00:46:03.590 --> 00:46:06.900
What about compensation for the medical experts that you're potentially putting

685
00:46:06.901 --> 00:46:10.640
out of business with a free tool like the one that you're,

686
00:46:10.760 --> 00:46:15.470
you're developing or you know,
and just in general because their,

687
00:46:15.480 --> 00:46:20.400
their knowledge is being used to train these models.
It's not free.
Yeah.

688
00:46:20.401 --> 00:46:21.260
So the question,

689
00:46:21.290 --> 00:46:24.710
<v 1>shit it was,
we're having these,
uh,</v>

690
00:46:25.040 --> 00:46:29.510
automated AI models trained with the knowledge of medical experts.

691
00:46:30.080 --> 00:46:30.913
Um,

692
00:46:31.340 --> 00:46:36.030
what are ways in which we're thinking of compensating these medical experts,
uh,

693
00:46:36.320 --> 00:46:41.240
right now or in the future when we have,
uh,
possibly automated models?

694
00:46:41.690 --> 00:46:42.523
Uh,

695
00:46:42.590 --> 00:46:46.970
I think a lot of people are thinking about these problems and working on them

696
00:46:47.540 --> 00:46:48.373
right now.

697
00:46:48.380 --> 00:46:53.380
There are a variety of approaches that people are thinking about in terms of

698
00:46:54.140 --> 00:46:58.970
economic incentives.
And there's a lot of fear about sort of,

699
00:46:59.360 --> 00:46:59.770
well,

700
00:46:59.770 --> 00:47:04.770
AI actually work with or augment experts in whatever field they're working on.

701
00:47:05.720 --> 00:47:09.890
I don't have a great a silver bullet for this.
Uh,

702
00:47:09.950 --> 00:47:12.350
but I know there's,
there's a lot of work going on in there.

703
00:47:14.620 --> 00:47:15.350
<v 2>No.</v>

704
00:47:15.350 --> 00:47:18.780
<v 3>Um,
when you're looking through Mris,</v>

705
00:47:18.781 --> 00:47:21.800
we show looking at four or five gap issues,

706
00:47:22.060 --> 00:47:26.100
like one of them is the most lazy.
Uh,

707
00:47:26.440 --> 00:47:30.100
it's possible that the human looking at it quite of something that was

708
00:47:30.370 --> 00:47:34.530
<v 5>not be looked at by the EA model at that time.
Yes.</v>

709
00:47:34.870 --> 00:47:38.290
So how do you address that?
Yeah,
that's a great question.
So the,

710
00:47:38.350 --> 00:47:41.290
just to repeat the question,
it's,
uh,
we have,

711
00:47:41.291 --> 00:47:45.260
we're looking at MRI exams and we're saying for these three pathologies,
Yep.

712
00:47:45.370 --> 00:47:48.100
<v 1>We're able to put the probabilities,</v>

713
00:47:48.310 --> 00:47:52.990
what happens if there's another pathology that we have it looked at.
Uh,

714
00:47:53.020 --> 00:47:57.520
so I have a couple of answers for that.
The first is that one,

715
00:47:57.521 --> 00:48:01.780
if the one of the categories here was simply to tell whether it was normal or

716
00:48:01.781 --> 00:48:02.614
abnormal.

717
00:48:02.860 --> 00:48:07.860
So the idea here is that the abnormality class will capture a lot of different

718
00:48:08.081 --> 00:48:12.280
pathologies there,
at least the ones seen at Stanford.
Uh,

719
00:48:12.310 --> 00:48:16.330
but it's often the case that we're building for one particular pathology.

720
00:48:16.600 --> 00:48:20.290
And then there's obviously a,
um,

721
00:48:20.410 --> 00:48:25.030
a burden on the model and the model developers to be able to convey,
hey,
look,

722
00:48:25.270 --> 00:48:27.970
our algorithm model only does this.

723
00:48:28.240 --> 00:48:31.660
And you really need to watch out for everything else that the model doesn't

724
00:48:31.661 --> 00:48:36.190
cover.
Maybe that's the,

725
00:48:36.520 --> 00:48:40.780
unless there's one more question.
No.
All right,
that's the last question.

726
00:48:40.781 --> 00:48:42.160
We'll take them.
Thank you once again.

727
00:48:42.640 --> 00:48:43.473
<v 4>Okay,</v>

728
00:48:47.550 --> 00:48:50.820
<v 0>well now you've got,
you've got the perspective.
Is the microphone working?
Yeah.</v>

729
00:48:51.420 --> 00:48:56.220
Now you've got the perspective of a knee,
I researcher working in healthcare.

730
00:48:56.290 --> 00:48:59.930
Now you are going to be the AI research,
a researcher working in healthcare.

731
00:49:00.120 --> 00:49:04.950
We're going to go over a case study and that is targeted at skin disease.

732
00:49:05.190 --> 00:49:08.550
So you know,
uh,
in order to detect skin disease,

733
00:49:08.580 --> 00:49:10.290
sometimes you take pictures,

734
00:49:10.320 --> 00:49:14.010
microscopic pictures of cells on your skin and then in the lies those pictures.

735
00:49:14.190 --> 00:49:16.590
So that's what we're going to talk about today.

736
00:49:17.220 --> 00:49:18.990
So let me talk about the problem statement.

737
00:49:20.130 --> 00:49:24.870
You're a deep yearning engineer and you've been chosen by your group of

738
00:49:24.900 --> 00:49:27.250
healthcare practitioners,
uh,

739
00:49:27.300 --> 00:49:32.010
to determine which parts of a microscopic image corresponds to a cell.

740
00:49:33.210 --> 00:49:37.800
Okay?
So here's how it looks like,
um,
on did the black and white,

741
00:49:37.830 --> 00:49:40.530
it's not black and white image.
It's a color image,
but it looks black and white.

742
00:49:40.530 --> 00:49:44.650
The input image is the,
the one that is closer to me.
Um,

743
00:49:44.730 --> 00:49:49.730
and the yellow one is the ground truth that has been labeled by a doctor.

744
00:49:51.660 --> 00:49:55.380
Let's see.
So what you're trying to do is two segments.

745
00:49:55.500 --> 00:49:56.790
The sales on this image,

746
00:49:56.820 --> 00:50:01.820
and we didn't talk about segmentation yet or a little bit segmentation is uh,

747
00:50:02.310 --> 00:50:05.370
is about producing a value,

748
00:50:05.460 --> 00:50:09.030
a class for each of the pixels on our image.
So in this case,

749
00:50:09.090 --> 00:50:14.090
each pixel would correspond to either no cell or cell zero or one.

750
00:50:15.360 --> 00:50:20.360
And once we output a matrix of Zeros and ones telling us which peaks those

751
00:50:21.091 --> 00:50:22.410
corresponded to a cell,

752
00:50:22.860 --> 00:50:27.860
we should get hopefully a mask like the yellow mask that I overlapped with the

753
00:50:28.670 --> 00:50:33.350
input image.
Does that make sense?
Yeah.
Third category.
That's the boundary.

754
00:50:34.110 --> 00:50:38.210
The colored image,
the yellow one.
You don't have the boundaries part of yourself.

755
00:50:38.240 --> 00:50:39.860
Yeah,
we'll talk about the boundary later,

756
00:50:40.370 --> 00:50:43.940
but right now I assume it's a binary segmentation.
So zero in one,

757
00:50:43.970 --> 00:50:48.920
no selling seller.
Okay.
So,
uh,

758
00:50:49.040 --> 00:50:54.040
is going to be very interactive and I think we're going to use mentee for

759
00:50:54.051 --> 00:50:57.650
several question and groups.
You guys into groups of three.

760
00:50:57.890 --> 00:51:02.570
So here are other examples of images that were segmented with a mask.

761
00:51:03.830 --> 00:51:08.830
Now doctors have collected 100,000 images coming from microscopes but d images

762
00:51:10.610 --> 00:51:13.940
come from treaty,
from microscopes.
There is a type A,

763
00:51:14.000 --> 00:51:19.000
type B and type C microscope and the data is split in between these tree as 50%

764
00:51:19.251 --> 00:51:23.980
for type a 25% for type B,
25% for type C.
Um,

765
00:51:24.530 --> 00:51:29.530
the first question I have for you is given that the doctors want to be able to

766
00:51:30.291 --> 00:51:34.100
use your algorithm on images from the microscope of type C,

767
00:51:34.700 --> 00:51:36.410
these microscopies,
the latest one,

768
00:51:36.411 --> 00:51:40.640
it's the one that is going to be used widely in the field and they want your

769
00:51:40.700 --> 00:51:41.840
network to work on this one.

770
00:51:41.841 --> 00:51:46.670
How would you split your data set into train Dev and test set as a question and

771
00:51:46.671 --> 00:51:51.671
please group in teams of two or three and discuss it for a minute on how you

772
00:51:52.611 --> 00:51:53.750
would split this Dataset.

773
00:52:03.890 --> 00:52:05.950
<v 4>Yeah,
thanks.</v>

774
00:53:03.540 --> 00:53:06.900
<v 0>You can start going on.
Mentee and write down your answers is what,</v>

775
00:53:27.270 --> 00:53:28.103
<v 4>okay,</v>

776
00:53:29.830 --> 00:53:34.410
<v 0>take 30 seconds to input your,
your insights on,
on mentee.</v>

777
00:53:34.450 --> 00:53:35.650
You can do one per team.

778
00:53:38.780 --> 00:53:39.613
<v 4>Okay.</v>

779
00:53:39.720 --> 00:53:42.540
<v 0>And we'll start going over some of the answers here.</v>

780
00:53:44.610 --> 00:53:45.443
<v 4>Okay.</v>

781
00:53:46.630 --> 00:53:51.250
<v 0>Dev tests,
sleeves split.
See Train on a plus B 20 chain train</v>

782
00:53:52.910 --> 00:53:57.610
2.5 in Dev and test
training.

783
00:53:58.110 --> 00:54:03.110
80 all a all be five KC DEF 10 Casey test 10 Casey 95 five where test and Dev is

784
00:54:07.391 --> 00:54:10.270
from population we care about.
I think these are good answers.

785
00:54:10.720 --> 00:54:12.430
I think there is no perfect answer to that.

786
00:54:12.431 --> 00:54:14.530
But two things to take into consideration.

787
00:54:14.980 --> 00:54:19.980
You have a lot of data so you probably want to split it into 95,

788
00:54:20.021 --> 00:54:24.490
five closer to that than to 60 20,
20 and most importantly,

789
00:54:24.700 --> 00:54:27.640
you want to have see images in the test.

790
00:54:27.670 --> 00:54:30.490
They haven't test set to have the same distribution among these two.

791
00:54:30.550 --> 00:54:33.550
That's what you've seen in the third course.
Uh,

792
00:54:33.580 --> 00:54:37.180
and we would prefer to have actually seen ages in the train set.

793
00:54:37.420 --> 00:54:39.580
You own your algorithm to a half since the images.

794
00:54:39.790 --> 00:54:41.310
So I would say a very good answer is,

795
00:54:41.311 --> 00:54:46.311
is this one 95 five where the five and five are exclusively from c and you also

796
00:54:47.621 --> 00:54:50.800
have seen images in the 90% of train images.

797
00:54:51.460 --> 00:54:55.940
Any other insights on that?
But he agrees.

798
00:54:56.200 --> 00:54:57.033
Yep.

799
00:55:01.630 --> 00:55:05.240
<v 3>Hidden features that will mess up.
Yeah.</v>

800
00:55:05.540 --> 00:55:07.430
<v 0>So there is much more thing with you to talk about here.</v>

801
00:55:07.490 --> 00:55:12.020
One is how do we know what's the distribution of microscope a images and

802
00:55:12.021 --> 00:55:15.470
microscopy images versus microscope.
See,
do they look like each other?

803
00:55:15.530 --> 00:55:18.830
If they do all good,
if they don't,
how can we,

804
00:55:19.310 --> 00:55:24.110
how can we make sure the model doesn't get bad hints from these two

805
00:55:24.111 --> 00:55:27.680
distributions?
Uh,
another thing is data,
data augmentation.

806
00:55:27.681 --> 00:55:31.940
We could augment this data set as well and try to get as much as see

807
00:55:31.941 --> 00:55:35.330
distribution images as possible.
We're going to talk about that.

808
00:55:37.020 --> 00:55:37.853
<v 4>Okay.</v>

809
00:55:39.170 --> 00:55:41.930
<v 0>Split has to roughly be 95,
560,</v>

810
00:55:41.931 --> 00:55:46.010
2020 distribution of Dev and test sets has to be the same continuations from CN

811
00:55:46.030 --> 00:55:48.590
there.
There should also be seeing much in the training set.

812
00:55:49.520 --> 00:55:52.560
Now Talk to you about data augmentation.
Uh,

813
00:55:52.640 --> 00:55:56.450
you think you can augment these data and if yes,

814
00:55:56.720 --> 00:56:00.110
give only three these things method you would use.

815
00:56:00.320 --> 00:56:04.460
If no xsplit explicate.
Explain why you cannot.

816
00:56:05.610 --> 00:56:09.050
You want to take 30 seconds to talk about it with your neighbors.

817
00:56:09.320 --> 00:56:10.153
<v 4>Yep.</v>

818
00:56:50.700 --> 00:56:55.700
[inaudible]

819
00:57:36.900 --> 00:57:41.450
<v 0>let's go over some of the answers.
So rotation,
zoom,</v>

820
00:57:41.510 --> 00:57:44.840
Blur.
I think looking at the images that we have from the sales,

821
00:57:44.841 --> 00:57:49.220
this might work very well.
Uh,
rotation,
zoom,
Blur,

822
00:57:49.400 --> 00:57:50.540
translation,

823
00:57:51.080 --> 00:57:55.430
a combination of those stretch symmetry.

824
00:57:55.940 --> 00:58:00.290
Like probably a lot of those work at one follow up question that I'll have is

825
00:58:00.320 --> 00:58:01.130
can you,

826
00:58:01.130 --> 00:58:06.130
can someone give an example of a task where data augmentation might hurt the

827
00:58:06.681 --> 00:58:08.630
model rather than helping it?

828
00:58:12.490 --> 00:58:13.323
<v 6>Yeah.</v>

829
00:58:16.060 --> 00:58:19.600
If you want to have over feet on the test set,
can you be more precise?

830
00:58:21.770 --> 00:58:24.490
You don't want to generalize.
Oh,

831
00:58:24.491 --> 00:58:29.410
you don't want your model to generalize it too much.
Okay.
Yeah,

832
00:58:29.411 --> 00:58:33.180
that there's some cases where you don't want the model to generalize too much,

833
00:58:33.181 --> 00:58:34.330
especially though doing quarter.

834
00:58:34.410 --> 00:58:39.410
But any other ideas like face detection would be like upside down or like either

835
00:58:41.141 --> 00:58:43.900
side.
I see.
So if you do face detection,

836
00:58:43.901 --> 00:58:46.930
you probably don't want the defense to be upside down.
Although we never know,

837
00:58:46.931 --> 00:58:51.790
depending on the youth.
But,
uh,
it's,

838
00:58:51.970 --> 00:58:53.800
it's not going to help much use.

839
00:58:53.810 --> 00:58:58.060
The camera is always like that and it's building humans that are not upside
down,

840
00:58:58.710 --> 00:59:00.310
but I don't think it's going to hurt the model.

841
00:59:00.750 --> 00:59:03.240
<v 0>It's probably going to not help the model I guess.</v>

842
00:59:06.140 --> 00:59:06.973
Yeah.

843
00:59:08.230 --> 00:59:10.570
<v 6>And if you stretch the image,</v>

844
00:59:12.840 --> 00:59:15.360
so they're their algorithms,
like maybe you know,
flow nets.

845
00:59:15.640 --> 00:59:20.290
It's an algorithm that's used for on videos to detect the speed of a car,

846
00:59:20.350 --> 00:59:23.600
let's say,
uh,
is you stretch you need is probably,

847
00:59:23.601 --> 00:59:25.370
you cannot detect the speed of the car anymore.

848
00:59:26.510 --> 00:59:27.530
<v 0>Any other examples?</v>

849
00:59:30.980 --> 00:59:31.813
<v 2>Yeah,</v>

850
00:59:35.420 --> 00:59:37.760
<v 6>character called mission I think is a good example.</v>

851
00:59:38.260 --> 00:59:41.950
So let's say you're trying to detect is,
and you do

852
00:59:42.370 --> 00:59:46.000
<v 0>three flip and you get that,
you know,
like you're,</v>

853
00:59:46.001 --> 00:59:48.580
you're labeling is be everything that was d and as the,

854
00:59:48.590 --> 00:59:52.130
everything that was B four,
nine and six.
It's the same story.

855
00:59:52.310 --> 00:59:56.060
So these data augmentations are actually hurting the model because you don't

856
00:59:56.061 --> 00:59:59.510
really label when you data when you're mature data.
Right.
Okay.

857
01:00:03.980 --> 01:00:07.700
Okay.
So yeah,
many augmentation methods are possible cropping,

858
01:00:07.701 --> 01:00:11.120
adding random noise,
um,
changing contrasts.

859
01:00:11.540 --> 01:00:16.280
I think they told my Chin is super important.
I remember a story of um,

860
01:00:16.880 --> 01:00:21.470
of a company that was working on a self driving cars and,
and also uh,

861
01:00:21.500 --> 01:00:23.210
virtual assistance in cars.

862
01:00:23.450 --> 01:00:27.440
You know what like these type of interaction you have with someone in your car

863
01:00:27.441 --> 01:00:31.430
or a virtual assistant and they notice that the speech recognition system was

864
01:00:31.431 --> 01:00:35.270
actually not working well when the car was going backwards.

865
01:00:36.440 --> 01:00:38.750
Like no idea why,

866
01:00:38.751 --> 01:00:42.230
like why is this doesn't seem related to the speech recognition system of the

867
01:00:42.231 --> 01:00:43.064
car.

868
01:00:43.070 --> 01:00:48.070
And they test it out and they looked and they figured out that people were

869
01:00:48.500 --> 01:00:51.260
putting their hands in the passenger seat looking back and talking to the

870
01:00:51.261 --> 01:00:54.500
virtual assistants.
And because of microphone was in the front,

871
01:00:54.501 --> 01:00:56.930
the voice was very different when you were talking to,
to,

872
01:00:56.931 --> 01:00:59.150
to do back of the car rather than the front of the car.

873
01:00:59.780 --> 01:01:03.320
And so they use data augmentation in order to augment their current data.

874
01:01:03.370 --> 01:01:07.580
They didn't have data on that type of,
of people talking to the back of the car.

875
01:01:07.790 --> 01:01:09.330
So by augmenting smarter,

876
01:01:09.331 --> 01:01:13.220
you can change the voices so that they look like they were used by someone who

877
01:01:13.221 --> 01:01:15.530
was talking to the back of the car and then solve the problem.

878
01:01:18.290 --> 01:01:22.760
Okay.
A small question.
Oh,
we can do it quickly.

879
01:01:22.761 --> 01:01:25.760
What is the mathematical relationship between annex and then why?
So remember,

880
01:01:25.761 --> 01:01:30.170
we have an RGB image
and we can,

881
01:01:30.200 --> 01:01:34.940
we can flatten it into a vector of size annex.
And the output is a mask of size.

882
01:01:34.941 --> 01:01:39.260
And why,
what's the relationship between Nx Nna?
Why someone wants to go for it?

883
01:01:45.970 --> 01:01:46.803
<v 2>Hmm.</v>

884
01:01:48.230 --> 01:01:51.050
<v 6>Very cool.
Very cool</v>

885
01:01:54.090 --> 01:01:55.490
with thinks they're not equal.
And why

886
01:02:03.730 --> 01:02:07.370
and why?
It would be three annex annex would be three.
And why?

887
01:02:07.940 --> 01:02:12.940
Because you have RGB images and for each RGB pixel you would have one hour to

888
01:02:13.700 --> 01:02:17.260
zero one.
Okay.
That was a question on one of the midterms.

889
01:02:17.700 --> 01:02:19.260
It was a complicated question.

890
01:02:20.130 --> 01:02:22.350
<v 0>Uh,
what's the last activation of your network</v>

891
01:02:24.940 --> 01:02:28.680
sigmoid you on?
Probably a with zero and one,
uh,

892
01:02:28.800 --> 01:02:33.430
Nsu had several classes.
So later on when we see we can also segment per disease,

893
01:02:33.490 --> 01:02:36.850
then you would have the softmax,
uh,
what loss functions should we use?

894
01:02:39.500 --> 01:02:42.080
Want to give it to you to go quickly because we don't have too much time.

895
01:02:42.590 --> 01:02:47.590
You're going to use a binary cross entropy loss over all the outputs and the

896
01:02:51.231 --> 01:02:54.770
entries of,
of the outputs of your network.
Does that make sense?

897
01:02:56.330 --> 01:02:59.450
So always think the thinking through the loss function is interesting.

898
01:03:02.530 --> 01:03:07.310
Okay.
So you have a first try in and you have coded your own neural network that

899
01:03:07.311 --> 01:03:08.180
you've left,

900
01:03:08.240 --> 01:03:13.160
you've named model and one and one and you've trained it for 1000 bucks.

901
01:03:13.790 --> 01:03:15.380
It doesn't end up performing well.

902
01:03:15.410 --> 01:03:19.880
So it looks like that you give it the input image to the model and get an output

903
01:03:19.940 --> 01:03:23.120
that is expected to be the following one,
but it's not.

904
01:03:23.780 --> 01:03:27.880
So one of your friends tells you about transfer learning and they,
they,

905
01:03:27.900 --> 01:03:32.900
they tell you about another label data set of 1 million microscope images that

906
01:03:33.681 --> 01:03:37.640
have been labeled for skin disease classification,

907
01:03:37.910 --> 01:03:42.040
which are very similar to those you want to work with from microscopes seat.

908
01:03:43.700 --> 01:03:48.110
So a model m two has already been trained by another research lab on this new

909
01:03:48.111 --> 01:03:51.200
data sets on a 10 class dcs classification.

910
01:03:52.040 --> 01:03:54.530
And so here is an example of input output of the model.

911
01:03:54.860 --> 01:03:58.430
You have an input to manage that probably looks very similar to the ones you're

912
01:03:58.431 --> 01:03:59.264
working on.

913
01:04:00.290 --> 01:04:04.730
The network has a certain number of layers and a soft Max last vacation at the

914
01:04:04.731 --> 01:04:07.910
end that gives you the probably distribution over the dcs that seems to

915
01:04:07.911 --> 01:04:11.870
correspond to this image.
So they're not doing segmentation anymore,
right?

916
01:04:12.440 --> 01:04:15.140
They're doing classification.
Okay.

917
01:04:15.141 --> 01:04:19.880
So the question here is going to be you want to perform transfer learning from

918
01:04:19.900 --> 01:04:24.740
[inaudible].
What are the hyper parameters that you will have to choose?

919
01:04:27.710 --> 01:04:31.340
It's more difficult than it looks like.
So think about it.

920
01:04:31.341 --> 01:04:34.580
Discuss with your neighbors for a minutes trying to figure out what are the

921
01:04:34.581 --> 01:04:37.190
hyper parameters involved in this transfer learning process.

922
01:04:58.260 --> 01:04:59.093
<v 4>Yeah,</v>

923
01:05:19.080 --> 01:05:19.913
yeah.

924
01:06:00.770 --> 01:06:02.990
<v 0>Okay,
thanks.
15 more seconds to wrap it up.</v>

925
01:06:12.520 --> 01:06:13.353
<v 4>Okay.</v>

926
01:06:14.870 --> 01:06:19.730
<v 0>Then see what you guys have learning rates Edis a hyper parameter.</v>

927
01:06:20.240 --> 01:06:21.620
I don't know if it's specific to the,

928
01:06:21.621 --> 01:06:25.370
to the trust for learning weights of the last layers.

929
01:06:25.520 --> 01:06:29.930
So I don't think that's a hyper parameter.
Weights are parameters.

930
01:06:31.100 --> 01:06:35.450
New cost function for additional output layers.
I think that's the hyper,

931
01:06:36.110 --> 01:06:38.420
the choice of the loss.
You might count it as high parameter.

932
01:06:38.421 --> 01:06:41.000
I don't think it's specifically related to transfer learning.

933
01:06:41.001 --> 01:06:44.240
You will have to train with the loss you've used on your model and one.

934
01:06:45.770 --> 01:06:46.550
<v 4>Okay.</v>

935
01:06:46.550 --> 01:06:50.330
<v 0>Number of new layers yet weights of the new,</v>

936
01:06:50.390 --> 01:06:51.650
not a hyper parameter.

937
01:06:56.160 --> 01:07:00.930
Okay.
Last one or two either layers of and too.
So do we train,
what do we find?

938
01:07:00.931 --> 01:07:05.460
Tune.
So a lot about layers actually.
Size of added layers.

939
01:07:07.300 --> 01:07:10.930
Not sure.
Okay,
let's,
let's,

940
01:07:10.931 --> 01:07:13.900
let's go over it together cause it seems that there's a lot of different answers

941
01:07:13.901 --> 01:07:18.440
here.
Um,
let me try to write it down here.

942
01:07:18.470 --> 01:07:20.810
So let's say we have,
we have the model m two

943
01:07:24.860 --> 01:07:27.740
is it big enough for the back?
We have the model m too.

944
01:07:27.800 --> 01:07:31.340
And so we give it to an input image.
Okay.
Input

945
01:07:33.020 --> 01:07:35.900
and the model and to gives us a probability distribution.

946
01:07:36.920 --> 01:07:41.840
Softmax so we have this soft Max here.
You,
you,

947
01:07:41.841 --> 01:07:45.230
you will agree that we probably don't need the softmax layer.
We don't want it.

948
01:07:45.231 --> 01:07:46.430
We want to do some segmentation.

949
01:07:46.431 --> 01:07:50.750
So one thing we have to choose is how much of these pretrade network,

950
01:07:50.751 --> 01:07:54.260
because it's a pretrained networking.
How much of these network do we keep?

951
01:07:55.010 --> 01:08:00.010
Let's say we keep these layers because they probably know the inheritance

952
01:08:01.370 --> 01:08:02.770
salient features of the Dataset,

953
01:08:02.840 --> 01:08:05.330
like the edges of the cells that were very interested in.

954
01:08:05.990 --> 01:08:09.080
So we take it so we have it here

955
01:08:10.790 --> 01:08:15.500
and you agreed at here we have the first hyper parameter that is l the number of

956
01:08:15.501 --> 01:08:19.520
layers from him too that we take.
Now,

957
01:08:20.060 --> 01:08:22.070
what other high performance do we have to choose?

958
01:08:25.750 --> 01:08:30.750
This is l we probably have to add a certain number of layers here in order to

959
01:08:31.271 --> 01:08:34.720
produce our segmentation.
So there's totally another hyper parameter

960
01:08:37.870 --> 01:08:41.840
which is n zero how many layers do I stuck on top of this one?

961
01:08:42.350 --> 01:08:44.510
And remember these layers are pretrained

962
01:08:46.510 --> 01:08:49.030
but these ones are randomly initialized.

963
01:08:56.430 --> 01:09:00.600
That makes sense.
So to hyper parameters,
anyone sees a third one?

964
01:09:07.340 --> 01:09:10.520
The third one comes when you decide to train this new network.

965
01:09:11.240 --> 01:09:15.360
You have the input image,
give it to the network.

966
01:09:16.140 --> 01:09:20.410
Did the outputs,
segmentation,
mask,
segmentation,
masculinity,

967
01:09:20.460 --> 01:09:21.420
seg mask.

968
01:09:23.100 --> 01:09:26.400
And what's your have to decide is how many of these layers will I freeze?

969
01:09:27.060 --> 01:09:29.040
How many of the pretrained layers I freeze?

970
01:09:30.360 --> 01:09:34.710
Probably you have a small data set you'd prefer keeping the seizures that are

971
01:09:34.711 --> 01:09:38.370
here freezing them and focusing on retraining the last few layers.

972
01:09:38.820 --> 01:09:40.260
So there is another high profile theater,

973
01:09:40.261 --> 01:09:44.070
which is how much of this will I freeze?
LF?

974
01:09:44.610 --> 01:09:47.970
What does it mean to freeze?
It means during training.
I don't train these layers.

975
01:09:48.480 --> 01:09:51.150
I assume that they've been seeing a lot of data already.

976
01:09:51.510 --> 01:09:56.490
They understand very well the edge is and a less complex features of the data.

977
01:09:56.730 --> 01:10:00.900
I'm going to use my knee,
my small data set to train the last layers.

978
01:10:01.380 --> 01:10:06.330
So three hyper-parameters,
l l zero and l f.

979
01:10:06.990 --> 01:10:11.520
Does that make sense?
Okay.
So this is for transfer learning.

980
01:10:11.850 --> 01:10:14.220
So it looks more complicated than the question.

981
01:10:14.460 --> 01:10:16.830
The question was more compact dictated and it looked like,

982
01:10:19.260 --> 01:10:20.420
okay,
let's wear on me.

983
01:10:22.260 --> 01:10:23.093
<v 4>Okay.</v>

984
01:10:23.140 --> 01:10:25.570
<v 0>Okay,
let's go over another question.</v>

985
01:10:30.790 --> 01:10:34.640
Okay.
So this,
we did it.
Now it's interesting because,
uh,

986
01:10:34.840 --> 01:10:38.860
here we have an input image and in the middle we have the output that the doctor

987
01:10:38.861 --> 01:10:39.694
would light.

988
01:10:40.360 --> 01:10:44.020
But under rights you have the output of your algorithm.

989
01:10:44.050 --> 01:10:47.110
So you see that there is a difference between what they want and what we're

990
01:10:47.111 --> 01:10:51.610
producing.
And it goes back to,
someone mentioned it earlier,

991
01:10:52.130 --> 01:10:53.230
there's a problem here.

992
01:10:53.350 --> 01:10:57.460
How would you think you can correct the model and or data set to satisfy the

993
01:10:57.461 --> 01:10:58.294
doctor's request?

994
01:10:58.510 --> 01:11:02.230
So these should we with this image is that they want to be able to separate the

995
01:11:02.231 --> 01:11:05.080
cells among them and they cannot do it based on your algorithm.

996
01:11:05.081 --> 01:11:08.710
It's still a little hard.
There is,
there's something to add.

997
01:11:09.370 --> 01:11:12.160
So can someone come up with the answer or do you want to explain it to the,

998
01:11:12.161 --> 01:11:15.420
you mentioned one of the,
so that we,
we can finish this like yeah,

999
01:11:16.350 --> 01:11:19.750
you want to add boundaries because now it looks like you could have like three

1000
01:11:19.751 --> 01:11:21.840
cells on the bottom left blurring in together.

1001
01:11:22.080 --> 01:11:24.780
And so if you asked her attic boundaries,

1002
01:11:24.781 --> 01:11:27.480
it makes the selling super well defined.
Good answer.

1003
01:11:27.481 --> 01:11:31.950
So one way is when you label your datasets origin nearly you labeled with Zeros

1004
01:11:31.951 --> 01:11:33.330
and ones for every pixel.

1005
01:11:33.960 --> 01:11:37.560
Now instead you will label with three classes,

1006
01:11:38.220 --> 01:11:42.870
zero one or boundary,
let's say zero,
one two,
four boundary.

1007
01:11:42.900 --> 01:11:46.890
Or even the best method I would say is that for each pixel,

1008
01:11:47.460 --> 01:11:50.190
for each input pixel,
the output will be

1009
01:11:51.780 --> 01:11:54.360
the corresponding,
okay,
this one is not good.

1010
01:11:55.980 --> 01:11:58.620
The corresponding label like this is a cell picture.

1011
01:11:59.490 --> 01:12:02.040
He off cell p of boundary

1012
01:12:05.760 --> 01:12:07.380
and Pete have no cell.

1013
01:12:09.210 --> 01:12:11.580
What you will do is that instead of having a sigmoid activation,

1014
01:12:11.581 --> 01:12:14.470
you will use a soft max activation.
Okay,

1015
01:12:16.080 --> 01:12:18.300
and the soft Max will be for a pixel.

1016
01:12:19.740 --> 01:12:23.370
One other way to do that,
if it still doesn't work,
doesn't work.

1017
01:12:23.400 --> 01:12:27.060
Even if you label the boundaries.
What is another way to do that?

1018
01:12:28.440 --> 01:12:32.100
You relabel your datasets by taking into account the boundaries.

1019
01:12:32.760 --> 01:12:34.110
The model still doesn't perform well.

1020
01:12:39.180 --> 01:12:41.250
I think it's all about the weighting of the loss function.

1021
01:12:41.370 --> 01:12:44.850
It's likely that the number of pixels that are boundaries are going to be fewer

1022
01:12:44.851 --> 01:12:47.220
than the number of pixels that ourselves or no cells.

1023
01:12:47.580 --> 01:12:52.140
So the network will be biased towards predicting sale or no sale.
Instead,

1024
01:12:52.141 --> 01:12:54.450
what you can do is when you computer lost function,

1025
01:12:54.870 --> 01:12:58.800
you're not functioning should have three terms.
One Binary Cross entropy,

1026
01:12:58.801 --> 01:12:59.970
let's say for no sell,

1027
01:13:01.410 --> 01:13:06.150
one for cell and one for boundary.

1028
01:13:09.240 --> 01:13:10.073
Okay?

1029
01:13:10.080 --> 01:13:15.080
And this is going to be summed over by equals one to n I

1030
01:13:17.470 --> 01:13:19.830
their whole output pixel values.

1031
01:13:20.160 --> 01:13:25.160
What you can do is to attribute acquisitions to each of those Alpha or Beta or

1032
01:13:25.711 --> 01:13:28.020
one and by tweaking this coefficient,

1033
01:13:28.050 --> 01:13:30.870
if you put a very high or very low number here and there,

1034
01:13:30.990 --> 01:13:33.420
it means you're telling your model to focus on the boundary.

1035
01:13:33.990 --> 01:13:37.410
You're telling them on model that if you miss the boundary,
it's a huge penalty.

1036
01:13:37.930 --> 01:13:40.140
We want you to train by figuring out all the boundaries.

1037
01:13:40.440 --> 01:13:45.180
That's another trick that you could use.
One question on that.
Yeah,

1038
01:13:50.890 --> 01:13:54.430
we'd question really will your Dataset,
this,

1039
01:13:54.720 --> 01:13:58.450
this last Friday section,
you'd be labeling bounding boxes,
you know,

1040
01:13:58.500 --> 01:13:59.550
for the Yolo Algorithm.

1041
01:13:59.850 --> 01:14:03.630
So the same tools are available for segmentation where you have an image and you

1042
01:14:03.631 --> 01:14:07.200
would draw the different lines in practice.

1043
01:14:07.710 --> 01:14:11.370
If the Ma is the tool that you were using the line used,

1044
01:14:11.380 --> 01:14:12.850
we'll just count as a sale.

1045
01:14:12.910 --> 01:14:16.530
Everything including the wound with everything inside what you drove.

1046
01:14:16.960 --> 01:14:21.130
Plus the boundary would count a cell and the rest has no cell is just a line of

1047
01:14:21.131 --> 01:14:24.880
code to make a difference.
The line you drew will count as boundary.

1048
01:14:25.150 --> 01:14:29.680
Everything inside will count as cell and everything outside would count as no

1049
01:14:29.681 --> 01:14:33.040
cell.
So it's the way you use your labeling tool.
That's all.

1050
01:14:40.090 --> 01:14:43.370
<v 6>I think it's not learnable.
Parametric,
it's more hyper parameters to choose.</v>

1051
01:14:44.120 --> 01:14:46.780
So you know the same way you to lambda for your regularization,

1052
01:14:46.960 --> 01:14:50.910
you would choose Alpha and Beta.
When do you make a distinction like that?

1053
01:14:51.010 --> 01:14:53.990
Attention back.
If you combine those two.

1054
01:14:54.460 --> 01:14:57.940
So this is not an attention mechanics and because he's just a training trick,

1055
01:14:58.210 --> 01:15:02.800
I would say you cannot know how much attention we tell you for each image,

1056
01:15:03.010 --> 01:15:05.650
how much the model is looking at this part versus that part.

1057
01:15:06.130 --> 01:15:08.530
This is not going to tell you that it's just a training thing.

1058
01:15:11.410 --> 01:15:14.950
What's the advantage to doing it this way as opposed to like object detection?

1059
01:15:15.010 --> 01:15:16.260
Like they're texting each.
So,

1060
01:15:17.710 --> 01:15:21.790
so the question is what's the advantage of doing segmentation rather than

1061
01:15:21.791 --> 01:15:24.230
detection?
Yeah.
So detection means means

1062
01:15:24.230 --> 01:15:27.830
<v 0>you want to output a bounding box.
If y'all put a bounding box,</v>

1063
01:15:27.890 --> 01:15:30.740
what you could do is I'll put the bounding box,
crop it out,

1064
01:15:30.920 --> 01:15:34.580
and then analyze the cell and try to find a contour of the set.

1065
01:15:34.970 --> 01:15:37.370
But if you want to separate the cells,

1066
01:15:38.960 --> 01:15:41.240
if you want to be very precise,
segmentation is going to work well.

1067
01:15:41.241 --> 01:15:43.790
If you want to be very fast,
bounding boxes would work better.

1068
01:15:44.150 --> 01:15:45.090
I think that's the general way.

1069
01:15:45.380 --> 01:15:49.400
Segmentation is not working as fast as the Ula algorithm works for object

1070
01:15:49.401 --> 01:15:51.920
detection.
Yeah,
I would say that,

1071
01:15:53.390 --> 01:15:56.390
but it's more much more precise.
Okay.

1072
01:15:58.250 --> 01:16:01.280
So when you find the data sets in order to label the boundaries on top of that,

1073
01:16:01.281 --> 01:16:04.580
you can change a lost function to give more weight to boundaries or penalize

1074
01:16:04.581 --> 01:16:08.100
false positives.
Okay.
Uh,

1075
01:16:08.120 --> 01:16:11.570
we have one more slide I think.
Uh,
so let's go over it.

1076
01:16:12.080 --> 01:16:13.250
So now the doctors,

1077
01:16:13.251 --> 01:16:17.960
they give you a new dataset that contains images similar to the previous ones.

1078
01:16:19.260 --> 01:16:20.060
Uh,

1079
01:16:20.060 --> 01:16:24.410
the differences that each image now is labeled with zero in one zero meaning

1080
01:16:24.650 --> 01:16:26.810
there are no cancer cells on that image.

1081
01:16:27.050 --> 01:16:30.880
And one means there is at least a cancer cell on this thing.

1082
01:16:31.370 --> 01:16:34.730
So we're not doing segmentation anymore.
It's a binary classification,
image,

1083
01:16:34.760 --> 01:16:37.610
cancer or no cancer.
Okay?

1084
01:16:38.300 --> 01:16:40.880
So you'll easily be the state of the art model because you're,

1085
01:16:41.030 --> 01:16:46.030
you're a very strong person in classification and you achieve 99% accuracy.

1086
01:16:47.570 --> 01:16:51.410
The doctors are super happy and they asked you to explain the network's

1087
01:16:51.411 --> 01:16:52.244
prediction.

1088
01:16:54.350 --> 01:16:56.720
So given an image classified as one,

1089
01:16:56.780 --> 01:17:01.170
how can you figure out based on which sell the model predicts one?
So print,

1090
01:17:01.171 --> 01:17:02.810
I've talked a little bit about that.

1091
01:17:02.990 --> 01:17:05.930
There are other methods that you should be able to figure out right now,

1092
01:17:06.440 --> 01:17:08.210
even if you don't know class activation maps.

1093
01:17:10.790 --> 01:17:11.623
So to sum it up,

1094
01:17:13.610 --> 01:17:16.730
we have a new image input image.

1095
01:17:17.450 --> 01:17:20.660
Put it in your new network.
That is a binary classifier.

1096
01:17:24.290 --> 01:17:25.940
And the network says one,

1097
01:17:26.990 --> 01:17:30.110
you want to figure out why the network stays one based on which pixels.

1098
01:17:30.530 --> 01:17:31.363
What'd you do?

1099
01:17:38.960 --> 01:17:43.690
Visualize the weights.
Uh,
what'd you visualize in the woods?

1100
01:17:45.970 --> 01:17:50.590
I think visualizing the waste,
uh,
is not related to the input.

1101
01:17:50.600 --> 01:17:52.550
The weights are not going to change based on the inputs.

1102
01:17:53.120 --> 01:17:56.510
So here you want to know why this inputs lead to one.

1103
01:17:56.690 --> 01:17:58.790
So it's not about the weights,

1104
01:18:01.360 --> 01:18:05.300
each pixel.
Good idea.
So you know,

1105
01:18:05.630 --> 01:18:09.650
after you get the one here,
this is why hats basically,
it's not exactly one,

1106
01:18:09.651 --> 01:18:11.510
let's say it's 0.7 probability.

1107
01:18:13.380 --> 01:18:17.370
What you going to remember is that this number derivative of y hat we'd be Spec

1108
01:18:17.371 --> 01:18:22.060
two x is what?
It's a matrix of shapes.

1109
01:18:22.061 --> 01:18:24.800
Same as x.
You know,
it's a matrix

1110
01:18:27.440 --> 01:18:32.440
and each entry of the Matrix is telling you how much moving these big sale

1111
01:18:33.300 --> 01:18:35.860
influences white hat.
You're great.

1112
01:18:36.340 --> 01:18:40.870
So the top left number here is telling you,
um,

1113
01:18:41.410 --> 01:18:45.850
how much x one is impacting y hat.
Is it or not?

1114
01:18:45.880 --> 01:18:49.420
Maybe it's nuts.
If you have a cad detector and the cat is here,

1115
01:18:49.780 --> 01:18:52.420
you can change the pixel.
It's never going to change anything.

1116
01:18:52.480 --> 01:18:55.690
So the value here is going to be very small,
closer to zero.

1117
01:18:56.230 --> 01:18:58.630
Let's assume the cancer cell is here,

1118
01:18:59.080 --> 01:19:03.300
you will see high number in this part of the matrix because these these pieces,

1119
01:19:03.301 --> 01:19:06.010
these are the peaks.
So let's,
if we move them,
it would change my hat.

1120
01:19:06.130 --> 01:19:09.640
Does it make sense?
So quick way to interpret your network.
It doesn't,

1121
01:19:09.700 --> 01:19:14.560
it's not too,
too good.
Like you're not going to have tremendous results,

1122
01:19:14.561 --> 01:19:18.970
but you should see these pixels have higher derivative values than the others.

1123
01:19:19.780 --> 01:19:20.710
Okay?
That's one way.

1124
01:19:21.070 --> 01:19:24.730
And then we will see in two weeks how to interpret neural networks,

1125
01:19:24.880 --> 01:19:28.000
visualizing the weights included and all the other methods.

1126
01:19:30.460 --> 01:19:35.020
Okay,
so gradients with respect to your model,
the techs cancer cells

1127
01:19:36.700 --> 01:19:39.760
from the test set images with 99% accuracy.

1128
01:19:40.030 --> 01:19:44.170
Why a doctor would on average perform 97% on the same task.

1129
01:19:44.530 --> 01:19:45.730
Is this possible or not?

1130
01:19:48.130 --> 01:19:52.520
Who thinks it's possible to have a network that I choose more accuracy on the

1131
01:19:52.521 --> 01:19:56.600
test set then the doctor.
Okay.
Can someone,
can someone say why

1132
01:20:03.700 --> 01:20:07.800
<v 2>you have an explanation?
Can you look at two</v>

1133
01:20:08.100 --> 01:20:12.170
<v 0>things?
Possibly it's going to get it.
Okay.</v>

1134
01:20:12.320 --> 01:20:15.150
The next one probably looks at complex things that doctor didn't say.

1135
01:20:15.420 --> 01:20:20.280
They didn't see it.
That's what you're saying.
Possibly.
I think there is a,
um,

1136
01:20:21.510 --> 01:20:26.180
a more rigorous explanation,
human errors and approximation for Basier,

1137
01:20:26.280 --> 01:20:29.070
but we don't actually know where it is.
So theoretically you can get better.

1138
01:20:30.040 --> 01:20:31.700
So here we're talking about base,

1139
01:20:31.701 --> 01:20:33.510
they're human level performance and all that stuff.

1140
01:20:33.511 --> 01:20:34.590
That's when you should see it.

1141
01:20:35.010 --> 01:20:37.630
So one thing is that there are many concepts that you will see in course street

1142
01:20:37.631 --> 01:20:39.690
at are actually implemented in industry,

1143
01:20:39.691 --> 01:20:42.230
but it's not because you know them that you're,

1144
01:20:42.231 --> 01:20:45.150
you're going to understand that it's time to use them.

1145
01:20:45.420 --> 01:20:48.410
And that's what we want you to get to.
Like now when I ask you these questions,

1146
01:20:48.420 --> 01:20:51.360
you have to talk,
think about baser human level accuracy and so on.

1147
01:20:51.720 --> 01:20:56.720
So the question that you should ask here is what was the data set labeled?

1148
01:20:57.990 --> 01:21:02.520
What were the labels coming from if the data set was labeled by individual

1149
01:21:02.521 --> 01:21:04.350
doctors?
I think that looks weird.

1150
01:21:04.890 --> 01:21:07.590
Like if it was labeled by individual doctors,

1151
01:21:07.591 --> 01:21:11.550
I think it's very weird that the model performs better on the test set.

1152
01:21:11.850 --> 01:21:16.850
Then what doctors have labor because simply because the labels are wrong 2% of

1153
01:21:17.281 --> 01:21:18.870
the time on average the labels are wrong.

1154
01:21:18.990 --> 01:21:22.170
So you're teaching wrong things to your model 3% of the time.

1155
01:21:22.950 --> 01:21:25.950
So it's surprising that it gets better.
It could happen but surprising.

1156
01:21:26.490 --> 01:21:31.050
But if every single image of the data set has been labeled by a group of doctors

1157
01:21:31.170 --> 01:21:32.400
as print,
I've talked about it,

1158
01:21:32.970 --> 01:21:37.500
then the average accuracy of these group of doctor is probably higher than one

1159
01:21:37.501 --> 01:21:38.310
doctor.

1160
01:21:38.310 --> 01:21:42.030
Maybe it's 99% in which case it makes sense that the model can be to one doctor.

1161
01:21:42.950 --> 01:21:46.050
Does it make sense?
So you have baser you're trying to approximate with,

1162
01:21:46.530 --> 01:21:50.430
we'd like the best area you can achieve.
So regrouping,

1163
01:21:50.431 --> 01:21:53.730
grouping a cluster of doctors probably better than one doctor.

1164
01:21:54.060 --> 01:21:57.330
This is your human level performance and then you should be able to beat one

1165
01:21:57.331 --> 01:22:01.890
doctor.
Fine.
Okay,

1166
01:22:03.060 --> 01:22:08.060
so you want to build the pipeline that goes from image taken by you,

1167
01:22:08.361 --> 01:22:11.700
the front of your car to steering direction

1168
01:22:14.220 --> 01:22:15.480
for autonomous driving.

1169
01:22:15.960 --> 01:22:20.960
What you could do is that you could send this image to a cord detector that

1170
01:22:21.061 --> 01:22:25.590
detects all the cars,
a pedestrian detector

1171
01:22:28.320 --> 01:22:30.780
that detects all the pedestrians

1172
01:22:32.730 --> 01:22:36.720
and then you can give it to a path planner.
Let's say

1173
01:22:38.940 --> 01:22:42.150
that plans the path and outputs the steering direction let's say.

1174
01:22:42.390 --> 01:22:46.680
So it's not end to end.
And to end would be I have an input image and I give it,

1175
01:22:47.280 --> 01:22:48.270
I want this output.

1176
01:22:49.380 --> 01:22:54.380
So a few other disadvantages of this is is something can go wrong anywhere in

1177
01:22:56.551 --> 01:22:57.910
the model,
you know,

1178
01:22:58.500 --> 01:23:02.180
how do you know which part of the model went wrong?

1179
01:23:03.630 --> 01:23:08.290
Can you tell me which parts I give you an image?
The same direction is wrong.

1180
01:23:08.680 --> 01:23:09.513
Why?

1181
01:23:18.900 --> 01:23:19.733
<v 2>Yep,</v>

1182
01:23:22.570 --> 01:23:24.430
<v 0>good idea.
Looking at the different components.</v>

1183
01:23:25.000 --> 01:23:29.920
So what you can do is look what happens here in there.
Look,

1184
01:23:29.940 --> 01:23:33.430
look what's happening here and there.
You think based on this image,

1185
01:23:33.490 --> 01:23:36.430
the car detector worked well or not,
you can check it out.

1186
01:23:37.710 --> 01:23:39.940
You think the pedestrian detector works well?
Not you can check it out.

1187
01:23:40.240 --> 01:23:43.030
If there is something wrong here,
it's probably one of these two items.

1188
01:23:43.390 --> 01:23:46.360
It doesn't mean this one is good,
it just means that these two items are wrong.

1189
01:23:47.290 --> 01:23:48.580
How do you check that?
This one is good.

1190
01:23:48.610 --> 01:23:53.610
You can label ground truth images and give them here as input to this one and

1191
01:23:54.011 --> 01:23:58.000
figure out is it's figuring out the steering direction or not.
If it is,

1192
01:23:58.001 --> 01:24:00.940
it seems that the past planner is working well.
If it is not,

1193
01:24:00.970 --> 01:24:01.960
it means there's a problem here.

1194
01:24:02.920 --> 01:24:07.920
Now what if every single components seem to work properly?

1195
01:24:07.960 --> 01:24:12.700
Like let's say these two are properly
but there is still a problem.

1196
01:24:14.650 --> 01:24:18.100
It might be because what you selected as a human was wrong.

1197
01:24:20.380 --> 01:24:21.790
The past Tanner cannot detect,

1198
01:24:21.791 --> 01:24:25.400
cannot get this team direction correct based on only the pedestrians and the car

1199
01:24:25.401 --> 01:24:29.020
detection and the car's probably need the stop signs and stuff like that as
well.

1200
01:24:29.320 --> 01:24:32.950
You know it so because you made hand engineering choices here,

1201
01:24:33.070 --> 01:24:37.990
your model might go wrong.
That's another thing.
And another advantage of,

1202
01:24:38.050 --> 01:24:38.883
of,
um,

1203
01:24:39.070 --> 01:24:44.070
of this type of pipeline is that data is probably easier to find out at m for

1204
01:24:44.681 --> 01:24:47.470
every algorithm rather than the four hole,
the whole end to end pipeline.

1205
01:24:47.860 --> 01:24:49.900
If you want to collect data for the entire pipeline,

1206
01:24:50.200 --> 01:24:54.520
you would need to take a car,
put a camera in the front,
uh,
like,

1207
01:24:55.780 --> 01:25:00.700
like build a kind of steering wheel angle detector that will measure your

1208
01:25:00.701 --> 01:25:04.030
ceiling will at every step while you're driving.

1209
01:25:04.480 --> 01:25:07.570
So you need to drive everywhere basically with a car that has this feature,

1210
01:25:08.050 --> 01:25:11.710
it's pretty hard.
You need a lot of data,
a lot of roads.

1211
01:25:12.100 --> 01:25:16.300
While this one you can collect data of images anywhere and labeled,

1212
01:25:16.390 --> 01:25:21.070
it's labeled the Palestinians on it.
You can detect cars by the same process.

1213
01:25:21.820 --> 01:25:24.340
Okay,
so these choices also,

1214
01:25:24.341 --> 01:25:28.930
depending on what data can you access easily or what data is harder to acquire.

1215
01:25:30.790 --> 01:25:34.110
Any questions on that?
Uh,

1216
01:25:34.150 --> 01:25:36.240
you are going to learn about convolutional neural networks.

1217
01:25:36.241 --> 01:25:39.220
Now we're going to get fun with a lot of imaging.
Uh,

1218
01:25:40.020 --> 01:25:43.080
you'll have a quiz into programming assignment for the first module,

1219
01:25:43.140 --> 01:25:46.950
second module,
same midterm next Friday,
not this one.

1220
01:25:47.580 --> 01:25:51.090
Everything up to c,
four and two will be included in the midterm.

1221
01:25:51.270 --> 01:25:54.250
So up to the videos you're watching this week,
uh,

1222
01:25:54.290 --> 01:25:58.800
includes ta sections and the next one and every in class lecture,

1223
01:25:59.070 --> 01:26:02.940
including next Wednesday's and destroy you.
You have a ta section.

1224
01:26:04.370 --> 01:26:08.780
<v 6>Any questions on that?
Okay.</v>

1225
01:26:09.050 --> 01:26:09.980
See you next week guys.

