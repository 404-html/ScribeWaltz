1
00:00:02,070 --> 00:00:06,720
A data science,
ai,

2
00:00:06,990 --> 00:00:07,823
machine learning,
or these are all things that are in the 

3
00:00:10,260 --> 00:00:11,093
middle year right now.
I think if they fundamentally point at 

4
00:00:12,661 --> 00:00:13,494
the same idea,
the same concept around my call it 

5
00:00:16,680 --> 00:00:17,513
machine intelligence,
where it's about how do you use 

6
00:00:20,520 --> 00:00:23,760
computers and the vast amount of data 
that's out there,

7
00:00:23,910 --> 00:00:24,743
that kind of big data and then leverage 
that to make more intelligent decisions 

8
00:00:28,860 --> 00:00:30,870
as an organization.
Um,

9
00:00:30,960 --> 00:00:33,180
as a government,
as a nonprofit,

10
00:00:33,510 --> 00:00:38,510
this really comes from a few major 
secular trends that are happening.

11
00:00:39,360 --> 00:00:42,840
One is the plummeting cost of 
computation,

12
00:00:43,280 --> 00:00:45,180
uh,
and the plumbing costs of storage.

13
00:00:45,390 --> 00:00:46,223
So now we have the capacity to store 
that data relatively cheaply and three 

14
00:00:50,800 --> 00:00:52,710
other process that data relatively 
cheaply.

15
00:00:53,400 --> 00:00:54,233
And then the other major trend is that 
everyone's walking around with 

16
00:00:57,370 --> 00:01:00,360
smartphones.
Everyone's interacting with the Internet

17
00:01:00,361 --> 00:01:01,194
for a large part portion of the day.
And so we're able to capture huge parts 

18
00:01:04,621 --> 00:01:05,454
of the human experience and digitize 
that information and store it in the 

19
00:01:08,641 --> 00:01:09,474
cloud.

20
00:01:09,720 --> 00:01:12,810
So when we have all these connected 
devices that are measuring us,

21
00:01:13,390 --> 00:01:15,690
we can actually understand a lot about 
human behavior.

22
00:01:16,110 --> 00:01:17,730
And that's actually really,
really fascinating.

23
00:01:18,190 --> 00:01:22,200
Um,
and we're from that we're able to create

24
00:01:22,320 --> 00:01:23,880
products,
services,

25
00:01:24,120 --> 00:01:24,953
uh,
that are so much more rich and so much 

26
00:01:27,661 --> 00:01:31,260
more personalized than we've been able 
to do before.

27
00:01:31,560 --> 00:01:35,820
And so if you think about maybe even the
simplest example might be something like

28
00:01:35,910 --> 00:01:37,050
a Netflix,
right?

29
00:01:37,051 --> 00:01:37,884
With recommendation engine that's able 
to serve up content in a very targeted 

30
00:01:42,541 --> 00:01:44,430
way.
So that's they give you,

31
00:01:44,490 --> 00:01:46,700
they show you out of there,
you know,

32
00:01:46,730 --> 00:01:47,563
library of probably millions of possible
videos where you watch the five to 10 

33
00:01:52,411 --> 00:01:54,420
that you're most likely to want to 
watch.

34
00:01:54,421 --> 00:01:55,254
And they can do this from what's called 
a lookalike analysis where they will 

35
00:01:58,531 --> 00:01:59,364
look at what other people who have 
watched a similar set of videos as you 

36
00:02:04,861 --> 00:02:07,710
have,
how have they've rated those videos,

37
00:02:07,740 --> 00:02:08,573
how much they've liked those videos,
and then see what other videos those 

38
00:02:11,341 --> 00:02:13,860
people of light that you haven't yet 
watched.

39
00:02:14,340 --> 00:02:17,040
And that's probably a good candidate for
a video that you should wash,

40
00:02:17,550 --> 00:02:20,190
right?
So that kind of look alike analysis.

41
00:02:20,191 --> 00:02:21,024
Or if you're a data scientist,
you'd probably call that a 

42
00:02:22,591 --> 00:02:23,424
recommendation engine.
That's actually a very powerful 

43
00:02:26,041 --> 00:02:27,390
technique,
uh,

44
00:02:27,450 --> 00:02:28,283
and it's sort of very fundamental to a 
business that has tens of millions of 

45
00:02:32,520 --> 00:02:34,260
videos and they know you're only going 
to watch one tonight,

46
00:02:34,750 --> 00:02:36,780
right?
How do you pick out that one good video?

47
00:02:36,781 --> 00:02:39,870
So that's not just a huge search problem
is for a consumer,

48
00:02:39,930 --> 00:02:43,080
but it's actually a pleasurable 
experience for them and that's,

49
00:02:43,440 --> 00:02:46,020
you know,
has implications beyond Netflix,

50
00:02:46,021 --> 00:02:49,260
right?
If you think about a company like Amazon

51
00:02:49,590 --> 00:02:52,220
that's incredibly important for them,
uh,

52
00:02:52,440 --> 00:02:55,410
they have billions of items on their 
store.

53
00:02:56,130 --> 00:02:56,963
You need to be able to figure out what 
to buy and so they can tell you the 

54
00:03:01,571 --> 00:03:02,404
right item.
They can maybe get you to buy something 

55
00:03:04,300 --> 00:03:05,133
that you otherwise would have purchased 
and that's know has a direct impact on 

56
00:03:09,311 --> 00:03:10,144
their bottom line.

57
00:03:10,180 --> 00:03:11,710
And it also makes consumers happier,
right?

58
00:03:11,711 --> 00:03:12,544
It helps you reduce the amount of time 
you spend searching for products and 

59
00:03:15,341 --> 00:03:16,174
services.
So I think these kind of data enabled 

60
00:03:18,970 --> 00:03:23,170
services where companies can give you 
what you want when you want it,

61
00:03:23,500 --> 00:03:24,333
that's becoming increasingly powerful 
within the kind of consumer markets and 

62
00:03:31,031 --> 00:03:36,031
it's becoming increasingly the standard.
So I think what we've seen is that for a

63
00:03:36,311 --> 00:03:38,520
lot of legacy enterprises,
um,

64
00:03:39,160 --> 00:03:39,993
that are not digital first,
that haven't been able to embrace data 

65
00:03:42,881 --> 00:03:45,490
and data science,
there's a,

66
00:03:45,940 --> 00:03:46,773
almost a kind of an adversarial 
relationship between the consumer and 

67
00:03:49,481 --> 00:03:52,990
that product or service where you're 
saying as a consumer,

68
00:03:53,260 --> 00:03:54,093
hey,
I have this great experience when I'm 

69
00:03:56,260 --> 00:04:00,010
interacting with facebook or google or 
Netflix,

70
00:04:00,011 --> 00:04:03,640
they seem to give me what I want.
Why can't you give me what I want?

71
00:04:04,000 --> 00:04:07,570
And when you experienced that,
right,

72
00:04:08,200 --> 00:04:10,840
there's a lot of fear on the part of 
those companies,

73
00:04:10,841 --> 00:04:15,841
the legacy companies that their ability 
to sort of maintain that market is going

74
00:04:16,471 --> 00:04:17,304
to rapidly evaporates and it's going to 
be deteriorate rates at a really quick 

75
00:04:21,851 --> 00:04:24,700
rate if any of these sort of digital 
companies,

76
00:04:24,960 --> 00:04:25,793
uh,
enter that market because they 

77
00:04:26,951 --> 00:04:28,300
understand how to leverage data.

78
00:04:28,360 --> 00:04:31,070
They understand how to use that,
uh,

79
00:04:31,140 --> 00:04:34,540
the information they have or consumers 
to give consumers a fundamentally better

80
00:04:34,750 --> 00:04:35,583
product.
There's obviously a lot in the news 

81
00:04:38,021 --> 00:04:40,550
about Cambridge Analytica,
um,

82
00:04:40,960 --> 00:04:45,960
facebook and how we societaly deal with 
data and how we are,

83
00:04:47,330 --> 00:04:49,750
how we should deal with data.
Should there be regulation?

84
00:04:49,751 --> 00:04:52,100
Should we have more privacy protections?
Um,

85
00:04:52,510 --> 00:04:55,510
and I think these are really interesting
and valid questions.

86
00:04:55,780 --> 00:04:56,613
Uh,
we,

87
00:04:56,680 --> 00:04:59,200
we as a society should ask,
right?

88
00:05:00,190 --> 00:05:01,023
And it seems that's where entering a 
period where there's certainly within 

89
00:05:04,811 --> 00:05:05,644
Europe with the passenger Gdpr,
there is increasing reluctance to just 

90
00:05:08,951 --> 00:05:11,980
say a facebook,
Google,

91
00:05:12,070 --> 00:05:15,940
you can have free reign over all of our 
data in an unchecked way.

92
00:05:16,210 --> 00:05:19,420
And I think we'll see what happens as we
move forward,

93
00:05:19,900 --> 00:05:20,733
uh,
with a regulation and this kind of 

94
00:05:24,550 --> 00:05:25,870
consumer protections.

