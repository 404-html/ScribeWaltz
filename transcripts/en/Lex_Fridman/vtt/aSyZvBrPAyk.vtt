WEBVTT

1
00:00:00.030 --> 00:00:02.520
The following is a conversation with Tomasa Pojo.

2
00:00:02.970 --> 00:00:06.870
He's a professor at Mit and as a director of the center for brains,

3
00:00:06.871 --> 00:00:11.010
minds and machines cited over 100,000 times.

4
00:00:11.190 --> 00:00:15.300
His work has had a profound impact on our understanding of the nature of

5
00:00:15.301 --> 00:00:19.440
intelligence in both biological and artificial neural networks.

6
00:00:19.890 --> 00:00:24.720
He has been an advisor to many highly impactful researchers and entrepreneurs

7
00:00:24.721 --> 00:00:29.640
and AI,
including Dennis,
has hobbyists of deep mind,
Amnon,
Shaw,
Schwab,
Mobileye,

8
00:00:29.880 --> 00:00:33.210
and Christof Koch of the Allen Institute for Brain Science.

9
00:00:34.080 --> 00:00:38.040
This conversation is part of the MIT course on artificial general intelligence

10
00:00:38.100 --> 00:00:42.720
and artificial intelligence podcasts.
If you enjoy,
subscribe on Youtube,
iTunes,

11
00:00:42.750 --> 00:00:47.550
or simply connect with me on Twitter at Lex Friedman,
spelled f,
r I.
D.

12
00:00:47.970 --> 00:00:50.880
And now here's my conversation with to muscle

13
00:00:51.180 --> 00:00:52.013
<v 1>[inaudible].</v>

14
00:00:52.490 --> 00:00:56.300
<v 0>You've mentioned that in your childhood you've developed a fascination with</v>

15
00:00:56.301 --> 00:00:58.790
physics,
especially the theory of relativity,

16
00:00:59.690 --> 00:01:03.440
and that Einstein was also a childhood hero.
To you,

17
00:01:04.490 --> 00:01:07.370
what aspect of Einstein's genius,

18
00:01:07.910 --> 00:01:11.720
the nature of his genius do you think was essential for discovering the theory

19
00:01:11.721 --> 00:01:12.554
of relativity?

20
00:01:12.920 --> 00:01:13.321
<v 1>You know,</v>

21
00:01:13.321 --> 00:01:18.321
Einstein was a hero to me and I'm sure to many people because he was able to

22
00:01:19.551 --> 00:01:22.190
make a,
of course a major,

23
00:01:22.191 --> 00:01:27.191
major contribution to physics with simplifying a bit,

24
00:01:28.480 --> 00:01:33.430
just a good Dunkin experiment if four to experiment,

25
00:01:35.190 --> 00:01:35.461
you know,

26
00:01:35.461 --> 00:01:40.461
imagining communication with the lights between a stationary observer and

27
00:01:41.731 --> 00:01:46.600
somebody on a train.
And,
uh,
I fought,
uh,
um,
you know,
the,

28
00:01:46.630 --> 00:01:50.470
the,
the,
the,
the fact that just with the force of his fault,

29
00:01:50.520 --> 00:01:54.360
of his thinking of his mind,
he could get to some,

30
00:01:54.630 --> 00:01:57.510
something so deep in term of physical reality,

31
00:01:57.511 --> 00:02:01.690
how time and depend on space and speed is,

32
00:02:01.770 --> 00:02:06.690
it was something absolutely fascinating was the power of intelligence,

33
00:02:06.691 --> 00:02:08.040
of the power of the mind.

34
00:02:08.410 --> 00:02:12.550
<v 0>Do you think the ability to imagine,
to visualize as he did,</v>

35
00:02:12.760 --> 00:02:14.410
as a lot of great physicists do,

36
00:02:15.150 --> 00:02:20.150
do you think that's in all of us human beings or is there something special to

37
00:02:20.711 --> 00:02:22.210
that one particular human being?

38
00:02:22.840 --> 00:02:24.810
<v 1>I think,
you know,</v>

39
00:02:24.990 --> 00:02:29.990
all of us can learn and have an in principle similar breakthroughs that are

40
00:02:34.180 --> 00:02:37.060
lesson to be learned from Einstein.

41
00:02:37.180 --> 00:02:42.180
He was one of five phd students at Atr and the,

42
00:02:42.881 --> 00:02:44.930
I'd cannot see shit,
take any shit off,

43
00:02:44.931 --> 00:02:48.400
shoot it in a sewer leak in physics.

44
00:02:48.520 --> 00:02:50.530
And he was the worst of the five.

45
00:02:50.770 --> 00:02:55.770
The only one who did not get an academic position when he graduated,

46
00:02:56.621 --> 00:02:58.870
when he finished his phd and he went to,

47
00:02:59.380 --> 00:03:02.470
as everybody knows for the patent office.

48
00:03:02.471 --> 00:03:05.830
And so it's not so much that he worked for the patent office,

49
00:03:05.831 --> 00:03:08.710
but the fact that obviously he was mad,

50
00:03:08.740 --> 00:03:13.500
but he was not the top student or obviously was the antique conformist,

51
00:03:13.580 --> 00:03:18.580
was not thinking in the traditional way that probably is teachers and the other

52
00:03:18.791 --> 00:03:23.410
students were doing.
So there is a lot to be said about,
you know,

53
00:03:23.411 --> 00:03:24.610
trying to be,

54
00:03:25.790 --> 00:03:30.790
to do the opposite or something quite different from what other people are doing

55
00:03:31.071 --> 00:03:35.570
that Saturday too for the stock market.
Never,
never buy for very bodies by

56
00:03:36.860 --> 00:03:39.110
<v 0>and also true for science.
Yes.</v>

57
00:03:39.680 --> 00:03:43.730
So you've also mentioned staying on the theme of physics,

58
00:03:43.760 --> 00:03:48.760
that you were excited and he young age by the mysteries of the universe that a

59
00:03:49.401 --> 00:03:54.401
physics could uncover such as I saw mentioned the possibility of time travel.

60
00:03:56.750 --> 00:04:00.410
So the most out of the box question I think I'll get to ask today.

61
00:04:00.411 --> 00:04:02.210
Do you think time travel is possible?

62
00:04:03.470 --> 00:04:07.450
<v 1>Oh,
why?
It would be nice if it were possible right now.
Uh,
you know,</v>

63
00:04:07.490 --> 00:04:12.060
you signed to never say no.
Um,

64
00:04:12.830 --> 00:04:14.780
<v 0>but your understanding of the nature of time,</v>

65
00:04:15.070 --> 00:04:15.903
<v 1>yeah.</v>

66
00:04:15.970 --> 00:04:20.970
It's very likely that it's not possible to travel in time.

67
00:04:21.190 --> 00:04:22.340
Um,
he,

68
00:04:22.341 --> 00:04:26.830
we may be able to travel forward in time if we can,

69
00:04:26.831 --> 00:04:31.420
for instance,
Freezo selves or uh,
you know,

70
00:04:31.421 --> 00:04:35.740
go on some spacecraft traveling close to the speed of light.

71
00:04:37.660 --> 00:04:41.740
But in terms of actively traveling,
for instance,
back in time,

72
00:04:41.741 --> 00:04:45.100
I find probably very unlikely.

73
00:04:45.270 --> 00:04:46.910
<v 0>So do you still hold the,</v>

74
00:04:47.040 --> 00:04:52.040
the underlying dream of the engineering intelligence that will build systems

75
00:04:53.341 --> 00:04:56.760
that are able to do such huge leaps,

76
00:04:56.790 --> 00:05:01.790
like discovering the kind of mechanism that will be required to travel through

77
00:05:02.221 --> 00:05:06.360
time?
Do you still hold that dream or are echoes of it from each childhood?

78
00:05:07.050 --> 00:05:08.610
<v 1>Yeah,
I,
you know,</v>

79
00:05:08.611 --> 00:05:13.611
I don't think whether there are certain problems that probably cannot be solved

80
00:05:14.460 --> 00:05:19.220
depending what,
what you believe about the physical reality,
like,
uh,

81
00:05:19.700 --> 00:05:19.950
you know,

82
00:05:19.950 --> 00:05:24.950
maybe totally impossible to create energy from nothing or to travel back in

83
00:05:26.851 --> 00:05:29.640
time.
But,
uh,
um,

84
00:05:29.790 --> 00:05:34.790
about making the machines that can think as well as we do or better or more

85
00:05:37.681 --> 00:05:42.320
likely,
especially in the short and midterm help us think better,

86
00:05:42.390 --> 00:05:46.770
which is in a sense it's happening already with the computers we have and it

87
00:05:46.771 --> 00:05:49.890
will happen more and more.
But that I certainly believe,

88
00:05:49.950 --> 00:05:54.950
and I don't seem principle why computers at some point could not become more

89
00:05:57.410 --> 00:05:59.390
intelligent than we are.

90
00:05:59.420 --> 00:06:04.170
Although the word intelligence is a tricky one and one no,

91
00:06:04.180 --> 00:06:04.540
shoot,

92
00:06:04.540 --> 00:06:09.540
<v 0>discuss what they mean with that intelligence consciousness where it's like,</v>

93
00:06:12.000 --> 00:06:16.470
love is all these are very,
uh,
need to be disentangled.

94
00:06:16.770 --> 00:06:21.360
So you've mentioned also that you believe the problem of intelligence is the

95
00:06:21.361 --> 00:06:25.110
greatest problem in science greater than the origin of life and the origin of

96
00:06:25.111 --> 00:06:29.190
the universe.
You've also,
uh,
in the talk,

97
00:06:29.191 --> 00:06:33.960
I've listened to a said that you're open to arguments against,
uh,

98
00:06:33.961 --> 00:06:35.310
against you.
So

99
00:06:37.320 --> 00:06:41.490
what do you think is the most captivating aspect of this problem of

100
00:06:41.491 --> 00:06:46.350
understanding the nature of intelligence?
Why does it a captivate you as it does

101
00:06:47.370 --> 00:06:52.240
<v 1>what originally?
I think one of the motivation that I,
uh,</v>

102
00:06:52.241 --> 00:06:53.074
there's a,

103
00:06:53.490 --> 00:06:58.490
I guess a teenager when I was infatuated with theory of relativity was really

104
00:06:59.251 --> 00:07:02.670
that I,
I found that there was a,

105
00:07:02.720 --> 00:07:07.410
the problem of time and space and general relativity,

106
00:07:07.950 --> 00:07:12.810
but there were so many other programs have this same level of difficulty and the

107
00:07:12.811 --> 00:07:16.620
importance that they could,
even if I were to Einstein,

108
00:07:16.621 --> 00:07:18.810
it was difficult to hope to solve all of that.

109
00:07:19.470 --> 00:07:22.440
So what about solving a problem?

110
00:07:22.700 --> 00:07:27.600
Who Solution and allow me to solve all the problems and the source.

111
00:07:27.960 --> 00:07:32.790
What if we could find the key to an intelligence,

112
00:07:33.030 --> 00:07:35.770
you know,
10 times better or faster,
the nice tight.

113
00:07:37.030 --> 00:07:40.530
<v 0>So that's sort of seeing artificial intelligence as a,</v>

114
00:07:40.531 --> 00:07:42.940
as a tool to expand our capabilities.

115
00:07:43.210 --> 00:07:48.210
But is there just an inherent curiosity in you and just understanding what it is

116
00:07:50.681 --> 00:07:53.680
in our in,
in here that makes it all all work?

117
00:07:54.340 --> 00:07:56.920
<v 1>Yes.
Episode.
The all right,
so I was starting,</v>

118
00:07:57.130 --> 00:08:01.990
I started saying this was the motivation when I was a teenager,
but you know,

119
00:08:01.991 --> 00:08:06.991
soon after I think the problem of human intelligence became a real focus of,

120
00:08:11.260 --> 00:08:12.093
you know,

121
00:08:12.340 --> 00:08:17.340
of my extent of my science and my research because I think he's,

122
00:08:18.550 --> 00:08:23.550
for me the most interesting problem is really asking,

123
00:08:24.850 --> 00:08:26.230
uh,
oh we,

124
00:08:26.470 --> 00:08:31.470
we are right is asking not only the question about science,

125
00:08:31.661 --> 00:08:36.610
but even about the very tool we are using to do science,
which is all brain.

126
00:08:37.810 --> 00:08:41.380
How does our brain work from,
where does it come from,

127
00:08:41.960 --> 00:08:44.800
what are its limitation?
Can we make it better?

128
00:08:46.110 --> 00:08:51.110
<v 0>And that in many ways is the ultimate question that underlies this whole effort</v>

129
00:08:52.561 --> 00:08:53.394
of science.

130
00:08:54.360 --> 00:08:58.530
So you've significant contributions in both the science of intelligence and the

131
00:08:58.531 --> 00:09:03.390
engineering of intelligence
in a hypothetical way.

132
00:09:03.391 --> 00:09:04.320
Let me ask,

133
00:09:04.950 --> 00:09:09.360
how far do you think we can get in creating intelligence systems without

134
00:09:09.361 --> 00:09:12.060
understanding the biological,

135
00:09:12.090 --> 00:09:16.590
the understanding of how the human brain creates intelligence?
Put another way.

136
00:09:16.591 --> 00:09:21.000
Do you think we can build a strong assets and without really getting at the
core,

137
00:09:21.030 --> 00:09:23.820
the functional and the understanding of the function,
nature of the brain?

138
00:09:25.190 --> 00:09:26.023
What are the seas

139
00:09:26.880 --> 00:09:31.440
<v 1>difficult question.
You know,
we did,
uh,
um,</v>

140
00:09:31.530 --> 00:09:36.240
solve problems like flying
without,

141
00:09:36.830 --> 00:09:38.010
oh really?

142
00:09:39.020 --> 00:09:42.530
Using too much or knowledge about how birds fly.

143
00:09:44.730 --> 00:09:49.730
It was important I guess to know that you could have a thinks heavier than,

144
00:09:50.460 --> 00:09:54.650
than air being able to fly like birds.

145
00:09:56.750 --> 00:10:01.710
But beyond that property we did not learn very much.
You know,
some,
you know,

146
00:10:01.711 --> 00:10:06.110
the,
the brothers,
right?
Did learn a lot of,

147
00:10:06.111 --> 00:10:10.580
of salvation about birds and designing their,

148
00:10:10.600 --> 00:10:13.670
their aircraft.
But,
uh,
you know,

149
00:10:13.671 --> 00:10:18.110
you can argue we did not use much of biology in that particular case.
Now,

150
00:10:18.111 --> 00:10:22.800
in the case of intelligence,
I think that,
uh,

151
00:10:24.770 --> 00:10:29.500
it's,
it's a bit of a bat right now
if you are,

152
00:10:29.540 --> 00:10:33.190
if you ask a,
okay.
Um,
we,

153
00:10:34.040 --> 00:10:37.010
we all agree we'll get at some point,
maybe soon,

154
00:10:37.011 --> 00:10:42.011
maybe later to a machine that is indistinguishable from my secretary saying in

155
00:10:42.801 --> 00:10:45.380
terms of what I can ask the machine to do.

156
00:10:47.840 --> 00:10:51.980
I think we'll get there.
And now the question is,
and you can ask people,

157
00:10:51.981 --> 00:10:55.780
do you think we'll get there without any knowledge about,
uh,
you know,

158
00:10:55.820 --> 00:11:00.820
the human brain or the to the best way to get their eyes to understand better

159
00:11:01.100 --> 00:11:03.290
the human brain?
Yeah.
Okay.
This is,

160
00:11:03.291 --> 00:11:08.291
I think an educated bet that different people with different background will

161
00:11:09.231 --> 00:11:10.880
decide in different ways.

162
00:11:11.790 --> 00:11:15.660
The recent history story of the progress in AI in the last,

163
00:11:16.440 --> 00:11:19.740
I would say five years or 10 years has been the,

164
00:11:20.730 --> 00:11:23.730
the main,
uh,
breakthroughs.

165
00:11:23.820 --> 00:11:27.910
The main recent breakthroughs.
Oh,
really?

166
00:11:28.720 --> 00:11:31.390
Start from neuroscience.
Okay.

167
00:11:32.240 --> 00:11:37.040
I can mention reinforcement learning as one.
And he's one of the,

168
00:11:37.041 --> 00:11:40.700
our goal teams at the core of Alpha goal,

169
00:11:41.120 --> 00:11:46.120
which is the system to beat the kind of an official world champion of goal Lisi

170
00:11:47.190 --> 00:11:52.010
doll.
And two free years ago in Sule,
um,

171
00:11:52.810 --> 00:11:56.800
that's one.
And that started really with the work of Pavlov.

172
00:11:57.280 --> 00:11:58.113
Um,

173
00:11:59.140 --> 00:12:04.140
900 Marvin Minsky in the 60s and many other neuroscientists later on.

174
00:12:07.080 --> 00:12:11.130
Um,
and deep learning,
uh,
started,
uh,

175
00:12:11.170 --> 00:12:16.170
which is at the core again of Alphago and systems like autonomous driving

176
00:12:17.711 --> 00:12:22.480
systems for cars,
like the systems that,
uh,
Mobileye,

177
00:12:22.510 --> 00:12:26.560
which is a company started by one of my expos.
Dot Com.
Nosha sure.

178
00:12:27.130 --> 00:12:32.110
Um,
the,
the salt that is a quarter of those things and deep learning,

179
00:12:33.100 --> 00:12:36.940
really the initial ideas in terms of the architecture of this layer.

180
00:12:37.150 --> 00:12:42.150
Heirarchical networks started with work of Torsten Visa,

181
00:12:42.990 --> 00:12:47.380
Lynn Individually,
Bill at Harvard,
up the river in the 60s.

182
00:12:47.770 --> 00:12:49.300
So recent,

183
00:12:49.400 --> 00:12:54.300
the story suggests that neuroscience played a big role in this breakthroughs.

184
00:12:54.310 --> 00:12:59.310
My personal bet is that there is a good chance they continue to play a big role,

185
00:12:59.890 --> 00:13:01.840
maybe not in all the future breakthroughs,

186
00:13:01.841 --> 00:13:06.200
but in some of them at least in inspiration.
So he at least in an inspiration.

187
00:13:06.201 --> 00:13:07.150
Absolutely,
yes.

188
00:13:07.380 --> 00:13:12.150
<v 0>So you see,
you studied both artificial and biologic when you're on that works.</v>

189
00:13:12.151 --> 00:13:12.660
He said,

190
00:13:12.660 --> 00:13:17.660
these mechanisms that underlie deep learning and reinforcement learning,

191
00:13:19.740 --> 00:13:24.740
but there is nevertheless a significant differences between biological and

192
00:13:24.991 --> 00:13:28.560
artificial neural networks as they stand now.
So between the two,

193
00:13:30.080 --> 00:13:32.580
what do you find is the most interesting,
mysterious,

194
00:13:32.581 --> 00:13:37.170
maybe even beautiful difference as as it currently stands in our understanding?

195
00:13:37.910 --> 00:13:42.910
<v 1>I must confess that until recently I found that the artificial networks too</v>

196
00:13:45.111 --> 00:13:50.060
simplistic relative to real neural networks.
But,

197
00:13:50.360 --> 00:13:53.360
uh,
you know,
recently I've been started to think that yes,

198
00:13:53.361 --> 00:13:58.361
there are very big simplification of what you find in the brain.

199
00:13:59.030 --> 00:14:04.030
But on the other hand that are much closer in terms of the architecture to the

200
00:14:06.141 --> 00:14:11.141
brain than other models that we had that computer science used as model of

201
00:14:12.530 --> 00:14:15.560
thinking which will mathematical logics,
you know,

202
00:14:15.561 --> 00:14:19.100
least prologue and those kind of things.

203
00:14:19.430 --> 00:14:23.270
So in comparison to those that are much closer to the brain,

204
00:14:23.300 --> 00:14:27.770
you have networks of neurons,
which is what the brain is about.

205
00:14:29.020 --> 00:14:32.180
The artificial neurons in the mod there is,
as I said,

206
00:14:32.181 --> 00:14:36.770
the caricature of the biological neurons,
but they're still neuron,

207
00:14:36.771 --> 00:14:39.710
single units communicating with other units.

208
00:14:40.010 --> 00:14:42.170
Something that is absent in,
you know,

209
00:14:42.410 --> 00:14:47.410
the traditional computer type models of mathematics or reasoning and so on.

210
00:14:50.780 --> 00:14:51.680
So aspect do you,

211
00:14:52.100 --> 00:14:57.100
<v 0>would you like to see in artificial neural networks added over time as we try to</v>

212
00:14:58.430 --> 00:14:59.810
figure out ways to improve them?

213
00:14:59.950 --> 00:15:04.950
<v 1>So one of the main differences and,</v>

214
00:15:05.310 --> 00:15:06.143
um,
you know,

215
00:15:06.210 --> 00:15:11.210
problems in terms of deep learning today and it's not only deep learning and the

216
00:15:13.470 --> 00:15:18.470
brain is the need for deep learning techniques to have,

217
00:15:18.660 --> 00:15:23.460
uh,
a lot of labeled examples,
you know,

218
00:15:23.461 --> 00:15:28.410
for Easter,
for image nature of like it training site,
which is 1 million images,

219
00:15:28.680 --> 00:15:33.680
each one labeled by some human in terms of which object is there.

220
00:15:34.680 --> 00:15:36.360
And,
um,
it's,

221
00:15:37.170 --> 00:15:41.040
it's clear that in biology the baby

222
00:15:42.740 --> 00:15:46.470
may be able to see millions of images in the first years of life,

223
00:15:46.980 --> 00:15:51.980
but we will not have million of labels given to him or her by parents or take

224
00:15:54.120 --> 00:15:59.100
take a caretakers.
So,
uh,
how do you solve that?

225
00:15:59.280 --> 00:15:59.581
You know,

226
00:15:59.581 --> 00:16:04.581
I think there is this interesting challenge that today,

227
00:16:05.130 --> 00:16:10.110
uh,
deep learning and related techniques are all about big data,
big data,

228
00:16:10.111 --> 00:16:15.111
meaning a lot of examples labelled by humans.

229
00:16:17.400 --> 00:16:22.370
Um,
whereas in nature you have,
uh,

230
00:16:22.740 --> 00:16:26.280
so the,
the,
the,
this big data is and go into infinity,

231
00:16:26.281 --> 00:16:29.820
that's the best you know,
and meaningly able data.

232
00:16:30.240 --> 00:16:35.240
But I think the biological world is more and going to one child can learn,

233
00:16:36.740 --> 00:16:37.770
it's a beautiful erotic,

234
00:16:37.860 --> 00:16:42.240
very small number of labeled examples.

235
00:16:42.690 --> 00:16:47.520
Like you tell a child this is a car you don't need to say like in image net,

236
00:16:47.521 --> 00:16:50.240
you know,
this is a car,
this is a car,
this is not a card,

237
00:16:50.241 --> 00:16:51.690
this is not the cat 1 million.

238
00:16:53.610 --> 00:16:58.410
<v 0>So,
and of course where the alpha go in or at least a Alpha zero variants,</v>

239
00:16:58.440 --> 00:17:01.450
there's because of the,
because of the world of Goa,

240
00:17:01.451 --> 00:17:06.451
so simplistic that you can actually learn by yourself or self play.

241
00:17:06.721 --> 00:17:09.360
You can play against each other and the real world,

242
00:17:09.361 --> 00:17:13.530
I mean the visual system that you've studied extensively is a lot more

243
00:17:13.531 --> 00:17:18.180
complicated than the game of go.
So on the comment about children,

244
00:17:18.210 --> 00:17:22.140
which your fascinatingly good at learning new stuff,

245
00:17:22.950 --> 00:17:26.310
how much of it do you think is hardware?
How much of it is software?

246
00:17:26.720 --> 00:17:28.310
<v 1>Yeah,
that's a good,</v>

247
00:17:28.311 --> 00:17:33.130
a deep question is in a sense it's the old question of nurture and nature and

248
00:17:33.180 --> 00:17:38.180
munchies in the gene and how much is in the experience of an individual.

249
00:17:41.360 --> 00:17:46.360
Obviously it's both that play a role and I believe that the way

250
00:17:50.610 --> 00:17:55.080
evolution gives put prior information,
so to speak,

251
00:17:55.081 --> 00:17:59.230
hardwired,
it's not really hardwired,
but um,

252
00:18:00.010 --> 00:18:02.570
uh,
that's essentially in a pot.

253
00:18:02.710 --> 00:18:07.680
I think what's going on is that evolution
as,

254
00:18:08.070 --> 00:18:09.000
um,
you know,

255
00:18:09.570 --> 00:18:14.570
most necessarily if you believe in Darwin is very opportunistic and,

256
00:18:16.530 --> 00:18:19.200
and think about,
uh,
uh,

257
00:18:19.680 --> 00:18:24.250
our DNA and the DNA of the resolve,
Sheila,
uh,

258
00:18:24.290 --> 00:18:27.810
out of DNA does not have many more genes than the resolve.

259
00:18:27.811 --> 00:18:31.620
Sheila aren't mountain the fly,
the fly,
the fruit fly.

260
00:18:32.520 --> 00:18:37.520
Now we know that the fruit fly does not learn a very much during its individual

261
00:18:38.910 --> 00:18:39.691
existence.

262
00:18:39.691 --> 00:18:44.691
It looks like one of these machinery that it's really mostly not 100%,

263
00:18:45.180 --> 00:18:49.680
but you know,
95% hardcore that by the jeans.

264
00:18:51.690 --> 00:18:55.800
But since we don't have many more genes than the Safiullah is evolution,

265
00:18:57.140 --> 00:19:02.140
couldn't call it in as a kind of general learning machinery and then had to give

266
00:19:05.430 --> 00:19:09.600
very weak priors.
Um,

267
00:19:09.870 --> 00:19:12.330
like for instance,
let me take,

268
00:19:12.331 --> 00:19:14.910
give a specific example,

269
00:19:14.911 --> 00:19:19.170
which is a recent work by a member of our center for brains,
minds and machines.

270
00:19:20.760 --> 00:19:25.360
We know him because of work of other people in our group and other groups that

271
00:19:25.361 --> 00:19:30.361
there are cells in a part of our brain neurons that are tuned to faces.

272
00:19:31.230 --> 00:19:34.140
They seems to be involved in face that a clinician now,

273
00:19:34.141 --> 00:19:38.380
this face area exist,
uh,
uh,

274
00:19:38.460 --> 00:19:42.630
seems to be present in young children and adults.

275
00:19:43.750 --> 00:19:46.590
Um,
and one question is,

276
00:19:46.920 --> 00:19:51.920
is there from the beginning is hard wired by evolution or you know,

277
00:19:53.010 --> 00:19:56.250
somehow he's,
it learned very quick.
So what's your,
by the way,

278
00:19:56.251 --> 00:20:00.630
a lot of the questions I'm asking we,
the answers we don't really know,

279
00:20:00.930 --> 00:20:04.110
but as a person who has contributors,

280
00:20:04.530 --> 00:20:06.210
some profound ideas and these fields,

281
00:20:06.450 --> 00:20:10.530
you're a good person to guests at some of these.
So,
of course,
as a caveat before,

282
00:20:10.531 --> 00:20:13.800
a lot of the stuff we're talking about,
but what is your hunch?

283
00:20:14.640 --> 00:20:19.290
Is the face of the part of the brain that that seems to be concentrated on face

284
00:20:19.291 --> 00:20:23.370
recognition.
Are you born with that or are you just,
is designed to,

285
00:20:23.590 --> 00:20:28.110
to learn that quickly?
Like the face of the mother and it's,
and I,
my hunch,

286
00:20:28.140 --> 00:20:32.280
your mom by bias was the second one learned very quickly.

287
00:20:32.281 --> 00:20:37.281
And it turns out that the marriage Livingstone at Harvard has done some amazing

288
00:20:39.001 --> 00:20:44.001
experiments in which she raised baby monkeys depriving them of fees during the

289
00:20:45.661 --> 00:20:47.070
first weeks of life.

290
00:20:48.550 --> 00:20:53.390
So they see technicians but that the conditioner have a mask.
Yes.

291
00:20:55.050 --> 00:20:59.440
And um,
and so when they look to,
mmm.

292
00:21:00.510 --> 00:21:05.510
At the area in the brain of this menchies that usually you find faces,

293
00:21:07.100 --> 00:21:08.960
they found no face preference.

294
00:21:10.820 --> 00:21:13.800
So my guess is the,

295
00:21:14.370 --> 00:21:19.370
what evolution does in this case is that he's a plastic and area and which is

296
00:21:19.891 --> 00:21:20.371
plastic,

297
00:21:20.371 --> 00:21:25.371
which is kind of predetermined to be imprinted very easily.

298
00:21:26.520 --> 00:21:31.520
But the command from the gene is not a detailed psychiatry for a face template.

299
00:21:32.310 --> 00:21:33.143
Could be,

300
00:21:33.420 --> 00:21:37.380
but this will let acquired property a lot of bits cause you had to specify a lot

301
00:21:37.381 --> 00:21:40.790
of connection of a lot of neurons instead at the command,

302
00:21:40.860 --> 00:21:44.790
the command from the gene is something like in print,

303
00:21:44.850 --> 00:21:49.350
memorize what you see most often in the first two weeks of life,

304
00:21:49.380 --> 00:21:53.410
especially in connection with food and maybe nipples.

305
00:21:53.470 --> 00:21:54.670
I don't write well

306
00:21:54.980 --> 00:21:59.750
<v 0>source of food and that area is very plastic at first and then solidifies.</v>

307
00:22:00.410 --> 00:22:05.410
It'd be interesting if a variant of that experiment would show a different kind

308
00:22:05.511 --> 00:22:10.100
of pattern associated with food than a face pattern.
Whether that could stick.

309
00:22:10.101 --> 00:22:10.170
Yeah.

310
00:22:10.170 --> 00:22:13.410
<v 1>That I don't indications that during the experiment,</v>

311
00:22:14.920 --> 00:22:19.140
well the monkey saw quite often where,
um,

312
00:22:19.530 --> 00:22:24.060
the blue gloves of the technicians that were given to the baby monkeys,

313
00:22:24.120 --> 00:22:29.120
the milk and some of the cells instead of being faced sensitive in that area.

314
00:22:30.480 --> 00:22:31.510
Uh,
Hanson's

315
00:22:33.550 --> 00:22:37.020
<v 0>Oh that's fascinating.
Can you,
uh,</v>

316
00:22:37.490 --> 00:22:42.350
talk about what are the different parts of the brain and in your view sort of

317
00:22:42.351 --> 00:22:45.800
loosely and how do they contribute to intelligence?

318
00:22:45.801 --> 00:22:50.801
Do you see the brain as a bunch of different modules and they together come in

319
00:22:51.591 --> 00:22:56.120
the human brain to create intelligence?
Or is it all one,

320
00:22:57.220 --> 00:23:01.240
um,
much of the same kind of fundamental,
uh,
um,

321
00:23:01.860 --> 00:23:03.620
the architecture?

322
00:23:04.660 --> 00:23:06.650
<v 1>Yeah,
that's,
um,
you know,</v>

323
00:23:06.940 --> 00:23:11.940
that's an important question and there was a phase in a neuroscience back in the

324
00:23:14.231 --> 00:23:19.231
1950 or so in which it was believed for a while that the brain was equipped

325
00:23:21.371 --> 00:23:21.911
potential.

326
00:23:21.911 --> 00:23:26.911
This was the term you could cut out a piece and I'm nothing special happened

327
00:23:28.481 --> 00:23:33.370
apart,
little bit less performance.
There was a,

328
00:23:33.960 --> 00:23:36.760
a surgeon.
Lastly,

329
00:23:37.010 --> 00:23:42.010
we'll need a lot of experiments of this type with mice and the riots and

330
00:23:42.580 --> 00:23:47.580
concluded that every part of the brain was essentially equivalent to any other

331
00:23:48.141 --> 00:23:48.974
one.

332
00:23:51.420 --> 00:23:55.670
It turns out that that's,
that's really not true.
It's a,

333
00:23:56.100 --> 00:24:00.030
that are very specific modules in the brain as you said.

334
00:24:00.570 --> 00:24:02.130
And uh,
you know,

335
00:24:02.160 --> 00:24:07.160
people may lose the ability to speak if you have a stroke in a certain region or

336
00:24:07.681 --> 00:24:12.681
me and lose control of their legs in another region or so.

337
00:24:13.081 --> 00:24:17.940
There are very specific,
the brain is also quite flexible and redundant.

338
00:24:17.940 --> 00:24:22.500
So often it can correct things and uh,
you know,

339
00:24:22.740 --> 00:24:25.610
uh,
kind of,
um,

340
00:24:25.770 --> 00:24:30.430
take over functions from one part of the brain to the other.
But uh,

341
00:24:30.930 --> 00:24:33.830
but,
but really that are specific modules.

342
00:24:33.831 --> 00:24:38.570
So the answer that we know from this old world,

343
00:24:39.840 --> 00:24:43.200
which was basically on based on lesions,

344
00:24:43.710 --> 00:24:48.710
either on animals or very often the where and the mine of what they did,

345
00:24:49.420 --> 00:24:54.210
there was a mine are very interesting data coming from,

346
00:24:54.630 --> 00:24:58.890
um,
from the war,
from different types of,
um,

347
00:24:59.410 --> 00:25:04.320
injuries,
injuries that soldiers had in the brain and uh,

348
00:25:05.070 --> 00:25:08.250
more recently,
um,
functional Mri,

349
00:25:09.120 --> 00:25:10.110
which allow you to,

350
00:25:11.030 --> 00:25:16.030
to check which part of the brain are active when you're doing different tasks

351
00:25:19.670 --> 00:25:23.630
as you know,
can replace some of this.

352
00:25:23.670 --> 00:25:28.470
You can see that certain parts of the brain are involved,
are active.

353
00:25:30.350 --> 00:25:31.750
<v 0>Yup.
And that's all right.</v>

354
00:25:32.290 --> 00:25:37.290
But sort of taking a step back to that part of the brain that discovers that,

355
00:25:37.741 --> 00:25:40.780
uh,
specializes in the face and how that might be learned.

356
00:25:41.650 --> 00:25:45.300
What's your intuition behind,
you know,

357
00:25:45.310 --> 00:25:48.880
is it possible that a sort of from a physicist perspective,

358
00:25:48.881 --> 00:25:52.660
when you get lower and lower that it's all the same stuff and it just,

359
00:25:52.720 --> 00:25:57.070
when you're born it's plastic and quickly figures out this part is going to be

360
00:25:57.071 --> 00:25:59.200
about vision.
This is going to be about language.

361
00:25:59.201 --> 00:26:01.600
This is about common sense reasoning.

362
00:26:01.960 --> 00:26:06.280
Do you have an intuition that that kind of learning is going on really quickly

363
00:26:06.281 --> 00:26:10.960
or is it really kinda solidified in hardware?
That's a great question.

364
00:26:11.290 --> 00:26:12.850
So there are parts of the brain

365
00:26:14.620 --> 00:26:19.620
<v 1>like the cerebellum or they put campus that are quite different from each other</v>

366
00:26:21.291 --> 00:26:26.070
that they clearly have different anatomy,
different connectivity there.

367
00:26:26.890 --> 00:26:30.520
Then there is uh,
the,
the CORTEX,

368
00:26:31.240 --> 00:26:36.240
which is the most developed part of the brain in humans and in the cortex you

369
00:26:38.351 --> 00:26:43.351
have different regions of the cortex that are responsible for vision,

370
00:26:43.410 --> 00:26:48.210
for audition,
for motor control,
for language.
Now,

371
00:26:48.211 --> 00:26:50.070
one of the big puzzles of,

372
00:26:50.100 --> 00:26:54.580
of this is that in the cortex is the Cortex,
is the Cortex,

373
00:26:54.870 --> 00:26:55.290
is,

374
00:26:55.290 --> 00:27:00.290
looks like it tastes the same in terms of hardware,

375
00:27:01.020 --> 00:27:06.020
in terms of type of neurons and connectivity across these different modalities.

376
00:27:08.430 --> 00:27:10.500
So for the Cortex,

377
00:27:11.410 --> 00:27:15.240
letting aside this other parts of the brain,
like spinal cord,

378
00:27:15.241 --> 00:27:18.880
hippocampus or a bedroom and so on for the Cortex,

379
00:27:18.881 --> 00:27:23.881
I think your question about hardware and software and learning and so on,

380
00:27:24.350 --> 00:27:29.260
it's think is rather open.
And um,

381
00:27:29.320 --> 00:27:30.130
you know,

382
00:27:30.130 --> 00:27:35.130
I find it very interesting for is it to think about an architecture,

383
00:27:35.620 --> 00:27:40.030
computer architecture that is good for vision and the same time he's good for

384
00:27:40.031 --> 00:27:42.560
language seems to be,
you know,

385
00:27:42.910 --> 00:27:47.910
saw different problem areas that you have to solve.

386
00:27:49.340 --> 00:27:52.820
<v 0>But the underlying mechanism might be the same and that's really instructive for</v>

387
00:27:52.890 --> 00:27:54.850
who may be artificial neuron nowadays.

388
00:27:55.160 --> 00:27:59.030
So we've done a lot of great work in vision and human vision,

389
00:27:59.720 --> 00:28:00.553
computer vision.

390
00:28:01.610 --> 00:28:05.960
And you mentioned the problem of human vision is really as difficult as the

391
00:28:05.961 --> 00:28:10.550
problem of general intelligence and maybe that connects to the cortex
discussion.

392
00:28:11.450 --> 00:28:16.450
Can you describe the human visual cortex and how the humans begin to understand

393
00:28:18.921 --> 00:28:23.780
the world through the raw sensory information that what's,
uh,

394
00:28:24.080 --> 00:28:27.020
for folks enough familiar,

395
00:28:27.800 --> 00:28:29.960
especially in on the computer vision side,

396
00:28:30.110 --> 00:28:34.370
we don't often actually take a step back except saying what the sentence or two

397
00:28:34.371 --> 00:28:38.840
that one is inspired by the other w w what is it that we know about the human

398
00:28:38.841 --> 00:28:40.580
visual cortex.
That's interesting.

399
00:28:40.610 --> 00:28:43.430
<v 1>So we know quite a bit at that same time we don't know about,</v>

400
00:28:43.460 --> 00:28:47.270
but the bit we know,
you know,
in a,

401
00:28:47.300 --> 00:28:51.700
in a sense we know a lot of the details and um,

402
00:28:52.040 --> 00:28:54.190
and men we don't know.
And uh,

403
00:28:54.550 --> 00:28:57.660
we know a lot of the top level,
um,

404
00:28:58.570 --> 00:29:00.130
the answer to top level question,

405
00:29:00.131 --> 00:29:04.030
but we don't know some basic ones even in terms of general neuroscience,

406
00:29:04.031 --> 00:29:07.450
forgetting vision,
you know,
why do we sleep?

407
00:29:08.420 --> 00:29:13.420
It's such a basic question and we really don't have an answer.

408
00:29:14.580 --> 00:29:18.270
<v 0>Do you think,
so taking a step back on that,
so sleep for example,</v>

409
00:29:18.271 --> 00:29:21.780
it's fascinating.
Do you think that's a neuroscience question?

410
00:29:22.020 --> 00:29:24.240
Or if we talk about abstractions,

411
00:29:24.840 --> 00:29:29.370
what do you think is an interesting way to study intelligence or most effective

412
00:29:29.400 --> 00:29:34.400
on the levels of abstraction as a chemicals and biological is electrophysical

413
00:29:34.710 --> 00:29:38.800
mathematical as you've done a lot of excellent work and not side.
Which uh,

414
00:29:38.840 --> 00:29:42.010
psychology,
sort of like a,
which level abstraction do

415
00:29:42.010 --> 00:29:46.900
<v 1>you think in terms of levels of abstraction?</v>

416
00:29:46.901 --> 00:29:50.030
I think we need all of them.
It's when you know,

417
00:29:50.130 --> 00:29:52.000
it's like if you ask me

418
00:29:53.950 --> 00:29:58.630
what does it mean to understand the computer,
that's much simpler.

419
00:29:58.631 --> 00:30:00.730
But in a computer I could say,

420
00:30:00.731 --> 00:30:03.760
well I understand how to use PowerPoint.

421
00:30:04.840 --> 00:30:09.370
That's my level of understanding a computer.
It's,
it has a reasonable,
you know,

422
00:30:09.371 --> 00:30:14.371
it gives me some power to produce lights and beautiful slides and now

423
00:30:16.700 --> 00:30:17.680
somebody else,
he says,

424
00:30:17.681 --> 00:30:21.310
well I know all of the transistor work that are inside the computer.

425
00:30:21.400 --> 00:30:24.020
I can write the equation for,
you know,

426
00:30:24.400 --> 00:30:26.690
transistors and diodes and silk.

427
00:30:26.700 --> 00:30:30.700
It's a logical circuits and I can ask this guy,

428
00:30:30.701 --> 00:30:32.980
do you know how to operate PowerPoint?
No idea.

429
00:30:34.030 --> 00:30:39.030
So do you think if we discovered computers locking amongst us full of these

430
00:30:40.601 --> 00:30:45.160
transistors that are also operating under windows and have PowerPoint,

431
00:30:45.550 --> 00:30:49.240
do you think it's the digging in a little bit more,

432
00:30:49.930 --> 00:30:54.930
how useful is it to understand the transistor in order to be able to understand

433
00:30:57.400 --> 00:31:00.270
PowerPoint and these higher level at a good intelligent,
fussy?

434
00:31:00.280 --> 00:31:03.370
So I think in the case of computers,

435
00:31:03.700 --> 00:31:08.290
because they were made by engineers by as these different level of

436
00:31:08.291 --> 00:31:11.680
understanding,
rather separate on purpose,

437
00:31:12.700 --> 00:31:17.700
they are separate modules so that the engineer that designed the silicate for

438
00:31:18.581 --> 00:31:23.581
the chiefs does not need to know what a PPO is inside PowerPoint and somebody

439
00:31:24.471 --> 00:31:29.471
you can write to the software translating from one to the end to the other.

440
00:31:30.220 --> 00:31:33.340
So
in that case,

441
00:31:33.790 --> 00:31:38.790
I don't think understanding the transistor help you understand PowerPoint or

442
00:31:38.980 --> 00:31:43.580
very lethal.
Um,
if you want to understand the computer,
these question,
you know,

443
00:31:43.900 --> 00:31:48.320
I would say you have to understanding a different levels if you really want to

444
00:31:48.750 --> 00:31:52.330
build it.
But,
uh,

445
00:31:53.350 --> 00:31:57.310
but for the brain,
I think this levels of understanding,

446
00:31:57.311 --> 00:32:00.790
so the algorithms,
which kind of competition,
you know,

447
00:32:00.791 --> 00:32:05.500
the equivalent of PowerPoint and the silk,
it's,
you know,
the transistors,

448
00:32:05.860 --> 00:32:10.210
I think they are much more intertwined with each other.
That is not,

449
00:32:10.880 --> 00:32:15.460
you know,
in neatly a level of the software separate from the hardware.

450
00:32:15.910 --> 00:32:20.080
And so that's why I think in the case of the brain,

451
00:32:20.081 --> 00:32:24.850
the problem is more difficult to more than four computers requires the

452
00:32:24.851 --> 00:32:29.260
interaction,
the collaboration between different types of expertise.

453
00:32:29.430 --> 00:32:34.230
That's a big,
the brain is a big car mass that you can't just and disentangle,
uh,

454
00:32:34.650 --> 00:32:39.200
like level Ken.
But his is much more difficult and it's not,
uh,
know.

455
00:32:39.230 --> 00:32:43.610
It's not completely obvious.
And as I said,
I think he's one of the,

456
00:32:44.480 --> 00:32:49.150
if you think he's the greatest problem in science.
So I think it's,

457
00:32:49.670 --> 00:32:50.690
it's fair that

458
00:32:51.040 --> 00:32:53.830
<v 0>it's difficult.
It's a difficult one that said,</v>

459
00:32:53.831 --> 00:32:58.270
you do talk about composition and why it might be useful.

460
00:32:58.271 --> 00:33:03.100
And when you discuss why these new neural networks in artificial or biological

461
00:33:03.101 --> 00:33:07.160
sense learn anything,
you talk about composition anally.

462
00:33:07.510 --> 00:33:12.510
See there's a sense that nature can be disentangled our proper,

463
00:33:14.070 --> 00:33:14.903
uh,

464
00:33:15.460 --> 00:33:20.460
well all aspects of our cognition could be disentangled a little bit to some

465
00:33:21.941 --> 00:33:25.870
degree.
So why do you think,
what,
first of all,

466
00:33:25.900 --> 00:33:30.730
how do you see composition malty and why do you think it exists at all in
nature?

467
00:33:31.660 --> 00:33:33.030
I spoke about the,

468
00:33:34.310 --> 00:33:37.130
<v 1>I use the,
the term composition</v>

469
00:33:39.780 --> 00:33:44.280
when we look at deep neural networks,

470
00:33:44.340 --> 00:33:49.340
multi layers and trying to understand when and why they are more powerful than a

471
00:33:52.050 --> 00:33:56.620
more classical one layer networks,
lakes linear classifier,

472
00:33:57.450 --> 00:34:00.960
[inaudible] machines,
so called.
Um,

473
00:34:01.590 --> 00:34:06.590
and what we found is that in terms of approximating or learning or representing

474
00:34:08.341 --> 00:34:12.180
a function,
Eh,
a mapping from an input to an output,

475
00:34:12.181 --> 00:34:16.060
like from an image to the label in the image.
Um,

476
00:34:16.830 --> 00:34:20.340
if this function as a particular structure,

477
00:34:20.880 --> 00:34:25.880
then deep networks are much more powerful than shallow network to approximate

478
00:34:27.031 --> 00:34:28.830
the underlying function.

479
00:34:28.860 --> 00:34:33.570
And their particular structure is a structure of composition.

480
00:34:33.571 --> 00:34:33.900
Reality.

481
00:34:33.900 --> 00:34:38.900
If the function is made up of functions or functions,

482
00:34:38.981 --> 00:34:43.981
so that you need to look on when you are interpreting an image,

483
00:34:45.810 --> 00:34:47.150
classifying an image,

484
00:34:47.180 --> 00:34:50.970
you don't need to look at all pixels at once,

485
00:34:51.030 --> 00:34:56.030
but you can compute something from a small groups of pixels and then you can

486
00:34:59.010 --> 00:35:03.510
compute something on the output of this local computation and so on,

487
00:35:04.760 --> 00:35:07.260
which is similar to what you do.
And you read the sentence,

488
00:35:07.261 --> 00:35:10.590
you don't need to read the first and the last letter,

489
00:35:11.280 --> 00:35:15.330
but you can read syllables combined them in words,

490
00:35:15.930 --> 00:35:20.730
combined their words in sentences.
So this is this kind of structure.

491
00:35:21.040 --> 00:35:26.040
<v 0>So that's as part of the discussion of why deep neural networks may be more</v>

492
00:35:26.081 --> 00:35:27.520
effective than the shallow methods.

493
00:35:27.820 --> 00:35:32.820
And is your sense for most things we can use neural networks for those problems

494
00:35:37.380 --> 00:35:40.200
are going to be compositional in nature,

495
00:35:40.260 --> 00:35:43.320
like a like language,
like vision.

496
00:35:44.190 --> 00:35:46.990
How far can we get in this kind of,
right.

497
00:35:47.820 --> 00:35:52.560
So Hazel,
most philosophy,
well let's go there.

498
00:35:53.090 --> 00:35:57.440
<v 1>Yeah,
let's go there.
So friend of mine,
Max Tegmark,</v>

499
00:35:57.520 --> 00:36:01.820
<v 0>who is a physicist at Mit,
I've talked to him on this thing.
Yeah.</v>

500
00:36:01.900 --> 00:36:04.480
And he disagrees with you right a little bit.

501
00:36:05.010 --> 00:36:09.850
<v 1>You know,
it we agree on most,
but the conclusion is a bit different.
He,
he,</v>

502
00:36:10.170 --> 00:36:14.190
his conclusion is that for images,
for instance,

503
00:36:14.700 --> 00:36:19.700
the compositional structure of this function that we have to learn or to solve

504
00:36:22.080 --> 00:36:25.710
these problems comes from physics,

505
00:36:25.740 --> 00:36:30.740
comes from the fact that you have local interactions in physics between atoms

506
00:36:34.231 --> 00:36:39.231
and other items between particle of matter and other particles between planets

507
00:36:41.131 --> 00:36:45.570
and other planets,
between stars and other.
It's all local.

508
00:36:45.630 --> 00:36:49.710
Yeah.
Um,
and that's true.

509
00:36:50.200 --> 00:36:54.900
Um,
but you could push this argument a bit farther.

510
00:36:55.020 --> 00:37:00.000
Um,
and not this argument actually,
you could argue that,

511
00:37:00.210 --> 00:37:02.820
um,
you know,
maybe that's part of the true,

512
00:37:02.821 --> 00:37:07.821
but maybe what happens is kind of the opposite is that our brain is wired up as

513
00:37:09.781 --> 00:37:12.390
a deep network so

514
00:37:13.920 --> 00:37:16.650
it can learn,
understand,

515
00:37:16.680 --> 00:37:21.210
solve problems that have this compositional structure

516
00:37:22.890 --> 00:37:25.350
and cannot do it,

517
00:37:25.351 --> 00:37:28.950
cannot solve problems that don't have this composition as traction.

518
00:37:29.400 --> 00:37:34.350
So the problem is who we are a customer too.
We think about,

519
00:37:34.920 --> 00:37:38.460
we test our algorithms on our,

520
00:37:38.461 --> 00:37:41.940
this composition instruction because our brain is made up.

521
00:37:42.680 --> 00:37:46.820
<v 0>And that's in a sense,
an evolutionary perspective that we've,
so the,</v>

522
00:37:46.880 --> 00:37:48.560
the ones that didn't have,
uh,

523
00:37:49.180 --> 00:37:53.170
there weren't dealing with a compositional nature of reality,
uh,

524
00:37:53.630 --> 00:37:55.700
died off.
Yeah.

525
00:37:56.280 --> 00:37:58.640
It could be maybe the reason

526
00:38:00.310 --> 00:38:05.310
<v 1>why we have this local connectivity in the brain,</v>

527
00:38:05.480 --> 00:38:10.480
like simple cells in cortex looking on in this small part of the image,

528
00:38:10.890 --> 00:38:15.450
each one of them and another says looking at the small number of this simple

529
00:38:15.451 --> 00:38:16.350
cells and so on.

530
00:38:16.351 --> 00:38:21.351
The reason for this may be purely that was difficult to grow longer range of

531
00:38:23.341 --> 00:38:26.610
connectivity.
So supposedly it's,
you know,

532
00:38:26.640 --> 00:38:31.640
for biology it's possible to grow short range connectivity but not longer.

533
00:38:34.741 --> 00:38:39.741
And because there is a limited number of long range and so you have it,

534
00:38:40.870 --> 00:38:41.703
this,

535
00:38:41.820 --> 00:38:46.820
this limitation from the biology and this means you build a deep convolutional

536
00:38:49.781 --> 00:38:52.900
neck.
This will be something that deep convolutional network,

537
00:38:53.650 --> 00:38:57.730
and this is great for solving certain class of problems.

538
00:38:57.770 --> 00:39:00.190
These are the ones we are,

539
00:39:00.340 --> 00:39:03.520
we find easy and important for our life.
And yes,

540
00:39:03.940 --> 00:39:05.770
they were enough for us to survive.

541
00:39:07.380 --> 00:39:12.030
<v 0>And uh,
and you can start a successful business on solving those problems,</v>

542
00:39:12.410 --> 00:39:17.040
right?
The Mobileye driving as a compositional problem.

543
00:39:17.360 --> 00:39:20.070
So on the,
on the learning tasks,

544
00:39:20.400 --> 00:39:25.320
I mean we don't know much about how the brain learns in terms of optimization,

545
00:39:25.680 --> 00:39:26.341
but uh,

546
00:39:26.341 --> 00:39:30.540
so the thing that's sick Cassick gradient descent is what artificial neural

547
00:39:30.541 --> 00:39:34.560
networks use for the most part to,
uh,

548
00:39:35.480 --> 00:39:40.350
adjust the parameters in such a way that it's able to deal based on the label

549
00:39:40.351 --> 00:39:42.210
data,
it's able to solve the problem.

550
00:39:42.540 --> 00:39:47.540
So what's your intuition about why it works at all?

551
00:39:50.100 --> 00:39:54.240
How hard of a problem it is to optimize in your own network,

552
00:39:54.510 --> 00:39:58.670
artificial neuron that work?
Is there other alternatives?
Yeah,

553
00:39:58.730 --> 00:39:59.850
just in general,

554
00:40:00.840 --> 00:40:04.680
your intuitions behind this very simplistic algorithm that seems to do pretty

555
00:40:04.681 --> 00:40:05.940
good.
Surprising.

556
00:40:06.610 --> 00:40:10.740
<v 1>Yes.
So I find neuroscience,</v>

557
00:40:10.780 --> 00:40:15.780
the architecture of Cortex is really similar to the architecture of deep

558
00:40:16.331 --> 00:40:17.230
networks.

559
00:40:17.320 --> 00:40:22.320
So there is a nice correspondence there between the biology and this kind of

560
00:40:23.710 --> 00:40:28.180
local connectivity heirarchical um,
architecture.

561
00:40:28.181 --> 00:40:31.640
The stochastic gradient descent,
as you said,
is,
um,

562
00:40:32.200 --> 00:40:34.180
it's a very simple technique.

563
00:40:35.820 --> 00:40:40.820
It seems pretty unlikely that biology could do that from,

564
00:40:42.480 --> 00:40:44.220
from what we know right now about

565
00:40:45.900 --> 00:40:49.680
cortex and neurons and signups is,
um,

566
00:40:50.220 --> 00:40:53.910
so it's a big question open whether there are other

567
00:40:55.740 --> 00:41:00.740
optimization learning algorithms that can replace stochastic gradient descent.

568
00:41:02.010 --> 00:41:04.620
And My,
my guess is yes,

569
00:41:06.690 --> 00:41:11.690
but nobody has found yet a real answer.

570
00:41:11.820 --> 00:41:16.170
I mean people are trying,
still trying and there are some interesting ideas.

571
00:41:18.380 --> 00:41:23.240
The fact that um,
stochastic gradient descent is so successful,

572
00:41:23.660 --> 00:41:27.110
this has become clear is not so mysterious.

573
00:41:27.740 --> 00:41:29.800
And the reason is that,
um,

574
00:41:30.920 --> 00:41:33.830
it's an interesting fact that you know,

575
00:41:33.831 --> 00:41:38.831
is a change in a sense in how people think about statistics and,

576
00:41:39.770 --> 00:41:44.770
and this is the following is that typically when you had data and you had say a

577
00:41:49.551 --> 00:41:54.500
model with parameters,
you are trying to fit the model to the data,
you know,

578
00:41:54.501 --> 00:41:58.580
to fit the parameters.
Typically the kind of,
um,

579
00:41:59.150 --> 00:42:04.150
kind of a crowd wisdom type Ida was you should have at least a,

580
00:42:07.731 --> 00:42:12.640
you know,
twice the number of data than the number of parameters.
Um,

581
00:42:12.950 --> 00:42:15.950
maybe 10 times is better.
Now,

582
00:42:16.130 --> 00:42:20.240
the way you train the neural net or this is,
is that a,

583
00:42:20.241 --> 00:42:24.020
have they have 10 or a hundred times more parameters than date.

584
00:42:24.290 --> 00:42:29.170
Exactly the opposite
and which,
you know,

585
00:42:30.840 --> 00:42:35.840
it has been one of the puzzles about the neuron NATO's how can you get something

586
00:42:36.021 --> 00:42:41.021
that really works when you have so much freedom in a scene on that a little

587
00:42:41.361 --> 00:42:44.900
Derek in general by somehow,
right,
exactly.
Do you think this,

588
00:42:44.980 --> 00:42:48.140
the stochastic nature of it is essential?
The randomness.

589
00:42:48.200 --> 00:42:53.170
So I think we have some initial understanding why this happens.
But um,

590
00:42:54.050 --> 00:42:59.050
one nice side effect of having this over parametrization more parameters than

591
00:42:59.511 --> 00:43:04.250
data is that when you look for the Minima of a loss,

592
00:43:04.251 --> 00:43:09.050
functional,
extra stochastic gradient descent is doing,
um,
you'll find,

593
00:43:09.051 --> 00:43:12.380
I,
I made some calculations based on

594
00:43:14.180 --> 00:43:19.180
some older basic theory move Algebra called the bizarre theory him.

595
00:43:19.790 --> 00:43:21.710
And that gives you an,
a,

596
00:43:21.711 --> 00:43:26.330
an estimate of the number of solution of a system of polynomial equation.
Anyway,

597
00:43:26.331 --> 00:43:31.331
the bottom line is that there are probably more minima for a typical deep

598
00:43:34.101 --> 00:43:38.240
networks.
Um,
then atoms in the universe.

599
00:43:39.500 --> 00:43:44.500
Just to say that I lost because of the over parametrization more global minimum

600
00:43:46.470 --> 00:43:48.230
zero minimum good meaning.

601
00:43:48.980 --> 00:43:53.180
So it's not mobile in their life.
Yeah,
a lot of them.

602
00:43:53.210 --> 00:43:57.890
So we have a lot of solutions so it's not so surprising that you can find them

603
00:43:57.920 --> 00:43:58.880
relatively is,

604
00:44:00.150 --> 00:44:03.910
and this is because of the overall parameters,
ish.

605
00:44:04.240 --> 00:44:08.700
They all PR parameterization sprinkles that entire space for solutions that

606
00:44:08.720 --> 00:44:12.590
pretty good.
So surprising.
It is like,
you know,

607
00:44:12.591 --> 00:44:16.400
if you have a system of linear equation and you have more unknowns than

608
00:44:16.401 --> 00:44:18.500
equations,
then you have,

609
00:44:18.501 --> 00:44:22.660
we know you have an infinite number of solutions and the,

610
00:44:23.090 --> 00:44:25.430
the question is to pick one that's another story,

611
00:44:25.431 --> 00:44:28.820
but have an infinite number of solutions so that a lot of,

612
00:44:29.200 --> 00:44:32.600
of value of euro knowns that satisfied the equation.

613
00:44:33.150 --> 00:44:37.320
But it's possible that there's a lot of those solutions that aren't very good.

614
00:44:37.400 --> 00:44:42.320
Uh,
what's surprising is it and why can you pick one that generalizes?

615
00:44:42.390 --> 00:44:46.560
While that's a separate question,
we'd support it.
Dancers.
Yep.

616
00:44:46.900 --> 00:44:47.880
One one,

617
00:44:47.881 --> 00:44:52.770
a theorem that people like to talk about that kind of inspires imagination of

618
00:44:52.771 --> 00:44:57.350
the power in your networks is the universality a university approximation there

619
00:44:57.800 --> 00:45:02.370
that you can approximate any computable function which just a finite number of

620
00:45:02.371 --> 00:45:07.200
neurons in a single hidden layer.
Do you find this theorem one surprising?

621
00:45:07.640 --> 00:45:11.370
Do you find it useful,
interesting,
inspiring?

622
00:45:12.570 --> 00:45:16.410
No,
they these who are now,
you know,
I never found it very surprising.

623
00:45:16.490 --> 00:45:21.490
It was known since the atcs since I entered the field because it's basically the

624
00:45:24.781 --> 00:45:26.850
same as via stress theory,

625
00:45:27.120 --> 00:45:32.120
which says that that can approximate any continuous function with a polynomial

626
00:45:33.121 --> 00:45:37.660
of sufficiently with a sufficient number of terms or norm news.
Yeah,

627
00:45:38.130 --> 00:45:41.490
it's basically the same and the proves a very similar,

628
00:45:41.700 --> 00:45:45.930
so your intuition was there was never any doubt that that works in theory could

629
00:45:46.140 --> 00:45:49.650
get,
I'd be very strong.
Approximately the question,

630
00:45:49.710 --> 00:45:54.570
the interesting question is that
if this theory

631
00:45:57.070 --> 00:45:58.530
says you can approximate fine,

632
00:45:58.531 --> 00:46:02.940
but when you ask how many neurons for instance,

633
00:46:03.150 --> 00:46:05.760
or in the case of Chordoma Khomeini monomials,

634
00:46:06.390 --> 00:46:09.270
I need to get a good approximation.

635
00:46:11.340 --> 00:46:16.340
Then it turns out that that depends on the dimensionality of your function,

636
00:46:18.061 --> 00:46:19.710
how many variables you have,

637
00:46:20.520 --> 00:46:24.840
but it depends on the dimensionality of your functioning in a bad way.

638
00:46:25.080 --> 00:46:28.650
It's footings.
And suppose you want a narrower,

639
00:46:28.651 --> 00:46:33.651
which is no worse than 10% in your approximation and come up with a net or the

640
00:46:36.331 --> 00:46:39.300
proximity or function within 10%.

641
00:46:40.440 --> 00:46:45.440
Then it turns out that the number of units you need are in the order of 10 to

642
00:46:46.501 --> 00:46:50.970
the dementia.
Now deep how many variables?
So if you have,
you know,

643
00:46:51.750 --> 00:46:56.700
two variables is the steward,
you have hundred units and okay,

644
00:46:57.270 --> 00:47:02.270
but if you have say 200 by 200 pixel image is now this is,

645
00:47:04.310 --> 00:47:06.390
you know,
40,000,
whatever.

646
00:47:06.970 --> 00:47:09.650
And we can go to the size of the universe pretty quickly.
Yeah.

647
00:47:09.790 --> 00:47:12.410
EXAC 10 to the 40,000 or something.

648
00:47:14.080 --> 00:47:19.020
And so this is called the curse of dimensionality,
not,

649
00:47:19.090 --> 00:47:20.570
you know,
quite appropriately.

650
00:47:22.200 --> 00:47:25.310
And the hope is with the extra layers,
you can,
uh,

651
00:47:26.580 --> 00:47:27.600
remove the curse.

652
00:47:28.020 --> 00:47:33.020
What we proved is that tissue have deep layers on heirarchical architecture,

653
00:47:33.731 --> 00:47:38.731
don't them with a local connectivity of the type of convolutional deep learning.

654
00:47:40.060 --> 00:47:45.060
And if you're dealing with a function that has this kind of hierarchical

655
00:47:45.911 --> 00:47:49.840
architecture,
then you avoid completely the curse.

656
00:47:50.710 --> 00:47:55.360
You spoken a lot about supervised deep learning.
Yeah.
Uh,
what are your thoughts,

657
00:47:55.361 --> 00:47:58.930
hopes,
views on the challenges of unsupervised learning,

658
00:47:59.820 --> 00:48:04.780
the,
with gans with a generative adversarial networks.

659
00:48:05.680 --> 00:48:09.530
Do you see those as distinct,
the,
the power of gans this,

660
00:48:09.670 --> 00:48:13.540
those as distinct from supervised methods in your networks?
Are they,

661
00:48:13.720 --> 00:48:15.790
are they all in the same representation?

662
00:48:15.791 --> 00:48:20.791
Ballpark Ganz is one way to get a estimation of a probability densities,

663
00:48:24.881 --> 00:48:29.650
which is somewhat new way people have not done before.

664
00:48:30.340 --> 00:48:32.500
I,
I don't know whether I'm,

665
00:48:33.250 --> 00:48:38.250
the suit really play an important role in intelligence or,

666
00:48:39.850 --> 00:48:43.660
um,
it's,
it's interesting.
I'm,

667
00:48:44.110 --> 00:48:47.920
I'm less enthusiastic about it to many people in the field.

668
00:48:48.670 --> 00:48:51.250
I have the feeling that many people in the field,
uh,

669
00:48:51.330 --> 00:48:56.330
I'm really impressed by the ability to have producing realistic looking images

670
00:48:58.990 --> 00:49:03.070
in this generic active way,
which describes the popularity and the methods.

671
00:49:03.071 --> 00:49:08.071
But you're saying that while that's exciting and called to look at it may not be

672
00:49:08.621 --> 00:49:11.140
the tool that's useful for four.

673
00:49:11.170 --> 00:49:15.550
So you described the kind of beautifully current supervised methods go and

674
00:49:15.610 --> 00:49:19.150
infinity in terms of number of labeled points and we really have to figure out

675
00:49:19.151 --> 00:49:24.151
how to go to and to one and you're thinking gans might help but they might not

676
00:49:24.401 --> 00:49:27.640
be the right I don't think in for that problem,

677
00:49:27.641 --> 00:49:31.510
which I really think is important.
I think they may have,
uh,

678
00:49:31.990 --> 00:49:36.850
they certainly have applications for instance in computer graphics.
And you know,

679
00:49:37.960 --> 00:49:42.960
I did work long ago which was a little bit similar in terms of saying,

680
00:49:44.861 --> 00:49:45.131
okay,

681
00:49:45.131 --> 00:49:50.131
I have a network and I present images and I can,

682
00:49:52.150 --> 00:49:56.860
um,
so the input,
it's images and output is for instance,
the pose of the image.

683
00:49:57.150 --> 00:50:02.020
You know,
a face a much is smiling is rotated 45 degrees or not.

684
00:50:02.580 --> 00:50:02.880
Uh,

685
00:50:02.880 --> 00:50:07.880
what about having a net or that I train with the same data set,

686
00:50:08.470 --> 00:50:10.540
but now I inverting input and output.

687
00:50:10.630 --> 00:50:15.630
Now the input is the pose or the expression in number certain numbers and the

688
00:50:17.201 --> 00:50:21.400
output is the image.
And they train it.
And we did pretty good,

689
00:50:21.401 --> 00:50:23.950
interesting results in terms of producing,

690
00:50:25.020 --> 00:50:29.120
but very rarely stick looking images was,
um,
you know,

691
00:50:29.290 --> 00:50:31.400
the less sophisticated mechanism,

692
00:50:31.940 --> 00:50:35.270
but the output was pretty less than gains,

693
00:50:35.271 --> 00:50:38.780
but the output to pretty much have the same quality.

694
00:50:38.840 --> 00:50:43.840
So I think for a computer graphics type application yet definitely gains can be

695
00:50:45.471 --> 00:50:49.510
quite useful.
And not only for that for,
but um,

696
00:50:49.910 --> 00:50:54.050
for,
you know,
helping for instance,

697
00:50:54.560 --> 00:50:59.560
on this problem and unsupervised example of reducing the number of labeled

698
00:51:00.051 --> 00:51:04.610
examples.
Um,
I think people,
uh,

699
00:51:04.730 --> 00:51:09.290
it's like they think they can get out more than they put in,

700
00:51:09.680 --> 00:51:10.513
you know,

701
00:51:10.950 --> 00:51:14.880
<v 0>it has no free lunches.
Right.
So what do you think,</v>

702
00:51:14.940 --> 00:51:17.170
what's your intuition?
Um,
uh,

703
00:51:17.350 --> 00:51:22.350
how can we slow the growth of ant infinity and supervise and to infinity and

704
00:51:23.880 --> 00:51:26.490
supervised learning?
So for example,
uh,

705
00:51:26.580 --> 00:51:30.120
Mobileye has very successfully,
I mean,

706
00:51:30.121 --> 00:51:35.040
essentially annotated large amounts of data to be able to drive a car.
Now,

707
00:51:35.500 --> 00:51:40.200
one thought is,
so we're trying to teach machines,
school of Ai,

708
00:51:40.950 --> 00:51:45.570
and we're trying to,
so how can we become better teachers?

709
00:51:45.571 --> 00:51:47.250
Maybe that's one,
one way.

710
00:51:47.320 --> 00:51:52.300
<v 1>No,
you got to your,
you know what,
I like that because when,
uh,</v>

711
00:51:53.430 --> 00:51:58.200
um,
again,
one caricature of the history of computer science,

712
00:51:58.210 --> 00:52:00.220
you could say is

713
00:52:01.060 --> 00:52:04.940
<v 0>begins with programmers.
Expensive.
Yep.</v>

714
00:52:05.330 --> 00:52:08.260
Continuous labelers cheap.
Yep.

715
00:52:09.610 --> 00:52:14.440
And the future it would be schools,
like we have four kids.

716
00:52:14.590 --> 00:52:18.490
Yeah.
Currently the labeling methods,

717
00:52:19.330 --> 00:52:24.260
we're not selective about which examples we,

718
00:52:24.550 --> 00:52:25.820
we teach network.
So it's,

719
00:52:25.870 --> 00:52:30.870
so I think the focus of making one shut and let networks I learn much faster as

720
00:52:31.391 --> 00:52:32.950
often on the architecture side.

721
00:52:33.580 --> 00:52:37.550
But how can we pick better examples of the wish to learn?
Uh,

722
00:52:37.760 --> 00:52:40.570
do you have intuitions about that?
Well,
that's part of the

723
00:52:41.670 --> 00:52:45.030
<v 1>part of the problem,
but the other one is,
um,
you know,</v>

724
00:52:45.270 --> 00:52:49.140
if we look at,
um,
biology,

725
00:52:50.290 --> 00:52:54.530
a reasonable assumption I think is,
um,

726
00:52:56.130 --> 00:53:01.130
in the same spirit of that I said evolution is opportunistic and has week

727
00:53:02.071 --> 00:53:03.350
prior's.
You know,

728
00:53:03.360 --> 00:53:08.250
the way I think the intelligence of a child,

729
00:53:08.251 --> 00:53:11.350
the baby may develop is,
um,

730
00:53:12.090 --> 00:53:13.620
by bootstrapping.

731
00:53:13.840 --> 00:53:18.400
<v 0>Hmm.
Week.
Prior's from evolution for instance.</v>

732
00:53:19.120 --> 00:53:21.700
Um,
in that

733
00:53:23.470 --> 00:53:24.520
you can assume that that

734
00:53:24.520 --> 00:53:29.520
<v 1>you have in organisms including human babies builtin some basic machinery to</v>

735
00:53:33.930 --> 00:53:36.750
detect motion and relative motion.

736
00:53:38.220 --> 00:53:39.870
And the effect that is,
you know,

737
00:53:39.871 --> 00:53:44.871
we know all insects from fruit flies are other animals.

738
00:53:45.150 --> 00:53:49.130
They have this,
uh,

739
00:53:49.750 --> 00:53:52.910
even in the rightness of it,
in the very peripheral part,

740
00:53:53.080 --> 00:53:58.030
it's very conserved across species.
Something that evolution discovered early.

741
00:53:59.110 --> 00:54:04.110
He may be the reason why babies tend to look in the first few days to moving

742
00:54:05.201 --> 00:54:07.860
objects and not to not moving on.

743
00:54:08.380 --> 00:54:11.620
Now moving objects means okay there are attracted by motion,

744
00:54:12.230 --> 00:54:17.230
but motion also means that motion gives automatic segmentation from the

745
00:54:18.521 --> 00:54:23.140
background.
So because of motion boundaries,

746
00:54:23.620 --> 00:54:23.831
you know,

747
00:54:23.831 --> 00:54:28.831
either the object is moving or the Aa of the BBC tracking the moving object and

748
00:54:30.881 --> 00:54:32.620
the background is moving.
Right?

749
00:54:32.850 --> 00:54:36.510
<v 0>Yeah.
So just purely on the visual characteristics of the scene,</v>

750
00:54:36.511 --> 00:54:37.120
that seems to be like

751
00:54:37.120 --> 00:54:39.460
<v 1>most useful,
right?
So it's like looking at</v>

752
00:54:41.040 --> 00:54:45.730
an object without backgrounds.
Tobacco,
it's ideal for learning the object.

753
00:54:45.731 --> 00:54:49.810
Otherwise it's really difficult because you have so much stuff.

754
00:54:50.350 --> 00:54:53.800
So suppose you do this at the beginning of the first weeks.

755
00:54:55.090 --> 00:55:00.020
Then after that you can recognize object.
Now they're imprinted the number of,

756
00:55:02.110 --> 00:55:05.110
even in the background,
even without motion.

757
00:55:05.740 --> 00:55:10.540
<v 0>So that's at the,
by the way,
I just want to ask on a object recognition problem.</v>

758
00:55:10.870 --> 00:55:15.870
So there is this being responsive to movement and as you detection essentially,

759
00:55:16.690 --> 00:55:21.690
what's the gap between being effectively effective at visually recognizing

760
00:55:22.181 --> 00:55:26.680
stuff,
detecting word it is and understanding the scene.

761
00:55:27.490 --> 00:55:32.410
Is this a huge gap of many layers or is it,
are we,
is it close?

762
00:55:32.860 --> 00:55:34.570
<v 1>No,
I'm thinking that's a huge guy.</v>

763
00:55:35.060 --> 00:55:37.940
I'm think present I'll go to,

764
00:55:38.180 --> 00:55:43.180
with all the success that we have and the fact that a lot of very useful,

765
00:55:45.130 --> 00:55:49.560
I think we are,
we are in a golden age for application,
so of um,

766
00:55:50.200 --> 00:55:54.550
low level vision and low level speech recognition and so on,
you know,

767
00:55:54.551 --> 00:55:57.820
Alexa and so on,
um,
that are many more things.

768
00:55:57.821 --> 00:56:02.020
So similar level to be done including medical diagnosis and so on.

769
00:56:02.021 --> 00:56:07.021
But we are far from what we call understanding of a scene of language,

770
00:56:08.320 --> 00:56:11.750
of actions of people.
Uh,

771
00:56:11.920 --> 00:56:15.400
that is despite the claims that's,

772
00:56:15.670 --> 00:56:19.930
I think they're very far or a little bit off.
So

773
00:56:19.990 --> 00:56:22.900
<v 0>in popular culture and among many researchers,</v>

774
00:56:23.110 --> 00:56:28.080
some of which I spoke with the sewer Russell and Eli Musk,
uh,

775
00:56:28.750 --> 00:56:30.520
in and out of the AI field,

776
00:56:30.850 --> 00:56:35.850
there's a concern about the existential threat of AI and how do you think about

777
00:56:37.001 --> 00:56:38.880
this concern in,

778
00:56:39.970 --> 00:56:44.500
and is it valuable to think about large scale,

779
00:56:44.770 --> 00:56:49.770
longterm unintended consequences of intelligence system?

780
00:56:50.230 --> 00:56:55.030
<v 1>So we tried to build,
I always think it's better to worry first,</v>

781
00:56:55.110 --> 00:56:59.500
you know,
early rather than late.
So some worry is good.

782
00:57:00.370 --> 00:57:03.880
I'm not against wearing at all.
Personally.

783
00:57:04.330 --> 00:57:08.380
I think that,
um,
uh,
you know,

784
00:57:08.381 --> 00:57:13.381
it will take a long time before there is a real reason to be worried.

785
00:57:15.900 --> 00:57:16.830
But as I said,

786
00:57:16.831 --> 00:57:21.831
that I think is good to put in place and think about possible safety against,

787
00:57:23.410 --> 00:57:24.320
uh,
uh,

788
00:57:24.360 --> 00:57:29.360
what I find a bit misleading or things like that have been said about people I

789
00:57:30.091 --> 00:57:32.140
know,
like Elon Musk and uh,

790
00:57:32.640 --> 00:57:37.170
what he's Bostrom in particular,
northeast first name know nick,

791
00:57:37.410 --> 00:57:41.670
Nick Bostrom,
right.
Um,
you know,
and the couple of other people that,

792
00:57:41.671 --> 00:57:46.380
for instance,
um,
AI is more than just the nuclear weapons,
right?

793
00:57:46.620 --> 00:57:46.890
Yeah.

794
00:57:46.890 --> 00:57:51.890
I think that's really a problem that can be misleading,

795
00:57:52.440 --> 00:57:55.260
right?
Because in terms of priority,

796
00:57:55.261 --> 00:57:59.970
which should still be more worried about nuclear weapons and uh,

797
00:58:00.390 --> 00:58:04.080
you know,
people are doing about it and so on.
Then yeah.

798
00:58:06.130 --> 00:58:06.963
<v 0>Uh,</v>

799
00:58:08.300 --> 00:58:11.760
spoken about Dennis Savvis and yourself seeing,
uh,

800
00:58:12.120 --> 00:58:17.120
that you think it'd be about a hundred years out before we have s and general

801
00:58:17.991 --> 00:58:20.150
intelligence system that's on par with a human being.

802
00:58:20.600 --> 00:58:24.950
Do you have any updates for those predictions?
What I think he said,
he said 28,

803
00:58:25.060 --> 00:58:29.530
<v 1>it said 20,
right?
This was a couple of years ago.
I have not asked him again.
So</v>

804
00:58:30.870 --> 00:58:32.700
<v 0>should I,
your own prediction,</v>

805
00:58:34.770 --> 00:58:38.770
what's your prediction about when you'll be truly surprised?
Uh,

806
00:58:38.830 --> 00:58:41.710
and what's the confidence interval on that?

807
00:58:42.760 --> 00:58:46.500
<v 1>And I,
it's so difficult to predict the future.
Even the presence,</v>

808
00:58:47.120 --> 00:58:51.310
it's pretty hard to predict,
but I would be,
as I said,

809
00:58:51.380 --> 00:58:53.440
this is completely is,
oh,

810
00:58:53.450 --> 00:58:56.520
it'd be more like a Rod Brooks,

811
00:58:56.940 --> 00:58:58.470
I think he's about 200

812
00:59:01.520 --> 00:59:06.440
<v 0>when we have this kind of Agi system,
artificial general intelligence system.</v>

813
00:59:06.830 --> 00:59:11.090
You're sitting in a room with,
uh,
her,
him at.

814
00:59:12.770 --> 00:59:15.260
Do you think it will be a,

815
00:59:15.261 --> 00:59:19.010
the underlying design and such a system is something we'll be able to
understand.

816
00:59:19.070 --> 00:59:23.720
It will be simple.
Do you think be explainable,

817
00:59:25.760 --> 00:59:28.820
understandable by us?
Yay.
Intuition.
Again,

818
00:59:28.850 --> 00:59:32.340
we're in the realm of philosophy a little bit.
What

819
00:59:32.850 --> 00:59:37.470
<v 1>Billy?
No,
but again,</v>

820
00:59:37.471 --> 00:59:42.000
it depends what you really mean for understanding.

821
00:59:42.001 --> 00:59:43.820
So I think,
uh,

822
00:59:46.890 --> 00:59:51.320
you know,
we don't,
and there's 10 what,

823
00:59:51.480 --> 00:59:55.590
how deep networks work.
I think we're beginning to have a theory now,

824
00:59:56.460 --> 01:00:01.320
but in the case of deep networks,
or even in the case of the simple,

825
01:00:01.530 --> 01:00:05.430
simpler canon machines or leaner classifier,

826
01:00:06.330 --> 01:00:11.330
we really don't understand the individual units also we,

827
01:00:12.480 --> 01:00:14.270
but we understand,
you know,

828
01:00:14.330 --> 01:00:19.330
the computation and the limitations and the properties of it are a,

829
01:00:20.380 --> 01:00:23.130
it's similar to manifest things in a we,

830
01:00:24.060 --> 01:00:25.870
what does it mean when the stand,

831
01:00:25.900 --> 01:00:30.540
how a fusion bomb works.
How many of us,

832
01:00:31.420 --> 01:00:32.253
you know,

833
01:00:32.580 --> 01:00:37.580
many of us understand the basic principle and some of us may understand deeper

834
01:00:39.361 --> 01:00:40.350
details.

835
01:00:40.650 --> 01:00:44.430
<v 0>In that sense,
understanding is as a community,
as a civilization,</v>

836
01:00:44.431 --> 01:00:48.510
can we build another copy of it?
Okay.
And in that sense,

837
01:00:48.670 --> 01:00:50.670
do you think there'll be,

838
01:00:50.700 --> 01:00:55.170
there'll need to be some evolutionary component where it runs away from our

839
01:00:55.171 --> 01:00:59.820
understanding or do you think it could be engineered from the ground up the same

840
01:00:59.821 --> 01:01:02.140
way you go from the transistor drop point?

841
01:01:02.470 --> 01:01:06.480
<v 1>It's so many years ago,
this was actually,
let me see,</v>

842
01:01:06.481 --> 01:01:11.481
40 41 years ago I wrote a paper with a David Marr was um,

843
01:01:15.060 --> 01:01:18.600
one of the founding father of computer vision competition.

844
01:01:18.601 --> 01:01:23.601
At least I wrote a paper about levels of understanding,

845
01:01:23.821 --> 01:01:28.280
which is relate to the question of discussed earlier about understanding power

846
01:01:28.281 --> 01:01:32.620
point or this transistors and so on.
And uh,

847
01:01:33.090 --> 01:01:34.410
uh,
you know,

848
01:01:34.411 --> 01:01:39.411
in that kind of framework we had the level of the hardware and the top level of

849
01:01:39.901 --> 01:01:44.901
the algorithms we did not have learning recently.

850
01:01:45.930 --> 01:01:50.930
I updated adding levels and one level I added to those free was learning.

851
01:01:54.330 --> 01:01:56.210
So,
and you can imagine,

852
01:01:56.310 --> 01:02:01.310
you could have a good understanding of how you construct learning machine like

853
01:02:02.821 --> 01:02:03.654
we do,

854
01:02:04.980 --> 01:02:09.980
but being enabled to describe in detail what the lend me machines we'll discover

855
01:02:12.330 --> 01:02:14.010
right now,

856
01:02:14.011 --> 01:02:18.900
that would be still a powerful understanding if I can build a learning machine,

857
01:02:19.440 --> 01:02:24.440
even if I don't understand in detail every time made it to learn something.

858
01:02:26.180 --> 01:02:27.770
<v 0>Just like our children,
if they,</v>

859
01:02:27.771 --> 01:02:31.310
if they start listening to a certain type of music,

860
01:02:31.370 --> 01:02:33.500
I don't know Miley Cyrus or something.

861
01:02:33.710 --> 01:02:37.640
You don't understand why they came to that particular preference,

862
01:02:37.641 --> 01:02:42.290
but you understand the learning process.
That's very interesting.
Yeah.
Yup.
So,

863
01:02:42.870 --> 01:02:43.703
uh,

864
01:02:44.910 --> 01:02:49.610
on learning for systems to be part of our world,

865
01:02:50.430 --> 01:02:51.920
it has a certain,

866
01:02:52.340 --> 01:02:56.480
one of the challenging things that you've spoken about is learning ethics,

867
01:02:56.930 --> 01:02:58.910
learning morals,

868
01:02:59.480 --> 01:03:02.690
and w w how hard do you think is the problem of,

869
01:03:04.040 --> 01:03:06.590
first of all,
humans understanding or ethics,

870
01:03:06.830 --> 01:03:10.310
what is the origin and the neural on low level of ethics?

871
01:03:10.610 --> 01:03:12.170
What is it at the higher level?

872
01:03:12.470 --> 01:03:15.460
Is that something that's learnable for machines and your

873
01:03:15.530 --> 01:03:19.460
<v 1>intuition?
I think,
yeah,</v>

874
01:03:19.461 --> 01:03:24.230
it thinks he's learnable,
very likely.
Um,
I,

875
01:03:24.330 --> 01:03:26.950
I think I is one of these problems where

876
01:03:29.050 --> 01:03:31.790
I think understanding the

877
01:03:33.320 --> 01:03:37.550
neuroscience of ethics,
you know,
people discuss,

878
01:03:37.570 --> 01:03:42.350
there is an ethics of neuroscience.
Yeah.
Yes.

879
01:03:42.500 --> 01:03:45.650
You know,
how a neuroscientist should,
those should not behave,

880
01:03:46.430 --> 01:03:50.530
can think of a neurosurgeon and the ethics rules,

881
01:03:50.760 --> 01:03:53.130
he has to be a,
he,
she has to be.

882
01:03:53.930 --> 01:03:57.660
But I'm more interested in the neuroscience of it

883
01:03:57.840 --> 01:04:01.380
<v 0>on my mind right now.
The neuroscience of ethics.
It's very matter.
Yeah.</v>

884
01:04:01.460 --> 01:04:02.390
<v 1>And uh,
you know,</v>

885
01:04:02.391 --> 01:04:06.350
I think that would be important to understand also for being able to,

886
01:04:07.070 --> 01:04:10.280
to design machines that have,

887
01:04:10.580 --> 01:04:14.360
that are ethical machines in our sense of ethics.

888
01:04:15.140 --> 01:04:19.730
And you think there is something in neuroscience,
there's patterns,

889
01:04:20.390 --> 01:04:25.390
tools in euro size that can help us shed some light on ethics or is it mostly on

890
01:04:26.721 --> 01:04:30.600
the psychologist as sociology in which higher level know that he's protect

891
01:04:30.620 --> 01:04:34.520
called you.
But there is also in the meantime that are,
um,

892
01:04:35.150 --> 01:04:40.150
that he's evidence if a high of a specific areas of the brain that are involved

893
01:04:42.081 --> 01:04:45.410
in certain ethical judgment and not only this,

894
01:04:45.411 --> 01:04:50.411
you can stimulate those area with magnetic fields and change the ethical

895
01:04:51.591 --> 01:04:55.450
decisions.
Yeah.
Okay.
Wow.

896
01:04:56.330 --> 01:05:00.350
So that's a work by a colleague of mine,
Rebecca sacs,

897
01:05:00.800 --> 01:05:05.800
and there is a sort of searches doing similar work and I think,

898
01:05:06.321 --> 01:05:10.090
you know,
this is the beginning,
but um,

899
01:05:10.370 --> 01:05:15.370
ideally at some point we'll have an understanding of how this works and white

900
01:05:16.260 --> 01:05:17.290
evolved.
Right.

901
01:05:18.490 --> 01:05:21.850
<v 0>The big why question.
Yeah.
It must have some,
some purpose.</v>

902
01:05:21.950 --> 01:05:26.740
<v 1>Yeah.
Obviously it has,
you know,
some social purpose is a</v>

903
01:05:28.980 --> 01:05:29.813
probably

904
01:05:30.090 --> 01:05:34.650
<v 0>if neuroscience holds the key to at least eliminate some aspect of ethics,</v>

905
01:05:34.651 --> 01:05:38.160
that means it could be a learnable problem.
Yup.
Exactly.

906
01:05:38.820 --> 01:05:42.750
And as we're getting into harder and harder questions,
let's go,
uh,

907
01:05:42.820 --> 01:05:45.360
to the hard problem of consciousness.
Yeah.
Uh,

908
01:05:45.430 --> 01:05:50.430
is this an important problem for us to think about and solve on the engineering

909
01:05:51.390 --> 01:05:55.140
of intelligence side of your work?
Of our dream?

910
01:05:55.890 --> 01:05:59.360
<v 1>You know,
it's clear.
So,
you know,
again,</v>

911
01:05:59.390 --> 01:06:04.390
this is a deep problem partly because it's very difficult to define

912
01:06:05.721 --> 01:06:07.300
consciousness and the,

913
01:06:10.600 --> 01:06:15.600
and that is the debate among a neuroscientist and a about what the consciousness

914
01:06:21.371 --> 01:06:22.990
and feed also for us of course,

915
01:06:23.060 --> 01:06:28.060
whether consciousness is something that requires flesh and blood so to speak or

916
01:06:33.070 --> 01:06:34.780
could be,
you know,

917
01:06:35.410 --> 01:06:40.410
that we could have silicon devices that are conscious or up to statement.

918
01:06:42.841 --> 01:06:47.841
Like everything has some degree of consciousness and some more than others.

919
01:06:48.480 --> 01:06:52.230
This is like Giulio Tononi and uh,

920
01:06:52.590 --> 01:06:54.390
she would just

921
01:06:54.620 --> 01:06:56.630
<v 0>recently talked to a Christof Koch.
Okay.</v>

922
01:06:56.960 --> 01:06:59.800
So Christopher was my first graduate student.

923
01:07:00.620 --> 01:07:05.620
Do you think it's important to eliminate aspects of consciousness in order to

924
01:07:08.090 --> 01:07:09.710
engineer intelligence systems?

925
01:07:10.280 --> 01:07:14.450
Do you think an intelligence system would ultimately have cautiousness?

926
01:07:14.480 --> 01:07:16.130
Are they too,
are they interlinked?

927
01:07:18.460 --> 01:07:22.810
<v 1>You know,
most of the people working in artificial intelligence,</v>

928
01:07:22.811 --> 01:07:24.070
I think we're done,
sir.

929
01:07:24.460 --> 01:07:29.110
We don't strictly need consciousness to have an intelligence system.

930
01:07:30.030 --> 01:07:33.400
<v 0>That's sort of the easier question because yeah.
Cause it's,</v>

931
01:07:33.470 --> 01:07:35.940
it's a very engineering answer to the question.
Yes.

932
01:07:36.010 --> 01:07:39.510
Facet during tasks on onion consciousness.
But if you were to go,

933
01:07:40.890 --> 01:07:44.280
you think it's possible that we need to have

934
01:07:46.170 --> 01:07:47.140
that kind of self awareness?

935
01:07:47.170 --> 01:07:51.820
<v 1>That's it.
We May,
yes.
So for instance,
I,</v>

936
01:07:52.240 --> 01:07:57.240
I personally think that when test and machine or a person in a Turing test in an

937
01:08:00.851 --> 01:08:02.540
extended Turing testing,

938
01:08:03.550 --> 01:08:08.550
I think consciousness is part of what we require in that test.

939
01:08:10.950 --> 01:08:11.470
You know,

940
01:08:11.470 --> 01:08:16.430
implicitly to say that this is intelligent disagrees.

941
01:08:17.150 --> 01:08:17.890
So yes,

942
01:08:17.890 --> 01:08:22.890
he does it that despite many other romantic notions he holds,

943
01:08:23.420 --> 01:08:28.190
he disagrees with that one.
Yes.
So,
you know,
we'd see.

944
01:08:29.840 --> 01:08:32.870
Do you think,
as a quick question,

945
01:08:34.610 --> 01:08:36.680
Ernest Becker,
fear of death,

946
01:08:38.300 --> 01:08:43.300
do you think mortality and those kinds of things that are important for

947
01:08:45.890 --> 01:08:48.440
well,
for consciousness and for intelligence,

948
01:08:49.130 --> 01:08:54.130
the finiteness of life finiteness of existence or is that just a side effect of

949
01:08:55.881 --> 01:09:00.830
evolution,
evolutionary side effect that's useful to,
uh,
for natural selection?

950
01:09:01.100 --> 01:09:03.220
Do you think this kind of thing that we're going to,

951
01:09:03.230 --> 01:09:06.170
this interview is going to run out of time soon.
Our life.

952
01:09:06.171 --> 01:09:07.790
We'll run out of time soon.

953
01:09:08.090 --> 01:09:11.990
Do you think that's needed to make this conversation good and,
and life good?

954
01:09:12.020 --> 01:09:16.700
You know,
I never fought the boat,
isn't that I stink,
which,
and I think,
uh,

955
01:09:17.390 --> 01:09:21.200
uh,
Steve Jobs in his commencement speech,

956
01:09:21.201 --> 01:09:25.130
it's time for our good that,
you know,

957
01:09:25.131 --> 01:09:28.280
having a finite life was important for,

958
01:09:28.700 --> 01:09:31.430
for stimulating achievements.
It's on,
it was a different,

959
01:09:31.700 --> 01:09:35.040
I live everyday like it's your last,
yeah.
So,

960
01:09:36.140 --> 01:09:37.490
and I should the,

961
01:09:37.491 --> 01:09:42.491
I don't think strictly you need more vitality for consciousness,

962
01:09:43.190 --> 01:09:45.890
but
who knows,

963
01:09:45.920 --> 01:09:49.550
they seem to go together in our biological systems,
right?
Yep,
Yep.

964
01:09:51.260 --> 01:09:55.970
You've mentioned,
uh,
before and students are associated with,

965
01:09:56.340 --> 01:10:01.220
uh,
the Alphago and mobilize the big recent success stories in Ai.

966
01:10:01.310 --> 01:10:05.720
I think it's captivated the entire world of what AI can do.

967
01:10:05.990 --> 01:10:10.990
So what do you think will be the next breakthrough and what's your intuition

968
01:10:11.511 --> 01:10:14.060
about the next breakthrough?
Of course,

969
01:10:14.061 --> 01:10:19.060
I don't know where the next breakthroughs is.
A,
I think that,
um,

970
01:10:19.640 --> 01:10:21.550
that he's a good chance,
as I said before,
the,

971
01:10:21.551 --> 01:10:26.000
the next break fruits also be inspired by,
you know,
neuroscience,

972
01:10:27.920 --> 01:10:31.970
but which one?
I don't know.

973
01:10:32.300 --> 01:10:33.040
And there's,

974
01:10:33.040 --> 01:10:37.460
so MIT has this quest for intelligence and there's a few moon shots,

975
01:10:37.490 --> 01:10:41.810
which in that spirit to which ones are you excited about?
What a,

976
01:10:41.811 --> 01:10:44.290
which projects kind of,
uh,
well,

977
01:10:44.300 --> 01:10:48.410
of course I'm excited about one of the moon with,

978
01:10:48.770 --> 01:10:51.740
we choose our center for brains,
minds and machines.

979
01:10:52.630 --> 01:10:57.020
That one which is feeling fully funded by NSF.

980
01:10:57.590 --> 01:11:02.240
Um,
and it's,
uh,
it is about visual intelligence.

981
01:11:02.700 --> 01:11:07.300
It's an that one is particularly about understanding visual in villages.

982
01:11:07.310 --> 01:11:09.590
So the visual cortex and,

983
01:11:10.230 --> 01:11:12.950
and visual intelligence in the,

984
01:11:13.380 --> 01:11:18.380
of how we look around ourselves and understand the word [inaudible],

985
01:11:22.520 --> 01:11:26.490
meaning what,
what is going on,
how we could,

986
01:11:27.220 --> 01:11:31.950
um,
go from here to there without hitting obstacles.
Um,

987
01:11:32.220 --> 01:11:36.070
you know,
whether that are other agents,
people in the India,

988
01:11:36.480 --> 01:11:40.470
these are all things that we perceive very quickly.

989
01:11:41.220 --> 01:11:42.150
And um,

990
01:11:43.350 --> 01:11:47.610
and it's something actually quite close to being conscious.
Not Quite,

991
01:11:47.640 --> 01:11:52.140
but you know,
there is this interesting experiment that was run at Google x,

992
01:11:53.690 --> 01:11:57.250
which is in a sense is just,
Eh,

993
01:11:57.570 --> 01:11:58.860
virtual reality experiment.

994
01:11:58.861 --> 01:12:03.861
But in which the had a subject sitting in a chair with the goggles,

995
01:12:05.490 --> 01:12:07.020
like calculus and so on,

996
01:12:09.610 --> 01:12:10.620
earphones.

997
01:12:11.760 --> 01:12:16.760
And they were seeing through the eyes of our whole boat nearby two cameras,

998
01:12:18.420 --> 01:12:19.890
microphones for a CV.

999
01:12:19.891 --> 01:12:24.891
So their sensory system was there and the impression of all the subject very

1000
01:12:27.481 --> 01:12:28.060
strong,

1001
01:12:28.060 --> 01:12:33.060
they could not shake it off was that they were where the robot wars,

1002
01:12:35.190 --> 01:12:40.190
they could look at themselves from the robot and steel few day where they were,

1003
01:12:41.761 --> 01:12:44.070
where the robot is still looking at their body,

1004
01:12:45.900 --> 01:12:47.790
their self worth had moved.

1005
01:12:48.480 --> 01:12:53.450
<v 0>Some aspect of seeing,
understanding has to have ability to place yourself,</v>

1006
01:12:54.590 --> 01:12:59.000
uh,
have a self awareness about your position in the world and what the world is,

1007
01:12:59.030 --> 01:13:01.000
right?
So yeah,

1008
01:13:01.400 --> 01:13:05.210
so we may have to solve the hard problem of consciousness to solve them on their

1009
01:13:05.211 --> 01:13:07.560
way,
but it's quite,
quite a moonshot eyes.

1010
01:13:07.760 --> 01:13:11.600
So you've been an advisor to some incredible minds,

1011
01:13:12.400 --> 01:13:16.370
including Dennis,
a Sabas,
Christof Koch.
I'm not shy.
Schwab,
like you said,

1012
01:13:17.330 --> 01:13:21.350
all went on to become seminal figures in their respective fields.

1013
01:13:21.980 --> 01:13:26.980
From your own success as a researcher and from perspective as a mentor of these

1014
01:13:27.530 --> 01:13:30.380
researchers,
having guided them

1015
01:13:32.910 --> 01:13:34.160
in the way of advice,

1016
01:13:34.170 --> 01:13:37.710
what does it take to be successful in science and engineering careers?

1017
01:13:39.780 --> 01:13:44.580
Whether you're talking to somebody in their teens,
twenties and thirties,

1018
01:13:44.640 --> 01:13:45.930
what does that path look like?

1019
01:13:48.160 --> 01:13:51.280
<v 1>It's curiosity and having fun</v>

1020
01:13:53.200 --> 01:13:55.510
and I think he's important.

1021
01:13:55.511 --> 01:14:00.511
Also having fun with other curious minds.

1022
01:14:02.410 --> 01:14:07.300
It's the people who surround with too.
So yeah.
Fun.
And curiosity is there

1023
01:14:08.790 --> 01:14:12.340
<v 0>mentioned Steve Jobs,
is there also underlying ambition</v>

1024
01:14:13.120 --> 01:14:17.260
<v 1>that's unique that you saw or is it really does boil down to insatiable</v>

1025
01:14:17.261 --> 01:14:21.520
curiosity and fun when of course,
you know,
it's been cooked.

1026
01:14:21.560 --> 01:14:26.020
Curious in a active and ambitious way.
Yes,

1027
01:14:26.260 --> 01:14:28.000
the um,

1028
01:14:28.570 --> 01:14:33.570
definitely but I think sometime mean in science and that are friends of mine

1029
01:14:35.860 --> 01:14:38.980
like this,
um,
you know,

1030
01:14:38.981 --> 01:14:43.981
that are some of the scientists like to work by themselves and kind of

1031
01:14:45.251 --> 01:14:46.560
communicate,
um,

1032
01:14:47.320 --> 01:14:52.320
only when they completed their work or discover something.

1033
01:14:53.440 --> 01:14:57.020
Um,
I think I always found that the,

1034
01:14:57.210 --> 01:15:00.480
the actual process of,
uh,
you know,

1035
01:15:00.490 --> 01:15:05.490
discovering something is more fun if it's together with other intelligent and

1036
01:15:07.421 --> 01:15:10.930
curious and fun people.
So he see the fun in that process.

1037
01:15:11.290 --> 01:15:14.890
The side effect of that process will be that you'll actually end up discovering

1038
01:15:14.900 --> 01:15:18.390
something.
Yes.
So as a,
if lad

1039
01:15:20.650 --> 01:15:25.480
many incredible efforts here,
what's the secret to being a good advisor,

1040
01:15:25.481 --> 01:15:27.640
mentor,
leader in a research setting?

1041
01:15:28.160 --> 01:15:33.160
As a similar spirit or,
yeah.
What,
what,
what advice could you give to people,

1042
01:15:33.970 --> 01:15:35.290
Young Faculty and so on?

1043
01:15:35.920 --> 01:15:40.920
It's partly repeating what I said about an environment that should be friendly

1044
01:15:41.231 --> 01:15:45.840
and fun and ambitious and uh,
you know,

1045
01:15:46.620 --> 01:15:51.620
I think I learned a lot from some of my advisors and friends and some would have

1046
01:15:53.620 --> 01:15:57.910
physicists and the,
that it was,
for instance,
this,

1047
01:15:59.200 --> 01:16:00.033
um,

1048
01:16:00.400 --> 01:16:05.400
behavior that was encouraged of when somebody comes with a new idea in the

1049
01:16:05.501 --> 01:16:09.010
group,
you're,
unless he's already stupid,

1050
01:16:09.011 --> 01:16:12.550
but you are always enthusiastic and then,

1051
01:16:12.660 --> 01:16:16.000
and the all enthusiastic for a few minutes,
for a few hours.
Then you start,

1052
01:16:17.890 --> 01:16:22.540
you know,
asking critical you a few questions.
I tasted this,

1053
01:16:23.020 --> 01:16:26.950
but this is a process that is,
I think is very,

1054
01:16:27.490 --> 01:16:30.520
very good.
This,
you have to be enthusiastic.

1055
01:16:30.521 --> 01:16:35.500
Sometime people are very critical from the beginning.
That's not,

1056
01:16:36.220 --> 01:16:39.960
yes,
you have to give it a chance to see,
to grow that sad.

1057
01:16:39.960 --> 01:16:42.730
It was some of your ideas that you're quite revolutionary.

1058
01:16:42.731 --> 01:16:44.260
So there's eye witness,

1059
01:16:44.270 --> 01:16:46.960
especially in the human vision side and neuroscience side.

1060
01:16:47.260 --> 01:16:51.070
There can be some pretty heated arguments.
Um,
do you enjoy these?

1061
01:16:51.080 --> 01:16:53.360
Does that a part of science and that could uh,
yeah,

1062
01:16:53.440 --> 01:16:56.510
academic pursuits that you enjoy,
is it,

1063
01:16:58.030 --> 01:17:01.990
is that something that happens in your group as well?
Uh,
yeah,
absolutely.

1064
01:17:02.380 --> 01:17:06.820
I also spent some time in Germany.
Again,
there is this tradition in which people,

1065
01:17:07.450 --> 01:17:10.790
uh,
more forthright,

1066
01:17:11.450 --> 01:17:14.750
less kind than here.
Yeah.
So

1067
01:17:16.980 --> 01:17:20.080
you know,
in the u s I knew,
right.
A bad letter,

1068
01:17:20.110 --> 01:17:23.930
you still say this guy is nice know.
Yes.

1069
01:17:25.470 --> 01:17:26.140
So,

1070
01:17:26.140 --> 01:17:31.090
<v 0>yeah,
here in America is degrees of Nice.
It's all just degrees of Nicea.
Right?</v>

1071
01:17:31.290 --> 01:17:34.750
<v 1>Right.
So as long as this does not become personal</v>

1072
01:17:36.400 --> 01:17:38.890
and it's really like,
you know,

1073
01:17:39.730 --> 01:17:43.360
the football game with his rules.
That's great.

1074
01:17:44.660 --> 01:17:45.493
<v 0>It's fun.</v>

1075
01:17:46.380 --> 01:17:51.380
So if you some off on yourself in a position to ask one question of an oracle,

1076
01:17:51.800 --> 01:17:54.610
like a genie,
maybe a God,
whoa.

1077
01:17:55.490 --> 01:18:00.410
And you're guaranteed to get a clear answer,
what kind of question would you ask?

1078
01:18:01.320 --> 01:18:03.500
What would be the question you would ask

1079
01:18:04.470 --> 01:18:06.030
<v 1>in the spirit of our discussion?</v>

1080
01:18:06.031 --> 01:18:09.900
It could be how could be they become 10 times more intelligent.

1081
01:18:11.770 --> 01:18:16.090
<v 0>And so,
but see,
you only get a clear,
short answer.</v>

1082
01:18:16.210 --> 01:18:18.970
So do you think there's a clear short answer to that?
No.

1083
01:18:20.740 --> 01:18:23.020
And that's the answer you'll get.
Okay.

1084
01:18:23.650 --> 01:18:27.170
So you've mentioned a flowers of uh,
Elgar not oh yeah.

1085
01:18:28.000 --> 01:18:33.000
As a story that inspires you and you in your childhood as this a story of a

1086
01:18:36.341 --> 01:18:40.840
mouse and human achieving genius level intelligence and then understanding what

1087
01:18:40.841 --> 01:18:45.190
was happening while slowly becoming not intelligent again in this tragedy of

1088
01:18:45.490 --> 01:18:47.770
gaining intelligence and losing intelligence.

1089
01:18:48.580 --> 01:18:51.400
Do you think in that spirit and that story,

1090
01:18:51.430 --> 01:18:56.430
do you think intelligence is a gift or a curse from the perspective of happiness

1091
01:18:58.211 --> 01:19:00.410
and meaning of life?
You,

1092
01:19:00.420 --> 01:19:03.580
you tried to create intelligence system that understands the universe,

1093
01:19:03.850 --> 01:19:06.430
but on an individual level,
the meaning of life?

1094
01:19:06.460 --> 01:19:08.290
Do you think intelligence is a gift?

1095
01:19:10.780 --> 01:19:11.800
<v 1>That's a good question.</v>

1096
01:19:17.120 --> 01:19:17.953
I don't know.

1097
01:19:22.830 --> 01:19:23.663
<v 0>As one of the,</v>

1098
01:19:24.630 --> 01:19:29.120
as one people consider the smartest people in the world in some,

1099
01:19:29.310 --> 01:19:33.180
in some dimension,
at the very least.
Uh,
what do you think?

1100
01:19:34.850 --> 01:19:37.540
<v 1>I don't know.
He may be invariant to intelligence.</v>

1101
01:19:38.240 --> 01:19:41.050
Happiness would be nice if it were.

1102
01:19:43.670 --> 01:19:45.110
<v 0>That's the hope.
Yeah.</v>

1103
01:19:46.130 --> 01:19:50.540
You can be smart and happy and clueless.
I'm happy.
Yeah.

1104
01:19:51.800 --> 01:19:56.030
As always on the discussion or the meaning of life is probably a good place to

1105
01:19:56.031 --> 01:20:00.380
end.
Tommaso thank you so much for talking today.
This was great.

