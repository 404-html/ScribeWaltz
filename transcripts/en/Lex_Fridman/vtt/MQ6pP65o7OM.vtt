WEBVTT

1
00:00:00.030 --> 00:00:02.760
<v Speaker 1>Today we will talk about deep </v>
<v Speaker 1>reinforcement learning.</v>

2
00:00:07.060 --> 00:00:12.060
<v Speaker 1>The question we would like to explore is</v>
<v Speaker 1>to which degree we can teach systems to </v>

3
00:00:14.381 --> 00:00:19.381
<v Speaker 1>act,</v>
<v Speaker 1>to perceive and act in this world from </v>

4
00:00:19.381 --> 00:00:23.161
<v Speaker 1>data.</v>
<v Speaker 1>So let's take a step back and think of </v>

5
00:00:23.260 --> 00:00:28.260
<v Speaker 1>what is the full range of tasks than </v>
<v Speaker 1>artificial intelligence system needs to </v>

6
00:00:28.260 --> 00:00:31.420
<v Speaker 1>accomplish.</v>
<v Speaker 1>Here's the stack from top to bottom,</v>

7
00:00:31.660 --> 00:00:36.040
<v Speaker 1>top the input bottom output,</v>
<v Speaker 1>the environment at the top,</v>

8
00:00:36.050 --> 00:00:41.050
<v Speaker 1>the world that the agent is operating in</v>
<v Speaker 1>sensed by sensors taking in the world </v>

9
00:00:42.971 --> 00:00:46.660
<v Speaker 1>outside and converting it to raw data </v>
<v Speaker 1>and interpretable by machines,</v>

10
00:00:48.070 --> 00:00:51.580
<v Speaker 1>sensor data,</v>
<v Speaker 1>and from that raw sensor data,</v>

11
00:00:51.940 --> 00:00:56.940
<v Speaker 1>you extract features.</v>
<v Speaker 1>You extract structure from that data </v>

12
00:00:58.060 --> 00:01:01.450
<v Speaker 1>such that you can input it,</v>
<v Speaker 1>makes sense of it,</v>

13
00:01:01.750 --> 00:01:03.430
<v Speaker 1>discriminate,</v>
<v Speaker 1>separate,</v>

14
00:01:03.490 --> 00:01:08.490
<v Speaker 1>understand the data,</v>
<v Speaker 1>and as we discussed,</v>

15
00:01:09.720 --> 00:01:12.420
<v Speaker 1>you form higher and higher order </v>
<v Speaker 1>representations,</v>

16
00:01:12.750 --> 00:01:17.750
<v Speaker 1>a hierarchy of representations based on </v>
<v Speaker 1>which the machine learning techniques </v>

17
00:01:17.750 --> 00:01:22.431
<v Speaker 1>can then be applied.</v>
<v Speaker 1>Once the machine learning techniques,</v>

18
00:01:24.791 --> 00:01:27.250
<v Speaker 1>the understanding,</v>
<v Speaker 1>as I mentioned,</v>

19
00:01:27.520 --> 00:01:32.520
<v Speaker 1>converts the data into features,</v>
<v Speaker 1>into higher order representations and </v>

20
00:01:32.520 --> 00:01:34.450
<v Speaker 1>into simple actionable,</v>
<v Speaker 1>useful information.</v>

21
00:01:35.170 --> 00:01:37.780
<v Speaker 1>We aggregate that information into </v>
<v Speaker 1>knowledge,</v>

22
00:01:38.020 --> 00:01:43.020
<v Speaker 1>would take the pieces of knowledge </v>
<v Speaker 1>extracted from the data through the </v>

23
00:01:43.020 --> 00:01:45.820
<v Speaker 1>machine learning techniques and to build</v>
<v Speaker 1>a taxonomy,</v>

24
00:01:47.990 --> 00:01:52.990
<v Speaker 1>a library of knowledge,</v>
<v Speaker 1>and would that knowledge we reason an </v>

25
00:01:53.631 --> 00:01:58.631
<v Speaker 1>agent is tasked to reason,</v>
<v Speaker 1>to aggregate,</v>

26
00:01:59.600 --> 00:02:04.600
<v Speaker 1>to connect pieces of data seen in the </v>
<v Speaker 1>recent past or the distant past to make </v>

27
00:02:05.871 --> 00:02:10.871
<v Speaker 1>sense of the world that's operating in </v>
<v Speaker 1>and finally to make a plan of how to act</v>

28
00:02:10.881 --> 00:02:14.600
<v Speaker 1>in that world based on its objectives </v>
<v Speaker 1>based on what he wants to accomplish.</v>

29
00:02:14.930 --> 00:02:19.930
<v Speaker 1>As I mentioned,</v>
<v Speaker 1>a simple but commonly accepted </v>

30
00:02:19.930 --> 00:02:23.741
<v Speaker 1>definition of intelligence is a system </v>
<v Speaker 1>that's able to accomplish complex goals,</v>

31
00:02:24.830 --> 00:02:29.830
<v Speaker 1>so system that's operating in an </v>
<v Speaker 1>environment and this world must have a </v>

32
00:02:29.830 --> 00:02:33.521
<v Speaker 1>goal,</v>
<v Speaker 1>must have an objective function or </v>

33
00:02:33.521 --> 00:02:36.221
<v Speaker 1>reward function and based on that it </v>
<v Speaker 1>forms of plan and takes action and </v>

34
00:02:36.411 --> 00:02:39.350
<v Speaker 1>because they're operates in many cases </v>
<v Speaker 1>in the physical world,</v>

35
00:02:39.470 --> 00:02:44.470
<v Speaker 1>it must have tools,</v>
<v Speaker 1>effectors with which it applies to </v>

36
00:02:44.470 --> 00:02:46.340
<v Speaker 1>actions to change.</v>
<v Speaker 1>Something about the world.</v>

37
00:02:46.940 --> 00:02:51.940
<v Speaker 1>That's the full stack of an artificial </v>
<v Speaker 1>intelligence system that acts in the </v>

38
00:02:51.940 --> 00:02:55.400
<v Speaker 1>world.</v>
<v Speaker 1>And the question is,</v>

39
00:02:56.910 --> 00:03:01.910
<v Speaker 1>what kind of task can such a take on </v>
<v Speaker 1>what kind of tasks can an artificial </v>

40
00:03:02.981 --> 00:03:06.820
<v Speaker 1>intelligence system learn?</v>
<v Speaker 1>As we understand ai today,</v>

41
00:03:07.750 --> 00:03:12.750
<v Speaker 1>we will talk about the advancement of </v>
<v Speaker 1>deeper enforcement learning approaches </v>

42
00:03:12.850 --> 00:03:17.850
<v Speaker 1>and some of the fascinating ways it's </v>
<v Speaker 1>able to take much of the stack and treat</v>

43
00:03:18.521 --> 00:03:23.290
<v Speaker 1>it as an end to end learning problem.</v>
<v Speaker 1>But we look at games,</v>

44
00:03:23.440 --> 00:03:27.430
<v Speaker 1>we'll look at simple formalized worlds.</v>
<v Speaker 1>While it's still impressive,</v>

45
00:03:27.431 --> 00:03:29.950
<v Speaker 1>beautiful and unprecedented </v>
<v Speaker 1>accomplishments.</v>

46
00:03:30.880 --> 00:03:35.880
<v Speaker 1>It's nevertheless formal tasks.</v>
<v Speaker 1>Can we then move beyond games into </v>

47
00:03:37.450 --> 00:03:42.450
<v Speaker 1>expert tasks of medical diagnosis,</v>
<v Speaker 1>of design and into natural language,</v>

48
00:03:45.940 --> 00:03:49.210
<v Speaker 1>and finally the human level tasks of </v>
<v Speaker 1>emotion,</v>

49
00:03:49.360 --> 00:03:53.110
<v Speaker 1>imagination,</v>
<v Speaker 1>consciousness.</v>

50
00:03:56.650 --> 00:04:01.000
<v Speaker 1>That's once again,</v>
<v Speaker 1>review the stack and practicality in the</v>

51
00:04:01.001 --> 00:04:06.001
<v Speaker 1>tools.</v>
<v Speaker 1>We have the input for robots operating </v>

52
00:04:06.001 --> 00:04:10.840
<v Speaker 1>in the world from cars to humanoid to </v>
<v Speaker 1>drones as Lidar,</v>

53
00:04:10.870 --> 00:04:12.610
<v Speaker 1>camera radar,</v>
<v Speaker 1>gps,</v>

54
00:04:12.611 --> 00:04:15.580
<v Speaker 1>stereo cameras,</v>
<v Speaker 1>audio microphone,</v>

55
00:04:16.060 --> 00:04:21.060
<v Speaker 1>networking for communication,</v>
<v Speaker 1>and the various ways to measure </v>

56
00:04:21.060 --> 00:04:24.211
<v Speaker 1>kinematics with Imu.</v>
<v Speaker 1>The raw sensory data is then processed </v>

57
00:04:28.000 --> 00:04:33.000
<v Speaker 1>features.</v>
<v Speaker 1>The forum to representations are formed </v>

58
00:04:33.000 --> 00:04:33.400
<v Speaker 1>and multiple higher and higher order </v>
<v Speaker 1>representations.</v>

59
00:04:33.580 --> 00:04:36.700
<v Speaker 1>That's what deep learning gets us before</v>
<v Speaker 1>neural networks.</v>

60
00:04:36.701 --> 00:04:41.701
<v Speaker 1>Before the advent of,</v>
<v Speaker 1>before the recent successes of neural </v>

61
00:04:41.701 --> 00:04:46.021
<v Speaker 1>networks to go deeper and therefore be </v>
<v Speaker 1>able to form high order representations </v>

62
00:04:46.021 --> 00:04:50.170
<v Speaker 1>of the data that was done by experts by </v>
<v Speaker 1>human experts today,</v>

63
00:04:50.200 --> 00:04:55.200
<v Speaker 1>networks are able to do that.</v>
<v Speaker 1>That's the representation piece and on </v>

64
00:04:55.200 --> 00:04:57.370
<v Speaker 1>top of the representation piece,</v>
<v Speaker 1>the final layers.</v>

65
00:04:57.371 --> 00:05:01.340
<v Speaker 1>These networks are able to accomplish </v>
<v Speaker 1>the supervised learning task,</v>

66
00:05:01.390 --> 00:05:06.390
<v Speaker 1>the generative tasks,</v>
<v Speaker 1>and the unsupervised clustering tasks </v>

67
00:05:07.870 --> 00:05:09.400
<v Speaker 1>through machine learning.</v>

68
00:05:09.820 --> 00:05:14.820
<v Speaker 1>That's what we talked about a little in </v>
<v Speaker 1>lecture one and we'll continue tomorrow </v>

69
00:05:15.400 --> 00:05:20.040
<v Speaker 1>and Wednesday.</v>
<v Speaker 1>That's supervised learning and you could</v>

70
00:05:20.041 --> 00:05:24.010
<v Speaker 1>think about the output of those networks</v>
<v Speaker 1>as simple,</v>

71
00:05:24.011 --> 00:05:24.850
<v Speaker 1>clean,</v>
<v Speaker 1>useful,</v>

72
00:05:24.851 --> 00:05:29.851
<v Speaker 1>valuable information.</v>
<v Speaker 1>That's the knowledge and that knowledge </v>

73
00:05:30.610 --> 00:05:34.600
<v Speaker 1>can be in the form of single numbers.</v>
<v Speaker 1>It can be regression,</v>

74
00:05:34.601 --> 00:05:38.260
<v Speaker 1>continuous variables.</v>
<v Speaker 1>It can be a sequence of numbers,</v>

75
00:05:38.261 --> 00:05:41.680
<v Speaker 1>it can be images,</v>
<v Speaker 1>audio sentences,</v>

76
00:05:41.710 --> 00:05:44.410
<v Speaker 1>text,</v>
<v Speaker 1>speech.</v>

77
00:05:44.770 --> 00:05:47.830
<v Speaker 1>Once that knowledge is extracted and </v>
<v Speaker 1>aggregated,</v>

78
00:05:47.831 --> 00:05:51.460
<v Speaker 1>how do we connect it in multi </v>
<v Speaker 1>resolution?</v>

79
00:05:51.470 --> 00:05:55.240
<v Speaker 1>Always form hierarchies of ideas,</v>
<v Speaker 1>connect ideas.</v>

80
00:05:56.530 --> 00:06:00.440
<v Speaker 1>The trivial silly example is connecting </v>
<v Speaker 1>images,</v>

81
00:06:00.800 --> 00:06:03.320
<v Speaker 1>activity recognition and audio.</v>
<v Speaker 1>For example,</v>

82
00:06:04.130 --> 00:06:09.130
<v Speaker 1>if it looks like a duck,</v>
<v Speaker 1>quacks like a duck and swims like a </v>

83
00:06:09.130 --> 00:06:10.850
<v Speaker 1>duck,</v>
<v Speaker 1>we do not currently have approaches that</v>

84
00:06:10.851 --> 00:06:15.851
<v Speaker 1>effectively integrate this information </v>
<v Speaker 1>to produce a higher confidence estimate </v>

85
00:06:16.850 --> 00:06:19.820
<v Speaker 1>that is in fact the duck and the </v>
<v Speaker 1>planning piece.</v>

86
00:06:21.350 --> 00:06:24.470
<v Speaker 1>The task of taking the sensory </v>
<v Speaker 1>information,</v>

87
00:06:24.920 --> 00:06:29.920
<v Speaker 1>fusing the sensory information,</v>
<v Speaker 1>and making action control and longer </v>

88
00:06:29.920 --> 00:06:34.751
<v Speaker 1>term plans based on that information,</v>
<v Speaker 1>as we will discuss today are more and </v>

89
00:06:37.221 --> 00:06:40.610
<v Speaker 1>more amenable to the learning approach </v>
<v Speaker 1>to the deep learning approach,</v>

90
00:06:40.790 --> 00:06:45.790
<v Speaker 1>but to date have been the most </v>
<v Speaker 1>successful as non learning optimization </v>

91
00:06:45.790 --> 00:06:47.870
<v Speaker 1>based approaches like with the several </v>
<v Speaker 1>of the guest speakers would have,</v>

92
00:06:47.930 --> 00:06:52.880
<v Speaker 1>including the creator of this robot </v>
<v Speaker 1>atlas in Boston Dynamics.</v>

93
00:06:54.910 --> 00:06:59.500
<v Speaker 1>So the question how much of the stack </v>
<v Speaker 1>can be learned and to end from the input</v>

94
00:06:59.501 --> 00:07:04.501
<v Speaker 1>to the output.</v>
<v Speaker 1>We know we can learn the representation </v>

95
00:07:04.501 --> 00:07:08.370
<v Speaker 1>and the knowledge from the </v>
<v Speaker 1>representation and to knowledge even </v>

96
00:07:08.370 --> 00:07:11.761
<v Speaker 1>with the kernel methods of Svm and </v>
<v Speaker 1>certainly with with neural networks.</v>

97
00:07:14.560 --> 00:07:19.560
<v Speaker 1>Mapping from representation to </v>
<v Speaker 1>information has been where the primary </v>

98
00:07:21.311 --> 00:07:26.311
<v Speaker 1>success and machine learning over the </v>
<v Speaker 1>past three decades has been mapping from</v>

99
00:07:26.351 --> 00:07:31.030
<v Speaker 1>raw sensory data to knowledge.</v>
<v Speaker 1>That's where the success,</v>

100
00:07:31.031 --> 00:07:36.031
<v Speaker 1>the automated representation,</v>
<v Speaker 1>learning of deep learning has been a </v>

101
00:07:36.031 --> 00:07:40.861
<v Speaker 1>success,</v>
<v Speaker 1>going straight from raw data to </v>

102
00:07:40.861 --> 00:07:43.141
<v Speaker 1>knowledge.</v>
<v Speaker 1>The open question for us today and </v>

103
00:07:43.141 --> 00:07:47.491
<v Speaker 1>beyond is if we can expand the red box </v>
<v Speaker 1>there of what can be learned end to end </v>

104
00:07:47.491 --> 00:07:52.380
<v Speaker 1>from sensory data to reasoning,</v>
<v Speaker 1>so aggregating for me higher </v>

105
00:07:52.380 --> 00:07:56.521
<v Speaker 1>representations of the extract of </v>
<v Speaker 1>knowledge and forming plans and acting </v>

106
00:07:57.491 --> 00:08:02.491
<v Speaker 1>in this world from the raw sensory data,</v>
<v Speaker 1>we will show the incredible fact that </v>

107
00:08:02.491 --> 00:08:07.411
<v Speaker 1>we're able to do learn exactly what's </v>
<v Speaker 1>shown here and to end with deeper </v>

108
00:08:07.411 --> 00:08:11.800
<v Speaker 1>enforcement learning on trivial tasks in</v>
<v Speaker 1>a generalizable way.</v>

109
00:08:12.070 --> 00:08:17.070
<v Speaker 1>The question is whether that can then </v>
<v Speaker 1>move on to real world tasks of </v>

110
00:08:17.070 --> 00:08:19.210
<v Speaker 1>autonomous vehicles,</v>
<v Speaker 1>of humanoid,</v>

111
00:08:19.600 --> 00:08:21.100
<v Speaker 1>robotics and so on.</v>

112
00:08:23.760 --> 00:08:28.760
<v Speaker 1>That's the open question.</v>
<v Speaker 1>So today let's talk about reinforcement </v>

113
00:08:28.760 --> 00:08:29.700
<v Speaker 1>learning.</v>
<v Speaker 1>There's three types of machine learning.</v>

114
00:08:32.340 --> 00:08:37.340
<v Speaker 1>Supervised unsupervised are the </v>
<v Speaker 1>categories that the extremes in relative</v>

115
00:08:39.601 --> 00:08:44.070
<v Speaker 1>to the amount of human and human input </v>
<v Speaker 1>that's required for supervised learning.</v>

116
00:08:44.400 --> 00:08:49.400
<v Speaker 1>Every piece of data that's used for </v>
<v Speaker 1>teaching these systems is first labeled </v>

117
00:08:49.561 --> 00:08:54.561
<v Speaker 1>by human beings and unsupervised </v>
<v Speaker 1>learning on the right is no data is </v>

118
00:08:54.561 --> 00:08:59.511
<v Speaker 1>labeled by beings in between is some </v>
<v Speaker 1>sparse input from humans.</v>

119
00:09:01.470 --> 00:09:06.470
<v Speaker 1>Semi supervised learning is when only </v>
<v Speaker 1>part of the data is provided by humans.</v>

120
00:09:07.230 --> 00:09:11.640
<v Speaker 1>Ground Truth and the rest was being </v>
<v Speaker 1>first gen analyzed by the system,</v>

121
00:09:11.790 --> 00:09:16.790
<v Speaker 1>and that's what reinforcement learning </v>
<v Speaker 1>falls reinforcement learning as shown </v>

122
00:09:17.041 --> 00:09:20.010
<v Speaker 1>there with the cats.</v>
<v Speaker 1>As I said,</v>

123
00:09:20.011 --> 00:09:22.590
<v Speaker 1>every successful presentation must </v>
<v Speaker 1>include cats.</v>

124
00:09:24.030 --> 00:09:29.030
<v Speaker 1>They're supposed to be Pavlov's cats and</v>
<v Speaker 1>a ringing a bell,</v>

125
00:09:29.281 --> 00:09:34.281
<v Speaker 1>and every time they ring a bell,</v>
<v Speaker 1>they're given food and they learn this </v>

126
00:09:34.281 --> 00:09:37.371
<v Speaker 1>process.</v>
<v Speaker 1>The goal of reinforcement learning is to</v>

127
00:09:38.250 --> 00:09:43.250
<v Speaker 1>learn from sparse reward data from learn</v>
<v Speaker 1>from spar,</v>

128
00:09:43.841 --> 00:09:48.841
<v Speaker 1>supervised data,</v>
<v Speaker 1>and take advantage of the fact that in </v>

129
00:09:48.841 --> 00:09:52.641
<v Speaker 1>simulation or in the real world,</v>
<v Speaker 1>there is a temporal consistency to the </v>

130
00:09:52.641 --> 00:09:52.641
<v Speaker 1>world.</v>

131
00:09:52.641 --> 00:09:57.470
<v Speaker 1>There is a temporal dynamics that </v>
<v Speaker 1>follows from state to state to state </v>

132
00:09:57.470 --> 00:10:02.061
<v Speaker 1>through time and so you can propagate </v>
<v Speaker 1>information even if the information that</v>

133
00:10:02.071 --> 00:10:06.420
<v Speaker 1>you received about the supervision.</v>
<v Speaker 1>The ground truth is sparse.</v>

134
00:10:06.780 --> 00:10:11.780
<v Speaker 1>You can follow that information back </v>
<v Speaker 1>through time to infer something about </v>

135
00:10:11.780 --> 00:10:15.960
<v Speaker 1>the reality of what happened before then</v>
<v Speaker 1>even if your reward signals were weak,</v>

136
00:10:16.530 --> 00:10:21.180
<v Speaker 1>so it's using the fact that the physical</v>
<v Speaker 1>world evolves through time in some,</v>

137
00:10:21.900 --> 00:10:26.900
<v Speaker 1>some sort of predictable way to take </v>
<v Speaker 1>sparse information and generalize it </v>

138
00:10:28.710 --> 00:10:31.680
<v Speaker 1>over the entirety of the experience as </v>
<v Speaker 1>being learned.</v>

139
00:10:32.580 --> 00:10:37.580
<v Speaker 1>So we apply this to two problems today.</v>
<v Speaker 1>We'll talk about deep traffic as a </v>

140
00:10:38.641 --> 00:10:39.530
<v Speaker 1>methodology,</v>
<v Speaker 1>as a,</v>

141
00:10:39.531 --> 00:10:41.340
<v Speaker 1>as a way to introduce deeper </v>
<v Speaker 1>enforcement.</v>

142
00:10:41.341 --> 00:10:46.341
<v Speaker 1>Learning.</v>
<v Speaker 1>The deep traffic is a competition that </v>

143
00:10:46.341 --> 00:10:49.221
<v Speaker 1>we ran last year and expanded </v>
<v Speaker 1>significantly this year and I'll talk </v>

144
00:10:50.101 --> 00:10:55.101
<v Speaker 1>about some of the details and how the </v>
<v Speaker 1>folks in this room can on your </v>

145
00:10:55.101 --> 00:10:59.660
<v Speaker 1>smartphone today or if you have a laptop</v>
<v Speaker 1>training agent while I'm talking </v>

146
00:11:00.480 --> 00:11:02.220
<v Speaker 1>training a neural network in the </v>
<v Speaker 1>browser.</v>

147
00:11:02.670 --> 00:11:06.390
<v Speaker 1>Some of the things you've added our.</v>
<v Speaker 1>We've added the capability.</v>

148
00:11:06.900 --> 00:11:11.900
<v Speaker 1>We've now turned it into a multiagent </v>
<v Speaker 1>deep reinforcement learning problem </v>

149
00:11:11.900 --> 00:11:13.770
<v Speaker 1>where he can control up to 10 cars </v>
<v Speaker 1>within your own network.</v>

150
00:11:15.240 --> 00:11:20.240
<v Speaker 1>Perhaps less significant but pretty cool</v>
<v Speaker 1>is the ability to customize the way the </v>

151
00:11:21.331 --> 00:11:26.331
<v Speaker 1>agent looks so you can upload and people</v>
<v Speaker 1>have to an absurd degree have already </v>

152
00:11:27.241 --> 00:11:32.241
<v Speaker 1>begun doing so,</v>
<v Speaker 1>uploading different images instead of </v>

153
00:11:32.241 --> 00:11:34.920
<v Speaker 1>the car that's shown there as long as it</v>
<v Speaker 1>maintains the dimensions.</v>

154
00:11:34.950 --> 00:11:36.810
<v Speaker 1>Shown here is a space x rocket.</v>

155
00:11:38.850 --> 00:11:43.850
<v Speaker 1>The competition is hosted on the website</v>
<v Speaker 1>self driving cars that mit.edu/deep</v>

156
00:11:44.701 --> 00:11:46.770
<v Speaker 1>traffic.</v>
<v Speaker 1>We'll return to this later.</v>

157
00:11:48.390 --> 00:11:51.650
<v Speaker 1>The code is on get hub with some more </v>
<v Speaker 1>information,</v>

158
00:11:51.660 --> 00:11:56.660
<v Speaker 1>a starter code and the paper describing </v>
<v Speaker 1>some of the fundamental insights that </v>

159
00:11:58.031 --> 00:12:02.050
<v Speaker 1>will help you win at this competition is</v>
<v Speaker 1>an archive,</v>

160
00:12:04.180 --> 00:12:08.050
<v Speaker 1>so from supervised learning and lecture </v>
<v Speaker 1>one to today.</v>

161
00:12:09.220 --> 00:12:14.220
<v Speaker 1>Supervised learning we can think of is </v>
<v Speaker 1>memorization of ground true data in </v>

162
00:12:15.661 --> 00:12:19.590
<v Speaker 1>order to form representations that </v>
<v Speaker 1>generalizes from that Ground Truth.</v>

163
00:12:20.610 --> 00:12:25.610
<v Speaker 1>Reinforcement learning is we can think </v>
<v Speaker 1>of as a way to brute force propagate </v>

164
00:12:26.191 --> 00:12:31.191
<v Speaker 1>that information,</v>
<v Speaker 1>the sparse information through time to </v>

165
00:12:35.120 --> 00:12:40.120
<v Speaker 1>to assign quality reward to state that </v>
<v Speaker 1>does not directly have a reward to make </v>

166
00:12:43.221 --> 00:12:48.221
<v Speaker 1>sense of this world.</v>
<v Speaker 1>When the rewards a sparse but are </v>

167
00:12:48.221 --> 00:12:51.260
<v Speaker 1>connected through time.</v>
<v Speaker 1>You can think of that as reasoning,</v>

168
00:12:53.390 --> 00:12:58.390
<v Speaker 1>so the connection through time is </v>
<v Speaker 1>modeled in most reinforcement learning </v>

169
00:13:00.321 --> 00:13:05.321
<v Speaker 1>approach is very simply that there's an </v>
<v Speaker 1>agent taken an action in the state and </v>

170
00:13:07.071 --> 00:13:12.071
<v Speaker 1>receiving a reward,</v>
<v Speaker 1>and the agent operating in an </v>

171
00:13:12.071 --> 00:13:15.371
<v Speaker 1>environment executes an action,</v>
<v Speaker 1>receives an observed state and use state</v>

172
00:13:15.440 --> 00:13:20.440
<v Speaker 1>and receives the award.</v>
<v Speaker 1>This process continues over and over and</v>

173
00:13:22.771 --> 00:13:26.670
<v Speaker 1>some examples we can think of any of the</v>
<v Speaker 1>video games,</v>

174
00:13:26.671 --> 00:13:31.671
<v Speaker 1>some of which we'll talk about today,</v>
<v Speaker 1>like Atari breakout as the environment.</v>

175
00:13:32.370 --> 00:13:34.770
<v Speaker 1>The agent is the paddle.</v>

176
00:13:36.680 --> 00:13:41.680
<v Speaker 1>Each action that the agent takes has an </v>
<v Speaker 1>influence on the evolution of the </v>

177
00:13:43.071 --> 00:13:47.870
<v Speaker 1>environment and the success is measured </v>
<v Speaker 1>by some reward mechanism.</v>

178
00:13:48.350 --> 00:13:53.350
<v Speaker 1>In this case,</v>
<v Speaker 1>points are given by the game and every </v>

179
00:13:53.350 --> 00:13:56.771
<v Speaker 1>game has a different point scheme that </v>
<v Speaker 1>must be converted normalized until a way</v>

180
00:13:58.551 --> 00:14:03.110
<v Speaker 1>that's interpretable by the system and </v>
<v Speaker 1>the goal is to maximize those points,</v>

181
00:14:03.140 --> 00:14:08.140
<v Speaker 1>maximize the reward.</v>
<v Speaker 1>The continuous problem of card poll by </v>

182
00:14:10.300 --> 00:14:13.450
<v Speaker 1>balancing the goal is to balance the </v>
<v Speaker 1>pole on top of a moving cart.</v>

183
00:14:14.290 --> 00:14:17.110
<v Speaker 1>The state is the angle that angular </v>
<v Speaker 1>speed,</v>

184
00:14:17.111 --> 00:14:19.000
<v Speaker 1>the position,</v>
<v Speaker 1>the horizontal velocity,</v>

185
00:14:19.960 --> 00:14:22.870
<v Speaker 1>the actions are the horizontal force </v>
<v Speaker 1>applied to the cart,</v>

186
00:14:23.140 --> 00:14:27.100
<v Speaker 1>and the award is one at each time step.</v>
<v Speaker 1>If the pole is still upright,</v>

187
00:14:30.130 --> 00:14:33.250
<v Speaker 1>all the first person shooters,</v>
<v Speaker 1>the video games,</v>

188
00:14:33.251 --> 00:14:36.710
<v Speaker 1>and now star craft the,</v>
<v Speaker 1>uh,</v>

189
00:14:36.780 --> 00:14:41.780
<v Speaker 1>strategy games in case of first person </v>
<v Speaker 1>shooter and doom.</v>

190
00:14:43.050 --> 00:14:46.290
<v Speaker 1>What is the goal?</v>
<v Speaker 1>The environment is the game that goes to</v>

191
00:14:46.320 --> 00:14:49.200
<v Speaker 1>eliminate all opponents.</v>
<v Speaker 1>The state is the raw game.</v>

192
00:14:49.201 --> 00:14:52.550
<v Speaker 1>Pixels coming in.</v>
<v Speaker 1>The actions is moving up,</v>

193
00:14:52.551 --> 00:14:53.240
<v Speaker 1>down,</v>
<v Speaker 1>left,</v>

194
00:14:53.241 --> 00:14:55.130
<v Speaker 1>right,</v>
<v Speaker 1>and so on.</v>

195
00:14:55.550 --> 00:15:00.550
<v Speaker 1>And the reward is positive when </v>
<v Speaker 1>eliminating and opponent and negative.</v>

196
00:15:01.550 --> 00:15:06.550
<v Speaker 1>When the agent has eliminated industrial</v>
<v Speaker 1>robotics,</v>

197
00:15:08.220 --> 00:15:13.220
<v Speaker 1>been packing with a robotic arm,</v>
<v Speaker 1>the goal is to pick up a device from a </v>

198
00:15:13.220 --> 00:15:16.491
<v Speaker 1>box and put it into a container.</v>
<v Speaker 1>The state is the raw pixels of the real </v>

199
00:15:16.491 --> 00:15:21.001
<v Speaker 1>world that the robot observes the </v>
<v Speaker 1>actions or the possible actions with the</v>

200
00:15:21.151 --> 00:15:24.150
<v Speaker 1>robot that different degrees of freedom </v>
<v Speaker 1>and moving through those degrees,</v>

201
00:15:24.360 --> 00:15:28.800
<v Speaker 1>moving the different actuators to </v>
<v Speaker 1>realize the position of the arm,</v>

202
00:15:28.950 --> 00:15:32.550
<v Speaker 1>and then award is positive on placing a </v>
<v Speaker 1>device successfully and negative.</v>

203
00:15:32.551 --> 00:15:33.240
<v Speaker 1>Otherwise,</v>

204
00:15:35.230 --> 00:15:40.230
<v Speaker 1>everything can be modeled in this way.</v>
<v Speaker 1>Mark off decision process as a state,</v>

205
00:15:40.481 --> 00:15:45.440
<v Speaker 1>as zero action,</v>
<v Speaker 1>a zero and reward received and you state</v>

206
00:15:45.441 --> 00:15:46.720
<v Speaker 1>is achieved.</v>
<v Speaker 1>Again,</v>

207
00:15:46.721 --> 00:15:51.430
<v Speaker 1>Action Reward State Action Award,</v>
<v Speaker 1>state until a terminal state is,</v>

208
00:15:51.570 --> 00:15:56.570
<v Speaker 1>is reached,</v>
<v Speaker 1>and the major components of </v>

209
00:15:56.570 --> 00:16:00.901
<v Speaker 1>reinforcement learning is a policy,</v>
<v Speaker 1>some kind of plan of what to do in every</v>

210
00:16:01.001 --> 00:16:06.001
<v Speaker 1>single state.</v>
<v Speaker 1>What kind of action to perform a value </v>

211
00:16:06.001 --> 00:16:07.290
<v Speaker 1>function,</v>
<v Speaker 1>uh,</v>

212
00:16:07.300 --> 00:16:11.530
<v Speaker 1>some kind of sense of what is a good </v>
<v Speaker 1>state to be in,</v>

213
00:16:11.590 --> 00:16:16.590
<v Speaker 1>of what is a good action to take in a </v>
<v Speaker 1>state and sometimes a model</v>

214
00:16:19.330 --> 00:16:24.330
<v Speaker 1>that the agent represents the </v>
<v Speaker 1>environment with some kind of sense of </v>

215
00:16:24.330 --> 00:16:28.891
<v Speaker 1>the environment.</v>
<v Speaker 1>It's operating in the dynamics of that </v>

216
00:16:28.891 --> 00:16:30.040
<v Speaker 1>environment that's useful for making </v>
<v Speaker 1>decisions about actions.</v>

217
00:16:30.880 --> 00:16:35.880
<v Speaker 1>Let's take a trivial example,</v>
<v Speaker 1>a grid world,</v>

218
00:16:36.461 --> 00:16:40.300
<v Speaker 1>a three by four,</v>
<v Speaker 1>12 squares where you start at the bottom</v>

219
00:16:40.301 --> 00:16:45.301
<v Speaker 1>left and their task with walking about </v>
<v Speaker 1>this world to maximize reward.</v>

220
00:16:47.400 --> 00:16:52.400
<v Speaker 1>The award that the top right is a plus </v>
<v Speaker 1>one and a one square below that is a </v>

221
00:16:52.400 --> 00:16:56.851
<v Speaker 1>negative one.</v>
<v Speaker 1>And every step you take is a punishment </v>

222
00:16:56.851 --> 00:16:58.960
<v Speaker 1>or as a negative reward of zero point </v>
<v Speaker 1>zero four.</v>

223
00:17:00.040 --> 00:17:02.380
<v Speaker 1>So what is the optimal policy in this </v>
<v Speaker 1>world?</v>

224
00:17:04.310 --> 00:17:06.170
<v Speaker 1>Now,</v>
<v Speaker 1>when everything is deterministic,</v>

225
00:17:07.220 --> 00:17:11.650
<v Speaker 1>perhaps this is the policy.</v>
<v Speaker 1>When you start at the bottom left,</v>

226
00:17:11.860 --> 00:17:14.290
<v Speaker 1>well,</v>
<v Speaker 1>because every step hurts,</v>

227
00:17:14.291 --> 00:17:19.291
<v Speaker 1>every step has a negative reward.</v>
<v Speaker 1>Then you want to take the shortest path </v>

228
00:17:19.291 --> 00:17:20.830
<v Speaker 1>to the maximum square with a maximum </v>
<v Speaker 1>reward.</v>

229
00:17:21.760 --> 00:17:24.640
<v Speaker 1>When the state space is non </v>
<v Speaker 1>deterministic,</v>

230
00:17:26.390 --> 00:17:30.560
<v Speaker 1>as presented before,</v>
<v Speaker 1>with the probability of point eight,</v>

231
00:17:30.770 --> 00:17:33.110
<v Speaker 1>when you choose to go up,</v>
<v Speaker 1>you go up.</v>

232
00:17:33.140 --> 00:17:37.400
<v Speaker 1>But with probability point one,</v>
<v Speaker 1>you go left and point one,</v>

233
00:17:37.430 --> 00:17:39.740
<v Speaker 1>you go right,</v>
<v Speaker 1>unfair.</v>

234
00:17:39.980 --> 00:17:41.360
<v Speaker 1>Again,</v>
<v Speaker 1>much like life,</v>

235
00:17:42.470 --> 00:17:47.470
<v Speaker 1>that would be the optimal policy.</v>
<v Speaker 1>What is the key observation here that </v>

236
00:17:48.531 --> 00:17:51.480
<v Speaker 1>every single in the space must have a </v>
<v Speaker 1>plan</v>

237
00:17:53.320 --> 00:17:58.320
<v Speaker 1>because you can't because then a non </v>
<v Speaker 1>deterministic aspect of the control,</v>

238
00:17:58.390 --> 00:18:00.400
<v Speaker 1>you can't control where are going to end</v>
<v Speaker 1>up.</v>

239
00:18:00.401 --> 00:18:03.580
<v Speaker 1>So you must have a plan for every place </v>
<v Speaker 1>that's the policy.</v>

240
00:18:03.850 --> 00:18:08.850
<v Speaker 1>Having an action,</v>
<v Speaker 1>an optimal action to take in every </v>

241
00:18:08.850 --> 00:18:08.850
<v Speaker 1>single state.</v>
<v Speaker 1>Now,</v>

242
00:18:08.850 --> 00:18:13.360
<v Speaker 1>suppose we change their reward structure</v>
<v Speaker 1>and for every step we take as a negative</v>

243
00:18:13.890 --> 00:18:16.960
<v Speaker 1>or award is a negative two,</v>
<v Speaker 1>so it really hurts.</v>

244
00:18:16.990 --> 00:18:19.420
<v Speaker 1>There's a high punishment for every </v>
<v Speaker 1>single step we take.</v>

245
00:18:19.600 --> 00:18:23.470
<v Speaker 1>So no matter what,</v>
<v Speaker 1>we always take the shortest path,</v>

246
00:18:23.471 --> 00:18:28.471
<v Speaker 1>the optimal policies to take the </v>
<v Speaker 1>shortest path to the to the only spot on</v>

247
00:18:28.601 --> 00:18:31.900
<v Speaker 1>the board that doesn't result in </v>
<v Speaker 1>punishment.</v>

248
00:18:34.450 --> 00:18:38.560
<v Speaker 1>If we decrease the reward of each step </v>
<v Speaker 1>two negative point one,</v>

249
00:18:39.820 --> 00:18:44.820
<v Speaker 1>the policy changes were there a some </v>
<v Speaker 1>extra degree of wandering encouraged,</v>

250
00:18:48.560 --> 00:18:53.560
<v Speaker 1>and as we go further and further in,</v>
<v Speaker 1>lowering the punishment does before to </v>

251
00:18:53.560 --> 00:18:55.680
<v Speaker 1>negative zero point zero,</v>
<v Speaker 1>four more.</v>

252
00:18:55.681 --> 00:19:00.681
<v Speaker 1>Wandering and wandering is allowed.</v>
<v Speaker 1>And when we finally turned the reward </v>

253
00:19:05.300 --> 00:19:09.200
<v Speaker 1>into positive,</v>
<v Speaker 1>so every step it,</v>

254
00:19:10.290 --> 00:19:15.290
<v Speaker 1>every step is increases the award.</v>
<v Speaker 1>Then there's a significant incentive to </v>

255
00:19:16.830 --> 00:19:20.010
<v Speaker 1>to stay on the board without ever </v>
<v Speaker 1>reaching the destination.</v>

256
00:19:22.530 --> 00:19:24.160
<v Speaker 1>Kind of like college for a lot of </v>
<v Speaker 1>people.</v>

257
00:19:28.620 --> 00:19:33.620
<v Speaker 1>So the value function,</v>
<v Speaker 1>the way we think about the value of the </v>

258
00:19:33.620 --> 00:19:37.711
<v Speaker 1>estate or the value of anything in the </v>
<v Speaker 1>environment is the reward we're likely </v>

259
00:19:40.321 --> 00:19:45.321
<v Speaker 1>to receive in the future and the way we </v>
<v Speaker 1>see the reward we're likely to receive </v>

260
00:19:46.260 --> 00:19:51.260
<v Speaker 1>as we discount the future award because </v>
<v Speaker 1>we can't always count on it,</v>

261
00:19:53.730 --> 00:19:57.740
<v Speaker 1>hair or gamma further and further out </v>
<v Speaker 1>into the future.</v>

262
00:19:57.800 --> 00:20:00.680
<v Speaker 1>More and more discounts decreases the </v>
<v Speaker 1>era,</v>

263
00:20:00.890 --> 00:20:05.890
<v Speaker 1>the the importance of that reward </v>
<v Speaker 1>received and a good strategy is taking </v>

264
00:20:06.651 --> 00:20:08.720
<v Speaker 1>the summer of these rewards and </v>
<v Speaker 1>maximizing it,</v>

265
00:20:09.020 --> 00:20:13.820
<v Speaker 1>maximizing discounted future award.</v>
<v Speaker 1>That's what reinforcement learning hopes</v>

266
00:20:13.821 --> 00:20:17.370
<v Speaker 1>to achieve.</v>
<v Speaker 1>And with cue learning,</v>

267
00:20:18.750 --> 00:20:23.750
<v Speaker 1>we use any policy to estimate the value </v>
<v Speaker 1>of taking an action in the state.</v>

268
00:20:28.140 --> 00:20:30.810
<v Speaker 1>So off policy,</v>
<v Speaker 1>forget policy,</v>

269
00:20:31.410 --> 00:20:36.410
<v Speaker 1>we move about the world and use the </v>
<v Speaker 1>bellman equation here on the bottom to </v>

270
00:20:36.410 --> 00:20:40.971
<v Speaker 1>continuously update our estimate of how </v>
<v Speaker 1>good a certain action is in a certain </v>

271
00:20:40.971 --> 00:20:45.060
<v Speaker 1>state.</v>
<v Speaker 1>So we don't need this.</v>

272
00:20:45.090 --> 00:20:50.090
<v Speaker 1>This allows us to operate in a much </v>
<v Speaker 1>larger space in a much larger action </v>

273
00:20:50.090 --> 00:20:53.581
<v Speaker 1>space.</v>
<v Speaker 1>We move about this world or simulation </v>

274
00:20:53.581 --> 00:20:55.150
<v Speaker 1>or in the real world,</v>
<v Speaker 1>taking actions and updating our estimate</v>

275
00:20:55.330 --> 00:20:57.450
<v Speaker 1>of how good certain actions are over </v>
<v Speaker 1>time.</v>

276
00:20:59.620 --> 00:21:03.030
<v Speaker 1>The new state that the left is the is </v>
<v Speaker 1>the update of value.</v>

277
00:21:03.050 --> 00:21:08.050
<v Speaker 1>The old state is the starting value for </v>
<v Speaker 1>the equation and we update that old </v>

278
00:21:08.050 --> 00:21:09.180
<v Speaker 1>state estimation with,</v>
<v Speaker 1>uh,</v>

279
00:21:09.190 --> 00:21:14.190
<v Speaker 1>some of the reward received by taking </v>
<v Speaker 1>action as a tax action,</v>

280
00:21:15.161 --> 00:21:20.161
<v Speaker 1>a in status,</v>
<v Speaker 1>and the maximum reward that's possible </v>

281
00:21:22.330 --> 00:21:27.330
<v Speaker 1>to be received in the following states.</v>
<v Speaker 1>Discounted that update is decreased with</v>

282
00:21:30.591 --> 00:21:33.170
<v Speaker 1>the learning rate.</v>
<v Speaker 1>The higher the learning rate,</v>

283
00:21:33.230 --> 00:21:35.200
<v Speaker 1>the more value we,</v>
<v Speaker 1>the,</v>

284
00:21:35.270 --> 00:21:40.270
<v Speaker 1>the faster we learn,</v>
<v Speaker 1>the more value we assigned to new </v>

285
00:21:40.270 --> 00:21:40.550
<v Speaker 1>information.</v>
<v Speaker 1>That's simple.</v>

286
00:21:40.580 --> 00:21:41.720
<v Speaker 1>That's it.</v>
<v Speaker 1>That's cute.</v>

287
00:21:41.721 --> 00:21:44.990
<v Speaker 1>Learning as simple update rule allows us</v>
<v Speaker 1>to,</v>

288
00:21:45.770 --> 00:21:50.770
<v Speaker 1>to explore the world and as we explore,</v>
<v Speaker 1>get more and more information about </v>

289
00:21:52.401 --> 00:21:57.401
<v Speaker 1>what's good to do in this world,</v>
<v Speaker 1>and there's always a balance in the </v>

290
00:21:57.401 --> 00:22:01.151
<v Speaker 1>various problem spaces we'll discuss.</v>
<v Speaker 1>There's always a balance between </v>

291
00:22:01.151 --> 00:22:02.120
<v Speaker 1>exploration and exploitation.</v>

292
00:22:04.660 --> 00:22:08.200
<v Speaker 1>As you form a better and better estimate</v>
<v Speaker 1>of the function of what actions are good</v>

293
00:22:08.230 --> 00:22:13.230
<v Speaker 1>to take,</v>
<v Speaker 1>you start to get a sense of what is the </v>

294
00:22:13.230 --> 00:22:15.040
<v Speaker 1>best action to take,</v>
<v Speaker 1>but it's not a perfect sense.</v>

295
00:22:15.070 --> 00:22:18.400
<v Speaker 1>It's still an approximation and so </v>
<v Speaker 1>there's value of exploration,</v>

296
00:22:18.670 --> 00:22:21.730
<v Speaker 1>but the better and better your estimate </v>
<v Speaker 1>becomes less and less.</v>

297
00:22:21.731 --> 00:22:26.731
<v Speaker 1>Exploration has a benefit,</v>
<v Speaker 1>so usually want to explore a lot in the </v>

298
00:22:26.731 --> 00:22:31.390
<v Speaker 1>beginning and less and less so towards </v>
<v Speaker 1>the end and when we finally released the</v>

299
00:22:31.391 --> 00:22:35.170
<v Speaker 1>system out into the world and wish it to</v>
<v Speaker 1>operate as best,</v>

300
00:22:35.380 --> 00:22:39.070
<v Speaker 1>then we have it operate as a greedy </v>
<v Speaker 1>system,</v>

301
00:22:39.071 --> 00:22:41.440
<v Speaker 1>always taking the optimal action </v>
<v Speaker 1>according to the q,</v>

302
00:22:41.450 --> 00:22:46.450
<v Speaker 1>a q value function and everything I'm </v>
<v Speaker 1>talking about now is parametrized and </v>

303
00:22:49.631 --> 00:22:54.631
<v Speaker 1>our parameters that are very important </v>
<v Speaker 1>for winning the deep traffic </v>

304
00:22:54.851 --> 00:22:59.851
<v Speaker 1>competition,</v>
<v Speaker 1>which is using this very algorithm </v>

305
00:22:59.851 --> 00:23:01.420
<v Speaker 1>within your own network as a score.</v>

306
00:23:03.870 --> 00:23:08.870
<v Speaker 1>So for simple table representation of a </v>
<v Speaker 1>function where the y axis to state four </v>

307
00:23:10.231 --> 00:23:11.400
<v Speaker 1>states s one,</v>
<v Speaker 1>two,</v>

308
00:23:11.401 --> 00:23:12.180
<v Speaker 1>three,</v>
<v Speaker 1>four,</v>

309
00:23:12.540 --> 00:23:16.260
<v Speaker 1>and the x axis is actions a,</v>
<v Speaker 1>one,</v>

310
00:23:16.290 --> 00:23:16.860
<v Speaker 1>two,</v>
<v Speaker 1>three,</v>

311
00:23:16.861 --> 00:23:21.861
<v Speaker 1>four.</v>
<v Speaker 1>We can think of this table as randomly </v>

312
00:23:21.861 --> 00:23:25.761
<v Speaker 1>initiated or initiated initialized in </v>
<v Speaker 1>any kind of way that's not </v>

313
00:23:25.831 --> 00:23:30.831
<v Speaker 1>representative of actual reality.</v>
<v Speaker 1>And as we move about the world and we </v>

314
00:23:30.831 --> 00:23:34.521
<v Speaker 1>take actions,</v>
<v Speaker 1>we update this table with a bellman </v>

315
00:23:34.521 --> 00:23:36.420
<v Speaker 1>equation shown up top and here slides </v>
<v Speaker 1>now are online.</v>

316
00:23:36.600 --> 00:23:40.800
<v Speaker 1>You can see a simple pseudo code </v>
<v Speaker 1>algorithm of how to update it.</v>

317
00:23:40.980 --> 00:23:45.980
<v Speaker 1>I to run this bellman equation and over </v>
<v Speaker 1>the approximation becomes the optimal q </v>

318
00:23:48.470 --> 00:23:51.860
<v Speaker 1>table.</v>
<v Speaker 1>The problem is when that cute table,</v>

319
00:23:51.920 --> 00:23:56.920
<v Speaker 1>it becomes exponential in size.</v>
<v Speaker 1>When we take in raw sensor information </v>

320
00:23:57.620 --> 00:24:02.620
<v Speaker 1>as we do with cameras with deep crash or</v>
<v Speaker 1>deep traffic sticking the full grid </v>

321
00:24:03.891 --> 00:24:06.560
<v Speaker 1>space and taking that information,</v>
<v Speaker 1>the Ra,</v>

322
00:24:07.460 --> 00:24:12.460
<v Speaker 1>the Ra grid pixels of deep traffic.</v>
<v Speaker 1>When you take the arcade games here,</v>

323
00:24:13.131 --> 00:24:18.131
<v Speaker 1>they're taking the raw pixels of the </v>
<v Speaker 1>game or when we take go the game of go </v>

324
00:24:19.640 --> 00:24:21.770
<v Speaker 1>when it's taking the units,</v>
<v Speaker 1>um,</v>

325
00:24:21.810 --> 00:24:23.150
<v Speaker 1>the,</v>
<v Speaker 1>the board,</v>

326
00:24:23.300 --> 00:24:28.300
<v Speaker 1>the raw state of the board as the input,</v>
<v Speaker 1>the potential state space,</v>

327
00:24:31.040 --> 00:24:36.040
<v Speaker 1>the number of possible combinatorial </v>
<v Speaker 1>variations of what states as possible is</v>

328
00:24:37.040 --> 00:24:42.040
<v Speaker 1>extremely large,</v>
<v Speaker 1>larger than we can certainly hold the </v>

329
00:24:42.040 --> 00:24:45.391
<v Speaker 1>memory and larger than we can ever be </v>
<v Speaker 1>able to accurately approximate through </v>

330
00:24:45.561 --> 00:24:48.980
<v Speaker 1>the bellman equation over time to </v>
<v Speaker 1>simulation</v>

331
00:24:50.880 --> 00:24:52.920
<v Speaker 1>through the simple update of the </v>
<v Speaker 1>equation.</v>

332
00:24:53.730 --> 00:24:56.940
<v Speaker 1>So this is where deep reinforcement </v>
<v Speaker 1>learning comes in.</v>

333
00:24:57.780 --> 00:25:00.600
<v Speaker 1>Neural networks are really good </v>
<v Speaker 1>approximators.</v>

334
00:25:00.930 --> 00:25:05.490
<v Speaker 1>They're really good at exactly this task</v>
<v Speaker 1>of learning this kind of queue table.</v>

335
00:25:09.850 --> 00:25:11.800
<v Speaker 1>So as we started with supervised </v>
<v Speaker 1>learning,</v>

336
00:25:12.280 --> 00:25:14.320
<v Speaker 1>when all networks help us memorize </v>
<v Speaker 1>patterns,</v>

337
00:25:14.321 --> 00:25:19.060
<v Speaker 1>using supervised ground truth data,</v>
<v Speaker 1>and we'll move to reinforcement learning</v>

338
00:25:19.061 --> 00:25:23.080
<v Speaker 1>that hopes to propagate outcomes to </v>
<v Speaker 1>knowledge.</v>

339
00:25:25.060 --> 00:25:30.060
<v Speaker 1>Deep learning allows us to do so.</v>
<v Speaker 1>A much larger state spaces are much </v>

340
00:25:30.131 --> 00:25:35.131
<v Speaker 1>larger action spaces,</v>
<v Speaker 1>which means it's generalizable,</v>

341
00:25:36.430 --> 00:25:41.430
<v Speaker 1>it's much more capable to deal with the </v>
<v Speaker 1>raw stuff of sensory data,</v>

342
00:25:43.630 --> 00:25:48.630
<v Speaker 1>which means it's much more capable to </v>
<v Speaker 1>deal with a broad variation of real </v>

343
00:25:48.630 --> 00:25:53.011
<v Speaker 1>world applications,</v>
<v Speaker 1>and it does so because it's able to </v>

344
00:25:57.510 --> 00:26:01.920
<v Speaker 1>learn the representations as we've </v>
<v Speaker 1>discussed on Monday.</v>

345
00:26:04.920 --> 00:26:09.920
<v Speaker 1>They're understanding comes from </v>
<v Speaker 1>converting the raw sensor information </v>

346
00:26:10.110 --> 00:26:15.110
<v Speaker 1>into into simple,</v>
<v Speaker 1>useful information based on which the </v>

347
00:26:15.110 --> 00:26:18.510
<v Speaker 1>action in this particular state can be </v>
<v Speaker 1>taken in the same exact way,</v>

348
00:26:18.870 --> 00:26:23.870
<v Speaker 1>so instead of the queue table,</v>
<v Speaker 1>instead of this queue function with </v>

349
00:26:23.870 --> 00:26:25.800
<v Speaker 1>plugging in your network where they </v>
<v Speaker 1>input is the state space.</v>

350
00:26:25.980 --> 00:26:30.980
<v Speaker 1>No matter how complex and the output is </v>
<v Speaker 1>a value for each of the actions that you</v>

351
00:26:32.011 --> 00:26:32.520
<v Speaker 1>could take.</v>

352
00:26:34.980 --> 00:26:38.850
<v Speaker 1>Input is the state.</v>
<v Speaker 1>Opera is the value of the function.</v>

353
00:26:39.150 --> 00:26:44.150
<v Speaker 1>It's simple.</v>
<v Speaker 1>This is deep q network dqn at the core </v>

354
00:26:47.160 --> 00:26:52.160
<v Speaker 1>of the success at deep mind.</v>
<v Speaker 1>A lot of the cool stuff you see about </v>

355
00:26:52.160 --> 00:26:54.360
<v Speaker 1>video games,</v>
<v Speaker 1>dqn or variants of dqs or play.</v>

356
00:26:55.530 --> 00:26:58.080
<v Speaker 1>This is what a first with a nature </v>
<v Speaker 1>paper,</v>

357
00:26:59.110 --> 00:27:04.110
<v Speaker 1>deep mind,</v>
<v Speaker 1>the success came of playing the </v>

358
00:27:04.110 --> 00:27:08.171
<v Speaker 1>different games including Atari Games.</v>
<v Speaker 1>So how are these things trained?</v>

359
00:27:12.560 --> 00:27:17.560
<v Speaker 1>Very similar to supervise learning.</v>
<v Speaker 1>The bellman equation,</v>

360
00:27:19.660 --> 00:27:24.660
<v Speaker 1>laptop takes the reward and the </v>
<v Speaker 1>discounted expected reward from future </v>

361
00:27:28.471 --> 00:27:33.471
<v Speaker 1>states.</v>
<v Speaker 1>The loss function here for neural </v>

362
00:27:34.181 --> 00:27:36.320
<v Speaker 1>network.</v>
<v Speaker 1>Then you'll network learns with the loss</v>

363
00:27:36.321 --> 00:27:41.321
<v Speaker 1>function.</v>
<v Speaker 1>It takes the reward received at the </v>

364
00:27:41.321 --> 00:27:45.601
<v Speaker 1>current state.</v>
<v Speaker 1>Does the forward pass through in your </v>

365
00:27:45.601 --> 00:27:49.300
<v Speaker 1>network to estimate the value of the </v>
<v Speaker 1>future state of the best action to take </v>

366
00:27:50.771 --> 00:27:55.771
<v Speaker 1>in the future state and then subtracts </v>
<v Speaker 1>that from the forward pass through the </v>

367
00:27:58.901 --> 00:28:03.901
<v Speaker 1>network for the current state of action.</v>
<v Speaker 1>So you take the difference between what </v>

368
00:28:04.660 --> 00:28:09.520
<v Speaker 1>your Acu estimator,</v>
<v Speaker 1>than your own network believes the value</v>

369
00:28:09.521 --> 00:28:14.521
<v Speaker 1>of the current state is,</v>
<v Speaker 1>and what it more likely as to be based </v>

370
00:28:17.260 --> 00:28:22.260
<v Speaker 1>on the value of the future states that </v>
<v Speaker 1>are reachable based on the actions you </v>

371
00:28:22.260 --> 00:28:22.260
<v Speaker 1>can take.</v>

372
00:28:26.680 --> 00:28:31.680
<v Speaker 1>Here's the algorithm.</v>
<v Speaker 1>Input is the state output as the q value</v>

373
00:28:32.421 --> 00:28:36.350
<v Speaker 1>for each action or in this diagram and </v>
<v Speaker 1>as a state of inaction,</v>

374
00:28:36.410 --> 00:28:40.520
<v Speaker 1>and the output is the q value.</v>
<v Speaker 1>It's very similar architectures,</v>

375
00:28:41.090 --> 00:28:46.090
<v Speaker 1>so given the transition of s a r s prime</v>
<v Speaker 1>as current state taken an action,</v>

376
00:28:49.761 --> 00:28:52.530
<v Speaker 1>we're seeing your reward and achieving </v>
<v Speaker 1>as prime state.</v>

377
00:28:55.530 --> 00:29:00.530
<v Speaker 1>The update is do a feed forward past to </v>
<v Speaker 1>the network for the current state,</v>

378
00:29:02.760 --> 00:29:07.760
<v Speaker 1>do a feed forward,</v>
<v Speaker 1>pass for each of the possible actions </v>

379
00:29:07.760 --> 00:29:11.751
<v Speaker 1>taken in the next state,</v>
<v Speaker 1>and that's how we compute the two parts </v>

380
00:29:11.751 --> 00:29:15.291
<v Speaker 1>of the loss function and the update the </v>
<v Speaker 1>weights using backpropagation.</v>

381
00:29:16.890 --> 00:29:19.260
<v Speaker 1>Again,</v>
<v Speaker 1>last function backpropagation is how the</v>

382
00:29:19.261 --> 00:29:24.261
<v Speaker 1>network is trained.</v>
<v Speaker 1>This has actually been around for much </v>

383
00:29:24.541 --> 00:29:29.541
<v Speaker 1>longer than deep mind.</v>
<v Speaker 1>A few tricks made made it really work.</v>

384
00:29:34.370 --> 00:29:36.200
<v Speaker 1>Experience replays the biggest one,</v>

385
00:29:38.920 --> 00:29:43.920
<v Speaker 1>so as the games are played through </v>
<v Speaker 1>simulation or if it's a physical system </v>

386
00:29:43.920 --> 00:29:47.551
<v Speaker 1>as it acts in the world,</v>
<v Speaker 1>it's actually collecting the </v>

387
00:29:48.431 --> 00:29:53.431
<v Speaker 1>observations into a library of </v>
<v Speaker 1>experiences and that training is </v>

388
00:29:53.431 --> 00:29:58.141
<v Speaker 1>performed by randomly sampling the </v>
<v Speaker 1>library in the past by randomly sampling</v>

389
00:29:59.710 --> 00:30:04.710
<v Speaker 1>that previous experiences in batches.</v>
<v Speaker 1>So you're not always training on the </v>

390
00:30:05.411 --> 00:30:10.411
<v Speaker 1>natural continuous evolution to the </v>
<v Speaker 1>system you're training on randomly </v>

391
00:30:10.411 --> 00:30:13.210
<v Speaker 1>picked batches of those experiences.</v>
<v Speaker 1>That's a huge.</v>

392
00:30:14.220 --> 00:30:15.090
<v Speaker 1>It's a.</v>
<v Speaker 1>it's a,</v>

393
00:30:15.100 --> 00:30:17.590
<v Speaker 1>seems like a subtle trick,</v>
<v Speaker 1>but it's a really important one.</v>

394
00:30:19.150 --> 00:30:24.150
<v Speaker 1>So the system doesn't overfit a </v>
<v Speaker 1>particular evolution of,</v>

395
00:30:24.720 --> 00:30:25.380
<v Speaker 1>of,</v>
<v Speaker 1>uh,</v>

396
00:30:25.390 --> 00:30:30.130
<v Speaker 1>of the game of the simulation.</v>
<v Speaker 1>Uh,</v>

397
00:30:30.131 --> 00:30:33.530
<v Speaker 1>another important,</v>
<v Speaker 1>again,</v>

398
00:30:33.531 --> 00:30:36.640
<v Speaker 1>subtle trick as in a lot of deep </v>
<v Speaker 1>learning approaches,</v>

399
00:30:36.850 --> 00:30:41.850
<v Speaker 1>the subtle tricks make all the </v>
<v Speaker 1>difference is fixing the target network </v>

400
00:30:43.180 --> 00:30:45.340
<v Speaker 1>for the last function.</v>
<v Speaker 1>If you notice,</v>

401
00:30:46.060 --> 00:30:49.660
<v Speaker 1>you have to use the neural network,</v>
<v Speaker 1>the neural network,</v>

402
00:30:49.661 --> 00:30:54.661
<v Speaker 1>the Gq,</v>
<v Speaker 1>I network to estimate the value of the </v>

403
00:30:54.661 --> 00:30:57.601
<v Speaker 1>current state and action pair and the </v>
<v Speaker 1>next.</v>

404
00:30:58.760 --> 00:31:03.760
<v Speaker 1>So using it multiple times.</v>
<v Speaker 1>And as you perform that operation,</v>

405
00:31:06.310 --> 00:31:11.310
<v Speaker 1>you're updating the network,</v>
<v Speaker 1>which means that target function inside </v>

406
00:31:11.310 --> 00:31:13.370
<v Speaker 1>that lost function is always changing.</v>
<v Speaker 1>So you're,</v>

407
00:31:13.480 --> 00:31:18.480
<v Speaker 1>the very nature of your loss function is</v>
<v Speaker 1>changing all the time is you're </v>

408
00:31:18.480 --> 00:31:22.530
<v Speaker 1>learning.</v>
<v Speaker 1>And that's a big problem for stability </v>

409
00:31:22.530 --> 00:31:23.650
<v Speaker 1>that can create big problems for the </v>
<v Speaker 1>learning process.</v>

410
00:31:23.980 --> 00:31:28.980
<v Speaker 1>So this little trick is to fix the </v>
<v Speaker 1>network and only update it every say </v>

411
00:31:30.930 --> 00:31:35.530
<v Speaker 1>thousand steps.</v>
<v Speaker 1>So as you train the network,</v>

412
00:31:36.390 --> 00:31:41.320
<v Speaker 1>the network that's used to compute the </v>
<v Speaker 1>target function inside the last function</v>

413
00:31:41.321 --> 00:31:45.740
<v Speaker 1>is fixed.</v>
<v Speaker 1>It produces a more stable computation on</v>

414
00:31:45.741 --> 00:31:50.590
<v Speaker 1>the loss function.</v>
<v Speaker 1>So the ground doesn't shift under you as</v>

415
00:31:50.591 --> 00:31:54.430
<v Speaker 1>you're trying to find a minimal for the </v>
<v Speaker 1>loss function.</v>

416
00:31:54.730 --> 00:31:57.820
<v Speaker 1>The last function doesn't change and </v>
<v Speaker 1>unpredictable,</v>

417
00:31:57.821 --> 00:32:02.110
<v Speaker 1>difficult to understand ways and reward </v>
<v Speaker 1>clipping,</v>

418
00:32:02.920 --> 00:32:07.920
<v Speaker 1>which is always true with general.</v>
<v Speaker 1>Those systems that are operating a </v>

419
00:32:09.220 --> 00:32:14.220
<v Speaker 1>seeking to operate in a generalized way </v>
<v Speaker 1>is for very for these various games,</v>

420
00:32:15.490 --> 00:32:17.320
<v Speaker 1>the points are different,</v>
<v Speaker 1>some,</v>

421
00:32:17.350 --> 00:32:19.300
<v Speaker 1>some points are low,</v>
<v Speaker 1>some points are high,</v>

422
00:32:19.450 --> 00:32:20.890
<v Speaker 1>some go positive,</v>
<v Speaker 1>negative,</v>

423
00:32:20.980 --> 00:32:24.880
<v Speaker 1>and they're all normalized to a point </v>
<v Speaker 1>where the good points are,</v>

424
00:32:24.881 --> 00:32:29.830
<v Speaker 1>the positive points are a one and </v>
<v Speaker 1>negative points are a negative one.</v>

425
00:32:30.360 --> 00:32:35.360
<v Speaker 1>That's reward clipping,</v>
<v Speaker 1>simplify the reward structure and </v>

426
00:32:35.360 --> 00:32:39.871
<v Speaker 1>because a lot of the games have 30 fps </v>
<v Speaker 1>or 60 fps and the actions are not,</v>

427
00:32:42.080 --> 00:32:47.080
<v Speaker 1>it's not valuable to take actions as </v>
<v Speaker 1>such a high rate inside of these </v>

428
00:32:47.080 --> 00:32:51.430
<v Speaker 1>particularly Atari Games.</v>
<v Speaker 1>Then you only take an action every four </v>

429
00:32:51.430 --> 00:32:54.731
<v Speaker 1>steps while still taking into the frames</v>
<v Speaker 1>as part of the temporal window to make </v>

430
00:32:54.731 --> 00:32:56.150
<v Speaker 1>decisions,</v>
<v Speaker 1>tricks.</v>

431
00:32:56.330 --> 00:33:01.330
<v Speaker 1>But hopefully it gives you a sense of </v>
<v Speaker 1>the kinds of things necessary for both </v>

432
00:33:03.890 --> 00:33:08.750
<v Speaker 1>seminal papers like this one and for the</v>
<v Speaker 1>more important accomplishment of winning</v>

433
00:33:08.751 --> 00:33:13.751
<v Speaker 1>deep traffic is the.</v>
<v Speaker 1>Is the tricks make all the difference </v>

434
00:33:13.820 --> 00:33:18.820
<v Speaker 1>here?</v>
<v Speaker 1>On the bottom is the circle is when the </v>

435
00:33:19.401 --> 00:33:23.720
<v Speaker 1>technique is used and the excellent.</v>
<v Speaker 1>It's not looking at replay and target,</v>

436
00:33:24.080 --> 00:33:29.080
<v Speaker 1>fixed target,</v>
<v Speaker 1>network and experience replay when both </v>

437
00:33:29.080 --> 00:33:30.620
<v Speaker 1>are used for the game of breakout river,</v>
<v Speaker 1>raid,</v>

438
00:33:30.680 --> 00:33:34.040
<v Speaker 1>sciquest and space invaders.</v>
<v Speaker 1>The higher the number,</v>

439
00:33:34.041 --> 00:33:39.041
<v Speaker 1>the better it is,</v>
<v Speaker 1>the more points achieved so when it </v>

440
00:33:39.041 --> 00:33:43.871
<v Speaker 1>gives you a sense that one replay and </v>
<v Speaker 1>target both give significant </v>

441
00:33:43.871 --> 00:33:45.230
<v Speaker 1>improvements in the performance of the </v>
<v Speaker 1>system,</v>

442
00:33:47.370 --> 00:33:51.360
<v Speaker 1>order of magnitude improvements to </v>
<v Speaker 1>orders of magnitude for breakup.</v>

443
00:33:54.690 --> 00:33:59.450
<v Speaker 1>And here is pseudocode of implementing </v>
<v Speaker 1>dqn.</v>

444
00:33:59.610 --> 00:34:00.270
<v Speaker 1>The learning.</v>

445
00:34:02.540 --> 00:34:05.510
<v Speaker 1>The key thing to notice and you can look</v>
<v Speaker 1>through the slides,</v>

446
00:34:06.140 --> 00:34:08.820
<v Speaker 1>is the,</v>
<v Speaker 1>the,</v>

447
00:34:09.020 --> 00:34:14.020
<v Speaker 1>the loop,</v>
<v Speaker 1>the wild loop of playing through the </v>

448
00:34:14.020 --> 00:34:16.871
<v Speaker 1>games and selecting the actions to play </v>
<v Speaker 1>is not part of the training.</v>

449
00:34:17.920 --> 00:34:20.570
<v Speaker 1>It's part of the saving,</v>
<v Speaker 1>uh,</v>

450
00:34:20.900 --> 00:34:25.900
<v Speaker 1>the observations,</v>
<v Speaker 1>the state action reward next state </v>

451
00:34:25.900 --> 00:34:29.420
<v Speaker 1>observations is saving them and to </v>
<v Speaker 1>replay memory into that library.</v>

452
00:34:29.660 --> 00:34:34.660
<v Speaker 1>And then you sample randomly from that </v>
<v Speaker 1>replay memory to then train the network </v>

453
00:34:35.600 --> 00:34:36.950
<v Speaker 1>based on the last function</v>

454
00:34:38.360 --> 00:34:43.360
<v Speaker 1>and with probability up up top of the </v>
<v Speaker 1>probability epsilon select a random </v>

455
00:34:43.701 --> 00:34:48.701
<v Speaker 1>action that epsilon is the probability </v>
<v Speaker 1>of exploration that decreases at </v>

456
00:34:50.990 --> 00:34:55.990
<v Speaker 1>something you'll see in traffic as well,</v>
<v Speaker 1>is the rate of which that exploration </v>

457
00:34:56.871 --> 00:34:59.270
<v Speaker 1>decreases over time through the training</v>
<v Speaker 1>process.</v>

458
00:34:59.420 --> 00:35:03.170
<v Speaker 1>You want to explore a lot first and less</v>
<v Speaker 1>and less over time.</v>

459
00:35:03.980 --> 00:35:08.980
<v Speaker 1>So this algorithm has been able to </v>
<v Speaker 1>accomplish in 2015 and since a lot of </v>

460
00:35:10.191 --> 00:35:15.191
<v Speaker 1>incredible things,</v>
<v Speaker 1>things that made the ai world think that</v>

461
00:35:17.601 --> 00:35:22.601
<v Speaker 1>we were onto something that general ai </v>
<v Speaker 1>is within reach for the first time that </v>

462
00:35:27.561 --> 00:35:31.650
<v Speaker 1>raw sensor information was used to </v>
<v Speaker 1>create a system that acts it,</v>

463
00:35:31.651 --> 00:35:36.651
<v Speaker 1>make sense of the world,</v>
<v Speaker 1>make sense of the physics of the world </v>

464
00:35:36.651 --> 00:35:38.190
<v Speaker 1>enough to be able to succeed in it for </v>
<v Speaker 1>very little information.</v>

465
00:35:39.180 --> 00:35:44.180
<v Speaker 1>But these games are trivial even though </v>
<v Speaker 1>there is a lot of them.</v>

466
00:35:48.360 --> 00:35:52.680
<v Speaker 1>This dq and approach has been able to </v>
<v Speaker 1>outperform a lot of the Atari Games.</v>

467
00:35:53.070 --> 00:35:56.730
<v Speaker 1>That's what been reported on,</v>
<v Speaker 1>outperform the human level performance.</v>

468
00:35:57.480 --> 00:35:59.430
<v Speaker 1>But again,</v>
<v Speaker 1>these games are trivial.</v>

469
00:36:00.740 --> 00:36:03.630
<v Speaker 1>What I think,</v>
<v Speaker 1>and perhaps biased,</v>

470
00:36:03.810 --> 00:36:08.810
<v Speaker 1>I'm biased,</v>
<v Speaker 1>but one of the greatest accomplishments </v>

471
00:36:08.810 --> 00:36:09.150
<v Speaker 1>of artificial intelligence in the last </v>
<v Speaker 1>decade,</v>

472
00:36:10.620 --> 00:36:14.520
<v Speaker 1>at least from the philosophical or the </v>
<v Speaker 1>research perspective,</v>

473
00:36:15.780 --> 00:36:20.780
<v Speaker 1>is Alphago Zero First Alphago,</v>
<v Speaker 1>Alphago zero is deepmind system that </v>

474
00:36:26.160 --> 00:36:29.250
<v Speaker 1>beat the best in the world in the game </v>
<v Speaker 1>of go.</v>

475
00:36:29.820 --> 00:36:34.820
<v Speaker 1>So what's the game of go simple.</v>
<v Speaker 1>I won't get into the rules,</v>

476
00:36:36.390 --> 00:36:41.390
<v Speaker 1>but basically it's a 19 by 19 board </v>
<v Speaker 1>showing on the bottom of the slide for </v>

477
00:36:43.021 --> 00:36:47.520
<v Speaker 1>the bottom row of the table for board of</v>
<v Speaker 1>19 by 19.</v>

478
00:36:48.510 --> 00:36:53.510
<v Speaker 1>The number of legal game positions is </v>
<v Speaker 1>two times 10 to the power of one 70.</v>

479
00:36:56.300 --> 00:37:01.140
<v Speaker 1>It's a very large number of possible </v>
<v Speaker 1>positions to consider at any one time,</v>

480
00:37:01.320 --> 00:37:05.610
<v Speaker 1>especially the game evolves.</v>
<v Speaker 1>The number of possible moves is huge,</v>

481
00:37:07.170 --> 00:37:12.170
<v Speaker 1>much larger than in chess.</v>
<v Speaker 1>So that's why Ai Community thought that </v>

482
00:37:14.161 --> 00:37:19.161
<v Speaker 1>this game is not solvable until 2016 </v>
<v Speaker 1>when Alphago used to use human expert </v>

483
00:37:25.231 --> 00:37:29.730
<v Speaker 1>position play to seed in a supervised </v>
<v Speaker 1>way.</v>

484
00:37:30.270 --> 00:37:35.270
<v Speaker 1>Reinforcement learning approach,</v>
<v Speaker 1>and I'll describe it in a little bit of </v>

485
00:37:35.270 --> 00:37:39.081
<v Speaker 1>detail in this couple of slides here to </v>
<v Speaker 1>beat the best in the world</v>

486
00:37:42.330 --> 00:37:47.330
<v Speaker 1>and then Alphago zero.</v>
<v Speaker 1>That is the accomplishment of the decade</v>

487
00:37:49.320 --> 00:37:54.320
<v Speaker 1>for me in Ai is being able to play with </v>
<v Speaker 1>no training data on human expert Games </v>

488
00:38:03.600 --> 00:38:06.930
<v Speaker 1>and beat the best in the world and an </v>
<v Speaker 1>extremely complex game.</v>

489
00:38:06.960 --> 00:38:11.960
<v Speaker 1>This is not a tare.</v>
<v Speaker 1>This is an this is a a much higher order</v>

490
00:38:15.060 --> 00:38:20.060
<v Speaker 1>difficulty game and the and the quality </v>
<v Speaker 1>of player that is competing in as much </v>

491
00:38:20.060 --> 00:38:24.981
<v Speaker 1>higher and it's able to extremely </v>
<v Speaker 1>quickly here to achieve a rating that's </v>

492
00:38:25.410 --> 00:38:30.410
<v Speaker 1>better than Alphago and better than the </v>
<v Speaker 1>different variants of Alphago and </v>

493
00:38:31.531 --> 00:38:36.531
<v Speaker 1>certainly better than the best of the </v>
<v Speaker 1>human players in 20 days of self play.</v>

494
00:38:38.830 --> 00:38:41.980
<v Speaker 1>So how does it work?</v>
<v Speaker 1>All of these approaches,</v>

495
00:38:42.550 --> 00:38:44.770
<v Speaker 1>much,</v>
<v Speaker 1>much like the previous ones.</v>

496
00:38:44.800 --> 00:38:49.800
<v Speaker 1>The traditional ones that are not based </v>
<v Speaker 1>on deep learning are using Monte Carlo </v>

497
00:38:51.131 --> 00:38:56.131
<v Speaker 1>tree search MCTs,</v>
<v Speaker 1>which is when you have such a large </v>

498
00:38:57.670 --> 00:39:02.670
<v Speaker 1>state space,</v>
<v Speaker 1>you start at a board and you play and </v>

499
00:39:02.831 --> 00:39:07.831
<v Speaker 1>you choose moves with some exploitation </v>
<v Speaker 1>exploration.</v>

500
00:39:08.740 --> 00:39:13.740
<v Speaker 1>Balancing choosing to explore a totally </v>
<v Speaker 1>new positions or to go deep on the </v>

501
00:39:14.730 --> 00:39:19.730
<v Speaker 1>positions you know are good until the </v>
<v Speaker 1>bottom of the game is reached until the </v>

502
00:39:19.730 --> 00:39:23.700
<v Speaker 1>final state is reached and then you back</v>
<v Speaker 1>propagate the quality of the choices </v>

503
00:39:24.731 --> 00:39:29.731
<v Speaker 1>you've made leading to that position and</v>
<v Speaker 1>in that way you learn the value of of </v>

504
00:39:30.940 --> 00:39:35.670
<v Speaker 1>board positions and play that's been </v>
<v Speaker 1>used by the most successful.</v>

505
00:39:35.710 --> 00:39:40.360
<v Speaker 1>Go playing engines before and Alphago </v>
<v Speaker 1>sense,</v>

506
00:39:41.710 --> 00:39:46.710
<v Speaker 1>but you might be able to guess what's </v>
<v Speaker 1>the difference with Alphago versus the </v>

507
00:39:46.710 --> 00:39:50.671
<v Speaker 1>previous approaches.</v>
<v Speaker 1>They use the neural network as the </v>

508
00:39:52.540 --> 00:39:56.560
<v Speaker 1>intuition quote on quote to what are the</v>
<v Speaker 1>good states,</v>

509
00:39:56.890 --> 00:40:00.790
<v Speaker 1>what are the good next board positions </v>
<v Speaker 1>to explore,</v>

510
00:40:05.870 --> 00:40:08.030
<v Speaker 1>and the key things,</v>
<v Speaker 1>again,</v>

511
00:40:08.090 --> 00:40:13.090
<v Speaker 1>the tricks make all the difference that </v>
<v Speaker 1>made Alphago zero work and work much </v>

512
00:40:15.231 --> 00:40:20.231
<v Speaker 1>better than Alphago is first because </v>
<v Speaker 1>there was no expert play instead of </v>

513
00:40:20.511 --> 00:40:21.530
<v Speaker 1>human games.</v>

514
00:40:23.890 --> 00:40:28.890
<v Speaker 1>Alphago used that very same Monte Carlo </v>
<v Speaker 1>tree search algorithm,</v>

515
00:40:30.640 --> 00:40:35.640
<v Speaker 1>MCTs to do an intelligent look ahead </v>
<v Speaker 1>based on in neural network prediction of</v>

516
00:40:36.461 --> 00:40:41.461
<v Speaker 1>what are the good states to take.</v>
<v Speaker 1>It checked that instead of human expert </v>

517
00:40:41.651 --> 00:40:46.000
<v Speaker 1>play it checked.</v>
<v Speaker 1>How good indeed are those states?</v>

518
00:40:46.990 --> 00:40:51.990
<v Speaker 1>It's a simple look ahead action that </v>
<v Speaker 1>does the ground truth that does the </v>

519
00:40:52.240 --> 00:40:57.240
<v Speaker 1>target,</v>
<v Speaker 1>a correction that produces the last </v>

520
00:40:57.240 --> 00:40:59.941
<v Speaker 1>function.</v>
<v Speaker 1>The second part is the multitask </v>

521
00:40:59.941 --> 00:41:02.611
<v Speaker 1>learning or what's now called multitask </v>
<v Speaker 1>learning is the network is is a quote </v>

522
00:41:02.621 --> 00:41:07.621
<v Speaker 1>unquote two headed in the sense that </v>
<v Speaker 1>first it outputs the probability of </v>

523
00:41:07.621 --> 00:41:11.080
<v Speaker 1>which moved to take the obvious thing </v>
<v Speaker 1>and it's also producing a probability of</v>

524
00:41:11.081 --> 00:41:16.081
<v Speaker 1>winning and there's a few ways to </v>
<v Speaker 1>combine that information and </v>

525
00:41:16.081 --> 00:41:20.521
<v Speaker 1>continuously train both parts of the </v>
<v Speaker 1>network depending on the choice taken.</v>

526
00:41:21.280 --> 00:41:26.280
<v Speaker 1>So you want to take the best choice in </v>
<v Speaker 1>the short term and achieved the </v>

527
00:41:26.280 --> 00:41:29.890
<v Speaker 1>positions that are highly likelihood of </v>
<v Speaker 1>winning for the player.</v>

528
00:41:29.920 --> 00:41:34.920
<v Speaker 1>That's whose turn it is,</v>
<v Speaker 1>and another big step is that they </v>

529
00:41:37.261 --> 00:41:41.820
<v Speaker 1>updated from 2015 the update of the </v>
<v Speaker 1>state of the art architecture,</v>

530
00:41:41.880 --> 00:41:46.670
<v Speaker 1>which are now the architecture.</v>
<v Speaker 1>That one image that is residual networks</v>

531
00:41:46.820 --> 00:41:50.880
<v Speaker 1>resume yet for image net.</v>
<v Speaker 1>Those that's it.</v>

532
00:41:51.350 --> 00:41:54.240
<v Speaker 1>And those little changes made all the </v>
<v Speaker 1>difference.</v>

533
00:41:55.680 --> 00:42:00.000
<v Speaker 1>So that takes us to do deep traffic and </v>
<v Speaker 1>8 billion hours stuck in traffic</v>

534
00:42:02.010 --> 00:42:07.010
<v Speaker 1>America's pastime.</v>
<v Speaker 1>So we tried to simulate driving that </v>

535
00:42:07.971 --> 00:42:12.050
<v Speaker 1>behavior layer of driving,</v>
<v Speaker 1>so not the immediate control,</v>

536
00:42:12.200 --> 00:42:15.830
<v Speaker 1>not the motion planning,</v>
<v Speaker 1>but beyond that on top,</v>

537
00:42:16.040 --> 00:42:21.040
<v Speaker 1>on top of those control decisions,</v>
<v Speaker 1>the human interpretable decisions of </v>

538
00:42:21.321 --> 00:42:23.450
<v Speaker 1>changing lane speeding up,</v>
<v Speaker 1>slowing down,</v>

539
00:42:23.660 --> 00:42:28.660
<v Speaker 1>modeling that in a microtrauma traffic </v>
<v Speaker 1>simulation framework that's popular and </v>

540
00:42:28.660 --> 00:42:31.250
<v Speaker 1>traffic engineering,</v>
<v Speaker 1>the kindness shown here,</v>

541
00:42:32.890 --> 00:42:35.320
<v Speaker 1>we applied deeper enforcement learning </v>
<v Speaker 1>to that,</v>

542
00:42:35.720 --> 00:42:40.720
<v Speaker 1>we call it deep traffic.</v>
<v Speaker 1>The goal is to achieve the highest </v>

543
00:42:40.720 --> 00:42:44.701
<v Speaker 1>average speed over a long period of time</v>
<v Speaker 1>weaving in and out of traffic for </v>

544
00:42:45.161 --> 00:42:50.161
<v Speaker 1>students here,</v>
<v Speaker 1>their requirement is to follow the </v>

545
00:42:50.161 --> 00:42:51.370
<v Speaker 1>tutorial and that Shiva speed of 65 </v>
<v Speaker 1>miles an hour,</v>

546
00:42:54.060 --> 00:42:59.060
<v Speaker 1>and if you really want to achieve a </v>
<v Speaker 1>speed over 70 miles an hour,</v>

547
00:42:59.460 --> 00:43:04.460
<v Speaker 1>which is what's required to win and </v>
<v Speaker 1>perhaps upload your own image to make </v>

548
00:43:06.431 --> 00:43:11.110
<v Speaker 1>sure you look good doing it.</v>
<v Speaker 1>What you should do,</v>

549
00:43:11.230 --> 00:43:14.620
<v Speaker 1>clear instructions to compete.</v>
<v Speaker 1>Read the tutorial.</v>

550
00:43:16.980 --> 00:43:20.340
<v Speaker 1>You can change parameters in the cold </v>
<v Speaker 1>box on that website.</v>

551
00:43:20.430 --> 00:43:22.800
<v Speaker 1>Cars that mit.edu</v>
<v Speaker 1>site traffic.</v>

552
00:43:23.250 --> 00:43:27.690
<v Speaker 1>Click the button that says apply code,</v>
<v Speaker 1>which applies the code that you write.</v>

553
00:43:27.720 --> 00:43:29.690
<v Speaker 1>These are the parameters that you </v>
<v Speaker 1>specify for.</v>

554
00:43:29.691 --> 00:43:33.450
<v Speaker 1>Then you'll network it,</v>
<v Speaker 1>applies those parameters,</v>

555
00:43:33.451 --> 00:43:35.790
<v Speaker 1>creates the architecture that you </v>
<v Speaker 1>specify,</v>

556
00:43:35.940 --> 00:43:40.940
<v Speaker 1>and now you have a network written in </v>
<v Speaker 1>javascript living in the browser ready </v>

557
00:43:40.940 --> 00:43:44.130
<v Speaker 1>to be trained.</v>
<v Speaker 1>Then you click the blue button that says</v>

558
00:43:44.131 --> 00:43:47.670
<v Speaker 1>run training and that trains the network</v>

559
00:43:49.030 --> 00:43:54.030
<v Speaker 1>much faster than what's actually being </v>
<v Speaker 1>visualized in the browser a thousand </v>

560
00:43:54.221 --> 00:43:57.700
<v Speaker 1>times faster by evolving the game,</v>
<v Speaker 1>making decisions,</v>

561
00:43:57.701 --> 00:44:00.610
<v Speaker 1>taking in the grid space,</v>
<v Speaker 1>as I'll talk about here in a second,</v>

562
00:44:00.940 --> 00:44:05.940
<v Speaker 1>the speed limit is 80 miles an hour </v>
<v Speaker 1>based on the various adjustments were </v>

563
00:44:05.940 --> 00:44:08.670
<v Speaker 1>made to the game.</v>
<v Speaker 1>Reaching 80 miles an hour,</v>

564
00:44:08.700 --> 00:44:13.700
<v Speaker 1>certainly impossible on average and </v>
<v Speaker 1>reaching some of the speeds that we </v>

565
00:44:13.700 --> 00:44:16.120
<v Speaker 1>achieved last year is much,</v>
<v Speaker 1>much,</v>

566
00:44:16.121 --> 00:44:19.180
<v Speaker 1>much more difficult.</v>
<v Speaker 1>Finally,</v>

567
00:44:19.210 --> 00:44:21.430
<v Speaker 1>when you are happy and the training is </v>
<v Speaker 1>done,</v>

568
00:44:22.950 --> 00:44:27.950
<v Speaker 1>submit the model to competition for </v>
<v Speaker 1>those super eager,</v>

569
00:44:28.281 --> 00:44:33.281
<v Speaker 1>dedicated students.</v>
<v Speaker 1>You can do so every five minutes and to </v>

570
00:44:33.751 --> 00:44:38.751
<v Speaker 1>visualize your submission,</v>
<v Speaker 1>you can click the request visualization,</v>

571
00:44:40.141 --> 00:44:42.360
<v Speaker 1>specifying the custom image and the </v>
<v Speaker 1>color.</v>

572
00:44:44.980 --> 00:44:47.110
<v Speaker 1>Okay,</v>
<v Speaker 1>so here's the simulation.</v>

573
00:44:47.140 --> 00:44:48.970
<v Speaker 1>Speed limit,</v>
<v Speaker 1>80 miles an hour,</v>

574
00:44:49.570 --> 00:44:53.860
<v Speaker 1>cars 20 on the screen,</v>
<v Speaker 1>one of them is a red one in this case,</v>

575
00:44:54.060 --> 00:44:56.320
<v Speaker 1>that's that one is controlled by neural </v>
<v Speaker 1>network.</v>

576
00:44:56.770 --> 00:44:59.290
<v Speaker 1>It's speed.</v>
<v Speaker 1>It's allowed the actions of speed up,</v>

577
00:44:59.291 --> 00:45:01.990
<v Speaker 1>slow down,</v>
<v Speaker 1>change lanes,</v>

578
00:45:02.170 --> 00:45:05.110
<v Speaker 1>left,</v>
<v Speaker 1>right or stay exactly the same.</v>

579
00:45:09.000 --> 00:45:13.080
<v Speaker 1>The other cars are pretty dumb.</v>
<v Speaker 1>They speed up,</v>

580
00:45:13.081 --> 00:45:14.400
<v Speaker 1>slow down,</v>
<v Speaker 1>turn left,</v>

581
00:45:14.401 --> 00:45:19.401
<v Speaker 1>right,</v>
<v Speaker 1>but they don't have a purpose in their </v>

582
00:45:19.401 --> 00:45:21.621
<v Speaker 1>existence.</v>
<v Speaker 1>They do so randomly or at least purpose </v>

583
00:45:21.621 --> 00:45:24.480
<v Speaker 1>has not been discovered.</v>
<v Speaker 1>The road,</v>

584
00:45:24.481 --> 00:45:25.560
<v Speaker 1>the car,</v>
<v Speaker 1>the speed,</v>

585
00:45:25.620 --> 00:45:30.620
<v Speaker 1>the road is a grid space and occupancy </v>
<v Speaker 1>grid that specifies when it's empty.</v>

586
00:45:33.300 --> 00:45:34.830
<v Speaker 1>It's set to</v>

587
00:45:35.920 --> 00:45:40.920
<v Speaker 1>Ab,</v>
<v Speaker 1>meaning that the grid value is whatever </v>

588
00:45:42.881 --> 00:45:47.881
<v Speaker 1>speed is achievable.</v>
<v Speaker 1>If you were inside that grid and when </v>

589
00:45:47.881 --> 00:45:52.261
<v Speaker 1>there's other cars that are going slow,</v>
<v Speaker 1>the value in that grid is the speed of </v>

590
00:45:52.261 --> 00:45:53.530
<v Speaker 1>that car.</v>
<v Speaker 1>That's the state space.</v>

591
00:45:53.531 --> 00:45:56.920
<v Speaker 1>That's the state representation and you </v>
<v Speaker 1>can choose how much,</v>

592
00:45:57.040 --> 00:45:59.230
<v Speaker 1>what slice that state space you've </v>
<v Speaker 1>taken.</v>

593
00:45:59.380 --> 00:46:04.380
<v Speaker 1>That's the input to the neural network.</v>
<v Speaker 1>For visualization purposes,</v>

594
00:46:07.261 --> 00:46:12.261
<v Speaker 1>you can choose normal speed or fast </v>
<v Speaker 1>speed for watching the network operate</v>

595
00:46:15.140 --> 00:46:20.140
<v Speaker 1>and there's display options to help you </v>
<v Speaker 1>build intuition about the network takes </v>

596
00:46:20.140 --> 00:46:24.251
<v Speaker 1>in and what space the cars operating in.</v>
<v Speaker 1>The default is there's no extra </v>

597
00:46:24.321 --> 00:46:29.321
<v Speaker 1>information is added.</v>
<v Speaker 1>Then there's the learning input which </v>

598
00:46:29.321 --> 00:46:32.891
<v Speaker 1>visualizes exactly which part of the </v>
<v Speaker 1>road the serves as the input to the </v>

599
00:46:32.961 --> 00:46:36.230
<v Speaker 1>network.</v>
<v Speaker 1>Then there is the safety system,</v>

600
00:46:36.440 --> 00:46:41.440
<v Speaker 1>which I'll describe in a little bit,</v>
<v Speaker 1>which is all the parts of the road the </v>

601
00:46:41.440 --> 00:46:44.981
<v Speaker 1>car is not allowed to go into because it</v>
<v Speaker 1>would result in a collision and that </v>

602
00:46:44.981 --> 00:46:48.710
<v Speaker 1>with javascript will be very difficult </v>
<v Speaker 1>to animate and the full map.</v>

603
00:46:50.100 --> 00:46:55.100
<v Speaker 1>Here's the safety system.</v>
<v Speaker 1>You can think of this system as ACC </v>

604
00:46:55.100 --> 00:47:00.050
<v Speaker 1>basic radar ultrasonic sensors,</v>
<v Speaker 1>helping you avoid the obvious collisions</v>

605
00:47:00.571 --> 00:47:03.240
<v Speaker 1>to obviously detectable objects around </v>
<v Speaker 1>you.</v>

606
00:47:03.450 --> 00:47:07.650
<v Speaker 1>And the task for this red car,</v>
<v Speaker 1>for this neural network is to move about</v>

607
00:47:07.890 --> 00:47:09.840
<v Speaker 1>this space,</v>
<v Speaker 1>uh,</v>

608
00:47:10.190 --> 00:47:14.400
<v Speaker 1>is to move about the space under the </v>
<v Speaker 1>constraints of the safety system.</v>

609
00:47:16.150 --> 00:47:18.710
<v Speaker 1>The red shows all the parts of the </v>
<v Speaker 1>greatest,</v>

610
00:47:18.730 --> 00:47:23.730
<v Speaker 1>not able to move into.</v>
<v Speaker 1>So the goal for the car is to not get </v>

611
00:47:23.730 --> 00:47:28.651
<v Speaker 1>stuck in traffic,</v>
<v Speaker 1>is make big sweeping motions to avoid </v>

612
00:47:29.290 --> 00:47:34.290
<v Speaker 1>crowds of cars.</v>
<v Speaker 1>The input like dqn is the state space.</v>

613
00:47:36.490 --> 00:47:41.490
<v Speaker 1>The output is the value of the different</v>
<v Speaker 1>actions and based on the absalon </v>

614
00:47:41.490 --> 00:47:46.291
<v Speaker 1>parameter to training and through </v>
<v Speaker 1>inference evaluation process,</v>

615
00:47:47.470 --> 00:47:50.290
<v Speaker 1>you choose how much exploration you want</v>
<v Speaker 1>to do.</v>

616
00:47:50.380 --> 00:47:55.380
<v Speaker 1>These are all parameters.</v>
<v Speaker 1>The learning is done in the browser on </v>

617
00:47:55.481 --> 00:47:56.440
<v Speaker 1>your own computer,</v>

618
00:47:59.020 --> 00:48:00.820
<v Speaker 1>utilizing only the CPU,</v>

619
00:48:02.700 --> 00:48:04.680
<v Speaker 1>the action space.</v>
<v Speaker 1>There's five,</v>

620
00:48:05.460 --> 00:48:10.460
<v Speaker 1>giving you some of the variables here.</v>
<v Speaker 1>Perhaps you'd go back to the slides to </v>

621
00:48:10.460 --> 00:48:10.710
<v Speaker 1>look at it.</v>
<v Speaker 1>The brain,</v>

622
00:48:10.711 --> 00:48:15.711
<v Speaker 1>quote unquote,</v>
<v Speaker 1>is the thing that takes in the state and</v>

623
00:48:15.961 --> 00:48:19.730
<v Speaker 1>the reward takes a forward pass to the </v>
<v Speaker 1>state and producer.</v>

624
00:48:19.740 --> 00:48:23.320
<v Speaker 1>The next action,</v>
<v Speaker 1>the brain is where the neural can.</v>

625
00:48:23.410 --> 00:48:26.490
<v Speaker 1>It's contained both of the training and </v>
<v Speaker 1>the evaluation.</v>

626
00:48:27.770 --> 00:48:30.990
<v Speaker 1>The learning input can be controlled in </v>
<v Speaker 1>width,</v>

627
00:48:32.070 --> 00:48:37.070
<v Speaker 1>forward length and backward length.</v>
<v Speaker 1>Lane side number of lanes to the side </v>

628
00:48:37.070 --> 00:48:41.781
<v Speaker 1>that you see patches ahead as the </v>
<v Speaker 1>patches ahead that you see patches </v>

629
00:48:41.781 --> 00:48:46.071
<v Speaker 1>behind his patches behind these.</v>
<v Speaker 1>See new this year can control the number</v>

630
00:48:47.641 --> 00:48:52.641
<v Speaker 1>of agents that are controlled by the </v>
<v Speaker 1>neural network anywhere from one to 10,</v>

631
00:48:58.170 --> 00:49:01.320
<v Speaker 1>and the evaluation is performed exactly </v>
<v Speaker 1>the same way.</v>

632
00:49:01.680 --> 00:49:04.680
<v Speaker 1>You have to achieve the highest average </v>
<v Speaker 1>speed for the agents.</v>

633
00:49:06.000 --> 00:49:11.000
<v Speaker 1>The very critical thing here is the </v>
<v Speaker 1>agents are not aware of each other so </v>

634
00:49:12.481 --> 00:49:17.481
<v Speaker 1>they're not jointly jointly planning.</v>
<v Speaker 1>The network is trained under the joint </v>

635
00:49:20.880 --> 00:49:24.510
<v Speaker 1>objective of achieving the average speed</v>
<v Speaker 1>for all of them,</v>

636
00:49:25.200 --> 00:49:28.650
<v Speaker 1>but the actions of taking in a greedy </v>
<v Speaker 1>way for each.</v>

637
00:49:29.370 --> 00:49:34.370
<v Speaker 1>It's very interesting what can be </v>
<v Speaker 1>learned in this way because this kinds </v>

638
00:49:34.370 --> 00:49:38.751
<v Speaker 1>of approaches are scalable to an </v>
<v Speaker 1>arbitrary number of cars and you could </v>

639
00:49:38.751 --> 00:49:43.191
<v Speaker 1>imagine us plopping down the best cars </v>
<v Speaker 1>from this class together and having them</v>

640
00:49:44.881 --> 00:49:49.881
<v Speaker 1>compete in this way,</v>
<v Speaker 1>the best neural networks because there </v>

641
00:49:51.291 --> 00:49:56.291
<v Speaker 1>are full in their greedy operation.</v>
<v Speaker 1>The number of networks that can </v>

642
00:49:56.291 --> 00:50:00.901
<v Speaker 1>concurrently operate is fully scalable.</v>
<v Speaker 1>There's a lot of parameters.</v>

643
00:50:03.460 --> 00:50:07.600
<v Speaker 1>The temporal window,</v>
<v Speaker 1>the layers,</v>

644
00:50:07.601 --> 00:50:12.601
<v Speaker 1>the many layers types that can be added </v>
<v Speaker 1>here is a fully connected layer with 10 </v>

645
00:50:12.601 --> 00:50:13.840
<v Speaker 1>year olds.</v>
<v Speaker 1>The activation functions.</v>

646
00:50:13.841 --> 00:50:17.710
<v Speaker 1>All of these things can be customized as</v>
<v Speaker 1>specified in the tutorial,</v>

647
00:50:18.790 --> 00:50:23.790
<v Speaker 1>the final layer,</v>
<v Speaker 1>a fully connected layer with I'll put a </v>

648
00:50:23.790 --> 00:50:27.841
<v Speaker 1>five regression given the value of each </v>
<v Speaker 1>of the five actions and there's a lot of</v>

649
00:50:29.750 --> 00:50:34.750
<v Speaker 1>more specific parameters,</v>
<v Speaker 1>some of which I've discussed from gamma </v>

650
00:50:36.000 --> 00:50:37.070
<v Speaker 1>to epsilon</v>

651
00:50:38.350 --> 00:50:43.350
<v Speaker 1>to experience replay size to learning </v>
<v Speaker 1>rate and temporal window,</v>

652
00:50:45.850 --> 00:50:48.730
<v Speaker 1>the optimizer,</v>
<v Speaker 1>the learning rate momentum,</v>

653
00:50:48.731 --> 00:50:50.970
<v Speaker 1>batch size l two l,</v>
<v Speaker 1>one,</v>

654
00:50:50.990 --> 00:50:52.990
<v Speaker 1>two,</v>
<v Speaker 1>k for regularization and so on.</v>

655
00:50:53.650 --> 00:50:58.180
<v Speaker 1>There's a big white button that says </v>
<v Speaker 1>apply code that you press that kills all</v>

656
00:50:58.181 --> 00:51:00.820
<v Speaker 1>the work you've done up to this point,</v>
<v Speaker 1>so be careful doing it.</v>

657
00:51:00.821 --> 00:51:03.700
<v Speaker 1>It should be doing it only at the very </v>
<v Speaker 1>beginning.</v>

658
00:51:05.580 --> 00:51:09.360
<v Speaker 1>If you happen to leave your computer </v>
<v Speaker 1>running and training for several days is</v>

659
00:51:09.600 --> 00:51:13.020
<v Speaker 1>as folks have done the blue training </v>
<v Speaker 1>button,</v>

660
00:51:13.080 --> 00:51:18.080
<v Speaker 1>you press and it trains based on the </v>
<v Speaker 1>parameters you specify and the network </v>

661
00:51:18.080 --> 00:51:20.910
<v Speaker 1>state gets shipped to the main </v>
<v Speaker 1>simulation from time to time.</v>

662
00:51:21.060 --> 00:51:26.060
<v Speaker 1>So the thing you see in the browser,</v>
<v Speaker 1>as you open up the website is running </v>

663
00:51:26.060 --> 00:51:30.050
<v Speaker 1>the same network that's being trained </v>
<v Speaker 1>and regularly updates that network so </v>

664
00:51:30.050 --> 00:51:34.881
<v Speaker 1>it's getting better and better.</v>
<v Speaker 1>Even if the training takes weeks for </v>

665
00:51:34.881 --> 00:51:37.941
<v Speaker 1>you,</v>
<v Speaker 1>it's constantly updated the network you </v>

666
00:51:37.941 --> 00:51:40.811
<v Speaker 1>see on the left,</v>
<v Speaker 1>so if the car for the network that your </v>

667
00:51:40.811 --> 00:51:41.970
<v Speaker 1>training is just standing in place and </v>
<v Speaker 1>not moving,</v>

668
00:51:42.690 --> 00:51:47.430
<v Speaker 1>it's probably time to restart and change</v>
<v Speaker 1>the parameters.</v>

669
00:51:47.520 --> 00:51:52.110
<v Speaker 1>Maybe add a few layers to your network,</v>
<v Speaker 1>number of iterations,</v>

670
00:51:52.111 --> 00:51:57.111
<v Speaker 1>a certainly an important parameter to </v>
<v Speaker 1>control and the evaluation is something </v>

671
00:51:58.111 --> 00:52:03.111
<v Speaker 1>we've done a lot of work done since last</v>
<v Speaker 1>year to remove the degree of randomness,</v>

672
00:52:03.271 --> 00:52:08.271
<v Speaker 1>to remove the the incentive to submit </v>
<v Speaker 1>the same code over and over again to </v>

673
00:52:09.151 --> 00:52:13.050
<v Speaker 1>hope to produce a higher award,</v>
<v Speaker 1>a higher evaluation score.</v>

674
00:52:14.580 --> 00:52:19.580
<v Speaker 1>The method for evaluation is we collect </v>
<v Speaker 1>the average speed over 10 runs,</v>

675
00:52:19.950 --> 00:52:24.950
<v Speaker 1>about 45 seconds of game each,</v>
<v Speaker 1>not minutes,</v>

676
00:52:26.400 --> 00:52:31.400
<v Speaker 1>45 seconds,</v>
<v Speaker 1>and there is five hundreds of those and </v>

677
00:52:31.400 --> 00:52:35.490
<v Speaker 1>we take the median speed of the 500 runs</v>
<v Speaker 1>has done server side.</v>

678
00:52:35.550 --> 00:52:38.670
<v Speaker 1>It's extremely difficult to cheat.</v>
<v Speaker 1>I urge you to try.</v>

679
00:52:39.870 --> 00:52:43.470
<v Speaker 1>You can try it locally.</v>
<v Speaker 1>There's a start evaluation run,</v>

680
00:52:43.471 --> 00:52:46.200
<v Speaker 1>but that one doesn't count.</v>
<v Speaker 1>That's just for you to feel better about</v>

681
00:52:46.201 --> 00:52:48.780
<v Speaker 1>your network.</v>
<v Speaker 1>That's.</v>

682
00:52:48.810 --> 00:52:52.140
<v Speaker 1>That should produce a result that's very</v>
<v Speaker 1>similar to the one we were produced on.</v>

683
00:52:52.141 --> 00:52:57.141
<v Speaker 1>The server is to build your own </v>
<v Speaker 1>intuition and as I said,</v>

684
00:52:57.301 --> 00:52:59.490
<v Speaker 1>was significantly reduce the influence </v>
<v Speaker 1>of randomness,</v>

685
00:52:59.820 --> 00:53:04.820
<v Speaker 1>so the the score of the speed you get </v>
<v Speaker 1>for the network you designed should be </v>

686
00:53:05.010 --> 00:53:10.010
<v Speaker 1>very similar with every evaluation.</v>
<v Speaker 1>Loading is saving.</v>

687
00:53:10.620 --> 00:53:13.350
<v Speaker 1>If the network is huge and you want to </v>
<v Speaker 1>switch computers,</v>

688
00:53:13.351 --> 00:53:18.351
<v Speaker 1>you can save the network.</v>
<v Speaker 1>It saves both the architecture of the </v>

689
00:53:18.351 --> 00:53:21.021
<v Speaker 1>network and the weights and the on the </v>
<v Speaker 1>network and you can load it back in.</v>

690
00:53:22.950 --> 00:53:27.950
<v Speaker 1>Obviously when you load it in,</v>
<v Speaker 1>it's not a saving any of the data you've</v>

691
00:53:28.591 --> 00:53:33.591
<v Speaker 1>already done.</v>
<v Speaker 1>You can't do transfer learning with </v>

692
00:53:33.591 --> 00:53:35.640
<v Speaker 1>javascript in the browser yet submitting</v>
<v Speaker 1>your network,</v>

693
00:53:35.641 --> 00:53:39.720
<v Speaker 1>submit model to competition and make </v>
<v Speaker 1>sure you run training first.</v>

694
00:53:39.810 --> 00:53:44.810
<v Speaker 1>Otherwise it will be initiated or </v>
<v Speaker 1>initiated randomly and will not do so </v>

695
00:53:44.810 --> 00:53:47.370
<v Speaker 1>well.</v>
<v Speaker 1>You can resubmit as often as you like,</v>

696
00:53:47.371 --> 00:53:52.371
<v Speaker 1>and the highest score is what counts.</v>
<v Speaker 1>The coolest part is you can load your </v>

697
00:53:52.371 --> 00:53:54.570
<v Speaker 1>custom image,</v>
<v Speaker 1>specify colors,</v>

698
00:53:54.630 --> 00:53:56.220
<v Speaker 1>and request the visualization.</v>

699
00:53:57.490 --> 00:54:02.490
<v Speaker 1>We have not yet shown the visualization,</v>
<v Speaker 1>but I promise you it's going to be </v>

700
00:54:02.490 --> 00:54:03.580
<v Speaker 1>awesome.</v>
<v Speaker 1>Again,</v>

701
00:54:03.850 --> 00:54:06.850
<v Speaker 1>read the tutorial,</v>
<v Speaker 1>change the parameters and the code box.</v>

702
00:54:06.880 --> 00:54:11.880
<v Speaker 1>Click apply code run training.</v>
<v Speaker 1>Everybody in this room on the way home </v>

703
00:54:11.880 --> 00:54:14.350
<v Speaker 1>on the train,</v>
<v Speaker 1>hopefully not in your car,</v>

704
00:54:14.410 --> 00:54:19.410
<v Speaker 1>should be able to do this in the browser</v>
<v Speaker 1>and they can visualize request </v>

705
00:54:19.410 --> 00:54:20.530
<v Speaker 1>visualization because it's an expensive </v>
<v Speaker 1>process.</v>

706
00:54:20.680 --> 00:54:25.630
<v Speaker 1>You have to want it for us to do it well</v>
<v Speaker 1>because we have to run in server side.</v>

707
00:54:28.030 --> 00:54:31.840
<v Speaker 1>Competition link is their get hub </v>
<v Speaker 1>started.</v>

708
00:54:31.841 --> 00:54:35.920
<v Speaker 1>Code is there and the details for those </v>
<v Speaker 1>that truly want to win is in the archive</v>

709
00:54:35.921 --> 00:54:40.921
<v Speaker 1>paper.</v>
<v Speaker 1>So the question that will come up </v>

710
00:54:40.921 --> 00:54:44.130
<v Speaker 1>throughout is whether these </v>
<v Speaker 1>reinforcement learning approaches are at</v>

711
00:54:44.131 --> 00:54:48.720
<v Speaker 1>all or rather if action planning control</v>
<v Speaker 1>is amenable to learning.</v>

712
00:54:50.510 --> 00:54:53.340
<v Speaker 1>Certainly in the case of driving,</v>
<v Speaker 1>we can't do it.</v>

713
00:54:53.341 --> 00:54:58.341
<v Speaker 1>Alpha go zero did.</v>
<v Speaker 1>We can't learn from scratch from self </v>

714
00:54:59.331 --> 00:55:04.331
<v Speaker 1>play because that will result in </v>
<v Speaker 1>millions of crashes in order to learn to</v>

715
00:55:06.621 --> 00:55:11.621
<v Speaker 1>avoid the crashes.</v>
<v Speaker 1>Unless we're working like we are deep </v>

716
00:55:11.621 --> 00:55:15.161
<v Speaker 1>crashed on the RC car or we're working </v>
<v Speaker 1>on a simulation so we can look at expert</v>

717
00:55:15.741 --> 00:55:17.330
<v Speaker 1>data,</v>
<v Speaker 1>we can look at driver data,</v>

718
00:55:17.331 --> 00:55:20.110
<v Speaker 1>which we have a lot of and learn from.</v>
<v Speaker 1>It's an open question.</v>

719
00:55:20.260 --> 00:55:25.260
<v Speaker 1>This is applicable to date that I'll </v>
<v Speaker 1>bring up two companies because they're </v>

720
00:55:25.811 --> 00:55:26.980
<v Speaker 1>both guest speakers</v>

721
00:55:28.420 --> 00:55:33.420
<v Speaker 1>deep irl is not involved in the most </v>
<v Speaker 1>successful robots operating in the real </v>

722
00:55:33.420 --> 00:55:37.500
<v Speaker 1>world.</v>
<v Speaker 1>In the case of Boston dynamics,</v>

723
00:55:38.550 --> 00:55:43.550
<v Speaker 1>most of the perception control and </v>
<v Speaker 1>planning I can.</v>

724
00:55:43.841 --> 00:55:48.841
<v Speaker 1>This robot does not involve learning </v>
<v Speaker 1>approaches except with minimal addition </v>

725
00:55:49.861 --> 00:55:54.861
<v Speaker 1>on the perception side.</v>
<v Speaker 1>Best of our knowledge and certainly the </v>

726
00:55:56.311 --> 00:55:59.490
<v Speaker 1>same is true with Waymo.</v>
<v Speaker 1>As the speaker on Friday,</v>

727
00:55:59.491 --> 00:56:04.110
<v Speaker 1>we'll talk about deep learning is used a</v>
<v Speaker 1>little bit in perception on top,</v>

728
00:56:04.350 --> 00:56:09.350
<v Speaker 1>but most of the work is done from the </v>
<v Speaker 1>sensors and the optimization based,</v>

729
00:56:10.141 --> 00:56:15.141
<v Speaker 1>the model based approaches,</v>
<v Speaker 1>trajectory generation and optimizing </v>

730
00:56:15.141 --> 00:56:18.120
<v Speaker 1>which trajectory trajectory is best to </v>
<v Speaker 1>avoid collisions.</v>

731
00:56:19.280 --> 00:56:24.130
<v Speaker 1>Deep Arrows not involved and coming</v>

732
00:56:24.130 --> 00:56:29.130
<v Speaker 2>back and back again.</v>
<v Speaker 2>The unexpected local is a higher award </v>

733
00:56:29.130 --> 00:56:31.990
<v Speaker 2>which arise in all of these situations </v>
<v Speaker 2>and applied in the real world.</v>

734
00:56:32.680 --> 00:56:37.680
<v Speaker 2>So for the cat video,</v>
<v Speaker 2>that's pretty short where the cats are </v>

735
00:56:37.680 --> 00:56:41.620
<v Speaker 2>ringing the bell and they're learning </v>
<v Speaker 2>that the ringing of the bell is,</v>

736
00:56:41.980 --> 00:56:46.980
<v Speaker 2>is mapping to food.</v>
<v Speaker 2>I urge you to think about how that can </v>

737
00:56:47.681 --> 00:56:52.681
<v Speaker 2>evolve over time in unexpected ways that</v>
<v Speaker 2>may not have a desirable effect where </v>

738
00:56:53.591 --> 00:56:58.591
<v Speaker 2>the final reward is in the form of food </v>
<v Speaker 2>and the intended effect is to ring the </v>

739
00:56:59.711 --> 00:57:00.250
<v Speaker 2>bell.</v>

740
00:57:03.390 --> 00:57:08.390
<v Speaker 1>That's where ai safety comes in for the </v>
<v Speaker 1>artificial general intelligence course </v>

741
00:57:08.390 --> 00:57:12.591
<v Speaker 1>in two weeks.</v>
<v Speaker 1>That's something we'll explore </v>

742
00:57:12.591 --> 00:57:14.631
<v Speaker 1>extensively.</v>
<v Speaker 1>It's how these reinforcement learning </v>

743
00:57:16.110 --> 00:57:21.110
<v Speaker 1>planning algorithms will evolve in ways </v>
<v Speaker 1>that not expected and how we can </v>

744
00:57:22.291 --> 00:57:27.291
<v Speaker 1>constrain them,</v>
<v Speaker 1>how we can design reward functions that </v>

745
00:57:27.291 --> 00:57:31.311
<v Speaker 1>result in safe operation.</v>
<v Speaker 1>So I encourage you to come to the talk </v>

746
00:57:32.190 --> 00:57:35.090
<v Speaker 1>on Friday at 1:00</v>
<v Speaker 1>PM as a reminder.</v>

747
00:57:35.100 --> 00:57:36.750
<v Speaker 1>So 1:00</v>
<v Speaker 1>PM that 7:00</v>

748
00:57:36.750 --> 00:57:38.430
<v Speaker 1>PM instead of 32,</v>
<v Speaker 1>one,</v>

749
00:57:38.431 --> 00:57:39.030
<v Speaker 1>two,</v>
<v Speaker 1>three,</v>

750
00:57:39.360 --> 00:57:44.360
<v Speaker 1>and two,</v>
<v Speaker 1>the awesome talks in two weeks from </v>

751
00:57:44.360 --> 00:57:46.140
<v Speaker 1>Boston dynamics to Ray Kurzweil and so </v>
<v Speaker 1>on for Agi.</v>

752
00:57:46.830 --> 00:57:51.030
<v Speaker 1>Now tomorrow we'll talk about computer </v>
<v Speaker 1>vision and psych fuse.</v>

753
00:57:51.300 --> 00:57:51.930
<v Speaker 1>Thank you everybody.</v>

