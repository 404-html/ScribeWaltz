WEBVTT

1
00:00:00.090 --> 00:00:02.760
<v Speaker 1>You've studied the human mind,</v>
<v Speaker 1>cognition,</v>

2
00:00:02.790 --> 00:00:03.870
<v Speaker 1>language,</v>
<v Speaker 1>vision,</v>

3
00:00:03.871 --> 00:00:06.600
<v Speaker 1>evolution,</v>
<v Speaker 1>psychology from child to adult,</v>

4
00:00:07.350 --> 00:00:11.400
<v Speaker 1>from the level of individual to the </v>
<v Speaker 1>level of our entire civilization.</v>

5
00:00:11.760 --> 00:00:15.060
<v Speaker 1>So I feel like I can start with a simple</v>
<v Speaker 1>multiple choice question.</v>

6
00:00:16.320 --> 00:00:20.010
<v Speaker 1>What is the meaning of life?</v>
<v Speaker 1>Is it a,</v>

7
00:00:20.250 --> 00:00:22.890
<v Speaker 1>to attain knowledge is Plato said,</v>
<v Speaker 1>b,</v>

8
00:00:22.891 --> 00:00:24.990
<v Speaker 1>to attain power,</v>
<v Speaker 1>as Nisha said.</v>

9
00:00:25.380 --> 00:00:28.140
<v Speaker 1>See to escape death,</v>
<v Speaker 1>as Ernest Becker said,</v>

10
00:00:28.650 --> 00:00:33.650
<v Speaker 1>d,</v>
<v Speaker 1>to propagate our genes as Darwin and </v>

11
00:00:33.650 --> 00:00:33.720
<v Speaker 1>others have said,</v>
<v Speaker 1>he,</v>

12
00:00:33.810 --> 00:00:36.600
<v Speaker 1>there is no meaning as the nihilists </v>
<v Speaker 1>have said,</v>

13
00:00:37.570 --> 00:00:41.550
<v Speaker 1>f knowing the meaning of life is beyond </v>
<v Speaker 1>our cognitive capabilities.</v>

14
00:00:41.551 --> 00:00:43.050
<v Speaker 1>Uh,</v>
<v Speaker 1>Steven pinker said,</v>

15
00:00:43.140 --> 00:00:46.680
<v Speaker 1>based on my interpretation 20 years ago,</v>
<v Speaker 1>and gee,</v>

16
00:00:46.710 --> 00:00:47.580
<v Speaker 1>none of the above,</v>

17
00:00:48.200 --> 00:00:53.200
<v Speaker 2>I'd say eight comes closest,</v>
<v Speaker 2>but I would amend that to attaining not </v>

18
00:00:53.200 --> 00:00:55.600
<v Speaker 2>only knowledge but a fulfillment more </v>
<v Speaker 2>generally.</v>

19
00:00:56.080 --> 00:00:57.880
<v Speaker 2>That is life,</v>
<v Speaker 2>health,</v>

20
00:00:57.910 --> 00:00:59.730
<v Speaker 2>stimulation,</v>
<v Speaker 2>uh,</v>

21
00:01:00.540 --> 00:01:05.540
<v Speaker 2>access to the living,</v>
<v Speaker 2>cultural and social world.</v>

22
00:01:06.210 --> 00:01:09.270
<v Speaker 2>Now this is our meeting of life.</v>
<v Speaker 2>It's not the meaning of life.</v>

23
00:01:09.620 --> 00:01:14.620
<v Speaker 2>If you were to ask our genes,</v>
<v Speaker 2>they're meeting is to propagate copies </v>

24
00:01:15.241 --> 00:01:20.241
<v Speaker 2>of themselves,</v>
<v Speaker 2>but that is distinct from the meaning </v>

25
00:01:20.241 --> 00:01:21.810
<v Speaker 2>that the brain that they lead to sets </v>
<v Speaker 2>for itself.</v>

26
00:01:22.410 --> 00:01:27.410
<v Speaker 2>So to you,</v>
<v Speaker 2>knowledge is a small subset or a large </v>

27
00:01:27.410 --> 00:01:28.860
<v Speaker 2>subset.</v>
<v Speaker 2>It's a large subset,</v>

28
00:01:28.861 --> 00:01:32.490
<v Speaker 2>but it's not the entirety of human </v>
<v Speaker 2>striding because,</v>

29
00:01:32.540 --> 00:01:37.540
<v Speaker 2>uh,</v>
<v Speaker 2>we also want to interact with people we </v>

30
00:01:37.540 --> 00:01:41.270
<v Speaker 2>want to experience beauty,</v>
<v Speaker 2>we want to experience the richness of </v>

31
00:01:41.270 --> 00:01:41.280
<v Speaker 2>the natural world.</v>
<v Speaker 2>Uh,</v>

32
00:01:41.410 --> 00:01:43.530
<v Speaker 2>but uh,</v>
<v Speaker 2>understanding the,</v>

33
00:01:43.531 --> 00:01:48.531
<v Speaker 2>what makes the universe a tech is a,</v>
<v Speaker 2>is way up there for some of us more than</v>

34
00:01:48.961 --> 00:01:49.410
<v Speaker 2>others.</v>

35
00:01:49.630 --> 00:01:51.730
<v Speaker 2>Uh,</v>
<v Speaker 2>certainly for me that's a,</v>

36
00:01:52.150 --> 00:01:57.150
<v Speaker 2>that that's one of the top five.</v>
<v Speaker 2>So is that a fundamental aspect ages </v>

37
00:01:57.811 --> 00:02:02.811
<v Speaker 2>describing your own preference or is </v>
<v Speaker 2>this a fundamental aspect of human </v>

38
00:02:02.811 --> 00:02:05.430
<v Speaker 2>nature is to seek knowledge to a.</v>
<v Speaker 2>In your latest book,</v>

39
00:02:05.431 --> 00:02:06.650
<v Speaker 2>you talk about the,</v>
<v Speaker 2>the,</v>

40
00:02:06.730 --> 00:02:11.730
<v Speaker 2>the power,</v>
<v Speaker 2>the usefulness of rationality and </v>

41
00:02:11.730 --> 00:02:11.730
<v Speaker 2>reason.</v>
<v Speaker 2>So on.</v>

42
00:02:11.730 --> 00:02:15.720
<v Speaker 2>Is that a fundamental nature of human </v>
<v Speaker 2>beings or is it something we should just</v>

43
00:02:15.721 --> 00:02:18.030
<v Speaker 2>strive for it both.</v>
<v Speaker 2>It is,</v>

44
00:02:18.060 --> 00:02:20.550
<v Speaker 2>we're,</v>
<v Speaker 2>we're capable of striving for it because</v>

45
00:02:20.551 --> 00:02:25.551
<v Speaker 2>it is one of the things that make us </v>
<v Speaker 2>what we are homo sapiens wise,</v>

46
00:02:26.130 --> 00:02:31.130
<v Speaker 2>man,</v>
<v Speaker 2>we are unusual among animals in the </v>

47
00:02:31.130 --> 00:02:34.440
<v Speaker 2>degree to which we acquire knowledge and</v>
<v Speaker 2>use it to survive.</v>

48
00:02:34.710 --> 00:02:36.300
<v Speaker 2>We,</v>
<v Speaker 2>we make tools,</v>

49
00:02:36.360 --> 00:02:38.590
<v Speaker 2>we strike agreements,</v>
<v Speaker 2>uh,</v>

50
00:02:38.640 --> 00:02:42.390
<v Speaker 2>via language,</v>
<v Speaker 2>we extract poisons,</v>

51
00:02:42.450 --> 00:02:47.450
<v Speaker 2>we predict the behavior of animals,</v>
<v Speaker 2>we try to get at the workings of plants </v>

52
00:02:47.911 --> 00:02:50.640
<v Speaker 2>and when I say we,</v>
<v Speaker 2>I don't just mean we in the modern west,</v>

53
00:02:50.940 --> 00:02:55.940
<v Speaker 2>but we as a species everywhere,</v>
<v Speaker 2>which is how we've managed to occupy </v>

54
00:02:55.940 --> 00:02:59.040
<v Speaker 2>every niche on the planet and how we've </v>
<v Speaker 2>managed to drive other animals to,</v>

55
00:03:00.010 --> 00:03:04.170
<v Speaker 2>and the refinement of reason in pursuit </v>
<v Speaker 2>of human wellbeing,</v>

56
00:03:04.410 --> 00:03:07.320
<v Speaker 2>of a health,</v>
<v Speaker 2>happiness,</v>

57
00:03:07.380 --> 00:03:10.980
<v Speaker 2>social richness,</v>
<v Speaker 2>cultural richness is our,</v>

58
00:03:11.010 --> 00:03:16.010
<v Speaker 2>uh,</v>
<v Speaker 2>our main challenge in the present that </v>

59
00:03:16.010 --> 00:03:18.561
<v Speaker 2>is using our intellect,</v>
<v Speaker 2>using our knowledge to figure out how </v>

60
00:03:18.561 --> 00:03:21.270
<v Speaker 2>the world works,</v>
<v Speaker 2>how we work in order to make discoveries</v>

61
00:03:21.271 --> 00:03:24.840
<v Speaker 2>and strike agreements that make us all </v>
<v Speaker 2>better off in the long run.</v>

62
00:03:25.280 --> 00:03:29.060
<v Speaker 1>Right?</v>
<v Speaker 1>And you do that almost undeniably,</v>

63
00:03:29.061 --> 00:03:31.850
<v Speaker 1>and I'm in a data driven way and your </v>
<v Speaker 1>recent book,</v>

64
00:03:32.060 --> 00:03:37.060
<v Speaker 1>but I'd like to focus on the artificial </v>
<v Speaker 1>intelligence aspect of things and not </v>

65
00:03:37.060 --> 00:03:38.660
<v Speaker 1>just artificial intelligence,</v>
<v Speaker 1>but natural intelligence too.</v>

66
00:03:38.900 --> 00:03:43.900
<v Speaker 1>So,</v>
<v Speaker 1>20 years ago in the book you've written </v>

67
00:03:43.900 --> 00:03:44.900
<v Speaker 1>on how the mind works,</v>
<v Speaker 1>you conjecture,</v>

68
00:03:44.901 --> 00:03:48.150
<v Speaker 1>again my right to interpret things.</v>
<v Speaker 1>You can,</v>

69
00:03:48.450 --> 00:03:50.150
<v Speaker 1>uh,</v>
<v Speaker 1>you can correct me if I'm wrong,</v>

70
00:03:50.151 --> 00:03:55.151
<v Speaker 1>but you conjecture that human thought in</v>
<v Speaker 1>the brain may be a result of a now a </v>

71
00:03:55.151 --> 00:03:57.050
<v Speaker 1>massive network of highly interconnected</v>
<v Speaker 1>neurons.</v>

72
00:03:57.080 --> 00:03:58.600
<v Speaker 1>So from this,</v>
<v Speaker 1>uh,</v>

73
00:03:58.940 --> 00:04:03.260
<v Speaker 1>interconnectivity and mergers thought </v>
<v Speaker 1>compared to artificial neural networks,</v>

74
00:04:03.261 --> 00:04:05.840
<v Speaker 1>we should,</v>
<v Speaker 1>we use for machine learning today.</v>

75
00:04:06.200 --> 00:04:09.410
<v Speaker 1>Is there something fundamentally more </v>
<v Speaker 1>complex,</v>

76
00:04:09.440 --> 00:04:13.040
<v Speaker 1>mysterious,</v>
<v Speaker 1>even magical about the biological neural</v>

77
00:04:13.041 --> 00:04:17.150
<v Speaker 1>networks versus the ones we've been </v>
<v Speaker 1>starting to use,</v>

78
00:04:17.500 --> 00:04:22.500
<v Speaker 1>uh,</v>
<v Speaker 1>over the past 60 years and it become to </v>

79
00:04:22.500 --> 00:04:22.500
<v Speaker 1>success in the past 10.</v>

80
00:04:22.500 --> 00:04:27.070
<v Speaker 2>There is something a little bit </v>
<v Speaker 2>mysterious about the human neural </v>

81
00:04:27.070 --> 00:04:31.741
<v Speaker 2>networks,</v>
<v Speaker 2>which is that each one of us who is a </v>

82
00:04:31.741 --> 00:04:33.040
<v Speaker 2>neural network knows that we ourselves </v>
<v Speaker 2>are conscious,</v>

83
00:04:33.470 --> 00:04:38.470
<v Speaker 2>a conscious,</v>
<v Speaker 2>not have a sense of registering our </v>

84
00:04:38.470 --> 00:04:38.470
<v Speaker 2>surroundings or even registering our </v>
<v Speaker 2>internal state.</v>

85
00:04:38.740 --> 00:04:42.850
<v Speaker 2>But in having subjective,</v>
<v Speaker 2>first person present tense experience.</v>

86
00:04:42.940 --> 00:04:46.330
<v Speaker 2>That is when I see red,</v>
<v Speaker 2>it's not just different for green,</v>

87
00:04:47.230 --> 00:04:48.080
<v Speaker 2>but it just,</v>
<v Speaker 2>there's,</v>

88
00:04:48.100 --> 00:04:53.100
<v Speaker 2>there's a redness to it.</v>
<v Speaker 2>But I feel whether an artificial system </v>

89
00:04:53.100 --> 00:04:54.070
<v Speaker 2>would experience that or not,</v>
<v Speaker 2>I don't know.</v>

90
00:04:54.071 --> 00:04:56.770
<v Speaker 2>And I don't think I can know that.</v>
<v Speaker 2>That's why it's mysterious.</v>

91
00:04:56.771 --> 00:05:01.771
<v Speaker 2>If we had a perfectly lifelike robot </v>
<v Speaker 2>that was behaviorally indistinguishable </v>

92
00:05:01.771 --> 00:05:06.720
<v Speaker 2>from a human,</v>
<v Speaker 2>would we attribute consciousness to it </v>

93
00:05:06.720 --> 00:05:08.800
<v Speaker 2>or a or ought we to attribute </v>
<v Speaker 2>consciousness to it and that's something</v>

94
00:05:08.801 --> 00:05:12.310
<v Speaker 2>that it's a very hard to know.</v>
<v Speaker 2>But putting that aside,</v>

95
00:05:12.311 --> 00:05:15.040
<v Speaker 2>putting aside that,</v>
<v Speaker 2>that largely philosophical question,</v>

96
00:05:15.970 --> 00:05:20.970
<v Speaker 2>the question is,</v>
<v Speaker 2>is there some difference between the </v>

97
00:05:20.970 --> 00:05:21.990
<v Speaker 2>hero human neural network and the ones </v>
<v Speaker 2>that we were building?</v>

98
00:05:22.020 --> 00:05:26.440
<v Speaker 2>An artificial intelligence will mean </v>
<v Speaker 2>that we're on the current trajectory not</v>

99
00:05:26.441 --> 00:05:31.441
<v Speaker 2>going to reach the point where we've got</v>
<v Speaker 2>a lifelike robot indistinguishable from </v>

100
00:05:31.441 --> 00:05:36.391
<v Speaker 2>a human because the way their neural,</v>
<v Speaker 2>so-called neural networks are organized </v>

101
00:05:36.391 --> 00:05:37.270
<v Speaker 2>are different from the way ours are </v>
<v Speaker 2>organized.</v>

102
00:05:37.960 --> 00:05:40.100
<v Speaker 2>I think there's overlap,</v>
<v Speaker 2>but I think there are some,</v>

103
00:05:40.360 --> 00:05:45.360
<v Speaker 2>some big differences that a current </v>
<v Speaker 2>neural networks current so called deep </v>

104
00:05:46.781 --> 00:05:49.270
<v Speaker 2>learning systems are,</v>
<v Speaker 2>are in reality,</v>

105
00:05:49.271 --> 00:05:54.271
<v Speaker 2>not all that deep that is,</v>
<v Speaker 2>they are very good at extracting high </v>

106
00:05:54.271 --> 00:05:57.901
<v Speaker 2>order statistical regularities,</v>
<v Speaker 2>but most of the systems don't have a </v>

107
00:05:57.901 --> 00:05:59.000
<v Speaker 2>semantic level,</v>
<v Speaker 2>a level of,</v>

108
00:05:59.320 --> 00:06:04.320
<v Speaker 2>uh,</v>
<v Speaker 2>actual understanding of who did what to </v>

109
00:06:04.320 --> 00:06:04.320
<v Speaker 2>whom.</v>
<v Speaker 2>Uh,</v>

110
00:06:04.320 --> 00:06:04.460
<v Speaker 2>why,</v>
<v Speaker 2>where,</v>

111
00:06:04.640 --> 00:06:06.080
<v Speaker 2>how things work,</v>
<v Speaker 2>what causes,</v>

112
00:06:06.081 --> 00:06:06.590
<v Speaker 2>what else</v>

113
00:06:07.010 --> 00:06:09.190
<v Speaker 1>do you think?</v>
<v Speaker 1>That kind of thing can emerge as it does</v>

114
00:06:09.191 --> 00:06:11.530
<v Speaker 1>so artificial,</v>
<v Speaker 1>you're so much smaller,</v>

115
00:06:11.531 --> 00:06:16.180
<v Speaker 1>the number of connections and so on and </v>
<v Speaker 1>the current human biological networks,</v>

116
00:06:16.181 --> 00:06:21.181
<v Speaker 1>but do you think sort of go to go to </v>
<v Speaker 1>consciousness or to go to this higher </v>

117
00:06:21.181 --> 00:06:25.921
<v Speaker 1>level semantic reasoning about things?</v>
<v Speaker 1>Do you think that kind of merge with </v>

118
00:06:25.921 --> 00:06:29.371
<v Speaker 1>just a larger network with more richly </v>
<v Speaker 1>weirdly interconnected network?</v>

119
00:06:29.790 --> 00:06:31.440
<v Speaker 2>Separate again,</v>
<v Speaker 2>consciousness,</v>

120
00:06:31.441 --> 00:06:33.400
<v Speaker 2>because consciousness is even a matter </v>
<v Speaker 2>of complexity.</v>

121
00:06:33.430 --> 00:06:34.590
<v Speaker 2>Really Weird one.</v>
<v Speaker 2>Yeah.</v>

122
00:06:34.591 --> 00:06:35.180
<v Speaker 2>You could have,</v>
<v Speaker 2>you could,</v>

123
00:06:35.210 --> 00:06:38.580
<v Speaker 2>you could sensibly asked the question of</v>
<v Speaker 2>whether shrimp conscious for example,</v>

124
00:06:38.581 --> 00:06:41.790
<v Speaker 2>they're not terribly complex,</v>
<v Speaker 2>but maybe they feel pain.</v>

125
00:06:41.850 --> 00:06:43.130
<v Speaker 2>So let's,</v>
<v Speaker 2>let's just put that one,</v>

126
00:06:43.170 --> 00:06:44.820
<v Speaker 2>that part of it aside yet.</v>
<v Speaker 2>But,</v>

127
00:06:45.320 --> 00:06:50.320
<v Speaker 2>uh,</v>
<v Speaker 2>I think sheer size of a neural network </v>

128
00:06:50.320 --> 00:06:52.410
<v Speaker 2>is not enough to give it a structure and</v>
<v Speaker 2>knowledge,</v>

129
00:06:52.620 --> 00:06:56.130
<v Speaker 2>but if it's suitably engineered then uh,</v>
<v Speaker 2>then why not?</v>

130
00:06:56.370 --> 00:07:00.200
<v Speaker 2>That is we're neural networks.</v>
<v Speaker 2>Natural selection did a,</v>

131
00:07:00.230 --> 00:07:05.230
<v Speaker 2>a,</v>
<v Speaker 2>a kind of equivalent of engineering of </v>

132
00:07:05.230 --> 00:07:07.461
<v Speaker 2>our brains.</v>
<v Speaker 2>So I don't think there's anything </v>

133
00:07:07.461 --> 00:07:07.461
<v Speaker 2>mysterious in the sense that no,</v>
<v Speaker 2>uh,</v>

134
00:07:07.860 --> 00:07:11.670
<v Speaker 2>no system made it of silicon could ever </v>
<v Speaker 2>do what a human brain can do.</v>

135
00:07:11.671 --> 00:07:16.671
<v Speaker 2>I think it's possible in principle,</v>
<v Speaker 2>whether it'll ever happen depends not </v>

136
00:07:16.671 --> 00:07:19.020
<v Speaker 2>only on how clever we are in engineering</v>
<v Speaker 2>these systems,</v>

137
00:07:19.230 --> 00:07:22.590
<v Speaker 2>but whether even we even want to,</v>
<v Speaker 2>whether that's even a sensible goal that</v>

138
00:07:22.591 --> 00:07:23.880
<v Speaker 2>is,</v>
<v Speaker 2>you can ask the question,</v>

139
00:07:23.881 --> 00:07:25.380
<v Speaker 2>is there any,</v>
<v Speaker 2>um,</v>

140
00:07:25.730 --> 00:07:28.420
<v Speaker 2>locomotion system that is,</v>
<v Speaker 2>uh,</v>

141
00:07:28.430 --> 00:07:29.720
<v Speaker 2>as,</v>
<v Speaker 2>as good as a human?</v>

142
00:07:30.050 --> 00:07:35.050
<v Speaker 2>Well,</v>
<v Speaker 2>we kind of want to do better than a </v>

143
00:07:35.050 --> 00:07:35.050
<v Speaker 2>human ultimately in terms of legged </v>
<v Speaker 2>locomotion.</v>

144
00:07:35.050 --> 00:07:39.260
<v Speaker 2>Uh,</v>
<v Speaker 2>there's no reason that humans should be </v>

145
00:07:39.260 --> 00:07:42.401
<v Speaker 2>our benchmark there.</v>
<v Speaker 2>They're tools that might be better in </v>

146
00:07:42.401 --> 00:07:43.080
<v Speaker 2>some ways.</v>
<v Speaker 2>It may just be not as a,</v>

147
00:07:44.170 --> 00:07:48.260
<v Speaker 2>maybe that we can't duplicate a natural </v>
<v Speaker 2>system because,</v>

148
00:07:48.300 --> 00:07:53.300
<v Speaker 2>uh,</v>
<v Speaker 2>at some point it's so much cheaper to </v>

149
00:07:53.300 --> 00:07:55.301
<v Speaker 2>use a natural system that we're not </v>
<v Speaker 2>going to invest more brainpower and </v>

150
00:07:55.301 --> 00:07:55.430
<v Speaker 2>resources.</v>
<v Speaker 2>So for example,</v>

151
00:07:55.431 --> 00:08:00.431
<v Speaker 2>we don't really have a subsequent exact </v>
<v Speaker 2>substitute for would still build houses </v>

152
00:08:00.501 --> 00:08:02.360
<v Speaker 2>out of wood.</v>
<v Speaker 2>We still build furniture out of wood.</v>

153
00:08:02.720 --> 00:08:04.280
<v Speaker 2>We liked the look.</v>
<v Speaker 2>We like the feel.</v>

154
00:08:04.281 --> 00:08:07.020
<v Speaker 2>It's what has certain properties that </v>
<v Speaker 2>synthetic stones.</v>

155
00:08:07.590 --> 00:08:11.240
<v Speaker 2>It's not that there's any magical and </v>
<v Speaker 2>mysterious about would a,</v>

156
00:08:11.270 --> 00:08:16.270
<v Speaker 2>it's just that the extra steps of </v>
<v Speaker 2>duplicating everything about wood is </v>

157
00:08:16.581 --> 00:08:21.581
<v Speaker 2>something we just haven't bothered </v>
<v Speaker 2>because we have what likewise say </v>

158
00:08:21.581 --> 00:08:21.581
<v Speaker 2>cotton.</v>

159
00:08:21.581 --> 00:08:21.581
<v Speaker 2>I mean,</v>
<v Speaker 2>I'm wearing cotton clothing now.</v>

160
00:08:21.581 --> 00:08:23.630
<v Speaker 2>It feels much better than than </v>
<v Speaker 2>polyester.</v>

161
00:08:24.720 --> 00:08:28.430
<v Speaker 2>It's not that cotton has something magic</v>
<v Speaker 2>in it and it's not that.</v>

162
00:08:28.580 --> 00:08:33.580
<v Speaker 2>If there was that we couldn't ever </v>
<v Speaker 2>synthesize something exactly like </v>

163
00:08:33.580 --> 00:08:34.730
<v Speaker 2>cotton,</v>
<v Speaker 2>but at some point it's just a.</v>

164
00:08:34.731 --> 00:08:39.731
<v Speaker 2>it's just not worth it.</v>
<v Speaker 2>We've got cotton and likewise in the </v>

165
00:08:39.731 --> 00:08:42.311
<v Speaker 2>case of human intelligence,</v>
<v Speaker 2>the goal of making an artificial system </v>

166
00:08:42.311 --> 00:08:46.750
<v Speaker 2>that is exactly like the human brain is </v>
<v Speaker 2>a a goal that we probably known as going</v>

167
00:08:47.021 --> 00:08:51.530
<v Speaker 2>to pursue to the bitter end,</v>
<v Speaker 2>I suspect because if you want tools that</v>

168
00:08:51.531 --> 00:08:53.930
<v Speaker 2>do things better than humans,</v>
<v Speaker 2>you're not going to care whether it does</v>

169
00:08:53.931 --> 00:08:58.931
<v Speaker 2>something like humans.</v>
<v Speaker 2>So for going to diagnosing cancer or </v>

170
00:08:58.931 --> 00:09:00.600
<v Speaker 2>predicting the weather.</v>
<v Speaker 2>Why set humans as your benchmark</v>

171
00:09:01.600 --> 00:09:06.600
<v Speaker 1>in in general?</v>
<v Speaker 1>I suspect you also believe that even if </v>

172
00:09:07.080 --> 00:09:10.530
<v Speaker 1>the human should not be a benchmark and </v>
<v Speaker 1>we don't want to imitate humans in their</v>

173
00:09:10.531 --> 00:09:13.710
<v Speaker 1>system.</v>
<v Speaker 1>There's a lot to be learned about how to</v>

174
00:09:13.711 --> 00:09:16.520
<v Speaker 1>create an artificial intelligence system</v>
<v Speaker 1>by studying the human.</v>

175
00:09:16.920 --> 00:09:17.420
<v Speaker 2>Yeah,</v>
<v Speaker 2>I,</v>

176
00:09:17.421 --> 00:09:22.421
<v Speaker 2>I think that's right.</v>
<v Speaker 2>In the same way that to build flying </v>

177
00:09:22.421 --> 00:09:27.081
<v Speaker 2>machines,</v>
<v Speaker 2>we want under the laws of aerodynamics </v>

178
00:09:27.081 --> 00:09:27.630
<v Speaker 2>and including birds but not mimic the </v>
<v Speaker 2>birds,</v>

179
00:09:27.880 --> 00:09:29.070
<v Speaker 2>but at the same loss,</v>

180
00:09:30.240 --> 00:09:33.690
<v Speaker 1>uh,</v>
<v Speaker 1>you have a view on ai,</v>

181
00:09:33.691 --> 00:09:37.230
<v Speaker 1>artificial intelligence and safety that,</v>
<v Speaker 1>uh,</v>

182
00:09:37.340 --> 00:09:42.340
<v Speaker 1>from my perspective is refreshingly </v>
<v Speaker 1>rational or perhaps more importantly has</v>

183
00:09:47.011 --> 00:09:52.011
<v Speaker 1>elements of positivity to it which I </v>
<v Speaker 1>think can be inspiring and empowering as</v>

184
00:09:52.231 --> 00:09:57.030
<v Speaker 1>opposed to paralyzing for many people,</v>
<v Speaker 1>including AI researchers.</v>

185
00:09:57.031 --> 00:10:00.660
<v Speaker 1>The eventual existential threat of ai is</v>
<v Speaker 1>obvious,</v>

186
00:10:01.020 --> 00:10:04.350
<v Speaker 1>not only possible but obvious.</v>
<v Speaker 1>And for many others,</v>

187
00:10:04.410 --> 00:10:07.890
<v Speaker 1>including AI researchers,</v>
<v Speaker 1>the threat is not obvious.</v>

188
00:10:08.400 --> 00:10:13.400
<v Speaker 1>So Elon Musk is famously in the highly </v>
<v Speaker 1>concerned about ai camp,</v>

189
00:10:14.790 --> 00:10:19.790
<v Speaker 1>saying things like ai is far more </v>
<v Speaker 1>dangerous than nuclear weapons and that </v>

190
00:10:19.790 --> 00:10:22.350
<v Speaker 1>ai will likely destroy a human </v>
<v Speaker 1>civilization.</v>

191
00:10:23.040 --> 00:10:27.960
<v Speaker 1>So in February you said that if Elon was</v>
<v Speaker 1>really serious about Ai,</v>

192
00:10:28.360 --> 00:10:30.090
<v Speaker 1>the,</v>
<v Speaker 1>the threat of Ai,</v>

193
00:10:30.330 --> 00:10:35.330
<v Speaker 1>he would stop building self driving cars</v>
<v Speaker 1>that he's doing very successful as part </v>

194
00:10:35.330 --> 00:10:36.870
<v Speaker 1>of Tesla.</v>
<v Speaker 1>Then he said,</v>

195
00:10:37.080 --> 00:10:42.080
<v Speaker 1>wow,</v>
<v Speaker 1>if even pinker doesn't understand the </v>

196
00:10:42.080 --> 00:10:42.630
<v Speaker 1>difference between narrow ai,</v>
<v Speaker 1>like a car and general ai,</v>

197
00:10:43.470 --> 00:10:48.470
<v Speaker 1>when the ladder literally has a million </v>
<v Speaker 1>times more compute power and an open </v>

198
00:10:48.470 --> 00:10:51.990
<v Speaker 1>ended utility function,</v>
<v Speaker 1>humanity is in deep trouble.</v>

199
00:10:52.230 --> 00:10:55.500
<v Speaker 1>So first,</v>
<v Speaker 1>what did you mean by the statement about</v>

200
00:10:55.770 --> 00:10:59.300
<v Speaker 1>Ilan mosque should stop building self </v>
<v Speaker 1>driving cars if he's deeply concerned,</v>

201
00:11:00.180 --> 00:11:02.680
<v Speaker 2>not the last time that Elon musk is </v>
<v Speaker 2>fired off in,</v>

202
00:11:02.681 --> 00:11:03.650
<v Speaker 2>in temporary tweet.</v>

203
00:11:05.080 --> 00:11:07.600
<v Speaker 1>We live in a world where a twitter has </v>
<v Speaker 1>power.</v>

204
00:11:07.720 --> 00:11:08.400
<v Speaker 2>Yes.</v>
<v Speaker 2>Uh,</v>

205
00:11:08.860 --> 00:11:09.690
<v Speaker 2>yeah.</v>
<v Speaker 2>I think the,</v>

206
00:11:09.700 --> 00:11:10.540
<v Speaker 2>the,</v>
<v Speaker 2>um,</v>

207
00:11:10.910 --> 00:11:15.910
<v Speaker 2>the,</v>
<v Speaker 2>there are to a kinds of existential </v>

208
00:11:15.910 --> 00:11:20.511
<v Speaker 2>threat that had been discussed in </v>
<v Speaker 2>connection with artificial intelligence </v>

209
00:11:20.511 --> 00:11:20.511
<v Speaker 2>and I think that they're both </v>
<v Speaker 2>incoherent.</v>

210
00:11:20.511 --> 00:11:25.310
<v Speaker 2>Uh,</v>
<v Speaker 2>one of them is a vague fear of ai </v>

211
00:11:25.310 --> 00:11:25.310
<v Speaker 2>takeover that,</v>
<v Speaker 2>uh,</v>

212
00:11:25.310 --> 00:11:30.200
<v Speaker 2>it just as we subjugated animals and </v>
<v Speaker 2>less technologically advanced peoples.</v>

213
00:11:30.900 --> 00:11:33.450
<v Speaker 2>So if we build something that's more </v>
<v Speaker 2>advanced than us,</v>

214
00:11:33.480 --> 00:11:36.300
<v Speaker 2>it will inevitably turn us into paths or</v>
<v Speaker 2>slaves or,</v>

215
00:11:36.630 --> 00:11:41.630
<v Speaker 2>or a domesticated animal equivalence.</v>
<v Speaker 2>I think this confuses intelligence with </v>

216
00:11:42.810 --> 00:11:47.810
<v Speaker 2>a will to power that.</v>
<v Speaker 2>It so happens that in the intelligence </v>

217
00:11:47.810 --> 00:11:50.490
<v Speaker 2>system we are most familiar with namely </v>
<v Speaker 2>Homo Sapiens,</v>

218
00:11:50.940 --> 00:11:55.940
<v Speaker 2>we are products of natural selection,</v>
<v Speaker 2>which is a competitive process and so </v>

219
00:11:55.940 --> 00:11:59.671
<v Speaker 2>bundled together with our problem </v>
<v Speaker 2>solving capacity are a number of nasty </v>

220
00:11:59.671 --> 00:12:04.051
<v Speaker 2>traits,</v>
<v Speaker 2>a dominance and a exploitation and </v>

221
00:12:04.051 --> 00:12:08.590
<v Speaker 2>maximize nation of of power and glory </v>
<v Speaker 2>and resources and influence.</v>

222
00:12:08.920 --> 00:12:12.220
<v Speaker 2>There's no reason to think that she or </v>
<v Speaker 2>problem solving capability.</v>

223
00:12:12.221 --> 00:12:17.221
<v Speaker 2>We'll set that as one of its goals.</v>
<v Speaker 2>Its goals will be whatever we said its </v>

224
00:12:17.221 --> 00:12:21.330
<v Speaker 2>goals as and as long as someone isn't </v>
<v Speaker 2>building a megalomanic maniacal </v>

225
00:12:21.330 --> 00:12:23.980
<v Speaker 2>artificial intelligence,</v>
<v Speaker 2>there's no reason to think that it would</v>

226
00:12:23.981 --> 00:12:26.020
<v Speaker 2>naturally evolve in that direction.</v>
<v Speaker 2>Now you might say,</v>

227
00:12:26.021 --> 00:12:31.021
<v Speaker 2>well,</v>
<v Speaker 2>what if we gave it the goal of </v>

228
00:12:31.021 --> 00:12:33.180
<v Speaker 2>maximizing its own power source?</v>
<v Speaker 2>That's a pretty stupid goal to give a a,</v>

229
00:12:33.401 --> 00:12:35.500
<v Speaker 2>an autonomous system.</v>
<v Speaker 2>You don't give it that goal.</v>

230
00:12:36.170 --> 00:12:41.170
<v Speaker 2>I mean that just self-evidently idiotic.</v>
<v Speaker 2>So if you look at the history of the </v>

231
00:12:41.170 --> 00:12:45.841
<v Speaker 2>world,</v>
<v Speaker 2>there's been a lot of opportunities </v>

232
00:12:45.841 --> 00:12:46.660
<v Speaker 2>where engineers could instill in a </v>
<v Speaker 2>system destructive power and they choose</v>

233
00:12:46.661 --> 00:12:49.330
<v Speaker 2>not to because that's the natural </v>
<v Speaker 2>process of engineering.</v>

234
00:12:49.600 --> 00:12:50.770
<v Speaker 2>Well,</v>
<v Speaker 2>except for weapons.</v>

235
00:12:50.771 --> 00:12:51.820
<v Speaker 2>I mean,</v>
<v Speaker 2>if you're building a weapon,</v>

236
00:12:51.850 --> 00:12:53.320
<v Speaker 2>it's goal is to destroy people.</v>

237
00:12:53.380 --> 00:12:55.900
<v Speaker 2>Uh,</v>
<v Speaker 2>and so I think there are good reasons to</v>

238
00:12:55.920 --> 00:12:59.590
<v Speaker 2>not not build certain kinds of weapons.</v>
<v Speaker 2>I think new building nuclear weapons was</v>

239
00:12:59.591 --> 00:13:01.860
<v Speaker 2>a massive mistake,</v>
<v Speaker 2>a big problem.</v>

240
00:13:01.870 --> 00:13:04.450
<v Speaker 2>Do you think.</v>
<v Speaker 2>So,</v>

241
00:13:04.530 --> 00:13:09.530
<v Speaker 2>uh,</v>
<v Speaker 2>maybe pause on that because that is one </v>

242
00:13:09.530 --> 00:13:09.530
<v Speaker 2>of the serious threats.</v>
<v Speaker 2>Do you think that,</v>

243
00:13:09.670 --> 00:13:14.670
<v Speaker 2>uh,</v>
<v Speaker 2>it was a mistake in a sense that it </v>

244
00:13:14.670 --> 00:13:16.330
<v Speaker 2>should have been stopped early on,</v>
<v Speaker 2>or do you think it's just an unfortunate</v>

245
00:13:16.331 --> 00:13:19.870
<v Speaker 2>event of invention that this was </v>
<v Speaker 2>invented?</v>

246
00:13:20.310 --> 00:13:22.300
<v Speaker 2>It was possible to stop,</v>
<v Speaker 2>I guess is the question.</v>

247
00:13:22.480 --> 00:13:25.900
<v Speaker 2>It's hard to rewind the clock because of</v>
<v Speaker 2>course it was invented in the context of</v>

248
00:13:25.901 --> 00:13:30.280
<v Speaker 2>World War II and the fear that the Nazis</v>
<v Speaker 2>might develop one first.</v>

249
00:13:31.030 --> 00:13:33.220
<v Speaker 2>Then once it was initiated for that </v>
<v Speaker 2>reason,</v>

250
00:13:33.221 --> 00:13:35.380
<v Speaker 2>it was,</v>
<v Speaker 2>it was hard to turn off,</v>

251
00:13:36.040 --> 00:13:41.040
<v Speaker 2>especially since winning the war against</v>
<v Speaker 2>the Japanese and the Nazis was such an </v>

252
00:13:41.380 --> 00:13:46.380
<v Speaker 2>overwhelming goal of every responsible </v>
<v Speaker 2>person that they were just nothing that </v>

253
00:13:46.380 --> 00:13:48.250
<v Speaker 2>people wouldn't have done then to ensure</v>
<v Speaker 2>victory.</v>

254
00:13:49.270 --> 00:13:51.490
<v Speaker 2>It's quite possible if world war two </v>
<v Speaker 2>hadn't happened,</v>

255
00:13:51.550 --> 00:13:53.530
<v Speaker 2>that nuclear weapons wouldn't have been </v>
<v Speaker 2>invented,</v>

256
00:13:53.630 --> 00:13:55.250
<v Speaker 2>we can't know,</v>
<v Speaker 2>um,</v>

257
00:13:55.360 --> 00:14:00.360
<v Speaker 2>but I don't think it was a by any means </v>
<v Speaker 2>a necessity anymore than some of the </v>

258
00:14:00.360 --> 00:14:03.661
<v Speaker 2>other weapon systems that were </v>
<v Speaker 2>envisioned but never implemented like </v>

259
00:14:03.661 --> 00:14:08.340
<v Speaker 2>planes that would disperse poison gas </v>
<v Speaker 2>over cities like crop dusters or systems</v>

260
00:14:09.071 --> 00:14:10.160
<v Speaker 2>to try to,</v>
<v Speaker 2>to,</v>

261
00:14:10.180 --> 00:14:10.620
<v Speaker 2>to,</v>
<v Speaker 2>uh,</v>

262
00:14:10.660 --> 00:14:11.550
<v Speaker 2>create,</v>
<v Speaker 2>uh,</v>

263
00:14:11.590 --> 00:14:16.060
<v Speaker 2>earthquakes and tsunamis and enemy </v>
<v Speaker 2>countries to weaponize the weather,</v>

264
00:14:16.180 --> 00:14:19.390
<v Speaker 2>weaponize solar flares,</v>
<v Speaker 2>all kinds of crazy schemes that,</v>

265
00:14:19.391 --> 00:14:19.810
<v Speaker 2>uh,</v>
<v Speaker 2>that,</v>

266
00:14:19.870 --> 00:14:24.870
<v Speaker 2>that we thought the better of analogies </v>
<v Speaker 2>between nuclear weapons and artificial </v>

267
00:14:24.870 --> 00:14:29.191
<v Speaker 2>intelligence are fundamentally misguided</v>
<v Speaker 2>because the whole point of nuclear </v>

268
00:14:29.191 --> 00:14:33.151
<v Speaker 2>weapons is to destroy things.</v>
<v Speaker 2>The point of artificial intelligence is </v>

269
00:14:33.151 --> 00:14:35.320
<v Speaker 2>not to destroy things.</v>
<v Speaker 2>So the analogy is,</v>

270
00:14:35.470 --> 00:14:37.810
<v Speaker 2>is misleading.</v>
<v Speaker 2>So there's two artificial intelligence.</v>

271
00:14:37.811 --> 00:14:42.110
<v Speaker 2>You mentioned the first one.</v>
<v Speaker 2>It was the intelligence or hungry.</v>

272
00:14:42.111 --> 00:14:47.111
<v Speaker 2>Yeah.</v>
<v Speaker 2>The system that we designed ourselves </v>

273
00:14:47.111 --> 00:14:48.961
<v Speaker 2>where we give it the goals goals are,</v>
<v Speaker 2>are external to the a means to attain </v>

274
00:14:49.241 --> 00:14:49.810
<v Speaker 2>the goals.</v>

275
00:14:51.430 --> 00:14:56.430
<v Speaker 2>If we don't design an artificially </v>
<v Speaker 2>intelligent system to maximize a </v>

276
00:14:56.621 --> 00:14:59.050
<v Speaker 2>dominance,</v>
<v Speaker 2>than it won't maximize dominance.</v>

277
00:14:59.190 --> 00:15:01.450
<v Speaker 2>A,</v>
<v Speaker 2>it just that we're so familiar with Homo</v>

278
00:15:01.510 --> 00:15:04.720
<v Speaker 2>sapiens were these two traits come </v>
<v Speaker 2>bundled together,</v>

279
00:15:04.870 --> 00:15:09.870
<v Speaker 2>particularly in men that we are apt to </v>
<v Speaker 2>confuse high intelligence with a,</v>

280
00:15:11.100 --> 00:15:11.930
<v Speaker 2>a,</v>
<v Speaker 2>a,</v>

281
00:15:11.940 --> 00:15:14.650
<v Speaker 2>a will to power,</v>
<v Speaker 2>but that's just an error.</v>

282
00:15:15.700 --> 00:15:20.700
<v Speaker 2>The other fear is that we'll be </v>
<v Speaker 2>collateral damage that will give a </v>

283
00:15:20.700 --> 00:15:24.391
<v Speaker 2>artificial intelligence a goal like make</v>
<v Speaker 2>paperclips and it will pursue that goal </v>

284
00:15:24.911 --> 00:15:27.550
<v Speaker 2>so brilliantly that before we can stop </v>
<v Speaker 2>it,</v>

285
00:15:27.551 --> 00:15:32.551
<v Speaker 2>it turns us into paperclips,</v>
<v Speaker 2>will give it the goal of curing cancer </v>

286
00:15:32.551 --> 00:15:36.451
<v Speaker 2>and it will turn us into Guinea pigs for</v>
<v Speaker 2>lethal experiments or give it the goal </v>

287
00:15:36.671 --> 00:15:39.330
<v Speaker 2>of world peace in its conception of </v>
<v Speaker 2>world pieces.</v>

288
00:15:39.340 --> 00:15:40.990
<v Speaker 2>No people,</v>
<v Speaker 2>therefore no fighting.</v>

289
00:15:40.991 --> 00:15:42.610
<v Speaker 2>And so it will kill us all.</v>
<v Speaker 2>Now,</v>

290
00:15:42.611 --> 00:15:44.590
<v Speaker 2>I think these are utterly fanciful.</v>
<v Speaker 2>In fact,</v>

291
00:15:44.591 --> 00:15:46.370
<v Speaker 2>I think they're actually self-defeating.</v>

292
00:15:46.840 --> 00:15:51.840
<v Speaker 2>They first of all,</v>
<v Speaker 2>assume that we're going to be so </v>

293
00:15:51.840 --> 00:15:54.721
<v Speaker 2>brilliant that we can design an </v>
<v Speaker 2>artificial intelligence that can cure </v>

294
00:15:54.721 --> 00:15:57.901
<v Speaker 2>cancer but so stupid that we don't </v>
<v Speaker 2>specify what we mean by curing cancer in</v>

295
00:15:58.411 --> 00:16:00.880
<v Speaker 2>enough detail.</v>
<v Speaker 2>That it won't kill us in the process.</v>

296
00:16:01.380 --> 00:16:06.380
<v Speaker 2>Uh,</v>
<v Speaker 2>and it assumes that the system will be </v>

297
00:16:06.380 --> 00:16:08.350
<v Speaker 2>so smart that it can cure cancer,</v>
<v Speaker 2>but so idiotic that it doesn't,</v>

298
00:16:08.410 --> 00:16:12.070
<v Speaker 2>can't figure out that what we mean by </v>
<v Speaker 2>curing cancer is not killing everyone.</v>

299
00:16:12.820 --> 00:16:15.430
<v Speaker 2>So I think that the,</v>
<v Speaker 2>the collateral damage scenario,</v>

300
00:16:15.431 --> 00:16:17.570
<v Speaker 2>the value alignment problem is,</v>
<v Speaker 2>uh,</v>

301
00:16:17.590 --> 00:16:19.270
<v Speaker 2>is also based on a misconception.</v>

302
00:16:19.550 --> 00:16:24.550
<v Speaker 1>So one of the challenges,</v>
<v Speaker 1>of course we don't know how to build </v>

303
00:16:24.550 --> 00:16:25.490
<v Speaker 1>either system currently or are we even </v>
<v Speaker 1>close to knowing.</v>

304
00:16:25.850 --> 00:16:27.530
<v Speaker 1>Of course those things can change </v>
<v Speaker 1>overnight,</v>

305
00:16:27.531 --> 00:16:31.130
<v Speaker 1>but at this time theorizing about is </v>
<v Speaker 1>very challenging,</v>

306
00:16:31.480 --> 00:16:32.170
<v Speaker 1>uh,</v>
<v Speaker 1>in,</v>

307
00:16:32.180 --> 00:16:37.180
<v Speaker 1>in either direction.</v>
<v Speaker 1>So that's probably at the core of the </v>

308
00:16:37.180 --> 00:16:39.851
<v Speaker 1>problem is without that ability to </v>
<v Speaker 1>reason about the real engineering things</v>

309
00:16:39.981 --> 00:16:43.580
<v Speaker 1>here at hand is a,</v>
<v Speaker 1>your imagination runs away with things.</v>

310
00:16:43.610 --> 00:16:45.350
<v Speaker 1>Exactly.</v>
<v Speaker 1>But let me sort of ask,</v>

311
00:16:45.680 --> 00:16:50.680
<v Speaker 1>what do you think was the motivation and</v>
<v Speaker 1>the thought process of Isla Moscow?</v>

312
00:16:50.991 --> 00:16:53.750
<v Speaker 1>I build autonomy vehicles.</v>
<v Speaker 1>I studied autonomous vehicles.</v>

313
00:16:53.960 --> 00:16:58.960
<v Speaker 1>I studied tesla autopilot.</v>
<v Speaker 1>I think it is one of the greatest </v>

314
00:16:58.960 --> 00:17:03.010
<v Speaker 1>currently application,</v>
<v Speaker 1>large scale applications of artificial </v>

315
00:17:03.010 --> 00:17:06.161
<v Speaker 1>intelligence in the world.</v>
<v Speaker 1>It has a potentially a very positive </v>

316
00:17:06.161 --> 00:17:10.091
<v Speaker 1>impact on society.</v>
<v Speaker 1>So how does a person who is creating </v>

317
00:17:10.091 --> 00:17:13.811
<v Speaker 1>this very good quote unquote narrow ai </v>
<v Speaker 1>system also seem to be so concerned </v>

318
00:17:15.321 --> 00:17:20.321
<v Speaker 1>about this other general ai?</v>
<v Speaker 1>What do you think is the motivation </v>

319
00:17:20.321 --> 00:17:25.061
<v Speaker 1>there?</v>
<v Speaker 1>What do you think is the thing you'll </v>

320
00:17:25.061 --> 00:17:25.061
<v Speaker 1>probably have to ask him,</v>

321
00:17:25.061 --> 00:17:28.160
<v Speaker 2>but they're an and he is notoriously a </v>
<v Speaker 2>flamboyant,</v>

322
00:17:29.110 --> 00:17:32.410
<v Speaker 2>impulsive to the,</v>
<v Speaker 2>as we have just seen to the detriment of</v>

323
00:17:32.411 --> 00:17:33.950
<v Speaker 2>his own goals of,</v>
<v Speaker 2>of the,</v>

324
00:17:33.951 --> 00:17:36.340
<v Speaker 2>the health of the company.</v>
<v Speaker 2>And so I,</v>

325
00:17:36.341 --> 00:17:38.710
<v Speaker 2>I don't know what's going on in his </v>
<v Speaker 2>mind.</v>

326
00:17:38.711 --> 00:17:41.470
<v Speaker 2>You probably have to ask him,</v>
<v Speaker 2>but I don't think the.</v>

327
00:17:41.620 --> 00:17:46.250
<v Speaker 2>And I don't think the distinction </v>
<v Speaker 2>between special purpose ai and so-called</v>

328
00:17:46.270 --> 00:17:51.270
<v Speaker 2>General Iai is relevant.</v>
<v Speaker 2>That in the same way that special </v>

329
00:17:51.270 --> 00:17:55.280
<v Speaker 2>purpose ai is not going to do anything </v>
<v Speaker 2>conceivable in order to attain a goal.</v>

330
00:17:55.310 --> 00:17:57.600
<v Speaker 2>All engineering systems have to,</v>
<v Speaker 2>uh,</v>

331
00:17:57.750 --> 00:18:02.750
<v Speaker 2>are designed to trade off across </v>
<v Speaker 2>multiple goals where we build cars in </v>

332
00:18:02.750 --> 00:18:06.851
<v Speaker 2>the first place.</v>
<v Speaker 2>We didn't forget to install breaks </v>

333
00:18:06.851 --> 00:18:07.680
<v Speaker 2>because the goal of a car is to go fast.</v>
<v Speaker 2>Uh,</v>

334
00:18:07.760 --> 00:18:09.310
<v Speaker 2>it occurred to people,</v>
<v Speaker 2>yes,</v>

335
00:18:09.311 --> 00:18:11.170
<v Speaker 2>you want it to go fast,</v>
<v Speaker 2>but not always.</v>

336
00:18:11.290 --> 00:18:16.290
<v Speaker 2>So you build in breaks to likewise,</v>
<v Speaker 2>if a car is going to be autonomous,</v>

337
00:18:16.960 --> 00:18:20.290
<v Speaker 2>that doesn't and program it to take the </v>
<v Speaker 2>shortest route to the airport,</v>

338
00:18:20.470 --> 00:18:25.470
<v Speaker 2>it's not going to take the diagonal and </v>
<v Speaker 2>mow down people and trees and fences </v>

339
00:18:25.470 --> 00:18:25.470
<v Speaker 2>because that's the shortest route.</v>

340
00:18:25.540 --> 00:18:28.690
<v Speaker 2>That's not what we mean by the shortest </v>
<v Speaker 2>route when we programmed it.</v>

341
00:18:28.691 --> 00:18:32.830
<v Speaker 2>And that's just what an uh,</v>
<v Speaker 2>an intelligent system is.</v>

342
00:18:32.831 --> 00:18:37.831
<v Speaker 2>By definition,</v>
<v Speaker 2>it takes into account multiple </v>

343
00:18:37.831 --> 00:18:37.831
<v Speaker 2>constraints.</v>
<v Speaker 2>The same is true,</v>

344
00:18:37.831 --> 00:18:40.420
<v Speaker 2>in fact even more true of so-called </v>
<v Speaker 2>general intelligence.</v>

345
00:18:40.720 --> 00:18:43.450
<v Speaker 2>That is,</v>
<v Speaker 2>if it's genuinely intelligent,</v>

346
00:18:43.451 --> 00:18:48.451
<v Speaker 2>it's not going to pursue some goal </v>
<v Speaker 2>singlemindedly omitting every other </v>

347
00:18:49.750 --> 00:18:54.750
<v Speaker 2>consideration and collateral effect.</v>
<v Speaker 2>That's not artificial general </v>

348
00:18:54.750 --> 00:18:54.750
<v Speaker 2>intelligence.</v>
<v Speaker 2>That's a,</v>

349
00:18:54.750 --> 00:18:57.120
<v Speaker 2>that's artificial stupidity.</v>
<v Speaker 2>Um,</v>

350
00:18:57.520 --> 00:18:58.600
<v Speaker 2>I agree with you,</v>
<v Speaker 2>by the way,</v>

351
00:18:58.601 --> 00:19:02.170
<v Speaker 2>on the promise of autonomous vehicles </v>
<v Speaker 2>for improving human welfare.</v>

352
00:19:02.200 --> 00:19:07.200
<v Speaker 2>I think it's spectacular and I'm </v>
<v Speaker 2>surprised at how little press coverage </v>

353
00:19:07.200 --> 00:19:09.310
<v Speaker 2>notes that in the United States alone,</v>
<v Speaker 2>something like 40,000</v>

354
00:19:09.311 --> 00:19:14.311
<v Speaker 2>people die every year on the highways,</v>
<v Speaker 2>fastly more than are killed by </v>

355
00:19:14.311 --> 00:19:14.830
<v Speaker 2>terrorists.</v>
<v Speaker 2>And we spend.</v>

356
00:19:15.190 --> 00:19:19.330
<v Speaker 2>You spent a trillion dollars on a war to</v>
<v Speaker 2>combat deaths by terrorism,</v>

357
00:19:19.540 --> 00:19:21.210
<v Speaker 2>but half a dozen a year,</v>
<v Speaker 2>uh,</v>

358
00:19:21.340 --> 00:19:23.530
<v Speaker 2>whereas every year and year out,</v>
<v Speaker 2>40,000</v>

359
00:19:23.531 --> 00:19:28.531
<v Speaker 2>people are,</v>
<v Speaker 2>are massacred on the highways which </v>

360
00:19:28.531 --> 00:19:28.531
<v Speaker 2>could be brought down to very close to </v>
<v Speaker 2>zero.</v>

361
00:19:28.531 --> 00:19:29.020
<v Speaker 2>Uh,</v>
<v Speaker 2>so I'm,</v>

362
00:19:29.021 --> 00:19:34.021
<v Speaker 2>I'm,</v>
<v Speaker 2>I'm with you on the humanitarian </v>

363
00:19:34.021 --> 00:19:34.021
<v Speaker 2>benefit.</v>

364
00:19:34.021 --> 00:19:36.970
<v Speaker 1>Let me just mention that it's,</v>
<v Speaker 1>as a person who is building these cars </v>

365
00:19:36.970 --> 00:19:37.730
<v Speaker 1>that is a little bit offensive to me to </v>
<v Speaker 1>say that engine in years,</v>

366
00:19:37.731 --> 00:19:40.520
<v Speaker 1>we'd be clueless enough not to engineer </v>
<v Speaker 1>safety into systems.</v>

367
00:19:40.790 --> 00:19:44.180
<v Speaker 1>I often stay up at night thinking about </v>
<v Speaker 1>those 40,000</v>

368
00:19:44.181 --> 00:19:49.181
<v Speaker 1>people that are dying and everything I </v>
<v Speaker 1>tried to engineer is to save those </v>

369
00:19:49.181 --> 00:19:53.591
<v Speaker 1>people's lives.</v>
<v Speaker 1>So every new invention that I'm super </v>

370
00:19:53.591 --> 00:19:53.591
<v Speaker 1>excited about every new,</v>
<v Speaker 1>uh,</v>

371
00:19:53.591 --> 00:19:54.910
<v Speaker 1>and uh,</v>
<v Speaker 1>uh,</v>

372
00:19:55.000 --> 00:20:00.000
<v Speaker 1>in,</v>
<v Speaker 1>in all the deep learning literature and </v>

373
00:20:00.000 --> 00:20:01.680
<v Speaker 1>cvpr conferences and nips,</v>
<v Speaker 1>everything I'm super excited about is </v>

374
00:20:01.680 --> 00:20:06.550
<v Speaker 1>all grounded in making it safe and help </v>
<v Speaker 1>people.</v>

375
00:20:06.770 --> 00:20:11.770
<v Speaker 1>So I just don't see how that trajectory </v>
<v Speaker 1>can all of a sudden slip into a </v>

376
00:20:11.770 --> 00:20:13.760
<v Speaker 1>situation where intelligence will be </v>
<v Speaker 1>highly negative.</v>

377
00:20:13.970 --> 00:20:14.230
<v Speaker 1>I just want</v>

378
00:20:14.790 --> 00:20:19.790
<v Speaker 2>you and I certainly agree on that and I </v>
<v Speaker 2>think that's only the beginning of the </v>

379
00:20:19.790 --> 00:20:21.010
<v Speaker 2>potential humanitarian benefits of </v>
<v Speaker 2>artificial intelligence.</v>

380
00:20:21.730 --> 00:20:26.730
<v Speaker 2>There has been enormous attention to </v>
<v Speaker 2>what are we going to do with the people </v>

381
00:20:26.730 --> 00:20:28.450
<v Speaker 2>whose jobs are made obsolete by </v>
<v Speaker 2>artificial intelligence,</v>

382
00:20:28.810 --> 00:20:33.810
<v Speaker 2>but very little attention given to the </v>
<v Speaker 2>fact that the jobs that are gonna be </v>

383
00:20:33.810 --> 00:20:36.721
<v Speaker 2>made obsolete are horrible jobs.</v>
<v Speaker 2>The fact that people aren't going to be </v>

384
00:20:36.721 --> 00:20:40.500
<v Speaker 2>picking crops and making beds and </v>
<v Speaker 2>driving trucks and mining coal.</v>

385
00:20:41.050 --> 00:20:46.050
<v Speaker 2>These are soul deadening jobs and we </v>
<v Speaker 2>have a whole literature sympathizing </v>

386
00:20:46.050 --> 00:20:50.641
<v Speaker 2>with the people stuck in menial mind </v>
<v Speaker 2>deadening a dangerous jobs if we can </v>

387
00:20:52.601 --> 00:20:55.660
<v Speaker 2>eliminate them.</v>
<v Speaker 2>This is a fantastic boon to humanity.</v>

388
00:20:55.720 --> 00:20:56.260
<v Speaker 2>Now,</v>
<v Speaker 2>granted,</v>

389
00:20:56.280 --> 00:20:58.780
<v Speaker 2>we you solve one problem in.</v>
<v Speaker 2>There's another one,</v>

390
00:20:58.781 --> 00:21:02.680
<v Speaker 2>namely how do we get these people a,</v>
<v Speaker 2>a decent income.</v>

391
00:21:02.710 --> 00:21:07.710
<v Speaker 2>But if we're smart enough to invent </v>
<v Speaker 2>machines that can make beds and put away</v>

392
00:21:07.841 --> 00:21:09.450
<v Speaker 2>dishes and uh,</v>
<v Speaker 2>and,</v>

393
00:21:09.451 --> 00:21:14.451
<v Speaker 2>and handle hospital patients where I </v>
<v Speaker 2>think we're smart enough to figure out </v>

394
00:21:14.451 --> 00:21:17.881
<v Speaker 2>how to redistribute income to apportion </v>
<v Speaker 2>some of the vast economic savings to the</v>

395
00:21:18.821 --> 00:21:20.850
<v Speaker 2>the human beings who will no longer be </v>
<v Speaker 2>needed to,</v>

396
00:21:20.920 --> 00:21:21.430
<v Speaker 2>to make better.</v>

397
00:21:22.690 --> 00:21:27.690
<v Speaker 1>Okay.</v>
<v Speaker 1>Sam Harris says that it's obvious that </v>

398
00:21:27.690 --> 00:21:28.600
<v Speaker 1>eventually ai will be an existential </v>
<v Speaker 1>risk,</v>

399
00:21:29.500 --> 00:21:34.500
<v Speaker 1>is one of the people says it's obvious.</v>
<v Speaker 1>We don't know when the claim goes,</v>

400
00:21:35.411 --> 00:21:39.670
<v Speaker 1>but eventually it's obvious and because </v>
<v Speaker 1>we don't know when we should worry about</v>

401
00:21:39.671 --> 00:21:44.671
<v Speaker 1>it now,</v>
<v Speaker 1>it's a very interesting argument in my </v>

402
00:21:44.671 --> 00:21:44.671
<v Speaker 1>eyes.</v>
<v Speaker 1>Um,</v>

403
00:21:44.671 --> 00:21:47.820
<v Speaker 1>so how do,</v>
<v Speaker 1>how do we think about timescale?</v>

404
00:21:47.890 --> 00:21:52.890
<v Speaker 1>How do we think about existential </v>
<v Speaker 1>threats when we don't really know so </v>

405
00:21:52.890 --> 00:21:54.400
<v Speaker 1>little about the threat,</v>
<v Speaker 1>unlike nuclear weapons,</v>

406
00:21:54.401 --> 00:21:59.401
<v Speaker 1>perhaps about this particular threat </v>
<v Speaker 1>that it could happen tomorrow,</v>

407
00:21:59.950 --> 00:22:00.910
<v Speaker 1>right?</v>
<v Speaker 1>So,</v>

408
00:22:01.120 --> 00:22:04.630
<v Speaker 1>but very likely won't likely to be 100 </v>
<v Speaker 1>years away.</v>

409
00:22:04.631 --> 00:22:07.120
<v Speaker 1>So how do we ignore it?</v>
<v Speaker 1>Do,</v>

410
00:22:07.180 --> 00:22:10.100
<v Speaker 1>how do we talk about it?</v>
<v Speaker 1>Do we worry about it?</v>

411
00:22:10.110 --> 00:22:11.710
<v Speaker 1>So what,</v>
<v Speaker 1>how do we think about those?</v>

412
00:22:11.740 --> 00:22:12.420
<v Speaker 1>W W,</v>
<v Speaker 1>W,</v>

413
00:22:12.430 --> 00:22:13.240
<v Speaker 1>W,</v>
<v Speaker 1>what is it,</v>

414
00:22:13.720 --> 00:22:18.430
<v Speaker 1>a threat that we can imagine it's within</v>
<v Speaker 1>the limits of our imagination,</v>

415
00:22:18.700 --> 00:22:23.700
<v Speaker 1>but not within our limits of </v>
<v Speaker 1>understanding to sufficient to </v>

416
00:22:23.700 --> 00:22:23.700
<v Speaker 1>accurately predict it.</v>

417
00:22:23.970 --> 00:22:24.790
<v Speaker 2>Uh,</v>
<v Speaker 2>but what,</v>

418
00:22:24.830 --> 00:22:25.710
<v Speaker 2>what is,</v>
<v Speaker 2>what is the</v>

419
00:22:26.620 --> 00:22:28.160
<v Speaker 1>AI,</v>
<v Speaker 1>ai,</v>

420
00:22:28.890 --> 00:22:30.750
<v Speaker 1>ai being the existential threat ai,</v>

421
00:22:31.280 --> 00:22:34.640
<v Speaker 2>how enslaving us or turning us into </v>
<v Speaker 2>paperclips?</v>

422
00:22:35.270 --> 00:22:40.270
<v Speaker 2>I think the most compelling from the Sam</v>
<v Speaker 2>Harris' perspective would be the </v>

423
00:22:40.270 --> 00:22:40.270
<v Speaker 2>paperclip situation.</v>
<v Speaker 2>Yeah,</v>

424
00:22:40.270 --> 00:22:40.540
<v Speaker 2>I mean,</v>
<v Speaker 2>I think,</v>

425
00:22:40.541 --> 00:22:41.870
<v Speaker 2>I just think it's totally fanciful.</v>
<v Speaker 2>Ivy.</v>

426
00:22:41.900 --> 00:22:44.010
<v Speaker 2>I don't build a system.</v>
<v Speaker 2>Don't give,</v>

427
00:22:44.800 --> 00:22:45.830
<v Speaker 2>don't.</v>
<v Speaker 2>First of all,</v>

428
00:22:46.240 --> 00:22:51.240
<v Speaker 2>uh,</v>
<v Speaker 2>the code of engineering is you don't </v>

429
00:22:51.240 --> 00:22:51.680
<v Speaker 2>implement a system with massive control </v>
<v Speaker 2>before testing it.</v>

430
00:22:52.040 --> 00:22:57.040
<v Speaker 2>Now perhaps the culture of engineering </v>
<v Speaker 2>will radically change then I would </v>

431
00:22:57.040 --> 00:22:57.980
<v Speaker 2>worry,</v>
<v Speaker 2>but I don't see any signs that engineers</v>

432
00:22:57.981 --> 00:23:01.420
<v Speaker 2>will suddenly do idiotic things like put</v>
<v Speaker 2>a,</v>

433
00:23:01.740 --> 00:23:06.740
<v Speaker 2>uh,</v>
<v Speaker 2>an electrical power plant in control of </v>

434
00:23:06.740 --> 00:23:06.740
<v Speaker 2>a system that they haven't tested,</v>
<v Speaker 2>uh,</v>

435
00:23:06.740 --> 00:23:07.350
<v Speaker 2>first,</v>
<v Speaker 2>uh,</v>

436
00:23:07.400 --> 00:23:11.150
<v Speaker 2>or all of these scenarios.</v>
<v Speaker 2>Not only imagine a,</v>

437
00:23:11.580 --> 00:23:16.580
<v Speaker 2>um,</v>
<v Speaker 2>almost a magically powered intelligence </v>

438
00:23:16.580 --> 00:23:20.650
<v Speaker 2>including things like cure cancer,</v>
<v Speaker 2>which is probably an incoherent goal </v>

439
00:23:20.650 --> 00:23:24.290
<v Speaker 2>because there's so many different kinds </v>
<v Speaker 2>of cancer or bring about world peace.</v>

440
00:23:24.350 --> 00:23:26.270
<v Speaker 2>I mean,</v>
<v Speaker 2>how do you even specify that as a goal,</v>

441
00:23:26.630 --> 00:23:31.630
<v Speaker 2>but the scenario is also imagined some </v>
<v Speaker 2>degree of control of every molecule in </v>

442
00:23:31.630 --> 00:23:32.780
<v Speaker 2>the universe,</v>
<v Speaker 2>uh,</v>

443
00:23:32.781 --> 00:23:37.781
<v Speaker 2>which not only is itself unlikely,</v>
<v Speaker 2>but we would not start to connect these </v>

444
00:23:37.791 --> 00:23:42.791
<v Speaker 2>systems to a infrastructure without a,</v>
<v Speaker 2>without testing as we would any kind of </v>

445
00:23:45.320 --> 00:23:45.770
<v Speaker 2>system.</v>

446
00:23:45.810 --> 00:23:49.640
<v Speaker 2>There may be some engineers will be </v>
<v Speaker 2>you're responsible and we need legal and</v>

447
00:23:50.760 --> 00:23:55.760
<v Speaker 2>a regulatory and legal responsibility </v>
<v Speaker 2>implemented so that engineers don't do </v>

448
00:23:57.710 --> 00:23:59.660
<v Speaker 2>things that are stupid by their own </v>
<v Speaker 2>standards.</v>

449
00:24:00.720 --> 00:24:01.800
<v Speaker 2>But the,</v>
<v Speaker 2>uh,</v>

450
00:24:02.100 --> 00:24:07.100
<v Speaker 2>uh,</v>
<v Speaker 2>I've never seen enough of a plausible </v>

451
00:24:07.100 --> 00:24:09.741
<v Speaker 2>scenario of a existential threat to </v>
<v Speaker 2>devote large amounts of brain power to,</v>

452
00:24:10.350 --> 00:24:12.940
<v Speaker 2>to forestall it.</v>
<v Speaker 2>You believe in</v>

453
00:24:13.410 --> 00:24:18.410
<v Speaker 1>sort of the power and mass of the </v>
<v Speaker 1>engineering of reason as the arguing </v>

454
00:24:18.410 --> 00:24:22.460
<v Speaker 1>your latest book of reason and science </v>
<v Speaker 1>and sort of be the very thing that </v>

455
00:24:22.590 --> 00:24:24.800
<v Speaker 1>guides the development of new </v>
<v Speaker 1>technology.</v>

456
00:24:24.801 --> 00:24:26.580
<v Speaker 1>So it's safe and also keeps us safe.</v>

457
00:24:26.660 --> 00:24:28.160
<v Speaker 2>Yeah.</v>
<v Speaker 2>If the same and you know,</v>

458
00:24:28.310 --> 00:24:33.310
<v Speaker 2>granted that same culture of safety that</v>
<v Speaker 2>currently is part of the engineering </v>

459
00:24:33.680 --> 00:24:35.960
<v Speaker 2>mindset for airplanes,</v>
<v Speaker 2>for example.</v>

460
00:24:36.290 --> 00:24:37.760
<v Speaker 2>So yeah,</v>
<v Speaker 2>I don't think that,</v>

461
00:24:37.820 --> 00:24:38.510
<v Speaker 2>that,</v>
<v Speaker 2>uh,</v>

462
00:24:38.511 --> 00:24:43.511
<v Speaker 2>that,</v>
<v Speaker 2>that should be thrown out the window </v>

463
00:24:43.511 --> 00:24:43.511
<v Speaker 2>and,</v>
<v Speaker 2>uh,</v>

464
00:24:43.511 --> 00:24:45.610
<v Speaker 2>that untested,</v>
<v Speaker 2>all powerful systems should be suddenly </v>

465
00:24:45.610 --> 00:24:46.280
<v Speaker 2>implemented,</v>
<v Speaker 2>but there's no reason to think they are.</v>

466
00:24:46.290 --> 00:24:51.290
<v Speaker 2>And in fact,</v>
<v Speaker 2>if you look at the progress of </v>

467
00:24:51.290 --> 00:24:51.290
<v Speaker 2>artificial intelligence,</v>
<v Speaker 2>it's been,</v>

468
00:24:51.290 --> 00:24:53.090
<v Speaker 2>it's been impressive,</v>
<v Speaker 2>especially in the last 10 years or so,</v>

469
00:24:53.330 --> 00:24:58.330
<v Speaker 2>but the idea that suddenly there'll be a</v>
<v Speaker 2>step function that all of a sudden </v>

470
00:24:58.330 --> 00:24:58.700
<v Speaker 2>before we know it,</v>
<v Speaker 2>it will be,</v>

471
00:24:59.610 --> 00:25:00.270
<v Speaker 2>oh,</v>
<v Speaker 2>powerful.</v>

472
00:25:00.271 --> 00:25:02.790
<v Speaker 2>That there'll be some kind of recursive </v>
<v Speaker 2>self improvement,</v>

473
00:25:02.791 --> 00:25:04.800
<v Speaker 2>some kind of a foom,</v>
<v Speaker 2>uh,</v>

474
00:25:04.801 --> 00:25:08.040
<v Speaker 2>is also a fanciful.</v>
<v Speaker 2>We certainly,</v>

475
00:25:08.041 --> 00:25:12.270
<v Speaker 2>by the technology that we,</v>
<v Speaker 2>that we're now impresses us,</v>

476
00:25:12.271 --> 00:25:17.271
<v Speaker 2>such as deep learning where you train </v>
<v Speaker 2>something on a hundreds of thousands or </v>

477
00:25:17.271 --> 00:25:21.771
<v Speaker 2>millions of examples.</v>
<v Speaker 2>They're not hundreds of thousands of </v>

478
00:25:21.771 --> 00:25:25.410
<v Speaker 2>problems of which curing cancer is a </v>
<v Speaker 2>typical example.</v>

479
00:25:25.850 --> 00:25:30.850
<v Speaker 2>Uh,</v>
<v Speaker 2>and so the kind of techniques that have </v>

480
00:25:30.850 --> 00:25:32.841
<v Speaker 2>allowed a ai too increased in the last </v>
<v Speaker 2>five years are not the kind that are </v>

481
00:25:32.841 --> 00:25:35.880
<v Speaker 2>gonna lead to this fantasy of,</v>
<v Speaker 2>uh,</v>

482
00:25:35.881 --> 00:25:38.550
<v Speaker 2>of,</v>
<v Speaker 2>of exponential sudden self improvement.</v>

483
00:25:38.730 --> 00:25:41.550
<v Speaker 2>So it's,</v>
<v Speaker 2>I think it's kind of a magical thinking.</v>

484
00:25:41.551 --> 00:25:45.410
<v Speaker 2>It's not based on our understanding of </v>
<v Speaker 2>how ai actually works.</v>

485
00:25:45.560 --> 00:25:48.410
<v Speaker 1>Now give me a chance here.</v>
<v Speaker 1>So you said fanciful,</v>

486
00:25:48.411 --> 00:25:51.200
<v Speaker 1>magical thinking.</v>
<v Speaker 1>In his Ted talk,</v>

487
00:25:51.201 --> 00:25:56.201
<v Speaker 1>Sam Harris says that thinking about ai </v>
<v Speaker 1>killing all human civilization is </v>

488
00:25:56.201 --> 00:25:59.861
<v Speaker 1>somehow fun intellectually.</v>
<v Speaker 1>Now I have to say as a scientist </v>

489
00:25:59.861 --> 00:26:00.740
<v Speaker 1>engineer,</v>
<v Speaker 1>I don't find it fun,</v>

490
00:26:01.370 --> 00:26:04.540
<v Speaker 1>but when I'm having beer with my </v>
<v Speaker 1>friends,</v>

491
00:26:05.000 --> 00:26:08.850
<v Speaker 1>there is indeed something fun and </v>
<v Speaker 1>appealing about it.</v>

492
00:26:08.851 --> 00:26:10.790
<v Speaker 1>Like talking about an episode of Black </v>
<v Speaker 1>Mirror,</v>

493
00:26:10.791 --> 00:26:14.930
<v Speaker 1>considering if a large media or is </v>
<v Speaker 1>headed towards her,</v>

494
00:26:14.990 --> 00:26:17.780
<v Speaker 1>we were just told a large media is </v>
<v Speaker 1>headed towards earth,</v>

495
00:26:18.290 --> 00:26:23.290
<v Speaker 1>something like this.</v>
<v Speaker 1>And can you relate to this sense of fun </v>

496
00:26:23.290 --> 00:26:24.580
<v Speaker 1>and do you understand the psychology of </v>
<v Speaker 1>it?</v>

497
00:26:25.450 --> 00:26:26.650
<v Speaker 2>Good question.</v>
<v Speaker 2>Uh,</v>

498
00:26:26.670 --> 00:26:28.790
<v Speaker 2>I,</v>
<v Speaker 2>I personally don't find it fun.</v>

499
00:26:28.810 --> 00:26:33.810
<v Speaker 2>Um,</v>
<v Speaker 2>I find it kind of a actually a waste of </v>

500
00:26:33.810 --> 00:26:37.150
<v Speaker 2>time because there are genuine threats </v>
<v Speaker 2>that we ought to be thinking about,</v>

501
00:26:37.300 --> 00:26:38.530
<v Speaker 2>like,</v>
<v Speaker 2>like pandemics,</v>

502
00:26:38.590 --> 00:26:41.710
<v Speaker 2>like,</v>
<v Speaker 2>like a cybersecurity vulnerabilities.</v>

503
00:26:42.240 --> 00:26:47.240
<v Speaker 2>I'm like the possibility of nuclear war </v>
<v Speaker 2>and certainly climate change is enough </v>

504
00:26:47.821 --> 00:26:49.730
<v Speaker 2>to fill that many,</v>
<v Speaker 2>uh,</v>

505
00:26:50.160 --> 00:26:52.140
<v Speaker 2>uh,</v>
<v Speaker 2>conversations without.</v>

506
00:26:52.390 --> 00:26:57.390
<v Speaker 2>And I think they're,</v>
<v Speaker 2>I think Sam did put his finger on </v>

507
00:26:57.390 --> 00:26:57.390
<v Speaker 2>something,</v>
<v Speaker 2>namely that there is a community,</v>

508
00:26:57.680 --> 00:27:02.680
<v Speaker 2>uh,</v>
<v Speaker 2>sometimes called the rationality </v>

509
00:27:02.680 --> 00:27:05.270
<v Speaker 2>community that delights in using it's </v>
<v Speaker 2>brain power to come up with scenarios </v>

510
00:27:06.121 --> 00:27:08.930
<v Speaker 2>that would not occur to me are mortals </v>
<v Speaker 2>to,</v>

511
00:27:08.960 --> 00:27:13.960
<v Speaker 2>to so the less cerebral people.</v>
<v Speaker 2>So there is a kind of intellectual </v>

512
00:27:13.960 --> 00:27:18.351
<v Speaker 2>thrill and finding new things to worry </v>
<v Speaker 2>about that no one has a worried about </v>

513
00:27:18.351 --> 00:27:19.050
<v Speaker 2>yet.</v>
<v Speaker 2>I actually think though,</v>

514
00:27:19.051 --> 00:27:24.051
<v Speaker 2>that it's not only,</v>
<v Speaker 2>that is a kind of fun that doesn't give </v>

515
00:27:24.051 --> 00:27:24.051
<v Speaker 2>me particular pleasure,</v>
<v Speaker 2>uh,</v>

516
00:27:24.090 --> 00:27:26.490
<v Speaker 2>but I think there can be a pernicious </v>
<v Speaker 2>side to it,</v>

517
00:27:26.520 --> 00:27:30.570
<v Speaker 2>namely that you overcome people with </v>
<v Speaker 2>such a dread,</v>

518
00:27:30.630 --> 00:27:33.590
<v Speaker 2>such fatalism that there's so many ways </v>
<v Speaker 2>to,</v>

519
00:27:33.640 --> 00:27:34.350
<v Speaker 2>to,</v>
<v Speaker 2>uh,</v>

520
00:27:34.540 --> 00:27:35.280
<v Speaker 2>to,</v>
<v Speaker 2>to die,</v>

521
00:27:35.281 --> 00:27:39.840
<v Speaker 2>to annihilate our civilization that we </v>
<v Speaker 2>may as well enjoy life while we can.</v>

522
00:27:39.840 --> 00:27:42.360
<v Speaker 2>There's nothing we can do about it.</v>
<v Speaker 2>If climate change doesn't do us in,</v>

523
00:27:42.390 --> 00:27:44.580
<v Speaker 2>then runaway robots will.</v>
<v Speaker 2>So,</v>

524
00:27:44.581 --> 00:27:45.320
<v Speaker 2>uh,</v>
<v Speaker 2>what,</v>

525
00:27:45.330 --> 00:27:48.510
<v Speaker 2>let's enjoy ourselves now,</v>
<v Speaker 2>we got to prioritize.</v>

526
00:27:48.910 --> 00:27:53.910
<v Speaker 2>We have to look at threats that are </v>
<v Speaker 2>close to certainty,</v>

527
00:27:54.421 --> 00:27:57.900
<v Speaker 2>such as climate change,</v>
<v Speaker 2>and distinguish those from ones that are</v>

528
00:27:57.901 --> 00:28:01.080
<v Speaker 2>merely imaginable.</v>
<v Speaker 2>But with infinitesimal probabilities,</v>

529
00:28:02.010 --> 00:28:05.760
<v Speaker 2>and we have to take into account </v>
<v Speaker 2>people's worry budget,</v>

530
00:28:05.940 --> 00:28:10.940
<v Speaker 2>you can't worry about everything and if </v>
<v Speaker 2>you so dread and fear and terror and </v>

531
00:28:11.311 --> 00:28:16.311
<v Speaker 2>numb and fatalism,</v>
<v Speaker 2>it can lead to a kind of numbness while </v>

532
00:28:16.311 --> 00:28:19.521
<v Speaker 2>they just,</v>
<v Speaker 2>these problems are overwhelming and the </v>

533
00:28:19.521 --> 00:28:19.521
<v Speaker 2>engineers are just going to kill us all.</v>
<v Speaker 2>Um,</v>

534
00:28:19.521 --> 00:28:20.010
<v Speaker 2>so,</v>
<v Speaker 2>uh,</v>

535
00:28:20.190 --> 00:28:25.190
<v Speaker 2>let's either destroy the entire </v>
<v Speaker 2>infrastructure of science technology or </v>

536
00:28:27.170 --> 00:28:29.450
<v Speaker 2>let's just enjoy life while we can.</v>

537
00:28:29.670 --> 00:28:34.670
<v Speaker 1>So there's a certain line of worrying,</v>
<v Speaker 1>which I'm worried about a lot of things </v>

538
00:28:34.670 --> 00:28:35.550
<v Speaker 1>engineering,</v>
<v Speaker 1>there's a certain line of worry when you</v>

539
00:28:35.551 --> 00:28:40.551
<v Speaker 1>cross a law to cross a that it becomes </v>
<v Speaker 1>paralyzing fear as opposed to productive</v>

540
00:28:42.171 --> 00:28:47.171
<v Speaker 1>here.</v>
<v Speaker 1>And that's kind of what I'm </v>

541
00:28:47.171 --> 00:28:47.171
<v Speaker 1>highlighting.</v>

542
00:28:47.171 --> 00:28:47.171
<v Speaker 2>Exactly right.</v>
<v Speaker 2>And,</v>

543
00:28:47.171 --> 00:28:47.960
<v Speaker 2>and we've seen some,</v>
<v Speaker 2>uh,</v>

544
00:28:48.090 --> 00:28:53.090
<v Speaker 2>we know that human effort is not well </v>
<v Speaker 2>calibrated against risk in that because </v>

545
00:28:55.000 --> 00:29:00.000
<v Speaker 2>a basic tenant of cognitive psychology </v>
<v Speaker 2>is that a perception of risk and his </v>

546
00:29:00.931 --> 00:29:05.931
<v Speaker 2>perception of fear.</v>
<v Speaker 2>It's driven by imagine ability not by </v>

547
00:29:05.931 --> 00:29:05.931
<v Speaker 2>data.</v>
<v Speaker 2>Uh,</v>

548
00:29:05.970 --> 00:29:10.970
<v Speaker 2>and so we miss allocate fast amounts of </v>
<v Speaker 2>resources to avoiding terrorism,</v>

549
00:29:11.340 --> 00:29:14.760
<v Speaker 2>which kills on average about six </v>
<v Speaker 2>Americans a year with a one exception of</v>

550
00:29:14.761 --> 00:29:15.840
<v Speaker 2>nine slash 11.</v>
<v Speaker 2>Uh,</v>

551
00:29:15.870 --> 00:29:20.870
<v Speaker 2>we invade countries,</v>
<v Speaker 2>we invent an entire new departments of </v>

552
00:29:20.870 --> 00:29:25.191
<v Speaker 2>government with massive,</v>
<v Speaker 2>massive expenditure of resources in </v>

553
00:29:25.191 --> 00:29:27.690
<v Speaker 2>life's to defend ourselves against a </v>
<v Speaker 2>trivial risk,</v>

554
00:29:28.110 --> 00:29:30.840
<v Speaker 2>uh,</v>
<v Speaker 2>whereas guaranteed risks,</v>

555
00:29:30.870 --> 00:29:34.140
<v Speaker 2>and you mentioned as one of them is,</v>
<v Speaker 2>you mentioned traffic fatalities,</v>

556
00:29:34.480 --> 00:29:37.080
<v Speaker 2>uh,</v>
<v Speaker 2>and even risks that are,</v>

557
00:29:37.600 --> 00:29:42.600
<v Speaker 2>I'm not here but are plausible enough to</v>
<v Speaker 2>worry about like pandemics,</v>

558
00:29:44.350 --> 00:29:48.610
<v Speaker 2>like nuclear war received far too little</v>
<v Speaker 2>attention,</v>

559
00:29:48.750 --> 00:29:52.720
<v Speaker 2>the in presidential debates.</v>
<v Speaker 2>There's no discussion of how to minimize</v>

560
00:29:52.721 --> 00:29:54.190
<v Speaker 2>the risk of,</v>
<v Speaker 2>of nuclear war,</v>

561
00:29:54.250 --> 00:29:56.830
<v Speaker 2>lots of discussion of terrorism,</v>
<v Speaker 2>for example.</v>

562
00:29:57.060 --> 00:29:58.180
<v Speaker 2>Uh,</v>
<v Speaker 2>and,</v>

563
00:29:58.250 --> 00:30:03.250
<v Speaker 2>and so we,</v>
<v Speaker 2>I think it's essential to calibrate our </v>

564
00:30:03.250 --> 00:30:04.090
<v Speaker 2>budget of fear,</v>
<v Speaker 2>worry,</v>

565
00:30:04.091 --> 00:30:06.580
<v Speaker 2>concern,</v>
<v Speaker 2>planning to the,</v>

566
00:30:06.581 --> 00:30:09.200
<v Speaker 2>uh,</v>
<v Speaker 2>actual probability of a,</v>

567
00:30:09.201 --> 00:30:09.730
<v Speaker 2>of harm.</v>

568
00:30:10.160 --> 00:30:12.150
<v Speaker 1>Yep.</v>
<v Speaker 1>So let me ask this then,</v>

569
00:30:12.151 --> 00:30:15.560
<v Speaker 1>this question.</v>
<v Speaker 1>So speaking of imagine an ability,</v>

570
00:30:15.890 --> 00:30:17.990
<v Speaker 1>you said that it's important to think </v>
<v Speaker 1>about reason.</v>

571
00:30:18.320 --> 00:30:23.320
<v Speaker 1>And one of my favorite people who,</v>
<v Speaker 1>who likes to dip into the outskirts of </v>

572
00:30:23.320 --> 00:30:24.130
<v Speaker 1>reason,</v>
<v Speaker 1>uh,</v>

573
00:30:24.140 --> 00:30:28.070
<v Speaker 1>through fascinating exploration of his </v>
<v Speaker 1>imagination is Joe Rogan.</v>

574
00:30:28.430 --> 00:30:29.660
<v Speaker 1>Oh yes.</v>
<v Speaker 1>Uh,</v>

575
00:30:29.661 --> 00:30:30.440
<v Speaker 1>you,</v>
<v Speaker 1>uh,</v>

576
00:30:30.470 --> 00:30:35.470
<v Speaker 1>so who has three reasons to believe a </v>
<v Speaker 1>lot of conspiracies and through a reason</v>

577
00:30:35.751 --> 00:30:38.780
<v Speaker 1>has stripped away a lot of his beliefs </v>
<v Speaker 1>in that way.</v>

578
00:30:38.781 --> 00:30:42.560
<v Speaker 1>So it's fascinating actually to watch </v>
<v Speaker 1>him through rationality.</v>

579
00:30:42.620 --> 00:30:45.650
<v Speaker 1>Kinda throw away the ideas.</v>
<v Speaker 1>Have a big foot.</v>

580
00:30:45.651 --> 00:30:47.270
<v Speaker 1>And um,</v>
<v Speaker 1>nine slash 11,</v>

581
00:30:47.300 --> 00:30:49.880
<v Speaker 1>I'm not sure exactly the trails.</v>
<v Speaker 1>I don't know what he believes,</v>

582
00:30:50.690 --> 00:30:52.780
<v Speaker 1>but he no longer believed in.</v>
<v Speaker 1>That's all right.</v>

583
00:30:52.810 --> 00:30:53.200
<v Speaker 1>No,</v>
<v Speaker 1>he's,</v>

584
00:30:53.280 --> 00:30:54.820
<v Speaker 1>he's become a real force for,</v>
<v Speaker 1>uh,</v>

585
00:30:55.100 --> 00:30:55.790
<v Speaker 1>for Greg.</v>
<v Speaker 1>Yep.</v>

586
00:30:56.030 --> 00:31:01.030
<v Speaker 1>So you were on the Joe Rogan podcast in </v>
<v Speaker 1>February and had a fascinating </v>

587
00:31:01.030 --> 00:31:01.760
<v Speaker 1>conversation.</v>
<v Speaker 1>But as far as I remember,</v>

588
00:31:01.761 --> 00:31:04.160
<v Speaker 1>didn't talk much about artificial </v>
<v Speaker 1>intelligence.</v>

589
00:31:04.760 --> 00:31:06.890
<v Speaker 1>I will be on this podcast and a couple </v>
<v Speaker 1>weeks,</v>

590
00:31:07.370 --> 00:31:11.030
<v Speaker 1>joe is very much concerned about </v>
<v Speaker 1>existential threat of Ai.</v>

591
00:31:11.031 --> 00:31:16.031
<v Speaker 1>I'm not sure if you're a.</v>
<v Speaker 1>This is why I was hoping that you will </v>

592
00:31:16.031 --> 00:31:17.150
<v Speaker 1>get into that topic.</v>
<v Speaker 1>And in this way,</v>

593
00:31:17.151 --> 00:31:21.350
<v Speaker 1>he represents quite a lot of people who </v>
<v Speaker 1>look at the topic of Ai from 10,000</v>

594
00:31:21.351 --> 00:31:26.351
<v Speaker 1>foot level.</v>
<v Speaker 1>So as an exercise of a communication,</v>

595
00:31:26.840 --> 00:31:29.930
<v Speaker 1>you said it's important to be rational </v>
<v Speaker 1>and reason about these things.</v>

596
00:31:30.200 --> 00:31:35.200
<v Speaker 1>Let me ask,</v>
<v Speaker 1>if you were to coach me as an ai </v>

597
00:31:35.200 --> 00:31:36.920
<v Speaker 1>researcher about how to speak to Joe and</v>
<v Speaker 1>the general public about Ai,</v>

598
00:31:36.950 --> 00:31:38.120
<v Speaker 1>what would you advise?</v>

599
00:31:38.370 --> 00:31:38.920
<v Speaker 2>Well,</v>
<v Speaker 2>like,</v>

600
00:31:38.990 --> 00:31:43.990
<v Speaker 2>uh,</v>
<v Speaker 2>the short answer would be to read the </v>

601
00:31:43.990 --> 00:31:43.990
<v Speaker 2>sections that I wrote in an enlightened </v>
<v Speaker 2>about ai,</v>

602
00:31:44.160 --> 00:31:46.440
<v Speaker 2>but longer reason would be I think to </v>
<v Speaker 2>emphasize,</v>

603
00:31:46.530 --> 00:31:51.530
<v Speaker 2>and I think you're very well positioned </v>
<v Speaker 2>as an engineer to remind people about </v>

604
00:31:51.530 --> 00:31:54.450
<v Speaker 2>the culture of engineering,</v>
<v Speaker 2>that it really is a safety oriented.</v>

605
00:31:54.870 --> 00:31:59.520
<v Speaker 2>That another discussion in enlightenment</v>
<v Speaker 2>now I plot,</v>

606
00:32:00.010 --> 00:32:05.010
<v Speaker 2>uh,</v>
<v Speaker 2>rates of accidental death from various </v>

607
00:32:05.010 --> 00:32:05.010
<v Speaker 2>causes,</v>
<v Speaker 2>plane crashes,</v>

608
00:32:05.010 --> 00:32:07.270
<v Speaker 2>car crashes,</v>
<v Speaker 2>occupational accidents,</v>

609
00:32:07.360 --> 00:32:12.360
<v Speaker 2>even death by lightning strikes.</v>
<v Speaker 2>And they all plummet because the culture</v>

610
00:32:13.421 --> 00:32:15.870
<v Speaker 2>of engineering is how do you squeeze out</v>
<v Speaker 2>the,</v>

611
00:32:15.871 --> 00:32:16.100
<v Speaker 2>uh,</v>
<v Speaker 2>the,</v>

612
00:32:16.110 --> 00:32:17.950
<v Speaker 2>the lethal risks,</v>
<v Speaker 2>death by fire,</v>

613
00:32:17.951 --> 00:32:21.800
<v Speaker 2>death by drowning,</v>
<v Speaker 2>death by a fixation.</v>

614
00:32:21.880 --> 00:32:25.000
<v Speaker 2>All them drastically declined because of</v>
<v Speaker 2>advances in engineering.</v>

615
00:32:25.180 --> 00:32:30.180
<v Speaker 2>Then I got to say,</v>
<v Speaker 2>I did not appreciate until I saw those </v>

616
00:32:30.180 --> 00:32:33.241
<v Speaker 2>graphs,</v>
<v Speaker 2>and it is because exactly people like </v>

617
00:32:33.241 --> 00:32:34.120
<v Speaker 2>you who stay up at night thinking,</v>
<v Speaker 2>oh my God,</v>

618
00:32:34.190 --> 00:32:35.740
<v Speaker 2>is what am I,</v>
<v Speaker 2>what I'm,</v>

619
00:32:36.090 --> 00:32:41.090
<v Speaker 2>what I'm inventing likely to hurt people</v>
<v Speaker 2>and to deploy ingenuity to prevent that </v>

620
00:32:41.991 --> 00:32:43.760
<v Speaker 2>from happening.</v>
<v Speaker 2>Now I'm not an engineer,</v>

621
00:32:44.030 --> 00:32:47.600
<v Speaker 2>although I spent 22 years at Mit,</v>
<v Speaker 2>so I know something about the culture of</v>

622
00:32:47.601 --> 00:32:49.820
<v Speaker 2>engineering,</v>
<v Speaker 2>my understanding that this is the way,</v>

623
00:32:50.150 --> 00:32:55.150
<v Speaker 2>this is what you think if you're an </v>
<v Speaker 2>engineer and it's essential that that </v>

624
00:32:55.150 --> 00:32:59.060
<v Speaker 2>culture not be suddenly switched off </v>
<v Speaker 2>when it comes to artificial intelligence</v>

625
00:32:59.120 --> 00:32:59.980
<v Speaker 2>so that,</v>
<v Speaker 2>that,</v>

626
00:33:00.050 --> 00:33:05.050
<v Speaker 2>that could be a problem.</v>
<v Speaker 2>But is there any reason to think it </v>

627
00:33:05.050 --> 00:33:05.050
<v Speaker 2>would be switched off?</v>

628
00:33:05.050 --> 00:33:06.330
<v Speaker 1>Think so and one.</v>
<v Speaker 1>There's not enough engineers speaking up</v>

629
00:33:06.360 --> 00:33:07.890
<v Speaker 1>for this way,</v>
<v Speaker 1>for this.</v>

630
00:33:07.900 --> 00:33:09.920
<v Speaker 1>The excitement for,</v>
<v Speaker 1>uh,</v>

631
00:33:10.200 --> 00:33:13.580
<v Speaker 1>the positive view of human nature,</v>
<v Speaker 1>what you're trying to create,</v>

632
00:33:13.660 --> 00:33:18.660
<v Speaker 1>the positivity,</v>
<v Speaker 1>like everything we try to invent is </v>

633
00:33:18.660 --> 00:33:21.411
<v Speaker 1>trying to do good for the world.</v>
<v Speaker 1>But let me ask you about the psychology </v>

634
00:33:21.411 --> 00:33:25.191
<v Speaker 1>of negativity.</v>
<v Speaker 1>It seems just objectively not </v>

635
00:33:25.191 --> 00:33:28.971
<v Speaker 1>considering the topic.</v>
<v Speaker 1>It seems that being negative about the </v>

636
00:33:28.971 --> 00:33:31.701
<v Speaker 1>future,</v>
<v Speaker 1>it makes you sound smarter than being </v>

637
00:33:31.701 --> 00:33:31.830
<v Speaker 1>positive about the future.</v>
<v Speaker 1>It regardless of topic.</v>

638
00:33:31.860 --> 00:33:35.160
<v Speaker 1>Am I correct in this observation?</v>
<v Speaker 1>And if so,</v>

639
00:33:35.220 --> 00:33:35.940
<v Speaker 1>why do you think</v>

640
00:33:35.940 --> 00:33:36.540
<v Speaker 2>that is?</v>
<v Speaker 2>Yeah,</v>

641
00:33:36.541 --> 00:33:41.541
<v Speaker 2>I think I,</v>
<v Speaker 2>I think there is that a phenomenon that </v>

642
00:33:41.541 --> 00:33:42.210
<v Speaker 2>as Tom Lehrer,</v>
<v Speaker 2>the Sandra said,</v>

643
00:33:42.211 --> 00:33:44.540
<v Speaker 2>always predict the worst and you'll be </v>
<v Speaker 2>hailed as a profit.</v>

644
00:33:45.850 --> 00:33:49.170
<v Speaker 2>Maybe part of our overall negativity </v>
<v Speaker 2>bias.</v>

645
00:33:49.171 --> 00:33:54.171
<v Speaker 2>We are,</v>
<v Speaker 2>as a species more attuned to the </v>

646
00:33:54.171 --> 00:33:56.250
<v Speaker 2>negative than the positive.</v>
<v Speaker 2>We dread losses more than we enjoy gains</v>

647
00:33:57.110 --> 00:34:02.110
<v Speaker 2>and uh,</v>
<v Speaker 2>that may might open up a space for a </v>

648
00:34:02.540 --> 00:34:06.800
<v Speaker 2>profits to remind us of harms and risks </v>
<v Speaker 2>and losses that we may have overlooked.</v>

649
00:34:07.310 --> 00:34:08.510
<v Speaker 2>Uh,</v>
<v Speaker 2>so I think they're,</v>

650
00:34:08.511 --> 00:34:09.450
<v Speaker 2>they're,</v>
<v Speaker 2>uh,</v>

651
00:34:09.830 --> 00:34:11.480
<v Speaker 2>they're there.</v>
<v Speaker 2>Is that a symmetry?</v>

652
00:34:11.990 --> 00:34:16.990
<v Speaker 1>So you've written some of my favorite </v>
<v Speaker 1>books all over the place.</v>

653
00:34:17.091 --> 00:34:19.940
<v Speaker 1>So starting from enlightenment now to,</v>
<v Speaker 1>uh,</v>

654
00:34:19.941 --> 00:34:22.400
<v Speaker 1>the better angels of our nature.</v>
<v Speaker 1>Blank slate,</v>

655
00:34:22.401 --> 00:34:27.401
<v Speaker 1>how the mind works,</v>
<v Speaker 1>the one about language language </v>

656
00:34:27.401 --> 00:34:30.971
<v Speaker 1>instinct,</v>
<v Speaker 1>a Bill Gates big fan to a set of your </v>

657
00:34:31.731 --> 00:34:35.810
<v Speaker 1>most recent book that it's my new </v>
<v Speaker 1>favorite book of all time.</v>

658
00:34:37.460 --> 00:34:42.460
<v Speaker 1>So for you as an author,</v>
<v Speaker 1>what was the book early on in your life </v>

659
00:34:42.460 --> 00:34:44.930
<v Speaker 1>that had a profound impact on the way </v>
<v Speaker 1>you saw the world?</v>

660
00:34:45.460 --> 00:34:50.460
<v Speaker 2>Certainly this book enlightened me now </v>
<v Speaker 2>is influenced by David Deutsch is the </v>

661
00:34:50.460 --> 00:34:55.321
<v Speaker 2>beginning of infinity,</v>
<v Speaker 2>have rather deep reflection on a </v>

662
00:34:55.321 --> 00:34:58.570
<v Speaker 2>knowledge and the power of knowledge to </v>
<v Speaker 2>improve the human condition.</v>

663
00:34:58.980 --> 00:35:03.980
<v Speaker 2>Uh,</v>
<v Speaker 2>the end with bits of wisdom such as the </v>

664
00:35:03.980 --> 00:35:06.810
<v Speaker 2>problems are inevitable,</v>
<v Speaker 2>but problems are solvable given the </v>

665
00:35:06.810 --> 00:35:09.891
<v Speaker 2>right knowledge and that solutions </v>
<v Speaker 2>create new problems that have to be </v>

666
00:35:09.891 --> 00:35:09.891
<v Speaker 2>solved in their turn.</v>
<v Speaker 2>That's,</v>

667
00:35:10.050 --> 00:35:15.050
<v Speaker 2>I think,</v>
<v Speaker 2>a kind of wisdom about the human </v>

668
00:35:15.050 --> 00:35:15.050
<v Speaker 2>condition that influenced the writing of</v>
<v Speaker 2>this book.</v>

669
00:35:15.050 --> 00:35:16.980
<v Speaker 2>There's some books that are excellent </v>
<v Speaker 2>but obscure,</v>

670
00:35:17.040 --> 00:35:18.620
<v Speaker 2>some of which I have on my,</v>
<v Speaker 2>uh,</v>

671
00:35:18.650 --> 00:35:23.650
<v Speaker 2>I'm on a page of my website and I read a</v>
<v Speaker 2>book called the history of force self </v>

672
00:35:23.650 --> 00:35:27.510
<v Speaker 2>published by a political scientist named</v>
<v Speaker 2>James Payne on the historical decline of</v>

673
00:35:27.511 --> 00:35:29.640
<v Speaker 2>violence.</v>
<v Speaker 2>And that was one of the inspirations for</v>

674
00:35:29.641 --> 00:35:32.100
<v Speaker 2>the better angels of our nature.</v>
<v Speaker 2>Uh,</v>

675
00:35:32.610 --> 00:35:33.630
<v Speaker 2>the,</v>
<v Speaker 2>um,</v>

676
00:35:33.720 --> 00:35:36.990
<v Speaker 1>what about early on back when you're </v>
<v Speaker 1>maybe a,</v>

677
00:35:38.010 --> 00:35:39.720
<v Speaker 2>is a book called one,</v>
<v Speaker 2>two,</v>

678
00:35:39.721 --> 00:35:42.150
<v Speaker 2>three infinity.</v>
<v Speaker 2>When I was a young adult,</v>

679
00:35:42.270 --> 00:35:45.010
<v Speaker 2>I read that book by George Gamma off the</v>
<v Speaker 2>physicist,</v>

680
00:35:45.570 --> 00:35:50.570
<v Speaker 2>very accessible and humorous </v>
<v Speaker 2>explanations of relativity of a number </v>

681
00:35:51.121 --> 00:35:56.121
<v Speaker 2>theory of a dimentionality high,</v>
<v Speaker 2>multiple dimensional spaces,</v>

682
00:35:58.290 --> 00:35:58.880
<v Speaker 2>uh,</v>
<v Speaker 2>in a,</v>

683
00:35:58.881 --> 00:36:00.720
<v Speaker 2>in a way that I think is still </v>
<v Speaker 2>delightful.</v>

684
00:36:00.900 --> 00:36:05.900
<v Speaker 2>Seventy years after it was published,</v>
<v Speaker 2>I liked the time life science series.</v>

685
00:36:06.210 --> 00:36:11.210
<v Speaker 2>These were books that would arrive every</v>
<v Speaker 2>month that my mother subscribe to each </v>

686
00:36:11.210 --> 00:36:13.440
<v Speaker 2>one on a different topic.</v>
<v Speaker 2>A one would be on,</v>

687
00:36:13.570 --> 00:36:18.570
<v Speaker 2>on electricity.</v>
<v Speaker 2>What would it be on forests wonder </v>

688
00:36:18.570 --> 00:36:21.520
<v Speaker 2>beyond my evolution.</v>
<v Speaker 2>And then one was on the mind and I was </v>

689
00:36:21.520 --> 00:36:24.980
<v Speaker 2>just intrigued that there could be a </v>
<v Speaker 2>science of mind and that that book I </v>

690
00:36:24.980 --> 00:36:27.360
<v Speaker 2>would a site as an influence as well.</v>
<v Speaker 2>Then later on,</v>

691
00:36:27.361 --> 00:36:29.880
<v Speaker 2>you fell in love with the idea of </v>
<v Speaker 2>studying the mind.</v>

692
00:36:29.910 --> 00:36:31.350
<v Speaker 2>It was,</v>
<v Speaker 2>that's one thing that grabbed you.</v>

693
00:36:31.560 --> 00:36:33.990
<v Speaker 2>It was one of the things I would say,</v>
<v Speaker 2>um,</v>

694
00:36:34.020 --> 00:36:34.650
<v Speaker 2>the,</v>
<v Speaker 2>uh,</v>

695
00:36:34.950 --> 00:36:35.700
<v Speaker 2>I read,</v>
<v Speaker 2>uh,</v>

696
00:36:35.710 --> 00:36:40.200
<v Speaker 2>as a college student.</v>
<v Speaker 2>The book reflections on language by Noam</v>

697
00:36:40.200 --> 00:36:42.450
<v Speaker 2>Chomsky spent most of his career here at</v>
<v Speaker 2>Mit.</v>

698
00:36:43.270 --> 00:36:44.730
<v Speaker 2>Uh,</v>
<v Speaker 2>Richard Dawkins,</v>

699
00:36:44.910 --> 00:36:49.910
<v Speaker 2>two books,</v>
<v Speaker 2>the blind watchmaker and the selfish </v>

700
00:36:49.910 --> 00:36:52.050
<v Speaker 2>gene were enormously influential partly </v>
<v Speaker 2>for mainly for the content,</v>

701
00:36:52.051 --> 00:36:57.051
<v Speaker 2>but also for the writing style,</v>
<v Speaker 2>the ability to explain abstract concepts</v>

702
00:36:58.020 --> 00:37:03.020
<v Speaker 2>in lively pros,</v>
<v Speaker 2>Stephen j Dot Gould's first collection </v>

703
00:37:03.020 --> 00:37:06.180
<v Speaker 2>ever since Darwin.</v>
<v Speaker 2>Also a excellent example of,</v>

704
00:37:06.210 --> 00:37:09.180
<v Speaker 2>of our life.</v>
<v Speaker 2>We writing a George Miller,</v>

705
00:37:09.270 --> 00:37:14.270
<v Speaker 2>psychologists that most psychologists </v>
<v Speaker 2>that are familiar with came up with the </v>

706
00:37:14.270 --> 00:37:18.291
<v Speaker 2>idea that human memory has a capacity of</v>
<v Speaker 2>a seven plus or minus two chunks and </v>

707
00:37:18.490 --> 00:37:23.490
<v Speaker 2>supposedly his biggest claim to fame,</v>
<v Speaker 2>but he wrote a couple of books on </v>

708
00:37:23.490 --> 00:37:24.150
<v Speaker 2>language and communication that I had </v>
<v Speaker 2>made as an undergraduate.</v>

709
00:37:24.330 --> 00:37:26.310
<v Speaker 2>Again,</v>
<v Speaker 2>beautifully written and,</v>

710
00:37:26.340 --> 00:37:27.690
<v Speaker 2>uh,</v>
<v Speaker 2>intellectually deep.</v>

711
00:37:28.530 --> 00:37:30.210
<v Speaker 2>Wonderful.</v>
<v Speaker 2>Steven,</v>

712
00:37:30.211 --> 00:37:31.800
<v Speaker 2>thank you so much for taking the time </v>
<v Speaker 2>today.</v>

713
00:37:32.040 --> 00:37:32.970
<v Speaker 2>My pleasure.</v>
<v Speaker 2>Thanks a lot.</v>

