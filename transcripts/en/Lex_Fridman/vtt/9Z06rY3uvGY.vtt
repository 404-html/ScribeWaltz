WEBVTT

1
00:00:00.390 --> 00:00:05.390
<v Speaker 1>Welcome to mit course six zero nine,</v>
<v Speaker 1>nine artificial general intelligence.</v>

2
00:00:06.900 --> 00:00:11.900
<v Speaker 1>Today we have ray Kurzweil.</v>
<v Speaker 1>He is one of the world's leading </v>

3
00:00:11.900 --> 00:00:16.401
<v Speaker 1>inventors,</v>
<v Speaker 1>thinkers and futurists with a 30 year </v>

4
00:00:16.401 --> 00:00:19.341
<v Speaker 1>track record of accurate predictions </v>
<v Speaker 1>called the restless genius by the Wall </v>

5
00:00:19.341 --> 00:00:22.080
<v Speaker 1>Street Journal and the ultimate thinking</v>
<v Speaker 1>machine by Forbes magazine.</v>

6
00:00:22.890 --> 00:00:26.910
<v Speaker 1>He was selected as one of the top </v>
<v Speaker 1>entrepreneurs by ink magazine,</v>

7
00:00:27.060 --> 00:00:30.630
<v Speaker 1>which described him as the rightful heir</v>
<v Speaker 1>to Thomas Edison.</v>

8
00:00:31.470 --> 00:00:35.220
<v Speaker 1>PBS selected him as one of the 16 </v>
<v Speaker 1>revolutionaries who made America.</v>

9
00:00:36.810 --> 00:00:40.980
<v Speaker 1>Ray was the principal investigator of </v>
<v Speaker 1>the first CCD flatbed scanner,</v>

10
00:00:41.010 --> 00:00:44.250
<v Speaker 1>the first Omni font optical character </v>
<v Speaker 1>recognition,</v>

11
00:00:44.251 --> 00:00:46.950
<v Speaker 1>the first point to speech reading </v>
<v Speaker 1>machine for the blind,</v>

12
00:00:46.951 --> 00:00:51.951
<v Speaker 1>the first text to speech synthesizer,</v>
<v Speaker 1>the first music synthesizer capable of </v>

13
00:00:51.951 --> 00:00:54.930
<v Speaker 1>recreating the grand piano and other </v>
<v Speaker 1>orchestra instruments,</v>

14
00:00:55.020 --> 00:00:58.650
<v Speaker 1>and the first commercially marketed </v>
<v Speaker 1>large vocabulary speech recognition.</v>

15
00:00:59.340 --> 00:01:04.340
<v Speaker 1>Among his many honors,</v>
<v Speaker 1>he received a Grammy Award for </v>

16
00:01:04.340 --> 00:01:04.770
<v Speaker 1>outstanding achievements in music </v>
<v Speaker 1>technology.</v>

17
00:01:04.980 --> 00:01:07.560
<v Speaker 1>He's the recipient of the National Medal</v>
<v Speaker 1>of Technology,</v>

18
00:01:07.650 --> 00:01:10.410
<v Speaker 1>was inducted into the national inventors</v>
<v Speaker 1>hall of fame,</v>

19
00:01:10.860 --> 00:01:15.860
<v Speaker 1>holds 21 honorary doctorates,</v>
<v Speaker 1>and honors from three US presidents.</v>

20
00:01:16.890 --> 00:01:21.890
<v Speaker 1>Ray has written five national </v>
<v Speaker 1>bestselling books including the New York</v>

21
00:01:22.440 --> 00:01:27.440
<v Speaker 1>Times bestsellers.</v>
<v Speaker 1>The singularity is near from 2005 and </v>

22
00:01:27.440 --> 00:01:30.981
<v Speaker 1>how to create a mind from 2012.</v>
<v Speaker 1>He's Co founder and Chancellor of </v>

23
00:01:31.381 --> 00:01:35.400
<v Speaker 1>Singularity University and a director of</v>
<v Speaker 1>engineering at Google.</v>

24
00:01:36.210 --> 00:01:41.210
<v Speaker 1>Heading up a team developing machine </v>
<v Speaker 1>intelligence and natural language </v>

25
00:01:41.210 --> 00:01:42.180
<v Speaker 1>understanding.</v>
<v Speaker 1>Please give a warm welcome.</v>

26
00:01:49.840 --> 00:01:54.840
<v Speaker 2>It's good to be back.</v>
<v Speaker 2>I've been in this lecture hall many </v>

27
00:01:54.840 --> 00:01:59.431
<v Speaker 2>times and walked the infinite Carter.</v>
<v Speaker 2>I came here as an undergraduate in 1965.</v>

28
00:02:03.400 --> 00:02:08.050
<v Speaker 2>Within a year of my being here,</v>
<v Speaker 2>they started a new major called computer</v>

29
00:02:08.051 --> 00:02:12.310
<v Speaker 2>science.</v>
<v Speaker 2>It did not get its own course number.</v>

30
00:02:12.530 --> 00:02:17.530
<v Speaker 2>It's six one.</v>
<v Speaker 2>Even biotechnology recently got its own </v>

31
00:02:17.530 --> 00:02:21.100
<v Speaker 2>course number,</v>
<v Speaker 2>but how many of you are cs majors?</v>

32
00:02:22.570 --> 00:02:27.570
<v Speaker 2>Okay.</v>
<v Speaker 2>How many of you do work in deep </v>

33
00:02:27.570 --> 00:02:29.911
<v Speaker 2>learning?</v>
<v Speaker 2>How many of you have heard of deep </v>

34
00:02:29.911 --> 00:02:33.781
<v Speaker 2>learning?</v>
<v Speaker 2>I came here first in 1962 when I was 14.</v>

35
00:02:38.310 --> 00:02:41.520
<v Speaker 2>I became excited about artificial </v>
<v Speaker 2>intelligence.</v>

36
00:02:42.000 --> 00:02:45.450
<v Speaker 2>It it had only gotten its name six years</v>
<v Speaker 2>earlier,</v>

37
00:02:45.451 --> 00:02:50.451
<v Speaker 2>the 1956 dartmouth conference by Marvin </v>
<v Speaker 2>Minsky and John Mccarthy,</v>

38
00:02:52.560 --> 00:02:55.860
<v Speaker 2>so I wrote a Minsky,</v>
<v Speaker 2>a letter.</v>

39
00:02:56.390 --> 00:03:00.490
<v Speaker 2>There was no email back then and he </v>
<v Speaker 2>invited me up.</v>

40
00:03:00.491 --> 00:03:03.040
<v Speaker 2>He spent all day with me,</v>
<v Speaker 2>is if he had nothing else to do,</v>

41
00:03:03.070 --> 00:03:05.530
<v Speaker 2>he was the consummate educator.</v>

42
00:03:07.240 --> 00:03:08.920
<v Speaker 2>I then,</v>
<v Speaker 2>and uh,</v>

43
00:03:09.630 --> 00:03:13.000
<v Speaker 2>the AI field had already bifurcated into</v>
<v Speaker 2>two warring camps,</v>

44
00:03:14.250 --> 00:03:19.090
<v Speaker 2>the symbolic school which Minsky was </v>
<v Speaker 2>associated with a.</v>

45
00:03:19.091 --> 00:03:21.180
<v Speaker 2>and the connectionist school,</v>
<v Speaker 2>uh,</v>

46
00:03:21.540 --> 00:03:23.020
<v Speaker 2>was not widely known.</v>
<v Speaker 2>In fact,</v>

47
00:03:23.021 --> 00:03:28.021
<v Speaker 2>I think it's still not widely known that</v>
<v Speaker 2>minsky actually invented the neural net </v>

48
00:03:28.021 --> 00:03:31.240
<v Speaker 2>in 1953,</v>
<v Speaker 2>but he had become negative about.</v>

49
00:03:31.250 --> 00:03:36.250
<v Speaker 2>It's largely because there's a lot of </v>
<v Speaker 2>hype that these giant brains could solve</v>

50
00:03:36.581 --> 00:03:41.581
<v Speaker 2>any problem.</v>
<v Speaker 2>So the first popular neural nets,</v>

51
00:03:42.431 --> 00:03:47.431
<v Speaker 2>the perceptron was being promulgated by </v>
<v Speaker 2>Frank Rosenblatt at Cornell.</v>

52
00:03:49.270 --> 00:03:50.860
<v Speaker 2>So minsky says,</v>
<v Speaker 2>where are you going now?</v>

53
00:03:50.861 --> 00:03:55.861
<v Speaker 2>And saying,</v>
<v Speaker 2>I said to c Rosenblatt at Cornell is </v>

54
00:03:55.861 --> 00:03:56.200
<v Speaker 2>that,</v>
<v Speaker 2>don't bother doing that.</v>

55
00:03:56.920 --> 00:04:01.920
<v Speaker 2>And I went there and Rosenblatt was </v>
<v Speaker 2>touting the perceptron that had </v>

56
00:04:02.231 --> 00:04:04.720
<v Speaker 2>ultimately would be able to solve any </v>
<v Speaker 2>problem.</v>

57
00:04:05.860 --> 00:04:10.860
<v Speaker 2>So I brought some printed letters that </v>
<v Speaker 2>had the camera and it did a perfect job </v>

58
00:04:10.860 --> 00:04:14.191
<v Speaker 2>of recognizing them as long as they were</v>
<v Speaker 2>carrier 10 different type style.</v>

59
00:04:15.160 --> 00:04:17.230
<v Speaker 2>Didn't work at all.</v>
<v Speaker 2>And he said,</v>

60
00:04:17.231 --> 00:04:20.530
<v Speaker 2>but don't worry,</v>
<v Speaker 2>we can take the output of the perceptron</v>

61
00:04:20.531 --> 00:04:25.531
<v Speaker 2>undefeated as the input to another </v>
<v Speaker 2>perceptron and take the output of that </v>

62
00:04:25.531 --> 00:04:26.680
<v Speaker 2>and feed it to a third layer,</v>
<v Speaker 2>and as we add more layers,</v>

63
00:04:26.681 --> 00:04:28.840
<v Speaker 2>it'll get smarter and smarter and </v>
<v Speaker 2>generalize.</v>

64
00:04:29.500 --> 00:04:31.900
<v Speaker 2>And so that's interesting.</v>
<v Speaker 2>Have you tried that?</v>

65
00:04:31.901 --> 00:04:32.560
<v Speaker 2>Well,</v>
<v Speaker 2>no,</v>

66
00:04:32.561 --> 00:04:37.561
<v Speaker 2>but it's high in our research agenda.</v>
<v Speaker 2>Things did not move quite as quickly </v>

67
00:04:38.170 --> 00:04:41.020
<v Speaker 2>back then as they do now.</v>
<v Speaker 2>He died nine years later.</v>

68
00:04:41.110 --> 00:04:45.970
<v Speaker 2>Never having tried that idea turns out </v>
<v Speaker 2>to be remarkably prescient.</v>

69
00:04:45.971 --> 00:04:50.971
<v Speaker 2>I mean he never tried multilayer neural </v>
<v Speaker 2>nets and all the excitement that we've </v>

70
00:04:50.971 --> 00:04:55.741
<v Speaker 2>seen now about deep learning comes from </v>
<v Speaker 2>a combination of two things,</v>

71
00:05:00.190 --> 00:05:04.450
<v Speaker 2>both many layer neural nets and the law </v>
<v Speaker 2>of accelerating returns,</v>

72
00:05:04.451 --> 00:05:09.451
<v Speaker 2>which I'll get to a little bit later,</v>
<v Speaker 2>which is basically the exponential </v>

73
00:05:09.451 --> 00:05:13.801
<v Speaker 2>growth of computing so that we can run </v>
<v Speaker 2>these massive nets and handle massive </v>

74
00:05:14.021 --> 00:05:14.980
<v Speaker 2>amounts of data.</v>

75
00:05:17.530 --> 00:05:22.530
<v Speaker 2>It would be decades before that idea was</v>
<v Speaker 2>tried several decades later,</v>

76
00:05:23.091 --> 00:05:25.930
<v Speaker 2>the three level neural nets were tried.</v>
<v Speaker 2>There were a little bit better.</v>

77
00:05:25.931 --> 00:05:30.250
<v Speaker 2>They could deal with multiple types.</v>
<v Speaker 2>Styles still weren't very flexible.</v>

78
00:05:31.250 --> 00:05:35.530
<v Speaker 2>That's not hard to add other layers.</v>
<v Speaker 2>It's a very straightforward concept.</v>

79
00:05:35.970 --> 00:05:37.600
<v Speaker 2>Uh,</v>
<v Speaker 2>there was a math problem,</v>

80
00:05:38.170 --> 00:05:43.170
<v Speaker 2>a,</v>
<v Speaker 2>the disappearing gradient or the </v>

81
00:05:43.170 --> 00:05:45.331
<v Speaker 2>exploding gradient,</v>
<v Speaker 2>which I'm sure many of you are familiar </v>

82
00:05:45.331 --> 00:05:45.760
<v Speaker 2>with.</v>
<v Speaker 2>Basically,</v>

83
00:05:45.761 --> 00:05:50.761
<v Speaker 2>you need to take maximum advantage of </v>
<v Speaker 2>the range of values,</v>

84
00:05:53.350 --> 00:05:55.390
<v Speaker 2>uh,</v>
<v Speaker 2>in the gradients,</v>

85
00:05:55.810 --> 00:05:58.070
<v Speaker 2>uh,</v>
<v Speaker 2>not let them disappear.</v>

86
00:05:58.071 --> 00:06:01.120
<v Speaker 2>And a loser resolution,</v>
<v Speaker 2>a,</v>

87
00:06:01.130 --> 00:06:04.310
<v Speaker 2>that's a fairly straightforward </v>
<v Speaker 2>mathematical transformation.</v>

88
00:06:04.760 --> 00:06:08.420
<v Speaker 2>With that insight,</v>
<v Speaker 2>we could now go to 100 layer neural nets</v>

89
00:06:09.450 --> 00:06:14.450
<v Speaker 2>and s that's behind sort of all the </v>
<v Speaker 2>fantastic gains that we've seen </v>

90
00:06:15.201 --> 00:06:18.830
<v Speaker 2>recently.</v>
<v Speaker 2>A alpha go,</v>

91
00:06:18.831 --> 00:06:23.831
<v Speaker 2>a trained on every online game and then </v>
<v Speaker 2>became a fair go player and then trained</v>

92
00:06:28.701 --> 00:06:33.701
<v Speaker 2>itself by playing itself and soared past</v>
<v Speaker 2>the best human alphago.</v>

93
00:06:33.860 --> 00:06:38.860
<v Speaker 2>Zero started with no human input at all,</v>
<v Speaker 2>within hours of iteration,</v>

94
00:06:40.610 --> 00:06:43.440
<v Speaker 2>soared past Alphago.</v>
<v Speaker 2>Uh,</v>

95
00:06:43.460 --> 00:06:48.460
<v Speaker 2>also sort past the best chest programs </v>
<v Speaker 2>that had another innovation.</v>

96
00:06:49.350 --> 00:06:54.350
<v Speaker 2>Uh,</v>
<v Speaker 2>basically you need to evaluate the </v>

97
00:06:54.350 --> 00:06:54.900
<v Speaker 2>quality of the board at each point.</v>
<v Speaker 2>And they used a,</v>

98
00:06:55.370 --> 00:06:59.720
<v Speaker 2>another 100 layer neural nets to do that</v>
<v Speaker 2>evaluation.</v>

99
00:07:00.320 --> 00:07:05.320
<v Speaker 2>Um,</v>
<v Speaker 2>so there's still a problem in the field,</v>

100
00:07:06.590 --> 00:07:11.590
<v Speaker 2>uh,</v>
<v Speaker 2>which is there's a motto that life </v>

101
00:07:11.590 --> 00:07:14.531
<v Speaker 2>begins at a billion examples.</v>
<v Speaker 2>One of the reasons I'm at Google is we </v>

102
00:07:14.531 --> 00:07:16.730
<v Speaker 2>have a billion examples,</v>
<v Speaker 2>for example,</v>

103
00:07:16.731 --> 00:07:20.120
<v Speaker 2>does have pictures of dogs and cats that</v>
<v Speaker 2>are labeled.</v>

104
00:07:20.350 --> 00:07:25.350
<v Speaker 2>So you've got a picture of a cat and it </v>
<v Speaker 2>says cat and then you can learn from it </v>

105
00:07:25.350 --> 00:07:28.361
<v Speaker 2>and you need a lot of them,</v>
<v Speaker 2>a Alphago trained on a million online </v>

106
00:07:28.760 --> 00:07:33.760
<v Speaker 2>moves.</v>
<v Speaker 2>That's how many we had of master games </v>

107
00:07:33.760 --> 00:07:37.690
<v Speaker 2>and that only created a,</v>
<v Speaker 2>a sort of fair go player.</v>

108
00:07:37.700 --> 00:07:42.700
<v Speaker 2>A good amateur could defeat it.</v>
<v Speaker 2>So they worked around that in the case </v>

109
00:07:43.251 --> 00:07:48.251
<v Speaker 2>of goal by basically generating an </v>
<v Speaker 2>infinite amount of data by having the </v>

110
00:07:49.581 --> 00:07:52.900
<v Speaker 2>system play itself,</v>
<v Speaker 2>uh,</v>

111
00:07:52.940 --> 00:07:57.940
<v Speaker 2>had a chat with Dennis Hassabis.</v>
<v Speaker 2>What kind of situations can you do that </v>

112
00:07:57.940 --> 00:08:02.501
<v Speaker 2>with?</v>
<v Speaker 2>You have to have some way of simulating </v>

113
00:08:02.501 --> 00:08:05.591
<v Speaker 2>the world.</v>
<v Speaker 2>So go or chess or even though go is </v>

114
00:08:05.591 --> 00:08:07.310
<v Speaker 2>considered a difficult game,</v>
<v Speaker 2>it's a,</v>

115
00:08:07.580 --> 00:08:12.580
<v Speaker 2>you know,</v>
<v Speaker 2>the definition of it is exists on one </v>

116
00:08:12.580 --> 00:08:12.580
<v Speaker 2>page,</v>
<v Speaker 2>uh,</v>

117
00:08:12.580 --> 00:08:13.630
<v Speaker 2>so you can simulate it.</v>
<v Speaker 2>A,</v>

118
00:08:13.640 --> 00:08:15.480
<v Speaker 2>that applies to math.</v>
<v Speaker 2>I mean,</v>

119
00:08:15.660 --> 00:08:19.400
<v Speaker 2>mass axioms are going to be contained on</v>
<v Speaker 2>a page or two.</v>

120
00:08:19.820 --> 00:08:22.250
<v Speaker 2>It's not very complicated.</v>
<v Speaker 2>Uh,</v>

121
00:08:22.251 --> 00:08:27.140
<v Speaker 2>it gets more difficult when you have </v>
<v Speaker 2>real life situations like biology,</v>

122
00:08:27.620 --> 00:08:30.920
<v Speaker 2>so we have biological simulators,</v>
<v Speaker 2>but the simulators on perfect,</v>

123
00:08:30.921 --> 00:08:35.390
<v Speaker 2>so learning from the simulators will </v>
<v Speaker 2>only be as good as the simulators.</v>

124
00:08:36.410 --> 00:08:40.250
<v Speaker 2>That's actually the key to being able to</v>
<v Speaker 2>do deep learning on biology.</v>

125
00:08:42.560 --> 00:08:45.770
<v Speaker 2>Autonomous Vehicles.</v>
<v Speaker 2>You need real life data.</v>

126
00:08:46.240 --> 00:08:50.240
<v Speaker 2>Uh,</v>
<v Speaker 2>so the waymo systems have gone three and</v>

127
00:08:50.241 --> 00:08:52.280
<v Speaker 2>a half million miles.</v>
<v Speaker 2>Uh,</v>

128
00:08:52.281 --> 00:08:54.860
<v Speaker 2>that's good.</v>
<v Speaker 2>That's enough data to then create a very</v>

129
00:08:54.861 --> 00:08:59.861
<v Speaker 2>good simulator.</v>
<v Speaker 2>The simulator is really quite realistic </v>

130
00:08:59.861 --> 00:09:02.550
<v Speaker 2>because they had a lot of real world </v>
<v Speaker 2>experience and the,</v>

131
00:09:02.580 --> 00:09:05.100
<v Speaker 2>they've got a billion miles in the </v>
<v Speaker 2>simulator,</v>

132
00:09:06.060 --> 00:09:11.060
<v Speaker 2>but we don't always have that </v>
<v Speaker 2>opportunity to either create the data or</v>

133
00:09:11.101 --> 00:09:16.101
<v Speaker 2>have the data around humans can learn </v>
<v Speaker 2>from a small number of examples.</v>

134
00:09:18.150 --> 00:09:20.160
<v Speaker 2>Uh,</v>
<v Speaker 2>your significant other,</v>

135
00:09:20.161 --> 00:09:21.540
<v Speaker 2>your professor,</v>
<v Speaker 2>your boss,</v>

136
00:09:21.541 --> 00:09:26.541
<v Speaker 2>your investor a can tell you something </v>
<v Speaker 2>once or twice and you might actually </v>

137
00:09:26.541 --> 00:09:31.221
<v Speaker 2>learn from that.</v>
<v Speaker 2>Some humans have been reported to do </v>

138
00:09:31.221 --> 00:09:31.221
<v Speaker 2>that.</v>
<v Speaker 2>And,</v>

139
00:09:31.510 --> 00:09:31.890
<v Speaker 2>uh,</v>
<v Speaker 2>that's,</v>

140
00:09:32.020 --> 00:09:37.020
<v Speaker 2>that's,</v>
<v Speaker 2>that's kind of the remaining advantage </v>

141
00:09:37.020 --> 00:09:37.020
<v Speaker 2>of humans.</v>
<v Speaker 2>Now.</v>

142
00:09:37.020 --> 00:09:39.540
<v Speaker 2>There's actually no backpropagation and </v>
<v Speaker 2>the human brain,</v>

143
00:09:39.570 --> 00:09:44.570
<v Speaker 2>it doesn't use deep learning.</v>
<v Speaker 2>It uses a different architecture that </v>

144
00:09:45.330 --> 00:09:48.600
<v Speaker 2>same year in 1962 or older paper,</v>
<v Speaker 2>how I thought the human brain worked.</v>

145
00:09:48.920 --> 00:09:53.920
<v Speaker 2>Uh,</v>
<v Speaker 2>there was actually very little </v>

146
00:09:53.920 --> 00:09:54.250
<v Speaker 2>neuroscience to go on.</v>
<v Speaker 2>There was one neuro science test,</v>

147
00:09:54.270 --> 00:09:56.880
<v Speaker 2>Vernon mountcastle.</v>
<v Speaker 2>It had something relevant to say,</v>

148
00:09:57.860 --> 00:09:59.430
<v Speaker 2>which he did.</v>
<v Speaker 2>I mean,</v>

149
00:09:59.431 --> 00:10:01.770
<v Speaker 2>there was a,</v>
<v Speaker 2>the common wisdom at the time,</v>

150
00:10:01.771 --> 00:10:04.020
<v Speaker 2>and there's still a lot of neuroscience </v>
<v Speaker 2>instead say this.</v>

151
00:10:04.021 --> 00:10:06.300
<v Speaker 2>So we have all these different regions </v>
<v Speaker 2>of the brain.</v>

152
00:10:06.301 --> 00:10:11.301
<v Speaker 2>They do different things.</v>
<v Speaker 2>They must be different than v one in the</v>

153
00:10:11.461 --> 00:10:16.461
<v Speaker 2>back of the head where the optic nerve </v>
<v Speaker 2>skills into that can tell that that's a </v>

154
00:10:16.680 --> 00:10:19.650
<v Speaker 2>curved line.</v>
<v Speaker 2>That's a straight line.</v>

155
00:10:20.100 --> 00:10:23.640
<v Speaker 2>Does a simple feature extractions on </v>
<v Speaker 2>visual images.</v>

156
00:10:23.760 --> 00:10:28.760
<v Speaker 2>It's actually a large part of the </v>
<v Speaker 2>neocortex does a cruciform gyrus up </v>

157
00:10:28.760 --> 00:10:30.270
<v Speaker 2>here,</v>
<v Speaker 2>which can recognize faces.</v>

158
00:10:30.660 --> 00:10:35.660
<v Speaker 2>Uh,</v>
<v Speaker 2>we know that because if it gets knocked </v>

159
00:10:35.660 --> 00:10:36.210
<v Speaker 2>out through injury or stroke,</v>
<v Speaker 2>people can't recognize faces.</v>

160
00:10:36.450 --> 00:10:40.080
<v Speaker 2>They will learn it again with a </v>
<v Speaker 2>different region of the neocortex is the</v>

161
00:10:40.081 --> 00:10:45.081
<v Speaker 2>famous frontal Cortex,</v>
<v Speaker 2>which does language and poetry and </v>

162
00:10:45.081 --> 00:10:45.630
<v Speaker 2>music.</v>

163
00:10:46.030 --> 00:10:51.030
<v Speaker 2>Uh,</v>
<v Speaker 2>so these must work on different </v>

164
00:10:51.030 --> 00:10:53.061
<v Speaker 2>principles.</v>
<v Speaker 2>He did autopsies on the neocortex and </v>

165
00:10:53.061 --> 00:10:53.730
<v Speaker 2>all these different regions and found </v>
<v Speaker 2>they all look the same.</v>

166
00:10:53.731 --> 00:10:57.090
<v Speaker 2>They had the same repeating pattern,</v>
<v Speaker 2>same interconnections.</v>

167
00:10:57.390 --> 00:10:59.760
<v Speaker 2>Uh,</v>
<v Speaker 2>he said Neocortex is neocortex.</v>

168
00:10:59.761 --> 00:11:04.761
<v Speaker 2>So I had that hint.</v>
<v Speaker 2>Otherwise I could actually observe human</v>

169
00:11:04.801 --> 00:11:07.440
<v Speaker 2>brains and action,</v>
<v Speaker 2>which I did from time to time.</v>

170
00:11:07.920 --> 00:11:10.860
<v Speaker 2>And there's a lot of hints that you can </v>
<v Speaker 2>get that way.</v>

171
00:11:11.070 --> 00:11:13.680
<v Speaker 2>For example,</v>
<v Speaker 2>if I ask you to recite the alphabet,</v>

172
00:11:14.070 --> 00:11:18.660
<v Speaker 2>you actually don't do it from a to z,</v>
<v Speaker 2>you do it as a sequence of sequences a,</v>

173
00:11:18.661 --> 00:11:19.090
<v Speaker 2>b,</v>
<v Speaker 2>c,</v>

174
00:11:19.110 --> 00:11:19.920
<v Speaker 2>d,</v>
<v Speaker 2>e,</v>

175
00:11:19.921 --> 00:11:20.700
<v Speaker 2>f,</v>
<v Speaker 2>g,</v>

176
00:11:20.940 --> 00:11:21.360
<v Speaker 2>H,</v>
<v Speaker 2>I,</v>

177
00:11:21.361 --> 00:11:22.110
<v Speaker 2>j,</v>
<v Speaker 2>k,</v>

178
00:11:22.340 --> 00:11:27.340
<v Speaker 2>two.</v>
<v Speaker 2>We learn things as forward sequences of </v>

179
00:11:27.340 --> 00:11:29.980
<v Speaker 2>sequences forward because if I ask you </v>
<v Speaker 2>to recite the alphabet backwards,</v>

180
00:11:30.330 --> 00:11:33.270
<v Speaker 2>you can't do it unless you learn that as</v>
<v Speaker 2>a new sequence.</v>

181
00:11:33.540 --> 00:11:37.650
<v Speaker 2>So these are all interesting.</v>
<v Speaker 2>Hence I wrote a paper that I,</v>

182
00:11:37.800 --> 00:11:42.630
<v Speaker 2>that the Neocortex is organized as a </v>
<v Speaker 2>hierarchy of modules and each module can</v>

183
00:11:42.631 --> 00:11:47.631
<v Speaker 2>learn to simple pattern and that's how I</v>
<v Speaker 2>got to meet President Johnson.</v>

184
00:11:48.570 --> 00:11:53.570
<v Speaker 2>And that initiated a half century of </v>
<v Speaker 2>thinking about issue.</v>

185
00:11:54.370 --> 00:11:57.640
<v Speaker 2>I came to mit to study with Marvin </v>
<v Speaker 2>Minsky.</v>

186
00:11:58.030 --> 00:11:59.350
<v Speaker 2>Actually,</v>
<v Speaker 2>I came for two reasons.</v>

187
00:11:59.360 --> 00:12:04.030
<v Speaker 2>Once Minsky became my mentor,</v>
<v Speaker 2>which was the mentorship that lasted for</v>

188
00:12:04.031 --> 00:12:09.031
<v Speaker 2>over 50 years,</v>
<v Speaker 2>the fact that mit was so advanced and </v>

189
00:12:09.031 --> 00:12:12.841
<v Speaker 2>actually had a computer,</v>
<v Speaker 2>which the other colleges I considered </v>

190
00:12:12.841 --> 00:12:15.450
<v Speaker 2>didn't have a.</v>
<v Speaker 2>It was an IBM 70,</v>

191
00:12:15.470 --> 00:12:19.600
<v Speaker 2>94,</v>
<v Speaker 2>32 K of 36 fit words.</v>

192
00:12:19.601 --> 00:12:24.601
<v Speaker 2>So it's 150 k of core storage to </v>
<v Speaker 2>microsecond cycle time to cycles for </v>

193
00:12:24.821 --> 00:12:29.350
<v Speaker 2>instruction.</v>
<v Speaker 2>So a quarter of a map and that thousands</v>

194
00:12:29.351 --> 00:12:34.060
<v Speaker 2>of students and professors share that </v>
<v Speaker 2>one machine in 2012.</v>

195
00:12:34.061 --> 00:12:39.061
<v Speaker 2>I wrote a book about this thesis is now </v>
<v Speaker 2>actually an explosion of neuroscience </v>

196
00:12:39.221 --> 00:12:44.221
<v Speaker 2>evidence to support it.</v>
<v Speaker 2>The European brain reverse engineering </v>

197
00:12:44.221 --> 00:12:47.380
<v Speaker 2>project has identified or repeating </v>
<v Speaker 2>module of about 100 neurons,</v>

198
00:12:47.530 --> 00:12:51.880
<v Speaker 2>repeated it $300 million times.</v>
<v Speaker 2>So it's about $30 billion neurons in the</v>

199
00:12:51.881 --> 00:12:56.881
<v Speaker 2>NEOCORTEX.</v>
<v Speaker 2>The neocortex is the outer layer of the </v>

200
00:12:56.881 --> 00:12:56.881
<v Speaker 2>brain.</v>

201
00:12:56.881 --> 00:12:58.740
<v Speaker 2>That's part where we do our thinking,</v>
<v Speaker 2>uh,</v>

202
00:12:58.741 --> 00:13:02.590
<v Speaker 2>and they can see in each module,</v>
<v Speaker 2>axons coming in from another,</v>

203
00:13:02.930 --> 00:13:05.970
<v Speaker 2>a module,</v>
<v Speaker 2>and then the output acts,</v>

204
00:13:05.980 --> 00:13:10.980
<v Speaker 2>the single output accident of that </v>
<v Speaker 2>module goes as the input to another </v>

205
00:13:10.980 --> 00:13:14.791
<v Speaker 2>module.</v>
<v Speaker 2>So we can see it organized as a </v>

206
00:13:14.791 --> 00:13:15.190
<v Speaker 2>hierarchy.</v>
<v Speaker 2>It's not a physical hierarchy.</v>

207
00:13:16.240 --> 00:13:18.250
<v Speaker 2>The hierarchy comes from these </v>
<v Speaker 2>connections.</v>

208
00:13:18.580 --> 00:13:22.420
<v Speaker 2>The neocortex is a very thin structure.</v>
<v Speaker 2>It's actually one module thick,</v>

209
00:13:22.870 --> 00:13:27.820
<v Speaker 2>the six layers of neurons,</v>
<v Speaker 2>but it constitutes one module.</v>

210
00:13:27.970 --> 00:13:32.970
<v Speaker 2>And we can see that it learns and simple</v>
<v Speaker 2>pattern and various reasons I cite in </v>

211
00:13:33.791 --> 00:13:38.791
<v Speaker 2>the book the pattern recognition model </v>
<v Speaker 2>that's using is basically a hidden </v>

212
00:13:38.791 --> 00:13:43.741
<v Speaker 2>Markov model.</v>
<v Speaker 2>How many of you have worked with mark </v>

213
00:13:43.741 --> 00:13:46.981
<v Speaker 2>off models?</v>
<v Speaker 2>And that's usually no hands go up when I</v>

214
00:13:48.790 --> 00:13:50.450
<v Speaker 2>asked that question.</v>
<v Speaker 2>Um,</v>

215
00:13:51.160 --> 00:13:54.610
<v Speaker 2>but a Markov model is not.</v>
<v Speaker 2>It is learned a,</v>

216
00:13:54.611 --> 00:13:56.470
<v Speaker 2>but it's not backpropagation.</v>

217
00:13:56.680 --> 00:14:00.430
<v Speaker 2>It can learn local features.</v>
<v Speaker 2>So it's very good for speech recognition</v>

218
00:14:00.431 --> 00:14:01.810
<v Speaker 2>and speech recognition.</v>
<v Speaker 2>I work,</v>

219
00:14:01.811 --> 00:14:06.811
<v Speaker 2>I did in the eighties,</v>
<v Speaker 2>used these mark models that became the </v>

220
00:14:06.811 --> 00:14:11.230
<v Speaker 2>standard approach because it can deal </v>
<v Speaker 2>with local variations.</v>

221
00:14:11.470 --> 00:14:14.500
<v Speaker 2>So the fact that a vow will is </v>
<v Speaker 2>stretched.</v>

222
00:14:15.020 --> 00:14:20.020
<v Speaker 2>It can learn that in a Markov model,</v>
<v Speaker 2>it doesn't learn long distance </v>

223
00:14:20.591 --> 00:14:25.591
<v Speaker 2>relationships that's handled by the </v>
<v Speaker 2>hierarchy and something we don't fully </v>

224
00:14:25.591 --> 00:14:29.650
<v Speaker 2>understand yet is exactly how the </v>
<v Speaker 2>neocortex creates that hierarchy.</v>

225
00:14:30.370 --> 00:14:35.170
<v Speaker 2>But we have figured out how I can </v>
<v Speaker 2>connect this module two,</v>

226
00:14:35.171 --> 00:14:37.000
<v Speaker 2>this module,</v>
<v Speaker 2>does it then grow?</v>

227
00:14:37.270 --> 00:14:42.270
<v Speaker 2>I mean,</v>
<v Speaker 2>there's no virtual communication or </v>

228
00:14:42.270 --> 00:14:42.880
<v Speaker 2>wireless communication.</v>
<v Speaker 2>It's actually a connection.</v>

229
00:14:42.881 --> 00:14:47.881
<v Speaker 2>So does it grow and Axon in are from one</v>
<v Speaker 2>place to another which could be inches </v>

230
00:14:48.580 --> 00:14:49.930
<v Speaker 2>apart.</v>
<v Speaker 2>Uh,</v>

231
00:14:49.931 --> 00:14:54.931
<v Speaker 2>actually they all,</v>
<v Speaker 2>all these connections are there from </v>

232
00:14:54.931 --> 00:14:54.931
<v Speaker 2>birth,</v>
<v Speaker 2>uh,</v>

233
00:14:54.931 --> 00:14:57.230
<v Speaker 2>like the streets and avenues of </v>
<v Speaker 2>Manhattan,</v>

234
00:14:57.620 --> 00:14:59.540
<v Speaker 2>this vertical and horizontal connection.</v>

235
00:14:59.540 --> 00:15:04.540
<v Speaker 2>So if the decides and how it makes that </v>
<v Speaker 2>decision is still not fully understood </v>

236
00:15:05.660 --> 00:15:07.220
<v Speaker 2>that it wants to connect this module </v>
<v Speaker 2>two,</v>

237
00:15:07.221 --> 00:15:09.260
<v Speaker 2>this module,</v>
<v Speaker 2>there's already a vertical,</v>

238
00:15:09.530 --> 00:15:12.470
<v Speaker 2>horizontal and a vertical connection.</v>
<v Speaker 2>It just activates them.</v>

239
00:15:13.620 --> 00:15:18.620
<v Speaker 2>We can actually see that now and could </v>
<v Speaker 2>see that happening in real time on </v>

240
00:15:18.620 --> 00:15:21.180
<v Speaker 2>noninvasive brain scans.</v>
<v Speaker 2>Uh,</v>

241
00:15:21.320 --> 00:15:26.320
<v Speaker 2>so this utterance amount of evidence </v>
<v Speaker 2>that in fact the neocortex is a </v>

242
00:15:26.320 --> 00:15:30.761
<v Speaker 2>hierarchy of modules that can learn.</v>
<v Speaker 2>Each module learns so simple sequential </v>

243
00:15:33.680 --> 00:15:38.000
<v Speaker 2>pattern.</v>
<v Speaker 2>And even though the patterns we perceive</v>

244
00:15:38.030 --> 00:15:43.030
<v Speaker 2>don't seem like sequences,</v>
<v Speaker 2>they may seem three dimensional or even </v>

245
00:15:43.030 --> 00:15:44.420
<v Speaker 2>more complicated.</v>
<v Speaker 2>They are in fact represented,</v>

246
00:15:44.750 --> 00:15:46.160
<v Speaker 2>uh,</v>
<v Speaker 2>as sequences.</v>

247
00:15:46.161 --> 00:15:48.380
<v Speaker 2>But the complexity comes in with the </v>
<v Speaker 2>hierarchy.</v>

248
00:15:49.430 --> 00:15:53.450
<v Speaker 2>So the NEOCORTEX emerged 200 million </v>
<v Speaker 2>years ago,</v>

249
00:15:53.950 --> 00:15:55.160
<v Speaker 2>uh,</v>
<v Speaker 2>with mammals.</v>

250
00:15:55.161 --> 00:16:00.161
<v Speaker 2>All mammals have a neocortex.</v>
<v Speaker 2>It's one of the distinguishing features </v>

251
00:16:00.161 --> 00:16:02.660
<v Speaker 2>of mammals.</v>
<v Speaker 2>These first mammals were small.</v>

252
00:16:02.690 --> 00:16:07.190
<v Speaker 2>They were rodents where they were </v>
<v Speaker 2>capable of a new type of thinking.</v>

253
00:16:07.670 --> 00:16:12.670
<v Speaker 2>Uh,</v>
<v Speaker 2>other nonmelanoma animals had fixed </v>

254
00:16:12.670 --> 00:16:13.070
<v Speaker 2>behaviors,</v>
<v Speaker 2>but those fixed behaviors were very well</v>

255
00:16:13.460 --> 00:16:18.460
<v Speaker 2>adapted for their ecological niche.</v>
<v Speaker 2>But these new mammals could invent a new</v>

256
00:16:19.281 --> 00:16:24.281
<v Speaker 2>behavior.</v>
<v Speaker 2>So creativity and innovation was one </v>

257
00:16:24.281 --> 00:16:25.320
<v Speaker 2>feature of the NEOCORTEX.</v>
<v Speaker 2>So analysis,</v>

258
00:16:25.450 --> 00:16:27.470
<v Speaker 2>escaping a predator,</v>
<v Speaker 2>it's usual.</v>

259
00:16:27.471 --> 00:16:32.471
<v Speaker 2>Escape Path is blocked.</v>
<v Speaker 2>It will invent a new behavior to deal </v>

260
00:16:32.471 --> 00:16:33.500
<v Speaker 2>with.</v>
<v Speaker 2>It probably wouldn't work,</v>

261
00:16:33.501 --> 00:16:35.120
<v Speaker 2>but if it did work,</v>
<v Speaker 2>it would remember it.</v>

262
00:16:35.121 --> 00:16:40.121
<v Speaker 2>And we'd have a new behavior and that </v>
<v Speaker 2>behavior could spread virally through </v>

263
00:16:40.121 --> 00:16:43.841
<v Speaker 2>the community.</v>
<v Speaker 2>Another mouse watching this say to </v>

264
00:16:43.841 --> 00:16:44.660
<v Speaker 2>itself,</v>
<v Speaker 2>that was really clever going around that</v>

265
00:16:44.661 --> 00:16:46.970
<v Speaker 2>rock,</v>
<v Speaker 2>I'm going to remember to do that.</v>

266
00:16:47.390 --> 00:16:50.300
<v Speaker 2>Uh,</v>
<v Speaker 2>and it would have a new behavior.</v>

267
00:16:51.200 --> 00:16:54.950
<v Speaker 2>Didn't help these early mammals that </v>
<v Speaker 2>much because as I say,</v>

268
00:16:55.010 --> 00:17:00.010
<v Speaker 2>the Mammalian and animals who were very </v>
<v Speaker 2>well adapted to their niches and nothing</v>

269
00:17:02.211 --> 00:17:07.211
<v Speaker 2>much happened for $135 million years,</v>
<v Speaker 2>but then 65 million years ago something </v>

270
00:17:08.541 --> 00:17:11.750
<v Speaker 2>did happened,</v>
<v Speaker 2>there was a sudden violent change to the</v>

271
00:17:11.751 --> 00:17:12.470
<v Speaker 2>environment.</v>

272
00:17:12.890 --> 00:17:15.380
<v Speaker 2>We now call it the cretaceous extinction</v>
<v Speaker 2>event.</v>

273
00:17:15.950 --> 00:17:19.790
<v Speaker 2>There's been debate as to whether it was</v>
<v Speaker 2>a immediate or an asteroid.</v>

274
00:17:20.120 --> 00:17:25.120
<v Speaker 2>I mean a media or a volcanic eruption.</v>
<v Speaker 2>A the asteroid or media or hypothesis is</v>

275
00:17:27.651 --> 00:17:32.651
<v Speaker 2>in the ascendancy.</v>
<v Speaker 2>But if you dig down to an area of rock </v>

276
00:17:32.651 --> 00:17:37.010
<v Speaker 2>reflecting 65 million years ago,</v>
<v Speaker 2>the geologist will explain that it shows</v>

277
00:17:37.011 --> 00:17:39.800
<v Speaker 2>a very violent sudden change to the </v>
<v Speaker 2>environment.</v>

278
00:17:39.801 --> 00:17:44.180
<v Speaker 2>And we see it all around the globe.</v>
<v Speaker 2>So it was a worldwide phenomenon.</v>

279
00:17:44.750 --> 00:17:49.750
<v Speaker 2>The reason we call it an extinction </v>
<v Speaker 2>event is that's when the dinosaurs went </v>

280
00:17:49.831 --> 00:17:51.210
<v Speaker 2>extinct.</v>
<v Speaker 2>Uh,</v>

281
00:17:51.240 --> 00:17:56.220
<v Speaker 2>that's when 75 percent of all the animal</v>
<v Speaker 2>and plant species went extinct.</v>

282
00:17:56.580 --> 00:17:59.640
<v Speaker 2>And that's when mammals overtook their </v>
<v Speaker 2>ecological niche.</v>

283
00:18:00.270 --> 00:18:04.200
<v Speaker 2>So to anthropomorphize,</v>
<v Speaker 2>biological evolution said to itself,</v>

284
00:18:04.590 --> 00:18:06.390
<v Speaker 2>hmm,</v>
<v Speaker 2>this neocortex is pretty good stuff.</v>

285
00:18:06.391 --> 00:18:09.450
<v Speaker 2>And it began to grow it.</v>
<v Speaker 2>So now mammals got bigger,</v>

286
00:18:09.660 --> 00:18:11.970
<v Speaker 2>their brains got bigger at an even </v>
<v Speaker 2>faster pace,</v>

287
00:18:11.971 --> 00:18:13.980
<v Speaker 2>taking up a larger fraction of their </v>
<v Speaker 2>body.</v>

288
00:18:14.460 --> 00:18:17.280
<v Speaker 2>The neocortex got bigger even faster </v>
<v Speaker 2>than that.</v>

289
00:18:17.640 --> 00:18:20.310
<v Speaker 2>And developed these curvatures that are </v>
<v Speaker 2>distinctive,</v>

290
00:18:20.340 --> 00:18:25.340
<v Speaker 2>have a primate brain basically to </v>
<v Speaker 2>increase its surface area.</v>

291
00:18:25.501 --> 00:18:27.610
<v Speaker 2>But if you stretched it out,</v>
<v Speaker 2>uh,</v>

292
00:18:27.611 --> 00:18:29.850
<v Speaker 2>the human neocortex is still a flat </v>
<v Speaker 2>structure.</v>

293
00:18:29.851 --> 00:18:33.360
<v Speaker 2>It's about the size of a table,</v>
<v Speaker 2>Napkin justice thin,</v>

294
00:18:33.900 --> 00:18:36.750
<v Speaker 2>uh,</v>
<v Speaker 2>and uh,</v>

295
00:18:37.030 --> 00:18:39.510
<v Speaker 2>it's basically,</v>
<v Speaker 2>uh,</v>

296
00:18:40.050 --> 00:18:45.050
<v Speaker 2>created a primates which became dominant</v>
<v Speaker 2>in their ecological niche.</v>

297
00:18:46.400 --> 00:18:49.550
<v Speaker 2>A then something else happened 2 million</v>
<v Speaker 2>years ago.</v>

298
00:18:49.551 --> 00:18:54.551
<v Speaker 2>And biological evolution decided to </v>
<v Speaker 2>increase the neocortex further and </v>

299
00:18:55.201 --> 00:18:59.850
<v Speaker 2>increase the size of the enclosure and </v>
<v Speaker 2>basically filled up the frontal cortex,</v>

300
00:19:00.110 --> 00:19:01.590
<v Speaker 2>uh,</v>
<v Speaker 2>with our big skulls,</v>

301
00:19:01.740 --> 00:19:05.070
<v Speaker 2>with more neocortex.</v>
<v Speaker 2>And up until recently,</v>

302
00:19:05.071 --> 00:19:06.900
<v Speaker 2>it was felt that,</v>
<v Speaker 2>as I said,</v>

303
00:19:06.901 --> 00:19:09.990
<v Speaker 2>that this was,</v>
<v Speaker 2>the frontal cortex was different because</v>

304
00:19:09.991 --> 00:19:12.300
<v Speaker 2>it does these qualitatively different </v>
<v Speaker 2>things.</v>

305
00:19:13.920 --> 00:19:18.920
<v Speaker 2>But we now realized that it's really </v>
<v Speaker 2>just additional neocortex.</v>

306
00:19:24.390 --> 00:19:29.370
<v Speaker 2>So remember what we did with it.</v>
<v Speaker 2>We were already doing a very good job of</v>

307
00:19:29.371 --> 00:19:34.371
<v Speaker 2>being primates.</v>
<v Speaker 2>So we put it at the top of the </v>

308
00:19:34.371 --> 00:19:36.831
<v Speaker 2>neocortical hierarchy.</v>
<v Speaker 2>And we increased the size of the </v>

309
00:19:36.831 --> 00:19:39.000
<v Speaker 2>hierarchy,</v>
<v Speaker 2>it was maybe 20 percent more neocortex,</v>

310
00:19:39.001 --> 00:19:44.001
<v Speaker 2>but it doubled and tripled the number of</v>
<v Speaker 2>levels because as you go up the </v>

311
00:19:44.001 --> 00:19:44.280
<v Speaker 2>hierarchy,</v>
<v Speaker 2>it's kind of like a pyramid.</v>

312
00:19:44.281 --> 00:19:49.281
<v Speaker 2>There's fewer and fewer modules and that</v>
<v Speaker 2>was the enabling factor for us to invent</v>

313
00:19:51.630 --> 00:19:53.790
<v Speaker 2>language and art.</v>
<v Speaker 2>Music.</v>

314
00:19:53.791 --> 00:19:57.180
<v Speaker 2>Every human culture we've ever </v>
<v Speaker 2>discovered has music.</v>

315
00:19:57.240 --> 00:20:00.750
<v Speaker 2>No primate culture has music.</v>
<v Speaker 2>There's debate about that,</v>

316
00:20:00.751 --> 00:20:02.820
<v Speaker 2>but it's really true.</v>

317
00:20:04.890 --> 00:20:09.890
<v Speaker 2>Invention technology,</v>
<v Speaker 2>a technology required another </v>

318
00:20:09.890 --> 00:20:13.110
<v Speaker 2>evolutionary adaptation,</v>
<v Speaker 2>which is this humble appendage here.</v>

319
00:20:13.620 --> 00:20:15.210
<v Speaker 2>Uh,</v>
<v Speaker 2>no other animal has add.</v>

320
00:20:15.211 --> 00:20:17.400
<v Speaker 2>If you look at a chimpanzee,</v>
<v Speaker 2>it looks like they have a similar hand,</v>

321
00:20:17.401 --> 00:20:22.401
<v Speaker 2>but the thumb is actually down here.</v>
<v Speaker 2>It doesn't work very well if you watch </v>

322
00:20:22.401 --> 00:20:25.461
<v Speaker 2>them trying to grab a stick a so we </v>
<v Speaker 2>could imagine creative solutions.</v>

323
00:20:26.061 --> 00:20:31.061
<v Speaker 2>Yeah.</v>
<v Speaker 2>I could take that branch and strip off </v>

324
00:20:31.061 --> 00:20:33.861
<v Speaker 2>the leaves and put a point on it and we </v>
<v Speaker 2>could actually carry out these ideas and</v>

325
00:20:34.771 --> 00:20:39.771
<v Speaker 2>create tools and then use tools to </v>
<v Speaker 2>create new tools and started a whole </v>

326
00:20:39.771 --> 00:20:42.210
<v Speaker 2>nother evolutionary process of tool </v>
<v Speaker 2>making.</v>

327
00:20:42.690 --> 00:20:45.420
<v Speaker 2>And that all came with the,</v>
<v Speaker 2>with the NEOCORTEX.</v>

328
00:20:47.110 --> 00:20:52.110
<v Speaker 2>So Larry Page read my book a 2012 and </v>
<v Speaker 2>liked it,</v>

329
00:20:52.511 --> 00:20:57.511
<v Speaker 2>so I met with him in essence for an </v>
<v Speaker 2>investment in a company I'd started </v>

330
00:20:57.511 --> 00:21:01.780
<v Speaker 2>actually a couple of weeks earlier to </v>
<v Speaker 2>develop those ideas commercially because</v>

331
00:21:01.781 --> 00:21:06.781
<v Speaker 2>that's how I went about things as a </v>
<v Speaker 2>serial entrepreneur and he said,</v>

332
00:21:07.541 --> 00:21:08.380
<v Speaker 2>well,</v>
<v Speaker 2>we'll invest,</v>

333
00:21:08.381 --> 00:21:09.550
<v Speaker 2>but let me give you a better idea.</v>

334
00:21:09.550 --> 00:21:14.550
<v Speaker 2>Why don't you do it here at Google,</v>
<v Speaker 2>we have a billion pictures of dogs and </v>

335
00:21:14.550 --> 00:21:17.320
<v Speaker 2>cats and we got a lot of other data and </v>
<v Speaker 2>lots of computers and lots of talents,</v>

336
00:21:17.710 --> 00:21:19.810
<v Speaker 2>all of which is true.</v>
<v Speaker 2>And says,</v>

337
00:21:19.811 --> 00:21:20.260
<v Speaker 2>well,</v>
<v Speaker 2>I don't know.</v>

338
00:21:20.261 --> 00:21:22.920
<v Speaker 2>I just started this company,</v>
<v Speaker 2>uh,</v>

339
00:21:23.020 --> 00:21:25.780
<v Speaker 2>to develop this as well by your company </v>
<v Speaker 2>and said,</v>

340
00:21:25.800 --> 00:21:30.800
<v Speaker 2>Hey,</v>
<v Speaker 2>are you going to value a company that </v>

341
00:21:30.800 --> 00:21:32.161
<v Speaker 2>hasn't done anything?</v>
<v Speaker 2>And just started a couple of weeks ago </v>

342
00:21:32.161 --> 00:21:33.250
<v Speaker 2>and he said we can value anything.</v>
<v Speaker 2>Uh,</v>

343
00:21:33.910 --> 00:21:38.910
<v Speaker 2>so I took my first job five years ago </v>
<v Speaker 2>and had been basically applying this </v>

344
00:21:40.511 --> 00:21:43.120
<v Speaker 2>model,</v>
<v Speaker 2>this hierarchical model,</v>

345
00:21:43.510 --> 00:21:46.570
<v Speaker 2>um,</v>
<v Speaker 2>to understanding language,</v>

346
00:21:46.600 --> 00:21:49.930
<v Speaker 2>which I think really is the holy grail </v>
<v Speaker 2>of Ai.</v>

347
00:21:50.440 --> 00:21:55.440
<v Speaker 2>I think touring was correct in </v>
<v Speaker 2>designating basically text communication</v>

348
00:21:58.060 --> 00:22:03.060
<v Speaker 2>as well.</v>
<v Speaker 2>We now call a turing complete problem </v>

349
00:22:03.060 --> 00:22:06.211
<v Speaker 2>that it requires.</v>
<v Speaker 2>There's no simple NLP tricks that you </v>

350
00:22:06.211 --> 00:22:09.691
<v Speaker 2>can apply to pass a valid turing test </v>
<v Speaker 2>with an emphasis on the word valid Mitch</v>

351
00:22:11.350 --> 00:22:11.350
<v Speaker 2>Kapor.</v>

352
00:22:11.350 --> 00:22:16.350
<v Speaker 2>And I had a six month debate on what the</v>
<v Speaker 2>rules should be because if you read </v>

353
00:22:16.350 --> 00:22:18.620
<v Speaker 2>turing's 1950 paper,</v>
<v Speaker 2>uh,</v>

354
00:22:18.730 --> 00:22:23.730
<v Speaker 2>he describes this in a few paragraphs </v>
<v Speaker 2>and doesn't really describe how to go </v>

355
00:22:23.730 --> 00:22:25.120
<v Speaker 2>about it,</v>
<v Speaker 2>but if it's a valid turing test,</v>

356
00:22:25.121 --> 00:22:30.121
<v Speaker 2>meaning it's really convincing you </v>
<v Speaker 2>through into interrogation and dialogue,</v>

357
00:22:31.420 --> 00:22:36.420
<v Speaker 2>uh,</v>
<v Speaker 2>that it's a human that requires a full </v>

358
00:22:36.420 --> 00:22:39.901
<v Speaker 2>range of human intelligence.</v>
<v Speaker 2>And I think that a test says to the test</v>

359
00:22:40.601 --> 00:22:44.230
<v Speaker 2>of time,</v>
<v Speaker 2>we're making very good progress on that.</v>

360
00:22:44.231 --> 00:22:49.231
<v Speaker 2>I mean,</v>
<v Speaker 2>just last week you may have read that </v>

361
00:22:49.231 --> 00:22:51.090
<v Speaker 2>two systems,</v>
<v Speaker 2>uh,</v>

362
00:22:51.310 --> 00:22:55.900
<v Speaker 2>past the paragraph comprehension test.</v>
<v Speaker 2>It's really very impressive.</v>

363
00:22:55.901 --> 00:23:00.901
<v Speaker 2>When I came to Google,</v>
<v Speaker 2>we were trying to pass these paragraph </v>

364
00:23:00.901 --> 00:23:03.490
<v Speaker 2>comprehension tests.</v>
<v Speaker 2>We aced the first,</v>

365
00:23:03.580 --> 00:23:08.580
<v Speaker 2>the first grade test,</v>
<v Speaker 2>second grade tests were Kinda got </v>

366
00:23:08.580 --> 00:23:12.211
<v Speaker 2>average performance.</v>
<v Speaker 2>And the third grade test had too much </v>

367
00:23:12.211 --> 00:23:12.470
<v Speaker 2>inference.</v>
<v Speaker 2>I already,</v>

368
00:23:12.471 --> 00:23:17.471
<v Speaker 2>you had to know some common sense </v>
<v Speaker 2>knowledge is it's called a and make </v>

369
00:23:18.101 --> 00:23:23.101
<v Speaker 2>implications of things that were in </v>
<v Speaker 2>different parts of the paragraph and </v>

370
00:23:23.101 --> 00:23:23.770
<v Speaker 2>there's too much in France and really </v>
<v Speaker 2>didn't,</v>

371
00:23:23.830 --> 00:23:24.340
<v Speaker 2>didn't work.</v>

372
00:23:24.340 --> 00:23:29.340
<v Speaker 2>So this is now an adult level,</v>
<v Speaker 2>just slightly surpassed average human </v>

373
00:23:29.621 --> 00:23:31.450
<v Speaker 2>performance.</v>
<v Speaker 2>Uh,</v>

374
00:23:31.480 --> 00:23:36.480
<v Speaker 2>but we've seen that one something,</v>
<v Speaker 2>an ai does something at average human </v>

375
00:23:36.791 --> 00:23:39.460
<v Speaker 2>levels.</v>
<v Speaker 2>It doesn't take long for it to soar past</v>

376
00:23:40.000 --> 00:23:45.000
<v Speaker 2>a average human levels.</v>
<v Speaker 2>I think it'll take longer in language </v>

377
00:23:45.000 --> 00:23:45.010
<v Speaker 2>and it didn't sort of simple games like </v>
<v Speaker 2>go,</v>

378
00:23:45.680 --> 00:23:50.680
<v Speaker 2>uh,</v>
<v Speaker 2>it's actually very impressive that it </v>

379
00:23:50.680 --> 00:23:53.080
<v Speaker 2>surpasses now average human performance </v>
<v Speaker 2>to use an Lstm long short temporal </v>

380
00:23:54.470 --> 00:23:59.470
<v Speaker 2>memory.</v>
<v Speaker 2>But if you look at the adult test in </v>

381
00:23:59.470 --> 00:24:03.671
<v Speaker 2>order to answer these questions,</v>
<v Speaker 2>it has to put together inferences and </v>

382
00:24:03.671 --> 00:24:06.800
<v Speaker 2>implications of several different things</v>
<v Speaker 2>in the paragraph with some common sense.</v>

383
00:24:06.801 --> 00:24:10.430
<v Speaker 2>Knowledge is not explicitly stated.</v>
<v Speaker 2>So that's,</v>

384
00:24:10.431 --> 00:24:15.431
<v Speaker 2>I think a pretty impressive milestone.</v>
<v Speaker 2>So I've been developing,</v>

385
00:24:16.610 --> 00:24:20.140
<v Speaker 2>I've got a team of about 45 people,</v>
<v Speaker 2>um,</v>

386
00:24:20.600 --> 00:24:23.570
<v Speaker 2>and we've been developing this </v>
<v Speaker 2>hierarchical model.</v>

387
00:24:24.110 --> 00:24:29.110
<v Speaker 2>We don't use Markov models because we </v>
<v Speaker 2>can use deep learning for each module.</v>

388
00:24:30.260 --> 00:24:35.260
<v Speaker 2>And so we create an embedding for each </v>
<v Speaker 2>word and recreate an embedding for each </v>

389
00:24:35.260 --> 00:24:36.110
<v Speaker 2>sentence,</v>
<v Speaker 2>uh,</v>

390
00:24:36.170 --> 00:24:38.720
<v Speaker 2>this week.</v>
<v Speaker 2>Have a can talk about it because we have</v>

391
00:24:38.721 --> 00:24:43.640
<v Speaker 2>a published paper on it.</v>
<v Speaker 2>It can take into consideration context.</v>

392
00:24:45.620 --> 00:24:49.660
<v Speaker 2>If you use smart reply on Ge Fuse,</v>
<v Speaker 2>g mail on your phone,</v>

393
00:24:49.680 --> 00:24:53.120
<v Speaker 2>you'll see it gives you three </v>
<v Speaker 2>suggestions for responses.</v>

394
00:24:53.121 --> 00:24:58.121
<v Speaker 2>That's called smart reply there that are</v>
<v Speaker 2>simple suggestions.</v>

395
00:24:59.031 --> 00:25:02.750
<v Speaker 2>But it has to actually understand </v>
<v Speaker 2>perhaps a complicated email,</v>

396
00:25:03.110 --> 00:25:03.820
<v Speaker 2>uh,</v>
<v Speaker 2>and the,</v>

397
00:25:03.860 --> 00:25:06.290
<v Speaker 2>the quality of the suggestions and </v>
<v Speaker 2>squarely quite good.</v>

398
00:25:06.530 --> 00:25:08.120
<v Speaker 2>Quite on point.</v>
<v Speaker 2>Uh,</v>

399
00:25:08.121 --> 00:25:11.510
<v Speaker 2>that's where my team using this kind of </v>
<v Speaker 2>hierarchical model,</v>

400
00:25:11.850 --> 00:25:13.770
<v Speaker 2>uh,</v>
<v Speaker 2>so instead of mark off models,</v>

401
00:25:13.780 --> 00:25:16.130
<v Speaker 2>it uses embeddings,</v>
<v Speaker 2>uh,</v>

402
00:25:16.640 --> 00:25:20.630
<v Speaker 2>cause we can use backpropagation,</v>
<v Speaker 2>we might as well use it.</v>

403
00:25:21.120 --> 00:25:26.120
<v Speaker 2>Uh,</v>
<v Speaker 2>but I think what's missing from deep </v>

404
00:25:26.120 --> 00:25:29.500
<v Speaker 2>learning is this hierarchical aspect of </v>
<v Speaker 2>understanding because the world is </v>

405
00:25:29.841 --> 00:25:34.841
<v Speaker 2>hierarchical.</v>
<v Speaker 2>That's why a evolution developed a </v>

406
00:25:34.841 --> 00:25:39.071
<v Speaker 2>hierarchical brain structure to </v>
<v Speaker 2>understand the natural hierarchy in the </v>

407
00:25:39.071 --> 00:25:39.071
<v Speaker 2>world.</v>

408
00:25:41.120 --> 00:25:44.810
<v Speaker 2>And there are several problems with big,</v>
<v Speaker 2>deep neural nets.</v>

409
00:25:44.811 --> 00:25:48.560
<v Speaker 2>One is the fact that you really do need </v>
<v Speaker 2>a billion examples and we don't.</v>

410
00:25:48.830 --> 00:25:51.560
<v Speaker 2>Sometimes we can generate them as in the</v>
<v Speaker 2>case of go,</v>

411
00:25:52.050 --> 00:25:54.920
<v Speaker 2>uh,</v>
<v Speaker 2>or if we have a really good simulator as</v>

412
00:25:54.921 --> 00:25:58.610
<v Speaker 2>in the case of autonomous vehicles,</v>
<v Speaker 2>not quite the case yet in biology,</v>

413
00:25:59.120 --> 00:26:02.740
<v Speaker 2>a very often you don't have ability to </v>
<v Speaker 2>example,</v>

414
00:26:02.741 --> 00:26:07.741
<v Speaker 2>if you suddenly have billions of </v>
<v Speaker 2>examples of language but they're not </v>

415
00:26:07.741 --> 00:26:10.961
<v Speaker 2>annotated.</v>
<v Speaker 2>And how would you annotate it anyway </v>

416
00:26:10.961 --> 00:26:10.961
<v Speaker 2>with more language that we can </v>
<v Speaker 2>understand in the first place.</v>

417
00:26:10.961 --> 00:26:13.670
<v Speaker 2>So it's kind of a chicken and an egg </v>
<v Speaker 2>problem.</v>

418
00:26:14.210 --> 00:26:19.210
<v Speaker 2>Uh,</v>
<v Speaker 2>so I believe this hierarchical </v>

419
00:26:19.210 --> 00:26:19.510
<v Speaker 2>structures needed another criticism of </v>
<v Speaker 2>deep neural nets.</v>

420
00:26:19.550 --> 00:26:22.010
<v Speaker 2>They don't explain themselves very well.</v>
<v Speaker 2>It's a big,</v>

421
00:26:22.370 --> 00:26:27.280
<v Speaker 2>a black box that gives you pretty </v>
<v Speaker 2>remarkable answers.</v>

422
00:26:27.320 --> 00:26:32.320
<v Speaker 2>I mean,</v>
<v Speaker 2>in the case of these Games Dennis </v>

423
00:26:32.320 --> 00:26:34.971
<v Speaker 2>described it's playing in both go and </v>
<v Speaker 2>chess is almost an alien intelligence </v>

424
00:26:34.971 --> 00:26:39.521
<v Speaker 2>because it will do things that were </v>
<v Speaker 2>shocking to you and experts like </v>

425
00:26:39.521 --> 00:26:41.090
<v Speaker 2>sacrificing a queen and a bishop at the </v>
<v Speaker 2>same time,</v>

426
00:26:41.600 --> 00:26:46.600
<v Speaker 2>uh,</v>
<v Speaker 2>or in close succession which shocked </v>

427
00:26:46.600 --> 00:26:50.511
<v Speaker 2>everybody,</v>
<v Speaker 2>but then went on win or early in a go </v>

428
00:26:50.511 --> 00:26:53.541
<v Speaker 2>game,</v>
<v Speaker 2>putting a piece at the corner of the </v>

429
00:26:53.541 --> 00:26:56.691
<v Speaker 2>board,</v>
<v Speaker 2>which is kinda crazy to most experts </v>

430
00:26:56.691 --> 00:26:57.360
<v Speaker 2>because you really want to start </v>
<v Speaker 2>controlling territory.</v>

431
00:26:57.690 --> 00:27:01.410
<v Speaker 2>And yet on reflection,</v>
<v Speaker 2>that was the brilliant move that enabled</v>

432
00:27:01.411 --> 00:27:02.700
<v Speaker 2>us to win that game.</v>

433
00:27:05.250 --> 00:27:07.560
<v Speaker 2>But it doesn't really explain how it </v>
<v Speaker 2>does these things.</v>

434
00:27:07.561 --> 00:27:08.060
<v Speaker 2>So if,</v>
<v Speaker 2>yeah,</v>

435
00:27:08.090 --> 00:27:13.090
<v Speaker 2>if you have a hierarchy,</v>
<v Speaker 2>it's much better at explaining it </v>

436
00:27:13.090 --> 00:27:13.830
<v Speaker 2>because you could look at the content of</v>
<v Speaker 2>the,</v>

437
00:27:14.070 --> 00:27:19.070
<v Speaker 2>of the modules in the hierarchy and </v>
<v Speaker 2>they'll explain what they're doing.</v>

438
00:27:20.310 --> 00:27:22.070
<v Speaker 2>And,</v>
<v Speaker 2>uh,</v>

439
00:27:22.320 --> 00:27:27.320
<v Speaker 2>just to end on the first application of </v>
<v Speaker 2>applying this to health and medicine,</v>

440
00:27:28.290 --> 00:27:33.290
<v Speaker 2>this will get into high gear and we're </v>
<v Speaker 2>going to really see a breakout of the </v>

441
00:27:33.290 --> 00:27:36.621
<v Speaker 2>linear extension to longevity that we've</v>
<v Speaker 2>experienced.</v>

442
00:27:37.200 --> 00:27:39.210
<v Speaker 2>A,</v>
<v Speaker 2>I believe we're only about a decade away</v>

443
00:27:39.211 --> 00:27:41.310
<v Speaker 2>from longevity,</v>
<v Speaker 2>escape velocity.</v>

444
00:27:41.311 --> 00:27:46.290
<v Speaker 2>We're adding more time than is going by,</v>
<v Speaker 2>not just to infant life expectancy,</v>

445
00:27:46.291 --> 00:27:50.730
<v Speaker 2>but to your remaining life expectancy.</v>
<v Speaker 2>I think if someone is diligent,</v>

446
00:27:50.731 --> 00:27:54.570
<v Speaker 2>they can be there already.</v>
<v Speaker 2>I think I've at longevity,</v>

447
00:27:54.571 --> 00:27:59.571
<v Speaker 2>escape velocity now a word on what life </v>
<v Speaker 2>expectancy means.</v>

448
00:28:01.050 --> 00:28:03.790
<v Speaker 2>It used to be assumed that not much </v>
<v Speaker 2>would happen.</v>

449
00:28:03.791 --> 00:28:08.791
<v Speaker 2>So whatever your life expectancy is a </v>
<v Speaker 2>with or without scientific progress,</v>

450
00:28:09.781 --> 00:28:12.450
<v Speaker 2>it really didn't matter.</v>
<v Speaker 2>Now it matters a lot.</v>

451
00:28:12.451 --> 00:28:14.700
<v Speaker 2>So life expectancy really means,</v>
<v Speaker 2>you know,</v>

452
00:28:14.730 --> 00:28:19.730
<v Speaker 2>how long would you live,</v>
<v Speaker 2>what's the statistical likelihood if </v>

453
00:28:21.151 --> 00:28:23.910
<v Speaker 2>there were not continued tying antic </v>
<v Speaker 2>progress,</v>

454
00:28:24.270 --> 00:28:29.270
<v Speaker 2>but that's a very inaccurate assumption </v>
<v Speaker 2>that scientific progress is extremely </v>

455
00:28:29.270 --> 00:28:29.370
<v Speaker 2>rapid.</v>
<v Speaker 2>I mean,</v>

456
00:28:29.371 --> 00:28:32.730
<v Speaker 2>just as an ai and biotech,</v>
<v Speaker 2>there are advances now.</v>

457
00:28:33.030 --> 00:28:35.790
<v Speaker 2>Every week is quite stunning.</v>

458
00:28:38.040 --> 00:28:41.190
<v Speaker 2>Now you could have a computed life </v>
<v Speaker 2>expectancy,</v>

459
00:28:41.191 --> 00:28:43.400
<v Speaker 2>let's say 30 years,</v>
<v Speaker 2>50 years,</v>

460
00:28:43.401 --> 00:28:47.880
<v Speaker 2>70 years from now,</v>
<v Speaker 2>you could still be hit by the proverbial</v>

461
00:28:47.881 --> 00:28:52.881
<v Speaker 2>bus tomorrow.</v>
<v Speaker 2>We're working on that with self driving </v>

462
00:28:52.881 --> 00:28:53.300
<v Speaker 2>vehicles.</v>
<v Speaker 2>Um,</v>

463
00:28:54.780 --> 00:28:56.490
<v Speaker 2>but we'll get,</v>
<v Speaker 2>we'll get to a point.</v>

464
00:28:56.540 --> 00:29:01.540
<v Speaker 2>I think if you're diligent,</v>
<v Speaker 2>you can be there now in terms of </v>

465
00:29:01.540 --> 00:29:04.410
<v Speaker 2>basically advancing your own statistical</v>
<v Speaker 2>life expectancy,</v>

466
00:29:06.070 --> 00:29:08.800
<v Speaker 2>at least to keep pace with the passage </v>
<v Speaker 2>of time.</v>

467
00:29:08.801 --> 00:29:13.801
<v Speaker 2>I think it will be there for most of the</v>
<v Speaker 2>population at least if they're diligent </v>

468
00:29:14.820 --> 00:29:18.300
<v Speaker 2>within about a decade.</v>
<v Speaker 2>So if he can hang in there,</v>

469
00:29:19.200 --> 00:29:21.570
<v Speaker 2>we may get to see the remarkable century</v>
<v Speaker 2>ahead.</v>

470
00:29:21.660 --> 00:29:22.530
<v Speaker 2>Thank you very much.</v>

471
00:29:26.010 --> 00:29:27.710
<v Speaker 3>You,</v>
<v Speaker 3>I have a</v>

472
00:29:27.720 --> 00:29:29.520
<v Speaker 2>question.</v>
<v Speaker 2>Please raise your hand and we'll get you</v>

473
00:29:29.521 --> 00:29:32.780
<v Speaker 2>on mic.</v>
<v Speaker 2>Uh,</v>

474
00:29:32.970 --> 00:29:34.110
<v Speaker 2>high.</v>
<v Speaker 2>Uh,</v>

475
00:29:34.111 --> 00:29:35.490
<v Speaker 2>so you mentioned,</v>
<v Speaker 2>uh,</v>

476
00:29:35.520 --> 00:29:39.030
<v Speaker 2>both your neural network models and </v>
<v Speaker 2>symbolic models.</v>

477
00:29:39.280 --> 00:29:41.470
<v Speaker 2>Uh,</v>
<v Speaker 2>and I was wondering how far have</v>

478
00:29:42.040 --> 00:29:44.680
<v Speaker 4>you had been thinking about combining </v>
<v Speaker 4>these two approaches?</v>

479
00:29:44.980 --> 00:29:47.820
<v Speaker 4>Creating a symbiosis between neural </v>
<v Speaker 4>models.</v>

480
00:29:47.830 --> 00:29:48.940
<v Speaker 4>Send symbolic ones.</v>

481
00:29:51.140 --> 00:29:56.140
<v Speaker 5>I don't think we want to use symbolic </v>
<v Speaker 5>models as they've been used.</v>

482
00:29:58.600 --> 00:30:00.760
<v Speaker 5>How many are familiar with the psych </v>
<v Speaker 5>project?</v>

483
00:30:03.920 --> 00:30:08.920
<v Speaker 5>That was a very diligent effort in Texas</v>
<v Speaker 5>to define all of common sense reasoning </v>

484
00:30:10.590 --> 00:30:15.590
<v Speaker 5>and it kind of collapsed on itself and </v>
<v Speaker 5>became impossible to debug because you </v>

485
00:30:17.041 --> 00:30:19.170
<v Speaker 5>fix one thing and it would break three </v>
<v Speaker 5>other things.</v>

486
00:30:19.860 --> 00:30:24.760
<v Speaker 5>That complexity ceiling has become </v>
<v Speaker 5>typical of,</v>

487
00:30:25.660 --> 00:30:29.770
<v Speaker 5>of trying to define things through </v>
<v Speaker 5>logical rules.</v>

488
00:30:30.480 --> 00:30:35.480
<v Speaker 5>Uh,</v>
<v Speaker 5>now it does seem that humans can </v>

489
00:30:35.480 --> 00:30:37.711
<v Speaker 5>understand logical rules.</v>
<v Speaker 5>We have logical rules written down for </v>

490
00:30:37.711 --> 00:30:40.840
<v Speaker 5>things like law and game playing and so </v>
<v Speaker 5>on.</v>

491
00:30:41.360 --> 00:30:45.040
<v Speaker 5>Uh,</v>
<v Speaker 5>but you can actually define a connection</v>

492
00:30:45.041 --> 00:30:50.041
<v Speaker 5>as system to have such a high </v>
<v Speaker 5>reliability on a certain type of action </v>

493
00:30:53.021 --> 00:30:58.021
<v Speaker 5>that it looks like it's a symbolic role,</v>
<v Speaker 5>even though it's represented in a </v>

494
00:30:58.960 --> 00:31:03.960
<v Speaker 5>connectionist way.</v>
<v Speaker 5>And connection systems can both capture </v>

495
00:31:03.960 --> 00:31:08.881
<v Speaker 5>the soft edges because many things in </v>
<v Speaker 5>life are not a sharply defined.</v>

496
00:31:09.490 --> 00:31:12.430
<v Speaker 5>They can also generate exceptions.</v>
<v Speaker 5>So you,</v>

497
00:31:12.520 --> 00:31:17.080
<v Speaker 5>you don't want to sacrifice your queen </v>
<v Speaker 5>and chest except certain situations that</v>

498
00:31:17.081 --> 00:31:22.081
<v Speaker 5>might be a good idea.</v>
<v Speaker 5>So you can capture that kind of </v>

499
00:31:22.081 --> 00:31:22.081
<v Speaker 5>complexity.</v>
<v Speaker 5>Uh,</v>

500
00:31:22.540 --> 00:31:27.540
<v Speaker 5>so we do want to be able to learn from </v>
<v Speaker 5>accumulated human wisdom that looks like</v>

501
00:31:28.661 --> 00:31:30.070
<v Speaker 5>it's symbolic.</v>
<v Speaker 5>But,</v>

502
00:31:30.270 --> 00:31:32.780
<v Speaker 5>uh,</v>
<v Speaker 5>I think we'll do it with a connection of</v>

503
00:31:32.790 --> 00:31:33.760
<v Speaker 5>system.</v>
<v Speaker 5>But again,</v>

504
00:31:33.761 --> 00:31:38.761
<v Speaker 5>I'm,</v>
<v Speaker 5>I think the connection systems should </v>

505
00:31:38.761 --> 00:31:42.890
<v Speaker 5>develop a sense of hierarchy and not </v>
<v Speaker 5>just be one big massive neural net.</v>

506
00:31:44.800 --> 00:31:47.260
<v Speaker 4>So I understand how we won,</v>
<v Speaker 4>you know,</v>

507
00:31:47.340 --> 00:31:51.420
<v Speaker 4>using your cortex to extract useful </v>
<v Speaker 4>stuff and commercialize that,</v>

508
00:31:51.421 --> 00:31:53.650
<v Speaker 4>but I'm wondering how,</v>
<v Speaker 4>you know,</v>

509
00:31:53.790 --> 00:31:58.150
<v Speaker 4>our middle brain and the organs that are</v>
<v Speaker 4>below the neocortex will be useful for,</v>

510
00:31:58.460 --> 00:31:59.330
<v Speaker 4>um,</v>
<v Speaker 4>you know,</v>

511
00:31:59.680 --> 00:32:01.840
<v Speaker 4>turning that into what you want to do.</v>
<v Speaker 4>So.</v>

512
00:32:03.360 --> 00:32:05.490
<v Speaker 5>Well,</v>
<v Speaker 5>the cerebellum is an interesting case in</v>

513
00:32:05.491 --> 00:32:10.491
<v Speaker 5>point.</v>
<v Speaker 5>It actually has more neurons in the </v>

514
00:32:10.491 --> 00:32:13.530
<v Speaker 5>neocortex and it's used to govern most </v>
<v Speaker 5>of our behavior.</v>

515
00:32:15.650 --> 00:32:16.760
<v Speaker 5>Uh,</v>
<v Speaker 5>some things,</v>

516
00:32:17.000 --> 00:32:19.890
<v Speaker 5>if you write a signature that's actually</v>
<v Speaker 5>controlled by the cerebellum,</v>

517
00:32:19.950 --> 00:32:24.870
<v Speaker 5>so a simple sequence is stored in the </v>
<v Speaker 5>cerebellum,</v>

518
00:32:25.950 --> 00:32:29.760
<v Speaker 5>but there's not any reasoning to it.</v>
<v Speaker 5>It's basically a script.</v>

519
00:32:30.240 --> 00:32:35.240
<v Speaker 5>Uh,</v>
<v Speaker 5>and most of our movement now has </v>

520
00:32:35.240 --> 00:32:37.500
<v Speaker 5>actually been migrated from the </v>
<v Speaker 5>cerebellum to the NEOCORTEX.</v>

521
00:32:37.860 --> 00:32:40.530
<v Speaker 5>Cerebellum is still there.</v>
<v Speaker 5>Uh,</v>

522
00:32:40.580 --> 00:32:45.580
<v Speaker 5>some people,</v>
<v Speaker 5>a tire cerebellum is destroyed through </v>

523
00:32:45.580 --> 00:32:45.830
<v Speaker 5>disease.</v>
<v Speaker 5>Uh,</v>

524
00:32:45.860 --> 00:32:48.950
<v Speaker 5>they still function fairly.</v>
<v Speaker 5>Normally they're moving,</v>

525
00:32:48.951 --> 00:32:53.951
<v Speaker 5>might be a little erratic as our </v>
<v Speaker 5>movement is largely controlled by the </v>

526
00:32:53.951 --> 00:32:58.631
<v Speaker 5>Neocortex,</v>
<v Speaker 5>but some of the subtlety is a kind of </v>

527
00:32:58.631 --> 00:33:01.310
<v Speaker 5>preprogrammed script and so they'll look</v>
<v Speaker 5>a little clumsy,</v>

528
00:33:01.311 --> 00:33:04.570
<v Speaker 5>but they're actually functioning okay.</v>
<v Speaker 5>Um,</v>

529
00:33:05.060 --> 00:33:07.350
<v Speaker 5>a lot of other areas of the brain </v>
<v Speaker 5>control,</v>

530
00:33:07.351 --> 00:33:11.200
<v Speaker 5>autonomic functions like breathing and </v>
<v Speaker 5>uh,</v>

531
00:33:11.310 --> 00:33:16.310
<v Speaker 5>but our thinking really is,</v>
<v Speaker 5>is controlled by the neocortex in terms </v>

532
00:33:16.310 --> 00:33:19.871
<v Speaker 5>of a mastering intelligence.</v>
<v Speaker 5>I think the NEOCORTEX is the brain </v>

533
00:33:22.551 --> 00:33:23.780
<v Speaker 5>region we want to study.</v>

534
00:33:25.860 --> 00:33:30.860
<v Speaker 4>I'm curious what you think might happen </v>
<v Speaker 4>after the singularity is reached in </v>

535
00:33:31.411 --> 00:33:34.260
<v Speaker 4>terms of this exponential growth of </v>
<v Speaker 4>information.</v>

536
00:33:35.390 --> 00:33:40.390
<v Speaker 4>Yeah.</v>
<v Speaker 4>Do you think it will continue or will </v>

537
00:33:40.390 --> 00:33:41.010
<v Speaker 4>there be a whole paradigm shift?</v>
<v Speaker 4>What do you predict?</v>

538
00:33:42.270 --> 00:33:47.270
<v Speaker 5>Well,</v>
<v Speaker 5>in the singularity is near talk about </v>

539
00:33:47.270 --> 00:33:49.701
<v Speaker 5>the atomic limits based on molecular </v>
<v Speaker 5>computing as we understand it,</v>

540
00:33:50.340 --> 00:33:55.340
<v Speaker 5>uh,</v>
<v Speaker 5>and it can actually go well past 20 </v>

541
00:33:55.340 --> 00:33:57.781
<v Speaker 5>slash 45 and actually go to trillions of</v>
<v Speaker 5>trillions of times a greater </v>

542
00:33:57.781 --> 00:34:00.060
<v Speaker 5>computational capacity than we have </v>
<v Speaker 5>today.</v>

543
00:34:01.890 --> 00:34:06.890
<v Speaker 5>So I don't see that stopping anytime </v>
<v Speaker 5>soon and we'll go way beyond what we can</v>

544
00:34:08.580 --> 00:34:10.210
<v Speaker 5>imagine.</v>
<v Speaker 5>Um,</v>

545
00:34:11.760 --> 00:34:13.740
<v Speaker 5>and it becomes an interesting </v>
<v Speaker 5>discussion.</v>

546
00:34:13.741 --> 00:34:18.210
<v Speaker 5>What the impact on a human civilization </v>
<v Speaker 5>will be.</v>

547
00:34:18.900 --> 00:34:23.900
<v Speaker 5>So take it maybe slightly more mundane </v>
<v Speaker 5>issue that comes up as us going to </v>

548
00:34:24.150 --> 00:34:29.150
<v Speaker 5>eliminate most jobs are all jobs.</v>
<v Speaker 5>A point I make is it's not the first </v>

549
00:34:29.791 --> 00:34:31.650
<v Speaker 5>time in human history.</v>
<v Speaker 5>We've done that.</v>

550
00:34:32.100 --> 00:34:36.350
<v Speaker 5>How many jobs circa 1900 exists today.</v>
<v Speaker 5>Uh,</v>

551
00:34:36.360 --> 00:34:39.690
<v Speaker 5>and that was the feeling of the </v>
<v Speaker 5>Luddites,</v>

552
00:34:39.691 --> 00:34:44.691
<v Speaker 5>which was some actual society and that </v>
<v Speaker 5>formed in 1800 after the automation of </v>

553
00:34:44.691 --> 00:34:46.590
<v Speaker 5>the textile industry in England.</v>

554
00:34:47.180 --> 00:34:52.180
<v Speaker 5>They looked at all these jobs going away</v>
<v Speaker 5>and felt old employment's going to be </v>

555
00:34:52.180 --> 00:34:53.790
<v Speaker 5>just limited to an elite.</v>
<v Speaker 5>Uh,</v>

556
00:34:53.791 --> 00:34:55.560
<v Speaker 5>indeed,</v>
<v Speaker 5>those jobs did go away,</v>

557
00:34:55.561 --> 00:35:00.561
<v Speaker 5>but new jobs were created.</v>
<v Speaker 5>So if I were oppression futurist in </v>

558
00:35:00.561 --> 00:35:02.370
<v Speaker 5>1900,</v>
<v Speaker 5>I would say,</v>

559
00:35:02.371 --> 00:35:07.371
<v Speaker 5>well,</v>
<v Speaker 5>38 percent of your work on farms and 25 </v>

560
00:35:07.371 --> 00:35:08.880
<v Speaker 5>percent work in factories.</v>
<v Speaker 5>That's two thirds of the working for us,</v>

561
00:35:11.490 --> 00:35:16.200
<v Speaker 5>but I predict by 20 1,515</v>
<v Speaker 5>years from now,</v>

562
00:35:16.201 --> 00:35:19.470
<v Speaker 5>it's going to be two percent on farms </v>
<v Speaker 5>and nine percent in factories.</v>

563
00:35:19.471 --> 00:35:21.210
<v Speaker 5>And everybody would go,</v>
<v Speaker 5>oh my God,</v>

564
00:35:21.211 --> 00:35:22.950
<v Speaker 5>we're going to be out of work.</v>
<v Speaker 5>And I said,</v>

565
00:35:22.951 --> 00:35:27.951
<v Speaker 5>well,</v>
<v Speaker 5>don't worry for all these jobs we </v>

566
00:35:27.951 --> 00:35:27.951
<v Speaker 5>eliminate through automation.</v>
<v Speaker 5>We're going to invent new jobs.</v>

567
00:35:27.951 --> 00:35:28.410
<v Speaker 5>People say,</v>
<v Speaker 5>Oh,</v>

568
00:35:28.411 --> 00:35:29.470
<v Speaker 5>really?</v>
<v Speaker 5>What new jobs?</v>

569
00:35:29.471 --> 00:35:30.510
<v Speaker 5>And I'd say,</v>
<v Speaker 5>well,</v>

570
00:35:30.511 --> 00:35:32.100
<v Speaker 5>I don't know.</v>
<v Speaker 5>We haven't invented them yet.</v>

571
00:35:33.570 --> 00:35:38.570
<v Speaker 5>That's the political problem.</v>
<v Speaker 5>We can see jobs very clearly going away </v>

572
00:35:38.570 --> 00:35:38.570
<v Speaker 5>fairly soon.</v>

573
00:35:38.570 --> 00:35:40.550
<v Speaker 5>Like driving a car or truck,</v>
<v Speaker 5>uh,</v>

574
00:35:41.190 --> 00:35:42.960
<v Speaker 5>and the new jobs haven't been invented.</v>
<v Speaker 5>I mean,</v>

575
00:35:42.961 --> 00:35:47.961
<v Speaker 5>it's just look at the last five or six </v>
<v Speaker 5>years as many of the increase in </v>

576
00:35:47.961 --> 00:35:52.461
<v Speaker 5>employment has been through mobile app </v>
<v Speaker 5>related types of ways of making money </v>

577
00:35:53.311 --> 00:35:56.220
<v Speaker 5>that just weren't contemplated even six </v>
<v Speaker 5>years ago.</v>

578
00:35:57.990 --> 00:35:59.430
<v Speaker 5>If I really pressured,</v>
<v Speaker 5>I would say,</v>

579
00:35:59.431 --> 00:36:01.200
<v Speaker 5>well,</v>
<v Speaker 5>you're going to get jobs creating mobile</v>

580
00:36:01.201 --> 00:36:06.201
<v Speaker 5>apps and websites and doing data </v>
<v Speaker 5>analytics and a self driving cars,</v>

581
00:36:07.770 --> 00:36:08.820
<v Speaker 5>cars.</v>
<v Speaker 5>What's a car?</v>

582
00:36:08.940 --> 00:36:13.940
<v Speaker 5>And nobody would have any idea what I'm </v>
<v Speaker 5>talking about a now the new job.</v>

583
00:36:16.050 --> 00:36:16.920
<v Speaker 5>Some people say,</v>
<v Speaker 5>yeah,</v>

584
00:36:16.921 --> 00:36:19.740
<v Speaker 5>we created new jobs for.</v>
<v Speaker 5>It's not as many actually.</v>

585
00:36:20.210 --> 00:36:25.210
<v Speaker 5>We've gone from 24 million jobs in 1900,</v>
<v Speaker 5>242 million jobs today for 30 percent of</v>

586
00:36:26.191 --> 00:36:28.680
<v Speaker 5>the population to 45 percent of the </v>
<v Speaker 5>population.</v>

587
00:36:29.330 --> 00:36:34.330
<v Speaker 5>The new jobs pay 11 times as much in </v>
<v Speaker 5>constant dollars and they're more </v>

588
00:36:34.330 --> 00:36:34.330
<v Speaker 5>interesting.</v>

589
00:36:34.330 --> 00:36:39.260
<v Speaker 5>I mean,</v>
<v Speaker 5>as I talk to people starting out their </v>

590
00:36:39.260 --> 00:36:39.380
<v Speaker 5>career now,</v>
<v Speaker 5>they really want a career that gives him</v>

591
00:36:39.381 --> 00:36:43.560
<v Speaker 5>some life definition and purpose and </v>
<v Speaker 5>gratification.</v>

592
00:36:43.561 --> 00:36:47.700
<v Speaker 5>We're moving up maslow's hierarchy a 100</v>
<v Speaker 5>years ago.</v>

593
00:36:47.701 --> 00:36:51.060
<v Speaker 5>You are happy if you had a backbreaking </v>
<v Speaker 5>job to put food on your family's table.</v>

594
00:36:51.061 --> 00:36:56.061
<v Speaker 5>So,</v>
<v Speaker 5>and we couldn't do these new jobs </v>

595
00:36:56.061 --> 00:37:00.141
<v Speaker 5>without enhancing our intelligence.</v>
<v Speaker 5>So we've been doing that a well for most</v>

596
00:37:00.271 --> 00:37:05.040
<v Speaker 5>of the last 100 years through education.</v>
<v Speaker 5>We've expanded k through 12 and constant</v>

597
00:37:05.041 --> 00:37:09.660
<v Speaker 5>dollars tenfold.</v>
<v Speaker 5>We've gone from 38,000</v>

598
00:37:09.661 --> 00:37:13.530
<v Speaker 5>college students in 1870 to $50 million </v>
<v Speaker 5>today.</v>

599
00:37:14.070 --> 00:37:19.070
<v Speaker 5>Uh,</v>
<v Speaker 5>more recently we have brain extenders </v>

600
00:37:19.070 --> 00:37:19.920
<v Speaker 5>and not yet connected directly in our </v>
<v Speaker 5>brain,</v>

601
00:37:20.250 --> 00:37:25.250
<v Speaker 5>but they're great close at hand when I </v>
<v Speaker 5>was here at mit to take my bicycle </v>

602
00:37:25.250 --> 00:37:28.440
<v Speaker 5>across campus to get to the computer and</v>
<v Speaker 5>show an ID to get in the building.</v>

603
00:37:28.441 --> 00:37:33.441
<v Speaker 5>Now we carry them in our,</v>
<v Speaker 5>in our pockets and under our belts.</v>

604
00:37:33.990 --> 00:37:38.990
<v Speaker 5>A,</v>
<v Speaker 5>they're going to go inside our bodies </v>

605
00:37:38.990 --> 00:37:41.391
<v Speaker 5>and brains.</v>
<v Speaker 5>I think that's a really important </v>

606
00:37:41.391 --> 00:37:44.660
<v Speaker 5>distinction,</v>
<v Speaker 5>but so we're basically going to be </v>

607
00:37:44.660 --> 00:37:46.620
<v Speaker 5>continuing to enhance our capability </v>
<v Speaker 5>through merging with Ai.</v>

608
00:37:47.790 --> 00:37:52.790
<v Speaker 5>And that's the,</v>
<v Speaker 5>I think ultimate answer to the kind of </v>

609
00:37:52.790 --> 00:37:56.360
<v Speaker 5>Dystopian view we see in futures movies </v>
<v Speaker 5>where it's the ai versus a brave band of</v>

610
00:37:56.731 --> 00:38:01.731
<v Speaker 5>humans for control of humanity.</v>
<v Speaker 5>We don't have one or two ais in the </v>

611
00:38:01.731 --> 00:38:03.090
<v Speaker 5>world today.</v>
<v Speaker 5>We have several billion,</v>

612
00:38:03.120 --> 00:38:08.120
<v Speaker 5>3 billion smartphones at last count and </v>
<v Speaker 5>it'll be 6 billion in just a couple of </v>

613
00:38:08.250 --> 00:38:10.310
<v Speaker 5>years according to the projections.</v>
<v Speaker 5>Uh,</v>

614
00:38:10.430 --> 00:38:15.430
<v Speaker 5>so we're,</v>
<v Speaker 5>we're already deeply integrated with </v>

615
00:38:15.430 --> 00:38:15.430
<v Speaker 5>this,</v>
<v Speaker 5>uh,</v>

616
00:38:15.430 --> 00:38:18.170
<v Speaker 5>and I think that's going to continue and</v>
<v Speaker 5>it's going to continue to do things that</v>

617
00:38:18.171 --> 00:38:21.330
<v Speaker 5>we can't even imagine today just as we </v>
<v Speaker 5>are doing today.</v>

618
00:38:21.331 --> 00:38:24.420
<v Speaker 5>Things we couldn't imagine know even 20 </v>
<v Speaker 5>years ago</v>

619
00:38:26.490 --> 00:38:29.670
<v Speaker 6>you showed many grasp that go through </v>
<v Speaker 6>exponential growth,</v>

620
00:38:29.671 --> 00:38:33.360
<v Speaker 6>but I haven't seen one that isn't.</v>
<v Speaker 6>So I would be very interested in hearing</v>

621
00:38:33.660 --> 00:38:36.280
<v Speaker 6>you haven't seen what that,</v>
<v Speaker 6>what that is not exponential.</v>

622
00:38:36.790 --> 00:38:41.790
<v Speaker 6>So tell me about regions that you've </v>
<v Speaker 6>investigated that have not seen </v>

623
00:38:41.980 --> 00:38:44.800
<v Speaker 6>exponential growth.</v>
<v Speaker 6>And why do you think that's the case?</v>

624
00:38:45.980 --> 00:38:47.720
<v Speaker 5>Well,</v>
<v Speaker 5>price,</v>

625
00:38:47.721 --> 00:38:52.721
<v Speaker 5>performance and capacity of information </v>
<v Speaker 5>technology invariably follows a </v>

626
00:38:52.910 --> 00:38:57.350
<v Speaker 5>exponential where it impacts human </v>
<v Speaker 5>society.</v>

627
00:38:57.351 --> 00:38:59.240
<v Speaker 5>It can be linear.</v>
<v Speaker 5>So for example,</v>

628
00:38:59.241 --> 00:39:02.010
<v Speaker 5>the growth of democracy,</v>
<v Speaker 5>uh,</v>

629
00:39:02.300 --> 00:39:07.300
<v Speaker 5>has been linear but still pretty steady.</v>
<v Speaker 5>I could count the number of democracies </v>

630
00:39:07.641 --> 00:39:11.330
<v Speaker 5>on the fingers of one hand to century,</v>
<v Speaker 5>go a two centuries ago.</v>

631
00:39:11.331 --> 00:39:16.331
<v Speaker 5>You could count the number of </v>
<v Speaker 5>democracies in the world on the fingers </v>

632
00:39:16.331 --> 00:39:18.881
<v Speaker 5>of one finger.</v>
<v Speaker 5>Now there are dozens of them that this </v>

633
00:39:18.881 --> 00:39:22.470
<v Speaker 5>has become kind of a consensus that </v>
<v Speaker 5>that's how we should be governed.</v>

634
00:39:25.030 --> 00:39:30.030
<v Speaker 5>So the,</v>
<v Speaker 5>and I attribute all this to the growth </v>

635
00:39:30.030 --> 00:39:34.231
<v Speaker 5>in information technology,</v>
<v Speaker 5>communication in particular for a </v>

636
00:39:34.780 --> 00:39:39.430
<v Speaker 5>progression of social cultural </v>
<v Speaker 5>institutions,</v>

637
00:39:40.140 --> 00:39:45.140
<v Speaker 5>um,</v>
<v Speaker 5>but information technology because it </v>

638
00:39:45.140 --> 00:39:48.211
<v Speaker 5>ultimately depends on a vanishingly </v>
<v Speaker 5>small energy and material requirement </v>

639
00:39:51.730 --> 00:39:54.580
<v Speaker 5>grows exponentially and will for a long </v>
<v Speaker 5>time.</v>

640
00:39:55.390 --> 00:39:57.340
<v Speaker 5>Uh,</v>
<v Speaker 5>there was recently a criticism that,</v>

641
00:39:57.370 --> 00:39:58.480
<v Speaker 5>well,</v>
<v Speaker 5>chest scars have.</v>

642
00:39:59.080 --> 00:40:04.080
<v Speaker 5>It's actually a remarkably a straight </v>
<v Speaker 5>linear progression,</v>

643
00:40:05.020 --> 00:40:10.020
<v Speaker 5>so humans think it's like 2,800</v>
<v Speaker 5>and it just soared past that in 1997 </v>

644
00:40:10.871 --> 00:40:13.780
<v Speaker 5>with deep blue and it's kept going,</v>
<v Speaker 5>uh,</v>

645
00:40:14.160 --> 00:40:16.000
<v Speaker 5>and a remarkably straight and St.</v>
<v Speaker 5>well,</v>

646
00:40:16.001 --> 00:40:17.500
<v Speaker 5>this is linear,</v>
<v Speaker 5>not exponential,</v>

647
00:40:17.501 --> 00:40:20.380
<v Speaker 5>but the chest score is a logarithmic,</v>
<v Speaker 5>uh,</v>

648
00:40:20.410 --> 00:40:22.140
<v Speaker 5>measurement,</v>
<v Speaker 5>uh,</v>

649
00:40:22.540 --> 00:40:24.080
<v Speaker 5>so,</v>
<v Speaker 5>uh,</v>

650
00:40:24.110 --> 00:40:26.650
<v Speaker 5>it,</v>
<v Speaker 5>it really is exponential progression.</v>

651
00:40:28.590 --> 00:40:32.250
<v Speaker 6>So philosophers like to think about the </v>
<v Speaker 6>meaning of things,</v>

652
00:40:32.251 --> 00:40:34.680
<v Speaker 6>especially in the 20th century.</v>
<v Speaker 6>So for instance,</v>

653
00:40:34.681 --> 00:40:39.681
<v Speaker 6>Martin Heidegger gave a couple of </v>
<v Speaker 6>speeches and lectures on the </v>

654
00:40:39.681 --> 00:40:41.460
<v Speaker 6>relationship of human society to </v>
<v Speaker 6>technology,</v>

655
00:40:41.760 --> 00:40:45.960
<v Speaker 6>and he particularly distinguished </v>
<v Speaker 6>between the mode of thinking,</v>

656
00:40:45.961 --> 00:40:48.600
<v Speaker 6>which is calculating,</v>
<v Speaker 6>thinking and a mode of thinking,</v>

657
00:40:48.601 --> 00:40:51.390
<v Speaker 6>which is reflective thinking or </v>
<v Speaker 6>meditative thinking.</v>

658
00:40:51.440 --> 00:40:52.560
<v Speaker 6>Um,</v>
<v Speaker 6>and,</v>

659
00:40:52.590 --> 00:40:53.970
<v Speaker 6>uh,</v>
<v Speaker 6>he posed this question,</v>

660
00:40:53.971 --> 00:40:58.080
<v Speaker 6>what is the meaning and purpose of </v>
<v Speaker 6>technological development?</v>

661
00:40:58.110 --> 00:41:00.120
<v Speaker 6>And he couldn't find an answer.</v>
<v Speaker 6>He,</v>

662
00:41:00.240 --> 00:41:03.530
<v Speaker 6>he recommended it to remain open to what</v>
<v Speaker 6>you called.</v>

663
00:41:04.530 --> 00:41:06.690
<v Speaker 6>He called this an openness to the </v>
<v Speaker 6>mystery.</v>

664
00:41:07.040 --> 00:41:12.040
<v Speaker 6>Uh,</v>
<v Speaker 6>I wonder whether you have any thoughts </v>

665
00:41:12.040 --> 00:41:12.040
<v Speaker 6>on this.</v>
<v Speaker 6>Is there a meaning,</v>

666
00:41:12.040 --> 00:41:13.290
<v Speaker 6>a purpose of technological element?</v>
<v Speaker 6>And,</v>

667
00:41:13.291 --> 00:41:17.040
<v Speaker 6>and is there a way for us human success,</v>
<v Speaker 6>excess that meaning.</v>

668
00:41:21.430 --> 00:41:26.430
<v Speaker 5>Well,</v>
<v Speaker 5>we started using technology to shore up </v>

669
00:41:26.430 --> 00:41:30.850
<v Speaker 5>weaknesses and our own capabilities.</v>
<v Speaker 5>So physically,</v>

670
00:41:31.180 --> 00:41:33.190
<v Speaker 5>I mean,</v>
<v Speaker 5>who here could build this building?</v>

671
00:41:33.200 --> 00:41:36.370
<v Speaker 5>So we've leveraged the power of our </v>
<v Speaker 5>muscles with machines,</v>

672
00:41:36.750 --> 00:41:41.750
<v Speaker 5>uh,</v>
<v Speaker 5>and were in fact very bad at doing </v>

673
00:41:41.750 --> 00:41:42.000
<v Speaker 5>things that,</v>
<v Speaker 5>you know,</v>

674
00:41:42.080 --> 00:41:47.080
<v Speaker 5>the simplest computers can do a pike </v>
<v Speaker 5>factor numbers or even just multiply two</v>

675
00:41:49.730 --> 00:41:53.750
<v Speaker 5>digit numbers.</v>
<v Speaker 5>Computers can do that trivially.</v>

676
00:41:53.751 --> 00:41:57.920
<v Speaker 5>We can't do it.</v>
<v Speaker 5>So we originally started using computers</v>

677
00:41:57.921 --> 00:42:02.160
<v Speaker 5>to make up for that weakness.</v>
<v Speaker 5>Uh,</v>

678
00:42:02.210 --> 00:42:07.210
<v Speaker 5>I think the essence of what I've been </v>
<v Speaker 5>writing about is to master the unique </v>

679
00:42:07.210 --> 00:42:09.110
<v Speaker 5>strengths of,</v>
<v Speaker 5>of humanity,</v>

680
00:42:10.370 --> 00:42:14.420
<v Speaker 5>creating loving expressions and poetry </v>
<v Speaker 5>and music and,</v>

681
00:42:15.050 --> 00:42:20.050
<v Speaker 5>uh,</v>
<v Speaker 5>the kinds of things we associate with </v>

682
00:42:20.050 --> 00:42:20.720
<v Speaker 5>the better qualities of humanity with </v>
<v Speaker 5>machines.</v>

683
00:42:20.721 --> 00:42:23.910
<v Speaker 5>That's the true promise of Ai,</v>
<v Speaker 5>uh,</v>

684
00:42:24.250 --> 00:42:29.250
<v Speaker 5>that we're not there yet,</v>
<v Speaker 5>but we're making pretty stunning </v>

685
00:42:29.250 --> 00:42:31.930
<v Speaker 5>progress just in the last year,</v>
<v Speaker 5>this so many milestones that are really </v>

686
00:42:31.930 --> 00:42:33.830
<v Speaker 5>significant,</v>
<v Speaker 5>including in language.</v>

687
00:42:34.400 --> 00:42:36.890
<v Speaker 5>Um,</v>
<v Speaker 5>and,</v>

688
00:42:37.130 --> 00:42:41.090
<v Speaker 5>but I think of technology as an </v>
<v Speaker 5>expression of humanity.</v>

689
00:42:41.510 --> 00:42:45.860
<v Speaker 5>It's part of who we are and the human </v>
<v Speaker 5>species is already a,</v>

690
00:42:46.390 --> 00:42:51.390
<v Speaker 5>a biological technological civilization </v>
<v Speaker 5>and it's part of who we are and ai is as</v>

691
00:42:52.910 --> 00:42:55.110
<v Speaker 5>part of humans.</v>
<v Speaker 5>Uh,</v>

692
00:42:55.550 --> 00:43:00.550
<v Speaker 5>so ai is human and it's just,</v>
<v Speaker 5>it's part of the technological </v>

693
00:43:00.951 --> 00:43:05.951
<v Speaker 5>expression of humanity.</v>
<v Speaker 5>And we use technology to extend our </v>

694
00:43:05.951 --> 00:43:06.820
<v Speaker 5>reach.</v>
<v Speaker 5>You know,</v>

695
00:43:06.880 --> 00:43:09.500
<v Speaker 5>I couldn't reach that fruit at that </v>
<v Speaker 5>higher branch a thousand years ago.</v>

696
00:43:09.501 --> 00:43:11.990
<v Speaker 5>So we invented a tool to extend our </v>
<v Speaker 5>physical reach.</v>

697
00:43:12.550 --> 00:43:17.550
<v Speaker 5>Now extend our mental reach.</v>
<v Speaker 5>We can access all of human knowledge </v>

698
00:43:17.600 --> 00:43:20.400
<v Speaker 5>with a few keystrokes.</v>
<v Speaker 5>Um,</v>

699
00:43:21.200 --> 00:43:26.200
<v Speaker 5>and we're going to make ourselves </v>
<v Speaker 5>literally smarter by merging with Ai.</v>

700
00:43:31.410 --> 00:43:33.950
<v Speaker 7>Hi,</v>
<v Speaker 7>first of all,</v>

701
00:43:34.160 --> 00:43:36.180
<v Speaker 7>honor to hear you speak here.</v>
<v Speaker 7>Uh,</v>

702
00:43:36.410 --> 00:43:41.410
<v Speaker 7>so I first read the singularity is near </v>
<v Speaker 7>nine years ago or so,</v>

703
00:43:42.260 --> 00:43:45.260
<v Speaker 7>and it changed the way I thought </v>
<v Speaker 7>entirely,</v>

704
00:43:45.350 --> 00:43:50.350
<v Speaker 7>but something I think it caused me to </v>
<v Speaker 7>oversee deeply discount was tail risk in</v>

705
00:43:54.500 --> 00:43:59.500
<v Speaker 7>geopolitics in systems that span the </v>
<v Speaker 7>entire globe.</v>

706
00:44:00.920 --> 00:44:05.920
<v Speaker 7>Um,</v>
<v Speaker 7>and my concern is that there are,</v>

707
00:44:06.980 --> 00:44:10.820
<v Speaker 7>there's obviously the possibility of </v>
<v Speaker 7>tail risk,</v>

708
00:44:12.490 --> 00:44:17.490
<v Speaker 7>existential level events,</v>
<v Speaker 7>swamping all of these trends that are </v>

709
00:44:17.650 --> 00:44:22.240
<v Speaker 7>otherwise waterproof climate proof,</v>
<v Speaker 7>you name it.</v>

710
00:44:22.450 --> 00:44:27.450
<v Speaker 7>So my question for you is,</v>
<v Speaker 7>what steps do you think we can take in </v>

711
00:44:29.051 --> 00:44:34.051
<v Speaker 7>designing engineered systems in </v>
<v Speaker 7>designing social and economic </v>

712
00:44:34.051 --> 00:44:38.961
<v Speaker 7>institutions to kind of minimize our </v>
<v Speaker 7>exposure to these tail risks and,</v>

713
00:44:40.050 --> 00:44:42.480
<v Speaker 7>and survive to make it,</v>
<v Speaker 7>to,</v>

714
00:44:42.481 --> 00:44:43.740
<v Speaker 7>um,</v>
<v Speaker 7>you know,</v>

715
00:44:43.741 --> 00:44:47.520
<v Speaker 7>a beautiful mind filled future?</v>

716
00:44:48.350 --> 00:44:49.280
<v Speaker 5>Yeah.</v>
<v Speaker 5>Well,</v>

717
00:44:51.920 --> 00:44:56.920
<v Speaker 5>the world was first introduced to a </v>
<v Speaker 5>human made existential risk.</v>

718
00:44:57.770 --> 00:45:00.020
<v Speaker 5>Uh,</v>
<v Speaker 5>when I was in elementary school,</v>

719
00:45:00.021 --> 00:45:05.021
<v Speaker 5>we would have these civil defense drills</v>
<v Speaker 5>get under our desks and put our hands </v>

720
00:45:05.021 --> 00:45:07.490
<v Speaker 5>behind our head to protect this former </v>
<v Speaker 5>thermonuclear war.</v>

721
00:45:07.970 --> 00:45:09.470
<v Speaker 5>Uh,</v>
<v Speaker 5>and it worked.</v>

722
00:45:09.471 --> 00:45:14.471
<v Speaker 5>We made it through,</v>
<v Speaker 5>but that was really the first </v>

723
00:45:14.751 --> 00:45:17.740
<v Speaker 5>introduction to an existential risk,</v>
<v Speaker 5>uh,</v>

724
00:45:17.860 --> 00:45:21.050
<v Speaker 5>and those weapons are still there by the</v>
<v Speaker 5>way,</v>

725
00:45:21.051 --> 00:45:25.970
<v Speaker 5>and they're still on a hair trigger and </v>
<v Speaker 5>they don't get that much attention.</v>

726
00:45:26.430 --> 00:45:28.460
<v Speaker 5>Uh,</v>
<v Speaker 5>there's been a lot of discussion,</v>

727
00:45:29.030 --> 00:45:34.030
<v Speaker 5>a much of which I've been in the </v>
<v Speaker 5>forefront of initiating the existential </v>

728
00:45:34.030 --> 00:45:37.940
<v Speaker 5>risks of what's sometimes referred to as</v>
<v Speaker 5>gnr g for genetics,</v>

729
00:45:37.941 --> 00:45:42.410
<v Speaker 5>which is biotechnology and financial </v>
<v Speaker 5>technology and grey goo robotics,</v>

730
00:45:42.411 --> 00:45:44.480
<v Speaker 5>which is ai.</v>
<v Speaker 5>Uh,</v>

731
00:45:45.110 --> 00:45:47.990
<v Speaker 5>and I've been accused of being an </v>
<v Speaker 5>optimist.</v>

732
00:45:48.520 --> 00:45:51.130
<v Speaker 5>I think you have to be an optimist to be</v>
<v Speaker 5>an entrepreneur.</v>

733
00:45:51.140 --> 00:45:53.480
<v Speaker 5>If you knew all the problems you were </v>
<v Speaker 5>going to encounter,</v>

734
00:45:53.481 --> 00:45:55.790
<v Speaker 5>you'd never start any project.</v>
<v Speaker 5>But,</v>

735
00:45:56.270 --> 00:46:00.230
<v Speaker 5>uh,</v>
<v Speaker 5>I've written a lot about the downsides.</v>

736
00:46:00.410 --> 00:46:03.060
<v Speaker 5>I remain optimistic.</v>
<v Speaker 5>Uh,</v>

737
00:46:03.080 --> 00:46:08.080
<v Speaker 5>there are specific paradigms and not </v>
<v Speaker 5>foolproof that we can follow to keep </v>

738
00:46:08.080 --> 00:46:09.920
<v Speaker 5>these technologies safe.</v>
<v Speaker 5>So,</v>

739
00:46:09.921 --> 00:46:11.960
<v Speaker 5>for example,</v>
<v Speaker 5>uh,</v>

740
00:46:12.020 --> 00:46:13.740
<v Speaker 5>over 40 years ago,</v>
<v Speaker 5>uh,</v>

741
00:46:13.880 --> 00:46:17.720
<v Speaker 5>some visionaries recognize the </v>
<v Speaker 5>revolutionary potential.</v>

742
00:46:18.080 --> 00:46:21.530
<v Speaker 5>Both were promise and peril of </v>
<v Speaker 5>biotechnology,</v>

743
00:46:22.430 --> 00:46:25.700
<v Speaker 5>neither of the promise and the peril of </v>
<v Speaker 5>what's feasible 40 years ago.</v>

744
00:46:26.690 --> 00:46:30.800
<v Speaker 5>But they had a conference at the Sylmar </v>
<v Speaker 5>Conference Center in California,</v>

745
00:46:31.240 --> 00:46:36.240
<v Speaker 5>uh,</v>
<v Speaker 5>to develop both a professional ethics </v>

746
00:46:36.240 --> 00:46:40.601
<v Speaker 5>and strategies to keep biotechnology </v>
<v Speaker 5>safe and they've been known to see in </v>

747
00:46:40.601 --> 00:46:45.000
<v Speaker 5>Sylmar guidelines.</v>
<v Speaker 5>They've been refined through successive </v>

748
00:46:45.000 --> 00:46:48.810
<v Speaker 5>sylmar conferences.</v>
<v Speaker 5>Much of that is baked into law and an,</v>

749
00:46:49.250 --> 00:46:51.110
<v Speaker 5>in my opinion,</v>
<v Speaker 5>it's worked quite well.</v>

750
00:46:51.111 --> 00:46:52.220
<v Speaker 5>We're now,</v>
<v Speaker 5>as I mentioned,</v>

751
00:46:52.221 --> 00:46:55.100
<v Speaker 5>getting profound benefit.</v>
<v Speaker 5>It's a trickle today.</v>

752
00:46:55.101 --> 00:46:57.740
<v Speaker 5>It'll be a flood over the next decade.</v>

753
00:46:58.250 --> 00:47:03.250
<v Speaker 5>And the number of people who have been </v>
<v Speaker 5>harmed either through intentional or </v>

754
00:47:03.250 --> 00:47:04.760
<v Speaker 5>accidental abuse of biotechnology so </v>
<v Speaker 5>far,</v>

755
00:47:04.790 --> 00:47:06.410
<v Speaker 5>zero.</v>
<v Speaker 5>Actually,</v>

756
00:47:06.411 --> 00:47:11.411
<v Speaker 5>I take that back.</v>
<v Speaker 5>There was one boy who died in gene </v>

757
00:47:11.411 --> 00:47:14.561
<v Speaker 5>therapy trials,</v>
<v Speaker 5>but 12 years ago and there's </v>

758
00:47:14.561 --> 00:47:15.060
<v Speaker 5>congressional hearings and the,</v>
<v Speaker 5>uh,</v>

759
00:47:15.110 --> 00:47:20.110
<v Speaker 5>canceled all research for a gene therapy</v>
<v Speaker 5>for a number of years.</v>

760
00:47:20.661 --> 00:47:23.900
<v Speaker 5>You could do an interesting message </v>
<v Speaker 5>thesis and demonstrate that,</v>

761
00:47:23.930 --> 00:47:25.010
<v Speaker 5>you know,</v>
<v Speaker 5>300,000</v>

762
00:47:25.011 --> 00:47:28.370
<v Speaker 5>people died as a result of that delay,</v>
<v Speaker 5>but you can't name them.</v>

763
00:47:28.371 --> 00:47:30.670
<v Speaker 5>They can't go on.</v>
<v Speaker 5>So we don't know who they are.</v>

764
00:47:30.671 --> 00:47:32.040
<v Speaker 5>But,</v>
<v Speaker 5>um,</v>

765
00:47:32.200 --> 00:47:34.300
<v Speaker 5>so it has to do with the balancing of </v>
<v Speaker 5>risk.</v>

766
00:47:34.301 --> 00:47:39.301
<v Speaker 5>But in large measure,</v>
<v Speaker 5>virtually no one has been hurt by </v>

767
00:47:39.301 --> 00:47:39.560
<v Speaker 5>biotechnology.</v>
<v Speaker 5>Now,</v>

768
00:47:39.570 --> 00:47:41.380
<v Speaker 5>that doesn't mean you can cross it off </v>
<v Speaker 5>our list.</v>

769
00:47:41.660 --> 00:47:46.660
<v Speaker 5>Okay?</v>
<v Speaker 5>We took care of that one because the </v>

770
00:47:46.660 --> 00:47:46.660
<v Speaker 5>technology keeps getting more </v>
<v Speaker 5>sophisticated.</v>

771
00:47:46.730 --> 00:47:51.730
<v Speaker 5>Crispers great opportunity.</v>
<v Speaker 5>This hundreds of trials of crispr </v>

772
00:47:52.240 --> 00:47:56.440
<v Speaker 5>technologies overcome disease,</v>
<v Speaker 5>but it could be abused.</v>

773
00:47:56.440 --> 00:48:00.370
<v Speaker 5>You can easily describe scenarios,</v>
<v Speaker 5>so we have to keep reinventing it.</v>

774
00:48:00.730 --> 00:48:02.110
<v Speaker 5>Uh,</v>
<v Speaker 5>January,</v>

775
00:48:02.111 --> 00:48:05.440
<v Speaker 5>we had our first Sylmar conference on ai</v>
<v Speaker 5>ethics.</v>

776
00:48:05.950 --> 00:48:09.310
<v Speaker 5>And so I think this is a good paradigm.</v>
<v Speaker 5>It's not foolproof.</v>

777
00:48:09.870 --> 00:48:14.870
<v Speaker 5>Uh,</v>
<v Speaker 5>I think the best way we can assure a </v>

778
00:48:16.060 --> 00:48:21.060
<v Speaker 5>democratic,</v>
<v Speaker 5>a future that includes ideas of liberty </v>

779
00:48:21.060 --> 00:48:25.201
<v Speaker 5>is to practice that in the world today </v>
<v Speaker 5>because the future world of the </v>

780
00:48:25.201 --> 00:48:29.011
<v Speaker 5>singularity,</v>
<v Speaker 5>which is a merger of biological </v>

781
00:48:29.011 --> 00:48:29.500
<v Speaker 5>biological intelligence,</v>
<v Speaker 5>uh,</v>

782
00:48:29.520 --> 00:48:31.300
<v Speaker 5>is not going to come from Mars.</v>
<v Speaker 5>I mean,</v>

783
00:48:31.301 --> 00:48:33.520
<v Speaker 5>it's going to emerge from our society </v>
<v Speaker 5>today.</v>

784
00:48:34.050 --> 00:48:36.310
<v Speaker 5>Uh,</v>
<v Speaker 5>so if we practice these ideals today,</v>

785
00:48:36.311 --> 00:48:41.311
<v Speaker 5>it's gonna have a higher chance of us </v>
<v Speaker 5>practicing them as we get more enhanced </v>

786
00:48:41.311 --> 00:48:42.730
<v Speaker 5>with technology.</v>
<v Speaker 5>Uh,</v>

787
00:48:42.731 --> 00:48:45.550
<v Speaker 5>that doesn't sound like a foolproof </v>
<v Speaker 5>solution or it isn't.</v>

788
00:48:45.551 --> 00:48:50.350
<v Speaker 5>But I think that's the best approach in </v>
<v Speaker 5>terms of technological solutions.</v>

789
00:48:50.710 --> 00:48:52.450
<v Speaker 5>I mean,</v>
<v Speaker 5>ai is the most daunting.</v>

790
00:48:52.451 --> 00:48:53.470
<v Speaker 5>You can imagine.</v>

791
00:48:53.800 --> 00:48:58.800
<v Speaker 5>A,</v>
<v Speaker 5>there are technical solutions to </v>

792
00:48:58.800 --> 00:49:00.190
<v Speaker 5>biotechnology and nanotechnology.</v>
<v Speaker 5>Uh,</v>

793
00:49:00.191 --> 00:49:05.191
<v Speaker 5>there's really no set routine.</v>
<v Speaker 5>You can put it in your ai software that </v>

794
00:49:05.191 --> 00:49:08.080
<v Speaker 5>will assure that have remained safe </v>
<v Speaker 5>intelligence.</v>

795
00:49:08.250 --> 00:49:13.250
<v Speaker 5>It's inherently not controllable.</v>
<v Speaker 5>There's some ai that's smarter than you </v>

796
00:49:13.250 --> 00:49:16.840
<v Speaker 5>that's out for your destruction.</v>
<v Speaker 5>The best way to deal with that is not to</v>

797
00:49:16.841 --> 00:49:19.480
<v Speaker 5>get in that situation in the first </v>
<v Speaker 5>place.</v>

798
00:49:20.590 --> 00:49:24.460
<v Speaker 5>If you are in that situation,</v>
<v Speaker 5>find some ai that will be on your side.</v>

799
00:49:25.030 --> 00:49:27.490
<v Speaker 5>Um,</v>
<v Speaker 5>but basically,</v>

800
00:49:27.700 --> 00:49:28.910
<v Speaker 5>uh,</v>
<v Speaker 5>it's going to it,</v>

801
00:49:28.990 --> 00:49:32.500
<v Speaker 5>I believe we have been headed through </v>
<v Speaker 5>technology,</v>

802
00:49:32.730 --> 00:49:33.390
<v Speaker 5>uh,</v>
<v Speaker 5>to a,</v>

803
00:49:33.670 --> 00:49:38.670
<v Speaker 5>to a better reality,</v>
<v Speaker 5>a look around the world and people </v>

804
00:49:38.670 --> 00:49:40.720
<v Speaker 5>really think things are getting worse.</v>
<v Speaker 5>Uh,</v>

805
00:49:40.721 --> 00:49:45.721
<v Speaker 5>and I think that's because our </v>
<v Speaker 5>information about what's wrong with the </v>

806
00:49:45.721 --> 00:49:46.300
<v Speaker 5>world is getting exponentially better.</v>
<v Speaker 5>I say,</v>

807
00:49:46.301 --> 00:49:51.301
<v Speaker 5>Oh,</v>
<v Speaker 5>this is the most peaceful time in human </v>

808
00:49:51.301 --> 00:49:51.301
<v Speaker 5>history in previously.</v>
<v Speaker 5>What are you crazy?</v>

809
00:49:51.301 --> 00:49:54.430
<v Speaker 5>Didn't you hear about the event </v>
<v Speaker 5>yesterday and last week and a while,</v>

810
00:49:54.510 --> 00:49:57.190
<v Speaker 5>a hundred years ago,</v>
<v Speaker 5>that could be a battle and wiped out the</v>

811
00:49:57.191 --> 00:50:02.191
<v Speaker 5>next village.</v>
<v Speaker 5>And you wouldn't even hear about it for </v>

812
00:50:02.191 --> 00:50:02.191
<v Speaker 5>months.</v>
<v Speaker 5>Uh,</v>

813
00:50:02.191 --> 00:50:06.080
<v Speaker 5>have all these graphs on education and </v>
<v Speaker 5>literacy has gone from 10 percent to 90 </v>

814
00:50:07.841 --> 00:50:12.841
<v Speaker 5>percent over a century and a health </v>
<v Speaker 5>wealth,</v>

815
00:50:15.140 --> 00:50:20.140
<v Speaker 5>uh,</v>
<v Speaker 5>poverty's declined 95 percent in Asia </v>

816
00:50:20.140 --> 00:50:23.110
<v Speaker 5>over the last 25 years was documentary </v>
<v Speaker 5>about the World Bank.</v>

817
00:50:23.620 --> 00:50:28.620
<v Speaker 5>All these trends are very smoothly </v>
<v Speaker 5>getting better and everybody thinks </v>

818
00:50:28.620 --> 00:50:28.620
<v Speaker 5>things are getting,</v>
<v Speaker 5>but,</v>

819
00:50:28.620 --> 00:50:29.540
<v Speaker 5>but,</v>
<v Speaker 5>but you're right,</v>

820
00:50:30.170 --> 00:50:34.910
<v Speaker 5>like on violence,</v>
<v Speaker 5>that curve could be quite disruptive.</v>

821
00:50:35.000 --> 00:50:38.270
<v Speaker 5>There's an existential event,</v>
<v Speaker 5>uh,</v>

822
00:50:38.600 --> 00:50:40.040
<v Speaker 5>as I say,</v>
<v Speaker 5>I'm optimistic,</v>

823
00:50:40.070 --> 00:50:45.070
<v Speaker 5>but I think that is something we need to</v>
<v Speaker 5>deal with and a lot of it is not </v>

824
00:50:45.141 --> 00:50:50.141
<v Speaker 5>technological,</v>
<v Speaker 5>it's dealing with our social cultural </v>

825
00:50:50.141 --> 00:50:50.141
<v Speaker 5>institutions.</v>

826
00:50:51.760 --> 00:50:56.170
<v Speaker 8>So you mentioned also exponential growth</v>
<v Speaker 8>of software and Ivs,</v>

827
00:50:56.171 --> 00:51:00.310
<v Speaker 8>I guess related to software.</v>
<v Speaker 8>So one of the reasons for which you said</v>

828
00:51:00.311 --> 00:51:05.311
<v Speaker 8>that all that information technology is </v>
<v Speaker 8>exponential is because of fundamental </v>

829
00:51:05.311 --> 00:51:08.350
<v Speaker 8>properties of matter and energy.</v>
<v Speaker 8>But in the case of ideas,</v>

830
00:51:08.380 --> 00:51:10.000
<v Speaker 8>why would it have to be exponential?</v>

831
00:51:10.950 --> 00:51:15.950
<v Speaker 5>Well,</v>
<v Speaker 5>a lot of ideas produce exponential </v>

832
00:51:15.950 --> 00:51:18.800
<v Speaker 5>gains.</v>
<v Speaker 5>They don't increase performance </v>

833
00:51:18.800 --> 00:51:23.241
<v Speaker 5>linearly.</v>
<v Speaker 5>There's a case study during the Obama </v>

834
00:51:23.241 --> 00:51:26.660
<v Speaker 5>Administration by the scientific </v>
<v Speaker 5>advisory board on assessing this </v>

835
00:51:26.660 --> 00:51:31.071
<v Speaker 5>question,</v>
<v Speaker 5>how much gains on 23 classical </v>

836
00:51:31.071 --> 00:51:35.271
<v Speaker 5>engineering problems were gained through</v>
<v Speaker 5>hardware improvements over the last </v>

837
00:51:37.830 --> 00:51:42.830
<v Speaker 5>decade and software improvement.</v>
<v Speaker 5>So there's about a thousand to one </v>

838
00:51:42.830 --> 00:51:45.450
<v Speaker 5>improvement.</v>
<v Speaker 5>It's about doubling every year from </v>

839
00:51:45.450 --> 00:51:46.510
<v Speaker 5>hardware that was an average of </v>
<v Speaker 5>something like 26,000</v>

840
00:51:46.560 --> 00:51:50.340
<v Speaker 5>to one through software improvements,</v>
<v Speaker 5>algorithmic improvements.</v>

841
00:51:52.380 --> 00:51:57.380
<v Speaker 5>So we do see both ends.</v>
<v Speaker 5>Apparently if you come up with an </v>

842
00:51:57.380 --> 00:52:01.881
<v Speaker 5>advance,</v>
<v Speaker 5>it doubles the performance so it </v>

843
00:52:01.881 --> 00:52:03.870
<v Speaker 5>multiplies it by 10.</v>
<v Speaker 5>We see basically exponential growth from</v>

844
00:52:03.871 --> 00:52:05.950
<v Speaker 5>each innovation.</v>
<v Speaker 5>Um,</v>

845
00:52:07.350 --> 00:52:12.350
<v Speaker 5>so,</v>
<v Speaker 5>and we certainly see that in deep </v>

846
00:52:12.350 --> 00:52:12.350
<v Speaker 5>learning.</v>
<v Speaker 5>Uh,</v>

847
00:52:12.350 --> 00:52:15.980
<v Speaker 5>the architectures are getting better </v>
<v Speaker 5>while we also have more data and more </v>

848
00:52:15.980 --> 00:52:19.260
<v Speaker 5>computation and more memory to throw at </v>
<v Speaker 5>these at these algorithms.</v>

849
00:52:19.610 --> 00:52:20.480
<v Speaker 5>Thank you very much.</v>

850
00:52:23.540 --> 00:52:26.210
<v Speaker 3>Thank you.</v>

