WEBVTT

1
00:00:00.030 --> 00:00:02.220
<v Speaker 1>As part of Mit core six zero,</v>
<v Speaker 1>nine,</v>

2
00:00:02.221 --> 00:00:07.221
<v Speaker 1>nine artificial general intelligence,</v>
<v Speaker 1>I've gotten the chance to sit down with </v>

3
00:00:07.221 --> 00:00:08.280
<v Speaker 1>Max tegmark.</v>
<v Speaker 1>He is a professor he hit on my tea,</v>

4
00:00:08.670 --> 00:00:13.670
<v Speaker 1>is a physicist,</v>
<v Speaker 1>spent a large part of his career </v>

5
00:00:13.670 --> 00:00:16.530
<v Speaker 1>studying the mysteries of our </v>
<v Speaker 1>cosmological universe,</v>

6
00:00:16.950 --> 00:00:21.600
<v Speaker 1>but he's also studied in,</v>
<v Speaker 1>delved into the beneficial possibilities</v>

7
00:00:21.660 --> 00:00:26.660
<v Speaker 1>and the existential risks of artificial </v>
<v Speaker 1>intelligence amongst many other things.</v>

8
00:00:27.211 --> 00:00:31.710
<v Speaker 1>He's the cofounder of the future of Life</v>
<v Speaker 1>Institute,</v>

9
00:00:31.860 --> 00:00:34.740
<v Speaker 1>author of two books,</v>
<v Speaker 1>both of which I highly recommend.</v>

10
00:00:35.160 --> 00:00:38.880
<v Speaker 1>First our mathematical universe.</v>
<v Speaker 1>Second is life,</v>

11
00:00:38.881 --> 00:00:42.510
<v Speaker 1>three point zero.</v>
<v Speaker 1>He's truly an out of the box thinker and</v>

12
00:00:42.870 --> 00:00:45.390
<v Speaker 1>a fun personality,</v>
<v Speaker 1>so I really enjoyed talking to him.</v>

13
00:00:45.450 --> 00:00:47.730
<v Speaker 1>If you'd like to see more of these </v>
<v Speaker 1>videos in the future,</v>

14
00:00:47.970 --> 00:00:52.970
<v Speaker 1>please subscribe and also click the </v>
<v Speaker 1>little bell icon to make sure you don't </v>

15
00:00:52.970 --> 00:00:53.430
<v Speaker 1>miss any videos.</v>
<v Speaker 1>Also,</v>

16
00:00:53.550 --> 00:00:54.900
<v Speaker 1>twitter,</v>
<v Speaker 1>linkedin,</v>

17
00:00:55.140 --> 00:00:59.580
<v Speaker 1>Agi.mit.edu.</v>
<v Speaker 1>If you want to watch other lectures,</v>

18
00:00:59.581 --> 00:01:01.680
<v Speaker 1>are conversations like this one better </v>
<v Speaker 1>yet?</v>

19
00:01:01.770 --> 00:01:03.180
<v Speaker 1>Go read Max's book,</v>
<v Speaker 1>light.</v>

20
00:01:03.181 --> 00:01:07.440
<v Speaker 1>Three point zero.</v>
<v Speaker 1>Chapter seven on goals is my favorite.</v>

21
00:01:07.890 --> 00:01:12.890
<v Speaker 1>It's really more philosophy and </v>
<v Speaker 1>engineering come together and it opens </v>

22
00:01:12.890 --> 00:01:13.240
<v Speaker 1>with a quote by dusty.</v>
<v Speaker 1>See,</v>

23
00:01:14.340 --> 00:01:17.610
<v Speaker 1>the mystery of human existence lies not </v>
<v Speaker 1>in just staying alive,</v>

24
00:01:17.880 --> 00:01:21.090
<v Speaker 1>but in finding something to live for.</v>
<v Speaker 1>Lastly,</v>

25
00:01:21.270 --> 00:01:26.270
<v Speaker 1>I believe that every failure rewards us </v>
<v Speaker 1>with an opportunity to learn and that </v>

26
00:01:26.821 --> 00:01:31.821
<v Speaker 1>sense.</v>
<v Speaker 1>I've been very fortunate to fail and so </v>

27
00:01:31.821 --> 00:01:32.340
<v Speaker 1>many new and exciting ways and,</v>
<v Speaker 1>uh,</v>

28
00:01:32.341 --> 00:01:37.341
<v Speaker 1>this conversation was no different.</v>
<v Speaker 1>I've learned about something called </v>

29
00:01:37.341 --> 00:01:39.750
<v Speaker 1>radio frequency interference,</v>
<v Speaker 1>RFI.</v>

30
00:01:39.930 --> 00:01:42.880
<v Speaker 1>Look it up.</v>
<v Speaker 1>Apparently music and conversations,</v>

31
00:01:42.881 --> 00:01:46.890
<v Speaker 1>some local radio stations can bleed into</v>
<v Speaker 1>the audio that we're recording in such a</v>

32
00:01:46.891 --> 00:01:49.080
<v Speaker 1>way that almost completely ruins that </v>
<v Speaker 1>audio.</v>

33
00:01:49.290 --> 00:01:51.810
<v Speaker 1>It's an exceptionally difficult sound </v>
<v Speaker 1>source to remove,</v>

34
00:01:53.160 --> 00:01:58.160
<v Speaker 1>so I've gotten the opportunity to learn </v>
<v Speaker 1>how to avoid rfi in the future.</v>

35
00:01:59.071 --> 00:02:04.071
<v Speaker 1>During the recording sessions have also </v>
<v Speaker 1>gotten the opportunity to learn how to </v>

36
00:02:04.071 --> 00:02:07.851
<v Speaker 1>use adobe audition and isotope rx six to</v>
<v Speaker 1>do some noise,</v>

37
00:02:09.330 --> 00:02:12.210
<v Speaker 1>some audio repair.</v>
<v Speaker 1>Of course,</v>

38
00:02:12.211 --> 00:02:14.940
<v Speaker 1>this is exceptionally difficult noise to</v>
<v Speaker 1>remove.</v>

39
00:02:14.970 --> 00:02:17.700
<v Speaker 1>I am an engineer.</v>
<v Speaker 1>I'm not an audio engineer.</v>

40
00:02:18.230 --> 00:02:21.390
<v Speaker 1>Neither is anybody else in our group,</v>
<v Speaker 1>but would a best.</v>

41
00:02:21.840 --> 00:02:25.680
<v Speaker 1>Nevertheless,</v>
<v Speaker 1>I thank you for your patience and I hope</v>

42
00:02:25.681 --> 00:02:27.450
<v Speaker 1>you're still able to enjoy this </v>
<v Speaker 1>conversation.</v>

43
00:02:27.930 --> 00:02:30.270
<v Speaker 1>Do you think there's intelligent life </v>
<v Speaker 1>out there in the universe?</v>

44
00:02:31.330 --> 00:02:35.940
<v Speaker 2>Let's open up with an easy question.</v>
<v Speaker 2>I have a minority view here.</v>

45
00:02:35.950 --> 00:02:37.600
<v Speaker 2>Actually,</v>
<v Speaker 2>when I give public lectures,</v>

46
00:02:37.880 --> 00:02:42.880
<v Speaker 2>I often ask for a show of hands who </v>
<v Speaker 2>thinks there's intelligent life out </v>

47
00:02:42.880 --> 00:02:46.510
<v Speaker 2>there somewhere else and almost everyone</v>
<v Speaker 2>put their hands up and when I ask why,</v>

48
00:02:46.930 --> 00:02:47.710
<v Speaker 2>they'll be like,</v>
<v Speaker 2>oh,</v>

49
00:02:47.711 --> 00:02:50.740
<v Speaker 2>there's so many galaxies out there.</v>
<v Speaker 2>There's gonna be.</v>

50
00:02:51.790 --> 00:02:53.890
<v Speaker 2>But I'm a numbers nerd.</v>
<v Speaker 2>Right?</v>

51
00:02:54.490 --> 00:02:58.620
<v Speaker 2>So when you look more carefully at it,</v>
<v Speaker 2>it's not so clear at all the.</v>

52
00:02:59.080 --> 00:03:00.670
<v Speaker 2>When we talk about our universe,</v>
<v Speaker 2>first of all,</v>

53
00:03:00.671 --> 00:03:04.060
<v Speaker 2>we don't mean all of space.</v>
<v Speaker 2>Do we actually mean I don't know.</v>

54
00:03:04.061 --> 00:03:06.130
<v Speaker 2>You can throw me in the university.</v>
<v Speaker 2>She wants to behind you there.</v>

55
00:03:07.250 --> 00:03:10.330
<v Speaker 2>It's,</v>
<v Speaker 2>we'd simply mean the spherical region of</v>

56
00:03:10.331 --> 00:03:15.331
<v Speaker 2>space from which light has to reach us.</v>
<v Speaker 2>So far during the 14 point 8 billion </v>

57
00:03:16.661 --> 00:03:21.661
<v Speaker 2>year,</v>
<v Speaker 2>13 point 8 billion years since our big </v>

58
00:03:21.661 --> 00:03:21.661
<v Speaker 2>bang,</v>
<v Speaker 2>there's more space here,</v>

59
00:03:21.661 --> 00:03:23.620
<v Speaker 2>but this is what we call a universe </v>
<v Speaker 2>because that's all we have access to.</v>

60
00:03:24.010 --> 00:03:29.010
<v Speaker 2>So is there intelligent life here that's</v>
<v Speaker 2>gotten to the point of building </v>

61
00:03:29.010 --> 00:03:33.040
<v Speaker 2>telescopes and computers?</v>
<v Speaker 2>My guess is no,</v>

62
00:03:33.140 --> 00:03:36.360
<v Speaker 2>actually that the probability of it </v>
<v Speaker 2>happening on it,</v>

63
00:03:36.640 --> 00:03:37.690
<v Speaker 2>any given planet,</v>

64
00:03:39.200 --> 00:03:44.200
<v Speaker 2>there's some number we don't know what </v>
<v Speaker 2>it is and what we do know is that the </v>

65
00:03:47.090 --> 00:03:50.420
<v Speaker 2>number can be super high because there's</v>
<v Speaker 2>over a billion earth like planets in the</v>

66
00:03:50.430 --> 00:03:52.460
<v Speaker 2>mill keyway,</v>
<v Speaker 2>galaxy alone,</v>

67
00:03:52.820 --> 00:03:55.610
<v Speaker 2>many of which are billions of years </v>
<v Speaker 2>older than earth.</v>

68
00:03:56.210 --> 00:04:01.210
<v Speaker 2>And um,</v>
<v Speaker 2>aside from some of you will for </v>

69
00:04:01.210 --> 00:04:01.210
<v Speaker 2>believers,</v>
<v Speaker 2>you know,</v>

70
00:04:01.210 --> 00:04:05.531
<v Speaker 2>there isn't much evidence that any </v>
<v Speaker 2>superintendent civilization has come </v>

71
00:04:05.531 --> 00:04:07.670
<v Speaker 2>here at all.</v>
<v Speaker 2>And so that's the famous Fermi paradox.</v>

72
00:04:07.671 --> 00:04:09.140
<v Speaker 2>Right?</v>
<v Speaker 2>And then if you,</v>

73
00:04:09.141 --> 00:04:11.630
<v Speaker 2>if you work the numbers,</v>
<v Speaker 2>what you find is that the,</v>

74
00:04:12.440 --> 00:04:15.800
<v Speaker 2>if you have no clue what the probability</v>
<v Speaker 2>is of getting life on a given planet.</v>

75
00:04:16.800 --> 00:04:21.800
<v Speaker 2>So it could be 10 to the minus 10 to the</v>
<v Speaker 2>minus 20 or temperament is to any power </v>

76
00:04:22.201 --> 00:04:24.960
<v Speaker 2>of 10 is sort of equally likely if you </v>
<v Speaker 2>want to be really open minded,</v>

77
00:04:25.500 --> 00:04:30.500
<v Speaker 2>that translates into it being equally </v>
<v Speaker 2>likely that our nearest neighbor is 10 </v>

78
00:04:30.500 --> 00:04:34.581
<v Speaker 2>to the 16 meters away,</v>
<v Speaker 2>tend to the 70 meters away and of 18 </v>

79
00:04:35.010 --> 00:04:40.010
<v Speaker 2>don't.</v>
<v Speaker 2>By the time he gets much less than 10,</v>

80
00:04:40.090 --> 00:04:42.220
<v Speaker 2>16 already,</v>
<v Speaker 2>we pretty much.</v>

81
00:04:42.221 --> 00:04:45.080
<v Speaker 2>No,</v>
<v Speaker 2>there is nothing else that's close.</v>

82
00:04:45.940 --> 00:04:48.370
<v Speaker 2>And when you get the opposite would have</v>
<v Speaker 2>discovered us.</v>

83
00:04:48.700 --> 00:04:53.700
<v Speaker 2>Yeah,</v>
<v Speaker 2>they would have been discovered as long </v>

84
00:04:53.700 --> 00:04:55.171
<v Speaker 2>ago or if they're really close,</v>
<v Speaker 2>we would have probably noted some </v>

85
00:04:55.171 --> 00:04:57.691
<v Speaker 2>engineering projects that they're doing </v>
<v Speaker 2>and if it's beyond 10 to 26 meters </v>

86
00:04:57.910 --> 00:05:02.910
<v Speaker 2>that's already outside of here.</v>
<v Speaker 2>So my guess is actually that there are,</v>

87
00:05:04.320 --> 00:05:09.320
<v Speaker 2>we are the only life in here.</v>
<v Speaker 2>They've gotten to the point of building </v>

88
00:05:09.320 --> 00:05:09.920
<v Speaker 2>advanced tech,</v>
<v Speaker 2>which I think is,</v>

89
00:05:09.921 --> 00:05:10.910
<v Speaker 2>is very,</v>
<v Speaker 2>um,</v>

90
00:05:12.710 --> 00:05:15.170
<v Speaker 2>puts a lot of responsibility on our </v>
<v Speaker 2>shoulders not screw up.</v>

91
00:05:15.171 --> 00:05:20.171
<v Speaker 2>You know,</v>
<v Speaker 2>I think actually people who take for </v>

92
00:05:20.171 --> 00:05:20.171
<v Speaker 2>granted that it's okay for us to screw </v>
<v Speaker 2>up,</v>

93
00:05:20.171 --> 00:05:25.121
<v Speaker 2>have an accidental nuclear war or go </v>
<v Speaker 2>extinct somehow because there's a sort </v>

94
00:05:25.121 --> 00:05:28.631
<v Speaker 2>of star trek like situation out there </v>
<v Speaker 2>with some other life forms are going to </v>

95
00:05:28.631 --> 00:05:30.140
<v Speaker 2>come and bail us out.</v>
<v Speaker 2>And it doesn't matter as much.</v>

96
00:05:30.410 --> 00:05:32.900
<v Speaker 2>I think allowing us into a false sense </v>
<v Speaker 2>of security.</v>

97
00:05:33.400 --> 00:05:35.220
<v Speaker 2>I think it's much more prudent to say,</v>
<v Speaker 2>you know,</v>

98
00:05:35.221 --> 00:05:38.180
<v Speaker 2>let's be really grateful for this </v>
<v Speaker 2>amazing opportunity we've had.</v>

99
00:05:38.720 --> 00:05:43.720
<v Speaker 2>And um,</v>
<v Speaker 2>makes the best of it just in case it is </v>

100
00:05:43.720 --> 00:05:45.650
<v Speaker 2>down to us.</v>
<v Speaker 2>So from a physics perspective,</v>

101
00:05:45.680 --> 00:05:50.680
<v Speaker 2>do you think intelligent life says </v>
<v Speaker 2>unique from a sort of statistical view </v>

102
00:05:51.321 --> 00:05:56.321
<v Speaker 2>of the size of the universe,</v>
<v Speaker 2>but from the basic matter of the </v>

103
00:05:56.321 --> 00:05:58.070
<v Speaker 2>universe,</v>
<v Speaker 2>how difficult is it for intelligent</v>

104
00:05:58.070 --> 00:06:03.070
<v Speaker 1>life to come about?</v>
<v Speaker 1>Was the kind of advanced tech building </v>

105
00:06:03.070 --> 00:06:05.681
<v Speaker 1>life is implied in your statement that </v>
<v Speaker 1>it's really difficult to create </v>

106
00:06:06.051 --> 00:06:07.550
<v Speaker 1>something like a human species?</v>

107
00:06:08.010 --> 00:06:13.010
<v Speaker 2>I think what we know is that going from </v>
<v Speaker 2>no life to having life that can do </v>

108
00:06:14.060 --> 00:06:19.060
<v Speaker 2>archive level of tech,</v>
<v Speaker 2>there was some sort of to going beyond </v>

109
00:06:19.060 --> 00:06:23.441
<v Speaker 2>that.</v>
<v Speaker 2>Then it actually settling our whole </v>

110
00:06:23.441 --> 00:06:25.811
<v Speaker 2>universe with life.</v>
<v Speaker 2>There's some road major roadblock there </v>

111
00:06:26.480 --> 00:06:31.170
<v Speaker 2>which is some great filter as um,</v>
<v Speaker 2>it's sometimes called the which,</v>

112
00:06:31.610 --> 00:06:36.610
<v Speaker 2>which is tough to get through.</v>
<v Speaker 2>It's either that roadblock is either be </v>

113
00:06:36.610 --> 00:06:38.360
<v Speaker 2>behind us or in front of us.</v>
<v Speaker 2>Right?</v>

114
00:06:38.660 --> 00:06:40.730
<v Speaker 2>I'm hoping very much that it's behind </v>
<v Speaker 2>us.</v>

115
00:06:40.960 --> 00:06:42.950
<v Speaker 2>I'm,</v>
<v Speaker 2>I'm super excited.</v>

116
00:06:42.951 --> 00:06:47.951
<v Speaker 2>Every time we get a new report from NASA</v>
<v Speaker 2>saying they failed to find any life on </v>

117
00:06:47.951 --> 00:06:49.080
<v Speaker 2>Mars,</v>
<v Speaker 2>like yes,</v>

118
00:06:49.970 --> 00:06:51.520
<v Speaker 2>because that suggests that the hard </v>
<v Speaker 2>part.</v>

119
00:06:51.560 --> 00:06:54.710
<v Speaker 2>Maybe what maybe it was getting the </v>
<v Speaker 2>first right Bozon or or something.</v>

120
00:06:54.860 --> 00:06:58.820
<v Speaker 2>Some very low level kind of stepping </v>
<v Speaker 2>stone,</v>

121
00:06:59.390 --> 00:07:01.280
<v Speaker 2>so they were home free because if that's</v>
<v Speaker 2>true,</v>

122
00:07:01.580 --> 00:07:04.910
<v Speaker 2>then the future is really only limited </v>
<v Speaker 2>by our own imagination.</v>

123
00:07:05.090 --> 00:07:10.090
<v Speaker 2>It would be much suckier if it turns out</v>
<v Speaker 2>that this level of life is kind of a </v>

124
00:07:10.090 --> 00:07:12.650
<v Speaker 2>diamond dozen,</v>
<v Speaker 2>but maybe there's some other problem,</v>

125
00:07:12.651 --> 00:07:16.880
<v Speaker 2>like as soon as a civilization gets </v>
<v Speaker 2>advanced technology within 100 years,</v>

126
00:07:16.881 --> 00:07:19.160
<v Speaker 2>they get into some stupid fight with </v>
<v Speaker 2>themselves and poof.</v>

127
00:07:19.320 --> 00:07:20.930
<v Speaker 2>Yep.</v>
<v Speaker 2>That would be a bummer.</v>

128
00:07:21.650 --> 00:07:22.700
<v Speaker 2>Yeah.</v>
<v Speaker 2>So</v>

129
00:07:23.000 --> 00:07:27.260
<v Speaker 1>you've explored the mysteries of the </v>
<v Speaker 1>universe of the cosmological universe,</v>

130
00:07:27.261 --> 00:07:28.610
<v Speaker 1>the one that's sitting</v>

131
00:07:28.850 --> 00:07:33.850
<v Speaker 2>between us today.</v>
<v Speaker 2>I think you've also have begun to </v>

132
00:07:33.850 --> 00:07:33.920
<v Speaker 2>explore</v>

133
00:07:34.220 --> 00:07:37.910
<v Speaker 1>the other universe,</v>
<v Speaker 1>which is sort of the mystery,</v>

134
00:07:37.911 --> 00:07:40.850
<v Speaker 1>the mysterious universe of the mind of </v>
<v Speaker 1>intelligence,</v>

135
00:07:40.851 --> 00:07:44.540
<v Speaker 1>of intelligent life.</v>
<v Speaker 1>So is there a common thread between your</v>

136
00:07:44.541 --> 00:07:48.260
<v Speaker 1>interests are in the way you think about</v>
<v Speaker 1>space and intelligence?</v>

137
00:07:48.610 --> 00:07:53.610
<v Speaker 2>Oh yeah.</v>
<v Speaker 2>When I was a teenager I was already very</v>

138
00:07:54.191 --> 00:07:59.170
<v Speaker 2>fascinated by the biggest questions and </v>
<v Speaker 2>I felt that the two biggest mysteries of</v>

139
00:07:59.171 --> 00:08:03.940
<v Speaker 2>all in science where our universe out </v>
<v Speaker 2>there and our universe in here.</v>

140
00:08:04.050 --> 00:08:07.030
<v Speaker 2>Yup.</v>
<v Speaker 2>So it's quite natural after having spent</v>

141
00:08:08.080 --> 00:08:12.100
<v Speaker 2>quarter of a century on my career,</v>
<v Speaker 2>thinking a lot about this one,</v>

142
00:08:12.630 --> 00:08:15.910
<v Speaker 2>I'm now indulging in the luxury of doing</v>
<v Speaker 2>research on this one.</v>

143
00:08:15.911 --> 00:08:19.990
<v Speaker 2>It's just so cool.</v>
<v Speaker 2>I feel the time is ripe now.</v>

144
00:08:20.100 --> 00:08:25.100
<v Speaker 2>Refer you directly.</v>
<v Speaker 2>Deepening our understanding of this to </v>

145
00:08:25.241 --> 00:08:26.740
<v Speaker 2>start exploring this one.</v>
<v Speaker 2>Yeah,</v>

146
00:08:26.741 --> 00:08:31.741
<v Speaker 2>because I think I think a lot of people </v>
<v Speaker 2>view intelligence as something </v>

147
00:08:31.741 --> 00:08:36.361
<v Speaker 2>mysterious that can only exist in </v>
<v Speaker 2>biological organisms like us and </v>

148
00:08:36.361 --> 00:08:41.281
<v Speaker 2>therefor dismiss all.</v>
<v Speaker 2>Talk about artificial general </v>

149
00:08:41.281 --> 00:08:42.850
<v Speaker 2>intelligence is science fiction,</v>
<v Speaker 2>but from my perspective as a physicist,</v>

150
00:08:42.970 --> 00:08:47.970
<v Speaker 2>you know I am a blob of corks and </v>
<v Speaker 2>electrons moving around in a certain </v>

151
00:08:47.970 --> 00:08:52.951
<v Speaker 2>pattern and process the information in </v>
<v Speaker 2>certain ways and this is also a blob of </v>

152
00:08:52.951 --> 00:08:52.960
<v Speaker 2>quirks that electrons.</v>

153
00:08:53.590 --> 00:08:57.330
<v Speaker 2>I'm not smarter than the water because I</v>
<v Speaker 2>made a different kind of works.</v>

154
00:08:57.440 --> 00:08:59.640
<v Speaker 2>Right?</v>
<v Speaker 2>I made up quirks and down quirks.</v>

155
00:08:59.641 --> 00:09:03.420
<v Speaker 2>Exact same kind as this.</v>
<v Speaker 2>It's A.</v>

156
00:09:03.420 --> 00:09:06.900
<v Speaker 2>There's no secret sauce.</v>
<v Speaker 2>I think in me it's all about the pattern</v>

157
00:09:06.901 --> 00:09:11.901
<v Speaker 2>of the information processing and this </v>
<v Speaker 2>means that there's no law of physics </v>

158
00:09:12.271 --> 00:09:17.271
<v Speaker 2>saying that we can't create technology </v>
<v Speaker 2>which can help us by being incredibly </v>

159
00:09:19.381 --> 00:09:21.540
<v Speaker 2>intelligent than help us crack Mr.</v>
<v Speaker 2>so we couldn't.</v>

160
00:09:21.690 --> 00:09:26.690
<v Speaker 2>In other words,</v>
<v Speaker 2>I think we've really only seen the tip </v>

161
00:09:26.690 --> 00:09:26.690
<v Speaker 2>of,</v>
<v Speaker 2>of intelligence iceberg so far.</v>

162
00:09:26.690 --> 00:09:30.480
<v Speaker 2>Yeah.</v>
<v Speaker 2>So the perceptronium yeah.</v>

163
00:09:31.050 --> 00:09:33.490
<v Speaker 2>Uh,</v>
<v Speaker 2>so you current coin this amazing term as</v>

164
00:09:33.500 --> 00:09:38.400
<v Speaker 2>a hypothetical state of matter sort of </v>
<v Speaker 2>thinking from a physics perspective,</v>

165
00:09:38.401 --> 00:09:39.990
<v Speaker 2>what is the kind of matter that can </v>
<v Speaker 2>help,</v>

166
00:09:40.080 --> 00:09:42.540
<v Speaker 2>as you're saying a subjective </v>
<v Speaker 2>experience,</v>

167
00:09:42.541 --> 00:09:43.980
<v Speaker 2>emerge?</v>
<v Speaker 2>Consciousness emerge.</v>

168
00:09:44.280 --> 00:09:47.940
<v Speaker 2>So how do you think about consciousness </v>
<v Speaker 2>from this physics perspective?</v>

169
00:09:49.980 --> 00:09:53.280
<v Speaker 2>Very good question.</v>
<v Speaker 2>So again,</v>

170
00:09:53.281 --> 00:09:54.030
<v Speaker 2>I'm.</v>
<v Speaker 2>No,</v>

171
00:09:54.050 --> 00:09:54.390
<v Speaker 2>I think</v>

172
00:09:56.090 --> 00:10:01.090
<v Speaker 2>many people have underestimated our </v>
<v Speaker 2>ability to make progress on this by </v>

173
00:10:02.630 --> 00:10:07.630
<v Speaker 2>convincing themselves it's hopeless </v>
<v Speaker 2>because somehow we're missing some </v>

174
00:10:07.630 --> 00:10:10.070
<v Speaker 2>ingredients that we need.</v>
<v Speaker 2>There's some new consciousness,</v>

175
00:10:10.071 --> 00:10:15.071
<v Speaker 2>part it go or whatever.</v>
<v Speaker 2>I happened to think that we're not </v>

176
00:10:15.071 --> 00:10:19.661
<v Speaker 2>missing anything and that it's not.</v>
<v Speaker 2>The interesting thing about </v>

177
00:10:19.661 --> 00:10:24.640
<v Speaker 2>consciousness that gives us this amazing</v>
<v Speaker 2>subjective experience of colors and </v>

178
00:10:24.640 --> 00:10:29.471
<v Speaker 2>sounds and emotions on is rather </v>
<v Speaker 2>something at the higher level about the </v>

179
00:10:30.441 --> 00:10:34.010
<v Speaker 2>patterns of information processing and </v>
<v Speaker 2>that's why I.</v>

180
00:10:34.130 --> 00:10:37.970
<v Speaker 2>that's why I am like thinking about this</v>
<v Speaker 2>idea of perceptronium.</v>

181
00:10:38.120 --> 00:10:42.560
<v Speaker 2>What does it mean for an arbitrary </v>
<v Speaker 2>physical system to be conscious in terms</v>

182
00:10:42.561 --> 00:10:45.830
<v Speaker 2>of what it's particle to do anything or,</v>
<v Speaker 2>or,</v>

183
00:10:45.980 --> 00:10:47.600
<v Speaker 2>or it's inflammation is doing.</v>
<v Speaker 2>I don't think.</v>

184
00:10:47.880 --> 00:10:49.550
<v Speaker 2>I don't.</v>
<v Speaker 2>I hate the carbon chauvinism.</v>

185
00:10:49.551 --> 00:10:50.330
<v Speaker 2>You know,</v>
<v Speaker 2>this attitude.</v>

186
00:10:50.331 --> 00:10:53.510
<v Speaker 2>You have to be made of carbon atoms to </v>
<v Speaker 2>be smart or conscious.</v>

187
00:10:54.090 --> 00:10:58.420
<v Speaker 2>Something about the information </v>
<v Speaker 2>processing kind of matter performs.</v>

188
00:10:58.430 --> 00:10:59.110
<v Speaker 2>Yeah.</v>
<v Speaker 2>And you know,</v>

189
00:10:59.120 --> 00:11:04.120
<v Speaker 2>you can see I have my favorite equations</v>
<v Speaker 2>here describing various fundamental </v>

190
00:11:04.120 --> 00:11:07.751
<v Speaker 2>aspects of the world.</v>
<v Speaker 2>I feel that I think one day maybe </v>

191
00:11:07.751 --> 00:11:11.201
<v Speaker 2>someone is watching this or come up with</v>
<v Speaker 2>the equations that information </v>

192
00:11:11.201 --> 00:11:12.080
<v Speaker 2>processing has to satisfy to be </v>
<v Speaker 2>consciously.</v>

193
00:11:12.081 --> 00:11:17.081
<v Speaker 2>I'm quite convinced there is big </v>
<v Speaker 2>discovery to be made there because let's</v>

194
00:11:17.691 --> 00:11:22.691
<v Speaker 2>face it,</v>
<v Speaker 2>some sometimes we know that some </v>

195
00:11:22.691 --> 00:11:25.151
<v Speaker 2>information processing is conscious </v>
<v Speaker 2>because we are conscious,</v>

196
00:11:25.730 --> 00:11:28.580
<v Speaker 2>but we also know that a lot of </v>
<v Speaker 2>information processing is not conscious.</v>

197
00:11:28.581 --> 00:11:32.420
<v Speaker 2>Like most of the information presently </v>
<v Speaker 2>happening in your brain right now is not</v>

198
00:11:32.421 --> 00:11:35.960
<v Speaker 2>conscious.</v>
<v Speaker 2>They like 10 megabytes per second coming</v>

199
00:11:35.961 --> 00:11:40.961
<v Speaker 2>in even just through your visual system </v>
<v Speaker 2>and you are not conscious about your </v>

200
00:11:40.961 --> 00:11:41.780
<v Speaker 2>heartbeat regulation or,</v>
<v Speaker 2>or most things,</v>

201
00:11:41.840 --> 00:11:42.400
<v Speaker 2>right?</v>
<v Speaker 2>Uh,</v>

202
00:11:42.410 --> 00:11:45.350
<v Speaker 2>even,</v>
<v Speaker 2>even like if I just ask you to read what</v>

203
00:11:45.351 --> 00:11:46.790
<v Speaker 2>it says here,</v>
<v Speaker 2>you look at it and then,</v>

204
00:11:46.850 --> 00:11:48.020
<v Speaker 2>oh,</v>
<v Speaker 2>now you know what it said,</v>

205
00:11:48.200 --> 00:11:50.690
<v Speaker 2>but you're not aware of how the </v>
<v Speaker 2>computation actually happened.</v>

206
00:11:51.030 --> 00:11:56.030
<v Speaker 2>You're like,</v>
<v Speaker 2>to your consciousness is like the that </v>

207
00:11:56.030 --> 00:11:56.050
<v Speaker 2>got an email at the end with a final </v>
<v Speaker 2>answer.</v>

208
00:11:56.800 --> 00:12:01.800
<v Speaker 2>So what is it that makes a difference?</v>
<v Speaker 2>I think that's both those great science </v>

209
00:12:04.421 --> 00:12:09.421
<v Speaker 2>mystery.</v>
<v Speaker 2>We're actually studying it a little bit </v>

210
00:12:09.421 --> 00:12:11.551
<v Speaker 2>in my lab here at mit a.</v>
<v Speaker 2>But I also think it's just a really </v>

211
00:12:11.551 --> 00:12:12.970
<v Speaker 2>urgent question answer for starters.</v>
<v Speaker 2>I mean,</v>

212
00:12:12.971 --> 00:12:17.971
<v Speaker 2>if you're an emergency room doctor and </v>
<v Speaker 2>you have an unresponsive patient coming </v>

213
00:12:17.971 --> 00:12:21.811
<v Speaker 2>in,</v>
<v Speaker 2>wouldn't it be great if in addition to </v>

214
00:12:21.811 --> 00:12:21.811
<v Speaker 2>having</v>

215
00:12:22.360 --> 00:12:23.480
<v Speaker 2>a ct scanner,</v>
<v Speaker 2>you,</v>

216
00:12:23.530 --> 00:12:28.530
<v Speaker 2>you had a conscience of scanner that you</v>
<v Speaker 2>figure out whether this person is </v>

217
00:12:28.530 --> 00:12:30.850
<v Speaker 2>actually having locked in syndrome,</v>
<v Speaker 2>right?</v>

218
00:12:31.020 --> 00:12:33.130
<v Speaker 2>Whereas actually comatose,</v>
<v Speaker 2>uh,</v>

219
00:12:33.380 --> 00:12:38.380
<v Speaker 2>and in the future,</v>
<v Speaker 2>imagine if we build the robots or the </v>

220
00:12:38.380 --> 00:12:38.380
<v Speaker 2>machine that</v>

221
00:12:39.820 --> 00:12:42.010
<v Speaker 2>we could have really good conversations </v>
<v Speaker 2>with students.</v>

222
00:12:42.040 --> 00:12:44.440
<v Speaker 2>I think it's very,</v>
<v Speaker 2>very likely to happen.</v>

223
00:12:44.441 --> 00:12:45.760
<v Speaker 2>Right?</v>
<v Speaker 2>Wouldn't you want to know,</v>

224
00:12:45.761 --> 00:12:50.761
<v Speaker 2>like if you're a home health,</v>
<v Speaker 2>a robot is actually experiencing </v>

225
00:12:50.761 --> 00:12:51.580
<v Speaker 2>anything or just like a Zombie,</v>
<v Speaker 2>I mean,</v>

226
00:12:52.630 --> 00:12:54.160
<v Speaker 2>would you prefer.</v>
<v Speaker 2>What would you prefer?</v>

227
00:12:54.161 --> 00:12:59.161
<v Speaker 2>Would you prefer that it's actually </v>
<v Speaker 2>unconscious so that you don't have to </v>

228
00:12:59.161 --> 00:13:02.161
<v Speaker 2>feel guilty about switching it off or </v>
<v Speaker 2>giving boring chores or what would you </v>

229
00:13:02.161 --> 00:13:02.530
<v Speaker 2>prefer?</v>
<v Speaker 2>Well,</v>

230
00:13:02.590 --> 00:13:06.490
<v Speaker 2>the certainly would,</v>
<v Speaker 2>we would prefer,</v>

231
00:13:06.580 --> 00:13:08.740
<v Speaker 2>I would prefer the appearance of </v>
<v Speaker 2>consciousness,</v>

232
00:13:08.970 --> 00:13:13.970
<v Speaker 2>but the question is whether the </v>
<v Speaker 2>appearance of cautiousness is different </v>

233
00:13:13.970 --> 00:13:17.381
<v Speaker 2>than cost consciousness itself.</v>
<v Speaker 2>And sort of ask that as a question,</v>

234
00:13:18.280 --> 00:13:20.230
<v Speaker 2>do you think we need to,</v>
<v Speaker 2>you know,</v>

235
00:13:20.231 --> 00:13:25.231
<v Speaker 2>understand what consciousness is,</v>
<v Speaker 2>solve the hard problem of consciousness </v>

236
00:13:25.231 --> 00:13:28.240
<v Speaker 2>in order to build something,</v>
<v Speaker 2>a light in a GI system?</v>

237
00:13:28.270 --> 00:13:30.220
<v Speaker 2>No,</v>
<v Speaker 2>I don't think that.</v>

238
00:13:31.170 --> 00:13:36.170
<v Speaker 2>I think we will probably be able to </v>
<v Speaker 2>build things even if we don't answer </v>

239
00:13:36.170 --> 00:13:40.651
<v Speaker 2>that question,</v>
<v Speaker 2>but if we want to make sure that what </v>

240
00:13:40.651 --> 00:13:40.651
<v Speaker 2>happens is a good thing,</v>
<v Speaker 2>we better solve it first.</v>

241
00:13:40.990 --> 00:13:45.990
<v Speaker 2>So it's a wonderful controversy you're </v>
<v Speaker 2>raising there where you have basically </v>

242
00:13:46.781 --> 00:13:48.740
<v Speaker 2>three points of view about the hard </v>
<v Speaker 2>problems.</v>

243
00:13:48.741 --> 00:13:53.741
<v Speaker 2>So there are two different points of </v>
<v Speaker 2>view that both conclude that the hard </v>

244
00:13:54.310 --> 00:13:56.830
<v Speaker 2>problem of crunches is bs.</v>
<v Speaker 2>Do you have,</v>

245
00:13:56.880 --> 00:14:01.880
<v Speaker 2>on one hand you have some people like </v>
<v Speaker 2>Daniel Dennett who say this is </v>

246
00:14:01.880 --> 00:14:05.250
<v Speaker 2>unconscious is just bs because </v>
<v Speaker 2>consciousness is the same thing as </v>

247
00:14:05.250 --> 00:14:06.010
<v Speaker 2>intelligence.</v>
<v Speaker 2>There's no difference.</v>

248
00:14:06.460 --> 00:14:11.460
<v Speaker 2>So anything which acts conscious is </v>
<v Speaker 2>conscious dislike,</v>

249
00:14:11.660 --> 00:14:16.000
<v Speaker 2>like we are.</v>
<v Speaker 2>And then there are also a lot of people,</v>

250
00:14:16.001 --> 00:14:18.640
<v Speaker 2>including many top ai researchers.</v>
<v Speaker 2>I know you say,</v>

251
00:14:18.641 --> 00:14:19.960
<v Speaker 2>Oh,</v>
<v Speaker 2>I have a consciousness is just bullshit.</v>

252
00:14:19.961 --> 00:14:21.970
<v Speaker 2>Because of course machines can never be </v>
<v Speaker 2>conscious,</v>

253
00:14:22.000 --> 00:14:23.380
<v Speaker 2>right?</v>
<v Speaker 2>They're always going to.</v>

254
00:14:23.381 --> 00:14:28.381
<v Speaker 2>It's going to be zombies,</v>
<v Speaker 2>never have to feel guilty about how you </v>

255
00:14:28.381 --> 00:14:28.381
<v Speaker 2>treat them</v>

256
00:14:28.381 --> 00:14:32.880
<v Speaker 2>and then there's a third group of people</v>
<v Speaker 2>including Giulio Tononi for example,</v>

257
00:14:34.060 --> 00:14:36.610
<v Speaker 2>and another here's the cochrane number </v>
<v Speaker 2>of others.</v>

258
00:14:37.510 --> 00:14:42.510
<v Speaker 2>I would put myself on this middle camp </v>
<v Speaker 2>who say that actually some information </v>

259
00:14:42.510 --> 00:14:46.471
<v Speaker 2>processing is conscious and some is not,</v>
<v Speaker 2>so let's find the equation which can be </v>

260
00:14:46.811 --> 00:14:51.811
<v Speaker 2>used to determine which it is and I </v>
<v Speaker 2>think we've just been a little bit lazy </v>

261
00:14:52.101 --> 00:14:54.610
<v Speaker 2>kind of running away from this problem </v>
<v Speaker 2>for a long time.</v>

262
00:14:55.010 --> 00:14:57.740
<v Speaker 2>It's been almost taboo.</v>
<v Speaker 2>Even mentioned the c word,</v>

263
00:14:58.120 --> 00:15:03.120
<v Speaker 2>a lot of circles because.</v>
<v Speaker 2>But we should stop making excuses.</v>

264
00:15:03.600 --> 00:15:08.600
<v Speaker 2>This is a science question and there are</v>
<v Speaker 2>ways we can even test test any scenario </v>

265
00:15:10.380 --> 00:15:13.700
<v Speaker 2>that makes predictions for this.</v>
<v Speaker 2>And coming back to this health a robot.</v>

266
00:15:13.701 --> 00:15:18.701
<v Speaker 2>I mean,</v>
<v Speaker 2>so you said you would want your help a </v>

267
00:15:18.701 --> 00:15:20.891
<v Speaker 2>robot to certainly conscious and treat </v>
<v Speaker 2>you like to have conversations with you </v>

268
00:15:20.891 --> 00:15:22.070
<v Speaker 2>and I think so wouldn't you?</v>

269
00:15:22.100 --> 00:15:27.100
<v Speaker 2>Would you feel,</v>
<v Speaker 2>would you feel a little bit creeped out </v>

270
00:15:27.100 --> 00:15:27.730
<v Speaker 2>if you realize that it was just glossed </v>
<v Speaker 2>up the tape recorder?</v>

271
00:15:27.760 --> 00:15:32.760
<v Speaker 2>You know,</v>
<v Speaker 2>there was this Zombie and faking </v>

272
00:15:32.760 --> 00:15:33.740
<v Speaker 2>emotion.</v>
<v Speaker 2>Would you prefer that it actually had an</v>

273
00:15:33.741 --> 00:15:38.240
<v Speaker 2>experience or what would you prefer that</v>
<v Speaker 2>it's actually not experiencing anything,</v>

274
00:15:38.241 --> 00:15:41.390
<v Speaker 2>so you feel you don't have to feel </v>
<v Speaker 2>guilty about what you do to it.</v>

275
00:15:41.740 --> 00:15:44.170
<v Speaker 2>It's such a difficult question because,</v>
<v Speaker 2>uh,</v>

276
00:15:44.720 --> 00:15:49.720
<v Speaker 2>you know,</v>
<v Speaker 2>it's like when you're in a relationship </v>

277
00:15:49.720 --> 00:15:49.720
<v Speaker 2>and you say,</v>
<v Speaker 2>well,</v>

278
00:15:49.720 --> 00:15:49.720
<v Speaker 2>I love you and the other person will </v>
<v Speaker 2>love you back.</v>

279
00:15:49.790 --> 00:15:54.790
<v Speaker 2>It's like asking or do they really love </v>
<v Speaker 2>you back or are they just saying they </v>

280
00:15:54.790 --> 00:15:54.820
<v Speaker 2>love you back?</v>
<v Speaker 2>Uh,</v>

281
00:15:55.030 --> 00:16:00.030
<v Speaker 2>do you,</v>
<v Speaker 2>don't you really want them to actually </v>

282
00:16:00.030 --> 00:16:00.770
<v Speaker 2>love you?</v>
<v Speaker 2>It's hard to.</v>

283
00:16:01.040 --> 00:16:06.040
<v Speaker 2>It's hard to really know the difference </v>
<v Speaker 2>between a everything seeming like </v>

284
00:16:08.181 --> 00:16:10.610
<v Speaker 2>there's consciousness present,</v>
<v Speaker 2>there's intelligence president,</v>

285
00:16:10.620 --> 00:16:12.740
<v Speaker 2>there's a affection,</v>
<v Speaker 2>passion,</v>

286
00:16:12.770 --> 00:16:14.350
<v Speaker 2>love,</v>
<v Speaker 2>and,</v>

287
00:16:14.540 --> 00:16:16.190
<v Speaker 2>and it actually being there.</v>

288
00:16:16.190 --> 00:16:17.280
<v Speaker 2>I'm not sure.</v>
<v Speaker 2>Do you have,</v>

289
00:16:17.640 --> 00:16:18.270
<v Speaker 2>like,</v>
<v Speaker 2>do you ask,</v>

290
00:16:18.271 --> 00:16:20.600
<v Speaker 2>you can ask a question to make it a bit </v>
<v Speaker 2>more point.</v>

291
00:16:20.601 --> 00:16:22.700
<v Speaker 2>That's a mass general hospital is right </v>
<v Speaker 2>across the river,</v>

292
00:16:22.701 --> 00:16:23.100
<v Speaker 2>right?</v>
<v Speaker 2>Yes.</v>

293
00:16:23.210 --> 00:16:28.210
<v Speaker 2>Suppose,</v>
<v Speaker 2>I suppose you're going in for a medical </v>

294
00:16:28.210 --> 00:16:28.210
<v Speaker 2>procedure and they're like,</v>
<v Speaker 2>you know,</v>

295
00:16:28.210 --> 00:16:28.490
<v Speaker 2>uh,</v>
<v Speaker 2>for,</v>

296
00:16:28.550 --> 00:16:33.550
<v Speaker 2>for anesthesia,</v>
<v Speaker 2>what we're going to do is we're gonna </v>

297
00:16:33.550 --> 00:16:35.620
<v Speaker 2>give you a muscle relaxant so you won't </v>
<v Speaker 2>be able to move a and you're gonna feel </v>

298
00:16:35.620 --> 00:16:35.840
<v Speaker 2>excruciating pain during the whole </v>
<v Speaker 2>surgery,</v>

299
00:16:35.841 --> 00:16:37.310
<v Speaker 2>but you won't be able to do anything </v>
<v Speaker 2>about it.</v>

300
00:16:37.550 --> 00:16:40.520
<v Speaker 2>But then we're going to give you this </v>
<v Speaker 2>drug that race is your memory of it.</v>

301
00:16:41.930 --> 00:16:42.980
<v Speaker 2>Would you be cool about that?</v>

302
00:16:45.020 --> 00:16:48.970
<v Speaker 2>What's the difference that you're </v>
<v Speaker 2>conscious about it or not?</v>

303
00:16:48.980 --> 00:16:49.220
<v Speaker 2>If,</v>
<v Speaker 2>if,</v>

304
00:16:49.260 --> 00:16:51.250
<v Speaker 2>if there's no behavioral change.</v>
<v Speaker 2>Right,</v>

305
00:16:51.570 --> 00:16:53.130
<v Speaker 2>right.</v>
<v Speaker 2>That's a really interesting.</v>

306
00:16:53.131 --> 00:16:54.860
<v Speaker 2>That's a really clear where to put it.</v>
<v Speaker 2>That's.</v>

307
00:16:55.660 --> 00:16:57.320
<v Speaker 2>Yeah,</v>
<v Speaker 2>it feels like in that sense,</v>

308
00:16:57.350 --> 00:17:02.350
<v Speaker 2>experiencing it as a valuable quality.</v>
<v Speaker 2>So actually being able to have </v>

309
00:17:03.201 --> 00:17:07.130
<v Speaker 2>subjective experiences,</v>
<v Speaker 2>at least in that case is,</v>

310
00:17:07.730 --> 00:17:12.730
<v Speaker 2>is valuable.</v>
<v Speaker 2>And I think we humans have a little bit </v>

311
00:17:12.730 --> 00:17:16.181
<v Speaker 2>of a bad track record also of making </v>
<v Speaker 2>these selfserving arguments that other </v>

312
00:17:16.640 --> 00:17:18.200
<v Speaker 2>entities aren't conscious.</v>
<v Speaker 2>You know,</v>

313
00:17:18.201 --> 00:17:19.250
<v Speaker 2>people often say,</v>
<v Speaker 2>oh,</v>

314
00:17:19.251 --> 00:17:21.050
<v Speaker 2>these animals can't feel pain.</v>
<v Speaker 2>Right.</v>

315
00:17:21.710 --> 00:17:26.710
<v Speaker 2>It's okay to boil lobster is because we </v>
<v Speaker 2>asked them if it hurt and I didn't say </v>

316
00:17:26.710 --> 00:17:30.191
<v Speaker 2>anything.</v>
<v Speaker 2>And now that was just the paper out </v>

317
00:17:30.191 --> 00:17:30.191
<v Speaker 2>saying lobsters did do feel pain when </v>
<v Speaker 2>you boil them lemon,</v>

318
00:17:30.191 --> 00:17:31.030
<v Speaker 2>they're banning it in Switzerland and,</v>
<v Speaker 2>and,</v>

319
00:17:31.290 --> 00:17:33.470
<v Speaker 2>and we did this with slaves too often </v>
<v Speaker 2>and say,</v>

320
00:17:33.500 --> 00:17:38.500
<v Speaker 2>oh,</v>
<v Speaker 2>they don't mind a bit on maybe </v>

321
00:17:38.750 --> 00:17:41.060
<v Speaker 2>unconscious or women don't have souls or</v>
<v Speaker 2>whatever.</v>

322
00:17:41.070 --> 00:17:46.070
<v Speaker 2>So I'm a little bit nervous when I hear </v>
<v Speaker 2>people just take as an axiom that </v>

323
00:17:46.070 --> 00:17:50.891
<v Speaker 2>machines can't have experience ever.</v>
<v Speaker 2>I think this is just this really </v>

324
00:17:50.891 --> 00:17:52.140
<v Speaker 2>fascinating science question is what it </v>
<v Speaker 2>is.</v>

325
00:17:52.210 --> 00:17:57.210
<v Speaker 2>Yeah,</v>
<v Speaker 2>let's research it and try to figure out </v>

326
00:17:57.210 --> 00:17:58.790
<v Speaker 2>what it is that makes the difference </v>
<v Speaker 2>between unconscious intelligent behavior</v>

327
00:17:58.920 --> 00:18:00.470
<v Speaker 2>and conscious intelligent behavior.</v>

328
00:18:01.080 --> 00:18:04.680
<v Speaker 1>So in terms of,</v>
<v Speaker 1>if you think about Boston dynamics,</v>

329
00:18:04.681 --> 00:18:09.681
<v Speaker 1>humanoid robot being sort of a,</v>
<v Speaker 1>with a broom being pushed around the </v>

330
00:18:09.840 --> 00:18:14.840
<v Speaker 1>it's starts is,</v>
<v Speaker 1>it starts pushing on us consciousness </v>

331
00:18:14.840 --> 00:18:14.840
<v Speaker 1>question.</v>
<v Speaker 1>So let me ask,</v>

332
00:18:14.840 --> 00:18:19.490
<v Speaker 1>do you think an agi system,</v>
<v Speaker 1>like a few neuroscientists believe a </v>

333
00:18:19.710 --> 00:18:24.710
<v Speaker 1>nice to have a physical embodiment needs</v>
<v Speaker 1>to have a body or something like a body?</v>

334
00:18:25.740 --> 00:18:26.130
<v Speaker 1>No,</v>

335
00:18:27.040 --> 00:18:32.040
<v Speaker 2>I don't think so.</v>
<v Speaker 2>You mean to have to have a conscious </v>

336
00:18:32.040 --> 00:18:34.551
<v Speaker 2>experience to have consciousness?</v>
<v Speaker 2>I do think it helps a lot to have a </v>

337
00:18:35.071 --> 00:18:40.071
<v Speaker 2>physical embodiment to learn the kind of</v>
<v Speaker 2>things about the world that are </v>

338
00:18:40.071 --> 00:18:41.370
<v Speaker 2>important to us humans,</v>
<v Speaker 2>for sure,</v>

339
00:18:42.570 --> 00:18:46.200
<v Speaker 2>but I don't think the physical </v>
<v Speaker 2>embodiment doesn't necessarily,</v>

340
00:18:46.260 --> 00:18:51.260
<v Speaker 2>after you've learned it,</v>
<v Speaker 2>just have the experience thinking about </v>

341
00:18:51.260 --> 00:18:51.260
<v Speaker 2>when you're dreaming,</v>
<v Speaker 2>right?</v>

342
00:18:51.420 --> 00:18:53.340
<v Speaker 2>Your eyes are closed,</v>
<v Speaker 2>you're like any,</v>

343
00:18:53.341 --> 00:18:58.341
<v Speaker 2>any sensory input.</v>
<v Speaker 2>You're not behaving or moving in any </v>

344
00:18:58.341 --> 00:18:58.341
<v Speaker 2>way,</v>
<v Speaker 2>but there's still an experience there.</v>

345
00:18:58.341 --> 00:19:02.540
<v Speaker 2>Right,</v>
<v Speaker 2>and so clearly the experience that you </v>

346
00:19:02.540 --> 00:19:04.500
<v Speaker 2>have when you see something cool in your</v>
<v Speaker 2>dreams isn't coming from your eyes.</v>

347
00:19:04.830 --> 00:19:08.250
<v Speaker 2>It's just the information processing </v>
<v Speaker 2>itself in your brain,</v>

348
00:19:08.610 --> 00:19:10.080
<v Speaker 2>which is that experience.</v>
<v Speaker 2>Right?</v>

349
00:19:10.910 --> 00:19:13.640
<v Speaker 1>But if I put it another way,</v>
<v Speaker 1>I will say,</v>

350
00:19:13.641 --> 00:19:17.730
<v Speaker 1>because it comes from neuroscience,</v>
<v Speaker 1>is the reason you want to have a body in</v>

351
00:19:17.731 --> 00:19:22.731
<v Speaker 1>a physical,</v>
<v Speaker 1>something physical like a physical </v>

352
00:19:23.201 --> 00:19:28.201
<v Speaker 1>system is because you want to be able to</v>
<v Speaker 1>preserve something in order to have a </v>

353
00:19:28.201 --> 00:19:28.570
<v Speaker 1>self.</v>

354
00:19:29.270 --> 00:19:31.820
<v Speaker 2>You could argue,</v>
<v Speaker 2>would you?</v>

355
00:19:31.890 --> 00:19:36.890
<v Speaker 2>You'd need to have some kind of </v>
<v Speaker 2>embodiment of self to want to preserve.</v>

356
00:19:38.870 --> 00:19:43.870
<v Speaker 2>Well,</v>
<v Speaker 2>now we're getting a little bit on throw </v>

357
00:19:43.870 --> 00:19:44.510
<v Speaker 2>up amorphic paths into </v>
<v Speaker 2>anthropomorphizing things,</v>

358
00:19:45.300 --> 00:19:47.210
<v Speaker 2>talking about self preservation </v>
<v Speaker 2>instincts.</v>

359
00:19:47.211 --> 00:19:49.670
<v Speaker 2>I mean we are evolved organisms,</v>
<v Speaker 2>right?</v>

360
00:19:49.760 --> 00:19:53.870
<v Speaker 2>Right.</v>
<v Speaker 2>So darwinian evolution then doubt us and</v>

361
00:19:54.200 --> 00:19:57.590
<v Speaker 2>other involve all the organism with a </v>
<v Speaker 2>self preservation instinct because those</v>

362
00:19:58.340 --> 00:20:01.610
<v Speaker 2>that didn't have those self preservation</v>
<v Speaker 2>genes got cleaned out of the gene pool.</v>

363
00:20:01.900 --> 00:20:06.900
<v Speaker 2>But um,</v>
<v Speaker 2>but if you build an artificial general </v>

364
00:20:06.900 --> 00:20:10.751
<v Speaker 2>intelligence,</v>
<v Speaker 2>the mind space that you can design is </v>

365
00:20:10.751 --> 00:20:13.781
<v Speaker 2>much,</v>
<v Speaker 2>much larger than just a specific subset </v>

366
00:20:13.781 --> 00:20:13.781
<v Speaker 2>of,</v>
<v Speaker 2>of minds that can evolve,</v>

367
00:20:13.781 --> 00:20:18.550
<v Speaker 2>that have so certain Agi mind doesn't </v>
<v Speaker 2>necessarily have to have any self </v>

368
00:20:18.550 --> 00:20:18.920
<v Speaker 2>preservation.</v>
<v Speaker 2>These things.</v>

369
00:20:19.820 --> 00:20:24.820
<v Speaker 2>It also doesn't necessarily have to be </v>
<v Speaker 2>so individualistic as like imagine if </v>

370
00:20:24.820 --> 00:20:25.970
<v Speaker 2>you could just.</v>
<v Speaker 2>First of all,</v>

371
00:20:26.410 --> 00:20:27.860
<v Speaker 2>we also very afraid of death.</v>

372
00:20:28.110 --> 00:20:33.110
<v Speaker 2>I suppose you could back yourself up </v>
<v Speaker 2>every five minutes and then your </v>

373
00:20:33.110 --> 00:20:33.440
<v Speaker 2>airplane is about to crash with like,</v>
<v Speaker 2>shucks,</v>

374
00:20:33.441 --> 00:20:34.090
<v Speaker 2>I'm just,</v>
<v Speaker 2>I'm,</v>

375
00:20:35.010 --> 00:20:37.450
<v Speaker 2>I'm going to lose the last five minutes </v>
<v Speaker 2>of experiences.</v>

376
00:20:37.451 --> 00:20:39.780
<v Speaker 2>That's my last cloud backup bang,</v>
<v Speaker 2>you know,</v>

377
00:20:39.810 --> 00:20:44.810
<v Speaker 2>it's not as big a deal where if we could</v>
<v Speaker 2>just copy experiences between our minds </v>

378
00:20:45.621 --> 00:20:47.590
<v Speaker 2>easily,</v>
<v Speaker 2>like we wish we easily do.</v>

379
00:20:47.591 --> 00:20:52.591
<v Speaker 2>If we were a silicon based right then </v>
<v Speaker 2>maybe we would feel a little bit more </v>

380
00:20:53.981 --> 00:20:56.830
<v Speaker 2>like a hive mind actually that maybe </v>
<v Speaker 2>it's the so,</v>

381
00:20:57.300 --> 00:21:02.300
<v Speaker 2>so there's a.</v>
<v Speaker 2>So I don't think we should take for </v>

382
00:21:02.300 --> 00:21:04.111
<v Speaker 2>granted at all that Agi will have to </v>
<v Speaker 2>have any of those sort of competitive as</v>

383
00:21:05.681 --> 00:21:07.240
<v Speaker 2>an Alpha male instincts.</v>
<v Speaker 2>Right.</v>

384
00:21:07.300 --> 00:21:08.290
<v Speaker 2>On the other hand,</v>
<v Speaker 2>you know,</v>

385
00:21:09.010 --> 00:21:13.750
<v Speaker 2>this is really interesting because I </v>
<v Speaker 2>think some people go too far and say,</v>

386
00:21:13.751 --> 00:21:18.751
<v Speaker 2>of course we don't have to have any </v>
<v Speaker 2>concerns either that advanced ai will </v>

387
00:21:19.991 --> 00:21:23.010
<v Speaker 2>have those instincts because we can </v>
<v Speaker 2>build anything you want that there's,</v>

388
00:21:23.240 --> 00:21:26.590
<v Speaker 2>there's a very nice set of arguments </v>
<v Speaker 2>going back to Steve.</v>

389
00:21:26.600 --> 00:21:31.600
<v Speaker 2>I'm Nick Bostrom and others just </v>
<v Speaker 2>pointing out that when we build </v>

390
00:21:31.600 --> 00:21:33.580
<v Speaker 2>machines,</v>
<v Speaker 2>we normally build them with some kind of</v>

391
00:21:33.581 --> 00:21:34.390
<v Speaker 2>goal.</v>
<v Speaker 2>You know,</v>

392
00:21:34.600 --> 00:21:38.200
<v Speaker 2>when this chess game drive this car </v>
<v Speaker 2>safely or whatever,</v>

393
00:21:38.440 --> 00:21:40.570
<v Speaker 2>and as soon as you put in a goal into </v>
<v Speaker 2>machine,</v>

394
00:21:41.050 --> 00:21:46.050
<v Speaker 2>especially if it's kind of open ended </v>
<v Speaker 2>goal and the mission is very </v>

395
00:21:46.050 --> 00:21:48.451
<v Speaker 2>intelligent,</v>
<v Speaker 2>it'll break that down into a bunch of </v>

396
00:21:48.451 --> 00:21:50.821
<v Speaker 2>sub goals and I'm one of those goals </v>
<v Speaker 2>will almost always be self preservation </v>

397
00:21:52.240 --> 00:21:57.240
<v Speaker 2>because if it breaks or dies and the </v>
<v Speaker 2>process is not going to accomplish the </v>

398
00:21:57.240 --> 00:21:57.880
<v Speaker 2>goal,</v>
<v Speaker 2>I suppose you just build a little.</v>

399
00:21:57.930 --> 00:22:02.930
<v Speaker 2>You have a little robot.</v>
<v Speaker 2>Can you tell it to go down the star </v>

400
00:22:02.930 --> 00:22:03.910
<v Speaker 2>market here and,</v>
<v Speaker 2>and get you some food,</v>

401
00:22:03.911 --> 00:22:05.730
<v Speaker 2>make a cooking Italian dinner,</v>
<v Speaker 2>you know,</v>

402
00:22:06.100 --> 00:22:11.100
<v Speaker 2>and then someone mugs that and tries to </v>
<v Speaker 2>break it on the way that robot has an </v>

403
00:22:11.100 --> 00:22:15.181
<v Speaker 2>incentive to the to not get destroyed </v>
<v Speaker 2>and defend itself or runaway because </v>

404
00:22:15.181 --> 00:22:17.270
<v Speaker 2>otherwise it's going to fail and cooking</v>
<v Speaker 2>your dinner.</v>

405
00:22:17.270 --> 00:22:22.270
<v Speaker 2>It is not afraid of death,</v>
<v Speaker 2>but it really wants to complete the </v>

406
00:22:22.270 --> 00:22:26.970
<v Speaker 2>dinner and gold.</v>
<v Speaker 2>So it will have a self preservation </v>

407
00:22:26.970 --> 00:22:26.970
<v Speaker 2>instinct to continue being a functional </v>
<v Speaker 2>agent.</v>

408
00:22:26.970 --> 00:22:27.990
<v Speaker 2>And,</v>
<v Speaker 2>and,</v>

409
00:22:28.080 --> 00:22:33.080
<v Speaker 2>and,</v>
<v Speaker 2>and similarly if you give her any kind </v>

410
00:22:33.080 --> 00:22:36.301
<v Speaker 2>of more and they just go to an Agi,</v>
<v Speaker 2>it's very likely they want to acquire </v>

411
00:22:36.911 --> 00:22:41.440
<v Speaker 2>more resources,</v>
<v Speaker 2>can do that better and it's exactly from</v>

412
00:22:41.441 --> 00:22:46.441
<v Speaker 2>those sort of sub goals that we might </v>
<v Speaker 2>not have intended that some of the </v>

413
00:22:46.441 --> 00:22:49.771
<v Speaker 2>concerns about Agi safety come,</v>
<v Speaker 2>you give it some goal that seems </v>

414
00:22:49.771 --> 00:22:53.290
<v Speaker 2>completely harmless and then before you </v>
<v Speaker 2>realize it,</v>

415
00:22:53.291 --> 00:22:57.190
<v Speaker 2>it's also trying to do these other </v>
<v Speaker 2>things you didn't want it to do and it's</v>

416
00:22:57.220 --> 00:22:59.220
<v Speaker 2>more maybe smarter than us.</v>
<v Speaker 2>So.</v>

417
00:23:00.070 --> 00:23:03.550
<v Speaker 2>So let me pause just because I'm,</v>
<v Speaker 2>I am,</v>

418
00:23:04.310 --> 00:23:07.120
<v Speaker 1>uh,</v>
<v Speaker 1>in a very kind of human centric way.</v>

419
00:23:07.570 --> 00:23:10.870
<v Speaker 1>See fear of death as a valuable </v>
<v Speaker 1>motivator.</v>

420
00:23:10.940 --> 00:23:11.750
<v Speaker 1>Uh Huh.</v>
<v Speaker 1>Um,</v>

421
00:23:11.800 --> 00:23:16.800
<v Speaker 1>so you don't think as you think that's </v>
<v Speaker 1>an artifact of evolution.</v>

422
00:23:17.121 --> 00:23:22.121
<v Speaker 1>So that's the kind of mind space </v>
<v Speaker 1>evolution created that were sort of </v>

423
00:23:22.121 --> 00:23:23.960
<v Speaker 1>almost obsessed about self preservation </v>
<v Speaker 1>kind of generic.</v>

424
00:23:23.961 --> 00:23:25.940
<v Speaker 1>Well,</v>
<v Speaker 1>you don't think that's necessary</v>

425
00:23:26.440 --> 00:23:27.250
<v Speaker 2>to be</v>

426
00:23:28.490 --> 00:23:33.490
<v Speaker 1>afraid of death.</v>
<v Speaker 1>So not just a kind of sub goal of self </v>

427
00:23:33.490 --> 00:23:34.370
<v Speaker 1>preservation,</v>
<v Speaker 1>just so you can keep doing the thing,</v>

428
00:23:34.850 --> 00:23:39.620
<v Speaker 1>but more fundamentally sort of have the </v>
<v Speaker 1>finite thing like this ends</v>

429
00:23:40.610 --> 00:23:43.610
<v Speaker 2>for you as some point.</v>
<v Speaker 2>Interesting.</v>

430
00:23:43.780 --> 00:23:46.070
<v Speaker 2>Why do I think it's necessary before,</v>
<v Speaker 2>what?</v>

431
00:23:46.220 --> 00:23:50.870
<v Speaker 2>Precisely for intelligence but also for </v>
<v Speaker 2>consciousness.</v>

432
00:23:50.900 --> 00:23:55.900
<v Speaker 2>So for those for both,</v>
<v Speaker 2>do you think really like a finite death </v>

433
00:23:57.260 --> 00:24:01.450
<v Speaker 2>and the fear of it is important?</v>
<v Speaker 2>So</v>

434
00:24:02.820 --> 00:24:07.820
<v Speaker 2>before I can answer before we can agree </v>
<v Speaker 2>on whether it's necessarily for </v>

435
00:24:07.820 --> 00:24:11.331
<v Speaker 2>intelligence or for consciousness that </v>
<v Speaker 2>we should be clear on how we define </v>

436
00:24:11.331 --> 00:24:14.151
<v Speaker 2>those two words because a lot are really</v>
<v Speaker 2>smart people to find them in very </v>

437
00:24:14.151 --> 00:24:14.151
<v Speaker 2>different ways.</v>
<v Speaker 2>Right?</v>

438
00:24:14.151 --> 00:24:18.350
<v Speaker 2>I was in this,</v>
<v Speaker 2>on this panel with AI experts and they </v>

439
00:24:18.350 --> 00:24:19.070
<v Speaker 2>couldn't eat,</v>
<v Speaker 2>they couldn't agree on how to define the</v>

440
00:24:19.080 --> 00:24:20.450
<v Speaker 2>village and Steven.</v>
<v Speaker 2>So I,</v>

441
00:24:20.451 --> 00:24:25.451
<v Speaker 2>I define intelligence simply as the </v>
<v Speaker 2>ability to accomplish complex goals or </v>

442
00:24:25.760 --> 00:24:27.300
<v Speaker 2>like your broad definition because </v>
<v Speaker 2>again,</v>

443
00:24:27.301 --> 00:24:29.320
<v Speaker 2>I don't want to be a carbon chava this </v>
<v Speaker 2>right.</v>

444
00:24:30.450 --> 00:24:33.270
<v Speaker 2>And um,</v>
<v Speaker 2>in that case,</v>

445
00:24:33.300 --> 00:24:35.610
<v Speaker 2>no,</v>
<v Speaker 2>it certainly doesn't require it.</v>

446
00:24:35.880 --> 00:24:40.880
<v Speaker 2>Fear of death.</v>
<v Speaker 2>I would say Alphago or Alpha zero is </v>

447
00:24:40.880 --> 00:24:43.881
<v Speaker 2>quite intelligent.</v>
<v Speaker 2>Alpha zero has any fear of being turned </v>

448
00:24:43.881 --> 00:24:45.140
<v Speaker 2>off because it doesn't understand the </v>
<v Speaker 2>concept of,</v>

449
00:24:45.330 --> 00:24:50.330
<v Speaker 2>of even and similarly consciousness.</v>
<v Speaker 2>I mean you could certainly imagine a </v>

450
00:24:51.270 --> 00:24:54.410
<v Speaker 2>very simple kind of experience if,</v>
<v Speaker 2>if,</v>

451
00:24:54.411 --> 00:24:56.250
<v Speaker 2>if,</v>
<v Speaker 2>if certain plans,</v>

452
00:24:56.251 --> 00:25:01.251
<v Speaker 2>70 kind of experience,</v>
<v Speaker 2>I don't think they're very afraid of </v>

453
00:25:01.251 --> 00:25:01.251
<v Speaker 2>dying and there's nothing they can do </v>
<v Speaker 2>about it anyway.</v>

454
00:25:01.251 --> 00:25:05.720
<v Speaker 2>So there wasn't that much value in.</v>
<v Speaker 2>But more seriously I think if you ask </v>

455
00:25:07.631 --> 00:25:10.620
<v Speaker 2>not just about being conscious but maybe</v>
<v Speaker 2>having a</v>

456
00:25:12.740 --> 00:25:13.740
<v Speaker 2>what you will be,</v>
<v Speaker 2>we will,</v>

457
00:25:13.760 --> 00:25:18.760
<v Speaker 2>we might call an exciting life for you.</v>
<v Speaker 2>Feel passionate and really appreciate </v>

458
00:25:18.760 --> 00:25:23.380
<v Speaker 2>the things maybe there,</v>
<v Speaker 2>but somehow maybe there perhaps it does </v>

459
00:25:24.081 --> 00:25:25.880
<v Speaker 2>help having a,</v>
<v Speaker 2>having that backdrop,</v>

460
00:25:25.881 --> 00:25:27.300
<v Speaker 2>the hey,</v>
<v Speaker 2>it's finite,</v>

461
00:25:27.320 --> 00:25:29.770
<v Speaker 2>it's no limits this,</v>
<v Speaker 2>make the most of this,</v>

462
00:25:29.780 --> 00:25:31.610
<v Speaker 2>this live to the fullest.</v>
<v Speaker 2>So if you,</v>

463
00:25:31.620 --> 00:25:36.620
<v Speaker 2>if,</v>
<v Speaker 2>if you knew you were going to live </v>

464
00:25:36.620 --> 00:25:36.620
<v Speaker 2>forever,</v>

465
00:25:36.620 --> 00:25:37.720
<v Speaker 2>do you think you would change your.</v>
<v Speaker 2>Yeah,</v>

466
00:25:37.721 --> 00:25:42.721
<v Speaker 2>I mean in some perspective it would be </v>
<v Speaker 2>an incredibly boring life living </v>

467
00:25:43.601 --> 00:25:46.750
<v Speaker 2>forever.</v>
<v Speaker 2>So in the sort of loose subjective terms</v>

468
00:25:46.751 --> 00:25:51.751
<v Speaker 2>that you said of something exciting and </v>
<v Speaker 2>something in this that other humans </v>

469
00:25:51.751 --> 00:25:52.990
<v Speaker 2>would understand,</v>
<v Speaker 2>I think is.</v>

470
00:25:52.991 --> 00:25:57.991
<v Speaker 2>Yeah,</v>
<v Speaker 2>it seems that the finiteness of it is </v>

471
00:25:57.991 --> 00:25:57.991
<v Speaker 2>important.</v>
<v Speaker 2>Well,</v>

472
00:25:57.991 --> 00:26:02.341
<v Speaker 2>the good news I have for you then is </v>
<v Speaker 2>based on what we understand about </v>

473
00:26:02.341 --> 00:26:05.551
<v Speaker 2>cosmology,</v>
<v Speaker 2>everything is in our university </v>

474
00:26:05.551 --> 00:26:10.110
<v Speaker 2>ultimately probably finite dollar,</v>
<v Speaker 2>although they crunch or b a or big,</v>

475
00:26:10.300 --> 00:26:15.300
<v Speaker 2>what's the expense and the infinite.</v>
<v Speaker 2>You could have a big chill or a big </v>

476
00:26:15.300 --> 00:26:17.470
<v Speaker 2>challenge or a big rip or the big snap </v>
<v Speaker 2>or death bubbles.</v>

477
00:26:18.430 --> 00:26:20.020
<v Speaker 2>All of them are more than a billion </v>
<v Speaker 2>years away,</v>

478
00:26:20.021 --> 00:26:25.021
<v Speaker 2>so we should.</v>
<v Speaker 2>We certainly have vastly more time than </v>

479
00:26:25.021 --> 00:26:27.850
<v Speaker 2>our ancestors thought,</v>
<v Speaker 2>but they're still.</v>

480
00:26:29.150 --> 00:26:34.150
<v Speaker 2>It's still pretty hard to squeeze in an </v>
<v Speaker 2>infinite number of compute cycles even </v>

481
00:26:34.150 --> 00:26:34.150
<v Speaker 2>though</v>

482
00:26:35.080 --> 00:26:37.720
<v Speaker 2>there are some loopholes that just might</v>
<v Speaker 2>be possible.</v>

483
00:26:37.721 --> 00:26:42.721
<v Speaker 2>But I think I know some people like to </v>
<v Speaker 2>say that you should live as if you're </v>

484
00:26:44.460 --> 00:26:49.460
<v Speaker 2>about to.</v>
<v Speaker 2>You're going to die in five years or is </v>

485
00:26:49.460 --> 00:26:49.460
<v Speaker 2>that when that's sort of optimal?</v>
<v Speaker 2>Maybe.</v>

486
00:26:49.470 --> 00:26:54.470
<v Speaker 2>It's a good assumption we should build </v>
<v Speaker 2>our civilization as if it's all finite </v>

487
00:26:54.720 --> 00:26:55.950
<v Speaker 2>to be on the safe side.</v>
<v Speaker 2>Right,</v>

488
00:26:56.040 --> 00:27:01.040
<v Speaker 2>exactly.</v>
<v Speaker 2>So you mentioned in defining </v>

489
00:27:01.040 --> 00:27:02.520
<v Speaker 2>intelligence as the ability to solve </v>
<v Speaker 2>complex goals,</v>

490
00:27:02.970 --> 00:27:07.970
<v Speaker 2>the where would you draw a line?</v>
<v Speaker 2>How would you try to define human level </v>

491
00:27:07.970 --> 00:27:10.590
<v Speaker 2>intelligence and super human level </v>
<v Speaker 2>intelligence?</v>

492
00:27:10.680 --> 00:27:13.260
<v Speaker 2>Where does consciousness part of that </v>
<v Speaker 2>definition?</v>

493
00:27:13.290 --> 00:27:16.530
<v Speaker 2>No consciousness does not come into this</v>
<v Speaker 2>definition.</v>

494
00:27:16.650 --> 00:27:18.660
<v Speaker 2>So,</v>
<v Speaker 2>so I think of intelligence.</v>

495
00:27:19.150 --> 00:27:22.380
<v Speaker 2>It's a spectrum of very many different </v>
<v Speaker 2>kinds of goals you can have.</v>

496
00:27:22.381 --> 00:27:24.040
<v Speaker 2>You can have a goal to be a good chess </v>
<v Speaker 2>player,</v>

497
00:27:24.180 --> 00:27:28.010
<v Speaker 2>googleplay or a good car driver,</v>
<v Speaker 2>a good investor,</v>

498
00:27:28.530 --> 00:27:30.270
<v Speaker 2>good poet,</v>
<v Speaker 2>etc.</v>

499
00:27:31.170 --> 00:27:33.730
<v Speaker 2>So intelligence that by,</v>
<v Speaker 2>by its very nature,</v>

500
00:27:33.731 --> 00:27:36.660
<v Speaker 2>it isn't something you can measure,</v>
<v Speaker 2>but it's one number.</v>

501
00:27:36.690 --> 00:27:38.140
<v Speaker 2>It waS an overall goodness.</v>
<v Speaker 2>No,</v>

502
00:27:38.180 --> 00:27:43.180
<v Speaker 2>no.</v>
<v Speaker 2>There's some people who are better at </v>

503
00:27:43.180 --> 00:27:43.180
<v Speaker 2>this.</v>
<v Speaker 2>Some people are better than that.</v>

504
00:27:43.180 --> 00:27:44.280
<v Speaker 2>Um,</v>
<v Speaker 2>right now we have machines that are much</v>

505
00:27:44.281 --> 00:27:49.050
<v Speaker 2>better than us at some very narrow tasks</v>
<v Speaker 2>like multiplying large numbers,</v>

506
00:27:49.051 --> 00:27:53.220
<v Speaker 2>fast memorizing large databases of </v>
<v Speaker 2>playing chess,</v>

507
00:27:53.221 --> 00:27:58.221
<v Speaker 2>playing go and soon driving cars.</v>
<v Speaker 2>But there's still no machine that can </v>

508
00:27:58.831 --> 00:28:02.730
<v Speaker 2>match a human child in general </v>
<v Speaker 2>intelligence,</v>

509
00:28:02.760 --> 00:28:05.700
<v Speaker 2>but artificial general intelligence,</v>
<v Speaker 2>agi,</v>

510
00:28:05.730 --> 00:28:10.730
<v Speaker 2>the name of your course,</v>
<v Speaker 2>of course that is by its very </v>

511
00:28:10.891 --> 00:28:14.240
<v Speaker 2>definition,</v>
<v Speaker 2>the the quest to build a machine,</v>

512
00:28:14.241 --> 00:28:19.241
<v Speaker 2>a machine that can do everything as well</v>
<v Speaker 2>as we can up to the old holy grail of ai</v>

513
00:28:19.711 --> 00:28:24.711
<v Speaker 2>from,</v>
<v Speaker 2>from back to its inception and the and </v>

514
00:28:24.711 --> 00:28:24.810
<v Speaker 2>the sixties.</v>
<v Speaker 2>If that ever happens,</v>

515
00:28:24.811 --> 00:28:29.811
<v Speaker 2>of course I think it's going to be the </v>
<v Speaker 2>biggest transition in the history of </v>

516
00:28:29.811 --> 00:28:29.811
<v Speaker 2>life on earth,</v>
<v Speaker 2>but it.</v>

517
00:28:29.811 --> 00:28:34.270
<v Speaker 2>But it doesn't necessarily have to wait </v>
<v Speaker 2>the big impact on until machines are </v>

518
00:28:34.270 --> 00:28:36.480
<v Speaker 2>better than us at knitting,</v>
<v Speaker 2>that they're really big.</v>

519
00:28:36.481 --> 00:28:39.570
<v Speaker 2>Change doesn't come exactly at the </v>
<v Speaker 2>moment.</v>

520
00:28:39.571 --> 00:28:42.580
<v Speaker 2>They're better than us at everything.</v>
<v Speaker 2>They're really big.</v>

521
00:28:42.610 --> 00:28:47.610
<v Speaker 2>chains comes first.</v>
<v Speaker 2>There are big changes when they start </v>

522
00:28:47.610 --> 00:28:50.241
<v Speaker 2>becoming better at doing most of the </v>
<v Speaker 2>jobs that we do because that takes away </v>

523
00:28:50.241 --> 00:28:55.220
<v Speaker 2>much of the demand for human labor and </v>
<v Speaker 2>then the really whopping change comes </v>

524
00:28:55.650 --> 00:29:00.080
<v Speaker 2>when they become better than us at ai </v>
<v Speaker 2>research.</v>

525
00:29:00.490 --> 00:29:00.970
<v Speaker 2>Right?</v>
<v Speaker 2>Right.</v>

526
00:29:01.080 --> 00:29:03.400
<v Speaker 2>Because right now the timescale of the </v>
<v Speaker 2>irish,</v>

527
00:29:03.420 --> 00:29:08.420
<v Speaker 2>which is limited by the human research </v>
<v Speaker 2>and development cycle of the year is </v>

528
00:29:09.211 --> 00:29:09.930
<v Speaker 2>typically,</v>
<v Speaker 2>you know,</v>

529
00:29:10.200 --> 00:29:15.200
<v Speaker 2>along the tafe from one release of some </v>
<v Speaker 2>software or iphone or whatever to the </v>

530
00:29:15.200 --> 00:29:16.660
<v Speaker 2>next,</v>
<v Speaker 2>but once,</v>

531
00:29:16.710 --> 00:29:20.370
<v Speaker 2>once we have,</v>
<v Speaker 2>once google can replace 40,000</v>

532
00:29:20.371 --> 00:29:25.371
<v Speaker 2>engineers by 40,000</v>
<v Speaker 2>equivalent pieces of software or </v>

533
00:29:25.801 --> 00:29:28.230
<v Speaker 2>whatever,</v>
<v Speaker 2>but then that doesn't.</v>

534
00:29:28.260 --> 00:29:33.260
<v Speaker 2>There's no reason that has to be yours.</v>
<v Speaker 2>It can be in principle much faster and </v>

535
00:29:33.260 --> 00:29:37.100
<v Speaker 2>the timescale of future progress in ai </v>
<v Speaker 2>and all of science and technology will </v>

536
00:29:38.340 --> 00:29:40.440
<v Speaker 2>will be driven by machines,</v>
<v Speaker 2>not humans.</v>

537
00:29:40.910 --> 00:29:45.910
<v Speaker 2>So It's this simple point which gives </v>
<v Speaker 2>rise to this incredibly fun controversy </v>

538
00:29:48.670 --> 00:29:51.580
<v Speaker 2>about whether there can be intelligence </v>
<v Speaker 2>explosion.</v>

539
00:29:51.790 --> 00:29:53.970
<v Speaker 2>So called singularity is vernor vinge </v>
<v Speaker 2>called it.</v>

540
00:29:54.460 --> 00:29:59.410
<v Speaker 2>The idea is articulated by iga.</v>
<v Speaker 2>Good is obviously way back fifties,</v>

541
00:29:59.440 --> 00:30:03.430
<v Speaker 2>but you can see alan turing and others </v>
<v Speaker 2>thought about it even earlier.</v>

542
00:30:06.950 --> 00:30:10.780
<v Speaker 2>You asked me what exactly what I define </v>
<v Speaker 2>engelman level.</v>

543
00:30:12.630 --> 00:30:17.630
<v Speaker 2>so this.</v>
<v Speaker 2>The glib aNswer is if to say something </v>

544
00:30:17.630 --> 00:30:20.731
<v Speaker 2>which is better than us at all,</v>
<v Speaker 2>cognitive tasks with a lot better than </v>

545
00:30:20.731 --> 00:30:21.760
<v Speaker 2>any human at all,</v>
<v Speaker 2>cognitive tasks,</v>

546
00:30:21.820 --> 00:30:24.520
<v Speaker 2>but they're really interesting.</v>
<v Speaker 2>Bar I Think goes a little bit lower than</v>

547
00:30:24.521 --> 00:30:26.710
<v Speaker 2>that actually.</v>
<v Speaker 2>It's when they can,</v>

548
00:30:27.040 --> 00:30:32.040
<v Speaker 2>when they're better than us at ai </v>
<v Speaker 2>programming and and qa and a general </v>

549
00:30:32.040 --> 00:30:36.421
<v Speaker 2>learning so that they can can,</v>
<v Speaker 2>if they want to get better than us at </v>

550
00:30:36.421 --> 00:30:37.240
<v Speaker 2>anything by the studying,</v>

551
00:30:37.620 --> 00:30:42.620
<v Speaker 1>they're better as a keyword and better </v>
<v Speaker 1>is towards this kind of spectrum of the </v>

552
00:30:42.620 --> 00:30:45.270
<v Speaker 1>complexity of goals it's able to </v>
<v Speaker 1>accomplish.</v>

553
00:30:45.271 --> 00:30:47.830
<v Speaker 1>Yeah.</v>
<v Speaker 1>So another way to say,</v>

554
00:30:48.270 --> 00:30:53.020
<v Speaker 1>and that's certainly a very,</v>
<v Speaker 1>a clear definition of human love.</v>

555
00:30:53.021 --> 00:30:55.200
<v Speaker 1>So there's,</v>
<v Speaker 1>it's almost like a c that's rising.</v>

556
00:30:55.210 --> 00:30:58.290
<v Speaker 1>You could do more and more and more </v>
<v Speaker 1>things as a graphic that you show.</v>

557
00:30:58.590 --> 00:31:01.230
<v Speaker 1>It's really nice way to put it.</v>
<v Speaker 1>So there's some peaks that,</v>

558
00:31:01.500 --> 00:31:04.440
<v Speaker 1>and there's an ocean level elevating and</v>
<v Speaker 1>you solve more and more problems,</v>

559
00:31:04.740 --> 00:31:09.740
<v Speaker 1>but you know,</v>
<v Speaker 1>just kind of a to take a pause and we </v>

560
00:31:09.740 --> 00:31:12.951
<v Speaker 1>took a bunch of questions and a lot of </v>
<v Speaker 1>social networks and a bunch of people </v>

561
00:31:12.951 --> 00:31:16.221
<v Speaker 1>asked a sort of a slightly different </v>
<v Speaker 1>direction and creativity and um,</v>

562
00:31:16.780 --> 00:31:19.910
<v Speaker 1>and,</v>
<v Speaker 1>and things that perhaps aren't a peak,</v>

563
00:31:20.880 --> 00:31:21.680
<v Speaker 1>the,</v>
<v Speaker 1>it,</v>

564
00:31:21.700 --> 00:31:22.620
<v Speaker 1>it,</v>
<v Speaker 1>it's,</v>

565
00:31:23.190 --> 00:31:28.190
<v Speaker 1>you know,</v>
<v Speaker 1>human beings are flawed and perhaps </v>

566
00:31:28.190 --> 00:31:28.190
<v Speaker 1>better means having,</v>
<v Speaker 1>being a,</v>

567
00:31:28.190 --> 00:31:29.700
<v Speaker 1>having contradiction,</v>
<v Speaker 1>being flawed in some way.</v>

568
00:31:30.120 --> 00:31:34.230
<v Speaker 1>So let me sort of start and start easy.</v>
<v Speaker 1>First of all.</v>

569
00:31:34.270 --> 00:31:36.420
<v Speaker 1>Uh,</v>
<v Speaker 1>so you have a lot of cool equations.</v>

570
00:31:36.510 --> 00:31:38.040
<v Speaker 1>Let me ask,</v>
<v Speaker 1>what's your favorite equation?</v>

571
00:31:38.070 --> 00:31:41.010
<v Speaker 1>First of all,</v>
<v Speaker 1>I know they're all like your children,</v>

572
00:31:41.011 --> 00:31:43.500
<v Speaker 1>but which one is that?</v>

573
00:31:45.550 --> 00:31:48.460
<v Speaker 2>The master key of quantum mechanics.</v>

574
00:31:48.510 --> 00:31:53.510
<v Speaker 1>Oh,</v>
<v Speaker 1>the micro world check everything to do </v>

575
00:31:53.510 --> 00:31:55.630
<v Speaker 1>with add on molecules and all the way </v>
<v Speaker 1>up.</v>

576
00:31:58.510 --> 00:31:59.260
<v Speaker 1>Yeah.</v>
<v Speaker 1>So,</v>

577
00:31:59.261 --> 00:32:01.480
<v Speaker 1>okay.</v>
<v Speaker 1>So quantum mechanics is certainly a,</v>

578
00:32:01.481 --> 00:32:04.780
<v Speaker 1>a beautiful,</v>
<v Speaker 1>mysterious formulation of our world.</v>

579
00:32:05.110 --> 00:32:08.440
<v Speaker 1>So I'd like to sort of ask you,</v>
<v Speaker 1>and just as an example,</v>

580
00:32:08.710 --> 00:32:12.100
<v Speaker 1>it perhaps doesn't have the same beauty </v>
<v Speaker 1>is physics does,</v>

581
00:32:12.101 --> 00:32:17.101
<v Speaker 1>but in mathematics at abstract,</v>
<v Speaker 1>the andrew weil's who proved the was </v>

582
00:32:18.050 --> 00:32:22.720
<v Speaker 1>last year,</v>
<v Speaker 1>so he just saw this recently and it kind</v>

583
00:32:22.721 --> 00:32:25.210
<v Speaker 1>of caught my eye a little bit.</v>
<v Speaker 1>This is 350,</v>

584
00:32:25.211 --> 00:32:29.560
<v Speaker 1>eight years after it was conjectured.</v>
<v Speaker 1>So this very simple formulation,</v>

585
00:32:29.860 --> 00:32:34.090
<v Speaker 1>everybody tried to prove it,</v>
<v Speaker 1>everybody failed and say here's this guy</v>

586
00:32:34.091 --> 00:32:39.091
<v Speaker 1>comes along and eventually the proves it</v>
<v Speaker 1>and fails to prove it and then boost it </v>

587
00:32:39.171 --> 00:32:44.171
<v Speaker 1>again in [inaudible] 94.</v>
<v Speaker 1>And he said like the moment when </v>

588
00:32:44.171 --> 00:32:47.651
<v Speaker 1>everything connected into place in an </v>
<v Speaker 1>interview said it was so indescribably </v>

589
00:32:47.651 --> 00:32:47.810
<v Speaker 1>beaUtiful.</v>

590
00:32:47.810 --> 00:32:52.710
<v Speaker 1>That moment when you finally realize the</v>
<v Speaker 1>connecting piece of two conjectures,</v>

591
00:32:52.720 --> 00:32:54.980
<v Speaker 1>he said it was so indescribably </v>
<v Speaker 1>beautiful.</v>

592
00:32:55.160 --> 00:33:00.160
<v Speaker 1>It was so simple and so elegant.</v>
<v Speaker 1>I couldn't understand how I missed it </v>

593
00:33:00.160 --> 00:33:01.640
<v Speaker 1>and I just stared at it and the </v>
<v Speaker 1>disbelief for 20 minutes.</v>

594
00:33:01.990 --> 00:33:06.990
<v Speaker 1>Then.</v>
<v Speaker 1>Then during the day I walked around the </v>

595
00:33:06.990 --> 00:33:09.311
<v Speaker 1>department and had kimi keep coming back</v>
<v Speaker 1>to my desk looking to see if it was </v>

596
00:33:09.311 --> 00:33:10.550
<v Speaker 1>still there.</v>
<v Speaker 1>It was still there.</v>

597
00:33:10.551 --> 00:33:12.620
<v Speaker 1>I couldn't contain myself.</v>
<v Speaker 1>I was so excited.</v>

598
00:33:12.800 --> 00:33:15.500
<v Speaker 1>It was the most important moment of my </v>
<v Speaker 1>working life.</v>

599
00:33:15.770 --> 00:33:20.770
<v Speaker 1>Nothing I ever do again will mean as </v>
<v Speaker 1>much to that particular moment and it </v>

600
00:33:21.111 --> 00:33:26.111
<v Speaker 1>kinda made me think of what would it </v>
<v Speaker 1>take and I think we have all been there </v>

601
00:33:26.841 --> 00:33:30.620
<v Speaker 1>at small levels.</v>
<v Speaker 1>Maybe let me ask,</v>

602
00:33:30.650 --> 00:33:35.490
<v Speaker 1>have you had a moment like that in your </v>
<v Speaker 1>life where you just had an idea as like,</v>

603
00:33:35.900 --> 00:33:36.980
<v Speaker 1>wow,</v>
<v Speaker 1>yes,</v>

604
00:33:40.090 --> 00:33:43.330
<v Speaker 2>I wouldn't mind some myself and in the </v>
<v Speaker 2>same breath as andrew weil's,</v>

605
00:33:43.331 --> 00:33:48.331
<v Speaker 2>but I certainly had a number of have aha</v>
<v Speaker 2>moments when I realized something very </v>

606
00:33:51.951 --> 00:33:56.951
<v Speaker 2>cool about physics,</v>
<v Speaker 2>just this completely made my head </v>

607
00:33:56.951 --> 00:33:56.951
<v Speaker 2>explode.</v>
<v Speaker 2>In fact,</v>

608
00:33:56.951 --> 00:33:58.330
<v Speaker 2>some of my favorite discoveries I made </v>
<v Speaker 2>late.</v>

609
00:33:58.350 --> 00:34:03.350
<v Speaker 2>I later realized that it had been </v>
<v Speaker 2>discovered earlier by someone quite </v>

610
00:34:03.350 --> 00:34:07.341
<v Speaker 2>famous for it,</v>
<v Speaker 2>so it's too late for me to even publish </v>

611
00:34:07.341 --> 00:34:07.430
<v Speaker 2>it,</v>
<v Speaker 2>but that doesn't diminish in any way the</v>

612
00:34:07.510 --> 00:34:09.900
<v Speaker 2>emotional experience you have when you </v>
<v Speaker 2>realize that like,</v>

613
00:34:10.660 --> 00:34:11.750
<v Speaker 1>wow.</v>
<v Speaker 1>yeah.</v>

614
00:34:12.220 --> 00:34:15.040
<v Speaker 1>So what would it take in that moment </v>
<v Speaker 1>that wow,</v>

615
00:34:15.580 --> 00:34:20.580
<v Speaker 1>that was yours in that moment.</v>
<v Speaker 1>So what do you think it takes for an </v>

616
00:34:20.580 --> 00:34:24.410
<v Speaker 1>intelligent system and agi system?</v>
<v Speaker 1>An ai system to have a moment like that.</v>

617
00:34:25.460 --> 00:34:30.460
<v Speaker 2>Yeah,</v>
<v Speaker 2>that's a tricky question because there </v>

618
00:34:30.460 --> 00:34:30.460
<v Speaker 2>are actually two parts to it,</v>
<v Speaker 2>right?</v>

619
00:34:30.460 --> 00:34:33.280
<v Speaker 2>One of them is candidates accomplish </v>
<v Speaker 2>that proof.</v>

620
00:34:33.900 --> 00:34:38.900
<v Speaker 2>I can prove that you can never write a </v>
<v Speaker 2>to the end plus b to the n equals three.</v>

621
00:34:39.750 --> 00:34:44.470
<v Speaker 2>That equals z for all integer as well,</v>
<v Speaker 2>etc.</v>

622
00:34:44.471 --> 00:34:45.760
<v Speaker 2>Etc.</v>
<v Speaker 2>When,</v>

623
00:34:45.820 --> 00:34:50.820
<v Speaker 2>when is bigger than two,</v>
<v Speaker 2>the best simply in the question about </v>

624
00:34:50.820 --> 00:34:55.791
<v Speaker 2>intelligence,</v>
<v Speaker 2>can you build machines that are </v>

625
00:34:55.791 --> 00:34:55.791
<v Speaker 2>intelligent?</v>

626
00:34:55.791 --> 00:34:55.791
<v Speaker 2>Uh,</v>
<v Speaker 2>no.</v>

627
00:34:55.791 --> 00:34:59.680
<v Speaker 2>I think by the time we get a machine </v>
<v Speaker 2>that can independently come up with that</v>

628
00:34:59.701 --> 00:35:02.580
<v Speaker 2>level of proofs,</v>
<v Speaker 2>probably quite close to agi.</v>

629
00:35:03.370 --> 00:35:06.250
<v Speaker 2>The second question is a question about </v>
<v Speaker 2>consciousness.</v>

630
00:35:06.490 --> 00:35:08.480
<v Speaker 2>Oh,</v>
<v Speaker 2>when will we,</v>

631
00:35:08.660 --> 00:35:13.660
<v Speaker 2>will we'll ins how likely it is if it's </v>
<v Speaker 2>such a machine would actually have any </v>

632
00:35:13.660 --> 00:35:17.541
<v Speaker 2>experience at all as opposed to just </v>
<v Speaker 2>being like a zombie and would we fact </v>

633
00:35:18.630 --> 00:35:23.570
<v Speaker 2>that they have some sort of emotional </v>
<v Speaker 2>response to this or anything at all akin</v>

634
00:35:23.571 --> 00:35:28.220
<v Speaker 2>to human emotion where now when it </v>
<v Speaker 2>accomplishes its machine goal,</v>

635
00:35:28.250 --> 00:35:32.180
<v Speaker 2>is it that the views that this somehow </v>
<v Speaker 2>something very positive and,</v>

636
00:35:32.670 --> 00:35:35.270
<v Speaker 2>and,</v>
<v Speaker 2>and sublime and,</v>

637
00:35:35.271 --> 00:35:35.680
<v Speaker 2>and,</v>
<v Speaker 2>and,</v>

638
00:35:35.780 --> 00:35:40.780
<v Speaker 2>and deeply meaningful.</v>
<v Speaker 2>I would certainly hope that if in the </v>

639
00:35:41.011 --> 00:35:46.011
<v Speaker 2>future we do create machines that are </v>
<v Speaker 2>our peers or even our descendants.</v>

640
00:35:48.560 --> 00:35:53.560
<v Speaker 2>Yeah,</v>
<v Speaker 2>but I would certainly hope that they do </v>

641
00:35:53.560 --> 00:35:54.390
<v Speaker 2>have this sort of supply him as a blind </v>
<v Speaker 2>appreciation of life.</v>

642
00:35:55.630 --> 00:36:00.630
<v Speaker 2>In a way.</v>
<v Speaker 2>My absolutely worst nightmare would be </v>

643
00:36:00.630 --> 00:36:04.410
<v Speaker 2>that in at some point in the future</v>

644
00:36:05.850 --> 00:36:10.850
<v Speaker 2>distance future,</v>
<v Speaker 2>maybe our cosmos is teaming with all </v>

645
00:36:10.850 --> 00:36:13.341
<v Speaker 2>this post biological life,</v>
<v Speaker 2>doing all the seemingly cool stuff and </v>

646
00:36:13.341 --> 00:36:18.211
<v Speaker 2>maybe the last humans by the time our </v>
<v Speaker 2>our species eventually fizzes out will </v>

647
00:36:20.241 --> 00:36:20.960
<v Speaker 2>be like,</v>
<v Speaker 2>well,</v>

648
00:36:20.961 --> 00:36:25.961
<v Speaker 2>that's okay because we're so proud of </v>
<v Speaker 2>our descendants here and look at what </v>

649
00:36:25.961 --> 00:36:29.570
<v Speaker 2>all the.</v>
<v Speaker 2>My worst nightmare is that we haven't </v>

650
00:36:29.570 --> 00:36:33.221
<v Speaker 2>solved the consciousness problem and we </v>
<v Speaker 2>haven't realized that these are all the </v>

651
00:36:33.221 --> 00:36:34.780
<v Speaker 2>zombies.</v>
<v Speaker 2>They're not aware of anything anymore.</v>

652
00:36:34.790 --> 00:36:37.460
<v Speaker 2>Then the tape recorder,</v>
<v Speaker 2>is it any kind of experience?</v>

653
00:36:37.790 --> 00:36:41.450
<v Speaker 2>So at the whole thing has just become a </v>
<v Speaker 2>play for empty benches.</v>

654
00:36:41.570 --> 00:36:44.120
<v Speaker 2>That will be the ultimate zombie </v>
<v Speaker 2>apocalypse.</v>

655
00:36:44.620 --> 00:36:49.620
<v Speaker 2>So I.</v>
<v Speaker 2>I would much rather in that case that's </v>

656
00:36:49.620 --> 00:36:54.590
<v Speaker 2>we have these beings when we just really</v>
<v Speaker 2>appreciate how the,</v>

657
00:36:55.210 --> 00:36:56.280
<v Speaker 2>how amazing it is,</v>

658
00:36:56.870 --> 00:37:00.860
<v Speaker 1>and in that picture of what would be the</v>
<v Speaker 1>role of creativity,</v>

659
00:37:00.861 --> 00:37:05.861
<v Speaker 1>but a few people ask about creativity.</v>
<v Speaker 1>Do you think when you think about </v>

660
00:37:05.861 --> 00:37:07.760
<v Speaker 1>intelligence,</v>
<v Speaker 1>I mean certainly the,</v>

661
00:37:07.761 --> 00:37:12.761
<v Speaker 1>uh,</v>
<v Speaker 1>the story he told at the beginning of </v>

662
00:37:12.761 --> 00:37:12.761
<v Speaker 1>your book and,</v>
<v Speaker 1>you know,</v>

663
00:37:12.761 --> 00:37:12.761
<v Speaker 1>creating movies and so on,</v>
<v Speaker 1>started making,</v>

664
00:37:13.840 --> 00:37:18.840
<v Speaker 1>making money.</v>
<v Speaker 1>You can make a lot of money in our </v>

665
00:37:18.840 --> 00:37:20.870
<v Speaker 1>modern world with music and movies.</v>
<v Speaker 1>So if you are an intelligent system,</v>

666
00:37:20.871 --> 00:37:25.871
<v Speaker 1>you may want to get good at that.</v>
<v Speaker 1>But that's not necessarily what I mean </v>

667
00:37:25.871 --> 00:37:30.101
<v Speaker 1>by creativity is an important on that </v>
<v Speaker 1>complex goals where the seas rising for </v>

668
00:37:31.431 --> 00:37:36.431
<v Speaker 1>there to be something creative or am I </v>
<v Speaker 1>being very human centric and thinking </v>

669
00:37:36.801 --> 00:37:39.320
<v Speaker 1>creativity somehow special,</v>
<v Speaker 1>uh,</v>

670
00:37:39.530 --> 00:37:40.880
<v Speaker 1>relative to intelligence.</v>

671
00:37:41.840 --> 00:37:46.840
<v Speaker 2>My hunch is the,</v>
<v Speaker 2>we should think of creativity simply as </v>

672
00:37:47.630 --> 00:37:50.980
<v Speaker 2>an aspect of intelligence.</v>
<v Speaker 2>And,</v>

673
00:37:50.990 --> 00:37:55.990
<v Speaker 2>uh,</v>
<v Speaker 2>we would have to be very careful with </v>

674
00:37:55.990 --> 00:38:00.230
<v Speaker 2>the human vanity we have.</v>
<v Speaker 2>We have this tendency at the very often </v>

675
00:38:00.230 --> 00:38:01.520
<v Speaker 2>one and say,</v>
<v Speaker 2>as soon as machines can do something,</v>

676
00:38:01.521 --> 00:38:03.230
<v Speaker 2>we try to diminish it and saying,</v>
<v Speaker 2>oh,</v>

677
00:38:03.231 --> 00:38:05.290
<v Speaker 2>but that's not like real intelligence,</v>
<v Speaker 2>you know,</v>

678
00:38:05.850 --> 00:38:08.930
<v Speaker 2>he was a traitor or this or that.</v>
<v Speaker 2>The other thing,</v>

679
00:38:09.250 --> 00:38:14.250
<v Speaker 2>um,</v>
<v Speaker 2>maybe if we ask ourselves to write down </v>

680
00:38:14.250 --> 00:38:16.290
<v Speaker 2>a definition of what we actually mean by</v>
<v Speaker 2>being creative,</v>

681
00:38:16.920 --> 00:38:18.990
<v Speaker 2>what we mean by andrew weil's,</v>
<v Speaker 2>what he did there,</v>

682
00:38:18.991 --> 00:38:22.010
<v Speaker 2>for example,</v>
<v Speaker 2>don't we often mean that someone takes a</v>

683
00:38:22.590 --> 00:38:27.590
<v Speaker 2>very unexpected leap.</v>
<v Speaker 2>It's not like taking 573 and multiplying</v>

684
00:38:29.561 --> 00:38:34.561
<v Speaker 2>it By 224 by does this step of </v>
<v Speaker 2>straightforward cookbook like rules,</v>

685
00:38:34.630 --> 00:38:37.990
<v Speaker 2>right?</v>
<v Speaker 2>This maybe major you have,</v>

686
00:38:38.010 --> 00:38:40.600
<v Speaker 2>haven't make a connection between two </v>
<v Speaker 2>things that people have never thought it</v>

687
00:38:40.601 --> 00:38:43.000
<v Speaker 2>was connected to a surprising or </v>
<v Speaker 2>something like that.</v>

688
00:38:44.320 --> 00:38:49.320
<v Speaker 2>I think.</v>
<v Speaker 2>I think this is an aspect of </v>

689
00:38:49.320 --> 00:38:49.320
<v Speaker 2>intelligence.</v>
<v Speaker 2>Uh,</v>

690
00:38:49.320 --> 00:38:52.841
<v Speaker 2>and uh,</v>
<v Speaker 2>this is actually one of the most </v>

691
00:38:52.841 --> 00:38:55.321
<v Speaker 2>important aspects of it.</v>
<v Speaker 2>Maybe the reason we humans tend to be </v>

692
00:38:55.321 --> 00:38:59.911
<v Speaker 2>better at it than a traditional computer</v>
<v Speaker 2>is because it's something that comes </v>

693
00:38:59.911 --> 00:39:00.520
<v Speaker 2>more naturally.</v>
<v Speaker 2>If you're a neural network,</v>

694
00:39:01.230 --> 00:39:06.230
<v Speaker 2>then if you're a traditional logic gates</v>
<v Speaker 2>based computer machine and we physically</v>

695
00:39:06.371 --> 00:39:10.810
<v Speaker 2>have all these connections and you </v>
<v Speaker 2>activate here,</v>

696
00:39:10.820 --> 00:39:15.820
<v Speaker 2>activated here,</v>
<v Speaker 2>activity here at the time,</v>

697
00:39:16.530 --> 00:39:19.090
<v Speaker 2>my hunch is that if we ever build a </v>
<v Speaker 2>machine,</v>

698
00:39:20.600 --> 00:39:22.280
<v Speaker 2>well you could just give it the task.</v>
<v Speaker 2>Hey,</v>

699
00:39:22.281 --> 00:39:23.090
<v Speaker 2>hey,</v>
<v Speaker 2>uh,</v>

700
00:39:23.290 --> 00:39:24.740
<v Speaker 2>uh,</v>
<v Speaker 2>you,</v>

701
00:39:25.060 --> 00:39:25.670
<v Speaker 2>you say,</v>
<v Speaker 2>hey,</v>

702
00:39:25.671 --> 00:39:29.180
<v Speaker 2>you know,</v>
<v Speaker 2>I just realized that I have,</v>

703
00:39:29.240 --> 00:39:32.300
<v Speaker 2>I want to travel around the world.</v>
<v Speaker 2>Is that this month?</v>

704
00:39:32.301 --> 00:39:34.580
<v Speaker 2>Can you teach my eight age course for </v>
<v Speaker 2>me?</v>

705
00:39:34.581 --> 00:39:35.390
<v Speaker 2>And it's like,</v>
<v Speaker 2>okay,</v>

706
00:39:35.391 --> 00:39:40.391
<v Speaker 2>I'll do it.</v>
<v Speaker 2>And it does everything that you would </v>

707
00:39:40.391 --> 00:39:40.391
<v Speaker 2>have done and the improvisers and stuff.</v>
<v Speaker 2>Yeah.</v>

708
00:39:40.391 --> 00:39:44.530
<v Speaker 2>That,</v>
<v Speaker 2>that would in my mind involve a lot of </v>

709
00:39:44.530 --> 00:39:44.530
<v Speaker 2>creativity.</v>

710
00:39:44.530 --> 00:39:45.660
<v Speaker 1>Yeah.</v>
<v Speaker 1>So at sexy and beautiful way to put it.</v>

711
00:39:45.661 --> 00:39:50.661
<v Speaker 1>I think we do try to grab grasp at the,</v>
<v Speaker 1>you know,</v>

712
00:39:50.881 --> 00:39:51.130
<v Speaker 1>the,</v>
<v Speaker 1>the,</v>

713
00:39:51.210 --> 00:39:53.070
<v Speaker 1>the definition of intelligence is </v>
<v Speaker 1>everything.</v>

714
00:39:53.071 --> 00:39:56.340
<v Speaker 1>We don't understand how a,</v>
<v Speaker 1>how to build.</v>

715
00:39:56.341 --> 00:40:01.341
<v Speaker 1>So like,</v>
<v Speaker 1>so we as humans try to find things that </v>

716
00:40:01.341 --> 00:40:01.341
<v Speaker 1>we have,</v>
<v Speaker 1>uh,</v>

717
00:40:01.341 --> 00:40:05.401
<v Speaker 1>machines don't have.</v>
<v Speaker 1>And maybe creativity is just one of the </v>

718
00:40:05.401 --> 00:40:05.430
<v Speaker 1>things.</v>
<v Speaker 1>One of the words we use to describe that</v>

719
00:40:05.820 --> 00:40:07.010
<v Speaker 1>as sort,</v>
<v Speaker 1>really interesting way to put it.</v>

720
00:40:07.060 --> 00:40:08.370
<v Speaker 1>I don't think we need to</v>

721
00:40:08.590 --> 00:40:11.290
<v Speaker 2>be that defensive.</v>
<v Speaker 2>I don't think anything good comes out of</v>

722
00:40:11.291 --> 00:40:11.640
<v Speaker 2>saying,</v>
<v Speaker 2>well,</v>

723
00:40:11.650 --> 00:40:13.330
<v Speaker 2>we're somehow special.</v>
<v Speaker 2>You know,</v>

724
00:40:13.940 --> 00:40:14.890
<v Speaker 2>it's,</v>
<v Speaker 2>it's,</v>

725
00:40:14.891 --> 00:40:19.891
<v Speaker 2>um,</v>
<v Speaker 2>con is there are many examples in </v>

726
00:40:20.651 --> 00:40:25.651
<v Speaker 2>history of where trying to pretend that </v>
<v Speaker 2>we're somehow superior to all other </v>

727
00:40:29.630 --> 00:40:33.110
<v Speaker 2>intelligent beings has led to pretty bad</v>
<v Speaker 2>results.</v>

728
00:40:33.111 --> 00:40:35.920
<v Speaker 2>Right?</v>
<v Speaker 2>Right.</v>

729
00:40:35.921 --> 00:40:40.921
<v Speaker 2>And nazi Germany,</v>
<v Speaker 2>they said that they were superior to </v>

730
00:40:40.921 --> 00:40:40.921
<v Speaker 2>other people.</v>
<v Speaker 2>Uh,</v>

731
00:40:40.921 --> 00:40:43.990
<v Speaker 2>today we still do a lot of cruelty to </v>
<v Speaker 2>animals by saying that we're so superior</v>

732
00:40:44.050 --> 00:40:45.940
<v Speaker 2>somehow on the,</v>
<v Speaker 2>they cAn't feel pain.</v>

733
00:40:46.300 --> 00:40:49.880
<v Speaker 2>A slavery was justified by the same kind</v>
<v Speaker 2>of just really weak,</v>

734
00:40:50.260 --> 00:40:51.300
<v Speaker 2>weak arguments.</v>
<v Speaker 2>And,</v>

735
00:40:51.340 --> 00:40:52.480
<v Speaker 2>and,</v>
<v Speaker 2>and,</v>

736
00:40:52.481 --> 00:40:57.481
<v Speaker 2>uh,</v>
<v Speaker 2>I don't think if we actually go ahead </v>

737
00:40:57.481 --> 00:40:58.570
<v Speaker 2>and build artificial general </v>
<v Speaker 2>intelligence,</v>

738
00:40:59.160 --> 00:41:02.840
<v Speaker 2>it can do things better than us.</v>
<v Speaker 2>I don't think we should try to found our</v>

739
00:41:02.841 --> 00:41:07.841
<v Speaker 2>self worth on some sort of bogus claims </v>
<v Speaker 2>of superiority in terms of our </v>

740
00:41:09.931 --> 00:41:12.100
<v Speaker 2>intelligence.</v>
<v Speaker 2>Alright.</v>

741
00:41:12.150 --> 00:41:15.900
<v Speaker 2>I think we should instead find our,</v>
<v Speaker 2>a</v>

742
00:41:17.550 --> 00:41:20.860
<v Speaker 2>calling and then the meaning of life </v>
<v Speaker 2>from,</v>

743
00:41:20.870 --> 00:41:23.140
<v Speaker 2>from the experiences that we have right </v>
<v Speaker 2>now,</v>

744
00:41:23.430 --> 00:41:28.430
<v Speaker 2>I can have,</v>
<v Speaker 2>I can have very meaningful experiences </v>

745
00:41:28.830 --> 00:41:30.720
<v Speaker 2>even if there are other people who are </v>
<v Speaker 2>smarter than me,</v>

746
00:41:30.750 --> 00:41:35.750
<v Speaker 2>you know,</v>
<v Speaker 2>when I go to the faculty meeting here </v>

747
00:41:35.750 --> 00:41:35.780
<v Speaker 2>and I was like,</v>
<v Speaker 2>are we talking about something?</v>

748
00:41:35.781 --> 00:41:36.680
<v Speaker 2>And then I suddenly realize,</v>
<v Speaker 2>oh,</v>

749
00:41:36.730 --> 00:41:39.080
<v Speaker 2>but he has an old price.</v>
<v Speaker 2>He has an old price.</v>

750
00:41:39.081 --> 00:41:40.700
<v Speaker 2>He has no pride.</v>
<v Speaker 2>I don't have one.</v>

751
00:41:40.820 --> 00:41:45.820
<v Speaker 2>Does that make me enjoy life any less or</v>
<v Speaker 2>enjoy talking to those people?</v>

752
00:41:47.241 --> 00:41:51.220
<v Speaker 2>That's of course not right.</v>
<v Speaker 2>And the ontario is,</v>

753
00:41:51.270 --> 00:41:56.270
<v Speaker 2>I,</v>
<v Speaker 2>I feel very honored and privileged to </v>

754
00:41:56.270 --> 00:41:56.270
<v Speaker 2>get to interact with,</v>
<v Speaker 2>with,</v>

755
00:41:56.270 --> 00:41:57.390
<v Speaker 2>uh,</v>
<v Speaker 2>other,</v>

756
00:41:57.410 --> 00:41:59.500
<v Speaker 2>very intelligent beings that are better </v>
<v Speaker 2>than me.</v>

757
00:41:59.750 --> 00:42:04.750
<v Speaker 2>A lot of stuff.</v>
<v Speaker 2>So I don't think there's any reason why </v>

758
00:42:04.750 --> 00:42:05.570
<v Speaker 2>we can't have the same approach with </v>
<v Speaker 2>intelligent machines.</v>

759
00:42:06.080 --> 00:42:07.190
<v Speaker 2>That's a really interesting.</v>

760
00:42:07.310 --> 00:42:10.150
<v Speaker 2>So people don't often think about that.</v>
<v Speaker 2>They think about when there's going.</v>

761
00:42:10.520 --> 00:42:12.980
<v Speaker 2>If there's machines that are more </v>
<v Speaker 2>intelligent,</v>

762
00:42:13.220 --> 00:42:18.220
<v Speaker 2>he naturally think that that's not going</v>
<v Speaker 2>to be a beneficial type of intelligence.</v>

763
00:42:19.010 --> 00:42:21.440
<v Speaker 2>You don't realize it could be,</v>
<v Speaker 2>you know,</v>

764
00:42:21.470 --> 00:42:26.470
<v Speaker 2>like peers with nobel prizes that,</v>
<v Speaker 2>that will be just fun to talk with and </v>

765
00:42:26.470 --> 00:42:28.730
<v Speaker 2>they might be clever about certain </v>
<v Speaker 2>topics and uh,</v>

766
00:42:28.731 --> 00:42:30.740
<v Speaker 2>you can have fun having a few drinks </v>
<v Speaker 2>with them.</v>

767
00:42:31.130 --> 00:42:32.990
<v Speaker 2>So.</v>
<v Speaker 2>Well also,</v>

768
00:42:32.991 --> 00:42:35.030
<v Speaker 2>you know,</v>
<v Speaker 2>another example.</v>

769
00:42:35.170 --> 00:42:40.170
<v Speaker 2>So we can all relate to it of why it </v>
<v Speaker 2>doesn't have to be a terrible thing to </v>

770
00:42:40.170 --> 00:42:42.650
<v Speaker 2>be impressed with the friends of the </v>
<v Speaker 2>people that are even smarter than us all</v>

771
00:42:42.651 --> 00:42:45.560
<v Speaker 2>around is when,</v>
<v Speaker 2>when you and I were both two years old,</v>

772
00:42:45.561 --> 00:42:48.710
<v Speaker 2>I mean our parents were much more </v>
<v Speaker 2>intelligent than us right here.</v>

773
00:42:48.980 --> 00:42:53.980
<v Speaker 2>Worked out okay because their goals were</v>
<v Speaker 2>aligned with our goals and that I think </v>

774
00:42:54.861 --> 00:42:59.861
<v Speaker 2>is really the number one issue we have </v>
<v Speaker 2>to solve if we value in line with the </v>

775
00:43:01.521 --> 00:43:03.050
<v Speaker 2>value alignment problem.</v>
<v Speaker 2>Exactly.</v>

776
00:43:03.051 --> 00:43:06.500
<v Speaker 2>Because people who see too many </v>
<v Speaker 2>hollywood movies,</v>

777
00:43:06.550 --> 00:43:09.770
<v Speaker 2>a lousy science fiction,</v>
<v Speaker 2>a plot lines,</v>

778
00:43:10.010 --> 00:43:11.620
<v Speaker 2>they worry about the wrong thing,</v>
<v Speaker 2>right?</v>

779
00:43:12.140 --> 00:43:14.780
<v Speaker 2>They worry about some machines only </v>
<v Speaker 2>attorney evil.</v>

780
00:43:16.310 --> 00:43:20.520
<v Speaker 2>It's not malice that we,</v>
<v Speaker 2>that's the issue.</v>

781
00:43:20.830 --> 00:43:23.750
<v Speaker 2>The concern,</v>
<v Speaker 2>it's competence by definition.</v>

782
00:43:24.530 --> 00:43:27.410
<v Speaker 2>Intelligent makes you,</v>
<v Speaker 2>makes you very competent.</v>

783
00:43:27.411 --> 00:43:32.411
<v Speaker 2>Did you have a more intelligent goal </v>
<v Speaker 2>playing mr computer playing as the less </v>

784
00:43:33.021 --> 00:43:38.021
<v Speaker 2>intelligent one and when we define </v>
<v Speaker 2>intelligence is the ability to </v>

785
00:43:38.021 --> 00:43:38.021
<v Speaker 2>accomplish,</v>
<v Speaker 2>go winning,</v>

786
00:43:38.021 --> 00:43:39.410
<v Speaker 2>right?</v>
<v Speaker 2>It's going to be in the more intelligent</v>

787
00:43:39.411 --> 00:43:41.430
<v Speaker 2>one that wins and if you have</v>

788
00:43:42.760 --> 00:43:47.760
<v Speaker 2>are human and then you have um,</v>
<v Speaker 2>an agi and that's more intelligent than </v>

789
00:43:47.760 --> 00:43:50.200
<v Speaker 2>all ways and they have different goals.</v>
<v Speaker 2>Guess was going to get their way.</v>

790
00:43:50.201 --> 00:43:51.490
<v Speaker 2>Right?</v>
<v Speaker 2>So I was just reading about,</v>

791
00:43:52.000 --> 00:43:57.000
<v Speaker 2>I was just reading about this particular</v>
<v Speaker 2>rhinoceros species that was driven </v>

792
00:43:57.641 --> 00:44:02.641
<v Speaker 2>extinct just a few years ago.</v>
<v Speaker 2>I was looking at this cute picture of </v>

793
00:44:02.641 --> 00:44:05.140
<v Speaker 2>mommy rhinoceros with it's that child,</v>
<v Speaker 2>you know,</v>

794
00:44:05.141 --> 00:44:10.141
<v Speaker 2>and why did we humans drive it to </v>
<v Speaker 2>extinction wasn't as big as we were </v>

795
00:44:10.210 --> 00:44:12.070
<v Speaker 2>evolved rhino haters,</v>
<v Speaker 2>right?</v>

796
00:44:12.220 --> 00:44:13.810
<v Speaker 2>As a whole.</v>
<v Speaker 2>It was just because we,</v>

797
00:44:13.840 --> 00:44:16.930
<v Speaker 2>our goals weren't aligned with those of </v>
<v Speaker 2>the rhinoceros and it didn't work out so</v>

798
00:44:16.931 --> 00:44:18.970
<v Speaker 2>well for the rhinoceros because we were </v>
<v Speaker 2>more intelligent.</v>

799
00:44:19.000 --> 00:44:24.000
<v Speaker 2>Right?</v>
<v Speaker 2>So I think is just so important that if </v>

800
00:44:24.000 --> 00:44:26.671
<v Speaker 2>we ever do build agi before we unleash </v>
<v Speaker 2>anything,</v>

801
00:44:27.160 --> 00:44:32.160
<v Speaker 2>we have to make sure that it learns to </v>
<v Speaker 2>understand our goals adopts our goals.</v>

802
00:44:36.030 --> 00:44:36.630
<v Speaker 2>And it</v>

803
00:44:36.940 --> 00:44:38.770
<v Speaker 1>retains those goals.</v>
<v Speaker 1>So the cool,</v>

804
00:44:38.830 --> 00:44:43.830
<v Speaker 1>interesting problem there is being able </v>
<v Speaker 1>to us as human beings trying to </v>

805
00:44:44.801 --> 00:44:47.920
<v Speaker 1>formulate our values.</v>
<v Speaker 1>So you know,</v>

806
00:44:47.921 --> 00:44:50.740
<v Speaker 1>you could think of the United States </v>
<v Speaker 1>constitution as a,</v>

807
00:44:50.741 --> 00:44:54.490
<v Speaker 1>as a way that people sat down at the </v>
<v Speaker 1>time,</v>

808
00:44:54.491 --> 00:44:57.700
<v Speaker 1>a bunch of white men,</v>
<v Speaker 1>but which is a good example,</v>

809
00:44:57.701 --> 00:44:59.200
<v Speaker 1>I should,</v>
<v Speaker 1>should say,</v>

810
00:44:59.530 --> 00:45:04.530
<v Speaker 1>uh,</v>
<v Speaker 1>they formulated the goals for this </v>

811
00:45:04.530 --> 00:45:06.091
<v Speaker 1>country and a lot of people agree that </v>
<v Speaker 1>those goals actually held up pretty </v>

812
00:45:06.091 --> 00:45:09.391
<v Speaker 1>well.</v>
<v Speaker 1>It's an interesting formulation of </v>

813
00:45:09.391 --> 00:45:09.391
<v Speaker 1>values and failed miserably in other </v>
<v Speaker 1>ways.</v>

814
00:45:09.400 --> 00:45:13.360
<v Speaker 1>So for the value alignment problem and </v>
<v Speaker 1>the solution to it,</v>

815
00:45:13.361 --> 00:45:16.880
<v Speaker 1>we have to be able to put on paper,</v>
<v Speaker 1>uh,</v>

816
00:45:16.881 --> 00:45:18.210
<v Speaker 1>or in,</v>
<v Speaker 1>in a,</v>

817
00:45:18.211 --> 00:45:20.440
<v Speaker 1>in a program,</v>
<v Speaker 1>human values.</v>

818
00:45:20.460 --> 00:45:22.030
<v Speaker 1>How difficult do you think that is?</v>

819
00:45:22.390 --> 00:45:25.810
<v Speaker 2>Varied,</v>
<v Speaker 2>but it's so important.</v>

820
00:45:25.870 --> 00:45:29.800
<v Speaker 2>We really have to give it our best and </v>
<v Speaker 2>it's difficult for two separate reasons.</v>

821
00:45:30.190 --> 00:45:35.190
<v Speaker 2>There's the technical value alignment </v>
<v Speaker 2>problem of figuring out how to make </v>

822
00:45:37.270 --> 00:45:40.180
<v Speaker 2>machines understand the goals,</v>
<v Speaker 2>document routine them.</v>

823
00:45:40.480 --> 00:45:43.210
<v Speaker 2>And then there's the separate part of </v>
<v Speaker 2>it.</v>

824
00:45:43.211 --> 00:45:45.370
<v Speaker 2>The philosophical part was values </v>
<v Speaker 2>anyway.</v>

825
00:45:46.000 --> 00:45:51.000
<v Speaker 2>And since we,</v>
<v Speaker 2>it's not like we have any great </v>

826
00:45:51.000 --> 00:45:51.000
<v Speaker 2>consensus on this planet,</v>
<v Speaker 2>on values,</v>

827
00:45:51.000 --> 00:45:52.780
<v Speaker 2>how,</v>
<v Speaker 2>what mechanisms should we create then to</v>

828
00:45:52.781 --> 00:45:54.100
<v Speaker 2>aggregate them decide,</v>
<v Speaker 2>okay,</v>

829
00:45:54.101 --> 00:45:58.000
<v Speaker 2>what's a good compromise?</v>
<v Speaker 2>Right at that second discussion,</v>

830
00:45:58.001 --> 00:46:00.880
<v Speaker 2>can't just be left the tech nerds like </v>
<v Speaker 2>myself,</v>

831
00:46:00.910 --> 00:46:01.540
<v Speaker 2>right?</v>
<v Speaker 2>That's right.</v>

832
00:46:01.620 --> 00:46:05.680
<v Speaker 2>And if we refused to talk about it and </v>
<v Speaker 2>then agi gets built,</v>

833
00:46:05.730 --> 00:46:10.730
<v Speaker 2>who is going to be actually making the </v>
<v Speaker 2>decision about whose values it's gonna </v>

834
00:46:10.730 --> 00:46:11.170
<v Speaker 2>be a bunch of dudes and some tech </v>
<v Speaker 2>company.</v>

835
00:46:11.171 --> 00:46:16.171
<v Speaker 2>Okay.</v>
<v Speaker 2>And are they necessarily should so </v>

836
00:46:16.330 --> 00:46:19.290
<v Speaker 2>representative all of human kind that we</v>
<v Speaker 2>want to just enjoy the development.</v>

837
00:46:19.510 --> 00:46:24.510
<v Speaker 2>Are they even uniquely qualified to </v>
<v Speaker 2>speak to future human happiness just </v>

838
00:46:24.910 --> 00:46:29.910
<v Speaker 2>because they're good at programming ai?</v>
<v Speaker 2>I'd much rather have this be a really </v>

839
00:46:29.910 --> 00:46:29.910
<v Speaker 2>inclusive conversation,</v>

840
00:46:30.240 --> 00:46:32.530
<v Speaker 1>but do you think it's possible sort of </v>
<v Speaker 1>that.</v>

841
00:46:32.600 --> 00:46:36.600
<v Speaker 1>So you create a beautiful vision that </v>
<v Speaker 1>includes a.</v>

842
00:46:36.601 --> 00:46:41.601
<v Speaker 1>So the diversity,</v>
<v Speaker 1>cultural diversity and various </v>

843
00:46:41.601 --> 00:46:42.000
<v Speaker 1>perspectives on discussing rights,</v>
<v Speaker 1>freedoms,</v>

844
00:46:42.090 --> 00:46:47.090
<v Speaker 1>human dignity,</v>
<v Speaker 1>but how hard is it to come to that </v>

845
00:46:47.090 --> 00:46:47.160
<v Speaker 1>consensus?</v>
<v Speaker 1>Do you think?</v>

846
00:46:48.060 --> 00:46:51.690
<v Speaker 1>It's certainly a really important thing </v>
<v Speaker 1>that we should all try to do,</v>

847
00:46:51.900 --> 00:46:53.160
<v Speaker 1>but do you think it's feasible?</v>

848
00:46:54.260 --> 00:46:59.260
<v Speaker 2>I think there's no better way to </v>
<v Speaker 2>guarantee failure than through to refuse</v>

849
00:47:00.591 --> 00:47:05.591
<v Speaker 2>to talk about it or if you try,</v>
<v Speaker 2>and I also think it's a really bad at </v>

850
00:47:05.591 --> 00:47:05.810
<v Speaker 2>strategy to say,</v>
<v Speaker 2>okay,</v>

851
00:47:05.811 --> 00:47:10.811
<v Speaker 2>let's first have a discussion for a long</v>
<v Speaker 2>time and then once we reached complete </v>

852
00:47:10.811 --> 00:47:14.891
<v Speaker 2>consensus,</v>
<v Speaker 2>then we'll try to load it into some </v>

853
00:47:14.891 --> 00:47:14.891
<v Speaker 2>machine.</v>
<v Speaker 2>No,</v>

854
00:47:14.891 --> 00:47:16.280
<v Speaker 2>we shouldn't let perfect be the enemy of</v>
<v Speaker 2>good.</v>

855
00:47:16.550 --> 00:47:20.660
<v Speaker 2>Instead we should start with the </v>
<v Speaker 2>kindergarten ethics.</v>

856
00:47:20.770 --> 00:47:23.450
<v Speaker 2>Pretty much everybody agrees on and put </v>
<v Speaker 2>that into our machines.</v>

857
00:47:23.451 --> 00:47:26.650
<v Speaker 2>Now we're not doing that.</v>
<v Speaker 2>Even look at the,</v>

858
00:47:26.690 --> 00:47:31.690
<v Speaker 2>you know,</v>
<v Speaker 2>anyone who builds as a passenger </v>

859
00:47:31.690 --> 00:47:34.441
<v Speaker 2>aircraft wants it to never under any </v>
<v Speaker 2>circumstances and fly into a building or</v>

860
00:47:34.571 --> 00:47:35.320
<v Speaker 2>a mountain.</v>
<v Speaker 2>Right.</v>

861
00:47:35.710 --> 00:47:40.040
<v Speaker 2>Yet the september 11 hijackers were able</v>
<v Speaker 2>to do that and even more embarrassing,</v>

862
00:47:40.110 --> 00:47:41.820
<v Speaker 2>you know,</v>
<v Speaker 2>under his little bits,</v>

863
00:47:41.890 --> 00:47:46.600
<v Speaker 2>this depressed germanwings pilot when he</v>
<v Speaker 2>flew his passenger jeff into the alps,</v>

864
00:47:46.660 --> 00:47:47.710
<v Speaker 2>killing over a hundred people.</v>

865
00:47:48.460 --> 00:47:52.670
<v Speaker 2>He just told him what a pilot to do it.</v>
<v Speaker 2>He told the freaking computer change the</v>

866
00:47:52.710 --> 00:47:57.710
<v Speaker 2>altitude a hundred meters and even </v>
<v Speaker 2>though it had the gps maps everything,</v>

867
00:47:58.150 --> 00:47:59.740
<v Speaker 2>the computer was like,</v>
<v Speaker 2>okay,</v>

868
00:48:00.610 --> 00:48:04.240
<v Speaker 2>so we should.</v>
<v Speaker 2>We should take those very basic values,</v>

869
00:48:04.520 --> 00:48:07.000
<v Speaker 2>so where the problem is,</v>
<v Speaker 2>not that we don't agree,</v>

870
00:48:08.270 --> 00:48:13.270
<v Speaker 2>maybe a problem is just we've been too </v>
<v Speaker 2>lazy to try to put it into our machines </v>

871
00:48:13.270 --> 00:48:14.290
<v Speaker 2>and make sure that from now on arab </v>
<v Speaker 2>airplanes,</v>

872
00:48:14.291 --> 00:48:16.950
<v Speaker 2>we'll just,</v>
<v Speaker 2>which all have computers in them,</v>

873
00:48:16.960 --> 00:48:19.450
<v Speaker 2>but we'll just just refused to do </v>
<v Speaker 2>something like that,</v>

874
00:48:19.780 --> 00:48:21.990
<v Speaker 2>go into safe mode,</v>
<v Speaker 2>maybe lock the cockpit,</v>

875
00:48:22.000 --> 00:48:27.000
<v Speaker 2>the org or the nearest airport and and </v>
<v Speaker 2>there's so much other technology and in </v>

876
00:48:27.250 --> 00:48:32.250
<v Speaker 2>our world as well now where it's really </v>
<v Speaker 2>coming quite timely to put in some sort </v>

877
00:48:32.651 --> 00:48:35.080
<v Speaker 2>of very basic values like this.</v>
<v Speaker 2>Even in cars,</v>

878
00:48:35.720 --> 00:48:40.720
<v Speaker 2>we had enough a vehicle terrorism </v>
<v Speaker 2>attacks by now will be have driven </v>

879
00:48:40.720 --> 00:48:45.301
<v Speaker 2>trucks and vans into pedestrians.</v>
<v Speaker 2>That is not at all a crazy idea that it </v>

880
00:48:45.671 --> 00:48:49.360
<v Speaker 2>does have that hardwired into the car </v>
<v Speaker 2>because yeah,</v>

881
00:48:49.361 --> 00:48:54.361
<v Speaker 2>there were a lot of.</v>
<v Speaker 2>There's always going to be people who </v>

882
00:48:54.361 --> 00:48:56.581
<v Speaker 2>for some reason want to harm others,</v>
<v Speaker 2>but most of those people don't have the </v>

883
00:48:56.581 --> 00:48:58.570
<v Speaker 2>technical expertise to figure out how to</v>
<v Speaker 2>work around something like that.</v>

884
00:48:58.571 --> 00:49:01.780
<v Speaker 2>So if the car just won't do it,</v>
<v Speaker 2>it helps.</v>

885
00:49:01.781 --> 00:49:02.770
<v Speaker 2>So let's start there.</v>

886
00:49:02.840 --> 00:49:04.970
<v Speaker 1>So there's a lot of,</v>
<v Speaker 1>as a great point,</v>

887
00:49:04.971 --> 00:49:06.830
<v Speaker 1>so not,</v>
<v Speaker 1>not chasing perfect.</v>

888
00:49:06.831 --> 00:49:10.790
<v Speaker 1>There's a lot of things that,</v>
<v Speaker 1>a lot that most of the world agrees on.</v>

889
00:49:10.840 --> 00:49:11.810
<v Speaker 1>Yeah.</v>
<v Speaker 1>Let's start there.</v>

890
00:49:11.870 --> 00:49:12.470
<v Speaker 1>Let's start there.</v>

891
00:49:12.590 --> 00:49:12.960
<v Speaker 2>And,</v>
<v Speaker 2>and,</v>

892
00:49:13.060 --> 00:49:18.060
<v Speaker 2>and then once we start there,</v>
<v Speaker 2>we'll also get into the habit of having </v>

893
00:49:18.060 --> 00:49:18.560
<v Speaker 2>these kinds of conversations about,</v>
<v Speaker 2>okay,</v>

894
00:49:18.561 --> 00:49:20.200
<v Speaker 2>what else should we put in here?</v>
<v Speaker 2>And</v>

895
00:49:20.660 --> 00:49:23.520
<v Speaker 1>having these discussions.</v>
<v Speaker 1>This should be a gradual process then.</v>

896
00:49:23.940 --> 00:49:24.720
<v Speaker 1>Great.</v>
<v Speaker 1>So,</v>

897
00:49:25.140 --> 00:49:30.140
<v Speaker 1>but that also means describing these </v>
<v Speaker 1>things and describing it to a machine.</v>

898
00:49:31.230 --> 00:49:35.210
<v Speaker 1>So one thing we had a few conversations </v>
<v Speaker 1>with Steven Wolf from,</v>

899
00:49:35.660 --> 00:49:37.310
<v Speaker 1>I'm not sure if you're familiar with </v>
<v Speaker 1>stephen bosch.</v>

900
00:49:37.311 --> 00:49:39.470
<v Speaker 1>I know him quite well,</v>
<v Speaker 1>so he has,</v>

901
00:49:39.550 --> 00:49:40.240
<v Speaker 1>you know,</v>
<v Speaker 1>he played,</v>

902
00:49:40.250 --> 00:49:42.500
<v Speaker 1>he works at a bunch of things,</v>
<v Speaker 1>but you know,</v>

903
00:49:42.501 --> 00:49:46.370
<v Speaker 1>cellular automata,</v>
<v Speaker 1>these simple computable things,</v>

904
00:49:46.580 --> 00:49:49.520
<v Speaker 1>these computation systems and you kind </v>
<v Speaker 1>of mentioned that,</v>

905
00:49:49.521 --> 00:49:54.521
<v Speaker 1>you know,</v>
<v Speaker 1>we probably have already within these </v>

906
00:49:54.521 --> 00:49:54.560
<v Speaker 1>systems already something that's agi.</v>

907
00:49:55.280 --> 00:49:55.940
<v Speaker 2>Wow.</v>
<v Speaker 2>Um,</v>

908
00:49:56.100 --> 00:49:56.970
<v Speaker 2>meaning like</v>

909
00:49:57.710 --> 00:49:59.930
<v Speaker 1>we just don't know it because we can't </v>
<v Speaker 1>talk to it.</v>

910
00:50:00.410 --> 00:50:05.410
<v Speaker 1>So if you give me this chance to try to </v>
<v Speaker 1>try to at least form a question out of </v>

911
00:50:05.691 --> 00:50:06.590
<v Speaker 1>this is</v>

912
00:50:07.630 --> 00:50:10.080
<v Speaker 2>I think it's an interesting idea to.</v>

913
00:50:10.680 --> 00:50:12.660
<v Speaker 1>I think that we can have intelligent </v>
<v Speaker 1>systems,</v>

914
00:50:12.661 --> 00:50:17.661
<v Speaker 1>but we don't know how to describe </v>
<v Speaker 1>something to them and they can't </v>

915
00:50:17.661 --> 00:50:18.690
<v Speaker 1>communicate with us.</v>
<v Speaker 1>I know you're doing a little bit of work</v>

916
00:50:18.691 --> 00:50:21.540
<v Speaker 1>and explainable ai trying to get ai to </v>
<v Speaker 1>explain itself.</v>

917
00:50:22.020 --> 00:50:27.020
<v Speaker 1>So what are your thoughts of natural </v>
<v Speaker 1>language processIng or some kind of </v>

918
00:50:27.020 --> 00:50:28.000
<v Speaker 1>other communication?</v>
<v Speaker 1>How.</v>

919
00:50:28.100 --> 00:50:30.110
<v Speaker 1>How does the ai explained something to </v>
<v Speaker 1>us?</v>

920
00:50:30.111 --> 00:50:34.910
<v Speaker 1>How do we explain something to it to </v>
<v Speaker 1>machines or you think of it differently.</v>

921
00:50:35.310 --> 00:50:38.880
<v Speaker 2>So there are two separate parts to your </v>
<v Speaker 2>question there.</v>

922
00:50:38.900 --> 00:50:42.440
<v Speaker 2>Either one of them has to do with </v>
<v Speaker 2>communication,</v>

923
00:50:42.441 --> 00:50:44.210
<v Speaker 2>which is super interesting.</v>
<v Speaker 2>Yell and get that in a sec.</v>

924
00:50:44.550 --> 00:50:49.550
<v Speaker 2>The other is whether we already have agi</v>
<v Speaker 2>will you just haven't noticed that </v>

925
00:50:51.790 --> 00:50:52.810
<v Speaker 2>there.</v>
<v Speaker 2>I beg to differ.</v>

926
00:50:52.811 --> 00:50:57.811
<v Speaker 2>I don't think there's anything in any </v>
<v Speaker 2>cellular automaton or anything or the </v>

927
00:50:57.811 --> 00:51:02.731
<v Speaker 2>internet itself or whatever that has </v>
<v Speaker 2>artificial general intelligence ends it.</v>

928
00:51:03.890 --> 00:51:06.700
<v Speaker 2>It's an really do exactly everything we </v>
<v Speaker 2>humans can do better.</v>

929
00:51:07.000 --> 00:51:08.390
<v Speaker 2>I think that they,</v>
<v Speaker 2>if they,</v>

930
00:51:08.430 --> 00:51:11.620
<v Speaker 2>that happens,</v>
<v Speaker 2>when that happens,</v>

931
00:51:11.621 --> 00:51:15.640
<v Speaker 2>we will very soon notice will probably </v>
<v Speaker 2>notice even before and if,</v>

932
00:51:15.641 --> 00:51:17.050
<v Speaker 2>because in a very,</v>
<v Speaker 2>very big way,</v>

933
00:51:17.200 --> 00:51:17.790
<v Speaker 2>uh,</v>
<v Speaker 2>but for,</v>

934
00:51:17.840 --> 00:51:18.660
<v Speaker 2>for the second part,</v>

935
00:51:19.690 --> 00:51:20.740
<v Speaker 1>can I,</v>
<v Speaker 1>sorry.</v>

936
00:51:20.741 --> 00:51:22.060
<v Speaker 1>So,</v>
<v Speaker 1>uh,</v>

937
00:51:22.120 --> 00:51:27.120
<v Speaker 1>the cause you,</v>
<v Speaker 1>you have this beautiful way to </v>

938
00:51:27.120 --> 00:51:27.880
<v Speaker 1>formulating consciousness as a,</v>
<v Speaker 1>as a,</v>

939
00:51:28.810 --> 00:51:33.810
<v Speaker 1>you know,</v>
<v Speaker 1>as Information processing and you can </v>

940
00:51:33.810 --> 00:51:33.810
<v Speaker 1>think of intelligence and information </v>
<v Speaker 1>processing in this.</v>

941
00:51:33.810 --> 00:51:37.980
<v Speaker 1>You can think of the entire universe as </v>
<v Speaker 1>these particles and these systems </v>

942
00:51:37.980 --> 00:51:40.870
<v Speaker 1>roaming around that have this </v>
<v Speaker 1>information processing power.</v>

943
00:51:41.300 --> 00:51:42.010
<v Speaker 1>It,</v>
<v Speaker 1>you don't,</v>

944
00:51:42.430 --> 00:51:47.430
<v Speaker 1>you don't think there is something with </v>
<v Speaker 1>the power to process information in the </v>

945
00:51:47.591 --> 00:51:52.430
<v Speaker 1>way that we human beings do that's out </v>
<v Speaker 1>there that um,</v>

946
00:51:52.970 --> 00:51:57.970
<v Speaker 1>that needs to be sort of connected to.</v>
<v Speaker 1>It seems a little bit philosophical </v>

947
00:51:57.970 --> 00:52:01.990
<v Speaker 1>perhaps,</v>
<v Speaker 1>but there's something compelling to the </v>

948
00:52:01.990 --> 00:52:04.340
<v Speaker 1>idea that the power is already there,</v>
<v Speaker 1>which is the focus should be more on the</v>

949
00:52:04.930 --> 00:52:07.730
<v Speaker 1>and being able to communicate with it.</v>
<v Speaker 1>Well,</v>

950
00:52:08.420 --> 00:52:09.560
<v Speaker 1>I agree that the,</v>

951
00:52:10.750 --> 00:52:11.960
<v Speaker 2>and some,</v>
<v Speaker 2>in a certain sense,</v>

952
00:52:11.961 --> 00:52:14.900
<v Speaker 2>the hardware processing power is already</v>
<v Speaker 2>out there.</v>

953
00:52:15.610 --> 00:52:16.790
<v Speaker 2>Our universe itself</v>

954
00:52:18.400 --> 00:52:20.720
<v Speaker 2>can think of it as being a computer </v>
<v Speaker 2>already,</v>

955
00:52:20.780 --> 00:52:25.780
<v Speaker 2>right?</v>
<v Speaker 2>it's constantly computing what water </v>

956
00:52:25.780 --> 00:52:28.151
<v Speaker 2>waves,</v>
<v Speaker 2>how to evolve the waterway waves in the </v>

957
00:52:28.151 --> 00:52:30.401
<v Speaker 2>river charles and how to move the air </v>
<v Speaker 2>molecules around the seth lloyd has </v>

958
00:52:30.401 --> 00:52:34.191
<v Speaker 2>pointed out my colleague here that you </v>
<v Speaker 2>can even in a very rigorous way of </v>

959
00:52:34.191 --> 00:52:35.270
<v Speaker 2>thinking of our entire universe is being</v>
<v Speaker 2>a quantum computer.</v>

960
00:52:35.510 --> 00:52:40.510
<v Speaker 2>It's pretty clear that our universe </v>
<v Speaker 2>supports this amazing processing power </v>

961
00:52:40.510 --> 00:52:44.750
<v Speaker 2>because you can even within this physics</v>
<v Speaker 2>computer that we live in,</v>

962
00:52:44.751 --> 00:52:49.751
<v Speaker 2>right,</v>
<v Speaker 2>we can even build actually laptops and </v>

963
00:52:49.751 --> 00:52:49.751
<v Speaker 2>stuff so cleaner.</v>
<v Speaker 2>The power is there.</v>

964
00:52:49.751 --> 00:52:52.270
<v Speaker 2>It's just that most of the compute power</v>
<v Speaker 2>that nature has its,</v>

965
00:52:52.280 --> 00:52:57.280
<v Speaker 2>in my opinion,</v>
<v Speaker 2>kind of wasting on boring stuff like </v>

966
00:52:57.280 --> 00:52:57.450
<v Speaker 2>simulating yet another ocean wave </v>
<v Speaker 2>somewhere where no one is even looking.</v>

967
00:52:57.480 --> 00:52:59.900
<v Speaker 2>Right.</v>
<v Speaker 2>So in a sense or what life does,</v>

968
00:53:00.170 --> 00:53:04.400
<v Speaker 2>what we are doing when we build the </v>
<v Speaker 2>computers is where rechanneling all this</v>

969
00:53:04.910 --> 00:53:06.920
<v Speaker 2>compute that nature is doing anyway.</v>

970
00:53:07.030 --> 00:53:12.030
<v Speaker 2>Right?</v>
<v Speaker 2>And they're doing things that are more </v>

971
00:53:12.030 --> 00:53:12.030
<v Speaker 2>interesting than just yet another ocean </v>
<v Speaker 2>wave,</v>

972
00:53:12.030 --> 00:53:13.100
<v Speaker 2>you know,</v>
<v Speaker 2>and do something cool here.</v>

973
00:53:14.090 --> 00:53:17.110
<v Speaker 2>So the roy hardware power is there for </v>
<v Speaker 2>sure,</v>

974
00:53:17.120 --> 00:53:22.120
<v Speaker 2>But.</v>
<v Speaker 2>And then even just keep shooting what's </v>

975
00:53:22.120 --> 00:53:23.030
<v Speaker 2>going to happen for the next five </v>
<v Speaker 2>seconds in this water bottle,</v>

976
00:53:23.031 --> 00:53:28.031
<v Speaker 2>you know,</v>
<v Speaker 2>takes a ridiculous amount of compute if </v>

977
00:53:28.031 --> 00:53:28.031
<v Speaker 2>you do it on a human computer.</v>
<v Speaker 2>Yeah,</v>

978
00:53:28.031 --> 00:53:29.340
<v Speaker 2>just bought a ball,</v>
<v Speaker 2>does did it.</v>

979
00:53:29.940 --> 00:53:34.940
<v Speaker 2>But that does not mean that this water </v>
<v Speaker 2>bottle has agi and because agi means it </v>

980
00:53:35.911 --> 00:53:39.270
<v Speaker 2>should also be able to have written my </v>
<v Speaker 2>book done this interview.</v>

981
00:53:39.320 --> 00:53:44.320
<v Speaker 2>Yes.</v>
<v Speaker 2>And I don't think it's just the </v>

982
00:53:44.320 --> 00:53:46.160
<v Speaker 2>communication problems.</v>
<v Speaker 2>I think it's 10 do it and other </v>

983
00:53:47.101 --> 00:53:51.730
<v Speaker 2>buddhists say when they watch the water </v>
<v Speaker 2>and that there is some beauty that there</v>

984
00:53:51.731 --> 00:53:54.720
<v Speaker 2>is some depth in nature that they can </v>
<v Speaker 2>communicate with.</v>

985
00:53:54.870 --> 00:53:57.350
<v Speaker 2>Communication is also very important </v>
<v Speaker 2>because I mean,</v>

986
00:53:57.660 --> 00:54:02.660
<v Speaker 2>look,</v>
<v Speaker 2>I'm a part of my job is being a teacher </v>

987
00:54:02.660 --> 00:54:06.230
<v Speaker 2>and I know some very intelligent </v>
<v Speaker 2>professors even who just have a bit of </v>

988
00:54:08.080 --> 00:54:09.120
<v Speaker 2>hard time communicating.</v>

989
00:54:09.810 --> 00:54:14.520
<v Speaker 2>They have all these brilliant ideas,</v>
<v Speaker 2>but to communicate with somebody else,</v>

990
00:54:14.521 --> 00:54:16.650
<v Speaker 2>you have to also be able to simulate the</v>
<v Speaker 2>own mind.</v>

991
00:54:16.920 --> 00:54:21.920
<v Speaker 2>Yes.</v>
<v Speaker 2>Empathy built well enough that under </v>

992
00:54:21.920 --> 00:54:24.400
<v Speaker 2>that model of their mind that you can </v>
<v Speaker 2>say things that they will understand and</v>

993
00:54:24.540 --> 00:54:28.380
<v Speaker 2>that's quite difficult.</v>
<v Speaker 2>And that's why today it's so frustrating</v>

994
00:54:28.440 --> 00:54:33.200
<v Speaker 2>if you have a computer that's makes some</v>
<v Speaker 2>cancer diagnosis and you ask it,</v>

995
00:54:33.210 --> 00:54:38.210
<v Speaker 2>well,</v>
<v Speaker 2>why are you saying I should have this </v>

996
00:54:38.210 --> 00:54:38.210
<v Speaker 2>surgery if it and if you don't want to </v>
<v Speaker 2>reply,</v>

997
00:54:38.210 --> 00:54:42.990
<v Speaker 2>I was trained on five terabytes of data </v>
<v Speaker 2>and this is my diagnosis.</v>

998
00:54:43.771 --> 00:54:44.400
<v Speaker 2>Boop,</v>
<v Speaker 2>boop,</v>

999
00:54:44.401 --> 00:54:49.401
<v Speaker 2>beep,</v>
<v Speaker 2>beep doesn't really instill a lot of </v>

1000
00:54:49.401 --> 00:54:49.401
<v Speaker 2>confidence.</v>
<v Speaker 2>Right,</v>

1001
00:54:49.401 --> 00:54:53.450
<v Speaker 2>right.</v>
<v Speaker 2>So I think we have a lot of work to do </v>

1002
00:54:53.450 --> 00:54:53.910
<v Speaker 2>on um,</v>
<v Speaker 2>on communication there.</v>

1003
00:54:54.330 --> 00:54:55.890
<v Speaker 2>So what kind of,</v>
<v Speaker 2>we'll kind of,</v>

1004
00:54:55.891 --> 00:55:00.891
<v Speaker 2>um,</v>
<v Speaker 2>I think you're doing a little bit of </v>

1005
00:55:00.891 --> 00:55:00.891
<v Speaker 2>work and explainable ai,</v>
<v Speaker 2>uh,</v>

1006
00:55:00.891 --> 00:55:01.320
<v Speaker 2>what do you think are the most promising</v>
<v Speaker 2>avenues?</v>

1007
00:55:01.320 --> 00:55:06.320
<v Speaker 2>Is it mostly about sort of the alexa </v>
<v Speaker 2>problem of natural language processing,</v>

1008
00:55:06.691 --> 00:55:11.691
<v Speaker 2>of being able to actually use human </v>
<v Speaker 2>interpretable methods of communication?</v>

1009
00:55:13.110 --> 00:55:15.840
<v Speaker 2>So being able to talk to a system and it</v>
<v Speaker 2>talked back to you,</v>

1010
00:55:16.020 --> 00:55:18.480
<v Speaker 2>or is there some more fundamental </v>
<v Speaker 2>problems to be solved?</v>

1011
00:55:18.630 --> 00:55:20.920
<v Speaker 2>I think it's all of the above.</v>
<v Speaker 2>Human,</v>

1012
00:55:21.070 --> 00:55:23.520
<v Speaker 2>the natural language processing is </v>
<v Speaker 2>obviously important,</v>

1013
00:55:23.521 --> 00:55:27.560
<v Speaker 2>but they're also more nerdy fundamental </v>
<v Speaker 2>problems.</v>

1014
00:55:27.570 --> 00:55:30.130
<v Speaker 2>Like if you,</v>
<v Speaker 2>if you take a,</v>

1015
00:55:30.150 --> 00:55:34.280
<v Speaker 2>you play chess,</v>
<v Speaker 2>you have to have to go to,</v>

1016
00:55:38.040 --> 00:55:43.040
<v Speaker 2>when did you learn russian?</v>
<v Speaker 2>When you watch them teach yourself </v>

1017
00:55:44.001 --> 00:55:45.940
<v Speaker 2>rushing shit though.</v>
<v Speaker 2>Watch a mogul mom.</v>

1018
00:55:46.140 --> 00:55:48.240
<v Speaker 2>Bill stuff enough.</v>
<v Speaker 2>Wow.</v>

1019
00:55:50.320 --> 00:55:52.380
<v Speaker 2>Languages do you know?</v>
<v Speaker 2>Wow,</v>

1020
00:55:52.381 --> 00:55:56.320
<v Speaker 2>that's really impressive.</v>
<v Speaker 2>Wife has some contact basis.</v>

1021
00:55:56.330 --> 00:55:58.530
<v Speaker 2>But my point was if you play chess,</v>
<v Speaker 2>you have,</v>

1022
00:55:58.531 --> 00:56:00.330
<v Speaker 2>you looked at the alpha zero games,</v>
<v Speaker 2>the,</v>

1023
00:56:00.620 --> 00:56:01.680
<v Speaker 2>uh,</v>
<v Speaker 2>the actual games.</v>

1024
00:56:01.681 --> 00:56:04.800
<v Speaker 2>Now check it out.</v>
<v Speaker 2>Some of them are just mind blowing.</v>

1025
00:56:06.330 --> 00:56:07.920
<v Speaker 2>Really beautiful.</v>
<v Speaker 2>And,</v>

1026
00:56:07.921 --> 00:56:12.921
<v Speaker 2>and if you ask how did it do that?</v>
<v Speaker 2>You go talk to them as our base.</v>

1027
00:56:16.300 --> 00:56:20.460
<v Speaker 2>I know others from beat mine,</v>
<v Speaker 2>all they will ultimately be able to give</v>

1028
00:56:20.461 --> 00:56:25.461
<v Speaker 2>you is a big tables of numbers,</v>
<v Speaker 2>matrices that define the neural network </v>

1029
00:56:25.720 --> 00:56:30.720
<v Speaker 2>and you can stare at these people's </v>
<v Speaker 2>numbers until your face turns blue and </v>

1030
00:56:30.720 --> 00:56:34.270
<v Speaker 2>you're not going to understand much </v>
<v Speaker 2>about why it made that move.</v>

1031
00:56:34.540 --> 00:56:39.540
<v Speaker 2>And uh,</v>
<v Speaker 2>even if you have a natural language </v>

1032
00:56:39.540 --> 00:56:42.151
<v Speaker 2>processing,</v>
<v Speaker 2>they can tell you in human language </v>

1033
00:56:42.151 --> 00:56:42.151
<v Speaker 2>about,</v>
<v Speaker 2>oh,</v>

1034
00:56:42.151 --> 00:56:42.250
<v Speaker 2>five,</v>
<v Speaker 2>seven points to eight,</v>

1035
00:56:42.540 --> 00:56:47.540
<v Speaker 2>still not going to really help.</v>
<v Speaker 2>So I think think there's a whole </v>

1036
00:56:47.540 --> 00:56:50.521
<v Speaker 2>speCtrum of,</v>
<v Speaker 2>of a fun challenge they're involved in </v>

1037
00:56:50.521 --> 00:56:54.121
<v Speaker 2>and taking it computation does </v>
<v Speaker 2>intelligent things and transforming it </v>

1038
00:56:54.121 --> 00:56:58.141
<v Speaker 2>into something equally good,</v>
<v Speaker 2>equally intelligent,</v>

1039
00:56:59.770 --> 00:57:04.770
<v Speaker 2>but that's more understandable and I </v>
<v Speaker 2>think that's really valuable because I </v>

1040
00:57:04.770 --> 00:57:04.770
<v Speaker 2>think</v>

1041
00:57:05.860 --> 00:57:09.730
<v Speaker 2>as we put machines in charge of ever </v>
<v Speaker 2>more infrastructure in our world,</v>

1042
00:57:09.731 --> 00:57:12.580
<v Speaker 2>the power grid that is trading on the </v>
<v Speaker 2>stock market,</v>

1043
00:57:12.640 --> 00:57:17.640
<v Speaker 2>weapons systems and so on,</v>
<v Speaker 2>it's absolutely vital that we can trust </v>

1044
00:57:17.740 --> 00:57:22.740
<v Speaker 2>these ais to do all we want and trust </v>
<v Speaker 2>really comes from understanding in a </v>

1045
00:57:22.740 --> 00:57:24.980
<v Speaker 2>very fundamental way.</v>
<v Speaker 2>And um,</v>

1046
00:57:25.630 --> 00:57:27.760
<v Speaker 2>that's why I'm,</v>
<v Speaker 2>that's why I'm working on this because I</v>

1047
00:57:27.761 --> 00:57:30.510
<v Speaker 2>think the more if we're going to have </v>
<v Speaker 2>some hope of,</v>

1048
00:57:30.511 --> 00:57:35.511
<v Speaker 2>of ensuring that machines have adopted </v>
<v Speaker 2>our goals and that they're going to </v>

1049
00:57:35.511 --> 00:57:39.151
<v Speaker 2>retain them,</v>
<v Speaker 2>that kind of trust I think needs to be </v>

1050
00:57:39.431 --> 00:57:41.170
<v Speaker 2>based on things you can actually </v>
<v Speaker 2>understand,</v>

1051
00:57:41.171 --> 00:57:44.270
<v Speaker 2>preferably even make it had preferably </v>
<v Speaker 2>have improved serums on it.</v>

1052
00:57:44.350 --> 00:57:45.940
<v Speaker 2>Even with a self driving car,</v>
<v Speaker 2>right?</v>

1053
00:57:47.050 --> 00:57:52.050
<v Speaker 2>If someone just tells you it's been </v>
<v Speaker 2>trained on tons of data and it never </v>

1054
00:57:52.050 --> 00:57:52.050
<v Speaker 2>crashed,</v>
<v Speaker 2>it's,</v>

1055
00:57:52.050 --> 00:57:53.710
<v Speaker 2>it's less reassuring then if someone </v>
<v Speaker 2>actually has a proof,</v>

1056
00:57:54.220 --> 00:57:59.220
<v Speaker 2>maybe it's a computer verified proof,</v>
<v Speaker 2>but still it says that under no </v>

1057
00:57:59.220 --> 00:58:01.930
<v Speaker 2>circumstances is this car just going to </v>
<v Speaker 2>swerve into oncoming traffic</v>

1058
00:58:02.030 --> 00:58:04.160
<v Speaker 1>and,</v>
<v Speaker 1>and that kind of information helps build</v>

1059
00:58:04.161 --> 00:58:07.910
<v Speaker 1>trust and build the alignment,</v>
<v Speaker 1>the alignment of goals,</v>

1060
00:58:07.911 --> 00:58:11.920
<v Speaker 1>the at least awareness that your goals,</v>
<v Speaker 1>your values that align.</v>

1061
00:58:12.220 --> 00:58:14.500
<v Speaker 2>And I think even the short term,</v>
<v Speaker 2>if you look at her,</v>

1062
00:58:14.860 --> 00:58:16.390
<v Speaker 2>you know that today,</v>
<v Speaker 2>right?</v>

1063
00:58:16.391 --> 00:58:21.391
<v Speaker 2>This absolutely pathetic state of </v>
<v Speaker 2>cybersecurity that we have right when </v>

1064
00:58:21.391 --> 00:58:21.730
<v Speaker 2>it's,</v>
<v Speaker 2>what is it,</v>

1065
00:58:21.731 --> 00:58:26.731
<v Speaker 2>3 billion yahoo accounts,</v>
<v Speaker 2>which app pack almost every american's </v>

1066
00:58:29.741 --> 00:58:32.130
<v Speaker 2>credit card and so on,</v>
<v Speaker 2>uh,</v>

1067
00:58:32.800 --> 00:58:37.800
<v Speaker 2>why is this happening?</v>
<v Speaker 2>It's ultimately happening because we </v>

1068
00:58:37.800 --> 00:58:40.570
<v Speaker 2>have software took nobody fully </v>
<v Speaker 2>understood how it worked.</v>

1069
00:58:41.260 --> 00:58:44.200
<v Speaker 2>That's why the bugs hadn't been found.</v>
<v Speaker 2>Right?</v>

1070
00:58:44.210 --> 00:58:49.210
<v Speaker 2>Right.</v>
<v Speaker 2>And I think ai can be used very </v>

1071
00:58:49.210 --> 00:58:51.400
<v Speaker 2>effectively for offense for hacking,</v>
<v Speaker 2>but it can also be used for defense.</v>

1072
00:58:52.390 --> 00:58:57.390
<v Speaker 2>Hopefully automating verifiability and </v>
<v Speaker 2>creating systems that are built in </v>

1073
00:59:00.361 --> 00:59:02.340
<v Speaker 2>different ways so you can actually prove</v>
<v Speaker 2>things about them.</v>

1074
00:59:02.950 --> 00:59:04.410
<v Speaker 2>And it's,</v>
<v Speaker 2>it's important.</v>

1075
00:59:05.290 --> 00:59:08.470
<v Speaker 1>So speaking of software that nobody </v>
<v Speaker 1>understands how it works,</v>

1076
00:59:08.740 --> 00:59:13.740
<v Speaker 1>of course a bunch of people asked by </v>
<v Speaker 1>your paper about your thoughts of why </v>

1077
00:59:13.740 --> 00:59:15.430
<v Speaker 1>does deep and cheap learning work so </v>
<v Speaker 1>well as the paper,</v>

1078
00:59:15.431 --> 00:59:16.410
<v Speaker 1>but um,</v>
<v Speaker 1>what,</v>

1079
00:59:16.411 --> 00:59:17.980
<v Speaker 1>what,</v>
<v Speaker 1>what are your thoughts on deep learning,</v>

1080
00:59:18.280 --> 00:59:23.280
<v Speaker 1>these kind of simplified models of our </v>
<v Speaker 1>own brains and have been able to do some</v>

1081
00:59:23.870 --> 00:59:28.760
<v Speaker 1>successful perception work pattern </v>
<v Speaker 1>recognition work and now with alpha zero</v>

1082
00:59:28.761 --> 00:59:29.880
<v Speaker 1>and so on,</v>
<v Speaker 1>do some,</v>

1083
00:59:29.960 --> 00:59:33.110
<v Speaker 1>some clever things.</v>
<v Speaker 1>What are your thoughts about the promise</v>

1084
00:59:33.111 --> 00:59:35.210
<v Speaker 1>limitations of this piece?</v>

1085
00:59:35.650 --> 00:59:40.650
<v Speaker 2>Right.</v>
<v Speaker 2>I think there are a number of very </v>

1086
00:59:41.031 --> 00:59:46.031
<v Speaker 2>important insights,</v>
<v Speaker 2>very important lessons we can already </v>

1087
00:59:46.031 --> 00:59:49.340
<v Speaker 2>draw from these kinds of successes.</v>
<v Speaker 2>One of them is when you look at the </v>

1088
00:59:49.340 --> 00:59:50.270
<v Speaker 2>human brain and you see it's very </v>
<v Speaker 2>complicated,</v>

1089
00:59:50.300 --> 00:59:55.300
<v Speaker 2>10th of 11 neurons and they're all </v>
<v Speaker 2>different kinds of neurons and yada </v>

1090
00:59:55.300 --> 00:59:58.631
<v Speaker 2>yada.</v>
<v Speaker 2>And there's been as long debate about </v>

1091
00:59:58.631 --> 01:00:00.700
<v Speaker 2>whether the fact that we have dozens of </v>
<v Speaker 2>different kinds is actually necessary </v>

1092
01:00:00.700 --> 01:00:02.020
<v Speaker 2>for intelligence,</v>
<v Speaker 2>which are,</v>

1093
01:00:02.021 --> 01:00:04.870
<v Speaker 2>I think quite convincingly answer that </v>
<v Speaker 2>question of no,</v>

1094
01:00:05.740 --> 01:00:10.740
<v Speaker 2>it's enough to have just one kind.</v>
<v Speaker 2>If you look under the hood of alpha </v>

1095
01:00:10.740 --> 01:00:11.290
<v Speaker 2>zero,</v>
<v Speaker 2>there's only one kind of neuron and this</v>

1096
01:00:11.291 --> 01:00:14.770
<v Speaker 2>ridiculously simple,</v>
<v Speaker 2>that simple mathematical thing,</v>

1097
01:00:14.970 --> 01:00:17.260
<v Speaker 2>so it's not the.</v>
<v Speaker 2>It's just like in physics,</v>

1098
01:00:17.261 --> 01:00:20.290
<v Speaker 2>it's not the.</v>
<v Speaker 2>If you have a gas with waves in it,</v>

1099
01:00:20.320 --> 01:00:22.560
<v Speaker 2>it's not the detailed nature of the </v>
<v Speaker 2>molecule,</v>

1100
01:00:22.561 --> 01:00:25.440
<v Speaker 2>doesn't matter.</v>
<v Speaker 2>It's the collective behavior.</v>

1101
01:00:25.450 --> 01:00:27.980
<v Speaker 2>Somehow.</v>
<v Speaker 2>Similarly it's,</v>

1102
01:00:28.490 --> 01:00:33.490
<v Speaker 2>it's,</v>
<v Speaker 2>it's this higher level structure of the </v>

1103
01:00:33.490 --> 01:00:34.040
<v Speaker 2>network matters.</v>
<v Speaker 2>Not that you have 20 kinds of nuances.</v>

1104
01:00:34.060 --> 01:00:39.060
<v Speaker 2>I think my brain is such a complicated </v>
<v Speaker 2>mess because it wasn't devolved just to </v>

1105
01:00:41.151 --> 01:00:46.151
<v Speaker 2>be intelligent.</v>
<v Speaker 2>They wasn't involved to also be self </v>

1106
01:00:46.151 --> 01:00:48.090
<v Speaker 2>assembling,</v>
<v Speaker 2>right and self repairing.</v>

1107
01:00:48.091 --> 01:00:52.910
<v Speaker 2>Right,</v>
<v Speaker 2>and the evolutionarily attainable and so</v>

1108
01:00:52.911 --> 01:00:57.911
<v Speaker 2>on.</v>
<v Speaker 2>So I think it's pretty my hunches that </v>

1109
01:00:57.911 --> 01:01:01.171
<v Speaker 2>we're going to understand how to build </v>
<v Speaker 2>agi before we fully understand how our </v>

1110
01:01:01.171 --> 01:01:04.761
<v Speaker 2>brains work.</v>
<v Speaker 2>We we understood how to build flying </v>

1111
01:01:04.761 --> 01:01:07.530
<v Speaker 2>machines long before we were able to </v>
<v Speaker 2>build a mechanical work bird.</v>

1112
01:01:07.770 --> 01:01:08.370
<v Speaker 2>Yeah,</v>
<v Speaker 2>that's all right.</v>

1113
01:01:08.400 --> 01:01:12.180
<v Speaker 2>You're given.</v>
<v Speaker 2>You're given that the example exactly.</v>

1114
01:01:12.420 --> 01:01:16.170
<v Speaker 2>Mechanical birds and airplanes and </v>
<v Speaker 2>airplanes do a pretty good job of flying</v>

1115
01:01:16.171 --> 01:01:19.230
<v Speaker 2>without really mimicking bird flight.</v>
<v Speaker 2>And even now,</v>

1116
01:01:19.231 --> 01:01:20.880
<v Speaker 2>after 100 years,</v>
<v Speaker 2>100 years later,</v>

1117
01:01:20.890 --> 01:01:25.890
<v Speaker 2>they just see the ted talk with this </v>
<v Speaker 2>german mechanical heard you mentioned </v>

1118
01:01:25.890 --> 01:01:25.890
<v Speaker 2>it.</v>

1119
01:01:25.890 --> 01:01:27.390
<v Speaker 2>IT's amazing.</v>
<v Speaker 2>But even after that,</v>

1120
01:01:27.410 --> 01:01:32.410
<v Speaker 2>right,</v>
<v Speaker 2>we still don't fly and mechanical bar </v>

1121
01:01:32.410 --> 01:01:34.340
<v Speaker 2>because it turned out the way we came up</v>
<v Speaker 2>with simpler is better if our purpose </v>

1122
01:01:34.340 --> 01:01:35.100
<v Speaker 2>is.</v>
<v Speaker 2>And I think it might be the same there.</v>

1123
01:01:35.250 --> 01:01:39.150
<v Speaker 2>So that's one lesson.</v>
<v Speaker 2>And another lesson,</v>

1124
01:01:39.480 --> 01:01:42.480
<v Speaker 2>which is more what the paper was about.</v>
<v Speaker 2>Well,</v>

1125
01:01:42.640 --> 01:01:47.640
<v Speaker 2>first I,</v>
<v Speaker 2>a physicist thought it was fascinating </v>

1126
01:01:47.640 --> 01:01:49.920
<v Speaker 2>how there's a very close mathematical </v>
<v Speaker 2>relationship actually between artificial</v>

1127
01:01:49.921 --> 01:01:50.950
<v Speaker 2>neural networks.</v>
<v Speaker 2>Um,</v>

1128
01:01:50.951 --> 01:01:55.951
<v Speaker 2>a lot of things that we've studied for </v>
<v Speaker 2>and physics go by nerdy names like the </v>

1129
01:01:55.951 --> 01:01:59.190
<v Speaker 2>renormalization group equation and um,</v>
<v Speaker 2>opinions and yada,</v>

1130
01:01:59.191 --> 01:01:59.760
<v Speaker 2>yada,</v>
<v Speaker 2>yada.</v>

1131
01:01:59.761 --> 01:02:01.200
<v Speaker 2>And,</v>
<v Speaker 2>and,</v>

1132
01:02:01.201 --> 01:02:06.201
<v Speaker 2>um,</v>
<v Speaker 2>when you look a liTtle more closely at </v>

1133
01:02:06.201 --> 01:02:07.060
<v Speaker 2>this,</v>
<v Speaker 2>you have a,</v>

1134
01:02:08.830 --> 01:02:11.020
<v Speaker 2>you,</v>
<v Speaker 2>as far as I was like,</v>

1135
01:02:11.070 --> 01:02:16.070
<v Speaker 2>whoa,</v>
<v Speaker 2>there's something crazy here that </v>

1136
01:02:16.070 --> 01:02:17.481
<v Speaker 2>doesn't make sense because we know that </v>
<v Speaker 2>if you even want to a super simple </v>

1137
01:02:19.951 --> 01:02:23.430
<v Speaker 2>neural network pel and tap pictures and </v>
<v Speaker 2>dog pictures,</v>

1138
01:02:23.431 --> 01:02:24.580
<v Speaker 2>right?</v>
<v Speaker 2>That you can do that very well.</v>

1139
01:02:24.600 --> 01:02:29.040
<v Speaker 2>Very well now.</v>
<v Speaker 2>but if you think about it a little bit,</v>

1140
01:02:29.041 --> 01:02:34.041
<v Speaker 2>do you convince yourself that must be </v>
<v Speaker 2>impossible because if I have one </v>

1141
01:02:34.041 --> 01:02:37.320
<v Speaker 2>megapixel,</v>
<v Speaker 2>even if each pixel is just black or </v>

1142
01:02:37.320 --> 01:02:40.401
<v Speaker 2>white,</v>
<v Speaker 2>there's to the power 1 million possible </v>

1143
01:02:40.401 --> 01:02:40.620
<v Speaker 2>images is way more than there are atoms </v>
<v Speaker 2>in the universe,</v>

1144
01:02:40.621 --> 01:02:42.450
<v Speaker 2>right?</v>
<v Speaker 2>So in order to.</v>

1145
01:02:43.740 --> 01:02:46.380
<v Speaker 2>And then for eaCh one of those I have to</v>
<v Speaker 2>assign a number,</v>

1146
01:02:46.381 --> 01:02:48.210
<v Speaker 2>which is the probability of that.</v>
<v Speaker 2>It's a dog,</v>

1147
01:02:48.420 --> 01:02:50.910
<v Speaker 2>right?</v>
<v Speaker 2>So an arbitrary function of images</v>

1148
01:02:52.080 --> 01:02:56.610
<v Speaker 2>is a list of more numbers than there are</v>
<v Speaker 2>atoms in our universe.</v>

1149
01:02:56.730 --> 01:02:58.940
<v Speaker 2>So clearly I can't store that under the </v>
<v Speaker 2>hood of my,</v>

1150
01:02:59.000 --> 01:03:02.430
<v Speaker 2>my gpu or my computer,</v>
<v Speaker 2>yet somehow works.</v>

1151
01:03:02.970 --> 01:03:04.200
<v Speaker 2>So whaT does that mean?</v>
<v Speaker 2>Well,</v>

1152
01:03:04.201 --> 01:03:09.201
<v Speaker 2>it means that out of all of the problems</v>
<v Speaker 2>that you could try to solve with the </v>

1153
01:03:09.871 --> 01:03:14.871
<v Speaker 2>neural network,</v>
<v Speaker 2>almost all of them are impossible to </v>

1154
01:03:14.871 --> 01:03:16.750
<v Speaker 2>solve with a reasonably sized one.</v>

1155
01:03:17.800 --> 01:03:20.050
<v Speaker 2>But then what we showed in our paper </v>
<v Speaker 2>was,</v>

1156
01:03:20.620 --> 01:03:22.550
<v Speaker 2>was that the,</v>
<v Speaker 2>the,</v>

1157
01:03:23.040 --> 01:03:28.040
<v Speaker 2>the kind of problems,</v>
<v Speaker 2>the fraction of all the problems that </v>

1158
01:03:28.040 --> 01:03:30.340
<v Speaker 2>you could possibly pose that we actually</v>
<v Speaker 2>care about,</v>

1159
01:03:30.370 --> 01:03:33.790
<v Speaker 2>given the laws of physics,</v>
<v Speaker 2>is also an infant of testimony.</v>

1160
01:03:33.791 --> 01:03:36.850
<v Speaker 2>Tiny little part and amazingly they were</v>
<v Speaker 2>basically the same part.</v>

1161
01:03:37.460 --> 01:03:42.460
<v Speaker 2>Yeah.</v>
<v Speaker 2>It's almost like the world was created </v>

1162
01:03:42.460 --> 01:03:42.460
<v Speaker 2>for.</v>
<v Speaker 2>I mean they kind of come together.</v>

1163
01:03:42.460 --> 01:03:46.360
<v Speaker 2>Yeah.</v>
<v Speaker 2>You could say maybe where the world </v>

1164
01:03:46.360 --> 01:03:46.360
<v Speaker 2>created the world.</v>
<v Speaker 2>The world was created for us,</v>

1165
01:03:46.360 --> 01:03:47.260
<v Speaker 2>but I have a more modest than </v>
<v Speaker 2>interpretation,</v>

1166
01:03:47.261 --> 01:03:52.261
<v Speaker 2>which is that instead evolution in that,</v>
<v Speaker 2>but neural networks are precisely for </v>

1167
01:03:52.261 --> 01:03:53.050
<v Speaker 2>that reason,</v>
<v Speaker 2>right?</v>

1168
01:03:53.051 --> 01:03:58.051
<v Speaker 2>Because this particular architecture as </v>
<v Speaker 2>opposed to the one in your laptop is </v>

1169
01:03:58.051 --> 01:04:01.530
<v Speaker 2>very,</v>
<v Speaker 2>very well adapted to solving the kinds </v>

1170
01:04:01.891 --> 01:04:05.320
<v Speaker 2>of problems that nature chapter </v>
<v Speaker 2>presenting yet our ancestors with.</v>

1171
01:04:05.321 --> 01:04:10.321
<v Speaker 2>Right.</v>
<v Speaker 2>So it makes sense that why do we have a </v>

1172
01:04:10.321 --> 01:04:12.721
<v Speaker 2>brain in the first place?</v>
<v Speaker 2>It's to be able to make predictions </v>

1173
01:04:12.721 --> 01:04:15.541
<v Speaker 2>about the future and so on.</v>
<v Speaker 2>So if we had a sucky system which could </v>

1174
01:04:15.541 --> 01:04:17.730
<v Speaker 2>never solve,</v>
<v Speaker 2>it wouldn't have been so.</v>

1175
01:04:17.850 --> 01:04:18.720
<v Speaker 2>But it's so this,</v>
<v Speaker 2>this,</v>

1176
01:04:18.721 --> 01:04:19.900
<v Speaker 2>this,</v>
<v Speaker 2>this is a.</v>

1177
01:04:21.960 --> 01:04:24.330
<v Speaker 2>I think you're very beautiful fact.</v>
<v Speaker 2>We also,</v>

1178
01:04:24.331 --> 01:04:28.260
<v Speaker 2>we also realize that there's a there </v>
<v Speaker 2>that we have been.</v>

1179
01:04:28.290 --> 01:04:29.640
<v Speaker 2>It's been earlier work on,</v>

1180
01:04:30.570 --> 01:04:32.080
<v Speaker 2>yes,</v>
<v Speaker 2>deeper networks are good,</v>

1181
01:04:32.081 --> 01:04:37.081
<v Speaker 2>but we were able to show an additional </v>
<v Speaker 2>cool factor which is that the even </v>

1182
01:04:37.081 --> 01:04:41.971
<v Speaker 2>incredibly simple problems like support,</v>
<v Speaker 2>like give you a follow the numbers and </v>

1183
01:04:41.971 --> 01:04:46.291
<v Speaker 2>asked you to multiply them together and </v>
<v Speaker 2>already you can write a few lines of </v>

1184
01:04:46.291 --> 01:04:46.291
<v Speaker 2>code,</v>
<v Speaker 2>boom,</v>

1185
01:04:46.291 --> 01:04:48.460
<v Speaker 2>done trivial.</v>
<v Speaker 2>If you just try to do that with a neural</v>

1186
01:04:48.461 --> 01:04:51.520
<v Speaker 2>network that has only one single hidden </v>
<v Speaker 2>layer in it,</v>

1187
01:04:52.430 --> 01:04:53.270
<v Speaker 2>you can do it,</v>

1188
01:04:54.350 --> 01:04:59.350
<v Speaker 2>but you're going to need two to the </v>
<v Speaker 2>power of thousand neurons and to </v>

1189
01:04:59.560 --> 01:05:04.560
<v Speaker 2>multiply the numbers,</v>
<v Speaker 2>which is again more neurons than there </v>

1190
01:05:04.560 --> 01:05:05.300
<v Speaker 2>are atoms in our universe.</v>
<v Speaker 2>That's not saying,</v>

1191
01:05:05.510 --> 01:05:10.510
<v Speaker 2>but if you're allowed,</v>
<v Speaker 2>if you allow yourself make it a deep </v>

1192
01:05:10.510 --> 01:05:12.450
<v Speaker 2>networks with many layers,</v>
<v Speaker 2>you only need 4,000</v>

1193
01:05:12.490 --> 01:05:14.360
<v Speaker 2>euros.</v>
<v Speaker 2>It's perfectly feasible.</v>

1194
01:05:15.180 --> 01:05:18.270
<v Speaker 2>So that's an.</v>
<v Speaker 2>Yeah.</v>

1195
01:05:18.470 --> 01:05:23.470
<v Speaker 2>So on another architecture type,</v>
<v Speaker 2>I mean you mentioned schrodinger's </v>

1196
01:05:23.470 --> 01:05:27.221
<v Speaker 2>equation and what are your thoughts </v>
<v Speaker 2>about quantum computing and the role of </v>

1197
01:05:29.421 --> 01:05:34.421
<v Speaker 2>this kind of computational unit and </v>
<v Speaker 2>creating an intelligence system in some </v>

1198
01:05:35.241 --> 01:05:40.241
<v Speaker 2>hollywood movies.</v>
<v Speaker 2>Not mentioned by name because I don't </v>

1199
01:05:40.241 --> 01:05:44.741
<v Speaker 2>want to spoil them.</v>
<v Speaker 2>The way they get agi is building a </v>

1200
01:05:44.741 --> 01:05:46.670
<v Speaker 2>quantum computer because of the word </v>
<v Speaker 2>quantum.</v>

1201
01:05:46.671 --> 01:05:47.610
<v Speaker 2>Sounds cool.</v>
<v Speaker 2>And so on.</v>

1202
01:05:48.830 --> 01:05:50.510
<v Speaker 2>My,</v>
<v Speaker 2>first of all,</v>

1203
01:05:50.511 --> 01:05:53.990
<v Speaker 2>I think we don't need quantum computers.</v>
<v Speaker 2>They build agi.</v>

1204
01:05:54.950 --> 01:05:59.950
<v Speaker 2>I suspect your brain is not quantum </v>
<v Speaker 2>computer and a new found sense.</v>

1205
01:06:01.520 --> 01:06:04.280
<v Speaker 2>So you don't even wrote a paper about </v>
<v Speaker 2>that many years ago.</v>

1206
01:06:04.560 --> 01:06:09.560
<v Speaker 2>I checked the data,</v>
<v Speaker 2>the decoherence decoherence time that </v>

1207
01:06:09.560 --> 01:06:13.150
<v Speaker 2>how long it takes until the quantum </v>
<v Speaker 2>computer ness of what your new orleans </v>

1208
01:06:13.150 --> 01:06:16.780
<v Speaker 2>doing gets erased by just random noise </v>
<v Speaker 2>from the environment.</v>

1209
01:06:17.990 --> 01:06:20.600
<v Speaker 2>And if it's about 10 to the minus 21 </v>
<v Speaker 2>seconds.</v>

1210
01:06:21.320 --> 01:06:25.070
<v Speaker 2>So as cool as it would be that have a </v>
<v Speaker 2>quantum computer in my head,</v>

1211
01:06:25.071 --> 01:06:28.010
<v Speaker 2>I don't think that fast.</v>
<v Speaker 2>On the other hand,</v>

1212
01:06:28.370 --> 01:06:29.150
<v Speaker 2>there are</v>

1213
01:06:31.490 --> 01:06:34.070
<v Speaker 2>very cool things you could do with </v>
<v Speaker 2>quantum computers.</v>

1214
01:06:35.270 --> 01:06:37.730
<v Speaker 2>Alright.</v>
<v Speaker 2>I think we'll be able to do soon when we</v>

1215
01:06:37.731 --> 01:06:42.731
<v Speaker 2>get big ones,</v>
<v Speaker 2>bigger ones that might actually help </v>

1216
01:06:42.731 --> 01:06:42.731
<v Speaker 2>machine learning do even better than the</v>
<v Speaker 2>brain.</v>

1217
01:06:43.150 --> 01:06:45.530
<v Speaker 2>So for example,</v>

1218
01:06:47.090 --> 01:06:50.210
<v Speaker 2>one,</v>
<v Speaker 2>this is a moonshot,</v>

1219
01:06:50.780 --> 01:06:52.010
<v Speaker 2>but um,</v>
<v Speaker 2>I'm,</v>

1220
01:06:52.860 --> 01:06:57.860
<v Speaker 2>you know,</v>
<v Speaker 2>learning is very much the same thing as </v>

1221
01:06:58.911 --> 01:07:01.760
<v Speaker 2>a search.</v>
<v Speaker 2>If you have a,</v>

1222
01:07:01.790 --> 01:07:03.140
<v Speaker 2>if you're trying to train a neural </v>
<v Speaker 2>network,</v>

1223
01:07:03.170 --> 01:07:05.750
<v Speaker 2>they get really learned to do something </v>
<v Speaker 2>really well.</v>

1224
01:07:06.260 --> 01:07:07.790
<v Speaker 2>You have some loss function,</v>
<v Speaker 2>you have some,</v>

1225
01:07:08.360 --> 01:07:13.360
<v Speaker 2>you have a bunch of knobs you can turn </v>
<v Speaker 2>represented by a bunch of numbers and </v>

1226
01:07:13.360 --> 01:07:17.201
<v Speaker 2>you're trying to tweak them so that it </v>
<v Speaker 2>becomes as good as possible as this </v>

1227
01:07:17.201 --> 01:07:19.781
<v Speaker 2>thing,</v>
<v Speaker 2>so if you think have a landscape with </v>

1228
01:07:19.781 --> 01:07:23.240
<v Speaker 2>some valley where each dimension of the </v>
<v Speaker 2>landscape corresponds to some number you</v>

1229
01:07:23.241 --> 01:07:28.241
<v Speaker 2>can change.</v>
<v Speaker 2>You're trying to find the minimum and </v>

1230
01:07:28.241 --> 01:07:28.910
<v Speaker 2>it's well known that if you have a very </v>
<v Speaker 2>high dimensional landscape,</v>

1231
01:07:29.060 --> 01:07:31.280
<v Speaker 2>complicated things,</v>
<v Speaker 2>it's super hard to find the minimum.</v>

1232
01:07:31.340 --> 01:07:35.580
<v Speaker 2>Right?</v>
<v Speaker 2>Quantum mechanics is amazing.</v>

1233
01:07:35.620 --> 01:07:36.570
<v Speaker 2>Good at this,</v>
<v Speaker 2>right?</v>

1234
01:07:36.750 --> 01:07:41.750
<v Speaker 2>I get it.</v>
<v Speaker 2>If I want to know what's the lowest </v>

1235
01:07:41.750 --> 01:07:41.750
<v Speaker 2>energy state,</v>
<v Speaker 2>this water can possibly have</v>

1236
01:07:42.510 --> 01:07:44.520
<v Speaker 2>incredibly hard to compute,</v>
<v Speaker 2>but we can put.</v>

1237
01:07:44.590 --> 01:07:47.590
<v Speaker 2>Nature will happily figure this out for </v>
<v Speaker 2>you if you just cool it down,</v>

1238
01:07:47.700 --> 01:07:48.850
<v Speaker 2>make it very,</v>
<v Speaker 2>very cold.</v>

1239
01:07:50.740 --> 01:07:53.290
<v Speaker 2>If you put a ball somewhere,</v>
<v Speaker 2>it'll roll down to its minimum,</v>

1240
01:07:53.291 --> 01:07:58.291
<v Speaker 2>and this happens metaphorically at the </v>
<v Speaker 2>energy landscape to and quantum </v>

1241
01:07:58.291 --> 01:08:00.280
<v Speaker 2>mechanics.</v>
<v Speaker 2>Even using some clever tricks,</v>

1242
01:08:00.310 --> 01:08:05.310
<v Speaker 2>which today is machine learning systems </v>
<v Speaker 2>don't like if you're trying to find the </v>

1243
01:08:05.310 --> 01:08:09.931
<v Speaker 2>minimum when you get stuck in the little</v>
<v Speaker 2>local minimum here in quantum mechanics </v>

1244
01:08:09.931 --> 01:08:13.020
<v Speaker 2>or connects the tunnel through the </v>
<v Speaker 2>barrier and get unstuck again.</v>

1245
01:08:13.770 --> 01:08:15.040
<v Speaker 2>And um,</v>
<v Speaker 2>that's really.</v>

1246
01:08:15.440 --> 01:08:15.610
<v Speaker 2>Yeah.</v>
<v Speaker 2>So,</v>

1247
01:08:15.611 --> 01:08:20.611
<v Speaker 2>so maybe for example,</v>
<v Speaker 2>we will one day use quantum computers </v>

1248
01:08:20.611 --> 01:08:23.360
<v Speaker 2>that help train neural networks better.</v>

1249
01:08:23.790 --> 01:08:25.050
<v Speaker 1>That's really interesting.</v>
<v Speaker 1>Okay.</v>

1250
01:08:25.051 --> 01:08:28.080
<v Speaker 1>So as a component of the learning </v>
<v Speaker 1>process,</v>

1251
01:08:28.081 --> 01:08:29.220
<v Speaker 1>for example.</v>
<v Speaker 1>Yeah.</v>

1252
01:08:29.790 --> 01:08:33.450
<v Speaker 1>Let me ask sort of wrapping up here a </v>
<v Speaker 1>little bit.</v>

1253
01:08:33.451 --> 01:08:34.860
<v Speaker 1>Let me,</v>
<v Speaker 1>let me return to,</v>

1254
01:08:34.861 --> 01:08:37.320
<v Speaker 1>uh,</v>
<v Speaker 1>the questions of our human nature and,</v>

1255
01:08:37.860 --> 01:08:41.880
<v Speaker 1>and love as I mentioned.</v>
<v Speaker 1>So do you think,</v>

1256
01:08:44.640 --> 01:08:49.640
<v Speaker 1>you mentioned sort of a helper robots,</v>
<v Speaker 1>but you can think of also personal </v>

1257
01:08:49.640 --> 01:08:52.611
<v Speaker 1>robots.</v>
<v Speaker 1>Do you think the way we human beings </v>

1258
01:08:52.611 --> 01:08:54.570
<v Speaker 1>fall in love and get connected to each </v>
<v Speaker 1>other,</v>

1259
01:08:55.050 --> 01:08:59.070
<v Speaker 1>it's possible to achieve in an ai system</v>
<v Speaker 1>and human level,</v>

1260
01:08:59.100 --> 01:09:00.720
<v Speaker 1>ai intelligent system.</v>
<v Speaker 1>Do you think,</v>

1261
01:09:00.721 --> 01:09:04.860
<v Speaker 1>what would ever see that kind of </v>
<v Speaker 1>connection or a,</v>

1262
01:09:05.000 --> 01:09:10.000
<v Speaker 1>you know,</v>
<v Speaker 1>in all this discussion about solving </v>

1263
01:09:10.000 --> 01:09:10.740
<v Speaker 1>complex goals as this kind of human </v>
<v Speaker 1>social connection,</v>

1264
01:09:10.770 --> 01:09:15.770
<v Speaker 1>do you think that's one of the goals and</v>
<v Speaker 1>the peaks and valleys that were the </v>

1265
01:09:15.770 --> 01:09:17.370
<v Speaker 1>raising sea levels that we'll be able to</v>
<v Speaker 1>achieve?</v>

1266
01:09:17.371 --> 01:09:20.070
<v Speaker 1>or do you think that's something that's </v>
<v Speaker 1>ultimately a,</v>

1267
01:09:20.140 --> 01:09:25.140
<v Speaker 1>or at least in the short term,</v>
<v Speaker 1>relevancy of the goals is not </v>

1268
01:09:25.140 --> 01:09:25.140
<v Speaker 1>achievable?</v>
<v Speaker 1>I think it's all possible</v>

1269
01:09:25.140 --> 01:09:25.650
<v Speaker 2>and um,</v>
<v Speaker 2>in,</v>

1270
01:09:25.800 --> 01:09:27.940
<v Speaker 2>in,</v>
<v Speaker 2>in recent there's a,</v>

1271
01:09:27.950 --> 01:09:30.690
<v Speaker 2>there's a very wide range of justice as </v>
<v Speaker 2>you know,</v>

1272
01:09:30.820 --> 01:09:33.580
<v Speaker 2>among ai researchers when we're going to</v>
<v Speaker 2>get agi.</v>

1273
01:09:35.080 --> 01:09:36.370
<v Speaker 2>Some people,</v>
<v Speaker 2>you know,</v>

1274
01:09:36.490 --> 01:09:41.490
<v Speaker 2>like our friend rodney brooks said it's </v>
<v Speaker 2>going to be a hundred year lease and </v>

1275
01:09:41.490 --> 01:09:46.171
<v Speaker 2>then there are many others.</v>
<v Speaker 2>I think it's going to happen relative </v>

1276
01:09:46.171 --> 01:09:48.420
<v Speaker 2>much sooner and recent polls maybe half </v>
<v Speaker 2>or so or I received your thinking we're </v>

1277
01:09:49.121 --> 01:09:53.050
<v Speaker 2>going to get a gi within decades.</v>
<v Speaker 2>So if that happens,</v>

1278
01:09:53.051 --> 01:09:55.750
<v Speaker 2>of couRse,</v>
<v Speaker 2>I think these things are all possible,</v>

1279
01:09:56.140 --> 01:10:01.140
<v Speaker 2>but in terms of whether it will happen,</v>
<v Speaker 2>I think we shouldn't spend so much time </v>

1280
01:10:01.211 --> 01:10:06.211
<v Speaker 2>asking what do we think will happen in </v>
<v Speaker 2>the future as if we are just some sort </v>

1281
01:10:06.211 --> 01:10:07.540
<v Speaker 2>of pathetic.</v>
<v Speaker 2>You're passive bystanders,</v>

1282
01:10:07.541 --> 01:10:10.060
<v Speaker 2>you know,</v>
<v Speaker 2>waiting for the future to happen to us.</v>

1283
01:10:10.290 --> 01:10:12.430
<v Speaker 2>Hey,</v>
<v Speaker 2>we're the ones creating this future,</v>

1284
01:10:12.460 --> 01:10:17.460
<v Speaker 2>right?</v>
<v Speaker 2>So we should be pRoactive about it and </v>

1285
01:10:17.460 --> 01:10:19.100
<v Speaker 2>ask yourself what sort of future we </v>
<v Speaker 2>would like to have happen.</v>

1286
01:10:19.290 --> 01:10:22.790
<v Speaker 2>I want to make it like that.</v>
<v Speaker 2>What would I prefer it to?</v>

1287
01:10:22.800 --> 01:10:27.800
<v Speaker 2>Some sort of incredibly boring zombie </v>
<v Speaker 2>like future were just all these </v>

1288
01:10:27.800 --> 01:10:30.821
<v Speaker 2>mechanical things happen and there's no </v>
<v Speaker 2>passion or emotion or experience maybe </v>

1289
01:10:30.821 --> 01:10:34.231
<v Speaker 2>even know I would much rather prefer if,</v>
<v Speaker 2>if all the things that we find that we </v>

1290
01:10:35.831 --> 01:10:40.831
<v Speaker 2>value the most about humanity or </v>
<v Speaker 2>subjective experience,</v>

1291
01:10:42.280 --> 01:10:43.660
<v Speaker 2>passion,</v>
<v Speaker 2>inspiration,</v>

1292
01:10:43.720 --> 01:10:44.290
<v Speaker 2>you love,</v>
<v Speaker 2>you know,</v>

1293
01:10:44.470 --> 01:10:44.920
<v Speaker 2>if,</v>
<v Speaker 2>if,</v>

1294
01:10:45.010 --> 01:10:45.620
<v Speaker 2>if,</v>
<v Speaker 2>uh,</v>

1295
01:10:46.210 --> 01:10:50.830
<v Speaker 2>we can create a future where those are,</v>
<v Speaker 2>those things do exist now.</v>

1296
01:10:50.860 --> 01:10:55.510
<v Speaker 2>I think ultimately it's not our universe</v>
<v Speaker 2>giving meaning to us,</v>

1297
01:10:56.050 --> 01:11:01.050
<v Speaker 2>us giving me the universe and if we </v>
<v Speaker 2>Build more advanced and pillages let's,</v>

1298
01:11:02.180 --> 01:11:07.180
<v Speaker 2>let's make sure we're building in such a</v>
<v Speaker 2>way that meetings as part of it,</v>

1299
01:11:09.150 --> 01:11:14.150
<v Speaker 1>a lot of people that are seriously study</v>
<v Speaker 1>this problem and think of it from </v>

1300
01:11:14.150 --> 01:11:14.150
<v Speaker 1>different angles have</v>

1301
01:11:14.190 --> 01:11:19.190
<v Speaker 2>troubling the majority of cases if they </v>
<v Speaker 2>think through that happen are the ones </v>

1302
01:11:19.801 --> 01:11:22.380
<v Speaker 2>that are not beneficial to humanity.</v>
<v Speaker 2>Right.</v>

1303
01:11:22.520 --> 01:11:24.290
<v Speaker 2>And so yeah.</v>
<v Speaker 2>So what,</v>

1304
01:11:24.291 --> 01:11:25.590
<v Speaker 2>what,</v>
<v Speaker 2>what are your thoughts?</v>

1305
01:11:25.591 --> 01:11:26.910
<v Speaker 2>What's an inch?</v>
<v Speaker 2>What's,</v>

1306
01:11:27.090 --> 01:11:29.400
<v Speaker 2>what should people,</v>
<v Speaker 2>you know,</v>

1307
01:11:29.401 --> 01:11:33.200
<v Speaker 2>I really don't like people to be </v>
<v Speaker 2>terrified he should.</v>

1308
01:11:33.201 --> 01:11:38.201
<v Speaker 2>What?</v>
<v Speaker 2>What's a way for people to think about </v>

1309
01:11:38.201 --> 01:11:38.201
<v Speaker 2>it in the way that it's set?</v>
<v Speaker 2>In a way we can solve it.</v>

1310
01:11:38.201 --> 01:11:39.190
<v Speaker 2>We couldn't make it,</v>
<v Speaker 2>but yeah.</v>

1311
01:11:39.630 --> 01:11:42.600
<v Speaker 2>No,</v>
<v Speaker 2>I don't think panicking is gonna help in</v>

1312
01:11:42.601 --> 01:11:47.601
<v Speaker 2>any way.</v>
<v Speaker 2>Not going to increase chances of things </v>

1313
01:11:47.601 --> 01:11:50.211
<v Speaker 2>going well either.</v>
<v Speaker 2>Even if you are in a situation where </v>

1314
01:11:50.211 --> 01:11:52.281
<v Speaker 2>there is a real threat,</v>
<v Speaker 2>does it help if everybody just freaks </v>

1315
01:11:52.281 --> 01:11:52.281
<v Speaker 2>out?</v>
<v Speaker 2>Right?</v>

1316
01:11:52.281 --> 01:11:52.281
<v Speaker 2>No,</v>
<v Speaker 2>of course,</v>

1317
01:11:52.281 --> 01:11:54.360
<v Speaker 2>of course not.</v>
<v Speaker 2>I think,</v>

1318
01:11:54.660 --> 01:11:57.120
<v Speaker 2>yeah,</v>
<v Speaker 2>there are of course ways in which things</v>

1319
01:11:57.121 --> 01:12:00.180
<v Speaker 2>can go horribly wrong.</v>
<v Speaker 2>First of all,</v>

1320
01:12:00.360 --> 01:12:02.950
<v Speaker 2>it's important when we think about this </v>
<v Speaker 2>thing,</v>

1321
01:12:03.690 --> 01:12:08.690
<v Speaker 2>about the problems and risks.</v>
<v Speaker 2>The also remember how huge the upsides </v>

1322
01:12:08.690 --> 01:12:08.690
<v Speaker 2>can be if we get it right,</v>
<v Speaker 2>right?</v>

1323
01:12:08.690 --> 01:12:11.770
<v Speaker 2>Everything,</v>
<v Speaker 2>everything you love about society and to</v>

1324
01:12:11.810 --> 01:12:16.810
<v Speaker 2>the nation as a product of intelligence.</v>
<v Speaker 2>So if we can amplify our intelligence </v>

1325
01:12:16.810 --> 01:12:17.740
<v Speaker 2>with machine intelligence and not any </v>
<v Speaker 2>more,</v>

1326
01:12:17.760 --> 01:12:21.900
<v Speaker 2>lose our loved ones that were told in an</v>
<v Speaker 2>uncurable disease and things like this.</v>

1327
01:12:21.960 --> 01:12:26.220
<v Speaker 2>Of course we should aspire to that,</v>
<v Speaker 2>so that can be a motivator.</v>

1328
01:12:26.221 --> 01:12:29.100
<v Speaker 2>I think reminding ourselves that the </v>
<v Speaker 2>reason we try to solve problems,</v>

1329
01:12:29.101 --> 01:12:30.120
<v Speaker 2>it's not just because</v>

1330
01:12:31.760 --> 01:12:34.580
<v Speaker 2>we're trying to avoid gluten,</v>
<v Speaker 2>but because we're trying to do something</v>

1331
01:12:34.581 --> 01:12:37.460
<v Speaker 2>great,</v>
<v Speaker 2>but then in in terms of the risks,</v>

1332
01:12:37.670 --> 01:12:38.470
<v Speaker 2>I think jim,</v>

1333
01:12:41.200 --> 01:12:46.200
<v Speaker 2>the entry of the important question is </v>
<v Speaker 2>to ask what can we do today that will </v>

1334
01:12:46.200 --> 01:12:49.581
<v Speaker 2>actually helped?</v>
<v Speaker 2>I'll come good and the dismissing the </v>

1335
01:12:49.581 --> 01:12:50.570
<v Speaker 2>risk is not one of them.</v>
<v Speaker 2>You know?</v>

1336
01:12:51.430 --> 01:12:56.430
<v Speaker 2>I find it quite funny often when I'm in </v>
<v Speaker 2>on the discussion panels about these </v>

1337
01:12:56.430 --> 01:13:00.330
<v Speaker 2>things,</v>
<v Speaker 2>how the people who worked for hca,</v>

1338
01:13:00.400 --> 01:13:01.750
<v Speaker 2>for companies,</v>
<v Speaker 2>what we say,</v>

1339
01:13:01.751 --> 01:13:02.680
<v Speaker 2>I always like,</v>
<v Speaker 2>ah,</v>

1340
01:13:02.681 --> 01:13:04.370
<v Speaker 2>nothing to worry about,</v>
<v Speaker 2>nothing to worry about necessarily worry</v>

1341
01:13:04.371 --> 01:13:06.250
<v Speaker 2>about.</v>
<v Speaker 2>And it's always,</v>

1342
01:13:06.251 --> 01:13:11.251
<v Speaker 2>oh,</v>
<v Speaker 2>it's only academics sometimes express </v>

1343
01:13:11.251 --> 01:13:11.251
<v Speaker 2>concerns.</v>
<v Speaker 2>That's not surprising at all.</v>

1344
01:13:11.251 --> 01:13:12.170
<v Speaker 2>If you think about it,</v>
<v Speaker 2>right?</v>

1345
01:13:12.940 --> 01:13:15.280
<v Speaker 2>Upton sinclair quipped,</v>
<v Speaker 2>right?</v>

1346
01:13:15.281 --> 01:13:20.281
<v Speaker 2>That the,</v>
<v Speaker 2>it's hard to make you believe in </v>

1347
01:13:20.281 --> 01:13:20.281
<v Speaker 2>something when his income depends on not</v>
<v Speaker 2>believing in it.</v>

1348
01:13:20.281 --> 01:13:25.220
<v Speaker 2>And frankly we know a lot of these </v>
<v Speaker 2>people in companies that they are just </v>

1349
01:13:25.220 --> 01:13:29.851
<v Speaker 2>as concerned as anyone else.</v>
<v Speaker 2>But if you're the ceo of a company </v>

1350
01:13:29.851 --> 01:13:33.570
<v Speaker 2>that's not something you want to go on </v>
<v Speaker 2>record saying when you have silly </v>

1351
01:13:33.570 --> 01:13:36.061
<v Speaker 2>journalists,</v>
<v Speaker 2>Oregon that put a picture of a </v>

1352
01:13:36.061 --> 01:13:36.170
<v Speaker 2>terminator robot when they quote you.</v>
<v Speaker 2>So,</v>

1353
01:13:36.640 --> 01:13:37.150
<v Speaker 2>so the,</v>
<v Speaker 2>the,</v>

1354
01:13:37.180 --> 01:13:39.920
<v Speaker 2>the issues are real.</v>
<v Speaker 2>And the way I,</v>

1355
01:13:39.970 --> 01:13:41.650
<v Speaker 2>the way I think you might,</v>
<v Speaker 2>what the issue is,</v>

1356
01:13:41.651 --> 01:13:43.280
<v Speaker 2>it is basically,</v>
<v Speaker 2>you know,</v>

1357
01:13:44.510 --> 01:13:49.510
<v Speaker 2>the real choice we have is first of all,</v>
<v Speaker 2>are we going to just dismiss this,</v>

1358
01:13:50.520 --> 01:13:51.660
<v Speaker 2>the risks and say,</v>
<v Speaker 2>well,</v>

1359
01:13:51.661 --> 01:13:56.661
<v Speaker 2>you know,</v>
<v Speaker 2>let's just go ahead and build machines </v>

1360
01:13:56.661 --> 01:13:57.120
<v Speaker 2>that can do everything we can do better </v>
<v Speaker 2>and cheaper,</v>

1361
01:13:57.121 --> 01:14:02.121
<v Speaker 2>you know,</v>
<v Speaker 2>let's just make ourselves obsolete this </v>

1362
01:14:02.121 --> 01:14:02.121
<v Speaker 2>fast as possible.</v>
<v Speaker 2>And what could possibly go wrong?</v>

1363
01:14:02.121 --> 01:14:04.550
<v Speaker 2>That's one attitude.</v>
<v Speaker 2>The opposite attitude.</v>

1364
01:14:04.570 --> 01:14:05.430
<v Speaker 2>I think it's the same,</v>

1365
01:14:06.530 --> 01:14:08.380
<v Speaker 2>here's this incredible potential,</v>
<v Speaker 2>you know,</v>

1366
01:14:08.910 --> 01:14:10.790
<v Speaker 2>let's,</v>
<v Speaker 2>let's think about what kind of.</v>

1367
01:14:12.060 --> 01:14:15.580
<v Speaker 2>We're really,</v>
<v Speaker 2>really excited about what are the shared</v>

1368
01:14:15.581 --> 01:14:20.581
<v Speaker 2>goals that we can really aspire towards.</v>
<v Speaker 2>And then let's think really hard about </v>

1369
01:14:20.581 --> 01:14:22.830
<v Speaker 2>how we can actually get there as to </v>
<v Speaker 2>start with not.</v>

1370
01:14:22.840 --> 01:14:25.520
<v Speaker 2>Don't start thinking about the risks.</v>
<v Speaker 2>Start thinking about the goals.</v>

1371
01:14:25.540 --> 01:14:26.140
<v Speaker 2>Goals.</v>
<v Speaker 2>Yeah.</v>

1372
01:14:26.820 --> 01:14:31.820
<v Speaker 2>And then when you do that,</v>
<v Speaker 2>then you can think about the obstacles </v>

1373
01:14:31.820 --> 01:14:34.750
<v Speaker 2>you want to avoid.</v>
<v Speaker 2>Why they often get students coming in </v>

1374
01:14:34.750 --> 01:14:34.750
<v Speaker 2>right here inTo my office for career </v>
<v Speaker 2>advice.</v>

1375
01:14:34.750 --> 01:14:36.940
<v Speaker 2>I always asked them this question,</v>
<v Speaker 2>where do you want to be in the future?</v>

1376
01:14:36.990 --> 01:14:39.060
<v Speaker 2>Man?</v>
<v Speaker 2>If all she can say as,</v>

1377
01:14:39.070 --> 01:14:40.600
<v Speaker 2>oh,</v>
<v Speaker 2>maybe I'll have cancer,</v>

1378
01:14:40.750 --> 01:14:45.750
<v Speaker 2>maybe I'll run over by obstacles instead</v>
<v Speaker 2>of the goal and he's just going to end </v>

1379
01:14:45.750 --> 01:14:46.850
<v Speaker 2>up a hypochondriac,</v>
<v Speaker 2>paranoid.</v>

1380
01:14:48.010 --> 01:14:50.340
<v Speaker 2>Whereas if she comes in and fire in her </v>
<v Speaker 2>eyes and is like,</v>

1381
01:14:50.350 --> 01:14:55.350
<v Speaker 2>I want to be there and then we can talk </v>
<v Speaker 2>about the obstacles and see how we can </v>

1382
01:14:55.350 --> 01:14:55.360
<v Speaker 2>circumvent them.</v>

1383
01:14:55.840 --> 01:14:57.310
<v Speaker 2>That's,</v>
<v Speaker 2>I think a much,</v>

1384
01:14:57.340 --> 01:15:00.770
<v Speaker 2>much healthier attitude and um,</v>
<v Speaker 2>that's really well put.</v>

1385
01:15:01.010 --> 01:15:02.050
<v Speaker 2>And,</v>
<v Speaker 2>and uh,</v>

1386
01:15:02.260 --> 01:15:07.260
<v Speaker 2>I,</v>
<v Speaker 2>I feel it's very challenging to come up </v>

1387
01:15:07.260 --> 01:15:10.441
<v Speaker 2>with a vision for the future which were,</v>
<v Speaker 2>which were unequivocal excited about it.</v>

1388
01:15:10.720 --> 01:15:12.910
<v Speaker 2>I'm not just talking now in the vague </v>
<v Speaker 2>terms like,</v>

1389
01:15:12.911 --> 01:15:13.910
<v Speaker 2>yeah,</v>
<v Speaker 2>let's cure cancer.</v>

1390
01:15:13.940 --> 01:15:18.940
<v Speaker 2>Fine.</v>
<v Speaker 2>Talking about what kind of society that </v>

1391
01:15:18.940 --> 01:15:22.321
<v Speaker 2>we want to create,</v>
<v Speaker 2>what do we want it to mean to be human </v>

1392
01:15:22.321 --> 01:15:24.130
<v Speaker 2>in the age of ai,</v>
<v Speaker 2>when are in the age of agi.</v>

1393
01:15:25.240 --> 01:15:28.570
<v Speaker 2>So if we can have this conversation,</v>
<v Speaker 2>broad,</v>

1394
01:15:28.571 --> 01:15:33.571
<v Speaker 2>inclusive conversation and gradually </v>
<v Speaker 2>start converging towards some,</v>

1395
01:15:33.740 --> 01:15:38.740
<v Speaker 2>some future that with some direction at </v>
<v Speaker 2>least that we want to steer towards </v>

1396
01:15:38.740 --> 01:15:42.571
<v Speaker 2>right then,</v>
<v Speaker 2>then now will be much more motivated to </v>

1397
01:15:42.571 --> 01:15:44.300
<v Speaker 2>constructively take on the obstacles.</v>
<v Speaker 2>And I think if I,</v>

1398
01:15:44.470 --> 01:15:45.440
<v Speaker 2>uh,</v>
<v Speaker 2>if I had to,</v>

1399
01:15:45.610 --> 01:15:50.610
<v Speaker 2>if you make me,</v>
<v Speaker 2>if I read a wrap this up in a more </v>

1400
01:15:50.610 --> 01:15:50.610
<v Speaker 2>succinct way,</v>
<v Speaker 2>I think,</v>

1401
01:15:51.610 --> 01:15:54.140
<v Speaker 2>I think we can all agree already now </v>
<v Speaker 2>though,</v>

1402
01:15:54.210 --> 01:15:59.210
<v Speaker 2>we should aspire to build agi but </v>
<v Speaker 2>doesn't overpower us,</v>

1403
01:16:02.950 --> 01:16:04.270
<v Speaker 2>but that empowers us</v>

1404
01:16:05.390 --> 01:16:08.660
<v Speaker 1>and think of the many various ways they </v>
<v Speaker 1>can do that.</v>

1405
01:16:08.690 --> 01:16:12.350
<v Speaker 1>Whether that's from a mile side of the </v>
<v Speaker 1>world of autonomous vehicles.</v>

1406
01:16:12.930 --> 01:16:17.930
<v Speaker 1>I personally actually from the camp that</v>
<v Speaker 1>believes this human level intelligence </v>

1407
01:16:17.930 --> 01:16:21.971
<v Speaker 1>is required to,</v>
<v Speaker 1>to achieve something like vehicles that </v>

1408
01:16:21.971 --> 01:16:25.781
<v Speaker 1>would actually be something we would </v>
<v Speaker 1>enjoy using and being part of sets the </v>

1409
01:16:25.781 --> 01:16:30.630
<v Speaker 1>one example and certainly there's a lot </v>
<v Speaker 1>of other types of robots and medicine </v>

1410
01:16:30.630 --> 01:16:31.010
<v Speaker 1>and so on.</v>
<v Speaker 1>Uh,</v>

1411
01:16:31.011 --> 01:16:34.010
<v Speaker 1>so focusing on those and then,</v>
<v Speaker 1>and then coming up with the obstacles,</v>

1412
01:16:34.011 --> 01:16:38.060
<v Speaker 1>coming up with the ways that that can go</v>
<v Speaker 1>wrong and solving those one at a time.</v>

1413
01:16:38.300 --> 01:16:41.180
<v Speaker 2>And just because you can build an </v>
<v Speaker 2>autonomous vehicle,</v>

1414
01:16:41.660 --> 01:16:43.810
<v Speaker 2>even if you could build one that would </v>
<v Speaker 2>drive this final,</v>

1415
01:16:43.811 --> 01:16:46.970
<v Speaker 2>as you know,</v>
<v Speaker 2>maybe there was some things in life that</v>

1416
01:16:46.971 --> 01:16:49.260
<v Speaker 2>we would actually want to do ourselves.</v>
<v Speaker 2>That's right.</v>

1417
01:16:49.550 --> 01:16:53.150
<v Speaker 2>Like for example,</v>
<v Speaker 2>if you think of our society as a whole,</v>

1418
01:16:53.151 --> 01:16:57.690
<v Speaker 2>there are some things that we find very </v>
<v Speaker 2>meaningful to do and uh,</v>

1419
01:16:57.691 --> 01:16:59.650
<v Speaker 2>that doesn't mean you have to stop doing</v>
<v Speaker 2>it.</v>

1420
01:16:59.651 --> 01:17:01.340
<v Speaker 2>I'm just because machines can do them </v>
<v Speaker 2>better,</v>

1421
01:17:01.341 --> 01:17:06.341
<v Speaker 2>you know,</v>
<v Speaker 2>I'm not Gonna stop playing tennis that </v>

1422
01:17:06.341 --> 01:17:06.350
<v Speaker 2>day.</v>
<v Speaker 2>Someone build a tennis robot,</v>

1423
01:17:06.650 --> 01:17:08.040
<v Speaker 2>beat me.</v>
<v Speaker 2>People are still still</v>

1424
01:17:08.040 --> 01:17:09.710
<v Speaker 1>playing chess and even go</v>

1425
01:17:10.160 --> 01:17:10.910
<v Speaker 2>and,</v>
<v Speaker 2>and uh,</v>

1426
01:17:11.330 --> 01:17:14.180
<v Speaker 2>in this,</v>
<v Speaker 2>in the very near term,</v>

1427
01:17:14.181 --> 01:17:18.680
<v Speaker 2>even some people are advocating basic </v>
<v Speaker 2>income replace jobs,</v>

1428
01:17:18.950 --> 01:17:19.600
<v Speaker 2>but if you,</v>
<v Speaker 2>if,</v>

1429
01:17:19.670 --> 01:17:24.670
<v Speaker 2>if the government is going to be willing</v>
<v Speaker 2>to just hand out cash to people for </v>

1430
01:17:24.670 --> 01:17:24.670
<v Speaker 2>doing nothing,</v>
<v Speaker 2>uh,</v>

1431
01:17:24.670 --> 01:17:26.190
<v Speaker 2>ben,</v>
<v Speaker 2>once you also seriously consider whether</v>

1432
01:17:26.191 --> 01:17:30.630
<v Speaker 2>the government will also hire a lot more</v>
<v Speaker 2>teachers and nurses and the kind of jobs</v>

1433
01:17:30.631 --> 01:17:33.840
<v Speaker 2>which people often find great </v>
<v Speaker 2>fulfillment in doing,</v>

1434
01:17:33.841 --> 01:17:38.841
<v Speaker 2>right?</v>
<v Speaker 2>I get very tired of hearing politicians </v>

1435
01:17:38.841 --> 01:17:38.841
<v Speaker 2>saying,</v>
<v Speaker 2>oh,</v>

1436
01:17:38.841 --> 01:17:41.541
<v Speaker 2>we can't afford hiring more teachers,</v>
<v Speaker 2>but we're going to maybe have basic </v>

1437
01:17:41.541 --> 01:17:45.651
<v Speaker 2>income if we can have more,</v>
<v Speaker 2>more serious research and thought into </v>

1438
01:17:45.651 --> 01:17:49.670
<v Speaker 2>what gives meaning to our lives.</v>
<v Speaker 2>And the jobs give so much more than </v>

1439
01:17:49.670 --> 01:17:49.670
<v Speaker 2>income.</v>
<v Speaker 2>Right?</v>

1440
01:17:50.590 --> 01:17:54.610
<v Speaker 2>And then thinking about in the future,</v>
<v Speaker 2>what are the role of the,</v>

1441
01:17:56.290 --> 01:18:01.290
<v Speaker 2>the roles that we want to have people </v>
<v Speaker 2>feeling and powered by machines</v>

1442
01:18:03.110 --> 01:18:05.010
<v Speaker 1>and I think sort of,</v>
<v Speaker 1>um,</v>

1443
01:18:05.060 --> 01:18:08.350
<v Speaker 1>I come from the Russia,</v>
<v Speaker 1>from the soviet union and um,</v>

1444
01:18:08.360 --> 01:18:11.180
<v Speaker 1>I think for a lot of people in the 20th </v>
<v Speaker 1>century going to the moon,</v>

1445
01:18:11.330 --> 01:18:13.910
<v Speaker 1>going into space was in an inspiring </v>
<v Speaker 1>thing.</v>

1446
01:18:14.180 --> 01:18:15.900
<v Speaker 1>I feel like the,</v>
<v Speaker 1>the,</v>

1447
01:18:16.160 --> 01:18:18.140
<v Speaker 1>the,</v>
<v Speaker 1>the universe of the mind.</v>

1448
01:18:18.141 --> 01:18:23.141
<v Speaker 1>So ai understanding,</v>
<v Speaker 1>creating intelligence is that for the </v>

1449
01:18:23.141 --> 01:18:24.860
<v Speaker 1>21st century.</v>
<v Speaker 1>So it's really surprising and I've heard</v>

1450
01:18:24.861 --> 01:18:27.950
<v Speaker 1>you mentioned this,</v>
<v Speaker 1>it's really surprising to me both on the</v>

1451
01:18:27.951 --> 01:18:32.951
<v Speaker 1>research funding side,</v>
<v Speaker 1>that it's not funded as greatly as it </v>

1452
01:18:32.951 --> 01:18:36.011
<v Speaker 1>could be,</v>
<v Speaker 1>but most importantly on the politician </v>

1453
01:18:36.011 --> 01:18:38.951
<v Speaker 1>side,</v>
<v Speaker 1>that it's not part of the public </v>

1454
01:18:38.951 --> 01:18:41.111
<v Speaker 1>discourse except in the killer bots,</v>
<v Speaker 1>terminator kind of view that people are </v>

1455
01:18:41.361 --> 01:18:46.361
<v Speaker 1>not yet,</v>
<v Speaker 1>I think perhaps excited by the possible </v>

1456
01:18:46.361 --> 01:18:48.170
<v Speaker 1>positive future that we can build </v>
<v Speaker 1>together.</v>

1457
01:18:48.171 --> 01:18:48.350
<v Speaker 1>So yeah,</v>

1458
01:18:48.440 --> 01:18:53.440
<v Speaker 2>we should be,</v>
<v Speaker 2>because politicians usually just focus </v>

1459
01:18:53.440 --> 01:18:53.440
<v Speaker 2>on the next election cycle,</v>
<v Speaker 2>right?</v>

1460
01:18:53.440 --> 01:18:58.090
<v Speaker 2>Right.</v>
<v Speaker 2>The single most important thing I feel </v>

1461
01:18:58.090 --> 01:19:01.281
<v Speaker 2>we humans have learned and the entire </v>
<v Speaker 2>history of science is there were the </v>

1462
01:19:01.281 --> 01:19:05.201
<v Speaker 2>masters of underestimation.</v>
<v Speaker 2>We underestimated the size of a,</v>

1463
01:19:06.370 --> 01:19:11.370
<v Speaker 2>our cosmos again and again,</v>
<v Speaker 2>realizing that everything we thought </v>

1464
01:19:11.370 --> 01:19:14.811
<v Speaker 2>existed,</v>
<v Speaker 2>it was just a small part of something </v>

1465
01:19:14.811 --> 01:19:14.811
<v Speaker 2>grander.</v>
<v Speaker 2>I find solar system,</v>

1466
01:19:14.811 --> 01:19:15.040
<v Speaker 2>the galaxy,</v>
<v Speaker 2>you know,</v>

1467
01:19:15.750 --> 01:19:17.400
<v Speaker 2>clusters of galaxies,</v>
<v Speaker 2>universe,</v>

1468
01:19:18.450 --> 01:19:23.450
<v Speaker 2>and we now know that the future has so </v>
<v Speaker 2>much more potential than our ancestors </v>

1469
01:19:25.850 --> 01:19:30.850
<v Speaker 2>could ever have dreamt of this cosmos.</v>
<v Speaker 2>But imagine if all of earth was </v>

1470
01:19:33.751 --> 01:19:37.590
<v Speaker 2>completely devoid of life except for </v>
<v Speaker 2>cambridge,</v>

1471
01:19:37.591 --> 01:19:38.880
<v Speaker 2>Massachusetts.</v>
<v Speaker 2>I would.</v>

1472
01:19:39.570 --> 01:19:44.570
<v Speaker 2>Wouldn't it be kind of lame if all we </v>
<v Speaker 2>ever aspired to was to stay in </v>

1473
01:19:44.570 --> 01:19:48.381
<v Speaker 2>cambridge,</v>
<v Speaker 2>Massachusetts forever and then go </v>

1474
01:19:48.381 --> 01:19:50.991
<v Speaker 2>extinct in one week.</v>
<v Speaker 2>Even though earth was going to continue </v>

1475
01:19:50.991 --> 01:19:51.150
<v Speaker 2>on for longer than that.</v>
<v Speaker 2>That sort of attitude.</v>

1476
01:19:51.151 --> 01:19:56.151
<v Speaker 2>I think we have now on the cosmic scale.</v>
<v Speaker 2>We can fill it.</v>

1477
01:19:56.400 --> 01:19:59.310
<v Speaker 2>Life can flourish on earth,</v>
<v Speaker 2>not for for four years,</v>

1478
01:19:59.311 --> 01:20:04.311
<v Speaker 2>but for billions of years.</v>
<v Speaker 2>I couldn't even tell you about how to </v>

1479
01:20:04.311 --> 01:20:06.861
<v Speaker 2>move it out of harm's way when the sun </v>
<v Speaker 2>gets too hot and and then we have so </v>

1480
01:20:07.261 --> 01:20:08.070
<v Speaker 2>much more</v>

1481
01:20:08.130 --> 01:20:09.900
<v Speaker 1>resources out here,</v>
<v Speaker 1>which</v>

1482
01:20:10.550 --> 01:20:11.240
<v Speaker 2>today,</v>
<v Speaker 2>yeah,</v>

1483
01:20:11.241 --> 01:20:14.960
<v Speaker 2>maybe there are a lot of other planets </v>
<v Speaker 2>with bacteria or childlike life on them,</v>

1484
01:20:14.961 --> 01:20:19.961
<v Speaker 2>but I most of this,</v>
<v Speaker 2>all this opportunity to seems as far as </v>

1485
01:20:20.191 --> 01:20:25.191
<v Speaker 2>we can fail to be largely bed like the </v>
<v Speaker 2>sahara desert and yet we have the </v>

1486
01:20:25.441 --> 01:20:30.441
<v Speaker 2>opportunity but the help life flourish </v>
<v Speaker 2>billions of and so like let's quit </v>

1487
01:20:31.531 --> 01:20:36.531
<v Speaker 2>squabbling of a.</v>
<v Speaker 2>Some little borders should be drawn one,</v>

1488
01:20:37.010 --> 01:20:38.400
<v Speaker 2>one mile to the left or right.</v>

1489
01:20:39.450 --> 01:20:41.070
<v Speaker 1>Look up into the skies.</v>
<v Speaker 1>Did you realize,</v>

1490
01:20:41.071 --> 01:20:41.710
<v Speaker 1>hey,</v>
<v Speaker 1>you know,</v>

1491
01:20:42.300 --> 01:20:47.300
<v Speaker 1>we can do such incredible things and </v>
<v Speaker 1>that's I think why it's really exciting </v>

1492
01:20:47.300 --> 01:20:51.651
<v Speaker 1>that yeah,</v>
<v Speaker 1>you and others are connected with some </v>

1493
01:20:51.651 --> 01:20:55.041
<v Speaker 1>of the working elon musk is doing </v>
<v Speaker 1>because he's literally going out into </v>

1494
01:20:55.041 --> 01:20:58.911
<v Speaker 1>that space,</v>
<v Speaker 1>really exploring our universe and it's </v>

1495
01:20:58.911 --> 01:21:01.491
<v Speaker 1>wonderful.</v>
<v Speaker 1>That is exactly why elon musk is so </v>

1496
01:21:01.491 --> 01:21:05.060
<v Speaker 1>misunderstood.</v>
<v Speaker 1>Misconstrued him as some kind of </v>

1497
01:21:05.060 --> 01:21:08.570
<v Speaker 1>pessimistic doomsayer the region.</v>
<v Speaker 1>He chairs so much about ai safety is </v>

1498
01:21:08.570 --> 01:21:09.000
<v Speaker 1>because he more than</v>

1499
01:21:10.050 --> 01:21:13.290
<v Speaker 2>almost anyone else appreciates these </v>
<v Speaker 2>amazing opportunities.</v>

1500
01:21:13.291 --> 01:21:18.291
<v Speaker 2>It will squander it if we wipe out here </v>
<v Speaker 2>on earth and we're not just going to </v>

1501
01:21:18.291 --> 01:21:22.461
<v Speaker 2>wipe out the next generation,</v>
<v Speaker 2>but all generations and this incredible </v>

1502
01:21:22.740 --> 01:21:27.740
<v Speaker 2>opportunity that's out there and that </v>
<v Speaker 2>would be really be a waste and ai for </v>

1503
01:21:28.381 --> 01:21:31.470
<v Speaker 2>people who think that they'll be better </v>
<v Speaker 2>to do without technology.</v>

1504
01:21:32.520 --> 01:21:33.570
<v Speaker 2>Let me just mention that</v>

1505
01:21:34.660 --> 01:21:39.660
<v Speaker 2>if we don't improve our technology,</v>
<v Speaker 2>the question isn't whether humanity is </v>

1506
01:21:39.660 --> 01:21:42.810
<v Speaker 2>going to go extinct,</v>
<v Speaker 2>questioning whether we're going to get </v>

1507
01:21:42.810 --> 01:21:45.690
<v Speaker 2>taken out by the next big asteroid or </v>
<v Speaker 2>the next super volcano or or something </v>

1508
01:21:45.690 --> 01:21:48.940
<v Speaker 2>else dumb that we could easily prevent </v>
<v Speaker 2>with more tech.</v>

1509
01:21:48.941 --> 01:21:53.941
<v Speaker 2>Right,</v>
<v Speaker 2>and if we want life to flourish </v>

1510
01:21:53.941 --> 01:21:54.610
<v Speaker 2>throughout the cosmos,</v>
<v Speaker 2>ai is the key to it.</v>

1511
01:21:56.140 --> 01:21:58.930
<v Speaker 2>As I mentioned in a lot of detail in my </v>
<v Speaker 2>book right there,</v>

1512
01:21:59.860 --> 01:22:04.860
<v Speaker 2>even many of the most inspired scifi </v>
<v Speaker 2>writers I feel have totally </v>

1513
01:22:05.861 --> 01:22:09.850
<v Speaker 2>underestimated the opportunities for </v>
<v Speaker 2>space travel,</v>

1514
01:22:09.851 --> 01:22:14.851
<v Speaker 2>especially if the other galaxies,</v>
<v Speaker 2>because they weren't thinking about the </v>

1515
01:22:14.851 --> 01:22:16.720
<v Speaker 2>possibility of agi,</v>
<v Speaker 2>which just makes it so much easier.</v>

1516
01:22:17.480 --> 01:22:18.360
<v Speaker 1>Right?</v>
<v Speaker 1>Yeah.</v>

1517
01:22:18.390 --> 01:22:19.950
<v Speaker 1>So that,</v>
<v Speaker 1>that goes to your,</v>

1518
01:22:20.330 --> 01:22:21.180
<v Speaker 1>uh,</v>
<v Speaker 1>uh,</v>

1519
01:22:21.181 --> 01:22:25.620
<v Speaker 1>view of agi that enables our progress,</v>
<v Speaker 1>said enables a better life.</v>

1520
01:22:25.740 --> 01:22:30.740
<v Speaker 1>So that's a beautiful,</v>
<v Speaker 1>that's a beautiful way to put it and </v>

1521
01:22:30.740 --> 01:22:30.740
<v Speaker 1>it's something to strive for.</v>
<v Speaker 1>So,</v>

1522
01:22:30.740 --> 01:22:31.440
<v Speaker 1>max,</v>
<v Speaker 1>Thank you so much.</v>

1523
01:22:31.441 --> 01:22:33.210
<v Speaker 1>Thank you for your time today has been </v>
<v Speaker 1>awesome.</v>

1524
01:22:33.540 --> 01:22:36.020
<v Speaker 1>Thank you so much.</v>
<v Speaker 1>Yes.</v>

