WEBVTT

1
00:00:00.090 --> 00:00:04.410
The following is a conversation with Elon Musk.
He's a CEO of Tesla,

2
00:00:04.440 --> 00:00:08.850
SpaceX Neuralink and a co founder of several other companies.

3
00:00:09.300 --> 00:00:12.690
This conversation is part of the artificial intelligence podcasts.

4
00:00:13.290 --> 00:00:18.290
The series includes leading researchers in academia and industry including CEOs

5
00:00:18.631 --> 00:00:23.250
and Ctos of automotive,
robotics,
AI at technology companies.

6
00:00:24.150 --> 00:00:28.380
This conversation happened after the release of the paper from our group at MIT

7
00:00:28.650 --> 00:00:32.280
on driver functional vigilance during use of Tesla's autopilot.

8
00:00:32.880 --> 00:00:36.540
The Tesla team reached out to me offering a podcast conversation with mister

9
00:00:36.541 --> 00:00:37.374
Musk.

10
00:00:37.530 --> 00:00:42.180
I accepted with full control of questions I could ask and the choice of what is

11
00:00:42.181 --> 00:00:46.320
released publicly.
I ended up editing out nothing of substance.

12
00:00:46.890 --> 00:00:51.210
I've never spoken with Ilan before this conversation publicly or privately,

13
00:00:51.780 --> 00:00:55.200
neither he nor his companies have any influence on my opinion,

14
00:00:55.580 --> 00:00:59.950
nor on the rigor and integrity of the scientific method that I practice them a

15
00:00:59.951 --> 00:01:01.110
position and MIT.

16
00:01:01.800 --> 00:01:06.350
Tessa has never financially supported my research and I've never owned a Tesla

17
00:01:06.360 --> 00:01:09.300
vehicle.
I've never owned Tesla stock.

18
00:01:10.140 --> 00:01:13.890
This podcast is not a scientific paper.
It is a conversation.

19
00:01:14.340 --> 00:01:18.090
I respect Ilan as I do all other leaders and engineers have spoken with.

20
00:01:18.660 --> 00:01:21.060
We agree on some things and this is a grant.
Others,

21
00:01:21.420 --> 00:01:25.440
my goal is always with these conversations is to understand the way the guests

22
00:01:25.441 --> 00:01:26.274
sees the world.

23
00:01:26.910 --> 00:01:30.750
One particular point of disagreement in this conversation was the extent to

24
00:01:30.751 --> 00:01:35.751
which camera based driver monitoring will improve outcomes and for how long it

25
00:01:36.241 --> 00:01:38.940
would remain relevant for AI assisted driving.

26
00:01:39.930 --> 00:01:44.130
As someone who works on and is fascinated by human centered artificial

27
00:01:44.131 --> 00:01:44.964
intelligence,

28
00:01:45.150 --> 00:01:49.650
I believe that if implemented an integrated effectively camera based driver

29
00:01:49.651 --> 00:01:54.651
monitoring is likely to be of benefit in both the short term and the longterm.

30
00:01:55.620 --> 00:01:56.550
In contrast,

31
00:01:56.880 --> 00:02:01.880
Elan and Tesla's focus is on the improvement of autopilot such that it's

32
00:02:02.041 --> 00:02:07.041
statistical safety benefits override any concern of human behavior and

33
00:02:07.500 --> 00:02:11.490
psychology.
Elon and I may not agree on everything,

34
00:02:12.000 --> 00:02:15.840
but I deeply respect the engineering and innovation behind the efforts that he

35
00:02:15.841 --> 00:02:16.674
leads.

36
00:02:16.830 --> 00:02:21.830
My goal here is to catalyze a rigorous nuanced and objected discussion in

37
00:02:21.931 --> 00:02:25.620
industry and academia on AI assisted driving,

38
00:02:26.250 --> 00:02:30.030
one that ultimately makes for safer and better world.

39
00:02:30.870 --> 00:02:34.440
And now here's my conversation with Elon Musk.

40
00:02:35.610 --> 00:02:39.930
What was the vision,
the dream of autopilot when uh,
in the beginning,

41
00:02:40.020 --> 00:02:44.490
the big picture system level when it was first conceived and started being

42
00:02:44.491 --> 00:02:48.390
installed in 2014 and the hardware and the cars.
What was the vision?

43
00:02:48.420 --> 00:02:51.470
The dream I would characterize as vision or dream it's,

44
00:02:51.480 --> 00:02:56.480
and we've got there obviously two massive revolutions in,

45
00:02:57.800 --> 00:02:59.500
in the automobile industry.

46
00:03:00.100 --> 00:03:04.660
<v 1>One is the transition to electric electrification.
Um,</v>

47
00:03:04.690 --> 00:03:08.920
and then the other is autonomy and yeah,

48
00:03:09.740 --> 00:03:13.480
it became obvious to me that in the future any,

49
00:03:13.481 --> 00:03:18.260
any car that does not have autonomy,
I would be about as useful as a host.

50
00:03:19.160 --> 00:03:20.810
Which is not to say that there's no use,

51
00:03:20.811 --> 00:03:24.770
it's just rare and somewhat it isn't Craddick if somebody has a horse at this

52
00:03:24.771 --> 00:03:27.890
point,
it's just obvious that cars will drive themselves completely.

53
00:03:28.070 --> 00:03:32.210
It's just a question of time.
And if we did not

54
00:03:33.880 --> 00:03:38.880
participate in the autonomy revolution than our costs would not be useful to

55
00:03:40.091 --> 00:03:42.910
people relative to cars that are autonomous,

56
00:03:43.720 --> 00:03:48.720
not autonomous car is arguably worth five to 10 times more than I call,

57
00:03:51.740 --> 00:03:56.230
which is not autonomous in a longterm trends.
What you mean by long term.

58
00:03:56.231 --> 00:04:00.370
But let's say at least for the next five years,
path,
10 years.

59
00:04:01.450 --> 00:04:05.320
<v 0>So there a lot of very interesting design choices with autopilot early on.</v>

60
00:04:05.740 --> 00:04:10.740
First is showing on the instrument cluster or in the model three and the center

61
00:04:11.651 --> 00:04:14.800
stack display what the combined sends the sweet seas.

62
00:04:15.760 --> 00:04:18.940
What was the thinking behind that choice?
Was there a debate?

63
00:04:18.970 --> 00:04:20.020
What was the process?

64
00:04:20.520 --> 00:04:25.520
<v 1>The whole point of the display is to provide a health check on the,</v>

65
00:04:26.660 --> 00:04:29.600
the vehicles perception of reality.
So the vehicles,
uh,

66
00:04:29.610 --> 00:04:32.250
taking in information for a bunch of sensors,
primarily cameras,

67
00:04:32.251 --> 00:04:35.820
but also radar ultrasonics,
a gps and so forth.

68
00:04:37.260 --> 00:04:38.320
And then,
uh,

69
00:04:38.820 --> 00:04:43.380
that that information is then rendered into vector space.

70
00:04:43.650 --> 00:04:46.400
Uh,
and that,
you know,
with a bunch of objects with PR,

71
00:04:46.440 --> 00:04:51.370
with properties like lane lines and traffic lights and other cars.
Um,

72
00:04:51.450 --> 00:04:56.450
and then in vector space that is rerender it onto a display so you can confirm

73
00:04:57.630 --> 00:05:01.650
whether the car knows what's going on or not by looking out the window.

74
00:05:02.700 --> 00:05:03.120
Right.

75
00:05:03.120 --> 00:05:07.650
<v 0>I think that's an extremely powerful thing for people to get an understanding.</v>

76
00:05:07.950 --> 00:05:11.010
So it become one with the system and understanding what the system is capable
of.

77
00:05:11.730 --> 00:05:16.500
Now have you considered showing more?
So if we look at the computer vision,

78
00:05:17.790 --> 00:05:20.460
you know like road segmentation,
lane detection,
vehicle detection,

79
00:05:20.461 --> 00:05:25.200
object detection,
underlying the system there is at the edges some uncertainty.

80
00:05:25.710 --> 00:05:30.340
Have you considered revealing the parts that the,

81
00:05:30.350 --> 00:05:31.860
the uncertainty in the system,

82
00:05:31.861 --> 00:05:36.510
the set apart movies associated with with say image recognition or something

83
00:05:36.530 --> 00:05:40.920
that code right now it shows like the vehicles in the vicinity of very clean,

84
00:05:40.921 --> 00:05:44.880
crisp image and people do confirm that there's a car in front of me and the

85
00:05:44.881 --> 00:05:46.350
system sees there's a car in front of me.

86
00:05:46.650 --> 00:05:51.540
But to help people build an intuition of what computer vision is by showing some

87
00:05:51.541 --> 00:05:52.374
of the uncertainty,

88
00:05:53.070 --> 00:05:55.590
<v 1>well,
I think I'm in my car.
I always look,</v>

89
00:05:55.680 --> 00:06:00.320
look at this sort of the debug view and it's theirs to debug views.
Uh,

90
00:06:00.321 --> 00:06:04.100
W one is augmented vision,
uh,
where,

91
00:06:04.160 --> 00:06:08.960
which I'm sure you've seen where it basically we draw boxes and labels around

92
00:06:08.990 --> 00:06:13.990
objects that are recognized and then there's what we call the visualizer,

93
00:06:15.320 --> 00:06:18.820
which is basically a vector space representation.
Summing up,
uh,

94
00:06:18.970 --> 00:06:23.000
the input from all sensors that that doesn't,

95
00:06:23.030 --> 00:06:26.990
does not show any pictures,
but it shows,
uh,
all of the,

96
00:06:28.060 --> 00:06:33.020
it's basically shows that cause view of,
of,
of the world in vector space.
Um,

97
00:06:33.500 --> 00:06:35.550
but I think this is very difficult for people to know.

98
00:06:35.630 --> 00:06:38.450
Normal people to understand.
They were not know what the heck they're looking at.

99
00:06:39.190 --> 00:06:40.350
He said it's almost an Asian.

100
00:06:40.380 --> 00:06:45.380
My challenge to the current things that are being displayed is optimized for the

101
00:06:45.891 --> 00:06:48.630
general public understanding of what the system is capable of.

102
00:06:48.770 --> 00:06:51.680
It's like if have no idea what,
how computer vision works or anything,

103
00:06:51.710 --> 00:06:54.650
you can still look at the screen and see if the car knows what's going on.

104
00:06:55.820 --> 00:06:59.350
And then if you're,
you know,
if you're a development engineer or if you're,

105
00:06:59.351 --> 00:07:03.350
you know,
if you're,
if you have the development bold like I do,
then you can see,

106
00:07:03.410 --> 00:07:05.660
uh,
you know,
all the debugging information.

107
00:07:06.020 --> 00:07:10.370
But those were just be like total diverse to most people.

108
00:07:11.270 --> 00:07:15.620
What's your view on how to best this effort?
So there's three,

109
00:07:15.621 --> 00:07:18.860
I would say technical aspects of autopilot that I really important.

110
00:07:19.090 --> 00:07:21.980
It's just the underlying algorithms like the neural network architecture.

111
00:07:22.310 --> 00:07:24.530
There's the data so that the strain on,

112
00:07:24.531 --> 00:07:28.430
and then there's the hardware development and maybe others.
But so look,

113
00:07:28.431 --> 00:07:32.540
algorithm data,
hardware,
you don't,

114
00:07:32.570 --> 00:07:35.000
you only have so much money.
I only have so much time.

115
00:07:35.240 --> 00:07:38.390
What do you think is the most important thing to,
to uh,

116
00:07:38.570 --> 00:07:43.430
allocate resources to do you see it as pretty evenly distributed between those

117
00:07:43.431 --> 00:07:44.264
three?

118
00:07:44.540 --> 00:07:48.620
We automatically get a fast amounts of data because all of our cars have

119
00:07:51.630 --> 00:07:56.630
eight external facing cameras and radar and usually 12 ultrasonic sensors.

120
00:07:58.410 --> 00:08:02.390
Uh,
GPS obviously.
Um,
and I am you.

121
00:08:03.980 --> 00:08:08.450
And so we basically have a fleet that has,

122
00:08:09.340 --> 00:08:13.640
uh,
and we've got about 400,000 cars on the road that have that level of data.

123
00:08:13.820 --> 00:08:17.270
Actually,
I think you keep close track of it actually.
Yes.
Yeah.
So we're,

124
00:08:17.800 --> 00:08:21.710
we're approaching half a million cars on the road that have the full sensor

125
00:08:21.711 --> 00:08:25.230
suite.
Yeah.
Though,
so this is,
I'm,

126
00:08:25.420 --> 00:08:29.150
I'm not sure how many other cars on the road have this central suite,

127
00:08:29.390 --> 00:08:31.490
but I'd be surprised if it's more than 5,000,

128
00:08:32.300 --> 00:08:35.000
which means that we have 99% of all the data.

129
00:08:36.110 --> 00:08:39.860
So there's this huge inflow of data,
absolutely massive inflow of data.

130
00:08:40.700 --> 00:08:44.420
And then we,
it's,
it's taken about three years,

131
00:08:44.421 --> 00:08:47.000
but now we're finally developed off full self driving computer,

132
00:08:47.660 --> 00:08:50.480
which can process

133
00:08:52.480 --> 00:08:55.810
an order of magnitude as much as the nvidia system that we currently have in
the,

134
00:08:55.811 --> 00:08:58.410
in the cars.
And it's really just a,
to use it,

135
00:08:58.411 --> 00:09:02.490
you unplug the Nvidia computer and plug that tells the computer in and that's
it.

136
00:09:03.960 --> 00:09:06.360
And it's,
it's uh,
in fact we're not even,

137
00:09:06.750 --> 00:09:10.110
we still exploring the boundaries of capabilities.
Uh,

138
00:09:10.170 --> 00:09:13.230
we were able to run the cameras at full frame rate,
full resolution,
uh,

139
00:09:13.231 --> 00:09:14.610
not even crop the images.

140
00:09:15.450 --> 00:09:19.770
And it's still got headroom even on one of the systems,

141
00:09:19.980 --> 00:09:23.130
the harder the full self driving computer is really two computers,

142
00:09:23.490 --> 00:09:26.010
two systems on a chip that a fully redundant.

143
00:09:26.100 --> 00:09:29.070
So you could put a bolt through basically any part of that system.

144
00:09:29.160 --> 00:09:29.993
And it still works.

145
00:09:30.240 --> 00:09:34.870
There were done and see are they perfect copies of each other or also it's

146
00:09:35.040 --> 00:09:38.610
purely for a diamond C as opposed to an argue machine kind of architecture where

147
00:09:38.611 --> 00:09:41.610
they're both making decisions.
This is purely for redundancy.

148
00:09:41.910 --> 00:09:45.120
I think even more like it's,
if you ever say a twin engine aircraft,

149
00:09:45.600 --> 00:09:46.433
commercial aircraft,

150
00:09:47.520 --> 00:09:51.630
this system will operate best if both systems are operating,

151
00:09:52.890 --> 00:09:57.180
but it's,
it's capable of operating safely on one.
So,

152
00:09:57.760 --> 00:10:02.120
but as it is right now,
we can just run,
we're,
we haven't even hit the,
the,
the,

153
00:10:02.140 --> 00:10:04.890
at the edge of performance.
So

154
00:10:06.930 --> 00:10:11.930
there's no need to actually distribute to functionality across both soc.

155
00:10:13.510 --> 00:10:17.070
We can actually just run a full duplicate on each one.

156
00:10:17.200 --> 00:10:20.690
Do you haven't really explored or hit the limit of the soon?

157
00:10:20.691 --> 00:10:21.800
Not yet at the limit.
No.

158
00:10:22.500 --> 00:10:27.060
So the magic of deep learning is that it gets better with data.

159
00:10:27.190 --> 00:10:30.540
And he said there's a huge inflow of data.
But yeah,

160
00:10:30.570 --> 00:10:35.310
the thing about driving the really valuable data to learn from as the edge

161
00:10:35.311 --> 00:10:39.270
cases.
So how do you,
I mean,
I've,

162
00:10:39.280 --> 00:10:42.960
I've heard you talk somewhere about,
uh,

163
00:10:42.990 --> 00:10:46.620
autopilot disengagements being an important moment of time to use.

164
00:10:46.980 --> 00:10:51.980
Is there other edge cases or perhaps can you speak to those edge cases?

165
00:10:52.590 --> 00:10:56.160
What aspects of that might be valuable or if you have other ideas,

166
00:10:56.161 --> 00:11:00.450
how to discover more and more and more agile cases in driving?
Well,

167
00:11:00.451 --> 00:11:02.520
there's a lot of things that learnt though.

168
00:11:02.521 --> 00:11:07.521
Suddenly a edge cases where I say somebody is on autopilot and they take over

169
00:11:08.460 --> 00:11:09.920
and then,
okay,
that,
that,

170
00:11:10.030 --> 00:11:13.680
that that's a trigger that goes up to a system that says,
okay,
did the trip,

171
00:11:13.770 --> 00:11:18.360
the takeover for convenience or do they take over because the autopilot wasn't

172
00:11:18.361 --> 00:11:21.870
working properly.
There's also like,
let's say what we're trying to figure out,

173
00:11:21.871 --> 00:11:26.220
what is the optimal spline for traversing at an intersection?

174
00:11:27.410 --> 00:11:32.300
Um,
then then the ones where there are no interventions and we,
uh,

175
00:11:32.520 --> 00:11:36.000
are the right ones.
So you then say,
okay,
when it looks like this,

176
00:11:36.390 --> 00:11:39.030
do the following and the,
and the,
and,

177
00:11:39.031 --> 00:11:42.030
and then you get the optimal spine for a complex,
uh,

178
00:11:42.060 --> 00:11:44.620
now getting a complex intersection.

179
00:11:45.600 --> 00:11:49.170
So that's for this or this kind of the common case,

180
00:11:49.470 --> 00:11:53.530
you're trying to capture a huge amount of samples of a particular intersection.

181
00:11:53.530 --> 00:11:58.530
How when things went right and then there's the edge case where as you said,

182
00:11:59.261 --> 00:12:02.820
not for convenience but something didn't go exactly right.
Somebody took over,

183
00:12:02.930 --> 00:12:07.420
somebody sorted manual control from autopilot and it really like the way to look

184
00:12:07.421 --> 00:12:09.100
at this as view all input as error.

185
00:12:09.910 --> 00:12:13.660
If the user had to do input it though something,
all input is error.

186
00:12:13.930 --> 00:12:17.750
That's a powerful line to think of it that way because they may very well be
air,

187
00:12:17.751 --> 00:12:21.170
but if you want to exit the highway or if you want to uh,

188
00:12:21.220 --> 00:12:25.360
isn't navigation decision that all autopilots not currently designed to do,

189
00:12:25.390 --> 00:12:27.220
then the driver takes over.

190
00:12:27.550 --> 00:12:30.160
How do you know the difference is going to change with navigate an autopilot

191
00:12:30.161 --> 00:12:33.580
which were just released and without stoking phone.

192
00:12:33.820 --> 00:12:35.950
So the navigation like lane lane change based,

193
00:12:36.540 --> 00:12:40.990
it like assuming control in order to change their lane change or exit the

194
00:12:40.991 --> 00:12:43.390
freeway or,
or doing highway interchange.

195
00:12:44.710 --> 00:12:48.460
The vast majority of that will go away with the release that just went out.

196
00:12:48.800 --> 00:12:50.380
He has that.

197
00:12:50.440 --> 00:12:55.090
I don't think people quite understand how big of a step that is.
Yeah,
they don't.

198
00:12:55.870 --> 00:12:57.700
So if you drive the car then you do,

199
00:12:58.240 --> 00:13:00.760
so you still have to keep your hands on the steering wheel.
Currently.

200
00:13:00.790 --> 00:13:05.770
When it does the automatic lane change,
what are,
so there's these,

201
00:13:05.860 --> 00:13:10.630
these big leaps through the development of autopilot through its history and

202
00:13:11.290 --> 00:13:14.800
what stands out to you as the big leaps?
I would say this one,

203
00:13:15.040 --> 00:13:18.010
navigate and autopilot without,
uh,

204
00:13:18.040 --> 00:13:22.010
confirm what I'll having to confirm is a huge leap.
It was a huge leap,

205
00:13:22.740 --> 00:13:25.270
automatically overtakes low cars.
So it's,

206
00:13:25.271 --> 00:13:30.130
it's both navigation and seeking the fastest lane.

207
00:13:31.030 --> 00:13:35.330
So it'll,
it'll,
it'll to overtake slow cars,
um,

208
00:13:35.470 --> 00:13:39.020
and exit the freeway and take highway interchanges and,

209
00:13:40.540 --> 00:13:41.840
and then,
uh,

210
00:13:42.040 --> 00:13:46.180
we have a traffic light traffic lights to recognition,

211
00:13:47.170 --> 00:13:49.780
which introduced initially as a,
as a warning.

212
00:13:50.230 --> 00:13:53.020
I mean on the development version that I'm driving the car fully,

213
00:13:53.200 --> 00:13:55.990
fully stops and goes at traffic lights.

214
00:13:56.890 --> 00:13:58.480
So those are the steps.
Right.

215
00:13:58.510 --> 00:14:02.230
You've just mentioned somethings that an inkling of a step towards full
autonomy.

216
00:14:03.280 --> 00:14:08.280
What would you say are the biggest technological roadblocks to full cell

217
00:14:08.771 --> 00:14:10.750
driving?
Actually,
I don't think we're,

218
00:14:10.751 --> 00:14:14.940
I think we're just the full self driving computer that we're just the Tosa,

219
00:14:15.370 --> 00:14:19.480
oracle,
the FSD computer,
uh,
that,
that's now in production.

220
00:14:21.620 --> 00:14:23.410
Uh,
so if your order,
uh,

221
00:14:23.980 --> 00:14:28.980
any model SRX or any model three that has the full self driving package,

222
00:14:29.561 --> 00:14:32.740
you will get the FSD computer that that was,

223
00:14:32.830 --> 00:14:36.910
that's important to have enough base computation.
Uh,

224
00:14:37.220 --> 00:14:40.650
then refining the neural net and the control software.
Uh,

225
00:14:41.240 --> 00:14:43.930
but all of that can just be providers know their update.

226
00:14:45.160 --> 00:14:50.020
The thing that's really profound and where I'll be emphasizing at the sort of

227
00:14:50.021 --> 00:14:53.960
what the investor day that we're having focused on autonomy is that the car is

228
00:14:53.961 --> 00:14:56.630
currently being produced with the hardware.

229
00:14:56.631 --> 00:15:01.631
Cardi being produced is capable of full self driving like capable is an

230
00:15:01.970 --> 00:15:05.480
interesting word because I'm like the hardware is,

231
00:15:05.880 --> 00:15:07.400
and as we were the software,

232
00:15:09.390 --> 00:15:11.970
the capabilities will increase dramatically.
Um,

233
00:15:12.000 --> 00:15:14.810
and then the reliability will increase dramatically and then it will receive

234
00:15:14.811 --> 00:15:15.920
regulatory approval.

235
00:15:16.130 --> 00:15:19.380
So essentially buying a car today is an investment in the future.
You want,

236
00:15:19.390 --> 00:15:21.400
you're essentially buying a car.

237
00:15:21.460 --> 00:15:26.460
You're buying th th th th I think the most profound thing is that if you buy a

238
00:15:26.961 --> 00:15:30.410
Tesla today,
I believe you are buying an appreciating asset,

239
00:15:30.500 --> 00:15:35.330
not a depreciating asset.
So that's a really important statement there.

240
00:15:35.331 --> 00:15:39.830
Because if hardware is capable enough,
that's the hard thing to upgrade.
Yes,

241
00:15:39.831 --> 00:15:44.180
usually exact.
So then the rest is a software problem.
Yes,

242
00:15:44.300 --> 00:15:46.760
I've software has no marginal costs really.

243
00:15:47.930 --> 00:15:51.440
But what's your intuition on the software side?

244
00:15:51.470 --> 00:15:56.470
How hard are the remaining steps to get it to where,

245
00:15:58.400 --> 00:16:02.570
uh,
you know,
uh,
the experience,
uh,

246
00:16:02.600 --> 00:16:07.220
not just the safety,
but the full experience is something that people would,

247
00:16:07.221 --> 00:16:11.770
uh,
enjoy.
Well,
I think people enjoy it very much so on,
on,

248
00:16:11.840 --> 00:16:16.370
on the highway.
So it's a total game changer for quality of life,

249
00:16:16.790 --> 00:16:21.260
for using,
you know,
Tesla autopilot on the highways.
Uh,

250
00:16:21.330 --> 00:16:24.380
so it's really just extending that functionality to Sydney streets,

251
00:16:25.440 --> 00:16:29.130
adding in the traffic,
light traffic,
light recognition,
uh,

252
00:16:29.240 --> 00:16:32.950
navigating complex intersections and um,
and,

253
00:16:33.320 --> 00:16:38.240
and then being able to navigate a complicated po parking lots so the car can,

254
00:16:38.560 --> 00:16:42.210
uh,
exit a parking space and come and find you,
even if it's in a,

255
00:16:42.620 --> 00:16:47.530
a complete maze of a parking lot.
And,
uh,

256
00:16:47.540 --> 00:16:49.040
and,
and then if,
and then you can just,

257
00:16:49.041 --> 00:16:53.180
it just drop you off and find a parking spot by itself.
Yeah.

258
00:16:53.181 --> 00:16:56.870
In terms of enjoyability and something that people would,
uh,

259
00:16:57.340 --> 00:17:01.310
would actually find a lot of use from the parking lot is a,
is a really,
you know,

260
00:17:01.400 --> 00:17:04.730
it's,
it's rich of annoyance when you have to do it manually.

261
00:17:04.731 --> 00:17:07.700
So there's a lot of benefit to be gained from automation there.

262
00:17:08.660 --> 00:17:12.740
So let me start injecting the human into this discussion a little bit.
Uh,

263
00:17:12.741 --> 00:17:14.720
so let's talk,
talk about full autonomy.

264
00:17:15.620 --> 00:17:18.260
If you look at the current level four vehicles being tests on row,

265
00:17:18.261 --> 00:17:19.670
like Waymo and so on,

266
00:17:20.720 --> 00:17:23.330
they're only technically autonomous.

267
00:17:23.360 --> 00:17:28.360
They're really level two systems with just the different design philosophy

268
00:17:28.881 --> 00:17:32.300
because there's always a safety driver in almost all cases in their monitoring

269
00:17:32.310 --> 00:17:34.250
system.
Right.
Do you see

270
00:17:35.810 --> 00:17:40.430
Teslas full self driving as still for a time to come,

271
00:17:41.810 --> 00:17:44.450
requiring supervision of the human being.

272
00:17:44.810 --> 00:17:48.680
So it's capabilities and Paul phone off to drive but nevertheless requires a

273
00:17:48.681 --> 00:17:50.580
human to still be supervising.

274
00:17:50.580 --> 00:17:53.160
<v 0>Just like a safety driver is in a</v>

275
00:17:54.830 --> 00:17:56.450
other fully autonomous vehicles.

276
00:17:57.400 --> 00:17:57.910
<v 1>I think it,</v>

277
00:17:57.910 --> 00:18:02.910
it will require detecting hands on wheel for at least a six months or something

278
00:18:06.321 --> 00:18:10.740
like that from here.
It really is a question of like

279
00:18:12.240 --> 00:18:15.840
from a regulatory standpoint,
uh,
what,

280
00:18:16.080 --> 00:18:21.080
how much safer than a person does autopilot needs to be for it took to be okay

281
00:18:21.421 --> 00:18:25.050
to not monitor the car,
you know,
and,

282
00:18:25.060 --> 00:18:28.410
and this was a debate that one can have it and then if you,

283
00:18:28.430 --> 00:18:32.700
but you need a large samples,
large amount of data,
um,

284
00:18:32.770 --> 00:18:36.400
so that you can prove with high confidence,
statistically speaking,

285
00:18:36.610 --> 00:18:40.410
that the car is dramatically safer than a person.
Um,

286
00:18:40.630 --> 00:18:45.550
and that adding in the person monitoring does not materially affect the safety.

287
00:18:45.940 --> 00:18:49.300
So it might,
it need to be like two or 300% safe a person.

288
00:18:50.140 --> 00:18:52.150
And how do you prove that incidents per mile

289
00:18:52.420 --> 00:18:56.260
<v 0>incidence per mile.
So crashes and fatalities,</v>

290
00:18:57.310 --> 00:18:58.680
<v 1>fridge fatality would be a factor,</v>

291
00:18:58.681 --> 00:19:01.320
but there's there they're just not enough fatalities to be statistically

292
00:19:01.321 --> 00:19:03.990
significant at scale.

293
00:19:04.020 --> 00:19:07.350
But there are enough crashes,
you know,

294
00:19:07.380 --> 00:19:09.450
the former crashes and fatalities.

295
00:19:10.980 --> 00:19:13.960
So you can assess where is the probability of a,

296
00:19:13.970 --> 00:19:18.970
of a crash that then there's another step which properly of injury and

297
00:19:19.801 --> 00:19:22.110
probability of Ponant injury,
their power,

298
00:19:22.111 --> 00:19:27.111
probability of death and all of those need to be a much better than a person,

299
00:19:27.400 --> 00:19:31.800
uh,
by at least
paps 200%.

300
00:19:33.030 --> 00:19:34.240
<v 0>And you think there is a,</v>

301
00:19:35.230 --> 00:19:39.520
the ability to have a healthy discourse with the regulatory bodies on this
topic.

302
00:19:40.090 --> 00:19:43.020
<v 1>I mean,
there's no question that um,
uh,</v>

303
00:19:43.500 --> 00:19:47.800
regulators paid a disproportionate amount of attention to that which generates

304
00:19:47.801 --> 00:19:51.700
press.
This is just an objective fact and tells the,

305
00:19:51.701 --> 00:19:55.720
generates a lot of press.
So the,
you know,

306
00:19:56.140 --> 00:20:01.060
in the United States,
this I think almost 40,000 automotive deaths per year,

307
00:20:02.050 --> 00:20:04.420
but if there are four and Tesla,

308
00:20:04.450 --> 00:20:07.900
they will probably receive a thousand times more press than anyone else.

309
00:20:08.850 --> 00:20:11.460
<v 0>So the psychology of that is actually fascinating.</v>

310
00:20:11.490 --> 00:20:13.410
I don't think we'll have enough time to talk about that,

311
00:20:13.411 --> 00:20:16.560
but I have to talk to you about the human side of things.

312
00:20:17.010 --> 00:20:22.010
So myself and our team and MIT recently released a paper on functional vigilance

313
00:20:22.321 --> 00:20:24.000
of drivers while using autopilot.

314
00:20:24.600 --> 00:20:29.370
This is work we've been doing since autopilot was first released publicly over

315
00:20:29.371 --> 00:20:33.720
three years ago,
collecting video driver faces and drive her body.

316
00:20:34.590 --> 00:20:39.590
So I saw that you tweeted a quote from the abstract so I can at least a guests

317
00:20:42.001 --> 00:20:46.620
the,
you've glanced at it.
Yeah.
Right.
Can I talk you through what we found?
Sure.

318
00:20:46.710 --> 00:20:47.260
Okay.

319
00:20:47.260 --> 00:20:52.120
So it appears that in the data that we've collected,

320
00:20:52.450 --> 00:20:56.470
that drivers are maintaining functional vigilance such that we're looking at

321
00:20:56.530 --> 00:20:58.540
18,000 disengagement from autopilot,

322
00:20:58.840 --> 00:21:03.840
18,900 and annotating were they able to take over control in a timely manner?

323
00:21:05.770 --> 00:21:10.630
So they were there present looking at the road.
Uh,
to take over control.
Okay.

324
00:21:11.430 --> 00:21:14.240
So this goes against what,

325
00:21:14.340 --> 00:21:18.180
what many would predict from the body of literature and vigilance with

326
00:21:18.181 --> 00:21:20.700
automation.
Now the question is,

327
00:21:21.540 --> 00:21:24.570
do you think these results hold across the broader population?

328
00:21:24.870 --> 00:21:28.710
So ours is just a small subset.
Do you think,
uh,

329
00:21:28.740 --> 00:21:31.740
one of the criticism is that,
you know,

330
00:21:31.741 --> 00:21:36.420
there's a small minority of drivers that may be highly responsible where their

331
00:21:36.421 --> 00:21:40.920
vigilance decrement would increase with autopilot use.
I think

332
00:21:40.940 --> 00:21:43.250
<v 1>this was all really going to be swept.
I mean,</v>

333
00:21:43.350 --> 00:21:48.350
the systems improving so much so fast that this is going to be a moot point very

334
00:21:49.831 --> 00:21:50.664
soon.

335
00:21:52.100 --> 00:21:57.100
Where vigilance is if something's many times safer than a person,

336
00:21:58.990 --> 00:22:03.610
than adding a person.
Uh,
just the,
the,
the,
the effect on safety is,

337
00:22:03.640 --> 00:22:05.290
is limited.
Um,

338
00:22:05.740 --> 00:22:08.680
and in fact it could be negative.

339
00:22:10.680 --> 00:22:14.640
<v 0>That's really interesting.
So the,
uh,
the,
so the fact that a human may,</v>

340
00:22:14.970 --> 00:22:17.880
some percent of the population may,
uh,

341
00:22:17.910 --> 00:22:21.480
exhibit a visualist sacrament will not affect overall statistics,

342
00:22:21.481 --> 00:22:22.314
numbers of safety?

343
00:22:22.350 --> 00:22:26.220
<v 1>No.
In fact,
I think it will become very,</v>

344
00:22:26.221 --> 00:22:29.220
very quickly maybe and towards the end of this year.

345
00:22:29.250 --> 00:22:31.950
But I would say I would be shocked if it's not next year,

346
00:22:32.020 --> 00:22:35.190
but at the latest that um,
having the post,

347
00:22:35.250 --> 00:22:37.770
having a human intervene will decrease safety,

348
00:22:39.730 --> 00:22:40.450
decrease the,

349
00:22:40.450 --> 00:22:43.990
it's like imagine if you're in an elevator and I used to be that the elevator

350
00:22:43.991 --> 00:22:45.670
operators,
um,

351
00:22:45.710 --> 00:22:49.010
and you couldn't go in an elevator by yourself and work the,

352
00:22:49.011 --> 00:22:54.000
the lever to move between floors.
Um,
and now,
uh,

353
00:22:55.120 --> 00:22:59.950
nobody wants it in elevator operator because the automated elevator that stops

354
00:22:59.951 --> 00:23:02.560
the floors is much safer than the elevator operator.

355
00:23:04.030 --> 00:23:07.390
And in fact it will be quite dangerous to have someone with a lever that can

356
00:23:07.510 --> 00:23:09.580
move the elevator between floors.
Okay.

357
00:23:09.790 --> 00:23:13.930
<v 0>So that's a,
that's a really powerful statement and really interesting one.</v>

358
00:23:14.640 --> 00:23:18.280
But I also have to ask from a user experience and from a safety perspective,

359
00:23:18.730 --> 00:23:23.730
one of the passions for me algorithmically is a camera based detection of a,

360
00:23:24.551 --> 00:23:28.240
of just sensing the human but detecting what the driver is looking at cognitive

361
00:23:28.241 --> 00:23:32.110
load,
body pose on the computer vision side.
That's a fascinating problem.

362
00:23:32.111 --> 00:23:32.944
But do you,

363
00:23:33.110 --> 00:23:36.670
and there's many in industry you believe you have to have camera based driver

364
00:23:36.671 --> 00:23:40.840
monitoring.
Do you think there could be benefit gained from driving?
Monitoring?

365
00:23:41.750 --> 00:23:46.650
<v 1>If you have a system that's,
that's Adam,
that's outer below a human level,</v>

366
00:23:46.651 --> 00:23:48.780
reliability,
then driving monitoring,
it makes sense.

367
00:23:50.220 --> 00:23:54.240
But if your system is dramatically better,
more liable than,
than a human,

368
00:23:54.241 --> 00:23:58.620
then drive monitoring,
monitoring is not just not help much.

369
00:23:59.490 --> 00:24:03.920
And uh,
like I said,
you just like as you wouldn't want someone interview,

370
00:24:03.960 --> 00:24:06.630
like you wouldn't want someone in the elevator if you're in an elevator,

371
00:24:06.631 --> 00:24:08.250
do you really want someone with a big lever?

372
00:24:08.310 --> 00:24:11.430
So some random person operating elevator between floors,

373
00:24:12.990 --> 00:24:15.390
I wouldn't trust that or I'd rather have the buttons.

374
00:24:17.470 --> 00:24:21.430
<v 0>Okay.
You're optimistic about the pace of improvement of the system,</v>

375
00:24:21.730 --> 00:24:24.940
not from what you've seen with a full self driving car computer.

376
00:24:25.480 --> 00:24:27.190
The rate of improvement is exponential.

377
00:24:28.540 --> 00:24:33.220
So one of the other very interesting design choices early on that connects to

378
00:24:33.221 --> 00:24:38.221
this is the operational design domain of autopilot.

379
00:24:38.291 --> 00:24:41.590
So where autopilot is able to be turned on.

380
00:24:42.580 --> 00:24:44.110
The contrast.

381
00:24:44.111 --> 00:24:49.000
Another vehicle system that we were studying is the Cadillac supercrew system.

382
00:24:49.060 --> 00:24:53.200
That's in terms of OGD very constrained to particular kinds of highways.

383
00:24:53.560 --> 00:24:54.400
Well mapped,

384
00:24:54.610 --> 00:24:58.570
tested as much narrower than the odd of Tesla vehicles.

385
00:24:59.140 --> 00:25:03.040
What's theirs?
There's blogs and add.
Yeah,

386
00:25:04.570 --> 00:25:07.270
that's good.
That's it.
That's a good line.
Uh,

387
00:25:08.050 --> 00:25:11.160
what was the design decision,
uh,

388
00:25:11.240 --> 00:25:15.550
what's in that different philosophy of thinking where there's pros and cons?

389
00:25:15.580 --> 00:25:20.150
What we see with a,
uh,
a wide o d d is drive test.

390
00:25:20.151 --> 00:25:24.130
As drivers are able to explore more the limitations of the system at least early

391
00:25:24.131 --> 00:25:28.120
on.
And they understand together with the instrument cluster display,

392
00:25:28.240 --> 00:25:31.450
they start to understand what are the capabilities.
So that's a benefit.

393
00:25:31.960 --> 00:25:36.940
The con is you're,
you're letting drivers use it basically anywhere.

394
00:25:37.480 --> 00:25:41.340
Uh,
so anyways,
that can detect lanes with Collins,

395
00:25:41.650 --> 00:25:42.940
was there a philosophy,

396
00:25:43.840 --> 00:25:47.470
a design decisions that were challenging there were being made there?

397
00:25:48.130 --> 00:25:51.550
Or from the very beginning?
Was that,
uh,

398
00:25:51.820 --> 00:25:54.130
don on purpose with intent?

399
00:25:54.630 --> 00:25:57.390
<v 1>Well,
I mean I think it's frankly,
it's pretty crazy giving it,</v>

400
00:25:57.391 --> 00:26:02.350
letting people drive it a two ton death machine manually.
Uh,

401
00:26:02.910 --> 00:26:06.150
that's crazy.
Like,
I guess like in the future of people were like,

402
00:26:06.210 --> 00:26:11.210
I can't believe anyone was just a larger drive for one of these two ton death

403
00:26:12.001 --> 00:26:15.460
machines and think just drive wherever they wanted to just like elevators and

404
00:26:15.470 --> 00:26:18.150
just like move the elevator with the lever wherever you want.

405
00:26:18.151 --> 00:26:19.920
It can stop it halfway between floors if you want.

406
00:26:22.480 --> 00:26:25.080
It's pretty crazy.
So

407
00:26:27.360 --> 00:26:31.710
it's going to seem like a mad thing in the future that people were driving cars.

408
00:26:32.900 --> 00:26:35.670
<v 0>So I have a bunch of questions about the human psychology,</v>

409
00:26:35.671 --> 00:26:37.140
about behavior and so on.

410
00:26:37.530 --> 00:26:42.200
I don't know what it would become mean that we're told him.
Uh,
because uh,

411
00:26:43.140 --> 00:26:48.000
you have faith in the AI system,
uh,
not faith,
but uh,

412
00:26:48.330 --> 00:26:51.930
the both on the hardware side and the deep learning approach of learning from

413
00:26:51.931 --> 00:26:56.490
data,
we'll make it just far safer than humans.
Yeah,
exactly.

414
00:26:57.300 --> 00:27:01.250
Recently there are a few hackers who are tricked autopilot act and not

415
00:27:01.251 --> 00:27:03.420
unexpected ways of adversarial examples.

416
00:27:03.900 --> 00:27:07.770
So we all know that neural network systems are very sensitive to minor

417
00:27:07.771 --> 00:27:10.530
disturbances.
These adversarial examples on input.

418
00:27:11.250 --> 00:27:14.730
Do you think it's possible to defend against something like this for the
product?

419
00:27:15.150 --> 00:27:18.800
For the industry?
Sure.
Yeah.

420
00:27:19.080 --> 00:27:22.530
Can you elaborate on the,
on the confidence behind that answer?

421
00:27:23.540 --> 00:27:26.960
<v 1>Um,
well the,
you know,
in New Orleans is just like basically a bunch of mates,</v>

422
00:27:27.060 --> 00:27:27.990
matrix math.

423
00:27:28.640 --> 00:27:32.700
Oh you have to be like a very sophisticated somebody who really understands

424
00:27:32.701 --> 00:27:37.701
neural nets and like basically reverse engineer how the matrix is being built

425
00:27:38.100 --> 00:27:42.890
and then create a little thing that's just exactly,
um,

426
00:27:42.990 --> 00:27:47.140
causes the matrix math to be slightly off,
but it's very easy to then block it.

427
00:27:47.141 --> 00:27:51.790
Block that by having,
well basically Anthydrate negative recognition.

428
00:27:51.820 --> 00:27:56.800
It's like if the system sees something that looks like a matrix hack,
uh,

429
00:27:56.830 --> 00:27:57.663
excluded.

430
00:27:58.420 --> 00:27:58.910
<v 2>Okay.</v>

431
00:27:58.910 --> 00:28:02.260
<v 1>The Sofa,
it is such an easy thing to do.
Okay.</v>

432
00:28:02.680 --> 00:28:06.160
<v 0>So learn both on the,
the validated and the invalid data.</v>

433
00:28:06.220 --> 00:28:09.700
So basically learn on the adversarial examples to be able to exclude them.

434
00:28:09.740 --> 00:28:12.740
<v 1>Yeah.
You like,
you're basically wanted us to both know what is,</v>

435
00:28:12.741 --> 00:28:17.741
what is a car and what is definitely not a car and you train for this is a car

436
00:28:17.871 --> 00:28:20.150
and this is definitely not a car.
Those are two different things.

437
00:28:21.190 --> 00:28:23.980
Repeat people have no idea.
Neural Nets really they,

438
00:28:23.990 --> 00:28:27.140
it's probably thinking unless it involves like,
you know,
fishing net thing.

439
00:28:29.180 --> 00:28:30.860
<v 0>So as you know,</v>

440
00:28:31.450 --> 00:28:35.660
so taking it a step beyond just Tesla and autopilot,

441
00:28:36.620 --> 00:28:41.620
current deep learning approaches still seem in some ways to beef far from

442
00:28:42.920 --> 00:28:44.360
General Intelligence Systems.

443
00:28:44.660 --> 00:28:49.660
Do you think the current approaches will take us to general intelligence or do

444
00:28:51.050 --> 00:28:53.690
totally new ideas need to be invented?

445
00:28:54.040 --> 00:28:54.873
<v 2>Okay.</v>

446
00:28:55.710 --> 00:28:59.610
<v 1>I think we're missing a few key ideas for general intelligence,</v>

447
00:29:00.090 --> 00:29:01.710
general artificial,
general intelligence,

448
00:29:05.160 --> 00:29:07.200
but it's going to be upon us very quickly

449
00:29:08.870 --> 00:29:13.820
and then we'll need to figure out what should we do if we even have that choice.

450
00:29:15.380 --> 00:29:15.890
<v 2>Yes.</v>

451
00:29:15.890 --> 00:29:17.670
<v 1>But it's,
it's amazing how he will,</v>

452
00:29:17.671 --> 00:29:22.671
can't differentiate between say the narrow AI that allows a car to figure out

453
00:29:23.171 --> 00:29:25.710
what a lane line is and,
and,
and you know,
and,

454
00:29:25.930 --> 00:29:30.250
and navigate streets versus general intelligence.

455
00:29:30.520 --> 00:29:34.180
Like these are just very different things like your toaster and uh,

456
00:29:34.260 --> 00:29:35.830
and your computer or both machines,

457
00:29:35.831 --> 00:29:38.760
but once much more sophisticated than another,
you're

458
00:29:39.080 --> 00:29:42.040
<v 0>confident with Tesla,
you can create the world's</v>

459
00:29:42.040 --> 00:29:46.570
<v 1>best toaster Bose best host for.
Yes,
most of the world's best self driving.</v>

460
00:29:48.670 --> 00:29:52.730
I'm a yes to to me the,

461
00:29:52.731 --> 00:29:55.190
right now this seems game set match.
I don't,

462
00:29:55.280 --> 00:29:57.820
I mean that's how I don't want to be complacent or over confident,

463
00:29:57.830 --> 00:30:01.000
but that's what it appears.
That is just literally what it,

464
00:30:01.250 --> 00:30:03.650
how it appears right now.
I could be wrong,

465
00:30:03.710 --> 00:30:08.710
but it appears to be the case that Tesla is vastly ahead of everyone.

466
00:30:10.910 --> 00:30:15.910
Do you think we will ever create an AI system that we can love and loves his

467
00:30:15.981 --> 00:30:18.320
back in a deep,
meaningful way?
Like in the movie her.

468
00:30:19.580 --> 00:30:20.280
<v 2>Okay.</v>

469
00:30:20.280 --> 00:30:25.280
<v 1>I think AI will be capable of convincing you to fall in love with it very well.</v>

470
00:30:25.860 --> 00:30:29.350
And that's different than us humans.
You know,

471
00:30:29.550 --> 00:30:31.920
we start getting into a metaphysical question of like,

472
00:30:32.070 --> 00:30:35.340
do my emotions and thoughts exist in a different realm,
the physical,

473
00:30:35.640 --> 00:30:39.750
and maybe they do,
maybe they don't.
I don't know.
But from a physics standpoint,

474
00:30:40.020 --> 00:30:43.210
I think I tend to think of things,
you know,

475
00:30:43.980 --> 00:30:48.980
like physics was my main sort of training and from a physics standpoint

476
00:30:50.520 --> 00:30:54.660
essentially if it loves you in a way that is that you can't tell whether it's

477
00:30:54.661 --> 00:30:59.570
real or not.
It is real.
It's a physics view of love.
Yeah.

478
00:31:00.960 --> 00:31:04.620
If there's no,
if the,
if you cannot,
if you cannot prove that it does not,

479
00:31:05.970 --> 00:31:10.020
if there's no tests that you can apply that would make it

480
00:31:13.370 --> 00:31:17.350
make it,
allow you to tell the difference,
then there is no difference.
Right.

481
00:31:17.500 --> 00:31:21.130
And it's similar to a seeing our world of simulation.

482
00:31:21.131 --> 00:31:24.640
There may not be a test to tell the difference between what the real world

483
00:31:24.820 --> 00:31:28.870
simulation and therefore from a physics perspective it might as well be the same

484
00:31:28.871 --> 00:31:32.980
thing.
Yes.
There may be ways to test whether it's a simulation.

485
00:31:34.060 --> 00:31:35.680
They might be.
I'm not saying there aren't,

486
00:31:36.040 --> 00:31:39.580
but you could certainly imagine that assimilation could,
could correct that.

487
00:31:39.790 --> 00:31:42.820
Once an entity in the simulation found a way to detect the simulation,

488
00:31:43.120 --> 00:31:46.570
it could either restart the pores,
the rip simulation,

489
00:31:47.370 --> 00:31:48.460
start a new simulation,

490
00:31:48.461 --> 00:31:51.490
or do one of any other things that then correct for that error.

491
00:31:53.140 --> 00:31:58.140
So when maybe you or somebody else creates an AGI system and you get to ask her

492
00:32:01.571 --> 00:32:04.030
one question,
what would that question be?

493
00:32:17.050 --> 00:32:18.520
What's outside of the simulation?

494
00:32:21.080 --> 00:32:21.740
<v 2>Okay.</v>

495
00:32:21.740 --> 00:32:24.950
<v 1>Ilan,
thank you so much for talking today as a pleasure.
All right.
Thank you.</v>

