WEBVTT

1
00:00:00.120 --> 00:00:03.720
<v Speaker 1>What difference between biological and </v>
<v Speaker 1>you own that works and artificial neural</v>

2
00:00:03.721 --> 00:00:06.900
<v Speaker 1>networks is most mysterious,</v>
<v Speaker 1>captivating,</v>

3
00:00:06.901 --> 00:00:07.770
<v Speaker 1>and profound for you.</v>

4
00:00:11.240 --> 00:00:16.240
<v Speaker 2>First of all,</v>
<v Speaker 2>there's so much we don't know about </v>

5
00:00:16.240 --> 00:00:19.091
<v Speaker 2>biological neural networks and that's </v>
<v Speaker 2>very mysterious and captivating because </v>

6
00:00:19.091 --> 00:00:23.270
<v Speaker 2>maybe it holds the key to improving </v>
<v Speaker 2>artificial neural networks.</v>

7
00:00:24.260 --> 00:00:29.260
<v Speaker 2>One of the things I studied recently as </v>
<v Speaker 2>something that we don't know how </v>

8
00:00:31.310 --> 00:00:36.310
<v Speaker 2>biological neural networks do but would </v>
<v Speaker 2>be really useful for artificial ones is </v>

9
00:00:38.330 --> 00:00:43.330
<v Speaker 2>the ability to do credit assignment </v>
<v Speaker 2>through very long time spans.</v>

10
00:00:44.150 --> 00:00:49.150
<v Speaker 2>There are things that we can in </v>
<v Speaker 2>principle do with artificial neural </v>

11
00:00:49.150 --> 00:00:53.951
<v Speaker 2>nets,</v>
<v Speaker 2>but it's not very convenient and it's </v>

12
00:00:53.951 --> 00:00:55.460
<v Speaker 2>not biologically plausible.</v>
<v Speaker 2>And this mismatch and think this kind of</v>

13
00:00:55.461 --> 00:01:00.461
<v Speaker 2>mismatch.</v>
<v Speaker 2>Maybe an interesting thing to study to a</v>

14
00:01:01.791 --> 00:01:06.791
<v Speaker 2>understand better how brains my duties </v>
<v Speaker 2>things because we don't have good </v>

15
00:01:06.791 --> 00:01:09.200
<v Speaker 2>corresponding theories with artificial </v>
<v Speaker 2>neural nets and B,</v>

16
00:01:10.280 --> 00:01:15.280
<v Speaker 2>maybe provide new ideas that we could </v>
<v Speaker 2>explore about,</v>

17
00:01:15.370 --> 00:01:20.370
<v Speaker 2>um,</v>
<v Speaker 2>things like brain do differently and </v>

18
00:01:20.370 --> 00:01:22.130
<v Speaker 2>that we could incorporate in artificial </v>
<v Speaker 2>neural nets.</v>

19
00:01:22.370 --> 00:01:23.210
<v Speaker 2>So let's break</v>

20
00:01:23.320 --> 00:01:27.640
<v Speaker 1>credit assignment up a little bit.</v>
<v Speaker 1>It's a beautifully technical term,</v>

21
00:01:27.790 --> 00:01:32.790
<v Speaker 1>but it can incorporate so many things.</v>
<v Speaker 1>So is it more on the rnn memory side </v>

22
00:01:35.290 --> 00:01:40.290
<v Speaker 1>that thinking like that or is it </v>
<v Speaker 1>something about knowledge building up </v>

23
00:01:40.290 --> 00:01:43.921
<v Speaker 1>common sense knowledge over time,</v>
<v Speaker 1>or is it more in the reinforcement </v>

24
00:01:44.231 --> 00:01:49.231
<v Speaker 1>learning sense that you're picking up </v>
<v Speaker 1>rewards over time for a particular </v>

25
00:01:49.231 --> 00:01:50.210
<v Speaker 1>charge you a certain kinds of goals?</v>

26
00:01:50.470 --> 00:01:55.470
<v Speaker 2>I was thinking more about the first two </v>
<v Speaker 2>meetings whereby we store all kinds of </v>

27
00:01:57.521 --> 00:02:01.270
<v Speaker 2>memories,</v>
<v Speaker 2>episodic memories in our brain,</v>

28
00:02:02.200 --> 00:02:07.200
<v Speaker 2>which we can access later in order to </v>
<v Speaker 2>help us both infor causes of things that</v>

29
00:02:11.531 --> 00:02:16.531
<v Speaker 2>we are observing now and assign credit </v>
<v Speaker 2>to decisions or interpretations we came </v>

30
00:02:20.081 --> 00:02:25.081
<v Speaker 2>up with a while ago when you know those </v>
<v Speaker 2>memories were stored and then we can </v>

31
00:02:25.211 --> 00:02:30.211
<v Speaker 2>change the way we would have reacted or </v>
<v Speaker 2>interpreted things in the past.</v>

32
00:02:31.870 --> 00:02:34.930
<v Speaker 2>And now that's credit assignment used </v>
<v Speaker 2>for learning.</v>

33
00:02:36.220 --> 00:02:38.800
<v Speaker 2>So in which way do you think</v>

34
00:02:39.440 --> 00:02:44.150
<v Speaker 1>artificial neural networks,</v>
<v Speaker 1>the current Lstm,</v>

35
00:02:44.630 --> 00:02:47.690
<v Speaker 1>the current architectures are not able </v>
<v Speaker 1>to capture</v>

36
00:02:48.910 --> 00:02:50.740
<v Speaker 2>the.</v>
<v Speaker 2>Presumably you're,</v>

37
00:02:50.750 --> 00:02:53.440
<v Speaker 2>you're thinking of very longterm.</v>
<v Speaker 2>Yes.</v>

38
00:02:53.740 --> 00:02:58.740
<v Speaker 2>So current recurrent nets are doing a </v>
<v Speaker 2>fairly good jobs for sequences,</v>

39
00:02:59.890 --> 00:03:04.890
<v Speaker 2>dozens or hundreds of time steps and </v>
<v Speaker 2>then it gets harder and harder and </v>

40
00:03:06.161 --> 00:03:11.161
<v Speaker 2>depending on what you have to remember </v>
<v Speaker 2>and so on as you consider longer </v>

41
00:03:11.161 --> 00:03:14.341
<v Speaker 2>durations.</v>
<v Speaker 2>Whereas humans seem to be able to do </v>

42
00:03:15.161 --> 00:03:17.580
<v Speaker 2>credit assignment to essentially </v>
<v Speaker 2>arbitrary times.</v>

43
00:03:17.581 --> 00:03:22.581
<v Speaker 2>Like I,</v>
<v Speaker 2>I could remember something I did last </v>

44
00:03:22.581 --> 00:03:23.170
<v Speaker 2>year and now because I see some new </v>
<v Speaker 2>evidence,</v>

45
00:03:23.200 --> 00:03:28.200
<v Speaker 2>I'm gonna change my mind about the way I</v>
<v Speaker 2>was thinking last year and hopefully not</v>

46
00:03:29.621 --> 00:03:30.910
<v Speaker 2>do the same mistake again.</v>

47
00:03:33.060 --> 00:03:38.060
<v Speaker 1>I think a big part of that,</v>
<v Speaker 1>it's probably forgetting your only </v>

48
00:03:38.060 --> 00:03:40.710
<v Speaker 1>remembering the really important things </v>
<v Speaker 1>that's very efficient for getting.</v>

49
00:03:42.190 --> 00:03:47.190
<v Speaker 2>Yes.</v>
<v Speaker 2>So there's a selection of what we </v>

50
00:03:47.190 --> 00:03:49.291
<v Speaker 2>remember and I think they're a really </v>
<v Speaker 2>cool connection to higher level </v>

51
00:03:49.291 --> 00:03:53.911
<v Speaker 2>cognition here regarding consciousness </v>
<v Speaker 2>deciding and any emotions that sort of </v>

52
00:03:54.850 --> 00:03:58.180
<v Speaker 2>deciding what comes to consciousness and</v>
<v Speaker 2>what gets stored in memory,</v>

53
00:03:58.930 --> 00:04:01.300
<v Speaker 2>which,</v>
<v Speaker 2>which are not trivial either.</v>

54
00:04:02.890 --> 00:04:06.160
<v Speaker 1>So you've been at the forefront there </v>
<v Speaker 1>all along.</v>

55
00:04:07.000 --> 00:04:09.820
<v Speaker 1>Showing some of the amazing things that </v>
<v Speaker 1>neural networks,</v>

56
00:04:09.910 --> 00:04:14.910
<v Speaker 1>deep neural networks can do in the field</v>
<v Speaker 1>of artificial intelligence is just </v>

57
00:04:14.910 --> 00:04:17.430
<v Speaker 1>broadly in all kinds of applications.</v>
<v Speaker 1>But uh,</v>

58
00:04:17.780 --> 00:04:21.370
<v Speaker 1>we can talk about that forever.</v>
<v Speaker 1>But what in your view,</v>

59
00:04:21.970 --> 00:04:23.560
<v Speaker 1>because we're thinking towards the </v>
<v Speaker 1>future,</v>

60
00:04:24.010 --> 00:04:27.970
<v Speaker 1>is the weakest aspect of the way deep </v>
<v Speaker 1>neural networks represent the world.</v>

61
00:04:28.030 --> 00:04:31.210
<v Speaker 1>What is the,</v>
<v Speaker 1>what is in your view is missing?</v>

62
00:04:32.200 --> 00:04:33.700
<v Speaker 1>So currently current</v>

63
00:04:33.920 --> 00:04:38.920
<v Speaker 2>state of the art neural nets trained on </v>
<v Speaker 2>large quantities of images or texts have</v>

64
00:04:44.131 --> 00:04:48.840
<v Speaker 2>some level of understanding of what </v>
<v Speaker 2>explains those datasets,</v>

65
00:04:48.990 --> 00:04:52.170
<v Speaker 2>but it's very basic.</v>
<v Speaker 2>It's,</v>

66
00:04:52.210 --> 00:04:57.210
<v Speaker 2>it's very low level and it's not nearly </v>
<v Speaker 2>as robust and abstract and general as </v>

67
00:05:00.500 --> 00:05:03.350
<v Speaker 2>our understanding.</v>
<v Speaker 2>Okay.</v>

68
00:05:03.351 --> 00:05:05.360
<v Speaker 2>So that doesn't tell us how to fix </v>
<v Speaker 2>things,</v>

69
00:05:05.780 --> 00:05:10.780
<v Speaker 2>but I think it encourages us to think </v>
<v Speaker 2>about how we can maybe train our neural </v>

70
00:05:14.781 --> 00:05:19.781
<v Speaker 2>nets differently so that they would </v>
<v Speaker 2>focus,</v>

71
00:05:20.391 --> 00:05:25.391
<v Speaker 2>for example,</v>
<v Speaker 2>on causal explanation is something that </v>

72
00:05:25.391 --> 00:05:27.050
<v Speaker 2>we don't do currently with neural net </v>
<v Speaker 2>training.</v>

73
00:05:27.830 --> 00:05:28.850
<v Speaker 2>Also,</v>

74
00:05:30.000 --> 00:05:35.000
<v Speaker 2>one thing I'll talk about in my talk </v>
<v Speaker 2>this afternoon is instead of learning </v>

75
00:05:35.851 --> 00:05:40.140
<v Speaker 2>separately from images and videos on one</v>
<v Speaker 2>hand and from texts,</v>

76
00:05:40.141 --> 00:05:45.141
<v Speaker 2>on the other hand,</v>
<v Speaker 2>we need to do a better job of jointly </v>

77
00:05:45.841 --> 00:05:50.841
<v Speaker 2>learning about language and about the </v>
<v Speaker 2>world to which it refers so that both </v>

78
00:05:53.821 --> 00:05:58.340
<v Speaker 2>sides can help each other.</v>
<v Speaker 2>We need to have good world models in,</v>

79
00:05:58.350 --> 00:06:03.350
<v Speaker 2>in our neural nets for them to really </v>
<v Speaker 2>understand sentences which talk about </v>

80
00:06:04.311 --> 00:06:09.311
<v Speaker 2>what's going on in the world and I think</v>
<v Speaker 2>we need language input to help provide </v>

81
00:06:12.081 --> 00:06:17.081
<v Speaker 2>clues about what the high level concepts</v>
<v Speaker 2>like semantic concepts should be </v>

82
00:06:17.781 --> 00:06:21.170
<v Speaker 2>represented at the top levels of these </v>
<v Speaker 2>neural nets.</v>

83
00:06:21.770 --> 00:06:26.770
<v Speaker 2>In fact,</v>
<v Speaker 2>there is evidence that the purely </v>

84
00:06:26.990 --> 00:06:31.990
<v Speaker 2>unsupervised learning of representations</v>
<v Speaker 2>doesn't give rise to high level </v>

85
00:06:32.690 --> 00:06:37.690
<v Speaker 2>representations that are as powerful as </v>
<v Speaker 2>the ones we're getting from supervised </v>

86
00:06:37.690 --> 00:06:39.010
<v Speaker 2>learning.</v>
<v Speaker 2>And so the,</v>

87
00:06:39.011 --> 00:06:44.011
<v Speaker 2>the,</v>
<v Speaker 2>the clues were getting just with the </v>

88
00:06:44.011 --> 00:06:45.851
<v Speaker 2>labels.</v>
<v Speaker 2>Not even sentences is already very </v>

89
00:06:45.851 --> 00:06:45.851
<v Speaker 2>powerful.</v>

90
00:06:45.851 --> 00:06:49.420
<v Speaker 1>Do you think that's an architecture </v>
<v Speaker 1>challenge or is it a Dataset challenge?</v>

91
00:06:49.630 --> 00:06:54.470
<v Speaker 1>Neither.</v>
<v Speaker 1>A.</v>

92
00:06:54.670 --> 00:06:58.900
<v Speaker 1>I'm tempted to just end it there.</v>
<v Speaker 1>Can you elaborate?</v>

93
00:07:01.690 --> 00:07:03.460
<v Speaker 2>Uh,</v>
<v Speaker 2>of course,</v>

94
00:07:03.461 --> 00:07:06.370
<v Speaker 2>data sets on architectures are something</v>
<v Speaker 2>you want to always play with,</v>

95
00:07:06.371 --> 00:07:11.371
<v Speaker 2>but,</v>
<v Speaker 2>but I think the crucial thing is more </v>

96
00:07:11.371 --> 00:07:11.371
<v Speaker 2>the training objectives,</v>
<v Speaker 2>the training frameworks,</v>

97
00:07:12.400 --> 00:07:17.400
<v Speaker 2>for example,</v>
<v Speaker 2>going from passive observation of data </v>

98
00:07:17.400 --> 00:07:22.200
<v Speaker 2>to more active agents,</v>
<v Speaker 2>which I'm a learn by intervening in the </v>

99
00:07:23.771 --> 00:07:24.820
<v Speaker 2>world,</v>
<v Speaker 2>uh,</v>

100
00:07:25.000 --> 00:07:27.610
<v Speaker 2>the relationships between causes and </v>
<v Speaker 2>effects,</v>

101
00:07:28.570 --> 00:07:33.570
<v Speaker 2>the sort of objective functions which </v>
<v Speaker 2>could be important to allow the,</v>

102
00:07:35.460 --> 00:07:37.120
<v Speaker 2>the highest level,</v>
<v Speaker 2>uh,</v>

103
00:07:37.121 --> 00:07:38.470
<v Speaker 2>explanations to,</v>
<v Speaker 2>to,</v>

104
00:07:38.550 --> 00:07:43.550
<v Speaker 2>to rise from the learning,</v>
<v Speaker 2>which I don't think we have now a kinds </v>

105
00:07:43.931 --> 00:07:46.570
<v Speaker 2>of objective functions which could be </v>
<v Speaker 2>used to,</v>

106
00:07:46.880 --> 00:07:48.910
<v Speaker 2>uh,</v>
<v Speaker 2>reward exploration,</v>

107
00:07:48.911 --> 00:07:53.911
<v Speaker 2>the right kind of exploration to these </v>
<v Speaker 2>kinds of questions are neither in the </v>

108
00:07:53.911 --> 00:07:55.600
<v Speaker 2>Dataset nor,</v>
<v Speaker 2>and the architecture,</v>

109
00:07:55.601 --> 00:08:00.601
<v Speaker 2>but more in how we learn under what </v>
<v Speaker 2>objectives and so on.</v>

110
00:08:01.590 --> 00:08:02.970
<v Speaker 1>Yeah,</v>
<v Speaker 1>that's a fruit.</v>

111
00:08:02.990 --> 00:08:06.480
<v Speaker 1>You mentioned in several contexts.</v>
<v Speaker 1>The idea sort of the way children learn,</v>

112
00:08:06.481 --> 00:08:11.481
<v Speaker 1>they interact with objects of the world.</v>
<v Speaker 1>And it seems a fascinating because in </v>

113
00:08:11.671 --> 00:08:16.671
<v Speaker 1>some sense,</v>
<v Speaker 1>except with some cases in reinforcement </v>

114
00:08:16.671 --> 00:08:19.701
<v Speaker 1>learning,</v>
<v Speaker 1>that idea is not part of the learning </v>

115
00:08:20.100 --> 00:08:23.880
<v Speaker 1>process in artificial neural networks.</v>
<v Speaker 1>So it's almost like,</v>

116
00:08:24.350 --> 00:08:29.350
<v Speaker 1>do you envision something like an </v>
<v Speaker 1>objective function saying,</v>

117
00:08:30.420 --> 00:08:35.420
<v Speaker 1>you know what,</v>
<v Speaker 1>if you poke this object in this kind of </v>

118
00:08:35.420 --> 00:08:39.111
<v Speaker 1>way will be really helpful for me at to </v>
<v Speaker 1>four further further learn sort of </v>

119
00:08:40.291 --> 00:08:43.410
<v Speaker 1>almost guiding some aspect of learning.</v>
<v Speaker 1>Right,</v>

120
00:08:43.450 --> 00:08:43.660
<v Speaker 1>right.</v>

121
00:08:43.660 --> 00:08:48.660
<v Speaker 2>So I was talking to Rebecca Saxe just an</v>
<v Speaker 2>hour ago and she was talking about lots </v>

122
00:08:49.721 --> 00:08:54.721
<v Speaker 2>and lots of evidence from a infants seem</v>
<v Speaker 2>to clearly take what interests them in a</v>

123
00:09:00.691 --> 00:09:04.950
<v Speaker 2>directed way.</v>
<v Speaker 2>And so they're not passive learners.</v>

124
00:09:05.010 --> 00:09:09.000
<v Speaker 2>They,</v>
<v Speaker 2>they focus their attention on aspects of</v>

125
00:09:09.001 --> 00:09:12.850
<v Speaker 2>the world which are most interesting,</v>
<v Speaker 2>surprising in,</v>

126
00:09:12.851 --> 00:09:17.851
<v Speaker 2>in a non trivial way that makes them </v>
<v Speaker 2>change their theories of the world.</v>

127
00:09:20.010 --> 00:09:21.300
<v Speaker 2>So,</v>
<v Speaker 2>uh,</v>

128
00:09:21.301 --> 00:09:24.120
<v Speaker 2>that's a fascinating view of the future </v>
<v Speaker 2>progress.</v>

129
00:09:24.121 --> 00:09:27.840
<v Speaker 2>But on a,</v>
<v Speaker 2>on a more maybe boring</v>

130
00:09:27.970 --> 00:09:32.740
<v Speaker 1>a question,</v>
<v Speaker 1>do you think going deeper and larger.</v>

131
00:09:32.840 --> 00:09:37.840
<v Speaker 1>So do you think just increasing the size</v>
<v Speaker 1>of the things that have been increasing </v>

132
00:09:38.781 --> 00:09:41.130
<v Speaker 1>a lot in the past few years?</v>
<v Speaker 1>We'll,</v>

133
00:09:41.300 --> 00:09:46.300
<v Speaker 1>we'll also make significant progress.</v>
<v Speaker 1>So some of the representational issues </v>

134
00:09:47.211 --> 00:09:52.211
<v Speaker 1>that you mentioned,</v>
<v Speaker 1>that they're kind of shallow in some </v>

135
00:09:52.211 --> 00:09:52.211
<v Speaker 1>sense.</v>

136
00:09:52.330 --> 00:09:54.940
<v Speaker 2>Oh,</v>
<v Speaker 2>prior and a sense of abstraction.</v>

137
00:09:55.000 --> 00:09:57.340
<v Speaker 2>Abstract in the sense of abstraction,</v>
<v Speaker 2>they're not getting some.</v>

138
00:09:58.110 --> 00:10:03.110
<v Speaker 2>I don't think that having more,</v>
<v Speaker 2>more depth in the network in the sense </v>

139
00:10:03.110 --> 00:10:06.160
<v Speaker 2>of instead of 100 layers we have 10,000</v>
<v Speaker 2>is going to solve our problem.</v>

140
00:10:06.640 --> 00:10:07.390
<v Speaker 2>You don't think so?</v>
<v Speaker 2>No.</v>

141
00:10:08.950 --> 00:10:10.840
<v Speaker 2>Is that obvious to you?</v>
<v Speaker 2>Yes.</v>

142
00:10:11.440 --> 00:10:15.400
<v Speaker 2>What is clear to me is that engineers </v>
<v Speaker 2>and companies and labs,</v>

143
00:10:15.910 --> 00:10:20.910
<v Speaker 2>a Grad students will continue to tune </v>
<v Speaker 2>architectures and explore all kinds of </v>

144
00:10:22.241 --> 00:10:26.110
<v Speaker 2>tweaks to make the current state of the </v>
<v Speaker 2>arts that he ever slightly better.</v>

145
00:10:26.280 --> 00:10:31.280
<v Speaker 2>Right.</v>
<v Speaker 2>But I don't think that's going to be </v>

146
00:10:31.280 --> 00:10:33.991
<v Speaker 2>nearly enough.</v>
<v Speaker 2>I think we need some fairly drastic </v>

147
00:10:33.991 --> 00:10:36.391
<v Speaker 2>changes in the way that we're </v>
<v Speaker 2>considering learning to achieve the goal</v>

148
00:10:38.141 --> 00:10:43.141
<v Speaker 2>that these learners actually understand </v>
<v Speaker 2>in a deep way the environment in which </v>

149
00:10:43.141 --> 00:10:43.950
<v Speaker 2>they are,</v>
<v Speaker 2>uh,</v>

150
00:10:44.010 --> 00:10:45.820
<v Speaker 2>you know,</v>
<v Speaker 2>observing and acting.</v>

151
00:10:46.570 --> 00:10:47.860
<v Speaker 2>But I,</v>
<v Speaker 2>I guess,</v>

152
00:10:47.861 --> 00:10:52.861
<v Speaker 2>uh,</v>
<v Speaker 2>I was trying to ask a question is more </v>

153
00:10:52.861 --> 00:10:55.471
<v Speaker 2>interesting than just more layers is </v>
<v Speaker 2>basically once you figure out a way to </v>

154
00:10:56.291 --> 00:11:01.291
<v Speaker 2>learn to interacting,</v>
<v Speaker 2>how many parameters does it take to </v>

155
00:11:01.421 --> 00:11:02.530
<v Speaker 2>store that information?</v>

156
00:11:02.920 --> 00:11:07.810
<v Speaker 2>So I think our brain is quite bigger </v>
<v Speaker 2>than most neural networks.</v>

157
00:11:07.890 --> 00:11:08.410
<v Speaker 2>Right,</v>
<v Speaker 2>right.</v>

158
00:11:08.440 --> 00:11:08.830
<v Speaker 2>Oh,</v>
<v Speaker 2>I see.</v>

159
00:11:08.831 --> 00:11:09.690
<v Speaker 2>What you mean,</v>
<v Speaker 2>I'll I,</v>

160
00:11:09.720 --> 00:11:14.720
<v Speaker 2>I'm with you there.</v>
<v Speaker 2>So I agree that in order to build you'll</v>

161
00:11:14.831 --> 00:11:18.010
<v Speaker 2>nets with a kind of a broad knowledge of</v>
<v Speaker 2>the world.</v>

162
00:11:18.011 --> 00:11:23.011
<v Speaker 2>The typical adult humans have probably </v>
<v Speaker 2>the kind of computing power we have now.</v>

163
00:11:23.941 --> 00:11:26.650
<v Speaker 2>He's going to be insufficient.</v>
<v Speaker 2>So far.</v>

164
00:11:26.780 --> 00:11:29.170
<v Speaker 2>The good news is there are hardware </v>
<v Speaker 2>companies building,</v>

165
00:11:29.171 --> 00:11:31.330
<v Speaker 2>you'll net chips and so it's gonna get </v>
<v Speaker 2>better.</v>

166
00:11:33.040 --> 00:11:36.100
<v Speaker 2>However,</v>
<v Speaker 2>the good news in a way,</v>

167
00:11:36.220 --> 00:11:41.220
<v Speaker 2>which is also a bad news,</v>
<v Speaker 2>is that even our state of the art deep </v>

168
00:11:41.801 --> 00:11:46.801
<v Speaker 2>learning methods fail to learn models </v>
<v Speaker 2>that understand even very simple </v>

169
00:11:47.951 --> 00:11:50.620
<v Speaker 2>environments like some grid worlds that </v>
<v Speaker 2>we have built.</v>

170
00:11:52.090 --> 00:11:54.070
<v Speaker 2>Even these fairly environments.</v>
<v Speaker 2>I mean,</v>

171
00:11:54.340 --> 00:11:56.230
<v Speaker 2>of course if you trim them with enough </v>
<v Speaker 2>examples,</v>

172
00:11:56.231 --> 00:11:59.680
<v Speaker 2>eventually they get it,</v>
<v Speaker 2>but it's just like instead of what,</v>

173
00:12:00.040 --> 00:12:05.040
<v Speaker 2>instead of what humans might need,</v>
<v Speaker 2>just a dozens of examples of these </v>

174
00:12:05.040 --> 00:12:06.760
<v Speaker 2>things,</v>
<v Speaker 2>we'll need millions,</v>

175
00:12:06.880 --> 00:12:08.120
<v Speaker 2>right?</v>
<v Speaker 2>For very,</v>

176
00:12:08.130 --> 00:12:09.340
<v Speaker 2>very,</v>
<v Speaker 2>very simple tasks.</v>

177
00:12:10.120 --> 00:12:15.120
<v Speaker 2>And so I think there's an opportunity </v>
<v Speaker 2>for academics who don't have the kind of</v>

178
00:12:16.391 --> 00:12:21.391
<v Speaker 2>computing power that say Google has to </v>
<v Speaker 2>do really important and exciting </v>

179
00:12:21.641 --> 00:12:26.290
<v Speaker 2>research to advance the state of the art</v>
<v Speaker 2>in training frameworks,</v>

180
00:12:26.710 --> 00:12:28.160
<v Speaker 2>learning models,</v>
<v Speaker 2>uh,</v>

181
00:12:28.210 --> 00:12:33.210
<v Speaker 2>agent learning in even simple </v>
<v Speaker 2>environments that are synthetic that </v>

182
00:12:33.821 --> 00:12:38.821
<v Speaker 2>seemed trivial,</v>
<v Speaker 2>but yet current machine learning fails </v>

183
00:12:38.821 --> 00:12:38.821
<v Speaker 2>on.</v>

184
00:12:38.821 --> 00:12:42.830
<v Speaker 1>We talked about priors and common sense </v>
<v Speaker 1>knowledge.</v>

185
00:12:43.360 --> 00:12:48.360
<v Speaker 1>It seems like a,</v>
<v Speaker 1>we humans take a lot of knowledge for </v>

186
00:12:48.471 --> 00:12:49.610
<v Speaker 1>granted.</v>
<v Speaker 1>Uh,</v>

187
00:12:50.390 --> 00:12:55.390
<v Speaker 1>so what's your view of these priors of </v>
<v Speaker 1>forming this broad view of the world,</v>

188
00:12:56.470 --> 00:13:01.470
<v Speaker 1>this accumulation of information and how</v>
<v Speaker 1>we can teach and your networks are </v>

189
00:13:01.470 --> 00:13:03.380
<v Speaker 1>learning systems to pick that knowledge </v>
<v Speaker 1>up.</v>

190
00:13:03.680 --> 00:13:04.750
<v Speaker 1>So knowledge,</v>
<v Speaker 1>you know,</v>

191
00:13:04.890 --> 00:13:09.890
<v Speaker 1>for awhile the artificial intelligence </v>
<v Speaker 1>maybe in the 80 [inaudible] like there's</v>

192
00:13:11.901 --> 00:13:16.790
<v Speaker 1>a time or knowledge representation,</v>
<v Speaker 1>knowledge acquisition expert systems,</v>

193
00:13:17.280 --> 00:13:22.280
<v Speaker 1>symbolic ai was the view was an </v>
<v Speaker 1>interesting problem set to solve.</v>

194
00:13:23.330 --> 00:13:26.090
<v Speaker 1>And it was kind of put on hold a little </v>
<v Speaker 1>bit.</v>

195
00:13:26.450 --> 00:13:29.030
<v Speaker 1>It seems like because it doesn't work,</v>
<v Speaker 1>it doesn't work.</v>

196
00:13:29.031 --> 00:13:31.580
<v Speaker 1>That's right.</v>
<v Speaker 1>But that's right.</v>

197
00:13:31.970 --> 00:13:36.080
<v Speaker 1>But the goals of that remain important.</v>
<v Speaker 1>Yes.</v>

198
00:13:36.081 --> 00:13:41.081
<v Speaker 1>Remain important.</v>
<v Speaker 1>And how do you think those goals can be </v>

199
00:13:41.081 --> 00:13:41.690
<v Speaker 1>addressed?</v>
<v Speaker 1>So first of all,</v>

200
00:13:42.330 --> 00:13:47.330
<v Speaker 2>I believe that one reason why the </v>
<v Speaker 2>classical expert systems approach failed</v>

201
00:13:49.230 --> 00:13:52.290
<v Speaker 2>is because a lot of the knowledge we </v>
<v Speaker 2>have.</v>

202
00:13:52.291 --> 00:13:54.900
<v Speaker 2>So you talked about common sense </v>
<v Speaker 2>intuition.</v>

203
00:13:55.950 --> 00:14:00.950
<v Speaker 2>Um,</v>
<v Speaker 2>there's a lot of knowledge like this </v>

204
00:14:00.950 --> 00:14:04.280
<v Speaker 2>which is not consciously accessible.</v>
<v Speaker 2>There are lots of decisions we're taking</v>

205
00:14:04.621 --> 00:14:09.621
<v Speaker 2>that we can really explain even if </v>
<v Speaker 2>sometimes we make up a story and that </v>

206
00:14:11.611 --> 00:14:15.100
<v Speaker 2>knowledge is also necessary for machines</v>
<v Speaker 2>to,</v>

207
00:14:15.190 --> 00:14:19.850
<v Speaker 2>to take good decisions.</v>
<v Speaker 2>And that knowledge is hard to codify and</v>

208
00:14:20.020 --> 00:14:25.020
<v Speaker 2>expert systems,</v>
<v Speaker 2>rule based systems and classical ai </v>

209
00:14:25.020 --> 00:14:28.370
<v Speaker 2>formalism.</v>
<v Speaker 2>And there are other issues of course </v>

210
00:14:28.370 --> 00:14:31.071
<v Speaker 2>with the old I like,</v>
<v Speaker 2>I'm not really good ways of handling </v>

211
00:14:31.250 --> 00:14:34.530
<v Speaker 2>uncertainty.</v>
<v Speaker 2>I would say something more subtle,</v>

212
00:14:35.460 --> 00:14:40.460
<v Speaker 2>which we understand better now,</v>
<v Speaker 2>but I think still isn't enough in the </v>

213
00:14:40.460 --> 00:14:44.721
<v Speaker 2>minds of people.</v>
<v Speaker 2>There's something really powerful that </v>

214
00:14:44.721 --> 00:14:49.251
<v Speaker 2>comes from distributed representations.</v>
<v Speaker 2>The thing that really makes neural nets </v>

215
00:14:50.160 --> 00:14:55.160
<v Speaker 2>work so well and it's hard to replicate </v>
<v Speaker 2>that kind of power in symbolic world.</v>

216
00:14:59.540 --> 00:15:04.540
<v Speaker 2>The knowledge in an expert systems and </v>
<v Speaker 2>so on is nicely decomposed into like a </v>

217
00:15:05.361 --> 00:15:07.920
<v Speaker 2>bunch of rules.</v>
<v Speaker 2>Whereas if you think about a new net,</v>

218
00:15:08.020 --> 00:15:13.020
<v Speaker 2>it's the opposite.</v>
<v Speaker 2>You have this big blob of parameters </v>

219
00:15:13.020 --> 00:15:17.111
<v Speaker 2>which work intensely together to are </v>
<v Speaker 2>presented everything the network knows </v>

220
00:15:17.111 --> 00:15:21.520
<v Speaker 2>and it's not sufficiently factorized and</v>
<v Speaker 2>so I think this is one of the weaknesses</v>

221
00:15:22.370 --> 00:15:27.370
<v Speaker 2>of current neural nets that we have to </v>
<v Speaker 2>take lessons from classical ai in order </v>

222
00:15:29.001 --> 00:15:34.001
<v Speaker 2>to bring in another kind of composition,</v>
<v Speaker 2>let which is common in language for </v>

223
00:15:34.001 --> 00:15:35.150
<v Speaker 2>example,</v>
<v Speaker 2>and in these rules,</v>

224
00:15:35.870 --> 00:15:40.870
<v Speaker 2>but that isn't so native to neural nets </v>
<v Speaker 2>and on that line of thinking </v>

225
00:15:43.580 --> 00:15:46.010
<v Speaker 2>disentangled representations.</v>
<v Speaker 2>Yes,</v>

226
00:15:46.011 --> 00:15:51.011
<v Speaker 2>so,</v>
<v Speaker 2>so when you connect with disentangled </v>

227
00:15:51.011 --> 00:15:51.200
<v Speaker 2>representations,</v>
<v Speaker 2>if you might,</v>

228
00:15:51.201 --> 00:15:56.201
<v Speaker 2>if you don't mind it.</v>
<v Speaker 2>So for many years I've thought and I </v>

229
00:15:56.201 --> 00:16:00.650
<v Speaker 2>still believe that it's really important</v>
<v Speaker 2>that we come up with learning algorithms</v>

230
00:16:00.680 --> 00:16:03.720
<v Speaker 2>either unsupervised or supervised,</v>
<v Speaker 2>but reinforcement,</v>

231
00:16:03.860 --> 00:16:08.860
<v Speaker 2>whatever that builds representations in </v>
<v Speaker 2>which the important factors,</v>

232
00:16:09.420 --> 00:16:14.420
<v Speaker 2>hopefully causal factors are nicely </v>
<v Speaker 2>separated and easy to pick up from the </v>

233
00:16:14.420 --> 00:16:14.450
<v Speaker 2>representation.</v>

234
00:16:15.080 --> 00:16:17.180
<v Speaker 2>So that's the idea of disentangled </v>
<v Speaker 2>representations.</v>

235
00:16:17.350 --> 00:16:21.800
<v Speaker 2>It says transform the data into a space </v>
<v Speaker 2>where everything becomes easy.</v>

236
00:16:21.801 --> 00:16:26.390
<v Speaker 2>We can maybe just learn with linear </v>
<v Speaker 2>models about the things we care about.</v>

237
00:16:27.560 --> 00:16:32.560
<v Speaker 2>And I still think this is important,</v>
<v Speaker 2>but I think this is missing out on a </v>

238
00:16:32.560 --> 00:16:36.371
<v Speaker 2>very important ingredient which </v>
<v Speaker 2>classical ai systems can remind us of.</v>

239
00:16:38.150 --> 00:16:40.700
<v Speaker 2>So let's say we have these,</v>
<v Speaker 2>these integral presentation,</v>

240
00:16:40.730 --> 00:16:45.470
<v Speaker 2>you still need to learn about the </v>
<v Speaker 2>relationships between the variables,</v>

241
00:16:45.500 --> 00:16:48.140
<v Speaker 2>those high level semantic variables.</v>
<v Speaker 2>They're not going to be independent.</v>

242
00:16:48.141 --> 00:16:52.100
<v Speaker 2>I mean this is like too much of a.</v>
<v Speaker 2>An assumption they're going to have some</v>

243
00:16:52.101 --> 00:16:57.101
<v Speaker 2>interesting relationships that allow us </v>
<v Speaker 2>to predict things in the future to </v>

244
00:16:57.101 --> 00:17:00.431
<v Speaker 2>explain what happened in the past.</v>
<v Speaker 2>The kind of knowledge about those </v>

245
00:17:00.431 --> 00:17:04.421
<v Speaker 2>relationships in a classical ai system </v>
<v Speaker 2>is encoded in the rules like rule is </v>

246
00:17:04.421 --> 00:17:06.110
<v Speaker 2>just like a little piece of knowledge </v>
<v Speaker 2>that says,</v>

247
00:17:06.111 --> 00:17:07.400
<v Speaker 2>oh,</v>
<v Speaker 2>I have these two,</v>

248
00:17:07.401 --> 00:17:12.401
<v Speaker 2>three,</v>
<v Speaker 2>four variables that are linked in this </v>

249
00:17:12.401 --> 00:17:12.401
<v Speaker 2>interesting way.</v>

250
00:17:12.401 --> 00:17:14.540
<v Speaker 2>Then I can say something about one or </v>
<v Speaker 2>two of them given a couple of others.</v>

251
00:17:14.541 --> 00:17:15.890
<v Speaker 2>Right?</v>
<v Speaker 2>In addition to design,</v>

252
00:17:15.891 --> 00:17:20.891
<v Speaker 2>tangling the,</v>
<v Speaker 2>the elements of the representation which</v>

253
00:17:21.171 --> 00:17:23.630
<v Speaker 2>are like the variables in a rule based </v>
<v Speaker 2>system,</v>

254
00:17:24.230 --> 00:17:29.230
<v Speaker 2>you also need to disentangle the,</v>
<v Speaker 2>the mechanisms that relate those </v>

255
00:17:31.911 --> 00:17:34.370
<v Speaker 2>variables to each other.</v>
<v Speaker 2>So like the rules.</v>

256
00:17:34.400 --> 00:17:36.230
<v Speaker 2>So the rules are neatly separated.</v>
<v Speaker 2>And I,</v>

257
00:17:36.290 --> 00:17:37.510
<v Speaker 2>each rule is,</v>
<v Speaker 2>you know,</v>

258
00:17:37.570 --> 00:17:42.360
<v Speaker 2>living on his own and when I,</v>
<v Speaker 2>I changed a rule because I'm learning a,</v>

259
00:17:42.540 --> 00:17:46.790
<v Speaker 2>it doesn't need to break other roles,</v>
<v Speaker 2>whereas current neural nets for example,</v>

260
00:17:46.791 --> 00:17:49.950
<v Speaker 2>are very sensitive to what's called </v>
<v Speaker 2>catastrophic forgetting,</v>

261
00:17:49.951 --> 00:17:51.050
<v Speaker 2>where,</v>
<v Speaker 2>uh,</v>

262
00:17:51.060 --> 00:17:53.970
<v Speaker 2>after I've learned some things and then </v>
<v Speaker 2>they're learn new things,</v>

263
00:17:54.160 --> 00:17:56.100
<v Speaker 2>they can destroy the old things that I </v>
<v Speaker 2>had learned,</v>

264
00:17:56.160 --> 00:18:01.160
<v Speaker 2>right?</v>
<v Speaker 2>If the knowledge was better factorized </v>

265
00:18:01.160 --> 00:18:03.180
<v Speaker 2>and separated,</v>
<v Speaker 2>disentangled,</v>

266
00:18:03.510 --> 00:18:08.510
<v Speaker 2>then you would avoid a lot of that.</v>
<v Speaker 2>Now you can't do this in the sensory </v>

267
00:18:09.391 --> 00:18:13.140
<v Speaker 2>domain,</v>
<v Speaker 2>but what do you mean?</v>

268
00:18:13.200 --> 00:18:15.270
<v Speaker 2>Like in Pixel space.</v>
<v Speaker 2>But,</v>

269
00:18:15.271 --> 00:18:18.780
<v Speaker 2>but my idea is that when you project the</v>
<v Speaker 2>data and the right semantic space,</v>

270
00:18:18.781 --> 00:18:23.781
<v Speaker 2>it becomes possible to now represent </v>
<v Speaker 2>this extra knowledge beyond the </v>

271
00:18:24.151 --> 00:18:26.060
<v Speaker 2>transformation from input to </v>
<v Speaker 2>representations,</v>

272
00:18:26.130 --> 00:18:30.120
<v Speaker 2>which is how representations act on each</v>
<v Speaker 2>other and predict the future and so on,</v>

273
00:18:30.820 --> 00:18:35.520
<v Speaker 2>in a way that can be neatly </v>
<v Speaker 2>disentangled.</v>

274
00:18:35.550 --> 00:18:40.550
<v Speaker 2>So now it's the rules that are </v>
<v Speaker 2>disentangled from each other and not </v>

275
00:18:40.550 --> 00:18:40.590
<v Speaker 2>just the variables that are disentangled</v>
<v Speaker 2>from each other.</v>

276
00:18:41.190 --> 00:18:46.190
<v Speaker 2>And you draw a distinction between </v>
<v Speaker 2>semantic space and pixel needs to be an </v>

277
00:18:46.190 --> 00:18:47.790
<v Speaker 2>architectural difference or.</v>
<v Speaker 2>Well,</v>

278
00:18:47.791 --> 00:18:48.180
<v Speaker 2>yeah.</v>
<v Speaker 2>So,</v>

279
00:18:48.181 --> 00:18:50.280
<v Speaker 2>so there's the sensory space like </v>
<v Speaker 2>pixels,</v>

280
00:18:50.310 --> 00:18:52.920
<v Speaker 2>which where everything is entangled,</v>
<v Speaker 2>the,</v>

281
00:18:53.150 --> 00:18:58.150
<v Speaker 2>the information like the variables are </v>
<v Speaker 2>completely interdependent in very </v>

282
00:18:58.150 --> 00:19:01.010
<v Speaker 2>complicated ways and also computation </v>
<v Speaker 2>like the,</v>

283
00:19:01.890 --> 00:19:06.890
<v Speaker 2>it's not just variables,</v>
<v Speaker 2>it's also how they are related to each </v>

284
00:19:06.890 --> 00:19:07.050
<v Speaker 2>other as is all intertwined.</v>
<v Speaker 2>But,</v>

285
00:19:07.110 --> 00:19:10.500
<v Speaker 2>but I,</v>
<v Speaker 2>I'm hypothesizing that in the right high</v>

286
00:19:10.501 --> 00:19:15.501
<v Speaker 2>level representation space,</v>
<v Speaker 2>both the variables and how they relate </v>

287
00:19:16.081 --> 00:19:21.081
<v Speaker 2>to each other can be disentangled and </v>
<v Speaker 2>that will provide a lot of </v>

288
00:19:21.081 --> 00:19:21.081
<v Speaker 2>generalization.</v>

289
00:19:21.081 --> 00:19:22.890
<v Speaker 2>Power.</v>
<v Speaker 2>Generalization power.</v>

290
00:19:23.010 --> 00:19:28.010
<v Speaker 2>Yes.</v>
<v Speaker 2>A distribution of the test set you </v>

291
00:19:28.010 --> 00:19:30.120
<v Speaker 2>assume to be the same as the </v>
<v Speaker 2>distribution of the training set.</v>

292
00:19:30.180 --> 00:19:35.180
<v Speaker 2>Right.</v>
<v Speaker 2>This is where current machine learning </v>

293
00:19:35.180 --> 00:19:38.090
<v Speaker 2>is too weak.</v>
<v Speaker 2>It doesn't tell us anything is not able </v>

294
00:19:38.090 --> 00:19:42.261
<v Speaker 2>to tell us anything about how our neural</v>
<v Speaker 2>nets say are going to generalize to a </v>

295
00:19:42.261 --> 00:19:43.680
<v Speaker 2>new distribution.</v>
<v Speaker 2>And,</v>

296
00:19:43.740 --> 00:19:44.910
<v Speaker 2>and you know,</v>
<v Speaker 2>people may think,</v>

297
00:19:44.911 --> 00:19:49.911
<v Speaker 2>well,</v>
<v Speaker 2>but there's nothing we can say if we </v>

298
00:19:49.911 --> 00:19:49.911
<v Speaker 2>don't know what the new distribution </v>
<v Speaker 2>will be.</v>

299
00:19:49.911 --> 00:19:53.900
<v Speaker 2>The truth is humans are able to </v>
<v Speaker 2>generalize to new distributions how </v>

300
00:19:54.001 --> 00:19:56.460
<v Speaker 2>we're able to do that.</v>
<v Speaker 2>So because there is something,</v>

301
00:19:56.461 --> 00:20:01.461
<v Speaker 2>these new distributions,</v>
<v Speaker 2>even though they could look very </v>

302
00:20:01.461 --> 00:20:02.520
<v Speaker 2>different from the distributions,</v>
<v Speaker 2>they have things in common.</v>

303
00:20:02.521 --> 00:20:05.940
<v Speaker 2>So let me give you a concrete example.</v>
<v Speaker 2>You read a science fiction novel,</v>

304
00:20:06.340 --> 00:20:09.120
<v Speaker 2>the science fiction novel maybe you </v>
<v Speaker 2>know,</v>

305
00:20:09.810 --> 00:20:14.810
<v Speaker 2>brings you in some other planet where </v>
<v Speaker 2>things look very different on the </v>

306
00:20:15.331 --> 00:20:17.970
<v Speaker 2>surface,</v>
<v Speaker 2>but it's still the same laws of physics.</v>

307
00:20:18.740 --> 00:20:23.740
<v Speaker 2>Right?</v>
<v Speaker 2>And so you can read the book and You </v>

308
00:20:23.740 --> 00:20:24.780
<v Speaker 2>understand what's going on.</v>
<v Speaker 2>So the distribution is very different,</v>

309
00:20:25.790 --> 00:20:30.790
<v Speaker 2>but because you can transport a lot of </v>
<v Speaker 2>the knowledge you had from Earth about </v>

310
00:20:30.790 --> 00:20:35.250
<v Speaker 2>the underlying cause and effect </v>
<v Speaker 2>relationships and physical encounters,</v>

311
00:20:35.290 --> 00:20:38.220
<v Speaker 2>isms and all that,</v>
<v Speaker 2>and maybe even social interactions,</v>

312
00:20:39.020 --> 00:20:44.020
<v Speaker 2>you can now make sense of what is going </v>
<v Speaker 2>on on this planet where like visually </v>

313
00:20:44.020 --> 00:20:44.020
<v Speaker 2>for example,</v>
<v Speaker 2>things are totally different.</v>

314
00:20:46.080 --> 00:20:48.010
<v Speaker 2>Taking the analogy and distorting</v>

315
00:20:48.010 --> 00:20:50.040
<v Speaker 1>it.</v>
<v Speaker 1>That's enter a science,</v>

316
00:20:50.250 --> 00:20:54.640
<v Speaker 1>science fiction world of say a space </v>
<v Speaker 1>odyssey 2001 with how?</v>

317
00:20:54.740 --> 00:20:55.980
<v Speaker 1>Yeah,</v>
<v Speaker 1>or,</v>

318
00:20:56.130 --> 00:20:57.430
<v Speaker 1>or maybe,</v>
<v Speaker 1>uh,</v>

319
00:20:57.640 --> 00:21:01.690
<v Speaker 1>which is probably one of my favorite ai </v>
<v Speaker 1>movie's and then to it.</v>

320
00:21:02.810 --> 00:21:04.940
<v Speaker 1>And then there's another one that a lot </v>
<v Speaker 1>of people love that.</v>

321
00:21:04.941 --> 00:21:09.700
<v Speaker 1>It may be a little bit outside of the Ai</v>
<v Speaker 1>Community is x Mokena.</v>

322
00:21:09.940 --> 00:21:12.820
<v Speaker 1>I don't know if you've seen it in a </v>
<v Speaker 1>bottle.</v>

323
00:21:12.860 --> 00:21:15.130
<v Speaker 1>What are your views on that movie?</v>
<v Speaker 1>Does it,</v>

324
00:21:15.760 --> 00:21:20.760
<v Speaker 1>does,</v>
<v Speaker 1>are you able to enjoy the things I like </v>

325
00:21:20.760 --> 00:21:23.881
<v Speaker 1>and things I hate.</v>
<v Speaker 1>So maybe you could talk about that in </v>

326
00:21:23.881 --> 00:21:26.680
<v Speaker 1>the context of a question I want to ask,</v>
<v Speaker 1>which is a,</v>

327
00:21:26.740 --> 00:21:31.740
<v Speaker 1>there's quite a large community of </v>
<v Speaker 1>people from different backgrounds often </v>

328
00:21:31.740 --> 00:21:35.761
<v Speaker 1>outside of ai who are concerned about </v>
<v Speaker 1>existential threat of artificial </v>

329
00:21:35.761 --> 00:21:35.761
<v Speaker 1>intelligence.</v>
<v Speaker 1>Right?</v>

330
00:21:35.761 --> 00:21:38.600
<v Speaker 1>And you've seen this community develop </v>
<v Speaker 1>overtime,</v>

331
00:21:38.710 --> 00:21:40.540
<v Speaker 1>you've seen you have a perspective.</v>
<v Speaker 1>So,</v>

332
00:21:40.541 --> 00:21:41.470
<v Speaker 1>uh,</v>
<v Speaker 1>what,</v>

333
00:21:41.490 --> 00:21:44.140
<v Speaker 1>what do you think is the best way to </v>
<v Speaker 1>talk about Ai Safety?</v>

334
00:21:44.470 --> 00:21:49.470
<v Speaker 1>To think about it,</v>
<v Speaker 1>to have discourse about it within ai </v>

335
00:21:49.470 --> 00:21:52.741
<v Speaker 1>community and outside and grounded in </v>
<v Speaker 1>the fact that x Mokena is one of the </v>

336
00:21:53.231 --> 00:21:56.410
<v Speaker 1>main sources of information for the </v>
<v Speaker 1>general public about Ai.</v>

337
00:21:56.670 --> 00:21:58.500
<v Speaker 2>So I think,</v>
<v Speaker 2>I think you're putting it right.</v>

338
00:21:58.590 --> 00:22:03.590
<v Speaker 2>There is a big difference between the </v>
<v Speaker 2>sort of discussion we outta have within </v>

339
00:22:03.601 --> 00:22:08.601
<v Speaker 2>the community and the sort of discussion</v>
<v Speaker 2>that really matter in the general </v>

340
00:22:08.601 --> 00:22:12.870
<v Speaker 2>public.</v>
<v Speaker 2>So I think the picture of terminator and</v>

341
00:22:13.200 --> 00:22:18.200
<v Speaker 2>Ai Loose and killing people and </v>
<v Speaker 2>superintelligence that's going to </v>

342
00:22:19.201 --> 00:22:24.201
<v Speaker 2>destroy us.</v>
<v Speaker 2>Whatever we try isn't really so useful </v>

343
00:22:24.451 --> 00:22:27.170
<v Speaker 2>for the public discussion because,</v>
<v Speaker 2>uh,</v>

344
00:22:27.530 --> 00:22:32.530
<v Speaker 2>for the public discussion,</v>
<v Speaker 2>the things I believe really matter are </v>

345
00:22:32.530 --> 00:22:37.220
<v Speaker 2>the short term and medium term,</v>
<v Speaker 2>very likely negative impacts of ai on </v>

346
00:22:37.220 --> 00:22:40.710
<v Speaker 2>society,</v>
<v Speaker 2>whether it's from a security,</v>

347
00:22:40.711 --> 00:22:45.711
<v Speaker 2>like big brother's scenarios with face </v>
<v Speaker 2>recognition or killer robots or the </v>

348
00:22:45.711 --> 00:22:50.331
<v Speaker 2>impact on the job market or a </v>
<v Speaker 2>concentration of power and </v>

349
00:22:50.331 --> 00:22:54.351
<v Speaker 2>discrimination,</v>
<v Speaker 2>all kinds of social issues which could </v>

350
00:22:54.351 --> 00:22:58.071
<v Speaker 2>actually,</v>
<v Speaker 2>some of them could really threaten </v>

351
00:22:58.071 --> 00:22:58.470
<v Speaker 2>democracy.</v>
<v Speaker 2>For example,</v>

352
00:22:58.880 --> 00:23:01.190
<v Speaker 1>just to clarify,</v>
<v Speaker 1>when you said killer robots,</v>

353
00:23:01.191 --> 00:23:04.110
<v Speaker 1>you mean autonomous weapon weapon </v>
<v Speaker 1>system,</v>

354
00:23:05.300 --> 00:23:06.470
<v Speaker 1>terminator.</v>
<v Speaker 1>That's right.</v>

355
00:23:07.480 --> 00:23:07.750
<v Speaker 1>I think</v>

356
00:23:07.750 --> 00:23:08.550
<v Speaker 2>these,</v>
<v Speaker 2>these,</v>

357
00:23:08.551 --> 00:23:12.070
<v Speaker 2>uh,</v>
<v Speaker 2>short and medium term concerns should be</v>

358
00:23:12.130 --> 00:23:14.350
<v Speaker 2>important parts of the public debate.</v>
<v Speaker 2>Now,</v>

359
00:23:14.410 --> 00:23:19.410
<v Speaker 2>existential risk for me is a very </v>
<v Speaker 2>unlikely consideration,</v>

360
00:23:20.230 --> 00:23:25.230
<v Speaker 2>but it's still worth a academic </v>
<v Speaker 2>investigation and the same way that you </v>

361
00:23:26.321 --> 00:23:29.820
<v Speaker 2>could say,</v>
<v Speaker 2>should we study what could happen if I'm</v>

362
00:23:29.840 --> 00:23:30.870
<v Speaker 2>motorized,</v>
<v Speaker 2>you know,</v>

363
00:23:31.060 --> 00:23:36.060
<v Speaker 2>came to earth and destroyed it.</v>
<v Speaker 2>So I think it's very unlikely that this </v>

364
00:23:36.060 --> 00:23:37.780
<v Speaker 2>is going to happen in or happening in a </v>
<v Speaker 2>reasonable future.</v>

365
00:23:37.781 --> 00:23:39.190
<v Speaker 2>It's,</v>
<v Speaker 2>it's very,</v>

366
00:23:40.240 --> 00:23:40.600
<v Speaker 2>um,</v>
<v Speaker 2>the,</v>

367
00:23:40.620 --> 00:23:45.620
<v Speaker 2>the sort of scenario of an ai getting </v>
<v Speaker 2>loose goes against my understanding it </v>

368
00:23:45.620 --> 00:23:47.930
<v Speaker 2>police current machine learning and </v>
<v Speaker 2>current neural nets and so on,</v>

369
00:23:47.940 --> 00:23:52.940
<v Speaker 2>and it's not plausible to me,</v>
<v Speaker 2>but of course I don't have a crystal </v>

370
00:23:52.940 --> 00:23:54.320
<v Speaker 2>ball and who knows what ai will be in 50</v>
<v Speaker 2>years from now.</v>

371
00:23:54.350 --> 00:23:57.200
<v Speaker 2>So I think it is worth that.</v>
<v Speaker 2>Scientists study those problems,</v>

372
00:23:57.650 --> 00:24:00.350
<v Speaker 2>it's just not a pressing question as far</v>
<v Speaker 2>as I'm concerned.</v>

373
00:24:01.630 --> 00:24:04.780
<v Speaker 1>So before continuing down that line,</v>
<v Speaker 1>have a few questions there.</v>

374
00:24:04.781 --> 00:24:06.600
<v Speaker 1>But what,</v>
<v Speaker 1>what,</v>

375
00:24:06.610 --> 00:24:07.180
<v Speaker 1>what,</v>
<v Speaker 1>uh,</v>

376
00:24:07.181 --> 00:24:10.390
<v Speaker 1>what do you like and not like about x </v>
<v Speaker 1>Mokena as a movie because I,</v>

377
00:24:10.600 --> 00:24:13.090
<v Speaker 1>I actually watched it for the second </v>
<v Speaker 1>time and enjoyed it.</v>

378
00:24:13.660 --> 00:24:18.430
<v Speaker 1>I hated it the first time and I enjoyed </v>
<v Speaker 1>it quite a bit more the second time when</v>

379
00:24:18.431 --> 00:24:23.431
<v Speaker 1>I sort of learned to accept a certain </v>
<v Speaker 1>pieces of it as a concept movie.</v>

380
00:24:25.721 --> 00:24:27.040
<v Speaker 1>Hi,</v>
<v Speaker 1>what was your experience?</v>

381
00:24:27.110 --> 00:24:28.270
<v Speaker 1>What are your thoughts?</v>

382
00:24:29.200 --> 00:24:34.200
<v Speaker 2>So the negative is the picture it paints</v>
<v Speaker 2>of science is totally wrong.</v>

383
00:24:38.050 --> 00:24:40.150
<v Speaker 2>Science in general,</v>
<v Speaker 2>and ai in particular,</v>

384
00:24:40.600 --> 00:24:43.180
<v Speaker 2>science is not happening,</v>
<v Speaker 2>uh,</v>

385
00:24:43.210 --> 00:24:44.350
<v Speaker 2>in some,</v>
<v Speaker 2>uh,</v>

386
00:24:44.351 --> 00:24:46.590
<v Speaker 2>hidden place by some,</v>
<v Speaker 2>you know,</v>

387
00:24:46.720 --> 00:24:49.390
<v Speaker 2>really smart guy,</v>
<v Speaker 2>one person,</v>

388
00:24:49.420 --> 00:24:51.970
<v Speaker 2>one person.</v>
<v Speaker 2>This is totally unrealistic.</v>

389
00:24:51.971 --> 00:24:56.971
<v Speaker 2>This is not how it happens.</v>
<v Speaker 2>Even a team of people in some isolated </v>

390
00:24:57.221 --> 00:25:01.720
<v Speaker 2>place will not make it.</v>
<v Speaker 2>Science moves by small steps,</v>

391
00:25:01.750 --> 00:25:06.750
<v Speaker 2>thanks to the collaboration and um,</v>
<v Speaker 2>community have a large number of people </v>

392
00:25:08.680 --> 00:25:13.680
<v Speaker 2>interacting and um,</v>
<v Speaker 2>all the scientists who are experts in </v>

393
00:25:14.801 --> 00:25:16.540
<v Speaker 2>their field kind of know what is going </v>
<v Speaker 2>on.</v>

394
00:25:16.570 --> 00:25:21.570
<v Speaker 2>Even in the industrial labs.</v>
<v Speaker 2>It's information flows and leaks and so </v>

395
00:25:21.731 --> 00:25:22.570
<v Speaker 2>on and,</v>
<v Speaker 2>and,</v>

396
00:25:22.720 --> 00:25:27.720
<v Speaker 2>um,</v>
<v Speaker 2>and the spirit of it is very different </v>

397
00:25:27.720 --> 00:25:28.510
<v Speaker 2>from the way science is painted in this </v>
<v Speaker 2>movie.</v>

398
00:25:28.760 --> 00:25:29.180
<v Speaker 1>Yeah.</v>
<v Speaker 1>Let me,</v>

399
00:25:29.210 --> 00:25:30.800
<v Speaker 1>let me ask on that.</v>
<v Speaker 1>On that point,</v>

400
00:25:31.490 --> 00:25:34.970
<v Speaker 1>it's been the case to this point that </v>
<v Speaker 1>kind of,</v>

401
00:25:34.971 --> 00:25:38.120
<v Speaker 1>even if the research happens inside </v>
<v Speaker 1>Google and facebook inside companies,</v>

402
00:25:38.121 --> 00:25:40.480
<v Speaker 1>it's still kind of comes out.</v>
<v Speaker 1>Isaiah's come out.</v>

403
00:25:40.740 --> 00:25:42.640
<v Speaker 1>Absolutely.</v>
<v Speaker 1>Think there will always be the case with</v>

404
00:25:42.641 --> 00:25:47.641
<v Speaker 1>ai is,</v>
<v Speaker 1>is it possible to bottle ideas to the </v>

405
00:25:47.641 --> 00:25:50.411
<v Speaker 1>point where there's a set of </v>
<v Speaker 1>breakthrough to go completely </v>

406
00:25:50.411 --> 00:25:52.430
<v Speaker 1>undiscovered by the general research </v>
<v Speaker 1>community?</v>

407
00:25:52.460 --> 00:25:53.720
<v Speaker 1>Do you think that's even possible?</v>

408
00:25:54.980 --> 00:25:56.780
<v Speaker 2>It's possible,</v>
<v Speaker 2>but it's unlikely.</v>

409
00:25:56.781 --> 00:26:00.410
<v Speaker 2>Unlikely.</v>
<v Speaker 2>It's not how it is done now.</v>

410
00:26:00.830 --> 00:26:04.490
<v Speaker 2>It's not how I can foresee it in the </v>
<v Speaker 2>foreseeable future,</v>

411
00:26:05.630 --> 00:26:10.340
<v Speaker 2>but of course I don't have a crystal </v>
<v Speaker 2>ball.</v>

412
00:26:10.341 --> 00:26:14.430
<v Speaker 2>And so who knows?</v>
<v Speaker 2>This is science fiction after all,</v>

413
00:26:15.230 --> 00:26:17.330
<v Speaker 2>but,</v>
<v Speaker 2>but usually it's ominous that the lights</v>

414
00:26:17.331 --> 00:26:19.250
<v Speaker 2>went off during,</v>
<v Speaker 2>during that discussion.</v>

415
00:26:21.290 --> 00:26:23.080
<v Speaker 2>So the problem again,</v>
<v Speaker 2>there's a,</v>

416
00:26:23.110 --> 00:26:28.110
<v Speaker 2>you know,</v>
<v Speaker 2>one thing is the movie and you could </v>

417
00:26:28.110 --> 00:26:28.110
<v Speaker 2>imagine all kinds of science fiction.</v>
<v Speaker 2>The problem wouldn't for me,</v>

418
00:26:28.110 --> 00:26:33.040
<v Speaker 2>maybe similar to the question about </v>
<v Speaker 2>existential risk is that this kind of </v>

419
00:26:33.171 --> 00:26:38.171
<v Speaker 2>movie paints such a picture of what his </v>
<v Speaker 2>actual,</v>

420
00:26:38.320 --> 00:26:40.610
<v Speaker 2>you know,</v>
<v Speaker 2>the actual science and how it's going on</v>

421
00:26:40.611 --> 00:26:44.250
<v Speaker 2>that,</v>
<v Speaker 2>that it can have unfortunate on people's</v>

422
00:26:44.251 --> 00:26:49.251
<v Speaker 2>understanding of current science.</v>
<v Speaker 2>And so that's kind of sad is an </v>

423
00:26:51.151 --> 00:26:56.070
<v Speaker 2>important principle in research which is</v>
<v Speaker 2>diversity.</v>

424
00:26:56.100 --> 00:26:59.680
<v Speaker 2>So in other words,</v>
<v Speaker 2>research is exploration resources,</v>

425
00:26:59.790 --> 00:27:04.500
<v Speaker 2>expression in the space of ideas and </v>
<v Speaker 2>different people will focus on different</v>

426
00:27:04.501 --> 00:27:06.960
<v Speaker 2>directions and this is not just good.</v>

427
00:27:07.080 --> 00:27:12.080
<v Speaker 2>It's essential.</v>
<v Speaker 2>So I'm totally fine with people </v>

428
00:27:12.080 --> 00:27:16.750
<v Speaker 2>exploring directions that are contrary </v>
<v Speaker 2>to mine or look orthogonal into mine.</v>

429
00:27:18.640 --> 00:27:21.150
<v Speaker 2>I am more than fine.</v>
<v Speaker 2>I think it's important.</v>

430
00:27:21.900 --> 00:27:26.130
<v Speaker 2>I and my friends don't claim we have </v>
<v Speaker 2>universal truth about what will,</v>

431
00:27:26.160 --> 00:27:27.960
<v Speaker 2>especially about what will happen in the</v>
<v Speaker 2>future.</v>

432
00:27:28.830 --> 00:27:33.830
<v Speaker 2>Now that being said,</v>
<v Speaker 2>we have our intuitions and then we act </v>

433
00:27:33.830 --> 00:27:37.821
<v Speaker 2>accordingly according to where we think </v>
<v Speaker 2>we can be most useful and where society </v>

434
00:27:38.611 --> 00:27:43.611
<v Speaker 2>has the most to gain or to lose.</v>
<v Speaker 2>We should have those debates and um,</v>

435
00:27:44.910 --> 00:27:45.850
<v Speaker 2>and,</v>
<v Speaker 2>and,</v>

436
00:27:45.851 --> 00:27:50.851
<v Speaker 2>and not end up in a society where </v>
<v Speaker 2>there's only one voice and one way of </v>

437
00:27:50.851 --> 00:27:53.190
<v Speaker 2>thinking and research money is spread </v>
<v Speaker 2>out.</v>

438
00:27:53.370 --> 00:27:58.370
<v Speaker 2>So the disagreement is a is a sign of a </v>
<v Speaker 2>good research,</v>

439
00:27:58.441 --> 00:27:59.580
<v Speaker 2>good size.</v>
<v Speaker 2>So yes,</v>

440
00:28:01.370 --> 00:28:03.980
<v Speaker 1>the idea of bias in the human sense of </v>
<v Speaker 1>bias.</v>

441
00:28:04.030 --> 00:28:05.540
<v Speaker 2>Yeah.</v>
<v Speaker 2>Uh,</v>

442
00:28:05.680 --> 00:28:06.700
<v Speaker 2>how do you think about</v>

443
00:28:07.570 --> 00:28:12.570
<v Speaker 1>instilling in machine learning something</v>
<v Speaker 1>that's aligned with human values in </v>

444
00:28:12.941 --> 00:28:15.260
<v Speaker 1>terms of bias?</v>
<v Speaker 1>We intuitively,</v>

445
00:28:15.261 --> 00:28:19.630
<v Speaker 1>as human beings have a concept of what </v>
<v Speaker 1>bias means of what a fundamental respect</v>

446
00:28:19.960 --> 00:28:24.960
<v Speaker 1>for other human beings means.</v>
<v Speaker 1>But how do we instill that into machine </v>

447
00:28:24.960 --> 00:28:25.420
<v Speaker 1>learning systems,</v>
<v Speaker 1>do you think?</v>

448
00:28:26.620 --> 00:28:31.620
<v Speaker 2>So,</v>
<v Speaker 2>I think they're short term things that </v>

449
00:28:31.620 --> 00:28:35.340
<v Speaker 2>are already happening.</v>
<v Speaker 2>And then there are longterm things that </v>

450
00:28:35.340 --> 00:28:38.311
<v Speaker 2>we need to do in the short term.</v>
<v Speaker 2>There are techniques that have been </v>

451
00:28:38.411 --> 00:28:43.411
<v Speaker 2>proposed and I think we'll continue to </v>
<v Speaker 2>be improved and maybe alternative will </v>

452
00:28:43.411 --> 00:28:47.161
<v Speaker 2>come up to take data sets in which we </v>
<v Speaker 2>know there is bias,</v>

453
00:28:47.230 --> 00:28:52.230
<v Speaker 2>we can measure it pretty much any </v>
<v Speaker 2>dataset we're humans or being observed </v>

454
00:28:52.230 --> 00:28:57.211
<v Speaker 2>taking decisions will have some sort of </v>
<v Speaker 2>bias or discrimination against </v>

455
00:28:57.211 --> 00:29:00.841
<v Speaker 2>particular groups and so on.</v>
<v Speaker 2>And we can use machine learning </v>

456
00:29:00.841 --> 00:29:05.461
<v Speaker 2>techniques to try to build predictor is </v>
<v Speaker 2>classifiers that are going to be less </v>

457
00:29:05.981 --> 00:29:07.760
<v Speaker 2>biased.</v>
<v Speaker 2>Uh,</v>

458
00:29:08.080 --> 00:29:09.490
<v Speaker 2>we can do it,</v>
<v Speaker 2>for example,</v>

459
00:29:09.491 --> 00:29:14.491
<v Speaker 2>using adversarial methods to make our </v>
<v Speaker 2>systems less sensitive to these </v>

460
00:29:15.520 --> 00:29:19.600
<v Speaker 2>variables we should not be sensitive to.</v>
<v Speaker 2>So these are clear,</v>

461
00:29:19.601 --> 00:29:22.210
<v Speaker 2>well defined ways of trying to address </v>
<v Speaker 2>the problem.</v>

462
00:29:22.211 --> 00:29:25.570
<v Speaker 2>Maybe they have weaknesses and you know,</v>
<v Speaker 2>more research is needed and so on.</v>

463
00:29:25.570 --> 00:29:30.570
<v Speaker 2>But I think in fact there are </v>
<v Speaker 2>sufficiently mature that governments </v>

464
00:29:30.570 --> 00:29:33.400
<v Speaker 2>should start regulating companies where </v>
<v Speaker 2>it matters,</v>

465
00:29:33.401 --> 00:29:38.401
<v Speaker 2>say like insurance companies so that </v>
<v Speaker 2>they use those techniques because those </v>

466
00:29:38.401 --> 00:29:41.140
<v Speaker 2>techniques will probably reduce the </v>
<v Speaker 2>bias.</v>

467
00:29:41.890 --> 00:29:43.660
<v Speaker 2>But at a cost,</v>
<v Speaker 2>for example,</v>

468
00:29:43.661 --> 00:29:45.550
<v Speaker 2>maybe their predictions will be less </v>
<v Speaker 2>accurate.</v>

469
00:29:45.820 --> 00:29:48.040
<v Speaker 2>And so companies will not do it until </v>
<v Speaker 2>you force them.</v>

470
00:29:49.000 --> 00:29:51.580
<v Speaker 2>Alright,</v>
<v Speaker 2>so this is short term longterm.</v>

471
00:29:51.670 --> 00:29:56.670
<v Speaker 2>I'm really interested in thinking about </v>
<v Speaker 2>how we can instill moral values into </v>

472
00:29:58.691 --> 00:29:59.990
<v Speaker 2>computers.</v>
<v Speaker 2>Obviously this is not,</v>

473
00:29:59.991 --> 00:30:04.991
<v Speaker 2>uh,</v>
<v Speaker 2>something will achieve in the next five </v>

474
00:30:04.991 --> 00:30:05.200
<v Speaker 2>or 10 years.</v>
<v Speaker 2>How can we,</v>

475
00:30:05.201 --> 00:30:10.201
<v Speaker 2>you know,</v>
<v Speaker 2>there's already work and detecting </v>

476
00:30:10.201 --> 00:30:10.201
<v Speaker 2>emotions,</v>
<v Speaker 2>for example,</v>

477
00:30:10.201 --> 00:30:14.220
<v Speaker 2>in images and sounds and texts and also </v>
<v Speaker 2>studying how different agents </v>

478
00:30:18.071 --> 00:30:23.071
<v Speaker 2>interacting in different ways may </v>
<v Speaker 2>correspond to patterns of say injustice,</v>

479
00:30:26.080 --> 00:30:31.080
<v Speaker 2>which could trigger anger.</v>
<v Speaker 2>So these are things we can do in the </v>

480
00:30:31.080 --> 00:30:35.101
<v Speaker 2>medium term and eventually train </v>
<v Speaker 2>computers to model,</v>

481
00:30:37.720 --> 00:30:41.260
<v Speaker 2>for example,</v>
<v Speaker 2>how humans react emotionally.</v>

482
00:30:41.330 --> 00:30:45.100
<v Speaker 2>Uh,</v>
<v Speaker 2>I would say the simplest thing is unfair</v>

483
00:30:45.850 --> 00:30:50.850
<v Speaker 2>situations which trigger anger.</v>
<v Speaker 2>This is one of the most basic emotions </v>

484
00:30:50.850 --> 00:30:55.501
<v Speaker 2>that we share with other animals.</v>
<v Speaker 2>I think it's quite feasible within the </v>

485
00:30:55.501 --> 00:31:00.360
<v Speaker 2>next few years.</v>
<v Speaker 2>So we can build systems that can detect </v>

486
00:31:00.360 --> 00:31:00.640
<v Speaker 2>these kinds of things to the extent </v>
<v Speaker 2>unfortunately,</v>

487
00:31:00.641 --> 00:31:04.240
<v Speaker 2>that they understand enough about the </v>
<v Speaker 2>world around us,</v>

488
00:31:04.241 --> 00:31:09.241
<v Speaker 2>which is a long time away.</v>
<v Speaker 2>But maybe we can initially do this in </v>

489
00:31:09.280 --> 00:31:14.280
<v Speaker 2>virtual environments.</v>
<v Speaker 2>So you can imagine like a video game </v>

490
00:31:14.280 --> 00:31:18.000
<v Speaker 2>where agents interact in,</v>
<v Speaker 2>in some ways and then some situations </v>

491
00:31:18.000 --> 00:31:19.210
<v Speaker 2>trigger an emotion.</v>
<v Speaker 2>Uh,</v>

492
00:31:19.270 --> 00:31:23.260
<v Speaker 2>I think we could train machines to </v>
<v Speaker 2>detect those situations and predict that</v>

493
00:31:23.300 --> 00:31:24.380
<v Speaker 2>particular emotion,</v>
<v Speaker 2>you know,</v>

494
00:31:24.390 --> 00:31:28.810
<v Speaker 2>will likely be felt if a human was </v>
<v Speaker 2>playing one of the characters.</v>

495
00:31:29.260 --> 00:31:33.210
<v Speaker 1>You have shown excitement and done a lot</v>
<v Speaker 1>of excellent work with,</v>

496
00:31:33.220 --> 00:31:34.570
<v Speaker 1>uh,</v>
<v Speaker 1>on supervised learning,</v>

497
00:31:35.500 --> 00:31:36.870
<v Speaker 1>but on a supervisor,</v>
<v Speaker 1>you know,</v>

498
00:31:36.880 --> 00:31:39.550
<v Speaker 1>there's been a lot of success on the </v>
<v Speaker 1>supervised learning.</v>

499
00:31:39.551 --> 00:31:40.580
<v Speaker 1>So yes.</v>
<v Speaker 1>Yes.</v>

500
00:31:40.920 --> 00:31:45.920
<v Speaker 1>And one of the things I'm really </v>
<v Speaker 1>passionate about is how humans and </v>

501
00:31:45.920 --> 00:31:47.260
<v Speaker 1>robots work together.</v>
<v Speaker 1>And uh,</v>

502
00:31:47.530 --> 00:31:51.490
<v Speaker 1>in the context of supervised learning,</v>
<v Speaker 1>that means the process of annotation.</v>

503
00:31:52.210 --> 00:31:57.210
<v Speaker 1>Do you think about the problem of </v>
<v Speaker 1>annotation of put in a more interesting </v>

504
00:31:58.051 --> 00:31:59.240
<v Speaker 1>way humans</v>

505
00:31:59.430 --> 00:32:02.730
<v Speaker 2>teaching machines is there?</v>
<v Speaker 2>Yes,</v>

506
00:32:02.790 --> 00:32:06.450
<v Speaker 2>I think it's an important subject </v>
<v Speaker 2>reducing it to annotation,</v>

507
00:32:06.480 --> 00:32:11.160
<v Speaker 2>maybe useful for somebody building a </v>
<v Speaker 2>system tomorrow,</v>

508
00:32:11.161 --> 00:32:16.161
<v Speaker 2>but longer term the process of teaching,</v>
<v Speaker 2>I think it's something that deserves a </v>

509
00:32:16.981 --> 00:32:19.320
<v Speaker 2>lot more attention from the machine </v>
<v Speaker 2>learning community.</v>

510
00:32:19.321 --> 00:32:24.321
<v Speaker 2>So there are,</v>
<v Speaker 2>people have coined the term machine </v>

511
00:32:24.321 --> 00:32:24.510
<v Speaker 2>teaching.</v>
<v Speaker 2>So what are good strategies for teaching</v>

512
00:32:24.540 --> 00:32:27.570
<v Speaker 2>a learning agent?</v>
<v Speaker 2>And can we,</v>

513
00:32:28.100 --> 00:32:29.350
<v Speaker 2>uh,</v>
<v Speaker 2>design,</v>

514
00:32:29.430 --> 00:32:32.100
<v Speaker 2>train a system that's going to be,</v>
<v Speaker 2>it's going to be a good teacher.</v>

515
00:32:32.550 --> 00:32:37.550
<v Speaker 2>So,</v>
<v Speaker 2>so in my group we have a project called </v>

516
00:32:37.550 --> 00:32:41.061
<v Speaker 2>a bbi or baby I game where there is a,</v>
<v Speaker 2>a game at or scenario where there's a,</v>

517
00:32:43.321 --> 00:32:48.321
<v Speaker 2>a learning agent and a teaching agent,</v>
<v Speaker 2>presumably the teaching agent would </v>

518
00:32:48.500 --> 00:32:51.890
<v Speaker 2>eventually be a human,</v>
<v Speaker 2>but we're not there yet.</v>

519
00:32:52.430 --> 00:32:54.830
<v Speaker 2>Um,</v>
<v Speaker 2>and the,</v>

520
00:32:54.950 --> 00:32:59.950
<v Speaker 2>um,</v>
<v Speaker 2>the role of the teacher is to use its </v>

521
00:32:59.950 --> 00:33:03.251
<v Speaker 2>knowledge of the environment which it </v>
<v Speaker 2>can acquire using whatever way brute </v>

522
00:33:03.251 --> 00:33:06.851
<v Speaker 2>force to help the learner learn as </v>
<v Speaker 2>quickly as possible.</v>

523
00:33:09.020 --> 00:33:11.180
<v Speaker 2>So the learner is going to try to learn </v>
<v Speaker 2>by itself,</v>

524
00:33:11.181 --> 00:33:14.120
<v Speaker 2>may be using some exploration and,</v>
<v Speaker 2>and whatever.</v>

525
00:33:14.660 --> 00:33:17.270
<v Speaker 2>Um,</v>
<v Speaker 2>but the teacher can choose,</v>

526
00:33:18.140 --> 00:33:18.860
<v Speaker 2>can,</v>
<v Speaker 2>can,</v>

527
00:33:18.890 --> 00:33:21.470
<v Speaker 2>can have an influence on the interaction</v>
<v Speaker 2>with the learner,</v>

528
00:33:21.500 --> 00:33:26.500
<v Speaker 2>so as to guide the learner,</v>
<v Speaker 2>maybe teach it the things that the </v>

529
00:33:27.711 --> 00:33:32.711
<v Speaker 2>learner has most trouble with or just at</v>
<v Speaker 2>the boundary between what it knows and </v>

530
00:33:32.711 --> 00:33:32.711
<v Speaker 2>doesn't know.</v>
<v Speaker 2>And so on.</v>

531
00:33:32.711 --> 00:33:33.740
<v Speaker 2>So there's,</v>
<v Speaker 2>there's a,</v>

532
00:33:33.741 --> 00:33:37.640
<v Speaker 2>there's a tradition of these kinds of </v>
<v Speaker 2>ideas from other fields and um,</v>

533
00:33:38.230 --> 00:33:39.880
<v Speaker 2>uh,</v>
<v Speaker 2>like tutorial systems,</v>

534
00:33:39.890 --> 00:33:41.570
<v Speaker 2>for example,</v>
<v Speaker 2>an Ai.</v>

535
00:33:41.640 --> 00:33:46.640
<v Speaker 2>Um,</v>
<v Speaker 2>and and of course people in the </v>

536
00:33:46.640 --> 00:33:46.700
<v Speaker 2>humanities have been thinking about </v>
<v Speaker 2>these questions,</v>

537
00:33:46.701 --> 00:33:51.701
<v Speaker 2>but I think it's time that machine </v>
<v Speaker 2>learning people look at this because in </v>

538
00:33:51.701 --> 00:33:56.150
<v Speaker 2>the future we'll have more and more a </v>
<v Speaker 2>human machine interaction with the human</v>

539
00:33:56.151 --> 00:34:00.500
<v Speaker 2>in the loop and I think understanding </v>
<v Speaker 2>how to make this work better,</v>

540
00:34:00.710 --> 00:34:05.710
<v Speaker 1>problems around that are very </v>
<v Speaker 1>interesting and not sufficiently </v>

541
00:34:05.710 --> 00:34:08.711
<v Speaker 1>addressed.</v>
<v Speaker 1>You've done a lot of work with language </v>

542
00:34:08.711 --> 00:34:12.220
<v Speaker 1>to what aspect of the traditionally </v>
<v Speaker 1>formulated turing test,</v>

543
00:34:13.250 --> 00:34:18.250
<v Speaker 1>a test of natural language understanding</v>
<v Speaker 1>and generation in your eyes is the most </v>

544
00:34:18.250 --> 00:34:21.110
<v Speaker 1>difficult of conversation.</v>
<v Speaker 1>What in your eyes is the hardest part of</v>

545
00:34:21.111 --> 00:34:23.630
<v Speaker 1>conversation to solve from machines?</v>

546
00:34:24.530 --> 00:34:29.530
<v Speaker 2>So I would say it's everything having to</v>
<v Speaker 2>do with the non linguistic knowledge </v>

547
00:34:30.320 --> 00:34:34.220
<v Speaker 2>which implicitly you need in order to </v>
<v Speaker 2>make sense of sentences.</v>

548
00:34:34.820 --> 00:34:38.360
<v Speaker 2>Things like the Winograd Schema.</v>
<v Speaker 2>So these sentences that are semantically</v>

549
00:34:38.361 --> 00:34:43.361
<v Speaker 2>ambiguous.</v>
<v Speaker 2>Now the words you need to understand </v>

550
00:34:43.361 --> 00:34:46.991
<v Speaker 2>enough about the world in order to </v>
<v Speaker 2>really interpret properly those </v>

551
00:34:46.991 --> 00:34:49.400
<v Speaker 2>sentences.</v>
<v Speaker 2>I think these are interesting challenges</v>

552
00:34:49.401 --> 00:34:54.401
<v Speaker 2>for machine learning because they point </v>
<v Speaker 2>in the direction of building systems </v>

553
00:34:55.520 --> 00:35:00.520
<v Speaker 2>that both understand how the world works</v>
<v Speaker 2>and this causal relationships in the </v>

554
00:35:00.520 --> 00:35:05.441
<v Speaker 2>world and associate that knowledge with </v>
<v Speaker 2>how to express it in language.</v>

555
00:35:07.370 --> 00:35:12.370
<v Speaker 2>Either for reading or writing.</v>
<v Speaker 2>You speak French?</v>

556
00:35:13.370 --> 00:35:14.330
<v Speaker 2>Yes.</v>
<v Speaker 2>It's my mother tongue.</v>

557
00:35:14.680 --> 00:35:19.680
<v Speaker 1>It's one of the romance languages.</v>
<v Speaker 1>Do you think passing the turing test and</v>

558
00:35:20.141 --> 00:35:23.200
<v Speaker 1>all the underlying challenges you just </v>
<v Speaker 1>mentioned depend on language.</v>

559
00:35:23.201 --> 00:35:28.201
<v Speaker 1>Do you think it might be easier in </v>
<v Speaker 1>French that is in English and that is </v>

560
00:35:28.201 --> 00:35:28.201
<v Speaker 1>independent of language?</v>

561
00:35:28.950 --> 00:35:33.950
<v Speaker 2>I think it's independent of language.</v>
<v Speaker 2>I would like to build systems that can </v>

562
00:35:36.721 --> 00:35:41.721
<v Speaker 2>use same principles,</v>
<v Speaker 2>the same learning mechanisms to learn </v>

563
00:35:43.771 --> 00:35:46.110
<v Speaker 2>from human agents.</v>
<v Speaker 2>Whatever their language.</v>

564
00:35:47.190 --> 00:35:52.190
<v Speaker 1>Well certainly us humans can talk more </v>
<v Speaker 1>beautifully and smoothly in poetry.</v>

565
00:35:52.201 --> 00:35:55.980
<v Speaker 1>So I'm Russian originally.</v>
<v Speaker 1>I know poetry and Russian is</v>

566
00:35:57.060 --> 00:35:58.200
<v Speaker 2>maybe easier</v>

567
00:35:58.570 --> 00:36:01.570
<v Speaker 1>to convey complex ideas and then it is </v>
<v Speaker 1>in English.</v>

568
00:36:02.320 --> 00:36:02.950
<v Speaker 1>But,</v>
<v Speaker 1>uh,</v>

569
00:36:02.980 --> 00:36:07.150
<v Speaker 1>maybe I'm showing my bias and some </v>
<v Speaker 1>people say that about a French,</v>

570
00:36:07.570 --> 00:36:08.430
<v Speaker 1>but,</v>
<v Speaker 1>uh,</v>

571
00:36:08.470 --> 00:36:13.470
<v Speaker 1>of course the goal ultimately is our </v>
<v Speaker 1>human brain is able to utilize any kind </v>

572
00:36:13.960 --> 00:36:17.920
<v Speaker 1>of those languages to use them as tools </v>
<v Speaker 1>to convey meaning.</v>

573
00:36:18.400 --> 00:36:19.090
<v Speaker 1>Yeah,</v>
<v Speaker 1>of course,</v>

574
00:36:19.260 --> 00:36:22.680
<v Speaker 2>differences between languages and maybe </v>
<v Speaker 2>some are slightly better at some things,</v>

575
00:36:22.681 --> 00:36:26.520
<v Speaker 2>but in the grand scheme of things where </v>
<v Speaker 2>we're trying to understand how the brain</v>

576
00:36:26.521 --> 00:36:31.230
<v Speaker 2>works and language and so on,</v>
<v Speaker 2>I think these differences are minute.</v>

577
00:36:32.950 --> 00:36:37.950
<v Speaker 1>So you've lived perhaps through an ai </v>
<v Speaker 1>winter of sorts?</v>

578
00:36:38.710 --> 00:36:43.710
<v Speaker 1>Yes.</v>
<v Speaker 1>How did you stay warm and continuing to </v>

579
00:36:43.710 --> 00:36:46.570
<v Speaker 1>research?</v>
<v Speaker 1>Stay warm with friends with friends.</v>

580
00:36:46.571 --> 00:36:51.571
<v Speaker 1>Okay.</v>
<v Speaker 1>So it's important to have friends and </v>

581
00:36:51.571 --> 00:36:53.881
<v Speaker 1>uh,</v>
<v Speaker 1>what have you learned from the </v>

582
00:36:53.881 --> 00:36:53.881
<v Speaker 1>experience?</v>

583
00:36:53.881 --> 00:36:56.950
<v Speaker 2>Listen to your inner voice.</v>
<v Speaker 2>Don't you know,</v>

584
00:36:57.010 --> 00:37:02.010
<v Speaker 2>be trying to just please the crowds and </v>
<v Speaker 2>the fashion and uh,</v>

585
00:37:04.420 --> 00:37:09.420
<v Speaker 2>if you have a strong intuition about </v>
<v Speaker 2>something that is not contradicted by </v>

586
00:37:10.390 --> 00:37:12.730
<v Speaker 2>actual evidence,</v>
<v Speaker 2>go for it.</v>

587
00:37:13.820 --> 00:37:15.790
<v Speaker 2>I mean,</v>
<v Speaker 2>it could be contradicted by people</v>

588
00:37:16.930 --> 00:37:19.720
<v Speaker 1>that not your own instinct of base title</v>
<v Speaker 1>everything.</v>

589
00:37:19.810 --> 00:37:20.980
<v Speaker 1>So of course,</v>
<v Speaker 1>of course</v>

590
00:37:21.170 --> 00:37:26.170
<v Speaker 2>you have to adapt your beliefs when your</v>
<v Speaker 2>experiments contradict those beliefs.</v>

591
00:37:27.530 --> 00:37:31.130
<v Speaker 2>But you have to stick to your beliefs </v>
<v Speaker 2>otherwise,</v>

592
00:37:31.180 --> 00:37:32.780
<v Speaker 2>and it's,</v>
<v Speaker 2>it's,</v>

593
00:37:32.810 --> 00:37:34.940
<v Speaker 2>it's what allowed me to go through those</v>
<v Speaker 2>years.</v>

594
00:37:34.941 --> 00:37:39.941
<v Speaker 2>It's what allowed me to persist in </v>
<v Speaker 2>directions that took time.</v>

595
00:37:41.420 --> 00:37:46.420
<v Speaker 2>Whatever other people think it took time</v>
<v Speaker 2>to mature and you bring fruits.</v>

596
00:37:47.910 --> 00:37:51.490
<v Speaker 1>So history of Ai is marked with these,</v>
<v Speaker 1>uh,</v>

597
00:37:52.380 --> 00:37:54.360
<v Speaker 1>of course it's Margaret technical </v>
<v Speaker 1>breakthroughs,</v>

598
00:37:54.361 --> 00:37:59.361
<v Speaker 1>but it's also mark with these seminal </v>
<v Speaker 1>events that capture the imagination of </v>

599
00:37:59.361 --> 00:38:01.710
<v Speaker 1>the community.</v>
<v Speaker 1>Most recent,</v>

600
00:38:02.130 --> 00:38:07.130
<v Speaker 1>I would say a Alphago beating the world </v>
<v Speaker 1>champion human goal player was one of </v>

601
00:38:07.130 --> 00:38:11.571
<v Speaker 1>those moments.</v>
<v Speaker 1>What do you think the next such moment </v>

602
00:38:11.580 --> 00:38:13.100
<v Speaker 1>might be for?</v>

603
00:38:13.540 --> 00:38:18.540
<v Speaker 2>First of all,</v>
<v Speaker 2>I think that these so called seminal </v>

604
00:38:18.540 --> 00:38:21.091
<v Speaker 2>events are overrated.</v>
<v Speaker 2>Uh,</v>

605
00:38:22.570 --> 00:38:25.750
<v Speaker 2>as I said,</v>
<v Speaker 2>science really moves by small steps.</v>

606
00:38:25.930 --> 00:38:30.930
<v Speaker 2>Now what happens is you make one more </v>
<v Speaker 2>small step and it's like the drop that </v>

607
00:38:33.860 --> 00:38:35.140
<v Speaker 2>you know,</v>
<v Speaker 2>allows to,</v>

608
00:38:35.190 --> 00:38:35.740
<v Speaker 2>uh,</v>
<v Speaker 2>you know,</v>

609
00:38:35.860 --> 00:38:37.570
<v Speaker 2>that fills the bucket and,</v>
<v Speaker 2>and,</v>

610
00:38:37.571 --> 00:38:42.571
<v Speaker 2>uh,</v>
<v Speaker 2>and then you have drastic consequences </v>

611
00:38:42.571 --> 00:38:45.181
<v Speaker 2>because now you are able to do something</v>
<v Speaker 2>you were not able to do before or now </v>

612
00:38:45.181 --> 00:38:49.770
<v Speaker 2>say the cost of building some device or </v>
<v Speaker 2>solving the problem becomes cheaper than</v>

613
00:38:49.811 --> 00:38:50.490
<v Speaker 2>what existed.</v>
<v Speaker 2>And,</v>

614
00:38:50.491 --> 00:38:52.720
<v Speaker 2>and you have a new market that opens up,</v>
<v Speaker 2>right?</v>

615
00:38:52.721 --> 00:38:57.721
<v Speaker 2>So,</v>
<v Speaker 2>so especially in the world of commerce </v>

616
00:38:57.721 --> 00:39:00.110
<v Speaker 2>and in applications,</v>
<v Speaker 2>the impact of a small scientific </v>

617
00:39:00.161 --> 00:39:03.280
<v Speaker 2>progress could be huge.</v>
<v Speaker 2>Um,</v>

618
00:39:03.610 --> 00:39:06.040
<v Speaker 2>but in the science itself,</v>
<v Speaker 2>I think it's very,</v>

619
00:39:06.041 --> 00:39:08.220
<v Speaker 2>very gradual.</v>
<v Speaker 2>And um,</v>

620
00:39:08.510 --> 00:39:09.700
<v Speaker 2>where are these steps</v>

621
00:39:09.760 --> 00:39:13.770
<v Speaker 1>being taken now?</v>
<v Speaker 1>So there is unsupervised learning.</v>

622
00:39:13.880 --> 00:39:17.360
<v Speaker 2>If I look at one trend that I like in,</v>
<v Speaker 2>uh,</v>

623
00:39:17.390 --> 00:39:18.740
<v Speaker 2>in,</v>
<v Speaker 2>in my community,</v>

624
00:39:19.190 --> 00:39:21.300
<v Speaker 2>um,</v>
<v Speaker 2>so for example,</v>

625
00:39:21.310 --> 00:39:23.180
<v Speaker 2>in my line,</v>
<v Speaker 2>my institute,</v>

626
00:39:23.240 --> 00:39:28.240
<v Speaker 2>what are the two hottest topics gans and</v>
<v Speaker 2>reinforced for planning even though in </v>

627
00:39:31.450 --> 00:39:36.450
<v Speaker 2>the Montreal in particular,</v>
<v Speaker 2>like reinforcement learning was </v>

628
00:39:36.450 --> 00:39:37.520
<v Speaker 2>something pretty much absent just two or</v>
<v Speaker 2>three years ago.</v>

629
00:39:37.970 --> 00:39:42.970
<v Speaker 2>So it is really a big interest from </v>
<v Speaker 2>students and there's a big interest from</v>

630
00:39:44.270 --> 00:39:49.270
<v Speaker 2>people like me.</v>
<v Speaker 2>So I would say this is something where </v>

631
00:39:49.270 --> 00:39:54.101
<v Speaker 2>we're going to see more progress even </v>
<v Speaker 2>though it hasn't yet provided much in </v>

632
00:39:54.501 --> 00:39:59.501
<v Speaker 2>terms of actual industrial fallout.</v>
<v Speaker 2>Like even though there's Alphago,</v>

633
00:39:59.511 --> 00:40:04.511
<v Speaker 2>there's no,</v>
<v Speaker 2>like Google is not making money on this </v>

634
00:40:04.511 --> 00:40:06.881
<v Speaker 2>right now.</v>
<v Speaker 2>But I think over the long term this is </v>

635
00:40:06.881 --> 00:40:06.980
<v Speaker 2>really,</v>
<v Speaker 2>really important for many reasons.</v>

636
00:40:08.840 --> 00:40:09.860
<v Speaker 2>So in other words,</v>
<v Speaker 2>agent,</v>

637
00:40:09.861 --> 00:40:11.690
<v Speaker 2>I would say reinforcement learning,</v>
<v Speaker 2>baby,</v>

638
00:40:11.910 --> 00:40:12.950
<v Speaker 2>uh,</v>
<v Speaker 2>more generally,</v>

639
00:40:12.980 --> 00:40:15.650
<v Speaker 2>agent learning because it doesn't have </v>
<v Speaker 2>to be with rewards,</v>

640
00:40:15.651 --> 00:40:19.340
<v Speaker 2>it could be in all kinds of ways that an</v>
<v Speaker 2>agent is learning about its environment.</v>

641
00:40:20.650 --> 00:40:22.330
<v Speaker 2>Now reinforced learning.</v>
<v Speaker 2>You're excited.</v>

642
00:40:22.331 --> 00:40:22.390
<v Speaker 2>Yeah.</v>

643
00:40:22.760 --> 00:40:24.470
<v Speaker 1>Do you think,</v>
<v Speaker 1>uh,</v>

644
00:40:24.810 --> 00:40:28.650
<v Speaker 1>you think gans could provide something?</v>
<v Speaker 1>Yes.</v>

645
00:40:28.651 --> 00:40:31.260
<v Speaker 1>Some moment in,</v>
<v Speaker 1>in well</v>

646
00:40:32.000 --> 00:40:37.000
<v Speaker 2>or other generative models I believe </v>
<v Speaker 2>will be crucial ingredients in building </v>

647
00:40:39.710 --> 00:40:44.710
<v Speaker 2>agents that can understand the world.</v>
<v Speaker 2>A lot of the successes in reinforcement </v>

648
00:40:45.531 --> 00:40:50.531
<v Speaker 2>learning in the past has been with a </v>
<v Speaker 2>policy gradient where you just learn to </v>

649
00:40:50.541 --> 00:40:55.541
<v Speaker 2>policy.</v>
<v Speaker 2>You don't actually learn a model of the </v>

650
00:40:55.541 --> 00:40:55.541
<v Speaker 2>world,</v>
<v Speaker 2>but there are lots of issues with that.</v>

651
00:40:55.541 --> 00:40:59.930
<v Speaker 2>Um,</v>
<v Speaker 2>and we don't know how to model based </v>

652
00:40:59.930 --> 00:41:02.291
<v Speaker 2>around right now,</v>
<v Speaker 2>but I think this is where we have to go </v>

653
00:41:02.291 --> 00:41:07.000
<v Speaker 2>in order to build models that can </v>
<v Speaker 2>generalize faster and better to new </v>

654
00:41:07.000 --> 00:41:09.080
<v Speaker 2>distributions.</v>
<v Speaker 2>Um,</v>

655
00:41:09.110 --> 00:41:14.110
<v Speaker 2>that capture to some extent at least the</v>
<v Speaker 2>underlying causal mechanisms in the </v>

656
00:41:14.391 --> 00:41:16.610
<v Speaker 2>world.</v>
<v Speaker 2>Last question,</v>

657
00:41:17.480 --> 00:41:19.480
<v Speaker 2>what made you fall in love with her</v>

658
00:41:19.540 --> 00:41:22.030
<v Speaker 1>official intelligence?</v>
<v Speaker 1>If you look back,</v>

659
00:41:22.690 --> 00:41:27.690
<v Speaker 1>what was the first moment in your life </v>
<v Speaker 1>when you were fascinated by either the </v>

660
00:41:28.571 --> 00:41:30.400
<v Speaker 1>human mind or the artificial mind?</v>

661
00:41:31.320 --> 00:41:33.680
<v Speaker 2>Know when I was an adolescent,</v>
<v Speaker 2>I was a lot,</v>

662
00:41:33.681 --> 00:41:36.350
<v Speaker 2>and then I,</v>
<v Speaker 2>I started reading science fiction.</v>

663
00:41:37.790 --> 00:41:40.110
<v Speaker 2>There you go.</v>
<v Speaker 2>That's it.</v>

664
00:41:40.570 --> 00:41:41.990
<v Speaker 2>That's,</v>
<v Speaker 2>that's where I got hooked.</v>

665
00:41:42.530 --> 00:41:43.730
<v Speaker 2>And then,</v>
<v Speaker 2>um,</v>

666
00:41:43.770 --> 00:41:44.540
<v Speaker 2>and then,</v>
<v Speaker 2>you know,</v>

667
00:41:44.570 --> 00:41:48.080
<v Speaker 2>I had one of the first personal </v>
<v Speaker 2>computers and uh,</v>

668
00:41:48.081 --> 00:41:51.890
<v Speaker 2>I got hooked in programming and so it </v>
<v Speaker 2>just,</v>

669
00:41:51.970 --> 00:41:56.970
<v Speaker 2>you know,</v>
<v Speaker 2>start with fiction and then make it a </v>

670
00:41:56.970 --> 00:41:56.970
<v Speaker 2>reality.</v>
<v Speaker 2>That's right.</v>

671
00:41:56.970 --> 00:41:56.970
<v Speaker 2>You.</v>
<v Speaker 2>Ashley,</v>

672
00:41:56.970 --> 00:41:58.070
<v Speaker 2>thank you so much for talking to my </v>
<v Speaker 2>pleasure.</v>

