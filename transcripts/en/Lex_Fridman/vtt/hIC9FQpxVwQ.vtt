WEBVTT

1
00:00:00.420 --> 00:00:02.790
The following is a conversation with Eric Schmidt.

2
00:00:03.180 --> 00:00:07.320
He was the CEO of Google for 10 years and a chairman for six more guiding the

3
00:00:07.321 --> 00:00:11.670
company through an incredible period of growth and a series of world changing

4
00:00:11.671 --> 00:00:12.504
innovations.

5
00:00:12.960 --> 00:00:17.940
He is one of the most impactful leaders in the era of the Internet and the

6
00:00:17.970 --> 00:00:21.750
powerful voice for the promise of technology in our society.

7
00:00:22.290 --> 00:00:27.290
It was truly an honor to speak with him as part of the MIT course on artificial

8
00:00:27.511 --> 00:00:31.800
general intelligence and the artificial intelligence podcast.

9
00:00:31.890 --> 00:00:35.940
And now here's my conversation with Eric Schmidt.

10
00:00:37.010 --> 00:00:39.830
What was the first moment when you fell in love with technology?

11
00:00:40.590 --> 00:00:41.000
<v 1>Um,
I,</v>

12
00:00:41.000 --> 00:00:46.000
I grew up in the 1960s as a boy where every boy wanted to be an astronaut and

13
00:00:46.881 --> 00:00:51.290
part of the space program.
So like everyone else of my age,

14
00:00:51.320 --> 00:00:54.050
we would go out to the cow pasture behind my house,

15
00:00:54.320 --> 00:00:58.010
which was literally a cow pasture and we would shoot model rockets off.

16
00:00:58.520 --> 00:01:01.280
And that I think is the beginning.
Um,
and of course,

17
00:01:01.281 --> 00:01:05.990
generationally today he will be video games and all the amazing things that you

18
00:01:05.991 --> 00:01:08.030
can do online with computers.

19
00:01:09.090 --> 00:01:10.470
<v 0>There's a transformative,</v>

20
00:01:10.471 --> 00:01:15.471
inspiring aspect of science and math that maybe rockets would bring,

21
00:01:15.720 --> 00:01:17.400
would instill in individuals.

22
00:01:17.401 --> 00:01:21.150
You've mentioned yesterday the eighth grade math is where the journey through

23
00:01:21.151 --> 00:01:23.520
mathematical universe diverges from many people.

24
00:01:23.790 --> 00:01:26.250
Is this a fork in the roadway?

25
00:01:26.910 --> 00:01:31.140
There's a professor of Math of Berkeley,
Edward Franco.
He,
uh,

26
00:01:31.170 --> 00:01:35.430
I'm not sure if you're familiar with him.
I am.
He has written this amazing book.

27
00:01:35.431 --> 00:01:39.690
I recommend to everybody called love and math.
Two of my favorite,
uh,
words.

28
00:01:40.730 --> 00:01:43.950
Uh,
he says that,
uh,

29
00:01:43.951 --> 00:01:46.620
if if painting was taught like math,

30
00:01:46.650 --> 00:01:49.110
then the students would be asked to paint a fence,

31
00:01:49.620 --> 00:01:53.910
which is his analogy of essentially how math is taught and she never get a

32
00:01:53.911 --> 00:01:58.520
chance to discover the beauty of the art of painting or the beauty of the art of

33
00:01:58.521 --> 00:02:00.840
math.
So how,

34
00:02:00.841 --> 00:02:03.690
when and where did you discover that beauty?

35
00:02:05.230 --> 00:02:10.230
<v 1>I think what happens with people like myself is that your math enabled pretty</v>

36
00:02:10.301 --> 00:02:15.010
early and all of a sudden you discover that you can use that to discover new

37
00:02:15.011 --> 00:02:19.090
insights.
The great scientists will all tell a story.

38
00:02:19.091 --> 00:02:21.550
The men and women who are fantastic today,

39
00:02:22.030 --> 00:02:24.610
it's somewhere when they were in high school or in college,

40
00:02:24.611 --> 00:02:27.130
they discovered that they could discover something in themselves.

41
00:02:27.790 --> 00:02:29.860
And that sense of building something,

42
00:02:29.861 --> 00:02:33.340
of having an impact that you own drives knowledge,

43
00:02:33.341 --> 00:02:35.960
acquisition and learning.
In my case,

44
00:02:35.961 --> 00:02:40.900
he was programming and the notion that I could build things that had not existed

45
00:02:41.050 --> 00:02:43.870
that I had built and I did it had my name on it,

46
00:02:44.380 --> 00:02:46.150
and this was before open source,

47
00:02:46.151 --> 00:02:48.640
but you could think of it as open source contributions.

48
00:02:49.090 --> 00:02:51.730
So today if I were 16 or 17 year old boy,

49
00:02:51.760 --> 00:02:56.320
I'm sure that I would aspire as a computer scientist to make a contribution like

50
00:02:56.321 --> 00:02:58.900
the open source heroes of the world today.

51
00:02:58.900 --> 00:03:03.040
That would be what would be driving me and I be trying and learning and making

52
00:03:03.041 --> 00:03:06.070
mistakes and so forth in the ways that it works.

53
00:03:06.610 --> 00:03:08.620
The repository that represent,

54
00:03:08.621 --> 00:03:13.270
that get hub represents and that open source libraries represent is an enormous

55
00:03:13.271 --> 00:03:16.750
bank of knowledge of all of the people who are doing that.

56
00:03:17.110 --> 00:03:21.070
And one of the lessons that I learned at Google was that the world is a very big

57
00:03:21.071 --> 00:03:25.210
place and there's an awful lot of smart people and an awful lot of them are

58
00:03:25.211 --> 00:03:28.930
underutilized.
So here's an opportunity,
for example,

59
00:03:28.931 --> 00:03:30.370
building parts of programs,

60
00:03:30.400 --> 00:03:33.670
building new ideas to contribute to the greater of society.

61
00:03:36.500 --> 00:03:38.780
<v 0>So in that moment in the 70s,
the,</v>

62
00:03:38.781 --> 00:03:41.900
the inspiring moment where there was nothing and then you created something

63
00:03:41.901 --> 00:03:44.660
through programming,
that magical moment.
Uh,

64
00:03:44.661 --> 00:03:48.800
so in 1975 I think you've created a program called Lex,

65
00:03:49.160 --> 00:03:52.310
which I especially like because my name is Lex.
So thank you.

66
00:03:52.311 --> 00:03:57.311
Thank you for creating a brand that establish a reputation that's long lasting,

67
00:03:57.561 --> 00:04:00.650
reliable,
and has a big impact on the world and still use today.

68
00:04:01.160 --> 00:04:03.200
So thank you for that.
Uh,
but

69
00:04:04.030 --> 00:04:05.260
<v 1>more seriously</v>

70
00:04:06.550 --> 00:04:11.020
<v 0>in that time,
in the 70s,
as an engineer,
personal computers were being born,</v>

71
00:04:12.520 --> 00:04:17.470
<v 1>do you think you'd be able to predict the eighties nineties and the arts of
work?</v>

72
00:04:17.471 --> 00:04:21.850
Computers would go?
I'm sure I could not.
I would not have gotten it right.

73
00:04:22.600 --> 00:04:25.750
Um,
I was the beneficiary of the great work of many,

74
00:04:25.751 --> 00:04:29.770
many people who saw it clearer than I did.
Um,
with Lex,

75
00:04:29.771 --> 00:04:32.110
I worked with a felony,
Michael Lisc,

76
00:04:32.530 --> 00:04:36.820
who was my supervisor and he essentially helped me architect and deliver a

77
00:04:36.821 --> 00:04:39.700
system that's still in use today.
After that,

78
00:04:39.701 --> 00:04:43.840
I worked at Xerox Palo Alto Research Center where the Aalto was invented and the

79
00:04:43.841 --> 00:04:48.841
alto is the predecessor of the modern personal computer or Macintosh and so

80
00:04:49.301 --> 00:04:50.134
forth.

81
00:04:50.140 --> 00:04:54.640
And the altos were very rare and I had to drive an hour from Berkeley to go use

82
00:04:54.641 --> 00:04:55.210
them.

83
00:04:55.210 --> 00:05:00.210
But I made a point of skipping classes and doing whatever it took to have access

84
00:05:00.941 --> 00:05:04.360
to this extraordinary achievement.
I knew that they were consequential.

85
00:05:04.870 --> 00:05:07.780
What I did not understand with scaling,

86
00:05:08.230 --> 00:05:12.310
I did not understand what would happen when you had 100 million as opposed to a

87
00:05:12.310 --> 00:05:15.910
hundred.
And so the since then and I have learned the benefit of scale,

88
00:05:16.210 --> 00:05:19.630
I always look for things which are going to scale to platforms,
right?

89
00:05:19.631 --> 00:05:23.440
So mobile phones,
android,
all those things.
There are,

90
00:05:23.530 --> 00:05:27.160
the world is in numerous there,
there,
there are many,
many people in the world.

91
00:05:27.370 --> 00:05:28.510
People really have needs.

92
00:05:28.511 --> 00:05:31.720
They really will use these platforms and you can build big businesses on top of

93
00:05:31.721 --> 00:05:32.540
them.

94
00:05:32.540 --> 00:05:35.540
<v 0>So it's interesting.
So when you see a piece of technology now you think,</v>

95
00:05:35.570 --> 00:05:38.810
what will this technology look like when it's in the hands of a billion people?

96
00:05:38.810 --> 00:05:42.360
<v 1>People.
That's right.
So,
so an example would be that um,</v>

97
00:05:42.490 --> 00:05:47.080
the market is so competitive now that if you can't figure out a way for

98
00:05:47.081 --> 00:05:50.440
something to have a million users or a billion users,

99
00:05:50.770 --> 00:05:55.510
it probably is not going to be successful because something else will become the

100
00:05:55.511 --> 00:05:56.710
general platform.

101
00:05:56.990 --> 00:06:01.990
And your idea will become a last idea or a specialized service with relatively

102
00:06:02.901 --> 00:06:07.640
few users.
So it's a path to generality.
It's a path to general platform use.

103
00:06:07.641 --> 00:06:09.470
It's a path to broad applicability.

104
00:06:10.010 --> 00:06:12.590
Now there are plenty of good businesses that are tiny,

105
00:06:12.620 --> 00:06:14.390
so luxury goods for example.

106
00:06:14.840 --> 00:06:18.290
But if you want to have an impact at scale,

107
00:06:18.470 --> 00:06:22.340
you have to look for things which are of common value,
common pricing,

108
00:06:22.341 --> 00:06:24.590
common distribution,
and solve common problems.

109
00:06:24.710 --> 00:06:27.800
They're problems that everyone has.
And by the way,
people have lots of problems,

110
00:06:28.070 --> 00:06:32.240
information,
medicine,
health,
education,
so forth.
Work on those problems.

111
00:06:33.070 --> 00:06:36.590
<v 0>You said,
uh,
you're a big fan of the middleclass.
Uh,</v>

112
00:06:36.690 --> 00:06:39.630
cause there's so many of them.
There's so many of them by definition.

113
00:06:40.140 --> 00:06:41.420
So any product,
any,

114
00:06:41.421 --> 00:06:46.200
any thing that has a huge impact of improves their lives is,
is a,

115
00:06:46.201 --> 00:06:48.540
is a great business decision,
is just good for society.

116
00:06:48.850 --> 00:06:51.940
<v 1>And there's nothing wrong with starting off in the high end.</v>

117
00:06:52.330 --> 00:06:55.090
As long as you have a plan to get to the middle class.

118
00:06:55.390 --> 00:06:59.110
There's nothing wrong with starting with a specialized market in order to learn

119
00:06:59.111 --> 00:07:01.780
and to build and to fun things.
So you start,
you know,

120
00:07:01.781 --> 00:07:04.060
luxury market to build a general purpose market.

121
00:07:04.450 --> 00:07:07.270
But if you define yourself as only a narrow market,

122
00:07:07.480 --> 00:07:11.740
someone else can come along with a general purpose market that can push you to

123
00:07:11.741 --> 00:07:14.260
the corner,
can restrict the scale of operation,

124
00:07:14.261 --> 00:07:17.320
can force you to be a lesser impact than you might be.

125
00:07:17.780 --> 00:07:22.030
So it's very important to think in terms of broad businesses and broad impact.

126
00:07:22.330 --> 00:07:24.820
Even if you start in a little corner somewhere.

127
00:07:26.230 --> 00:07:31.230
<v 0>So as you look to the 70s but also in the decades to come and you saw computers,</v>

128
00:07:33.100 --> 00:07:38.100
did you see them as tools or was there a little element of another entity?

129
00:07:40.270 --> 00:07:45.270
And remember a quote saying AI began with our a dream to create the gods.

130
00:07:46.120 --> 00:07:49.720
Is there a feeling when you wrote that program that you are creating another

131
00:07:49.721 --> 00:07:52.630
entity?
Um,
giving life to something

132
00:07:52.790 --> 00:07:54.620
<v 1>I wish I could say otherwise,</v>

133
00:07:54.621 --> 00:07:58.730
but I simply found the technology platforms so exciting.

134
00:07:58.731 --> 00:07:59.870
That's what I was focused on.

135
00:08:00.470 --> 00:08:02.990
I think the majority of the people that I've worked with,

136
00:08:03.410 --> 00:08:06.230
and there are a few exceptions,
Steve jobs being an example,

137
00:08:06.680 --> 00:08:09.920
really saw this as a great technology technological play.

138
00:08:09.950 --> 00:08:14.510
I think relatively few of the technical people understood the scale of its

139
00:08:14.511 --> 00:08:19.280
impact.
So I used NCP,
which is a predecessor to TCP IP.

140
00:08:19.550 --> 00:08:21.170
It just made sense to connect things.

141
00:08:21.171 --> 00:08:25.460
We didn't think of it in terms of the Internet and then companies and then

142
00:08:25.461 --> 00:08:29.150
Facebook and then Twitter and then you know,
politics and so forth.

143
00:08:29.151 --> 00:08:32.240
We never did that build,
we didn't have that vision.

144
00:08:32.810 --> 00:08:37.340
And I think most people to rare person who can see compounding at scale,

145
00:08:37.940 --> 00:08:40.520
most people can see if you ask people to predict the future,

146
00:08:40.521 --> 00:08:43.940
they'll say they'll give you an answer of six to nine months or 12 months.

147
00:08:44.420 --> 00:08:48.590
Because that's about as far as people can imagine.
But there's an old saying,

148
00:08:48.620 --> 00:08:51.710
which actually was attributed to a professor at MIT a long time ago,

149
00:08:52.010 --> 00:08:57.010
that we overestimate what can be done in one year and we underestimate what can

150
00:08:58.321 --> 00:08:59.460
be done in a decade.

151
00:09:00.060 --> 00:09:04.920
And there's a great deal of evidence that these core platforms at hardware and

152
00:09:04.921 --> 00:09:09.390
software take a decade,
right?
So think about self driving cars.

153
00:09:09.391 --> 00:09:13.290
Self driving cars were thought about in the 90s there were projects around them.

154
00:09:13.291 --> 00:09:17.070
The first Darpa challenge her and challenge was roughly 2004.

155
00:09:17.071 --> 00:09:19.610
So that's roughly 15 years ago.
Uh,

156
00:09:19.620 --> 00:09:23.870
and today we have self driving cars operating in a city in Arizona,
right?

157
00:09:23.880 --> 00:09:27.600
It's 15 years and we still have a ways to go before they're more generally

158
00:09:27.601 --> 00:09:28.434
available.

159
00:09:31.550 --> 00:09:33.680
<v 0>So you've spoken about the importance,</v>

160
00:09:33.740 --> 00:09:37.010
you just talked about predicting a into the future.

161
00:09:37.040 --> 00:09:38.770
He's spoken about the importance of uh,

162
00:09:39.470 --> 00:09:42.380
thinking five years ahead and having a plan for those five years.

163
00:09:42.870 --> 00:09:43.970
The way to say it is that

164
00:09:44.690 --> 00:09:47.210
<v 1>almost everybody has a one year plan,</v>

165
00:09:47.480 --> 00:09:50.150
almost no one has a proper five year plant.

166
00:09:50.900 --> 00:09:54.020
And the key thing to have on a five year plan is to having a model for what's

167
00:09:54.021 --> 00:09:57.980
going to happen under the underlying platforms.
So here's an example.

168
00:09:58.590 --> 00:10:02.460
Um,
Computer Moore's law as we know it,
the thing that powered,
um,

169
00:10:02.510 --> 00:10:07.510
improvements in CPS has largely halted in its traditional shrinking mechanisms

170
00:10:07.521 --> 00:10:10.110
because the costs have just gotten so high.
Um,

171
00:10:10.220 --> 00:10:11.720
and it's getting harder and harder,

172
00:10:12.080 --> 00:10:15.590
but there's plenty of algorithmic improvements and specialized hardware

173
00:10:15.591 --> 00:10:16.424
improvements.

174
00:10:16.490 --> 00:10:20.720
So you need to understand the nature of those improvements and where they'll go

175
00:10:20.960 --> 00:10:24.230
in order to understand how it will change the platform.
Um,

176
00:10:24.260 --> 00:10:26.030
in the area of network connectivity,

177
00:10:26.060 --> 00:10:28.940
what are the gains that are going to be possible in wireless?

178
00:10:29.360 --> 00:10:34.360
It looks like there is an enormous expansion of wireless connectivity at many

179
00:10:35.451 --> 00:10:39.170
different bands,
right?
And that we will primarily,
historically,

180
00:10:39.171 --> 00:10:41.600
I've always thought that we were primarily going to be using fiber,

181
00:10:42.050 --> 00:10:45.860
but now it looks like we're going to be using fiber plus very powerful high

182
00:10:45.861 --> 00:10:49.490
bandwidth,
uh,
sort of short distance connectivity to,

183
00:10:49.810 --> 00:10:54.170
to bridge the last mile,
right?
That's an amazing achievement.
If you know that,

184
00:10:54.410 --> 00:10:57.170
then you're going to build your systems differently.
By the way,

185
00:10:57.171 --> 00:10:59.570
those networks have different latency properties,
right?

186
00:10:59.600 --> 00:11:03.680
Because they're more symmetric.
Um,
the algorithms feel faster for that reason.

187
00:11:04.930 --> 00:11:09.340
<v 0>And so when you think about whether it's fiber or just technologies in general,</v>

188
00:11:09.790 --> 00:11:10.630
so there's this,

189
00:11:10.631 --> 00:11:15.070
a barber would poem or quote that I really like.

190
00:11:15.790 --> 00:11:20.290
It's from the champions of the impossible rather than the slaves of the possible

191
00:11:20.291 --> 00:11:22.360
that evolution draws its creative force.

192
00:11:23.160 --> 00:11:25.900
So in predicting the next five years,

193
00:11:25.930 --> 00:11:30.280
I'd like to talk about the impossible and the possible well,
and again,
and again,

194
00:11:30.281 --> 00:11:34.660
one of the great things about humanity is that we produce dreamers,
right?
Right.

195
00:11:34.661 --> 00:11:39.160
We literally have people who have a vision and a dream.
They are,
if you will,

196
00:11:39.190 --> 00:11:42.640
disagreeable in the sense that they disagree with the WWE.

197
00:11:42.670 --> 00:11:45.910
They disagree with what the sort of Zeitgeisty is.
They,

198
00:11:46.130 --> 00:11:47.570
they say there is another,
another

199
00:11:47.580 --> 00:11:51.570
<v 1>way.
They have a belief,
they have a vision.
If you look at science,</v>

200
00:11:51.750 --> 00:11:55.490
science is always marked by such people who,

201
00:11:55.750 --> 00:11:57.790
who went against some conventional wisdom,

202
00:11:58.210 --> 00:12:01.900
collected the knowledge at the time and assembled in a way they produced a

203
00:12:01.901 --> 00:12:02.800
powerful platform.

204
00:12:03.590 --> 00:12:08.590
<v 0>And you've been a amazingly honest about in an inspiring way about things you'd</v>

205
00:12:10.691 --> 00:12:12.130
been wrong about predicting.

206
00:12:12.131 --> 00:12:14.620
And you've obviously had been right about a lot of things,

207
00:12:14.621 --> 00:12:18.670
but in this kind of tension,

208
00:12:19.000 --> 00:12:24.000
how do you balance as a company in predicting the next five years the impossible

209
00:12:24.581 --> 00:12:28.420
planning for the impossible.
So listening to those crazy dreamers,

210
00:12:28.840 --> 00:12:32.020
letting them do,
letting them run away and make,

211
00:12:32.380 --> 00:12:36.950
make the impossible real,
make it happen.
And slowly,
you know,

212
00:12:37.030 --> 00:12:41.890
that's how programmers often think and slowing things down and uh,
saying,
well,

213
00:12:41.891 --> 00:12:46.630
this is the rational,
this is the possible,
the pragmatic,
they,
uh,

214
00:12:46.960 --> 00:12:48.860
the dream of versus the pragmatist show.

215
00:12:49.070 --> 00:12:54.070
<v 1>So it's helpful to have a model of wishing encourages a predictable revenue</v>

216
00:12:55.191 --> 00:12:59.540
stream as well as the ability to do new things.
So in Google's case,

217
00:12:59.541 --> 00:13:03.650
we're big enough and well enough managed and so forth that we have a pretty good

218
00:13:03.651 --> 00:13:07.250
sense of what our revenue will be for the next year or two,
at least for a while.

219
00:13:07.850 --> 00:13:12.850
And so we have enough cash generation that we can make bets and indeed Google

220
00:13:15.021 --> 00:13:19.280
has become alphabet to the corporation is organized around these bets.

221
00:13:19.640 --> 00:13:23.960
And these bets are in areas of fundamental importance to,
to the world,

222
00:13:24.000 --> 00:13:27.920
whether it's artificial intelligence,
uh,
medical technology,

223
00:13:28.130 --> 00:13:32.570
self driving cars,
uh,
connectivity through balloons,
on and on and on.

224
00:13:33.290 --> 00:13:35.390
And there's more coming in,
more coming.

225
00:13:35.900 --> 00:13:40.900
So one way you could stress this is that the current business is successful

226
00:13:41.091 --> 00:13:43.490
enough that we have the luxury of making bets.

227
00:13:44.540 --> 00:13:48.830
And another one that you could say is that we have the wisdom of being able to

228
00:13:48.831 --> 00:13:53.270
see that a corporate structure needs to be created to enhance the likelihood of

229
00:13:53.271 --> 00:13:54.500
the success of as bets.

230
00:13:55.220 --> 00:13:59.450
So we essentially turned ourselves into a conglomerate of bets.

231
00:13:59.480 --> 00:14:03.380
And then this underlying corporation,
Google,
which is itself innovative.

232
00:14:04.220 --> 00:14:07.670
So in order to pull this off,
you have to have a bunch of belief systems.

233
00:14:08.030 --> 00:14:12.050
And one of them is that you have to have bottoms up and tops down and bottoms
up.

234
00:14:12.051 --> 00:14:13.490
We call 20% time.

235
00:14:13.520 --> 00:14:16.610
And the idea is that people can spend 20% of the time and whatever they want.

236
00:14:16.910 --> 00:14:20.840
And the top down is that our founders in particular have a keen eye on

237
00:14:20.841 --> 00:14:23.480
technology and they're reviewing things constantly.

238
00:14:23.870 --> 00:14:27.260
So an example would be that they'll hear about an idea or I'll hear about

239
00:14:27.261 --> 00:14:28.550
something and it sounds interesting,

240
00:14:28.700 --> 00:14:33.700
let's go visit them and then let's begin to assemble the pieces to see if that's

241
00:14:33.771 --> 00:14:36.020
possible.
And if you do this long enough,

242
00:14:36.021 --> 00:14:39.110
you get pretty good at predicting what's likely to work.

243
00:14:39.780 --> 00:14:41.970
<v 0>So that's a,
that's a beautiful balance that's struck.</v>

244
00:14:42.030 --> 00:14:45.260
Is this something that applies at all scale?
So in the same

245
00:14:45.400 --> 00:14:48.040
<v 1>seems to be,
um,
that,
uh,</v>

246
00:14:49.030 --> 00:14:52.700
Sergei again,
15 years ago,
um,

247
00:14:52.830 --> 00:14:57.830
it came up with a concept called 10 10% of the budget should be on things that

248
00:14:57.891 --> 00:15:00.380
are unrelated.
It was called 70 2010,

249
00:15:00.860 --> 00:15:03.170
70% of our time on core business,

250
00:15:03.500 --> 00:15:06.230
20% on adjacent business and 10% on other.

251
00:15:06.770 --> 00:15:10.280
And he proved mathematically,
of course,
he's a brilliant mathematician,

252
00:15:10.580 --> 00:15:14.630
that you needed that 10% right to make the,
some of the growth work.

253
00:15:14.631 --> 00:15:15.560
And it turns out he was right.

254
00:15:18.600 --> 00:15:21.210
<v 0>So getting into the world of artificial intelligence,
you've,</v>

255
00:15:21.240 --> 00:15:26.240
you've talked quite extensively and effectively to the impact in the near term,

256
00:15:28.800 --> 00:15:32.910
the positive impact of artificial intelligence,
uh,
whether it's machine,

257
00:15:32.940 --> 00:15:37.940
especially machine learning in a medical applications in education and just

258
00:15:39.001 --> 00:15:41.370
making information more accessible,
right?

259
00:15:41.610 --> 00:15:45.270
In the Ai community there is a kind of debate.
Uh,

260
00:15:45.360 --> 00:15:49.560
so there's this shroud of uncertainty as we face this new world of artificial

261
00:15:49.561 --> 00:15:54.240
intelligence in it.
And there is some people,
uh,
like Elon Musk,

262
00:15:54.270 --> 00:15:57.660
you've disagreed on at least some degree of emphasis.

263
00:15:57.661 --> 00:16:02.550
He places on the existential threat of Ai.
So I've spoken with Stuart Russell,

264
00:16:02.551 --> 00:16:03.390
Max Tegmark,

265
00:16:03.391 --> 00:16:08.391
who Sherry la mosques view and Yoshua Bengio Steven pinker who do not.

266
00:16:09.170 --> 00:16:10.230
And so there's,
there's a,

267
00:16:10.231 --> 00:16:14.610
there's a lot of very smart people who are thinking about this stuff
disagreeing,

268
00:16:14.611 --> 00:16:16.650
which is really healthy.
Uh,
of course.

269
00:16:17.160 --> 00:16:21.940
So what do you think is the healthiest way for the Ai community too?
And,

270
00:16:22.170 --> 00:16:27.170
and really for the general public to think about Ai and the concern of the

271
00:16:28.471 --> 00:16:32.640
technology being mismanaged in some,
in some kind of way.

272
00:16:32.930 --> 00:16:37.250
<v 1>So the source of education for the general public has been a robot killer
movies,</v>

273
00:16:37.370 --> 00:16:40.730
right?
And Terminator,
et Cetera.

274
00:16:40.820 --> 00:16:43.160
And the one thing I can assure you,

275
00:16:43.161 --> 00:16:47.120
we're not building or those kinds of solutions.
Furthermore,

276
00:16:47.121 --> 00:16:50.840
if they were to show up,
someone would notice an unplugged them,
right?

277
00:16:51.080 --> 00:16:53.120
So as exciting as those movies are,

278
00:16:53.121 --> 00:16:57.110
and they're great movies were the killer robots to start,

279
00:16:57.530 --> 00:17:01.060
we would find a way to,
to stop them,
right?
So I'm,

280
00:17:01.061 --> 00:17:03.830
I'm not concerned about that.
Um,

281
00:17:04.040 --> 00:17:08.040
and much of this has to do with the timeframe of conversation,
right?

282
00:17:08.540 --> 00:17:13.540
So you can imagine a situation a hundred years from now when the human brain is

283
00:17:14.511 --> 00:17:19.010
fully understood and the next generation and next generation of brilliant MIT

284
00:17:19.011 --> 00:17:20.690
scientists have figured all this out.

285
00:17:20.840 --> 00:17:25.160
We're going to have a large number of ethics questions,
right?

286
00:17:25.161 --> 00:17:29.150
Around science and thinking and robots and computers and so forth and so on.

287
00:17:29.660 --> 00:17:31.730
So it depends on the question of the timeframe.

288
00:17:32.240 --> 00:17:36.560
In the next five to 10 years,
we're not facing those questions.

289
00:17:37.190 --> 00:17:40.550
What we're facing in the next five to 10 years is how do we spread this

290
00:17:40.551 --> 00:17:45.551
disruptive technology as broadly as possible to gain the maximum benefit of it.

291
00:17:46.460 --> 00:17:49.890
The primary benefits should be in healthcare and an education,

292
00:17:50.560 --> 00:17:53.220
a healthcare because it's obvious we're all the same.

293
00:17:53.221 --> 00:17:57.090
Even though we don't somehow believe we're not as a medical matter.

294
00:17:57.330 --> 00:18:01.140
The fact that we have big data,
better health,
we'll save lives,
allow us to get,

295
00:18:01.170 --> 00:18:05.460
you know,
deal with skin cancer and other cancers,
them a logical problems.

296
00:18:05.461 --> 00:18:09.390
There's people working on psychological diseases and so forth using these

297
00:18:09.391 --> 00:18:11.130
techniques.
I go on and on.

298
00:18:11.640 --> 00:18:16.410
The promise of Ai in medicine is extraordinary.
Uh,
there are many,

299
00:18:16.411 --> 00:18:20.730
many companies and startups and funds and solutions and we will all live much

300
00:18:20.731 --> 00:18:24.600
better for that.
The same argument and in education.

301
00:18:25.530 --> 00:18:29.430
Can you imagine that if for each generation of child and even adult,

302
00:18:29.700 --> 00:18:32.850
you have a tutor educator that's AI based,

303
00:18:33.030 --> 00:18:37.140
that's not a human but is properly trained that helps you get smarter,

304
00:18:37.141 --> 00:18:40.530
helps you address your language difficulties or your math difficulties or what

305
00:18:40.531 --> 00:18:42.960
have you,
why don't we focus on those two?

306
00:18:43.290 --> 00:18:48.290
The gains societaly of making humans smarter and healthier are enormous.

307
00:18:48.630 --> 00:18:48.931
Right?

308
00:18:48.931 --> 00:18:53.820
And those translate for decades and decades and we'll all benefit from them.
Um,

309
00:18:53.880 --> 00:18:56.280
there are people who are working on Ai Safety,

310
00:18:56.310 --> 00:18:59.790
which is the issue that you're describing and there are conversations in the

311
00:18:59.791 --> 00:19:03.810
community that should there be such problems.
What should the rules be like?

312
00:19:04.380 --> 00:19:08.730
Google for example,
has announced its,
uh,
policies with respect to Ai Safety,

313
00:19:08.731 --> 00:19:12.810
which I certainly support and I think most everybody would support and they make

314
00:19:12.811 --> 00:19:15.870
sense,
right?
So it helps guide the research.

315
00:19:16.290 --> 00:19:20.430
But the killer robots are not arriving this year and they're not even being

316
00:19:20.431 --> 00:19:21.264
built.

317
00:19:22.500 --> 00:19:22.690
<v 0>And,</v>

318
00:19:22.690 --> 00:19:27.690
and on that line of thinking he's at the timescale in,

319
00:19:28.300 --> 00:19:31.160
in,
in this topic or other topics,
have you found it

320
00:19:31.270 --> 00:19:32.103
<v 1>useful</v>

321
00:19:33.300 --> 00:19:36.990
<v 0>that on the business side or the intellectual side to think beyond five,</v>

322
00:19:36.991 --> 00:19:41.340
10 years to think 50 years out?
Has It ever been useful?

323
00:19:41.910 --> 00:19:42.990
<v 1>In our industry,</v>

324
00:19:42.991 --> 00:19:47.130
there are essentially no examples of 50 year predictions that have been correct.

325
00:19:48.030 --> 00:19:50.790
Um,
let's review Ai,
right?
Ai,

326
00:19:50.791 --> 00:19:54.630
which was largely invented here at MIT and a couple of other universities in the

327
00:19:54.631 --> 00:19:59.631
95619571958 the original claims were a decade or two.

328
00:20:01.290 --> 00:20:03.630
And when I was a phd student,

329
00:20:03.660 --> 00:20:08.660
I studied Ai Ebit and it entered during my looking at a period which is known as

330
00:20:08.911 --> 00:20:12.150
AI winter,
which went on for about 30 years,

331
00:20:12.750 --> 00:20:16.650
which is a whole generation of science scientists and a whole group of people

332
00:20:16.651 --> 00:20:20.310
who didn't make a lot of progress because the algorithms had not improved in the

333
00:20:20.311 --> 00:20:21.480
computers,
did not approved.

334
00:20:22.020 --> 00:20:25.590
It took some brilliant mathematicians starting with a fella named Jeff Hinton at

335
00:20:25.800 --> 00:20:30.750
Toronto and Montreal,
who basically invented this deep learning model,

336
00:20:30.751 --> 00:20:33.000
which empowers us today.
Those,

337
00:20:33.001 --> 00:20:38.001
the seminal work there was 20 years ago and in the last 10 years it's become

338
00:20:38.701 --> 00:20:43.470
popularized.
So think about the timeframes for that level of discovery.

339
00:20:43.830 --> 00:20:45.390
It's very hard to predict.

340
00:20:45.840 --> 00:20:49.210
Many people think that will be flying around and the equivalent of flying cars,

341
00:20:49.630 --> 00:20:52.770
um,
who knows.
My own view,
uh,

342
00:20:52.900 --> 00:20:56.800
if I want to go out on a limb is to say that when we,
we know a couple of things,

343
00:20:56.801 --> 00:20:59.740
about 50 years from now,
we know that there'll be more people alive.

344
00:21:00.400 --> 00:21:03.850
We know that we'll have to have platforms that are more sustainable because the

345
00:21:03.851 --> 00:21:06.880
earth is limited in the ways we all know.

346
00:21:07.300 --> 00:21:11.350
And that the kind of platforms that are going to get billed will be consistent

347
00:21:11.351 --> 00:21:12.910
with the principles that I've described.

348
00:21:12.940 --> 00:21:15.670
They will be much more empowering of individuals.

349
00:21:15.671 --> 00:21:18.520
They'll be much more sensitive to the ecology because they have to be,

350
00:21:18.970 --> 00:21:19.840
they just have to be.

351
00:21:20.500 --> 00:21:23.110
I also think that humans are going to be a great deal smarter,

352
00:21:23.710 --> 00:21:26.680
and I think they're going to be a lot smarter because of the tools that I've,
uh,

353
00:21:26.681 --> 00:21:29.170
that I've discussed with you.
And of course people will live longer.

354
00:21:29.171 --> 00:21:31.750
Life extension is continuing a pace.

355
00:21:32.170 --> 00:21:35.980
A baby born today has a reasonable chance of living to 100.
Right,

356
00:21:36.010 --> 00:21:38.560
which is pretty exciting.
Well passed the 21st century,

357
00:21:38.561 --> 00:21:39.760
so we better take care of them.

358
00:21:40.570 --> 00:21:45.040
<v 0>And you mentioned an interesting statistic and uh,
some very large percentage,
60,</v>

359
00:21:45.041 --> 00:21:47.830
70% of people may live in cities.
Yeah.

360
00:21:48.100 --> 00:21:50.410
<v 1>Today more than half the world lives in cities.</v>

361
00:21:50.440 --> 00:21:55.440
And one of the great stories of humanity in the last 20 years has been the rural

362
00:21:55.901 --> 00:21:58.960
to urban migration.
This has occurred in the United States.

363
00:21:59.170 --> 00:22:02.680
It's occurred in Europe,
it's occurring in Asia,

364
00:22:02.740 --> 00:22:05.950
and it's occurring in Africa.
When people move to cities,

365
00:22:05.951 --> 00:22:10.030
the cities get more crowded.
But believe it or not,
their health gets better,

366
00:22:10.180 --> 00:22:15.100
their productivity gets better,
their Iq and educational capabilities improve.

367
00:22:15.400 --> 00:22:19.420
So it's good news that people are moving to cities that we have to make them

368
00:22:19.421 --> 00:22:20.680
livable and safe.

369
00:22:22.660 --> 00:22:25.630
<v 0>So you,
you,
first of all you are,</v>

370
00:22:25.870 --> 00:22:29.350
but you've also worked with some of the greatest leaders in the history of tech.

371
00:22:29.950 --> 00:22:34.950
What insights do you draw from the difference in leadership styles of yourself?

372
00:22:35.650 --> 00:22:40.110
Steve Jobs,
Elon Musk,
Larry Page now than UCL.

373
00:22:40.220 --> 00:22:44.950
Sundar Pichai and others from the,
I would say,

374
00:22:45.010 --> 00:22:48.610
calm sages to the mad geniuses.

375
00:22:49.480 --> 00:22:53.470
<v 1>One of the things that I learned as a young executive is that there's no single</v>

376
00:22:53.471 --> 00:22:57.400
formula for leadership.
Um,
they try to teach one,

377
00:22:57.760 --> 00:22:59.380
but that's not how it really works.

378
00:22:59.950 --> 00:23:03.370
There are people who just understand what they need to do and they need to do it

379
00:23:03.371 --> 00:23:06.580
quickly.
Those tapes,
people are often entrepreneurs.

380
00:23:06.760 --> 00:23:08.500
They just know and they move fast.

381
00:23:08.990 --> 00:23:11.560
There are other people who are systems thinkers and planners.

382
00:23:11.590 --> 00:23:16.390
That's more who I am somewhat more conservative,
more thorough in execution,

383
00:23:16.750 --> 00:23:18.190
a little bit more risk averse.

384
00:23:18.640 --> 00:23:22.150
There's also people who are sort of slightly insane,
right?

385
00:23:22.151 --> 00:23:27.151
In the sense that they are emphatic and charismatic and they feel it and they

386
00:23:27.251 --> 00:23:31.120
drive it and so forth.
There's no single formula to success.

387
00:23:31.360 --> 00:23:34.270
There is one thing that unifies all of the people that you named,

388
00:23:34.540 --> 00:23:38.530
which is very high intelligence,
right?
At the end of the day,

389
00:23:39.010 --> 00:23:43.030
the thing that characterizes all of them is that they saw the world quicker,

390
00:23:43.031 --> 00:23:44.950
faster.
They processed information.

391
00:23:45.710 --> 00:23:47.930
They didn't necessarily make the right decisions all the time,

392
00:23:48.200 --> 00:23:51.230
but they were on top of it.
And the other thing that's interesting,

393
00:23:51.231 --> 00:23:53.540
but all those people is,
they all started young.

394
00:23:54.170 --> 00:23:58.640
So think about Steve Jobs started starting apple roughly at 18 or 19 I think

395
00:23:58.641 --> 00:24:03.641
about Bill Gates starting at roughly 2021 think about by the time they were 30

396
00:24:03.710 --> 00:24:08.450
mark Zuckerberg and more a good example at 1920 by the time they were 30 they

397
00:24:08.451 --> 00:24:11.540
had 10 years at 30 years old.

398
00:24:11.541 --> 00:24:16.370
They had 10 years of experience of dealing with people and products and

399
00:24:16.371 --> 00:24:19.370
shipments and the press and business and so forth.

400
00:24:19.760 --> 00:24:23.990
It's incredible how much experience they had compared to the rest of us who are

401
00:24:23.991 --> 00:24:26.930
busy getting our PhDs.
Yes,
exactly.
So we,
we,

402
00:24:27.110 --> 00:24:31.280
we should celebrate these people because they've just had more life experience.

403
00:24:31.940 --> 00:24:35.960
Right?
And that helps inform the judgment.
At the end of the day,

404
00:24:37.340 --> 00:24:39.560
when you're at the top of these organizations,

405
00:24:40.040 --> 00:24:43.040
all the easy questions have been dealt with,
right?

406
00:24:43.490 --> 00:24:45.350
How should we design the buildings?

407
00:24:45.620 --> 00:24:49.940
Where should we put the colors on our product?
What should the box look like?

408
00:24:50.360 --> 00:24:54.440
Right?
The problems.
That's why it's so interesting to be in these rooms.

409
00:24:54.470 --> 00:24:58.040
The problems that they face,
right?
In terms of the way they operate,

410
00:24:58.340 --> 00:25:00.080
the way they deal with their employees,

411
00:25:00.081 --> 00:25:03.650
their customers or innovation are profoundly challenging.

412
00:25:03.920 --> 00:25:08.450
Each of the companies is demonstrably different culturally,

413
00:25:09.130 --> 00:25:11.390
right?
They are not in fact cut of the same.

414
00:25:11.720 --> 00:25:15.860
They behave differently based on input.
Their internal cultures are different,

415
00:25:15.861 --> 00:25:18.770
their compensation schemes are different,
their values are different.

416
00:25:19.280 --> 00:25:21.800
So there's proof that diversity works.

417
00:25:24.710 --> 00:25:25.543
<v 0>So,</v>

418
00:25:26.600 --> 00:25:31.010
so when faced with a tough decision in need of advice,

419
00:25:31.880 --> 00:25:35.960
it's been said that the best thing one can do is to find the best person in the

420
00:25:35.961 --> 00:25:40.961
world who can give that advice and find a way to be in a room with them one on

421
00:25:42.411 --> 00:25:45.620
one and ask.
So here we are.

422
00:25:45.970 --> 00:25:48.980
And let me ask in a long winded way,
I wrote this down

423
00:25:50.720 --> 00:25:55.460
in 1998,
there were many good search engines.
Lycos excite,
Altavista,

424
00:25:55.461 --> 00:26:00.170
infoseek as Jeeves,
maybe.
Uh,
Yahoo even.

425
00:26:01.210 --> 00:26:04.490
Uh,
so Google stepped in and disrupted everything.

426
00:26:04.660 --> 00:26:08.810
It disrupted the nature of search,
the nature of our access to information,

427
00:26:08.840 --> 00:26:10.490
the way we discover new knowledge.

428
00:26:11.840 --> 00:26:15.200
So now it's 2018 actually 20 years later,

429
00:26:15.980 --> 00:26:18.350
there are many good personal AI assistance,

430
00:26:18.710 --> 00:26:20.840
including of course the bus from Google.

431
00:26:22.220 --> 00:26:25.490
So you've spoken a in,
in medical and education,

432
00:26:25.491 --> 00:26:30.020
the impact of such an AI assistant could bring.
So we arrive at this question,

433
00:26:30.290 --> 00:26:31.820
so it's a personal one for me,

434
00:26:32.120 --> 00:26:36.140
but I hope my situation represents that of many other,

435
00:26:37.100 --> 00:26:40.550
as we said,
dreamers and the crazy engineers.

436
00:26:41.030 --> 00:26:45.390
So my whole life I have dreamed of creating session as assistant.

437
00:26:46.330 --> 00:26:48.660
Every step I've taken has been towards that goal.

438
00:26:48.930 --> 00:26:52.380
Now I'm a research scientist in human centered AI here at Mit.

439
00:26:52.800 --> 00:26:56.760
So the next step for me as I sit here,
sit facing my passion,

440
00:26:58.110 --> 00:27:00.870
is to do what Larry and Sergei did it 98

441
00:27:02.850 --> 00:27:06.390
simple startup.
And so here's my simple question.

442
00:27:06.810 --> 00:27:08.520
Given the low odds of success,

443
00:27:08.521 --> 00:27:12.270
the timing and lock required the calmness of the factors that can't be

444
00:27:12.271 --> 00:27:15.960
controlled or predicted,
which is all the things that Larry and Sergei faced.

445
00:27:16.440 --> 00:27:17.970
Is there some calculations,

446
00:27:17.971 --> 00:27:22.971
some strategy to follow in this step or do you simply follow the passion just

447
00:27:23.881 --> 00:27:27.450
because there's no other choice?
I think the

448
00:27:27.850 --> 00:27:32.850
<v 1>people who are in universities are always trying to study the extraordinarily</v>

449
00:27:32.951 --> 00:27:36.670
chaotic nature of innovation and entrepreneurship.

450
00:27:37.210 --> 00:27:42.010
My answer is that they didn't have that conversation.
They just did it.

451
00:27:42.790 --> 00:27:46.900
They sensed a moment when in the case of Google,

452
00:27:47.200 --> 00:27:50.440
there was all of this data that needed to be organized and they had a better

453
00:27:50.441 --> 00:27:53.110
algorithm.
They had invented a better way.

454
00:27:53.740 --> 00:27:57.640
So today with human centered Ai,
which is your area of research,

455
00:27:58.030 --> 00:28:02.410
there must be new approaches.
It's such a big field.

456
00:28:02.440 --> 00:28:07.210
There must be new approaches different from what we and others are doing.

457
00:28:07.211 --> 00:28:11.680
There must be startups to fund.
There must be research projects to try.

458
00:28:11.890 --> 00:28:14.350
There must be graduate students to work on new approaches.

459
00:28:15.010 --> 00:28:18.880
Here at MIT there are people who are looking at learning from the standpoint of

460
00:28:18.881 --> 00:28:20.550
looking at child learning,
right?

461
00:28:20.560 --> 00:28:23.980
How do children learn starting at each one and the bottom and others and the

462
00:28:23.981 --> 00:28:25.060
work is fantastic.

463
00:28:25.300 --> 00:28:29.410
Those approaches are different from the approach that most people are taking.

464
00:28:29.740 --> 00:28:33.250
Perhaps that's a bet that you should make,
or perhaps there's another one.

465
00:28:33.760 --> 00:28:35.320
But at the end of the day,

466
00:28:35.540 --> 00:28:39.490
the successful entrepreneurs are not as crazy as they sound.

467
00:28:40.090 --> 00:28:44.530
They see an opportunity based on what's happened.
Let's use Uber as an example.

468
00:28:45.280 --> 00:28:46.540
As Travis tells the story,

469
00:28:46.840 --> 00:28:50.170
he and his cofounder was sitting in Paris and they had this idea because they

470
00:28:50.171 --> 00:28:52.750
couldn't get a cab and they said,

471
00:28:52.870 --> 00:28:56.320
we have smartphones and the rest is history.

472
00:28:56.650 --> 00:29:00.940
So what's the equivalent of that Travis Eiffel Tower?

473
00:29:00.960 --> 00:29:05.740
Where as a cab moment that you could as an entrepreneur or take advantage of

474
00:29:05.920 --> 00:29:06.880
whether it's in humans,

475
00:29:06.890 --> 00:29:09.940
entered AI or something else that's the next great startup

476
00:29:11.220 --> 00:29:13.560
<v 0>and the psychology of that moment.</v>

477
00:29:13.620 --> 00:29:18.620
So when Sergei and Larry Talk about it in listened to a few interviews,

478
00:29:18.871 --> 00:29:21.000
is very nonchalant.
Well here's,

479
00:29:21.510 --> 00:29:25.710
here's the very fascinating web data and uh,

480
00:29:25.830 --> 00:29:27.720
here's an algorithm.
We have four.
You know,

481
00:29:27.721 --> 00:29:30.360
we just kind of want to play around with that data and it seems like that's a

482
00:29:30.361 --> 00:29:34.160
really nice way to organize this data.
Say

483
00:29:34.170 --> 00:29:35.100
<v 1>I should say,
what happened,</v>

484
00:29:35.101 --> 00:29:38.680
remember is that they were graduate students at Stanford and they thought there

485
00:29:38.681 --> 00:29:39.301
was this interesting.

486
00:29:39.301 --> 00:29:44.301
So they built a search engine and they kept it in their room and they had to get

487
00:29:44.741 --> 00:29:48.010
power from the room next door because they were using too much power in the
room.

488
00:29:48.011 --> 00:29:51.090
So they ran an extension cord over,
right?
Yeah.

489
00:29:51.460 --> 00:29:55.330
And then they went and they found a house and they had Google world headquarters

490
00:29:55.331 --> 00:29:56.740
of five people,
right.

491
00:29:56.741 --> 00:30:00.790
To start the company and they raised $100,000 from Andy Ductile shy who was the

492
00:30:00.791 --> 00:30:04.000
son founder to do this and Dave Chariton and a few others.

493
00:30:04.450 --> 00:30:07.570
The point is their beginnings were very simple,

494
00:30:08.200 --> 00:30:10.300
but they were based on a powerful insight.

495
00:30:11.680 --> 00:30:16.420
That is a replicable model for any startup you has to be a powerful insight.

496
00:30:16.450 --> 00:30:20.160
The beginnings are simple and there has to be an innovation in,

497
00:30:20.170 --> 00:30:23.950
in [inaudible] case it was page rank,
which is a brilliant idea.

498
00:30:23.951 --> 00:30:27.640
One of the most cited papers in,
in the world today.
What's the next one?

499
00:30:29.700 --> 00:30:34.700
<v 0>So you're one of five may say richest people in the world and yet it seems that</v>

500
00:30:37.081 --> 00:30:41.790
money is simply a side effect of your passions and not an inherent goal,

501
00:30:42.930 --> 00:30:47.340
but it's a,
you're a fascinating person to ask.

502
00:30:48.210 --> 00:30:53.210
So much of our society at the individual level and at the company level and this

503
00:30:53.780 --> 00:30:57.240
nations is driven by the desire for wealth.

504
00:30:58.650 --> 00:31:02.880
What do you think about this drive and what have you learned about?

505
00:31:03.170 --> 00:31:04.980
I me romanticize the notion,

506
00:31:05.010 --> 00:31:09.870
the meaning of life having achieved success on so many dimensions.

507
00:31:10.240 --> 00:31:10.390
<v 1>Well,</v>

508
00:31:10.390 --> 00:31:15.390
there've been many studies of human happiness and above some threshold which is

509
00:31:16.631 --> 00:31:19.150
typically relatively low for this conversation.

510
00:31:19.350 --> 00:31:23.290
There is no difference in happiness about and money.

511
00:31:23.291 --> 00:31:28.210
It's the happiness is correlated with meaning and purpose,
a sense of family,

512
00:31:28.240 --> 00:31:31.840
a sense of impact.
So if you organize your life,

513
00:31:31.900 --> 00:31:35.500
assuming you have enough to get around and having a nice home and so forth,

514
00:31:35.860 --> 00:31:40.860
you'll be far happier if you figure out what you care about and work on that

515
00:31:41.620 --> 00:31:46.300
it's often being in service to others is great deal of evidence that people are

516
00:31:46.301 --> 00:31:49.000
happiest when they're serving others and not themselves.

517
00:31:49.540 --> 00:31:53.590
This goes directly against the sort of,
uh,
press induced,

518
00:31:53.950 --> 00:31:58.440
uh,
excitement about powerful and wealthy leaders,
uh,

519
00:31:58.510 --> 00:32:01.180
of one car.
And indeed,
these are consequential people.

520
00:32:01.690 --> 00:32:05.770
But if you are in a situation where you've been very fortunate as I have,

521
00:32:06.070 --> 00:32:10.960
you also have to take that as a responsibility and you have to basically work

522
00:32:10.961 --> 00:32:13.360
both to educate others and give them that opportunity,

523
00:32:13.540 --> 00:32:17.350
but also use that wealth to advance human society.
In my case,

524
00:32:17.351 --> 00:32:20.650
I'm particularly interested in using the tools of artificial intelligence and

525
00:32:20.651 --> 00:32:25.120
machine learning to make society better.
I've mentioned education.
I mentioned,
uh,

526
00:32:25.180 --> 00:32:27.730
inequality and middle class and things like this,

527
00:32:28.000 --> 00:32:31.600
all of which are a passion of mine.
It doesn't matter what you do,

528
00:32:31.810 --> 00:32:33.430
it matters that you believe in it,

529
00:32:33.670 --> 00:32:38.350
that it's important to you and that your life will be far more satisfying if you

530
00:32:38.351 --> 00:32:39.184
spend your life doing.

531
00:32:40.520 --> 00:32:45.200
<v 0>I think there's no better place to,
and then a discussion of the meaning of life.</v>

532
00:32:45.230 --> 00:32:46.960
Eric,
thank you so very much.

