WEBVTT

1
00:00:00.060 --> 00:00:01.740
<v Speaker 1>Welcome back to six,</v>
<v Speaker 1>says zero,</v>

2
00:00:01.741 --> 00:00:04.050
<v Speaker 1>nine,</v>
<v Speaker 1>nine artificial general intelligence.</v>

3
00:00:04.080 --> 00:00:09.080
<v Speaker 1>Today we have Eylea says Giver,</v>
<v Speaker 1>Cofounder and Research Director of Open </v>

4
00:00:12.721 --> 00:00:15.900
<v Speaker 1>Ai.</v>
<v Speaker 1>He started in the aml group in Toronto.</v>

5
00:00:15.901 --> 00:00:18.990
<v Speaker 1>Geoffrey Hinton,</v>
<v Speaker 1>then at Stanford with Andrew Wang.</v>

6
00:00:19.080 --> 00:00:24.080
<v Speaker 1>Co founded DNN research for three years </v>
<v Speaker 1>as a research scientist at Google brain </v>

7
00:00:24.080 --> 00:00:28.290
<v Speaker 1>and finally cofounded open ai citations </v>
<v Speaker 1>aren't everything,</v>

8
00:00:28.590 --> 00:00:32.190
<v Speaker 1>but they do indicate impact.</v>
<v Speaker 1>And his work,</v>

9
00:00:32.370 --> 00:00:37.370
<v Speaker 1>recent work in the past five years has </v>
<v Speaker 1>been cited over 46,000</v>

10
00:00:38.131 --> 00:00:43.131
<v Speaker 1>times.</v>
<v Speaker 1>He has been the key creative intellect </v>

11
00:00:43.131 --> 00:00:46.630
<v Speaker 1>and driver behind some of the biggest </v>
<v Speaker 1>breakthrough ideas and deep learning and</v>

12
00:00:46.631 --> 00:00:51.631
<v Speaker 1>artificial intelligence ever.</v>
<v Speaker 1>So please welcome Eylea.</v>

13
00:00:56.890 --> 00:00:58.680
<v Speaker 2>Thanks.</v>

14
00:00:59.460 --> 00:01:01.080
<v Speaker 1>Alright,</v>
<v Speaker 1>thanks for the introduction next.</v>

15
00:01:02.220 --> 00:01:04.410
<v Speaker 1>Alright,</v>
<v Speaker 1>thanks for coming to my talk.</v>

16
00:01:04.530 --> 00:01:09.530
<v Speaker 1>I will tell you about some work we've </v>
<v Speaker 1>done over the past year on on Melania </v>

17
00:01:09.761 --> 00:01:14.761
<v Speaker 1>and self at open Ai and before I dive </v>
<v Speaker 1>into some of the more technical details </v>

18
00:01:15.961 --> 00:01:20.961
<v Speaker 1>of the work,</v>
<v Speaker 1>I wanted to spend a little bit of time </v>

19
00:01:20.961 --> 00:01:24.500
<v Speaker 1>talking about deep learning and why it </v>
<v Speaker 1>works at all in the first place,</v>

20
00:01:26.010 --> 00:01:28.710
<v Speaker 1>which I think it's actually not as self </v>
<v Speaker 1>evident,</v>

21
00:01:28.760 --> 00:01:33.760
<v Speaker 1>seeing that they should work.</v>
<v Speaker 1>One fact it's actually a fact,</v>

22
00:01:35.010 --> 00:01:40.010
<v Speaker 1>it's a mathematical theory that you can </v>
<v Speaker 1>prove is that if you could find the this</v>

23
00:01:42.240 --> 00:01:46.740
<v Speaker 1>program does very,</v>
<v Speaker 1>very well in your data,</v>

24
00:01:47.310 --> 00:01:52.310
<v Speaker 1>then you will achieve the best </v>
<v Speaker 1>generalization possible with a little </v>

25
00:01:52.310 --> 00:01:56.121
<v Speaker 1>bit of modification.</v>
<v Speaker 1>You can turn it into a precise theory </v>

26
00:01:56.121 --> 00:01:59.390
<v Speaker 1>and on a very intuitive level,</v>
<v Speaker 1>it's easy to see why it shouldn't be the</v>

27
00:01:59.551 --> 00:02:00.090
<v Speaker 1>case.</v>

28
00:02:00.900 --> 00:02:05.900
<v Speaker 1>If you have some data and you're able to</v>
<v Speaker 1>find a short term program which </v>

29
00:02:06.121 --> 00:02:11.121
<v Speaker 1>generates this data,</v>
<v Speaker 1>then you've essentially extracted all </v>

30
00:02:11.121 --> 00:02:14.631
<v Speaker 1>the old conceivable regularity from this</v>
<v Speaker 1>data into your program and then you can </v>

31
00:02:14.631 --> 00:02:16.560
<v Speaker 1>use this objects to make the best </v>
<v Speaker 1>predictions possible.</v>

32
00:02:18.050 --> 00:02:23.050
<v Speaker 1>If if you have data which is so complex </v>
<v Speaker 1>and there is no way to express it as a </v>

33
00:02:24.361 --> 00:02:27.660
<v Speaker 1>shorter program and it means that your </v>
<v Speaker 1>data is totally random,</v>

34
00:02:28.170 --> 00:02:31.080
<v Speaker 1>there is no way to extract any </v>
<v Speaker 1>irregularity from it whatsoever.</v>

35
00:02:32.370 --> 00:02:37.370
<v Speaker 1>Now,</v>
<v Speaker 1>there is little known mathematical </v>

36
00:02:37.370 --> 00:02:37.370
<v Speaker 1>theory behind this and the proofs of </v>
<v Speaker 1>these statements.</v>

37
00:02:37.370 --> 00:02:38.400
<v Speaker 1>Actually,</v>
<v Speaker 1>not even that hard,</v>

38
00:02:39.170 --> 00:02:44.170
<v Speaker 1>but the one minor,</v>
<v Speaker 1>slight disappointment is that it's </v>

39
00:02:44.170 --> 00:02:48.051
<v Speaker 1>actually not possible,</v>
<v Speaker 1>at least given today's tools and </v>

40
00:02:48.051 --> 00:02:50.931
<v Speaker 1>understanding to find the best short </v>
<v Speaker 1>program that explains or generates or </v>

41
00:02:52.830 --> 00:02:55.080
<v Speaker 1>solves your problem.</v>
<v Speaker 1>Given your data.</v>

42
00:02:55.710 --> 00:02:57.650
<v Speaker 1>This problem is computationally </v>
<v Speaker 1>intractable.</v>

43
00:02:59.270 --> 00:03:03.460
<v Speaker 1>The space of old programs is a very </v>
<v Speaker 1>nasty space.</v>

44
00:03:03.970 --> 00:03:08.970
<v Speaker 1>Small changes to a program result in </v>
<v Speaker 1>massive changes in the behavior of the </v>

45
00:03:08.970 --> 00:03:09.760
<v Speaker 1>program as it should be.</v>
<v Speaker 1>It makes sense.</v>

46
00:03:10.090 --> 00:03:12.940
<v Speaker 1>We have a loop.</v>
<v Speaker 1>We changed the inside of the loop.</v>

47
00:03:13.420 --> 00:03:14.950
<v Speaker 1>Of course you get something totally </v>
<v Speaker 1>different,</v>

48
00:03:15.760 --> 00:03:19.300
<v Speaker 1>so the space of programs is so hard,</v>
<v Speaker 1>at least given what we know today,</v>

49
00:03:19.301 --> 00:03:24.301
<v Speaker 1>search,</v>
<v Speaker 1>there seems to be completely off the </v>

50
00:03:24.301 --> 00:03:24.750
<v Speaker 1>table.</v>
<v Speaker 1>Well,</v>

51
00:03:25.780 --> 00:03:28.720
<v Speaker 1>if we give up on the short side on short</v>
<v Speaker 1>programs,</v>

52
00:03:28.870 --> 00:03:32.290
<v Speaker 1>what about small circuits?</v>
<v Speaker 1>Well,</v>

53
00:03:32.770 --> 00:03:36.520
<v Speaker 1>it turns out that we are lucky.</v>
<v Speaker 1>It turns out that when it comes to small</v>

54
00:03:36.521 --> 00:03:41.521
<v Speaker 1>circuits,</v>
<v Speaker 1>you can just find the best small </v>

55
00:03:41.521 --> 00:03:42.310
<v Speaker 1>circuits circuit that solves a problem </v>
<v Speaker 1>using backpropagation,</v>

56
00:03:43.030 --> 00:03:48.030
<v Speaker 1>and this is the miraculous fact on which</v>
<v Speaker 1>the rest of ai stands.</v>

57
00:03:49.510 --> 00:03:54.510
<v Speaker 1>It is the fact that then you have a </v>
<v Speaker 1>circuit and you impose constraints on </v>

58
00:03:54.510 --> 00:03:56.230
<v Speaker 1>your circuits.</v>
<v Speaker 1>On your circuit using data,</v>

59
00:03:56.560 --> 00:03:59.860
<v Speaker 1>you can find a way to satisfy these </v>
<v Speaker 1>constraints,</v>

60
00:04:00.340 --> 00:04:05.340
<v Speaker 1>these constraints using backdrop by </v>
<v Speaker 1>iteratively making small changes to the </v>

61
00:04:06.260 --> 00:04:11.260
<v Speaker 1>weights of your neural network until its</v>
<v Speaker 1>predictions satisfy the data.</v>

62
00:04:13.060 --> 00:04:18.060
<v Speaker 1>Well,</v>
<v Speaker 1>this means is that the computational </v>

63
00:04:18.060 --> 00:04:18.820
<v Speaker 1>problem that sober backpropagation is </v>
<v Speaker 1>extremely profound.</v>

64
00:04:19.630 --> 00:04:24.630
<v Speaker 1>It is circuit search.</v>
<v Speaker 1>Now we know that you can solve with </v>

65
00:04:24.630 --> 00:04:27.660
<v Speaker 1>solid all this,</v>
<v Speaker 1>but you can solve it sometimes and you </v>

66
00:04:28.091 --> 00:04:31.810
<v Speaker 1>can solve it that those times where we </v>
<v Speaker 1>have a practical data set,</v>

67
00:04:32.170 --> 00:04:37.170
<v Speaker 1>it is easy to design artificial data </v>
<v Speaker 1>sets for which you not find the best </v>

68
00:04:37.170 --> 00:04:40.711
<v Speaker 1>neural network,</v>
<v Speaker 1>but in practice it seems to be not a </v>

69
00:04:40.711 --> 00:04:43.801
<v Speaker 1>problem.</v>
<v Speaker 1>Are you going to think of training </v>

70
00:04:43.801 --> 00:04:46.350
<v Speaker 1>neural network as solving an equation?</v>
<v Speaker 1>In many cases where you have a large </v>

71
00:04:47.531 --> 00:04:50.750
<v Speaker 1>number of equation terms like this,</v>
<v Speaker 1>Fox,</v>

72
00:04:50.880 --> 00:04:55.880
<v Speaker 1>Scifi equals way,</v>
<v Speaker 1>so you've got your parameters and they </v>

73
00:04:55.880 --> 00:04:59.331
<v Speaker 1>are present all your degrees of freedom </v>
<v Speaker 1>and you use gradient descent to push the</v>

74
00:05:00.671 --> 00:05:04.210
<v Speaker 1>information from these equations into </v>
<v Speaker 1>the parameters of satisfy them all.</v>

75
00:05:05.620 --> 00:05:10.620
<v Speaker 1>And you can see that the neural network,</v>
<v Speaker 1>let's say one with 50 layers is </v>

76
00:05:10.620 --> 00:05:15.391
<v Speaker 1>basically a parallel computer that is </v>
<v Speaker 1>given 50 times steps to run and you can </v>

77
00:05:17.771 --> 00:05:19.760
<v Speaker 1>look quite a lot to be the 50 slash 50 </v>
<v Speaker 1>times step,</v>

78
00:05:19.761 --> 00:05:21.400
<v Speaker 1>sort of very,</v>
<v Speaker 1>very powerful.</v>

79
00:05:21.590 --> 00:05:24.700
<v Speaker 1>I'm massively parallel computer.</v>
<v Speaker 1>So for example,</v>

80
00:05:25.980 --> 00:05:30.980
<v Speaker 1>I think it is not widely known that you </v>
<v Speaker 1>can learn to sort sort an n beat numbers</v>

81
00:05:36.190 --> 00:05:39.610
<v Speaker 1>using a modestly sized neural network </v>
<v Speaker 1>with just two hidden layers,</v>

82
00:05:40.540 --> 00:05:43.780
<v Speaker 1>which is not bad.</v>
<v Speaker 1>It's not self evident.</v>

83
00:05:44.680 --> 00:05:49.680
<v Speaker 1>Especially since we've been taught that </v>
<v Speaker 1>sorting requires logan parallel steps </v>

84
00:05:49.680 --> 00:05:54.600
<v Speaker 1>with a neural network,</v>
<v Speaker 1>you can sort successful using only two </v>

85
00:05:54.600 --> 00:05:58.231
<v Speaker 1>parallel steps.</v>
<v Speaker 1>So there's some things like an obvious </v>

86
00:05:58.231 --> 00:06:01.020
<v Speaker 1>going on now these a part of those steps</v>
<v Speaker 1>of thresholds for threshold neurons,</v>

87
00:06:01.180 --> 00:06:04.190
<v Speaker 1>so they're doing a little bit more work.</v>
<v Speaker 1>That's an answer to the mystery,</v>

88
00:06:04.430 --> 00:06:07.160
<v Speaker 1>but if you've got 50 such layers,</v>
<v Speaker 1>you can do quite a bit of logic,</v>

89
00:06:07.280 --> 00:06:09.650
<v Speaker 1>quite a bit of reasoning all inside the </v>
<v Speaker 1>neural network.</v>

90
00:06:10.250 --> 00:06:13.460
<v Speaker 1>And that's why it works.</v>
<v Speaker 1>Given the data,</v>

91
00:06:14.600 --> 00:06:19.600
<v Speaker 1>we are able to find the best neural </v>
<v Speaker 1>network and because the neural network </v>

92
00:06:19.600 --> 00:06:22.250
<v Speaker 1>is deep because it can run computation </v>
<v Speaker 1>inside of inside of its layers,</v>

93
00:06:22.910 --> 00:06:26.900
<v Speaker 1>the best neural network is worth finding</v>
<v Speaker 1>because that's really what you need.</v>

94
00:06:27.050 --> 00:06:32.050
<v Speaker 1>You need something you need to model </v>
<v Speaker 1>class which is worth optimizing,</v>

95
00:06:33.410 --> 00:06:38.410
<v Speaker 1>but it also needs to be optimized.</v>
<v Speaker 1>Mobile and deep neural networks </v>

96
00:06:38.410 --> 00:06:40.700
<v Speaker 1>satisfied both of these constraints,</v>
<v Speaker 1>and this is why everything works,</v>

97
00:06:40.701 --> 00:06:43.670
<v Speaker 1>so this is the basis on which everything</v>
<v Speaker 1>else resides.</v>

98
00:06:45.350 --> 00:06:50.350
<v Speaker 1>Now,</v>
<v Speaker 1>I wanted to talk a little bit about </v>

99
00:06:50.350 --> 00:06:50.350
<v Speaker 1>reinforcement learning.</v>
<v Speaker 1>Reinforcement learning is a framework.</v>

100
00:06:50.750 --> 00:06:55.750
<v Speaker 1>It's a framework of evaluating agents in</v>
<v Speaker 1>their ability to achieve goals and </v>

101
00:06:56.381 --> 00:07:01.381
<v Speaker 1>complicated stochastic environments.</v>
<v Speaker 1>You've got an agent which is plugged </v>

102
00:07:01.381 --> 00:07:05.411
<v Speaker 1>into an environment as shown in the </v>
<v Speaker 1>figure right here and for any given </v>

103
00:07:06.651 --> 00:07:11.651
<v Speaker 1>agent,</v>
<v Speaker 1>you can simply run it many times and </v>

104
00:07:11.651 --> 00:07:13.160
<v Speaker 1>computers average award.</v>
<v Speaker 1>Now,</v>

105
00:07:13.250 --> 00:07:16.850
<v Speaker 1>the thing that's interesting about the </v>
<v Speaker 1>reinforcement learning framework is that</v>

106
00:07:16.851 --> 00:07:21.851
<v Speaker 1>there exist interesting,</v>
<v Speaker 1>useful reinforcement learning </v>

107
00:07:21.851 --> 00:07:24.500
<v Speaker 1>algorithms.</v>
<v Speaker 1>The framework existed for a long time.</v>

108
00:07:25.310 --> 00:07:28.820
<v Speaker 1>It became interesting once we realized </v>
<v Speaker 1>that good algorithms exist.</v>

109
00:07:28.970 --> 00:07:30.650
<v Speaker 1>Now,</v>
<v Speaker 1>these are perfect algorithms,</v>

110
00:07:30.920 --> 00:07:35.920
<v Speaker 1>but they're good enough to do </v>
<v Speaker 1>interesting things and all you want.</v>

111
00:07:36.650 --> 00:07:41.650
<v Speaker 1>The mathematical problem is one where </v>
<v Speaker 1>you need to maximize the expected </v>

112
00:07:41.650 --> 00:07:41.650
<v Speaker 1>reward.</v>

113
00:07:43.280 --> 00:07:48.280
<v Speaker 1>Now,</v>
<v Speaker 1>one important way in which the </v>

114
00:07:48.280 --> 00:07:51.221
<v Speaker 1>reinforcement learning framework is not </v>
<v Speaker 1>quite complete is that it assumes that </v>

115
00:07:51.221 --> 00:07:53.450
<v Speaker 1>the reward is given by the environment.</v>
<v Speaker 1>You see this picture,</v>

116
00:07:54.090 --> 00:07:58.040
<v Speaker 1>the agent sends an action while the </v>
<v Speaker 1>reward sends it the observation,</v>

117
00:07:58.100 --> 00:08:00.980
<v Speaker 1>and they're both of the observation and </v>
<v Speaker 1>the reward backwards.</v>

118
00:08:01.460 --> 00:08:03.020
<v Speaker 1>That's what the environment communicates</v>
<v Speaker 1>back.</v>

119
00:08:04.410 --> 00:08:09.410
<v Speaker 1>One way in which this is not the case in</v>
<v Speaker 1>the real world is that we figure out </v>

120
00:08:11.990 --> 00:08:15.410
<v Speaker 1>what the reward is from the observation </v>
<v Speaker 1>we reward ourselves.</v>

121
00:08:15.850 --> 00:08:17.630
<v Speaker 1>We are not told.</v>
<v Speaker 1>Environment doesn't say,</v>

122
00:08:17.631 --> 00:08:19.190
<v Speaker 1>Hey,</v>
<v Speaker 1>here's some negative reward.</v>

123
00:08:20.180 --> 00:08:25.180
<v Speaker 1>It's our interpretation over census that</v>
<v Speaker 1>lets us determine what the reward is and</v>

124
00:08:26.151 --> 00:08:31.151
<v Speaker 1>there is only one real reward in life </v>
<v Speaker 1>and this is existence or nonexistence </v>

125
00:08:31.151 --> 00:08:32.570
<v Speaker 1>and everything else is a corollary of </v>
<v Speaker 1>that.</v>

126
00:08:33.920 --> 00:08:36.980
<v Speaker 1>So well,</v>
<v Speaker 1>what should the region be?</v>

127
00:08:37.010 --> 00:08:42.010
<v Speaker 1>You already know the answers should be a</v>
<v Speaker 1>your own network because whenever you </v>

128
00:08:42.010 --> 00:08:45.851
<v Speaker 1>want to do something,</v>
<v Speaker 1>then it's going to be a neural network </v>

129
00:08:45.851 --> 00:08:48.491
<v Speaker 1>and you want the agents to map </v>
<v Speaker 1>observations to actions so you let it be</v>

130
00:08:48.681 --> 00:08:49.760
<v Speaker 1>parameterized within your own net.</v>

131
00:08:49.760 --> 00:08:54.760
<v Speaker 1>Then you apply learning algorithm,</v>
<v Speaker 1>so I want to explain to you how </v>

132
00:08:54.760 --> 00:08:57.731
<v Speaker 1>reinforcement learning works.</v>
<v Speaker 1>This is model free reinforcement </v>

133
00:08:57.731 --> 00:08:57.870
<v Speaker 1>learning.</v>
<v Speaker 1>Reinforcement learning has actually been</v>

134
00:08:57.871 --> 00:09:02.871
<v Speaker 1>using practice everywhere,</v>
<v Speaker 1>but he's also deeply in it.</v>

135
00:09:02.950 --> 00:09:05.310
<v Speaker 1>It's very robust.</v>
<v Speaker 1>It's very simple.</v>

136
00:09:05.700 --> 00:09:08.850
<v Speaker 1>It's also not very efficient,</v>
<v Speaker 1>so the way it works is the following.</v>

137
00:09:08.851 --> 00:09:12.270
<v Speaker 1>This is literally the one sentence </v>
<v Speaker 1>description of what happens.</v>

138
00:09:13.680 --> 00:09:16.590
<v Speaker 1>In short,</v>
<v Speaker 1>try something new.</v>

139
00:09:17.990 --> 00:09:22.990
<v Speaker 1>I'd randomness directions and compare </v>
<v Speaker 1>the results to your expectation.</v>

140
00:09:24.960 --> 00:09:29.960
<v Speaker 1>If the result surprises you.</v>
<v Speaker 1>If you find that the results exceeded </v>

141
00:09:30.091 --> 00:09:32.850
<v Speaker 1>your expectation,</v>
<v Speaker 1>then change your parameters,</v>

142
00:09:32.930 --> 00:09:34.290
<v Speaker 1>but they could those actions in the </v>
<v Speaker 1>future.</v>

143
00:09:35.610 --> 00:09:40.610
<v Speaker 1>That's it.</v>
<v Speaker 1>This is the full idea of reinforcement </v>

144
00:09:40.610 --> 00:09:40.610
<v Speaker 1>learning.</v>
<v Speaker 1>Try it out,</v>

145
00:09:40.610 --> 00:09:45.080
<v Speaker 1>see if you like it,</v>
<v Speaker 1>and if you do more of that in the </v>

146
00:09:45.080 --> 00:09:45.630
<v Speaker 1>future,</v>
<v Speaker 1>and that's it,</v>

147
00:09:45.900 --> 00:09:48.030
<v Speaker 1>that's literally,</v>
<v Speaker 1>this is the core idea.</v>

148
00:09:48.810 --> 00:09:51.210
<v Speaker 1>Now it turns out it's not difficult to </v>
<v Speaker 1>formalize mathematically,</v>

149
00:09:51.690 --> 00:09:54.330
<v Speaker 1>but this is really what's going on in a </v>
<v Speaker 1>neural network,</v>

150
00:09:54.360 --> 00:09:57.690
<v Speaker 1>in irregular neural network,</v>
<v Speaker 1>I guess you might say,</v>

151
00:09:57.691 --> 00:09:59.370
<v Speaker 1>okay,</v>
<v Speaker 1>what's the goal?</v>

152
00:09:59.460 --> 00:10:00.540
<v Speaker 1>Iran,</v>
<v Speaker 1>the neural network.</v>

153
00:10:00.990 --> 00:10:05.190
<v Speaker 1>You get an answer,</v>
<v Speaker 1>you compare it to the desired answer and</v>

154
00:10:05.191 --> 00:10:06.510
<v Speaker 1>whatever difference you have between </v>
<v Speaker 1>those,</v>

155
00:10:06.511 --> 00:10:11.511
<v Speaker 1>do you send it back?</v>
<v Speaker 1>The changing the neural network that's </v>

156
00:10:11.511 --> 00:10:12.930
<v Speaker 1>supervised learning and reinforcement </v>
<v Speaker 1>learning.</v>

157
00:10:13.670 --> 00:10:15.790
<v Speaker 1>You're running your own network,</v>
<v Speaker 1>you had a bit of randomness,</v>

158
00:10:15.810 --> 00:10:18.960
<v Speaker 1>direction,</v>
<v Speaker 1>and then you feel like the result,</v>

159
00:10:19.410 --> 00:10:22.530
<v Speaker 1>you're randomness turns into the desired</v>
<v Speaker 1>target in effect,</v>

160
00:10:23.290 --> 00:10:25.950
<v Speaker 1>so that's it.</v>
<v Speaker 1>Trivial.</v>

161
00:10:28.290 --> 00:10:33.290
<v Speaker 1>Now math exists without explaining what </v>
<v Speaker 1>these equations mean.</v>

162
00:10:35.940 --> 00:10:38.700
<v Speaker 1>The point is not for you to derive them,</v>
<v Speaker 1>but just to show that they exist.</v>

163
00:10:39.780 --> 00:10:42.120
<v Speaker 1>There are two classes of reinforcement </v>
<v Speaker 1>learning algorithms.</v>

164
00:10:42.570 --> 00:10:47.570
<v Speaker 1>One of them is the policy gradients </v>
<v Speaker 1>where basically what you do is that you </v>

165
00:10:47.570 --> 00:10:49.910
<v Speaker 1>take this expression right there,</v>
<v Speaker 1>the will expect expected,</v>

166
00:10:50.190 --> 00:10:53.850
<v Speaker 1>some of rewards and it just crunched </v>
<v Speaker 1>through the derivatives.</v>

167
00:10:53.851 --> 00:10:58.851
<v Speaker 1>You expand the terms,</v>
<v Speaker 1>Iran do some Algebra and you get the </v>

168
00:10:58.851 --> 00:11:03.351
<v Speaker 1>derivative and miraculously the </v>
<v Speaker 1>derivative has exactly the form that I </v>

169
00:11:05.941 --> 00:11:10.941
<v Speaker 1>told you which is try some actions and </v>
<v Speaker 1>if you like them increasing the low </v>

170
00:11:11.220 --> 00:11:13.890
<v Speaker 1>probability of the actions.</v>
<v Speaker 1>That's literally follows from the math.</v>

171
00:11:14.010 --> 00:11:19.010
<v Speaker 1>It's very nice when the intuitive </v>
<v Speaker 1>explanation as a one to one </v>

172
00:11:19.010 --> 00:11:19.680
<v Speaker 1>correspondence to what you get in the </v>
<v Speaker 1>equation.</v>

173
00:11:20.160 --> 00:11:23.340
<v Speaker 1>Even though you'll have to take my word </v>
<v Speaker 1>for it if you're not familiar with it.</v>

174
00:11:24.070 --> 00:11:29.070
<v Speaker 1>That's the question.</v>
<v Speaker 1>At the top there was a different class </v>

175
00:11:29.070 --> 00:11:32.151
<v Speaker 1>of reinforcement learning algorithms </v>
<v Speaker 1>which is a little bit more difficult to </v>

176
00:11:32.151 --> 00:11:34.611
<v Speaker 1>explain.</v>
<v Speaker 1>It's called a cue learning based </v>

177
00:11:34.611 --> 00:11:35.690
<v Speaker 1>algorithms.</v>
<v Speaker 1>They are a bit less stable and beat,</v>

178
00:11:35.700 --> 00:11:40.700
<v Speaker 1>more simple efficient and it has the </v>
<v Speaker 1>property is that it can learn not only </v>

179
00:11:42.841 --> 00:11:47.841
<v Speaker 1>from the data generated by the actor but</v>
<v Speaker 1>from any other data as well.</v>

180
00:11:47.851 --> 00:11:49.290
<v Speaker 1>So it has,</v>
<v Speaker 1>it has some it,</v>

181
00:11:49.410 --> 00:11:52.980
<v Speaker 1>it has a different robustness profile.</v>
<v Speaker 1>It should be a little bit important,</v>

182
00:11:53.350 --> 00:11:56.770
<v Speaker 1>but it's only going to be the legality.</v>
<v Speaker 1>So yeah,</v>

183
00:11:56.771 --> 00:11:59.110
<v Speaker 1>this is the policy of policy </v>
<v Speaker 1>distinction,</v>

184
00:11:59.290 --> 00:12:03.130
<v Speaker 1>but it's a little bit technical.</v>
<v Speaker 1>So if you find this hard to understand,</v>

185
00:12:03.190 --> 00:12:04.970
<v Speaker 1>don't worry about it.</v>
<v Speaker 1>If you already know,</v>

186
00:12:05.230 --> 00:12:10.230
<v Speaker 1>then you already know it.</v>
<v Speaker 1>So now what's the potential of </v>

187
00:12:10.230 --> 00:12:11.050
<v Speaker 1>reinforcement learning?</v>
<v Speaker 1>Wasn't the promise.</v>

188
00:12:11.770 --> 00:12:15.370
<v Speaker 1>What is it actually why?</v>
<v Speaker 1>Why should we be excited about it?</v>

189
00:12:16.330 --> 00:12:21.330
<v Speaker 1>Now,</v>
<v Speaker 1>there are two reasons the reinforcement </v>

190
00:12:21.330 --> 00:12:22.690
<v Speaker 1>learning algorithms have today already </v>
<v Speaker 1>useful and interesting,</v>

191
00:12:22.780 --> 00:12:25.870
<v Speaker 1>and especially if you have a really good</v>
<v Speaker 1>simulation of your world,</v>

192
00:12:25.871 --> 00:12:28.510
<v Speaker 1>you could train agents to lots of </v>
<v Speaker 1>interesting things,</v>

193
00:12:31.060 --> 00:12:36.060
<v Speaker 1>but what's really exciting is if you can</v>
<v Speaker 1>build a super amazing sample efficient </v>

194
00:12:36.060 --> 00:12:37.360
<v Speaker 1>algorithms,</v>
<v Speaker 1>reinforcement learning algorithm,</v>

195
00:12:37.690 --> 00:12:42.690
<v Speaker 1>which is give it a tiny amount of data </v>
<v Speaker 1>and the algorithm just crunches through </v>

196
00:12:42.690 --> 00:12:46.321
<v Speaker 1>it and extract every bit of entropy out </v>
<v Speaker 1>of it in order to learn in the fastest </v>

197
00:12:46.321 --> 00:12:47.770
<v Speaker 1>way possible.</v>
<v Speaker 1>Now,</v>

198
00:12:47.830 --> 00:12:50.350
<v Speaker 1>today or algorithms are not particularly</v>
<v Speaker 1>efficient.</v>

199
00:12:50.380 --> 00:12:54.970
<v Speaker 1>They are data inefficient,</v>
<v Speaker 1>but as our field keeps making progress,</v>

200
00:12:55.000 --> 00:12:58.120
<v Speaker 1>this will change.</v>
<v Speaker 1>Next,</v>

201
00:12:58.150 --> 00:13:00.340
<v Speaker 1>I want to dive into the topic of metal </v>
<v Speaker 1>learning.</v>

202
00:13:02.690 --> 00:13:07.690
<v Speaker 1>The goal of metal learning.</v>
<v Speaker 1>So Methadone is a beautiful idea that </v>

203
00:13:07.690 --> 00:13:12.211
<v Speaker 1>doesn't really work,</v>
<v Speaker 1>but it kind of works and it's really </v>

204
00:13:12.211 --> 00:13:13.000
<v Speaker 1>promising too.</v>
<v Speaker 1>It's another promising idea.</v>

205
00:13:14.610 --> 00:13:18.100
<v Speaker 1>So what's the dream?</v>
<v Speaker 1>We have some learning algorithms,</v>

206
00:13:19.150 --> 00:13:22.330
<v Speaker 1>perhaps you could use those learning </v>
<v Speaker 1>algorithm is in order to learn to learn.</v>

207
00:13:23.680 --> 00:13:28.210
<v Speaker 1>I'd be nice if we could learn to learn,</v>
<v Speaker 1>so how can you do that?</v>

208
00:13:28.810 --> 00:13:33.810
<v Speaker 1>It will take a system which you train it</v>
<v Speaker 1>not on one task but on many tasks and </v>

209
00:13:36.521 --> 00:13:41.521
<v Speaker 1>you ask it if it learns to solve these </v>
<v Speaker 1>tasks quickly and that may actually be </v>

210
00:13:41.521 --> 00:13:43.300
<v Speaker 1>enough.</v>
<v Speaker 1>So here's how it looks like.</v>

211
00:13:43.301 --> 00:13:48.301
<v Speaker 1>Here's how most traditional method </v>
<v Speaker 1>learning look works like a looks like </v>

212
00:13:48.301 --> 00:13:49.600
<v Speaker 1>you have a model which is a big neural </v>
<v Speaker 1>network,</v>

213
00:13:50.950 --> 00:13:55.950
<v Speaker 1>but what you do is that you treat every </v>
<v Speaker 1>instead of training cases,</v>

214
00:13:57.400 --> 00:13:59.890
<v Speaker 1>you have training tasks and instead of </v>
<v Speaker 1>test cases,</v>

215
00:13:59.891 --> 00:14:04.210
<v Speaker 1>you have test tasks,</v>
<v Speaker 1>so your input maybe instead of just your</v>

216
00:14:04.211 --> 00:14:07.600
<v Speaker 1>current test case,</v>
<v Speaker 1>it will be all the information about the</v>

217
00:14:07.601 --> 00:14:10.690
<v Speaker 1>news,</v>
<v Speaker 1>about the test tasks plus the test case,</v>

218
00:14:10.990 --> 00:14:14.620
<v Speaker 1>and you'll try to output the prediction </v>
<v Speaker 1>reaction for that test case.</v>

219
00:14:14.830 --> 00:14:16.930
<v Speaker 1>So basically you say,</v>
<v Speaker 1>yeah,</v>

220
00:14:16.990 --> 00:14:20.740
<v Speaker 1>I'm going to give you your 10 examples </v>
<v Speaker 1>as part of your input to your model,</v>

221
00:14:21.010 --> 00:14:22.630
<v Speaker 1>figured out how to make the best use of </v>
<v Speaker 1>them.</v>

222
00:14:24.280 --> 00:14:29.280
<v Speaker 1>It's a really straightforward idea.</v>
<v Speaker 1>You turn the neural network into the </v>

223
00:14:29.280 --> 00:14:34.261
<v Speaker 1>learning algorithm by turning a training</v>
<v Speaker 1>task into a training case,</v>

224
00:14:34.750 --> 00:14:37.060
<v Speaker 1>so training,</v>
<v Speaker 1>task it constraining case.</v>

225
00:14:37.480 --> 00:14:42.480
<v Speaker 1>This is learning this one sentence and </v>
<v Speaker 1>so there've been several success stories</v>

226
00:14:45.071 --> 00:14:50.071
<v Speaker 1>which I think are very interesting.</v>
<v Speaker 1>One of the success stories of learning </v>

227
00:14:50.080 --> 00:14:52.940
<v Speaker 1>is learning to recognize characters </v>
<v Speaker 1>quickly.</v>

228
00:14:53.390 --> 00:14:56.930
<v Speaker 1>So they've been a dataset produced by </v>
<v Speaker 1>MIT,</v>

229
00:14:56.931 --> 00:15:01.931
<v Speaker 1>by Lake at all,</v>
<v Speaker 1>and this is a data set.</v>

230
00:15:02.691 --> 00:15:07.691
<v Speaker 1>We have a large number of different </v>
<v Speaker 1>handwritten characters and people have </v>

231
00:15:07.691 --> 00:15:12.161
<v Speaker 1>been able to train extremely strong </v>
<v Speaker 1>metal learning system for this task and </v>

232
00:15:12.161 --> 00:15:17.051
<v Speaker 1>now the successful and other very </v>
<v Speaker 1>successful example of metal learning is </v>

233
00:15:17.051 --> 00:15:20.440
<v Speaker 1>neural architecture.</v>
<v Speaker 1>Search by is openly from Google where </v>

234
00:15:21.590 --> 00:15:24.440
<v Speaker 1>they found a neural architecture that </v>
<v Speaker 1>sold one problem,</v>

235
00:15:24.441 --> 00:15:29.441
<v Speaker 1>well small problem and then you could </v>
<v Speaker 1>generalize it and then it will </v>

236
00:15:29.441 --> 00:15:29.441
<v Speaker 1>successfully solve large problems as </v>
<v Speaker 1>well.</v>

237
00:15:29.441 --> 00:15:34.210
<v Speaker 1>So this is the kind of the,</v>
<v Speaker 1>the small number of bits matter </v>

238
00:15:34.300 --> 00:15:39.300
<v Speaker 1>learning.</v>
<v Speaker 1>It's like when you learn the </v>

239
00:15:39.300 --> 00:15:39.300
<v Speaker 1>architecture or maybe even learn a </v>
<v Speaker 1>program,</v>

240
00:15:39.300 --> 00:15:40.340
<v Speaker 1>a small program or learning algorithm,</v>
<v Speaker 1>and she applied to new tasks.</v>

241
00:15:40.640 --> 00:15:42.830
<v Speaker 1>So this is the other way of doing metal </v>
<v Speaker 1>learning.</v>

242
00:15:43.490 --> 00:15:45.950
<v Speaker 1>So anyway,</v>
<v Speaker 1>but the point is what's happening,</v>

243
00:15:46.010 --> 00:15:51.010
<v Speaker 1>what's really happening in matter </v>
<v Speaker 1>learning in most cases is that you turn </v>

244
00:15:51.010 --> 00:15:55.070
<v Speaker 1>a training task into a training case and</v>
<v Speaker 1>pretend that this is totally normal,</v>

245
00:15:55.100 --> 00:15:55.910
<v Speaker 1>normal,</v>
<v Speaker 1>deep learning.</v>

246
00:15:56.360 --> 00:15:59.360
<v Speaker 1>That's it.</v>
<v Speaker 1>This is the entirety of metal learning.</v>

247
00:15:59.361 --> 00:16:04.361
<v Speaker 1>Everything else that just minor details.</v>
<v Speaker 1>Next I want to dive in.</v>

248
00:16:05.150 --> 00:16:07.280
<v Speaker 1>So now that I finished the introduction </v>
<v Speaker 1>section,</v>

249
00:16:07.550 --> 00:16:12.550
<v Speaker 1>I want to start discussing different </v>
<v Speaker 1>work by different people from open ai </v>

250
00:16:12.830 --> 00:16:15.350
<v Speaker 1>and I want to start by talking about </v>
<v Speaker 1>mindset experience,</v>

251
00:16:15.351 --> 00:16:20.351
<v Speaker 1>replay.</v>
<v Speaker 1>There's been a large effort by </v>

252
00:16:20.351 --> 00:16:20.351
<v Speaker 1>unrecalled.</v>
<v Speaker 1>Richard,</v>

253
00:16:20.351 --> 00:16:24.240
<v Speaker 1>I'll develop a learning algorithm for </v>
<v Speaker 1>reinforcement learning that doesn't </v>

254
00:16:25.821 --> 00:16:30.821
<v Speaker 1>solve just one task,</v>
<v Speaker 1>but it solves many tasks and it learns </v>

255
00:16:30.821 --> 00:16:34.370
<v Speaker 1>to make use of it's experience in a much</v>
<v Speaker 1>more efficient way.</v>

256
00:16:35.530 --> 00:16:38.300
<v Speaker 1>And I want to discuss one problem in </v>
<v Speaker 1>reinforcement learning.</v>

257
00:16:38.600 --> 00:16:43.600
<v Speaker 1>It's actually,</v>
<v Speaker 1>I guess a set of problems which are </v>

258
00:16:43.600 --> 00:16:46.691
<v Speaker 1>related to each other,</v>
<v Speaker 1>like one really important thing you need</v>

259
00:16:48.471 --> 00:16:53.471
<v Speaker 1>to learn to do is to explore your in </v>
<v Speaker 1>that you start out in an environment you</v>

260
00:16:53.811 --> 00:16:55.760
<v Speaker 1>don't know what to do,</v>
<v Speaker 1>what do you do?</v>

261
00:16:56.290 --> 00:16:59.780
<v Speaker 1>So one very important thing that has to </v>
<v Speaker 1>happen is that you must get rewards from</v>

262
00:16:59.781 --> 00:17:04.781
<v Speaker 1>time to time.</v>
<v Speaker 1>If you try something and you don't get </v>

263
00:17:04.781 --> 00:17:06.800
<v Speaker 1>rewards,</v>
<v Speaker 1>then how can you learn?</v>

264
00:17:08.170 --> 00:17:10.940
<v Speaker 1>So said that's the kind of the crux of </v>
<v Speaker 1>the problem.</v>

265
00:17:11.300 --> 00:17:14.030
<v Speaker 1>How do you learn?</v>
<v Speaker 1>And relatedly,</v>

266
00:17:15.140 --> 00:17:19.010
<v Speaker 1>is there any way to meaningfully benefit</v>
<v Speaker 1>from your,</v>

267
00:17:19.150 --> 00:17:21.470
<v Speaker 1>from the experience,</v>
<v Speaker 1>from your attempts to,</v>

268
00:17:21.810 --> 00:17:22.970
<v Speaker 1>from,</v>
<v Speaker 1>from your failures.</v>

269
00:17:23.300 --> 00:17:25.280
<v Speaker 1>If you try to achieve a goal and you </v>
<v Speaker 1>fail,</v>

270
00:17:25.310 --> 00:17:30.310
<v Speaker 1>can you still learn from it?</v>
<v Speaker 1>You tell you instead of asking your </v>

271
00:17:30.310 --> 00:17:32.300
<v Speaker 1>algorithm to achieve a single goal,</v>
<v Speaker 1>you want to learn a policy that they can</v>

272
00:17:32.301 --> 00:17:33.890
<v Speaker 1>achieve a very large family of goals.</v>

273
00:17:34.340 --> 00:17:36.410
<v Speaker 1>For example,</v>
<v Speaker 1>instead of reaching one state,</v>

274
00:17:36.740 --> 00:17:40.490
<v Speaker 1>you want to learn a policy that reaches </v>
<v Speaker 1>every state on your system.</v>

275
00:17:40.760 --> 00:17:41.750
<v Speaker 1>Now,</v>
<v Speaker 1>what's the implication?</v>

276
00:17:42.710 --> 00:17:45.980
<v Speaker 1>Anytime you do something,</v>
<v Speaker 1>you achieved some state.</v>

277
00:17:46.820 --> 00:17:49.830
<v Speaker 1>So let's suppose you say I want to </v>
<v Speaker 1>achieve state A.</v>

278
00:17:51.180 --> 00:17:54.120
<v Speaker 1>I try my best and I ended up achieving </v>
<v Speaker 1>state B.</v>

279
00:17:55.920 --> 00:17:58.170
<v Speaker 1>I can either conclude while that was </v>
<v Speaker 1>disappointing,</v>

280
00:17:58.230 --> 00:18:01.800
<v Speaker 1>I haven't flown almost anything.</v>
<v Speaker 1>I still have no idea how to,</v>

281
00:18:02.010 --> 00:18:05.310
<v Speaker 1>how to achieve state aid.</v>
<v Speaker 1>But alternatively I can say,</v>

282
00:18:05.311 --> 00:18:06.330
<v Speaker 1>well,</v>
<v Speaker 1>wait a second,</v>

283
00:18:06.750 --> 00:18:08.540
<v Speaker 1>I've just reached the perfectly good </v>
<v Speaker 1>state,</v>

284
00:18:08.670 --> 00:18:11.450
<v Speaker 1>which is b,</v>
<v Speaker 1>can I learn how to achieve state,</v>

285
00:18:11.470 --> 00:18:15.510
<v Speaker 1>be for my attempt to achieve state a and</v>
<v Speaker 1>answer is yes,</v>

286
00:18:15.511 --> 00:18:20.511
<v Speaker 1>you can,</v>
<v Speaker 1>and it just works and I just want to </v>

287
00:18:20.511 --> 00:18:23.331
<v Speaker 1>point out this is the one case,</v>
<v Speaker 1>there's a small subtlety here which may </v>

288
00:18:23.820 --> 00:18:28.820
<v Speaker 1>be interesting to those of you who are </v>
<v Speaker 1>very familiar with the distinction </v>

289
00:18:28.820 --> 00:18:30.120
<v Speaker 1>between non policy and off policy.</v>

290
00:18:31.230 --> 00:18:36.230
<v Speaker 1>When you try to achieve a,</v>
<v Speaker 1>you are on your own policy learning for </v>

291
00:18:36.230 --> 00:18:41.090
<v Speaker 1>reaching the state a,</v>
<v Speaker 1>but you're doing off policy learning </v>

292
00:18:41.090 --> 00:18:44.450
<v Speaker 1>fruition to state b because you will </v>
<v Speaker 1>take different actions if you actually </v>

293
00:18:44.450 --> 00:18:48.081
<v Speaker 1>try to reach them beat.</v>
<v Speaker 1>So that's why it's very important that </v>

294
00:18:48.081 --> 00:18:49.230
<v Speaker 1>the algorithm you use here can support </v>
<v Speaker 1>of policy learning.</v>

295
00:18:49.800 --> 00:18:52.680
<v Speaker 1>But that's a minor technicality at the </v>
<v Speaker 1>crux.</v>

296
00:18:52.740 --> 00:18:57.740
<v Speaker 1>The crux of the idea is you make the </v>
<v Speaker 1>problem easier by ostensibly making it </v>

297
00:18:58.621 --> 00:19:03.621
<v Speaker 1>harder by training assistant which can,</v>
<v Speaker 1>which aspires to reach,</v>

298
00:19:03.780 --> 00:19:07.290
<v Speaker 1>to learn to reach every state,</v>
<v Speaker 1>to learn to achieve every goal,</v>

299
00:19:07.500 --> 00:19:10.140
<v Speaker 1>to learn to master it's environment.</v>
<v Speaker 1>In general,</v>

300
00:19:10.650 --> 00:19:14.670
<v Speaker 1>you build a system which always learned </v>
<v Speaker 1>something,</v>

301
00:19:15.030 --> 00:19:20.030
<v Speaker 1>it learns from success as well as from </v>
<v Speaker 1>failure because if it tries to do one </v>

302
00:19:20.030 --> 00:19:20.940
<v Speaker 1>thing,</v>
<v Speaker 1>one thing and it does something else,</v>

303
00:19:21.510 --> 00:19:23.010
<v Speaker 1>it now Australian data for how to </v>
<v Speaker 1>achieve.</v>

304
00:19:23.010 --> 00:19:28.010
<v Speaker 1>That's something else we want to show </v>
<v Speaker 1>you a video of how this thing works in </v>

305
00:19:28.010 --> 00:19:31.701
<v Speaker 1>practice.</v>
<v Speaker 1>So one challenge in reinforcement </v>

306
00:19:31.701 --> 00:19:33.780
<v Speaker 1>learning systems is the need to shape </v>
<v Speaker 1>the reward.</v>

307
00:19:34.260 --> 00:19:38.220
<v Speaker 1>So what does it mean?</v>
<v Speaker 1>It means that at the beginning of the at</v>

308
00:19:38.240 --> 00:19:39.420
<v Speaker 1>the,</v>
<v Speaker 1>at the start of learning,</v>

309
00:19:39.421 --> 00:19:43.680
<v Speaker 1>then the system doesn't know much,</v>
<v Speaker 1>it will probably not achieve your goal,</v>

310
00:19:43.830 --> 00:19:48.830
<v Speaker 1>and so it's important that you design </v>
<v Speaker 1>your reward function to give it gradual </v>

311
00:19:48.830 --> 00:19:51.981
<v Speaker 1>increments to make it smooth and </v>
<v Speaker 1>continuous so that even when the system </v>

312
00:19:51.981 --> 00:19:51.981
<v Speaker 1>is not very good,</v>
<v Speaker 1>it achieves the goal.</v>

313
00:19:52.230 --> 00:19:53.550
<v Speaker 1>Now,</v>
<v Speaker 1>if you give your state,</v>

314
00:19:53.551 --> 00:19:58.551
<v Speaker 1>your system a very sparse report where </v>
<v Speaker 1>the reward is achieved only when you </v>

315
00:19:58.551 --> 00:20:02.031
<v Speaker 1>reach a final state,</v>
<v Speaker 1>very hard for normal reinforcement </v>

316
00:20:03.051 --> 00:20:08.051
<v Speaker 1>learning algorithms to solve the problem</v>
<v Speaker 1>because naturally you'll never get the </v>

317
00:20:08.051 --> 00:20:09.170
<v Speaker 1>reward so you never learn.</v>
<v Speaker 1>No reward means no learning.</v>

318
00:20:10.100 --> 00:20:15.100
<v Speaker 1>But here,</v>
<v Speaker 1>because you learn from failure as well </v>

319
00:20:15.100 --> 00:20:18.251
<v Speaker 1>as some success,</v>
<v Speaker 1>there's this problem simply doesn't </v>

320
00:20:18.251 --> 00:20:18.251
<v Speaker 1>occur.</v>
<v Speaker 1>And so this,</v>

321
00:20:18.251 --> 00:20:19.100
<v Speaker 1>this,</v>
<v Speaker 1>this is nice.</v>

322
00:20:19.101 --> 00:20:20.500
<v Speaker 1>I think,</v>
<v Speaker 1>you know,</v>

323
00:20:20.510 --> 00:20:25.510
<v Speaker 1>let's,</v>
<v Speaker 1>let's look at the videos a little bit </v>

324
00:20:25.510 --> 00:20:26.741
<v Speaker 1>more.</v>
<v Speaker 1>Like it's nice how say it confidently </v>

325
00:20:26.741 --> 00:20:28.100
<v Speaker 1>and energetically moves the little green</v>
<v Speaker 1>buck.</v>

326
00:20:28.160 --> 00:20:29.930
<v Speaker 1>Please target.</v>
<v Speaker 1>And here's another one.</v>

327
00:20:50.030 --> 00:20:51.800
<v Speaker 1>Okay.</v>
<v Speaker 1>So we can skip the physical.</v>

328
00:20:51.850 --> 00:20:55.060
<v Speaker 1>It works on the physical robot as well,</v>
<v Speaker 1>but he can skip it.</v>

329
00:20:56.440 --> 00:20:59.650
<v Speaker 1>So I think the point is the hindsight </v>
<v Speaker 1>central experience,</v>

330
00:20:59.651 --> 00:21:04.651
<v Speaker 1>replay algorithm is directionally </v>
<v Speaker 1>correct because you want to make use of </v>

331
00:21:06.730 --> 00:21:09.130
<v Speaker 1>all your data and not only a small </v>
<v Speaker 1>fraction of it.</v>

332
00:21:10.000 --> 00:21:14.770
<v Speaker 1>Now one huge question is where do you </v>
<v Speaker 1>get the high level states?</v>

333
00:21:15.640 --> 00:21:17.410
<v Speaker 1>Where do the high level states come </v>
<v Speaker 1>from?</v>

334
00:21:19.120 --> 00:21:21.580
<v Speaker 1>Because in the work that I've shown you </v>
<v Speaker 1>so far,</v>

335
00:21:22.660 --> 00:21:24.610
<v Speaker 1>the system is asking your goal in the </v>
<v Speaker 1>states.</v>

336
00:21:25.030 --> 00:21:30.030
<v Speaker 1>So I think one thing too will become </v>
<v Speaker 1>very important for these kinds of </v>

337
00:21:30.030 --> 00:21:30.400
<v Speaker 1>approaches.</v>
<v Speaker 1>Is it a presentation,</v>

338
00:21:30.401 --> 00:21:33.910
<v Speaker 1>learning and unsupervised learning.</v>
<v Speaker 1>Figure out what are the rates,</v>

339
00:21:33.970 --> 00:21:38.970
<v Speaker 1>what are the right states?</v>
<v Speaker 1>What's the state space of goals that's </v>

340
00:21:38.970 --> 00:21:38.970
<v Speaker 1>worth achieving?</v>

341
00:21:41.320 --> 00:21:46.320
<v Speaker 1>Now I want to go through some real </v>
<v Speaker 1>method learning results and I'll show </v>

342
00:21:47.891 --> 00:21:52.891
<v Speaker 1>you a very simple way of doing to real </v>
<v Speaker 1>from simulation to the physical.</v>

343
00:21:54.011 --> 00:21:55.720
<v Speaker 1>Robert,</v>
<v Speaker 1>with Metta learning,</v>

344
00:21:56.830 --> 00:22:01.830
<v Speaker 1>and this is where went panadol was in a </v>
<v Speaker 1>an entrance and an a really nice intern </v>

345
00:22:01.830 --> 00:22:05.881
<v Speaker 1>project and vain 17,</v>
<v Speaker 1>so I think we can agree that in the </v>

346
00:22:07.421 --> 00:22:11.680
<v Speaker 1>domain of robotics,</v>
<v Speaker 1>it would be nice if you could train your</v>

347
00:22:11.681 --> 00:22:16.681
<v Speaker 1>policy in simulation and then somehow </v>
<v Speaker 1>this knowledge would carry over to the </v>

348
00:22:18.581 --> 00:22:22.360
<v Speaker 1>physical robbed.</v>
<v Speaker 1>Now we can build,</v>

349
00:22:22.600 --> 00:22:27.600
<v Speaker 1>we can build simulators that are okay,</v>
<v Speaker 1>but they can never perfectly match the </v>

350
00:22:28.901 --> 00:22:31.900
<v Speaker 1>real world unless you want to have an </v>
<v Speaker 1>insanely slow simulator.</v>

351
00:22:32.590 --> 00:22:37.590
<v Speaker 1>And the reason for that is that it turns</v>
<v Speaker 1>out that stimulating freakers simulating</v>

352
00:22:38.021 --> 00:22:41.710
<v Speaker 1>contacts is super hard.</v>
<v Speaker 1>And I heard somewhere,</v>

353
00:22:41.711 --> 00:22:45.220
<v Speaker 1>correct me if I'm wrong,</v>
<v Speaker 1>that simulating friction is NP complete.</v>

354
00:22:45.640 --> 00:22:49.390
<v Speaker 1>I'm not sure,</v>
<v Speaker 1>but it's like stuff like that.</v>

355
00:22:49.780 --> 00:22:53.710
<v Speaker 1>So your simulation is just not going to </v>
<v Speaker 1>match reality.</v>

356
00:22:53.860 --> 00:22:55.510
<v Speaker 1>There'll be some resemblance,</v>
<v Speaker 1>but that's it.</v>

357
00:22:56.230 --> 00:23:01.230
<v Speaker 1>How can we address this problem?</v>
<v Speaker 1>And I wanted to show you one simple </v>

358
00:23:01.230 --> 00:23:01.230
<v Speaker 1>idea.</v>

359
00:23:03.430 --> 00:23:08.430
<v Speaker 1>So let's say one thing.</v>
<v Speaker 1>What's one thing that would be nice is </v>

360
00:23:08.921 --> 00:23:13.921
<v Speaker 1>that if you could learn a policy,</v>
<v Speaker 1>learn a policy that will quickly adapt </v>

361
00:23:13.921 --> 00:23:17.620
<v Speaker 1>itself to the real world.</v>
<v Speaker 1>Well,</v>

362
00:23:17.621 --> 00:23:19.990
<v Speaker 1>if you want to learn a policy that can </v>
<v Speaker 1>quickly adapt,</v>

363
00:23:20.290 --> 00:23:25.290
<v Speaker 1>we need to make sure that it has </v>
<v Speaker 1>opportunities to adapt you in training </v>

364
00:23:25.290 --> 00:23:25.290
<v Speaker 1>time.</v>
<v Speaker 1>So what do we do?</v>

365
00:23:25.540 --> 00:23:30.070
<v Speaker 1>Instead of solving a problem in just one</v>
<v Speaker 1>simulator,</v>

366
00:23:30.670 --> 00:23:33.220
<v Speaker 1>we add a huge amount of variability to </v>
<v Speaker 1>the simulator.</v>

367
00:23:33.620 --> 00:23:38.140
<v Speaker 1>We say we will randomize the friction so</v>
<v Speaker 1>we will randomize the masses,</v>

368
00:23:38.530 --> 00:23:43.530
<v Speaker 1>the length of the different objects and </v>
<v Speaker 1>their I guess and dimensions.</v>

369
00:23:44.560 --> 00:23:49.560
<v Speaker 1>So you tried to randomize physics,</v>
<v Speaker 1>the simulator and lots of different </v>

370
00:23:49.560 --> 00:23:53.951
<v Speaker 1>ways,</v>
<v Speaker 1>and then importantly you don't tell the </v>

371
00:23:53.951 --> 00:23:55.640
<v Speaker 1>policy how we randomized it,</v>
<v Speaker 1>so what is it going to do?</v>

372
00:23:55.641 --> 00:23:58.150
<v Speaker 1>Then you'll take your policy and you put</v>
<v Speaker 1>it in an environment.</v>

373
00:23:58.150 --> 00:23:58.820
<v Speaker 1>Then says,</v>
<v Speaker 1>well,</v>

374
00:23:58.850 --> 00:24:00.260
<v Speaker 1>this is really,</v>
<v Speaker 1>really tough.</v>

375
00:24:00.410 --> 00:24:03.560
<v Speaker 1>I don't know what the masses are and I </v>
<v Speaker 1>don't know what the frictions are.</v>

376
00:24:03.890 --> 00:24:07.220
<v Speaker 1>I need to try things out and figure out </v>
<v Speaker 1>where the friction is.</v>

377
00:24:07.340 --> 00:24:11.030
<v Speaker 1>As I get responses from the environment.</v>
<v Speaker 1>So you build it,</v>

378
00:24:11.040 --> 00:24:16.040
<v Speaker 1>you learn a certain degree of </v>
<v Speaker 1>adaptability into the policy and it </v>

379
00:24:16.611 --> 00:24:18.950
<v Speaker 1>actually works.</v>
<v Speaker 1>I just want to show you,</v>

380
00:24:19.130 --> 00:24:24.130
<v Speaker 1>this is what happens when you just </v>
<v Speaker 1>straight up policy in simulation and </v>

381
00:24:24.130 --> 00:24:28.421
<v Speaker 1>deployed on the physical robots and here</v>
<v Speaker 1>the goal is to bring the hockey puck </v>

382
00:24:28.421 --> 00:24:32.300
<v Speaker 1>towards the red dots and you will see </v>
<v Speaker 1>that it will struggle</v>

383
00:24:37.850 --> 00:24:42.850
<v Speaker 1>and there isn't it struggles is because </v>
<v Speaker 1>of the systematic differences between </v>

384
00:24:42.850 --> 00:24:45.580
<v Speaker 1>the simulator and the real physical </v>
<v Speaker 1>Robert.</v>

385
00:24:47.290 --> 00:24:52.290
<v Speaker 1>So I could even the basic movement is </v>
<v Speaker 1>difficult for the policy because the </v>

386
00:24:52.290 --> 00:24:54.820
<v Speaker 1>assumptions of allocated so much.</v>
<v Speaker 1>So if you do the training,</v>

387
00:24:54.821 --> 00:24:59.821
<v Speaker 1>as I discussed,</v>
<v Speaker 1>we train the recurrent neural network </v>

388
00:24:59.821 --> 00:25:02.171
<v Speaker 1>policy which learns to quickly infor </v>
<v Speaker 1>properties of the simulator in order to </v>

389
00:25:03.321 --> 00:25:05.900
<v Speaker 1>accomplish the task.</v>
<v Speaker 1>You can give it to the real thing,</v>

390
00:25:06.590 --> 00:25:08.720
<v Speaker 1>the real physics and it will do much </v>
<v Speaker 1>better.</v>

391
00:25:10.150 --> 00:25:12.890
<v Speaker 1>Now this is not a perfect technique,</v>
<v Speaker 1>but it's definitely very promising.</v>

392
00:25:12.891 --> 00:25:17.180
<v Speaker 1>It's promising whenever you are able to </v>
<v Speaker 1>sufficiently randomized simulator.</v>

393
00:25:19.070 --> 00:25:22.820
<v Speaker 1>So he's definitely very nice to see the </v>
<v Speaker 1>closed loop nature of the policy.</v>

394
00:25:22.821 --> 00:25:27.821
<v Speaker 1>You consider that it would push the </v>
<v Speaker 1>hockey puck and he would correct it </v>

395
00:25:27.821 --> 00:25:31.600
<v Speaker 1>very,</v>
<v Speaker 1>very gently to bring into the goal so </v>

396
00:25:31.600 --> 00:25:31.600
<v Speaker 1>that,</v>
<v Speaker 1>that was cool.</v>

397
00:25:33.830 --> 00:25:36.040
<v Speaker 1>So that was very,</v>
<v Speaker 1>uh,</v>

398
00:25:36.041 --> 00:25:36.450
<v Speaker 1>that,</v>
<v Speaker 1>that,</v>

399
00:25:36.451 --> 00:25:38.210
<v Speaker 1>that was a cool application of metal </v>
<v Speaker 1>learning.</v>

400
00:25:39.380 --> 00:25:41.950
<v Speaker 1>I want to discuss one more application </v>
<v Speaker 1>of Methadone,</v>

401
00:25:41.990 --> 00:25:45.410
<v Speaker 1>which is learning the hierarchy of </v>
<v Speaker 1>actions.</v>

402
00:25:46.790 --> 00:25:49.040
<v Speaker 1>And this was working with and by France </v>
<v Speaker 1>at all.</v>

403
00:25:49.310 --> 00:25:52.370
<v Speaker 1>Actually I'm Kevin France or the </v>
<v Speaker 1>intranasal did.</v>

404
00:25:52.371 --> 00:25:55.160
<v Speaker 1>It wasn't high school.</v>
<v Speaker 1>I mean he wrote this paper.</v>

405
00:25:57.440 --> 00:25:58.430
<v Speaker 1>So</v>

406
00:26:02.400 --> 00:26:07.400
<v Speaker 1>one thing that would be nice is if </v>
<v Speaker 1>reinforcement learning was hierarchical,</v>

407
00:26:09.280 --> 00:26:11.670
<v Speaker 1>if instead of simply taking my </v>
<v Speaker 1>corrections,</v>

408
00:26:11.910 --> 00:26:16.080
<v Speaker 1>you had some kind of util sub routines </v>
<v Speaker 1>that you could deploy.</v>

409
00:26:16.410 --> 00:26:18.390
<v Speaker 1>Maybe the term sub routine is a little </v>
<v Speaker 1>bit too crude,</v>

410
00:26:18.391 --> 00:26:22.860
<v Speaker 1>but if you had some idea of which action</v>
<v Speaker 1>primitives are,</v>

411
00:26:23.280 --> 00:26:28.280
<v Speaker 1>we're starting to now no one has been </v>
<v Speaker 1>able to get actually like real value add</v>

412
00:26:31.861 --> 00:26:36.861
<v Speaker 1>from curriculum reinforcement learning.</v>
<v Speaker 1>Yet so far all the really cool results </v>

413
00:26:36.861 --> 00:26:38.400
<v Speaker 1>or that really convincing is also </v>
<v Speaker 1>reinforcement learning does not use it.</v>

414
00:26:39.900 --> 00:26:44.900
<v Speaker 1>That's because we haven't quite figured </v>
<v Speaker 1>out what's the right way for </v>

415
00:26:44.900 --> 00:26:46.620
<v Speaker 1>reinforcement learning.</v>
<v Speaker 1>Reinforcement learning.</v>

416
00:26:48.190 --> 00:26:53.190
<v Speaker 1>I just want to show you one very simple </v>
<v Speaker 1>approach where you use metal learning to</v>

417
00:26:55.120 --> 00:26:58.580
<v Speaker 1>learn to the hierarchy of actions.</v>
<v Speaker 1>So here's what you do.</v>

418
00:26:59.720 --> 00:27:04.720
<v Speaker 1>You have in this specific work,</v>
<v Speaker 1>you have a certain yellow,</v>

419
00:27:05.270 --> 00:27:08.640
<v Speaker 1>let's say you have a certain number of </v>
<v Speaker 1>low level primitive,</v>

420
00:27:08.670 --> 00:27:13.670
<v Speaker 1>so let's say you have to of them and you</v>
<v Speaker 1>have a distribution of tasks and your </v>

421
00:27:14.541 --> 00:27:19.541
<v Speaker 1>goal is to learn low level primitives </v>
<v Speaker 1>such that when they are used inside a </v>

422
00:27:23.541 --> 00:27:26.570
<v Speaker 1>very brief run of some reinforcement </v>
<v Speaker 1>learning algorithm,</v>

423
00:27:26.860 --> 00:27:28.520
<v Speaker 1>it will make as much progress as </v>
<v Speaker 1>possible.</v>

424
00:27:30.020 --> 00:27:33.560
<v Speaker 1>So the idea is you want to get the </v>
<v Speaker 1>greatest amount of progress.</v>

425
00:27:33.610 --> 00:27:36.960
<v Speaker 1>You want to learn policies that result </v>
<v Speaker 1>in the great.</v>

426
00:27:36.961 --> 00:27:41.961
<v Speaker 1>Sorry.</v>
<v Speaker 1>You want to learn primitives the result </v>

427
00:27:41.961 --> 00:27:43.940
<v Speaker 1>in the greatest amount of progress </v>
<v Speaker 1>possible when used inside learning.</v>

428
00:27:44.510 --> 00:27:47.570
<v Speaker 1>So this is a mental learning center </v>
<v Speaker 1>because any distribution of tasks,</v>

429
00:27:47.750 --> 00:27:50.810
<v Speaker 1>and here we've had before,</v>
<v Speaker 1>we've had a little maze,</v>

430
00:27:51.970 --> 00:27:56.970
<v Speaker 1>he have a distribution of amazes and in </v>
<v Speaker 1>this case and the little bug learned </v>

431
00:27:56.970 --> 00:28:00.110
<v Speaker 1>three policies which morbid in speaks to</v>
<v Speaker 1>direction.</v>

432
00:28:00.410 --> 00:28:02.120
<v Speaker 1>And as a result of having this </v>
<v Speaker 1>hierarchy,</v>

433
00:28:02.121 --> 00:28:03.770
<v Speaker 1>you are able to solve problems really </v>
<v Speaker 1>fast,</v>

434
00:28:03.890 --> 00:28:06.800
<v Speaker 1>but only when the hierarchies correct.</v>
<v Speaker 1>So correct.</v>

435
00:28:06.801 --> 00:28:08.960
<v Speaker 1>Called reinforcement learning is still a</v>
<v Speaker 1>work in progress.</v>

436
00:28:08.990 --> 00:28:13.990
<v Speaker 1>And this was in this work is an </v>
<v Speaker 1>interesting proof point of how </v>

437
00:28:18.640 --> 00:28:20.490
<v Speaker 1>curricular reinforcement would be like.</v>
<v Speaker 1>Hi,</v>

438
00:28:20.491 --> 00:28:25.491
<v Speaker 1>how correct.</v>
<v Speaker 1>Called reinforcement learning could be </v>

439
00:28:25.491 --> 00:28:27.351
<v Speaker 1>like if it worked now I wanted to just </v>
<v Speaker 1>spend one slide addressing the </v>

440
00:28:31.521 --> 00:28:33.800
<v Speaker 1>limitations of high capacity method </v>
<v Speaker 1>learning.</v>

441
00:28:35.180 --> 00:28:37.760
<v Speaker 1>The specific limitation is that</v>

442
00:28:40.600 --> 00:28:45.600
<v Speaker 1>the training task distribution has to be</v>
<v Speaker 1>equal to the test task distribution and </v>

443
00:28:46.661 --> 00:28:51.490
<v Speaker 1>I think this is a real limitation </v>
<v Speaker 1>because in reality you the new task that</v>

444
00:28:51.491 --> 00:28:56.491
<v Speaker 1>you want to learn to in some ways being </v>
<v Speaker 1>fundamentally different from anything </v>

445
00:28:57.071 --> 00:28:58.780
<v Speaker 1>you've seen so far.</v>
<v Speaker 1>So for example,</v>

446
00:28:58.781 --> 00:29:01.630
<v Speaker 1>if you go to school,</v>
<v Speaker 1>you learn lots of useful things,</v>

447
00:29:02.680 --> 00:29:06.430
<v Speaker 1>but then when you go to work,</v>
<v Speaker 1>only a fraction of this,</v>

448
00:29:06.880 --> 00:29:08.560
<v Speaker 1>of the things that you've learned </v>
<v Speaker 1>carries over.</v>

449
00:29:09.220 --> 00:29:14.220
<v Speaker 1>Can you need to learn if he wouldn't </v>
<v Speaker 1>have quite a few more things from </v>

450
00:29:14.220 --> 00:29:16.441
<v Speaker 1>scratch.</v>
<v Speaker 1>So madeloni would struggle with that </v>

451
00:29:16.441 --> 00:29:18.190
<v Speaker 1>because it really assumes that the </v>
<v Speaker 1>training,</v>

452
00:29:18.191 --> 00:29:23.191
<v Speaker 1>the training data is the distribution </v>
<v Speaker 1>over the training task has to be equals </v>

453
00:29:23.191 --> 00:29:24.970
<v Speaker 1>with distribution over the test asks.</v>
<v Speaker 1>That's the limitation,</v>

454
00:29:24.980 --> 00:29:29.980
<v Speaker 1>the thing that as we develop better </v>
<v Speaker 1>algorithms for being robust when the</v>

455
00:29:33.240 --> 00:29:38.240
<v Speaker 1>test tasks outside of the distribution </v>
<v Speaker 1>of the training desk than metal and got </v>

456
00:29:38.240 --> 00:29:43.130
<v Speaker 1>much better.</v>
<v Speaker 1>Now I want to talk about self play </v>

457
00:29:44.200 --> 00:29:49.200
<v Speaker 1>thing.</v>
<v Speaker 1>Self plays a very cool topic that's </v>

458
00:29:49.200 --> 00:29:53.550
<v Speaker 1>starting to get attention only now and I</v>
<v Speaker 1>want to start by reviewing very old work</v>

459
00:29:55.480 --> 00:30:00.010
<v Speaker 1>cold td Gammon.</v>
<v Speaker 1>It's back from the older way from 1992.</v>

460
00:30:00.011 --> 00:30:03.650
<v Speaker 1>So 26 years old now.</v>
<v Speaker 1>It was done by Jerry to Sarah.</v>

461
00:30:04.120 --> 00:30:09.120
<v Speaker 1>So this work is really incredible </v>
<v Speaker 1>because it has so much relevance today.</v>

462
00:30:15.100 --> 00:30:18.220
<v Speaker 1>What they did basically they said,</v>
<v Speaker 1>okay,</v>

463
00:30:18.730 --> 00:30:23.730
<v Speaker 1>let's take two neural networks and lets </v>
<v Speaker 1>them let them play against each other,</v>

464
00:30:25.720 --> 00:30:28.750
<v Speaker 1>let them play backgammon against each </v>
<v Speaker 1>other and let them try.</v>

465
00:30:28.810 --> 00:30:33.810
<v Speaker 1>Let them be trained with culinary.</v>
<v Speaker 1>So it's a super modern approach and you </v>

466
00:30:35.741 --> 00:30:40.210
<v Speaker 1>would think this was a paper from 2017 </v>
<v Speaker 1>except with you look at the splots.</v>

467
00:30:40.240 --> 00:30:42.340
<v Speaker 1>It shows that you only have 10 hidden </v>
<v Speaker 1>units,</v>

468
00:30:42.341 --> 00:30:44.260
<v Speaker 1>20 hidden units,</v>
<v Speaker 1>40 and 84.</v>

469
00:30:44.261 --> 00:30:49.261
<v Speaker 1>The difference in colors where you </v>
<v Speaker 1>noticed that the largest neural network </v>

470
00:30:49.261 --> 00:30:52.000
<v Speaker 1>works best,</v>
<v Speaker 1>so in some ways not much has changed and</v>

471
00:30:52.001 --> 00:30:57.001
<v Speaker 1>this is the evidence and in fact they </v>
<v Speaker 1>were able to beat the world champion and</v>

472
00:30:58.050 --> 00:31:03.050
<v Speaker 1>backgammon and they were able to </v>
<v Speaker 1>discover new strategies that the best </v>

473
00:31:03.050 --> 00:31:06.451
<v Speaker 1>human,</v>
<v Speaker 1>a backgammon players weren't a have not </v>

474
00:31:06.451 --> 00:31:07.870
<v Speaker 1>noticed and they've determined that the </v>
<v Speaker 1>strategy is covered,</v>

475
00:31:07.871 --> 00:31:10.540
<v Speaker 1>but ed gamut actually better.</v>
<v Speaker 1>So that's pure stuff.</v>

476
00:31:10.541 --> 00:31:15.541
<v Speaker 1>Play with cue learning which is which </v>
<v Speaker 1>remained dormant until the Deq and work </v>

477
00:31:18.311 --> 00:31:19.540
<v Speaker 1>with Atari Buddy of mine.</v>

478
00:31:21.880 --> 00:31:26.880
<v Speaker 1>So now other examples of self blame </v>
<v Speaker 1>include Alphago Zero,</v>

479
00:31:30.340 --> 00:31:35.340
<v Speaker 1>which was able to learn to beat the </v>
<v Speaker 1>world champion in go without using any </v>

480
00:31:35.340 --> 00:31:39.061
<v Speaker 1>external data whatsoever.</v>
<v Speaker 1>And other results of this vein is by </v>

481
00:31:39.061 --> 00:31:40.650
<v Speaker 1>open ai,</v>
<v Speaker 1>which is our daughter to bought,</v>

482
00:31:40.960 --> 00:31:43.960
<v Speaker 1>which was able to build the world </v>
<v Speaker 1>champion on the one,</v>

483
00:31:43.961 --> 00:31:48.961
<v Speaker 1>the one version of the game.</v>
<v Speaker 1>And so I want to spend a little bit of </v>

484
00:31:49.841 --> 00:31:54.841
<v Speaker 1>time talking about the allure of self </v>
<v Speaker 1>play and why I think it's exciting.</v>

485
00:31:57.580 --> 00:32:02.460
<v Speaker 1>So one important problem,</v>
<v Speaker 1>that's it,</v>

486
00:32:02.540 --> 00:32:07.540
<v Speaker 1>that's the must face as we try to build </v>
<v Speaker 1>truly intelligent systems is what is the</v>

487
00:32:10.121 --> 00:32:15.121
<v Speaker 1>task,</v>
<v Speaker 1>what are we actually teaching the </v>

488
00:32:15.121 --> 00:32:17.521
<v Speaker 1>systems to do,</v>
<v Speaker 1>and one very attractive attribute of </v>

489
00:32:17.521 --> 00:32:21.811
<v Speaker 1>self is that the agents create the </v>
<v Speaker 1>environment by virtue of the agent </v>

490
00:32:26.321 --> 00:32:31.321
<v Speaker 1>acting in the environment.</v>
<v Speaker 1>The environment becomes difficult for </v>

491
00:32:31.321 --> 00:32:35.701
<v Speaker 1>the other agents and you can see here an</v>
<v Speaker 1>example of an iguana interacting with </v>

492
00:32:35.711 --> 00:32:40.711
<v Speaker 1>snakes,</v>
<v Speaker 1>the try to eat it on successfully this </v>

493
00:32:40.711 --> 00:32:41.270
<v Speaker 1>time so we can see what will happen in a</v>
<v Speaker 1>moment.</v>

494
00:32:43.310 --> 00:32:48.310
<v Speaker 1>The Guan Australian's best and so the </v>
<v Speaker 1>fact that you have this arms race </v>

495
00:32:48.310 --> 00:32:53.231
<v Speaker 1>between the snakes and the iguanas </v>
<v Speaker 1>motivates their development potentially </v>

496
00:32:54.710 --> 00:32:59.710
<v Speaker 1>without bound and this is what happened </v>
<v Speaker 1>in effective but in biological </v>

497
00:32:59.710 --> 00:33:00.950
<v Speaker 1>evolution.</v>
<v Speaker 1>Now,</v>

498
00:33:00.980 --> 00:33:04.640
<v Speaker 1>interesting work in this direction was </v>
<v Speaker 1>done in 1994,</v>

499
00:33:04.670 --> 00:33:09.670
<v Speaker 1>but Carl,</v>
<v Speaker 1>since there is a really cool video on </v>

500
00:33:09.670 --> 00:33:10.880
<v Speaker 1>youtube by Carl scenes,</v>
<v Speaker 1>you should check it out,</v>

501
00:33:11.000 --> 00:33:13.070
<v Speaker 1>which really kind of shows all the work </v>
<v Speaker 1>that he's done.</v>

502
00:33:14.180 --> 00:33:18.110
<v Speaker 1>And here you have a little competition </v>
<v Speaker 1>between agents where you evolve both the</v>

503
00:33:18.111 --> 00:33:21.250
<v Speaker 1>behavior and their morphology.</v>
<v Speaker 1>When you Walkman,</v>

504
00:33:21.320 --> 00:33:26.320
<v Speaker 1>the agents is trying to gain possession </v>
<v Speaker 1>of a green cube and so you can see that </v>

505
00:33:28.371 --> 00:33:33.371
<v Speaker 1>the agents create the challenge for each</v>
<v Speaker 1>other and that's why they need to </v>

506
00:33:33.371 --> 00:33:33.371
<v Speaker 1>develop.</v>

507
00:33:34.820 --> 00:33:39.820
<v Speaker 1>So one thing that we deed and this is </v>
<v Speaker 1>work by and sell it up from open ai is </v>

508
00:33:43.251 --> 00:33:43.700
<v Speaker 1>we said,</v>
<v Speaker 1>okay,</v>

509
00:33:43.701 --> 00:33:48.701
<v Speaker 1>well can be demonstrates some unusual </v>
<v Speaker 1>results in self play that would really </v>

510
00:33:49.580 --> 00:33:51.830
<v Speaker 1>convince us that there is something </v>
<v Speaker 1>there.</v>

511
00:33:52.400 --> 00:33:54.650
<v Speaker 1>So what we did here is that we created a</v>
<v Speaker 1>small,</v>

512
00:33:55.110 --> 00:34:00.110
<v Speaker 1>a small ring and you have these two </v>
<v Speaker 1>humanoid figures and their goal is just </v>

513
00:34:00.110 --> 00:34:04.661
<v Speaker 1>to push each other outside the ring and </v>
<v Speaker 1>they don't know anything about </v>

514
00:34:04.661 --> 00:34:09.491
<v Speaker 1>wrestling.</v>
<v Speaker 1>They don't know anything about standing </v>

515
00:34:09.491 --> 00:34:11.741
<v Speaker 1>or balancing each other.</v>
<v Speaker 1>They don't know anything about centrals </v>

516
00:34:11.741 --> 00:34:14.080
<v Speaker 1>gravity.</v>
<v Speaker 1>All they know is that if you don't do a </v>

517
00:34:14.080 --> 00:34:17.291
<v Speaker 1>good job,</v>
<v Speaker 1>then your competition is going to do a </v>

518
00:34:17.291 --> 00:34:17.291
<v Speaker 1>better job.</v>
<v Speaker 1>Now,</v>

519
00:34:17.291 --> 00:34:21.821
<v Speaker 1>one of the really attractive things </v>
<v Speaker 1>about self play is that you always have </v>

520
00:34:24.800 --> 00:34:26.600
<v Speaker 1>an opponent that's roughly as good as </v>
<v Speaker 1>you are.</v>

521
00:34:28.340 --> 00:34:33.340
<v Speaker 1>In order to learn,</v>
<v Speaker 1>you need to sometimes in and sometimes </v>

522
00:34:33.340 --> 00:34:33.650
<v Speaker 1>lose,</v>
<v Speaker 1>like you can't always win.</v>

523
00:34:34.640 --> 00:34:37.130
<v Speaker 1>Sometimes you must fail,</v>
<v Speaker 1>sometimes you must succeed,</v>

524
00:34:39.490 --> 00:34:42.380
<v Speaker 1>so let's see what will happen here.</v>
<v Speaker 1>Yeah,</v>

525
00:34:42.800 --> 00:34:47.800
<v Speaker 1>so it was able to be so the green human </v>
<v Speaker 1>was able to block the ball in a so in a </v>

526
00:34:49.021 --> 00:34:50.630
<v Speaker 1>well balanced environment.</v>

527
00:34:52.850 --> 00:34:57.850
<v Speaker 1>The competition is always level.</v>
<v Speaker 1>No matter how good you are or how bad </v>

528
00:34:57.850 --> 00:35:02.561
<v Speaker 1>you are,</v>
<v Speaker 1>you have a competition that makes it </v>

529
00:35:02.561 --> 00:35:02.630
<v Speaker 1>exactly,</v>
<v Speaker 1>exactly have exactly the right challenge</v>

530
00:35:02.631 --> 00:35:04.970
<v Speaker 1>for you in one thing here.</v>
<v Speaker 1>So this with your shows,</v>

531
00:35:04.971 --> 00:35:09.971
<v Speaker 1>transfer learning,</v>
<v Speaker 1>you take the little wrestling humanoid </v>

532
00:35:09.971 --> 00:35:12.740
<v Speaker 1>and you take its friend away and you </v>
<v Speaker 1>start applying a big,</v>

533
00:35:12.741 --> 00:35:17.741
<v Speaker 1>large random forces on it and you see if</v>
<v Speaker 1>it can maintain its balance and the </v>

534
00:35:17.741 --> 00:35:22.181
<v Speaker 1>answer turns out to be that yes it can </v>
<v Speaker 1>because it's been trained against an </v>

535
00:35:22.611 --> 00:35:27.611
<v Speaker 1>opponent,</v>
<v Speaker 1>it pushes it and so that's why even if </v>

536
00:35:27.611 --> 00:35:29.270
<v Speaker 1>it's doesn't understand where the </v>
<v Speaker 1>pressure forces being applied on it,</v>

537
00:35:29.450 --> 00:35:34.450
<v Speaker 1>it's still able to balance itself.</v>
<v Speaker 1>So this is one potentially attractive </v>

538
00:35:34.671 --> 00:35:39.671
<v Speaker 1>feature of software environments.</v>
<v Speaker 1>The QTC would learn a certain broad set </v>

539
00:35:39.671 --> 00:35:43.400
<v Speaker 1>of skills,</v>
<v Speaker 1>although it's a little hard to control </v>

540
00:35:43.400 --> 00:35:46.971
<v Speaker 1>those with the skills will be.</v>
<v Speaker 1>And so the biggest open question with </v>

541
00:35:46.971 --> 00:35:50.151
<v Speaker 1>this research is how do you learn agents</v>
<v Speaker 1>in a software environment such that they</v>

542
00:35:52.831 --> 00:35:57.831
<v Speaker 1>do whatever they do,</v>
<v Speaker 1>but then they are able to solve a </v>

543
00:35:57.831 --> 00:35:59.850
<v Speaker 1>battery of tasks that is useful for us </v>
<v Speaker 1>that is explicitly specified externally.</v>

544
00:36:01.720 --> 00:36:02.130
<v Speaker 1>Yeah.</v>

545
00:36:05.010 --> 00:36:09.660
<v Speaker 1>I also want to want to highlight one </v>
<v Speaker 1>attribute of self play environments that</v>

546
00:36:09.720 --> 00:36:14.720
<v Speaker 1>people observed in our daughter bought </v>
<v Speaker 1>and that is that you've seen a very </v>

547
00:36:14.720 --> 00:36:15.680
<v Speaker 1>rapid increase in the competence of the </v>
<v Speaker 1>Bot.</v>

548
00:36:16.020 --> 00:36:18.990
<v Speaker 1>So over the period over the course of </v>
<v Speaker 1>maybe five months,</v>

549
00:36:18.991 --> 00:36:23.991
<v Speaker 1>we've seen the bottom go from playing </v>
<v Speaker 1>totally randomly all the way to the </v>

550
00:36:26.341 --> 00:36:30.360
<v Speaker 1>world champion.</v>
<v Speaker 1>And the reason for that is that once you</v>

551
00:36:30.361 --> 00:36:33.750
<v Speaker 1>have a self clean environment,</v>
<v Speaker 1>if you put computing into it,</v>

552
00:36:34.500 --> 00:36:39.500
<v Speaker 1>you turn it into data.</v>
<v Speaker 1>Self play allows you to turn compute </v>

553
00:36:39.500 --> 00:36:44.060
<v Speaker 1>into data and I think we will see a lot </v>
<v Speaker 1>more of that as being an extremely </v>

554
00:36:44.060 --> 00:36:48.441
<v Speaker 1>important thing to be able to turn </v>
<v Speaker 1>compute into essentially data </v>

555
00:36:48.441 --> 00:36:52.251
<v Speaker 1>generalization simply because the speed </v>
<v Speaker 1>of neural net processors will increased </v>

556
00:36:52.251 --> 00:36:54.210
<v Speaker 1>very dramatically over the next few </v>
<v Speaker 1>years.</v>

557
00:36:54.870 --> 00:36:58.650
<v Speaker 1>So neural net cycles will be cheap and </v>
<v Speaker 1>it will be important to make use of this</v>

558
00:36:58.830 --> 00:37:01.560
<v Speaker 1>new,</v>
<v Speaker 1>of newly found over abundance of cycles.</v>

559
00:37:03.340 --> 00:37:07.170
<v Speaker 1>I also wanted to talk a little bit about</v>
<v Speaker 1>the end game of the self blame approach.</v>

560
00:37:08.400 --> 00:37:13.400
<v Speaker 1>So one thing that we know about the </v>
<v Speaker 1>human brain is that it has increased in </v>

561
00:37:13.831 --> 00:37:17.070
<v Speaker 1>size fairly rapidly over the past 2 </v>
<v Speaker 1>million years.</v>

562
00:37:18.660 --> 00:37:23.660
<v Speaker 1>My theory,</v>
<v Speaker 1>the reason I think it happened is </v>

563
00:37:23.660 --> 00:37:27.021
<v Speaker 1>because our ancestors got to a point </v>
<v Speaker 1>where the thing that's most important </v>

564
00:37:28.021 --> 00:37:33.021
<v Speaker 1>for your survival is your standing in </v>
<v Speaker 1>the tribe and less the tiger and the </v>

565
00:37:33.301 --> 00:37:38.070
<v Speaker 1>lion.</v>
<v Speaker 1>Once the most important thing is how you</v>

566
00:37:38.071 --> 00:37:40.230
<v Speaker 1>deal with those other things which have </v>
<v Speaker 1>a large brain.</v>

567
00:37:40.560 --> 00:37:42.330
<v Speaker 1>Then it really helps to have a slightly </v>
<v Speaker 1>larger brain.</v>

568
00:37:43.110 --> 00:37:48.110
<v Speaker 1>And I think that's what happened and </v>
<v Speaker 1>that exists at least one paper from </v>

569
00:37:48.110 --> 00:37:48.990
<v Speaker 1>science which supports this point of </v>
<v Speaker 1>view.</v>

570
00:37:50.130 --> 00:37:55.130
<v Speaker 1>So apparently there has been convergent </v>
<v Speaker 1>evolution between social apps and social</v>

571
00:37:55.471 --> 00:38:00.000
<v Speaker 1>birds even though in terms of various </v>
<v Speaker 1>behaviors,</v>

572
00:38:00.780 --> 00:38:05.780
<v Speaker 1>even though the divergence in </v>
<v Speaker 1>evolutionary timescale between humans </v>

573
00:38:05.851 --> 00:38:08.400
<v Speaker 1>and birds as occurred a very long time </v>
<v Speaker 1>ago,</v>

574
00:38:08.580 --> 00:38:10.980
<v Speaker 1>and humans and humans,</v>
<v Speaker 1>apes and humans,</v>

575
00:38:11.040 --> 00:38:14.370
<v Speaker 1>apes and birds have very different brain</v>
<v Speaker 1>structure.</v>

576
00:38:16.440 --> 00:38:19.770
<v Speaker 1>So I think what should happen if we </v>
<v Speaker 1>succeed,</v>

577
00:38:19.800 --> 00:38:22.950
<v Speaker 1>if we successfully follow the path of </v>
<v Speaker 1>this approach,</v>

578
00:38:23.190 --> 00:38:28.190
<v Speaker 1>is that we should create a society of </v>
<v Speaker 1>agents which will have language and </v>

579
00:38:28.190 --> 00:38:30.690
<v Speaker 1>theory of mind negotiation,</v>
<v Speaker 1>social skills,</v>

580
00:38:31.350 --> 00:38:34.470
<v Speaker 1>trade economy,</v>
<v Speaker 1>politics and justice system.</v>

581
00:38:35.020 --> 00:38:40.020
<v Speaker 1>All these things should happen inside </v>
<v Speaker 1>the multiagent environment and it will </v>

582
00:38:40.020 --> 00:38:44.311
<v Speaker 1>also be some alignment issue of how do </v>
<v Speaker 1>you make sure that the agents we learn </v>

583
00:38:44.311 --> 00:38:44.500
<v Speaker 1>behave innovative,</v>
<v Speaker 1>want.</v>

584
00:38:45.220 --> 00:38:50.220
<v Speaker 1>Now,</v>
<v Speaker 1>I want to make speculative digression </v>

585
00:38:50.220 --> 00:38:53.311
<v Speaker 1>here,</v>
<v Speaker 1>which is I want to make the following </v>

586
00:38:54.960 --> 00:38:59.960
<v Speaker 1>observations.</v>
<v Speaker 1>If you believe that this kind of society</v>

587
00:39:01.241 --> 00:39:06.241
<v Speaker 1>of agents is a plausible place where </v>
<v Speaker 1>truly where the full fully general </v>

588
00:39:10.301 --> 00:39:15.301
<v Speaker 1>intelligence will emerge and if you </v>
<v Speaker 1>accept that our experience with Dota </v>

589
00:39:16.421 --> 00:39:18.640
<v Speaker 1>both where we've seen a very rapid </v>
<v Speaker 1>increase in competence,</v>

590
00:39:18.650 --> 00:39:21.190
<v Speaker 1>will carry over once all the details are</v>
<v Speaker 1>right.</v>

591
00:39:21.910 --> 00:39:26.910
<v Speaker 1>If you assume both of these conditions,</v>
<v Speaker 1>then it should follow that we should see</v>

592
00:39:26.981 --> 00:39:31.981
<v Speaker 1>a very rapid increase in the competence </v>
<v Speaker 1>of our agents as they live in the </v>

593
00:39:32.261 --> 00:39:33.310
<v Speaker 1>society of agents.</v>

594
00:39:34.420 --> 00:39:39.420
<v Speaker 1>So now that we've talked about </v>
<v Speaker 1>potentially interesting way of </v>

595
00:39:40.690 --> 00:39:45.690
<v Speaker 1>increasing the competence and teaching </v>
<v Speaker 1>teaching social skills and language and </v>

596
00:39:45.881 --> 00:39:48.850
<v Speaker 1>a lot of things that actually exist in </v>
<v Speaker 1>humans as well.</v>

597
00:39:49.150 --> 00:39:54.150
<v Speaker 1>We want to talk a little bit about how </v>
<v Speaker 1>you convey goals to Asians and the </v>

598
00:39:57.461 --> 00:39:59.500
<v Speaker 1>question of conveying goal to eight </v>
<v Speaker 1>calls to agents.</v>

599
00:39:59.501 --> 00:40:04.501
<v Speaker 1>It's just a technical problem,</v>
<v Speaker 1>but it will be important because it is </v>

600
00:40:05.770 --> 00:40:10.770
<v Speaker 1>more likely than not that the agents </v>
<v Speaker 1>that people train will eventually be </v>

601
00:40:12.400 --> 00:40:15.190
<v Speaker 1>dramatically smarter than us.</v>
<v Speaker 1>And this is work by,</v>

602
00:40:15.191 --> 00:40:20.191
<v Speaker 1>um,</v>
<v Speaker 1>are they opening a safety team by Paul </v>

603
00:40:20.191 --> 00:40:23.360
<v Speaker 1>Cristiana at all and others.</v>
<v Speaker 1>So I'm just going to show you this video</v>

604
00:40:23.410 --> 00:40:25.690
<v Speaker 1>which basically explains how the whole </v>
<v Speaker 1>thing works.</v>

605
00:40:27.480 --> 00:40:32.480
<v Speaker 1>You there is some behavior looking for </v>
<v Speaker 1>and you the human gets to see pairs of </v>

606
00:40:33.281 --> 00:40:38.281
<v Speaker 1>behaviors and you simply click on the </v>
<v Speaker 1>one that looks better and after a very </v>

607
00:40:42.160 --> 00:40:47.160
<v Speaker 1>modest number of clicks,</v>
<v Speaker 1>you can get this little simulated leg to</v>

608
00:40:49.541 --> 00:40:50.500
<v Speaker 1>do backflips.</v>

609
00:40:57.400 --> 00:41:02.400
<v Speaker 1>There you go.</v>
<v Speaker 1>He can now do backflips and in this to </v>

610
00:41:02.400 --> 00:41:05.770
<v Speaker 1>get this specific behavior,</v>
<v Speaker 1>it took about 500 clicks by human </v>

611
00:41:06.621 --> 00:41:11.621
<v Speaker 1>annotators.</v>
<v Speaker 1>The way it works is that you take all </v>

612
00:41:11.621 --> 00:41:14.831
<v Speaker 1>the,</v>
<v Speaker 1>so this is a very data efficient </v>

613
00:41:14.831 --> 00:41:17.561
<v Speaker 1>reinforcement learning algorithm,</v>
<v Speaker 1>but it is efficient in terms of rewards </v>

614
00:41:17.561 --> 00:41:19.550
<v Speaker 1>and not in terms of the environment </v>
<v Speaker 1>interactions.</v>

615
00:41:20.360 --> 00:41:22.880
<v Speaker 1>So what you do here is that you take all</v>
<v Speaker 1>the clicks,</v>

616
00:41:22.910 --> 00:41:27.020
<v Speaker 1>so you've got your here is one here,</v>
<v Speaker 1>which is better than the other.</v>

617
00:41:27.680 --> 00:41:32.680
<v Speaker 1>You fit a reward function and numerical </v>
<v Speaker 1>reward function to those.</v>

618
00:41:33.470 --> 00:41:36.150
<v Speaker 1>So you want to fit a reward function </v>
<v Speaker 1>which satisfies those cleats clicks,</v>

619
00:41:36.151 --> 00:41:39.570
<v Speaker 1>and then you optimize this reward </v>
<v Speaker 1>function with reinforcement learning and</v>

620
00:41:39.690 --> 00:41:44.690
<v Speaker 1>it actually works.</v>
<v Speaker 1>So this requires 500 bits of </v>

621
00:41:44.690 --> 00:41:48.431
<v Speaker 1>information.</v>
<v Speaker 1>We've also been able to train him lots </v>

622
00:41:48.431 --> 00:41:49.550
<v Speaker 1>of Atari Games using several thousand </v>
<v Speaker 1>bits of information.</v>

623
00:41:49.551 --> 00:41:54.551
<v Speaker 1>So in all these cases you had human,</v>
<v Speaker 1>a human annotators or human judges just </v>

624
00:41:54.801 --> 00:41:59.801
<v Speaker 1>like in the previous slide,</v>
<v Speaker 1>looking at the pairs of trajectories and</v>

625
00:42:00.680 --> 00:42:05.680
<v Speaker 1>clicking on the one that they thought </v>
<v Speaker 1>was better and here's an example of an </v>

626
00:42:07.311 --> 00:42:09.560
<v Speaker 1>unusual goal where this is a car racing </v>
<v Speaker 1>game,</v>

627
00:42:10.010 --> 00:42:15.010
<v Speaker 1>but the goal was to ask the agent to </v>
<v Speaker 1>train the white car drive right behind </v>

628
00:42:17.450 --> 00:42:18.290
<v Speaker 1>the orange car.</v>

629
00:42:18.740 --> 00:42:22.310
<v Speaker 1>So it's a different goal and it was very</v>
<v Speaker 1>straightforward to communicate this goal</v>

630
00:42:23.330 --> 00:42:28.330
<v Speaker 1>using this approach.</v>
<v Speaker 1>So then to finish off alignment is a </v>

631
00:42:30.591 --> 00:42:32.090
<v Speaker 1>technical problem.</v>
<v Speaker 1>It has to be solved,</v>

632
00:42:32.960 --> 00:42:37.960
<v Speaker 1>but of course the determination of the </v>
<v Speaker 1>correct goals we want our systems to </v>

633
00:42:37.960 --> 00:42:42.881
<v Speaker 1>have.</v>
<v Speaker 1>You'll be a very challenging political </v>

634
00:42:42.881 --> 00:42:43.460
<v Speaker 1>problem.</v>
<v Speaker 1>And on this note,</v>

635
00:42:43.520 --> 00:42:47.390
<v Speaker 1>I want to thank you so much for your </v>
<v Speaker 1>attention and I just want to say that it</v>

636
00:42:47.391 --> 00:42:49.310
<v Speaker 1>will be a happy hour at Cambridge </v>
<v Speaker 1>brewing company.</v>

637
00:42:49.490 --> 00:42:54.490
<v Speaker 1>40th five.</v>
<v Speaker 1>If you want to chat more about Ai and </v>

638
00:42:54.490 --> 00:42:54.490
<v Speaker 1>other topics,</v>
<v Speaker 1>please come by.</v>

639
00:42:54.710 --> 00:42:56.360
<v Speaker 1>I think that deserves an applause.</v>

640
00:42:59.920 --> 00:43:03.330
<v Speaker 2>Thank you.</v>

641
00:43:03.530 --> 00:43:08.530
<v Speaker 3>So my population is a neural networks at</v>
<v Speaker 3>buyer inspired,</v>

642
00:43:09.020 --> 00:43:12.440
<v Speaker 3>but backpropagation doesn't look as </v>
<v Speaker 3>though it's what's going on in the brain</v>

643
00:43:12.441 --> 00:43:17.441
<v Speaker 3>because signals in the brain go one </v>
<v Speaker 3>direction down the excellence where as </v>

644
00:43:17.441 --> 00:43:20.540
<v Speaker 3>back propagation requires the areas to </v>
<v Speaker 3>be publicated backup the wires.</v>

645
00:43:20.600 --> 00:43:25.600
<v Speaker 3>So can you just talk a little bit about </v>
<v Speaker 3>that whole situation where it looks like</v>

646
00:43:26.931 --> 00:43:31.931
<v Speaker 3>the greatest doing something a bit </v>
<v Speaker 3>different than our highest successful </v>

647
00:43:31.931 --> 00:43:35.530
<v Speaker 3>algorithms algorithm is going to be </v>
<v Speaker 3>improved once we figure out what the </v>

648
00:43:35.530 --> 00:43:39.341
<v Speaker 3>brain is doing or what is the brain </v>
<v Speaker 3>sending signals back even though it's </v>

649
00:43:39.341 --> 00:43:41.750
<v Speaker 3>got no obvious way of doing that.</v>
<v Speaker 3>What's what's happening in that area?</v>

650
00:43:42.250 --> 00:43:44.980
<v Speaker 1>So that's a great question.</v>
<v Speaker 1>So first of all,</v>

651
00:43:44.981 --> 00:43:48.910
<v Speaker 1>I'll say that the true answer is that </v>
<v Speaker 1>the honest answer is that I don't know,</v>

652
00:43:48.911 --> 00:43:53.911
<v Speaker 1>but I have opinions.</v>
<v Speaker 1>And so I'll say two things.</v>

653
00:43:55.040 --> 00:43:59.400
<v Speaker 1>First of all,</v>
<v Speaker 1>given that like if you agree,</v>

654
00:43:59.420 --> 00:44:02.440
<v Speaker 1>if me agreed.</v>
<v Speaker 1>So rather it is a true fact.</v>

655
00:44:02.920 --> 00:44:06.790
<v Speaker 1>Backpropagation solves the problem of </v>
<v Speaker 1>circuit search.</v>

656
00:44:07.930 --> 00:44:10.990
<v Speaker 1>This problem feels like an extremely </v>
<v Speaker 1>fundamental problem.</v>

657
00:44:12.070 --> 00:44:14.590
<v Speaker 1>And for this reason I think that it's </v>
<v Speaker 1>unlikely to go away.</v>

658
00:44:15.010 --> 00:44:19.500
<v Speaker 1>Now you also right that the brain </v>
<v Speaker 1>doesn't obviously do backpropagation,</v>

659
00:44:19.540 --> 00:44:21.680
<v Speaker 1>although they've been in multiple </v>
<v Speaker 1>proposals of how it could be,</v>

660
00:44:21.970 --> 00:44:23.590
<v Speaker 1>how it could be doing them,</v>
<v Speaker 1>for example,</v>

661
00:44:23.890 --> 00:44:28.890
<v Speaker 1>there's been a work by and others where </v>
<v Speaker 1>they've shown that if you use that,</v>

662
00:44:31.261 --> 00:44:36.261
<v Speaker 1>it's possible to learn a different set </v>
<v Speaker 1>of connections that can be used for the </v>

663
00:44:36.261 --> 00:44:38.250
<v Speaker 1>backward pass and that can result in </v>
<v Speaker 1>successful learning.</v>

664
00:44:38.640 --> 00:44:43.640
<v Speaker 1>Now the reason this hasn't been really </v>
<v Speaker 1>pushed to the limit by practitioners is </v>

665
00:44:43.640 --> 00:44:44.370
<v Speaker 1>because they say,</v>
<v Speaker 1>well,</v>

666
00:44:44.371 --> 00:44:47.790
<v Speaker 1>I got tf the gradients.</v>
<v Speaker 1>I'm just not going to worry about it,</v>

667
00:44:48.570 --> 00:44:51.570
<v Speaker 1>but you're right that this is an </v>
<v Speaker 1>important issue and you know,</v>

668
00:44:51.870 --> 00:44:56.870
<v Speaker 1>one of two things is going to happen.</v>
<v Speaker 1>So my personal opinion is that </v>

669
00:44:56.870 --> 00:45:00.381
<v Speaker 1>backpropagation is just going to stay </v>
<v Speaker 1>with us til the very end and we'll </v>

670
00:45:00.381 --> 00:45:03.501
<v Speaker 1>actually build fully human level and </v>
<v Speaker 1>beyond systems before we understand how </v>

671
00:45:03.501 --> 00:45:04.140
<v Speaker 1>the brain does what it does.</v>

672
00:45:06.180 --> 00:45:11.180
<v Speaker 1>So that's what I believe,</v>
<v Speaker 1>but of course it is a difference that </v>

673
00:45:11.180 --> 00:45:11.940
<v Speaker 1>has to be acknowledged.</v>

674
00:45:14.280 --> 00:45:19.280
<v Speaker 4>Do you think it was a fair match up for </v>
<v Speaker 4>the Dota bought and that person given </v>

675
00:45:20.221 --> 00:45:21.540
<v Speaker 4>the constraints of the system?</v>

676
00:45:21.710 --> 00:45:26.710
<v Speaker 1>So I'd say that the biggest advantage </v>
<v Speaker 1>computers having games like this,</v>

677
00:45:27.050 --> 00:45:32.050
<v Speaker 1>like one of the big advantages is that </v>
<v Speaker 1>they obviously have a better reaction </v>

678
00:45:32.050 --> 00:45:36.431
<v Speaker 1>time.</v>
<v Speaker 1>Although in Dota in particular the </v>

679
00:45:36.431 --> 00:45:38.990
<v Speaker 1>number of clicks per second over the top</v>
<v Speaker 1>players is fairly small,</v>

680
00:45:39.080 --> 00:45:41.660
<v Speaker 1>which is different from starcraft.</v>
<v Speaker 1>Starcraft,</v>

681
00:45:41.930 --> 00:45:46.930
<v Speaker 1>starcraft is a very mechanically heavy </v>
<v Speaker 1>game because if a large number of units </v>

682
00:45:46.930 --> 00:45:48.530
<v Speaker 1>and so the top players,</v>
<v Speaker 1>they just click all the time.</v>

683
00:45:50.630 --> 00:45:55.630
<v Speaker 1>In Dota,</v>
<v Speaker 1>every player controls just one hero and </v>

684
00:45:55.630 --> 00:45:56.750
<v Speaker 1>so that greatly reduces the total number</v>
<v Speaker 1>of actions they need to make.</v>

685
00:45:56.960 --> 00:46:01.960
<v Speaker 1>Now still precision matters.</v>
<v Speaker 1>I think that we'll discover that later,</v>

686
00:46:02.400 --> 00:46:07.400
<v Speaker 1>but I think will really happen is if you</v>
<v Speaker 1>will discover that computers have the </v>

687
00:46:07.400 --> 00:46:10.691
<v Speaker 1>advantage in any domain or rather every </v>
<v Speaker 1>domain.</v>

688
00:46:14.520 --> 00:46:15.020
<v Speaker 1>Not yet.</v>

689
00:46:15.530 --> 00:46:20.530
<v Speaker 4>Do you think that the emergent behaviors</v>
<v Speaker 4>from the agent we're actually kind of </v>

690
00:46:20.530 --> 00:46:22.630
<v Speaker 4>directed because the constraints are </v>
<v Speaker 4>already kind of in place.</v>

691
00:46:22.631 --> 00:46:26.840
<v Speaker 4>Like so it was kinda forced discover </v>
<v Speaker 4>those or do you think that like that was</v>

692
00:46:26.841 --> 00:46:28.550
<v Speaker 4>actually something quite novel that like</v>
<v Speaker 4>wow,</v>

693
00:46:28.610 --> 00:46:33.610
<v Speaker 4>it actually discovered these on its own.</v>
<v Speaker 4>Like you didn't actually have bias </v>

694
00:46:33.610 --> 00:46:33.610
<v Speaker 4>towards constraining it.</v>

695
00:46:33.610 --> 00:46:38.500
<v Speaker 1>So it's definitely need discover new </v>
<v Speaker 1>strategies and I can share an anecdote </v>

696
00:46:38.500 --> 00:46:40.270
<v Speaker 1>where our tester,</v>
<v Speaker 1>we have a prohibition,</v>

697
00:46:40.271 --> 00:46:45.271
<v Speaker 1>would test the Bot and he played against</v>
<v Speaker 1>it for a long time and the bottom will </v>

698
00:46:45.431 --> 00:46:47.890
<v Speaker 1>do all kinds of things against the </v>
<v Speaker 1>player,</v>

699
00:46:47.920 --> 00:46:52.920
<v Speaker 1>the human player which were effective.</v>
<v Speaker 1>Then at some point that pro decided to </v>

700
00:46:52.991 --> 00:46:57.991
<v Speaker 1>play against the better flow pro and he </v>
<v Speaker 1>decided to imitate one of the things </v>

701
00:46:57.991 --> 00:47:00.880
<v Speaker 1>that they both was doing and this image.</v>
<v Speaker 1>But by imitating it,</v>

702
00:47:00.881 --> 00:47:03.880
<v Speaker 1>he was able to defeat a better pro.</v>
<v Speaker 1>So I think,</v>

703
00:47:03.910 --> 00:47:08.910
<v Speaker 1>I think the strategies to discoveries </v>
<v Speaker 1>are real and so like it means that </v>

704
00:47:08.910 --> 00:47:10.050
<v Speaker 1>there's very little transport,</v>
<v Speaker 1>but you know,</v>

705
00:47:11.530 --> 00:47:16.530
<v Speaker 1>I would say I think what that means is </v>
<v Speaker 1>that because the strategy's discovered </v>

706
00:47:17.201 --> 00:47:20.460
<v Speaker 1>by the bulk of the humans,</v>
<v Speaker 1>it means that the fundamental game plays</v>

707
00:47:20.461 --> 00:47:23.690
<v Speaker 1>the deeply related for,</v>
<v Speaker 1>for</v>

708
00:47:23.790 --> 00:47:28.790
<v Speaker 4>a long time now I've heard that the </v>
<v Speaker 4>objective of reinforcement learning is </v>

709
00:47:28.790 --> 00:47:29.950
<v Speaker 4>to determine policy</v>

710
00:47:30.100 --> 00:47:34.120
<v Speaker 5>that chooses an action to maximize the </v>
<v Speaker 5>expected reward,</v>

711
00:47:34.450 --> 00:47:39.450
<v Speaker 5>which is what you said earlier.</v>
<v Speaker 5>Would you ever want to look at the </v>

712
00:47:39.450 --> 00:47:42.520
<v Speaker 5>standard deviation of possible rewards?</v>
<v Speaker 5>Does that even make sense?</v>

713
00:47:42.690 --> 00:47:44.850
<v Speaker 1>Yeah,</v>
<v Speaker 1>I mean I think for sure,</v>

714
00:47:44.880 --> 00:47:46.560
<v Speaker 1>I think it's really application </v>
<v Speaker 1>dependent,</v>

715
00:47:47.490 --> 00:47:52.490
<v Speaker 1>one of the reasons to maximize the </v>
<v Speaker 1>expected reward just because it's easier</v>

716
00:47:52.501 --> 00:47:57.501
<v Speaker 1>to design algorithms for it.</v>
<v Speaker 1>So you write down this equation of the </v>

717
00:47:58.531 --> 00:48:00.480
<v Speaker 1>formula,</v>
<v Speaker 1>you do a little bit of derivation,</v>

718
00:48:00.720 --> 00:48:03.410
<v Speaker 1>you get something which amounts to a </v>
<v Speaker 1>nice looking algorithm.</v>

719
00:48:03.840 --> 00:48:08.840
<v Speaker 1>Now I think there exists like really </v>
<v Speaker 1>their existing applications where you'd </v>

720
00:48:10.051 --> 00:48:15.051
<v Speaker 1>never want to make mistakes and you </v>
<v Speaker 1>wanna work on the standard deviation as </v>

721
00:48:15.051 --> 00:48:16.050
<v Speaker 1>well.</v>
<v Speaker 1>But in practice it seems that the,</v>

722
00:48:16.410 --> 00:48:21.410
<v Speaker 1>just looking at the expected reward </v>
<v Speaker 1>covers a large fraction of the situation</v>

723
00:48:23.121 --> 00:48:25.010
<v Speaker 1>is you'd like to apply this to.</v>
<v Speaker 1>Okay,</v>

724
00:48:25.150 --> 00:48:26.000
<v Speaker 1>thanks.</v>
<v Speaker 1>Cam.</v>

725
00:48:27.340 --> 00:48:32.340
<v Speaker 5>Um,</v>
<v Speaker 5>we talked last week about motivations,</v>

726
00:48:32.860 --> 00:48:37.860
<v Speaker 5>um,</v>
<v Speaker 5>and that has a lot to do with the </v>

727
00:48:37.860 --> 00:48:40.160
<v Speaker 5>reinforcement and some of the ideas is </v>
<v Speaker 5>that the,</v>

728
00:48:40.161 --> 00:48:45.161
<v Speaker 5>uh,</v>
<v Speaker 5>our motivations are actually connection </v>

729
00:48:45.161 --> 00:48:48.230
<v Speaker 5>with others and cooperation and I'm </v>
<v Speaker 5>wondering if there through enough,</v>

730
00:48:48.470 --> 00:48:53.470
<v Speaker 5>and I understand it's very popular to </v>
<v Speaker 5>have the computers play these </v>

731
00:48:53.470 --> 00:48:54.530
<v Speaker 5>competitive games.</v>
<v Speaker 5>Um,</v>

732
00:48:54.860 --> 00:48:59.860
<v Speaker 5>but is there any use in like having an </v>
<v Speaker 5>agent self play collaboratively </v>

733
00:49:01.970 --> 00:49:02.870
<v Speaker 5>collaborative games?</v>

734
00:49:03.700 --> 00:49:08.700
<v Speaker 1>Yeah,</v>
<v Speaker 1>I think that's an extremely good </v>

735
00:49:08.700 --> 00:49:10.261
<v Speaker 1>question.</v>
<v Speaker 1>I think one place from which we can get </v>

736
00:49:10.261 --> 00:49:12.430
<v Speaker 1>some inspiration is from the evolution </v>
<v Speaker 1>of cooperation.</v>

737
00:49:13.470 --> 00:49:18.470
<v Speaker 1>I think cooperation be cooperate </v>
<v Speaker 1>ultimately because it's much better for </v>

738
00:49:21.581 --> 00:49:23.770
<v Speaker 1>you,</v>
<v Speaker 1>the person to be cooperative or not.</v>

739
00:49:24.850 --> 00:49:29.850
<v Speaker 1>And so I think what should happen if you</v>
<v Speaker 1>have a sufficiently open-ended game </v>

740
00:49:33.580 --> 00:49:35.560
<v Speaker 1>incorporation of it will be the winning </v>
<v Speaker 1>strategy.</v>

741
00:49:36.670 --> 00:49:39.400
<v Speaker 1>And so I think we will get cooperation </v>
<v Speaker 1>whether it be like it or not.</v>

742
00:49:43.990 --> 00:49:45.100
<v Speaker 6>Hi.</v>
<v Speaker 6>Um,</v>

743
00:49:45.130 --> 00:49:49.900
<v Speaker 6>you mentioned the complexity of the </v>
<v Speaker 6>simulation of friction.</v>

744
00:49:50.230 --> 00:49:54.670
<v Speaker 6>I was wondering if you feel that there </v>
<v Speaker 6>exist open complexity theoretic problems</v>

745
00:49:54.671 --> 00:49:59.671
<v Speaker 6>relevant to relevant to ai or whether </v>
<v Speaker 6>it's just a matter of finding good </v>

746
00:49:59.671 --> 00:50:03.811
<v Speaker 6>approximations that humans have,</v>
<v Speaker 6>the types of problems that humans tend </v>

747
00:50:03.811 --> 00:50:03.811
<v Speaker 6>to solve?</v>

748
00:50:04.270 --> 00:50:06.040
<v Speaker 1>Yeah.</v>
<v Speaker 1>So complexity theory.</v>

749
00:50:07.150 --> 00:50:10.240
<v Speaker 1>Well,</v>
<v Speaker 1>like at the very basic level,</v>

750
00:50:10.870 --> 00:50:13.780
<v Speaker 1>we know that whatever algorithm we're </v>
<v Speaker 1>going to run,</v>

751
00:50:13.930 --> 00:50:16.330
<v Speaker 1>he's going to run fairly efficiently on </v>
<v Speaker 1>some hardware.</v>

752
00:50:16.900 --> 00:50:21.900
<v Speaker 1>So that puts a pretty strict upper bound</v>
<v Speaker 1>and the true complexity of the problems </v>

753
00:50:22.661 --> 00:50:24.550
<v Speaker 1>you're solving,</v>
<v Speaker 1>like by definition,</v>

754
00:50:24.970 --> 00:50:28.430
<v Speaker 1>via solving problems which aren't too </v>
<v Speaker 1>hard in complexity theoretic sense.</v>

755
00:50:28.760 --> 00:50:33.140
<v Speaker 1>Now it is also the case that many of the</v>
<v Speaker 1>problems.</v>

756
00:50:33.440 --> 00:50:38.440
<v Speaker 1>So while the overall thing that we do is</v>
<v Speaker 1>not hard from a complexity theoretic </v>

757
00:50:38.440 --> 00:50:41.690
<v Speaker 1>sense and indeed humans cannot solve and</v>
<v Speaker 1>be complete problems in general,</v>

758
00:50:43.400 --> 00:50:48.400
<v Speaker 1>it is true that many of the like </v>
<v Speaker 1>optimization problems that we pose to </v>

759
00:50:48.400 --> 00:50:50.240
<v Speaker 1>our algorithms are in intractable in the</v>
<v Speaker 1>general case,</v>

760
00:50:50.450 --> 00:50:52.880
<v Speaker 1>starting from neural net optimization </v>
<v Speaker 1>itself,</v>

761
00:50:53.370 --> 00:50:58.370
<v Speaker 1>it is easy to create a family of </v>
<v Speaker 1>datasets for a neural network with a </v>

762
00:50:58.370 --> 00:51:00.530
<v Speaker 1>very small number of neurons such that </v>
<v Speaker 1>find the global optimum is not complete.</v>

763
00:51:01.370 --> 00:51:04.700
<v Speaker 1>And so how do we avoid it?</v>
<v Speaker 1>Well,</v>

764
00:51:04.701 --> 00:51:07.580
<v Speaker 1>we just try grading dissent anyway and </v>
<v Speaker 1>somehow it works,</v>

765
00:51:08.780 --> 00:51:13.780
<v Speaker 1>but without question like we cannot,</v>
<v Speaker 1>we do not solve problems which are truly</v>

766
00:51:16.131 --> 00:51:21.131
<v Speaker 1>intractable.</v>
<v Speaker 1>So I mean I have the sense of the </v>

767
00:51:21.131 --> 00:51:21.131
<v Speaker 1>question.</v>

768
00:51:21.131 --> 00:51:25.930
<v Speaker 7>Hello.</v>
<v Speaker 7>It seems like an important sub problem </v>

769
00:51:25.930 --> 00:51:28.130
<v Speaker 7>on the path towards Agi will be </v>
<v Speaker 7>understanding language.</v>

770
00:51:28.490 --> 00:51:32.600
<v Speaker 7>And the state of generative language </v>
<v Speaker 7>modeling right now is pretty abysmal.</v>

771
00:51:33.010 --> 00:51:36.740
<v Speaker 7>What do you think are the most </v>
<v Speaker 7>productive research trajectories towards</v>

772
00:51:36.741 --> 00:51:37.970
<v Speaker 7>generative language models?</v>

773
00:51:38.330 --> 00:51:43.330
<v Speaker 1>So I'll first say that you are </v>
<v Speaker 1>completely correct that the situation </v>

774
00:51:43.330 --> 00:51:46.190
<v Speaker 1>with language is still far from great.</v>
<v Speaker 1>Although progress has been made,</v>

775
00:51:46.550 --> 00:51:51.550
<v Speaker 1>even without any particular innovations </v>
<v Speaker 1>beyond models that exist today,</v>

776
00:51:52.850 --> 00:51:57.850
<v Speaker 1>simply scaling up models that exist </v>
<v Speaker 1>today on larger data sets is going to go</v>

777
00:51:58.011 --> 00:52:00.020
<v Speaker 1>surprisingly far,</v>
<v Speaker 1>not even larger datasets,</v>

778
00:52:00.080 --> 00:52:02.180
<v Speaker 1>but larger and deeper models.</v>
<v Speaker 1>For example,</v>

779
00:52:02.360 --> 00:52:06.710
<v Speaker 1>if you train a language model with a </v>
<v Speaker 1>thousand layers and it's the same layer,</v>

780
00:52:06.800 --> 00:52:11.800
<v Speaker 1>I think it's going to be a pretty </v>
<v Speaker 1>amazing language model like we don't </v>

781
00:52:11.800 --> 00:52:15.140
<v Speaker 1>have the cycles for week yet,</v>
<v Speaker 1>but to think it will change very soon.</v>

782
00:52:15.650 --> 00:52:20.650
<v Speaker 1>Now.</v>
<v Speaker 1>I also agree with you that there are </v>

783
00:52:20.650 --> 00:52:23.380
<v Speaker 1>some fundamental things missing in our </v>
<v Speaker 1>current understanding of deep learning </v>

784
00:52:24.560 --> 00:52:28.070
<v Speaker 1>which prevent us from really solving the</v>
<v Speaker 1>problem that we want.</v>

785
00:52:28.370 --> 00:52:33.370
<v Speaker 1>So I think one of these problems,</v>
<v Speaker 1>one of the things that's missing is </v>

786
00:52:33.370 --> 00:52:33.370
<v Speaker 1>that,</v>
<v Speaker 1>or that seems like blatantly wrong,</v>

787
00:52:33.470 --> 00:52:38.470
<v Speaker 1>is the fact that we train a model and </v>
<v Speaker 1>then you stop training the model and you</v>

788
00:52:40.761 --> 00:52:41.270
<v Speaker 1>freeze it.</v>

789
00:52:41.900 --> 00:52:46.900
<v Speaker 1>Even though it's the training process </v>
<v Speaker 1>for the magic really happens of the </v>

790
00:52:47.691 --> 00:52:52.691
<v Speaker 1>magic is if you think about it,</v>
<v Speaker 1>like the training process is the true </v>

791
00:52:52.821 --> 00:52:55.640
<v Speaker 1>general part of the whole,</v>
<v Speaker 1>of the whole of the whole story.</v>

792
00:52:56.150 --> 00:53:01.150
<v Speaker 1>Because you're tends to flow code </v>
<v Speaker 1>doesn't care what your data set to </v>

793
00:53:01.150 --> 00:53:01.150
<v Speaker 1>optimize.</v>
<v Speaker 1>He just says whatever,</v>

794
00:53:01.150 --> 00:53:05.051
<v Speaker 1>just give me the data set,</v>
<v Speaker 1>I don't care which one solve also the </v>

795
00:53:05.051 --> 00:53:07.491
<v Speaker 1>mall.</v>
<v Speaker 1>So like the ability to do that feels </v>

796
00:53:07.491 --> 00:53:11.060
<v Speaker 1>really special and I think we are not </v>
<v Speaker 1>using it at test time.</v>

797
00:53:11.570 --> 00:53:14.990
<v Speaker 1>It gets hard to speculate about like </v>
<v Speaker 1>things you don't know the answer,</v>

798
00:53:14.991 --> 00:53:18.260
<v Speaker 1>but all I'll say is that simply train </v>
<v Speaker 1>bigger,</v>

799
00:53:18.261 --> 00:53:22.850
<v Speaker 1>deeper language models.</v>
<v Speaker 1>They'll go surprisingly far scaling out,</v>

800
00:53:22.860 --> 00:53:24.770
<v Speaker 1>but also doing things like training or </v>
<v Speaker 1>test stamina in,</v>

801
00:53:24.780 --> 00:53:25.380
<v Speaker 1>for instance,</v>
<v Speaker 1>time.</v>

802
00:53:25.381 --> 00:53:28.830
<v Speaker 1>I think it will be another important </v>
<v Speaker 1>boosts the performance.</v>

803
00:53:30.290 --> 00:53:31.670
<v Speaker 8>Hi.</v>
<v Speaker 8>Thank you for the talk.</v>

804
00:53:32.090 --> 00:53:37.090
<v Speaker 8>Um,</v>
<v Speaker 8>so it seems like right now another </v>

805
00:53:37.090 --> 00:53:37.190
<v Speaker 8>interesting approach to solving </v>
<v Speaker 8>reinforcement learning problems could be</v>

806
00:53:37.191 --> 00:53:42.191
<v Speaker 8>to go for the evolutionary evolutionary </v>
<v Speaker 8>strategies and although they have their </v>

807
00:53:43.010 --> 00:53:48.010
<v Speaker 8>caveats,</v>
<v Speaker 8>I wanted to know if I'd open ai </v>

808
00:53:48.010 --> 00:53:48.380
<v Speaker 8>particularly you're working on something</v>
<v Speaker 8>related and what are,</v>

809
00:53:48.410 --> 00:53:49.850
<v Speaker 8>what is your general opinion on them?</v>

810
00:53:51.080 --> 00:53:56.080
<v Speaker 1>So like at present,</v>
<v Speaker 1>I believe that something like </v>

811
00:53:56.080 --> 00:53:58.190
<v Speaker 1>evolutionary strategies is another great</v>
<v Speaker 1>for reinforcement learning.</v>

812
00:53:58.580 --> 00:54:00.620
<v Speaker 1>I think that normal reinforcement </v>
<v Speaker 1>learning algorithms,</v>

813
00:54:00.621 --> 00:54:03.170
<v Speaker 1>especially if it's big policies a </v>
<v Speaker 1>better,</v>

814
00:54:03.800 --> 00:54:08.800
<v Speaker 1>but I think if you want to evolve a </v>
<v Speaker 1>small compact object like like a piece </v>

815
00:54:08.800 --> 00:54:12.620
<v Speaker 1>of code for example,</v>
<v Speaker 1>I think that would be a place where it's</v>

816
00:54:12.621 --> 00:54:15.230
<v Speaker 1>gonna be a seriously worth considering,</v>
<v Speaker 1>but this,</v>

817
00:54:15.410 --> 00:54:20.410
<v Speaker 1>you know,</v>
<v Speaker 1>you've all been a useful piece of code </v>

818
00:54:20.410 --> 00:54:21.560
<v Speaker 1>is a cool idea.</v>
<v Speaker 1>Hasn't been done yet,</v>

819
00:54:21.561 --> 00:54:23.990
<v Speaker 1>so still a lot of work to be done.</v>
<v Speaker 1>Before we get there.</v>

820
00:54:25.100 --> 00:54:26.420
<v Speaker 9>Hi.</v>
<v Speaker 9>Thank you so much for coming.</v>

821
00:54:26.750 --> 00:54:31.750
<v Speaker 9>My question is,</v>
<v Speaker 9>you mentioned what is the right goal is </v>

822
00:54:31.750 --> 00:54:36.131
<v Speaker 9>a political problem,</v>
<v Speaker 9>so I'm wondering if you can elaborate a </v>

823
00:54:36.131 --> 00:54:38.651
<v Speaker 9>bit on that and it also,</v>
<v Speaker 9>what do you think would be the approach </v>

824
00:54:38.651 --> 00:54:38.651
<v Speaker 9>for us to maybe get.</v>

825
00:54:40.340 --> 00:54:40.880
<v Speaker 1>Well?</v>
<v Speaker 1>Again,</v>

826
00:54:40.910 --> 00:54:45.760
<v Speaker 1>I can't really comment too much because </v>
<v Speaker 1>all the thoughts that we have.</v>

827
00:54:45.990 --> 00:54:50.990
<v Speaker 1>You now have a few people who are </v>
<v Speaker 1>thinking about this full time at open </v>

828
00:54:50.990 --> 00:54:54.870
<v Speaker 1>Ai.</v>
<v Speaker 1>I don't have enough of this super strong</v>

829
00:54:55.101 --> 00:55:00.101
<v Speaker 1>opinion to say anything too definitive.</v>
<v Speaker 1>All I can say at the very high level </v>

830
00:55:00.101 --> 00:55:02.510
<v Speaker 1>isn't given the size.</v>
<v Speaker 1>Like if you go into the future,</v>

831
00:55:02.540 --> 00:55:05.690
<v Speaker 1>whenever soon or whenever it's going to </v>
<v Speaker 1>happen,</v>

832
00:55:05.691 --> 00:55:10.691
<v Speaker 1>when you build a computer,</v>
<v Speaker 1>which can do anything better than a </v>

833
00:55:10.691 --> 00:55:14.621
<v Speaker 1>human,</v>
<v Speaker 1>it's will happen because the brain is </v>

834
00:55:14.621 --> 00:55:16.841
<v Speaker 1>physical,</v>
<v Speaker 1>being picked on society is going to be </v>

835
00:55:16.841 --> 00:55:20.531
<v Speaker 1>completely massive and overwhelming.</v>
<v Speaker 1>It's very difficult to imagine even if </v>

836
00:55:22.131 --> 00:55:25.200
<v Speaker 1>you try really hard.</v>
<v Speaker 1>And I think what it means is that people</v>

837
00:55:25.210 --> 00:55:29.240
<v Speaker 1>do care a lot and that's what I was </v>
<v Speaker 1>alluding to,</v>

838
00:55:29.241 --> 00:55:32.960
<v Speaker 1>the fact that this will be something </v>
<v Speaker 1>that many people who care about strongly</v>

839
00:55:34.880 --> 00:55:38.870
<v Speaker 1>and like as the impact increases </v>
<v Speaker 1>gradually be self driving cars,</v>

840
00:55:38.871 --> 00:55:43.871
<v Speaker 1>more automation,</v>
<v Speaker 1>I think we will see a lot more people </v>

841
00:55:43.871 --> 00:55:43.871
<v Speaker 1>care.</v>

842
00:55:43.871 --> 00:55:47.830
<v Speaker 9>Do we need to have a very accurate model</v>
<v Speaker 9>of the physical world and then simulate </v>

843
00:55:48.321 --> 00:55:50.540
<v Speaker 9>that in order to have,</v>
<v Speaker 9>uh,</v>

844
00:55:50.780 --> 00:55:55.100
<v Speaker 9>these agents that can eventually come </v>
<v Speaker 9>out into the real world and do something</v>

845
00:55:55.101 --> 00:55:56.640
<v Speaker 9>approaching,</v>
<v Speaker 9>you know,</v>

846
00:55:56.810 --> 00:55:58.700
<v Speaker 9>human level intelligence tasks.</v>

847
00:55:59.590 --> 00:56:03.220
<v Speaker 1>That's a very good question.</v>
<v Speaker 1>So I think if that were the case,</v>

848
00:56:03.280 --> 00:56:08.280
<v Speaker 1>we'd be in trouble and I am very certain</v>
<v Speaker 1>that it could be avoided.</v>

849
00:56:10.990 --> 00:56:14.410
<v Speaker 1>So specifically,</v>
<v Speaker 1>the real answer has to be that look,</v>

850
00:56:14.800 --> 00:56:17.830
<v Speaker 1>you learn to problem solve,</v>
<v Speaker 1>we learned to negotiate,</v>

851
00:56:17.831 --> 00:56:19.030
<v Speaker 1>you learn to persist,</v>
<v Speaker 1>you know,</v>

852
00:56:19.050 --> 00:56:22.230
<v Speaker 1>lots of different useful life lessons in</v>
<v Speaker 1>the simulation and yes,</v>

853
00:56:22.231 --> 00:56:24.970
<v Speaker 1>you learn some physics too,</v>
<v Speaker 1>but I ain't go outside to the real world</v>

854
00:56:25.390 --> 00:56:30.390
<v Speaker 1>and you have to start over to some </v>
<v Speaker 1>extent because many of your deeply held </v>

855
00:56:30.390 --> 00:56:32.290
<v Speaker 1>assumptions will be false and amount of </v>
<v Speaker 1>the goals,</v>

856
00:56:32.390 --> 00:56:37.120
<v Speaker 1>so that's one of the reasons I care so </v>
<v Speaker 1>much about never stopped in training.</v>

857
00:56:37.930 --> 00:56:42.930
<v Speaker 1>You've accumulated your knowledge.</v>
<v Speaker 1>Now we go into an environment for some </v>

858
00:56:42.930 --> 00:56:42.930
<v Speaker 1>of your assumptions of let you continued</v>
<v Speaker 1>training.</v>

859
00:56:42.970 --> 00:56:47.970
<v Speaker 1>We tried to connect the new data to your</v>
<v Speaker 1>old data and this is an important </v>

860
00:56:47.970 --> 00:56:48.700
<v Speaker 1>requirement from our algorithms,</v>
<v Speaker 1>which is already met to some extent,</v>

861
00:56:48.701 --> 00:56:53.701
<v Speaker 1>but it will have to be met a lot more so</v>
<v Speaker 1>that you can take the partial knowledge </v>

862
00:56:53.701 --> 00:56:57.700
<v Speaker 1>is required and go to a new situation,</v>
<v Speaker 1>learn some more.</v>

863
00:56:57.980 --> 00:57:00.730
<v Speaker 1>Literally the example of you go to </v>
<v Speaker 1>school alone,</v>

864
00:57:00.750 --> 00:57:02.410
<v Speaker 1>learn useful things,</v>
<v Speaker 1>then you go to work.</v>

865
00:57:02.830 --> 00:57:04.300
<v Speaker 1>It's not perfect.</v>
<v Speaker 1>It's not.</v>

866
00:57:04.301 --> 00:57:09.301
<v Speaker 1>You know,</v>
<v Speaker 1>you pull your four years of cs in </v>

867
00:57:09.301 --> 00:57:11.131
<v Speaker 1>Undergrad is not going to fully prepare </v>
<v Speaker 1>you for whatever it is you need to know </v>

868
00:57:11.131 --> 00:57:11.530
<v Speaker 1>how to work.</v>
<v Speaker 1>It will help somewhat.</v>

869
00:57:11.860 --> 00:57:13.960
<v Speaker 1>You'll be able to get off the ground,</v>
<v Speaker 1>but there will be lots of new things you</v>

870
00:57:13.961 --> 00:57:15.010
<v Speaker 1>need to learn.</v>
<v Speaker 1>So that's,</v>

871
00:57:15.011 --> 00:57:17.190
<v Speaker 1>that's the spirit of it.</v>
<v Speaker 1>I think of those,</v>

872
00:57:17.210 --> 00:57:17.920
<v Speaker 1>of the school,</v>

873
00:57:18.920 --> 00:57:21.950
<v Speaker 10>one of the things you mentioned pretty </v>
<v Speaker 10>early on in your talk is that one of the</v>

874
00:57:21.951 --> 00:57:26.951
<v Speaker 10>limitations of this sort of style of </v>
<v Speaker 10>reinforcement learning is there is no </v>

875
00:57:26.951 --> 00:57:30.731
<v Speaker 10>self organization so you have to tell it</v>
<v Speaker 10>went into the good thing or is it a bad </v>

876
00:57:30.731 --> 00:57:34.481
<v Speaker 10>thing and that's actually a problem in </v>
<v Speaker 10>neuroscience as well and you're trying </v>

877
00:57:34.481 --> 00:57:34.481
<v Speaker 10>to teach a rat to,</v>
<v Speaker 10>you know,</v>

878
00:57:34.481 --> 00:57:35.600
<v Speaker 10>navigate a maze.</v>
<v Speaker 10>You have to artificially tell it what to</v>

879
00:57:35.601 --> 00:57:40.601
<v Speaker 10>do.</v>
<v Speaker 10>So where do you see moving forward when </v>

880
00:57:40.601 --> 00:57:40.601
<v Speaker 10>we already have this problem with </v>
<v Speaker 10>teaching,</v>

881
00:57:40.601 --> 00:57:44.381
<v Speaker 10>you know,</v>
<v Speaker 10>not necessarily learning but also </v>

882
00:57:44.381 --> 00:57:46.571
<v Speaker 10>teaching.</v>
<v Speaker 10>So where do you see the research moving </v>

883
00:57:46.571 --> 00:57:46.571
<v Speaker 10>forward in that respect?</v>
<v Speaker 10>How do you sort of introduce this notion</v>

884
00:57:46.571 --> 00:57:47.600
<v Speaker 10>of self organization?</v>

885
00:57:48.180 --> 00:57:53.180
<v Speaker 1>So I think without question one really </v>
<v Speaker 1>important thing you need to do is to be </v>

886
00:57:53.180 --> 00:57:57.160
<v Speaker 1>able to infer the goals and strategies </v>
<v Speaker 1>of other agents by observing them.</v>

887
00:57:58.470 --> 00:58:01.160
<v Speaker 1>That's a fundamental skill we need to be</v>
<v Speaker 1>able to learn to,</v>

888
00:58:01.330 --> 00:58:03.150
<v Speaker 1>to embed into the agents.</v>
<v Speaker 1>So if,</v>

889
00:58:03.151 --> 00:58:04.450
<v Speaker 1>for example,</v>
<v Speaker 1>we have two agents,</v>

890
00:58:04.780 --> 00:58:06.970
<v Speaker 1>one of them is doing something and the </v>
<v Speaker 1>other agents says,</v>

891
00:58:06.971 --> 00:58:08.830
<v Speaker 1>well that's really cool.</v>
<v Speaker 1>I want to be able to do that too,</v>

892
00:58:09.250 --> 00:58:14.250
<v Speaker 1>and then you go on and do that.</v>
<v Speaker 1>And so I'd say that this is a very </v>

893
00:58:14.250 --> 00:58:16.801
<v Speaker 1>important component in terms of setting </v>
<v Speaker 1>the reward of you see what they do,</v>

894
00:58:17.480 --> 00:58:22.480
<v Speaker 1>you further reward and now we have a </v>
<v Speaker 1>knob which says you see what they're </v>

895
00:58:22.480 --> 00:58:23.380
<v Speaker 1>doing now go and try to do the same </v>
<v Speaker 1>thing.</v>

896
00:58:24.120 --> 00:58:26.160
<v Speaker 1>So let's say this,</v>
<v Speaker 1>this is as far as I,</v>

897
00:58:26.230 --> 00:58:31.230
<v Speaker 1>as far as I know,</v>
<v Speaker 1>this was one of the important ways in </v>

898
00:58:31.230 --> 00:58:33.871
<v Speaker 1>which humans are quite different from </v>
<v Speaker 1>other animals in the way which in the </v>

899
00:58:37.750 --> 00:58:42.280
<v Speaker 1>scale and scope in which we copy the </v>
<v Speaker 1>behavior of other humans</v>

900
00:58:43.790 --> 00:58:45.620
<v Speaker 10>mind.</v>
<v Speaker 10>If I ask a quick followup or go for it.</v>

901
00:58:45.950 --> 00:58:48.950
<v Speaker 10>So that's kind of obvious how that works</v>
<v Speaker 10>in this scope of competition,</v>

902
00:58:48.951 --> 00:58:53.951
<v Speaker 10>but what about just sort of arbitrary </v>
<v Speaker 10>tasks like I'm in a math class with </v>

903
00:58:53.951 --> 00:58:57.551
<v Speaker 10>someone and I see someone doing a </v>
<v Speaker 10>problem in a particular way and I'm </v>

904
00:58:57.551 --> 00:58:57.551
<v Speaker 10>like,</v>
<v Speaker 10>oh that's a good strategy.</v>

905
00:58:57.551 --> 00:59:01.181
<v Speaker 10>Maybe I should try that out.</v>
<v Speaker 10>How does that work in a sort of non </v>

906
00:59:01.181 --> 00:59:01.181
<v Speaker 10>competitive environment?</v>

907
00:59:01.181 --> 00:59:03.310
<v Speaker 1>So I think that this movie,</v>
<v Speaker 1>I think that that's going to be a little</v>

908
00:59:03.311 --> 00:59:06.370
<v Speaker 1>bit separate from the competitive </v>
<v Speaker 1>environment,</v>

909
00:59:06.640 --> 00:59:11.530
<v Speaker 1>but it will have to be somehow either </v>
<v Speaker 1>bake,</v>

910
00:59:11.860 --> 00:59:16.860
<v Speaker 1>probably baked in,</v>
<v Speaker 1>maybe evolved into the system where if </v>

911
00:59:17.291 --> 00:59:22.291
<v Speaker 1>you have other agents doing things,</v>
<v Speaker 1>they're generating data which you and </v>

912
00:59:22.291 --> 00:59:26.480
<v Speaker 1>the only way to truly make sense of the </v>
<v Speaker 1>data that you see used to infer the goal</v>

913
00:59:26.481 --> 00:59:29.480
<v Speaker 1>of the agent to strategy their beliefs </v>
<v Speaker 1>state.</v>

914
00:59:29.810 --> 00:59:31.700
<v Speaker 1>That's important also for communicating </v>
<v Speaker 1>with them.</v>

915
00:59:32.470 --> 00:59:34.160
<v Speaker 1>If you want to successfully communicate </v>
<v Speaker 1>with someone,</v>

916
00:59:34.161 --> 00:59:39.161
<v Speaker 1>you have to keep track both of their </v>
<v Speaker 1>goal and their beliefs state instead of </v>

917
00:59:39.161 --> 00:59:41.951
<v Speaker 1>knowledge.</v>
<v Speaker 1>So I think people find that there are </v>

918
00:59:41.951 --> 00:59:44.381
<v Speaker 1>many,</v>
<v Speaker 1>I guess connections between </v>

919
00:59:44.381 --> 00:59:45.020
<v Speaker 1>understanding what other agents are </v>
<v Speaker 1>doing,</v>

920
00:59:45.140 --> 00:59:50.140
<v Speaker 1>inferring their goals,</v>
<v Speaker 1>imitating them and community </v>

921
00:59:50.140 --> 00:59:50.140
<v Speaker 1>successfully communicating them.</v>
<v Speaker 1>Alright,</v>

922
00:59:50.140 --> 00:59:51.790
<v Speaker 1>let's give Eylea and the happy hour </v>
<v Speaker 1>hand.</v>

923
00:59:52.190 --> 00:59:57.190
<v Speaker 2>Thank you.</v>

