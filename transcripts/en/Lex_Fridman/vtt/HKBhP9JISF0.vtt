WEBVTT

1
00:00:00.510 --> 00:00:05.510
<v Speaker 1>Today we have sterling Anderson is the </v>
<v Speaker 1>cofounder of Aurora and exciting new </v>

2
00:00:05.641 --> 00:00:10.641
<v Speaker 1>self driving car company.</v>
<v Speaker 1>Previously he was the head of the Tesla </v>

3
00:00:10.641 --> 00:00:13.950
<v Speaker 1>autopilot team that brought both the </v>
<v Speaker 1>first and second generation autopilot to</v>

4
00:00:13.951 --> 00:00:17.880
<v Speaker 1>life.</v>
<v Speaker 1>Before that he did his phd at Mit,</v>

5
00:00:17.881 --> 00:00:21.660
<v Speaker 1>working on shared humor,</v>
<v Speaker 1>machine control of ground vehicles,</v>

6
00:00:21.990 --> 00:00:25.170
<v Speaker 1>the very thing I've been harping on over</v>
<v Speaker 1>and over in this class,</v>

7
00:00:25.680 --> 00:00:29.010
<v Speaker 1>and now he's back at mit to talk with </v>
<v Speaker 1>us.</v>

8
00:00:29.190 --> 00:00:30.540
<v Speaker 1>Please give him a warm welcome.</v>

9
00:00:36.130 --> 00:00:37.810
<v Speaker 2>Thank you.</v>
<v Speaker 2>It's good to be here.</v>

10
00:00:37.811 --> 00:00:42.811
<v Speaker 2>I was telling Lex just before,</v>
<v Speaker 2>I think it's been a little while since </v>

11
00:00:42.811 --> 00:00:44.050
<v Speaker 2>I've been back to the institute and so </v>
<v Speaker 2>great to be here.</v>

12
00:00:44.620 --> 00:00:49.620
<v Speaker 2>I want to apologize in advance.</v>
<v Speaker 2>I've just landed this afternoon from </v>

13
00:00:49.620 --> 00:00:53.581
<v Speaker 2>Korea via Germany where I've been </v>
<v Speaker 2>spending the last week and so I may </v>

14
00:00:53.921 --> 00:00:58.921
<v Speaker 2>speak a little slower than normal.</v>
<v Speaker 2>Please bear with me if I become </v>

15
00:00:58.921 --> 00:00:59.770
<v Speaker 2>incoherent or a slur,</v>
<v Speaker 2>my speech,</v>

16
00:01:00.010 --> 00:01:03.220
<v Speaker 2>somebody flag it too and we'll uh,</v>
<v Speaker 2>we'll try to make corrections.</v>

17
00:01:03.520 --> 00:01:08.520
<v Speaker 2>So tonight I thought I'd chat with you a</v>
<v Speaker 2>little bit about my journey over the </v>

18
00:01:08.520 --> 00:01:10.930
<v Speaker 2>last decade span,</v>
<v Speaker 2>just over 10 years since I was at mit.</v>

19
00:01:11.410 --> 00:01:16.410
<v Speaker 2>A lot has changed,</v>
<v Speaker 2>a lot has changed for the better in the </v>

20
00:01:16.410 --> 00:01:18.991
<v Speaker 2>self driving community and I've been </v>
<v Speaker 2>privileged to be a part of many of those</v>

21
00:01:19.191 --> 00:01:24.191
<v Speaker 2>changes and so on at the time with you,</v>
<v Speaker 2>a little bit about some of the things </v>

22
00:01:24.191 --> 00:01:24.191
<v Speaker 2>that I've learned,</v>
<v Speaker 2>some of the things that I've experienced</v>

23
00:01:24.230 --> 00:01:29.230
<v Speaker 2>and then maybe end by talking about sort</v>
<v Speaker 2>of where we go from here and what the </v>

24
00:01:29.781 --> 00:01:32.600
<v Speaker 2>next steps are both for the industry at </v>
<v Speaker 2>large,</v>

25
00:01:32.601 --> 00:01:37.601
<v Speaker 2>but also for the company that we're </v>
<v Speaker 2>building that Alex mentioned is called </v>

26
00:01:37.601 --> 00:01:39.290
<v Speaker 2>Aurora to start out with.</v>

27
00:01:40.230 --> 00:01:45.230
<v Speaker 2>There are a few sort of key phases or </v>
<v Speaker 2>transitions in my journey over the last </v>

28
00:01:45.591 --> 00:01:49.550
<v Speaker 2>10 years.</v>
<v Speaker 2>As I mentioned when I started at Mit,</v>

29
00:01:49.551 --> 00:01:52.560
<v Speaker 2>I worked with Carl Yani,</v>
<v Speaker 2>Mr Amelio for Zalia,</v>

30
00:01:52.561 --> 00:01:57.561
<v Speaker 2>John Leonard,</v>
<v Speaker 2>a few others on some of these sort of </v>

31
00:01:57.561 --> 00:02:01.170
<v Speaker 2>shared adaptive automation approaches.</v>
<v Speaker 2>Um,</v>

32
00:02:01.171 --> 00:02:04.310
<v Speaker 2>I'll talk a little bit about those from </v>
<v Speaker 2>there.</v>

33
00:02:04.340 --> 00:02:09.340
<v Speaker 2>I spent some time at Tesla where I first</v>
<v Speaker 2>sled the model x program as we both </v>

34
00:02:10.851 --> 00:02:12.660
<v Speaker 2>finished the development and ultimately </v>
<v Speaker 2>launched it,</v>

35
00:02:13.640 --> 00:02:18.640
<v Speaker 2>took over the autopilot program where we</v>
<v Speaker 2>introduce a number of new,</v>

36
00:02:19.790 --> 00:02:24.790
<v Speaker 2>both active safety,</v>
<v Speaker 2>but also sort of enhanced convenience </v>

37
00:02:25.371 --> 00:02:30.371
<v Speaker 2>features from autosteer to debit cruise </v>
<v Speaker 2>control that we're able to refine a few </v>

38
00:02:30.381 --> 00:02:35.381
<v Speaker 2>unique ways and we'll talk a little bit </v>
<v Speaker 2>about that and then from there in </v>

39
00:02:35.381 --> 00:02:39.551
<v Speaker 2>December of last year of 2016,</v>
<v Speaker 2>I guess now we started a new company </v>

40
00:02:39.551 --> 00:02:41.480
<v Speaker 2>called Aurora and I'll tell you a little</v>
<v Speaker 2>bit about that.</v>

41
00:02:42.350 --> 00:02:45.530
<v Speaker 2>So to start out with,</v>
<v Speaker 2>when I came,</v>

42
00:02:45.531 --> 00:02:46.910
<v Speaker 2>no,</v>
<v Speaker 2>it was 2007.</v>

43
00:02:47.030 --> 00:02:52.030
<v Speaker 2>The Darpa urban challenges were well </v>
<v Speaker 2>underway at that stage and one of the </v>

44
00:02:52.030 --> 00:02:56.201
<v Speaker 2>things that we wanted to do is find a </v>
<v Speaker 2>way to address some of these safety </v>

45
00:02:56.201 --> 00:03:00.281
<v Speaker 2>issues in human driving earlier.</v>
<v Speaker 2>Then potentially full cell driving could</v>

46
00:03:00.971 --> 00:03:05.380
<v Speaker 2>do and so we developed what became known</v>
<v Speaker 2>as the intelligent copilot.</v>

47
00:03:06.010 --> 00:03:09.280
<v Speaker 2>What you see here is a simulation of </v>
<v Speaker 2>that operating.</v>

48
00:03:09.281 --> 00:03:11.260
<v Speaker 2>I'll tell you a little bit more about </v>
<v Speaker 2>that in just a second,</v>

49
00:03:12.520 --> 00:03:15.250
<v Speaker 2>but to explain a little bit about the </v>
<v Speaker 2>methodology.</v>

50
00:03:15.251 --> 00:03:20.251
<v Speaker 2>The innovation,</v>
<v Speaker 2>the key approach that we took there was </v>

51
00:03:20.251 --> 00:03:22.620
<v Speaker 2>slightly different from what a </v>
<v Speaker 2>traditional planning control theory,</v>

52
00:03:22.630 --> 00:03:26.920
<v Speaker 2>which we were doing was instead of </v>
<v Speaker 2>designing in path space,</v>

53
00:03:26.960 --> 00:03:27.910
<v Speaker 2>uh,</v>
<v Speaker 2>for the robot,</v>

54
00:03:27.911 --> 00:03:32.140
<v Speaker 2>we instead a found a way to identify a </v>
<v Speaker 2>plan,</v>

55
00:03:32.150 --> 00:03:37.150
<v Speaker 2>optimize and design a controller subject</v>
<v Speaker 2>to a set of constraints rather than </v>

56
00:03:37.150 --> 00:03:37.420
<v Speaker 2>paths.</v>

57
00:03:38.080 --> 00:03:40.940
<v Speaker 2>And so what we're doing is looking for </v>
<v Speaker 2>Hamas certain environment.</v>

58
00:03:40.941 --> 00:03:45.941
<v Speaker 2>So imagine for a moment in an </v>
<v Speaker 2>environment that's a pockmarked by </v>

59
00:03:45.941 --> 00:03:46.630
<v Speaker 2>objects,</v>
<v Speaker 2>by their vehicles,</v>

60
00:03:46.631 --> 00:03:48.040
<v Speaker 2>by pedestrians,</v>
<v Speaker 2>etc.</v>

61
00:03:49.210 --> 00:03:53.020
<v Speaker 2>If you were to create the voronoi </v>
<v Speaker 2>diagrams through that environment,</v>

62
00:03:53.170 --> 00:03:56.580
<v Speaker 2>you would have a set of each unique,</v>
<v Speaker 2>uh,</v>

63
00:03:56.710 --> 00:04:01.000
<v Speaker 2>set of pads or hama is continuously </v>
<v Speaker 2>deformable paths that will take you from</v>

64
00:04:01.001 --> 00:04:03.970
<v Speaker 2>one,</v>
<v Speaker 2>one location to another through it.</v>

65
00:04:04.960 --> 00:04:07.360
<v Speaker 2>If you then turn that into its dual,</v>
<v Speaker 2>which is the dullest,</v>

66
00:04:07.361 --> 00:04:12.361
<v Speaker 2>a triangulation of set environment,</v>
<v Speaker 2>presuming that you've got conduct's </v>

67
00:04:12.361 --> 00:04:15.991
<v Speaker 2>obstacles,</v>
<v Speaker 2>you can then tie all those together </v>

68
00:04:15.991 --> 00:04:15.991
<v Speaker 2>rather trivially to create a set of,</v>
<v Speaker 2>uh,</v>

69
00:04:15.991 --> 00:04:17.650
<v Speaker 2>[inaudible] and,</v>
<v Speaker 2>uh,</v>

70
00:04:17.680 --> 00:04:19.780
<v Speaker 2>transitions across,</v>
<v Speaker 2>switch those paths.</v>

71
00:04:20.110 --> 00:04:25.110
<v Speaker 2>A canon can stake out sort of a giVen </v>
<v Speaker 2>set of options for the human.</v>

72
00:04:25.181 --> 00:04:30.181
<v Speaker 2>It turns out humans tend to.</v>
<v Speaker 2>This tends to be a more intuitive way of</v>

73
00:04:30.310 --> 00:04:35.310
<v Speaker 2>imposing certain constraints on human </v>
<v Speaker 2>operation rather than enforcing that the</v>

74
00:04:36.160 --> 00:04:41.160
<v Speaker 2>ego vehicle stick to some arbitrary </v>
<v Speaker 2>position within some distance of a safe </v>

75
00:04:41.831 --> 00:04:42.190
<v Speaker 2>path.</v>

76
00:04:42.590 --> 00:04:46.210
<v Speaker 2>A,</v>
<v Speaker 2>you instead look to enforce only that,</v>

77
00:04:46.211 --> 00:04:51.211
<v Speaker 2>the,</v>
<v Speaker 2>that the state of the vehicle remain </v>

78
00:04:51.211 --> 00:04:52.090
<v Speaker 2>within a constraint bounded end </v>
<v Speaker 2>dimensional tube in state space.</v>

79
00:04:52.570 --> 00:04:57.570
<v Speaker 2>Those constraints being spatial.</v>
<v Speaker 2>Imagine for a moment edges of the </v>

80
00:04:57.570 --> 00:04:59.950
<v Speaker 2>roadway or circumventing various objects</v>
<v Speaker 2>in the roadway.</v>

81
00:05:00.360 --> 00:05:03.580
<v Speaker 2>I'm a imagine them also being dynamic,</v>
<v Speaker 2>right?</v>

82
00:05:03.581 --> 00:05:08.581
<v Speaker 2>So limits of tire,</v>
<v Speaker 2>a tire friction imposed limits on side </v>

83
00:05:09.201 --> 00:05:11.650
<v Speaker 2>slip angles.</v>
<v Speaker 2>And so using that,</v>

84
00:05:11.920 --> 00:05:15.400
<v Speaker 2>what we did is found a way to create </v>
<v Speaker 2>those hammad topis forward simulate,</v>

85
00:05:15.480 --> 00:05:17.380
<v Speaker 2>uh,</v>
<v Speaker 2>the trajectory of the vehicle,</v>

86
00:05:17.660 --> 00:05:21.110
<v Speaker 2>a given its current state and some </v>
<v Speaker 2>optimal set of controls.</v>

87
00:05:21.130 --> 00:05:26.130
<v Speaker 2>Seek inputs that would optimize its </v>
<v Speaker 2>stability through that we use model </v>

88
00:05:26.130 --> 00:05:26.800
<v Speaker 2>predictive control,</v>
<v Speaker 2>uh,</v>

89
00:05:26.830 --> 00:05:30.340
<v Speaker 2>in that work and then taking that </v>
<v Speaker 2>forward,</v>

90
00:05:30.341 --> 00:05:34.810
<v Speaker 2>simulated trajectory computing some </v>
<v Speaker 2>metric of threat.</v>

91
00:05:34.840 --> 00:05:37.930
<v Speaker 2>For instance,</v>
<v Speaker 2>if the objective function for that,</v>

92
00:05:38.290 --> 00:05:43.290
<v Speaker 2>uh,</v>
<v Speaker 2>minimize the or maximize stability or </v>

93
00:05:43.290 --> 00:05:43.840
<v Speaker 2>minimize some,</v>
<v Speaker 2>some of these parameters like we'll side</v>

94
00:05:43.841 --> 00:05:48.841
<v Speaker 2>slip,</v>
<v Speaker 2>then we'll side slip is a fairly good </v>

95
00:05:48.841 --> 00:05:52.090
<v Speaker 2>indication of how threatening that </v>
<v Speaker 2>optimal maneuver is becoming.</v>

96
00:05:52.780 --> 00:05:57.130
<v Speaker 2>And so what we did is then use that in a</v>
<v Speaker 2>modulation of control between the human,</v>

97
00:05:57.290 --> 00:06:02.290
<v Speaker 2>the car,</v>
<v Speaker 2>such that should the car ever find </v>

98
00:06:02.290 --> 00:06:05.421
<v Speaker 2>yourself in a state where that forward </v>
<v Speaker 2>simulated optimal trajectory is very </v>

99
00:06:05.421 --> 00:06:07.310
<v Speaker 2>near the limits of what the vehicle and </v>
<v Speaker 2>it can actually handle.</v>

100
00:06:07.940 --> 00:06:10.700
<v Speaker 2>We will have transitioned control fully </v>
<v Speaker 2>to the,</v>

101
00:06:10.760 --> 00:06:15.760
<v Speaker 2>to the vehicle,</v>
<v Speaker 2>to the automated system so that it can </v>

102
00:06:15.760 --> 00:06:18.041
<v Speaker 2>avoid an accident.</v>
<v Speaker 2>And then it transitions back in some </v>

103
00:06:18.041 --> 00:06:18.041
<v Speaker 2>manner and we played with a number of </v>
<v Speaker 2>different,</v>

104
00:06:18.260 --> 00:06:22.100
<v Speaker 2>uh,</v>
<v Speaker 2>methods of transitioning this control to</v>

105
00:06:22.101 --> 00:06:23.220
<v Speaker 2>ensure that,</v>
<v Speaker 2>um,</v>

106
00:06:23.790 --> 00:06:28.790
<v Speaker 2>uh,</v>
<v Speaker 2>that we didn't throw off the human </v>

107
00:06:28.790 --> 00:06:28.880
<v Speaker 2>mental model which was,</v>
<v Speaker 2>which was one of the key concerns.</v>

108
00:06:29.270 --> 00:06:34.270
<v Speaker 2>We also wanted to make sure that we were</v>
<v Speaker 2>able to arrest a accidents before they </v>

109
00:06:34.270 --> 00:06:39.071
<v Speaker 2>happen.</v>
<v Speaker 2>What you see here is a simulation that </v>

110
00:06:39.071 --> 00:06:42.671
<v Speaker 2>was fairly faithful to the behavior we </v>
<v Speaker 2>saw in test drivers up at dearborn in </v>

111
00:06:43.911 --> 00:06:48.911
<v Speaker 2>dearborn,</v>
<v Speaker 2>Michigan ford provided was provided us </v>

112
00:06:48.911 --> 00:06:50.900
<v Speaker 2>with a jaguar site to test this on and </v>
<v Speaker 2>what we did.</v>

113
00:06:50.900 --> 00:06:53.240
<v Speaker 2>So what you see here is there's a blue </v>
<v Speaker 2>vehicle and a gray vehicle,</v>

114
00:06:53.270 --> 00:06:57.800
<v Speaker 2>both in both cases.</v>
<v Speaker 2>We have a poorly tuned driver model.</v>

115
00:06:57.801 --> 00:07:01.640
<v Speaker 2>In this case it pure pursuit controller </v>
<v Speaker 2>with a fairly short look ahead,</v>

116
00:07:01.690 --> 00:07:06.500
<v Speaker 2>a shorter than would be appropriate </v>
<v Speaker 2>given this scenario in these dynamics.</v>

117
00:07:07.160 --> 00:07:12.160
<v Speaker 2>Uh,</v>
<v Speaker 2>the gray vehicle is without the </v>

118
00:07:12.160 --> 00:07:12.160
<v Speaker 2>intelligent copilot in the loop.</v>
<v Speaker 2>Um,</v>

119
00:07:12.230 --> 00:07:15.470
<v Speaker 2>you'll notice that obviously the driver </v>
<v Speaker 2>becomes unstable,</v>

120
00:07:15.471 --> 00:07:17.510
<v Speaker 2>loses control,</v>
<v Speaker 2>and leaves the safe roadway.</v>

121
00:07:18.380 --> 00:07:19.610
<v Speaker 2>The copilot,</v>
<v Speaker 2>remember,</v>

122
00:07:19.820 --> 00:07:23.390
<v Speaker 2>is in,</v>
<v Speaker 2>is interested not in following any given</v>

123
00:07:23.391 --> 00:07:28.391
<v Speaker 2>path.</v>
<v Speaker 2>It doesn't care where the vehicle lands </v>

124
00:07:28.391 --> 00:07:30.080
<v Speaker 2>on this roadway provided it remains on </v>
<v Speaker 2>inside the road.</v>

125
00:07:30.510 --> 00:07:32.750
<v Speaker 2>Uh,</v>
<v Speaker 2>in the blue vehicles case.</v>

126
00:07:32.820 --> 00:07:35.000
<v Speaker 2>Uh,</v>
<v Speaker 2>it's the exact same human driver model.</v>

127
00:07:35.630 --> 00:07:39.250
<v Speaker 2>Now at the copilot and the loop,</v>
<v Speaker 2>you'll notice that as a,</v>

128
00:07:39.260 --> 00:07:44.260
<v Speaker 2>as this scenario a continues.</v>
<v Speaker 2>What you see here on the left is the </v>

129
00:07:44.260 --> 00:07:48.821
<v Speaker 2>green is in this green bar,</v>
<v Speaker 2>is the portion of available control </v>

130
00:07:48.821 --> 00:07:49.970
<v Speaker 2>authority is being taken by the </v>
<v Speaker 2>automated system.</v>

131
00:07:49.970 --> 00:07:52.180
<v Speaker 2>You don't understand,</v>
<v Speaker 2>I've never exceeds half of the available</v>

132
00:07:52.190 --> 00:07:54.800
<v Speaker 2>control,</v>
<v Speaker 2>which is to say that the steering inputs</v>

133
00:07:54.801 --> 00:07:56.560
<v Speaker 2>received by the vehicle,</v>
<v Speaker 2>uh,</v>

134
00:07:56.570 --> 00:08:01.570
<v Speaker 2>and end up being a blend of what the </v>
<v Speaker 2>human and what the automation are </v>

135
00:08:01.570 --> 00:08:02.420
<v Speaker 2>providing,</v>
<v Speaker 2>um,</v>

136
00:08:03.260 --> 00:08:08.260
<v Speaker 2>and what,</v>
<v Speaker 2>what results is a path for the blue </v>

137
00:08:08.260 --> 00:08:11.801
<v Speaker 2>vehicle that actually better tracks the </v>
<v Speaker 2>humans intended trajectory.</v>

138
00:08:12.290 --> 00:08:15.170
<v Speaker 2>Then even the copilot understood,</v>
<v Speaker 2>right?</v>

139
00:08:15.230 --> 00:08:20.230
<v Speaker 2>Again,</v>
<v Speaker 2>the copilot is keeping the vehicle </v>

140
00:08:20.230 --> 00:08:21.701
<v Speaker 2>stable as keeping it on the road.</v>
<v Speaker 2>The human is hewing to the center line </v>

141
00:08:21.830 --> 00:08:26.830
<v Speaker 2>of that roadway.</v>
<v Speaker 2>So there was some very interesting </v>

142
00:08:26.830 --> 00:08:26.830
<v Speaker 2>things that came out of this.</v>
<v Speaker 2>There were a lot of,</v>

143
00:08:26.830 --> 00:08:30.660
<v Speaker 2>uh,</v>
<v Speaker 2>we did a lot of work in understanding </v>

144
00:08:30.660 --> 00:08:31.850
<v Speaker 2>what kind of feedback was most natural </v>
<v Speaker 2>to provide a human.</v>

145
00:08:32.030 --> 00:08:37.030
<v Speaker 2>Our biggest concern was if you throw off</v>
<v Speaker 2>a human's mental model by causing the </v>

146
00:08:37.030 --> 00:08:41.651
<v Speaker 2>vehicles behaviors to deviate from what </v>
<v Speaker 2>they expect it to do in response to </v>

147
00:08:41.751 --> 00:08:43.900
<v Speaker 2>various control inputs,</v>
<v Speaker 2>that can be a problem.</v>

148
00:08:43.920 --> 00:08:46.100
<v Speaker 2>So we tried various things from,</v>
<v Speaker 2>you know,</v>

149
00:08:46.101 --> 00:08:47.150
<v Speaker 2>adjusting,</v>
<v Speaker 2>for instance,</v>

150
00:08:47.151 --> 00:08:52.151
<v Speaker 2>one of the,</v>
<v Speaker 2>one of the key questions that we had </v>

151
00:08:52.151 --> 00:08:53.741
<v Speaker 2>early on was a,</v>
<v Speaker 2>if we couple the computer control and </v>

152
00:08:54.591 --> 00:08:59.591
<v Speaker 2>the human control via planetary gear and</v>
<v Speaker 2>allow the human to feel a actually a </v>

153
00:09:01.321 --> 00:09:03.180
<v Speaker 2>backwards torque to what the vehicle is </v>
<v Speaker 2>doing.</v>

154
00:09:03.181 --> 00:09:07.230
<v Speaker 2>So the car starts to turn right,</v>
<v Speaker 2>human will feel the wheel turn left,</v>

155
00:09:07.231 --> 00:09:10.950
<v Speaker 2>they'll see it started to turn left.</v>
<v Speaker 2>Is that more confusing or less confusing</v>

156
00:09:10.951 --> 00:09:15.951
<v Speaker 2>to human.</v>
<v Speaker 2>And it turns out it depends on how </v>

157
00:09:15.951 --> 00:09:15.951
<v Speaker 2>experienced the human is.</v>
<v Speaker 2>Some,</v>

158
00:09:15.951 --> 00:09:19.820
<v Speaker 2>some drivers will modulate their input </v>
<v Speaker 2>space on tHe torque feedback that they </v>

159
00:09:19.820 --> 00:09:20.670
<v Speaker 2>feel through the wheel.</v>
<v Speaker 2>For instance,</v>

160
00:09:20.671 --> 00:09:25.671
<v Speaker 2>a very experienced driver expects to </v>
<v Speaker 2>fuel the we'll pull left when they're </v>

161
00:09:25.671 --> 00:09:27.120
<v Speaker 2>turning right.</v>
<v Speaker 2>However,</v>

162
00:09:27.150 --> 00:09:32.150
<v Speaker 2>less experienced drivers in response to </v>
<v Speaker 2>seeing the wheel turning opposite to </v>

163
00:09:32.150 --> 00:09:36.441
<v Speaker 2>what the,</v>
<v Speaker 2>what the car is supposed to be doing </v>

164
00:09:36.441 --> 00:09:36.441
<v Speaker 2>this for a rather confusing experience.</v>

165
00:09:36.441 --> 00:09:39.540
<v Speaker 2>So there were a lot of really </v>
<v Speaker 2>interesting a human interface challenges</v>

166
00:09:39.960 --> 00:09:43.030
<v Speaker 2>that we were dealing with here.</v>
<v Speaker 2>Um,</v>

167
00:09:43.890 --> 00:09:45.930
<v Speaker 2>we ended up working through a lot of </v>
<v Speaker 2>that,</v>

168
00:09:46.290 --> 00:09:48.360
<v Speaker 2>developing a number of,</v>
<v Speaker 2>uh,</v>

169
00:09:50.220 --> 00:09:52.530
<v Speaker 2>uh,</v>
<v Speaker 2>sort of micro applications for it.</v>

170
00:09:53.040 --> 00:09:53.910
<v Speaker 2>One of those,</v>
<v Speaker 2>uh,</v>

171
00:09:53.911 --> 00:09:58.911
<v Speaker 2>at the time gill pratt was leading a </v>
<v Speaker 2>darpa program focused on what they call </v>

172
00:09:58.911 --> 00:10:00.510
<v Speaker 2>the time maximum ability and </v>
<v Speaker 2>manipulation.</v>

173
00:10:01.440 --> 00:10:06.440
<v Speaker 2>Uh,</v>
<v Speaker 2>we decided to see what the system could </v>

174
00:10:06.440 --> 00:10:08.820
<v Speaker 2>do in application to unmanned ground </v>
<v Speaker 2>vehicles.</v>

175
00:10:08.850 --> 00:10:11.110
<v Speaker 2>So in this case,</v>
<v Speaker 2>what you see is a,</v>

176
00:10:11.111 --> 00:10:14.370
<v Speaker 2>a human driver sitting at a remote </v>
<v Speaker 2>console as,</v>

177
00:10:14.371 --> 00:10:18.360
<v Speaker 2>as one would want operating and unmanned</v>
<v Speaker 2>vehicle,</v>

178
00:10:18.361 --> 00:10:19.590
<v Speaker 2>for instance,</v>
<v Speaker 2>in the military.</v>

179
00:10:20.090 --> 00:10:24.510
<v Speaker 2>Um,</v>
<v Speaker 2>what you see on the left top left is the</v>

180
00:10:24.780 --> 00:10:29.780
<v Speaker 2>top down view of what the vehicle sees.</v>
<v Speaker 2>I should've played this in repeat mode </v>

181
00:10:30.810 --> 00:10:32.880
<v Speaker 2>with bounding boxes,</v>
<v Speaker 2>bounding various cones.</v>

182
00:10:32.881 --> 00:10:35.070
<v Speaker 2>And what we did is he set up about 20 </v>
<v Speaker 2>drivers,</v>

183
00:10:35.071 --> 00:10:37.440
<v Speaker 2>20,</v>
<v Speaker 2>20 a test subjects,</v>

184
00:10:37.880 --> 00:10:39.360
<v Speaker 2>uh,</v>
<v Speaker 2>looking at this,</v>

185
00:10:39.450 --> 00:10:40.340
<v Speaker 2>this,</v>
<v Speaker 2>uh,</v>

186
00:10:40.380 --> 00:10:44.040
<v Speaker 2>control screen and operating the vehicle</v>
<v Speaker 2>through this track.</v>

187
00:10:44.070 --> 00:10:49.070
<v Speaker 2>And we set this up as a race with a </v>
<v Speaker 2>prizes for the winners as,</v>

188
00:10:49.251 --> 00:10:54.251
<v Speaker 2>as one would expect.</v>
<v Speaker 2>And I'm penalize them for every barely </v>

189
00:10:54.251 --> 00:10:56.130
<v Speaker 2>hits.</v>
<v Speaker 2>If they knocked over the barrel,</v>

190
00:10:56.131 --> 00:11:01.131
<v Speaker 2>I think they got a five second penalty.</v>
<v Speaker 2>If they brushed a barely got one second </v>

191
00:11:01.131 --> 00:11:04.371
<v Speaker 2>penalty and they were to cross,</v>
<v Speaker 2>they worked across the field as fast as </v>

192
00:11:04.371 --> 00:11:05.010
<v Speaker 2>possible and they couldn't.</v>
<v Speaker 2>They had no line of sight connection the</v>

193
00:11:05.011 --> 00:11:06.320
<v Speaker 2>vehicle.</v>
<v Speaker 2>And we played with some things on,</v>

194
00:11:06.370 --> 00:11:07.470
<v Speaker 2>on their interface.</v>
<v Speaker 2>We did,</v>

195
00:11:07.930 --> 00:11:08.390
<v Speaker 2>uh,</v>
<v Speaker 2>you know,</v>

196
00:11:08.410 --> 00:11:13.410
<v Speaker 2>we,</v>
<v Speaker 2>we caused it to drop out occasionally </v>

197
00:11:13.410 --> 00:11:13.410
<v Speaker 2>we,</v>
<v Speaker 2>we delayed it as,</v>

198
00:11:13.410 --> 00:11:14.220
<v Speaker 2>as one realistically expect in the </v>
<v Speaker 2>field.</v>

199
00:11:15.030 --> 00:11:20.030
<v Speaker 2>And then we either engaged or didn't </v>
<v Speaker 2>engage the copilot to try to understand </v>

200
00:11:20.030 --> 00:11:22.500
<v Speaker 2>what effect that had on their </v>
<v Speaker 2>performance and their experience.</v>

201
00:11:23.070 --> 00:11:27.210
<v Speaker 2>And what we found was not surprisingly,</v>
<v Speaker 2>the incidence of collisions declined,</v>

202
00:11:27.290 --> 00:11:32.290
<v Speaker 2>declined by about 72 percent when the </v>
<v Speaker 2>copilot was engaged versus when it was </v>

203
00:11:32.290 --> 00:11:32.290
<v Speaker 2>not a.</v>

204
00:11:32.290 --> 00:11:34.320
<v Speaker 2>We also found that,</v>
<v Speaker 2>you know,</v>

205
00:11:34.950 --> 00:11:37.380
<v Speaker 2>even with that 72 percent decline in </v>
<v Speaker 2>collisions,</v>

206
00:11:37.590 --> 00:11:39.690
<v Speaker 2>uh,</v>
<v Speaker 2>the speed increased by,</v>

207
00:11:39.750 --> 00:11:43.280
<v Speaker 2>I'm blanking on the amount,</v>
<v Speaker 2>but it was 20 or 30 percent ish.</v>

208
00:11:44.700 --> 00:11:47.610
<v Speaker 2>Finally,</v>
<v Speaker 2>and perhaps most interesting to me after</v>

209
00:11:47.611 --> 00:11:49.050
<v Speaker 2>every run.</v>
<v Speaker 2>Uh,</v>

210
00:11:49.051 --> 00:11:51.120
<v Speaker 2>I would ask the driver in the,</v>
<v Speaker 2>again,</v>

211
00:11:51.121 --> 00:11:56.121
<v Speaker 2>these were blind tests.</v>
<v Speaker 2>They didn't know if the copilot was </v>

212
00:11:56.121 --> 00:11:57.960
<v Speaker 2>accurate or not,</v>
<v Speaker 2>and I would ask them how much control </v>

213
00:11:57.960 --> 00:11:57.960
<v Speaker 2>did you feel like you've had over the </v>
<v Speaker 2>vehicle?</v>

214
00:11:59.080 --> 00:12:04.080
<v Speaker 2>And I found that there was a </v>
<v Speaker 2>statistically significant increase of </v>

215
00:12:04.080 --> 00:12:04.370
<v Speaker 2>about 12 percent when the copilot was </v>
<v Speaker 2>engaged in,</v>

216
00:12:04.490 --> 00:12:08.800
<v Speaker 2>in that is to say drivers reported </v>
<v Speaker 2>feeling more control the vehicle,</v>

217
00:12:09.160 --> 00:12:13.150
<v Speaker 2>12 percent more of the time when the </v>
<v Speaker 2>copilot was engaged and when it wasn't.</v>

218
00:12:13.780 --> 00:12:15.340
<v Speaker 2>And then I,</v>
<v Speaker 2>the statistics,</v>

219
00:12:15.341 --> 00:12:18.280
<v Speaker 2>it turns out they actually at the </v>
<v Speaker 2>average level of control that the,</v>

220
00:12:18.281 --> 00:12:20.560
<v Speaker 2>that the copilot was taking was 43 </v>
<v Speaker 2>percent.</v>

221
00:12:20.560 --> 00:12:25.560
<v Speaker 2>So they were reporting that they felt </v>
<v Speaker 2>more in control when in fact there were </v>

222
00:12:25.560 --> 00:12:26.970
<v Speaker 2>43 percent less in control.</v>
<v Speaker 2>um,</v>

223
00:12:27.190 --> 00:12:28.810
<v Speaker 2>which was,</v>
<v Speaker 2>which was interesting.</v>

224
00:12:28.811 --> 00:12:29.500
<v Speaker 2>And,</v>
<v Speaker 2>uh,</v>

225
00:12:29.501 --> 00:12:34.501
<v Speaker 2>I think a bears a little bit on sort of </v>
<v Speaker 2>the human psyche in terms of,</v>

226
00:12:35.050 --> 00:12:35.500
<v Speaker 2>uh,</v>
<v Speaker 2>you know,</v>

227
00:12:35.501 --> 00:12:37.840
<v Speaker 2>they were reporting the vehicle was </v>
<v Speaker 2>doing what I wanted to do,</v>

228
00:12:37.841 --> 00:12:39.570
<v Speaker 2>maybe not what I told it to do,</v>
<v Speaker 2>uh,</v>

229
00:12:39.750 --> 00:12:41.350
<v Speaker 2>which was,</v>
<v Speaker 2>which was kind of fun.</v>

230
00:12:41.640 --> 00:12:43.450
<v Speaker 2>Uh,</v>
<v Speaker 2>observation and then fund a,</v>

231
00:12:43.850 --> 00:12:44.340
<v Speaker 2>uh,</v>
<v Speaker 2>I think,</v>

232
00:12:44.390 --> 00:12:46.390
<v Speaker 2>I think the most enjoyable part of this </v>
<v Speaker 2>was,</v>

233
00:12:46.391 --> 00:12:48.280
<v Speaker 2>uh,</v>
<v Speaker 2>getting together with the,</v>

234
00:12:48.310 --> 00:12:50.130
<v Speaker 2>with the whole group at the end of the </v>
<v Speaker 2>study and,</v>

235
00:12:50.131 --> 00:12:52.840
<v Speaker 2>and presenting some of this and seeing </v>
<v Speaker 2>some of the reactions.</v>

236
00:12:54.700 --> 00:12:56.710
<v Speaker 2>So from there,</v>
<v Speaker 2>um,</v>

237
00:12:56.800 --> 00:12:57.160
<v Speaker 2>you know,</v>
<v Speaker 2>we,</v>

238
00:12:57.190 --> 00:12:59.020
<v Speaker 2>we,</v>
<v Speaker 2>we looked at a few other areas,</v>

239
00:12:59.080 --> 00:13:00.610
<v Speaker 2>uh,</v>
<v Speaker 2>my,</v>

240
00:13:00.670 --> 00:13:03.910
<v Speaker 2>uh,</v>
<v Speaker 2>carl unm and I looked at a few different</v>

241
00:13:03.940 --> 00:13:05.940
<v Speaker 2>opportunities to commercialize this.</v>
<v Speaker 2>Again,</v>

242
00:13:05.950 --> 00:13:10.950
<v Speaker 2>this was years ago and the industry was </v>
<v Speaker 2>in a very different place than it is </v>

243
00:13:10.950 --> 00:13:10.950
<v Speaker 2>today.</v>
<v Speaker 2>Uh,</v>

244
00:13:10.950 --> 00:13:13.540
<v Speaker 2>we started a company called gimlet than </v>
<v Speaker 2>another called ride.</v>

245
00:13:13.930 --> 00:13:18.930
<v Speaker 2>Um,</v>
<v Speaker 2>this is the logo may look familiar to </v>

246
00:13:18.930 --> 00:13:19.660
<v Speaker 2>you.</v>
<v Speaker 2>We turn that into a,</v>

247
00:13:19.680 --> 00:13:23.380
<v Speaker 2>we had at the time it intended to roll </v>
<v Speaker 2>this out across,</v>

248
00:13:23.470 --> 00:13:25.690
<v Speaker 2>um,</v>
<v Speaker 2>um,</v>

249
00:13:26.290 --> 00:13:28.870
<v Speaker 2>you know,</v>
<v Speaker 2>various automakers in their operations,</v>

250
00:13:28.970 --> 00:13:29.950
<v Speaker 2>uh,</v>
<v Speaker 2>at the time,</v>

251
00:13:30.220 --> 00:13:34.820
<v Speaker 2>very few saw self driving as a </v>
<v Speaker 2>technology.</v>

252
00:13:34.821 --> 00:13:37.330
<v Speaker 2>It was really going to impact their </v>
<v Speaker 2>business going forward.</v>

253
00:13:38.270 --> 00:13:43.270
<v Speaker 2>They were,</v>
<v Speaker 2>in fact even even ride sharing at the </v>

254
00:13:43.270 --> 00:13:44.570
<v Speaker 2>time was a fairly new concept.</v>
<v Speaker 2>Um,</v>

255
00:13:44.830 --> 00:13:49.830
<v Speaker 2>that was,</v>
<v Speaker 2>I think to a large degree viewed as </v>

256
00:13:49.830 --> 00:13:49.830
<v Speaker 2>unproven.</v>
<v Speaker 2>Um,</v>

257
00:13:49.830 --> 00:13:52.250
<v Speaker 2>so as I mentioned december of last year,</v>

258
00:13:56.200 --> 00:14:01.200
<v Speaker 2>I cofounded aurora with a couple of </v>
<v Speaker 2>folks who have been making significant </v>

259
00:14:02.171 --> 00:14:04.510
<v Speaker 2>progress in this space for many years.</v>
<v Speaker 2>Chris urmson,</v>

260
00:14:04.990 --> 00:14:08.890
<v Speaker 2>who formerly led google's self driving </v>
<v Speaker 2>car group at drew back now,</v>

261
00:14:08.891 --> 00:14:10.720
<v Speaker 2>is a professor at carnegie mellon </v>
<v Speaker 2>university,</v>

262
00:14:11.030 --> 00:14:12.580
<v Speaker 2>uh,</v>
<v Speaker 2>exceptional machine learning,</v>

263
00:14:12.680 --> 00:14:14.200
<v Speaker 2>uh,</v>
<v Speaker 2>and an applied machine learning,</v>

264
00:14:14.520 --> 00:14:19.520
<v Speaker 2>uh,</v>
<v Speaker 2>was one of the founding members of uber </v>

265
00:14:19.520 --> 00:14:19.520
<v Speaker 2>self driving car team and lead autonomy </v>
<v Speaker 2>and perception there.</v>

266
00:14:19.530 --> 00:14:22.120
<v Speaker 2>Um,</v>
<v Speaker 2>we felt like we had a unique opportunity</v>

267
00:14:22.121 --> 00:14:24.670
<v Speaker 2>at the convergence of a few things.</v>
<v Speaker 2>One,</v>

268
00:14:25.060 --> 00:14:30.060
<v Speaker 2>the automotive world has really come </v>
<v Speaker 2>into the full on realization that self </v>

269
00:14:31.271 --> 00:14:36.271
<v Speaker 2>driving and particularly self driving </v>
<v Speaker 2>and ride sharing and vehicle </v>

270
00:14:36.271 --> 00:14:39.331
<v Speaker 2>electrification,</v>
<v Speaker 2>our three vectors that will change the </v>

271
00:14:39.331 --> 00:14:39.331
<v Speaker 2>industry.</v>
<v Speaker 2>Um,</v>

272
00:14:39.331 --> 00:14:43.710
<v Speaker 2>that was something that didn't exist 10 </v>
<v Speaker 2>years ago to a significant advances have</v>

273
00:14:44.261 --> 00:14:46.470
<v Speaker 2>been made in,</v>
<v Speaker 2>um,</v>

274
00:14:46.510 --> 00:14:51.510
<v Speaker 2>you know,</v>
<v Speaker 2>some of these machine learning </v>

275
00:14:51.510 --> 00:14:51.510
<v Speaker 2>techniques in particular deep learning,</v>
<v Speaker 2>um,</v>

276
00:14:51.510 --> 00:14:52.020
<v Speaker 2>and other neural network</v>

277
00:14:52.040 --> 00:14:55.940
<v Speaker 3>network approaches in the computers that</v>
<v Speaker 3>run them.</v>

278
00:14:56.530 --> 00:15:01.530
<v Speaker 3>And the availability of low power gpu </v>
<v Speaker 3>and tpu options to really do that well </v>

279
00:15:04.400 --> 00:15:07.640
<v Speaker 3>in sensing technologies in high </v>
<v Speaker 3>resolution radar.</v>

280
00:15:07.730 --> 00:15:09.140
<v Speaker 3>And a lot of the light,</v>
<v Speaker 3>our development,</v>

281
00:15:09.620 --> 00:15:12.140
<v Speaker 3>so it's really a unique time in the self</v>
<v Speaker 3>driving world.</v>

282
00:15:12.141 --> 00:15:14.780
<v Speaker 3>A lot of these things are really coming </v>
<v Speaker 3>together now.</v>

283
00:15:15.140 --> 00:15:20.140
<v Speaker 3>Uh,</v>
<v Speaker 3>and we felt like by bringing together </v>

284
00:15:20.140 --> 00:15:22.451
<v Speaker 3>and experienced team,</v>
<v Speaker 3>we had an interesting opportunity to </v>

285
00:15:22.451 --> 00:15:23.240
<v Speaker 3>build from a clean sheet,</v>
<v Speaker 3>a new,</v>

286
00:15:23.420 --> 00:15:24.740
<v Speaker 3>uh,</v>
<v Speaker 3>platform,</v>

287
00:15:25.010 --> 00:15:30.010
<v Speaker 3>a new self driving a architecture that </v>
<v Speaker 3>leverages the latest events as applied </v>

288
00:15:30.040 --> 00:15:33.250
<v Speaker 3>machine learning together with our,</v>
<v Speaker 3>um,</v>

289
00:15:33.970 --> 00:15:35.790
<v Speaker 3>uh,</v>
<v Speaker 3>together with our experience of,</v>

290
00:15:35.840 --> 00:15:40.840
<v Speaker 3>of where some of the pitfalls tend to be</v>
<v Speaker 3>down the road as you develop these </v>

291
00:15:40.840 --> 00:15:41.180
<v Speaker 3>systems because you don't tend to see </v>
<v Speaker 3>them early on.</v>

292
00:15:41.181 --> 00:15:46.181
<v Speaker 3>they tend to express themselves as you </v>
<v Speaker 3>get into the long tail of corner cases </v>

293
00:15:46.181 --> 00:15:46.700
<v Speaker 3>that she ended up needing to resolve.</v>

294
00:15:47.780 --> 00:15:49.490
<v Speaker 3>So we've built that team.</v>
<v Speaker 3>Uh,</v>

295
00:15:49.520 --> 00:15:52.550
<v Speaker 3>we have offices in palo alto,</v>
<v Speaker 3>California and pittsburgh,</v>

296
00:15:52.551 --> 00:15:54.140
<v Speaker 3>Pennsylvania.</v>
<v Speaker 3>Uh,</v>

297
00:15:54.141 --> 00:15:57.170
<v Speaker 3>we've got fleets of vehicles operating </v>
<v Speaker 3>in both politics in Pennsylvania.</v>

298
00:15:57.770 --> 00:15:59.330
<v Speaker 3>A couple of weeks ago,</v>
<v Speaker 3>uh,</v>

299
00:15:59.390 --> 00:16:01.520
<v Speaker 3>we announced that a volkswagen group,</v>
<v Speaker 3>uh,</v>

300
00:16:01.580 --> 00:16:02.990
<v Speaker 3>one of the largest auto makers in the </v>
<v Speaker 3>world,</v>

301
00:16:03.050 --> 00:16:08.050
<v Speaker 3>hyundai motor company,</v>
<v Speaker 3>also on the largest automakers in the </v>

302
00:16:08.050 --> 00:16:08.330
<v Speaker 3>world,</v>
<v Speaker 3>have both partnered with aurora.</v>

303
00:16:08.380 --> 00:16:10.640
<v Speaker 3>Um,</v>
<v Speaker 3>we will be developing.</v>

304
00:16:10.670 --> 00:16:14.640
<v Speaker 3>And are developing with them a set of </v>
<v Speaker 3>platforms and ultimately we'll,</v>

305
00:16:14.650 --> 00:16:16.030
<v Speaker 3>we'll scale that,</v>
<v Speaker 3>uh,</v>

306
00:16:16.031 --> 00:16:19.040
<v Speaker 3>our technology.</v>
<v Speaker 3>I'm on their vehicles across the world.</v>

307
00:16:19.041 --> 00:16:20.450
<v Speaker 3>and,</v>
<v Speaker 3>and one of the important,</v>

308
00:16:21.480 --> 00:16:25.550
<v Speaker 3>the important elements of building.</v>
<v Speaker 3>I asked lex before coming out here,</v>

309
00:16:25.551 --> 00:16:28.460
<v Speaker 3>what's this group would be most </v>
<v Speaker 3>interested in hearing?</v>

310
00:16:28.461 --> 00:16:33.461
<v Speaker 3>One of the things that he mentioned was </v>
<v Speaker 3>what does it take to build a self </v>

311
00:16:33.461 --> 00:16:33.461
<v Speaker 3>driving,</v>
<v Speaker 3>you know,</v>

312
00:16:33.461 --> 00:16:33.461
<v Speaker 3>build a new company in a space like </v>
<v Speaker 3>this.</v>

313
00:16:34.100 --> 00:16:39.100
<v Speaker 3>One of the things that we found very </v>
<v Speaker 3>important was a business model that was </v>

314
00:16:39.100 --> 00:16:41.720
<v Speaker 3>nonthreatening to others.</v>
<v Speaker 3>Uh,</v>

315
00:16:41.780 --> 00:16:46.780
<v Speaker 3>we recognize that our strengths and our </v>
<v Speaker 3>experience over the last say in my case </v>

316
00:16:46.780 --> 00:16:51.431
<v Speaker 3>at decade in chris's case,</v>
<v Speaker 3>almost hear a really lies in the </v>

317
00:16:51.471 --> 00:16:54.170
<v Speaker 3>development of the self driving systems.</v>
<v Speaker 3>Uh,</v>

318
00:16:54.200 --> 00:16:57.170
<v Speaker 3>not in building vehicles that I have had</v>
<v Speaker 3>some experience there.</v>

319
00:16:57.171 --> 00:16:59.060
<v Speaker 3>But,</v>
<v Speaker 3>but in developing the self driving.</v>

320
00:16:59.061 --> 00:17:04.061
<v Speaker 3>And so our feeling was if our mission is</v>
<v Speaker 3>to get a signal to market as quickly as </v>

321
00:17:04.131 --> 00:17:09.131
<v Speaker 3>broadly and as safely as possible.</v>
<v Speaker 3>And that mission is best served by </v>

322
00:17:09.131 --> 00:17:12.050
<v Speaker 3>playing our position and working well </v>
<v Speaker 3>with others who can play.</v>

323
00:17:12.070 --> 00:17:17.070
<v Speaker 3>There's.</v>
<v Speaker 3>Which is why you see the model that </v>

324
00:17:17.070 --> 00:17:20.171
<v Speaker 3>we've adopted and is now a,</v>
<v Speaker 3>you'll start to see some of the fruits </v>

325
00:17:20.171 --> 00:17:21.100
<v Speaker 3>of that through these partnerships with </v>
<v Speaker 3>some of these automakers.</v>

326
00:17:21.580 --> 00:17:26.580
<v Speaker 3>So at the end of the day are our </v>
<v Speaker 3>aspiration and our hope is that this </v>

327
00:17:26.580 --> 00:17:28.170
<v Speaker 3>technology that,</v>
<v Speaker 3>that is so important,</v>

328
00:17:28.210 --> 00:17:33.210
<v Speaker 3>the world in increasing safety and </v>
<v Speaker 3>improving access to transportation and </v>

329
00:17:33.210 --> 00:17:35.870
<v Speaker 3>improving efficiency in the utilization </v>
<v Speaker 3>of our roadways and our cities.</v>

330
00:17:36.260 --> 00:17:36.830
<v Speaker 3>I mean,</v>
<v Speaker 3>I,</v>

331
00:17:36.840 --> 00:17:41.840
<v Speaker 3>I,</v>
<v Speaker 3>this is maybe the first doc I've ever </v>

332
00:17:41.840 --> 00:17:44.441
<v Speaker 3>given where I didn't start by rattling </v>
<v Speaker 3>off statistics about safety and all the </v>

333
00:17:44.441 --> 00:17:45.950
<v Speaker 3>other things.</v>
<v Speaker 3>If you haven't heard them yet,</v>

334
00:17:45.980 --> 00:17:48.050
<v Speaker 3>you should look them up.</v>
<v Speaker 3>They're there stark,</v>

335
00:17:48.200 --> 00:17:53.200
<v Speaker 3>right?</v>
<v Speaker 3>The fact that most vehicles in the </v>

336
00:17:53.200 --> 00:17:56.361
<v Speaker 3>United States today have an average on </v>
<v Speaker 3>average three parking spaces as spaces </v>

337
00:17:56.641 --> 00:18:01.641
<v Speaker 3>allocated to them.</v>
<v Speaker 3>The amount of land that's taken up </v>

338
00:18:01.641 --> 00:18:05.290
<v Speaker 3>across the world in housing vehicles </v>
<v Speaker 3>that are used less than five percent of </v>

339
00:18:05.881 --> 00:18:08.580
<v Speaker 3>the time.</v>
<v Speaker 3>Uh,</v>

340
00:18:08.790 --> 00:18:10.710
<v Speaker 3>the number of people,</v>
<v Speaker 3>I think the United States,</v>

341
00:18:10.720 --> 00:18:15.720
<v Speaker 3>the estimate to spend somewhere between </v>
<v Speaker 3>six and 15 million people don't have </v>

342
00:18:15.720 --> 00:18:19.671
<v Speaker 3>access to the transportation they need </v>
<v Speaker 3>either the because they're elderly or </v>

343
00:18:19.671 --> 00:18:22.911
<v Speaker 3>disabled or one of many other factors,</v>
<v Speaker 3>and so this technology is potentially </v>

344
00:18:24.991 --> 00:18:27.990
<v Speaker 3>one of the most impactful for our </v>
<v Speaker 3>society in the coming years.</v>

345
00:18:28.770 --> 00:18:33.770
<v Speaker 3>It's a tremendously exciting </v>
<v Speaker 3>technological challenge and the </v>

346
00:18:33.770 --> 00:18:38.180
<v Speaker 3>confluence of those two things I think </v>
<v Speaker 3>is a really unique opportunity for </v>

347
00:18:38.180 --> 00:18:41.871
<v Speaker 3>engineers and others who are not </v>
<v Speaker 3>engineers who really want to get </v>

348
00:18:41.871 --> 00:18:44.880
<v Speaker 3>involved to play a role in changing our,</v>
<v Speaker 3>changing our world going forward.</v>

349
00:18:46.200 --> 00:18:47.310
<v Speaker 3>So with that,</v>
<v Speaker 3>maybe I'll.</v>

350
00:18:47.311 --> 00:18:50.130
<v Speaker 3>Maybe I'll stop with this and we can go </v>
<v Speaker 3>to go to questions.</v>

351
00:18:52.380 --> 00:18:52.890
<v Speaker 4>It's good.</v>

352
00:18:58.310 --> 00:18:59.530
<v Speaker 2>Hello.</v>
<v Speaker 2>Thanks for coming on.</v>

353
00:18:59.630 --> 00:19:04.630
<v Speaker 2>The question.</v>
<v Speaker 2>A lot of a self driving car companies </v>

354
00:19:04.630 --> 00:19:07.650
<v Speaker 2>are making extensive use of lidar but </v>
<v Speaker 2>you don't see a lot of that with tesla.</v>

355
00:19:07.710 --> 00:19:08.400
<v Speaker 2>One of the.</v>
<v Speaker 2>No.</v>

356
00:19:08.401 --> 00:19:10.420
<v Speaker 2>If you had any thoughts about that.</v>
<v Speaker 2>The.</v>

357
00:19:10.500 --> 00:19:13.740
<v Speaker 2>I don't want to talk about tesla too </v>
<v Speaker 2>much in terms of our specific.</v>

358
00:19:13.900 --> 00:19:16.050
<v Speaker 2>Any anything that wasn't public </v>
<v Speaker 2>information,</v>

359
00:19:16.051 --> 00:19:19.140
<v Speaker 2>I'm not going to get into it.</v>
<v Speaker 2>I will say that for aurora,</v>

360
00:19:19.890 --> 00:19:24.030
<v Speaker 2>we believe that the right approach is </v>
<v Speaker 2>getting to market quickly and you get to</v>

361
00:19:24.031 --> 00:19:27.870
<v Speaker 2>market and doing so safely and you get </v>
<v Speaker 2>to market quickly and safely.</v>

362
00:19:27.940 --> 00:19:30.630
<v Speaker 3>You leveraged multiple that modalities </v>
<v Speaker 3>including lighter.</v>

363
00:19:31.890 --> 00:19:33.770
<v Speaker 3>These are,</v>
<v Speaker 3>these are all just just to clarify what,</v>

364
00:19:33.790 --> 00:19:37.550
<v Speaker 3>what's running in the background.</v>
<v Speaker 3>These are all just aurora videos of um,</v>

365
00:19:38.400 --> 00:19:40.370
<v Speaker 2>our car cars driving on various test </v>
<v Speaker 2>routes.</v>

366
00:19:40.990 --> 00:19:43.370
<v Speaker 2>Luke from the sloan school,</v>
<v Speaker 2>a lot of,</v>

367
00:19:43.380 --> 00:19:46.210
<v Speaker 2>so a lot of customers have visceral type</v>
<v Speaker 2>connections to their automobile.</v>

368
00:19:46.530 --> 00:19:51.530
<v Speaker 2>Um,</v>
<v Speaker 2>I was wondering how you see that market </v>

369
00:19:51.530 --> 00:19:52.300
<v Speaker 2>to the car enthusiast market being </v>
<v Speaker 2>affected by avs and then vice versa,</v>

370
00:19:52.301 --> 00:19:53.250
<v Speaker 2>how the,</v>
<v Speaker 2>uh,</v>

371
00:19:53.350 --> 00:19:56.230
<v Speaker 2>how the ads will be designed around </v>
<v Speaker 2>those types of customers.</v>

372
00:19:56.950 --> 00:19:59.640
<v Speaker 3>And thanks for asking me.</v>
<v Speaker 3>If I am one of those enthusiasts,</v>

373
00:20:00.540 --> 00:20:05.540
<v Speaker 3>I very much appreciate being able to </v>
<v Speaker 3>drive a car in certain settings.</v>

374
00:20:08.640 --> 00:20:10.710
<v Speaker 3>I very much don't appreciate driving it </v>
<v Speaker 3>and others.</v>

375
00:20:10.800 --> 00:20:11.790
<v Speaker 3>Right?</v>
<v Speaker 3>Uh,</v>

376
00:20:11.850 --> 00:20:16.080
<v Speaker 3>I remember distinctly several evenings </v>
<v Speaker 3>I,</v>

377
00:20:16.250 --> 00:20:21.250
<v Speaker 3>I almost literally pounding my steering </v>
<v Speaker 3>wheel sitting in quogue and in boston </v>

378
00:20:21.250 --> 00:20:22.560
<v Speaker 3>traffic,</v>
<v Speaker 3>you know,</v>

379
00:20:23.320 --> 00:20:25.080
<v Speaker 3>on my way to somewhere,</v>
<v Speaker 3>uh,</v>

380
00:20:25.230 --> 00:20:30.230
<v Speaker 3>I do the same in san francisco.</v>
<v Speaker 3>I think the opportunity really is to </v>

381
00:20:30.230 --> 00:20:34.821
<v Speaker 3>turn that in turn sort of personal </v>
<v Speaker 3>vehicle ownership and driving into more </v>

382
00:20:35.581 --> 00:20:37.470
<v Speaker 3>of a sport and something you do for </v>
<v Speaker 3>leisure.</v>

383
00:20:38.890 --> 00:20:43.890
<v Speaker 3>I see it a gentleman sometime I go,</v>
<v Speaker 3>uh,</v>

384
00:20:44.190 --> 00:20:46.020
<v Speaker 3>asked me to talk.</v>
<v Speaker 3>Hey,</v>

385
00:20:46.230 --> 00:20:50.830
<v Speaker 3>don't you think this is a problem for </v>
<v Speaker 3>the country?</v>

386
00:20:50.890 --> 00:20:55.030
<v Speaker 3>I think you meant the world.</v>
<v Speaker 3>If people don't learn how to drive,</v>

387
00:20:55.031 --> 00:20:56.830
<v Speaker 3>that's just something a human should </v>
<v Speaker 3>know how to do.</v>

388
00:20:57.370 --> 00:21:02.370
<v Speaker 3>A,</v>
<v Speaker 3>my perspective is it's as much of a </v>

389
00:21:02.370 --> 00:21:04.750
<v Speaker 3>problem as people not intrinsically </v>
<v Speaker 3>knowing how to ride a horse today.</v>

390
00:21:05.110 --> 00:21:07.240
<v Speaker 3>If you want to know how to ride a horse,</v>
<v Speaker 3>go ride a horse.</v>

391
00:21:07.810 --> 00:21:09.160
<v Speaker 3>If you want to,</v>
<v Speaker 3>you want to race a car,</v>

392
00:21:09.170 --> 00:21:14.170
<v Speaker 3>go to a race track or go out to a </v>
<v Speaker 3>mountain road that's been allocated for </v>

393
00:21:14.170 --> 00:21:14.330
<v Speaker 3>it.</v>
<v Speaker 3>Um,</v>

394
00:21:14.410 --> 00:21:19.410
<v Speaker 3>ultimately I think,</v>
<v Speaker 3>I think there is an important place for </v>

395
00:21:19.410 --> 00:21:21.871
<v Speaker 3>that because I certainly agree with you.</v>
<v Speaker 3>I'm very much a vehicle enthusiast </v>

396
00:21:21.871 --> 00:21:22.120
<v Speaker 3>myself.</v>
<v Speaker 3>Um,</v>

397
00:21:22.600 --> 00:21:27.600
<v Speaker 3>but I think there is so much opportunity</v>
<v Speaker 3>here in alleviating some of these other </v>

398
00:21:28.331 --> 00:21:33.331
<v Speaker 3>problems,</v>
<v Speaker 3>particularly in places where it's not </v>

399
00:21:33.331 --> 00:21:33.331
<v Speaker 3>fun to drive that.</v>
<v Speaker 3>I think there's a place for both.</v>

400
00:21:33.890 --> 00:21:37.760
<v Speaker 4>Yeah,</v>
<v Speaker 4>right.</v>

401
00:21:38.700 --> 00:21:39.940
<v Speaker 4>You need to get</v>

402
00:21:41.540 --> 00:21:44.540
<v Speaker 5>congratulations on the partnership was </v>
<v Speaker 5>announced recently,</v>

403
00:21:44.541 --> 00:21:45.430
<v Speaker 5>I think.</v>
<v Speaker 5>Um,</v>

404
00:21:45.860 --> 00:21:48.260
<v Speaker 5>so I have a two part question.</v>
<v Speaker 5>The first one is,</v>

405
00:21:48.261 --> 00:21:51.000
<v Speaker 5>um,</v>
<v Speaker 5>so we heard last week from a,</v>

406
00:21:51.280 --> 00:21:56.280
<v Speaker 5>I think there was a gentleman from bimo </v>
<v Speaker 5>talking about how long they've been </v>

407
00:21:56.280 --> 00:21:59.351
<v Speaker 5>working on this autonomous car </v>
<v Speaker 5>technology and you seem to have ramped </v>

408
00:21:59.351 --> 00:22:01.130
<v Speaker 5>up extremely fast.</v>
<v Speaker 5>So,</v>

409
00:22:01.640 --> 00:22:04.400
<v Speaker 5>uh,</v>
<v Speaker 5>is there a licensing model that you have</v>

410
00:22:04.401 --> 00:22:05.090
<v Speaker 5>taken out?</v>
<v Speaker 5>I mean,</v>

411
00:22:05.091 --> 00:22:09.290
<v Speaker 5>how were you able to commercialize the </v>
<v Speaker 5>technology in one year?</v>

412
00:22:11.160 --> 00:22:16.160
<v Speaker 3>Just to be clear,</v>
<v Speaker 3>we're not actually commercializing </v>

413
00:22:16.160 --> 00:22:20.790
<v Speaker 3>distinguish,</v>
<v Speaker 3>we are partnering and developing </v>

414
00:22:20.790 --> 00:22:23.490
<v Speaker 3>vehicles and will ultimately be running </v>
<v Speaker 3>pilots as we announced a week or two ago</v>

415
00:22:24.120 --> 00:22:27.150
<v Speaker 3>with the shuttles we are.</v>
<v Speaker 3>However,</v>

416
00:22:27.151 --> 00:22:32.151
<v Speaker 3>I will distinguish that from ron </v>
<v Speaker 3>commercialization of the technology and </v>

417
00:22:32.151 --> 00:22:35.200
<v Speaker 3>I don't want to get too much into,</v>
<v Speaker 3>you know,</v>

418
00:22:35.220 --> 00:22:39.660
<v Speaker 3>the nuances of that business model.</v>
<v Speaker 3>I will say that it is,</v>

419
00:22:39.730 --> 00:22:43.800
<v Speaker 3>is one that's done in very close </v>
<v Speaker 3>partnership with our automotive partners</v>

420
00:22:45.620 --> 00:22:48.170
<v Speaker 3>because at the end of the day they </v>
<v Speaker 3>understand their cars,</v>

421
00:22:48.171 --> 00:22:51.290
<v Speaker 3>they understand their customers,</v>
<v Speaker 3>they have distribution networks.</v>

422
00:22:51.340 --> 00:22:53.170
<v Speaker 3>Um,</v>
<v Speaker 3>they are,</v>

423
00:22:53.310 --> 00:22:56.000
<v Speaker 3>our automotive partners are fairly well </v>
<v Speaker 3>positioned,</v>

424
00:22:56.560 --> 00:23:01.560
<v Speaker 3>uh,</v>
<v Speaker 3>it provided they have the right support </v>

425
00:23:01.560 --> 00:23:01.560
<v Speaker 3>in developing the self driving </v>
<v Speaker 3>technology,</v>

426
00:23:01.560 --> 00:23:02.210
<v Speaker 3>the fairly,</v>
<v Speaker 3>fairly well positioned to,</v>

427
00:23:02.560 --> 00:23:03.420
<v Speaker 3>uh,</v>
<v Speaker 3>you know,</v>

428
00:23:03.470 --> 00:23:04.580
<v Speaker 3>roll it out at the scale.</v>

429
00:23:05.820 --> 00:23:08.060
<v Speaker 5>So the second month,</v>
<v Speaker 5>my question is again,</v>

430
00:23:08.070 --> 00:23:11.400
<v Speaker 5>looking at the you pace of adoption and </v>
<v Speaker 5>the maturity of technology,</v>

431
00:23:12.450 --> 00:23:17.450
<v Speaker 5>do you see like an opensource model for </v>
<v Speaker 5>autonomous cars as they become more and </v>

432
00:23:17.731 --> 00:23:17.940
<v Speaker 5>more</v>

433
00:23:19.550 --> 00:23:22.170
<v Speaker 3>unclear?</v>
<v Speaker 3>I,</v>

434
00:23:22.410 --> 00:23:27.410
<v Speaker 3>I'm not convinced that an open source </v>
<v Speaker 3>model is what gets to market most </v>

435
00:23:27.410 --> 00:23:28.760
<v Speaker 3>quickly.</v>
<v Speaker 3>Um,</v>

436
00:23:29.480 --> 00:23:31.280
<v Speaker 3>in the long run,</v>
<v Speaker 3>I,</v>

437
00:23:31.281 --> 00:23:32.160
<v Speaker 3>I,</v>
<v Speaker 3>it's,</v>

438
00:23:32.161 --> 00:23:34.250
<v Speaker 3>it's not clear to me,</v>
<v Speaker 3>uh,</v>

439
00:23:34.280 --> 00:23:39.280
<v Speaker 3>what will happen.</v>
<v Speaker 3>I think there will be a handful of </v>

440
00:23:39.280 --> 00:23:41.590
<v Speaker 3>successful self driving stacks that will</v>
<v Speaker 3>make it a nowhere near the number of </v>

441
00:23:43.330 --> 00:23:45.240
<v Speaker 3>self driving companies today.</v>
<v Speaker 3>Um,</v>

442
00:23:45.380 --> 00:23:46.970
<v Speaker 3>but a handful,</v>
<v Speaker 3>I think</v>

443
00:23:50.960 --> 00:23:55.960
<v Speaker 6>two questions.</v>
<v Speaker 6>One is in invariably new product </v>

444
00:23:55.960 --> 00:23:59.201
<v Speaker 6>development,</v>
<v Speaker 6>there's typically two types of </v>

445
00:23:59.201 --> 00:24:01.330
<v Speaker 6>bottlenecks.</v>
<v Speaker 6>There's a technological bottleneck and </v>

446
00:24:01.330 --> 00:24:01.330
<v Speaker 6>an economic bottleneck,</v>
<v Speaker 6>right?</v>

447
00:24:01.330 --> 00:24:04.070
<v Speaker 6>So technological bottleneck might be,</v>
<v Speaker 6>hey,</v>

448
00:24:04.071 --> 00:24:05.840
<v Speaker 6>you know,</v>
<v Speaker 6>the sensors aren't good enough,</v>

449
00:24:05.870 --> 00:24:08.510
<v Speaker 6>or the machine learning algorithms </v>
<v Speaker 6>aren't good enough and so on.</v>

450
00:24:08.511 --> 00:24:11.600
<v Speaker 6>I'd be interested to hear and it'll </v>
<v Speaker 6>shift obviously over time,</v>

451
00:24:11.930 --> 00:24:16.930
<v Speaker 6>so I'd be interested to know what you </v>
<v Speaker 6>would say is the current thing that if </v>

452
00:24:16.930 --> 00:24:16.930
<v Speaker 6>hey,</v>
<v Speaker 6>if,</v>

453
00:24:16.930 --> 00:24:19.190
<v Speaker 6>if this part of the,</v>
<v Speaker 6>of the architecture was 10 times better,</v>

454
00:24:19.191 --> 00:24:21.260
<v Speaker 6>we would.</v>
<v Speaker 6>And then on the economic side,</v>

455
00:24:21.261 --> 00:24:22.500
<v Speaker 6>I'd be interested to know,</v>
<v Speaker 6>you know,</v>

456
00:24:22.590 --> 00:24:26.210
<v Speaker 6>jeff,</v>
<v Speaker 6>if sensors were 100 times cheaper than,</v>

457
00:24:26.240 --> 00:24:27.740
<v Speaker 6>so it'd be interesting to hear your </v>
<v Speaker 6>perspective.</v>

458
00:24:28.270 --> 00:24:33.270
<v Speaker 3>That's a great question.</v>
<v Speaker 3>Let me start with the economic side of </v>

459
00:24:33.270 --> 00:24:37.591
<v Speaker 3>it.</v>
<v Speaker 3>And just to get that at the wake is a </v>

460
00:24:37.591 --> 00:24:37.591
<v Speaker 3>little bit quicker.</v>
<v Speaker 3>Answer.</v>

461
00:24:37.591 --> 00:24:42.250
<v Speaker 3>The economics of operating a self </v>
<v Speaker 3>driving vehicle in a shared network </v>

462
00:24:42.250 --> 00:24:47.011
<v Speaker 3>today would close that,</v>
<v Speaker 3>that business case closes even with high</v>

463
00:24:47.231 --> 00:24:48.160
<v Speaker 3>costs.</v>
<v Speaker 3>So sensors,</v>

464
00:24:48.470 --> 00:24:49.720
<v Speaker 3>um,</v>
<v Speaker 3>that is not,</v>

465
00:24:49.750 --> 00:24:54.750
<v Speaker 3>that is not what's stopping us.</v>
<v Speaker 3>And that's part of why the gentleman </v>

466
00:24:54.750 --> 00:24:55.900
<v Speaker 3>earlier who asked,</v>
<v Speaker 3>you know,</v>

467
00:24:56.290 --> 00:25:01.290
<v Speaker 3>should use lidar or not.</v>
<v Speaker 3>If your target is to initially deploy </v>

468
00:25:01.290 --> 00:25:06.271
<v Speaker 3>these in fleets,</v>
<v Speaker 3>you would be wise to start at the top </v>

469
00:25:06.271 --> 00:25:09.991
<v Speaker 3>end of the market,</v>
<v Speaker 3>develop and deploy a system that's as </v>

470
00:25:09.991 --> 00:25:10.450
<v Speaker 3>capable as possible,</v>
<v Speaker 3>as quickly as possible,</v>

471
00:25:10.780 --> 00:25:15.780
<v Speaker 3>and then costs it down over time.</v>
<v Speaker 3>And you can do that as computer vision </v>

472
00:25:15.780 --> 00:25:17.650
<v Speaker 3>with precision recall increase today.</v>
<v Speaker 3>They're not good enough,</v>

473
00:25:17.740 --> 00:25:20.590
<v Speaker 3>right?</v>
<v Speaker 3>And so,</v>

474
00:25:20.700 --> 00:25:25.700
<v Speaker 3>so economically,</v>
<v Speaker 3>depending on your model of going to </v>

475
00:25:25.700 --> 00:25:28.631
<v Speaker 3>market,</v>
<v Speaker 3>and we believe that the right model is </v>

476
00:25:28.631 --> 00:25:30.290
<v Speaker 3>through a,</v>
<v Speaker 3>you know,</v>

477
00:25:30.291 --> 00:25:31.690
<v Speaker 3>on the mobility services,</v>

478
00:25:33.960 --> 00:25:36.320
<v Speaker 3>you can cost out your costs down the </v>
<v Speaker 3>center.</v>

479
00:25:36.380 --> 00:25:37.530
<v Speaker 3>Inevitably,</v>
<v Speaker 3>you know,</v>

480
00:25:37.531 --> 00:25:39.990
<v Speaker 3>there's no unobtainium in lidar units </v>
<v Speaker 3>today.</v>

481
00:25:40.020 --> 00:25:45.020
<v Speaker 3>There's no reason fundamentally that he </v>
<v Speaker 3>should cost of lidar unit will lead you </v>

482
00:25:45.020 --> 00:25:45.540
<v Speaker 3>to a $70,000</v>
<v Speaker 3>price point,</v>

483
00:25:45.890 --> 00:25:46.890
<v Speaker 3>right?</v>
<v Speaker 3>Um,</v>

484
00:25:46.950 --> 00:25:51.950
<v Speaker 3>however,</v>
<v Speaker 3>if you build anything in low enough </v>

485
00:25:51.950 --> 00:25:52.500
<v Speaker 3>volumes is going to be expensive.</v>
<v Speaker 3>Many of these things will work their way</v>

486
00:25:52.501 --> 00:25:57.501
<v Speaker 3>into the standard automotive process.</v>
<v Speaker 3>They'll work their way into tier one </v>

487
00:25:57.501 --> 00:26:01.431
<v Speaker 3>suppliers and when they do,</v>
<v Speaker 3>the automotive community has shown </v>

488
00:26:01.431 --> 00:26:02.850
<v Speaker 3>themselves to be exceptional driving </v>
<v Speaker 3>those costs down.</v>

489
00:26:02.851 --> 00:26:06.510
<v Speaker 3>And so I expect them to come way down to</v>
<v Speaker 3>your other question,</v>

490
00:26:06.590 --> 00:26:08.640
<v Speaker 3>a technological bottlenecks and </v>
<v Speaker 3>challenges.</v>

491
00:26:09.800 --> 00:26:14.800
<v Speaker 3>One of the key challenges of self </v>
<v Speaker 3>driving is and remains that of </v>

492
00:26:14.800 --> 00:26:19.390
<v Speaker 3>forecasting the intent and behave and </v>
<v Speaker 3>future behaviors of other actors both in</v>

493
00:26:21.041 --> 00:26:26.041
<v Speaker 3>response to one another,</v>
<v Speaker 3>but also in response to your own </v>

494
00:26:26.041 --> 00:26:28.000
<v Speaker 3>decisions in motion.</v>
<v Speaker 3>That's a perception problem,</v>

495
00:26:28.390 --> 00:26:30.310
<v Speaker 3>but it's something more than a </v>
<v Speaker 3>perception problem.</v>

496
00:26:30.311 --> 00:26:34.790
<v Speaker 3>It's also a prediction and you know,</v>
<v Speaker 3>there,</v>

497
00:26:34.791 --> 00:26:39.791
<v Speaker 3>there are a number of different things </v>
<v Speaker 3>that come together to have that have to </v>

498
00:26:39.791 --> 00:26:43.981
<v Speaker 3>come together to solve this.</v>
<v Speaker 3>We're excited about some of the that </v>

499
00:26:43.981 --> 00:26:48.441
<v Speaker 3>we're using and interleaving various,</v>
<v Speaker 3>a modern machine learning techniques </v>

500
00:26:48.441 --> 00:26:53.300
<v Speaker 3>throughout the system to do things like </v>
<v Speaker 3>project our own behaviors that were </v>

501
00:26:53.911 --> 00:26:58.911
<v Speaker 3>learned for the ego vehicle on others </v>
<v Speaker 3>and assume that they'll behave as we </v>

502
00:26:58.911 --> 00:27:00.930
<v Speaker 3>would had we been in that situation.</v>
<v Speaker 3>Like an expert system kind of approach,</v>

503
00:27:00.931 --> 00:27:01.580
<v Speaker 3>right?</v>
<v Speaker 3>Yeah.</v>

504
00:27:01.980 --> 00:27:02.640
<v Speaker 3>Yeah.</v>
<v Speaker 3>Uh,</v>

505
00:27:02.641 --> 00:27:07.641
<v Speaker 3>you,</v>
<v Speaker 3>you assume nominal behavior and you </v>

506
00:27:07.641 --> 00:27:07.641
<v Speaker 3>guard against all phenomenal,</v>
<v Speaker 3>right?</v>

507
00:27:07.641 --> 00:27:07.940
<v Speaker 3>Um,</v>
<v Speaker 3>but it's,</v>

508
00:27:07.941 --> 00:27:09.990
<v Speaker 3>it's very much a,</v>
<v Speaker 3>it's not a solved problem.</v>

509
00:27:09.991 --> 00:27:14.991
<v Speaker 3>I wouldn't say it's very much as you get</v>
<v Speaker 3>into that really long tail development </v>

510
00:27:16.790 --> 00:27:20.780
<v Speaker 3>when you're no longer putting out </v>
<v Speaker 3>demonstration videos,</v>

511
00:27:20.781 --> 00:27:25.781
<v Speaker 3>but you're,</v>
<v Speaker 3>instead of just putting your head down </v>

512
00:27:25.781 --> 00:27:25.781
<v Speaker 3>and seeking out those fine online's </v>
<v Speaker 3>yeah.</v>

513
00:27:25.781 --> 00:27:27.590
<v Speaker 3>That's the kind of problem you tend to </v>
<v Speaker 3>deal with.</v>

514
00:27:28.610 --> 00:27:29.490
<v Speaker 3>Yeah.</v>

515
00:27:31.530 --> 00:27:34.550
<v Speaker 7>Um,</v>
<v Speaker 7>so this question isn't necessarily about</v>

516
00:27:34.551 --> 00:27:38.660
<v Speaker 7>the development of a self driving cars,</v>
<v Speaker 7>but more of like an ethics question.</v>

517
00:27:39.040 --> 00:27:44.040
<v Speaker 7>Um,</v>
<v Speaker 7>when you're putting a human lives into </v>

518
00:27:44.040 --> 00:27:46.480
<v Speaker 7>the hands of software,</v>
<v Speaker 7>isn't there always the possibility for </v>

519
00:27:46.480 --> 00:27:50.110
<v Speaker 7>like outside agents with malicious </v>
<v Speaker 7>intent to use it for their own gain and </v>

520
00:27:50.110 --> 00:27:52.460
<v Speaker 7>how do you guys,</v>
<v Speaker 7>if you do have a plan,</v>

521
00:27:52.461 --> 00:27:56.190
<v Speaker 7>how do you intend to protect against </v>
<v Speaker 7>that?</v>

522
00:27:57.330 --> 00:28:02.330
<v Speaker 3>So security is a very real aspect of </v>
<v Speaker 3>this that has to be solved.</v>

523
00:28:07.070 --> 00:28:12.070
<v Speaker 3>It's a constant game of cat and mouse </v>
<v Speaker 3>and so I think it just requires a very </v>

524
00:28:12.621 --> 00:28:15.470
<v Speaker 3>good team and a concerted effort over </v>
<v Speaker 3>time.</v>

525
00:28:16.220 --> 00:28:16.700
<v Speaker 3>Um,</v>
<v Speaker 3>I,</v>

526
00:28:16.701 --> 00:28:21.701
<v Speaker 3>I don't think,</v>
<v Speaker 3>I don't think you solve it once and I </v>

527
00:28:21.701 --> 00:28:24.370
<v Speaker 3>certainly wouldn't pretend to have a </v>
<v Speaker 3>plan that solves it and has done with </v>

528
00:28:24.370 --> 00:28:24.370
<v Speaker 3>it.</v>
<v Speaker 3>Um,</v>

529
00:28:24.490 --> 00:28:25.280
<v Speaker 3>we're,</v>
<v Speaker 3>we're,</v>

530
00:28:25.310 --> 00:28:30.310
<v Speaker 3>we,</v>
<v Speaker 3>we try to leverage best practices where </v>

531
00:28:30.310 --> 00:28:32.291
<v Speaker 3>we can in the fundamental architecture </v>
<v Speaker 3>of the system to make it less exposed </v>

532
00:28:32.330 --> 00:28:37.330
<v Speaker 3>and in particular,</v>
<v Speaker 3>key parts of the system was exposed to </v>

533
00:28:37.330 --> 00:28:38.960
<v Speaker 3>nefarious actions of others.</v>
<v Speaker 3>But at the end of the day,</v>

534
00:28:38.961 --> 00:28:41.510
<v Speaker 3>it's just a constant,</v>
<v Speaker 3>um,</v>

535
00:28:41.610 --> 00:28:43.010
<v Speaker 3>it's a constant development effort.</v>

536
00:28:46.060 --> 00:28:47.270
<v Speaker 8>Thanks for being here.</v>
<v Speaker 8>Um,</v>

537
00:28:47.590 --> 00:28:52.590
<v Speaker 8>so I had a question about what </v>
<v Speaker 8>opportunities self driving cars open up </v>

538
00:28:52.590 --> 00:28:56.940
<v Speaker 8>since driving has been designed around a</v>
<v Speaker 8>human being at the center since the </v>

539
00:28:56.940 --> 00:28:58.780
<v Speaker 8>beginning.</v>
<v Speaker 8>If you put a computer at the center,</v>

540
00:28:59.470 --> 00:29:00.610
<v Speaker 8>what,</v>
<v Speaker 8>you know,</v>

541
00:29:00.670 --> 00:29:05.670
<v Speaker 8>a society wide differences and maybe </v>
<v Speaker 8>even within individual car differences </v>

542
00:29:05.670 --> 00:29:09.661
<v Speaker 8>that open up like cars go 150 miles an </v>
<v Speaker 8>hour on the highway and get places much </v>

543
00:29:09.661 --> 00:29:10.860
<v Speaker 8>faster with cars,</v>
<v Speaker 8>be like,</v>

544
00:29:11.080 --> 00:29:16.080
<v Speaker 8>look differently when a human doesn't </v>
<v Speaker 8>need to be paying attention and stuff </v>

545
00:29:16.080 --> 00:29:16.080
<v Speaker 8>like that.</v>

546
00:29:16.080 --> 00:29:17.700
<v Speaker 3>I think the answer is yes.</v>
<v Speaker 3>The,</v>

547
00:29:17.780 --> 00:29:18.580
<v Speaker 3>and,</v>
<v Speaker 3>and that's,</v>

548
00:29:18.610 --> 00:29:19.960
<v Speaker 3>that's something that's very exciting,</v>
<v Speaker 3>right?</v>

549
00:29:19.961 --> 00:29:24.961
<v Speaker 3>So one of the,</v>
<v Speaker 3>I think one of the unique opportunities </v>

550
00:29:24.961 --> 00:29:29.821
<v Speaker 3>that automakers in particular have when </v>
<v Speaker 3>self driving technology gets </v>

551
00:29:29.821 --> 00:29:31.820
<v Speaker 3>incorporated into the vehicles is they </v>
<v Speaker 3>can do things like play with,</v>

552
00:29:31.900 --> 00:29:35.170
<v Speaker 3>like differentiate the user experience.</v>
<v Speaker 3>They can provide services,</v>

553
00:29:35.560 --> 00:29:36.490
<v Speaker 3>um,</v>
<v Speaker 3>you know,</v>

554
00:29:37.010 --> 00:29:40.070
<v Speaker 3>uh,</v>
<v Speaker 3>augmented reality services or you know,</v>

555
00:29:40.240 --> 00:29:42.640
<v Speaker 3>a location resilient and many other sort</v>
<v Speaker 3>of.</v>

556
00:29:42.880 --> 00:29:47.880
<v Speaker 3>It opens a new window into an entirely </v>
<v Speaker 3>new market that automakers haven't </v>

557
00:29:47.880 --> 00:29:49.870
<v Speaker 3>historically played in.</v>
<v Speaker 3>Um,</v>

558
00:29:49.900 --> 00:29:53.190
<v Speaker 3>and it allows them to change the,</v>
<v Speaker 3>the,</v>

559
00:29:53.290 --> 00:29:56.140
<v Speaker 3>the vehicles themselves,</v>
<v Speaker 3>as you've mentioned,</v>

560
00:29:56.230 --> 00:30:01.230
<v Speaker 3>the interior.</v>
<v Speaker 3>Can change as we validate some of these </v>

561
00:30:02.051 --> 00:30:06.220
<v Speaker 3>self driving systems and confirm that </v>
<v Speaker 3>they do in fact reduce the collision,</v>

562
00:30:06.230 --> 00:30:08.380
<v Speaker 3>the rate of collisions as we hope they </v>
<v Speaker 3>will.</v>

563
00:30:08.850 --> 00:30:11.740
<v Speaker 3>Um,</v>
<v Speaker 3>you can start to pull out a lot of the,</v>

564
00:30:12.000 --> 00:30:13.930
<v Speaker 3>um,</v>
<v Speaker 3>extra,</v>

565
00:30:14.390 --> 00:30:19.390
<v Speaker 3>you know,</v>
<v Speaker 3>mass and other things that we've added </v>

566
00:30:19.390 --> 00:30:19.390
<v Speaker 3>to vehicles to make them more passively </v>
<v Speaker 3>safe.</v>

567
00:30:19.390 --> 00:30:20.200
<v Speaker 3>Right?</v>
<v Speaker 3>Roll cages,</v>

568
00:30:20.201 --> 00:30:21.750
<v Speaker 3>crumple zones,</v>
<v Speaker 3>airbags,</v>

569
00:30:22.060 --> 00:30:23.500
<v Speaker 3>you know,</v>
<v Speaker 3>a lot of these things.</v>

570
00:30:23.720 --> 00:30:25.150
<v Speaker 3>Um,</v>
<v Speaker 3>you know,</v>

571
00:30:25.270 --> 00:30:28.060
<v Speaker 3>presumably in a world where we don't </v>
<v Speaker 3>crash,</v>

572
00:30:28.600 --> 00:30:33.600
<v Speaker 3>there is,</v>
<v Speaker 3>there is much less need for passive </v>

573
00:30:33.600 --> 00:30:33.910
<v Speaker 3>safety system.</v>
<v Speaker 3>So yes.</v>

574
00:30:36.260 --> 00:30:41.260
<v Speaker 5>I have a question about the gullah nogo.</v>
<v Speaker 5>A desk that you conduct for certain </v>

575
00:30:41.260 --> 00:30:42.080
<v Speaker 5>features.</v>
<v Speaker 5>Like you mentioned,</v>

576
00:30:42.081 --> 00:30:44.180
<v Speaker 5>the throttle control,</v>
<v Speaker 5>where your slow down the throttle.</v>

577
00:30:45.380 --> 00:30:47.930
<v Speaker 5>Assuming that the driver has pressed the</v>
<v Speaker 5>wrong wrong pedal.</v>

578
00:30:48.530 --> 00:30:51.020
<v Speaker 5>When you test,</v>
<v Speaker 5>when you decided to launch that feature,</v>

579
00:30:51.021 --> 00:30:56.021
<v Speaker 5>how do you know it's definitely going to</v>
<v Speaker 5>work in all scenarios because your data </v>

580
00:30:56.021 --> 00:30:56.021
<v Speaker 5>set might not be censored.</v>

581
00:30:56.021 --> 00:31:00.750
<v Speaker 3>It's a,</v>
<v Speaker 3>it's a statistical evaluation every </v>

582
00:31:00.750 --> 00:31:00.750
<v Speaker 3>case,</v>
<v Speaker 3>right?</v>

583
00:31:00.750 --> 00:31:01.250
<v Speaker 3>You're right there,</v>
<v Speaker 3>you will.</v>

584
00:31:01.730 --> 00:31:06.730
<v Speaker 3>This is,</v>
<v Speaker 3>this is part of the art of self driving </v>

585
00:31:06.730 --> 00:31:09.221
<v Speaker 3>vehicle development is you will never </v>
<v Speaker 3>have comprehensively captured every </v>

586
00:31:09.741 --> 00:31:13.100
<v Speaker 3>case,</v>
<v Speaker 3>every scenario that is as uh,</v>

587
00:31:14.800 --> 00:31:15.440
<v Speaker 3>my,</v>
<v Speaker 3>my,</v>

588
00:31:15.470 --> 00:31:16.250
<v Speaker 3>my,</v>
<v Speaker 3>uh,</v>

589
00:31:16.340 --> 00:31:18.200
<v Speaker 3>some of you may want to correct me on </v>
<v Speaker 3>this.</v>

590
00:31:18.201 --> 00:31:21.710
<v Speaker 3>I think that's an unbounded set in fact,</v>
<v Speaker 3>have factory banner at some point,</v>

591
00:31:21.711 --> 00:31:23.240
<v Speaker 3>but I think it's on.</v>
<v Speaker 3>Um,</v>

592
00:31:23.590 --> 00:31:26.090
<v Speaker 3>and so you'll,</v>
<v Speaker 3>you'll never actually have characterized</v>

593
00:31:26.091 --> 00:31:27.890
<v Speaker 3>everything.</v>
<v Speaker 3>What you will have done,</v>

594
00:31:28.100 --> 00:31:33.100
<v Speaker 3>hopefully if you do it right,</v>
<v Speaker 3>is you will have established with a </v>

595
00:31:33.100 --> 00:31:37.241
<v Speaker 3>reasonable degree of confidence that you</v>
<v Speaker 3>can perform at a level of safety that's </v>

596
00:31:37.241 --> 00:31:41.020
<v Speaker 3>better than the average human driver.</v>
<v Speaker 3>And once you've reached that threshold </v>

597
00:31:41.020 --> 00:31:41.020
<v Speaker 3>and you're confident that you have </v>
<v Speaker 3>reached that threshold,</v>

598
00:31:41.570 --> 00:31:45.020
<v Speaker 3>I think the,</v>
<v Speaker 3>the opportunity to launch is,</v>

599
00:31:45.170 --> 00:31:47.720
<v Speaker 3>is real,</v>
<v Speaker 3>and you should seriously consider it.</v>

600
00:31:49.840 --> 00:31:52.420
<v Speaker 5>So thank you for your talk today.</v>
<v Speaker 5>First a,</v>

601
00:31:52.421 --> 00:31:57.421
<v Speaker 5>and my question is self driving seems to</v>
<v Speaker 5>be able to ultimately take over their </v>

602
00:31:57.431 --> 00:32:02.431
<v Speaker 5>world to some extent,</v>
<v Speaker 5>but just like other technologists today </v>

603
00:32:02.830 --> 00:32:06.670
<v Speaker 5>that open up new opportunities but also </v>
<v Speaker 5>bring in adverse effects.</v>

604
00:32:07.340 --> 00:32:12.340
<v Speaker 5>Um,</v>
<v Speaker 5>so how do you respond to fear and </v>

605
00:32:12.340 --> 00:32:15.541
<v Speaker 5>negative effects that may come in one </v>
<v Speaker 5>day and specifically what do you see as </v>

606
00:32:15.541 --> 00:32:18.640
<v Speaker 5>the positive and negative implications </v>
<v Speaker 5>of future day self driving</v>

607
00:32:19.710 --> 00:32:24.630
<v Speaker 3>positive and negative implications?</v>
<v Speaker 3>So the positive ones,</v>

608
00:32:24.631 --> 00:32:29.631
<v Speaker 3>I kind of listen and go find your </v>
<v Speaker 3>favorite press article on note list them</v>

609
00:32:29.701 --> 00:32:34.620
<v Speaker 3>as well.</v>
<v Speaker 3>The negative ones in the near term.</v>

610
00:32:34.920 --> 00:32:36.090
<v Speaker 3>Uh,</v>
<v Speaker 3>I do.</v>

611
00:32:36.390 --> 00:32:40.910
<v Speaker 3>I do worry a little bit about the </v>
<v Speaker 3>displacement of jobs,</v>

612
00:32:41.290 --> 00:32:46.290
<v Speaker 3>not a little bit this will happen.</v>
<v Speaker 3>It happens with every technology like </v>

613
00:32:46.290 --> 00:32:50.021
<v Speaker 3>this.</v>
<v Speaker 3>I think it's incumbent on us to find a </v>

614
00:32:50.021 --> 00:32:55.010
<v Speaker 3>good way of transitioning those who are </v>
<v Speaker 3>employed in some of the transportation </v>

615
00:32:55.010 --> 00:32:57.990
<v Speaker 3>sectors that will be affected into a </v>
<v Speaker 3>better work.</v>

616
00:32:58.340 --> 00:32:59.150
<v Speaker 3>Right.</v>
<v Speaker 3>Um,</v>

617
00:32:59.880 --> 00:33:03.990
<v Speaker 3>there are a few opportunities that are </v>
<v Speaker 3>interesting in that regard,</v>

618
00:33:04.230 --> 00:33:09.230
<v Speaker 3>but I think it's an important thing to </v>
<v Speaker 3>start discussing now because it's going </v>

619
00:33:09.230 --> 00:33:09.230
<v Speaker 3>to take,</v>
<v Speaker 3>you know,</v>

620
00:33:09.230 --> 00:33:14.211
<v Speaker 3>a few years and you know,</v>
<v Speaker 3>by the time we got these self driving </v>

621
00:33:14.211 --> 00:33:16.350
<v Speaker 3>systems on the roads really starting to </v>
<v Speaker 3>place that labor,</v>

622
00:33:16.351 --> 00:33:18.270
<v Speaker 3>I'd really like to have a new home for </v>
<v Speaker 3>it.</v>

623
00:33:19.280 --> 00:33:20.140
<v Speaker 5>Hi.</v>
<v Speaker 5>Uh,</v>

624
00:33:20.690 --> 00:33:22.780
<v Speaker 5>I'm kasha from the sloan school.</v>
<v Speaker 5>Uh,</v>

625
00:33:22.940 --> 00:33:25.130
<v Speaker 5>my question was more about your business</v>
<v Speaker 5>model,</v>

626
00:33:25.190 --> 00:33:30.190
<v Speaker 5>again,</v>
<v Speaker 5>with partnering with both vw and he and </v>

627
00:33:30.190 --> 00:33:32.930
<v Speaker 5>day and you just perspective on how you </v>
<v Speaker 5>were able to effectively do that.</v>

628
00:33:33.600 --> 00:33:36.560
<v Speaker 5>Did not one of them want to go sort of </v>
<v Speaker 5>exclusive with you.</v>

629
00:33:37.490 --> 00:33:39.920
<v Speaker 5>And what was your sort of thought </v>
<v Speaker 5>process about that?</v>

630
00:33:40.880 --> 00:33:41.600
<v Speaker 3>So,</v>
<v Speaker 3>our,</v>

631
00:33:41.660 --> 00:33:43.160
<v Speaker 3>our mission,</v>
<v Speaker 3>as I mentioned,</v>

632
00:33:43.720 --> 00:33:47.660
<v Speaker 3>is to get the technology to market </v>
<v Speaker 3>broadly and quickly and safely.</v>

633
00:33:48.030 --> 00:33:50.450
<v Speaker 3>Uh,</v>
<v Speaker 3>we are,</v>

634
00:33:50.660 --> 00:33:55.660
<v Speaker 3>you know,</v>
<v Speaker 3>haven't been and remain convinced that </v>

635
00:33:55.660 --> 00:33:55.820
<v Speaker 3>the right way to do that is by providing</v>
<v Speaker 3>it to,</v>

636
00:33:55.821 --> 00:34:00.821
<v Speaker 3>as much of the industry as possible to </v>
<v Speaker 3>every automaker who shares our vision </v>

637
00:34:01.041 --> 00:34:02.300
<v Speaker 3>and our approach.</v>
<v Speaker 3>Um,</v>

638
00:34:02.330 --> 00:34:05.630
<v Speaker 3>we were pleased to see that both </v>
<v Speaker 3>volkswagen group,</v>

639
00:34:06.120 --> 00:34:11.120
<v Speaker 3>uh,</v>
<v Speaker 3>and I'm assuming you all know the scope </v>

640
00:34:11.120 --> 00:34:11.120
<v Speaker 3>of volkswagen,</v>
<v Speaker 3>right?</v>

641
00:34:11.120 --> 00:34:14.700
<v Speaker 3>This is a massive automaker hyundai </v>
<v Speaker 3>motor,</v>

642
00:34:14.701 --> 00:34:18.090
<v Speaker 3>also very large cross honda,</v>
<v Speaker 3>ikea and genesis.</v>

643
00:34:19.080 --> 00:34:22.410
<v Speaker 3>They both shared our vision of how we </v>
<v Speaker 3>should do this,</v>

644
00:34:22.740 --> 00:34:24.660
<v Speaker 3>which was important to us.</v>
<v Speaker 3>Um,</v>

645
00:34:24.690 --> 00:34:26.420
<v Speaker 3>they both shared,</v>
<v Speaker 3>you know,</v>

646
00:34:26.610 --> 00:34:31.610
<v Speaker 3>a,</v>
<v Speaker 3>a keen interest in making a difference </v>

647
00:34:32.070 --> 00:34:37.070
<v Speaker 3>at scale through their platforms.</v>
<v Speaker 3>A volkswagen has a very admirable set of</v>

648
00:34:37.441 --> 00:34:39.460
<v Speaker 3>initiatives around electric vehicle </v>
<v Speaker 3>electrification.</v>

649
00:34:39.461 --> 00:34:41.940
<v Speaker 3>And few other things,</v>
<v Speaker 3>honda is doing similar things.</v>

650
00:34:42.350 --> 00:34:47.350
<v Speaker 3>Um,</v>
<v Speaker 3>and so for us it was important that we </v>

651
00:34:47.350 --> 00:34:49.170
<v Speaker 3>enable everyone and that was kind of </v>
<v Speaker 3>what aurora was started to do.</v>

652
00:34:49.830 --> 00:34:52.980
<v Speaker 5>Hi,</v>
<v Speaker 5>I had a question now that I see a lot of</v>

653
00:34:53.280 --> 00:34:55.020
<v Speaker 5>companies are coming up with self </v>
<v Speaker 5>driving cars,</v>

654
00:34:55.080 --> 00:34:58.110
<v Speaker 5>right?</v>
<v Speaker 5>So most of the costs are pretty much all</v>

655
00:34:58.111 --> 00:35:03.111
<v Speaker 5>the technology is bound only to the car.</v>
<v Speaker 5>So would we see something like an open </v>

656
00:35:03.111 --> 00:35:07.281
<v Speaker 5>network where car communicate with each </v>
<v Speaker 5>other regardless of which company they </v>

657
00:35:07.281 --> 00:35:08.640
<v Speaker 5>come from and what this in any way,</v>
<v Speaker 5>uh,</v>

658
00:35:08.641 --> 00:35:13.641
<v Speaker 5>you know,</v>
<v Speaker 5>increase the safety or the performance </v>

659
00:35:13.641 --> 00:35:13.641
<v Speaker 5>of vehicles and stuff like that.</v>

660
00:35:13.641 --> 00:35:17.200
<v Speaker 3>I think.</v>
<v Speaker 3>I think you're getting a vehicle to </v>

661
00:35:17.200 --> 00:35:19.030
<v Speaker 3>vehicle vehicle to infrastructure </v>
<v Speaker 3>communication efforts ongoing and that,</v>

662
00:35:19.080 --> 00:35:20.210
<v Speaker 3>uh,</v>
<v Speaker 3>and it's certainly,</v>

663
00:35:20.211 --> 00:35:22.010
<v Speaker 3>it's,</v>
<v Speaker 3>it's only positive,</v>

664
00:35:22.011 --> 00:35:22.850
<v Speaker 3>right?</v>
<v Speaker 3>The,</v>

665
00:35:22.930 --> 00:35:27.560
<v Speaker 3>it having that information available to </v>
<v Speaker 3>you can only make things better.</v>

666
00:35:28.070 --> 00:35:33.070
<v Speaker 3>The challenge who's historically </v>
<v Speaker 3>bandwidth vehicle to vehicle and break </v>

667
00:35:33.070 --> 00:35:33.680
<v Speaker 3>your particular vehicle to </v>
<v Speaker 3>infrastructure or vice versa,</v>

668
00:35:34.050 --> 00:35:36.440
<v Speaker 3>um,</v>
<v Speaker 3>that it doesn't scale well,</v>

669
00:35:36.680 --> 00:35:38.910
<v Speaker 3>one and two,</v>
<v Speaker 3>it's been slow.</v>

670
00:35:38.970 --> 00:35:40.770
<v Speaker 3>It's been much slower and coming in our </v>
<v Speaker 3>development.</v>

671
00:35:41.400 --> 00:35:46.400
<v Speaker 3>And so when we develop these systems,</v>
<v Speaker 3>we develop them without the expectation </v>

672
00:35:46.681 --> 00:35:51.681
<v Speaker 3>that those,</v>
<v Speaker 3>that those communication protocol are </v>

673
00:35:51.681 --> 00:35:54.621
<v Speaker 3>available to us,</v>
<v Speaker 3>we'll certainly protect for them and it </v>

674
00:35:54.621 --> 00:35:54.621
<v Speaker 3>will certainly be,</v>
<v Speaker 3>you know,</v>

675
00:35:54.621 --> 00:35:56.550
<v Speaker 3>a benefit once they're,</v>
<v Speaker 3>once they're here,</v>

676
00:35:56.910 --> 00:36:01.910
<v Speaker 3>but until then,</v>
<v Speaker 3>many of the hard problems that I would </v>

677
00:36:01.910 --> 00:36:04.731
<v Speaker 3>have welcomed 10 years ago to have a </v>
<v Speaker 3>beacon on every traffic light that just </v>

678
00:36:04.731 --> 00:36:06.120
<v Speaker 3>told me it's state rather than having to</v>
<v Speaker 3>perceive it.</v>

679
00:36:06.510 --> 00:36:11.510
<v Speaker 3>Uh,</v>
<v Speaker 3>I would have certainly use those 10 </v>

680
00:36:11.510 --> 00:36:13.401
<v Speaker 3>years ago.</v>
<v Speaker 3>Now they're less significant because </v>

681
00:36:13.401 --> 00:36:17.061
<v Speaker 3>we've kind of worked our way through a </v>
<v Speaker 3>lot of the problems they would have </v>

682
00:36:17.061 --> 00:36:17.061
<v Speaker 3>solved.</v>

683
00:36:17.061 --> 00:36:17.870
<v Speaker 9>Sinky food talk.</v>
<v Speaker 9>My question is,</v>

684
00:36:18.290 --> 00:36:23.290
<v Speaker 9>what's your opinion about cooperation of</v>
<v Speaker 9>self driving vr curse?</v>

685
00:36:23.570 --> 00:36:28.570
<v Speaker 9>So maybe I think if you can control a </v>
<v Speaker 9>group of self driving because at the </v>

686
00:36:28.570 --> 00:36:31.280
<v Speaker 9>same time you can achieve a lot of </v>
<v Speaker 9>benefits to the traffic.</v>

687
00:36:31.430 --> 00:36:33.140
<v Speaker 3>Yes.</v>
<v Speaker 3>That is where one of the,</v>

688
00:36:33.370 --> 00:36:36.230
<v Speaker 3>that is where a lot of the benefits come</v>
<v Speaker 3>from and infrastructure utilization,</v>

689
00:36:36.231 --> 00:36:41.231
<v Speaker 3>right,</v>
<v Speaker 3>is in ride sharing with autonomous </v>

690
00:36:41.231 --> 00:36:41.440
<v Speaker 3>vehicles.</v>
<v Speaker 3>And specifically,</v>

691
00:36:42.090 --> 00:36:47.090
<v Speaker 3>you know,</v>
<v Speaker 3>the better we understand demand </v>

692
00:36:47.090 --> 00:36:47.900
<v Speaker 3>patterns,</v>
<v Speaker 3>people movement goods movement,</v>

693
00:36:48.380 --> 00:36:53.380
<v Speaker 3>the better we can sort of optimally </v>
<v Speaker 3>allocate these vehicles at locations </v>

694
00:36:53.871 --> 00:36:55.400
<v Speaker 3>where they're needed.</v>
<v Speaker 3>So yes,</v>

695
00:36:55.401 --> 00:36:57.560
<v Speaker 3>that's certainly that,</v>
<v Speaker 3>that coordination,</v>

696
00:36:57.890 --> 00:36:59.000
<v Speaker 3>this is where,</v>
<v Speaker 3>as I mentioned,</v>

697
00:36:59.001 --> 00:37:01.550
<v Speaker 3>these three vectors of vehicle </v>
<v Speaker 3>electrification,</v>

698
00:37:01.850 --> 00:37:04.470
<v Speaker 3>ride sharing and autonomy or,</v>
<v Speaker 3>or transparent,</v>

699
00:37:04.480 --> 00:37:09.480
<v Speaker 3>you know,</v>
<v Speaker 3>mobility as a service and autonomy </v>

700
00:37:09.480 --> 00:37:09.620
<v Speaker 3>really come together with a unique value</v>
<v Speaker 3>proposition.</v>

701
00:37:10.300 --> 00:37:11.120
<v Speaker 3>Yeah.</v>
<v Speaker 3>Okay.</v>

702
00:37:11.121 --> 00:37:12.150
<v Speaker 3>Thank you.</v>
<v Speaker 3>Yeah.</v>

703
00:37:13.200 --> 00:37:17.540
<v Speaker 9>Thank you so much for a great talk.</v>
<v Speaker 9>Thank you.</v>

