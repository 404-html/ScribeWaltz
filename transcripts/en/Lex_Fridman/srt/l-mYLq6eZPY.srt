1
00:00:00,060 --> 00:00:02,730
The following is a conversation
with Peter at bill.

2
00:00:03,180 --> 00:00:06,780
He's a professor at UC Berkeley and
the director of the Berkeley robotics

3
00:00:06,781 --> 00:00:07,614
learning lab.

4
00:00:07,890 --> 00:00:11,820
He's one of the top researchers in the
world working on how we make robots

5
00:00:12,270 --> 00:00:15,180
understand and interact
with the world around them,

6
00:00:15,420 --> 00:00:18,600
especially using imitation and
deeper enforcement learning.

7
00:00:19,800 --> 00:00:24,060
This conversation is part of the MIT
course and artificial general intelligence

8
00:00:24,240 --> 00:00:27,180
and the artificial intelligence podcast.
If you enjoy it,

9
00:00:27,360 --> 00:00:29,880
please subscribe on Youtube,
Itunes,

10
00:00:30,000 --> 00:00:34,680
or your podcast provider of choice or
simply connect with me on Twitter at Lex

11
00:00:34,680 --> 00:00:36,410
Friedman, spelled f, r I. D.

12
00:00:36,990 --> 00:00:40,720
And now here's my conversation
with Peter a bill.

13
00:00:41,460 --> 00:00:45,840
You've mentioned that if there was
one person you could meet you be Roger

14
00:00:45,840 --> 00:00:47,010
Federer.
So let me ask,

15
00:00:48,090 --> 00:00:51,180
when do you think will have a
robot that fully autonomous,

16
00:00:51,181 --> 00:00:54,090
he can beat Roger Federer at tennis,

17
00:00:54,810 --> 00:00:58,680
Roger Federer level player a tennis?
Well first,

18
00:00:58,681 --> 00:01:02,570
if you could make it happen for me
to meet Roger, let me know terms,

19
00:01:02,910 --> 00:01:07,020
getting a robot to be him in tennis.

20
00:01:07,470 --> 00:01:12,470
It's kind of an interesting question
because for a lot of the challenges we

21
00:01:12,871 --> 00:01:16,770
think about in Ai, the software,
it's really the missing piece.

22
00:01:16,771 --> 00:01:18,120
But for something like this,

23
00:01:18,630 --> 00:01:23,630
the hardware is nowhere near either to
really have a robot that can physically

24
00:01:25,891 --> 00:01:28,530
run around the Boston dynamics robots,
they're starting to get there,

25
00:01:28,531 --> 00:01:32,320
but still not really
human level ability to do,

26
00:01:32,330 --> 00:01:34,800
run around and then swing a racket.

27
00:01:36,820 --> 00:01:39,950
So you think that's a hardware problem?
I don't think it's a harder problem only.

28
00:01:39,951 --> 00:01:42,680
I think it's a hardware and a
software problem. I think it's both.

29
00:01:43,200 --> 00:01:47,580
I think they'll, they'll have
independent progress. So I'd say the,

30
00:01:47,680 --> 00:01:52,180
the hardware maybe in 10
15 years and this way,

31
00:01:52,250 --> 00:01:56,160
not grass grass plague. I,

32
00:01:56,170 --> 00:02:00,920
I'm not sure what's Carter? Cras
or clay. The clay involves sliding,

33
00:02:01,580 --> 00:02:06,370
which might be harder to
master actually. Yeah, but you,

34
00:02:06,390 --> 00:02:08,510
you're not limited to a bipedal.

35
00:02:08,930 --> 00:02:11,450
I mean I'm sure there's only
going to build a machine.

36
00:02:11,451 --> 00:02:14,510
It's a whole different question. Of
course if it can, if you can say, okay,

37
00:02:14,511 --> 00:02:15,980
this robot can be on wheels,

38
00:02:16,280 --> 00:02:19,370
it can move around on wheels
and can be designed differently,

39
00:02:19,371 --> 00:02:24,371
then I think that that can be done sooner
probably down a full human humanoid

40
00:02:24,840 --> 00:02:27,480
type of setup.
What do you think his swing a rack,

41
00:02:27,740 --> 00:02:30,650
so you've worked at a basic manipulation,

42
00:02:31,220 --> 00:02:34,710
how hard it do you think is the
task of swinging Arachat would uh,

43
00:02:34,760 --> 00:02:37,310
be able to hit a nice
backhand or a forehand,

44
00:02:39,800 --> 00:02:44,090
let's say, let's say we just
set up stationary, uh, an a
nice robot arm, let's say,

45
00:02:44,120 --> 00:02:44,360
you know,

46
00:02:44,360 --> 00:02:49,360
standard industrial arm and it can wash
the ball calm and then swing the racket.

47
00:02:50,690 --> 00:02:53,090
It's a good question.
I'm not sure it would be

48
00:02:54,590 --> 00:02:56,120
super hard to do.

49
00:02:56,150 --> 00:02:59,990
I mean I'm sure it would require a lot
if we do it with reinforcement learning

50
00:02:59,991 --> 00:03:01,480
would require a lot of trial and error.

51
00:03:01,481 --> 00:03:05,650
It's not going to swing it right the
first time around. But yeah, I don't,

52
00:03:05,680 --> 00:03:10,270
I don't see why I couldn't see anything.
I think it's learnable.

53
00:03:10,271 --> 00:03:12,070
I think if you set up a ball machine,

54
00:03:12,100 --> 00:03:16,810
let's say on one side and then a robot
with a tennis racket on the other side,

55
00:03:17,740 --> 00:03:22,690
I think it's learnable and maybe a little
bit of pre training and simulation.

56
00:03:23,050 --> 00:03:26,350
Yeah, I think that's, I think that's
feasible. I think, I think the swing,

57
00:03:26,351 --> 00:03:27,280
the racket is feasible.

58
00:03:27,281 --> 00:03:30,160
It'd be very interesting to see how
much precision it can get. Okay.

59
00:03:31,810 --> 00:03:35,110
Cause I mean that's,
that's where,

60
00:03:35,170 --> 00:03:37,870
I mean some of the human
players can hit it on the lines,

61
00:03:37,900 --> 00:03:42,190
which is very high precision with spin.
This one is, it is, it is an interesting,

62
00:03:42,191 --> 00:03:45,670
uh, well the RL can learn
to put a spin on the ball.

63
00:03:45,730 --> 00:03:48,410
All you got me interested maybe
some day we'll set this off

64
00:03:51,130 --> 00:03:51,560
your hands

65
00:03:51,560 --> 00:03:54,170
sir. Is basically okay for this
problem. It sounds fascinating,

66
00:03:54,171 --> 00:03:56,420
but for the general
problem of a tennis player,

67
00:03:56,490 --> 00:03:58,010
we might be a little bit farther away.

68
00:03:58,580 --> 00:04:02,360
What's the most impressive thing you've
seen a robot doing? The physical world?

69
00:04:04,160 --> 00:04:07,100
So physically for me it's

70
00:04:08,890 --> 00:04:13,890
the Boston dynamics videos always just
ring home and just super impressed.

71
00:04:15,700 --> 00:04:19,450
Recently, the robot running up the stairs
during the park who are type thing.

72
00:04:19,480 --> 00:04:22,270
I mean, yes, we don't
know what's underneath.

73
00:04:22,300 --> 00:04:23,950
They don't really ride a lot of detail.

74
00:04:23,951 --> 00:04:28,450
But even if it's hard code than than the
neath, which you might or might not be.

75
00:04:28,480 --> 00:04:30,910
Just the physical abilities
of doing that park who are at,

76
00:04:30,911 --> 00:04:33,720
that's a very impressive.
So a lot right there.

77
00:04:33,790 --> 00:04:37,780
Have you met spot many or any of those
robots in person? It's spot mini.

78
00:04:38,050 --> 00:04:42,860
Last year in April at demarce
event that Jeff pieces organizes,

79
00:04:42,970 --> 00:04:47,670
they brought it out there and it
was nicely falling around Jeff,

80
00:04:47,830 --> 00:04:51,790
when Jeff left the room,
they had it follow him along,
which is pretty impressive.

81
00:04:52,180 --> 00:04:53,740
So I think

82
00:04:53,780 --> 00:04:57,230
good. There's some confidence to know
that there's no learning going on on those

83
00:04:57,231 --> 00:05:01,190
robots, the psychology of it. So while
knowing that, while knowing there's not,

84
00:05:01,191 --> 00:05:03,500
if there's any learning going on,
it's very limited.

85
00:05:04,100 --> 00:05:08,900
I met spot mini earlier this year and
knowing everything that's going on,

86
00:05:09,590 --> 00:05:12,510
having one on one interaction.
So I've got to spend some time alone

87
00:05:14,520 --> 00:05:18,540
and there's a immediately a deep
connection on the psychological level.

88
00:05:18,780 --> 00:05:22,500
Even though you know the fundamentals,
how it works, there's something magical.

89
00:05:23,280 --> 00:05:28,280
So do you think about the psychology
of interacting with robots and this

90
00:05:28,441 --> 00:05:29,101
physical world?

91
00:05:29,101 --> 00:05:34,101
Even you just showed me the PR to the
robot and there was a little bit something

92
00:05:35,431 --> 00:05:38,520
like a face head, a little
bit, something like a face.

93
00:05:38,521 --> 00:05:40,590
There's something that
immediately you draws you to it.

94
00:05:40,620 --> 00:05:44,880
Do you think about that aspect of,
of the robotics problem?

95
00:05:45,200 --> 00:05:49,460
Well, it's very hard with Brett
here. We're giving them a name.

96
00:05:49,850 --> 00:05:53,900
Berkeley robot for the elimination of
tedious task is very hard to not think of

97
00:05:55,160 --> 00:05:59,600
the robot as a person and
it like everybody calls him
a he for whatever reason,

98
00:05:59,601 --> 00:06:02,120
but that also makes it more
a person than if it was a it

99
00:06:03,710 --> 00:06:07,220
and it's, it seems pretty natural
to think of it. That would,

100
00:06:07,250 --> 00:06:12,250
this past weekend really struck me of
FCM pepper many times on on videos.

101
00:06:13,340 --> 00:06:15,320
But then I was at an event organized by,

102
00:06:15,350 --> 00:06:20,350
this was by fidelity and they had scripted
pepper to help moderate some sessions

103
00:06:22,790 --> 00:06:26,150
and at scripted pepper to have the
personality of a child a little bit.

104
00:06:26,510 --> 00:06:31,510
And it was very hard to not think of it
as it's own person in some sense because

105
00:06:32,091 --> 00:06:33,020
it was just going to jump in.

106
00:06:33,021 --> 00:06:36,390
It would just jump into conversation and
in very interactive with moderate we'd

107
00:06:36,400 --> 00:06:39,590
be saving, the pepper would just
jump in, hold on, how about me?

108
00:06:40,100 --> 00:06:43,580
Can I participate in this? Doing? And just
like, okay, this is like like a person.

109
00:06:43,730 --> 00:06:45,140
And that was 100% scripted.

110
00:06:45,590 --> 00:06:49,520
And even then it was hard not to have
that sense of somehow there is something

111
00:06:49,521 --> 00:06:54,410
there. So as we have robots
interact in this physical world,

112
00:06:54,411 --> 00:06:58,040
is that a signal that can be used
in reinforcement learning? You've,

113
00:06:58,430 --> 00:07:00,230
you've worked a little
bit in this direction,

114
00:07:00,231 --> 00:07:03,110
but do you think that psychology
can be somehow pulled in?

115
00:07:04,360 --> 00:07:08,690
So that's a question I would say a lot.
A lot of people ask.

116
00:07:09,050 --> 00:07:14,050
And I think part of why they ask it is
there thinking about how unique are we

117
00:07:15,261 --> 00:07:18,080
really still ask people.
Like after they see some results,

118
00:07:18,081 --> 00:07:22,370
they see a computer play go to say
computer do this, that they're like, okay,

119
00:07:22,371 --> 00:07:26,420
but can it really have emotion? Can it
really interact with us in that way?

120
00:07:26,750 --> 00:07:30,110
And then once you're around robots,
you already start feeling it.

121
00:07:30,111 --> 00:07:35,111
And I think that kind of
maybe mythologically the
way that I think of it as if

122
00:07:36,261 --> 00:07:37,610
you run something like
reinforcement learning,

123
00:07:37,611 --> 00:07:40,430
it's about optimizing some objective and

124
00:07:42,230 --> 00:07:47,230
there was no reason that the objective
couldn't be tied into how much does a

125
00:07:48,651 --> 00:07:51,800
person like interacting with
this system and why it could not.

126
00:07:51,920 --> 00:07:56,240
The reinforcement learning
system optimized for the
robot being fun to be around

127
00:07:56,690 --> 00:08:00,290
and why wouldn't it then naturally become
more and more attractive and more and

128
00:08:00,291 --> 00:08:04,580
more maybe like a person or like a pet.
I don't know what it would exactly be,

129
00:08:04,581 --> 00:08:08,090
but more and more have those features
and acquire them automatically.

130
00:08:08,300 --> 00:08:13,300
As long as you can formalize an objective
of what it means to like something.

131
00:08:13,400 --> 00:08:17,300
What, how you exhibit, uh, what's
the ground truth? How do you,

132
00:08:17,330 --> 00:08:19,130
how do you get the reward from human?

133
00:08:19,520 --> 00:08:22,340
Cause you have to somehow collect
that information with the new human,

134
00:08:22,370 --> 00:08:25,490
but you're, you're saying if you
can formulate it as an objective,

135
00:08:26,270 --> 00:08:29,360
it can be learned. There is no reason
it couldn't emerge through learning.

136
00:08:29,361 --> 00:08:32,630
And maybe one way to formulate as an
objective you wouldn't have to necessarily

137
00:08:32,631 --> 00:08:33,800
score and explicitly.

138
00:08:33,801 --> 00:08:37,850
So standard rewards had numbers
and numbers are hard to come by.

139
00:08:38,720 --> 00:08:42,650
I see. It's a 1.5 or 1.7 on some skill.
It's very hard to do for a person,

140
00:08:43,040 --> 00:08:45,890
but much easier is for a person to say,
okay,

141
00:08:46,160 --> 00:08:50,090
what you did the last five minutes was
much nicer than when we did the previous

142
00:08:50,091 --> 00:08:53,170
five minutes.
And that now gives a comparison comparing,

143
00:08:53,180 --> 00:08:55,770
in fact there has been some results that,
for example,

144
00:08:55,771 --> 00:09:00,120
Paul Christian and collaborators
had opening. I had the hopper,

145
00:09:00,180 --> 00:09:05,180
we Joko hopper one legged robot that
flew back flips purely from feedback.

146
00:09:05,640 --> 00:09:08,340
I like this better than that.
That's kind of equally good.

147
00:09:08,640 --> 00:09:10,860
And after a bunch of interactions,

148
00:09:10,890 --> 00:09:14,070
it figured out what it was the person
was asking for it, namely a backflip.

149
00:09:14,430 --> 00:09:18,660
And so I think the same thing, oh,
it wasn't trying to do a backflip,

150
00:09:18,661 --> 00:09:21,450
it was just getting a score from
the comparison score from the person

151
00:09:21,550 --> 00:09:23,250
based on,
uh,

152
00:09:23,330 --> 00:09:27,400
person having a mind in their own
mind what I wanted to do a back flip.

153
00:09:27,401 --> 00:09:31,690
But the robot didn't know what it was
supposed to be doing. It just knew them.

154
00:09:31,750 --> 00:09:34,240
Sometimes the person said this is better,
this is force.

155
00:09:34,570 --> 00:09:38,320
And then the robot figure it out what
the person was actually after it was a

156
00:09:38,321 --> 00:09:38,801
back flip.

157
00:09:38,801 --> 00:09:43,150
And I imagine the same would be true for
things like a more interactive robots

158
00:09:43,151 --> 00:09:45,310
that the robot would figure out over time.
Oh,

159
00:09:45,311 --> 00:09:49,330
this kind of thing apparently
as appreciate it more than
his other kind of thing.

160
00:09:50,200 --> 00:09:50,690
So

161
00:09:50,690 --> 00:09:53,800
when I first picked up a sudden's,
uh,

162
00:09:53,930 --> 00:09:56,030
which is sons reinforcement learning book

163
00:09:58,370 --> 00:10:00,680
before sort of this deep learning,
um,

164
00:10:01,280 --> 00:10:04,760
before the reemergence of neural networks
is a powerful mechanism for machine

165
00:10:04,761 --> 00:10:07,760
learning.
Rl seemed to me like magic.

166
00:10:08,300 --> 00:10:10,250
It was a as beautiful.

167
00:10:10,280 --> 00:10:14,930
So it that seemed like what intelligence
is a RL reinforcement learning.

168
00:10:15,500 --> 00:10:16,333
So,
uh,

169
00:10:17,540 --> 00:10:21,560
how do you think we can possibly learn
anything about the world when they're

170
00:10:21,561 --> 00:10:25,460
award for the actions is delayed,
is so sparse.

171
00:10:25,790 --> 00:10:29,510
Like where is,
why do you think are works?

172
00:10:30,530 --> 00:10:34,640
Why do you think he can learn
anything under such sparser awards?

173
00:10:35,020 --> 00:10:38,030
Whether it's regular reinforcement
learning, deep reinforcement learning,

174
00:10:38,620 --> 00:10:39,560
it's your intuition.

175
00:10:40,840 --> 00:10:43,580
The counterpart of that is why is RL,

176
00:10:44,480 --> 00:10:48,740
why does it need so many samples?
So many experiences to learn from.

177
00:10:49,240 --> 00:10:52,250
Um, because really what's happening
is when you have a sparse reward,

178
00:10:53,000 --> 00:10:55,130
you do something maybe for like,
I dunno,

179
00:10:55,160 --> 00:10:58,400
you'd take a hundred actions and then
you get a reward and maybe get like a

180
00:10:58,401 --> 00:11:02,660
score of three. And I'm like, okay,
three, not sure what that means.

181
00:11:02,960 --> 00:11:04,520
You go again and now I get to,

182
00:11:05,030 --> 00:11:07,760
and now you know that that sequence of a
hundred actions that you did the second

183
00:11:07,761 --> 00:11:08,271
time around,

184
00:11:08,271 --> 00:11:11,330
somehow it was worse than the sequence
of hundred actions he did the first time

185
00:11:11,331 --> 00:11:15,170
around. But that's tough to know which
one of those were better or worse.

186
00:11:15,171 --> 00:11:16,940
Some might have been good
and bad and either one,

187
00:11:17,510 --> 00:11:19,850
and so that's why I need
so many experiences.

188
00:11:19,851 --> 00:11:23,210
But once you have enough experiences,
effectively RLS teasing that apart,

189
00:11:23,450 --> 00:11:26,650
it's turned to say, okay, well
what is consistently there?

190
00:11:26,660 --> 00:11:28,640
When you get a high reward
and what's consistently there,

191
00:11:28,641 --> 00:11:29,690
we can get a lower award.

192
00:11:29,990 --> 00:11:34,310
And then kind of the magic of it as
the policy grant update is to say,

193
00:11:34,700 --> 00:11:38,780
now let's update the neural network
to make the actions that were kind of

194
00:11:38,781 --> 00:11:39,890
precedent when things are good,

195
00:11:40,220 --> 00:11:44,000
more likely and make the actions that
are present when things are not as good,

196
00:11:44,030 --> 00:11:44,870
less likely.

197
00:11:45,130 --> 00:11:46,920
So that's the,
that is the counterpoint,

198
00:11:46,990 --> 00:11:50,920
but it seems like you would need
to run it a lot more than you do.

199
00:11:50,921 --> 00:11:54,190
Even though right now people could
say that is very inefficient,

200
00:11:54,430 --> 00:11:59,430
but it seems to be way more efficient
than one would imagine on paper that the,

201
00:11:59,650 --> 00:12:02,860
the simple updates to the policy,
the policy gradient,

202
00:12:02,861 --> 00:12:05,890
that that somehow you can
learn is executives just said,

203
00:12:05,891 --> 00:12:10,180
what are the common actions that seem
to produce some good results that that

204
00:12:10,181 --> 00:12:14,140
somehow can learn anything.
It seems counterintuitive,

205
00:12:14,141 --> 00:12:18,220
at least did they, is there some
intuition behind that? So yeah,

206
00:12:18,480 --> 00:12:21,990
I think there's a few
ways to think about this.

207
00:12:22,470 --> 00:12:26,040
The way I tend to think about
it mostly originally when,

208
00:12:26,430 --> 00:12:30,090
so when we started working on deep
reinforcement learning here at Berkeley,

209
00:12:30,120 --> 00:12:32,700
which was maybe 2011, 12, 13,

210
00:12:32,730 --> 00:12:36,540
around that time challenge Schulman
was a phd student initially.

211
00:12:36,541 --> 00:12:38,250
Kind of driving it forward here.

212
00:12:39,510 --> 00:12:44,510
And kind of the way we thought about
it at the time was if you think about

213
00:12:45,150 --> 00:12:50,130
rectified linear units or kind of
rectifier type neural networks, um,

214
00:12:50,220 --> 00:12:54,360
what do you get? You get something
that's piecewise linear feedback control.

215
00:12:55,050 --> 00:12:57,080
And if you look at the literature,
uh,

216
00:12:57,090 --> 00:13:00,120
linear feedback control is extremely
successful, can solve many,

217
00:13:00,121 --> 00:13:04,610
many problems surprisingly well.
I remember for example,

218
00:13:04,611 --> 00:13:07,280
when we did the helicopter flight,
if you're in a stationary flight regime,

219
00:13:07,310 --> 00:13:10,970
not a non station but the station,
their flight regime like hover,

220
00:13:11,210 --> 00:13:13,550
you can use linear feedback control
to stabilize and helicopter,

221
00:13:13,560 --> 00:13:18,470
very complex dynamical system.
But the controller is relatively simple.

222
00:13:18,471 --> 00:13:22,100
And so I think that's a big part of
is that if you do feedback control,

223
00:13:22,310 --> 00:13:24,980
even though the system you
control can be very, very complex,

224
00:13:25,010 --> 00:13:29,840
often relatively simple control
architectures can already do a lot.

225
00:13:30,530 --> 00:13:32,330
But then also just linear
is not good enough.

226
00:13:32,570 --> 00:13:35,810
And so one way you can think
of this neural networks is
that the incidences they

227
00:13:35,811 --> 00:13:40,130
tiled a space which people were already
trying to do more by hand or with finite

228
00:13:40,131 --> 00:13:43,490
state machines. Say this linear controller
here, this linear controller here,

229
00:13:43,820 --> 00:13:45,760
neural network learns the towel dispenser,

230
00:13:45,800 --> 00:13:47,840
Linear Controller here at
another leaner controller here.

231
00:13:48,290 --> 00:13:49,580
But it's more subtle than that.

232
00:13:50,060 --> 00:13:52,670
And so it's benefiting from this linear
control aspect is benefiting from the

233
00:13:52,671 --> 00:13:56,810
tiling, but it's somehow telling
it one dimension at a time.

234
00:13:57,380 --> 00:14:00,680
Because if, let's say you have a two
layer network, even the hidden layer,

235
00:14:01,190 --> 00:14:06,190
you make a transition from active to
inactive or the other way around that is

236
00:14:06,891 --> 00:14:11,891
essentially one axes but not access
align but one direction that you change.

237
00:14:12,350 --> 00:14:15,350
And so you have this kind of
very gradual tiling of the space.

238
00:14:15,351 --> 00:14:19,280
We have a lot of sharing between the
linear controllers that towel the space.

239
00:14:19,580 --> 00:14:24,110
And that was always my intuition as to
why to expect that this might work pretty

240
00:14:24,111 --> 00:14:24,800
well.

241
00:14:24,800 --> 00:14:27,890
It's essentially leveraging the fact
that linear feedback control is so good,

242
00:14:28,550 --> 00:14:29,660
but of course not enough.

243
00:14:29,870 --> 00:14:33,600
And this is a gradual tiling of the
space with leaner feedback controls that

244
00:14:33,601 --> 00:14:34,434
share a lot of

245
00:14:35,090 --> 00:14:38,980
expertise across them. So that, that's
a, that's really nice intuition.

246
00:14:39,010 --> 00:14:43,250
What do you think that scales to the more
and more general problems of when you

247
00:14:43,251 --> 00:14:48,230
start going up the number
of control dimensions, uh,

248
00:14:48,260 --> 00:14:50,180
when you start going

249
00:14:51,230 --> 00:14:54,920
down in terms of how often
you get a clean reward signal,

250
00:14:55,400 --> 00:15:00,110
does that intuition carry forward to
those crazy or weird or worlds that we

251
00:15:00,111 --> 00:15:03,860
think of as the real world?
So

252
00:15:05,780 --> 00:15:09,740
I think where things get really tricky
in the real world compared to the things

253
00:15:09,741 --> 00:15:14,360
we've looked at so far with great
success in reinforcement learning is

254
00:15:16,310 --> 00:15:18,740
the timescales,
which takes us to an extreme.

255
00:15:18,950 --> 00:15:23,360
So when you think about the
real world, I mean, I dunno,

256
00:15:23,390 --> 00:15:27,840
maybe some student decided to do
a phd here. Right, okay. That's,

257
00:15:27,870 --> 00:15:30,200
that's a decision that's a
very high level decision.

258
00:15:30,800 --> 00:15:33,920
But if you think about their lives,
I mean any person's life,

259
00:15:34,070 --> 00:15:38,930
it's a sequence of muscle
fiber contractions and
relaxations and that's how you

260
00:15:38,931 --> 00:15:42,320
interact with the world and that's
a very high frequency control thing,

261
00:15:42,770 --> 00:15:47,240
but it's ultimately what you do and how
you affect the world until I guess we

262
00:15:47,241 --> 00:15:49,790
have brain readings and you
can maybe do it differently,

263
00:15:49,791 --> 00:15:54,791
but typically that's how you affect the
world and the decision of doing a phd,

264
00:15:55,071 --> 00:15:58,940
it's like so abstract relative to what
you're actually doing in the world.

265
00:15:59,330 --> 00:16:04,330
And I think that's where credit assignment
becomes just completely beyond what

266
00:16:05,031 --> 00:16:06,650
any current RL algorithm can do.

267
00:16:06,740 --> 00:16:11,420
And we need hierarchical reasoning at a
level that is just not available at all

268
00:16:11,421 --> 00:16:16,160
yet. Where do you think we can pick up
hierarchal reasoning? By which mechanisms?

269
00:16:16,940 --> 00:16:19,010
Yeah,
so maybe let me highlight for that.

270
00:16:19,070 --> 00:16:24,070
I think the limitations are of what
already was done 2030 years ago.

271
00:16:26,060 --> 00:16:30,950
In fact, you'll find reasoning systems
that reason over relatively long horizons,

272
00:16:30,951 --> 00:16:33,650
but the problem is that that we're
not grounded in the real world.

273
00:16:34,160 --> 00:16:38,390
So people would have to hand design,
uh,

274
00:16:39,110 --> 00:16:44,110
some kind of logical
dynamical descriptions of the
world and that didn't tie

275
00:16:45,051 --> 00:16:49,690
into perception and so then tie into
real objects and so forth. And so that,

276
00:16:49,720 --> 00:16:52,070
that was, that was a big
gap. Now with deep learning,

277
00:16:52,310 --> 00:16:57,310
we start having the ability to really
see with sensors process that and

278
00:16:59,901 --> 00:17:01,190
understand what's in the world.

279
00:17:01,460 --> 00:17:04,880
And so it's a good time to try to
bring these things together. One,

280
00:17:04,910 --> 00:17:06,470
I see a few ways of getting there.

281
00:17:06,471 --> 00:17:10,340
One way to get there would be to say deep
learning can get bolted on somehow to

282
00:17:10,341 --> 00:17:11,990
some of these more traditional approaches.

283
00:17:12,290 --> 00:17:15,470
Now bolted on would probably mean you
need to do some kind of end to end

284
00:17:15,471 --> 00:17:17,030
training where you say,

285
00:17:17,330 --> 00:17:22,330
my deep learning processing somehow leads
to a representation that in term uses

286
00:17:23,211 --> 00:17:26,920
some kind of traditional
underlying dynamical, uh,

287
00:17:26,930 --> 00:17:30,650
systems that can be used for
planning. And that's, for example,

288
00:17:30,651 --> 00:17:31,840
the direction of Eve.

289
00:17:31,841 --> 00:17:35,270
Tamara and the North Korea attach here
have been pushing with causal Infigen and

290
00:17:35,271 --> 00:17:40,271
of course other people too that that's
one way can we somehow force it into the

291
00:17:40,491 --> 00:17:43,610
form factor that is amenable to reasoning.

292
00:17:45,270 --> 00:17:49,410
Another direction we've been thinking
about for a long time and didn't any

293
00:17:49,411 --> 00:17:52,890
progress on was more information
theoretic approaches.

294
00:17:53,610 --> 00:17:58,610
So the idea there was that what it means
to take high level action is to taken

295
00:18:00,570 --> 00:18:01,920
choose a latent variable.

296
00:18:01,920 --> 00:18:05,580
Now that tells you a lot about what's
going to be the case in the future because

297
00:18:05,581 --> 00:18:10,050
that's what it means to, to take
a high level auction. I say, okay,

298
00:18:10,860 --> 00:18:13,830
what do I decide I'm going to
navigate to the gas station?

299
00:18:13,831 --> 00:18:15,480
Cause you need to get gas for my car.

300
00:18:15,481 --> 00:18:19,290
Well that will not take five minutes to
get there. But the fact that I get there,

301
00:18:19,291 --> 00:18:24,230
I could already tell that from the high
level action at took much earlier, um,

302
00:18:24,480 --> 00:18:27,780
that we had a very hard
time getting success with.

303
00:18:28,140 --> 00:18:30,630
I'm not saying it's a dead end,
the silly,

304
00:18:30,631 --> 00:18:32,430
but we had a lot of trouble
getting that to work.

305
00:18:33,120 --> 00:18:36,330
And then we start revisiting the notion
of what are we really trying to achieve.

306
00:18:37,040 --> 00:18:41,100
Um, what we're trying to achieve is
not necessarily how he perceived.

307
00:18:41,101 --> 00:18:43,930
We could think about what
this hierarchy give us. Um,

308
00:18:44,250 --> 00:18:48,190
what it's we hope it would give us
this better credit assignment. Um,

309
00:18:48,850 --> 00:18:51,840
Kinda what is better criticized is given,
is giving us,

310
00:18:51,841 --> 00:18:55,320
it gives us faster learning,
right?

311
00:18:55,830 --> 00:18:59,520
And so fast to learning is
ultimate maybe what we're after.

312
00:18:59,790 --> 00:19:03,990
And so that's what we ended up with
the RL squared paper on learning to

313
00:19:03,991 --> 00:19:08,070
reinforcement learning, which
had a time rocky Dwan led. Um,

314
00:19:08,820 --> 00:19:11,790
and that's exactly the metal
learning approach where we say, okay,

315
00:19:11,820 --> 00:19:15,540
we don't know how to design hierarchy.
We know what we want to get from it.

316
00:19:15,720 --> 00:19:20,150
Let's just end to an optimize for one to
get from it and see if it might emerge.

317
00:19:20,151 --> 00:19:21,150
And we saw things emerged,

318
00:19:21,180 --> 00:19:24,930
the amaze navigation had
consistent motion down hallways,

319
00:19:26,070 --> 00:19:28,260
which is what you want a
hierarchical control should say,

320
00:19:28,261 --> 00:19:31,590
I want to go down this hallway. And then
when there is an option to take a turn,

321
00:19:31,591 --> 00:19:33,000
I can decide whether to
take a turn in naught.

322
00:19:33,030 --> 00:19:36,900
And repeat even had the notion of
where have you been before or not,

323
00:19:37,250 --> 00:19:39,900
do not revisit places you've been before.
Um,

324
00:19:39,901 --> 00:19:44,901
it still didn't scale yet to the real
world kind of scenarios I think you had in

325
00:19:45,631 --> 00:19:45,991
mind,

326
00:19:45,991 --> 00:19:49,530
but it was some sign of life that maybe
you can metal learn these hierarchal

327
00:19:49,531 --> 00:19:50,364
concepts.

328
00:19:51,150 --> 00:19:55,990
I mean, it seems like, uh, through
these metal learning concepts,

329
00:19:56,140 --> 00:19:57,070
get at the,

330
00:19:57,100 --> 00:20:02,100
what I think is one of the hardest
and most important problems of AI,

331
00:20:02,321 --> 00:20:05,320
which is transfer learning.
So this generalization,

332
00:20:06,250 --> 00:20:11,250
how far along this journey
towards building general
systems are we being able to

333
00:20:11,681 --> 00:20:13,000
do transfer learning?
Well,

334
00:20:13,570 --> 00:20:16,930
so there's some signs that you
can generalize a little bit,

335
00:20:17,500 --> 00:20:22,500
but do you think we're on the right path
or s totally different breakthroughs

336
00:20:22,901 --> 00:20:27,901
and needed to be able to
transfer knowledge between
different learned models?

337
00:20:31,220 --> 00:20:33,750
Yeah,
I'm pretty tired on this and that.

338
00:20:33,800 --> 00:20:37,350
I think there are some very
of the day they're there.

339
00:20:37,420 --> 00:20:40,970
There's just some very impressive
results already, right? I mean, yes.

340
00:20:41,180 --> 00:20:45,080
I would say when even with the initial,

341
00:20:45,440 --> 00:20:49,390
kind of a big breakthrough in 2000
with Alex snap, right. Did initial,

342
00:20:49,420 --> 00:20:53,620
the initial thing is, okay great,
this does better on image net,

343
00:20:53,621 --> 00:20:54,940
the hands image recognition,

344
00:20:55,630 --> 00:21:00,630
but then immediately thereafter there
was of course the notion that wow,

345
00:21:01,450 --> 00:21:04,930
what was learned on image net and
you and I want to solve a new task,

346
00:21:04,960 --> 00:21:09,960
you can fine tune Alex net for new tasks
and that was often found to be the even

347
00:21:11,411 --> 00:21:14,020
bigger deal that you learned
something that was reusable,

348
00:21:14,290 --> 00:21:16,030
which was not often the case before.

349
00:21:16,031 --> 00:21:19,060
Usually machine learning you learned
something for one scenario and that was it

350
00:21:19,300 --> 00:21:22,240
and that's really exciting.
I mean that's a huge application.

351
00:21:22,270 --> 00:21:25,930
That's probably the biggest success of
transfer learning to date in terms of

352
00:21:26,230 --> 00:21:29,050
scope and impact.
That was a huge breakthrough.

353
00:21:29,051 --> 00:21:32,260
And then recently I feel like similar

354
00:21:33,760 --> 00:21:36,010
kind of by scaling things up,

355
00:21:36,040 --> 00:21:38,770
it seems like this has been
expanded upon like people training,

356
00:21:38,771 --> 00:21:42,280
given bigger networks they might transfer
even better if you looked out for

357
00:21:42,281 --> 00:21:43,114
example,

358
00:21:43,150 --> 00:21:46,930
some of the opening I results on language
models and some of the recent Google

359
00:21:46,931 --> 00:21:51,931
results on language models they are learn
for just prediction and then they get

360
00:21:54,430 --> 00:21:56,530
reused for other tasks.

361
00:21:56,920 --> 00:22:00,100
And so I think there is something there
where somehow if you train a big enough

362
00:22:00,101 --> 00:22:05,101
model and enough things it seems to
transfer some deep mine results that I

363
00:22:05,171 --> 00:22:08,730
thought were very impressive
to unreal results where um,

364
00:22:08,770 --> 00:22:12,970
it was learning to navigate mazes
in ways where it wasn't just doing

365
00:22:12,971 --> 00:22:16,810
reinforcement learning but it had
other objectives it was optimizing for.

366
00:22:16,820 --> 00:22:19,210
So I think there's a lot of
interesting results already.

367
00:22:19,780 --> 00:22:24,780
I think maybe where it's hard to wrap
my head around this to which extent l or

368
00:22:26,531 --> 00:22:29,530
when they would call something
generalization, right?

369
00:22:29,580 --> 00:22:33,850
Or the levels of generalization
involved in this different tasks.

370
00:22:34,270 --> 00:22:37,240
All right,
so you draw this by the way,

371
00:22:37,241 --> 00:22:41,530
just to frame things I've heard you
say somewhere is the difference between

372
00:22:41,531 --> 00:22:43,690
learning to master versus learning.

373
00:22:43,691 --> 00:22:48,490
To generalize that it's a nice
line to think about and it gets,

374
00:22:48,520 --> 00:22:53,170
you're saying there's a gray area of
what learning to master and learning to

375
00:22:53,171 --> 00:22:55,990
generalize where one starts.
And the might have heard this,

376
00:22:56,080 --> 00:22:57,760
I might never heard it somewhere else.

377
00:22:57,761 --> 00:23:00,880
And I think it might've been one of one
of your interviews and maybe the one

378
00:23:00,881 --> 00:23:03,250
with Yosha Benjamin,
I'm not a hundred percent sure,

379
00:23:03,670 --> 00:23:08,410
but I liked the example and I'm to
not sure who it was.

380
00:23:08,410 --> 00:23:13,240
But the example was essentially if you
use current deep learning techniques,

381
00:23:13,270 --> 00:23:18,090
what we're doing to predict, um,
let's say they're out of motion of,

382
00:23:18,160 --> 00:23:22,250
of um, of our planets, it
would do pretty well. Um,

383
00:23:22,630 --> 00:23:27,630
but then now if a massive new
mass enters our solar system,

384
00:23:28,420 --> 00:23:31,750
it would proud not predict
what will happen. Right.

385
00:23:32,110 --> 00:23:34,480
And that's a different kind
of channelization that's
a generalization that

386
00:23:34,481 --> 00:23:37,090
relies on the ultimate,
simplest,

387
00:23:37,091 --> 00:23:41,230
simplest explanation that we have
available today to explain the motion of

388
00:23:41,231 --> 00:23:44,440
planets where I was just pattern
recognition could predict our current soil

389
00:23:44,441 --> 00:23:47,000
system motion. Well, no problem.

390
00:23:47,330 --> 00:23:52,070
And so I think that's an example of a
kind of generalization that is a little

391
00:23:52,071 --> 00:23:54,200
different from what we've achieved so far.

392
00:23:54,560 --> 00:23:58,450
And it's not clear if just,
you know,

393
00:23:58,760 --> 00:24:02,240
regularizing more enforcing it to come
up with a simpler, simpler, simpler,

394
00:24:03,200 --> 00:24:05,990
simpler. But that's what physics
researchers do, right? They say,

395
00:24:06,470 --> 00:24:09,410
can I make this even simpler?
How simple can I get this?

396
00:24:09,411 --> 00:24:12,050
What's the simplest equation that
can explain everything? All right.

397
00:24:12,360 --> 00:24:15,140
The master equation for the
entire dynamics of the universe.

398
00:24:15,530 --> 00:24:19,310
We haven't really pushed that direction
as hard. And in the planning I would say,

399
00:24:19,730 --> 00:24:22,010
I'm not sure if it should be pushed,

400
00:24:22,011 --> 00:24:24,560
but it seems a kind of general
history you'd get from that,

401
00:24:24,561 --> 00:24:27,080
that you don't get in our
current methods so far.

402
00:24:27,290 --> 00:24:30,020
So I just talked to Vladimir
vap. Nick, for example,

403
00:24:30,050 --> 00:24:32,770
who was a statistician,

404
00:24:32,771 --> 00:24:37,771
a statistical learning and he kind of
dreams of creating these are the a equals

405
00:24:38,270 --> 00:24:42,260
e equals mc squared for a learning.
Right? The general theory of learning.

406
00:24:42,440 --> 00:24:44,570
Do you think that's a fruitless pursuit

407
00:24:46,520 --> 00:24:50,810
in a near term in within
the next several decades?

408
00:24:51,770 --> 00:24:54,420
I think that's a really
interesting pursuit. And uh,

409
00:24:54,421 --> 00:24:58,400
and in the following sense in that
there is a lot of evidence that

410
00:25:01,340 --> 00:25:05,510
the brain is pretty modular. And so I
wouldn't maybe think of it as the theory,

411
00:25:05,511 --> 00:25:07,040
maybe the underlying theory,

412
00:25:07,041 --> 00:25:12,041
but more kind of the principal
were there have been findings where

413
00:25:14,280 --> 00:25:19,280
people who are blind we'll use the part
of the brand usually used for vision for

414
00:25:19,531 --> 00:25:24,050
other functions. And even after, uh,

415
00:25:24,080 --> 00:25:26,650
some kind of, if people get
rewired in some way they might,

416
00:25:26,680 --> 00:25:29,570
I'm able to reuse parts of
their brand for other functions.

417
00:25:30,380 --> 00:25:35,380
And so what that suggests is some kind
of modularity and I think it is a pretty

418
00:25:37,971 --> 00:25:41,690
natural thing to strive for to
see can we find that modularity?

419
00:25:41,691 --> 00:25:44,300
Can we find this thing?
Of course it's not every,

420
00:25:44,301 --> 00:25:45,890
every part of the brain
is not exactly the same.

421
00:25:45,920 --> 00:25:48,080
Not Everything can be rewired arbitrarily.

422
00:25:48,530 --> 00:25:50,200
But if you think of
things like the Neocortex,

423
00:25:50,210 --> 00:25:54,920
which a pretty big part of the brain
that seems fairly modular from what the

424
00:25:54,921 --> 00:25:55,940
findings so far,

425
00:25:56,480 --> 00:26:00,860
can you design something equally modular
and if you can just grow it becomes

426
00:26:00,861 --> 00:26:01,970
more capable probably.

427
00:26:02,450 --> 00:26:06,440
I think that would be the
kind of interesting underlying
principle to shoot for

428
00:26:06,441 --> 00:26:10,130
that is not unrealistic.
Do you think

429
00:26:10,890 --> 00:26:15,890
you prefer math or empirical trial and
error for the discovery of the essence of

430
00:26:16,981 --> 00:26:20,670
what it means to do something intelligent.
So reinforcement learning and bodies,

431
00:26:20,671 --> 00:26:24,720
both groups, right? The prove
that something converges,

432
00:26:24,990 --> 00:26:29,250
prove the bounds. And then at the same
time a lot of those successes that are,

433
00:26:29,310 --> 00:26:33,380
well let's try this and see if it works.
So which do you gravitate towards?

434
00:26:33,390 --> 00:26:35,910
How do you think of those
two parts of your brain?

435
00:26:36,840 --> 00:26:37,673
So

436
00:26:39,870 --> 00:26:44,870
maybe I would prefer we can
make progress with mathematics.

437
00:26:45,570 --> 00:26:49,140
And the reason maybe I would prefer that
is because often if you have something

438
00:26:49,141 --> 00:26:52,260
you can mathematically formalize,

439
00:26:52,770 --> 00:26:57,690
you can leap frog a lot of experimentation
and experimentation takes a long time

440
00:26:57,691 --> 00:27:02,010
to get through and a lot of trial
and error kind of reinforcement,

441
00:27:02,011 --> 00:27:04,090
letting your research process.
Um,

442
00:27:04,110 --> 00:27:06,690
but you need to do a lot of trial and
error before you get to a success.

443
00:27:06,691 --> 00:27:09,930
So if we can leap out at, to my
mind, that's what the math is about.

444
00:27:10,440 --> 00:27:14,430
And hopefully once you do a bunch of
experiments, you start seeing a pattern.

445
00:27:14,431 --> 00:27:18,960
You can do some derivations that leapfrog
some experiments. But I agree with you.

446
00:27:18,961 --> 00:27:22,230
I mean in practice a lot of the progress
has been such that we have not been

447
00:27:22,231 --> 00:27:27,180
able to find the math that allows us to
leapfrog ahead and we are kind of making

448
00:27:27,181 --> 00:27:28,080
gradual progress,

449
00:27:28,081 --> 00:27:31,650
wants to out at time a new experiment
here in new experiment there that gives us

450
00:27:31,651 --> 00:27:36,240
new insights and gradually building up
but not getting to something yet where

451
00:27:36,241 --> 00:27:39,480
we're just, okay, here is some equation
and now explains how, you know,

452
00:27:39,960 --> 00:27:42,510
that would be have been two years
of experimentation to get there.

453
00:27:42,510 --> 00:27:45,000
But this tells us what
the result's going to be.

454
00:27:45,390 --> 00:27:49,470
I'm unfortunately not so much as not
so much yet, but your hope is there

455
00:27:50,170 --> 00:27:51,930
in trying to teach uh,

456
00:27:52,000 --> 00:27:57,000
robots or systems to do every
day task or even in simulation,

457
00:27:59,010 --> 00:28:03,490
what, what do you think you're
more excited about imitation,

458
00:28:03,491 --> 00:28:04,790
learning or self play.

459
00:28:04,791 --> 00:28:09,791
So letting robots learn from humans or
letting robots plan their own to try to

460
00:28:11,741 --> 00:28:15,910
figure out and then their
own way and eventually play,

461
00:28:16,670 --> 00:28:19,840
eventually interact with humans
or solve whatever problem is.

462
00:28:20,170 --> 00:28:21,730
What's the more exciting to you?

463
00:28:21,880 --> 00:28:26,440
What's more promising you think
as a research direction. So

464
00:28:27,930 --> 00:28:31,560
when, when we look at self play,

465
00:28:32,280 --> 00:28:36,510
what's so beautiful about it is because
back to kind of the challenges and

466
00:28:36,511 --> 00:28:38,460
reinforcement learning. So the
challenge for me and forced planting,

467
00:28:38,461 --> 00:28:42,000
it's getting signal.
And if you don't never succeed,

468
00:28:42,030 --> 00:28:46,590
you don't get any signal in self play.
You're on both sides.

469
00:28:46,710 --> 00:28:49,680
So one of you succeeds and the
beauty is also one of you fails.

470
00:28:49,980 --> 00:28:51,090
And so you see the contrast,

471
00:28:51,091 --> 00:28:54,000
you see the one version of me that
it better than the other version.

472
00:28:54,001 --> 00:28:56,880
And so every time you play
yourself you get signal.

473
00:28:57,240 --> 00:28:59,760
And so whenever you can turn
something into self play,

474
00:29:00,090 --> 00:29:04,800
you're in a beautiful situation where
you can naturally learn much more quickly

475
00:29:04,801 --> 00:29:08,460
than in most other reinforced
wanting environments. So I think,

476
00:29:09,120 --> 00:29:14,120
I think if somehow we can turn more
reinforcement learning problems into self

477
00:29:14,431 --> 00:29:17,610
play formulations, that would
go really, really far. So far.

478
00:29:17,611 --> 00:29:22,470
South play has been largely around games
where the risk Nashville opponents,

479
00:29:22,800 --> 00:29:25,110
but if we could do self play
for other things and let's say,

480
00:29:25,111 --> 00:29:26,910
and I know a robot
learns to build a house,

481
00:29:26,940 --> 00:29:29,490
I mean that's a pretty advanced
thing to try to do for our robot,

482
00:29:29,491 --> 00:29:31,620
but maybe tries to build
a hut or something.

483
00:29:31,860 --> 00:29:34,110
If that can be done through self play,

484
00:29:34,140 --> 00:29:36,480
it would learn a lot more quickly
if somebody can figure it out.

485
00:29:36,481 --> 00:29:40,020
And I think that would be something
where it goes closer to kind of the

486
00:29:40,050 --> 00:29:44,170
mathematical leapfrogging where somebody
figures out a formalism to say, okay,

487
00:29:44,290 --> 00:29:46,960
any RL problem by playing
this and this idea,

488
00:29:47,170 --> 00:29:50,560
you can turn it into self play problem
where you get signal a lot more easily.

489
00:29:52,540 --> 00:29:55,750
Reality is many problems who don't
know how to turn to self play.

490
00:29:55,900 --> 00:30:00,190
And so either we need to provide detailed
reward that doesn't just reward for

491
00:30:00,191 --> 00:30:02,800
achieving a goal,
but rewards for making progress.

492
00:30:02,800 --> 00:30:05,890
And that becomes time consuming.
And once you're starting to do that,

493
00:30:05,920 --> 00:30:07,090
let's say you want a
robot to do something,

494
00:30:07,091 --> 00:30:09,370
you need to give all this detailed reward.
Well,

495
00:30:09,371 --> 00:30:11,090
why not just give a demonstration,
right?

496
00:30:11,230 --> 00:30:16,210
Because why not just show the robot and
now the question is how do you show the

497
00:30:16,211 --> 00:30:16,511
robot?

498
00:30:16,511 --> 00:30:19,750
One way to show is to teleoperate to robot
and then the robot really experienced

499
00:30:19,751 --> 00:30:23,890
this things and that's nice cause that's
really high signal to noise ratio there.

500
00:30:23,891 --> 00:30:27,910
Then we've done a lot of that and you
teach her about skills in just 10 minutes.

501
00:30:27,911 --> 00:30:30,140
You can teach a robot a
new basic skill like okay,

502
00:30:30,170 --> 00:30:31,810
pick up the bottle and
place it somewhere else.

503
00:30:32,200 --> 00:30:34,240
That's a skill no matter
where the bottle starts,

504
00:30:34,241 --> 00:30:38,230
maybe it always goes up to a target or
something that's fairly used to teach her

505
00:30:38,231 --> 00:30:39,100
about with telly off.

506
00:30:39,940 --> 00:30:43,810
Now what's even more interesting if you
can now teach her about through third

507
00:30:43,811 --> 00:30:48,640
person learning where the robot watches
you do something and doesn't experience

508
00:30:48,641 --> 00:30:50,320
it but just watches it and says,
okay,

509
00:30:50,321 --> 00:30:54,250
well if you're showing me that that
means I should be doing this and I'm not

510
00:30:54,251 --> 00:30:57,220
going to be using your hand because I
don't get to control your hand but I'm

511
00:30:57,221 --> 00:30:59,020
going to use my hand.
I'd do that mapping.

512
00:30:59,500 --> 00:31:03,160
And so that's where I think one of the
big breakthroughs has happened this year.

513
00:31:03,280 --> 00:31:06,010
This was led by a Chelsea Finn here.
Um,

514
00:31:06,400 --> 00:31:10,210
it's almost like learning and machine
translation for demonstrations where you

515
00:31:10,211 --> 00:31:14,320
have a human demonstration and the robot
learns to translate it into what it

516
00:31:14,321 --> 00:31:15,550
means for the robot to do it.

517
00:31:15,820 --> 00:31:19,990
And that was a metal learning from English
and learn from one to get the other.

518
00:31:20,320 --> 00:31:24,130
Um, and that I think opens up a lot of
opportunities to learn a lot more quickly.

519
00:31:24,460 --> 00:31:26,530
So my focus is in autonomous vehicles.

520
00:31:26,531 --> 00:31:29,810
Do you think this approach of
third person watching is a Ma the,

521
00:31:29,890 --> 00:31:33,130
the autonomous driving is
amenable to this kind of approach?

522
00:31:33,790 --> 00:31:36,550
So for autonomous driving,

523
00:31:36,640 --> 00:31:41,080
I would say it's their
person is slightly easier.

524
00:31:41,530 --> 00:31:44,950
And the reason I'm going to say slightly
easier to do with third person is

525
00:31:44,951 --> 00:31:48,970
because the hard dynamics
are very well understood.

526
00:31:49,510 --> 00:31:54,510
So the easier than a first
person you mean or easier.

527
00:31:55,640 --> 00:31:59,260
So I think that distinction between third
person and first person is not a very

528
00:31:59,261 --> 00:32:01,530
important distinction for
autonomous driving. Yeah,

529
00:32:01,810 --> 00:32:06,810
they're very similar because the
distinction is really about who turns the

530
00:32:07,151 --> 00:32:11,800
steering wheel or maybe I'll,
let me put it differently.

531
00:32:12,340 --> 00:32:15,400
How it gets from a point
where you are now to a point,

532
00:32:15,401 --> 00:32:18,550
let's say a couple of meters in front
of you and that's a problem that's very

533
00:32:18,551 --> 00:32:21,430
well understood and that's the only
distinction between third and first person

534
00:32:21,431 --> 00:32:22,600
there.
Whereas with robot,

535
00:32:22,601 --> 00:32:26,200
many places and interaction forces
are very complex and it's still a very

536
00:32:26,201 --> 00:32:29,140
different thing. Um,
for autonomous driving,

537
00:32:29,920 --> 00:32:34,120
I think there is still the
question imitation versus RL.

538
00:32:34,540 --> 00:32:37,210
So imitation gives you a lot more signal.
I think we're,

539
00:32:37,211 --> 00:32:41,360
imitation is lacking and
needs some extra machinery.

540
00:32:41,361 --> 00:32:42,710
Is it

541
00:32:43,150 --> 00:32:43,490
okay?

542
00:32:43,490 --> 00:32:47,860
Doesn't and it's normal format. It
doesn't think about goals or objectives.

543
00:32:48,550 --> 00:32:51,970
And of course there are
versions of imitation,
learning, inverse reinforcement,

544
00:32:51,980 --> 00:32:54,400
any type imitational in which
I'm also thinks about goals.

545
00:32:54,640 --> 00:32:56,830
I think then we're getting much closer.

546
00:32:57,100 --> 00:33:02,100
But I think it's very hard to think
of a fully reactive har generalizing.

547
00:33:03,670 --> 00:33:07,960
Well if it really doesn't have a notion
of objectives to generalize well to the

548
00:33:07,961 --> 00:33:09,460
kind of general that you would want,

549
00:33:09,490 --> 00:33:13,330
you'd want more than just that reactivity
that you get from just behavioral

550
00:33:13,331 --> 00:33:15,280
cloning.
Slash supervise learning.

551
00:33:16,410 --> 00:33:17,090
Okay.

552
00:33:17,090 --> 00:33:18,530
So a lot of the work,

553
00:33:19,550 --> 00:33:23,300
whether it's self play or even imitation
learning would benefit significantly

554
00:33:23,301 --> 00:33:26,210
from simulation,
from effect as simulation.

555
00:33:26,540 --> 00:33:29,210
And you're doing a lot of stuff in
the physical world and in simulation.

556
00:33:29,630 --> 00:33:34,630
Do you have hope for greater
and greater power of simulation,

557
00:33:34,970 --> 00:33:39,970
Lou being boundless eventually to where
most of what we need to operate in the

558
00:33:40,851 --> 00:33:41,810
physical world,

559
00:33:42,020 --> 00:33:47,000
what could be simulated to degree that's
directly transferable to the physical

560
00:33:47,001 --> 00:33:49,250
world or are we still
very far away from that?

561
00:33:51,600 --> 00:33:52,433
So,

562
00:33:54,120 --> 00:33:54,520
okay.

563
00:33:54,520 --> 00:33:59,140
I think we could even rephrase
that question in some sense please.

564
00:34:01,370 --> 00:34:03,820
So the power of simulation,
right,

565
00:34:04,880 --> 00:34:09,660
as similar as get better and better of
course becomes stronger and we can learn

566
00:34:09,661 --> 00:34:10,550
more assimilation.

567
00:34:11,250 --> 00:34:13,580
But there's also another version
which is where you said assimilate,

568
00:34:13,620 --> 00:34:17,940
it doesn't even have to be that precise
as long as it's somewhat representative.

569
00:34:18,660 --> 00:34:22,870
And instead of trying to get one simulator
that is sufficiently precise to learn

570
00:34:22,871 --> 00:34:24,990
in and transfer really
well to the real world.

571
00:34:25,290 --> 00:34:28,230
I'm going to build many simulators,
ensemble of simulators,

572
00:34:28,231 --> 00:34:29,490
ensemble of simulators.

573
00:34:29,970 --> 00:34:34,950
Not any single one of them is sufficiently
representative of the real world such

574
00:34:34,951 --> 00:34:39,600
that it would work if you train in there.
But if you trained in all of them,

575
00:34:40,680 --> 00:34:44,880
then there was something that's good in
all of them. The real world will just be,

576
00:34:45,480 --> 00:34:49,680
you know, another one I've done, but
that's not identical to any one of them,

577
00:34:49,681 --> 00:34:53,130
but just another one of them.
Now, the sample from the
distribution of simulators,

578
00:34:53,500 --> 00:34:57,570
we do live in a simulation. So, uh,
this is just one, one other one.

579
00:34:57,750 --> 00:34:59,260
I'm not sure about that.
But yeah,

580
00:35:01,530 --> 00:35:03,800
it's definitely a very advanced
simulator of it is. Yeah.

581
00:35:04,320 --> 00:35:08,010
Pretty good one.
I've talked to us to rustle something.

582
00:35:08,030 --> 00:35:09,420
You think about it a little bit too,

583
00:35:09,421 --> 00:35:12,030
of course you're like really
trying to build these systems,

584
00:35:12,031 --> 00:35:13,770
but do you think about the future of AI?

585
00:35:13,771 --> 00:35:15,750
A lot of people have concern about safety.

586
00:35:16,380 --> 00:35:20,190
How do you think about AI safety as you
build robots that are operating in the

587
00:35:20,191 --> 00:35:23,100
physical world? What, what is, um, yeah,

588
00:35:23,101 --> 00:35:26,520
how do you approach this problem
in an engineering kind of way?

589
00:35:26,521 --> 00:35:27,540
In a systematic way.

590
00:35:28,580 --> 00:35:29,170
Okay.

591
00:35:29,170 --> 00:35:30,003
So,

592
00:35:30,680 --> 00:35:31,040
okay,

593
00:35:31,040 --> 00:35:35,990
what a robot is doing things to kind of
have a few notions of safety to worry

594
00:35:35,991 --> 00:35:36,231
about.

595
00:35:36,231 --> 00:35:41,220
One is that through about as physically
strong and of course could do a lot of

596
00:35:41,221 --> 00:35:45,420
damage. Um, same for cars, which we
can think of as robots to in some way.

597
00:35:46,740 --> 00:35:48,330
And this could be
completely unintentional,

598
00:35:48,331 --> 00:35:52,440
so it could be not the kind of
longterm AI safety concerns that, okay,

599
00:35:52,441 --> 00:35:55,860
AI is smarter than us and now what do we
do? But it could be just very practical.

600
00:35:55,861 --> 00:36:00,240
Okay. This robot, if it makes a mistake,
whether the results going to be,

601
00:36:00,660 --> 00:36:04,180
of course simulation comes in a lot
there too. Test testing, simulation.

602
00:36:06,530 --> 00:36:09,500
It's a difficult question and I'm always
wondering like, oh, there's one there.

603
00:36:09,560 --> 00:36:10,590
Let's say you look at it,

604
00:36:11,170 --> 00:36:14,200
let's go back to driving because a lot
of people know driving well of course.

605
00:36:15,200 --> 00:36:18,680
Well what do we do to test
somebody for driving right to,

606
00:36:18,690 --> 00:36:21,410
to get a driver's license?
What do they really do?

607
00:36:21,440 --> 00:36:26,440
I mean you fill out some tests and
then you'd drive and I mean we're a few

608
00:36:28,000 --> 00:36:30,940
minutes of Bourbon California.
That driving test is just,

609
00:36:30,941 --> 00:36:32,950
you drive around the block,

610
00:36:32,951 --> 00:36:37,270
pull over you to do a stop sign
successfully and then you know,

611
00:36:37,300 --> 00:36:41,110
you've pulled over again and you
pretty much done and you're like, okay,

612
00:36:41,440 --> 00:36:44,130
if a self driving car did die,

613
00:36:44,410 --> 00:36:47,560
would you trust it that it can
drive? And I'd be like, no,

614
00:36:47,561 --> 00:36:48,880
that's not enough for me to trust it.

615
00:36:48,881 --> 00:36:53,560
But somehow for humans we've figured
out that somebody being able to do that,

616
00:36:53,590 --> 00:36:57,640
it's representative of them being
able to do a lot of other things.

617
00:36:57,880 --> 00:37:02,880
And so I think somehow
for humans we figured out
representative tests of what it

618
00:37:03,491 --> 00:37:07,220
means. If you can do this where you can
really do of course testing, you must,

619
00:37:07,380 --> 00:37:09,130
you must don't want to
be tested at all times.

620
00:37:09,160 --> 00:37:11,380
Self driving cars or robots
can be tested more often.

621
00:37:11,381 --> 00:37:14,770
Probably you can have replicates that
get tested are known to be identical

622
00:37:14,771 --> 00:37:16,450
because they use the same
neural net and so forth.

623
00:37:17,140 --> 00:37:22,140
But still I feel like we don't have this
kind of unit tests or proper tests for

624
00:37:23,830 --> 00:37:24,431
for robots.

625
00:37:24,431 --> 00:37:26,560
And I think there's something very
interesting to be thought about there.

626
00:37:26,770 --> 00:37:29,590
Especially as you update things,
your software improves,

627
00:37:29,591 --> 00:37:32,050
you have a better self driving cars,
you update it.

628
00:37:32,290 --> 00:37:37,150
How do you know it's indeed more capable
on everything than what you had before

629
00:37:37,240 --> 00:37:41,020
that you didn't have any
bad things creep into it.

630
00:37:41,470 --> 00:37:44,800
So I think that's a very
interesting direction of
research that there is no real

631
00:37:44,801 --> 00:37:48,310
solution yet except that somehow
for us we do, cause we say,

632
00:37:48,311 --> 00:37:50,650
okay you have a driving test you passed,

633
00:37:50,800 --> 00:37:55,000
you can go on the road now and you must
have accents that every like million or

634
00:37:55,001 --> 00:37:55,641
10 million miles,

635
00:37:55,641 --> 00:38:00,641
some pretty phenomenal compared to
that short test that is being done.

636
00:38:01,650 --> 00:38:03,930
So let me ask a,
you've mentioned,

637
00:38:04,110 --> 00:38:09,110
you've mentioned that Andrew
Ang by example showed you
the value of kindness and

638
00:38:09,341 --> 00:38:12,340
Tim,
do you think the space of a

639
00:38:13,850 --> 00:38:14,571
policies,

640
00:38:14,571 --> 00:38:19,571
good policies for humans and for AI is
populated by policies that with kindness

641
00:38:22,530 --> 00:38:26,910
or ones that are the opposite,
exploitation,

642
00:38:26,940 --> 00:38:27,900
even evil.

643
00:38:28,200 --> 00:38:32,820
So if you just look at the Sea of policies
we operate under as human beings or

644
00:38:32,821 --> 00:38:35,040
if AI system had to
operate in this real world,

645
00:38:35,290 --> 00:38:39,730
do you think it's really easy to find
policies that are full of kindness like

646
00:38:39,731 --> 00:38:44,620
when naturally fall into them or is it
like a very hard optimization problem?

647
00:38:48,070 --> 00:38:52,660
I mean there is kind of to optimizations
happening for humans, right? So for you,

648
00:38:52,661 --> 00:38:54,760
most just kind of the very
long term optimization,

649
00:38:54,761 --> 00:38:56,830
which ever evolution has done for us,

650
00:38:56,831 --> 00:39:00,310
and we're kind of predisposed
to like certain things.

651
00:39:00,730 --> 00:39:03,400
And that's something that's what makes
our learning easier. Because I mean,

652
00:39:03,401 --> 00:39:06,690
we know things like pain and,
uh,

653
00:39:06,720 --> 00:39:11,200
hunger and thirst and the fact that we
know about those as not something that we

654
00:39:11,201 --> 00:39:13,300
were taught that's kind of
innate when we're hungry,

655
00:39:13,301 --> 00:39:18,050
we're unhappy when we're thirsty, we're
unhappy, won't have Penn, were unhappy.

656
00:39:18,380 --> 00:39:22,510
And ultimately evolution built that
into us to think about this thing.

657
00:39:22,520 --> 00:39:27,520
So I think there is a notion that it
seems somehow humans evolved in general to

658
00:39:29,600 --> 00:39:32,420
prefer to get along and in some ways,

659
00:39:32,780 --> 00:39:37,780
but at the same time also
to be very territorial and
can of centric to their own

660
00:39:39,351 --> 00:39:41,860
tribe.
Is it like,

661
00:39:41,890 --> 00:39:44,990
it seems like that's the kind of
space we conversed onto it. I mean,

662
00:39:45,120 --> 00:39:46,620
I'm not an expert in anthropology,

663
00:39:46,640 --> 00:39:50,300
but it seems like we're very kind
of good within our own tribe,

664
00:39:50,301 --> 00:39:54,560
but need to be taught to
be nice to other tribes.

665
00:39:54,640 --> 00:39:59,590
Well, if you look at Steven Pinker, he
highlights is pretty nicely. And, uh, uh,

666
00:40:00,250 --> 00:40:00,700
better,

667
00:40:00,700 --> 00:40:04,780
better angels of our nature where he
talks about violence decreasing over time

668
00:40:04,930 --> 00:40:08,320
consistently. So whatever
tension, whatever teams we pick,

669
00:40:08,321 --> 00:40:13,321
it seems that the long arc of history
goes towards us getting along more and

670
00:40:13,541 --> 00:40:18,310
more. So. I hope so. Uh,
so do you think that,

671
00:40:19,800 --> 00:40:24,710
do you think is possible to
cheat, teach, Rl base robots, the,

672
00:40:24,711 --> 00:40:28,380
this kind of kindness, this kind
of ability to interact with humans,

673
00:40:28,381 --> 00:40:32,220
this kind of policy even to, let
me ask, let me ask a fun one.

674
00:40:32,850 --> 00:40:37,230
Do you think it's possible teach RL
base robot to love a human being and to

675
00:40:37,231 --> 00:40:41,340
inspire that human to love
the robot back so to like, uh,

676
00:40:41,760 --> 00:40:45,990
RL based a algorithm that
leads to a happy marriage?

677
00:40:47,100 --> 00:40:47,550
Okay,

678
00:40:47,550 --> 00:40:51,260
that's an interesting question. Maybe
I'll, I'll, I'll, I'll answer it with,

679
00:40:51,290 --> 00:40:55,320
with another question,
right? Because I mean it's,

680
00:40:55,800 --> 00:40:59,940
but I'll come back to it. So another
question I can have is, okay. I mean, how,

681
00:40:59,960 --> 00:41:04,960
how close to some people's happiness
get from interacting with just a really

682
00:41:05,431 --> 00:41:06,264
nice,

683
00:41:06,640 --> 00:41:07,110
okay

684
00:41:07,110 --> 00:41:11,310
dog. I mean, dogs, you come home,
that's what dogs do. They greet you,

685
00:41:11,340 --> 00:41:15,150
they're excited, makes you happy when
you come home to your door. It just like,

686
00:41:15,151 --> 00:41:16,470
okay,
this is exciting.

687
00:41:16,471 --> 00:41:20,210
They're always happy when I'm here and
if they don't greet you because maybe

688
00:41:20,260 --> 00:41:23,510
whatever your partner took them
on a trip or something, you,

689
00:41:23,650 --> 00:41:27,810
you might not be nearly as happy when
you get home. Right. And so the kind of,

690
00:41:28,650 --> 00:41:32,190
it seems like the level of reasoning
a dog has is, is pretty sophisticated,

691
00:41:32,191 --> 00:41:35,390
but then it's still not yet at
the level of, of human reasoning.

692
00:41:35,660 --> 00:41:38,990
And so it seems like we don't even need
to achieve human level reasoning to get

693
00:41:38,991 --> 00:41:43,940
like very strong affection with humans.
And so my thinking is why not?

694
00:41:43,970 --> 00:41:45,650
Right? Why? Why couldn't with an AI,

695
00:41:45,651 --> 00:41:50,651
couldn't we achieve the kind of level
of affection that humans feel among each

696
00:41:51,801 --> 00:41:55,910
other or with friendly
animals and so forth?

697
00:41:57,390 --> 00:42:01,070
It's a question, is it a good thing for
us or not that, that's another thing,

698
00:42:01,071 --> 00:42:05,960
right? Because I mean,
but I don't see why not.

699
00:42:05,961 --> 00:42:09,020
Why not? Um, so Eli Musk
says love is the answer.

700
00:42:09,021 --> 00:42:12,590
Maybe he should say love
is the objective function.

701
00:42:12,620 --> 00:42:16,340
And then RL is the answer.
Maybe.

702
00:42:17,800 --> 00:42:20,210
Peter, thank you so much. I don't
want to take up more of your time.

703
00:42:20,240 --> 00:42:23,450
Thank you so much for talking today.
Well, likewise. Thanks for coming. Bye.

704
00:42:23,480 --> 00:42:24,320
Great to have you visit.

