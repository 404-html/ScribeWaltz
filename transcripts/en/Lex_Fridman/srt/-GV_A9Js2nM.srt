1
00:00:00,600 --> 00:00:04,170
Welcome to course six as zero,
nine,

2
00:00:04,171 --> 00:00:09,171
nine artificial general intelligence.
We will explore the nature of 

3
00:00:09,691 --> 00:00:14,610
intelligence from as much as possible.
An engineering perspective.

4
00:00:15,630 --> 00:00:20,630
You will hear many voices.
My voice will be that of an engineer.

5
00:00:23,130 --> 00:00:28,130
Our mission is to engineer intelligence.
The mit motto is mined in hand.

6
00:00:32,580 --> 00:00:37,580
What that means is we want to explore 
the fundamental science of what makes an

7
00:00:41,131 --> 00:00:46,131
intelligent system,
the core concepts behind our 

8
00:00:46,321 --> 00:00:51,321
understanding of what is intelligence,
but we always want to ground it in the 

9
00:00:52,561 --> 00:00:57,561
creation of intelligence systems.
We always want to be in the now in today

10
00:00:59,760 --> 00:01:04,080
in understanding how today we can build 
artificial intelligence systems that can

11
00:01:04,081 --> 00:01:09,000
make for a better world that is the core
for us here at Mit,

12
00:01:10,020 --> 00:01:13,770
first and foremost,
where scientists and engineers,

13
00:01:14,280 --> 00:01:16,680
our goal is to engineer intelligence.

14
00:01:19,710 --> 00:01:24,710
We want to provide with this approach a 
balanced to the very important but 

15
00:01:28,110 --> 00:01:31,740
overrepresented view of artificial 
general intelligence.

16
00:01:32,400 --> 00:01:37,200
That black box reasoning view,
where the idea is,

17
00:01:37,201 --> 00:01:41,580
once we know how to create a human level
intelligence system,

18
00:01:41,880 --> 00:01:46,880
how will society be impacted?
Will robots take over and kill everyone?

19
00:01:50,340 --> 00:01:55,340
Will we achieve a utopia that will 
remove the need to do any of the messy 

20
00:01:55,591 --> 00:01:58,560
jobs that will make us all extremely 
happy?

21
00:01:59,040 --> 00:02:03,450
Those kinds of beautiful philosophical 
concepts are interesting to explore,

22
00:02:03,451 --> 00:02:05,520
but that's not what we're interested in 
doing.

23
00:02:06,210 --> 00:02:09,660
I believe that from an engineering 
perspective,

24
00:02:09,810 --> 00:02:13,110
we want to focus on the black box of 
Agi,

25
00:02:14,040 --> 00:02:19,040
start to build insights and intuitions 
about how we create systems that 

26
00:02:19,291 --> 00:02:24,291
approach human level intelligence.
I believe we're very far away from 

27
00:02:25,741 --> 00:02:29,970
creating anything resembling human level
intelligence.

28
00:02:31,530 --> 00:02:36,530
However,
the dimension of the metric behind the 

29
00:02:37,321 --> 00:02:42,321
ward far may not be time in time,
perhaps through a few breakthroughs may 

30
00:02:48,381 --> 00:02:50,130
be even one breakthrough.

31
00:02:50,370 --> 00:02:53,670
Everything can change,
but as we stand now,

32
00:02:54,120 --> 00:02:59,120
our current methods as we will explore 
from the various ideas and approaches 

33
00:02:59,120 --> 00:03:00,750
and the guest speakers coming here,
uh,

34
00:03:00,930 --> 00:03:05,880
over the next two weeks and beyond,
our best understanding,

35
00:03:05,881 --> 00:03:10,881
our best intuition insights are not yet 
at the level of reaching without a major

36
00:03:13,991 --> 00:03:17,890
leap and breakthrough and paradigm shift
towards human level intelligence.

37
00:03:18,670 --> 00:03:23,670
So it's not constructive to consider the
impact of artificial intelligence to 

38
00:03:23,681 --> 00:03:28,681
consider questions of safety and ethics.
Fundamental,

39
00:03:30,970 --> 00:03:35,970
extremely important questions,
we it's not constructive to consider 

40
00:03:35,970 --> 00:03:40,861
those questions without also deeply 
considering the black box of the actual 

41
00:03:41,261 --> 00:03:45,190
methods of artificial intelligence,
human level,

42
00:03:45,191 --> 00:03:48,670
artificial intelligence,
and that's what I see.

43
00:03:48,820 --> 00:03:52,540
What I hope this course can be,
its first iteration,

44
00:03:52,960 --> 00:03:57,960
it's first exploratory attempt to try to
look at different approaches of how we 

45
00:03:59,921 --> 00:04:04,921
can engineer intelligence.
That's the role of mit is tradition of 

46
00:04:04,921 --> 00:04:07,480
mine in hand is to consider the big 
picture,

47
00:04:07,630 --> 00:04:10,030
the future impact of society 10,
20,

48
00:04:10,031 --> 00:04:11,470
30,
40 years out,

49
00:04:11,770 --> 00:04:16,770
but fundamentally grounded in what kind 
of methods do we have today and what are

50
00:04:17,741 --> 00:04:21,700
the limitations and possibilities of 
achieving that.

51
00:04:21,760 --> 00:04:26,760
The black box of Agi and in the future 
impact on society of creating artificial

52
00:04:31,451 --> 00:04:35,740
intelligence systems that get become 
increasingly more intelligent.

53
00:04:36,190 --> 00:04:41,190
The fundamental disagreement lies in the
fact the very core of that black box,

54
00:04:43,360 --> 00:04:48,160
which is how hard is it to build an Agi 
system?

55
00:04:48,520 --> 00:04:52,150
How hard is it to create a human level 
artificial intelligence system?

56
00:04:52,360 --> 00:04:55,120
That's the open question for all of us,
from,

57
00:04:55,480 --> 00:05:00,480
from Josh Tenenbaum to Andrea carpathy 
to folks some open ai to Boston 

58
00:05:03,371 --> 00:05:08,371
dynamics,
the brilliant leaders in various fields 

59
00:05:08,371 --> 00:05:09,370
of artificial intelligence that will 
come here.

60
00:05:09,371 --> 00:05:11,620
That's the open question.
How hard is.

61
00:05:11,621 --> 00:05:16,621
It has been a lot of incredibly 
impressive results in deep learning in 

62
00:05:17,291 --> 00:05:22,291
neuroscience and computational cognitive
science in robotics,

63
00:05:23,620 --> 00:05:28,210
but how far are we still to go to the 
Agi?

64
00:05:28,211 --> 00:05:33,211
That's the fundamental question that we 
need to explore before we consider the 

65
00:05:33,610 --> 00:05:36,340
questions,
the future impact on society.

66
00:05:38,550 --> 00:05:41,610
And the goal for this class is to build 
intuition.

67
00:05:41,650 --> 00:05:42,810
One,
talk at a time,

68
00:05:43,320 --> 00:05:48,320
a project at a time build intuition 
about where we stand,

69
00:05:48,750 --> 00:05:51,180
about what the limitations of current 
approaches are.

70
00:05:51,240 --> 00:05:52,680
How can we close the gap?

71
00:05:55,140 --> 00:06:00,140
A Nice meme that I caught on twitter 
recently of a,

72
00:06:01,480 --> 00:06:06,480
the difference between the engineering 
approach at the very simplest of a 

73
00:06:07,460 --> 00:06:12,460
google intern typing a for loop that 
just does a grid search on parameters 

74
00:06:12,460 --> 00:06:14,660
for neural network.
Uh,

75
00:06:14,780 --> 00:06:19,780
and on the right is the way media would 
report this for loop the Google ai 

76
00:06:21,711 --> 00:06:26,711
created its own baby ai.
I think it's easy for us to go one way 

77
00:06:31,191 --> 00:06:33,890
or the other,
but we'd like to do both.

78
00:06:34,490 --> 00:06:39,490
Our first goal is to avoid the pitfalls 
of black box thinking of the futurism 

79
00:06:39,621 --> 00:06:44,621
thinking that results in hype,
that's detached from scientific 

80
00:06:44,621 --> 00:06:46,550
engineering,
understanding of what the actual systems

81
00:06:46,560 --> 00:06:50,120
are doing.
That's what the media often reports.

82
00:06:50,300 --> 00:06:55,300
That's what some of our speakers will 
explore in a rigorous way.

83
00:06:57,231 --> 00:06:59,600
It's still an important topic to 
explore.

84
00:06:59,780 --> 00:07:02,310
Ray Kurzweil on Wednesday.
We'll look at.

85
00:07:02,450 --> 00:07:07,450
We'll explore this topic next week 
talking about Ai Safety and autonomous 

86
00:07:07,450 --> 00:07:08,270
weapons systems.

87
00:07:08,270 --> 00:07:11,000
We'll explore this topic that future 
impact 10,

88
00:07:11,001 --> 00:07:16,001
20 years out.
How do we design systems today that 

89
00:07:16,001 --> 00:07:17,690
would lead to safe systems tomorrow.
Still very important,

90
00:07:17,900 --> 00:07:22,790
but the reality is a lot of us need to 
put a lot more emphasis on the left,

91
00:07:23,000 --> 00:07:28,000
on the for loops,
on creating these systems at the same 

92
00:07:28,000 --> 00:07:31,901
time.
The second goal of what we're trying to 

93
00:07:31,901 --> 00:07:34,100
do here is not emphasize the silliness,
the simplicity,

94
00:07:34,101 --> 00:07:39,101
the naive basic nature of this four loop
and the same way as was the process in 

95
00:07:41,421 --> 00:07:46,340
creating nuclear weapons before,
during World War Two.

96
00:07:48,170 --> 00:07:50,630
The idea that as an engineer,
as a scientist,

97
00:07:50,631 --> 00:07:55,580
that I am just the scientist is also a 
flawed way of thinking.

98
00:07:55,910 --> 00:08:00,910
We have to consider the big picture 
impact the near term negative 

99
00:08:00,910 --> 00:08:03,650
consequences that are preventable,
the low hanging fruit,

100
00:08:03,651 --> 00:08:07,010
the that can be prevented through that 
engineering process.

101
00:08:07,400 --> 00:08:12,400
We have to do both and in this 
engineering approach we always have to 

102
00:08:15,801 --> 00:08:20,801
be cautious that just because we don't 
understand,

103
00:08:21,650 --> 00:08:26,650
we're just because we our intuition,
our best understanding of the 

104
00:08:27,231 --> 00:08:32,210
capabilities of modern systems that 
learn to act in this world seem limited,

105
00:08:32,211 --> 00:08:34,340
seem far from human level intelligence.

106
00:08:34,490 --> 00:08:39,020
Our ability to learn and represent 
common sense reasoning seems limited.

107
00:08:39,830 --> 00:08:44,570
The exponential potentially exponential.
It's could be argued and he will.

108
00:08:45,020 --> 00:08:50,020
A growth of technology of these ideas 
means that just around the corner is a 

109
00:08:51,051 --> 00:08:56,051
singularity is a breakthrough idea that 
will change everything.

110
00:08:57,390 --> 00:09:00,330
We have to be cautious of that.
Moreover,

111
00:09:00,990 --> 00:09:05,990
we have to be cautious of the fact that 
every decade over the past century,

112
00:09:07,590 --> 00:09:11,550
our adoption of new technologies has 
gotten faster and faster.

113
00:09:11,580 --> 00:09:16,580
The rate at which a new technology from 
its birth to its wide mass adoption has 

114
00:09:19,381 --> 00:09:24,381
shortened and shortened and shortened.
That means that new idea,

115
00:09:26,040 --> 00:09:31,040
the moment it drops into the world can 
have widespread effects overnight,

116
00:09:33,180 --> 00:09:38,180
so as an I think the in in the 
engineering approach is fundamentally 

117
00:09:38,180 --> 00:09:42,561
cynical on artificial general 
intelligence because every aspect of it 

118
00:09:42,561 --> 00:09:46,561
is so difficult.
We have to always remember that 

119
00:09:46,561 --> 00:09:49,131
overnight.
Everything can change through this 

120
00:09:49,131 --> 00:09:53,010
question of beginning to approach from a
deep learning perspective,

121
00:09:53,011 --> 00:09:55,580
from deeper enforcement learning from 
brain simulation,

122
00:09:55,730 --> 00:10:00,600
complex cognitive science from 
computational neuroscience,

123
00:10:00,601 --> 00:10:05,601
from cognitive architecture,
some robotics from a legal perspective 

124
00:10:07,710 --> 00:10:09,480
and autonomous weapons systems.

125
00:10:09,870 --> 00:10:14,430
As we begin to approach these questions,
we need to start to build intuition.

126
00:10:14,910 --> 00:10:18,150
How far away are we from creating 
intelligence systems?

127
00:10:18,830 --> 00:10:23,830
The singularity here is that spark,
that moment when we're truly surprised 

128
00:10:25,830 --> 00:10:28,650
by the intelligence of the systems we 
create.

129
00:10:30,600 --> 00:10:35,600
I'd like to visualize it by the,
by the certain analogy that we're in 

130
00:10:36,451 --> 00:10:41,451
this dark room looking for a light 
switch with no knowledge of where the 

131
00:10:42,121 --> 00:10:43,170
lights,
which is.

132
00:10:43,440 --> 00:10:45,570
There's going to be people that say,
well,

133
00:10:45,780 --> 00:10:49,250
it's a small rooms are all small.
We're right there and say anywhere.

134
00:10:49,260 --> 00:10:53,550
We'll be able to find it anytime time.
The reality is we know very little so we

135
00:10:53,551 --> 00:10:58,551
have to stumble around,
feel our way around to build the 

136
00:10:58,551 --> 00:10:59,310
intuition of far,
far away.

137
00:10:59,311 --> 00:11:00,330
We really are.

138
00:11:03,810 --> 00:11:06,450
And many will.
Speakers here,

139
00:11:06,451 --> 00:11:09,330
we'll talk about how we define 
intelligence,

140
00:11:09,331 --> 00:11:14,331
how we can begin to see intelligence.
What are the fundamental impacts of 

141
00:11:14,331 --> 00:11:18,771
creating intelligence systems?
I'd like to sort of see the positive 

142
00:11:20,580 --> 00:11:25,580
reason for this little class and for 
these efforts that have fascinated 

143
00:11:26,700 --> 00:11:31,700
people throughout the century of trying 
to create intelligence systems is that 

144
00:11:31,700 --> 00:11:36,141
there is something about human beings 
that want that craze to explore,

145
00:11:39,600 --> 00:11:41,700
to uncover the mysteries of the 
universe.

146
00:11:42,660 --> 00:11:46,290
Fundamental in itself,
a desire to uncover the mysteries of the

147
00:11:46,291 --> 00:11:48,630
universe,
not for a purpose,

148
00:11:49,380 --> 00:11:51,840
and there's often an underlying purpose 
of money,

149
00:11:51,841 --> 00:11:53,520
of greed,
of,

150
00:11:53,610 --> 00:11:54,980
uh,
the,

151
00:11:54,981 --> 00:11:57,550
uh,
power craving for power and so on.

152
00:11:57,551 --> 00:12:00,880
But there's seems to be an underlying 
desire to explore.

153
00:12:01,990 --> 00:12:04,220
Nice little book,
an exploration,

154
00:12:04,240 --> 00:12:06,250
a very short introduction by Stewart 
Weaver.

155
00:12:06,251 --> 00:12:11,251
He says,
for all the different forums that takes 

156
00:12:11,251 --> 00:12:14,760
in different historical periods for for 
all the worthy and unworthy motives that

157
00:12:15,581 --> 00:12:20,581
lie behind it.
Exploration travel for the sake of 

158
00:12:21,071 --> 00:12:24,010
discovery and adventure is a human 
compulsion,

159
00:12:24,760 --> 00:12:29,760
a human obsession even.
It is defining element of a distinctly 

160
00:12:29,760 --> 00:12:33,730
human identity and it will never arrest 
at any frontier,

161
00:12:33,850 --> 00:12:38,850
whether terrestrial or extraterrestrial.
From 325 BC.

162
00:12:40,200 --> 00:12:43,880
He with a long,
7,500

163
00:12:43,881 --> 00:12:45,020
mile journey

164
00:12:46,350 --> 00:12:51,350
on the ocean to explore the Arctic,
to Christopher Columbus and his flawed 

165
00:12:55,030 --> 00:12:59,440
harshly criticize the modern 
scholarshipped trip that ultimately pave

166
00:12:59,441 --> 00:13:01,600
the way,
didn't discover,

167
00:13:02,290 --> 00:13:05,440
paved the way to colonization of the 
Americas,

168
00:13:08,660 --> 00:13:12,290
to the Darwin trip,
the voyage of the Beagle.

169
00:13:13,800 --> 00:13:17,220
Whilst this planet has gone,
cycling on according to the fixed law of

170
00:13:17,221 --> 00:13:19,830
gravity from so simple,
a beginning,

171
00:13:20,230 --> 00:13:25,230
endless forms most beautiful and most 
wonderful have been and are being 

172
00:13:25,230 --> 00:13:29,421
evolved to the first venture into space 
by Urunga,

173
00:13:34,541 --> 00:13:38,620
Guardian,
first human in space in 1961.

174
00:13:40,550 --> 00:13:44,060
What he said over the radio is,
the earth is blue.

175
00:13:44,120 --> 00:13:49,120
It is amazing this.
These are the words that I think drive 

176
00:13:50,070 --> 00:13:55,070
our exploration in the sciences and the 
engineering and today in ai from the 

177
00:13:56,611 --> 00:14:01,611
first walk on the moon and now the 
desire to colonize Mars and beyond.

178
00:14:11,270 --> 00:14:15,050
That's where I see this desire to create
intelligence systems,

179
00:14:16,430 --> 00:14:20,150
talking about the positive and negative 
impact of ai on society.

180
00:14:20,390 --> 00:14:23,030
Talking about the business case so the 
jobs lost,

181
00:14:23,060 --> 00:14:24,170
jobs gained,
jobs,

182
00:14:24,171 --> 00:14:29,171
created diseases cured the autonomous 
vehicles,

183
00:14:30,470 --> 00:14:33,800
the ethical questions,
the safety of autonomous weapons,

184
00:14:34,130 --> 00:14:36,950
of the misuse of ai in the financial 
markets.

185
00:14:37,550 --> 00:14:39,500
Underneath it all,
and there are people,

186
00:14:39,510 --> 00:14:44,510
many people have spoken about this.
What drives myself and many in the 

187
00:14:44,510 --> 00:14:48,911
community is a desire to explore,
to uncover the mystery of the universe 

188
00:14:48,911 --> 00:14:52,160
and that hope that you join me in that 
very effort with speakers that come here

189
00:14:52,161 --> 00:14:54,830
in the next two weeks and beyond.

190
00:14:57,890 --> 00:15:02,000
The website for the course is Agi that 
mit did you.

191
00:15:03,140 --> 00:15:08,140
I am a part of an amazing team,
many of whom you know Agi at Mit.

192
00:15:10,961 --> 00:15:14,540
That edu is the email.
We're on slack.

193
00:15:14,570 --> 00:15:19,570
Deep Dash Mit does slack up for 
registered at Mit students.

194
00:15:21,890 --> 00:15:26,890
It created account on the website and 
submit five new links and vote on 10 to 

195
00:15:28,251 --> 00:15:33,251
vote Ai,
which is an aggregator of information 

196
00:15:33,251 --> 00:15:36,461
and material we've put together for the 
topic of Agi and submit a entry to one 

197
00:15:39,081 --> 00:15:44,081
of the competitions,
one of the three competitions projects 

198
00:15:44,081 --> 00:15:46,991
that we have in this course and the 
projects are dream vision.

199
00:15:47,271 --> 00:15:49,580
I'll go over them in a little bit.
Dream Vision,

200
00:15:49,670 --> 00:15:54,530
Angele ethical car,
and the aggregator of material.

201
00:15:54,531 --> 00:15:56,840
Vote Ai.
We have guest speakers,

202
00:15:56,841 --> 00:16:01,841
incredible guest speakers.
I will go over them today and as before 

203
00:16:03,540 --> 00:16:06,110
with the deep learning for self driving 
cars course,

204
00:16:06,111 --> 00:16:10,100
we have shirts and they're free for in 
person,

205
00:16:10,190 --> 00:16:13,010
for people that attend in person for the
last lecture,

206
00:16:13,011 --> 00:16:15,590
most likely or you can order them 
online.

207
00:16:17,720 --> 00:16:22,720
Okay.
Dream vision will take the Google g 

208
00:16:22,720 --> 00:16:25,411
dream idea.
We explore the idea of creativity where 

209
00:16:26,690 --> 00:16:31,250
Einstein's view of intelligence,
the mark of intelligence is creativity.

210
00:16:32,330 --> 00:16:37,330
This idea is something we explore by 
using neural networks and interesting 

211
00:16:37,371 --> 00:16:42,371
ways to visualize what the network see 
and in so doing,

212
00:16:43,491 --> 00:16:47,660
create beautiful visualizations in time 
through video.

213
00:16:48,050 --> 00:16:53,050
So taking the ideas of deep dream and 
combining them together with multiple 

214
00:16:53,241 --> 00:16:57,140
video streams to mix,
dream and reality,

215
00:16:58,730 --> 00:17:03,730
and the competition is through 
mechanical Turk was set up a competition

216
00:17:05,241 --> 00:17:10,241
of who produces the most beautiful 
visualization will provide code to 

217
00:17:10,971 --> 00:17:15,971
generate this visualization and ideas of
how you can make it more and more 

218
00:17:15,971 --> 00:17:20,441
beautiful and how to submit it to the 
competition.

219
00:17:22,410 --> 00:17:27,410
Angel,
the artificial neural generator of 

220
00:17:27,410 --> 00:17:30,251
emotion and language is a different 
twist on the turing test where we don't 

221
00:17:32,841 --> 00:17:37,841
use words,
we only use emotions to speak expression

222
00:17:38,871 --> 00:17:43,530
of those emotions and we create.
We use an age a,

223
00:17:43,540 --> 00:17:44,330
a,
a,

224
00:17:44,720 --> 00:17:49,720
a face customizable with 26 muscles.
All of which can be controlled or 

225
00:17:50,340 --> 00:17:55,340
controlled with an Lstm we use in your 
network to train the generation of 

226
00:17:56,851 --> 00:17:57,630
emotion

227
00:17:59,430 --> 00:18:04,430
and the competition in you.
Submitting the code to the competition 

228
00:18:05,400 --> 00:18:10,400
is you get 10 seconds to impress with 
the these expressions of emotion,

229
00:18:12,901 --> 00:18:15,510
the viewer.
It's Ab testing.

230
00:18:15,930 --> 00:18:20,930
Your goal is to impress the viewer 
enough to where they choose your agent 

231
00:18:21,091 --> 00:18:26,091
versus another agent,
and those that are most loved agents,

232
00:18:26,521 --> 00:18:31,521
most loved will be the ones that are 
declared winners in a twist.

233
00:18:32,580 --> 00:18:37,580
We will add human beings into this mix,
so we've created a system that maps are 

234
00:18:39,181 --> 00:18:44,181
human faces,
myself and the Ta's to where we 

235
00:18:44,181 --> 00:18:49,140
ourselves enter an outcome in the 
competition and try to convince you to 

236
00:18:49,261 --> 00:18:52,710
keep us as your friend.
That's the turing test.

237
00:18:56,070 --> 00:19:01,070
Okay.
Ethical car building in the ideas of the

238
00:19:01,200 --> 00:19:04,530
trolley problem and the moral machine 
done here in the media lab.

239
00:19:04,710 --> 00:19:06,390
They incredible,
interesting work.

240
00:19:06,660 --> 00:19:10,980
We take a machine learning approach to 
it and take what we've developed,

241
00:19:11,010 --> 00:19:15,090
the deep reinforcement learning 
competition for a six zero,

242
00:19:15,091 --> 00:19:20,091
nine for the deep traffic and we add 
pedestrians into it to cast stochastic 

243
00:19:24,820 --> 00:19:28,150
irrational,
unpredictable pedestrians,

244
00:19:28,840 --> 00:19:33,840
and we add human life to the loss 
function where there's a tradeoff 

245
00:19:33,840 --> 00:19:37,490
between getting from point a to point b.
So in deep traffic,

246
00:19:37,491 --> 00:19:39,140
the deeper enforcement learning 
competition,

247
00:19:39,141 --> 00:19:44,141
the goal was to go as fast as possible.
Here it's up to you to decide what you,

248
00:19:46,070 --> 00:19:51,070
what your agents goal is.
There's a parade of front tradeoff 

249
00:19:51,080 --> 00:19:56,080
between getting from point a to point B 
as fast as possible and hurting 

250
00:19:59,630 --> 00:20:00,530
pedestrians.

251
00:20:08,940 --> 00:20:13,940
This is not a ethical question.
It's an engineering question and it's a 

252
00:20:18,901 --> 00:20:23,901
serious one because fundamentally in 
creating autonomous vehicles that 

253
00:20:23,901 --> 00:20:28,821
function in this world,
we want them to get from point a to 

254
00:20:28,821 --> 00:20:33,561
point B as quickly as possible.
The United States government insurance 

255
00:20:34,741 --> 00:20:39,741
companies put a price tag on human life.
We put that power in your hands in 

256
00:20:41,161 --> 00:20:45,690
designing these agents to ask the 
question of a,

257
00:20:45,780 --> 00:20:50,320
how can we create machine learning 
systems where the objective function,

258
00:20:50,321 --> 00:20:55,321
the loss function has human life as part
of it and vote ai is an aggregator of 

259
00:20:59,950 --> 00:21:03,400
different links,
different articles,

260
00:21:03,401 --> 00:21:08,401
papers,
videos on the topic of artificial 

261
00:21:08,401 --> 00:21:10,120
general intelligence where people vote 
on a vote,

262
00:21:10,150 --> 00:21:15,150
quality articles up and down and choose 
on the sentiment of positive and 

263
00:21:17,291 --> 00:21:22,291
negative.
We'd like to explore the different ways 

264
00:21:22,291 --> 00:21:24,010
to the different arguments for and 
against artificial general intelligence.

265
00:21:26,980 --> 00:21:31,980
There is an incredible list of speakers 
the best in their disciplines from Josh 

266
00:21:34,450 --> 00:21:35,160
Tenenbaum,
Ghana,

267
00:21:35,161 --> 00:21:38,980
mit to ray Kurzweil at Google,
to Lisa Feldman,

268
00:21:38,981 --> 00:21:43,570
Barrett and nature Bensky from 
northeastern university,

269
00:21:43,840 --> 00:21:48,840
Andre Carpathy,
Steven Wolf from Richard Moise,

270
00:21:49,210 --> 00:21:51,970
Mark Reiber,
ltss giver,

271
00:21:53,230 --> 00:21:56,140
and myself,
Josh Tenenbaum.

272
00:21:56,170 --> 00:22:01,170
Tomorrow I'd like to go through each of 
these speakers and talk about the 

273
00:22:01,331 --> 00:22:06,331
perspectives they bring that to try to 
try to see the approach,

274
00:22:06,610 --> 00:22:09,670
the ideas they bring to the table.
They're not,

275
00:22:10,270 --> 00:22:15,270
in most cases interested in the 
discussion of the future impact on 

276
00:22:16,601 --> 00:22:20,470
society without grounding it into the 
expertise,

277
00:22:20,620 --> 00:22:25,620
into the actual engineering,
into creating these intelligence 

278
00:22:25,620 --> 00:22:28,641
systems.
So Josh is a computation and cognitive 

279
00:22:28,641 --> 00:22:31,480
science expert.
Professor Faculty here at Mit.

280
00:22:31,720 --> 00:22:36,720
He will talk about how we can create 
common sense understanding systems that 

281
00:22:37,900 --> 00:22:42,900
see a world of physical objects and 
their interactions and our own 

282
00:22:42,900 --> 00:22:44,920
possibilities to act,
interact with others,

283
00:22:44,980 --> 00:22:46,060
the intuitive physics.

284
00:22:46,060 --> 00:22:50,860
How do we build into systems the 
intuitive physics of the world more than

285
00:22:50,861 --> 00:22:55,861
just the deep learning memorization 
engines that take patterns and learn 

286
00:22:56,511 --> 00:23:00,070
through supervised way to map those 
patterns to classification.

287
00:23:01,690 --> 00:23:06,690
Actually begin to understand the 
intuitive common sense physics of the 

288
00:23:06,690 --> 00:23:10,000
world and learn rapid model based 
learning.

289
00:23:10,090 --> 00:23:12,310
Learn from nothing,
learned from very little,

290
00:23:12,490 --> 00:23:17,490
just like we do as children,
just like we do as human being 

291
00:23:17,490 --> 00:23:20,451
successfully,
often only need one example to learn a 

292
00:23:20,451 --> 00:23:22,270
concept.
How do we create systems that learn from

293
00:23:22,390 --> 00:23:27,280
very few,
sometimes a single example and integrate

294
00:23:27,281 --> 00:23:29,830
ideas from various disciplines,
of course,

295
00:23:29,831 --> 00:23:33,520
from neural networks,
but also probabilistic generative models

296
00:23:33,521 --> 00:23:38,521
and symbol processing architectures.
It's gonna be incredible of course,

297
00:23:40,300 --> 00:23:42,610
from a,
from a different area of the world.

298
00:23:43,030 --> 00:23:45,530
Uh,
another incredible thinker,

299
00:23:45,890 --> 00:23:50,890
intellectual speaker is Ray Kurzweil.
He'll be here on Wednesday at 1:00

300
00:23:51,410 --> 00:23:56,410
PM and he will do a whirlwind discussion
of where we stand with intelligence,

301
00:23:59,541 --> 00:24:03,020
creating intelligence systems,
how we see natural intelligence,

302
00:24:03,021 --> 00:24:05,180
our own human intelligence,
how we define it,

303
00:24:05,600 --> 00:24:10,070
how we understand it,
and how that transfers to the increasing

304
00:24:10,071 --> 00:24:13,970
exponential growth of development,
of artificial general intelligence.

305
00:24:16,820 --> 00:24:19,820
Something I'm very excited about

306
00:24:20,660 --> 00:24:24,820
is Lisa Feldman Barrett coming here on 
Thursday.

307
00:24:25,450 --> 00:24:27,760
She's written a book,
I believe,

308
00:24:27,761 --> 00:24:32,761
how emotions are made.
He argues that emotions are created,

309
00:24:34,300 --> 00:24:39,300
that there is a distinction,
there is a detachment between what we 

310
00:24:39,300 --> 00:24:42,670
feel in our bodies,
the physical state of our bodies,

311
00:24:42,820 --> 00:24:47,820
and the expression of emotion from,
from body to the contextually grounded 

312
00:24:48,970 --> 00:24:53,970
to the face expressing that emotion,
which means now why is this a person who

313
00:24:54,941 --> 00:24:59,941
is a psychology person in a 
fundamentally engineer and computer 

314
00:24:59,941 --> 00:25:03,511
science topic like Agi?
Because if emotions are created and the 

315
00:25:03,611 --> 00:25:06,310
way she argues and she'll systematically
break it down,

316
00:25:06,610 --> 00:25:11,290
that means we're learning societal.
As human beings,

317
00:25:11,291 --> 00:25:14,020
we're learning societal norms of how to 
express emotion.

318
00:25:14,260 --> 00:25:16,330
The idea of emotional intelligence has 
learned,

319
00:25:16,540 --> 00:25:20,230
which means we can have machines learn 
this idea.

320
00:25:20,740 --> 00:25:23,140
It's a machine learn just like it's a 
human learning problem.

321
00:25:23,141 --> 00:25:27,880
It's some machine learning problem in a 
little bit of a twist.

322
00:25:28,270 --> 00:25:33,270
She asked that instead of giving a talk,
I have a conversation with her so 

323
00:25:33,791 --> 00:25:37,150
there's going to be a little bit 
challenging and fun and,

324
00:25:37,200 --> 00:25:42,200
uh,
she's great looking forward to it and 

325
00:25:42,200 --> 00:25:45,800
we'll explore different ways that we can
get emotion expressed through video,

326
00:25:47,251 --> 00:25:50,190
through audio,
through the project.

327
00:25:50,490 --> 00:25:55,490
The Angel Project that I mentioned.
So there's has been worked and 

328
00:25:55,490 --> 00:25:58,911
reenacting intelligence,
so a well reenacting mapping face to 

329
00:25:59,761 --> 00:26:03,750
face mapping different emotions on video
that was previously recorded.

330
00:26:04,440 --> 00:26:09,440
So if you can imagine that means we can 
take emotions that we've created,

331
00:26:09,930 --> 00:26:14,930
the kind of emotion creation we've been 
discussing and remap it on previous 

332
00:26:15,831 --> 00:26:20,831
video.
That's one way to see intelligence is 

333
00:26:20,831 --> 00:26:23,771
taking raw human data that we already 
have and mapping new computer generated 

334
00:26:25,360 --> 00:26:29,030
the the,
the underlying fundamentals of human,

335
00:26:29,600 --> 00:26:33,020
but the surface appearance,
the representation of emotion,

336
00:26:33,021 --> 00:26:37,970
visual or auditory is generated by 
computer.

337
00:26:40,460 --> 00:26:43,160
It could be in the embodied form like 
with Sophia,

338
00:26:52,310 --> 00:26:57,310
so different if you take a long time for
to context and jealousy and it might be 

339
00:27:07,211 --> 00:27:09,200
possible than any.
The more ethical,

340
00:27:10,330 --> 00:27:12,530
a good partnership.

341
00:27:17,860 --> 00:27:22,860
Very important to note for those 
captivated by Sophia in the press or 

342
00:27:22,860 --> 00:27:27,370
have seen these videos.
Sophia is an art exhibit.

343
00:27:28,240 --> 00:27:32,500
She's not a strong natural language 
processing system.

344
00:27:32,501 --> 00:27:37,501
This is not an agi system,
but it's a beautiful visualization of 

345
00:27:38,741 --> 00:27:43,741
embodying of.
It's a beautiful visualization of how 

346
00:27:43,741 --> 00:27:45,760
easy it is to trick us human beings.
That there is intelligence,

347
00:27:46,300 --> 00:27:50,290
an underlying something that the 
emotional expression,

348
00:27:50,590 --> 00:27:55,590
the physical embodiment and the 
emotional expression that has that has 

349
00:27:55,901 --> 00:28:00,901
some degree of humor that has some 
degree of wit and intelligence is enough

350
00:28:01,691 --> 00:28:06,691
to captivate us,
so that's an argument for not creating 

351
00:28:06,691 --> 00:28:10,210
intelligence from scratch,
but having machines at the very surface,

352
00:28:10,450 --> 00:28:13,390
the display of that emotion,
the generation,

353
00:28:13,510 --> 00:28:18,280
the mapping of the visual and the 
auditory elements were underneath.

354
00:28:18,281 --> 00:28:23,020
It is really trivial technology that's 
fundamentally relying on humans.

355
00:28:23,080 --> 00:28:27,550
Like in Sophia's case and in the 
simplest form,

356
00:28:27,670 --> 00:28:31,310
we remove all elements of shit.

357
00:28:31,720 --> 00:28:34,250
How should I say,
attractive appearance from a,

358
00:28:34,540 --> 00:28:39,540
from an agent.
We really keep it to the simplest 

359
00:28:39,540 --> 00:28:41,400
muscles aspect,
characteristics of the face and see with

360
00:28:41,401 --> 00:28:44,980
26 muscles controlled by a neural 
network through time,

361
00:28:44,981 --> 00:28:46,670
so recurrent neural network.
I was tm.

362
00:28:47,410 --> 00:28:50,710
How can we explore the generation of 
emotion?

363
00:28:50,890 --> 00:28:53,740
Can we get this thing,
and this is an open question for us too.

364
00:28:53,741 --> 00:28:55,870
We just created the system.
We don't know if we can.

365
00:28:56,170 --> 00:29:01,170
Can we get it to make us feel something,
make us feel something.

366
00:29:01,541 --> 00:29:06,541
By watching it express its feelings,
can it become human before our eyes can 

367
00:29:09,451 --> 00:29:12,400
now learn to back competing against 
other agents?

368
00:29:12,930 --> 00:29:16,940
Ab testing on Turk.
I'm mechanical Turk.

369
00:29:17,810 --> 00:29:22,810
Can the winters be very convincing to 
make us feel entertained?

370
00:29:24,920 --> 00:29:27,350
Pity,
love.

371
00:29:27,560 --> 00:29:30,320
Maybe some of you will fall in love with
Angela here,

372
00:29:32,840 --> 00:29:37,840
mate Devenski.
On Friday we'll talk about cognitive 

373
00:29:37,840 --> 00:29:41,471
modeling architectures,
so you will speak about the cognitive 

374
00:29:41,471 --> 00:29:42,950
modeling aspect.
Can have a a Ma.

375
00:29:43,180 --> 00:29:48,180
Can we model cognition in some kind of 
systematic way to try to build intuition

376
00:29:48,611 --> 00:29:53,611
of Hok?
Complicated cognition is Andrea 

377
00:29:54,260 --> 00:29:59,260
[inaudible],
famous for being the state of the art 

378
00:29:59,260 --> 00:30:02,650
human on the image net challenge,
the representative,

379
00:30:02,710 --> 00:30:07,710
the 95 percent accuracy performance 
among other things he's also famous for 

380
00:30:09,220 --> 00:30:11,890
is now a tesla.
He will talk about

381
00:30:12,990 --> 00:30:15,570
the role,
the limitations,

382
00:30:15,571 --> 00:30:20,571
the possibilities of deep learning.
We'll talk,

383
00:30:21,400 --> 00:30:26,400
as I have spoken about in the past few 
weeks and throughout about our 

384
00:30:28,060 --> 00:30:33,060
misunderstanding or are flawed intuition
about what are the difficult and what 

385
00:30:33,161 --> 00:30:38,161
are the easy problems in deep learning 
and the power of the representational 

386
00:30:38,861 --> 00:30:43,861
learning,
the ability of neural networks to form 

387
00:30:43,861 --> 00:30:47,071
deeper and deeper representations that 
the underlying raw data that ultimately 

388
00:30:47,071 --> 00:30:51,451
forms that takes complex information 
that's hard to make sense of and a 

389
00:30:53,580 --> 00:30:56,070
converted into useful actionable 
knowledge

390
00:30:57,980 --> 00:31:02,980
that is from a certain Lens in a certain
certain Lens and a certain problem space

391
00:31:07,280 --> 00:31:12,280
can be clearly defined as understanding 
of the complex information understanding

392
00:31:13,021 --> 00:31:18,021
is ultimately taking complex information
and reducing it to a simple essential 

393
00:31:18,021 --> 00:31:22,701
elements,
representational learning and the 

394
00:31:22,701 --> 00:31:26,871
trivial case here in drawing a having to
draw a straight line to separate the 

395
00:31:27,451 --> 00:31:32,451
blue and the red curves.
That's impossible to do in a in a Nigel 

396
00:31:32,790 --> 00:31:37,790
input space on the left.
What the act of learning is for deep 

397
00:31:37,790 --> 00:31:41,931
neural networks in this formulation is 
to construct the topology under which 

398
00:31:41,931 --> 00:31:45,960
there exists a straight line to 
accurately classify blue versus red.

399
00:31:47,010 --> 00:31:50,250
That's the problem,
and for a simple blue and red line,

400
00:31:50,400 --> 00:31:55,400
it seems trivial here,
but this works in the general case for 

401
00:31:55,400 --> 00:31:56,910
arbitrary input spaces for arbitrary,
nonlinear,

402
00:31:56,911 --> 00:32:01,911
highly dimensional input spaces and the 
ability to automatically learn features 

403
00:32:02,850 --> 00:32:07,850
too,
to learn hierarchical representations of

404
00:32:08,341 --> 00:32:11,400
the raw sensory data means that you 
could do a lot more with data,

405
00:32:11,401 --> 00:32:16,401
which means you can expand further and 
further and further to create 

406
00:32:16,401 --> 00:32:18,400
intelligence systems that,
uh,

407
00:32:18,430 --> 00:32:20,520
operate successfully with real world 
data.

408
00:32:20,670 --> 00:32:22,350
That's what representation learning 
means.

409
00:32:22,351 --> 00:32:26,580
That deep learning allows because the 
arbitrary number of features that can be

410
00:32:26,581 --> 00:32:31,581
automatically determined,
you can learn a lot of things about a 

411
00:32:31,581 --> 00:32:34,050
pretty complex world.
Unfortunately,

412
00:32:34,470 --> 00:32:36,420
there needs to be a lot of supervised 
data.

413
00:32:36,421 --> 00:32:38,550
There still needs to be a lot of human 
input.

414
00:32:41,420 --> 00:32:46,420
Andre and others,
Josh will talk about the difference 

415
00:32:46,420 --> 00:32:50,981
between our human brain are biological 
and neural network and the artificial 

416
00:32:50,981 --> 00:32:55,301
neural network,
the full human brain with 100 billion 

417
00:32:55,301 --> 00:33:00,131
neurons,
1000 trillion synapses and the biggest 

418
00:33:00,131 --> 00:33:04,931
neural networks out there,
the artificial neural networks having 

419
00:33:04,931 --> 00:33:06,200
much smaller,
$60,

420
00:33:06,201 --> 00:33:11,201
million synapses for resident at 1:52.
The biggest difference,

421
00:33:12,170 --> 00:33:17,170
the parameter is the human brain being 
several orders of magnitude more 

422
00:33:17,170 --> 00:33:20,300
synapses.
That topology being much more complex,

423
00:33:20,540 --> 00:33:25,540
chaotic,
the asynchronous nature of the human 

424
00:33:25,540 --> 00:33:28,601
brain and the learning algorithm of 
artificial neural networks is trivial 

425
00:33:29,331 --> 00:33:34,331
and constrained with backpropagation is 
essentially an optimization function 

426
00:33:34,331 --> 00:33:38,561
over a,
over a clearly defined last function 

427
00:33:38,561 --> 00:33:39,410
from the output to the,
uh,

428
00:33:39,411 --> 00:33:42,770
to the input using backpropagation to 
teach,

429
00:33:43,160 --> 00:33:48,160
to adjust the waist on that network.
The learning algorithm for our human 

430
00:33:48,171 --> 00:33:52,400
brain is most of the unknown,
but it's certainly much more complicated

431
00:33:52,870 --> 00:33:54,230
than backpropagation.

432
00:33:56,270 --> 00:34:01,270
The power consumption.
The human brain is a lot more efficient 

433
00:34:01,270 --> 00:34:04,880
than artificial neural networks.
And there's a very kind of artificial,

434
00:34:05,990 --> 00:34:10,990
a trivial supervised learning process 
for training artificial neural networks.

435
00:34:12,170 --> 00:34:16,700
You have to have a training stage and 
you have to have an evaluation stage and

436
00:34:16,701 --> 00:34:21,701
once the network is trained,
there's no clear way to continue 

437
00:34:21,701 --> 00:34:21,701
training it or there's,
there's a lot of ways,

438
00:34:21,701 --> 00:34:25,550
but they're inefficient.
It's not designed to do online learning,

439
00:34:26,210 --> 00:34:31,210
uh,
naturally to always be learning is 

440
00:34:31,210 --> 00:34:32,960
designed to be,
to learn and then be applied.

441
00:34:33,560 --> 00:34:36,110
Obviously our human brains are always 
learning,

442
00:34:36,950 --> 00:34:41,950
but the beautiful,
fascinating thing is that they're both 

443
00:34:41,950 --> 00:34:43,850
distributed computation systems on a 
large scale,

444
00:34:44,030 --> 00:34:46,430
so it's not a,
uh,

445
00:34:46,710 --> 00:34:51,710
there's,
it doesn't ultimately boil down to a 

446
00:34:51,710 --> 00:34:53,360
single compute unit.
The computation is distributed.

447
00:34:53,720 --> 00:34:58,190
The backpropagation learning process 
distributed can be paralyzed in a GPU,

448
00:34:58,490 --> 00:34:59,750
massively parallelized.

449
00:35:00,290 --> 00:35:04,520
The underlying computational unit of a 
neuron is trivial,

450
00:35:04,580 --> 00:35:07,910
but can be stacked together to form 
forward neural networks,

451
00:35:07,911 --> 00:35:12,911
recurrent neural networks to represent 
both spacial information with images and

452
00:35:14,930 --> 00:35:18,980
temporal information with a audio 
speech,

453
00:35:19,520 --> 00:35:24,520
text sequences of images and video and 
so on.

454
00:35:25,580 --> 00:35:27,380
Mapping from one to one,
one to many,

455
00:35:27,381 --> 00:35:29,540
many to one,
so the mapping,

456
00:35:29,570 --> 00:35:34,570
any kind of structure vector and time 
data as an input to any kind of 

457
00:35:35,691 --> 00:35:38,460
classification,
regression sequences,

458
00:35:38,760 --> 00:35:40,440
captioning,
video,

459
00:35:40,441 --> 00:35:44,010
audio as output,
learning in the general sense,

460
00:35:44,670 --> 00:35:49,670
but in a domain that's precisely defined
for the supervised training process.

461
00:35:53,730 --> 00:35:57,330
We can think of the in deep learning 
case.

462
00:35:57,331 --> 00:36:02,331
You can think of the supervised methods 
where humans have to annotate the data 

463
00:36:02,331 --> 00:36:07,310
as memorization of the data.
We can think of the exciting new and 

464
00:36:07,310 --> 00:36:11,451
growing field of semi supervised 
learning when most of the data through 

465
00:36:11,451 --> 00:36:15,920
off through generative adversarial 
networks or through significant data 

466
00:36:15,920 --> 00:36:17,640
augmentation,
clever data augmentation,

467
00:36:17,880 --> 00:36:22,880
most of it is done automatically.
The annotation process or through 

468
00:36:22,880 --> 00:36:25,440
simulation and then reinforcement 
learning where most of the,

469
00:36:25,530 --> 00:36:30,530
uh,
most of the labels are extremely sparse 

470
00:36:30,530 --> 00:36:33,740
and come rarely.
And so the system has to figure out how 

471
00:36:33,740 --> 00:36:34,290
to operate in the world with very little
human input,

472
00:36:34,320 --> 00:36:35,550
very little human data.

473
00:36:38,550 --> 00:36:42,030
We can think of that as reasoning 
because you take very little information

474
00:36:42,031 --> 00:36:45,360
from our teachers,
the humans and transfer it across,

475
00:36:45,361 --> 00:36:48,480
generalize it across a to reason about 
the world.

476
00:36:49,050 --> 00:36:49,800
And finally,
uh,

477
00:36:49,920 --> 00:36:54,920
unsupervised learning,
the excitement of the community that 

478
00:36:54,920 --> 00:36:54,920
promise,
the hope.

479
00:36:54,920 --> 00:36:58,410
You could think of that as understanding
because ultimately it's taking data with

480
00:36:58,411 --> 00:37:03,411
very little or no human input.
And for me representations that that 

481
00:37:03,411 --> 00:37:05,700
data is how we think of understanding,
requiring,

482
00:37:06,630 --> 00:37:11,630
making sense of the world without strict
input of how to make sense of the world.

483
00:37:13,350 --> 00:37:16,770
The kind of process of discovering 
information,

484
00:37:17,790 --> 00:37:22,790
maybe discovering new ideas,
new ways to simplify the world to 

485
00:37:22,790 --> 00:37:24,720
represent the world that you can do new 
things with it.

486
00:37:25,050 --> 00:37:30,050
The new is the key element there.
Understanding and uh,

487
00:37:30,300 --> 00:37:35,130
Andrea and Eylea and others will talk 
about the certainly the past,

488
00:37:35,131 --> 00:37:36,480
but the future of deep learning.

489
00:37:36,720 --> 00:37:41,720
Whereas going to go is it over hyped,
under hyped?

490
00:37:42,180 --> 00:37:47,180
What is the future?
Will the compute of CPU GPU as six 

491
00:37:47,180 --> 00:37:51,360
continue with the breakthroughs,
the Moore's law and its various forms of

492
00:37:51,361 --> 00:37:56,361
massive parallelization continue and the
large data sets with tens of millions of

493
00:37:57,420 --> 00:38:02,420
images grow to billions and trillions.
Will the algorithms improve?

494
00:38:02,580 --> 00:38:06,300
Is there a groundbreaking idea that's 
still coming with,

495
00:38:06,430 --> 00:38:07,610
uh,
with Geoff Hinton's?

496
00:38:07,620 --> 00:38:12,620
Capsule networks is a fundamental 
architectural changes in your networks 

497
00:38:12,620 --> 00:38:15,120
that we can come up with that will 
change everything,

498
00:38:15,121 --> 00:38:20,121
that will ease the learning process.
They'll make the learning process more 

499
00:38:20,121 --> 00:38:23,781
efficient or will be able to represent 
higher and higher orders of information 

500
00:38:24,521 --> 00:38:29,521
is such that you can transfer knowledge 
between domains and the software 

501
00:38:30,631 --> 00:38:34,530
architectures that support intensive 
Florida Pi Torch.

502
00:38:34,840 --> 00:38:39,840
Uh,
I would say the last year and this year 

503
00:38:39,840 --> 00:38:39,840
will be the year of deep learning 
frameworks.

504
00:38:39,840 --> 00:38:44,251
So will,
those will certainly keep coming in 

505
00:38:44,251 --> 00:38:46,321
their various forms and the financial 
backing is growing and growing the open 

506
00:38:48,761 --> 00:38:50,740
challenges for deep learning.

507
00:38:51,400 --> 00:38:56,400
Really a lot of this course is kind of 
connected to deep learning because 

508
00:38:56,400 --> 00:39:01,201
that's where a lot of the recent 
breakthroughs that inspire us to think 

509
00:39:01,961 --> 00:39:05,310
about intelligence systems come from.
But the challenges are many,

510
00:39:05,520 --> 00:39:06,580
the,
the need,

511
00:39:06,610 --> 00:39:11,610
the ability to transfer between 
different domains as in reinforcement 

512
00:39:11,610 --> 00:39:15,511
learning and robotics.
The need for huge data and an efficient 

513
00:39:15,511 --> 00:39:16,270
learning.
Uh,

514
00:39:16,330 --> 00:39:19,090
we're,
we're still need supervised data.

515
00:39:19,270 --> 00:39:24,270
Uh,
inability to learn in an unsupervised 

516
00:39:24,270 --> 00:39:26,650
way is a huge problem and not fully 
automated learning.

517
00:39:27,030 --> 00:39:32,030
There's still a degree,
a significant degree of hyper parameter 

518
00:39:32,030 --> 00:39:32,860
tuning necessary with the reward 
functions.

519
00:39:32,861 --> 00:39:37,861
The loss functions are ultimately 
defined by humans and therefore are 

520
00:39:38,081 --> 00:39:43,081
deeply flawed.
When we released those systems into the 

521
00:39:43,081 --> 00:39:46,831
real world where there is no ground 
truth for the testing set and the goal 

522
00:39:46,831 --> 00:39:50,380
isn't achieving a class,
a high classification on a trivial,

523
00:39:50,670 --> 00:39:54,460
a image classification,
localization detection problem,

524
00:39:54,640 --> 00:39:59,640
but rather to have an autonomous vehicle
that doesn't kill pedestrians or an 

525
00:40:01,451 --> 00:40:06,451
industrial robot that operates in 
jointly with other human beings and all 

526
00:40:06,521 --> 00:40:08,410
the edge cases that come up.

527
00:40:08,650 --> 00:40:13,650
How does deep learning methods,
how to machine learning methods 

528
00:40:13,650 --> 00:40:14,920
generalize over the edge cases,
the weird stuff that happens in the real

529
00:40:14,921 --> 00:40:16,900
world?
Those are all the problems there.

530
00:40:18,040 --> 00:40:23,040
Stephen Wolfram will be here on Monday 
evening at 7:00

531
00:40:23,230 --> 00:40:26,410
PM,
has done a lot of amazing things,

532
00:40:26,411 --> 00:40:31,411
I would say is very interesting from his
recent interest in knowledge based 

533
00:40:31,411 --> 00:40:36,120
programming.
Wolfram Alpha I think is the fuel for 

534
00:40:36,120 --> 00:40:38,890
most middle school and high school 
students.

535
00:40:38,891 --> 00:40:43,891
Now,
for the first time taking calculus I 

536
00:40:43,891 --> 00:40:47,071
pray,
probably go to Wolfram Alpha to answer 

537
00:40:47,071 --> 00:40:47,071
their own questions,
but more seriously,

538
00:40:47,071 --> 00:40:51,480
there is a,
a deep connected graph of knowledge as 

539
00:40:51,480 --> 00:40:53,030
being built there with the wool from 
wool,

540
00:40:53,050 --> 00:40:58,050
from Alpha and wool from language that 
still will explore in terms of language 

541
00:40:59,410 --> 00:41:04,410
and interesting thing.
He was part of the team on arrival that,

542
00:41:05,040 --> 00:41:06,550
uh,
worked on the language.

543
00:41:06,730 --> 00:41:11,730
If for those of you are familiar to 
arrival where a alien species spoke with

544
00:41:12,400 --> 00:41:16,510
us,
US humans through a very interesting,

545
00:41:16,511 --> 00:41:21,511
beautiful,
complicated language and he was brought 

546
00:41:21,511 --> 00:41:24,301
in as a representative human to 
interpret that language just like in the

547
00:41:24,941 --> 00:41:27,640
movie,
who's represent that in real life.

548
00:41:28,150 --> 00:41:33,150
And you use the skills that him and his 
son Christopher used to analyze this 

549
00:41:33,381 --> 00:41:34,550
language.
Very interesting.

550
00:41:34,580 --> 00:41:39,580
That process is extremely interesting.
I hope he talks about it and his 

551
00:41:39,580 --> 00:41:42,710
background with Mathematica and a new 
kind of science.

552
00:41:44,300 --> 00:41:49,300
The sort of another set of ideas that 
have inspired people in terms of 

553
00:41:53,360 --> 00:41:58,360
creating intelligence systems is the 
idea that from very simple things,

554
00:42:03,650 --> 00:42:08,330
very simple rules,
extremely complex patterns can emerge.

555
00:42:09,010 --> 00:42:14,010
His work with cellular Automata did just
that take an extremely simple 

556
00:42:14,780 --> 00:42:18,830
mathematical constructs here with 
cellular Automata.

557
00:42:18,831 --> 00:42:23,831
These are,
these are grids of computational units 

558
00:42:23,831 --> 00:42:28,061
that switch on and off and some kind of 
predefined way and only operate locally 

559
00:42:28,061 --> 00:42:32,801
based on their local neighborhood and 
somehow based on different kinds of 

560
00:42:32,801 --> 00:42:33,890
rules,
different patterns emerge.

561
00:42:33,891 --> 00:42:38,891
Here's the three dimensional cellular 
automata with a simple rule starting 

562
00:42:38,891 --> 00:42:41,300
with nothing with a single cell.
They grow and really interesting,

563
00:42:41,301 --> 00:42:42,260
complex ways.

564
00:42:42,440 --> 00:42:47,440
This emergent complexity is inspiring.
It's the same kind of thing that 

565
00:42:47,751 --> 00:42:51,980
inspires us about neural networks that 
you can take a simple computational unit

566
00:42:52,190 --> 00:42:57,190
and when combined together in arbitrary 
ways can form complex representations.

567
00:42:57,890 --> 00:43:02,890
That's also very interesting.
You can see knowledge from a knowledge 

568
00:43:02,890 --> 00:43:03,590
perspective.
You could see knowledge formation in the

569
00:43:03,591 --> 00:43:08,591
same kind of way.
Simplicity at a mass distributed scale,

570
00:43:09,740 --> 00:43:13,880
resulting complexity.
Next Tuesday,

571
00:43:14,480 --> 00:43:17,780
Richard,
noise from article 36 coming all the way

572
00:43:17,781 --> 00:43:22,781
from UK for us.
We'll talk about it works with 

573
00:43:22,781 --> 00:43:26,870
autonomous weapons systems,
works with also a nuclear weapons,

574
00:43:26,990 --> 00:43:31,990
but primarily autonomous weapon systems 
and concern legal policy and 

575
00:43:32,961 --> 00:43:35,780
technological aspects of banning these 
weapons.

576
00:43:35,970 --> 00:43:40,970
There's been a lot of agreement about 
the safety hazards of autonomous systems

577
00:43:41,361 --> 00:43:43,880
that make decisions to kill a human 
being.

578
00:43:45,500 --> 00:43:47,490
Mara Criber,
CEO,

579
00:43:47,520 --> 00:43:51,860
Boston Dynamics,
previously long time ago,

580
00:43:51,890 --> 00:43:54,920
faculty here at Mit,
we'll talk about.

581
00:43:54,980 --> 00:43:59,980
We'll bring robots and talked to us 
about his work of robots in the real 

582
00:44:00,471 --> 00:44:05,471
world as a,
doing a lot of exciting stuff with 

583
00:44:05,471 --> 00:44:06,830
humanoid robotics at 80 kinds of robots 
operating on legs.

584
00:44:07,280 --> 00:44:09,200
Uh,
it's incredible work,

585
00:44:09,201 --> 00:44:13,910
extremely exciting and gets to explore 
the idea of how difficult it is to build

586
00:44:13,911 --> 00:44:17,120
these robots systems that operate in the
real world,

587
00:44:18,020 --> 00:44:23,020
uh,
from both the Qa control aspect and from

588
00:44:24,230 --> 00:44:28,070
the way the final result is perceived by
our society.

589
00:44:28,580 --> 00:44:33,580
It's very interesting to see when 
intelligence in robotics is embodied and

590
00:44:34,171 --> 00:44:37,680
then taking in by us and what that 
inspires.

591
00:44:37,890 --> 00:44:39,540
Fear,
excitement,

592
00:44:39,660 --> 00:44:41,310
hope,
concern,

593
00:44:42,330 --> 00:44:47,330
and all of the above.
Les is a expert in many aspects of 

594
00:44:49,001 --> 00:44:52,150
machine learning.
Is the cofounder of open Ai.

595
00:44:53,700 --> 00:44:58,700
Talk about there different aspects of 
game playing that they've recently been 

596
00:44:59,551 --> 00:45:03,900
exploring by using deeper enforcement,
learning to play arcade games

597
00:45:05,890 --> 00:45:10,890
and d on the deep mind side,
using deep reinforcement learning to 

598
00:45:10,890 --> 00:45:15,620
beat the best in the world that the game
of go in 2017.

599
00:45:15,621 --> 00:45:19,370
The big fascinating breakthrough 
achieved by that team with Alphago,

600
00:45:19,371 --> 00:45:23,270
zero training and agent that through 
self play playing itself,

601
00:45:23,271 --> 00:45:26,540
not an expert games so truly from 
scratch,

602
00:45:26,690 --> 00:45:31,690
learning to beat the best in the world,
including the previous iteration of 

603
00:45:31,690 --> 00:45:35,591
Alphago.
We'll explore what aspects of the stack 

604
00:45:35,720 --> 00:45:40,720
of intelligent robotics systems 
intelligent agents can be learned in 

605
00:45:40,720 --> 00:45:42,310
this way.
So deep learning,

606
00:45:42,340 --> 00:45:47,340
the memorization,
the supervised learning memorization 

607
00:45:47,340 --> 00:45:50,291
approach.
It looks at the sensor data feature 

608
00:45:50,291 --> 00:45:52,220
extraction representation,
learning aspect of this,

609
00:45:52,700 --> 00:45:55,700
taking the sensor data from camera,
a lidar audio,

610
00:45:56,300 --> 00:46:01,300
extracting the features for me,
higher order of representations and on 

611
00:46:01,300 --> 00:46:05,921
those representations.
Learning to actually accomplish some 

612
00:46:05,921 --> 00:46:08,381
kind of classification regression task,
figuring out based on the representation

613
00:46:09,410 --> 00:46:14,270
what is going on in the raw sensory data
and then combining that data together to

614
00:46:14,271 --> 00:46:18,640
reason about it and finally in the 
robotic domains,

615
00:46:18,670 --> 00:46:22,600
taking it all together as with humanoid,
robotics,

616
00:46:22,660 --> 00:46:27,660
industrial robotics,
autonomous vehicles taking altogether 

617
00:46:27,660 --> 00:46:31,380
and actually acting in this world where 
the effectors and the open question is,

618
00:46:31,811 --> 00:46:34,150
how much of this ai stack can be 
learned?

619
00:46:35,830 --> 00:46:38,770
That's something for us to discuss,
to think about.

620
00:46:40,000 --> 00:46:42,550
The eylea will touch on with deeper 
enforcement learning.

621
00:46:42,910 --> 00:46:46,630
We can certainly learn representations 
and perform classifications.

622
00:46:46,750 --> 00:46:49,490
They are better than human I damaged 
classification,

623
00:46:49,510 --> 00:46:52,360
image,
net and segmentation tasks

624
00:46:54,450 --> 00:46:59,450
and the excitement of deep learning is 
what's highlighted there in the red box 

625
00:46:59,450 --> 00:47:00,900
can be done end to ends.
Raw sensory data out to the knowledge,

626
00:47:00,901 --> 00:47:02,580
to the output,
to the classification.

627
00:47:03,240 --> 00:47:08,240
Can we begin to reason is the open 
question with the knowledge based 

628
00:47:08,240 --> 00:47:09,990
programming that Stephen Wolfman,
we'll talk about.

629
00:47:09,991 --> 00:47:14,991
Can we begin to take these automatically
generated high order representations and

630
00:47:16,201 --> 00:47:19,170
combine them together to form knowledge 
basis,

631
00:47:19,860 --> 00:47:24,720
to form a aggregate grass of ideas that 
can then be used the reason

632
00:47:26,780 --> 00:47:31,780
and can we then combine them together to
act in the world for whether in 

633
00:47:32,621 --> 00:47:37,621
simulation with arcade games or 
simulation of autonomous vehicles or box

634
00:47:37,681 --> 00:47:42,681
systems are actually in the physical 
world with robots moving about that end 

635
00:47:42,681 --> 00:47:46,210
to end from raw sensory data to action 
be learned.

636
00:47:47,140 --> 00:47:52,140
That's the open question for for 
artificial general intelligence for this

637
00:47:52,211 --> 00:47:56,920
class.
Can this entire process be end to end?

638
00:47:58,090 --> 00:48:03,090
Can we build systems and how do we do it
that achieve this process end to end in 

639
00:48:03,641 --> 00:48:08,641
the same way that humans do.
We're born in this raw sensory 

640
00:48:08,641 --> 00:48:12,301
environment,
taking in very little information and 

641
00:48:12,301 --> 00:48:15,961
learn to operate successfully an 
arbitrary constraints,

642
00:48:17,470 --> 00:48:21,370
arbitrary goals,
and to do so.

643
00:48:21,610 --> 00:48:26,560
We have lectures,
we have three projects and we have guest

644
00:48:26,561 --> 00:48:31,561
speakers from various disciplines.
I hope that all these voices will be 

645
00:48:32,081 --> 00:48:37,081
heard and will feed a conversation about
artificial intelligence and it's 

646
00:48:39,581 --> 00:48:44,581
positive and it's concerning effects in 
society and how do we move forward from 

647
00:48:45,821 --> 00:48:47,200
an engineering approach.

648
00:48:47,620 --> 00:48:51,010
The topics will be deep learning,
deep reinforcement learning,

649
00:48:51,700 --> 00:48:53,700
cognitive modeling,
competition,

650
00:48:53,720 --> 00:48:55,450
cognitive science,
emotion creation,

651
00:48:55,451 --> 00:49:00,451
knowledge based programming,
ai safety with autonomous weapons 

652
00:49:00,451 --> 00:49:03,160
systems and personal robotics with human
centered artificial intelligence.

653
00:49:03,280 --> 00:49:05,260
That's for the first two weeks of this 
class.

654
00:49:05,530 --> 00:49:09,070
That's the part where if you're actually
registered students,

655
00:49:09,071 --> 00:49:10,960
that's where you need to submit the 
project.

656
00:49:11,170 --> 00:49:15,310
That's when we all meet here every,
every night with an incredible speakers,

657
00:49:15,790 --> 00:49:20,790
but this will continue.
We're already have several speakers 

658
00:49:20,790 --> 00:49:22,240
scheduled the next couple of months yet 
to be announced,

659
00:49:22,600 --> 00:49:27,220
but they're incredible and we have 
conversations on video.

660
00:49:27,460 --> 00:49:32,460
We'll have new projects.
I hope this continues throughout 2018 on

661
00:49:32,591 --> 00:49:37,591
the topics of Ai Ethics and bias.
There's a lot of incredible work in a 

662
00:49:38,680 --> 00:49:43,680
way know of a speaker.
There are coming on the topic of how do 

663
00:49:43,680 --> 00:49:45,160
we create artificial intelligence 
systems that are do not discriminate,

664
00:49:45,400 --> 00:49:50,400
did not form the kind of biases that US 
humans do in this world that are 

665
00:49:51,310 --> 00:49:56,310
operating under social norms,
but our reasoning beyond the flawed 

666
00:49:57,160 --> 00:50:02,160
aspects of those social norms with bias,
creativity as well.

667
00:50:02,800 --> 00:50:07,800
The project of dream vision and beyond.
There's so much exciting work qa and 

668
00:50:08,930 --> 00:50:13,270
using machine learning methods to create
beautiful art and music,

669
00:50:15,380 --> 00:50:17,680
a brain stimulation,
neuroscience,

670
00:50:17,681 --> 00:50:18,910
computation,
neuroscience.

671
00:50:18,940 --> 00:50:23,940
Shockingly,
in the first two weeks we don't have a 

672
00:50:23,940 --> 00:50:26,670
computation neuroscience speaker,
which is a fascinating perspective.

673
00:50:26,930 --> 00:50:30,410
Brain simulation or neuroscience in 
general.

674
00:50:30,560 --> 00:50:35,560
Computation neuroscience is a 
fascinating approach from the from the 

675
00:50:35,560 --> 00:50:40,121
mark of actual brain work to get the 
perspective of how our brain works and 

676
00:50:40,341 --> 00:50:45,341
how we can create something that mimics,
that resembles the fundamentals of what 

677
00:50:45,341 --> 00:50:49,160
makes our brain intelligent.
And finally the turing test.

678
00:50:49,161 --> 00:50:54,161
The traditional definite shouldn't have 
intelligence defined by Alan Turing was 

679
00:50:54,161 --> 00:50:57,740
grounded in natural language processing 
and creating chat bots that impress us,

680
00:50:57,920 --> 00:51:00,890
that amaze us and trick us into thinking
they're human.

681
00:51:02,030 --> 00:51:07,030
We will have a project and a speaker on 
natural language processing in March 

682
00:51:09,250 --> 00:51:14,250
that I like to thank you for coming 
today and look forward to seeing your 

683
00:51:14,250 --> 00:51:15,740
submissions for the three projects.
Thank you very much.

