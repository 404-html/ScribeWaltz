1
00:00:00,090 --> 00:00:02,760
You've studied the human mind,
cognition,

2
00:00:02,790 --> 00:00:03,870
language,
vision,

3
00:00:03,871 --> 00:00:06,600
evolution,
psychology from child to adult,

4
00:00:07,350 --> 00:00:11,400
from the level of individual to the 
level of our entire civilization.

5
00:00:11,760 --> 00:00:15,060
So I feel like I can start with a simple
multiple choice question.

6
00:00:16,320 --> 00:00:20,010
What is the meaning of life?
Is it a,

7
00:00:20,250 --> 00:00:22,890
to attain knowledge is Plato said,
b,

8
00:00:22,891 --> 00:00:24,990
to attain power,
as Nisha said.

9
00:00:25,380 --> 00:00:28,140
See to escape death,
as Ernest Becker said,

10
00:00:28,650 --> 00:00:33,650
d,
to propagate our genes as Darwin and 

11
00:00:33,650 --> 00:00:33,720
others have said,
he,

12
00:00:33,810 --> 00:00:36,600
there is no meaning as the nihilists 
have said,

13
00:00:37,570 --> 00:00:41,550
f knowing the meaning of life is beyond 
our cognitive capabilities.

14
00:00:41,551 --> 00:00:43,050
Uh,
Steven pinker said,

15
00:00:43,140 --> 00:00:46,680
based on my interpretation 20 years ago,
and gee,

16
00:00:46,710 --> 00:00:47,580
none of the above,

17
00:00:48,200 --> 00:00:53,200
I'd say eight comes closest,
but I would amend that to attaining not 

18
00:00:53,200 --> 00:00:55,600
only knowledge but a fulfillment more 
generally.

19
00:00:56,080 --> 00:00:57,880
That is life,
health,

20
00:00:57,910 --> 00:00:59,730
stimulation,
uh,

21
00:01:00,540 --> 00:01:05,540
access to the living,
cultural and social world.

22
00:01:06,210 --> 00:01:09,270
Now this is our meeting of life.
It's not the meaning of life.

23
00:01:09,620 --> 00:01:14,620
If you were to ask our genes,
they're meeting is to propagate copies 

24
00:01:15,241 --> 00:01:20,241
of themselves,
but that is distinct from the meaning 

25
00:01:20,241 --> 00:01:21,810
that the brain that they lead to sets 
for itself.

26
00:01:22,410 --> 00:01:27,410
So to you,
knowledge is a small subset or a large 

27
00:01:27,410 --> 00:01:28,860
subset.
It's a large subset,

28
00:01:28,861 --> 00:01:32,490
but it's not the entirety of human 
striding because,

29
00:01:32,540 --> 00:01:37,540
uh,
we also want to interact with people we 

30
00:01:37,540 --> 00:01:41,270
want to experience beauty,
we want to experience the richness of 

31
00:01:41,270 --> 00:01:41,280
the natural world.
Uh,

32
00:01:41,410 --> 00:01:43,530
but uh,
understanding the,

33
00:01:43,531 --> 00:01:48,531
what makes the universe a tech is a,
is way up there for some of us more than

34
00:01:48,961 --> 00:01:49,410
others.

35
00:01:49,630 --> 00:01:51,730
Uh,
certainly for me that's a,

36
00:01:52,150 --> 00:01:57,150
that that's one of the top five.
So is that a fundamental aspect ages 

37
00:01:57,811 --> 00:02:02,811
describing your own preference or is 
this a fundamental aspect of human 

38
00:02:02,811 --> 00:02:05,430
nature is to seek knowledge to a.
In your latest book,

39
00:02:05,431 --> 00:02:06,650
you talk about the,
the,

40
00:02:06,730 --> 00:02:11,730
the power,
the usefulness of rationality and 

41
00:02:11,730 --> 00:02:11,730
reason.
So on.

42
00:02:11,730 --> 00:02:15,720
Is that a fundamental nature of human 
beings or is it something we should just

43
00:02:15,721 --> 00:02:18,030
strive for it both.
It is,

44
00:02:18,060 --> 00:02:20,550
we're,
we're capable of striving for it because

45
00:02:20,551 --> 00:02:25,551
it is one of the things that make us 
what we are homo sapiens wise,

46
00:02:26,130 --> 00:02:31,130
man,
we are unusual among animals in the 

47
00:02:31,130 --> 00:02:34,440
degree to which we acquire knowledge and
use it to survive.

48
00:02:34,710 --> 00:02:36,300
We,
we make tools,

49
00:02:36,360 --> 00:02:38,590
we strike agreements,
uh,

50
00:02:38,640 --> 00:02:42,390
via language,
we extract poisons,

51
00:02:42,450 --> 00:02:47,450
we predict the behavior of animals,
we try to get at the workings of plants 

52
00:02:47,911 --> 00:02:50,640
and when I say we,
I don't just mean we in the modern west,

53
00:02:50,940 --> 00:02:55,940
but we as a species everywhere,
which is how we've managed to occupy 

54
00:02:55,940 --> 00:02:59,040
every niche on the planet and how we've 
managed to drive other animals to,

55
00:03:00,010 --> 00:03:04,170
and the refinement of reason in pursuit 
of human wellbeing,

56
00:03:04,410 --> 00:03:07,320
of a health,
happiness,

57
00:03:07,380 --> 00:03:10,980
social richness,
cultural richness is our,

58
00:03:11,010 --> 00:03:16,010
uh,
our main challenge in the present that 

59
00:03:16,010 --> 00:03:18,561
is using our intellect,
using our knowledge to figure out how 

60
00:03:18,561 --> 00:03:21,270
the world works,
how we work in order to make discoveries

61
00:03:21,271 --> 00:03:24,840
and strike agreements that make us all 
better off in the long run.

62
00:03:25,280 --> 00:03:29,060
Right?
And you do that almost undeniably,

63
00:03:29,061 --> 00:03:31,850
and I'm in a data driven way and your 
recent book,

64
00:03:32,060 --> 00:03:37,060
but I'd like to focus on the artificial 
intelligence aspect of things and not 

65
00:03:37,060 --> 00:03:38,660
just artificial intelligence,
but natural intelligence too.

66
00:03:38,900 --> 00:03:43,900
So,
20 years ago in the book you've written 

67
00:03:43,900 --> 00:03:44,900
on how the mind works,
you conjecture,

68
00:03:44,901 --> 00:03:48,150
again my right to interpret things.
You can,

69
00:03:48,450 --> 00:03:50,150
uh,
you can correct me if I'm wrong,

70
00:03:50,151 --> 00:03:55,151
but you conjecture that human thought in
the brain may be a result of a now a 

71
00:03:55,151 --> 00:03:57,050
massive network of highly interconnected
neurons.

72
00:03:57,080 --> 00:03:58,600
So from this,
uh,

73
00:03:58,940 --> 00:04:03,260
interconnectivity and mergers thought 
compared to artificial neural networks,

74
00:04:03,261 --> 00:04:05,840
we should,
we use for machine learning today.

75
00:04:06,200 --> 00:04:09,410
Is there something fundamentally more 
complex,

76
00:04:09,440 --> 00:04:13,040
mysterious,
even magical about the biological neural

77
00:04:13,041 --> 00:04:17,150
networks versus the ones we've been 
starting to use,

78
00:04:17,500 --> 00:04:22,500
uh,
over the past 60 years and it become to 

79
00:04:22,500 --> 00:04:22,500
success in the past 10.

80
00:04:22,500 --> 00:04:27,070
There is something a little bit 
mysterious about the human neural 

81
00:04:27,070 --> 00:04:31,741
networks,
which is that each one of us who is a 

82
00:04:31,741 --> 00:04:33,040
neural network knows that we ourselves 
are conscious,

83
00:04:33,470 --> 00:04:38,470
a conscious,
not have a sense of registering our 

84
00:04:38,470 --> 00:04:38,470
surroundings or even registering our 
internal state.

85
00:04:38,740 --> 00:04:42,850
But in having subjective,
first person present tense experience.

86
00:04:42,940 --> 00:04:46,330
That is when I see red,
it's not just different for green,

87
00:04:47,230 --> 00:04:48,080
but it just,
there's,

88
00:04:48,100 --> 00:04:53,100
there's a redness to it.
But I feel whether an artificial system 

89
00:04:53,100 --> 00:04:54,070
would experience that or not,
I don't know.

90
00:04:54,071 --> 00:04:56,770
And I don't think I can know that.
That's why it's mysterious.

91
00:04:56,771 --> 00:05:01,771
If we had a perfectly lifelike robot 
that was behaviorally indistinguishable 

92
00:05:01,771 --> 00:05:06,720
from a human,
would we attribute consciousness to it 

93
00:05:06,720 --> 00:05:08,800
or a or ought we to attribute 
consciousness to it and that's something

94
00:05:08,801 --> 00:05:12,310
that it's a very hard to know.
But putting that aside,

95
00:05:12,311 --> 00:05:15,040
putting aside that,
that largely philosophical question,

96
00:05:15,970 --> 00:05:20,970
the question is,
is there some difference between the 

97
00:05:20,970 --> 00:05:21,990
hero human neural network and the ones 
that we were building?

98
00:05:22,020 --> 00:05:26,440
An artificial intelligence will mean 
that we're on the current trajectory not

99
00:05:26,441 --> 00:05:31,441
going to reach the point where we've got
a lifelike robot indistinguishable from 

100
00:05:31,441 --> 00:05:36,391
a human because the way their neural,
so-called neural networks are organized 

101
00:05:36,391 --> 00:05:37,270
are different from the way ours are 
organized.

102
00:05:37,960 --> 00:05:40,100
I think there's overlap,
but I think there are some,

103
00:05:40,360 --> 00:05:45,360
some big differences that a current 
neural networks current so called deep 

104
00:05:46,781 --> 00:05:49,270
learning systems are,
are in reality,

105
00:05:49,271 --> 00:05:54,271
not all that deep that is,
they are very good at extracting high 

106
00:05:54,271 --> 00:05:57,901
order statistical regularities,
but most of the systems don't have a 

107
00:05:57,901 --> 00:05:59,000
semantic level,
a level of,

108
00:05:59,320 --> 00:06:04,320
uh,
actual understanding of who did what to 

109
00:06:04,320 --> 00:06:04,320
whom.
Uh,

110
00:06:04,320 --> 00:06:04,460
why,
where,

111
00:06:04,640 --> 00:06:06,080
how things work,
what causes,

112
00:06:06,081 --> 00:06:06,590
what else

113
00:06:07,010 --> 00:06:09,190
do you think?
That kind of thing can emerge as it does

114
00:06:09,191 --> 00:06:11,530
so artificial,
you're so much smaller,

115
00:06:11,531 --> 00:06:16,180
the number of connections and so on and 
the current human biological networks,

116
00:06:16,181 --> 00:06:21,181
but do you think sort of go to go to 
consciousness or to go to this higher 

117
00:06:21,181 --> 00:06:25,921
level semantic reasoning about things?
Do you think that kind of merge with 

118
00:06:25,921 --> 00:06:29,371
just a larger network with more richly 
weirdly interconnected network?

119
00:06:29,790 --> 00:06:31,440
Separate again,
consciousness,

120
00:06:31,441 --> 00:06:33,400
because consciousness is even a matter 
of complexity.

121
00:06:33,430 --> 00:06:34,590
Really Weird one.
Yeah.

122
00:06:34,591 --> 00:06:35,180
You could have,
you could,

123
00:06:35,210 --> 00:06:38,580
you could sensibly asked the question of
whether shrimp conscious for example,

124
00:06:38,581 --> 00:06:41,790
they're not terribly complex,
but maybe they feel pain.

125
00:06:41,850 --> 00:06:43,130
So let's,
let's just put that one,

126
00:06:43,170 --> 00:06:44,820
that part of it aside yet.
But,

127
00:06:45,320 --> 00:06:50,320
uh,
I think sheer size of a neural network 

128
00:06:50,320 --> 00:06:52,410
is not enough to give it a structure and
knowledge,

129
00:06:52,620 --> 00:06:56,130
but if it's suitably engineered then uh,
then why not?

130
00:06:56,370 --> 00:07:00,200
That is we're neural networks.
Natural selection did a,

131
00:07:00,230 --> 00:07:05,230
a,
a kind of equivalent of engineering of 

132
00:07:05,230 --> 00:07:07,461
our brains.
So I don't think there's anything 

133
00:07:07,461 --> 00:07:07,461
mysterious in the sense that no,
uh,

134
00:07:07,860 --> 00:07:11,670
no system made it of silicon could ever 
do what a human brain can do.

135
00:07:11,671 --> 00:07:16,671
I think it's possible in principle,
whether it'll ever happen depends not 

136
00:07:16,671 --> 00:07:19,020
only on how clever we are in engineering
these systems,

137
00:07:19,230 --> 00:07:22,590
but whether even we even want to,
whether that's even a sensible goal that

138
00:07:22,591 --> 00:07:23,880
is,
you can ask the question,

139
00:07:23,881 --> 00:07:25,380
is there any,
um,

140
00:07:25,730 --> 00:07:28,420
locomotion system that is,
uh,

141
00:07:28,430 --> 00:07:29,720
as,
as good as a human?

142
00:07:30,050 --> 00:07:35,050
Well,
we kind of want to do better than a 

143
00:07:35,050 --> 00:07:35,050
human ultimately in terms of legged 
locomotion.

144
00:07:35,050 --> 00:07:39,260
Uh,
there's no reason that humans should be 

145
00:07:39,260 --> 00:07:42,401
our benchmark there.
They're tools that might be better in 

146
00:07:42,401 --> 00:07:43,080
some ways.
It may just be not as a,

147
00:07:44,170 --> 00:07:48,260
maybe that we can't duplicate a natural 
system because,

148
00:07:48,300 --> 00:07:53,300
uh,
at some point it's so much cheaper to 

149
00:07:53,300 --> 00:07:55,301
use a natural system that we're not 
going to invest more brainpower and 

150
00:07:55,301 --> 00:07:55,430
resources.
So for example,

151
00:07:55,431 --> 00:08:00,431
we don't really have a subsequent exact 
substitute for would still build houses 

152
00:08:00,501 --> 00:08:02,360
out of wood.
We still build furniture out of wood.

153
00:08:02,720 --> 00:08:04,280
We liked the look.
We like the feel.

154
00:08:04,281 --> 00:08:07,020
It's what has certain properties that 
synthetic stones.

155
00:08:07,590 --> 00:08:11,240
It's not that there's any magical and 
mysterious about would a,

156
00:08:11,270 --> 00:08:16,270
it's just that the extra steps of 
duplicating everything about wood is 

157
00:08:16,581 --> 00:08:21,581
something we just haven't bothered 
because we have what likewise say 

158
00:08:21,581 --> 00:08:21,581
cotton.

159
00:08:21,581 --> 00:08:21,581
I mean,
I'm wearing cotton clothing now.

160
00:08:21,581 --> 00:08:23,630
It feels much better than than 
polyester.

161
00:08:24,720 --> 00:08:28,430
It's not that cotton has something magic
in it and it's not that.

162
00:08:28,580 --> 00:08:33,580
If there was that we couldn't ever 
synthesize something exactly like 

163
00:08:33,580 --> 00:08:34,730
cotton,
but at some point it's just a.

164
00:08:34,731 --> 00:08:39,731
it's just not worth it.
We've got cotton and likewise in the 

165
00:08:39,731 --> 00:08:42,311
case of human intelligence,
the goal of making an artificial system 

166
00:08:42,311 --> 00:08:46,750
that is exactly like the human brain is 
a a goal that we probably known as going

167
00:08:47,021 --> 00:08:51,530
to pursue to the bitter end,
I suspect because if you want tools that

168
00:08:51,531 --> 00:08:53,930
do things better than humans,
you're not going to care whether it does

169
00:08:53,931 --> 00:08:58,931
something like humans.
So for going to diagnosing cancer or 

170
00:08:58,931 --> 00:09:00,600
predicting the weather.
Why set humans as your benchmark

171
00:09:01,600 --> 00:09:06,600
in in general?
I suspect you also believe that even if 

172
00:09:07,080 --> 00:09:10,530
the human should not be a benchmark and 
we don't want to imitate humans in their

173
00:09:10,531 --> 00:09:13,710
system.
There's a lot to be learned about how to

174
00:09:13,711 --> 00:09:16,520
create an artificial intelligence system
by studying the human.

175
00:09:16,920 --> 00:09:17,420
Yeah,
I,

176
00:09:17,421 --> 00:09:22,421
I think that's right.
In the same way that to build flying 

177
00:09:22,421 --> 00:09:27,081
machines,
we want under the laws of aerodynamics 

178
00:09:27,081 --> 00:09:27,630
and including birds but not mimic the 
birds,

179
00:09:27,880 --> 00:09:29,070
but at the same loss,

180
00:09:30,240 --> 00:09:33,690
uh,
you have a view on ai,

181
00:09:33,691 --> 00:09:37,230
artificial intelligence and safety that,
uh,

182
00:09:37,340 --> 00:09:42,340
from my perspective is refreshingly 
rational or perhaps more importantly has

183
00:09:47,011 --> 00:09:52,011
elements of positivity to it which I 
think can be inspiring and empowering as

184
00:09:52,231 --> 00:09:57,030
opposed to paralyzing for many people,
including AI researchers.

185
00:09:57,031 --> 00:10:00,660
The eventual existential threat of ai is
obvious,

186
00:10:01,020 --> 00:10:04,350
not only possible but obvious.
And for many others,

187
00:10:04,410 --> 00:10:07,890
including AI researchers,
the threat is not obvious.

188
00:10:08,400 --> 00:10:13,400
So Elon Musk is famously in the highly 
concerned about ai camp,

189
00:10:14,790 --> 00:10:19,790
saying things like ai is far more 
dangerous than nuclear weapons and that 

190
00:10:19,790 --> 00:10:22,350
ai will likely destroy a human 
civilization.

191
00:10:23,040 --> 00:10:27,960
So in February you said that if Elon was
really serious about Ai,

192
00:10:28,360 --> 00:10:30,090
the,
the threat of Ai,

193
00:10:30,330 --> 00:10:35,330
he would stop building self driving cars
that he's doing very successful as part 

194
00:10:35,330 --> 00:10:36,870
of Tesla.
Then he said,

195
00:10:37,080 --> 00:10:42,080
wow,
if even pinker doesn't understand the 

196
00:10:42,080 --> 00:10:42,630
difference between narrow ai,
like a car and general ai,

197
00:10:43,470 --> 00:10:48,470
when the ladder literally has a million 
times more compute power and an open 

198
00:10:48,470 --> 00:10:51,990
ended utility function,
humanity is in deep trouble.

199
00:10:52,230 --> 00:10:55,500
So first,
what did you mean by the statement about

200
00:10:55,770 --> 00:10:59,300
Ilan mosque should stop building self 
driving cars if he's deeply concerned,

201
00:11:00,180 --> 00:11:02,680
not the last time that Elon musk is 
fired off in,

202
00:11:02,681 --> 00:11:03,650
in temporary tweet.

203
00:11:05,080 --> 00:11:07,600
We live in a world where a twitter has 
power.

204
00:11:07,720 --> 00:11:08,400
Yes.
Uh,

205
00:11:08,860 --> 00:11:09,690
yeah.
I think the,

206
00:11:09,700 --> 00:11:10,540
the,
um,

207
00:11:10,910 --> 00:11:15,910
the,
there are to a kinds of existential 

208
00:11:15,910 --> 00:11:20,511
threat that had been discussed in 
connection with artificial intelligence 

209
00:11:20,511 --> 00:11:20,511
and I think that they're both 
incoherent.

210
00:11:20,511 --> 00:11:25,310
Uh,
one of them is a vague fear of ai 

211
00:11:25,310 --> 00:11:25,310
takeover that,
uh,

212
00:11:25,310 --> 00:11:30,200
it just as we subjugated animals and 
less technologically advanced peoples.

213
00:11:30,900 --> 00:11:33,450
So if we build something that's more 
advanced than us,

214
00:11:33,480 --> 00:11:36,300
it will inevitably turn us into paths or
slaves or,

215
00:11:36,630 --> 00:11:41,630
or a domesticated animal equivalence.
I think this confuses intelligence with 

216
00:11:42,810 --> 00:11:47,810
a will to power that.
It so happens that in the intelligence 

217
00:11:47,810 --> 00:11:50,490
system we are most familiar with namely 
Homo Sapiens,

218
00:11:50,940 --> 00:11:55,940
we are products of natural selection,
which is a competitive process and so 

219
00:11:55,940 --> 00:11:59,671
bundled together with our problem 
solving capacity are a number of nasty 

220
00:11:59,671 --> 00:12:04,051
traits,
a dominance and a exploitation and 

221
00:12:04,051 --> 00:12:08,590
maximize nation of of power and glory 
and resources and influence.

222
00:12:08,920 --> 00:12:12,220
There's no reason to think that she or 
problem solving capability.

223
00:12:12,221 --> 00:12:17,221
We'll set that as one of its goals.
Its goals will be whatever we said its 

224
00:12:17,221 --> 00:12:21,330
goals as and as long as someone isn't 
building a megalomanic maniacal 

225
00:12:21,330 --> 00:12:23,980
artificial intelligence,
there's no reason to think that it would

226
00:12:23,981 --> 00:12:26,020
naturally evolve in that direction.
Now you might say,

227
00:12:26,021 --> 00:12:31,021
well,
what if we gave it the goal of 

228
00:12:31,021 --> 00:12:33,180
maximizing its own power source?
That's a pretty stupid goal to give a a,

229
00:12:33,401 --> 00:12:35,500
an autonomous system.
You don't give it that goal.

230
00:12:36,170 --> 00:12:41,170
I mean that just self-evidently idiotic.
So if you look at the history of the 

231
00:12:41,170 --> 00:12:45,841
world,
there's been a lot of opportunities 

232
00:12:45,841 --> 00:12:46,660
where engineers could instill in a 
system destructive power and they choose

233
00:12:46,661 --> 00:12:49,330
not to because that's the natural 
process of engineering.

234
00:12:49,600 --> 00:12:50,770
Well,
except for weapons.

235
00:12:50,771 --> 00:12:51,820
I mean,
if you're building a weapon,

236
00:12:51,850 --> 00:12:53,320
it's goal is to destroy people.

237
00:12:53,380 --> 00:12:55,900
Uh,
and so I think there are good reasons to

238
00:12:55,920 --> 00:12:59,590
not not build certain kinds of weapons.
I think new building nuclear weapons was

239
00:12:59,591 --> 00:13:01,860
a massive mistake,
a big problem.

240
00:13:01,870 --> 00:13:04,450
Do you think.
So,

241
00:13:04,530 --> 00:13:09,530
uh,
maybe pause on that because that is one 

242
00:13:09,530 --> 00:13:09,530
of the serious threats.
Do you think that,

243
00:13:09,670 --> 00:13:14,670
uh,
it was a mistake in a sense that it 

244
00:13:14,670 --> 00:13:16,330
should have been stopped early on,
or do you think it's just an unfortunate

245
00:13:16,331 --> 00:13:19,870
event of invention that this was 
invented?

246
00:13:20,310 --> 00:13:22,300
It was possible to stop,
I guess is the question.

247
00:13:22,480 --> 00:13:25,900
It's hard to rewind the clock because of
course it was invented in the context of

248
00:13:25,901 --> 00:13:30,280
World War II and the fear that the Nazis
might develop one first.

249
00:13:31,030 --> 00:13:33,220
Then once it was initiated for that 
reason,

250
00:13:33,221 --> 00:13:35,380
it was,
it was hard to turn off,

251
00:13:36,040 --> 00:13:41,040
especially since winning the war against
the Japanese and the Nazis was such an 

252
00:13:41,380 --> 00:13:46,380
overwhelming goal of every responsible 
person that they were just nothing that 

253
00:13:46,380 --> 00:13:48,250
people wouldn't have done then to ensure
victory.

254
00:13:49,270 --> 00:13:51,490
It's quite possible if world war two 
hadn't happened,

255
00:13:51,550 --> 00:13:53,530
that nuclear weapons wouldn't have been 
invented,

256
00:13:53,630 --> 00:13:55,250
we can't know,
um,

257
00:13:55,360 --> 00:14:00,360
but I don't think it was a by any means 
a necessity anymore than some of the 

258
00:14:00,360 --> 00:14:03,661
other weapon systems that were 
envisioned but never implemented like 

259
00:14:03,661 --> 00:14:08,340
planes that would disperse poison gas 
over cities like crop dusters or systems

260
00:14:09,071 --> 00:14:10,160
to try to,
to,

261
00:14:10,180 --> 00:14:10,620
to,
uh,

262
00:14:10,660 --> 00:14:11,550
create,
uh,

263
00:14:11,590 --> 00:14:16,060
earthquakes and tsunamis and enemy 
countries to weaponize the weather,

264
00:14:16,180 --> 00:14:19,390
weaponize solar flares,
all kinds of crazy schemes that,

265
00:14:19,391 --> 00:14:19,810
uh,
that,

266
00:14:19,870 --> 00:14:24,870
that we thought the better of analogies 
between nuclear weapons and artificial 

267
00:14:24,870 --> 00:14:29,191
intelligence are fundamentally misguided
because the whole point of nuclear 

268
00:14:29,191 --> 00:14:33,151
weapons is to destroy things.
The point of artificial intelligence is 

269
00:14:33,151 --> 00:14:35,320
not to destroy things.
So the analogy is,

270
00:14:35,470 --> 00:14:37,810
is misleading.
So there's two artificial intelligence.

271
00:14:37,811 --> 00:14:42,110
You mentioned the first one.
It was the intelligence or hungry.

272
00:14:42,111 --> 00:14:47,111
Yeah.
The system that we designed ourselves 

273
00:14:47,111 --> 00:14:48,961
where we give it the goals goals are,
are external to the a means to attain 

274
00:14:49,241 --> 00:14:49,810
the goals.

275
00:14:51,430 --> 00:14:56,430
If we don't design an artificially 
intelligent system to maximize a 

276
00:14:56,621 --> 00:14:59,050
dominance,
than it won't maximize dominance.

277
00:14:59,190 --> 00:15:01,450
A,
it just that we're so familiar with Homo

278
00:15:01,510 --> 00:15:04,720
sapiens were these two traits come 
bundled together,

279
00:15:04,870 --> 00:15:09,870
particularly in men that we are apt to 
confuse high intelligence with a,

280
00:15:11,100 --> 00:15:11,930
a,
a,

281
00:15:11,940 --> 00:15:14,650
a will to power,
but that's just an error.

282
00:15:15,700 --> 00:15:20,700
The other fear is that we'll be 
collateral damage that will give a 

283
00:15:20,700 --> 00:15:24,391
artificial intelligence a goal like make
paperclips and it will pursue that goal 

284
00:15:24,911 --> 00:15:27,550
so brilliantly that before we can stop 
it,

285
00:15:27,551 --> 00:15:32,551
it turns us into paperclips,
will give it the goal of curing cancer 

286
00:15:32,551 --> 00:15:36,451
and it will turn us into Guinea pigs for
lethal experiments or give it the goal 

287
00:15:36,671 --> 00:15:39,330
of world peace in its conception of 
world pieces.

288
00:15:39,340 --> 00:15:40,990
No people,
therefore no fighting.

289
00:15:40,991 --> 00:15:42,610
And so it will kill us all.
Now,

290
00:15:42,611 --> 00:15:44,590
I think these are utterly fanciful.
In fact,

291
00:15:44,591 --> 00:15:46,370
I think they're actually self-defeating.

292
00:15:46,840 --> 00:15:51,840
They first of all,
assume that we're going to be so 

293
00:15:51,840 --> 00:15:54,721
brilliant that we can design an 
artificial intelligence that can cure 

294
00:15:54,721 --> 00:15:57,901
cancer but so stupid that we don't 
specify what we mean by curing cancer in

295
00:15:58,411 --> 00:16:00,880
enough detail.
That it won't kill us in the process.

296
00:16:01,380 --> 00:16:06,380
Uh,
and it assumes that the system will be 

297
00:16:06,380 --> 00:16:08,350
so smart that it can cure cancer,
but so idiotic that it doesn't,

298
00:16:08,410 --> 00:16:12,070
can't figure out that what we mean by 
curing cancer is not killing everyone.

299
00:16:12,820 --> 00:16:15,430
So I think that the,
the collateral damage scenario,

300
00:16:15,431 --> 00:16:17,570
the value alignment problem is,
uh,

301
00:16:17,590 --> 00:16:19,270
is also based on a misconception.

302
00:16:19,550 --> 00:16:24,550
So one of the challenges,
of course we don't know how to build 

303
00:16:24,550 --> 00:16:25,490
either system currently or are we even 
close to knowing.

304
00:16:25,850 --> 00:16:27,530
Of course those things can change 
overnight,

305
00:16:27,531 --> 00:16:31,130
but at this time theorizing about is 
very challenging,

306
00:16:31,480 --> 00:16:32,170
uh,
in,

307
00:16:32,180 --> 00:16:37,180
in either direction.
So that's probably at the core of the 

308
00:16:37,180 --> 00:16:39,851
problem is without that ability to 
reason about the real engineering things

309
00:16:39,981 --> 00:16:43,580
here at hand is a,
your imagination runs away with things.

310
00:16:43,610 --> 00:16:45,350
Exactly.
But let me sort of ask,

311
00:16:45,680 --> 00:16:50,680
what do you think was the motivation and
the thought process of Isla Moscow?

312
00:16:50,991 --> 00:16:53,750
I build autonomy vehicles.
I studied autonomous vehicles.

313
00:16:53,960 --> 00:16:58,960
I studied tesla autopilot.
I think it is one of the greatest 

314
00:16:58,960 --> 00:17:03,010
currently application,
large scale applications of artificial 

315
00:17:03,010 --> 00:17:06,161
intelligence in the world.
It has a potentially a very positive 

316
00:17:06,161 --> 00:17:10,091
impact on society.
So how does a person who is creating 

317
00:17:10,091 --> 00:17:13,811
this very good quote unquote narrow ai 
system also seem to be so concerned 

318
00:17:15,321 --> 00:17:20,321
about this other general ai?
What do you think is the motivation 

319
00:17:20,321 --> 00:17:25,061
there?
What do you think is the thing you'll 

320
00:17:25,061 --> 00:17:25,061
probably have to ask him,

321
00:17:25,061 --> 00:17:28,160
but they're an and he is notoriously a 
flamboyant,

322
00:17:29,110 --> 00:17:32,410
impulsive to the,
as we have just seen to the detriment of

323
00:17:32,411 --> 00:17:33,950
his own goals of,
of the,

324
00:17:33,951 --> 00:17:36,340
the health of the company.
And so I,

325
00:17:36,341 --> 00:17:38,710
I don't know what's going on in his 
mind.

326
00:17:38,711 --> 00:17:41,470
You probably have to ask him,
but I don't think the.

327
00:17:41,620 --> 00:17:46,250
And I don't think the distinction 
between special purpose ai and so-called

328
00:17:46,270 --> 00:17:51,270
General Iai is relevant.
That in the same way that special 

329
00:17:51,270 --> 00:17:55,280
purpose ai is not going to do anything 
conceivable in order to attain a goal.

330
00:17:55,310 --> 00:17:57,600
All engineering systems have to,
uh,

331
00:17:57,750 --> 00:18:02,750
are designed to trade off across 
multiple goals where we build cars in 

332
00:18:02,750 --> 00:18:06,851
the first place.
We didn't forget to install breaks 

333
00:18:06,851 --> 00:18:07,680
because the goal of a car is to go fast.
Uh,

334
00:18:07,760 --> 00:18:09,310
it occurred to people,
yes,

335
00:18:09,311 --> 00:18:11,170
you want it to go fast,
but not always.

336
00:18:11,290 --> 00:18:16,290
So you build in breaks to likewise,
if a car is going to be autonomous,

337
00:18:16,960 --> 00:18:20,290
that doesn't and program it to take the 
shortest route to the airport,

338
00:18:20,470 --> 00:18:25,470
it's not going to take the diagonal and 
mow down people and trees and fences 

339
00:18:25,470 --> 00:18:25,470
because that's the shortest route.

340
00:18:25,540 --> 00:18:28,690
That's not what we mean by the shortest 
route when we programmed it.

341
00:18:28,691 --> 00:18:32,830
And that's just what an uh,
an intelligent system is.

342
00:18:32,831 --> 00:18:37,831
By definition,
it takes into account multiple 

343
00:18:37,831 --> 00:18:37,831
constraints.
The same is true,

344
00:18:37,831 --> 00:18:40,420
in fact even more true of so-called 
general intelligence.

345
00:18:40,720 --> 00:18:43,450
That is,
if it's genuinely intelligent,

346
00:18:43,451 --> 00:18:48,451
it's not going to pursue some goal 
singlemindedly omitting every other 

347
00:18:49,750 --> 00:18:54,750
consideration and collateral effect.
That's not artificial general 

348
00:18:54,750 --> 00:18:54,750
intelligence.
That's a,

349
00:18:54,750 --> 00:18:57,120
that's artificial stupidity.
Um,

350
00:18:57,520 --> 00:18:58,600
I agree with you,
by the way,

351
00:18:58,601 --> 00:19:02,170
on the promise of autonomous vehicles 
for improving human welfare.

352
00:19:02,200 --> 00:19:07,200
I think it's spectacular and I'm 
surprised at how little press coverage 

353
00:19:07,200 --> 00:19:09,310
notes that in the United States alone,
something like 40,000

354
00:19:09,311 --> 00:19:14,311
people die every year on the highways,
fastly more than are killed by 

355
00:19:14,311 --> 00:19:14,830
terrorists.
And we spend.

356
00:19:15,190 --> 00:19:19,330
You spent a trillion dollars on a war to
combat deaths by terrorism,

357
00:19:19,540 --> 00:19:21,210
but half a dozen a year,
uh,

358
00:19:21,340 --> 00:19:23,530
whereas every year and year out,
40,000

359
00:19:23,531 --> 00:19:28,531
people are,
are massacred on the highways which 

360
00:19:28,531 --> 00:19:28,531
could be brought down to very close to 
zero.

361
00:19:28,531 --> 00:19:29,020
Uh,
so I'm,

362
00:19:29,021 --> 00:19:34,021
I'm,
I'm with you on the humanitarian 

363
00:19:34,021 --> 00:19:34,021
benefit.

364
00:19:34,021 --> 00:19:36,970
Let me just mention that it's,
as a person who is building these cars 

365
00:19:36,970 --> 00:19:37,730
that is a little bit offensive to me to 
say that engine in years,

366
00:19:37,731 --> 00:19:40,520
we'd be clueless enough not to engineer 
safety into systems.

367
00:19:40,790 --> 00:19:44,180
I often stay up at night thinking about 
those 40,000

368
00:19:44,181 --> 00:19:49,181
people that are dying and everything I 
tried to engineer is to save those 

369
00:19:49,181 --> 00:19:53,591
people's lives.
So every new invention that I'm super 

370
00:19:53,591 --> 00:19:53,591
excited about every new,
uh,

371
00:19:53,591 --> 00:19:54,910
and uh,
uh,

372
00:19:55,000 --> 00:20:00,000
in,
in all the deep learning literature and 

373
00:20:00,000 --> 00:20:01,680
cvpr conferences and nips,
everything I'm super excited about is 

374
00:20:01,680 --> 00:20:06,550
all grounded in making it safe and help 
people.

375
00:20:06,770 --> 00:20:11,770
So I just don't see how that trajectory 
can all of a sudden slip into a 

376
00:20:11,770 --> 00:20:13,760
situation where intelligence will be 
highly negative.

377
00:20:13,970 --> 00:20:14,230
I just want

378
00:20:14,790 --> 00:20:19,790
you and I certainly agree on that and I 
think that's only the beginning of the 

379
00:20:19,790 --> 00:20:21,010
potential humanitarian benefits of 
artificial intelligence.

380
00:20:21,730 --> 00:20:26,730
There has been enormous attention to 
what are we going to do with the people 

381
00:20:26,730 --> 00:20:28,450
whose jobs are made obsolete by 
artificial intelligence,

382
00:20:28,810 --> 00:20:33,810
but very little attention given to the 
fact that the jobs that are gonna be 

383
00:20:33,810 --> 00:20:36,721
made obsolete are horrible jobs.
The fact that people aren't going to be 

384
00:20:36,721 --> 00:20:40,500
picking crops and making beds and 
driving trucks and mining coal.

385
00:20:41,050 --> 00:20:46,050
These are soul deadening jobs and we 
have a whole literature sympathizing 

386
00:20:46,050 --> 00:20:50,641
with the people stuck in menial mind 
deadening a dangerous jobs if we can 

387
00:20:52,601 --> 00:20:55,660
eliminate them.
This is a fantastic boon to humanity.

388
00:20:55,720 --> 00:20:56,260
Now,
granted,

389
00:20:56,280 --> 00:20:58,780
we you solve one problem in.
There's another one,

390
00:20:58,781 --> 00:21:02,680
namely how do we get these people a,
a decent income.

391
00:21:02,710 --> 00:21:07,710
But if we're smart enough to invent 
machines that can make beds and put away

392
00:21:07,841 --> 00:21:09,450
dishes and uh,
and,

393
00:21:09,451 --> 00:21:14,451
and handle hospital patients where I 
think we're smart enough to figure out 

394
00:21:14,451 --> 00:21:17,881
how to redistribute income to apportion 
some of the vast economic savings to the

395
00:21:18,821 --> 00:21:20,850
the human beings who will no longer be 
needed to,

396
00:21:20,920 --> 00:21:21,430
to make better.

397
00:21:22,690 --> 00:21:27,690
Okay.
Sam Harris says that it's obvious that 

398
00:21:27,690 --> 00:21:28,600
eventually ai will be an existential 
risk,

399
00:21:29,500 --> 00:21:34,500
is one of the people says it's obvious.
We don't know when the claim goes,

400
00:21:35,411 --> 00:21:39,670
but eventually it's obvious and because 
we don't know when we should worry about

401
00:21:39,671 --> 00:21:44,671
it now,
it's a very interesting argument in my 

402
00:21:44,671 --> 00:21:44,671
eyes.
Um,

403
00:21:44,671 --> 00:21:47,820
so how do,
how do we think about timescale?

404
00:21:47,890 --> 00:21:52,890
How do we think about existential 
threats when we don't really know so 

405
00:21:52,890 --> 00:21:54,400
little about the threat,
unlike nuclear weapons,

406
00:21:54,401 --> 00:21:59,401
perhaps about this particular threat 
that it could happen tomorrow,

407
00:21:59,950 --> 00:22:00,910
right?
So,

408
00:22:01,120 --> 00:22:04,630
but very likely won't likely to be 100 
years away.

409
00:22:04,631 --> 00:22:07,120
So how do we ignore it?
Do,

410
00:22:07,180 --> 00:22:10,100
how do we talk about it?
Do we worry about it?

411
00:22:10,110 --> 00:22:11,710
So what,
how do we think about those?

412
00:22:11,740 --> 00:22:12,420
W W,
W,

413
00:22:12,430 --> 00:22:13,240
W,
what is it,

414
00:22:13,720 --> 00:22:18,430
a threat that we can imagine it's within
the limits of our imagination,

415
00:22:18,700 --> 00:22:23,700
but not within our limits of 
understanding to sufficient to 

416
00:22:23,700 --> 00:22:23,700
accurately predict it.

417
00:22:23,970 --> 00:22:24,790
Uh,
but what,

418
00:22:24,830 --> 00:22:25,710
what is,
what is the

419
00:22:26,620 --> 00:22:28,160
AI,
ai,

420
00:22:28,890 --> 00:22:30,750
ai being the existential threat ai,

421
00:22:31,280 --> 00:22:34,640
how enslaving us or turning us into 
paperclips?

422
00:22:35,270 --> 00:22:40,270
I think the most compelling from the Sam
Harris' perspective would be the 

423
00:22:40,270 --> 00:22:40,270
paperclip situation.
Yeah,

424
00:22:40,270 --> 00:22:40,540
I mean,
I think,

425
00:22:40,541 --> 00:22:41,870
I just think it's totally fanciful.
Ivy.

426
00:22:41,900 --> 00:22:44,010
I don't build a system.
Don't give,

427
00:22:44,800 --> 00:22:45,830
don't.
First of all,

428
00:22:46,240 --> 00:22:51,240
uh,
the code of engineering is you don't 

429
00:22:51,240 --> 00:22:51,680
implement a system with massive control 
before testing it.

430
00:22:52,040 --> 00:22:57,040
Now perhaps the culture of engineering 
will radically change then I would 

431
00:22:57,040 --> 00:22:57,980
worry,
but I don't see any signs that engineers

432
00:22:57,981 --> 00:23:01,420
will suddenly do idiotic things like put
a,

433
00:23:01,740 --> 00:23:06,740
uh,
an electrical power plant in control of 

434
00:23:06,740 --> 00:23:06,740
a system that they haven't tested,
uh,

435
00:23:06,740 --> 00:23:07,350
first,
uh,

436
00:23:07,400 --> 00:23:11,150
or all of these scenarios.
Not only imagine a,

437
00:23:11,580 --> 00:23:16,580
um,
almost a magically powered intelligence 

438
00:23:16,580 --> 00:23:20,650
including things like cure cancer,
which is probably an incoherent goal 

439
00:23:20,650 --> 00:23:24,290
because there's so many different kinds 
of cancer or bring about world peace.

440
00:23:24,350 --> 00:23:26,270
I mean,
how do you even specify that as a goal,

441
00:23:26,630 --> 00:23:31,630
but the scenario is also imagined some 
degree of control of every molecule in 

442
00:23:31,630 --> 00:23:32,780
the universe,
uh,

443
00:23:32,781 --> 00:23:37,781
which not only is itself unlikely,
but we would not start to connect these 

444
00:23:37,791 --> 00:23:42,791
systems to a infrastructure without a,
without testing as we would any kind of 

445
00:23:45,320 --> 00:23:45,770
system.

446
00:23:45,810 --> 00:23:49,640
There may be some engineers will be 
you're responsible and we need legal and

447
00:23:50,760 --> 00:23:55,760
a regulatory and legal responsibility 
implemented so that engineers don't do 

448
00:23:57,710 --> 00:23:59,660
things that are stupid by their own 
standards.

449
00:24:00,720 --> 00:24:01,800
But the,
uh,

450
00:24:02,100 --> 00:24:07,100
uh,
I've never seen enough of a plausible 

451
00:24:07,100 --> 00:24:09,741
scenario of a existential threat to 
devote large amounts of brain power to,

452
00:24:10,350 --> 00:24:12,940
to forestall it.
You believe in

453
00:24:13,410 --> 00:24:18,410
sort of the power and mass of the 
engineering of reason as the arguing 

454
00:24:18,410 --> 00:24:22,460
your latest book of reason and science 
and sort of be the very thing that 

455
00:24:22,590 --> 00:24:24,800
guides the development of new 
technology.

456
00:24:24,801 --> 00:24:26,580
So it's safe and also keeps us safe.

457
00:24:26,660 --> 00:24:28,160
Yeah.
If the same and you know,

458
00:24:28,310 --> 00:24:33,310
granted that same culture of safety that
currently is part of the engineering 

459
00:24:33,680 --> 00:24:35,960
mindset for airplanes,
for example.

460
00:24:36,290 --> 00:24:37,760
So yeah,
I don't think that,

461
00:24:37,820 --> 00:24:38,510
that,
uh,

462
00:24:38,511 --> 00:24:43,511
that,
that should be thrown out the window 

463
00:24:43,511 --> 00:24:43,511
and,
uh,

464
00:24:43,511 --> 00:24:45,610
that untested,
all powerful systems should be suddenly 

465
00:24:45,610 --> 00:24:46,280
implemented,
but there's no reason to think they are.

466
00:24:46,290 --> 00:24:51,290
And in fact,
if you look at the progress of 

467
00:24:51,290 --> 00:24:51,290
artificial intelligence,
it's been,

468
00:24:51,290 --> 00:24:53,090
it's been impressive,
especially in the last 10 years or so,

469
00:24:53,330 --> 00:24:58,330
but the idea that suddenly there'll be a
step function that all of a sudden 

470
00:24:58,330 --> 00:24:58,700
before we know it,
it will be,

471
00:24:59,610 --> 00:25:00,270
oh,
powerful.

472
00:25:00,271 --> 00:25:02,790
That there'll be some kind of recursive 
self improvement,

473
00:25:02,791 --> 00:25:04,800
some kind of a foom,
uh,

474
00:25:04,801 --> 00:25:08,040
is also a fanciful.
We certainly,

475
00:25:08,041 --> 00:25:12,270
by the technology that we,
that we're now impresses us,

476
00:25:12,271 --> 00:25:17,271
such as deep learning where you train 
something on a hundreds of thousands or 

477
00:25:17,271 --> 00:25:21,771
millions of examples.
They're not hundreds of thousands of 

478
00:25:21,771 --> 00:25:25,410
problems of which curing cancer is a 
typical example.

479
00:25:25,850 --> 00:25:30,850
Uh,
and so the kind of techniques that have 

480
00:25:30,850 --> 00:25:32,841
allowed a ai too increased in the last 
five years are not the kind that are 

481
00:25:32,841 --> 00:25:35,880
gonna lead to this fantasy of,
uh,

482
00:25:35,881 --> 00:25:38,550
of,
of exponential sudden self improvement.

483
00:25:38,730 --> 00:25:41,550
So it's,
I think it's kind of a magical thinking.

484
00:25:41,551 --> 00:25:45,410
It's not based on our understanding of 
how ai actually works.

485
00:25:45,560 --> 00:25:48,410
Now give me a chance here.
So you said fanciful,

486
00:25:48,411 --> 00:25:51,200
magical thinking.
In his Ted talk,

487
00:25:51,201 --> 00:25:56,201
Sam Harris says that thinking about ai 
killing all human civilization is 

488
00:25:56,201 --> 00:25:59,861
somehow fun intellectually.
Now I have to say as a scientist 

489
00:25:59,861 --> 00:26:00,740
engineer,
I don't find it fun,

490
00:26:01,370 --> 00:26:04,540
but when I'm having beer with my 
friends,

491
00:26:05,000 --> 00:26:08,850
there is indeed something fun and 
appealing about it.

492
00:26:08,851 --> 00:26:10,790
Like talking about an episode of Black 
Mirror,

493
00:26:10,791 --> 00:26:14,930
considering if a large media or is 
headed towards her,

494
00:26:14,990 --> 00:26:17,780
we were just told a large media is 
headed towards earth,

495
00:26:18,290 --> 00:26:23,290
something like this.
And can you relate to this sense of fun 

496
00:26:23,290 --> 00:26:24,580
and do you understand the psychology of 
it?

497
00:26:25,450 --> 00:26:26,650
Good question.
Uh,

498
00:26:26,670 --> 00:26:28,790
I,
I personally don't find it fun.

499
00:26:28,810 --> 00:26:33,810
Um,
I find it kind of a actually a waste of 

500
00:26:33,810 --> 00:26:37,150
time because there are genuine threats 
that we ought to be thinking about,

501
00:26:37,300 --> 00:26:38,530
like,
like pandemics,

502
00:26:38,590 --> 00:26:41,710
like,
like a cybersecurity vulnerabilities.

503
00:26:42,240 --> 00:26:47,240
I'm like the possibility of nuclear war 
and certainly climate change is enough 

504
00:26:47,821 --> 00:26:49,730
to fill that many,
uh,

505
00:26:50,160 --> 00:26:52,140
uh,
conversations without.

506
00:26:52,390 --> 00:26:57,390
And I think they're,
I think Sam did put his finger on 

507
00:26:57,390 --> 00:26:57,390
something,
namely that there is a community,

508
00:26:57,680 --> 00:27:02,680
uh,
sometimes called the rationality 

509
00:27:02,680 --> 00:27:05,270
community that delights in using it's 
brain power to come up with scenarios 

510
00:27:06,121 --> 00:27:08,930
that would not occur to me are mortals 
to,

511
00:27:08,960 --> 00:27:13,960
to so the less cerebral people.
So there is a kind of intellectual 

512
00:27:13,960 --> 00:27:18,351
thrill and finding new things to worry 
about that no one has a worried about 

513
00:27:18,351 --> 00:27:19,050
yet.
I actually think though,

514
00:27:19,051 --> 00:27:24,051
that it's not only,
that is a kind of fun that doesn't give 

515
00:27:24,051 --> 00:27:24,051
me particular pleasure,
uh,

516
00:27:24,090 --> 00:27:26,490
but I think there can be a pernicious 
side to it,

517
00:27:26,520 --> 00:27:30,570
namely that you overcome people with 
such a dread,

518
00:27:30,630 --> 00:27:33,590
such fatalism that there's so many ways 
to,

519
00:27:33,640 --> 00:27:34,350
to,
uh,

520
00:27:34,540 --> 00:27:35,280
to,
to die,

521
00:27:35,281 --> 00:27:39,840
to annihilate our civilization that we 
may as well enjoy life while we can.

522
00:27:39,840 --> 00:27:42,360
There's nothing we can do about it.
If climate change doesn't do us in,

523
00:27:42,390 --> 00:27:44,580
then runaway robots will.
So,

524
00:27:44,581 --> 00:27:45,320
uh,
what,

525
00:27:45,330 --> 00:27:48,510
let's enjoy ourselves now,
we got to prioritize.

526
00:27:48,910 --> 00:27:53,910
We have to look at threats that are 
close to certainty,

527
00:27:54,421 --> 00:27:57,900
such as climate change,
and distinguish those from ones that are

528
00:27:57,901 --> 00:28:01,080
merely imaginable.
But with infinitesimal probabilities,

529
00:28:02,010 --> 00:28:05,760
and we have to take into account 
people's worry budget,

530
00:28:05,940 --> 00:28:10,940
you can't worry about everything and if 
you so dread and fear and terror and 

531
00:28:11,311 --> 00:28:16,311
numb and fatalism,
it can lead to a kind of numbness while 

532
00:28:16,311 --> 00:28:19,521
they just,
these problems are overwhelming and the 

533
00:28:19,521 --> 00:28:19,521
engineers are just going to kill us all.
Um,

534
00:28:19,521 --> 00:28:20,010
so,
uh,

535
00:28:20,190 --> 00:28:25,190
let's either destroy the entire 
infrastructure of science technology or 

536
00:28:27,170 --> 00:28:29,450
let's just enjoy life while we can.

537
00:28:29,670 --> 00:28:34,670
So there's a certain line of worrying,
which I'm worried about a lot of things 

538
00:28:34,670 --> 00:28:35,550
engineering,
there's a certain line of worry when you

539
00:28:35,551 --> 00:28:40,551
cross a law to cross a that it becomes 
paralyzing fear as opposed to productive

540
00:28:42,171 --> 00:28:47,171
here.
And that's kind of what I'm 

541
00:28:47,171 --> 00:28:47,171
highlighting.

542
00:28:47,171 --> 00:28:47,171
Exactly right.
And,

543
00:28:47,171 --> 00:28:47,960
and we've seen some,
uh,

544
00:28:48,090 --> 00:28:53,090
we know that human effort is not well 
calibrated against risk in that because 

545
00:28:55,000 --> 00:29:00,000
a basic tenant of cognitive psychology 
is that a perception of risk and his 

546
00:29:00,931 --> 00:29:05,931
perception of fear.
It's driven by imagine ability not by 

547
00:29:05,931 --> 00:29:05,931
data.
Uh,

548
00:29:05,970 --> 00:29:10,970
and so we miss allocate fast amounts of 
resources to avoiding terrorism,

549
00:29:11,340 --> 00:29:14,760
which kills on average about six 
Americans a year with a one exception of

550
00:29:14,761 --> 00:29:15,840
nine slash 11.
Uh,

551
00:29:15,870 --> 00:29:20,870
we invade countries,
we invent an entire new departments of 

552
00:29:20,870 --> 00:29:25,191
government with massive,
massive expenditure of resources in 

553
00:29:25,191 --> 00:29:27,690
life's to defend ourselves against a 
trivial risk,

554
00:29:28,110 --> 00:29:30,840
uh,
whereas guaranteed risks,

555
00:29:30,870 --> 00:29:34,140
and you mentioned as one of them is,
you mentioned traffic fatalities,

556
00:29:34,480 --> 00:29:37,080
uh,
and even risks that are,

557
00:29:37,600 --> 00:29:42,600
I'm not here but are plausible enough to
worry about like pandemics,

558
00:29:44,350 --> 00:29:48,610
like nuclear war received far too little
attention,

559
00:29:48,750 --> 00:29:52,720
the in presidential debates.
There's no discussion of how to minimize

560
00:29:52,721 --> 00:29:54,190
the risk of,
of nuclear war,

561
00:29:54,250 --> 00:29:56,830
lots of discussion of terrorism,
for example.

562
00:29:57,060 --> 00:29:58,180
Uh,
and,

563
00:29:58,250 --> 00:30:03,250
and so we,
I think it's essential to calibrate our 

564
00:30:03,250 --> 00:30:04,090
budget of fear,
worry,

565
00:30:04,091 --> 00:30:06,580
concern,
planning to the,

566
00:30:06,581 --> 00:30:09,200
uh,
actual probability of a,

567
00:30:09,201 --> 00:30:09,730
of harm.

568
00:30:10,160 --> 00:30:12,150
Yep.
So let me ask this then,

569
00:30:12,151 --> 00:30:15,560
this question.
So speaking of imagine an ability,

570
00:30:15,890 --> 00:30:17,990
you said that it's important to think 
about reason.

571
00:30:18,320 --> 00:30:23,320
And one of my favorite people who,
who likes to dip into the outskirts of 

572
00:30:23,320 --> 00:30:24,130
reason,
uh,

573
00:30:24,140 --> 00:30:28,070
through fascinating exploration of his 
imagination is Joe Rogan.

574
00:30:28,430 --> 00:30:29,660
Oh yes.
Uh,

575
00:30:29,661 --> 00:30:30,440
you,
uh,

576
00:30:30,470 --> 00:30:35,470
so who has three reasons to believe a 
lot of conspiracies and through a reason

577
00:30:35,751 --> 00:30:38,780
has stripped away a lot of his beliefs 
in that way.

578
00:30:38,781 --> 00:30:42,560
So it's fascinating actually to watch 
him through rationality.

579
00:30:42,620 --> 00:30:45,650
Kinda throw away the ideas.
Have a big foot.

580
00:30:45,651 --> 00:30:47,270
And um,
nine slash 11,

581
00:30:47,300 --> 00:30:49,880
I'm not sure exactly the trails.
I don't know what he believes,

582
00:30:50,690 --> 00:30:52,780
but he no longer believed in.
That's all right.

583
00:30:52,810 --> 00:30:53,200
No,
he's,

584
00:30:53,280 --> 00:30:54,820
he's become a real force for,
uh,

585
00:30:55,100 --> 00:30:55,790
for Greg.
Yep.

586
00:30:56,030 --> 00:31:01,030
So you were on the Joe Rogan podcast in 
February and had a fascinating 

587
00:31:01,030 --> 00:31:01,760
conversation.
But as far as I remember,

588
00:31:01,761 --> 00:31:04,160
didn't talk much about artificial 
intelligence.

589
00:31:04,760 --> 00:31:06,890
I will be on this podcast and a couple 
weeks,

590
00:31:07,370 --> 00:31:11,030
joe is very much concerned about 
existential threat of Ai.

591
00:31:11,031 --> 00:31:16,031
I'm not sure if you're a.
This is why I was hoping that you will 

592
00:31:16,031 --> 00:31:17,150
get into that topic.
And in this way,

593
00:31:17,151 --> 00:31:21,350
he represents quite a lot of people who 
look at the topic of Ai from 10,000

594
00:31:21,351 --> 00:31:26,351
foot level.
So as an exercise of a communication,

595
00:31:26,840 --> 00:31:29,930
you said it's important to be rational 
and reason about these things.

596
00:31:30,200 --> 00:31:35,200
Let me ask,
if you were to coach me as an ai 

597
00:31:35,200 --> 00:31:36,920
researcher about how to speak to Joe and
the general public about Ai,

598
00:31:36,950 --> 00:31:38,120
what would you advise?

599
00:31:38,370 --> 00:31:38,920
Well,
like,

600
00:31:38,990 --> 00:31:43,990
uh,
the short answer would be to read the 

601
00:31:43,990 --> 00:31:43,990
sections that I wrote in an enlightened 
about ai,

602
00:31:44,160 --> 00:31:46,440
but longer reason would be I think to 
emphasize,

603
00:31:46,530 --> 00:31:51,530
and I think you're very well positioned 
as an engineer to remind people about 

604
00:31:51,530 --> 00:31:54,450
the culture of engineering,
that it really is a safety oriented.

605
00:31:54,870 --> 00:31:59,520
That another discussion in enlightenment
now I plot,

606
00:32:00,010 --> 00:32:05,010
uh,
rates of accidental death from various 

607
00:32:05,010 --> 00:32:05,010
causes,
plane crashes,

608
00:32:05,010 --> 00:32:07,270
car crashes,
occupational accidents,

609
00:32:07,360 --> 00:32:12,360
even death by lightning strikes.
And they all plummet because the culture

610
00:32:13,421 --> 00:32:15,870
of engineering is how do you squeeze out
the,

611
00:32:15,871 --> 00:32:16,100
uh,
the,

612
00:32:16,110 --> 00:32:17,950
the lethal risks,
death by fire,

613
00:32:17,951 --> 00:32:21,800
death by drowning,
death by a fixation.

614
00:32:21,880 --> 00:32:25,000
All them drastically declined because of
advances in engineering.

615
00:32:25,180 --> 00:32:30,180
Then I got to say,
I did not appreciate until I saw those 

616
00:32:30,180 --> 00:32:33,241
graphs,
and it is because exactly people like 

617
00:32:33,241 --> 00:32:34,120
you who stay up at night thinking,
oh my God,

618
00:32:34,190 --> 00:32:35,740
is what am I,
what I'm,

619
00:32:36,090 --> 00:32:41,090
what I'm inventing likely to hurt people
and to deploy ingenuity to prevent that 

620
00:32:41,991 --> 00:32:43,760
from happening.
Now I'm not an engineer,

621
00:32:44,030 --> 00:32:47,600
although I spent 22 years at Mit,
so I know something about the culture of

622
00:32:47,601 --> 00:32:49,820
engineering,
my understanding that this is the way,

623
00:32:50,150 --> 00:32:55,150
this is what you think if you're an 
engineer and it's essential that that 

624
00:32:55,150 --> 00:32:59,060
culture not be suddenly switched off 
when it comes to artificial intelligence

625
00:32:59,120 --> 00:32:59,980
so that,
that,

626
00:33:00,050 --> 00:33:05,050
that could be a problem.
But is there any reason to think it 

627
00:33:05,050 --> 00:33:05,050
would be switched off?

628
00:33:05,050 --> 00:33:06,330
Think so and one.
There's not enough engineers speaking up

629
00:33:06,360 --> 00:33:07,890
for this way,
for this.

630
00:33:07,900 --> 00:33:09,920
The excitement for,
uh,

631
00:33:10,200 --> 00:33:13,580
the positive view of human nature,
what you're trying to create,

632
00:33:13,660 --> 00:33:18,660
the positivity,
like everything we try to invent is 

633
00:33:18,660 --> 00:33:21,411
trying to do good for the world.
But let me ask you about the psychology 

634
00:33:21,411 --> 00:33:25,191
of negativity.
It seems just objectively not 

635
00:33:25,191 --> 00:33:28,971
considering the topic.
It seems that being negative about the 

636
00:33:28,971 --> 00:33:31,701
future,
it makes you sound smarter than being 

637
00:33:31,701 --> 00:33:31,830
positive about the future.
It regardless of topic.

638
00:33:31,860 --> 00:33:35,160
Am I correct in this observation?
And if so,

639
00:33:35,220 --> 00:33:35,940
why do you think

640
00:33:35,940 --> 00:33:36,540
that is?
Yeah,

641
00:33:36,541 --> 00:33:41,541
I think I,
I think there is that a phenomenon that 

642
00:33:41,541 --> 00:33:42,210
as Tom Lehrer,
the Sandra said,

643
00:33:42,211 --> 00:33:44,540
always predict the worst and you'll be 
hailed as a profit.

644
00:33:45,850 --> 00:33:49,170
Maybe part of our overall negativity 
bias.

645
00:33:49,171 --> 00:33:54,171
We are,
as a species more attuned to the 

646
00:33:54,171 --> 00:33:56,250
negative than the positive.
We dread losses more than we enjoy gains

647
00:33:57,110 --> 00:34:02,110
and uh,
that may might open up a space for a 

648
00:34:02,540 --> 00:34:06,800
profits to remind us of harms and risks 
and losses that we may have overlooked.

649
00:34:07,310 --> 00:34:08,510
Uh,
so I think they're,

650
00:34:08,511 --> 00:34:09,450
they're,
uh,

651
00:34:09,830 --> 00:34:11,480
they're there.
Is that a symmetry?

652
00:34:11,990 --> 00:34:16,990
So you've written some of my favorite 
books all over the place.

653
00:34:17,091 --> 00:34:19,940
So starting from enlightenment now to,
uh,

654
00:34:19,941 --> 00:34:22,400
the better angels of our nature.
Blank slate,

655
00:34:22,401 --> 00:34:27,401
how the mind works,
the one about language language 

656
00:34:27,401 --> 00:34:30,971
instinct,
a Bill Gates big fan to a set of your 

657
00:34:31,731 --> 00:34:35,810
most recent book that it's my new 
favorite book of all time.

658
00:34:37,460 --> 00:34:42,460
So for you as an author,
what was the book early on in your life 

659
00:34:42,460 --> 00:34:44,930
that had a profound impact on the way 
you saw the world?

660
00:34:45,460 --> 00:34:50,460
Certainly this book enlightened me now 
is influenced by David Deutsch is the 

661
00:34:50,460 --> 00:34:55,321
beginning of infinity,
have rather deep reflection on a 

662
00:34:55,321 --> 00:34:58,570
knowledge and the power of knowledge to 
improve the human condition.

663
00:34:58,980 --> 00:35:03,980
Uh,
the end with bits of wisdom such as the 

664
00:35:03,980 --> 00:35:06,810
problems are inevitable,
but problems are solvable given the 

665
00:35:06,810 --> 00:35:09,891
right knowledge and that solutions 
create new problems that have to be 

666
00:35:09,891 --> 00:35:09,891
solved in their turn.
That's,

667
00:35:10,050 --> 00:35:15,050
I think,
a kind of wisdom about the human 

668
00:35:15,050 --> 00:35:15,050
condition that influenced the writing of
this book.

669
00:35:15,050 --> 00:35:16,980
There's some books that are excellent 
but obscure,

670
00:35:17,040 --> 00:35:18,620
some of which I have on my,
uh,

671
00:35:18,650 --> 00:35:23,650
I'm on a page of my website and I read a
book called the history of force self 

672
00:35:23,650 --> 00:35:27,510
published by a political scientist named
James Payne on the historical decline of

673
00:35:27,511 --> 00:35:29,640
violence.
And that was one of the inspirations for

674
00:35:29,641 --> 00:35:32,100
the better angels of our nature.
Uh,

675
00:35:32,610 --> 00:35:33,630
the,
um,

676
00:35:33,720 --> 00:35:36,990
what about early on back when you're 
maybe a,

677
00:35:38,010 --> 00:35:39,720
is a book called one,
two,

678
00:35:39,721 --> 00:35:42,150
three infinity.
When I was a young adult,

679
00:35:42,270 --> 00:35:45,010
I read that book by George Gamma off the
physicist,

680
00:35:45,570 --> 00:35:50,570
very accessible and humorous 
explanations of relativity of a number 

681
00:35:51,121 --> 00:35:56,121
theory of a dimentionality high,
multiple dimensional spaces,

682
00:35:58,290 --> 00:35:58,880
uh,
in a,

683
00:35:58,881 --> 00:36:00,720
in a way that I think is still 
delightful.

684
00:36:00,900 --> 00:36:05,900
Seventy years after it was published,
I liked the time life science series.

685
00:36:06,210 --> 00:36:11,210
These were books that would arrive every
month that my mother subscribe to each 

686
00:36:11,210 --> 00:36:13,440
one on a different topic.
A one would be on,

687
00:36:13,570 --> 00:36:18,570
on electricity.
What would it be on forests wonder 

688
00:36:18,570 --> 00:36:21,520
beyond my evolution.
And then one was on the mind and I was 

689
00:36:21,520 --> 00:36:24,980
just intrigued that there could be a 
science of mind and that that book I 

690
00:36:24,980 --> 00:36:27,360
would a site as an influence as well.
Then later on,

691
00:36:27,361 --> 00:36:29,880
you fell in love with the idea of 
studying the mind.

692
00:36:29,910 --> 00:36:31,350
It was,
that's one thing that grabbed you.

693
00:36:31,560 --> 00:36:33,990
It was one of the things I would say,
um,

694
00:36:34,020 --> 00:36:34,650
the,
uh,

695
00:36:34,950 --> 00:36:35,700
I read,
uh,

696
00:36:35,710 --> 00:36:40,200
as a college student.
The book reflections on language by Noam

697
00:36:40,200 --> 00:36:42,450
Chomsky spent most of his career here at
Mit.

698
00:36:43,270 --> 00:36:44,730
Uh,
Richard Dawkins,

699
00:36:44,910 --> 00:36:49,910
two books,
the blind watchmaker and the selfish 

700
00:36:49,910 --> 00:36:52,050
gene were enormously influential partly 
for mainly for the content,

701
00:36:52,051 --> 00:36:57,051
but also for the writing style,
the ability to explain abstract concepts

702
00:36:58,020 --> 00:37:03,020
in lively pros,
Stephen j Dot Gould's first collection 

703
00:37:03,020 --> 00:37:06,180
ever since Darwin.
Also a excellent example of,

704
00:37:06,210 --> 00:37:09,180
of our life.
We writing a George Miller,

705
00:37:09,270 --> 00:37:14,270
psychologists that most psychologists 
that are familiar with came up with the 

706
00:37:14,270 --> 00:37:18,291
idea that human memory has a capacity of
a seven plus or minus two chunks and 

707
00:37:18,490 --> 00:37:23,490
supposedly his biggest claim to fame,
but he wrote a couple of books on 

708
00:37:23,490 --> 00:37:24,150
language and communication that I had 
made as an undergraduate.

709
00:37:24,330 --> 00:37:26,310
Again,
beautifully written and,

710
00:37:26,340 --> 00:37:27,690
uh,
intellectually deep.

711
00:37:28,530 --> 00:37:30,210
Wonderful.
Steven,

712
00:37:30,211 --> 00:37:31,800
thank you so much for taking the time 
today.

713
00:37:32,040 --> 00:37:32,970
My pleasure.
Thanks a lot.

