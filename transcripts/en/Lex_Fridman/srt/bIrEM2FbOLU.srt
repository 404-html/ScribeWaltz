1
00:00:00,060 --> 00:00:02,460
The following is a conversation
with Greg Brockman.

2
00:00:02,940 --> 00:00:07,470
He's the Co founder and Cto of open Ai,
a world class research organization,

3
00:00:07,471 --> 00:00:12,471
developing ideas and AI with a goal of
eventually creating a safe and friendly

4
00:00:12,960 --> 00:00:15,030
artificial general intelligence,

5
00:00:15,390 --> 00:00:18,150
one that benefits and empowers humanity.

6
00:00:18,860 --> 00:00:23,090
Open Ai is not only a source of
publications, algorithms, tools,

7
00:00:23,091 --> 00:00:24,060
and datasets.

8
00:00:24,460 --> 00:00:29,460
Their mission is a catalyst
for an important public
discourse about our future

9
00:00:29,780 --> 00:00:33,410
with both narrow and general
intelligence systems.

10
00:00:34,010 --> 00:00:38,480
This conversation is part of
the artificial intelligence
podcasts at MIT and

11
00:00:38,481 --> 00:00:42,590
beyond. If you enjoy,
subscribe on Youtube, iTunes,

12
00:00:42,740 --> 00:00:47,660
or simply connect with me on Twitter
at Lex Friedman, spelled f, r I. D.

13
00:00:48,050 --> 00:00:51,680
And now here's my conversation
with Greg Brockman.

14
00:00:52,770 --> 00:00:56,250
So in high school, and right after he
wrote a draft of a chemistry textbook,

15
00:00:56,670 --> 00:01:00,240
I saw that that covers everything from
basic structure of the atom to quantum

16
00:01:00,241 --> 00:01:05,190
mechanics. So it's clear you have an
intuition and a passion for both the,

17
00:01:05,191 --> 00:01:06,024
uh,

18
00:01:06,030 --> 00:01:11,030
the physical world with chemistry and
now robotics to the digital world with a

19
00:01:12,510 --> 00:01:14,820
AI deep learning reinforcement learning.
So on.

20
00:01:15,360 --> 00:01:18,390
Do you see the physical world and
the digital world is different?

21
00:01:18,600 --> 00:01:19,860
And what do you think is the gap?

22
00:01:20,510 --> 00:01:23,330
A lot of it actually boils down
to iteration. Speed, right?

23
00:01:23,331 --> 00:01:26,510
That I think that a lot of what really
motivates me is building things right?

24
00:01:26,511 --> 00:01:29,300
Is the, uh, you know, think
about mathematics for example,

25
00:01:29,301 --> 00:01:30,710
where you're thinking
really hard about a problem.

26
00:01:30,860 --> 00:01:33,300
You understand that you're right
down in this period of secure form.

27
00:01:33,320 --> 00:01:37,610
They call it proof. But then this
is in humanities library, right?

28
00:01:37,611 --> 00:01:40,880
It's there forever. This is some truth
that we've discovered and you know,

29
00:01:40,881 --> 00:01:42,740
maybe only five people in
your field will ever read it,

30
00:01:43,030 --> 00:01:44,840
but somehow you've kind
of move humanity forward.

31
00:01:45,440 --> 00:01:48,380
And so I actually used to really think
that I was going to be a mathematician

32
00:01:48,620 --> 00:01:51,530
and a then I actually started
writing this chemistry textbook.

33
00:01:51,620 --> 00:01:54,620
One of my friends told me, you'll never
publish it because you don't have a phd.

34
00:01:54,830 --> 00:01:59,630
So instead I decided to build a website
and try to promote my ideas that way.

35
00:01:59,930 --> 00:02:02,270
And then I discovered
programming. And, uh,

36
00:02:02,960 --> 00:02:06,050
you know that in programming you think
hard about a problem, you understand it,

37
00:02:06,051 --> 00:02:09,200
you're right down in a very obscure
a form that we call a program.

38
00:02:09,980 --> 00:02:12,170
But then once again, it's an
humanities library, right?

39
00:02:12,200 --> 00:02:15,470
And anyone could get the benefit from
it and the scalability is massive.

40
00:02:15,680 --> 00:02:18,860
And so I think that the thing that really
appeals to me about the digital world

41
00:02:19,040 --> 00:02:21,950
is that you can have this, this,
this insane leverage, right?

42
00:02:21,951 --> 00:02:26,110
As single individual with an idea is
able to affect the entire planet. Um,

43
00:02:26,220 --> 00:02:29,050
and that's something I think is really
hard to do if you're moving around

44
00:02:29,070 --> 00:02:29,930
physical atoms.

45
00:02:30,200 --> 00:02:33,300
But you said a mathematics.
So if you look at the,

46
00:02:33,510 --> 00:02:36,440
the wet thing over here,
our mind,

47
00:02:36,830 --> 00:02:41,830
do you ultimately see it as just math
has just information processing or is

48
00:02:41,991 --> 00:02:44,330
there some other magic as you've seen,

49
00:02:44,390 --> 00:02:46,670
if you've seen through biology
and chemistry and so on?

50
00:02:47,040 --> 00:02:49,770
I think it's really interesting to
think about humans as just information

51
00:02:49,771 --> 00:02:54,150
processing systems and that it seems
like it's actually a pretty good way of

52
00:02:54,151 --> 00:02:55,330
describing a lot of,

53
00:02:55,331 --> 00:02:59,860
of kind of how the world works are a lot
of what we're capable of. To think that,

54
00:03:00,110 --> 00:03:03,520
you know, again, if you just look at
technological innovations over time,

55
00:03:03,570 --> 00:03:06,290
that in some ways the most
transformative innovation that we've had,

56
00:03:06,291 --> 00:03:08,770
it has been the computer, right?
In some ways the internet,

57
00:03:08,800 --> 00:03:10,540
you know that what is done right?

58
00:03:10,541 --> 00:03:12,520
The Internet is not about
these physical cables.

59
00:03:12,670 --> 00:03:16,090
It's about the fact that I am suddenly
able to instantly communicate with any

60
00:03:16,091 --> 00:03:17,110
other human on the planet.

61
00:03:17,620 --> 00:03:21,430
I'm able to retrieve any
piece of knowledge that in
some ways the human race has

62
00:03:21,431 --> 00:03:25,930
ever had a, and those are
these insane transformations.

63
00:03:26,080 --> 00:03:26,950
Do you see the

64
00:03:27,020 --> 00:03:28,310
our society as a whole,

65
00:03:28,311 --> 00:03:32,090
the collective as another extension of
the intelligence of the human being.

66
00:03:32,240 --> 00:03:34,880
So if you look at the human beings
and information processing system,

67
00:03:35,060 --> 00:03:36,710
you mentioned the Internet and networking.

68
00:03:36,900 --> 00:03:39,860
Do you see us all together
as a civilization, as a,

69
00:03:39,861 --> 00:03:41,360
as a kind of intelligence system?

70
00:03:41,640 --> 00:03:42,060
Yeah,

71
00:03:42,060 --> 00:03:45,120
I think this is actually a really
interesting perspective have to take in to

72
00:03:45,121 --> 00:03:49,320
think about the, you sort of have this
collective intelligence of all of society.

73
00:03:49,500 --> 00:03:54,400
The economy itself is the
superhuman machine that is
optimizing something, right?

74
00:03:54,401 --> 00:03:57,970
And it's all in some ways a company
has a will of its own right,

75
00:03:57,971 --> 00:04:00,490
that you have all these individuals who
are all pursuing their own individual

76
00:04:00,491 --> 00:04:03,460
goals and thinking really hard and
thinking about the right things to do.

77
00:04:03,610 --> 00:04:07,450
But somehow the company does
something that is this emergent thing.

78
00:04:07,860 --> 00:04:12,250
And that is a really useful abstraction.
And so I think that in some ways,

79
00:04:12,251 --> 00:04:12,401
you know,

80
00:04:12,401 --> 00:04:15,940
we think of ourselves as
the most intelligent things
on the planet and the most

81
00:04:15,941 --> 00:04:16,990
powerful things on the planet,

82
00:04:17,470 --> 00:04:20,260
but there are things that are bigger
than us through the systems that we all

83
00:04:20,261 --> 00:04:23,110
contribute to. Um, and so I
think actually, you know, it's a,

84
00:04:23,260 --> 00:04:26,830
it's interesting to think about, uh, if
you've read Isaac Asimov's foundation,

85
00:04:27,070 --> 00:04:29,860
right, that, uh, that there's this
concept of psycho history in there,

86
00:04:30,020 --> 00:04:32,880
which is effectively this, that if
you have trillions or quadrillions of,

87
00:04:32,910 --> 00:04:36,370
of beings, then maybe you could
actually predict what that being,

88
00:04:36,500 --> 00:04:39,720
that huge macro being we'll do I, and, uh,

89
00:04:39,880 --> 00:04:41,740
almost independent of
what the individuals want.

90
00:04:42,370 --> 00:04:44,950
I actually have a second angle
on this I think is interesting,

91
00:04:44,951 --> 00:04:47,950
which is thinking about
technological determinism.

92
00:04:48,370 --> 00:04:51,460
One thing that actually thinks a
lot about with, with open Ai, right?

93
00:04:51,461 --> 00:04:55,870
It's that we're kind of coming on onto
this insanely transformational technology

94
00:04:55,871 --> 00:04:58,570
of, of general intelligence, right?
That will happen at some point.

95
00:04:58,930 --> 00:05:02,260
And there's a question of how can you
take actions that will actually steer it

96
00:05:02,261 --> 00:05:06,700
to go better rather than worse? And that,
I think one question you need to ask is,

97
00:05:06,730 --> 00:05:11,380
as a scientist, as an inventor,
as a creator, what impact
can you have in general,

98
00:05:11,500 --> 00:05:11,711
right?

99
00:05:11,711 --> 00:05:14,590
You look at things like the telephone
invented by two people on the same day.

100
00:05:14,830 --> 00:05:17,890
Like what does that mean? Like, what does
that mean about the shape of innovation?

101
00:05:18,250 --> 00:05:20,890
And I think that what's going
on is everyone's building
on the shoulders of the

102
00:05:20,891 --> 00:05:22,720
same giants.
And so you can kind of,

103
00:05:22,870 --> 00:05:25,840
you can't really hope to create something
no one else ever would, you know,

104
00:05:25,900 --> 00:05:28,810
if Einstein wasn't born, someone else
would have come up with relativity.

105
00:05:29,200 --> 00:05:30,960
You know, he changed the
timeline a bit, right?

106
00:05:31,000 --> 00:05:32,800
That maybe it would have
taken another 20 years,

107
00:05:32,980 --> 00:05:36,100
but it wouldn't be that fundamentally
humanity would never discover these,

108
00:05:36,101 --> 00:05:39,400
these fundamental truths.
So there's some kind of invisible

109
00:05:39,740 --> 00:05:44,740
momentum that some people like Einstein
or open the eyes plugging into a that

110
00:05:46,220 --> 00:05:47,810
anybody else can also plug into.

111
00:05:47,811 --> 00:05:51,110
And ultimately that wave takes us into
a certain direction. That's for sure.

112
00:05:51,820 --> 00:05:53,200
That's right. That's right. And you know,

113
00:05:53,201 --> 00:05:55,630
this kind of seems to play out
in a bunch of different ways, uh,

114
00:05:55,660 --> 00:05:56,980
that there's some exponential,

115
00:05:57,080 --> 00:06:00,500
it's being written and that
the exponential itself,
which one it is changes,

116
00:06:00,630 --> 00:06:02,690
think about Moore's law
and an entire industry set.

117
00:06:02,691 --> 00:06:07,160
It's cocked to it for 50 years. Like how
can that be? Right? How is that possible?

118
00:06:07,370 --> 00:06:08,960
And yet somehow it happened.

119
00:06:09,410 --> 00:06:13,130
And so I think you can't hope to ever
invent something that no one else will.

120
00:06:13,310 --> 00:06:15,020
Maybe you can change the
timeline a little bit,

121
00:06:15,320 --> 00:06:17,360
but if you really want
to make a difference,

122
00:06:17,390 --> 00:06:19,280
I think that the thing
that you really have to do,

123
00:06:19,400 --> 00:06:23,240
the only real degree of freedom you have
is to set the initial conditions under

124
00:06:23,241 --> 00:06:26,660
which it technology is born. And so
you think about the Internet, right?

125
00:06:26,661 --> 00:06:29,840
That there are lots of other competitors
trying to build similar things and the

126
00:06:29,841 --> 00:06:30,674
Internet one,

127
00:06:30,740 --> 00:06:34,820
and that the initial conditions where
there was created by this group that

128
00:06:34,821 --> 00:06:38,210
really valued people being
able to be, uh, you know,

129
00:06:38,211 --> 00:06:40,460
anyone being able to plug in
this very academic mindset of,

130
00:06:40,640 --> 00:06:41,810
of being open and connected.

131
00:06:42,470 --> 00:06:45,470
And I think that the Internet for the
next 40 years really played out that way.

132
00:06:45,920 --> 00:06:48,890
Um, you know, maybe today, uh, if
things are starting to shift in a,

133
00:06:48,891 --> 00:06:49,790
in a different direction,

134
00:06:49,791 --> 00:06:52,820
but I think that those initial conditions
were really important to determine the

135
00:06:52,821 --> 00:06:56,420
next 40 years worth of progress.
That's really beautifully put.

136
00:06:56,421 --> 00:06:58,220
So another example of that I think about,

137
00:06:58,800 --> 00:07:01,770
you know, I recently looked
at it, I looked at Wikipedia,

138
00:07:01,771 --> 00:07:03,030
the formation of Wikipedia,

139
00:07:03,780 --> 00:07:07,950
and I wonder what the Internet would be
like if Wikipedia had ads. Hmm. You know,

140
00:07:07,951 --> 00:07:10,200
there's an interesting argument that,
uh,

141
00:07:10,290 --> 00:07:14,790
why they chose not to make it a put
advertisement on Wikipedia. I think it's,

142
00:07:14,850 --> 00:07:18,780
uh, I think be, it is one of the greatest
resources we have on the Internet.

143
00:07:18,870 --> 00:07:22,920
It's as assuming surprising how it works
and how all it was able to aggregate

144
00:07:22,921 --> 00:07:27,270
all this kind of good information and the,
essentially the creator Wikipedia.

145
00:07:27,271 --> 00:07:29,280
I don't know,
there's probably some debates there,

146
00:07:29,281 --> 00:07:32,940
but set the initial conditions
and now it carried itself forward.

147
00:07:33,210 --> 00:07:34,530
That's really interesting.
So you're,

148
00:07:34,860 --> 00:07:38,400
the way you're thinking about Agi or
artificial intelligence is you're focused

149
00:07:38,401 --> 00:07:42,930
on setting the initial conditions for the
progress. That's right. That's powerful.

150
00:07:42,960 --> 00:07:45,120
Okay.
So look into the future.

151
00:07:45,480 --> 00:07:50,480
If you create an AGI system like one
that can ace the Turing test natural

152
00:07:50,521 --> 00:07:51,354
language,

153
00:07:51,510 --> 00:07:55,740
what do you think would be the
interactions you would have with it?

154
00:07:55,800 --> 00:07:57,840
What do you think are the
questions you would ask? Like,

155
00:07:57,841 --> 00:08:02,210
what would be the first question you
would ask it her. Him. That's right.

156
00:08:02,220 --> 00:08:03,840
I think that at that point,

157
00:08:03,870 --> 00:08:07,680
if you've really built a powerful system
that is capable of shaping the future

158
00:08:07,681 --> 00:08:08,430
of humanity,

159
00:08:08,430 --> 00:08:11,100
the first question that you really should
ask is how do we make sure that this

160
00:08:11,101 --> 00:08:14,910
plays out well? Um, and so that's actually
the first question that I would ask.

161
00:08:15,750 --> 00:08:18,330
Powerful AGI system.
So you wouldn't ask,

162
00:08:18,331 --> 00:08:22,450
your colleague wouldn't ask like
Eylea you would ask the Gi System, oh,

163
00:08:22,560 --> 00:08:25,670
we've already had the conversation with
Ilya, right? And everyone here. Uh,

164
00:08:25,680 --> 00:08:27,450
and so you want as many perspectives,

165
00:08:27,760 --> 00:08:30,930
uh, a piece of wisdom as you can
before it for answering this question.

166
00:08:31,170 --> 00:08:34,740
So I don't think you necessarily defer
to whatever your powerful system tells

167
00:08:34,741 --> 00:08:39,720
you, but you use it as one input to try
to figure out what to do. But, and I,

168
00:08:39,740 --> 00:08:42,810
I guess fundamentally what it really
comes down to is if you build something

169
00:08:42,811 --> 00:08:46,230
really powerful and you think about it,
think about, for example, the creation of,

170
00:08:46,260 --> 00:08:48,840
uh, of, uh, shortly after the
creation of nuclear weapons, right?

171
00:08:48,841 --> 00:08:52,560
The most important question the world was,
what's the world are going to be like,

172
00:08:52,740 --> 00:08:56,730
how do we set ourselves up in a place
we're going to be able to survive as a

173
00:08:56,731 --> 00:09:00,600
species with Agi? I think the
question slightly different, right?

174
00:09:00,601 --> 00:09:03,600
That there is a question of how do we
make sure that we don't get the negative

175
00:09:03,601 --> 00:09:07,830
effects, but there's also
the positive side, right? You
imagine that, you know, like,

176
00:09:07,980 --> 00:09:11,100
like what won't Agi be like,
like what will be capable of,

177
00:09:11,340 --> 00:09:15,060
and I think that one of the core
reasons that an Agi can powerful and

178
00:09:15,061 --> 00:09:18,840
transformative is actually due to
technological development, right?

179
00:09:18,841 --> 00:09:22,350
If you have something that's capable as
capable as a human and that it's much

180
00:09:22,351 --> 00:09:23,690
more scalable,
uh,

181
00:09:23,820 --> 00:09:27,480
that you absolutely want that thing to
go read the whole scientific literature

182
00:09:27,600 --> 00:09:29,940
and think about how to create
cures for all the diseases, right?

183
00:09:29,941 --> 00:09:33,630
You want it to think about how to go and
build a technologies to help us create

184
00:09:33,631 --> 00:09:38,040
material abundance and to
figure out societal problems
that we have trouble with.

185
00:09:38,041 --> 00:09:40,500
Like how we're supposed to clean
up the environment. And you know,

186
00:09:40,501 --> 00:09:44,070
maybe you want this to go and invent a
bunch of little robots who will go out

187
00:09:44,071 --> 00:09:48,050
and be biodegradable and
turn ocean debris into heart,

188
00:09:48,070 --> 00:09:52,380
harmless molecules. And,
um, I think that that,

189
00:09:52,820 --> 00:09:56,450
that positive side is something that I
think people miss sometimes when thinking

190
00:09:56,451 --> 00:09:58,310
about what an Agi will be like.
Um,

191
00:09:58,370 --> 00:10:01,400
and so I think that if you have a
system that's capable of all of that,

192
00:10:01,580 --> 00:10:06,080
you absolutely want it to advice about
how do I make sure that we're using your,

193
00:10:06,420 --> 00:10:08,840
uh, your kid buildings in a
positive way for humanity.

194
00:10:09,160 --> 00:10:12,160
So what do you think about
that psychology that, uh,

195
00:10:12,670 --> 00:10:16,090
looks at all the different possible
trajectories of an AGI system,

196
00:10:16,510 --> 00:10:20,890
many of which perhaps the majority of
which are positive and nevertheless

197
00:10:20,891 --> 00:10:24,670
focuses on the negative trajectories.
And then you get to interact with folks.

198
00:10:24,671 --> 00:10:28,700
You get to think about this maybe
within yourself as well. Uh,

199
00:10:28,780 --> 00:10:32,710
you look at Sam Harris and so on. It
seems to be, sorry to put it this way,

200
00:10:32,711 --> 00:10:36,550
but almost more fun to think
about the negative possibilities,

201
00:10:37,780 --> 00:10:39,550
whatever that's deepen our psychology.

202
00:10:39,551 --> 00:10:41,890
What do you think about that
and how do we deal with it?

203
00:10:41,891 --> 00:10:44,080
Because we want AI to help us.

204
00:10:44,370 --> 00:10:49,370
So I think there's kind of two
problems entailed in that question.

205
00:10:49,920 --> 00:10:54,750
The first is more of the question of how
can you even picture what a world with

206
00:10:54,751 --> 00:10:56,730
a new technology will be like.
Now,

207
00:10:56,731 --> 00:10:59,640
imagine we're in 1950 and I'm
trying to describe Uber to someone,

208
00:11:02,840 --> 00:11:07,020
apps and the Internet.
Yeah, I mean that's, that's,

209
00:11:07,280 --> 00:11:10,200
that's going to be extremely complicated,
but it's imaginable cool. It's,

210
00:11:10,240 --> 00:11:14,980
it's imaginable, right? But, and now
imagine being in 1950 and predicting Uber,

211
00:11:15,010 --> 00:11:18,730
right? And you need to describe the
Internet. You need described gps.

212
00:11:18,731 --> 00:11:22,120
You need to describe the
fact that everyone's going
to have this phone in their

213
00:11:22,121 --> 00:11:22,954
pocket.

214
00:11:23,920 --> 00:11:28,190
And so I think that that just the first
truth is that it is hard to picture how

215
00:11:28,191 --> 00:11:31,100
a transformative technology
will play out in the world. Um,

216
00:11:31,150 --> 00:11:34,600
we've seen that before with technologies
that are far less transformative than

217
00:11:34,601 --> 00:11:35,434
Agi will be.

218
00:11:35,560 --> 00:11:39,760
And so I think that that one piece is
that it's just even hard to imagine and to

219
00:11:39,761 --> 00:11:43,160
really put yourself in a world
where you can predict what that,

220
00:11:43,210 --> 00:11:47,580
that positive vision would be like.
And you know,

221
00:11:47,590 --> 00:11:49,450
I think the second thing is that it is,

222
00:11:49,510 --> 00:11:54,510
I think it is always easier to support
the negative side than the positive side.

223
00:11:55,061 --> 00:11:59,680
It's always easier to destroy than create
and you know, less than a, in a, in a,

224
00:11:59,681 --> 00:12:03,490
in a physical science and more just in a,
in an intellectual sense, right? Because,

225
00:12:03,550 --> 00:12:05,650
you know,
I think that that with creating something,

226
00:12:05,651 --> 00:12:08,350
you need to just get a bunch
of things right and to destroy,

227
00:12:08,380 --> 00:12:11,500
you just need to get one thing wrong.
Yeah. And so I think that that,

228
00:12:11,501 --> 00:12:14,500
what that means is that I think a lot of
people's thinking dead ends as soon as

229
00:12:14,501 --> 00:12:18,640
they see the negative story.
But that being said,

230
00:12:18,641 --> 00:12:21,700
I actually actually have some
hope, right? I think that the,

231
00:12:21,820 --> 00:12:25,830
that the positive vision is
something that I think can be, uh,

232
00:12:25,930 --> 00:12:27,490
is something that we can,
we can talk about.

233
00:12:27,491 --> 00:12:31,180
And I think that just simply saying this
fact of, yeah, like there's positives,

234
00:12:31,181 --> 00:12:33,340
there's negatives.
Everyone likes to dwell on the negative.

235
00:12:33,550 --> 00:12:36,250
People have to respond well to that
message and say, Huh, you're right.

236
00:12:36,251 --> 00:12:39,100
There's a part of this that we're
not talking about not thinking about.

237
00:12:39,610 --> 00:12:40,600
And that's actually something that's,

238
00:12:40,670 --> 00:12:45,670
that's that's I think really been a key
part of how we think about Agi at open

239
00:12:45,671 --> 00:12:48,860
Ai. Right. You can kind of look
at it is like, okay, like opening,

240
00:12:48,861 --> 00:12:52,120
I talks about the fact that there are
risks and yet they're trying to build this

241
00:12:52,121 --> 00:12:55,510
system.
Like how do you square those two facts?

242
00:12:56,050 --> 00:12:59,350
So do you share the intuition
that some people have a,

243
00:12:59,351 --> 00:13:02,110
I mean from Sam Harris to
even Elon Musk himself,

244
00:13:02,710 --> 00:13:07,710
that it's tricky as you develop Agi
to keep it from slipping into the

245
00:13:09,370 --> 00:13:13,030
existential threats into the negative.
What's your intuition about?

246
00:13:13,630 --> 00:13:18,630
How hard is it to keep AI development
on the positive track and your intuition

247
00:13:20,350 --> 00:13:23,650
there to, to answer the question, you can
really look at how we structure open Ai.

248
00:13:23,920 --> 00:13:26,920
So we really have three main
arms that we have capabilities,

249
00:13:26,921 --> 00:13:30,160
which is actually doing the technical
work and pushing forward what these

250
00:13:30,161 --> 00:13:32,320
systems can do if there's safety,

251
00:13:32,500 --> 00:13:37,030
which is working on technical mechanisms
to ensure that the systems we build are

252
00:13:37,031 --> 00:13:39,580
aligned with human values.
And then there's policy,

253
00:13:39,610 --> 00:13:42,760
which is making sure that we
have governance mechanisms
answering that question

254
00:13:42,761 --> 00:13:44,710
of well,
whose values?

255
00:13:45,250 --> 00:13:49,390
And so I think that the technical safety
one is the one that people kind of talk

256
00:13:49,391 --> 00:13:52,210
about the most, right? You talked
about like think about, you know,

257
00:13:52,211 --> 00:13:54,160
all of the just topic AI movies.

258
00:13:54,161 --> 00:13:57,820
A lot of that is about not having
good technical safety in place. Um,

259
00:13:57,850 --> 00:13:59,940
and what we've been finding is that,
you know,

260
00:13:59,950 --> 00:14:02,740
I think that actually a lot of people
look at the technical safety problem and

261
00:14:02,741 --> 00:14:05,400
think it's just intractable,
right?

262
00:14:05,410 --> 00:14:09,160
This question of what do humans want?
How am I supposed to write that down?

263
00:14:09,161 --> 00:14:13,990
Can I even write down what I want?
No Way. And then they stopped there.

264
00:14:14,830 --> 00:14:19,270
But the thing is we've already built
systems that are able to learn things that

265
00:14:19,271 --> 00:14:21,150
humans can't specify.
You know,

266
00:14:21,340 --> 00:14:24,460
even the rules for how to recognize
if there's a cat or a dog in an image,

267
00:14:24,670 --> 00:14:27,640
but it turns out it's intractable to write
that down and yet we're able to learn

268
00:14:27,641 --> 00:14:28,420
it.

269
00:14:28,420 --> 00:14:31,990
And that what we're seeing with systems
we built at open AI and they're still in

270
00:14:31,991 --> 00:14:36,310
early proof of concept stage is that
you are able to learn human preferences.

271
00:14:36,311 --> 00:14:39,100
You're able to learn what
humans want from data. Um,

272
00:14:39,101 --> 00:14:41,650
and so that's kind of the core
focus for our technical safety team.

273
00:14:41,860 --> 00:14:43,780
And I think that they're actually,

274
00:14:43,781 --> 00:14:46,810
we've had some pretty encouraging updates
in terms of what we've been able to

275
00:14:46,811 --> 00:14:51,070
make work. So you have an
intuition in a hope that from data,

276
00:14:51,620 --> 00:14:54,410
you know, looking at the value
alignment problem from data,

277
00:14:54,440 --> 00:14:59,440
we can build systems that align with the
collective better angels of our nature.

278
00:15:00,560 --> 00:15:02,170
So align with,
um,

279
00:15:02,540 --> 00:15:05,840
the ethics and the morals of human beings
to even say this in a different way.

280
00:15:05,841 --> 00:15:08,570
I mean, think about how
do we align humans, right?

281
00:15:08,571 --> 00:15:12,530
Think about like a human baby can grow up
to be an evil person or a great person.

282
00:15:12,860 --> 00:15:15,200
And a lot of that is from
learning from data, right?

283
00:15:15,201 --> 00:15:17,540
That you have some feedback
as a child is growing up,

284
00:15:17,690 --> 00:15:22,330
they get to see positive examples.
And so I think that that just like the,

285
00:15:22,331 --> 00:15:26,990
the only example we have of a general
intelligence that is able to learn from

286
00:15:26,991 --> 00:15:31,230
data I to aligned with human
values and to learn values, um,

287
00:15:31,400 --> 00:15:36,080
I think we shouldn't be surprised that
we can do the same sorts of techniques or

288
00:15:36,081 --> 00:15:39,860
whether the same sort of techniques end
up being how we saw value alignment for

289
00:15:39,861 --> 00:15:44,240
Agi Gis. So let's go even
a highest, I don't know if
you've read the book Sapiens,

290
00:15:44,750 --> 00:15:47,670
but uh, there's an idea
that, you know, um,

291
00:15:48,200 --> 00:15:53,200
that as a collective as us human beings
who kind of developed together and ideas

292
00:15:53,780 --> 00:15:57,860
that we hold, there is no in
that context, objective truth,

293
00:15:57,861 --> 00:16:01,130
we just kind of all agree to certain
ideas and hold them as a collective.

294
00:16:01,370 --> 00:16:04,970
Did you have a sense that there
is and the world of good and evil?

295
00:16:05,300 --> 00:16:08,630
Do you have a sense that to the first
approximation there are some things that

296
00:16:08,631 --> 00:16:13,631
are good and that we could teach
systems to behave to be good.

297
00:16:14,450 --> 00:16:18,470
So I think that this actually
blends into our third team, right?

298
00:16:18,471 --> 00:16:22,100
Which is the policy team. And this is
the one, the, the aspect that he'd,

299
00:16:22,110 --> 00:16:25,280
people really talk about weigh
less than they should, right?

300
00:16:25,281 --> 00:16:28,820
Because imagine that we built super
powerful systems that we've managed to

301
00:16:28,821 --> 00:16:31,820
figure it all of the mechanisms for
these things to do whatever the operator

302
00:16:31,821 --> 00:16:35,480
wants. The most important question
becomes who's the operator,

303
00:16:35,570 --> 00:16:39,350
what do they want and how is that
going to affect everyone else? Right.

304
00:16:39,490 --> 00:16:43,970
And I think that this question of
what is good, what are those values?

305
00:16:43,971 --> 00:16:46,580
I mean I think you, you don't
even have to go to those, those,

306
00:16:46,581 --> 00:16:50,720
those very grand exit central places to
start to realize how hard this problem

307
00:16:50,721 --> 00:16:54,260
is. You just look at different countries
and cultures across the world, right?

308
00:16:54,500 --> 00:16:58,430
And that there's, there's a very different
conception of how the world works.

309
00:16:58,431 --> 00:17:03,230
And you know, what, what, what kinds
of ways that society wants to operate.

310
00:17:03,620 --> 00:17:08,100
And so I think that that the, that
the really core question is, is, is,

311
00:17:08,110 --> 00:17:09,770
is actually a very concrete,
um,

312
00:17:09,771 --> 00:17:12,710
and I think it's not a question that
we have ready answers to, right,

313
00:17:12,711 --> 00:17:17,150
is how do you have a world where all
of the different countries that we have

314
00:17:17,270 --> 00:17:21,110
United States, China, Russia,
and you know, the, the, the,

315
00:17:21,111 --> 00:17:25,580
the hundreds of other countries out
there are able to continue to not just

316
00:17:25,970 --> 00:17:29,430
operate in the way that, that the,
that they see fit. But in, in, uh,

317
00:17:29,560 --> 00:17:34,550
the world that emerges in these were
where you have these very powerful systems

318
00:17:36,080 --> 00:17:39,680
operating alongside humans ends up being
something that empowers humans more,

319
00:17:39,800 --> 00:17:44,770
that makes like exist human existence
be a more meaningful thing and a,

320
00:17:44,771 --> 00:17:48,650
that people are happier and wealthier and,
uh, able to live more fulfilling lives.

321
00:17:48,990 --> 00:17:52,470
It's not an obvious thing for how to
design that world once you have that very

322
00:17:52,471 --> 00:17:54,210
powerful system.
So if

323
00:17:54,420 --> 00:17:58,530
take a little step back and we're having
it like a fascinating conversation and

324
00:17:58,531 --> 00:18:01,650
a opening, as in many ways
a tech leader in the world.

325
00:18:02,010 --> 00:18:05,460
And yet we're thinking about
these big existential questions,

326
00:18:05,461 --> 00:18:06,990
which is fascinating,
really important.

327
00:18:07,020 --> 00:18:11,190
I think you're a leader in that space
and it's a really important space of just

328
00:18:11,191 --> 00:18:16,050
thinking how AI affects society in a, in
a big picture view. So Oscar Wilde said,

329
00:18:16,410 --> 00:18:17,280
we're all in the gutter,

330
00:18:17,281 --> 00:18:22,281
but some of us are looking at the stars
and I think open AI has a charter that

331
00:18:22,471 --> 00:18:25,770
looks to the stars, I would
say, to create intelligence,

332
00:18:25,771 --> 00:18:29,010
to create general intelligence, make
it beneficial, safe, and collaborative.

333
00:18:29,490 --> 00:18:33,480
Can you tell me
how that came about?

334
00:18:33,660 --> 00:18:37,980
How a mission like that and the path to
creating a mission like that open? Yeah,

335
00:18:37,981 --> 00:18:38,814
I was founded.

336
00:18:39,120 --> 00:18:44,040
Yeah. So I think that in some ways it
really boils down to taking a look at the

337
00:18:44,041 --> 00:18:46,890
landscape, right? So if you
think about the history of Ai,

338
00:18:47,010 --> 00:18:49,590
that basically for the
past 60 or 70 years,

339
00:18:49,920 --> 00:18:53,730
people have thought about those goal of
what could happen if you could automate

340
00:18:53,940 --> 00:18:56,460
human intellectual labor. Right? Okay.

341
00:18:56,650 --> 00:18:58,960
Imagine you can build a computer
system that could do that.

342
00:18:59,260 --> 00:19:00,400
What becomes possible.

343
00:19:00,540 --> 00:19:04,170
We have a lot of Scifi that tells stories
of various dystopias and you know,

344
00:19:04,171 --> 00:19:06,690
increasingly you have movies like heard
that tell you a little bit about maybe

345
00:19:06,691 --> 00:19:08,430
more of a little bit utopic vision.

346
00:19:09,380 --> 00:19:14,380
You think about the impacts that we've
seen from being able to have bicycles for

347
00:19:15,571 --> 00:19:20,500
our minds and computers. Uh, and
that I think that the impact of,

348
00:19:20,530 --> 00:19:24,770
of computers and the Internet has just
far outstripped what anyone really could

349
00:19:24,771 --> 00:19:29,190
have predicted. And so I think that it's
very clear that if you can build an Agi,

350
00:19:29,310 --> 00:19:32,670
it will be the most transformative
technology that humans will ever create.

351
00:19:34,650 --> 00:19:38,580
And so what it boils down to then is
a question of, well, is there a path?

352
00:19:38,670 --> 00:19:41,130
Is there hope?
Is there a way to build such a system?

353
00:19:41,640 --> 00:19:46,010
And I think that for 60 or 70 years
that people got excited and, uh, uh,

354
00:19:46,020 --> 00:19:50,760
that ended up not being able to deliver
on the hopes that the people had pinned

355
00:19:50,761 --> 00:19:54,340
on them. And I think that then, you
know, that after, you know, two,

356
00:19:54,390 --> 00:19:57,530
two winters of Ai Development,
uh, that people, uh, you know,

357
00:19:57,540 --> 00:20:00,540
I think kind of almost stopped
daring to dream, right?

358
00:20:00,541 --> 00:20:04,380
That they really talking about Agi or
thinking about Agi became almost this

359
00:20:04,381 --> 00:20:05,460
taboo in the community.

360
00:20:06,600 --> 00:20:10,020
But I actually think that people took
the wrong lesson from AI history.

361
00:20:10,050 --> 00:20:14,160
And if you look back, starting in 1959
is when the perceptron was released.

362
00:20:14,280 --> 00:20:17,190
And this is basically, you know,
one of the earliest neural networks.

363
00:20:17,670 --> 00:20:20,550
It was released to what was
perceived as this massive overhype.

364
00:20:20,790 --> 00:20:24,600
So in the New York Times in 1959 and you
have this article saying that, you know,

365
00:20:24,601 --> 00:20:27,810
the,
the perceptron will one day recognize,

366
00:20:27,811 --> 00:20:31,050
people call it their names instantly
translate speech between languages.

367
00:20:31,410 --> 00:20:35,100
And people at the time looked at this
and said, this is, check your system,

368
00:20:35,101 --> 00:20:36,000
can't do any of that.

369
00:20:36,090 --> 00:20:39,300
And basically spent 10 years trying to
discredit the whole perceptron direction

370
00:20:39,660 --> 00:20:43,650
and succeeded and all the funding dried
up and you know, people kind of, uh,

371
00:20:43,680 --> 00:20:45,930
went in other directions and you know,
the 80s,

372
00:20:45,931 --> 00:20:49,750
there's a resurgence and I'd always heard
that the resurgence in the 80s was due

373
00:20:49,751 --> 00:20:51,760
to the invention of
backpropagation and these,

374
00:20:51,910 --> 00:20:53,440
these algorithms that got people excited,

375
00:20:53,650 --> 00:20:57,520
but actually the causality was due to
people building larger computers that you

376
00:20:57,521 --> 00:20:58,121
can find these,

377
00:20:58,121 --> 00:21:00,790
these articles from the 80 [inaudible]
that the democratization of computing

378
00:21:00,791 --> 00:21:04,300
power suddenly meant that you could run
these larger neural networks and then

379
00:21:04,301 --> 00:21:06,250
people start to do all
these amazing things.

380
00:21:06,270 --> 00:21:08,900
Backpropagation algorithm was
invented and you know that the,

381
00:21:08,901 --> 00:21:11,620
the neural nets people were running
were these tiny little like 20 near on

382
00:21:11,621 --> 00:21:14,650
neural nets. Right? Right. Like what are
you supposed to learn with 20 neurons?

383
00:21:15,160 --> 00:21:16,330
And so of course,

384
00:21:16,810 --> 00:21:21,460
they weren't able to get great results
and it really wasn't until 2012 that this

385
00:21:21,461 --> 00:21:21,941
approach,

386
00:21:21,941 --> 00:21:26,110
that's almost the most simple natural
approach that people would come up with in

387
00:21:26,111 --> 00:21:28,090
the fifties. Right? In some ways,

388
00:21:28,091 --> 00:21:31,420
even in the forties before there were
computers with the pits mackolen there in

389
00:21:31,421 --> 00:21:32,254
neuron,

390
00:21:32,980 --> 00:21:37,420
suddenly this became the best
way of solving problems, right?

391
00:21:37,421 --> 00:21:42,070
And I think there are three
core properties that deep
learning has that I think

392
00:21:42,071 --> 00:21:45,280
are very worth paying attention to.
The first is generality.

393
00:21:45,840 --> 00:21:50,290
We have a very small number of deep
learning tools, SGD deep neural net,

394
00:21:50,440 --> 00:21:55,090
maybe some, some, you know RL and it
solves this huge variety of problems.

395
00:21:55,570 --> 00:21:57,940
Speech recognition, machine
translation, game playing,

396
00:21:58,540 --> 00:22:02,050
all of these problems since small set
of tools. So there's the generality,

397
00:22:02,740 --> 00:22:03,820
there's a second piece,

398
00:22:03,821 --> 00:22:07,360
which is the competence you want to
solve any of those problems, throw it.

399
00:22:07,361 --> 00:22:11,440
40 years worth of normal computer vision
research replaced with a deep neural

400
00:22:11,441 --> 00:22:14,530
net. It's kind of worked better.
And there's a third piece,

401
00:22:14,560 --> 00:22:16,270
which is the scalability,
right?

402
00:22:16,271 --> 00:22:19,360
That one thing that has been shown
time and time again is that you,

403
00:22:19,660 --> 00:22:23,110
if you have a larger neural network,
throw more compute, more data at it,

404
00:22:23,170 --> 00:22:24,040
it will work better.

405
00:22:25,090 --> 00:22:29,620
Those three properties together feel like
essential parts of building a general

406
00:22:29,621 --> 00:22:33,730
intelligence. Now, it doesn't just
mean that if we scale up what we have,

407
00:22:33,790 --> 00:22:36,640
that we will have an Agi, right?
They're clearly missing pieces.

408
00:22:36,760 --> 00:22:39,580
They're missing ideas.
We need to have answers for reasoning.

409
00:22:39,970 --> 00:22:44,970
But I think that the core here is that
for the first time it feels that we have

410
00:22:46,001 --> 00:22:49,690
a paradigm that gives us hope that
general intelligence can be achievable.

411
00:22:50,560 --> 00:22:53,980
And so as soon as you
believe that everything else
becomes, it comes into focus,

412
00:22:54,250 --> 00:22:56,560
right?
If you imagine that you may be able to,

413
00:22:56,561 --> 00:22:59,520
and you know that the timeline
I think remains uncertain. Um,

414
00:22:59,740 --> 00:23:00,930
but I think that that,
you know,

415
00:23:01,060 --> 00:23:04,630
certainly within our lifetimes
and possibly within a much
shorter period of time

416
00:23:04,631 --> 00:23:06,100
than,
than people would expect.

417
00:23:06,520 --> 00:23:10,270
If you can really build the most
transformative technology will ever exist,

418
00:23:10,630 --> 00:23:12,550
you stop thinking about yourself so much,
right?

419
00:23:12,551 --> 00:23:15,730
And you start thinking about just like
how do you have a world where this goes

420
00:23:15,731 --> 00:23:18,850
well and that you need to think about
the practicalities of how do you build an

421
00:23:18,851 --> 00:23:22,150
organization and get together a
bunch of people and resources, um,

422
00:23:22,330 --> 00:23:26,590
and to make sure that people feel,
uh, motivated and ready to do it.

423
00:23:28,030 --> 00:23:32,320
But I think that then you start thinking
about, well, what if we succeed? Um,

424
00:23:32,321 --> 00:23:35,170
and how do we make sure that when we
succeed that the world is actually the

425
00:23:35,171 --> 00:23:38,700
place that we want ourselves to exist.
And you know,

426
00:23:38,710 --> 00:23:42,540
almost in the Rawlsian bail sense of
the word. And so that's kind of the,

427
00:23:42,550 --> 00:23:44,410
the broader landscape and opening.

428
00:23:44,411 --> 00:23:49,411
I was really formed in 2015 with that
high level picture of Agi might be

429
00:23:50,541 --> 00:23:53,850
possible sooner than
people think. And that, uh,

430
00:23:53,960 --> 00:23:56,960
we need to try to do our best to
make sure it's going to go well.

431
00:23:57,440 --> 00:24:00,440
And then we spent the next couple of years
really trying to figure out what does

432
00:24:00,441 --> 00:24:04,580
that mean, how do we do it?
Um, and you know, I think
that typically with accompany,

433
00:24:04,760 --> 00:24:09,000
you start out very small. So you want
to co founder and you build a product,

434
00:24:09,001 --> 00:24:11,300
do you get some users, you get
product market fit, you know,

435
00:24:11,301 --> 00:24:15,770
then at some point you raise some money,
you hire people, you scale and then, uh,

436
00:24:15,771 --> 00:24:18,170
you know, down the road then the
big companies realize he existed in,

437
00:24:18,171 --> 00:24:19,330
tried to kill you.
Um,

438
00:24:19,340 --> 00:24:22,760
and for open AI it was basically
everything in exactly the opposite order.

439
00:24:23,160 --> 00:24:26,760
Uh,
let me just pause for a second.

440
00:24:26,761 --> 00:24:31,761
You said a lot of things and let me just
admire the jarring aspect of what open

441
00:24:31,981 --> 00:24:36,810
AI stands for, which is a daring to dream.
I mean, you said it's pretty powerful.

442
00:24:37,120 --> 00:24:41,980
It caught me off guard because I
think that's very true. The, the,

443
00:24:42,180 --> 00:24:45,570
the step of just daring to dream
about the possibilities of creating

444
00:24:45,571 --> 00:24:48,330
intelligence in a positive,
in a safe way,

445
00:24:48,720 --> 00:24:51,270
but just even creating intelligence is a,

446
00:24:51,550 --> 00:24:53,820
a much needed refreshing,

447
00:24:55,140 --> 00:24:59,040
a catalyst for the AI community. So
that's, that's the starting point. Okay.

448
00:24:59,041 --> 00:25:00,260
So then formation

449
00:25:00,510 --> 00:25:05,400
an AI, uh, I would just say
that, you know, when we were
starting opening, I, uh,

450
00:25:05,580 --> 00:25:07,740
that kind of the first
question that we had is,

451
00:25:07,741 --> 00:25:12,741
is it too late to start a lab with
a bunch of the best people possible?

452
00:25:13,140 --> 00:25:15,410
That wasn't an actual question.
That was really, that was,

453
00:25:15,411 --> 00:25:18,620
that was the core question of uh, of, you
know, we had this dinner in July of 28,

454
00:25:18,630 --> 00:25:20,220
2015 and was that was,

455
00:25:20,221 --> 00:25:23,480
that was really what we spent the whole
time talking about. And uh, you know,

456
00:25:23,490 --> 00:25:26,820
cause it's,
you think about kind of where AI was,

457
00:25:26,821 --> 00:25:31,821
is that it transition from being an
academic pursuit to an industrial pursuit.

458
00:25:32,250 --> 00:25:35,790
And so a lot of the best people were
in these big research labs and that we

459
00:25:35,791 --> 00:25:37,650
wanted to start our own one that,
you know,

460
00:25:37,651 --> 00:25:42,280
no matter how much resources we could
accumulate would be no pale in comparison

461
00:25:42,281 --> 00:25:43,300
to the big tech companies.

462
00:25:43,540 --> 00:25:47,380
And we knew that and there was a question
of are we going to be actually able to

463
00:25:47,381 --> 00:25:49,750
get this thing off the ground?
You need critical mass.

464
00:25:49,751 --> 00:25:52,120
You can't just do you and a
cofounder build the product, right?

465
00:25:52,121 --> 00:25:56,340
You really need to have a group of,
you know, five to 10 people. And uh,

466
00:25:57,070 --> 00:26:00,460
we kind of concluded it wasn't obviously
impossible. Uh, so it seems worth trying.

467
00:26:02,230 --> 00:26:05,530
Well, you were also dreamers, so who
knows, right? That's right. Okay.

468
00:26:05,531 --> 00:26:10,330
So this is speaking of that competing
with, with the, with the big players.

469
00:26:10,331 --> 00:26:12,610
Um,
let's talk about some of the,

470
00:26:12,670 --> 00:26:16,900
some of the tricky things as you
think through this process of growing,

471
00:26:17,470 --> 00:26:22,060
of, uh, seeing how you can develop these
systems at USC at scale that competes.

472
00:26:22,600 --> 00:26:23,290
So you recently,

473
00:26:23,290 --> 00:26:28,290
recently formed Openai LP and you cap
profit company and now carries the name

474
00:26:30,281 --> 00:26:32,650
open as open as now this official company,

475
00:26:33,190 --> 00:26:38,190
the original nonprofit company still
exists and carries the open AI nonprofit

476
00:26:38,711 --> 00:26:41,890
name.
So can you explain what this company is,

477
00:26:41,980 --> 00:26:43,740
what the purpose of his is

478
00:26:44,250 --> 00:26:48,000
and how did you arrive at the decision?
Yup.

479
00:26:48,210 --> 00:26:49,560
To create it open Ai,

480
00:26:50,040 --> 00:26:54,810
the whole entity and opening I LP as
a vehicle is trying to accomplish the

481
00:26:54,840 --> 00:26:58,290
mission of ensuring that artificial
general intelligence benefits everyone.

482
00:26:58,800 --> 00:27:01,590
And the main way that we're trying to
do that is by actually trying to build

483
00:27:01,800 --> 00:27:04,890
general intelligence ourselves and make
sure the benefits are distributed to the

484
00:27:04,891 --> 00:27:09,540
world. That's the primary way. We're also
fine if someone else does this right?

485
00:27:09,541 --> 00:27:13,200
It doesn't have to be us if someone else
is going to build an Agi and make sure

486
00:27:13,201 --> 00:27:16,620
that the benefits don't get knocked up
in one company or you know, one, one,

487
00:27:16,680 --> 00:27:20,460
one with one set of people, um,
like we're actually fine with that.

488
00:27:21,090 --> 00:27:25,290
And so those ideas are
baked into our charter,

489
00:27:25,350 --> 00:27:28,740
which is kind of the
foundational document that are,

490
00:27:29,160 --> 00:27:32,500
describes kind of our values
and how we operate. Um,

491
00:27:32,800 --> 00:27:35,960
it was also really baked into
the structure of open ILP.

492
00:27:36,330 --> 00:27:40,650
And so the way that we've set up
opening ILP is that in the case where we

493
00:27:40,651 --> 00:27:44,160
succeed, right? If we actually
build what we're trying to build,

494
00:27:45,300 --> 00:27:47,130
then investors are able to get a return.

495
00:27:47,730 --> 00:27:50,160
And that return is something that is kept.

496
00:27:50,430 --> 00:27:53,880
And so if you think of Agi in terms of
the value that you could really create,

497
00:27:54,120 --> 00:27:57,000
you're talking about the most
transformative technology ever created.

498
00:27:57,030 --> 00:28:01,200
It's going to create orders of magnitude
more value than any existing company,

499
00:28:01,800 --> 00:28:05,820
and that all of that value
will be owned by the world,

500
00:28:05,910 --> 00:28:10,830
like legally titled to the nonprofit
to fulfill that mission. And so that's,

501
00:28:10,860 --> 00:28:11,820
that's the structure.

502
00:28:12,720 --> 00:28:16,080
So the mission is a
powerful one and it's a,

503
00:28:16,530 --> 00:28:18,690
it's one that I think most
people would agree with.

504
00:28:18,870 --> 00:28:22,050
It's how we would hope AI progresses.

505
00:28:22,890 --> 00:28:25,350
And so how do you tie
yourself to that mission?

506
00:28:25,380 --> 00:28:30,180
How do you make sure you did not
deviate from that mission that, um,

507
00:28:30,720 --> 00:28:34,890
you know, other incentives that
are profit driven wouldn't,

508
00:28:35,250 --> 00:28:36,630
don't interfere with the mission.

509
00:28:36,720 --> 00:28:39,410
So th this was actually a
really core question for us, uh,

510
00:28:39,510 --> 00:28:41,760
for the past couple of years because,
you know,

511
00:28:41,761 --> 00:28:43,230
I'd say that like the
way that our history,

512
00:28:43,231 --> 00:28:46,200
when was that for the first year we
were getting off the ground, right?

513
00:28:46,201 --> 00:28:47,490
We had this high level picture,

514
00:28:47,910 --> 00:28:52,420
but we didn't know exactly how we
want it to accomplish it. And, uh,

515
00:28:52,421 --> 00:28:56,100
it really two years ago is when we first
started realizing in order to build Agi,

516
00:28:56,101 --> 00:29:00,530
we're just going to need to raise way
more money than we can as a nonprofit. Um,

517
00:29:00,540 --> 00:29:02,160
and you are talking many
billions of dollars.

518
00:29:02,790 --> 00:29:07,500
And so the first question is how are
you supposed to do that and stay true to

519
00:29:07,501 --> 00:29:08,334
this mission?

520
00:29:08,670 --> 00:29:11,460
And we looked at every legal structure
out there and concluded none of them are

521
00:29:11,461 --> 00:29:12,930
quite right for what we wanted to do.

522
00:29:13,380 --> 00:29:15,300
And I guess it shouldn't be too surprising
if you're going to do something like

523
00:29:15,301 --> 00:29:15,610
crazy,

524
00:29:15,610 --> 00:29:18,510
unprecedented technology that you're going
to have to come up with some crazy on

525
00:29:18,511 --> 00:29:21,660
precedent and structure to
do it in. And a, a lot of,

526
00:29:21,780 --> 00:29:26,100
a lot of our conversation was, uh,
with people at opening, I right,

527
00:29:26,101 --> 00:29:29,490
the people who really joined because
they believe so much in this mission and

528
00:29:29,520 --> 00:29:33,750
thinking about how do we actually raise
the resources to do it and also stay

529
00:29:33,751 --> 00:29:35,640
true to what we stand for.

530
00:29:35,880 --> 00:29:38,880
And the place you got to start is to
really align on what is it that we stand

531
00:29:38,881 --> 00:29:41,680
for? What are those values?
What's really important to us.

532
00:29:41,980 --> 00:29:46,060
And so I'd say that we spent about a year
really compiling the opening I charter

533
00:29:46,210 --> 00:29:48,940
and that determines,
and if you even look at the first,

534
00:29:48,941 --> 00:29:50,830
the first line item in
there, it says that, look,

535
00:29:50,831 --> 00:29:53,560
we expect we're going to have to
marshal huge amounts of resources,

536
00:29:53,710 --> 00:29:56,740
but we're going to make sure that we
minimize conflict of interest with the

537
00:29:56,741 --> 00:29:57,574
mission.

538
00:29:57,880 --> 00:30:02,290
And that kind of aligning on all of
those pieces was the most important step

539
00:30:02,530 --> 00:30:07,390
towards figuring out how do we structure
a company that can actually raise the

540
00:30:07,391 --> 00:30:09,790
resources to do what we need to do.

541
00:30:10,330 --> 00:30:12,580
I imagine open Ai,
uh,

542
00:30:12,640 --> 00:30:16,240
the decision to create open a
LP was a really difficult one.

543
00:30:16,300 --> 00:30:20,820
And there was a lot of discussions
as you mentioned for a year and, uh,

544
00:30:20,850 --> 00:30:24,970
there's different ideas perhaps
to tractors within open AI,

545
00:30:25,390 --> 00:30:30,190
uh, sort of different paths that you could
have taken. What were those concerns?

546
00:30:30,191 --> 00:30:31,960
What were the different paths considered?

547
00:30:32,020 --> 00:30:34,600
What was that process of making
that decision like? Yup. Um,

548
00:30:34,650 --> 00:30:38,800
but so if you look actually at the
opening I charter that there's almost two

549
00:30:38,801 --> 00:30:41,590
paths embedded within it.
There is,

550
00:30:41,650 --> 00:30:44,680
we are primarily trying
to build AGI ourselves,

551
00:30:44,860 --> 00:30:48,610
but we're also okay if someone else
does it and this is a weird thing for a

552
00:30:48,611 --> 00:30:51,350
company. It's really interesting
actually. Yeah, there,

553
00:30:51,360 --> 00:30:56,050
there is an element of competition that
you do want to be the one that does it,

554
00:30:56,620 --> 00:30:57,940
but at the same time you,
okay,

555
00:30:57,941 --> 00:31:00,970
somebody has us and we'll talk about
that a little bit. That trade off.

556
00:31:00,971 --> 00:31:03,670
That's the dance. That's really
interesting and I think this was the

557
00:31:03,710 --> 00:31:08,030
core tension as we were designing opening
I LP and really the opening I strategy

558
00:31:08,210 --> 00:31:12,170
is how do you make sure that both you
have a shot at being a primary actor,

559
00:31:12,600 --> 00:31:15,390
which really requires
building an organization,

560
00:31:15,870 --> 00:31:20,280
raising massive resources and really
having the will to go and execute on some

561
00:31:20,281 --> 00:31:21,990
really, really hard vision. Right?

562
00:31:21,991 --> 00:31:25,770
You need to really sign up for a long
period to go and take on a lot of pain and

563
00:31:25,771 --> 00:31:28,740
a lot of risk.
And to do that,

564
00:31:29,010 --> 00:31:32,510
normally you just import the startup
mindset, right? And that you think about,

565
00:31:32,520 --> 00:31:35,880
okay, like how do we out execute
everyone if this very competitive angle.

566
00:31:36,150 --> 00:31:38,610
But you also have the second
angle of saying that, well,

567
00:31:38,790 --> 00:31:41,400
the true mission isn't
for open AI to build AGI.

568
00:31:41,610 --> 00:31:44,310
The true mission is for Agi
to go well for humanity.

569
00:31:45,090 --> 00:31:49,020
And so how do you take all of those first
actions and make sure you don't close

570
00:31:49,021 --> 00:31:54,021
the door on outcomes that would actually
be positive and fulfill the mission?

571
00:31:54,570 --> 00:31:56,640
And so I think it's a very
delicate balance, right?

572
00:31:56,641 --> 00:32:00,300
And I think that going 100% one direction
and the other is clearly not the

573
00:32:00,301 --> 00:32:01,134
correct answer.

574
00:32:01,320 --> 00:32:04,770
And so I think that even in terms of
just how we talk about open AI and think

575
00:32:04,771 --> 00:32:06,460
about it,
there's just like,

576
00:32:06,750 --> 00:32:10,320
one thing that's always in the back of
my mind is to make sure that we're not

577
00:32:10,321 --> 00:32:13,980
just saying open your eyes
goal is to build Agi, right?

578
00:32:13,981 --> 00:32:17,490
That it's actually much broader than that,
right? That first of all, uh, you know,

579
00:32:17,491 --> 00:32:21,000
it's not just Agi, it's safe Agi,
that's very important. But secondly,

580
00:32:21,410 --> 00:32:23,060
our goal isn't to be the ones to build it.

581
00:32:23,061 --> 00:32:24,590
Our goal is to make sure
it goes well for the world.

582
00:32:24,800 --> 00:32:28,260
And so I think that figuring out how do
you balance all of those, um, and to,

583
00:32:28,280 --> 00:32:32,660
to get people to really come to
the table and compile the, the,

584
00:32:32,810 --> 00:32:37,310
like a single document that that
encompasses, all of that wasn't trivial.

585
00:32:37,510 --> 00:32:41,240
So part of the challenge here is a,
your mission is,

586
00:32:41,630 --> 00:32:43,430
I would say beautiful,

587
00:32:43,460 --> 00:32:47,810
empowering and a beacon of hope for
people in the research community and just

588
00:32:47,811 --> 00:32:48,920
people thinking about Ai.

589
00:32:49,130 --> 00:32:54,130
So your decisions are scrutinized more
than I think a regular profit driven

590
00:32:54,981 --> 00:32:55,814
company.

591
00:32:55,850 --> 00:32:59,120
Do you feel the burden of this in the
creation of the chart and just in the way

592
00:32:59,121 --> 00:33:02,920
you operate? Yes. Uh,

593
00:33:02,930 --> 00:33:07,930
so why do you lean into the
burden by creating such a charter?

594
00:33:08,601 --> 00:33:10,980
Why not to keep it
quiet? I mean, it's just

595
00:33:11,000 --> 00:33:13,180
boils down to the, to
the mission, right? Like,

596
00:33:13,390 --> 00:33:16,340
like I'm here and everyone else was
here because we think this is the most

597
00:33:16,341 --> 00:33:17,174
important mission,

598
00:33:17,380 --> 00:33:20,060
right? To dare to dream.
All right. So, uh,

599
00:33:20,290 --> 00:33:25,210
do you think you can be good for the
world or create an AGI system that's good

600
00:33:25,960 --> 00:33:29,170
when you're a for profit company?
From my perspective,

601
00:33:29,171 --> 00:33:32,800
I don't understand why profit,
uh,

602
00:33:32,890 --> 00:33:36,970
interferes with a positive
impact on society.

603
00:33:37,600 --> 00:33:39,810
I don't understand, uh, why, uh,

604
00:33:39,850 --> 00:33:44,850
Google it makes most of its money from
ads can't also do good for the world or

605
00:33:45,550 --> 00:33:47,830
other companies, Facebook,
anything. I don't,

606
00:33:48,040 --> 00:33:51,420
I don't understand why those have to
interfere. You know, that you can, um,

607
00:33:51,820 --> 00:33:56,820
profit isn't the thing in my view
that affects the impact of a company.

608
00:33:57,191 --> 00:34:01,330
What affects the impact of the company
is the charter, is the culture,

609
00:34:01,331 --> 00:34:04,120
is the, you know, the people inside.

610
00:34:04,121 --> 00:34:07,510
And profit is the thing that
just fuels those people. So what,

611
00:34:07,511 --> 00:34:08,620
what are your views there?

612
00:34:08,700 --> 00:34:11,870
Yeah, so I think it's a really
good question. And there's,
there's, there's some,

613
00:34:11,900 --> 00:34:12,571
some,
you know,

614
00:34:12,571 --> 00:34:16,170
real like long standing debates in human
society that are wrapped up in it. Uh,

615
00:34:16,420 --> 00:34:18,990
the way that I think about
it is just think about what,

616
00:34:19,350 --> 00:34:21,300
what are the most impactful
nonprofits in the world?

617
00:34:23,930 --> 00:34:26,960
What are the most impactful for
profits in the world? Right?

618
00:34:27,030 --> 00:34:30,560
It's much easier to list the four profits.
That's right. And I think that there's,

619
00:34:30,561 --> 00:34:35,561
there's some real truth year that the
system that we set up the system for kind

620
00:34:35,691 --> 00:34:38,190
of how, you know, today's
world is organized, um,

621
00:34:38,270 --> 00:34:42,850
is one that that really allows for huge
impact. Um, and that the, you know,

622
00:34:42,860 --> 00:34:47,120
kind of part of that is that you need to
be the, you know, for, for profits are,

623
00:34:47,240 --> 00:34:50,930
are self sustaining and able to, to kind
of, you know, build on their own momentum.

624
00:34:51,140 --> 00:34:52,880
And I think that's a
really powerful thing.

625
00:34:53,000 --> 00:34:56,890
It's something that when it turns out
that we haven't set the guardrails

626
00:34:56,930 --> 00:34:58,780
correctly causes problems,
right?

627
00:34:58,790 --> 00:35:02,330
Think about logging companies that
go into the forest, the rain forest,

628
00:35:02,660 --> 00:35:04,830
that's really bad. We don't want that. Um,

629
00:35:04,940 --> 00:35:07,570
and it's actually really
interesting to me that kind of this,

630
00:35:07,571 --> 00:35:11,120
this question of how do you get positive
benefits out of a for profit company.

631
00:35:11,330 --> 00:35:15,360
It's actually very similar to how do you
get positive benefits out of an Agi. Um,

632
00:35:15,510 --> 00:35:17,780
right? That you have this
like very powerful system.

633
00:35:17,930 --> 00:35:21,680
It's more powerful than any human and
it's kind of autonomous in some ways.

634
00:35:21,681 --> 00:35:23,600
You know,
it's super superhuman in a lot of axes.

635
00:35:23,780 --> 00:35:26,480
And somehow you have to set the guard
rails to get good things to happen.

636
00:35:26,750 --> 00:35:31,700
But when you do, the benefits are
massive. And so I think that that when,

637
00:35:31,710 --> 00:35:33,830
when I think about
nonprofit versus for profit,

638
00:35:34,370 --> 00:35:37,500
I think just not enough happens
in nonprofits. They're very pure,

639
00:35:37,740 --> 00:35:40,690
but it's just kind of, you know, it's
just hard to do things there, um,

640
00:35:40,800 --> 00:35:45,170
in for profits in some ways like
too much happens. Um, but, um, if,

641
00:35:45,200 --> 00:35:47,670
if kind of shaped in the right way,
it can actually be very positive.

642
00:35:47,880 --> 00:35:52,440
And so with opening NLP, we're,
we're picking a road in between. Now,

643
00:35:52,470 --> 00:35:55,860
the thing I think is really important to
recognize is that the way that we think

644
00:35:55,861 --> 00:36:00,420
about opening Ilp is that in the world
we're Agi actually happens, right?

645
00:36:00,421 --> 00:36:01,650
In a world where we are successful,

646
00:36:01,651 --> 00:36:03,390
we built the most
transformative technology ever.

647
00:36:03,750 --> 00:36:05,940
The amount of value we're going
to create will be astronomical.

648
00:36:07,560 --> 00:36:10,890
And so then in that case that the,
the,

649
00:36:10,950 --> 00:36:15,760
the cap that we have will be a small
fraction of the value we create. Um,

650
00:36:15,780 --> 00:36:18,360
and the amount of value that goes
back to investors and employees,

651
00:36:18,500 --> 00:36:20,250
it looks pretty similar
to what would happen in a,

652
00:36:20,251 --> 00:36:21,510
in a pretty successful startup.

653
00:36:23,730 --> 00:36:26,520
And that's really the case that
we're optimizing for, right?

654
00:36:26,521 --> 00:36:28,440
That we're thinking about
and the success case,

655
00:36:28,560 --> 00:36:31,890
making sure that the value we create,
it doesn't get locked up.

656
00:36:32,160 --> 00:36:33,950
And I expected in another,
you know,

657
00:36:34,020 --> 00:36:37,140
for profit companies that it's
possible to do something like that.

658
00:36:37,800 --> 00:36:39,690
I think it's not obvious
how to do it right.

659
00:36:39,691 --> 00:36:41,430
And I think that as a for profit company,

660
00:36:41,431 --> 00:36:44,940
you have a lot of fiduciary duty to your
shareholders and that there are certain

661
00:36:44,941 --> 00:36:48,420
decisions that you just cannot
make. Um, in our structure,

662
00:36:48,480 --> 00:36:52,160
we've set it up so that we have a
fiduciary duty to the charter, right?

663
00:36:52,470 --> 00:36:56,440
That we always get to make the decision
that is right for the charter, um,

664
00:36:56,700 --> 00:37:01,320
rather than even if it comes at the
expense of our own stakeholders and a,

665
00:37:01,321 --> 00:37:04,330
and so I think that when I think
about what's really important,

666
00:37:04,331 --> 00:37:06,240
it's not really about
nonprofit versus for profit.

667
00:37:06,241 --> 00:37:10,560
It's really a question of if you build
the Gi and you kind of, you know,

668
00:37:10,570 --> 00:37:15,300
communities now and this new age who
benefits, whose lives are better.

669
00:37:16,020 --> 00:37:19,350
And I think that what's really important
is to have an answer that is everyone.

670
00:37:20,330 --> 00:37:24,950
Yeah. Which is one of the core aspects of
the charter. So one concern people have,

671
00:37:25,460 --> 00:37:29,320
not just with open AI but with
Google, Facebook, Amazon, anybody,

672
00:37:29,920 --> 00:37:32,380
uh, really, uh, that's, uh,

673
00:37:32,410 --> 00:37:36,260
that's creating impact at
scale is how do we avoid,

674
00:37:36,560 --> 00:37:37,640
as your charter says,

675
00:37:37,641 --> 00:37:42,641
avoid enabling the use of AI or
Agi to unduly concentrate power.

676
00:37:43,640 --> 00:37:45,830
Why would not accompany like open AI,

677
00:37:45,920 --> 00:37:50,270
keep all the power of [inaudible] system
to itself, the charter, the charter. So,

678
00:37:50,480 --> 00:37:50,721
you know,

679
00:37:50,721 --> 00:37:55,721
how does the charter actualize
itself in a day to day?

680
00:37:57,220 --> 00:38:00,580
So I think that the first
two to zoom out right there,

681
00:38:00,581 --> 00:38:02,560
the way that we structured
the company so that the,

682
00:38:02,561 --> 00:38:05,560
the power first sort of
dictating the actions,

683
00:38:05,561 --> 00:38:09,310
that opening I takes ultimately
respite the board or the board of, of,

684
00:38:09,311 --> 00:38:10,750
of the nonprofit. Um, and,

685
00:38:10,990 --> 00:38:13,600
and the board is set up in certain ways
with certain certain restrictions that

686
00:38:13,601 --> 00:38:15,820
you can read about in the
opening I LP blog posts.

687
00:38:16,270 --> 00:38:20,500
But effectively the board is the, is the
governing body for opening Openai LP.

688
00:38:21,090 --> 00:38:25,960
Um, and the board has a duty to
fulfill the mission of the nonprofit.

689
00:38:26,380 --> 00:38:30,280
Um, and so that's kind of how we tie
how we all these things together.

690
00:38:30,910 --> 00:38:33,730
Now there's a question of,
so day to day, how do people,

691
00:38:33,760 --> 00:38:37,480
the individuals who in ways are the most
empowered ones right now the board sort

692
00:38:37,481 --> 00:38:39,550
of gets to call the
shots at the high level,

693
00:38:39,790 --> 00:38:43,090
but the people who were actually
executing are the employees, right?

694
00:38:43,090 --> 00:38:45,860
The people here on a day to day
basis who have the, you know, the,

695
00:38:45,940 --> 00:38:49,570
the keys to the technical all kingdom.
And there,

696
00:38:49,571 --> 00:38:52,180
I think that the answer looks a lot like,
well,

697
00:38:52,240 --> 00:38:55,030
how does any company's
values get actualized? Right?

698
00:38:55,031 --> 00:38:58,090
And I think that a lot of that comes
down to do you need people who are here

699
00:38:58,091 --> 00:39:02,890
because they really believe
in that mission and they
believe in the charter and

700
00:39:02,891 --> 00:39:07,450
that they are willing to take actions
that maybe our worst for them but are

701
00:39:07,451 --> 00:39:08,780
better for the charter.
Um,

702
00:39:08,790 --> 00:39:11,980
and that's something that's really
baked into the culture. And honestly,

703
00:39:11,981 --> 00:39:13,150
I think it's, uh, you know,

704
00:39:13,151 --> 00:39:17,080
I think that that's one of the things
that we really have to work to preserve as

705
00:39:17,081 --> 00:39:18,170
time goes on.
Um,

706
00:39:18,270 --> 00:39:21,670
and that's a really important part of
how we think about hiring people and

707
00:39:21,671 --> 00:39:24,220
bringing people into open Ai.
So there's people here,

708
00:39:24,610 --> 00:39:29,350
there's people here who
could speak up and say like,

709
00:39:29,440 --> 00:39:30,400
hold on a second.

710
00:39:30,790 --> 00:39:35,350
This is totally against what we stand
for a culture wise. Yeah, yeah, for sure.

711
00:39:35,351 --> 00:39:37,410
I mean, I think that, uh, that
we actually have, I think a,

712
00:39:37,420 --> 00:39:41,830
that's like a pretty important part of,
of how we operate and how we have,

713
00:39:41,860 --> 00:39:44,300
even again,
with designing the charter and,

714
00:39:44,301 --> 00:39:46,600
and designing open LP in the first place,
uh,

715
00:39:46,601 --> 00:39:49,720
that there has been a lot of
conversation with employees here.

716
00:39:49,721 --> 00:39:52,180
And a lot of times where employee said,
wait a second,

717
00:39:52,330 --> 00:39:54,790
this seems like it's going in the wrong
direction and let's talk about it.

718
00:39:55,120 --> 00:39:58,090
And so I think one thing that's, that's
I think a really, and you know, here's,

719
00:39:58,091 --> 00:40:01,420
here's actually one thing that I think
is very unique about us as a small

720
00:40:01,421 --> 00:40:05,170
company is that if you're at a massive
tech giant that's a little bit hard for

721
00:40:05,171 --> 00:40:08,830
someone who's aligned employee to
go and talk to the CEO and say,

722
00:40:09,010 --> 00:40:11,230
I think that we're doing
this wrong. Um, and you know,

723
00:40:11,231 --> 00:40:15,160
you look at companies like Google that
have had some collective action from

724
00:40:15,161 --> 00:40:19,150
employees to, you know, make, uh,
ethical change around things like maven.

725
00:40:19,480 --> 00:40:22,210
And so maybe there are mechanisms that
are other companies that work. Um,

726
00:40:22,211 --> 00:40:26,060
but here super easy for anyone to pull
me aside to pull Sam aside to lily aside

727
00:40:26,160 --> 00:40:27,190
and people do it all the time.

728
00:40:27,790 --> 00:40:31,720
One of the interesting things in the
charter is this idea that it'd be great if

729
00:40:31,721 --> 00:40:35,620
you could try to describe or untangle
switching from competition to

730
00:40:35,621 --> 00:40:39,730
collaboration in late stage Agi
development. It was really interesting.

731
00:40:39,731 --> 00:40:43,210
This dance between competition
and collaboration. How
do you think about that?

732
00:40:43,360 --> 00:40:46,840
Yeah. Assuming that you can actually do
the technical side of Agi Development,

733
00:40:47,020 --> 00:40:49,780
I think there's going to be two key
problems with figuring out how do you

734
00:40:49,781 --> 00:40:51,130
actually deploy it,
make it go well.

735
00:40:51,490 --> 00:40:55,450
The first one of these is the
runup to building the first AGI.

736
00:40:56,320 --> 00:40:59,560
You look at how self driving cars are
being developed and it's a competitive

737
00:40:59,561 --> 00:41:03,220
race. And the thing that always happens
in competitive race is that you have huge

738
00:41:03,221 --> 00:41:05,440
amounts of pressure to get rid of safety.

739
00:41:06,780 --> 00:41:09,790
And so that's one thing we're very
concerned about, right, is that people,

740
00:41:10,030 --> 00:41:13,120
multiple teams figuring out,
we can actually get there,

741
00:41:13,600 --> 00:41:15,300
but you know,

742
00:41:15,310 --> 00:41:19,960
if we took the slower path that is more
guaranteed to be safe, we will lose.

743
00:41:20,230 --> 00:41:21,820
And so we're going to take the fast path.

744
00:41:22,360 --> 00:41:26,860
And so the more that we can both
ourselves be in a position where we don't

745
00:41:26,861 --> 00:41:30,820
generate that competitive race where we
say if the race is being run and that

746
00:41:30,840 --> 00:41:34,340
you know, someone else's is further ahead
than we are, we're not going to try to,

747
00:41:34,500 --> 00:41:37,160
to leap frog, we're going to
actually work with them. Right?

748
00:41:37,161 --> 00:41:38,540
We will help them succeed.

749
00:41:38,780 --> 00:41:42,770
As long as what they're trying to do is
to fulfill our mission, then we're good.

750
00:41:42,900 --> 00:41:44,930
We don't have to build AGI ourselves.
Um,

751
00:41:44,950 --> 00:41:46,760
and I think that's a really
important commitment from us.

752
00:41:47,060 --> 00:41:49,070
But it can't just be you unilateral,
right.

753
00:41:49,071 --> 00:41:52,880
I think it's really important that other
players were serious about building Agi,

754
00:41:53,090 --> 00:41:56,600
make similar commitments. Right.
And I think that that, again,

755
00:41:56,601 --> 00:41:59,180
to the extent that everyone believes
that Agi should be something to benefit

756
00:41:59,181 --> 00:42:02,120
everyone, then it actually really
shouldn't matter which company builds it.

757
00:42:02,390 --> 00:42:05,780
And we should all be concerned about the
case where we just raced so hard to get

758
00:42:05,781 --> 00:42:09,410
there. Then something goes wrong. So
what role do you think government,

759
00:42:10,490 --> 00:42:15,490
our favorite entity has in setting
policy and rules about this domain?

760
00:42:16,250 --> 00:42:18,260
From research to the development?

761
00:42:18,261 --> 00:42:22,730
To too early stage to late
stage AI and Agi development.

762
00:42:22,850 --> 00:42:27,350
So I think that first of
all, um, is really important
that governments in there,

763
00:42:27,800 --> 00:42:29,780
right in some way,
shape or form. You know,

764
00:42:29,781 --> 00:42:33,440
at the end of the day we're talking
about Olin technology that will shape how

765
00:42:33,560 --> 00:42:38,390
the world operates and that there needs
to be government as part of that answer.

766
00:42:39,020 --> 00:42:41,150
And so that's why we've, uh, we've,

767
00:42:41,151 --> 00:42:44,200
we've done a number of different
congressional testimonies. We, uh,

768
00:42:44,270 --> 00:42:47,390
interact with a number of different
lawmakers. Uh, and, uh, you know,

769
00:42:47,391 --> 00:42:52,391
right now a lot of our message to them
is that it's not the time for regulation.

770
00:42:54,290 --> 00:42:58,370
It is the time for measurement that
our main policy recommendation is that

771
00:42:58,371 --> 00:43:02,090
people, and you know, the government does
this all the time with bodies like nest.

772
00:43:02,290 --> 00:43:06,590
Um, spend time trying to figure
out just where the technology is,

773
00:43:06,591 --> 00:43:07,640
how fast it's moving,

774
00:43:08,120 --> 00:43:12,800
and can really become literate and up
to speed with respect to what to expect.

775
00:43:13,140 --> 00:43:17,420
Um, so I think that today
the answer really is about,
about, about measurement. Um,

776
00:43:17,570 --> 00:43:22,460
and I think that there will be a time and
place where that will change. And, uh,

777
00:43:22,550 --> 00:43:25,190
I think it's a little bit
hard to predict exactly what,

778
00:43:25,520 --> 00:43:27,050
what exactly that
trajectory should look like.

779
00:43:27,080 --> 00:43:32,080
So there will be a point
at which regulation federal
in the United States the

780
00:43:33,051 --> 00:43:36,980
government steps in and helps be the,

781
00:43:37,410 --> 00:43:41,180
I don't want to say the adult in the
room to make sure that there is strict

782
00:43:41,181 --> 00:43:45,740
rules, maybe conservative
rules that nobody can cross.
Oh, well I think there's,

783
00:43:45,741 --> 00:43:47,360
there's kind of maybe two,
two angles to it.

784
00:43:47,390 --> 00:43:51,530
So today with narrow AI applications
that I think there are already existing

785
00:43:51,531 --> 00:43:54,710
bodies that are responsible and
should be responsible for regulation.

786
00:43:54,860 --> 00:43:58,130
You think about it, for example, with
self driving cars that you want the, uh,

787
00:43:58,131 --> 00:44:02,780
you know, the national highway, I Nitsa
exactly. To, to be very good and that,

788
00:44:02,900 --> 00:44:04,130
that makes sense, right? The,

789
00:44:04,131 --> 00:44:07,400
basically what we're saying is that
we're going to have these technological

790
00:44:07,401 --> 00:44:11,630
systems that are going to be performing
applications that humans already do.

791
00:44:12,260 --> 00:44:16,090
Great. We already have ways of thinking
about standards and safety for those. Um,

792
00:44:16,100 --> 00:44:19,430
so I think it actually empowering
those regulators today is also pretty

793
00:44:19,431 --> 00:44:23,180
important. And then I think
for, for Agi, you know,

794
00:44:23,181 --> 00:44:25,970
that there's going to be a point
where we'll have better answers.

795
00:44:25,971 --> 00:44:29,810
And I think that maybe a similar approach
of first measurement and you know,

796
00:44:29,811 --> 00:44:31,380
start thinking about what
the rules are should be.

797
00:44:31,590 --> 00:44:35,850
I think it's really important that we
don't prematurely squash, you know,

798
00:44:35,851 --> 00:44:39,930
progress. I've got to get very easy
to kind of smother the abutting field.

799
00:44:40,140 --> 00:44:41,970
And I think that's
something to really avoid.

800
00:44:42,090 --> 00:44:44,520
But I don't think it's the
right way of doing it is to say,

801
00:44:44,660 --> 00:44:49,660
let's just try to blaze ahead and not
involve all of these other stakeholders.

802
00:44:51,450 --> 00:44:52,470
So,
uh,

803
00:44:53,370 --> 00:44:58,330
you've recently released a paper on
a gpt to language modeling, uh, uh,

804
00:44:58,800 --> 00:44:59,550
but,
uh,

805
00:44:59,550 --> 00:45:04,350
did not release the full model because
he had concerns about the possible

806
00:45:04,351 --> 00:45:07,200
negative effects of the
availability of such model.

807
00:45:07,470 --> 00:45:10,680
It's a outside of just that decision.

808
00:45:10,681 --> 00:45:15,590
It's super interesting because of the
discussion as at a societal level,

809
00:45:15,610 --> 00:45:19,200
the discourse it creates.
So it's fascinating in that aspect.

810
00:45:19,230 --> 00:45:22,470
But if you think it's the
specifics here at first,

811
00:45:22,830 --> 00:45:26,700
what are some negative effects
that you envisioned? And of course,

812
00:45:26,701 --> 00:45:28,320
what is some of the positive effects?

813
00:45:28,520 --> 00:45:30,530
Yeah, so again, I think to zoom out,

814
00:45:30,560 --> 00:45:35,540
like the way that we thought about gpt
too is that with language modeling,

815
00:45:35,750 --> 00:45:40,750
we are clearly on a trajectory right now
where we scale up our models and we get

816
00:45:42,440 --> 00:45:44,420
qualitatively better performance,
right?

817
00:45:44,421 --> 00:45:49,421
Gpt to itself was actually just a scale
up of a model that we've released in the

818
00:45:49,970 --> 00:45:50,601
previous June,
right?

819
00:45:50,601 --> 00:45:54,370
And we just ran it at a much larger
scale and we got these results where if

820
00:45:54,380 --> 00:45:57,180
suddenly starting to write coherent pros,
uh,

821
00:45:57,200 --> 00:46:01,610
which was not something
we'd seen previously. And
what are we doing now? Well,

822
00:46:01,611 --> 00:46:06,080
we're going to scale up GPD two by 10 x
by a hundred acts by thousand decks and

823
00:46:06,110 --> 00:46:07,310
we don't know what we're going to get.

824
00:46:07,820 --> 00:46:12,020
And so it's very clear that the model
that we, that we released last June,

825
00:46:12,590 --> 00:46:15,740
you know, I think it's kind of like, it's,
it's, it's, it's a good academic toy.

826
00:46:16,400 --> 00:46:19,790
It's not something that we think is
something that can really have negative

827
00:46:19,791 --> 00:46:21,710
applications or you know,
to the extent that a candidate,

828
00:46:21,720 --> 00:46:25,390
the positive of people being able to
play with it, uh, is, is you know,

829
00:46:25,400 --> 00:46:26,420
far far outweighs the,

830
00:46:26,670 --> 00:46:31,670
the possible harms you fast
forward to not gpt to but you 20.

831
00:46:32,120 --> 00:46:34,170
Okay. And you think about
what that's going to be like.

832
00:46:34,650 --> 00:46:37,740
And I think that the capabilities
are going to be substantive.

833
00:46:38,160 --> 00:46:41,970
And so there needs to be a point
in between the two where you say,

834
00:46:42,480 --> 00:46:46,200
this is something where we are drawing
the line and that we need to start

835
00:46:46,201 --> 00:46:49,050
thinking about the safety aspects.
And I think for GPD too,

836
00:46:49,051 --> 00:46:52,260
we could have gone either
way. And in fact, when we
had conversations internally,

837
00:46:52,430 --> 00:46:56,160
uh, that we had a bunch of pros and
cons, um, and it wasn't clear which one,

838
00:46:56,190 --> 00:47:00,060
which one outweighed the other. Um, and
I think that when we announced that, hey,

839
00:47:00,061 --> 00:47:01,710
we decided not to release this model,

840
00:47:02,100 --> 00:47:04,410
then there was a bunch of conversation
where various people said,

841
00:47:04,411 --> 00:47:06,210
it's so obvious that you
should have just released it.

842
00:47:06,330 --> 00:47:08,460
There are other people said it's so
obvious you should not have released it.

843
00:47:08,820 --> 00:47:12,270
And I think that that almost
definitionally means that
holding it back was the

844
00:47:12,271 --> 00:47:14,640
correct decision, right?
If it's con, if there's,

845
00:47:14,641 --> 00:47:17,370
if it's not obvious whether
something is beneficial or not,

846
00:47:17,610 --> 00:47:21,100
you should probably default to caution.
And so I think that the, that the,

847
00:47:21,210 --> 00:47:25,140
that the overall landscape for how we
think about it is that this decision could

848
00:47:25,141 --> 00:47:27,600
have gone either way. There's
great arguments of both directions,

849
00:47:27,900 --> 00:47:30,150
but for future models down the road,
um,

850
00:47:30,250 --> 00:47:32,760
and possibly sooner than you'd
expect because, you know,

851
00:47:32,770 --> 00:47:34,600
scaling these things up
doesn't have to take that long.

852
00:47:35,650 --> 00:47:39,160
Those ones you're definitely not going
to want to, to release into the wild.

853
00:47:39,550 --> 00:47:40,383
And so,
um,

854
00:47:40,450 --> 00:47:44,920
I think that we almost view this as a
test case and to see can we even design,

855
00:47:45,100 --> 00:47:46,480
you know,
how do you have a society,

856
00:47:46,820 --> 00:47:49,750
how do you have a system that goes
from having no concept of responsible

857
00:47:49,751 --> 00:47:50,470
disclosure,

858
00:47:50,470 --> 00:47:55,180
where the mere idea of not releasing
something for safety reasons is unfamiliar

859
00:47:55,900 --> 00:47:58,600
to a world where you say,
okay, we have a powerful model.

860
00:47:58,660 --> 00:47:59,650
Let's at least think about it.

861
00:47:59,651 --> 00:48:02,650
Let's go through some process and you
think about the security community,

862
00:48:02,651 --> 00:48:05,920
it took them a long time to design
responsible disclosure, right? You know,

863
00:48:05,921 --> 00:48:08,560
you think about this question of,
well, I have a security exploit.

864
00:48:08,740 --> 00:48:12,610
I sent it to the company. The company is
like, tries to prosecute me or just sit,

865
00:48:12,880 --> 00:48:17,440
just ignores it. What do I do? Right?
And so you know, the alternatives of, Oh,

866
00:48:17,441 --> 00:48:20,140
I just just always publish your exploits.
That doesn't seem good either. Right?

867
00:48:20,141 --> 00:48:23,500
And so it really took a long
time and took this, this, uh,

868
00:48:23,501 --> 00:48:25,270
it was bigger than any individual,
right?

869
00:48:25,271 --> 00:48:27,910
It's really about building a whole
community that believed that, okay,

870
00:48:27,911 --> 00:48:29,980
we'll have this process where you
sent it to the company. You know,

871
00:48:30,130 --> 00:48:31,660
if they don't act in a certain time,

872
00:48:31,661 --> 00:48:35,470
then you can go public and you're not a
bad person, you've done the right thing.

873
00:48:35,700 --> 00:48:38,290
Um,
and I think that in AI,

874
00:48:38,620 --> 00:48:42,760
part of the response to GPD to just
proves that we don't have any concept of

875
00:48:42,761 --> 00:48:46,910
this. Um, so that's the
high level picture. Um,

876
00:48:47,050 --> 00:48:49,300
and so I think that,
I think this was,

877
00:48:49,301 --> 00:48:52,990
this was a really important move to make
and we could have maybe delayed it for

878
00:48:52,991 --> 00:48:55,570
TBT three,
but I'm really glad we did it for gpt to,

879
00:48:56,050 --> 00:48:59,200
and so now you look at gpt to itself and
you think about the substance of okay,

880
00:48:59,380 --> 00:49:01,060
what are potential negative applications?

881
00:49:01,270 --> 00:49:04,630
So you have this model that's been
trained on the Internet, which, you know,

882
00:49:04,631 --> 00:49:07,840
it's also going to be a
bunch of very biased data. A
bunch of, you know, very, uh,

883
00:49:07,841 --> 00:49:10,210
offensive content in there. Uh, and a,

884
00:49:10,211 --> 00:49:14,560
you can ask it to generate content
for you on basically any topic, right?

885
00:49:14,561 --> 00:49:17,320
You just give it a prompt and we'll just
start and start writing and our rights

886
00:49:17,650 --> 00:49:19,240
content like you see on the Internet,
you know,

887
00:49:19,250 --> 00:49:22,470
even down to like saying
advertisement in the middle of,

888
00:49:22,471 --> 00:49:24,850
of some of it's generations. And, uh,

889
00:49:24,851 --> 00:49:28,540
you think about the possibilities for
generating fake news or abusive content.

890
00:49:29,230 --> 00:49:31,840
And you know, it's interesting seeing
what people have done with uh, you know,

891
00:49:31,841 --> 00:49:33,970
we released a smaller version of gpt to,

892
00:49:34,360 --> 00:49:37,180
and a that people have done
things like try to generate,

893
00:49:37,710 --> 00:49:42,370
I take my own Facebook message history
and generate more Facebook messages like

894
00:49:42,371 --> 00:49:43,760
me. Um, and uh,

895
00:49:43,840 --> 00:49:48,260
people generating fake politician
I content or I, you know, there,

896
00:49:48,261 --> 00:49:52,690
there's a bunch of things there were
nice to have to think is this going to be

897
00:49:52,691 --> 00:49:55,720
good for the world?
There's the flip side,

898
00:49:55,721 --> 00:49:58,570
which is I think that there's a lot of
awesome applications that we really want

899
00:49:58,571 --> 00:49:59,350
to see,

900
00:49:59,350 --> 00:50:04,350
like creative applications in terms of
if you have saifai authors that can work

901
00:50:04,751 --> 00:50:08,620
with this tool and come up with cool
ideas like that seems, that seems awesome.

902
00:50:08,621 --> 00:50:11,830
If we can write better Saifai through
the use of these tools and we've actually

903
00:50:11,831 --> 00:50:16,070
had a bunch of people write into us
asking, Hey, can we use it for, you know,

904
00:50:16,100 --> 00:50:19,360
a variety of different creative
applications. The positive,

905
00:50:19,361 --> 00:50:24,340
I actually pretty easy to
imagine, uh, there if, you know,

906
00:50:24,341 --> 00:50:28,550
the, the usual and a applications
are really interesting.

907
00:50:28,790 --> 00:50:30,920
But let's go there.

908
00:50:30,921 --> 00:50:35,000
It's kind of interesting to think
about a world where I look at Twitter

909
00:50:36,120 --> 00:50:37,860
where that just fake news,

910
00:50:37,861 --> 00:50:41,670
but smarter and smarter
bots being able to uh,

911
00:50:42,390 --> 00:50:47,390
spread in an interesting
complex networking way in
information that just floods

912
00:50:48,211 --> 00:50:52,260
out us regular human beings
with our original thoughts.

913
00:50:52,800 --> 00:50:53,633
So

914
00:50:54,220 --> 00:50:59,080
what are your views of this
world will gpt 20, right?

915
00:50:59,760 --> 00:51:01,530
What do you, how do we
think about it? Again,

916
00:51:01,531 --> 00:51:06,420
it's like one of those things about in
the fifties trying to describe the, uh,

917
00:51:06,510 --> 00:51:09,930
the internet or the smartphone.
What do you think about that world,

918
00:51:09,931 --> 00:51:11,190
the nature of information?

919
00:51:11,730 --> 00:51:16,730
Do we add one possibility is that
we'll always try to design systems that

920
00:51:16,951 --> 00:51:21,240
identify it, robot versus human,
and we'll do so successfully.

921
00:51:21,241 --> 00:51:24,300
And so we will authenticate
that we're still human.

922
00:51:24,600 --> 00:51:26,840
And the other world is
that we just accept the,

923
00:51:27,100 --> 00:51:31,740
the fact that we're swimming in a sea of
fake news and just learn to swim there.

924
00:51:32,170 --> 00:51:33,750
Well,
have you ever seen the uh,

925
00:51:34,770 --> 00:51:39,580
pop popular meme of a, of a robot, uh,

926
00:51:39,610 --> 00:51:42,970
with, with a physical, physical arm in
pen clicking the I am not a robot button.

927
00:51:43,420 --> 00:51:45,560
Yeah.
I think,

928
00:51:45,580 --> 00:51:49,990
I think that the truth is that that really
trying to distinguish between a robot

929
00:51:49,991 --> 00:51:53,770
and human is a losing battle.
Ultimately you think it's a losing battle.

930
00:51:53,800 --> 00:51:55,510
I think it's a losing battle ultimately.
Right?

931
00:51:55,511 --> 00:51:57,760
I think that that is that
in terms of of the content,

932
00:51:57,761 --> 00:51:59,200
in terms of the actions that you can take.

933
00:51:59,380 --> 00:52:01,180
I mean think about how captures have gone,
right.

934
00:52:01,181 --> 00:52:03,790
The capture is used to be a very nice,
simple, you just have this image.

935
00:52:04,750 --> 00:52:08,800
All of our OCR is terrible. You put a
couple of of artifacts in it, you know,

936
00:52:08,860 --> 00:52:12,910
humans are going to be able to tell what
it is an AI system wouldn't be able to

937
00:52:13,240 --> 00:52:15,640
today like I can barely do captures.
Yeah,

938
00:52:15,700 --> 00:52:18,310
and I think that this is just
kind of where we're going,

939
00:52:18,311 --> 00:52:21,580
I think captures where we're a moment
in time thing and as AI system to become

940
00:52:21,581 --> 00:52:25,630
more powerful that they're being human
capabilities that can be measured in a

941
00:52:25,631 --> 00:52:29,950
very easy automated way that
the AI's will not be capable of.

942
00:52:30,130 --> 00:52:33,070
I think that's just like, it's just
an increasingly hard technical battle,

943
00:52:34,090 --> 00:52:36,190
but it's not that all hope is lost.
Right?

944
00:52:36,191 --> 00:52:40,940
And you think about how do we already
authenticate ourselves, right? You know,

945
00:52:40,990 --> 00:52:44,470
we have systems, we have social security
numbers. If you're in the US or you know,

946
00:52:44,471 --> 00:52:46,670
you have, you have, uh, you know,

947
00:52:46,720 --> 00:52:50,830
ways of identifying individual people
and having real world identity tied to

948
00:52:50,890 --> 00:52:54,870
digital identities seems like
a step towards, you know,

949
00:52:54,890 --> 00:52:57,520
authenticating the source of content
rather than the content itself.

950
00:52:58,240 --> 00:52:59,590
Now there are problems with that.

951
00:52:59,980 --> 00:53:04,030
How can you have privacy and anonymity
in a world where the only content you can

952
00:53:04,031 --> 00:53:05,560
really trust is we're,

953
00:53:05,561 --> 00:53:08,690
the only way you can trust content is
by looking at where it comes from. Um,

954
00:53:08,700 --> 00:53:11,860
and so I think that building up
good reputation networks, maybe,

955
00:53:12,040 --> 00:53:15,800
maybe one possible solution. But yeah,
I think that this, this question is,

956
00:53:15,801 --> 00:53:19,100
is not an obvious one. And
I think that we, you know,

957
00:53:19,300 --> 00:53:21,310
maybe sooner than we think we'll
be in a world where, you know,

958
00:53:21,311 --> 00:53:24,310
today I often will read
a tweet and be like,

959
00:53:24,550 --> 00:53:26,520
do I feel like real human
wrote this? Or, you know,

960
00:53:26,521 --> 00:53:27,510
don't feel like it's just like genuine.

961
00:53:27,511 --> 00:53:30,060
I feel like I can kind of judge
the content a little bit. Um,

962
00:53:30,090 --> 00:53:33,720
and I think in the future it just won't
be the case. You look at, for example,

963
00:53:33,750 --> 00:53:36,420
the FCC comments on net neutrality,

964
00:53:36,540 --> 00:53:40,470
I came out later that millions of
those were autogenerated and that the

965
00:53:40,650 --> 00:53:43,530
researchers were able to do various
statistical tick techniques to do that.

966
00:53:43,980 --> 00:53:47,670
What do you do in a world where those
statistical techniques don't exist?

967
00:53:47,671 --> 00:53:50,610
It's just impossible to tell the
difference between humans and AI highs.

968
00:53:50,611 --> 00:53:54,740
And in fact, the, uh, the, the, the most
persuasive arguments are written by,

969
00:53:55,010 --> 00:53:58,950
by Ai, all that stuff. It's
not saifai anymore. Okay.

970
00:53:58,980 --> 00:54:02,130
Gpt to making a great argument for
why recycling is bad for the world.

971
00:54:02,520 --> 00:54:07,320
You got to read that, you know, like,
Huh. You're right. Yeah. That's,

972
00:54:07,350 --> 00:54:08,340
that's quite interesting.
I mean,

973
00:54:08,341 --> 00:54:12,960
ultimately it boils down to the physical
world being the last frontier of

974
00:54:12,961 --> 00:54:13,720
proving right.

975
00:54:13,720 --> 00:54:16,040
You said like basically
networks of people,

976
00:54:16,070 --> 00:54:21,070
humans vouching for humans in the physical
world and somehow the authentication

977
00:54:21,670 --> 00:54:24,350
and ends there. I mean,
if I had to ask you,

978
00:54:25,560 --> 00:54:28,130
you're way too eloquent for a human.

979
00:54:28,131 --> 00:54:31,730
So if I had to ask you to authenticate,
like prove,

980
00:54:31,731 --> 00:54:34,490
how do I know you're not a robot,
how do you know I'm not a robot?

981
00:54:35,080 --> 00:54:39,080
I think that's so far we're,
we're

982
00:54:39,190 --> 00:54:41,860
this in this space,
this conversation we just had,

983
00:54:42,070 --> 00:54:47,070
the physical movements we did is the
biggest gap between us and AI systems as

984
00:54:47,201 --> 00:54:50,800
the physical manipulation.
So maybe that's the last frontier.

985
00:54:51,310 --> 00:54:54,550
Well, here's another question
is, is, you know, why, why is,

986
00:54:55,000 --> 00:54:57,310
why is solving this problem important,
right?

987
00:54:57,311 --> 00:54:59,080
Like what aspects are
really important to us?

988
00:54:59,081 --> 00:55:03,250
I think that probably where we'll end
up is we'll hone in on what do we really

989
00:55:03,251 --> 00:55:08,050
want out of knowing if we're talking to
a human. Um, and, uh, and I think that,

990
00:55:08,051 --> 00:55:09,460
again,
this comes down to identity.

991
00:55:09,461 --> 00:55:11,770
And so I think that that
the Internet of the future,

992
00:55:11,771 --> 00:55:14,700
I expect to be one that we'll
have lots of agents out there, uh,

993
00:55:14,810 --> 00:55:19,100
that will interact with you. But I think
that the question of is this, you know,

994
00:55:19,210 --> 00:55:23,660
flush, real flesh and blood human,
or is this an automated system? Uh,

995
00:55:23,860 --> 00:55:27,250
may actually just be less important.
Let's actually go there.

996
00:55:27,370 --> 00:55:32,020
It's gpt to is impressive
and that's like a gpt 20.

997
00:55:32,440 --> 00:55:37,440
Why is it so bad that all
my friends are gpt 20?

998
00:55:39,160 --> 00:55:44,010
Why? Why is it so, why is it so important
on the intranet do you think, uh,

999
00:55:44,700 --> 00:55:47,310
to interact with only human beings?

1000
00:55:47,311 --> 00:55:52,290
Why can't we live in a world where ideas
can come from models trained on human

1001
00:55:52,291 --> 00:55:52,960
data?

1002
00:55:52,960 --> 00:55:55,660
Yeah, I think this is, I think this is
actually a really interesting question.

1003
00:55:55,661 --> 00:55:59,200
This comes back to the how do you even
picture a world with some new technology,

1004
00:55:59,250 --> 00:56:03,070
right? And I think that the one thing that
I think is important is, is, you know,

1005
00:56:03,220 --> 00:56:06,820
good say honesty. Um, and I
think that if you have, you know,

1006
00:56:06,821 --> 00:56:11,050
almost in the Turing test style a
sense sense of, of, of technology,

1007
00:56:11,051 --> 00:56:15,280
you have ais that are pretending to be
humans and deceiving you. I think that is,

1008
00:56:15,500 --> 00:56:17,500
you know, that that feels
like a bad thing, right?

1009
00:56:17,501 --> 00:56:20,560
I think that it's really important that
we feel like we're in control of our

1010
00:56:20,561 --> 00:56:21,200
environment,
right?

1011
00:56:21,200 --> 00:56:26,070
That we understand who we're interacting
and if it's an AI or a human, um, that,

1012
00:56:26,270 --> 00:56:28,330
that that's not something that
we're being deceived about.

1013
00:56:28,600 --> 00:56:32,110
But I think that the flip side of can
I have as meaningful of an interaction

1014
00:56:32,111 --> 00:56:34,150
with an AI as I can with the human?
Uh,

1015
00:56:34,180 --> 00:56:38,350
well I actually think here you can turn
into Scifi and her I think is a great

1016
00:56:38,351 --> 00:56:40,720
example of asking this very question,
right?

1017
00:56:41,050 --> 00:56:44,440
One thing I really love about her is it
really starts out almost by asking how

1018
00:56:44,441 --> 00:56:47,830
meaningful our human virtual
relationships, right? And, uh,

1019
00:56:48,040 --> 00:56:51,700
and then you have a human who has
a relationship with an AI and, uh,

1020
00:56:51,900 --> 00:56:54,250
that you really start to
be drawn into that, right?

1021
00:56:54,251 --> 00:56:57,790
And that all of your emotional buttons
get triggered in the same way as if there

1022
00:56:57,791 --> 00:57:00,190
was a real human that was on
the other side of that phone.

1023
00:57:00,640 --> 00:57:05,140
And so I think that this is one way of
thinking about it is that I think that we

1024
00:57:05,141 --> 00:57:08,800
can have meaningful interactions
and that if there's a funny joke,

1025
00:57:09,640 --> 00:57:12,550
some sense it doesn't really matter
if it was written by a human or an AI,

1026
00:57:12,850 --> 00:57:16,210
but what you don't want and away where
I think we should really draw hard lines

1027
00:57:16,360 --> 00:57:20,590
is deception. Um, and I think that as long
as we're in a world where, you know, why,

1028
00:57:20,610 --> 00:57:22,580
why do we build AI systems at all,
right?

1029
00:57:22,600 --> 00:57:24,820
The reason we went to build
them is to enhance human lives,

1030
00:57:24,940 --> 00:57:26,470
to make humans be able to do more things,

1031
00:57:26,590 --> 00:57:28,720
to have human humans feel more fulfilled.

1032
00:57:29,020 --> 00:57:32,830
And if we can build AI systems that
do that, uh, you know, sign me up.

1033
00:57:33,130 --> 00:57:34,960
So the process of language modeling,

1034
00:57:37,060 --> 00:57:41,590
how far do you think it takes us? Let's
look a movie her. Do you think, uh,

1035
00:57:42,410 --> 00:57:46,720
uh, dialogue, natural
language conversation is
formulated by the Turing test,

1036
00:57:46,721 --> 00:57:47,554
for example.

1037
00:57:47,800 --> 00:57:51,970
Do you think that process could be
achieved through this kind of unsupervised

1038
00:57:51,971 --> 00:57:52,804
language modeling?

1039
00:57:53,080 --> 00:57:58,080
So I think the Turing test in its in
its real form isn't just about language,

1040
00:57:58,450 --> 00:58:01,870
right? It's really about reasoning to
write that to really pass the Turing test.

1041
00:58:01,870 --> 00:58:06,280
I should be able to teach calculus to
whoever's on the other side and have it

1042
00:58:06,281 --> 00:58:08,430
really understand calculus and be able to,
you know,

1043
00:58:08,460 --> 00:58:10,360
go in and solve new calculus problems.

1044
00:58:11,290 --> 00:58:13,840
And so I think that to
really solve the Turing test,

1045
00:58:13,960 --> 00:58:16,300
we need more than what we're
seeing with language models.

1046
00:58:16,420 --> 00:58:19,120
We need some way of
plugging in reasoning now,

1047
00:58:19,750 --> 00:58:23,500
how different will that be from what
we already do? That's an open question,

1048
00:58:23,590 --> 00:58:23,831
right?

1049
00:58:23,831 --> 00:58:27,640
It might be that we need some sequence
of totally radical new ideas or it might

1050
00:58:27,641 --> 00:58:31,120
be that we just need to kind of shape
our existing systems in a slightly

1051
00:58:31,121 --> 00:58:35,770
different way. But I think that in terms
of how far language model we'll go,

1052
00:58:35,860 --> 00:58:39,700
it's already gone way further than
many people would have expected. Right.

1053
00:58:39,701 --> 00:58:40,630
I think that things like,

1054
00:58:40,860 --> 00:58:43,840
and they think there's a lot of really
interesting angles to poke in terms of

1055
00:58:43,990 --> 00:58:48,570
how much does gpt to understand
physical world. Like, you know, you,

1056
00:58:48,571 --> 00:58:52,870
you read a little bit about
fire underwater, uh, in, in
Gbd too. So it's like, okay,

1057
00:58:52,871 --> 00:58:56,650
maybe it doesn't quite understand what
these things are. But at the same time,

1058
00:58:56,890 --> 00:59:01,030
I think that you also see various things
like smoke coming from flame and you

1059
00:59:01,031 --> 00:59:03,610
know, a bunch of these things
that Gbtq it has no body,

1060
00:59:03,640 --> 00:59:07,410
it has no physical experience,
it's just statically red data. Um,

1061
00:59:07,480 --> 00:59:11,440
and I think that,
I think that if the answer is like,

1062
00:59:11,620 --> 00:59:14,460
we don't know yet, um,
these questions though,

1063
00:59:14,560 --> 00:59:17,860
we're starting to be able to actually
ask them to physical systems that real

1064
00:59:17,890 --> 00:59:20,290
systems that exist and that's
very exciting. Do you think,

1065
00:59:20,410 --> 00:59:22,700
what's your intuition you
think if you just scale

1066
00:59:22,730 --> 00:59:26,330
language modeling, uh, like, uh,

1067
00:59:26,360 --> 00:59:28,400
significantly scale that reasoning

1068
00:59:28,510 --> 00:59:31,030
can emerge from the same exact mechanisms?

1069
00:59:31,270 --> 00:59:36,270
I think it's unlikely that if we just
scale GPD to that will have reasoning in

1070
00:59:37,030 --> 00:59:39,580
the full fledged way. And I think
that there's like, you know,

1071
00:59:39,700 --> 00:59:41,430
the type signatures a little bit wrong,
right?

1072
00:59:41,620 --> 00:59:45,760
Like there's something we do with
that we call thinking, right,

1073
00:59:45,761 --> 00:59:47,440
where we spend a lot of compute,

1074
00:59:47,580 --> 00:59:50,620
like a variable amount of compute
to get to better answers, right?

1075
00:59:50,621 --> 00:59:53,650
I think a little bit harder, I
get a better answer. And that,

1076
00:59:53,890 --> 00:59:58,840
that kind of type signature isn't
quite encoded in a Gbt, right?

1077
00:59:58,841 --> 01:00:00,430
Gbt will kind of like,

1078
01:00:00,431 --> 01:00:04,270
it spent a long time and it's like
evolutionary history baking and all this

1079
01:00:04,271 --> 01:00:06,670
information getting very,
very good at this predictive process.

1080
01:00:07,000 --> 01:00:11,400
And then at runtime I just kind
of do one Ford Pass and uh, and,

1081
01:00:11,440 --> 01:00:14,290
and be able to generate
stuff. And so, you know,

1082
01:00:14,291 --> 01:00:17,710
there might be small tweaks to what we
do in order to get the type signature

1083
01:00:17,711 --> 01:00:21,010
right. For example. Well, you know, it's
not really one for had passed. Right.

1084
01:00:21,011 --> 01:00:21,131
You know,

1085
01:00:21,131 --> 01:00:24,640
you generate some boy I symbol and so
maybe you generate like a whole sequence

1086
01:00:24,641 --> 01:00:27,880
of, of, of thoughts and you only keep
the last bit or something. Right. Um,

1087
01:00:28,180 --> 01:00:29,830
but I think that at the very least,

1088
01:00:29,831 --> 01:00:31,450
I would expect you have
to make changes like that.

1089
01:00:32,170 --> 01:00:33,820
Yeah. Yeah. Just exactly how we,

1090
01:00:34,090 --> 01:00:38,980
you said think is the process
of generating thought by
thought in the same kind

1091
01:00:38,981 --> 01:00:43,700
of way. Like you said, keep the last bit
the thing that we converge towards. Yup.

1092
01:00:44,970 --> 01:00:47,290
And I think there's, there's another
piece which is, which is interesting,

1093
01:00:47,291 --> 01:00:50,100
which is this out of distribution
generalization, right?

1094
01:00:50,110 --> 01:00:52,600
And they're like thinking somehow
that's, let's do that, right.

1095
01:00:52,601 --> 01:00:54,310
That we haven't experienced a thing.

1096
01:00:54,430 --> 01:00:57,540
And yet somehow we just kind of keep
refining our mental model of it. Um,

1097
01:00:58,060 --> 01:00:58,841
this is again,

1098
01:00:58,841 --> 01:01:03,841
something that feels tied to whatever
reasoning is and maybe it's a small tweak

1099
01:01:04,901 --> 01:01:07,690
to what we do, maybe as many ideas
and we'll take as many decades.

1100
01:01:08,060 --> 01:01:09,740
Yeah. So the, the assumption there,

1101
01:01:09,770 --> 01:01:14,770
a generalization out of distribution is
that it's possible to create new ideas,

1102
01:01:17,500 --> 01:01:20,840
the Po, you know, as possible that
nobody's ever created any new ideas.

1103
01:01:20,841 --> 01:01:25,300
And then with scaling Gpt to did gpt 20,
uh,

1104
01:01:25,301 --> 01:01:26,134
you would,

1105
01:01:26,180 --> 01:01:30,640
you would essentially generalize
to all possible thoughts I'll,

1106
01:01:30,680 --> 01:01:31,513
Siemens could have.

1107
01:01:32,790 --> 01:01:35,980
I think so, just to play
devil's advocate here, right?
I mean, how many, how many, uh,

1108
01:01:36,010 --> 01:01:40,030
new, new story ideas have we come up
with Shakespeare, right? Yeah, exactly.

1109
01:01:41,590 --> 01:01:44,980
It's just all different forms of
love and drama and so on. Okay.

1110
01:01:45,790 --> 01:01:46,840
Not sure if you read,
but

1111
01:01:46,840 --> 01:01:49,970
a lesson, a recent blog post
by rich Sutton. Yup. I have,

1112
01:01:50,860 --> 01:01:52,280
he basically says,
uh,

1113
01:01:52,300 --> 01:01:55,450
something that echoes some of the
ideas that you've been talking about,

1114
01:01:55,451 --> 01:01:56,330
which is a,

1115
01:01:56,800 --> 01:02:00,760
he says the biggest lesson that can be
read from so many years of AI research is

1116
01:02:00,761 --> 01:02:05,440
that general methods that leverage
computation are ultimately going to, uh,

1117
01:02:05,920 --> 01:02:08,890
ultimately went out. Uh,
do you agree with this?

1118
01:02:08,891 --> 01:02:13,891
So basically have an open the in general
about ideas that you're exploring about

1119
01:02:14,450 --> 01:02:19,450
coming up with methods or there's gpt
to modeling or whether it's a five

1120
01:02:20,100 --> 01:02:25,100
playing dota where a general method is
better than a more fine tuned expert

1121
01:02:26,100 --> 01:02:26,933
tuned

1122
01:02:28,630 --> 01:02:31,510
method. Yeah. So I think that, well,

1123
01:02:31,511 --> 01:02:34,570
one thing that I think was
really interesting about
the reaction to that blog

1124
01:02:34,571 --> 01:02:35,650
post was an audit.

1125
01:02:35,651 --> 01:02:38,980
People have read this as saying
that compute is all that matters.

1126
01:02:39,380 --> 01:02:41,290
And that's a very threatening idea.
Right.

1127
01:02:41,320 --> 01:02:43,660
And I don't think it's a true idea either.
Right?

1128
01:02:43,661 --> 01:02:47,170
It's very clear that we have algorithmic
ideas that have been very important for

1129
01:02:47,171 --> 01:02:49,240
making progress and to really build AGI.

1130
01:02:49,480 --> 01:02:52,570
You want to push as far as you can and
the computational scale and you want to

1131
01:02:52,571 --> 01:02:56,830
push as far as you can on human human
ingenuity. And so I think you need both.

1132
01:02:56,950 --> 01:02:59,590
But I think the way that you phrased the
question is actually very good, right?

1133
01:02:59,591 --> 01:03:03,580
That it's really about what kind
of ideas should we be striving for.

1134
01:03:03,970 --> 01:03:08,850
And absolutely, if you can find a
scalable idea, pour more computing power,

1135
01:03:08,860 --> 01:03:13,510
more data into it, it gets better. Like
that's, that's the real holy grail.

1136
01:03:13,780 --> 01:03:17,560
And so I think that, uh,
the, the, the answer to the
question, and I think it's yes.

1137
01:03:17,740 --> 01:03:18,261
Um,
that,

1138
01:03:18,261 --> 01:03:21,730
that that's really how we think about
it and that part of why we're excited

1139
01:03:21,820 --> 01:03:23,280
about the power of deep learning,

1140
01:03:23,310 --> 01:03:27,730
the potential for building Agi is because
we look at the systems that exist in

1141
01:03:27,731 --> 01:03:32,590
the most successful AI systems and
we realize that you scale those up,

1142
01:03:32,650 --> 01:03:33,550
they're going to work better.

1143
01:03:33,970 --> 01:03:37,180
And I think that that scalability is
something that really gives us hope for

1144
01:03:37,181 --> 01:03:39,040
being able to build
transformative systems.

1145
01:03:39,550 --> 01:03:43,450
So I'll tell you this, it
partially and emotional, you know,

1146
01:03:43,451 --> 01:03:45,700
a thing that response
that people often have,

1147
01:03:45,701 --> 01:03:49,720
this computer is so important for
state of the outperformance. You know,

1148
01:03:49,721 --> 01:03:50,710
individual developers,

1149
01:03:50,711 --> 01:03:53,770
maybe a 13 year olds sitting somewhere
in Kansas or something like that,

1150
01:03:54,160 --> 01:03:55,000
you know,
they're sitting,

1151
01:03:55,001 --> 01:03:58,660
they might not even have a GPU
and or may have a single GPU,

1152
01:03:58,661 --> 01:04:02,590
a 10 80 or something like that. And
there's this feeling like, well,

1153
01:04:02,591 --> 01:04:07,591
how can I possibly compete or contribute
to this world of AI if a scale is so

1154
01:04:08,621 --> 01:04:12,490
important? So if you can
comment on that. And in general,

1155
01:04:12,491 --> 01:04:15,860
do you think we need to also
in the future focus on, uh,

1156
01:04:15,940 --> 01:04:19,990
democratizing compute resources more,
more,

1157
01:04:19,991 --> 01:04:22,300
or as much as we
democratize the algorithms?

1158
01:04:22,620 --> 01:04:26,850
Well, so the way that I think about
it is that there's this space of,

1159
01:04:27,230 --> 01:04:28,860
of possible progress,
right?

1160
01:04:28,861 --> 01:04:31,500
There's a space of ideas and
sort of systems that will work,

1161
01:04:31,501 --> 01:04:34,650
that will move us forward.
And there's a portion of that space.

1162
01:04:34,800 --> 01:04:35,640
And to some extent,

1163
01:04:35,740 --> 01:04:39,150
increasingly significant portion of that
space that does just require massive

1164
01:04:39,151 --> 01:04:40,110
compute resources.

1165
01:04:41,030 --> 01:04:46,030
And for that that I think that the answer
is kind of clear that part of why we

1166
01:04:46,670 --> 01:04:49,810
have the structure that we do is because
we think it's really important to you

1167
01:04:49,811 --> 01:04:53,360
pushing the scale and to be, you know,
building these large clusters and systems.

1168
01:04:53,750 --> 01:04:57,470
But there's another part portion of the
space that isn't about the large scale

1169
01:04:57,471 --> 01:04:59,900
compute that are these ideas that,
and again,

1170
01:04:59,901 --> 01:05:03,200
I think that for the days to really
be impactful and really shine,

1171
01:05:03,350 --> 01:05:06,740
that they should be ideas that if you
scale them up would work way better than

1172
01:05:06,741 --> 01:05:08,630
they do at small scale.
Um,

1173
01:05:08,780 --> 01:05:12,230
but that you can discover them without
massive computational resources.

1174
01:05:12,710 --> 01:05:15,140
And if you look at the history of,
of recent developments,

1175
01:05:15,141 --> 01:05:19,270
you think about things like the Gan or
the VAE that these are ones that I think

1176
01:05:19,271 --> 01:05:22,300
you could come up with them
without having. And you know,

1177
01:05:22,301 --> 01:05:24,820
in practice people did come up with,
with them without having massive,

1178
01:05:24,821 --> 01:05:27,840
massive computational resources.
Right. I just talked to Ian Goodfellow.

1179
01:05:27,910 --> 01:05:32,910
But the thing is the initial Gan
produce pretty terrible results,

1180
01:05:33,850 --> 01:05:36,250
right?
So only because it wasn't a very specific,

1181
01:05:36,251 --> 01:05:39,970
it was only because they're smart enough
to know that this is quite surprising

1182
01:05:39,971 --> 01:05:43,120
him generated anything.
Did they know any,

1183
01:05:43,140 --> 01:05:47,560
do you see a world or is
that too optimistic and
dream or like to imagine that

1184
01:05:48,040 --> 01:05:53,040
the compute resources or something that's
owned by governments and provided a as

1185
01:05:53,891 --> 01:05:57,540
utility? Actually, someone sent
this, this question reminds me of,

1186
01:05:57,750 --> 01:06:01,590
of a blog post from one of my former
professors at Harvard. It's this guy map,

1187
01:06:01,630 --> 01:06:05,050
not Welsh, who was assistant professor.
I remember sitting in his tenure talk,

1188
01:06:05,051 --> 01:06:08,620
right? And, uh, you know that he
had literally just gotten tenure.

1189
01:06:08,770 --> 01:06:11,770
You went to Google for
the summer. Uh, and uh,

1190
01:06:11,980 --> 01:06:16,600
I then decided he wasn't going back to
academia and that kind of in his blog

1191
01:06:16,601 --> 01:06:16,790
posts,

1192
01:06:16,790 --> 01:06:21,790
he makes this point that look as a systems
researcher that I come up with these

1193
01:06:21,911 --> 01:06:24,700
cool system ideas, right? And I
kind of belittle a proof of concept.

1194
01:06:25,030 --> 01:06:29,410
And the best thing I can hope for is
that the people at Google or Yahoo,

1195
01:06:30,070 --> 01:06:31,240
which was around at the time,

1196
01:06:32,220 --> 01:06:35,020
we'll implement it and
actually make it work at scale.

1197
01:06:35,330 --> 01:06:36,580
That's like the dream for me,
right?

1198
01:06:36,581 --> 01:06:38,740
I built the little thing and they turned
into the big thing that's actually

1199
01:06:38,741 --> 01:06:43,300
working and a for him. He
said, I'm done with that.

1200
01:06:43,330 --> 01:06:46,840
I want to be the person who is, who's
actually doing building and deploying.

1201
01:06:47,290 --> 01:06:49,540
And I think that there's a
similar dichotomy here, right?

1202
01:06:49,541 --> 01:06:53,680
I think that there are people who've
really actually find value and I think it

1203
01:06:53,681 --> 01:06:57,400
is a valuable thing to do to be the
person who produces those ideas, right?

1204
01:06:57,401 --> 01:06:59,110
Who builds the proof of concept and yeah,

1205
01:06:59,111 --> 01:07:02,620
you don't get to generate the
coolest possible gang images,

1206
01:07:02,710 --> 01:07:06,130
but you invented the gown. Right?
Right. And so that there's, there's,

1207
01:07:06,131 --> 01:07:09,010
there's a real trade off there and I
think that that's a very personal choice,

1208
01:07:09,011 --> 01:07:11,950
but I think there's value in
both sides. Did you think, uh,

1209
01:07:12,520 --> 01:07:17,470
creating AGI something or some new models?
Uh,

1210
01:07:17,800 --> 01:07:21,970
would, we would see echoes of the
brilliance even at the prototype level.

1211
01:07:22,210 --> 01:07:27,040
So you would be able to
develop those ideas without
scale. The initial sow seeds.

1212
01:07:27,700 --> 01:07:31,710
Take a look at, you know, I always like
to look at examples that exist, right?

1213
01:07:31,760 --> 01:07:35,920
Look at real precedent.
And so take a look at the June,

1214
01:07:35,920 --> 01:07:38,590
2018 models that we released,
we scaled up to turn into gpt to,

1215
01:07:39,160 --> 01:07:42,790
and you can see that at small
scale, it set some records, right?

1216
01:07:42,791 --> 01:07:46,810
This was the original gpt. We actually
had some, some cool generations.

1217
01:07:46,811 --> 01:07:50,950
They weren't nearly as as, as amazing
and really stunning as the gpt to ones,

1218
01:07:51,160 --> 01:07:52,720
but it was promising.
It was interesting.

1219
01:07:53,020 --> 01:07:55,960
And so I think it is the case
that with a lot of these ideas,

1220
01:07:56,110 --> 01:07:59,530
do you see promise at small scale?
But there isn't an asterisk here,

1221
01:07:59,531 --> 01:08:00,610
a very big asterisk,

1222
01:08:00,790 --> 01:08:05,790
which is sometimes we see behaviors that
emerge that are qualitatively different

1223
01:08:07,241 --> 01:08:08,770
from anything we saw at small scale.

1224
01:08:09,040 --> 01:08:13,570
And that the original inventor of
whatever algorithm looks at and says,

1225
01:08:13,630 --> 01:08:17,390
I didn't think it could do that. Hmm.
This is what we saw in Dota, right?

1226
01:08:17,391 --> 01:08:21,250
So Ppo was, was created by John Coleman,
who's a researcher here. And, uh,

1227
01:08:22,070 --> 01:08:26,990
and with, with Dota, we basically just
ran PPO at massive, massive scale. And,

1228
01:08:26,991 --> 01:08:29,090
uh, you know, there's some tweaks in
the north star in order to make it work,

1229
01:08:29,091 --> 01:08:30,830
but fundamentally it's PPO with the core.

1230
01:08:31,490 --> 01:08:35,510
And we were able to get this
longterm planning these,

1231
01:08:35,511 --> 01:08:39,890
these behaviors to really play out on a
timescale that we just thought was not

1232
01:08:39,891 --> 01:08:42,680
possible. Um, and John looked
at that and it was like,

1233
01:08:42,681 --> 01:08:43,850
I didn't think it could do that.

1234
01:08:44,210 --> 01:08:47,150
That's what happens when you're at three
orders of magnitude more scale than

1235
01:08:47,200 --> 01:08:52,110
tested at. Yeah. But it still has the
same flavors of, uh, you know, uh,

1236
01:08:52,950 --> 01:08:55,710
and these echoes of the expected billions,
uh,

1237
01:08:55,970 --> 01:08:59,000
although I suspect with gpt
is scaled more and more,

1238
01:08:59,001 --> 01:09:03,320
you might get surprising things. Uh,
so yeah. Yeah, you're right. It's,

1239
01:09:03,350 --> 01:09:04,670
it's interesting that it's,

1240
01:09:04,730 --> 01:09:08,780
it is difficult to see how far an
idea we'll go when it's scaled.

1241
01:09:09,260 --> 01:09:13,460
It's an open question. Well,
so to that point with, with
Dota and PPO, look, I mean,

1242
01:09:13,520 --> 01:09:15,720
here's, here's a very concrete
one, right? It's like, um,

1243
01:09:15,920 --> 01:09:18,440
it's actually one thing that's very
surprising about Dota that I think people

1244
01:09:18,590 --> 01:09:22,580
don't really pay that much attention to
is the decree of generalization out of

1245
01:09:22,581 --> 01:09:24,560
distribution that happens,
right?

1246
01:09:24,561 --> 01:09:28,730
That you have this AI that's trained
against other bots for its entirety.

1247
01:09:28,850 --> 01:09:31,970
The entirety of its existence is, sorry to
take a step back. Can you, can you tell,

1248
01:09:32,270 --> 01:09:37,190
talk through is, you
know, a story of Dota,

1249
01:09:37,250 --> 01:09:38,340
uh,
story of uh,

1250
01:09:38,360 --> 01:09:43,360
leading up to opening I five and a that
passed and what was the process of self

1251
01:09:43,671 --> 01:09:43,851
plan?

1252
01:09:43,851 --> 01:09:48,800
So a lot of training on what is Dota Dota,

1253
01:09:48,801 --> 01:09:51,290
it's a complex video game.
And we started training,

1254
01:09:51,291 --> 01:09:54,980
we started trying to solve Dota because
we felt like this was a step towards the

1255
01:09:54,981 --> 01:09:59,150
real world relative to other games like
chess or go write those very Sri Board

1256
01:09:59,150 --> 01:10:01,430
Games where you just kind of have
this board of very discreet moves.

1257
01:10:01,730 --> 01:10:05,300
Dota starts to be much more continuous
time that you have this huge variety of

1258
01:10:05,301 --> 01:10:09,350
different actions that you
have a 45 minute game with
all these different units.

1259
01:10:09,351 --> 01:10:09,860
And,
uh,

1260
01:10:09,860 --> 01:10:13,760
it's got a lot of messiness to it that
really hasn't been captured by previous

1261
01:10:13,761 --> 01:10:18,350
games. And famously all of the hard
coated bots for Dota were terrible, right?

1262
01:10:18,351 --> 01:10:21,080
It's just impossible to write anything
good for it because it's so complex.

1263
01:10:21,380 --> 01:10:23,900
And so this seemed like a
really good place to push.

1264
01:10:23,990 --> 01:10:26,480
What's the state of the art
in reinforcement learning?

1265
01:10:26,930 --> 01:10:30,610
And so we started by focusing on the one
versus one version of the game and uh,

1266
01:10:30,611 --> 01:10:32,330
and,
and we're able to solve that.

1267
01:10:32,331 --> 01:10:35,720
We able to beat the world champions
and that the, the, the, the learning,

1268
01:10:35,930 --> 01:10:38,900
the skill curve was this
crazy exponential, right?

1269
01:10:38,901 --> 01:10:42,350
And it was like constantly we were just
scaling up that we were fixing bugs and

1270
01:10:42,560 --> 01:10:45,380
you know, that you look at the, at the
skill curve and it was really very,

1271
01:10:45,381 --> 01:10:46,220
very smooth one.

1272
01:10:46,480 --> 01:10:49,400
And so it's actually really interesting
to see how that like human iteration

1273
01:10:49,401 --> 01:10:53,850
loop yielded very steady
exponential progress. And a two one,

1274
01:10:53,860 --> 01:10:56,870
one side note. First of all, it's
an exceptionally popular video game.

1275
01:10:57,080 --> 01:11:01,670
The side effect is that there's a lot of
incredible human experts at that video.

1276
01:11:01,671 --> 01:11:05,780
Again, so the benchmark they are trying
to reach is very high. And the other,

1277
01:11:05,781 --> 01:11:10,580
can you talk about the approach that was
used initially and throughout training

1278
01:11:10,581 --> 01:11:13,380
these agents to play this game?
And so the approach that we used,

1279
01:11:13,381 --> 01:11:17,010
his self pay and so you have two
agents that don't know anything.

1280
01:11:17,310 --> 01:11:18,150
They battle each other,

1281
01:11:18,600 --> 01:11:22,350
they discover something a little bit
good and now they both know it and they

1282
01:11:22,351 --> 01:11:24,030
just get better and better
and better without bound.

1283
01:11:24,510 --> 01:11:26,670
And that's a really powerful idea,
right?

1284
01:11:27,020 --> 01:11:31,200
That we then went from the one versus
one version of the game and scaled up to

1285
01:11:31,201 --> 01:11:32,400
for five versus five,
right?

1286
01:11:32,400 --> 01:11:35,230
So you think about kind of like with
basketball where you have this like team

1287
01:11:35,350 --> 01:11:35,560
and you,

1288
01:11:35,560 --> 01:11:40,560
I need to do all this coordination
and we're able to push the same idea,

1289
01:11:40,890 --> 01:11:44,330
the same self play, uh, to, to, to,

1290
01:11:44,331 --> 01:11:48,000
to really get to the professional level
at the full five versus five version of

1291
01:11:48,001 --> 01:11:50,780
the game. And, uh, and, and,

1292
01:11:50,781 --> 01:11:54,780
and the things I think are
really interesting here is
that these agents in some

1293
01:11:54,781 --> 01:11:57,000
ways, they're almost like an
insect like intelligence, right?

1294
01:11:57,001 --> 01:12:00,330
Where they have a lot in common with
how an insect is trained, right.

1295
01:12:00,340 --> 01:12:03,270
Insect kind of lives in this environment
for a very long time. Or you know, the,

1296
01:12:03,660 --> 01:12:06,510
the ancestors of this insect have been
around for a long time and had a lot of

1297
01:12:06,511 --> 01:12:10,650
experience. It gets baked into,
into, into this agent. And you know,

1298
01:12:10,651 --> 01:12:12,930
it's not really smart in
the sense of a human right.

1299
01:12:12,930 --> 01:12:14,640
It's not able to go in and learn calculus,

1300
01:12:14,760 --> 01:12:17,820
but able to navigate it's environment
extremely well and is able to handle

1301
01:12:17,850 --> 01:12:21,960
unexpected things in the environment
that's never seen before pretty well. Um,

1302
01:12:22,230 --> 01:12:24,930
and we see the same sort of thing
with our dough to bots, right?

1303
01:12:24,931 --> 01:12:28,440
That they're able to, within this game,
they're able to play against humans,

1304
01:12:28,620 --> 01:12:31,500
which is something that never existed
in its evolutionary environment.

1305
01:12:31,530 --> 01:12:34,410
Totally different play styles
from humans versus bots.

1306
01:12:34,620 --> 01:12:36,810
And yet it's able to
handle it extremely well.

1307
01:12:37,380 --> 01:12:41,340
And that's something that I think was
very surprising to us with something that

1308
01:12:41,910 --> 01:12:46,910
doesn't really emerge from what we've
seen with PPO at smaller scale or in the

1309
01:12:47,461 --> 01:12:49,890
kind of scale we running this
stuff out was uh, you know,

1310
01:12:49,891 --> 01:12:54,170
I could take 100,000 CPU cores running
with like hundreds of Gpu. Yes. Uh,

1311
01:12:54,210 --> 01:12:55,530
it was probably about, uh, you know,

1312
01:12:55,531 --> 01:13:00,531
like something like hundreds of of years
of experience going into this bought

1313
01:13:01,410 --> 01:13:03,120
every single real day.

1314
01:13:04,020 --> 01:13:08,100
And so that scale is massive and we
start to see very different kinds of

1315
01:13:08,101 --> 01:13:10,380
behaviors out of the algorithms
that we all know. And love.

1316
01:13:10,910 --> 01:13:14,650
Dora you mentioned beat
the world expert One v One,

1317
01:13:15,220 --> 01:13:19,660
and then, uh, we, you, uh,
didn't, weren't able to win five,

1318
01:13:19,661 --> 01:13:24,100
the five this year at the, at the
best players in the world. Uh,

1319
01:13:24,101 --> 01:13:26,650
so what's, what's the
comeback story? What's,

1320
01:13:26,680 --> 01:13:30,010
first of all I'll talk through that does
exceptionally exciting event and uh,

1321
01:13:30,011 --> 01:13:33,790
what's, what's the, the following months
and this year look like? Yeah, yeah.

1322
01:13:34,240 --> 01:13:38,720
One thing that's interesting is that, uh,
you know, we lose all the time. All right.

1323
01:13:39,980 --> 01:13:41,680
So the DOTA team at open the eye,

1324
01:13:41,750 --> 01:13:45,670
we w we played against better
players than our system all the time,

1325
01:13:45,880 --> 01:13:48,570
or at least we used to write
like, you know, the, the, the,

1326
01:13:48,571 --> 01:13:52,450
the first time we lost publicly was we
went up on stage at the international and

1327
01:13:52,451 --> 01:13:54,870
we played against some of the
best teams in the world. Um,

1328
01:13:54,910 --> 01:13:58,630
and we ended up losing both games, but we
give them a run for their money, right?

1329
01:13:58,631 --> 01:14:02,710
The both games were kind of 30 minutes,
25 minutes, and they went back and forth,

1330
01:14:02,711 --> 01:14:03,940
back and forth,
back and forth.

1331
01:14:04,300 --> 01:14:08,310
And so I think that really shows that
we're at the professional level. Um,

1332
01:14:08,380 --> 01:14:09,730
and that kind of looking at those games,

1333
01:14:09,731 --> 01:14:12,840
we think that the coin could have gone
a different direction and we could of

1334
01:14:13,030 --> 01:14:16,220
could of had some wins and thought that
was actually very encouraging for us. Um,

1335
01:14:16,360 --> 01:14:19,390
and you know, it's interesting because
the international, was that a fixed time?

1336
01:14:19,470 --> 01:14:20,221
All right.
So we,

1337
01:14:20,221 --> 01:14:23,650
we knew exactly what day we were going
to be playing and we pushed as far as we

1338
01:14:23,651 --> 01:14:26,340
could, as fast as we could.
Yeah. Two weeks later,

1339
01:14:26,370 --> 01:14:30,190
we had a bot that had an 80% win rate
versus the one that played at Tci. Um,

1340
01:14:30,200 --> 01:14:31,830
so the march of progress, uh, you know,

1341
01:14:31,831 --> 01:14:35,030
that you should think of it as a
snapshot rather than as an end state. Um,

1342
01:14:35,031 --> 01:14:39,120
and so in fact, we'll, we'll be announcing
our, uh, our, our finals pretty soon.

1343
01:14:39,121 --> 01:14:42,220
I actually think that, uh, we'll
announce our final match. I,

1344
01:14:42,860 --> 01:14:47,450
prior to this podcast being released. So
there should be, uh, we'll be playing.

1345
01:14:47,451 --> 01:14:50,480
Well, you're playing against the
world champions and you know,

1346
01:14:50,481 --> 01:14:52,920
for us it's really less about like the,

1347
01:14:52,980 --> 01:14:57,710
the way that we think about what's
upcoming is the final milestone,

1348
01:14:57,740 --> 01:15:00,410
the file competitive milestone
for the project, right.

1349
01:15:00,410 --> 01:15:04,730
That our goal in all of this isn't
really about beating humans at Dota.

1350
01:15:05,300 --> 01:15:07,910
Our goal is to push the state of
the art in reinforcement learning.

1351
01:15:07,970 --> 01:15:09,020
And we've done that,
right?

1352
01:15:09,021 --> 01:15:12,500
And we've actually learned a lot from
our system and that we have, I, you know,

1353
01:15:12,501 --> 01:15:15,440
I think a lot of exciting next steps
that we want to take. And so, you know,

1354
01:15:15,520 --> 01:15:18,770
of the final showcase of what we built,
we're going to do this match. Um,

1355
01:15:18,890 --> 01:15:22,940
but for us it's not really the success or
failure, um, to see, you know, do, do, do,

1356
01:15:22,941 --> 01:15:25,880
do we have the coin flip go in our
direction or against [inaudible]

1357
01:15:25,940 --> 01:15:30,940
where do you see the field of deep
learning heading in the next few years?

1358
01:15:31,390 --> 01:15:35,600
Uh, where do you see the work
in reinforcement learning,

1359
01:15:35,601 --> 01:15:37,570
perhaps hiding and uh,

1360
01:15:38,720 --> 01:15:40,670
more specifically with open AI,

1361
01:15:41,180 --> 01:15:45,680
all the exciting projects that
you're working on. What is 2019?

1362
01:15:45,681 --> 01:15:49,640
Hold for you? Massive scale scale. I will
put an asterisk on that and just say,

1363
01:15:49,641 --> 01:15:52,910
you know, I think that it's about
ideas plus scale. You need both.

1364
01:15:52,940 --> 01:15:55,010
So that's a really good point.

1365
01:15:55,040 --> 01:15:57,980
So the question in terms of ideas,

1366
01:15:58,580 --> 01:16:03,580
you have a lot of projects that are
exploring different areas of intelligence.

1367
01:16:04,370 --> 01:16:07,610
And a, the question is, when
you, when you think of scale,

1368
01:16:07,640 --> 01:16:09,800
do you think about growing scale,

1369
01:16:09,801 --> 01:16:13,790
those individual projects or do you
think about adding new projects and uh,

1370
01:16:13,830 --> 01:16:18,710
sorry if you were thinking of adding
new projects or if you look at the past,

1371
01:16:18,980 --> 01:16:22,340
what's the process of coming up
with new projects, new ideas? Yup.

1372
01:16:22,680 --> 01:16:25,350
Uh, so we really have a life
cycle of project here. Uh,

1373
01:16:25,351 --> 01:16:29,270
so we start with a few people just working
on a small scale idea and language is

1374
01:16:29,271 --> 01:16:31,400
actually a very good example of
this. That it was really, you know,

1375
01:16:31,401 --> 01:16:35,170
one person here who was pushing on
language for a long time. I mean,

1376
01:16:35,180 --> 01:16:38,840
then you get signs of life, right? And so
this is like, let's say a, you know, with,

1377
01:16:38,841 --> 01:16:43,610
with the original gpt, we had something
that was interesting and we said, okay,

1378
01:16:43,640 --> 01:16:46,100
it's time to scale this, right?
It's time to put more people on it,

1379
01:16:46,101 --> 01:16:48,890
put more computational
resources behind it. And uh,

1380
01:16:49,040 --> 01:16:51,530
and then we just kind of keep
pushing and keep pushing.

1381
01:16:51,650 --> 01:16:54,740
And the end state is something that looks
like Dota or robotics where you have a

1382
01:16:54,741 --> 01:16:55,650
large team of,
you know,

1383
01:16:55,730 --> 01:16:59,960
10 or 15 people that are running things
at very large scale and that you're able

1384
01:16:59,961 --> 01:17:04,400
to really have material engineering,
uh, and, and, uh, and, and you know,

1385
01:17:04,401 --> 01:17:09,320
sort of machine learning science coming
together to make systems that work and

1386
01:17:09,380 --> 01:17:12,380
get material results that just would
have been impossible otherwise. Um,

1387
01:17:12,381 --> 01:17:15,740
so we do that whole lifecycle. We've
done it a number of times, uh, you know,

1388
01:17:15,800 --> 01:17:20,460
typically end to end. It's
probably two, uh, two years or
so, a to do it. I, you know,

1389
01:17:20,461 --> 01:17:21,860
if the organization's been
around for three years,

1390
01:17:21,861 --> 01:17:24,770
so maybe we'll find that we also
have longer life cycle projects. Um,

1391
01:17:24,880 --> 01:17:29,680
but you know, we, we uh, uh, we'll
work up to those. We have, uh, so,

1392
01:17:29,681 --> 01:17:30,140
so one,

1393
01:17:30,140 --> 01:17:33,080
one team that we were actually just
starting Ilien and I are kicking off a new

1394
01:17:33,081 --> 01:17:36,710
team called the reasoning team and that
this is to really try to tackle how do

1395
01:17:36,711 --> 01:17:39,260
you get neural networks to reason and,
uh,

1396
01:17:39,410 --> 01:17:42,470
we think that this will
be a long term project.

1397
01:17:42,650 --> 01:17:45,950
It's one that we're very excited
about. Uh, in terms of reasoning,

1398
01:17:45,980 --> 01:17:50,540
super exciting topic. What do
you, what kind of benchmarks,

1399
01:17:51,110 --> 01:17:55,820
uh, what kind of tests of reasoning
do you envision? What would,

1400
01:17:55,821 --> 01:18:00,821
if you set back with whatever drink and
you will be impressed that this system

1401
01:18:01,221 --> 01:18:04,580
is able to do something, what would
that look like? Are improving,

1402
01:18:04,920 --> 01:18:05,753
they are improving.

1403
01:18:06,440 --> 01:18:11,370
So some kind of logic and especially in
mathematical logic. I think so. Right.

1404
01:18:11,371 --> 01:18:12,320
And I think that there's,
there's,

1405
01:18:12,500 --> 01:18:15,470
there's kind of other problems that are
dual to theorem improving in particular.

1406
01:18:15,500 --> 01:18:17,750
Um, you know, you think
about, uh, programming,

1407
01:18:17,790 --> 01:18:20,170
I think about even like
security analysis of,

1408
01:18:20,171 --> 01:18:25,171
of code that these all kind of capture
the same sorts of core reasoning and,

1409
01:18:25,501 --> 01:18:28,190
and being able to do some out
of distribution generalization,

1410
01:18:29,370 --> 01:18:33,820
it would be quite exciting if open AI
reasoning team was able to prove that p

1411
01:18:33,821 --> 01:18:35,560
equals NP.
That'd be very nice.

1412
01:18:35,660 --> 01:18:39,770
Uh, it'd be very, very, very
exciting, especially if it
turns out the people's NP.

1413
01:18:39,771 --> 01:18:43,160
That'll be interesting
to maybe, uh, it just,

1414
01:18:43,161 --> 01:18:47,360
it would be a ironic
and humorous. Yup. Uh,

1415
01:18:47,570 --> 01:18:48,920
so what problem

1416
01:18:49,000 --> 01:18:53,440
stands out to you as, uh, the
most, uh, exciting and challenging,

1417
01:18:53,441 --> 01:18:58,441
impactful to the work for
us as a community in general
and for open AI and this

1418
01:18:58,510 --> 01:19:01,330
year you mentioned reasoning. I think
that's, that's a heck of a problem.

1419
01:19:01,400 --> 01:19:02,900
Yeah. So I think reasoning,
it's an important one.

1420
01:19:02,901 --> 01:19:06,050
I think it's going to be hard to get good
results in 2019. Um, you know, again,

1421
01:19:06,051 --> 01:19:08,630
just like we think about the
life cycle, it takes time. Um,

1422
01:19:08,710 --> 01:19:12,620
I think for 2019 language model that
seems to be kind of on that ramp, right?

1423
01:19:12,621 --> 01:19:14,570
It's at the point that we
have a technique that works.

1424
01:19:14,960 --> 01:19:18,500
We want to scale a hundred x thousand
x, see what happens. Awesome.

1425
01:19:19,070 --> 01:19:22,310
Do you think we're living in a
simulation? Uh, I think it's,

1426
01:19:22,311 --> 01:19:25,090
I think it's hard to have a, a
real opinion about it. I, you know,

1427
01:19:25,550 --> 01:19:26,331
it's actually interesting,

1428
01:19:26,331 --> 01:19:30,860
I separate out things that I think can
have like yield materially different

1429
01:19:30,861 --> 01:19:34,420
predictions about the world from
ones that are just kind of fun,

1430
01:19:34,640 --> 01:19:37,850
fun to speculate about. And I kind
of view simulation. It's more like,

1431
01:19:37,940 --> 01:19:42,290
is there a flying teapot between
Mars and Jupiter? Like maybe,

1432
01:19:42,320 --> 01:19:45,050
but it's a little bit hard to know
what that would mean for my life.

1433
01:19:45,080 --> 01:19:45,920
So there is some something

1434
01:19:45,960 --> 01:19:46,793
actionable.

1435
01:19:46,940 --> 01:19:51,940
So some of the best work opening has
done is in the field of reinforcement

1436
01:19:51,981 --> 01:19:56,981
learning and some of the
success of reinforcement
learning come from being able

1437
01:19:57,771 --> 01:20:00,920
to simulate the problem you try to solve.
So it,

1438
01:20:01,020 --> 01:20:05,060
do you have a hope for reinforcement
for the future of reinforcement learning

1439
01:20:05,330 --> 01:20:08,010
and for the future of simulation,
like whether it's, we're talking

1440
01:20:08,010 --> 01:20:10,380
about autonomous vehicles
or any kind of system,

1441
01:20:10,890 --> 01:20:15,250
do you see that scaling to where
we'll be able to simulate systems and,

1442
01:20:15,280 --> 01:20:19,940
and hence be able to create a simulator
that echoes our real world and uh,

1443
01:20:19,980 --> 01:20:22,650
proving once and for all,
even though you're denying it,

1444
01:20:22,651 --> 01:20:23,880
that we're living in a simulation.

1445
01:20:25,520 --> 01:20:27,070
Two separate questions,
right? So, you know, kind of,

1446
01:20:27,071 --> 01:20:30,500
kind of with the core there of like, can
we use simulation for self driving cars?

1447
01:20:30,730 --> 01:20:33,860
Uh, take a look at our
robotic system dactyl right.

1448
01:20:33,861 --> 01:20:37,550
That was trained in simulation
using the DOTA system in fact,

1449
01:20:37,820 --> 01:20:39,740
and it transfers to a physical robot.

1450
01:20:40,460 --> 01:20:42,710
And I think everyone looks at our
dota system, they're like, okay,

1451
01:20:42,711 --> 01:20:45,080
it's just a game. How are you ever
going to escape to the real world?

1452
01:20:45,260 --> 01:20:48,320
And the answer as well. We did it with a
physical robot that no one can program.

1453
01:20:48,710 --> 01:20:51,530
And so I think the answer is simulation
goes a lot further than you think.

1454
01:20:52,070 --> 01:20:55,390
If you apply the right techniques to
it. Now there's a question of, you know,

1455
01:20:55,460 --> 01:20:58,640
are the beings in that simulation can
not gonna wake up and have consciousness.

1456
01:20:58,990 --> 01:21:02,990
Um, I think that one seems a lot, lot
harder to, to again, reason about,

1457
01:21:03,020 --> 01:21:05,610
I think that, you know, you really
should think about like, we're,

1458
01:21:05,690 --> 01:21:09,140
we're exactly just human consciousness
come from in our own self awareness.

1459
01:21:09,141 --> 01:21:09,791
And you know,

1460
01:21:09,791 --> 01:21:12,350
is it just that like once you have
like a complicated enough neural net,

1461
01:21:12,420 --> 01:21:17,020
you have to worry about the agents
feeling pain. Um, and uh, you know,

1462
01:21:17,040 --> 01:21:20,660
I think there's like
interesting speculation to do
there, but uh, but you know,

1463
01:21:20,810 --> 01:21:22,670
again, I think it's, it's a
little bit hard to know for sure.

1464
01:21:23,110 --> 01:21:25,910
Well, let me just keep with the
speculation. Do you think, uh,

1465
01:21:25,930 --> 01:21:28,240
to create intelligence,
general intelligence,

1466
01:21:28,600 --> 01:21:32,770
you need one consciousness and to a body,

1467
01:21:33,160 --> 01:21:35,830
do you think any of those elements
are needed or as intelligence,

1468
01:21:35,831 --> 01:21:38,380
something that's that sort
of fog and all to those?

1469
01:21:38,460 --> 01:21:41,910
I'll stick to the, the kind of like the,
the, the non grand answer first, right?

1470
01:21:41,911 --> 01:21:44,340
So the non grand answer is
just to look at, you know,

1471
01:21:44,341 --> 01:21:46,800
what are we already making work?
You'll get GPD too.

1472
01:21:46,920 --> 01:21:49,320
A lot of people would have said that
to even get these kinds of results,

1473
01:21:49,470 --> 01:21:52,380
you need real world experience.
You need a body unique grounding.

1474
01:21:52,560 --> 01:21:54,960
How are you supposed to reason
about any of these things?

1475
01:21:55,080 --> 01:21:57,540
How are you supposed to like even kind
of know about smoke and fire and those

1476
01:21:57,541 --> 01:21:59,280
things if you've never experienced them.

1477
01:21:59,600 --> 01:22:04,380
And GPD two shows that you can actually
go way further than that kind of

1478
01:22:04,410 --> 01:22:08,510
reasoning would predict.
So I think that the,

1479
01:22:09,270 --> 01:22:11,640
in terms of do we need consciousness,
do we need a body?

1480
01:22:11,820 --> 01:22:13,380
It seems the answer is probably not right.

1481
01:22:13,381 --> 01:22:16,140
That we could probably just continue
to push kind of the systems we have.

1482
01:22:16,140 --> 01:22:18,230
They already feel general,
um,

1483
01:22:18,240 --> 01:22:22,710
they're not as competent or as general
or able to learn as quickly as an AGI

1484
01:22:22,711 --> 01:22:23,670
would,
but you know,

1485
01:22:23,671 --> 01:22:28,590
there are at least like kind of Prodo Agi
in some way and they don't need any of

1486
01:22:28,591 --> 01:22:33,210
those things. Now, now let's move to
the grand answer, which is, you know,

1487
01:22:33,240 --> 01:22:37,380
if our, our neural nets, nets
conscious already, would we ever know,

1488
01:22:37,410 --> 01:22:42,300
how can we tell right here? Here's where
the speculation starts to become, become,

1489
01:22:42,600 --> 01:22:43,050
you know,

1490
01:22:43,050 --> 01:22:47,010
at least interesting or fun and maybe
a little bit disturbing it depending on

1491
01:22:47,011 --> 01:22:51,270
where you take it. But it certainly
seems that when we think about animals,

1492
01:22:51,271 --> 01:22:54,480
that there's some continuum
of, of, of consciousness.
You know, my cat I think is,

1493
01:22:54,481 --> 01:22:58,200
uh, is conscious in some way, right? I,
you know, not as conscious as a human.

1494
01:22:58,201 --> 01:23:01,230
And you could imagine that you could
build a little consciousness meter, right?

1495
01:23:01,230 --> 01:23:03,600
You pointed a cat, it gives you a little
reading, putting out a human. Again,

1496
01:23:03,601 --> 01:23:04,830
it gives you much bigger reading.

1497
01:23:06,370 --> 01:23:09,220
What would happen if he appointed one
of those at a doughnut neural net?

1498
01:23:09,940 --> 01:23:13,390
And if you're training in this massive
simulation, do the neural nets feel pain?

1499
01:23:14,740 --> 01:23:15,340
You know,

1500
01:23:15,340 --> 01:23:20,170
it becomes pretty hard to know that the
answer is no and it becomes pretty hard

1501
01:23:20,171 --> 01:23:24,160
too to really think about what that
would mean if the answer were yes.

1502
01:23:25,420 --> 01:23:27,550
And it's very possible,
you know, for example,

1503
01:23:27,580 --> 01:23:31,720
you could imagine that maybe the reason
that humans are have consciousness is

1504
01:23:31,721 --> 01:23:35,170
because it's a, it's a convenient
computational shortcut, right?

1505
01:23:35,171 --> 01:23:38,260
If you think about it, if you have
a being that wants to avoid pain,

1506
01:23:38,320 --> 01:23:41,090
which seems pretty important to
survive in this environment, um,

1507
01:23:41,170 --> 01:23:43,510
and wants to like, you know, eat food, um,

1508
01:23:43,750 --> 01:23:46,810
then that maybe the best way of doing it
is to have it being of it's conscious,

1509
01:23:46,840 --> 01:23:49,620
right? That, you know, in order
to succeed in the environment,

1510
01:23:49,640 --> 01:23:52,570
you need to have those properties and
how he's supposed to implement them.

1511
01:23:52,750 --> 01:23:56,470
And maybe this, this consciousness
is way of doing that. If that's true,

1512
01:23:56,500 --> 01:23:59,170
then actually maybe we should expect
that really competent reinforcement

1513
01:23:59,171 --> 01:24:02,560
learning agents. We'll also have
consciousness, but you know,

1514
01:24:02,620 --> 01:24:05,230
it's a big enough and I think there are
a lot of other arguments that can make

1515
01:24:05,231 --> 01:24:06,960
another directions.
And

1516
01:24:07,040 --> 01:24:10,580
I think that's a really interesting
idea that even gpt to has some degree of

1517
01:24:10,581 --> 01:24:12,680
consciousness and it's something,
uh,

1518
01:24:13,070 --> 01:24:17,690
is actually not as crazy to think of all
it's useful to think about as we think

1519
01:24:17,691 --> 01:24:22,691
about what it means to create intelligence
of a dog intelligence of a cat and

1520
01:24:22,881 --> 01:24:26,180
the intelligence of human. So
last question. Do you think

1521
01:24:27,850 --> 01:24:29,140
we will

1522
01:24:29,460 --> 01:24:34,460
or fall in love like in the movie her
with an artificial intelligence system or

1523
01:24:34,891 --> 01:24:38,220
an artificial intelligence system
falling in love with a human?

1524
01:24:38,490 --> 01:24:39,323
I hope so.

1525
01:24:40,230 --> 01:24:44,490
If there's any better way to
end it is on a love. So, Greg,

1526
01:24:44,491 --> 01:24:46,410
thanks so much for talking today.
Thank you for having me.

