1
00:00:00,030 --> 00:00:05,030
So today we have Nader Bensky,
he's a professor at northeastern 

2
00:00:05,030 --> 00:00:09,561
university working on various aspects of
computational agents that exhibit human 

3
00:00:10,201 --> 00:00:13,200
level intelligence.
Please give a warm welcome.

4
00:00:18,630 --> 00:00:20,640
Thanks a lot.
Thanks for having me here.

5
00:00:20,800 --> 00:00:24,980
So the title that was on the page was a 
cognitive modeling.

6
00:00:25,730 --> 00:00:28,220
I'll kind of get there,
but I wanted to put it in context.

7
00:00:28,250 --> 00:00:33,250
So the bigger theme here is I want to 
talk about what's called cognitive 

8
00:00:33,250 --> 00:00:34,430
architecture and if you've never heard 
about that before,

9
00:00:34,460 --> 00:00:39,460
that's great.
And I wanted to contextualize that as 

10
00:00:39,460 --> 00:00:42,760
how are we what,
how is that one approach to get us to 

11
00:00:42,760 --> 00:00:46,150
Agi and I say what my view of Agi is and
put up a whole bunch of TV and movie 

12
00:00:49,561 --> 00:00:51,990
characters that I grew up with that 
inspire me.

13
00:00:52,520 --> 00:00:55,320
That'll lead us into what is this thing 
called cognitive architecture.

14
00:00:55,321 --> 00:00:58,740
It's a whole research field that crosses
neuroscience,

15
00:00:58,741 --> 00:01:00,720
psychology,
cognitive science,

16
00:01:00,780 --> 00:01:05,780
and all the way into Ai.
So I'll try to give you kind of the 

17
00:01:05,780 --> 00:01:08,631
historical big picture view of it,
what some of the actual systems are out 

18
00:01:08,631 --> 00:01:09,690
there that might be of interest to you.

19
00:01:09,960 --> 00:01:14,960
And then we'll kind of zoom in on one of
them that I've done a good amount of 

20
00:01:14,960 --> 00:01:17,460
work with called soar.
And what I'll try to do is tell a story,

21
00:01:17,610 --> 00:01:22,610
a research story of how we started with 
kind of a core research question we look

22
00:01:23,071 --> 00:01:28,071
to how humans operate,
understood that phenomenon and then took

23
00:01:29,161 --> 00:01:31,320
it and saw a really interesting results 
from it.

24
00:01:31,380 --> 00:01:34,230
And so at the end,
if this field is of interest,

25
00:01:34,231 --> 00:01:39,231
there's a few pointers for you to go 
read more and go experience more of 

26
00:01:39,231 --> 00:01:43,250
cognitive architecture.
So just rough definition of agi given us

27
00:01:44,971 --> 00:01:49,971
an Agi class,
depending on the direction that you're 

28
00:01:49,971 --> 00:01:53,541
coming from,
it might be kind of understanding 

29
00:01:53,541 --> 00:01:56,150
intelligence or maybe developing a 
intelligent systems that are operating 

30
00:01:56,150 --> 00:01:57,030
at the level of human level 
intelligence,

31
00:01:57,430 --> 00:02:00,930
the,
the typical differences between this and

32
00:02:00,931 --> 00:02:03,750
other sorts of maybe ai machine learning
systems.

33
00:02:03,750 --> 00:02:07,320
We want systems that are going to 
persist for a long period of time.

34
00:02:07,660 --> 00:02:12,660
Uh,
we want them robust to different 

35
00:02:12,660 --> 00:02:12,660
conditions.
We want them learning over time.

36
00:02:12,660 --> 00:02:17,540
And here's the crux of it.
Working on different tasks and a lot of 

37
00:02:17,540 --> 00:02:22,191
cases,
task they didn't know were coming ahead 

38
00:02:22,191 --> 00:02:22,191
of time.
Uh,

39
00:02:22,191 --> 00:02:26,540
I got into this because I clearly 
watched too much TV and too many movies 

40
00:02:26,650 --> 00:02:28,380
and,
and I looked back at this and I realized

41
00:02:28,381 --> 00:02:31,110
I think I'm covering seventies,
eighties,

42
00:02:31,230 --> 00:02:34,080
nineties,
I guess it is.

43
00:02:34,140 --> 00:02:39,140
And today,
and so this is what I wanted out of ai 

44
00:02:39,140 --> 00:02:43,741
and this is what I wanted to work with.
And then there's the reality that we 

45
00:02:43,931 --> 00:02:47,470
have today.
So instead of,

46
00:02:47,530 --> 00:02:50,170
so who's watched a night writer for 
instance?

47
00:02:50,590 --> 00:02:51,140
Uh,
I,

48
00:02:51,141 --> 00:02:54,200
I don't think that exists yet.
Uh,

49
00:02:54,470 --> 00:02:59,470
but,
but maybe we're getting there and in 

50
00:02:59,470 --> 00:03:01,180
particular for fun,
during the Amazon Sale Day,

51
00:03:01,240 --> 00:03:04,960
I got myself an Alexa and I could just 
see myself at some point saying,

52
00:03:05,210 --> 00:03:07,760
Hey Alexa,
please might write me an arcing script,

53
00:03:08,110 --> 00:03:08,840
uh,
you know,

54
00:03:08,841 --> 00:03:10,270
to,
to sync my class.

55
00:03:10,480 --> 00:03:13,330
And if you have an Alexa,
you probably know the following phrase.

56
00:03:13,331 --> 00:03:15,220
This,
this just always hurts me inside.

57
00:03:15,370 --> 00:03:16,690
Which is.
Sorry,

58
00:03:16,990 --> 00:03:20,110
I don't know that one,
which is okay.

59
00:03:20,111 --> 00:03:22,930
Right?
That's a lot of people have no idea what

60
00:03:22,931 --> 00:03:24,610
I'm asking,
let alone how to do that.

61
00:03:24,730 --> 00:03:28,240
So what I want Alexa to respond with 
after that is,

62
00:03:28,660 --> 00:03:30,920
do you have time to teach me,
uh,

63
00:03:31,480 --> 00:03:35,440
and to provide some sort of interface by
which back and forth we can kind of talk

64
00:03:35,441 --> 00:03:36,760
through this,
uh,

65
00:03:36,761 --> 00:03:39,370
that we,
we aren't there yet to say the least,

66
00:03:39,371 --> 00:03:44,371
but I'll talk later about some work on a
system called Rosie that's working in 

67
00:03:45,581 --> 00:03:47,200
that direction.
We're starting to see,

68
00:03:47,230 --> 00:03:50,980
see some ideas about being able to teach
systems how to work.

69
00:03:52,810 --> 00:03:54,780
So folks who are in this field,
uh,

70
00:03:54,790 --> 00:03:58,180
I think generally fall into these three 
categories.

71
00:03:58,690 --> 00:04:01,510
They're just curious.
They want to learn new things,

72
00:04:01,511 --> 00:04:03,550
generate knowledge,
work on hard problems.

73
00:04:04,090 --> 00:04:05,320
Great.
Uh,

74
00:04:05,321 --> 00:04:09,490
I think there are folks who are in kind 
of that mental cognitive modeling realm.

75
00:04:09,900 --> 00:04:14,900
And so I'll use this term a lot.
It's really understanding how humans 

76
00:04:14,900 --> 00:04:17,770
think,
how humans operate human intelligence at

77
00:04:17,771 --> 00:04:20,980
multiple levels.
And if you can do that one,

78
00:04:20,981 --> 00:04:23,830
there's just knowledge in and of itself 
of how we operate.

79
00:04:23,831 --> 00:04:27,370
But there's a lot of really important 
applications that you can think of if we

80
00:04:27,371 --> 00:04:32,371
were able to not only understand but 
predict how humans would respond,

81
00:04:32,861 --> 00:04:37,330
react and various tasks.
Medicine is an easy one.

82
00:04:37,900 --> 00:04:41,730
There's some work in HCI or Hri.
Uh,

83
00:04:41,740 --> 00:04:46,740
I'll get to later where if you can 
predict how humans would respond to it 

84
00:04:46,740 --> 00:04:49,600
as you can iterate tightly and develop 
better interfaces.

85
00:04:49,760 --> 00:04:54,760
Uh,
it's already being used in the realm of 

86
00:04:54,760 --> 00:04:54,970
simulation and defense industries.
Um,

87
00:04:55,120 --> 00:04:58,780
I happen to fall into the latter group 
which are the bottom group,

88
00:04:58,781 --> 00:05:02,440
which is systems development,
which is to say just the desire to build

89
00:05:02,441 --> 00:05:07,441
systems for various tasks that are 
working on tasks that kind of current 

90
00:05:07,441 --> 00:05:09,130
ai,
machine learning can't operate on.

91
00:05:09,760 --> 00:05:11,150
And I think,
uh,

92
00:05:11,180 --> 00:05:16,180
when you're working at this level or on 
any system that nobody's really achieved

93
00:05:16,331 --> 00:05:16,840
before,
what do you,

94
00:05:16,841 --> 00:05:21,841
do you,
you kind of look to the examples that 

95
00:05:21,841 --> 00:05:21,850
you have,
which in this case that we know of,

96
00:05:21,880 --> 00:05:23,300
it's just humans,
right?

97
00:05:25,720 --> 00:05:30,720
Irrespective of your motivation,
when you have kind of an intent that you

98
00:05:31,661 --> 00:05:36,661
want to achieve in your research,
you kind of let that drive your 

99
00:05:36,661 --> 00:05:38,140
approach.
And so I often show my students this,

100
00:05:39,460 --> 00:05:44,460
the turing test,
you might've heard of a or variants of 

101
00:05:44,460 --> 00:05:48,211
it that have come before.
These were folks who were trying to 

102
00:05:48,211 --> 00:05:51,150
create systems that acted in a certain 
way that acted intelligently and the 

103
00:05:51,150 --> 00:05:53,980
kind of line that they drew,
the benchmark that they used was to say,

104
00:05:54,160 --> 00:05:56,830
let's make systems that operate like 
humans do.

105
00:05:58,370 --> 00:06:03,110
Cognitive modelers will fit up into his 
top point here to say it's not enough to

106
00:06:03,170 --> 00:06:06,710
act that way,
but by some definition of thinking,

107
00:06:07,160 --> 00:06:12,160
uh,
we want the system to do what humans do 

108
00:06:12,160 --> 00:06:12,920
or at least be able to make predictions 
about it.

109
00:06:12,921 --> 00:06:16,790
So that might be things like what errors
with the human make on this task,

110
00:06:16,970 --> 00:06:19,400
or how long would it take them to 
perform this task?

111
00:06:19,730 --> 00:06:22,580
Or what emotion would be produced in 
this task?

112
00:06:23,930 --> 00:06:27,710
There are folks who are still thinking 
about how the computer is operating,

113
00:06:28,040 --> 00:06:32,540
but trying to apply a kind of rational 
rules to it.

114
00:06:32,541 --> 00:06:37,541
So a logician for instance,
would say if you have a and you have be 

115
00:06:37,800 --> 00:06:42,800
a gives you,
he gives you ca should definitely give 

116
00:06:42,800 --> 00:06:42,800
you see,
that's just what's rational.

117
00:06:42,800 --> 00:06:47,530
And so there folks operating in that 
direction and then if you go to a intro 

118
00:06:47,530 --> 00:06:50,120
ai class anywhere around the country,
particularly Berkeley,

119
00:06:50,121 --> 00:06:53,180
because they have graphics designers 
that I get to steal from,

120
00:06:54,100 --> 00:06:59,100
the benchmark would be what the system 
produces in terms of action and the 

121
00:07:00,201 --> 00:07:05,201
benchmark is some sort of optimal 
rational bound irrespective of where you

122
00:07:05,901 --> 00:07:07,540
work in this space.
Uh,

123
00:07:07,890 --> 00:07:12,890
there,
there's kind of a common output that 

124
00:07:12,890 --> 00:07:16,420
arrives when you research these areas,
which is you can learn individual bits 

125
00:07:16,420 --> 00:07:21,341
and pieces and it can be hard to bring 
them together to build a system that 

126
00:07:21,951 --> 00:07:24,950
either predicts or act on,
on different tasks.

127
00:07:24,950 --> 00:07:26,990
So this is part of the transfer learning
problem,

128
00:07:26,991 --> 00:07:31,991
but it's also part of a having distinct 
theories that are hard to combine 

129
00:07:31,991 --> 00:07:33,550
together.
So I'm going to give an example and that

130
00:07:33,560 --> 00:07:37,010
come comes out of cognitive modeling or 
perhaps three examples.

131
00:07:38,120 --> 00:07:41,900
So if you were in a HCI class or some 
interest psychology classes,

132
00:07:42,530 --> 00:07:44,840
one of the first things you'll learn 
about his fits law,

133
00:07:45,290 --> 00:07:50,290
which provides you the ability to 
predict the difficulty level of 

134
00:07:51,440 --> 00:07:56,240
basically human pointing from where they
start to a particular place and it turns

135
00:07:56,241 --> 00:08:01,241
out that you can learn some parameters 
and model this based upon just the 

136
00:08:01,400 --> 00:08:05,270
distance from where you are to the 
target and the size of the target.

137
00:08:05,990 --> 00:08:08,320
So both moving along distance will take 
awhile.

138
00:08:08,450 --> 00:08:12,260
But also if you're aiming for a very 
small point that can take longer than if

139
00:08:12,261 --> 00:08:15,140
there's a large area that you just kinda
have to get yourself to.

140
00:08:15,590 --> 00:08:18,770
And so this has held true for many 
humans.

141
00:08:19,040 --> 00:08:24,040
So let's say we've learned this and then
we move onto the next task and we 

142
00:08:24,040 --> 00:08:26,360
learned about what's called the power 
law of practice,

143
00:08:26,570 --> 00:08:30,530
which has been shown true in a number of
different tasks.

144
00:08:30,620 --> 00:08:35,620
What I'm showing here is one of them 
where you're going to draw a line 

145
00:08:35,620 --> 00:08:37,370
through sequential set of circles here,
starting at one,

146
00:08:37,371 --> 00:08:42,371
going to two and so forth.
Not Making a mistake or at least not 

147
00:08:42,371 --> 00:08:44,420
trying to.
And try to do this as fast as possible.

148
00:08:45,440 --> 00:08:49,460
And so for a particular person,
we would fit the AB and c parameters and

149
00:08:49,461 --> 00:08:53,210
we'd see a power law.
So as you perform this task more,

150
00:08:53,211 --> 00:08:58,211
you're going to see a,
a decrease in the amount of reaction 

151
00:08:58,211 --> 00:08:59,550
time required to complete the task.
Great.

152
00:08:59,551 --> 00:09:02,880
We've learned two things about humans.
Let's add some more in.

153
00:09:03,150 --> 00:09:03,960
So,
uh,

154
00:09:03,990 --> 00:09:05,910
for those who might've done some 
reinforcement learning,

155
00:09:05,911 --> 00:09:10,911
the learning is one of those approaches,
a temporal difference learning that's 

156
00:09:10,911 --> 00:09:15,740
had some evidence of similar sorts of 
processes and the dopamine centers of 

157
00:09:16,141 --> 00:09:21,141
the brain.
And it basically says in a sequential 

158
00:09:21,141 --> 00:09:21,600
learning task,
you perform the task,

159
00:09:21,601 --> 00:09:24,990
you get some sort of reward.
How are you going to kind of update your

160
00:09:24,991 --> 00:09:26,750
representation of what to do in the 
future,

161
00:09:26,760 --> 00:09:29,610
such as to maximize expectation of 
future reward.

162
00:09:29,850 --> 00:09:33,030
And there are various models of how that
changes over time.

163
00:09:33,150 --> 00:09:38,150
And you can build up functions that 
allow you to form better and better and 

164
00:09:38,150 --> 00:09:38,970
better given trial and error.
Great.

165
00:09:39,270 --> 00:09:44,270
So we've learned three interesting 
models here that hold true over multiple

166
00:09:45,870 --> 00:09:47,280
people,
multiple tasks.

167
00:09:47,490 --> 00:09:52,490
And so my question is,
if we take these together and add them 

168
00:09:52,490 --> 00:09:52,560
together,
uh,

169
00:09:52,890 --> 00:09:57,890
how do we start to understand a task as 
quote unquote simple as chess,

170
00:09:58,290 --> 00:10:01,050
which is to say we could ask questions,
how,

171
00:10:01,180 --> 00:10:04,600
how long would it take for a person to 
play a,

172
00:10:04,680 --> 00:10:08,250
what mistakes would they make after they
played a few games?

173
00:10:08,340 --> 00:10:13,340
How will they adapt themselves or if we 
want to develop system that ended up 

174
00:10:13,340 --> 00:10:16,470
being good at chess or at least learning
to become better at chess.

175
00:10:16,920 --> 00:10:18,060
My question is,
if you could,

176
00:10:19,120 --> 00:10:22,770
there doesn't seem to be a clear way to 
take these very,

177
00:10:22,771 --> 00:10:26,880
very individual theories and kind of 
smash them together and get a reasonable

178
00:10:26,881 --> 00:10:30,690
answer of how to play chess or how do 
humans play chess?

179
00:10:32,570 --> 00:10:37,230
And so a gentleman in this slide is 
Allan Newell,

180
00:10:37,231 --> 00:10:42,231
one of the founders of ai did incredible
work in psychology and other fields.

181
00:10:43,500 --> 00:10:48,500
Uh,
he gave a series of lectures at Harvard 

182
00:10:48,500 --> 00:10:51,021
in 1987 and they were published in 1990 
called the unified theories of 

183
00:10:51,021 --> 00:10:55,431
cognition.
And his argument to the psychology 

184
00:10:55,431 --> 00:10:56,730
community at that point was the argument
on the prior slide.

185
00:10:56,910 --> 00:11:00,570
They had many individual studies,
many individual results,

186
00:11:00,900 --> 00:11:05,900
and so the question was how do you bring
them together to gain this overall 

187
00:11:05,900 --> 00:11:06,660
theory,
how do you make forward progress?

188
00:11:07,050 --> 00:11:10,050
And so his proposal was unified theories
of cognition.

189
00:11:10,380 --> 00:11:13,560
She became known as cognitive 
architecture,

190
00:11:13,920 --> 00:11:18,920
which is to say,
to bring together your core assumptions,

191
00:11:19,381 --> 00:11:24,381
your core beliefs of what are the fixed 
mechanisms and processes that 

192
00:11:24,990 --> 00:11:27,540
intelligent agents would use across 
tasks.

193
00:11:27,660 --> 00:11:30,450
So the representations,
the learning mechanisms,

194
00:11:30,710 --> 00:11:33,120
um,
the memory systems,

195
00:11:33,690 --> 00:11:38,690
bring them together,
implement them in a theory and use that 

196
00:11:38,690 --> 00:11:42,201
across tasks.
And the core idea is that when you 

197
00:11:42,201 --> 00:11:45,651
actually have to implement this and see 
how it's going to work across different 

198
00:11:45,651 --> 00:11:48,861
tasks,
the interconnections between these 

199
00:11:48,861 --> 00:11:48,861
different,
uh,

200
00:11:48,861 --> 00:11:52,610
processes and representations would add 
constraints and overtime the constraints

201
00:11:53,261 --> 00:11:58,261
would start limiting the design space of
what is necessary and what is possible 

202
00:11:58,261 --> 00:11:59,320
in terms of building intelligent 
systems.

203
00:11:59,620 --> 00:12:04,620
And so the overall goal from there was 
to understand and exhibit a human level 

204
00:12:04,620 --> 00:12:06,090
intelligence using these cognitive 
architectures.

205
00:12:09,920 --> 00:12:12,130
A natural question to ask is,
okay,

206
00:12:12,131 --> 00:12:17,131
so we've gone from a methodology of 
science that we understand how to 

207
00:12:17,321 --> 00:12:20,380
operate in a.
We make a hypothesis,

208
00:12:20,410 --> 00:12:24,040
we construct a study,
we gather our data,

209
00:12:24,250 --> 00:12:29,250
we evaluate that data and we false fire.
We do not falsify the original 

210
00:12:29,250 --> 00:12:31,870
hypothesis and we can do that over and 
over again and we know that we're making

211
00:12:31,900 --> 00:12:35,760
forward progress scientifically.
If I've now taken that model and changed

212
00:12:35,761 --> 00:12:40,761
it into I have a piece of software and 
it's representing my theories and to 

213
00:12:42,341 --> 00:12:47,341
some extent I can configure that 
software in different ways to work on 

214
00:12:47,341 --> 00:12:48,190
different tasks.
How do I know that I'm making progress?

215
00:12:48,640 --> 00:12:53,640
And so there's a form of science called 
lactose iom and it's kind of shown 

216
00:12:55,331 --> 00:13:00,331
pictorially here where you start with 
your core of what your beliefs are 

217
00:13:00,880 --> 00:13:02,110
about,
uh,

218
00:13:02,830 --> 00:13:06,310
where you're headed,
what is necessary for achieving the goal

219
00:13:06,311 --> 00:13:06,940
that you have.

220
00:13:07,060 --> 00:13:12,060
And around that you'll have kind of 
ephemeral hypotheses and assumptions 

221
00:13:12,060 --> 00:13:16,261
that over time may grow and shrink.
And so you're trying out different 

222
00:13:16,261 --> 00:13:16,261
things,
trying out different things.

223
00:13:16,420 --> 00:13:18,550
And if an assumption is around there 
long enough,

224
00:13:19,060 --> 00:13:24,060
it becomes part of that core.
And so as you work on more task and 

225
00:13:24,060 --> 00:13:26,620
learn more,
either by your work or by data coming in

226
00:13:26,621 --> 00:13:29,800
from when someone else,
the core is growing larger and larger,

227
00:13:30,430 --> 00:13:33,010
you've got more constraints and you've 
made more progress.

228
00:13:33,970 --> 00:13:37,600
And so what I wanted to look at,
where in this community,

229
00:13:37,830 --> 00:13:42,830
uh,
what are some of the core assumptions 

230
00:13:42,830 --> 00:13:42,830
that are driving forward scientific 
progress.

231
00:13:43,390 --> 00:13:46,450
So one of them actually came out of 
those lectures,

232
00:13:46,451 --> 00:13:49,150
they're referred to as noodles,
timescales of human action,

233
00:13:49,420 --> 00:13:54,420
and so off on the left,
the left two columns are both time units

234
00:13:54,941 --> 00:13:59,941
just expressed somewhat differently.
A second from the left being maybe more 

235
00:13:59,941 --> 00:14:02,530
useful to a lot of us in understanding 
daily life.

236
00:14:03,070 --> 00:14:08,070
Uh,
one step over from there would be kind 

237
00:14:08,070 --> 00:14:08,070
of at what level processes are 
occurring.

238
00:14:08,070 --> 00:14:10,960
So the lowest three are down at of the 
substrate,

239
00:14:11,020 --> 00:14:16,020
the neuronal level.
We're building up to deliberate tasks 

240
00:14:16,020 --> 00:14:19,381
that occur in the brain and tasks that 
are operating on the order of 10 

241
00:14:19,381 --> 00:14:23,371
seconds.
Some of these might occur in the 

242
00:14:23,371 --> 00:14:25,680
psychology laboratory,
but probably a step up to minutes and 

243
00:14:25,680 --> 00:14:30,601
hours and then above that really becomes
the interactions between agents over 

244
00:14:30,601 --> 00:14:34,361
time and so if we start with that,
the things to take away is that a 

245
00:14:34,811 --> 00:14:39,811
regular,
the hypothesis is that regularity is 

246
00:14:39,811 --> 00:14:42,790
will occur at these different timescales
and that they're useful and so those who

247
00:14:42,791 --> 00:14:47,230
operate at that lowest timescale might 
be considering neuroscience,

248
00:14:47,231 --> 00:14:48,460
cognitive,
neuroscience.

249
00:14:48,850 --> 00:14:50,980
When you shift up to the next couple of 
levels,

250
00:14:50,990 --> 00:14:55,990
what we would think about in terms of 
the areas of science that deal without 

251
00:14:55,990 --> 00:14:59,410
would be psychology and cognitive 
science and that will shift up a level 

252
00:14:59,410 --> 00:15:02,651
and we're talking about sociology and 
economics and the interplay between a 

253
00:15:03,200 --> 00:15:08,200
agents over time and so what we'll find 
with cognitive architecture is that most

254
00:15:09,351 --> 00:15:11,540
of them will tend to sit at the 
deliberate act.

255
00:15:11,690 --> 00:15:16,690
We're trying to take knowledge of a 
situation and make a single decision and

256
00:15:16,791 --> 00:15:20,900
then sequences of decisions over time.
We'll build to tasks and tasks.

257
00:15:20,901 --> 00:15:23,480
Over time we'll build to more 
interesting phenomenon.

258
00:15:23,960 --> 00:15:26,270
I'm actually going to show that that 
isn't strictly true,

259
00:15:26,300 --> 00:15:29,900
that there are folks working in this 
field that actually do operate one level

260
00:15:29,901 --> 00:15:33,560
below a,
some other assumptions.

261
00:15:33,590 --> 00:15:38,120
So this is a herb Simon receiving the 
Nobel prize in economics,

262
00:15:39,130 --> 00:15:43,760
and part of what he received that award 
for was an idea of bounded rationality.

263
00:15:44,500 --> 00:15:49,040
So in various fields we tend to model 
humans as rational.

264
00:15:49,150 --> 00:15:51,660
Uh,
and his argument was a,

265
00:15:51,680 --> 00:15:56,680
let's consider that human beings are 
operating under various kinds of 

266
00:15:57,020 --> 00:16:02,020
constraints.
And so to model the rationality with 

267
00:16:02,020 --> 00:16:05,030
respect to unbounded by how complex the 
problem is that they're working on,

268
00:16:05,060 --> 00:16:10,040
how big is that search space that they 
have to conquer cognitive limitations.

269
00:16:10,250 --> 00:16:14,570
So a speed of operations,
amount of memory,

270
00:16:15,110 --> 00:16:20,110
short term as well as longterm,
as well as other aspects of our 

271
00:16:20,110 --> 00:16:23,801
computing infrastructure that are going 
to keep us from being able to 

272
00:16:23,801 --> 00:16:26,950
arbitrarily solve complex problems as 
well as how much time is available to 

273
00:16:27,111 --> 00:16:29,810
make that decision.
And so,

274
00:16:29,811 --> 00:16:34,811
uh,
this is actually a phrase that came out 

275
00:16:34,811 --> 00:16:34,811
of his speech when he received the Nobel
prize.

276
00:16:34,811 --> 00:16:38,990
Decision makers can satisfy ice either 
by finding optimum solutions for a 

277
00:16:38,990 --> 00:16:40,730
simplified world,
which is to say,

278
00:16:41,000 --> 00:16:43,400
take your big problem,
simplify in some way,

279
00:16:43,430 --> 00:16:48,430
and then solve that or by finding 
satisfactory solutions for a more 

280
00:16:48,430 --> 00:16:50,690
realistic world,
take the world and all its complexity,

281
00:16:50,691 --> 00:16:54,320
take the problem in all its complexity 
and try to find something that works.

282
00:16:55,160 --> 00:17:00,160
Neither approach in general dominates 
the other and both have continued to 

283
00:17:00,160 --> 00:17:00,620
coexist.
And so what you're actually going to see

284
00:17:00,820 --> 00:17:05,820
a throughout the cognitive architecture 
community is this understanding that 

285
00:17:05,840 --> 00:17:09,680
some problems you're not gonna be able 
to get an optimal solution to,

286
00:17:09,681 --> 00:17:11,360
if you consider,
for instance,

287
00:17:11,620 --> 00:17:14,420
a bounded amount of computation,
bounded time,

288
00:17:14,600 --> 00:17:17,270
the need to be reactive to a changing 
environment,

289
00:17:17,420 --> 00:17:18,530
these sorts of issues.

290
00:17:18,860 --> 00:17:23,860
And so in some sense,
we can decompose problems that come up 

291
00:17:23,860 --> 00:17:24,590
over and over again into simpler 
problems,

292
00:17:25,010 --> 00:17:26,750
solve those,
uh,

293
00:17:27,000 --> 00:17:30,500
near optimally or optimally fix those 
in,

294
00:17:30,501 --> 00:17:32,870
optimize those,
but more general problems.

295
00:17:32,871 --> 00:17:37,871
We might have to satisfy some.
There's also the idea of the symbol 

296
00:17:37,871 --> 00:17:42,500
system hypothesis.
So this is a Allen Newell and Herb Simon

297
00:17:42,501 --> 00:17:43,140
there,
uh,

298
00:17:43,610 --> 00:17:46,310
considering how a computer could play 
the game of chess.

299
00:17:46,520 --> 00:17:51,520
So the fiscal system,
physical symbol system talks about the 

300
00:17:51,520 --> 00:17:55,101
idea of taking something,
some signal abstractly referred to as 

301
00:17:55,101 --> 00:17:59,150
symbol,
combining them in some ways to form 

302
00:17:59,150 --> 00:18:01,530
expressions and then having operations 
that produced new expressions.

303
00:18:01,950 --> 00:18:06,950
I'm a weak interpretation of the idea 
that symbol systems are necessary and 

304
00:18:08,191 --> 00:18:13,191
sufficient for intelligent systems.
A very weak way of talking about it is 

305
00:18:13,191 --> 00:18:16,850
the claim that there's nothing unique 
about the neuronal infrastructure that 

306
00:18:16,850 --> 00:18:20,190
we have,
but if we got to the software right,

307
00:18:20,310 --> 00:18:25,310
we could implement it in the bits bytes,
ram and processor that makeup modern 

308
00:18:25,310 --> 00:18:25,560
computers.

309
00:18:25,560 --> 00:18:27,510
That's kind of the weakest way to look 
at this.

310
00:18:27,780 --> 00:18:31,830
That we can do it with silicon and not 
carbon.

311
00:18:32,590 --> 00:18:37,590
I'm stronger way that this used to be 
looked at was more of a logical 

312
00:18:38,041 --> 00:18:41,700
standpoint,
which is to say if we can encode a rules

313
00:18:41,701 --> 00:18:46,701
of logic,
these tend to line up if we think 

314
00:18:46,701 --> 00:18:47,790
intuitively have a planning and problem 
solving.

315
00:18:47,910 --> 00:18:52,910
And if we can just get that right and 
get enough fats in there and enough 

316
00:18:52,910 --> 00:18:53,670
rules in there that somehow 
intelligence,

317
00:18:54,000 --> 00:18:54,600
uh,
well,

318
00:18:54,660 --> 00:18:59,660
that's what we need for intelligence and
eventually we can get to the point of 

319
00:18:59,660 --> 00:19:00,270
intelligence and that's what you need 
for intelligence.

320
00:19:01,170 --> 00:19:04,720
And that was a,
a starting point that lasted for awhile.

321
00:19:04,740 --> 00:19:09,740
I think by now,
most folks in this field would agree 

322
00:19:09,740 --> 00:19:12,930
that that's necessary to be able to 
operate logically,

323
00:19:13,140 --> 00:19:18,140
but that there are going to be 
representations and processes that'll 

324
00:19:18,140 --> 00:19:19,110
benefit from non symbolic 
representations.

325
00:19:19,111 --> 00:19:24,111
So particularly perceptual processing,
visual auditory and processing things in

326
00:19:24,631 --> 00:19:27,330
a more kind of standard machine learning
sort of way.

327
00:19:27,700 --> 00:19:29,400
Uh,
uh,

328
00:19:29,670 --> 00:19:34,670
as well as kind of statistic taking 
advantage of statistical rep a 

329
00:19:34,670 --> 00:19:34,670
representations.

330
00:19:36,570 --> 00:19:40,830
So we're getting closer to actually 
looking at cognitive architectures.

331
00:19:41,100 --> 00:19:46,100
Uh,
I did want to go back to the idea that 

332
00:19:46,100 --> 00:19:47,990
different researchers are coming with 
different research Fokai Fossa.

333
00:19:48,770 --> 00:19:53,770
Uh,
and we'll start off with kind of the 

334
00:19:53,770 --> 00:19:53,940
lowest level and understanding 
biological modeling.

335
00:19:53,941 --> 00:19:58,941
So libra and spawn both try to model 
different degrees of low level details,

336
00:20:00,420 --> 00:20:02,670
parameters,
firing rates,

337
00:20:03,090 --> 00:20:08,090
connectivities between different kind of
levels of neuronal representations.

338
00:20:09,540 --> 00:20:13,230
They build that up and then they tried 
to build tasks above that layer,

339
00:20:13,231 --> 00:20:18,231
but always being very cautious about 
being true to a human biological 

340
00:20:21,390 --> 00:20:23,910
processes.
At a layer above,

341
00:20:23,911 --> 00:20:28,911
there would be psychological modeling,
which is to say trying to build systems 

342
00:20:28,981 --> 00:20:31,650
that are true in some sense to,
uh,

343
00:20:31,680 --> 00:20:36,150
areas of the brain interactions in the 
brain and being able to predict a errors

344
00:20:36,151 --> 00:20:41,151
that we made,
a timing that we produced by the human 

345
00:20:41,151 --> 00:20:44,631
mind.
And so they're all talk a little bit 

346
00:20:44,631 --> 00:20:46,701
about act are his final level down here.
These are systems that are focused 

347
00:20:47,650 --> 00:20:52,650
mainly on producing functional systems 
that exhibit really cool artifacts and 

348
00:20:55,721 --> 00:21:00,721
solve really cool problems.
And so I'll spend most of the time 

349
00:21:00,721 --> 00:21:03,481
talking about soar,
but I wanted to point out a relative 

350
00:21:03,481 --> 00:21:06,780
newcomer in the game called sigma.
So to talk about spawn a little bit.

351
00:21:07,460 --> 00:21:12,460
We'll see if the sound works in here.
I'm going to let the creator take this 

352
00:21:13,641 --> 00:21:13,970
one

353
00:21:16,790 --> 00:21:21,790
or not see how the AB system likes this.

354
00:21:27,170 --> 00:21:27,630
There we go.

355
00:21:31,690 --> 00:21:36,690
Why do you pick the license and the 
director of the Center for Science at 

356
00:21:36,690 --> 00:21:40,130
the University of philosophy and the 
philosophy is considered a general 

357
00:21:42,431 --> 00:21:47,431
conceptual issues is any breakdown 
equations very concise.

358
00:21:55,250 --> 00:22:00,250
We can like building actual models 
recently it's called the small sign 

359
00:22:01,790 --> 00:22:06,790
because she wouldn't have million 
individual neurons and the model isn't 

360
00:22:07,000 --> 00:22:12,000
it and the movement,
so essentially see images and numbers 

361
00:22:13,060 --> 00:22:17,060
and in the case of just gotten into that
seat,

362
00:22:18,050 --> 00:22:21,610
reproduce the south and it's looking at.
So for instance,

363
00:22:28,240 --> 00:22:33,240
we all know that we have that show up 
and we can simulate a potential area.

364
00:22:39,760 --> 00:22:43,150
Hold on.
I'm working on agents that are extremely

365
00:22:43,151 --> 00:22:48,151
good at one task for instance.
What's special is that pass and this 

366
00:22:49,650 --> 00:22:53,640
adds the additional kind of coordinate 
the flow of information,

367
00:22:53,680 --> 00:22:54,960
food,
different parts of the model,

368
00:22:55,210 --> 00:22:55,620
something

369
00:23:02,240 --> 00:23:07,240
so provide a pointer at the end.
He's got a really cool book called how 

370
00:23:07,240 --> 00:23:08,990
to build a brain and if you google you 
can google spawn.

371
00:23:09,230 --> 00:23:14,230
You can find a toolkit where you can 
kind of construct circuits that will 

372
00:23:14,960 --> 00:23:16,840
approximate functions that you're 
interested in,

373
00:23:16,850 --> 00:23:21,850
connect them together,
a set certain properties that you would 

374
00:23:21,850 --> 00:23:26,080
want at a low level and build them up 
and actually work on tasks at the level 

375
00:23:26,451 --> 00:23:31,451
of vision and robotic actuation.
So that's a really cool system as we 

376
00:23:33,681 --> 00:23:37,730
move into architectures that are sitting
above that biological level.

377
00:23:37,940 --> 00:23:42,940
I wanted to give you kind of an overall 
sense of what they're going to look 

378
00:23:42,940 --> 00:23:45,851
like,
what a prototypical architecture is 

379
00:23:45,851 --> 00:23:45,980
going to look like.
So they're going to have some ability to

380
00:23:45,981 --> 00:23:47,300
have perception.
Uh,

381
00:23:47,390 --> 00:23:52,390
the modalities typically are more 
digital symbolic,

382
00:23:52,490 --> 00:23:53,780
but,
uh,

383
00:23:53,840 --> 00:23:55,960
they will,
depending on the architecture,

384
00:23:55,980 --> 00:24:00,980
be able to handle vision,
audition a and various sensory inputs.

385
00:24:02,390 --> 00:24:05,420
These little gadgets represented in some
sort of short term memory.

386
00:24:05,421 --> 00:24:10,200
Whatever the state's representation for 
the particular system is there.

387
00:24:10,620 --> 00:24:15,620
It's typical to have a representation of
the knowledge of what tasks can be 

388
00:24:15,620 --> 00:24:18,620
performed when they should be performed,
how they should be controlled.

389
00:24:18,890 --> 00:24:23,890
And so these are typically both actions 
that take place internally that manage 

390
00:24:24,230 --> 00:24:28,490
the internal state of the system and 
perform internal computations,

391
00:24:28,820 --> 00:24:33,820
but also about external actuation and 
external might be a digital system,

392
00:24:34,041 --> 00:24:39,041
a game ai,
but it might also be some sort of 

393
00:24:39,041 --> 00:24:39,420
robotic actuation and real world.
Uh,

394
00:24:39,860 --> 00:24:44,860
there's typically some sort of mechanism
by which to select from the available 

395
00:24:44,860 --> 00:24:49,100
actions in a particular situation.
There's typically some way to augment 

396
00:24:49,730 --> 00:24:54,730
this procedural information,
which is to say learn about new actions 

397
00:24:54,730 --> 00:24:59,171
possibly modify existing ones.
There's typically some semblance of 

398
00:24:59,171 --> 00:25:01,580
what's called a declarative memory.
So whereas procedural,

399
00:25:01,581 --> 00:25:03,060
at least in humans,
uh,

400
00:25:03,110 --> 00:25:08,110
if I asked you to describe how to ride a
bike a,

401
00:25:08,450 --> 00:25:11,510
you might be able to say,
get on the seats and pedal.

402
00:25:11,720 --> 00:25:13,640
But in terms of keeping your balance 
there,

403
00:25:13,641 --> 00:25:17,420
you'd have a pretty hard time describing
it declaratively.

404
00:25:18,110 --> 00:25:23,110
So that's kind of the procedural side,
the implicit representation of 

405
00:25:23,110 --> 00:25:25,640
knowledge.
Whereas declarative would include facts,

406
00:25:25,700 --> 00:25:27,080
geography,
math,

407
00:25:27,240 --> 00:25:29,780
uh,
but it calls to include experiences that

408
00:25:29,781 --> 00:25:32,930
the hns had a more episodic 
representation of declarative memory.

409
00:25:33,290 --> 00:25:36,500
And they'll typically have some way of,
of learning this information.

410
00:25:36,501 --> 00:25:41,501
Augmenting it over time and then finally
some way of taking actions in the world 

411
00:25:42,260 --> 00:25:47,260
and they'll all have some sort of cycle 
which is perception comes in knowledge 

412
00:25:47,871 --> 00:25:50,390
that the agent has is brought to bear on
that.

413
00:25:51,130 --> 00:25:56,130
An action is selected.
Knowledge that knows to condition on 

414
00:25:56,130 --> 00:25:59,501
that action will act accordingly,
both with internal processes as well as 

415
00:25:59,501 --> 00:26:00,980
eventually to take action and then rinse
and repeat.

416
00:26:02,170 --> 00:26:07,170
Um,
so when we talk about an AI system and 

417
00:26:07,170 --> 00:26:09,080
agent in this context,
that would be the fixed representation,

418
00:26:09,081 --> 00:26:13,820
which is whatever architecture we're 
talking about plus set of knowledge that

419
00:26:13,821 --> 00:26:17,420
is typically specific to the task button
might be more general.

420
00:26:17,450 --> 00:26:22,450
So oftentimes these systems could 
incorporate a more general knowledge 

421
00:26:22,450 --> 00:26:24,710
base of facts,
of linguistic facts,

422
00:26:24,711 --> 00:26:29,711
of a geographic facts.
Let's take wikipedia and let's just 

423
00:26:29,711 --> 00:26:32,120
stick it in the brain of the system.
There'll be more task general,

424
00:26:32,420 --> 00:26:34,580
but then also whatever it is that you're
doing right now,

425
00:26:35,180 --> 00:26:40,180
how should you proceed in that?
And then it's typical to see this 

426
00:26:40,180 --> 00:26:43,620
processing cycle.
And going back to the prior assumption,

427
00:26:43,621 --> 00:26:45,380
the idea is that,
uh,

428
00:26:45,420 --> 00:26:50,370
these primitive cycles allow for the 
agent to be reactive to its environment.

429
00:26:50,550 --> 00:26:54,120
So if new things come into his react to,
if the lions sitting over there,

430
00:26:54,121 --> 00:26:56,490
I better run and maybe not do my 
calculus homework,

431
00:26:56,491 --> 00:26:59,970
right?
So as long as this cycle is going,

432
00:26:59,971 --> 00:27:02,040
I'm reactive.
But at the same time,

433
00:27:02,160 --> 00:27:05,970
if multiple actions are taken over time,
I'm able to get complex behavior,

434
00:27:06,200 --> 00:27:07,260
uh,
over the longterm.

435
00:27:08,610 --> 00:27:12,090
So this is the act are cognitive 
architecture.

436
00:27:12,330 --> 00:27:17,330
Uh,
it has many of the kind of core pieces 

437
00:27:17,330 --> 00:27:19,560
that I talked about before.
Let's see if the,

438
00:27:20,120 --> 00:27:22,020
some mouse,
yes,

439
00:27:22,070 --> 00:27:26,190
as useful up there.
So we have the procedural model here.

440
00:27:26,430 --> 00:27:30,300
A short term memory is going to be these
buffers that are on the outside,

441
00:27:30,620 --> 00:27:33,500
uh,
the procedural memory is encoded as what

442
00:27:33,510 --> 00:27:36,750
it called production rules,
or if then rules,

443
00:27:37,250 --> 00:27:40,110
if this is the state of my short term 
memory,

444
00:27:40,111 --> 00:27:42,240
this is what I think should happen as a 
result,

445
00:27:42,450 --> 00:27:47,450
uh,
you have a selection of the appropriate 

446
00:27:47,550 --> 00:27:52,550
rule to fire and an execution.
You're seeing a associated parts of the 

447
00:27:52,891 --> 00:27:57,891
brain being represented here,
cool thing that has been done over time 

448
00:27:57,891 --> 00:27:58,740
in the act.
Our community is to,

449
00:27:59,000 --> 00:28:04,000
uh,
make predictions about brain areas and 

450
00:28:04,000 --> 00:28:07,401
then perform a Mris and,
and gather that data and correlate that 

451
00:28:07,401 --> 00:28:08,130
data.
So when you use the system,

452
00:28:08,131 --> 00:28:12,570
you will get predictions about things 
like timing of operations,

453
00:28:12,630 --> 00:28:15,900
errors that will occur,
probabilities that something is learned,

454
00:28:16,170 --> 00:28:21,170
but you'll also get predictions about a 
to the degree that they can kind of 

455
00:28:21,170 --> 00:28:24,921
brain areas that are going to light a 
light up a and if you want to,

456
00:28:24,960 --> 00:28:28,500
that's actively being developed at 
Carnegie Mellon.

457
00:28:29,760 --> 00:28:34,760
To the left is John Anderson who 
developed this cognitive architecture.

458
00:28:35,510 --> 00:28:38,040
Oh,
30 ish years ago.

459
00:28:38,380 --> 00:28:39,840
Uh,
and until the last,

460
00:28:39,841 --> 00:28:44,841
about five years,
he was the primary researcher developer 

461
00:28:44,841 --> 00:28:48,051
behind it with Christian.
And then recently he's decided to spend 

462
00:28:48,051 --> 00:28:49,950
more time on a cognitive tutoring 
systems.

463
00:28:50,160 --> 00:28:53,250
And so Christian has become the primary 
developer.

464
00:28:53,460 --> 00:28:56,760
There is a annual act.
Our workshop,

465
00:28:57,210 --> 00:29:02,210
there's a summer school which if you're 
thinking about modeling a particular 

466
00:29:02,611 --> 00:29:04,770
task,
you can kind of bring your task to them,

467
00:29:04,771 --> 00:29:07,350
bring your data.
They teach you how to use the system and

468
00:29:07,530 --> 00:29:09,780
try to get that study going right there 
on the spot,

469
00:29:10,690 --> 00:29:15,690
uh,
to give you a sense of what kinds of 

470
00:29:15,690 --> 00:29:15,690
tasks this could be applied to.
So,

471
00:29:15,840 --> 00:29:20,840
uh,
this is a representative of a certain 

472
00:29:20,840 --> 00:29:21,630
class of task.
Certainly not the only one.

473
00:29:23,520 --> 00:29:28,520
Let's try this again.
Think powerpoint is going to want a 

474
00:29:28,520 --> 00:29:28,620
restart every time.

475
00:29:28,680 --> 00:29:30,270
Okay.
So,

476
00:29:30,300 --> 00:29:35,300
uh,
we're getting predictions about 

477
00:29:35,300 --> 00:29:35,300
basically where the eye is going to 
move.

478
00:29:35,300 --> 00:29:39,280
What you're not seeing is it's actually 
processing things like text and colors 

479
00:29:39,280 --> 00:29:42,100
and making predictions about what to do 
and how to represent information and how

480
00:29:42,101 --> 00:29:45,460
to process the graph as a whole.
Uh,

481
00:29:45,670 --> 00:29:48,970
I had alluded to this earlier.
There's work by Bonnie John,

482
00:29:49,330 --> 00:29:54,330
a very similar,
so making predictions about how humans 

483
00:29:54,330 --> 00:29:57,631
would use computer interfaces.
And at the time she got hired away by 

484
00:29:57,631 --> 00:30:01,951
IBM and so they wanted the ability to 
have software that you can put in front 

485
00:30:01,951 --> 00:30:05,230
of a software designers and when they 
think they have a good interface,

486
00:30:05,231 --> 00:30:09,180
press a button.
This model of human cognition with tried

487
00:30:09,181 --> 00:30:14,181
to perform the tasks that had been told 
to do and make predictions about how 

488
00:30:14,181 --> 00:30:17,401
long it would take.
And so you can have this tight feedback 

489
00:30:17,401 --> 00:30:18,670
loop from designer saying,
here's how good your particular,

490
00:30:18,790 --> 00:30:22,090
uh,
interfaces so act are as a whole.

491
00:30:22,290 --> 00:30:27,290
It's very prevalent in this community.
I went to their webpage and counted up 

492
00:30:27,290 --> 00:30:30,751
just the papers that they knew about.
It was over 1100 papers over time.

493
00:30:31,840 --> 00:30:33,100
Uh,
if you're interested in it,

494
00:30:33,400 --> 00:30:36,370
the main distribution is in lisp,
but,

495
00:30:36,460 --> 00:30:41,460
uh,
many people have used this and wanting 

496
00:30:41,460 --> 00:30:41,460
to apply it to systems that need a 
little more processing power.

497
00:30:42,030 --> 00:30:42,970
Uh,
so there's,

498
00:30:43,210 --> 00:30:46,780
the NRL has a java part of it that they 
use in robotics.

499
00:30:46,990 --> 00:30:51,990
The air force research lab in Dayton has
implemented it in Erlangen for parallel 

500
00:30:53,561 --> 00:30:55,900
processing of large declarative 
knowledge bases.

501
00:30:55,980 --> 00:31:00,980
Uh,
they're trying to do service oriented 

502
00:31:00,980 --> 00:31:02,860
architectures with it Kuda because they 
want what it has to say.

503
00:31:02,861 --> 00:31:05,260
They don't want to wait around for a tap
to figure that stuff out.

504
00:31:05,860 --> 00:31:08,380
Uh,
so that's,

505
00:31:08,410 --> 00:31:12,490
uh,
the two minutes about act are a sigma as

506
00:31:12,491 --> 00:31:17,080
a relative newcomer.
And it's developed out at the University

507
00:31:17,080 --> 00:31:20,200
of southern California by a man named 
Paul Rosenbloom,

508
00:31:20,220 --> 00:31:25,220
and I'll mentioned a couple of minutes 
because he was one of the prime 

509
00:31:25,220 --> 00:31:25,970
developers have soar at Carnegie Mellon,
uh,

510
00:31:26,020 --> 00:31:29,560
so he knows a lot about how solar works 
and he's worked on it over the years.

511
00:31:29,890 --> 00:31:34,890
And I think originally I'm going to 
speak for him and he'll probably say I 

512
00:31:34,890 --> 00:31:38,251
was wrong.
I think originally it was kind of a 

513
00:31:38,251 --> 00:31:41,370
mental exercise of can I reproduce sore 
using a uniform substrate?

514
00:31:43,000 --> 00:31:44,680
I'll talk about in a little bit.
Uh,

515
00:31:45,070 --> 00:31:49,030
it's 30 years of research code.
If anybody's dealt with research code,

516
00:31:49,031 --> 00:31:54,031
it's 30 years of C and c plus plus with 
dozens of graduate students over time.

517
00:31:55,510 --> 00:31:57,830
It's not pretty at all.
Uh,

518
00:31:57,840 --> 00:31:59,210
and,
and theoretically it's,

519
00:31:59,220 --> 00:32:01,450
it's got these boxes sitting out here.
And so,

520
00:32:01,610 --> 00:32:04,960
uh,
he re implemented the core functionality

521
00:32:04,961 --> 00:32:09,961
of sore all using factor graphs and 
message passing algorithms under the 

522
00:32:09,961 --> 00:32:14,521
hood.
He got to that point and then said 

523
00:32:14,521 --> 00:32:14,830
there's nothing stopping me from going 
further.

524
00:32:15,010 --> 00:32:18,130
And so now it can do all sorts of modern
machine learning,

525
00:32:18,131 --> 00:32:22,450
vision optimization,
sort of things that would take some time

526
00:32:22,451 --> 00:32:25,240
in any other architecture to be able to 
integrate well.

527
00:32:25,750 --> 00:32:26,700
So,
uh,

528
00:32:26,710 --> 00:32:29,010
it's been an interesting experience.
Uh,

529
00:32:29,020 --> 00:32:34,020
it's now going to be the basis for the 
virtual human project out at the 

530
00:32:34,020 --> 00:32:37,921
institute for creative technology.
It's a institute associated with the 

531
00:32:37,921 --> 00:32:41,310
University of southern California for,
until recently could get your hands on 

532
00:32:41,451 --> 00:32:46,451
it,
but in the last couple of years he's 

533
00:32:46,451 --> 00:32:48,280
done some tutorials on it.
He's got a public release with 

534
00:32:48,280 --> 00:32:51,250
documentation.
So that's something interesting to keep 

535
00:32:51,250 --> 00:32:54,371
an eye on what I'm going to spend all 
the remaining time on the sore cognitive

536
00:32:54,531 --> 00:32:56,660
architecture.
And so you see,

537
00:32:56,900 --> 00:33:00,350
it looks quite a bit like the 
prototypical architecture.

538
00:33:00,351 --> 00:33:02,240
And I'll,
I'll give you a sentence again about how

539
00:33:02,241 --> 00:33:05,480
this all operates.
Give a sense of the people involved.

540
00:33:05,670 --> 00:33:07,670
Uh,
we already talked about Alan Newell.

541
00:33:07,671 --> 00:33:12,671
So both John Laird,
who is my advisor and Paul Rosenbloom 

542
00:33:12,671 --> 00:33:14,730
were students of Allan Newell.
Uh,

543
00:33:14,900 --> 00:33:19,900
John's thesis project was related to the
chunking mechanism and soar,

544
00:33:21,051 --> 00:33:24,950
which learns new rules based upon 
subgoal reasoning.

545
00:33:25,760 --> 00:33:27,590
Uh,
so he finished that,

546
00:33:27,980 --> 00:33:32,980
I believe the year I was born,
and so he's one of the few researchers 

547
00:33:33,711 --> 00:33:37,580
you'll find who still actively working 
on their thesis project,

548
00:33:37,980 --> 00:33:40,600
uh,
beyond that,

549
00:33:40,640 --> 00:33:42,950
about I think about 10 years ago.
He,

550
00:33:43,040 --> 00:33:44,270
it.
So our technology,

551
00:33:44,271 --> 00:33:49,271
which is company up in Ann Arbor,
Michigan a while it was called soar 

552
00:33:49,271 --> 00:33:49,410
technology,
it doesn't do exclusively sore,

553
00:33:49,420 --> 00:33:54,020
but that's a part of the portfolio,
a general intelligence system stuff,

554
00:33:54,021 --> 00:33:58,310
a lot of defense association.
So,

555
00:33:58,370 --> 00:34:03,370
uh,
some notes of what's going to make sore 

556
00:34:03,370 --> 00:34:05,411
different from the other architectures 
that fall into this kind of functional 

557
00:34:05,411 --> 00:34:06,060
architecture category.
Uh,

558
00:34:06,170 --> 00:34:11,170
a big thing is a focus on efficiency.
So John wants to be able to run sore on 

559
00:34:11,170 --> 00:34:15,010
just about anything.
We just got on the mailing list,

560
00:34:15,230 --> 00:34:20,230
a desire to run it on a real time 
processor and our answer,

561
00:34:20,661 --> 00:34:23,990
while we had never done it before,
it was probably,

562
00:34:23,991 --> 00:34:25,470
it'll work,
uh,

563
00:34:25,610 --> 00:34:28,670
every release,
there's timing tests and we always,

564
00:34:29,270 --> 00:34:34,270
what we,
what we look at is in a bunch of 

565
00:34:34,270 --> 00:34:36,311
different domains for a bunch of 
different reasons that relate to human 

566
00:34:36,311 --> 00:34:36,311
processing.

567
00:34:36,311 --> 00:34:37,400
There's this magic number that comes out
which is 50 milliseconds,

568
00:34:37,760 --> 00:34:41,990
which is to say in terms of responding 
to tasks,

569
00:34:42,140 --> 00:34:44,150
if you're above that time,
uh,

570
00:34:44,240 --> 00:34:47,840
humans will sense a delay and you don't 
want that to happen.

571
00:34:48,020 --> 00:34:49,790
Now,
if we're working in a robotics task,

572
00:34:49,791 --> 00:34:52,700
50 milliseconds,
if you're dramatically above that,

573
00:34:52,701 --> 00:34:56,600
you just fell off the curb or worse or 
you just hit somebody in a car.

574
00:34:56,601 --> 00:34:57,580
Right?
So we're,

575
00:34:57,581 --> 00:35:01,490
we're trying to keep that as low as 
possible and for most agents it,

576
00:35:01,520 --> 00:35:06,520
it doesn't even register.
It's below one millisecond fractions of 

577
00:35:06,520 --> 00:35:07,880
a millisecond.
But I'll come back to this because a lot

578
00:35:07,881 --> 00:35:10,460
of the work that I was doing was 
computer science,

579
00:35:10,461 --> 00:35:11,960
Ai,
uh,

580
00:35:11,970 --> 00:35:16,970
and,
and a lot of efficient algorithms and 

581
00:35:16,970 --> 00:35:17,180
data structures and 50 milliseconds was 
that very high upper bound.

582
00:35:18,380 --> 00:35:21,080
It's also one of the projects that has a
public distribution.

583
00:35:21,080 --> 00:35:23,990
You can get it in all sorts of operating
systems.

584
00:35:24,770 --> 00:35:29,770
We use something called Swig that allows
you to interface with it and a bunch of 

585
00:35:29,770 --> 00:35:32,801
different languages.
We kind of described the Meta 

586
00:35:32,801 --> 00:35:34,421
description and you are able to 
basically generate bindings and a bunch 

587
00:35:34,421 --> 00:35:37,380
of different platforms.
A core of plus,

588
00:35:37,381 --> 00:35:41,220
plus there was a team at soar attack 
that said we don't like c Plus,

589
00:35:41,221 --> 00:35:45,210
plus it gets messy,
so they actually did a port over to pure

590
00:35:45,240 --> 00:35:50,240
Java in case that appeals to you.
There's an annual workshop that takes 

591
00:35:50,251 --> 00:35:53,370
place in anarbor.
Typically it's free.

592
00:35:54,150 --> 00:35:59,150
You can go there and get a sore tutorial
and talked to folks who are working on 

593
00:35:59,150 --> 00:36:00,300
sore and it's fun.
I've been there every year,

594
00:36:00,301 --> 00:36:05,301
but wine in the last decade.
It's just fun to see the people around 

595
00:36:05,301 --> 00:36:08,001
the world better using the system and 
all sorts of interesting ways to give 

596
00:36:08,311 --> 00:36:10,110
you a sense of the diversity of the 
applications.

597
00:36:10,110 --> 00:36:15,110
One of the first was our one store,
which was back in the days when it was 

598
00:36:15,110 --> 00:36:19,341
an actual challenge to build a computer,
which is to say that your choice of 

599
00:36:19,341 --> 00:36:24,200
certain components would have radical 
implications for other parts of the 

600
00:36:24,200 --> 00:36:26,280
computer,
so it wasn't just the Dell website where

601
00:36:26,281 --> 00:36:27,690
you just,
I want this much ram,

602
00:36:27,691 --> 00:36:32,691
I want this much cpu.
There was a lot of thinking that went 

603
00:36:32,691 --> 00:36:35,181
behind it and then physical Labor that 
went to construct your computer and so 

604
00:36:35,181 --> 00:36:38,460
it was making that process a lot better.
There are folks that apply it to natural

605
00:36:38,461 --> 00:36:43,461
language processing.
A source seven was the core of the 

606
00:36:43,461 --> 00:36:46,050
virtual humans project for a long time.
Hci Tasks,

607
00:36:46,450 --> 00:36:49,110
TAC air soar was one of the largest rule
based systems,

608
00:36:49,440 --> 00:36:51,990
tens of thousands of rules over 48 
hours.

609
00:36:51,991 --> 00:36:56,070
It was a very large scale simulation,
a defense simulation,

610
00:36:56,310 --> 00:36:59,550
lots of games it's been applied to for 
various reasons.

611
00:37:00,460 --> 00:37:04,800
And then in the last few years putting 
it onto mobile robotics platforms.

612
00:37:04,801 --> 00:37:08,940
This is Edwin Olson's.
A splinter bought an early version of it

613
00:37:08,941 --> 00:37:11,400
that went on to win the magic 
competition.

614
00:37:12,470 --> 00:37:17,200
Uh,
then I went onto put sore on the web and

615
00:37:17,260 --> 00:37:22,260
if after this talk,
you're really interested in a dice game 

616
00:37:22,260 --> 00:37:24,550
that I'm going to talk about what you 
can actually go to the ios app store and

617
00:37:24,551 --> 00:37:26,350
download.
It's called Michigan Liar's dice.

618
00:37:26,351 --> 00:37:28,360
It's free.
You don't have to pay for it,

619
00:37:28,570 --> 00:37:33,570
but you can actually play a liar's dice 
with soar and it's.

620
00:37:33,760 --> 00:37:36,340
You can set the difficulty level.
It's pretty good.

621
00:37:36,570 --> 00:37:40,800
It beats me on a regular basis.
I wanted to give you a couple other just

622
00:37:40,801 --> 00:37:45,801
kind of really weird feeling sort of 
applications and really cool 

623
00:37:45,801 --> 00:37:49,200
applications.
The first one is out of Georgia tech.

624
00:37:49,910 --> 00:37:51,110
Go powerpoint.
Yes.

625
00:37:58,580 --> 00:38:03,580
Interactive art installation in which 
you can engage in collaborative 

626
00:38:03,911 --> 00:38:08,911
movement,
improvisation with each other and 

627
00:38:08,911 --> 00:38:11,251
virtual dance partners.
This actually creates a hybrid space in 

628
00:38:11,281 --> 00:38:16,281
which virtual and corporate real bodies 
meet for human and nonhuman is blurred 

629
00:38:17,740 --> 00:38:21,570
staring contest as to examine their 
relationship with technology.

630
00:38:23,010 --> 00:38:28,010
Installation ultimately examines how 
humans and machine can cocreate 

631
00:38:28,010 --> 00:38:32,440
experiences in a playful environment.
It creates a social space that 

632
00:38:34,030 --> 00:38:38,110
encourages human interaction and 
collected dance experiences,

633
00:38:38,500 --> 00:38:42,520
allowing them to send this to creatively
explore movement while having fun.

634
00:38:43,870 --> 00:38:44,890
The development

635
00:38:45,070 --> 00:38:50,070
and I had been a hybrid exploration and 
art forms of theater and dance as well 

636
00:38:50,271 --> 00:38:53,810
as research and artificial intelligence 
and cognitive science.

637
00:38:55,750 --> 00:38:58,850
Moving on.
Inspiration from the ancient art form,

638
00:38:58,851 --> 00:39:03,851
a shadow here,
the original two dimensional version of 

639
00:39:03,851 --> 00:39:06,110
the installation.
Let us the conceptualization of the dome

640
00:39:06,111 --> 00:39:11,111
and the liminal space when human 
silhouettes and virtual characters 

641
00:39:11,111 --> 00:39:13,430
meaning dance together on the projection
surface,

642
00:39:14,980 --> 00:39:18,900
rather than rely on preoperative library
of responses,

643
00:39:19,390 --> 00:39:24,390
the virtual dancer learns his partner 
and utilize new theories and 

644
00:39:24,600 --> 00:39:29,600
systematically reason about them in 
order to choose under response.

645
00:39:30,620 --> 00:39:35,620
The points theory is based in dance and 
theater and performance along the 

646
00:39:35,741 --> 00:39:39,700
dimension,
the tempo reputation,

647
00:39:39,970 --> 00:39:44,040
kinesthetic response,
shape station relationships,

648
00:39:44,080 --> 00:39:47,060
gesture architecture,
and your topic.

649
00:39:47,130 --> 00:39:52,130
Be Virtual.
Dancer is able to use several different 

650
00:39:52,130 --> 00:39:56,791
strategies to respond to human rights.
These include transformation and he 

651
00:39:59,240 --> 00:40:04,240
mentions for calling it similar or 
complimentary movement from memory in 

652
00:40:04,240 --> 00:40:08,581
terms of viewpoints,
admissions and apply action response 

653
00:40:08,581 --> 00:40:10,210
patterns that the Asian consumer.

654
00:40:14,750 --> 00:40:19,750
This is part of a larger effort from 
setting the relationship between 

655
00:40:19,750 --> 00:40:24,441
communication,
cognition and creativity where a large 

656
00:40:24,441 --> 00:40:28,980
amount of our efforts go into 
understanding human creativity and how 

657
00:40:28,980 --> 00:40:31,910
we make things together.
Our work together as a way to understand

658
00:40:31,920 --> 00:40:36,920
how we can build cocreative ai that 
serves the same purpose where it could 

659
00:40:37,561 --> 00:40:41,160
be a colleague and collaborator with us 
and create things with us.

660
00:40:47,730 --> 00:40:52,730
Uh,
so brian was a graduate student in John 

661
00:40:52,730 --> 00:40:53,930
Lyric lab as well.
Uh,

662
00:40:54,240 --> 00:40:59,240
before I start this,
I alluded to this earlier where we're 

663
00:40:59,240 --> 00:41:00,870
getting closer to rosie saying,
can you teach me?

664
00:41:01,020 --> 00:41:02,760
So let me give you some introduction to 
this.

665
00:41:02,820 --> 00:41:07,820
In the lower left,
you're seeing the view of a connect 

666
00:41:07,820 --> 00:41:08,980
camera onto a flat surface.
There's a,

667
00:41:09,720 --> 00:41:12,720
a robotic arm,
mainly three d printed parts,

668
00:41:12,721 --> 00:41:14,670
few Servos,
uh,

669
00:41:14,750 --> 00:41:19,750
above that.
You're seeing an interpretation of the 

670
00:41:19,750 --> 00:41:22,881
scene a,
we're giving it a kind of associations 

671
00:41:22,881 --> 00:41:24,210
of the four areas with semantic titles 
like,

672
00:41:24,840 --> 00:41:26,280
uh,
one is the table,

673
00:41:26,281 --> 00:41:30,270
one is the garbage,
just just semantic terms for areas.

674
00:41:30,450 --> 00:41:33,030
But other than that,
the agent doesn't actually know all that

675
00:41:33,050 --> 00:41:35,930
much and it's going to operate in two 
modalities.

676
00:41:35,931 --> 00:41:38,180
One is,
we'll call it natural language,

677
00:41:38,210 --> 00:41:43,210
natural language,
a restricted subset of English as well 

678
00:41:44,181 --> 00:41:45,980
as some quote unquote pointing.

679
00:41:45,980 --> 00:41:49,700
So you're going to see some mouse 
pointers in the upper left saying,

680
00:41:49,820 --> 00:41:51,330
I'm talking about this.
Uh,

681
00:41:51,340 --> 00:41:53,960
and this is just a way to indicate 
location.

682
00:41:54,410 --> 00:41:56,600
And so starting off we're going to say 
things like,

683
00:41:56,630 --> 00:41:58,400
you know,
pick up the blue block and it's going to

684
00:41:58,401 --> 00:42:00,140
be like,
I don't know what blue is,

685
00:42:00,170 --> 00:42:02,090
what is blue?
And we say,

686
00:42:02,091 --> 00:42:03,170
oh,
well that's a color.

687
00:42:03,380 --> 00:42:04,830
Okay.
Uh,

688
00:42:05,090 --> 00:42:08,480
you know,
so go get the green thing.

689
00:42:08,510 --> 00:42:09,320
What's green?
Oh,

690
00:42:09,321 --> 00:42:10,160
it's a color.
Okay.

691
00:42:10,370 --> 00:42:15,370
Uh,
move the blue thing to a particular 

692
00:42:15,370 --> 00:42:15,370
location.
Where's that point at?

693
00:42:15,370 --> 00:42:20,111
Okay,
what is moving like really it has to 

694
00:42:20,111 --> 00:42:20,870
start from the beginning and it's 
described and it said,

695
00:42:20,871 --> 00:42:22,220
okay,
now you finished.

696
00:42:23,420 --> 00:42:28,420
And once we got to that point,
now I can say move the green thing over 

697
00:42:28,420 --> 00:42:29,360
here.
And it's got everything that it needs to

698
00:42:29,361 --> 00:42:32,390
be able to then reproduce the task given
a new parameters.

699
00:42:32,480 --> 00:42:36,980
And it's learned that ability.
So let me give it a little bit of time.

700
00:42:39,020 --> 00:42:44,020
Oh,
uh,

701
00:42:45,620 --> 00:42:48,740
so you can look a little bit at top left
in terms of the pointers.

702
00:42:48,741 --> 00:42:51,890
You're going to see some text commands 
being entered.

703
00:42:54,080 --> 00:42:57,440
So what kind of attributes is blue?
We're going to say it's a color.

704
00:42:57,680 --> 00:43:00,800
And so that can map it then to a 
particular sensory modality.

705
00:43:01,940 --> 00:43:06,940
This is green.
So the pointing what kind of thing is 

706
00:43:06,940 --> 00:43:06,940
green,
okay,

707
00:43:06,940 --> 00:43:10,211
color.
So now it knows how to understand blue 

708
00:43:10,211 --> 00:43:10,211
and green as colors with respect to the 
visual scene.

709
00:43:10,520 --> 00:43:13,730
Um,
move rectangle to the table.

710
00:43:14,870 --> 00:43:17,570
Uh,
what is rectangle?

711
00:43:17,571 --> 00:43:20,510
Okay,
now I can map that onto or understanding

712
00:43:20,900 --> 00:43:23,150
parts of the world.
Is this the blue rectangle?

713
00:43:23,151 --> 00:43:28,100
So the arm is actually pointing itself 
to get confirmation from the instructor.

714
00:43:28,500 --> 00:43:33,500
Uh,
and then we're trying to understand in 

715
00:43:33,500 --> 00:43:33,500
general,
when you say move something,

716
00:43:33,500 --> 00:43:37,091
what is the goal of this operation?
And so then it also has to declare of 

717
00:43:37,091 --> 00:43:39,950
representation of the idea of this task,
not only that had completed it,

718
00:43:40,220 --> 00:43:45,220
then it can look back on having 
completed the task and understand what 

719
00:43:45,220 --> 00:43:47,030
were the steps that lead to achieving a 
particular goal.

720
00:43:50,780 --> 00:43:53,450
So in order to move it,
you're gonna have to pick it up.

721
00:43:54,260 --> 00:43:59,260
It knows which one the blue thing is.
Great.

722
00:44:04,400 --> 00:44:08,330
Now put it in the table.
So that's a particular location.

723
00:44:09,970 --> 00:44:14,970
At this point we can say you're done.
You have accomplished the move the blue 

724
00:44:14,970 --> 00:44:19,360
rectangle to the table.
And so I can understand what that very 

725
00:44:19,360 --> 00:44:23,111
simple kind of processes like and 
associate that with the verb to move and

726
00:44:25,510 --> 00:44:30,510
now we can say move the green object or 
not doing the garbage and without any 

727
00:44:35,340 --> 00:44:40,320
further interaction based on everything 
that I've learned up until that point,

728
00:44:40,350 --> 00:44:45,350
it can successfully complete that task.
So this is a work of Shivali Mohan and 

729
00:44:45,350 --> 00:44:49,731
others at the soar group at the 
University of Michigan on the rosy 

730
00:44:49,731 --> 00:44:53,571
project and they were extending this to 
playing games and learning the rules of 

731
00:44:53,571 --> 00:44:56,730
games through a tech space descriptions 
and multimodal experience.

732
00:44:57,960 --> 00:44:58,950
So,
uh,

733
00:44:59,100 --> 00:45:00,690
in order to build up to here's a story.

734
00:45:00,690 --> 00:45:03,600
And so I wanted to give you a sense of 
how research occurs in the group.

735
00:45:03,930 --> 00:45:08,490
And so there's these back and forth that
occur over time between,

736
00:45:08,760 --> 00:45:13,760
there's this piece of software called 
soar and we want to make this thing 

737
00:45:13,760 --> 00:45:16,461
better and give it new capabilities.
And so all our agents are going to 

738
00:45:16,461 --> 00:45:17,310
become better.
And we always have to keep in mind,

739
00:45:17,311 --> 00:45:22,311
and you'll see this as I go further,
that it has to be useful to a wide 

740
00:45:22,311 --> 00:45:25,410
variety of agents.
It has to be task independent and it has

741
00:45:25,411 --> 00:45:27,600
to be efficient for us to do anything in
the architecture.

742
00:45:27,601 --> 00:45:32,601
All of those have to hold true,
so we do something cool in the 

743
00:45:32,601 --> 00:45:33,160
architecture and they,
we say,

744
00:45:33,161 --> 00:45:34,830
okay,
let's solve a cool problem.

745
00:45:35,250 --> 00:45:39,000
So let's build some agents to do this.
And so this ends up testing what are the

746
00:45:39,001 --> 00:45:41,790
limitations,
what are the issues that arise,

747
00:45:41,990 --> 00:45:46,990
uh,
in a particular mechanism as well as 

748
00:45:46,990 --> 00:45:46,990
integration with others.

749
00:45:46,990 --> 00:45:50,780
Uh,
and we get to solve interesting 

750
00:45:50,780 --> 00:45:52,011
problems.
We usually find there was something 

751
00:45:52,011 --> 00:45:52,011
missing and then we can go back to the 
architecture and rinse and repeat.

752
00:45:53,010 --> 00:45:55,470
Just to give you an idea again how sore 
works.

753
00:45:55,650 --> 00:45:58,770
So the working memory is actually a 
directed connected graph.

754
00:45:59,370 --> 00:46:02,250
The perception is just a subset of that 
graph.

755
00:46:02,251 --> 00:46:05,910
And so there's going to be symbolic 
representations of most of the world.

756
00:46:06,260 --> 00:46:09,180
There is a visual subsystem in which you
can provide a scene graph.

757
00:46:09,310 --> 00:46:12,240
I'm just not showing it here.
Uh,

758
00:46:12,241 --> 00:46:14,610
actions are also a subset of that graph.
And so,

759
00:46:14,850 --> 00:46:16,830
uh,
the procedural knowledge,

760
00:46:16,831 --> 00:46:19,010
which is also production rules can 
modify.

761
00:46:19,011 --> 00:46:22,200
It can read sections of the input,
modify sections of the output as well as

762
00:46:22,201 --> 00:46:24,930
arbitrary parts of the graph to take 
actions.

763
00:46:25,200 --> 00:46:30,200
So the decision procedure says,
of all the things that I know to do and 

764
00:46:30,200 --> 00:46:30,930
I've kind of ranked them according to 
various preferences,

765
00:46:31,290 --> 00:46:32,850
what single thing should I do?

766
00:46:33,750 --> 00:46:37,920
A semantic memory for facts.
There's episodic memory.

767
00:46:38,070 --> 00:46:42,750
The agent is always actually storing 
every experience it's ever had over time

768
00:46:42,751 --> 00:46:45,270
and episodic memory and it has the 
ability to get back to that.

769
00:46:45,840 --> 00:46:50,840
And so the similar cycle we saw before 
we get input in the perception called 

770
00:46:50,840 --> 00:46:54,300
the input link rules are going to fire 
all in parallel and say,

771
00:46:54,390 --> 00:46:55,950
here's everything I know about this 
situation.

772
00:46:55,951 --> 00:46:58,860
Here's all the things I could do.
Decision Procedure says,

773
00:46:59,160 --> 00:47:03,720
here's what we're going to do based on 
the selected operator.

774
00:47:04,020 --> 00:47:08,160
All sorts of things could happen with 
respect to memories providing input,

775
00:47:08,360 --> 00:47:13,020
a rules firing to perform computations,
and as well as potentially output in the

776
00:47:13,021 --> 00:47:15,240
world.
Uh,

777
00:47:15,510 --> 00:47:17,910
I don't remember.
Agent reactivity is required.

778
00:47:18,120 --> 00:47:23,120
We want the system to be able to react 
to things in the world at a very quick 

779
00:47:23,581 --> 00:47:28,581
pace.
So anything that happens in this cycle 

780
00:47:28,581 --> 00:47:31,280
at Max,
the overall cycle has to be under 50 

781
00:47:31,280 --> 00:47:31,280
milliseconds.

782
00:47:31,280 --> 00:47:31,960
So that's gonna be a constraint we hold 
ourselves to.

783
00:47:32,440 --> 00:47:37,440
And so the story I'll be telling will 
say how we got to a point where we 

784
00:47:37,440 --> 00:47:42,031
started actually forgetting things and 
we're an architecture that doesn't want 

785
00:47:42,031 --> 00:47:44,500
to be like humans.
We want to create cool systems,

786
00:47:44,680 --> 00:47:48,130
but what we realized was something that 
we do,

787
00:47:48,160 --> 00:47:53,160
there's probably some benefit to it and 
we actually put it into our system and 

788
00:47:53,160 --> 00:47:55,000
the lead to good outputs.
So here's the research path.

789
00:47:55,001 --> 00:47:59,500
I'm going to walk down a.
We had a just a simple problem which was

790
00:47:59,800 --> 00:48:04,800
we have these memory systems and 
sometimes they're going to get a queue 

791
00:48:04,800 --> 00:48:07,830
that could relate to multiple memories.
And the question is if you have a fixed 

792
00:48:07,830 --> 00:48:11,881
mechanism,
what should you return in a task 

793
00:48:11,881 --> 00:48:12,280
independent way?
Which one of these many,

794
00:48:12,281 --> 00:48:17,281
many memories shitty return.
That was our question and we look to 

795
00:48:17,281 --> 00:48:17,560
some human data on this.

796
00:48:17,590 --> 00:48:22,590
Something called the rational analysis 
of memory done by John Anderson and 

797
00:48:22,590 --> 00:48:26,070
realized that in human language,
their recency and frequency effects that

798
00:48:27,670 --> 00:48:29,890
maybe it would be useful.
And so,

799
00:48:30,130 --> 00:48:35,130
uh,
we actually did an analysis found that 

800
00:48:35,130 --> 00:48:37,290
not only does this occur but it's useful
in what are called word sense 

801
00:48:37,290 --> 00:48:40,291
disambiguation tasks.
And I'll get to that what that means in 

802
00:48:40,291 --> 00:48:43,381
a second.
Develop some algorithms to scale this 

803
00:48:43,381 --> 00:48:46,171
really well and it turned out to work 
out well not only in the original task 

804
00:48:46,171 --> 00:48:48,370
but we look to,
to other completely different ones.

805
00:48:48,490 --> 00:48:53,490
The same underlying mechanism ended up 
producing some really interesting 

806
00:48:53,490 --> 00:48:57,811
outputs.
Suddenly talking about word sense 

807
00:48:57,811 --> 00:48:58,690
disambiguation real quick.
So as a core problem in natural language

808
00:48:58,691 --> 00:49:00,460
processing,
if you haven't heard of it before,

809
00:49:00,730 --> 00:49:05,730
let's say we have an agent and for some 
reason it needs to understand the verb 

810
00:49:05,730 --> 00:49:09,331
to run looks to its memory and finds 
that it could,

811
00:49:09,730 --> 00:49:10,790
you know,
run in the park.

812
00:49:10,810 --> 00:49:15,810
It could be running a fever,
could run an election that could run a 

813
00:49:15,810 --> 00:49:15,970
program.
And the question is,

814
00:49:15,971 --> 00:49:20,971
what should an task,
independent memory mechanism return if 

815
00:49:20,971 --> 00:49:23,200
all you've been given is the verb to 
run?

816
00:49:24,160 --> 00:49:28,480
And so the rational analysis of memory 
look through multiple texts Corpora,

817
00:49:28,540 --> 00:49:33,490
and what they found was if a particular 
word had been used recently,

818
00:49:33,640 --> 00:49:38,290
it's very likely to be reused again.
And if it hadn't been used recently,

819
00:49:38,410 --> 00:49:40,330
there's going to be this effect to 
where,

820
00:49:40,540 --> 00:49:41,680
uh,
the expression here,

821
00:49:41,681 --> 00:49:46,681
the t is time since the most recent use.
It's going to some those with a 

822
00:49:46,780 --> 00:49:51,780
exponential decay.
So what it looks like if time is going 

823
00:49:51,780 --> 00:49:54,700
to the right,
a activation higher is better.

824
00:49:55,270 --> 00:50:00,270
As you get these individual usages,
you get these little drops and then 

825
00:50:00,270 --> 00:50:03,440
venture he dropped down.
And so if we had just one usage of a 

826
00:50:03,440 --> 00:50:06,481
word,
the red would be what the decay would 

827
00:50:06,481 --> 00:50:06,481
look like.

828
00:50:06,481 --> 00:50:10,170
And so the core problem here is if we're
at a particular point and we want to 

829
00:50:10,170 --> 00:50:11,170
select between kind of the blue thing or
the red thing,

830
00:50:11,530 --> 00:50:15,160
blue would have a higher activation.
And so maybe that's useful.

831
00:50:15,910 --> 00:50:19,480
This is how things are modeled with 
human memory,

832
00:50:20,080 --> 00:50:25,080
but is it useful in general for tasks?
And so we looked at common corpora used 

833
00:50:25,871 --> 00:50:28,100
in word sense distribuition and just 
said,

834
00:50:28,101 --> 00:50:33,101
well,
if we just look at this corporate twice 

835
00:50:33,101 --> 00:50:33,101
and we just use answers,
prior answers,

836
00:50:33,101 --> 00:50:34,160
you know,
I asked the question,

837
00:50:34,600 --> 00:50:36,470
what is the sense of this word?
I took a guess,

838
00:50:36,471 --> 00:50:41,471
I got the right answer and I used that 
recency and frequency information to my 

839
00:50:41,471 --> 00:50:41,990
task.
Independent memory.

840
00:50:42,140 --> 00:50:45,350
Would that be useful?
And somewhat of a surprise,

841
00:50:45,351 --> 00:50:49,610
but somewhat maybe not have a surprise.
It actually performed really well across

842
00:50:49,611 --> 00:50:52,040
multiple corporate.
So we said,

843
00:50:52,041 --> 00:50:56,660
okay,
this seems like a reasonable mechanism.

844
00:50:57,140 --> 00:51:00,560
Let's look at implementing this 
efficiently and the architecture.

845
00:51:00,830 --> 00:51:04,130
And the problem was this term right here
said,

846
00:51:04,160 --> 00:51:09,160
for every memory,
for every time step you're having to k 

847
00:51:09,160 --> 00:51:14,050
everything.
That doesn't sound like a recipe for 

848
00:51:14,050 --> 00:51:18,491
efficiency if you're talking about lots 
and lots of knowledge over long periods 

849
00:51:18,491 --> 00:51:18,890
of time.
So we,

850
00:51:19,110 --> 00:51:24,110
uh,
made use of a nice approximation that 

851
00:51:24,110 --> 00:51:25,100
Petrofac come up with to approximate 
tail effect.

852
00:51:25,101 --> 00:51:26,030
So,
uh,

853
00:51:26,730 --> 00:51:29,360
access to that happened long,
long ago.

854
00:51:29,361 --> 00:51:31,880
We could basically approximate their 
effect on the overall sum.

855
00:51:32,060 --> 00:51:35,300
So we had a fixed set of values,
uh,

856
00:51:35,301 --> 00:51:40,301
and what we basically said is,
since these are always decreasing and 

857
00:51:40,301 --> 00:51:44,741
all we care about is relative order,
let's just only recompute when someone 

858
00:51:44,741 --> 00:51:45,550
gets a new value.
So it's,

859
00:51:45,551 --> 00:51:48,350
it's a guests,
it's a heuristic and approximation,

860
00:51:48,800 --> 00:51:53,800
but we looked at how this worked on the 
same set of Corpora and in terms of 

861
00:51:54,021 --> 00:51:59,021
query time,
if we made these approximations well 

862
00:51:59,021 --> 00:52:02,021
under our 50 millisecond,
the effect on task performance was 

863
00:52:02,021 --> 00:52:02,270
negligible.

864
00:52:02,270 --> 00:52:07,270
In fact,
on a couple of these that God's ever so 

865
00:52:07,270 --> 00:52:09,731
slightly better in terms of accuracy.
And actually if we looked at the 

866
00:52:09,731 --> 00:52:11,480
individual decisions that were being 
made,

867
00:52:11,930 --> 00:52:16,100
making these sorts of approximations,
we're leading to a,

868
00:52:16,101 --> 00:52:17,460
up to 90,
sorry,

869
00:52:17,510 --> 00:52:22,400
at least 90 percent of the decisions 
being made were identical to having done

870
00:52:22,401 --> 00:52:27,401
the true a full calculation.
So we said this is great and we 

871
00:52:28,551 --> 00:52:33,551
implemented this and worked really well.
And then we started working on what 

872
00:52:33,551 --> 00:52:35,630
seemed like completely unrelated 
problems.

873
00:52:35,840 --> 00:52:39,140
One was in mobile robotics.
We had a mobile robot.

874
00:52:39,141 --> 00:52:40,760
I'll show a picture of in a little 
while,

875
00:52:40,880 --> 00:52:45,880
roaming around the halls performing all 
sorts of tasks and what we're finding 

876
00:52:45,880 --> 00:52:49,511
was if you have a system that's 
remembering everything in your short 

877
00:52:49,511 --> 00:52:51,320
term memory and your short term memory 
gets really,

878
00:52:51,321 --> 00:52:53,300
really big.
I don't know about you.

879
00:52:53,301 --> 00:52:55,400
My short term memory feels really,
really small.

880
00:52:55,400 --> 00:53:00,400
I would love it to be big,
but if you make your memory really big 

881
00:53:00,400 --> 00:53:04,151
and you try to remember something,
you're not having to pull lots and lots 

882
00:53:04,151 --> 00:53:04,850
and lots of information into your short 
term memory.

883
00:53:05,090 --> 00:53:10,090
So the system was actually getting 
slower simply because it had a lot of 

884
00:53:10,090 --> 00:53:12,020
short term memory,
a representation of the overall map.

885
00:53:12,021 --> 00:53:16,250
It was looking at so large working 
memory problem,

886
00:53:17,000 --> 00:53:18,860
Liar's dice game,
you play with dice.

887
00:53:19,010 --> 00:53:23,420
We were doing an rl based system on this
reinforcement learning and it turned out

888
00:53:24,450 --> 00:53:26,130
a really,
really big value function.

889
00:53:26,131 --> 00:53:31,131
Worse having to store lots of data and 
we didn't know which stuff we had to 

890
00:53:31,131 --> 00:53:31,840
keep around to keep the performance,
uh,

891
00:53:31,980 --> 00:53:36,980
up.
So we had a hypothesis that forgetting 

892
00:53:36,980 --> 00:53:40,880
was actually going to be a beneficial 
thing that maybe maybe the the problem 

893
00:53:42,451 --> 00:53:43,890
we have with our memories that we 
really,

894
00:53:43,891 --> 00:53:45,720
really disliked this forgetting thing.

895
00:53:46,050 --> 00:53:51,050
Maybe it's actually useful.
And so we experimented with the 

896
00:53:51,050 --> 00:53:51,050
following policy.
We said,

897
00:53:51,050 --> 00:53:55,370
let's forget a memory if one we haven't 
really.

898
00:53:55,630 --> 00:53:58,410
It's not predicted to be useful by this 
base level activation.

899
00:53:58,560 --> 00:54:00,840
We haven't used it recently,
we haven't used it frequently,

900
00:54:00,900 --> 00:54:02,850
maybe it's not worth it.
That.

901
00:54:02,851 --> 00:54:07,851
And we felt confident that we could 
approximately reconstructed if we 

902
00:54:07,851 --> 00:54:10,560
absolutely had to.
And if those two things held,

903
00:54:10,590 --> 00:54:13,500
we could forget something.
Uh,

904
00:54:13,501 --> 00:54:17,760
so it's this bay same basic algorithm,
but instead of the ranking them,

905
00:54:17,761 --> 00:54:22,761
it's if we set a threshold for base 
level activation finding when it is that

906
00:54:24,380 --> 00:54:29,380
a memory is going to pass that threshold
and try to forget based upon that in a 

907
00:54:29,380 --> 00:54:31,170
way that's efficient,
that isn't going to scale really,

908
00:54:31,171 --> 00:54:36,171
really poorly.
So we were able to come up with an 

909
00:54:36,171 --> 00:54:40,431
efficient way to implement this using an
approximation that ended up for most 

910
00:54:46,110 --> 00:54:48,840
memories to be exactly correct to the 
original.

911
00:54:48,990 --> 00:54:51,990
I'm happy to go over details of this if 
anybody's interested later,

912
00:54:52,320 --> 00:54:54,570
but it ended up being a fairly close 
approximation.

913
00:54:55,020 --> 00:55:00,020
One that as compared to an accurate,
completely accurate search for the value

914
00:55:01,590 --> 00:55:05,340
ended up being somewhere between 15 to 
20 times faster.

915
00:55:06,750 --> 00:55:08,850
And so what we looked at our mobile 
robot here.

916
00:55:09,270 --> 00:55:10,860
Oh,
sorry,

917
00:55:10,890 --> 00:55:14,190
let me get this back because our little 
robots actually going around.

918
00:55:14,191 --> 00:55:19,191
That's the third floor of the computer 
science building at the University of 

919
00:55:19,191 --> 00:55:19,191
Michigan.
He's going around,

920
00:55:19,191 --> 00:55:23,451
he's building a map and again,
the idea was this map is getting too 

921
00:55:23,451 --> 00:55:24,600
big,
so here was the basic idea as the robots

922
00:55:24,601 --> 00:55:29,601
going around,
it's going to need this map information 

923
00:55:29,601 --> 00:55:31,791
about rooms.
The color there is describing kind of 

924
00:55:31,791 --> 00:55:34,311
the strength of the memory and as it 
gets farther and farther away and it 

925
00:55:34,311 --> 00:55:35,970
hasn't used part of the map for planning
or other purposes,

926
00:55:36,060 --> 00:55:39,150
basically make a decay away so that by 
the time it gets to the bottom,

927
00:55:39,300 --> 00:55:40,590
it's forgotten about the top.

928
00:55:41,130 --> 00:55:46,130
But we had the belief that we could 
reconstruct portion that map if 

929
00:55:47,071 --> 00:55:52,071
necessary.
And so the hypothesis was this would 

930
00:55:52,071 --> 00:55:54,330
take care of our speed problems and so 
what we looked at was,

931
00:55:54,331 --> 00:55:57,210
here's our 50 millisecond thresholds.
If we do know,

932
00:55:57,211 --> 00:56:00,210
forgetting whatsoever,
bad things were happening over time.

933
00:56:00,510 --> 00:56:04,080
So just a 3,600
seconds,

934
00:56:04,560 --> 00:56:07,470
this isn't a very long time,
we're passing that threshold.

935
00:56:07,471 --> 00:56:11,220
This is dangerous for the robot.
If we implemented task specific,

936
00:56:11,221 --> 00:56:14,010
basically cleanup rules,
which is really hard to get right,

937
00:56:14,490 --> 00:56:19,490
that basically solved the problem when 
we looked at our general forgetting 

938
00:56:19,490 --> 00:56:22,260
mechanism that we're using in other 
places at an appropriate level of decay.

939
00:56:22,440 --> 00:56:24,640
We were actually better than hand tune 
rules.

940
00:56:25,030 --> 00:56:27,310
So this was kind of a surprise when for 
us,

941
00:56:28,880 --> 00:56:31,300
uh,
the other task seems totally unrelated.

942
00:56:31,301 --> 00:56:34,060
It's a dice game.
You cover your Dayas,

943
00:56:34,061 --> 00:56:37,300
you make bids about what are under other
people's cups.

944
00:56:37,510 --> 00:56:42,510
This is played in pirates of the 
Caribbean when they're on the boat and 

945
00:56:42,510 --> 00:56:43,120
the second movie and bidding for lives 
of service.

946
00:56:43,270 --> 00:56:48,270
Honestly,
this is a game we love to play in the 

947
00:56:48,270 --> 00:56:48,270
University of Michigan lab.
Uh,

948
00:56:48,270 --> 00:56:48,460
and so we're like,
hm,

949
00:56:48,490 --> 00:56:49,900
could soar,
play this?

950
00:56:50,230 --> 00:56:54,460
And so we built a system that could 
learn to play this game rather well with

951
00:56:54,461 --> 00:56:59,461
reinforcement learning.
And so the basic idea was in a 

952
00:56:59,461 --> 00:57:01,720
particular state of the game store would
have options of actions to perform,

953
00:57:02,020 --> 00:57:06,070
it could construct a estimates of their 
associated value,

954
00:57:06,090 --> 00:57:08,800
it would choose one of those,
and depending on the outcome,

955
00:57:08,950 --> 00:57:11,230
something good happened,
you might update that value.

956
00:57:11,620 --> 00:57:14,660
And the big problem was that the size of
the state space,

957
00:57:14,670 --> 00:57:18,730
the number of possible states and 
actions just is enormous.

958
00:57:19,240 --> 00:57:21,850
And so memory was blowing up.
And so what we said,

959
00:57:21,910 --> 00:57:25,810
similar sort of hypothesis,
if we decay away,

960
00:57:26,170 --> 00:57:31,170
these estimates that we could probably 
reconstruct and we haven't used in 

961
00:57:31,170 --> 00:57:31,690
awhile,
are things going to get better?

962
00:57:33,070 --> 00:57:36,610
And so if we don't forget it all,
40,000

963
00:57:36,611 --> 00:57:38,920
games isn't a whole lot when it comes to
reinforcement learning.

964
00:57:39,190 --> 00:57:44,190
We were up at two gigs.
We wanted to put this on an iphone that 

965
00:57:44,190 --> 00:57:47,971
wasn't gonna work.
So well there had been prior work that 

966
00:57:48,850 --> 00:57:53,350
had used a similar approach.
They were down at four or 500 megs.

967
00:57:53,470 --> 00:57:56,370
The iphone is not going to be happy,
but it'll work.

968
00:57:56,800 --> 00:57:57,510
Uh,
so that,

969
00:57:57,511 --> 00:58:00,580
that gave us some hope and we 
implemented our system.

970
00:58:01,130 --> 00:58:02,350
Okay.
We're somewhere in the middle.

971
00:58:02,351 --> 00:58:05,110
We can fit on the iphone.
A very good iphone,

972
00:58:05,140 --> 00:58:08,350
maybe an ipad.
The question was though,

973
00:58:08,620 --> 00:58:10,090
a one.
Efficiency.

974
00:58:10,150 --> 00:58:11,230
Yeah,
we,

975
00:58:11,310 --> 00:58:13,480
we fit under a 50 milliseconds,
but two,

976
00:58:13,510 --> 00:58:15,970
how does the system actually performed 
when you start forgetting stuff,

977
00:58:16,360 --> 00:58:20,230
can it learn to play well?
And so y axis here,

978
00:58:20,231 --> 00:58:22,990
you're seeing Qa competency,
you play a thousand games.

979
00:58:22,991 --> 00:58:24,810
How many do you win?
So the bottom here,

980
00:58:24,820 --> 00:58:29,820
500.
That's flipping a coin whether or not 

981
00:58:29,820 --> 00:58:32,530
you're going to win a.
If we do know for getting whatsoever,

982
00:58:32,590 --> 00:58:35,550
this is a pretty good system.
Uh,

983
00:58:36,280 --> 00:58:41,280
the prior work,
while keeping the memory low is also 

984
00:58:41,280 --> 00:58:44,401
suffering with respect to how well it 
was playing the game and kind of cool 

985
00:58:45,041 --> 00:58:50,041
was the system that was basically more 
than having the memory requirement was 

986
00:58:50,041 --> 00:58:53,080
still performing at the level of no 
forgetting whatsoever.

987
00:58:55,360 --> 00:59:00,360
So just to bring back why I went through
the story was we had a problem.

988
00:59:00,550 --> 00:59:03,700
We look to our example of human level 
ai,

989
00:59:03,701 --> 00:59:06,340
which is humans themselves.
We took an idea,

990
00:59:06,550 --> 00:59:11,550
it turned out to be beneficial,
we found in a efficient implementations 

991
00:59:11,550 --> 00:59:15,571
and then found it was useful in other 
parts of the architecture and other 

992
00:59:15,571 --> 00:59:15,571
tasks that didn't seem to relate 
whatsoever.

993
00:59:16,450 --> 00:59:21,450
But if you download sore right now,
you would gain access to all these 

994
00:59:21,450 --> 00:59:22,670
mechanisms for whatever task you want it
to perform.

995
00:59:24,890 --> 00:59:27,320
Just to give some sense in the field of 
cognitive architecture,

996
00:59:27,321 --> 00:59:32,321
what's some of the open issues are.
I think this is true in a lot of fields 

997
00:59:32,321 --> 00:59:33,200
in ai,
but a integration of systems over time.

998
00:59:33,350 --> 00:59:38,060
The goal was they wouldn't have all 
these theories and uh,

999
00:59:38,090 --> 00:59:39,860
so you could just kind of build over 
time,

1000
00:59:40,250 --> 00:59:42,320
particularly when folks are working on 
different architectures,

1001
00:59:42,321 --> 00:59:43,160
that becomes hard.

1002
00:59:43,540 --> 00:59:48,540
Uh,
but also when you have very different 

1003
00:59:48,540 --> 00:59:48,540
initial starting points,
that can still be an issue.

1004
00:59:48,540 --> 00:59:53,080
Transfer learning is an issue we're 
building into the space of multimodal 

1005
00:59:53,080 --> 00:59:56,981
representations,
which is to say not only abstract 

1006
00:59:56,981 --> 00:59:56,981
symbolic,
but also visual.

1007
00:59:56,981 --> 00:59:59,420
Wouldn't it be nice if we had auditory 
and other senses,

1008
00:59:59,660 --> 01:00:03,770
but building that into memories and 
processing is still an open question.

1009
01:00:04,460 --> 01:00:09,460
There's folks working on metacognition,
which is to say the agent self-assessing

1010
01:00:09,590 --> 01:00:11,600
its own state,
its own processing,

1011
01:00:11,900 --> 01:00:16,900
some work has been done in here but 
still a lot and I think the last one is 

1012
01:00:16,900 --> 01:00:19,490
a really important question for anybody 
taking this kind of class,

1013
01:00:19,700 --> 01:00:22,790
which is what would happen if we did 
succeed,

1014
01:00:22,970 --> 01:00:27,970
if we did make human level ai and if you
don't know that picture right there is 

1015
01:00:28,580 --> 01:00:31,250
from a show that I recommend that you 
watch a,

1016
01:00:31,251 --> 01:00:36,251
it's by the BBC,
it's called humans and it's basically 

1017
01:00:36,251 --> 01:00:37,620
what if we were able to develop what are
called synths in the show.

1018
01:00:37,620 --> 01:00:42,620
I think the,
a robot that can clean up after your 

1019
01:00:42,620 --> 01:00:45,350
laundry and cook and all that good stuff
interact with you if looks and interacts

1020
01:00:45,351 --> 01:00:50,351
as a human but is completely or servant 
and then hilarity and complex issues in 

1021
01:00:51,701 --> 01:00:56,701
sue.
So I highly recommend if you haven't 

1022
01:00:56,701 --> 01:00:57,590
seen that to go watch that.
Uh,

1023
01:00:58,940 --> 01:01:03,940
I think these days there's a lot of 
attention play a paid to machine 

1024
01:01:04,701 --> 01:01:07,130
learning in particular deep learning 
methods as well.

1025
01:01:07,131 --> 01:01:09,290
It should.
They're doing absolutely amazing things.

1026
01:01:09,590 --> 01:01:11,660
Uh,
and often the question is,

1027
01:01:11,661 --> 01:01:16,661
well,
you're doing this and there's deep 

1028
01:01:16,661 --> 01:01:16,661
learning over there,
you know,

1029
01:01:16,661 --> 01:01:21,371
how do they compare?
And I honestly don't feel that that's 

1030
01:01:21,411 --> 01:01:26,411
always a fruitful question because most 
of the time they tend to be working on 

1031
01:01:26,411 --> 01:01:27,480
different problems.
Uh,

1032
01:01:27,890 --> 01:01:30,620
if I'm trying to find objects in the 
scene,

1033
01:01:31,370 --> 01:01:33,210
I'm going to pull out tensor flow.
Uh,

1034
01:01:33,220 --> 01:01:34,750
I'm really not going to pull out store.

1035
01:01:34,760 --> 01:01:38,180
It doesn't make sense.
It's not the right tool for the job that

1036
01:01:38,181 --> 01:01:43,181
haven't been said.
There are times when they tend to work 

1037
01:01:43,181 --> 01:01:43,181
together really,
really well.

1038
01:01:43,181 --> 01:01:45,140
So the rosy system that you saw there,
there was some,

1039
01:01:45,320 --> 01:01:46,590
uh,
uh,

1040
01:01:47,000 --> 01:01:52,000
I believe neural networks being used in 
the object recognition mechanisms for 

1041
01:01:52,000 --> 01:01:54,230
the vision system.
There's td learning going on in terms of

1042
01:01:54,231 --> 01:01:57,200
the dice game where we can pick and 
choose and use this stuff.

1043
01:01:57,201 --> 01:02:02,201
Absolutely great because there are 
problems that are best solved by these 

1044
01:02:02,201 --> 01:02:02,201
methods.
So why avoid it?

1045
01:02:02,201 --> 01:02:03,200
Uh,
and then on the other side,

1046
01:02:03,380 --> 01:02:06,150
if you're trying to develop a system 
where you,

1047
01:02:06,350 --> 01:02:08,120
you know,
in different situations,

1048
01:02:08,121 --> 01:02:10,130
know exactly what you want the system to
do,

1049
01:02:11,150 --> 01:02:14,210
soar or other rule based systems and to 
being the right tool for the right job.

1050
01:02:14,211 --> 01:02:17,540
So absolutely why not make it a piece of
the overall system?

1051
01:02:19,550 --> 01:02:24,550
Uh,
some recommended readings and some 

1052
01:02:24,550 --> 01:02:24,550
venues.
Uh,

1053
01:02:24,550 --> 01:02:24,550
I'd mentioned unified theories of 
cognition.

1054
01:02:24,550 --> 01:02:26,700
This is Harvard press,
I believe.

1055
01:02:27,630 --> 01:02:30,720
Uh,
the cognitive architecture was mit press

1056
01:02:30,750 --> 01:02:35,750
came out in 2012.
I'll say I'm coauthor and a 

1057
01:02:35,970 --> 01:02:40,970
theoretically would get proceeds,
but I've donated them all to the 

1058
01:02:40,970 --> 01:02:44,240
University of Michigan.
So I can just make this recommendation 

1059
01:02:44,240 --> 01:02:44,240
free of ethical concerns.
Personally,

1060
01:02:45,030 --> 01:02:47,910
it's an interesting bug.
It brings together lots of a history and

1061
01:02:47,911 --> 01:02:49,820
lots of new features.
It's if,

1062
01:02:49,880 --> 01:02:53,640
if you're really interested in soar,
it's an easy sell.

1063
01:02:54,740 --> 01:02:57,210
I had mentioned crystallized Smith's how
to build a brain.

1064
01:02:57,510 --> 01:02:59,520
Really cool read,
download the software,

1065
01:02:59,530 --> 01:03:01,350
go through tutorials.
It's really great.

1066
01:03:02,130 --> 01:03:07,130
How can the human mind and occur in the 
physical universe is one of the core 

1067
01:03:07,130 --> 01:03:07,350
act,
our books.

1068
01:03:07,680 --> 01:03:12,680
So it talks through a lot of the 
psychological underpinnings and how the 

1069
01:03:12,680 --> 01:03:12,770
architecture works.
It's,

1070
01:03:12,780 --> 01:03:15,790
it's a fascinating read,
uh,

1071
01:03:15,870 --> 01:03:19,260
one of the papers trying to remember 
what year 2008.

1072
01:03:20,700 --> 01:03:23,790
This goes through a lot of different 
architectures in the field.

1073
01:03:23,820 --> 01:03:28,820
It's 10 years old,
but it gives you a good kind of broad 

1074
01:03:28,820 --> 01:03:29,760
sweep if you want something a little 
more recent.

1075
01:03:29,790 --> 01:03:34,790
This is last month's issue of Ai 
magazine completely dedicated to 

1076
01:03:35,131 --> 01:03:37,200
cognitive systems.
Uh,

1077
01:03:37,201 --> 01:03:39,930
so it's a good place to look for this 
sort of stuff.

1078
01:03:40,170 --> 01:03:42,650
In terms of academic venues,
a aaa,

1079
01:03:42,651 --> 01:03:47,651
I often has cognitive systems track.
There's a conference called ICCM 

1080
01:03:47,651 --> 01:03:48,600
international conference on cognitive 
modeling,

1081
01:03:48,880 --> 01:03:53,880
a where you'll see kind of a span from 
biologic all the way up to ai cognitive 

1082
01:03:53,911 --> 01:03:58,911
science or cog psy.
They have a conference as well as the 

1083
01:03:58,911 --> 01:04:01,551
journal.
A ACS has a conference as well as an 

1084
01:04:01,551 --> 01:04:02,240
online journal,
uh,

1085
01:04:02,241 --> 01:04:07,241
advances in cognitive systems.
Cognitive Systems Research is a journal 

1086
01:04:07,241 --> 01:04:09,620
that has a lot of this good stuff.
There's Agi.

1087
01:04:09,660 --> 01:04:13,290
The conference,
Becca is biologically inspired cognitive

1088
01:04:13,291 --> 01:04:18,291
architectures and I had mentioned both.
There's a sore workshop and enact our 

1089
01:04:18,291 --> 01:04:22,761
workshop that go on annually,
so leave it at this.

1090
01:04:23,461 --> 01:04:28,461
There's some contact information there 
and a lot of when I do these days 

1091
01:04:28,461 --> 01:04:31,690
actually involves kind of explainable 
machine learning,

1092
01:04:31,691 --> 01:04:36,691
integrating that with cognitive systems 
as well as a optimization and robotics 

1093
01:04:37,241 --> 01:04:40,510
that scales really well and also 
integrates with cognitive systems.

1094
01:04:40,900 --> 01:04:41,770
So thank you.

1095
01:04:42,250 --> 01:04:47,250
Thank you.
Do you have

1096
01:04:48,660 --> 01:04:53,660
a question?
Please line up to one of these two 

1097
01:04:53,660 --> 01:04:53,660
microphones.
So,

1098
01:04:53,660 --> 01:04:53,970
uh,
what's the,

1099
01:04:54,250 --> 01:04:56,820
what's the main heuristics that you're 
using?

1100
01:04:57,250 --> 01:05:02,250
I'm in soar that they're gonna be 
heuristics at the task level and the 

1101
01:05:02,250 --> 01:05:06,831
agent level or there's the heuristics 
that are built into the architecture to 

1102
01:05:06,831 --> 01:05:11,510
operate efficiently.
So I'll give you a core example that 

1103
01:05:11,510 --> 01:05:14,301
comes into the architecture and it's a 
fun trick that if you're a programmer 

1104
01:05:14,301 --> 01:05:19,230
you could use all the time,
which is a only changes which is to say 

1105
01:05:19,660 --> 01:05:24,660
one of the cool things about [inaudible]
is you can load it up with literally 

1106
01:05:24,660 --> 01:05:27,691
billions of rules.
And I say literally because we've done 

1107
01:05:27,691 --> 01:05:28,240
it and we know that it can turn over 
still in,

1108
01:05:28,241 --> 01:05:31,450
under a millisecond.
And this happens because instead of most

1109
01:05:31,451 --> 01:05:34,360
systems which process all the rules,
we just say,

1110
01:05:34,361 --> 01:05:36,820
well,
anytime anything changes in the world,

1111
01:05:36,910 --> 01:05:38,080
that's what we're going to react to.

1112
01:05:38,410 --> 01:05:40,300
And of course if you look at the 
biological world,

1113
01:05:40,330 --> 01:05:45,330
similar sorts of tricks are being used.
So that's one of the core ones that 

1114
01:05:45,330 --> 01:05:46,990
actually permeates a multiple of the 
mechanisms.

1115
01:05:47,210 --> 01:05:49,960
Uh,
when it comes to individual tasks,

1116
01:05:51,300 --> 01:05:54,090
it really is task specific,
what that is.

1117
01:05:54,091 --> 01:05:57,450
So for instance,
with the liars dice game,

1118
01:05:57,900 --> 01:06:02,900
if you were to go and download it,
when you're setting the level of 

1119
01:06:02,900 --> 01:06:07,161
difficulty of it,
what you're basically selecting is the 

1120
01:06:07,161 --> 01:06:07,770
subset of sharistics that are being 
applied.

1121
01:06:07,890 --> 01:06:12,750
And it starts very simply with things 
like if I see lots of sixes,

1122
01:06:12,870 --> 01:06:16,110
then I'm likely to believe a high number
of six exist.

1123
01:06:16,200 --> 01:06:18,600
But if I don't,
they're probably not there at all.

1124
01:06:18,870 --> 01:06:23,870
So it's a start.
But any Bayesian wouldn't really buy 

1125
01:06:23,870 --> 01:06:27,711
that argument.
So then you start tacking on a little 

1126
01:06:27,711 --> 01:06:31,371
bit of probabilistic calculation and 
then attacks on some history of prior 

1127
01:06:31,371 --> 01:06:36,021
actions of the agents.
So it really just builds now the rosy 

1128
01:06:36,331 --> 01:06:41,331
system.
One of the cool things they're doing is 

1129
01:06:41,331 --> 01:06:43,161
game learning and specifically having 
the agent be able to accept by a text 

1130
01:06:45,500 --> 01:06:50,500
like natural text,
a heuristics about how to play the game 

1131
01:06:50,941 --> 01:06:52,350
even when it's not sure what to do.

1132
01:06:52,860 --> 01:06:53,880
Oh,
so you.

1133
01:06:53,940 --> 01:06:56,820
At one point you mentioned about 
generating new rules.

1134
01:06:57,660 --> 01:06:59,580
So I'm wondering like how do you do 
that?

1135
01:06:59,670 --> 01:07:01,280
So okay,
so I'm.

1136
01:07:01,800 --> 01:07:04,230
The first thing that comes to my mind 
are local search methods.

1137
01:07:04,320 --> 01:07:05,270
Okay.
So

1138
01:07:06,170 --> 01:07:11,170
one thing is you can actually implement 
heuristic search in rules in the system 

1139
01:07:11,170 --> 01:07:12,770
and that's actually how the robot 
navigates itself.

1140
01:07:12,771 --> 01:07:17,771
So it does heuristic search,
but at the level of rules generate new 

1141
01:07:17,771 --> 01:07:22,121
rules.
That chunking mechanism says the 

1142
01:07:22,121 --> 01:07:24,731
following,
if it's the case that in order to solve 

1143
01:07:24,731 --> 01:07:27,911
a problem you had to kind of sub goal 
and do some other work and you figure 

1144
01:07:27,911 --> 01:07:30,680
out how to solve all that work and 
you've got a result then,

1145
01:07:30,820 --> 01:07:35,820
and I'm greatly oversimplifying,
but if you ever were in the same 

1146
01:07:35,820 --> 01:07:39,131
situation again,
why don't I just memorize the solution 

1147
01:07:39,131 --> 01:07:43,151
for that same situation.
So it basically learns over all the sub 

1148
01:07:43,151 --> 01:07:48,101
processing that was done and encodes the
situation that was in those conditions 

1149
01:07:48,101 --> 01:07:49,490
and the results that were produced as 
action.

1150
01:07:49,610 --> 01:07:51,680
And that's the new rule.
Alright,

1151
01:07:51,710 --> 01:07:51,950
thank you.

1152
01:07:53,540 --> 01:07:57,160
I'm high.
So deep learning and neural networks.

1153
01:07:57,190 --> 01:08:02,190
So it looks at it as a bit of an 
impedance mismatch between your system 

1154
01:08:02,190 --> 01:08:06,241
and those types of system because you've
got a fixed kind of memory architecture 

1155
01:08:06,241 --> 01:08:10,711
and they've got the memory and the wills
all kind of mixed together into one 

1156
01:08:10,711 --> 01:08:13,441
system.
But could you interface your system or 

1157
01:08:13,441 --> 01:08:15,980
sort of like system with deep learning 
by playing in deep learning agents as

1158
01:08:15,980 --> 01:08:20,980
rules in your system.
So it would have to have some local 

1159
01:08:20,980 --> 01:08:23,471
memory but some reason you can't plug in
deep learning as a kind of a rule like 

1160
01:08:24,441 --> 01:08:29,441
module.
So I'm going to answer this.

1161
01:08:29,511 --> 01:08:32,460
You work on it is has there been any 
work on that or.

1162
01:08:32,960 --> 01:08:36,410
Yeah,
so I'll answer it at multiple levels.

1163
01:08:36,470 --> 01:08:41,470
One is you are writing a system and you 
want to use both of these things.

1164
01:08:41,811 --> 01:08:46,811
How do you make them talk and there is 
an Api that you can interface with any 

1165
01:08:46,821 --> 01:08:49,550
environment in any set of tools and if 
people are earning is one of them.

1166
01:08:49,551 --> 01:08:51,200
Great.
And if so is the other one.

1167
01:08:51,201 --> 01:08:56,201
Cool.
Do you have no problem and you can do 

1168
01:08:56,201 --> 01:08:57,851
that today.
And we have done this numerous times in 

1169
01:08:57,851 --> 01:08:57,851
terms of integration into the 
architecture.

1170
01:08:58,670 --> 01:09:03,670
All we have to do is think of a sub 
problem in which all over simplify this,

1171
01:09:06,471 --> 01:09:08,540
but basically function approximation is 
useful.

1172
01:09:08,690 --> 01:09:13,690
I'm seeing basically kind of the,
a fixed structure of input.

1173
01:09:14,780 --> 01:09:19,780
I'm getting feedback as to the output 
and I want to learn the mapping to that 

1174
01:09:19,780 --> 01:09:20,900
over time.
If you can make that case,

1175
01:09:21,170 --> 01:09:23,570
then you integrate it as a part of the 
module.

1176
01:09:23,960 --> 01:09:24,970
Great.
Uh,

1177
01:09:24,971 --> 01:09:29,971
and we have learning mechanisms that do 
some of that deep learning just hasn't 

1178
01:09:30,290 --> 01:09:33,380
been used to my knowledge to solve any 
of those sub problems.

1179
01:09:33,381 --> 01:09:36,230
There's nothing keeping it from being 
one of those,

1180
01:09:36,380 --> 01:09:40,940
particularly when it comes down to the 
low level visual part of things,

1181
01:09:42,340 --> 01:09:47,340
a problem that arises.
So I'll say what actually makes some of 

1182
01:09:48,321 --> 01:09:49,650
this difficult,
uh,

1183
01:09:49,670 --> 01:09:51,770
and it's a general problem called symbol
grounding.

1184
01:09:52,550 --> 01:09:56,450
So at the level of what most,
what happens mostly in store,

1185
01:09:56,451 --> 01:10:01,451
it is symbols being manipulated in a 
highly discreet way and so how do you 

1186
01:10:02,541 --> 01:10:07,541
get yourself from pixels and low level 
non symbolic representations to 

1187
01:10:07,541 --> 01:10:12,461
something that's stable and discreet and
can be manipulated and that is 

1188
01:10:12,770 --> 01:10:17,770
absolutely an open question in that 
community and that will make things 

1189
01:10:17,770 --> 01:10:17,770
hard.

1190
01:10:17,770 --> 01:10:22,480
So spawn actually has an interesting 
answer to that and it has a distributed 

1191
01:10:22,480 --> 01:10:27,071
representation and it operates over 
distributed representations in what 

1192
01:10:27,071 --> 01:10:30,530
might feel like a symbolic way.
So they're kind of ahead of us on that,

1193
01:10:30,890 --> 01:10:35,890
but they're,
they're starting from a lower point and 

1194
01:10:35,890 --> 01:10:38,621
so they've dealt with some of these 
issues and they have a pretty good 

1195
01:10:38,621 --> 01:10:38,621
answer to that and that's how they're 
moving up.

1196
01:10:38,621 --> 01:10:41,600
And that's also why I showed sigma,
which is at its low level,

1197
01:10:41,660 --> 01:10:46,660
it's message passing algorithms.
It's implementing things like a slam and

1198
01:10:47,360 --> 01:10:52,360
sat solving and other sorts of really,
really it can implement those on very 

1199
01:10:52,360 --> 01:10:55,580
low level primitives but higher up it 
can also be doing what soar is doing.

1200
01:10:55,610 --> 01:10:57,790
So there's an answer there as well.
Okay.

1201
01:10:57,800 --> 01:10:58,330
Thank you.
Um,

1202
01:10:58,340 --> 01:11:00,070
so another way of doing it would be to 
layer the,

1203
01:11:00,240 --> 01:11:01,360
um,
the system,

1204
01:11:01,370 --> 01:11:04,750
so have a system,
preprocessing the,

1205
01:11:05,050 --> 01:11:07,390
the,
the sensory input or post processing and

1206
01:11:07,391 --> 01:11:12,391
then draft and the other one,
that'd be another way of combining the 

1207
01:11:12,391 --> 01:11:12,391
two.

1208
01:11:12,391 --> 01:11:12,500
And that's actually what's going on in 
the rosy system.

1209
01:11:12,501 --> 01:11:17,501
So the detection of objects in the scene
is a Jew just software that somebody 

1210
01:11:17,501 --> 01:11:22,401
wrote.
I don't believe it's the deep learning 

1211
01:11:22,401 --> 01:11:22,740
specifically,
but like the color detection out of it I

1212
01:11:23,190 --> 01:11:28,170
think is an Svm if I'm correct so easily
could be deep learning.

1213
01:11:28,640 --> 01:11:33,640
Thanks.
You mentioned like the importance of 

1214
01:11:33,640 --> 01:11:33,990
forgetting in order to food memory 
issues,

1215
01:11:34,080 --> 01:11:39,080
but you said you could only forget 
because you could reconstruct and I'm 

1216
01:11:39,080 --> 01:11:39,080
curious,
how do you,

1217
01:11:39,080 --> 01:11:40,410
when you said we can show you each know 
that it happened before,

1218
01:11:40,411 --> 01:11:45,411
so do you just compress the data?
Like do you really forget it or okay,

1219
01:11:46,390 --> 01:11:51,390
so and I put quotes up and I said,
you think you can reconstruct it?

1220
01:11:52,050 --> 01:11:57,050
So we came up with approximations of 
this and so let me try to answer this 

1221
01:11:57,511 --> 01:12:02,511
very grounded when it comes to the 
mobile robot and you had rooms that you 

1222
01:12:04,681 --> 01:12:09,681
had been to before the entire map and 
its entirety was being constructed in 

1223
01:12:09,681 --> 01:12:10,890
the robots.

1224
01:12:11,340 --> 01:12:13,080
Semantic memory.
So here's facts.

1225
01:12:13,081 --> 01:12:18,081
This room is connected to this room 
which is connected this room which 

1226
01:12:18,081 --> 01:12:18,510
connected this room.
So we had those sorts of representations

1227
01:12:18,511 --> 01:12:23,511
that existed up in semantic memory.
The rules can only operate down on 

1228
01:12:23,511 --> 01:12:28,371
anything that's in short term memory.
So basically we were removing things 

1229
01:12:28,371 --> 01:12:30,540
from the short term memory and as 
necessary be able to reconstruct it from

1230
01:12:30,541 --> 01:12:35,541
the longterm.
You could end up in some situations in 

1231
01:12:35,541 --> 01:12:37,380
which you had made a change locally in 
short term memory,

1232
01:12:37,560 --> 01:12:42,560
didn't get a chance to get it up and it 
actually happened to be forgotten a way 

1233
01:12:42,560 --> 01:12:46,880
so you weren't guaranteed,
but it was good enough that the 

1234
01:12:46,880 --> 01:12:51,591
connectivity survived.
The agent was able to perform the exact 

1235
01:12:51,591 --> 01:12:52,920
same task and we gained some benefit for
the RL system.

1236
01:12:53,790 --> 01:12:58,590
The rule we came up with was the initial
estimates in the value system,

1237
01:12:58,591 --> 01:13:00,150
which is,
here's how good I think that is,

1238
01:13:00,180 --> 01:13:03,240
that's based on the heuristics I 
described earlier.

1239
01:13:03,300 --> 01:13:05,790
Some simple probabilistic calculations 
of counting some stuff.

1240
01:13:05,791 --> 01:13:10,791
That's where that number came from.
We computer before we could compute it 

1241
01:13:10,791 --> 01:13:13,761
again.
The only time we can't reconstruct it 

1242
01:13:13,761 --> 01:13:15,460
completely is if it had seen a certain 
number of updates over time.

1243
01:13:16,290 --> 01:13:20,550
It's such a large state space.
There are so many actions,

1244
01:13:20,551 --> 01:13:24,960
so many states that most of the states 
were never being seen.

1245
01:13:26,010 --> 01:13:29,270
So most of those could be exactly 
reproduced by,

1246
01:13:29,271 --> 01:13:34,271
uh,
the agent just thinking about it a 

1247
01:13:34,271 --> 01:13:34,271
little bit.
And there was only a tiny,

1248
01:13:34,271 --> 01:13:38,121
tiny,
I'm going to say under one percent of 

1249
01:13:38,121 --> 01:13:38,850
the estimate the value system that ever 
got updates.

1250
01:13:38,940 --> 01:13:43,940
And that's actually not inconsistent 
with a lot of these kinds of problems 

1251
01:13:43,940 --> 01:13:44,430
that have really,
really large state spaces.

1252
01:13:44,820 --> 01:13:46,020
So,
uh,

1253
01:13:46,021 --> 01:13:48,000
I think the statement was something 
like,

1254
01:13:49,110 --> 01:13:52,650
if we had ever updated it,
don't forget it.

1255
01:13:53,820 --> 01:13:57,870
And you saw that was already reducing 
more than half of the memory load.

1256
01:13:58,170 --> 01:14:02,040
We could have something higher to say 10
times something like that.

1257
01:14:02,041 --> 01:14:06,270
And that would say we could reconstruct 
almost all of it.

1258
01:14:06,720 --> 01:14:11,720
The prior work that I referenced was 
strictly saying if it falls below 

1259
01:14:11,720 --> 01:14:16,321
threshold,
no matter how many times they ended up 

1260
01:14:16,321 --> 01:14:17,911
dating,
no matter how much information was 

1261
01:14:17,911 --> 01:14:19,801
there.
And so when we were adding was probably 

1262
01:14:19,801 --> 01:14:22,321
can reconstruct and that was getting us 
the balance between the efficiency and 

1263
01:14:23,260 --> 01:14:24,040
the ability to forget.

1264
01:14:24,270 --> 01:14:25,950
So just in its infancy,
we can probably,

1265
01:14:25,951 --> 01:14:30,951
we can show you state you keep trying to
use generic is the issue needs to be 

1266
01:14:30,951 --> 01:14:30,951
constructed.
Your will,

1267
01:14:30,951 --> 01:14:32,490
you're going to run it again sometime

1268
01:14:33,520 --> 01:14:38,520
fly.
If I get back into that situation and I 

1269
01:14:38,520 --> 01:14:40,531
happen to forget it,
the system knew how to compute it the 

1270
01:14:40,531 --> 01:14:43,951
first time it goes and looks at all the 
hand and just pretends it's in that 

1271
01:14:43,951 --> 01:14:44,020
situation for the very,
very first time.

1272
01:14:44,200 --> 01:14:45,580
Reconstruct that value estimate.

1273
01:14:47,070 --> 01:14:50,290
Just a quick question on top of that 
again,

1274
01:14:50,950 --> 01:14:51,840
question.
Okay.

1275
01:14:52,320 --> 01:14:55,920
So the actual mechanism of forgetting is
fascinating.

1276
01:14:55,921 --> 01:15:00,921
So lstmrs,
rns have mechanisms for learning what to

1277
01:15:01,621 --> 01:15:03,600
forget and whatnot to forget.
Have you,

1278
01:15:04,400 --> 01:15:09,400
has there been any exploration of 
learning the forgetting process says 

1279
01:15:09,670 --> 01:15:14,670
doing something complicated or 
interesting with which parts to forget 

1280
01:15:14,670 --> 01:15:14,670
or not?

1281
01:15:14,670 --> 01:15:19,270
Uh,
the closest I will say was kind of a 

1282
01:15:19,270 --> 01:15:23,050
metacognition project that's 10 or 15 
years old at this point,

1283
01:15:23,380 --> 01:15:28,380
which was what happens when it gets into
a place where it actually knows that it 

1284
01:15:28,380 --> 01:15:33,331
learned something that's harmful to it,
that's leading to poor decisions.

1285
01:15:34,150 --> 01:15:39,150
Uh,
and in that case it was still a very 

1286
01:15:39,150 --> 01:15:41,641
rule based process,
but it wasn't learning to forget he was 

1287
01:15:41,641 --> 01:15:43,330
actually learning to override it's prior
knowledge,

1288
01:15:43,570 --> 01:15:48,220
which might be closer to some of what we
do when we know we have a bad habit.

1289
01:15:48,460 --> 01:15:51,280
We don't have a way of forgetting that 
habit,

1290
01:15:51,281 --> 01:15:56,281
but instead we can try to learn 
something on top of that that leads to 

1291
01:15:56,281 --> 01:15:58,660
better a operation in the future.
To my knowledge that's the only work,

1292
01:15:58,690 --> 01:16:00,130
at least in sore that's been done.

1293
01:16:00,890 --> 01:16:05,890
Just find the topic really fascinating.
What lessons do you think we can draw 

1294
01:16:06,311 --> 01:16:11,311
from the fact that forgetting.
So ultimately you're the action of 

1295
01:16:11,311 --> 01:16:15,130
forgetting a is driven by the fact that 
you want to improve performance,

1296
01:16:15,310 --> 01:16:19,540
but do you think for getting is 
essential for Agi,

1297
01:16:20,200 --> 01:16:24,310
the act of forgetting for building 
systems that operate in as well.

1298
01:16:24,580 --> 01:16:26,110
How important is forgetting?

1299
01:16:26,520 --> 01:16:30,030
I can think of easy answers to that.
So one might be,

1300
01:16:31,890 --> 01:16:33,750
if we take the cognitive modeling 
approach,

1301
01:16:33,810 --> 01:16:38,810
we know humans do forget and we know 
regularities of how humans forget.

1302
01:16:40,080 --> 01:16:45,080
And so whether or not the system itself 
forgets at least has to model the fact 

1303
01:16:45,080 --> 01:16:47,550
that the humans it's interacting with 
are going to forget.

1304
01:16:47,730 --> 01:16:52,730
And so at least it has to have that 
ability to model in order to interact 

1305
01:16:52,730 --> 01:16:56,781
effectively because if it assumes we 
always remember everything and can't 

1306
01:16:56,781 --> 01:16:58,660
operate well in that environment,
uh,

1307
01:16:58,680 --> 01:17:03,680
I think we're going to have a problem is
true for getting going to be necessary.

1308
01:17:09,680 --> 01:17:11,030
That's interesting.
Our,

1309
01:17:11,070 --> 01:17:14,420
our agi system is going to hold a grudge
for all eternity.

1310
01:17:15,230 --> 01:17:19,340
We might want them to forget this early 
age when we were forcing them to work in

1311
01:17:19,341 --> 01:17:21,260
our laboratory.
I think I know what you're trying to.

1312
01:17:21,320 --> 01:17:22,100
Yeah,
exactly.

1313
01:17:22,400 --> 01:17:25,820
Exactly.
And how to build such a system.

1314
01:17:25,850 --> 01:17:26,650
Yeah,
exactly.

1315
01:17:28,640 --> 01:17:28,880
Go ahead.

1316
01:17:29,530 --> 01:17:32,500
So I have two quick,
two quick questions.

1317
01:17:33,010 --> 01:17:38,010
One is,
would you be able to speculate on how 

1318
01:17:38,010 --> 01:17:41,521
you can connect function approximators 
such as deep down works to symbols.

1319
01:17:42,070 --> 01:17:44,350
And the second question is completely 
different.

1320
01:17:44,800 --> 01:17:49,800
This is regarding your action selection.
I know you didn't speak much about that 

1321
01:17:50,350 --> 01:17:54,430
when you have different theories in your
knowledge representation and you have an

1322
01:17:54,431 --> 01:17:59,431
action selection which has to make 
constructive plan by reasoning about the

1323
01:17:59,651 --> 01:18:04,651
different theories and the different 
pieces of knowledge that are now held 

1324
01:18:04,651 --> 01:18:04,680
within a,
you know,

1325
01:18:04,810 --> 01:18:06,570
your,
your memory or anything like that.

1326
01:18:06,610 --> 01:18:11,610
All your rules.
What kind of algorithms to use in the 

1327
01:18:11,610 --> 01:18:12,340
action selection to come up with a 
plant,

1328
01:18:12,370 --> 01:18:17,370
you know,
is there any concept of differentiation 

1329
01:18:17,370 --> 01:18:19,741
of the symbols or you know,
or grammars or admissible grammars and 

1330
01:18:19,741 --> 01:18:20,680
things like that that you use in action 
selection?

1331
01:18:21,290 --> 01:18:26,290
I'm actually going to answer the second 
question first and then you're going to 

1332
01:18:26,290 --> 01:18:27,670
have to remind me of what the first one 
was.

1333
01:18:27,980 --> 01:18:28,600
Uh,
when I,

1334
01:18:28,601 --> 01:18:31,510
when I get to the end.
So the action selection mechanism,

1335
01:18:32,020 --> 01:18:35,080
one of these core tenants I said is it's
got to get through this cycle fast.

1336
01:18:35,200 --> 01:18:38,140
So everything that's really,
really built in has to be really,

1337
01:18:38,141 --> 01:18:43,141
really simple.
And so the decision procedure is 

1338
01:18:43,141 --> 01:18:43,141
actually really,
really simple.

1339
01:18:43,141 --> 01:18:46,030
It says the rules are going to fire,
the rules are going,

1340
01:18:46,230 --> 01:18:51,230
the production rules are going to fire,
and there's gonna be a subset of them 

1341
01:18:51,230 --> 01:18:52,810
that will say something like,
here's an operator that you could select

1342
01:18:53,020 --> 01:18:58,020
to.
These are of acceptable operator 

1343
01:18:58,020 --> 01:18:58,020
preferences.
There are ones are gonna say,

1344
01:18:58,020 --> 01:19:01,861
well,
based upon the fact that you said that 

1345
01:19:01,861 --> 01:19:01,861
that was acceptable,
I think it's the best thing,

1346
01:19:01,861 --> 01:19:04,150
or the worst thing,
or I think 50 50 chance I'm going to get

1347
01:19:04,151 --> 01:19:05,080
reward out of this.

1348
01:19:05,230 --> 01:19:10,230
There's actually a fixed language of 
preferences that are being asserted and 

1349
01:19:10,230 --> 01:19:14,131
actually a nice fixed procedure by which
if I have a set of preferences to make a

1350
01:19:15,431 --> 01:19:20,431
very quick and clean decision.
So what's basically happened is you've 

1351
01:19:20,431 --> 01:19:24,481
pushed the hard questions of how to make
complex decisions about actions up to a 

1352
01:19:25,481 --> 01:19:30,481
higher level.
The low level architecture is always 

1353
01:19:30,481 --> 01:19:33,960
given a set of options going to be able 
to make a relatively quick decision and 

1354
01:19:34,031 --> 01:19:39,031
it gets pushed into the knowledge of the
agent to construct a sequence of 

1355
01:19:40,271 --> 01:19:43,080
decisions that overtime is going to get 
to.

1356
01:19:43,081 --> 01:19:44,290
The more interesting questions you're 
talking,

1357
01:19:44,590 --> 01:19:49,590
but how can you reason that that 
sequence will take you to the goal that 

1358
01:19:49,590 --> 01:19:49,590
you desire?

1359
01:19:49,590 --> 01:19:50,520
So

1360
01:19:51,610 --> 01:19:53,170
is there any guarantee on that?
Is that.

1361
01:19:53,260 --> 01:19:53,380
Yeah.

1362
01:19:53,710 --> 01:19:58,300
Uh,
in general across tasks.

1363
01:19:58,301 --> 01:20:01,390
No,
but people have,

1364
01:20:01,391 --> 01:20:06,391
for instance,
implemented a star I was mentioning as 

1365
01:20:06,391 --> 01:20:06,391
rules,
right?

1366
01:20:06,391 --> 01:20:10,380
Yeah.
So I know given certain properties about

1367
01:20:10,381 --> 01:20:15,381
the search task,
that task that's being searched based 

1368
01:20:15,381 --> 01:20:15,870
upon these rules,
given a finite search space,

1369
01:20:16,080 --> 01:20:19,500
eventually it will get there.
And if I have a good heuristic in there,

1370
01:20:19,501 --> 01:20:21,300
I know certain properties about the 
optimality.

1371
01:20:22,050 --> 01:20:24,390
So I can reason at that level.
In general,

1372
01:20:24,391 --> 01:20:29,391
I think this comes back to the 
assumption I made earlier about bounded 

1373
01:20:29,391 --> 01:20:32,061
rationality to say parts of the 
architecture or solving sub-problems 

1374
01:20:32,310 --> 01:20:37,310
optimally.
The general problems that it's going to 

1375
01:20:37,310 --> 01:20:40,281
work on.
It's going to try its best based upon 

1376
01:20:40,281 --> 01:20:43,281
the knowledge that it has.
And that's about the end of guarantees 

1377
01:20:43,281 --> 01:20:43,340
that you can typically make any 
architecture.

1378
01:20:43,540 --> 01:20:46,980
Okay.
I think your first question was

1379
01:20:47,060 --> 01:20:52,060
speculate on connecting symbol,
a product function approximators a 

1380
01:20:52,700 --> 01:20:57,700
multiple layer function approximators 
like deep learning networks to do 

1381
01:20:57,980 --> 01:21:00,470
symbols that you can reason about at a 
higher level.

1382
01:21:00,980 --> 01:21:01,250
Yeah,

1383
01:21:02,430 --> 01:21:05,250
I think that's a great open space.
If I had time,

1384
01:21:05,251 --> 01:21:06,810
this will be somebody I'll be working on
right now,

1385
01:21:06,811 --> 01:21:11,811
which is somewhere before I basically 
said taking a scene and then detecting 

1386
01:21:13,741 --> 01:21:18,741
objects out of that scene and using 
those as simple as and reading about 

1387
01:21:18,741 --> 01:21:22,011
those over time.
I think the spawn work is quite 

1388
01:21:22,011 --> 01:21:22,680
interesting.
So

1389
01:21:24,110 --> 01:21:29,110
the symbols that they're operating on 
are actually,

1390
01:21:29,530 --> 01:21:34,530
uh,
a distributed representation of the 

1391
01:21:34,530 --> 01:21:37,991
input space.
And the closest I can get to this is if 

1392
01:21:37,991 --> 01:21:40,580
you've seen a word tobacco where you're 
taking a language corpus and what you're

1393
01:21:40,581 --> 01:21:43,610
getting out of there as a vector of 
numbers that has certain properties,

1394
01:21:43,760 --> 01:21:46,940
but it's also a vector that you could 
operate on as a unit.

1395
01:21:47,330 --> 01:21:52,330
So it has nice properties.
You can operate with it on other 

1396
01:21:52,330 --> 01:21:56,111
vectors.
You know that if I got the same word in 

1397
01:21:56,111 --> 01:21:59,920
the same context,
I would get back to that exact same 

1398
01:21:59,920 --> 01:22:03,220
vector,
so those are the kind of representation 

1399
01:22:03,220 --> 01:22:06,341
that seems like it's going to be able to
bridge that chasm where we can get from 

1400
01:22:06,590 --> 01:22:11,450
sensory information to something that 
can be operated on and reasoned about in

1401
01:22:11,451 --> 01:22:16,070
this sort of symbolic architecture and 
get us from there.

1402
01:22:16,071 --> 01:22:18,140
From actual sensory information.

1403
01:22:22,090 --> 01:22:27,090
I had a question.
What do you think are the biggest 

1404
01:22:27,090 --> 01:22:29,740
strengths of the cognitive architecture 
approach compared to other approaches in

1405
01:22:29,741 --> 01:22:33,010
artificial intelligence?
And the flip side of that,

1406
01:22:33,040 --> 01:22:38,040
what do you think are the biggest 
shortcomings of cognitive architecture 

1407
01:22:38,040 --> 01:22:38,380
with respect to us

1408
01:22:39,490 --> 01:22:43,710
actually you being humans,
human level,

1409
01:22:43,711 --> 01:22:48,711
like like what needs to be like?
How come cognitive architecture has not 

1410
01:22:48,711 --> 01:22:53,001
solved agi because we want job security.
That's the answer.

1411
01:22:54,150 --> 01:22:59,150
We've totally solved it already.
So strength I think conceptually is 

1412
01:23:02,970 --> 01:23:07,970
keeping an eye on the ball,
which is if what you're looking at is 

1413
01:23:07,970 --> 01:23:08,320
trying to make human level ai

1414
01:23:10,130 --> 01:23:10,280
I.

1415
01:23:11,660 --> 01:23:13,550
it's hard.
It's challenging.

1416
01:23:13,551 --> 01:23:18,551
It's ambitious to say that's the goal 
because for decades we haven't done it.

1417
01:23:20,090 --> 01:23:22,220
It's extraordinarily hard.
It,

1418
01:23:24,020 --> 01:23:29,020
it is less difficult in some ways to 
constrain yourself down to a single 

1419
01:23:29,020 --> 01:23:33,881
problem that hadn't been said.
I'm not very good at making a car drive 

1420
01:23:34,551 --> 01:23:37,520
itself.
In some ways that's a simpler problem.

1421
01:23:38,380 --> 01:23:43,380
It's great at challenging and of itself 
and it will have great impact on 

1422
01:23:43,380 --> 01:23:43,970
humanity.
It's a great problem to work on.

1423
01:23:44,750 --> 01:23:46,340
Human level.
Ai is huge.

1424
01:23:46,370 --> 01:23:50,720
It's not even well defined as a problem.
And so,

1425
01:23:52,040 --> 01:23:52,960
uh,
what,

1426
01:23:52,970 --> 01:23:56,540
what's the strength here?
Bravery.

1427
01:23:56,690 --> 01:24:01,690
Stupidity in the face of failure,
a resilience over time,

1428
01:24:04,040 --> 01:24:09,020
keeping alive.
This idea of trying to reproduce a level

1429
01:24:09,021 --> 01:24:11,000
of human intelligence that's more 
general.

1430
01:24:11,870 --> 01:24:13,670
I don't know if that's a very 
satisfactory answer for you.

1431
01:24:15,140 --> 01:24:16,310
Downside

1432
01:24:18,600 --> 01:24:23,600
home runs are fairly rare and by Homerun
I mean a system that finds its way to 

1433
01:24:26,610 --> 01:24:29,580
the,
the general populace to the marketplace.

1434
01:24:30,510 --> 01:24:33,700
I'd mentioned Bonnie John specifically 
because you know,

1435
01:24:33,701 --> 01:24:38,701
this is 20,
30 years of research and then she found 

1436
01:24:38,701 --> 01:24:39,930
a way that actually makes a whole lot of
sense under direct application.

1437
01:24:39,931 --> 01:24:42,120
So it was a lot,
a lot of years of basic research,

1438
01:24:42,121 --> 01:24:44,100
a lot of researchers.
And then there was,

1439
01:24:44,130 --> 01:24:47,220
there was a big win,
there was this one,

1440
01:24:47,400 --> 01:24:50,220
oh,
this was a Bonnie,

1441
01:24:50,221 --> 01:24:53,010
John was a researcher,
or this was using act,

1442
01:24:53,011 --> 01:24:58,011
our models of eye gaze and reaction and 
so forth to be able to make predictions 

1443
01:24:59,940 --> 01:25:04,650
about how humans would use a user 
interfaces.

1444
01:25:06,990 --> 01:25:11,990
So those sorts of outcomes are rare.
It if you work in ai,

1445
01:25:13,260 --> 01:25:15,930
one of the first things you learn about 
his blocks world,

1446
01:25:16,410 --> 01:25:21,410
it's kind of in the classic Ai Textbook.
I will tell you I've worked on that 

1447
01:25:22,711 --> 01:25:24,420
problem in about three different 
variants.

1448
01:25:24,421 --> 01:25:29,421
I've gone to many conferences where 
presentations have been made about 

1449
01:25:29,421 --> 01:25:31,680
blocks world,
which is to say we're good.

1450
01:25:31,681 --> 01:25:35,450
Progress is being made,
but the way you end up thinking about is

1451
01:25:35,460 --> 01:25:38,340
in really,
really small constrained problems.

1452
01:25:38,341 --> 01:25:41,970
Ironically you have this big vision,
but in order to make progress,

1453
01:25:41,971 --> 01:25:45,150
that ends up being on moving blocks on a
table.

1454
01:25:45,570 --> 01:25:49,580
And so it's.
It's a big challenge.

1455
01:25:49,740 --> 01:25:54,740
I just think it'll take a lot of time.
The I'll say the other thing they 

1456
01:25:54,910 --> 01:25:57,090
haven't,
we haven't really gotten too.

1457
01:25:57,091 --> 01:26:01,520
Although I brought up spawn and I 
brought up a sigma,

1458
01:26:02,160 --> 01:26:04,160
an idea of how scale this thing,

1459
01:26:05,390 --> 01:26:10,390
something I like about deep learning is 
to some extent with lots of asterisks 

1460
01:26:10,401 --> 01:26:11,690
and 10,000
foot view,

1461
01:26:11,691 --> 01:26:12,770
it's kind of like,
well,

1462
01:26:13,040 --> 01:26:14,780
we've gotten this far.
All right,

1463
01:26:14,781 --> 01:26:16,880
let's just provide a different inputs,
different outputs,

1464
01:26:16,881 --> 01:26:21,881
and we'll have some tricks on the middle
and suddenly you have end to end deep 

1465
01:26:21,881 --> 01:26:25,241
learning,
but a bigger problem and a bigger 

1466
01:26:25,241 --> 01:26:26,441
problem there.
There's a way to see how this expands 

1467
01:26:26,441 --> 01:26:29,231
given enough data,
given her enough computing and 

1468
01:26:29,231 --> 01:26:30,530
incremental advances.
When it comes to sore,

1469
01:26:30,560 --> 01:26:35,560
it takes not only a big idea,
but it takes a lot of software 

1470
01:26:35,560 --> 01:26:39,521
engineering to integrate it.
There's a lot of constraints built into 

1471
01:26:39,521 --> 01:26:39,521
it.
It,

1472
01:26:39,521 --> 01:26:42,440
it slows it down.
So something like sigma is a,

1473
01:26:42,530 --> 01:26:45,890
Oh well I can change a little bit of the
configuration of the graph.

1474
01:26:45,891 --> 01:26:48,230
I can use variance on the algorithm.
Boom.

1475
01:26:48,350 --> 01:26:50,600
It's integrated like an experiment 
fairly quickly.

1476
01:26:50,900 --> 01:26:55,900
So starting with that sort of 
infrastructure does not give you the 

1477
01:26:55,900 --> 01:27:00,831
constraint.
You kind of want with your big picture 

1478
01:27:00,831 --> 01:27:03,501
vision of going towards human level ai,
but in terms of being able to be agile 

1479
01:27:03,501 --> 01:27:04,020
in your research,
it's,

1480
01:27:04,021 --> 01:27:04,800
it's kind of incredible.

1481
01:27:05,180 --> 01:27:06,420
I see.
Thank you.

1482
01:27:07,380 --> 01:27:08,280
A couple more.

1483
01:27:09,660 --> 01:27:12,680
You had mentioned that ideas such as 
[inaudible] Kate,

1484
01:27:12,910 --> 01:27:17,910
these techniques,
they were based on the original 

1485
01:27:17,910 --> 01:27:20,471
inspirations were based off of a human 
cognition and because humans can't 

1486
01:27:20,471 --> 01:27:23,960
remember everything.
So were there any instances of the other

1487
01:27:23,961 --> 01:27:28,850
way round where some discovery in 
cognitive bottling fueled it?

1488
01:27:28,910 --> 01:27:30,890
Another discovery in cognitive science?

1489
01:27:32,310 --> 01:27:35,310
Uh,
so one thing I'm gonna

1490
01:27:36,150 --> 01:27:41,150
went out and your question was based 
indicated with respect to human 

1491
01:27:41,150 --> 01:27:44,751
cognition.
The study actually was let's look at 

1492
01:27:44,751 --> 01:27:47,871
text and properties of text and use that
to then make predictions about what must

1493
01:27:51,031 --> 01:27:56,031
be true about human cognition.
So John Anderson and the other 

1494
01:27:56,251 --> 01:28:01,251
researchers looked at,
I believe it was New York Times 

1495
01:28:01,251 --> 01:28:01,620
articles,

1496
01:28:03,980 --> 01:28:08,980
his own John Anderson's emails,
and I'm trying to remember what the 

1497
01:28:08,980 --> 01:28:13,661
third,
I think it was parents utterances with 

1498
01:28:13,661 --> 01:28:18,431
their kids or something like this.
It was actually looking at text corpora 

1499
01:28:18,431 --> 01:28:22,090
and the words that were occurring in a 
varying frequencies that,

1500
01:28:24,650 --> 01:28:29,650
that analysis,
that rational analysis actually lead to 

1501
01:28:29,650 --> 01:28:33,310
models that got integrated within the 
act art architecture that then became 

1502
01:28:34,641 --> 01:28:39,641
validated through multiple trials that 
then became validated with respect to 

1503
01:28:39,641 --> 01:28:43,211
Mri scans and is now being used to both 
do study back with humans,

1504
01:28:44,691 --> 01:28:48,320
but also develop systems that interact 
well with humans.

1505
01:28:48,620 --> 01:28:52,580
So I think that in and of itself ends up
being an example to cheat.

1506
01:28:52,581 --> 01:28:57,581
But the,
uh,

1507
01:28:59,480 --> 01:29:02,490
the UAV,
the sor UAV system,

1508
01:29:02,520 --> 01:29:06,600
I believe is a single robot that has a 
multi,

1509
01:29:06,630 --> 01:29:10,670
multiple agents running on it.
So where is this?

1510
01:29:12,840 --> 01:29:14,690
I got it off your website.
Okay.

1511
01:29:14,940 --> 01:29:18,320
But either way,
your systems allow for multi agents.

1512
01:29:18,380 --> 01:29:19,040
Okay.
Uh,

1513
01:29:19,120 --> 01:29:24,120
so my question is how are you preventing
them from converging with new data and 

1514
01:29:24,661 --> 01:29:29,661
are you changing what they're forgetting
selectively as one of those ways?

1515
01:29:30,600 --> 01:29:35,600
So I'll say yes,
you can have multiple source systems on 

1516
01:29:35,600 --> 01:29:39,171
a single system or multiple systems.
There's not any real strong theory that 

1517
01:29:41,341 --> 01:29:45,110
relates to multi agent system.
So there's no real constraint there that

1518
01:29:45,300 --> 01:29:47,670
you can come up with a protocol for them
interacting.

1519
01:29:48,810 --> 01:29:52,500
Each one is going to have its own set of
memories,

1520
01:29:52,501 --> 01:29:57,501
set of knowledge.
There really is no constraint on you 

1521
01:29:57,501 --> 01:30:01,131
being able to communicate like you would
if it were any other system interacting 

1522
01:30:01,131 --> 01:30:05,811
with sore.
So I don't really think I have a great 

1523
01:30:05,811 --> 01:30:05,811
answer for it.
Okay.

1524
01:30:06,660 --> 01:30:10,530
So that is to say if,
if you had good theories,

1525
01:30:10,531 --> 01:30:15,531
good algorithms about how systems work 
and how they can bring knowledge 

1526
01:30:15,961 --> 01:30:20,961
together form of fusion sort of way.
It might be something that you could 

1527
01:30:22,231 --> 01:30:27,231
bring to a multiagent source system,
but there's nothing really there to help

1528
01:30:27,271 --> 01:30:32,271
you.
There's no mechanisms there really to 

1529
01:30:32,271 --> 01:30:32,271
help you do that any better than you 
would otherwise.

1530
01:30:32,271 --> 01:30:36,770
And you would have to kind of 
constraints on your representations of 

1531
01:30:36,811 --> 01:30:41,811
processes to what it has fixed in terms 
of it's sort of memory and it's sort of 

1532
01:30:41,811 --> 01:30:42,220
processing cycle.
Okay.

1533
01:30:42,960 --> 01:30:43,410
Thank you.

1534
01:30:45,510 --> 01:30:50,510
Thank you.

