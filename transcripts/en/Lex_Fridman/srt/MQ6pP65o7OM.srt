1
00:00:00,030 --> 00:00:02,760
Today we will talk about deep 
reinforcement learning.

2
00:00:07,060 --> 00:00:12,060
The question we would like to explore is
to which degree we can teach systems to 

3
00:00:14,381 --> 00:00:19,381
act,
to perceive and act in this world from 

4
00:00:19,381 --> 00:00:23,161
data.
So let's take a step back and think of 

5
00:00:23,260 --> 00:00:28,260
what is the full range of tasks than 
artificial intelligence system needs to 

6
00:00:28,260 --> 00:00:31,420
accomplish.
Here's the stack from top to bottom,

7
00:00:31,660 --> 00:00:36,040
top the input bottom output,
the environment at the top,

8
00:00:36,050 --> 00:00:41,050
the world that the agent is operating in
sensed by sensors taking in the world 

9
00:00:42,971 --> 00:00:46,660
outside and converting it to raw data 
and interpretable by machines,

10
00:00:48,070 --> 00:00:51,580
sensor data,
and from that raw sensor data,

11
00:00:51,940 --> 00:00:56,940
you extract features.
You extract structure from that data 

12
00:00:58,060 --> 00:01:01,450
such that you can input it,
makes sense of it,

13
00:01:01,750 --> 00:01:03,430
discriminate,
separate,

14
00:01:03,490 --> 00:01:08,490
understand the data,
and as we discussed,

15
00:01:09,720 --> 00:01:12,420
you form higher and higher order 
representations,

16
00:01:12,750 --> 00:01:17,750
a hierarchy of representations based on 
which the machine learning techniques 

17
00:01:17,750 --> 00:01:22,431
can then be applied.
Once the machine learning techniques,

18
00:01:24,791 --> 00:01:27,250
the understanding,
as I mentioned,

19
00:01:27,520 --> 00:01:32,520
converts the data into features,
into higher order representations and 

20
00:01:32,520 --> 00:01:34,450
into simple actionable,
useful information.

21
00:01:35,170 --> 00:01:37,780
We aggregate that information into 
knowledge,

22
00:01:38,020 --> 00:01:43,020
would take the pieces of knowledge 
extracted from the data through the 

23
00:01:43,020 --> 00:01:45,820
machine learning techniques and to build
a taxonomy,

24
00:01:47,990 --> 00:01:52,990
a library of knowledge,
and would that knowledge we reason an 

25
00:01:53,631 --> 00:01:58,631
agent is tasked to reason,
to aggregate,

26
00:01:59,600 --> 00:02:04,600
to connect pieces of data seen in the 
recent past or the distant past to make 

27
00:02:05,871 --> 00:02:10,871
sense of the world that's operating in 
and finally to make a plan of how to act

28
00:02:10,881 --> 00:02:14,600
in that world based on its objectives 
based on what he wants to accomplish.

29
00:02:14,930 --> 00:02:19,930
As I mentioned,
a simple but commonly accepted 

30
00:02:19,930 --> 00:02:23,741
definition of intelligence is a system 
that's able to accomplish complex goals,

31
00:02:24,830 --> 00:02:29,830
so system that's operating in an 
environment and this world must have a 

32
00:02:29,830 --> 00:02:33,521
goal,
must have an objective function or 

33
00:02:33,521 --> 00:02:36,221
reward function and based on that it 
forms of plan and takes action and 

34
00:02:36,411 --> 00:02:39,350
because they're operates in many cases 
in the physical world,

35
00:02:39,470 --> 00:02:44,470
it must have tools,
effectors with which it applies to 

36
00:02:44,470 --> 00:02:46,340
actions to change.
Something about the world.

37
00:02:46,940 --> 00:02:51,940
That's the full stack of an artificial 
intelligence system that acts in the 

38
00:02:51,940 --> 00:02:55,400
world.
And the question is,

39
00:02:56,910 --> 00:03:01,910
what kind of task can such a take on 
what kind of tasks can an artificial 

40
00:03:02,981 --> 00:03:06,820
intelligence system learn?
As we understand ai today,

41
00:03:07,750 --> 00:03:12,750
we will talk about the advancement of 
deeper enforcement learning approaches 

42
00:03:12,850 --> 00:03:17,850
and some of the fascinating ways it's 
able to take much of the stack and treat

43
00:03:18,521 --> 00:03:23,290
it as an end to end learning problem.
But we look at games,

44
00:03:23,440 --> 00:03:27,430
we'll look at simple formalized worlds.
While it's still impressive,

45
00:03:27,431 --> 00:03:29,950
beautiful and unprecedented 
accomplishments.

46
00:03:30,880 --> 00:03:35,880
It's nevertheless formal tasks.
Can we then move beyond games into 

47
00:03:37,450 --> 00:03:42,450
expert tasks of medical diagnosis,
of design and into natural language,

48
00:03:45,940 --> 00:03:49,210
and finally the human level tasks of 
emotion,

49
00:03:49,360 --> 00:03:53,110
imagination,
consciousness.

50
00:03:56,650 --> 00:04:01,000
That's once again,
review the stack and practicality in the

51
00:04:01,001 --> 00:04:06,001
tools.
We have the input for robots operating 

52
00:04:06,001 --> 00:04:10,840
in the world from cars to humanoid to 
drones as Lidar,

53
00:04:10,870 --> 00:04:12,610
camera radar,
gps,

54
00:04:12,611 --> 00:04:15,580
stereo cameras,
audio microphone,

55
00:04:16,060 --> 00:04:21,060
networking for communication,
and the various ways to measure 

56
00:04:21,060 --> 00:04:24,211
kinematics with Imu.
The raw sensory data is then processed 

57
00:04:28,000 --> 00:04:33,000
features.
The forum to representations are formed 

58
00:04:33,000 --> 00:04:33,400
and multiple higher and higher order 
representations.

59
00:04:33,580 --> 00:04:36,700
That's what deep learning gets us before
neural networks.

60
00:04:36,701 --> 00:04:41,701
Before the advent of,
before the recent successes of neural 

61
00:04:41,701 --> 00:04:46,021
networks to go deeper and therefore be 
able to form high order representations 

62
00:04:46,021 --> 00:04:50,170
of the data that was done by experts by 
human experts today,

63
00:04:50,200 --> 00:04:55,200
networks are able to do that.
That's the representation piece and on 

64
00:04:55,200 --> 00:04:57,370
top of the representation piece,
the final layers.

65
00:04:57,371 --> 00:05:01,340
These networks are able to accomplish 
the supervised learning task,

66
00:05:01,390 --> 00:05:06,390
the generative tasks,
and the unsupervised clustering tasks 

67
00:05:07,870 --> 00:05:09,400
through machine learning.

68
00:05:09,820 --> 00:05:14,820
That's what we talked about a little in 
lecture one and we'll continue tomorrow 

69
00:05:15,400 --> 00:05:20,040
and Wednesday.
That's supervised learning and you could

70
00:05:20,041 --> 00:05:24,010
think about the output of those networks
as simple,

71
00:05:24,011 --> 00:05:24,850
clean,
useful,

72
00:05:24,851 --> 00:05:29,851
valuable information.
That's the knowledge and that knowledge 

73
00:05:30,610 --> 00:05:34,600
can be in the form of single numbers.
It can be regression,

74
00:05:34,601 --> 00:05:38,260
continuous variables.
It can be a sequence of numbers,

75
00:05:38,261 --> 00:05:41,680
it can be images,
audio sentences,

76
00:05:41,710 --> 00:05:44,410
text,
speech.

77
00:05:44,770 --> 00:05:47,830
Once that knowledge is extracted and 
aggregated,

78
00:05:47,831 --> 00:05:51,460
how do we connect it in multi 
resolution?

79
00:05:51,470 --> 00:05:55,240
Always form hierarchies of ideas,
connect ideas.

80
00:05:56,530 --> 00:06:00,440
The trivial silly example is connecting 
images,

81
00:06:00,800 --> 00:06:03,320
activity recognition and audio.
For example,

82
00:06:04,130 --> 00:06:09,130
if it looks like a duck,
quacks like a duck and swims like a 

83
00:06:09,130 --> 00:06:10,850
duck,
we do not currently have approaches that

84
00:06:10,851 --> 00:06:15,851
effectively integrate this information 
to produce a higher confidence estimate 

85
00:06:16,850 --> 00:06:19,820
that is in fact the duck and the 
planning piece.

86
00:06:21,350 --> 00:06:24,470
The task of taking the sensory 
information,

87
00:06:24,920 --> 00:06:29,920
fusing the sensory information,
and making action control and longer 

88
00:06:29,920 --> 00:06:34,751
term plans based on that information,
as we will discuss today are more and 

89
00:06:37,221 --> 00:06:40,610
more amenable to the learning approach 
to the deep learning approach,

90
00:06:40,790 --> 00:06:45,790
but to date have been the most 
successful as non learning optimization 

91
00:06:45,790 --> 00:06:47,870
based approaches like with the several 
of the guest speakers would have,

92
00:06:47,930 --> 00:06:52,880
including the creator of this robot 
atlas in Boston Dynamics.

93
00:06:54,910 --> 00:06:59,500
So the question how much of the stack 
can be learned and to end from the input

94
00:06:59,501 --> 00:07:04,501
to the output.
We know we can learn the representation 

95
00:07:04,501 --> 00:07:08,370
and the knowledge from the 
representation and to knowledge even 

96
00:07:08,370 --> 00:07:11,761
with the kernel methods of Svm and 
certainly with with neural networks.

97
00:07:14,560 --> 00:07:19,560
Mapping from representation to 
information has been where the primary 

98
00:07:21,311 --> 00:07:26,311
success and machine learning over the 
past three decades has been mapping from

99
00:07:26,351 --> 00:07:31,030
raw sensory data to knowledge.
That's where the success,

100
00:07:31,031 --> 00:07:36,031
the automated representation,
learning of deep learning has been a 

101
00:07:36,031 --> 00:07:40,861
success,
going straight from raw data to 

102
00:07:40,861 --> 00:07:43,141
knowledge.
The open question for us today and 

103
00:07:43,141 --> 00:07:47,491
beyond is if we can expand the red box 
there of what can be learned end to end 

104
00:07:47,491 --> 00:07:52,380
from sensory data to reasoning,
so aggregating for me higher 

105
00:07:52,380 --> 00:07:56,521
representations of the extract of 
knowledge and forming plans and acting 

106
00:07:57,491 --> 00:08:02,491
in this world from the raw sensory data,
we will show the incredible fact that 

107
00:08:02,491 --> 00:08:07,411
we're able to do learn exactly what's 
shown here and to end with deeper 

108
00:08:07,411 --> 00:08:11,800
enforcement learning on trivial tasks in
a generalizable way.

109
00:08:12,070 --> 00:08:17,070
The question is whether that can then 
move on to real world tasks of 

110
00:08:17,070 --> 00:08:19,210
autonomous vehicles,
of humanoid,

111
00:08:19,600 --> 00:08:21,100
robotics and so on.

112
00:08:23,760 --> 00:08:28,760
That's the open question.
So today let's talk about reinforcement 

113
00:08:28,760 --> 00:08:29,700
learning.
There's three types of machine learning.

114
00:08:32,340 --> 00:08:37,340
Supervised unsupervised are the 
categories that the extremes in relative

115
00:08:39,601 --> 00:08:44,070
to the amount of human and human input 
that's required for supervised learning.

116
00:08:44,400 --> 00:08:49,400
Every piece of data that's used for 
teaching these systems is first labeled 

117
00:08:49,561 --> 00:08:54,561
by human beings and unsupervised 
learning on the right is no data is 

118
00:08:54,561 --> 00:08:59,511
labeled by beings in between is some 
sparse input from humans.

119
00:09:01,470 --> 00:09:06,470
Semi supervised learning is when only 
part of the data is provided by humans.

120
00:09:07,230 --> 00:09:11,640
Ground Truth and the rest was being 
first gen analyzed by the system,

121
00:09:11,790 --> 00:09:16,790
and that's what reinforcement learning 
falls reinforcement learning as shown 

122
00:09:17,041 --> 00:09:20,010
there with the cats.
As I said,

123
00:09:20,011 --> 00:09:22,590
every successful presentation must 
include cats.

124
00:09:24,030 --> 00:09:29,030
They're supposed to be Pavlov's cats and
a ringing a bell,

125
00:09:29,281 --> 00:09:34,281
and every time they ring a bell,
they're given food and they learn this 

126
00:09:34,281 --> 00:09:37,371
process.
The goal of reinforcement learning is to

127
00:09:38,250 --> 00:09:43,250
learn from sparse reward data from learn
from spar,

128
00:09:43,841 --> 00:09:48,841
supervised data,
and take advantage of the fact that in 

129
00:09:48,841 --> 00:09:52,641
simulation or in the real world,
there is a temporal consistency to the 

130
00:09:52,641 --> 00:09:52,641
world.

131
00:09:52,641 --> 00:09:57,470
There is a temporal dynamics that 
follows from state to state to state 

132
00:09:57,470 --> 00:10:02,061
through time and so you can propagate 
information even if the information that

133
00:10:02,071 --> 00:10:06,420
you received about the supervision.
The ground truth is sparse.

134
00:10:06,780 --> 00:10:11,780
You can follow that information back 
through time to infer something about 

135
00:10:11,780 --> 00:10:15,960
the reality of what happened before then
even if your reward signals were weak,

136
00:10:16,530 --> 00:10:21,180
so it's using the fact that the physical
world evolves through time in some,

137
00:10:21,900 --> 00:10:26,900
some sort of predictable way to take 
sparse information and generalize it 

138
00:10:28,710 --> 00:10:31,680
over the entirety of the experience as 
being learned.

139
00:10:32,580 --> 00:10:37,580
So we apply this to two problems today.
We'll talk about deep traffic as a 

140
00:10:38,641 --> 00:10:39,530
methodology,
as a,

141
00:10:39,531 --> 00:10:41,340
as a way to introduce deeper 
enforcement.

142
00:10:41,341 --> 00:10:46,341
Learning.
The deep traffic is a competition that 

143
00:10:46,341 --> 00:10:49,221
we ran last year and expanded 
significantly this year and I'll talk 

144
00:10:50,101 --> 00:10:55,101
about some of the details and how the 
folks in this room can on your 

145
00:10:55,101 --> 00:10:59,660
smartphone today or if you have a laptop
training agent while I'm talking 

146
00:11:00,480 --> 00:11:02,220
training a neural network in the 
browser.

147
00:11:02,670 --> 00:11:06,390
Some of the things you've added our.
We've added the capability.

148
00:11:06,900 --> 00:11:11,900
We've now turned it into a multiagent 
deep reinforcement learning problem 

149
00:11:11,900 --> 00:11:13,770
where he can control up to 10 cars 
within your own network.

150
00:11:15,240 --> 00:11:20,240
Perhaps less significant but pretty cool
is the ability to customize the way the 

151
00:11:21,331 --> 00:11:26,331
agent looks so you can upload and people
have to an absurd degree have already 

152
00:11:27,241 --> 00:11:32,241
begun doing so,
uploading different images instead of 

153
00:11:32,241 --> 00:11:34,920
the car that's shown there as long as it
maintains the dimensions.

154
00:11:34,950 --> 00:11:36,810
Shown here is a space x rocket.

155
00:11:38,850 --> 00:11:43,850
The competition is hosted on the website
self driving cars that mit.edu/deep

156
00:11:44,701 --> 00:11:46,770
traffic.
We'll return to this later.

157
00:11:48,390 --> 00:11:51,650
The code is on get hub with some more 
information,

158
00:11:51,660 --> 00:11:56,660
a starter code and the paper describing 
some of the fundamental insights that 

159
00:11:58,031 --> 00:12:02,050
will help you win at this competition is
an archive,

160
00:12:04,180 --> 00:12:08,050
so from supervised learning and lecture 
one to today.

161
00:12:09,220 --> 00:12:14,220
Supervised learning we can think of is 
memorization of ground true data in 

162
00:12:15,661 --> 00:12:19,590
order to form representations that 
generalizes from that Ground Truth.

163
00:12:20,610 --> 00:12:25,610
Reinforcement learning is we can think 
of as a way to brute force propagate 

164
00:12:26,191 --> 00:12:31,191
that information,
the sparse information through time to 

165
00:12:35,120 --> 00:12:40,120
to assign quality reward to state that 
does not directly have a reward to make 

166
00:12:43,221 --> 00:12:48,221
sense of this world.
When the rewards a sparse but are 

167
00:12:48,221 --> 00:12:51,260
connected through time.
You can think of that as reasoning,

168
00:12:53,390 --> 00:12:58,390
so the connection through time is 
modeled in most reinforcement learning 

169
00:13:00,321 --> 00:13:05,321
approach is very simply that there's an 
agent taken an action in the state and 

170
00:13:07,071 --> 00:13:12,071
receiving a reward,
and the agent operating in an 

171
00:13:12,071 --> 00:13:15,371
environment executes an action,
receives an observed state and use state

172
00:13:15,440 --> 00:13:20,440
and receives the award.
This process continues over and over and

173
00:13:22,771 --> 00:13:26,670
some examples we can think of any of the
video games,

174
00:13:26,671 --> 00:13:31,671
some of which we'll talk about today,
like Atari breakout as the environment.

175
00:13:32,370 --> 00:13:34,770
The agent is the paddle.

176
00:13:36,680 --> 00:13:41,680
Each action that the agent takes has an 
influence on the evolution of the 

177
00:13:43,071 --> 00:13:47,870
environment and the success is measured 
by some reward mechanism.

178
00:13:48,350 --> 00:13:53,350
In this case,
points are given by the game and every 

179
00:13:53,350 --> 00:13:56,771
game has a different point scheme that 
must be converted normalized until a way

180
00:13:58,551 --> 00:14:03,110
that's interpretable by the system and 
the goal is to maximize those points,

181
00:14:03,140 --> 00:14:08,140
maximize the reward.
The continuous problem of card poll by 

182
00:14:10,300 --> 00:14:13,450
balancing the goal is to balance the 
pole on top of a moving cart.

183
00:14:14,290 --> 00:14:17,110
The state is the angle that angular 
speed,

184
00:14:17,111 --> 00:14:19,000
the position,
the horizontal velocity,

185
00:14:19,960 --> 00:14:22,870
the actions are the horizontal force 
applied to the cart,

186
00:14:23,140 --> 00:14:27,100
and the award is one at each time step.
If the pole is still upright,

187
00:14:30,130 --> 00:14:33,250
all the first person shooters,
the video games,

188
00:14:33,251 --> 00:14:36,710
and now star craft the,
uh,

189
00:14:36,780 --> 00:14:41,780
strategy games in case of first person 
shooter and doom.

190
00:14:43,050 --> 00:14:46,290
What is the goal?
The environment is the game that goes to

191
00:14:46,320 --> 00:14:49,200
eliminate all opponents.
The state is the raw game.

192
00:14:49,201 --> 00:14:52,550
Pixels coming in.
The actions is moving up,

193
00:14:52,551 --> 00:14:53,240
down,
left,

194
00:14:53,241 --> 00:14:55,130
right,
and so on.

195
00:14:55,550 --> 00:15:00,550
And the reward is positive when 
eliminating and opponent and negative.

196
00:15:01,550 --> 00:15:06,550
When the agent has eliminated industrial
robotics,

197
00:15:08,220 --> 00:15:13,220
been packing with a robotic arm,
the goal is to pick up a device from a 

198
00:15:13,220 --> 00:15:16,491
box and put it into a container.
The state is the raw pixels of the real 

199
00:15:16,491 --> 00:15:21,001
world that the robot observes the 
actions or the possible actions with the

200
00:15:21,151 --> 00:15:24,150
robot that different degrees of freedom 
and moving through those degrees,

201
00:15:24,360 --> 00:15:28,800
moving the different actuators to 
realize the position of the arm,

202
00:15:28,950 --> 00:15:32,550
and then award is positive on placing a 
device successfully and negative.

203
00:15:32,551 --> 00:15:33,240
Otherwise,

204
00:15:35,230 --> 00:15:40,230
everything can be modeled in this way.
Mark off decision process as a state,

205
00:15:40,481 --> 00:15:45,440
as zero action,
a zero and reward received and you state

206
00:15:45,441 --> 00:15:46,720
is achieved.
Again,

207
00:15:46,721 --> 00:15:51,430
Action Reward State Action Award,
state until a terminal state is,

208
00:15:51,570 --> 00:15:56,570
is reached,
and the major components of 

209
00:15:56,570 --> 00:16:00,901
reinforcement learning is a policy,
some kind of plan of what to do in every

210
00:16:01,001 --> 00:16:06,001
single state.
What kind of action to perform a value 

211
00:16:06,001 --> 00:16:07,290
function,
uh,

212
00:16:07,300 --> 00:16:11,530
some kind of sense of what is a good 
state to be in,

213
00:16:11,590 --> 00:16:16,590
of what is a good action to take in a 
state and sometimes a model

214
00:16:19,330 --> 00:16:24,330
that the agent represents the 
environment with some kind of sense of 

215
00:16:24,330 --> 00:16:28,891
the environment.
It's operating in the dynamics of that 

216
00:16:28,891 --> 00:16:30,040
environment that's useful for making 
decisions about actions.

217
00:16:30,880 --> 00:16:35,880
Let's take a trivial example,
a grid world,

218
00:16:36,461 --> 00:16:40,300
a three by four,
12 squares where you start at the bottom

219
00:16:40,301 --> 00:16:45,301
left and their task with walking about 
this world to maximize reward.

220
00:16:47,400 --> 00:16:52,400
The award that the top right is a plus 
one and a one square below that is a 

221
00:16:52,400 --> 00:16:56,851
negative one.
And every step you take is a punishment 

222
00:16:56,851 --> 00:16:58,960
or as a negative reward of zero point 
zero four.

223
00:17:00,040 --> 00:17:02,380
So what is the optimal policy in this 
world?

224
00:17:04,310 --> 00:17:06,170
Now,
when everything is deterministic,

225
00:17:07,220 --> 00:17:11,650
perhaps this is the policy.
When you start at the bottom left,

226
00:17:11,860 --> 00:17:14,290
well,
because every step hurts,

227
00:17:14,291 --> 00:17:19,291
every step has a negative reward.
Then you want to take the shortest path 

228
00:17:19,291 --> 00:17:20,830
to the maximum square with a maximum 
reward.

229
00:17:21,760 --> 00:17:24,640
When the state space is non 
deterministic,

230
00:17:26,390 --> 00:17:30,560
as presented before,
with the probability of point eight,

231
00:17:30,770 --> 00:17:33,110
when you choose to go up,
you go up.

232
00:17:33,140 --> 00:17:37,400
But with probability point one,
you go left and point one,

233
00:17:37,430 --> 00:17:39,740
you go right,
unfair.

234
00:17:39,980 --> 00:17:41,360
Again,
much like life,

235
00:17:42,470 --> 00:17:47,470
that would be the optimal policy.
What is the key observation here that 

236
00:17:48,531 --> 00:17:51,480
every single in the space must have a 
plan

237
00:17:53,320 --> 00:17:58,320
because you can't because then a non 
deterministic aspect of the control,

238
00:17:58,390 --> 00:18:00,400
you can't control where are going to end
up.

239
00:18:00,401 --> 00:18:03,580
So you must have a plan for every place 
that's the policy.

240
00:18:03,850 --> 00:18:08,850
Having an action,
an optimal action to take in every 

241
00:18:08,850 --> 00:18:08,850
single state.
Now,

242
00:18:08,850 --> 00:18:13,360
suppose we change their reward structure
and for every step we take as a negative

243
00:18:13,890 --> 00:18:16,960
or award is a negative two,
so it really hurts.

244
00:18:16,990 --> 00:18:19,420
There's a high punishment for every 
single step we take.

245
00:18:19,600 --> 00:18:23,470
So no matter what,
we always take the shortest path,

246
00:18:23,471 --> 00:18:28,471
the optimal policies to take the 
shortest path to the to the only spot on

247
00:18:28,601 --> 00:18:31,900
the board that doesn't result in 
punishment.

248
00:18:34,450 --> 00:18:38,560
If we decrease the reward of each step 
two negative point one,

249
00:18:39,820 --> 00:18:44,820
the policy changes were there a some 
extra degree of wandering encouraged,

250
00:18:48,560 --> 00:18:53,560
and as we go further and further in,
lowering the punishment does before to 

251
00:18:53,560 --> 00:18:55,680
negative zero point zero,
four more.

252
00:18:55,681 --> 00:19:00,681
Wandering and wandering is allowed.
And when we finally turned the reward 

253
00:19:05,300 --> 00:19:09,200
into positive,
so every step it,

254
00:19:10,290 --> 00:19:15,290
every step is increases the award.
Then there's a significant incentive to 

255
00:19:16,830 --> 00:19:20,010
to stay on the board without ever 
reaching the destination.

256
00:19:22,530 --> 00:19:24,160
Kind of like college for a lot of 
people.

257
00:19:28,620 --> 00:19:33,620
So the value function,
the way we think about the value of the 

258
00:19:33,620 --> 00:19:37,711
estate or the value of anything in the 
environment is the reward we're likely 

259
00:19:40,321 --> 00:19:45,321
to receive in the future and the way we 
see the reward we're likely to receive 

260
00:19:46,260 --> 00:19:51,260
as we discount the future award because 
we can't always count on it,

261
00:19:53,730 --> 00:19:57,740
hair or gamma further and further out 
into the future.

262
00:19:57,800 --> 00:20:00,680
More and more discounts decreases the 
era,

263
00:20:00,890 --> 00:20:05,890
the the importance of that reward 
received and a good strategy is taking 

264
00:20:06,651 --> 00:20:08,720
the summer of these rewards and 
maximizing it,

265
00:20:09,020 --> 00:20:13,820
maximizing discounted future award.
That's what reinforcement learning hopes

266
00:20:13,821 --> 00:20:17,370
to achieve.
And with cue learning,

267
00:20:18,750 --> 00:20:23,750
we use any policy to estimate the value 
of taking an action in the state.

268
00:20:28,140 --> 00:20:30,810
So off policy,
forget policy,

269
00:20:31,410 --> 00:20:36,410
we move about the world and use the 
bellman equation here on the bottom to 

270
00:20:36,410 --> 00:20:40,971
continuously update our estimate of how 
good a certain action is in a certain 

271
00:20:40,971 --> 00:20:45,060
state.
So we don't need this.

272
00:20:45,090 --> 00:20:50,090
This allows us to operate in a much 
larger space in a much larger action 

273
00:20:50,090 --> 00:20:53,581
space.
We move about this world or simulation 

274
00:20:53,581 --> 00:20:55,150
or in the real world,
taking actions and updating our estimate

275
00:20:55,330 --> 00:20:57,450
of how good certain actions are over 
time.

276
00:20:59,620 --> 00:21:03,030
The new state that the left is the is 
the update of value.

277
00:21:03,050 --> 00:21:08,050
The old state is the starting value for 
the equation and we update that old 

278
00:21:08,050 --> 00:21:09,180
state estimation with,
uh,

279
00:21:09,190 --> 00:21:14,190
some of the reward received by taking 
action as a tax action,

280
00:21:15,161 --> 00:21:20,161
a in status,
and the maximum reward that's possible 

281
00:21:22,330 --> 00:21:27,330
to be received in the following states.
Discounted that update is decreased with

282
00:21:30,591 --> 00:21:33,170
the learning rate.
The higher the learning rate,

283
00:21:33,230 --> 00:21:35,200
the more value we,
the,

284
00:21:35,270 --> 00:21:40,270
the faster we learn,
the more value we assigned to new 

285
00:21:40,270 --> 00:21:40,550
information.
That's simple.

286
00:21:40,580 --> 00:21:41,720
That's it.
That's cute.

287
00:21:41,721 --> 00:21:44,990
Learning as simple update rule allows us
to,

288
00:21:45,770 --> 00:21:50,770
to explore the world and as we explore,
get more and more information about 

289
00:21:52,401 --> 00:21:57,401
what's good to do in this world,
and there's always a balance in the 

290
00:21:57,401 --> 00:22:01,151
various problem spaces we'll discuss.
There's always a balance between 

291
00:22:01,151 --> 00:22:02,120
exploration and exploitation.

292
00:22:04,660 --> 00:22:08,200
As you form a better and better estimate
of the function of what actions are good

293
00:22:08,230 --> 00:22:13,230
to take,
you start to get a sense of what is the 

294
00:22:13,230 --> 00:22:15,040
best action to take,
but it's not a perfect sense.

295
00:22:15,070 --> 00:22:18,400
It's still an approximation and so 
there's value of exploration,

296
00:22:18,670 --> 00:22:21,730
but the better and better your estimate 
becomes less and less.

297
00:22:21,731 --> 00:22:26,731
Exploration has a benefit,
so usually want to explore a lot in the 

298
00:22:26,731 --> 00:22:31,390
beginning and less and less so towards 
the end and when we finally released the

299
00:22:31,391 --> 00:22:35,170
system out into the world and wish it to
operate as best,

300
00:22:35,380 --> 00:22:39,070
then we have it operate as a greedy 
system,

301
00:22:39,071 --> 00:22:41,440
always taking the optimal action 
according to the q,

302
00:22:41,450 --> 00:22:46,450
a q value function and everything I'm 
talking about now is parametrized and 

303
00:22:49,631 --> 00:22:54,631
our parameters that are very important 
for winning the deep traffic 

304
00:22:54,851 --> 00:22:59,851
competition,
which is using this very algorithm 

305
00:22:59,851 --> 00:23:01,420
within your own network as a score.

306
00:23:03,870 --> 00:23:08,870
So for simple table representation of a 
function where the y axis to state four 

307
00:23:10,231 --> 00:23:11,400
states s one,
two,

308
00:23:11,401 --> 00:23:12,180
three,
four,

309
00:23:12,540 --> 00:23:16,260
and the x axis is actions a,
one,

310
00:23:16,290 --> 00:23:16,860
two,
three,

311
00:23:16,861 --> 00:23:21,861
four.
We can think of this table as randomly 

312
00:23:21,861 --> 00:23:25,761
initiated or initiated initialized in 
any kind of way that's not 

313
00:23:25,831 --> 00:23:30,831
representative of actual reality.
And as we move about the world and we 

314
00:23:30,831 --> 00:23:34,521
take actions,
we update this table with a bellman 

315
00:23:34,521 --> 00:23:36,420
equation shown up top and here slides 
now are online.

316
00:23:36,600 --> 00:23:40,800
You can see a simple pseudo code 
algorithm of how to update it.

317
00:23:40,980 --> 00:23:45,980
I to run this bellman equation and over 
the approximation becomes the optimal q 

318
00:23:48,470 --> 00:23:51,860
table.
The problem is when that cute table,

319
00:23:51,920 --> 00:23:56,920
it becomes exponential in size.
When we take in raw sensor information 

320
00:23:57,620 --> 00:24:02,620
as we do with cameras with deep crash or
deep traffic sticking the full grid 

321
00:24:03,891 --> 00:24:06,560
space and taking that information,
the Ra,

322
00:24:07,460 --> 00:24:12,460
the Ra grid pixels of deep traffic.
When you take the arcade games here,

323
00:24:13,131 --> 00:24:18,131
they're taking the raw pixels of the 
game or when we take go the game of go 

324
00:24:19,640 --> 00:24:21,770
when it's taking the units,
um,

325
00:24:21,810 --> 00:24:23,150
the,
the board,

326
00:24:23,300 --> 00:24:28,300
the raw state of the board as the input,
the potential state space,

327
00:24:31,040 --> 00:24:36,040
the number of possible combinatorial 
variations of what states as possible is

328
00:24:37,040 --> 00:24:42,040
extremely large,
larger than we can certainly hold the 

329
00:24:42,040 --> 00:24:45,391
memory and larger than we can ever be 
able to accurately approximate through 

330
00:24:45,561 --> 00:24:48,980
the bellman equation over time to 
simulation

331
00:24:50,880 --> 00:24:52,920
through the simple update of the 
equation.

332
00:24:53,730 --> 00:24:56,940
So this is where deep reinforcement 
learning comes in.

333
00:24:57,780 --> 00:25:00,600
Neural networks are really good 
approximators.

334
00:25:00,930 --> 00:25:05,490
They're really good at exactly this task
of learning this kind of queue table.

335
00:25:09,850 --> 00:25:11,800
So as we started with supervised 
learning,

336
00:25:12,280 --> 00:25:14,320
when all networks help us memorize 
patterns,

337
00:25:14,321 --> 00:25:19,060
using supervised ground truth data,
and we'll move to reinforcement learning

338
00:25:19,061 --> 00:25:23,080
that hopes to propagate outcomes to 
knowledge.

339
00:25:25,060 --> 00:25:30,060
Deep learning allows us to do so.
A much larger state spaces are much 

340
00:25:30,131 --> 00:25:35,131
larger action spaces,
which means it's generalizable,

341
00:25:36,430 --> 00:25:41,430
it's much more capable to deal with the 
raw stuff of sensory data,

342
00:25:43,630 --> 00:25:48,630
which means it's much more capable to 
deal with a broad variation of real 

343
00:25:48,630 --> 00:25:53,011
world applications,
and it does so because it's able to 

344
00:25:57,510 --> 00:26:01,920
learn the representations as we've 
discussed on Monday.

345
00:26:04,920 --> 00:26:09,920
They're understanding comes from 
converting the raw sensor information 

346
00:26:10,110 --> 00:26:15,110
into into simple,
useful information based on which the 

347
00:26:15,110 --> 00:26:18,510
action in this particular state can be 
taken in the same exact way,

348
00:26:18,870 --> 00:26:23,870
so instead of the queue table,
instead of this queue function with 

349
00:26:23,870 --> 00:26:25,800
plugging in your network where they 
input is the state space.

350
00:26:25,980 --> 00:26:30,980
No matter how complex and the output is 
a value for each of the actions that you

351
00:26:32,011 --> 00:26:32,520
could take.

352
00:26:34,980 --> 00:26:38,850
Input is the state.
Opera is the value of the function.

353
00:26:39,150 --> 00:26:44,150
It's simple.
This is deep q network dqn at the core 

354
00:26:47,160 --> 00:26:52,160
of the success at deep mind.
A lot of the cool stuff you see about 

355
00:26:52,160 --> 00:26:54,360
video games,
dqn or variants of dqs or play.

356
00:26:55,530 --> 00:26:58,080
This is what a first with a nature 
paper,

357
00:26:59,110 --> 00:27:04,110
deep mind,
the success came of playing the 

358
00:27:04,110 --> 00:27:08,171
different games including Atari Games.
So how are these things trained?

359
00:27:12,560 --> 00:27:17,560
Very similar to supervise learning.
The bellman equation,

360
00:27:19,660 --> 00:27:24,660
laptop takes the reward and the 
discounted expected reward from future 

361
00:27:28,471 --> 00:27:33,471
states.
The loss function here for neural 

362
00:27:34,181 --> 00:27:36,320
network.
Then you'll network learns with the loss

363
00:27:36,321 --> 00:27:41,321
function.
It takes the reward received at the 

364
00:27:41,321 --> 00:27:45,601
current state.
Does the forward pass through in your 

365
00:27:45,601 --> 00:27:49,300
network to estimate the value of the 
future state of the best action to take 

366
00:27:50,771 --> 00:27:55,771
in the future state and then subtracts 
that from the forward pass through the 

367
00:27:58,901 --> 00:28:03,901
network for the current state of action.
So you take the difference between what 

368
00:28:04,660 --> 00:28:09,520
your Acu estimator,
than your own network believes the value

369
00:28:09,521 --> 00:28:14,521
of the current state is,
and what it more likely as to be based 

370
00:28:17,260 --> 00:28:22,260
on the value of the future states that 
are reachable based on the actions you 

371
00:28:22,260 --> 00:28:22,260
can take.

372
00:28:26,680 --> 00:28:31,680
Here's the algorithm.
Input is the state output as the q value

373
00:28:32,421 --> 00:28:36,350
for each action or in this diagram and 
as a state of inaction,

374
00:28:36,410 --> 00:28:40,520
and the output is the q value.
It's very similar architectures,

375
00:28:41,090 --> 00:28:46,090
so given the transition of s a r s prime
as current state taken an action,

376
00:28:49,761 --> 00:28:52,530
we're seeing your reward and achieving 
as prime state.

377
00:28:55,530 --> 00:29:00,530
The update is do a feed forward past to 
the network for the current state,

378
00:29:02,760 --> 00:29:07,760
do a feed forward,
pass for each of the possible actions 

379
00:29:07,760 --> 00:29:11,751
taken in the next state,
and that's how we compute the two parts 

380
00:29:11,751 --> 00:29:15,291
of the loss function and the update the 
weights using backpropagation.

381
00:29:16,890 --> 00:29:19,260
Again,
last function backpropagation is how the

382
00:29:19,261 --> 00:29:24,261
network is trained.
This has actually been around for much 

383
00:29:24,541 --> 00:29:29,541
longer than deep mind.
A few tricks made made it really work.

384
00:29:34,370 --> 00:29:36,200
Experience replays the biggest one,

385
00:29:38,920 --> 00:29:43,920
so as the games are played through 
simulation or if it's a physical system 

386
00:29:43,920 --> 00:29:47,551
as it acts in the world,
it's actually collecting the 

387
00:29:48,431 --> 00:29:53,431
observations into a library of 
experiences and that training is 

388
00:29:53,431 --> 00:29:58,141
performed by randomly sampling the 
library in the past by randomly sampling

389
00:29:59,710 --> 00:30:04,710
that previous experiences in batches.
So you're not always training on the 

390
00:30:05,411 --> 00:30:10,411
natural continuous evolution to the 
system you're training on randomly 

391
00:30:10,411 --> 00:30:13,210
picked batches of those experiences.
That's a huge.

392
00:30:14,220 --> 00:30:15,090
It's a.
it's a,

393
00:30:15,100 --> 00:30:17,590
seems like a subtle trick,
but it's a really important one.

394
00:30:19,150 --> 00:30:24,150
So the system doesn't overfit a 
particular evolution of,

395
00:30:24,720 --> 00:30:25,380
of,
uh,

396
00:30:25,390 --> 00:30:30,130
of the game of the simulation.
Uh,

397
00:30:30,131 --> 00:30:33,530
another important,
again,

398
00:30:33,531 --> 00:30:36,640
subtle trick as in a lot of deep 
learning approaches,

399
00:30:36,850 --> 00:30:41,850
the subtle tricks make all the 
difference is fixing the target network 

400
00:30:43,180 --> 00:30:45,340
for the last function.
If you notice,

401
00:30:46,060 --> 00:30:49,660
you have to use the neural network,
the neural network,

402
00:30:49,661 --> 00:30:54,661
the Gq,
I network to estimate the value of the 

403
00:30:54,661 --> 00:30:57,601
current state and action pair and the 
next.

404
00:30:58,760 --> 00:31:03,760
So using it multiple times.
And as you perform that operation,

405
00:31:06,310 --> 00:31:11,310
you're updating the network,
which means that target function inside 

406
00:31:11,310 --> 00:31:13,370
that lost function is always changing.
So you're,

407
00:31:13,480 --> 00:31:18,480
the very nature of your loss function is
changing all the time is you're 

408
00:31:18,480 --> 00:31:22,530
learning.
And that's a big problem for stability 

409
00:31:22,530 --> 00:31:23,650
that can create big problems for the 
learning process.

410
00:31:23,980 --> 00:31:28,980
So this little trick is to fix the 
network and only update it every say 

411
00:31:30,930 --> 00:31:35,530
thousand steps.
So as you train the network,

412
00:31:36,390 --> 00:31:41,320
the network that's used to compute the 
target function inside the last function

413
00:31:41,321 --> 00:31:45,740
is fixed.
It produces a more stable computation on

414
00:31:45,741 --> 00:31:50,590
the loss function.
So the ground doesn't shift under you as

415
00:31:50,591 --> 00:31:54,430
you're trying to find a minimal for the 
loss function.

416
00:31:54,730 --> 00:31:57,820
The last function doesn't change and 
unpredictable,

417
00:31:57,821 --> 00:32:02,110
difficult to understand ways and reward 
clipping,

418
00:32:02,920 --> 00:32:07,920
which is always true with general.
Those systems that are operating a 

419
00:32:09,220 --> 00:32:14,220
seeking to operate in a generalized way 
is for very for these various games,

420
00:32:15,490 --> 00:32:17,320
the points are different,
some,

421
00:32:17,350 --> 00:32:19,300
some points are low,
some points are high,

422
00:32:19,450 --> 00:32:20,890
some go positive,
negative,

423
00:32:20,980 --> 00:32:24,880
and they're all normalized to a point 
where the good points are,

424
00:32:24,881 --> 00:32:29,830
the positive points are a one and 
negative points are a negative one.

425
00:32:30,360 --> 00:32:35,360
That's reward clipping,
simplify the reward structure and 

426
00:32:35,360 --> 00:32:39,871
because a lot of the games have 30 fps 
or 60 fps and the actions are not,

427
00:32:42,080 --> 00:32:47,080
it's not valuable to take actions as 
such a high rate inside of these 

428
00:32:47,080 --> 00:32:51,430
particularly Atari Games.
Then you only take an action every four 

429
00:32:51,430 --> 00:32:54,731
steps while still taking into the frames
as part of the temporal window to make 

430
00:32:54,731 --> 00:32:56,150
decisions,
tricks.

431
00:32:56,330 --> 00:33:01,330
But hopefully it gives you a sense of 
the kinds of things necessary for both 

432
00:33:03,890 --> 00:33:08,750
seminal papers like this one and for the
more important accomplishment of winning

433
00:33:08,751 --> 00:33:13,751
deep traffic is the.
Is the tricks make all the difference 

434
00:33:13,820 --> 00:33:18,820
here?
On the bottom is the circle is when the 

435
00:33:19,401 --> 00:33:23,720
technique is used and the excellent.
It's not looking at replay and target,

436
00:33:24,080 --> 00:33:29,080
fixed target,
network and experience replay when both 

437
00:33:29,080 --> 00:33:30,620
are used for the game of breakout river,
raid,

438
00:33:30,680 --> 00:33:34,040
sciquest and space invaders.
The higher the number,

439
00:33:34,041 --> 00:33:39,041
the better it is,
the more points achieved so when it 

440
00:33:39,041 --> 00:33:43,871
gives you a sense that one replay and 
target both give significant 

441
00:33:43,871 --> 00:33:45,230
improvements in the performance of the 
system,

442
00:33:47,370 --> 00:33:51,360
order of magnitude improvements to 
orders of magnitude for breakup.

443
00:33:54,690 --> 00:33:59,450
And here is pseudocode of implementing 
dqn.

444
00:33:59,610 --> 00:34:00,270
The learning.

445
00:34:02,540 --> 00:34:05,510
The key thing to notice and you can look
through the slides,

446
00:34:06,140 --> 00:34:08,820
is the,
the,

447
00:34:09,020 --> 00:34:14,020
the loop,
the wild loop of playing through the 

448
00:34:14,020 --> 00:34:16,871
games and selecting the actions to play 
is not part of the training.

449
00:34:17,920 --> 00:34:20,570
It's part of the saving,
uh,

450
00:34:20,900 --> 00:34:25,900
the observations,
the state action reward next state 

451
00:34:25,900 --> 00:34:29,420
observations is saving them and to 
replay memory into that library.

452
00:34:29,660 --> 00:34:34,660
And then you sample randomly from that 
replay memory to then train the network 

453
00:34:35,600 --> 00:34:36,950
based on the last function

454
00:34:38,360 --> 00:34:43,360
and with probability up up top of the 
probability epsilon select a random 

455
00:34:43,701 --> 00:34:48,701
action that epsilon is the probability 
of exploration that decreases at 

456
00:34:50,990 --> 00:34:55,990
something you'll see in traffic as well,
is the rate of which that exploration 

457
00:34:56,871 --> 00:34:59,270
decreases over time through the training
process.

458
00:34:59,420 --> 00:35:03,170
You want to explore a lot first and less
and less over time.

459
00:35:03,980 --> 00:35:08,980
So this algorithm has been able to 
accomplish in 2015 and since a lot of 

460
00:35:10,191 --> 00:35:15,191
incredible things,
things that made the ai world think that

461
00:35:17,601 --> 00:35:22,601
we were onto something that general ai 
is within reach for the first time that 

462
00:35:27,561 --> 00:35:31,650
raw sensor information was used to 
create a system that acts it,

463
00:35:31,651 --> 00:35:36,651
make sense of the world,
make sense of the physics of the world 

464
00:35:36,651 --> 00:35:38,190
enough to be able to succeed in it for 
very little information.

465
00:35:39,180 --> 00:35:44,180
But these games are trivial even though 
there is a lot of them.

466
00:35:48,360 --> 00:35:52,680
This dq and approach has been able to 
outperform a lot of the Atari Games.

467
00:35:53,070 --> 00:35:56,730
That's what been reported on,
outperform the human level performance.

468
00:35:57,480 --> 00:35:59,430
But again,
these games are trivial.

469
00:36:00,740 --> 00:36:03,630
What I think,
and perhaps biased,

470
00:36:03,810 --> 00:36:08,810
I'm biased,
but one of the greatest accomplishments 

471
00:36:08,810 --> 00:36:09,150
of artificial intelligence in the last 
decade,

472
00:36:10,620 --> 00:36:14,520
at least from the philosophical or the 
research perspective,

473
00:36:15,780 --> 00:36:20,780
is Alphago Zero First Alphago,
Alphago zero is deepmind system that 

474
00:36:26,160 --> 00:36:29,250
beat the best in the world in the game 
of go.

475
00:36:29,820 --> 00:36:34,820
So what's the game of go simple.
I won't get into the rules,

476
00:36:36,390 --> 00:36:41,390
but basically it's a 19 by 19 board 
showing on the bottom of the slide for 

477
00:36:43,021 --> 00:36:47,520
the bottom row of the table for board of
19 by 19.

478
00:36:48,510 --> 00:36:53,510
The number of legal game positions is 
two times 10 to the power of one 70.

479
00:36:56,300 --> 00:37:01,140
It's a very large number of possible 
positions to consider at any one time,

480
00:37:01,320 --> 00:37:05,610
especially the game evolves.
The number of possible moves is huge,

481
00:37:07,170 --> 00:37:12,170
much larger than in chess.
So that's why Ai Community thought that 

482
00:37:14,161 --> 00:37:19,161
this game is not solvable until 2016 
when Alphago used to use human expert 

483
00:37:25,231 --> 00:37:29,730
position play to seed in a supervised 
way.

484
00:37:30,270 --> 00:37:35,270
Reinforcement learning approach,
and I'll describe it in a little bit of 

485
00:37:35,270 --> 00:37:39,081
detail in this couple of slides here to 
beat the best in the world

486
00:37:42,330 --> 00:37:47,330
and then Alphago zero.
That is the accomplishment of the decade

487
00:37:49,320 --> 00:37:54,320
for me in Ai is being able to play with 
no training data on human expert Games 

488
00:38:03,600 --> 00:38:06,930
and beat the best in the world and an 
extremely complex game.

489
00:38:06,960 --> 00:38:11,960
This is not a tare.
This is an this is a a much higher order

490
00:38:15,060 --> 00:38:20,060
difficulty game and the and the quality 
of player that is competing in as much 

491
00:38:20,060 --> 00:38:24,981
higher and it's able to extremely 
quickly here to achieve a rating that's 

492
00:38:25,410 --> 00:38:30,410
better than Alphago and better than the 
different variants of Alphago and 

493
00:38:31,531 --> 00:38:36,531
certainly better than the best of the 
human players in 20 days of self play.

494
00:38:38,830 --> 00:38:41,980
So how does it work?
All of these approaches,

495
00:38:42,550 --> 00:38:44,770
much,
much like the previous ones.

496
00:38:44,800 --> 00:38:49,800
The traditional ones that are not based 
on deep learning are using Monte Carlo 

497
00:38:51,131 --> 00:38:56,131
tree search MCTs,
which is when you have such a large 

498
00:38:57,670 --> 00:39:02,670
state space,
you start at a board and you play and 

499
00:39:02,831 --> 00:39:07,831
you choose moves with some exploitation 
exploration.

500
00:39:08,740 --> 00:39:13,740
Balancing choosing to explore a totally 
new positions or to go deep on the 

501
00:39:14,730 --> 00:39:19,730
positions you know are good until the 
bottom of the game is reached until the 

502
00:39:19,730 --> 00:39:23,700
final state is reached and then you back
propagate the quality of the choices 

503
00:39:24,731 --> 00:39:29,731
you've made leading to that position and
in that way you learn the value of of 

504
00:39:30,940 --> 00:39:35,670
board positions and play that's been 
used by the most successful.

505
00:39:35,710 --> 00:39:40,360
Go playing engines before and Alphago 
sense,

506
00:39:41,710 --> 00:39:46,710
but you might be able to guess what's 
the difference with Alphago versus the 

507
00:39:46,710 --> 00:39:50,671
previous approaches.
They use the neural network as the 

508
00:39:52,540 --> 00:39:56,560
intuition quote on quote to what are the
good states,

509
00:39:56,890 --> 00:40:00,790
what are the good next board positions 
to explore,

510
00:40:05,870 --> 00:40:08,030
and the key things,
again,

511
00:40:08,090 --> 00:40:13,090
the tricks make all the difference that 
made Alphago zero work and work much 

512
00:40:15,231 --> 00:40:20,231
better than Alphago is first because 
there was no expert play instead of 

513
00:40:20,511 --> 00:40:21,530
human games.

514
00:40:23,890 --> 00:40:28,890
Alphago used that very same Monte Carlo 
tree search algorithm,

515
00:40:30,640 --> 00:40:35,640
MCTs to do an intelligent look ahead 
based on in neural network prediction of

516
00:40:36,461 --> 00:40:41,461
what are the good states to take.
It checked that instead of human expert 

517
00:40:41,651 --> 00:40:46,000
play it checked.
How good indeed are those states?

518
00:40:46,990 --> 00:40:51,990
It's a simple look ahead action that 
does the ground truth that does the 

519
00:40:52,240 --> 00:40:57,240
target,
a correction that produces the last 

520
00:40:57,240 --> 00:40:59,941
function.
The second part is the multitask 

521
00:40:59,941 --> 00:41:02,611
learning or what's now called multitask 
learning is the network is is a quote 

522
00:41:02,621 --> 00:41:07,621
unquote two headed in the sense that 
first it outputs the probability of 

523
00:41:07,621 --> 00:41:11,080
which moved to take the obvious thing 
and it's also producing a probability of

524
00:41:11,081 --> 00:41:16,081
winning and there's a few ways to 
combine that information and 

525
00:41:16,081 --> 00:41:20,521
continuously train both parts of the 
network depending on the choice taken.

526
00:41:21,280 --> 00:41:26,280
So you want to take the best choice in 
the short term and achieved the 

527
00:41:26,280 --> 00:41:29,890
positions that are highly likelihood of 
winning for the player.

528
00:41:29,920 --> 00:41:34,920
That's whose turn it is,
and another big step is that they 

529
00:41:37,261 --> 00:41:41,820
updated from 2015 the update of the 
state of the art architecture,

530
00:41:41,880 --> 00:41:46,670
which are now the architecture.
That one image that is residual networks

531
00:41:46,820 --> 00:41:50,880
resume yet for image net.
Those that's it.

532
00:41:51,350 --> 00:41:54,240
And those little changes made all the 
difference.

533
00:41:55,680 --> 00:42:00,000
So that takes us to do deep traffic and 
8 billion hours stuck in traffic

534
00:42:02,010 --> 00:42:07,010
America's pastime.
So we tried to simulate driving that 

535
00:42:07,971 --> 00:42:12,050
behavior layer of driving,
so not the immediate control,

536
00:42:12,200 --> 00:42:15,830
not the motion planning,
but beyond that on top,

537
00:42:16,040 --> 00:42:21,040
on top of those control decisions,
the human interpretable decisions of 

538
00:42:21,321 --> 00:42:23,450
changing lane speeding up,
slowing down,

539
00:42:23,660 --> 00:42:28,660
modeling that in a microtrauma traffic 
simulation framework that's popular and 

540
00:42:28,660 --> 00:42:31,250
traffic engineering,
the kindness shown here,

541
00:42:32,890 --> 00:42:35,320
we applied deeper enforcement learning 
to that,

542
00:42:35,720 --> 00:42:40,720
we call it deep traffic.
The goal is to achieve the highest 

543
00:42:40,720 --> 00:42:44,701
average speed over a long period of time
weaving in and out of traffic for 

544
00:42:45,161 --> 00:42:50,161
students here,
their requirement is to follow the 

545
00:42:50,161 --> 00:42:51,370
tutorial and that Shiva speed of 65 
miles an hour,

546
00:42:54,060 --> 00:42:59,060
and if you really want to achieve a 
speed over 70 miles an hour,

547
00:42:59,460 --> 00:43:04,460
which is what's required to win and 
perhaps upload your own image to make 

548
00:43:06,431 --> 00:43:11,110
sure you look good doing it.
What you should do,

549
00:43:11,230 --> 00:43:14,620
clear instructions to compete.
Read the tutorial.

550
00:43:16,980 --> 00:43:20,340
You can change parameters in the cold 
box on that website.

551
00:43:20,430 --> 00:43:22,800
Cars that mit.edu
site traffic.

552
00:43:23,250 --> 00:43:27,690
Click the button that says apply code,
which applies the code that you write.

553
00:43:27,720 --> 00:43:29,690
These are the parameters that you 
specify for.

554
00:43:29,691 --> 00:43:33,450
Then you'll network it,
applies those parameters,

555
00:43:33,451 --> 00:43:35,790
creates the architecture that you 
specify,

556
00:43:35,940 --> 00:43:40,940
and now you have a network written in 
javascript living in the browser ready 

557
00:43:40,940 --> 00:43:44,130
to be trained.
Then you click the blue button that says

558
00:43:44,131 --> 00:43:47,670
run training and that trains the network

559
00:43:49,030 --> 00:43:54,030
much faster than what's actually being 
visualized in the browser a thousand 

560
00:43:54,221 --> 00:43:57,700
times faster by evolving the game,
making decisions,

561
00:43:57,701 --> 00:44:00,610
taking in the grid space,
as I'll talk about here in a second,

562
00:44:00,940 --> 00:44:05,940
the speed limit is 80 miles an hour 
based on the various adjustments were 

563
00:44:05,940 --> 00:44:08,670
made to the game.
Reaching 80 miles an hour,

564
00:44:08,700 --> 00:44:13,700
certainly impossible on average and 
reaching some of the speeds that we 

565
00:44:13,700 --> 00:44:16,120
achieved last year is much,
much,

566
00:44:16,121 --> 00:44:19,180
much more difficult.
Finally,

567
00:44:19,210 --> 00:44:21,430
when you are happy and the training is 
done,

568
00:44:22,950 --> 00:44:27,950
submit the model to competition for 
those super eager,

569
00:44:28,281 --> 00:44:33,281
dedicated students.
You can do so every five minutes and to 

570
00:44:33,751 --> 00:44:38,751
visualize your submission,
you can click the request visualization,

571
00:44:40,141 --> 00:44:42,360
specifying the custom image and the 
color.

572
00:44:44,980 --> 00:44:47,110
Okay,
so here's the simulation.

573
00:44:47,140 --> 00:44:48,970
Speed limit,
80 miles an hour,

574
00:44:49,570 --> 00:44:53,860
cars 20 on the screen,
one of them is a red one in this case,

575
00:44:54,060 --> 00:44:56,320
that's that one is controlled by neural 
network.

576
00:44:56,770 --> 00:44:59,290
It's speed.
It's allowed the actions of speed up,

577
00:44:59,291 --> 00:45:01,990
slow down,
change lanes,

578
00:45:02,170 --> 00:45:05,110
left,
right or stay exactly the same.

579
00:45:09,000 --> 00:45:13,080
The other cars are pretty dumb.
They speed up,

580
00:45:13,081 --> 00:45:14,400
slow down,
turn left,

581
00:45:14,401 --> 00:45:19,401
right,
but they don't have a purpose in their 

582
00:45:19,401 --> 00:45:21,621
existence.
They do so randomly or at least purpose 

583
00:45:21,621 --> 00:45:24,480
has not been discovered.
The road,

584
00:45:24,481 --> 00:45:25,560
the car,
the speed,

585
00:45:25,620 --> 00:45:30,620
the road is a grid space and occupancy 
grid that specifies when it's empty.

586
00:45:33,300 --> 00:45:34,830
It's set to

587
00:45:35,920 --> 00:45:40,920
Ab,
meaning that the grid value is whatever 

588
00:45:42,881 --> 00:45:47,881
speed is achievable.
If you were inside that grid and when 

589
00:45:47,881 --> 00:45:52,261
there's other cars that are going slow,
the value in that grid is the speed of 

590
00:45:52,261 --> 00:45:53,530
that car.
That's the state space.

591
00:45:53,531 --> 00:45:56,920
That's the state representation and you 
can choose how much,

592
00:45:57,040 --> 00:45:59,230
what slice that state space you've 
taken.

593
00:45:59,380 --> 00:46:04,380
That's the input to the neural network.
For visualization purposes,

594
00:46:07,261 --> 00:46:12,261
you can choose normal speed or fast 
speed for watching the network operate

595
00:46:15,140 --> 00:46:20,140
and there's display options to help you 
build intuition about the network takes 

596
00:46:20,140 --> 00:46:24,251
in and what space the cars operating in.
The default is there's no extra 

597
00:46:24,321 --> 00:46:29,321
information is added.
Then there's the learning input which 

598
00:46:29,321 --> 00:46:32,891
visualizes exactly which part of the 
road the serves as the input to the 

599
00:46:32,961 --> 00:46:36,230
network.
Then there is the safety system,

600
00:46:36,440 --> 00:46:41,440
which I'll describe in a little bit,
which is all the parts of the road the 

601
00:46:41,440 --> 00:46:44,981
car is not allowed to go into because it
would result in a collision and that 

602
00:46:44,981 --> 00:46:48,710
with javascript will be very difficult 
to animate and the full map.

603
00:46:50,100 --> 00:46:55,100
Here's the safety system.
You can think of this system as ACC 

604
00:46:55,100 --> 00:47:00,050
basic radar ultrasonic sensors,
helping you avoid the obvious collisions

605
00:47:00,571 --> 00:47:03,240
to obviously detectable objects around 
you.

606
00:47:03,450 --> 00:47:07,650
And the task for this red car,
for this neural network is to move about

607
00:47:07,890 --> 00:47:09,840
this space,
uh,

608
00:47:10,190 --> 00:47:14,400
is to move about the space under the 
constraints of the safety system.

609
00:47:16,150 --> 00:47:18,710
The red shows all the parts of the 
greatest,

610
00:47:18,730 --> 00:47:23,730
not able to move into.
So the goal for the car is to not get 

611
00:47:23,730 --> 00:47:28,651
stuck in traffic,
is make big sweeping motions to avoid 

612
00:47:29,290 --> 00:47:34,290
crowds of cars.
The input like dqn is the state space.

613
00:47:36,490 --> 00:47:41,490
The output is the value of the different
actions and based on the absalon 

614
00:47:41,490 --> 00:47:46,291
parameter to training and through 
inference evaluation process,

615
00:47:47,470 --> 00:47:50,290
you choose how much exploration you want
to do.

616
00:47:50,380 --> 00:47:55,380
These are all parameters.
The learning is done in the browser on 

617
00:47:55,481 --> 00:47:56,440
your own computer,

618
00:47:59,020 --> 00:48:00,820
utilizing only the CPU,

619
00:48:02,700 --> 00:48:04,680
the action space.
There's five,

620
00:48:05,460 --> 00:48:10,460
giving you some of the variables here.
Perhaps you'd go back to the slides to 

621
00:48:10,460 --> 00:48:10,710
look at it.
The brain,

622
00:48:10,711 --> 00:48:15,711
quote unquote,
is the thing that takes in the state and

623
00:48:15,961 --> 00:48:19,730
the reward takes a forward pass to the 
state and producer.

624
00:48:19,740 --> 00:48:23,320
The next action,
the brain is where the neural can.

625
00:48:23,410 --> 00:48:26,490
It's contained both of the training and 
the evaluation.

626
00:48:27,770 --> 00:48:30,990
The learning input can be controlled in 
width,

627
00:48:32,070 --> 00:48:37,070
forward length and backward length.
Lane side number of lanes to the side 

628
00:48:37,070 --> 00:48:41,781
that you see patches ahead as the 
patches ahead that you see patches 

629
00:48:41,781 --> 00:48:46,071
behind his patches behind these.
See new this year can control the number

630
00:48:47,641 --> 00:48:52,641
of agents that are controlled by the 
neural network anywhere from one to 10,

631
00:48:58,170 --> 00:49:01,320
and the evaluation is performed exactly 
the same way.

632
00:49:01,680 --> 00:49:04,680
You have to achieve the highest average 
speed for the agents.

633
00:49:06,000 --> 00:49:11,000
The very critical thing here is the 
agents are not aware of each other so 

634
00:49:12,481 --> 00:49:17,481
they're not jointly jointly planning.
The network is trained under the joint 

635
00:49:20,880 --> 00:49:24,510
objective of achieving the average speed
for all of them,

636
00:49:25,200 --> 00:49:28,650
but the actions of taking in a greedy 
way for each.

637
00:49:29,370 --> 00:49:34,370
It's very interesting what can be 
learned in this way because this kinds 

638
00:49:34,370 --> 00:49:38,751
of approaches are scalable to an 
arbitrary number of cars and you could 

639
00:49:38,751 --> 00:49:43,191
imagine us plopping down the best cars 
from this class together and having them

640
00:49:44,881 --> 00:49:49,881
compete in this way,
the best neural networks because there 

641
00:49:51,291 --> 00:49:56,291
are full in their greedy operation.
The number of networks that can 

642
00:49:56,291 --> 00:50:00,901
concurrently operate is fully scalable.
There's a lot of parameters.

643
00:50:03,460 --> 00:50:07,600
The temporal window,
the layers,

644
00:50:07,601 --> 00:50:12,601
the many layers types that can be added 
here is a fully connected layer with 10 

645
00:50:12,601 --> 00:50:13,840
year olds.
The activation functions.

646
00:50:13,841 --> 00:50:17,710
All of these things can be customized as
specified in the tutorial,

647
00:50:18,790 --> 00:50:23,790
the final layer,
a fully connected layer with I'll put a 

648
00:50:23,790 --> 00:50:27,841
five regression given the value of each 
of the five actions and there's a lot of

649
00:50:29,750 --> 00:50:34,750
more specific parameters,
some of which I've discussed from gamma 

650
00:50:36,000 --> 00:50:37,070
to epsilon

651
00:50:38,350 --> 00:50:43,350
to experience replay size to learning 
rate and temporal window,

652
00:50:45,850 --> 00:50:48,730
the optimizer,
the learning rate momentum,

653
00:50:48,731 --> 00:50:50,970
batch size l two l,
one,

654
00:50:50,990 --> 00:50:52,990
two,
k for regularization and so on.

655
00:50:53,650 --> 00:50:58,180
There's a big white button that says 
apply code that you press that kills all

656
00:50:58,181 --> 00:51:00,820
the work you've done up to this point,
so be careful doing it.

657
00:51:00,821 --> 00:51:03,700
It should be doing it only at the very 
beginning.

658
00:51:05,580 --> 00:51:09,360
If you happen to leave your computer 
running and training for several days is

659
00:51:09,600 --> 00:51:13,020
as folks have done the blue training 
button,

660
00:51:13,080 --> 00:51:18,080
you press and it trains based on the 
parameters you specify and the network 

661
00:51:18,080 --> 00:51:20,910
state gets shipped to the main 
simulation from time to time.

662
00:51:21,060 --> 00:51:26,060
So the thing you see in the browser,
as you open up the website is running 

663
00:51:26,060 --> 00:51:30,050
the same network that's being trained 
and regularly updates that network so 

664
00:51:30,050 --> 00:51:34,881
it's getting better and better.
Even if the training takes weeks for 

665
00:51:34,881 --> 00:51:37,941
you,
it's constantly updated the network you 

666
00:51:37,941 --> 00:51:40,811
see on the left,
so if the car for the network that your 

667
00:51:40,811 --> 00:51:41,970
training is just standing in place and 
not moving,

668
00:51:42,690 --> 00:51:47,430
it's probably time to restart and change
the parameters.

669
00:51:47,520 --> 00:51:52,110
Maybe add a few layers to your network,
number of iterations,

670
00:51:52,111 --> 00:51:57,111
a certainly an important parameter to 
control and the evaluation is something 

671
00:51:58,111 --> 00:52:03,111
we've done a lot of work done since last
year to remove the degree of randomness,

672
00:52:03,271 --> 00:52:08,271
to remove the the incentive to submit 
the same code over and over again to 

673
00:52:09,151 --> 00:52:13,050
hope to produce a higher award,
a higher evaluation score.

674
00:52:14,580 --> 00:52:19,580
The method for evaluation is we collect 
the average speed over 10 runs,

675
00:52:19,950 --> 00:52:24,950
about 45 seconds of game each,
not minutes,

676
00:52:26,400 --> 00:52:31,400
45 seconds,
and there is five hundreds of those and 

677
00:52:31,400 --> 00:52:35,490
we take the median speed of the 500 runs
has done server side.

678
00:52:35,550 --> 00:52:38,670
It's extremely difficult to cheat.
I urge you to try.

679
00:52:39,870 --> 00:52:43,470
You can try it locally.
There's a start evaluation run,

680
00:52:43,471 --> 00:52:46,200
but that one doesn't count.
That's just for you to feel better about

681
00:52:46,201 --> 00:52:48,780
your network.
That's.

682
00:52:48,810 --> 00:52:52,140
That should produce a result that's very
similar to the one we were produced on.

683
00:52:52,141 --> 00:52:57,141
The server is to build your own 
intuition and as I said,

684
00:52:57,301 --> 00:52:59,490
was significantly reduce the influence 
of randomness,

685
00:52:59,820 --> 00:53:04,820
so the the score of the speed you get 
for the network you designed should be 

686
00:53:05,010 --> 00:53:10,010
very similar with every evaluation.
Loading is saving.

687
00:53:10,620 --> 00:53:13,350
If the network is huge and you want to 
switch computers,

688
00:53:13,351 --> 00:53:18,351
you can save the network.
It saves both the architecture of the 

689
00:53:18,351 --> 00:53:21,021
network and the weights and the on the 
network and you can load it back in.

690
00:53:22,950 --> 00:53:27,950
Obviously when you load it in,
it's not a saving any of the data you've

691
00:53:28,591 --> 00:53:33,591
already done.
You can't do transfer learning with 

692
00:53:33,591 --> 00:53:35,640
javascript in the browser yet submitting
your network,

693
00:53:35,641 --> 00:53:39,720
submit model to competition and make 
sure you run training first.

694
00:53:39,810 --> 00:53:44,810
Otherwise it will be initiated or 
initiated randomly and will not do so 

695
00:53:44,810 --> 00:53:47,370
well.
You can resubmit as often as you like,

696
00:53:47,371 --> 00:53:52,371
and the highest score is what counts.
The coolest part is you can load your 

697
00:53:52,371 --> 00:53:54,570
custom image,
specify colors,

698
00:53:54,630 --> 00:53:56,220
and request the visualization.

699
00:53:57,490 --> 00:54:02,490
We have not yet shown the visualization,
but I promise you it's going to be 

700
00:54:02,490 --> 00:54:03,580
awesome.
Again,

701
00:54:03,850 --> 00:54:06,850
read the tutorial,
change the parameters and the code box.

702
00:54:06,880 --> 00:54:11,880
Click apply code run training.
Everybody in this room on the way home 

703
00:54:11,880 --> 00:54:14,350
on the train,
hopefully not in your car,

704
00:54:14,410 --> 00:54:19,410
should be able to do this in the browser
and they can visualize request 

705
00:54:19,410 --> 00:54:20,530
visualization because it's an expensive 
process.

706
00:54:20,680 --> 00:54:25,630
You have to want it for us to do it well
because we have to run in server side.

707
00:54:28,030 --> 00:54:31,840
Competition link is their get hub 
started.

708
00:54:31,841 --> 00:54:35,920
Code is there and the details for those 
that truly want to win is in the archive

709
00:54:35,921 --> 00:54:40,921
paper.
So the question that will come up 

710
00:54:40,921 --> 00:54:44,130
throughout is whether these 
reinforcement learning approaches are at

711
00:54:44,131 --> 00:54:48,720
all or rather if action planning control
is amenable to learning.

712
00:54:50,510 --> 00:54:53,340
Certainly in the case of driving,
we can't do it.

713
00:54:53,341 --> 00:54:58,341
Alpha go zero did.
We can't learn from scratch from self 

714
00:54:59,331 --> 00:55:04,331
play because that will result in 
millions of crashes in order to learn to

715
00:55:06,621 --> 00:55:11,621
avoid the crashes.
Unless we're working like we are deep 

716
00:55:11,621 --> 00:55:15,161
crashed on the RC car or we're working 
on a simulation so we can look at expert

717
00:55:15,741 --> 00:55:17,330
data,
we can look at driver data,

718
00:55:17,331 --> 00:55:20,110
which we have a lot of and learn from.
It's an open question.

719
00:55:20,260 --> 00:55:25,260
This is applicable to date that I'll 
bring up two companies because they're 

720
00:55:25,811 --> 00:55:26,980
both guest speakers

721
00:55:28,420 --> 00:55:33,420
deep irl is not involved in the most 
successful robots operating in the real 

722
00:55:33,420 --> 00:55:37,500
world.
In the case of Boston dynamics,

723
00:55:38,550 --> 00:55:43,550
most of the perception control and 
planning I can.

724
00:55:43,841 --> 00:55:48,841
This robot does not involve learning 
approaches except with minimal addition 

725
00:55:49,861 --> 00:55:54,861
on the perception side.
Best of our knowledge and certainly the 

726
00:55:56,311 --> 00:55:59,490
same is true with Waymo.
As the speaker on Friday,

727
00:55:59,491 --> 00:56:04,110
we'll talk about deep learning is used a
little bit in perception on top,

728
00:56:04,350 --> 00:56:09,350
but most of the work is done from the 
sensors and the optimization based,

729
00:56:10,141 --> 00:56:15,141
the model based approaches,
trajectory generation and optimizing 

730
00:56:15,141 --> 00:56:18,120
which trajectory trajectory is best to 
avoid collisions.

731
00:56:19,280 --> 00:56:24,130
Deep Arrows not involved and coming

732
00:56:24,130 --> 00:56:29,130
back and back again.
The unexpected local is a higher award 

733
00:56:29,130 --> 00:56:31,990
which arise in all of these situations 
and applied in the real world.

734
00:56:32,680 --> 00:56:37,680
So for the cat video,
that's pretty short where the cats are 

735
00:56:37,680 --> 00:56:41,620
ringing the bell and they're learning 
that the ringing of the bell is,

736
00:56:41,980 --> 00:56:46,980
is mapping to food.
I urge you to think about how that can 

737
00:56:47,681 --> 00:56:52,681
evolve over time in unexpected ways that
may not have a desirable effect where 

738
00:56:53,591 --> 00:56:58,591
the final reward is in the form of food 
and the intended effect is to ring the 

739
00:56:59,711 --> 00:57:00,250
bell.

740
00:57:03,390 --> 00:57:08,390
That's where ai safety comes in for the 
artificial general intelligence course 

741
00:57:08,390 --> 00:57:12,591
in two weeks.
That's something we'll explore 

742
00:57:12,591 --> 00:57:14,631
extensively.
It's how these reinforcement learning 

743
00:57:16,110 --> 00:57:21,110
planning algorithms will evolve in ways 
that not expected and how we can 

744
00:57:22,291 --> 00:57:27,291
constrain them,
how we can design reward functions that 

745
00:57:27,291 --> 00:57:31,311
result in safe operation.
So I encourage you to come to the talk 

746
00:57:32,190 --> 00:57:35,090
on Friday at 1:00
PM as a reminder.

747
00:57:35,100 --> 00:57:36,750
So 1:00
PM that 7:00

748
00:57:36,750 --> 00:57:38,430
PM instead of 32,
one,

749
00:57:38,431 --> 00:57:39,030
two,
three,

750
00:57:39,360 --> 00:57:44,360
and two,
the awesome talks in two weeks from 

751
00:57:44,360 --> 00:57:46,140
Boston dynamics to Ray Kurzweil and so 
on for Agi.

752
00:57:46,830 --> 00:57:51,030
Now tomorrow we'll talk about computer 
vision and psych fuse.

753
00:57:51,300 --> 00:57:51,930
Thank you everybody.

