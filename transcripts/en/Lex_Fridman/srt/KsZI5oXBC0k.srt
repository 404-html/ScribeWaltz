1
00:00:00,140 --> 00:00:02,630
The following is a conversation
with Stuart Russell.

2
00:00:03,080 --> 00:00:07,850
He's a professor of computer science
at UC Berkeley and a coauthor of a book

3
00:00:07,851 --> 00:00:12,140
that introduced me and millions of
other people to the amazing world of AI

4
00:00:12,560 --> 00:00:15,140
called artificial intelligence
and modern approach.

5
00:00:15,740 --> 00:00:20,740
So it was an honor for me to have this
conversation as part of MIT course and

6
00:00:21,591 --> 00:00:25,280
artificial general intelligence and
the artificial intelligence podcasts.

7
00:00:25,790 --> 00:00:29,390
If you enjoy it, please
subscribe on Youtube, Itunes,

8
00:00:29,420 --> 00:00:31,070
or your podcast provider of choice,

9
00:00:31,520 --> 00:00:35,960
or simply connect with me on Twitter
at Lex Friedman, spelled f, r I. D.

10
00:00:36,480 --> 00:00:40,200
And now here's my conversation
with Stuart Russell.

11
00:00:41,570 --> 00:00:45,470
So you've mentioned in
1975 in high school,

12
00:00:45,471 --> 00:00:48,620
you've created one year first
AI programs that play chess,

13
00:00:50,130 --> 00:00:55,130
where you ever able to build a program
that beat you at chess or another board

14
00:00:57,501 --> 00:01:02,440
game? Uh, so my program
never beat me at chess.

15
00:01:03,680 --> 00:01:07,150
I actually wrote the
program at Imperial College.

16
00:01:07,230 --> 00:01:12,230
I used to take the bus every Wednesday
with a box of cards this big,

17
00:01:12,870 --> 00:01:13,610
uh,

18
00:01:13,610 --> 00:01:17,390
and shoved them into the card reader and
they gave us eight seconds of CPU time.

19
00:01:18,900 --> 00:01:23,550
It took about five seconds to read
the cards in and compile the code.

20
00:01:23,551 --> 00:01:27,960
So we had three seconds, a CPU time,
which was enough to make one move,

21
00:01:28,290 --> 00:01:30,030
you know,
with a not very deep search.

22
00:01:30,690 --> 00:01:33,240
And then we would print that move out
and then we have to go to the back of the

23
00:01:33,241 --> 00:01:37,870
queue and wait to feed the cards. And
again, how deep was the search? Well,

24
00:01:37,900 --> 00:01:41,660
I were talking about the move to move.
So now I think we got a,

25
00:01:41,670 --> 00:01:42,900
we got an eight move,

26
00:01:43,090 --> 00:01:48,090
eight depth fleet with Alpha Beta and we
had some tricks of our own about a move

27
00:01:50,041 --> 00:01:52,350
ordering and some pruning of the tree.
And,

28
00:01:52,940 --> 00:01:55,920
but you were still able to
beat that program? Yeah, yeah.

29
00:01:55,921 --> 00:01:58,380
I was a reasonable chess
player in my youth.

30
00:01:59,040 --> 00:02:03,030
I did an a fellow program,
uh, and a backgammon program.

31
00:02:03,240 --> 00:02:04,170
So when I got to Berkeley,

32
00:02:04,171 --> 00:02:08,710
I worked a lot on what
we call Meta reasoning,

33
00:02:08,740 --> 00:02:11,750
which really means
reasoning about reasoning.

34
00:02:11,751 --> 00:02:14,620
And in the case of a game playing program,

35
00:02:15,250 --> 00:02:18,790
you need to reason about what parts of
the search tree you're actually going to

36
00:02:18,791 --> 00:02:22,620
explore. Because the search
tree is enormous, you know,

37
00:02:22,630 --> 00:02:26,290
bigger than the number of atoms
in the universe. And, and uh,

38
00:02:27,040 --> 00:02:31,900
the way programs succeed and the way human
succeed is by only looking at a small

39
00:02:31,901 --> 00:02:35,970
fraction of the search tree.
And if you look at the right fraction,

40
00:02:35,980 --> 00:02:38,530
you played really well.
If you look at the wrong fraction,

41
00:02:38,800 --> 00:02:42,520
if you waste your time thinking about
things that are never going to happen,

42
00:02:42,521 --> 00:02:46,430
the moves and no one's ever going to make,
then you're going to lose cause you,

43
00:02:46,550 --> 00:02:49,410
you won't be able to figure
out the right decision.

44
00:02:50,690 --> 00:02:55,520
So that question of how machines can
manage their own computation, how they,

45
00:02:55,670 --> 00:02:59,920
how they decide what to think
about is the reasoning question.

46
00:02:59,921 --> 00:03:04,921
And we developed some methods for doing
that and very simply the machine should

47
00:03:05,471 --> 00:03:10,150
think about whatever thoughts are
going to improve its decision quality.

48
00:03:10,720 --> 00:03:14,110
We were able to show
that both for a fellow,

49
00:03:14,111 --> 00:03:17,680
which is a standard to play game and uh,
for backgammon,

50
00:03:17,681 --> 00:03:22,030
which includes a dice roll.
So it's a to play a game with uncertainty.

51
00:03:22,870 --> 00:03:23,860
For both of those cases,

52
00:03:23,861 --> 00:03:28,861
we could come up with algorithms that
were actually much more efficient than the

53
00:03:28,871 --> 00:03:31,240
standard Alpha Bita search,
uh,

54
00:03:31,241 --> 00:03:36,241
which chest programs at the time were
using and that those programs could beat

55
00:03:36,461 --> 00:03:37,294
me.

56
00:03:38,260 --> 00:03:43,260
And I think you can see the same basic
ideas in Alphago and Alpha zero today.

57
00:03:46,180 --> 00:03:51,180
The way they explore the tree is using a
form of matter reasoning to select what

58
00:03:53,051 --> 00:03:55,930
to think about based on how
useful it is to think about it.

59
00:03:57,010 --> 00:04:02,010
Is there any insights you can describe
without Greek symbols of how do we select

60
00:04:02,951 --> 00:04:07,390
which paths to go down? There's
really two kinds of learning going on.

61
00:04:07,391 --> 00:04:12,290
So as you say,
Alphago learns to evaluate board position.

62
00:04:12,291 --> 00:04:12,820
So it can,

63
00:04:12,820 --> 00:04:17,820
it can look at a goal board ended actually
has powerfully a superhuman ability

64
00:04:19,510 --> 00:04:22,540
to instantly tell how
promising that situation is.

65
00:04:24,370 --> 00:04:27,370
To me, the amazing thing
about Alphago is not that

66
00:04:29,140 --> 00:04:33,160
it can be the world champion, whether
its hands tied behind his back, but,

67
00:04:33,760 --> 00:04:34,750
uh,
the fact that

68
00:04:36,340 --> 00:04:40,360
if you stop it from searching
altogether, Susie, okay,

69
00:04:40,361 --> 00:04:42,820
you're not allowed to do
any thinking ahead, right?

70
00:04:42,820 --> 00:04:47,680
You can just consider each of your
legal moves in and look at the resulting

71
00:04:47,681 --> 00:04:51,580
situation and evaluate it. So what
we call a, a depth of one search.

72
00:04:51,820 --> 00:04:55,660
So just the immediate outcome of your
moods and decide if that's good or bad.

73
00:04:56,260 --> 00:04:58,150
That version of Alphago

74
00:05:00,040 --> 00:05:03,100
can still play at a professional level,
right?

75
00:05:03,101 --> 00:05:05,620
And human professionals
is sitting there for five,

76
00:05:05,621 --> 00:05:09,550
10 minutes deciding what to do
and Alphago in less than a second

77
00:05:11,050 --> 00:05:15,820
can't instantly intuit what is the right
move to make based on its ability to

78
00:05:15,821 --> 00:05:19,640
evaluate positions. Um,
and that is remarkable. Um,

79
00:05:19,840 --> 00:05:24,280
because you know, we don't have
that level of intuition about go,

80
00:05:24,281 --> 00:05:28,720
we actually have to think
about the situation. So anyway,

81
00:05:28,721 --> 00:05:31,020
that capability that,
um,

82
00:05:31,420 --> 00:05:35,380
Alphago has is one big part
of why it beats humans.

83
00:05:35,840 --> 00:05:38,890
The other big part is that
it's able to look ahead.

84
00:05:40,360 --> 00:05:44,860
40 50 60 moves into the future.
And

85
00:05:46,420 --> 00:05:49,360
you know,
if it was considering all possibilities,

86
00:05:49,390 --> 00:05:53,760
40 or 50 or 60 moves into the
future, that would be, you know,

87
00:05:53,830 --> 00:05:58,420
10 to the 200 possibilities. So way,

88
00:05:58,421 --> 00:06:02,350
way more than atoms in the
universe and and so on.

89
00:06:02,351 --> 00:06:05,970
So eats very,
very selective about what it looks at.

90
00:06:07,970 --> 00:06:12,970
So let me try to give you an intuition
about how you decide what to think about

91
00:06:13,740 --> 00:06:17,490
is a combination of two things.
One is how promising it is,

92
00:06:18,630 --> 00:06:22,310
right? So if you're already
convinced that a movie is terrible,

93
00:06:22,790 --> 00:06:26,690
there's no point spending a lot more time
convincing yourself that it's terrible

94
00:06:27,680 --> 00:06:31,100
because it's powerfully not going
to change your mind. So the,

95
00:06:31,490 --> 00:06:35,570
the real reason you think it's because
there's some possibility of changing your

96
00:06:35,571 --> 00:06:37,820
mind about what to do,
right?

97
00:06:38,240 --> 00:06:41,750
And is that changing of mind
that would result then in,

98
00:06:41,751 --> 00:06:44,720
in a better final action
in the real world.

99
00:06:44,720 --> 00:06:49,280
So that's the purpose of thinking is
to improve the final action in the real

100
00:06:49,281 --> 00:06:50,060
world.

101
00:06:50,060 --> 00:06:55,060
And so if you think about a move
that is guaranteed to be terrible,

102
00:06:55,610 --> 00:06:57,080
you can convince yourself as terrible.

103
00:06:57,081 --> 00:07:01,370
And you're still not going to change
your mind, right? But on the other hand,

104
00:07:01,371 --> 00:07:03,530
you were suppose you had a
choice between two moves.

105
00:07:03,620 --> 00:07:07,760
One of them you've already figured out
is guaranteed to be a draw, let's say.

106
00:07:08,270 --> 00:07:11,030
And then the other one
looks a little bit worse.

107
00:07:11,031 --> 00:07:14,180
Like it looks fairly likely that if you
make that move you're going to lose.

108
00:07:14,750 --> 00:07:19,190
But there's still some uncertainty
about the value of that move.

109
00:07:19,520 --> 00:07:23,080
There is still some possibility that
it will turn out to be a win. Right?

110
00:07:23,150 --> 00:07:24,890
Then it's worth thinking about that.

111
00:07:25,070 --> 00:07:29,210
So even though it's less promising
on average than the other move,

112
00:07:29,211 --> 00:07:30,500
which is guaranteed to be a draw,

113
00:07:30,950 --> 00:07:33,950
there's still some purpose in thinking
about it because there's a chance that

114
00:07:33,951 --> 00:07:37,610
you will change your mind and and
discover that in fact it's a better move.

115
00:07:37,940 --> 00:07:42,890
So it's a combination of how good
the move appears to be and how much

116
00:07:42,891 --> 00:07:46,640
uncertainty there is about its value.
The more uncertainty,

117
00:07:46,910 --> 00:07:50,690
the more it's worth thinking
about because as a higher upside,

118
00:07:50,691 --> 00:07:53,720
if you want to think of it that way.
And of course in the beginning

119
00:07:54,850 --> 00:07:57,510
machine,
the Alphago zero formulation,

120
00:07:57,990 --> 00:08:00,900
it's everything is
shrouded in uncertainty.

121
00:08:01,320 --> 00:08:06,030
So you really swimming in a sea of,
uh, uncertainty. So it had benefits.

122
00:08:06,031 --> 00:08:10,080
You too, I mean actually following
the same process as you described,

123
00:08:10,090 --> 00:08:12,090
but because you're so
uncertain about everything,

124
00:08:12,540 --> 00:08:15,360
you basically have to try a
lot of different directions.

125
00:08:15,440 --> 00:08:19,930
Yeah. So, so the, the early parts
of the search tree, a fairly bushy,

126
00:08:20,400 --> 00:08:23,570
um, that it will, it will look in
a lot of different possibilities,

127
00:08:23,571 --> 00:08:28,190
but fairly quickly the degree of
certainty about some of the moves. I mean,

128
00:08:28,191 --> 00:08:31,490
if a movie's really terrible, you'll
pretty quickly find out, right?

129
00:08:31,491 --> 00:08:35,690
You lose half your pieces or half your
territory and uh, and then you'll say,

130
00:08:35,691 --> 00:08:38,870
okay, this, this is not worth
thinking about anymore. And then,

131
00:08:38,871 --> 00:08:43,871
so a further down the tree
becomes very long and narrow and,

132
00:08:43,881 --> 00:08:47,090
uh, you're, you're following
various lines of play.

133
00:08:49,000 --> 00:08:53,020
10, 20, 30, 40, 50 moves
into the future. And, um,

134
00:08:54,070 --> 00:08:58,320
that's again, it's something that the
human beings have a very hard time doing.

135
00:08:58,700 --> 00:09:02,100
Uh, mainly because they just
lack the short term memory.

136
00:09:02,610 --> 00:09:07,300
You just can't remember a sequence
of moves that's 50 movies long. Uh,

137
00:09:07,320 --> 00:09:11,500
and you can't do, you can't
imagine the board correctly, uh,

138
00:09:11,520 --> 00:09:14,520
for that money moves into the future.
Of course the top

139
00:09:15,080 --> 00:09:19,640
players, I'm much more familiar with
chess, but the top players probably have,

140
00:09:20,010 --> 00:09:25,010
they have echoes of the same kind of
intuition instinct that in a moment's time

141
00:09:25,420 --> 00:09:28,760
alpha go applies when they see a board.
I mean,

142
00:09:28,761 --> 00:09:32,780
they've seen those patterns. Human
beings have seen those patterns before.

143
00:09:33,170 --> 00:09:35,150
At the top of the grand master level,

144
00:09:35,630 --> 00:09:38,250
it seems that there is some

145
00:09:40,010 --> 00:09:42,300
similarities or maybe it's,

146
00:09:42,301 --> 00:09:45,710
it's our imagination creates a
vision of those similarities.

147
00:09:45,711 --> 00:09:50,711
But it feels like this kind of pattern
recognition that the Alphago approaches

148
00:09:51,680 --> 00:09:55,910
are using is similar to what human
beings at the top level or using.

149
00:09:56,780 --> 00:10:00,830
I think there's, uh,
there's some truth to that,

150
00:10:01,720 --> 00:10:04,370
but not entirely. I mean, I think the,

151
00:10:04,400 --> 00:10:09,400
the extent to which human grandmaster
can reliably rec instantly recognize the

152
00:10:11,611 --> 00:10:14,640
right move and instantly recognize
the value of a position. Uh,

153
00:10:14,660 --> 00:10:17,070
I think that's a little bit overrated.

154
00:10:17,490 --> 00:10:19,980
But if you sacrifice a queen for example,
I mean there's these,

155
00:10:20,430 --> 00:10:23,220
there's these beautiful games
of chess with Bobby Fischer,

156
00:10:23,221 --> 00:10:28,080
somebody where it's
seeming to make a bad move.

157
00:10:28,500 --> 00:10:33,500
And I'm not sure there's a perfect degree
of calculation involved where they've

158
00:10:34,351 --> 00:10:38,340
calculated all the possible thing that
happened. But there's an instinct there,

159
00:10:38,370 --> 00:10:40,390
right?
That somehow adds up to,

160
00:10:42,230 --> 00:10:44,410
yeah, I think what happens is you, you,

161
00:10:44,520 --> 00:10:48,650
you get a sense that there's some
possibility in the position, uh,

162
00:10:48,920 --> 00:10:52,930
even if you make a weird looking
move, um, that it opens up some,

163
00:10:54,310 --> 00:10:55,430
some lines of,

164
00:10:57,490 --> 00:11:02,410
of calculation that otherwise
would be, uh, definitely bad. And,

165
00:11:02,411 --> 00:11:03,244
um,

166
00:11:03,280 --> 00:11:08,280
and is that intuition that
there's something here in
this position that might,

167
00:11:10,960 --> 00:11:15,940
uh, might yield at when don the site
and then you follow that, right?

168
00:11:15,941 --> 00:11:17,290
And,
and in some sense when a,

169
00:11:17,291 --> 00:11:22,291
when a chess player is following a line
in his or her mind there they mentally

170
00:11:23,621 --> 00:11:27,520
simulating what the other person is going
to do while the opponent is going to

171
00:11:27,521 --> 00:11:32,470
do. And they can do that as long
as the moves a kind of forced,

172
00:11:33,040 --> 00:11:34,950
right. As long as there's,
you know, there's a,

173
00:11:35,340 --> 00:11:38,950
a fort we call a forcing variation where
the opponent doesn't really have much

174
00:11:38,951 --> 00:11:42,820
choice how to respond and then you see
if you can force them into a situation

175
00:11:42,821 --> 00:11:46,590
where you win. You know, we
see plenty of mistakes, uh,

176
00:11:46,650 --> 00:11:51,650
even even in grandmaster games where
they just miss some simple three,

177
00:11:52,781 --> 00:11:56,420
four, five move, a
combination that, you know,

178
00:11:56,421 --> 00:12:00,490
it wasn't particularly apparent in,
in the position but was still there.

179
00:12:01,090 --> 00:12:02,920
That's the thing that makes us human.
Yup.

180
00:12:03,520 --> 00:12:06,850
So when you mentioned that in Othello,

181
00:12:06,851 --> 00:12:11,851
those games or after some matter reasoning
improvements and research was able to

182
00:12:12,011 --> 00:12:14,590
beat you,
how did that make you feel?

183
00:12:15,040 --> 00:12:18,720
Part of the metal reasoning
capability that it had? Um,

184
00:12:18,770 --> 00:12:22,030
[inaudible] was based on learning and,
um,

185
00:12:23,410 --> 00:12:27,430
and you could sit down the next day and
you could just feel that it had got a

186
00:12:27,431 --> 00:12:32,210
lot smarter and all of a sudden
you really felt like, you know,

187
00:12:32,211 --> 00:12:36,400
sort of pressed against
the wall because a, it was,

188
00:12:36,460 --> 00:12:40,390
it was much more aggressive and,
and was totally unforgiving of,

189
00:12:40,391 --> 00:12:44,140
of any minor mistake that you
might make. And, uh, and Ashley's,

190
00:12:44,800 --> 00:12:47,920
it seemed understood the
game better than I did.

191
00:12:47,930 --> 00:12:52,000
And Garry Kasparov has
this quote where it, um,

192
00:12:52,120 --> 00:12:54,280
during his match against deep blue,

193
00:12:54,460 --> 00:12:58,000
he said he suddenly felt that there was
a new kind of intelligence across the

194
00:12:58,001 --> 00:12:58,834
board.

195
00:13:00,250 --> 00:13:03,970
Do you think that's a scary
or an exciting possibility?

196
00:13:04,720 --> 00:13:09,070
Kasparov and for yourself
in the context of chess?

197
00:13:09,100 --> 00:13:14,050
Purely sort of in this, like
that feeling, whatever that is.

198
00:13:14,320 --> 00:13:19,180
I think it's definitely an exciting
feeling. You know, this is what made me

199
00:13:20,800 --> 00:13:22,510
work on AI in the first place,

200
00:13:22,511 --> 00:13:24,670
was as soon as I really
understood what a computer was,

201
00:13:24,671 --> 00:13:28,750
they wanted to make it smart.
You know, I started out with,

202
00:13:29,340 --> 00:13:34,080
the first program I wrote was for the
sink layer programmable calculator. Uh,

203
00:13:34,090 --> 00:13:37,970
and I think you could right at 21 step,
uh,

204
00:13:38,020 --> 00:13:42,560
algorithm that was the biggest program.
You could write something like that. Um,

205
00:13:42,670 --> 00:13:44,960
and do a little arithmetic calculation.

206
00:13:44,961 --> 00:13:48,570
So I sent think are implemented
Newton's method for square roots,

207
00:13:48,910 --> 00:13:53,150
a few other things like that. Um,
but then, you know, I thought, okay,

208
00:13:53,330 --> 00:13:57,310
if I just had more space,
I could make this thing intelligent.

209
00:13:58,960 --> 00:14:02,290
And so I started thinking about Ai and,

210
00:14:04,350 --> 00:14:06,010
and I think the,
the,

211
00:14:06,011 --> 00:14:09,820
the thing that's scary is not,

212
00:14:10,030 --> 00:14:14,640
is not the chess program
because, you know,

213
00:14:14,641 --> 00:14:19,540
chess programs, they're not in the
taking over the world business. Uh,

214
00:14:19,570 --> 00:14:24,550
but
if you extrapolate,

215
00:14:25,870 --> 00:14:30,820
you know, there are things about chess
that don't resemble the real world, right?

216
00:14:30,910 --> 00:14:32,740
We know,
we know the rules of chess.

217
00:14:35,170 --> 00:14:39,730
The chess board is completely visible
to the program where, of course,

218
00:14:39,731 --> 00:14:41,140
the real world is not most,

219
00:14:41,440 --> 00:14:44,920
most of the real world is he's not
visible from wherever you're sitting,

220
00:14:45,310 --> 00:14:48,880
so to speak. And, uh,

221
00:14:50,470 --> 00:14:52,700
to overcome those kinds of problems,

222
00:14:52,730 --> 00:14:55,670
you need qualitatively
different algorithms.

223
00:14:56,300 --> 00:15:00,500
Another thing about the real world
is that, you know, we, we regularly

224
00:15:02,240 --> 00:15:07,240
plan ahead on the timescales involving
billions or trillions of steps.

225
00:15:08,180 --> 00:15:12,230
Um, now we don't plan those
in detail, but you know,

226
00:15:12,231 --> 00:15:15,230
when you choose to do a phd at Berkeley,

227
00:15:16,190 --> 00:15:20,780
that's a five year commitment and that
amounts to about a trillion motor control

228
00:15:20,990 --> 00:15:25,990
steps that you will eventually
be committed to including
going up the stairs,

229
00:15:26,330 --> 00:15:29,690
opening doors, drinking water type. Yeah,

230
00:15:29,691 --> 00:15:33,890
I mean every finger movement while you're
typing every character of every paper

231
00:15:33,891 --> 00:15:34,960
and the thesis and everything.

232
00:15:35,120 --> 00:15:38,300
So you're not committing and advanced
to the specific motor control steps,

233
00:15:38,301 --> 00:15:43,301
but you're still reasoning on a
timescale that will eventually reduce to

234
00:15:44,570 --> 00:15:48,710
trillions of motor control actions
and a,

235
00:15:48,711 --> 00:15:52,420
so for all of these reasons,
you know,

236
00:15:52,490 --> 00:15:57,490
Alphago and deep blue and so on don't
represent any kind of threat to humanity,

237
00:15:58,131 --> 00:16:00,830
but they are a step towards it,
right?

238
00:16:01,520 --> 00:16:03,680
And progress in AI

239
00:16:05,240 --> 00:16:08,090
occurs by essentially removing one by one.

240
00:16:08,360 --> 00:16:11,870
These assumptions that make problems easy,

241
00:16:11,871 --> 00:16:16,190
like the assumption of complete
observability of the situation, right?

242
00:16:16,191 --> 00:16:17,950
We remove that assumption.

243
00:16:17,960 --> 00:16:22,790
You need a much more complicated
kind of computing design.

244
00:16:22,791 --> 00:16:23,170
It needs,

245
00:16:23,170 --> 00:16:26,240
it needs something that actually keeps
track of all the things you can't see and

246
00:16:26,241 --> 00:16:28,840
tries to estimate what's going on.
Uh,

247
00:16:28,850 --> 00:16:31,550
and there's inevitable
uncertainty in that,

248
00:16:31,551 --> 00:16:34,970
so it becomes a much more
complicated problem. But you know,

249
00:16:34,971 --> 00:16:37,580
we are removing those assumptions.

250
00:16:37,581 --> 00:16:42,140
We are starting to have algorithms that
can cope with much longer timescales.

251
00:16:42,500 --> 00:16:45,890
They can cope with uncertainty that
can cope with partial observability.

252
00:16:47,660 --> 00:16:52,660
And so each of those steps
sort of magnifies by a
thousand the range of things

253
00:16:53,961 --> 00:16:57,030
that we can do with AI systems.
So the way I started in a,

254
00:16:57,040 --> 00:17:00,210
I wanted to be a psychiatrist for a long
time and energy and the mine in high

255
00:17:00,211 --> 00:17:05,060
school and of course program
and so on. And I showed up, uh,

256
00:17:05,061 --> 00:17:09,470
university of Illinois to an
AI lab and they said, okay,

257
00:17:09,471 --> 00:17:13,670
I don't have time for you, but
here's a book, Ai, a modern approach.

258
00:17:13,700 --> 00:17:18,440
I think it was the first edition at the
time. Here it go, go, go learn this.

259
00:17:18,650 --> 00:17:22,940
And I remember the lay of the land was
all, it's incredible that we solved chess,

260
00:17:22,941 --> 00:17:24,260
but we'll never solve go.

261
00:17:24,650 --> 00:17:29,650
I mean it was pretty certain that go in
the way we thought about systems that

262
00:17:30,201 --> 00:17:34,460
reason was impossible to solve
and now solve this as a very,

263
00:17:34,760 --> 00:17:39,760
I think I would have said
that it's unlikely we could
take the kind of algorithm

264
00:17:41,211 --> 00:17:46,211
that was used for chess and just get
it to scale up a and work well for go

265
00:17:48,840 --> 00:17:53,840
and at the time what we thought
was that in order to solve go,

266
00:17:56,191 --> 00:18:01,191
we would have to do something similar
to the way humans manage the complexity

267
00:18:01,411 --> 00:18:04,950
you go, which is to break it
down into kind of sub games.

268
00:18:04,950 --> 00:18:07,090
So when a human thinks about a go board,

269
00:18:07,091 --> 00:18:11,760
they think about different parts of the
board as sort of weakly connected to

270
00:18:11,761 --> 00:18:14,910
each other and they think about,
okay, within this part of the board,

271
00:18:15,150 --> 00:18:18,930
here's how things could go in that
part of what his, how things could go.

272
00:18:19,290 --> 00:18:23,820
And then you try to sort of couple those
two analyses together and deal with the

273
00:18:23,821 --> 00:18:27,120
interactions and maybe revise your views
of how things are going to go in each

274
00:18:27,121 --> 00:18:31,230
park in. Then you've got maybe five,
six, seven, 10 parts of the board.

275
00:18:31,920 --> 00:18:32,770
And,
um,

276
00:18:33,480 --> 00:18:38,480
that actually resembles the real world
much more than Chester's because in the

277
00:18:40,231 --> 00:18:44,280
real world, you know, we
have work, we have home life,

278
00:18:44,310 --> 00:18:48,930
we have sport, you know, whatever
different kinds of activities, you know,

279
00:18:48,931 --> 00:18:53,400
shopping.
These all are connected to each other,

280
00:18:53,401 --> 00:18:58,230
but they're weakly connected. So
when I'm typing a paper, you know,

281
00:18:58,710 --> 00:19:02,700
I don't simultaneously have
to decide which order I'm
going to get the, you know,

282
00:19:02,701 --> 00:19:07,230
the milk and the butter, you know,
that doesn't affect the typing.

283
00:19:07,920 --> 00:19:09,210
But I do need to realize,
okay,

284
00:19:09,211 --> 00:19:12,660
but if an this before the shops close
is cause I don't have any ticket.

285
00:19:12,680 --> 00:19:14,250
You don't have any food
at home. Right, right.

286
00:19:14,251 --> 00:19:19,251
So there's some weak connection but
not in the way that chess works where

287
00:19:19,651 --> 00:19:23,040
everything is tied into a
single stream of thought.

288
00:19:23,490 --> 00:19:26,520
So the thought was that go to solve go,

289
00:19:26,521 --> 00:19:29,460
we'd have to make progress on stuff
that will be useful for the real world.

290
00:19:29,461 --> 00:19:33,660
And in a way Alpha goes a little bit
disappointing, right? Because the,

291
00:19:33,990 --> 00:19:38,610
the program designed for Alpha was
actually not that different from,

292
00:19:40,830 --> 00:19:42,020
from deep blue or,

293
00:19:42,180 --> 00:19:46,080
or even from Arthur Samuel's checker
playing program for the 1950s.

294
00:19:48,330 --> 00:19:52,150
And in fact, the, so the two things
that make Alphago work is one,

295
00:19:52,151 --> 00:19:55,320
one is his amazing ability to
ability to evaluate the positions.

296
00:19:55,590 --> 00:19:59,670
And the other is the Metta reasoning
capability, which, which allows it to,

297
00:20:00,750 --> 00:20:05,750
to explore some paths in the tree very
deeply into abandoned other paths very

298
00:20:06,121 --> 00:20:09,630
quickly. So this word
matter reasoning, uh,

299
00:20:09,660 --> 00:20:13,930
while technically correct
inspires perhaps the,

300
00:20:14,030 --> 00:20:18,840
the wrong degree of power
that Alphago has. For example,

301
00:20:18,841 --> 00:20:23,700
the word reasonings as a powerful word.
So let me ask you sort of the,

302
00:20:23,701 --> 00:20:27,790
you were part of the
symbolic AI world for awhile.

303
00:20:27,950 --> 00:20:30,990
Like learn AI was, uh,
there's a lot of excellent,

304
00:20:30,991 --> 00:20:35,991
interesting ideas there that
unfortunately met I winter.

305
00:20:36,780 --> 00:20:41,000
And so it,
do you think it re-emerges oh,

306
00:20:41,020 --> 00:20:45,240
so I would say, yeah, it's not
quite as simple as that. So the,

307
00:20:45,960 --> 00:20:47,040
the Ai Winter,

308
00:20:50,050 --> 00:20:54,610
the first winter that was actually
named as such was the one in the late

309
00:20:54,610 --> 00:20:59,450
eighties. And that came about because, uh,

310
00:20:59,470 --> 00:21:01,390
in the mid eighties,
there was a

311
00:21:03,280 --> 00:21:08,240
really a concerted attempt to push
AI out into the real world, uh,

312
00:21:08,320 --> 00:21:12,040
using what was called
expert system technology.

313
00:21:12,490 --> 00:21:14,140
And for the most part,

314
00:21:15,160 --> 00:21:19,600
that technology was just not ready
for prime time. They were trying,

315
00:21:20,220 --> 00:21:24,220
uh, in many cases to do a
form of uncertain reasoning,

316
00:21:24,430 --> 00:21:29,260
a judge judgment, combinations
of evidence, diagnosis,
those kinds of things,

317
00:21:30,460 --> 00:21:32,510
which was simply invalid.
Uh,

318
00:21:32,511 --> 00:21:37,511
and when you try to apply in valid
reasoning methods to real problems,

319
00:21:38,140 --> 00:21:41,080
you can fudge it for small
versions of the problem.

320
00:21:41,110 --> 00:21:44,650
But when it starts to get larger,
the thing just falls apart.

321
00:21:45,610 --> 00:21:48,880
So many companies found that,
uh,

322
00:21:49,090 --> 00:21:52,870
the stuff just didn't work and they were
spending tons of money on consultants

323
00:21:52,871 --> 00:21:57,820
to try and make it work.
And there were other practical reasons,

324
00:21:57,821 --> 00:21:58,654
like, you know, they,

325
00:21:59,110 --> 00:22:04,110
they were asking the companies to
buy incredibly expensive lisp machine

326
00:22:05,410 --> 00:22:10,390
workstations, which were literally
between 50 and $100,000 in,

327
00:22:11,160 --> 00:22:13,480
uh, you know, in 1980 is money, which was,

328
00:22:13,810 --> 00:22:18,810
would be like between 150 and $300,000
per workstation in current prices.

329
00:22:20,771 --> 00:22:25,570
So the bottom line and they weren't
seeing a profit from it. Yeah. Um, they,

330
00:22:26,100 --> 00:22:28,600
they, in many cases, I think
there were some successes.

331
00:22:28,601 --> 00:22:31,540
There's no doubt about that.
But people,

332
00:22:31,541 --> 00:22:36,541
I would say overinvested every major
company was starting an AI department just

333
00:22:37,691 --> 00:22:38,524
like now.

334
00:22:40,570 --> 00:22:44,410
And I worry a bit that we might
see similar disappointments,

335
00:22:44,800 --> 00:22:48,460
not because of the tech.
The current technology is invalid,

336
00:22:48,461 --> 00:22:53,310
but it's limited in its scope. And, uh,

337
00:22:53,590 --> 00:22:57,580
it's almost the, the
duel of the, you know,

338
00:22:57,581 --> 00:22:59,920
the scope problems that
expert systems had.

339
00:22:59,921 --> 00:23:04,780
So what have you learned from that hype
cycle and what can we do to prevent

340
00:23:04,781 --> 00:23:08,230
another winter, for example? Uh, yeah,

341
00:23:08,231 --> 00:23:10,450
so when I'm giving talks these days,

342
00:23:10,510 --> 00:23:15,020
that's one of the warnings that
I give. Uh, so those, those two,

343
00:23:15,080 --> 00:23:18,580
two pot warning slide one
is that, uh, you know,

344
00:23:18,581 --> 00:23:22,820
rather than data being the new
oil data is the new snake oil, uh,

345
00:23:22,870 --> 00:23:27,770
it's a good line. And then,
um, and then the other, uh,

346
00:23:27,940 --> 00:23:32,940
is that we might see a kind of very
visible failure in some of the major

347
00:23:34,781 --> 00:23:38,470
application areas. And I think self
driving cars would be the flagship.

348
00:23:40,540 --> 00:23:45,160
And, uh, I think when
you look at the history,

349
00:23:46,060 --> 00:23:48,770
so the first self driving
car was on the freeway,

350
00:23:50,540 --> 00:23:52,190
uh,
driving itself,

351
00:23:52,191 --> 00:23:55,880
changing lanes overtaking in 1987

352
00:23:57,860 --> 00:23:58,920
and uh,

353
00:23:58,970 --> 00:24:03,970
so it's more than 30 years and that
kind of looks like where we are today.

354
00:24:04,491 --> 00:24:09,230
Right. You know, prototypes
on the freeway, changing
lanes and overtaking. Um, now

355
00:24:11,000 --> 00:24:14,930
I think significant progress has been
made, particularly on the perception side.

356
00:24:14,931 --> 00:24:19,931
So we worked a lot on autonomous
vehicles in the early,

357
00:24:20,600 --> 00:24:24,620
mid nineties at Berkeley, you know,
and we had our own big demonstrations,

358
00:24:24,621 --> 00:24:28,950
you know, we, we put congressmen
into self driving cars and,

359
00:24:28,951 --> 00:24:33,320
and had them zooming along
the freeway. And, uh,

360
00:24:34,070 --> 00:24:38,670
the problem was clearly perception
at the time. The problem.

361
00:24:39,020 --> 00:24:43,010
Yeah.
W in simulation with perfect perception,

362
00:24:43,011 --> 00:24:46,760
you could actually show that you can
drive safely for a long time and even if

363
00:24:46,761 --> 00:24:48,980
the other cars and he's
behaving and so on.

364
00:24:48,980 --> 00:24:53,980
But simultaneously we worked on machine
vision for detecting cars and tracking

365
00:24:56,630 --> 00:24:57,770
pedestrians and so on.

366
00:24:57,771 --> 00:25:02,771
And we couldn't get the reliability
of detection and tracking up to a high

367
00:25:03,891 --> 00:25:06,230
enough particular level,

368
00:25:06,231 --> 00:25:09,530
particularly in bad weather conditions.

369
00:25:09,590 --> 00:25:12,830
A nighttime rain fall.
Good enough for demos,

370
00:25:12,831 --> 00:25:16,390
but perhaps not good enough to cover
the general, the general operation.

371
00:25:16,410 --> 00:25:18,300
The thing about driving
is, you know, so it,

372
00:25:18,301 --> 00:25:20,960
suppose you're a taxi driver
and you drive every day,

373
00:25:20,961 --> 00:25:22,790
eight hours a day for 10 years,
right?

374
00:25:22,791 --> 00:25:26,360
That's a hundred million
seconds of driving, you know,

375
00:25:26,450 --> 00:25:29,530
and any one of those seconds you
could make a fatal mistake. Yeah.

376
00:25:29,780 --> 00:25:34,760
So you're talking about eight
nines of reliability right?

377
00:25:35,060 --> 00:25:35,331
Now,

378
00:25:35,331 --> 00:25:40,331
if your vision system only
detects 98.3% of the vehicles,

379
00:25:41,730 --> 00:25:45,680
right? That's sort of, you know,
one on a bit nines of reliability.

380
00:25:45,890 --> 00:25:49,460
So you have another seven
orders of magnitude to go.

381
00:25:49,970 --> 00:25:54,920
And um, and this is what people
don't understand. They think, oh,

382
00:25:54,921 --> 00:25:59,410
because I had a successful demo,
I'm pretty much done, but you're,

383
00:25:59,660 --> 00:26:04,040
you're not even within seven
orders of magnitude of being done.

384
00:26:05,480 --> 00:26:09,060
And that's the difficulty.
And it's, it's not the, Oh,

385
00:26:09,061 --> 00:26:12,680
can I follow up white line?
That's not the problem. Right?

386
00:26:12,681 --> 00:26:15,440
We follow a white line all
the way across the country,

387
00:26:16,790 --> 00:26:20,300
but it's the,
it's the weird stuff that happens.

388
00:26:20,301 --> 00:26:24,560
It's sort of the edge cases.
Yeah. The edge case, other
drivers doing weird things.

389
00:26:24,590 --> 00:26:28,490
Um, you know, so if you
talk to Google, right?

390
00:26:28,491 --> 00:26:30,320
So they had,
um,

391
00:26:30,330 --> 00:26:34,280
actually very classical
architecture where, you know,

392
00:26:34,281 --> 00:26:35,570
you had machine vision,

393
00:26:35,571 --> 00:26:40,160
which would detect all the other cars and
pedestrians and the white lines in the

394
00:26:40,161 --> 00:26:44,790
road signs. And then basically that
was fed into a logical database.

395
00:26:45,240 --> 00:26:49,890
And then you had a classical 19
seventy's rule based expert system.

396
00:26:50,460 --> 00:26:52,980
I'm telling you,
okay,

397
00:26:53,010 --> 00:26:56,070
if you're in the middle lane and there's
a bicyclist in the right lane who is

398
00:26:56,071 --> 00:26:59,550
signaling this, then then, then,
then he'd do that. Yeah. Right.

399
00:27:00,720 --> 00:27:04,050
And what they found was that every
day they go out and there'd be another

400
00:27:04,051 --> 00:27:06,380
situation that the rules didn't
cover. You know, so they,

401
00:27:06,430 --> 00:27:10,410
they come to a traffic circle and there's
a little girl riding a bicycle the

402
00:27:10,411 --> 00:27:13,290
wrong way around the traffic circle. Okay,
what do you do? We don't have a rule.

403
00:27:13,350 --> 00:27:17,130
Oh my God. Okay, stop.
And then you, you know,

404
00:27:17,131 --> 00:27:20,730
they'd come back and add more rules and
they just found that this was not really

405
00:27:20,731 --> 00:27:25,260
converging. And, um, and if
you think about it, right? How,

406
00:27:25,320 --> 00:27:28,380
how do you deal with an
unexpected situation,

407
00:27:28,381 --> 00:27:33,381
meaning one that you've never previously
encountered and the sort of the

408
00:27:33,880 --> 00:27:37,110
reasoning required to
figure out the solution.

409
00:27:37,111 --> 00:27:40,750
But that situation has never been
done. It's, it doesn't match any, uh,

410
00:27:40,950 --> 00:27:45,180
previous situation in terms of the
kind of reasoning you have to do. Well,

411
00:27:45,720 --> 00:27:47,700
you know, in chess programs,
this happens all the time.

412
00:27:48,600 --> 00:27:53,340
You're constantly coming
up with situations that you
haven't seen before and you

413
00:27:53,341 --> 00:27:56,070
have to reason about them. And
you have to think about, okay,

414
00:27:56,071 --> 00:27:59,280
here are the possible things I could do.
Here are the outcomes,

415
00:27:59,520 --> 00:28:03,270
here's how desirable the outcomes are.
And then pick the right one. You know,

416
00:28:03,271 --> 00:28:04,710
in the 90s we were saying,
Hey,

417
00:28:04,711 --> 00:28:07,230
this is how you're going to
have to do automated vehicles.

418
00:28:07,260 --> 00:28:09,210
They're going to have to
have a look ahead capability.

419
00:28:10,020 --> 00:28:14,480
But the look ahead for driving is more
difficult than it is for chess because

420
00:28:14,590 --> 00:28:17,490
humans, the other, right,
there's humans in there,

421
00:28:17,491 --> 00:28:19,860
less predictable than chess pieces,

422
00:28:19,890 --> 00:28:24,660
then well then you have an opponent in
chess who's also somewhat unpredictable,

423
00:28:25,890 --> 00:28:29,940
but for example, in chess, you
always know the opponent's intention.

424
00:28:29,941 --> 00:28:34,020
They're trying to beat you, right?
Whereas in driving, you don't know,

425
00:28:34,021 --> 00:28:38,130
is this guy trying to turn left or has
he just forgotten to turn off his turn

426
00:28:38,131 --> 00:28:41,700
signal? Or is he drunk or is he, you know,

427
00:28:41,730 --> 00:28:44,730
changing the channel on his
radio or whatever it might be.

428
00:28:45,030 --> 00:28:47,700
You've got to try and
figure out the mental state,

429
00:28:47,701 --> 00:28:52,701
the intent of the other drivers to
forecast the possible evolutions of their

430
00:28:53,011 --> 00:28:56,490
trajectories. And then you
gotta figure out, okay,

431
00:28:56,491 --> 00:28:59,220
which is the directory for me,
that's going to be safest.

432
00:29:00,600 --> 00:29:04,230
And those all interact with each other
because the other drivers are going to

433
00:29:04,231 --> 00:29:07,560
react to your trajectory.
Um, and so on. So, you know,

434
00:29:07,600 --> 00:29:09,600
they've got the classic
merging onto the freeway,

435
00:29:09,601 --> 00:29:14,340
a problem where you're kind of racing
a vehicle that's already on the freeway

436
00:29:14,341 --> 00:29:15,900
and you're gonna pull ahead of them.

437
00:29:15,901 --> 00:29:17,960
Or are you going to let them go
first and pull in behind and,

438
00:29:18,330 --> 00:29:21,330
and you get this sort of
uncertainty about who's going first.

439
00:29:22,590 --> 00:29:24,000
So all those kinds of things

440
00:29:26,010 --> 00:29:31,010
mean that you need a decision making
architecture that's very different from

441
00:29:33,811 --> 00:29:38,100
either a rule based system or it seems
to me it kind of an end to end neural

442
00:29:38,101 --> 00:29:40,290
network system,
you know,

443
00:29:40,291 --> 00:29:44,440
so just as Alphago is pretty good
when it doesn't do any look ahead,

444
00:29:44,500 --> 00:29:47,680
but it's way, way, way,
way better when it does.

445
00:29:48,760 --> 00:29:50,770
I think the same is going
to be true for driving.

446
00:29:50,771 --> 00:29:55,240
You can have a driving system that's
pretty good when it doesn't do any look

447
00:29:55,241 --> 00:29:58,600
ahead, but that's not
good enough. Um, you know,

448
00:29:58,601 --> 00:30:03,601
and we've already seen multiple deaths
caused by poorly designed machine

449
00:30:05,201 --> 00:30:09,880
learning algorithms that don't really
understand what they're doing. Yeah.

450
00:30:09,881 --> 00:30:13,870
And on several levels, I think.
Uh, so on the perception side,

451
00:30:14,290 --> 00:30:18,190
there's mistakes being made by those
algorithms where the perception is very

452
00:30:18,191 --> 00:30:21,670
shallow on the planning side of
the look ahead, like you said.

453
00:30:22,030 --> 00:30:26,950
And the thing that we
come up against that's

454
00:30:28,660 --> 00:30:33,660
really interesting when you try to deploy
systems in the real world is you can't

455
00:30:33,821 --> 00:30:37,390
think of an artificial intelligence system
as a thing that responds to the world

456
00:30:37,391 --> 00:30:38,224
always.

457
00:30:38,530 --> 00:30:41,950
You have to realize that it's an agent
that others will respond to as well.

458
00:30:42,370 --> 00:30:44,560
So in order to drive successfully,

459
00:30:44,561 --> 00:30:47,830
you can't just try to do
obstacle avoidance, right?

460
00:30:48,000 --> 00:30:52,260
You got and pretend that you're invisible,
right? Are the invisible car, right?

461
00:30:52,560 --> 00:30:53,890
So that way,
I mean,

462
00:30:53,891 --> 00:30:58,040
but you have to assert yet others have
to be scared of you just we're all,

463
00:30:58,510 --> 00:31:00,640
there's this tension,
there's this game.

464
00:31:01,180 --> 00:31:03,910
So we study a lot of
work with pedestrians.

465
00:31:04,180 --> 00:31:08,370
If you approached pedestrians as
purely an obstacle avoidance of you,

466
00:31:08,630 --> 00:31:13,060
you are doing look ahead, is in
modeling the intent that you're, you're,

467
00:31:13,090 --> 00:31:15,190
they're not going to,
they're going to take advantage of you.

468
00:31:15,191 --> 00:31:19,180
They're not going to respect you at
all. There has to be a tension, a fear.

469
00:31:19,460 --> 00:31:24,040
Some amount of uncertainty.
That's how we have Cree,

470
00:31:24,640 --> 00:31:27,960
or at least just have kind of
a a resoluteness right? Yes.

471
00:31:28,500 --> 00:31:30,910
You have to display a certain
amount of resolution is you can't,

472
00:31:30,911 --> 00:31:35,660
you can't be too tentative.
Yeah. And uh, yeah, so the,

473
00:31:36,080 --> 00:31:40,890
the, the solutions then become
pretty complicated, right?

474
00:31:40,890 --> 00:31:45,550
You get into game theoretic
analyses and so we, you know,

475
00:31:45,551 --> 00:31:50,551
at Berkeley now we're working a lot on
this kind of interaction between machines

476
00:31:51,101 --> 00:31:55,840
and humans and that's exciting.
And, uh, so my colleague,

477
00:31:56,050 --> 00:32:00,870
uh, and could drag an actually, you know,

478
00:32:00,910 --> 00:32:01,211
if you,

479
00:32:01,211 --> 00:32:05,670
if you formulate the problem game
theoretically and you just let the system

480
00:32:05,680 --> 00:32:08,950
figure out the solution, you
know, it does interesting.

481
00:32:08,951 --> 00:32:11,710
Unexpected things like
sometimes at a stop sign,

482
00:32:12,520 --> 00:32:15,910
if no one is going first,
right?

483
00:32:15,911 --> 00:32:19,180
The car will actually back up a little,
right.

484
00:32:19,210 --> 00:32:23,060
And just to indicate to the other
cars that they should go, uh,

485
00:32:23,070 --> 00:32:26,260
and that's something it invented entirely
by itself. That's interesting. Right?

486
00:32:26,261 --> 00:32:26,831
There was,
you know,

487
00:32:26,831 --> 00:32:30,040
we didn't say this is the language
of communication at stop signs.

488
00:32:30,041 --> 00:32:33,340
It figured it out.
That's really interesting.

489
00:32:34,000 --> 00:32:37,480
So let me one,
just step back for a second.

490
00:32:38,110 --> 00:32:40,250
Just this beautiful philosophical notion.

491
00:32:40,550 --> 00:32:44,690
So Pamela McCormick in 1979 wrote,

492
00:32:45,030 --> 00:32:48,200
Ai began with the ancho
wish to forge the gods.

493
00:32:49,340 --> 00:32:52,070
So when you think about the
history of our civilization,

494
00:32:53,030 --> 00:32:57,890
do you think that there is
an inherent desire to create,

495
00:32:59,080 --> 00:33:02,470
let's not say gods,
but to create superintelligence?

496
00:33:02,530 --> 00:33:05,560
Is it inherent to us is in our genes,

497
00:33:05,830 --> 00:33:10,830
that the natural arc of human civilization
is to create things that are of

498
00:33:12,911 --> 00:33:16,930
greater and greater power and perhaps,
uh,

499
00:33:17,280 --> 00:33:21,910
echoes of ourselves. So to create
the gods as, as, uh, Pamela said,

500
00:33:25,400 --> 00:33:29,790
if the, maybe, I mean, you know,
we're all, we're all individuals,

501
00:33:31,220 --> 00:33:35,760
but certainly we see over and
over again in history, uh,

502
00:33:35,810 --> 00:33:38,960
individuals who thought
about this possibility,

503
00:33:39,590 --> 00:33:42,170
hopefully when I'm not being
too philosophical here.

504
00:33:42,530 --> 00:33:45,620
But if you look at the arc of this,

505
00:33:45,710 --> 00:33:48,740
you know where this is going
and we'll talk about AI safety.

506
00:33:48,741 --> 00:33:50,840
We'll talk about greater
and greater intelligence.

507
00:33:52,620 --> 00:33:54,740
Do you see that they're in,

508
00:33:54,770 --> 00:33:58,040
when you created the Othello program
and you felt this excitement,

509
00:33:58,790 --> 00:33:59,990
what was that excitement?

510
00:34:00,050 --> 00:34:05,050
Was it excitement of a tinkerer who
created something cool like a clock or was

511
00:34:05,511 --> 00:34:10,511
there a magic or was it more
like a child being born that,

512
00:34:10,940 --> 00:34:15,650
yeah, yeah. So I mean, I
certainly understand that
viewpoint. And if you look at,

513
00:34:15,710 --> 00:34:20,530
um, the light hill report,
um, which was coming,

514
00:34:20,590 --> 00:34:22,180
so in the 70s,

515
00:34:22,181 --> 00:34:26,260
there was a lot of controversy
in the UK about Ai and you know,

516
00:34:26,261 --> 00:34:29,020
whether it was for real
and how much the money,

517
00:34:29,080 --> 00:34:33,530
money the government should
invest. And there was a long,

518
00:34:33,560 --> 00:34:36,830
long story, but the government
commissioned a report by,

519
00:34:38,580 --> 00:34:41,510
by Light Hill who was a physicist and uh,

520
00:34:41,890 --> 00:34:46,840
he wrote a very damning report about Ai,
which I think was the point.

521
00:34:47,260 --> 00:34:52,090
Uh, and uh, he said
that that these are, uh,

522
00:34:52,091 --> 00:34:56,980
you know, frustrated man who unable
to have children, would like to,

523
00:34:57,000 --> 00:35:01,490
to create a and, you
know, create a life, um,

524
00:35:01,540 --> 00:35:04,870
you know, as a kind of
replacement, which I,

525
00:35:05,110 --> 00:35:08,590
which I think is really pretty unfair.

526
00:35:13,300 --> 00:35:17,330
But there is, I mean there,
there is a kind of magic,

527
00:35:17,870 --> 00:35:21,920
I would say you when you,
you build something

528
00:35:25,850 --> 00:35:27,910
and, and what you're
building in is really just,

529
00:35:28,030 --> 00:35:32,260
you're building in some understanding of
the principles of learning and decision

530
00:35:32,261 --> 00:35:33,094
making.

531
00:35:34,780 --> 00:35:39,780
And to see those principles actually
then turn into intelligent behavior

532
00:35:41,730 --> 00:35:44,050
in,
in specific situations.

533
00:35:45,560 --> 00:35:50,540
It's an incredible thing.
And, uh, you know, that

534
00:35:53,180 --> 00:35:57,620
is a naturally going to make you think,
okay,

535
00:35:57,621 --> 00:35:58,700
where does this end?

536
00:36:00,280 --> 00:36:05,260
And so there's a, there's magical
optimistic views of where it ends.

537
00:36:05,590 --> 00:36:08,440
Whatever your view of optimism is,

538
00:36:08,470 --> 00:36:11,770
whatever your view of utopia is
probably different for everybody,

539
00:36:12,310 --> 00:36:17,310
but you've often talk about concerns
you have of how things might go wrong.

540
00:36:19,420 --> 00:36:20,960
So,
uh,

541
00:36:21,160 --> 00:36:24,990
I've talked to a Max Tegmark.
Uh,

542
00:36:25,320 --> 00:36:28,930
there's a lot of interesting
ways to think about AI safety.

543
00:36:30,010 --> 00:36:34,690
You're one of the Seminole people thinking
about this problem amongst sort of

544
00:36:34,691 --> 00:36:39,520
being in the weeds of actually
solving specific AI problems.

545
00:36:39,521 --> 00:36:41,830
You also think about the big
picture of where we're going.

546
00:36:42,010 --> 00:36:45,220
So can you talk about
several elements of it?

547
00:36:45,221 --> 00:36:47,940
Let's just talk about
maybe the control problems.

548
00:36:47,941 --> 00:36:52,941
So this idea of losing the ability
to control the behavior I AI system.

549
00:36:55,930 --> 00:36:59,920
So how do you see that?
How do you see that coming about?

550
00:37:00,380 --> 00:37:04,670
Do you think we can do, uh, to manage it

551
00:37:06,890 --> 00:37:07,161
well?

552
00:37:07,161 --> 00:37:11,210
So it doesn't take a genius to realize
that if you make something that's smarter

553
00:37:11,211 --> 00:37:15,770
than you, you might have a
problem. You know, and Turing,

554
00:37:16,430 --> 00:37:17,263
Alan Turing,

555
00:37:18,420 --> 00:37:22,810
you wrote about this and gave
lectures about this ninth, 19, 51,

556
00:37:24,730 --> 00:37:27,850
he did a lecture on the radio and uh,

557
00:37:27,851 --> 00:37:29,880
he basically says no,

558
00:37:30,320 --> 00:37:34,760
once the machine thinking
method starts, uh, you know,

559
00:37:34,790 --> 00:37:39,660
very quickly they'll outstrip humanity.
And uh,

560
00:37:40,120 --> 00:37:43,470
you know, if we're lucky we might
be able to, I think he says it,

561
00:37:44,350 --> 00:37:48,550
we may be able to turn off the power
at strategic moments, but even so,

562
00:37:48,551 --> 00:37:53,330
our species would be humbled. Yeah.
Actually I think was wrong about that.

563
00:37:53,331 --> 00:37:54,710
Right, because you, you know,

564
00:37:54,711 --> 00:37:57,590
if it's a sufficiently intelligent machine
is not going to let you switch it off.

565
00:37:58,460 --> 00:38:01,670
It's actually in competition with you.
So what do you think is meant?

566
00:38:01,700 --> 00:38:03,320
Just for quick tangent,

567
00:38:04,250 --> 00:38:09,110
if we shut off this super intelligent
machine that our species will be humbled,

568
00:38:11,990 --> 00:38:16,990
I think he means that we would
realize that we are inferior,

569
00:38:18,340 --> 00:38:19,020
right?
That we,

570
00:38:19,020 --> 00:38:22,690
we only survived by the skin of our
teeth because we happen to get to the off

571
00:38:22,691 --> 00:38:27,490
switch, you know, Justin, Justin
time, uh, you know, and if we hadn't,

572
00:38:27,520 --> 00:38:32,250
then, uh, we would have lost
control over the earth. So do you,

573
00:38:32,310 --> 00:38:34,620
are you more worried when
you think about this,

574
00:38:34,621 --> 00:38:39,621
the stuff about superintelligent AI or
are you more worried about super powerful

575
00:38:40,421 --> 00:38:44,640
AI that's not aligned with their
values? So the paperclip, uh,

576
00:38:44,680 --> 00:38:48,480
scenarios kind of, uh, I think, um,

577
00:38:48,880 --> 00:38:53,470
so the main problem I'm working
on is he's the control problem.

578
00:38:53,471 --> 00:38:58,471
The problem of machines pursuing
objectives that are not aligned with human

579
00:39:01,001 --> 00:39:03,410
objectives. And, and this has been,

580
00:39:04,270 --> 00:39:07,720
it has been the way we've thought
about AI since the beginning.

581
00:39:09,880 --> 00:39:10,690
You,

582
00:39:10,690 --> 00:39:15,550
you build a machine for optimizing and
then you put in some objective and it

583
00:39:15,551 --> 00:39:19,510
optimizes right? And,
and, um, you know, we,

584
00:39:19,960 --> 00:39:24,930
we can think of this as the, the king
midas problem, right? Because if you know,

585
00:39:24,960 --> 00:39:28,390
the King Midas put in this objective,
right,

586
00:39:28,420 --> 00:39:31,900
everything I touched turned to
gold and the gods, you know,

587
00:39:31,901 --> 00:39:34,890
that's like the machine. They
said, okay, done. You know,

588
00:39:34,891 --> 00:39:39,670
you now have this power and of course
his food and his drink and his family all

589
00:39:39,671 --> 00:39:44,560
turned to gold and then he's dies,
misery and starvation. And, um,

590
00:39:44,590 --> 00:39:48,250
this is, you know, it's, it's a warning.

591
00:39:48,260 --> 00:39:53,260
It's a failure mode that pretty much every
culture in history has had some story

592
00:39:54,370 --> 00:39:56,150
along the same lines.
You know, there's the,

593
00:39:56,151 --> 00:39:59,830
the genie that gives you three wishes
and you know, third wish is always,

594
00:40:00,060 --> 00:40:02,980
you know, please undo the first
two wishes because I messed up.

595
00:40:05,140 --> 00:40:09,730
And, uh, you know, and when
author Samuel wrote his chest,

596
00:40:10,000 --> 00:40:11,110
his checker playing program,

597
00:40:11,111 --> 00:40:14,950
which learned to play checkers
considerably better than
Arthur Samuel could play

598
00:40:14,951 --> 00:40:17,380
and actually reached a
pretty decent standard.

599
00:40:19,910 --> 00:40:21,700
Uh, no, but we know who was, uh,

600
00:40:22,390 --> 00:40:24,820
one of the major mathematicians
of the 20th century.

601
00:40:24,821 --> 00:40:28,360
He sort of the father of modern
automation control systems.

602
00:40:29,650 --> 00:40:33,550
You know, he saw this and he
basically extrapolated, uh, you know,

603
00:40:33,551 --> 00:40:35,230
as touring did and said,
okay,

604
00:40:36,360 --> 00:40:40,990
this is how we could lose control and,
uh,

605
00:40:41,710 --> 00:40:46,710
specifically that we have to be certain
that the purpose we put into the machine

606
00:40:49,601 --> 00:40:51,520
as the purpose,
which we really desire.

607
00:40:53,320 --> 00:40:55,720
And the problem is we can't do that.

608
00:40:57,910 --> 00:41:01,160
You mean we're not,
it's a very difficult to encode to,

609
00:41:01,210 --> 00:41:04,990
to put our values on paper is really
difficult or you're just saying it's

610
00:41:04,991 --> 00:41:09,750
impossible. Uh, line is,

611
00:41:09,760 --> 00:41:14,180
hey, did you mean the test? So
theoretically it's possible, but, uh,

612
00:41:14,320 --> 00:41:19,320
in practice it's extremely unlikely that
we could specify correctly in advance

613
00:41:21,460 --> 00:41:25,140
the full range of concerns of humanity.
The,

614
00:41:25,141 --> 00:41:29,020
you talked about cultural and transmission
of values I think is how humans to

615
00:41:29,021 --> 00:41:33,590
human transmission of values happens.
Right? Uh, well we learned, yeah. I mean,

616
00:41:33,870 --> 00:41:38,510
as we grow up,
we learn about the values that matter,

617
00:41:38,511 --> 00:41:40,610
how things,
how things should go,

618
00:41:40,611 --> 00:41:43,760
what is reasonable to pursue and
what isn't reasonable to pursue.

619
00:41:44,550 --> 00:41:49,030
Big machines can learn it in the same
kind of way. Yeah. So I think that, um,

620
00:41:49,310 --> 00:41:53,930
what we need to do is to get away from
this idea that you build an optimizing

621
00:41:53,931 --> 00:41:55,790
the sheet and then you
put the objective into it.

622
00:41:56,960 --> 00:42:01,960
Because if it's possible that you might
put in a wrong objective and we already

623
00:42:03,561 --> 00:42:06,470
know there's as possible cause it's
happened lots of times, all right?

624
00:42:06,680 --> 00:42:11,680
That means that the machine should
never take an objective that's given as

625
00:42:13,281 --> 00:42:17,270
Gospel truth because once it takes the,
the,

626
00:42:17,280 --> 00:42:20,690
the objective is Gospel truth,
right?

627
00:42:20,691 --> 00:42:25,691
Then it's the leaves that whatever
actions it's taking in procedure,

628
00:42:26,270 --> 00:42:28,400
that objective are the
correct things to do.

629
00:42:28,580 --> 00:42:31,400
So you could be jumping up and down
and saying, no, you know, no, no, no,

630
00:42:31,401 --> 00:42:32,570
you're going to destroy the world.

631
00:42:32,900 --> 00:42:37,250
But the machine knows what the true
objective is and is pursuing it and tough

632
00:42:37,251 --> 00:42:41,870
luck to you. You know, and this
is not restricted to AI, right?

633
00:42:41,871 --> 00:42:46,010
This is, you know, I think many of
the 20th century technologies, right?

634
00:42:46,011 --> 00:42:49,100
So in statistics you,
you minimize a loss function.

635
00:42:49,101 --> 00:42:52,520
The loss function is exogenously
specified in control theory.

636
00:42:52,970 --> 00:42:55,520
You minimize the cost function
in operations research,

637
00:42:55,521 --> 00:43:00,140
you maximize the reward function. And
so on. So in all these disciplines,

638
00:43:00,410 --> 00:43:02,540
this is how we conceive of the problem.

639
00:43:03,080 --> 00:43:08,080
And it's the wrong problem because we
cannot specify with certainty the correct

640
00:43:10,820 --> 00:43:14,380
objective, right? We need uncertainty.

641
00:43:14,390 --> 00:43:18,740
We the machine to be uncertain about as
objective what it is that it's posted.

642
00:43:18,741 --> 00:43:23,360
It's maximizing favorite idea of years.
Uh, I've heard you say somewhere, uh,

643
00:43:23,390 --> 00:43:27,140
well I shouldn't pick favorites,
but it just sounds beautiful.

644
00:43:27,410 --> 00:43:31,430
We need to teach machines
humility. Uh, yeah. I mean,

645
00:43:31,431 --> 00:43:34,350
that's a beautiful way
to put it. I love it. Um,

646
00:43:34,660 --> 00:43:37,130
but they are humble in it.
They know,

647
00:43:37,460 --> 00:43:41,150
they know that they don't know what it
is they're supposed to be doing and the,

648
00:43:41,160 --> 00:43:43,850
and that, those, those objectives, I mean,

649
00:43:43,851 --> 00:43:46,940
they exist there within us,

650
00:43:46,941 --> 00:43:51,180
but we may not be able to explicate them.
We may not even know,

651
00:43:51,860 --> 00:43:56,450
uh,
you know how we want our future to go.

652
00:43:57,230 --> 00:43:59,980
So you exactly. And the machine, you know,

653
00:44:00,270 --> 00:44:05,270
machine that's uncertain is
going to be deferential to us.

654
00:44:06,051 --> 00:44:10,520
So if we say don't do that well now
the machines learn something a bit more

655
00:44:10,521 --> 00:44:15,521
about our true objectives
because something that it
thought was reasonable in

656
00:44:15,861 --> 00:44:18,530
pursuit of our objective, it turns out
not to be. So now it's learn something.

657
00:44:18,950 --> 00:44:23,240
So it's going to differ because it
wants to be doing what we really want.

658
00:44:23,930 --> 00:44:25,910
And um,
you know,

659
00:44:25,911 --> 00:44:30,911
that that point I think is absolutely
central to solving the control problem.

660
00:44:32,010 --> 00:44:34,500
Yeah. Uh, and it's a
different kind of AI when you,

661
00:44:35,340 --> 00:44:40,340
when you take away this idea that the
objective is known then in fact a lot of

662
00:44:42,841 --> 00:44:46,200
the theoretical frameworks
that we're so familiar with,

663
00:44:48,140 --> 00:44:52,280
you know, mark off decision
processes, a goal based planning,

664
00:44:52,650 --> 00:44:56,850
uh, you know, standard game
tree search, all of these, uh,

665
00:44:57,080 --> 00:45:00,740
techniques actually become in applicable.
Uh,

666
00:45:01,220 --> 00:45:03,140
and you get a more
complicated problem because,

667
00:45:05,350 --> 00:45:06,280
because now

668
00:45:08,100 --> 00:45:12,210
the interaction with the human
becomes part of the problem

669
00:45:14,240 --> 00:45:19,240
cause the human by making choices is
giving you more information about the true

670
00:45:21,811 --> 00:45:25,470
objective and that information helps
you achieve the objective better.

671
00:45:26,820 --> 00:45:31,590
And so that really means
that you're mostly dealing
with game theoretic problems

672
00:45:31,591 --> 00:45:35,820
where you've got the machine and the
human and they're coupled together, uh,

673
00:45:35,850 --> 00:45:39,210
rather than a machine going off
by itself with a fixed objective,

674
00:45:39,900 --> 00:45:44,010
which is fascinating on the machine
and the human level that we,

675
00:45:44,460 --> 00:45:49,460
when you don't have an objective means
you're together coming up with an

676
00:45:50,881 --> 00:45:53,820
objective. I mean, there's a lot
of philosophy that, you know,

677
00:45:54,240 --> 00:45:58,560
you could argue the life doesn't really
have meaning we we together agree on

678
00:45:58,561 --> 00:45:59,850
what gives it meaning.

679
00:45:59,851 --> 00:46:04,110
And we've kind of culturally
create a things that give,

680
00:46:04,140 --> 00:46:07,050
why the heck we are in this earth anyway.
Uh,

681
00:46:07,051 --> 00:46:10,320
we together as a society create that
meeting and you have to learn that

682
00:46:10,321 --> 00:46:12,780
objective.
And one of the biggest,

683
00:46:13,050 --> 00:46:15,360
I thought that's what you were
going to go for a second. Uh,

684
00:46:15,840 --> 00:46:19,770
one of the biggest troubles we run
into outside of statistics and machine

685
00:46:19,771 --> 00:46:24,390
learning and AI and just human
civilization is when, uh,

686
00:46:24,420 --> 00:46:26,390
you look at,
I came from this,

687
00:46:26,410 --> 00:46:30,150
I was born in the Soviet Union and
the history of the 20th century,

688
00:46:30,510 --> 00:46:34,750
we run into the most trouble
us humans when there was a,

689
00:46:35,290 --> 00:46:36,123
uh,
uh,

690
00:46:36,300 --> 00:46:40,440
a certainty about the objective and you
do whatever it takes to achieve that

691
00:46:40,441 --> 00:46:44,190
objective with he talking bout
in Germany or communist Russia.

692
00:46:44,970 --> 00:46:45,570
All right.

693
00:46:45,570 --> 00:46:49,980
Yeah, I get into trouble with humans say
with, uh, you know, corporations. In fact,

694
00:46:49,981 --> 00:46:52,520
some people argue that,
you know,

695
00:46:52,530 --> 00:46:56,490
we don't have to look forward to a time
when AI systems take over the world they

696
00:46:56,491 --> 00:46:59,300
already have and they call a corporations,
right?

697
00:46:59,301 --> 00:47:01,890
That corporations happened to be,
uh,

698
00:47:01,891 --> 00:47:06,060
using people as components right now.
Um,

699
00:47:06,090 --> 00:47:10,470
but they are effectively algorithmic
machines and they're optimizing an

700
00:47:10,471 --> 00:47:11,011
objective,

701
00:47:11,011 --> 00:47:16,011
which is quarterly profit
that isn't aligned with
overall wellbeing of the human

702
00:47:17,261 --> 00:47:19,210
race and they are destroying the world.

703
00:47:19,660 --> 00:47:24,310
They are primarily responsible for our
inability to tackle climate change.

704
00:47:24,520 --> 00:47:29,230
Right? So I think that's one way of
thinking about what's going on with,

705
00:47:29,280 --> 00:47:30,340
uh,
with corporations.

706
00:47:30,341 --> 00:47:35,341
But I think the point
you're making is valid.

707
00:47:35,451 --> 00:47:35,870
That there,

708
00:47:35,870 --> 00:47:40,870
there are many systems in the real world
where we've sort of prematurely fixed

709
00:47:42,231 --> 00:47:46,430
on the objective and then decoupled the,
uh,

710
00:47:46,431 --> 00:47:50,850
the machine from those and it's
supposed to be serving. Um,

711
00:47:50,990 --> 00:47:53,970
and I think you see this with government,
right?

712
00:47:53,971 --> 00:47:57,300
Government is supposed to be
a machine that serves people,

713
00:47:57,840 --> 00:48:02,840
but instead it tends to be taken over
by people who have their own objective,

714
00:48:04,110 --> 00:48:05,280
uh,
and use government

715
00:48:05,340 --> 00:48:09,990
to optimize that objective regardless
of what people want. Do, do you have,

716
00:48:10,020 --> 00:48:12,660
do you find appealing the idea of almost,
uh,

717
00:48:12,680 --> 00:48:17,040
arguing machines where you have
multiple AI systems with a clear fixed

718
00:48:17,041 --> 00:48:21,180
objective? We have in government,
the red team and the blue team,

719
00:48:21,690 --> 00:48:25,400
they're are very fixed on their objectives
and they argue and it kind of, uh,

720
00:48:25,950 --> 00:48:27,180
made would disagree,

721
00:48:27,181 --> 00:48:32,040
but it kind of seems to
make it work somewhat that,

722
00:48:32,940 --> 00:48:37,650
uh, the, the, the duality
of it I though, okay,

723
00:48:37,651 --> 00:48:40,830
let's go a hundred years back when
there was still was going on or at the

724
00:48:40,831 --> 00:48:45,150
founding of this country,
there was disagreements and
that disagreement is where,

725
00:48:45,860 --> 00:48:46,590
uh,

726
00:48:46,590 --> 00:48:51,590
so it was a balance between certainty
and forced humility because the power was

727
00:48:51,661 --> 00:48:52,494
distributed.

728
00:48:53,980 --> 00:48:57,040
Yeah. I think that the, um, the,

729
00:48:57,041 --> 00:49:01,950
the nature of debate and
disagreement argument takes,

730
00:49:02,500 --> 00:49:06,570
uh, as a premise, the idea
that you could be wrong,

731
00:49:07,350 --> 00:49:07,591
right?

732
00:49:07,591 --> 00:49:12,591
Which means that you're not necessarily
absolutely convinced that your objective

733
00:49:13,411 --> 00:49:16,380
is, is the correct one. All right? Um,

734
00:49:16,770 --> 00:49:18,680
if you were absolutely,

735
00:49:18,950 --> 00:49:21,720
there'd be no point in having
any discussion or argument
because you would never

736
00:49:21,721 --> 00:49:24,810
change your mind and
there wouldn't be any,

737
00:49:25,650 --> 00:49:28,200
any sort of synthesis or,
or anything like that. So,

738
00:49:28,300 --> 00:49:30,870
so I think you can think
of argumentation as a,

739
00:49:31,660 --> 00:49:36,270
as an implementation of a
form of uncertain reasoning.

740
00:49:36,450 --> 00:49:41,420
Uh, and, um, you know, I, I,

741
00:49:41,490 --> 00:49:46,490
I've been reading recently
about utilitarianism and
the history of efforts to

742
00:49:47,980 --> 00:49:51,880
define in a sort of clear
mathematical way, a,

743
00:49:53,750 --> 00:49:57,950
if you like, a formula for moral
or political decision making.

744
00:49:59,380 --> 00:50:03,310
And it's really interesting that the
parallels between the philosophical

745
00:50:03,311 --> 00:50:08,311
discussions going back 200 years and
what you see now in discussions about

746
00:50:08,861 --> 00:50:13,710
existential risk because yeah,
it's almost exactly the same.

747
00:50:13,770 --> 00:50:15,000
So someone would say,
okay,

748
00:50:15,001 --> 00:50:18,120
well here's a formula for how
we should make decisions, right?

749
00:50:18,121 --> 00:50:20,440
So utilitarian design roughly,
you know,

750
00:50:20,610 --> 00:50:25,610
each person has a utility function and
then we make decisions to maximize the

751
00:50:25,771 --> 00:50:28,790
sum of everybody's utility,
right?

752
00:50:28,820 --> 00:50:32,930
And then people point out,
well, you know, in that case,

753
00:50:33,530 --> 00:50:38,530
the best policy is one that leads
to the enormously vast population,

754
00:50:39,880 --> 00:50:43,890
all of whom are living a life that's
barely worth living, right? And,

755
00:50:43,891 --> 00:50:47,450
and this is called the repugnant
conclusion. And, uh, you know,

756
00:50:47,451 --> 00:50:49,690
another version is, you know, that we,

757
00:50:49,720 --> 00:50:53,780
we should maximize pleasure and
that's what we mean by utility.

758
00:50:54,140 --> 00:50:58,770
And then you'll get people effectively
saying, well, in that case, you know,

759
00:50:58,790 --> 00:51:02,420
we might as well just have everyone
hooked up to a heroin drip, you know,

760
00:51:02,450 --> 00:51:05,600
and they didn't use those words.
But that debate, you know,

761
00:51:05,610 --> 00:51:09,970
was happening in the 19th
century as it is now, uh,

762
00:51:10,010 --> 00:51:13,760
about AI that if we get the formula wrong,
you know,

763
00:51:13,761 --> 00:51:18,761
we're going to have AI systems working
towards an outcome that in retrospect

764
00:51:20,240 --> 00:51:24,560
would be exactly wrong.
Do you think theirs has beautifully put,

765
00:51:24,561 --> 00:51:29,250
so the echoes are there, but do you think,
uh, I mean if you look at Sam Harris,

766
00:51:29,251 --> 00:51:34,251
is our imagination worries about the
Ai version of that because of the,

767
00:51:36,291 --> 00:51:37,850
uh,
the,

768
00:51:38,270 --> 00:51:43,270
the speed at which the things going
wrong in the utilitarian context could

769
00:51:44,421 --> 00:51:48,890
happen? Yeah. Is that, is that a
worry for you? Yeah, I think that,

770
00:51:50,140 --> 00:51:53,340
you know, in most cases,
not in all, but if we,

771
00:51:53,360 --> 00:51:56,540
if we have a wrong political idea,
you know,

772
00:51:56,541 --> 00:51:59,090
we see it starting to go wrong
and we're, you know, we're,

773
00:51:59,091 --> 00:52:01,880
we're not completely stupid.
And so we said, okay, that was,

774
00:52:02,120 --> 00:52:06,050
maybe that was a mistake. Uh,
let's try something different.

775
00:52:07,210 --> 00:52:07,940
And,

776
00:52:07,940 --> 00:52:12,140
and also we're very slow and inefficient
about implementing these things and so

777
00:52:12,141 --> 00:52:12,290
on.

778
00:52:12,290 --> 00:52:17,150
So you have to worry when you have
corporations or political systems that are

779
00:52:17,151 --> 00:52:18,050
extremely efficient.

780
00:52:19,490 --> 00:52:24,350
But when we look at AI systems or
even just computers in general, right?

781
00:52:24,351 --> 00:52:29,351
They have this different characteristic
from ordinary human activity in the

782
00:52:30,951 --> 00:52:33,350
past.
So let's say you were a surgeon,

783
00:52:34,340 --> 00:52:38,060
you had some idea about how to
do some operation, right? Well,

784
00:52:38,061 --> 00:52:40,040
and let's say you were wrong,
right?

785
00:52:40,041 --> 00:52:44,150
That that way of doing the operation
would mostly kill the patient. Well,

786
00:52:44,151 --> 00:52:47,930
you'd find out pretty quickly
that like off to three,

787
00:52:48,140 --> 00:52:52,070
maybe three or four tries. Right? But

788
00:52:54,290 --> 00:52:59,290
that isn't true for
pharmaceutical companies because
they don't do three or four

789
00:53:00,321 --> 00:53:01,350
operations.
They,

790
00:53:01,370 --> 00:53:05,570
they manufacturer three or 4 billion pills
and they sell them and then they find

791
00:53:05,571 --> 00:53:08,780
out maybe six months or a year later that,
oh,

792
00:53:08,900 --> 00:53:12,260
people are dying of heart attacks
or getting cancer from this drug.

793
00:53:13,440 --> 00:53:15,920
And so that's why we have the FDA,
right?

794
00:53:15,921 --> 00:53:20,810
Because of the scalability of
pharmaceutical production and you know,

795
00:53:20,811 --> 00:53:25,460
and there have been some
unbelievably bad episodes

796
00:53:27,510 --> 00:53:31,590
in the history of pharmaceuticals
and adulteration of,

797
00:53:32,040 --> 00:53:36,510
of products and so on that debt have
killed tens of thousands or paralyzed

798
00:53:36,511 --> 00:53:40,710
hundreds of thousands of people.
Now with computers,

799
00:53:40,860 --> 00:53:45,660
we have that same scalability problem
that you can sit there and type for I

800
00:53:45,661 --> 00:53:49,020
equals one to 5 billion do,
right?

801
00:53:49,021 --> 00:53:52,020
And all of a sudden you are having
an impact on a global scale.

802
00:53:52,410 --> 00:53:54,060
And yet we have no FDA,
right?

803
00:53:54,061 --> 00:53:59,061
There's absolutely no controls at all
over what a bunch of undergraduates with

804
00:54:00,661 --> 00:54:04,800
too much caffeine can do to
the world. And uh, you know,

805
00:54:04,801 --> 00:54:07,880
we look at what happened with Facebook,
well,

806
00:54:07,920 --> 00:54:11,040
social media in general and
click through optimization.

807
00:54:12,000 --> 00:54:17,000
So you have a simple feedback algorithm
that's trying to just optimize,

808
00:54:18,630 --> 00:54:22,410
click through, right? That
sounds reasonable. Right?

809
00:54:22,411 --> 00:54:25,560
Cause you don't want to be feeding
people ads that they don't care about.

810
00:54:25,561 --> 00:54:26,430
I'm not interested in,

811
00:54:28,230 --> 00:54:33,230
and you might even think of that
process as simply adjusting the,

812
00:54:37,050 --> 00:54:42,050
the feeding of ads or news articles or
whatever it might be to match people's

813
00:54:42,960 --> 00:54:45,570
preferences. Right. Which
sounds like a good idea,

814
00:54:47,440 --> 00:54:51,970
but in fact that isn't how
the algorithm works, right?

815
00:54:52,770 --> 00:54:57,000
You make more money, the
algorithm makes more money. If

816
00:54:58,680 --> 00:55:01,920
it can better predict what people are
going to click on it cause then it can

817
00:55:01,921 --> 00:55:04,260
feed them exactly that.
Right?

818
00:55:04,740 --> 00:55:09,740
So the way to maximize click true is
actually to modify the people to make them

819
00:55:11,431 --> 00:55:12,390
more predictable.

820
00:55:13,190 --> 00:55:18,190
And one way to do that is to feed them
information which will change their

821
00:55:19,320 --> 00:55:24,320
behavior and preferences towards
extremes that make them predictable and

822
00:55:25,180 --> 00:55:29,160
whatever is the nearest extreme or
the nearest predictable point, uh,

823
00:55:29,320 --> 00:55:34,050
that's where you're going to end up.
And the machines will force you there.

824
00:55:34,650 --> 00:55:38,880
Now, and I think there's a
reasonable argument to say that this,

825
00:55:39,600 --> 00:55:44,130
among other things is contributing to the
destruction of democracy in the world.

826
00:55:47,460 --> 00:55:51,090
And where was the
oversight of this process?

827
00:55:51,091 --> 00:55:53,040
Where would the people saying,
okay,

828
00:55:53,400 --> 00:55:56,940
you would like to apply this
algorithm to 5 billion people on the,

829
00:55:56,941 --> 00:56:00,870
on the face of the earth?
Can you show me that it's safe?

830
00:56:00,871 --> 00:56:05,370
Can you show me that it won't have
various kinds of negative effects? No.

831
00:56:05,371 --> 00:56:10,080
There was no one asking that question.
There was no one placed between,

832
00:56:10,620 --> 00:56:15,000
you know, the undergrads with too
much caffeine and the human race.

833
00:56:15,410 --> 00:56:18,840
Well it's just they just
did it as, but some, uh,

834
00:56:18,900 --> 00:56:22,800
we're outside of the scope of
my knowledge. So economists
would argue that the,

835
00:56:22,801 --> 00:56:24,040
what is it?
The invisible

836
00:56:24,040 --> 00:56:27,880
hand. So the, the capitalist
system, it was the oversight.

837
00:56:28,030 --> 00:56:32,050
So if you're going to corrupt society
with whatever decision you make as a

838
00:56:32,051 --> 00:56:35,620
company, then that's going to be reflected
in people not using your product.

839
00:56:35,950 --> 00:56:40,150
Sort of one. That's one model
of oversight. So we shall see

840
00:56:41,350 --> 00:56:45,970
time, you know, but you, you
might even have broken the,

841
00:56:46,020 --> 00:56:46,853
uh,

842
00:56:46,900 --> 00:56:51,580
the political system that
enables capitalism to function
while you've changed it.

843
00:56:53,090 --> 00:56:58,010
So we should see change changes
often painful. So my question is, uh,

844
00:56:58,460 --> 00:57:00,320
absolutely,
it's fascinating.

845
00:57:00,590 --> 00:57:05,030
You're absolutely right that there were
zero oversight on algorithms that can

846
00:57:05,031 --> 00:57:09,710
have a profound civilization
changing a effect.

847
00:57:10,220 --> 00:57:13,230
So do you think it's possible?
I mean I haven't,

848
00:57:14,030 --> 00:57:15,290
have you seen government,

849
00:57:15,620 --> 00:57:20,540
so do you think is possible to
create regulatory bodies, uh,

850
00:57:20,570 --> 00:57:22,910
oversight over Ai Algorithms,

851
00:57:22,911 --> 00:57:27,911
which are inherently such cutting
edge set of ideas and technologies?

852
00:57:31,120 --> 00:57:31,391
Yeah,

853
00:57:31,391 --> 00:57:36,391
but I think it takes time to
figure out what kind of oversight,

854
00:57:36,630 --> 00:57:41,490
what kinds of controls and it took time
to design the FDA regime. Uh, you know,

855
00:57:41,491 --> 00:57:44,580
and some people still don't like
it and they want to fix it. Um,

856
00:57:44,581 --> 00:57:48,120
and I think there are clear
ways that it could be improved,

857
00:57:49,510 --> 00:57:52,690
but the whole notion that you have
stage one, stage two, stage three,

858
00:57:52,691 --> 00:57:57,691
and here are the criteria for what you
have to do to pass the stage one trial.

859
00:57:58,630 --> 00:58:02,140
Right? We haven't even thought about
what those would be for algorithms.

860
00:58:02,141 --> 00:58:03,520
So I mean,
I think there are,

861
00:58:04,990 --> 00:58:09,990
there are things we could do
right now with regard to bias.

862
00:58:10,250 --> 00:58:14,850
For example, we have a pretty
good technical handle on, uh,

863
00:58:15,550 --> 00:58:20,550
how to detect algorithms that are
propagating bias that exists in datasets,

864
00:58:21,130 --> 00:58:25,270
um, how to de bias, those
algorithms, um, and,

865
00:58:25,280 --> 00:58:27,400
and even what it's going
to cost you to do that.

866
00:58:27,970 --> 00:58:32,760
So I think we could start
having some standards on that.

867
00:58:32,761 --> 00:58:34,910
I think there are,
there are things to do with

868
00:58:36,590 --> 00:58:40,610
impersonation of falsification
that we could, we could work on.

869
00:58:40,611 --> 00:58:41,980
So I thanks.
Yeah.

870
00:58:43,520 --> 00:58:48,180
Or in a very simple point. So
impersonation ism is a machine, uh,

871
00:58:48,950 --> 00:58:50,660
acting as if it was a person.

872
00:58:51,520 --> 00:58:56,520
I can't see a real justification for why
we shouldn't insist that machine self

873
00:58:58,251 --> 00:59:01,910
identify as machines. Uh, you know, where,

874
00:59:01,911 --> 00:59:04,340
where is the social benefit in,

875
00:59:04,670 --> 00:59:09,320
in fooling people into thinking that
this is really a person when it isn't,

876
00:59:10,610 --> 00:59:14,780
you know, I don't mind if it uses a human
light voice that's easy to understand,

877
00:59:14,781 --> 00:59:18,920
that's fine, but it should just say, oh,
I'm a machine in some shape, some form,

878
00:59:20,490 --> 00:59:22,410
and not many are speaking to that.

879
00:59:22,920 --> 00:59:26,280
I would think relatively obvious facts.
So I think most people,

880
00:59:26,660 --> 00:59:31,250
it's actually a law in California
that bands impersonation,

881
00:59:31,340 --> 00:59:35,580
but only in certain restricted
circumstances. So, uh,

882
00:59:35,650 --> 00:59:40,650
for the purpose of engaging in a forging
and transaction and for the purpose of

883
00:59:42,580 --> 00:59:46,690
modifying someone's voting
behavior. Uh, so those are,

884
00:59:46,691 --> 00:59:50,890
those are the circumstances where,
uh, machines have to self identify.

885
00:59:51,600 --> 00:59:56,310
Um, but I think there's, you
know, arguably it should be
in all circumstances. Um,

886
00:59:56,500 --> 01:00:00,410
and then when you, when you
talk about deep fakes, you know,

887
01:00:00,411 --> 01:00:01,760
we're just beginning,

888
01:00:02,300 --> 01:00:07,300
but already it's possible to make
a movie of anybody saying anything,

889
01:00:08,380 --> 01:00:10,700
uh,
in ways that are pretty hard to detect,

890
01:00:11,690 --> 01:00:14,840
including yourself because
you're on camera now and
your voice is coming through

891
01:00:14,841 --> 01:00:15,674
with a high riser.

892
01:00:15,710 --> 01:00:16,331
Yeah. As, as a,

893
01:00:16,331 --> 01:00:19,720
you could take what I'm saying and replace
it with a pretty much anything else

894
01:00:19,750 --> 01:00:23,760
you wanted me to be saying. And even it
would change my lips and expressions,

895
01:00:23,761 --> 01:00:28,030
expressions to fit. And, uh,

896
01:00:28,150 --> 01:00:33,150
there's actually not much in the way of
a real legal protection against that.

897
01:00:36,120 --> 01:00:39,740
I think in the commercial area
you could say, yeah, that's a,

898
01:00:39,750 --> 01:00:43,620
you're using my brand. And so on that,
there, there are rules about that,

899
01:00:44,620 --> 01:00:49,360
but in the political sphere, I think it's,
uh, at, at the moment it's, you know,

900
01:00:49,390 --> 01:00:52,600
anything goes. So, and that, that
could be really, really damaging.

901
01:00:53,920 --> 01:00:58,240
And let me just, uh, tried
to make, not an argument,

902
01:00:58,300 --> 01:01:03,300
but try to look back at history and a
say something a dark in essence is while

903
01:01:06,251 --> 01:01:10,930
regulation seems to be oversight seems
to be exactly the right thing to do here.

904
01:01:11,320 --> 01:01:12,610
It seems that human beings,

905
01:01:12,611 --> 01:01:16,030
what they naturally do is they
wait for something to go wrong.

906
01:01:16,031 --> 01:01:17,980
If you're talking about nuclear weapons,

907
01:01:18,790 --> 01:01:23,790
you can't talk about nuclear weapons
being dangerous until somebody actually

908
01:01:23,950 --> 01:01:28,030
like the United States drops
the bomb or Chernobyl melting.

909
01:01:28,210 --> 01:01:30,580
Do you think we will have to wait

910
01:01:32,880 --> 01:01:36,990
for things going wrong in a way
that's obviously damaging to society?

911
01:01:37,000 --> 01:01:40,610
Not An existential risk, but
obviously damaging. Okay.

912
01:01:42,580 --> 01:01:45,190
Or do you have faith that,
I hope not, but I mean,

913
01:01:46,420 --> 01:01:51,320
I think we do have to look
at history and uh, you know,

914
01:01:51,330 --> 01:01:56,330
so the two examples you gave UCLE
weapons and nuclear power are very,

915
01:01:56,761 --> 01:02:00,960
very interesting because, you
know, when nuclear weapons,

916
01:02:01,080 --> 01:02:06,080
we knew in the early years of the 20th
century that atoms contained a huge

917
01:02:08,021 --> 01:02:10,690
amount of energy, right? We had
equals MC squared. We knew the,

918
01:02:11,170 --> 01:02:14,590
the mass differences between the
different atoms and their components.

919
01:02:14,591 --> 01:02:15,670
And we knew that

920
01:02:18,000 --> 01:02:21,360
you might be able make an
incredibly powerful explosive.

921
01:02:21,361 --> 01:02:25,020
So Hg wells wrote science fiction book,
I think in 1912.

922
01:02:25,720 --> 01:02:30,610
Um, Frederick Saudi, who was
the guy who discovered isotopes,

923
01:02:30,650 --> 01:02:31,650
a Nobel Prize winner,

924
01:02:32,130 --> 01:02:36,060
he gave a speech in 1915 saying that,

925
01:02:37,760 --> 01:02:37,981
you know,

926
01:02:37,981 --> 01:02:42,270
one pound of this new explosive would be
the equivalent of 150 tons of dynamite,

927
01:02:42,300 --> 01:02:46,920
which turns out to be about
right. And, uh, you know,

928
01:02:47,220 --> 01:02:48,480
this was in World War One,
right?

929
01:02:48,481 --> 01:02:53,430
So he was imagining how much
worse the World War would be,

930
01:02:53,790 --> 01:02:56,100
uh,
if we were using that kind of explosive.

931
01:02:56,370 --> 01:03:01,370
But the physics establishment simply
refused to believe that these things could

932
01:03:02,610 --> 01:03:06,840
be made, including the people
who are making it well.

933
01:03:06,841 --> 01:03:08,670
So they were doing the nuclear physics,

934
01:03:09,480 --> 01:03:13,830
I mean eventually wore the ones who made
it and Jaguar for me or whoever. Well,

935
01:03:13,831 --> 01:03:18,780
so up to, um, the, the development, uh,

936
01:03:18,870 --> 01:03:21,060
was,
was mostly theoretical.

937
01:03:21,061 --> 01:03:24,770
So it was people using sort of primitive
kinds of particle acceleration and

938
01:03:24,780 --> 01:03:27,420
doing experiments, uh, at the,

939
01:03:27,930 --> 01:03:30,910
at the level of single particles are
collections of particles that they,

940
01:03:30,970 --> 01:03:31,803
they weren't

941
01:03:34,410 --> 01:03:37,980
yet thinking about how to actually
make a bomb or anything like that,

942
01:03:38,220 --> 01:03:40,770
but they knew the energy it was there
and they figured if they understood it

943
01:03:40,771 --> 01:03:44,490
better, uh, it might be possible,
but the physics establishment,

944
01:03:45,450 --> 01:03:49,110
their view, and I think because
they did not want it to be true,

945
01:03:49,680 --> 01:03:53,360
the view was that it could not be true,
uh,

946
01:03:53,400 --> 01:03:57,210
that this could not provide
a way to make a super weapon.

947
01:03:57,870 --> 01:03:59,340
And, um, you know,

948
01:03:59,341 --> 01:04:04,341
there was this famous speech given by
Rutherford who was the sort of leader of

949
01:04:05,071 --> 01:04:06,920
nuclear physics. And, um,

950
01:04:08,010 --> 01:04:11,960
it was on September 11th, 1933.
And he, he said, you know,

951
01:04:11,970 --> 01:04:16,970
anyone who talks about the possibility
of obtaining energy from transformation

952
01:04:17,401 --> 01:04:22,080
of atoms is talking complete
moonshine. And, uh, the next,

953
01:04:22,320 --> 01:04:23,370
uh,
the next morning,

954
01:04:23,371 --> 01:04:28,371
Leo's Zelar read about that speech and
then invented the nuclear chain reaction.

955
01:04:28,650 --> 01:04:32,160
And so as soon as he invented the,

956
01:04:32,280 --> 01:04:36,780
as soon as he had that idea that you
could make a chain reaction with neutrons

957
01:04:36,781 --> 01:04:39,990
because neutrons were not repelled by
the nuclear so they could enter the

958
01:04:39,991 --> 01:04:44,670
nuclear center and think, continue the
reaction. As soon as he has that idea,

959
01:04:44,671 --> 01:04:49,671
he instantly realized that
the world was in deep doodoo.

960
01:04:51,240 --> 01:04:53,130
Uh,
because this is 1933,

961
01:04:53,131 --> 01:04:58,131
right in a Hitler had a recently
come to power in Germany.

962
01:04:59,250 --> 01:05:01,330
Zilara was in London,
uh,

963
01:05:01,410 --> 01:05:05,090
and eventually became a refugee and a,

964
01:05:05,100 --> 01:05:07,590
and came to the u s and the,

965
01:05:07,591 --> 01:05:12,210
I'm in the process of, of having
the idea about the chain reaction.

966
01:05:12,600 --> 01:05:16,830
He figured out basically how to make
a bomb and also how to make a reactor.

967
01:05:18,160 --> 01:05:21,670
And he patented the reactor in 1934,

968
01:05:22,270 --> 01:05:25,600
but because of the situation,

969
01:05:25,630 --> 01:05:29,250
the great power conflict situation
that he could see happening, um,

970
01:05:29,380 --> 01:05:33,200
he kept that a secret. And so, um,

971
01:05:33,370 --> 01:05:38,370
between then and the
beginning of world war two,

972
01:05:39,670 --> 01:05:42,700
people were working,
including the Germans on

973
01:05:45,490 --> 01:05:48,190
how to actually create neutron sources.
Right.

974
01:05:48,191 --> 01:05:53,191
What specific fission reactions would
produce neutrons have the right energy to

975
01:05:54,221 --> 01:05:56,260
continue the reaction.
And,

976
01:05:57,720 --> 01:06:01,630
and that was demonstrated in Germany,
I think in 1938 if I remember correctly.

977
01:06:02,170 --> 01:06:03,770
The first,
uh,

978
01:06:03,850 --> 01:06:08,710
nuclear weapon patent
was 1939 by the French.

979
01:06:09,310 --> 01:06:13,060
Um, so this was actually a,
you know, this was actually

980
01:06:14,860 --> 01:06:19,570
going on, you know, well before World War
Two really got going. And then, you know,

981
01:06:19,571 --> 01:06:23,530
the British parable, he had the most
advanced capability in this area,

982
01:06:23,531 --> 01:06:28,210
but for safety reasons among others and
plus, which is sort of just resources.

983
01:06:28,840 --> 01:06:33,280
They moved the program from Britain to
the u s and then that became Manhattan

984
01:06:33,281 --> 01:06:38,200
project. Uh, so the, the,
the reason why we couldn't

985
01:06:40,600 --> 01:06:45,600
have any kind of oversight of nuclear
weapons and nuclear technology was because

986
01:06:47,411 --> 01:06:50,980
we were basically already in a,
an arms race in a war.

987
01:06:52,390 --> 01:06:55,990
And, um, but you, you mentioned
that in the twenties and thirties.

988
01:06:55,991 --> 01:06:57,910
So what are the echoes,

989
01:06:59,960 --> 01:07:03,100
the way you've described the story,
I mean, there's clearly echoes.

990
01:07:03,101 --> 01:07:05,320
Why do you think most AI researchers,

991
01:07:06,970 --> 01:07:11,550
folks who are really close to the metal,
a really are not concerned about AI?

992
01:07:11,580 --> 01:07:14,770
They don't think about it, uh, whether
it's, they don't want to think about it.

993
01:07:14,771 --> 01:07:18,700
It's, well, what are the,
yeah, why do you think that is?

994
01:07:19,030 --> 01:07:24,030
What are the echoes of the
nuclear situation to the
current situation and what

995
01:07:26,681 --> 01:07:30,370
can we do about it? I
think there is a, you know,

996
01:07:30,371 --> 01:07:33,490
a kind of motive motivated cognition,
which is a,

997
01:07:34,110 --> 01:07:38,380
a term in psychology means that you
believe what you would like to be true,

998
01:07:38,970 --> 01:07:43,450
uh, rather than what is true. And, uh,

999
01:07:44,260 --> 01:07:44,800
you know,
it's,

1000
01:07:44,800 --> 01:07:49,800
it's unsettling to think that what
you're working on might be the end of the

1001
01:07:49,841 --> 01:07:51,400
human race,
obviously.

1002
01:07:52,600 --> 01:07:57,600
So you would rather instantly deny it
and come up with some reason why it

1003
01:07:58,091 --> 01:08:01,030
couldn't be true. And
the, you know, I have,

1004
01:08:01,210 --> 01:08:05,800
I collected a long list of regions
that extremely intelligent,

1005
01:08:05,801 --> 01:08:10,801
competent AI scientists have come up with
for why we shouldn't worry about this.

1006
01:08:13,310 --> 01:08:14,470
You know,
for example,

1007
01:08:15,140 --> 01:08:18,770
calculators are superhuman at arithmetic
and they haven't taken over the world.

1008
01:08:18,800 --> 01:08:23,330
So there's nothing to worry about.
Well, okay, my five year old, you know,

1009
01:08:23,410 --> 01:08:27,680
could've figured out why that
was, uh, uh, an unreasonable and,

1010
01:08:27,790 --> 01:08:31,670
and really quite weak
argument. Um, you know,

1011
01:08:31,671 --> 01:08:34,800
another one was, uh, you know,

1012
01:08:35,240 --> 01:08:40,240
while it's theoretically possible that
you could have a super human AI destroy

1013
01:08:42,291 --> 01:08:42,891
the world,
you know,

1014
01:08:42,891 --> 01:08:47,630
it's also theoretically possible that a
black hole could materialize right next

1015
01:08:47,631 --> 01:08:50,810
to the earth and destroy
humanity. I mean, yes,

1016
01:08:50,811 --> 01:08:52,310
it's theoretically possible quantum,

1017
01:08:52,311 --> 01:08:56,900
theoretically extremely unlikely that
it would just materialize right there.

1018
01:08:57,310 --> 01:08:58,143
Um,

1019
01:08:58,580 --> 01:09:03,580
but that's a completely bogus and allergy
because if the whole physics community

1020
01:09:03,621 --> 01:09:07,550
on earth was working to materialize
a black hole in near Earth orbit,

1021
01:09:08,330 --> 01:09:12,590
right, wouldn't you ask them, is that
a good idea? Is that going to be safe?

1022
01:09:12,591 --> 01:09:16,610
You know, what if you succeed?
Right? And that's the thing, right?

1023
01:09:17,570 --> 01:09:22,310
The Ai community is sort of refused
to ask itself what if you succeed?

1024
01:09:24,760 --> 01:09:27,620
And initially I think that was
because it was too hard, but you know,

1025
01:09:27,740 --> 01:09:32,740
Alan Turing asked himself that
and he said we'd be toast.

1026
01:09:34,050 --> 01:09:36,950
Right? If we were lucky, we might
be able to switch off the power,

1027
01:09:36,951 --> 01:09:38,420
but preferably we'd be toast.

1028
01:09:38,690 --> 01:09:41,990
But there's also an aspect that,

1029
01:09:42,410 --> 01:09:46,820
because we're not exactly
sure what the future holds,

1030
01:09:47,270 --> 01:09:50,900
it's not clear exactly.
So technically what to worry about,

1031
01:09:51,380 --> 01:09:55,610
sort of how things go wrong. And so, uh,

1032
01:09:55,640 --> 01:09:59,930
there is something, it feels like,
maybe you can correct me if I'm wrong,

1033
01:10:00,230 --> 01:10:05,230
but there's something paralyzing about
worrying about something that logically

1034
01:10:05,991 --> 01:10:10,370
is inevitable, but you don't really
know what that will look like.

1035
01:10:10,850 --> 01:10:15,170
Yeah, I think that's, that's it's a
reasonable point. And you know, the,

1036
01:10:16,970 --> 01:10:19,580
you know, it's certainly in
terms of existential risks,

1037
01:10:19,581 --> 01:10:24,530
it's different from asteroid collides
with the earth. Right, right. Which again,

1038
01:10:24,531 --> 01:10:27,860
is quite possible. Uh, you
know, it's happened in the past.

1039
01:10:28,450 --> 01:10:31,760
It'll probably happen again. We
don't, right. We don't know right now.

1040
01:10:32,000 --> 01:10:35,990
But if we did detect an asteroid that
was going to hit the earth in 75 years

1041
01:10:35,991 --> 01:10:40,100
time, we'd certainly be doing
something about it. Well,

1042
01:10:40,101 --> 01:10:43,610
it's clear that he's got a big rock and
as we'll probably have a meeting and see

1043
01:10:43,611 --> 01:10:46,880
what do we do about the big
rock with Ai, right. Where they,

1044
01:10:46,881 --> 01:10:50,360
I mean there are very few people who
think it's not going to happen within the

1045
01:10:50,361 --> 01:10:55,310
next 75 years. I know Rod Brooks
doesn't think it's going to happen. Uh,

1046
01:10:55,370 --> 01:10:58,130
maybe Andrew Ng doesn't think
it's happened, but you know,

1047
01:10:58,160 --> 01:11:02,890
a lot of the people who work day
to day, uh, you know, as you say,

1048
01:11:02,891 --> 01:11:06,620
he had the rock face,
they think it's going to happen.

1049
01:11:06,621 --> 01:11:11,621
I think the median estimate from AI
researchers is somewhere in 40 to 50 years

1050
01:11:12,291 --> 01:11:14,390
from, from now when maybe
you little, you know,

1051
01:11:14,400 --> 01:11:19,240
I think in Asia they think it's going
to be even faster than that. I am,

1052
01:11:19,310 --> 01:11:22,170
I'm a little bit more conservative.

1053
01:11:22,171 --> 01:11:25,620
I think probably take longer than
that. But I think it's, you know,

1054
01:11:25,621 --> 01:11:30,621
as happened with nuclear weapons and
overnight it can happen overnight that you

1055
01:11:31,651 --> 01:11:35,390
have these breakthroughs and we need
more than one breakthrough. But you know,

1056
01:11:36,210 --> 01:11:40,200
it's on the order of half a dozen.
This is a very rough scale,

1057
01:11:40,201 --> 01:11:44,130
but sort of half a dozen
breakthroughs of that nature

1058
01:11:45,960 --> 01:11:50,490
would have to happen for us to
reach superhuman Ai. But the,

1059
01:11:51,000 --> 01:11:54,630
you know,
the AI research community is vast now.

1060
01:11:54,660 --> 01:11:59,460
The massive investments from
governments, from corporations, uh,

1061
01:11:59,940 --> 01:12:02,370
tons of really, really
smart people. You know,

1062
01:12:02,371 --> 01:12:06,360
you just have to look at the rate of
progress in different areas of AI to see

1063
01:12:06,361 --> 01:12:09,630
that things are moving pretty
fast. So, so to say, oh,

1064
01:12:09,631 --> 01:12:13,710
it's just going to be thousands of years.
I mean, I don't see any basis for that.

1065
01:12:14,070 --> 01:12:18,600
You know, I see, you
know, for example, the,

1066
01:12:19,570 --> 01:12:24,090
the, the Stanford hundred year
Ai Project, right? Which, um, is

1067
01:12:25,650 --> 01:12:29,450
supposed to be sort of, you know,
the serious establishment view, uh,

1068
01:12:29,590 --> 01:12:34,530
their most recent report actually said
it's probably not even possible. Oh,

1069
01:12:34,531 --> 01:12:35,630
wow.
Right.

1070
01:12:35,700 --> 01:12:39,630
Which if you want a perfect
example of people in denial,

1071
01:12:41,040 --> 01:12:44,520
that's it. Because you know,
for the whole history of Ai,

1072
01:12:45,300 --> 01:12:49,880
we've been saying to philosophers
who said it wasn't possible. Well,

1073
01:12:49,881 --> 01:12:52,650
you have no idea what you're talking
about. Of course it's possible. Right.

1074
01:12:52,651 --> 01:12:56,370
Give me an, give me an argument for why
it couldn't happen and there isn't one.

1075
01:12:56,910 --> 01:13:01,040
Right. And now because people
are worried that maybe a,

1076
01:13:01,041 --> 01:13:04,260
I might get a bad name or,
or I just don't want to think about this,

1077
01:13:04,850 --> 01:13:08,150
they are saying, okay, well of course
it's not really possible. You know,

1078
01:13:08,310 --> 01:13:10,500
if we imagine, right?
Imagine if, you know, the,

1079
01:13:11,240 --> 01:13:15,990
the leaders of the cancer
biology community, uh, got
up and said, well, you know,

1080
01:13:15,991 --> 01:13:18,420
of course curing cancer,
it's not really possible.

1081
01:13:19,980 --> 01:13:23,580
It'd be at complete outrage,
dismay,

1082
01:13:23,581 --> 01:13:25,590
and you know,

1083
01:13:26,700 --> 01:13:31,080
I find this really strange phenomenon,
so,

1084
01:13:33,600 --> 01:13:33,841
okay,

1085
01:13:33,841 --> 01:13:38,400
so if you accept that as possible and if
you accept that it's probably going to

1086
01:13:38,401 --> 01:13:42,690
happen.
The point that you're making that,

1087
01:13:43,310 --> 01:13:44,700
you know,
how does it go wrong?

1088
01:13:46,460 --> 01:13:50,400
A valid question without that,
without an answer to that question,

1089
01:13:50,401 --> 01:13:53,640
then you're stuck with what I call the
griller problem, which is, you know,

1090
01:13:53,670 --> 01:13:56,130
the problem that the gorillas face,
right?

1091
01:13:56,131 --> 01:13:58,260
They made something more
intelligent than them,

1092
01:13:58,650 --> 01:14:03,510
namely us a few million years ago,
and now, now they're in deep doodoo.

1093
01:14:04,020 --> 01:14:08,430
Uh, so there's really nothing they
can do. They've lost the control.

1094
01:14:08,660 --> 01:14:13,620
They failed to solve the control problem
of controlling humans. And, uh, so they,

1095
01:14:13,621 --> 01:14:17,020
they lost. Um, so we don't
want to be in that situation.

1096
01:14:17,021 --> 01:14:21,240
And if the gorilla problem is,
is the only formulation you have it,

1097
01:14:21,400 --> 01:14:25,660
there's not a lot you can do, right, other
than to say, okay, we should try to stop,

1098
01:14:26,670 --> 01:14:31,150
you know, we should just not make the
humans or in this case, not make the AI.

1099
01:14:31,600 --> 01:14:36,160
And I think that's
really hard to do to, uh,

1100
01:14:36,690 --> 01:14:41,540
and I'm not actually proposing that
that's a feasible of course of action. Um,

1101
01:14:41,541 --> 01:14:46,180
and I also think that if
properly controlled AI could
be incredibly beneficial.

1102
01:14:47,050 --> 01:14:51,940
So the,
but it seems to me that there's a,

1103
01:14:53,200 --> 01:14:57,760
there's a consensus that one of the major
failure modes is this loss of control.

1104
01:14:57,761 --> 01:15:02,761
That we create AI systems that
are pursuing incorrect objectives.

1105
01:15:03,640 --> 01:15:08,640
And because the AI system believes
it knows what the objective is,

1106
01:15:09,460 --> 01:15:13,420
it has no incentive to listen to
us anymore, so to speak. Right. It,

1107
01:15:14,680 --> 01:15:19,120
it's just carrying out the,
the strategy that it,

1108
01:15:19,240 --> 01:15:22,360
it has computed as being
the optimal solution.

1109
01:15:24,460 --> 01:15:25,510
And, uh, you know,

1110
01:15:25,511 --> 01:15:30,511
it may be that in the process it needs
to acquire more resources to increase the

1111
01:15:31,720 --> 01:15:33,610
possibility of success or you know,

1112
01:15:33,670 --> 01:15:37,600
prevent various failure modes by
defending itself against interference.

1113
01:15:38,140 --> 01:15:42,880
And so that collection of problems
I think is something we can address.

1114
01:15:45,550 --> 01:15:50,550
The other problems are roughly speaking,

1115
01:15:51,520 --> 01:15:55,840
you know, misuse, right? So even
if we solve the control problem,

1116
01:15:55,841 --> 01:16:00,250
we make perfectly safe, controllable
AI systems. Well, why, you know,

1117
01:16:00,270 --> 01:16:02,560
why does Dr Evil, we're
going to use those, right?

1118
01:16:02,561 --> 01:16:06,700
He wants to just take over the world and
he'll make unsafe AI systems but then

1119
01:16:06,701 --> 01:16:11,530
get out of control. So that's one
problem, which is sort of a, you know, uh,

1120
01:16:11,531 --> 01:16:16,140
partly a policing problem.
Parlier uh,

1121
01:16:16,180 --> 01:16:21,180
sort of a cultural problem for the
profession of how we teach people.

1122
01:16:21,600 --> 01:16:23,770
Uh,
what kinds of AI systems are safe.

1123
01:16:23,830 --> 01:16:27,730
You talk about autonomous weapon system
and how pretty much everybody agrees,

1124
01:16:27,800 --> 01:16:30,640
there's too many ways that
that can go horribly wrong.

1125
01:16:30,880 --> 01:16:35,290
Had this great a slot of arts movie that
kind of illustrates that beautifully.

1126
01:16:35,630 --> 01:16:37,450
I want to talk about that.
That's another,

1127
01:16:38,170 --> 01:16:40,030
there's another topic I'm
happy we talk about the,

1128
01:16:40,360 --> 01:16:43,960
just want to mention that what I
see is the third major failure mode,

1129
01:16:43,961 --> 01:16:48,250
which is overused,
not so much misuse but overuse of AI

1130
01:16:49,810 --> 01:16:53,860
that we become overly dependent.
So I call this the Wally problems.

1131
01:16:53,861 --> 01:16:56,530
If you seen Morley, the movie, all right,

1132
01:16:56,531 --> 01:17:01,090
all the humans are on the spaceship and
the machines look after everything for

1133
01:17:01,091 --> 01:17:05,920
them and they just watch TV
and drink big gulps and uh,

1134
01:17:05,921 --> 01:17:07,960
they're all sort of obese and stupid and,

1135
01:17:07,961 --> 01:17:12,961
and they sort of totally any
notion of human autonomy and um,

1136
01:17:14,850 --> 01:17:17,210
you know, so it in effect, right?

1137
01:17:18,260 --> 01:17:22,010
This would happen like those
slow boiling frog, right?

1138
01:17:22,011 --> 01:17:27,011
We would gradually turn over more and
more of the management of our civilization

1139
01:17:27,021 --> 01:17:29,990
to machines as we are already
doing. And this, you know, this,

1140
01:17:30,160 --> 01:17:33,010
if this process continues, you know, we,

1141
01:17:33,011 --> 01:17:38,011
we sort of gradually switch from sort of
being the masters of technology to just

1142
01:17:39,321 --> 01:17:44,120
being the guests, right? So, so
we become guests on a cruise ship,

1143
01:17:44,150 --> 01:17:48,110
you know, which is fine for a week,
but not, not for the rest of eternity,

1144
01:17:49,410 --> 01:17:53,810
you know. And it's almost
irreversible, right? Once you,

1145
01:17:54,440 --> 01:17:57,680
once you lose the incentive to,
for example,

1146
01:17:57,930 --> 01:18:02,180
learn to be an engineer or
a doctor or a sanitation,

1147
01:18:02,570 --> 01:18:04,930
uh, operative or, or any other of the,

1148
01:18:05,900 --> 01:18:10,190
the infinitely many ways that we
maintain and propagate our civilization.

1149
01:18:11,840 --> 01:18:15,290
You know, if you, if you don't have the
incentive to do any of that, you won't.

1150
01:18:16,940 --> 01:18:18,620
And then it's really hard to recover.

1151
01:18:19,160 --> 01:18:21,500
And of course they are just one
of the technologies that could,

1152
01:18:21,501 --> 01:18:25,940
that third failure mode results in that
there's probably other technology in

1153
01:18:25,941 --> 01:18:30,110
general detaches us from, uh,
it does a bit, but the, the,

1154
01:18:30,170 --> 01:18:33,920
the differences that in
terms of the knowledge to,

1155
01:18:34,280 --> 01:18:36,460
to run our civilization,
you know,

1156
01:18:36,470 --> 01:18:40,990
up to now we've had no alternative but
to put it into people's heads. Right,

1157
01:18:41,090 --> 01:18:45,890
right. And if you, the software
with Google, I mean, so
software in general, so cute.

1158
01:18:45,950 --> 01:18:48,260
The computers in general,
but, but the, you know,

1159
01:18:49,310 --> 01:18:53,240
the knowledge of how, you know, how
a sanitation system works, you know,

1160
01:18:53,270 --> 01:18:57,290
that's an AI has to understand that
it's no good putting it into Google.

1161
01:18:58,280 --> 01:19:00,980
So, I mean, we, we've always
put knowledge in on paper,

1162
01:19:01,580 --> 01:19:05,360
but paper doesn't run our civilization
and it only runs when it goes from the

1163
01:19:05,361 --> 01:19:07,580
paper into people's heads again,
right?

1164
01:19:07,581 --> 01:19:11,780
So we've always propagated
civilization through human minds.

1165
01:19:12,140 --> 01:19:17,080
And we've spent about a trillion person
years doing that. I literally write,

1166
01:19:17,100 --> 01:19:18,530
and you can,
you can work it out.

1167
01:19:18,620 --> 01:19:22,430
It's about right is about just over
100 billion people who've ever lived.

1168
01:19:22,880 --> 01:19:23,720
And uh,

1169
01:19:23,721 --> 01:19:28,100
each of them has spent about 10 years
learning stuff to keep their civilization

1170
01:19:28,101 --> 01:19:31,700
going. And, uh, so that's a trillion
person years we put into this effort,

1171
01:19:31,940 --> 01:19:35,930
beautiful way to this hand, all of
civilization. And now we're, you know,

1172
01:19:35,940 --> 01:19:39,700
we're in danger of throwing that away.
So this is a problem that AI can't solve.

1173
01:19:39,701 --> 01:19:44,120
It's not a technical problem. It's
a, you know, if we do our job right,

1174
01:19:44,990 --> 01:19:48,560
the AI systems will say, you
know, the human race doesn't,

1175
01:19:48,890 --> 01:19:51,530
in the long run want to be
passengers and a cruise ship.

1176
01:19:52,250 --> 01:19:56,180
The human race wants autonomy.
This is part of human preferences.

1177
01:19:56,540 --> 01:20:00,620
So we, the AI systems are not
going to do this stuff for you.

1178
01:20:00,920 --> 01:20:03,290
You've got to do it for yourself,
right?

1179
01:20:03,380 --> 01:20:07,380
I'm not going to carry you to the top
of Everest in an autonomous helicopter.

1180
01:20:07,560 --> 01:20:12,090
You have to climb it if you want to
get the benefit. Um, and so on. So,

1181
01:20:14,340 --> 01:20:18,030
but I'm afraid that because
we are short sighted and lazy,

1182
01:20:18,170 --> 01:20:21,540
we're going to override the AI systems.
And,

1183
01:20:22,110 --> 01:20:27,110
and there's an amazing short story that
I recommend to everyone that I talk to

1184
01:20:27,121 --> 01:20:32,121
about this called the machine stops
written in 1909 by Ian Forster,

1185
01:20:33,480 --> 01:20:34,313
who,
you know,

1186
01:20:35,070 --> 01:20:39,330
wrote novels about the British Empire
and sort of things that became costume

1187
01:20:39,331 --> 01:20:42,540
dramas on the BBC. But he wrote
this one science fiction story,

1188
01:20:43,320 --> 01:20:47,850
which is an amazing vision of the future.
It has,

1189
01:20:48,000 --> 01:20:52,920
it has basically iPads. Uh, it has
video conferencing, it has moocs.

1190
01:20:53,400 --> 01:20:57,810
Uh, it has computer, computer
induced obesity. I mean,

1191
01:20:57,811 --> 01:21:02,811
literally it's what people spend their
time doing is giving online courses or

1192
01:21:03,121 --> 01:21:06,090
listening to online courses and,
and, and talking about ideas.

1193
01:21:06,091 --> 01:21:10,620
But they never get out there in the real
world that they don't really have a lot

1194
01:21:10,621 --> 01:21:14,610
of face to face contact. Uh,
everything is done online, you know,

1195
01:21:14,880 --> 01:21:19,260
so all the things we're worrying about
now, uh, were described in the story and,

1196
01:21:19,261 --> 01:21:22,830
and then the human race becomes more
and more dependent on the machine,

1197
01:21:23,550 --> 01:21:27,660
loses knowledge of how
things really run a,

1198
01:21:27,720 --> 01:21:32,160
and then becomes vulnerable to
collapse. And, uh, so it's, uh,

1199
01:21:32,250 --> 01:21:37,250
it's a pretty unbelievably amazing story
for someone writing in 1909 to two.

1200
01:21:38,250 --> 01:21:39,650
Imagine all this less.
Yeah.

1201
01:21:39,990 --> 01:21:44,990
So there's very few people that represent
artificial intelligence more than you

1202
01:21:45,540 --> 01:21:48,000
suit Russell, if you say, okay,

1203
01:21:49,620 --> 01:21:53,990
so it was all my fault. It's all right.

1204
01:21:54,500 --> 01:21:55,333
Um,

1205
01:21:56,430 --> 01:22:00,000
you're often brought up as the
person while Stuart Russell,

1206
01:22:00,520 --> 01:22:04,800
like the AI person is worried about this.
That's why you should be worried about,

1207
01:22:06,090 --> 01:22:10,350
do you feel the burden of that?
I don't know if you feel that at all,

1208
01:22:10,380 --> 01:22:13,000
but when I talk to people like from it,

1209
01:22:13,001 --> 01:22:15,900
you talk about people
outside of computer science,

1210
01:22:15,960 --> 01:22:20,580
when they think about this still
Russell, uh, is worried about AI safety.

1211
01:22:20,581 --> 01:22:23,070
You should be worried too.
Do you feel the burden of that?

1212
01:22:23,970 --> 01:22:28,440
I mean in a practical sense,
uh, yeah, because, uh,

1213
01:22:28,441 --> 01:22:30,780
I get, uh, you know, a dozen,

1214
01:22:30,810 --> 01:22:35,430
sometimes 25 invitations
a day to talk about it,

1215
01:22:36,570 --> 01:22:40,850
to give interviews, to write
press articles and so on. So, um,

1216
01:22:40,920 --> 01:22:43,530
in that very practical sense,

1217
01:22:43,531 --> 01:22:48,070
I'm seeing that people are concerned
and really interested about this. Um,

1218
01:22:48,540 --> 01:22:52,920
by you worried that you could be wrong
as all good scientists are. Of course,

1219
01:22:52,950 --> 01:22:57,840
I worry about that all the time. I mean,
that's, that's always been the way that I,

1220
01:22:58,080 --> 01:23:02,850
I worked, you know, is like I have
an argument in my head with myself,

1221
01:23:02,851 --> 01:23:06,640
right? So I have, I have
some idea and I think, okay,

1222
01:23:07,240 --> 01:23:11,140
how could that be wrong? Or did
someone else already have that idea?

1223
01:23:11,141 --> 01:23:16,141
So I'll go and search and
as much literature as I can
and to see whether someone

1224
01:23:16,841 --> 01:23:21,370
else already thought of that
or, or even refuted it. So,

1225
01:23:21,880 --> 01:23:25,990
you know, I, right now I'm reading
a lot of philosophy because

1226
01:23:27,600 --> 01:23:29,740
you know, in, in the form of the debate,

1227
01:23:29,741 --> 01:23:34,741
so over utilitarianism
and other kinds of moral,

1228
01:23:35,230 --> 01:23:38,770
uh, moral formula formulas
shouldn't, shall we say,

1229
01:23:40,360 --> 01:23:44,770
people have already thought through
some of these issues. But you know what,

1230
01:23:44,800 --> 01:23:48,700
one of the things I'm, I'm not
seeing in a lot of these debates is,

1231
01:23:48,701 --> 01:23:52,420
is this specific idea about,
uh,

1232
01:23:52,780 --> 01:23:56,030
the importance of uncertainty
in the objective, um,

1233
01:23:56,410 --> 01:24:01,390
that this is the way we should think
about machines that are beneficial to

1234
01:24:01,391 --> 01:24:02,710
humans.
So this idea of,

1235
01:24:02,711 --> 01:24:07,150
of provably beneficial machines based on,
uh,

1236
01:24:07,330 --> 01:24:11,860
explicit uncertainty in the objective.
Um, you know, it seems to be,

1237
01:24:13,530 --> 01:24:16,960
you know, my, my gut feeling
is this is the core of it.

1238
01:24:16,961 --> 01:24:21,790
It's going to have to be elaborated in
a lot of different directions and there

1239
01:24:21,791 --> 01:24:26,640
are a lot of really beneficial, yeah. But
they're there. I mean, it has to be right.

1240
01:24:26,830 --> 01:24:31,390
We can't afford, you know, hand wavy
beneficial because they're, you know,

1241
01:24:31,391 --> 01:24:34,210
whenever we do hand wavy stuff,
there are loopholes.

1242
01:24:34,211 --> 01:24:37,420
And the thing about super intelligent
machines is they find the loopholes,

1243
01:24:38,520 --> 01:24:42,140
you know, just like, you
know, tax evaders. Uh,

1244
01:24:42,160 --> 01:24:45,610
if you don't write your tax
little properly that people
will find the loopholes

1245
01:24:45,611 --> 01:24:48,640
and end up paying no tax and, and, uh,

1246
01:24:48,910 --> 01:24:53,910
so you should think of it this way
and getting those definitions right.

1247
01:24:56,590 --> 01:24:56,980
You know,

1248
01:24:56,980 --> 01:25:01,720
it is really a long process,

1249
01:25:02,230 --> 01:25:03,070
you know,
so you can,

1250
01:25:03,071 --> 01:25:06,430
you can define mathematical frameworks
and within that framework you can prove

1251
01:25:06,431 --> 01:25:09,140
mathematical theorems
that yes, this, this,

1252
01:25:09,520 --> 01:25:13,420
this theoretical entity will be provably
beneficial to that theoretical entity.

1253
01:25:13,630 --> 01:25:18,630
But that framework may not match
the real world in some crucial way,

1254
01:25:19,480 --> 01:25:23,260
a long process of thinking through it to
iterating and so on. Last question. Yep.

1255
01:25:23,950 --> 01:25:26,950
Uh,
you have 10 seconds to answer it.

1256
01:25:27,250 --> 01:25:30,310
What is your favorite
Scifi movie about Ai?

1257
01:25:31,390 --> 01:25:35,300
I would say interstellar has
my favorite robots or beets?

1258
01:25:36,000 --> 01:25:40,180
Topsy. Yeah, yeah. Yeah.
So, so Taz, the robots,

1259
01:25:40,420 --> 01:25:43,750
one of the robots in interstellar
is the way a robot should behave.

1260
01:25:45,400 --> 01:25:46,540
And,
uh,

1261
01:25:47,200 --> 01:25:52,200
I would say x Makena is in some ways the
one that's the one that makes you think

1262
01:25:54,900 --> 01:25:58,720
a, in a nervous kind of way about,
about where we're going. Osser sir,

1263
01:25:58,721 --> 01:26:00,520
thank you so much for talking today.
Pleasure.

