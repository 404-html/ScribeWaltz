1
00:00:00,450 --> 00:00:05,450
Today we'll talk about how to make 
machines see computer vision and we will

2
00:00:06,331 --> 00:00:09,150
present thank you.
Whoever said yes,

3
00:00:11,250 --> 00:00:16,250
and today we will present a competition 
that unlike deep traffic which is 

4
00:00:17,911 --> 00:00:22,911
designed to explore ideas,
teach you about concepts so deeper,

5
00:00:23,490 --> 00:00:28,490
deeper enforcement learning SegFuse,
the deep dynamic driving scene 

6
00:00:28,861 --> 00:00:32,700
segmentation,
competition that are present today is at

7
00:00:32,701 --> 00:00:37,650
the very cutting edge.
Whoever does well in this competition is

8
00:00:37,651 --> 00:00:42,651
likely to produce a publication or ideas
that would lead the world in the area of

9
00:00:44,221 --> 00:00:48,390
perception,
perhaps together with the people running

10
00:00:48,391 --> 00:00:50,580
this class,
perhaps in your own.

11
00:00:51,200 --> 00:00:56,200
I encourage you to do so even more cast 
today.

12
00:00:57,870 --> 00:01:02,870
Computer Vision today as it stands is 
deep learning majority of the successes 

13
00:01:07,230 --> 00:01:10,080
in how we interpret form 
representations,

14
00:01:10,230 --> 00:01:15,230
understand images and videos utilize to 
a significant degree and you're on that 

15
00:01:16,181 --> 00:01:21,181
works.
The very ideas we've been talking about 

16
00:01:21,181 --> 00:01:24,560
that applies for supervised,
unsupervised and reinforcement learning 

17
00:01:26,290 --> 00:01:29,810
and for the supervised case is the focus
of today.

18
00:01:30,740 --> 00:01:34,850
The process is the same.
The data is essential.

19
00:01:34,940 --> 00:01:39,940
There's annotated data where the human 
provides the labels that serves as the 

20
00:01:39,940 --> 00:01:44,141
ground truth and the training process.
Then the neural network goes through 

21
00:01:45,531 --> 00:01:50,531
that data,
learning to map from the raw sensory 

22
00:01:50,531 --> 00:01:55,031
input to the ground truth labels and 
then generalize or the testing data set

23
00:01:57,320 --> 00:02:00,200
and the kind of raw senses we're dealing
with are numbers.

24
00:02:01,280 --> 00:02:05,900
I'll say this again and again that for 
human vision for us here,

25
00:02:05,930 --> 00:02:10,930
we take for granted this particular 
aspect of our ability is to take in raw 

26
00:02:10,930 --> 00:02:13,130
sensor information through our eyes and 
interpret it,

27
00:02:13,880 --> 00:02:18,880
but it's just numbers.
That's something whether you're an 

28
00:02:18,880 --> 00:02:20,960
expert in computer vision person or new 
to the field,

29
00:02:21,020 --> 00:02:26,020
you have to always go back to meditate 
on is what kind of things the machine is

30
00:02:27,381 --> 00:02:28,610
given,
what,

31
00:02:28,640 --> 00:02:33,640
what?
What is the data that is tasked to work 

32
00:02:33,640 --> 00:02:35,150
with in order to perform the task you're
asking it to do?

33
00:02:35,750 --> 00:02:40,750
Perhaps the data is given is highly 
insufficient to do what you wanted to 

34
00:02:40,971 --> 00:02:45,971
do.
That's the question I'll come up again 

35
00:02:45,971 --> 00:02:48,071
and again our images and enough to 
understand the world around you and 

36
00:02:51,710 --> 00:02:54,830
given these numbers,
these set of numbers,

37
00:02:54,831 --> 00:02:59,831
sometimes with one channel,
sometimes with three rgb where every 

38
00:02:59,831 --> 00:03:03,761
single have three different colors.
The task is to classify or regress 

39
00:03:07,440 --> 00:03:12,440
producing continuous variable or one of 
a set of class labels as before,

40
00:03:16,550 --> 00:03:21,550
we must be careful about our intuition 
of what is hard,

41
00:03:21,990 --> 00:03:23,600
what is easy and computer vision.

42
00:03:28,210 --> 00:03:33,210
Let's take a step back to the 
inspiration for an year old networks,

43
00:03:34,420 --> 00:03:39,420
our own biological neural networks 
because the human vision system and the 

44
00:03:40,061 --> 00:03:44,050
computer vision system is a little bit 
more similar in these regards.

45
00:03:52,360 --> 00:03:57,360
The structure of the human visual Cortex
is in layers and his information passes 

46
00:03:58,480 --> 00:04:03,480
from the eyes of the to the parts of the
brain that makes sense of the influence,

47
00:04:03,700 --> 00:04:07,750
the raw sensor information hiring higher
order representations of formed.

48
00:04:08,830 --> 00:04:13,830
This is the inspiration,
the idea behind using deep neural 

49
00:04:13,830 --> 00:04:17,821
networks for images higher and higher 
order representation is a form for the 

50
00:04:17,821 --> 00:04:18,190
layers,

51
00:04:19,980 --> 00:04:24,980
the early layers taking in the very raw 
in sensory information that extracting 

52
00:04:25,830 --> 00:04:28,830
edges,
connecting those edges,

53
00:04:28,831 --> 00:04:33,831
forming those edges to form more complex
features and finally into the higher 

54
00:04:33,831 --> 00:04:38,511
order semantic meaning that we hope to 
get from these images and computer 

55
00:04:39,241 --> 00:04:41,160
vision.
Deep learning is hard.

56
00:04:42,180 --> 00:04:47,180
I'll say this again.
The illumination variability is the 

57
00:04:47,180 --> 00:04:48,030
biggest challenge,
or at least one of the,

58
00:04:48,120 --> 00:04:53,120
one of the biggest challenges in driving
for visible light cameras pose 

59
00:04:55,351 --> 00:04:58,110
variability.
The objects,

60
00:04:59,010 --> 00:05:04,010
as I'll also discuss about some of the 
advances from Geoff Hinton and the 

61
00:05:04,010 --> 00:05:07,521
capsule networks.
The idea with neural networks as they 

62
00:05:07,521 --> 00:05:12,341
are currently used for computer vision 
are not good with representing variable 

63
00:05:12,571 --> 00:05:17,571
pose.
These objects in images and it's too 

64
00:05:17,571 --> 00:05:21,891
deep.
Plane of color and texture look very 

65
00:05:21,891 --> 00:05:25,641
different numerically when the object is
rotated and the object is mangled and 

66
00:05:27,681 --> 00:05:32,681
shaped in different ways.
The deformable will truncate a cat 

67
00:05:32,681 --> 00:05:36,690
interclass variability.
The classification task,

68
00:05:36,691 --> 00:05:41,691
which would be an example today 
throughout to introduce some of the 

69
00:05:41,691 --> 00:05:46,011
networks over the past decade that have 
received success and some of the 

70
00:05:46,011 --> 00:05:47,370
intuition and insight that made those 
networks work.

71
00:05:47,670 --> 00:05:52,110
Classification,
there is a lot of variability inside the

72
00:05:52,111 --> 00:05:55,470
classes and very little variability 
between the classes.

73
00:05:57,070 --> 00:05:58,350
All of these cats

74
00:05:58,390 --> 00:06:00,610
at top,
all those are dogs a bottom.

75
00:06:01,060 --> 00:06:06,060
They look very different and the other,
I would say the second biggest problem 

76
00:06:06,060 --> 00:06:08,760
in driving perception,
visible light camera perceptions,

77
00:06:08,810 --> 00:06:13,810
occlusion when part of the object is 
occluded due to the three dimensional

78
00:06:15,220 --> 00:06:20,220
nature of our world,
some objects in front of others and they

79
00:06:20,441 --> 00:06:25,441
occlude the background object.
And yet we're still tasked with 

80
00:06:25,441 --> 00:06:28,720
identifying the object when only part of
it is visible.

81
00:06:29,200 --> 00:06:34,200
And sometimes that part I told you 
there's cats is very hardly visible 

82
00:06:34,690 --> 00:06:37,510
here.
We're tasked with classifying a cat with

83
00:06:37,511 --> 00:06:42,511
just an ears visible,
just the leg and on a philosophical 

84
00:06:46,121 --> 00:06:50,110
level as we'll talk about the motivation
for our competition here.

85
00:06:50,530 --> 00:06:51,550
Here's a,
a,

86
00:06:51,620 --> 00:06:53,380
a,
a cat dressed as a monk,

87
00:06:53,381 --> 00:06:56,920
eating a banana on a philosophical 
level.

88
00:06:58,240 --> 00:07:00,520
Most of us,
uh,

89
00:07:00,940 --> 00:07:05,140
understand what's going on in the scene.
In fact,

90
00:07:05,280 --> 00:07:10,280
a neural network to today successfully 
classify this,

91
00:07:12,460 --> 00:07:16,930
uh,
image this video as a cat,

92
00:07:18,010 --> 00:07:21,820
but the context,
the humor of the situation,

93
00:07:21,821 --> 00:07:26,680
and in fact you could argue it's a 
monkey is missing.

94
00:07:27,250 --> 00:07:30,640
And what else is missing is the dynamic 
information,

95
00:07:30,820 --> 00:07:32,530
the temporal dynamics of the scene.

96
00:07:34,990 --> 00:07:39,990
That's what's missing in a lot of the 
perception work that has been done to 

97
00:07:39,990 --> 00:07:42,460
date in the autonomous vehicle space,
uh,

98
00:07:42,670 --> 00:07:47,020
in terms of visible light cameras and 
we're looking to expand on that.

99
00:07:47,470 --> 00:07:49,600
That's what segue.
Fuse is all about.

100
00:07:50,380 --> 00:07:54,550
Image classification pipeline.
There's a been with different categories

101
00:07:54,551 --> 00:07:56,770
inside each class.
Cat,

102
00:07:56,771 --> 00:07:58,120
dog Mug,
hat,

103
00:07:58,840 --> 00:08:03,840
those bins.
There's a lot of examples of each and 

104
00:08:03,840 --> 00:08:07,141
your task with when a new example comes 
along you've never seen before to put 

105
00:08:07,141 --> 00:08:11,161
that image in a bin.
It's the same as the machine learning 

106
00:08:11,161 --> 00:08:15,091
task before and everything relies on the
data that has been ground truth,

107
00:08:16,480 --> 00:08:21,480
that have been labeled by human beings.
Amnesty as a toy data set of handwritten

108
00:08:22,691 --> 00:08:27,130
digits,
often using as examples and Coco psyfari

109
00:08:27,160 --> 00:08:30,550
image net places,
and a lot of other incredible datasets.

110
00:08:30,580 --> 00:08:35,580
Rich data sets of 100 thousands,
millions of images out there represent 

111
00:08:35,721 --> 00:08:39,310
scenes,
people's faces and different objects.

112
00:08:39,670 --> 00:08:44,670
Those are all ground truth data for 
testing algorithms and for competing 

113
00:08:46,271 --> 00:08:48,940
architectures to be evaluated against 
each other.

114
00:08:49,720 --> 00:08:51,880
Cfr Ten,
one of the simplest,

115
00:08:52,780 --> 00:08:57,380
almost toy datasets of tiny with 10 
categories of airplane,

116
00:08:57,390 --> 00:08:58,830
automobile,
Bird,

117
00:08:58,831 --> 00:08:59,430
cat,
deere,

118
00:08:59,431 --> 00:09:00,150
dog,
frog,

119
00:09:00,151 --> 00:09:05,151
course,
ship and truck is commonly used to 

120
00:09:05,151 --> 00:09:08,031
explore.
Some of the basic convolutional neural 

121
00:09:08,031 --> 00:09:08,190
networks will discuss,
so let's come up with a very trivial,

122
00:09:08,191 --> 00:09:11,880
classify it to explain the concept of 
how we could go about it.

123
00:09:12,600 --> 00:09:17,600
In fact,
this is maybe if you start to think 

124
00:09:17,600 --> 00:09:20,121
about how to classify an image.
If you don't know any of these 

125
00:09:20,121 --> 00:09:23,241
techniques,
this is perhaps the approach you would 

126
00:09:23,241 --> 00:09:25,641
take is you would subtract images.
So in order to know that an image of a 

127
00:09:25,771 --> 00:09:30,771
cat is different than image of a dog,
you have to compare them when given 

128
00:09:30,771 --> 00:09:30,860
those two images,
what?

129
00:09:30,890 --> 00:09:33,090
What's the what's the way you compare 
them?

130
00:09:33,900 --> 00:09:38,900
One way you could do it is you just 
subtract it and then some all the pixel 

131
00:09:38,900 --> 00:09:42,840
wise differences in the image.
Just subtract the intensity of the image

132
00:09:42,870 --> 00:09:46,530
pixel by Pixel.
Sum It up if that intent,

133
00:09:46,560 --> 00:09:51,560
if that difference is really high,
that means the image is a very 

134
00:09:51,560 --> 00:09:51,560
different.

135
00:09:51,560 --> 00:09:56,180
Using that metric,
we can look at cfr 10 and use it as a 

136
00:09:56,180 --> 00:10:00,120
classifier saying,
based on this difference function,

137
00:10:00,390 --> 00:10:05,390
I'm going to find one of the 10 bins for
a new image that that is cool,

138
00:10:07,240 --> 00:10:12,240
that has the lowest difference.
Find an image in this dataset that is 

139
00:10:13,511 --> 00:10:16,540
most like the image I have and put it in
the same bin.

140
00:10:16,541 --> 00:10:21,520
Is that images in?
So there's 10 classes.

141
00:10:21,521 --> 00:10:26,521
If we just flip a coin,
the accuracy of our classifier will be 

142
00:10:26,521 --> 00:10:28,420
10 percent.
Using our image difference classify,

143
00:10:28,421 --> 00:10:33,421
we can actually do pretty good.
Much better than random was better than 

144
00:10:33,421 --> 00:10:34,780
10 percent.
We can do 35,

145
00:10:34,781 --> 00:10:39,781
38 percent accuracy.
That's a classifier wherever first 

146
00:10:40,750 --> 00:10:45,750
classifier,
k nearest neighbors.

147
00:10:46,530 --> 00:10:51,530
Let's take our classifier to a whole new
level instead of comparing it to just 

148
00:10:51,960 --> 00:10:56,960
fight.
Trying to find one image that's the 

149
00:10:56,960 --> 00:10:59,511
closest in our dataset.
We tried to find k closest and say what 

150
00:10:59,791 --> 00:11:02,820
is what class do the majority of them 
belong to?

151
00:11:03,330 --> 00:11:06,210
And we take that K and increase it for 
one to two,

152
00:11:06,211 --> 00:11:07,560
to three,
to four to five,

153
00:11:08,790 --> 00:11:13,790
and see how that changes the problem 
with seven years neighbors,

154
00:11:14,541 --> 00:11:17,450
which is the optimal under this approach
for cfr 10,

155
00:11:20,610 --> 00:11:25,610
we achieved 30 percent accuracy.
Human level is 95 percent accuracy and 

156
00:11:28,390 --> 00:11:31,760
convolutional neural networks will get 
very close to a 100 percent.

157
00:11:34,260 --> 00:11:39,260
That's were you on.
That works shine this very task of 

158
00:11:41,691 --> 00:11:46,691
binning images.
It all starts with this basic 

159
00:11:46,691 --> 00:11:49,490
computational unit signal in each of the
signals are wade summed,

160
00:11:51,980 --> 00:11:53,150
bias added

161
00:11:55,140 --> 00:12:00,140
and put an input into a nonlinear 
activation function that produces an 

162
00:12:00,140 --> 00:12:04,181
output.
The nonlinear activation function is 

163
00:12:04,181 --> 00:12:07,811
key.
All of these put together and more and 

164
00:12:07,971 --> 00:12:12,560
more hidden layers form a deep neural 
network,

165
00:12:12,650 --> 00:12:17,650
and that deep neural network is trained 
as we've discussed by taking a forward 

166
00:12:17,661 --> 00:12:20,600
pass on examples,
have ground truth labels.

167
00:12:20,690 --> 00:12:24,050
Seeing how close those labels are too,
the real ground truth,

168
00:12:24,350 --> 00:12:29,350
and then punishing the weights that 
resulted in the incorrect decisions and 

169
00:12:29,871 --> 00:12:32,480
rewarding the weights that results in 
incorrect decisions.

170
00:12:33,800 --> 00:12:38,800
For the case of 10 examples,
the output of the network is 10 

171
00:12:40,041 --> 00:12:45,041
different values.
The input being handwritten digits from 

172
00:12:46,651 --> 00:12:51,651
zero to nine,
10 of those and one of our network to 

173
00:12:52,441 --> 00:12:57,441
classify what is in this image of a 
handwritten digit is at one zero,

174
00:12:58,201 --> 00:12:58,620
one,
two,

175
00:12:58,621 --> 00:13:03,621
three through nine.
The way it's often done is there's 10 

176
00:13:03,811 --> 00:13:08,811
outputs of the network and each of the 
neurons and the output is responsible 

177
00:13:12,061 --> 00:13:17,061
for getting really excited when it's 
number is called and everybody else is 

178
00:13:18,811 --> 00:13:23,811
supposed to be not excited.
Therefore the number of classes is the 

179
00:13:24,301 --> 00:13:29,301
number of outputs.
That's how it's commonly done and you 

180
00:13:29,301 --> 00:13:32,460
assign a class to the input image based 
on the highest,

181
00:13:32,760 --> 00:13:35,250
the neuron which produces the highest 
output,

182
00:13:36,870 --> 00:13:40,530
but that's for a fully connected network
that we've discussed on Monday.

183
00:13:42,320 --> 00:13:47,320
There is in deep learning a lot of 
tricks that make things work that make 

184
00:13:47,721 --> 00:13:52,721
training much more efficient on large 
class problems where there's a lot of 

185
00:13:54,051 --> 00:13:59,051
classes on large data sets.
When the representation that the neural 

186
00:13:59,051 --> 00:14:03,581
network is tasked with learning is 
extremely complex and that's where 

187
00:14:03,581 --> 00:14:05,090
convolutional neural network step in 
that trick.

188
00:14:05,091 --> 00:14:10,091
They use a spatial invariance.
They use the idea that a cat in the top 

189
00:14:12,261 --> 00:14:17,261
left corner of an image is the same as a
cat in the bottom right corner of an 

190
00:14:17,261 --> 00:14:20,330
image,
so we can learn the same features across

191
00:14:20,331 --> 00:14:25,331
the image.
That's where the convolution operation 

192
00:14:25,331 --> 00:14:29,850
steps in.
Instead of the fully connected networks 

193
00:14:29,850 --> 00:14:32,840
here,
there's a third dimension of depth,

194
00:14:33,530 --> 00:14:38,530
so the blocks in this neural network 
that as input take three d volumes in 

195
00:14:39,131 --> 00:14:41,180
this output produced three d volumes.

196
00:14:46,890 --> 00:14:51,890
They take a slice of the image,
a window and it across applying this 

197
00:14:53,491 --> 00:14:56,030
same exact weights and we'll go through 
an example,

198
00:14:56,330 --> 00:15:01,330
the same exact weights as in the fully 
connected network on the edges that are 

199
00:15:01,330 --> 00:15:06,251
used to map the input to the output.
Here are used to map the slice of an 

200
00:15:08,001 --> 00:15:10,880
image,
this window of an image to the output,

201
00:15:12,350 --> 00:15:17,350
and you can make several,
many of such convolutional filters,

202
00:15:17,870 --> 00:15:22,870
many layers,
many different options of what kind of 

203
00:15:22,870 --> 00:15:27,530
features do you look for in an image or 
kind of window you slide across in order

204
00:15:27,531 --> 00:15:31,430
to extract all kinds of things.
All kinds of edges,

205
00:15:31,730 --> 00:15:34,490
all kinds of higher order patterns and 
the images.

206
00:15:36,530 --> 00:15:40,580
The very important thing is the 
parameters on each of these filters,

207
00:15:40,730 --> 00:15:43,850
these subset of the image,
these windows are shared.

208
00:15:44,780 --> 00:15:49,780
If the feature,
the that defines a cat is useful in the 

209
00:15:49,780 --> 00:15:50,960
top left corner,
it's useful on the top right corner.

210
00:15:50,961 --> 00:15:53,390
It's useful in every aspect of the 
image.

211
00:15:53,690 --> 00:15:58,690
This is the trick that makes 
convolutional neural network save a lot 

212
00:15:58,690 --> 00:16:02,771
of a lot of parameters.
Reduced parameter significantly is the 

213
00:16:04,121 --> 00:16:09,121
reuse,
the spatial sharing of features across 

214
00:16:09,121 --> 00:16:09,490
the space of the image.

215
00:16:12,870 --> 00:16:16,500
The depth of these three d volumes is 
the number of filters.

216
00:16:17,820 --> 00:16:21,960
The stride is the skip of the filter,
the step size,

217
00:16:22,260 --> 00:16:27,240
how many pixels you skip when you apply 
the filter to the input,

218
00:16:28,110 --> 00:16:33,110
and the padding is the padding,
the zero padding on the outside of the 

219
00:16:34,591 --> 00:16:39,330
input to a convolutional layer.
Let's go through an example,

220
00:16:41,160 --> 00:16:45,600
so on the left here and the slides are 
available online.

221
00:16:45,601 --> 00:16:49,380
You can follow them along and I'll step 
through this example.

222
00:16:50,070 --> 00:16:53,940
On the left here is input volume of 
three channels.

223
00:16:55,140 --> 00:16:58,430
The left column is the input.
The three blocks,

224
00:16:58,460 --> 00:17:02,460
the three squares.
There are the three channels and there's

225
00:17:02,461 --> 00:17:07,461
numbers inside those channels,
and then we have a filter in red,

226
00:17:12,490 --> 00:17:16,990
two of them,
two channels of filters with a bias,

227
00:17:17,440 --> 00:17:22,440
and we those filters are three by three.
Each one of them is size three by three,

228
00:17:25,090 --> 00:17:30,090
and what we do is we take those three by
three filters that are to be learned.

229
00:17:30,310 --> 00:17:33,100
These are variables are weights that 
will have to learn,

230
00:17:34,120 --> 00:17:38,740
and then we slide it across an image to 
produce the output on the right,

231
00:17:38,800 --> 00:17:42,850
the green,
so by applying the filters in the red,

232
00:17:43,150 --> 00:17:48,150
there's two of them,
and within each one there's one for 

233
00:17:48,150 --> 00:17:51,210
every input channel,
we go from the left to the right,

234
00:17:51,720 --> 00:17:56,310
from the input volume in the left to the
output volume green on the right,

235
00:17:57,780 --> 00:18:02,070
and you can look at it.
You can pull up the slides yourself.

236
00:18:02,071 --> 00:18:04,350
Now if you can't see the numbers on the 
screen,

237
00:18:04,740 --> 00:18:09,740
but the operations are performed on the 
input to produce the single value that's

238
00:18:13,081 --> 00:18:15,120
highlighted there in the green and the 
output,

239
00:18:15,840 --> 00:18:20,840
and we slide this convolution.
No filter along the image with the 

240
00:18:22,591 --> 00:18:27,591
stride in this case,
have to skipping,

241
00:18:28,380 --> 00:18:33,380
skipping along.
They some to the to the right,

242
00:18:33,540 --> 00:18:38,100
the two channel I'll put in green,

243
00:18:39,240 --> 00:18:41,550
that's it.
That's the convolutional operation.

244
00:18:42,330 --> 00:18:46,830
That's what's called a convolutional 
layer neural networks and the parameters

245
00:18:46,831 --> 00:18:51,831
here,
besides the bias of the red values in 

246
00:18:51,831 --> 00:18:56,181
the middle,
that's what we're trying to learn and 

247
00:18:56,181 --> 00:18:58,830
there's a lot of interesting tricks 
we'll discuss today on top of those,

248
00:18:58,980 --> 00:19:01,680
but this is at the core.
This is the specialty.

249
00:19:01,681 --> 00:19:06,681
Invariant sharing of parameters that may
convolution you're on that works.

250
00:19:07,390 --> 00:19:12,390
I'm able to efficiently learn and find 
patterns and images to build your 

251
00:19:14,311 --> 00:19:18,180
intuition a little bit more about 
convolution.

252
00:19:18,181 --> 00:19:21,240
Here's an input image on the left and on
the right,

253
00:19:22,260 --> 00:19:26,370
the identity filter produces the output 
you see on the right,

254
00:19:26,430 --> 00:19:28,710
and then there is different ways you 
can,

255
00:19:28,711 --> 00:19:33,711
different kinds of edges.
You can extract with the activate where 

256
00:19:34,171 --> 00:19:36,780
the resulting activation maps scene on 
the right,

257
00:19:37,260 --> 00:19:42,260
so when applying the filters,
those edge detection filters to the 

258
00:19:43,021 --> 00:19:48,021
image on the left you produce in white 
are the parts that activate the 

259
00:19:48,210 --> 00:19:51,570
convolution.
The results of these filters,

260
00:19:55,310 --> 00:19:58,430
and so you can do any kind of filter.
That's what we're trying to learn.

261
00:19:58,610 --> 00:20:03,610
Any kind of edge,
any kind of any kind of pattern you can 

262
00:20:04,101 --> 00:20:06,920
move along in this window and this way 
that's shown here.

263
00:20:06,921 --> 00:20:11,921
You slide around the image and you 
produce the output you see on the right 

264
00:20:12,080 --> 00:20:14,570
and depending on how many filters you 
have in every level,

265
00:20:14,571 --> 00:20:17,510
you have many of the slices,
the on the right,

266
00:20:17,750 --> 00:20:20,570
the input on the left,
I'll put it on the right.

267
00:20:20,900 --> 00:20:25,900
If you have dozens of filters,
you would have dozens of images on the 

268
00:20:25,900 --> 00:20:30,101
right,
each with different results that show 

269
00:20:30,410 --> 00:20:35,410
where each of the individual filter 
patterns were found and we learn what 

270
00:20:35,451 --> 00:20:40,160
patterns are useful to look for in order
to perform the classification task.

271
00:20:40,850 --> 00:20:43,760
That's the task for the neural network 
to learn.

272
00:20:43,790 --> 00:20:48,790
These filters and the filters have 
higher and higher order of 

273
00:20:50,070 --> 00:20:55,070
representation.
Going from the very basic edges to the 

274
00:20:56,001 --> 00:21:01,001
high semantic meaning.
That spans entire images and the ability

275
00:21:03,251 --> 00:21:06,430
to span images can be done in several 
ways,

276
00:21:06,730 --> 00:21:09,760
but traditionally has been successfully 
done through Max pooling,

277
00:21:09,761 --> 00:21:14,761
through pooling of taking the output of 
a convolutional operation and reducing 

278
00:21:18,981 --> 00:21:23,981
the resolution of that by by condensing 
that information,

279
00:21:24,321 --> 00:21:26,570
but for example,
taking the maximum values,

280
00:21:26,571 --> 00:21:27,830
the maximum activations,

281
00:21:31,920 --> 00:21:36,920
therefore reducing the spatial 
resolution which has detrimental effects

282
00:21:36,991 --> 00:21:39,240
as we'll talk about in scene 
segmentation,

283
00:21:39,480 --> 00:21:44,480
but it's beneficial for finding higher 
order representations and the images 

284
00:21:44,670 --> 00:21:49,670
that bring images together that bring 
features together to form an entity that

285
00:21:49,831 --> 00:21:53,790
we're trying to identify and classify.
Okay,

286
00:21:54,210 --> 00:21:59,210
so that forms a convolution.
You'll networks such convolutional 

287
00:21:59,210 --> 00:22:03,651
layers stacked on top of each other is 
the only addition to a neural network 

288
00:22:03,651 --> 00:22:07,701
that makes for a convolutional neural 
network and then at the end the fully 

289
00:22:07,701 --> 00:22:12,531
connected layers or any kind of other 
architectures allow us to apply 

290
00:22:13,840 --> 00:22:18,840
particular domains.
Let's take image net as a case study in 

291
00:22:21,330 --> 00:22:26,330
image net dataset and image net.
The challenge,

292
00:22:27,120 --> 00:22:30,780
the task is classification.
As I mentioned,

293
00:22:30,781 --> 00:22:33,840
the first lecture,
imaging as a data set,

294
00:22:34,830 --> 00:22:36,900
one of the largest in the world of 
images.

295
00:22:37,480 --> 00:22:40,100
The $14 million images,
21,000

296
00:22:40,110 --> 00:22:45,110
categories and a lot of depth to many of
the categories,

297
00:22:46,590 --> 00:22:49,200
as I mentioned,
1200 granny Smith apples.

298
00:22:53,190 --> 00:22:58,190
These allow to these allow them to learn
the rich representations in both posed 

299
00:23:00,091 --> 00:23:05,091
lighting variability and intercluster 
class variation for the particular 

300
00:23:05,091 --> 00:23:08,571
things.
Particular classes like Granny Smith 

301
00:23:08,571 --> 00:23:12,351
apples,
so let's look through the various 

302
00:23:12,351 --> 00:23:12,540
networks.
Let's discuss them.

303
00:23:12,660 --> 00:23:15,950
Let's see the insights.
It started with Alex Net,

304
00:23:15,990 --> 00:23:20,990
the first really big successful GPU 
train neural network on image net that's

305
00:23:21,421 --> 00:23:26,421
achieved a significant boost over the 
previous year and moved onto vgg net,

306
00:23:28,530 --> 00:23:33,530
Google net,
a Goo lanette resonate yet cua image and

307
00:23:35,490 --> 00:23:38,280
as ynet in 2017.

308
00:23:40,780 --> 00:23:45,780
Again,
the numbers will show for the accuracy 

309
00:23:45,780 --> 00:23:48,871
or based on the top five error rate,
we get five guesses and it's a one or 

310
00:23:49,731 --> 00:23:51,260
zero.
If you get guests,

311
00:23:51,290 --> 00:23:54,640
if one of the five is correct,
you get a one for that particular guest.

312
00:23:55,010 --> 00:23:56,150
Otherwise it's a zero

313
00:24:01,780 --> 00:24:06,780
and human error is five point one.
When a human tries to achieve the same 

314
00:24:06,780 --> 00:24:09,930
tries to perform the same task.
As the machinist asks,

315
00:24:09,931 --> 00:24:14,380
we're doing the air is five point one,
the human adaptation that's performed on

316
00:24:14,381 --> 00:24:17,110
the images based on binary 
classification,

317
00:24:17,230 --> 00:24:22,180
Granny Smith,
apple are not cat or not the actual task

318
00:24:22,270 --> 00:24:26,620
that the machine has to perform and that
the human competing has to perform,

319
00:24:26,800 --> 00:24:31,480
is given an image,
is provide one of the many classes under

320
00:24:31,481 --> 00:24:34,000
that human error is five point one 
percent,

321
00:24:34,060 --> 00:24:37,690
which was surpassed in 2015 by residents
yet,

322
00:24:38,320 --> 00:24:40,840
uh,
to achieve four percent error.

323
00:24:44,250 --> 00:24:48,240
So let's start with Alex Net.
I'll zoom in on the later networks.

324
00:24:48,241 --> 00:24:53,241
They have some interesting insights,
but Alex Net and Vgg net both followed a

325
00:24:54,391 --> 00:24:58,680
very similar architecture,
very uniform throughout its depth.

326
00:25:02,030 --> 00:25:07,030
Vgg Net in 2014 is convolution 
convolution pooling,

327
00:25:08,360 --> 00:25:10,580
convolution pooling,
convolution pooling,

328
00:25:10,760 --> 00:25:15,760
and fully connected layers at the end 
does a certain kind of beautiful 

329
00:25:15,760 --> 00:25:20,711
simplicity uniformity to these 
architectures because you can just make 

330
00:25:20,711 --> 00:25:24,441
a deeper and deeper and makes it very 
amenable to a implementation in the 

331
00:25:24,501 --> 00:25:29,501
layer stack kind of way.
And in any of the deep learning 

332
00:25:29,501 --> 00:25:30,980
frameworks,
it's clean and beautiful to understand.

333
00:25:31,460 --> 00:25:36,380
In the case of egg net 16 or 19 layers 
with 138 million parameters,

334
00:25:36,680 --> 00:25:38,840
not many optimizations on these 
parameters.

335
00:25:38,841 --> 00:25:43,841
Therefore the number of parameters is 
much higher than the networks that 

336
00:25:43,841 --> 00:25:45,500
followed it.
Despite the layers not being that large.

337
00:25:48,070 --> 00:25:50,590
Google net introduced the inception 
module,

338
00:25:51,100 --> 00:25:56,100
starting to do some interesting things 
with the small modules within these 

339
00:25:57,191 --> 00:26:01,690
networks which allow for the training to
be more efficient and effective.

340
00:26:03,760 --> 00:26:08,760
The idea behind the inception module 
shown here with the previous layer on 

341
00:26:08,771 --> 00:26:13,771
bottom and the convolutional layer here 
with the inception module

342
00:26:16,690 --> 00:26:21,690
on top producer on top is it used the 
idea that different size convolutions 

343
00:26:25,541 --> 00:26:30,541
provide different value for the network.
Smaller convolutions are able to capture

344
00:26:31,630 --> 00:26:36,630
or propagate forward features that are 
very local in high resolution and in,

345
00:26:39,430 --> 00:26:40,750
in,
in texture.

346
00:26:41,530 --> 00:26:46,530
Larger convolutions are better able to 
to represent and capture and catch 

347
00:26:47,850 --> 00:26:51,030
highly abstracted features,
higher order features.

348
00:26:51,690 --> 00:26:54,600
So the idea behind the inception module 
is to say,

349
00:26:54,601 --> 00:26:59,601
well,
as opposed to choosing an in a hyper 

350
00:26:59,601 --> 00:27:01,710
parameter tuning process or architecture
design process,

351
00:27:01,711 --> 00:27:05,010
choosing which convolution size we want 
to go with,

352
00:27:05,430 --> 00:27:09,570
why not do all of them?
To get well several together in the case

353
00:27:09,571 --> 00:27:13,200
of the Google net model,
there's the one by one,

354
00:27:13,230 --> 00:27:18,230
three by three and five by five 
convolutions with the old trusty friend 

355
00:27:18,230 --> 00:27:19,980
of Max pooling still left in there as 
well,

356
00:27:21,540 --> 00:27:25,800
which has lost favor more and more over 
time for the image classification task,

357
00:27:27,450 --> 00:27:31,170
and the result is there's fewer 
parameters are required.

358
00:27:31,200 --> 00:27:36,200
If you pick the placing of these 
inception modules correctly,

359
00:27:37,350 --> 00:27:42,350
the number of parameters required to 
achieve a higher performance is much 

360
00:27:42,350 --> 00:27:42,780
lower

361
00:27:46,220 --> 00:27:51,220
radnet,
one of the most popular still to date 

362
00:27:53,770 --> 00:27:58,770
architectures that we'll discuss in and 
scene segmentation as well came up and 

363
00:28:01,751 --> 00:28:06,751
use the idea of a residual block.
The initial inspiring observation,

364
00:28:08,381 --> 00:28:11,230
which doesn't necessarily hold true as 
it turns out,

365
00:28:11,560 --> 00:28:16,560
but that network depth increases 
representation power,

366
00:28:16,960 --> 00:28:21,520
so these residual blocks allow you to 
have much deeper networks,

367
00:28:21,521 --> 00:28:26,521
and I'll explain why in a second here,
but the thought was they worked so well 

368
00:28:28,121 --> 00:28:33,121
because the networks how much deeper.
The key thing that makes these blocks so

369
00:28:33,941 --> 00:28:38,941
effective is the same idea.
It's very reminiscent of recurrent 

370
00:28:38,941 --> 00:28:43,741
neural networks that I hope we get a 
chance to talk about the training of 

371
00:28:45,011 --> 00:28:50,011
them as much easier.
They will take a simple block repeated 

372
00:28:50,921 --> 00:28:55,010
over and over,
and they passed the input along with our

373
00:28:55,011 --> 00:29:00,011
transformation along with the ability to
transform and to learn to learn the 

374
00:29:01,421 --> 00:29:05,980
filters,
learn the weights so you're allowed to.

375
00:29:07,030 --> 00:29:12,030
You're allowed every layer to not only 
take on the processing of previous 

376
00:29:12,941 --> 00:29:17,941
layers,
but to take in the wrong transform data 

377
00:29:17,941 --> 00:29:21,750
and learn something new.
The ability to learn something new 

378
00:29:21,750 --> 00:29:26,521
allows you to have much deeper networks 
and the simplicity of this block allows 

379
00:29:27,761 --> 00:29:30,010
for more effective training.

380
00:29:33,330 --> 00:29:38,330
These state of the art in 2017,
the winner is squeezing excitation 

381
00:29:38,330 --> 00:29:42,770
networks.
That unlike the previous year was cu 

382
00:29:42,770 --> 00:29:47,430
image with shimply took ensemble methods
and combined a lot of successful 

383
00:29:47,430 --> 00:29:50,530
approaches to take a marginal 
improvement as seen.

384
00:29:50,531 --> 00:29:55,531
Net got a significant improvement,
at least in percentages,

385
00:29:55,751 --> 00:29:58,810
I think as a 25 percent reduction in 
error

386
00:30:00,640 --> 00:30:05,320
from four percent to three percent.
Something like that.

387
00:30:06,550 --> 00:30:11,550
By using a very simple idea that I think
is important to mention as simple 

388
00:30:11,550 --> 00:30:16,261
insight.
It added a parameter to each channel and

389
00:30:16,331 --> 00:30:20,560
the convolutional layer and the 
convolutional block,

390
00:30:21,400 --> 00:30:26,400
so the network can now adjust the 
weighting and each channel based for for

391
00:30:28,180 --> 00:30:30,010
each feature map,
based on the content,

392
00:30:30,011 --> 00:30:35,011
based on the input to the network.
This is kind of a takeaway to think 

393
00:30:35,011 --> 00:30:39,721
about about any of the networks who talk
about any of the architectures is a lot 

394
00:30:40,721 --> 00:30:44,980
of times you were recurrent neural 
networks and convolution neural networks

395
00:30:45,250 --> 00:30:49,390
have tricks that significantly reduced 
the number of parameters,

396
00:30:50,320 --> 00:30:52,690
the bulk,
the sort of low hanging fruit.

397
00:30:52,930 --> 00:30:57,930
They use spatial invariants of temporal 
and to reduce the number of parameters 

398
00:30:57,930 --> 00:31:02,311
to represent the input data,
but they also lead certain things not 

399
00:31:02,771 --> 00:31:07,771
parametrized.
They don't allow the network to learn 

400
00:31:07,771 --> 00:31:10,471
it.
Allowing this case to network to learn 

401
00:31:10,471 --> 00:31:10,720
the waiting on each of the individual 
channels.

402
00:31:10,810 --> 00:31:15,810
So each of the individual filters is 
something that you learn as a long with 

403
00:31:15,810 --> 00:31:18,310
the filters takes.
It makes a huge boost.

404
00:31:18,490 --> 00:31:22,000
The cool thing about this is it's 
applicable to any architecture.

405
00:31:22,300 --> 00:31:27,300
This kind of block,
this kind of what the squeeze and 

406
00:31:27,300 --> 00:31:29,140
excitation block is applicable to any 
architecture.

407
00:31:30,830 --> 00:31:32,450
And,
uh,

408
00:31:32,510 --> 00:31:37,510
because obviously it's a,
it's just simply a parametrized is the 

409
00:31:37,510 --> 00:31:39,830
ability to choose which filter you go 
with based on the content.

410
00:31:40,220 --> 00:31:43,370
It's a subtle but crucial thing I think 
is pretty cool.

411
00:31:43,490 --> 00:31:47,210
And for future research it inspires to 
think about,

412
00:31:47,650 --> 00:31:49,520
uh,
what else can be parametrized and neural

413
00:31:49,521 --> 00:31:54,521
networks,
what else can be controlled as part of 

414
00:31:54,521 --> 00:31:57,251
the learning process,
including hiring higher order hyper 

415
00:31:57,251 --> 00:31:57,530
parameters,
which,

416
00:31:57,620 --> 00:32:02,620
which aspects of the training and the 
architecture of the network can be part 

417
00:32:02,620 --> 00:32:05,750
of the learning.
This is what this network inspires.

418
00:32:11,830 --> 00:32:16,830
Another network has been in development 
since the nineties ideas but Jeff 

419
00:32:19,000 --> 00:32:20,200
Hinton,
but really received,

420
00:32:20,260 --> 00:32:22,270
has been published on received 
significant attention,

421
00:32:22,271 --> 00:32:27,271
2017 that I won't go into detail here.
Uh,

422
00:32:27,810 --> 00:32:32,810
we are going to release an online only 
video about capsule networks.

423
00:32:35,230 --> 00:32:40,230
It's a little bit too technical,
but they inspire very important point 

424
00:32:41,210 --> 00:32:43,520
that we should always think about deep 
learning.

425
00:32:44,060 --> 00:32:49,060
Uh,
whenever it's successful is to think 

426
00:32:49,060 --> 00:32:51,371
about what,
as I mentioned with the cat eating a 

427
00:32:51,371 --> 00:32:53,970
banana on a philosophical and the 
mathematical level,

428
00:32:53,971 --> 00:32:58,971
we have to consider what assumptions 
these networks make and what through 

429
00:33:00,171 --> 00:33:05,171
those assumptions they throw away.
So neural networks due to the spacial 

430
00:33:05,171 --> 00:33:09,821
with convolutional neural networks due 
to their spatial invariance throwaway 

431
00:33:09,821 --> 00:33:14,560
information about the relationship 
between the hierarchies between the 

432
00:33:16,281 --> 00:33:21,281
simple and the complex objects.
So the face on the left and the face on 

433
00:33:21,281 --> 00:33:23,270
the right looks the same to accomplish 
in neural network.

434
00:33:23,690 --> 00:33:28,690
The presence of eyes and nose and mouth 
is the essential aspect of what makes 

435
00:33:31,941 --> 00:33:34,660
the classification task work for 
accomplished or network.

436
00:33:34,700 --> 00:33:38,360
It will what worry will fire and say 
this is definitely a face,

437
00:33:39,230 --> 00:33:43,400
but the spatial relationship is lost,
is ignored.

438
00:33:43,550 --> 00:33:47,000
Which means there's a lot of 
implications to this.

439
00:33:47,090 --> 00:33:52,090
But for things like pose variation,
that information is lost.

440
00:33:54,050 --> 00:33:59,050
We're throwing away that away completely
and hoping that the pooling operation 

441
00:33:59,720 --> 00:34:04,720
that's performing these networks is able
to sort of Mesh everything together to 

442
00:34:04,820 --> 00:34:09,820
come up with the features that are 
firing of the different parts of the 

443
00:34:09,820 --> 00:34:13,751
face.
That then come up with a total 

444
00:34:13,751 --> 00:34:15,221
classification that is a face without 
representing really the relationship 

445
00:34:15,221 --> 00:34:19,271
between these features at the low level 
and the high level at the low level of 

446
00:34:19,611 --> 00:34:22,700
the hierarchy,
the simple and the complex level.

447
00:34:23,840 --> 00:34:28,610
This is super exciting field now that's 
hopefully will spark developments of how

448
00:34:28,611 --> 00:34:31,870
we design your networks that are able to
learn this,

449
00:34:32,240 --> 00:34:37,240
the rotational,
the orientation and variance a as well.

450
00:34:40,520 --> 00:34:42,500
Okay,
so as I mentioned,

451
00:34:43,470 --> 00:34:46,010
you take these convolutional neural 
networks,

452
00:34:46,011 --> 00:34:50,600
chop off the final layer in order to 
apply to a particular domain,

453
00:34:51,330 --> 00:34:54,380
and that is what we'll do with fully 
convolutional neural networks.

454
00:34:54,500 --> 00:34:58,580
The ones that we tasked to segment the 
image at a pixel level.

455
00:35:00,020 --> 00:35:04,400
As a reminder,
these networks through the convolutional

456
00:35:04,401 --> 00:35:09,401
process are really producing a heat map,
different parts of the network and 

457
00:35:12,451 --> 00:35:17,451
getting excited based on the different 
aspects of the image and so it can be 

458
00:35:17,451 --> 00:35:18,720
used to do the localization of 
detecting,

459
00:35:18,721 --> 00:35:21,980
not just classifying the image but 
localized in the object.

460
00:35:22,820 --> 00:35:27,820
And it could do so at a pixel level.
So the convolutional layers are doing 

461
00:35:29,661 --> 00:35:34,661
the encoding process.
They're taking the rich raw sensory 

462
00:35:34,661 --> 00:35:38,040
information in the image and encoding 
them into

463
00:35:38,040 --> 00:35:43,040
an interpretable set of features,
a representation that can then be used 

464
00:35:43,040 --> 00:35:47,601
for classification,
but we can also then use a decoder up 

465
00:35:47,601 --> 00:35:51,451
sample that information and produce a 
map like this fully convolution neural 

466
00:35:52,351 --> 00:35:55,600
network segmentation,
semantic segmentation,

467
00:35:55,610 --> 00:35:58,170
image segmentation.
The goal is to,

468
00:35:58,260 --> 00:36:02,430
as opposed to classify the entire image,
it can classify every single pixel,

469
00:36:02,600 --> 00:36:07,600
this pixel level segmentation you call 
her every single pixel with what that 

470
00:36:07,600 --> 00:36:12,591
Pixel,
what object that pixel belongs to in 

471
00:36:12,591 --> 00:36:13,950
this two d space of the image,
the two d projection,

472
00:36:14,700 --> 00:36:15,700
the,
uh,

473
00:36:15,990 --> 00:36:18,210
in the image of a three dimensional 
world.

474
00:36:19,110 --> 00:36:22,110
So the thing is,
there's been a lot of advancement in the

475
00:36:22,111 --> 00:36:27,111
last three years,
but it's still an incredibly difficult 

476
00:36:29,061 --> 00:36:32,360
problem if you think,
if you think about,

477
00:36:34,000 --> 00:36:39,000
uh,
the amount of data that's used for 

478
00:36:39,000 --> 00:36:40,400
training and the task of Pixel level,
uh,

479
00:36:40,401 --> 00:36:45,401
of megapixels here of millions of pixels
that are tasked with having a,

480
00:36:45,880 --> 00:36:48,470
a single label,
it's an extremely difficult problem.

481
00:36:50,990 --> 00:36:55,990
Why is this interesting,
important problems to try to solve as 

482
00:36:55,990 --> 00:36:59,090
opposed to Bonnie boxes around cats?
Well,

483
00:36:59,150 --> 00:37:01,970
it's whenever precise boundaries of 
objects are important.

484
00:37:02,270 --> 00:37:07,270
Certainly medical applications when 
looking at imaging and detecting 

485
00:37:07,270 --> 00:37:08,300
particular,
for example,

486
00:37:08,301 --> 00:37:10,950
detecting tumors and,
uh,

487
00:37:11,310 --> 00:37:13,650
in,
in medical imaging of,

488
00:37:13,700 --> 00:37:14,620
of different,
uh,

489
00:37:14,700 --> 00:37:15,890
uh,
different organs

490
00:37:17,490 --> 00:37:22,490
and in driving and robotics,
when objects are involved as a dense 

491
00:37:23,391 --> 00:37:25,220
scene of all the vehicles,
pedestrians,

492
00:37:25,221 --> 00:37:30,221
cyclists,
we need to be able to not just have a 

493
00:37:30,221 --> 00:37:34,031
loose estimate of where objects are.
We need to be able to have the exact 

494
00:37:34,031 --> 00:37:37,580
boundaries and then potentially through 
data fusion,

495
00:37:37,610 --> 00:37:42,610
fusing sensors together,
fusing this rich textual information 

496
00:37:42,610 --> 00:37:43,650
about pedestrians,
cyclists,

497
00:37:43,651 --> 00:37:48,651
and vehicles to light our data that's 
providing us the three dimensional map 

498
00:37:48,651 --> 00:37:52,811
of the world or have both these semantic
meaning of the different objects and 

499
00:37:52,811 --> 00:37:56,861
their exact three mentioned location.
Um,

500
00:38:00,040 --> 00:38:05,040
a lot of this work successfully,
a lot of the work and the semantic 

501
00:38:05,040 --> 00:38:09,691
segmentation started with fully 
convolutional networks for semantic 

502
00:38:09,691 --> 00:38:10,420
segmentation,
paper,

503
00:38:10,720 --> 00:38:15,720
fcn.
That's where the name fcn came from in 

504
00:38:15,720 --> 00:38:17,620
November 2014.
Now go through a few papers here to give

505
00:38:17,621 --> 00:38:22,621
you some intuition where the field has 
gone and how that takes us to segue 

506
00:38:22,781 --> 00:38:25,090
views,
the segmentation competition.

507
00:38:26,840 --> 00:38:29,060
So fcn,
repurpose the image,

508
00:38:29,061 --> 00:38:33,140
net pretreating nets the nets that were 
trained to classify what's in an image,

509
00:38:33,980 --> 00:38:38,980
entire image,
and chopped off the fully connected 

510
00:38:38,980 --> 00:38:43,201
layers.
And then have added decoder parts that 

511
00:38:43,201 --> 00:38:47,630
up,
sample the image to produce a heat map 

512
00:38:48,820 --> 00:38:49,800
here,
showing a,

513
00:38:49,801 --> 00:38:51,370
uh,
with a tabby cat,

514
00:38:51,460 --> 00:38:53,530
a heat map of where the cat is in the 
image.

515
00:38:54,880 --> 00:38:59,880
It's a much slower,
much coarser resolution than the input 

516
00:38:59,880 --> 00:38:59,880
image

517
00:39:00,170 --> 00:39:05,170
one eighth at best.
Skip connections to improve courses of 

518
00:39:05,541 --> 00:39:09,470
upsampling.
There's a few tricks.

519
00:39:10,350 --> 00:39:15,350
If you do the most naive approach,
the upsampling is going to be extremely 

520
00:39:15,350 --> 00:39:17,510
course because that's the whole point of
the neural network.

521
00:39:17,810 --> 00:39:21,890
The encoding part is you throw away all 
the useless data,

522
00:39:22,410 --> 00:39:23,180
uh,
the,

523
00:39:23,210 --> 00:39:26,210
you to the most essential aspects that 
represent that image.

524
00:39:26,690 --> 00:39:31,690
So you're throwing away a lot of 
information that's necessary to then 

525
00:39:31,690 --> 00:39:35,260
form a high resolution image.
So there's a few tricks where you skip a

526
00:39:36,021 --> 00:39:41,021
few of the final pooling operations to 
go in a similar way.

527
00:39:41,871 --> 00:39:46,871
And this is the residual block to a go 
to go to the output produced higher and 

528
00:39:46,871 --> 00:39:51,341
higher resolution heat map at the end 
segment in 2015.

529
00:39:52,870 --> 00:39:57,870
I applied this to the driving context 
and really taking it to Kitty Dataset 

530
00:39:58,791 --> 00:40:03,791
and have have have shown a lot of 
interesting results and really explored 

531
00:40:03,801 --> 00:40:06,890
the encoder decoder or formulation of 
the problem.

532
00:40:09,590 --> 00:40:14,590
Really solidifying this the place of the
encoder decoder framework for the 

533
00:40:14,590 --> 00:40:19,181
segmentation task dilated convolution.
I'm taking you through a few components 

534
00:40:20,691 --> 00:40:25,400
which are critical here to the state of 
the art dilated convolutions,

535
00:40:26,120 --> 00:40:31,120
so the convolution operation as the 
pooling operation reduces resolution 

536
00:40:33,870 --> 00:40:38,870
significantly and dilated convolution 
has a certain kind of gritting as 

537
00:40:40,551 --> 00:40:45,551
visualized there that maintains the the 
local high resolution textures while 

538
00:40:50,481 --> 00:40:55,481
still capturing the spatial window 
necessary.

539
00:40:56,790 --> 00:41:01,790
It's called dilated convolutional layer 
and that's in a 2015 paper,

540
00:41:03,680 --> 00:41:08,510
proved to be much better at upsampling.
A high resolution image.

541
00:41:11,540 --> 00:41:16,540
The lab with a B v One v Two v three 
added conditional random fields,

542
00:41:21,290 --> 00:41:26,030
which is the final piece of the of the 
state of the art puzzle here.

543
00:41:26,330 --> 00:41:30,350
A lot of the successful networks today 
that do segmentation,

544
00:41:30,560 --> 00:41:35,560
not all do post process using a crs 
conditional random fields,

545
00:41:38,090 --> 00:41:40,910
and what they do is they smoothed the 
segmentation,

546
00:41:41,300 --> 00:41:46,300
the ups sampled segmentation that 
results from the fcn by looking at the 

547
00:41:46,300 --> 00:41:47,150
underlying image intensities.

548
00:41:50,890 --> 00:41:53,730
So that's the key aspects of the 
successful approaches.

549
00:41:53,740 --> 00:41:58,740
Today.
You have the encoder decoder framework 

550
00:41:58,740 --> 00:42:01,591
of a fully accomplished in your network.
It replaces the fully connected layers 

551
00:42:01,591 --> 00:42:04,540
with a convolutional layers,
dee dee convolutional layers,

552
00:42:04,960 --> 00:42:09,960
and as the years progressed from 2014 to
today as usual,

553
00:42:11,470 --> 00:42:16,470
the underlying networks from Alex Net to
Vgg net and to now Raza net have been 

554
00:42:19,900 --> 00:42:24,900
one of the big reasons for the 
improvements of these networks to be 

555
00:42:24,900 --> 00:42:27,340
able to perform the segmentation.
So naturally they mirrored the image,

556
00:42:27,341 --> 00:42:30,910
net challenge performance and adapting 
these networks.

557
00:42:30,970 --> 00:42:35,970
So the state of the art uses resonate or
similar networks conditional random 

558
00:42:35,970 --> 00:42:40,290
fields for smoothing based on the input 
image intensities and the dilated 

559
00:42:42,760 --> 00:42:46,450
convolution that maintains the 
computational cost,

560
00:42:46,870 --> 00:42:51,870
but increases the resolution of the 
upsampling throughout the intermediate 

561
00:42:51,870 --> 00:42:56,640
feature maps.
And that takes us to the state of the 

562
00:42:57,181 --> 00:43:02,181
art that we used to produce the images 
to produce the images for the 

563
00:43:04,261 --> 00:43:09,261
competition.
Brezhnev do you see for dance upsampling

564
00:43:09,421 --> 00:43:13,530
convolution instead of by linear 
upsampling,

565
00:43:13,560 --> 00:43:16,170
you make the upsampling learnable.

566
00:43:18,210 --> 00:43:21,960
You learned the upscaling filters that's
on the bottom.

567
00:43:22,470 --> 00:43:24,210
That's really the key part that made it 
work.

568
00:43:26,270 --> 00:43:30,120
There should be a theme here.
Sometimes the biggest addition,

569
00:43:30,121 --> 00:43:33,420
they can be done.
This parametrizing one of the aspects of

570
00:43:33,421 --> 00:43:35,040
the network that you've taken for 
granted.

571
00:43:35,460 --> 00:43:39,480
Letting the network learn that aspect 
and the other.

572
00:43:40,230 --> 00:43:42,780
I'm not sure how important it is to the 
success,

573
00:43:42,781 --> 00:43:45,090
but it's a.
it's a cool little addition,

574
00:43:45,091 --> 00:43:50,091
is a hybrid dilated convolution.
As I showed that visualization where the

575
00:43:51,170 --> 00:43:56,170
convolution is spread apart a little bit
at in the input from the input to the 

576
00:43:56,170 --> 00:44:01,071
output,
the steps of that dilated convolution 

577
00:44:01,071 --> 00:44:01,760
filter,
when they are changed,

578
00:44:01,770 --> 00:44:04,620
it produces a smoother results because 
when it's kept the same,

579
00:44:06,740 --> 00:44:11,740
are there certain input?
Pixels get a lot more attention than 

580
00:44:11,740 --> 00:44:14,740
others,
so losing that favoritism as well.

581
00:44:14,741 --> 00:44:18,500
It's achieved by using a variable 
difference dilation rate.

582
00:44:20,310 --> 00:44:25,310
Those are the two tricks,
but really the biggest one is the 

583
00:44:25,310 --> 00:44:26,010
parametric,
one of the upscaling filters.

584
00:44:27,600 --> 00:44:29,010
Okay,
so that's what we're.

585
00:44:29,160 --> 00:44:34,160
That's what we use to generate that data
and that's we provide you the code with 

586
00:44:34,160 --> 00:44:35,130
if you're interested in competing in 
psych fuse.

587
00:44:36,030 --> 00:44:39,000
The other aspect here that everything 
we've talked about,

588
00:44:39,001 --> 00:44:44,001
ball from the classification to the 
segmentation to making sense of images 

589
00:44:45,060 --> 00:44:50,060
is there the information about a time 
the temporal dynamics of the scene is 

590
00:44:50,971 --> 00:44:55,971
thrown away and for the driving context 
of the robotics contests and what we'd 

591
00:44:57,841 --> 00:45:02,841
like to do is take fuse for the 
segmentation dynamic scene segmentation 

592
00:45:02,841 --> 00:45:06,921
context of when you try to interpret 
what's going on in and seen over time 

593
00:45:06,921 --> 00:45:10,170
and use that information.
Time is essential.

594
00:45:11,310 --> 00:45:15,090
The the movement of pixels is essential 
through time.

595
00:45:15,750 --> 00:45:20,750
That that understanding how those 
objects move in a three d space through 

596
00:45:23,521 --> 00:45:28,521
the two d projection of an image is 
fascinating and there's a lot of set of 

597
00:45:29,041 --> 00:45:34,041
open problems there.
So flow is what's very helpful too as a 

598
00:45:36,651 --> 00:45:41,250
starting point to help us understand how
these pixels move flow,

599
00:45:41,970 --> 00:45:46,970
optical flow,
dance optical computation that are best 

600
00:45:46,970 --> 00:45:51,680
at our best approximation of where each 
pixel in image one

601
00:45:55,380 --> 00:46:00,120
and moved in temporarily following 
image.

602
00:46:00,121 --> 00:46:03,210
After that,
there's two images in 30 frames.

603
00:46:03,211 --> 00:46:08,211
A second is one image at time zero,
the other is 33 point three milliseconds

604
00:46:08,371 --> 00:46:13,371
later,
and the dense optical flow is our best 

605
00:46:13,371 --> 00:46:15,600
estimate of how each pixel in the input 
image moved to end.

606
00:46:15,601 --> 00:46:20,601
The output image and the optical flow 
for every pixel produces a direction of 

607
00:46:21,571 --> 00:46:26,571
where we think that pixel moved and the 
magnitude of how far moved that allows 

608
00:46:26,571 --> 00:46:31,161
us to take information that we detected 
about the first frame and try to 

609
00:46:31,980 --> 00:46:36,980
propagate it forward.
This is the competition is to try to 

610
00:46:36,980 --> 00:46:41,570
segment and image and propagate that 
information forward for manual 

611
00:46:43,071 --> 00:46:48,071
annotation of a of an image.
So this kind of coloring book 

612
00:46:48,880 --> 00:46:53,880
annotation,
will you call every single pixel in the 

613
00:46:53,880 --> 00:46:56,641
state of the art data set for driving 
city scapes that it takes one point 

614
00:46:59,451 --> 00:47:01,310
five,
one point five hours,

615
00:47:01,311 --> 00:47:05,450
90 minutes to do that coloring,
that's 90 minutes per image.

616
00:47:06,380 --> 00:47:10,490
That's extremely long time.
That's why it doesn't exist today.

617
00:47:10,500 --> 00:47:15,500
Dataset.
And in this class we're going to create 

618
00:47:15,500 --> 00:47:15,500
one

619
00:47:15,610 --> 00:47:19,250
of segmentation of these images is 
through time,

620
00:47:20,270 --> 00:47:25,270
through video,
so long videos where every single frame 

621
00:47:25,340 --> 00:47:30,340
is fully segmented.
That's still an open problem that we 

622
00:47:30,340 --> 00:47:33,130
need to solve flows,
a piece of that.

623
00:47:33,580 --> 00:47:38,580
And we also provide you the this 
computer state of the art flow using 

624
00:47:39,461 --> 00:47:40,850
flow net two point.
Oh,

625
00:47:41,710 --> 00:47:43,720
so flow net one point,
oh,

626
00:47:43,750 --> 00:47:48,750
in May 2015,
use neural networks to learn the optical

627
00:47:50,471 --> 00:47:52,120
flow,
the dense optical flow.

628
00:47:53,410 --> 00:47:56,260
And it did so with two kinds of 
architectures.

629
00:47:56,740 --> 00:47:59,590
Flow net s flown out,
simple and flown on.

630
00:47:59,620 --> 00:48:04,620
Core flow in that.
See the simple one is simply taking the 

631
00:48:04,620 --> 00:48:05,200
two images.
So what's,

632
00:48:05,230 --> 00:48:10,230
what's the task here?
There's two images and you want to 

633
00:48:10,230 --> 00:48:11,500
produce in those two images,
they follow each other in time,

634
00:48:11,770 --> 00:48:15,010
33 point three milliseconds apart,
and uh,

635
00:48:15,011 --> 00:48:18,370
your task is the Zl output to produce 
the dense optical flow.

636
00:48:18,880 --> 00:48:22,300
So for the simple architecture,
you just stack them together,

637
00:48:22,390 --> 00:48:27,390
each our rgb,
so it produces a six channel input to 

638
00:48:27,390 --> 00:48:28,780
the network.
There's a lot of convolution and finally

639
00:48:28,781 --> 00:48:33,781
it's the same kind of process as the 
fully convolutional neural networks to 

640
00:48:33,781 --> 00:48:38,461
produce the optical flow.
Then there is flown at correlation 

641
00:48:38,461 --> 00:48:43,380
architecture where you performed some 
convolutions separately before using a 

642
00:48:43,380 --> 00:48:45,970
correlation later to combine the feature
maps.

643
00:48:48,620 --> 00:48:53,620
Both are effective and different 
datasets and different applications.

644
00:48:55,010 --> 00:49:00,010
So flow net two point zero in December 
2016 is one of the state of the art 

645
00:49:02,211 --> 00:49:06,500
frameworks code bases that we use to 
generate the data.

646
00:49:06,501 --> 00:49:11,501
I'll show combines the flown at sm,
flown as C and improves over the initial

647
00:49:12,351 --> 00:49:17,351
flow net.
Producing a smoother flow field 

648
00:49:17,351 --> 00:49:20,680
preserves the fine motion detail along 
the edges of the objects and it runs 

649
00:49:22,040 --> 00:49:26,540
extremely efficiently depending on the 
architecture as a few variants,

650
00:49:26,810 --> 00:49:31,810
either eight to 140 frames a second,
and the process there is essentially one

651
00:49:34,941 --> 00:49:36,740
that's common across various 
applications.

652
00:49:36,770 --> 00:49:39,560
Deep learning is stacking these networks
together.

653
00:49:40,700 --> 00:49:45,700
The very interesting aspect here that 
we're still exploring,

654
00:49:47,271 --> 00:49:51,140
and again applicable in all of deep 
learning in this case,

655
00:49:51,170 --> 00:49:56,000
it seemed that there was a strong effect
in taking sparse small,

656
00:49:56,030 --> 00:50:01,030
multiple Dataset and doing the training,
the order of which those datasets were 

657
00:50:01,030 --> 00:50:03,140
used for the training process mattered a
lot.

658
00:50:04,060 --> 00:50:09,060
That's very interesting.
So using flow net two point zero,

659
00:50:10,420 --> 00:50:13,470
here's the Dataset we're making 
available for psych fuse.

660
00:50:13,480 --> 00:50:18,480
The competition cars that mit did,
you use less sex views first.

661
00:50:18,961 --> 00:50:23,961
The original video,
US driving in high definition 10 ADP and

662
00:50:26,641 --> 00:50:29,390
a eightK ,
three 60 video

663
00:50:31,900 --> 00:50:35,580
original video driving around a 
cambridge.

664
00:50:37,300 --> 00:50:42,300
Then we're providing the ground truth 
for a training set for that training set

665
00:50:45,521 --> 00:50:48,010
for every single frame.
30 frames a second,

666
00:50:48,011 --> 00:50:53,011
we're providing the segmentation frame 
to frame to frame segmented on 

667
00:50:53,591 --> 00:50:58,591
mechanical Turk.
We're also providing the output of the 

668
00:51:00,490 --> 00:51:02,050
network that I mentioned,
the dad,

669
00:51:02,051 --> 00:51:06,490
there are segmentation network that's 
pretty damn close to the ground truth,

670
00:51:07,570 --> 00:51:11,690
but still not.
And our task is.

671
00:51:11,840 --> 00:51:16,840
This is the interesting thing is our 
task is to take the output of this 

672
00:51:17,571 --> 00:51:20,000
network.
Well there's two options.

673
00:51:20,150 --> 00:51:25,150
One is to take the output of this 
network and use CA use other networks to

674
00:51:27,441 --> 00:51:29,780
help you propagate the information 
better.

675
00:51:30,200 --> 00:51:35,200
So what this segmentation,
the output of this network does is it 

676
00:51:35,901 --> 00:51:40,901
only takes a frame by frame by frame.
It's not using the temporal information 

677
00:51:40,901 --> 00:51:45,341
at all.
So the question is can we figure out a 

678
00:51:45,341 --> 00:51:46,190
way,
can we figure out tricks to use temporal

679
00:51:46,191 --> 00:51:49,280
information to improve this 
segmentation?

680
00:51:49,460 --> 00:51:54,460
So it looks more like this segmentation 
and we're also providing the optical 

681
00:51:57,031 --> 00:52:02,031
flow from frame to frame to frame.
So the optical flow based on flown at 

682
00:52:02,031 --> 00:52:04,380
two point zero of how each of the pixels
moved.

683
00:52:07,390 --> 00:52:12,390
Okay.
And now forms the psych fuse 

684
00:52:12,390 --> 00:52:12,940
competition.
10,000

685
00:52:12,941 --> 00:52:17,530
images.
And the task is to submit code.

686
00:52:17,531 --> 00:52:22,531
We have starter code in python and on 
good hub to take in the original video,

687
00:52:25,580 --> 00:52:28,060
take in for the training,
set the ground truth,

688
00:52:28,450 --> 00:52:31,900
the segmentation from the state of the 
art segmentation network,

689
00:52:31,960 --> 00:52:36,370
the optical flow from the state of the 
art optical flow network,

690
00:52:36,670 --> 00:52:40,690
and taking that together to improve the.
The stuff on the bottom left,

691
00:52:40,720 --> 00:52:44,800
the segmentation to try to achieve the 
ground truth on the on the top right.

692
00:52:47,060 --> 00:52:48,980
Okay.
With that,

693
00:52:49,040 --> 00:52:54,040
I'd like to thank you.
Tomorrow at one PM is Waymo in data 32,

694
00:52:56,001 --> 00:52:56,420
one,
two,

695
00:52:56,421 --> 00:52:58,310
three.
The next lecture,

696
00:52:58,610 --> 00:53:03,610
next week we'll be on deep learning for 
a sensing the human understanding the 

697
00:53:03,610 --> 00:53:07,331
human,
and we will release online only lecture 

698
00:53:07,331 --> 00:53:10,040
on capsule networks and Gans generative 
adversarial networks.

699
00:53:10,250 --> 00:53:11,030
Thank you very much.

