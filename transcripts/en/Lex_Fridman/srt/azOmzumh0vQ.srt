1
00:00:00,120 --> 00:00:03,720
What difference between biological and 
you own that works and artificial neural

2
00:00:03,721 --> 00:00:06,900
networks is most mysterious,
captivating,

3
00:00:06,901 --> 00:00:07,770
and profound for you.

4
00:00:11,240 --> 00:00:16,240
First of all,
there's so much we don't know about 

5
00:00:16,240 --> 00:00:19,091
biological neural networks and that's 
very mysterious and captivating because 

6
00:00:19,091 --> 00:00:23,270
maybe it holds the key to improving 
artificial neural networks.

7
00:00:24,260 --> 00:00:29,260
One of the things I studied recently as 
something that we don't know how 

8
00:00:31,310 --> 00:00:36,310
biological neural networks do but would 
be really useful for artificial ones is 

9
00:00:38,330 --> 00:00:43,330
the ability to do credit assignment 
through very long time spans.

10
00:00:44,150 --> 00:00:49,150
There are things that we can in 
principle do with artificial neural 

11
00:00:49,150 --> 00:00:53,951
nets,
but it's not very convenient and it's 

12
00:00:53,951 --> 00:00:55,460
not biologically plausible.
And this mismatch and think this kind of

13
00:00:55,461 --> 00:01:00,461
mismatch.
Maybe an interesting thing to study to a

14
00:01:01,791 --> 00:01:06,791
understand better how brains my duties 
things because we don't have good 

15
00:01:06,791 --> 00:01:09,200
corresponding theories with artificial 
neural nets and B,

16
00:01:10,280 --> 00:01:15,280
maybe provide new ideas that we could 
explore about,

17
00:01:15,370 --> 00:01:20,370
um,
things like brain do differently and 

18
00:01:20,370 --> 00:01:22,130
that we could incorporate in artificial 
neural nets.

19
00:01:22,370 --> 00:01:23,210
So let's break

20
00:01:23,320 --> 00:01:27,640
credit assignment up a little bit.
It's a beautifully technical term,

21
00:01:27,790 --> 00:01:32,790
but it can incorporate so many things.
So is it more on the rnn memory side 

22
00:01:35,290 --> 00:01:40,290
that thinking like that or is it 
something about knowledge building up 

23
00:01:40,290 --> 00:01:43,921
common sense knowledge over time,
or is it more in the reinforcement 

24
00:01:44,231 --> 00:01:49,231
learning sense that you're picking up 
rewards over time for a particular 

25
00:01:49,231 --> 00:01:50,210
charge you a certain kinds of goals?

26
00:01:50,470 --> 00:01:55,470
I was thinking more about the first two 
meetings whereby we store all kinds of 

27
00:01:57,521 --> 00:02:01,270
memories,
episodic memories in our brain,

28
00:02:02,200 --> 00:02:07,200
which we can access later in order to 
help us both infor causes of things that

29
00:02:11,531 --> 00:02:16,531
we are observing now and assign credit 
to decisions or interpretations we came 

30
00:02:20,081 --> 00:02:25,081
up with a while ago when you know those 
memories were stored and then we can 

31
00:02:25,211 --> 00:02:30,211
change the way we would have reacted or 
interpreted things in the past.

32
00:02:31,870 --> 00:02:34,930
And now that's credit assignment used 
for learning.

33
00:02:36,220 --> 00:02:38,800
So in which way do you think

34
00:02:39,440 --> 00:02:44,150
artificial neural networks,
the current Lstm,

35
00:02:44,630 --> 00:02:47,690
the current architectures are not able 
to capture

36
00:02:48,910 --> 00:02:50,740
the.
Presumably you're,

37
00:02:50,750 --> 00:02:53,440
you're thinking of very longterm.
Yes.

38
00:02:53,740 --> 00:02:58,740
So current recurrent nets are doing a 
fairly good jobs for sequences,

39
00:02:59,890 --> 00:03:04,890
dozens or hundreds of time steps and 
then it gets harder and harder and 

40
00:03:06,161 --> 00:03:11,161
depending on what you have to remember 
and so on as you consider longer 

41
00:03:11,161 --> 00:03:14,341
durations.
Whereas humans seem to be able to do 

42
00:03:15,161 --> 00:03:17,580
credit assignment to essentially 
arbitrary times.

43
00:03:17,581 --> 00:03:22,581
Like I,
I could remember something I did last 

44
00:03:22,581 --> 00:03:23,170
year and now because I see some new 
evidence,

45
00:03:23,200 --> 00:03:28,200
I'm gonna change my mind about the way I
was thinking last year and hopefully not

46
00:03:29,621 --> 00:03:30,910
do the same mistake again.

47
00:03:33,060 --> 00:03:38,060
I think a big part of that,
it's probably forgetting your only 

48
00:03:38,060 --> 00:03:40,710
remembering the really important things 
that's very efficient for getting.

49
00:03:42,190 --> 00:03:47,190
Yes.
So there's a selection of what we 

50
00:03:47,190 --> 00:03:49,291
remember and I think they're a really 
cool connection to higher level 

51
00:03:49,291 --> 00:03:53,911
cognition here regarding consciousness 
deciding and any emotions that sort of 

52
00:03:54,850 --> 00:03:58,180
deciding what comes to consciousness and
what gets stored in memory,

53
00:03:58,930 --> 00:04:01,300
which,
which are not trivial either.

54
00:04:02,890 --> 00:04:06,160
So you've been at the forefront there 
all along.

55
00:04:07,000 --> 00:04:09,820
Showing some of the amazing things that 
neural networks,

56
00:04:09,910 --> 00:04:14,910
deep neural networks can do in the field
of artificial intelligence is just 

57
00:04:14,910 --> 00:04:17,430
broadly in all kinds of applications.
But uh,

58
00:04:17,780 --> 00:04:21,370
we can talk about that forever.
But what in your view,

59
00:04:21,970 --> 00:04:23,560
because we're thinking towards the 
future,

60
00:04:24,010 --> 00:04:27,970
is the weakest aspect of the way deep 
neural networks represent the world.

61
00:04:28,030 --> 00:04:31,210
What is the,
what is in your view is missing?

62
00:04:32,200 --> 00:04:33,700
So currently current

63
00:04:33,920 --> 00:04:38,920
state of the art neural nets trained on 
large quantities of images or texts have

64
00:04:44,131 --> 00:04:48,840
some level of understanding of what 
explains those datasets,

65
00:04:48,990 --> 00:04:52,170
but it's very basic.
It's,

66
00:04:52,210 --> 00:04:57,210
it's very low level and it's not nearly 
as robust and abstract and general as 

67
00:05:00,500 --> 00:05:03,350
our understanding.
Okay.

68
00:05:03,351 --> 00:05:05,360
So that doesn't tell us how to fix 
things,

69
00:05:05,780 --> 00:05:10,780
but I think it encourages us to think 
about how we can maybe train our neural 

70
00:05:14,781 --> 00:05:19,781
nets differently so that they would 
focus,

71
00:05:20,391 --> 00:05:25,391
for example,
on causal explanation is something that 

72
00:05:25,391 --> 00:05:27,050
we don't do currently with neural net 
training.

73
00:05:27,830 --> 00:05:28,850
Also,

74
00:05:30,000 --> 00:05:35,000
one thing I'll talk about in my talk 
this afternoon is instead of learning 

75
00:05:35,851 --> 00:05:40,140
separately from images and videos on one
hand and from texts,

76
00:05:40,141 --> 00:05:45,141
on the other hand,
we need to do a better job of jointly 

77
00:05:45,841 --> 00:05:50,841
learning about language and about the 
world to which it refers so that both 

78
00:05:53,821 --> 00:05:58,340
sides can help each other.
We need to have good world models in,

79
00:05:58,350 --> 00:06:03,350
in our neural nets for them to really 
understand sentences which talk about 

80
00:06:04,311 --> 00:06:09,311
what's going on in the world and I think
we need language input to help provide 

81
00:06:12,081 --> 00:06:17,081
clues about what the high level concepts
like semantic concepts should be 

82
00:06:17,781 --> 00:06:21,170
represented at the top levels of these 
neural nets.

83
00:06:21,770 --> 00:06:26,770
In fact,
there is evidence that the purely 

84
00:06:26,990 --> 00:06:31,990
unsupervised learning of representations
doesn't give rise to high level 

85
00:06:32,690 --> 00:06:37,690
representations that are as powerful as 
the ones we're getting from supervised 

86
00:06:37,690 --> 00:06:39,010
learning.
And so the,

87
00:06:39,011 --> 00:06:44,011
the,
the clues were getting just with the 

88
00:06:44,011 --> 00:06:45,851
labels.
Not even sentences is already very 

89
00:06:45,851 --> 00:06:45,851
powerful.

90
00:06:45,851 --> 00:06:49,420
Do you think that's an architecture 
challenge or is it a Dataset challenge?

91
00:06:49,630 --> 00:06:54,470
Neither.
A.

92
00:06:54,670 --> 00:06:58,900
I'm tempted to just end it there.
Can you elaborate?

93
00:07:01,690 --> 00:07:03,460
Uh,
of course,

94
00:07:03,461 --> 00:07:06,370
data sets on architectures are something
you want to always play with,

95
00:07:06,371 --> 00:07:11,371
but,
but I think the crucial thing is more 

96
00:07:11,371 --> 00:07:11,371
the training objectives,
the training frameworks,

97
00:07:12,400 --> 00:07:17,400
for example,
going from passive observation of data 

98
00:07:17,400 --> 00:07:22,200
to more active agents,
which I'm a learn by intervening in the 

99
00:07:23,771 --> 00:07:24,820
world,
uh,

100
00:07:25,000 --> 00:07:27,610
the relationships between causes and 
effects,

101
00:07:28,570 --> 00:07:33,570
the sort of objective functions which 
could be important to allow the,

102
00:07:35,460 --> 00:07:37,120
the highest level,
uh,

103
00:07:37,121 --> 00:07:38,470
explanations to,
to,

104
00:07:38,550 --> 00:07:43,550
to rise from the learning,
which I don't think we have now a kinds 

105
00:07:43,931 --> 00:07:46,570
of objective functions which could be 
used to,

106
00:07:46,880 --> 00:07:48,910
uh,
reward exploration,

107
00:07:48,911 --> 00:07:53,911
the right kind of exploration to these 
kinds of questions are neither in the 

108
00:07:53,911 --> 00:07:55,600
Dataset nor,
and the architecture,

109
00:07:55,601 --> 00:08:00,601
but more in how we learn under what 
objectives and so on.

110
00:08:01,590 --> 00:08:02,970
Yeah,
that's a fruit.

111
00:08:02,990 --> 00:08:06,480
You mentioned in several contexts.
The idea sort of the way children learn,

112
00:08:06,481 --> 00:08:11,481
they interact with objects of the world.
And it seems a fascinating because in 

113
00:08:11,671 --> 00:08:16,671
some sense,
except with some cases in reinforcement 

114
00:08:16,671 --> 00:08:19,701
learning,
that idea is not part of the learning 

115
00:08:20,100 --> 00:08:23,880
process in artificial neural networks.
So it's almost like,

116
00:08:24,350 --> 00:08:29,350
do you envision something like an 
objective function saying,

117
00:08:30,420 --> 00:08:35,420
you know what,
if you poke this object in this kind of 

118
00:08:35,420 --> 00:08:39,111
way will be really helpful for me at to 
four further further learn sort of 

119
00:08:40,291 --> 00:08:43,410
almost guiding some aspect of learning.
Right,

120
00:08:43,450 --> 00:08:43,660
right.

121
00:08:43,660 --> 00:08:48,660
So I was talking to Rebecca Saxe just an
hour ago and she was talking about lots 

122
00:08:49,721 --> 00:08:54,721
and lots of evidence from a infants seem
to clearly take what interests them in a

123
00:09:00,691 --> 00:09:04,950
directed way.
And so they're not passive learners.

124
00:09:05,010 --> 00:09:09,000
They,
they focus their attention on aspects of

125
00:09:09,001 --> 00:09:12,850
the world which are most interesting,
surprising in,

126
00:09:12,851 --> 00:09:17,851
in a non trivial way that makes them 
change their theories of the world.

127
00:09:20,010 --> 00:09:21,300
So,
uh,

128
00:09:21,301 --> 00:09:24,120
that's a fascinating view of the future 
progress.

129
00:09:24,121 --> 00:09:27,840
But on a,
on a more maybe boring

130
00:09:27,970 --> 00:09:32,740
a question,
do you think going deeper and larger.

131
00:09:32,840 --> 00:09:37,840
So do you think just increasing the size
of the things that have been increasing 

132
00:09:38,781 --> 00:09:41,130
a lot in the past few years?
We'll,

133
00:09:41,300 --> 00:09:46,300
we'll also make significant progress.
So some of the representational issues 

134
00:09:47,211 --> 00:09:52,211
that you mentioned,
that they're kind of shallow in some 

135
00:09:52,211 --> 00:09:52,211
sense.

136
00:09:52,330 --> 00:09:54,940
Oh,
prior and a sense of abstraction.

137
00:09:55,000 --> 00:09:57,340
Abstract in the sense of abstraction,
they're not getting some.

138
00:09:58,110 --> 00:10:03,110
I don't think that having more,
more depth in the network in the sense 

139
00:10:03,110 --> 00:10:06,160
of instead of 100 layers we have 10,000
is going to solve our problem.

140
00:10:06,640 --> 00:10:07,390
You don't think so?
No.

141
00:10:08,950 --> 00:10:10,840
Is that obvious to you?
Yes.

142
00:10:11,440 --> 00:10:15,400
What is clear to me is that engineers 
and companies and labs,

143
00:10:15,910 --> 00:10:20,910
a Grad students will continue to tune 
architectures and explore all kinds of 

144
00:10:22,241 --> 00:10:26,110
tweaks to make the current state of the 
arts that he ever slightly better.

145
00:10:26,280 --> 00:10:31,280
Right.
But I don't think that's going to be 

146
00:10:31,280 --> 00:10:33,991
nearly enough.
I think we need some fairly drastic 

147
00:10:33,991 --> 00:10:36,391
changes in the way that we're 
considering learning to achieve the goal

148
00:10:38,141 --> 00:10:43,141
that these learners actually understand 
in a deep way the environment in which 

149
00:10:43,141 --> 00:10:43,950
they are,
uh,

150
00:10:44,010 --> 00:10:45,820
you know,
observing and acting.

151
00:10:46,570 --> 00:10:47,860
But I,
I guess,

152
00:10:47,861 --> 00:10:52,861
uh,
I was trying to ask a question is more 

153
00:10:52,861 --> 00:10:55,471
interesting than just more layers is 
basically once you figure out a way to 

154
00:10:56,291 --> 00:11:01,291
learn to interacting,
how many parameters does it take to 

155
00:11:01,421 --> 00:11:02,530
store that information?

156
00:11:02,920 --> 00:11:07,810
So I think our brain is quite bigger 
than most neural networks.

157
00:11:07,890 --> 00:11:08,410
Right,
right.

158
00:11:08,440 --> 00:11:08,830
Oh,
I see.

159
00:11:08,831 --> 00:11:09,690
What you mean,
I'll I,

160
00:11:09,720 --> 00:11:14,720
I'm with you there.
So I agree that in order to build you'll

161
00:11:14,831 --> 00:11:18,010
nets with a kind of a broad knowledge of
the world.

162
00:11:18,011 --> 00:11:23,011
The typical adult humans have probably 
the kind of computing power we have now.

163
00:11:23,941 --> 00:11:26,650
He's going to be insufficient.
So far.

164
00:11:26,780 --> 00:11:29,170
The good news is there are hardware 
companies building,

165
00:11:29,171 --> 00:11:31,330
you'll net chips and so it's gonna get 
better.

166
00:11:33,040 --> 00:11:36,100
However,
the good news in a way,

167
00:11:36,220 --> 00:11:41,220
which is also a bad news,
is that even our state of the art deep 

168
00:11:41,801 --> 00:11:46,801
learning methods fail to learn models 
that understand even very simple 

169
00:11:47,951 --> 00:11:50,620
environments like some grid worlds that 
we have built.

170
00:11:52,090 --> 00:11:54,070
Even these fairly environments.
I mean,

171
00:11:54,340 --> 00:11:56,230
of course if you trim them with enough 
examples,

172
00:11:56,231 --> 00:11:59,680
eventually they get it,
but it's just like instead of what,

173
00:12:00,040 --> 00:12:05,040
instead of what humans might need,
just a dozens of examples of these 

174
00:12:05,040 --> 00:12:06,760
things,
we'll need millions,

175
00:12:06,880 --> 00:12:08,120
right?
For very,

176
00:12:08,130 --> 00:12:09,340
very,
very simple tasks.

177
00:12:10,120 --> 00:12:15,120
And so I think there's an opportunity 
for academics who don't have the kind of

178
00:12:16,391 --> 00:12:21,391
computing power that say Google has to 
do really important and exciting 

179
00:12:21,641 --> 00:12:26,290
research to advance the state of the art
in training frameworks,

180
00:12:26,710 --> 00:12:28,160
learning models,
uh,

181
00:12:28,210 --> 00:12:33,210
agent learning in even simple 
environments that are synthetic that 

182
00:12:33,821 --> 00:12:38,821
seemed trivial,
but yet current machine learning fails 

183
00:12:38,821 --> 00:12:38,821
on.

184
00:12:38,821 --> 00:12:42,830
We talked about priors and common sense 
knowledge.

185
00:12:43,360 --> 00:12:48,360
It seems like a,
we humans take a lot of knowledge for 

186
00:12:48,471 --> 00:12:49,610
granted.
Uh,

187
00:12:50,390 --> 00:12:55,390
so what's your view of these priors of 
forming this broad view of the world,

188
00:12:56,470 --> 00:13:01,470
this accumulation of information and how
we can teach and your networks are 

189
00:13:01,470 --> 00:13:03,380
learning systems to pick that knowledge 
up.

190
00:13:03,680 --> 00:13:04,750
So knowledge,
you know,

191
00:13:04,890 --> 00:13:09,890
for awhile the artificial intelligence 
maybe in the 80 [inaudible] like there's

192
00:13:11,901 --> 00:13:16,790
a time or knowledge representation,
knowledge acquisition expert systems,

193
00:13:17,280 --> 00:13:22,280
symbolic ai was the view was an 
interesting problem set to solve.

194
00:13:23,330 --> 00:13:26,090
And it was kind of put on hold a little 
bit.

195
00:13:26,450 --> 00:13:29,030
It seems like because it doesn't work,
it doesn't work.

196
00:13:29,031 --> 00:13:31,580
That's right.
But that's right.

197
00:13:31,970 --> 00:13:36,080
But the goals of that remain important.
Yes.

198
00:13:36,081 --> 00:13:41,081
Remain important.
And how do you think those goals can be 

199
00:13:41,081 --> 00:13:41,690
addressed?
So first of all,

200
00:13:42,330 --> 00:13:47,330
I believe that one reason why the 
classical expert systems approach failed

201
00:13:49,230 --> 00:13:52,290
is because a lot of the knowledge we 
have.

202
00:13:52,291 --> 00:13:54,900
So you talked about common sense 
intuition.

203
00:13:55,950 --> 00:14:00,950
Um,
there's a lot of knowledge like this 

204
00:14:00,950 --> 00:14:04,280
which is not consciously accessible.
There are lots of decisions we're taking

205
00:14:04,621 --> 00:14:09,621
that we can really explain even if 
sometimes we make up a story and that 

206
00:14:11,611 --> 00:14:15,100
knowledge is also necessary for machines
to,

207
00:14:15,190 --> 00:14:19,850
to take good decisions.
And that knowledge is hard to codify and

208
00:14:20,020 --> 00:14:25,020
expert systems,
rule based systems and classical ai 

209
00:14:25,020 --> 00:14:28,370
formalism.
And there are other issues of course 

210
00:14:28,370 --> 00:14:31,071
with the old I like,
I'm not really good ways of handling 

211
00:14:31,250 --> 00:14:34,530
uncertainty.
I would say something more subtle,

212
00:14:35,460 --> 00:14:40,460
which we understand better now,
but I think still isn't enough in the 

213
00:14:40,460 --> 00:14:44,721
minds of people.
There's something really powerful that 

214
00:14:44,721 --> 00:14:49,251
comes from distributed representations.
The thing that really makes neural nets 

215
00:14:50,160 --> 00:14:55,160
work so well and it's hard to replicate 
that kind of power in symbolic world.

216
00:14:59,540 --> 00:15:04,540
The knowledge in an expert systems and 
so on is nicely decomposed into like a 

217
00:15:05,361 --> 00:15:07,920
bunch of rules.
Whereas if you think about a new net,

218
00:15:08,020 --> 00:15:13,020
it's the opposite.
You have this big blob of parameters 

219
00:15:13,020 --> 00:15:17,111
which work intensely together to are 
presented everything the network knows 

220
00:15:17,111 --> 00:15:21,520
and it's not sufficiently factorized and
so I think this is one of the weaknesses

221
00:15:22,370 --> 00:15:27,370
of current neural nets that we have to 
take lessons from classical ai in order 

222
00:15:29,001 --> 00:15:34,001
to bring in another kind of composition,
let which is common in language for 

223
00:15:34,001 --> 00:15:35,150
example,
and in these rules,

224
00:15:35,870 --> 00:15:40,870
but that isn't so native to neural nets 
and on that line of thinking 

225
00:15:43,580 --> 00:15:46,010
disentangled representations.
Yes,

226
00:15:46,011 --> 00:15:51,011
so,
so when you connect with disentangled 

227
00:15:51,011 --> 00:15:51,200
representations,
if you might,

228
00:15:51,201 --> 00:15:56,201
if you don't mind it.
So for many years I've thought and I 

229
00:15:56,201 --> 00:16:00,650
still believe that it's really important
that we come up with learning algorithms

230
00:16:00,680 --> 00:16:03,720
either unsupervised or supervised,
but reinforcement,

231
00:16:03,860 --> 00:16:08,860
whatever that builds representations in 
which the important factors,

232
00:16:09,420 --> 00:16:14,420
hopefully causal factors are nicely 
separated and easy to pick up from the 

233
00:16:14,420 --> 00:16:14,450
representation.

234
00:16:15,080 --> 00:16:17,180
So that's the idea of disentangled 
representations.

235
00:16:17,350 --> 00:16:21,800
It says transform the data into a space 
where everything becomes easy.

236
00:16:21,801 --> 00:16:26,390
We can maybe just learn with linear 
models about the things we care about.

237
00:16:27,560 --> 00:16:32,560
And I still think this is important,
but I think this is missing out on a 

238
00:16:32,560 --> 00:16:36,371
very important ingredient which 
classical ai systems can remind us of.

239
00:16:38,150 --> 00:16:40,700
So let's say we have these,
these integral presentation,

240
00:16:40,730 --> 00:16:45,470
you still need to learn about the 
relationships between the variables,

241
00:16:45,500 --> 00:16:48,140
those high level semantic variables.
They're not going to be independent.

242
00:16:48,141 --> 00:16:52,100
I mean this is like too much of a.
An assumption they're going to have some

243
00:16:52,101 --> 00:16:57,101
interesting relationships that allow us 
to predict things in the future to 

244
00:16:57,101 --> 00:17:00,431
explain what happened in the past.
The kind of knowledge about those 

245
00:17:00,431 --> 00:17:04,421
relationships in a classical ai system 
is encoded in the rules like rule is 

246
00:17:04,421 --> 00:17:06,110
just like a little piece of knowledge 
that says,

247
00:17:06,111 --> 00:17:07,400
oh,
I have these two,

248
00:17:07,401 --> 00:17:12,401
three,
four variables that are linked in this 

249
00:17:12,401 --> 00:17:12,401
interesting way.

250
00:17:12,401 --> 00:17:14,540
Then I can say something about one or 
two of them given a couple of others.

251
00:17:14,541 --> 00:17:15,890
Right?
In addition to design,

252
00:17:15,891 --> 00:17:20,891
tangling the,
the elements of the representation which

253
00:17:21,171 --> 00:17:23,630
are like the variables in a rule based 
system,

254
00:17:24,230 --> 00:17:29,230
you also need to disentangle the,
the mechanisms that relate those 

255
00:17:31,911 --> 00:17:34,370
variables to each other.
So like the rules.

256
00:17:34,400 --> 00:17:36,230
So the rules are neatly separated.
And I,

257
00:17:36,290 --> 00:17:37,510
each rule is,
you know,

258
00:17:37,570 --> 00:17:42,360
living on his own and when I,
I changed a rule because I'm learning a,

259
00:17:42,540 --> 00:17:46,790
it doesn't need to break other roles,
whereas current neural nets for example,

260
00:17:46,791 --> 00:17:49,950
are very sensitive to what's called 
catastrophic forgetting,

261
00:17:49,951 --> 00:17:51,050
where,
uh,

262
00:17:51,060 --> 00:17:53,970
after I've learned some things and then 
they're learn new things,

263
00:17:54,160 --> 00:17:56,100
they can destroy the old things that I 
had learned,

264
00:17:56,160 --> 00:18:01,160
right?
If the knowledge was better factorized 

265
00:18:01,160 --> 00:18:03,180
and separated,
disentangled,

266
00:18:03,510 --> 00:18:08,510
then you would avoid a lot of that.
Now you can't do this in the sensory 

267
00:18:09,391 --> 00:18:13,140
domain,
but what do you mean?

268
00:18:13,200 --> 00:18:15,270
Like in Pixel space.
But,

269
00:18:15,271 --> 00:18:18,780
but my idea is that when you project the
data and the right semantic space,

270
00:18:18,781 --> 00:18:23,781
it becomes possible to now represent 
this extra knowledge beyond the 

271
00:18:24,151 --> 00:18:26,060
transformation from input to 
representations,

272
00:18:26,130 --> 00:18:30,120
which is how representations act on each
other and predict the future and so on,

273
00:18:30,820 --> 00:18:35,520
in a way that can be neatly 
disentangled.

274
00:18:35,550 --> 00:18:40,550
So now it's the rules that are 
disentangled from each other and not 

275
00:18:40,550 --> 00:18:40,590
just the variables that are disentangled
from each other.

276
00:18:41,190 --> 00:18:46,190
And you draw a distinction between 
semantic space and pixel needs to be an 

277
00:18:46,190 --> 00:18:47,790
architectural difference or.
Well,

278
00:18:47,791 --> 00:18:48,180
yeah.
So,

279
00:18:48,181 --> 00:18:50,280
so there's the sensory space like 
pixels,

280
00:18:50,310 --> 00:18:52,920
which where everything is entangled,
the,

281
00:18:53,150 --> 00:18:58,150
the information like the variables are 
completely interdependent in very 

282
00:18:58,150 --> 00:19:01,010
complicated ways and also computation 
like the,

283
00:19:01,890 --> 00:19:06,890
it's not just variables,
it's also how they are related to each 

284
00:19:06,890 --> 00:19:07,050
other as is all intertwined.
But,

285
00:19:07,110 --> 00:19:10,500
but I,
I'm hypothesizing that in the right high

286
00:19:10,501 --> 00:19:15,501
level representation space,
both the variables and how they relate 

287
00:19:16,081 --> 00:19:21,081
to each other can be disentangled and 
that will provide a lot of 

288
00:19:21,081 --> 00:19:21,081
generalization.

289
00:19:21,081 --> 00:19:22,890
Power.
Generalization power.

290
00:19:23,010 --> 00:19:28,010
Yes.
A distribution of the test set you 

291
00:19:28,010 --> 00:19:30,120
assume to be the same as the 
distribution of the training set.

292
00:19:30,180 --> 00:19:35,180
Right.
This is where current machine learning 

293
00:19:35,180 --> 00:19:38,090
is too weak.
It doesn't tell us anything is not able 

294
00:19:38,090 --> 00:19:42,261
to tell us anything about how our neural
nets say are going to generalize to a 

295
00:19:42,261 --> 00:19:43,680
new distribution.
And,

296
00:19:43,740 --> 00:19:44,910
and you know,
people may think,

297
00:19:44,911 --> 00:19:49,911
well,
but there's nothing we can say if we 

298
00:19:49,911 --> 00:19:49,911
don't know what the new distribution 
will be.

299
00:19:49,911 --> 00:19:53,900
The truth is humans are able to 
generalize to new distributions how 

300
00:19:54,001 --> 00:19:56,460
we're able to do that.
So because there is something,

301
00:19:56,461 --> 00:20:01,461
these new distributions,
even though they could look very 

302
00:20:01,461 --> 00:20:02,520
different from the distributions,
they have things in common.

303
00:20:02,521 --> 00:20:05,940
So let me give you a concrete example.
You read a science fiction novel,

304
00:20:06,340 --> 00:20:09,120
the science fiction novel maybe you 
know,

305
00:20:09,810 --> 00:20:14,810
brings you in some other planet where 
things look very different on the 

306
00:20:15,331 --> 00:20:17,970
surface,
but it's still the same laws of physics.

307
00:20:18,740 --> 00:20:23,740
Right?
And so you can read the book and You 

308
00:20:23,740 --> 00:20:24,780
understand what's going on.
So the distribution is very different,

309
00:20:25,790 --> 00:20:30,790
but because you can transport a lot of 
the knowledge you had from Earth about 

310
00:20:30,790 --> 00:20:35,250
the underlying cause and effect 
relationships and physical encounters,

311
00:20:35,290 --> 00:20:38,220
isms and all that,
and maybe even social interactions,

312
00:20:39,020 --> 00:20:44,020
you can now make sense of what is going 
on on this planet where like visually 

313
00:20:44,020 --> 00:20:44,020
for example,
things are totally different.

314
00:20:46,080 --> 00:20:48,010
Taking the analogy and distorting

315
00:20:48,010 --> 00:20:50,040
it.
That's enter a science,

316
00:20:50,250 --> 00:20:54,640
science fiction world of say a space 
odyssey 2001 with how?

317
00:20:54,740 --> 00:20:55,980
Yeah,
or,

318
00:20:56,130 --> 00:20:57,430
or maybe,
uh,

319
00:20:57,640 --> 00:21:01,690
which is probably one of my favorite ai 
movie's and then to it.

320
00:21:02,810 --> 00:21:04,940
And then there's another one that a lot 
of people love that.

321
00:21:04,941 --> 00:21:09,700
It may be a little bit outside of the Ai
Community is x Mokena.

322
00:21:09,940 --> 00:21:12,820
I don't know if you've seen it in a 
bottle.

323
00:21:12,860 --> 00:21:15,130
What are your views on that movie?
Does it,

324
00:21:15,760 --> 00:21:20,760
does,
are you able to enjoy the things I like 

325
00:21:20,760 --> 00:21:23,881
and things I hate.
So maybe you could talk about that in 

326
00:21:23,881 --> 00:21:26,680
the context of a question I want to ask,
which is a,

327
00:21:26,740 --> 00:21:31,740
there's quite a large community of 
people from different backgrounds often 

328
00:21:31,740 --> 00:21:35,761
outside of ai who are concerned about 
existential threat of artificial 

329
00:21:35,761 --> 00:21:35,761
intelligence.
Right?

330
00:21:35,761 --> 00:21:38,600
And you've seen this community develop 
overtime,

331
00:21:38,710 --> 00:21:40,540
you've seen you have a perspective.
So,

332
00:21:40,541 --> 00:21:41,470
uh,
what,

333
00:21:41,490 --> 00:21:44,140
what do you think is the best way to 
talk about Ai Safety?

334
00:21:44,470 --> 00:21:49,470
To think about it,
to have discourse about it within ai 

335
00:21:49,470 --> 00:21:52,741
community and outside and grounded in 
the fact that x Mokena is one of the 

336
00:21:53,231 --> 00:21:56,410
main sources of information for the 
general public about Ai.

337
00:21:56,670 --> 00:21:58,500
So I think,
I think you're putting it right.

338
00:21:58,590 --> 00:22:03,590
There is a big difference between the 
sort of discussion we outta have within 

339
00:22:03,601 --> 00:22:08,601
the community and the sort of discussion
that really matter in the general 

340
00:22:08,601 --> 00:22:12,870
public.
So I think the picture of terminator and

341
00:22:13,200 --> 00:22:18,200
Ai Loose and killing people and 
superintelligence that's going to 

342
00:22:19,201 --> 00:22:24,201
destroy us.
Whatever we try isn't really so useful 

343
00:22:24,451 --> 00:22:27,170
for the public discussion because,
uh,

344
00:22:27,530 --> 00:22:32,530
for the public discussion,
the things I believe really matter are 

345
00:22:32,530 --> 00:22:37,220
the short term and medium term,
very likely negative impacts of ai on 

346
00:22:37,220 --> 00:22:40,710
society,
whether it's from a security,

347
00:22:40,711 --> 00:22:45,711
like big brother's scenarios with face 
recognition or killer robots or the 

348
00:22:45,711 --> 00:22:50,331
impact on the job market or a 
concentration of power and 

349
00:22:50,331 --> 00:22:54,351
discrimination,
all kinds of social issues which could 

350
00:22:54,351 --> 00:22:58,071
actually,
some of them could really threaten 

351
00:22:58,071 --> 00:22:58,470
democracy.
For example,

352
00:22:58,880 --> 00:23:01,190
just to clarify,
when you said killer robots,

353
00:23:01,191 --> 00:23:04,110
you mean autonomous weapon weapon 
system,

354
00:23:05,300 --> 00:23:06,470
terminator.
That's right.

355
00:23:07,480 --> 00:23:07,750
I think

356
00:23:07,750 --> 00:23:08,550
these,
these,

357
00:23:08,551 --> 00:23:12,070
uh,
short and medium term concerns should be

358
00:23:12,130 --> 00:23:14,350
important parts of the public debate.
Now,

359
00:23:14,410 --> 00:23:19,410
existential risk for me is a very 
unlikely consideration,

360
00:23:20,230 --> 00:23:25,230
but it's still worth a academic 
investigation and the same way that you 

361
00:23:26,321 --> 00:23:29,820
could say,
should we study what could happen if I'm

362
00:23:29,840 --> 00:23:30,870
motorized,
you know,

363
00:23:31,060 --> 00:23:36,060
came to earth and destroyed it.
So I think it's very unlikely that this 

364
00:23:36,060 --> 00:23:37,780
is going to happen in or happening in a 
reasonable future.

365
00:23:37,781 --> 00:23:39,190
It's,
it's very,

366
00:23:40,240 --> 00:23:40,600
um,
the,

367
00:23:40,620 --> 00:23:45,620
the sort of scenario of an ai getting 
loose goes against my understanding it 

368
00:23:45,620 --> 00:23:47,930
police current machine learning and 
current neural nets and so on,

369
00:23:47,940 --> 00:23:52,940
and it's not plausible to me,
but of course I don't have a crystal 

370
00:23:52,940 --> 00:23:54,320
ball and who knows what ai will be in 50
years from now.

371
00:23:54,350 --> 00:23:57,200
So I think it is worth that.
Scientists study those problems,

372
00:23:57,650 --> 00:24:00,350
it's just not a pressing question as far
as I'm concerned.

373
00:24:01,630 --> 00:24:04,780
So before continuing down that line,
have a few questions there.

374
00:24:04,781 --> 00:24:06,600
But what,
what,

375
00:24:06,610 --> 00:24:07,180
what,
uh,

376
00:24:07,181 --> 00:24:10,390
what do you like and not like about x 
Mokena as a movie because I,

377
00:24:10,600 --> 00:24:13,090
I actually watched it for the second 
time and enjoyed it.

378
00:24:13,660 --> 00:24:18,430
I hated it the first time and I enjoyed 
it quite a bit more the second time when

379
00:24:18,431 --> 00:24:23,431
I sort of learned to accept a certain 
pieces of it as a concept movie.

380
00:24:25,721 --> 00:24:27,040
Hi,
what was your experience?

381
00:24:27,110 --> 00:24:28,270
What are your thoughts?

382
00:24:29,200 --> 00:24:34,200
So the negative is the picture it paints
of science is totally wrong.

383
00:24:38,050 --> 00:24:40,150
Science in general,
and ai in particular,

384
00:24:40,600 --> 00:24:43,180
science is not happening,
uh,

385
00:24:43,210 --> 00:24:44,350
in some,
uh,

386
00:24:44,351 --> 00:24:46,590
hidden place by some,
you know,

387
00:24:46,720 --> 00:24:49,390
really smart guy,
one person,

388
00:24:49,420 --> 00:24:51,970
one person.
This is totally unrealistic.

389
00:24:51,971 --> 00:24:56,971
This is not how it happens.
Even a team of people in some isolated 

390
00:24:57,221 --> 00:25:01,720
place will not make it.
Science moves by small steps,

391
00:25:01,750 --> 00:25:06,750
thanks to the collaboration and um,
community have a large number of people 

392
00:25:08,680 --> 00:25:13,680
interacting and um,
all the scientists who are experts in 

393
00:25:14,801 --> 00:25:16,540
their field kind of know what is going 
on.

394
00:25:16,570 --> 00:25:21,570
Even in the industrial labs.
It's information flows and leaks and so 

395
00:25:21,731 --> 00:25:22,570
on and,
and,

396
00:25:22,720 --> 00:25:27,720
um,
and the spirit of it is very different 

397
00:25:27,720 --> 00:25:28,510
from the way science is painted in this 
movie.

398
00:25:28,760 --> 00:25:29,180
Yeah.
Let me,

399
00:25:29,210 --> 00:25:30,800
let me ask on that.
On that point,

400
00:25:31,490 --> 00:25:34,970
it's been the case to this point that 
kind of,

401
00:25:34,971 --> 00:25:38,120
even if the research happens inside 
Google and facebook inside companies,

402
00:25:38,121 --> 00:25:40,480
it's still kind of comes out.
Isaiah's come out.

403
00:25:40,740 --> 00:25:42,640
Absolutely.
Think there will always be the case with

404
00:25:42,641 --> 00:25:47,641
ai is,
is it possible to bottle ideas to the 

405
00:25:47,641 --> 00:25:50,411
point where there's a set of 
breakthrough to go completely 

406
00:25:50,411 --> 00:25:52,430
undiscovered by the general research 
community?

407
00:25:52,460 --> 00:25:53,720
Do you think that's even possible?

408
00:25:54,980 --> 00:25:56,780
It's possible,
but it's unlikely.

409
00:25:56,781 --> 00:26:00,410
Unlikely.
It's not how it is done now.

410
00:26:00,830 --> 00:26:04,490
It's not how I can foresee it in the 
foreseeable future,

411
00:26:05,630 --> 00:26:10,340
but of course I don't have a crystal 
ball.

412
00:26:10,341 --> 00:26:14,430
And so who knows?
This is science fiction after all,

413
00:26:15,230 --> 00:26:17,330
but,
but usually it's ominous that the lights

414
00:26:17,331 --> 00:26:19,250
went off during,
during that discussion.

415
00:26:21,290 --> 00:26:23,080
So the problem again,
there's a,

416
00:26:23,110 --> 00:26:28,110
you know,
one thing is the movie and you could 

417
00:26:28,110 --> 00:26:28,110
imagine all kinds of science fiction.
The problem wouldn't for me,

418
00:26:28,110 --> 00:26:33,040
maybe similar to the question about 
existential risk is that this kind of 

419
00:26:33,171 --> 00:26:38,171
movie paints such a picture of what his 
actual,

420
00:26:38,320 --> 00:26:40,610
you know,
the actual science and how it's going on

421
00:26:40,611 --> 00:26:44,250
that,
that it can have unfortunate on people's

422
00:26:44,251 --> 00:26:49,251
understanding of current science.
And so that's kind of sad is an 

423
00:26:51,151 --> 00:26:56,070
important principle in research which is
diversity.

424
00:26:56,100 --> 00:26:59,680
So in other words,
research is exploration resources,

425
00:26:59,790 --> 00:27:04,500
expression in the space of ideas and 
different people will focus on different

426
00:27:04,501 --> 00:27:06,960
directions and this is not just good.

427
00:27:07,080 --> 00:27:12,080
It's essential.
So I'm totally fine with people 

428
00:27:12,080 --> 00:27:16,750
exploring directions that are contrary 
to mine or look orthogonal into mine.

429
00:27:18,640 --> 00:27:21,150
I am more than fine.
I think it's important.

430
00:27:21,900 --> 00:27:26,130
I and my friends don't claim we have 
universal truth about what will,

431
00:27:26,160 --> 00:27:27,960
especially about what will happen in the
future.

432
00:27:28,830 --> 00:27:33,830
Now that being said,
we have our intuitions and then we act 

433
00:27:33,830 --> 00:27:37,821
accordingly according to where we think 
we can be most useful and where society 

434
00:27:38,611 --> 00:27:43,611
has the most to gain or to lose.
We should have those debates and um,

435
00:27:44,910 --> 00:27:45,850
and,
and,

436
00:27:45,851 --> 00:27:50,851
and not end up in a society where 
there's only one voice and one way of 

437
00:27:50,851 --> 00:27:53,190
thinking and research money is spread 
out.

438
00:27:53,370 --> 00:27:58,370
So the disagreement is a is a sign of a 
good research,

439
00:27:58,441 --> 00:27:59,580
good size.
So yes,

440
00:28:01,370 --> 00:28:03,980
the idea of bias in the human sense of 
bias.

441
00:28:04,030 --> 00:28:05,540
Yeah.
Uh,

442
00:28:05,680 --> 00:28:06,700
how do you think about

443
00:28:07,570 --> 00:28:12,570
instilling in machine learning something
that's aligned with human values in 

444
00:28:12,941 --> 00:28:15,260
terms of bias?
We intuitively,

445
00:28:15,261 --> 00:28:19,630
as human beings have a concept of what 
bias means of what a fundamental respect

446
00:28:19,960 --> 00:28:24,960
for other human beings means.
But how do we instill that into machine 

447
00:28:24,960 --> 00:28:25,420
learning systems,
do you think?

448
00:28:26,620 --> 00:28:31,620
So,
I think they're short term things that 

449
00:28:31,620 --> 00:28:35,340
are already happening.
And then there are longterm things that 

450
00:28:35,340 --> 00:28:38,311
we need to do in the short term.
There are techniques that have been 

451
00:28:38,411 --> 00:28:43,411
proposed and I think we'll continue to 
be improved and maybe alternative will 

452
00:28:43,411 --> 00:28:47,161
come up to take data sets in which we 
know there is bias,

453
00:28:47,230 --> 00:28:52,230
we can measure it pretty much any 
dataset we're humans or being observed 

454
00:28:52,230 --> 00:28:57,211
taking decisions will have some sort of 
bias or discrimination against 

455
00:28:57,211 --> 00:29:00,841
particular groups and so on.
And we can use machine learning 

456
00:29:00,841 --> 00:29:05,461
techniques to try to build predictor is 
classifiers that are going to be less 

457
00:29:05,981 --> 00:29:07,760
biased.
Uh,

458
00:29:08,080 --> 00:29:09,490
we can do it,
for example,

459
00:29:09,491 --> 00:29:14,491
using adversarial methods to make our 
systems less sensitive to these 

460
00:29:15,520 --> 00:29:19,600
variables we should not be sensitive to.
So these are clear,

461
00:29:19,601 --> 00:29:22,210
well defined ways of trying to address 
the problem.

462
00:29:22,211 --> 00:29:25,570
Maybe they have weaknesses and you know,
more research is needed and so on.

463
00:29:25,570 --> 00:29:30,570
But I think in fact there are 
sufficiently mature that governments 

464
00:29:30,570 --> 00:29:33,400
should start regulating companies where 
it matters,

465
00:29:33,401 --> 00:29:38,401
say like insurance companies so that 
they use those techniques because those 

466
00:29:38,401 --> 00:29:41,140
techniques will probably reduce the 
bias.

467
00:29:41,890 --> 00:29:43,660
But at a cost,
for example,

468
00:29:43,661 --> 00:29:45,550
maybe their predictions will be less 
accurate.

469
00:29:45,820 --> 00:29:48,040
And so companies will not do it until 
you force them.

470
00:29:49,000 --> 00:29:51,580
Alright,
so this is short term longterm.

471
00:29:51,670 --> 00:29:56,670
I'm really interested in thinking about 
how we can instill moral values into 

472
00:29:58,691 --> 00:29:59,990
computers.
Obviously this is not,

473
00:29:59,991 --> 00:30:04,991
uh,
something will achieve in the next five 

474
00:30:04,991 --> 00:30:05,200
or 10 years.
How can we,

475
00:30:05,201 --> 00:30:10,201
you know,
there's already work and detecting 

476
00:30:10,201 --> 00:30:10,201
emotions,
for example,

477
00:30:10,201 --> 00:30:14,220
in images and sounds and texts and also 
studying how different agents 

478
00:30:18,071 --> 00:30:23,071
interacting in different ways may 
correspond to patterns of say injustice,

479
00:30:26,080 --> 00:30:31,080
which could trigger anger.
So these are things we can do in the 

480
00:30:31,080 --> 00:30:35,101
medium term and eventually train 
computers to model,

481
00:30:37,720 --> 00:30:41,260
for example,
how humans react emotionally.

482
00:30:41,330 --> 00:30:45,100
Uh,
I would say the simplest thing is unfair

483
00:30:45,850 --> 00:30:50,850
situations which trigger anger.
This is one of the most basic emotions 

484
00:30:50,850 --> 00:30:55,501
that we share with other animals.
I think it's quite feasible within the 

485
00:30:55,501 --> 00:31:00,360
next few years.
So we can build systems that can detect 

486
00:31:00,360 --> 00:31:00,640
these kinds of things to the extent 
unfortunately,

487
00:31:00,641 --> 00:31:04,240
that they understand enough about the 
world around us,

488
00:31:04,241 --> 00:31:09,241
which is a long time away.
But maybe we can initially do this in 

489
00:31:09,280 --> 00:31:14,280
virtual environments.
So you can imagine like a video game 

490
00:31:14,280 --> 00:31:18,000
where agents interact in,
in some ways and then some situations 

491
00:31:18,000 --> 00:31:19,210
trigger an emotion.
Uh,

492
00:31:19,270 --> 00:31:23,260
I think we could train machines to 
detect those situations and predict that

493
00:31:23,300 --> 00:31:24,380
particular emotion,
you know,

494
00:31:24,390 --> 00:31:28,810
will likely be felt if a human was 
playing one of the characters.

495
00:31:29,260 --> 00:31:33,210
You have shown excitement and done a lot
of excellent work with,

496
00:31:33,220 --> 00:31:34,570
uh,
on supervised learning,

497
00:31:35,500 --> 00:31:36,870
but on a supervisor,
you know,

498
00:31:36,880 --> 00:31:39,550
there's been a lot of success on the 
supervised learning.

499
00:31:39,551 --> 00:31:40,580
So yes.
Yes.

500
00:31:40,920 --> 00:31:45,920
And one of the things I'm really 
passionate about is how humans and 

501
00:31:45,920 --> 00:31:47,260
robots work together.
And uh,

502
00:31:47,530 --> 00:31:51,490
in the context of supervised learning,
that means the process of annotation.

503
00:31:52,210 --> 00:31:57,210
Do you think about the problem of 
annotation of put in a more interesting 

504
00:31:58,051 --> 00:31:59,240
way humans

505
00:31:59,430 --> 00:32:02,730
teaching machines is there?
Yes,

506
00:32:02,790 --> 00:32:06,450
I think it's an important subject 
reducing it to annotation,

507
00:32:06,480 --> 00:32:11,160
maybe useful for somebody building a 
system tomorrow,

508
00:32:11,161 --> 00:32:16,161
but longer term the process of teaching,
I think it's something that deserves a 

509
00:32:16,981 --> 00:32:19,320
lot more attention from the machine 
learning community.

510
00:32:19,321 --> 00:32:24,321
So there are,
people have coined the term machine 

511
00:32:24,321 --> 00:32:24,510
teaching.
So what are good strategies for teaching

512
00:32:24,540 --> 00:32:27,570
a learning agent?
And can we,

513
00:32:28,100 --> 00:32:29,350
uh,
design,

514
00:32:29,430 --> 00:32:32,100
train a system that's going to be,
it's going to be a good teacher.

515
00:32:32,550 --> 00:32:37,550
So,
so in my group we have a project called 

516
00:32:37,550 --> 00:32:41,061
a bbi or baby I game where there is a,
a game at or scenario where there's a,

517
00:32:43,321 --> 00:32:48,321
a learning agent and a teaching agent,
presumably the teaching agent would 

518
00:32:48,500 --> 00:32:51,890
eventually be a human,
but we're not there yet.

519
00:32:52,430 --> 00:32:54,830
Um,
and the,

520
00:32:54,950 --> 00:32:59,950
um,
the role of the teacher is to use its 

521
00:32:59,950 --> 00:33:03,251
knowledge of the environment which it 
can acquire using whatever way brute 

522
00:33:03,251 --> 00:33:06,851
force to help the learner learn as 
quickly as possible.

523
00:33:09,020 --> 00:33:11,180
So the learner is going to try to learn 
by itself,

524
00:33:11,181 --> 00:33:14,120
may be using some exploration and,
and whatever.

525
00:33:14,660 --> 00:33:17,270
Um,
but the teacher can choose,

526
00:33:18,140 --> 00:33:18,860
can,
can,

527
00:33:18,890 --> 00:33:21,470
can have an influence on the interaction
with the learner,

528
00:33:21,500 --> 00:33:26,500
so as to guide the learner,
maybe teach it the things that the 

529
00:33:27,711 --> 00:33:32,711
learner has most trouble with or just at
the boundary between what it knows and 

530
00:33:32,711 --> 00:33:32,711
doesn't know.
And so on.

531
00:33:32,711 --> 00:33:33,740
So there's,
there's a,

532
00:33:33,741 --> 00:33:37,640
there's a tradition of these kinds of 
ideas from other fields and um,

533
00:33:38,230 --> 00:33:39,880
uh,
like tutorial systems,

534
00:33:39,890 --> 00:33:41,570
for example,
an Ai.

535
00:33:41,640 --> 00:33:46,640
Um,
and and of course people in the 

536
00:33:46,640 --> 00:33:46,700
humanities have been thinking about 
these questions,

537
00:33:46,701 --> 00:33:51,701
but I think it's time that machine 
learning people look at this because in 

538
00:33:51,701 --> 00:33:56,150
the future we'll have more and more a 
human machine interaction with the human

539
00:33:56,151 --> 00:34:00,500
in the loop and I think understanding 
how to make this work better,

540
00:34:00,710 --> 00:34:05,710
problems around that are very 
interesting and not sufficiently 

541
00:34:05,710 --> 00:34:08,711
addressed.
You've done a lot of work with language 

542
00:34:08,711 --> 00:34:12,220
to what aspect of the traditionally 
formulated turing test,

543
00:34:13,250 --> 00:34:18,250
a test of natural language understanding
and generation in your eyes is the most 

544
00:34:18,250 --> 00:34:21,110
difficult of conversation.
What in your eyes is the hardest part of

545
00:34:21,111 --> 00:34:23,630
conversation to solve from machines?

546
00:34:24,530 --> 00:34:29,530
So I would say it's everything having to
do with the non linguistic knowledge 

547
00:34:30,320 --> 00:34:34,220
which implicitly you need in order to 
make sense of sentences.

548
00:34:34,820 --> 00:34:38,360
Things like the Winograd Schema.
So these sentences that are semantically

549
00:34:38,361 --> 00:34:43,361
ambiguous.
Now the words you need to understand 

550
00:34:43,361 --> 00:34:46,991
enough about the world in order to 
really interpret properly those 

551
00:34:46,991 --> 00:34:49,400
sentences.
I think these are interesting challenges

552
00:34:49,401 --> 00:34:54,401
for machine learning because they point 
in the direction of building systems 

553
00:34:55,520 --> 00:35:00,520
that both understand how the world works
and this causal relationships in the 

554
00:35:00,520 --> 00:35:05,441
world and associate that knowledge with 
how to express it in language.

555
00:35:07,370 --> 00:35:12,370
Either for reading or writing.
You speak French?

556
00:35:13,370 --> 00:35:14,330
Yes.
It's my mother tongue.

557
00:35:14,680 --> 00:35:19,680
It's one of the romance languages.
Do you think passing the turing test and

558
00:35:20,141 --> 00:35:23,200
all the underlying challenges you just 
mentioned depend on language.

559
00:35:23,201 --> 00:35:28,201
Do you think it might be easier in 
French that is in English and that is 

560
00:35:28,201 --> 00:35:28,201
independent of language?

561
00:35:28,950 --> 00:35:33,950
I think it's independent of language.
I would like to build systems that can 

562
00:35:36,721 --> 00:35:41,721
use same principles,
the same learning mechanisms to learn 

563
00:35:43,771 --> 00:35:46,110
from human agents.
Whatever their language.

564
00:35:47,190 --> 00:35:52,190
Well certainly us humans can talk more 
beautifully and smoothly in poetry.

565
00:35:52,201 --> 00:35:55,980
So I'm Russian originally.
I know poetry and Russian is

566
00:35:57,060 --> 00:35:58,200
maybe easier

567
00:35:58,570 --> 00:36:01,570
to convey complex ideas and then it is 
in English.

568
00:36:02,320 --> 00:36:02,950
But,
uh,

569
00:36:02,980 --> 00:36:07,150
maybe I'm showing my bias and some 
people say that about a French,

570
00:36:07,570 --> 00:36:08,430
but,
uh,

571
00:36:08,470 --> 00:36:13,470
of course the goal ultimately is our 
human brain is able to utilize any kind 

572
00:36:13,960 --> 00:36:17,920
of those languages to use them as tools 
to convey meaning.

573
00:36:18,400 --> 00:36:19,090
Yeah,
of course,

574
00:36:19,260 --> 00:36:22,680
differences between languages and maybe 
some are slightly better at some things,

575
00:36:22,681 --> 00:36:26,520
but in the grand scheme of things where 
we're trying to understand how the brain

576
00:36:26,521 --> 00:36:31,230
works and language and so on,
I think these differences are minute.

577
00:36:32,950 --> 00:36:37,950
So you've lived perhaps through an ai 
winter of sorts?

578
00:36:38,710 --> 00:36:43,710
Yes.
How did you stay warm and continuing to 

579
00:36:43,710 --> 00:36:46,570
research?
Stay warm with friends with friends.

580
00:36:46,571 --> 00:36:51,571
Okay.
So it's important to have friends and 

581
00:36:51,571 --> 00:36:53,881
uh,
what have you learned from the 

582
00:36:53,881 --> 00:36:53,881
experience?

583
00:36:53,881 --> 00:36:56,950
Listen to your inner voice.
Don't you know,

584
00:36:57,010 --> 00:37:02,010
be trying to just please the crowds and 
the fashion and uh,

585
00:37:04,420 --> 00:37:09,420
if you have a strong intuition about 
something that is not contradicted by 

586
00:37:10,390 --> 00:37:12,730
actual evidence,
go for it.

587
00:37:13,820 --> 00:37:15,790
I mean,
it could be contradicted by people

588
00:37:16,930 --> 00:37:19,720
that not your own instinct of base title
everything.

589
00:37:19,810 --> 00:37:20,980
So of course,
of course

590
00:37:21,170 --> 00:37:26,170
you have to adapt your beliefs when your
experiments contradict those beliefs.

591
00:37:27,530 --> 00:37:31,130
But you have to stick to your beliefs 
otherwise,

592
00:37:31,180 --> 00:37:32,780
and it's,
it's,

593
00:37:32,810 --> 00:37:34,940
it's what allowed me to go through those
years.

594
00:37:34,941 --> 00:37:39,941
It's what allowed me to persist in 
directions that took time.

595
00:37:41,420 --> 00:37:46,420
Whatever other people think it took time
to mature and you bring fruits.

596
00:37:47,910 --> 00:37:51,490
So history of Ai is marked with these,
uh,

597
00:37:52,380 --> 00:37:54,360
of course it's Margaret technical 
breakthroughs,

598
00:37:54,361 --> 00:37:59,361
but it's also mark with these seminal 
events that capture the imagination of 

599
00:37:59,361 --> 00:38:01,710
the community.
Most recent,

600
00:38:02,130 --> 00:38:07,130
I would say a Alphago beating the world 
champion human goal player was one of 

601
00:38:07,130 --> 00:38:11,571
those moments.
What do you think the next such moment 

602
00:38:11,580 --> 00:38:13,100
might be for?

603
00:38:13,540 --> 00:38:18,540
First of all,
I think that these so called seminal 

604
00:38:18,540 --> 00:38:21,091
events are overrated.
Uh,

605
00:38:22,570 --> 00:38:25,750
as I said,
science really moves by small steps.

606
00:38:25,930 --> 00:38:30,930
Now what happens is you make one more 
small step and it's like the drop that 

607
00:38:33,860 --> 00:38:35,140
you know,
allows to,

608
00:38:35,190 --> 00:38:35,740
uh,
you know,

609
00:38:35,860 --> 00:38:37,570
that fills the bucket and,
and,

610
00:38:37,571 --> 00:38:42,571
uh,
and then you have drastic consequences 

611
00:38:42,571 --> 00:38:45,181
because now you are able to do something
you were not able to do before or now 

612
00:38:45,181 --> 00:38:49,770
say the cost of building some device or 
solving the problem becomes cheaper than

613
00:38:49,811 --> 00:38:50,490
what existed.
And,

614
00:38:50,491 --> 00:38:52,720
and you have a new market that opens up,
right?

615
00:38:52,721 --> 00:38:57,721
So,
so especially in the world of commerce 

616
00:38:57,721 --> 00:39:00,110
and in applications,
the impact of a small scientific 

617
00:39:00,161 --> 00:39:03,280
progress could be huge.
Um,

618
00:39:03,610 --> 00:39:06,040
but in the science itself,
I think it's very,

619
00:39:06,041 --> 00:39:08,220
very gradual.
And um,

620
00:39:08,510 --> 00:39:09,700
where are these steps

621
00:39:09,760 --> 00:39:13,770
being taken now?
So there is unsupervised learning.

622
00:39:13,880 --> 00:39:17,360
If I look at one trend that I like in,
uh,

623
00:39:17,390 --> 00:39:18,740
in,
in my community,

624
00:39:19,190 --> 00:39:21,300
um,
so for example,

625
00:39:21,310 --> 00:39:23,180
in my line,
my institute,

626
00:39:23,240 --> 00:39:28,240
what are the two hottest topics gans and
reinforced for planning even though in 

627
00:39:31,450 --> 00:39:36,450
the Montreal in particular,
like reinforcement learning was 

628
00:39:36,450 --> 00:39:37,520
something pretty much absent just two or
three years ago.

629
00:39:37,970 --> 00:39:42,970
So it is really a big interest from 
students and there's a big interest from

630
00:39:44,270 --> 00:39:49,270
people like me.
So I would say this is something where 

631
00:39:49,270 --> 00:39:54,101
we're going to see more progress even 
though it hasn't yet provided much in 

632
00:39:54,501 --> 00:39:59,501
terms of actual industrial fallout.
Like even though there's Alphago,

633
00:39:59,511 --> 00:40:04,511
there's no,
like Google is not making money on this 

634
00:40:04,511 --> 00:40:06,881
right now.
But I think over the long term this is 

635
00:40:06,881 --> 00:40:06,980
really,
really important for many reasons.

636
00:40:08,840 --> 00:40:09,860
So in other words,
agent,

637
00:40:09,861 --> 00:40:11,690
I would say reinforcement learning,
baby,

638
00:40:11,910 --> 00:40:12,950
uh,
more generally,

639
00:40:12,980 --> 00:40:15,650
agent learning because it doesn't have 
to be with rewards,

640
00:40:15,651 --> 00:40:19,340
it could be in all kinds of ways that an
agent is learning about its environment.

641
00:40:20,650 --> 00:40:22,330
Now reinforced learning.
You're excited.

642
00:40:22,331 --> 00:40:22,390
Yeah.

643
00:40:22,760 --> 00:40:24,470
Do you think,
uh,

644
00:40:24,810 --> 00:40:28,650
you think gans could provide something?
Yes.

645
00:40:28,651 --> 00:40:31,260
Some moment in,
in well

646
00:40:32,000 --> 00:40:37,000
or other generative models I believe 
will be crucial ingredients in building 

647
00:40:39,710 --> 00:40:44,710
agents that can understand the world.
A lot of the successes in reinforcement 

648
00:40:45,531 --> 00:40:50,531
learning in the past has been with a 
policy gradient where you just learn to 

649
00:40:50,541 --> 00:40:55,541
policy.
You don't actually learn a model of the 

650
00:40:55,541 --> 00:40:55,541
world,
but there are lots of issues with that.

651
00:40:55,541 --> 00:40:59,930
Um,
and we don't know how to model based 

652
00:40:59,930 --> 00:41:02,291
around right now,
but I think this is where we have to go 

653
00:41:02,291 --> 00:41:07,000
in order to build models that can 
generalize faster and better to new 

654
00:41:07,000 --> 00:41:09,080
distributions.
Um,

655
00:41:09,110 --> 00:41:14,110
that capture to some extent at least the
underlying causal mechanisms in the 

656
00:41:14,391 --> 00:41:16,610
world.
Last question,

657
00:41:17,480 --> 00:41:19,480
what made you fall in love with her

658
00:41:19,540 --> 00:41:22,030
official intelligence?
If you look back,

659
00:41:22,690 --> 00:41:27,690
what was the first moment in your life 
when you were fascinated by either the 

660
00:41:28,571 --> 00:41:30,400
human mind or the artificial mind?

661
00:41:31,320 --> 00:41:33,680
Know when I was an adolescent,
I was a lot,

662
00:41:33,681 --> 00:41:36,350
and then I,
I started reading science fiction.

663
00:41:37,790 --> 00:41:40,110
There you go.
That's it.

664
00:41:40,570 --> 00:41:41,990
That's,
that's where I got hooked.

665
00:41:42,530 --> 00:41:43,730
And then,
um,

666
00:41:43,770 --> 00:41:44,540
and then,
you know,

667
00:41:44,570 --> 00:41:48,080
I had one of the first personal 
computers and uh,

668
00:41:48,081 --> 00:41:51,890
I got hooked in programming and so it 
just,

669
00:41:51,970 --> 00:41:56,970
you know,
start with fiction and then make it a 

670
00:41:56,970 --> 00:41:56,970
reality.
That's right.

671
00:41:56,970 --> 00:41:56,970
You.
Ashley,

672
00:41:56,970 --> 00:41:58,070
thank you so much for talking to my 
pleasure.

