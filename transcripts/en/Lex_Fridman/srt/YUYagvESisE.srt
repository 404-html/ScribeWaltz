1
00:00:00,090 --> 00:00:01,900
The following is a
conversation with Kyle vote.

2
00:00:02,280 --> 00:00:07,230
He's the president and the CTO of cruise
automation leading an effort to solve

3
00:00:07,231 --> 00:00:10,830
one of the biggest robotics challenges
of our time vehicle automation.

4
00:00:10,890 --> 00:00:13,110
He's a co founder of two
successful companies,

5
00:00:13,111 --> 00:00:16,860
twitch and crews that have
east sold for $1 billion.

6
00:00:17,040 --> 00:00:21,750
And he's a great example of the innovative
spirit that flourishes in Silicon

7
00:00:21,750 --> 00:00:22,170
Valley.

8
00:00:22,170 --> 00:00:27,000
And now it's facing an interesting and
exciting challenge of matching that

9
00:00:27,001 --> 00:00:32,001
spirit with the mass production and
the safety century culture of a major

10
00:00:32,120 --> 00:00:33,900
automaker like General Motors.

11
00:00:34,440 --> 00:00:38,910
This conversation as part of the MIT
artificial general intelligence series and

12
00:00:38,911 --> 00:00:41,820
the artificial intelligence podcasts.
If you enjoy it,

13
00:00:42,120 --> 00:00:44,700
please subscribe on Youtube,
iTunes,

14
00:00:44,850 --> 00:00:49,350
or simply connect with me on Twitter
at Lex Friedman, spelled f, r I. D.

15
00:00:49,790 --> 00:00:53,310
And now here's my
conversation with Kyle vote.

16
00:00:54,660 --> 00:00:58,190
You grew up in Kansas, right? Yeah. And I
just saw that picture you had to hit. No,

17
00:00:58,200 --> 00:01:00,810
there's some a little bit, a little
bit worried about that now. Yeah.

18
00:01:00,811 --> 00:01:02,600
So in high school in Kansas City,

19
00:01:02,601 --> 00:01:07,540
you joined Shawnee mission north
high school robotics team. Yeah.

20
00:01:07,650 --> 00:01:10,920
Now that wasn't your high school.
That's right. That was, that was, uh,

21
00:01:10,970 --> 00:01:13,510
the only high school in the
area that had a, like a,

22
00:01:13,511 --> 00:01:16,170
a teacher who was willing to
sponsor our first robotics team.

23
00:01:16,620 --> 00:01:20,260
I was going to troll you a little bit,
jog your memory a little bit. Yeah,

24
00:01:20,280 --> 00:01:23,490
that kid trying to look super cool
and intent. It does, cause you know,

25
00:01:23,491 --> 00:01:25,620
this was battlebots
it's a serious business.

26
00:01:25,650 --> 00:01:29,670
So we're standing there with a welded
steel frame and look and tough.

27
00:01:30,240 --> 00:01:34,320
So go back there. What does
that jury to robotics? Well,

28
00:01:34,350 --> 00:01:36,710
I think I, I've been trying
to figure this out for awhile,

29
00:01:36,780 --> 00:01:39,120
but I've always liked building things
with legos and when I was really,

30
00:01:39,121 --> 00:01:43,470
really young, I wanted the
Legos. I had motors and other
things and then, you know,

31
00:01:43,471 --> 00:01:47,430
Lego Mindstorms came out and for the
first time you could program Lego

32
00:01:47,431 --> 00:01:52,200
contraptions. And I think, uh, things
just sort of snowballed from that.

33
00:01:52,530 --> 00:01:55,760
But I remember, um, seeing, you know,

34
00:01:55,770 --> 00:01:59,250
the battlebots TV show on comedy central
and thinking that is the coolest thing

35
00:01:59,280 --> 00:02:01,840
in the world. I want to be
a part of that. And, uh,

36
00:02:01,980 --> 00:02:06,630
not knowing a whole lot about how to
build these 200 pound fighting robots.

37
00:02:06,840 --> 00:02:09,930
So I sort of obsessive
Lee poured over the, uh,

38
00:02:09,990 --> 00:02:13,590
Internet forums where all the creators
for battlebots would sort of hang out and

39
00:02:13,591 --> 00:02:17,630
talk about, you know, document their
build progress and everything. And, uh,

40
00:02:17,640 --> 00:02:20,490
I think I read, I must
have read like, you know,

41
00:02:20,520 --> 00:02:24,900
tens of thousands of foreign posts from
basically everything that was out there

42
00:02:24,901 --> 00:02:28,170
on what these people were doing and
eventually like sort of triangulate it how

43
00:02:28,171 --> 00:02:31,980
to, how to put some of these
things together and, and uh, uh,

44
00:02:32,010 --> 00:02:34,800
ended up doing battlebots, which
was, you know, I was like 13 or 14,

45
00:02:34,801 --> 00:02:37,650
which was pretty awesome.
I'm not sure if the show is still running,

46
00:02:37,651 --> 00:02:42,540
but so battlebots is, there's not an
artificial intelligence component.

47
00:02:42,690 --> 00:02:44,470
It's remotely controlled and you're,

48
00:02:44,480 --> 00:02:47,000
it's almost like a mechanical
engineering challenge of yeah,

49
00:02:47,010 --> 00:02:51,300
I think things that can be broken
there, radio controlled. So, uh,

50
00:02:51,301 --> 00:02:55,440
and I think that they allowed some
limited form of autonomy, but you know,

51
00:02:55,441 --> 00:02:58,530
in a two minute match your you're
in and the way these things ran,

52
00:02:58,770 --> 00:03:01,900
you're doing yourself a disservice
by trying to automate it versus just,

53
00:03:02,020 --> 00:03:04,210
you know, do the practical thing,
which is drive it yourself.

54
00:03:04,690 --> 00:03:08,140
And there's an entertainment aspect,
uh, just just going, I need to,

55
00:03:08,141 --> 00:03:12,100
but there's like, some of them wield an
ax, some of them, I mean there's that fun.

56
00:03:12,130 --> 00:03:15,040
So what drew you to that ass but it
wasn't the mechanical engineering.

57
00:03:15,340 --> 00:03:20,340
Was it the dream to create
like a Frankenstein and
tell essentially and being,

58
00:03:21,040 --> 00:03:24,370
or was it just like the Lego, you like
tinkering with stuff? I mean that,

59
00:03:24,371 --> 00:03:24,940
that was just

60
00:03:24,940 --> 00:03:28,090
building something. I think
the idea of, you know, this,

61
00:03:28,091 --> 00:03:29,590
this radio controlled machine that,

62
00:03:29,800 --> 00:03:32,350
that can do various things if it has
like a weapon or something was pretty

63
00:03:32,351 --> 00:03:36,360
interesting. I agree. It's, it doesn't
have the same appeal as, you know,

64
00:03:36,400 --> 00:03:40,000
autonomous robots, which I, which I, you
know, sort of gravitated towards later on.

65
00:03:40,270 --> 00:03:44,710
But it was definitely an engineering
challenge because everything you did in

66
00:03:44,770 --> 00:03:48,350
that competition was pushing
components to their limits.

67
00:03:48,410 --> 00:03:53,410
So we would buy like these $40
DC motors that came out of a,

68
00:03:54,140 --> 00:03:58,550
a winch like on the front of a pickup
truck or something and we power their car

69
00:03:58,551 --> 00:04:02,630
with those and we'd run them at like
double or triple their rated voltage so

70
00:04:02,631 --> 00:04:05,360
they immediately start overheating.
But for that two minute match,

71
00:04:05,361 --> 00:04:09,860
you can get a significant increase in
the power output of those motors before

72
00:04:09,861 --> 00:04:12,740
they burn out. And so you're doing
the same thing for your battery packs.

73
00:04:12,741 --> 00:04:16,900
All the materials and the
system. And I think there is
something, something, uh, uh,

74
00:04:16,910 --> 00:04:20,480
intrinsically interesting about just
seeing like where things break and

75
00:04:20,660 --> 00:04:25,030
did you offline see where they break?
Did you take it to the test stand point?

76
00:04:25,040 --> 00:04:28,010
Like how did you know the two minutes or
was there a reckless let's just go with

77
00:04:28,011 --> 00:04:29,420
it and see.

78
00:04:29,690 --> 00:04:33,340
We weren't very good at battlebots. We
lost all of our matches the first round,

79
00:04:34,370 --> 00:04:36,200
the one I built first,

80
00:04:36,201 --> 00:04:38,720
both of them were these wedge
shaped robots cause of wedge.

81
00:04:38,960 --> 00:04:41,210
Even though it's sort of boring
and look at is extremely effective.

82
00:04:41,211 --> 00:04:44,690
You'd drive towards another robot and
the front edge of it gets under him.

83
00:04:44,691 --> 00:04:47,760
And then they sort of flip over,
kinda like a door stopper.

84
00:04:48,240 --> 00:04:52,680
And the first one had a pneumatic polished
stainless steel spike on the front

85
00:04:52,681 --> 00:04:54,180
that would shoot out about eight inches.

86
00:04:54,830 --> 00:04:57,530
The purpose of which is what
pretty pretty ineffective actually,

87
00:04:57,531 --> 00:05:01,460
but it looks cool. And uh, it was,
it helped with the lift. No it was,

88
00:05:01,461 --> 00:05:04,820
it was just to try to poke holes
in the other robot. And then, uh,

89
00:05:04,850 --> 00:05:06,890
the second time I did it,
which is the following,

90
00:05:07,220 --> 00:05:10,310
I think maybe 18 months later we had a,

91
00:05:10,910 --> 00:05:12,500
well a titanium acts with a,

92
00:05:12,530 --> 00:05:17,180
with a hardened steel tip on it that
was powered by a hydraulic cylinder,

93
00:05:17,181 --> 00:05:20,300
which we were a activating
with liquid co two,

94
00:05:20,390 --> 00:05:24,290
which was, had its own
set of problems. So great.

95
00:05:24,291 --> 00:05:27,060
So that's kind of on the
hardware side. I mean, uh,

96
00:05:27,170 --> 00:05:32,120
at a certain point there must have been
born or fascination on the software

97
00:05:32,121 --> 00:05:36,710
side. So what was the first piece of
code you've written? A man go back there.

98
00:05:36,750 --> 00:05:41,240
See what language was it? What a, what
was the, was the emacs vim, was it a,

99
00:05:41,241 --> 00:05:45,740
a more respectable modern id?
Do you remember any of this?

100
00:05:45,800 --> 00:05:47,270
Yeah, well I remember, um,

101
00:05:47,780 --> 00:05:51,590
I think maybe when I was in
third or fourth grade school,

102
00:05:51,591 --> 00:05:53,240
I was at elementary school,
had a bunch of apple,

103
00:05:53,241 --> 00:05:58,130
two computers and we'd play games on those
and I every once in a while something

104
00:05:58,131 --> 00:06:00,440
would, would uh, would crash.

105
00:06:00,441 --> 00:06:02,360
It wouldn't start up correctly
and it would dump you out too.

106
00:06:02,570 --> 00:06:06,800
What I later learned was like sort of a
command prompt and my teacher would come

107
00:06:06,801 --> 00:06:09,200
over and type actually remember
this to this day for some reason,

108
00:06:09,410 --> 00:06:13,100
like PR number six or PR pound six,
which is peripheral six,

109
00:06:13,101 --> 00:06:15,650
which is the disk drive, which is
fire up the disk and load the program.

110
00:06:15,920 --> 00:06:18,770
And I just remember thinking, wow, she's
like a hacker. Like teach me these,

111
00:06:18,830 --> 00:06:21,950
these codes, these error codes. This
is what I called them at the time.

112
00:06:22,720 --> 00:06:23,740
But she had no interest in that.

113
00:06:23,750 --> 00:06:28,260
So it wasn't until I think about fifth
grade that I had a a a school where you

114
00:06:28,261 --> 00:06:30,600
could actually go on these
apple twos and learn to program.

115
00:06:30,600 --> 00:06:33,000
And so there's all in basic,
you know, where every line,

116
00:06:33,020 --> 00:06:34,500
you know the line numbers
are all numb or the,

117
00:06:34,530 --> 00:06:38,340
every line is numbered and you have
to like leave enough space between the

118
00:06:38,341 --> 00:06:41,760
numbers so that if you want to tweak
your code, you go back and it's,

119
00:06:41,761 --> 00:06:44,790
the first line was 10 in the second
line is 20 now you have to go back and

120
00:06:44,791 --> 00:06:48,240
insert 15 and if you need to add
code in front of that, you know,

121
00:06:48,241 --> 00:06:50,760
11 or 12 and you hope you don't run out
of line numbers and have to Redo the

122
00:06:50,761 --> 00:06:55,730
whole thing. There's go to statements.
Yeah. Goto and is very basic.

123
00:06:55,910 --> 00:07:00,800
Maybe hence the name. But
yeah, a lot of fun. And that
was like, that was, you know,

124
00:07:00,801 --> 00:07:03,500
that's when, that's when you know, when
you first program you see the magic of it.

125
00:07:03,530 --> 00:07:03,960
It's like,

126
00:07:03,960 --> 00:07:08,270
it just like this world opens up with
endless possibilities for the things you

127
00:07:08,271 --> 00:07:10,520
could build or,
or accomplish with that computer.

128
00:07:10,600 --> 00:07:11,800
So you got the bug then.

129
00:07:11,801 --> 00:07:16,690
So even started with basic and then what
c plus plus throughout, uh, what did you,

130
00:07:16,720 --> 00:07:19,720
was there a computer programming,
computer science classes in high school?

131
00:07:19,870 --> 00:07:24,080
Not, not where I went. So it
was self taught, but I did
a lot of programming. Right.

132
00:07:24,660 --> 00:07:27,280
The thing that
you know,

133
00:07:27,281 --> 00:07:30,700
sort of pushed me in the path of
eventually working on self driving cars is

134
00:07:30,701 --> 00:07:35,701
actually one of these really long trips
driving from my house in Kansas to uh,

135
00:07:36,500 --> 00:07:39,430
uh, to I think Las Vegas where we
did the battlebots competition.

136
00:07:39,460 --> 00:07:41,040
And I had just gotten my,

137
00:07:41,260 --> 00:07:44,770
I think my learner's permit
or early driver's permit.

138
00:07:45,040 --> 00:07:47,360
And so I was driving this,
you know,

139
00:07:47,361 --> 00:07:50,350
10 hour stretch across western
Kansas where it's just,

140
00:07:50,560 --> 00:07:53,290
you're going straight on a highway
and it is mind numbingly boring.

141
00:07:53,590 --> 00:07:56,680
And I remember thinking even then
with my sort of mediocre programming

142
00:07:57,560 --> 00:08:00,020
background that this is something
that a computer can do. Right?

143
00:08:00,021 --> 00:08:03,580
Let's take a picture of the road and
let's find the yellow lane markers and you

144
00:08:03,581 --> 00:08:07,340
know, steer the wheel. And you know, later
I'd come to realize this had been done,

145
00:08:07,580 --> 00:08:11,180
you know, since, since the eighties
or the seventies or even earlier,

146
00:08:11,620 --> 00:08:12,700
but I still wanted to do it.

147
00:08:12,730 --> 00:08:16,360
And sort of immediately after that trip
switched from sort of battlebots which

148
00:08:16,361 --> 00:08:20,820
is more radio controlled machines too.
Think about building,

149
00:08:21,370 --> 00:08:23,590
you know,
autonomous vehicles of some scale.

150
00:08:23,591 --> 00:08:26,790
Start off with really small
electric ones and then, you know,

151
00:08:26,791 --> 00:08:28,200
progressed to what we're doing now.

152
00:08:28,270 --> 00:08:30,830
So what was your view of artificial
intelligence at that point? What,

153
00:08:30,840 --> 00:08:32,980
what did you think?
So this is a,

154
00:08:33,040 --> 00:08:37,000
before there's been ways in artificial
intelligence, right? Uh, the,

155
00:08:37,010 --> 00:08:42,010
the current wave with deep learning makes
people believe that you can solve in a

156
00:08:42,041 --> 00:08:45,760
really rich deep way the computer
vision perception problem.

157
00:08:46,180 --> 00:08:50,560
But like in a,
before the deep learning craze,

158
00:08:51,340 --> 00:08:52,750
you know,
how do you think about,

159
00:08:52,780 --> 00:08:56,130
how would you even go about
building a thing that perceives

160
00:08:56,130 --> 00:08:59,070
itself in the world, localize itself
in the world, moves around the world.

161
00:08:59,160 --> 00:09:02,010
Like when you were younger,
what was your thinking about it?

162
00:09:02,100 --> 00:09:04,940
Well, prior to deep neural networks
are accomplished, all known it,

163
00:09:04,960 --> 00:09:08,760
New Orleans modern techniques we have
or at least ones that are in use today.

164
00:09:09,030 --> 00:09:13,530
It was all a heuristic space and so like
old school image processing and I think

165
00:09:13,531 --> 00:09:14,870
a extracting,
you know,

166
00:09:15,010 --> 00:09:20,010
yellow lane markers out of an image of
a road is one of the problems that lends

167
00:09:20,401 --> 00:09:23,490
itself reasonably well to
those heuristic based methods.

168
00:09:23,520 --> 00:09:27,870
You know like just do a threshold on the
color yellow and then try to fit some

169
00:09:27,871 --> 00:09:32,220
lines to that using a half transform
or something and then go from there.

170
00:09:32,280 --> 00:09:35,850
Traffic, light detection and then stop
signs detection, red, yellow, green.

171
00:09:35,910 --> 00:09:39,780
And I think you can, you could, I
mean if you wanted to do a full, it's,

172
00:09:39,840 --> 00:09:43,200
I was just trying to make something that
would stay in between the lanes on a

173
00:09:43,201 --> 00:09:44,790
highway.
But if you wanted to do the full,

174
00:09:46,920 --> 00:09:51,150
the full set of capabilities needed for
a driverless car, I think you could,

175
00:09:51,370 --> 00:09:53,580
and we'd done this at cruise.
You know,

176
00:09:53,590 --> 00:09:56,610
in the very first days you can start
off with a really simple, you know,

177
00:09:56,611 --> 00:10:00,540
human written heuristic just to get the
scaffolding in place for your system.

178
00:10:00,720 --> 00:10:02,900
Traffic, light detection, probably
a really simple, you know,

179
00:10:02,940 --> 00:10:06,990
color thresholding on day one just to
get the system up and running before you

180
00:10:06,991 --> 00:10:10,700
migrate to, you know, a deep learning
based technique or something else.

181
00:10:11,090 --> 00:10:13,330
And you know, back in, when I
was doing this, my first one,

182
00:10:13,331 --> 00:10:18,210
it was on a pentium 203 233
megahertz computer in it. And I,

183
00:10:18,211 --> 00:10:19,940
I think I wrote the
first version in basic,

184
00:10:19,941 --> 00:10:21,620
which is like an interpreted language.

185
00:10:21,621 --> 00:10:24,530
It's extremely slow cause that's
the thing I knew at the time.

186
00:10:24,800 --> 00:10:28,220
And so there was no, no chance
at all of using, there was no,

187
00:10:28,250 --> 00:10:33,240
no computational power to do any sort
of reasonable deep nets like you have

188
00:10:33,241 --> 00:10:36,270
today. So, I don't know what kids these
days are doing. Our kids these days,

189
00:10:36,300 --> 00:10:39,540
you know, at age 13, using neural
networks in their garage. I mean,

190
00:10:39,541 --> 00:10:40,374
that would be awesome.

191
00:10:40,640 --> 00:10:45,050
I get emails all the time from, you
know, like 11, 12 year olds saying,

192
00:10:45,051 --> 00:10:46,130
I'm having,
you know,

193
00:10:46,131 --> 00:10:50,060
I'm trying to follow this tends of flood
tutorial and I'm having this problem.

194
00:10:50,750 --> 00:10:55,750
And their general approach in the deep
learning community is of extreme optimism

195
00:10:57,861 --> 00:11:02,410
of, as opposed to, you mentioned
like heuristics. You can, you can,

196
00:11:02,450 --> 00:11:04,780
you can separate the
autonomous driving problem,

197
00:11:04,840 --> 00:11:07,310
two modules and try to
solve it sort of rigorously,

198
00:11:07,520 --> 00:11:08,810
or you can just do it end to end.

199
00:11:09,050 --> 00:11:12,020
And most people just kind of
love the idea that, you know,

200
00:11:12,021 --> 00:11:14,900
all us humans do it end to end.
We just perceive and act.

201
00:11:15,320 --> 00:11:16,930
We should be able to use that,
uh,

202
00:11:17,000 --> 00:11:20,660
do the same kind of thing when you're
on that and that, that kind of thinking,

203
00:11:20,900 --> 00:11:23,870
you don't want to criticize that kind of
thinking because eventually there will

204
00:11:23,871 --> 00:11:24,860
be, right. Yeah.

205
00:11:25,310 --> 00:11:29,260
And so it's exciting and especially when
they're younger to explore that as a

206
00:11:29,270 --> 00:11:34,240
really exciting approach. But
yeah, it's, it's changed. Uh, the,

207
00:11:34,300 --> 00:11:37,140
the language, the kind of
stuff you're tinkering with it.

208
00:11:37,220 --> 00:11:40,310
It's kind of exciting to see
when these teenagers grow up.

209
00:11:40,910 --> 00:11:42,290
Yeah.
I can only imagine if you,

210
00:11:42,291 --> 00:11:47,291
if your starting point is python and
tensor flow at age 13 where you end up,

211
00:11:47,540 --> 00:11:49,340
you know,
after 10 or 15 years of that,

212
00:11:50,120 --> 00:11:54,280
that's pretty cool because a get hub,
because the stair tools for solving

213
00:11:54,280 --> 00:11:58,480
most of the major problems in artificial
intelligence are within a few lines of

214
00:11:58,481 --> 00:11:59,920
code for most kids.

215
00:12:00,250 --> 00:12:04,570
And that's incredible to think about also
on the entrepreneurial side. And, and,

216
00:12:04,610 --> 00:12:05,590
and at that point,

217
00:12:05,980 --> 00:12:10,980
was there any thought about
entrepreneurship before
you came to college is sort

218
00:12:13,031 --> 00:12:16,930
of doing years of building this into a
thing that impacts the world on a large

219
00:12:16,931 --> 00:12:17,760
scale?

220
00:12:17,760 --> 00:12:21,060
Yeah, I've always wanted to start a
company. I think that's, you know,

221
00:12:21,510 --> 00:12:26,160
just a cool concept of creating
something and exchanging it for value or

222
00:12:26,210 --> 00:12:30,790
creating value I guess. So in high
school I was, I was trying to build like,

223
00:12:31,090 --> 00:12:32,820
you know,
a servo motor drivers,

224
00:12:32,830 --> 00:12:36,870
little circuit boards and sell them
online or are there other things like that

225
00:12:36,930 --> 00:12:40,240
and certainly knew at some
point I wanted to do a startup,

226
00:12:40,300 --> 00:12:43,570
but it wasn't really a, I'd say until
college, until I felt like I had the,

227
00:12:45,810 --> 00:12:47,500
I guess the right combination
of the environment,

228
00:12:47,501 --> 00:12:51,910
the smart people around you and uh, some
free time and a lot of free time at Mit.

229
00:12:52,330 --> 00:12:56,140
So you came to MIT as an
Undergrad, 2004. That's right.

230
00:12:57,050 --> 00:12:59,960
And that's when the first Darpa
grand challenge was happening. Yeah.

231
00:13:00,020 --> 00:13:03,290
The timing of that is
a beautifully poetic,

232
00:13:03,320 --> 00:13:05,420
so how'd you get yourself
involved in that one?

233
00:13:05,730 --> 00:13:09,480
Originally there wasn't a official
injury. Yeah. Faculty sponsored thing.

234
00:13:09,481 --> 00:13:11,940
And so a bunch of undergrads,
myself included,

235
00:13:12,790 --> 00:13:16,020
started meeting and got
together and tried to, to, uh,

236
00:13:16,170 --> 00:13:18,360
Hagel together some sponsorships.
We've got a vehicle,

237
00:13:18,361 --> 00:13:21,600
donated a bunch of sensors and
try to put something together.

238
00:13:21,601 --> 00:13:25,800
And so we had our team was probably
mostly freshmen and sophomores, you know,

239
00:13:25,801 --> 00:13:30,350
which, which was not really a fair, fair
fight against maybe the, uh, you know,

240
00:13:30,370 --> 00:13:33,850
postdoc and faculty led teams
from other schools. But we, uh,

241
00:13:33,910 --> 00:13:37,360
we got something up and running. We had
our vehicle drive by wire and you know,

242
00:13:37,390 --> 00:13:40,240
very, very basic control
and things. But, uh,

243
00:13:41,840 --> 00:13:46,450
on the day of the qualifying
prequalifying round,

244
00:13:46,790 --> 00:13:51,560
the one and only steering motor that we
had purchased, the thing that we had,

245
00:13:51,620 --> 00:13:55,690
you know, retrofitted to turn the
steering wheel on the truck died.

246
00:13:55,780 --> 00:13:58,420
And so our vehicle was just dead
in the water, couldn't steer.

247
00:13:58,480 --> 00:13:59,800
So we didn't make it very far

248
00:13:59,890 --> 00:14:03,640
and the hardware side. So was there
a software component? Was there,

249
00:14:04,300 --> 00:14:07,060
like how did your view of autonomous
vehicles in terms of artificial

250
00:14:07,061 --> 00:14:11,700
intelligence, right, evolve in
those moment? I mean, you know,

251
00:14:11,870 --> 00:14:14,080
like you said from the 80s
has been autonomous vehicles,

252
00:14:14,081 --> 00:14:17,140
but really that was the birth
of the modern wave, the,

253
00:14:17,450 --> 00:14:21,160
the thing that captivated everyone's
imagination that we can actually do this.

254
00:14:21,520 --> 00:14:26,260
So how were you captivated
in that way? We, so, well,

255
00:14:26,261 --> 00:14:28,420
how did your view of a
vehicles change at that point?

256
00:14:28,530 --> 00:14:30,900
Um, I'd say at that point
in time it was, it was a

257
00:14:32,870 --> 00:14:35,760
curiosity as in like,
is this really possible?

258
00:14:35,850 --> 00:14:40,850
And I think that was generally the spirit
and the purpose of that original Darpa

259
00:14:42,651 --> 00:14:43,281
grand challenge,

260
00:14:43,281 --> 00:14:48,281
which was to just get a whole bunch of
really brilliant people exploring this

261
00:14:48,381 --> 00:14:49,760
space and pushing the limits.

262
00:14:49,910 --> 00:14:54,910
And I think like to day that
Darpa challenge with it's
$1 million prize pool was

263
00:14:56,421 --> 00:15:00,830
probably one of the most effective,
you know, uses of taxpayer money,

264
00:15:00,860 --> 00:15:04,030
dollar for dollar that I've
seen, you know, because that,

265
00:15:04,360 --> 00:15:07,850
that small sort of initiative
that Darfa put, uh,

266
00:15:07,890 --> 00:15:12,890
put out sort of in my view was the
catalyst or the tipping point for this,

267
00:15:13,771 --> 00:15:17,100
this whole next wave of autonomous vehicle
development. So that was pretty cool.

268
00:15:17,180 --> 00:15:19,280
So let me jump around a
little bit on that point,

269
00:15:20,240 --> 00:15:23,240
they also did the urban challenge
where it was in the city,

270
00:15:23,241 --> 00:15:26,810
but it was very artificial and there's
no pedestrians and there's very little

271
00:15:26,811 --> 00:15:30,830
human involvement except the a
few professional drivers. Yeah.

272
00:15:31,640 --> 00:15:34,550
Do you think there's room and then
there was the robotics challenge with

273
00:15:34,551 --> 00:15:38,690
humanoid robots, right? In your
now role is looking at this,

274
00:15:38,720 --> 00:15:43,130
you're trying to solve one of the
autonomous driving, one of the harder,

275
00:15:43,131 --> 00:15:44,840
more difficult places,
San Francisco.

276
00:15:45,470 --> 00:15:50,450
Is there a role for Darpa to step in to
also kind of help out big challenge with

277
00:15:50,451 --> 00:15:54,440
new ideas,
specifically a pedestrians and so on,

278
00:15:54,441 --> 00:15:55,670
all these kinds of interesting things?

279
00:15:55,860 --> 00:15:57,660
Well I haven't, I haven't thought
about it from that perspective.

280
00:15:57,661 --> 00:16:00,660
Is there anything Darpa could do
today to further accelerate things?

281
00:16:00,661 --> 00:16:05,661
And I would say my instinct is that that's
maybe not the highest and best use of

282
00:16:05,821 --> 00:16:10,600
their resources and time because like
kickstarting and spinning up the flywheel

283
00:16:10,620 --> 00:16:13,890
is I think what, what they did in this
case for a very, very little money.

284
00:16:14,160 --> 00:16:15,660
But today this has become,

285
00:16:16,820 --> 00:16:20,120
this has become like commercially
interesting to very large companies in the

286
00:16:20,121 --> 00:16:21,380
amount of money going into it.

287
00:16:21,381 --> 00:16:25,070
And the amount of people like you going
through your class and learning about

288
00:16:25,071 --> 00:16:28,030
these things and developing
his skills is just, you know,

289
00:16:28,100 --> 00:16:30,620
orders of magnitude more
than it was back then.

290
00:16:30,860 --> 00:16:35,510
And so there's enough momentum and
inertia and energy and investment dollars

291
00:16:35,511 --> 00:16:39,650
into this space right now that, uh, I
don't, I don't, um, I think they're,

292
00:16:39,980 --> 00:16:43,160
I think they can just say mission
accomplished and move on to the next.

293
00:16:43,180 --> 00:16:45,140
That's an area of technology that,
that needs help.

294
00:16:46,190 --> 00:16:50,750
So then stepping back to MIT, he
left them it junior, junior year.

295
00:16:50,810 --> 00:16:52,610
What was that decision like?

296
00:16:52,990 --> 00:16:55,640
As I said,
I always wanted to do a company in a,

297
00:16:55,641 --> 00:16:58,990
or start a company and this
opportunity landed in my lap,

298
00:16:58,991 --> 00:17:01,760
which was a couple guys from Yale,
a,

299
00:17:01,870 --> 00:17:05,650
we're starting a new company and I googled
them and found that they had started

300
00:17:05,651 --> 00:17:10,540
a company previously and sold it actually
on Ebay for about a quarter million

301
00:17:10,540 --> 00:17:14,200
bucks, which was pretty interesting
story. But, so I thought to myself,

302
00:17:14,201 --> 00:17:18,100
these guys are, you know, rock star
entrepreneurs, they've done this before.

303
00:17:19,050 --> 00:17:24,000
They must be driving around in Ferrari's
cause they sold their company. And uh,

304
00:17:24,001 --> 00:17:25,920
you know,
I thought I could learn a lot from him.

305
00:17:25,921 --> 00:17:29,280
So I teamed up with those guys
and you know, went out during,

306
00:17:30,480 --> 00:17:34,200
went out to California during IAP,
which is MIT's a month off,

307
00:17:35,030 --> 00:17:37,590
a on one on one way ticket
and basically never went back.

308
00:17:38,010 --> 00:17:39,210
We were having so much fun.

309
00:17:39,240 --> 00:17:42,440
We felt like we were building something
and creating something and it was going

310
00:17:42,441 --> 00:17:45,350
to be interesting. You
know, I was just all in and,

311
00:17:45,370 --> 00:17:49,620
and got completely hooked and
that that business was Justin TV,

312
00:17:49,621 --> 00:17:52,320
which is originally a reality
show about a guy named Justin,

313
00:17:53,720 --> 00:17:57,080
which morphed into a live
video streaming platform,

314
00:17:57,110 --> 00:18:00,830
which then morphed into what
is twitch today. So that was,

315
00:18:00,890 --> 00:18:05,300
that was quite a and unexpected journey.
So

316
00:18:05,430 --> 00:18:10,380
no regrets? No. Looking back it
was just an obvious one way ticket.

317
00:18:10,650 --> 00:18:13,680
I mean if we just pause on that
for a second. There was no,

318
00:18:15,360 --> 00:18:18,840
how did you know these were the right
guys, this is the right decision.

319
00:18:19,440 --> 00:18:22,470
You didn't think it was just
follow the heart kind of thing.

320
00:18:22,550 --> 00:18:24,410
Well I didn't know.
But you know,

321
00:18:24,411 --> 00:18:27,960
just trying something for a month during
IAP seems pretty low risk. Right, right.

322
00:18:28,190 --> 00:18:30,710
And then, you know, well maybe
I'll take a semester off.

323
00:18:30,711 --> 00:18:33,410
Mit's pretty flexible about that.
You can always go back. Right.

324
00:18:33,800 --> 00:18:37,280
And then after two or three
cycles of that, I eventually
threw in the towel. But,

325
00:18:37,281 --> 00:18:38,900
uh, you know, I think it's, um,

326
00:18:41,070 --> 00:18:44,580
I guess in that case I felt like I could
always hit the undo button if I had to.

327
00:18:44,840 --> 00:18:49,610
Right. But it nevertheless, from
a, when you look in retrospect,

328
00:18:49,611 --> 00:18:52,580
I mean it seems like a brave decision
that, you know, as difficult,

329
00:18:52,581 --> 00:18:55,430
it would be difficult for a lot of
people to make it, it wasn't as popular.

330
00:18:55,431 --> 00:18:57,140
I'd say that the general,

331
00:18:57,860 --> 00:18:58,101
you know,

332
00:18:58,101 --> 00:19:03,101
flux of people out of MIT at the time
it was mostly into finance or consulting

333
00:19:03,771 --> 00:19:05,210
jobs in Boston or New York.

334
00:19:05,690 --> 00:19:09,020
And very few people were going
to California to start companies.

335
00:19:09,021 --> 00:19:11,210
But today I'd say that's
probably inverted,

336
00:19:12,180 --> 00:19:15,240
which is just a sign of a
sign of the times, I guess.

337
00:19:15,350 --> 00:19:15,980
Yep.

338
00:19:15,980 --> 00:19:20,980
So there's a story about
midnight of March 18 2007 where,

339
00:19:21,861 --> 00:19:23,300
uh,
whether we're a tech crunch,

340
00:19:23,301 --> 00:19:27,140
I guess announced Justin TV earlier
than it was supposed to, a few hours.

341
00:19:29,000 --> 00:19:31,430
The site didn't work.
I don't know if any of this is true.

342
00:19:31,431 --> 00:19:36,431
You can tell me and a u and w w one of
the folks adjusted t Emmett shear coded

343
00:19:37,611 --> 00:19:42,200
through the night. Can you take me
through that experience? So let me,

344
00:19:42,230 --> 00:19:46,140
let me say a few nice things
that, uh, the art card,

345
00:19:46,150 --> 00:19:49,730
red quoted a Justin Kahn said that you
were known for bureau coding through

346
00:19:49,731 --> 00:19:53,000
problems and being a creative quote,
creative genius.

347
00:19:53,450 --> 00:19:57,230
So a on that night, what, what,

348
00:19:57,240 --> 00:20:00,750
what was going through your
head or maybe put another way,

349
00:20:00,751 --> 00:20:02,430
how do you solve these problems?

350
00:20:02,460 --> 00:20:06,210
What's your approach to solving these
kinds of problems with the line between

351
00:20:06,211 --> 00:20:09,060
success and failure?
It seems to be pretty thin.

352
00:20:09,940 --> 00:20:11,560
That's a good question.
Well, first of all, that's,

353
00:20:11,561 --> 00:20:16,561
that's a nice of Justin to say that I
think I would have been maybe 21 years old

354
00:20:16,691 --> 00:20:21,460
then and not very experienced at
programming. But as with, um, uh,

355
00:20:21,490 --> 00:20:24,640
with everything in a startup, you're,
you're sort of racing against the clock.

356
00:20:24,700 --> 00:20:29,700
And so our plan was the second we had
this live streaming camera backpack up and

357
00:20:32,171 --> 00:20:35,320
running, we're Justin could wear it and
no matter where he went in the city,

358
00:20:35,321 --> 00:20:37,930
it would be streaming live video.
And this is even before the iPhones,

359
00:20:37,950 --> 00:20:41,740
this is like hard to do back then
we would launch.

360
00:20:41,800 --> 00:20:43,720
And so we thought we were there and,

361
00:20:43,750 --> 00:20:47,710
and the backpack was working and then
we send out all the emails, launch the,

362
00:20:47,800 --> 00:20:51,160
launch the company and do the
press thing. And then, you know,

363
00:20:51,250 --> 00:20:55,750
we weren't quite actually there. And
then, um, we thought, oh well, you know,

364
00:20:55,751 --> 00:21:00,680
they're not going to announce it until
maybe 10:00 AM the next morning and it's,

365
00:21:00,740 --> 00:21:03,640
I don't know, it's 5:00 PM now.
So how many hours do we have left?

366
00:21:03,650 --> 00:21:06,780
What is that like, you know,
17 hours to go. And uh,

367
00:21:07,940 --> 00:21:10,370
and that was,
that was going to be fine

368
00:21:10,430 --> 00:21:13,610
was a problem I have is did you
understand what could possibly be like how

369
00:21:13,611 --> 00:21:15,170
complicated was the system at that point?

370
00:21:16,490 --> 00:21:18,830
It was,
it was pretty messy.

371
00:21:18,831 --> 00:21:23,831
So to get a live video feed that looked
decent working from anywhere in San

372
00:21:24,110 --> 00:21:24,943
Francisco,

373
00:21:25,090 --> 00:21:29,290
I put together this system where we
had like three or four cell phone data

374
00:21:29,291 --> 00:21:33,070
modems and they were like,
we take the video stream and s you know,

375
00:21:33,071 --> 00:21:36,550
sort of sprayed across these three or
four modems and then try to catch all the

376
00:21:36,551 --> 00:21:39,450
packets on the other side, you know,
with unreliable cell phone networks.

377
00:21:39,460 --> 00:21:43,540
Pretty low level networking. Yeah.
And putting these like, you know,

378
00:21:43,541 --> 00:21:47,530
sort of protocols on top of all that to
to reassemble and reorder the packets

379
00:21:47,531 --> 00:21:51,670
and have time buffers and error correction
and all that kind of stuff. And um,

380
00:21:52,580 --> 00:21:54,450
the, the night before
it was just static. He,

381
00:21:54,460 --> 00:21:55,900
every once in a while the image would,

382
00:21:55,901 --> 00:22:00,310
would go staticky and there would be
this horrible like screeching audio noise

383
00:22:00,311 --> 00:22:04,000
because the audio was also corrupted and
this would happen like every five to 10

384
00:22:04,001 --> 00:22:07,870
minutes or so. And it was a
really off putting to the viewers.

385
00:22:08,860 --> 00:22:11,230
How do you tackle that
problem? What was the, uh,

386
00:22:11,410 --> 00:22:14,230
he just freaking out behind a computer.
There's the word.

387
00:22:14,231 --> 00:22:18,100
Are there other other folks working on
this problem that we behind a white board

388
00:22:18,101 --> 00:22:20,380
where you doing? Uh, uh,

389
00:22:20,540 --> 00:22:21,520
there's a,
there's a little hill,

390
00:22:22,000 --> 00:22:24,990
he has a little lonely because there's
four of us working on the company and I

391
00:22:25,000 --> 00:22:28,810
only two people really wrote code and
Emmett wrote the website in the chat

392
00:22:28,811 --> 00:22:33,190
system and I wrote the software for this
video streaming device in video server.

393
00:22:34,220 --> 00:22:37,260
And so I, you know, it was my sole
responsibility to figure that out. Yeah.

394
00:22:37,280 --> 00:22:41,150
And I think, I think it's those, you
know, setting, setting deadlines,

395
00:22:41,151 --> 00:22:43,430
trying to move quickly and everything
were where you're in that moment of

396
00:22:43,431 --> 00:22:46,910
intense pressure that sometimes people
do their best and most interesting work.

397
00:22:46,911 --> 00:22:48,770
And so even though that
was a terrible moment,

398
00:22:48,771 --> 00:22:50,700
I look back on it fondly
cause that's like, you know,

399
00:22:50,720 --> 00:22:52,940
that's one of those character
defining moments. I think.

400
00:22:54,690 --> 00:22:59,280
So in a 2013 October you
founded cruise automation.

401
00:22:59,400 --> 00:23:02,290
Yeah. So progressing forward, uh,

402
00:23:02,380 --> 00:23:07,380
another exception successful company was
acquired by GM and 16 for $1 billion.

403
00:23:09,870 --> 00:23:14,100
But uh, in October of 2013,
what was on your mind?

404
00:23:14,101 --> 00:23:15,030
What was the plan?

405
00:23:16,140 --> 00:23:21,140
How does one seriously start to
tackle one of the hardest robotics,

406
00:23:21,750 --> 00:23:24,180
most important,
impactful robotics problems of our age

407
00:23:24,890 --> 00:23:28,100
after going through
twitch? Twitch was, was,

408
00:23:28,730 --> 00:23:33,350
and is today pretty successful.
But the um,

409
00:23:33,950 --> 00:23:37,310
the work was, the result was
entertainment mostly like the,

410
00:23:37,350 --> 00:23:41,140
the better the product was, the more we
would entertain people and then, you know,

411
00:23:41,450 --> 00:23:44,270
make money on them ad revenues
and other things. And that was,

412
00:23:44,400 --> 00:23:47,540
it was a good thing. It felt, it felt good
to entertain people, but I figured like,

413
00:23:47,720 --> 00:23:47,961
you know,

414
00:23:47,961 --> 00:23:51,780
what is really the point of becoming a
really good engineer and developing the

415
00:23:51,781 --> 00:23:53,900
skills other than, you
know, my own enjoyment.

416
00:23:54,200 --> 00:23:57,140
And I realized I wanted something that
scratched more of an existential itch,

417
00:23:57,170 --> 00:23:58,970
like something that truly matters.

418
00:23:59,240 --> 00:24:04,240
And so I basically made this
list of requirements for a new,

419
00:24:04,730 --> 00:24:07,760
if I was going to do another company and
the one thing I knew in the back of my

420
00:24:07,761 --> 00:24:11,480
head that twitch took like eight
years to become successful.

421
00:24:12,320 --> 00:24:14,980
And so whatever I do, I better
be willing to commit, you know,

422
00:24:14,990 --> 00:24:16,970
at least 10 years to something.

423
00:24:16,971 --> 00:24:18,980
And when you think about
things from that perspective,

424
00:24:20,380 --> 00:24:23,590
you certainly I think raise the bar on
what you choose to work on. So for me,

425
00:24:23,591 --> 00:24:27,130
the three things where it had to be
something where the technology itself

426
00:24:27,131 --> 00:24:29,620
determines the success of the product,
like hard,

427
00:24:29,680 --> 00:24:33,220
really juicy technology problems
because that's what motivates me.

428
00:24:33,580 --> 00:24:37,480
And then it had to have a direct and
positive impact on society in some way.

429
00:24:37,630 --> 00:24:39,170
So an example would be like you know,

430
00:24:39,290 --> 00:24:41,560
healthcare self driving
cars cause they save lives.

431
00:24:41,561 --> 00:24:44,080
Other things where there's a clear
connection to somehow improving other

432
00:24:44,081 --> 00:24:44,914
people's lives.

433
00:24:45,190 --> 00:24:49,780
And the last one is it had to be a big
business because for the positive impact

434
00:24:49,781 --> 00:24:51,640
to matter,
it's got to be a large scale scale.

435
00:24:51,940 --> 00:24:54,430
And as thinking about that
for a while and I made like a,

436
00:24:54,431 --> 00:24:58,510
I tried writing at Gmail clone and looked
at some other ideas and then it just

437
00:24:58,511 --> 00:25:00,400
sort of light bulb went
off like self driving cars.

438
00:25:00,400 --> 00:25:03,700
Like that was the most fun I'd ever
had in college. Working on that.

439
00:25:04,000 --> 00:25:07,270
And like well what's the state of the
technology has been 10 years, maybe,

440
00:25:07,300 --> 00:25:10,270
maybe times have changed. And maybe
now is the time to make this work.

441
00:25:10,750 --> 00:25:12,580
And I poked around and looked at,

442
00:25:12,670 --> 00:25:15,460
the only other thing out there really
at the time was the Google self driving

443
00:25:15,461 --> 00:25:19,570
car project. And I thought surely
there's a way to, you know,

444
00:25:19,580 --> 00:25:22,690
have an entrepreneurial mindset and
sort of solve the minimum viable product

445
00:25:22,691 --> 00:25:25,750
here. And so I, I just took the plunge
right then and there and said this,

446
00:25:25,751 --> 00:25:27,850
this is something I know
I can commit 10 years too.

447
00:25:27,851 --> 00:25:32,380
It's the probably the greatest applied
AI problem of our generation and if it

448
00:25:32,381 --> 00:25:36,340
works, it's going to be both
a huge business and therefore
like probably the most

449
00:25:36,341 --> 00:25:38,230
positive impact I can
possibly have on the world.

450
00:25:38,260 --> 00:25:40,880
So after that light bulb went off,

451
00:25:40,910 --> 00:25:45,080
I went all in on cruise
immediately and got to work.

452
00:25:45,500 --> 00:25:47,340
Did you have an idea how
to solve this problem?

453
00:25:47,341 --> 00:25:49,820
Which aspects of the problem to solve?
You know,

454
00:25:49,830 --> 00:25:53,650
s slowly we just had to
Oliver for voyage here,

455
00:25:53,670 --> 00:25:58,040
slow moving a retirement communities,
uh, urban driving highway driving.

456
00:25:58,041 --> 00:25:58,874
Did you have,

457
00:25:59,510 --> 00:26:03,410
did you have a vision of the city
of the future or you know, uh,

458
00:26:03,500 --> 00:26:07,100
the transportation is largely automated,
that kind of thing?

459
00:26:07,130 --> 00:26:12,110
Or was it sort of more fuzzy
and gray area than that?

460
00:26:12,280 --> 00:26:16,630
My analysis of the situation is
that Google is putting a lot,

461
00:26:16,660 --> 00:26:19,230
it had been putting a lot
of money into that project,

462
00:26:19,250 --> 00:26:21,520
had a lot more resources and so,

463
00:26:22,610 --> 00:26:27,430
and they still hadn't cracked the fully
driverless car. You know, this is 20, 20,

464
00:26:27,431 --> 00:26:31,010
uh, 13 I guess. So I thought, what,

465
00:26:31,011 --> 00:26:34,680
what can I do to sort of go from zero to,
you know,

466
00:26:34,681 --> 00:26:37,320
significant scale so I can
actually solve the real problem,

467
00:26:37,321 --> 00:26:40,530
which is the driverless cars. And
I thought, here's the strategy.

468
00:26:40,531 --> 00:26:45,531
We'll start by doing a really simple
or solving a really simple problem that

469
00:26:47,030 --> 00:26:47,970
creates value for people.

470
00:26:48,090 --> 00:26:51,480
So eventually ended up deciding
on automating highway driving,

471
00:26:51,810 --> 00:26:54,140
which is relatively more straightforward.

472
00:26:54,270 --> 00:26:58,530
As long as there's a backup driver there
and I'll, you know, the go to market,

473
00:26:58,531 --> 00:27:01,800
we'll be able to retrofit people's cars
and just sell these products directly.

474
00:27:02,280 --> 00:27:06,870
And the idea was we'll take
all the revenue and profits
from that and use it to

475
00:27:06,871 --> 00:27:11,190
do the assessor will reinvest
that in research for doing fully,

476
00:27:11,200 --> 00:27:13,950
fully driverless cars.
And that was the plan.

477
00:27:13,980 --> 00:27:17,520
The only thing that really changed
along the way between then and now is we

478
00:27:17,521 --> 00:27:18,990
never really launched the first product.

479
00:27:19,020 --> 00:27:23,520
We had enough interest from investors
in enough of a signal that this was

480
00:27:23,790 --> 00:27:25,680
something that we should
be working on that, um,

481
00:27:25,740 --> 00:27:29,280
after about a year of working on the
highway autopilot, we had it working,

482
00:27:29,310 --> 00:27:33,420
you know, on a prototype stage, but we
just completely abandoned that and said,

483
00:27:33,421 --> 00:27:36,430
we're going to go all in on
driverless cars. Now is the time. Um,

484
00:27:36,510 --> 00:27:39,750
can't think of anything that's more
exciting and if it works more impactful,

485
00:27:39,751 --> 00:27:43,320
so we're just going to go for it. The
idea of retrofit is kind of interesting.

486
00:27:43,470 --> 00:27:48,020
Yeah. A of being able to,
it's how you choose scale.
It's really interesting ideas.

487
00:27:48,030 --> 00:27:53,030
It's something that's still in the back
of your mind as a possibility and not at

488
00:27:53,191 --> 00:27:53,370
all.

489
00:27:53,370 --> 00:27:58,370
I've come full circle on that one after
trying to build a retrofit product and

490
00:27:59,340 --> 00:28:03,030
I'll touch on some of the complexities
of that and then also having been inside

491
00:28:03,750 --> 00:28:07,560
an OEM and seeing how things work and
how a vehicle is developed and validated.

492
00:28:08,310 --> 00:28:10,260
When it comes to
something that has safety,

493
00:28:10,261 --> 00:28:13,120
critical implications like
controlling the steering and you know,

494
00:28:13,270 --> 00:28:14,610
other controlling puts on your car,

495
00:28:15,270 --> 00:28:19,260
it's pretty hard to get there with
with a retrofit or, or if you did,

496
00:28:19,890 --> 00:28:20,611
even if you did it,

497
00:28:20,611 --> 00:28:24,390
it creates a whole bunch
of new complications around
liability or how did you

498
00:28:24,391 --> 00:28:26,040
truly validate that or you know,

499
00:28:26,041 --> 00:28:28,640
something in the base vehicle fails
and causes your system to fail.

500
00:28:28,650 --> 00:28:29,483
Who's fault is it?

501
00:28:31,560 --> 00:28:35,790
Or if the car's antilock brake systems
or other things kick in or the software

502
00:28:35,791 --> 00:28:36,624
has been,

503
00:28:36,690 --> 00:28:39,600
it's different in one version of the
car you retrofit versus another and you

504
00:28:39,601 --> 00:28:42,630
don't know because the manufacturer
has updated it behind the scenes.

505
00:28:42,990 --> 00:28:46,500
There's basically an infinite list of
long tail issues that can get you and if

506
00:28:46,501 --> 00:28:48,690
you're dealing with the safety critical
product, that's not really acceptable.

507
00:28:48,930 --> 00:28:53,100
That's a really convincing summary
of why this really challenging,

508
00:28:53,160 --> 00:28:55,290
but I didn't know that at the time.
So we tried it anyway.

509
00:28:55,440 --> 00:28:58,200
But as a pitch also at the
time it's a really strong one.

510
00:28:58,260 --> 00:29:01,580
This is how you achieve scale and
that's how you beat the current, the,

511
00:29:01,581 --> 00:29:04,590
the leader at the time of Google
or the only one in the market.

512
00:29:04,710 --> 00:29:06,840
The other big problem we ran into,

513
00:29:06,870 --> 00:29:11,100
which is perhaps the biggest problem
from a business model perspective is um,

514
00:29:11,430 --> 00:29:15,990
we had kind of assumed that we started
with an Audi s four as the vehicle.

515
00:29:15,990 --> 00:29:20,040
We retrofitted with this highway driving
capability and we had kind of assumed

516
00:29:20,041 --> 00:29:24,030
that if we just knock out like three
make and models of vehicle that'll cover

517
00:29:24,031 --> 00:29:27,400
like 80% of the San Francisco market
doesn't everyone, their drive, I dunno,

518
00:29:27,420 --> 00:29:29,880
a BMW or Honda civic or
one of these three cars.

519
00:29:30,300 --> 00:29:33,390
And then we surveyed our users when we
found out that it's all over the place,

520
00:29:33,450 --> 00:29:37,830
we would get even a decent number of
units sold. We'd have to support like,

521
00:29:37,831 --> 00:29:38,041
you know,

522
00:29:38,041 --> 00:29:42,250
20 or 50 different models and each one
is a little butterfly that takes time and

523
00:29:42,251 --> 00:29:43,330
effort to maintain,
you know,

524
00:29:43,420 --> 00:29:46,240
that retrofit integration and
custom hardware and all this.

525
00:29:47,120 --> 00:29:48,470
So is it is a tough business.

526
00:29:49,220 --> 00:29:54,220
So GM manufactures and sells
over 9 million cars a year
and what you with crews

527
00:29:56,900 --> 00:30:01,900
are trying to do some of the most cutting
edge innovation in terms of applying

528
00:30:02,001 --> 00:30:06,440
AI and says how do those,
we've talked about it a little bit before,

529
00:30:06,441 --> 00:30:10,490
but it's also just fascinating to me. We
work a lot of automakers, uh, you know,

530
00:30:10,520 --> 00:30:14,630
the difference between the gap between
Detroit and Silicon Valley, let's say,

531
00:30:14,660 --> 00:30:18,650
just to be sort of poetic about it,
I guess. How do you close that gap?

532
00:30:18,680 --> 00:30:23,180
How do you take GM into the future
where a large part of the fleet will be

533
00:30:23,181 --> 00:30:24,380
autonomous?
Perhaps?

534
00:30:24,820 --> 00:30:29,380
I want to start by acknowledging that
that GM is made up of tens of thousands of

535
00:30:29,381 --> 00:30:33,240
really brilliant, motivated people who
want to be a part of the future. And uh,

536
00:30:33,280 --> 00:30:35,230
so it's,
it's pretty fun to work with them.

537
00:30:35,231 --> 00:30:38,900
The attitude inside a car company
like that is, you know, uh,

538
00:30:38,950 --> 00:30:42,250
embracing this transformation and
change rather than fearing it.

539
00:30:42,310 --> 00:30:46,060
And I think that's a testament to the
leadership at GM and that's fallen all the

540
00:30:46,061 --> 00:30:46,894
way through to,

541
00:30:46,940 --> 00:30:49,780
to everyone you talk to even the people
in the assembly plants working on these

542
00:30:49,781 --> 00:30:51,790
cars. Right? So that's really great.

543
00:30:51,791 --> 00:30:54,940
So that starting from that
position makes it a lot easier.

544
00:30:55,150 --> 00:30:57,730
So then when the,

545
00:30:57,740 --> 00:31:01,350
the people in San Francisco at cruise
interact with the people at GM,

546
00:31:01,351 --> 00:31:02,910
at least we have this
common set of values,

547
00:31:02,911 --> 00:31:05,960
which is that we really want this stuff
to work because we think it's important

548
00:31:05,990 --> 00:31:09,580
and we think it's the future.
That's not to say, you know,

549
00:31:10,030 --> 00:31:13,000
those two cultures don't clash. They
absolutely do. There's different,

550
00:31:13,090 --> 00:31:16,180
different sort of value systems.
Like in a car company,

551
00:31:16,760 --> 00:31:18,200
the thing that gets you promoted and,

552
00:31:18,230 --> 00:31:22,510
and sort of the reward system
is following the processes,

553
00:31:22,511 --> 00:31:25,630
delivering the, the, the
program on time and on budget.

554
00:31:25,990 --> 00:31:30,990
So any sort of risk taking is discouraged
in many ways because if a program is

555
00:31:33,691 --> 00:31:36,030
late or if you shut down the
plant for a day, it's, you know,

556
00:31:36,031 --> 00:31:39,120
you can count the millions of dollars
that burned by pretty quickly.

557
00:31:39,540 --> 00:31:44,450
Whereas I think in, uh, uh, most
silicon valley companies and, and in,

558
00:31:44,500 --> 00:31:48,090
in, in cruise and the
methodology we were employing,

559
00:31:48,150 --> 00:31:49,680
especially around the
time of the acquisition,

560
00:31:50,040 --> 00:31:55,040
the reward structure is about trying to
solve these complex problems in any way,

561
00:31:55,321 --> 00:31:57,990
shape or form or coming up with
crazy ideas that, you know,

562
00:31:57,991 --> 00:32:01,230
90% of them won't work. And, uh, and so,

563
00:32:01,260 --> 00:32:05,220
so meshing that culture of sort of
continuous improvement and experimentation

564
00:32:05,430 --> 00:32:07,230
with one where everything needs to be,
you know,

565
00:32:07,320 --> 00:32:10,120
rigorously defined up front
so that you never slip a,

566
00:32:10,121 --> 00:32:15,121
a deadline or miss a budget was a
pretty big challenge in that we're,

567
00:32:15,370 --> 00:32:19,140
we're over three years in now, uh,
after the acquisition. And I'd say like,

568
00:32:19,230 --> 00:32:19,470
you know,

569
00:32:19,470 --> 00:32:23,740
the investment we made in figuring out
how to work together successfully and who

570
00:32:23,741 --> 00:32:27,640
should do what and a how we bridge the
gaps between these very different systems

571
00:32:27,641 --> 00:32:31,060
and way of doing engineering work is
now one of our greatest assets because I

572
00:32:31,061 --> 00:32:32,290
think we have this really powerful thing.

573
00:32:32,291 --> 00:32:35,680
But for awhile it was
both boogeyman crews were,

574
00:32:35,681 --> 00:32:37,090
were very steep on the learning curve.

575
00:32:37,390 --> 00:32:40,490
Yes. I'm sure it was very stressful.
It's really important work cause that's,

576
00:32:40,491 --> 00:32:43,490
that's how to revolutionize
the transportation.

577
00:32:43,520 --> 00:32:46,820
It really to revolutionize any system.
You know,

578
00:32:46,821 --> 00:32:49,670
you look at the healthcare system
or you look at the legal system.

579
00:32:49,671 --> 00:32:52,010
I have people like Laura has
come up to me all the time,

580
00:32:52,011 --> 00:32:55,520
like everything they're working on,
it can easily be automated,

581
00:32:55,910 --> 00:32:58,760
but then that's not a good feeling.
Yeah. That was not a good feeling.

582
00:32:58,761 --> 00:33:02,360
But also there is no way to
automate because the, the,

583
00:33:02,630 --> 00:33:05,480
the entire infrastructure
is really, uh, you know,

584
00:33:05,481 --> 00:33:08,630
based is older and it
moves very slowly and so,

585
00:33:08,631 --> 00:33:13,370
so how do you close the gap between
I haven't, how can I replace?

586
00:33:13,850 --> 00:33:15,650
Of course lawyers don't want
to be replaced with an APP,

587
00:33:15,680 --> 00:33:19,940
but you could replace a lot of aspect
when most of the data's still on paper.

588
00:33:20,180 --> 00:33:23,690
And so the same thing was with automotive.
I mean,

589
00:33:23,691 --> 00:33:25,370
it's fundamentally software.

590
00:33:26,040 --> 00:33:30,530
So it's basically hiring
software engineers is thinking
a software world. I mean,

591
00:33:30,531 --> 00:33:35,320
I'm pretty sure nobody in silicon valley,
he's ever hit a deadline. And then, uh,

592
00:33:35,321 --> 00:33:39,170
on, on Janice, probably true. Yeah.
And Geoscience probably the opposite.

593
00:33:39,850 --> 00:33:42,740
So that's,
that culture gap is really fascinating.

594
00:33:42,741 --> 00:33:45,080
So you're optimistic
about the future of that?

595
00:33:45,140 --> 00:33:48,320
Yeah, I mean, from what I've seen,
uh, it's impressive. And I think like,

596
00:33:48,440 --> 00:33:51,860
especially in Silicon Valley, it's easy
to write off building cars because,

597
00:33:51,890 --> 00:33:52,340
you know,

598
00:33:52,340 --> 00:33:54,890
people had been doing that for over
a hundred years now in this country.

599
00:33:54,891 --> 00:33:57,020
And so it seems like
that's a solved problem,

600
00:33:57,021 --> 00:33:59,440
but that doesn't mean it's
an easy problem. And, uh,

601
00:33:59,740 --> 00:34:03,730
I think it would be easy to sort of
overlook that and think that, you know,

602
00:34:04,590 --> 00:34:08,930
where Silicon Valley engineers,
we can solve any problem,
you know, building a car,

603
00:34:08,931 --> 00:34:12,080
it's been done, therefore it's,
you know, it's, it's, it's not a,

604
00:34:12,110 --> 00:34:13,640
it's not a real engineering challenge.

605
00:34:14,560 --> 00:34:19,250
But after having seen just the sheer
scale and magnitude and industrialization

606
00:34:20,830 --> 00:34:22,960
that occurs inside of an
automotive assembly plant,

607
00:34:23,230 --> 00:34:28,000
that is a lot of work that I am very
glad that we don't have to reinvent, um,

608
00:34:28,150 --> 00:34:29,350
to make self driving cars work.

609
00:34:29,440 --> 00:34:31,930
And so to have partners who've
done that for a hundred years now,

610
00:34:31,950 --> 00:34:36,100
these great processes and this huge
infrastructure and supply base that we can

611
00:34:36,101 --> 00:34:40,400
tap into is just remarkable
because the scope

612
00:34:42,320 --> 00:34:43,390
and surface area of,

613
00:34:43,450 --> 00:34:47,630
of the problem of deploying fleets of
self driving cars is so large that we're

614
00:34:47,631 --> 00:34:51,890
constantly looking for ways to do less
so we can focus on the things that really

615
00:34:51,891 --> 00:34:55,540
matter more. And if we had to figure
out how to build and assemble and

616
00:34:58,170 --> 00:35:01,570
yeah, build the cars themselves. I
mean we work closely with Jim on that,

617
00:35:01,571 --> 00:35:05,360
but if we had to develop all that
capability in house as well, you know,

618
00:35:05,361 --> 00:35:10,340
that that would just make, make the
problem really intractable. I think so,

619
00:35:10,341 --> 00:35:12,110
yeah.
Just like your first

620
00:35:13,310 --> 00:35:17,390
and tree, the Mit Darpa challenge when
there was, what the motor that failed,

621
00:35:17,710 --> 00:35:19,910
somebody that knows what they're
doing with a motor did it,

622
00:35:20,000 --> 00:35:22,040
I would have been nice if we
can get focus on the software,

623
00:35:22,580 --> 00:35:26,900
not the hardware platform. Yeah. Right.
So, uh, from your perspective now,

624
00:35:27,840 --> 00:35:28,070
you know,

625
00:35:28,070 --> 00:35:32,540
there's so many ways that autonomous
vehicles can impact society in the next

626
00:35:32,540 --> 00:35:33,920
year, five years, 10 years.

627
00:35:34,220 --> 00:35:39,180
What do you think is the
biggest opportunity to make
money in autonomous driving?

628
00:35:40,050 --> 00:35:44,460
So sort of make it a financially
viable thing in the near term.

629
00:35:44,670 --> 00:35:48,190
What do you think would be
the biggest impact there?

630
00:35:49,060 --> 00:35:50,470
Well,
the things that,

631
00:35:50,530 --> 00:35:54,790
that drive the economics for fleets of
self driving cars are, there's sort of a,

632
00:35:54,940 --> 00:35:58,100
a handful of variables. One is, you know,

633
00:35:58,101 --> 00:36:02,270
the cost to build the vehicle itself. So
the material costs, how many, you know,

634
00:36:02,450 --> 00:36:05,450
what's the cost of all your sensors?
Plus the cost of the vehicle and every,

635
00:36:05,451 --> 00:36:09,320
all the other components on it. Another
one is the lifetime of the vehicle.

636
00:36:09,470 --> 00:36:12,680
It's very different if your vehicle drives
a hundred thousand miles and then it

637
00:36:12,681 --> 00:36:17,630
falls apart versus you know, 2
million. Right. And then, you know,

638
00:36:17,631 --> 00:36:21,430
if you have a fleet,
it's kind of like an airplane where,

639
00:36:21,460 --> 00:36:26,460
or a airline where once you produce the
vehicle you want it to be in operation

640
00:36:27,880 --> 00:36:31,660
as many hours a day as possible producing
revenue. And then, uh, you know,

641
00:36:31,770 --> 00:36:35,190
the other piece of that is how
are you generating revenue?

642
00:36:35,250 --> 00:36:36,400
And I think that's kind
of what you're asking.

643
00:36:36,400 --> 00:36:39,200
And I think the obvious things today are,
you know,

644
00:36:39,210 --> 00:36:41,820
the ride sharing business because that's
pretty clear that there's demand for

645
00:36:41,821 --> 00:36:42,654
that.

646
00:36:42,740 --> 00:36:47,340
There's existing markets you can
tap into and large urban and areas,

647
00:36:47,341 --> 00:36:49,230
that kind of thing. Yeah. Yeah. And, and,

648
00:36:49,231 --> 00:36:54,231
and I think that there are some real
benefits to having cars without drivers

649
00:36:54,480 --> 00:36:57,780
compared to sort of the status quo for
people who use ride share services today.

650
00:36:58,480 --> 00:37:02,410
You know, you get privacy consistency,
hopefully you significant improve safety,

651
00:37:02,411 --> 00:37:05,750
all these benefits versus the
current product, but it's a, it's a,

652
00:37:05,751 --> 00:37:07,300
it's a crowded market. And then, uh,

653
00:37:07,320 --> 00:37:09,840
other opportunities which you've seen
a lot of activity in the last really in

654
00:37:09,841 --> 00:37:12,530
the last six to 12 months
is, uh, you know, delivery,

655
00:37:12,531 --> 00:37:17,060
whether that's parcels and
packages, uh, food or groceries.

656
00:37:17,380 --> 00:37:20,630
Um, those are all sort of, I
think, opportunities that are,

657
00:37:20,660 --> 00:37:23,590
that are pretty ripe for these,
you know,

658
00:37:23,600 --> 00:37:28,070
once you have this core technology,
which is the fleet of autonomous vehicles,

659
00:37:28,071 --> 00:37:31,850
there's all sorts of different business
opportunities you can build on top of

660
00:37:31,851 --> 00:37:34,520
that. But I think the
important thing of, of course,

661
00:37:34,521 --> 00:37:37,520
is that there's zero
monetization opportunity until
you actually have that fleet

662
00:37:37,521 --> 00:37:39,500
of very capable of
driverless cars that are,

663
00:37:39,710 --> 00:37:41,390
that are as good or better than humans.
And that's

664
00:37:43,040 --> 00:37:45,770
sort of where the entire industry is sort
of in this holding pattern right now.

665
00:37:45,910 --> 00:37:49,120
Yeah. They're trying to do their
baseline. So, but you said sort of relied,

666
00:37:49,170 --> 00:37:52,300
not reliability, consistency.
It's kind of interesting.

667
00:37:52,301 --> 00:37:55,420
I think I heard you say somewhere,
I'm not sure if that's what you meant,

668
00:37:55,421 --> 00:37:56,730
but you know,

669
00:37:56,740 --> 00:38:01,740
I can imagine a situation where you
would get an autonomous vehicle and uh,

670
00:38:02,230 --> 00:38:03,910
you know,
when you get into an Uber or Lyft,

671
00:38:04,540 --> 00:38:07,390
you don't get to choose the driver in a
sense that you don't get to choose the

672
00:38:07,391 --> 00:38:10,330
personality of the driving.
Do you think there's, uh,

673
00:38:10,380 --> 00:38:14,110
there's room to define the
personality of the car,

674
00:38:14,111 --> 00:38:16,990
the way it drives you in terms
of aggressiveness for example,

675
00:38:17,590 --> 00:38:20,470
in terms of sort of pushing the bar,
the,

676
00:38:21,070 --> 00:38:23,310
one of the biggest challenges
in autonomous driving is the,

677
00:38:23,400 --> 00:38:28,400
is the trade off between sort of safety
and assertiveness and do you think

678
00:38:29,651 --> 00:38:34,651
there's any room for the human
to take a role in that decision?

679
00:38:35,920 --> 00:38:37,810
So to accept some of the liability?
I guess

680
00:38:38,110 --> 00:38:40,960
I wouldn't have said no.
I'd say within reasonable bounds,

681
00:38:40,961 --> 00:38:42,090
as in we're not going to,

682
00:38:43,150 --> 00:38:46,210
I think it'd be higher than likely we'd
expose any knob that would let you,

683
00:38:46,230 --> 00:38:50,860
you know, significantly increase
senior safety risk. I think that's,

684
00:38:50,950 --> 00:38:52,580
that's just not something
we'd be willing to do.

685
00:38:53,000 --> 00:38:56,700
But I think driving style or like,
you know,

686
00:38:56,701 --> 00:38:59,910
are you going to relax the comfort
constraints slightly or things like that.

687
00:39:00,120 --> 00:39:03,390
All of those things make sense in our
plausible, I see all those as you know,

688
00:39:03,391 --> 00:39:04,410
nice optimizations.

689
00:39:04,411 --> 00:39:07,710
Once again we get the core problem
solved and these fleets out there.

690
00:39:08,070 --> 00:39:12,670
But the other thing we've sort of observed
is that you have this intuition that

691
00:39:12,671 --> 00:39:16,750
if you sort of slam your foot on the gas
right after the light turns green and

692
00:39:16,751 --> 00:39:19,390
aggressively accelerate,
you're going to get there faster.

693
00:39:19,660 --> 00:39:22,000
But the actual impact of
doing that is pretty small.

694
00:39:22,030 --> 00:39:23,620
You feel like you're getting there faster,

695
00:39:23,621 --> 00:39:28,370
but so that it's the same would be true
for Avs. Even if they don't slam there,

696
00:39:28,460 --> 00:39:30,620
you know, the pedal to the floor
when the light turns green,

697
00:39:30,980 --> 00:39:32,480
they're going to get you there within,
you know,

698
00:39:32,481 --> 00:39:35,990
if it's a 15 minute trip within 30 seconds
of what you would have done otherwise

699
00:39:36,390 --> 00:39:37,770
if you are going really aggressively.

700
00:39:37,771 --> 00:39:41,860
So I think there's this sort of
self deception that, that uh,

701
00:39:42,040 --> 00:39:44,230
my aggressive driving style
is getting me there faster.

702
00:39:44,420 --> 00:39:46,610
Well, so that's, you know,
some of the things I study,

703
00:39:46,620 --> 00:39:48,740
some of the things I'm fascinated
by the psychology of that.

704
00:39:48,760 --> 00:39:52,890
I don't think it matters. That
doesn't get you there faster. It's,

705
00:39:53,040 --> 00:39:54,770
it's the emotional release.

706
00:39:55,460 --> 00:39:58,790
Driving is a place being etc. Car,

707
00:39:59,060 --> 00:40:03,410
somebody said it's like the real world
version of being a troll as he have this

708
00:40:03,411 --> 00:40:04,071
protection,

709
00:40:04,071 --> 00:40:06,760
this mineral protection and you're
able to sort of yell at the world like

710
00:40:06,770 --> 00:40:08,150
release your anger,
whatever is.

711
00:40:08,151 --> 00:40:12,320
But so there's an element of that that
I think autonomous vehicles will also

712
00:40:12,321 --> 00:40:16,610
have to s you know, giving an outlet to
people, but it doesn't have to be through,

713
00:40:17,060 --> 00:40:20,870
through, through driving or honking
or, so there might be other outlets,

714
00:40:21,110 --> 00:40:25,610
but I think to just sort of even just
put that aside, the baseline is really,

715
00:40:25,611 --> 00:40:28,130
you know, that's the focus. That's
the thing you need to solve.

716
00:40:28,131 --> 00:40:30,920
And then the fund human
things can be solved after.

717
00:40:30,921 --> 00:40:35,150
But so from the baseline of just solving
antonymous driving and you're working

718
00:40:35,151 --> 00:40:39,860
in San Francisco, one of the more
difficult cities to operate in, what is,

719
00:40:39,920 --> 00:40:40,880
what is the,

720
00:40:41,070 --> 00:40:46,070
in your view currently the hardest aspect
of autonomous driving is negotiated

721
00:40:47,541 --> 00:40:51,370
with pedestrians?
Is it a edge cases of perception?

722
00:40:51,380 --> 00:40:56,270
Is it planning? Is there a mechanical
engineering's it data fleet stuff?

723
00:40:56,271 --> 00:40:59,120
I w w what are your
thoughts on the challenge,

724
00:40:59,150 --> 00:41:00,650
the more challenging aspects there?

725
00:41:01,140 --> 00:41:03,510
That's a good, that's a good question. I
think before we, before we go to that too,

726
00:41:03,511 --> 00:41:04,200
I just want to,

727
00:41:04,200 --> 00:41:08,250
I liked what you said about the psychology
aspect of this because I think one

728
00:41:08,310 --> 00:41:09,420
observation I've made is,

729
00:41:09,660 --> 00:41:13,410
I think I read somewhere that I think
it's maybe Americans on average spent,

730
00:41:13,670 --> 00:41:17,880
you know, over an hour, a day on
social media, like staring at Facebook.

731
00:41:18,300 --> 00:41:21,600
And so that's just, you know, 60 minutes
of your life, you're not getting back.

732
00:41:21,601 --> 00:41:26,550
It's probably not super productive. And
so that's 3,600 seconds. Right. And,

733
00:41:26,551 --> 00:41:30,330
uh, that's, that's tiny. You know,
it's a lot of time you're giving up.

734
00:41:30,630 --> 00:41:35,360
And if you compare that to people
being on road, if another vehicle,

735
00:41:35,390 --> 00:41:37,430
whether it's a human driver
or autonomous vehicle,

736
00:41:37,610 --> 00:41:41,700
delays them by even three seconds right
there laying in on the horn. You know,

737
00:41:41,910 --> 00:41:44,690
even though that's, that's, you
know, one, 1000th of the time,

738
00:41:44,691 --> 00:41:46,880
they waste looking at Facebook every day.
So there's,

739
00:41:47,150 --> 00:41:50,600
there's definitely some psychology
aspects of this I think that are pretty

740
00:41:50,601 --> 00:41:51,770
interesting.
Road rage in general.

741
00:41:51,771 --> 00:41:55,460
And then the question of course is if
everyone is in self driving cars to even

742
00:41:55,461 --> 00:41:59,060
notice these three second delays anymore
cause they're doing other things or

743
00:41:59,061 --> 00:42:01,700
reading or working or just
talking to each other.

744
00:42:01,760 --> 00:42:04,850
So it'll be interesting to see where
that goes in a certain aspect. People,

745
00:42:05,150 --> 00:42:07,400
people need to be distracted
by something entertaining,

746
00:42:07,401 --> 00:42:10,610
something useful inside the car so they
don't pay attention to the external

747
00:42:10,611 --> 00:42:11,720
world.
And then the,

748
00:42:11,760 --> 00:42:16,180
and then they can take whatever
psychology and bring it back to Twitter.

749
00:42:16,450 --> 00:42:20,210
And the focus on that as opposed
to sort of interacting, uh,

750
00:42:20,460 --> 00:42:23,640
is sort of putting the motion out
there into the world. So it's a,

751
00:42:23,660 --> 00:42:26,240
it's an interesting problem,
but baseline autonomy,

752
00:42:26,990 --> 00:42:29,000
I guess you could say self driving cars,
you know,

753
00:42:29,001 --> 00:42:32,240
at scale we'll lower the collective
blood pressure of society,

754
00:42:32,290 --> 00:42:35,690
probably buy a couple of points
without all that road rage and stress.

755
00:42:35,770 --> 00:42:38,570
So that's a good, good externality. Um,

756
00:42:38,571 --> 00:42:42,320
so back to your question about
the technology and the, the,

757
00:42:42,740 --> 00:42:43,820
I guess the biggest problems,

758
00:42:43,821 --> 00:42:46,750
and I have a hard time answering
that question because you know,

759
00:42:46,760 --> 00:42:51,760
we've been at this like specifically
focusing on driverless cars and all the

760
00:42:51,921 --> 00:42:55,040
technology needed to enable that for a
little over four and a half years now.

761
00:42:55,190 --> 00:42:59,060
And even a year or two in,
I felt like we had

762
00:43:00,710 --> 00:43:05,360
completed the functionality needed to
get someone from point a to point B as in

763
00:43:05,660 --> 00:43:09,080
if we need to do a left turn maneuver or
if we need to drive around, uh, you know,

764
00:43:09,230 --> 00:43:13,130
double parked vehicle into oncoming
traffic or navigate through construction

765
00:43:13,131 --> 00:43:16,280
zones. The, the scaffolding
and the building blocks were,

766
00:43:16,281 --> 00:43:17,630
was there pretty early on.

767
00:43:17,870 --> 00:43:22,870
And so the challenge is not any one
scenario or situation for which we fail at

768
00:43:23,991 --> 00:43:26,710
100% of those. It's more, you know,

769
00:43:26,720 --> 00:43:30,020
we're benchmarking against a pretty
good or pretty high standard,

770
00:43:30,021 --> 00:43:32,300
which is human driving.
All things considered.

771
00:43:32,301 --> 00:43:36,530
Humans are excellent at handling edge
cases and unexpected scenarios where

772
00:43:36,670 --> 00:43:37,610
computers are the opposite.

773
00:43:38,450 --> 00:43:43,130
And so beating that that a baseline
set by humans is the challenge.

774
00:43:43,131 --> 00:43:47,480
And so what we've been doing for
quite some time now is basically

775
00:43:49,130 --> 00:43:53,300
it's this continuous improvement process
where we find sort of the, the most, um,

776
00:43:53,750 --> 00:43:57,000
you know, uncomfortable or,
or the things that that, um,

777
00:43:58,010 --> 00:44:00,950
could lead to a, uh, a safety issue,
other things, all these events.

778
00:44:00,951 --> 00:44:03,350
And then we sort of categorize them and,
uh,

779
00:44:03,351 --> 00:44:06,890
rework parts of our system to
make incremental improvements
and do that over and

780
00:44:06,891 --> 00:44:07,730
over and over again.

781
00:44:08,060 --> 00:44:11,970
And we just see sort of the overall
performance of the system. You know,

782
00:44:12,140 --> 00:44:15,350
actually increasing at a pretty
steady clip. But there's no one thing,

783
00:44:15,351 --> 00:44:19,190
there's actually like thousands of
little things and just like polishing

784
00:44:19,191 --> 00:44:22,260
functionality and making sure that
it handles, you know, every Vr,

785
00:44:22,310 --> 00:44:27,310
every version and possible permutation
of a situation by either applying more

786
00:44:27,801 --> 00:44:31,580
deep learning systems or
just by, uh, you know,

787
00:44:31,581 --> 00:44:35,420
adding more test coverage or new
scenarios that, that we develop against.

788
00:44:35,750 --> 00:44:38,000
And just grinding on that.
It's, we're sort of in the,

789
00:44:38,030 --> 00:44:40,100
the unsexy phase of development right now,

790
00:44:40,101 --> 00:44:43,550
which is doing the real engineering work
that it takes to go from prototype to

791
00:44:43,551 --> 00:44:44,120
production.

792
00:44:44,120 --> 00:44:46,370
You're basically scaling the,
the grinding.

793
00:44:46,940 --> 00:44:51,200
So sort of taking seriously
that the process of a,

794
00:44:51,210 --> 00:44:52,520
all those edge cases,

795
00:44:52,880 --> 00:44:57,410
both with human experts in machine
learning methods to, to cover,

796
00:44:57,500 --> 00:44:59,120
to cover all of those situations.

797
00:44:59,300 --> 00:45:02,690
Yeah. And the exciting thing for me is
I don't think that grinding ever stops,

798
00:45:02,720 --> 00:45:05,660
right? Because there's a
moment in time where you,

799
00:45:05,740 --> 00:45:09,800
you've crossed that threshold of, of
human performance and become superhuman,

800
00:45:11,190 --> 00:45:12,180
but there's no reason,

801
00:45:12,181 --> 00:45:16,890
there's no first principles reason that
ab capability will tap out anywhere near

802
00:45:16,891 --> 00:45:20,850
humans. Like there's no reason it couldn't
be 20 times better, whether that's,

803
00:45:21,090 --> 00:45:21,330
you know,

804
00:45:21,330 --> 00:45:24,720
just better driving or safer driving
and more comfortable driving or even a

805
00:45:24,721 --> 00:45:29,010
thousand times better given enough time.
And we intend to basically chase that,

806
00:45:29,130 --> 00:45:31,020
you know,
forever

807
00:45:31,510 --> 00:45:34,720
to build the best possible product better
and better and better and always new

808
00:45:34,721 --> 00:45:36,700
edge cases come up and you experiences so,

809
00:45:37,170 --> 00:45:40,510
and you want to automate that
process as much as possible.

810
00:45:42,700 --> 00:45:44,920
So what do you think
in general in society,

811
00:45:45,160 --> 00:45:49,510
when do you think we may have hundreds
of thousands of fully autonomous vehicles

812
00:45:49,511 --> 00:45:53,350
driving around? So first of all,
predictions, nobody knows the future.

813
00:45:53,500 --> 00:45:57,400
You're a part of the leading people trying
to define that future. But even then,

814
00:45:57,430 --> 00:46:02,110
you still don't know. But if you think
about a hundreds of thousands of vehicles,

815
00:46:02,170 --> 00:46:07,170
so a significant fraction of vehicles
in major cities that are autonomous.

816
00:46:07,560 --> 00:46:12,560
Do you think [inaudible] with Rodney
Brooks who is 2050 and beyond?

817
00:46:13,900 --> 00:46:14,910
Or are you more

818
00:46:16,030 --> 00:46:19,360
with Ilan Musk who is,
we should have had that two years ago.

819
00:46:20,530 --> 00:46:23,800
Well, I mean, I don't mean
to have two years ago,

820
00:46:23,801 --> 00:46:26,410
but we're not there yet.
So,

821
00:46:26,530 --> 00:46:28,840
I guess the way I would
think about that is let's,

822
00:46:28,841 --> 00:46:31,180
let's flip that question around.

823
00:46:31,181 --> 00:46:35,820
So what would prevent you to reach
hundreds of thousands of vehicles?

824
00:46:35,910 --> 00:46:39,420
And that's a good, that's a good
rephrasing it. Yeah. So the,

825
00:46:42,040 --> 00:46:44,650
I'd say that it seems the consensus

826
00:46:46,920 --> 00:46:50,850
among the people developing self driving
cars today is to sort of start with

827
00:46:50,851 --> 00:46:54,340
some form of an easier environment.
Whether it means you know,

828
00:46:54,360 --> 00:46:58,890
lacking inclement weather or you know,
mostly sunny or whatever it is,

829
00:46:59,990 --> 00:47:04,520
and then add add capability for
more complex situations over time.

830
00:47:05,030 --> 00:47:10,030
And so if you're only able to deploy
in areas that that meets sort of your

831
00:47:10,701 --> 00:47:13,900
criteria or that the current domain,
you know, operating domain of,

832
00:47:13,901 --> 00:47:15,530
of the software you developed a,

833
00:47:15,531 --> 00:47:17,570
that may put a cap on how many
cities you could deploy in,

834
00:47:18,990 --> 00:47:21,510
but then as those restrictions
start to fall away,

835
00:47:21,540 --> 00:47:25,620
like maybe you add a capability to drive
really well and safely and heavy rain

836
00:47:25,621 --> 00:47:27,260
or snow,
you know,

837
00:47:27,261 --> 00:47:30,760
that that probably opens up the market
by two or two or three fold in terms of

838
00:47:30,761 --> 00:47:34,740
the cities you can expand into and so on.
And so the real question is, you know,

839
00:47:34,990 --> 00:47:38,310
I know today if we wanted to,
we could produce that,

840
00:47:38,330 --> 00:47:39,870
that many autonomous vehicles,

841
00:47:40,350 --> 00:47:42,780
but we wouldn't be able to make use of
all of them yet cause we would sort of

842
00:47:42,781 --> 00:47:47,400
saturate the demand in the cities in
which we would want to operate initially.

843
00:47:48,890 --> 00:47:51,890
So if I were to guess like what the
timeline is for those things falling away

844
00:47:51,891 --> 00:47:53,780
and reaching hundreds of
thousands of vehicles,

845
00:47:55,140 --> 00:47:58,680
maybe a range is, but I would say less
than five years. That's the five years.

846
00:47:58,710 --> 00:48:02,280
Yeah. And of course you're
working hard to make that happen.

847
00:48:03,280 --> 00:48:08,280
So you started two companies that were
eventually acquired for each $4 billion.

848
00:48:09,460 --> 00:48:11,020
So you're pretty good person to ask.

849
00:48:11,350 --> 00:48:13,390
What does it take to build
a successful startup?

850
00:48:14,080 --> 00:48:18,390
Hmm. I think, uh, there's, there's sort
of survivor bias here a little bit,

851
00:48:19,300 --> 00:48:21,850
but I can try to find some common threads
for the, the things that worked for me,

852
00:48:21,851 --> 00:48:26,800
which is, you know, I, in,
in both of these companies,

853
00:48:26,801 --> 00:48:29,910
it was really passionate about the core
technology. I actually like, you know,

854
00:48:29,911 --> 00:48:32,760
lay awake at night thinking about
these problems and how to solve them.

855
00:48:33,060 --> 00:48:35,640
And I think that's helpful
because when you start a business,

856
00:48:35,641 --> 00:48:40,480
there are that like to this day
they're are these crazy ups and downs.

857
00:48:40,481 --> 00:48:42,330
Like one day you think the
business is just on your,

858
00:48:42,340 --> 00:48:45,580
you're just on top of the
world and unstoppable and
the next day you think, okay,

859
00:48:45,581 --> 00:48:47,570
this is all gonna end.
You know, it's, it's just,

860
00:48:47,571 --> 00:48:50,920
it's just gone south and it's going
to be over tomorrow. Um, and uh,

861
00:48:52,720 --> 00:48:55,900
and so I think like having a true passion
that you can fall back on and knowing

862
00:48:55,901 --> 00:48:58,060
that you would be doing it even
if you weren't getting paid for,

863
00:48:58,061 --> 00:49:01,800
it helps you weather those tough times.
So that's one thing.

864
00:49:01,830 --> 00:49:05,290
I think the other one is
really good people.

865
00:49:05,320 --> 00:49:09,070
So I've always been surrounded by
really good co founders that are logical

866
00:49:09,071 --> 00:49:12,910
thinkers are always pushing their limits
and have very high levels of integrity.

867
00:49:12,970 --> 00:49:16,750
So that's Dan Kahn and my current company
and actually his brother and a couple

868
00:49:16,751 --> 00:49:21,170
other guys for Justin TV and twitch.
And then I think the last thing is

869
00:49:22,690 --> 00:49:26,790
just, uh, I guess persistence
or perseverance like and,

870
00:49:26,810 --> 00:49:29,800
and that that can apply to
sticking to sort of, uh,

871
00:49:30,110 --> 00:49:34,150
or having conviction around the original
premise of your idea and sticking

872
00:49:34,151 --> 00:49:35,830
around to do all the,
you know,

873
00:49:35,831 --> 00:49:39,700
the unsexy work to actually make it come
to fruition, including dealing with,

874
00:49:39,730 --> 00:49:43,520
you know, well, whatever it is
that you're not passionate about,

875
00:49:43,521 --> 00:49:47,540
whether that's finance or, or HR
or, or operations or those things.

876
00:49:47,920 --> 00:49:52,610
It as long as you are grinding away and
working towards that North Star for your

877
00:49:52,611 --> 00:49:53,630
business,
whatever it is,

878
00:49:54,020 --> 00:49:56,690
and you don't give up and you're
making progress every day,

879
00:49:56,930 --> 00:49:58,970
it seems like eventually
we'll end up in a good place.

880
00:49:59,000 --> 00:50:00,860
And the only things that can
slow you down are, you know,

881
00:50:00,861 --> 00:50:03,410
running out of money or I suppose
your competitors destroying you.

882
00:50:03,411 --> 00:50:07,700
But I think most of the time it's people
giving upper or somehow destroying

883
00:50:07,701 --> 00:50:10,340
things themselves rather than being beaten
by their competition are running out

884
00:50:10,341 --> 00:50:11,174
of money.

885
00:50:11,350 --> 00:50:13,360
Yeah. If you never quit,
eventually you'll arrive.

886
00:50:14,250 --> 00:50:17,190
So a more concise version
of what I was trying to say

887
00:50:18,690 --> 00:50:21,390
that you went to y Combinator out twice.
Yeah.

888
00:50:21,480 --> 00:50:23,970
What do you think in a quick question,

889
00:50:23,971 --> 00:50:28,971
do you think is the best way to raise
funds in the early days or not just funds

890
00:50:29,241 --> 00:50:31,520
but just the community
develop your idea and so on.

891
00:50:32,210 --> 00:50:37,210
Can you do it solo or maybe with
a cofounder with Zach self funded?

892
00:50:38,900 --> 00:50:41,570
Do you think y Combinator is good,
is a good to do VC route?

893
00:50:41,630 --> 00:50:46,280
Is there no right answer or was there
from the y Combinator experience something

894
00:50:46,281 --> 00:50:48,650
that you could take away that
that was the right path to take?

895
00:50:48,790 --> 00:50:51,910
There's no one size fits all answer,
but if your ambition I think is to,

896
00:50:53,470 --> 00:50:55,750
you see how big you can make something or,
or,

897
00:50:55,810 --> 00:51:00,100
or rapidly expand and capture market
or solve a problem or whatever it is,

898
00:51:00,750 --> 00:51:04,140
then then you know, going to venture
background is probably a good approach.

899
00:51:04,141 --> 00:51:07,020
So that, so that capital doesn't
become your primary constraint.

900
00:51:07,850 --> 00:51:11,930
Y Combinator I love because
it puts you in this, uh,

901
00:51:11,990 --> 00:51:15,080
sort of competitive environment while
you're where you're surrounded by,

902
00:51:15,320 --> 00:51:19,310
you know, the top maybe 1% of other
really highly motivated, you know,

903
00:51:19,920 --> 00:51:22,890
peers who are in the same,
same place and that, uh,

904
00:51:22,920 --> 00:51:27,080
that environment I think just
breeds, breeds success, right?

905
00:51:27,081 --> 00:51:29,180
If you're surrounded by really brilliant,
hardworking people,

906
00:51:29,390 --> 00:51:34,010
you're going to feel sort of compelled
or inspired to to try to emulate them or

907
00:51:34,011 --> 00:51:38,720
beat them. And uh, so even though I had
done it once before and I felt like,

908
00:51:39,670 --> 00:51:42,010
yeah, I'm pretty self motivated,
I thought like, I look,

909
00:51:42,011 --> 00:51:44,380
this is going to be a hard problem.
I can use all the help I can get.

910
00:51:44,620 --> 00:51:47,710
So surrounding myself with
other entrepreneurs is going
to make me work a little

911
00:51:47,711 --> 00:51:51,890
bit harder or push a little harder than
it's worth it and society. Why? Why?

912
00:51:51,891 --> 00:51:53,510
I did it for example,
the second time,

913
00:51:54,100 --> 00:51:56,590
let's uh,
let's go philosophical existential.

914
00:51:57,010 --> 00:52:02,010
If you go back and do something
differently in your life starting in high

915
00:52:03,221 --> 00:52:05,830
school,
then MIT leaving MIT,

916
00:52:05,890 --> 00:52:09,100
you could have gone the phd
route doing the startup,

917
00:52:10,490 --> 00:52:14,780
going to see about a startup in California
and you or maybe some aspects of

918
00:52:14,781 --> 00:52:19,240
fundraising. Is there something
you regret, something,
well not necessarily grab,

919
00:52:19,250 --> 00:52:21,650
but if you go back good to differently.

920
00:52:22,130 --> 00:52:24,380
I think I've made a lot of
mistakes. Like, you know,

921
00:52:24,410 --> 00:52:27,080
pretty much everything you can screw up.
I think I've screwed up at least once,

922
00:52:28,490 --> 00:52:31,460
but I, you know, I don't regret those
things. I think it's, it's hard to,

923
00:52:31,550 --> 00:52:34,490
hard to look back on things even if
they didn't go well and call it a regret

924
00:52:34,520 --> 00:52:35,510
because hopefully,
you know,

925
00:52:35,600 --> 00:52:38,510
it took away some new knowledge
or learning from that. So

926
00:52:42,990 --> 00:52:47,350
I would say there, there was a
period, yeah. Uh, the closest
I can, I can come to us.

927
00:52:47,370 --> 00:52:51,680
There was a period, um, in, in Justin
TV I think after seven years where,

928
00:52:53,380 --> 00:52:56,620
you know, the, the company was going one
direction, which is towards twitch, uh,

929
00:52:56,621 --> 00:53:00,700
in video gaming. I'm not a video gamer.
I don't really even use twitch at all.

930
00:53:01,840 --> 00:53:04,450
And I was still working on
the core technology there,

931
00:53:04,451 --> 00:53:07,870
but my heart was no longer in it because
the business that we were creating was

932
00:53:07,871 --> 00:53:09,910
not something that I was
personally passionate about.

933
00:53:10,000 --> 00:53:12,730
It didn't meet your bar of
existential impact. Yeah.

934
00:53:12,760 --> 00:53:17,760
And I'd say I probably spent an extra
year or two working on that and uh,

935
00:53:18,350 --> 00:53:22,000
and I'd say like I would have just
tried to do something different sooner.

936
00:53:23,020 --> 00:53:27,060
Cause those were, those were two
years where I felt like, um, you know,

937
00:53:27,061 --> 00:53:30,090
from this philosophical or
existential thing, I just,

938
00:53:30,091 --> 00:53:32,820
I just felt something was missing.
And so I, I would've, I would've,

939
00:53:32,850 --> 00:53:34,140
if I could look back now and tell myself,

940
00:53:34,141 --> 00:53:35,760
it's like I would have said exactly that.
Like,

941
00:53:35,761 --> 00:53:39,810
you're not getting any meeting out of your
work personally right now. You should,

942
00:53:39,840 --> 00:53:41,010
you should find a way to change that.

943
00:53:41,670 --> 00:53:45,990
And that's part of the pitch I used to
basically everyone who joins cruise today,

944
00:53:45,991 --> 00:53:48,540
it's like, Hey, you've got
that now by coming here. Well,

945
00:53:48,541 --> 00:53:52,410
maybe you need the two years of that
existential dread to develop the feeling

946
00:53:52,411 --> 00:53:55,590
that ultimately it was the fire that
create a cruise. So you never know.

947
00:53:55,620 --> 00:53:58,770
You can't be a good theory.
Yeah. So last question,

948
00:53:58,800 --> 00:54:02,520
what does 2018 holds for Cruz after this?

949
00:54:02,521 --> 00:54:04,620
I guess we're going to go
and talk to your class,

950
00:54:04,621 --> 00:54:08,100
but one of the big things is going
from prototype to production, uh,

951
00:54:08,130 --> 00:54:09,540
for autonomous cars.
And what does that mean?

952
00:54:09,541 --> 00:54:12,980
What's that look like in 2019 for us is,

953
00:54:12,981 --> 00:54:16,050
is the year that we try to cross over
that threshold and reach, you know,

954
00:54:16,110 --> 00:54:20,070
superhuman level of performance to some
degree with the software and have all

955
00:54:20,071 --> 00:54:24,920
the other of the thousands
of little building blocks
in place to, to launch. Um,

956
00:54:25,460 --> 00:54:27,870
you know, our, our first
commercial product. So that's,

957
00:54:28,350 --> 00:54:29,630
that's what's in store for us,
are in the,

958
00:54:29,631 --> 00:54:32,610
in store for us and we've
got a lot of work to do.

959
00:54:32,700 --> 00:54:37,140
We've got a lot of brilliant
people working on it. So
it's, it's all up to us now.

960
00:54:37,650 --> 00:54:40,580
Yeah.
From Charlie Miller and Chris Val's like,

961
00:54:40,740 --> 00:54:45,660
people have crossed paths with it
sounds like given an amazing team.

962
00:54:45,661 --> 00:54:47,940
So, um, like I said, it's one of the most,

963
00:54:48,300 --> 00:54:52,080
I think one of the most important
problems in artificial intelligence of the

964
00:54:52,080 --> 00:54:55,170
century will be one of the most defining,
the super exciting that you work on it.

965
00:54:55,350 --> 00:54:59,100
And, uh, the bus to look at 2018.

966
00:54:59,101 --> 00:55:01,920
I'm really excited to see what
Cruz comes up with. Thank you.

967
00:55:01,921 --> 00:55:03,150
Thanks for having me today.
Nice call.

