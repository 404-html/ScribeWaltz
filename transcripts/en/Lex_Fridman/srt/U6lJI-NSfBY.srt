1
00:00:00,080 --> 00:00:02,690
Welcome back to six zero,
nine,

2
00:00:02,691 --> 00:00:07,691
nine artificial general intelligence.
Today we have Richard Moise.

3
00:00:08,000 --> 00:00:11,180
He's the founder and managing director 
of article 36,

4
00:00:11,630 --> 00:00:16,630
a UK based not for profit organization,
working to prevent the unintended,

5
00:00:17,840 --> 00:00:21,800
unnecessary and unacceptable harm caused
by certain weapons,

6
00:00:22,160 --> 00:00:25,400
including autonomous weapons and nuclear
weapons.

7
00:00:26,150 --> 00:00:31,150
He will talk with us today about 
autonomous weapon systems and the 

8
00:00:31,150 --> 00:00:35,351
context of Ai Safety.
This is an extremely important topic for

9
00:00:36,830 --> 00:00:39,230
engineers,
humanitarians,

10
00:00:39,231 --> 00:00:40,850
legal minds,
policymakers,

11
00:00:40,851 --> 00:00:45,851
and everybody involved in paving the 
path for a safe,

12
00:00:46,071 --> 00:00:51,071
positive future for ai in our society,
which I hope is what this course is 

13
00:00:51,831 --> 00:00:56,831
about.
Richard flew all the way from the UK to 

14
00:00:56,831 --> 00:01:00,410
visit us today in snow in Massachusetts,
so please give him a warm welcome.

15
00:01:02,520 --> 00:01:07,520
Thank you.

16
00:01:07,800 --> 00:01:11,160
Thanks very much lex and thank you all 
for coming out.

17
00:01:11,470 --> 00:01:12,630
I was,
like I said,

18
00:01:12,631 --> 00:01:15,210
I work for a not for profit organization
based in the UK.

19
00:01:15,420 --> 00:01:20,420
We specialize in thinking about policy 
and legal frameworks around weapon 

20
00:01:20,420 --> 00:01:24,540
technologies particularly and generally 
about how to establish more constraining

21
00:01:24,960 --> 00:01:29,960
policy and legal frameworks around 
around weapons and I guess I'm mainly 

22
00:01:31,171 --> 00:01:36,171
going to talk today about these issues 
of to what extent we should enable 

23
00:01:36,171 --> 00:01:40,440
machines to kill people to make 
decisions to kill people.

24
00:01:41,220 --> 00:01:41,830
It's,
I think,

25
00:01:41,831 --> 00:01:45,360
a conceptually very interesting topic.
Quite challenging in lots of ways.

26
00:01:45,960 --> 00:01:50,960
There's lots of unstable terminology,
lots of sort of blurry boundaries.

27
00:01:52,710 --> 00:01:57,710
My own background,
the sidewalk on weapons policy issues 

28
00:01:57,710 --> 00:01:59,280
that I've worked on,
the development of have two legal,

29
00:01:59,340 --> 00:02:03,030
international legal treaties prohibiting
certain types of weapons.

30
00:02:03,031 --> 00:02:06,870
I worked on the development of 2008 
convention on cluster munitions,

31
00:02:06,871 --> 00:02:11,190
which prohibits cluster bombs and worked
on our organization.

32
00:02:11,190 --> 00:02:14,640
Pyre pioneered the idea of a treaty 
prohibition on nuclear weapons,

33
00:02:14,641 --> 00:02:19,641
which was agreed last year in the UN and
we're part of the steering group of 

34
00:02:19,641 --> 00:02:24,441
Icann,
the international campaign to abolish 

35
00:02:24,441 --> 00:02:26,151
nuclear weapons,
which won the Nobel peace prize last 

36
00:02:26,151 --> 00:02:26,151
year,
so that was a good year for us.

37
00:02:26,490 --> 00:02:31,490
The issue of autonomous weapons,
killer robots which are going to talk 

38
00:02:31,490 --> 00:02:33,240
about the day.
We're also part of an NGO,

39
00:02:33,270 --> 00:02:38,270
nongovernmental organization coalition 
on this issue called the campaign to 

40
00:02:38,270 --> 00:02:39,450
stop killer robots.
It's a good name.

41
00:02:40,830 --> 00:02:45,830
However,
I think when we get into some of the 

42
00:02:45,830 --> 00:02:47,930
details of the issue,
we'll find that perhaps the snappiness 

43
00:02:47,930 --> 00:02:51,891
of the name in a way masks some of the 
complexity that lies underneath this,

44
00:02:55,020 --> 00:02:57,720
but this is a live issue in 
international policy,

45
00:02:57,721 --> 00:03:02,320
in legal discussions at the United 
Nations for the last several years,

46
00:03:02,321 --> 00:03:06,760
three or four years now that are being 
groups of governments coming together to

47
00:03:06,761 --> 00:03:11,761
discuss autonomous weapons and whether 
or not there should be some new legal 

48
00:03:11,761 --> 00:03:12,180
instrument that,
uh,

49
00:03:12,220 --> 00:03:14,230
the tackles this,
this issue.

50
00:03:14,230 --> 00:03:19,230
So it's a,
it's a live political issue that is 

51
00:03:19,230 --> 00:03:19,660
being debated in,
in policy,

52
00:03:19,690 --> 00:03:22,490
legal circles.
And really my,

53
00:03:22,690 --> 00:03:25,420
my comments a day are going to be 
speaking to that context.

54
00:03:25,421 --> 00:03:30,421
I guess I'm going to try and give us a 
bit of a briefing about what the issues 

55
00:03:31,121 --> 00:03:33,640
are in this international debate,
how different actors,

56
00:03:33,641 --> 00:03:38,190
a orientating to these issues.
Some of the conceptual models that we,

57
00:03:38,540 --> 00:03:43,540
we use in that.
So I'm not really going to give you a 

58
00:03:43,540 --> 00:03:44,920
particular sales pitches to what you 
should think about this issue though.

59
00:03:45,130 --> 00:03:49,630
My own biases are probably going to be 
fairly evident during the process,

60
00:03:50,140 --> 00:03:52,160
but really to try and lay out a bit of a
sense of,

61
00:03:52,180 --> 00:03:53,430
of how these,
uh,

62
00:03:53,620 --> 00:03:58,620
these questions debated in the 
international political scene and maybe 

63
00:03:58,620 --> 00:04:02,551
in a way that's useful for reflecting on
sort of wider questions of how a AI 

64
00:04:03,011 --> 00:04:06,490
technologies might be orientate it to an
approach by,

65
00:04:06,491 --> 00:04:08,090
uh,
by policy makers and,

66
00:04:08,110 --> 00:04:09,970
and,
and the legal framework.

67
00:04:11,140 --> 00:04:13,390
So in terms of the structure of my 
comments,

68
00:04:13,391 --> 00:04:18,391
I'm gonna talk a bit about some of the 
pros and cons that are put forward a 

69
00:04:18,391 --> 00:04:22,590
around autonomous weapons or movements 
towards greater autonomy in weapons 

70
00:04:22,590 --> 00:04:22,590
systems.
Uh,

71
00:04:22,840 --> 00:04:26,150
I'm going to talk a bit more about the 
political legal framework within,

72
00:04:26,170 --> 00:04:28,060
within which these discussions are 
taking place.

73
00:04:28,600 --> 00:04:31,570
And then I'm going to try to sort of lay
out some of the models,

74
00:04:31,571 --> 00:04:35,320
the conceptual models that we're as an 
organization have developed and are sort

75
00:04:35,321 --> 00:04:40,321
of using a in relation to these issues.
And perhaps to reflect a bit on where I 

76
00:04:40,331 --> 00:04:42,160
see the,
the political conversation,

77
00:04:42,161 --> 00:04:47,161
the legal conversation on this going at 
an international level and maybe just 

78
00:04:47,161 --> 00:04:51,601
finally to try to reflect on or just 
draw out some more general thoughts that

79
00:04:53,111 --> 00:04:58,111
I think occurred to me a about what some
of those says about thinking about ai 

80
00:04:58,240 --> 00:05:01,570
functions in a,
in different social social roles.

81
00:05:02,160 --> 00:05:07,160
Um,
but before getting into that sort of 

82
00:05:07,210 --> 00:05:12,210
pros and cons type type stuff,
I just wanted to start by suggesting a 

83
00:05:12,761 --> 00:05:17,761
bit of a sort of conceptual timeline 
because one of the things this could be 

84
00:05:20,471 --> 00:05:25,471
the president,
one of the things you find when we 

85
00:05:26,861 --> 00:05:31,861
start,
when you say to somebody who will work 

86
00:05:31,861 --> 00:05:32,210
on this issue of autonomous weapons,
they tend to orientate to it.

87
00:05:32,211 --> 00:05:37,211
And in two fairly distinct ways.
Some people will say,

88
00:05:37,480 --> 00:05:39,700
oh,
you mean om drones and you know,

89
00:05:39,701 --> 00:05:42,940
we know drones are they being used,
uh,

90
00:05:42,941 --> 00:05:47,941
in the world today.
And that's kind of an issue here in the 

91
00:05:47,941 --> 00:05:47,941
present.
Right?

92
00:05:51,660 --> 00:05:54,640
But other people,
most of the media,

93
00:05:54,670 --> 00:05:59,670
and certainly pretty much every media 
photo editor thinks she talking about 

94
00:06:00,530 --> 00:06:05,530
terminator of hair.
Maybe even a skynet thrown in,

95
00:06:08,510 --> 00:06:12,980
so this is a sort of advanced,
futuristic,

96
00:06:13,040 --> 00:06:16,340
Saifai orientation to the,
to the issues.

97
00:06:16,970 --> 00:06:19,030
My thinking about this,
I come from a background of,

98
00:06:19,130 --> 00:06:21,980
of working on the impact of weapons in 
the present.

99
00:06:23,330 --> 00:06:27,560
I'm less,
I'm less concerned about this area of my

100
00:06:27,800 --> 00:06:30,980
thinking and my anxieties or concerns 
around this issue.

101
00:06:30,981 --> 00:06:33,410
Don't come from,
from this area.

102
00:06:33,870 --> 00:06:38,870
I'll do this light a bit wiggly here 
because I also don't want to suggest 

103
00:06:38,870 --> 00:06:38,870
there's any kind of,
you know,

104
00:06:38,870 --> 00:06:43,540
telia logical certainty going on here.
This is just an imaginary timeline,

105
00:06:45,800 --> 00:06:50,800
but I think it's important just in terms
of situating where I'm coming from in 

106
00:06:50,800 --> 00:06:51,320
the debate that I'm definitely not 
starting at that end.

107
00:06:51,650 --> 00:06:55,580
And yet in the political discussion 
amongst governments and states,

108
00:06:55,880 --> 00:06:59,660
well you have people come in and it all 
sorts of different positions along here.

109
00:06:59,660 --> 00:07:03,220
Imagining a little autonomous weapons 
may existed,

110
00:07:03,280 --> 00:07:04,740
you know,
somewhere along this,

111
00:07:04,741 --> 00:07:09,741
this sort of spectrum.
So I'm going to think more about stuff 

112
00:07:09,741 --> 00:07:13,751
that's going on around here and how some
of our conceptual model is really built 

113
00:07:13,751 --> 00:07:16,570
around some of this thinking.
Not so much actually armed drones,

114
00:07:16,640 --> 00:07:18,080
but some other,
some other systems.

115
00:07:19,100 --> 00:07:24,100
But my background before I started 
working on policy and law around weapons

116
00:07:24,921 --> 00:07:25,730
was,
um,

117
00:07:26,180 --> 00:07:31,180
was setting up and managing landmine 
clearance operations overseas and well,

118
00:07:31,971 --> 00:07:36,260
they'd been around for.
I'd been around for quite a long time,

119
00:07:36,290 --> 00:07:37,550
like months.
Um,

120
00:07:39,680 --> 00:07:42,830
and I think it's interesting just to 
start with,

121
00:07:42,831 --> 00:07:46,540
just to reflect on the basic 
anti-personnel landmine.

122
00:07:46,830 --> 00:07:48,710
A simple,
but it gives us,

123
00:07:48,711 --> 00:07:53,711
I think some sort of useful entry points
into thinking about what an autonomous 

124
00:07:53,711 --> 00:07:56,240
weapon system might be in its most 
simple form.

125
00:07:56,930 --> 00:07:58,570
Uh,
if we think about a landmine,

126
00:07:58,640 --> 00:08:03,640
well,
essentially we have a person and as an 

127
00:08:06,141 --> 00:08:10,190
input into the landmine and there's a 
function that goes on here.

128
00:08:11,850 --> 00:08:15,410
Pressure is greater than x person.
They tried on the landmine.

129
00:08:15,860 --> 00:08:19,670
There's a basic mechanical algorithm 
goes on and you get an output.

130
00:08:22,320 --> 00:08:27,320
That explosion that goes back against 
the person who taught on the Lamont.

131
00:08:29,270 --> 00:08:32,260
So it's a fairly simple system of a 
signal,

132
00:08:32,290 --> 00:08:35,210
a sensor taking a signal from the 
outside world.

133
00:08:35,240 --> 00:08:40,240
The landmine is viewing the outside 
world through its sensor to basic 

134
00:08:40,240 --> 00:08:42,890
pressure plate and according to a 
certain calculus here,

135
00:08:43,100 --> 00:08:46,760
you got output and it's directed back at
this person and it's a loop.

136
00:08:47,060 --> 00:08:52,060
And that's one of the things that I 
think is fundamental essentially to 

137
00:08:52,060 --> 00:08:52,520
understanding the idea of autonomous 
weapons.

138
00:08:52,521 --> 00:08:54,320
And in a way this is where the autonomy 
comes in,

139
00:08:54,700 --> 00:08:58,560
that there's no other person intervening
in this process at any point.

140
00:08:58,790 --> 00:09:03,790
There's just a sort of straightforward 
relationship from the person or object 

141
00:09:04,141 --> 00:09:06,090
that has initiated the,
uh,

142
00:09:06,590 --> 00:09:09,120
the system back into the,
uh,

143
00:09:09,450 --> 00:09:11,910
back into the effects that are being,
that are being applied.

144
00:09:12,180 --> 00:09:17,180
So in some ways,
we'll come back to this later and think 

145
00:09:17,180 --> 00:09:18,150
about how some of the basic building 
blocks of this,

146
00:09:18,151 --> 00:09:23,151
uh,
maybe they're not thinking about other 

147
00:09:23,151 --> 00:09:26,061
weapons systems and weapons technologies
as they're developing and maybe thinking

148
00:09:26,761 --> 00:09:30,480
about landmines and thinking about these
processes of technological change.

149
00:09:30,840 --> 00:09:32,590
We see a number of different,
uh,

150
00:09:32,970 --> 00:09:36,180
dynamics at play in this sort of 
imaginary timeline.

151
00:09:36,990 --> 00:09:39,240
Anti personnel,
land mines of course a static.

152
00:09:39,600 --> 00:09:41,850
They just sit in the ground where you 
left them.

153
00:09:42,420 --> 00:09:45,630
But we get more and more mobility 
perhaps as we go through this system.

154
00:09:45,631 --> 00:09:48,780
Certainly drones and other systems that 
I'll talk about,

155
00:09:48,900 --> 00:09:52,020
you start to see more mobility in the,
in the weapon system.

156
00:09:53,880 --> 00:09:57,300
Perhaps greater sophistication of 
sensors,

157
00:09:57,390 --> 00:10:01,410
I mean a a basic pressure plate.
Just gauging,

158
00:10:01,411 --> 00:10:06,411
wait,
that's a very simple a census structure 

159
00:10:06,411 --> 00:10:10,050
for interrogating the world.
We have much more sophisticated a sensor

160
00:10:10,051 --> 00:10:15,051
systems and weapons now.
So we have weapons systems now that are 

161
00:10:15,051 --> 00:10:18,351
looking at radar signatures.
They're looking at the heat shapes of 

162
00:10:18,351 --> 00:10:19,230
objects and we'll come back and talk 
about that.

163
00:10:19,231 --> 00:10:22,680
But more sophistication of sensors and 
more sophistication of the,

164
00:10:22,681 --> 00:10:27,681
um,
of the computer algorithms that are 

165
00:10:27,681 --> 00:10:29,190
basically interrogating those sensor,
the sensor inputs.

166
00:10:31,180 --> 00:10:36,180
I'm perhaps a little bit as well of a 
movement in this sort of trajectory from

167
00:10:36,960 --> 00:10:41,550
physically very unified objects always 
sort of wrestle slightly whether this is

168
00:10:41,551 --> 00:10:46,551
the word I want,
but it's a sort of self contained 

169
00:10:46,551 --> 00:10:46,551
entity.
The landmine.

170
00:10:46,551 --> 00:10:50,130
Whereas as we move in this direction,
maybe we see more dispersal of functions

171
00:10:50,131 --> 00:10:51,750
through different three different 
systems.

172
00:10:51,751 --> 00:10:56,751
And I think that's another dynamic that 
when we think about the development of 

173
00:10:56,751 --> 00:11:00,860
autonomy and weapon systems,
it might not all live in one place 

174
00:11:00,860 --> 00:11:02,130
physically moving around in one place.
It can be,

175
00:11:02,340 --> 00:11:07,340
uh,
an array of different systems 

176
00:11:07,340 --> 00:11:07,340
functioning in different,
in different places,

177
00:11:07,830 --> 00:11:12,830
uh,
perhaps for people with a sort of ai 

178
00:11:12,830 --> 00:11:13,350
type of mindset.
Maybe there's some sort of movement from

179
00:11:13,710 --> 00:11:16,890
more specific types of ai functioning 
here,

180
00:11:17,280 --> 00:11:22,280
use of different specific ai functions 
here to something more general going in 

181
00:11:22,280 --> 00:11:26,001
this direction.
I'm wary of necessarily buying straight 

182
00:11:26,001 --> 00:11:26,970
forward into that,
but maybe you could see some movement in

183
00:11:26,971 --> 00:11:28,410
that sort of direction.

184
00:11:29,190 --> 00:11:32,370
So I just want to sort of put this to 
one side for now,

185
00:11:32,371 --> 00:11:37,371
but we'll come back to it and think 
about some systems that are existing 

186
00:11:37,371 --> 00:11:40,610
here that I think sort of raise issues 
for us and around which we could expand 

187
00:11:40,610 --> 00:11:41,070
some,
some models.

188
00:11:41,400 --> 00:11:43,380
But I just wanted to have this in mind 
when thinking about this.

189
00:11:43,381 --> 00:11:48,381
So we're not necessarily,
we're definitely not for me thinking 

190
00:11:48,381 --> 00:11:50,820
about a humanoid robots walking around 
fighting like a soldier,

191
00:11:51,080 --> 00:11:56,080
a rather.
We're thinking about developments and 

192
00:11:56,080 --> 00:11:59,071
trajectories.
We can see coming out of established 

193
00:11:59,071 --> 00:12:02,430
military systems now,
so I was going to talk now a bit about 

194
00:12:03,701 --> 00:12:08,701
the political and the legal context.
Obviously there's a lot of complexity in

195
00:12:09,641 --> 00:12:14,641
the worlds of politics and legal 
structures and I don't want to get too 

196
00:12:14,641 --> 00:12:18,121
bogged down in it,
but I think in terms of understanding 

197
00:12:18,121 --> 00:12:21,330
the basics of this,
a debate on the international landscape 

198
00:12:21,330 --> 00:12:22,000
have to have a bit of background in that
area.

199
00:12:23,770 --> 00:12:28,770
Essentially there's,
I think three main types of 

200
00:12:28,770 --> 00:12:30,460
international law that we're concerned 
with here and again,

201
00:12:30,461 --> 00:12:33,250
concerned with international law rather 
than domestic legislation,

202
00:12:33,251 --> 00:12:37,390
which any individual state can put in 
place whatever domestic legislation they

203
00:12:37,391 --> 00:12:39,580
want.
We're looking at the international legal

204
00:12:39,610 --> 00:12:44,610
landscape.
Basically you have international human 

205
00:12:44,610 --> 00:12:47,851
rights law which applies in pretty much 
all circumstances and it involves the 

206
00:12:47,920 --> 00:12:49,660
right to life and the right to dignity 
and,

207
00:12:49,661 --> 00:12:50,380
uh,
various other,

208
00:12:51,150 --> 00:12:53,110
uh,
legal protections people.

209
00:12:54,370 --> 00:12:59,370
And then particularly prominent in this 
debate if you have what's called 

210
00:12:59,370 --> 00:13:02,431
international humanitarian law,
which is the rules that govern behavior 

211
00:13:02,431 --> 00:13:07,231
during armed conflict and provide 
obligations on military is engaged in 

212
00:13:07,231 --> 00:13:08,590
armed conflict for how they have to 
conduct themselves.

213
00:13:09,190 --> 00:13:14,190
This isn't the legal framework that 
decides whether it's okay to have a war 

214
00:13:14,190 --> 00:13:17,611
or not.
This is a legal framework that once you 

215
00:13:17,611 --> 00:13:20,550
have in the war,
this is the obligations that you've got 

216
00:13:20,550 --> 00:13:20,550
to,
you've got to follow.

217
00:13:20,550 --> 00:13:21,940
And it,
it basically includes rules that say,

218
00:13:21,990 --> 00:13:26,990
you know,
you're not allowed to directly kill 

219
00:13:26,990 --> 00:13:29,101
civilians.
You've got to aim your military efforts 

220
00:13:29,101 --> 00:13:29,950
at the forces of the,
of the enemy that enemy competence.

221
00:13:31,210 --> 00:13:33,880
You're not allowed to kill civilians 
directly or deliberately,

222
00:13:34,060 --> 00:13:39,060
but you are allowed to kill some 
civilians as long as you don't kill too 

223
00:13:39,060 --> 00:13:41,320
many of them for the military advantage 
that you're trying to achieve.

224
00:13:41,321 --> 00:13:43,570
So there's a sort of balancing acts like
this.

225
00:13:43,571 --> 00:13:48,571
This is called proportionality.
Nobody ever really knows where the 

226
00:13:48,571 --> 00:13:49,990
balance lies,
but it's a,

227
00:13:50,050 --> 00:13:51,550
it's a sort of principle of the law 
that,

228
00:13:51,551 --> 00:13:53,890
uh,
while she can kill civilians,

229
00:13:53,891 --> 00:13:56,680
you mustn't kill an excessive number of 
civilians.

230
00:13:58,330 --> 00:14:01,000
These are general rules.
These apply pretty much to all states,

231
00:14:01,030 --> 00:14:01,680
uh,
in,

232
00:14:01,681 --> 00:14:01,980
in,
um,

233
00:14:02,020 --> 00:14:07,020
conflict situations.
And then you have treaties on specific 

234
00:14:07,020 --> 00:14:08,410
weapon,
a specific weapon types.

235
00:14:08,470 --> 00:14:13,470
And this is really where you have 
weapons that are considered to be 

236
00:14:13,470 --> 00:14:16,891
particularly problematic in some way,
and it's decided a group of states 

237
00:14:16,891 --> 00:14:18,750
decides to develop and put in place 
agree,

238
00:14:18,790 --> 00:14:19,980
a treaty that,
uh,

239
00:14:20,230 --> 00:14:23,170
that applies specifically to those,
to those weapons.

240
00:14:24,190 --> 00:14:29,190
I think it's important to recognize that
the illegal treaties are all developed 

241
00:14:31,271 --> 00:14:36,271
and agreed by states that are agreed by 
international governments talking 

242
00:14:36,271 --> 00:14:40,561
together,
negotiating what they think the law 

243
00:14:40,561 --> 00:14:43,051
should say.
And they generally only bind on states 

244
00:14:43,051 --> 00:14:44,380
if they choose to adopt that legal 
instrument.

245
00:14:44,381 --> 00:14:45,850
So,
uh,

246
00:14:46,420 --> 00:14:51,420
I guess what I'm emphasizing there is a 
sense that these are sort of social 

247
00:14:51,420 --> 00:14:52,970
products in a way that political 
products,

248
00:14:53,330 --> 00:14:56,210
it isn't the sort of magical law that's 
come down from on high,

249
00:14:56,211 --> 00:15:00,140
perfectly written to match the needs of 
humanity.

250
00:15:00,380 --> 00:15:05,380
It's a negotiated outcome developed by a
complicated set of actors who may or may

251
00:15:06,411 --> 00:15:08,270
not agree with each other and all sorts 
of things.

252
00:15:09,020 --> 00:15:14,020
And what I means is there's quite a lot 
of wiggle room in these legal 

253
00:15:14,020 --> 00:15:16,661
frameworks.
And quite a lot of uncertainty within 

254
00:15:16,661 --> 00:15:19,260
the lawyers of international 
humanitarian law will tell you that's 

255
00:15:19,260 --> 00:15:19,260
not true.

256
00:15:19,260 --> 00:15:23,320
But that's because there are 
particularly keen on that legal 

257
00:15:23,320 --> 00:15:23,690
framework,
but in reality there's a lot of,

258
00:15:23,691 --> 00:15:25,330
um,
a lot of fuzziness to,

259
00:15:25,360 --> 00:15:28,240
to what some of the legal provisions or 
some of the legal provisions say.

260
00:15:29,180 --> 00:15:33,110
And it also means that the extent to 
which this law binds on people and bears

261
00:15:33,111 --> 00:15:36,320
on people is also require some social 
enactment.

262
00:15:36,620 --> 00:15:40,790
There's not a sort of world police who,
who could follow up on all of these,

263
00:15:40,791 --> 00:15:42,380
uh,
these legal frameworks.

264
00:15:42,620 --> 00:15:47,620
It requires a sort of social function 
from states and from other actors to 

265
00:15:47,620 --> 00:15:50,690
keep articulating a sense of the 
importance of these legal rules and keep

266
00:15:50,691 --> 00:15:52,390
trying to put pressure on other factors 
to,

267
00:15:52,610 --> 00:15:57,440
to a chord with them.
So the issue of autonomous weapons is in

268
00:15:57,441 --> 00:16:02,441
discussion at the United Nations under a
framework called the UN Convention on 

269
00:16:02,441 --> 00:16:04,790
conventional weapons.
And this is a body that has the capacity

270
00:16:04,791 --> 00:16:09,791
to agree new protocols,
new treaties essentially on specific 

271
00:16:09,791 --> 00:16:10,460
weapon systems.

272
00:16:10,940 --> 00:16:13,880
And that means that diplomats from lots 
of countries,

273
00:16:13,881 --> 00:16:16,220
diplomats from the US,
from the UK,

274
00:16:16,430 --> 00:16:20,120
from Russia,
and Brazil and China and other countries

275
00:16:20,121 --> 00:16:23,270
of the world will be sitting around in a
conference room,

276
00:16:23,730 --> 00:16:28,730
put it forward that perspectives on this
issue and trying to find common ground 

277
00:16:28,730 --> 00:16:33,220
or trying not to find common ground,
just depending on what sort of outcome 

278
00:16:33,220 --> 00:16:33,650
that they're working towards.
So,

279
00:16:34,180 --> 00:16:39,180
you know,
the UN isn't a completely separate 

280
00:16:39,180 --> 00:16:39,830
entity of its own.
It's just the community of states in the

281
00:16:39,831 --> 00:16:43,160
world sitting together talking about 
talking about things.

282
00:16:44,540 --> 00:16:49,540
So main focus of concern and those 
discussions when it comes to autonomy is

283
00:16:50,150 --> 00:16:52,850
not some sort of generalized,
uh,

284
00:16:53,360 --> 00:16:56,780
autonomy,
autonomy of all of its forms that may be

285
00:16:56,781 --> 00:16:59,030
pertinent in the military space.
It's,

286
00:16:59,060 --> 00:17:03,200
it's rather much more these,
these questions of how the targets of an

287
00:17:03,201 --> 00:17:04,700
attack,
a selected,

288
00:17:05,090 --> 00:17:06,410
uh,
identified,

289
00:17:06,470 --> 00:17:11,470
decided upon.
And how is the decision to apply force 

290
00:17:11,470 --> 00:17:12,420
to those targets made.
And it's really a,

291
00:17:12,440 --> 00:17:17,440
these sort of the critical functions of 
weapons systems where the movement 

292
00:17:17,440 --> 00:17:20,810
towards greater autonomy is considered a
source of anxiety,

293
00:17:20,811 --> 00:17:25,520
essentially that we may see machines 
making decisions on what is a target for

294
00:17:25,521 --> 00:17:30,260
an attack and choosing when and where 
forces apply to that specific.

295
00:17:30,360 --> 00:17:31,520
So that specific target.

296
00:17:34,760 --> 00:17:39,760
So obviously in this context,
not everybody is like minded on this.

297
00:17:40,851 --> 00:17:45,851
There are potential advantages to 
increasing autonomy in weapons systems 

298
00:17:45,851 --> 00:17:48,750
and there's potential disadvantages and,
and,

299
00:17:48,900 --> 00:17:50,480
uh,
problems associated with it.

300
00:17:50,490 --> 00:17:52,710
Then within the,
within this,

301
00:17:52,711 --> 00:17:54,090
uh,
international discussion,

302
00:17:54,360 --> 00:17:57,240
we say different perspectives laid out 
on some states.

303
00:17:57,241 --> 00:18:00,540
Of course we'll be able to see some,
some advantages and some disadvantages.

304
00:18:00,541 --> 00:18:05,541
It's not a black and white sort of a 
discussion in terms of the possible 

305
00:18:05,671 --> 00:18:10,290
advantages for autonomy.
I mean one of the key ones ultimately is

306
00:18:10,291 --> 00:18:15,291
framed in terms of military advantage 
that we want to have more autonomy and 

307
00:18:15,291 --> 00:18:19,521
weapons systems because it will maintain
or give us military advantage over 

308
00:18:19,521 --> 00:18:24,170
possible adversaries.
Because in the end military stuff is 

309
00:18:24,170 --> 00:18:24,170
about winning wars,
right?

310
00:18:24,170 --> 00:18:28,821
So you want to maintain military 
advantage and military advantage number 

311
00:18:29,881 --> 00:18:33,360
of factors really within that speed is 
one of them.

312
00:18:33,370 --> 00:18:38,370
Speed of decision making can computerize
autonomous systems make decisions about 

313
00:18:38,971 --> 00:18:42,840
where to apply force faster than a human
would be capable of doing.

314
00:18:42,910 --> 00:18:46,850
And therefore this is a,
this is advantageous for us.

315
00:18:47,390 --> 00:18:48,680
Uh,
also,

316
00:18:48,700 --> 00:18:50,970
speed allows for coordination of 
numbers.

317
00:18:50,971 --> 00:18:55,260
So if you want to have swarms of a 
systems,

318
00:18:55,670 --> 00:19:00,670
you know,
swarms of small drones or some such a 

319
00:19:00,670 --> 00:19:04,700
unique,
quite a lot of probably autonomy in 

320
00:19:04,700 --> 00:19:04,830
decision making and communication 
between those systems because again,

321
00:19:04,831 --> 00:19:07,310
the level of complexity and the speed 
involved is,

322
00:19:07,600 --> 00:19:11,580
is greater than a human would be able to
sort of manually a engineer.

323
00:19:11,581 --> 00:19:16,581
So speed,
both in terms of responding to external 

324
00:19:16,581 --> 00:19:17,460
effects,
but also coordinating your own forces,

325
00:19:18,720 --> 00:19:20,140
reach,
um,

326
00:19:20,730 --> 00:19:25,730
potential for autonomous systems to be 
able to operate in a communication 

327
00:19:25,730 --> 00:19:28,410
denied environments where if you're 
relying on a,

328
00:19:28,411 --> 00:19:31,920
on an electronic communications link to 
say a current arm drone,

329
00:19:32,220 --> 00:19:37,220
maybe in a future battle space where the
enemy is denying communications in some 

330
00:19:37,220 --> 00:19:41,571
way,
you could use an autonomous system to 

331
00:19:41,571 --> 00:19:44,361
still fulfill a mission without a,
without needing to rely on that 

332
00:19:44,361 --> 00:19:47,610
communications infrastructure,
general force multiplication.

333
00:19:47,610 --> 00:19:52,610
There's a bit of a sense that there's 
going to be more and more teaming of 

334
00:19:52,610 --> 00:19:52,610
machines with,
with humans,

335
00:19:52,610 --> 00:19:54,900
so machines operating alongside humans 
in the battle space.

336
00:19:56,850 --> 00:20:01,850
And then this importantly as it presents
it at least a sense that these are 

337
00:20:01,850 --> 00:20:03,780
systems which could allow you to reduce 
the risk to your own forces.

338
00:20:03,781 --> 00:20:08,781
That maybe if we can put some sort of 
autonomous robotic system at work in a 

339
00:20:08,781 --> 00:20:11,100
specific environment,
then we don't need to put one of our own

340
00:20:11,101 --> 00:20:13,470
soldiers in that position.
And as a result,

341
00:20:13,471 --> 00:20:15,660
we're less likely to have casualties 
coming home.

342
00:20:15,661 --> 00:20:20,661
Which of course politically is a 
problematic for maintaining any sort of 

343
00:20:20,661 --> 00:20:24,210
conflict,
posture set against all that stuff.

344
00:20:24,240 --> 00:20:29,240
Um,
there's a sense that I think most 

345
00:20:29,240 --> 00:20:31,680
fundamentally this perhaps a moral 
hazard that we come across at some point

346
00:20:31,820 --> 00:20:36,820
that the,
some sort of a boundary where seeing or 

347
00:20:38,131 --> 00:20:43,131
conceptualizing a situation where 
machines are deciding who to kill in a 

348
00:20:43,231 --> 00:20:47,710
certain context is just somehow wrong 
and well,

349
00:20:47,740 --> 00:20:50,260
that's not a very easy argument to just 
start,

350
00:20:50,320 --> 00:20:55,320
you know,
articulate in a sort of rationalized 

351
00:20:55,320 --> 00:20:56,560
sense,
but there's some sort of moral revulsion

352
00:20:56,620 --> 00:21:01,150
that perhaps it comes about at this 
sense that machines are now deciding who

353
00:21:01,151 --> 00:21:03,880
should be killed in a particular,
in a particular environment.

354
00:21:04,740 --> 00:21:07,480
Um,
there's a set of legal concerns.

355
00:21:08,380 --> 00:21:13,380
Can these systems be used in accordance 
with the existing legal obligations?

356
00:21:14,270 --> 00:21:19,270
And I'm going to come on a little bit 
later to our orientation in the legal 

357
00:21:19,270 --> 00:21:19,270
side,
which is,

358
00:21:19,270 --> 00:21:23,740
is also about how they may stretch the 
fabric of the law and the structure of 

359
00:21:23,740 --> 00:21:23,770
the law.
As we say it.

360
00:21:24,550 --> 00:21:28,540
The some concerns in this sort of legal 
arguments for me that,

361
00:21:28,541 --> 00:21:33,541
um,
we sometimes slip into a language of 

362
00:21:33,541 --> 00:21:35,170
talking about machines making legal 
decisions.

363
00:21:35,410 --> 00:21:39,700
Will a machine be able to apply the rule
of proportionality properly?

364
00:21:40,750 --> 00:21:43,030
There's dangers in that.
I know what it means,

365
00:21:43,420 --> 00:21:46,870
but at the same time the law is 
addressed to humans.

366
00:21:46,871 --> 00:21:51,871
The law isn't addressed to machines.
So it's humans who have the obligation 

367
00:21:51,871 --> 00:21:51,871
to,
uh,

368
00:21:52,000 --> 00:21:57,000
an act,
the legal obligation and machine may do 

369
00:21:57,000 --> 00:21:58,120
a function that is sort of analogous to 
that legal decision.

370
00:21:58,480 --> 00:22:03,480
But ultimately to my mind is still a 
human who has to be making the legal 

371
00:22:03,480 --> 00:22:06,430
determination based on some predict 
prediction of what that machine will do.

372
00:22:06,660 --> 00:22:08,830
And I think this is a very dangerous 
slippage because even,

373
00:22:09,270 --> 00:22:14,270
you know,
senior legal academics can slip into 

374
00:22:14,270 --> 00:22:17,881
this mindset which is,
which is a little bit like handing over 

375
00:22:17,881 --> 00:22:21,600
the legal framework to machines before 
you've even got onto arguing about what 

376
00:22:21,600 --> 00:22:23,770
we should or shouldn't have.
So we need to be careful in that area.

377
00:22:23,771 --> 00:22:28,771
And it's a little bit to do with,
for me continuing to treat these 

378
00:22:28,771 --> 00:22:31,120
technologies as machines rather than 
into treating them as agents in some way

379
00:22:31,121 --> 00:22:35,710
of sort of equal or equivalent or 
similar moral standing to,

380
00:22:36,040 --> 00:22:41,040
to humans.
And then we have a whole set of wider 

381
00:22:41,040 --> 00:22:42,190
concerns that are raised.
So we've got moral anxieties,

382
00:22:42,250 --> 00:22:47,250
a legal concerns,
and then a set of other concerns around 

383
00:22:47,250 --> 00:22:47,620
risks,
uh,

384
00:22:47,621 --> 00:22:52,621
that could be unpredictable risks,
does the sort of normal accidents 

385
00:22:52,621 --> 00:22:52,621
theory.

386
00:22:52,621 --> 00:22:56,760
Maybe you've come across that stuff.
There's a bit of that sort of language 

387
00:22:56,760 --> 00:22:58,330
in the debate about complicated systems 
and not been able to,

388
00:22:58,630 --> 00:22:59,330
uh,
you know,

389
00:22:59,380 --> 00:23:04,380
not be able to avoid accidents and in 
some respects some anxieties about maybe

390
00:23:05,591 --> 00:23:09,430
this will reduce the barriers to 
engaging in military action,

391
00:23:09,431 --> 00:23:11,830
may be being able to use autonomous 
weapons,

392
00:23:11,831 --> 00:23:16,831
will make it easier to go to war.
And some anxieties about sort of 

393
00:23:16,831 --> 00:23:19,960
international security and balance of 
power and arms races and the like.

394
00:23:21,460 --> 00:23:26,460
These are all significant concerns.
I don't tend to think much in this area,

395
00:23:26,890 --> 00:23:30,400
partly because they involve quite a lot 
of speculation about what may or may not

396
00:23:30,401 --> 00:23:33,190
be in the future and they're quite 
difficult to populate with,

397
00:23:33,191 --> 00:23:38,191
um,
with sort of more grounded arguments I 

398
00:23:38,191 --> 00:23:38,191
find.
But uh,

399
00:23:38,191 --> 00:23:39,460
that doesn't mean that they are 
significant in themselves,

400
00:23:39,461 --> 00:23:41,980
but I find them less straightforward as 
an entry point.

401
00:23:43,870 --> 00:23:48,440
So been all of these different issues.
There's multiple unstable terminology,

402
00:23:48,441 --> 00:23:50,330
lots of arguments coming in,
different directions.

403
00:23:50,630 --> 00:23:55,630
And our job as an NGO in a way is we're 
trying to find ways of building a 

404
00:23:55,630 --> 00:23:59,540
constructive conversation in this 
environment which can move toward states

405
00:23:59,541 --> 00:24:04,130
adopting a more constraining orientation
to this movement towards autonomy.

406
00:24:05,810 --> 00:24:10,810
And the main tool we've used to work 
towards that so far has been to perhaps 

407
00:24:13,491 --> 00:24:18,491
stop focusing on the technology per se 
and the idea of what is autonomy and how

408
00:24:18,711 --> 00:24:23,711
much autonomy is a problem.
And to bring the focus back a bit onto 

409
00:24:23,711 --> 00:24:25,490
what is the human element that we want 
to preserve in all of this.

410
00:24:25,790 --> 00:24:30,790
Because it seems like most of the 
anxieties that come from a sense of a 

411
00:24:30,790 --> 00:24:35,741
problem with autonomous weapons are 
about some sort of absence of a human 

412
00:24:35,741 --> 00:24:40,181
element that we want to preserve,
but unless we can in some way define 

413
00:24:40,181 --> 00:24:41,030
what this human element is that we want 
to preserve,

414
00:24:41,300 --> 00:24:44,090
I'm not sure we can expect to define its
absence.

415
00:24:44,090 --> 00:24:49,090
Very straightforward.
So I kind of feel like we want to pull 

416
00:24:49,090 --> 00:24:49,640
the discussion onto a focus on the,
on the human,

417
00:24:49,660 --> 00:24:50,990
uh,
on the human element.

418
00:24:51,540 --> 00:24:56,540
Um,
and the tool we've used for this so far 

419
00:24:56,540 --> 00:24:59,000
has been basically a terminology about 
the need for meaningful human control.

420
00:24:59,270 --> 00:25:04,010
And this is just a form of words that 
we've sort of introduced into the debate

421
00:25:04,310 --> 00:25:09,310
and we've promoted it in discussions 
with diplomats and with different 

422
00:25:09,310 --> 00:25:12,731
actors.
And we've built up the idea of this 

423
00:25:12,731 --> 00:25:12,731
terminology as,
as being a sort of tool.

424
00:25:12,740 --> 00:25:14,230
It's a bit like a meme,
right?

425
00:25:14,231 --> 00:25:15,660
You,
you create the,

426
00:25:15,690 --> 00:25:19,580
the terms and then you use that to sort 
of structure the discussion in a,

427
00:25:19,810 --> 00:25:21,890
in a productive,
in a productive way.

428
00:25:22,700 --> 00:25:26,150
One of the reasons I like it is it works
partly because the word meaningful,

429
00:25:26,151 --> 00:25:31,151
it doesn't mean anything particular or 
at least it means whatever you might 

430
00:25:31,151 --> 00:25:32,450
want it to mean.

431
00:25:32,720 --> 00:25:33,590
And I,
I find that,

432
00:25:33,591 --> 00:25:34,180
uh,
uh,

433
00:25:34,260 --> 00:25:36,850
an enjoyable sort of tension and in 
that,

434
00:25:36,860 --> 00:25:40,790
but the term meaningful human control 
has been quite well picked up in and the

435
00:25:40,791 --> 00:25:44,720
literature on this issue and in the 
diplomatic discourse and it's helping to

436
00:25:44,721 --> 00:25:46,880
structure us towards a,
uh,

437
00:25:46,970 --> 00:25:51,970
what we think are the key questions,
basic arguments for the idea of 

438
00:25:52,821 --> 00:25:54,590
meaningful human control from my 
perspective,

439
00:25:54,860 --> 00:25:59,860
a quite simple and intended to use 
basically a sort of absurdist sort of 

440
00:26:01,100 --> 00:26:05,240
logic if there is such a thing.
First of all,

441
00:26:05,241 --> 00:26:10,241
really to recognize that no governments 
are in favor of an autonomous weapon 

442
00:26:10,491 --> 00:26:13,520
system that has no human control 
whatsoever,

443
00:26:13,521 --> 00:26:18,521
right?
Nobody is arguing that it would be a 

444
00:26:18,521 --> 00:26:20,810
good idea for us to have some sort of 
autonomous weapon that just flies around

445
00:26:20,811 --> 00:26:25,370
the world deciding to kill people.
We don't know who it's gonna count a why

446
00:26:25,700 --> 00:26:27,800
it doesn't have to report back to us,
but you know,

447
00:26:28,220 --> 00:26:29,900
we're in favor.
Nobody's in favor of this,

448
00:26:29,901 --> 00:26:31,850
right?
This is obviously ridiculous,

449
00:26:31,851 --> 00:26:36,851
so there needs to be some form of human 
control because we can rule out that 

450
00:26:37,220 --> 00:26:42,110
sort of ridiculous extension of the 
argument and on the other hand,

451
00:26:42,890 --> 00:26:47,890
if you just have a person in a dark room
with a red light that comes on every now

452
00:26:48,251 --> 00:26:50,650
and again and they don't know anything 
else about what's going on,

453
00:26:51,040 --> 00:26:56,040
but that the human who's controlling 
this autonomous weapon and when the red 

454
00:26:56,040 --> 00:26:59,581
light comes on,
they pushed the fire button to launch a 

455
00:26:59,581 --> 00:26:59,581
rocket or something.

456
00:26:59,630 --> 00:27:02,720
We know that that isn't sufficient tumor
control either,

457
00:27:02,721 --> 00:27:04,350
right?
There's a person doing something,

458
00:27:04,351 --> 00:27:09,351
there's a person engaged in the process,
but clearly it's just some sort of 

459
00:27:09,351 --> 00:27:11,540
mechanistic,
pro forma human engagement.

460
00:27:11,541 --> 00:27:15,740
So between these two kinds of ridiculous
extremes,

461
00:27:15,800 --> 00:27:20,130
I think we get the idea that there's.
There's some sort of fuzzy,

462
00:27:20,220 --> 00:27:23,160
fuzzy line that that must exist in 
there,

463
00:27:23,460 --> 00:27:27,230
in there somewhere and that everybody 
can in some way agree to the idea that's

464
00:27:27,231 --> 00:27:32,231
such a line should exist.
So the question then for us is how to 

465
00:27:32,231 --> 00:27:37,161
move the conversation in the 
international community towards a 

466
00:27:37,161 --> 00:27:40,761
productive sort of discussion of where 
the parameters of this line might be 

467
00:27:41,070 --> 00:27:41,880
conceptualized.

468
00:27:43,820 --> 00:27:48,820
So that's brought us on to thinking 
about a more substantive side of 

469
00:27:48,820 --> 00:27:52,760
questions about what are the key 
elements of meaningful human control and

470
00:27:53,780 --> 00:27:56,420
we've laid out some basic elements.
So I'm like,

471
00:27:56,421 --> 00:27:58,340
get rid of that fuzzy line because it's 
a bit useless anyway,

472
00:27:58,341 --> 00:28:03,341
isn't it?
And then I can put my key elements on 

473
00:28:03,341 --> 00:28:03,341
here.
Well,

474
00:28:03,341 --> 00:28:06,821
one of them is I'm predictable,
reliable,

475
00:28:12,720 --> 00:28:17,720
transparent technology.
This is kind of before you get into 

476
00:28:24,290 --> 00:28:28,470
exactly what the system is going to do,
we want the technology itself to be sort

477
00:28:28,471 --> 00:28:31,140
of well made and it's,
you know,

478
00:28:31,500 --> 00:28:36,180
it's going to basically do what it says 
it's going to do whatever that is and we

479
00:28:36,181 --> 00:28:38,370
want to be able to understand it to some
extent.

480
00:28:38,760 --> 00:28:43,760
Obviously this becomes a bit of a 
challenge and some of the Ai type 

481
00:28:43,760 --> 00:28:44,400
functions where you start to have 
machine learning,

482
00:28:44,700 --> 00:28:46,980
uh,
issues and these issues of transparency.

483
00:28:46,981 --> 00:28:48,480
Perhaps they start to come up a little 
bit,

484
00:28:48,700 --> 00:28:53,700
a little bit,
but these kinds of issues in the design 

485
00:28:53,700 --> 00:28:56,571
and the development of of of systems.
Another thing we want to have is,

486
00:28:57,580 --> 00:29:01,710
and I think this is a key one,
is accurate information

487
00:29:09,120 --> 00:29:12,900
and it's accurate information on the 
intent of the commander.

488
00:29:14,800 --> 00:29:19,800
Well the outcome,
what's the outcome we're trying to 

489
00:29:19,800 --> 00:29:22,551
achieve?
How does the technology work and what's 

490
00:29:30,811 --> 00:29:35,811
the context so it is already full so it 
won't take long.

491
00:29:41,230 --> 00:29:42,100
Third one is

492
00:29:43,000 --> 00:29:48,000
timely intervention,
human intervention.

493
00:29:50,151 --> 00:29:50,680
It should be.

494
00:29:53,200 --> 00:29:58,200
It'd be good if we could turn it off 
some points if it's going to be a very 

495
00:29:58,200 --> 00:29:59,950
long acting system would be good if we 
turn it off.

496
00:30:00,370 --> 00:30:04,240
Maybe that's the fourth one is just the 
sort of framework of accountability.

497
00:30:09,890 --> 00:30:14,890
So we're thinking the basic elements of 
humor control can be broken down into 

498
00:30:15,691 --> 00:30:20,691
these areas.
Some of them have about the technology 

499
00:30:20,691 --> 00:30:21,170
itself,
how it's designed and made.

500
00:30:21,180 --> 00:30:26,180
How do you verify and validate that it's
gonna do what the manufacturers have 

501
00:30:26,180 --> 00:30:28,170
said it's going to do.
Can you understand it?

502
00:30:29,950 --> 00:30:34,950
This one I think is the key one in terms
of thinking about the issue and this is 

503
00:30:34,950 --> 00:30:35,260
what I'm going to talk about a bit more 
now.

504
00:30:35,800 --> 00:30:38,800
The accurate information on what's the 
commander's intent,

505
00:30:38,830 --> 00:30:41,200
what do you want to achieve in the use 
of this system?

506
00:30:42,940 --> 00:30:47,940
What effects is it going to have?
I mean this makes a big difference how 

507
00:30:47,940 --> 00:30:51,721
it works.
This factors here involve what are the 

508
00:30:51,721 --> 00:30:53,270
target profiles that it's going to use.
Whereas mine on the landmine,

509
00:30:53,290 --> 00:30:57,460
of course it was just pressure.
Pressure is being taken as a pressure on

510
00:30:57,461 --> 00:31:02,461
the ground is being taken as a proxy for
a military target for a human who we're 

511
00:31:02,461 --> 00:31:04,610
going to assume as a military target,
but,

512
00:31:05,410 --> 00:31:10,410
and these systems,
we're going to have different target 

513
00:31:10,410 --> 00:31:10,410
profiles,
different heat shapes,

514
00:31:10,410 --> 00:31:10,410
different,
uh,

515
00:31:10,410 --> 00:31:12,190
different patterns of data that the 
system is going to operate on.

516
00:31:12,190 --> 00:31:16,770
The basis of what sort of actual weapons
are going to use to apply force.

517
00:31:16,790 --> 00:31:20,090
It makes a difference if it's going to 
just fire a bullet from a gun or if it's

518
00:31:20,150 --> 00:31:23,390
gonna drop a 2000 pound bomb.
I mean,

519
00:31:23,400 --> 00:31:28,400
that has a different effect.
And the way in which you envision sort 

520
00:31:28,400 --> 00:31:31,220
of control for those effects is going to
be different in those different cases.

521
00:31:32,490 --> 00:31:33,990
And finally,
very importantly,

522
00:31:34,410 --> 00:31:39,410
these issues of context,
information on the context in which the 

523
00:31:39,410 --> 00:31:40,650
system will will operate.
Okay.

524
00:31:41,880 --> 00:31:46,880
Contexts of course,
includes all the going to be civilians 

525
00:31:46,880 --> 00:31:47,250
present in the area.
Can you assess,

526
00:31:47,490 --> 00:31:52,490
are they going to be other objects in 
the area that may present a similar 

527
00:31:52,490 --> 00:31:53,750
pattern to the proxy data?
You know,

528
00:31:53,751 --> 00:31:55,870
if you're using a heat shape of a 
vehicle engine,

529
00:31:56,640 --> 00:31:58,470
oh,
it might be aimed at a tank,

530
00:31:58,710 --> 00:32:02,230
but if there's an ambulance in the same 
location is an ambulance,

531
00:32:02,231 --> 00:32:07,231
is vehicle engine heat shape 
sufficiently similar to the tank to 

532
00:32:07,470 --> 00:32:11,970
cause some confusion between the two 
sets of information like that?

533
00:32:13,810 --> 00:32:15,570
And context of course varies in 
different.

534
00:32:15,650 --> 00:32:20,650
You know,
obviously varies in different 

535
00:32:20,650 --> 00:32:20,650
environments.
But I think we can see different domains

536
00:32:20,650 --> 00:32:23,871
in this area as well,
which is significant operating in the 

537
00:32:23,871 --> 00:32:27,631
water or in the ocean.
You've probably got a less cluttered 

538
00:32:27,631 --> 00:32:30,481
environment or less complex environment 
than if you're operating in an urban in 

539
00:32:30,481 --> 00:32:33,961
an urban area.
So that's another factor that needs to 

540
00:32:33,961 --> 00:32:33,970
be taken into,
into accountant in this.

541
00:32:35,400 --> 00:32:40,400
So I just wanted to talk a little bit 
about some existing perhaps and think 

542
00:32:41,901 --> 00:32:46,490
about them in the context of this,
these sort of set of issues here.

543
00:32:50,700 --> 00:32:55,700
One system that you may maybe aware of 
is it's on a boat.

544
00:33:02,550 --> 00:33:07,400
Um,
okay.

545
00:33:07,430 --> 00:33:10,310
So,
so then like the Phalanx,

546
00:33:12,680 --> 00:33:14,330
anti missile system,
it's on a boats,

547
00:33:14,340 --> 00:33:16,000
but there's various anti missile 
systems.

548
00:33:16,001 --> 00:33:21,001
I mean it doesn't,
the details don't matter in this 

549
00:33:21,001 --> 00:33:23,051
context.
These are systems that human terms that 

550
00:33:23,051 --> 00:33:27,091
aren't so human is choosing when to turn
it on and a human turns it off again.

551
00:33:29,540 --> 00:33:34,540
But when it's operating it's the radar 
is basically scanning and area of an 

552
00:33:34,540 --> 00:33:39,371
area of sky up here.
And it's looking for fast moving 

553
00:33:40,500 --> 00:33:45,500
incoming objects because basically it's 
designed to automatically shoot down 

554
00:33:45,500 --> 00:33:46,280
incoming missiles.
Rockets,

555
00:33:46,281 --> 00:33:50,540
right?
So thinking about these characteristics,

556
00:33:51,790 --> 00:33:56,790
you know,
what the outcome you want is you want 

557
00:33:56,790 --> 00:33:58,511
your boat not to get blown up by an 
incoming missile and you want to shoot 

558
00:33:58,511 --> 00:34:00,820
down any incoming missiles.
You know,

559
00:34:00,830 --> 00:34:05,830
how the technology works because you 
know that it's basically using radar to 

560
00:34:05,830 --> 00:34:07,210
see incoming fast moving,
uh,

561
00:34:07,340 --> 00:34:12,260
signatures and you have a pretty good 
idea of the context because the sky is a

562
00:34:12,261 --> 00:34:17,261
fairly uncluttered comparatively and 
you'd like to think that any fast moving

563
00:34:19,011 --> 00:34:24,011
incoming objects towards your hair are 
probably going to be a incoming 

564
00:34:24,011 --> 00:34:25,970
missiles,
not guaranteed to be the case.

565
00:34:26,030 --> 00:34:27,490
One of these systems shut down,
uh,

566
00:34:27,590 --> 00:34:29,570
an Iranian passenger airliner,
but,

567
00:34:29,571 --> 00:34:30,740
uh,
by accident,

568
00:34:30,800 --> 00:34:32,600
which is obviously a significant 
accident,

569
00:34:33,010 --> 00:34:37,160
um,
but basically you have a sentence of,

570
00:34:37,220 --> 00:34:39,680
you know,
the fact that the data that you're using

571
00:34:39,681 --> 00:34:41,720
tracks pretty well to the target 
objects,

572
00:34:42,020 --> 00:34:47,020
if not absolutely precisely.
You Bet are relatively controllable 

573
00:34:47,020 --> 00:34:49,970
environments in terms of the sky and 
you've got a human being.

574
00:34:50,420 --> 00:34:52,010
The system isn't really mobile.
I mean,

575
00:34:52,011 --> 00:34:54,680
it's kind of mobile insofar as the boat 
can move around,

576
00:34:54,681 --> 00:34:57,260
but the person who's operating it as you
know,

577
00:34:58,590 --> 00:35:00,380
the mobile in the same place.
So,

578
00:35:00,381 --> 00:35:02,660
uh,
so it's relatively static.

579
00:35:04,370 --> 00:35:09,370
So I think looking at that,
you could suggest that there's still a 

580
00:35:09,370 --> 00:35:13,721
reasonable amount of human control over 
this system because when we look at it 

581
00:35:13,721 --> 00:35:14,750
in terms of a number of the functions 
here,

582
00:35:14,990 --> 00:35:16,490
we can understand how that,
um,

583
00:35:17,270 --> 00:35:22,270
how that system is being managed in a 
human controls by and although there's 

584
00:35:22,270 --> 00:35:25,691
still a degree of autonomy or at least 
it's sort of highly automated and the 

585
00:35:25,691 --> 00:35:29,681
way that it actually identifies the 
targets and moves the gun and shoots 

586
00:35:29,681 --> 00:35:31,430
down the incoming object.
The basic framework is volume,

587
00:35:31,431 --> 00:35:33,380
which I fail like it.
I mean,

588
00:35:33,381 --> 00:35:38,381
it's not for me to say,
but I feel like I'm still a reasonable 

589
00:35:38,381 --> 00:35:38,700
amount of human controlled is bang,
Bang,

590
00:35:38,710 --> 00:35:41,130
applaud.
Okay.

591
00:35:41,131 --> 00:35:46,131
Another sort of system.
I'm gonna draw some tanks or something 

592
00:35:46,131 --> 00:35:49,240
now say,
okay,

593
00:35:49,830 --> 00:35:54,120
well I'm just going to draw them out a 
lot because otherwise it's too long.

594
00:35:57,380 --> 00:36:02,310
Days of tanks all but fighting vehicles.
Ignore the graphic design skills.

595
00:36:02,620 --> 00:36:06,150
Um,
there a sense of use weapons systems,

596
00:36:06,480 --> 00:36:09,660
Blah commander,
a significant distance,

597
00:36:09,720 --> 00:36:14,720
right?
Can't necessarily see the location of 

598
00:36:14,720 --> 00:36:14,720
the,
of the tanks,

599
00:36:14,720 --> 00:36:17,580
but they know that the cement tanks in 
this area over here,

600
00:36:17,750 --> 00:36:22,750
right,
and maybe they have some sense of what 

601
00:36:22,861 --> 00:36:24,870
this area is that aren't in the middle 
of a town.

602
00:36:25,170 --> 00:36:27,840
They're out in the open so they have an 
understanding of the context,

603
00:36:27,841 --> 00:36:29,760
but maybe not a detailed understanding 
of the context.

604
00:36:30,720 --> 00:36:35,720
So the weapons system is going to file 
multiple warheads into this target area.

605
00:36:36,480 --> 00:36:39,930
The commander is decided upon the target
of the attack,

606
00:36:40,410 --> 00:36:45,410
this group of tanks here,
but as the war heads approached the 

607
00:36:45,410 --> 00:36:49,730
target area,
the warheads are going to communicate 

608
00:36:49,730 --> 00:36:51,501
amongst themselves and that kind of 
allocate it themselves to the specific 

609
00:36:52,050 --> 00:36:57,050
to the specific objects.
And I got to detect the heat shape of 

610
00:36:57,050 --> 00:36:58,230
the vehicles engines.

611
00:36:58,700 --> 00:37:03,010
They're going to match that with some 
profile that says this is a enemy almost

612
00:37:03,040 --> 00:37:04,770
fighting vehicle as far as we're 
concerned.

613
00:37:05,640 --> 00:37:08,700
And then they're going to apply force 
downwards from the,

614
00:37:08,701 --> 00:37:13,701
uh,
using a better of explosive engineering 

615
00:37:13,701 --> 00:37:16,491
shape charge which focus a plus a plus 
to have explosive basically a jet of 

616
00:37:16,770 --> 00:37:19,590
explosives downwards onto the specific 
targets.

617
00:37:21,000 --> 00:37:26,000
And so in this situation,
well have the,

618
00:37:28,710 --> 00:37:33,710
has the weapon system chosen the target?
It's a bit ambiguous because as long as 

619
00:37:34,591 --> 00:37:37,770
we conceptualize the group of tanks as 
the as the target,

620
00:37:37,800 --> 00:37:42,800
then a human has chosen the target and 
the weapon system is essentially just 

621
00:37:42,810 --> 00:37:46,500
being efficient in its distribution of 
force to the target objects.

622
00:37:47,400 --> 00:37:50,820
But if we see the individual vehicles as
individual targets,

623
00:37:50,821 --> 00:37:55,821
maybe the weapon system has chosen a,
the targets potentially some advantages 

624
00:37:56,311 --> 00:37:59,250
of automated of autonomy in this 
situation.

625
00:37:59,700 --> 00:38:04,700
From my perspective,
this kind of ability to focus a jet of 

626
00:38:05,161 --> 00:38:09,150
explosive force directly on the object 
that you're looking to strike so long as

627
00:38:09,151 --> 00:38:10,170
you've got the right object.

628
00:38:10,530 --> 00:38:14,430
This is much better than setting off 
lots of artillery shells in this area,

629
00:38:14,431 --> 00:38:19,431
which would have a much greater 
explosive force effect on the 

630
00:38:19,431 --> 00:38:20,850
surrounding area.
Probably put a wider population at risk.

631
00:38:22,440 --> 00:38:26,730
So just sort of set of considerations 
here that I think a significant.

632
00:38:26,731 --> 00:38:29,310
So we have these systems,
these systems exist,

633
00:38:29,820 --> 00:38:34,820
exist today.
You could ask questions about whether 

634
00:38:34,820 --> 00:38:38,211
those heat shaped profiles of those 
objects sufficiently tightly tied to 

635
00:38:38,211 --> 00:38:42,780
energy fighting vehicles or whatever.
But I think it can be conceptualized 

636
00:38:42,780 --> 00:38:43,420
reasonably straightforwardly in those 
times,

637
00:38:45,070 --> 00:38:50,070
but the area where I started having a 
problem with this stuff is in the 

638
00:38:50,501 --> 00:38:55,501
potential for this circle or this 
pattern just to get bigger and bigger 

639
00:38:57,071 --> 00:39:00,430
essentially because it's all reasonably 
straightforward.

640
00:39:00,431 --> 00:39:04,150
When you put the tanks reasonably close 
together and you can envisage having one

641
00:39:04,570 --> 00:39:09,570
sort of one set of information about 
this area which allows you to make a 

642
00:39:09,570 --> 00:39:10,570
legal determination is that you need to 
make.

643
00:39:10,990 --> 00:39:14,650
But once these tanks get spread out over
a much larger area and you have a weapon

644
00:39:14,651 --> 00:39:19,651
system that using basically the same 
sorts of technological approach is able 

645
00:39:20,081 --> 00:39:25,081
to cover a substantially wider area of 
enemy terrain over a longer period of 

646
00:39:25,081 --> 00:39:29,821
time.
Then it suddenly gets much more 

647
00:39:29,821 --> 00:39:32,011
difficult for the.
For the military commander to have any 

648
00:39:32,011 --> 00:39:35,880
really detailed information about the 
context in which force will actually be 

649
00:39:35,880 --> 00:39:36,160
applied.
And for me,

650
00:39:36,161 --> 00:39:41,161
this is.
I think the main point of anxiety or 

651
00:39:41,161 --> 00:39:43,820
point of concern that I have in the way 
in which autonomy and weapons systems 

652
00:39:43,820 --> 00:39:46,861
is,
is likely to develop over the immediate 

653
00:39:46,861 --> 00:39:48,430
future.
Because under the legal framework,

654
00:39:48,431 --> 00:39:53,431
a military commander has an obligation 
to apply certain rules in an attack and 

655
00:39:53,701 --> 00:39:58,180
an attack is.
It's not precisely defined,

656
00:39:58,600 --> 00:39:59,980
but it needs to have some,
I think,

657
00:39:59,981 --> 00:40:04,981
some spatial and conceptual boundaries 
to it that allow a sufficient 

658
00:40:04,981 --> 00:40:06,310
granularity of legal application.

659
00:40:07,000 --> 00:40:09,820
Because if you.
If you treat this as an attack,

660
00:40:09,880 --> 00:40:11,710
I think that's fine as you expand it 
out,

661
00:40:11,740 --> 00:40:15,640
so you've got vehicles across a whole 
wide area of a country,

662
00:40:16,180 --> 00:40:21,180
say across the country as a whole,
using the same sort of extension logic 

663
00:40:21,180 --> 00:40:24,781
has as in some previous arguments.
Once you've got vehicles across the 

664
00:40:24,781 --> 00:40:26,140
whole country and you're saying,
in this attack,

665
00:40:26,141 --> 00:40:31,141
I'm going to just talk at the vehicles 
of the enemy and you send out your 

666
00:40:31,141 --> 00:40:32,100
warheads across the whole location.
Now,

667
00:40:32,101 --> 00:40:34,600
I don't think that's gonna happen in the
immediate term,

668
00:40:34,601 --> 00:40:38,830
but I'm just using that as a sort of 
conceptual challenge.

669
00:40:39,310 --> 00:40:42,880
You start to have applications of actual
physical force in all sorts of locations

670
00:40:42,881 --> 00:40:47,881
where a commander really can't assess in
any realistic way what the actual 

671
00:40:47,881 --> 00:40:50,670
effects of that are going to be.
And I think at that point you can't.

672
00:40:50,710 --> 00:40:55,710
You can no longer say that there is 
sufficient a human control being being 

673
00:40:55,710 --> 00:40:59,761
applied,
so this capacity of ai enabled systems 

674
00:41:00,521 --> 00:41:05,521
or ai driven systems to expand the 
attacks across a much wider geographical

675
00:41:05,741 --> 00:41:10,741
area and potentially over a longer 
period of time I think is a significant 

676
00:41:10,741 --> 00:41:13,510
challenge to how the legal framework is,
is understood at present,

677
00:41:13,810 --> 00:41:18,810
not one that relies upon determinations 
about whether this weapon system will 

678
00:41:18,810 --> 00:41:23,520
apply the rules properly or not,
but rather one which involves the 

679
00:41:24,460 --> 00:41:29,460
frequency and the proximity of human 
decision making to be sort of diluted a 

680
00:41:29,590 --> 00:41:31,930
progressively over progressively over 
time.

681
00:41:31,960 --> 00:41:35,200
So that's a significant area of concern 
for me.

682
00:41:36,020 --> 00:41:41,020
Final sort of set of concerns in these 
areas is around these issues about 

683
00:41:41,020 --> 00:41:44,210
encoding of targets.
I think we could say pretty clearly that

684
00:41:44,240 --> 00:41:49,240
weight is a very mega,
a basis for evaluating whether something

685
00:41:51,021 --> 00:41:52,880
is a valid military target.

686
00:41:52,880 --> 00:41:55,150
All right,
um,

687
00:41:55,190 --> 00:42:00,190
the significant problems,
but it's suggesting that we could just 

688
00:42:00,190 --> 00:42:03,011
take the weight of something as being 
sufficient for us to decide is this a 

689
00:42:03,011 --> 00:42:05,780
target or not.
In any of these processes,

690
00:42:06,020 --> 00:42:11,020
we have to decide that certain patterns 
of data represent a military objects of 

691
00:42:12,231 --> 00:42:17,231
some type and of course in a way I think
what we sort of see in sort of 

692
00:42:17,231 --> 00:42:21,641
proponents of greater and greater 
autonomy and weapons systems is a sense 

693
00:42:21,641 --> 00:42:21,641
that,
well,

694
00:42:21,641 --> 00:42:25,990
as we expand the scope of this attack,
we just need to have a more 

695
00:42:25,990 --> 00:42:29,171
sophisticated system that's undertaking 
the attack that can take on more of the 

696
00:42:29,171 --> 00:42:30,940
evaluation and,
and more of this,

697
00:42:30,941 --> 00:42:35,941
um,
process of basically mapping the coding 

698
00:42:35,941 --> 00:42:38,270
of the world into a set of decisions 
about the application of force.

699
00:42:39,260 --> 00:42:42,770
But overall,
yeah,

700
00:42:42,771 --> 00:42:47,771
I'm skeptical about the way in which our
social systems are likely to go about 

701
00:42:47,771 --> 00:42:52,690
mapping people's indicators of 
identities into some sort of fixed sense

702
00:42:52,881 --> 00:42:57,881
of military objects or military targets 
as a society over the last hundred 

703
00:42:59,930 --> 00:42:59,930
years.

704
00:43:00,260 --> 00:43:05,260
There's been plenty of times where we've
applied certain labels to certain types 

705
00:43:05,271 --> 00:43:06,890
of people,
certain groups of people,

706
00:43:07,080 --> 00:43:09,380
um,
based on various indicators,

707
00:43:09,620 --> 00:43:14,620
which apparently seemed reasonable to 
some significant section of society at 

708
00:43:15,441 --> 00:43:20,441
the time.
But then ultimately I think we've 

709
00:43:20,441 --> 00:43:20,441
subsequently thought,
well,

710
00:43:20,441 --> 00:43:23,771
highly problematic.
And so I think we need to be very wary 

711
00:43:23,771 --> 00:43:26,990
of any sort of ideas of thinking that we
can encode in terms of humans,

712
00:43:26,991 --> 00:43:31,430
particularly I'm very concrete 
indicators that certain groups of people

713
00:43:31,431 --> 00:43:36,431
should be considered by the targets or 
not just going to say a couple of final 

714
00:43:37,580 --> 00:43:42,580
things about future discussions in the 
CCW.

715
00:43:43,910 --> 00:43:45,980
The chair of the Group of governmental 
experts,

716
00:43:45,981 --> 00:43:48,410
that's the body that's going discuss 
autonomous weapons,

717
00:43:48,810 --> 00:43:53,810
uh,
has us states for the next meeting we 

718
00:43:53,810 --> 00:43:55,721
should take place in April to come 
prepared with ideas about the 

719
00:43:55,721 --> 00:43:58,010
touchpoints of human machine 
interaction.

720
00:43:58,970 --> 00:44:03,970
This is a sort of code for one of the 
ways in which we can control technology.

721
00:44:04,580 --> 00:44:07,640
So I suppose from our context as an 
organization,

722
00:44:08,570 --> 00:44:13,570
we'll be looking to get states to start 
to try and lay out this kind of 

723
00:44:13,570 --> 00:44:17,141
framework as being the basis for their 
perception of the ways in which the 

724
00:44:17,141 --> 00:44:19,100
entry points to control of technology 
could be thought about.

725
00:44:19,670 --> 00:44:24,670
Again,
it's really a question of structuring 

726
00:44:24,670 --> 00:44:26,321
the debate.
We won't get into detail across all of 

727
00:44:26,321 --> 00:44:29,531
this,
but I think it's plausible that this 

728
00:44:29,531 --> 00:44:32,410
year and next we'll start to see the 
debate falling into some adoption of 

729
00:44:32,410 --> 00:44:34,890
this kind of framework,
which I think will give us some tools to

730
00:44:34,891 --> 00:44:37,620
work with.
I think at least if we start to get some

731
00:44:37,621 --> 00:44:42,621
agreement from a significant body of 
states that these are the sort of entry 

732
00:44:42,621 --> 00:44:45,860
points we should be thinking about in 
terms of control of technology that will

733
00:44:45,861 --> 00:44:50,861
give us a bit of leverage for a start 
towards suggesting an overarching 

734
00:44:50,861 --> 00:44:55,301
obligation that there should be some 
sort of meaningful or sufficient human 

735
00:44:55,301 --> 00:44:55,301
control,
um,

736
00:44:55,301 --> 00:44:59,920
but also in a way of thinking about that
and interrogating that as new 

737
00:44:59,920 --> 00:44:59,920
technologies develop in the future,
uh,

738
00:44:59,920 --> 00:45:01,580
that we can leverage in some.

739
00:45:01,700 --> 00:45:05,430
In some ways I felt reasonably confident
about that.

740
00:45:05,490 --> 00:45:08,490
But it's a difficult political 
environment and you know,

741
00:45:08,640 --> 00:45:13,640
it's quite possible that I don't see any
rush amongst states to move towards any 

742
00:45:13,640 --> 00:45:14,880
legal controls in this,
in this area.

743
00:45:15,900 --> 00:45:20,900
Just as a few very final thoughts,
which may be a bit more abstract in my 

744
00:45:20,900 --> 00:45:22,590
thinking on this.
I feel like,

745
00:45:23,740 --> 00:45:27,340
and this sort of reflecting on maybe 
some dynamics of ai functioning,

746
00:45:28,110 --> 00:45:31,930
my anxiety here about the expansion of 
the concept of attacks and in the same,

747
00:45:32,960 --> 00:45:37,960
in conjunction with that,
a sort of breaking down of the 

748
00:45:37,960 --> 00:45:39,740
granularity of the legal framework.
I think this is another,

749
00:45:40,310 --> 00:45:45,310
a sort of generalizing function again,
and it's a movement away from more 

750
00:45:45,310 --> 00:45:49,631
specific legal application by humans to 
perhaps a pushing humans people towards 

751
00:45:52,210 --> 00:45:54,720
a more general,
a legal orientation.

752
00:45:54,721 --> 00:45:57,700
And I feel like in the context of 
conflict,

753
00:45:57,910 --> 00:46:02,910
we should be pushing for a more specific
and more focused and more regular,

754
00:46:03,520 --> 00:46:08,520
um,
application of human judgments and 

755
00:46:08,520 --> 00:46:08,520
moral.

756
00:46:08,520 --> 00:46:11,290
A moral agency that isn't to say that I 
think humans are perfect in any way.

757
00:46:11,350 --> 00:46:13,150
Uh,
there's lots of problems with humans,

758
00:46:13,780 --> 00:46:18,780
but at the same time I think the,
we should be very wary of thinking that 

759
00:46:18,780 --> 00:46:21,380
violence is something that can be 
somehow perfected and that we can encode

760
00:46:22,040 --> 00:46:27,040
how to conduct violence in some 
machinery that will then provide an 

761
00:46:27,231 --> 00:46:30,700
adequate social product for society as 
a,

762
00:46:30,701 --> 00:46:33,380
as a whole.
And I guess it was a very final thought,

763
00:46:33,381 --> 00:46:37,340
a bit linked to that is some questions 
in my mind about how this all relates to

764
00:46:37,341 --> 00:46:42,341
bureaucracy in a way and in a sense that
some of the functions that we're saying 

765
00:46:42,341 --> 00:46:45,791
here and some of the Ai functions that 
we see here in many ways related to 

766
00:46:45,800 --> 00:46:50,800
bureaucracy,
to the encoding and categorization of 

767
00:46:50,800 --> 00:46:54,491
data in certain ways and just a very 
fast management of that bureaucracy,

768
00:46:55,760 --> 00:46:58,310
which is really an extension of the 
bureaucracies that we already,

769
00:46:58,540 --> 00:47:03,540
that we already have.
And I think extending that to far into 

770
00:47:03,540 --> 00:47:06,660
the world of violence and the 
application of force to people will well

771
00:47:06,661 --> 00:47:09,750
precipitate painful effects for us as a,
as a society.

772
00:47:09,751 --> 00:47:14,751
And as it brings to the fore.
I think some of the underpinning 

773
00:47:14,751 --> 00:47:15,120
underpinnings of the rationales of that,
uh,

774
00:47:15,130 --> 00:47:18,290
of that bureaucratic framework so that 
we go.

775
00:47:18,310 --> 00:47:21,260
It's a bit of a broad brush sketch.

776
00:47:23,070 --> 00:47:28,070
Thank you.

777
00:47:31,080 --> 00:47:32,060
So,
um,

778
00:47:32,190 --> 00:47:34,510
this question's kind of a little bit of 
multifaceted,

779
00:47:34,511 --> 00:47:35,600
but,
um,

780
00:47:35,800 --> 00:47:39,130
as humans evolve and adapt to 
increasingly autonomous weapons,

781
00:47:39,520 --> 00:47:44,520
the complexity and sophistication could 
increase with expansion of targets and 

782
00:47:44,520 --> 00:47:48,340
types and target area is.
Do you think there's a limit to which we

783
00:47:48,341 --> 00:47:52,030
can prepare against such an evolution 
and a,

784
00:47:52,210 --> 00:47:56,410
do you think that'd be ocracy can keep 
up with how fast these,

785
00:47:57,040 --> 00:47:59,680
the autonomy of you couldn't develop 
over time?

786
00:48:00,680 --> 00:48:05,680
Yeah,
I'm not sure I caught all the first bit 

787
00:48:05,680 --> 00:48:05,680
of the question,
but there's definitely,

788
00:48:05,680 --> 00:48:09,970
it's definitely a challenge that the 
types of legal discussions at the UN 

789
00:48:09,970 --> 00:48:13,820
Convention on Conventional Weapons,
they are not famous for going too 

790
00:48:13,820 --> 00:48:14,570
quickly.
In fact,

791
00:48:14,750 --> 00:48:18,050
they're incredibly slow.
And in that framework,

792
00:48:18,051 --> 00:48:21,470
every state essentially has a veto over 
everything.

793
00:48:21,770 --> 00:48:23,900
So even over the agenda of the next 
meeting,

794
00:48:24,310 --> 00:48:26,930
if you know,
if the US wants to block the agenda,

795
00:48:26,931 --> 00:48:31,931
they can lock the agenda,
let alone block the outcome that might 

796
00:48:31,931 --> 00:48:32,630
come if you could agree on agenda.
So,

797
00:48:32,850 --> 00:48:36,740
so every state has an ability to keep 
things moving very slowly there.

798
00:48:36,950 --> 00:48:41,950
And that's definitely a challenge in the
context where the pace of technological 

799
00:48:41,950 --> 00:48:44,600
development moves pretty quickly.
The only thing I would say,

800
00:48:44,780 --> 00:48:49,780
which I forgot to mention before in 
terms of thinking about the dynamics in 

801
00:48:49,780 --> 00:48:52,871
this debate,
is that it's not straightforwardly a 

802
00:48:52,871 --> 00:48:55,860
situation where militaries really want 
loads more autonomous weapons and other 

803
00:48:56,421 --> 00:49:01,421
people don't have in military commanders
also like control and they they like 

804
00:49:01,550 --> 00:49:06,550
troops on the ground like control and 
they like trust and confidence in the 

805
00:49:06,550 --> 00:49:06,620
systems that they're operating around.

806
00:49:06,980 --> 00:49:11,980
They don't want to get blown up by their
own equipment and military commanders 

807
00:49:11,980 --> 00:49:13,100
like controlling and like to know what's
happening.

808
00:49:13,101 --> 00:49:16,940
So there are some constraints within the
military structures as well.

809
00:49:18,680 --> 00:49:23,680
So the overall sort of development here,
I guess from our side in terms of this 

810
00:49:23,680 --> 00:49:27,221
sort of how to constrain against the 
expansion of attacks and the expansion 

811
00:49:27,221 --> 00:49:31,571
of sort of objects that may be attacked 
by autonomous systems in a way.

812
00:49:31,911 --> 00:49:36,911
That's where I felt like developing the 
idea that there's a principle of human 

813
00:49:36,911 --> 00:49:40,391
control that needs to be applied.
Even if it's a bit fuzzy in its 

814
00:49:40,391 --> 00:49:44,200
boundaries,
we can use that and interrogated as a 

815
00:49:44,200 --> 00:49:47,021
social process to try and keep 
constraint going back towards the 

816
00:49:47,021 --> 00:49:50,000
specific because in the end,
like I said earlier,

817
00:49:50,001 --> 00:49:55,001
these legal structures or sort of social
processes as well and it's not very 

818
00:49:55,001 --> 00:49:59,441
easy.
It's not something where you can just 

819
00:49:59,441 --> 00:49:59,441
straightforward to draw a line and then 
no new technologies will come along that

820
00:49:59,441 --> 00:50:01,310
challenge your expectations,
right?

821
00:50:01,990 --> 00:50:06,320
We need to find the sort of camp on the 
international legal political landscape.

822
00:50:06,620 --> 00:50:11,000
We need to sketch out the parameters of 
that come in legal terms.

823
00:50:11,380 --> 00:50:15,920
Then we need people to turn up at those 
meetings and continuously complain about

824
00:50:15,921 --> 00:50:20,921
things and put pressure on things 
because that's the only way over time 

825
00:50:20,921 --> 00:50:24,071
where you maintain that sort of 
interrogation of future technologies as 

826
00:50:24,071 --> 00:50:25,560
they come out of the pipeline or or 
whatever.

827
00:50:25,640 --> 00:50:27,590
So it's a sort of social I think.

828
00:50:28,010 --> 00:50:33,010
Yeah,
that answered my question is like the 

829
00:50:33,010 --> 00:50:34,871
balance between like how fast science 
would be like advancing in this field 

830
00:50:34,871 --> 00:50:36,590
versus like how fast we obviously can 
move to keep up

831
00:50:37,740 --> 00:50:39,140
be resolved.
I think it's an ongoing.

832
00:50:39,141 --> 00:50:41,600
It's got to be an ongoing social 
political process in a way.

833
00:50:41,601 --> 00:50:41,820
Right.

834
00:50:41,930 --> 00:50:42,620
Awesome.
Thank you.

835
00:50:45,400 --> 00:50:45,830
So

836
00:50:46,480 --> 00:50:51,480
given that this course is on Agi and 
we'll likely see a wide variety of 

837
00:50:51,480 --> 00:50:53,980
different kinds of autonomous systems in
the future,

838
00:50:54,430 --> 00:50:59,430
can you give us perhaps some sort of 
extrapolation from this domain to a 

839
00:50:59,430 --> 00:51:04,080
broader set of potentially risky 
behaviors that more autonomous and more 

840
00:51:04,080 --> 00:51:09,001
intelligent systems would do and ways 
that the creators of such systems such 

841
00:51:09,001 --> 00:51:13,891
as potentially the folks sitting in this
room can change what they're doing to 

842
00:51:13,891 --> 00:51:15,380
make those safer?
Yeah,

843
00:51:15,440 --> 00:51:20,440
I mean I think useful to think about in 
some ways these ideas of from the 

844
00:51:22,790 --> 00:51:23,990
president,
from where we are now,

845
00:51:23,991 --> 00:51:27,410
how can people involved in developing 
different technologies,

846
00:51:27,440 --> 00:51:32,440
new technological capacities,
just be thinking of the potential 

847
00:51:32,440 --> 00:51:36,800
outcomes in this sort of weaponization 
area and building in some orientation to

848
00:51:36,801 --> 00:51:41,801
their work that thinks about that and 
thinks about what the potential 

849
00:51:41,801 --> 00:51:44,000
consequences of work can be.
I mean,

850
00:51:44,001 --> 00:51:47,960
I think in some ways the risk outcomes 
type thinking.

851
00:51:47,990 --> 00:51:51,860
I mean again,
it gets you into hypothetical arguments,

852
00:51:51,861 --> 00:51:54,050
but they.
The idea of two sides,

853
00:51:54,051 --> 00:51:59,051
both with substantial autonomous weapons
system capabilities is probably the sort

854
00:51:59,151 --> 00:52:04,151
of area where these ideas of accidental 
escalations come to the fore,

855
00:52:04,731 --> 00:52:09,731
that if you've got to adversarily 
orientated states with substantial 

856
00:52:12,920 --> 00:52:17,920
autonomous systems,
then there's a potential for 

857
00:52:17,920 --> 00:52:20,381
interactions to occur between those 
systems that rapidly escalating violence

858
00:52:20,541 --> 00:52:25,541
situation in a way that greater capacity
for human engagement would allow you to,

859
00:52:26,400 --> 00:52:30,020
to curtail it and stole it.
And I think,

860
00:52:30,380 --> 00:52:32,400
I mean I know in other areas of,
you know,

861
00:52:32,780 --> 00:52:37,190
of Algo written functioning in society,
we've seen aspects of that,

862
00:52:37,191 --> 00:52:42,191
right?
And sort of probably in the financial 

863
00:52:42,191 --> 00:52:42,640
sector and other such location.
So.

864
00:52:43,020 --> 00:52:46,160
So I think yeah,
those are those ideas of sort of rapidly

865
00:52:46,161 --> 00:52:49,800
escalating,
cascading risks is a,

866
00:52:50,270 --> 00:52:51,740
is a,
is a concern in that area.

867
00:52:51,800 --> 00:52:54,740
But again,
based on hypothetical thinking about

868
00:52:55,020 --> 00:52:57,420
stuff.
Last question,

869
00:52:58,010 --> 00:52:58,520
right?

870
00:52:58,920 --> 00:53:03,920
What do you think of this criteria?
So we have this tank example on the 

871
00:53:03,991 --> 00:53:06,600
right,
our simulations,

872
00:53:06,601 --> 00:53:09,060
our ability to simulate things is 
getting better and better.

873
00:53:09,960 --> 00:53:14,960
What if we showed a simulation of what 
would happen to a person that has the 

874
00:53:15,841 --> 00:53:20,841
ability to hit the go button on it and 
if the simulation does not have enough 

875
00:53:20,841 --> 00:53:25,461
fidelity,
we consider that a Nogo we cannot do 

876
00:53:25,861 --> 00:53:30,861
that.
Or if the simulation shows it does have 

877
00:53:30,861 --> 00:53:32,460
enough fidelity and it shows a,
a bad outcome,

878
00:53:33,660 --> 00:53:37,990
then maybe that would be a criteria in 
which to,

879
00:53:38,710 --> 00:53:41,730
uh,
to judge this circumstance on the right.

880
00:53:41,731 --> 00:53:46,731
And that could also let us a,
as that circle gets bigger and bigger,

881
00:53:46,890 --> 00:53:49,910
it can let us kind of put a,
uh,

882
00:53:50,790 --> 00:53:53,430
it could let us cap that by saying,
hey,

883
00:53:53,431 --> 00:53:58,431
if we don't,
if we do not have enough information to 

884
00:53:58,431 --> 00:54:00,270
make this a simulation to even show the 
person,

885
00:54:00,510 --> 00:54:01,860
then it's a no go.

886
00:54:02,280 --> 00:54:03,780
Yup.
Yup.

887
00:54:03,840 --> 00:54:07,770
I think in a way this is an issue of 
modeling,

888
00:54:07,771 --> 00:54:12,771
right?
Based on contextual information that 

889
00:54:12,771 --> 00:54:12,771
you,
that you have.

890
00:54:12,771 --> 00:54:16,281
So maybe with technological developments
you have a better capacity for modeling 

891
00:54:17,460 --> 00:54:22,200
specific situations.
I suppose the challenge is how do you,

892
00:54:22,580 --> 00:54:27,580
in a sort of timely manner,
especially in a conflict environment 

893
00:54:27,580 --> 00:54:29,550
where tempo is significant.
Can you,

894
00:54:29,730 --> 00:54:34,730
can you put the data that you have into 
some sort of modeling system adequately,

895
00:54:35,191 --> 00:54:40,191
but I don't see any problem with the 
idea of using ai to model the outcomes 

896
00:54:42,091 --> 00:54:47,091
of specific attacks and you know,
give you read outs on what the likely 

897
00:54:47,091 --> 00:54:49,560
effects are going to be.
I guess the challenges that,

898
00:54:49,620 --> 00:54:54,620
what counts are adequate effect where 
the boundary lines have sufficient 

899
00:54:54,620 --> 00:54:58,881
information and insufficient information
fall that kind of open questions as 

900
00:54:58,881 --> 00:54:58,881
well.
Right.

901
00:54:58,881 --> 00:55:03,861
And you know,
military is tend to like to leave some 

902
00:55:03,861 --> 00:55:05,070
openness on those,
those points as well.

903
00:55:05,071 --> 00:55:10,071
But,
but I think that can be definitely a 

904
00:55:10,071 --> 00:55:10,860
goal for modeling and in better 
understanding what the facts.

905
00:55:10,861 --> 00:55:14,550
It cannot be great.
Give Richard a big hand.

906
00:55:15,030 --> 00:55:15,630
Thank you very much.

