1
00:00:00,030 --> 00:00:02,220
As part of Mit core six zero,
nine,

2
00:00:02,221 --> 00:00:07,221
nine artificial general intelligence,
I've gotten the chance to sit down with 

3
00:00:07,221 --> 00:00:08,280
Max tegmark.
He is a professor he hit on my tea,

4
00:00:08,670 --> 00:00:13,670
is a physicist,
spent a large part of his career 

5
00:00:13,670 --> 00:00:16,530
studying the mysteries of our 
cosmological universe,

6
00:00:16,950 --> 00:00:21,600
but he's also studied in,
delved into the beneficial possibilities

7
00:00:21,660 --> 00:00:26,660
and the existential risks of artificial 
intelligence amongst many other things.

8
00:00:27,211 --> 00:00:31,710
He's the cofounder of the future of Life
Institute,

9
00:00:31,860 --> 00:00:34,740
author of two books,
both of which I highly recommend.

10
00:00:35,160 --> 00:00:38,880
First our mathematical universe.
Second is life,

11
00:00:38,881 --> 00:00:42,510
three point zero.
He's truly an out of the box thinker and

12
00:00:42,870 --> 00:00:45,390
a fun personality,
so I really enjoyed talking to him.

13
00:00:45,450 --> 00:00:47,730
If you'd like to see more of these 
videos in the future,

14
00:00:47,970 --> 00:00:52,970
please subscribe and also click the 
little bell icon to make sure you don't 

15
00:00:52,970 --> 00:00:53,430
miss any videos.
Also,

16
00:00:53,550 --> 00:00:54,900
twitter,
linkedin,

17
00:00:55,140 --> 00:00:59,580
Agi.mit.edu.
If you want to watch other lectures,

18
00:00:59,581 --> 00:01:01,680
are conversations like this one better 
yet?

19
00:01:01,770 --> 00:01:03,180
Go read Max's book,
light.

20
00:01:03,181 --> 00:01:07,440
Three point zero.
Chapter seven on goals is my favorite.

21
00:01:07,890 --> 00:01:12,890
It's really more philosophy and 
engineering come together and it opens 

22
00:01:12,890 --> 00:01:13,240
with a quote by dusty.
See,

23
00:01:14,340 --> 00:01:17,610
the mystery of human existence lies not 
in just staying alive,

24
00:01:17,880 --> 00:01:21,090
but in finding something to live for.
Lastly,

25
00:01:21,270 --> 00:01:26,270
I believe that every failure rewards us 
with an opportunity to learn and that 

26
00:01:26,821 --> 00:01:31,821
sense.
I've been very fortunate to fail and so 

27
00:01:31,821 --> 00:01:32,340
many new and exciting ways and,
uh,

28
00:01:32,341 --> 00:01:37,341
this conversation was no different.
I've learned about something called 

29
00:01:37,341 --> 00:01:39,750
radio frequency interference,
RFI.

30
00:01:39,930 --> 00:01:42,880
Look it up.
Apparently music and conversations,

31
00:01:42,881 --> 00:01:46,890
some local radio stations can bleed into
the audio that we're recording in such a

32
00:01:46,891 --> 00:01:49,080
way that almost completely ruins that 
audio.

33
00:01:49,290 --> 00:01:51,810
It's an exceptionally difficult sound 
source to remove,

34
00:01:53,160 --> 00:01:58,160
so I've gotten the opportunity to learn 
how to avoid rfi in the future.

35
00:01:59,071 --> 00:02:04,071
During the recording sessions have also 
gotten the opportunity to learn how to 

36
00:02:04,071 --> 00:02:07,851
use adobe audition and isotope rx six to
do some noise,

37
00:02:09,330 --> 00:02:12,210
some audio repair.
Of course,

38
00:02:12,211 --> 00:02:14,940
this is exceptionally difficult noise to
remove.

39
00:02:14,970 --> 00:02:17,700
I am an engineer.
I'm not an audio engineer.

40
00:02:18,230 --> 00:02:21,390
Neither is anybody else in our group,
but would a best.

41
00:02:21,840 --> 00:02:25,680
Nevertheless,
I thank you for your patience and I hope

42
00:02:25,681 --> 00:02:27,450
you're still able to enjoy this 
conversation.

43
00:02:27,930 --> 00:02:30,270
Do you think there's intelligent life 
out there in the universe?

44
00:02:31,330 --> 00:02:35,940
Let's open up with an easy question.
I have a minority view here.

45
00:02:35,950 --> 00:02:37,600
Actually,
when I give public lectures,

46
00:02:37,880 --> 00:02:42,880
I often ask for a show of hands who 
thinks there's intelligent life out 

47
00:02:42,880 --> 00:02:46,510
there somewhere else and almost everyone
put their hands up and when I ask why,

48
00:02:46,930 --> 00:02:47,710
they'll be like,
oh,

49
00:02:47,711 --> 00:02:50,740
there's so many galaxies out there.
There's gonna be.

50
00:02:51,790 --> 00:02:53,890
But I'm a numbers nerd.
Right?

51
00:02:54,490 --> 00:02:58,620
So when you look more carefully at it,
it's not so clear at all the.

52
00:02:59,080 --> 00:03:00,670
When we talk about our universe,
first of all,

53
00:03:00,671 --> 00:03:04,060
we don't mean all of space.
Do we actually mean I don't know.

54
00:03:04,061 --> 00:03:06,130
You can throw me in the university.
She wants to behind you there.

55
00:03:07,250 --> 00:03:10,330
It's,
we'd simply mean the spherical region of

56
00:03:10,331 --> 00:03:15,331
space from which light has to reach us.
So far during the 14 point 8 billion 

57
00:03:16,661 --> 00:03:21,661
year,
13 point 8 billion years since our big 

58
00:03:21,661 --> 00:03:21,661
bang,
there's more space here,

59
00:03:21,661 --> 00:03:23,620
but this is what we call a universe 
because that's all we have access to.

60
00:03:24,010 --> 00:03:29,010
So is there intelligent life here that's
gotten to the point of building 

61
00:03:29,010 --> 00:03:33,040
telescopes and computers?
My guess is no,

62
00:03:33,140 --> 00:03:36,360
actually that the probability of it 
happening on it,

63
00:03:36,640 --> 00:03:37,690
any given planet,

64
00:03:39,200 --> 00:03:44,200
there's some number we don't know what 
it is and what we do know is that the 

65
00:03:47,090 --> 00:03:50,420
number can be super high because there's
over a billion earth like planets in the

66
00:03:50,430 --> 00:03:52,460
mill keyway,
galaxy alone,

67
00:03:52,820 --> 00:03:55,610
many of which are billions of years 
older than earth.

68
00:03:56,210 --> 00:04:01,210
And um,
aside from some of you will for 

69
00:04:01,210 --> 00:04:01,210
believers,
you know,

70
00:04:01,210 --> 00:04:05,531
there isn't much evidence that any 
superintendent civilization has come 

71
00:04:05,531 --> 00:04:07,670
here at all.
And so that's the famous Fermi paradox.

72
00:04:07,671 --> 00:04:09,140
Right?
And then if you,

73
00:04:09,141 --> 00:04:11,630
if you work the numbers,
what you find is that the,

74
00:04:12,440 --> 00:04:15,800
if you have no clue what the probability
is of getting life on a given planet.

75
00:04:16,800 --> 00:04:21,800
So it could be 10 to the minus 10 to the
minus 20 or temperament is to any power 

76
00:04:22,201 --> 00:04:24,960
of 10 is sort of equally likely if you 
want to be really open minded,

77
00:04:25,500 --> 00:04:30,500
that translates into it being equally 
likely that our nearest neighbor is 10 

78
00:04:30,500 --> 00:04:34,581
to the 16 meters away,
tend to the 70 meters away and of 18 

79
00:04:35,010 --> 00:04:40,010
don't.
By the time he gets much less than 10,

80
00:04:40,090 --> 00:04:42,220
16 already,
we pretty much.

81
00:04:42,221 --> 00:04:45,080
No,
there is nothing else that's close.

82
00:04:45,940 --> 00:04:48,370
And when you get the opposite would have
discovered us.

83
00:04:48,700 --> 00:04:53,700
Yeah,
they would have been discovered as long 

84
00:04:53,700 --> 00:04:55,171
ago or if they're really close,
we would have probably noted some 

85
00:04:55,171 --> 00:04:57,691
engineering projects that they're doing 
and if it's beyond 10 to 26 meters 

86
00:04:57,910 --> 00:05:02,910
that's already outside of here.
So my guess is actually that there are,

87
00:05:04,320 --> 00:05:09,320
we are the only life in here.
They've gotten to the point of building 

88
00:05:09,320 --> 00:05:09,920
advanced tech,
which I think is,

89
00:05:09,921 --> 00:05:10,910
is very,
um,

90
00:05:12,710 --> 00:05:15,170
puts a lot of responsibility on our 
shoulders not screw up.

91
00:05:15,171 --> 00:05:20,171
You know,
I think actually people who take for 

92
00:05:20,171 --> 00:05:20,171
granted that it's okay for us to screw 
up,

93
00:05:20,171 --> 00:05:25,121
have an accidental nuclear war or go 
extinct somehow because there's a sort 

94
00:05:25,121 --> 00:05:28,631
of star trek like situation out there 
with some other life forms are going to 

95
00:05:28,631 --> 00:05:30,140
come and bail us out.
And it doesn't matter as much.

96
00:05:30,410 --> 00:05:32,900
I think allowing us into a false sense 
of security.

97
00:05:33,400 --> 00:05:35,220
I think it's much more prudent to say,
you know,

98
00:05:35,221 --> 00:05:38,180
let's be really grateful for this 
amazing opportunity we've had.

99
00:05:38,720 --> 00:05:43,720
And um,
makes the best of it just in case it is 

100
00:05:43,720 --> 00:05:45,650
down to us.
So from a physics perspective,

101
00:05:45,680 --> 00:05:50,680
do you think intelligent life says 
unique from a sort of statistical view 

102
00:05:51,321 --> 00:05:56,321
of the size of the universe,
but from the basic matter of the 

103
00:05:56,321 --> 00:05:58,070
universe,
how difficult is it for intelligent

104
00:05:58,070 --> 00:06:03,070
life to come about?
Was the kind of advanced tech building 

105
00:06:03,070 --> 00:06:05,681
life is implied in your statement that 
it's really difficult to create 

106
00:06:06,051 --> 00:06:07,550
something like a human species?

107
00:06:08,010 --> 00:06:13,010
I think what we know is that going from 
no life to having life that can do 

108
00:06:14,060 --> 00:06:19,060
archive level of tech,
there was some sort of to going beyond 

109
00:06:19,060 --> 00:06:23,441
that.
Then it actually settling our whole 

110
00:06:23,441 --> 00:06:25,811
universe with life.
There's some road major roadblock there 

111
00:06:26,480 --> 00:06:31,170
which is some great filter as um,
it's sometimes called the which,

112
00:06:31,610 --> 00:06:36,610
which is tough to get through.
It's either that roadblock is either be 

113
00:06:36,610 --> 00:06:38,360
behind us or in front of us.
Right?

114
00:06:38,660 --> 00:06:40,730
I'm hoping very much that it's behind 
us.

115
00:06:40,960 --> 00:06:42,950
I'm,
I'm super excited.

116
00:06:42,951 --> 00:06:47,951
Every time we get a new report from NASA
saying they failed to find any life on 

117
00:06:47,951 --> 00:06:49,080
Mars,
like yes,

118
00:06:49,970 --> 00:06:51,520
because that suggests that the hard 
part.

119
00:06:51,560 --> 00:06:54,710
Maybe what maybe it was getting the 
first right Bozon or or something.

120
00:06:54,860 --> 00:06:58,820
Some very low level kind of stepping 
stone,

121
00:06:59,390 --> 00:07:01,280
so they were home free because if that's
true,

122
00:07:01,580 --> 00:07:04,910
then the future is really only limited 
by our own imagination.

123
00:07:05,090 --> 00:07:10,090
It would be much suckier if it turns out
that this level of life is kind of a 

124
00:07:10,090 --> 00:07:12,650
diamond dozen,
but maybe there's some other problem,

125
00:07:12,651 --> 00:07:16,880
like as soon as a civilization gets 
advanced technology within 100 years,

126
00:07:16,881 --> 00:07:19,160
they get into some stupid fight with 
themselves and poof.

127
00:07:19,320 --> 00:07:20,930
Yep.
That would be a bummer.

128
00:07:21,650 --> 00:07:22,700
Yeah.
So

129
00:07:23,000 --> 00:07:27,260
you've explored the mysteries of the 
universe of the cosmological universe,

130
00:07:27,261 --> 00:07:28,610
the one that's sitting

131
00:07:28,850 --> 00:07:33,850
between us today.
I think you've also have begun to 

132
00:07:33,850 --> 00:07:33,920
explore

133
00:07:34,220 --> 00:07:37,910
the other universe,
which is sort of the mystery,

134
00:07:37,911 --> 00:07:40,850
the mysterious universe of the mind of 
intelligence,

135
00:07:40,851 --> 00:07:44,540
of intelligent life.
So is there a common thread between your

136
00:07:44,541 --> 00:07:48,260
interests are in the way you think about
space and intelligence?

137
00:07:48,610 --> 00:07:53,610
Oh yeah.
When I was a teenager I was already very

138
00:07:54,191 --> 00:07:59,170
fascinated by the biggest questions and 
I felt that the two biggest mysteries of

139
00:07:59,171 --> 00:08:03,940
all in science where our universe out 
there and our universe in here.

140
00:08:04,050 --> 00:08:07,030
Yup.
So it's quite natural after having spent

141
00:08:08,080 --> 00:08:12,100
quarter of a century on my career,
thinking a lot about this one,

142
00:08:12,630 --> 00:08:15,910
I'm now indulging in the luxury of doing
research on this one.

143
00:08:15,911 --> 00:08:19,990
It's just so cool.
I feel the time is ripe now.

144
00:08:20,100 --> 00:08:25,100
Refer you directly.
Deepening our understanding of this to 

145
00:08:25,241 --> 00:08:26,740
start exploring this one.
Yeah,

146
00:08:26,741 --> 00:08:31,741
because I think I think a lot of people 
view intelligence as something 

147
00:08:31,741 --> 00:08:36,361
mysterious that can only exist in 
biological organisms like us and 

148
00:08:36,361 --> 00:08:41,281
therefor dismiss all.
Talk about artificial general 

149
00:08:41,281 --> 00:08:42,850
intelligence is science fiction,
but from my perspective as a physicist,

150
00:08:42,970 --> 00:08:47,970
you know I am a blob of corks and 
electrons moving around in a certain 

151
00:08:47,970 --> 00:08:52,951
pattern and process the information in 
certain ways and this is also a blob of 

152
00:08:52,951 --> 00:08:52,960
quirks that electrons.

153
00:08:53,590 --> 00:08:57,330
I'm not smarter than the water because I
made a different kind of works.

154
00:08:57,440 --> 00:08:59,640
Right?
I made up quirks and down quirks.

155
00:08:59,641 --> 00:09:03,420
Exact same kind as this.
It's A.

156
00:09:03,420 --> 00:09:06,900
There's no secret sauce.
I think in me it's all about the pattern

157
00:09:06,901 --> 00:09:11,901
of the information processing and this 
means that there's no law of physics 

158
00:09:12,271 --> 00:09:17,271
saying that we can't create technology 
which can help us by being incredibly 

159
00:09:19,381 --> 00:09:21,540
intelligent than help us crack Mr.
so we couldn't.

160
00:09:21,690 --> 00:09:26,690
In other words,
I think we've really only seen the tip 

161
00:09:26,690 --> 00:09:26,690
of,
of intelligence iceberg so far.

162
00:09:26,690 --> 00:09:30,480
Yeah.
So the perceptronium yeah.

163
00:09:31,050 --> 00:09:33,490
Uh,
so you current coin this amazing term as

164
00:09:33,500 --> 00:09:38,400
a hypothetical state of matter sort of 
thinking from a physics perspective,

165
00:09:38,401 --> 00:09:39,990
what is the kind of matter that can 
help,

166
00:09:40,080 --> 00:09:42,540
as you're saying a subjective 
experience,

167
00:09:42,541 --> 00:09:43,980
emerge?
Consciousness emerge.

168
00:09:44,280 --> 00:09:47,940
So how do you think about consciousness 
from this physics perspective?

169
00:09:49,980 --> 00:09:53,280
Very good question.
So again,

170
00:09:53,281 --> 00:09:54,030
I'm.
No,

171
00:09:54,050 --> 00:09:54,390
I think

172
00:09:56,090 --> 00:10:01,090
many people have underestimated our 
ability to make progress on this by 

173
00:10:02,630 --> 00:10:07,630
convincing themselves it's hopeless 
because somehow we're missing some 

174
00:10:07,630 --> 00:10:10,070
ingredients that we need.
There's some new consciousness,

175
00:10:10,071 --> 00:10:15,071
part it go or whatever.
I happened to think that we're not 

176
00:10:15,071 --> 00:10:19,661
missing anything and that it's not.
The interesting thing about 

177
00:10:19,661 --> 00:10:24,640
consciousness that gives us this amazing
subjective experience of colors and 

178
00:10:24,640 --> 00:10:29,471
sounds and emotions on is rather 
something at the higher level about the 

179
00:10:30,441 --> 00:10:34,010
patterns of information processing and 
that's why I.

180
00:10:34,130 --> 00:10:37,970
that's why I am like thinking about this
idea of perceptronium.

181
00:10:38,120 --> 00:10:42,560
What does it mean for an arbitrary 
physical system to be conscious in terms

182
00:10:42,561 --> 00:10:45,830
of what it's particle to do anything or,
or,

183
00:10:45,980 --> 00:10:47,600
or it's inflammation is doing.
I don't think.

184
00:10:47,880 --> 00:10:49,550
I don't.
I hate the carbon chauvinism.

185
00:10:49,551 --> 00:10:50,330
You know,
this attitude.

186
00:10:50,331 --> 00:10:53,510
You have to be made of carbon atoms to 
be smart or conscious.

187
00:10:54,090 --> 00:10:58,420
Something about the information 
processing kind of matter performs.

188
00:10:58,430 --> 00:10:59,110
Yeah.
And you know,

189
00:10:59,120 --> 00:11:04,120
you can see I have my favorite equations
here describing various fundamental 

190
00:11:04,120 --> 00:11:07,751
aspects of the world.
I feel that I think one day maybe 

191
00:11:07,751 --> 00:11:11,201
someone is watching this or come up with
the equations that information 

192
00:11:11,201 --> 00:11:12,080
processing has to satisfy to be 
consciously.

193
00:11:12,081 --> 00:11:17,081
I'm quite convinced there is big 
discovery to be made there because let's

194
00:11:17,691 --> 00:11:22,691
face it,
some sometimes we know that some 

195
00:11:22,691 --> 00:11:25,151
information processing is conscious 
because we are conscious,

196
00:11:25,730 --> 00:11:28,580
but we also know that a lot of 
information processing is not conscious.

197
00:11:28,581 --> 00:11:32,420
Like most of the information presently 
happening in your brain right now is not

198
00:11:32,421 --> 00:11:35,960
conscious.
They like 10 megabytes per second coming

199
00:11:35,961 --> 00:11:40,961
in even just through your visual system 
and you are not conscious about your 

200
00:11:40,961 --> 00:11:41,780
heartbeat regulation or,
or most things,

201
00:11:41,840 --> 00:11:42,400
right?
Uh,

202
00:11:42,410 --> 00:11:45,350
even,
even like if I just ask you to read what

203
00:11:45,351 --> 00:11:46,790
it says here,
you look at it and then,

204
00:11:46,850 --> 00:11:48,020
oh,
now you know what it said,

205
00:11:48,200 --> 00:11:50,690
but you're not aware of how the 
computation actually happened.

206
00:11:51,030 --> 00:11:56,030
You're like,
to your consciousness is like the that 

207
00:11:56,030 --> 00:11:56,050
got an email at the end with a final 
answer.

208
00:11:56,800 --> 00:12:01,800
So what is it that makes a difference?
I think that's both those great science 

209
00:12:04,421 --> 00:12:09,421
mystery.
We're actually studying it a little bit 

210
00:12:09,421 --> 00:12:11,551
in my lab here at mit a.
But I also think it's just a really 

211
00:12:11,551 --> 00:12:12,970
urgent question answer for starters.
I mean,

212
00:12:12,971 --> 00:12:17,971
if you're an emergency room doctor and 
you have an unresponsive patient coming 

213
00:12:17,971 --> 00:12:21,811
in,
wouldn't it be great if in addition to 

214
00:12:21,811 --> 00:12:21,811
having

215
00:12:22,360 --> 00:12:23,480
a ct scanner,
you,

216
00:12:23,530 --> 00:12:28,530
you had a conscience of scanner that you
figure out whether this person is 

217
00:12:28,530 --> 00:12:30,850
actually having locked in syndrome,
right?

218
00:12:31,020 --> 00:12:33,130
Whereas actually comatose,
uh,

219
00:12:33,380 --> 00:12:38,380
and in the future,
imagine if we build the robots or the 

220
00:12:38,380 --> 00:12:38,380
machine that

221
00:12:39,820 --> 00:12:42,010
we could have really good conversations 
with students.

222
00:12:42,040 --> 00:12:44,440
I think it's very,
very likely to happen.

223
00:12:44,441 --> 00:12:45,760
Right?
Wouldn't you want to know,

224
00:12:45,761 --> 00:12:50,761
like if you're a home health,
a robot is actually experiencing 

225
00:12:50,761 --> 00:12:51,580
anything or just like a Zombie,
I mean,

226
00:12:52,630 --> 00:12:54,160
would you prefer.
What would you prefer?

227
00:12:54,161 --> 00:12:59,161
Would you prefer that it's actually 
unconscious so that you don't have to 

228
00:12:59,161 --> 00:13:02,161
feel guilty about switching it off or 
giving boring chores or what would you 

229
00:13:02,161 --> 00:13:02,530
prefer?
Well,

230
00:13:02,590 --> 00:13:06,490
the certainly would,
we would prefer,

231
00:13:06,580 --> 00:13:08,740
I would prefer the appearance of 
consciousness,

232
00:13:08,970 --> 00:13:13,970
but the question is whether the 
appearance of cautiousness is different 

233
00:13:13,970 --> 00:13:17,381
than cost consciousness itself.
And sort of ask that as a question,

234
00:13:18,280 --> 00:13:20,230
do you think we need to,
you know,

235
00:13:20,231 --> 00:13:25,231
understand what consciousness is,
solve the hard problem of consciousness 

236
00:13:25,231 --> 00:13:28,240
in order to build something,
a light in a GI system?

237
00:13:28,270 --> 00:13:30,220
No,
I don't think that.

238
00:13:31,170 --> 00:13:36,170
I think we will probably be able to 
build things even if we don't answer 

239
00:13:36,170 --> 00:13:40,651
that question,
but if we want to make sure that what 

240
00:13:40,651 --> 00:13:40,651
happens is a good thing,
we better solve it first.

241
00:13:40,990 --> 00:13:45,990
So it's a wonderful controversy you're 
raising there where you have basically 

242
00:13:46,781 --> 00:13:48,740
three points of view about the hard 
problems.

243
00:13:48,741 --> 00:13:53,741
So there are two different points of 
view that both conclude that the hard 

244
00:13:54,310 --> 00:13:56,830
problem of crunches is bs.
Do you have,

245
00:13:56,880 --> 00:14:01,880
on one hand you have some people like 
Daniel Dennett who say this is 

246
00:14:01,880 --> 00:14:05,250
unconscious is just bs because 
consciousness is the same thing as 

247
00:14:05,250 --> 00:14:06,010
intelligence.
There's no difference.

248
00:14:06,460 --> 00:14:11,460
So anything which acts conscious is 
conscious dislike,

249
00:14:11,660 --> 00:14:16,000
like we are.
And then there are also a lot of people,

250
00:14:16,001 --> 00:14:18,640
including many top ai researchers.
I know you say,

251
00:14:18,641 --> 00:14:19,960
Oh,
I have a consciousness is just bullshit.

252
00:14:19,961 --> 00:14:21,970
Because of course machines can never be 
conscious,

253
00:14:22,000 --> 00:14:23,380
right?
They're always going to.

254
00:14:23,381 --> 00:14:28,381
It's going to be zombies,
never have to feel guilty about how you 

255
00:14:28,381 --> 00:14:28,381
treat them

256
00:14:28,381 --> 00:14:32,880
and then there's a third group of people
including Giulio Tononi for example,

257
00:14:34,060 --> 00:14:36,610
and another here's the cochrane number 
of others.

258
00:14:37,510 --> 00:14:42,510
I would put myself on this middle camp 
who say that actually some information 

259
00:14:42,510 --> 00:14:46,471
processing is conscious and some is not,
so let's find the equation which can be 

260
00:14:46,811 --> 00:14:51,811
used to determine which it is and I 
think we've just been a little bit lazy 

261
00:14:52,101 --> 00:14:54,610
kind of running away from this problem 
for a long time.

262
00:14:55,010 --> 00:14:57,740
It's been almost taboo.
Even mentioned the c word,

263
00:14:58,120 --> 00:15:03,120
a lot of circles because.
But we should stop making excuses.

264
00:15:03,600 --> 00:15:08,600
This is a science question and there are
ways we can even test test any scenario 

265
00:15:10,380 --> 00:15:13,700
that makes predictions for this.
And coming back to this health a robot.

266
00:15:13,701 --> 00:15:18,701
I mean,
so you said you would want your help a 

267
00:15:18,701 --> 00:15:20,891
robot to certainly conscious and treat 
you like to have conversations with you 

268
00:15:20,891 --> 00:15:22,070
and I think so wouldn't you?

269
00:15:22,100 --> 00:15:27,100
Would you feel,
would you feel a little bit creeped out 

270
00:15:27,100 --> 00:15:27,730
if you realize that it was just glossed 
up the tape recorder?

271
00:15:27,760 --> 00:15:32,760
You know,
there was this Zombie and faking 

272
00:15:32,760 --> 00:15:33,740
emotion.
Would you prefer that it actually had an

273
00:15:33,741 --> 00:15:38,240
experience or what would you prefer that
it's actually not experiencing anything,

274
00:15:38,241 --> 00:15:41,390
so you feel you don't have to feel 
guilty about what you do to it.

275
00:15:41,740 --> 00:15:44,170
It's such a difficult question because,
uh,

276
00:15:44,720 --> 00:15:49,720
you know,
it's like when you're in a relationship 

277
00:15:49,720 --> 00:15:49,720
and you say,
well,

278
00:15:49,720 --> 00:15:49,720
I love you and the other person will 
love you back.

279
00:15:49,790 --> 00:15:54,790
It's like asking or do they really love 
you back or are they just saying they 

280
00:15:54,790 --> 00:15:54,820
love you back?
Uh,

281
00:15:55,030 --> 00:16:00,030
do you,
don't you really want them to actually 

282
00:16:00,030 --> 00:16:00,770
love you?
It's hard to.

283
00:16:01,040 --> 00:16:06,040
It's hard to really know the difference 
between a everything seeming like 

284
00:16:08,181 --> 00:16:10,610
there's consciousness present,
there's intelligence president,

285
00:16:10,620 --> 00:16:12,740
there's a affection,
passion,

286
00:16:12,770 --> 00:16:14,350
love,
and,

287
00:16:14,540 --> 00:16:16,190
and it actually being there.

288
00:16:16,190 --> 00:16:17,280
I'm not sure.
Do you have,

289
00:16:17,640 --> 00:16:18,270
like,
do you ask,

290
00:16:18,271 --> 00:16:20,600
you can ask a question to make it a bit 
more point.

291
00:16:20,601 --> 00:16:22,700
That's a mass general hospital is right 
across the river,

292
00:16:22,701 --> 00:16:23,100
right?
Yes.

293
00:16:23,210 --> 00:16:28,210
Suppose,
I suppose you're going in for a medical 

294
00:16:28,210 --> 00:16:28,210
procedure and they're like,
you know,

295
00:16:28,210 --> 00:16:28,490
uh,
for,

296
00:16:28,550 --> 00:16:33,550
for anesthesia,
what we're going to do is we're gonna 

297
00:16:33,550 --> 00:16:35,620
give you a muscle relaxant so you won't 
be able to move a and you're gonna feel 

298
00:16:35,620 --> 00:16:35,840
excruciating pain during the whole 
surgery,

299
00:16:35,841 --> 00:16:37,310
but you won't be able to do anything 
about it.

300
00:16:37,550 --> 00:16:40,520
But then we're going to give you this 
drug that race is your memory of it.

301
00:16:41,930 --> 00:16:42,980
Would you be cool about that?

302
00:16:45,020 --> 00:16:48,970
What's the difference that you're 
conscious about it or not?

303
00:16:48,980 --> 00:16:49,220
If,
if,

304
00:16:49,260 --> 00:16:51,250
if there's no behavioral change.
Right,

305
00:16:51,570 --> 00:16:53,130
right.
That's a really interesting.

306
00:16:53,131 --> 00:16:54,860
That's a really clear where to put it.
That's.

307
00:16:55,660 --> 00:16:57,320
Yeah,
it feels like in that sense,

308
00:16:57,350 --> 00:17:02,350
experiencing it as a valuable quality.
So actually being able to have 

309
00:17:03,201 --> 00:17:07,130
subjective experiences,
at least in that case is,

310
00:17:07,730 --> 00:17:12,730
is valuable.
And I think we humans have a little bit 

311
00:17:12,730 --> 00:17:16,181
of a bad track record also of making 
these selfserving arguments that other 

312
00:17:16,640 --> 00:17:18,200
entities aren't conscious.
You know,

313
00:17:18,201 --> 00:17:19,250
people often say,
oh,

314
00:17:19,251 --> 00:17:21,050
these animals can't feel pain.
Right.

315
00:17:21,710 --> 00:17:26,710
It's okay to boil lobster is because we 
asked them if it hurt and I didn't say 

316
00:17:26,710 --> 00:17:30,191
anything.
And now that was just the paper out 

317
00:17:30,191 --> 00:17:30,191
saying lobsters did do feel pain when 
you boil them lemon,

318
00:17:30,191 --> 00:17:31,030
they're banning it in Switzerland and,
and,

319
00:17:31,290 --> 00:17:33,470
and we did this with slaves too often 
and say,

320
00:17:33,500 --> 00:17:38,500
oh,
they don't mind a bit on maybe 

321
00:17:38,750 --> 00:17:41,060
unconscious or women don't have souls or
whatever.

322
00:17:41,070 --> 00:17:46,070
So I'm a little bit nervous when I hear 
people just take as an axiom that 

323
00:17:46,070 --> 00:17:50,891
machines can't have experience ever.
I think this is just this really 

324
00:17:50,891 --> 00:17:52,140
fascinating science question is what it 
is.

325
00:17:52,210 --> 00:17:57,210
Yeah,
let's research it and try to figure out 

326
00:17:57,210 --> 00:17:58,790
what it is that makes the difference 
between unconscious intelligent behavior

327
00:17:58,920 --> 00:18:00,470
and conscious intelligent behavior.

328
00:18:01,080 --> 00:18:04,680
So in terms of,
if you think about Boston dynamics,

329
00:18:04,681 --> 00:18:09,681
humanoid robot being sort of a,
with a broom being pushed around the 

330
00:18:09,840 --> 00:18:14,840
it's starts is,
it starts pushing on us consciousness 

331
00:18:14,840 --> 00:18:14,840
question.
So let me ask,

332
00:18:14,840 --> 00:18:19,490
do you think an agi system,
like a few neuroscientists believe a 

333
00:18:19,710 --> 00:18:24,710
nice to have a physical embodiment needs
to have a body or something like a body?

334
00:18:25,740 --> 00:18:26,130
No,

335
00:18:27,040 --> 00:18:32,040
I don't think so.
You mean to have to have a conscious 

336
00:18:32,040 --> 00:18:34,551
experience to have consciousness?
I do think it helps a lot to have a 

337
00:18:35,071 --> 00:18:40,071
physical embodiment to learn the kind of
things about the world that are 

338
00:18:40,071 --> 00:18:41,370
important to us humans,
for sure,

339
00:18:42,570 --> 00:18:46,200
but I don't think the physical 
embodiment doesn't necessarily,

340
00:18:46,260 --> 00:18:51,260
after you've learned it,
just have the experience thinking about 

341
00:18:51,260 --> 00:18:51,260
when you're dreaming,
right?

342
00:18:51,420 --> 00:18:53,340
Your eyes are closed,
you're like any,

343
00:18:53,341 --> 00:18:58,341
any sensory input.
You're not behaving or moving in any 

344
00:18:58,341 --> 00:18:58,341
way,
but there's still an experience there.

345
00:18:58,341 --> 00:19:02,540
Right,
and so clearly the experience that you 

346
00:19:02,540 --> 00:19:04,500
have when you see something cool in your
dreams isn't coming from your eyes.

347
00:19:04,830 --> 00:19:08,250
It's just the information processing 
itself in your brain,

348
00:19:08,610 --> 00:19:10,080
which is that experience.
Right?

349
00:19:10,910 --> 00:19:13,640
But if I put it another way,
I will say,

350
00:19:13,641 --> 00:19:17,730
because it comes from neuroscience,
is the reason you want to have a body in

351
00:19:17,731 --> 00:19:22,731
a physical,
something physical like a physical 

352
00:19:23,201 --> 00:19:28,201
system is because you want to be able to
preserve something in order to have a 

353
00:19:28,201 --> 00:19:28,570
self.

354
00:19:29,270 --> 00:19:31,820
You could argue,
would you?

355
00:19:31,890 --> 00:19:36,890
You'd need to have some kind of 
embodiment of self to want to preserve.

356
00:19:38,870 --> 00:19:43,870
Well,
now we're getting a little bit on throw 

357
00:19:43,870 --> 00:19:44,510
up amorphic paths into 
anthropomorphizing things,

358
00:19:45,300 --> 00:19:47,210
talking about self preservation 
instincts.

359
00:19:47,211 --> 00:19:49,670
I mean we are evolved organisms,
right?

360
00:19:49,760 --> 00:19:53,870
Right.
So darwinian evolution then doubt us and

361
00:19:54,200 --> 00:19:57,590
other involve all the organism with a 
self preservation instinct because those

362
00:19:58,340 --> 00:20:01,610
that didn't have those self preservation
genes got cleaned out of the gene pool.

363
00:20:01,900 --> 00:20:06,900
But um,
but if you build an artificial general 

364
00:20:06,900 --> 00:20:10,751
intelligence,
the mind space that you can design is 

365
00:20:10,751 --> 00:20:13,781
much,
much larger than just a specific subset 

366
00:20:13,781 --> 00:20:13,781
of,
of minds that can evolve,

367
00:20:13,781 --> 00:20:18,550
that have so certain Agi mind doesn't 
necessarily have to have any self 

368
00:20:18,550 --> 00:20:18,920
preservation.
These things.

369
00:20:19,820 --> 00:20:24,820
It also doesn't necessarily have to be 
so individualistic as like imagine if 

370
00:20:24,820 --> 00:20:25,970
you could just.
First of all,

371
00:20:26,410 --> 00:20:27,860
we also very afraid of death.

372
00:20:28,110 --> 00:20:33,110
I suppose you could back yourself up 
every five minutes and then your 

373
00:20:33,110 --> 00:20:33,440
airplane is about to crash with like,
shucks,

374
00:20:33,441 --> 00:20:34,090
I'm just,
I'm,

375
00:20:35,010 --> 00:20:37,450
I'm going to lose the last five minutes 
of experiences.

376
00:20:37,451 --> 00:20:39,780
That's my last cloud backup bang,
you know,

377
00:20:39,810 --> 00:20:44,810
it's not as big a deal where if we could
just copy experiences between our minds 

378
00:20:45,621 --> 00:20:47,590
easily,
like we wish we easily do.

379
00:20:47,591 --> 00:20:52,591
If we were a silicon based right then 
maybe we would feel a little bit more 

380
00:20:53,981 --> 00:20:56,830
like a hive mind actually that maybe 
it's the so,

381
00:20:57,300 --> 00:21:02,300
so there's a.
So I don't think we should take for 

382
00:21:02,300 --> 00:21:04,111
granted at all that Agi will have to 
have any of those sort of competitive as

383
00:21:05,681 --> 00:21:07,240
an Alpha male instincts.
Right.

384
00:21:07,300 --> 00:21:08,290
On the other hand,
you know,

385
00:21:09,010 --> 00:21:13,750
this is really interesting because I 
think some people go too far and say,

386
00:21:13,751 --> 00:21:18,751
of course we don't have to have any 
concerns either that advanced ai will 

387
00:21:19,991 --> 00:21:23,010
have those instincts because we can 
build anything you want that there's,

388
00:21:23,240 --> 00:21:26,590
there's a very nice set of arguments 
going back to Steve.

389
00:21:26,600 --> 00:21:31,600
I'm Nick Bostrom and others just 
pointing out that when we build 

390
00:21:31,600 --> 00:21:33,580
machines,
we normally build them with some kind of

391
00:21:33,581 --> 00:21:34,390
goal.
You know,

392
00:21:34,600 --> 00:21:38,200
when this chess game drive this car 
safely or whatever,

393
00:21:38,440 --> 00:21:40,570
and as soon as you put in a goal into 
machine,

394
00:21:41,050 --> 00:21:46,050
especially if it's kind of open ended 
goal and the mission is very 

395
00:21:46,050 --> 00:21:48,451
intelligent,
it'll break that down into a bunch of 

396
00:21:48,451 --> 00:21:50,821
sub goals and I'm one of those goals 
will almost always be self preservation 

397
00:21:52,240 --> 00:21:57,240
because if it breaks or dies and the 
process is not going to accomplish the 

398
00:21:57,240 --> 00:21:57,880
goal,
I suppose you just build a little.

399
00:21:57,930 --> 00:22:02,930
You have a little robot.
Can you tell it to go down the star 

400
00:22:02,930 --> 00:22:03,910
market here and,
and get you some food,

401
00:22:03,911 --> 00:22:05,730
make a cooking Italian dinner,
you know,

402
00:22:06,100 --> 00:22:11,100
and then someone mugs that and tries to 
break it on the way that robot has an 

403
00:22:11,100 --> 00:22:15,181
incentive to the to not get destroyed 
and defend itself or runaway because 

404
00:22:15,181 --> 00:22:17,270
otherwise it's going to fail and cooking
your dinner.

405
00:22:17,270 --> 00:22:22,270
It is not afraid of death,
but it really wants to complete the 

406
00:22:22,270 --> 00:22:26,970
dinner and gold.
So it will have a self preservation 

407
00:22:26,970 --> 00:22:26,970
instinct to continue being a functional 
agent.

408
00:22:26,970 --> 00:22:27,990
And,
and,

409
00:22:28,080 --> 00:22:33,080
and,
and similarly if you give her any kind 

410
00:22:33,080 --> 00:22:36,301
of more and they just go to an Agi,
it's very likely they want to acquire 

411
00:22:36,911 --> 00:22:41,440
more resources,
can do that better and it's exactly from

412
00:22:41,441 --> 00:22:46,441
those sort of sub goals that we might 
not have intended that some of the 

413
00:22:46,441 --> 00:22:49,771
concerns about Agi safety come,
you give it some goal that seems 

414
00:22:49,771 --> 00:22:53,290
completely harmless and then before you 
realize it,

415
00:22:53,291 --> 00:22:57,190
it's also trying to do these other 
things you didn't want it to do and it's

416
00:22:57,220 --> 00:22:59,220
more maybe smarter than us.
So.

417
00:23:00,070 --> 00:23:03,550
So let me pause just because I'm,
I am,

418
00:23:04,310 --> 00:23:07,120
uh,
in a very kind of human centric way.

419
00:23:07,570 --> 00:23:10,870
See fear of death as a valuable 
motivator.

420
00:23:10,940 --> 00:23:11,750
Uh Huh.
Um,

421
00:23:11,800 --> 00:23:16,800
so you don't think as you think that's 
an artifact of evolution.

422
00:23:17,121 --> 00:23:22,121
So that's the kind of mind space 
evolution created that were sort of 

423
00:23:22,121 --> 00:23:23,960
almost obsessed about self preservation 
kind of generic.

424
00:23:23,961 --> 00:23:25,940
Well,
you don't think that's necessary

425
00:23:26,440 --> 00:23:27,250
to be

426
00:23:28,490 --> 00:23:33,490
afraid of death.
So not just a kind of sub goal of self 

427
00:23:33,490 --> 00:23:34,370
preservation,
just so you can keep doing the thing,

428
00:23:34,850 --> 00:23:39,620
but more fundamentally sort of have the 
finite thing like this ends

429
00:23:40,610 --> 00:23:43,610
for you as some point.
Interesting.

430
00:23:43,780 --> 00:23:46,070
Why do I think it's necessary before,
what?

431
00:23:46,220 --> 00:23:50,870
Precisely for intelligence but also for 
consciousness.

432
00:23:50,900 --> 00:23:55,900
So for those for both,
do you think really like a finite death 

433
00:23:57,260 --> 00:24:01,450
and the fear of it is important?
So

434
00:24:02,820 --> 00:24:07,820
before I can answer before we can agree 
on whether it's necessarily for 

435
00:24:07,820 --> 00:24:11,331
intelligence or for consciousness that 
we should be clear on how we define 

436
00:24:11,331 --> 00:24:14,151
those two words because a lot are really
smart people to find them in very 

437
00:24:14,151 --> 00:24:14,151
different ways.
Right?

438
00:24:14,151 --> 00:24:18,350
I was in this,
on this panel with AI experts and they 

439
00:24:18,350 --> 00:24:19,070
couldn't eat,
they couldn't agree on how to define the

440
00:24:19,080 --> 00:24:20,450
village and Steven.
So I,

441
00:24:20,451 --> 00:24:25,451
I define intelligence simply as the 
ability to accomplish complex goals or 

442
00:24:25,760 --> 00:24:27,300
like your broad definition because 
again,

443
00:24:27,301 --> 00:24:29,320
I don't want to be a carbon chava this 
right.

444
00:24:30,450 --> 00:24:33,270
And um,
in that case,

445
00:24:33,300 --> 00:24:35,610
no,
it certainly doesn't require it.

446
00:24:35,880 --> 00:24:40,880
Fear of death.
I would say Alphago or Alpha zero is 

447
00:24:40,880 --> 00:24:43,881
quite intelligent.
Alpha zero has any fear of being turned 

448
00:24:43,881 --> 00:24:45,140
off because it doesn't understand the 
concept of,

449
00:24:45,330 --> 00:24:50,330
of even and similarly consciousness.
I mean you could certainly imagine a 

450
00:24:51,270 --> 00:24:54,410
very simple kind of experience if,
if,

451
00:24:54,411 --> 00:24:56,250
if,
if certain plans,

452
00:24:56,251 --> 00:25:01,251
70 kind of experience,
I don't think they're very afraid of 

453
00:25:01,251 --> 00:25:01,251
dying and there's nothing they can do 
about it anyway.

454
00:25:01,251 --> 00:25:05,720
So there wasn't that much value in.
But more seriously I think if you ask 

455
00:25:07,631 --> 00:25:10,620
not just about being conscious but maybe
having a

456
00:25:12,740 --> 00:25:13,740
what you will be,
we will,

457
00:25:13,760 --> 00:25:18,760
we might call an exciting life for you.
Feel passionate and really appreciate 

458
00:25:18,760 --> 00:25:23,380
the things maybe there,
but somehow maybe there perhaps it does 

459
00:25:24,081 --> 00:25:25,880
help having a,
having that backdrop,

460
00:25:25,881 --> 00:25:27,300
the hey,
it's finite,

461
00:25:27,320 --> 00:25:29,770
it's no limits this,
make the most of this,

462
00:25:29,780 --> 00:25:31,610
this live to the fullest.
So if you,

463
00:25:31,620 --> 00:25:36,620
if,
if you knew you were going to live 

464
00:25:36,620 --> 00:25:36,620
forever,

465
00:25:36,620 --> 00:25:37,720
do you think you would change your.
Yeah,

466
00:25:37,721 --> 00:25:42,721
I mean in some perspective it would be 
an incredibly boring life living 

467
00:25:43,601 --> 00:25:46,750
forever.
So in the sort of loose subjective terms

468
00:25:46,751 --> 00:25:51,751
that you said of something exciting and 
something in this that other humans 

469
00:25:51,751 --> 00:25:52,990
would understand,
I think is.

470
00:25:52,991 --> 00:25:57,991
Yeah,
it seems that the finiteness of it is 

471
00:25:57,991 --> 00:25:57,991
important.
Well,

472
00:25:57,991 --> 00:26:02,341
the good news I have for you then is 
based on what we understand about 

473
00:26:02,341 --> 00:26:05,551
cosmology,
everything is in our university 

474
00:26:05,551 --> 00:26:10,110
ultimately probably finite dollar,
although they crunch or b a or big,

475
00:26:10,300 --> 00:26:15,300
what's the expense and the infinite.
You could have a big chill or a big 

476
00:26:15,300 --> 00:26:17,470
challenge or a big rip or the big snap 
or death bubbles.

477
00:26:18,430 --> 00:26:20,020
All of them are more than a billion 
years away,

478
00:26:20,021 --> 00:26:25,021
so we should.
We certainly have vastly more time than 

479
00:26:25,021 --> 00:26:27,850
our ancestors thought,
but they're still.

480
00:26:29,150 --> 00:26:34,150
It's still pretty hard to squeeze in an 
infinite number of compute cycles even 

481
00:26:34,150 --> 00:26:34,150
though

482
00:26:35,080 --> 00:26:37,720
there are some loopholes that just might
be possible.

483
00:26:37,721 --> 00:26:42,721
But I think I know some people like to 
say that you should live as if you're 

484
00:26:44,460 --> 00:26:49,460
about to.
You're going to die in five years or is 

485
00:26:49,460 --> 00:26:49,460
that when that's sort of optimal?
Maybe.

486
00:26:49,470 --> 00:26:54,470
It's a good assumption we should build 
our civilization as if it's all finite 

487
00:26:54,720 --> 00:26:55,950
to be on the safe side.
Right,

488
00:26:56,040 --> 00:27:01,040
exactly.
So you mentioned in defining 

489
00:27:01,040 --> 00:27:02,520
intelligence as the ability to solve 
complex goals,

490
00:27:02,970 --> 00:27:07,970
the where would you draw a line?
How would you try to define human level 

491
00:27:07,970 --> 00:27:10,590
intelligence and super human level 
intelligence?

492
00:27:10,680 --> 00:27:13,260
Where does consciousness part of that 
definition?

493
00:27:13,290 --> 00:27:16,530
No consciousness does not come into this
definition.

494
00:27:16,650 --> 00:27:18,660
So,
so I think of intelligence.

495
00:27:19,150 --> 00:27:22,380
It's a spectrum of very many different 
kinds of goals you can have.

496
00:27:22,381 --> 00:27:24,040
You can have a goal to be a good chess 
player,

497
00:27:24,180 --> 00:27:28,010
googleplay or a good car driver,
a good investor,

498
00:27:28,530 --> 00:27:30,270
good poet,
etc.

499
00:27:31,170 --> 00:27:33,730
So intelligence that by,
by its very nature,

500
00:27:33,731 --> 00:27:36,660
it isn't something you can measure,
but it's one number.

501
00:27:36,690 --> 00:27:38,140
It waS an overall goodness.
No,

502
00:27:38,180 --> 00:27:43,180
no.
There's some people who are better at 

503
00:27:43,180 --> 00:27:43,180
this.
Some people are better than that.

504
00:27:43,180 --> 00:27:44,280
Um,
right now we have machines that are much

505
00:27:44,281 --> 00:27:49,050
better than us at some very narrow tasks
like multiplying large numbers,

506
00:27:49,051 --> 00:27:53,220
fast memorizing large databases of 
playing chess,

507
00:27:53,221 --> 00:27:58,221
playing go and soon driving cars.
But there's still no machine that can 

508
00:27:58,831 --> 00:28:02,730
match a human child in general 
intelligence,

509
00:28:02,760 --> 00:28:05,700
but artificial general intelligence,
agi,

510
00:28:05,730 --> 00:28:10,730
the name of your course,
of course that is by its very 

511
00:28:10,891 --> 00:28:14,240
definition,
the the quest to build a machine,

512
00:28:14,241 --> 00:28:19,241
a machine that can do everything as well
as we can up to the old holy grail of ai

513
00:28:19,711 --> 00:28:24,711
from,
from back to its inception and the and 

514
00:28:24,711 --> 00:28:24,810
the sixties.
If that ever happens,

515
00:28:24,811 --> 00:28:29,811
of course I think it's going to be the 
biggest transition in the history of 

516
00:28:29,811 --> 00:28:29,811
life on earth,
but it.

517
00:28:29,811 --> 00:28:34,270
But it doesn't necessarily have to wait 
the big impact on until machines are 

518
00:28:34,270 --> 00:28:36,480
better than us at knitting,
that they're really big.

519
00:28:36,481 --> 00:28:39,570
Change doesn't come exactly at the 
moment.

520
00:28:39,571 --> 00:28:42,580
They're better than us at everything.
They're really big.

521
00:28:42,610 --> 00:28:47,610
chains comes first.
There are big changes when they start 

522
00:28:47,610 --> 00:28:50,241
becoming better at doing most of the 
jobs that we do because that takes away 

523
00:28:50,241 --> 00:28:55,220
much of the demand for human labor and 
then the really whopping change comes 

524
00:28:55,650 --> 00:29:00,080
when they become better than us at ai 
research.

525
00:29:00,490 --> 00:29:00,970
Right?
Right.

526
00:29:01,080 --> 00:29:03,400
Because right now the timescale of the 
irish,

527
00:29:03,420 --> 00:29:08,420
which is limited by the human research 
and development cycle of the year is 

528
00:29:09,211 --> 00:29:09,930
typically,
you know,

529
00:29:10,200 --> 00:29:15,200
along the tafe from one release of some 
software or iphone or whatever to the 

530
00:29:15,200 --> 00:29:16,660
next,
but once,

531
00:29:16,710 --> 00:29:20,370
once we have,
once google can replace 40,000

532
00:29:20,371 --> 00:29:25,371
engineers by 40,000
equivalent pieces of software or 

533
00:29:25,801 --> 00:29:28,230
whatever,
but then that doesn't.

534
00:29:28,260 --> 00:29:33,260
There's no reason that has to be yours.
It can be in principle much faster and 

535
00:29:33,260 --> 00:29:37,100
the timescale of future progress in ai 
and all of science and technology will 

536
00:29:38,340 --> 00:29:40,440
will be driven by machines,
not humans.

537
00:29:40,910 --> 00:29:45,910
So It's this simple point which gives 
rise to this incredibly fun controversy 

538
00:29:48,670 --> 00:29:51,580
about whether there can be intelligence 
explosion.

539
00:29:51,790 --> 00:29:53,970
So called singularity is vernor vinge 
called it.

540
00:29:54,460 --> 00:29:59,410
The idea is articulated by iga.
Good is obviously way back fifties,

541
00:29:59,440 --> 00:30:03,430
but you can see alan turing and others 
thought about it even earlier.

542
00:30:06,950 --> 00:30:10,780
You asked me what exactly what I define 
engelman level.

543
00:30:12,630 --> 00:30:17,630
so this.
The glib aNswer is if to say something 

544
00:30:17,630 --> 00:30:20,731
which is better than us at all,
cognitive tasks with a lot better than 

545
00:30:20,731 --> 00:30:21,760
any human at all,
cognitive tasks,

546
00:30:21,820 --> 00:30:24,520
but they're really interesting.
Bar I Think goes a little bit lower than

547
00:30:24,521 --> 00:30:26,710
that actually.
It's when they can,

548
00:30:27,040 --> 00:30:32,040
when they're better than us at ai 
programming and and qa and a general 

549
00:30:32,040 --> 00:30:36,421
learning so that they can can,
if they want to get better than us at 

550
00:30:36,421 --> 00:30:37,240
anything by the studying,

551
00:30:37,620 --> 00:30:42,620
they're better as a keyword and better 
is towards this kind of spectrum of the 

552
00:30:42,620 --> 00:30:45,270
complexity of goals it's able to 
accomplish.

553
00:30:45,271 --> 00:30:47,830
Yeah.
So another way to say,

554
00:30:48,270 --> 00:30:53,020
and that's certainly a very,
a clear definition of human love.

555
00:30:53,021 --> 00:30:55,200
So there's,
it's almost like a c that's rising.

556
00:30:55,210 --> 00:30:58,290
You could do more and more and more 
things as a graphic that you show.

557
00:30:58,590 --> 00:31:01,230
It's really nice way to put it.
So there's some peaks that,

558
00:31:01,500 --> 00:31:04,440
and there's an ocean level elevating and
you solve more and more problems,

559
00:31:04,740 --> 00:31:09,740
but you know,
just kind of a to take a pause and we 

560
00:31:09,740 --> 00:31:12,951
took a bunch of questions and a lot of 
social networks and a bunch of people 

561
00:31:12,951 --> 00:31:16,221
asked a sort of a slightly different 
direction and creativity and um,

562
00:31:16,780 --> 00:31:19,910
and,
and things that perhaps aren't a peak,

563
00:31:20,880 --> 00:31:21,680
the,
it,

564
00:31:21,700 --> 00:31:22,620
it,
it's,

565
00:31:23,190 --> 00:31:28,190
you know,
human beings are flawed and perhaps 

566
00:31:28,190 --> 00:31:28,190
better means having,
being a,

567
00:31:28,190 --> 00:31:29,700
having contradiction,
being flawed in some way.

568
00:31:30,120 --> 00:31:34,230
So let me sort of start and start easy.
First of all.

569
00:31:34,270 --> 00:31:36,420
Uh,
so you have a lot of cool equations.

570
00:31:36,510 --> 00:31:38,040
Let me ask,
what's your favorite equation?

571
00:31:38,070 --> 00:31:41,010
First of all,
I know they're all like your children,

572
00:31:41,011 --> 00:31:43,500
but which one is that?

573
00:31:45,550 --> 00:31:48,460
The master key of quantum mechanics.

574
00:31:48,510 --> 00:31:53,510
Oh,
the micro world check everything to do 

575
00:31:53,510 --> 00:31:55,630
with add on molecules and all the way 
up.

576
00:31:58,510 --> 00:31:59,260
Yeah.
So,

577
00:31:59,261 --> 00:32:01,480
okay.
So quantum mechanics is certainly a,

578
00:32:01,481 --> 00:32:04,780
a beautiful,
mysterious formulation of our world.

579
00:32:05,110 --> 00:32:08,440
So I'd like to sort of ask you,
and just as an example,

580
00:32:08,710 --> 00:32:12,100
it perhaps doesn't have the same beauty 
is physics does,

581
00:32:12,101 --> 00:32:17,101
but in mathematics at abstract,
the andrew weil's who proved the was 

582
00:32:18,050 --> 00:32:22,720
last year,
so he just saw this recently and it kind

583
00:32:22,721 --> 00:32:25,210
of caught my eye a little bit.
This is 350,

584
00:32:25,211 --> 00:32:29,560
eight years after it was conjectured.
So this very simple formulation,

585
00:32:29,860 --> 00:32:34,090
everybody tried to prove it,
everybody failed and say here's this guy

586
00:32:34,091 --> 00:32:39,091
comes along and eventually the proves it
and fails to prove it and then boost it 

587
00:32:39,171 --> 00:32:44,171
again in [inaudible] 94.
And he said like the moment when 

588
00:32:44,171 --> 00:32:47,651
everything connected into place in an 
interview said it was so indescribably 

589
00:32:47,651 --> 00:32:47,810
beaUtiful.

590
00:32:47,810 --> 00:32:52,710
That moment when you finally realize the
connecting piece of two conjectures,

591
00:32:52,720 --> 00:32:54,980
he said it was so indescribably 
beautiful.

592
00:32:55,160 --> 00:33:00,160
It was so simple and so elegant.
I couldn't understand how I missed it 

593
00:33:00,160 --> 00:33:01,640
and I just stared at it and the 
disbelief for 20 minutes.

594
00:33:01,990 --> 00:33:06,990
Then.
Then during the day I walked around the 

595
00:33:06,990 --> 00:33:09,311
department and had kimi keep coming back
to my desk looking to see if it was 

596
00:33:09,311 --> 00:33:10,550
still there.
It was still there.

597
00:33:10,551 --> 00:33:12,620
I couldn't contain myself.
I was so excited.

598
00:33:12,800 --> 00:33:15,500
It was the most important moment of my 
working life.

599
00:33:15,770 --> 00:33:20,770
Nothing I ever do again will mean as 
much to that particular moment and it 

600
00:33:21,111 --> 00:33:26,111
kinda made me think of what would it 
take and I think we have all been there 

601
00:33:26,841 --> 00:33:30,620
at small levels.
Maybe let me ask,

602
00:33:30,650 --> 00:33:35,490
have you had a moment like that in your 
life where you just had an idea as like,

603
00:33:35,900 --> 00:33:36,980
wow,
yes,

604
00:33:40,090 --> 00:33:43,330
I wouldn't mind some myself and in the 
same breath as andrew weil's,

605
00:33:43,331 --> 00:33:48,331
but I certainly had a number of have aha
moments when I realized something very 

606
00:33:51,951 --> 00:33:56,951
cool about physics,
just this completely made my head 

607
00:33:56,951 --> 00:33:56,951
explode.
In fact,

608
00:33:56,951 --> 00:33:58,330
some of my favorite discoveries I made 
late.

609
00:33:58,350 --> 00:34:03,350
I later realized that it had been 
discovered earlier by someone quite 

610
00:34:03,350 --> 00:34:07,341
famous for it,
so it's too late for me to even publish 

611
00:34:07,341 --> 00:34:07,430
it,
but that doesn't diminish in any way the

612
00:34:07,510 --> 00:34:09,900
emotional experience you have when you 
realize that like,

613
00:34:10,660 --> 00:34:11,750
wow.
yeah.

614
00:34:12,220 --> 00:34:15,040
So what would it take in that moment 
that wow,

615
00:34:15,580 --> 00:34:20,580
that was yours in that moment.
So what do you think it takes for an 

616
00:34:20,580 --> 00:34:24,410
intelligent system and agi system?
An ai system to have a moment like that.

617
00:34:25,460 --> 00:34:30,460
Yeah,
that's a tricky question because there 

618
00:34:30,460 --> 00:34:30,460
are actually two parts to it,
right?

619
00:34:30,460 --> 00:34:33,280
One of them is candidates accomplish 
that proof.

620
00:34:33,900 --> 00:34:38,900
I can prove that you can never write a 
to the end plus b to the n equals three.

621
00:34:39,750 --> 00:34:44,470
That equals z for all integer as well,
etc.

622
00:34:44,471 --> 00:34:45,760
Etc.
When,

623
00:34:45,820 --> 00:34:50,820
when is bigger than two,
the best simply in the question about 

624
00:34:50,820 --> 00:34:55,791
intelligence,
can you build machines that are 

625
00:34:55,791 --> 00:34:55,791
intelligent?

626
00:34:55,791 --> 00:34:55,791
Uh,
no.

627
00:34:55,791 --> 00:34:59,680
I think by the time we get a machine 
that can independently come up with that

628
00:34:59,701 --> 00:35:02,580
level of proofs,
probably quite close to agi.

629
00:35:03,370 --> 00:35:06,250
The second question is a question about 
consciousness.

630
00:35:06,490 --> 00:35:08,480
Oh,
when will we,

631
00:35:08,660 --> 00:35:13,660
will we'll ins how likely it is if it's 
such a machine would actually have any 

632
00:35:13,660 --> 00:35:17,541
experience at all as opposed to just 
being like a zombie and would we fact 

633
00:35:18,630 --> 00:35:23,570
that they have some sort of emotional 
response to this or anything at all akin

634
00:35:23,571 --> 00:35:28,220
to human emotion where now when it 
accomplishes its machine goal,

635
00:35:28,250 --> 00:35:32,180
is it that the views that this somehow 
something very positive and,

636
00:35:32,670 --> 00:35:35,270
and,
and sublime and,

637
00:35:35,271 --> 00:35:35,680
and,
and,

638
00:35:35,780 --> 00:35:40,780
and deeply meaningful.
I would certainly hope that if in the 

639
00:35:41,011 --> 00:35:46,011
future we do create machines that are 
our peers or even our descendants.

640
00:35:48,560 --> 00:35:53,560
Yeah,
but I would certainly hope that they do 

641
00:35:53,560 --> 00:35:54,390
have this sort of supply him as a blind 
appreciation of life.

642
00:35:55,630 --> 00:36:00,630
In a way.
My absolutely worst nightmare would be 

643
00:36:00,630 --> 00:36:04,410
that in at some point in the future

644
00:36:05,850 --> 00:36:10,850
distance future,
maybe our cosmos is teaming with all 

645
00:36:10,850 --> 00:36:13,341
this post biological life,
doing all the seemingly cool stuff and 

646
00:36:13,341 --> 00:36:18,211
maybe the last humans by the time our 
our species eventually fizzes out will 

647
00:36:20,241 --> 00:36:20,960
be like,
well,

648
00:36:20,961 --> 00:36:25,961
that's okay because we're so proud of 
our descendants here and look at what 

649
00:36:25,961 --> 00:36:29,570
all the.
My worst nightmare is that we haven't 

650
00:36:29,570 --> 00:36:33,221
solved the consciousness problem and we 
haven't realized that these are all the 

651
00:36:33,221 --> 00:36:34,780
zombies.
They're not aware of anything anymore.

652
00:36:34,790 --> 00:36:37,460
Then the tape recorder,
is it any kind of experience?

653
00:36:37,790 --> 00:36:41,450
So at the whole thing has just become a 
play for empty benches.

654
00:36:41,570 --> 00:36:44,120
That will be the ultimate zombie 
apocalypse.

655
00:36:44,620 --> 00:36:49,620
So I.
I would much rather in that case that's 

656
00:36:49,620 --> 00:36:54,590
we have these beings when we just really
appreciate how the,

657
00:36:55,210 --> 00:36:56,280
how amazing it is,

658
00:36:56,870 --> 00:37:00,860
and in that picture of what would be the
role of creativity,

659
00:37:00,861 --> 00:37:05,861
but a few people ask about creativity.
Do you think when you think about 

660
00:37:05,861 --> 00:37:07,760
intelligence,
I mean certainly the,

661
00:37:07,761 --> 00:37:12,761
uh,
the story he told at the beginning of 

662
00:37:12,761 --> 00:37:12,761
your book and,
you know,

663
00:37:12,761 --> 00:37:12,761
creating movies and so on,
started making,

664
00:37:13,840 --> 00:37:18,840
making money.
You can make a lot of money in our 

665
00:37:18,840 --> 00:37:20,870
modern world with music and movies.
So if you are an intelligent system,

666
00:37:20,871 --> 00:37:25,871
you may want to get good at that.
But that's not necessarily what I mean 

667
00:37:25,871 --> 00:37:30,101
by creativity is an important on that 
complex goals where the seas rising for 

668
00:37:31,431 --> 00:37:36,431
there to be something creative or am I 
being very human centric and thinking 

669
00:37:36,801 --> 00:37:39,320
creativity somehow special,
uh,

670
00:37:39,530 --> 00:37:40,880
relative to intelligence.

671
00:37:41,840 --> 00:37:46,840
My hunch is the,
we should think of creativity simply as 

672
00:37:47,630 --> 00:37:50,980
an aspect of intelligence.
And,

673
00:37:50,990 --> 00:37:55,990
uh,
we would have to be very careful with 

674
00:37:55,990 --> 00:38:00,230
the human vanity we have.
We have this tendency at the very often 

675
00:38:00,230 --> 00:38:01,520
one and say,
as soon as machines can do something,

676
00:38:01,521 --> 00:38:03,230
we try to diminish it and saying,
oh,

677
00:38:03,231 --> 00:38:05,290
but that's not like real intelligence,
you know,

678
00:38:05,850 --> 00:38:08,930
he was a traitor or this or that.
The other thing,

679
00:38:09,250 --> 00:38:14,250
um,
maybe if we ask ourselves to write down 

680
00:38:14,250 --> 00:38:16,290
a definition of what we actually mean by
being creative,

681
00:38:16,920 --> 00:38:18,990
what we mean by andrew weil's,
what he did there,

682
00:38:18,991 --> 00:38:22,010
for example,
don't we often mean that someone takes a

683
00:38:22,590 --> 00:38:27,590
very unexpected leap.
It's not like taking 573 and multiplying

684
00:38:29,561 --> 00:38:34,561
it By 224 by does this step of 
straightforward cookbook like rules,

685
00:38:34,630 --> 00:38:37,990
right?
This maybe major you have,

686
00:38:38,010 --> 00:38:40,600
haven't make a connection between two 
things that people have never thought it

687
00:38:40,601 --> 00:38:43,000
was connected to a surprising or 
something like that.

688
00:38:44,320 --> 00:38:49,320
I think.
I think this is an aspect of 

689
00:38:49,320 --> 00:38:49,320
intelligence.
Uh,

690
00:38:49,320 --> 00:38:52,841
and uh,
this is actually one of the most 

691
00:38:52,841 --> 00:38:55,321
important aspects of it.
Maybe the reason we humans tend to be 

692
00:38:55,321 --> 00:38:59,911
better at it than a traditional computer
is because it's something that comes 

693
00:38:59,911 --> 00:39:00,520
more naturally.
If you're a neural network,

694
00:39:01,230 --> 00:39:06,230
then if you're a traditional logic gates
based computer machine and we physically

695
00:39:06,371 --> 00:39:10,810
have all these connections and you 
activate here,

696
00:39:10,820 --> 00:39:15,820
activated here,
activity here at the time,

697
00:39:16,530 --> 00:39:19,090
my hunch is that if we ever build a 
machine,

698
00:39:20,600 --> 00:39:22,280
well you could just give it the task.
Hey,

699
00:39:22,281 --> 00:39:23,090
hey,
uh,

700
00:39:23,290 --> 00:39:24,740
uh,
you,

701
00:39:25,060 --> 00:39:25,670
you say,
hey,

702
00:39:25,671 --> 00:39:29,180
you know,
I just realized that I have,

703
00:39:29,240 --> 00:39:32,300
I want to travel around the world.
Is that this month?

704
00:39:32,301 --> 00:39:34,580
Can you teach my eight age course for 
me?

705
00:39:34,581 --> 00:39:35,390
And it's like,
okay,

706
00:39:35,391 --> 00:39:40,391
I'll do it.
And it does everything that you would 

707
00:39:40,391 --> 00:39:40,391
have done and the improvisers and stuff.
Yeah.

708
00:39:40,391 --> 00:39:44,530
That,
that would in my mind involve a lot of 

709
00:39:44,530 --> 00:39:44,530
creativity.

710
00:39:44,530 --> 00:39:45,660
Yeah.
So at sexy and beautiful way to put it.

711
00:39:45,661 --> 00:39:50,661
I think we do try to grab grasp at the,
you know,

712
00:39:50,881 --> 00:39:51,130
the,
the,

713
00:39:51,210 --> 00:39:53,070
the definition of intelligence is 
everything.

714
00:39:53,071 --> 00:39:56,340
We don't understand how a,
how to build.

715
00:39:56,341 --> 00:40:01,341
So like,
so we as humans try to find things that 

716
00:40:01,341 --> 00:40:01,341
we have,
uh,

717
00:40:01,341 --> 00:40:05,401
machines don't have.
And maybe creativity is just one of the 

718
00:40:05,401 --> 00:40:05,430
things.
One of the words we use to describe that

719
00:40:05,820 --> 00:40:07,010
as sort,
really interesting way to put it.

720
00:40:07,060 --> 00:40:08,370
I don't think we need to

721
00:40:08,590 --> 00:40:11,290
be that defensive.
I don't think anything good comes out of

722
00:40:11,291 --> 00:40:11,640
saying,
well,

723
00:40:11,650 --> 00:40:13,330
we're somehow special.
You know,

724
00:40:13,940 --> 00:40:14,890
it's,
it's,

725
00:40:14,891 --> 00:40:19,891
um,
con is there are many examples in 

726
00:40:20,651 --> 00:40:25,651
history of where trying to pretend that 
we're somehow superior to all other 

727
00:40:29,630 --> 00:40:33,110
intelligent beings has led to pretty bad
results.

728
00:40:33,111 --> 00:40:35,920
Right?
Right.

729
00:40:35,921 --> 00:40:40,921
And nazi Germany,
they said that they were superior to 

730
00:40:40,921 --> 00:40:40,921
other people.
Uh,

731
00:40:40,921 --> 00:40:43,990
today we still do a lot of cruelty to 
animals by saying that we're so superior

732
00:40:44,050 --> 00:40:45,940
somehow on the,
they cAn't feel pain.

733
00:40:46,300 --> 00:40:49,880
A slavery was justified by the same kind
of just really weak,

734
00:40:50,260 --> 00:40:51,300
weak arguments.
And,

735
00:40:51,340 --> 00:40:52,480
and,
and,

736
00:40:52,481 --> 00:40:57,481
uh,
I don't think if we actually go ahead 

737
00:40:57,481 --> 00:40:58,570
and build artificial general 
intelligence,

738
00:40:59,160 --> 00:41:02,840
it can do things better than us.
I don't think we should try to found our

739
00:41:02,841 --> 00:41:07,841
self worth on some sort of bogus claims 
of superiority in terms of our 

740
00:41:09,931 --> 00:41:12,100
intelligence.
Alright.

741
00:41:12,150 --> 00:41:15,900
I think we should instead find our,
a

742
00:41:17,550 --> 00:41:20,860
calling and then the meaning of life 
from,

743
00:41:20,870 --> 00:41:23,140
from the experiences that we have right 
now,

744
00:41:23,430 --> 00:41:28,430
I can have,
I can have very meaningful experiences 

745
00:41:28,830 --> 00:41:30,720
even if there are other people who are 
smarter than me,

746
00:41:30,750 --> 00:41:35,750
you know,
when I go to the faculty meeting here 

747
00:41:35,750 --> 00:41:35,780
and I was like,
are we talking about something?

748
00:41:35,781 --> 00:41:36,680
And then I suddenly realize,
oh,

749
00:41:36,730 --> 00:41:39,080
but he has an old price.
He has an old price.

750
00:41:39,081 --> 00:41:40,700
He has no pride.
I don't have one.

751
00:41:40,820 --> 00:41:45,820
Does that make me enjoy life any less or
enjoy talking to those people?

752
00:41:47,241 --> 00:41:51,220
That's of course not right.
And the ontario is,

753
00:41:51,270 --> 00:41:56,270
I,
I feel very honored and privileged to 

754
00:41:56,270 --> 00:41:56,270
get to interact with,
with,

755
00:41:56,270 --> 00:41:57,390
uh,
other,

756
00:41:57,410 --> 00:41:59,500
very intelligent beings that are better 
than me.

757
00:41:59,750 --> 00:42:04,750
A lot of stuff.
So I don't think there's any reason why 

758
00:42:04,750 --> 00:42:05,570
we can't have the same approach with 
intelligent machines.

759
00:42:06,080 --> 00:42:07,190
That's a really interesting.

760
00:42:07,310 --> 00:42:10,150
So people don't often think about that.
They think about when there's going.

761
00:42:10,520 --> 00:42:12,980
If there's machines that are more 
intelligent,

762
00:42:13,220 --> 00:42:18,220
he naturally think that that's not going
to be a beneficial type of intelligence.

763
00:42:19,010 --> 00:42:21,440
You don't realize it could be,
you know,

764
00:42:21,470 --> 00:42:26,470
like peers with nobel prizes that,
that will be just fun to talk with and 

765
00:42:26,470 --> 00:42:28,730
they might be clever about certain 
topics and uh,

766
00:42:28,731 --> 00:42:30,740
you can have fun having a few drinks 
with them.

767
00:42:31,130 --> 00:42:32,990
So.
Well also,

768
00:42:32,991 --> 00:42:35,030
you know,
another example.

769
00:42:35,170 --> 00:42:40,170
So we can all relate to it of why it 
doesn't have to be a terrible thing to 

770
00:42:40,170 --> 00:42:42,650
be impressed with the friends of the 
people that are even smarter than us all

771
00:42:42,651 --> 00:42:45,560
around is when,
when you and I were both two years old,

772
00:42:45,561 --> 00:42:48,710
I mean our parents were much more 
intelligent than us right here.

773
00:42:48,980 --> 00:42:53,980
Worked out okay because their goals were
aligned with our goals and that I think 

774
00:42:54,861 --> 00:42:59,861
is really the number one issue we have 
to solve if we value in line with the 

775
00:43:01,521 --> 00:43:03,050
value alignment problem.
Exactly.

776
00:43:03,051 --> 00:43:06,500
Because people who see too many 
hollywood movies,

777
00:43:06,550 --> 00:43:09,770
a lousy science fiction,
a plot lines,

778
00:43:10,010 --> 00:43:11,620
they worry about the wrong thing,
right?

779
00:43:12,140 --> 00:43:14,780
They worry about some machines only 
attorney evil.

780
00:43:16,310 --> 00:43:20,520
It's not malice that we,
that's the issue.

781
00:43:20,830 --> 00:43:23,750
The concern,
it's competence by definition.

782
00:43:24,530 --> 00:43:27,410
Intelligent makes you,
makes you very competent.

783
00:43:27,411 --> 00:43:32,411
Did you have a more intelligent goal 
playing mr computer playing as the less 

784
00:43:33,021 --> 00:43:38,021
intelligent one and when we define 
intelligence is the ability to 

785
00:43:38,021 --> 00:43:38,021
accomplish,
go winning,

786
00:43:38,021 --> 00:43:39,410
right?
It's going to be in the more intelligent

787
00:43:39,411 --> 00:43:41,430
one that wins and if you have

788
00:43:42,760 --> 00:43:47,760
are human and then you have um,
an agi and that's more intelligent than 

789
00:43:47,760 --> 00:43:50,200
all ways and they have different goals.
Guess was going to get their way.

790
00:43:50,201 --> 00:43:51,490
Right?
So I was just reading about,

791
00:43:52,000 --> 00:43:57,000
I was just reading about this particular
rhinoceros species that was driven 

792
00:43:57,641 --> 00:44:02,641
extinct just a few years ago.
I was looking at this cute picture of 

793
00:44:02,641 --> 00:44:05,140
mommy rhinoceros with it's that child,
you know,

794
00:44:05,141 --> 00:44:10,141
and why did we humans drive it to 
extinction wasn't as big as we were 

795
00:44:10,210 --> 00:44:12,070
evolved rhino haters,
right?

796
00:44:12,220 --> 00:44:13,810
As a whole.
It was just because we,

797
00:44:13,840 --> 00:44:16,930
our goals weren't aligned with those of 
the rhinoceros and it didn't work out so

798
00:44:16,931 --> 00:44:18,970
well for the rhinoceros because we were 
more intelligent.

799
00:44:19,000 --> 00:44:24,000
Right?
So I think is just so important that if 

800
00:44:24,000 --> 00:44:26,671
we ever do build agi before we unleash 
anything,

801
00:44:27,160 --> 00:44:32,160
we have to make sure that it learns to 
understand our goals adopts our goals.

802
00:44:36,030 --> 00:44:36,630
And it

803
00:44:36,940 --> 00:44:38,770
retains those goals.
So the cool,

804
00:44:38,830 --> 00:44:43,830
interesting problem there is being able 
to us as human beings trying to 

805
00:44:44,801 --> 00:44:47,920
formulate our values.
So you know,

806
00:44:47,921 --> 00:44:50,740
you could think of the United States 
constitution as a,

807
00:44:50,741 --> 00:44:54,490
as a way that people sat down at the 
time,

808
00:44:54,491 --> 00:44:57,700
a bunch of white men,
but which is a good example,

809
00:44:57,701 --> 00:44:59,200
I should,
should say,

810
00:44:59,530 --> 00:45:04,530
uh,
they formulated the goals for this 

811
00:45:04,530 --> 00:45:06,091
country and a lot of people agree that 
those goals actually held up pretty 

812
00:45:06,091 --> 00:45:09,391
well.
It's an interesting formulation of 

813
00:45:09,391 --> 00:45:09,391
values and failed miserably in other 
ways.

814
00:45:09,400 --> 00:45:13,360
So for the value alignment problem and 
the solution to it,

815
00:45:13,361 --> 00:45:16,880
we have to be able to put on paper,
uh,

816
00:45:16,881 --> 00:45:18,210
or in,
in a,

817
00:45:18,211 --> 00:45:20,440
in a program,
human values.

818
00:45:20,460 --> 00:45:22,030
How difficult do you think that is?

819
00:45:22,390 --> 00:45:25,810
Varied,
but it's so important.

820
00:45:25,870 --> 00:45:29,800
We really have to give it our best and 
it's difficult for two separate reasons.

821
00:45:30,190 --> 00:45:35,190
There's the technical value alignment 
problem of figuring out how to make 

822
00:45:37,270 --> 00:45:40,180
machines understand the goals,
document routine them.

823
00:45:40,480 --> 00:45:43,210
And then there's the separate part of 
it.

824
00:45:43,211 --> 00:45:45,370
The philosophical part was values 
anyway.

825
00:45:46,000 --> 00:45:51,000
And since we,
it's not like we have any great 

826
00:45:51,000 --> 00:45:51,000
consensus on this planet,
on values,

827
00:45:51,000 --> 00:45:52,780
how,
what mechanisms should we create then to

828
00:45:52,781 --> 00:45:54,100
aggregate them decide,
okay,

829
00:45:54,101 --> 00:45:58,000
what's a good compromise?
Right at that second discussion,

830
00:45:58,001 --> 00:46:00,880
can't just be left the tech nerds like 
myself,

831
00:46:00,910 --> 00:46:01,540
right?
That's right.

832
00:46:01,620 --> 00:46:05,680
And if we refused to talk about it and 
then agi gets built,

833
00:46:05,730 --> 00:46:10,730
who is going to be actually making the 
decision about whose values it's gonna 

834
00:46:10,730 --> 00:46:11,170
be a bunch of dudes and some tech 
company.

835
00:46:11,171 --> 00:46:16,171
Okay.
And are they necessarily should so 

836
00:46:16,330 --> 00:46:19,290
representative all of human kind that we
want to just enjoy the development.

837
00:46:19,510 --> 00:46:24,510
Are they even uniquely qualified to 
speak to future human happiness just 

838
00:46:24,910 --> 00:46:29,910
because they're good at programming ai?
I'd much rather have this be a really 

839
00:46:29,910 --> 00:46:29,910
inclusive conversation,

840
00:46:30,240 --> 00:46:32,530
but do you think it's possible sort of 
that.

841
00:46:32,600 --> 00:46:36,600
So you create a beautiful vision that 
includes a.

842
00:46:36,601 --> 00:46:41,601
So the diversity,
cultural diversity and various 

843
00:46:41,601 --> 00:46:42,000
perspectives on discussing rights,
freedoms,

844
00:46:42,090 --> 00:46:47,090
human dignity,
but how hard is it to come to that 

845
00:46:47,090 --> 00:46:47,160
consensus?
Do you think?

846
00:46:48,060 --> 00:46:51,690
It's certainly a really important thing 
that we should all try to do,

847
00:46:51,900 --> 00:46:53,160
but do you think it's feasible?

848
00:46:54,260 --> 00:46:59,260
I think there's no better way to 
guarantee failure than through to refuse

849
00:47:00,591 --> 00:47:05,591
to talk about it or if you try,
and I also think it's a really bad at 

850
00:47:05,591 --> 00:47:05,810
strategy to say,
okay,

851
00:47:05,811 --> 00:47:10,811
let's first have a discussion for a long
time and then once we reached complete 

852
00:47:10,811 --> 00:47:14,891
consensus,
then we'll try to load it into some 

853
00:47:14,891 --> 00:47:14,891
machine.
No,

854
00:47:14,891 --> 00:47:16,280
we shouldn't let perfect be the enemy of
good.

855
00:47:16,550 --> 00:47:20,660
Instead we should start with the 
kindergarten ethics.

856
00:47:20,770 --> 00:47:23,450
Pretty much everybody agrees on and put 
that into our machines.

857
00:47:23,451 --> 00:47:26,650
Now we're not doing that.
Even look at the,

858
00:47:26,690 --> 00:47:31,690
you know,
anyone who builds as a passenger 

859
00:47:31,690 --> 00:47:34,441
aircraft wants it to never under any 
circumstances and fly into a building or

860
00:47:34,571 --> 00:47:35,320
a mountain.
Right.

861
00:47:35,710 --> 00:47:40,040
Yet the september 11 hijackers were able
to do that and even more embarrassing,

862
00:47:40,110 --> 00:47:41,820
you know,
under his little bits,

863
00:47:41,890 --> 00:47:46,600
this depressed germanwings pilot when he
flew his passenger jeff into the alps,

864
00:47:46,660 --> 00:47:47,710
killing over a hundred people.

865
00:47:48,460 --> 00:47:52,670
He just told him what a pilot to do it.
He told the freaking computer change the

866
00:47:52,710 --> 00:47:57,710
altitude a hundred meters and even 
though it had the gps maps everything,

867
00:47:58,150 --> 00:47:59,740
the computer was like,
okay,

868
00:48:00,610 --> 00:48:04,240
so we should.
We should take those very basic values,

869
00:48:04,520 --> 00:48:07,000
so where the problem is,
not that we don't agree,

870
00:48:08,270 --> 00:48:13,270
maybe a problem is just we've been too 
lazy to try to put it into our machines 

871
00:48:13,270 --> 00:48:14,290
and make sure that from now on arab 
airplanes,

872
00:48:14,291 --> 00:48:16,950
we'll just,
which all have computers in them,

873
00:48:16,960 --> 00:48:19,450
but we'll just just refused to do 
something like that,

874
00:48:19,780 --> 00:48:21,990
go into safe mode,
maybe lock the cockpit,

875
00:48:22,000 --> 00:48:27,000
the org or the nearest airport and and 
there's so much other technology and in 

876
00:48:27,250 --> 00:48:32,250
our world as well now where it's really 
coming quite timely to put in some sort 

877
00:48:32,651 --> 00:48:35,080
of very basic values like this.
Even in cars,

878
00:48:35,720 --> 00:48:40,720
we had enough a vehicle terrorism 
attacks by now will be have driven 

879
00:48:40,720 --> 00:48:45,301
trucks and vans into pedestrians.
That is not at all a crazy idea that it 

880
00:48:45,671 --> 00:48:49,360
does have that hardwired into the car 
because yeah,

881
00:48:49,361 --> 00:48:54,361
there were a lot of.
There's always going to be people who 

882
00:48:54,361 --> 00:48:56,581
for some reason want to harm others,
but most of those people don't have the 

883
00:48:56,581 --> 00:48:58,570
technical expertise to figure out how to
work around something like that.

884
00:48:58,571 --> 00:49:01,780
So if the car just won't do it,
it helps.

885
00:49:01,781 --> 00:49:02,770
So let's start there.

886
00:49:02,840 --> 00:49:04,970
So there's a lot of,
as a great point,

887
00:49:04,971 --> 00:49:06,830
so not,
not chasing perfect.

888
00:49:06,831 --> 00:49:10,790
There's a lot of things that,
a lot that most of the world agrees on.

889
00:49:10,840 --> 00:49:11,810
Yeah.
Let's start there.

890
00:49:11,870 --> 00:49:12,470
Let's start there.

891
00:49:12,590 --> 00:49:12,960
And,
and,

892
00:49:13,060 --> 00:49:18,060
and then once we start there,
we'll also get into the habit of having 

893
00:49:18,060 --> 00:49:18,560
these kinds of conversations about,
okay,

894
00:49:18,561 --> 00:49:20,200
what else should we put in here?
And

895
00:49:20,660 --> 00:49:23,520
having these discussions.
This should be a gradual process then.

896
00:49:23,940 --> 00:49:24,720
Great.
So,

897
00:49:25,140 --> 00:49:30,140
but that also means describing these 
things and describing it to a machine.

898
00:49:31,230 --> 00:49:35,210
So one thing we had a few conversations 
with Steven Wolf from,

899
00:49:35,660 --> 00:49:37,310
I'm not sure if you're familiar with 
stephen bosch.

900
00:49:37,311 --> 00:49:39,470
I know him quite well,
so he has,

901
00:49:39,550 --> 00:49:40,240
you know,
he played,

902
00:49:40,250 --> 00:49:42,500
he works at a bunch of things,
but you know,

903
00:49:42,501 --> 00:49:46,370
cellular automata,
these simple computable things,

904
00:49:46,580 --> 00:49:49,520
these computation systems and you kind 
of mentioned that,

905
00:49:49,521 --> 00:49:54,521
you know,
we probably have already within these 

906
00:49:54,521 --> 00:49:54,560
systems already something that's agi.

907
00:49:55,280 --> 00:49:55,940
Wow.
Um,

908
00:49:56,100 --> 00:49:56,970
meaning like

909
00:49:57,710 --> 00:49:59,930
we just don't know it because we can't 
talk to it.

910
00:50:00,410 --> 00:50:05,410
So if you give me this chance to try to 
try to at least form a question out of 

911
00:50:05,691 --> 00:50:06,590
this is

912
00:50:07,630 --> 00:50:10,080
I think it's an interesting idea to.

913
00:50:10,680 --> 00:50:12,660
I think that we can have intelligent 
systems,

914
00:50:12,661 --> 00:50:17,661
but we don't know how to describe 
something to them and they can't 

915
00:50:17,661 --> 00:50:18,690
communicate with us.
I know you're doing a little bit of work

916
00:50:18,691 --> 00:50:21,540
and explainable ai trying to get ai to 
explain itself.

917
00:50:22,020 --> 00:50:27,020
So what are your thoughts of natural 
language processIng or some kind of 

918
00:50:27,020 --> 00:50:28,000
other communication?
How.

919
00:50:28,100 --> 00:50:30,110
How does the ai explained something to 
us?

920
00:50:30,111 --> 00:50:34,910
How do we explain something to it to 
machines or you think of it differently.

921
00:50:35,310 --> 00:50:38,880
So there are two separate parts to your 
question there.

922
00:50:38,900 --> 00:50:42,440
Either one of them has to do with 
communication,

923
00:50:42,441 --> 00:50:44,210
which is super interesting.
Yell and get that in a sec.

924
00:50:44,550 --> 00:50:49,550
The other is whether we already have agi
will you just haven't noticed that 

925
00:50:51,790 --> 00:50:52,810
there.
I beg to differ.

926
00:50:52,811 --> 00:50:57,811
I don't think there's anything in any 
cellular automaton or anything or the 

927
00:50:57,811 --> 00:51:02,731
internet itself or whatever that has 
artificial general intelligence ends it.

928
00:51:03,890 --> 00:51:06,700
It's an really do exactly everything we 
humans can do better.

929
00:51:07,000 --> 00:51:08,390
I think that they,
if they,

930
00:51:08,430 --> 00:51:11,620
that happens,
when that happens,

931
00:51:11,621 --> 00:51:15,640
we will very soon notice will probably 
notice even before and if,

932
00:51:15,641 --> 00:51:17,050
because in a very,
very big way,

933
00:51:17,200 --> 00:51:17,790
uh,
but for,

934
00:51:17,840 --> 00:51:18,660
for the second part,

935
00:51:19,690 --> 00:51:20,740
can I,
sorry.

936
00:51:20,741 --> 00:51:22,060
So,
uh,

937
00:51:22,120 --> 00:51:27,120
the cause you,
you have this beautiful way to 

938
00:51:27,120 --> 00:51:27,880
formulating consciousness as a,
as a,

939
00:51:28,810 --> 00:51:33,810
you know,
as Information processing and you can 

940
00:51:33,810 --> 00:51:33,810
think of intelligence and information 
processing in this.

941
00:51:33,810 --> 00:51:37,980
You can think of the entire universe as 
these particles and these systems 

942
00:51:37,980 --> 00:51:40,870
roaming around that have this 
information processing power.

943
00:51:41,300 --> 00:51:42,010
It,
you don't,

944
00:51:42,430 --> 00:51:47,430
you don't think there is something with 
the power to process information in the 

945
00:51:47,591 --> 00:51:52,430
way that we human beings do that's out 
there that um,

946
00:51:52,970 --> 00:51:57,970
that needs to be sort of connected to.
It seems a little bit philosophical 

947
00:51:57,970 --> 00:52:01,990
perhaps,
but there's something compelling to the 

948
00:52:01,990 --> 00:52:04,340
idea that the power is already there,
which is the focus should be more on the

949
00:52:04,930 --> 00:52:07,730
and being able to communicate with it.
Well,

950
00:52:08,420 --> 00:52:09,560
I agree that the,

951
00:52:10,750 --> 00:52:11,960
and some,
in a certain sense,

952
00:52:11,961 --> 00:52:14,900
the hardware processing power is already
out there.

953
00:52:15,610 --> 00:52:16,790
Our universe itself

954
00:52:18,400 --> 00:52:20,720
can think of it as being a computer 
already,

955
00:52:20,780 --> 00:52:25,780
right?
it's constantly computing what water 

956
00:52:25,780 --> 00:52:28,151
waves,
how to evolve the waterway waves in the 

957
00:52:28,151 --> 00:52:30,401
river charles and how to move the air 
molecules around the seth lloyd has 

958
00:52:30,401 --> 00:52:34,191
pointed out my colleague here that you 
can even in a very rigorous way of 

959
00:52:34,191 --> 00:52:35,270
thinking of our entire universe is being
a quantum computer.

960
00:52:35,510 --> 00:52:40,510
It's pretty clear that our universe 
supports this amazing processing power 

961
00:52:40,510 --> 00:52:44,750
because you can even within this physics
computer that we live in,

962
00:52:44,751 --> 00:52:49,751
right,
we can even build actually laptops and 

963
00:52:49,751 --> 00:52:49,751
stuff so cleaner.
The power is there.

964
00:52:49,751 --> 00:52:52,270
It's just that most of the compute power
that nature has its,

965
00:52:52,280 --> 00:52:57,280
in my opinion,
kind of wasting on boring stuff like 

966
00:52:57,280 --> 00:52:57,450
simulating yet another ocean wave 
somewhere where no one is even looking.

967
00:52:57,480 --> 00:52:59,900
Right.
So in a sense or what life does,

968
00:53:00,170 --> 00:53:04,400
what we are doing when we build the 
computers is where rechanneling all this

969
00:53:04,910 --> 00:53:06,920
compute that nature is doing anyway.

970
00:53:07,030 --> 00:53:12,030
Right?
And they're doing things that are more 

971
00:53:12,030 --> 00:53:12,030
interesting than just yet another ocean 
wave,

972
00:53:12,030 --> 00:53:13,100
you know,
and do something cool here.

973
00:53:14,090 --> 00:53:17,110
So the roy hardware power is there for 
sure,

974
00:53:17,120 --> 00:53:22,120
But.
And then even just keep shooting what's 

975
00:53:22,120 --> 00:53:23,030
going to happen for the next five 
seconds in this water bottle,

976
00:53:23,031 --> 00:53:28,031
you know,
takes a ridiculous amount of compute if 

977
00:53:28,031 --> 00:53:28,031
you do it on a human computer.
Yeah,

978
00:53:28,031 --> 00:53:29,340
just bought a ball,
does did it.

979
00:53:29,940 --> 00:53:34,940
But that does not mean that this water 
bottle has agi and because agi means it 

980
00:53:35,911 --> 00:53:39,270
should also be able to have written my 
book done this interview.

981
00:53:39,320 --> 00:53:44,320
Yes.
And I don't think it's just the 

982
00:53:44,320 --> 00:53:46,160
communication problems.
I think it's 10 do it and other 

983
00:53:47,101 --> 00:53:51,730
buddhists say when they watch the water 
and that there is some beauty that there

984
00:53:51,731 --> 00:53:54,720
is some depth in nature that they can 
communicate with.

985
00:53:54,870 --> 00:53:57,350
Communication is also very important 
because I mean,

986
00:53:57,660 --> 00:54:02,660
look,
I'm a part of my job is being a teacher 

987
00:54:02,660 --> 00:54:06,230
and I know some very intelligent 
professors even who just have a bit of 

988
00:54:08,080 --> 00:54:09,120
hard time communicating.

989
00:54:09,810 --> 00:54:14,520
They have all these brilliant ideas,
but to communicate with somebody else,

990
00:54:14,521 --> 00:54:16,650
you have to also be able to simulate the
own mind.

991
00:54:16,920 --> 00:54:21,920
Yes.
Empathy built well enough that under 

992
00:54:21,920 --> 00:54:24,400
that model of their mind that you can 
say things that they will understand and

993
00:54:24,540 --> 00:54:28,380
that's quite difficult.
And that's why today it's so frustrating

994
00:54:28,440 --> 00:54:33,200
if you have a computer that's makes some
cancer diagnosis and you ask it,

995
00:54:33,210 --> 00:54:38,210
well,
why are you saying I should have this 

996
00:54:38,210 --> 00:54:38,210
surgery if it and if you don't want to 
reply,

997
00:54:38,210 --> 00:54:42,990
I was trained on five terabytes of data 
and this is my diagnosis.

998
00:54:43,771 --> 00:54:44,400
Boop,
boop,

999
00:54:44,401 --> 00:54:49,401
beep,
beep doesn't really instill a lot of 

1000
00:54:49,401 --> 00:54:49,401
confidence.
Right,

1001
00:54:49,401 --> 00:54:53,450
right.
So I think we have a lot of work to do 

1002
00:54:53,450 --> 00:54:53,910
on um,
on communication there.

1003
00:54:54,330 --> 00:54:55,890
So what kind of,
we'll kind of,

1004
00:54:55,891 --> 00:55:00,891
um,
I think you're doing a little bit of 

1005
00:55:00,891 --> 00:55:00,891
work and explainable ai,
uh,

1006
00:55:00,891 --> 00:55:01,320
what do you think are the most promising
avenues?

1007
00:55:01,320 --> 00:55:06,320
Is it mostly about sort of the alexa 
problem of natural language processing,

1008
00:55:06,691 --> 00:55:11,691
of being able to actually use human 
interpretable methods of communication?

1009
00:55:13,110 --> 00:55:15,840
So being able to talk to a system and it
talked back to you,

1010
00:55:16,020 --> 00:55:18,480
or is there some more fundamental 
problems to be solved?

1011
00:55:18,630 --> 00:55:20,920
I think it's all of the above.
Human,

1012
00:55:21,070 --> 00:55:23,520
the natural language processing is 
obviously important,

1013
00:55:23,521 --> 00:55:27,560
but they're also more nerdy fundamental 
problems.

1014
00:55:27,570 --> 00:55:30,130
Like if you,
if you take a,

1015
00:55:30,150 --> 00:55:34,280
you play chess,
you have to have to go to,

1016
00:55:38,040 --> 00:55:43,040
when did you learn russian?
When you watch them teach yourself 

1017
00:55:44,001 --> 00:55:45,940
rushing shit though.
Watch a mogul mom.

1018
00:55:46,140 --> 00:55:48,240
Bill stuff enough.
Wow.

1019
00:55:50,320 --> 00:55:52,380
Languages do you know?
Wow,

1020
00:55:52,381 --> 00:55:56,320
that's really impressive.
Wife has some contact basis.

1021
00:55:56,330 --> 00:55:58,530
But my point was if you play chess,
you have,

1022
00:55:58,531 --> 00:56:00,330
you looked at the alpha zero games,
the,

1023
00:56:00,620 --> 00:56:01,680
uh,
the actual games.

1024
00:56:01,681 --> 00:56:04,800
Now check it out.
Some of them are just mind blowing.

1025
00:56:06,330 --> 00:56:07,920
Really beautiful.
And,

1026
00:56:07,921 --> 00:56:12,921
and if you ask how did it do that?
You go talk to them as our base.

1027
00:56:16,300 --> 00:56:20,460
I know others from beat mine,
all they will ultimately be able to give

1028
00:56:20,461 --> 00:56:25,461
you is a big tables of numbers,
matrices that define the neural network 

1029
00:56:25,720 --> 00:56:30,720
and you can stare at these people's 
numbers until your face turns blue and 

1030
00:56:30,720 --> 00:56:34,270
you're not going to understand much 
about why it made that move.

1031
00:56:34,540 --> 00:56:39,540
And uh,
even if you have a natural language 

1032
00:56:39,540 --> 00:56:42,151
processing,
they can tell you in human language 

1033
00:56:42,151 --> 00:56:42,151
about,
oh,

1034
00:56:42,151 --> 00:56:42,250
five,
seven points to eight,

1035
00:56:42,540 --> 00:56:47,540
still not going to really help.
So I think think there's a whole 

1036
00:56:47,540 --> 00:56:50,521
speCtrum of,
of a fun challenge they're involved in 

1037
00:56:50,521 --> 00:56:54,121
and taking it computation does 
intelligent things and transforming it 

1038
00:56:54,121 --> 00:56:58,141
into something equally good,
equally intelligent,

1039
00:56:59,770 --> 00:57:04,770
but that's more understandable and I 
think that's really valuable because I 

1040
00:57:04,770 --> 00:57:04,770
think

1041
00:57:05,860 --> 00:57:09,730
as we put machines in charge of ever 
more infrastructure in our world,

1042
00:57:09,731 --> 00:57:12,580
the power grid that is trading on the 
stock market,

1043
00:57:12,640 --> 00:57:17,640
weapons systems and so on,
it's absolutely vital that we can trust 

1044
00:57:17,740 --> 00:57:22,740
these ais to do all we want and trust 
really comes from understanding in a 

1045
00:57:22,740 --> 00:57:24,980
very fundamental way.
And um,

1046
00:57:25,630 --> 00:57:27,760
that's why I'm,
that's why I'm working on this because I

1047
00:57:27,761 --> 00:57:30,510
think the more if we're going to have 
some hope of,

1048
00:57:30,511 --> 00:57:35,511
of ensuring that machines have adopted 
our goals and that they're going to 

1049
00:57:35,511 --> 00:57:39,151
retain them,
that kind of trust I think needs to be 

1050
00:57:39,431 --> 00:57:41,170
based on things you can actually 
understand,

1051
00:57:41,171 --> 00:57:44,270
preferably even make it had preferably 
have improved serums on it.

1052
00:57:44,350 --> 00:57:45,940
Even with a self driving car,
right?

1053
00:57:47,050 --> 00:57:52,050
If someone just tells you it's been 
trained on tons of data and it never 

1054
00:57:52,050 --> 00:57:52,050
crashed,
it's,

1055
00:57:52,050 --> 00:57:53,710
it's less reassuring then if someone 
actually has a proof,

1056
00:57:54,220 --> 00:57:59,220
maybe it's a computer verified proof,
but still it says that under no 

1057
00:57:59,220 --> 00:58:01,930
circumstances is this car just going to 
swerve into oncoming traffic

1058
00:58:02,030 --> 00:58:04,160
and,
and that kind of information helps build

1059
00:58:04,161 --> 00:58:07,910
trust and build the alignment,
the alignment of goals,

1060
00:58:07,911 --> 00:58:11,920
the at least awareness that your goals,
your values that align.

1061
00:58:12,220 --> 00:58:14,500
And I think even the short term,
if you look at her,

1062
00:58:14,860 --> 00:58:16,390
you know that today,
right?

1063
00:58:16,391 --> 00:58:21,391
This absolutely pathetic state of 
cybersecurity that we have right when 

1064
00:58:21,391 --> 00:58:21,730
it's,
what is it,

1065
00:58:21,731 --> 00:58:26,731
3 billion yahoo accounts,
which app pack almost every american's 

1066
00:58:29,741 --> 00:58:32,130
credit card and so on,
uh,

1067
00:58:32,800 --> 00:58:37,800
why is this happening?
It's ultimately happening because we 

1068
00:58:37,800 --> 00:58:40,570
have software took nobody fully 
understood how it worked.

1069
00:58:41,260 --> 00:58:44,200
That's why the bugs hadn't been found.
Right?

1070
00:58:44,210 --> 00:58:49,210
Right.
And I think ai can be used very 

1071
00:58:49,210 --> 00:58:51,400
effectively for offense for hacking,
but it can also be used for defense.

1072
00:58:52,390 --> 00:58:57,390
Hopefully automating verifiability and 
creating systems that are built in 

1073
00:59:00,361 --> 00:59:02,340
different ways so you can actually prove
things about them.

1074
00:59:02,950 --> 00:59:04,410
And it's,
it's important.

1075
00:59:05,290 --> 00:59:08,470
So speaking of software that nobody 
understands how it works,

1076
00:59:08,740 --> 00:59:13,740
of course a bunch of people asked by 
your paper about your thoughts of why 

1077
00:59:13,740 --> 00:59:15,430
does deep and cheap learning work so 
well as the paper,

1078
00:59:15,431 --> 00:59:16,410
but um,
what,

1079
00:59:16,411 --> 00:59:17,980
what,
what are your thoughts on deep learning,

1080
00:59:18,280 --> 00:59:23,280
these kind of simplified models of our 
own brains and have been able to do some

1081
00:59:23,870 --> 00:59:28,760
successful perception work pattern 
recognition work and now with alpha zero

1082
00:59:28,761 --> 00:59:29,880
and so on,
do some,

1083
00:59:29,960 --> 00:59:33,110
some clever things.
What are your thoughts about the promise

1084
00:59:33,111 --> 00:59:35,210
limitations of this piece?

1085
00:59:35,650 --> 00:59:40,650
Right.
I think there are a number of very 

1086
00:59:41,031 --> 00:59:46,031
important insights,
very important lessons we can already 

1087
00:59:46,031 --> 00:59:49,340
draw from these kinds of successes.
One of them is when you look at the 

1088
00:59:49,340 --> 00:59:50,270
human brain and you see it's very 
complicated,

1089
00:59:50,300 --> 00:59:55,300
10th of 11 neurons and they're all 
different kinds of neurons and yada 

1090
00:59:55,300 --> 00:59:58,631
yada.
And there's been as long debate about 

1091
00:59:58,631 --> 01:00:00,700
whether the fact that we have dozens of 
different kinds is actually necessary 

1092
01:00:00,700 --> 01:00:02,020
for intelligence,
which are,

1093
01:00:02,021 --> 01:00:04,870
I think quite convincingly answer that 
question of no,

1094
01:00:05,740 --> 01:00:10,740
it's enough to have just one kind.
If you look under the hood of alpha 

1095
01:00:10,740 --> 01:00:11,290
zero,
there's only one kind of neuron and this

1096
01:00:11,291 --> 01:00:14,770
ridiculously simple,
that simple mathematical thing,

1097
01:00:14,970 --> 01:00:17,260
so it's not the.
It's just like in physics,

1098
01:00:17,261 --> 01:00:20,290
it's not the.
If you have a gas with waves in it,

1099
01:00:20,320 --> 01:00:22,560
it's not the detailed nature of the 
molecule,

1100
01:00:22,561 --> 01:00:25,440
doesn't matter.
It's the collective behavior.

1101
01:00:25,450 --> 01:00:27,980
Somehow.
Similarly it's,

1102
01:00:28,490 --> 01:00:33,490
it's,
it's this higher level structure of the 

1103
01:00:33,490 --> 01:00:34,040
network matters.
Not that you have 20 kinds of nuances.

1104
01:00:34,060 --> 01:00:39,060
I think my brain is such a complicated 
mess because it wasn't devolved just to 

1105
01:00:41,151 --> 01:00:46,151
be intelligent.
They wasn't involved to also be self 

1106
01:00:46,151 --> 01:00:48,090
assembling,
right and self repairing.

1107
01:00:48,091 --> 01:00:52,910
Right,
and the evolutionarily attainable and so

1108
01:00:52,911 --> 01:00:57,911
on.
So I think it's pretty my hunches that 

1109
01:00:57,911 --> 01:01:01,171
we're going to understand how to build 
agi before we fully understand how our 

1110
01:01:01,171 --> 01:01:04,761
brains work.
We we understood how to build flying 

1111
01:01:04,761 --> 01:01:07,530
machines long before we were able to 
build a mechanical work bird.

1112
01:01:07,770 --> 01:01:08,370
Yeah,
that's all right.

1113
01:01:08,400 --> 01:01:12,180
You're given.
You're given that the example exactly.

1114
01:01:12,420 --> 01:01:16,170
Mechanical birds and airplanes and 
airplanes do a pretty good job of flying

1115
01:01:16,171 --> 01:01:19,230
without really mimicking bird flight.
And even now,

1116
01:01:19,231 --> 01:01:20,880
after 100 years,
100 years later,

1117
01:01:20,890 --> 01:01:25,890
they just see the ted talk with this 
german mechanical heard you mentioned 

1118
01:01:25,890 --> 01:01:25,890
it.

1119
01:01:25,890 --> 01:01:27,390
IT's amazing.
But even after that,

1120
01:01:27,410 --> 01:01:32,410
right,
we still don't fly and mechanical bar 

1121
01:01:32,410 --> 01:01:34,340
because it turned out the way we came up
with simpler is better if our purpose 

1122
01:01:34,340 --> 01:01:35,100
is.
And I think it might be the same there.

1123
01:01:35,250 --> 01:01:39,150
So that's one lesson.
And another lesson,

1124
01:01:39,480 --> 01:01:42,480
which is more what the paper was about.
Well,

1125
01:01:42,640 --> 01:01:47,640
first I,
a physicist thought it was fascinating 

1126
01:01:47,640 --> 01:01:49,920
how there's a very close mathematical 
relationship actually between artificial

1127
01:01:49,921 --> 01:01:50,950
neural networks.
Um,

1128
01:01:50,951 --> 01:01:55,951
a lot of things that we've studied for 
and physics go by nerdy names like the 

1129
01:01:55,951 --> 01:01:59,190
renormalization group equation and um,
opinions and yada,

1130
01:01:59,191 --> 01:01:59,760
yada,
yada.

1131
01:01:59,761 --> 01:02:01,200
And,
and,

1132
01:02:01,201 --> 01:02:06,201
um,
when you look a liTtle more closely at 

1133
01:02:06,201 --> 01:02:07,060
this,
you have a,

1134
01:02:08,830 --> 01:02:11,020
you,
as far as I was like,

1135
01:02:11,070 --> 01:02:16,070
whoa,
there's something crazy here that 

1136
01:02:16,070 --> 01:02:17,481
doesn't make sense because we know that 
if you even want to a super simple 

1137
01:02:19,951 --> 01:02:23,430
neural network pel and tap pictures and 
dog pictures,

1138
01:02:23,431 --> 01:02:24,580
right?
That you can do that very well.

1139
01:02:24,600 --> 01:02:29,040
Very well now.
but if you think about it a little bit,

1140
01:02:29,041 --> 01:02:34,041
do you convince yourself that must be 
impossible because if I have one 

1141
01:02:34,041 --> 01:02:37,320
megapixel,
even if each pixel is just black or 

1142
01:02:37,320 --> 01:02:40,401
white,
there's to the power 1 million possible 

1143
01:02:40,401 --> 01:02:40,620
images is way more than there are atoms 
in the universe,

1144
01:02:40,621 --> 01:02:42,450
right?
So in order to.

1145
01:02:43,740 --> 01:02:46,380
And then for eaCh one of those I have to
assign a number,

1146
01:02:46,381 --> 01:02:48,210
which is the probability of that.
It's a dog,

1147
01:02:48,420 --> 01:02:50,910
right?
So an arbitrary function of images

1148
01:02:52,080 --> 01:02:56,610
is a list of more numbers than there are
atoms in our universe.

1149
01:02:56,730 --> 01:02:58,940
So clearly I can't store that under the 
hood of my,

1150
01:02:59,000 --> 01:03:02,430
my gpu or my computer,
yet somehow works.

1151
01:03:02,970 --> 01:03:04,200
So whaT does that mean?
Well,

1152
01:03:04,201 --> 01:03:09,201
it means that out of all of the problems
that you could try to solve with the 

1153
01:03:09,871 --> 01:03:14,871
neural network,
almost all of them are impossible to 

1154
01:03:14,871 --> 01:03:16,750
solve with a reasonably sized one.

1155
01:03:17,800 --> 01:03:20,050
But then what we showed in our paper 
was,

1156
01:03:20,620 --> 01:03:22,550
was that the,
the,

1157
01:03:23,040 --> 01:03:28,040
the kind of problems,
the fraction of all the problems that 

1158
01:03:28,040 --> 01:03:30,340
you could possibly pose that we actually
care about,

1159
01:03:30,370 --> 01:03:33,790
given the laws of physics,
is also an infant of testimony.

1160
01:03:33,791 --> 01:03:36,850
Tiny little part and amazingly they were
basically the same part.

1161
01:03:37,460 --> 01:03:42,460
Yeah.
It's almost like the world was created 

1162
01:03:42,460 --> 01:03:42,460
for.
I mean they kind of come together.

1163
01:03:42,460 --> 01:03:46,360
Yeah.
You could say maybe where the world 

1164
01:03:46,360 --> 01:03:46,360
created the world.
The world was created for us,

1165
01:03:46,360 --> 01:03:47,260
but I have a more modest than 
interpretation,

1166
01:03:47,261 --> 01:03:52,261
which is that instead evolution in that,
but neural networks are precisely for 

1167
01:03:52,261 --> 01:03:53,050
that reason,
right?

1168
01:03:53,051 --> 01:03:58,051
Because this particular architecture as 
opposed to the one in your laptop is 

1169
01:03:58,051 --> 01:04:01,530
very,
very well adapted to solving the kinds 

1170
01:04:01,891 --> 01:04:05,320
of problems that nature chapter 
presenting yet our ancestors with.

1171
01:04:05,321 --> 01:04:10,321
Right.
So it makes sense that why do we have a 

1172
01:04:10,321 --> 01:04:12,721
brain in the first place?
It's to be able to make predictions 

1173
01:04:12,721 --> 01:04:15,541
about the future and so on.
So if we had a sucky system which could 

1174
01:04:15,541 --> 01:04:17,730
never solve,
it wouldn't have been so.

1175
01:04:17,850 --> 01:04:18,720
But it's so this,
this,

1176
01:04:18,721 --> 01:04:19,900
this,
this is a.

1177
01:04:21,960 --> 01:04:24,330
I think you're very beautiful fact.
We also,

1178
01:04:24,331 --> 01:04:28,260
we also realize that there's a there 
that we have been.

1179
01:04:28,290 --> 01:04:29,640
It's been earlier work on,

1180
01:04:30,570 --> 01:04:32,080
yes,
deeper networks are good,

1181
01:04:32,081 --> 01:04:37,081
but we were able to show an additional 
cool factor which is that the even 

1182
01:04:37,081 --> 01:04:41,971
incredibly simple problems like support,
like give you a follow the numbers and 

1183
01:04:41,971 --> 01:04:46,291
asked you to multiply them together and 
already you can write a few lines of 

1184
01:04:46,291 --> 01:04:46,291
code,
boom,

1185
01:04:46,291 --> 01:04:48,460
done trivial.
If you just try to do that with a neural

1186
01:04:48,461 --> 01:04:51,520
network that has only one single hidden 
layer in it,

1187
01:04:52,430 --> 01:04:53,270
you can do it,

1188
01:04:54,350 --> 01:04:59,350
but you're going to need two to the 
power of thousand neurons and to 

1189
01:04:59,560 --> 01:05:04,560
multiply the numbers,
which is again more neurons than there 

1190
01:05:04,560 --> 01:05:05,300
are atoms in our universe.
That's not saying,

1191
01:05:05,510 --> 01:05:10,510
but if you're allowed,
if you allow yourself make it a deep 

1192
01:05:10,510 --> 01:05:12,450
networks with many layers,
you only need 4,000

1193
01:05:12,490 --> 01:05:14,360
euros.
It's perfectly feasible.

1194
01:05:15,180 --> 01:05:18,270
So that's an.
Yeah.

1195
01:05:18,470 --> 01:05:23,470
So on another architecture type,
I mean you mentioned schrodinger's 

1196
01:05:23,470 --> 01:05:27,221
equation and what are your thoughts 
about quantum computing and the role of 

1197
01:05:29,421 --> 01:05:34,421
this kind of computational unit and 
creating an intelligence system in some 

1198
01:05:35,241 --> 01:05:40,241
hollywood movies.
Not mentioned by name because I don't 

1199
01:05:40,241 --> 01:05:44,741
want to spoil them.
The way they get agi is building a 

1200
01:05:44,741 --> 01:05:46,670
quantum computer because of the word 
quantum.

1201
01:05:46,671 --> 01:05:47,610
Sounds cool.
And so on.

1202
01:05:48,830 --> 01:05:50,510
My,
first of all,

1203
01:05:50,511 --> 01:05:53,990
I think we don't need quantum computers.
They build agi.

1204
01:05:54,950 --> 01:05:59,950
I suspect your brain is not quantum 
computer and a new found sense.

1205
01:06:01,520 --> 01:06:04,280
So you don't even wrote a paper about 
that many years ago.

1206
01:06:04,560 --> 01:06:09,560
I checked the data,
the decoherence decoherence time that 

1207
01:06:09,560 --> 01:06:13,150
how long it takes until the quantum 
computer ness of what your new orleans 

1208
01:06:13,150 --> 01:06:16,780
doing gets erased by just random noise 
from the environment.

1209
01:06:17,990 --> 01:06:20,600
And if it's about 10 to the minus 21 
seconds.

1210
01:06:21,320 --> 01:06:25,070
So as cool as it would be that have a 
quantum computer in my head,

1211
01:06:25,071 --> 01:06:28,010
I don't think that fast.
On the other hand,

1212
01:06:28,370 --> 01:06:29,150
there are

1213
01:06:31,490 --> 01:06:34,070
very cool things you could do with 
quantum computers.

1214
01:06:35,270 --> 01:06:37,730
Alright.
I think we'll be able to do soon when we

1215
01:06:37,731 --> 01:06:42,731
get big ones,
bigger ones that might actually help 

1216
01:06:42,731 --> 01:06:42,731
machine learning do even better than the
brain.

1217
01:06:43,150 --> 01:06:45,530
So for example,

1218
01:06:47,090 --> 01:06:50,210
one,
this is a moonshot,

1219
01:06:50,780 --> 01:06:52,010
but um,
I'm,

1220
01:06:52,860 --> 01:06:57,860
you know,
learning is very much the same thing as 

1221
01:06:58,911 --> 01:07:01,760
a search.
If you have a,

1222
01:07:01,790 --> 01:07:03,140
if you're trying to train a neural 
network,

1223
01:07:03,170 --> 01:07:05,750
they get really learned to do something 
really well.

1224
01:07:06,260 --> 01:07:07,790
You have some loss function,
you have some,

1225
01:07:08,360 --> 01:07:13,360
you have a bunch of knobs you can turn 
represented by a bunch of numbers and 

1226
01:07:13,360 --> 01:07:17,201
you're trying to tweak them so that it 
becomes as good as possible as this 

1227
01:07:17,201 --> 01:07:19,781
thing,
so if you think have a landscape with 

1228
01:07:19,781 --> 01:07:23,240
some valley where each dimension of the 
landscape corresponds to some number you

1229
01:07:23,241 --> 01:07:28,241
can change.
You're trying to find the minimum and 

1230
01:07:28,241 --> 01:07:28,910
it's well known that if you have a very 
high dimensional landscape,

1231
01:07:29,060 --> 01:07:31,280
complicated things,
it's super hard to find the minimum.

1232
01:07:31,340 --> 01:07:35,580
Right?
Quantum mechanics is amazing.

1233
01:07:35,620 --> 01:07:36,570
Good at this,
right?

1234
01:07:36,750 --> 01:07:41,750
I get it.
If I want to know what's the lowest 

1235
01:07:41,750 --> 01:07:41,750
energy state,
this water can possibly have

1236
01:07:42,510 --> 01:07:44,520
incredibly hard to compute,
but we can put.

1237
01:07:44,590 --> 01:07:47,590
Nature will happily figure this out for 
you if you just cool it down,

1238
01:07:47,700 --> 01:07:48,850
make it very,
very cold.

1239
01:07:50,740 --> 01:07:53,290
If you put a ball somewhere,
it'll roll down to its minimum,

1240
01:07:53,291 --> 01:07:58,291
and this happens metaphorically at the 
energy landscape to and quantum 

1241
01:07:58,291 --> 01:08:00,280
mechanics.
Even using some clever tricks,

1242
01:08:00,310 --> 01:08:05,310
which today is machine learning systems 
don't like if you're trying to find the 

1243
01:08:05,310 --> 01:08:09,931
minimum when you get stuck in the little
local minimum here in quantum mechanics 

1244
01:08:09,931 --> 01:08:13,020
or connects the tunnel through the 
barrier and get unstuck again.

1245
01:08:13,770 --> 01:08:15,040
And um,
that's really.

1246
01:08:15,440 --> 01:08:15,610
Yeah.
So,

1247
01:08:15,611 --> 01:08:20,611
so maybe for example,
we will one day use quantum computers 

1248
01:08:20,611 --> 01:08:23,360
that help train neural networks better.

1249
01:08:23,790 --> 01:08:25,050
That's really interesting.
Okay.

1250
01:08:25,051 --> 01:08:28,080
So as a component of the learning 
process,

1251
01:08:28,081 --> 01:08:29,220
for example.
Yeah.

1252
01:08:29,790 --> 01:08:33,450
Let me ask sort of wrapping up here a 
little bit.

1253
01:08:33,451 --> 01:08:34,860
Let me,
let me return to,

1254
01:08:34,861 --> 01:08:37,320
uh,
the questions of our human nature and,

1255
01:08:37,860 --> 01:08:41,880
and love as I mentioned.
So do you think,

1256
01:08:44,640 --> 01:08:49,640
you mentioned sort of a helper robots,
but you can think of also personal 

1257
01:08:49,640 --> 01:08:52,611
robots.
Do you think the way we human beings 

1258
01:08:52,611 --> 01:08:54,570
fall in love and get connected to each 
other,

1259
01:08:55,050 --> 01:08:59,070
it's possible to achieve in an ai system
and human level,

1260
01:08:59,100 --> 01:09:00,720
ai intelligent system.
Do you think,

1261
01:09:00,721 --> 01:09:04,860
what would ever see that kind of 
connection or a,

1262
01:09:05,000 --> 01:09:10,000
you know,
in all this discussion about solving 

1263
01:09:10,000 --> 01:09:10,740
complex goals as this kind of human 
social connection,

1264
01:09:10,770 --> 01:09:15,770
do you think that's one of the goals and
the peaks and valleys that were the 

1265
01:09:15,770 --> 01:09:17,370
raising sea levels that we'll be able to
achieve?

1266
01:09:17,371 --> 01:09:20,070
or do you think that's something that's 
ultimately a,

1267
01:09:20,140 --> 01:09:25,140
or at least in the short term,
relevancy of the goals is not 

1268
01:09:25,140 --> 01:09:25,140
achievable?
I think it's all possible

1269
01:09:25,140 --> 01:09:25,650
and um,
in,

1270
01:09:25,800 --> 01:09:27,940
in,
in recent there's a,

1271
01:09:27,950 --> 01:09:30,690
there's a very wide range of justice as 
you know,

1272
01:09:30,820 --> 01:09:33,580
among ai researchers when we're going to
get agi.

1273
01:09:35,080 --> 01:09:36,370
Some people,
you know,

1274
01:09:36,490 --> 01:09:41,490
like our friend rodney brooks said it's 
going to be a hundred year lease and 

1275
01:09:41,490 --> 01:09:46,171
then there are many others.
I think it's going to happen relative 

1276
01:09:46,171 --> 01:09:48,420
much sooner and recent polls maybe half 
or so or I received your thinking we're 

1277
01:09:49,121 --> 01:09:53,050
going to get a gi within decades.
So if that happens,

1278
01:09:53,051 --> 01:09:55,750
of couRse,
I think these things are all possible,

1279
01:09:56,140 --> 01:10:01,140
but in terms of whether it will happen,
I think we shouldn't spend so much time 

1280
01:10:01,211 --> 01:10:06,211
asking what do we think will happen in 
the future as if we are just some sort 

1281
01:10:06,211 --> 01:10:07,540
of pathetic.
You're passive bystanders,

1282
01:10:07,541 --> 01:10:10,060
you know,
waiting for the future to happen to us.

1283
01:10:10,290 --> 01:10:12,430
Hey,
we're the ones creating this future,

1284
01:10:12,460 --> 01:10:17,460
right?
So we should be pRoactive about it and 

1285
01:10:17,460 --> 01:10:19,100
ask yourself what sort of future we 
would like to have happen.

1286
01:10:19,290 --> 01:10:22,790
I want to make it like that.
What would I prefer it to?

1287
01:10:22,800 --> 01:10:27,800
Some sort of incredibly boring zombie 
like future were just all these 

1288
01:10:27,800 --> 01:10:30,821
mechanical things happen and there's no 
passion or emotion or experience maybe 

1289
01:10:30,821 --> 01:10:34,231
even know I would much rather prefer if,
if all the things that we find that we 

1290
01:10:35,831 --> 01:10:40,831
value the most about humanity or 
subjective experience,

1291
01:10:42,280 --> 01:10:43,660
passion,
inspiration,

1292
01:10:43,720 --> 01:10:44,290
you love,
you know,

1293
01:10:44,470 --> 01:10:44,920
if,
if,

1294
01:10:45,010 --> 01:10:45,620
if,
uh,

1295
01:10:46,210 --> 01:10:50,830
we can create a future where those are,
those things do exist now.

1296
01:10:50,860 --> 01:10:55,510
I think ultimately it's not our universe
giving meaning to us,

1297
01:10:56,050 --> 01:11:01,050
us giving me the universe and if we 
Build more advanced and pillages let's,

1298
01:11:02,180 --> 01:11:07,180
let's make sure we're building in such a
way that meetings as part of it,

1299
01:11:09,150 --> 01:11:14,150
a lot of people that are seriously study
this problem and think of it from 

1300
01:11:14,150 --> 01:11:14,150
different angles have

1301
01:11:14,190 --> 01:11:19,190
troubling the majority of cases if they 
think through that happen are the ones 

1302
01:11:19,801 --> 01:11:22,380
that are not beneficial to humanity.
Right.

1303
01:11:22,520 --> 01:11:24,290
And so yeah.
So what,

1304
01:11:24,291 --> 01:11:25,590
what,
what are your thoughts?

1305
01:11:25,591 --> 01:11:26,910
What's an inch?
What's,

1306
01:11:27,090 --> 01:11:29,400
what should people,
you know,

1307
01:11:29,401 --> 01:11:33,200
I really don't like people to be 
terrified he should.

1308
01:11:33,201 --> 01:11:38,201
What?
What's a way for people to think about 

1309
01:11:38,201 --> 01:11:38,201
it in the way that it's set?
In a way we can solve it.

1310
01:11:38,201 --> 01:11:39,190
We couldn't make it,
but yeah.

1311
01:11:39,630 --> 01:11:42,600
No,
I don't think panicking is gonna help in

1312
01:11:42,601 --> 01:11:47,601
any way.
Not going to increase chances of things 

1313
01:11:47,601 --> 01:11:50,211
going well either.
Even if you are in a situation where 

1314
01:11:50,211 --> 01:11:52,281
there is a real threat,
does it help if everybody just freaks 

1315
01:11:52,281 --> 01:11:52,281
out?
Right?

1316
01:11:52,281 --> 01:11:52,281
No,
of course,

1317
01:11:52,281 --> 01:11:54,360
of course not.
I think,

1318
01:11:54,660 --> 01:11:57,120
yeah,
there are of course ways in which things

1319
01:11:57,121 --> 01:12:00,180
can go horribly wrong.
First of all,

1320
01:12:00,360 --> 01:12:02,950
it's important when we think about this 
thing,

1321
01:12:03,690 --> 01:12:08,690
about the problems and risks.
The also remember how huge the upsides 

1322
01:12:08,690 --> 01:12:08,690
can be if we get it right,
right?

1323
01:12:08,690 --> 01:12:11,770
Everything,
everything you love about society and to

1324
01:12:11,810 --> 01:12:16,810
the nation as a product of intelligence.
So if we can amplify our intelligence 

1325
01:12:16,810 --> 01:12:17,740
with machine intelligence and not any 
more,

1326
01:12:17,760 --> 01:12:21,900
lose our loved ones that were told in an
uncurable disease and things like this.

1327
01:12:21,960 --> 01:12:26,220
Of course we should aspire to that,
so that can be a motivator.

1328
01:12:26,221 --> 01:12:29,100
I think reminding ourselves that the 
reason we try to solve problems,

1329
01:12:29,101 --> 01:12:30,120
it's not just because

1330
01:12:31,760 --> 01:12:34,580
we're trying to avoid gluten,
but because we're trying to do something

1331
01:12:34,581 --> 01:12:37,460
great,
but then in in terms of the risks,

1332
01:12:37,670 --> 01:12:38,470
I think jim,

1333
01:12:41,200 --> 01:12:46,200
the entry of the important question is 
to ask what can we do today that will 

1334
01:12:46,200 --> 01:12:49,581
actually helped?
I'll come good and the dismissing the 

1335
01:12:49,581 --> 01:12:50,570
risk is not one of them.
You know?

1336
01:12:51,430 --> 01:12:56,430
I find it quite funny often when I'm in 
on the discussion panels about these 

1337
01:12:56,430 --> 01:13:00,330
things,
how the people who worked for hca,

1338
01:13:00,400 --> 01:13:01,750
for companies,
what we say,

1339
01:13:01,751 --> 01:13:02,680
I always like,
ah,

1340
01:13:02,681 --> 01:13:04,370
nothing to worry about,
nothing to worry about necessarily worry

1341
01:13:04,371 --> 01:13:06,250
about.
And it's always,

1342
01:13:06,251 --> 01:13:11,251
oh,
it's only academics sometimes express 

1343
01:13:11,251 --> 01:13:11,251
concerns.
That's not surprising at all.

1344
01:13:11,251 --> 01:13:12,170
If you think about it,
right?

1345
01:13:12,940 --> 01:13:15,280
Upton sinclair quipped,
right?

1346
01:13:15,281 --> 01:13:20,281
That the,
it's hard to make you believe in 

1347
01:13:20,281 --> 01:13:20,281
something when his income depends on not
believing in it.

1348
01:13:20,281 --> 01:13:25,220
And frankly we know a lot of these 
people in companies that they are just 

1349
01:13:25,220 --> 01:13:29,851
as concerned as anyone else.
But if you're the ceo of a company 

1350
01:13:29,851 --> 01:13:33,570
that's not something you want to go on 
record saying when you have silly 

1351
01:13:33,570 --> 01:13:36,061
journalists,
Oregon that put a picture of a 

1352
01:13:36,061 --> 01:13:36,170
terminator robot when they quote you.
So,

1353
01:13:36,640 --> 01:13:37,150
so the,
the,

1354
01:13:37,180 --> 01:13:39,920
the issues are real.
And the way I,

1355
01:13:39,970 --> 01:13:41,650
the way I think you might,
what the issue is,

1356
01:13:41,651 --> 01:13:43,280
it is basically,
you know,

1357
01:13:44,510 --> 01:13:49,510
the real choice we have is first of all,
are we going to just dismiss this,

1358
01:13:50,520 --> 01:13:51,660
the risks and say,
well,

1359
01:13:51,661 --> 01:13:56,661
you know,
let's just go ahead and build machines 

1360
01:13:56,661 --> 01:13:57,120
that can do everything we can do better 
and cheaper,

1361
01:13:57,121 --> 01:14:02,121
you know,
let's just make ourselves obsolete this 

1362
01:14:02,121 --> 01:14:02,121
fast as possible.
And what could possibly go wrong?

1363
01:14:02,121 --> 01:14:04,550
That's one attitude.
The opposite attitude.

1364
01:14:04,570 --> 01:14:05,430
I think it's the same,

1365
01:14:06,530 --> 01:14:08,380
here's this incredible potential,
you know,

1366
01:14:08,910 --> 01:14:10,790
let's,
let's think about what kind of.

1367
01:14:12,060 --> 01:14:15,580
We're really,
really excited about what are the shared

1368
01:14:15,581 --> 01:14:20,581
goals that we can really aspire towards.
And then let's think really hard about 

1369
01:14:20,581 --> 01:14:22,830
how we can actually get there as to 
start with not.

1370
01:14:22,840 --> 01:14:25,520
Don't start thinking about the risks.
Start thinking about the goals.

1371
01:14:25,540 --> 01:14:26,140
Goals.
Yeah.

1372
01:14:26,820 --> 01:14:31,820
And then when you do that,
then you can think about the obstacles 

1373
01:14:31,820 --> 01:14:34,750
you want to avoid.
Why they often get students coming in 

1374
01:14:34,750 --> 01:14:34,750
right here inTo my office for career 
advice.

1375
01:14:34,750 --> 01:14:36,940
I always asked them this question,
where do you want to be in the future?

1376
01:14:36,990 --> 01:14:39,060
Man?
If all she can say as,

1377
01:14:39,070 --> 01:14:40,600
oh,
maybe I'll have cancer,

1378
01:14:40,750 --> 01:14:45,750
maybe I'll run over by obstacles instead
of the goal and he's just going to end 

1379
01:14:45,750 --> 01:14:46,850
up a hypochondriac,
paranoid.

1380
01:14:48,010 --> 01:14:50,340
Whereas if she comes in and fire in her 
eyes and is like,

1381
01:14:50,350 --> 01:14:55,350
I want to be there and then we can talk 
about the obstacles and see how we can 

1382
01:14:55,350 --> 01:14:55,360
circumvent them.

1383
01:14:55,840 --> 01:14:57,310
That's,
I think a much,

1384
01:14:57,340 --> 01:15:00,770
much healthier attitude and um,
that's really well put.

1385
01:15:01,010 --> 01:15:02,050
And,
and uh,

1386
01:15:02,260 --> 01:15:07,260
I,
I feel it's very challenging to come up 

1387
01:15:07,260 --> 01:15:10,441
with a vision for the future which were,
which were unequivocal excited about it.

1388
01:15:10,720 --> 01:15:12,910
I'm not just talking now in the vague 
terms like,

1389
01:15:12,911 --> 01:15:13,910
yeah,
let's cure cancer.

1390
01:15:13,940 --> 01:15:18,940
Fine.
Talking about what kind of society that 

1391
01:15:18,940 --> 01:15:22,321
we want to create,
what do we want it to mean to be human 

1392
01:15:22,321 --> 01:15:24,130
in the age of ai,
when are in the age of agi.

1393
01:15:25,240 --> 01:15:28,570
So if we can have this conversation,
broad,

1394
01:15:28,571 --> 01:15:33,571
inclusive conversation and gradually 
start converging towards some,

1395
01:15:33,740 --> 01:15:38,740
some future that with some direction at 
least that we want to steer towards 

1396
01:15:38,740 --> 01:15:42,571
right then,
then now will be much more motivated to 

1397
01:15:42,571 --> 01:15:44,300
constructively take on the obstacles.
And I think if I,

1398
01:15:44,470 --> 01:15:45,440
uh,
if I had to,

1399
01:15:45,610 --> 01:15:50,610
if you make me,
if I read a wrap this up in a more 

1400
01:15:50,610 --> 01:15:50,610
succinct way,
I think,

1401
01:15:51,610 --> 01:15:54,140
I think we can all agree already now 
though,

1402
01:15:54,210 --> 01:15:59,210
we should aspire to build agi but 
doesn't overpower us,

1403
01:16:02,950 --> 01:16:04,270
but that empowers us

1404
01:16:05,390 --> 01:16:08,660
and think of the many various ways they 
can do that.

1405
01:16:08,690 --> 01:16:12,350
Whether that's from a mile side of the 
world of autonomous vehicles.

1406
01:16:12,930 --> 01:16:17,930
I personally actually from the camp that
believes this human level intelligence 

1407
01:16:17,930 --> 01:16:21,971
is required to,
to achieve something like vehicles that 

1408
01:16:21,971 --> 01:16:25,781
would actually be something we would 
enjoy using and being part of sets the 

1409
01:16:25,781 --> 01:16:30,630
one example and certainly there's a lot 
of other types of robots and medicine 

1410
01:16:30,630 --> 01:16:31,010
and so on.
Uh,

1411
01:16:31,011 --> 01:16:34,010
so focusing on those and then,
and then coming up with the obstacles,

1412
01:16:34,011 --> 01:16:38,060
coming up with the ways that that can go
wrong and solving those one at a time.

1413
01:16:38,300 --> 01:16:41,180
And just because you can build an 
autonomous vehicle,

1414
01:16:41,660 --> 01:16:43,810
even if you could build one that would 
drive this final,

1415
01:16:43,811 --> 01:16:46,970
as you know,
maybe there was some things in life that

1416
01:16:46,971 --> 01:16:49,260
we would actually want to do ourselves.
That's right.

1417
01:16:49,550 --> 01:16:53,150
Like for example,
if you think of our society as a whole,

1418
01:16:53,151 --> 01:16:57,690
there are some things that we find very 
meaningful to do and uh,

1419
01:16:57,691 --> 01:16:59,650
that doesn't mean you have to stop doing
it.

1420
01:16:59,651 --> 01:17:01,340
I'm just because machines can do them 
better,

1421
01:17:01,341 --> 01:17:06,341
you know,
I'm not Gonna stop playing tennis that 

1422
01:17:06,341 --> 01:17:06,350
day.
Someone build a tennis robot,

1423
01:17:06,650 --> 01:17:08,040
beat me.
People are still still

1424
01:17:08,040 --> 01:17:09,710
playing chess and even go

1425
01:17:10,160 --> 01:17:10,910
and,
and uh,

1426
01:17:11,330 --> 01:17:14,180
in this,
in the very near term,

1427
01:17:14,181 --> 01:17:18,680
even some people are advocating basic 
income replace jobs,

1428
01:17:18,950 --> 01:17:19,600
but if you,
if,

1429
01:17:19,670 --> 01:17:24,670
if the government is going to be willing
to just hand out cash to people for 

1430
01:17:24,670 --> 01:17:24,670
doing nothing,
uh,

1431
01:17:24,670 --> 01:17:26,190
ben,
once you also seriously consider whether

1432
01:17:26,191 --> 01:17:30,630
the government will also hire a lot more
teachers and nurses and the kind of jobs

1433
01:17:30,631 --> 01:17:33,840
which people often find great 
fulfillment in doing,

1434
01:17:33,841 --> 01:17:38,841
right?
I get very tired of hearing politicians 

1435
01:17:38,841 --> 01:17:38,841
saying,
oh,

1436
01:17:38,841 --> 01:17:41,541
we can't afford hiring more teachers,
but we're going to maybe have basic 

1437
01:17:41,541 --> 01:17:45,651
income if we can have more,
more serious research and thought into 

1438
01:17:45,651 --> 01:17:49,670
what gives meaning to our lives.
And the jobs give so much more than 

1439
01:17:49,670 --> 01:17:49,670
income.
Right?

1440
01:17:50,590 --> 01:17:54,610
And then thinking about in the future,
what are the role of the,

1441
01:17:56,290 --> 01:18:01,290
the roles that we want to have people 
feeling and powered by machines

1442
01:18:03,110 --> 01:18:05,010
and I think sort of,
um,

1443
01:18:05,060 --> 01:18:08,350
I come from the Russia,
from the soviet union and um,

1444
01:18:08,360 --> 01:18:11,180
I think for a lot of people in the 20th 
century going to the moon,

1445
01:18:11,330 --> 01:18:13,910
going into space was in an inspiring 
thing.

1446
01:18:14,180 --> 01:18:15,900
I feel like the,
the,

1447
01:18:16,160 --> 01:18:18,140
the,
the universe of the mind.

1448
01:18:18,141 --> 01:18:23,141
So ai understanding,
creating intelligence is that for the 

1449
01:18:23,141 --> 01:18:24,860
21st century.
So it's really surprising and I've heard

1450
01:18:24,861 --> 01:18:27,950
you mentioned this,
it's really surprising to me both on the

1451
01:18:27,951 --> 01:18:32,951
research funding side,
that it's not funded as greatly as it 

1452
01:18:32,951 --> 01:18:36,011
could be,
but most importantly on the politician 

1453
01:18:36,011 --> 01:18:38,951
side,
that it's not part of the public 

1454
01:18:38,951 --> 01:18:41,111
discourse except in the killer bots,
terminator kind of view that people are 

1455
01:18:41,361 --> 01:18:46,361
not yet,
I think perhaps excited by the possible 

1456
01:18:46,361 --> 01:18:48,170
positive future that we can build 
together.

1457
01:18:48,171 --> 01:18:48,350
So yeah,

1458
01:18:48,440 --> 01:18:53,440
we should be,
because politicians usually just focus 

1459
01:18:53,440 --> 01:18:53,440
on the next election cycle,
right?

1460
01:18:53,440 --> 01:18:58,090
Right.
The single most important thing I feel 

1461
01:18:58,090 --> 01:19:01,281
we humans have learned and the entire 
history of science is there were the 

1462
01:19:01,281 --> 01:19:05,201
masters of underestimation.
We underestimated the size of a,

1463
01:19:06,370 --> 01:19:11,370
our cosmos again and again,
realizing that everything we thought 

1464
01:19:11,370 --> 01:19:14,811
existed,
it was just a small part of something 

1465
01:19:14,811 --> 01:19:14,811
grander.
I find solar system,

1466
01:19:14,811 --> 01:19:15,040
the galaxy,
you know,

1467
01:19:15,750 --> 01:19:17,400
clusters of galaxies,
universe,

1468
01:19:18,450 --> 01:19:23,450
and we now know that the future has so 
much more potential than our ancestors 

1469
01:19:25,850 --> 01:19:30,850
could ever have dreamt of this cosmos.
But imagine if all of earth was 

1470
01:19:33,751 --> 01:19:37,590
completely devoid of life except for 
cambridge,

1471
01:19:37,591 --> 01:19:38,880
Massachusetts.
I would.

1472
01:19:39,570 --> 01:19:44,570
Wouldn't it be kind of lame if all we 
ever aspired to was to stay in 

1473
01:19:44,570 --> 01:19:48,381
cambridge,
Massachusetts forever and then go 

1474
01:19:48,381 --> 01:19:50,991
extinct in one week.
Even though earth was going to continue 

1475
01:19:50,991 --> 01:19:51,150
on for longer than that.
That sort of attitude.

1476
01:19:51,151 --> 01:19:56,151
I think we have now on the cosmic scale.
We can fill it.

1477
01:19:56,400 --> 01:19:59,310
Life can flourish on earth,
not for for four years,

1478
01:19:59,311 --> 01:20:04,311
but for billions of years.
I couldn't even tell you about how to 

1479
01:20:04,311 --> 01:20:06,861
move it out of harm's way when the sun 
gets too hot and and then we have so 

1480
01:20:07,261 --> 01:20:08,070
much more

1481
01:20:08,130 --> 01:20:09,900
resources out here,
which

1482
01:20:10,550 --> 01:20:11,240
today,
yeah,

1483
01:20:11,241 --> 01:20:14,960
maybe there are a lot of other planets 
with bacteria or childlike life on them,

1484
01:20:14,961 --> 01:20:19,961
but I most of this,
all this opportunity to seems as far as 

1485
01:20:20,191 --> 01:20:25,191
we can fail to be largely bed like the 
sahara desert and yet we have the 

1486
01:20:25,441 --> 01:20:30,441
opportunity but the help life flourish 
billions of and so like let's quit 

1487
01:20:31,531 --> 01:20:36,531
squabbling of a.
Some little borders should be drawn one,

1488
01:20:37,010 --> 01:20:38,400
one mile to the left or right.

1489
01:20:39,450 --> 01:20:41,070
Look up into the skies.
Did you realize,

1490
01:20:41,071 --> 01:20:41,710
hey,
you know,

1491
01:20:42,300 --> 01:20:47,300
we can do such incredible things and 
that's I think why it's really exciting 

1492
01:20:47,300 --> 01:20:51,651
that yeah,
you and others are connected with some 

1493
01:20:51,651 --> 01:20:55,041
of the working elon musk is doing 
because he's literally going out into 

1494
01:20:55,041 --> 01:20:58,911
that space,
really exploring our universe and it's 

1495
01:20:58,911 --> 01:21:01,491
wonderful.
That is exactly why elon musk is so 

1496
01:21:01,491 --> 01:21:05,060
misunderstood.
Misconstrued him as some kind of 

1497
01:21:05,060 --> 01:21:08,570
pessimistic doomsayer the region.
He chairs so much about ai safety is 

1498
01:21:08,570 --> 01:21:09,000
because he more than

1499
01:21:10,050 --> 01:21:13,290
almost anyone else appreciates these 
amazing opportunities.

1500
01:21:13,291 --> 01:21:18,291
It will squander it if we wipe out here 
on earth and we're not just going to 

1501
01:21:18,291 --> 01:21:22,461
wipe out the next generation,
but all generations and this incredible 

1502
01:21:22,740 --> 01:21:27,740
opportunity that's out there and that 
would be really be a waste and ai for 

1503
01:21:28,381 --> 01:21:31,470
people who think that they'll be better 
to do without technology.

1504
01:21:32,520 --> 01:21:33,570
Let me just mention that

1505
01:21:34,660 --> 01:21:39,660
if we don't improve our technology,
the question isn't whether humanity is 

1506
01:21:39,660 --> 01:21:42,810
going to go extinct,
questioning whether we're going to get 

1507
01:21:42,810 --> 01:21:45,690
taken out by the next big asteroid or 
the next super volcano or or something 

1508
01:21:45,690 --> 01:21:48,940
else dumb that we could easily prevent 
with more tech.

1509
01:21:48,941 --> 01:21:53,941
Right,
and if we want life to flourish 

1510
01:21:53,941 --> 01:21:54,610
throughout the cosmos,
ai is the key to it.

1511
01:21:56,140 --> 01:21:58,930
As I mentioned in a lot of detail in my 
book right there,

1512
01:21:59,860 --> 01:22:04,860
even many of the most inspired scifi 
writers I feel have totally 

1513
01:22:05,861 --> 01:22:09,850
underestimated the opportunities for 
space travel,

1514
01:22:09,851 --> 01:22:14,851
especially if the other galaxies,
because they weren't thinking about the 

1515
01:22:14,851 --> 01:22:16,720
possibility of agi,
which just makes it so much easier.

1516
01:22:17,480 --> 01:22:18,360
Right?
Yeah.

1517
01:22:18,390 --> 01:22:19,950
So that,
that goes to your,

1518
01:22:20,330 --> 01:22:21,180
uh,
uh,

1519
01:22:21,181 --> 01:22:25,620
view of agi that enables our progress,
said enables a better life.

1520
01:22:25,740 --> 01:22:30,740
So that's a beautiful,
that's a beautiful way to put it and 

1521
01:22:30,740 --> 01:22:30,740
it's something to strive for.
So,

1522
01:22:30,740 --> 01:22:31,440
max,
Thank you so much.

1523
01:22:31,441 --> 01:22:33,210
Thank you for your time today has been 
awesome.

1524
01:22:33,540 --> 01:22:36,020
Thank you so much.
Yes.

