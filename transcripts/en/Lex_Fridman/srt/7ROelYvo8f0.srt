1
00:00:00,180 --> 00:00:05,040
Today we have Josh Tenenbaum,
he's a professor here at mit leading the

2
00:00:05,070 --> 00:00:10,070
computational cognitive science group 
among many other topics and cognition 

3
00:00:10,070 --> 00:00:14,901
and intelligence.
He is fascinated with the question of 

4
00:00:14,901 --> 00:00:17,781
how human beings learn so much from so 
little and how these insights can lead 

5
00:00:18,511 --> 00:00:21,780
to build ai systems that are much more 
efficient learning from data.

6
00:00:22,410 --> 00:00:24,780
So please give Josh a warm welcome.

7
00:00:28,760 --> 00:00:31,370
Thank you.

8
00:00:31,640 --> 00:00:33,230
All right.
Thank you very much.

9
00:00:34,190 --> 00:00:39,190
Thanks for having me.
Excited to be part of what looks like 

10
00:00:39,190 --> 00:00:41,060
really quite a very impressive lineup,
especially starting after today and it's

11
00:00:41,120 --> 00:00:46,120
I think quite a great opportunity to get
to see perspectives on artificial 

12
00:00:46,120 --> 00:00:50,801
intelligence from many of the leaders in
industry and other entities working on 

13
00:00:50,961 --> 00:00:55,961
this great quest.
So I'm going to talk to you about some 

14
00:00:55,961 --> 00:01:00,011
of the work that we do in our group,
but also I'm going to try to give a 

15
00:01:00,011 --> 00:01:00,950
broader perspective,
reflective of a number of mit faculty,

16
00:01:01,100 --> 00:01:03,950
especially those who are affiliated with
the center for brains,

17
00:01:03,951 --> 00:01:08,951
minds and machines.
So you can see up there on my 

18
00:01:08,951 --> 00:01:10,751
affiliation academically,
I'm part of brain and cognitive science 

19
00:01:10,751 --> 00:01:11,990
or course nine.
I'm also part of resale,

20
00:01:12,200 --> 00:01:13,970
but I'm also part of the center for 
brains,

21
00:01:13,971 --> 00:01:18,971
minds and machines,
which is an NSF funded center science 

22
00:01:18,971 --> 00:01:22,271
and Technology Center,
which really stands for the bridge 

23
00:01:22,271 --> 00:01:22,271
between the science and the engineering 
of intelligence.

24
00:01:22,460 --> 00:01:25,910
It literally straddles Vassar Street and
that we have see sale and bcs members.

25
00:01:25,911 --> 00:01:29,840
We also have partners at Harvard and 
other academic institutions and again,

26
00:01:29,841 --> 00:01:34,841
what we stand for.
I want to try to convey some of the 

27
00:01:34,841 --> 00:01:37,091
specific things we're doing in the 
center and where we want to go with a 

28
00:01:37,091 --> 00:01:39,440
vision that really is about jointly 
pursuing the science,

29
00:01:39,470 --> 00:01:44,470
the basic science of how intelligence 
arises in the human mind and brain and 

30
00:01:44,470 --> 00:01:48,821
also the engineering enterprise of how 
to build something increasingly like 

31
00:01:48,821 --> 00:01:53,441
human intelligence in machines and we 
deeply believe that these two projects 

32
00:01:53,441 --> 00:01:55,370
have something to do with each other and
our best pursued jointly.

33
00:01:56,390 --> 00:02:01,390
Now it's a really exciting time to be 
doing anything related to intelligence 

34
00:02:01,390 --> 00:02:02,930
or certainly to ai for all the reasons 
that brought you all here.

35
00:02:02,931 --> 00:02:07,931
I don't have to tell you this.
We have all these ways in which ai is 

36
00:02:07,931 --> 00:02:07,931
kind of.

37
00:02:07,931 --> 00:02:11,830
Finally here we finally live in the era 
of something like real practical ai or 

38
00:02:12,200 --> 00:02:17,200
for those who've been around for awhile 
and have seen some of the rises and 

39
00:02:17,200 --> 00:02:17,200
falls,
you know,

40
00:02:17,200 --> 00:02:19,160
ai is back in a big way,
but from my perspective,

41
00:02:19,161 --> 00:02:24,161
and I think maybe this reflects why we 
distinguish what we might call Agi from 

42
00:02:24,161 --> 00:02:25,880
Ai,
we don't really have any real ai.

43
00:02:25,881 --> 00:02:28,880
Basically we have what I like to call ai
technologies,

44
00:02:29,180 --> 00:02:34,180
which are systems that do things we used
to think that only humans could do and 

45
00:02:34,180 --> 00:02:35,660
now we have machines that do them often 
quite well,

46
00:02:35,661 --> 00:02:38,630
maybe even better than any human who's 
ever lived.

47
00:02:38,680 --> 00:02:40,250
Right?
Like a machine that plays go,

48
00:02:41,030 --> 00:02:44,180
but none of these systems I would say 
are truly intelligent.

49
00:02:44,181 --> 00:02:45,920
None of them have anything like common 
sense.

50
00:02:46,110 --> 00:02:51,110
None of them have anything like the 
flexible general purpose intelligence 

51
00:02:51,110 --> 00:02:53,750
that each of you might use to learn 
every one of these skills or tasks.

52
00:02:53,880 --> 00:02:56,330
Right?
Each of these systems had to be built by

53
00:02:56,331 --> 00:03:00,220
large teams of engineers working 
together often for a number of years.

54
00:03:00,221 --> 00:03:05,221
I'd often at great cost to somebody 
who's willing to pay for it and each of 

55
00:03:05,221 --> 00:03:07,150
them just does one thing.
So alphago might beat the world's best,

56
00:03:07,870 --> 00:03:11,680
but it can't drive to the match or even 
tell you that.

57
00:03:11,681 --> 00:03:16,681
Go what go is,
I can't even tell you the go is a game 

58
00:03:16,681 --> 00:03:16,681
because it doesn't even know what a game
is.

59
00:03:16,681 --> 00:03:17,650
Right?
So what's missing?

60
00:03:18,130 --> 00:03:23,130
Why?
What is it that makes every one of your 

61
00:03:23,130 --> 00:03:23,130
brains,
maybe you can't beat,

62
00:03:23,130 --> 00:03:23,740
you know,
the world's best in go,

63
00:03:24,070 --> 00:03:26,830
but any one of you can get behind the 
wheel of a car.

64
00:03:26,831 --> 00:03:31,831
I think of this because my daughter is 
going to turn 16 tomorrow if she lived 

65
00:03:31,831 --> 00:03:33,040
in California,
she'd have a driver's license.

66
00:03:34,140 --> 00:03:36,880
It's a little bit down the line for us 
here in Massachusetts.

67
00:03:36,880 --> 00:03:38,190
But um,
you know,

68
00:03:38,200 --> 00:03:43,200
she didn't have to be specially 
engineered by a billion dollar startups 

69
00:03:43,200 --> 00:03:47,220
and you know,
she got really into chess recently and 

70
00:03:47,220 --> 00:03:49,351
now she's taught herself chest by 
playing just a handful of games 

71
00:03:49,351 --> 00:03:49,351
basically.
Um,

72
00:03:49,351 --> 00:03:51,460
and she can do any one of these 
activities at any one of us can.

73
00:03:51,490 --> 00:03:52,660
So what is it?
What's the,

74
00:03:52,661 --> 00:03:54,910
what makes up the difference?
Well,

75
00:03:54,911 --> 00:03:56,560
there's many things,
right?

76
00:03:56,860 --> 00:03:59,980
Um,
I'll talk about the focus for us and our

77
00:04:00,010 --> 00:04:01,810
research and a lot of us,
again,

78
00:04:01,811 --> 00:04:04,830
in CBMM is summarized here.
Um,

79
00:04:04,840 --> 00:04:09,840
what,
what drives the success is right now in 

80
00:04:09,840 --> 00:04:10,180
ai,
especially in industry.

81
00:04:10,480 --> 00:04:15,480
Okay.
And all of these ai technologies is 

82
00:04:15,480 --> 00:04:15,480
many,
many things,

83
00:04:15,480 --> 00:04:15,480
many things,
but what,

84
00:04:15,480 --> 00:04:19,460
what,
where the progress has been made most 

85
00:04:19,460 --> 00:04:19,930
recently and what's getting most of the 
attention is of course deep learning,

86
00:04:20,050 --> 00:04:23,470
but other kinds of machine learning 
technologies which essentially represent

87
00:04:23,471 --> 00:04:28,471
the maturation of a decades long effort 
to solve the problem of pattern 

88
00:04:28,471 --> 00:04:28,471
recognition.

89
00:04:28,471 --> 00:04:33,330
That means taking data and finding 
patterns in the data that tells you 

90
00:04:33,330 --> 00:04:37,381
something you care about,
like how to label a class or how to 

91
00:04:37,381 --> 00:04:37,390
predict some other signal.
Okay.

92
00:04:37,930 --> 00:04:39,760
Um,
and pattern recognition is great.

93
00:04:39,761 --> 00:04:44,761
It's an important part of intelligence 
and it's reasonable to say that deep 

94
00:04:44,761 --> 00:04:48,580
learning as a technology has really made
great strides on pattern recognition and

95
00:04:48,581 --> 00:04:49,660
maybe even,
you know,

96
00:04:49,980 --> 00:04:52,930
has coming close to solving the problems
with pattern recognition.

97
00:04:53,470 --> 00:04:55,450
But intelligence is about many other 
things.

98
00:04:55,451 --> 00:04:57,970
Intelligence about a lot more in 
particular.

99
00:04:57,971 --> 00:05:02,971
It's about modeling the world and think 
about all the activities that a human 

100
00:05:02,971 --> 00:05:05,200
does from all over the world that that 
go beyond just say,

101
00:05:05,230 --> 00:05:10,230
recognizing patterns in data,
but actually trying to explain and 

102
00:05:10,230 --> 00:05:10,330
understand what we see,
for instance,

103
00:05:10,610 --> 00:05:15,610
okay.
Or to be able to imagine things that 

104
00:05:15,610 --> 00:05:17,971
we've never seen that never seen,
maybe even very different from anything 

105
00:05:17,971 --> 00:05:19,160
we've ever seen,
but we want to see and then two,

106
00:05:19,270 --> 00:05:24,270
to set those as goals to make plans and 
solve problems needed to make those 

107
00:05:24,270 --> 00:05:26,650
things real or thinking about learning.

108
00:05:26,890 --> 00:05:29,110
Again,
some kinds of learning can be thought of

109
00:05:29,111 --> 00:05:34,111
as pattern recognition if you're 
learning sufficient statistics or 

110
00:05:34,111 --> 00:05:34,111
weights and a neural net that are used 
for those purposes,

111
00:05:34,390 --> 00:05:37,900
but many activities of learning are 
about building out new models,

112
00:05:38,020 --> 00:05:39,310
right?
Either refining,

113
00:05:39,311 --> 00:05:44,311
reusing and proving old bottles or 
actually building fundamentally new 

114
00:05:44,311 --> 00:05:44,500
models as you've experienced more of the
world.

115
00:05:44,710 --> 00:05:47,830
And then think about sharing our models,
communicating our models to others,

116
00:05:48,130 --> 00:05:50,020
modeling their models,
learning from them.

117
00:05:50,290 --> 00:05:55,290
All of these activities of modeling.
These are at the heart of human 

118
00:05:55,290 --> 00:05:58,190
intelligence and it requires a much 
broader set of tools.

119
00:05:58,191 --> 00:06:03,191
So I want to talk about the ways that 
we're studying these activities of 

120
00:06:03,191 --> 00:06:05,621
modeling the world and something in a 
pretty nontechnical way about what are 

121
00:06:05,621 --> 00:06:07,670
the kinds of tools that allow us to 
capture these abilities.

122
00:06:07,880 --> 00:06:12,880
Now I think it's.
I want to be very honest upfront and to 

123
00:06:12,880 --> 00:06:12,880
say this is just the beginning of a 
story,

124
00:06:12,880 --> 00:06:12,920
right?

125
00:06:13,310 --> 00:06:14,990
When you look at deep learning 
successes,

126
00:06:15,260 --> 00:06:17,360
that itself is a story that goes back 
decades.

127
00:06:17,361 --> 00:06:19,130
I'll say a little bit about that history
in a minute,

128
00:06:19,550 --> 00:06:24,550
but where we are now is just looking 
forward to a future when we might be 

129
00:06:24,550 --> 00:06:28,151
able to capture these abilities at a 
really mature engineering scale and I 

130
00:06:28,151 --> 00:06:31,580
would say we are far from being able to 
capture the all the ways in which humans

131
00:06:31,790 --> 00:06:34,880
richly flexibly,
quickly build models of the world at the

132
00:06:34,881 --> 00:06:39,881
kind of scale that say silicon valley 
wants either big tech companies like 

133
00:06:39,881 --> 00:06:42,170
Google or Microsoft or IBM or facebook 
or small startups.

134
00:06:42,200 --> 00:06:46,280
Right?
We can get there and I think what I want

135
00:06:46,281 --> 00:06:51,281
to talk to you about here is one route 
for trying to get there and this is the 

136
00:06:51,281 --> 00:06:53,780
route that CBMM stands for,
the idea that by reverse engineering how

137
00:06:53,781 --> 00:06:58,781
intelligence works in the human mind and
brain that will give us a route to 

138
00:06:58,781 --> 00:06:59,360
engineering these abilities in machines.

139
00:06:59,510 --> 00:07:02,000
When we say reverse engineering,
we're talking about science,

140
00:07:02,001 --> 00:07:07,001
but doing science like engineers.
This is our fundamental principle that 

141
00:07:07,001 --> 00:07:08,730
if we approach cognitive science and 
neuroscience like an engineer,

142
00:07:08,731 --> 00:07:13,731
where so the output of our science isn't
just a description of the brain or the 

143
00:07:13,731 --> 00:07:16,751
mind in words,
but in the same terms that an engineer 

144
00:07:16,751 --> 00:07:16,751
would use to build an intelligent 
system,

145
00:07:16,790 --> 00:07:19,730
then that will be both the basis for a 
much more rigorous and deep,

146
00:07:19,820 --> 00:07:24,820
the insightful science,
but also direct translation of those 

147
00:07:24,820 --> 00:07:27,821
insights into engineering applications.
Now I said before I talk a little about 

148
00:07:28,221 --> 00:07:29,210
history,
um,

149
00:07:29,211 --> 00:07:30,860
what I mean by that is this,
again,

150
00:07:30,861 --> 00:07:32,900
if,
if part of what brought you here as deep

151
00:07:32,901 --> 00:07:34,520
learning,
and I know even if you've never heard of

152
00:07:34,521 --> 00:07:37,970
deep learning before,
which I'm sure is unlikely you saw some,

153
00:07:38,050 --> 00:07:39,470
you know,
a good spectrum of that in the,

154
00:07:39,510 --> 00:07:41,460
in the overview session,
uh,

155
00:07:41,660 --> 00:07:42,200
last night.

156
00:07:42,350 --> 00:07:43,300
Okay.
Um,

157
00:07:43,640 --> 00:07:48,640
it's really interesting and important to
look back on the history of where did 

158
00:07:48,640 --> 00:07:50,090
techniques for deep learning come from 
or reinforcement learning.

159
00:07:50,091 --> 00:07:55,091
Those are the two tools in the,
in the current machine learning arsenal 

160
00:07:55,091 --> 00:07:58,181
that are getting the most attention,
things like backpropagation or end to 

161
00:07:58,181 --> 00:08:01,241
end,
stochastic gradient descent or temporal 

162
00:08:01,241 --> 00:08:01,910
difference learning or cue learning.
Here's a few papers from the literature.

163
00:08:01,911 --> 00:08:02,230
May,
you know,

164
00:08:02,240 --> 00:08:04,040
maybe some of you have read these 
original papers.

165
00:08:04,041 --> 00:08:09,041
Here's,
here's the original paper by Romel 

166
00:08:09,041 --> 00:08:11,201
heart,
Hinton and colleagues in which they 

167
00:08:11,201 --> 00:08:11,201
introduced the backpropagation algorithm
for training multilayer,

168
00:08:11,201 --> 00:08:11,690
perceptrons,
right?

169
00:08:11,691 --> 00:08:16,691
Multilayer neural networks.
Here's the original perceptron paper by 

170
00:08:16,691 --> 00:08:19,571
Rosenblatt,
which introduced the one layer version 

171
00:08:19,571 --> 00:08:19,880
of that architecture and the basic 
perceptron learning algorithm.

172
00:08:20,360 --> 00:08:25,360
Here's the first paper on sort of the 
temporal difference learning method for 

173
00:08:25,360 --> 00:08:26,420
reinforcement learning from Sutton and 
bartow.

174
00:08:27,020 --> 00:08:30,380
Here's the original bolt machine paper 
also by Hinton and colleagues,

175
00:08:31,170 --> 00:08:36,170
which you know again is a,
those who don't know that architecture 

176
00:08:36,170 --> 00:08:38,810
can give a kind of probabilistic 
undirected multilayer perceptron,

177
00:08:39,220 --> 00:08:40,530
um,
or for example,

178
00:08:41,090 --> 00:08:46,090
before the Lstm is if you know about 
current recurrent neural network 

179
00:08:46,090 --> 00:08:49,001
architecture earlier,
as much simpler versions of the same 

180
00:08:49,001 --> 00:08:49,460
idea were proposed by Jeff Ellman and 
his simple recurrent networks.

181
00:08:49,820 --> 00:08:54,820
The reason I want to put up the original
papers here is for you to look at both 

182
00:08:54,820 --> 00:08:56,310
when they were published,
where they were published.

183
00:08:56,400 --> 00:09:01,400
So if you look at the dates,
you'll see papers going back to the 

184
00:09:01,400 --> 00:09:05,820
eighties,
but even the sixties or even the 19 

185
00:09:05,820 --> 00:09:05,820
fifties,
and look at where they were published.

186
00:09:05,820 --> 00:09:07,920
Most of them were published in 
psychology journals.

187
00:09:07,921 --> 00:09:10,200
So the journal psychological review,
if you don't know it,

188
00:09:10,201 --> 00:09:15,201
is like the leading journal of 
theoretical psychology and mathematical 

189
00:09:15,201 --> 00:09:18,111
psychology or cognitive science.
The Journal of the cognitive science 

190
00:09:18,111 --> 00:09:20,160
society or the back prop paper was 
published in nature,

191
00:09:20,190 --> 00:09:21,990
which is a general interest science 
journal.

192
00:09:21,991 --> 00:09:25,170
But by people who are mostly affiliated 
with the Institute for Cognitive Science

193
00:09:25,200 --> 00:09:30,200
in San Diego,
so what you see here is already a long 

194
00:09:30,200 --> 00:09:30,690
history of scientists thinking like 
engineers,

195
00:09:30,840 --> 00:09:35,840
these are people who are in psychology 
or cognitive science departments and 

196
00:09:35,840 --> 00:09:39,351
publishing in those places,
but by formalizing even very basic 

197
00:09:39,351 --> 00:09:40,830
insights about how humans might learn or
how,

198
00:09:40,850 --> 00:09:45,850
you know,
brains might learn in the right kind of 

199
00:09:45,850 --> 00:09:45,850
math that led to,
of course,

200
00:09:45,850 --> 00:09:50,121
progress on the science side,
but it led to all the engineering that 

201
00:09:50,121 --> 00:09:50,121
we see now.

202
00:09:50,121 --> 00:09:50,121
It wasn't sufficient,
right?

203
00:09:50,121 --> 00:09:51,300
We needed,
we needed,

204
00:09:51,390 --> 00:09:56,390
of course,
lots of innovations and advances in 

205
00:09:56,390 --> 00:09:56,700
computing hardware and software systems,
right?

206
00:09:56,820 --> 00:10:01,820
But this is where the basic,
the basic math came from and it came 

207
00:10:01,820 --> 00:10:05,331
from doing science like an engineer.
So what I want to talk about and our 

208
00:10:05,331 --> 00:10:06,120
vision is what is the future of this 
look like?

209
00:10:06,121 --> 00:10:07,950
If we were to look 50 years into the 
future,

210
00:10:08,190 --> 00:10:10,580
what would we be looking back on now?
Or you know,

211
00:10:10,620 --> 00:10:12,090
over this timescale.
Well,

212
00:10:12,091 --> 00:10:17,091
here's a,
here's a longterm research roadmap that 

213
00:10:17,091 --> 00:10:19,461
reflects some of my ambitions and some 
of our centers goals and many others to 

214
00:10:19,461 --> 00:10:23,630
right.
We'd like to be able to address basic 

215
00:10:23,630 --> 00:10:25,611
questions,
fundamental questions of what it is to 

216
00:10:25,611 --> 00:10:25,680
be and to think like a human questions 
for example,

217
00:10:25,681 --> 00:10:28,620
of consciousness or meaning and language
or real learning,

218
00:10:28,890 --> 00:10:30,270
right?
Questions like,

219
00:10:30,271 --> 00:10:31,230
um,
you know,

220
00:10:31,290 --> 00:10:35,070
even beyond the individual at questions 
of culture or creativity.

221
00:10:35,130 --> 00:10:40,130
Those are our big ideas up there.
And for each of these there are basic 

222
00:10:40,130 --> 00:10:40,130
scientific questions,
right?

223
00:10:40,130 --> 00:10:42,300
How do we become aware of the world and 
ourselves in.

224
00:10:42,301 --> 00:10:45,330
It starts with perception,
but it really turns into awareness,

225
00:10:45,420 --> 00:10:48,390
awareness of yourself and of the world 
and what we might call consciousness,

226
00:10:48,450 --> 00:10:53,450
right?
Or how does a word starts to have a 

227
00:10:53,450 --> 00:10:56,181
meaning?
What really is a meaning and how does a 

228
00:10:56,181 --> 00:10:57,801
child grasp it or how to children 
actually learn what to babies brains 

229
00:10:57,801 --> 00:11:01,641
actually start with are they blank 
slates or do they start with some kind 

230
00:11:01,641 --> 00:11:04,401
of cognitive structure?
And then what is real learning look 

231
00:11:04,401 --> 00:11:06,831
like?
These are just some of the questions 

232
00:11:06,831 --> 00:11:07,500
that we're interested in working on or 
when we talk about culture remain,

233
00:11:07,800 --> 00:11:10,740
how do you learn all the things you 
didn't directly experience,

234
00:11:10,980 --> 00:11:15,980
right?
But that somehow you got from the 

235
00:11:15,980 --> 00:11:16,530
accumulation of knowledge in society 
over many generations or how do you ever

236
00:11:16,590 --> 00:11:19,020
think of new ideas or answers to new 
questions?

237
00:11:19,020 --> 00:11:20,490
How do you think of the new questions 
themselves?

238
00:11:20,491 --> 00:11:25,491
How do you decide what to think about?
These are all key activities of human 

239
00:11:25,491 --> 00:11:29,031
intelligence.
When we talk about how we model the 

240
00:11:29,031 --> 00:11:29,031
world,
where our models come from,

241
00:11:29,031 --> 00:11:32,601
what we do with our models.
This is what we're talking about and if 

242
00:11:32,601 --> 00:11:35,031
we could get machines that could do 
these things well again on the bottom 

243
00:11:35,031 --> 00:11:35,190
row,
think of all the actual real engineering

244
00:11:35,191 --> 00:11:40,191
payoffs now in our center and in both my
own activities and a lot of what my 

245
00:11:40,191 --> 00:11:45,081
group does these days and what a number 
of other colleagues in the center for 

246
00:11:45,081 --> 00:11:45,470
brains minds and machines do as well as 
an abrupt,

247
00:11:45,480 --> 00:11:47,650
very broadly.
People in pcs and see sale.

248
00:11:48,000 --> 00:11:50,940
One place where we work on the 
beginnings of these problems in the near

249
00:11:50,941 --> 00:11:51,990
term,
this is the longterm,

250
00:11:51,991 --> 00:11:53,560
like think 50 years,
maybe,

251
00:11:53,580 --> 00:11:54,820
maybe shorter,
maybe longer,

252
00:11:54,821 --> 00:11:56,440
I don't know,
but think well beyond,

253
00:11:57,160 --> 00:12:00,430
well beyond 10 years,
but in the short term,

254
00:12:00,431 --> 00:12:05,431
five to 10 years,
a lot of our focus is around visual 

255
00:12:05,431 --> 00:12:05,431
intelligence and there's many reasons 
for that.

256
00:12:05,431 --> 00:12:09,720
Again,
we can build on the successes of deep 

257
00:12:09,720 --> 00:12:09,720
networks and a lot of pattern 
recognition and machine vision.

258
00:12:09,850 --> 00:12:11,860
It's a good way to put these ideas into 
practice.

259
00:12:11,861 --> 00:12:13,630
When we,
when we look at the actual brain,

260
00:12:14,020 --> 00:12:18,130
the visual system in the brain,
in the human and other mammalian brains,

261
00:12:18,131 --> 00:12:23,131
for example,
is really very clearly the best 

262
00:12:23,131 --> 00:12:23,131
understood part of the brain and at a 
circuit level,

263
00:12:23,131 --> 00:12:25,180
it's the part of the brain that's most 
inspired current,

264
00:12:25,840 --> 00:12:27,520
deep learning and neural network 
systems.

265
00:12:27,790 --> 00:12:32,790
But even there,
there's things which we still don't 

266
00:12:32,790 --> 00:12:35,131
really understand like engineers.
So here's an example of a basic problem 

267
00:12:35,131 --> 00:12:39,121
in visual intelligence that we and 
others in the center are trying to 

268
00:12:39,121 --> 00:12:43,111
solve.
Look around you and you feel like 

269
00:12:43,111 --> 00:12:45,180
there's a whole world around you and 
there is a whole world around you,

270
00:12:45,181 --> 00:12:46,840
you know,
feel like your brain captures it,

271
00:12:47,380 --> 00:12:50,650
but what the actual sensor data that's 
coming in through your eyes,

272
00:12:50,910 --> 00:12:53,770
it looks more like this photograph here 
where you can see there's a crowd scene,

273
00:12:53,771 --> 00:12:58,771
but it's mostly blurry except for a 
small region of high resolution in the 

274
00:12:58,771 --> 00:12:59,770
center,
so that corresponds biologically to what

275
00:12:59,771 --> 00:13:01,330
part of the images in your phobia.

276
00:13:01,330 --> 00:13:06,330
That's the central region of cells in 
the retina where you have really high 

277
00:13:06,330 --> 00:13:09,721
resolution visual data.
The size of your phobia is roughly like 

278
00:13:09,721 --> 00:13:10,420
if you hold out your thumb at arm's 
length,

279
00:13:10,450 --> 00:13:12,340
it's a little bit bigger than that,
but not much bigger.

280
00:13:12,550 --> 00:13:17,550
Right?
Most of the image it in terms of the 

281
00:13:17,550 --> 00:13:20,461
actual information coming in and a 
bottom up sense to your brain is really 

282
00:13:20,461 --> 00:13:23,851
quite blurry,
but somehow by looking at just one part 

283
00:13:23,851 --> 00:13:25,210
and then by being around or making a few
eye movements,

284
00:13:25,300 --> 00:13:30,300
you get a few glimpses each.
Not much bigger than the size of your 

285
00:13:30,300 --> 00:13:33,151
thumb at arm's length.
Somehow you stitch that information 

286
00:13:33,151 --> 00:13:35,851
together into what feels like and really
is a rich representation of the whole 

287
00:13:35,851 --> 00:13:37,060
world around you.
And when I say around you,

288
00:13:37,061 --> 00:13:40,120
I mean literally around you.
So here's another kind of demonstration.

289
00:13:41,710 --> 00:13:43,930
Without turning around,
nobody's allowed to turn around.

290
00:13:44,560 --> 00:13:49,560
Ask yourself what's behind you.
Now the answer is going to be different 

291
00:13:49,560 --> 00:13:49,750
for different people.
Depending on where you're sitting,

292
00:13:49,780 --> 00:13:51,040
right?
For most of you,

293
00:13:51,190 --> 00:13:52,210
you might think,
well,

294
00:13:52,211 --> 00:13:57,211
there's,
I think there's a person pretty close 

295
00:13:57,211 --> 00:13:57,211
behind me,
right?

296
00:13:57,211 --> 00:13:57,760
You know you're in a crowded auditorium,
although you haven't seen that person,

297
00:13:57,761 --> 00:13:59,380
you know that they're there,
right?

298
00:14:00,910 --> 00:14:03,460
For people in the very back row,
you know there isn't a person behind you

299
00:14:03,461 --> 00:14:05,050
and you're conscious of being in the 
back row,

300
00:14:05,140 --> 00:14:07,030
right?
You might be conscious if there's a wall

301
00:14:07,031 --> 00:14:12,031
right behind you,
but now for the people who are in the 

302
00:14:12,031 --> 00:14:12,031
room,
not in the very back,

303
00:14:12,031 --> 00:14:13,630
think about how far behind you is the 
back.

304
00:14:13,631 --> 00:14:18,631
Like,
where's the nearest wall behind you so 

305
00:14:18,631 --> 00:14:20,251
we can look.
Maybe we can call out a try a little 

306
00:14:20,251 --> 00:14:20,251
demonstration.
So I didn't know.

307
00:14:20,251 --> 00:14:21,190
I'm pointing to someone there.
Can you see phrase,

308
00:14:21,340 --> 00:14:22,960
say something if you think I'm pointing 
at you?

309
00:14:24,290 --> 00:14:29,290
Uh,
well I could have been pointing at you 

310
00:14:29,290 --> 00:14:29,290
but I'm pointing someone behind you.
Okay.

311
00:14:29,290 --> 00:14:29,290
I'll point to you.
Yeah,

312
00:14:29,290 --> 00:14:29,320
I'm pointing to you.
Alright.

313
00:14:29,440 --> 00:14:31,170
So how far is the nearest wall?
But no,

314
00:14:31,220 --> 00:14:34,750
you can't turn around.
You've blown your chance without turning

315
00:14:34,751 --> 00:14:35,170
around.
Okay.

316
00:14:35,171 --> 00:14:36,410
So you,
you were loud.

317
00:14:36,430 --> 00:14:37,120
Okay.
Do you see,

318
00:14:37,121 --> 00:14:38,800
I'm pointing to you there with the tie.
Okay.

319
00:14:38,950 --> 00:14:42,100
So without turning around,
how far is the nearest wall behind you?

320
00:14:46,360 --> 00:14:49,450
That's how far.
Five meters.

321
00:14:49,451 --> 00:14:50,190
Okay.
Well let me,

322
00:14:50,191 --> 00:14:55,191
that might be about right.
No other people can turn around a how 

323
00:14:55,191 --> 00:14:55,640
now,
how about you?

324
00:14:55,641 --> 00:15:00,170
How far is the nearest wall behind you?
Ten meters.

325
00:15:00,350 --> 00:15:01,560
Okay.
Um,

326
00:15:01,580 --> 00:15:02,750
that might be right?
Yeah.

327
00:15:03,200 --> 00:15:04,920
How about here?
How,

328
00:15:05,040 --> 00:15:05,810
how,
what do you think?

329
00:15:06,560 --> 00:15:07,280
20.
Okay.

330
00:15:07,510 --> 00:15:08,330
Um,
so yeah,

331
00:15:08,331 --> 00:15:09,800
since I didn't grow up in the metric 
system,

332
00:15:09,801 --> 00:15:10,850
I barely know,
but yeah,

333
00:15:10,851 --> 00:15:13,790
I mean,
I mean the point is that like your,

334
00:15:14,070 --> 00:15:15,100
your,
your,

335
00:15:15,230 --> 00:15:20,230
each of you is surely not exactly right,
but you're certainly within an order of 

336
00:15:20,230 --> 00:15:24,311
magnitude.
And I guess if we actually tried to 

337
00:15:24,311 --> 00:15:24,311
measure,
you know,

338
00:15:24,311 --> 00:15:27,041
you're probably,
my guess is you're probably right 

339
00:15:27,041 --> 00:15:27,041
within,
you know,

340
00:15:27,041 --> 00:15:27,950
50 percent or less often,
maybe just 20 percent error.

341
00:15:28,550 --> 00:15:29,660
Okay.
So how do you know this?

342
00:15:29,780 --> 00:15:30,830
I mean,
even if it's not,

343
00:15:30,980 --> 00:15:31,940
what did you say?
Twenty meters.

344
00:15:31,941 --> 00:15:36,941
Even if it's not 20 meters,
it's probably closer to 20 meters than 

345
00:15:36,941 --> 00:15:38,120
it is to five or 10 meters.
And then it is to 50 meters.

346
00:15:38,210 --> 00:15:40,400
So how did you know this?
You haven't turned around in a while,

347
00:15:40,430 --> 00:15:45,430
right?
But some part of your brain is tracking 

348
00:15:45,430 --> 00:15:45,430
the whole world around you,
right?

349
00:15:45,480 --> 00:15:47,000
Um,
and how many people are behind you?

350
00:15:49,100 --> 00:15:50,090
Yeah,
like a few hundred,

351
00:15:50,091 --> 00:15:50,360
right?
I mean,

352
00:15:50,361 --> 00:15:53,510
I don't know if it's 200 or $300,
but it's not a thousand.

353
00:15:54,200 --> 00:15:55,450
I don't think so.
Um,

354
00:15:55,490 --> 00:15:58,460
and it's certainly not 10 or 20 or 50.
Right?

355
00:15:58,550 --> 00:16:01,970
So you track these things and you use 
them to plan your actions.

356
00:16:02,630 --> 00:16:03,320
Okay.
So again,

357
00:16:03,770 --> 00:16:06,980
think about how instantly,
effortlessly and very reliably,

358
00:16:07,160 --> 00:16:09,170
okay,
your brain computes all these things.

359
00:16:09,171 --> 00:16:12,350
So the people and objects around you.
And it's not just,

360
00:16:12,570 --> 00:16:17,570
you know,
approximations certainly when we're 

361
00:16:17,570 --> 00:16:17,570
talking about what's,
what's behind you in space,

362
00:16:17,570 --> 00:16:19,640
there's a lot of imprecision,
but when it comes to reaching for things

363
00:16:19,641 --> 00:16:22,880
right in front of you,
very precise shape and physical property

364
00:16:22,881 --> 00:16:25,160
estimates needed to pick up and 
manipulate objects.

365
00:16:25,340 --> 00:16:30,340
And then when it comes to people,
it's not just the existence of the 

366
00:16:30,340 --> 00:16:30,340
people but something about what's in 
their head,

367
00:16:30,340 --> 00:16:35,020
right?
You track whether someone's paying 

368
00:16:35,020 --> 00:16:35,020
attention to you and you're talking to 
them,

369
00:16:35,020 --> 00:16:35,600
what they might want from you,
what they might be thinking about you,

370
00:16:35,690 --> 00:16:37,550
what they might be thinking about other 
people.

371
00:16:37,730 --> 00:16:42,730
Okay?
So when we talk about visual 

372
00:16:42,730 --> 00:16:44,561
intelligence,
this is the whole stuff we're talking 

373
00:16:44,561 --> 00:16:45,820
about and you can start to see how it 
turns into basic questions I think of,

374
00:16:46,020 --> 00:16:51,020
of,
of what we might call the beginnings of 

375
00:16:51,020 --> 00:16:53,381
consciousness or at least our awareness 
of ourself in the world and of ourselves

376
00:16:53,721 --> 00:16:58,721
as a self in the world,
but also other aspects of higher level 

377
00:16:58,721 --> 00:17:00,290
intelligence and cognition that are not 
just about perception like symbols,

378
00:17:00,440 --> 00:17:02,690
right?
To describe even to ourselves,

379
00:17:03,050 --> 00:17:05,810
what's around us and where we are.
And what we can do with it,

380
00:17:05,960 --> 00:17:10,960
you have to go beyond just what we would
normally call the stuff that perception 

381
00:17:10,960 --> 00:17:13,280
to to say the thoughts in somebody's 
head in your own thoughts about that.

382
00:17:13,550 --> 00:17:18,550
Okay.
So what we've been doing in CBMM is 

383
00:17:18,550 --> 00:17:20,440
trying to develop an architecture for 
visual intelligence and I'm not going to

384
00:17:20,441 --> 00:17:23,180
go into any of the details of how this 
works and this is just notional.

385
00:17:23,180 --> 00:17:24,740
This is just a picture.
It's like a just a,

386
00:17:24,960 --> 00:17:27,740
a sketch from a grant proposal of what 
we say we want to do,

387
00:17:27,920 --> 00:17:32,060
but it's based on a lot of scientific 
understanding of how the brain works.

388
00:17:32,061 --> 00:17:37,061
There are different parts of the brain 
that correspond to these different 

389
00:17:37,061 --> 00:17:38,570
modules in our architecture as well as 
some kind of emerging engineering way to

390
00:17:38,571 --> 00:17:41,890
try to capture at the software and maybe
even hardware levels,

391
00:17:41,900 --> 00:17:46,900
how these modules might work.
So we talk about a sort of an early 

392
00:17:46,900 --> 00:17:49,991
module of a visual or perceptual stream,
which is like bottom up visual or other 

393
00:17:49,991 --> 00:17:54,231
perceptual input.
That's the kind of thing that is pretty 

394
00:17:54,231 --> 00:17:55,440
close to what we currently have and say 
deep convolutional neural networks.

395
00:17:55,950 --> 00:18:00,950
But then we talk about some kind of the 
output of that isn't just pattern class 

396
00:18:00,950 --> 00:18:02,310
labels,
but what we call the cognitive core core

397
00:18:02,311 --> 00:18:02,940
cognition.

398
00:18:03,120 --> 00:18:05,520
So we get an understanding of space and 
objects,

399
00:18:05,670 --> 00:18:07,320
their physics,
other people,

400
00:18:07,321 --> 00:18:09,810
their minds.
That's the real stuff of cognition.

401
00:18:09,811 --> 00:18:13,620
That has to be the output of perception.
But somehow we have to,

402
00:18:13,710 --> 00:18:14,670
we have,
we have to have,

403
00:18:14,700 --> 00:18:16,920
this is what we call the brain ostp in 
this picture.

404
00:18:17,160 --> 00:18:22,160
We have to get there by stitching 
together the bottom up inputs from a 

405
00:18:22,160 --> 00:18:22,160
glimpse here,
a glimpse here,

406
00:18:22,160 --> 00:18:24,900
a little bit here and there,
and accessing prior knowledge that comes

407
00:18:24,901 --> 00:18:29,520
from our memory systems to tell us how 
to stitch these things together into the

408
00:18:29,521 --> 00:18:32,430
really core cognitive representations of
what's out there in the world.

409
00:18:32,760 --> 00:18:37,760
And then if we're going to start to talk
about it in language or to build plans 

410
00:18:37,770 --> 00:18:40,620
on top of what we have seen and 
understood,

411
00:18:40,830 --> 00:18:43,440
that's where we talk about symbols 
coming into the picture.

412
00:18:44,180 --> 00:18:49,180
Okay.
The building blocks of language and 

413
00:18:49,180 --> 00:18:49,180
plans and so on.

414
00:18:49,180 --> 00:18:49,180
Okay.
Um,

415
00:18:49,350 --> 00:18:50,610
so now we might say,
well,

416
00:18:50,611 --> 00:18:55,611
okay,
this is an architecture that is brain 

417
00:18:55,611 --> 00:18:57,921
inspired and cognitively inspired and,
and we're planning to turn into real 

418
00:18:57,921 --> 00:18:58,110
engineering and you can say,
well,

419
00:18:58,111 --> 00:18:59,310
do we need that?
Maybe,

420
00:18:59,311 --> 00:18:59,730
you know,
again,

421
00:18:59,760 --> 00:19:02,220
I know this is a question you consider 
it in the first lecture,

422
00:19:02,430 --> 00:19:07,430
maybe the engineering tool kit that's 
currently been making a lot of progress 

423
00:19:07,430 --> 00:19:08,550
in let's say industry.
Maybe that's good enough.

424
00:19:08,551 --> 00:19:10,500
Maybe you know,
let's take deep learning,

425
00:19:10,501 --> 00:19:15,501
but to stand for a broader set of modern
pattern recognition based on 

426
00:19:15,501 --> 00:19:16,620
reinforcement learning based tools and 
say,

427
00:19:16,621 --> 00:19:17,700
okay,
well maybe,

428
00:19:18,150 --> 00:19:21,090
uh,
that can scale up to this and you might,

429
00:19:21,140 --> 00:19:22,830
you know,
maybe that's possible.

430
00:19:22,831 --> 00:19:24,930
I'm happy in the question period of 
people want to debate this.

431
00:19:24,931 --> 00:19:26,700
My sense is no.
Um,

432
00:19:26,701 --> 00:19:28,150
I think that,
um,

433
00:19:28,200 --> 00:19:29,490
it's not a.
When I say no,

434
00:19:29,491 --> 00:19:32,280
I don't mean like it,
it can't happen or it won't happen.

435
00:19:32,460 --> 00:19:37,460
What I mean is the highest value,
the highest expected route right now is 

436
00:19:37,460 --> 00:19:38,820
to take this more science based reverse 
engineering approach.

437
00:19:39,330 --> 00:19:44,330
And that if at least if you follow the 
current trajectory that industry 

438
00:19:44,330 --> 00:19:48,171
incentives especially optimize for,
it's not even really trying to take us 

439
00:19:48,171 --> 00:19:48,690
to these things.
So think about,

440
00:19:48,691 --> 00:19:51,720
for example,
a case study of visual intelligence that

441
00:19:51,721 --> 00:19:54,780
is in some ways as pattern recognition,
very much of a success.

442
00:19:54,810 --> 00:19:56,820
It's again been mostly driven by 
industry.

443
00:19:57,150 --> 00:20:01,230
It's something that if you read in the 
news or even play around with in certain

444
00:20:01,510 --> 00:20:04,740
publicly available data sets,
feels like we've made great progress.

445
00:20:04,890 --> 00:20:06,990
And this is an aspect of visual 
intelligence,

446
00:20:07,140 --> 00:20:09,390
which is sometimes called image 
captioning.

447
00:20:09,620 --> 00:20:12,860
It's bay or mapping images to text.
Um,

448
00:20:13,240 --> 00:20:18,240
you know,
basically there's been a bunch of 

449
00:20:18,240 --> 00:20:18,240
systems.
Here's a couple of press releases.

450
00:20:18,240 --> 00:20:18,240
I guess this one's about Google.

451
00:20:18,450 --> 00:20:21,330
Google's Ai can now capture images 
almost as well as humans.

452
00:20:21,610 --> 00:20:23,250
Um,
here's ones about Microsoft.

453
00:20:23,500 --> 00:20:26,170
Um,
a couple of years ago I think there were

454
00:20:26,171 --> 00:20:31,171
something like eight papers all released
on to archive around the same time from 

455
00:20:31,171 --> 00:20:34,761
basically all the major industry 
computer vision groups as well as a 

456
00:20:34,761 --> 00:20:34,761
couple of academic partners.
Okay.

457
00:20:35,100 --> 00:20:40,100
Which all driven by basically the same 
dataset produced by some Microsoft 

458
00:20:40,100 --> 00:20:41,310
researchers.
And other collaborators,

459
00:20:41,630 --> 00:20:44,910
I'm trained a combination of deep 
convolutional neural networks,

460
00:20:44,911 --> 00:20:45,690
you know,
state of the art,

461
00:20:45,691 --> 00:20:48,370
visual pattern recognition with 
recurrent neural networks,

462
00:20:48,371 --> 00:20:50,660
which had recently been developed for,
you know,

463
00:20:50,680 --> 00:20:53,080
basically kinds of neural statistical 
language modeling,

464
00:20:53,320 --> 00:20:55,300
glued them together and produce the 
system which,

465
00:20:55,330 --> 00:21:00,330
which,
which made very impressive results in a 

466
00:21:00,330 --> 00:21:03,151
big training set and a held out test set
where the goal was to take an image and 

467
00:21:03,151 --> 00:21:07,351
write a sentence like short sentence 
caption that that would seem like the 

468
00:21:07,351 --> 00:21:08,950
kind of way a human would describe that 
image.

469
00:21:09,580 --> 00:21:14,580
And these systems have surpassed human 
level accuracy on the held out test set 

470
00:21:14,580 --> 00:21:17,110
from a big training center.
But what you can see when you really dig

471
00:21:17,111 --> 00:21:22,111
into these things is there's often a lot
of what I would call Dataset 

472
00:21:22,111 --> 00:21:22,380
overfitting.
It's not overfitting to the trainings,

473
00:21:22,750 --> 00:21:27,750
but it's overfitting to whatever are the
particular characteristics of this data 

474
00:21:27,750 --> 00:21:27,750
set.
You know,

475
00:21:27,750 --> 00:21:31,950
wherever or wherever it came from.
Certain set of photographs in certain 

476
00:21:31,950 --> 00:21:32,020
ways of captioning them.
Okay.

477
00:21:32,080 --> 00:21:34,210
Which even a big Dataset,
um,

478
00:21:34,300 --> 00:21:39,300
it's not about quantity.
It's more about the quality of the 

479
00:21:39,300 --> 00:21:39,300
nature of what people are doing.
All right.

480
00:21:39,300 --> 00:21:44,300
Um,
so one way to test this system is to 

481
00:21:44,300 --> 00:21:46,760
apply it to what seems like basically 
the same problem but not within the,

482
00:21:46,870 --> 00:21:51,870
a certain curated or built dataset.
And there's a convenient a twitter bot 

483
00:21:51,870 --> 00:21:55,921
that lets you do this.
So there's something called the pick 

484
00:21:55,921 --> 00:21:58,261
desk bought,
which takes one of the state of the art 

485
00:21:58,261 --> 00:21:58,261
industry.

486
00:21:58,261 --> 00:21:58,261
Ai captioning systems.
A very good one.

487
00:21:58,261 --> 00:21:59,410
Again,
this is not meant to.

488
00:21:59,710 --> 00:22:02,290
I'm not trying to critique these systems
for what they're trying to do.

489
00:22:02,291 --> 00:22:04,600
I'm just trying to point out what they 
don't really even try to do.

490
00:22:04,930 --> 00:22:09,930
So this takes the Microsoft caption 
bought and just every couple of hours 

491
00:22:09,930 --> 00:22:14,551
takes a random image from the web,
captions it and uploads the results to 

492
00:22:14,551 --> 00:22:18,091
twitter.
And a couple of months ago when I 

493
00:22:18,091 --> 00:22:20,041
prepared a first version of this talk,
I just took a few days in the life of 

494
00:22:20,041 --> 00:22:21,250
this twitter Bot.
I didn't take every single image,

495
00:22:21,530 --> 00:22:22,430
but I took,
you know,

496
00:22:22,630 --> 00:22:27,630
most of the images in a way that was 
meant to be representative of the 

497
00:22:27,630 --> 00:22:30,511
successes.
And the kinds of failures that such a 

498
00:22:30,511 --> 00:22:32,701
system will make so we can go through 
this and it's a little bit entertaining 

499
00:22:32,701 --> 00:22:35,821
and I think quite informative.
So here's just a somewhat random sample 

500
00:22:35,821 --> 00:22:38,710
of a few days in the life of one of 
these caption bots.

501
00:22:39,220 --> 00:22:42,220
So here we have a picture of a person 
holding.

502
00:22:42,430 --> 00:22:45,010
Fortunately my screen is very small here
and I can't read up there,

503
00:22:45,011 --> 00:22:46,130
so maybe you'll have to tell me what 
sets,

504
00:22:46,170 --> 00:22:48,940
but a person holding a cell phone.
I guess I'll just read along with you.

505
00:22:49,090 --> 00:22:50,290
So you have a person holding a cell 
phone.

506
00:22:50,291 --> 00:22:51,760
Well,
it's not a person holding a cell phone,

507
00:22:51,790 --> 00:22:56,790
but it's kind of close.
It's a person holding some kind of 

508
00:22:56,790 --> 00:22:56,790
machine to.
I don't even know what that is,

509
00:22:56,790 --> 00:22:57,580
but it's some kind of musical 
instrument,

510
00:22:57,640 --> 00:22:58,630
right?
Um,

511
00:22:59,590 --> 00:23:02,770
so that's a mixed success or failure.
Here's a pretty good one.

512
00:23:02,771 --> 00:23:05,140
A group of people on a,
on a field playing football.

513
00:23:05,320 --> 00:23:06,490
That's,
I would call that a,

514
00:23:06,550 --> 00:23:07,780
you know,
a result,

515
00:23:08,290 --> 00:23:09,910
maybe even a plus.
Um,

516
00:23:09,930 --> 00:23:12,640
here's a group of people standing on top
of a mountain.

517
00:23:12,760 --> 00:23:17,760
So less good there was a mountain,
but as far as I can tell there's no 

518
00:23:17,760 --> 00:23:20,131
people,
but these systems like to see people 

519
00:23:20,131 --> 00:23:20,230
because of both the combination because 
in the data set they were trained on,

520
00:23:20,231 --> 00:23:22,270
there's a lot of people and people often
talk about people.

521
00:23:22,410 --> 00:23:27,410
Okay.
And the fact that you can appreciate 

522
00:23:27,410 --> 00:23:28,590
both what I said and why it's funny.
That's there.

523
00:23:28,710 --> 00:23:33,710
You did some of my cognitive activities 
that the system is not even trying to 

524
00:23:33,710 --> 00:23:33,710
do.
Okay.

525
00:23:33,710 --> 00:23:35,860
Here we've got a building with a cake.
I'll go through these fast building with

526
00:23:35,861 --> 00:23:40,861
a cake,
a large stone building with the clock 

527
00:23:40,861 --> 00:23:40,861
tower.
I think that's pretty good.

528
00:23:40,861 --> 00:23:41,440
I'd give that like a b plus.
There's no clock,

529
00:23:41,620 --> 00:23:44,290
but it's plausibly right.
There might be a clock in there.

530
00:23:44,291 --> 00:23:49,291
There's definitely something like that.
Here's a truck parked on the side of a 

531
00:23:49,291 --> 00:23:49,291
building.
I don't know,

532
00:23:49,291 --> 00:23:53,231
maybe a b minus.
There is a car on the side of a 

533
00:23:53,231 --> 00:23:53,231
building,
but it's not a truck and it's.

534
00:23:53,231 --> 00:23:54,980
And it's not.
Doesn't seem like the main thing and the

535
00:23:54,981 --> 00:23:55,740
image.
Okay.

536
00:23:56,330 --> 00:24:01,330
Here's a necklace made of bananas.
Here's a large ship in the water.

537
00:24:01,760 --> 00:24:06,760
This is pretty good.
I give this like an a minus or b plus 

538
00:24:06,760 --> 00:24:07,070
because there is a ship in the water,
but it's not very large.

539
00:24:07,071 --> 00:24:08,720
It's really more of like a tug boat or 
something.

540
00:24:09,320 --> 00:24:10,910
Here's a sign sitting on the grass,
you know,

541
00:24:10,911 --> 00:24:12,510
in some sense that's great.
No,

542
00:24:12,520 --> 00:24:13,700
but,
but in another sense,

543
00:24:13,701 --> 00:24:16,550
it's really missing what's actually 
interesting and important and meaningful

544
00:24:16,551 --> 00:24:18,000
to humans.
Um,

545
00:24:18,530 --> 00:24:19,310
here's a,
uh,

546
00:24:20,660 --> 00:24:25,070
here's a garden is in the dirt,
a pizza sitting on top of a building,

547
00:24:25,640 --> 00:24:27,830
a small house with a red brick building.
That's pretty good.

548
00:24:27,831 --> 00:24:29,210
Although it kind of weird way of saying 
it.

549
00:24:29,510 --> 00:24:31,190
A vintage photo of a pond.
That's good.

550
00:24:31,250 --> 00:24:36,250
They liked vintage photos.
A group of people that are standing in 

551
00:24:36,250 --> 00:24:36,250
the grass near bridge.
Again,

552
00:24:36,250 --> 00:24:36,800
there's two people and there's some 
grass and there was a bridge,

553
00:24:36,801 --> 00:24:38,870
but it's really not what's going on.

554
00:24:39,610 --> 00:24:41,330
A person in the yard.
Okay.

555
00:24:41,500 --> 00:24:42,600
Kind of,
um,

556
00:24:42,601 --> 00:24:44,270
a group of people standing on top of the
boat.

557
00:24:44,271 --> 00:24:49,271
There's a boat,
there's a group of people there 

558
00:24:49,271 --> 00:24:49,271
standing.
But again,

559
00:24:49,271 --> 00:24:51,761
it's the sentence that you see is,
is more based on a bias of what people 

560
00:24:51,761 --> 00:24:54,650
have said in the past about images that 
are only vaguely like this.

561
00:24:55,120 --> 00:24:57,170
A clock tower lit up at night.
That's really,

562
00:24:57,171 --> 00:24:58,220
I think,
pretty impressive.

563
00:24:58,600 --> 00:25:01,070
A large clock mounted to the side of a 
building a little bit less.

564
00:25:01,071 --> 00:25:05,540
So a snow covered feel very good.
A building with snow on the ground.

565
00:25:05,810 --> 00:25:07,830
A little bit less good.
There's no snow white.

566
00:25:08,600 --> 00:25:13,600
Some people who I don't know them,
but I bet that's probably right because 

567
00:25:13,600 --> 00:25:17,021
face identifying faces and recognizing 
people who are famous because they won 

568
00:25:17,021 --> 00:25:20,710
medals in the Olympics.
Probably I would trust current pattern 

569
00:25:20,710 --> 00:25:20,710
recognition systems to get that.

570
00:25:20,710 --> 00:25:22,430
A painting of a vase in front of a 
mirror.

571
00:25:22,710 --> 00:25:25,100
I'm less good.
Also a famous person there,

572
00:25:25,101 --> 00:25:28,880
but we didn't get him a person walking 
in the rain.

573
00:25:28,910 --> 00:25:33,910
Again,
there is sort of a person and there's 

574
00:25:33,910 --> 00:25:33,910
some puddles but not,
you know,

575
00:25:33,910 --> 00:25:38,740
a group of stuffed animals.
A car parked in a parking lot that's 

576
00:25:38,740 --> 00:25:40,580
good.
A car parked in front of a building.

577
00:25:41,540 --> 00:25:43,610
Less good.
A plate with a fork and knife.

578
00:25:44,090 --> 00:25:45,380
A clear blue sky.
Okay,

579
00:25:45,500 --> 00:25:50,500
so you get the idea again,
like if you actually go and play with 

580
00:25:50,500 --> 00:25:50,750
the system,
partly because I think Mike,

581
00:25:50,770 --> 00:25:52,860
but my friends at Microsoft told me 
they've improved it.

582
00:25:52,861 --> 00:25:53,720
Some,
you know,

583
00:25:54,000 --> 00:25:54,420
uh,
uh,

584
00:25:54,440 --> 00:25:56,550
this is partly for entertainment value 
is,

585
00:25:56,551 --> 00:26:01,551
you know,
I chose what also would be the fun here 

586
00:26:01,551 --> 00:26:01,551
example.
So I'm clear,

587
00:26:01,551 --> 00:26:01,551
I want to be quite honest about this and
these are.

588
00:26:01,551 --> 00:26:04,760
I'm not trying to take away what are 
impressive ai technologies,

589
00:26:05,270 --> 00:26:10,270
but I think it's clear that there's a 
sense of understanding any one of these 

590
00:26:10,270 --> 00:26:12,410
images that it's important to see that 
even when it seems to be correct,

591
00:26:12,650 --> 00:26:13,040
right?

592
00:26:13,070 --> 00:26:15,170
If it can make the kind of errors that 
it makes,

593
00:26:15,650 --> 00:26:20,650
that even when it seems to be correct,
it's probably not doing what you're 

594
00:26:20,650 --> 00:26:23,771
doing and it's probably not even trying 
to seel towards the dimensions of 

595
00:26:23,771 --> 00:26:25,640
intelligence that we think about when 
we're talking about human intelligence.

596
00:26:25,670 --> 00:26:27,290
Okay.
Another way to put this,

597
00:26:27,480 --> 00:26:32,480
I'm going to show you a really 
insightful blog post from one of your 

598
00:26:32,480 --> 00:26:32,480
other speakers.
So,

599
00:26:32,480 --> 00:26:32,750
uh,
in a couple of days,

600
00:26:32,751 --> 00:26:36,020
I'm not sure you're going to have Andre 
carpathy who's one of the leading people

601
00:26:36,440 --> 00:26:39,830
in deep learning.
This is a really great blog post.

602
00:26:39,831 --> 00:26:42,470
He wrote a couple of years ago when he 
was,

603
00:26:42,471 --> 00:26:45,120
I think still at Stanford.
He got his phd from Stanford.

604
00:26:45,121 --> 00:26:48,270
He did.
He worked at Google a little bit on some

605
00:26:48,330 --> 00:26:51,070
early big neural net ai projects there.
Uh,

606
00:26:51,150 --> 00:26:56,150
he was at open Ai.
He was one of the founders of open ai 

607
00:26:56,150 --> 00:26:57,840
and recently he joined Tesla as their 
director of Ai Research.

608
00:26:58,500 --> 00:27:03,500
But about five years ago he was looking 
at the state of computer vision from a 

609
00:27:03,500 --> 00:27:06,360
human intelligence point of view and,
and lamenting how far away we were.

610
00:27:06,361 --> 00:27:08,130
Okay.
So this is the title of his blog posts,

611
00:27:08,131 --> 00:27:10,170
the state of computer vision and ai and 
ai.

612
00:27:10,370 --> 00:27:11,730
We are really,
really far away.

613
00:27:11,880 --> 00:27:16,880
And he took this image which was a sort 
of a famous image in its own right.

614
00:27:16,891 --> 00:27:19,800
It was a popular image of Obama back 
when he was president,

615
00:27:19,920 --> 00:27:22,170
kind of playing around us.
He liked to do when he was on tour.

616
00:27:22,171 --> 00:27:27,171
So if you take a look at this,
you can see you probably all can 

617
00:27:27,171 --> 00:27:27,810
recognize the previous president of the 
United States,

618
00:27:27,960 --> 00:27:32,960
but you can also get the sense of where 
he is and what's going on and you might 

619
00:27:32,960 --> 00:27:36,531
see people smiling and you might get the
sense that he's playing a joke on 

620
00:27:36,531 --> 00:27:36,531
someone.

621
00:27:36,531 --> 00:27:36,531
Can you see that?
Right?

622
00:27:36,531 --> 00:27:39,210
So how do you know that he's playing a 
joke and what that joke is?

623
00:27:39,810 --> 00:27:44,810
Well,
as Andre goes on to talk about and his 

624
00:27:44,810 --> 00:27:44,810
blog posts to,
if you think about all the things that,

625
00:27:44,810 --> 00:27:49,610
that you have to really deploy in your 
mind to understand that it's a huge 

626
00:27:49,610 --> 00:27:53,961
list.
Of course it starts with seeing people 

627
00:27:53,961 --> 00:27:53,961
and objects and maybe doing some face 
recognition,

628
00:27:53,961 --> 00:27:55,080
but you have to do things like,
for example,

629
00:27:55,260 --> 00:28:00,260
notice his foot on the scale and 
understand enough about how scales work 

630
00:28:00,260 --> 00:28:04,071
that when a foot presses down,
it exerts forest at the scale of 

631
00:28:04,071 --> 00:28:07,131
sensitive,
doesn't just magically measure people's 

632
00:28:07,131 --> 00:28:07,131
weight.
But it does that somehow through force.

633
00:28:07,131 --> 00:28:09,150
You have to see who can see that he's 
doing that and who can't,

634
00:28:09,180 --> 00:28:11,280
who cannot see that he's doing that 
right.

635
00:28:11,281 --> 00:28:16,281
And particularly the person on the scale
and why some people can see that he's 

636
00:28:16,281 --> 00:28:16,890
doing that and can see that some other 
people can't see it,

637
00:28:17,040 --> 00:28:18,510
why that makes it funny to them.

638
00:28:18,710 --> 00:28:23,710
Okay.
And someday we shouldn't have machines 

639
00:28:23,710 --> 00:28:25,260
that can understand this,
but hopefully you can see why,

640
00:28:25,261 --> 00:28:26,130
what I,
what I,

641
00:28:26,420 --> 00:28:31,420
what,
what the kind of architecture that I'm 

642
00:28:31,420 --> 00:28:33,321
talking about would be the building 
blocks of the ingredients to be able to 

643
00:28:33,321 --> 00:28:34,640
get them to do that.
Now I,

644
00:28:34,860 --> 00:28:35,560
when I,
again,

645
00:28:35,561 --> 00:28:40,561
I,
I prepared a version of this talk a few 

646
00:28:40,561 --> 00:28:42,650
months ago and I wrote to Andre and I 
said I was going to use this and I was 

647
00:28:42,650 --> 00:28:45,321
curious if he,
what if he had any reflections on this 

648
00:28:45,321 --> 00:28:49,400
and where he thought we were relative to
five years ago because certainly a lot 

649
00:28:49,400 --> 00:28:50,220
of progress has been made,
but he said,

650
00:28:50,221 --> 00:28:51,660
here's this email.
Um,

651
00:28:52,010 --> 00:28:53,460
I hope he doesn't mind me sharing it,
but I mean,

652
00:28:53,461 --> 00:28:55,260
again,
he's a very honest person and that's one

653
00:28:55,261 --> 00:28:58,320
of the many reasons why he's such an 
important person right now in ai.

654
00:28:58,500 --> 00:29:03,500
Okay.
He's both very technically strong and 

655
00:29:03,500 --> 00:29:03,500
honest about what we can do,
but we can't do.

656
00:29:03,500 --> 00:29:03,500
And as he says,
but what does he say?

657
00:29:03,500 --> 00:29:04,940
It's nice to hear from you.
Uh,

658
00:29:05,140 --> 00:29:07,420
it's funny you should bring this up.
I was also thinking about writing a,

659
00:29:07,930 --> 00:29:12,930
a return to this and in short,
basically I don't believe we've made 

660
00:29:12,930 --> 00:29:12,930
very much progress,
right?

661
00:29:12,930 --> 00:29:17,450
He points out that in his long list of 
things that you need to understand the 

662
00:29:17,450 --> 00:29:18,480
image we have made progress on some the 
ability to again,

663
00:29:18,590 --> 00:29:21,720
the tech people and do face recognition 
for well known individuals.

664
00:29:21,780 --> 00:29:22,540
Okay.
Um,

665
00:29:22,560 --> 00:29:24,810
but that's kind of about it all right.
Um,

666
00:29:24,830 --> 00:29:29,830
and he wasn't particularly optimistic 
that the current route that's being 

667
00:29:29,830 --> 00:29:32,751
pursued and industry is,
is anywhere close to solving or even 

668
00:29:32,751 --> 00:29:33,690
really trying to solve these larger 
questions.

669
00:29:34,240 --> 00:29:39,240
Um,
if we give this image to that a 

670
00:29:39,240 --> 00:29:39,240
captioned bought,
you know,

671
00:29:39,240 --> 00:29:41,100
what we see is again,
represents the same point.

672
00:29:41,110 --> 00:29:42,370
So here's the caption,
but it says,

673
00:29:42,430 --> 00:29:45,610
I think it's a group of people standing 
next to a man in a suit and tie,

674
00:29:45,910 --> 00:29:47,350
right?
So that's right,

675
00:29:47,650 --> 00:29:49,420
right.
As far as it goes,

676
00:29:49,540 --> 00:29:51,610
it's just doesn't go far enough.
And the current,

677
00:29:52,120 --> 00:29:57,120
the current ideas of build a Dataset,
train a deep learning algorithm on it 

678
00:29:57,120 --> 00:29:57,730
and then repeat,
um,

679
00:29:58,180 --> 00:30:03,180
aren't really even,
I would venture trying to get to what 

680
00:30:03,180 --> 00:30:06,931
we're talking about or here's another.
I'll just give you one other example of 

681
00:30:06,931 --> 00:30:07,630
a couple of photographs from my recent 
vacation.

682
00:30:08,040 --> 00:30:10,300
I'm in a nice warm,
tropical local,

683
00:30:10,670 --> 00:30:12,820
um,
which I think illustrate ways in which,

684
00:30:12,821 --> 00:30:17,821
again,
the gap where we have machines that can 

685
00:30:17,821 --> 00:30:20,470
say beat the world's best at go but 
can't even beat a child.

686
00:30:20,471 --> 00:30:22,390
A tic TAC toe.
Now what do I mean by that?

687
00:30:22,391 --> 00:30:23,260
Well,
you know,

688
00:30:23,261 --> 00:30:28,261
of course we can build,
we don't even need reinforcement 

689
00:30:28,261 --> 00:30:30,811
learning or deep learning to build a 
machine that can win or tie do as do 

690
00:30:30,811 --> 00:30:30,940
optimally in tic TAC toe.

691
00:30:31,210 --> 00:30:33,190
But think about this.
This is a real tic TAC toe game,

692
00:30:33,191 --> 00:30:35,440
which I saw on the grass outside my 
hotel,

693
00:30:35,680 --> 00:30:36,810
right?
Um,

694
00:30:36,880 --> 00:30:41,880
what do you have to do to look at this 
and recognize that it's a tic Tac toe 

695
00:30:41,880 --> 00:30:44,671
game.
You have to see the objects that you 

696
00:30:44,671 --> 00:30:44,671
have to see what's,
you know,

697
00:30:44,671 --> 00:30:44,671
in some sense there's a three by three 
grid,

698
00:30:44,671 --> 00:30:46,120
but it's,
but it's only abstract,

699
00:30:46,150 --> 00:30:47,860
right?
It's only limited by this.

700
00:30:47,890 --> 00:30:49,690
These ropes are strings.
Okay?

701
00:30:51,220 --> 00:30:54,010
It's not actually a grid in any simple 
geometric sense.

702
00:30:54,280 --> 00:30:59,280
Alright?
But yet a child can look at that and 

703
00:30:59,280 --> 00:30:59,280
indeed here's an actual child who was 
looking at it and recognize,

704
00:30:59,280 --> 00:31:00,820
oh,
it's a game of tic Tac toe and even know

705
00:31:00,821 --> 00:31:05,821
what they need to do to win,
namely put the x and completed and now 

706
00:31:05,821 --> 00:31:05,821
they've got three in a row,
right?

707
00:31:05,821 --> 00:31:05,920
That's literally childsplay.

708
00:31:06,190 --> 00:31:07,280
Okay.
Um,

709
00:31:07,390 --> 00:31:09,280
you show this sort of thing though to 
one of these,

710
00:31:09,310 --> 00:31:11,290
you know,
image understanding captioned bots,

711
00:31:11,320 --> 00:31:13,930
and I think it's a closeup of a sign.
Okay?

712
00:31:14,180 --> 00:31:14,890
Um,
again,

713
00:31:14,950 --> 00:31:17,530
it's not saying that this is a closeup 
of a sign,

714
00:31:17,740 --> 00:31:21,430
is,
is not the same thing.

715
00:31:21,880 --> 00:31:25,180
I would venture as a,
as a cognitive or computational activity

716
00:31:25,390 --> 00:31:27,400
that's going to give us what we need to 
say.

717
00:31:27,430 --> 00:31:30,880
Recognize the objects to recognize it as
a game to understand the goal and how to

718
00:31:30,881 --> 00:31:35,881
plan to achieve those goals.
Whereas this kind of architecture is 

719
00:31:35,881 --> 00:31:36,550
designed to try to do all of these 
things ultimately.

720
00:31:36,551 --> 00:31:41,551
Right?
And I bring in these examples of games 

721
00:31:41,551 --> 00:31:44,380
or jokes to really show where perception
goes to cognition,

722
00:31:44,410 --> 00:31:45,480
you know,
that,

723
00:31:45,530 --> 00:31:46,990
uh,
at all the way up to symbols,

724
00:31:47,050 --> 00:31:52,050
right?
So to get objects and forces and mental 

725
00:31:52,050 --> 00:31:52,600
states,
that's the cognitive core,

726
00:31:52,960 --> 00:31:57,960
but to be able to get goals and plans 
and what do I do or how do I talk about 

727
00:31:57,960 --> 00:31:57,960
it?

728
00:31:57,960 --> 00:31:58,300
That's symbols.
Okay,

729
00:31:59,650 --> 00:32:02,290
here's another way into this.
And it's one that also motivates,

730
00:32:02,291 --> 00:32:07,291
I think a lot of really good work on the
engineering side and a lot of our 

731
00:32:07,291 --> 00:32:10,501
interest in the science side is think 
about robotics and think about what do 

732
00:32:10,501 --> 00:32:11,680
you have to do to,
you know,

733
00:32:11,700 --> 00:32:16,700
what,
what does the brain have to be light to 

734
00:32:16,700 --> 00:32:16,700
control the body?
So again,

735
00:32:16,700 --> 00:32:19,921
you're gonna hear from certainly,
I think maybe it's next week from Mark 

736
00:32:19,921 --> 00:32:23,970
Robert,
who's a one of the founders of Boston 

737
00:32:23,970 --> 00:32:26,701
dynamics,
which is one of my favorite companies 

738
00:32:26,701 --> 00:32:26,701
anywhere there,
uh,

739
00:32:26,701 --> 00:32:29,380
without doubt the leading maker of 
humanoid robots,

740
00:32:29,381 --> 00:32:33,010
legged locomoting robots and industry.
They have also all sorts of other really

741
00:32:33,011 --> 00:32:35,980
cool robots,
robots like dogs,

742
00:32:35,981 --> 00:32:37,270
robots that have,
you know,

743
00:32:37,300 --> 00:32:37,740
you'll,
you'll,

744
00:32:37,750 --> 00:32:42,750
you'll,
I think you'll even get to see live 

745
00:32:42,750 --> 00:32:42,750
demonstration of one of his robots.
This really awesome,

746
00:32:42,750 --> 00:32:42,750
impressive stuff.

747
00:32:44,090 --> 00:32:46,850
But what about the minds and brains of 
these robots will,

748
00:32:46,851 --> 00:32:47,840
again,
if you ask mark,

749
00:32:47,841 --> 00:32:52,130
ask them how much of of human light 
cognition do they have in their robots?

750
00:32:52,131 --> 00:32:54,770
And I think he would say very little.
In fact,

751
00:32:54,800 --> 00:32:56,510
we have asked him that and he would say 
very little,

752
00:32:57,110 --> 00:32:59,360
very little.
He's,

753
00:32:59,390 --> 00:33:01,280
he's actually one of the advisors of our
center.

754
00:33:01,281 --> 00:33:03,950
And I think in many ways we're very much
on the same page.

755
00:33:03,951 --> 00:33:08,951
We both want to know how do you build 
the kind of intelligence that can 

756
00:33:08,951 --> 00:33:10,250
control these bodies.
I'm like the way a human does.

757
00:33:10,660 --> 00:33:11,270
All right.
Um,

758
00:33:11,360 --> 00:33:13,340
here's another example of an industry 
robotics effort.

759
00:33:13,341 --> 00:33:15,410
This is Google's arm farm where you 
know,

760
00:33:15,411 --> 00:33:20,411
they've,
they've got lots of robot arms and 

761
00:33:20,411 --> 00:33:21,941
they're trying to train them to pick up 
objects using various kinds of deep 

762
00:33:21,941 --> 00:33:21,941
learning and reinforcement learning 
techniques.

763
00:33:21,980 --> 00:33:23,240
And I think it's one approach.

764
00:33:23,270 --> 00:33:26,510
I just think it's very,
very different from the way humans learn

765
00:33:26,511 --> 00:33:31,511
to say,
control their body and manipulate 

766
00:33:31,511 --> 00:33:33,251
objects and you can see that in terms of
things that go back to what you were 

767
00:33:33,251 --> 00:33:33,251
saying when you were introducing me,
right?

768
00:33:33,251 --> 00:33:35,660
Think about how quickly we learn the 
things right here.

769
00:33:35,661 --> 00:33:38,060
You have these arm farm is trying to 
generate,

770
00:33:38,150 --> 00:33:40,340
you know,
effectively maybe if not infinite,

771
00:33:40,370 --> 00:33:45,370
but hundreds of thousands,
millions of examples of reaches and 

772
00:33:45,370 --> 00:33:49,451
pickups of objects even with just a 
single gripper and yet a child who in 

773
00:33:49,451 --> 00:33:53,621
some ways can't control their body 
nearly as well as robots can be 

774
00:33:53,621 --> 00:33:55,430
controlled at the low level,
is able to do so much more.

775
00:33:55,730 --> 00:33:58,820
So I'll show you two of my favorite 
videos from youtube here,

776
00:33:59,030 --> 00:34:01,010
which motivates some of the research 
that we're doing.

777
00:34:01,520 --> 00:34:06,520
The one on the left is a one and a half 
year old and the other one's a one year 

778
00:34:06,520 --> 00:34:09,311
old.
So just watch this one and a half year 

779
00:34:09,311 --> 00:34:11,200
old here doing a popular activity for 
many kids as a playing a video up there.

780
00:34:19,040 --> 00:34:19,610
Okay,
there we go.

781
00:34:19,850 --> 00:34:20,840
Okay.
So He's,

782
00:34:20,870 --> 00:34:23,330
he's on doing this stacking cup 
activity.

783
00:34:23,930 --> 00:34:28,930
Alright.
He's stacking up cups to make a tall 

784
00:34:28,930 --> 00:34:28,930
tower.
He's got a stack of three.

785
00:34:28,930 --> 00:34:33,750
And what you can see for the first part 
of this video is it looks like he's 

786
00:34:33,750 --> 00:34:35,570
trying to make a second stack at that 
he's trying to pick up at once.

787
00:34:35,810 --> 00:34:40,430
Basically he's trying to make a stack of
two that'll go on the stack of three and

788
00:34:40,640 --> 00:34:41,140
you know,
he's,

789
00:34:41,200 --> 00:34:42,830
he's trying to debug his plan because 
it's,

790
00:34:42,850 --> 00:34:44,560
it got a little bit stuck here.
Um,

791
00:34:45,770 --> 00:34:46,870
but,
and,

792
00:34:46,871 --> 00:34:47,600
and think about,
I mean,

793
00:34:47,601 --> 00:34:48,920
again,
if you know anything about robots,

794
00:34:48,921 --> 00:34:50,870
manipulating objects,
even just what he did,

795
00:34:50,871 --> 00:34:53,570
no robot can,
can decide to do that and actually do it

796
00:34:53,780 --> 00:34:56,960
right at some point he's almost got it.
It's a little bit tricky,

797
00:34:56,961 --> 00:34:59,550
but some point he's going to get that 
stack of two.

798
00:35:00,590 --> 00:35:02,630
He realizes he has to move that opted 
out of the way,

799
00:35:02,631 --> 00:35:04,340
look at what he did,
move out of the way,

800
00:35:04,350 --> 00:35:09,350
use two hands to pick it up.
And now he's got a stack of two on a 

801
00:35:09,350 --> 00:35:09,350
stack of three and suddenly,
you know,

802
00:35:09,350 --> 00:35:11,270
sub goal completed.
He's now got a stack of five and he,

803
00:35:11,271 --> 00:35:15,050
he gives himself a hand because he know,
he knows he accomplished a key way point

804
00:35:15,080 --> 00:35:20,080
along the way to his final goal.
That's a kind of early symbolic 

805
00:35:20,080 --> 00:35:20,080
cognition,
right?

806
00:35:20,080 --> 00:35:22,160
To understand that I'm trying to build a
tall tower,

807
00:35:22,280 --> 00:35:24,080
but the tower is made up of little 
towers.

808
00:35:24,081 --> 00:35:24,540
It's,
you know,

809
00:35:24,700 --> 00:35:29,700
and,
and you can take a tower and put it on 

810
00:35:29,700 --> 00:35:29,700
top of another tower or a stack,
a stack on a stack and you have a bigger

811
00:35:29,700 --> 00:35:30,200
stack,
right?

812
00:35:30,650 --> 00:35:35,650
So think about how he goes from bottom 
up perception to the objects of the 

813
00:35:35,650 --> 00:35:35,720
physical needed to manipulate the 
objects.

814
00:35:35,720 --> 00:35:39,030
So the ability to make those early kinds
of symbolic plans.

815
00:35:39,720 --> 00:35:42,220
At some point he keeps doing this.
He puts another stack on there.

816
00:35:43,830 --> 00:35:45,270
I'll just jump to the end.
Oops,

817
00:35:45,271 --> 00:35:45,890
sorry.
You missed it.

818
00:35:45,891 --> 00:35:46,660
Sorry.
Keep.

819
00:35:47,640 --> 00:35:50,910
He gets really excited and he gives 
himself another big hand,

820
00:35:50,911 --> 00:35:53,370
but falls over again.
Um,

821
00:35:54,180 --> 00:35:57,450
Boston dynamics now has robots that 
could pick themselves up after that.

822
00:35:57,451 --> 00:35:59,210
That's really impressive again.
Um,

823
00:35:59,670 --> 00:36:01,440
but all the other stuff to get to that 
point,

824
00:36:01,470 --> 00:36:06,470
we don't really know how to do in a 
robotic setting or thinking about this 

825
00:36:06,470 --> 00:36:06,470
baby here.
This is a younger baby.

826
00:36:06,470 --> 00:36:11,060
This is one of the Internet's most 
popular videos because it features a 

827
00:36:11,060 --> 00:36:14,661
baby and a cap and,
but the baby's doing something 

828
00:36:14,661 --> 00:36:16,080
interesting.
He's got the same cups,

829
00:36:16,081 --> 00:36:19,290
but he's decided he's again,
decided to try a new thing.

830
00:36:19,291 --> 00:36:24,291
So this is thinking about creativity.
He's decided that his goal is to stack 

831
00:36:24,291 --> 00:36:25,260
up cups on the back of a cat.

832
00:36:25,260 --> 00:36:27,750
I guess he's asking how many cups going 
to fit on the back of a cat?

833
00:36:27,751 --> 00:36:28,590
Well,
three.

834
00:36:28,950 --> 00:36:30,360
Let's see.
Can I fit more?

835
00:36:31,440 --> 00:36:33,360
Let's try another one.
Okay.

836
00:36:33,650 --> 00:36:35,280
Um,
while he can't fit more than three,

837
00:36:35,281 --> 00:36:37,320
it turns out.
And then he then does.

838
00:36:37,410 --> 00:36:39,150
It's not working,
so he changes his goal.

839
00:36:39,270 --> 00:36:42,210
Now his goal appears to be to get the 
cuffs on the other side of the cat.

840
00:36:42,480 --> 00:36:45,000
Now watch that part when he reaches back
behind him there.

841
00:36:45,001 --> 00:36:46,650
That's.
I'll just pause it there for a moment.

842
00:36:47,060 --> 00:36:49,080
Um,
so many just reached back there.

843
00:36:49,210 --> 00:36:51,300
That's a particularly striking moment in
the video.

844
00:36:51,540 --> 00:36:54,960
It shows a very strong form of what we 
call in cognitive science.

845
00:36:55,620 --> 00:36:57,210
Object permanence.
Okay.

846
00:36:57,510 --> 00:37:02,510
That's the idea that you represent 
objects as these permanent enduring 

847
00:37:02,510 --> 00:37:05,720
entities in the world.
Even when you can't see them in this 

848
00:37:05,720 --> 00:37:08,361
case.
He hadn't seen or touched that optic 

849
00:37:08,361 --> 00:37:08,361
behind him for like at least a minute,
right?

850
00:37:08,361 --> 00:37:08,640
Maybe much longer.
I don't know.

851
00:37:09,240 --> 00:37:14,240
And yet he still knew it was there and 
he was able to incorporate it in this 

852
00:37:14,240 --> 00:37:14,240
plan.
Right.

853
00:37:14,240 --> 00:37:14,970
There's a moment before that when he's 
about to reach for it,

854
00:37:14,971 --> 00:37:16,410
but then he sees this other one.
Right.

855
00:37:16,411 --> 00:37:21,411
And it's only when he's now exhausted 
all the other objects here that he can 

856
00:37:21,411 --> 00:37:21,411
see.
He's like,

857
00:37:21,411 --> 00:37:24,591
okay,
now time to get this audit and bring it 

858
00:37:24,591 --> 00:37:24,591
into play.
Right?

859
00:37:24,591 --> 00:37:27,830
So think about what has to be going on 
in his brain for him to be able to do 

860
00:37:27,830 --> 00:37:27,830
that.
Right?

861
00:37:27,830 --> 00:37:29,850
That's like the analog of you 
understanding what's behind you.

862
00:37:30,540 --> 00:37:35,540
Okay.
It's not that these things are 

863
00:37:35,540 --> 00:37:35,540
impossible to capture machines far from 
it.

864
00:37:35,540 --> 00:37:39,200
It's just that like training a deep 
neural network or any kind of pattern 

865
00:37:39,200 --> 00:37:39,200
recognition system we don't think is 
going to do it,

866
00:37:39,330 --> 00:37:42,090
but we think by reverse engineering how 
it works in the brain,

867
00:37:42,420 --> 00:37:44,430
we might be able to do it because we can
do it.

868
00:37:44,550 --> 00:37:49,550
Okay.
It's not just humans that do this kind 

869
00:37:49,550 --> 00:37:49,550
of activity.
Here's a couple of,

870
00:37:49,550 --> 00:37:49,890
again,
fit rather famous videos.

871
00:37:49,891 --> 00:37:53,230
You can watch all of these on youtube.
Crows are famous objects,

872
00:37:53,231 --> 00:37:56,840
manipulators and tool users,
but also orangutans,

873
00:37:57,000 --> 00:37:58,980
other primates,
rodents.

874
00:37:59,970 --> 00:38:02,280
We can watch if we just had.
Let me pause this one for a second.

875
00:38:02,460 --> 00:38:07,460
If we watched this orangutan here,
he's got a bunch of big legos and over 

876
00:38:07,460 --> 00:38:10,140
the course of this video he's building 
up a stack.

877
00:38:10,710 --> 00:38:12,240
Legos.
It's really quite impressive.

878
00:38:13,640 --> 00:38:14,790
Just jumping to the end.

879
00:38:17,220 --> 00:38:20,550
There's actually some controversy out 
there of what this video is a fake.

880
00:38:21,050 --> 00:38:23,190
Um,
but the controversy isn't about,

881
00:38:23,480 --> 00:38:25,600
it's not like whether it was,
I dunno,

882
00:38:25,750 --> 00:38:28,800
done with computer animation,
some people think the video was actually

883
00:38:28,801 --> 00:38:33,801
filmed backwards,
that a human built up the stack and the 

884
00:38:33,801 --> 00:38:36,651
orangutan just slowly disassembled it 
piece by piece and it turns out it's 

885
00:38:36,651 --> 00:38:39,681
remarkably hard to tell whether it's 
played forward or backwards in time and 

886
00:38:39,681 --> 00:38:39,910
people have argued over a little details
because you know,

887
00:38:39,950 --> 00:38:44,950
it will be quite impressive if an 
orangutan actually was able to build up 

888
00:38:44,950 --> 00:38:48,181
this really impressive stack of Legos.
But I would submit that it would be 

889
00:38:48,181 --> 00:38:48,181
almost as impressive if he disassembled 
it.

890
00:38:48,640 --> 00:38:50,050
Think about the activity.
I mean,

891
00:38:50,051 --> 00:38:52,450
if I want it to disassemble that,
the easiest thing to do would just be to

892
00:38:52,451 --> 00:38:54,790
knock it over.
That's really all most robots could do.

893
00:38:55,020 --> 00:39:00,020
But to piece by piece,
disassemble it even if it's played 

894
00:39:00,020 --> 00:39:02,881
backwards like this.
That's still a really impressive act of 

895
00:39:02,881 --> 00:39:04,330
symbolic planning on physical objects.
Or here you've got this,

896
00:39:04,450 --> 00:39:09,450
this famous mouse,
this you can find on the internet under 

897
00:39:09,450 --> 00:39:12,160
the mouse versus cracker video.
And what you'll see here over the course

898
00:39:12,161 --> 00:39:17,161
of this video is a mouse valiantly and 
mostly hopelessly struggling with a 

899
00:39:17,231 --> 00:39:20,050
cracker that they're hoping to bring 
back to their nest.

900
00:39:20,051 --> 00:39:22,980
I guess it's a very appealing big meal.
Um,

901
00:39:23,260 --> 00:39:25,480
and at some point after just trying to 
get it over the,

902
00:39:25,570 --> 00:39:30,570
over the wall,
at some point the mouse just gives up 

903
00:39:30,570 --> 00:39:33,811
because it's just never going to happen 
and he just goes away except that 

904
00:39:33,811 --> 00:39:37,120
because even mouses can dream or my 
dream some point he decides,

905
00:39:37,121 --> 00:39:42,121
okay,
I'm just going to come back for one 

906
00:39:42,121 --> 00:39:43,201
more.
Try and he tries one more time and this 

907
00:39:43,201 --> 00:39:43,201
time validly gets it over.

908
00:39:43,201 --> 00:39:44,170
Yeah.
Isn't that very impressive?

909
00:39:44,740 --> 00:39:45,810
Congratulations about.
Okay.

910
00:39:46,090 --> 00:39:51,090
You don't have to clap.
You can clap for me at the end or clap 

911
00:39:51,090 --> 00:39:51,090
for whoever it later.
Okay.

912
00:39:51,090 --> 00:39:54,780
But I will,
I want to applaud the mouse there every 

913
00:39:54,780 --> 00:39:54,780
time I see that.
Okay.

914
00:39:54,780 --> 00:39:57,720
But again,
think what had to be going on in his 

915
00:39:57,720 --> 00:39:57,720
brain to be able to do that.
All right.

916
00:39:57,720 --> 00:39:59,410
Um,
it's a crazy thing and yet he formulated

917
00:39:59,411 --> 00:40:04,411
the goal and was able to achieve it.
I'll just show one more video that is 

918
00:40:04,411 --> 00:40:06,130
really more about science.
These other ones or you know,

919
00:40:06,160 --> 00:40:08,170
some of them actually were from 
scientific experiments,

920
00:40:08,171 --> 00:40:13,171
but this is one that motivates a lot of 
the science that I do and it's to me it 

921
00:40:13,171 --> 00:40:16,180
sets up kind of a grand cognitive 
science challenge for ai and robotics.

922
00:40:16,450 --> 00:40:18,970
It's from an experiment with humans,
again,

923
00:40:19,000 --> 00:40:22,230
18 month old or one and a half year old,
so the kids in this experiment,

924
00:40:22,231 --> 00:40:24,100
we're the same age as the first baby I 
showed you.

925
00:40:24,100 --> 00:40:26,830
The one who did the stacking and 18 
months is really a very,

926
00:40:26,860 --> 00:40:30,250
very good age to study.
If you're interested in intelligence for

927
00:40:30,251 --> 00:40:32,080
reasons we can talk about later if 
you're interested.

928
00:40:32,950 --> 00:40:36,070
This is from a very famous experiment 
done by two psychologists,

929
00:40:36,071 --> 00:40:38,070
Felix Worn Akin and Michael Thomas 
Cielo,

930
00:40:38,620 --> 00:40:42,790
and it was studying the spontaneous 
helping behavior of young children.

931
00:40:42,940 --> 00:40:47,940
It also contrasted humans and chimps and
the punchline is that chimps sometimes 

932
00:40:47,940 --> 00:40:49,030
do things that are kind of like with 
this human did,

933
00:40:49,031 --> 00:40:51,850
but not nearly as reliably or as 
flexibly.

934
00:40:52,000 --> 00:40:53,790
Okay,
so not nearly as an.

935
00:40:53,860 --> 00:40:58,860
I'll show you a particular kind of 
unusual situation where human kids had 

936
00:40:58,860 --> 00:41:03,780
relatively little trouble figuring out 
what to do or even whether they should 

937
00:41:03,780 --> 00:41:07,111
do it,
whereas basically no chimp did what 

938
00:41:07,111 --> 00:41:07,111
you're going to see humans sometimes 
doing here.

939
00:41:07,111 --> 00:41:11,820
So the experimenter in this movie.
I'll turn on the sound here if you can 

940
00:41:11,820 --> 00:41:11,820
hear it.

941
00:41:11,890 --> 00:41:16,890
The experimenter is the tall guy and the
participant is the little kid in the 

942
00:41:16,890 --> 00:41:20,800
corner.
The sound but no words,

943
00:41:20,801 --> 00:41:25,801
right?
And at some point he stops and then the 

944
00:41:25,801 --> 00:41:26,530
kid just does whatever they want to do.
So watch what he does.

945
00:41:26,531 --> 00:41:28,510
He goes over,
he opens the cabinet,

946
00:41:29,020 --> 00:41:34,020
looks inside,
then he steps back and he looks up at 

947
00:41:34,020 --> 00:41:35,360
Felix and then looks down,
okay,

948
00:41:35,361 --> 00:41:39,020
and then the action is completed.
Now what I want you to watch it one more

949
00:41:39,021 --> 00:41:44,021
time and think about what's going to be 
going inside the kid's head to 

950
00:41:44,021 --> 00:41:44,110
understand this,
to understand like.

951
00:41:44,240 --> 00:41:49,240
So it seems like what it looks like to 
us is the kid figured out that this guy 

952
00:41:49,240 --> 00:41:52,151
needed help and helped him.
And the paper is full of many other 

953
00:41:52,151 --> 00:41:52,151
situations like this.
This is just one.

954
00:41:52,151 --> 00:41:56,800
Okay.
But the key idea is that the situation 

955
00:41:56,800 --> 00:41:56,800
is somewhat novel.

956
00:41:56,800 --> 00:41:57,740
People have seen people holding books 
and opening cabinets,

957
00:41:57,741 --> 00:42:02,741
but probably it's been,
it's very rare to see this kind of 

958
00:42:02,741 --> 00:42:02,741
situation.
Exactly right.

959
00:42:02,741 --> 00:42:03,530
It's,
it's different in some important details

960
00:42:03,680 --> 00:42:05,900
from what you might have seen before.
And there's other ones in there that are

961
00:42:05,901 --> 00:42:08,270
really truly novel because they just 
made up a machine right there.

962
00:42:08,960 --> 00:42:09,650
Okay.
Um,

963
00:42:09,980 --> 00:42:14,980
but somehow he has to understand 
causally from the way the guys banging 

964
00:42:14,980 --> 00:42:15,270
the books against the thing that it's,
it's,

965
00:42:15,370 --> 00:42:20,370
it's sort of both a symbol,
but it's also somehow he's got to 

966
00:42:20,370 --> 00:42:22,400
understand what he can do and what he 
can't do and then what the kid can do to

967
00:42:22,401 --> 00:42:23,460
help.
And I'll,

968
00:42:23,461 --> 00:42:26,000
I'll show this again,
but really just watch.

969
00:42:26,030 --> 00:42:29,770
The main part I want you to see is,
um,

970
00:42:30,850 --> 00:42:35,850
I'll just sort of skip ahead.
So watch this part here.

971
00:42:36,440 --> 00:42:38,930
Let's say I'll just jump right when he 
watched.

972
00:42:38,990 --> 00:42:43,990
Right now he's about to look up.
He looks up and makes eye contact and 

973
00:42:43,990 --> 00:42:44,540
then his eyes look down.
So again,

974
00:42:45,380 --> 00:42:48,380
he looks up,
he looks up,

975
00:42:48,381 --> 00:42:51,960
and then a Sakata sudden rapid eye 
movement down down to his hands up,

976
00:42:51,980 --> 00:42:52,760
down.
Okay?

977
00:42:52,970 --> 00:42:54,770
So that's again,
that's this brain,

978
00:42:54,771 --> 00:42:56,330
ostp and action,
right?

979
00:42:56,360 --> 00:43:00,080
He's making one glance,
small glance at the big guys,

980
00:43:00,100 --> 00:43:02,570
eyes to to make eye contact,
to see,

981
00:43:02,720 --> 00:43:06,350
to get a signal.
Did I understand what you wanted and did

982
00:43:06,351 --> 00:43:11,351
you.
Did you register that joint attention 

983
00:43:11,351 --> 00:43:11,390
and then he makes a prediction about 
what the guy's going to do.

984
00:43:11,420 --> 00:43:16,420
So he looks right down.
He doesn't just like look around 

985
00:43:16,420 --> 00:43:16,550
randomly.
He looks right down to the guys hands to

986
00:43:16,551 --> 00:43:19,010
track the action that he expects to see 
happening.

987
00:43:19,220 --> 00:43:24,220
If I did the right thing to help you,
then I expect you're going to put the 

988
00:43:24,220 --> 00:43:24,220
books there.

989
00:43:24,220 --> 00:43:27,490
Okay?
So you can see these things happening 

990
00:43:27,490 --> 00:43:27,740
and we want to know what's going on 
inside the mind that guides all of that.

991
00:43:28,490 --> 00:43:33,490
All right?
So that's the sort of big scientific 

992
00:43:33,490 --> 00:43:35,621
agenda that we're working on over the 
next few years where we think some kind 

993
00:43:35,621 --> 00:43:39,611
of human understanding of human 
intelligence and scientific terms could 

994
00:43:39,611 --> 00:43:41,960
lead to all sorts of ai payoffs in 
particular.

995
00:43:42,140 --> 00:43:47,140
I suppose we could build a robot that 
could do what this kid and many other 

996
00:43:47,140 --> 00:43:50,111
kids in these experiments do just say,
help you out around the house without 

997
00:43:50,111 --> 00:43:51,830
having to be programmed or even really 
instructed just to kind of get a sense,

998
00:43:51,860 --> 00:43:52,390
oh yeah,
yeah,

999
00:43:52,391 --> 00:43:54,230
you need to have with that shirt.
Let me help you out.

1000
00:43:54,500 --> 00:43:56,540
Okay.
Even 18 month olds will do that.

1001
00:43:56,541 --> 00:43:58,670
Sometimes not very reliably or 
effectively.

1002
00:43:58,671 --> 00:44:01,370
Sometimes they'll try to help and really
do the opposite.

1003
00:44:01,580 --> 00:44:06,580
Right?
But imagine if you could take the 

1004
00:44:06,580 --> 00:44:06,980
flexible understanding of humans,
actions,

1005
00:44:06,981 --> 00:44:11,981
goals and so on,
and make those reliable engineering 

1006
00:44:11,981 --> 00:44:11,981
technology that would be very useful.

1007
00:44:12,290 --> 00:44:17,290
And it would also be related to say 
machines that you could actually start 

1008
00:44:17,290 --> 00:44:17,540
to talk to and trust in some ways.
Right?

1009
00:44:17,541 --> 00:44:20,360
That shared understanding.
So how are we going to do this?

1010
00:44:20,361 --> 00:44:25,361
Well,
let me spend the rest of the time 

1011
00:44:25,361 --> 00:44:25,361
talking about how we tried to do this,
right,

1012
00:44:25,361 --> 00:44:28,870
some of the,
some of the technology that we're 

1013
00:44:28,870 --> 00:44:31,001
building both in our group and more 
broadly to try to make these kinds of 

1014
00:44:31,001 --> 00:44:34,811
architectures real.
And I'll talk about two or three 

1015
00:44:34,811 --> 00:44:34,811
technical ideas,
again,

1016
00:44:34,811 --> 00:44:36,250
not in any detail.
Alright.

1017
00:44:36,450 --> 00:44:41,450
Um,
one is the idea of a probabilistic 

1018
00:44:41,450 --> 00:44:41,450
program.
So this is a kind of,

1019
00:44:41,450 --> 00:44:45,950
um,
think of it as a computational 

1020
00:44:45,950 --> 00:44:50,211
abstraction that we can use to capture 
the common sense knowledge of this core 

1021
00:44:50,211 --> 00:44:54,051
cognition.
So when I say we have an intuitive 

1022
00:44:54,051 --> 00:44:54,051
understanding of physical objects and 
people's goals,

1023
00:44:54,051 --> 00:44:58,910
how do I build a model of that model you
have in the head probabilistic programs 

1024
00:44:58,910 --> 00:45:03,051
a little bit more technically are one 
way to understand them is as a 

1025
00:45:03,051 --> 00:45:06,771
generalization of Bayesian networks or 
other kinds of directed graphical 

1026
00:45:06,771 --> 00:45:06,771
models.

1027
00:45:06,771 --> 00:45:07,050
If you know those.
Okay.

1028
00:45:07,200 --> 00:45:12,200
But we're,
instead of defining a probability model 

1029
00:45:12,200 --> 00:45:15,560
on a graph,
you defined it on a program and thereby 

1030
00:45:15,560 --> 00:45:19,230
have access to a much more expressive 
toolkit of knowledge representation.

1031
00:45:19,231 --> 00:45:24,231
So data structures,
other kinds of algorithmic tools for 

1032
00:45:24,231 --> 00:45:24,231
representing knowledge.
Okay.

1033
00:45:24,231 --> 00:45:27,510
But you still have access to the ability
to do probabilistic inference,

1034
00:45:27,540 --> 00:45:32,540
like in a graphical model,
but also causal inference in a directed 

1035
00:45:32,540 --> 00:45:36,531
graphical model.
So for those of you who know about 

1036
00:45:36,531 --> 00:45:36,531
graphical models,
that might make some sense to you,

1037
00:45:36,531 --> 00:45:37,980
but just more broadly what this is,
think of this as,

1038
00:45:37,981 --> 00:45:41,610
as a toolkit that allows us to combine 
several of the best ideas,

1039
00:45:41,760 --> 00:45:43,770
not just of the recent deep learning 
era,

1040
00:45:43,771 --> 00:45:48,771
but over if you look back over the whole
scope of ai and as well as cognitive 

1041
00:45:48,771 --> 00:45:52,371
science.
I think there's three or four ideas and 

1042
00:45:52,371 --> 00:45:56,091
more but definitely like three ideas we 
could really put up there that have 

1043
00:45:56,091 --> 00:45:59,571
proven their worth and have had that 
have risen and fallen in terms of each 

1044
00:45:59,571 --> 00:46:03,501
of these had ideas when the mainstream 
of the field thought this was totally 

1045
00:46:03,501 --> 00:46:06,651
the way to go and every other idea was 
was obviously a waste of time and also 

1046
00:46:06,651 --> 00:46:08,700
had its time when many people thought it
was a waste of time.

1047
00:46:08,820 --> 00:46:10,440
Okay.
And these three big ideas,

1048
00:46:10,441 --> 00:46:11,550
I would say our.
First of all,

1049
00:46:11,551 --> 00:46:16,551
the idea of symbolic representation or 
symbolic languages for knowledge 

1050
00:46:16,551 --> 00:46:20,421
representation,
probabilistic inference in generative 

1051
00:46:20,421 --> 00:46:20,820
models to capture uncertainty,
ambiguity,

1052
00:46:20,970 --> 00:46:23,880
learning from sparse data and in their 
hierarchical setting,

1053
00:46:24,030 --> 00:46:25,800
learning to learn,
right?

1054
00:46:25,950 --> 00:46:30,950
And then of course the recent 
developments with neuro inspired 

1055
00:46:30,950 --> 00:46:32,280
architectures for pattern recognition.
Okay?

1056
00:46:32,490 --> 00:46:34,350
Each of these things,
each of these ideas,

1057
00:46:34,590 --> 00:46:39,590
symbolic languages,
probabilistic inference and neural 

1058
00:46:39,590 --> 00:46:42,171
networks have some distinctive strengths
that are real weak points of the other 

1059
00:46:42,171 --> 00:46:42,171
approaches,
right?

1060
00:46:42,171 --> 00:46:44,130
So to take one example that I haven't 
really talked about here,

1061
00:46:44,390 --> 00:46:46,320
um,
people in the butt but you,

1062
00:46:46,360 --> 00:46:49,200
but you mentioned as an outstanding 
challenge for neural networks,

1063
00:46:49,350 --> 00:46:54,350
transfer learning or learning to take 
knowledge across a number of previous 

1064
00:46:54,350 --> 00:46:58,071
tasks to transfer to others.
This is a real challenge and has always 

1065
00:46:58,071 --> 00:46:58,071
been a challenge in a neural net.

1066
00:46:58,071 --> 00:47:02,270
Okay.
But if something that's addressed very 

1067
00:47:02,270 --> 00:47:02,270
naturally and very scalably in,
for example,

1068
00:47:02,270 --> 00:47:06,801
a hierarchical basie and model,
and if you look at some of the recent 

1069
00:47:06,801 --> 00:47:09,921
attempts,
really interesting attempts within the 

1070
00:47:09,921 --> 00:47:11,991
deep learning world to try to get kinds 
of transfer learning and learning to 

1071
00:47:11,991 --> 00:47:11,991
learn,
they're really cool.

1072
00:47:11,991 --> 00:47:16,011
Okay.
But many of them are in some ways kind 

1073
00:47:16,011 --> 00:47:17,910
of reinventing within a neural network 
paradigm ideas that people,

1074
00:47:17,940 --> 00:47:19,740
you know,
maybe just 10 or 15 years ago,

1075
00:47:19,920 --> 00:47:23,590
developed in very sophisticated ways in 
let's say hierarchical Bayesian models.

1076
00:47:23,720 --> 00:47:28,720
Okay.
And a lot of attempts to get sort of 

1077
00:47:28,720 --> 00:47:30,400
symbolic an algorithm like behavior in 
neural networks are really,

1078
00:47:30,990 --> 00:47:35,990
they're very small steps towards 
something which is a very mature 

1079
00:47:35,990 --> 00:47:37,450
technology in computer systems and 
programming languages,

1080
00:47:38,110 --> 00:47:41,800
probabilistic programs,
I'll just sort of advertise mostly are a

1081
00:47:41,801 --> 00:47:46,801
way to combine the strengths of all of 
these approaches to have knowledge 

1082
00:47:46,801 --> 00:47:48,790
representations which are as expressive 
as anything that anybody ever did in the

1083
00:47:48,791 --> 00:47:53,791
symbolic paradigm that are as flexible 
at dealing with uncertainty and sparse 

1084
00:47:53,791 --> 00:47:55,360
data as anything in the probabilistic 
paradigm.

1085
00:47:55,570 --> 00:47:59,650
But that also can support pattern 
recognition tools to be able to,

1086
00:47:59,680 --> 00:48:01,030
for example,
do very fast,

1087
00:48:01,031 --> 00:48:03,700
efficient inference in very complex 
scenarios.

1088
00:48:03,850 --> 00:48:05,420
And there's a number of,
probably that's the,

1089
00:48:05,450 --> 00:48:08,740
that's the kind of conceptual framework.
There's a number of actually implemented

1090
00:48:08,741 --> 00:48:09,880
tools.
Um,

1091
00:48:09,881 --> 00:48:14,881
I point to here on the slide a number of
public programming languages which you 

1092
00:48:14,881 --> 00:48:14,920
can go explore.
Um,

1093
00:48:15,130 --> 00:48:20,130
for example,
there's one that was developed in our 

1094
00:48:20,130 --> 00:48:20,130
group a few years ago,
almost 10 years ago now called church,

1095
00:48:20,130 --> 00:48:24,930
which was the antecedent of some of 
these other languages built on a 

1096
00:48:24,930 --> 00:48:27,511
functional programming course.
A church is a problematic programming 

1097
00:48:27,511 --> 00:48:28,660
language built on the lambda calculous 
or really enlist basically,

1098
00:48:29,020 --> 00:48:34,020
um,
but there are many other more modern 

1099
00:48:34,020 --> 00:48:37,140
tools,
especially if you are interested in 

1100
00:48:37,140 --> 00:48:37,140
neural networks.
There are tools like,

1101
00:48:37,140 --> 00:48:39,820
for example,
pyro or prob torch or base flow that try

1102
00:48:39,821 --> 00:48:42,760
to combine all these ideas in a or,
or for example,

1103
00:48:42,761 --> 00:48:47,761
jen here,
which is a project of the Kauffman 

1104
00:48:47,761 --> 00:48:47,761
singles problems,
the computing group.

1105
00:48:47,761 --> 00:48:51,790
Um,
these are all things which are just in 

1106
00:48:51,790 --> 00:48:51,790
the very beginning stages,
very,

1107
00:48:51,790 --> 00:48:56,091
very alpha.
You can find out more about them online 

1108
00:48:56,091 --> 00:48:56,650
or by writing to their creators.
And I think this is a,

1109
00:48:56,710 --> 00:49:01,710
this is a very exciting place where the 
convergence of a number of different ai 

1110
00:49:01,710 --> 00:49:06,181
tools are happening and when,
and this will be absolutely necessary 

1111
00:49:06,181 --> 00:49:08,440
for making the kind of architecture that
I'm talking about work.

1112
00:49:09,070 --> 00:49:11,770
Another key idea which we've been 
building on in our lab.

1113
00:49:12,400 --> 00:49:13,630
Um,
and I think again,

1114
00:49:13,631 --> 00:49:15,550
many people are using some version of 
this idea,

1115
00:49:15,551 --> 00:49:20,551
but maybe a little bit different from 
the way we're doing it is what will the 

1116
00:49:20,551 --> 00:49:25,050
version of this idea that I like to talk
about is what I call the game engine in 

1117
00:49:25,050 --> 00:49:28,351
the head.
So this is the idea that it's really 

1118
00:49:28,351 --> 00:49:31,741
what the programs are about.
When I talk about problems with 

1119
00:49:31,741 --> 00:49:33,931
programs,
I haven't said anything about what kind 

1120
00:49:33,931 --> 00:49:33,931
of programs we're using.

1121
00:49:33,931 --> 00:49:35,830
We're just basically these programming 
languages at their best and church.

1122
00:49:35,831 --> 00:49:40,831
The language that was developed by Noah 
Goodman and mccosh and others and Dan 

1123
00:49:40,831 --> 00:49:45,210
Roy in our group some 10 years ago was 
intended to be a turing complete 

1124
00:49:45,210 --> 00:49:49,140
probabilistic programming language.
So any probability model that was 

1125
00:49:49,140 --> 00:49:51,310
computable or for who's inferences 
conditional inferences are computable.

1126
00:49:51,311 --> 00:49:55,090
You could represent in these languages,
but that leaves completely open.

1127
00:49:55,180 --> 00:49:56,620
What,
what I'm actually going to,

1128
00:49:56,660 --> 00:49:59,680
what kind of protocol I'm going to write
to model the world.

1129
00:49:59,980 --> 00:50:04,060
And I've been very inspired in the last 
few years by thinking about the kinds of

1130
00:50:04,061 --> 00:50:06,670
programs that are in modern video game 
engines.

1131
00:50:06,880 --> 00:50:07,840
So again,
I'm probably,

1132
00:50:07,841 --> 00:50:12,841
most of you are familiar with these,
but if you're an increasingly they are 

1133
00:50:12,841 --> 00:50:12,841
playing a role in all sorts of ways in 
ai,

1134
00:50:12,841 --> 00:50:17,581
but these are tools that were developed 
by the video game industry to allow a 

1135
00:50:17,581 --> 00:50:20,770
game designer to make a new game with,
without having to do most of,

1136
00:50:20,820 --> 00:50:23,770
in some sense many it must have the hard
technical work by from scratch,

1137
00:50:23,980 --> 00:50:26,950
but rather to focus on the characters,
the world,

1138
00:50:27,020 --> 00:50:28,280
the story,
okay.

1139
00:50:28,280 --> 00:50:29,820
The things that are more interesting 
for,

1140
00:50:30,040 --> 00:50:32,680
for designing a novel game in 
particular,

1141
00:50:32,681 --> 00:50:37,681
we,
if we want the player to explore some 

1142
00:50:37,681 --> 00:50:40,390
new three dimensional world,
but to have them be able to interact 

1143
00:50:40,390 --> 00:50:42,470
with the world in real time and to 
render nice looking graphics in,

1144
00:50:42,550 --> 00:50:47,550
in real time,
in an interactive way as the player 

1145
00:50:47,550 --> 00:50:49,841
moves around and explores the world or 
if you want to populate the world with 

1146
00:50:49,841 --> 00:50:51,470
non player characters that will behave 
in a even vaguely intelligent way.

1147
00:50:51,800 --> 00:50:56,800
Okay.
Game engines give you tools for doing 

1148
00:50:56,800 --> 00:50:58,841
all of this without having to write all 
of graphics from scratch or all of 

1149
00:50:58,841 --> 00:51:00,110
physics.
The rules of physics from scratch.

1150
00:51:00,470 --> 00:51:05,470
Um,
so what are called game physics engines 

1151
00:51:05,470 --> 00:51:06,710
and in some sense are a set of 
principles but also hacks from newtonian

1152
00:51:06,711 --> 00:51:11,711
mechanics and other areas of physics 
that allow you to simulate plausible 

1153
00:51:11,711 --> 00:51:13,280
looking physical interactions in very 
complex world,

1154
00:51:14,000 --> 00:51:15,770
very approximately,
but very fast.

1155
00:51:16,370 --> 00:51:19,400
There's also what's called [inaudible],
which are basically very simple planning

1156
00:51:19,401 --> 00:51:21,920
models.
So let's say I want to have an ai in the

1157
00:51:21,921 --> 00:51:26,921
game that is like a guard that gardens 
have base and a player is going to 

1158
00:51:26,921 --> 00:51:28,040
attack the space.
So back in the old Atari days,

1159
00:51:28,041 --> 00:51:29,510
like when I was a kid,
you know,

1160
00:51:29,600 --> 00:51:34,600
the guards would just be like random 
things that would fire missiles kind of 

1161
00:51:34,600 --> 00:51:34,940
randomly in random directions at random 
times,

1162
00:51:34,970 --> 00:51:39,970
right?
But let's say you want a garden to be a 

1163
00:51:39,970 --> 00:51:39,970
little intelligent,
so to actually look around and Oh,

1164
00:51:39,970 --> 00:51:44,261
and I see the player and then to 
actually start shooting at you and to 

1165
00:51:44,261 --> 00:51:46,871
even maybe pursue you.
So that requires putting a little ai in 

1166
00:51:46,871 --> 00:51:50,111
the game.
And you do that by having basically 

1167
00:51:50,111 --> 00:51:50,900
simple agent models in the game.
So what we think,

1168
00:51:51,140 --> 00:51:56,140
and some of you might think this is 
crazy and some of you might think this 

1169
00:51:56,140 --> 00:51:56,660
is very natural idea,
I get both kinds of reactions.

1170
00:51:56,900 --> 00:51:59,830
What we think is that these tools have,
you know,

1171
00:51:59,870 --> 00:52:04,870
fast,
approximate renderers physics engines 

1172
00:52:04,870 --> 00:52:07,091
and sort of very simple kinds of ai 
planning are an interesting first 

1173
00:52:07,091 --> 00:52:10,781
approximation to the kinds of common 
sense knowledge representations that 

1174
00:52:10,781 --> 00:52:13,850
evolution has built into our brains.
So when we talk about the cognitive core

1175
00:52:14,210 --> 00:52:16,460
or how do babies start,
what's,

1176
00:52:16,610 --> 00:52:21,610
you know,
ways in which a baby's brain isn't a 

1177
00:52:21,610 --> 00:52:23,981
blank slate.
One interesting idea is that it starts 

1178
00:52:23,981 --> 00:52:26,861
with something like these tools and then
wrapped inside a framework for 

1179
00:52:26,861 --> 00:52:31,271
probabilistic inference.
That's what we mean by promising 

1180
00:52:31,271 --> 00:52:33,191
programs that can support many 
activities of common sense perception 

1181
00:52:33,191 --> 00:52:35,030
and thinking.
So I'll just give you one example,

1182
00:52:35,540 --> 00:52:37,460
what we call this intuitive physics 
engine.

1183
00:52:37,730 --> 00:52:42,730
Okay.
So this is work that we did in our 

1184
00:52:42,730 --> 00:52:42,730
groups that Pete,
Natalia,

1185
00:52:42,730 --> 00:52:46,031
and Jess Hamrick did started this work 
about five years ago now where we showed

1186
00:52:47,001 --> 00:52:52,001
people in some sense this is also an 
illustration of a kind of experiment 

1187
00:52:52,001 --> 00:52:52,001
that you might do.

1188
00:52:52,340 --> 00:52:57,340
I keep talking about science,
like I'll show you now a couple of 

1189
00:52:57,340 --> 00:52:57,340
experiments,
right?

1190
00:52:57,340 --> 00:52:59,570
So we would show people simple physical 
scenes like these blocks worlds scenes,

1191
00:52:59,571 --> 00:53:01,340
and asked them to make a number of 
judgments.

1192
00:53:01,610 --> 00:53:06,610
And the model we built does it basically
a little bit of probabilistic inference 

1193
00:53:06,610 --> 00:53:10,301
in a game style physics engine.
It perceives the physical state and 

1194
00:53:10,301 --> 00:53:14,320
imagines a few different possible ways 
the world could go over the next one or 

1195
00:53:14,320 --> 00:53:16,580
two seconds to answer questions like 
will the stack of blocks fall?

1196
00:53:16,790 --> 00:53:18,590
Or if they fall,
how far will they fall?

1197
00:53:18,650 --> 00:53:23,650
Or which way will they fall?
Or what would happen if say one of the 

1198
00:53:23,650 --> 00:53:27,310
colored one color of blocks or one 
material like the green stuff is 10 

1199
00:53:27,310 --> 00:53:28,140
times heavier than the gray stuff or 
vice versa.

1200
00:53:28,170 --> 00:53:33,170
How will that change the direction of 
fall or look at those red and yellow 

1201
00:53:33,170 --> 00:53:36,411
stack blocks,
some of which look like they should be 

1202
00:53:36,411 --> 00:53:36,411
falling but aren't.

1203
00:53:36,411 --> 00:53:40,010
So why can you infer from the fact that 
they are not fall in that one color 

1204
00:53:40,010 --> 00:53:44,781
block is much heavier than the other.
Or let me show you a sort of a slightly 

1205
00:53:44,781 --> 00:53:49,101
weird task.
It's an like other behavioral 

1206
00:53:49,101 --> 00:53:51,681
experiments.
Sometimes we do weird things so that we 

1207
00:53:51,681 --> 00:53:52,800
can test ways in which you use your 
knowledge that you didn't just,

1208
00:53:52,930 --> 00:53:54,750
you know,
learn from pattern recognition,

1209
00:53:54,900 --> 00:53:58,230
but use it to do new kinds of tasks that
you'd never seen before.

1210
00:53:58,470 --> 00:54:02,340
So here's a task which many of you have 
maybe seen me talk about these things.

1211
00:54:02,341 --> 00:54:07,341
So you might have seen this task,
but probably only if you saw me give a 

1212
00:54:07,341 --> 00:54:07,341
talk around here before we call this the
red,

1213
00:54:07,341 --> 00:54:12,201
yellow task.
And again we'll make this one 

1214
00:54:12,201 --> 00:54:13,731
interactive.
So imagine that the blocks on the table 

1215
00:54:13,731 --> 00:54:15,030
or knocked hard enough to bump the 
tables,

1216
00:54:15,040 --> 00:54:17,890
bumped hard enough to knock some of the 
blocks onto the floor.

1217
00:54:18,160 --> 00:54:23,160
So you tell me,
is it more likely to be red blocks or 

1218
00:54:23,160 --> 00:54:23,160
yellow blocks?
What do you say?

1219
00:54:23,160 --> 00:54:23,160
Red.
Okay,

1220
00:54:23,160 --> 00:54:24,940
good.
How about here?

1221
00:54:25,300 --> 00:54:25,390
Yeah,

1222
00:54:27,140 --> 00:54:27,770
yellow.
Good.

1223
00:54:27,800 --> 00:54:30,190
How about here?
Uh Huh.

1224
00:54:30,640 --> 00:54:33,250
Here,
here.

1225
00:54:34,890 --> 00:54:38,850
Okay.
Here.

1226
00:54:40,700 --> 00:54:40,950
Here.

1227
00:54:42,390 --> 00:54:43,130
Okay.
So,

1228
00:54:43,270 --> 00:54:48,270
so you just experienced for yourself 
what it's like to be a subject and one 

1229
00:54:48,270 --> 00:54:48,270
of these expenses.
We just did the experiment here.

1230
00:54:48,270 --> 00:54:50,100
The data's is all captured on video,
sort of.

1231
00:54:50,610 --> 00:54:54,240
Okay.
You could see that sometimes people were

1232
00:54:54,241 --> 00:54:55,950
very quick.
Other times people were slower,

1233
00:54:56,010 --> 00:55:01,010
sometimes there was a lot of consensus,
sometimes there was a little bit less 

1234
00:55:01,010 --> 00:55:01,010
consensus.
Right?

1235
00:55:01,010 --> 00:55:01,860
That reflects uncertainty.
So again,

1236
00:55:01,861 --> 00:55:04,320
there's a long history of studying this 
scientifically,

1237
00:55:04,680 --> 00:55:05,760
um,
that,

1238
00:55:05,790 --> 00:55:06,270
you know,
you could,

1239
00:55:06,300 --> 00:55:11,300
but you can see some,
you can see the probabilistic inference 

1240
00:55:11,300 --> 00:55:11,700
at work.
Probabilistic inference over what?

1241
00:55:11,850 --> 00:55:16,850
Well,
I would say one way to describe it is 

1242
00:55:16,850 --> 00:55:18,330
over a one or a few short,
low precision simulations of the physics

1243
00:55:18,331 --> 00:55:20,550
of these scenes.
So here is what I mean by this.

1244
00:55:20,850 --> 00:55:25,850
I'm going to show you a video of a game 
engine reconstruction of one of these 

1245
00:55:25,850 --> 00:55:26,440
scenes that simulates a small.

1246
00:55:26,810 --> 00:55:30,110
So here's a small bump,
here's that same scene with a big bump.

1247
00:55:30,170 --> 00:55:32,780
Okay,
now notice that at the micro level,

1248
00:55:32,810 --> 00:55:36,280
different things happen,
but at the cognitive or macro level that

1249
00:55:36,281 --> 00:55:38,500
matters for common sense reasoning,
the same thing happen,

1250
00:55:38,590 --> 00:55:43,590
namely all the yellow blocks went over 
onto one side of the table and few or 

1251
00:55:43,590 --> 00:55:47,041
none of the red blocks did so it didn't 
matter which of those simulations you 

1252
00:55:47,041 --> 00:55:47,470
run in your head,
you'd get the same answer in this case,

1253
00:55:47,471 --> 00:55:52,471
right?
This is one that's very easy and high 

1254
00:55:52,471 --> 00:55:52,471
confidence.
And also you,

1255
00:55:52,471 --> 00:55:52,990
you didn't have to run the simulation 
for very long.

1256
00:55:52,991 --> 00:55:57,991
You only have to run it for a few times.
Steps like that to see what's going to 

1257
00:55:57,991 --> 00:55:57,991
happen.
Or similarly here,

1258
00:55:57,991 --> 00:55:58,690
you only have to run it for a few times.
Steps,

1259
00:55:58,750 --> 00:56:03,750
okay?
And it doesn't have to be even very 

1260
00:56:03,750 --> 00:56:03,750
accurate.

1261
00:56:03,750 --> 00:56:06,210
Even a fair amount of imprecision will 
give you basically the same answer at 

1262
00:56:06,210 --> 00:56:09,931
the level that matters for common sense.
So that's the kind of thing our model 

1263
00:56:09,931 --> 00:56:13,111
does.
It runs a few low precision simulations 

1264
00:56:13,111 --> 00:56:15,721
for a few times steps.
But if you take the average of what 

1265
00:56:15,721 --> 00:56:15,760
happens there and you compare that with 
people's judgments,

1266
00:56:15,761 --> 00:56:17,680
you get results like what I show you 
here,

1267
00:56:17,730 --> 00:56:22,210
the scatter plot shows on the y axis,
the average judgments of people on the x

1268
00:56:22,211 --> 00:56:23,860
axis,
the average of this model,

1269
00:56:23,861 --> 00:56:25,690
and it does a pretty good job.
It's not perfect,

1270
00:56:25,870 --> 00:56:30,870
but the model basically captures peoples
graded sense of what's going on in this 

1271
00:56:30,870 --> 00:56:31,780
scene and many of these others,
okay.

1272
00:56:33,330 --> 00:56:36,130
And it doesn't do it with any learning,
but I'll come back to that in a second.

1273
00:56:36,160 --> 00:56:41,160
It just does it by probabilistic 
reasoning over a game physics 

1274
00:56:41,160 --> 00:56:42,220
simulation.
Now we can use and we have used the same

1275
00:56:42,221 --> 00:56:45,360
kind of technology to capture in very 
simple form.

1276
00:56:45,370 --> 00:56:47,440
It's really just proofs of concept at 
this point,

1277
00:56:47,680 --> 00:56:52,680
but kind of common sense physical scene 
understanding in child and a child 

1278
00:56:52,680 --> 00:56:55,120
playing with blocks or other objects or 
in what might go on in a young child,

1279
00:56:55,121 --> 00:57:00,121
understanding of other people's actions,
what we call the intuitive psychology 

1280
00:57:00,121 --> 00:57:00,121
engine.
We're now,

1281
00:57:00,121 --> 00:57:04,830
the programs are defined over these kind
of very simple planning and perception 

1282
00:57:04,830 --> 00:57:05,890
programs and I won't go into any 
details.

1283
00:57:05,891 --> 00:57:10,891
I'll just point to a couple of papers 
that my group played a very small role 

1284
00:57:10,891 --> 00:57:14,521
in,
but we provided some models which 

1285
00:57:14,521 --> 00:57:16,411
together with some infant researchers,
people working on both of these are 

1286
00:57:16,411 --> 00:57:18,250
experiments that were done with 10 or 12
month infant,

1287
00:57:18,251 --> 00:57:20,920
so younger than even some of the babies 
I showed you before,

1288
00:57:20,921 --> 00:57:23,710
but basically like that youngest baby,
the one with the cat.

1289
00:57:24,400 --> 00:57:27,160
Here's an example of showing simple 
physical scenes.

1290
00:57:27,161 --> 00:57:32,161
These are moving objects to 12 month 
olds where they saw a few objects 

1291
00:57:32,171 --> 00:57:37,171
bouncing around inside a gumball machine
and after some point in time the scene 

1292
00:57:37,171 --> 00:57:37,171
gets occluded.

1293
00:57:37,171 --> 00:57:39,520
You'll see the scene as occluded and 
then after another period of time,

1294
00:57:39,521 --> 00:57:42,760
but one of the objects will appear at 
the bottom and the question is,

1295
00:57:42,761 --> 00:57:46,360
is that the audit you expected to see or
not is it's expected or surprising?

1296
00:57:46,600 --> 00:57:51,600
The standard way you study what infants 
know is by is by what's called looking 

1297
00:57:51,600 --> 00:57:51,700
time methods.
Just like an adult.

1298
00:57:51,880 --> 00:57:53,320
If I show you something that's 
surprising,

1299
00:57:53,321 --> 00:57:54,700
you might look longer.
Okay.

1300
00:57:54,850 --> 00:57:56,140
If you're bored,
you'll look away.

1301
00:57:56,170 --> 00:57:57,130
All right?
Um,

1302
00:57:57,850 --> 00:58:02,850
so you can do that same kind of thing 
with infants and by measuring how long 

1303
00:58:02,850 --> 00:58:06,991
they look at a scene,
you can measure whether you've shown 

1304
00:58:06,991 --> 00:58:06,991
them something surprising or not.
All right?

1305
00:58:06,991 --> 00:58:08,470
People.
There are literally hundreds of studies,

1306
00:58:08,471 --> 00:58:13,471
if not more,
using looking time measures to study 

1307
00:58:13,471 --> 00:58:16,681
what infants know,
but only with this paper that we 

1308
00:58:16,681 --> 00:58:17,920
published a few years ago did we have a 
quantitative model,

1309
00:58:17,921 --> 00:58:21,490
but we're able to show a relation 
between inverse probability in this case

1310
00:58:21,491 --> 00:58:26,491
and surprise,
so things which were objectively lower 

1311
00:58:26,491 --> 00:58:26,980
probability under one of these 
probabilistic physics simulations across

1312
00:58:26,981 --> 00:58:30,300
a number of different manipulations of 
how fast the objects were,

1313
00:58:30,301 --> 00:58:31,840
where they were when the scene was 
occluded,

1314
00:58:31,841 --> 00:58:34,360
how long the delay was,
various physically relevant variables,

1315
00:58:34,510 --> 00:58:39,510
how many objects that were of one type 
or another infant's expectations 

1316
00:58:39,510 --> 00:58:42,480
connected with this model or another 
paper that we published that one was,

1317
00:58:42,580 --> 00:58:43,120
was done.

1318
00:58:43,490 --> 00:58:46,810
The experiments that were done by 
Aaronow tagless and Luca Menotti's lab.

1319
00:58:47,170 --> 00:58:50,790
Here is a study that was done just 
recently by Sheri Lou enlists,

1320
00:58:50,791 --> 00:58:52,510
spelled [inaudible] lab at there at 
Harvard,

1321
00:58:52,511 --> 00:58:54,670
but they're part,
they're partners with us in CBMM,

1322
00:58:55,030 --> 00:58:56,980
which was about infants understanding of
goals,

1323
00:58:56,981 --> 00:59:01,981
so this is more like again,
understanding of agents and intuitive 

1324
00:59:01,981 --> 00:59:01,981
psychology.
We're in,

1325
00:59:01,981 --> 00:59:02,800
again,
in very simple cartoon scenes,

1326
00:59:03,430 --> 00:59:08,430
you show an infant and agent that seems 
to be doing something like an animated 

1327
00:59:08,430 --> 00:59:12,031
cartoon character,
but it jumps over a wall or it rolls up 

1328
00:59:12,031 --> 00:59:15,481
a hill or it jumps over a gap and the 
question is basically how much does the 

1329
00:59:15,491 --> 00:59:20,491
agent want the goal that it seems to be 
trying to achieve and what this study 

1330
00:59:20,491 --> 00:59:20,491
showed.
Okay,

1331
00:59:20,491 --> 00:59:25,110
and the models here we're done by Tomer 
omen was that infants appeared to be 

1332
00:59:25,110 --> 00:59:26,780
sensitive to the physical work done by 
the agent.

1333
00:59:26,990 --> 00:59:31,990
The more work the agent did in the sense
of the integral of force applied over a 

1334
00:59:31,990 --> 00:59:36,641
path,
the more the the infant's thought the 

1335
00:59:36,641 --> 00:59:39,881
agent wanted the goal.
We think of this as representing what 

1336
00:59:39,881 --> 00:59:40,970
we've sometimes called the naive utility
calculus.

1337
00:59:41,180 --> 00:59:43,530
So the idea that there's a basic 
calculus of,

1338
00:59:43,760 --> 00:59:45,990
of costs and benefits,
as you know,

1339
00:59:46,040 --> 00:59:49,100
we take actions which are a little bit 
costly to achieve goal states,

1340
00:59:49,101 --> 00:59:51,920
which gives us some reward that's the 
most basic way,

1341
00:59:51,921 --> 00:59:56,921
the oldest way to think about rational,
intentional action and it seems that 

1342
00:59:56,921 --> 01:00:00,401
even 10 month olds understand some 
version of that where the cost can be 

1343
01:00:00,401 --> 01:00:01,220
measured in physical terms.
Okay.

1344
01:00:01,580 --> 01:00:03,260
Um,
I see I'm running a little bit behind on

1345
01:00:03,261 --> 01:00:04,130
time and,
and,

1346
01:00:04,430 --> 01:00:09,430
uh,
I wanted to leave some time for 

1347
01:00:09,430 --> 01:00:10,601
discussion so I'll,
I'll just go very quickly through a 

1348
01:00:10,601 --> 01:00:11,960
couple of other things and I'm happy to 
stay around at the end for discussion.

1349
01:00:11,990 --> 01:00:13,090
Okay.
Um,

1350
01:00:13,870 --> 01:00:16,400
the,
what I showed you here was the science.

1351
01:00:16,401 --> 01:00:18,380
Where does the engineering go?
So one way,

1352
01:00:18,381 --> 01:00:22,630
one thing you can do with this is say 
build a machine system that can look not

1353
01:00:22,640 --> 01:00:25,280
a little animated cartoon like these 
baby experiments,

1354
01:00:25,370 --> 01:00:27,350
but a real person doing something and 
again,

1355
01:00:27,440 --> 01:00:31,370
combined physical cough and constraints 
of actions with some understanding of,

1356
01:00:31,390 --> 01:00:32,720
of the agents,
utilities,

1357
01:00:32,721 --> 01:00:36,620
that's the math of planning.
Trying to figure out what they wanted.

1358
01:00:36,950 --> 01:00:41,950
So look in the scene here and see if you
can judge which object the woman is 

1359
01:00:41,950 --> 01:00:43,370
reaching for.
So you can see there's,

1360
01:00:43,970 --> 01:00:44,460
there's,
um,

1361
01:00:44,510 --> 01:00:49,510
a grid of four by four objects.
There's 16 objects here and she's going 

1362
01:00:49,510 --> 01:00:50,990
to be reaching for one of them raised.
It's going to play in slow motion,

1363
01:00:50,991 --> 01:00:53,390
but raise your hand when you know which 
ones he's reaching for.

1364
01:00:53,450 --> 01:00:58,450
Okay?
So just watch and raise your hand when 

1365
01:00:58,450 --> 01:00:58,450
you know which one she wants.

1366
01:01:00,550 --> 01:01:02,060
Okay,
so most of the hands are up by now.

1367
01:01:02,110 --> 01:01:04,370
All right.
And notice I was looking at your hands.

1368
01:01:04,371 --> 01:01:05,120
Not here,
but when.

1369
01:01:05,150 --> 01:01:10,150
But what happened is most of the hands 
were up at about the time when that 

1370
01:01:10,150 --> 01:01:11,810
great or the one that dash line shot up.
Okay.

1371
01:01:12,050 --> 01:01:14,750
That's not human data.
You provided the data.

1372
01:01:14,751 --> 01:01:19,751
This is our model.
So our model is predicting more or less 

1373
01:01:19,751 --> 01:01:19,790
when you're able to say what her goal 
was.

1374
01:01:19,791 --> 01:01:24,791
Okay.
It's well before she actually touched 

1375
01:01:24,791 --> 01:01:24,791
the object.
How does the model work?

1376
01:01:24,791 --> 01:01:24,791
Again,
I'll skip the details,

1377
01:01:24,791 --> 01:01:29,590
but it does the same kind of thing that,
that are models of those infants did 

1378
01:01:29,590 --> 01:01:30,230
namely get it.
But in this case,

1379
01:01:30,231 --> 01:01:32,360
it does it with a full body model from 
robotics,

1380
01:01:32,361 --> 01:01:34,490
so we use what's called the physics 
engine,

1381
01:01:35,060 --> 01:01:37,640
which is a standard tool in robotics for
planning,

1382
01:01:37,641 --> 01:01:40,130
physically efficient reaches of say,
a humanoid robot,

1383
01:01:40,340 --> 01:01:45,260
and we say we can give this planner 
program a goal object as input.

1384
01:01:45,260 --> 01:01:47,720
We can give each of the possible goal.
Optics is input and say,

1385
01:01:47,840 --> 01:01:49,760
plan the most physically efficient 
action,

1386
01:01:49,761 --> 01:01:52,670
so the one that uses like the least 
energy to get to that object,

1387
01:01:52,880 --> 01:01:57,880
and then we can do a Bayesian inference.
This is the probabilistic inference 

1388
01:01:57,880 --> 01:01:58,680
part.
The program is the planner.

1389
01:01:58,730 --> 01:02:00,170
Okay,
but then we can say,

1390
01:02:00,860 --> 01:02:04,010
I want to do basie and inference to work
backwards from what I observed,

1391
01:02:04,011 --> 01:02:06,380
which was the action to the input to 
that program.

1392
01:02:06,381 --> 01:02:08,780
What goal was provided as input to the 
planner,

1393
01:02:08,900 --> 01:02:13,900
and here you can see the full array of 
four by four possible inputs and those 

1394
01:02:13,900 --> 01:02:18,251
bars that are moving up and down.
That's the basie and posterior 

1395
01:02:18,251 --> 01:02:20,741
probability of how likely each of those 
was to be the goal and what you can see 

1396
01:02:20,741 --> 01:02:22,230
as it converges on the right answer at 
least.

1397
01:02:22,260 --> 01:02:27,260
Well,
it turns out to be the ground truth 

1398
01:02:27,260 --> 01:02:27,260
right answer,
but it's also the right answer according

1399
01:02:27,260 --> 01:02:28,770
to what people think with about the same
kind of data that people took.

1400
01:02:30,060 --> 01:02:30,960
Now you might say,
well,

1401
01:02:30,961 --> 01:02:35,961
okay,
I'm sure if I just wanted to build a 

1402
01:02:35,961 --> 01:02:35,961
system that could detect what somebody 
was reaching for,

1403
01:02:35,961 --> 01:02:39,951
I could generate a training data set of 
this sort of scene and train something 

1404
01:02:39,951 --> 01:02:41,790
up to analyze patterns of motion,
but again,

1405
01:02:41,791 --> 01:02:44,340
because the engine in your head actually
does something,

1406
01:02:44,341 --> 01:02:49,341
we think more like this,
it does what we call inverse planning 

1407
01:02:49,341 --> 01:02:52,341
over a physics model.
It can apply to much more interesting 

1408
01:02:52,341 --> 01:02:52,440
scenes that you haven't really seen much
of before.

1409
01:02:52,530 --> 01:02:54,780
So take the seat on the left,
right where again,

1410
01:02:54,781 --> 01:02:57,900
you see somebody reaching for one of a 
four by four array of objects,

1411
01:02:58,170 --> 01:02:59,910
but what you see as a strange kind of 
reach,

1412
01:02:59,911 --> 01:03:03,450
can you see why he's doing this?
Strange reach up there?

1413
01:03:03,451 --> 01:03:05,070
It's a little small,
but what is he?

1414
01:03:05,071 --> 01:03:07,260
You can see that he's reaching over 
something,

1415
01:03:07,261 --> 01:03:07,530
right?

1416
01:03:07,530 --> 01:03:09,180
It's actually a pane of glass,
right?

1417
01:03:09,190 --> 01:03:12,570
You see that and then there's this other
guy who's helping him,

1418
01:03:13,320 --> 01:03:15,990
who sees what he wants and hands him the
thing he wants.

1419
01:03:16,230 --> 01:03:21,230
So how does the,
for the guy in the foreground see the 

1420
01:03:21,230 --> 01:03:22,500
other guy's goal,
how does he in for his goal and know how

1421
01:03:22,501 --> 01:03:27,501
to help him and then how do we look at 
the two of them and figure out who's 

1422
01:03:27,501 --> 01:03:30,471
trying to help who or that in a scene 
like this one here that it's not 

1423
01:03:30,471 --> 01:03:31,620
somebody trying to help somebody but 
rather the opposite.

1424
01:03:31,860 --> 01:03:36,860
Okay,
so here's a model on the left of how 

1425
01:03:36,860 --> 01:03:36,860
that might work.
Right?

1426
01:03:36,860 --> 01:03:40,251
And we think this is the kind of model 
needed to tackle this sort of challenge 

1427
01:03:40,251 --> 01:03:40,251
here,
right?

1428
01:03:40,251 --> 01:03:41,940
Basically it's a model.
We take this model of,

1429
01:03:42,000 --> 01:03:47,000
of planning,
sort of maximal expected utility 

1430
01:03:47,000 --> 01:03:47,000
planning,
which you can run backwards,

1431
01:03:47,000 --> 01:03:48,960
but then we recursively nest these 
models inside each other.

1432
01:03:48,960 --> 01:03:53,520
So we say an agent is helping another 
agent if this agent is acting apparently

1433
01:03:53,521 --> 01:03:58,521
to us,
seems to be maximizing unexpected 

1434
01:03:58,521 --> 01:04:00,710
utility.
That's a positive function of that 

1435
01:04:00,710 --> 01:04:03,981
agent's expectation about another 
agent's expecting to tilly and that's 

1436
01:04:03,981 --> 01:04:05,550
what it means to be a helper.
Hindering is sort of the opposite if one

1437
01:04:05,551 --> 01:04:08,130
seems to be trying to lower somebody's 
else's utility.

1438
01:04:08,270 --> 01:04:10,980
Okay,
and we've used these same kind of models

1439
01:04:11,160 --> 01:04:16,160
to also describe infants understanding 
of helping and hindering and a range of 

1440
01:04:16,160 --> 01:04:19,551
scenes.
I'll just say one last word about 

1441
01:04:19,551 --> 01:04:21,330
learning because everybody wants to know
about learning and and the the key thing

1442
01:04:21,331 --> 01:04:21,950
here,
and it's.

1443
01:04:21,980 --> 01:04:24,480
It's definitely part of any picture of 
Agi,

1444
01:04:24,840 --> 01:04:28,020
but the thought I want to leave you on 
is really about what learning is about.

1445
01:04:28,021 --> 01:04:33,021
Okay.
It will be just a few more slides and 

1446
01:04:33,021 --> 01:04:34,311
then I'll stop.
I promise none of the models I showed 

1447
01:04:34,311 --> 01:04:34,311
you so far really did any learning.

1448
01:04:34,311 --> 01:04:36,360
They certainly didn't do any task 
specific learning.

1449
01:04:36,420 --> 01:04:41,420
Okay.
We set up a provost at program and then 

1450
01:04:41,420 --> 01:04:43,551
we let it do inference.
Now that's not to say that we don't 

1451
01:04:43,551 --> 01:04:43,551
think people learn to do these things.
We do,

1452
01:04:43,551 --> 01:04:45,810
but the real learning goes on when 
you're much younger,

1453
01:04:45,960 --> 01:04:48,150
right?
Everything I showed you in basic form,

1454
01:04:48,151 --> 01:04:50,400
even a one year old baby can do.
Okay.

1455
01:04:50,580 --> 01:04:54,000
The basic learning goes on to support 
these kinds of abilities.

1456
01:04:54,001 --> 01:04:55,650
Not that there isn't learning beyond one
year,

1457
01:04:55,860 --> 01:05:00,860
but the basic way you learn to say solve
these physics problems is what goes on 

1458
01:05:00,860 --> 01:05:02,940
in your bay,
in the brain of a child between zero and

1459
01:05:02,941 --> 01:05:07,941
12 months.
So this is just an example of some 

1460
01:05:07,941 --> 01:05:08,610
phenomena that come from the literature 
on infant cognitive development.

1461
01:05:08,880 --> 01:05:13,880
These are very rough timelines.
You can take pictures of this if you 

1462
01:05:13,880 --> 01:05:16,671
like.
This is always a popular slide because 

1463
01:05:16,671 --> 01:05:16,671
it's.

1464
01:05:16,671 --> 01:05:16,671
It's.
It really is quite inspiring I think,

1465
01:05:16,671 --> 01:05:20,421
and I can give you lots of literature,
but I'm summarizing in very broad 

1466
01:05:20,421 --> 01:05:24,241
strokes with big error bars,
what we've learned in the field of 

1467
01:05:24,241 --> 01:05:27,931
infant cognitive development about when 
and how kids seemed to at least come to 

1468
01:05:27,941 --> 01:05:30,280
a certain understanding of basic aspects
of physics.

1469
01:05:30,850 --> 01:05:34,150
So if you really want to study how 
people learn to be intelligent,

1470
01:05:34,480 --> 01:05:36,700
a lot of what you have to study our kids
at this age,

1471
01:05:36,701 --> 01:05:40,330
you have to study what's already in 
their brain at zero months and what they

1472
01:05:40,331 --> 01:05:42,460
learn and how they learn between four,
six,

1473
01:05:42,461 --> 01:05:43,000
eight,
10,

1474
01:05:43,001 --> 01:05:45,070
12,
and so on and on and on up beyond that.

1475
01:05:45,071 --> 01:05:47,060
Okay.
Now,

1476
01:05:47,380 --> 01:05:52,380
well,
effectively what that amounts to we 

1477
01:05:52,380 --> 01:05:52,380
think is if what you're learning is 
something like,

1478
01:05:52,380 --> 01:05:57,270
let's say an intuitive game,
physics engine to capture these basic 

1479
01:05:57,270 --> 01:05:57,490
abilities than what we need.

1480
01:05:57,490 --> 01:05:58,990
If we're going to try to reverse 
engineer,

1481
01:05:58,991 --> 01:06:01,870
that is what we might think of as a 
program learning program.

1482
01:06:01,871 --> 01:06:03,730
If your knowledge is in the form of a 
program,

1483
01:06:04,030 --> 01:06:06,370
then you have to have programs that 
build other programs.

1484
01:06:06,371 --> 01:06:11,371
Right?
This is what I was talking about the 

1485
01:06:11,371 --> 01:06:11,470
beginning about learning as building 
models of the world are ultimately.

1486
01:06:11,590 --> 01:06:16,590
If you think what we start off with is 
something like a game engine that can 

1487
01:06:16,590 --> 01:06:20,131
play any game than what you have to 
learn is the program of the game that 

1488
01:06:20,131 --> 01:06:23,041
you're actually playing or the many 
different games that you might be 

1489
01:06:23,041 --> 01:06:23,800
playing over your life.
So think of learning as like programming

1490
01:06:23,801 --> 01:06:28,801
the game engine in your head to fit with
your experience and to fit with the 

1491
01:06:28,801 --> 01:06:29,710
possible actions that you seem like you 
can take.

1492
01:06:30,190 --> 01:06:35,190
Now,
this is what you could call the hard 

1493
01:06:35,190 --> 01:06:35,190
problem of learning if you come to 
learning from,

1494
01:06:35,190 --> 01:06:38,581
say,
neural networks or other tools in 

1495
01:06:38,581 --> 01:06:38,581
machine learning.

1496
01:06:38,581 --> 01:06:41,160
Right?
So what makes machine makes most of 

1497
01:06:41,160 --> 01:06:43,201
machine learning go right now and 
certainly what makes neural networks so 

1498
01:06:43,201 --> 01:06:45,961
appealing is that you can set up a 
basically a big function approximator 

1499
01:06:45,961 --> 01:06:50,101
that can approximate many of the 
functions you might want to do in a 

1500
01:06:50,101 --> 01:06:53,071
certain application or task,
but in a way that's end to end 

1501
01:06:53,071 --> 01:06:53,950
differentiable and with a meaningful 
cost function.

1502
01:06:53,951 --> 01:06:56,350
So you can have one of these nice 
optimization landscapes.

1503
01:06:56,351 --> 01:07:01,351
You can compute the gradients on 
basically just rolled down hill until 

1504
01:07:01,351 --> 01:07:04,081
you get to an optimal solution,
but if you're talking about learning as 

1505
01:07:04,081 --> 01:07:05,290
something like search in the space of 
programs,

1506
01:07:05,680 --> 01:07:07,480
we don't know how to do anything like 
that yet.

1507
01:07:07,481 --> 01:07:11,170
We don't know how to set this up as any 
kind of a nice optimization problem with

1508
01:07:11,171 --> 01:07:13,350
any notion of smoothness or gradients.
Okay.

1509
01:07:13,660 --> 01:07:15,220
Rather,
what we need is a,

1510
01:07:15,550 --> 01:07:17,530
instead of learning as like rolling down
hill,

1511
01:07:17,531 --> 01:07:21,100
effectively write a process which just,
if you're willing to wait long enough,

1512
01:07:21,220 --> 01:07:21,940
you know,
some,

1513
01:07:22,240 --> 01:07:23,120
uh,
you know,

1514
01:07:23,350 --> 01:07:28,350
simple algorithm will take care of,
think of what we call the idea of 

1515
01:07:28,350 --> 01:07:28,350
learning as programming.

1516
01:07:28,450 --> 01:07:33,450
There's a popular metaphor in cognitive 
development called the child as 

1517
01:07:33,450 --> 01:07:36,871
scientists,
which emphasizes children as active 

1518
01:07:36,871 --> 01:07:39,100
theory builders and children's play as a
kind of casual experimentation.

1519
01:07:39,520 --> 01:07:41,470
But this is the algorithmic compliment 
to that.

1520
01:07:41,471 --> 01:07:44,980
What we could call the child is coder or
around Mit will say the child is hacker,

1521
01:07:44,981 --> 01:07:47,560
but the rest of the world,
if you say child is hacker,

1522
01:07:47,561 --> 01:07:52,561
they think of something that someone who
breaks into your email and steals your 

1523
01:07:52,561 --> 01:07:52,561
credit card numbers.
We all know that hacking is,

1524
01:07:52,561 --> 01:07:54,040
you know,
making your code more awesome,

1525
01:07:54,730 --> 01:07:55,540
right?
If,

1526
01:07:55,600 --> 01:08:00,600
if,
if your knowledge is some kind of code 

1527
01:08:00,600 --> 01:08:02,730
or Lena Library of programs,
then learning is all the ways that a 

1528
01:08:02,730 --> 01:08:03,490
child acts on their code to make it more
awesome,

1529
01:08:03,990 --> 01:08:07,240
more awesome can mean more accurate,
but it can also mean faster,

1530
01:08:07,420 --> 01:08:10,900
more elegant,
more transportable to other applications

1531
01:08:10,901 --> 01:08:12,910
or their tasks more explainable to 
others.

1532
01:08:13,000 --> 01:08:14,880
Maybe just more entertaining.
Okay.

1533
01:08:14,930 --> 01:08:19,930
Children do.
All of them have all of those goals and 

1534
01:08:19,930 --> 01:08:20,330
learning and the activities by which 
they make their code more awesome.

1535
01:08:20,570 --> 01:08:23,870
Also correspond to many of the 
activities of coding,

1536
01:08:24,540 --> 01:08:29,540
right?
So think about all the ways on a day to 

1537
01:08:29,540 --> 01:08:29,540
day basis.
You might make your code more awesome.

1538
01:08:29,540 --> 01:08:29,620
Alright?
Um,

1539
01:08:30,020 --> 01:08:32,720
you might tune,
you might have a big library of existing

1540
01:08:32,721 --> 01:08:37,721
functions with some parameters that you 
can tune on a data set that's basically 

1541
01:08:37,721 --> 01:08:41,171
what you do with backdrop or stochastic 
gradient descent and training a deep 

1542
01:08:41,171 --> 01:08:44,531
learning system,
but think about all the ways in which 

1543
01:08:44,531 --> 01:08:44,531
you might actually modify the underlying
function.

1544
01:08:44,531 --> 01:08:46,740
So write new code or take old code from 
some other thing.

1545
01:08:46,741 --> 01:08:50,690
And Map it over here or make a whole new
library of code or refactor your code to

1546
01:08:50,691 --> 01:08:54,260
some other,
some other basis for that that will work

1547
01:08:54,261 --> 01:08:58,340
more robustly and be more extensible or 
transpiling or compiling.

1548
01:08:58,610 --> 01:09:03,610
Right.
Or even just commenting your code or 

1549
01:09:03,610 --> 01:09:03,610
asking someone else for their code.
Okay.

1550
01:09:03,610 --> 01:09:05,180
Again,
these are all ways that we make our code

1551
01:09:05,181 --> 01:09:10,181
more awesome and children's learning has
analogs to all of these that we would 

1552
01:09:10,181 --> 01:09:12,140
want to understand as an engineer from 
an algorithmic point of view.

1553
01:09:12,650 --> 01:09:16,220
So in our group we've been working on on
various early steps towards this.

1554
01:09:16,221 --> 01:09:17,900
And again,
we don't have anything like,

1555
01:09:18,210 --> 01:09:20,570
um,
program writing programs at the level of

1556
01:09:20,571 --> 01:09:23,930
children's learning algorithms.
But one example of something that we did

1557
01:09:23,931 --> 01:09:28,931
in our group,
which you might not have thought of 

1558
01:09:28,931 --> 01:09:30,311
being about this,
but it's definitely the ai work we did 

1559
01:09:30,311 --> 01:09:33,130
that got the most attention.
And the last couple of years from our 

1560
01:09:33,130 --> 01:09:33,560
group,
we had this paper that was in science.

1561
01:09:33,561 --> 01:09:37,850
It was actually on the cover of science,
sort of just hit the market at the right

1562
01:09:37,851 --> 01:09:38,540
time if you like.

1563
01:09:38,540 --> 01:09:43,540
And it got about 100 times more 
publicity than anything else I've ever 

1564
01:09:43,540 --> 01:09:46,421
done,
which is partly a testament to the 

1565
01:09:46,421 --> 01:09:46,820
really great work that Brendan Lake,
who was the first author did for his phd

1566
01:09:46,821 --> 01:09:51,821
here,
but much more so it just about the 

1567
01:09:51,821 --> 01:09:53,420
hunger for ai systems at the time when 
we published this in 2015 and we built a

1568
01:09:53,421 --> 01:09:58,421
machine system that the way we described
it was doing human level concept 

1569
01:09:58,421 --> 01:10:00,500
learning for simple concept,
very simple visual concepts.

1570
01:10:00,501 --> 01:10:03,080
These handwritten characters in many of 
the world's alphabets.

1571
01:10:03,290 --> 01:10:05,630
For those of you who know the famous m,
this Dataset,

1572
01:10:05,660 --> 01:10:08,690
the Dataset of handwritten digits zero 
through 10 or 33 nine,

1573
01:10:08,691 --> 01:10:09,460
sorry.
Uh,

1574
01:10:09,620 --> 01:10:13,100
that drove so much good research in deep
learning and pattern recognition.

1575
01:10:13,430 --> 01:10:16,850
It did that not because Yann Macun who 
put that together or Jeff Hinton,

1576
01:10:16,851 --> 01:10:18,830
who did a lot of work on deep learning 
with Ms.

1577
01:10:19,190 --> 01:10:22,910
They weren't interested fundamentally in
character recognition that they saw that

1578
01:10:22,911 --> 01:10:25,730
as a very simple test bed for developing
more general ideas.

1579
01:10:26,060 --> 01:10:31,060
And similarly,
we did this work on getting machines to 

1580
01:10:31,060 --> 01:10:33,851
do what we are kind of one shot learning
of generative models also to to develop 

1581
01:10:34,011 --> 01:10:39,011
more general ideas.
We saw this as learning very simple 

1582
01:10:39,011 --> 01:10:39,410
little mini probabilistic programs in 
this case.

1583
01:10:39,411 --> 01:10:42,110
What are those programs that the 
programs you used to draw a character.

1584
01:10:42,290 --> 01:10:47,290
So ask yourself how can you look at any 
one of these characters and see in a 

1585
01:10:47,290 --> 01:10:49,820
sense how somebody might draw it.
The way we tested this in our system was

1586
01:10:49,821 --> 01:10:54,821
this little visual turing test where we 
showed people one character in a novel 

1587
01:10:54,821 --> 01:10:58,811
alphabet and we said draw another one 
and then we compared nine people like 

1588
01:10:58,811 --> 01:11:01,460
say on the left and nine samples from 
our machine say on the right.

1589
01:11:01,640 --> 01:11:03,020
And we said,
we asked other people,

1590
01:11:03,021 --> 01:11:08,021
could you tell which was the human 
drawing another example or imagining 

1591
01:11:08,021 --> 01:11:11,351
another example in which was the machine
and people couldn't tell when I said 

1592
01:11:11,351 --> 01:11:11,351
ones on the left,
ones on the right.

1593
01:11:11,351 --> 01:11:15,730
I don't actually remember.
And and different ones you could see if 

1594
01:11:15,730 --> 01:11:15,730
you can tell.
It's very hard to tell.

1595
01:11:15,730 --> 01:11:17,100
Can you tell which is for each one of 
these characters,

1596
01:11:17,310 --> 01:11:20,400
which new set of examples were drawn by 
a human versus the machine?

1597
01:11:21,360 --> 01:11:23,880
Here's the right answer and probably you
couldn't tell.

1598
01:11:24,240 --> 01:11:28,080
The way we did this was by assembling a 
simple kind of program learning program,

1599
01:11:28,160 --> 01:11:33,160
right?
So we basically said when you draw a 

1600
01:11:33,160 --> 01:11:35,301
character,
you're assembling strokes and sub 

1601
01:11:35,301 --> 01:11:35,670
strokes with goals and sub goals that 
produce ink on the page,

1602
01:11:35,820 --> 01:11:40,820
and when you see a character,
you're working backwards to figure out 

1603
01:11:40,820 --> 01:11:43,791
what was the program,
the most efficient program that did 

1604
01:11:43,791 --> 01:11:46,161
that.
So you're basically inverting a 

1605
01:11:46,161 --> 01:11:46,161
probabilistic program,
doing basie and inference to the program

1606
01:11:46,161 --> 01:11:47,850
most likely to have generated what you 
saw.

1607
01:11:48,240 --> 01:11:53,240
This is one small step we think towards 
being able to learn programs to being 

1608
01:11:53,240 --> 01:11:55,530
able to learn something ultimately like 
a whole game engine program.

1609
01:11:56,220 --> 01:12:01,220
The last thing I'll leave you with is 
just a pointer to sort of work in 

1610
01:12:01,220 --> 01:12:01,220
action,
right?

1611
01:12:01,220 --> 01:12:04,761
So this is some work being done by a 
current phd student who works partly 

1612
01:12:04,761 --> 01:12:04,990
with me,
but also with Armando solar,

1613
01:12:04,991 --> 01:12:09,991
Lezama and Cecil sale.
This is Kevin Ellis is an example of 

1614
01:12:09,991 --> 01:12:09,991
what's now.
I think again,

1615
01:12:09,991 --> 01:12:14,310
an emerging exciting area in Ai,
well beyond anything that we're doing is

1616
01:12:14,610 --> 01:12:17,190
combining techniques from where Amando 
comes from,

1617
01:12:17,191 --> 01:12:18,870
which is the world of programming 
languages,

1618
01:12:18,871 --> 01:12:23,871
not machine learning or ai,
but tools from programming languages 

1619
01:12:23,871 --> 01:12:24,900
which can be used to automatically 
synthesize code.

1620
01:12:25,090 --> 01:12:27,180
Okay.
With the machine learning tool kit,

1621
01:12:27,181 --> 01:12:32,181
in this case,
a kind of basie and minimum minimum 

1622
01:12:32,181 --> 01:12:32,310
description link idea to be able to make
again,

1623
01:12:32,340 --> 01:12:37,340
what is really one small step towards 
machines that can learn programs by 

1624
01:12:37,340 --> 01:12:38,730
basically trying to efficiently find the
shortest,

1625
01:12:38,731 --> 01:12:41,730
simplest program which can capture some 
data set.

1626
01:12:42,000 --> 01:12:44,220
So we think by combining these kinds of 
tools,

1627
01:12:44,280 --> 01:12:49,280
in this case,
let's say from basie and infants over 

1628
01:12:49,280 --> 01:12:51,381
programs with a number of tools that 
have been developed in other areas of 

1629
01:12:51,381 --> 01:12:55,161
computer science that don't look 
anything or haven't been considered to 

1630
01:12:55,161 --> 01:12:55,680
be machine learning or ai like 
programming languages.

1631
01:12:56,070 --> 01:13:01,070
It's one of the many ways that going 
forward we're going to be able to build 

1632
01:13:01,070 --> 01:13:01,070
smarter,
more human like machines.

1633
01:13:01,650 --> 01:13:06,650
So just to end then,
what I've tried to tell you here is 

1634
01:13:06,650 --> 01:13:10,130
taught first of all,
identify the ways in which human 

1635
01:13:10,130 --> 01:13:12,651
intelligence goes beyond pattern 
recognition to really all these 

1636
01:13:12,651 --> 01:13:12,900
activities of modeling the world.
Okay.

1637
01:13:13,320 --> 01:13:18,320
To give you a sense of some of the 
domains where you can start to study 

1638
01:13:18,320 --> 01:13:18,420
this in common sense,
seen understanding for example,

1639
01:13:18,640 --> 01:13:19,670
um,
or you know,

1640
01:13:19,680 --> 01:13:23,310
something like a one shot learning for 
example,

1641
01:13:23,311 --> 01:13:24,090
like what we were just doing.

1642
01:13:24,090 --> 01:13:26,430
Their learning is programming the engine
in your head.

1643
01:13:27,140 --> 01:13:32,140
Okay.
And to give you a sense of some of the 

1644
01:13:32,140 --> 01:13:32,140
technical tools,
probabilistic programs,

1645
01:13:32,140 --> 01:13:34,680
program synthesis,
game engines for example,

1646
01:13:34,681 --> 01:13:37,770
as well as a little bit of deep learning
that bringing together,

1647
01:13:37,800 --> 01:13:40,020
we're starting to be able to make these 
things real.

1648
01:13:40,110 --> 01:13:45,110
Okay.
Now that's the science agenda and the 

1649
01:13:45,110 --> 01:13:45,450
reverse engineering agenda.
But think about for those of you who are

1650
01:13:45,451 --> 01:13:50,451
interested in technology,
what are the many big ai frontiers that 

1651
01:13:50,451 --> 01:13:53,811
this opens up?
So the one I'm most excited about is 

1652
01:13:53,811 --> 01:13:57,621
this idea which is,
which I've highlighted here in our big 

1653
01:13:57,621 --> 01:13:59,990
research agenda.
This is the one I'm most excited about 

1654
01:13:59,990 --> 01:13:59,990
to work on for the,
you know,

1655
01:13:59,990 --> 01:14:00,090
it could be the rest of my career 
honestly,

1656
01:14:00,330 --> 01:14:05,330
but it's really what is,
what is the oldest and maybe the best 

1657
01:14:05,330 --> 01:14:08,070
dream of AI researchers of how to build 
a human like intelligence system,

1658
01:14:08,071 --> 01:14:09,570
a real agi system.

1659
01:14:10,110 --> 01:14:15,110
It's the idea that terrain proposed when
he proposed the turing test or Marvin 

1660
01:14:15,110 --> 01:14:16,360
Minsky proposed this at different times 
in his life or many people have proposed

1661
01:14:16,361 --> 01:14:16,930
this,
right?

1662
01:14:17,230 --> 01:14:22,230
Which is to build a system that grows 
into intelligence the way a human does 

1663
01:14:22,230 --> 01:14:22,660
that starts like a baby and learns like 
a child,

1664
01:14:22,960 --> 01:14:27,960
and I've tried to show you how we're 
starting to be able to understand those 

1665
01:14:27,960 --> 01:14:31,171
things.
What a baby's mind starts with how 

1666
01:14:31,171 --> 01:14:31,390
children actually learn and looking 
forward,

1667
01:14:31,391 --> 01:14:36,391
we might.
We might imagine that someday we'll be 

1668
01:14:36,391 --> 01:14:36,391
able to build machines that can do this.
I think we can actually start working on

1669
01:14:36,391 --> 01:14:39,520
this right now and we're.
And that's something that we're doing in

1670
01:14:39,521 --> 01:14:41,980
our group.
So if that kind of thing excites you,

1671
01:14:41,981 --> 01:14:46,981
then I encourage you to work on it and 
maybe even with us or if any one of 

1672
01:14:46,981 --> 01:14:47,770
these other activities of human 
intelligence excite you,

1673
01:14:48,070 --> 01:14:53,070
I think taking the kind of science based
reverse engineering approach that we're 

1674
01:14:53,070 --> 01:14:55,060
doing and then trying to put that into 
engineering practice,

1675
01:14:55,430 --> 01:14:56,410
it's,
this is,

1676
01:14:56,490 --> 01:14:56,980
this is,
uh,

1677
01:14:56,981 --> 01:14:59,250
this is not just a possible route,
but I think it's,

1678
01:14:59,560 --> 01:15:04,560
it's quite possibly the most valuable 
route that you could work on right now 

1679
01:15:04,560 --> 01:15:07,960
to try to actually achieve at least some
kind of artificial general intelligence,

1680
01:15:07,990 --> 01:15:12,990
especially the kind of intelligence ai 
system that's going to live in a human 

1681
01:15:12,990 --> 01:15:13,900
world and interact with humans.

1682
01:15:13,930 --> 01:15:18,930
There's many kinds of ai systems that 
could live in worlds of data that none 

1683
01:15:18,930 --> 01:15:18,940
of us can understand or whatever live in
ourselves.

1684
01:15:19,210 --> 01:15:24,210
But if you want to build machines that 
can live in our world and interact with 

1685
01:15:24,210 --> 01:15:24,970
us the way we are used to interacting 
with other people,

1686
01:15:25,390 --> 01:15:27,640
then I think this is a route that you 
should consider.

1687
01:15:27,790 --> 01:15:28,330
Okay.
Thank you.

1688
01:15:30,580 --> 01:15:35,580
Thank you.

1689
01:15:39,870 --> 01:15:44,870
So earlier in the talk you expressed 
some skepticism about whether or not 

1690
01:15:44,870 --> 01:15:46,420
industry would get us to understanding 
human level intelligence.

1691
01:15:46,990 --> 01:15:49,330
It seems that there's a couple of trends
that favorite industry.

1692
01:15:49,510 --> 01:15:54,510
One is the industry is better than 
academia accumulating resources and 

1693
01:15:54,510 --> 01:15:58,291
plowing back into the topic and it seems
at the moment we've got a bit of brain 

1694
01:15:58,291 --> 01:16:02,400
drain going on from academia into 
industry and that seems like an ongoing 

1695
01:16:02,400 --> 01:16:06,511
trend.
If you look at something like learning 

1696
01:16:06,511 --> 01:16:06,511
to fly,
learning to fly into space,

1697
01:16:06,520 --> 01:16:11,520
then it looks like the story is one of 
industry kind of taking over the field 

1698
01:16:11,520 --> 01:16:16,030
and going off on its own a little bit.
Academic academics still have a role.

1699
01:16:16,090 --> 01:16:19,630
The industry kind of dominate industry 
and I would take the field.

1700
01:16:19,631 --> 01:16:19,990
Do you think?

1701
01:16:20,400 --> 01:16:25,400
Well that's a really good question and 
it's got several good questions packed 

1702
01:16:25,400 --> 01:16:25,400
into one there.
Right?

1703
01:16:25,400 --> 01:16:27,960
I didn't mean to say this wasn't meant 
to say go academia,

1704
01:16:28,050 --> 01:16:29,890
bad industry.
What I was taught,

1705
01:16:30,500 --> 01:16:35,500
what I tried to say was the approaches 
that are currently getting the most 

1706
01:16:35,500 --> 01:16:39,921
attention in industry and they're really
because they are really the most 

1707
01:16:39,921 --> 01:16:39,921
valuable ones right now for the short 
term,

1708
01:16:39,921 --> 01:16:44,841
you know,
any industry is really focused on what 

1709
01:16:44,841 --> 01:16:46,701
it can do.
What are the value propositions on 

1710
01:16:46,701 --> 01:16:46,701
basically a two year timescale at most.
I mean if you ask,

1711
01:16:46,701 --> 01:16:49,050
say Google researchers to take the most 
prominent example,

1712
01:16:49,560 --> 01:16:50,850
that's pretty much what they'll all tell
you.

1713
01:16:51,120 --> 01:16:56,120
Okay.
Maybe maybe things that might pay off 

1714
01:16:56,120 --> 01:17:00,471
initially in two years,
but maybe take five years or more to 

1715
01:17:00,471 --> 01:17:00,471
really develop,
but if,

1716
01:17:00,471 --> 01:17:03,890
if you can't show that it's going to do 
something practical for us in two years 

1717
01:17:03,890 --> 01:17:03,900
in a way that matters for our bottom 
line,

1718
01:17:03,901 --> 01:17:05,160
then it's not really worth doing.

1719
01:17:05,210 --> 01:17:07,180
Okay,
so what,

1720
01:17:07,230 --> 01:17:09,670
when we see what I'm talking about is 
technologies,

1721
01:17:09,680 --> 01:17:13,220
which right now industry sees as meeting
that specification.

1722
01:17:13,430 --> 01:17:16,040
And what I'm saying is right now,
I think those.

1723
01:17:16,190 --> 01:17:19,570
That's not where the route is to 
something like human,

1724
01:17:19,571 --> 01:17:24,500
like the not the most valuable promising
route to humanlike kinds of AI systems.

1725
01:17:24,520 --> 01:17:26,680
All right,
but I hope that like in the case,

1726
01:17:26,681 --> 01:17:27,860
as you said,
you know,

1727
01:17:27,890 --> 01:17:32,890
the basic research that we're doing now 
will be successful enough that it will 

1728
01:17:32,890 --> 01:17:34,130
get the attention of industry when the 
time is right,

1729
01:17:34,610 --> 01:17:38,810
but I think so,
so I hope at some point it won't.

1730
01:17:38,811 --> 01:17:41,870
It will at least the engineering side 
will have to be done in industry,

1731
01:17:42,500 --> 01:17:47,500
not just in academia,
but you're also pointing to issues of 

1732
01:17:47,500 --> 01:17:47,540
like brain drain and other things like 
that.

1733
01:17:47,810 --> 01:17:50,120
That I think these are real issues 
confronting our community.

1734
01:17:50,121 --> 01:17:55,121
I think everybody knows this and I'm 
sure this will come up multiple times 

1735
01:17:55,121 --> 01:17:55,121
here,
which is,

1736
01:17:55,121 --> 01:17:57,920
you know,
I think we have to find ways to even now

1737
01:17:57,950 --> 01:18:01,910
to combine the best of the ideas,
the energy and the resources of academia

1738
01:18:01,911 --> 01:18:02,600
and industry.

1739
01:18:02,840 --> 01:18:05,900
If we want to keep doing basically 
something interesting,

1740
01:18:06,280 --> 01:18:06,970
right?
If we,

1741
01:18:07,030 --> 01:18:09,350
if,
if we just want to redefine ai to be,

1742
01:18:09,440 --> 01:18:14,440
well,
whatever people currently call ai but 

1743
01:18:14,440 --> 01:18:16,271
scaled up well then then then fine,
forget about it and or if we just want 

1744
01:18:16,271 --> 01:18:20,111
to say,
let me and people like me do what we're 

1745
01:18:20,111 --> 01:18:22,691
doing.
At what industry would consider a 

1746
01:18:22,691 --> 01:18:22,691
snail's pace on toy problems.
Okay fine,

1747
01:18:22,691 --> 01:18:24,240
but if but if we want to,
if you know,

1748
01:18:24,380 --> 01:18:29,380
if I want to take what I'm doing to the 
level that that will really be paying 

1749
01:18:29,380 --> 01:18:33,821
off that level the industry can 
appreciate or just that really has 

1750
01:18:33,821 --> 01:18:34,580
technological impact on a broad scale.
Right?

1751
01:18:34,850 --> 01:18:39,850
Or I think if industry wants to take 
what it's doing and really build 

1752
01:18:39,850 --> 01:18:40,670
machines that are actually intelligent,
right?

1753
01:18:40,850 --> 01:18:43,190
Or machine learning that actually learns
like a person,

1754
01:18:43,200 --> 01:18:46,460
then I think we need each other now and 
not just in some point in the future.

1755
01:18:46,610 --> 01:18:49,010
So this is a general challenge for mit 
and for,

1756
01:18:49,050 --> 01:18:50,300
for everywhere.
And for Google.

1757
01:18:50,301 --> 01:18:54,750
I mean we just spent a few days talking 
to Google about exactly this issue of,

1758
01:18:54,870 --> 01:18:57,650
in fact this was a talk I prepared 
partly for that purpose.

1759
01:18:57,800 --> 01:19:00,080
So we wanted to raise those issues and 
and it's just.

1760
01:19:00,330 --> 01:19:01,910
I mean really there,
I don't know what I mean.

1761
01:19:01,920 --> 01:19:06,920
What rather I can think of some 
solutions to that problem of what you 

1762
01:19:06,920 --> 01:19:11,581
could call brain drain from the academic
point of view or what you could call 

1763
01:19:11,581 --> 01:19:12,820
just narrowing in into certain local 
minima and the industry point of view,

1764
01:19:13,180 --> 01:19:18,180
but they will require the leadership of 
both academic institutions like mit and 

1765
01:19:18,180 --> 01:19:22,981
companies like Google being creative 
about how they might work together in 

1766
01:19:22,981 --> 01:19:22,981
ways that are a little bit outside of 
their comfort zone.

1767
01:19:22,981 --> 01:19:24,900
I hope that will start to happen,
um,

1768
01:19:25,180 --> 01:19:30,180
including at mit and at many other 
universities and companies like Google 

1769
01:19:30,180 --> 01:19:31,480
and many others.
And I think we need it to happen for the

1770
01:19:31,481 --> 01:19:33,970
health of all parties concerned.
Okay.

1771
01:19:34,000 --> 01:19:35,110
Thank you very much.
Thanks.

1772
01:19:36,020 --> 01:19:41,020
Uh,
I'm curious about sort of the premise 

1773
01:19:41,020 --> 01:19:44,600
that you gave that a.
One of the big gaps missing at 

1774
01:19:44,600 --> 01:19:47,931
determining intelligence is the fact 
that we need to teach machines how to 

1775
01:19:47,931 --> 01:19:52,880
recognize models.
And I'm curious as to what you think 

1776
01:19:52,880 --> 01:19:57,590
sort of non goal oriented cognitive 
activity comes into play there.

1777
01:19:58,170 --> 01:20:03,170
Things like feelings and emotions and,
and why you don't think that might not 

1778
01:20:04,860 --> 01:20:08,130
necessarily be like the most

1779
01:20:08,680 --> 01:20:13,680
important question.
The only reason emotions didn't appear 

1780
01:20:13,680 --> 01:20:15,030
on my slide is because there's a few 
reasons,

1781
01:20:15,031 --> 01:20:20,031
but the slide is only so big.
I wanted the font to be readable for 

1782
01:20:20,031 --> 01:20:21,390
such an important slide.
I had versions of my slide in which I do

1783
01:20:21,391 --> 01:20:22,550
talk about that.
Okay.

1784
01:20:22,740 --> 01:20:24,140
Um,
I,

1785
01:20:24,760 --> 01:20:27,330
it's not that I think feelings or 
emotions aren't important.

1786
01:20:27,590 --> 01:20:32,590
I think they are important and I,
and I used to not have many insights on 

1787
01:20:32,590 --> 01:20:35,740
it about what to do about them,
but actually partly based on some of my 

1788
01:20:35,740 --> 01:20:36,630
colleagues here at Mit,
bcs,

1789
01:20:36,631 --> 01:20:41,631
Laura Schultz and Rebecca Saxe to,
of my cognitive colleagues who I work 

1790
01:20:41,631 --> 01:20:41,750
closely with,
um,

1791
01:20:41,810 --> 01:20:43,140
they've been starting to,
to,

1792
01:20:43,350 --> 01:20:46,020
to do research on how people understand 
emotions,

1793
01:20:46,021 --> 01:20:51,021
both their own and others and we've been
starting to work with them on 

1794
01:20:51,021 --> 01:20:53,601
computational models.
So that's actually something I'm 

1795
01:20:53,601 --> 01:20:53,601
actively interested in and even working 
on.

1796
01:20:53,601 --> 01:20:53,790
But I would say,
and again,

1797
01:20:53,791 --> 01:20:55,560
for those of you who study emotion or 
know about this,

1798
01:20:55,561 --> 01:20:57,300
actually you're going to have lisa 
coming in,

1799
01:20:57,301 --> 01:20:57,670
right?
Oh.

1800
01:20:57,671 --> 01:21:00,090
So she's going to basically say a 
version of the same thing.

1801
01:21:00,091 --> 01:21:05,091
I think the deepest way to understand 
she's one of the world's experts on 

1802
01:21:05,091 --> 01:21:06,120
this.
The deepest way to understand emotion is

1803
01:21:06,121 --> 01:21:08,700
very much based on our mental models of 
ourselves,

1804
01:21:08,701 --> 01:21:10,410
of the situation we're in and of other 
people,

1805
01:21:10,650 --> 01:21:11,640
right?
Think about,

1806
01:21:11,641 --> 01:21:14,400
for example,
all of the different.

1807
01:21:14,820 --> 01:21:15,810
I mean if,
if,

1808
01:21:15,820 --> 01:21:16,230
if,
if,

1809
01:21:16,240 --> 01:21:17,220
if you think about them,
I mean,

1810
01:21:17,221 --> 01:21:18,450
again,
Lisa will talk all about this,

1811
01:21:18,451 --> 01:21:23,451
but if you think about emotion,
it's just a very small set of what are 

1812
01:21:23,451 --> 01:21:26,001
sometimes called basic emotions,
like being happy or angry or sad or you 

1813
01:21:28,041 --> 01:21:29,580
know,
those are small number of them,

1814
01:21:29,581 --> 01:21:31,230
right?
There's usually only a few,

1815
01:21:31,290 --> 01:21:33,980
right?
You might not say,

1816
01:21:34,020 --> 01:21:39,020
you might see that as somehow like very 
basic things that are opposed to some 

1817
01:21:39,020 --> 01:21:39,030
kind of cognitive activity.

1818
01:21:39,270 --> 01:21:41,880
But think about all the different words 
we have for emotion,

1819
01:21:42,060 --> 01:21:42,960
right?
Um,

1820
01:21:43,080 --> 01:21:44,860
for example,
think about an,

1821
01:21:44,880 --> 01:21:47,340
a,
a famous cognitive emotion like regret.

1822
01:21:47,700 --> 01:21:50,280
What does it mean to feel regret or 
frustration?

1823
01:21:50,670 --> 01:21:51,920
Right?
To just to,

1824
01:21:52,050 --> 01:21:57,050
to know both for yourself when you're 
not just feeling kind of down or 

1825
01:21:57,050 --> 01:22:00,531
negative,
but you're feeling regret that that 

1826
01:22:00,531 --> 01:22:02,901
means something like,
I have to feel like there's a situation 

1827
01:22:02,901 --> 01:22:06,201
that came out differently from how I 
hoped and I realized I could have done 

1828
01:22:06,201 --> 01:22:06,990
something differently.
Right?

1829
01:22:07,110 --> 01:22:09,090
So that means you have to be able to 
understand,

1830
01:22:09,160 --> 01:22:14,160
you have to have a model,
you have to be able to do a kind of 

1831
01:22:14,160 --> 01:22:14,160
counterfactual reasoning and to think,
oh,

1832
01:22:14,160 --> 01:22:18,171
if only I had acted a different way than
I can predict that the world would have 

1833
01:22:18,171 --> 01:22:18,171
come out differently and that's the 
situation I wanted,

1834
01:22:18,171 --> 01:22:20,160
but instead it came up this other way,
right?

1835
01:22:20,560 --> 01:22:22,530
Um,
or think about frustration again,

1836
01:22:22,890 --> 01:22:24,570
that requires something like 
understanding,

1837
01:22:24,571 --> 01:22:26,160
okay,
I've tried a bunch of times,

1838
01:22:26,161 --> 01:22:28,680
I thought this would work,
but it doesn't seem to be working.

1839
01:22:28,681 --> 01:22:31,410
Maybe I'm ready to give up though.
Those are all.

1840
01:22:31,440 --> 01:22:33,810
Those are.
Those are very important human emotions.

1841
01:22:34,110 --> 01:22:36,150
We have to understand to understand 
ourselves.

1842
01:22:36,151 --> 01:22:38,790
We need that to understand other people 
to understand communication,

1843
01:22:39,030 --> 01:22:42,750
but those are all filtered through the 
kinds of models of action that I was.

1844
01:22:42,840 --> 01:22:47,840
Just the ones I was talking about here 
with these say cost benefit analysis of 

1845
01:22:47,840 --> 01:22:47,840
action.
So what I'm.

1846
01:22:47,840 --> 01:22:52,340
So I'm just trying to say I think this 
is very basic stuff that will be the 

1847
01:22:52,340 --> 01:22:55,701
basis for building,
I think better engineering style models 

1848
01:22:55,701 --> 01:22:57,630
of the full spectrum of human emotion 
beyond just like,

1849
01:22:57,631 --> 01:22:59,400
well,
I'm feeling good or bad or scared.

1850
01:22:59,420 --> 01:23:01,770
Okay.
And if I think when you see Lisa,

1851
01:23:01,771 --> 01:23:04,410
she will in her own way,
say something very similar.

1852
01:23:05,470 --> 01:23:06,430
Interesting.
Thanks.

1853
01:23:06,431 --> 01:23:09,340
Yeah,
thanks Josh for your stock.

1854
01:23:09,550 --> 01:23:14,550
So all these are both human cognition 
and try to build a model to mimic those 

1855
01:23:14,550 --> 01:23:14,920
cognition.
What you don't.

1856
01:23:14,980 --> 01:23:18,310
How much could help you to understand 
how the circuit implement those things.

1857
01:23:18,890 --> 01:23:21,330
I mean like the circuits in the brain.
What's the.

1858
01:23:22,260 --> 01:23:23,820
Is that what you work on by Newcastle?
Sorry,

1859
01:23:23,890 --> 01:23:24,310
what?
Is that?

1860
01:23:24,311 --> 01:23:25,540
What you work on by any chance?
Yeah,

1861
01:23:26,330 --> 01:23:27,570
yeah.
Yeah.

1862
01:23:27,880 --> 01:23:29,920
So,
so in the center for brains,

1863
01:23:29,921 --> 01:23:32,530
minds and machines as well as in brain 
and cognitive science.

1864
01:23:32,531 --> 01:23:37,531
Yet we have a number of colleagues who 
study the actual hardware basis of this 

1865
01:23:37,531 --> 01:23:40,090
stuff in the brain and that includes 
like the large scale architecture of the

1866
01:23:40,091 --> 01:23:41,780
brain,
say like what Nancy Campbell shirt,

1867
01:23:41,800 --> 01:23:45,250
Rebecca Saxe study with functional brain
imaging or the more detailed circuitry,

1868
01:23:45,251 --> 01:23:50,251
which usually requires recording from 
say non brands right at the level of 

1869
01:23:50,251 --> 01:23:51,400
individual neurons and connections 
between neurons.

1870
01:23:51,470 --> 01:23:53,920
Alright.
So I'm very interested in those things,

1871
01:23:53,921 --> 01:23:56,160
although it's not mostly what I work on.
Right.

1872
01:23:56,350 --> 01:23:57,390
But I would say,
you know,

1873
01:23:57,460 --> 01:23:59,590
again like in many other areas of 
science,

1874
01:23:59,620 --> 01:24:04,620
certainly in neuroscience,
the kind of work I'm talking about here 

1875
01:24:04,620 --> 01:24:07,980
in a sort of classic reductionist 
program sets the target for what we 

1876
01:24:07,980 --> 01:24:07,980
might look for.
Like if I,

1877
01:24:08,100 --> 01:24:10,290
if I just want to go,
I would,

1878
01:24:10,430 --> 01:24:12,280
I would,
I would assert right.

1879
01:24:12,281 --> 01:24:17,281
Or my working conjecture is that if,
if you do the kind of work that I'm 

1880
01:24:17,281 --> 01:24:22,231
talking about here,
it gives you the right targets or gives 

1881
01:24:22,231 --> 01:24:25,261
you a candidates that have targets to 
look for what are the neural circuits 

1882
01:24:25,261 --> 01:24:25,261
computing.
Right?

1883
01:24:25,270 --> 01:24:30,270
Whereas if you just go in and just say,
start poking around in the brain or have

1884
01:24:30,491 --> 01:24:35,491
some idea that what you're going to try 
to do is find the neural circuits which 

1885
01:24:35,491 --> 01:24:38,131
underlie behavior without a sense of the
computations needed to produce those 

1886
01:24:38,131 --> 01:24:38,680
behaviors.

1887
01:24:39,010 --> 01:24:44,010
I don't,
I think it's going to be very difficult 

1888
01:24:44,010 --> 01:24:46,180
to,
to know what to look for and to know 

1889
01:24:46,180 --> 01:24:46,780
when you found even Bible answers.
So I think that's,

1890
01:24:46,810 --> 01:24:48,760
you know,
that's the standard kind of reductionist

1891
01:24:48,761 --> 01:24:51,410
program,
but it's not,

1892
01:24:51,500 --> 01:24:52,660
it's,
it's not.

1893
01:24:52,661 --> 01:24:53,810
Um,
I also think it's,

1894
01:24:53,830 --> 01:24:57,130
it's not one that is divorced from the 
study of neurocircuits.

1895
01:24:57,370 --> 01:25:02,370
It's also one,
if you look at the broad picture of 

1896
01:25:02,370 --> 01:25:04,741
reverse engineering,
it's one where you were neurocircuits 

1897
01:25:04,741 --> 01:25:07,570
and understanding the circuits in the 
brain play an absolutely critical role.

1898
01:25:07,660 --> 01:25:10,000
Okay.
I would say the main as an.

1899
01:25:10,060 --> 01:25:12,130
When you look at the brain at the 
hardware level,

1900
01:25:12,131 --> 01:25:17,131
as an engineer,
I'm mostly looking at the software 

1901
01:25:17,131 --> 01:25:17,131
level,
right?

1902
01:25:17,131 --> 01:25:18,100
But when you look at the hardware level,
there are some remarkable properties.

1903
01:25:18,490 --> 01:25:19,720
One remarkable property,
again,

1904
01:25:19,721 --> 01:25:24,370
is how much parallelism there is and in 
many ways how fast the computations are.

1905
01:25:24,400 --> 01:25:25,840
Okay,
neurons are slow,

1906
01:25:25,930 --> 01:25:28,030
but the computation of the tubs are very
fast.

1907
01:25:28,180 --> 01:25:33,180
So how do we get elements that are in 
some sense quite slow in their time 

1908
01:25:33,180 --> 01:25:34,840
constant to produce such intelligent 
behavior so quickly?

1909
01:25:34,841 --> 01:25:39,841
That's a great mystery.
And I think if we understood that it 

1910
01:25:39,841 --> 01:25:39,841
would have payoff for building all sorts
of,

1911
01:25:39,841 --> 01:25:40,350
uh,
you know,

1912
01:25:40,360 --> 01:25:42,850
apple basically application embedded 
circuits.

1913
01:25:42,851 --> 01:25:47,851
Okay.
But also maybe most important is the 

1914
01:25:47,851 --> 01:25:47,851
power consumption.
And again,

1915
01:25:47,851 --> 01:25:47,890
many people have,
have,

1916
01:25:48,280 --> 01:25:49,420
have noted this,
right?

1917
01:25:49,630 --> 01:25:52,810
If you look at the power consumption,
the power that the brain consumes,

1918
01:25:52,811 --> 01:25:54,280
like what did I eat today?
Okay?

1919
01:25:54,970 --> 01:25:57,520
Almost nothing.
My daughter,

1920
01:25:57,521 --> 01:25:59,560
who's again,
she's doing an internship here,

1921
01:25:59,561 --> 01:26:04,561
she literally yesterday,
all she ate was a burrito and yet she 

1922
01:26:04,561 --> 01:26:07,831
wrote 300 lines of code for her 
internship project on really cool 

1923
01:26:07,831 --> 01:26:11,951
computational linguistics project.
So somehow she turned a Burrito into a 

1924
01:26:11,951 --> 01:26:12,560
model of child language acquisition.

1925
01:26:12,740 --> 01:26:14,240
Okay,
but how did she do that?

1926
01:26:14,241 --> 01:26:15,710
Or how do any of us do this?
Right?

1927
01:26:15,980 --> 01:26:20,980
Um,
where if you look at the power that we 

1928
01:26:20,980 --> 01:26:22,541
consume when we simulate even a very,
very small chunk of cortex on our 

1929
01:26:22,541 --> 01:26:26,501
conventional hardware,
or we do any kind of machine learning 

1930
01:26:26,501 --> 01:26:26,600
thing,
we have systems which are very,

1931
01:26:26,601 --> 01:26:27,140
very,
very,

1932
01:26:27,141 --> 01:26:30,350
very far from the power of the human 
brain computationally,

1933
01:26:30,590 --> 01:26:34,790
but in terms of physical energy consumed
way,

1934
01:26:34,850 --> 01:26:36,860
way past what any individual brain is 
doing.

1935
01:26:37,010 --> 01:26:40,220
So how do we get circuitry of any sort,
biological,

1936
01:26:40,221 --> 01:26:43,370
or just any physical circuits to be as 
smart as we are with,

1937
01:26:43,550 --> 01:26:45,560
with as little energy as we are.
This is,

1938
01:26:45,590 --> 01:26:49,310
this is a huge problem for basically 
every area of engineering,

1939
01:26:49,311 --> 01:26:50,060
right?
If you want to,

1940
01:26:50,450 --> 01:26:55,450
if you want to have any kind of robot,
the power consumption is a key 

1941
01:26:55,450 --> 01:26:56,510
bottleneck.
Same for self driving cars.

1942
01:26:56,720 --> 01:27:01,720
If we want to build ai without 
contributing to global warming and 

1943
01:27:01,720 --> 01:27:05,921
climate change,
let alone use ai to solve climate 

1944
01:27:05,921 --> 01:27:08,621
change,
we really need to address these issues 

1945
01:27:08,621 --> 01:27:08,690
and the brain is a,
is a huge a guide there,

1946
01:27:08,920 --> 01:27:13,920
right?
I think there are some people who are 

1947
01:27:13,920 --> 01:27:13,920
really starting to think about this.
How can we say,

1948
01:27:13,920 --> 01:27:15,770
for example,
build somehow brain inspired computers,

1949
01:27:15,771 --> 01:27:20,771
which are very,
very low power but maybe only 

1950
01:27:20,771 --> 01:27:20,771
approximate.
So I'm thinking here of Joe Bates,

1951
01:27:20,771 --> 01:27:21,590
I don't know if I don't have,
you know,

1952
01:27:21,591 --> 01:27:22,160
Joe,
he's,

1953
01:27:22,460 --> 01:27:22,890
he's,
uh,

1954
01:27:22,940 --> 01:27:25,430
been around mit and other places for 
quite awhile.

1955
01:27:25,460 --> 01:27:27,200
Can I tell them about your company?
So,

1956
01:27:27,320 --> 01:27:32,320
so joe has a low,
a startup in Kendall Square called 

1957
01:27:32,320 --> 01:27:33,140
singular computing and they have some 
very interesting ideas,

1958
01:27:33,141 --> 01:27:37,130
including some actual implemented 
technology for low power,

1959
01:27:37,131 --> 01:27:42,131
approximate computing in a sort of a 
brain like way that might lead to 

1960
01:27:42,131 --> 01:27:42,890
possibly even like the ability to build 
something.

1961
01:27:42,890 --> 01:27:46,070
This is Joe's dream to build on this,
about the size of this table,

1962
01:27:46,071 --> 01:27:50,030
but that has a billion course,
a billion cores and runs on a reasonable

1963
01:27:50,031 --> 01:27:55,031
kind of power consumption.
I would love to have such a machine if 

1964
01:27:55,031 --> 01:27:56,360
anybody wants to help joe build that.
I think he'd love to talk to you,

1965
01:27:57,470 --> 01:27:59,130
but it's one of a number of,
of,

1966
01:27:59,150 --> 01:28:00,770
of ideas.
I mean google x,

1967
01:28:00,771 --> 01:28:05,771
people are working on similar things.
Probably most of the major chip 

1968
01:28:05,771 --> 01:28:06,080
companies are also inspired by this 
idea.

1969
01:28:06,230 --> 01:28:09,410
And I think even if you didn't think you
were interested in the brain,

1970
01:28:09,411 --> 01:28:14,411
if you want to build the kind of ai 
we're talking about and run it on 

1971
01:28:14,411 --> 01:28:17,741
physical hardware of any sort and 
understanding how the brain circuits 

1972
01:28:17,741 --> 01:28:21,900
compute what they do,
what I'm talking about with as little 

1973
01:28:21,900 --> 01:28:23,740
power as they do,
I don't know any better place to look.

1974
01:28:25,040 --> 01:28:30,040
It seems like a lot of the improvements 
in ai have been driven by increasing 

1975
01:28:30,040 --> 01:28:31,550
computational power.
How far are you,

1976
01:28:31,700 --> 01:28:36,700
would you say me like Gpu or CPU?
How far would you say we are from 

1977
01:28:36,700 --> 01:28:39,050
hardware that could run a general 
artificial intelligence

1978
01:28:40,560 --> 01:28:42,300
of the kind that I'm talking about?
Yeah,

1979
01:28:42,301 --> 01:28:44,290
I dunno.
I'll start with a billion cores and then

1980
01:28:44,291 --> 01:28:46,150
we'll see.
I mean,

1981
01:28:46,151 --> 01:28:47,980
I think we're,
I think we're,

1982
01:28:48,060 --> 01:28:48,460
I mean,
I think,

1983
01:28:48,550 --> 01:28:53,550
I think there's no way to answer that 
question in a way that software 

1984
01:28:53,550 --> 01:28:53,550
independent.
I don't know how to do that right.

1985
01:28:54,400 --> 01:28:58,410
But I think that um,
it's and,

1986
01:28:58,670 --> 01:28:59,650
and you know,
I don't know,

1987
01:28:59,651 --> 01:29:01,710
like when you say how far we you mean?
Uh,

1988
01:29:01,711 --> 01:29:03,960
how far am I with the resources I have 
right now?

1989
01:29:03,961 --> 01:29:04,940
How far my,
if,

1990
01:29:05,000 --> 01:29:08,250
if Google decides to put all of its 
resources at my disposal,

1991
01:29:08,251 --> 01:29:10,170
like they might if I were working at 
deepmind.

1992
01:29:11,440 --> 01:29:13,050
I don't know the answer to that 
question,

1993
01:29:13,650 --> 01:29:18,650
but I think the,
I think what we can say is this 

1994
01:29:18,650 --> 01:29:18,650
individual neurons.
I mean,

1995
01:29:18,650 --> 01:29:23,511
again,
this goes back to another reason to 

1996
01:29:23,511 --> 01:29:23,511
study neurocircuits.
Um,

1997
01:29:23,511 --> 01:29:25,020
if you look at what we currently call 
neural networks in the Ai side,

1998
01:29:25,230 --> 01:29:26,700
the model of a neuron is,
is very,

1999
01:29:26,701 --> 01:29:27,720
very simple thing.

2000
01:29:28,710 --> 01:29:31,050
Individual neurons are not only much 
more complex,

2001
01:29:31,170 --> 01:29:33,060
but I have a lot more computational 
power.

2002
01:29:33,061 --> 01:29:35,280
It's not clear how they use it or 
whether they use it,

2003
01:29:35,820 --> 01:29:39,300
but I think it's just as likely that a 
neuron is something like a revenue,

2004
01:29:39,720 --> 01:29:44,720
right?
Is that a neuron is something like a 

2005
01:29:44,720 --> 01:29:46,521
computer,
like under one neuron in your brain is 

2006
01:29:46,521 --> 01:29:46,521
more like a CPU note.
Okay.

2007
01:29:46,590 --> 01:29:47,440
Maybe,
um,

2008
01:29:47,550 --> 01:29:50,770
and thus the 10 billion or trillion,
you know,

2009
01:29:50,800 --> 01:29:52,710
the large number of neurons in your 
brain,

2010
01:29:53,100 --> 01:29:58,100
um,
I think it's like 10 billion cortical 

2011
01:29:58,100 --> 01:29:59,310
pyramidal neurons or something might be 
like 10 billion corps.

2012
01:29:59,520 --> 01:30:00,390
Okay.
For example,

2013
01:30:00,391 --> 01:30:02,940
that's at least as plausible I think to 
me as any other estimate.

2014
01:30:03,150 --> 01:30:04,410
So.
And I think so.

2015
01:30:04,500 --> 01:30:08,310
I think we're definitely on the 
underside with very big error bars,

2016
01:30:08,430 --> 01:30:13,430
so I completely agree that um,
or if this is what you might be 

2017
01:30:13,430 --> 01:30:13,580
suggesting and May,
you know,

2018
01:30:13,670 --> 01:30:15,060
going back to my answer to your 
question,

2019
01:30:15,061 --> 01:30:16,820
I don't think we're going to get to what
I'm talking about.

2020
01:30:16,860 --> 01:30:21,480
Anything like a real brain scale without
major innovations on the hardware side.

2021
01:30:21,690 --> 01:30:22,410
And you know,
it's,

2022
01:30:22,470 --> 01:30:27,470
it's interesting that what drove those 
innovations in the support current ai 

2023
01:30:27,470 --> 01:30:29,340
was mostly not ai.
It was the video game industry.

2024
01:30:29,660 --> 01:30:33,450
I'm a way when I point to the video game
engine in your head,

2025
01:30:33,510 --> 01:30:38,510
that's a similar thing that was driven 
by the video game industry on the 

2026
01:30:38,510 --> 01:30:39,180
software side.
I think we should all play as many video

2027
01:30:39,181 --> 01:30:44,181
games as we can and contribute to the 
growth of the video game industry 

2028
01:30:44,181 --> 01:30:44,430
because.
No,

2029
01:30:44,431 --> 01:30:46,790
because I mean,
I mean you can see this [inaudible] like

2030
01:30:46,980 --> 01:30:48,600
there are companies out there.
For example,

2031
01:30:48,601 --> 01:30:51,780
there's a company called improbable 
which is a London company.

2032
01:30:52,150 --> 01:30:55,200
I'm London based startup,
a pretty sizable startup at this point,

2033
01:30:55,410 --> 01:30:58,050
which is building something that they 
call spatial Oscp,

2034
01:30:58,260 --> 01:30:59,250
which is a.
it's a,

2035
01:30:59,251 --> 01:31:01,170
it's not a,
it's not a hardware idea,

2036
01:31:01,171 --> 01:31:03,000
but it's a kind of software idea for 
very,

2037
01:31:03,001 --> 01:31:06,420
very big distributed computing 
environments to run much,

2038
01:31:06,421 --> 01:31:09,000
much more complex,
realistic simulations of the world for a

2039
01:31:09,001 --> 01:31:11,580
much more interesting immersive 
permanent video games.

2040
01:31:11,730 --> 01:31:15,390
I think that's one thing that might 
hopefully that will lead to more fun new

2041
01:31:15,391 --> 01:31:18,600
kinds of games,
but that's one example of where we might

2042
01:31:18,601 --> 01:31:22,380
look to that industry to drive some of 
the computer systems,

2043
01:31:22,381 --> 01:31:26,340
really hardware and software systems 
that will take.

2044
01:31:26,370 --> 01:31:28,200
We'll take our game to the next level.

2045
01:31:29,400 --> 01:31:34,400
Just understanding algorithmic level or 
cognitive level is just to understanding

2046
01:31:35,041 --> 01:31:40,041
the learning.
The meaning of learning will be hard to 

2047
01:31:40,041 --> 01:31:41,931
predict,
but on the circuit level these 

2048
01:31:41,931 --> 01:31:44,271
different,
but at the what level under could live 

2049
01:31:44,271 --> 01:31:44,271
with.

2050
01:31:44,271 --> 01:31:44,820
Well of course it's different.
Right,

2051
01:31:45,110 --> 01:31:47,600
but already I think you made a mistake 
there.

2052
01:31:47,601 --> 01:31:48,860
Honestly,
like you said,

2053
01:31:48,861 --> 01:31:50,480
the cognitive goal is learning how to 
predict,

2054
01:31:50,481 --> 01:31:55,481
but I'm not sure what you mean by that.
There's many things you could mean and 

2055
01:31:55,481 --> 01:31:56,180
are what our cognitive science is about 
is learning which of those versions,

2056
01:31:56,270 --> 01:31:57,830
like I don't think it's learning how to 
predict.

2057
01:31:57,831 --> 01:32:01,120
I think it's learning what need to know 
to plan actions and to uh,

2058
01:32:01,230 --> 01:32:02,110
you know,
all those things.

2059
01:32:02,111 --> 01:32:07,111
Like it's not just about predicting,
it's because there are things we can 

2060
01:32:07,111 --> 01:32:10,201
imagine that you would never predict 
because they would never happen unless 

2061
01:32:10,201 --> 01:32:11,540
we somehow make the world different 
generalizations.

2062
01:32:11,560 --> 01:32:14,170
Sorry,
not when you were methodical journalize,

2063
01:32:14,350 --> 01:32:17,350
but especially in this transfer learning
that you are interested in a few hundred

2064
01:32:17,380 --> 01:32:20,080
neurons in prefrontal cortex.
The generalize a lot.

2065
01:32:20,260 --> 01:32:25,260
Yes,
but not kind of a Bayesian model could 

2066
01:32:25,260 --> 01:32:25,260
do that.

2067
01:32:25,670 --> 01:32:28,600
You said,
but lazy and model won't do that or they

2068
01:32:28,601 --> 01:32:30,850
don't do it the way a Bayesian model 
does for sure.

2069
01:32:30,851 --> 01:32:33,430
Because that's in the abstract level.
Well,

2070
01:32:33,431 --> 01:32:35,890
I mean,
how do you really know like,

2071
01:32:35,891 --> 01:32:38,000
and what does it mean to say that some 
neurons do it like.

2072
01:32:38,140 --> 01:32:40,420
So maybe another way to put this is to 
say,

2073
01:32:40,421 --> 01:32:45,421
look,
we have a certain math that we use to 

2074
01:32:45,421 --> 01:32:47,521
capture these.
You could call it abstract or I call it 

2075
01:32:47,521 --> 01:32:47,521
software level abstractions,
right?

2076
01:32:47,521 --> 01:32:49,810
I mean all engineering is based in some 
kind of abstraction,

2077
01:32:50,170 --> 01:32:52,060
but you might have a circuit level 
abstraction,

2078
01:32:52,061 --> 01:32:57,061
a certain kind of hardware level that 
you're interested in describing the 

2079
01:32:57,061 --> 01:32:59,791
brain at and I'm mostly working out or 
starting from a more software level of 

2080
01:32:59,791 --> 01:32:59,791
abstraction.
Right?

2081
01:32:59,791 --> 01:33:01,360
They're all abstractions.
We're not talking about molecules here.

2082
01:33:01,420 --> 01:33:03,640
Right.
We're talking about some abstract notion

2083
01:33:03,641 --> 01:33:05,800
of maybe a circuit or have a program.

2084
01:33:05,850 --> 01:33:10,850
Okay.
Right now it's a really interesting 

2085
01:33:10,850 --> 01:33:10,850
question.
If I look at some circuits,

2086
01:33:10,850 --> 01:33:12,250
how do I know what program they're 
implementing?

2087
01:33:12,251 --> 01:33:17,251
Right.
If I look at the circuits and this 

2088
01:33:17,251 --> 01:33:18,451
machine,
could I tell what program they 

2089
01:33:18,451 --> 01:33:18,451
implementing?
Well,

2090
01:33:18,451 --> 01:33:18,451
maybe,
but certainly it will be a lot easier if

2091
01:33:18,451 --> 01:33:22,801
I knew something about what programs 
they might be implementing before I 

2092
01:33:22,801 --> 01:33:25,261
start to look at the circuitry.
If I just looked at the circuitry 

2093
01:33:25,261 --> 01:33:27,990
without knowing what the program was or 
what programs the thing might be doing 

2094
01:33:27,990 --> 01:33:31,741
or what kind of programming components 
would be mappable two circuits in 

2095
01:33:31,741 --> 01:33:32,800
different ways.
Right.

2096
01:33:32,801 --> 01:33:35,020
I don't even know how to begin to answer
that question,

2097
01:33:35,260 --> 01:33:40,260
so I think we've made some progress at 
understanding what neurons are doing in 

2098
01:33:40,260 --> 01:33:44,971
certain low level parts of sensory 
system and certain parts of the motor 

2099
01:33:44,971 --> 01:33:44,971
system,
like primary motor cortex,

2100
01:33:44,971 --> 01:33:49,951
like basically the parts of the neurons 
that are closest to the inputs and 

2101
01:33:49,951 --> 01:33:52,741
outputs of the brain,
right where we don't eat when you can 

2102
01:33:52,741 --> 01:33:56,641
say we don't need the kind of software 
abstractions that I'm talking about or 

2103
01:33:56,641 --> 01:34:00,280
where we sort of agree on what those 
things already are so we can make enough

2104
01:34:00,281 --> 01:34:02,500
progress on knowing what to look for and
how to.

2105
01:34:02,560 --> 01:34:07,560
How to know when we found it.
But if you want to talk about flexible 

2106
01:34:07,560 --> 01:34:07,560
planning,
things that are more like cognition that

2107
01:34:07,870 --> 01:34:10,120
go on and prefrontal Cortex,
right?

2108
01:34:10,630 --> 01:34:11,710
I,
at this point,

2109
01:34:11,711 --> 01:34:16,711
I don't.
I don't think that just by recording 

2110
01:34:16,711 --> 01:34:18,751
from those neurons,
we're going to be able to answer those 

2111
01:34:18,751 --> 01:34:18,751
questions in a meaningful engineering 
way.

2112
01:34:18,751 --> 01:34:20,260
A way that that any engineer,
software,

2113
01:34:20,261 --> 01:34:21,220
hardware,
whatever,

2114
01:34:21,400 --> 01:34:22,150
could really say,
yeah,

2115
01:34:22,151 --> 01:34:22,840
okay,
I get it.

2116
01:34:22,841 --> 01:34:27,841
I get those insights in a way that I can
engineer with and that's what my goal 

2117
01:34:27,841 --> 01:34:27,841
is,
right?

2118
01:34:27,841 --> 01:34:31,680
So my goal,
that's my goal to do at the software 

2119
01:34:31,680 --> 01:34:31,680
level,
the hardware level or the entire systems

2120
01:34:31,680 --> 01:34:32,830
level,
connecting them and I think that,

2121
01:34:32,920 --> 01:34:37,920
you know,
we can do that by taking what we're 

2122
01:34:37,920 --> 01:34:37,920
doing and bringing into contact with 
people studying neural circuits,

2123
01:34:37,920 --> 01:34:40,360
but I don't think you can,
you can leave this level out and just go

2124
01:34:40,361 --> 01:34:42,360
straight to the neural circuits and I 
think the more you have,

2125
01:34:42,430 --> 01:34:47,430
the more progress we make,
the more we can help people who are 

2126
01:34:47,430 --> 01:34:50,580
studying at the neuro circuit level and 
they can help us address these other 

2127
01:34:50,580 --> 01:34:53,761
engineering questions that we don't 
really have access to like the power 

2128
01:34:53,761 --> 01:34:53,761
issue or the speed issue.

2129
01:34:53,761 --> 01:34:54,370
Okay.
Thanks.

2130
01:34:54,520 --> 01:34:57,030
That was great.
Maybe give Janssen again

2131
01:34:57,070 --> 01:35:02,070
man.
Thanks.

