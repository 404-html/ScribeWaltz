1
00:00:00,060 --> 00:00:01,740
Welcome back to six,
says zero,

2
00:00:01,741 --> 00:00:04,050
nine,
nine artificial general intelligence.

3
00:00:04,080 --> 00:00:09,080
Today we have Eylea says Giver,
Cofounder and Research Director of Open 

4
00:00:12,721 --> 00:00:15,900
Ai.
He started in the aml group in Toronto.

5
00:00:15,901 --> 00:00:18,990
Geoffrey Hinton,
then at Stanford with Andrew Wang.

6
00:00:19,080 --> 00:00:24,080
Co founded DNN research for three years 
as a research scientist at Google brain 

7
00:00:24,080 --> 00:00:28,290
and finally cofounded open ai citations 
aren't everything,

8
00:00:28,590 --> 00:00:32,190
but they do indicate impact.
And his work,

9
00:00:32,370 --> 00:00:37,370
recent work in the past five years has 
been cited over 46,000

10
00:00:38,131 --> 00:00:43,131
times.
He has been the key creative intellect 

11
00:00:43,131 --> 00:00:46,630
and driver behind some of the biggest 
breakthrough ideas and deep learning and

12
00:00:46,631 --> 00:00:51,631
artificial intelligence ever.
So please welcome Eylea.

13
00:00:56,890 --> 00:00:58,680
Thanks.

14
00:00:59,460 --> 00:01:01,080
Alright,
thanks for the introduction next.

15
00:01:02,220 --> 00:01:04,410
Alright,
thanks for coming to my talk.

16
00:01:04,530 --> 00:01:09,530
I will tell you about some work we've 
done over the past year on on Melania 

17
00:01:09,761 --> 00:01:14,761
and self at open Ai and before I dive 
into some of the more technical details 

18
00:01:15,961 --> 00:01:20,961
of the work,
I wanted to spend a little bit of time 

19
00:01:20,961 --> 00:01:24,500
talking about deep learning and why it 
works at all in the first place,

20
00:01:26,010 --> 00:01:28,710
which I think it's actually not as self 
evident,

21
00:01:28,760 --> 00:01:33,760
seeing that they should work.
One fact it's actually a fact,

22
00:01:35,010 --> 00:01:40,010
it's a mathematical theory that you can 
prove is that if you could find the this

23
00:01:42,240 --> 00:01:46,740
program does very,
very well in your data,

24
00:01:47,310 --> 00:01:52,310
then you will achieve the best 
generalization possible with a little 

25
00:01:52,310 --> 00:01:56,121
bit of modification.
You can turn it into a precise theory 

26
00:01:56,121 --> 00:01:59,390
and on a very intuitive level,
it's easy to see why it shouldn't be the

27
00:01:59,551 --> 00:02:00,090
case.

28
00:02:00,900 --> 00:02:05,900
If you have some data and you're able to
find a short term program which 

29
00:02:06,121 --> 00:02:11,121
generates this data,
then you've essentially extracted all 

30
00:02:11,121 --> 00:02:14,631
the old conceivable regularity from this
data into your program and then you can 

31
00:02:14,631 --> 00:02:16,560
use this objects to make the best 
predictions possible.

32
00:02:18,050 --> 00:02:23,050
If if you have data which is so complex 
and there is no way to express it as a 

33
00:02:24,361 --> 00:02:27,660
shorter program and it means that your 
data is totally random,

34
00:02:28,170 --> 00:02:31,080
there is no way to extract any 
irregularity from it whatsoever.

35
00:02:32,370 --> 00:02:37,370
Now,
there is little known mathematical 

36
00:02:37,370 --> 00:02:37,370
theory behind this and the proofs of 
these statements.

37
00:02:37,370 --> 00:02:38,400
Actually,
not even that hard,

38
00:02:39,170 --> 00:02:44,170
but the one minor,
slight disappointment is that it's 

39
00:02:44,170 --> 00:02:48,051
actually not possible,
at least given today's tools and 

40
00:02:48,051 --> 00:02:50,931
understanding to find the best short 
program that explains or generates or 

41
00:02:52,830 --> 00:02:55,080
solves your problem.
Given your data.

42
00:02:55,710 --> 00:02:57,650
This problem is computationally 
intractable.

43
00:02:59,270 --> 00:03:03,460
The space of old programs is a very 
nasty space.

44
00:03:03,970 --> 00:03:08,970
Small changes to a program result in 
massive changes in the behavior of the 

45
00:03:08,970 --> 00:03:09,760
program as it should be.
It makes sense.

46
00:03:10,090 --> 00:03:12,940
We have a loop.
We changed the inside of the loop.

47
00:03:13,420 --> 00:03:14,950
Of course you get something totally 
different,

48
00:03:15,760 --> 00:03:19,300
so the space of programs is so hard,
at least given what we know today,

49
00:03:19,301 --> 00:03:24,301
search,
there seems to be completely off the 

50
00:03:24,301 --> 00:03:24,750
table.
Well,

51
00:03:25,780 --> 00:03:28,720
if we give up on the short side on short
programs,

52
00:03:28,870 --> 00:03:32,290
what about small circuits?
Well,

53
00:03:32,770 --> 00:03:36,520
it turns out that we are lucky.
It turns out that when it comes to small

54
00:03:36,521 --> 00:03:41,521
circuits,
you can just find the best small 

55
00:03:41,521 --> 00:03:42,310
circuits circuit that solves a problem 
using backpropagation,

56
00:03:43,030 --> 00:03:48,030
and this is the miraculous fact on which
the rest of ai stands.

57
00:03:49,510 --> 00:03:54,510
It is the fact that then you have a 
circuit and you impose constraints on 

58
00:03:54,510 --> 00:03:56,230
your circuits.
On your circuit using data,

59
00:03:56,560 --> 00:03:59,860
you can find a way to satisfy these 
constraints,

60
00:04:00,340 --> 00:04:05,340
these constraints using backdrop by 
iteratively making small changes to the 

61
00:04:06,260 --> 00:04:11,260
weights of your neural network until its
predictions satisfy the data.

62
00:04:13,060 --> 00:04:18,060
Well,
this means is that the computational 

63
00:04:18,060 --> 00:04:18,820
problem that sober backpropagation is 
extremely profound.

64
00:04:19,630 --> 00:04:24,630
It is circuit search.
Now we know that you can solve with 

65
00:04:24,630 --> 00:04:27,660
solid all this,
but you can solve it sometimes and you 

66
00:04:28,091 --> 00:04:31,810
can solve it that those times where we 
have a practical data set,

67
00:04:32,170 --> 00:04:37,170
it is easy to design artificial data 
sets for which you not find the best 

68
00:04:37,170 --> 00:04:40,711
neural network,
but in practice it seems to be not a 

69
00:04:40,711 --> 00:04:43,801
problem.
Are you going to think of training 

70
00:04:43,801 --> 00:04:46,350
neural network as solving an equation?
In many cases where you have a large 

71
00:04:47,531 --> 00:04:50,750
number of equation terms like this,
Fox,

72
00:04:50,880 --> 00:04:55,880
Scifi equals way,
so you've got your parameters and they 

73
00:04:55,880 --> 00:04:59,331
are present all your degrees of freedom 
and you use gradient descent to push the

74
00:05:00,671 --> 00:05:04,210
information from these equations into 
the parameters of satisfy them all.

75
00:05:05,620 --> 00:05:10,620
And you can see that the neural network,
let's say one with 50 layers is 

76
00:05:10,620 --> 00:05:15,391
basically a parallel computer that is 
given 50 times steps to run and you can 

77
00:05:17,771 --> 00:05:19,760
look quite a lot to be the 50 slash 50 
times step,

78
00:05:19,761 --> 00:05:21,400
sort of very,
very powerful.

79
00:05:21,590 --> 00:05:24,700
I'm massively parallel computer.
So for example,

80
00:05:25,980 --> 00:05:30,980
I think it is not widely known that you 
can learn to sort sort an n beat numbers

81
00:05:36,190 --> 00:05:39,610
using a modestly sized neural network 
with just two hidden layers,

82
00:05:40,540 --> 00:05:43,780
which is not bad.
It's not self evident.

83
00:05:44,680 --> 00:05:49,680
Especially since we've been taught that 
sorting requires logan parallel steps 

84
00:05:49,680 --> 00:05:54,600
with a neural network,
you can sort successful using only two 

85
00:05:54,600 --> 00:05:58,231
parallel steps.
So there's some things like an obvious 

86
00:05:58,231 --> 00:06:01,020
going on now these a part of those steps
of thresholds for threshold neurons,

87
00:06:01,180 --> 00:06:04,190
so they're doing a little bit more work.
That's an answer to the mystery,

88
00:06:04,430 --> 00:06:07,160
but if you've got 50 such layers,
you can do quite a bit of logic,

89
00:06:07,280 --> 00:06:09,650
quite a bit of reasoning all inside the 
neural network.

90
00:06:10,250 --> 00:06:13,460
And that's why it works.
Given the data,

91
00:06:14,600 --> 00:06:19,600
we are able to find the best neural 
network and because the neural network 

92
00:06:19,600 --> 00:06:22,250
is deep because it can run computation 
inside of inside of its layers,

93
00:06:22,910 --> 00:06:26,900
the best neural network is worth finding
because that's really what you need.

94
00:06:27,050 --> 00:06:32,050
You need something you need to model 
class which is worth optimizing,

95
00:06:33,410 --> 00:06:38,410
but it also needs to be optimized.
Mobile and deep neural networks 

96
00:06:38,410 --> 00:06:40,700
satisfied both of these constraints,
and this is why everything works,

97
00:06:40,701 --> 00:06:43,670
so this is the basis on which everything
else resides.

98
00:06:45,350 --> 00:06:50,350
Now,
I wanted to talk a little bit about 

99
00:06:50,350 --> 00:06:50,350
reinforcement learning.
Reinforcement learning is a framework.

100
00:06:50,750 --> 00:06:55,750
It's a framework of evaluating agents in
their ability to achieve goals and 

101
00:06:56,381 --> 00:07:01,381
complicated stochastic environments.
You've got an agent which is plugged 

102
00:07:01,381 --> 00:07:05,411
into an environment as shown in the 
figure right here and for any given 

103
00:07:06,651 --> 00:07:11,651
agent,
you can simply run it many times and 

104
00:07:11,651 --> 00:07:13,160
computers average award.
Now,

105
00:07:13,250 --> 00:07:16,850
the thing that's interesting about the 
reinforcement learning framework is that

106
00:07:16,851 --> 00:07:21,851
there exist interesting,
useful reinforcement learning 

107
00:07:21,851 --> 00:07:24,500
algorithms.
The framework existed for a long time.

108
00:07:25,310 --> 00:07:28,820
It became interesting once we realized 
that good algorithms exist.

109
00:07:28,970 --> 00:07:30,650
Now,
these are perfect algorithms,

110
00:07:30,920 --> 00:07:35,920
but they're good enough to do 
interesting things and all you want.

111
00:07:36,650 --> 00:07:41,650
The mathematical problem is one where 
you need to maximize the expected 

112
00:07:41,650 --> 00:07:41,650
reward.

113
00:07:43,280 --> 00:07:48,280
Now,
one important way in which the 

114
00:07:48,280 --> 00:07:51,221
reinforcement learning framework is not 
quite complete is that it assumes that 

115
00:07:51,221 --> 00:07:53,450
the reward is given by the environment.
You see this picture,

116
00:07:54,090 --> 00:07:58,040
the agent sends an action while the 
reward sends it the observation,

117
00:07:58,100 --> 00:08:00,980
and they're both of the observation and 
the reward backwards.

118
00:08:01,460 --> 00:08:03,020
That's what the environment communicates
back.

119
00:08:04,410 --> 00:08:09,410
One way in which this is not the case in
the real world is that we figure out 

120
00:08:11,990 --> 00:08:15,410
what the reward is from the observation 
we reward ourselves.

121
00:08:15,850 --> 00:08:17,630
We are not told.
Environment doesn't say,

122
00:08:17,631 --> 00:08:19,190
Hey,
here's some negative reward.

123
00:08:20,180 --> 00:08:25,180
It's our interpretation over census that
lets us determine what the reward is and

124
00:08:26,151 --> 00:08:31,151
there is only one real reward in life 
and this is existence or nonexistence 

125
00:08:31,151 --> 00:08:32,570
and everything else is a corollary of 
that.

126
00:08:33,920 --> 00:08:36,980
So well,
what should the region be?

127
00:08:37,010 --> 00:08:42,010
You already know the answers should be a
your own network because whenever you 

128
00:08:42,010 --> 00:08:45,851
want to do something,
then it's going to be a neural network 

129
00:08:45,851 --> 00:08:48,491
and you want the agents to map 
observations to actions so you let it be

130
00:08:48,681 --> 00:08:49,760
parameterized within your own net.

131
00:08:49,760 --> 00:08:54,760
Then you apply learning algorithm,
so I want to explain to you how 

132
00:08:54,760 --> 00:08:57,731
reinforcement learning works.
This is model free reinforcement 

133
00:08:57,731 --> 00:08:57,870
learning.
Reinforcement learning has actually been

134
00:08:57,871 --> 00:09:02,871
using practice everywhere,
but he's also deeply in it.

135
00:09:02,950 --> 00:09:05,310
It's very robust.
It's very simple.

136
00:09:05,700 --> 00:09:08,850
It's also not very efficient,
so the way it works is the following.

137
00:09:08,851 --> 00:09:12,270
This is literally the one sentence 
description of what happens.

138
00:09:13,680 --> 00:09:16,590
In short,
try something new.

139
00:09:17,990 --> 00:09:22,990
I'd randomness directions and compare 
the results to your expectation.

140
00:09:24,960 --> 00:09:29,960
If the result surprises you.
If you find that the results exceeded 

141
00:09:30,091 --> 00:09:32,850
your expectation,
then change your parameters,

142
00:09:32,930 --> 00:09:34,290
but they could those actions in the 
future.

143
00:09:35,610 --> 00:09:40,610
That's it.
This is the full idea of reinforcement 

144
00:09:40,610 --> 00:09:40,610
learning.
Try it out,

145
00:09:40,610 --> 00:09:45,080
see if you like it,
and if you do more of that in the 

146
00:09:45,080 --> 00:09:45,630
future,
and that's it,

147
00:09:45,900 --> 00:09:48,030
that's literally,
this is the core idea.

148
00:09:48,810 --> 00:09:51,210
Now it turns out it's not difficult to 
formalize mathematically,

149
00:09:51,690 --> 00:09:54,330
but this is really what's going on in a 
neural network,

150
00:09:54,360 --> 00:09:57,690
in irregular neural network,
I guess you might say,

151
00:09:57,691 --> 00:09:59,370
okay,
what's the goal?

152
00:09:59,460 --> 00:10:00,540
Iran,
the neural network.

153
00:10:00,990 --> 00:10:05,190
You get an answer,
you compare it to the desired answer and

154
00:10:05,191 --> 00:10:06,510
whatever difference you have between 
those,

155
00:10:06,511 --> 00:10:11,511
do you send it back?
The changing the neural network that's 

156
00:10:11,511 --> 00:10:12,930
supervised learning and reinforcement 
learning.

157
00:10:13,670 --> 00:10:15,790
You're running your own network,
you had a bit of randomness,

158
00:10:15,810 --> 00:10:18,960
direction,
and then you feel like the result,

159
00:10:19,410 --> 00:10:22,530
you're randomness turns into the desired
target in effect,

160
00:10:23,290 --> 00:10:25,950
so that's it.
Trivial.

161
00:10:28,290 --> 00:10:33,290
Now math exists without explaining what 
these equations mean.

162
00:10:35,940 --> 00:10:38,700
The point is not for you to derive them,
but just to show that they exist.

163
00:10:39,780 --> 00:10:42,120
There are two classes of reinforcement 
learning algorithms.

164
00:10:42,570 --> 00:10:47,570
One of them is the policy gradients 
where basically what you do is that you 

165
00:10:47,570 --> 00:10:49,910
take this expression right there,
the will expect expected,

166
00:10:50,190 --> 00:10:53,850
some of rewards and it just crunched 
through the derivatives.

167
00:10:53,851 --> 00:10:58,851
You expand the terms,
Iran do some Algebra and you get the 

168
00:10:58,851 --> 00:11:03,351
derivative and miraculously the 
derivative has exactly the form that I 

169
00:11:05,941 --> 00:11:10,941
told you which is try some actions and 
if you like them increasing the low 

170
00:11:11,220 --> 00:11:13,890
probability of the actions.
That's literally follows from the math.

171
00:11:14,010 --> 00:11:19,010
It's very nice when the intuitive 
explanation as a one to one 

172
00:11:19,010 --> 00:11:19,680
correspondence to what you get in the 
equation.

173
00:11:20,160 --> 00:11:23,340
Even though you'll have to take my word 
for it if you're not familiar with it.

174
00:11:24,070 --> 00:11:29,070
That's the question.
At the top there was a different class 

175
00:11:29,070 --> 00:11:32,151
of reinforcement learning algorithms 
which is a little bit more difficult to 

176
00:11:32,151 --> 00:11:34,611
explain.
It's called a cue learning based 

177
00:11:34,611 --> 00:11:35,690
algorithms.
They are a bit less stable and beat,

178
00:11:35,700 --> 00:11:40,700
more simple efficient and it has the 
property is that it can learn not only 

179
00:11:42,841 --> 00:11:47,841
from the data generated by the actor but
from any other data as well.

180
00:11:47,851 --> 00:11:49,290
So it has,
it has some it,

181
00:11:49,410 --> 00:11:52,980
it has a different robustness profile.
It should be a little bit important,

182
00:11:53,350 --> 00:11:56,770
but it's only going to be the legality.
So yeah,

183
00:11:56,771 --> 00:11:59,110
this is the policy of policy 
distinction,

184
00:11:59,290 --> 00:12:03,130
but it's a little bit technical.
So if you find this hard to understand,

185
00:12:03,190 --> 00:12:04,970
don't worry about it.
If you already know,

186
00:12:05,230 --> 00:12:10,230
then you already know it.
So now what's the potential of 

187
00:12:10,230 --> 00:12:11,050
reinforcement learning?
Wasn't the promise.

188
00:12:11,770 --> 00:12:15,370
What is it actually why?
Why should we be excited about it?

189
00:12:16,330 --> 00:12:21,330
Now,
there are two reasons the reinforcement 

190
00:12:21,330 --> 00:12:22,690
learning algorithms have today already 
useful and interesting,

191
00:12:22,780 --> 00:12:25,870
and especially if you have a really good
simulation of your world,

192
00:12:25,871 --> 00:12:28,510
you could train agents to lots of 
interesting things,

193
00:12:31,060 --> 00:12:36,060
but what's really exciting is if you can
build a super amazing sample efficient 

194
00:12:36,060 --> 00:12:37,360
algorithms,
reinforcement learning algorithm,

195
00:12:37,690 --> 00:12:42,690
which is give it a tiny amount of data 
and the algorithm just crunches through 

196
00:12:42,690 --> 00:12:46,321
it and extract every bit of entropy out 
of it in order to learn in the fastest 

197
00:12:46,321 --> 00:12:47,770
way possible.
Now,

198
00:12:47,830 --> 00:12:50,350
today or algorithms are not particularly
efficient.

199
00:12:50,380 --> 00:12:54,970
They are data inefficient,
but as our field keeps making progress,

200
00:12:55,000 --> 00:12:58,120
this will change.
Next,

201
00:12:58,150 --> 00:13:00,340
I want to dive into the topic of metal 
learning.

202
00:13:02,690 --> 00:13:07,690
The goal of metal learning.
So Methadone is a beautiful idea that 

203
00:13:07,690 --> 00:13:12,211
doesn't really work,
but it kind of works and it's really 

204
00:13:12,211 --> 00:13:13,000
promising too.
It's another promising idea.

205
00:13:14,610 --> 00:13:18,100
So what's the dream?
We have some learning algorithms,

206
00:13:19,150 --> 00:13:22,330
perhaps you could use those learning 
algorithm is in order to learn to learn.

207
00:13:23,680 --> 00:13:28,210
I'd be nice if we could learn to learn,
so how can you do that?

208
00:13:28,810 --> 00:13:33,810
It will take a system which you train it
not on one task but on many tasks and 

209
00:13:36,521 --> 00:13:41,521
you ask it if it learns to solve these 
tasks quickly and that may actually be 

210
00:13:41,521 --> 00:13:43,300
enough.
So here's how it looks like.

211
00:13:43,301 --> 00:13:48,301
Here's how most traditional method 
learning look works like a looks like 

212
00:13:48,301 --> 00:13:49,600
you have a model which is a big neural 
network,

213
00:13:50,950 --> 00:13:55,950
but what you do is that you treat every 
instead of training cases,

214
00:13:57,400 --> 00:13:59,890
you have training tasks and instead of 
test cases,

215
00:13:59,891 --> 00:14:04,210
you have test tasks,
so your input maybe instead of just your

216
00:14:04,211 --> 00:14:07,600
current test case,
it will be all the information about the

217
00:14:07,601 --> 00:14:10,690
news,
about the test tasks plus the test case,

218
00:14:10,990 --> 00:14:14,620
and you'll try to output the prediction 
reaction for that test case.

219
00:14:14,830 --> 00:14:16,930
So basically you say,
yeah,

220
00:14:16,990 --> 00:14:20,740
I'm going to give you your 10 examples 
as part of your input to your model,

221
00:14:21,010 --> 00:14:22,630
figured out how to make the best use of 
them.

222
00:14:24,280 --> 00:14:29,280
It's a really straightforward idea.
You turn the neural network into the 

223
00:14:29,280 --> 00:14:34,261
learning algorithm by turning a training
task into a training case,

224
00:14:34,750 --> 00:14:37,060
so training,
task it constraining case.

225
00:14:37,480 --> 00:14:42,480
This is learning this one sentence and 
so there've been several success stories

226
00:14:45,071 --> 00:14:50,071
which I think are very interesting.
One of the success stories of learning 

227
00:14:50,080 --> 00:14:52,940
is learning to recognize characters 
quickly.

228
00:14:53,390 --> 00:14:56,930
So they've been a dataset produced by 
MIT,

229
00:14:56,931 --> 00:15:01,931
by Lake at all,
and this is a data set.

230
00:15:02,691 --> 00:15:07,691
We have a large number of different 
handwritten characters and people have 

231
00:15:07,691 --> 00:15:12,161
been able to train extremely strong 
metal learning system for this task and 

232
00:15:12,161 --> 00:15:17,051
now the successful and other very 
successful example of metal learning is 

233
00:15:17,051 --> 00:15:20,440
neural architecture.
Search by is openly from Google where 

234
00:15:21,590 --> 00:15:24,440
they found a neural architecture that 
sold one problem,

235
00:15:24,441 --> 00:15:29,441
well small problem and then you could 
generalize it and then it will 

236
00:15:29,441 --> 00:15:29,441
successfully solve large problems as 
well.

237
00:15:29,441 --> 00:15:34,210
So this is the kind of the,
the small number of bits matter 

238
00:15:34,300 --> 00:15:39,300
learning.
It's like when you learn the 

239
00:15:39,300 --> 00:15:39,300
architecture or maybe even learn a 
program,

240
00:15:39,300 --> 00:15:40,340
a small program or learning algorithm,
and she applied to new tasks.

241
00:15:40,640 --> 00:15:42,830
So this is the other way of doing metal 
learning.

242
00:15:43,490 --> 00:15:45,950
So anyway,
but the point is what's happening,

243
00:15:46,010 --> 00:15:51,010
what's really happening in matter 
learning in most cases is that you turn 

244
00:15:51,010 --> 00:15:55,070
a training task into a training case and
pretend that this is totally normal,

245
00:15:55,100 --> 00:15:55,910
normal,
deep learning.

246
00:15:56,360 --> 00:15:59,360
That's it.
This is the entirety of metal learning.

247
00:15:59,361 --> 00:16:04,361
Everything else that just minor details.
Next I want to dive in.

248
00:16:05,150 --> 00:16:07,280
So now that I finished the introduction 
section,

249
00:16:07,550 --> 00:16:12,550
I want to start discussing different 
work by different people from open ai 

250
00:16:12,830 --> 00:16:15,350
and I want to start by talking about 
mindset experience,

251
00:16:15,351 --> 00:16:20,351
replay.
There's been a large effort by 

252
00:16:20,351 --> 00:16:20,351
unrecalled.
Richard,

253
00:16:20,351 --> 00:16:24,240
I'll develop a learning algorithm for 
reinforcement learning that doesn't 

254
00:16:25,821 --> 00:16:30,821
solve just one task,
but it solves many tasks and it learns 

255
00:16:30,821 --> 00:16:34,370
to make use of it's experience in a much
more efficient way.

256
00:16:35,530 --> 00:16:38,300
And I want to discuss one problem in 
reinforcement learning.

257
00:16:38,600 --> 00:16:43,600
It's actually,
I guess a set of problems which are 

258
00:16:43,600 --> 00:16:46,691
related to each other,
like one really important thing you need

259
00:16:48,471 --> 00:16:53,471
to learn to do is to explore your in 
that you start out in an environment you

260
00:16:53,811 --> 00:16:55,760
don't know what to do,
what do you do?

261
00:16:56,290 --> 00:16:59,780
So one very important thing that has to 
happen is that you must get rewards from

262
00:16:59,781 --> 00:17:04,781
time to time.
If you try something and you don't get 

263
00:17:04,781 --> 00:17:06,800
rewards,
then how can you learn?

264
00:17:08,170 --> 00:17:10,940
So said that's the kind of the crux of 
the problem.

265
00:17:11,300 --> 00:17:14,030
How do you learn?
And relatedly,

266
00:17:15,140 --> 00:17:19,010
is there any way to meaningfully benefit
from your,

267
00:17:19,150 --> 00:17:21,470
from the experience,
from your attempts to,

268
00:17:21,810 --> 00:17:22,970
from,
from your failures.

269
00:17:23,300 --> 00:17:25,280
If you try to achieve a goal and you 
fail,

270
00:17:25,310 --> 00:17:30,310
can you still learn from it?
You tell you instead of asking your 

271
00:17:30,310 --> 00:17:32,300
algorithm to achieve a single goal,
you want to learn a policy that they can

272
00:17:32,301 --> 00:17:33,890
achieve a very large family of goals.

273
00:17:34,340 --> 00:17:36,410
For example,
instead of reaching one state,

274
00:17:36,740 --> 00:17:40,490
you want to learn a policy that reaches 
every state on your system.

275
00:17:40,760 --> 00:17:41,750
Now,
what's the implication?

276
00:17:42,710 --> 00:17:45,980
Anytime you do something,
you achieved some state.

277
00:17:46,820 --> 00:17:49,830
So let's suppose you say I want to 
achieve state A.

278
00:17:51,180 --> 00:17:54,120
I try my best and I ended up achieving 
state B.

279
00:17:55,920 --> 00:17:58,170
I can either conclude while that was 
disappointing,

280
00:17:58,230 --> 00:18:01,800
I haven't flown almost anything.
I still have no idea how to,

281
00:18:02,010 --> 00:18:05,310
how to achieve state aid.
But alternatively I can say,

282
00:18:05,311 --> 00:18:06,330
well,
wait a second,

283
00:18:06,750 --> 00:18:08,540
I've just reached the perfectly good 
state,

284
00:18:08,670 --> 00:18:11,450
which is b,
can I learn how to achieve state,

285
00:18:11,470 --> 00:18:15,510
be for my attempt to achieve state a and
answer is yes,

286
00:18:15,511 --> 00:18:20,511
you can,
and it just works and I just want to 

287
00:18:20,511 --> 00:18:23,331
point out this is the one case,
there's a small subtlety here which may 

288
00:18:23,820 --> 00:18:28,820
be interesting to those of you who are 
very familiar with the distinction 

289
00:18:28,820 --> 00:18:30,120
between non policy and off policy.

290
00:18:31,230 --> 00:18:36,230
When you try to achieve a,
you are on your own policy learning for 

291
00:18:36,230 --> 00:18:41,090
reaching the state a,
but you're doing off policy learning 

292
00:18:41,090 --> 00:18:44,450
fruition to state b because you will 
take different actions if you actually 

293
00:18:44,450 --> 00:18:48,081
try to reach them beat.
So that's why it's very important that 

294
00:18:48,081 --> 00:18:49,230
the algorithm you use here can support 
of policy learning.

295
00:18:49,800 --> 00:18:52,680
But that's a minor technicality at the 
crux.

296
00:18:52,740 --> 00:18:57,740
The crux of the idea is you make the 
problem easier by ostensibly making it 

297
00:18:58,621 --> 00:19:03,621
harder by training assistant which can,
which aspires to reach,

298
00:19:03,780 --> 00:19:07,290
to learn to reach every state,
to learn to achieve every goal,

299
00:19:07,500 --> 00:19:10,140
to learn to master it's environment.
In general,

300
00:19:10,650 --> 00:19:14,670
you build a system which always learned 
something,

301
00:19:15,030 --> 00:19:20,030
it learns from success as well as from 
failure because if it tries to do one 

302
00:19:20,030 --> 00:19:20,940
thing,
one thing and it does something else,

303
00:19:21,510 --> 00:19:23,010
it now Australian data for how to 
achieve.

304
00:19:23,010 --> 00:19:28,010
That's something else we want to show 
you a video of how this thing works in 

305
00:19:28,010 --> 00:19:31,701
practice.
So one challenge in reinforcement 

306
00:19:31,701 --> 00:19:33,780
learning systems is the need to shape 
the reward.

307
00:19:34,260 --> 00:19:38,220
So what does it mean?
It means that at the beginning of the at

308
00:19:38,240 --> 00:19:39,420
the,
at the start of learning,

309
00:19:39,421 --> 00:19:43,680
then the system doesn't know much,
it will probably not achieve your goal,

310
00:19:43,830 --> 00:19:48,830
and so it's important that you design 
your reward function to give it gradual 

311
00:19:48,830 --> 00:19:51,981
increments to make it smooth and 
continuous so that even when the system 

312
00:19:51,981 --> 00:19:51,981
is not very good,
it achieves the goal.

313
00:19:52,230 --> 00:19:53,550
Now,
if you give your state,

314
00:19:53,551 --> 00:19:58,551
your system a very sparse report where 
the reward is achieved only when you 

315
00:19:58,551 --> 00:20:02,031
reach a final state,
very hard for normal reinforcement 

316
00:20:03,051 --> 00:20:08,051
learning algorithms to solve the problem
because naturally you'll never get the 

317
00:20:08,051 --> 00:20:09,170
reward so you never learn.
No reward means no learning.

318
00:20:10,100 --> 00:20:15,100
But here,
because you learn from failure as well 

319
00:20:15,100 --> 00:20:18,251
as some success,
there's this problem simply doesn't 

320
00:20:18,251 --> 00:20:18,251
occur.
And so this,

321
00:20:18,251 --> 00:20:19,100
this,
this is nice.

322
00:20:19,101 --> 00:20:20,500
I think,
you know,

323
00:20:20,510 --> 00:20:25,510
let's,
let's look at the videos a little bit 

324
00:20:25,510 --> 00:20:26,741
more.
Like it's nice how say it confidently 

325
00:20:26,741 --> 00:20:28,100
and energetically moves the little green
buck.

326
00:20:28,160 --> 00:20:29,930
Please target.
And here's another one.

327
00:20:50,030 --> 00:20:51,800
Okay.
So we can skip the physical.

328
00:20:51,850 --> 00:20:55,060
It works on the physical robot as well,
but he can skip it.

329
00:20:56,440 --> 00:20:59,650
So I think the point is the hindsight 
central experience,

330
00:20:59,651 --> 00:21:04,651
replay algorithm is directionally 
correct because you want to make use of 

331
00:21:06,730 --> 00:21:09,130
all your data and not only a small 
fraction of it.

332
00:21:10,000 --> 00:21:14,770
Now one huge question is where do you 
get the high level states?

333
00:21:15,640 --> 00:21:17,410
Where do the high level states come 
from?

334
00:21:19,120 --> 00:21:21,580
Because in the work that I've shown you 
so far,

335
00:21:22,660 --> 00:21:24,610
the system is asking your goal in the 
states.

336
00:21:25,030 --> 00:21:30,030
So I think one thing too will become 
very important for these kinds of 

337
00:21:30,030 --> 00:21:30,400
approaches.
Is it a presentation,

338
00:21:30,401 --> 00:21:33,910
learning and unsupervised learning.
Figure out what are the rates,

339
00:21:33,970 --> 00:21:38,970
what are the right states?
What's the state space of goals that's 

340
00:21:38,970 --> 00:21:38,970
worth achieving?

341
00:21:41,320 --> 00:21:46,320
Now I want to go through some real 
method learning results and I'll show 

342
00:21:47,891 --> 00:21:52,891
you a very simple way of doing to real 
from simulation to the physical.

343
00:21:54,011 --> 00:21:55,720
Robert,
with Metta learning,

344
00:21:56,830 --> 00:22:01,830
and this is where went panadol was in a 
an entrance and an a really nice intern 

345
00:22:01,830 --> 00:22:05,881
project and vain 17,
so I think we can agree that in the 

346
00:22:07,421 --> 00:22:11,680
domain of robotics,
it would be nice if you could train your

347
00:22:11,681 --> 00:22:16,681
policy in simulation and then somehow 
this knowledge would carry over to the 

348
00:22:18,581 --> 00:22:22,360
physical robbed.
Now we can build,

349
00:22:22,600 --> 00:22:27,600
we can build simulators that are okay,
but they can never perfectly match the 

350
00:22:28,901 --> 00:22:31,900
real world unless you want to have an 
insanely slow simulator.

351
00:22:32,590 --> 00:22:37,590
And the reason for that is that it turns
out that stimulating freakers simulating

352
00:22:38,021 --> 00:22:41,710
contacts is super hard.
And I heard somewhere,

353
00:22:41,711 --> 00:22:45,220
correct me if I'm wrong,
that simulating friction is NP complete.

354
00:22:45,640 --> 00:22:49,390
I'm not sure,
but it's like stuff like that.

355
00:22:49,780 --> 00:22:53,710
So your simulation is just not going to 
match reality.

356
00:22:53,860 --> 00:22:55,510
There'll be some resemblance,
but that's it.

357
00:22:56,230 --> 00:23:01,230
How can we address this problem?
And I wanted to show you one simple 

358
00:23:01,230 --> 00:23:01,230
idea.

359
00:23:03,430 --> 00:23:08,430
So let's say one thing.
What's one thing that would be nice is 

360
00:23:08,921 --> 00:23:13,921
that if you could learn a policy,
learn a policy that will quickly adapt 

361
00:23:13,921 --> 00:23:17,620
itself to the real world.
Well,

362
00:23:17,621 --> 00:23:19,990
if you want to learn a policy that can 
quickly adapt,

363
00:23:20,290 --> 00:23:25,290
we need to make sure that it has 
opportunities to adapt you in training 

364
00:23:25,290 --> 00:23:25,290
time.
So what do we do?

365
00:23:25,540 --> 00:23:30,070
Instead of solving a problem in just one
simulator,

366
00:23:30,670 --> 00:23:33,220
we add a huge amount of variability to 
the simulator.

367
00:23:33,620 --> 00:23:38,140
We say we will randomize the friction so
we will randomize the masses,

368
00:23:38,530 --> 00:23:43,530
the length of the different objects and 
their I guess and dimensions.

369
00:23:44,560 --> 00:23:49,560
So you tried to randomize physics,
the simulator and lots of different 

370
00:23:49,560 --> 00:23:53,951
ways,
and then importantly you don't tell the 

371
00:23:53,951 --> 00:23:55,640
policy how we randomized it,
so what is it going to do?

372
00:23:55,641 --> 00:23:58,150
Then you'll take your policy and you put
it in an environment.

373
00:23:58,150 --> 00:23:58,820
Then says,
well,

374
00:23:58,850 --> 00:24:00,260
this is really,
really tough.

375
00:24:00,410 --> 00:24:03,560
I don't know what the masses are and I 
don't know what the frictions are.

376
00:24:03,890 --> 00:24:07,220
I need to try things out and figure out 
where the friction is.

377
00:24:07,340 --> 00:24:11,030
As I get responses from the environment.
So you build it,

378
00:24:11,040 --> 00:24:16,040
you learn a certain degree of 
adaptability into the policy and it 

379
00:24:16,611 --> 00:24:18,950
actually works.
I just want to show you,

380
00:24:19,130 --> 00:24:24,130
this is what happens when you just 
straight up policy in simulation and 

381
00:24:24,130 --> 00:24:28,421
deployed on the physical robots and here
the goal is to bring the hockey puck 

382
00:24:28,421 --> 00:24:32,300
towards the red dots and you will see 
that it will struggle

383
00:24:37,850 --> 00:24:42,850
and there isn't it struggles is because 
of the systematic differences between 

384
00:24:42,850 --> 00:24:45,580
the simulator and the real physical 
Robert.

385
00:24:47,290 --> 00:24:52,290
So I could even the basic movement is 
difficult for the policy because the 

386
00:24:52,290 --> 00:24:54,820
assumptions of allocated so much.
So if you do the training,

387
00:24:54,821 --> 00:24:59,821
as I discussed,
we train the recurrent neural network 

388
00:24:59,821 --> 00:25:02,171
policy which learns to quickly infor 
properties of the simulator in order to 

389
00:25:03,321 --> 00:25:05,900
accomplish the task.
You can give it to the real thing,

390
00:25:06,590 --> 00:25:08,720
the real physics and it will do much 
better.

391
00:25:10,150 --> 00:25:12,890
Now this is not a perfect technique,
but it's definitely very promising.

392
00:25:12,891 --> 00:25:17,180
It's promising whenever you are able to 
sufficiently randomized simulator.

393
00:25:19,070 --> 00:25:22,820
So he's definitely very nice to see the 
closed loop nature of the policy.

394
00:25:22,821 --> 00:25:27,821
You consider that it would push the 
hockey puck and he would correct it 

395
00:25:27,821 --> 00:25:31,600
very,
very gently to bring into the goal so 

396
00:25:31,600 --> 00:25:31,600
that,
that was cool.

397
00:25:33,830 --> 00:25:36,040
So that was very,
uh,

398
00:25:36,041 --> 00:25:36,450
that,
that,

399
00:25:36,451 --> 00:25:38,210
that was a cool application of metal 
learning.

400
00:25:39,380 --> 00:25:41,950
I want to discuss one more application 
of Methadone,

401
00:25:41,990 --> 00:25:45,410
which is learning the hierarchy of 
actions.

402
00:25:46,790 --> 00:25:49,040
And this was working with and by France 
at all.

403
00:25:49,310 --> 00:25:52,370
Actually I'm Kevin France or the 
intranasal did.

404
00:25:52,371 --> 00:25:55,160
It wasn't high school.
I mean he wrote this paper.

405
00:25:57,440 --> 00:25:58,430
So

406
00:26:02,400 --> 00:26:07,400
one thing that would be nice is if 
reinforcement learning was hierarchical,

407
00:26:09,280 --> 00:26:11,670
if instead of simply taking my 
corrections,

408
00:26:11,910 --> 00:26:16,080
you had some kind of util sub routines 
that you could deploy.

409
00:26:16,410 --> 00:26:18,390
Maybe the term sub routine is a little 
bit too crude,

410
00:26:18,391 --> 00:26:22,860
but if you had some idea of which action
primitives are,

411
00:26:23,280 --> 00:26:28,280
we're starting to now no one has been 
able to get actually like real value add

412
00:26:31,861 --> 00:26:36,861
from curriculum reinforcement learning.
Yet so far all the really cool results 

413
00:26:36,861 --> 00:26:38,400
or that really convincing is also 
reinforcement learning does not use it.

414
00:26:39,900 --> 00:26:44,900
That's because we haven't quite figured 
out what's the right way for 

415
00:26:44,900 --> 00:26:46,620
reinforcement learning.
Reinforcement learning.

416
00:26:48,190 --> 00:26:53,190
I just want to show you one very simple 
approach where you use metal learning to

417
00:26:55,120 --> 00:26:58,580
learn to the hierarchy of actions.
So here's what you do.

418
00:26:59,720 --> 00:27:04,720
You have in this specific work,
you have a certain yellow,

419
00:27:05,270 --> 00:27:08,640
let's say you have a certain number of 
low level primitive,

420
00:27:08,670 --> 00:27:13,670
so let's say you have to of them and you
have a distribution of tasks and your 

421
00:27:14,541 --> 00:27:19,541
goal is to learn low level primitives 
such that when they are used inside a 

422
00:27:23,541 --> 00:27:26,570
very brief run of some reinforcement 
learning algorithm,

423
00:27:26,860 --> 00:27:28,520
it will make as much progress as 
possible.

424
00:27:30,020 --> 00:27:33,560
So the idea is you want to get the 
greatest amount of progress.

425
00:27:33,610 --> 00:27:36,960
You want to learn policies that result 
in the great.

426
00:27:36,961 --> 00:27:41,961
Sorry.
You want to learn primitives the result 

427
00:27:41,961 --> 00:27:43,940
in the greatest amount of progress 
possible when used inside learning.

428
00:27:44,510 --> 00:27:47,570
So this is a mental learning center 
because any distribution of tasks,

429
00:27:47,750 --> 00:27:50,810
and here we've had before,
we've had a little maze,

430
00:27:51,970 --> 00:27:56,970
he have a distribution of amazes and in 
this case and the little bug learned 

431
00:27:56,970 --> 00:28:00,110
three policies which morbid in speaks to
direction.

432
00:28:00,410 --> 00:28:02,120
And as a result of having this 
hierarchy,

433
00:28:02,121 --> 00:28:03,770
you are able to solve problems really 
fast,

434
00:28:03,890 --> 00:28:06,800
but only when the hierarchies correct.
So correct.

435
00:28:06,801 --> 00:28:08,960
Called reinforcement learning is still a
work in progress.

436
00:28:08,990 --> 00:28:13,990
And this was in this work is an 
interesting proof point of how 

437
00:28:18,640 --> 00:28:20,490
curricular reinforcement would be like.
Hi,

438
00:28:20,491 --> 00:28:25,491
how correct.
Called reinforcement learning could be 

439
00:28:25,491 --> 00:28:27,351
like if it worked now I wanted to just 
spend one slide addressing the 

440
00:28:31,521 --> 00:28:33,800
limitations of high capacity method 
learning.

441
00:28:35,180 --> 00:28:37,760
The specific limitation is that

442
00:28:40,600 --> 00:28:45,600
the training task distribution has to be
equal to the test task distribution and 

443
00:28:46,661 --> 00:28:51,490
I think this is a real limitation 
because in reality you the new task that

444
00:28:51,491 --> 00:28:56,491
you want to learn to in some ways being 
fundamentally different from anything 

445
00:28:57,071 --> 00:28:58,780
you've seen so far.
So for example,

446
00:28:58,781 --> 00:29:01,630
if you go to school,
you learn lots of useful things,

447
00:29:02,680 --> 00:29:06,430
but then when you go to work,
only a fraction of this,

448
00:29:06,880 --> 00:29:08,560
of the things that you've learned 
carries over.

449
00:29:09,220 --> 00:29:14,220
Can you need to learn if he wouldn't 
have quite a few more things from 

450
00:29:14,220 --> 00:29:16,441
scratch.
So madeloni would struggle with that 

451
00:29:16,441 --> 00:29:18,190
because it really assumes that the 
training,

452
00:29:18,191 --> 00:29:23,191
the training data is the distribution 
over the training task has to be equals 

453
00:29:23,191 --> 00:29:24,970
with distribution over the test asks.
That's the limitation,

454
00:29:24,980 --> 00:29:29,980
the thing that as we develop better 
algorithms for being robust when the

455
00:29:33,240 --> 00:29:38,240
test tasks outside of the distribution 
of the training desk than metal and got 

456
00:29:38,240 --> 00:29:43,130
much better.
Now I want to talk about self play 

457
00:29:44,200 --> 00:29:49,200
thing.
Self plays a very cool topic that's 

458
00:29:49,200 --> 00:29:53,550
starting to get attention only now and I
want to start by reviewing very old work

459
00:29:55,480 --> 00:30:00,010
cold td Gammon.
It's back from the older way from 1992.

460
00:30:00,011 --> 00:30:03,650
So 26 years old now.
It was done by Jerry to Sarah.

461
00:30:04,120 --> 00:30:09,120
So this work is really incredible 
because it has so much relevance today.

462
00:30:15,100 --> 00:30:18,220
What they did basically they said,
okay,

463
00:30:18,730 --> 00:30:23,730
let's take two neural networks and lets 
them let them play against each other,

464
00:30:25,720 --> 00:30:28,750
let them play backgammon against each 
other and let them try.

465
00:30:28,810 --> 00:30:33,810
Let them be trained with culinary.
So it's a super modern approach and you 

466
00:30:35,741 --> 00:30:40,210
would think this was a paper from 2017 
except with you look at the splots.

467
00:30:40,240 --> 00:30:42,340
It shows that you only have 10 hidden 
units,

468
00:30:42,341 --> 00:30:44,260
20 hidden units,
40 and 84.

469
00:30:44,261 --> 00:30:49,261
The difference in colors where you 
noticed that the largest neural network 

470
00:30:49,261 --> 00:30:52,000
works best,
so in some ways not much has changed and

471
00:30:52,001 --> 00:30:57,001
this is the evidence and in fact they 
were able to beat the world champion and

472
00:30:58,050 --> 00:31:03,050
backgammon and they were able to 
discover new strategies that the best 

473
00:31:03,050 --> 00:31:06,451
human,
a backgammon players weren't a have not 

474
00:31:06,451 --> 00:31:07,870
noticed and they've determined that the 
strategy is covered,

475
00:31:07,871 --> 00:31:10,540
but ed gamut actually better.
So that's pure stuff.

476
00:31:10,541 --> 00:31:15,541
Play with cue learning which is which 
remained dormant until the Deq and work 

477
00:31:18,311 --> 00:31:19,540
with Atari Buddy of mine.

478
00:31:21,880 --> 00:31:26,880
So now other examples of self blame 
include Alphago Zero,

479
00:31:30,340 --> 00:31:35,340
which was able to learn to beat the 
world champion in go without using any 

480
00:31:35,340 --> 00:31:39,061
external data whatsoever.
And other results of this vein is by 

481
00:31:39,061 --> 00:31:40,650
open ai,
which is our daughter to bought,

482
00:31:40,960 --> 00:31:43,960
which was able to build the world 
champion on the one,

483
00:31:43,961 --> 00:31:48,961
the one version of the game.
And so I want to spend a little bit of 

484
00:31:49,841 --> 00:31:54,841
time talking about the allure of self 
play and why I think it's exciting.

485
00:31:57,580 --> 00:32:02,460
So one important problem,
that's it,

486
00:32:02,540 --> 00:32:07,540
that's the must face as we try to build 
truly intelligent systems is what is the

487
00:32:10,121 --> 00:32:15,121
task,
what are we actually teaching the 

488
00:32:15,121 --> 00:32:17,521
systems to do,
and one very attractive attribute of 

489
00:32:17,521 --> 00:32:21,811
self is that the agents create the 
environment by virtue of the agent 

490
00:32:26,321 --> 00:32:31,321
acting in the environment.
The environment becomes difficult for 

491
00:32:31,321 --> 00:32:35,701
the other agents and you can see here an
example of an iguana interacting with 

492
00:32:35,711 --> 00:32:40,711
snakes,
the try to eat it on successfully this 

493
00:32:40,711 --> 00:32:41,270
time so we can see what will happen in a
moment.

494
00:32:43,310 --> 00:32:48,310
The Guan Australian's best and so the 
fact that you have this arms race 

495
00:32:48,310 --> 00:32:53,231
between the snakes and the iguanas 
motivates their development potentially 

496
00:32:54,710 --> 00:32:59,710
without bound and this is what happened 
in effective but in biological 

497
00:32:59,710 --> 00:33:00,950
evolution.
Now,

498
00:33:00,980 --> 00:33:04,640
interesting work in this direction was 
done in 1994,

499
00:33:04,670 --> 00:33:09,670
but Carl,
since there is a really cool video on 

500
00:33:09,670 --> 00:33:10,880
youtube by Carl scenes,
you should check it out,

501
00:33:11,000 --> 00:33:13,070
which really kind of shows all the work 
that he's done.

502
00:33:14,180 --> 00:33:18,110
And here you have a little competition 
between agents where you evolve both the

503
00:33:18,111 --> 00:33:21,250
behavior and their morphology.
When you Walkman,

504
00:33:21,320 --> 00:33:26,320
the agents is trying to gain possession 
of a green cube and so you can see that 

505
00:33:28,371 --> 00:33:33,371
the agents create the challenge for each
other and that's why they need to 

506
00:33:33,371 --> 00:33:33,371
develop.

507
00:33:34,820 --> 00:33:39,820
So one thing that we deed and this is 
work by and sell it up from open ai is 

508
00:33:43,251 --> 00:33:43,700
we said,
okay,

509
00:33:43,701 --> 00:33:48,701
well can be demonstrates some unusual 
results in self play that would really 

510
00:33:49,580 --> 00:33:51,830
convince us that there is something 
there.

511
00:33:52,400 --> 00:33:54,650
So what we did here is that we created a
small,

512
00:33:55,110 --> 00:34:00,110
a small ring and you have these two 
humanoid figures and their goal is just 

513
00:34:00,110 --> 00:34:04,661
to push each other outside the ring and 
they don't know anything about 

514
00:34:04,661 --> 00:34:09,491
wrestling.
They don't know anything about standing 

515
00:34:09,491 --> 00:34:11,741
or balancing each other.
They don't know anything about centrals 

516
00:34:11,741 --> 00:34:14,080
gravity.
All they know is that if you don't do a 

517
00:34:14,080 --> 00:34:17,291
good job,
then your competition is going to do a 

518
00:34:17,291 --> 00:34:17,291
better job.
Now,

519
00:34:17,291 --> 00:34:21,821
one of the really attractive things 
about self play is that you always have 

520
00:34:24,800 --> 00:34:26,600
an opponent that's roughly as good as 
you are.

521
00:34:28,340 --> 00:34:33,340
In order to learn,
you need to sometimes in and sometimes 

522
00:34:33,340 --> 00:34:33,650
lose,
like you can't always win.

523
00:34:34,640 --> 00:34:37,130
Sometimes you must fail,
sometimes you must succeed,

524
00:34:39,490 --> 00:34:42,380
so let's see what will happen here.
Yeah,

525
00:34:42,800 --> 00:34:47,800
so it was able to be so the green human 
was able to block the ball in a so in a 

526
00:34:49,021 --> 00:34:50,630
well balanced environment.

527
00:34:52,850 --> 00:34:57,850
The competition is always level.
No matter how good you are or how bad 

528
00:34:57,850 --> 00:35:02,561
you are,
you have a competition that makes it 

529
00:35:02,561 --> 00:35:02,630
exactly,
exactly have exactly the right challenge

530
00:35:02,631 --> 00:35:04,970
for you in one thing here.
So this with your shows,

531
00:35:04,971 --> 00:35:09,971
transfer learning,
you take the little wrestling humanoid 

532
00:35:09,971 --> 00:35:12,740
and you take its friend away and you 
start applying a big,

533
00:35:12,741 --> 00:35:17,741
large random forces on it and you see if
it can maintain its balance and the 

534
00:35:17,741 --> 00:35:22,181
answer turns out to be that yes it can 
because it's been trained against an 

535
00:35:22,611 --> 00:35:27,611
opponent,
it pushes it and so that's why even if 

536
00:35:27,611 --> 00:35:29,270
it's doesn't understand where the 
pressure forces being applied on it,

537
00:35:29,450 --> 00:35:34,450
it's still able to balance itself.
So this is one potentially attractive 

538
00:35:34,671 --> 00:35:39,671
feature of software environments.
The QTC would learn a certain broad set 

539
00:35:39,671 --> 00:35:43,400
of skills,
although it's a little hard to control 

540
00:35:43,400 --> 00:35:46,971
those with the skills will be.
And so the biggest open question with 

541
00:35:46,971 --> 00:35:50,151
this research is how do you learn agents
in a software environment such that they

542
00:35:52,831 --> 00:35:57,831
do whatever they do,
but then they are able to solve a 

543
00:35:57,831 --> 00:35:59,850
battery of tasks that is useful for us 
that is explicitly specified externally.

544
00:36:01,720 --> 00:36:02,130
Yeah.

545
00:36:05,010 --> 00:36:09,660
I also want to want to highlight one 
attribute of self play environments that

546
00:36:09,720 --> 00:36:14,720
people observed in our daughter bought 
and that is that you've seen a very 

547
00:36:14,720 --> 00:36:15,680
rapid increase in the competence of the 
Bot.

548
00:36:16,020 --> 00:36:18,990
So over the period over the course of 
maybe five months,

549
00:36:18,991 --> 00:36:23,991
we've seen the bottom go from playing 
totally randomly all the way to the 

550
00:36:26,341 --> 00:36:30,360
world champion.
And the reason for that is that once you

551
00:36:30,361 --> 00:36:33,750
have a self clean environment,
if you put computing into it,

552
00:36:34,500 --> 00:36:39,500
you turn it into data.
Self play allows you to turn compute 

553
00:36:39,500 --> 00:36:44,060
into data and I think we will see a lot 
more of that as being an extremely 

554
00:36:44,060 --> 00:36:48,441
important thing to be able to turn 
compute into essentially data 

555
00:36:48,441 --> 00:36:52,251
generalization simply because the speed 
of neural net processors will increased 

556
00:36:52,251 --> 00:36:54,210
very dramatically over the next few 
years.

557
00:36:54,870 --> 00:36:58,650
So neural net cycles will be cheap and 
it will be important to make use of this

558
00:36:58,830 --> 00:37:01,560
new,
of newly found over abundance of cycles.

559
00:37:03,340 --> 00:37:07,170
I also wanted to talk a little bit about
the end game of the self blame approach.

560
00:37:08,400 --> 00:37:13,400
So one thing that we know about the 
human brain is that it has increased in 

561
00:37:13,831 --> 00:37:17,070
size fairly rapidly over the past 2 
million years.

562
00:37:18,660 --> 00:37:23,660
My theory,
the reason I think it happened is 

563
00:37:23,660 --> 00:37:27,021
because our ancestors got to a point 
where the thing that's most important 

564
00:37:28,021 --> 00:37:33,021
for your survival is your standing in 
the tribe and less the tiger and the 

565
00:37:33,301 --> 00:37:38,070
lion.
Once the most important thing is how you

566
00:37:38,071 --> 00:37:40,230
deal with those other things which have 
a large brain.

567
00:37:40,560 --> 00:37:42,330
Then it really helps to have a slightly 
larger brain.

568
00:37:43,110 --> 00:37:48,110
And I think that's what happened and 
that exists at least one paper from 

569
00:37:48,110 --> 00:37:48,990
science which supports this point of 
view.

570
00:37:50,130 --> 00:37:55,130
So apparently there has been convergent 
evolution between social apps and social

571
00:37:55,471 --> 00:38:00,000
birds even though in terms of various 
behaviors,

572
00:38:00,780 --> 00:38:05,780
even though the divergence in 
evolutionary timescale between humans 

573
00:38:05,851 --> 00:38:08,400
and birds as occurred a very long time 
ago,

574
00:38:08,580 --> 00:38:10,980
and humans and humans,
apes and humans,

575
00:38:11,040 --> 00:38:14,370
apes and birds have very different brain
structure.

576
00:38:16,440 --> 00:38:19,770
So I think what should happen if we 
succeed,

577
00:38:19,800 --> 00:38:22,950
if we successfully follow the path of 
this approach,

578
00:38:23,190 --> 00:38:28,190
is that we should create a society of 
agents which will have language and 

579
00:38:28,190 --> 00:38:30,690
theory of mind negotiation,
social skills,

580
00:38:31,350 --> 00:38:34,470
trade economy,
politics and justice system.

581
00:38:35,020 --> 00:38:40,020
All these things should happen inside 
the multiagent environment and it will 

582
00:38:40,020 --> 00:38:44,311
also be some alignment issue of how do 
you make sure that the agents we learn 

583
00:38:44,311 --> 00:38:44,500
behave innovative,
want.

584
00:38:45,220 --> 00:38:50,220
Now,
I want to make speculative digression 

585
00:38:50,220 --> 00:38:53,311
here,
which is I want to make the following 

586
00:38:54,960 --> 00:38:59,960
observations.
If you believe that this kind of society

587
00:39:01,241 --> 00:39:06,241
of agents is a plausible place where 
truly where the full fully general 

588
00:39:10,301 --> 00:39:15,301
intelligence will emerge and if you 
accept that our experience with Dota 

589
00:39:16,421 --> 00:39:18,640
both where we've seen a very rapid 
increase in competence,

590
00:39:18,650 --> 00:39:21,190
will carry over once all the details are
right.

591
00:39:21,910 --> 00:39:26,910
If you assume both of these conditions,
then it should follow that we should see

592
00:39:26,981 --> 00:39:31,981
a very rapid increase in the competence 
of our agents as they live in the 

593
00:39:32,261 --> 00:39:33,310
society of agents.

594
00:39:34,420 --> 00:39:39,420
So now that we've talked about 
potentially interesting way of 

595
00:39:40,690 --> 00:39:45,690
increasing the competence and teaching 
teaching social skills and language and 

596
00:39:45,881 --> 00:39:48,850
a lot of things that actually exist in 
humans as well.

597
00:39:49,150 --> 00:39:54,150
We want to talk a little bit about how 
you convey goals to Asians and the 

598
00:39:57,461 --> 00:39:59,500
question of conveying goal to eight 
calls to agents.

599
00:39:59,501 --> 00:40:04,501
It's just a technical problem,
but it will be important because it is 

600
00:40:05,770 --> 00:40:10,770
more likely than not that the agents 
that people train will eventually be 

601
00:40:12,400 --> 00:40:15,190
dramatically smarter than us.
And this is work by,

602
00:40:15,191 --> 00:40:20,191
um,
are they opening a safety team by Paul 

603
00:40:20,191 --> 00:40:23,360
Cristiana at all and others.
So I'm just going to show you this video

604
00:40:23,410 --> 00:40:25,690
which basically explains how the whole 
thing works.

605
00:40:27,480 --> 00:40:32,480
You there is some behavior looking for 
and you the human gets to see pairs of 

606
00:40:33,281 --> 00:40:38,281
behaviors and you simply click on the 
one that looks better and after a very 

607
00:40:42,160 --> 00:40:47,160
modest number of clicks,
you can get this little simulated leg to

608
00:40:49,541 --> 00:40:50,500
do backflips.

609
00:40:57,400 --> 00:41:02,400
There you go.
He can now do backflips and in this to 

610
00:41:02,400 --> 00:41:05,770
get this specific behavior,
it took about 500 clicks by human 

611
00:41:06,621 --> 00:41:11,621
annotators.
The way it works is that you take all 

612
00:41:11,621 --> 00:41:14,831
the,
so this is a very data efficient 

613
00:41:14,831 --> 00:41:17,561
reinforcement learning algorithm,
but it is efficient in terms of rewards 

614
00:41:17,561 --> 00:41:19,550
and not in terms of the environment 
interactions.

615
00:41:20,360 --> 00:41:22,880
So what you do here is that you take all
the clicks,

616
00:41:22,910 --> 00:41:27,020
so you've got your here is one here,
which is better than the other.

617
00:41:27,680 --> 00:41:32,680
You fit a reward function and numerical 
reward function to those.

618
00:41:33,470 --> 00:41:36,150
So you want to fit a reward function 
which satisfies those cleats clicks,

619
00:41:36,151 --> 00:41:39,570
and then you optimize this reward 
function with reinforcement learning and

620
00:41:39,690 --> 00:41:44,690
it actually works.
So this requires 500 bits of 

621
00:41:44,690 --> 00:41:48,431
information.
We've also been able to train him lots 

622
00:41:48,431 --> 00:41:49,550
of Atari Games using several thousand 
bits of information.

623
00:41:49,551 --> 00:41:54,551
So in all these cases you had human,
a human annotators or human judges just 

624
00:41:54,801 --> 00:41:59,801
like in the previous slide,
looking at the pairs of trajectories and

625
00:42:00,680 --> 00:42:05,680
clicking on the one that they thought 
was better and here's an example of an 

626
00:42:07,311 --> 00:42:09,560
unusual goal where this is a car racing 
game,

627
00:42:10,010 --> 00:42:15,010
but the goal was to ask the agent to 
train the white car drive right behind 

628
00:42:17,450 --> 00:42:18,290
the orange car.

629
00:42:18,740 --> 00:42:22,310
So it's a different goal and it was very
straightforward to communicate this goal

630
00:42:23,330 --> 00:42:28,330
using this approach.
So then to finish off alignment is a 

631
00:42:30,591 --> 00:42:32,090
technical problem.
It has to be solved,

632
00:42:32,960 --> 00:42:37,960
but of course the determination of the 
correct goals we want our systems to 

633
00:42:37,960 --> 00:42:42,881
have.
You'll be a very challenging political 

634
00:42:42,881 --> 00:42:43,460
problem.
And on this note,

635
00:42:43,520 --> 00:42:47,390
I want to thank you so much for your 
attention and I just want to say that it

636
00:42:47,391 --> 00:42:49,310
will be a happy hour at Cambridge 
brewing company.

637
00:42:49,490 --> 00:42:54,490
40th five.
If you want to chat more about Ai and 

638
00:42:54,490 --> 00:42:54,490
other topics,
please come by.

639
00:42:54,710 --> 00:42:56,360
I think that deserves an applause.

640
00:42:59,920 --> 00:43:03,330
Thank you.

641
00:43:03,530 --> 00:43:08,530
So my population is a neural networks at
buyer inspired,

642
00:43:09,020 --> 00:43:12,440
but backpropagation doesn't look as 
though it's what's going on in the brain

643
00:43:12,441 --> 00:43:17,441
because signals in the brain go one 
direction down the excellence where as 

644
00:43:17,441 --> 00:43:20,540
back propagation requires the areas to 
be publicated backup the wires.

645
00:43:20,600 --> 00:43:25,600
So can you just talk a little bit about 
that whole situation where it looks like

646
00:43:26,931 --> 00:43:31,931
the greatest doing something a bit 
different than our highest successful 

647
00:43:31,931 --> 00:43:35,530
algorithms algorithm is going to be 
improved once we figure out what the 

648
00:43:35,530 --> 00:43:39,341
brain is doing or what is the brain 
sending signals back even though it's 

649
00:43:39,341 --> 00:43:41,750
got no obvious way of doing that.
What's what's happening in that area?

650
00:43:42,250 --> 00:43:44,980
So that's a great question.
So first of all,

651
00:43:44,981 --> 00:43:48,910
I'll say that the true answer is that 
the honest answer is that I don't know,

652
00:43:48,911 --> 00:43:53,911
but I have opinions.
And so I'll say two things.

653
00:43:55,040 --> 00:43:59,400
First of all,
given that like if you agree,

654
00:43:59,420 --> 00:44:02,440
if me agreed.
So rather it is a true fact.

655
00:44:02,920 --> 00:44:06,790
Backpropagation solves the problem of 
circuit search.

656
00:44:07,930 --> 00:44:10,990
This problem feels like an extremely 
fundamental problem.

657
00:44:12,070 --> 00:44:14,590
And for this reason I think that it's 
unlikely to go away.

658
00:44:15,010 --> 00:44:19,500
Now you also right that the brain 
doesn't obviously do backpropagation,

659
00:44:19,540 --> 00:44:21,680
although they've been in multiple 
proposals of how it could be,

660
00:44:21,970 --> 00:44:23,590
how it could be doing them,
for example,

661
00:44:23,890 --> 00:44:28,890
there's been a work by and others where 
they've shown that if you use that,

662
00:44:31,261 --> 00:44:36,261
it's possible to learn a different set 
of connections that can be used for the 

663
00:44:36,261 --> 00:44:38,250
backward pass and that can result in 
successful learning.

664
00:44:38,640 --> 00:44:43,640
Now the reason this hasn't been really 
pushed to the limit by practitioners is 

665
00:44:43,640 --> 00:44:44,370
because they say,
well,

666
00:44:44,371 --> 00:44:47,790
I got tf the gradients.
I'm just not going to worry about it,

667
00:44:48,570 --> 00:44:51,570
but you're right that this is an 
important issue and you know,

668
00:44:51,870 --> 00:44:56,870
one of two things is going to happen.
So my personal opinion is that 

669
00:44:56,870 --> 00:45:00,381
backpropagation is just going to stay 
with us til the very end and we'll 

670
00:45:00,381 --> 00:45:03,501
actually build fully human level and 
beyond systems before we understand how 

671
00:45:03,501 --> 00:45:04,140
the brain does what it does.

672
00:45:06,180 --> 00:45:11,180
So that's what I believe,
but of course it is a difference that 

673
00:45:11,180 --> 00:45:11,940
has to be acknowledged.

674
00:45:14,280 --> 00:45:19,280
Do you think it was a fair match up for 
the Dota bought and that person given 

675
00:45:20,221 --> 00:45:21,540
the constraints of the system?

676
00:45:21,710 --> 00:45:26,710
So I'd say that the biggest advantage 
computers having games like this,

677
00:45:27,050 --> 00:45:32,050
like one of the big advantages is that 
they obviously have a better reaction 

678
00:45:32,050 --> 00:45:36,431
time.
Although in Dota in particular the 

679
00:45:36,431 --> 00:45:38,990
number of clicks per second over the top
players is fairly small,

680
00:45:39,080 --> 00:45:41,660
which is different from starcraft.
Starcraft,

681
00:45:41,930 --> 00:45:46,930
starcraft is a very mechanically heavy 
game because if a large number of units 

682
00:45:46,930 --> 00:45:48,530
and so the top players,
they just click all the time.

683
00:45:50,630 --> 00:45:55,630
In Dota,
every player controls just one hero and 

684
00:45:55,630 --> 00:45:56,750
so that greatly reduces the total number
of actions they need to make.

685
00:45:56,960 --> 00:46:01,960
Now still precision matters.
I think that we'll discover that later,

686
00:46:02,400 --> 00:46:07,400
but I think will really happen is if you
will discover that computers have the 

687
00:46:07,400 --> 00:46:10,691
advantage in any domain or rather every 
domain.

688
00:46:14,520 --> 00:46:15,020
Not yet.

689
00:46:15,530 --> 00:46:20,530
Do you think that the emergent behaviors
from the agent we're actually kind of 

690
00:46:20,530 --> 00:46:22,630
directed because the constraints are 
already kind of in place.

691
00:46:22,631 --> 00:46:26,840
Like so it was kinda forced discover 
those or do you think that like that was

692
00:46:26,841 --> 00:46:28,550
actually something quite novel that like
wow,

693
00:46:28,610 --> 00:46:33,610
it actually discovered these on its own.
Like you didn't actually have bias 

694
00:46:33,610 --> 00:46:33,610
towards constraining it.

695
00:46:33,610 --> 00:46:38,500
So it's definitely need discover new 
strategies and I can share an anecdote 

696
00:46:38,500 --> 00:46:40,270
where our tester,
we have a prohibition,

697
00:46:40,271 --> 00:46:45,271
would test the Bot and he played against
it for a long time and the bottom will 

698
00:46:45,431 --> 00:46:47,890
do all kinds of things against the 
player,

699
00:46:47,920 --> 00:46:52,920
the human player which were effective.
Then at some point that pro decided to 

700
00:46:52,991 --> 00:46:57,991
play against the better flow pro and he 
decided to imitate one of the things 

701
00:46:57,991 --> 00:47:00,880
that they both was doing and this image.
But by imitating it,

702
00:47:00,881 --> 00:47:03,880
he was able to defeat a better pro.
So I think,

703
00:47:03,910 --> 00:47:08,910
I think the strategies to discoveries 
are real and so like it means that 

704
00:47:08,910 --> 00:47:10,050
there's very little transport,
but you know,

705
00:47:11,530 --> 00:47:16,530
I would say I think what that means is 
that because the strategy's discovered 

706
00:47:17,201 --> 00:47:20,460
by the bulk of the humans,
it means that the fundamental game plays

707
00:47:20,461 --> 00:47:23,690
the deeply related for,
for

708
00:47:23,790 --> 00:47:28,790
a long time now I've heard that the 
objective of reinforcement learning is 

709
00:47:28,790 --> 00:47:29,950
to determine policy

710
00:47:30,100 --> 00:47:34,120
that chooses an action to maximize the 
expected reward,

711
00:47:34,450 --> 00:47:39,450
which is what you said earlier.
Would you ever want to look at the 

712
00:47:39,450 --> 00:47:42,520
standard deviation of possible rewards?
Does that even make sense?

713
00:47:42,690 --> 00:47:44,850
Yeah,
I mean I think for sure,

714
00:47:44,880 --> 00:47:46,560
I think it's really application 
dependent,

715
00:47:47,490 --> 00:47:52,490
one of the reasons to maximize the 
expected reward just because it's easier

716
00:47:52,501 --> 00:47:57,501
to design algorithms for it.
So you write down this equation of the 

717
00:47:58,531 --> 00:48:00,480
formula,
you do a little bit of derivation,

718
00:48:00,720 --> 00:48:03,410
you get something which amounts to a 
nice looking algorithm.

719
00:48:03,840 --> 00:48:08,840
Now I think there exists like really 
their existing applications where you'd 

720
00:48:10,051 --> 00:48:15,051
never want to make mistakes and you 
wanna work on the standard deviation as 

721
00:48:15,051 --> 00:48:16,050
well.
But in practice it seems that the,

722
00:48:16,410 --> 00:48:21,410
just looking at the expected reward 
covers a large fraction of the situation

723
00:48:23,121 --> 00:48:25,010
is you'd like to apply this to.
Okay,

724
00:48:25,150 --> 00:48:26,000
thanks.
Cam.

725
00:48:27,340 --> 00:48:32,340
Um,
we talked last week about motivations,

726
00:48:32,860 --> 00:48:37,860
um,
and that has a lot to do with the 

727
00:48:37,860 --> 00:48:40,160
reinforcement and some of the ideas is 
that the,

728
00:48:40,161 --> 00:48:45,161
uh,
our motivations are actually connection 

729
00:48:45,161 --> 00:48:48,230
with others and cooperation and I'm 
wondering if there through enough,

730
00:48:48,470 --> 00:48:53,470
and I understand it's very popular to 
have the computers play these 

731
00:48:53,470 --> 00:48:54,530
competitive games.
Um,

732
00:48:54,860 --> 00:48:59,860
but is there any use in like having an 
agent self play collaboratively 

733
00:49:01,970 --> 00:49:02,870
collaborative games?

734
00:49:03,700 --> 00:49:08,700
Yeah,
I think that's an extremely good 

735
00:49:08,700 --> 00:49:10,261
question.
I think one place from which we can get 

736
00:49:10,261 --> 00:49:12,430
some inspiration is from the evolution 
of cooperation.

737
00:49:13,470 --> 00:49:18,470
I think cooperation be cooperate 
ultimately because it's much better for 

738
00:49:21,581 --> 00:49:23,770
you,
the person to be cooperative or not.

739
00:49:24,850 --> 00:49:29,850
And so I think what should happen if you
have a sufficiently open-ended game 

740
00:49:33,580 --> 00:49:35,560
incorporation of it will be the winning 
strategy.

741
00:49:36,670 --> 00:49:39,400
And so I think we will get cooperation 
whether it be like it or not.

742
00:49:43,990 --> 00:49:45,100
Hi.
Um,

743
00:49:45,130 --> 00:49:49,900
you mentioned the complexity of the 
simulation of friction.

744
00:49:50,230 --> 00:49:54,670
I was wondering if you feel that there 
exist open complexity theoretic problems

745
00:49:54,671 --> 00:49:59,671
relevant to relevant to ai or whether 
it's just a matter of finding good 

746
00:49:59,671 --> 00:50:03,811
approximations that humans have,
the types of problems that humans tend 

747
00:50:03,811 --> 00:50:03,811
to solve?

748
00:50:04,270 --> 00:50:06,040
Yeah.
So complexity theory.

749
00:50:07,150 --> 00:50:10,240
Well,
like at the very basic level,

750
00:50:10,870 --> 00:50:13,780
we know that whatever algorithm we're 
going to run,

751
00:50:13,930 --> 00:50:16,330
he's going to run fairly efficiently on 
some hardware.

752
00:50:16,900 --> 00:50:21,900
So that puts a pretty strict upper bound
and the true complexity of the problems 

753
00:50:22,661 --> 00:50:24,550
you're solving,
like by definition,

754
00:50:24,970 --> 00:50:28,430
via solving problems which aren't too 
hard in complexity theoretic sense.

755
00:50:28,760 --> 00:50:33,140
Now it is also the case that many of the
problems.

756
00:50:33,440 --> 00:50:38,440
So while the overall thing that we do is
not hard from a complexity theoretic 

757
00:50:38,440 --> 00:50:41,690
sense and indeed humans cannot solve and
be complete problems in general,

758
00:50:43,400 --> 00:50:48,400
it is true that many of the like 
optimization problems that we pose to 

759
00:50:48,400 --> 00:50:50,240
our algorithms are in intractable in the
general case,

760
00:50:50,450 --> 00:50:52,880
starting from neural net optimization 
itself,

761
00:50:53,370 --> 00:50:58,370
it is easy to create a family of 
datasets for a neural network with a 

762
00:50:58,370 --> 00:51:00,530
very small number of neurons such that 
find the global optimum is not complete.

763
00:51:01,370 --> 00:51:04,700
And so how do we avoid it?
Well,

764
00:51:04,701 --> 00:51:07,580
we just try grading dissent anyway and 
somehow it works,

765
00:51:08,780 --> 00:51:13,780
but without question like we cannot,
we do not solve problems which are truly

766
00:51:16,131 --> 00:51:21,131
intractable.
So I mean I have the sense of the 

767
00:51:21,131 --> 00:51:21,131
question.

768
00:51:21,131 --> 00:51:25,930
Hello.
It seems like an important sub problem 

769
00:51:25,930 --> 00:51:28,130
on the path towards Agi will be 
understanding language.

770
00:51:28,490 --> 00:51:32,600
And the state of generative language 
modeling right now is pretty abysmal.

771
00:51:33,010 --> 00:51:36,740
What do you think are the most 
productive research trajectories towards

772
00:51:36,741 --> 00:51:37,970
generative language models?

773
00:51:38,330 --> 00:51:43,330
So I'll first say that you are 
completely correct that the situation 

774
00:51:43,330 --> 00:51:46,190
with language is still far from great.
Although progress has been made,

775
00:51:46,550 --> 00:51:51,550
even without any particular innovations 
beyond models that exist today,

776
00:51:52,850 --> 00:51:57,850
simply scaling up models that exist 
today on larger data sets is going to go

777
00:51:58,011 --> 00:52:00,020
surprisingly far,
not even larger datasets,

778
00:52:00,080 --> 00:52:02,180
but larger and deeper models.
For example,

779
00:52:02,360 --> 00:52:06,710
if you train a language model with a 
thousand layers and it's the same layer,

780
00:52:06,800 --> 00:52:11,800
I think it's going to be a pretty 
amazing language model like we don't 

781
00:52:11,800 --> 00:52:15,140
have the cycles for week yet,
but to think it will change very soon.

782
00:52:15,650 --> 00:52:20,650
Now.
I also agree with you that there are 

783
00:52:20,650 --> 00:52:23,380
some fundamental things missing in our 
current understanding of deep learning 

784
00:52:24,560 --> 00:52:28,070
which prevent us from really solving the
problem that we want.

785
00:52:28,370 --> 00:52:33,370
So I think one of these problems,
one of the things that's missing is 

786
00:52:33,370 --> 00:52:33,370
that,
or that seems like blatantly wrong,

787
00:52:33,470 --> 00:52:38,470
is the fact that we train a model and 
then you stop training the model and you

788
00:52:40,761 --> 00:52:41,270
freeze it.

789
00:52:41,900 --> 00:52:46,900
Even though it's the training process 
for the magic really happens of the 

790
00:52:47,691 --> 00:52:52,691
magic is if you think about it,
like the training process is the true 

791
00:52:52,821 --> 00:52:55,640
general part of the whole,
of the whole of the whole story.

792
00:52:56,150 --> 00:53:01,150
Because you're tends to flow code 
doesn't care what your data set to 

793
00:53:01,150 --> 00:53:01,150
optimize.
He just says whatever,

794
00:53:01,150 --> 00:53:05,051
just give me the data set,
I don't care which one solve also the 

795
00:53:05,051 --> 00:53:07,491
mall.
So like the ability to do that feels 

796
00:53:07,491 --> 00:53:11,060
really special and I think we are not 
using it at test time.

797
00:53:11,570 --> 00:53:14,990
It gets hard to speculate about like 
things you don't know the answer,

798
00:53:14,991 --> 00:53:18,260
but all I'll say is that simply train 
bigger,

799
00:53:18,261 --> 00:53:22,850
deeper language models.
They'll go surprisingly far scaling out,

800
00:53:22,860 --> 00:53:24,770
but also doing things like training or 
test stamina in,

801
00:53:24,780 --> 00:53:25,380
for instance,
time.

802
00:53:25,381 --> 00:53:28,830
I think it will be another important 
boosts the performance.

803
00:53:30,290 --> 00:53:31,670
Hi.
Thank you for the talk.

804
00:53:32,090 --> 00:53:37,090
Um,
so it seems like right now another 

805
00:53:37,090 --> 00:53:37,190
interesting approach to solving 
reinforcement learning problems could be

806
00:53:37,191 --> 00:53:42,191
to go for the evolutionary evolutionary 
strategies and although they have their 

807
00:53:43,010 --> 00:53:48,010
caveats,
I wanted to know if I'd open ai 

808
00:53:48,010 --> 00:53:48,380
particularly you're working on something
related and what are,

809
00:53:48,410 --> 00:53:49,850
what is your general opinion on them?

810
00:53:51,080 --> 00:53:56,080
So like at present,
I believe that something like 

811
00:53:56,080 --> 00:53:58,190
evolutionary strategies is another great
for reinforcement learning.

812
00:53:58,580 --> 00:54:00,620
I think that normal reinforcement 
learning algorithms,

813
00:54:00,621 --> 00:54:03,170
especially if it's big policies a 
better,

814
00:54:03,800 --> 00:54:08,800
but I think if you want to evolve a 
small compact object like like a piece 

815
00:54:08,800 --> 00:54:12,620
of code for example,
I think that would be a place where it's

816
00:54:12,621 --> 00:54:15,230
gonna be a seriously worth considering,
but this,

817
00:54:15,410 --> 00:54:20,410
you know,
you've all been a useful piece of code 

818
00:54:20,410 --> 00:54:21,560
is a cool idea.
Hasn't been done yet,

819
00:54:21,561 --> 00:54:23,990
so still a lot of work to be done.
Before we get there.

820
00:54:25,100 --> 00:54:26,420
Hi.
Thank you so much for coming.

821
00:54:26,750 --> 00:54:31,750
My question is,
you mentioned what is the right goal is 

822
00:54:31,750 --> 00:54:36,131
a political problem,
so I'm wondering if you can elaborate a 

823
00:54:36,131 --> 00:54:38,651
bit on that and it also,
what do you think would be the approach 

824
00:54:38,651 --> 00:54:38,651
for us to maybe get.

825
00:54:40,340 --> 00:54:40,880
Well?
Again,

826
00:54:40,910 --> 00:54:45,760
I can't really comment too much because 
all the thoughts that we have.

827
00:54:45,990 --> 00:54:50,990
You now have a few people who are 
thinking about this full time at open 

828
00:54:50,990 --> 00:54:54,870
Ai.
I don't have enough of this super strong

829
00:54:55,101 --> 00:55:00,101
opinion to say anything too definitive.
All I can say at the very high level 

830
00:55:00,101 --> 00:55:02,510
isn't given the size.
Like if you go into the future,

831
00:55:02,540 --> 00:55:05,690
whenever soon or whenever it's going to 
happen,

832
00:55:05,691 --> 00:55:10,691
when you build a computer,
which can do anything better than a 

833
00:55:10,691 --> 00:55:14,621
human,
it's will happen because the brain is 

834
00:55:14,621 --> 00:55:16,841
physical,
being picked on society is going to be 

835
00:55:16,841 --> 00:55:20,531
completely massive and overwhelming.
It's very difficult to imagine even if 

836
00:55:22,131 --> 00:55:25,200
you try really hard.
And I think what it means is that people

837
00:55:25,210 --> 00:55:29,240
do care a lot and that's what I was 
alluding to,

838
00:55:29,241 --> 00:55:32,960
the fact that this will be something 
that many people who care about strongly

839
00:55:34,880 --> 00:55:38,870
and like as the impact increases 
gradually be self driving cars,

840
00:55:38,871 --> 00:55:43,871
more automation,
I think we will see a lot more people 

841
00:55:43,871 --> 00:55:43,871
care.

842
00:55:43,871 --> 00:55:47,830
Do we need to have a very accurate model
of the physical world and then simulate 

843
00:55:48,321 --> 00:55:50,540
that in order to have,
uh,

844
00:55:50,780 --> 00:55:55,100
these agents that can eventually come 
out into the real world and do something

845
00:55:55,101 --> 00:55:56,640
approaching,
you know,

846
00:55:56,810 --> 00:55:58,700
human level intelligence tasks.

847
00:55:59,590 --> 00:56:03,220
That's a very good question.
So I think if that were the case,

848
00:56:03,280 --> 00:56:08,280
we'd be in trouble and I am very certain
that it could be avoided.

849
00:56:10,990 --> 00:56:14,410
So specifically,
the real answer has to be that look,

850
00:56:14,800 --> 00:56:17,830
you learn to problem solve,
we learned to negotiate,

851
00:56:17,831 --> 00:56:19,030
you learn to persist,
you know,

852
00:56:19,050 --> 00:56:22,230
lots of different useful life lessons in
the simulation and yes,

853
00:56:22,231 --> 00:56:24,970
you learn some physics too,
but I ain't go outside to the real world

854
00:56:25,390 --> 00:56:30,390
and you have to start over to some 
extent because many of your deeply held 

855
00:56:30,390 --> 00:56:32,290
assumptions will be false and amount of 
the goals,

856
00:56:32,390 --> 00:56:37,120
so that's one of the reasons I care so 
much about never stopped in training.

857
00:56:37,930 --> 00:56:42,930
You've accumulated your knowledge.
Now we go into an environment for some 

858
00:56:42,930 --> 00:56:42,930
of your assumptions of let you continued
training.

859
00:56:42,970 --> 00:56:47,970
We tried to connect the new data to your
old data and this is an important 

860
00:56:47,970 --> 00:56:48,700
requirement from our algorithms,
which is already met to some extent,

861
00:56:48,701 --> 00:56:53,701
but it will have to be met a lot more so
that you can take the partial knowledge 

862
00:56:53,701 --> 00:56:57,700
is required and go to a new situation,
learn some more.

863
00:56:57,980 --> 00:57:00,730
Literally the example of you go to 
school alone,

864
00:57:00,750 --> 00:57:02,410
learn useful things,
then you go to work.

865
00:57:02,830 --> 00:57:04,300
It's not perfect.
It's not.

866
00:57:04,301 --> 00:57:09,301
You know,
you pull your four years of cs in 

867
00:57:09,301 --> 00:57:11,131
Undergrad is not going to fully prepare 
you for whatever it is you need to know 

868
00:57:11,131 --> 00:57:11,530
how to work.
It will help somewhat.

869
00:57:11,860 --> 00:57:13,960
You'll be able to get off the ground,
but there will be lots of new things you

870
00:57:13,961 --> 00:57:15,010
need to learn.
So that's,

871
00:57:15,011 --> 00:57:17,190
that's the spirit of it.
I think of those,

872
00:57:17,210 --> 00:57:17,920
of the school,

873
00:57:18,920 --> 00:57:21,950
one of the things you mentioned pretty 
early on in your talk is that one of the

874
00:57:21,951 --> 00:57:26,951
limitations of this sort of style of 
reinforcement learning is there is no 

875
00:57:26,951 --> 00:57:30,731
self organization so you have to tell it
went into the good thing or is it a bad 

876
00:57:30,731 --> 00:57:34,481
thing and that's actually a problem in 
neuroscience as well and you're trying 

877
00:57:34,481 --> 00:57:34,481
to teach a rat to,
you know,

878
00:57:34,481 --> 00:57:35,600
navigate a maze.
You have to artificially tell it what to

879
00:57:35,601 --> 00:57:40,601
do.
So where do you see moving forward when 

880
00:57:40,601 --> 00:57:40,601
we already have this problem with 
teaching,

881
00:57:40,601 --> 00:57:44,381
you know,
not necessarily learning but also 

882
00:57:44,381 --> 00:57:46,571
teaching.
So where do you see the research moving 

883
00:57:46,571 --> 00:57:46,571
forward in that respect?
How do you sort of introduce this notion

884
00:57:46,571 --> 00:57:47,600
of self organization?

885
00:57:48,180 --> 00:57:53,180
So I think without question one really 
important thing you need to do is to be 

886
00:57:53,180 --> 00:57:57,160
able to infer the goals and strategies 
of other agents by observing them.

887
00:57:58,470 --> 00:58:01,160
That's a fundamental skill we need to be
able to learn to,

888
00:58:01,330 --> 00:58:03,150
to embed into the agents.
So if,

889
00:58:03,151 --> 00:58:04,450
for example,
we have two agents,

890
00:58:04,780 --> 00:58:06,970
one of them is doing something and the 
other agents says,

891
00:58:06,971 --> 00:58:08,830
well that's really cool.
I want to be able to do that too,

892
00:58:09,250 --> 00:58:14,250
and then you go on and do that.
And so I'd say that this is a very 

893
00:58:14,250 --> 00:58:16,801
important component in terms of setting 
the reward of you see what they do,

894
00:58:17,480 --> 00:58:22,480
you further reward and now we have a 
knob which says you see what they're 

895
00:58:22,480 --> 00:58:23,380
doing now go and try to do the same 
thing.

896
00:58:24,120 --> 00:58:26,160
So let's say this,
this is as far as I,

897
00:58:26,230 --> 00:58:31,230
as far as I know,
this was one of the important ways in 

898
00:58:31,230 --> 00:58:33,871
which humans are quite different from 
other animals in the way which in the 

899
00:58:37,750 --> 00:58:42,280
scale and scope in which we copy the 
behavior of other humans

900
00:58:43,790 --> 00:58:45,620
mind.
If I ask a quick followup or go for it.

901
00:58:45,950 --> 00:58:48,950
So that's kind of obvious how that works
in this scope of competition,

902
00:58:48,951 --> 00:58:53,951
but what about just sort of arbitrary 
tasks like I'm in a math class with 

903
00:58:53,951 --> 00:58:57,551
someone and I see someone doing a 
problem in a particular way and I'm 

904
00:58:57,551 --> 00:58:57,551
like,
oh that's a good strategy.

905
00:58:57,551 --> 00:59:01,181
Maybe I should try that out.
How does that work in a sort of non 

906
00:59:01,181 --> 00:59:01,181
competitive environment?

907
00:59:01,181 --> 00:59:03,310
So I think that this movie,
I think that that's going to be a little

908
00:59:03,311 --> 00:59:06,370
bit separate from the competitive 
environment,

909
00:59:06,640 --> 00:59:11,530
but it will have to be somehow either 
bake,

910
00:59:11,860 --> 00:59:16,860
probably baked in,
maybe evolved into the system where if 

911
00:59:17,291 --> 00:59:22,291
you have other agents doing things,
they're generating data which you and 

912
00:59:22,291 --> 00:59:26,480
the only way to truly make sense of the 
data that you see used to infer the goal

913
00:59:26,481 --> 00:59:29,480
of the agent to strategy their beliefs 
state.

914
00:59:29,810 --> 00:59:31,700
That's important also for communicating 
with them.

915
00:59:32,470 --> 00:59:34,160
If you want to successfully communicate 
with someone,

916
00:59:34,161 --> 00:59:39,161
you have to keep track both of their 
goal and their beliefs state instead of 

917
00:59:39,161 --> 00:59:41,951
knowledge.
So I think people find that there are 

918
00:59:41,951 --> 00:59:44,381
many,
I guess connections between 

919
00:59:44,381 --> 00:59:45,020
understanding what other agents are 
doing,

920
00:59:45,140 --> 00:59:50,140
inferring their goals,
imitating them and community 

921
00:59:50,140 --> 00:59:50,140
successfully communicating them.
Alright,

922
00:59:50,140 --> 00:59:51,790
let's give Eylea and the happy hour 
hand.

923
00:59:52,190 --> 00:59:57,190
Thank you.

