1
00:00:00,030 --> 00:00:03,150
The following is a conversation
with Ian Goodfellow.

2
00:00:03,450 --> 00:00:06,330
He is the author of the popular
textbook on deep learning,

3
00:00:06,331 --> 00:00:08,400
simply titled Deep Learning.

4
00:00:08,910 --> 00:00:12,060
He coined the term of
generative adversarial networks,

5
00:00:12,330 --> 00:00:17,330
otherwise known as Ganz and with his 2014
paper is responsible for launching the

6
00:00:19,141 --> 00:00:23,850
incredible growth of research
and innovation. In this
subfield of deep learning.

7
00:00:24,690 --> 00:00:27,120
He got his bs and ms at Stanford,

8
00:00:27,510 --> 00:00:32,510
his Phd at University of Montreal with
Yoshua Bengio and Aaron Kerrville.

9
00:00:33,270 --> 00:00:36,420
He held several research
positions including an open AI,

10
00:00:36,630 --> 00:00:40,710
Google brain and now at apple as
the director of machine learning.

11
00:00:41,550 --> 00:00:44,940
This recording happened while
Ian was still a Google brain,

12
00:00:45,360 --> 00:00:49,340
but we don't talk about it and you
think specific to Google or any other

13
00:00:49,341 --> 00:00:54,230
organization. This conversation is part
of the artificial intelligence podcast.

14
00:00:54,500 --> 00:00:56,630
If you enjoy,
subscribe on Youtube,

15
00:00:56,660 --> 00:01:01,640
iTunes or simply connect with me on
Twitter at Lex Friedman, spelled f,

16
00:01:01,641 --> 00:01:02,570
r I.
D.

17
00:01:02,990 --> 00:01:06,920
And now here's my conversation
with Ian Goodfellow.

18
00:01:08,240 --> 00:01:13,240
You Open your popular deep learning book
with a Russian doll type diagram that

19
00:01:13,761 --> 00:01:17,120
shows deep learning is a subset
of representation learning,

20
00:01:17,121 --> 00:01:21,980
which in turn is a subset of machine
learning. And finally a subset of Ai.

21
00:01:22,490 --> 00:01:27,050
So this kind of implies that there may
be limits to deep learning in the context

22
00:01:27,051 --> 00:01:27,710
of Ai.

23
00:01:27,710 --> 00:01:32,090
So what do you think is the current
limits of deep learning and a,

24
00:01:32,091 --> 00:01:35,210
are those limits something
that we can overcome with time?

25
00:01:35,730 --> 00:01:39,390
Yeah, I think one of the
biggest limitations of deep
learning is the right now it

26
00:01:39,391 --> 00:01:42,750
requires really a lot of data,
especially labeled data.

27
00:01:43,840 --> 00:01:47,550
There are some unsupervised and semi
supervised learning algorithms that can

28
00:01:47,551 --> 00:01:49,080
reduce the amount of label data you need,

29
00:01:49,470 --> 00:01:51,770
but they still require a
lot of unlabeled data. Hmm.

30
00:01:52,170 --> 00:01:54,210
Reinforcement learning algorithms,
they don't need labels,

31
00:01:54,211 --> 00:01:56,740
but they need really a lot of experiences.
Um,

32
00:01:57,270 --> 00:02:01,380
as human beings we don't learn to play
Pong by failing at park 2 million times.

33
00:02:02,720 --> 00:02:07,280
So just getting the generalization ability
better is one of the most important

34
00:02:07,460 --> 00:02:09,860
bottlenecks in the capability
of the technology today.

35
00:02:10,550 --> 00:02:14,840
And then I guess I'd also say deep
learning is like a component of a bigger

36
00:02:14,841 --> 00:02:18,920
system. Um, so far nobody
has really proposing to have,

37
00:02:19,490 --> 00:02:20,323
uh,

38
00:02:20,570 --> 00:02:25,520
only what you'd call deep learning as
the entire ingredient of intelligence.

39
00:02:25,521 --> 00:02:29,840
You use deep learning as sub
modules of other systems,

40
00:02:29,841 --> 00:02:34,840
like Alphago has a deep learning model
that estimates the value function. Um,

41
00:02:35,120 --> 00:02:38,150
most reinforcement learning algorithms
have a deep learning module that

42
00:02:38,480 --> 00:02:40,310
estimates which action to take next,

43
00:02:40,311 --> 00:02:44,660
but he might have other components if
you're basically building a function

44
00:02:44,661 --> 00:02:47,530
estimator.
Do you think it's

45
00:02:47,970 --> 00:02:50,970
possible? He said nobody's kind of
been thinking about this so far,

46
00:02:50,971 --> 00:02:54,750
but do you think you're on that works
could be made to reason in the way

47
00:02:54,751 --> 00:02:58,530
symbolic systems did in the
eighties and nineties to do more,

48
00:02:58,740 --> 00:03:00,130
create more like programs

49
00:03:00,130 --> 00:03:03,760
as opposed to functions? Yeah, I think
we already see that a little bit.

50
00:03:04,870 --> 00:03:08,470
I already kind of think of neural
nets as a kind of of program.

51
00:03:08,860 --> 00:03:13,860
I think of deep learning as basically
learning programs that have more than one

52
00:03:13,961 --> 00:03:17,430
step.
So if you'd draw a flow chart or,

53
00:03:17,700 --> 00:03:21,850
or if you draw a tensor flow graph
describing your machine learning model,

54
00:03:21,851 --> 00:03:25,300
I think of the depth of that graph is
describing the number of steps that run in

55
00:03:25,301 --> 00:03:28,720
sequence and then the width of that
graph has a number of steps that run in

56
00:03:28,721 --> 00:03:30,700
parallel now.

57
00:03:30,701 --> 00:03:33,370
It's been long enough that we've had
deep learning working that it's a little

58
00:03:33,371 --> 00:03:35,710
bit silly to even discuss
shallow learning anymore.

59
00:03:35,740 --> 00:03:40,060
But back when I first got involved in Ai,
when we used machine learning,

60
00:03:40,061 --> 00:03:42,850
we were usually learning things
like support vector machines.

61
00:03:43,730 --> 00:03:46,570
You could have a lot of input features
to the model and you could multiply each

62
00:03:46,571 --> 00:03:47,650
feature by a different weight.

63
00:03:48,100 --> 00:03:51,370
All those multiplication locations were
done in parallel to each other and there

64
00:03:51,371 --> 00:03:53,050
wasn't a lot done in series.
I think.

65
00:03:53,310 --> 00:03:56,690
Well we got with deep learning was
really the ability to have, uh,

66
00:03:57,280 --> 00:03:59,650
steps of a program that run in sequence.

67
00:04:00,280 --> 00:04:04,750
And I think that we've actually started
to see that what's important with deep

68
00:04:04,751 --> 00:04:08,770
learning is more the fact that we have
a multistep program rather than the fact

69
00:04:08,771 --> 00:04:10,150
that we've learned to representation.

70
00:04:10,720 --> 00:04:14,560
If you look at things like a resonance,
for example,

71
00:04:15,130 --> 00:04:20,130
they take one particular
kind of representation and
they update it several times

72
00:04:21,190 --> 00:04:25,990
back when deep learning first really took
off in the academic world in 2006 when

73
00:04:25,991 --> 00:04:29,530
Geoff Hinton showed that you
could train deep belief networks,

74
00:04:30,130 --> 00:04:33,880
everybody who was interested in the idea
thought of it as each layer learns a

75
00:04:33,881 --> 00:04:37,810
different level of abstraction,
that the first layer trained on images,

76
00:04:37,811 --> 00:04:40,390
learn something like edges and
the second layer learns corners.

77
00:04:40,390 --> 00:04:43,960
And eventually you get these kind of
grandmothers sell units that recognize

78
00:04:43,961 --> 00:04:45,130
specific objects.

79
00:04:45,880 --> 00:04:50,880
Today I think most people think of it
more as a computer program where as you

80
00:04:51,251 --> 00:04:54,790
add more layers, you can do more updates
before you output your final number.

81
00:04:55,090 --> 00:04:59,230
But I don't think anybody believes
that layer 150 of their resume yet.

82
00:04:59,710 --> 00:05:02,610
Uh, is a grandfather,
grandmother cell and you know,

83
00:05:02,611 --> 00:05:06,430
layer 100 is contours or
something like that. Okay.

84
00:05:06,490 --> 00:05:10,330
So you think you're not thinking of it
as a singular representation that keeps

85
00:05:10,870 --> 00:05:14,020
building,
you think of it as a program,

86
00:05:14,021 --> 00:05:18,460
sort of almost like a state representation
has a state of understanding.

87
00:05:18,620 --> 00:05:19,180
Yeah,

88
00:05:19,180 --> 00:05:22,840
I think of it as a program that makes
several updates and arrives at better and

89
00:05:22,841 --> 00:05:27,460
better understandings, but it's not
replacing the representation at each step.

90
00:05:27,490 --> 00:05:31,570
It's refining it and in some sense
that's a little bit like reasoning.

91
00:05:31,630 --> 00:05:33,550
It's not reasoning in
the form of deduction,

92
00:05:33,551 --> 00:05:38,260
but it's reasoning in the form of taking
a thought and refining it and refining

93
00:05:38,261 --> 00:05:42,250
it carefully until it's good
enough to use. Do you think,

94
00:05:42,580 --> 00:05:45,990
and I hope you don't mind, we'll jump
philosophical every once in a while.

95
00:05:46,790 --> 00:05:49,600
Do you think a of cognition,

96
00:05:49,630 --> 00:05:54,630
human cognition or even consciousness
as simply a result of this kind of a

97
00:05:55,610 --> 00:05:58,100
sequential sequential
representation learning,

98
00:05:58,100 --> 00:06:00,110
do you think that can emerge?
Okay.

99
00:06:00,380 --> 00:06:03,110
Cognition? Yes, I think so. Consciousness,

100
00:06:03,111 --> 00:06:06,260
that's really hard to even
define what we mean by that.

101
00:06:07,410 --> 00:06:10,980
I guess there's consciousness is often
defined as things like having self

102
00:06:10,981 --> 00:06:15,981
awareness and that's relatively easy
to turn into something actionable for a

103
00:06:16,231 --> 00:06:19,770
computer scientist to reason about
people also defined find consciousness in

104
00:06:19,771 --> 00:06:23,920
terms of having qualitative states
of experience like qual, Leah, right?

105
00:06:23,960 --> 00:06:25,410
There's all these philosophical problems,
like,

106
00:06:25,411 --> 00:06:29,910
could you imagine a Zombie who does all
the same information processing as a

107
00:06:29,911 --> 00:06:33,990
human but doesn't really have the
qualitative experiences that we have,

108
00:06:34,680 --> 00:06:35,580
that sort of thing.

109
00:06:35,581 --> 00:06:39,930
I have no idea how to formalize or
turn it into a scientific question.

110
00:06:39,931 --> 00:06:43,770
I don't know how you could run an
experiment to tell whether a person is a

111
00:06:43,771 --> 00:06:45,630
Zombie or not.
And similarly,

112
00:06:45,631 --> 00:06:49,200
I don't know how you could run an
experiment to tell whether an advanced AI

113
00:06:49,201 --> 00:06:52,470
system had become conscious in
the sense of quality or not.

114
00:06:52,970 --> 00:06:56,030
But in the more practical sense,
like almost like self attention,

115
00:06:56,240 --> 00:07:01,040
you think consciousness and cognition
can in an impressive way emerge from

116
00:07:01,250 --> 00:07:03,770
current types of architectures?

117
00:07:03,800 --> 00:07:06,560
Sure. Yeah. Or, or if,

118
00:07:06,590 --> 00:07:10,840
if you think of consciousness in
terms of self awareness and just, um,

119
00:07:11,030 --> 00:07:16,030
making plans based on the fact that
the agent itself exists in the world

120
00:07:16,700 --> 00:07:20,810
reinforcement learning algorithms that
are already more or less forced to muddle

121
00:07:20,811 --> 00:07:25,460
the agent's effect on the environment
so that that more limited version of

122
00:07:25,461 --> 00:07:30,461
consciousness is already something
that we get limited versions of with

123
00:07:31,671 --> 00:07:35,840
reinforcement learning algorithms
if they're trained well. But, uh,

124
00:07:36,420 --> 00:07:37,790
you say limited.
So the,

125
00:07:37,830 --> 00:07:41,820
the big question really is how you
jumped from limited to human level. Yeah.

126
00:07:41,850 --> 00:07:46,800
Right. And, uh, whether it's
possible the, you know, the,

127
00:07:46,801 --> 00:07:50,520
even just building common sense reasoning
seems to be exceptionally difficult.

128
00:07:50,521 --> 00:07:54,990
So Ken, if we scale things up, if we
get much better on supervised learning,

129
00:07:54,991 --> 00:07:59,070
if we get better at labeling, if
forget, get bigger datasets, uh,

130
00:07:59,480 --> 00:08:00,570
more compute,

131
00:08:00,600 --> 00:08:05,460
do you think we'll start to see really
impressive things that go from limited to,

132
00:08:06,570 --> 00:08:10,220
you know, uh, something echoes
of human level cognition?

133
00:08:10,310 --> 00:08:11,440
I think so. Yeah. I'm,

134
00:08:11,441 --> 00:08:15,500
I'm optimistic about what can happen
just with more computation and more data.

135
00:08:16,010 --> 00:08:20,450
Uh, I do think it'll be important to
get the right kind of data. Today,

136
00:08:20,451 --> 00:08:25,430
most of the machine learning systems we
train are mostly trained on one type of

137
00:08:25,431 --> 00:08:29,210
data for each model.
But the human brain,

138
00:08:29,270 --> 00:08:34,250
we get all of our different senses and
we have many different experiences like,

139
00:08:34,670 --> 00:08:38,420
you know, riding a bike, driving a
car, or talking to people, reading. Um,

140
00:08:39,140 --> 00:08:43,970
I think when you get that kind of
integrated dataset working with a machine

141
00:08:43,971 --> 00:08:47,210
learning model that can actually
close the loop and interact,

142
00:08:47,660 --> 00:08:51,500
we may find that algorithm's not so
different from what we have today.

143
00:08:51,830 --> 00:08:55,680
Learn really interesting things when you
scale them up a lot and train them on a

144
00:08:55,681 --> 00:08:59,610
large amount of multimodal data.
And so multimodal is really interesting.

145
00:08:59,611 --> 00:09:03,780
But within like your work
in a adversarial examples.

146
00:09:03,960 --> 00:09:07,020
So selecting within modal,

147
00:09:07,050 --> 00:09:10,740
within a one mode of data,
uh,

148
00:09:11,070 --> 00:09:14,620
selecting better at what are the difficult
cases when we share most of these

149
00:09:14,621 --> 00:09:16,740
slow to learn from.
Oh yeah.

150
00:09:16,750 --> 00:09:20,350
Like could we could each get a
whole lot of mileage out of uh,

151
00:09:20,670 --> 00:09:23,910
designing a model that's resistant to
adversarial examples or something like

152
00:09:23,911 --> 00:09:28,860
that. But my thinking on that has
evolved a lot over the last few years.

153
00:09:28,890 --> 00:09:29,723
When I asked him,

154
00:09:29,940 --> 00:09:32,730
when I first started to really invest
in studying adversarial examples,

155
00:09:32,731 --> 00:09:37,590
I was thinking of it mostly as adversarial
examples reveal a big problem with

156
00:09:37,591 --> 00:09:38,424
machine learning.

157
00:09:38,940 --> 00:09:43,940
And we would like to close the gap between
how machine learning models respond

158
00:09:44,101 --> 00:09:49,101
to adversarial examples and how humans
respond after studying the problem more.

159
00:09:49,170 --> 00:09:51,300
I still think that adversarial
examples are important.

160
00:09:51,930 --> 00:09:56,580
I think of them now more of as a
security liability then as an issue that

161
00:09:56,790 --> 00:10:01,410
necessarily shows there's something
uniquely wrong with machine learning as

162
00:10:01,411 --> 00:10:03,090
opposed to humans.
Also.

163
00:10:03,091 --> 00:10:06,720
Do you see them as a tool to improve
the performance of the system? Not,

164
00:10:06,990 --> 00:10:10,260
not on the security side,
but literally just accuracy.

165
00:10:10,740 --> 00:10:13,470
I do see them as a kind
of tool on that side,

166
00:10:13,471 --> 00:10:16,330
but maybe not quite as much
as I used to think. Ah,

167
00:10:16,620 --> 00:10:20,610
we've started to find that there's a
trade off between accuracy on adversarial

168
00:10:20,611 --> 00:10:25,611
examples and accuracy on cleaning
samples back in 2014 when I did the first

169
00:10:27,120 --> 00:10:31,530
adversarily trained classifier that
showed resistance to some kinds of

170
00:10:31,531 --> 00:10:32,640
adversarial examples.

171
00:10:33,000 --> 00:10:36,630
It also got better at the clean data
on omnist and that's something we've

172
00:10:36,631 --> 00:10:38,930
replicated several times and feminist,
uh,

173
00:10:38,940 --> 00:10:41,470
that when we train against
weak adversarial examples,

174
00:10:41,480 --> 00:10:45,990
Agnes classifiers get more accurate so
far that hasn't really held up on other

175
00:10:45,991 --> 00:10:50,700
datasets and hasn't held up. When we
train, I get against stronger adversaries.

176
00:10:50,730 --> 00:10:55,640
It seems like when you confront
a really strong adversary, uh,

177
00:10:55,650 --> 00:10:58,620
you tend to have to give something up.
Here's your thing.

178
00:10:58,980 --> 00:11:01,650
But it's such a compelling
idea cause it feels, uh,

179
00:11:02,100 --> 00:11:06,240
it feels like that's how us
humans learn the difficult cases.

180
00:11:06,270 --> 00:11:08,220
We try to think of what would it be,

181
00:11:08,221 --> 00:11:10,560
screw up and then we make
sure we fixed that. Yeah.

182
00:11:10,980 --> 00:11:13,230
It's also in a lot of
branches of engineering,

183
00:11:13,650 --> 00:11:17,580
you do a worst case analysis and make
sure that your system will work in the

184
00:11:17,581 --> 00:11:20,370
worst case and then that
guarantees that it'll work.

185
00:11:20,400 --> 00:11:25,400
And all of the messy average cases that
happen when you go out into a really

186
00:11:25,921 --> 00:11:29,520
randomized world. Yeah. With
driving with autonomous vehicles,

187
00:11:29,550 --> 00:11:33,420
there's seems to be a desire
to just look for think.

188
00:11:33,450 --> 00:11:36,900
Adversarially tried to figure
out how to mess up the system.

189
00:11:36,901 --> 00:11:41,130
And if you can be robust to all
those difficult cases, then you can,

190
00:11:41,400 --> 00:11:46,060
it's a hand wavy, empirical way
to show your system is a, yeah.

191
00:11:46,230 --> 00:11:47,280
Yeah.
Today,

192
00:11:47,281 --> 00:11:51,570
most adversarial example research isn't
really focused on a particular use case,

193
00:11:51,600 --> 00:11:56,230
but there are a lot different use cases
where you'd like to make sure that the

194
00:11:56,231 --> 00:12:01,040
adversary can interfere with the
operation of your system. Like in finance,

195
00:12:01,041 --> 00:12:02,960
if you have an algorithm
making trades for you,

196
00:12:03,290 --> 00:12:06,650
people go to a lot of an effort
to obfuscate their algorithm.

197
00:12:06,651 --> 00:12:11,651
That's both to protect their IP because
you don't want to research and develop a

198
00:12:12,410 --> 00:12:15,410
profitable trading algorithm than
have somebody else capture the gains,

199
00:12:16,100 --> 00:12:18,980
but it's at least partly because you
don't want people to make adversarial

200
00:12:18,981 --> 00:12:22,430
examples that full your algorithm
and to making bad trades

201
00:12:24,360 --> 00:12:28,920
or I guess one area that's been popular
in the academic literature is speech

202
00:12:28,921 --> 00:12:29,754
recognition.

203
00:12:30,150 --> 00:12:35,150
If you use speech recognition to hear
an audio wave form and then in turn that

204
00:12:36,871 --> 00:12:39,180
into a command that a
phone executes for you,

205
00:12:39,630 --> 00:12:43,620
you don't want a malicious adversary
to be able to produce audio.

206
00:12:43,621 --> 00:12:45,990
That gets interpreted
as malicious commands,

207
00:12:46,290 --> 00:12:49,380
especially if a human in the room doesn't
realize that something like that is

208
00:12:49,381 --> 00:12:53,370
happening in speech recognition.
Has there been much success

209
00:12:53,880 --> 00:12:56,260
in being able to uh,

210
00:12:56,740 --> 00:12:59,410
create adversarial examples
that fool the system?

211
00:12:59,720 --> 00:13:00,141
Yeah,

212
00:13:00,141 --> 00:13:03,830
actually I guess the first work that I'm
aware of is a paper called hidden voice

213
00:13:03,831 --> 00:13:07,580
commands that came out in 2016 I believe.

214
00:13:08,450 --> 00:13:12,470
And they were able to show that
they could make sounds that are not

215
00:13:12,920 --> 00:13:14,420
understandable by a human,

216
00:13:14,930 --> 00:13:19,820
but are recognized as the target phrase
that the attacker wants the phone to

217
00:13:19,821 --> 00:13:20,750
recognize it as.

218
00:13:21,290 --> 00:13:25,190
Since then things have gotten a little
bit better on the attacker side.

219
00:13:25,191 --> 00:13:27,440
When worse on the defender side,

220
00:13:28,670 --> 00:13:33,670
it's become possible to make sounds
that sound like normal speech but are

221
00:13:36,021 --> 00:13:39,440
actually interpreted as a
different sentence than the human.

222
00:13:39,510 --> 00:13:44,510
Here's the level of perceptibility of the
adversarial perturbation is still kind

223
00:13:44,571 --> 00:13:48,140
of high, uh, that when you
listened to the recording,

224
00:13:48,141 --> 00:13:52,670
it sounds like there's some noise in the
background just like rustling sounds.

225
00:13:52,910 --> 00:13:55,880
But those wrestling sounds are actually
the adversarial perturbation that makes

226
00:13:55,881 --> 00:13:59,330
the phone here are completely different
sentence. Yeah. That's so fascinating.

227
00:14:00,130 --> 00:14:03,010
Peter Norvig mentioned that you're
writing the deep learning chapter for the

228
00:14:03,011 --> 00:14:06,910
fourth edition of the artificial
intelligence and modern approach book,

229
00:14:07,300 --> 00:14:10,690
so how do you even begin a summarizing?

230
00:14:10,691 --> 00:14:12,430
They feel the deep learning in a chapter?

231
00:14:13,920 --> 00:14:18,630
Well, I am. In my case, I waited like
a year before I actually read anything.

232
00:14:18,650 --> 00:14:22,650
I think it has it even having written
a full length textbook before,

233
00:14:22,651 --> 00:14:27,651
it's still a pretty intimidating to try
to start writing just one chapter that

234
00:14:28,021 --> 00:14:28,890
covers everything.

235
00:14:31,150 --> 00:14:34,420
One thing that helped me make that
plan was actually the experience of,

236
00:14:34,490 --> 00:14:39,130
and having written the full book before
and then watching how the field changed

237
00:14:39,131 --> 00:14:40,210
after the book came out,

238
00:14:40,960 --> 00:14:44,470
I realized there's a lot of topics that
were maybe extraneous and the first book

239
00:14:44,980 --> 00:14:49,900
and just seeing what stood the test of
a few years of being published and what

240
00:14:50,380 --> 00:14:53,870
seems a little bit less important to
have included now help me pare down the

241
00:14:53,871 --> 00:14:55,530
topics I wanted to cover for the pork.

242
00:14:56,870 --> 00:15:00,970
It's also really nice now that the field
has kind of stabilized to the point

243
00:15:00,971 --> 00:15:03,970
where some core ideas from the
1980s are still used today.

244
00:15:04,800 --> 00:15:06,660
When I first started
studying machine learning,

245
00:15:06,690 --> 00:15:10,590
almost everything from the 1980s had
been rejected and now some of it has come

246
00:15:10,591 --> 00:15:11,310
back.

247
00:15:11,310 --> 00:15:15,030
So that stuff that's really stood the
test of time is what I focused on putting

248
00:15:15,031 --> 00:15:15,864
into the book.

249
00:15:16,510 --> 00:15:21,510
There is also I guess two different
philosophies about how you might write a

250
00:15:21,961 --> 00:15:23,880
book,
a one philosophy philosophies.

251
00:15:23,881 --> 00:15:27,270
He tried to write a reference that covers
everything and the other philosophy is

252
00:15:27,271 --> 00:15:31,170
you try to provide a high level summary
that gives people the language to

253
00:15:31,171 --> 00:15:34,380
understand a field and tells them
what the most important concepts are.

254
00:15:34,950 --> 00:15:37,510
The first deep learning
book that I wrote with the,

255
00:15:37,650 --> 00:15:39,860
and Erin was somewhere between the,

256
00:15:40,010 --> 00:15:44,490
the two philosophies that it's trying to
be both a reference and an introductory

257
00:15:44,491 --> 00:15:48,570
guide. Uh, writing this chapter
for Russell and Norvig's book.

258
00:15:48,930 --> 00:15:53,880
I was able to focus more on just a concise
introduction of the key concepts and

259
00:15:53,881 --> 00:15:55,590
the language you need
to read about them more.

260
00:15:55,950 --> 00:15:58,020
And a lot of cases actually
just wrote paragraphs that said,

261
00:15:58,710 --> 00:16:02,200
here's a rapidly evolving area that
you should pay attention to. It's,

262
00:16:02,370 --> 00:16:06,960
it's pointless to try to tell you
what the latest and best version of a,

263
00:16:07,500 --> 00:16:11,460
you know, learn to learn
model is, um, you know, I can,

264
00:16:11,670 --> 00:16:13,650
I can point you to a paper
that's recent right now,

265
00:16:13,651 --> 00:16:18,600
but there isn't a whole lot of a reason
to delve into exactly what's going on

266
00:16:18,601 --> 00:16:23,550
with the latest learning to learn
approach or the latest module produced by

267
00:16:23,551 --> 00:16:24,660
learning to learn algorithm.

268
00:16:24,960 --> 00:16:28,770
You should know that learning to learn
as a thing and that it may very well be

269
00:16:28,771 --> 00:16:33,771
the source of the latest and greatest
convolutional net or recurrent net module

270
00:16:33,781 --> 00:16:35,550
that you would want to use
in your latest project.

271
00:16:36,030 --> 00:16:41,030
But there isn't a lot of point in trying
to summarize exactly which architecture

272
00:16:41,371 --> 00:16:43,740
in which learning approach got
to which level of performance.

273
00:16:44,050 --> 00:16:49,050
So you may be focused more on
the basics of the methodology.

274
00:16:49,240 --> 00:16:53,740
So from backpropagation to feed
forward to recurrent neural networks,

275
00:16:53,741 --> 00:16:57,610
convolutional, that kind of thing.
Yeah. Yeah. So if I were to ask you,

276
00:16:57,611 --> 00:17:01,510
I remember I took a algorithms
and data structures algorithms.

277
00:17:01,560 --> 00:17:04,230
So of course I remember the,

278
00:17:04,280 --> 00:17:09,280
the professor asked what is an algorithm
and a yelled at everybody in a good way

279
00:17:12,180 --> 00:17:15,030
that nobody was answering it correctly.
Everybody knew what the Alpha,

280
00:17:15,060 --> 00:17:18,120
it was a graduate course.
Everybody knew what an algorithm was,

281
00:17:18,121 --> 00:17:21,030
but they weren't able to answer
it. Well. So let me ask you, uh,

282
00:17:21,090 --> 00:17:23,430
in that same spirit,
what is deep learning?

283
00:17:24,500 --> 00:17:29,500
I would say deep learning is any kind of
machine learning that involves learning

284
00:17:31,851 --> 00:17:35,810
parameters of more than
one consecutive step.

285
00:17:37,260 --> 00:17:41,340
So that would mean shallow learning
is things where you learn a lot of

286
00:17:41,341 --> 00:17:43,080
operations that happen in parallel.

287
00:17:43,710 --> 00:17:46,510
You might have a system that
makes multiple steps, um,

288
00:17:46,620 --> 00:17:50,940
and like you might have had
designed extractors, uh,

289
00:17:50,941 --> 00:17:52,530
but really only one step is learning.

290
00:17:52,590 --> 00:17:57,420
Deep learning is anything where you
have multiple operations in sequence and

291
00:17:57,690 --> 00:17:59,730
that includes the things that
are really popular today,

292
00:17:59,731 --> 00:18:03,510
like convolutional networks
and recurrent networks. Uh,

293
00:18:03,540 --> 00:18:06,540
but it also includes some of the
things that have died out, uh,

294
00:18:06,570 --> 00:18:11,570
like bolts and machines where we
weren't using backpropagation today.

295
00:18:12,301 --> 00:18:17,301
I hear a lot of people define
deep learning as gradient
descent applied to these

296
00:18:19,530 --> 00:18:21,030
differentiable functions.

297
00:18:21,450 --> 00:18:24,750
And I think that's a
legitimate usage of the term.

298
00:18:24,751 --> 00:18:27,390
It's just different from the
way that I used the term myself.

299
00:18:27,780 --> 00:18:29,250
So what's an example

300
00:18:29,830 --> 00:18:34,830
of a deep learning that is not Grady and
descendant differentiable functions in

301
00:18:35,141 --> 00:18:39,950
your, I mean, not specifically
perhaps, but more even
looking into the future. What,

302
00:18:39,951 --> 00:18:43,990
what, what's your thought
about that space of approaches?

303
00:18:44,270 --> 00:18:48,620
Yeah, so I tend to think of
machine learning algorithms
is decomposed into really

304
00:18:48,621 --> 00:18:49,790
three different pieces.

305
00:18:50,180 --> 00:18:54,230
There's the model which can be something
like a neural nut or a bolt and machine

306
00:18:54,590 --> 00:18:56,120
or a recurrent model.

307
00:18:56,600 --> 00:19:00,080
And that basically just describes how
do you take data and how do you take

308
00:19:00,081 --> 00:19:02,080
parameters and you know,

309
00:19:02,210 --> 00:19:06,320
what function do you use
to make a prediction given
the data on the parameters.

310
00:19:06,890 --> 00:19:11,870
Um, another piece of the learning
algorithm is the optimization algorithm.

311
00:19:12,350 --> 00:19:15,890
Or not every algorithm can be really
described in terms of optimization.

312
00:19:15,891 --> 00:19:20,330
But what's the algorithm for updating
the parameters or updating whatever the

313
00:19:20,331 --> 00:19:23,910
state of the network is? Uh, and then the,

314
00:19:24,140 --> 00:19:26,200
the last part is the,
the Dataset.

315
00:19:26,240 --> 00:19:31,190
Like how do you actually represent the
world as it comes into your machine

316
00:19:31,191 --> 00:19:32,760
learning system?
Um,

317
00:19:33,110 --> 00:19:37,580
so I think of deep learning as telling
us something about what does the model

318
00:19:37,581 --> 00:19:41,030
look like and basically
to qualify as deep,

319
00:19:41,240 --> 00:19:46,240
I say that it just has to have multiple
layers that can be multiple steps in a

320
00:19:46,611 --> 00:19:50,780
feed forward differentiable computation
that can be multiple layers in a

321
00:19:50,781 --> 00:19:51,770
graphical model.

322
00:19:52,010 --> 00:19:55,030
There's a lot of ways that you could
satisfy me that something has, uh,

323
00:19:55,250 --> 00:19:57,890
multiple steps that are each
parameterized separately.

324
00:19:58,910 --> 00:20:01,490
I think of gradient descent as
being all about that other piece.

325
00:20:01,491 --> 00:20:03,800
The how do you actually
update the parameters piece?

326
00:20:04,220 --> 00:20:08,050
So you could imagine having a deep model
like a convolutional net and training

327
00:20:08,051 --> 00:20:10,910
it with something like evolution
or a genetic algorithm.

328
00:20:11,270 --> 00:20:14,570
And I would say that still
qualifies as deep learning. Uh,

329
00:20:14,810 --> 00:20:18,680
and then in terms of models that
aren't necessarily differentiable, uh,

330
00:20:18,770 --> 00:20:23,770
I guess bolts and machines are probably
the main example of something where you

331
00:20:24,321 --> 00:20:28,790
can't really take a derivative
and use that for the learning
process. Uh, but you,

332
00:20:28,820 --> 00:20:33,680
you can still argue that the model has
many steps of processing that it applies

333
00:20:33,710 --> 00:20:35,060
when you run in France in the model.

334
00:20:35,750 --> 00:20:38,260
So the steps of processing,
that's key.

335
00:20:38,910 --> 00:20:43,650
So Geoff Hinton suggest that we need to
throw away back pop backpropagation and

336
00:20:43,651 --> 00:20:46,080
start all over.
What do you think about that?

337
00:20:46,470 --> 00:20:50,350
What could an alternative direction
of training neural networks look like?

338
00:20:50,920 --> 00:20:54,250
I don't know that backpropagation
is going to go away entirely.

339
00:20:54,650 --> 00:20:59,650
Most of the time when we decided that a
machine learning algorithm isn't on the

340
00:21:00,460 --> 00:21:04,660
critical path to research for improving
AI, the algorithm doesn't die.

341
00:21:04,661 --> 00:21:07,510
It just becomes used for some
specialized set of things.

342
00:21:08,080 --> 00:21:13,080
A lot of algorithms like logistic
regression don't seem that exciting to AI

343
00:21:13,331 --> 00:21:17,680
researchers who are working on things
like speech recognition or autonomous cars

344
00:21:17,681 --> 00:21:18,370
today.

345
00:21:18,370 --> 00:21:22,780
But there's still a lot of
use for logistic regression
and things like analyzing

346
00:21:22,781 --> 00:21:26,790
really noisy data in
medicine and finance or um,

347
00:21:26,890 --> 00:21:31,270
making really rapid predictions in
really time limited contexts. So I think,

348
00:21:31,300 --> 00:21:31,840
I think uh,

349
00:21:31,840 --> 00:21:36,730
backpropagation and gradient descent are
around to stay but they may not end up

350
00:21:36,731 --> 00:21:38,180
being,
um,

351
00:21:38,320 --> 00:21:42,130
everything that we need to get to
real human level or superhuman AI.

352
00:21:42,370 --> 00:21:43,203
Are you optimistic?

353
00:21:43,310 --> 00:21:46,630
Bought us discovering s you know,

354
00:21:46,680 --> 00:21:51,310
backpropagation has been
around for a few decades so uh,

355
00:21:51,470 --> 00:21:56,210
I optimistic about us as a community
being able to discover something better.

356
00:21:56,830 --> 00:22:01,360
Yeah, I am I think, I think we likely
will find something that works better.

357
00:22:01,810 --> 00:22:06,810
You could imagine things like having
stacks of models where some of the lower

358
00:22:07,061 --> 00:22:09,850
level models predict perimeters
of the higher level models.

359
00:22:10,180 --> 00:22:13,940
And so at the top level you're not
learning in terms of literally calculate

360
00:22:13,941 --> 00:22:17,140
ingredients, but just predicting
how different values will perform.

361
00:22:17,680 --> 00:22:21,520
You can kind of see that already in some
areas like Basie and optimization where

362
00:22:21,521 --> 00:22:24,790
you have a Gaussian process that predicts
how well different parameter values

363
00:22:24,791 --> 00:22:25,624
will perform.

364
00:22:25,870 --> 00:22:29,020
We already use those kinds of algorithms
for things like hyper parameter

365
00:22:29,021 --> 00:22:32,620
optimization and in general we know a
lot of things other than back prop that

366
00:22:32,621 --> 00:22:34,450
worked really well for specific problems.

367
00:22:34,960 --> 00:22:39,520
The main thing we haven't found is a way
of taking one of these other non back

368
00:22:39,521 --> 00:22:44,290
prop based algorithms and having it
really advanced the state of the art on an

369
00:22:44,560 --> 00:22:47,670
AI level problem. Right. But I, I,

370
00:22:47,790 --> 00:22:51,030
I wouldn't be surprised if eventually we
find that some of these algorithms that

371
00:22:51,540 --> 00:22:53,940
even the ones that already exist,
not even necessarily a new one,

372
00:22:54,210 --> 00:22:59,210
we might find some way of customizing
one of these algorithms to do something

373
00:22:59,761 --> 00:23:04,280
really interesting at the
level of cognition or, or the,

374
00:23:04,281 --> 00:23:05,580
the level of,
um,

375
00:23:06,390 --> 00:23:10,360
I think one system that we really don't
have working quite right yet is uh,

376
00:23:10,620 --> 00:23:14,410
like short term memory.
We have things like Lstm,

377
00:23:14,480 --> 00:23:16,820
they're called long short term memory.
Uh,

378
00:23:16,830 --> 00:23:21,600
they still don't do quite what a
human does with short term memory.

379
00:23:22,040 --> 00:23:22,830
Um,

380
00:23:22,830 --> 00:23:27,830
like gradient descent to learn a specific
fact has to do multiple steps on that

381
00:23:28,891 --> 00:23:33,180
fact. Like, if I, I tell you,
the meeting today is at 3:00 PM,

382
00:23:33,590 --> 00:23:37,080
um, I don't need to say over and
over again. It's at 3:00 PM 3:00 PM.

383
00:23:37,081 --> 00:23:40,380
It's at 3:00 PM. It's at 3:00 PM for
you to do a great step on each one.

384
00:23:40,380 --> 00:23:43,040
You just hear it once and you remember it.
Um,

385
00:23:43,140 --> 00:23:47,450
there's been some work on things like,
uh, self attention and attention,

386
00:23:47,451 --> 00:23:52,190
like mechanisms like the neural Turing
machine that can write to memory cells

387
00:23:52,191 --> 00:23:54,530
and update themselves with
facts like that right away.

388
00:23:54,860 --> 00:23:56,390
But I don't think we've
really nailed it yet.

389
00:23:56,870 --> 00:24:01,870
And that's one area where I'd imagine
that new optimization algorithms or

390
00:24:02,781 --> 00:24:07,280
different ways of applying
existing optimization
algorithms could give us a way

391
00:24:07,281 --> 00:24:11,270
of just lightening fast updating the
state of a machine learning system to

392
00:24:11,271 --> 00:24:15,590
contain a specific fact like that without
needing to have it presented over and

393
00:24:15,591 --> 00:24:16,424
over and over again.

394
00:24:16,930 --> 00:24:21,930
So some of the success of symbolic
systems in the 80s is they were able to

395
00:24:22,691 --> 00:24:25,660
assemble these kinds of facts, uh, better,

396
00:24:26,200 --> 00:24:30,700
but there's a lot of expert input required
and it's very limited in that sense.

397
00:24:31,130 --> 00:24:35,890
Do you ever look back to that as a
something that will have to return to

398
00:24:35,891 --> 00:24:39,610
eventually sort of dust off
the book from the shelf and uh,

399
00:24:39,670 --> 00:24:42,890
think about how we build knowledge,
representation, knowledge,

400
00:24:43,210 --> 00:24:45,760
you have to use graph
searches and searches, right?

401
00:24:45,761 --> 00:24:48,940
And like first order logic and entailment
and things like that. A thing. Yeah,

402
00:24:48,941 --> 00:24:49,530
exactly.

403
00:24:49,530 --> 00:24:53,340
In my particular line of work,
which has mostly been machine learning,

404
00:24:53,341 --> 00:24:56,340
security and,
and also generative modeling,

405
00:24:56,730 --> 00:25:01,440
I haven't usually found myself moving
in that direction for generative models.

406
00:25:01,441 --> 00:25:05,520
I could see a little bit of, it could
be useful if you had something like a,

407
00:25:06,540 --> 00:25:08,670
a differentiable,
uh,

408
00:25:08,700 --> 00:25:12,150
knowledge base or some
other kind of knowledge base
where it's possible for some

409
00:25:12,151 --> 00:25:16,260
of our fuzzy or machine
learning algorithms to interact
with the knowledge base.

410
00:25:16,860 --> 00:25:17,690
I mean,
you're on that one.

411
00:25:17,690 --> 00:25:21,200
It's kind of like that. It's a
differentiable knowledge base of sorts.

412
00:25:21,390 --> 00:25:22,170
Yeah.

413
00:25:22,170 --> 00:25:27,170
But if we had a really easy way of giving
feedback to machine learning models,

414
00:25:29,250 --> 00:25:32,370
that would clearly help a lot with,
with generative models.

415
00:25:32,371 --> 00:25:35,400
And so you could imagine one way of
getting there would be get a lot better at

416
00:25:35,401 --> 00:25:39,150
natural language processing. But another
way of getting there would be a tick,

417
00:25:39,151 --> 00:25:42,810
some kind of knowledge base and figure
out a way for it to actually interact

418
00:25:42,811 --> 00:25:46,560
with a neural network, being able to have
a chat, we'll then y'all network. Yeah.

419
00:25:47,820 --> 00:25:51,450
So like one thing and generative models
we see a lot today is you'll get things

420
00:25:51,451 --> 00:25:53,400
like faces that are not symmetrical,

421
00:25:54,640 --> 00:25:57,810
like people that have two eyes
that are different colors.

422
00:25:58,190 --> 00:26:00,810
And I mean there are people with eyes
that are different colors in real life,

423
00:26:00,840 --> 00:26:04,950
but not nearly as many of them as you
tend to see in the machine learning

424
00:26:04,951 --> 00:26:05,784
generated data.

425
00:26:06,060 --> 00:26:09,880
So if you had either a knowledge
base that could contain the fact, uh,

426
00:26:10,170 --> 00:26:15,170
people's faces are generally approximately
symmetric and eye color is especially

427
00:26:15,631 --> 00:26:17,850
likely to be the same on both sides.
Uh,

428
00:26:17,910 --> 00:26:22,350
being able to just inject that hint
into the machine learning model without

429
00:26:22,351 --> 00:26:27,150
having to discover that itself after
studying a lot of data would be a really

430
00:26:27,151 --> 00:26:27,984
useful feature.

431
00:26:28,320 --> 00:26:31,050
I could see a lot of ways they're getting
there without bringing back some of

432
00:26:31,051 --> 00:26:32,160
the 1980s technology.

433
00:26:32,161 --> 00:26:36,780
But I also see some ways that you could
imagine extending the 1980s technology

434
00:26:36,781 --> 00:26:39,390
to play nice with neural nets
and have it helped get there.

435
00:26:40,040 --> 00:26:41,100
Awesome. So, uh,

436
00:26:41,120 --> 00:26:45,870
he talked about the story of you coming
up with idea of gans at a bar with some

437
00:26:45,871 --> 00:26:50,340
friends. You were arguing
that this, you know, uh,

438
00:26:50,370 --> 00:26:54,240
gans would work generative adversarial
networks and the others didn't think so.

439
00:26:54,660 --> 00:26:59,190
Then you went home at midnight,
code it up and it worked. So if,

440
00:26:59,210 --> 00:27:02,670
if I was a friend of yours, a at
the bar, I would also have doubts.

441
00:27:02,700 --> 00:27:06,780
It's a really nice idea, but I'm very
skeptical that it would work. Uh,

442
00:27:06,781 --> 00:27:08,820
what was the basis of their skepticism?

443
00:27:09,270 --> 00:27:13,020
What was the basis of your intuition?
Why should work?

444
00:27:14,350 --> 00:27:17,650
I don't want to be somebody
who goes around to promoting
alcohol for the message

445
00:27:17,651 --> 00:27:19,990
of science,
but in this case,

446
00:27:19,991 --> 00:27:22,420
I do actually think that
drinking helped a little bit.

447
00:27:23,050 --> 00:27:25,360
When your inhibitions are lowered,

448
00:27:25,361 --> 00:27:29,230
you're more willing to try out things
that you wouldn't try out otherwise.

449
00:27:29,590 --> 00:27:33,760
So I have noticed in general that
I'm less prone to shooting down.

450
00:27:33,761 --> 00:27:37,060
So am I an idea is when I'm,
when I have had a little bit to drink.

451
00:27:37,930 --> 00:27:40,430
I think if I had had
that idea at lunch time,

452
00:27:40,800 --> 00:27:43,460
yeah it probably would have thought it.
It's hard enough to train wonder.

453
00:27:43,461 --> 00:27:46,690
I'll net you can't train a second neural
net in the inner loop of the outer

454
00:27:46,720 --> 00:27:49,810
neural net.
That was basically my friends objection.

455
00:27:49,811 --> 00:27:53,710
Was that trying to train to neural nets
at the same time, we'd be too hard.

456
00:27:54,250 --> 00:27:55,540
So it was more about the training

457
00:27:55,630 --> 00:27:58,810
process and less so my
skepticism would be, you know,

458
00:27:58,811 --> 00:28:03,190
I'm sure you could train it, but
uh, the thing would converge to,

459
00:28:03,191 --> 00:28:06,210
would not be able to generate
anything reasonable and, and,

460
00:28:06,211 --> 00:28:08,110
and you kind of reasonable realism.

461
00:28:08,210 --> 00:28:09,080
Yeah.
So,

462
00:28:09,081 --> 00:28:13,220
so part of what all of us were thinking
about when we had this conversation was

463
00:28:13,610 --> 00:28:16,970
deep bolts and machines, which a
lot of us in the lab, including me,

464
00:28:16,971 --> 00:28:18,980
we're a big fan of deep
Bolton machines at the time.

465
00:28:20,650 --> 00:28:23,980
They involved two separate
processes running at the same time.

466
00:28:25,090 --> 00:28:30,090
One of them is called the positive phase
where you load data into the model and

467
00:28:31,361 --> 00:28:32,980
tell the model to make
the data more likely.

468
00:28:33,550 --> 00:28:36,730
The other one is called the negative phase
where you draw samples from the model

469
00:28:37,000 --> 00:28:40,340
until the model to make those
samples less likely. Um,

470
00:28:41,170 --> 00:28:43,960
in a deep bolts and machine,
it's not trivial to generate a sample.

471
00:28:43,961 --> 00:28:48,310
You have to actually run an iterative
process that gets better and better

472
00:28:48,550 --> 00:28:52,390
samples coming closer and closer to
the distribution the model represents.

473
00:28:52,810 --> 00:28:53,890
So during the training process,

474
00:28:53,891 --> 00:28:57,110
you're always running these two
systems at the same time, uh,

475
00:28:57,130 --> 00:28:59,740
one that's updating the parameters of the
model and another one that's trying to

476
00:28:59,741 --> 00:29:01,000
generate samples from the model.

477
00:29:01,660 --> 00:29:04,290
And they worked really well
and things like [inaudible],

478
00:29:04,330 --> 00:29:05,800
but a lot of it's in the lab,
including me,

479
00:29:05,801 --> 00:29:10,801
had tried to get people to machines
to scale past feminist to things like

480
00:29:10,811 --> 00:29:14,680
generating color photos and we just
couldn't get the two processes to stay

481
00:29:14,950 --> 00:29:18,730
synchronized. Um, so when
I had the idea for gains,

482
00:29:18,731 --> 00:29:21,580
a lot of people thought
that the discriminator would
have more or less the same

483
00:29:21,581 --> 00:29:23,770
problem as the negative phase.

484
00:29:23,800 --> 00:29:27,490
And the bolts and machine that trying
to train the discriminator in the inner

485
00:29:27,491 --> 00:29:27,790
loop,

486
00:29:27,790 --> 00:29:31,690
you just couldn't get it to keep up with
the generator and the outer loop and

487
00:29:31,691 --> 00:29:35,590
that would prevent it from
converging to anything useful. Yeah,

488
00:29:35,680 --> 00:29:38,200
I share that intuition. Yeah. Um,

489
00:29:38,980 --> 00:29:40,960
well it turns out to not be the,

490
00:29:41,920 --> 00:29:43,750
a lot of the time with
machine learning algorithms,

491
00:29:43,751 --> 00:29:46,450
it's really hard to predict ahead of
time how well they'll actually perform.

492
00:29:46,570 --> 00:29:49,060
Right? You have to just run the
experiment and see what happens.

493
00:29:49,150 --> 00:29:54,150
And I would say I still today don't have
like one factor I can put my finger on

494
00:29:54,371 --> 00:29:54,820
and say,

495
00:29:54,820 --> 00:29:59,820
this is why Ganz worked for
photo generation and deep
bolts and machines don't.

496
00:30:01,960 --> 00:30:06,340
There are a lot of theory papers showing
that under some theoretical settings,

497
00:30:06,341 --> 00:30:09,460
the again algorithm
does actually converge,

498
00:30:10,660 --> 00:30:15,660
but those settings are restricted enough
that they don't necessarily explain the

499
00:30:16,661 --> 00:30:19,540
whole picture in terms of all the
results that we see in practice.

500
00:30:20,740 --> 00:30:22,990
So taking a step back, can you, uh,

501
00:30:22,991 --> 00:30:24,850
in the same way as we've
talked about deep learning,

502
00:30:24,851 --> 00:30:29,740
can you tell me what generative
adversarial networks are? A, yeah.

503
00:30:29,741 --> 00:30:33,460
So generative adversarial networks are
a particular kind of generative model.

504
00:30:33,970 --> 00:30:38,470
A generative model is a machine learning
model that can train on some set of

505
00:30:38,471 --> 00:30:38,860
data.

506
00:30:38,860 --> 00:30:42,640
Like say you have a collection of photos
of cats and you want to generate more

507
00:30:42,641 --> 00:30:47,641
photos of cats or you want to estimate
a probability distribution over cats.

508
00:30:47,670 --> 00:30:52,630
So you can ask how likely it is that
some new images, a photo of a cat, um,

509
00:30:52,960 --> 00:30:55,400
gans are one way of doing this.
Uh,

510
00:30:55,780 --> 00:30:58,870
some generative models are
good at creating new data. Uh,

511
00:30:59,170 --> 00:31:02,890
other generative models
are good at estimating that
density function and telling

512
00:31:02,891 --> 00:31:07,891
you how likely particular pieces of data
are to come from the same distribution

513
00:31:08,081 --> 00:31:08,914
as the training data.

514
00:31:09,620 --> 00:31:14,620
Gans are more focused on generating
samples rather than estimating the density

515
00:31:14,771 --> 00:31:18,460
function. There are some kinds of
gowns like slogan that can do both,

516
00:31:18,490 --> 00:31:21,540
but mostly gowns are about
generating samples, uh,

517
00:31:21,580 --> 00:31:25,150
generating new photos of
cats that look realistic. Uh,

518
00:31:25,210 --> 00:31:28,990
and they do that completely from scratch.

519
00:31:29,290 --> 00:31:31,870
It's analogous to human imagination.

520
00:31:32,170 --> 00:31:34,360
When again creates a new image of a cat.

521
00:31:34,720 --> 00:31:39,720
It's using a neural network to produce
a cat that has not existed before.

522
00:31:40,990 --> 00:31:44,890
It isn't doing something like
compositing photos together. You're not,

523
00:31:45,100 --> 00:31:47,950
you're not literally taking the eye off
of one cat and the Erath off of another

524
00:31:47,951 --> 00:31:51,790
cat. It's, it's more of this
digestive process where the,

525
00:31:52,180 --> 00:31:55,750
the neural net trains in a lot of data
and comes up with some representation of

526
00:31:56,110 --> 00:31:59,050
the probability distribution
and generates entirely new cats.

527
00:31:59,780 --> 00:32:01,930
There are a lot of different ways
of building a generative model.

528
00:32:01,960 --> 00:32:06,790
What's specific to Ganz is that we have
a two player game and the game theoretic

529
00:32:06,791 --> 00:32:09,880
sense and as the players
in this game compete,

530
00:32:10,270 --> 00:32:13,240
one of them becomes able
to generate realistic data.

531
00:32:13,900 --> 00:32:15,610
The first player is called the generator.

532
00:32:16,090 --> 00:32:20,110
It produces output data such
as just images for example.

533
00:32:20,620 --> 00:32:22,390
And at the start of the learning process,

534
00:32:22,391 --> 00:32:24,370
it'll just produce
completely random images.

535
00:32:25,090 --> 00:32:26,830
The other player is
called the discriminator.

536
00:32:27,370 --> 00:32:31,390
The discriminator takes images as input
and guesses whether they're real or fake.

537
00:32:32,040 --> 00:32:36,070
Uh, you train it both on real data. So
photos that come from your training set,

538
00:32:36,100 --> 00:32:39,380
actual photos of cats, and you try
not to say that those are real.

539
00:32:39,830 --> 00:32:44,570
You also train it on images that come
from the generator network and you train

540
00:32:44,571 --> 00:32:48,740
it to say that those are fake.
As the two players compete in this game,

541
00:32:49,190 --> 00:32:52,160
the discriminator tries to become better
at recognizing where their images are

542
00:32:52,161 --> 00:32:52,994
real or fake.

543
00:32:53,300 --> 00:32:57,350
And the genitor becomes better at fooling
the discriminator into thinking that

544
00:32:57,410 --> 00:33:00,370
it's outputs are our real.
Uh,

545
00:33:00,800 --> 00:33:04,040
and you can analyze this through the
language of Game Theory and find that

546
00:33:04,041 --> 00:33:09,041
there's a nash equilibrium where the
generator has captured the correct

547
00:33:09,141 --> 00:33:12,140
probability distribution.
So in the cat example,

548
00:33:12,141 --> 00:33:16,850
it makes perfectly realistic cat photos
and the discriminator is unable to do

549
00:33:16,851 --> 00:33:19,580
better than random
guessing because all the,

550
00:33:20,000 --> 00:33:23,990
all the samples coming from both the data
and the generator look equally likely

551
00:33:23,991 --> 00:33:26,930
to have come from either source.
So do you ever,

552
00:33:27,320 --> 00:33:32,320
do you ever sit back and just blow your
mind that this thing works so from very,

553
00:33:33,320 --> 00:33:36,650
so it's able to estimate that
Desi function enough to generate,

554
00:33:37,010 --> 00:33:41,900
generate realistic images? I mean,
uh, yeah. Do you ever sit back?

555
00:33:43,640 --> 00:33:46,730
How does this even,
why this is quite incredible,

556
00:33:46,731 --> 00:33:49,920
especially where against have gone
in terms of realism. Yeah. And,

557
00:33:49,930 --> 00:33:51,590
and not just a flutter in my own work,

558
00:33:51,591 --> 00:33:56,591
but generative models all over them have
this property that if they really did

559
00:33:57,531 --> 00:34:00,890
what we asked them to do, they would do
nothing but memorize the training data,

560
00:34:01,040 --> 00:34:05,120
right? Um, models that are based
on maximizing the likelihood,

561
00:34:05,730 --> 00:34:09,890
the way that you obtain the maximum
likelihood for a specific training set is

562
00:34:09,891 --> 00:34:13,820
you assign all of your probability mass
to the training examples and nowhere

563
00:34:13,821 --> 00:34:17,960
else, uh, for gans that game
is played using a training set.

564
00:34:18,350 --> 00:34:22,240
So the way that you become unbeatable
in the game is you literally memorized

565
00:34:22,241 --> 00:34:23,210
training examples.

566
00:34:25,310 --> 00:34:28,790
One of my former interns read a paper,
uh,

567
00:34:28,820 --> 00:34:30,590
his name is if I should
not have an Auger Rajan.

568
00:34:31,010 --> 00:34:35,060
And he showed that it's actually hard
for the generator to memorize the try and

569
00:34:35,061 --> 00:34:36,770
get a heart and uh,

570
00:34:37,050 --> 00:34:42,050
a statistical learning theory sense that
you can actually create reasons for why

571
00:34:43,580 --> 00:34:48,580
it would require quite a lot of learning
steps and a lot of observations of,

572
00:34:50,840 --> 00:34:53,840
of different latent variables before
you could memorize the training data.

573
00:34:54,290 --> 00:34:57,950
That still doesn't really explain why
when you produce samples that are new,

574
00:34:58,160 --> 00:35:01,820
why do you get compelling
images rather than just garbage?

575
00:35:01,821 --> 00:35:03,170
That's different from the training set.

576
00:35:03,710 --> 00:35:06,530
And I don't think we really
have a good answer for that,

577
00:35:06,890 --> 00:35:11,890
especially if you think about how many
possible images are out there and how few

578
00:35:11,991 --> 00:35:14,720
images the generative
model sees during training.

579
00:35:15,380 --> 00:35:19,610
It seems just unreasonable that generative
models create new images as well as

580
00:35:19,611 --> 00:35:20,444
they do,

581
00:35:20,770 --> 00:35:23,960
especially considering that we're
basically training them to memorize rather

582
00:35:23,961 --> 00:35:26,100
than generalized.
Uh,

583
00:35:26,180 --> 00:35:31,180
I think part of the answer is there's a
paper called deep image prior where they

584
00:35:31,251 --> 00:35:34,100
show that you can take a convolutional
net and you don't even need to learn the

585
00:35:34,101 --> 00:35:34,940
parameters of it at all.

586
00:35:34,970 --> 00:35:39,480
You just use the model architecture and
it's already useful for things like in

587
00:35:39,481 --> 00:35:40,380
painting images.

588
00:35:41,040 --> 00:35:44,790
I think that shows us that
the convolutional network
architecture captures

589
00:35:44,791 --> 00:35:47,370
something really important
about the structure of images.

590
00:35:47,940 --> 00:35:52,200
And we don't need to actually use the
learning to capture all the information

591
00:35:52,201 --> 00:35:56,160
coming out of the convolutional
net. Uh, that would,

592
00:35:56,700 --> 00:35:59,490
that would imply that it would be much
harder to make generative models and

593
00:35:59,491 --> 00:36:00,480
other domains.

594
00:36:01,230 --> 00:36:04,500
So far we're able to make reasonable
speech models and things like that.

595
00:36:04,860 --> 00:36:06,360
But to be honest,

596
00:36:06,361 --> 00:36:09,750
we haven't actually explored a whole lot
of different data sets all that much.

597
00:36:09,751 --> 00:36:14,550
We don't, for example, see a
lot of deep learning models of,

598
00:36:15,090 --> 00:36:15,923
um,

599
00:36:16,470 --> 00:36:21,470
like biology datasets where you have lots
of microarrays measuring the amount of

600
00:36:21,481 --> 00:36:22,830
different enzymes and things like that.

601
00:36:22,831 --> 00:36:26,850
So we may find that some of the progress
that we've seen for images and speech

602
00:36:26,851 --> 00:36:29,700
turns out to really rely heavily
on the model architecture.

603
00:36:30,120 --> 00:36:34,830
And we were able to do what we did for
vision by trying to reverse engineer the

604
00:36:34,831 --> 00:36:39,440
human visual system. Right.
Um, and, and maybe it'll turn
out that we can't just, uh,

605
00:36:39,780 --> 00:36:42,390
use that same trick for
arbitrary kinds of data.

606
00:36:43,430 --> 00:36:45,830
Right? So there's aspects
of the human vision system,

607
00:36:45,920 --> 00:36:50,120
the hardware of it that
makes it without learning,

608
00:36:50,121 --> 00:36:53,660
with all cognition just makes it really
effective at detecting the patterns

609
00:36:53,661 --> 00:36:58,340
we've seen. The visual world.
Yeah. That's, yeah, that's
really interesting. Uh, what,

610
00:36:58,341 --> 00:36:59,174
um,

611
00:36:59,790 --> 00:37:00,630
in a big

612
00:37:01,230 --> 00:37:04,560
quick overview,
in your view and your view,

613
00:37:04,590 --> 00:37:09,000
what types of gangs are there and what
other generative models besides Gans are

614
00:37:09,001 --> 00:37:09,834
there?

615
00:37:10,030 --> 00:37:13,480
Yeah. Um, so it's maybe a
little bit easier to start with.

616
00:37:13,481 --> 00:37:16,270
What kinds of generative models
are there other than cans? Um,

617
00:37:16,810 --> 00:37:21,810
so most generative models are
likelihood based where to train them.

618
00:37:23,051 --> 00:37:25,450
You have a model that tells you how,

619
00:37:25,720 --> 00:37:28,630
how much probability to signs
to a particular example,

620
00:37:29,080 --> 00:37:32,740
and you just maximize the probability
assigned to all the training examples.

621
00:37:33,670 --> 00:37:38,650
It turns out that it's hard to design a
model that can create really complicated

622
00:37:38,651 --> 00:37:39,161
images,

623
00:37:39,161 --> 00:37:44,161
are really complicated audio wave
forms and still have it be possible to

624
00:37:45,430 --> 00:37:48,310
estimate the likelihood function

625
00:37:49,990 --> 00:37:51,950
from a computational point of view.
Uh,

626
00:37:52,180 --> 00:37:54,190
most interesting models that
you would just write down.

627
00:37:54,191 --> 00:37:58,540
Intuitively it turns out that it's almost
impossible to calculate the amount of

628
00:37:58,541 --> 00:38:01,560
probability they assigned
to a particular point. Um,

629
00:38:01,630 --> 00:38:06,100
so there's a few different
schools of generative models
in the likelihood family.

630
00:38:06,560 --> 00:38:11,020
Uh, one approach is to very carefully
designed the model so that it is

631
00:38:11,021 --> 00:38:13,660
computationally tractable
to measure the density,

632
00:38:13,661 --> 00:38:17,080
to assign to a particular point.
So there are things like, uh,

633
00:38:17,260 --> 00:38:21,220
auto regressive models
like Pixel, CNN, uh,

634
00:38:21,250 --> 00:38:26,250
those basically break down the probability
distribution into a product over

635
00:38:27,161 --> 00:38:29,650
every single feature.
So for an image,

636
00:38:29,651 --> 00:38:34,300
you estimate the probability of each
pixel given all of the pixels that came

637
00:38:34,301 --> 00:38:38,740
before it. Uh, there's tricks where if
you want to measure the density function,

638
00:38:39,100 --> 00:38:42,070
you can actually calculate the
density for all these pixels,

639
00:38:42,100 --> 00:38:44,100
more or less in parallel,
uh,

640
00:38:44,320 --> 00:38:48,100
generating the image still tends to
require you to go one pixel at a time,

641
00:38:48,640 --> 00:38:51,880
and that can be very
slow. Uh, but there again,

642
00:38:51,881 --> 00:38:54,990
tricks for doing this in a hierarchical
pattern where you can keep the runtime

643
00:38:55,000 --> 00:39:00,000
under control or the quality of the
images the generates putting runtime aside

644
00:39:00,940 --> 00:39:03,670
pretty good. Uh, they're,
they're reasonable. Yeah.

645
00:39:04,390 --> 00:39:08,890
I would say a lot of the best
results are from gans these days,

646
00:39:08,920 --> 00:39:13,920
but it can be hard to tell how much of
that is based on who's studying which

647
00:39:14,921 --> 00:39:18,880
type of algorithm, if that makes sense.
The amount of effort and invest in empty.

648
00:39:18,910 --> 00:39:21,190
Yeah. Or, or like the kind of expertise.

649
00:39:21,370 --> 00:39:24,430
So a lot of people who have traditionally
been excited about graphics or art and

650
00:39:24,431 --> 00:39:28,210
things like that have gotten interested
in gans and to some extent it's hard to

651
00:39:28,211 --> 00:39:30,440
tell are gans doing better,
uh,

652
00:39:30,790 --> 00:39:35,580
because they have a lot of graphics and
art experts behind them or are against

653
00:39:35,590 --> 00:39:40,030
doing better because
they're more computationally
efficient or are against doing

654
00:39:40,031 --> 00:39:43,340
better because they prioritize
the realism of samples. All right.

655
00:39:43,480 --> 00:39:45,940
Over the accuracy of that
density function. I think,

656
00:39:46,240 --> 00:39:49,150
I think all of those are potentially
valid explanations and it's,

657
00:39:49,480 --> 00:39:50,590
it's hard to tell.

658
00:39:51,310 --> 00:39:56,310
So can you give a brief history
of gans from 2014 we paid for 13.

659
00:39:59,230 --> 00:39:59,951
Yeah.
Um,

660
00:39:59,951 --> 00:40:04,450
so a few highlights in the first paper
we just showed that gans basically work,

661
00:40:04,690 --> 00:40:09,690
if you look back at the samples we had
now they look terrible on the CFR 10 data

662
00:40:09,761 --> 00:40:13,450
set. You can't even recognize
objects in them. Your paper, sorry.

663
00:40:13,480 --> 00:40:18,010
It'll use see far 10. We Use Eminence,
which is a little handwritten digits. Uh,

664
00:40:18,011 --> 00:40:22,330
we use the Toronto face database,
which is small grayscale photos of faces.

665
00:40:22,660 --> 00:40:24,190
We did have recognizable faces.

666
00:40:24,191 --> 00:40:28,360
My colleague Bing Shoe put together the
first Gan face model for that paper.

667
00:40:28,900 --> 00:40:32,530
Um,
we also had the CFR 10 data set,

668
00:40:32,920 --> 00:40:37,700
which is things like very small,
32 by 32 pixels of, of uh,

669
00:40:37,720 --> 00:40:41,110
cars and cats and dogs have for that.

670
00:40:41,111 --> 00:40:42,910
We didn't get recognizable objects,

671
00:40:42,970 --> 00:40:47,440
but all the deep learning people back
then were really used to looking at these

672
00:40:47,441 --> 00:40:49,930
failed samples and kind of
reading them like tea leaves.

673
00:40:50,350 --> 00:40:54,910
And people who are used to reading the
tea leaves recognize that our tea leaves

674
00:40:54,911 --> 00:40:57,760
at least look different, right?
Maybe not necessarily better,

675
00:40:57,761 --> 00:41:02,590
but there was something unusual about them
and that got a lot of us excited.

676
00:41:03,190 --> 00:41:03,550
Uh,

677
00:41:03,550 --> 00:41:08,550
one of the next really big steps was lept
gown by Emily Denton and Sumif tele at

678
00:41:09,130 --> 00:41:14,130
Facebook AI research where they actually
got really good high resolution photos

679
00:41:14,381 --> 00:41:16,030
working with gangs for the first time.

680
00:41:16,510 --> 00:41:20,050
They had a complicated system where they
generated the image starting at low Rez

681
00:41:20,051 --> 00:41:24,280
and then scaling up to high res. Uh,
but they were able to get it to work.

682
00:41:24,850 --> 00:41:28,450
And then, um, in 2015,

683
00:41:28,720 --> 00:41:31,510
I believe later that same year,
uh,

684
00:41:31,690 --> 00:41:35,690
Alec Radford and [inaudible]
Chin Tele and Luke mets, uh,

685
00:41:35,900 --> 00:41:40,330
published the DC Gann paper,
which it stands for deep convolutional,

686
00:41:40,610 --> 00:41:41,443
again,

687
00:41:41,850 --> 00:41:46,640
it's kind of a non unique name because
these days basically all gans and even

688
00:41:46,641 --> 00:41:49,060
some before that were deepened
convolutional, but, uh,

689
00:41:49,110 --> 00:41:52,760
they just kind of picked a name for a
really great recipe where they were able

690
00:41:52,761 --> 00:41:56,930
to actually using only one model
instead of a multistep process,

691
00:41:57,260 --> 00:42:01,560
actually generate realistic images
of faces and things like that. Um,

692
00:42:01,940 --> 00:42:06,940
that was sort of like the beginning
of the Cambrian explosion of Gans.

693
00:42:07,340 --> 00:42:09,680
Like, you know, once, once you
had animals that had a backbone,

694
00:42:09,681 --> 00:42:12,230
you suddenly got lots of different
versions of, of, you know,

695
00:42:12,231 --> 00:42:15,640
like fish and four legged
animals and things like that. So,

696
00:42:15,680 --> 00:42:19,390
so DC Gann became kind of the backbone
for many different models that came out

697
00:42:19,410 --> 00:42:22,610
used as a baseline even still. Yeah. Yeah.

698
00:42:23,090 --> 00:42:27,500
And so from there I would say some
interesting things we've seen are um,

699
00:42:28,550 --> 00:42:31,610
there's a lot you can say about how
just the quality of standard image

700
00:42:31,611 --> 00:42:33,110
generation Ganz has increased.

701
00:42:33,500 --> 00:42:36,980
But what's also maybe more interesting
on an intellectual level is how the

702
00:42:36,981 --> 00:42:40,830
things you can use gowns
for has also changed. Um,

703
00:42:40,940 --> 00:42:45,380
one thing is that you can use them to
learn classifiers without having to have

704
00:42:45,680 --> 00:42:48,860
class labels for every example in your,
your training set.

705
00:42:48,860 --> 00:42:52,910
So that's called semi supervised
learning. Um, my colleague at open Ai,

706
00:42:52,911 --> 00:42:55,680
Tim Solomons,
who's at brain now a,

707
00:42:55,760 --> 00:42:59,650
wrote a paper called improved
techniques for training. Gans um,

708
00:42:59,720 --> 00:43:00,830
I'm a coauthor on this paper,

709
00:43:00,831 --> 00:43:03,520
but I can't claim any credit
for this particular part. Uh,

710
00:43:03,650 --> 00:43:07,970
one thing he showed in the paper is that
you can take the Gan discriminator and

711
00:43:07,971 --> 00:43:12,500
use it as a classifier that actually
tells you, you know, this image is a cat,

712
00:43:12,501 --> 00:43:16,360
this image is a dark, this image is a
car, this image is a truck. And so on.

713
00:43:16,361 --> 00:43:18,380
Not just to say whether
the image is real or fake,

714
00:43:18,740 --> 00:43:21,950
but if it is real to say specifically
what kind of object it is.

715
00:43:22,550 --> 00:43:27,290
And he found that you can train these
classifiers with far fewer labeled

716
00:43:27,291 --> 00:43:30,260
examples then traditional classifiers.

717
00:43:30,590 --> 00:43:35,240
So a few supervise based on also not
just your discrimination ability,

718
00:43:35,241 --> 00:43:38,390
but your ability to classify,
you going to do much,

719
00:43:38,600 --> 00:43:42,770
you're going to conversion much faster
to being effective at being at this

720
00:43:42,771 --> 00:43:46,240
command. Yeah. So for
example, for the MDS data set,

721
00:43:46,280 --> 00:43:51,020
you want to look at an image
of a handwritten digit and
say whether it's a zero,

722
00:43:51,021 --> 00:43:53,450
a one or a two and so on.
Um,

723
00:43:54,140 --> 00:43:59,140
to get down to less than
1% accuracy required around
60,000 examples until maybe

724
00:44:00,771 --> 00:44:05,771
about 2014 or so in 2016 with this
a semi supervised Gannon project,

725
00:44:07,400 --> 00:44:10,880
Tim was able to get below 1% error,
uh,

726
00:44:10,940 --> 00:44:13,070
using only a hundred labeled examples.

727
00:44:13,580 --> 00:44:17,840
So that was about a 600 x decrease in
the amount of labels that he needed.

728
00:44:18,050 --> 00:44:21,020
He's still using more images in that,

729
00:44:21,050 --> 00:44:24,410
but he doesn't need to have each of them
labeled. As you know, this one's a one,

730
00:44:24,411 --> 00:44:28,160
this one's a two, this one's a
zero and so on. Then to be able to,

731
00:44:28,390 --> 00:44:31,980
for gas to be able to generate
recognizable objects, so objects from

732
00:44:31,980 --> 00:44:33,030
a particular class,

733
00:44:33,360 --> 00:44:38,360
you still need labeled data because
you need to know what it means to be a

734
00:44:39,151 --> 00:44:44,040
particular class cat dog. How do you
think we can move away from that?

735
00:44:44,520 --> 00:44:45,090
Yeah.

736
00:44:45,090 --> 00:44:49,530
Some researchers at Brain Zurich actually
just released a really great paper on

737
00:44:49,560 --> 00:44:53,910
a semi supervised gans where their,
their goal isn't to classify,

738
00:44:53,911 --> 00:44:57,780
it's to make recognizable objects
despite not having a lot of labeled data.

739
00:44:58,620 --> 00:45:03,620
They were working off of deep mines began
project and they showed that they can

740
00:45:04,081 --> 00:45:09,060
match the performance of big began
using only 10%, I believe of the,

741
00:45:09,061 --> 00:45:12,270
of the labels. A big gun was
trained on the image net Dataset,

742
00:45:12,300 --> 00:45:16,740
which is about 1.2 million images
and had all of them labeled, um,

743
00:45:17,430 --> 00:45:20,520
this latest project from brains direct
shows that they're able to get away with

744
00:45:20,910 --> 00:45:24,390
only having about 10% of the,
uh, of the images labeled.

745
00:45:25,500 --> 00:45:27,980
And they do that essentially using a,

746
00:45:28,030 --> 00:45:33,030
a clustering algorithm where
the discriminator learns
to assign the objects to

747
00:45:33,541 --> 00:45:34,374
groups.

748
00:45:34,560 --> 00:45:39,560
And then this understanding that objects
can be grouped into similar types helps

749
00:45:40,681 --> 00:45:45,630
it to for more realistic ideas of what
should be appearing in the image because

750
00:45:45,631 --> 00:45:49,530
it knows that every image it creates has
to come from one of these architectural

751
00:45:49,531 --> 00:45:52,920
groups rather than just
being some arbitrary image.

752
00:45:53,190 --> 00:45:55,080
If you train again with no class labels,

753
00:45:55,081 --> 00:46:00,081
you tend to get things that look sort
of like grass or water or brick or dirt.

754
00:46:00,420 --> 00:46:03,810
But, um, but without necessarily
a lot going on in them.

755
00:46:04,380 --> 00:46:07,530
And I think that's partly because if
you look at a large image, that image,

756
00:46:07,860 --> 00:46:11,250
the object doesn't necessarily
occupy the whole image.

757
00:46:11,251 --> 00:46:15,630
And so you learn to create
realistic sets of Pixels,

758
00:46:15,631 --> 00:46:19,680
but you don't necessarily learn that
the object is the star of the show.

759
00:46:20,100 --> 00:46:21,910
And he wanted to be in
every image you make.

760
00:46:22,170 --> 00:46:24,810
Yeah. You've, I've heard
you talk about the, uh,

761
00:46:24,811 --> 00:46:29,100
the horse to zebra cycle again
mapping and how it turns out,

762
00:46:29,580 --> 00:46:30,413
uh,

763
00:46:30,810 --> 00:46:34,680
again thought provoking that horses are
usually on grass and zebras are usually

764
00:46:34,681 --> 00:46:38,010
on dry terrain. So when you're
doing that kind of generation,

765
00:46:38,190 --> 00:46:42,570
you're going to end up generating
greener horses or whatever.

766
00:46:43,220 --> 00:46:47,310
Uh, and so those are connected together.
It's not just, yeah, you can be able to,

767
00:46:47,400 --> 00:46:51,570
you're not able to segment, uh, be
able to generate in a segment way.

768
00:46:52,320 --> 00:46:56,310
So are there other types of games
you've come across in your mind, uh,

769
00:46:57,240 --> 00:47:00,920
that, uh, neural networks can
play with each other to, uh,

770
00:47:01,980 --> 00:47:04,740
uh, to, uh, to be able to solve problems?

771
00:47:05,190 --> 00:47:09,390
Yeah. The, the one that I spent
most of my time on is in security.

772
00:47:09,391 --> 00:47:14,391
You can muddle most interactions as a
game where there's attacker is trying to

773
00:47:14,911 --> 00:47:18,990
break your system and you are the defender
trying to build a resilient system.

774
00:47:19,510 --> 00:47:23,910
Um, there's also domain
adversarial learning, which is uh,

775
00:47:24,020 --> 00:47:28,440
an approach to a domain adaptation that
looks really a lot like gans. Uh, the,

776
00:47:28,441 --> 00:47:29,274
the authors,

777
00:47:29,290 --> 00:47:32,860
the idea before the Gan paper came out
with their paper came out a little bit

778
00:47:32,861 --> 00:47:34,600
later. Um, and you know, they,

779
00:47:36,520 --> 00:47:38,230
they were very nice and cited again paper,

780
00:47:38,231 --> 00:47:41,800
but I know that they actually had
the idea before I came out. Um,

781
00:47:42,460 --> 00:47:45,610
domain adaptation is when you want to
train a machine learning model in one,

782
00:47:46,060 --> 00:47:50,530
one setting called a domain and then
deploy it in another domain later and he

783
00:47:50,531 --> 00:47:53,680
would like it to perform well in the
new domain even though the new domain is

784
00:47:53,681 --> 00:47:57,010
different from how it was
trained. Um, so for example,

785
00:47:57,580 --> 00:48:01,000
you might want to train on a really
clean image dataset like image net,

786
00:48:01,360 --> 00:48:06,000
but then deploy on users' phones where
the user is taking pictures in the dark

787
00:48:06,470 --> 00:48:06,830
pictures.

788
00:48:06,830 --> 00:48:10,570
We'll move in quickly and just pictures
that aren't really centered or composed

789
00:48:10,571 --> 00:48:11,404
all that well.

790
00:48:13,390 --> 00:48:15,850
When you take a normal
machine learning model,

791
00:48:15,851 --> 00:48:19,570
it often degrades really badly when you
moved to the new domain because it looks

792
00:48:19,571 --> 00:48:21,250
so different from what
the model is trained on.

793
00:48:21,830 --> 00:48:24,910
A domain adaptation algorithms
try to smooth out that gap.

794
00:48:25,420 --> 00:48:29,500
And the domain adversarial approach is
based on training a feature extractor

795
00:48:29,800 --> 00:48:33,610
where the features have the same
statistics regardless of which domain you

796
00:48:33,611 --> 00:48:36,790
extracted them on.
So in the domain adversarial game,

797
00:48:36,850 --> 00:48:40,150
you have one player that's a feature
extractor and another player that's a

798
00:48:40,151 --> 00:48:41,970
domain recognizer.
Uh,

799
00:48:41,980 --> 00:48:45,940
the domain recognizer wants to look at
the output of the feature extractor and

800
00:48:45,941 --> 00:48:49,180
guests, which of the two domains,
oh, the features came from.

801
00:48:49,300 --> 00:48:52,720
So it's a lot like the real versus
fake discriminator in Gans. Uh,

802
00:48:52,870 --> 00:48:57,070
and then the feature extractor you can
think of as loosely analogous to the

803
00:48:57,071 --> 00:49:00,290
generator and gans, except
what's trying to do here is, uh,

804
00:49:00,310 --> 00:49:04,870
both full the domain recognizer and to
not knowing which domain the data came

805
00:49:04,871 --> 00:49:08,560
from and also extract features
that are good for classification.

806
00:49:09,040 --> 00:49:13,780
So at the end of the day you can,
in in the cases where it works out,

807
00:49:13,781 --> 00:49:15,810
you can actually get um,

808
00:49:16,090 --> 00:49:19,870
features that work about
the same in both domains.

809
00:49:20,620 --> 00:49:24,340
Sometimes this has a drawback where in
order to make things work the same in

810
00:49:24,341 --> 00:49:26,290
both domains,
it just gets worse at the first one.

811
00:49:26,770 --> 00:49:30,070
But there are a lot of cases where
it actually works out well on both.

812
00:49:31,420 --> 00:49:35,830
Do you think of gas being useful in
the context of data augmentation? Yeah.

813
00:49:35,831 --> 00:49:40,600
One thing you could hope for with Ganz
is you could imagine I've got a limited

814
00:49:40,601 --> 00:49:44,860
training set and I'd like to make more
training data to train something else,

815
00:49:44,861 --> 00:49:45,850
like a classifier.

816
00:49:47,200 --> 00:49:52,200
You could train the Gan on the training
set and then create more data and then

817
00:49:53,110 --> 00:49:56,710
maybe the classifier would perform better
on the test set after training on this

818
00:49:56,711 --> 00:50:00,760
bigger generate a data set. Uh,
so that's the simplest version of,

819
00:50:01,170 --> 00:50:02,560
of something you might hope would work.

820
00:50:03,070 --> 00:50:06,640
I've never heard of that
particular approach working.
But I think there's some,

821
00:50:07,390 --> 00:50:11,530
there's some closely related things that
that I think could work in the future

822
00:50:11,531 --> 00:50:13,800
and some that actually
already have worked. Um,

823
00:50:14,080 --> 00:50:16,090
so if you think a little bit about
what we'd be hoping for for use,

824
00:50:16,100 --> 00:50:17,410
again to make more training data,

825
00:50:18,190 --> 00:50:22,600
we're hoping that the Gan will generalize
to new examples better than the

826
00:50:22,601 --> 00:50:25,330
classifier would have generalized
if it was trained on the same data.

827
00:50:25,960 --> 00:50:28,910
And I don't know of any reason to believe
that again would generalize better

828
00:50:28,911 --> 00:50:31,160
than the class of firewood.
Um,

829
00:50:31,460 --> 00:50:35,870
but what we might hope for is that the
gun could generalize differently from a

830
00:50:35,871 --> 00:50:39,260
specific classifier.
So one thing I think is worth trying to,

831
00:50:39,261 --> 00:50:40,310
I haven't personally tried,

832
00:50:40,311 --> 00:50:44,030
but someone could try is what have
you trained a whole lot of different

833
00:50:44,031 --> 00:50:46,010
generative models on
the same training set,

834
00:50:46,490 --> 00:50:50,000
create samples from all of them and
then train a classifier on that.

835
00:50:50,570 --> 00:50:53,900
Because each of the generative models
might generalize in a slightly different

836
00:50:53,901 --> 00:50:54,440
way.

837
00:50:54,440 --> 00:50:58,100
They might capture many different axes
of variation that one individual model

838
00:50:58,101 --> 00:50:58,850
wouldn't.

839
00:50:58,850 --> 00:51:02,660
And then the classifier can capture all
of those ideas by training on all of

840
00:51:02,661 --> 00:51:03,494
their data.

841
00:51:03,530 --> 00:51:06,800
So it'd be a little bit like making an
ensemble of classifiers and I think that

842
00:51:06,950 --> 00:51:10,070
Ganz, yeah, in a way I think
that could generalize better.

843
00:51:10,100 --> 00:51:13,850
The other thing that gans
are really good for is um,

844
00:51:14,960 --> 00:51:18,860
not necessarily generating new data
that's exactly like what you already have,

845
00:51:19,340 --> 00:51:24,170
but by a generating new data that has
different properties from the data you

846
00:51:24,171 --> 00:51:25,004
already had.

847
00:51:25,310 --> 00:51:28,760
One thing that you can do is you can
create differentially private data.

848
00:51:29,090 --> 00:51:32,990
So suppose that you have something like
medical records and you don't want to

849
00:51:32,991 --> 00:51:36,170
train a classifier on the medical
records and then publish the classifier

850
00:51:36,470 --> 00:51:39,110
because someone might be able to reverse
engineer some of the medical records

851
00:51:39,111 --> 00:51:40,290
you trained on.
Uh,

852
00:51:40,560 --> 00:51:45,320
there's a paper from Casey Greens lab
that shows how you can train again using

853
00:51:45,321 --> 00:51:48,110
differential privacy.
And then the samples from,

854
00:51:48,120 --> 00:51:51,990
again still have the same differential
privacy guarantees as the parameters of,

855
00:51:52,010 --> 00:51:52,700
again,

856
00:51:52,700 --> 00:51:57,700
so you can make a fake patient data for
other researchers to use and they can do

857
00:51:57,741 --> 00:51:59,180
almost anything they want with that data.

858
00:51:59,181 --> 00:52:02,000
Because it doesn't come from real people.

859
00:52:02,030 --> 00:52:07,030
And the differential privacy mechanism
gives you clear guarantees on how much

860
00:52:07,490 --> 00:52:09,560
the original peoples
data has been protected.

861
00:52:09,930 --> 00:52:13,050
That's really interesting. Actually. I
haven't heard you talk about that before.

862
00:52:13,070 --> 00:52:17,760
Um, in terms of fairness,
a scene from a triple AI,

863
00:52:17,761 --> 00:52:18,594
your talk,

864
00:52:19,500 --> 00:52:23,410
how can adversarial machine learning
help models beef more fair? Well,

865
00:52:23,420 --> 00:52:25,290
the respect of sensitive variables.

866
00:52:25,770 --> 00:52:26,280
Yeah.

867
00:52:26,280 --> 00:52:31,020
So there's a paper from Amos Starkey's
lab about how to learn machine learning

868
00:52:31,021 --> 00:52:35,670
models that are incapable of using
specific variables. So say for example,

869
00:52:35,671 --> 00:52:39,100
you want it to make predictions
that are not affected by gender. Um,

870
00:52:39,570 --> 00:52:42,570
it isn't enough to just leave gender
out of the input to the model.

871
00:52:42,780 --> 00:52:45,450
You can often infer gender from
a lot of other characteristics,

872
00:52:45,480 --> 00:52:48,570
like say that you have the person's name,
but you're not told their gender.

873
00:52:48,600 --> 00:52:52,560
Well if, if their name is Ian,
they're kind of obviously a man. Um,

874
00:52:53,700 --> 00:52:57,210
so what you'd like to do is make a machine
learning model that can still take in

875
00:52:57,211 --> 00:53:01,980
a lot of different attributes and make
it really accurate informed prediction,

876
00:53:02,610 --> 00:53:06,690
but be confident that it isn't reverse
engineering gender or another sensitive

877
00:53:06,691 --> 00:53:07,650
variable internally.

878
00:53:08,370 --> 00:53:12,360
You can do that using something very
similar to the domain adversarial approach

879
00:53:12,840 --> 00:53:17,550
where you have one player that's a feature
extractor and another player that's a

880
00:53:17,551 --> 00:53:21,990
feature analyzer and you want to make
sure that the feature analyzer is not able

881
00:53:21,991 --> 00:53:25,740
to guess the value of the sensitive
variable that you're trying keep private.

882
00:53:26,630 --> 00:53:29,860
Right. That's, yeah. I love this
approach. So what do, yeah. With the,

883
00:53:30,010 --> 00:53:34,790
with the feature, uh, you're
not able to infer, right?

884
00:53:34,791 --> 00:53:38,510
This sensitive variables. Yeah. Brilliant.
That's quite, quite brilliant and simple.

885
00:53:38,511 --> 00:53:39,344
Actually.

886
00:53:39,510 --> 00:53:44,510
Another way I think that
gans in particular could be
used for fairness would be

887
00:53:44,581 --> 00:53:49,581
to make something like a cycle again where
you can take data from one domain and

888
00:53:49,831 --> 00:53:53,840
convert it into another. We've seen
cycle again turning horses into zebras.

889
00:53:53,880 --> 00:53:58,440
We've seen, uh, other
unsupervised gans made by menu.

890
00:53:58,480 --> 00:54:02,880
Lou doing things like turning
day photos into night photos. Um,

891
00:54:03,700 --> 00:54:04,810
I think for fairness,

892
00:54:04,811 --> 00:54:09,460
you could imagine taking records for
people in one group and transforming them

893
00:54:09,461 --> 00:54:13,790
into analogous people in another group
and testing to see if there there are

894
00:54:13,800 --> 00:54:16,150
treated equitably across those two groups.

895
00:54:16,450 --> 00:54:18,700
There's a lot of things it'd be hard
to get right to make sure that the

896
00:54:18,701 --> 00:54:21,880
conversion process
itself is fair. Uh, and,

897
00:54:21,910 --> 00:54:25,360
and I don't think it's anywhere near
something that we could actually use yet.

898
00:54:25,390 --> 00:54:28,870
But if you could design that conversion
process very carefully and might give

899
00:54:28,871 --> 00:54:32,710
you a way of doing audits where you say,
what if we took people from this group,

900
00:54:33,100 --> 00:54:35,140
converted them into equivalent
people in another group?

901
00:54:35,440 --> 00:54:38,560
Does this system actually
treat them how it ought to?

902
00:54:39,960 --> 00:54:43,680
That's also really interesting,
you know, in, in a popular,

903
00:54:44,640 --> 00:54:48,660
in popular press and in
general in our imagination,

904
00:54:48,661 --> 00:54:53,040
you think a well gans are able to
generate data and you start to think about

905
00:54:53,390 --> 00:54:58,390
deep fakes or being able to sort of
maliciously generate data that fakes the

906
00:54:59,581 --> 00:55:03,150
identity of other people.
Is this something of a concern to you?

907
00:55:03,151 --> 00:55:06,570
Is this something,
if you look 10 20 years into the future,

908
00:55:06,900 --> 00:55:11,550
is that something that pops up in your
work and the work of the community that's

909
00:55:11,551 --> 00:55:12,900
working on January models?

910
00:55:13,510 --> 00:55:17,350
I'm a lot less concerned about 20
years from now then the next few years.

911
00:55:17,380 --> 00:55:22,380
I think there'll be a kind
of bumpy cultural transition
as people encounter this

912
00:55:22,841 --> 00:55:25,870
idea that there can be very realistic
videos and audio that aren't real.

913
00:55:26,260 --> 00:55:30,700
I think 20 years from now people will
mostly understand that you shouldn't

914
00:55:30,701 --> 00:55:33,370
believe something is real just
because you saw a video of it,

915
00:55:34,030 --> 00:55:38,690
people will expect to see that it's
been cryptographically signed, uh, or,

916
00:55:38,691 --> 00:55:43,330
or have some other mechanism to make
them believe that the content is real.

917
00:55:43,830 --> 00:55:45,670
Um,
there's already people working on this,

918
00:55:45,671 --> 00:55:50,290
like there's a startup called trupick
that provides a lot of mechanisms for

919
00:55:50,291 --> 00:55:53,080
authenticating that an
image is real there.

920
00:55:53,090 --> 00:55:56,610
There may be not quite up to
having a state actor tried to,

921
00:55:57,080 --> 00:56:00,950
to evade their, their
verification techniques, but uh,

922
00:56:01,060 --> 00:56:03,250
it's not that people are already
working on and I think we'll get right

923
00:56:03,251 --> 00:56:04,084
eventually.

924
00:56:04,120 --> 00:56:08,170
So you think authentication will,
will eventually win out.

925
00:56:08,230 --> 00:56:12,400
So being able to authenticate it,
this is real and this is not, yeah,

926
00:56:13,240 --> 00:56:17,020
as opposed to gans just getting better
and better or generative models being

927
00:56:17,021 --> 00:56:20,980
able to get better and better to
where the nature of what is real.

928
00:56:21,460 --> 00:56:22,090
I just don't think

929
00:56:22,090 --> 00:56:26,890
you'll ever be able look at the pixels
of a photo and tell you for sure that

930
00:56:26,891 --> 00:56:28,180
it's real or not real.

931
00:56:28,540 --> 00:56:33,540
And I think it would actually
be a somewhat dangerous
to rely on that approach

932
00:56:33,731 --> 00:56:35,080
too much.
Um,

933
00:56:35,110 --> 00:56:38,170
if you make a really good fake detector
and then someone's able to full your

934
00:56:38,171 --> 00:56:41,830
thick detector and your fake
detector says this image is not fake,

935
00:56:42,070 --> 00:56:45,190
then it's even more credible than if
you've never made a thick detector in the

936
00:56:45,191 --> 00:56:48,780
first place. What I do
think we'll get to is, um,

937
00:56:49,510 --> 00:56:52,880
systems that we can kind of
use behind the scenes for,

938
00:56:53,260 --> 00:56:58,030
to make estimates of what's going on and
maybe not like use them in court for a

939
00:56:58,031 --> 00:56:59,230
definitive analysis.

940
00:56:59,500 --> 00:57:04,500
I also think we will likely get
better authentication systems where,

941
00:57:05,150 --> 00:57:05,741
uh,
you know,

942
00:57:05,741 --> 00:57:09,430
if imagine that every phone
cryptographically signs
everything that comes out of

943
00:57:09,431 --> 00:57:14,050
it, uh, you wouldn't able to
conclusively tell that an image was real,

944
00:57:14,470 --> 00:57:19,470
but you would be able to tell somebody
who knew the appropriate private key for

945
00:57:19,631 --> 00:57:21,160
this phone,
uh,

946
00:57:21,250 --> 00:57:26,250
was actually able to sign this image
and upload it to this server at this

947
00:57:26,651 --> 00:57:28,450
timestamp. Right. Um,

948
00:57:28,870 --> 00:57:33,250
so you could imagine maybe you make
phones that have the private keys hardware

949
00:57:33,251 --> 00:57:35,120
embedded in them.
Um,

950
00:57:35,500 --> 00:57:39,190
if like a state security agency really
wants to infiltrate the company,

951
00:57:39,191 --> 00:57:40,800
they could probably,
you know,

952
00:57:40,920 --> 00:57:44,830
planet private key of their choice or
break open the chip and learn the private

953
00:57:44,831 --> 00:57:45,910
key or something like that.

954
00:57:46,150 --> 00:57:51,150
But it would make it a lot harder for an
adversary with fewer resources to fake

955
00:57:51,160 --> 00:57:53,170
things for most of us
who, yeah. Okay. Okay.

956
00:57:53,680 --> 00:57:57,850
So you mentioned the beer and
the bar and the new ideas.

957
00:57:58,230 --> 00:58:02,980
You are able to implement this or come
up with this new idea pretty quickly and

958
00:58:02,981 --> 00:58:04,180
implemented pretty quickly.

959
00:58:04,360 --> 00:58:08,410
Do you think there are still many such
groundbreaking ideas and deep learning

960
00:58:08,411 --> 00:58:11,290
that could be developed so quickly?
Yeah,

961
00:58:11,291 --> 00:58:14,560
I do think that there are a lot of ideas
that can be developed really quickly.

962
00:58:15,970 --> 00:58:18,220
Gans we're probably a little
bit of an outlier on the whole,

963
00:58:18,221 --> 00:58:20,790
like one hour timescale,
but um,

964
00:58:20,920 --> 00:58:25,120
just in terms of like low resource I
ideas where you do something really

965
00:58:25,121 --> 00:58:29,140
different on the algorithm scale
and get an a big payback. Um,

966
00:58:30,130 --> 00:58:33,790
I think it's not as likely that you'll
see that in terms of things like core

967
00:58:33,791 --> 00:58:34,930
machine learning technologies,

968
00:58:34,931 --> 00:58:38,290
like a better classifier or a better
reinforcement learning algorithm or a

969
00:58:38,291 --> 00:58:42,400
better generative model. Um,
if I had the gun idea today,

970
00:58:42,401 --> 00:58:46,930
it'd be a lot harder to prove that it
was useful than it was back in 2014

971
00:58:46,931 --> 00:58:51,931
because I would need to get it running
on something like image net or sell a bay

972
00:58:52,271 --> 00:58:55,990
at high resolution. You know, those
take a while to train. You couldn't,

973
00:58:56,380 --> 00:58:59,680
you couldn't train it in an hour and
know that it was something really new and

974
00:58:59,681 --> 00:59:03,070
exciting. Uh, back in 2014
treading on amnesty was enough.

975
00:59:04,240 --> 00:59:09,240
But there are other areas of machine
learning where I think a new idea could

976
00:59:09,761 --> 00:59:13,210
actually be developed really
quickly with low resources.

977
00:59:13,270 --> 00:59:17,440
What's your intuition about what areas
of machine learning are ripe for this?

978
00:59:17,710 --> 00:59:21,660
Yeah, so I think, um, fairness and uh,

979
00:59:21,661 --> 00:59:26,661
interpretability are areas where we just
really don't have any idea how anything

980
00:59:27,591 --> 00:59:30,350
should be done yet.
Like for interpretability,

981
00:59:30,351 --> 00:59:34,700
I don't think we even have
the right definitions and
even just defining a really

982
00:59:34,701 --> 00:59:35,690
useful concept,

983
00:59:36,050 --> 00:59:39,620
you don't even need to run any experiments
could have a huge impact on the field.

984
00:59:40,070 --> 00:59:43,150
We've seen that, for example,
in differential privacy that uh,

985
00:59:43,220 --> 00:59:47,660
Cynthia Dwork and her collaborators made
this technical definition of privacy

986
00:59:48,020 --> 00:59:50,030
where before a lot of
things are really mushy.

987
00:59:50,031 --> 00:59:54,200
And then with that definition you could
actually design randomized algorithms

988
00:59:54,201 --> 00:59:58,250
for accessing databases and guarantee
that they preserved individual people's

989
00:59:58,251 --> 01:00:01,610
privacy in a,
in like a mathematical quantitative sense.

990
01:00:03,470 --> 01:00:06,530
Right now we all talk a lot about how
interpretable different machine learning

991
01:00:06,531 --> 01:00:07,364
algorithms are,

992
01:00:07,520 --> 01:00:11,060
but it's really just people's opinion
and everybody probably has a different

993
01:00:11,061 --> 01:00:13,430
idea of what interpretability
means in their head.

994
01:00:13,790 --> 01:00:16,970
If we could define some concept
related to interpretability,

995
01:00:16,971 --> 01:00:20,180
that's actually measurable,
that would be a huge leap forward.

996
01:00:20,570 --> 01:00:23,720
Even without a new algorithm
that increases that quantity.

997
01:00:24,170 --> 01:00:28,460
And also once, once we had the
definition of differential privacy,

998
01:00:28,730 --> 01:00:31,010
it was fast to get the
algorithms that guaranteed it.

999
01:00:31,340 --> 01:00:34,460
So you could imagine once we have
definitions of good concepts and

1000
01:00:34,461 --> 01:00:35,300
interpretability,

1001
01:00:35,720 --> 01:00:39,170
we might be able to provide the
algorithms that have the interpretability

1002
01:00:39,171 --> 01:00:40,400
guarantees quickly too.

1003
01:00:42,470 --> 01:00:47,470
What do you think it takes to build a
system with human level intelligence as we

1004
01:00:49,131 --> 01:00:51,500
quickly venture into the philosophical,

1005
01:00:51,950 --> 01:00:55,910
so artificial general intelligence
when you get to take, um, I,

1006
01:00:56,720 --> 01:01:01,720
I think that it definitely takes a better
environments than we currently have

1007
01:01:02,751 --> 01:01:03,770
for training agents.

1008
01:01:03,771 --> 01:01:08,470
That we want them to have a really
wide diversity of experiences. Uh,

1009
01:01:08,720 --> 01:01:11,240
I also think it's going to take
really a lot of computation.

1010
01:01:11,780 --> 01:01:13,640
It's hard to imagine exactly how much,

1011
01:01:13,760 --> 01:01:18,170
so you're optimistic about simulation
simulating a variety of environments.

1012
01:01:18,171 --> 01:01:23,160
Is the path forward as opposed to it's
a necessary ingredient? Yeah, I, I,

1013
01:01:23,230 --> 01:01:27,560
I don't think that we're going to get
to artificial general intelligence by

1014
01:01:27,561 --> 01:01:32,090
training on fixed datasets or, or by
thinking really hard about the problem.

1015
01:01:32,091 --> 01:01:36,920
I think that the agent really needs
to interact and have a variety of

1016
01:01:36,921 --> 01:01:41,450
experiences within the same a lifespan.

1017
01:01:41,570 --> 01:01:46,190
And today we have many different models
that can each do one thing and we tend

1018
01:01:46,191 --> 01:01:49,770
to train them on one dataset
or one RL environment. Um,

1019
01:01:50,090 --> 01:01:53,960
sometimes they're actually papers about
getting one set of perimeters to perform

1020
01:01:53,961 --> 01:01:56,330
well in many different RL environments,

1021
01:01:56,960 --> 01:02:01,010
but we don't really have anything like
an agent that goes seamlessly from one

1022
01:02:01,011 --> 01:02:03,440
type of experience to another and,

1023
01:02:03,470 --> 01:02:06,350
and really integrates all the different
things that it does over the course of

1024
01:02:06,351 --> 01:02:10,550
its life. Uh, when we do see
multi-agent environments,

1025
01:02:10,551 --> 01:02:14,630
they tend to be, um, there's so
many, uh, multi environment agents.

1026
01:02:14,660 --> 01:02:17,110
They tend to be similar environments.
Like all,

1027
01:02:17,111 --> 01:02:20,370
all of them are playing like an
action based game. Right? Um,

1028
01:02:20,400 --> 01:02:23,190
we don't really have an agent
that goes from, you know,

1029
01:02:23,220 --> 01:02:27,450
playing a video game to like
reading the Wall Street Journal, uh,

1030
01:02:27,480 --> 01:02:32,070
to predicting how effective a molecule
will be as a drug or something like that.

1031
01:02:33,220 --> 01:02:36,700
What do you think is a good test
for intelligence in you view?

1032
01:02:36,970 --> 01:02:40,270
Spend a lot of benchmarks
started with the, uh,

1033
01:02:40,271 --> 01:02:43,990
with Alan Turing and natural
conversation and being good,

1034
01:02:44,020 --> 01:02:45,970
being a good benchmark for intelligence.

1035
01:02:46,610 --> 01:02:48,650
What are,
what would a

1036
01:02:48,730 --> 01:02:49,563
uh,

1037
01:02:50,050 --> 01:02:54,430
Ian Goodfellow sit back and be really
damn impressed if a system was able to

1038
01:02:54,431 --> 01:02:55,264
accomplish,

1039
01:02:55,690 --> 01:02:59,740
um, something that doesn't take a
lot of glue from human engineers.

1040
01:02:59,741 --> 01:03:04,540
So imagine that instead
of having to go to the,

1041
01:03:04,630 --> 01:03:04,960
uh,

1042
01:03:04,960 --> 01:03:09,960
CFR website and download CFR 10 and then
write a python script to parse it and

1043
01:03:10,451 --> 01:03:11,284
all that,

1044
01:03:11,350 --> 01:03:16,350
you could just point an agent at the
CFR 10 problem and it downloads and

1045
01:03:18,191 --> 01:03:22,390
extracts the data and trains a model
and starts giving you predictions. Um,

1046
01:03:22,450 --> 01:03:27,400
I feel like something that doesn't
need to have every step of the pipeline

1047
01:03:27,401 --> 01:03:31,570
assembled for it. Definitely understand.
So what it's doing is auto Amelle moving,

1048
01:03:31,571 --> 01:03:31,670
right?

1049
01:03:31,670 --> 01:03:33,810
So that direction are you thinking wave

1050
01:03:33,960 --> 01:03:37,970
bigger? Mel has mostly
been moving toward, um,

1051
01:03:38,220 --> 01:03:42,010
once we've built all the glue, can
the machine learning system, uh,

1052
01:03:42,150 --> 01:03:45,960
to design the architecture really well?
And so I'm more of saying like,

1053
01:03:47,310 --> 01:03:50,700
if something knows how to preprocess
the data so that it successfully

1054
01:03:50,701 --> 01:03:51,990
accomplishes the task,

1055
01:03:52,380 --> 01:03:56,370
then it would be very hard to argue that
it doesn't truly understand the task in

1056
01:03:56,371 --> 01:03:57,570
some fundamental sense.

1057
01:03:58,540 --> 01:04:01,380
And I don't necessarily know that that's
like the philosophical definition of

1058
01:04:01,381 --> 01:04:03,810
intelligence, but that's something
that'd be really cool to build.

1059
01:04:03,811 --> 01:04:05,610
That'd be really useful
and would impress me.

1060
01:04:05,611 --> 01:04:09,180
And when he convinced me that we've
made a step forward in real AI,

1061
01:04:09,450 --> 01:04:10,283
so you give it

1062
01:04:11,100 --> 01:04:14,430
the URL for Wikipedia and then,
uh,

1063
01:04:14,940 --> 01:04:18,330
at next day expect it to
be able to solve CFR 10

1064
01:04:18,730 --> 01:04:23,380
or like you type in a paragraph explaining
what you want it to do and it figures

1065
01:04:23,381 --> 01:04:26,410
out what web searches it should
run and downloads all the,

1066
01:04:26,440 --> 01:04:27,850
all the necessary ingredients.

1067
01:04:28,290 --> 01:04:32,250
So, uh, you have a very clear,

1068
01:04:32,251 --> 01:04:37,080
calm way of speaking.
No ums, easy to edit.

1069
01:04:37,620 --> 01:04:40,230
I've seen comments for both you and I,
uh,

1070
01:04:40,260 --> 01:04:43,710
have been identified as both
potentially being robots.

1071
01:04:44,230 --> 01:04:47,970
If you have to prove to the world that
you are indeed human, how would you do it?

1072
01:04:50,030 --> 01:04:52,910
Uh, well I can understand
thinking that I'm a robot.

1073
01:04:55,220 --> 01:04:58,400
It's the flip side. Turing
test I think. Yeah. Yeah.

1074
01:04:58,401 --> 01:05:01,070
The proof prove you're human test.
Um,

1075
01:05:02,400 --> 01:05:03,960
she see you have to,
uh,

1076
01:05:04,470 --> 01:05:09,260
is there something that's truly
unique in your mind? I suppose as it,

1077
01:05:09,270 --> 01:05:12,780
does it go back to just natural
language again, just being able to, uh,

1078
01:05:13,870 --> 01:05:17,970
proving that I'm not a robot with today's
technology know that's pretty straight

1079
01:05:17,971 --> 01:05:22,240
forward. Like my conversation today
hasn't veered off into, you know,

1080
01:05:22,630 --> 01:05:25,540
talking about the stock
market or something because
it was in my training data.

1081
01:05:25,960 --> 01:05:29,140
But I guess more generally trying to
prove that something is real from the

1082
01:05:29,141 --> 01:05:31,390
content alone,
it was incredibly hard.

1083
01:05:31,420 --> 01:05:33,850
That's one of the main things I've
gotten out of Mike and Research that

1084
01:05:35,710 --> 01:05:37,540
you can simulate almost anything.

1085
01:05:37,660 --> 01:05:41,950
And so you have to really step back to
a separate channel to prove that song is

1086
01:05:41,951 --> 01:05:42,784
real.
So like,

1087
01:05:43,090 --> 01:05:46,840
I guess I should have had myself stepped
on a blockchain when I was born or

1088
01:05:46,841 --> 01:05:50,800
something, but I didn't do that. So I
coined to my own research methodology.

1089
01:05:50,801 --> 01:05:53,860
There's just no way to know
at this point. So what, uh,

1090
01:05:53,890 --> 01:05:57,910
last question problem stands off for
you that you're really excited about

1091
01:05:57,911 --> 01:06:02,860
challenging in the near future? So I think
a resistance to adversarial examples,

1092
01:06:02,920 --> 01:06:06,790
figuring out how to make machine learning
secure against an adversary who wants

1093
01:06:06,791 --> 01:06:09,530
to interfere it and control with it is,
uh,

1094
01:06:09,610 --> 01:06:13,210
one of the most important
things researchers today
could solve in all domains,

1095
01:06:13,230 --> 01:06:17,690
in image language driving and every,

1096
01:06:17,730 --> 01:06:22,360
I guess I'm most concerned about domains
we haven't really encountered yet. Like,

1097
01:06:22,660 --> 01:06:26,980
like imagine 20 years from now when we're
using advanced AI is to do things we

1098
01:06:26,981 --> 01:06:28,890
haven't even thought of yet.
Um,

1099
01:06:28,960 --> 01:06:33,960
like if you ask people what
are the important problems
in security of phones in

1100
01:06:35,831 --> 01:06:40,420
like 2002, I don't think we would have
anticipated that we're using them for,

1101
01:06:40,940 --> 01:06:43,270
you know, nearly as many things
as we're using them for today.

1102
01:06:43,660 --> 01:06:46,930
I think it's going to be like that with
AI that you can kind of try to speculate

1103
01:06:46,931 --> 01:06:47,920
about where it's going.

1104
01:06:47,921 --> 01:06:52,240
But really the business opportunities
that end up taking off would be hard to

1105
01:06:52,241 --> 01:06:53,260
predict ahead of time.

1106
01:06:54,190 --> 01:06:57,700
Well you can predict ahead of time is
that almost anything you can do with

1107
01:06:57,701 --> 01:06:58,361
machine learning,

1108
01:06:58,361 --> 01:07:03,361
you would like to make sure that people
can't get it to do what they want rather

1109
01:07:03,431 --> 01:07:08,230
than what you want. Just by showing it a
funny QR code or a funny input pattern.

1110
01:07:08,470 --> 01:07:10,840
And you think that this set
of methodologies to do that,

1111
01:07:10,960 --> 01:07:15,710
it can be bigger than any one domain. And
that's the thing. So yeah. Yeah. Like, um,

1112
01:07:16,030 --> 01:07:19,430
one methodology that I think is not,

1113
01:07:19,480 --> 01:07:23,320
not a specific methodology be like a
category of solutions that I'm excited

1114
01:07:23,321 --> 01:07:27,190
about today was making dynamic models
that change every time they make a

1115
01:07:27,191 --> 01:07:32,050
prediction. So right now we tend to train
models and then after they're trained,

1116
01:07:32,051 --> 01:07:36,610
we freeze them and we just use the same
rule to classify everything that comes

1117
01:07:36,611 --> 01:07:40,720
in from then on. Uh, that's
really a sitting duck from
a security point of view.

1118
01:07:41,190 --> 01:07:45,320
Uh, if you always output the same
answer for the same input, um,

1119
01:07:45,460 --> 01:07:49,270
then people can just run and puts
through until they find a mistake that

1120
01:07:49,271 --> 01:07:52,810
benefits them and then they use the same
mistake over and over and over again.

1121
01:07:53,360 --> 01:07:58,270
Um, I think having a model that updates
its predictions so that it's harder to

1122
01:07:58,271 --> 01:08:01,750
predict what you're going to get.
We'll make it harder for the,

1123
01:08:02,020 --> 01:08:05,470
for an adversary to really take control
of the system and make it do what they

1124
01:08:05,471 --> 01:08:07,800
want it to do. Yeah.
Models that maintain a,

1125
01:08:07,840 --> 01:08:12,580
a bit of a sense of mystery about them
cause they always keep changing. Yeah.

1126
01:08:12,760 --> 01:08:15,740
And thanks so much for talking today.
That was awesome. Thank you for coming in.

1127
01:08:15,860 --> 01:08:16,010
That's

1128
01:08:16,010 --> 01:08:16,490
great to see you.

